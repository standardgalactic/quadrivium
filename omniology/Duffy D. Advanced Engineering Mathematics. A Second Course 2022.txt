
 
 
Advanced 
Engineering 
Mathematics  
 
 

 
Advances in Applied Mathematics 
 
Series Editors:  
Daniel Zwillinger, H. T. Banks 
 
 
Advanced Mathematical Modeling with Technology 
William P. Fox, Robert E. Burks 
 
Introduction to Quantum Control and Dynamics 
Domenico Dâ€™Alessandro 
 
Handbook of Radar Signal Analysis 
Bassem R. Mahafza, Scott C. Winton, Atef Z. Elsherbeni 
 
Separation of Variables and Exact Solutions to Nonlinear PDEs 
Andrei D. Polyanin, Alexei I. Zhurov 
 
Boundary Value Problems on Time Scales, Volume I 
Svetlin Georgiev, Khaled Zennir 
 
Boundary Value Problems on Time Scales, Volume II 
Svetlin Georgiev, Khaled Zennir 
 
Observability and Mathematics 
Fluid Mechanics, Solutions of Navier-Stokes Equations, and Modeling 
Boris Khots 
 
Handbook of Differential Equations, 4th Edition 
Daniel Zwillinger, Vladimir Dobrushkin 
Experimental Statistics and Data Analysis for Mechanical and Aerospace Engineers 
James Middleton 
 
Advanced Engineering Mathematics with MATLAB 
Dean G. Duffy 
 
Handbook of Fractional Calculus for Engineering and Science 
Harendra Singh, H. M. Srivastava, Juan J Nieto 
 
Advanced Engineering Mathematics 
A Second Course  
Dean G. Duffy 
 
https://www.routledge.com/Advances-in-Applied-Mathematics/book-
series/CRCADVAPPMTH?pd=published,forthcoming&pg=1&pp=12&so=pub&view=list 

 
Advances in Applied Mathematics 
 
Advanced 
Engineering 
Mathematics  
A Second Course 
 
 
 
Dean G. Duffy 
 
 
 
 
 
 
 
 

 
MATLABÂ® is a trademark of The MathWorks, Inc. and is used with permission. The MathWorks does not 
warrant the accuracy of the text or exercises in this book. This bookâ€™s use or discussion of MATLABÂ® 
software or related products does not constitute endorsement or sponsorship by The MathWorks of a 
particular pedagogical approach or particular use of the MATLABÂ® software. 
First edition published 2022  
by CRC Press 
6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742 
and by CRC Press 
2 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN 
Â© 2022 Dean G. Duffy 
CRC Press is an imprint of Taylor & Francis Group, LLC 
Reasonable efforts have been made to publish reliable data and information, but the author and publisher 
cannot assume responsibility for the validity of all materials or the consequences of their use. The authors 
and publishers have attempted to trace the copyright holders of all material reproduced in this publication 
and apologize to copyright holders if permission to publish in this form has not been obtained. If any 
copyright material has not been acknowledged please write and let us know so we may rectify in any future 
reprint. 
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, 
transmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter 
invented, including photocopying, microfilming, and recording, or in any information storage or retrieval 
system, without written permission from the publishers. 
For permission to photocopy or use material electronically from this work, access www.copyright.com or 
contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 01923, 978-750-
8400. For works that are not available on CCC please contact mpkbookspermissions@tandf.co.uk 
Trademark notice: Product or corporate names may be trademarks or registered trademarks and are used 
only for identification and explanation without intent to infringe. 
ISBN: 9781032133423 (hbk) 
ISBN: 9781032223452 (pbk)  
ISBN: 9781003272205 (ebk) 
 
DOI: 10.1201/9781003272205 
 
Publisher's note: This book has been prepared from camera-ready copy provided by the authors 

Dedicated to the Brigade of Midshipmen
and the Corps of Cadets
v


Contents
Dedication
v
Contents
vii
Acknowledgments
xiii
Author
xv
Introduction
xvii
List of Deï¬nitions
xix
x
Ï€/3
1
C
Ï€i
e
/6
3
y
C2
C
Chapter 1:
Complex Variables
1
1.1
Complex Numbers
1
1.2
Finding Roots
5
1.3
The Derivative in the Complex Plane: The Cauchy-Riemann Equations
8
1.4
Line Integrals
16
1.5
The Cauchy-Goursat Theorem
20
vii

viii
Advanced Engineering Mathematics: A Second Course
1.6
Cauchyâ€™s Integral Formula
23
1.7
Taylor and Laurent Expansions and Singularities
27
1.8
Theory of Residues
33
1.9
Evaluation of Real Deï¬nite Integrals
37
1.10 Cauchyâ€™s Principal Value Integral
50
1.11 Conformal Mapping
59
t > 3
(c,0)
t < 3
Chapter 2: Advanced
Transform Methods
77
2.1 Inversion of Fourier Transforms by Contour Integration
77
2.2 Inversion of Laplace Transforms by Contour Integration
92
2.3 Integral Equations
100
2.4 The Solution of the Wave Equation by Using Laplace Transforms
105
2.5 The Solution of the Heat Equation by Using Laplace Transforms
129
2.6 The Solution of Laplaceâ€™s Equation by Using Laplace Transforms
154
0.0
1.0
2.0
3.0
Ï‰Î¤
0.1
1.0
10.0
Ratio of quadrature amplitudes to ideal integration      
Trapezoidal
Simpsonâ€™s
3/8âˆ’rule
Rule
Simpsonâ€™s
1/3âˆ’rule
Ideal Rule
Chapter 3:
The Z-Transform
159
3.1 The Relationship of the Z-Transform to the Laplace Transform
160
3.2 Some Useful Properties
164
3.3 Inverse Z-Transforms
173
3.4 Solution of Diï¬€erence Equations
183
3.5 Stability of Discrete-Time Systems
189

Table of Contents
ix
âˆ’15
âˆ’10
âˆ’5
0
5
10
15
âˆ’1
âˆ’0.8
âˆ’0.6
âˆ’0.4
âˆ’0.2
0
0.2
0.4
0.6
0.8
1
t
exact Hilbert transform
computed Hilbert transform
Chapter 4:
The Hilbert Transform
195
4.1 Deï¬nition
195
4.2 Some Useful Properties
205
4.3 Analytic Signals
211
4.4 Causality: The Kramers-Kronig Relationship
213
Chapter 5:
Greenâ€™s Functions
217
5.1 What Is a Greenâ€™s Function?
217
5.2 Ordinary Diï¬€erential Equations
223
5.3 Joint Transform Method
243
5.4 Wave Equation
247
5.5 Heat Equation
256
5.6 Helmholtzâ€™s Equation
266
5.7 Galerkin Method
285
âˆ’4
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
4
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
 x
Estimated and true PDF
Chapter 6:
Probability
293
6.1 Review of Set Theory
294
6.2 Classic Probability
295
6.3 Discrete Random Variables
308

x
Advanced Engineering Mathematics: A Second Course
6.4 Continuous Random Variables
313
6.5 Mean and Variance
318
6.6 Some Commonly Used Distributions
325
6.7 Joint Distributions
333
âˆ’2
âˆ’1.5
âˆ’1
âˆ’0.5
0
0.5
1
1.5
2
âˆ’1
âˆ’0.5
0
0.5
1
1.5
2
2.5
x
y
Chapter 7:
Random Processes
345
7.1 Fundamental Concepts
349
7.2 Power Spectrum
354
7.3 Two-State Markov Chains
357
7.4 Birth and Death Processes
366
7.5 Poisson Processes
377
Rt/L
0
0.5
1
1.5
2
2.5
3
3.5
4
I(t)
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
Chapter 8:
ItË†oâ€™s Stochastic Calculus
385
8.1 Random Diï¬€erential Equations
386
8.2 Random Walk and Brownian Motion
395
8.3 ItË†oâ€™s Stochastic Integral
406
8.4 ItË†oâ€™s Lemma
410
8.5 Stochastic Diï¬€erential Equations
419
8.6 Numerical Solution of Stochastic Diï¬€erential Equations
427
Answers to the Odd-Numbered Problems
435
Index
443

Acknowledgments
I would like to thank the many midshipmen and cadets who have taken engineering
mathematics from me. They have been willing or unwilling guinea pigs in testing out many
of the ideas and problems in this book. Special thanks go to Prof. William S. Price of
the University of Western Sydney (Australia) for his suggestions concerning a chapter on
Greenâ€™s functions. Most of the plots and calculations were done using MATLAB R
âƒ.
MATLAB is a registered trademark of
The MathWorks Inc.
24 Prime Park Way
Natick, MA 01760-1500
Phone: (508) 647-7000
Email: info@mathworks.com
www.mathworks.com
xi


Author
Dean G. Duï¬€y received his bachelor of science in geophysics from Case Institute of
Technology (Cleveland, Ohio) and his doctorate of science in meteorology from the Mas-
sachusetts Institute of Technology (Cambridge, Massachusetts). He served in the United
States Air Force from September 1975 to December 1979 as a numerical weather prediction
oï¬ƒcer. After his military service, he began a twenty-ï¬ve-year (1980 to 2005) association
with NASA at the Goddard Space Flight Center (Greenbelt, Maryland) where he focused on
numerical weather prediction, oceanic wave modeling, and dynamical meteorology. He also
wrote papers in the areas of Laplace transforms, antenna theory, railroad tracks, and heat
conduction. In addition to his NASA duties, he taught engineering mathematics, diï¬€erential
equations, and calculus at the United States Naval Academy (Annapolis, Maryland) and
the United States Military Academy (West Point, New York). Drawing from his teaching
experience, he has written several books on transform methods, engineering mathematics,
Greenâ€™s functions, and mixed-boundary-value problems.
xiii


Introduction
For the last twenty-ï¬ve years I have written a series of engineering mathematics books.
When it came to revising my fourth edition, I realized that something radical must be
done. The encyclopedic engineering mathematics tome is dead, killed by the growth of
the Internet and studentsâ€™ unwillingness to buy these books. When I surveyed the current
variety of engineering mathematics courses, I realized that I needed to write two books.
The companion to the present volume (Advanced Engineering Mathematics with MATLAB)
would only focus on those topics that are currently taught in most advanced engineering
mathematics courses. When one had studied that volume, he/she could feel conï¬dent that
he/she had a solid knowledge of those mathematical techniques used in current engineering
and scientiï¬c courses.
This volume (Advanced Engineering Mathematics: A Second Course) is my attempt to
look into the future of advanced engineering mathematics courses. Some of the material,
such as complex variables and probability, is currently taught to engineers, although not
usually in courses entitled advanced engineering mathematics. I have included these topics
because they are required for transform methods and random processes.
One trend that I see is that entering freshmen are increasingly likely to have had
calculus in high school. This means that they will probably place out of the traditional
diï¬€erential and integral calculus courses in their freshman year, allowing them to take
multivariable calculus and diï¬€erential equations during their freshman year and taking
advanced engineering mathematics courses during their sophomore year. Therefore, the
question arises as to nature of these courses. The answer appears to be that the current
traditional engineering mathematics course will occur during the fall semester and some
other mathematics course will occur during the spring semester. The present volume is
designed to meet this need, as well as stand as the advanced engineering mathematics
text on its own. For those past formal education, this book provides the professional with
powerful mathematical techniques.
The ï¬rst ï¬ve chapters are aimed at the systems, communications and electrical en-
gineering crowd: those involved in the digital revolution. First, that portion of complex
xv

xvi
Advanced Engineering Mathematics: A Second Course
variable theory is presented so that the reader will feel prepared in dealing with transform
methods. For example, Chapter 2 shows how complex variables can be used to invert par-
ticularly complicated Fourier and Laplace transforms. This chapter also illustrates how
transform methods can solve the heat, wave and Laplaceâ€™s equations.
In Chapters 3 and 4 we study two transforms, the z- and Hilbert transforms, that are
currently important in the digital revolution. Chapter 3 introduces the z-transform by ï¬rst
giving its deï¬nition and then developing some of its general properties. We also illustrate
how to compute the inverse by long division, partial fractions, and contour integration.
Finally, we use z-transforms to solve diï¬€erence equations, especially with respect to the
stability of the system.
The Hilbert transform is important in the explosion of interest in communications. The
Hilbert transform is introduced in Section 4.1 and its properties are explored in Section 4.2.
Two important applications of Hilbert transforms are introduced in Sections 4.3 and 4.4,
namely the concept of analytic signals and the Kramers-Kronig relationship.
To round out this area we present Greenâ€™s function in Chapter 5. Greenâ€™s function
gives the response of a system to impulse forcing without the clouding eï¬€ects of a particular
forcing function or initial conditions. Each successive section deals with ordinary, wave, heat
and Helmholtzâ€™s equations. The solution to general problems follows from the superposition
integral.
The book concludes by turning to the future. It is now recognized that random pro-
cesses are useful in describing many physical systems. We begin by introducing the fun-
damental concepts behind probability in Chapter 6 and random processes in Chapter 7.
Chapter 6 introduces the student to the concepts of probability distributions, mean, and
variance because these topics appear so frequently in random processes. Chapter 7 explores
common random processes such as Poisson processes and birth and death.
A unique aspect of this book appears in Chapter 8, which is devoted to stochastic
calculus. We start by exploring deterministic diï¬€erential equations with a stochastic forcing.
Next, the important stochastic process of Brownian motion is developed in depth. Using this
Brownian motion, we introduce the concept of (ItË†o) stochastic integration, ItË†oâ€™s lemma, and
stochastic diï¬€erential equations. The chapter concludes with various numerical methods to
integrate stochastic diï¬€erential equations.
MATLAB is still employed to reinforce the concepts that are taught. Of course, this
book still continues my principle of including a wealth of examples from the scientiï¬c and
engineering literature. Worked solutions to all of the problems are given at the end.

List of Deï¬nitions
Function
Deï¬nition
Î´(t âˆ’a)
=
 âˆ,
t = a,
0,
t Ì¸= a,
Z âˆ
âˆ’âˆ
Î´(t âˆ’a) dt = 1
erf(x)
=
2
âˆšÏ€
Z x
0
eâˆ’y2 dy
Î“(x)
gamma function
H(t âˆ’a)
=
 1,
t > a,
0,
t < a.
â„‘(z)
imaginary part of the complex variable z
In(x)
modiï¬ed Bessel function of the ï¬rst kind and order n
Jn(x)
Bessel function of the ï¬rst kind and order n
Kn(x)
modiï¬ed Bessel function of the second kind and order n
Pn(x)
Legendre polynomial of order n
â„œ(z)
real part of the complex variable z
sgn(t âˆ’a)
=
 âˆ’1,
t < a,
1,
t > a.
Yn(x)
Bessel function of the second kind and order n
xvii


x
Ï€/3
1
C
Ï€i
e
/6
3
y
C2
C
Chapter 1
Complex Variables
The theory of complex variables was originally developed by mathematicians as an aid
in understanding functions. Functions of a complex variable enjoy many powerful properties
that their real counterparts do not. That is not why we will study them. For us they provide
the keys for the complete mastery of transform methods and diï¬€erential equations.
In this chapter all of our work points to one objective: integration on the complex
plane by the method of residues. For this reason we minimize discussions of limits and
continuity, which play such an important role in conventional complex variables, in favor
of the computational aspects. We begin by introducing some simple facts about complex
variables. Then we progress to diï¬€erential and integral calculus on the complex plane.
1.1 COMPLEX NUMBERS
A complex number is any number of the form a+bi, where a and b are real and i = âˆšâˆ’1.
We denote any member of a set of complex numbers by the complex variable z = x + iy.
The real part of z, usually denoted by â„œ(z), is x while the imaginary part of z, â„‘(z), is y.
The complex conjugate, z or zâˆ—, of the complex number a + bi is a âˆ’bi.
Complex numbers obey the fundamental rules of algebra. Thus, two complex numbers
a + bi and c + di are equal if and only if a = c and b = d. Just as real numbers have
the fundamental operations of addition, subtraction, multiplication, and division, so too do
complex numbers. These operations are deï¬ned:
Addition
(a + bi) + (c + di) = (a + c) + (b + d)i
(1.1.1)
Subtraction
(a + bi) âˆ’(c + di) = (a âˆ’c) + (b âˆ’d)i
(1.1.2)
1

2
Advanced Engineering Mathematics: A Second Course
Multiplication
(a + bi)(c + di) = ac + bci + adi + i2bd = (ac âˆ’bd) + (ad + bc)i
(1.1.3)
Division
a + bi
c + di = a + bi
c + di
c âˆ’di
c âˆ’di = ac âˆ’adi + bci âˆ’bdi2
c2 + d2
= ac + bd + (bc âˆ’ad)i
c2 + d2
.
(1.1.4)
The absolute value or modulus
of a complex number a + bi, written |a + bi|, equals
âˆš
a2 + b2. Additional properties include:
|z1z2z3 Â· Â· Â· zn| = |z1||z2||z3| Â· Â· Â· |zn|
(1.1.5)
|z1/z2| = |z1|/|z2|
if
z2 Ì¸= 0
(1.1.6)
|z1 + z2 + z3 + Â· Â· Â· + zn| â‰¤|z1| + |z2| + |z3| + Â· Â· Â· + |zn|
(1.1.7)
and
|z1 + z2| â‰¥|z1| âˆ’|z2|.
(1.1.8)
The use of inequalities with complex variables has meaning only when they involve absolute
values.
It is often useful to plot the complex number x + iy as a point (x, y) in the xy-plane,
now called the complex plane. Figure 1.1.1 illustrates this representation.
This geometrical interpretation of a complex number suggests an alternative method
of expressing a complex number: the polar form. From the polar representation of x and y,
x = r cos(Î¸)
and
y = r sin(Î¸),
(1.1.9)
where r =
p
x2 + y2 is the modulus, amplitude, or absolute value of z and Î¸ is the argument
or phase, we have that
z = x + iy = r[cos(Î¸) + i sin(Î¸)].
(1.1.10)
However, from the Taylor expansion of the exponential in the real case,
eiÎ¸ =
âˆ
X
k=0
(Î¸i)k
k! .
(1.1.11)
Expanding Equation 1.1.11,
eiÎ¸ = 1 âˆ’Î¸2
2! + Î¸4
4! âˆ’Î¸6
6! + Â· Â· Â· + i

Î¸ âˆ’Î¸3
3! + Î¸5
5! âˆ’Î¸7
7! + Â· Â· Â·

(1.1.12)
= cos(Î¸) + i sin(Î¸).
(1.1.13)
Equation 1.1.13 is Eulerâ€™s formula. Consequently, we may express Equation 1.1.10 as
z = reiÎ¸,
(1.1.14)
which is the polar form of a complex number. Furthermore, because
zn = rneinÎ¸
(1.1.15)

Complex Variables
3
Î¸
(x,y)
r
y
x
Figure 1.1.1: The complex plane.
by the law of exponents,
zn = rn[cos(nÎ¸) + i sin(nÎ¸)].
(1.1.16)
Equation 1.1.16 is De Moivreâ€™s theorem.
â€¢ Example 1.1.1
Let us simplify the following complex number:
3 âˆ’2i
âˆ’1 + i = 3 âˆ’2i
âˆ’1 + i Ã— âˆ’1 âˆ’i
âˆ’1 âˆ’i = âˆ’3 âˆ’3i + 2i + 2i2
1 + 1
= âˆ’5 âˆ’i
2
= âˆ’5
2 âˆ’i
2. (1.1.17)
âŠ“âŠ”
â€¢ Example 1.1.2
Let us reexpress the complex number âˆ’
âˆš
6 âˆ’i
âˆš
2 in polar form. From Equation 1.1.9
r = âˆš6 + 2 and Î¸ = tanâˆ’1(b/a) = tanâˆ’1(1/
âˆš
3) = Ï€/6 or 7Ï€/6. Because âˆ’
âˆš
6 âˆ’i
âˆš
2 lies in
the third quadrant of the complex plane, Î¸ = 7Ï€/6 and
âˆ’
âˆš
6 âˆ’i
âˆš
2 = 2
âˆš
2e7Ï€i/6.
(1.1.18)
Note that Equation 1.1.18 is not a unique representation because Â±2nÏ€ may be added to
7Ï€/6 and we still have the same complex number since
ei(Î¸Â±2nÏ€) = cos(Î¸ Â± 2nÏ€) + i sin(Î¸ Â± 2nÏ€) = cos(Î¸) + i sin(Î¸) = eiÎ¸.
(1.1.19)
For uniqueness we often choose n = 0 and deï¬ne this choice as the principal branch. Other
branches correspond to diï¬€erent values of n.
âŠ“âŠ”
â€¢ Example 1.1.3
Find the curve described by the equation |z âˆ’z0| = a.
From the deï¬nition of the absolute value,
p
(x âˆ’x0)2 + (y âˆ’y0)2 = a
(1.1.20)

4
Advanced Engineering Mathematics: A Second Course
or
(x âˆ’x0)2 + (y âˆ’y0)2 = a2.
(1.1.21)
Equation 1.1.21, and hence |z âˆ’z0| = a, describes a circle of radius a with its center located
at (x0, y0). Later on, we shall use equations such as this to describe curves in the complex
plane.
âŠ“âŠ”
â€¢ Example 1.1.4
As an example in manipulating complex numbers, let us show that

a + bi
b + ai
 = 1.
(1.1.22)
We begin by simplifying
a + bi
b + ai = a + bi
b + ai Ã— b âˆ’ai
b âˆ’ai =
2ab
a2 + b2 + b2 âˆ’a2
a2 + b2 i.
(1.1.23)
Therefore,

a + bi
b + ai
 =
s
4a2b2
(a2 + b2)2 + b4 âˆ’2a2b2 + a4
(a2 + b2)2
=
s
a4 + 2a2b2 + b4
(a2 + b2)2
= 1.
(1.1.24)
MATLAB can also be used to solve this problem. Typing the commands
>> syms a b real
>> abs((a+b*i)/(b+a*i))
yields
ans =
1
Note that you must declare a and b real in order to get the ï¬nal result.
Problems
Simplify the following complex numbers. Represent the solution in the Cartesian form a+bi.
Check your answers using MATLAB.
1.
5i
2 + i
2. 5 + 5i
3 âˆ’4i +
20
4 + 3i
3. 1 + 2i
3 âˆ’4i + 2 âˆ’i
5i
4. (1 âˆ’i)4
5. i(1 âˆ’i
âˆš
3)(
âˆš
3 + i)
6. (7 + i)(1 âˆ’5i)
(4 âˆ’i)(6 + i)
Represent the following complex numbers in polar form:
7.
âˆ’i
8.
âˆ’4
9.
2 + 2
âˆš
3 i
10.
âˆ’5 + 5i
11.
2 âˆ’2i
12.
âˆ’1 +
âˆš
3 i
13. By the law of exponents, ei(Î±+Î²) = eiÎ±eiÎ². Use Eulerâ€™s formula to obtain expressions
for cos(Î± + Î²) and sin(Î± + Î²) in terms of sines and cosines of Î± and Î².

Complex Variables
5
14. Use De Moivreâ€™s theorem with r = 1 to express cos(4Î¸) and sin(4Î¸) in terms of cos(Î¸)
and sin(Î¸).
15.
Using the property that PN
n=0 qn = (1 âˆ’qN+1)/(1 âˆ’q) and the geometric series
PN
n=0 eint, obtain the following sums of trigonometric functions:
N
X
n=0
cos(nt) = cos
Nt
2
 sin[(N + 1)t/2]
sin(t/2)
and
N
X
n=1
sin(nt) = sin
Nt
2
 sin[(N + 1)t/2]
sin(t/2)
.
These results are often called Lagrangeâ€™s trigonometric identities.
16. (a) Using the property that Pâˆ
n=0 qn = 1/(1 âˆ’q), if |q| < 1, and the geometric series
Pâˆ
n=0 Ç«neint, |Ç«| < 1, show that
âˆ
X
n=0
Ç«n cos(nt) =
1 âˆ’Ç« cos(t)
1 + Ç«2 âˆ’2Ç« cos(t)
and
âˆ
X
n=1
Ç«n sin(nt) =
Ç« sin(t)
1 + Ç«2 âˆ’2Ç« cos(t).
(b) Let Ç« = eâˆ’a, where a > 0. Show that
2
âˆ
X
n=1
eâˆ’na sin(nt) =
sin(t)
cosh(a) âˆ’cos(t).
1.2 FINDING ROOTS
The concept of ï¬nding roots of a number, which is rather straightforward in the case of
real numbers, becomes more diï¬ƒcult in the case of complex numbers. By ï¬nding the roots
of a complex number, we wish to ï¬nd all of the solutions w of the equation wn = z, where
n is a positive integer for a given z.
We begin by writing z in the polar form:
z = reiÏ•,
(1.2.1)
while we write
w = ReiÎ¦
(1.2.2)
for the unknown. Consequently,
wn = RneinÎ¦ = reiÏ• = z.
(1.2.3)
We satisfy Equation 1.2.3 if
Rn = r
and
nÎ¦ = Ï• + 2kÏ€,
k = 0, Â±1, Â±2, . . . ,
(1.2.4)
because the addition of any multiple of 2Ï€ to the argument is also a solution.
Thus,
R = r1/n, where R is the uniquely determined real positive root, and
Î¦k = Ï•
n + 2Ï€k
n ,
k = 0, Â±1, Â±2, . . . .
(1.2.5)

6
Advanced Engineering Mathematics: A Second Course
Because wk = wkÂ±n, it is suï¬ƒcient to take k = 0, 1, 2, . . . , nâˆ’1. Therefore, there are exactly
n solutions:
wk = ReÎ¦ki = r1/n exp

i
Ï•
n + 2Ï€k
n

(1.2.6)
with k = 0, 1, 2, . . . , n âˆ’1. They are the n roots of z. Geometrically we can locate these
solutions wk on a circle, centered at the point (0, 0), with radius R and separated from each
other by 2Ï€/n radians. These roots also form the vertices of a regular polygon of n sides
inscribed inside a circle of radius R. (See Example 1.2.1.)
In summary, the method for ï¬nding the n roots of a complex number z0 is as follows.
First, write z0 in its polar form: z0 = reiÏ•. Then multiply the polar form by e2iÏ€k. Using
the law of exponents, take the 1/n power of both sides of the equation.
Finally, using
Eulerâ€™s formula, evaluate the roots for k = 0, 1, . . . , n âˆ’1.
â€¢ Example 1.2.1
Let us ï¬nd all of the values of z for which z5 = âˆ’32 and locate these values on the
complex plane.
Because
âˆ’32 = 32eÏ€i = 25eÏ€i,
(1.2.7)
zk = 2 exp
Ï€i
5 + 2Ï€ik
5

,
k = 0, 1, 2, 3, 4,
(1.2.8)
or
z0 = 2 exp
Ï€i
5

= 2
h
cos
Ï€
5

+ i sin
Ï€
5
i
,
(1.2.9)
z1 = 2 exp
3Ï€i
5

= 2

cos
3Ï€
5

+ i sin
3Ï€
5

,
(1.2.10)
z2 = 2eÏ€i = âˆ’2,
(1.2.11)
z3 = 2 exp
7Ï€i
5

= 2

cos
7Ï€
5

+ i sin
7Ï€
5

(1.2.12)
and
z4 = 2 exp
9Ï€i
5

= 2

cos
9Ï€
5

+ i sin
9Ï€
5

.
(1.2.13)
Figure 1.2.1 shows the location of these roots in the complex plane.
âŠ“âŠ”
â€¢ Example 1.2.2
Let us ï¬nd the cube roots of âˆ’1 + i and locate them graphically.
Because âˆ’1 + i =
âˆš
2 exp(3Ï€i/4),
zk = 21/6 exp
Ï€i
4 + 2iÏ€k
3

,
k = 0, 1, 2,
(1.2.14)
or
z0 = 21/6 exp
Ï€i
4

= 21/6 h
cos
Ï€
4

+ i sin
Ï€
4
i
,
(1.2.15)

Complex Variables
7
x
y
1
z
z
z
z
z
0
4
3
2
Figure 1.2.1: The zeros of z5 = âˆ’32.
z1 = 21/6 exp
11Ï€i
12

= 21/6

cos
11Ï€
12

+ i sin
11Ï€
12

,
(1.2.16)
and
z2 = 21/6 exp
19Ï€i
12

= 21/6

cos
19Ï€
12

+ i sin
19Ï€
12

.
(1.2.17)
Figure 1.2.2 gives the location of these zeros on the complex plane.
âŠ“âŠ”
â€¢ Example 1.2.3
The routine solve in MATLAB can also be used to compute the roots of complex
numbers. For example, let us ï¬nd all of the roots of z4 = âˆ’a4.
The MATLAB commands are as follows:
>> syms a z
>> solve(z^4+a^4)
This yields the solution
ans=
[
(1/2*2^(1/2)+1/2*i*2^(1/2))*a]
[ (-1/2*2^(1/2)+1/2*i*2^(1/2))*a]
[
(1/2*2^(1/2)-1/2*i*2^(1/2))*a]
[ (-1/2*2^(1/2)-1/2*i*2^(1/2))*a]
Problems
Extract all of the possible roots of the following complex numbers. Verify your answer using
MATLAB.
1.
81/6
2.
(âˆ’1)1/3
3.
(âˆ’i)1/3
4.
(âˆ’27i)1/6
5. Find algebraic expressions for the square roots of a âˆ’bi, where a > 0 and b > 0.
6. Find all of the roots for the algebraic equation z4 âˆ’3iz2 âˆ’2 = 0. Then check your answer
using solve in MATLAB.

8
Advanced Engineering Mathematics: A Second Course
y
0
x
z
z
z
1
2
Figure 1.2.2: The zeros of z3 = âˆ’1 + i.
7. Find all of the roots for the algebraic equation z4 + 6iz2 + 16 = 0. Then check your
answer using solve in MATLAB.
1.3 THE DERIVATIVE IN THE COMPLEX PLANE: THE CAUCHY-RIEMANN EQUATIONS
In the previous two sections, we introduced complex arithmetic. We are now ready for
the concept of function as it applies to complex variables.
We already deï¬ned the complex variable z = x+iy, where x and y are variable. We now
introduce another complex variable w = u+iv so that for each value of z there corresponds
a value of w = f(z). From all of the possible complex functions that we might invent, we
focus on those functions where for each z there is one, and only one, value of w. These
functions are single-valued. They diï¬€er from functions such as the square root, logarithm,
and inverse sine and cosine, where there are multiple answers for each z. These multivalued
functions do arise in various problems. However, they are beyond the scope of this book
and we shall always assume that we are dealing with single-valued functions.
A popular method for representing a complex function involves drawing some closed
domain in the z-plane and then showing the corresponding domain in the w-plane. This
procedure is called mapping and the z-plane illustrates the domain of the function while
the w-plane illustrates its image or range. Figure 1.3.1 shows the z-plane and w-plane for
w = z2; a pie-shaped wedge in the z-plane maps into a semicircle on the w-plane.
â€¢ Example 1.3.1
Given the complex function w = eâˆ’z2, let us ï¬nd the corresponding u(x, y) and v(x, y).
From Eulerâ€™s formula,
w = eâˆ’z2 = eâˆ’(x+iy)2 = ey2âˆ’x2eâˆ’2ixy = ey2âˆ’x2[cos(2xy) âˆ’i sin(2xy)].
(1.3.1)
Therefore, by inspection,
u(x, y) = ey2âˆ’x2 cos(2xy),
and
v(x, y) = âˆ’ey2âˆ’x2 sin(2xy).
(1.3.2)

Complex Variables
9
2
x
y
v
u
z-plane 
w-plane
2
1
3
3
1
Figure 1.3.1: The complex function w = z2.
Note that there is no i in the expression for v(x, y). The function w = f(z) is single-valued
because for each distinct value of z, there is a unique value of u(x, y) and v(x, y).
âŠ“âŠ”
â€¢ Example 1.3.2
As counterpoint, let us show that w = âˆšz is a multivalued function.
We begin by writing z = reiÎ¸+2Ï€ik, where r =
p
x2 + y2 and Î¸ = tanâˆ’1(y/x). Then,
wk = âˆšreiÎ¸/2+Ï€ik,
k = 0, 1,
(1.3.3)
or
w0 = âˆšr [cos(Î¸/2) + i sin(Î¸/2)]
and
w1 = âˆ’w0.
(1.3.4)
Therefore,
u0(x, y) = âˆšr cos(Î¸/2),
v0(x, y) = âˆšr sin(Î¸/2),
(1.3.5)
and
u1(x, y) = âˆ’âˆšr cos(Î¸/2),
v1(x, y) = âˆ’âˆšr sin(Î¸/2).
(1.3.6)
Each solution w0 or w1 is a branch of the multivalued function âˆšz. We can make âˆšz single-
valued by restricting ourselves to a single branch, say w0. In that case, the â„œ(w) > 0 if we
restrict âˆ’Ï€ < Î¸ < Ï€. Although this is not the only choice that we could have made, it is a
popular one. For example, most digital computers use this deï¬nition in their complex square
root function. The point here is our ability to make a multivalued function single-valued
by deï¬ning a particular branch.
âŠ“âŠ”
Although the requirement that a complex function be single-valued is important, it is
still too general and would cover all functions of two real variables. To have a useful theory,
we must introduce additional constraints. Because an important property associated with
most functions is the ability to take their derivative, let us examine the derivative in the
complex plane.
Following the deï¬nition of a derivative for a single real variable, the derivative of a
complex function w = f(z) is deï¬ned as
dw
dz = lim
âˆ†zâ†’0
âˆ†w
âˆ†z = lim
âˆ†zâ†’0
f(z + âˆ†z) âˆ’f(z)
âˆ†z
.
(1.3.7)
A function of a complex variable that has a derivative at every point within a region of the
complex plane is said to be analytic (or regular or holomorphic) over that region. If the
function is analytic everywhere in the complex plane, it is entire.

10
Advanced Engineering Mathematics: A Second Course
Because the derivative is deï¬ned as a limit and limits are well behaved with respect
to elementary algebraic operations, the following operations carry over from elementary
calculus:
d
dz

cf(z)

= cf â€²(z),
c a constant
(1.3.8)
d
dz

f(z) Â± g(z)

= f â€²(z) Â± gâ€²(z)
(1.3.9)
d
dz

f(z)g(z)

= f â€²(z)g(z) + f(z)gâ€²(z)
(1.3.10)
d
dz
f(z)
g(z)

= g(z)f â€²(z) âˆ’gâ€²(z)f(z)
g2(z)
(1.3.11)
d
dz

f[g(z)]

= f â€²[g(z)]gâ€²(z),
the chain rule.
(1.3.12)
Another important property that carries over from real variables is lâ€™HË†opitalâ€™s rule: Let
f(z) and g(z) be analytic at z0, where f(z) has a zero1 of order m and g(z) has a zero of
order n. Then, if m > n,
lim
zâ†’z0
f(z)
g(z) = 0;
(1.3.13)
if m = n,
lim
zâ†’z0
f(z)
g(z) = f (m)(z0)
g(m)(z0) ;
(1.3.14)
and if m < n,
lim
zâ†’z0
f(z)
g(z) = âˆ.
(1.3.15)
â€¢ Example 1.3.3
Let us evaluate limzâ†’i(z10 + 1)/(z6 + 1). From lâ€™HË†opitalâ€™s rule,
lim
zâ†’i
z10 + 1
z6 + 1 = lim
zâ†’i
10z9
6z5 = 5
3 lim
zâ†’i z4 = 5
3.
(1.3.16)
âŠ“âŠ”
So far, we introduced the derivative and some of its properties. But how do we actually
know whether a function is analytic or how do we compute its derivative? At this point we
must develop some relationships involving the known quantities u(x, y) and v(x, y).
We begin by returning to the deï¬nition of the derivative. Because âˆ†z = âˆ†x + iâˆ†y,
there is an inï¬nite number of diï¬€erent ways of approaching the limit âˆ†z â†’0. Uniqueness
of that limit requires that Equation 1.3.7 must be independent of the manner in which âˆ†z
approaches zero. A simple example is to take âˆ†z in the x-direction so that âˆ†z = âˆ†x;
another is to take âˆ†z in the y-direction so that âˆ†z = iâˆ†y. These examples yield
dw
dz = lim
âˆ†zâ†’0
âˆ†w
âˆ†z = lim
âˆ†xâ†’0
âˆ†u + iâˆ†v
âˆ†x
= âˆ‚u
âˆ‚x + iâˆ‚v
âˆ‚x
(1.3.17)
1 An analytic function f(z) has a zero of order m at z0 if and only if f(z0) = fâ€²(z0) = Â· Â· Â· = f(mâˆ’1)(z0) =
0 and f(m)(z0) Ì¸= 0.

Complex Variables
11
Although educated as an engineer, Augustin-Louis Cauchy (1789â€“1857) would become a mathe-
maticianâ€™s mathematician, publishing 789 papers and 7 books in the ï¬elds of pure and applied
mathematics. His greatest writings established the discipline of mathematical analysis as he reï¬ned
the notions of limit, continuity, function, and convergence. It was this work on analysis that led him
to develop complex function theory via the concept of residues. (Portrait courtesy of the Archives
de lâ€™AcadÂ´emie des sciences, Paris.)
and
dw
dz = lim
âˆ†zâ†’0
âˆ†w
âˆ†z = lim
âˆ†yâ†’0
âˆ†u + iâˆ†v
iâˆ†y
= âˆ‚v
âˆ‚y âˆ’iâˆ‚u
âˆ‚y .
(1.3.18)
In both cases we are approaching zero from the positive side. For the limit to be unique
and independent of path, Equation 1.3.17 must equal Equation 1.3.18, or
âˆ‚u
âˆ‚x = âˆ‚v
âˆ‚y
and
âˆ‚u
âˆ‚y = âˆ’âˆ‚v
âˆ‚x.
(1.3.19)
These equations that u and v must both satisfy are the Cauchy-Riemann equations.
They are necessary but not suï¬ƒcient to ensure that a function is diï¬€erentiable. The follow-
ing example illustrates this.
â€¢ Example 1.3.4
Consider the complex function
w =

z5/|z|4,
z Ì¸= 0
0,
z = 0.
(1.3.20)
The derivative at z = 0 is given by
dw
dz = lim
âˆ†zâ†’0
(âˆ†z)5/|âˆ†z|4 âˆ’0
âˆ†z
= lim
âˆ†zâ†’0
(âˆ†z)4
|âˆ†z|4 ,
(1.3.21)

12
Advanced Engineering Mathematics: A Second Course
Despite his short life, (Georg Friedrich) Bernhard Riemannâ€™s (1826â€“1866) mathematical work con-
tained many imaginative and profound concepts. It was in his doctoral thesis on complex function
theory (1851) that he introduced the Cauchy-Riemann diï¬€erential equations. Riemannâ€™s later work
dealt with the deï¬nition of the integral and the foundations of geometry and non-Euclidean (elliptic)
geometry. (Portrait courtesy of Photo AKG, London, with permission.)
provided that this limit exists. However, this limit does not exist because, in general, the
numerator depends upon the path used to approach zero. For example, if âˆ†z = reÏ€i/4 with
r â†’0, dw/dz = âˆ’1. On the other hand, if âˆ†z = reÏ€i/2 with r â†’0, dw/dz = 1.
Are the Cauchy-Riemann equations satisï¬ed in this case? To check this, we ï¬rst com-
pute
ux(0, 0) = lim
âˆ†xâ†’0
 âˆ†x
|âˆ†x|
4
= 1,
vy(0, 0) = lim
âˆ†yâ†’0
 iâˆ†y
|âˆ†y|
4
= 1,
(1.3.22)
uy(0, 0) = lim
âˆ†yâ†’0 â„œ
 (iâˆ†y)5
âˆ†y|âˆ†y|4

= 0,
and
vx(0, 0) = lim
âˆ†xâ†’0 â„‘
" âˆ†x
|âˆ†x|
4#
= 0.
(1.3.23)
Hence, the Cauchy-Riemann equations are satisï¬ed at the origin. Thus, even though the
derivative is not uniquely deï¬ned, Equation 1.3.21 happens to have the same value for paths
taken along the coordinate axes so that the Cauchy-Riemann equations are satisï¬ed.
âŠ“âŠ”
In summary, if a function is diï¬€erentiable at a point, the Cauchy-Riemann equations
hold. Similarly, if the Cauchy-Riemann equations are not satisï¬ed at a point, then the
function is not diï¬€erentiable at that point. This is one of the important uses of the Cauchy-
Riemann equations: the location of nonanalytic points. Isolated nonanalytic points of an
otherwise analytic function are called isolated singularities.
Functions that contain isolated
singularities are called meromorphic.
The Cauchy-Riemann condition can be modiï¬ed so that it is suï¬ƒcient for the derivative
to exist. Let us require that ux, uy, vx, and vy be continuous in some region surrounding a

Complex Variables
13
point z0 and satisfy the Cauchy-Riemann equations there. Then
f(z) âˆ’f(z0) = [u(z) âˆ’u(z0)] + i[v(z) âˆ’v(z0)]
(1.3.24)
= [ux(z0)(x âˆ’x0) + uy(z0)(y âˆ’y0) + Ç«1(x âˆ’x0) + Ç«2(y âˆ’y0)]
+ i[vx(z0)(x âˆ’x0) + vy(z0)(y âˆ’y0) + Ç«3(x âˆ’x0) + Ç«4(y âˆ’y0)]
(1.3.25)
= [ux(z0) + ivx(z0)](z âˆ’z0) + (Ç«1 + iÇ«3)(x âˆ’x0)
+ (Ç«2 + iÇ«4)(y âˆ’y0),
(1.3.26)
where we used the Cauchy-Riemann equations and Ç«1, Ç«2, Ç«3, Ç«4 â†’0 as âˆ†x, âˆ†y â†’0. Hence,
f â€²(z0) = lim
âˆ†zâ†’0
f(z) âˆ’f(z0)
âˆ†z
= ux(z0) + ivx(z0),
(1.3.27)
because |âˆ†x| â‰¤|âˆ†z| and |âˆ†y| â‰¤|âˆ†z|. Using Equation 1.3.27 and the Cauchy-Riemann
equations, we can obtain the derivative from any of the following formulas:
dw
dz = âˆ‚u
âˆ‚x + iâˆ‚v
âˆ‚x = âˆ‚v
âˆ‚y âˆ’iâˆ‚u
âˆ‚y ,
(1.3.28)
and
dw
dz = âˆ‚v
âˆ‚y + iâˆ‚v
âˆ‚x = âˆ‚u
âˆ‚x âˆ’iâˆ‚u
âˆ‚y .
(1.3.29)
Furthermore, f â€²(z0) is continuous because the partial derivatives are.
â€¢ Example 1.3.5
Let us show that sin(z) is an entire function.
w = sin(z)
(1.3.30)
u + iv = sin(x + iy) = sin(x) cos(iy) + cos(x) sin(iy)
(1.3.31)
= sin(x) cosh(y) + i cos(x) sinh(y),
(1.3.32)
because
cos(iy) = 1
2

ei(iy) + eâˆ’i(iy)
= 1
2

ey + eâˆ’y
= cosh(y),
(1.3.33)
and
sin(iy) = 1
2i

ei(iy) âˆ’eâˆ’i(iy)
= âˆ’1
2i

ey âˆ’eâˆ’y
= i sinh(y),
(1.3.34)
so that
u(x, y) = sin(x) cosh(y),
and
v(x, y) = cos(x) sinh(y).
(1.3.35)
Diï¬€erentiating both u(x, y) and v(x, y) with respect to x and y, we have that
âˆ‚u
âˆ‚x = cos(x) cosh(y),
âˆ‚u
âˆ‚y = sin(x) sinh(y),
(1.3.36)

14
Advanced Engineering Mathematics: A Second Course
âˆ‚v
âˆ‚x = âˆ’sin(x) sinh(y),
âˆ‚v
âˆ‚y = cos(x) cosh(y),
(1.3.37)
and u(x, y) and v(x, y) satisfy the Cauchy-Riemann equations for all values of x and y.
Furthermore, ux, uy, vx, and vy are continuous for all x and y. Therefore, the function
w = sin(z) is an entire function.
âŠ“âŠ”
â€¢ Example 1.3.6
Consider the function w = 1/z. Then
w = u + iv =
1
x + iy =
x
x2 + y2 âˆ’
iy
x2 + y2 .
(1.3.38)
Therefore,
u(x, y) =
x
x2 + y2 ,
and
v(x, y) = âˆ’
y
x2 + y2 .
(1.3.39)
Now
âˆ‚u
âˆ‚x = (x2 + y2) âˆ’2x2
(x2 + y2)2
=
y2 âˆ’x2
(x2 + y2)2 ,
(1.3.40)
âˆ‚v
âˆ‚y = âˆ’(x2 + y2) âˆ’2y2
(x2 + y2)2
=
y2 âˆ’x2
(x2 + y2)2 = âˆ‚u
âˆ‚x,
(1.3.41)
âˆ‚v
âˆ‚x = âˆ’0 âˆ’2xy
(x2 + y2)2 =
2xy
(x2 + y2)2 ,
(1.3.42)
and
âˆ‚u
âˆ‚y =
0 âˆ’2xy
(x2 + y2)2 = âˆ’
2xy
(x2 + y2)2 = âˆ’âˆ‚v
âˆ‚x.
(1.3.43)
The function is analytic at all points except the origin because the function itself ceases to
exist when both x and y are zero and the modulus of w becomes inï¬nite.
âŠ“âŠ”
â€¢ Example 1.3.7
Let us ï¬nd the derivative of sin(z).
Using Equation 1.3.28 and Equation 1.3.32,
d
dz

sin(z)

= âˆ‚u
âˆ‚x +iâˆ‚v
âˆ‚x = cos(x) cosh(y)âˆ’i sin(x) sinh(y) = cos(x+iy) = cos(z). (1.3.44)
Similarly,
d
dz
1
z

=
y2 âˆ’x2
(x2 + y2)2 +
2ixy
(x2 + y2)2 = âˆ’
1
(x + iy)2 = âˆ’1
z2 .
(1.3.45)
âŠ“âŠ”
The results in the above examples are identical to those for z real.
As we showed
earlier, the fundamental rules of elementary calculus apply to complex diï¬€erentiation. Con-
sequently, it is usually simpler to apply those rules to ï¬nd the derivative rather than breaking
f(z) down into its real and imaginary parts, applying either Equation 1.3.28 or Equation
1.3.29, and then putting everything back together.

Complex Variables
15
An additional property of analytic functions follows by cross diï¬€erentiating the Cauchy-
Riemann equations, or
âˆ‚2u
âˆ‚x2 = âˆ‚2v
âˆ‚xâˆ‚y = âˆ’âˆ‚2u
âˆ‚y2 ,
or
âˆ‚2u
âˆ‚x2 + âˆ‚2u
âˆ‚y2 = 0,
(1.3.46)
and
âˆ‚2v
âˆ‚x2 = âˆ’âˆ‚2u
âˆ‚xâˆ‚y = âˆ’âˆ‚2v
âˆ‚y2 ,
or
âˆ‚2v
âˆ‚x2 + âˆ‚2v
âˆ‚y2 = 0.
(1.3.47)
Any function that has continuous partial derivatives of second order and satisï¬es Laplaceâ€™s
equation, Equation 1.3.46 or Equation 1.3.47, is called a harmonic function. Because both
u(x, y) and v(x, y) satisfy Laplaceâ€™s equation if f(z) = u + iv is analytic, u(x, y) and v(x, y)
are called conjugate harmonic functions.
â€¢ Example 1.3.8
Given that u(x, y) = eâˆ’x[x sin(y) âˆ’y cos(y)], let us show that u is harmonic and ï¬nd
a conjugate harmonic function v(x, y) such that f(z) = u + iv is analytic.
Because
âˆ‚2u
âˆ‚x2 = âˆ’2eâˆ’x sin(y) + xeâˆ’x sin(y) âˆ’yeâˆ’x cos(y),
(1.3.48)
and
âˆ‚2u
âˆ‚y2 = âˆ’xeâˆ’x sin(y) + 2eâˆ’x sin(y) + yeâˆ’x cos(y),
(1.3.49)
it follows that uxx + uyy = 0. Therefore, u(x, y) is harmonic. From the Cauchy-Riemann
equations,
âˆ‚v
âˆ‚y = âˆ‚u
âˆ‚x = eâˆ’x sin(y) âˆ’xeâˆ’x sin(y) + yeâˆ’x cos(y),
(1.3.50)
and
âˆ‚v
âˆ‚x = âˆ’âˆ‚u
âˆ‚y = eâˆ’x cos(y) âˆ’xeâˆ’x cos(y) âˆ’yeâˆ’x sin(y).
(1.3.51)
Integrating Equation 1.3.50 with respect to y,
v(x, y) = yeâˆ’x sin(y) + xeâˆ’x cos(y) + g(x).
(1.3.52)
Using Equation 1.3.51,
vx = âˆ’yeâˆ’x sin(y) âˆ’xeâˆ’x cos(y) + eâˆ’x cos(y) + gâ€²(x)
= eâˆ’x cos(y) âˆ’xeâˆ’x cos(y) âˆ’yeâˆ’x sin(x).
(1.3.53
Therefore, gâ€²(x) = 0 or g(x) = constant. Consequently,
v(x, y) = eâˆ’x[y sin(y) + x cos(y)] + constant.
(1.3.54)
Hence, for our real harmonic function u(x, y), there are inï¬nitely many harmonic conjugates
v(x, y), which diï¬€er from each other by an additive constant.

16
Advanced Engineering Mathematics: A Second Course
Problems
Show that the following functions are entire:
1. f(z) = iz + 2
2. f(z) = eâˆ’z
3. f(z) = z3
4. f(z) = cosh(z)
Find the derivative of the following functions:
5. f(z) = (1 + z2)3/2
6. f(z) = (z + 2z1/2)1/3
7. f(z) = (1 + 4i)z2 âˆ’3z âˆ’2
8. f(z) = (2z âˆ’i)/(z + 2i)
9.
f(z) = (iz âˆ’1)âˆ’3
10. f(z) = z/(z3 + 1)
Evaluate the following limits:
11.
lim
zâ†’i
z2 âˆ’2iz âˆ’1
z4 + 2z2 + 1
12.
lim
zâ†’0
z âˆ’sin(z)
z3
13.
lim
zâ†’n
z âˆ’n
sin(Ï€z)
Here, n is an integer.
14. Show that the function f(z) = zâˆ—is nowhere diï¬€erentiable.
For each of the following u(x, y), show that it is harmonic and then ï¬nd a corresponding
v(x, y) such that f(z) = u + iv is analytic.
15. u(x, y) = x2 âˆ’y2
16. u(x, y) = x4 âˆ’6x2y2 + y4 + x
17. u(x, y) = x cos(x)eâˆ’y âˆ’y sin(x)eâˆ’y
18. u(x, y) = (x2 âˆ’y2) cos(y)ex âˆ’2xy sin(y)ex
1.4 LINE INTEGRALS
So far, we discussed complex numbers, complex functions, and complex diï¬€erentiation.
We are now ready for integration.
Just as we have integrals involving real variables, we can deï¬ne an integral that involves
complex variables. Because the z-plane is two-dimensional, there is clearly greater freedom
in what we mean by a complex integral. For example, we might ask whether the integral of
some function between points A and B depends upon the curve along which we integrate.
(In general it does.) Consequently, an important ingredient in any complex integration is
the contour that we follow during the integration.
The result of a line integral is a complex number or expression. Unlike its counterpart
in real variables, there is no physical interpretation for this quantity, such as area under
a curve.
Generally, integration in the complex plane is an intermediate process with a
physically realizable quantity occurring only after we take its real or imaginary part. For
example, in potential ï¬‚uid ï¬‚ow, the lift and drag are found by taking the real and imaginary
parts of a complex integral, respectively.
How do we compute
R
C f(z) dz? Let us deal with the deï¬nition; we illustrate the actual
method by examples.
A popular method for evaluating complex line integrals consists of breaking everything
up into real and imaginary parts. This reduces the integral to line integrals of real-valued
functions, which we know how to handle. Thus, we write f(z) = u(x, y) + iv(x, y) as usual,
and because z = x + iy, formally dz = dx + i dy. Therefore,
Z
C
f(z) dz =
Z
C
[u(x, y) + iv(x, y)][dx + i dy]
(1.4.1)
=
Z
C
u(x, y) dx âˆ’v(x, y) dy + i
Z
C
v(x, y) dx + u(x, y) dy.
(1.4.2)

Complex Variables
17
y
C
x
2
4
2
2a
C b
C
2
1
Figure 1.4.1: Contour used in Example 1.4.1.
The exact method used to evaluate Equation 1.4.2 depends upon the exact path speciï¬ed.
From the deï¬nition of the line integral, we have the following self-evident properties:
Z
C
f(z) dz = âˆ’
Z
Câ€² f(z) dz,
(1.4.3)
where Câ€² is the contour C taken in the opposite direction of C and
Z
C1+C2
f(z) dz =
Z
C1
f(z) dz +
Z
C2
f(z) dz.
(1.4.4)
â€¢ Example 1.4.1
Let us evaluate
R
C zâˆ—dz from z = 0 to z = 4+2i along two diï¬€erent contours. The ï¬rst
consists of the parametric equation z = t2 + it. The second consists of two â€œdog legsâ€: the
ï¬rst leg runs along the imaginary axis from z = 0 to z = 2i and then along a line parallel
to the x-axis from z = 2i to z = 4 + 2i. See Figure 1.4.1.
For the ï¬rst case, the points z = 0 and z = 4 + 2i on C1 correspond to t = 0 and t = 2,
respectively. Then the line integral equals
Z
C1
zâˆ—dz =
Z 2
0
(t2 + it)âˆ—d(t2 + it) =
Z 2
0
(2t3 âˆ’it2 + t) dt = 10 âˆ’8i
3 .
(1.4.5)
The line integral for the second contour C2 equals
Z
C2
zâˆ—dz =
Z
C2a
zâˆ—dz +
Z
C2b
zâˆ—dz,
(1.4.6)
where C2a denotes the integration from z = 0 to z = 2i while C2b denotes the integration
from z = 2i to z = 4 + 2i. For the ï¬rst integral,
Z
C2a
zâˆ—dz =
Z
C2a
(x âˆ’iy)(dx + i dy) =
Z 2
0
y dy = 2,
(1.4.7)
because x = 0 and dx = 0 along C2a. On the other hand, along C2b, y = 2 and dy = 0 so
that
Z
C2b
zâˆ—dz =
Z
C2b
(x âˆ’iy)(dx + i dy) =
Z 4
0
x dx + i
Z 4
0
âˆ’2 dx = 8 âˆ’8i.
(1.4.8)

18
Advanced Engineering Mathematics: A Second Course
C
1
C
C
y
1
x
2 b
2 a
1
2
Figure 1.4.2: Contour used in Example 1.4.2.
Thus the value of the entire C2 contour integral equals the sum of the two parts, or 10âˆ’8i.
The point here is that integration along two diï¬€erent paths has given us diï¬€erent results
even though we integrated from z = 0 to z = 4 + 2i both times. This result foreshadows
a general result that is extremely important. Because the integrand contains nonanalytic
points along and inside the region enclosed by our two curves, as shown by the Cauchy-
Riemann equations, the results depend upon the path taken. Since complex integrations
often involve integrands that have nonanalytic points, many line integrations depend upon
the contour taken.
âŠ“âŠ”
â€¢ Example 1.4.2
Let us integrate the entire function f(z) = z2 along the two paths from z = 0 to
z = 2 + i shown in Figure 1.4.2. For the ï¬rst integration, x = 2y, while along the second
path we have two straight paths: z = 0 to z = 2 and z = 2 to z = 2 + i.
For the ï¬rst contour integration,
Z
C1
z2dz =
Z 1
0
(2y + iy)2(2 dy + i dy) =
Z 1
0
(3y2 + 4y2i)(2 dy + i dy)
(1.4.9)
=
Z 1
0
6y2 dy + 8y2i dy + 3y2i dy âˆ’4y2 dy =
Z 1
0
2y2 dy + 11y2i dy
(1.4.10)
= 2
3y3|1
0 + 11
3 iy3|1
0 = 2
3 + 11i
3 .
(1.4.11)
For our second integration,
Z
C2
z2 dz =
Z
C2a
z2 dz +
Z
C2b
z2 dz.
(1.4.12)
Along C2a we ï¬nd that y = dy = 0 so that
Z
C2a
z2 dz =
Z 2
0
x2 dx = 1
3x3|2
0 = 8
3,
(1.4.13)
and
Z
C2b
z2 dz =
Z 1
0
(2 + iy)2i dy = i

4y + 2iy2 âˆ’y3
3

1
0
= 4i âˆ’2 âˆ’i
3,
(1.4.14)
because x = 2 and dx = 0. Consequently,
Z
C2
z2 dz = 2
3 + 11i
3 .
(1.4.15)

Complex Variables
19
y
x
(0,âˆ’1)
C
C
C 
2
1
3
(âˆ’1,0)
(0,1)
(1,0)
Figure 1.4.3: Contour used in Example 1.4.3.
In this problem we obtained the same results from two diï¬€erent contours of integration.
Exploring other contours, we would ï¬nd that the results are always the same; the integration
is path independent. But what makes these results path independent while the integration
in Example 1.4.1 was not? Perhaps it is the fact that the integrand is analytic everywhere
on the complex plane and there are no nonanalytic points. We will explore this later.
âŠ“âŠ”
Finally, an important class of line integrals involves closed contours. We denote this
special subclass of line integrals by placing a circle on the integral sign:
H
. Consider now
the following examples:
â€¢ Example 1.4.3
Let us integrate f(z) = z around the closed contour shown in Figure 1.4.3.
From Figure 1.4.3,
I
C
z dz =
Z
C1
z dz +
Z
C2
z dz +
Z
C3
z dz.
(1.4.16)
Now
Z
C1
z dz =
Z 0
1
iy (i dy) = âˆ’
Z 0
1
y dy = âˆ’y2
2

0
1
= 1
2,
(1.4.17)
Z
C2
z dz =
Z âˆ’1
0
x dx = x2
2

âˆ’1
0
= 1
2,
(1.4.18)
and
Z
C3
z dz =
Z Ï€/2
âˆ’Ï€
eÎ¸iieÎ¸idÎ¸ = e2Î¸i
2

Ï€/2
âˆ’Ï€
= âˆ’1,
(1.4.19)
where we used z = eÎ¸i around the portion of the unit circle. Therefore, the closed line
integral equals zero.
âŠ“âŠ”
â€¢ Example 1.4.4
Let us integrate f(z) = 1/(z âˆ’a) around any circle centered on z = a. The Cauchy-
Riemann equations show that f(z) is a meromorphic function. It is analytic everywhere
except at the isolated singularity z = a.

20
Advanced Engineering Mathematics: A Second Course
If we introduce polar coordinates by letting z âˆ’a = reÎ¸i and dz = ireÎ¸idÎ¸,
I
C
dz
z âˆ’a =
Z 2Ï€
0
ireÎ¸i
reÎ¸i dÎ¸ = i
Z 2Ï€
0
dÎ¸ = 2Ï€i.
(1.4.20)
Note that the integrand becomes undeï¬ned at z = a. Furthermore, the answer is indepen-
dent of the size of the circle. Our example suggests that when we have a closed contour
integration, it is the behavior of the function within the contour rather than the exact shape
of the closed contour that is of importance. We will return to this point in later sections.
Problems
1. Evaluate
H
C(zâˆ—)2 dz around the circle |z| = 1 taken in the counterclockwise direction.
2. Evaluate
H
C |z|2 dz around the square with vertices at (0,0), (1,0), (1,1), and (0,1) taken
in the counterclockwise direction.
3. Evaluate
R
C |z| dz along the right half of the circle |z| = 1 from z = âˆ’i to z = i.
4. Evaluate
R
C ez dz along the line y = x from (âˆ’1, âˆ’1) to (1, 1).
5. Evaluate
R
C(zâˆ—)2 dz along the line y = x2 from (0, 0) to (1, 1).
6. Evaluate
R
C zâˆ’1/2 dz, where C is (a) the upper semicircle |z| = 1 and (b) the lower semi-
circle |z| = 1. If z = reÎ¸i, restrict âˆ’Ï€ < Î¸ < Ï€. Take both contours in the counterclockwise
direction.
1.5 THE CAUCHY-GOURSAT THEOREM
In the previous section we showed how to evaluate line integrations by brute-force
reduction to real-valued integrals. In general, this direct approach is quite diï¬ƒcult and we
would like to apply some of the deeper properties of complex analysis to work smarter. In
the remaining portions of this chapter, we introduce several theorems that will do just that.
If we scan over the examples worked in the previous section, we see considerable diï¬€er-
ences when the function was analytic inside and on the contour and when it was not. We
may formalize this anecdotal evidence into the following theorem:
Cauchy-Goursat theorem:2 Let f(z) be analytic in a domain D and let C be a simple
Jordan curve3 inside D so that f(z) is analytic on and inside of C. Then
H
C f(z) dz = 0.
Proof : Let C denote the contour around which we will integrate w = f(z). We divide the
region within C into a series of inï¬nitesimal rectangles. See Figure 1.5.1. The integration
2 Goursat, E., 1900: Sur la dÂ´eï¬nition gÂ´enÂ´erale des fonctions analytiques, dâ€™apr`es Cauchy. Trans. Am.
Math. Soc., 1, 14â€“16.
3 A Jordan curve is a simply closed curve. It looks like a closed loop that does not cross itself. See
Figure 1.5.2.

Complex Variables
21
C
x
y
Figure 1.5.1: Diagram used in proving the Cauchy-Goursat theorem.
around each rectangle equals the product of the average value of w on each side and its
length,

w + âˆ‚w
âˆ‚x
dx
2

dx +

w + âˆ‚w
âˆ‚x dx + âˆ‚w
âˆ‚(iy)
d(iy)
2

d(iy)
+

w + âˆ‚w
âˆ‚x
dx
2 + âˆ‚w
âˆ‚(iy) d(iy)

(âˆ’dx) +

w + âˆ‚w
âˆ‚(iy)
d(iy)
2

d(âˆ’iy)
=
âˆ‚w
âˆ‚x âˆ’âˆ‚w
iâˆ‚y

(i dx dy).
(1.5.1)
Substituting w = u + iv into Equation 1.5.1,
âˆ‚w
âˆ‚x âˆ’âˆ‚w
i âˆ‚y =
âˆ‚u
âˆ‚x âˆ’âˆ‚v
âˆ‚y

+ i
âˆ‚v
âˆ‚x + âˆ‚u
âˆ‚y

.
(1.5.2)
Because the function is analytic, the right side of Equation 1.5.1 and Equation 1.5.2 equals
zero. Thus, the integration around each of these rectangles also equals zero.
We note next that in integrating around adjoining rectangles, we transverse each side
in opposite directions, the net result being equivalent to integrating around the outer curve
C. We therefore arrive at the result
H
C f(z) dz = 0, where f(z) is analytic within and on
the closed contour.
âŠ“âŠ”
The Cauchy-Goursat theorem has several useful implications. Suppose that we have a
domain where f(z) is analytic. Within this domain, let us evaluate a line integral from point
A to B along two diï¬€erent contours C1 and C2. Then, the integral around the closed contour
formed by integrating along C1 and then back along C2, only in the opposite direction, is
I
C
f(z) dz =
Z
C1
f(z) dz âˆ’
Z
C2
f(z) dz = 0
(1.5.3)
or
Z
C1
f(z) dz =
Z
C2
f(z) dz.
(1.5.4)

22
Advanced Engineering Mathematics: A Second Course
x
y
(a)
(b)
Figure 1.5.2: Examples of a (a) simply closed curve and (b) not simply closed curve.
Because C1 and C2 are completely arbitrary, we have the result that if, in a domain, f(z)
is analytic, the integral between any two points within the domain is path independent.
One obvious advantage of path independence is the ability to choose the contour so that
the computations are made easier. This obvious choice immediately leads to the following
principle:
The principle of deformation of contours: The value of a line integral of an analytic
function around any simple closed contour remains unchanged if we deform the contour in
such a manner that we do not pass over a nonanalytic point.
âŠ“âŠ”
â€¢ Example 1.5.1
Let us integrate f(z) = zâˆ’1 around the closed contour C in the counterclockwise
direction. This contour consists of a square, centered on the origin, with vertices at (1, 1),
(1, âˆ’1), (âˆ’1, 1), and (âˆ’1, âˆ’1).
The direct integration of
H
C zâˆ’1dz around the original contour is very cumbersome.
However, because the integrand is analytic everywhere except at the origin, we may deform
the origin contour into a circle of radius r, centered on the origin. Then, z = reÎ¸i and
dz = rieÎ¸idÎ¸ so that
I
C
dz
z =
Z 2Ï€
0
rieÎ¸i
reÎ¸i dÎ¸ = i
Z 2Ï€
0
dÎ¸ = 2Ï€i.
(1.5.5)
The point here is that no matter how bizarre the contour is, as long as it encircles the origin
and is a simply closed contour, we can deform it into a circle and we get the same answer
for the contour integral. This suggests that it is not the shape of the closed contour that
makes the diï¬€erence but whether we enclose any singularities (points where f(z) becomes
undeï¬ned) that matters. We shall return to this idea many times in the next few sections.âŠ“âŠ”
Finally, suppose that we have a function f(z) such that f(z) is analytic in some domain.
Furthermore, let us introduce the analytic function F(z) such that f(z) = F â€²(z). We would
like to evaluate
R b
a f(z) dz in terms of F(z).
We begin by noting that we can represent F, f as F(z) = U + iV and f(z) = u + iv.
From Example 1.3.28 we have that u = Ux and v = Vx. Therefore,
Z b
a
f(z) dz =
Z b
a
(u + iv)(dx + i dy) =
Z b
a
Ux dx âˆ’Vx dy + i
Z b
a
Vx dx + Ux dy
(1.5.6)

Complex Variables
23
=
Z b
a
Ux dx + Uy dy + i
Z b
a
Vx dx + Vy dy =
Z b
a
dU + i
Z b
a
dV = F(b) âˆ’F(a)
(1.5.7)
or
Z b
a
f(z) dz = F(b) âˆ’F(a).
(1.5.8)
Equation 1.5.8 is the complex variable form of the fundamental theorem of calculus. Thus,
if we can ï¬nd the antiderivative of a function f(z) that is analytic within a speciï¬c region,
we can evaluate the integral by evaluating the antiderivative at the endpoints for any curves
within that region.
â€¢ Example 1.5.2
Let us evaluate
R Ï€i
0 z sin(z2) dz.
The integrand f(z) = z sin(z2) is an entire function and its antiderivative equals
âˆ’1
2 cos(z2). Therefore,
Z Ï€i
0
z sin(z2) dz = âˆ’1
2 cos(z2)
Ï€i
0 = 1
2[cos(0) âˆ’cos(âˆ’Ï€2)] = 1
2[1 âˆ’cos(Ï€2)].
(1.5.9)
Problems
For the following integrals, show that they are path independent and determine the value
of the integral:
1.
Z 2+3Ï€i
1âˆ’Ï€i
eâˆ’2z dz
2.
Z 2Ï€
0
[ez âˆ’cos(z)] dz
3.
Z Ï€
0
sin2(z) dz
4.
Z 2i
âˆ’i
(z + 1) dz
5.
Z 2+2i
1
(z2 âˆ’z + 8) dz 6.
Z 2i
1
[(1 âˆ’i)z2 + 2iz âˆ’4] dz
7.
Z i
0
z2 cos(z3) dz
8.
Z 1+i
i
zeâˆ’z2 dz
1.6 CAUCHYâ€™S INTEGRAL FORMULA
In the previous section, our examples suggested that the presence of a singularity
within a contour really determines the value of a closed contour integral. Continuing with
this idea, let us consider a class of closed contour integrals that explicitly contains a single
singularity within the contour, namely
H
C g(z) dz, where g(z) = f(z)/(z âˆ’z0), and f(z) is
analytic within and on the contour C. We closed the contour in the positive sense where
the enclosed area lies to your left as you move along the contour.
We begin by examining a closed contour integral where the closed contour consists of
the C1, C2, C3, and C4 as shown in Figure 1.6.1. The gap or cut between C2 and C4 is very
small. Because g(z) is analytic within and on the closed integral, we have that
Z
C1
f(z)
z âˆ’z0
dz +
Z
C2
f(z)
z âˆ’z0
dz +
Z
C3
f(z)
z âˆ’z0
dz +
Z
C4
f(z)
z âˆ’z0
dz = 0.
(1.6.1)
It can be shown that the contribution to the integral from the path C2 going into the
singularity cancels the contribution from the path C4 going away from the singularity as
the gap between them vanishes. Because f(z) is analytic at z0, we can approximate its

24
Advanced Engineering Mathematics: A Second Course
2
C1
x
y
C
C
C
3
4
Figure 1.6.1: Diagram used to prove Cauchyâ€™s integral formula.
value on C3 by f(z) = f(z0) + Î´(z), where Î´ is a small quantity. Substituting into Equation
1.6.1,
I
C1
f(z)
z âˆ’z0
dz = âˆ’f(z0)
Z
C3
1
z âˆ’z0
dz âˆ’
Z
C3
Î´(z)
z âˆ’z0
dz.
(1.6.2)
Consequently, as the gap between C2 and C4 vanishes, the contour C1 becomes the closed
contour C so that Equation 1.6.2 may be written
I
C
f(z)
z âˆ’z0
dz = 2Ï€if(z0) + i
Z 2Ï€
0
Î´ dÎ¸,
(1.6.3)
where we set z âˆ’z0 = Ç«eÎ¸i and dz = iÇ«eÎ¸idÎ¸.
Let M denote the value of the integral on the right side of Equation 1.6.3 and âˆ†equal
the greatest value of the modulus of Î´ along the circle. Then
|M| <
Z 2Ï€
0
|Î´| dÎ¸ â‰¤
Z 2Ï€
0
âˆ†dÎ¸ = 2Ï€âˆ†.
(1.6.4)
As the radius of the circle diminishes to zero, âˆ†also diminishes to zero. Therefore, |M|,
which is positive, becomes less than any ï¬nite quantity, however small, and M itself equals
zero. Thus, we have that
f(z0) =
1
2Ï€i
I
C
f(z)
z âˆ’z0
dz.
(1.6.5)
This equation is Cauchyâ€™s integral formula. By taking n derivatives of Equation 1.6.5, we
can extend Cauchyâ€™s integral formula4 to
f (n)(z0) = n!
2Ï€i
I
C
f(z)
(z âˆ’z0)n+1 dz
(1.6.6)
4 See Carrier, G. F., M. Krook, and C. E. Pearson, 1966: Functions of a Complex Variable: Theory
and Technique. McGraw-Hill, pp. 39â€“40 for the proof.

Complex Variables
25
for n = 1, 2, 3, . . .. For computing integrals, it is convenient to rewrite Equation 1.6.6 as
I
C
f(z)
(z âˆ’z0)n+1 dz = 2Ï€i
n! f (n)(z0).
(1.6.7)
â€¢ Example 1.6.1
Let us ï¬nd the value of the integral
I
C
cos(Ï€z)
(z âˆ’1)(z âˆ’2) dz,
(1.6.8)
where C is the circle |z| = 5. Using partial fractions,
1
(z âˆ’1)(z âˆ’2) =
1
z âˆ’2 âˆ’
1
z âˆ’1,
(1.6.9)
and
I
C
cos(Ï€z)
(z âˆ’1)(z âˆ’2) dz =
I
C
cos(Ï€z)
z âˆ’2 dz âˆ’
I
C
cos(Ï€z)
z âˆ’1 dz.
(1.6.10)
By Cauchyâ€™s integral formula with z0 = 2 and z0 = 1,
I
C
cos(Ï€z)
z âˆ’2 dz = 2Ï€i cos(2Ï€) = 2Ï€i,
(1.6.11)
and
I
C
cos(Ï€z)
z âˆ’1 dz = 2Ï€i cos(Ï€) = âˆ’2Ï€i,
(1.6.12)
because z0 = 1 and z0 = 2 lie inside C and cos(Ï€z) is analytic there. Thus the required
integral has the value
I
C
cos(Ï€z)
(z âˆ’1)(z âˆ’2) dz = 4Ï€i.
(1.6.13)
âŠ“âŠ”
â€¢ Example 1.6.2
Let us use Cauchyâ€™s integral formula to evaluate
I =
I
|z|=2
ez
(z âˆ’1)2(z âˆ’3) dz.
(1.6.14)
We need to convert Equation 1.6.14 into the form Equation 1.6.7. To do this, we rewrite
Equation 1.6.14 as
I
|z|=2
ez
(z âˆ’1)2(z âˆ’3) dz =
I
|z|=2
ez/(z âˆ’3)
(z âˆ’1)2
dz.
(1.6.15)

26
Advanced Engineering Mathematics: A Second Course
Therefore, f(z) = ez/(z âˆ’3), n = 1, and z0 = 1. The function f(z) is analytic within
the closed contour because the point z = 3 lies outside of the contour. Applying Cauchyâ€™s
integral formula,
I
|z|=2
ez
(z âˆ’1)2(z âˆ’3) dz = 2Ï€i
1!
d
dz
 ez
z âˆ’3

z=1
= 2Ï€i
 ez
z âˆ’3 âˆ’
ez
(z âˆ’3)2

z=1
= âˆ’3Ï€ie
2
.
(1.6.16)
Project: Computing Derivatives of Any Order of a Complex or Real Function
The most common technique for computing a derivative is ï¬nite diï¬€erencing. Recently
Mahajerin and Burgess5 showed how Cauchyâ€™s integral formula can be used to compute the
derivatives of any order of a complex or real function via numerical quadrature. In this
project you will derive the algorithm, write code implementing it, and ï¬nally test it.
Step 1: Consider the complex function f(z) = u + iv, which is analytic inside the closed
circular contour C of radius R centered at z0. Using Cauchyâ€™s integral formula, show that
f (n)(z0) =
n!
2Ï€Rn
Z 2Ï€
0
[u(x, y) + iv(x, y)][cos(nÎ¸) âˆ’i sin(nÎ¸)] dÎ¸,
where x = x0 + R cos(Î¸), and y = y0 + R sin(Î¸).
Step 2: Using ï¬ve-point Gaussian quadrature, write code to implement the results from
Step 1.
Step 3: Test out this scheme by ï¬nding the ï¬rst, sixth, and eleventh derivative of f(x) =
8x/(x2 + 4) for x = 2. The exact answers are 0, 2.8125, and 1218.164, respectively. What
is the maximum value of R? How does the accuracy vary with the number of subdivisions
used in the numerical integration? Is the algorithm sensitive to the value of R and the
number of subdivisions? For a ï¬xed number of subdivisions, is there an optimal R?
Problems
Use Cauchyâ€™s integral formula to evaluate the following integrals. Assume all of the contours
are in the positive sense.
1.
I
|z|=1
sin6(z)
z âˆ’Ï€/6 dz
2.
I
|z|=1
sin6(z)
(z âˆ’Ï€/6)3 dz
3.
I
|z|=1
1
z(z2 + 4) dz
4.
I
|z|=1
tan(z)
z
dz
5.
I
|zâˆ’1|=1/2
1
(z âˆ’1)(z âˆ’2) dz
6.
I
|z|=5
exp(z2)
z3
dz
7.
I
|zâˆ’1|=1
z2 + 1
z2 âˆ’1 dz
8.
I
|z|=2
z2
(z âˆ’1)4 dz
9.
I
|z|=2
z3
(z + i)3 dz
10.
I
|z|=1
cos(z)
z2n+1 dz
11.
I
|z|=1
z2 + 3z âˆ’1
z(z2 âˆ’3) dz
12.
I
|z|=3
iez
(z âˆ’2 + i)4 dz
5 Mahajerin, E., and G. Burgess, 1993: An algorithm for computing derivatives of any order of a complex
or real function. Computers & Struct., 49, 385â€“387.

Complex Variables
27
1.7 TAYLOR AND LAURENT EXPANSIONS AND SINGULARITIES
In the previous section we showed what a crucial role singularities play in complex
integration. Before we can ï¬nd the most general way of computing a closed complex integral,
our understanding of singularities must deepen. For this, we employ power series.
One reason why power series are so important is their ability to provide locally a general
representation of a function even when its arguments are complex. For example, when we
were introduced to trigonometric functions in high school, it was in the context of a right
triangle and a real angle. However, when the argument becomes complex, this geometrical
description disappears and power series provide a formalism for deï¬ning the trigonometric
functions, regardless of the nature of the argument.
Let us begin our analysis by considering the complex function f(z), which is analytic
everywhere on the boundary, and the interior of a circle whose center is at z = z0. Then, if
z denotes any point within the circle, we have from Cauchyâ€™s integral formula that
f(z) =
1
2Ï€i
I
C
f(Î¶)
Î¶ âˆ’z dÎ¶ =
1
2Ï€i
I
C
f(Î¶)
Î¶ âˆ’z0

1
1 âˆ’(z âˆ’z0)/(Î¶ âˆ’z0)

dÎ¶,
(1.7.1)
where C denotes the closed contour. Expanding the bracketed term as a geometric series,
we ï¬nd that
f(z) =
1
2Ï€i
I
C
f(Î¶)
Î¶ âˆ’z0
dÎ¶ +(zâˆ’z0)
I
C
f(Î¶)
(Î¶ âˆ’z0)2 dÎ¶ +Â· Â· Â·+(zâˆ’z0)n
I
C
f(Î¶)
(Î¶ âˆ’z0)n+1 dÎ¶ +Â· Â· Â·

.
(1.7.2)
Applying Cauchyâ€™s integral formula to each integral in Equation 1.7.2, we ï¬nally obtain
f(z) = f(z0) + (z âˆ’z0)
1!
f â€²(z0) + Â· Â· Â· + (z âˆ’z0)n
n!
f (n)(z0) + Â· Â· Â·
(1.7.3)
or the familiar formula for a Taylor expansion. Consequently, we can expand any analytic
function into a Taylor series. Interestingly, the radius of convergence6 of this series may be
shown to be the distance between z0 and the nearest nonanalytic point of f(z).
â€¢ Example 1.7.1
Let us ï¬nd the expansion of f(z) = sin(z) about the point z0 = 0.
Because f(z) is an entire function, we can construct a Taylor expansion anywhere on
the complex plane. For z0 = 0,
f(z) = f(0) + 1
1!f â€²(0)z + 1
2!f â€²â€²(0)z2 + 1
3!f â€²â€²â€²(0)z3 + Â· Â· Â· .
(1.7.4)
Because f(0) = 0, f â€²(0) = 1, f â€²â€²(0) = 0, f â€²â€²â€²(0) = âˆ’1 and so forth,
f(z) = z âˆ’z3
3! + z5
5! âˆ’z7
7! + Â· Â· Â· .
(1.7.5)
Because sin(z) is an entire function, the radius of convergence is |z âˆ’0| < âˆ, i.e., all z. âŠ“âŠ”
6 A positive number h such that the series diverges for |z âˆ’z0| > h but converges absolutely for
|z âˆ’z0| < h.

28
Advanced Engineering Mathematics: A Second Course
0
C
C
C
z
y
x
2
1
z
Figure 1.7.1: Contour used in deriving the Laurent expansion.
â€¢ Example 1.7.2
Let us ï¬nd the expansion of f(z) = 1/(1 âˆ’z) about the point z0 = 0.
From the formula for a Taylor expansion,
f(z) = f(0) + 1
1!f â€²(0)z + 1
2!f â€²â€²(0)z2 + 1
3!f â€²â€²â€²(0)z3 + Â· Â· Â· .
(1.7.6)
Because f (n)(0) = n!, we ï¬nd that
f(z) = 1 + z + z2 + z3 + z4 + Â· Â· Â· =
1
1 âˆ’z .
(1.7.7)
Equation 1.7.7 is the familiar result for a geometric series. Because the only nonanalytic
point is at z = 1, the radius of convergence is |z âˆ’0| < 1, the unit circle centered at z = 0.âŠ“âŠ”
Consider now the situation where we draw two concentric circles about some arbitrary
point z0; we denote the outer circle by C while we denote the inner circle by C1.
See
Figure 1.7.1. Let us assume that f(z) is analytic inside the annulus between the two circles.
Outside of this area, the function may or may not be analytic. Within the annulus we pick
a point z and construct a small circle around it, denoting the circle by C2. As the gap or
cut in the annulus becomes inï¬nitesimally small, the line integrals that connect the circle
C2 to C1 and C sum to zero, leaving
I
C
f(Î¶)
Î¶ âˆ’z dÎ¶ =
I
C1
f(Î¶)
Î¶ âˆ’z dÎ¶ +
I
C2
f(Î¶)
Î¶ âˆ’z dÎ¶.
(1.7.8)
Because f(Î¶) is analytic everywhere within C2,
2Ï€if(z) =
I
C2
f(Î¶)
Î¶ âˆ’z dÎ¶.
(1.7.9)
Using the relationship:
I
C1
f(Î¶)
Î¶ âˆ’z dÎ¶ = âˆ’
I
C1
f(Î¶)
z âˆ’Î¶ dÎ¶,
(1.7.10)

Complex Variables
29
Equation 1.7.8 becomes
f(z) =
1
2Ï€i
I
C
f(Î¶)
Î¶ âˆ’z dÎ¶ +
1
2Ï€i
I
C1
f(Î¶)
z âˆ’Î¶ dÎ¶.
(1.7.11)
Now,
1
Î¶ âˆ’z =
1
Î¶ âˆ’z0 âˆ’z + z0
=
1
Î¶ âˆ’z0
1
1 âˆ’(z âˆ’z0)/(Î¶ âˆ’z0)
(1.7.12)
=
1
Î¶ âˆ’z0
"
1 +
z âˆ’z0
Î¶ âˆ’z0

+
z âˆ’z0
Î¶ âˆ’z0
2
+ Â· Â· Â· +
z âˆ’z0
Î¶ âˆ’z0
n
+ Â· Â· Â·
#
,
(1.7.13)
where |z âˆ’z0|/|Î¶ âˆ’z0| < 1 and
1
z âˆ’Î¶ =
1
z âˆ’z0 âˆ’Î¶ + z0
=
1
z âˆ’z0
1
1 âˆ’(Î¶ âˆ’z0)/(z âˆ’z0)
(1.7.14)
=
1
z âˆ’z0
"
1 +
Î¶ âˆ’z0
z âˆ’z0

+
Î¶ âˆ’z0
z âˆ’z0
2
+ Â· Â· Â· +
Î¶ âˆ’z0
z âˆ’z0
n
+ Â· Â· Â·
#
,
(1.7.15)
where |Î¶ âˆ’z0|/|z âˆ’z0| < 1. Upon substituting these expressions into Equation 1.7.11,
f(z) =
 1
2Ï€i
I
C
f(Î¶)
Î¶ âˆ’z0
dÎ¶ + z âˆ’z0
2Ï€i
I
C
f(Î¶)
(Î¶ âˆ’z0)2 dÎ¶ + Â· Â· Â·
+ (z âˆ’z0)n
2Ï€i
I
C
f(Î¶)
(Î¶ âˆ’z0)n+1 dÎ¶ + Â· Â· Â·

+

1
z âˆ’z0
1
2Ï€i
I
C1
f(Î¶) dÎ¶ +
1
(z âˆ’z0)2
1
2Ï€i
I
C1
f(Î¶)(Î¶ âˆ’z0) dÎ¶ + Â· Â· Â·
+
1
(z âˆ’z0)n
1
2Ï€i
I
C1
f(Î¶)(Î¶ âˆ’z0)nâˆ’1 dÎ¶ + Â· Â· Â·

(1.7.16)
or
f(z) =
a1
z âˆ’z0
+
a2
(z âˆ’z0)2 + Â· Â· Â· +
an
(z âˆ’z0)n + Â· Â· Â· + b0 + b1(z âˆ’z0) + Â· Â· Â· + bn(z âˆ’z0)n + Â· Â· Â· .
(1.7.17)
Equation 1.7.17 is a Laurent expansion.7 If f(z) is analytic at z0, then a1 = a2 = Â· Â· Â· = an =
Â· Â· Â· = 0 and the Laurent expansion reduces to a Taylor expansion. If z0 is a singularity of
f(z), then the Laurent expansion includes both positive and negative powers. The coeï¬ƒcient
of the (z âˆ’z0)âˆ’1 term, a1, is the residue, for reasons that will appear in the next section.
Unlike the Taylor series, a Laurent series provides no straightforward method for ob-
taining the coeï¬ƒcients. For the remaining portions of this section we illustrate their con-
struction. These techniques include replacing a function by its appropriate power series,
the use of geometric series to expand the denominator, and the use of algebraic tricks to
assist in applying the ï¬rst two methods.
7 Laurent, M., 1843: Extension du thÂ´eor`eme de M. Cauchy relatif `a la convergence du dÂ´eveloppement
dâ€™une fonction suivant les puissances ascendantes de la variable x. C. R. lâ€™Acad. Sci., 17, 938â€“942.

30
Advanced Engineering Mathematics: A Second Course
â€¢ Example 1.7.3
Laurent expansions provide a formalism for the classiï¬cation of singularities of a func-
tion. Isolated singularities fall into three types; they are as follows:
â€¢ Essential Singularity: Consider the function f(z) = cos(1/z). Using the expansion for
cosine,
cos
1
z

= 1 âˆ’
1
2!z2 +
1
4!z4 âˆ’
1
6!z6 + Â· Â· Â·
(1.7.18)
for 0 < |z| < âˆ. Note that this series never truncates in the inverse powers of z. Essential
singularities have Laurent expansions, which have an inï¬nite number of inverse powers of
z âˆ’z0. The value of the residue for this essential singularity at z = 0 is zero.
â€¢ Removable Singularity: Consider the function f(z) = sin(z)/z.
This function has a
singularity at z = 0. Upon applying the expansion for sine,
sin(z)
z
= 1
z

z âˆ’z3
3! + z5
5! âˆ’z7
7! + z9
9! âˆ’. . .

= 1 âˆ’z2
3! + z4
5! âˆ’z6
7! + z8
9! âˆ’. . .
(1.7.19)
for all z, if the division is permissible. We made f(z) analytic by deï¬ning it by Equation
1.7.19 and, in the process, removed the singularity. The residue for a removable singularity
always equals zero.
â€¢ Pole of order n: Consider the function
f(z) =
1
(z âˆ’1)3(z + 1).
(1.7.20)
This function has two singularities: one at z = 1 and the other at z = âˆ’1. We shall only
consider the case z = 1. After a little algebra,
f(z) =
1
(z âˆ’1)3
1
2 + (z âˆ’1) = 1
2
1
(z âˆ’1)3
1
1 + (z âˆ’1)/2
(1.7.21)
= 1
2
1
(z âˆ’1)3

1 âˆ’z âˆ’1
2
+ (z âˆ’1)2
4
âˆ’(z âˆ’1)3
8
+ Â· Â· Â·

(1.7.22)
=
1
2(z âˆ’1)3 âˆ’
1
4(z âˆ’1)2 +
1
8(z âˆ’1) âˆ’1
16 + Â· Â· Â·
(1.7.23)
for 0 < |z âˆ’1| < 2. Because the largest inverse (negative) power is three, the singularity
at z = 1 is a third-order pole; the value of the residue is 1/8. Generally, we refer to a
ï¬rst-order pole as a simple pole.
âŠ“âŠ”
â€¢ Example 1.7.4
Let us ï¬nd the Laurent expansion for
f(z) =
z
(z âˆ’1)(z âˆ’3)
(1.7.24)
about the point z = 1.

Complex Variables
31
We begin by rewriting f(z) as
f(z) =
1 + (z âˆ’1)
(z âˆ’1)[âˆ’2 + (z âˆ’1)] = âˆ’1
2
1 + (z âˆ’1)
(z âˆ’1)[1 âˆ’1
2(z âˆ’1)]
(1.7.25)
= âˆ’1
2
1 + (z âˆ’1)
(z âˆ’1)
[1 + 1
2(z âˆ’1) + 1
4(z âˆ’1)2 + Â· Â· Â·]
(1.7.26)
= âˆ’1
2
1
z âˆ’1 âˆ’3
4 âˆ’3
8 (z âˆ’1) âˆ’3
16 (z âˆ’1)2 âˆ’Â· Â· Â·
(1.7.27)
provided 0 < |z âˆ’1| < 2. Therefore we have a simple pole at z = 1 and the value of the
residue is âˆ’1/2. A similar procedure would yield the Laurent expansion about z = 3.
âŠ“âŠ”
â€¢ Example 1.7.5
Let us ï¬nd the Laurent expansion for
f(z) =
zn + zâˆ’n
z2 âˆ’2z cosh(Î±) + 1,
Î± > 0,
n â‰¥0,
(1.7.28)
about the point z = 0.
We begin by rewriting f(z) as
f(z) =
zn + zâˆ’n
(z âˆ’eÎ±)(z âˆ’eâˆ’Î±) =
1
2 sinh(Î±)
zn + zâˆ’n
z âˆ’eÎ±
âˆ’zn + zâˆ’n
z âˆ’eâˆ’Î±

.
(1.7.29)
Because
1
z âˆ’eÎ± = âˆ’
eâˆ’Î±
1 âˆ’zeâˆ’Î± = âˆ’eâˆ’Î±  1 + zeâˆ’Î± + z2eâˆ’2Î± + Â· Â· Â·

(1.7.30)
if |z| < eÎ± and
1
z âˆ’eâˆ’Î± = âˆ’
eÎ±
1 âˆ’zeÎ± = âˆ’eÎ±  1 + zeÎ± + z2e2Î± + Â· Â· Â·

(1.7.31)
if |z| < eâˆ’Î±,
f(z) =
eÎ±
2 sinh(Î±)
 zn + zn+1eÎ± + zn+2e2Î± + Â· Â· Â· + zâˆ’n + z1âˆ’neÎ± + z2âˆ’ne2Î± + Â· Â· Â·

(1.7.32)
âˆ’
eâˆ’Î±
2 sinh(Î±)
 zn + zn+1eâˆ’Î± + zn+2eâˆ’2Î± + Â· Â· Â· + zâˆ’n + z1âˆ’neâˆ’Î± + z2âˆ’neâˆ’2Î± + Â· Â· Â·

,
if |z| < eâˆ’Î±. Clearly we have an nth-order pole at z = 0. The residue, the coeï¬ƒcient of all
of the zâˆ’1 terms in Equation 1.7.32, is found directly and equals
Res[f(z); 0] = sinh(nÎ±)
sinh(Î±) .
(1.7.33)
âŠ“âŠ”
For complicated complex functions, it is very diï¬ƒcult to determine the nature of the
singularities by ï¬nding the complete Laurent expansion, and we must try another method.
We shall call it â€œa poor manâ€™s Laurent expansion.â€ The idea behind this method is the
fact that we generally need only the ï¬rst few terms of the Laurent expansion to discover

32
Advanced Engineering Mathematics: A Second Course
its nature. Consequently, we compute these terms through the application of power series
where we retain only the leading terms. Consider the following example.
â€¢ Example 1.7.6
Let us discover the nature of the singularity at z = 0 of the function
f(z) =
etz
z sinh(az),
(1.7.34)
where a and t are real.
We begin by replacing the exponential and hyperbolic sine by their Taylor expansion
about z = 0. Then
f(z) = 1 + tz + t2z2/2 + Â· Â· Â·
z(az + a3z3/6 + Â· Â· Â·) .
(1.7.35)
Factoring out az in the denominator,
f(z) = 1 + tz + t2z2/2 + Â· Â· Â·
az2(1 + a2z2/6 + Â· Â· Â·).
(1.7.36)
Within the parentheses, all of the terms except the leading one are small. Therefore, by
long division, we formally have that
f(z) =
1
az2 (1 + tz + t2z2/2 + Â· Â· Â·)(1 âˆ’a2z2/6 + Â· Â· Â·)
(1.7.37)
=
1
az2 (1 + tz + t2z2/2 âˆ’a2z2/6 + Â· Â· Â·) =
1
az2 + t
az + 3t2 âˆ’a2
6a
+ Â· Â· Â· .
(1.7.38)
Thus, we have a second-order pole at z = 0 and the residue equals t/a.
Problems
1. Find the Taylor expansion of f(z) = (1 âˆ’z)âˆ’2 about the point z = 0.
2. Find the Taylor expansion of f(z) = (z âˆ’1)ez about the point z = 1. (Hint: Donâ€™t ï¬nd
the expansion by taking derivatives.)
By constructing a Laurent expansion, describe the type of singularity and give the residue
at z0 for each of the following functions:
3. f(z) = z10eâˆ’1/z;
z0 = 0
4. f(z) = zâˆ’3 sin2(z);
z0 = 0
5. f(z) = cosh(z) âˆ’1
z2
;
z0 = 0
6. f(z) =
z
(z + 2)2 ;
z0 = âˆ’2
7. f(z) = ez + 1
eâˆ’z âˆ’1;
z0 = 0
8. f(z) =
eiz
z2 + b2 ;
z0 = bi
9. f(z) =
1
z(z âˆ’2);
z0 = 2
10. f(z) = exp(z2)
z4
;
z0 = 0

Complex Variables
33
1
x
y
C
n
2
1
z
z
z
Cn
C
C   
2
Figure 1.8.1: Contour used in deriving the residue theorem.
1.8 THEORY OF RESIDUES
Having shown that around any singularity we may construct a Laurent expansion,
we now use this result in the integration of closed complex integrals. Consider a closed
contour in which the function f(z) has a number of isolated singularities. As we did in the
case of Cauchyâ€™s integral formula, we introduce a new contour Câ€² that excludes all of the
singularities because they are isolated. See Figure 1.8.1. Therefore,
I
C
f(z) dz âˆ’
I
C1
f(z) dz âˆ’Â· Â· Â· âˆ’
I
Cn
f(z) dz =
I
Câ€² f(z) dz = 0.
(1.8.1)
Consider now the mth integral, where 1 â‰¤m â‰¤n. Constructing a Laurent expansion for
the function f(z) at the isolated singularity z = zm, this integral equals
I
Cm
f(z) dz =
âˆ
X
k=1
ak
I
Cm
1
(z âˆ’zm)k dz +
âˆ
X
k=0
bk
I
Cm
(z âˆ’zm)k dz.
(1.8.2)
Because (z âˆ’zm)k is an entire function if k â‰¥0, the integrals equal zero for each term in
the second summation. We use Cauchyâ€™s integral formula to evaluate the remaining terms.
The analytic function in the numerator is 1. Because dkâˆ’1(1)/dzkâˆ’1 = 0 if k > 1, all of
the terms vanish except for k = 1. In that case, the integral equals 2Ï€ia1, where a1 is the
value of the residue for that particular singularity. Applying this approach to each of the
singularities, we obtain the following:
Cauchyâ€™s residue theorem:8 If f(z) is analytic inside and on a closed contour C (taken
in the positive sense) except at points z1, z2, . . ., zn where f(z) has singularities, then
I
C
f(z) dz = 2Ï€i
n
X
j=1
Res[f(z); zj],
(1.8.3)
8 See MitrinoviÂ´c, D. S., and J. D. KeË˜ckiÂ´c, 1984: The Cauchy Method of Residues: Theory and Ap-
plications.
D. Reidel Publishing, 361 pp.
Section 10.3 gives the historical development of the residue
theorem.

34
Advanced Engineering Mathematics: A Second Course
where Res[f(z); zj] denotes the residue of the jth isolated singularity of f(z) located at
z = zj.
âŠ“âŠ”
â€¢ Example 1.8.1
Let us compute
H
|z|=2 z2/(z + 1) dz by the residue theorem, assuming that we take the
contour in the positive sense.
Because the contour is a circle of radius 2, centered on the origin, the singularity at
z = âˆ’1 lies within the contour. If the singularity were not inside the contour, then the
integrand would have been analytic inside and on the contour C. In this case, the answer
would then be zero by the Cauchy-Goursat theorem.
Returning to the original problem, we construct the Laurent expansion for the integrand
around the point z = 1 by noting that
z2
z + 1 = [(z + 1) âˆ’1]2
z + 1
=
1
z + 1 âˆ’2 + (z + 1).
(1.8.4)
The singularity at z = âˆ’1 is a simple pole and by inspection, the value of the residue equals
1. Therefore,
I
|z|=2
z2
z + 1 dz = 2Ï€i.
(1.8.5)
âŠ“âŠ”
As it presently stands, it would appear that we must always construct a Laurent expan-
sion for each singularity if we wish to use the residue theorem. This becomes increasingly
diï¬ƒcult as the structure of the integrand becomes more complicated.
In the following
paragraphs we show several techniques that avoid this problem in practice.
We begin by noting that many functions which we will encounter consist of the ratio of
two polynomials, i.e., rational functions: f(z) = g(z)/h(z). Generally, we can write h(z) as
(z âˆ’z1)m1(z âˆ’z2)m2 Â· Â· Â·. Here we assumed that we divided out any common factors between
g(z) and h(z) so that g(z) does not vanish at z1, z2, . . .. Clearly z1, z2, . . ., are singularities
of f(z). Further analysis shows that the nature of the singularities are a pole of order m1
at z = z1, a pole of order m2 at z = z2, and so forth.
Having found the nature and location of the singularity, we compute the residue as
follows. Suppose that we have a pole of order n. Then we know that its Laurent expansion
is
f(z) =
an
(z âˆ’z0)n +
anâˆ’1
(z âˆ’z0)nâˆ’1 + Â· Â· Â· + b0 + b1(z âˆ’z0) + Â· Â· Â· .
(1.8.6)
Multiplying both sides of Equation 1.8.6 by (z âˆ’z0)n,
F(z) = (z âˆ’z0)nf(z) = an + anâˆ’1(z âˆ’z0) + Â· Â· Â· + b0(z âˆ’z0)n + b1(z âˆ’z0)n+1 + Â· Â· Â· . (1.8.7)
Because F(z) is analytic at z = z0, it has the Taylor expansion
F(z) = F(z0) + F â€²(z0)(z âˆ’z0) + Â· Â· Â· + F (nâˆ’1)(z0)
(n âˆ’1)! (z âˆ’z0)nâˆ’1 + Â· Â· Â· .
(1.8.8)
Matching powers of z âˆ’z0 in Equation 1.8.7 and Equation 1.8.8, the residue equals
Res[f(z); z0] = a1 = F (nâˆ’1)(z0)
(n âˆ’1)! .
(1.8.9)

Complex Variables
35
Substituting in F(z) = (z âˆ’z0)nf(z), we can compute the residue of a pole of order n by
Res[f(z); zj] =
1
(n âˆ’1)! lim
zâ†’zj
dnâˆ’1
dznâˆ’1

(z âˆ’zj)nf(z)

.
(1.8.10)
For a simple pole, Equation 1.8.10 simpliï¬es to
Res[f(z); zj] = lim
zâ†’zj(z âˆ’zj)f(z).
(1.8.11)
Quite often, f(z) = p(z)/q(z). From lâ€™HË†opitalâ€™s rule, it follows that Equation 1.8.11 be-
comes
Res[f(z); zj] = p(zj)
qâ€²(zj).
(1.8.12)
Recall that these formulas work only for ï¬nite-order poles. For an essential singularity we
must compute the residue from its Laurent expansion; however, essential singularities are
very rare in applications.
â€¢ Example 1.8.2
Let us evaluate
I
C
eiz
z2 + a2 dz,
(1.8.13)
where C is any contour that includes both poles at z = Â±ai and is in the positive sense.
From Cauchyâ€™s residue theorem,
I
C
eiz
z2 + a2 dz = 2Ï€i

Res

eiz
z2 + a2 ; ai

+ Res

eiz
z2 + a2 ; âˆ’ai

.
(1.8.14)
The singularities at z = Â±ai are simple poles. The corresponding residues are
Res

eiz
z2 + a2 ; ai

= lim
zâ†’ai(z âˆ’ai)
eiz
(z âˆ’ai)(z + ai) = eâˆ’a
2ia
(1.8.15)
and
Res

eiz
z2 + a2 ; âˆ’ai

=
lim
zâ†’âˆ’ai(z + ai)
eiz
(z âˆ’ai)(z + ai) = âˆ’ea
2ia.
(1.8.16)
Consequently,
I
C
eiz
z2 + a2 dz = âˆ’2Ï€
2a
 ea âˆ’eâˆ’a
= âˆ’2Ï€
a sinh(a).
(1.8.17)
âŠ“âŠ”

36
Advanced Engineering Mathematics: A Second Course
â€¢ Example 1.8.3
Let us evaluate
1
2Ï€i
I
C
etz
z2(z2 + 2z + 2) dz,
(1.8.18)
where C includes all of the singularities and is in the positive sense.
The integrand has a second-order pole at z = 0 and two simple poles at z = âˆ’1 Â± i,
which are the roots of z2 + 2z + 2 = 0. Therefore, the residue at z = 0 is
Res

etz
z2(z2 + 2z + 2); 0

= lim
zâ†’0
1
1!
d
dz

(z âˆ’0)2

etz
z2(z2 + 2z + 2)

(1.8.19)
= lim
zâ†’0

tetz
z2 + 2z + 2 âˆ’
(2z + 2)etz
(z2 + 2z + 2)2

= t âˆ’1
2
.
(1.8.20)
The residue at z = âˆ’1 + i is
Res

etz
z2(z2 + 2z + 2); âˆ’1 + i

=
lim
zâ†’âˆ’1+i[z âˆ’(âˆ’1 + i)]
etz
z2(z2 + 2z + 2)
(1.8.21)
=

lim
zâ†’âˆ’1+i
etz
z2
 
lim
zâ†’âˆ’1+i
z + 1 âˆ’i
z2 + 2z + 2

(1.8.22)
= exp[(âˆ’1 + i)t]
2i(âˆ’1 + i)2
= exp[(âˆ’1 + i)t]
4
.
(1.8.23)
Similarly, the residue at z = âˆ’1 âˆ’i is
Res

etz
z2(z2 + 2z + 2); âˆ’1 âˆ’i

=
lim
zâ†’âˆ’1âˆ’i[z âˆ’(âˆ’1 âˆ’i)]
etz
z2(z2 + 2z + 2)
(1.8.24)
=

lim
zâ†’âˆ’1âˆ’i
etz
z2
 
lim
zâ†’âˆ’1âˆ’i
z + 1 + i
z2 + 2z + 2

(1.8.25)
= exp[(âˆ’1 âˆ’i)t]
(âˆ’2i)(âˆ’1 âˆ’i)2 = exp[(âˆ’1 âˆ’i)t]
4
.
(1.8.26)
Then by the residue theorem,
1
2Ï€i
I
C
etz
z2(z2 + 2z + 2) dz = Res

etz
z2(z2 + 2z + 2); 0

+ Res

etz
z2(z2 + 2z + 2); âˆ’1 + i

+ Res

etz
z2(z2 + 2z + 2); âˆ’1 âˆ’i

(1.8.27)
= t âˆ’1
2
+ exp[(âˆ’1 + i)t]
4
+ exp[(âˆ’1 âˆ’i)t]
4
(1.8.28)
= 1
2 [t âˆ’1 + eâˆ’t cos(t)] .
(1.8.29)

Complex Variables
37
Problems
Assuming that all of the following closed contours are in the positive sense, use the residue
theorem to evaluate the following integrals:
1.
I
|z|=1
z + 1
z4 âˆ’2z3 dz
2.
I
|z|=1
(z + 4)3
z4 + 5z3 + 6z2 dz
3.
I
|z|=1
1
1 âˆ’ez dz
4.
I
|z|=2
z2 âˆ’4
(z âˆ’1)4 dz
5.
I
|z|=2
z3
z4 âˆ’1 dz
6.
I
|z|=1
zne2/z dz,
n > 0
7.
I
|z|=1
e1/z cos(1/z) dz
8.
I
|z|=2
2 + 4 cos(Ï€z)
z(z âˆ’1)2
dz
9.
I
|zâˆ’1|= 1
2
z + 1
z âˆ’1
dz
sin(Ï€z)
Hint for Problem 9: sin(Ï€z) = âˆ’sin[Ï€(z âˆ’1)] and z + 1 = (z âˆ’1) + 2.
1.9 EVALUATION OF REAL DEFINITE INTEGRALS
One of the important applications of the theory of residues consists of the evaluation of
certain types of real deï¬nite integrals. Similar techniques apply when the integrand contains
a sine or cosine.
â€¢ Example 1.9.1
Let us evaluate the integral
Z âˆ
0
dx
x2 + 1 = 1
2
Z âˆ
âˆ’âˆ
dx
x2 + 1.
(1.9.1)
This integration occurs along the real axis. In terms of complex variables, we can rewrite
Equation 1.9.1 as
Z âˆ
0
dx
x2 + 1 = 1
2
Z
C1
dz
z2 + 1,
(1.9.2)
where the contour C1 is the line â„‘(z) = 0. However, the use of the residue theorem requires
an integration along a closed contour. Let us choose the one pictured in Figure 1.9.1. Then
I
C
dz
z2 + 1 =
Z
C1
dz
z2 + 1 +
Z
C2
dz
z2 + 1,
(1.9.3)
where C denotes the complete closed contour and C2 denotes the integration path along
a semicircle at inï¬nity. Clearly we want the second integral on the right side of Equation
1.9.3 to vanish; otherwise, our choice of the contour C2 is poor. Because z = ReÎ¸i and
dz = iReÎ¸i dÎ¸,

Z
C2
dz
z2 + 1
 =

Z Ï€
0
iR exp(Î¸i)
1 + R2 exp(2Î¸i) dÎ¸
 â‰¤
Z Ï€
0
R
R2 âˆ’1 dÎ¸,
(1.9.4)
which tends to zero as R â†’âˆ. On the other hand, the residue theorem gives
I
C
dz
z2 + 1 = 2Ï€i Res

1
z2 + 1; i

= 2Ï€i lim
zâ†’i
z âˆ’i
z2 + 1 = 2Ï€i Ã— 1
2i = Ï€.
(1.9.5)

38
Advanced Engineering Mathematics: A Second Course
1
x
y
C
C
2
Figure 1.9.1: Contour used in evaluating the integral, Equation 1.9.1.
Therefore,
Z âˆ
0
dx
x2 + 1 = Ï€
2 .
(1.9.6)
Note that we only evaluated the residue in the upper half-plane because it is the only one
inside the contour.
âŠ“âŠ”
This example illustrates the basic concepts of evaluating deï¬nite integrals by the residue
theorem.
We introduce a closed contour that includes the real axis and an additional
contour. We must then evaluate the integral along this additional contour as well as the
closed contour integral. If we properly choose our closed contour, this additional integral
vanishes. For certain classes of general integrals, we shall now show that this additional
contour is a circular arc at inï¬nity.
Theorem: If, on a circular arc CR with a radius R and center at the origin, zf(z) â†’0
uniformly with |z| âˆˆCR and as R â†’âˆ, then
lim
Râ†’âˆ
Z
CR
f(z) dz = 0.
(1.9.7)
The proof is as follows: If |zf(z)| â‰¤MR, then |f(z)| â‰¤MR/R. Because the length of
CR is Î±R, where Î± is the subtended angle,

Z
CR
f(z) dz
 â‰¤MR
R Î±R = Î±MR â†’0,
(1.9.8)
because MR â†’0 as R â†’âˆ.
âŠ“âŠ”
â€¢ Example 1.9.2
A simple illustration of this theorem is the integral
Z âˆ
âˆ’âˆ
dx
x2 + x + 1 =
Z
C1
dz
z2 + z + 1.
(1.9.9)
A quick check shows that z/(z2 + z + 1) tends to zero uniformly as R â†’âˆ. Therefore, if
we use the contour pictured in Figure 1.9.1,
Z âˆ
âˆ’âˆ
dx
x2 + x + 1 =
I
C
dz
z2 + z + 1 = 2Ï€i Res

1
z2 + z + 1; âˆ’1
2 +
âˆš
3
2 i

(1.9.10)
= 2Ï€i
lim
zâ†’âˆ’1
2 +
âˆš
3
2 i

1
2z + 1

= 2Ï€
âˆš
3.
(1.9.11)
âŠ“âŠ”

Complex Variables
39
x
Ï€/3
1
C
Ï€i
e
/6
3
y
C2
C
Figure 1.9.2: Contour used in evaluating the integral, Equation 1.9.13.
â€¢ Example 1.9.3
Let us evaluate
Z âˆ
0
dx
x6 + 1.
(1.9.12)
In place of an inï¬nite semicircle in the upper half-plane, consider the following integral
I
C
dz
z6 + 1,
(1.9.13)
where we show the closed contour in Figure 1.9.2. We chose this contour for two reasons.
First, we only have to evaluate one residue rather than the three enclosed in a traditional
upper half-plane contour. Second, the contour integral along C3 simpliï¬es to a particularly
simple and useful form.
Because the only enclosed singularity lies at z = eÏ€i/6,
I
C
dz
z6 + 1 = 2Ï€i Res

1
z6 + 1; eÏ€i/6

= 2Ï€i
lim
zâ†’eÏ€i/6
z âˆ’eÏ€i/6
z6 + 1
(1.9.14)
= 2Ï€i
lim
zâ†’eÏ€i/6
1
6z5 = âˆ’Ï€i
3 eÏ€i/6.
(1.9.15)
Let us now evaluate Equation 1.9.12 along each of the legs of the contour:
Z
C1
dz
z6 + 1 =
Z âˆ
0
dx
x6 + 1,
(1.9.16)
Z
C2
dz
z6 + 1 = 0,
(1.9.17)
because of Equation 1.9.7 and
Z
C3
dz
z6 + 1 =
Z 0
âˆ
eÏ€i/3 dr
r6 + 1 = âˆ’eÏ€i/3
Z âˆ
0
dx
x6 + 1,
(1.9.18)
since z = reÏ€i/3.

40
Advanced Engineering Mathematics: A Second Course
Substituting into Equation 1.9.15,

1 âˆ’eÏ€i/3 Z âˆ
0
dx
x6 + 1 = âˆ’Ï€i
3 eÏ€i/6
(1.9.19)
or
Z âˆ
0
dx
x6 + 1 = Ï€i
6
2ieÏ€i/6
eÏ€i/6  eÏ€i/6 âˆ’eâˆ’Ï€i/6 =
Ï€
6 sin(Ï€/6) = Ï€
3 .
(1.9.20)
âŠ“âŠ”
â€¢ Example 1.9.4
Rectangular closed contours are best for the evaluation of integrals that involve hyper-
bolic sines and cosines. To illustrate9 this, let us evaluate the integral
2
Z âˆ
0
sin(ax) sinh(x)
[b + cosh(x)]2 dx =
Z âˆ
âˆ’âˆ
sin(ax) sinh(x)
[b + cosh(x)]2 dx = â„‘
Z âˆ
âˆ’âˆ
sinh(x)eiax
[b + cosh(x)]2 dx

,
(1.9.21)
where a > 0 and b > 1.
We begin by determining the value of
I
C
sinh(z)eiaz
[b + cosh(z)]2 dz
about the closed contour shown in Figure 1.9.3. Writing this contour integral in terms of
the four line segments that constitute the closed contour, we have
I
C
sinh(z)eiaz
[b + cosh(z)]2 dz =
Z
C1
sinh(z)eiaz
[b + cosh(z)]2 dz +
Z
C2
sinh(z)eiaz
[b + cosh(z)]2 dz
+
Z
C3
sinh(z)eiaz
[b + cosh(z)]2 dz +
Z
C4
sinh(z)eiaz
[b + cosh(z)]2 dz.
(1.9.22)
Because the integrand behaves as eâˆ’R as R â†’âˆ, the integrals along C2 and C4 vanish.
On the other hand,
Z
C1
sinh(z)eiaz
[b + cosh(z)]2 dz =
Z âˆ
âˆ’âˆ
sinh(x)eiax
[b + cosh(x)]2 dx,
(1.9.23)
and
Z
C3
sinh(z)eiaz
[b + cosh(z)]2 dz = âˆ’eâˆ’2Ï€a
Z âˆ
âˆ’âˆ
sinh(x)eiax
[b + cosh(x)]2 dx,
(1.9.24)
9 This is a slight variation on a problem solved by Spyrou, K. J., B. Cotton, and B. Gurd, 2002:
Analytical expressions of capsize boundary for a ship with roll bias in beam waves.
J. Ship Res., 46,
167â€“174.

Complex Variables
41
  

C
C
C
C
1
2
3
4
(âˆ’R,0)
(R,0)
(R,2  i)
(âˆ’R,2  i)
Ï€
Ï€
y
x
zs
Figure 1.9.3: Rectangular closed contour used to obtain Equation 1.9.31.
because cosh(x + 2Ï€i) = cosh(x) and sinh(x + 2Ï€i) = sinh(x).
Within the closed contour C, we have a single singularity where b + cosh(zs) = 0 or
ezs = âˆ’b âˆ’
âˆš
b2 âˆ’1 or zs = ln(b +
âˆš
b2 âˆ’1 ) + Ï€i. To discover the nature of this singularity,
we expand b + cosh(z) in a Taylor expansion and ï¬nd that
b + cosh(z) = sinh(zs)(z âˆ’zs) + 1
2 cosh(zs)(z âˆ’zs)2 + Â· Â· Â· .
(1.9.25)
Therefore, we have a second-order pole at z = zs. Therefore, the value of the residue there
is
Res
 sinh(z)eiaz
[b + cosh(z)]2 ; zs

= lim
zâ†’zs
d
dz

sinh(z)eiaz
sinh2(zs) + sinh(zs) cosh(zs)(z âˆ’zs) + Â· Â· Â·

(1.9.26)
= ia eâˆ’Ï€a
sinh(zs) exp[ia coshâˆ’1(b)].
(1.9.27)
Therefore,
Z âˆ
âˆ’âˆ
sinh(x)eiax
[b + cosh(x)]2 dx = âˆ’2Ï€a exp[âˆ’Ï€a + ai coshâˆ’1(b)]
(1 âˆ’eâˆ’2Ï€a) sinh(zs)
= Ï€a exp[ai coshâˆ’1(b)]
âˆš
b2 âˆ’1 sinh(Ï€a)
, (1.9.28)
because
sinh(zs) = 1
2

âˆ’b âˆ’
p
b2 âˆ’1 +
1
b +
âˆš
b2 âˆ’1

= âˆ’
p
b2 âˆ’1.
(1.9.29)
Substituting Equation 1.9.28 into Equation 1.9.21 yields
Z âˆ
0
sin(ax) sinh(x)
[b + cosh(x)]2 dx = Ï€a sin[a coshâˆ’1(b)]
2
âˆš
b2 âˆ’1 sinh(Ï€a)
.
(1.9.30)
âŠ“âŠ”

42
Advanced Engineering Mathematics: A Second Course
â€¢ Example 1.9.5
The method of residues is also useful in the evaluation of deï¬nite integrals of the form
R 2Ï€
0
F[sin(Î¸), cos(Î¸)] dÎ¸, where F is a quotient of polynomials in sin(Î¸) and cos(Î¸).
For
example, let us evaluate the integral10
I =
Z 2Ï€
0
cos3(Î¸)
cos2(Î¸) âˆ’a2 dÎ¸,
a > 1.
(1.9.31)
We begin by introducing the complex variable z = eiÎ¸. This substitution yields the
closed contour integral
I = 1
2i
I
C
(z2 + 1)3
(z2 + 1)2 âˆ’4a2z2
dz
z2 ,
(1.9.32)
where C is a circle of radius 1 taken in the positive sense. The integrand of Equation 1.9.32
has ï¬ve singularities: a second-order pole at z5 = 0 and simple poles located at
z1 = âˆ’a âˆ’
p
a2 âˆ’1,
z2 = âˆ’a +
p
a2 âˆ’1,
(1.9.33)
z3 = a âˆ’
p
a2 âˆ’1,
and
z4 = a +
p
a2 âˆ’1.
(1.9.34)
Only the singularities z2, z3, and z5 lie within C. Consequently, the value of I equals 2Ï€i
times the sum of the residues at these three singularities. The residues equal
Res

(z2 + 1)3
z2[(z2 + 1)2 âˆ’4a2z2]; âˆ’a +
p
a2 âˆ’1

=
lim
zâ†’âˆ’a+
âˆš
a2âˆ’1
(z2 + 1)3
z2
lim
zâ†’âˆ’a+
âˆš
a2âˆ’1
z + a âˆ’
âˆš
a2 âˆ’1
(z2 + 1)2 âˆ’4a2z2
(1.9.35)
=
lim
zâ†’âˆ’a+
âˆš
a2âˆ’1
(z2 + 1)3
4z3(z2 + 1 âˆ’2a2)
(1.9.36)
= âˆ’
a2(a âˆ’
âˆš
a2 âˆ’1 )3
(2a2 âˆ’1 âˆ’2a
âˆš
a2 âˆ’1 )(a2 âˆ’1 âˆ’a
âˆš
a2 âˆ’1 )
,
(1.9.37)
Res

(z2 + 1)3
z2[(z2 + 1)2 âˆ’4a2z2]; a âˆ’
p
a2 âˆ’1

=
lim
zâ†’aâˆ’
âˆš
a2âˆ’1
(z2 + 1)3
z2
lim
zâ†’aâˆ’
âˆš
a2âˆ’1
z âˆ’a +
âˆš
a2 âˆ’1
(z2 + 1)2 âˆ’4a2z2
(1.9.38)
=
lim
zâ†’aâˆ’
âˆš
a2âˆ’1
(z2 + 1)3
4z3(z2 + 1 âˆ’2a2)
(1.9.39)
=
a2(a âˆ’
âˆš
a2 âˆ’1 )3
(2a2 âˆ’1 âˆ’2a
âˆš
a2 âˆ’1 )(a2 âˆ’1 âˆ’a
âˆš
a2 âˆ’1 )
,
(1.9.40)
10 Simpliï¬ed version of an integral presented by Jiang, Q. F., and R. B. Smith, 2000: V-waves, bow
shocks, and wakes in supercritical hydrostatic ï¬‚ow. J. Fluid Mech., 406, 27â€“53.

Complex Variables
43
and
Res

(z2 + 1)3
z2[(z2 + 1)2 âˆ’4a2z2]; 0

= lim
zâ†’0
d
dz

(z2 + 1)3
(z2 + 1)2 âˆ’4a2z2

(1.9.41)
= lim
zâ†’0
6z[(z2 + 1)4 âˆ’4a2z2(z2 + 1)2] âˆ’4z(z2 + 1)3(z2 + 1 âˆ’2a2)
[(z2 + 1)2 âˆ’4a2z2]2
(1.9.42)
= 0.
(1.9.43)
Summing the residues, we obtain 0. Therefore,
Z 2Ï€
0
cos3(Î¸)
cos2(Î¸) âˆ’a2 dÎ¸ = 0,
a > 1.
(1.9.44)
Problems
Use the residue theorem to verify the following integrals:
1.
Z âˆ
0
dx
x4 + 1 = Ï€
âˆš
2
4
2.
Z âˆ
âˆ’âˆ
dx
(x2 + 4x + 5)2 = Ï€
2
3.
Z âˆ
âˆ’âˆ
x dx
(x2 + 1)(x2 + 2x + 2) = âˆ’Ï€
5
4.
Z âˆ
0
x2
x6 + 1 dx = Ï€
6
5.
Z âˆ
0
dx
(x2 + 1)2 = Ï€
4
6.
Z âˆ
0
dx
(x2 + 1)(x2 + 4)2 = 5Ï€
288
7.
Z âˆ
âˆ’âˆ
x2 dx
(x2 + a2)(x2 + b2)2 =
Ï€
2b(a + b)2 ,
a, b > 0
8.
Z âˆ
0
t2
(t2 + 1)[t2(a/h + 1) + (a/h âˆ’1)] dt = Ï€
4
"
1 âˆ’
r
a âˆ’h
a + h
#
,
a > h
9. Show that
Z Ï€/2
0
dÎ¸
a + sin2(Î¸) =
Ï€
2
âˆš
a + a2 ,
a > 0.
Step 1: Convert the real integral into a closed contour integration:
Z Ï€/2
0
dÎ¸
a + sin2(Î¸) = i
I
|z|=1
z
(z2 âˆ’1)2 âˆ’4az2 dz,
where z = eÎ¸i.
Step 2: Show that the integrand has four poles: z = Â±âˆša Â± âˆš1 + a. Only two are located
inside the contour: z1 = âˆ’âˆša + âˆš1 + a and z2 = âˆša âˆ’âˆš1 + a.

44
Advanced Engineering Mathematics: A Second Course
Step 3: Show that the corresponding residues are
Res

z
(z2 âˆ’1)2 âˆ’4az2 ; z1

= Res

z
(z2 âˆ’1)2 âˆ’4az2 ; z2

= âˆ’
1
8
âˆš
a + a2 .
Step 4: Obtain the ï¬nal result by applying the residue theorem and the results from Step
1 through Step 3.
10. Show that
Z Ï€/2
0
dÎ¸
a2 cos2(Î¸) + b2 sin2(Î¸) =
Ï€
2ab,
b â‰¥a > 0.
Step 1: Convert the real integral into a closed contour integration:
Z Ï€/2
0
dÎ¸
a2 cos2(Î¸) + b2 sin2(Î¸) = âˆ’i
I
|z|=1
z
a2(z2 + 1)2 âˆ’b2(z2 âˆ’1)2 dz,
where z = eÎ¸i.
Step 2: Show that the integrand has four simple poles located at z2
+ = (b + a)/(b âˆ’a), and
z2
âˆ’= (bâˆ’a)/(b+a). Only two are located inside the contour: z(1)
âˆ’=
p
(b âˆ’a)/(b + a), and
z(2)
âˆ’= âˆ’
p
(b âˆ’a)/(b + a).
Step 3: Show that the corresponding residues are
Res

z
a2(z2 + 1)2 âˆ’b2(z2 âˆ’1)2 ; z(1)
âˆ’

= Res

z
a2(z2 + 1)2 âˆ’b2(z2 âˆ’1)2 ; z(2)
âˆ’

=
1
8ab.
Step 4: Obtain the ï¬nal result by employing the residue theorem and the results from Step
1 through Step 3.
11. Show that
Z Ï€
0
sin2(Î¸)
a + b cos(Î¸) dÎ¸ = Ï€
b2

a âˆ’
p
a2 âˆ’b2

,
a > b > 0.
Step 1: Convert the real integral into a closed contour integration:
Z Ï€
0
sin2(Î¸)
a + b cos(Î¸) dÎ¸ = i
4
I
|z|=1
(z2 âˆ’1)2
[b(z2 + 1) + 2az]z2 dz,
where z = eÎ¸i.
Step 2: Show that the integrand has a second-order pole at z = 0 and simple poles at
z1,2 =
 âˆ’a Â±
âˆš
a2 âˆ’b2 
/b. Only the poles located at z = 0 and z1 =
 âˆ’a +
âˆš
a2 âˆ’b2 
/b
lie within the closed contour.
Step 3: Show that the corresponding residues are
Res

(z2 âˆ’1)2
[b(z2 + 1) + 2az]z2 ; 0

= âˆ’2a
b2 ,
and Res

(z2 âˆ’1)2
[b(z2 + 1) + 2az]z2 ; z1

= 2
âˆš
a2 âˆ’b2
b2
.

Complex Variables
45
Step 4: Obtain the ï¬nal results by employing the residue theorem and Step 1 through Step
3.
12. Show that
Z 2Ï€
0
einÎ¸
1 + 2r cos(Î¸) + r2 dÎ¸ = 2Ï€ (âˆ’r)n
1 âˆ’r2 ,
1 > |r|,
n = 0, 1, 2, . . . .
Step 1: Convert the real integral into a closed contour integration:
Z 2Ï€
0
einÎ¸
1 + 2r cos(Î¸) + r2 dÎ¸ = âˆ’i
I
|z|=1
zn
r(z2 + 1) + (1 + r2)z dz,
where z = eÎ¸i.
Step 2: Show that the integrand has two simple poles: z+ = âˆ’r, and zâˆ’= âˆ’1/r. Why is
the z+ pole the only one inside the contour?
Step 3: Show that the corresponding residue is
Res

zn
r(z2 + 1) + (1 + r2)z ; z+

= (âˆ’r)n
1 âˆ’r2 .
Step 4: Obtain the ï¬nal result by using the residue theorem and Step 1 through Step 3.
13. Show that
Z 2Ï€
0
sin2n(Î¸) dÎ¸ = 2Ï€(2n)!
(2nn!)2 .
Step 1: Convert the real integral into a closed contour integration:
Z 2Ï€
0
sin2n(Î¸) dÎ¸ =
âˆ’i
(âˆ’1)n22n
I
|z|=1
(z2 âˆ’1)2n
z2n+1
dz,
where z = eÎ¸i.
Step 2: Show that the integrand has a pole of order 2n + 1 at z = 0.
Step 3: Because
(z2 âˆ’1)2n = z4n âˆ’2nz4nâˆ’1 + Â· Â· Â· + (2n)!(âˆ’1)n
n!n!
z2n + Â· Â· Â· ,
show that
Res
(z2 âˆ’1)2n
z2n+1
; 0

= (2n)!(âˆ’1)n
n!n!
.
Step 4: Obtain the ï¬nal result by using the residue theorem and Step 1 through Step 3.
14. Show that
Z Ï€
âˆ’Ï€
cos(nÎ¸)
cos(Î¸) + Î± dÎ¸ = 2Ï€
 âˆ’Î± +
âˆš
Î±2 âˆ’1
n
âˆš
Î±2 âˆ’1
,
Î± > 1,
n â‰¥0.

46
Advanced Engineering Mathematics: A Second Course
Step 1: Convert the real integral into a closed contour integration:
Z Ï€
âˆ’Ï€
cos(nÎ¸)
cos(Î¸) + Î± dÎ¸ = 1
i
I
|z|=1
zn + zâˆ’n
z2 + 2Î±z + 1 dz,
where z = eÎ¸i.
Step 2: Assume that n Ì¸= 0. Show that the integrand has an n-order pole at z = 0 and
simple poles at z1,2 = âˆ’Î± Â±
âˆš
Î±2 âˆ’1. Why is z1 = âˆ’Î± +
âˆš
Î±2 âˆ’1 the only simple pole that
lies inside the contour?
Step 3: Because
zn + zâˆ’n
z2 + 2Î±z + 1 =
1
2
âˆš
Î±2 âˆ’1
zn + zâˆ’n
z âˆ’z1
âˆ’zn + zâˆ’n
z âˆ’z2

,
show that
Res
zn + zâˆ’n
z âˆ’z1
; z1

= z2n
1
+ 1
zn
1
.
Step 4: Because
zn + zâˆ’n
z âˆ’z1
= âˆ’
 zn + zâˆ’n 1
z1
"
1 +
 z
z1

+
 z
z1
2
+
 z
z1
3
+ Â· Â· Â·
#
,
show that
Res
zn + zâˆ’n
z âˆ’z1
; 0

= âˆ’1
zn
1
,
and
Res
zn + zâˆ’n
z âˆ’z2
; 0

= âˆ’1
zn
2
= âˆ’zn
1 .
Step 5: Use the residue theorem plus Steps 1 through 4 to obtain the ï¬nal result when
n Ì¸= 0.
Step 6: Redo the problem when n = 0. In this case we only have the pole at z = z1.
15. Show that
Z Ï€
0
cos(nÎ¸)
cosh(Î±) âˆ’cos(Î¸) dÎ¸ =
Ï€
sinh(Î±)eâˆ’nÎ±,
Î± Ì¸= 0,
n â‰¥0.
Step 1: Convert the real integral into a closed contour integration:
Z Ï€
0
cos(nÎ¸)
cosh(Î±) âˆ’cos(Î¸) dÎ¸ = âˆ’1
i
I
|z|=1
zn + zâˆ’n
z2 âˆ’2z cosh(Î±) + 1 dz,
where z = eÎ¸i.
Step 2: Show that the integrand has an n-order pole at z = 0 and simple poles at z1,2 =
eÎ±, eâˆ’Î±. Because Î± can be taken as positive without loss of generality, then only the poles
located at z = 0 and z = eâˆ’Î± lie within the closed contour.

Complex Variables
47
Step 3: Because
zn + zâˆ’n
z2 âˆ’2z cosh(Î±) + 1 =
1
2 sinh(Î±)
zn + zâˆ’n
z âˆ’eÎ±
âˆ’zn + zâˆ’n
z âˆ’eÎ±

,
show that the corresponding residues are
Res

zn + zâˆ’n
z2 âˆ’2z cosh(Î±) + 1; 0

= sinh(nÎ±)
sinh(Î±) ,
from Example 1.7.5 and
Res

zn + zâˆ’n
z2 âˆ’2z cosh(Î±) + 1; eâˆ’Î±

= âˆ’cosh(nÎ±)
sinh(Î±) .
Step 4: Use the residue theorem plus Steps 1 through 3 to ï¬nish the problem.
16. Show that
Z âˆ
0
x2
(1 âˆ’x2)2 + a2x2 dx =
Ï€
2|a|,
where a is real and not equal to zero.
Step 1: Show that
Z âˆ
0
x2
(1 âˆ’x2)2 + a2x2 dx = 1
2
I
C
z2
(1 âˆ’z2)2 + a2z2 dz,
where C denotes a semicircle of inï¬nite radius in the upper half of the complex plane. Along
the real axis, the contour slightly above y = 0 when x < 0 and slightly below y = 0 when
x > 0.
Step 2: Show that the poles of the integrand are simple and equal
zn =
ï£±
ï£²
ï£³
Â± 1
2
 Â±
âˆš
4 âˆ’a2 + |a|i

,
if
0 < |a| < 2,
Â± i
2
 |a| Â±
âˆš
a2 âˆ’4

,
if
2 < |a|.
If |a| = 2, we have second-order poles at zn = Â±i.
Step 3: Show that the residues for the poles in the upper half plane are
Res

f(z); 1
2
 Â±
p
4 âˆ’a2 + |a|i

= Â±(Â±
âˆš
4 âˆ’a2 + |a|i)/2
2|a|i
âˆš
4 âˆ’a2
,
Res

f(z); i
2
 |a| Â±
p
a2 âˆ’4

= âˆ“i(|a| Â±
âˆš
a2 âˆ’4 )/2
2|a|
âˆš
a2 âˆ’4
,
and
Res[f(z); i ] = âˆ’i
4.
Step 4: Show that when you sum the residues for the cases for 0 < |a| < 2 and 2 < |a|, you
obtain âˆ’i/(2|a|)

48
Advanced Engineering Mathematics: A Second Course
Step 5: Redo the calculation when |a| = 2.
17. Evaluating the closed contour integral
I
C
eiaz
cosh2(bz) dz,
around the rectangular contour with vertices at (âˆ, 0), (âˆ’âˆ, 0), (âˆ, Ï€/b), and (âˆ’âˆ, Ï€/b),
show that
Z âˆ
0
cos(ax)
cosh2(bx) dx =
Ï€a
2b2 sinh[aÏ€/(2b)],
a, b > 0.
Step 1: Show that
I
C
eiaz
cosh2(bz) dz =
Z
C1
eiaz
cosh2(bz) dz +
Z
C2
eiaz
cosh2(bz) dz
+
Z
C3
eiaz
cosh2(bz) dz +
Z
C4
eiaz
cosh2(bz) dz,
where C1, C2, C3 and C4 are the contours along the bottom, right side, top, and left side
of the rectangle.
Step 2: Show that the integrals along C2 and C4 vanish. Why?
Step 3: Show that
Z
C1
eiaz
cosh2(bz) dz =
Z âˆ
âˆ’âˆ
eiax
cosh2(bx) dx,
and
Z
C3
eiaz
cosh2(bz) dz = âˆ’eâˆ’Ï€a/b
Z âˆ
âˆ’âˆ
eiax
cosh2(bx) dx.
Step 4: Setting zs = Ï€i/(2b), show that the Laurent expansion for eiaz/ cosh2(bz) at zs is
eiaz
cosh2(bz) = âˆ’
eiazs
b2(z âˆ’zs)2 âˆ’
ia eiazs
b2(z âˆ’zs) + Â· Â· Â· .
Hence, we have a second-order pole there.
Step 5: Show that
(1 âˆ’eâˆ’Ï€a/b)
Z âˆ
âˆ’âˆ
eiax
cosh2(bx) dx = 2Ï€aeâˆ’Ï€a/(2b)
b2
.
Step 6: Simplify Step 5 to obtain the desired result.
18. Using the closed contour integral
I
C
z
cosh(z) cosh(z + a) dz,

Complex Variables
49
where C is a rectangular contour with vertices at (âˆ, 0), (âˆ’âˆ, 0), (âˆ, Ï€), and (âˆ’âˆ, Ï€),
show11 that
Z âˆ
0
dx
cosh(x) cosh(x + a) =

2a/ sinh(a),
if
a Ì¸= 0,
2,
if
a = 2.
Step 1: If a Ì¸= 0, show that
I
C
f(z) dz = 2Ï€i Res[f(z); z1] + 2Ï€i Res[f(z); z2],
where z1 and z2 are simple poles with z1 = Ï€i/2 and z2 = âˆ’a + Ï€i/2, respectively.
Step 2: Show that in this case,
Res[f(z); z1] = âˆ’
Ï€i
2 sinh(a),
and
Res[f(z); z2] =
Ï€i
2 âˆ’a

1
sinh(a).
Step 3: If a = 0, show that we a second-order pole located at z1 = Ï€i/2 within the closed
contour with
Res[f(z); z1] =
lim
zâ†’Ï€i/2
d
dz
z(z âˆ’Ï€i/2)2
cosh2(z)

= âˆ’lim
Î·â†’0
d
dÎ·
Î·2(Î· + Ï€i/2)
sinh2(Î·)

= âˆ’lim
Î·â†’0
d
dÎ·

Î· + Ï€i/2
(1 + Î·2/6 + Â· Â· Â·)2

= âˆ’lim
Î·â†’0
(1 + Î·2/6 + Â· Â· Â·) âˆ’(Î· + Ï€i/2)(3Î· + Â· Â· Â·)
(1 + Î·2/6 + Â· Â· Â·)4

= âˆ’1.
Step 4: Denoting the contour along and parallel to the y-axis at x = âˆas C2 and the
contour along and parallel to the y-axis at x = âˆ’âˆas C4, show that
Z
C2
f(z) dz â†’0
as
x â†’âˆ,
and
Z
C4
f(z) dz â†’0
as
x â†’âˆ’âˆ.
Step 5: Along the real axis, call it C1, show that
Z
C1
f(z) dz =
Z âˆ
âˆ’âˆ
x
cosh(x) cosh(x + a) dx,
while along the contour C3 (which runs parallel to the real axis but Ï€ units above it),
Z
C3
f(z) dz = âˆ’
Z âˆ
âˆ’âˆ
x + Ï€i
cosh(x) cosh(x + a) dx.
Step 6: If a Ì¸= 0, show that
âˆ’Ï€i
Z âˆ
âˆ’âˆ
dx
cosh(x) cosh(x + a) = âˆ’2Ï€ai
sinh(a),
11 See Yan, J. R., X. H. Yan, J. Q. You, and J. X. Zhong, 1993: On the interaction between two
nonpropagating hydrodynamic solitons. Phys. Fluids A, 5, 1651â€“1656.

50
Advanced Engineering Mathematics: A Second Course
while for a = 0, show that
âˆ’Ï€i
Z âˆ
âˆ’âˆ
dx
cosh2(x) = âˆ’2Ï€i.
19. During an electromagnetic calculation, Strutt12 needed to prove that
Ï€ sinh(Ïƒx)
cosh(ÏƒÏ€) = 2Ïƒ
âˆ
X
n=0
cos
 n + 1
2

(x âˆ’Ï€)

Ïƒ2 +
 n + 1
2
2
,
|x| â‰¤Ï€.
Verify his proof by doing the following:
Step 1: Using the residue theorem, show that
1
2Ï€i
I
CN
Ï€ sinh(xz)
cosh(Ï€z)
dz
z âˆ’Ïƒ = Ï€ sinh(Ïƒx)
cosh(ÏƒÏ€) âˆ’
N
X
n=âˆ’Nâˆ’1
(âˆ’1)n sin
 n + 1
2

x

Ïƒ âˆ’i
 n + 1
2

,
where CN is a circular contour that includes the poles z = Ïƒ and zn = Â±i
 n + 1
2

, n =
0, 1, 2, . . . , N.
Step 2: Show that in the limit of N â†’âˆ, the contour integral vanishes. Hint: Examine
the behavior of z sinh(xz)/[(z âˆ’Ïƒ) cosh(Ï€z)] as |z| â†’âˆ. Use Equation 1.9.7 where CR is
the circular contour.
Step 3: Break the inï¬nite series in Step 1 into two parts and simplify.
You would obtain the same series by computing the Fourier series of sinh(Ïƒx)/ cosh(ÏƒÏ€)
and using direct integration.
1.10 CAUCHYâ€™S PRINCIPAL VALUE INTEGRAL
The conventional deï¬nition of the integral of a function f(x) of the real variable x over
a ï¬nite interval a â‰¤x â‰¤b assumes that f(x) has a deï¬nite ï¬nite value at each point within
the interval. We shall now extend this deï¬nition to cover cases when f(x) is inï¬nite at a
ï¬nite number of points within the interval.
Consider the case when there is only one point c at which f(x) becomes inï¬nite. If c
is not an endpoint of the interval, we take two small positive numbers Ç« and Î· and examine
the expression
Z câˆ’Ç«
a
f(x) dx +
Z b
c+Î·
f(x) dx.
(1.10.1)
If Equation 1.10.1 exists and tends to a unique limit as Ç« and Î· tend to zero independently,
we say that the improper integral of f(x) over the interval exists, its value being deï¬ned by
Z b
a
f(x) dx = lim
Ç«â†’0
Z câˆ’Ç«
a
f(x) dx + lim
Î·â†’0
Z b
c+Î·
f(x) dx.
(1.10.2)
12 Strutt, M. J. O., 1934: Berechnung des hochfrequenten Feldes einer Kreiszylinderspule in einer konzen-
trischen leitenden SchirmhÂ¨ulle mit ebenen Deckeln. Hochfrequenztechn. Elecktroak., 43, 121â€“123.

Complex Variables
51
If, however, the expression does not tend to a limit as Ç« and Î· tend to zero independently,
it may still happen that
lim
Ç«â†’0
(Z câˆ’Ç«
a
f(x) dx +
Z b
c+Ç«
f(x) dx
)
(1.10.3)
exists. When this is the case, we call this limit the Cauchy principal value of the improper
integral and denote it by
PV
Z b
a
f(x) dx.
(1.10.4)
Finally, if f(x) becomes inï¬nite at an endpoint, say a, of the range of integration, we say
that f(x) is integrable over a â‰¤x â‰¤b if
lim
Ç«â†’0+
Z b
a+Ç«
f(x) dx
(1.10.5)
exists.
â€¢ Example 1.10.1
Consider the integral
R 2
âˆ’1 dx/x.
This integral does not exist in the ordinary sense
because of the strong singularity at the origin. However, the integral would exist if
lim
Ç«â†’0
Z Ç«
âˆ’1
dx
x + lim
Î´â†’0
Z 2
Î´
dx
x
(1.10.6)
existed and had a unique value as Ç« and Î´ independently approach zero. Because this limit
equals
lim
Ç«,Î´â†’0 [ ln(Ç«) + ln(2) âˆ’ln(Î´)] = lim
Ç«,Î´â†’0 [ ln(2) âˆ’ln(Î´/Ç«)] ,
(1.10.7)
our integral would have the value of ln(2) if Î´ = Ç«. This particular limit is the Cauchy
principal value of the improper integral, which we express as
PV
Z 2
âˆ’1
dx
x = ln(2).
(1.10.8)
âŠ“âŠ”
We can extend these ideas to complex integrals used to determine the value or prin-
cipal value of an improper integral by Cauchyâ€™s residue theorem when the integrand has a
singularity on the contour of integration. We avoid this diï¬ƒculty by deleting from the area
within the contour, that portion which also lies within a small circle |z âˆ’c| = Ç«, and then
integrating around the boundary of the remaining region. This process is called indenting
the contour.
The integral around the indented contour is calculated by the theorem of residues
and then the radius of each indentation is made to tend to zero. This process gives the
Cauchy principal value of the improper integral. The details of this method are shown in
the following examples.

52
Advanced Engineering Mathematics: A Second Course
C
a
-a
C
-R
1
2
x
x
R
Î·
Îµ
Figure 1.10.1: Contour C used in Example 1.10.2.
â€¢ Example 1.10.2
Let us show that
PV
Z âˆ
âˆ’âˆ
cos(x)
a2 âˆ’x2 dx = Ï€ sin(a)
a
,
a > 0.
(1.10.9)
Consider the integral
I
C
eiz
a2 âˆ’z2 dz,
(1.10.10)
where the closed contour C consists of the real axis from âˆ’R to R and a semicircle in the
upper half of the z-plane where this segment is its diameter. See Figure 1.10.1. Because
the integrand has poles at z = Â±a, which lie on this contour, we modify C by making an
indentation of radius Ç« at a and another of radius Î· at âˆ’a. The integrand is now analytic
within and on C and Equation 1.10.10 equals zero by the Cauchy-Goursat theorem.
Evaluating each part of the integral, Equation 1.10.10, we have that
Z Ï€
0
eiR cos(Î¸)âˆ’R sin(Î¸)
a2 âˆ’R2e2Î¸i
iReÎ¸i dÎ¸ +
Z
C1
eiz
a2 âˆ’z2 dz +
Z
C2
eiz
a2 âˆ’z2 dz
+
Z âˆ’aâˆ’Î·
âˆ’R
eix
a2 âˆ’x2 dx +
Z aâˆ’Ç«
âˆ’a+Î·
eix
a2 âˆ’x2 dx +
Z R
aâˆ’Ç«
eix
a2 âˆ’x2 dx = 0,
(1.10.11)
where C1 and C2 denote the integrals around the indentations at a and âˆ’a, respectively.
The modulus of the ï¬rst term on the left side of Equation 1.10.11 is less than Ï€R/(R2 âˆ’a2),
so this term tends to zero as R â†’âˆ. To evaluate C1, we observe that z = a + Ç«eÎ¸i along
C1, where Î¸ decreases from Ï€ to 0. Hence,
Z
C1
eiz
a2 âˆ’z2 dz = lim
Ç«â†’0
Z 0
Ï€
exp
 ia + iÇ«eÎ¸i
Ç«ieÎ¸i
âˆ’2aÇ«eÎ¸i âˆ’Ç«2e2Î¸i dÎ¸
(1.10.12)
= lim
Ç«â†’0
Z Ï€
0
exp
 ia + iÇ«eÎ¸i
i
2a + Ç«eÎ¸i dÎ¸ = Ï€ieia
2a .
(1.10.13)
Similarly,
Z
C2
eiz
a2 âˆ’z2 dz = âˆ’Ï€ieâˆ’ia
2a
,
(1.10.14)
as Î· tends to zero.

Complex Variables
53
-R
x
1
Îµ
C
R
Figure 1.10.2: Contour C used in Example 1.10.3.
Upon letting R â†’âˆ, Ç« â†’0, and Î· â†’0, we ï¬nd that
PV
Z âˆ
âˆ’âˆ
eix
a2 âˆ’x2 dx = âˆ’Ï€i
2a
 eia âˆ’eâˆ’ia
= Ï€ sin(a)
a
.
(1.10.15)
Finally, equating the real and imaginary parts, we obtain
PV
Z âˆ
âˆ’âˆ
cos(x)
a2 âˆ’x2 dx = Ï€ sin(a)
a
,
PV
Z âˆ
âˆ’âˆ
sin(x)
a2 âˆ’x2 dx = 0.
(1.10.16)
âŠ“âŠ”
â€¢ Example 1.10.3
Let us show that
Z âˆ
âˆ’âˆ
sin(x)
x
dx = Ï€.
(1.10.17)
Consider the integral
I
C
eiz
z dz,
(1.10.18)
where the closed contour C consists of the real axis from âˆ’R to R and a semicircle in the
upper half of the z-plane where this segment is its diameter. Because the integrand has a
pole at z = 0, which lies on the contour, we modify C by making an indentation of radius
Ç« at z = 0. See Figure 1.10.2. Because eiz/z is analytic along C,
Z Ï€
0
eiR cos(Î¸)âˆ’R sin(Î¸)i dÎ¸ +
Z âˆ’Ç«
âˆ’R
eix
x dx +
Z
C1
eiz
z dz +
Z R
Ç«
eix
x dx = 0.
(1.10.19)
Since eâˆ’R sin(Î¸) < eâˆ’RÎ¸ for 0 < Î¸ < Ï€,

Z Ï€
0
eiR cos(Î¸)âˆ’R sin(Î¸)i dÎ¸
 â‰¤
Z Ï€
0
eâˆ’RÎ¸ dÎ¸ = 1 âˆ’eâˆ’Ï€R
R
,
(1.10.20)
which tends to zero as R â†’âˆ. Therefore,
Z âˆ’Ç«
âˆ’âˆ
eix
x dx +
Z âˆ
Ç«
eix
x dx = âˆ’
Z
C1
eiz
z dz.
(1.10.21)

54
Advanced Engineering Mathematics: A Second Course
Now,
Z
C1
eiz
z dz =
Z
C1
dz
z + i
Z
C1
dz âˆ’
Z
C1
z
2 dz + Â· Â· Â· = âˆ’Ï€i
(1.10.22)
in the limit Ç« â†’0 because z = Ç«eÎ¸i. Consequently, in the limit of Ç« â†’0,
PV
Z âˆ
âˆ’âˆ
eix
x dx = Ï€.
(1.10.23)
Upon separating the real and imaginary parts, we obtain
PV
Z âˆ
âˆ’âˆ
cos(x)
x
dx = 0,
Z âˆ
âˆ’âˆ
sin(x)
x
dx = Ï€.
(1.10.24)
Problems
1. Noting that
Z Î¸âˆ’Ç«
0
dÏ•
cos(Ï•) âˆ’cos(Î¸) =
1
sin(Î¸) ln

sin
 1
2(Î¸ + Ï•)

sin
 1
2(Î¸ âˆ’Ï•)


Î¸âˆ’Ç«
0
,
and
Z Ï€
Î¸+Ç«
dÏ•
cos(Ï•) âˆ’cos(Î¸) =
1
sin(Î¸) ln

sin
 1
2(Î¸ + Ï•)

sin
 1
2(Î¸ âˆ’Ï•)


Ï€
Î¸+Ç«
,
show that
PV
Z Ï€
0
dÏ•
cos(Ï•) âˆ’cos(Î¸) = 0,
0 < Î¸ < Ï€.
2. Show that
Z âˆ
âˆ’âˆ
cos(Ï€x/2)
x2 âˆ’1
dx = âˆ’Ï€.
Step 1: Show that
Z âˆ
âˆ’âˆ
cos(Ï€x/2)
x2 âˆ’1
dx = â„œ

PV
Z âˆ
âˆ’âˆ
eiÏ€x/2
x2 âˆ’1 dx

.
Step 2: Consider now the integral
H
C eiÏ€z/2 dz/(z2 âˆ’1), where the closed contour C consists
of the real axis from âˆ’R to R plus a semicircle of radius R in the upper half of the z-plane.
See Figure 1.10.1. Because the integrand has poles at z = Â±1 which lie on the contour, we
modify C by making an indentation of radius Î· above z = âˆ’1 and another indentation of
radius Ç« above z = 1. Why is this closed contour integral equal to zero?
Step 3: Show that this contour integral is given by
lim
Râ†’âˆ
Z Ï€
0
eiÏ€R cos(Î¸)/2âˆ’RÏ€ sin(Î¸)/2
R2e2Î¸i âˆ’1
iReÎ¸i dÎ¸ +
Z
C1
eiÏ€z/2
z2 âˆ’1 dz +
Z
C2
eiÏ€z/2
z2 âˆ’1 dz
+ lim
Î·â†’0
Z âˆ’1âˆ’Î·
âˆ’R
eiÏ€x/2
x2 âˆ’1 dx + lim
Ç«,Î·â†’0
Z 1âˆ’Ç«
âˆ’1+Î·
eiÏ€x/2
x2 âˆ’1 dx + lim
Ç«â†’0
Z R
1+Ç«
eiÏ€x/2
x2 âˆ’1 dx = 0,

Complex Variables
55
where C1 and C2 denote the integrals around the indentations at âˆ’1 and 1, respectively.
Step 4: Show that the ï¬rst term on the left side tends to zero as R â†’âˆ. Why?
Step 5: Taking z = âˆ’1 + Î·eÎ¸i along C1, where Î¸ decreases from Ï€ to 0, show that
Z
C1
eiÏ€z/2
z2 âˆ’1 dz = lim
Î·â†’0
Z 0
Ï€
exp

iÏ€(âˆ’1 + Î·eÎ¸i)/2

âˆ’2Î·eÎ¸i + Î·2e2Î¸i
iÎ·eÎ¸i dÎ¸ = Ï€
2 .
Step 6: Similarly, show that along C2,
Z
C2
eiÏ€z/2
z2 âˆ’1 dz = lim
Ç«â†’0
Z 0
Ï€
exp

iÏ€(1 + Ç«eÎ¸i)/2

2Ç«eÎ¸i + Ç«2e2Î¸i
iÇ«eÎ¸i dÎ¸ = Ï€
2 .
Step 7: Using Steps 1 through 6, obtain the ï¬nal result.
3. Show that
Z âˆ
âˆ’âˆ
eax âˆ’ebx
1 âˆ’ex
dx = Ï€[cot(aÏ€) âˆ’cot(bÏ€)],
0 < a, b < 1.
Step 1: Consider the integral
H
C
 eazâˆ’ebz
dz/(1âˆ’ez), where the closed contour C consists of
the rectangular box with vertices at (âˆ’R, 0), (R, 0), (âˆ’R, Ï€) and (R, Ï€), and a semicircular
indentation CÇ« at the origin. Show that this closed integral equals zero. Why?
Step 2: Show that this closed integral may be rewritten,
lim
Râ†’âˆ
Z âˆ’R+Ï€i
R+Ï€i
eaz âˆ’ebz
1 âˆ’ez
dz + lim
Râ†’âˆ
Z âˆ’R
âˆ’R+Ï€i
eaz âˆ’ebz
1 âˆ’ez
dz +
lim
Râ†’âˆ,Ç«â†’0
Z âˆ’Ç«
âˆ’R
eaz âˆ’ebz
1 âˆ’ez
dz
+
Z
CÇ«
eaz âˆ’ebz
1 âˆ’ez
dz +
lim
Râ†’âˆ,Ç«â†’0
Z R
Ç«
eaz âˆ’ebz
1 âˆ’ez
dz + lim
Râ†’âˆ
Z R+Ï€i
R
eaz âˆ’ebz
1 âˆ’ez
dz = 0.
Step 3: Show that
Z
CÇ«
eaz âˆ’ebz
1 âˆ’ez
dz = lim
Ç«â†’0
Z 0
Ï€
1 + aÇ«eÎ¸i + a2Ç«2e2Î¸i/2 + Â· Â· Â· âˆ’1 âˆ’bÇ«eÎ¸i âˆ’b2Ç«2e2Î¸i/2 âˆ’Â· Â· Â·
1 âˆ’1 âˆ’Ç«eÎ¸i âˆ’Ç«2e2Î¸i/2 âˆ’Â· Â· Â·
iÇ«eÎ¸i dÎ¸
= 0.
Step 4: Show that
lim
Râ†’âˆ
Z âˆ’R
âˆ’R+Ï€i
eaz âˆ’ebz
1 âˆ’ez
dz = lim
Râ†’âˆ
Z 0
Ï€
eâˆ’aReayi âˆ’eâˆ’bRebyi
1 âˆ’eâˆ’Reyi
i dy = 0,
and
lim
Râ†’âˆ
Z R+Ï€i
R
eaz âˆ’ebz
1 âˆ’ez
dz = lim
Râ†’âˆ
Z Ï€
0
eaReayi âˆ’ebRebyi
1 âˆ’eReyi
i dy = 0,
if 0 < a, b < 1.

56
Advanced Engineering Mathematics: A Second Course
Step 5: Show that
Z âˆ
âˆ’âˆ
eax âˆ’ebx
1 âˆ’ex
dx =
Z âˆ
âˆ’âˆ
eaxeaÏ€i
1 + ex dx âˆ’
Z âˆ
âˆ’âˆ
ebxebÏ€i
1 + ex dx
=
Ï€eaÏ€i
sin(aÏ€) âˆ’Ï€ebÏ€i
sin(bÏ€) = Ï€ [cot(aÏ€) + i] âˆ’Ï€ [cot(bÏ€) + i] .
4. Show13 that
Z âˆ
âˆ’âˆ
1 âˆ’cos[2a(x + Î¶)]
(x + Î¶)2(x2 + Î±2) dx =
Ï€
Î±(Î¶2 + Î±2)2

2aÎ±(Î¶2 + Î±2) + (Î¶2 âˆ’Î±2)
âˆ’eâˆ’2aÎ± 
(Î¶2 âˆ’Î±2) cos(2aÎ¶) + 2Î±Î¶ sin(2aÎ¶)
	
,
where a, Î±, and Î¶ are real.
Step 1: Show that
 Z
Câˆ
+
Z âˆ’Î¶âˆ’Ç«
âˆ’R
+
Z
CÇ«
+
Z R
âˆ’Î¶+Ç«
!
f(z) dz = 2Ï€i Res[f(z); iÎ±],
where Câˆdenotes the semicircular contour of inï¬nite radius, CÇ« is the semicircular inden-
tation above z = âˆ’Î¶ and
f(z) =
1 âˆ’e2ia(z+Î¶)
(z + Î¶)2(z2 + Î±2).
Step 2: Taking the limit of R â†’âˆand Ç« â†’0, show that
Z âˆ
âˆ’âˆ
f(x) dx = 2Ï€i Res[f(z); iÎ±] + Ï€i Res[f(z); âˆ’Î¶].
Step 3: Show that
Res[f(z); iÎ±] = Î¶2 âˆ’Î±2 âˆ’eâˆ’2aÎ±[(Î¶2 âˆ’Î±2) cos(2aÎ¶) + 2Î±Î¶ sin(2aÎ¶)]
2iÎ±(Î¶2 + Î±2)2
âˆ’2Î±Î¶ + eâˆ’2aÎ±[(Î¶2 âˆ’Î±2) sin(2aÎ¶) âˆ’2Î±Î¶ cos(2aÎ¶)]
2Î±(Î¶2 + Î±2)2
and
Res[f(z); âˆ’Î¶] = lim
zâ†’âˆ’Î¶
d
dz
1 âˆ’e2ia(z+Î¶)
z2 + Î±2

= âˆ’
2ia
Î¶2 + Î±2 .
Step 4: Use the results from Steps 1 through 3 to obtain the desired result.
5. Show that
PV
Z âˆ
âˆ’âˆ
cos(mx)
x âˆ’a
dx = âˆ’Ï€ sin(ma),
and
PV
Z âˆ
âˆ’âˆ
sin(mx)
x âˆ’a
dx = Ï€ cos(ma),
13 Ko, S. H., and A. H. Nuttall, 1991: Analytical evaluation of ï¬‚ush-mounted hydrophone array response
to the Corcos turbulent wall pressure spectrum. J. Acoust. Soc. Am., 90, 579â€“588.

Complex Variables
57
where m > 0 and a is real.
Step 1: Using the complex function eimz/(z âˆ’a) and a closed contour similar to that shown
in Figure 1.10.2, show that
 Z
Câˆ
+
Z aâˆ’Ç«
âˆ’R
+
Z
CÇ«
+
Z R
a+Ç«
!
eimz
z âˆ’a dz = 0.
Why? Here Câˆdenotes the semicircular contour of inï¬nite radius and CÇ« is the semicircular
indentation above z = a.
Step 2: Taking the limit of R â†’âˆand Ç« â†’0, show that
PV
Z âˆ
âˆ’âˆ
eimx
x âˆ’a dx = Ï€i Res
 eimz
z âˆ’a; a

= Ï€ieima.
Step 3: Complete the derivation by taking the real and imaginary parts of the equation in
Step 2.
6. Show that
PV
Z âˆ
âˆ’âˆ
xexi
x2 âˆ’Ï€2 dx = âˆ’Ï€i,
and
PV
Z âˆ
âˆ’âˆ
eimx
(x âˆ’1)(x âˆ’3) dx = Ï€i
2
 e3mi âˆ’emi
,
where m > 0.
Step 1: Show that
 Z
Câˆ
+
Z âˆ’Ï€âˆ’Ç«
âˆ’R
+
Z
CÇ«1
+
Z Ï€âˆ’Ç«
âˆ’Ï€+Ç«
+
Z
CÇ«2
+
Z R
Ï€+Ç«
!
zeiz
z2 âˆ’Ï€2 dz = 0
Why? Here Câˆdenotes the semicircular contour of inï¬nite radius and CÇ«1 and CÇ«2 are
semicircular indentations above z = âˆ’Ï€ and z = Ï€.
Step 2: Taking the limit of R â†’âˆand Ç« â†’0, show that
PV
Z âˆ
âˆ’âˆ
xeix
x2 âˆ’Ï€2 dx = Ï€i Res
 z eiz
z2 âˆ’Ï€2 ; âˆ’Ï€

+ Ï€i Res
 z eiz
z2 âˆ’Ï€2 ; Ï€

= 1
2Ï€ieâˆ’Ï€i + 1
2Ï€ieÏ€i = âˆ’Ï€i.
Step 3: To prove the second relationship, show that
 Z
Câˆ
+
Z 1âˆ’Ç«
âˆ’R
+
Z
CÇ«1
+
Z 3âˆ’Ç«
1+Ç«
+
Z
CÇ«2
+
Z R
3+Ç«
!
eimz
(z âˆ’1)(z âˆ’3) dz = 0.
Why? Here Câˆdenotes the semicircular contour of inï¬nite radius and CÇ«1 and CÇ«2 are
semicircular indentations above z = 1 and z = 3.
Step 4: Taking the limit of R â†’âˆand Ç« â†’0, show that
PV
Z âˆ
âˆ’âˆ
eimx
(x âˆ’1)(x âˆ’3) dx = Ï€i Res

eimz
(z âˆ’1)(z âˆ’3); 1

+ Ï€i Res

eimz
(z âˆ’1)(z âˆ’3); 3

= âˆ’1
2Ï€iemi + 1
2Ï€ie3mi = Ï€i
2
 e3mi âˆ’emi
.

58
Advanced Engineering Mathematics: A Second Course
7. Redo Example 1.10.3, except the contour is now a rectangle with vertices at Â±R and
Â±R + Ri indented at the origin.
Step 1: Show that along the left side,

Z âˆ’R+Ri
âˆ’R
eiz
z dz
 â‰¤
Z R
0
eâˆ’y
p
R2 + y2 dy < 1
R
Z R
0
eâˆ’y dy = 1
R
 1 âˆ’eâˆ’R
,
which tends to zero as R â†’âˆ.
Step 2: Show that along the top,

Z R+Ri
R
eiz
z dz
 â‰¤
Z R
0
eâˆ’y
p
R2 + y2 dy,
which also tends to zero as R â†’âˆ. Why?
Step 3: Show that along the right side,

Z R+Ri
âˆ’R+Ri
eiz
z dz
 â‰¤2eâˆ’R
Z R
0
dx
âˆš
R2 + x2 = 2 ln

1 +
âˆš
2

eâˆ’R,
which tends to zero as R â†’âˆ. Why?
Step 4: Just as in the case of the semicircle close contour, we only have an integration along
the real axis. Do this to complete the problem.
8. Let us show14 that
G(Î±) = PV
Z 1
âˆ’1
dx
(x + Î±)
âˆš
1 âˆ’x2 =
(
Î±Ï€
|Î±|
âˆš
Î±2 âˆ’1
,
|Î±| > 1,
0,
|Î±| < 1.
Step 1: Using the transformation 2ix = z âˆ’zâˆ’1, show that
2i dx = z2 + 1
z2
dz,
1 âˆ’x2 = 1
4

z + 1
z
2
,
p
1 âˆ’x2 = 1
2
z2 + 1
z

,
and
x + Î± = 1
2i

z âˆ’1
z

+ Î± = 1
2i
z2 + 2iÎ±z âˆ’1
z

.
Substitute these results into the original integral to ï¬nd G(Î±) as a contour integration on
the unit circle.
Step 2: For |Î±| < 1, we have two singularities within the contours located at z = Â±
âˆš
1 âˆ’Î±2âˆ’
Î±i. In that case, show that G(Î±) = 0.
Step 3: If Î± > 1, there is a single singularity within the contours and it is located at
z = i
âˆš
Î±2 âˆ’1 âˆ’Î±i. Show that G(Î±) = Ï€/
âˆš
Î±2 âˆ’1.
14 Ott, E., T. M. Antonsen, and R. V. Lovelace, 1977: Theory of foil-less diode generation of intense
relativistic electron beams. Phys. Fluids, 20, 1180â€“1184.

Complex Variables
59
Step 4: Finally, if Î± < âˆ’1, there is a single singularity within the contours and it is located
at z = âˆ’i
âˆš
Î±2 âˆ’1 âˆ’Î±i. Show that G(Î±) = âˆ’Ï€/
âˆš
Î±2 âˆ’1.
9. Let the function f(z) possess a simple pole with a residue Res[f(z); c] on a simply closed
contour C. If C is indented at c, show that the integral of f(z) around the indentation
tends to âˆ’Res[f(z); c]Î±i as the radius of the indentation tends to zero, Î± being the internal
angle between the two parts of C meeting at c.
1.11 CONFORMAL MAPPING
Conformal mapping is a powerful technique for ï¬nding solutions, or for simplifying
the process of ï¬nding solutions, to Laplaceâ€™s diï¬€erential equation in two dimensions. This
method involves introducing two complex variables: z = x + iy and Ï„ = Ï + iÏƒ. These
two complex variables are related to each other via the mapping z = f(Ï„). Under this
mapping the Argand diagram for the z-variable is mapped into one for the Ï„-variable. In
certain cases, for example Ï„ = âˆšz, the complex z-plane may only map into a portion of the
Ï„-plane. In other cases, say Ï„ = z + 3i, the complete z-plane would be mapped into the
complete Ï„-plane.
Once we map the original domain into a simpler geometry (a half-plane, circle or
square), how do we ï¬nd the solution? There are several techniques available. One method,
for example, recalls that the real and imaginary parts of an analytic function satisfy
Laplaceâ€™s equation. Therefore, if we could construct an analytic function whose real or
imaginary parts satisfy the boundary conditions in the new domain, we would have the
solution in the Ï„-plane. Then we could use the transformation to obtain the solution in the
original z-plane.
What types of functions f(z) are useful? Consider an arbitrary point z0 in the complex
z-plane. Assuming that f â€²(z0) Ì¸= 0, a straightforward transformation yields
âˆ‚2u
âˆ‚x2 + âˆ‚2u
âˆ‚y2

z0
= |f â€²(z0)|2
âˆ‚2v
âˆ‚Ï2 + âˆ‚2v
âˆ‚Ïƒ2

Ï„0
,
(1.11.1)
where u(x, y) and v(Ï, Ïƒ) are solutions to Laplaceâ€™s equation in the z and Ï„ planes, respec-
tively. Thus, f(z) must be analytic.
â€¢ Example 1.11.1
In their study of magnetic recording, Curland and Judy15 modeled the ring heads as
two semi-inï¬nite regions located below the x-axis and running to the right of x = a/2 and
to the left of x = âˆ’a/2. See Figure 1.11.1.
From symmetry we need only consider the half-space x > 0. Consequently, the new
boundary consists of the four line segments: AB, BC, CD and DE. If we require that
the point D in the Ï„-plane lies at Ï„ = 1, we shall show in Example 1.11.7 that the desired
conformal mapping is
z = a
Ï€
âˆš
Ï„ âˆ’1 âˆ’i
2 log
1 âˆ’iâˆšÏ„ âˆ’1
1 + iâˆšÏ„ âˆ’1

+ a
2.
(1.11.2)
15 Curland, N., and J. H. Judy, 1986: Calculation of exact ring head ï¬elds using conformal mapping.
IEEE Trans. Magnet., MAG-22, 1901â€“1903.

60
Advanced Engineering Mathematics: A Second Course
                                                                                                                  


                                                                                                                                                                                                                                                














                                                                                                                                                                                                                                                














               














zâˆ’plane
Ïƒ
Ï„âˆ’
Ï
plane
x
a/2
y
Ï„ = 1
                                          
                                          
              













                     




















      


      


              













              













A
D
E
B C
C
B
A
D
E
Figure 1.11.1: The conformal mapping used to ï¬nd the ï¬elds of a semi-inï¬nite ring head with a ï¬nite gap
of width a. The potential on the right pole face equals 1 while the potential of the left pole face equals âˆ’1.
In the z-plane the point A is located at (0, âˆ) while point B is located at (0, âˆ’âˆ). Because of symmetry
the potential along the center of the gap AB equals 0.
A useful method for illustrating this conformal mapping is to draw lines of constant Ï and
Ïƒ in the z-plane. See Figure 1.11.2. This ï¬gure shows the local orthogonality between lines
of constant Ï and Ïƒ.
The greatest diï¬ƒculty in creating this ï¬gure was computing Ï„ for a given z. This was
done using the Newton-Raphson method. Starting at the top of the domain, the ï¬rst guess
there was given by Ï„ = 1 + Ï€2z2. Marching downward, the Ï„ from the previous grid point
was used for the initial guess. The corresponding MATLAB script is as follows:
clear; delta = 0.01; % resolution of the grid
for jj = 1:201
for ii = 1:201
XX(jj,ii) = delta*ii; YY(jj,ii) = delta*(jj-101);
RHO(jj,ii) = NaN; SIGMA(jj,ii) = NaN;
end; end
% code for the domain x, y > 0
for jj = 1:100
y = 1 - delta*(jj-1);
for ii = 1:201
x = delta*ii; z = complex(x,y);
if (jj == 1) tau = 1+pi*pi*z*z; else tau = TAU(ii); end
for icount = 1:10
temp1 = sqrt(tau-1);
temp2 = temp1 - 0.5*i*log(1-i*temp1) + 0.5*i*log(1+i*temp1);
ff = temp2/pi + 0.5 - z; deriv = temp1 /(2*pi*tau);
temp3 = ff/deriv; tau = tau - temp3; % Newton-Raphson method
end
TAU(ii) = tau; RHO(202-jj,ii) = real(tau);
SIGMA(202-jj,ii) = imag(tau);
end; end

Complex Variables
61
0.002
0.05
0.5
0.5
0.5
0.5
5
5
5
10
10
20
20
30
-8
-4
0
0
0
4
4
10
20
30
x/a
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
y/a
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
Figure 1.11.2: Lines of constant Ï (dashed lines) and Ïƒ (solid lines) given by the conformal mapping
expressed by Equation 1.11.2.
% code for the domain 0 < x < 1
2 and y < 0
for jj = 1:101
y = delta - delta*jj;
for ii = 1:49
x = delta*ii; z = complex(x,y);
tau = TAU(ii); % first guess
for icount = 1:10
temp1 = sqrt(tau-1);
temp2 = temp1 - 0.5*i*log(1-i*temp1) + 0.5*i*log(1+i*temp1);
ff = temp2/pi + 0.5 - z; deriv = temp1 /(2*pi*tau);
temp3 = ff/deriv; tau = tau - temp3; % Newton-Raphson method
end
TAU(ii) = tau; RHO(102-jj,ii) = real(tau);
SIGMA(102-jj,ii) = imag(tau);
end; end
% plot the conformal mapping Equation 1.11.2
figure
[C,h] = contour(XX,YY,SIGMA,[0.002,0.05,0.5,5,10,20,30],â€™kâ€™);
clabel(C,h,â€™FontSizeâ€™,10,â€™Colorâ€™,â€™kâ€™,â€™Rotationâ€™,0)
xlabel(â€™x/aâ€™,â€™FontSizeâ€™,20); ylabel(â€™y/aâ€™,â€™FontSizeâ€™,20);
hold on
v = [-8,-4,0,4,10,20,30];
[C,h] = contour(XX,YY,RHO,v,â€™--bâ€™);
clabel(C,h,â€™FontSizeâ€™,10,â€™Colorâ€™,â€™bâ€™,â€™Rotationâ€™,0)

62
Advanced Engineering Mathematics: A Second Course
0.1
0.1
0.3
0.3
0.5
0.5
0.5
0.7
0.7
0.7
0.9
0.9
0.9
x/a
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
y/a
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
Figure 1.11.3: The solution to Laplaceâ€™s equation when the left boundary is held at 0 while the left and
top sides of the shaded rectangle are held at 1. This ï¬gure shows only a portion of the domain x > 0 and
|y| < âˆ.
Now that we can transform between the z-plane and the Ï„-plane, and vice versa, let us
turn our attention to ï¬nding the solution to Laplaceâ€™s equation in the Ï„-plane. There the
solution equals 1 for Ï > 0 and 0 for Ï < 0 along Ïƒ = 0.
Consider now the analytic function (except at the branch point Ï„ = 0)
f(Ï„) = i âˆ’log(Ï„)/Ï€.
(1.11.3)
A quick check (using Ï„ = reiÎ¸) shows that the imaginary part of f(Ï„), v(r, Î¸) = 1 âˆ’Î¸/Ï€,
satisï¬es Laplaceâ€™s equation and the boundary conditions. Thus, constructing the solution is
as follows: For a given x and y, we use our MATLAB code to compute Ï„. Substituting that
Ï„ into Equation 1.11.3 we compute f(Ï„). Taking the imaginary part, we have the solution
at x and y. Figure 1.11.3 illustrates the solution for the domain 0 < x < 2 and âˆ’1 < y < 1.
âŠ“âŠ”
In summary, conformal mapping allowed us to transform the original domain into one
(an upper half-plane) where we could construct another analytic function whose imaginary
part satisï¬ed Laplaceâ€™s equation and the boundary conditions. A natural question is what
do we do if we cannot ï¬nd this analytic function in the Ï„-plane? The next example shows
an alternative approach.
â€¢ Example 1.11.2
For our second example of conformal mapping, consider Ï„ =
âˆš
z2 + a2. To illustrate
this mapping we have constructed two Argand diagrams; one is for the z-plane while the
second is for the Ï„-plane. Figure 1.11.4 shows how a particular boundary in the z-plane
maps into the Ï„-plane. The advantage here is that the inï¬nitely thin ï¬lament or peg located
at z = 0 is completely eliminated in the Ï„-plane.
One source of concern is the presence of the square root; for any value of z we would
have two possible solutions. We make the mapping unique by requiring that â„‘(Ï„) â‰¥0.

Complex Variables
63
                                                                                                                  


                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        













x
zâˆ’plane
Ïƒ
Ï„âˆ’plane
y
Ï
âˆ’a
a
0
z2= ai
Î±
Î±
Î±
1 
3
2
= 0
= 0
z1 
z3
+
_
Ï  = 
Ï  = 
Ï  = 
1 
2
3
                                          
                                          
                     




















      


      


      


      


      


      


Figure 1.11.4: The conformal mapping between the z-plane and Ï„-plane achieved by the conformal map-
ping Ï„ =
âˆš
z2 + a2.
To better understand this transformation, Figure 1.11.5 illustrates various lines of
constant â„œ(Ï„/a) and â„‘(Ï„/a) as a function of x/a and y/a. This ï¬gure was constructed
using the MATLAB code:
clear;
% compute Ï„ for various values of z
for jj = 1:40
y = 0.05 * jj;
for ii = 1:42
x = 0.05 * (ii-21.5); z = x + i*y; tau(ii,jj) = sqrt(z*z+a*a);
if (imag(tau(ii,jj)) <= 0) tau(ii,jj) = -tau(ii,jj); end
X(ii,jj) = x; Y(ii,jj) = y;
IM(ii,jj) = imag(tau(ii,jj)); REAL(ii,jj) = real(tau(ii,jj));
end; end
% plot the conformal mapping Equation Ï„ =
âˆš
z2 + a2
figure
[C,h] = contour(X,Y,IM,[0.1,0.25,0.5,0.75,1,1.5,2],â€™kâ€™);
clabel(C,â€™FontSizeâ€™,10,â€™Colorâ€™,â€™kâ€™,â€™Rotationâ€™,0)
xlabel(â€™xâ€™,â€™FontSizeâ€™,20); ylabel(â€™yâ€™,â€™FontSizeâ€™,20);
hold on
v = [-1,-0.5,-0.25,-0.01,0.01,0.25,0.5,1];
[C,h] = contour(X,Y,REAL,v,â€™--bâ€™);
clabel(C,â€™manualâ€™,â€™FontSizeâ€™,10,â€™Colorâ€™,â€™bâ€™,â€™Rotationâ€™,0)
As y â†’âˆ, lines of constant â„‘(Ï„/a) become parallel to the boundary y = 0. Only for
smaller values of y, and as we approach the peg at x = 0, do these lines deviate strongly

64
Advanced Engineering Mathematics: A Second Course
0.1
0.25
0.5
0.75
1
1.5
-1
-0.5
-0.25
0.25
0.5
1
x/a
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
y/a
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
Figure 1.11.5: Lines of constant â„œ(Ï„/a) (dashed line) and â„‘(Ï„/a) (solid lines) as a function of x and y
for the conformal mapping Ï„ =
âˆš
z2 + a2.
from the horizontal as they pass over the obstacle. The smaller the value of â„‘(Ï„/a) the
more they conform to the shape of the obstacle.
The behavior of lines of constant â„œ(Ï„/a) are more diï¬ƒcult to understand. There are
two general classes, depending upon whether the absolute value of â„œ(Ï„/a) is less or greater
than 1. When |â„œ(Ï„/a)| > 1 they are clearly orthogonal to constant lines of â„‘(Ï„/a). Positive
values of â„œ(Ï„/a) exist for x > 0 while negative values occur when x < 0. |â„œ(Ï„/a)| < 1 for
y â‰¥a.
This example has two interesting aspects to it. The ï¬rst is the presence of the square
root. The second involves how we will ï¬nd the solution to Laplaceâ€™s equation in the Ï„-plane.
Let us assume that in the original z-plane the solution equals zero along the entire
boundary except along the â€œpeg.â€ There, the solution equals 1. In the Ï„-plane the solution
equals zero along the entire boundary except for the segment âˆ’a < Ï < a, where Ïƒ = 0,
along which the solution equals 1. Instead of ï¬nding an analytic function whose real or
imaginary part satisï¬es this boundary condition, we employ Poissonâ€™s integral formula16
for the half-plane y > 0 or Schwarz integral formula:17
u(x, y) = 1
Ï€
Z âˆ
âˆ’âˆ
yf(t)
(x âˆ’t)2 + y2 dt.
In the present case, we ï¬nd that
u(Ï, Ïƒ) = 1
Ï€
Z a
âˆ’a
Ïƒ
Ïƒ2 + (Î¾ âˆ’Ï)2 dÎ¾
(1.11.4)
= 1
Ï€

tanâˆ’1
a âˆ’Ï
Ïƒ

+ tanâˆ’1
a + Ï
Ïƒ

.
(1.11.5)
16
Poisson, S. D., 1823: Suite du mÂ´emoire sur les intÂ´egrales dÂ´eï¬nies et sur la sommation des sÂ´eries. J.
Â´Ecole Polytech., 19, 404â€“509. See pg. 462.
17 Schwarz, H. A., 1870: Â¨Uber die Integration der partiellen Diï¬€erentialgleichungâˆ‚2u/âˆ‚x2 +âˆ‚2u/âˆ‚y2 = 0
fÂ¨ur die FlÂ¨ache eines Kreises. Vierteljahrsschr. Naturforsch. Ges. ZÂ¨urich, 15, 113â€“128.

Complex Variables
65
0.05
0.05
0.2
0.2
0.4
0.6
0.8
x
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
y
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
Figure 1.11.6: The solution of Laplaceâ€™s equation when the solution (potential) along the boundary equals
zero except along the peg located at x = 0. There the solution (potential) equals one.
Given Equation 1.11.5 we can compute the solution as follows: For a speciï¬c value of
x and y, we ï¬nd the corresponding value of Ï and Ïƒ. Equation 1.11.5 gives us the solution
to Laplaceâ€™s equation at that point and the corresponding x and y. The MATLAB code is:
clear; a = 1;
for jj = 1:100
y = 0.02 * jj;
for ii = 1:202
x = 0.02 * (ii-101.5); z = x + i*y; tau = sqrt(z*z+a*a);
if (imag(tau) <= 0) tau = -tau; end
sigma = imag(tau); rho = real(tau);
X(ii,jj) = x; Y(ii,jj) = y;
% Equation 1.11.5
arg1 = (a-rho)/sigma; arg2 = (a+rho)/sigma;
T(ii,jj) = (atan(arg1)+atan(arg2)) / pi;
end; end
% plot the solution to Laplaceâ€™s equation
figure
[C,h] = contourf(X,Y,T,[0,0.05,0.2,0.4,0.6,0.8],â€™kâ€™);
colormap autumn
clabel(C,â€™FontSizeâ€™,10,â€™Colorâ€™,â€™kâ€™,â€™Rotationâ€™,0)
xlabel(â€™xâ€™,â€™FontSizeâ€™,20); ylabel(â€™yâ€™,â€™FontSizeâ€™,20);
Figure 1.11.6 illustrates this solution.
âŠ“âŠ”
So far we have not presented a strategy for ï¬nding our conformal mappings.
One
method would be to simply experiment with transforms that had been used in similar

66
Advanced Engineering Mathematics: A Second Course
problems. Fortunately, during the 1860s, two German mathematicians, E. B. Christoï¬€el18
(1829â€“1900) and H. A. Schwarz19 (1843â€“1921), developed a very popular method of mapping
a polygon into a half plane. Example 1.11.1 illustrated one of their transforms. Indeed, if
we imagine that the boundary of the polygon is constructed from a thin wire, the purpose
of the Schwarz-Christoï¬€el transformation is to unbend the corners so that the wire becomes
straight.
Our derivation begins by considering a mapping z = f(Ï„) where
dz
dÏ„ = C(Ï„ âˆ’Ï1)k1(Ï„ âˆ’Ï2)k2 Â· Â· Â· (Ï„ âˆ’Ïn)kn,
(1.11.6)
and Ï1, Ï2, . . . , Ïn are any n points arranged in order along the real axis in the Ï„-plane such
that Ï1 < Ï2 < . . . < Ïn. Here the kiâ€™s are real constants and C is a real or complex
constant. By taking the logarithm of both sides of Equation 1.11.6 we ï¬nd that
log
dz
dÏ„

= log(C) + k1 log(Ï„ âˆ’Ï1) + k2 log(Ï„ âˆ’Ï2) + Â· Â· Â· + kn log(Ï„ âˆ’Ïn).
(1.11.7)
We have assumed that the principal value20 of each logarithm is taken. The local magniï¬-
cation factor of the mapping from the Ï„-plane to the z-plane equals dz/dÏ„, while the angle
of dz/dÏ„ gives the angle through which a small portion of the mapped curve in the Ï„-plane
is rotated by the mapping. This angle is given by
Ì¸ )
dz
dÏ„

= Ì¸ ) (C) + k1Ì¸ ) (Ï„ âˆ’Ï1) + k2Ì¸ ) (Ï„ âˆ’Ï2) + Â· Â· Â· + knÌ¸ ) (Ï„ âˆ’Ïn).
(1.11.8)
Equation 1.11.8 follows by ï¬rst taking the imaginary part of Equation 1.11.7 and then
noting that Ì¸ ) (C) = â„‘[log(C)].
Let the point (Ï, Ïƒ) = (âˆ’âˆ, 0) in the Ï„-plane be mapped into the point zâˆ—in the
z-plane. See Figure 1.11.7. If we consider the image of a point Ï as it moves to the right
along the negative real axis in the Ï„-plane, then all of the Ï âˆ’Ïi are real and negative as
long as Ï < Ï1. Hence the angles for all of the Ïâˆ’Ïi are constant and equal to Ï€ in Equation
1.11.8. Therefore, this equation simpliï¬es to
Ì¸ )
dz
dÏ„

= Ì¸ ) (C) + (k1 + k2 + Â· Â· Â· + kn)Ï€.
(1.11.9)
Thus the portion of the Ï axis to the left of the point Ï1 is mapped into a straight line
segment, making the angle deï¬ned by Equation 1.11.9 with the real axis in the z-plane, and
extending from zâˆ—to z1 the image of Ï âˆ’Ï1.
Now as the point Ï crosses the point Ï1 on the real axis, the real number Ïâˆ’Ï1 becomes
positive so that its angle abruptly changes from Ï€ to 0. Hence Ì¸ ) (dz/dÏ„) abruptly decreases
by an amount k1Ï€ and then remains constant as Ï„ travels from Ï1 to Ï2. It follows that
18 Christoï¬€el, E. B., 1868: Sul problema delle temperature stazionarie e la rappresentazione di una data
superï¬cie.
Ann.
Mat.
Pura Appl., Series 2, 1, 89â€“103; Christoï¬€el, E. B., 1870: Sopra un problema
proposto da Dirichlet. Ann. Mat. Pura Appl., Series 2, 4, 1â€“9.
19 Schwarz, H. A., 1868: Â¨Uber einige Abbildungsaufgaben. J. Reine Angew. Math., 70, 105â€“120.
20 For the complex number z = reÎ¸i, r Ì¸= 0, the principal value of the logarithm is log(z) = ln(r) + Î¸i,
where Î¸ must lie between 0 and 2Ï€.

Complex Variables
67
                                                                                                                                                                                                                                                                                                                                                               












                                                                            

 
x
zâˆ’plane
Ïƒ
Ï„âˆ’
Ï
plane
y
1 
2
n
Î±
Î±
1 
2
z
z1 
2z
*
Ï
Ï
Ï
              













                     




















      


      


      


      


      


      


              













              













Figure 1.11.7: Diagram used in the derivation of the Schwarz-Christoï¬€el method.
the image of the segment (Ï1Ï2) in the z-plane makes an angle of âˆ’k1Ï€ with the segment
(zâˆ—z1).
Proceeding in this way, we see that each segment (Ïn, Ïn+1) is mapped into a line
segment (zn, zn+1) in the z-plane, making the angle of âˆ’knÏ€ with the segment previously
mapped. Thus, if the interior angle of the resultant polynomial contour at the point zn is
to have the magnitude Î±n, we must set Ï€ âˆ’Î±n = âˆ’knÏ€, or kn = Î±n/Ï€ âˆ’1 in Equation
1.11.6. After an integration, we then conclude that the mapping
z = C
Z Ï„
(Î· âˆ’Ï1)k1(Î· âˆ’Ï2)k2 Â· Â· Â· (Î· âˆ’Ïn)kn dÎ· + K,
(1.11.10)
where the arbitrary complex constants C and K map the real axis Ïƒ = 0 of the Ï„-plane into
a polynomial boundary in the z-plane in such a way that the vertices z1, z2, . . . , zn with
interior angles Î±1, Î±2, . . . , Î±n are the images of the points Ï1, Ï2, . . . , Ïn.
For the ï¬nal segment Ï„ âˆ’Ï > Ïn the numbers Ï„ âˆ’Ïi are all real, positive, and equal to
zero, so that this segment is rotated through the angle
Ì¸ )(dz/dÏ„) = Ì¸ ) (C),
Ï > Ïn.
(1.11.11)
For a closed polynomial the sum of the interior angles is
Î±1 + Î±2 + Â· Â· Â· + Î±n = (n âˆ’2)Ï€.
(1.11.12)
Therefore,
k1 + k2 + Â· Â· Â· + kn = (n âˆ’2)Ï€
Ï€
âˆ’n = âˆ’2.
(1.11.13)
Thus, according to Equations 1.11.8 and 1.11.11, the two inï¬nite segments of the line Ïƒ = 0
are rotated through the angle Ì¸ ) (C) âˆ’2Ï€ and Ì¸ ) (C), as is clearly necessary for a closed
ï¬gure.
What roles do C and K play? Because C is often complex, this constant introduces
any necessary magniï¬cation and rotation of the transformation so that any prescribed
polynomial in the z-plane is made to correspond point by point to the real axis Ïƒ = 0 in

68
Advanced Engineering Mathematics: A Second Course
                                                                                    


                                                                                                                                                                                                                                                                                                                           














                                                                                                                                                      














  

 
 
  

                                                                                                                  


x
zâˆ’plane
Ïƒ
Ï„âˆ’
Ï
plane
1
y
Î±
Î²
C
A
B
C
A
B
z1 
= 1
2
z
0
C
= 0
Ï  = 
Ï  = 
1 
2
                                         
                                         
              













                     




















      


      


      


      


              













              













Figure 1.11.8: The complex z- and Ï„-planes used in Example 1.11.4.
the Ï„-plane. In fact, this correspondence can be set up in inï¬nitely many ways, in that
three of the numbers Ï1, Ï2, . . . , Ïn can be determined arbitrarily. Finally, the mapping can
be shown to establish a one-to-one correspondence between points in the interior of the
polygon in the z-plane and points in the upper half of the Ï„-plane.
â€¢ Example 1.11.3
Let us derive the conformal mapping used in Example 1.11.2. Referring back to Figure
1.11.4, we see that Î±1 = Ï€/2, k1 = âˆ’1/2, and Ï1 = âˆ’a at z1 = 0âˆ’; Î±2 = 2Ï€, k2 = 1, and
Ï2 = 0 at z2 = ai; and Î±3 = Ï€/2, k3 = âˆ’1/2, and Ï3 = a at z3 = 0+. Therefore, from
Equation 1.11.6,
dz
dÏ„ = C(Ï„ + a)âˆ’1/2Ï„(Ï„ âˆ’a)âˆ’1/2 = C
Ï„
âˆš
Ï„ 2 âˆ’a2 .
(1.11.14)
Integrating this diï¬€erential equation,
z = C
p
Ï„ 2 âˆ’a2 + K.
(1.11.15)
Because the point Ï1 = âˆ’a corresponds to z = 0âˆ’, K = 0. Similarly, at Ï2 = 0, we have
that
ai = C
p
âˆ’a2,
or
C = 1.
(1.11.16)
Therefore, the conformal mapping is given by z =
âˆš
Ï„ 2 âˆ’a2, or Ï„ =
âˆš
z2 + a2.
âŠ“âŠ”
â€¢ Example 1.11.4
Consider the triangle ABC located in the z-plane as shown on Figure 1.11.8. Here
we desire to map the interior space of this triangle into the upper half of the Ï„-plane. At
point C, points along the boundary and to the left of C are to be mapped out to âˆ’âˆin
the Ï„-plane while points along the boundary and to the right of C are mapped to +âˆ.
From Equation 1.11.6 we have that
dz
dÏ„ = Câ€²Ï„ Î±/Ï€âˆ’1(Ï„ âˆ’1)Î²/Ï€âˆ’1 = CÏ„ Î±/Ï€âˆ’1(1 âˆ’Ï„)Î²/Ï€âˆ’1.
(1.11.17)

Complex Variables
69
                        


                                                                                                                                                                                                                                                                                                                                                               












                        


A
E
                                                                            

 
 
  

  

 
x
zâˆ’plane
Ïƒ
Ï„âˆ’plane
y
Î±
Î±
z
z1 
z2
3
= âˆ’a
= 0
C
B
D
1
âˆ’1
E
Ï
C
D
B
A
0
= a
Ï  = 
Ï  = 
Ï  = 
1 
2
3
                                          
                                          
                     




















      


      


      


      


      


      


              













              













Figure 1.11.9: The complex z- and Ï„-planes used in Example 1.11.5.
Integrating this diï¬€erential equation,
z = C
Z Ï„
Î·Î±/Ï€âˆ’1(1 âˆ’Î·)Î²/Ï€âˆ’1 dÎ· + K.
(1.11.18)
Because we want the points Ï„ = 0 and z = 0 to correspond to each other, K = 0. On the
other hand, if we wish Ï„ = 1 and z = 1 to correspond, Equation 1.11.18 yields
C
Z 1
0
Î·Î±/Ï€âˆ’1(1 âˆ’Î·)Î²/Ï€âˆ’1 dÎ· = C Î“(Î±/Ï€)Î“(Î²/Ï€)
Î“[(Î± + Î²)/Ï€] = 1,
(1.11.19)
where Î“(Â·) is the gamma function deï¬ned by
Î“(x) =
Z âˆ
0
txâˆ’1eâˆ’t dt.
(1.11.20)
Consequently,
C = Î“[(Î± + Î²)/Ï€]
Î“(Î±/Ï€)Î“(Î²/Ï€),
(1.11.21)
and
z = Î“[(Î± + Î²)/Ï€]
Î“(Î±/Ï€)Î“(Î²/Ï€)
Z Ï„
0
Î·Î±/Ï€âˆ’1(1 âˆ’Î·)Î²/Ï€âˆ’1 dÎ·.
(1.11.22)
A noteworthy aspect of this example is that the conformal mapping is given by an integral
and not some analytic expression.
âŠ“âŠ”
â€¢ Example 1.11.5
Consider the domain lying in the upper half of the z-plane except for a triangular section
BCD shown in Figure 1.11.9. We wish to construct the Schwarz-Christoï¬€el transformation
that maps this domain into the upper half of the Ï„-plane. From Equation 1.11.6 we have
that
dz
dÏ„ = Câ€²(Ï„ + 1)(Ï€âˆ’Î±)/Ï€âˆ’1Ï„ (Ï€+2Î±)/Ï€âˆ’1(Ï„ âˆ’1)(Ï€âˆ’Î±)/Ï€âˆ’1
(1.11.23)
= Câ€²
Ï„ 2Î±/Ï€
(Ï„ 2 âˆ’1)Î±/Ï€ = C
Ï„ 2Î±/Ï€
(1 âˆ’Ï„ 2)Î±/Ï€ .
(1.11.24)

70
Advanced Engineering Mathematics: A Second Course
                                      


















                                                                                 


                      










                                          


            





                                                


F
F
A
E
                                                                                                                  


 
 
 
 
 
 
  

x
zâˆ’plane
Ïƒ
Ï„âˆ’plane
y
Ï
1 
âˆ’1
âˆ’a
a
1
B
C
D
C
D
E
A
B
A
0
Ï  = 
Ï  = 
Ï  = 
Ï  = 
2
3
4
5
Ï  = 
                                          
                                           
      


      


      


      


      


      


      


      


      


      


              













Figure 1.11.10: The complex z- and Ï„-planes used in Example 1.11.6 with a < 1.
Integrating this diï¬€erential equation,
z = C
Z Ï„
0
Î·2Î±/Ï€
(1 âˆ’Î·2)Î±/Ï€ dÎ· + K.
(1.11.25)
If we want the point Ï„ = 0 to correspond to the point z = ki, then K = ki. On the other
hand, if the point Ï„ = 1 corresponds to z = a, then
a = C
Z 1
0
Î·2Î±/Ï€
(1 âˆ’Î·2)Î±/Ï€ dÎ· + ki.
(1.11.26)
Solving for C,
C =
âˆšÏ€(a âˆ’ki)
Î“
 Î±/Ï€ + 1
2

Î“(1 âˆ’Î±/Ï€).
(1.11.27)
Therefore, the ï¬nal answer is
z =
âˆšÏ€(a âˆ’ki)
Î“
 Î±/Ï€ + 1
2

Î“(1 âˆ’Î±/Ï€)
Z Ï„
0
Î·2Î±/Ï€
(1 âˆ’Î·2)Î±/Ï€ dÎ· + ki.
(1.11.28)
âŠ“âŠ”
â€¢ Example 1.11.6
Consider the domain within the L-shaped boundary shown in Figure 1.11.10. We wish
to construct the Schwarz-Christoï¬€el transform that maps the interior into the upper half
of the Ï„-plane. Note that we broke the boundary in such a manner that points slightly to
the left of point A are mapped to âˆ’âˆwhile points slightly below the point A are mapped
to +âˆ.
Because a < 1, Equation 1.11.6 gives
dz
dÏ„ = C(Ï„ + 1)âˆ’1/2(Ï„ + a)âˆ’1/2Ï„ âˆ’1/2(Ï„ âˆ’a)âˆ’1/2(Ï„ âˆ’1)1/2.
(1.11.29)

Complex Variables
71
                    









                                                      


























                        

                    

ai
                                      

Ïƒ
plane
Ï„âˆ’
Ï
1
y
x
plane
zâˆ’
                      
                      





















      


      


      


                       






















A
E
D
C
B
A
D
E
B
C
Figure 1.11.11: The complex z- and Ï„-planes used in Example 1.11.7.
Integrating this diï¬€erential equation,
z = C
Z Ï„
0
(Î· âˆ’1) dÎ·
Î·
p
(Î·2 âˆ’1)(Î·2 âˆ’a2)
+ K = C
a
Z Ï„
0
(Î· âˆ’1) dÎ·
Î·
p
(1 âˆ’Î·2)(1 âˆ’p2Î·2)
+ K,
(1.11.30)
where p2 = 1/a2. To compute C and K, we would need further information.
âŠ“âŠ”
â€¢ Example 1.11.7
Let us derive the conformal mapping, Equation 1.11.2, used in Example 1.11.1. The
zâˆ’and Ï„âˆ’planes are shown in Figure 1.11.11. From this ï¬gure we see that Î±1 = 3Ï€/2,
Î±2 = Ï€/2, Î±3 = Ï€/2, Ï1 = 1, Ï2 = 0âˆ’, and Ï3 = 0+. This yields
dz
dÏ„ = K(Ï„ âˆ’1)(3Ï€)/(2Ï€)âˆ’1(Ï„ âˆ’0âˆ’)(Ï€)/(2Ï€)âˆ’1(Ï„ âˆ’0+)(Ï€)/(2Ï€)âˆ’1 = K
âˆšÏ„ âˆ’1
Ï„
.
(1.11.31)
Integrating Equation 1.11.31, we ï¬nd that
z = 2K
âˆš
Ï„ âˆ’1 âˆ’arctan
 âˆš
Ï„ âˆ’1

+ C = 2K
âˆš
Ï„ âˆ’1 + i
2 log
1 + iâˆšÏ„ âˆ’1
1 âˆ’iâˆšÏ„ âˆ’1

+ C.
(1.11.32)
Because at Ï„ = 1, z = a/2, we have C = a/2.
The computation of K is more complicated. Referring to Figure 1.11.11, we note that
Z C
B
dz =
Z Câ€²
Bâ€² K
âˆšÏ„ âˆ’1
Ï„
dÏ„.
(1.11.33)
Setting Ï„ = r eÎ¸i with r â†’0, Equation 1.11.34 becomes
a
2 = K lim
râ†’0
Z 0
Ï€
âˆš
r eÎ¸i âˆ’1
r eÎ¸i
ir eÎ¸i dÎ¸ = KÏ€.
(1.11.34)
Thus K = a/(2Ï€) and we recover Equation 1.11.2.

72
Advanced Engineering Mathematics: A Second Course
                                      

                            













                                                               


Ïƒ
plane
Ï„âˆ’
Ï
x
y
plane
zâˆ’
1 
2
3
(âˆ’Ï€,Ï€)
Ï
Ï
Ï
                     
                     
                      





















      


      


        



      


                       






















A
C
B
C
A
B
Problem 3
                                                                                                                                                                                                                                                            

















                                      

                                                


Ïƒ
plane
Ï„âˆ’
Ï
1 
2
3
y
x
plane
zâˆ’
Ï
Ï
Ï
                      
                      
                      





















      


      


        



B
C
A
B
C
A
Problem 4
                                                                                                                                                                                                   












                    

                                        

Ïƒ
plane
Ï„âˆ’
Ï
y
x
1
1
plane
zâˆ’
7Ï€/4
                      
      


      


C
D
B
A
A
B
C
D
Problem 5
Problems
1. Verify that the function Ï„ = ez maps the strip 0 < â„‘(z) < Ï€ into the half-plane â„‘(Ï„) > 0.
2. Verify that the function Ï„ 2 = 1 âˆ’ez maps the strip âˆ’Ï€ < â„‘(z) < Ï€, except for the
negative real axis, into the upper half of the Ï„-plane.
3. Use the Schwarz-Christoï¬€el method to ï¬nd the conformal mapping that maps the quarter
plane x > âˆ’Ï€, y < Ï€ into the upper half of the Ï„-plane. We require that the point (âˆ’Ï€, Ï€)
in the z-plane maps to the point (0, 0) in the Ï„-plane.
4. Use the Schwarz-Christoï¬€el method to ï¬nd the conformal mapping that maps the sector
lying between the x-axis and the line Î¸ = Ï€/3 into the upper half of the Ï„-plane. We require
that the point (0, 0) in the z-plane maps to the point (0, 0) in the Ï„-plane.
5. Use the Schwarz-Christoï¬€el method to ï¬nd the conformal mapping that maps the portion
of the z-plane deï¬ned by 0 < r < âˆ, 0 < Î¸ < 7Ï€/4 into the upper half of the Ï„-plane. We

Complex Variables
73
                                      


















                                      

                                                         


                                      


















Ïƒ
1
âˆ’1
plane
Ï„âˆ’
Ï
x
y
plane
zâˆ’
Ï  = 
Ï  = 
2
1 
                     
                     
                   


















      


      


                       






















B
C
B
C
a
âˆ’a
Problem 6
                        











                                                               


                                                         


                                      

y
zâˆ’plane
x
Ïƒ
1 
1
2
âˆ’1
plane
Ï„âˆ’
Ï
Ï  = 
Ï  = 
                     
                     
                      





















      


      


                       






















a B
C
B
C
Problem 7
                    









                        

                              


ai
                                                         


          
Ïƒ
plane
Ï„âˆ’
Ï
1
y
x
plane
zâˆ’
                     
                     
                      





















      


      


C
D
A
B
C
D
A
B
Problem 8
require that the points (0, 0) and (1, 0) in the z-plane map to the points (0, 0) and (1, 0) in
the Ï„-plane, respectively.
6. Use the Schwarz-Christoï¬€el method to ï¬nd the conformal mapping that maps the domain
|x| < a, 0 < y into the upper half of the Ï„-plane. Let the point (âˆ’a, 0) become the point
(âˆ’1, 0) while the point (a, 0) becomes the point (1, 0).
7. Use the Schwarz-Christoï¬€el method to ï¬nd the conformal mapping that maps the region
x > 0, 0 < y < a into the upper half of the Ï„-plane. We require that the point (0, a) maps
to (âˆ’1, 0) in the Ï„-plane while the point (0, 0) maps to (1, 0) in the Ï„-plane.
8. Use the Schwarz-Christoï¬€el method to ï¬nd the conformal mapping that maps the region
shown in the ï¬gure into the upper half of the Ï„-plane. We require that the points (0, 0) and
(0, a) in the z-plane map to the points (0, 0) and (1, 0) in the Ï„-plane, respectively.
9. Construct a transform between a z-plane which has a barrier that runs parallel to the
x-axis from z = L + Ï€L i to âˆ+ Ï€L i and a Ï„-plane that has no barrier.

74
Advanced Engineering Mathematics: A Second Course
                                                                                                                                                        







                                                                                                                              






                                                                            

                                                                                             


  

  

  

  

x
zâˆ’plane
Ïƒ
Ï„âˆ’
Ï
plane
y
A
B
C
D
A
B
C
D
Î±
              













              













                                           
                     




















              













              













1
Problem 9a
Step 1: Begin by using the Schwarz-Christoffel method to show that the conformal mapping
pictured in Figure 9a is given by
dz
dÏ„ = CÏ„ k1(Ï„ âˆ’1)k2,
where k1 = âˆ’Î±/(2Ï€) and k2 = Î±/Ï€ âˆ’1.
Step 2: Next, consider the limit as the points B and D in the z-plane in Figure 9a move
out to inï¬nity (so that Î± â†’2Ï€) and we obtain Figure 9b. Consequently, the transform
approaches
dz
dÏ„ = C Ï„ âˆ’1
Ï„
,
or
z = C[Ï„ âˆ’log(Ï„)] + K.
Here we have taken the principal branch of the logarithm so that log(z) = ln(|z|)+iÎ¸ where
0 â‰¤Î¸ â‰¤Ï€. (We do not require that 0 â‰¤Î¸ < 2Ï€ because we are always in the upper
half-plane.)
Step 3: Following Example 1.11.7, consider the area around Ï„ = 0. Show that
dz â‰ˆâˆ’C dÏ„
Ï„ = âˆ’iC dÎ¸,
where Ï„ = r eÎ¸i. Integrating from point Bâ€²
1 to point Bâ€²
2, show that C = L.
Step 4: To compute K, note that if the point C, located at z = L + Ï€Li, corresponds to
the point Câ€², located at Ï„ = 1, then K = Ï€Li.
10. Use conformal mapping to solve Laplaceâ€™s equation for the inï¬nite strip âˆ’âˆ< x < âˆ,
0 â‰¤y â‰¤Ï€. The solution equals zero everywhere along the boundary except for x > 0,
y = 0, where u(x, 0) = 1.
Step 1: Consider the mapping Ï„ = ez. Show that Ï = ex cos(y) and Ïƒ = ex sin(y). In
particular, (âˆ, Ï€) â†’(âˆ’âˆ, 0), (0, Ï€) â†’(âˆ’1, 0), (âˆ’âˆ, y) â†’(0, 0), (0, 0) â†’(1, 0), and
(âˆ, 0) â†’(âˆ, 0).

Complex Variables
75
                                                                            

  

  

                                                                                                                                                                



 
  

  

x
zâˆ’plane
Ïƒ
Ï„âˆ’
Ï
plane
y
A
C
D
A
B
B
D
B
B
C
              













                                           
                     




















              













              













1
1
2
1
2
Problem 9b
                                                                                             


                                                                                                                                                                                                                                                                                                                                                         














                                                                                                                                                                                                                                                                                             














                                                                            

x
zâˆ’plane
Ïƒ
Ï„âˆ’
Ï
plane
0
1
y
= 0
z1 
z2 = 1
Î±
Ï  = 
Ï  = 
1 
2
                                          
                                          
              













                     




















      


      


      


              













              













Figure 1.11.12: The conformal mapping between the z-plane and Ï„-plane achieved by the conformal
mapping Ï„ = zÏ€/Î±.
Step 2: Using Poissonâ€™s integral formula for the upper half-plane, show that
u(Ï, Ïƒ) = 1
Ï€
Ï€
2 âˆ’tanâˆ’1
1 âˆ’x
y

= 1 âˆ’1
Ï€ tanâˆ’1

y
x âˆ’1

.
Step 3: Show that
u(x, y) = 1 âˆ’1
Ï€ tanâˆ’1

ex sin(y)
ex cos(y) âˆ’1

.
11. Use conformal mapping to solve Laplaceâ€™s equation for a pie-shaped sector in the ï¬rst
quadrant. See Figure 1.11.12. The solution equals zero along the entire boundary except
for 0 < x < 1 where it equals one.
Step 1: Show that the mapping z = Ï„ Î±/Ï€ or Ï„ = zÏ€/Î± maps the pie-shaped sector into the
half-plane â„‘(Ï„) > 0. See Figure 1.11.12.

76
Advanced Engineering Mathematics: A Second Course
Step 2: Using Poissonâ€™s integral formula for the upper half-plane, show that
u(Ï, Ïƒ) = 1
Ï€ cotâˆ’1
Ï2 + Ïƒ2 âˆ’Ï
Ïƒ

.
Step 3: Show that
u(r, Î¸) = 1
Ï€ cotâˆ’1
rÏ€/Î± âˆ’cos(Ï€Î¸/Î±)
sin(Ï€Î¸/Î±)

,
where x = r cos(Î¸) and y = r sin(Î¸).
12. Use conformal mapping to solve Laplaceâ€™s equation for the semi-inï¬nite strip 0 â‰¤x â‰¤a,
0 â‰¤y < âˆ, where u(x, 0) = 1, 0 â‰¤x â‰¤a, and u(0, y) = u(a, y) = 0, 0 â‰¤y < âˆ.
Step 1: Consider the mapping Ï„ = âˆ’cos(Ï€z/a). Show that
Ï = âˆ’cos(Ï€x/a) cosh(Ï€y/a),
and
Ïƒ = sin(Ï€x/a) sinh(Ï€y/a).
In particular, (0, âˆ) â†’(âˆ’âˆ, 0), (0, 0) â†’(âˆ’1, 0), (a/2, 0) â†’(0, 0), (a, 0) â†’(1, 0), and
(a, âˆ) â†’(âˆ, 0).
Step 2: Using Poissonâ€™s integral formula for the upper half-plane, show that
u(Ï, Ïƒ) = 1
Ï€ cotâˆ’1
Ï2 + Ïƒ2 âˆ’1
2Ïƒ

.
Step 3: Show that
u(x, y) = 1
Ï€ cotâˆ’1
h
sinh2Ï€y
a

âˆ’sin2Ï€x
a
i 
2 sin
Ï€x
a

sinh
Ï€y
a

= 2
Ï€ tanâˆ’1

sin
Ï€x
a
 
sinh
Ï€y
a

.
Step 4: In the case that boundary conditions read u(0, y) = u(a, y) = 1 for 0 â‰¤y < âˆ
and u(x, 0) = 0 for 0 â‰¤x â‰¤a, how could you use the solution in Step 3 to solve this new
problem?
Further Readings
Ablowitz, M. J., and A. S. Fokas, 2003: Complex Variables: Introduction and Applications.
Cambridge University Press, 660 pp. Covers a wide variety of topics, including complex
numbers, analytic functions, singularities, conformal mapping and the Riemann-Hilbert
problem.
Carrier, G. F., M. Krook, and C. E. Pearson, 1966: Functions of a Complex Variable:
Theory and Technique. McGraw-Hill Book Co., 438 pp. Graduate-level textbook.
Churchill, R. V., 1960: Complex Variables and Applications. McGraw-Hill Book Co., 297
pp. Classic textbook.
Flanigan, F. J., 1983: Complex Variables. Dover, 364 pp. A crystal clear exposition and
emphasis on an intuitive understanding of complex analysis.

t > 3
(c,0)
t < 3
Chapter 2
Advanced Transform Methods
In their course work, most engineering students are introduced to the concept of the
Fourier and Laplace transforms. The presentations are limited because the student has not
studied complex variables. Having presented this topic in the previous chapter, the reader
is ready to deepen his/her ability to use these transform methods.
This chapter deals with two important aspects of transform methods. In the past you
may have inverted Fourier and Laplace transforms using partial fractions, tables and some
general properties of the transform. Often these techniques fail and here we show how the
power of complex variables can overcome these diï¬ƒculties.
The reason that Laplace transforms are taught to engineers is their ability to solve
ordinary diï¬€erential equations. When it comes to partial diï¬€erential equations the student
is only taught one method: separation of variables. In Sections 2.4 through 2.6 we show
how Laplace transforms can be used to solve the wave, heat, and Laplace equations.
2.1 INVERSION OF FOURIER TRANSFORMS BY CONTOUR INTEGRATION
Although we may ï¬nd the inverse by direct integration or partial fractions, in many
instances the Fourier transform does not lend itself to these techniques. On the other hand,
if we view the inverse Fourier transform as a line integral along the real axis in the complex
Ï‰-plane, then some of the techniques that we developed in Chapter 1 can be applied to this
problem. To this end, we rewrite the inversion integral for the Fourier transform as
f(t) = 1
2Ï€
Z âˆ
âˆ’âˆ
F(Ï‰)eitÏ‰ dÏ‰ = 1
2Ï€
I
C
F(z)eitz dz âˆ’1
2Ï€
Z
CR
F(z)eitz dz,
(2.1.1)
77

78
Advanced Engineering Mathematics: A Second Course
where C denotes a closed contour consisting of the entire real axis plus a new contour
CR that joins the point (âˆ, 0) to (âˆ’âˆ, 0). There are countless possibilities for CR. For
example, it could be the loop (âˆ, 0) to (âˆ, R) to (âˆ’âˆ, R) to (âˆ’âˆ, 0) with R > 0. However,
any choice of CR must be such that we can compute
R
CR F(z)eitz dz. When we take that
constraint into account, the number of acceptable contours decreases to just a few. The
best is given by Jordanâ€™s lemma.1
Jordanâ€™s lemma: Suppose that, on a circular arc CR with radius R and center at the
origin, f(z) â†’0 uniformly as R â†’âˆ. Then
(1)
lim
Râ†’âˆ
Z
CR
f(z)eimz dz = 0,
(m > 0)
(2.1.2)
if CR lies in the ï¬rst and/or second quadrant;
(2)
lim
Râ†’âˆ
Z
CR
f(z)eâˆ’imz dz = 0,
(m > 0)
(2.1.3)
if CR lies in the third and/or fourth quadrant;
(3)
lim
Râ†’âˆ
Z
CR
f(z)emz dz = 0,
(m > 0)
(2.1.4)
if CR lies in the second and/or third quadrant; and
(4)
lim
Râ†’âˆ
Z
CR
f(z)eâˆ’mz dz = 0,
(m > 0)
(2.1.5)
if CR lies in the ï¬rst and/or fourth quadrant.
Technically, only (1) is actually Jordanâ€™s lemma, while the remaining points are varia-
tions.
Proof : We shall prove the ï¬rst part; the remaining portions follow by analog. We begin by
noting that
|IR| =

Z
CR
f(z)eimz dz
 â‰¤
Z
CR
|f(z)|
eimz |dz|.
(2.1.6)
Now
|dz| = R dÎ¸,
|f(z)| â‰¤MR,
(2.1.7)
eimz =
exp(imReÎ¸i)
 = |exp{imR[cos(Î¸) + i sin(Î¸)]}| = eâˆ’mR sin(Î¸).
(2.1.8)
Therefore,
|IR| â‰¤RMR
Z Î¸1
Î¸0
exp[âˆ’mR sin(Î¸)] dÎ¸,
(2.1.9)
1 Jordan, C., 1894: Cours Dâ€™Analyse de lâ€™ Â´Ecole Polytechnique. Vol. 2. Gauthier-Villars, pp. 285â€“286.
See also Whittaker, E. T., and G. N. Watson, 1963: A Course of Modern Analysis. Cambridge University
Press, p. 115.

Advanced Transform Methods
79
where 0 â‰¤Î¸0 < Î¸1 â‰¤Ï€. Because the integrand is positive, the right side of Equation 2.1.9
is largest if we take Î¸0 = 0 and Î¸1 = Ï€. Then
|IR| â‰¤RMR
Z Ï€
0
eâˆ’mR sin(Î¸) dÎ¸ = 2RMR
Z Ï€/2
0
eâˆ’mR sin(Î¸) dÎ¸.
(2.1.10)
We cannot evaluate the integrals in Equation 2.1.10 as they stand.
However, because
sin(Î¸) â‰¥2Î¸/Ï€ if 0 â‰¤Î¸ â‰¤Ï€/2, we can bound the value of the integral by
|IR| â‰¤2RMR
Z Ï€/2
0
eâˆ’2mRÎ¸/Ï€ dÎ¸ = Ï€
mMR
 1 âˆ’eâˆ’mR
.
(2.1.11)
If m > 0, |IR| tends to zero with MR as R â†’âˆ.
âŠ“âŠ”
Consider now the following inversions of Fourier transforms:
â€¢ Example 2.1.1
For our ï¬rst example we ï¬nd the inverse for
F(Ï‰) =
1
Ï‰2 âˆ’2ibÏ‰ âˆ’a2 âˆ’b2 ,
a, b > 0.
(2.1.12)
From the inversion integral,
f(t) = 1
2Ï€
Z âˆ
âˆ’âˆ
eitÏ‰
Ï‰2 âˆ’2ibÏ‰ âˆ’a2 âˆ’b2 dÏ‰,
(2.1.13)
or
f(t) = 1
2Ï€
I
C
eitz
z2 âˆ’2ibz âˆ’a2 âˆ’b2 dz âˆ’1
2Ï€
Z
CR
eitz
z2 âˆ’2ibz âˆ’a2 âˆ’b2 dz,
(2.1.14)
where C denotes a closed contour consisting of the entire real axis plus CR.
Because
f(z) = 1/(z2 âˆ’2ibz âˆ’a2 âˆ’b2) tends to zero uniformly as |z| â†’âˆand m = t, the second
integral in Equation 2.1.14 vanishes by Jordanâ€™s lemma if CR is a semicircle of inï¬nite radius
in the upper half of the z-plane when t > 0 and a semicircle in the lower half of the z-plane
when t < 0.
Next we must ï¬nd the location and nature of the singularities. They are located at
z2 âˆ’2ibz âˆ’a2 âˆ’b2 = 0,
or
z = Â±a + bi.
(2.1.15)
Therefore we can rewrite Equation 2.1.14 as
f(t) = 1
2Ï€
I
C
eitz
(z âˆ’a âˆ’bi)(z + a âˆ’bi) dz.
(2.1.16)
Thus, all of the singularities are simple poles.
Consider now t > 0.
As stated earlier, we close the line integral with an inï¬nite
semicircle in the upper half-plane. See Figure 2.1.1. Inside this closed contour there are
two singularities: z = Â±a + bi. For these poles,
Res

eitz
z2 âˆ’2ibz âˆ’a2 âˆ’b2 ; a + bi

=
lim
zâ†’a+bi
(z âˆ’a âˆ’bi)eitz
(z âˆ’a âˆ’bi)(z + a âˆ’bi)
(2.1.17)
= eiateâˆ’bt
2a
= eâˆ’bt
2a [cos(at) + i sin(at)],
(2.1.18)

80
Advanced Engineering Mathematics: A Second Course
                                 
original   contour
x
y
a+bi
-a+bi
R
R
C     for t > 0
C     for t < 0
Figure 2.1.1: Contour used to ï¬nd the inverse of the Fourier transform, Equation 2.1.12. The contour C
consists of the line integral along the real axis plus CR.
where we used Eulerâ€™s formula to eliminate eiat. Similarly,
Res

eitz
z2 âˆ’2ibz âˆ’a2 âˆ’b2 ; âˆ’a + bi

= âˆ’eâˆ’bt
2a [cos(at) âˆ’i sin(at)].
(2.1.19)
Consequently, the inverse Fourier transform follows from Equation 2.1.16 after applying the
residue theorem, and equals
f(t) = âˆ’eâˆ’bt
2a sin(at)
(2.1.20)
for t > 0.
For t < 0, the semicircle is in the lower half-plane because the contribution from the
semicircle vanishes as R â†’âˆ. Because there are no singularities within the closed contour,
f(t) = 0. Therefore, we can write in general that
f(t) = âˆ’eâˆ’bt
2a sin(at)H(t).
(2.1.21)
âŠ“âŠ”
â€¢ Example 2.1.2
Let us ï¬nd the inverse of the Fourier transform
F(Ï‰) =
eâˆ’Ï‰i
Ï‰2 + a2 ,
(2.1.22)
where a is real and positive.
From the inversion integral,
f(t) = 1
2Ï€
Z âˆ
âˆ’âˆ
ei(tâˆ’1)Ï‰
Ï‰2 + a2 dÏ‰ = 1
2Ï€
I
C
ei(tâˆ’1)z
z2 + a2 dz âˆ’1
2Ï€
Z
CR
ei(tâˆ’1)z
z2 + a2 dz,
(2.1.23)

Advanced Transform Methods
81
where C denotes a closed contour consisting of the entire real axis plus CR. The contour
CR is determined by Jordanâ€™s lemma because 1/(z2 + a2) â†’0 uniformly as |z| â†’âˆ. Since
m = t âˆ’1, the semicircle CR of inï¬nite radius lies in the upper half-plane if t > 1 and in
the lower half-plane if t < 1. Thus, if t > 1,
f(t) = 1
2Ï€ (2Ï€i)Res
ei(tâˆ’1)z
z2 + a2 ; ai

= eâˆ’a(tâˆ’1)
2a
,
(2.1.24)
whereas for t < 1,
f(t) = 1
2Ï€ (âˆ’2Ï€i)Res
ei(tâˆ’1)z
z2 + a2 ; âˆ’ai

= ea(tâˆ’1)
2a
.
(2.1.25)
The minus sign in front of the 2Ï€i arises from the clockwise direction or negative sense of
the contour. We can write the inverse as the single expression
f(t) = eâˆ’a|tâˆ’1|
2a
.
(2.1.26)
âŠ“âŠ”
â€¢ Example 2.1.3
Let us evaluate the integral
Z âˆ
âˆ’âˆ
cos(kx)
x2 + a2 dx,
(2.1.27)
where a, k > 0.
We begin by noting that
Z âˆ
âˆ’âˆ
cos(kx)
x2 + a2 dx = â„œ
Z âˆ
âˆ’âˆ
eikx
x2 + a2 dx

= â„œ
Z
C1
eikz
z2 + a2 dz

,
(2.1.28)
where C1 denotes a line integral along the real axis from âˆ’âˆto âˆ. A quick check shows
that the integrand of the right side of Equation 2.1.28 satisï¬es Jordanâ€™s lemma. Therefore,
Z âˆ
âˆ’âˆ
eikx
x2 + a2 dx =
I
C
eikz
z2 + a2 dz = 2Ï€i Res

eikz
z2 + a2 ; ai

(2.1.29)
= 2Ï€i lim
zâ†’ai
(z âˆ’ai)eikz
z2 + a2
= Ï€
a eâˆ’ka,
(2.1.30)
where C denotes the closed inï¬nite semicircle in the upper half-plane. Taking the real and
imaginary parts of Equation 2.1.30,
Z âˆ
âˆ’âˆ
cos(kx)
x2 + a2 dx = Ï€
a eâˆ’ka
and
Z âˆ
âˆ’âˆ
sin(kx)
x2 + a2 dx = 0.
(2.1.31)
âŠ“âŠ”

82
Advanced Engineering Mathematics: A Second Course
                



                



                    

              

                    

 
x
y
R
R
C     for t > 0
C     for t < 0
original   contour
a
-a
Figure 2.1.2: Contour used in Example 2.1.32.
â€¢ Example 2.1.4
Let us now invert the Fourier transform F(Ï‰) = 2a/(a2 âˆ’Ï‰2), where a is real. The
interesting aspect of this problem is the presence of singularities at Ï‰ = Â±a that lie along
the contour of integration. How do we use contour integration to compute
f(t) = a
Ï€
Z âˆ
âˆ’âˆ
eitÏ‰
a2 âˆ’Ï‰2 dÏ‰?
(2.1.32)
The answer to this question involves the concept of Cauchy principal value integrals,
which allows us to extend the conventional deï¬nition of integrals to include integrands
that become inï¬nite at a ï¬nite number of points.
See Section 1.10.
Thus, by treating
Equation 2.1.32 as a Cauchy principal value integral, we again convert it into a closed
contour integration by closing the line integration along the real axis as shown in Figure
2.1.2. The semicircles at inï¬nity vanish by Jordanâ€™s lemma and
f(t) = a
Ï€
I
C
eitz
a2 âˆ’z2 dz.
(2.1.33)
For t > 0,
f(t) = âˆ’2Ï€ia
Ï€
1
2Res

eitz
z2 âˆ’a2 ; âˆ’a

âˆ’2Ï€ia
Ï€
1
2Res

eitz
z2 âˆ’a2 ; a

.
(2.1.34)
We have the factor 1
2 because we are only passing over the â€œtopâ€ of the singularity at z = a
and z = âˆ’a. Computing the residues and simplifying the results, we obtain f(t) = sin(at).
Similarly, when t < 0,
f(t) = 2Ï€ia
Ï€
1
2Res

eitz
z2 âˆ’a2 ; âˆ’a

+ 2Ï€ia
Ï€
1
2Res

eitz
z2 âˆ’a2 ; a

= âˆ’sin(at).
(2.1.35)

Advanced Transform Methods
83
These results can be collapsed down to the single expression f(t) = sgn(t) sin(at).
âŠ“âŠ”
â€¢ Example 2.1.5
An additional beneï¬t of understanding inversion by the residue method is the ability
to qualitatively anticipate the inverse by knowing the location of the poles of F(Ï‰). This
intuition is important because many engineering analyses discuss stability and performance
entirely in terms of the properties of the systemâ€™s Fourier transform. In Figure 2.1.3 we
graphed the location of the poles of F(Ï‰) and the corresponding f(t). The student should
go through the mental exercise of connecting the two pictures.
âŠ“âŠ”
â€¢ Example 2.1.6
So far, we used only the ï¬rst two points of Jordanâ€™s lemma.
In this example2 we
illustrate how the remaining two points may be applied.
Consider the contour integral
I
C
cot(Ï€z)

eâˆ’cz
(Ï„ + 2Ï€z)2 + Î²2 +
eâˆ’cz
(Ï„ âˆ’2Ï€z)2 + Î²2

dz,
where c > 0 and Î², Ï„ are real. Let us evaluate this contour integral where the contour is
shown in Figure 2.1.4.
From the residue theorem,
I
C
cot(Ï€z)

eâˆ’cz
(Ï„ + 2Ï€z)2 + Î²2 +
eâˆ’cz
(Ï„ âˆ’2Ï€z)2 + Î²2

dz
= 2Ï€i
âˆ
X
n=1
Res

cot(Ï€z)

eâˆ’cz
(Ï„ + 2Ï€z)2 + Î²2 +
eâˆ’cz
(Ï„ âˆ’2Ï€z)2 + Î²2

; n

+ 2Ï€i Res

cot(Ï€z)

eâˆ’cz
(Ï„ + 2Ï€z)2 + Î²2 +
eâˆ’cz
(Ï„ âˆ’2Ï€z)2 + Î²2

; |Ï„| + Î²i
2Ï€

+ 2Ï€i Res

cot(Ï€z)

eâˆ’cz
(Ï„ + 2Ï€z)2 + Î²2 +
eâˆ’cz
(Ï„ âˆ’2Ï€z)2 + Î²2

; |Ï„| âˆ’Î²i
2Ï€

.
(2.1.36)
Now
Res

cot(Ï€z)

eâˆ’cz
(Ï„ + 2Ï€z)2 + Î²2 +
eâˆ’cz
(Ï„ âˆ’2Ï€z)2 + Î²2

; n

= lim
zâ†’n
(z âˆ’n) cos(Ï€z)
sin(Ï€z)
lim
zâ†’n

eâˆ’cz
(Ï„ + 2Ï€z)2 + Î²2 +
eâˆ’cz
(Ï„ âˆ’2Ï€z)2 + Î²2

(2.1.37)
= 1
Ï€

eâˆ’nc
(Ï„ + 2nÏ€)2 + Î²2 +
eâˆ’nc
(Ï„ âˆ’2nÏ€)2 + Î²2

,
(2.1.38)
Res

cot(Ï€z)

eâˆ’cz
(Ï„ + 2Ï€z)2 + Î²2 +
eâˆ’cz
(Ï„ âˆ’2Ï€z)2 + Î²2

; |Ï„| + Î²i
2Ï€

=
lim
zâ†’(|Ï„|+Î²i)/2Ï€
cot(Ï€z)
4Ï€2

(z âˆ’|Ï„| âˆ’Î²i)eâˆ’cz
(z + Ï„/2Ï€)2 + Î²2/4Ï€2 +
(z âˆ’|Ï„| âˆ’Î²i)eâˆ’cz
(z âˆ’Ï„/2Ï€)2 + Î²2/4Ï€2

(2.1.39)
= cot(|Ï„|/2 + Î²i/2) exp(âˆ’c|Ï„|/2Ï€)[cos(cÎ²/2Ï€) âˆ’i sin(cÎ²/2Ï€)]
4Ï€Î²i
,
(2.1.40)
2 See Hsieh, T. C., and R. Greif, 1972: Theoretical determination of the absorption coeï¬ƒcient and the
total band absorptance including a speciï¬c application to carbon monoxide. Int. J. Heat Mass Transfer,
15, 1477â€“1487.

84
Advanced Engineering Mathematics: A Second Course
-plane
f(t)
t
f(t)
t
Ï‰ -plane
Ï‰
-plane
f(t)
t
f(t)
t
Ï‰ -plane
Ï‰
-plane
f(t)
t
f(t)
t
Ï‰ -plane
Ï‰
Figure 2.1.3: The correspondence between the location of the simple poles of the Fourier transform F(Ï‰)
and the behavior of f(t).

Advanced Transform Methods
85
2Ï€
Îµ
|Ï„|âˆ’Î¹Î²
|Ï„|+Î¹Î²
2Ï€
Figure 2.1.4: Contour used in Example 2.1.6.
and
Res

cot(Ï€z)

eâˆ’cz
(Ï„ + 2Ï€z)2 + Î²2 +
eâˆ’cz
(Ï„ âˆ’2Ï€z)2 + Î²2

; |Ï„| âˆ’Î²i
2Ï€

=
lim
zâ†’(|Ï„|âˆ’Î²i)/2Ï€
cot(Ï€z)
4Ï€2

(z âˆ’|Ï„| + Î²i)eâˆ’cz
(z + Ï„/2Ï€)2 + Î²2/4Ï€2 +
(z âˆ’|Ï„| + Î²i)eâˆ’cz
(z âˆ’Ï„/2Ï€)2 + Î²2/4Ï€2

(2.1.41)
= cot(|Ï„|/2 âˆ’Î²i/2) exp(âˆ’c|Ï„|/2Ï€)[cos(cÎ²/2Ï€) + i sin(cÎ²/2Ï€)]
âˆ’4Ï€Î²i
.
(2.1.42)
Therefore,
I
C
cot(Ï€z)

eâˆ’cz
(Ï„ + 2Ï€z)2 + Î²2 +
eâˆ’cz
(Ï„ âˆ’2Ï€z)2 + Î²2

dz
= 2i
âˆ
X
n=1

eâˆ’nc
(Ï„ + 2nÏ€)2 + Î²2 +
eâˆ’nc
(Ï„ âˆ’2nÏ€)2 + Î²2

+ i
2Î²
ei|Ï„| + eÎ²
ei|Ï„| âˆ’eÎ² eâˆ’c|Ï„|/2Ï€[cos(cÎ²/2Ï€) âˆ’i sin(cÎ²/2Ï€)]
âˆ’i
2Î²
ei|Ï„| + eâˆ’Î²
ei|Ï„| âˆ’eâˆ’Î² eâˆ’c|Ï„|/2Ï€[cos(cÎ²/2Ï€) + i sin(cÎ²/2Ï€)]
(2.1.43)
= 2i
âˆ
X
n=1

eâˆ’nc
(Ï„ + 2nÏ€)2 + Î²2 +
eâˆ’nc
(Ï„ âˆ’2nÏ€)2 + Î²2

âˆ’i
Î²
sinh(Î²) cos(cÎ²/2Ï€) + sin(|Ï„|) sin(cÎ²/2Ï€)
cosh(Î²) âˆ’cos(Ï„)
eâˆ’c|Ï„|/2Ï€,
(2.1.44)
where cot(Î±) = i(e2iÎ± + 1)/(e2iÎ± âˆ’1), and we made extensive use of Eulerâ€™s formula.
Let us now evaluate the contour integral by direct integration. The contribution from
the integration along the semicircle at inï¬nity vanishes according to Jordanâ€™s lemma. In-
deed, that is why this particular contour was chosen. Therefore,

86
Advanced Engineering Mathematics: A Second Course
I
C
cot(Ï€z)

eâˆ’cz
(Ï„ + 2Ï€z)2 + Î²2 +
eâˆ’cz
(Ï„ âˆ’2Ï€z)2 + Î²2

dz
=
Z iÇ«
iâˆ
cot(Ï€z)

eâˆ’cz
(Ï„ + 2Ï€z)2 + Î²2 +
eâˆ’cz
(Ï„ âˆ’2Ï€z)2 + Î²2

dz
+
Z
CÇ«
cot(Ï€z)

eâˆ’cz
(Ï„ + 2Ï€z)2 + Î²2 +
eâˆ’cz
(Ï„ âˆ’2Ï€z)2 + Î²2

dz
+
Z âˆ’iâˆ
âˆ’iÇ«
cot(Ï€z)

eâˆ’cz
(Ï„ + 2Ï€z)2 + Î²2 +
eâˆ’cz
(Ï„ âˆ’2Ï€z)2 + Î²2

dz.
(2.1.45)
Now, because z = iy,
Z iÇ«
iâˆ
cot(Ï€z)

eâˆ’cz
(Ï„ + 2Ï€z)2 + Î²2 +
eâˆ’cz
(Ï„ âˆ’2Ï€z)2 + Î²2

dz
=
Z Ç«
âˆ
coth(Ï€y)

eâˆ’icy
(Ï„ + 2Ï€iy)2 + Î²2 +
eâˆ’icy
(Ï„ âˆ’2Ï€iy)2 + Î²2

dy
(2.1.46)
= âˆ’2
Z âˆ
Ç«
coth(Ï€y)(Ï„ 2 + Î²2 âˆ’4Ï€2y2)eâˆ’icy
(Ï„ 2 + Î²2 âˆ’4Ï€2y2)2 + 16Ï€2Ï„ 2y2 dy,
(2.1.47)
Z âˆ’iâˆ
âˆ’iÇ«
cot(Ï€z)

eâˆ’cz
(Ï„ + 2Ï€z)2 + Î²2 +
eâˆ’cz
(Ï„ âˆ’2Ï€z)2 + Î²2

dz
=
Z âˆ’âˆ
âˆ’Ç«
coth(Ï€y)

eâˆ’icy
(Ï„ + 2Ï€iy)2 + Î²2 +
eâˆ’icy
(Ï„ âˆ’2Ï€iy)2 + Î²2

dy
(2.1.48)
= 2
Z âˆ
Ç«
coth(Ï€y)(Ï„ 2 + Î²2 âˆ’4Ï€2y2)eicy
(Ï„ 2 + Î²2 âˆ’4Ï€2y2)2 + 16Ï€2Ï„ 2y2 dy,
(2.1.49)
and
Z
CÇ«
cot(Ï€z)

eâˆ’cz
(Ï„ + 2Ï€z)2 + Î²2 +
eâˆ’cz
(Ï„ âˆ’2Ï€z)2 + Î²2

dz
=
Z âˆ’Ï€/2
Ï€/2

1
Ï€Ç«eÎ¸i âˆ’Ï€Ç«eÎ¸i
3
âˆ’Â· Â· Â·

Ç«ieÎ¸i dÎ¸
Ã—

exp(âˆ’cÇ«eÎ¸i)
(Ï„ + 2Ï€Ç«eÎ¸i)2 + Î²2 +
exp(âˆ’cÇ«eÎ¸i)
(Ï„ âˆ’2Ï€Ç«eÎ¸i)2 + Î²2

.
(2.1.50)
In the limit of Ç« â†’0,
I
C
cot(Ï€z)

eâˆ’cz
(Ï„ + 2Ï€z)2 + Î²2 +
eâˆ’cz
(Ï„ âˆ’2Ï€z)2 + Î²2

dz
= 4i
Z âˆ
0
coth(Ï€y)(Ï„ 2 + Î²2 âˆ’4Ï€2y2) sin(cy)
(Ï„ 2 + Î²2 âˆ’4Ï€2y2)2 + 16Ï€2Ï„ 2y2
dy âˆ’
2i
Ï„ 2 + Î²2
(2.1.51)
= 2i
âˆ
X
n=1

eâˆ’nc
(Ï„ + 2nÏ€)2 + Î²2 +
eâˆ’nc
(Ï„ âˆ’2nÏ€)2 + Î²2

âˆ’i
Î²
sinh(Î²) cos(cÎ²/2Ï€) + sin(|Ï„|) sin(cÎ²/2Ï€)
cosh(Î²) âˆ’cos(Ï„)
eâˆ’c|Ï„|/2Ï€,
(2.1.52)

Advanced Transform Methods
87
or
4
Z âˆ
0
coth(Ï€y)(Ï„ 2 + Î²2 âˆ’4Ï€2y2) sin(cy)
(Ï„ 2 + Î²2 âˆ’4Ï€2y2)2 + 16Ï€2Ï„ 2y2
dy
= 2
âˆ
X
n=1

eâˆ’nc
(Ï„ + 2nÏ€)2 + Î²2 +
eâˆ’nc
(Ï„ âˆ’2nÏ€)2 + Î²2

(2.1.53)
âˆ’1
Î²
sinh(Î²) cos(cÎ²/2Ï€) + sin(|Ï„|) sin(cÎ²/2Ï€)
cosh(Î²) âˆ’cos(Ï„)
eâˆ’c|Ï„|/2Ï€ +
2
Ï„ 2 + Î²2 .
If we let y = x/2Ï€,
Î²
Ï€
Z âˆ
0
coth(x/2)(Ï„ 2 + Î²2 âˆ’x2) sin(cx/2Ï€)
(Ï„ 2 + Î²2 âˆ’x2)2 + 4Ï„ 2x2
dx
= 2Î²
âˆ
X
n=1

eâˆ’nc
(Ï„ + 2nÏ€)2 + Î²2 +
eâˆ’nc
(Ï„ âˆ’2nÏ€)2 + Î²2

(2.1.54)
âˆ’sinh(Î²) cos(cÎ²/2Ï€) + sin(|Ï„|) sin(cÎ²/2Ï€)
cosh(Î²) âˆ’cos(Ï„)
eâˆ’c|Ï„|/2Ï€ +
2Î²
Ï„ 2 + Î²2 .
Problems
By taking the appropriate closed contour, ï¬nd the inverse of the following Fourier transforms
F(Ï‰) by contour integration. The parameter a is real and positive.
1.
1
Ï‰2 + a2
2.
Ï‰
Ï‰2 + a2
3.
Ï‰
(Ï‰2 + a2)2
4.
Ï‰2
(Ï‰2 + a2)2
5.
1
Ï‰2 âˆ’3iÏ‰ âˆ’3
6.
1
(Ï‰ âˆ’ia)2n+2
7.
Ï‰2
(Ï‰2 âˆ’1)2 + 4a2Ï‰2
8.
3
(2 âˆ’Ï‰i)(1 + Ï‰i)
Then check your answer using MATLAB.
9. Find the inverse of F(Ï‰) = cos(Ï‰)/(Ï‰2 + a2), a > 0, by ï¬rst rewriting the transform as
F(Ï‰) =
eiÏ‰
2(Ï‰2 + a2) +
eâˆ’iÏ‰
2(Ï‰2 + a2)
and then using the residue theorem on each term.
10. Find3 the inverse Fourier transform for
FÂ±(Ï‰) =
eÂ±iÏ‰
(Ï‰ âˆ’ai) (R2eÏ‰i âˆ’eâˆ’Ï‰i) =
eÂ±iÏ‰âˆ’iÏ‰
(Ï‰ âˆ’ai) (R2 âˆ’eâˆ’2Ï‰i),
where a > 0 and R > 1.
3 See Scharstein, R. W., 1992: Transient electromagnetic plane wave reï¬‚ection from a dielectric slab.
IEEE Trans. Educ., 35, 170â€“175.

88
Advanced Engineering Mathematics: A Second Course
Step 1: Show that
fÂ±(t) = 1
2Ï€
Z âˆ
âˆ’âˆ
ei(tâˆ’1Â±1)Ï‰
(Ï‰ âˆ’ai)(R2 âˆ’eâˆ’2Ï‰i) dÏ‰.
Step 2: Show that the singularities consist of simple poles at z = ai and zn = Â±nÏ€+i ln(R),
where n = 0, Â±1, Â±2, Â±3, . . ..
Step 3: For t > 0 show that
f+(t) = 1
2Ï€
Z âˆ
âˆ’âˆ
eitÏ‰
(Ï‰ âˆ’ai)(R2 âˆ’eâˆ’2Ï‰i) dÏ‰.
and we must close the contour with an inï¬nite semi-circle in the top half-plane.
Step 4: Show that
Z âˆ
âˆ’âˆ
eitÏ‰
(Ï‰ âˆ’ai)(R2 âˆ’eâˆ’2Ï‰i) dÏ‰ = 2Ï€i Res

eitz
(z âˆ’ai)(R2 âˆ’eâˆ’2zi); ai

+ 2Ï€i
âˆ
X
n=âˆ’âˆ
Res

eitz
(z âˆ’ai)(R2 âˆ’eâˆ’2zi); zn

.
where
Res

eitz
(z âˆ’ai)(R2 âˆ’eâˆ’2zi); ai

=
eâˆ’at
R2 âˆ’e2a ,
and
Res

eitz
(z âˆ’ai)(R2 âˆ’eâˆ’2zi); zn

=
Râˆ’teÂ±inÏ€t
2iR2 {Â±nÏ€ + [ln(R) âˆ’a] i},
so that
f+(t) =
i eâˆ’at
R2 âˆ’e2a +
1
2Rt+2
âˆ
X
n=âˆ’âˆ
einÏ€t
nÏ€ + [ln(R) âˆ’a] i.
Step 5: For the case t < 0, show that we close the contour with an inï¬nite semi-circle in
the bottom half-plane to compute f+(t).
Step 6: Compute the residues of the enclosed singularities in Step 5 and show that f+(t) = 0.
Why?
Step 7: Show that f+(t) equals
f+(t) =
i eâˆ’at
R2 âˆ’e2a H(t) + H(t)
2Rt+2
âˆ
X
n=âˆ’âˆ
einÏ€t
nÏ€ + [ln(R) âˆ’a] i
at any time t.
Step 8: For Fâˆ’(Ï‰), show that
fâˆ’(t) = 1
2Ï€
Z âˆ
âˆ’âˆ
ei(tâˆ’2)Ï‰
(Ï‰ âˆ’ai)(R2 âˆ’eâˆ’2Ï‰i) dÏ‰.
Step 9: For t > 2, show that we close the contour with an inï¬nite semi-circle in the top
half-plane.

Advanced Transform Methods
89
Step 10: Compute the residue of the enclosed singularities and show that
Z âˆ
âˆ’âˆ
ei(tâˆ’2)Ï‰
(Ï‰ âˆ’ai)(R2 âˆ’eâˆ’2Ï‰i) dÏ‰ = 2Ï€i Res

ei(tâˆ’2)z
(z âˆ’ai)(R2 âˆ’eâˆ’2zi); ai

+ 2Ï€i
âˆ
X
n=âˆ’âˆ
Res

ei(tâˆ’2)z
(z âˆ’ai)(R2 âˆ’eâˆ’2zi); zn

.
where
Res

ei(tâˆ’2)z
(z âˆ’ai)(R2 âˆ’eâˆ’2zi); ai

=
eâˆ’at
R2eâˆ’2a âˆ’1,
and
Res

ei(tâˆ’2)z
(z âˆ’ai)(R2 âˆ’eâˆ’2zi); zn

=
Râˆ’t+2eÂ±inÏ€t
2iR2 {Â±nÏ€ + [ln(R) âˆ’a] i}.
so that
fâˆ’(t) =
i eâˆ’at
R2eâˆ’2a âˆ’1 +
1
2Rt
âˆ
X
n=âˆ’âˆ
einÏ€t
nÏ€ + [ln(R) âˆ’a] i.
Step 11: For t < 2 we must close the contour with an inï¬nite semi-circle in the bottom
half-plane. Compute the residue of the enclosed singularities and show that fâˆ’(t) = 0.
Why?
Step 12: Show that the ï¬nal answer for fâˆ’(t) at any time t is
fâˆ’(t) =
i eâˆ’at
R2eâˆ’2a âˆ’1H(t âˆ’2) + H(t âˆ’2)
2Rt
âˆ
X
n=âˆ’âˆ
einÏ€t
nÏ€ + [ln(R) âˆ’a] i.
11. During the solution of the heat equation, Taitel et al.4 inverted the Fourier transform
F(Ï‰) =
cosh(y
âˆš
Ï‰2 + 1 )
âˆš
Ï‰2 + 1 sinh(p
âˆš
Ï‰2 + 1/2)
,
where y and p are real.
Step 1: From the deï¬nition of the Fourier transform, show that
f(t) = 1
2Ï€
I
C
cosh
 y
âˆš
z2 + 1

eizt
âˆš
z2 + 1 sinh
 p
âˆš
z2 + 1/2
dz,
where we have closed the line integral with an inï¬nite semicircle in the upper half-plane if
t > 0. For t < 0 we close the contour in the lower half-plane.
Step 2: For t > 0, show that the enclosed singularities are simple poles that are located at
z = i and p
âˆš
z2 + 1 = 2nÏ€i, n = 0, 1, 2, . . ., or zn = i
p
1 + 4n2Ï€2/p2.
Step 3: Show that
Res

cosh
 y
âˆš
z2 + 1

eizt
âˆš
z2 + 1 sinh
 p
âˆš
z2 + 1/2
; i

= eâˆ’t
ip ,
4 Taitel, Y., M. Bentwich, and A. Tamir, 1973: Eï¬€ects of upstream and downstream boundary conditions
on heat (mass) transfer with axial diï¬€usion. Int. J. Heat Mass Transfer, 16, 359â€“369.

90
Advanced Engineering Mathematics: A Second Course
and
Res

cosh
 y
âˆš
z2 + 1

eizt
âˆš
z2 + 1 sinh
 p
âˆš
z2 + 1/2
; zn

= 2 cos(2nÏ€y/p) exp(âˆ’
p
1 + 4n2Ï€2/p2 t)
ip(âˆ’1)np
1 + 4n2Ï€2/p2
.
Step 4: For t > 0, show that
f(t) = eâˆ’t
p
+ 2
p
âˆ
X
n=1
(âˆ’1)n cos(2nÏ€y/p) exp(âˆ’
p
1 + 4n2Ï€2/p2 t)
p
1 + 4n2Ï€2/p2
.
Step 5: For t < 0, show that the enclosed singularities are simple poles located at z = âˆ’i
and zn = âˆ’i
p
1 + 4n2Ï€2/p2.
Step 6: Show that
Res

cosh
 y
âˆš
z2 + 1

eizt
âˆš
z2 + 1 sinh
 p
âˆš
z2 + 1/2
; âˆ’i

= âˆ’et
ip,
and
Res

cosh
 y
âˆš
z2 + 1

eizt
âˆš
z2 + 1 sinh
 p
âˆš
z2 + 1/2
; zn

= âˆ’2 cos(2nÏ€y/p) exp(
p
1 + 4n2Ï€2/p2 t)
ip(âˆ’1)np
1 + 4n2Ï€2/p2
.
Step 7: For t < 0, show that
f(t) = et
p + 2
p
âˆ
X
n=1
(âˆ’1)n cos(2nÏ€y/p) exp(
p
1 + 4n2Ï€2/p2 t)
p
1 + 4n2Ï€2/p2
.
Step 8: Show that we write the results from Step 4 and Step 7 as
f(t) = eâˆ’|t|
p
+ 2
p
âˆ
X
n=1
(âˆ’1)n
p
1 + 4n2Ï€2/p2 cos
2nÏ€y
p

eâˆ’âˆš
1+4n2Ï€2/p2 |t|.
In this case, our time variable t was their spatial variable x âˆ’Î¾.
12. Find the inverse of the Fourier transform
F(Ï‰) =

cos

Ï‰L
Î²[1 + iÎ³ sgn(Ï‰)]
âˆ’1
,
where L, Î², and Î³ are real and positive and sgn(z) = 1 if â„œ(z) > 0 and âˆ’1 if â„œ(z) < 0.
Step 1: From the deï¬nition of the Fourier transform, show that
f(t) = 1
2Ï€
Z âˆ
âˆ’âˆ
eitÏ‰
cos
n
Ï‰L
Î²[1+iÎ³sgn(Ï‰)]
odÏ‰ = 1
2Ï€
I
C
eitz
cos
n
zL
Î²[1+iÎ³sgn(z)]
o dz.
Step 2: Show that the integral has simple poles at znÂ± = Â±(2n âˆ’1)Î²Ï€/(2L) + (2n âˆ’
1)iÎ²Î³Ï€/(2L), where n = 1, 2, 3, . . .

Advanced Transform Methods
91
Step 3: For t > 0, use the residue theorem and show that
f(t) = i
" âˆ
X
n=1
Res
 
eitz
cos
n
zL
Î²[1+iÎ³sgn(z)]
o; zn+
!
+ Res
 
eitz
cos
n
zL
Î²[1+iÎ³sgn(z)]
o; znâˆ’
!#
,
where
Res

eitz
cos
n
zL
Î²[1+iÎ³sgn(z)]
o; znÂ±

= Â±(âˆ’1)nÎ²
L
[1 + iÎ³sgn(zn)]eâˆ’(2nâˆ’1)Î²Î³Ï€t/(2L)Â±(2nâˆ’1)Î²Ï€it/(2L).
Step 4: For t < 0, show that f(t) = 0. Why?
Step 5: Show that we can summarize the results from Step 3 and Step 4 by
f(t) = 2Î²
L H(t)
âˆ
X
n=1
(âˆ’1)n+1eâˆ’(2nâˆ’1)Î²Î³Ï€t/2L {Î³ cos[(2n âˆ’1)Î²Ï€t/2L] + sin[(2n âˆ’1)Î²Ï€t/2L]} .
Use the residue theorem to verify the following integrals:
13.
Z âˆ
âˆ’âˆ
sin(x)
x2 + 4x + 5 dx = âˆ’Ï€
e sin(2)
14.
Z âˆ
0
cos(x)
(x2 + 1)2 dx = Ï€
2e
15
Z âˆ
âˆ’âˆ
x sin(ax)
x2 + 4
dx = Ï€eâˆ’2a
16.
Z âˆ
0
x2 cos(ax)
(x2 + b2)2 dx = Ï€
4b(1 âˆ’ab)eâˆ’ab
where a, b > 0.
17. The concept of forced convection is normally associated with heat streaming through
a duct or past an obstacle.
Bentwich5 showed that a similar transport can exist when
convection results from a wave traveling through an essentially stagnant ï¬‚uid. In the process
of computing the amount of heating, he proved the following identity:
Z âˆ
âˆ’âˆ
cosh(hx) âˆ’1
x sinh(hx)
cos(ax) dx = ln[coth(|a|Ï€/h)],
h > 0.
Conï¬rm his result.
Step 1:
Z âˆ
âˆ’âˆ
cosh(hx) âˆ’1
x sinh(hx)
cos(ax) dx = â„œ
I
C
cosh(hz) âˆ’1
z sinh(hz) eaiz dz

,
if a > 0 and C is a semicircle of inï¬nite radius in the upper half-plane.
Step 2: Within the contour, show that there is a removal singularity at z = 0 and simple
poles at hzn = nÏ€i with n = 1, 2, 3, . . ..
5 Bentwich, M., 1966: Convection enforced by surface and tidal waves. Int. J. Heat Mass Transfer, 9,
663â€“670.

92
Advanced Engineering Mathematics: A Second Course
Step 3: Show that
I
C
cosh(hz) âˆ’1
z sinh(hz) eaiz dz = 2Ï€i
âˆ
X
n=1
Res
cosh(hz) âˆ’1
z sinh(hz) eaiz; nÏ€i
h

with
Res
cosh(hz) âˆ’1
z sinh(hz) eaiz; nÏ€i
h

= 1 âˆ’(âˆ’1)n
nÏ€i
eâˆ’nÏ€a/h.
Step 4: Show that
Z âˆ
âˆ’âˆ
cosh(hx) âˆ’1
x sinh(hx)
cos(ax) dx = 4
âˆ
X
m=1
exp[âˆ’(2m âˆ’1)Ï€a/h]
2m âˆ’1
= ln[coth(Ï€a/h)].
Step 5: Redo the analysis if we replace a by âˆ’a. Reconcile your results with those given
by Bentwich.
2.2 INVERSION OF LAPLACE TRANSFORMS BY CONTOUR INTEGRATION
Partial fractions and convolution are two common methods for ï¬nding the inverse of
the Laplace transform F(s). In many instances these methods fail simply because of the
complexity of the transform to be inverted. In this section we shall show how we can invert
transforms through the powerful method of contour integration. Of course, the student
must be proï¬cient in the use of complex variables.
Consider the piece-wise diï¬€erentiable function f(x), which vanishes for x < 0. We can
express the function eâˆ’cxf(x) by the complex Fourier representation of
f(x)eâˆ’cx = 1
2Ï€
Z âˆ
âˆ’âˆ
eiÏ‰x
Z âˆ
0
eâˆ’ctf(t)eâˆ’iÏ‰t dt

dÏ‰,
(2.2.1)
for any value of the real constant c, where the integral
I =
Z âˆ
0
eâˆ’ct|f(t)| dt
(2.2.2)
exists. By multiplying both sides of Equation 2.2.1 by ecx and bringing it inside the ï¬rst
integral,
f(x) = 1
2Ï€
Z âˆ
âˆ’âˆ
e(c+Ï‰i)x
Z âˆ
0
f(t)eâˆ’(c+Ï‰i)t dt

dÏ‰.
(2.2.3)
With the substitution z = c + Ï‰i, where z is a new, complex variable of integration,
f(x) =
1
2Ï€i
Z c+âˆi
câˆ’âˆi
ezx
Z âˆ
0
f(t)eâˆ’zt dt

dz.
(2.2.4)
The quantity inside the square brackets is the Laplace transform F(z). Therefore, we can
express f(t) in terms of its transform by the complex contour integral
f(t) =
1
2Ï€i
Z c+âˆi
câˆ’âˆi
F(z)etzdz.
(2.2.5)

Advanced Transform Methods
93
An outstanding mathematician at Cambridge University at the turn of the twentieth century,
Thomas John Iâ€™Anson Bromwich (1875â€“1929) came to Heavisideâ€™s operational calculus through
his interest in divergent series. Beginning a correspondence with Heaviside, Bromwich was able to
justify operational calculus through the use of contour integrals by 1915. After his premature death,
individuals such as J. R. Carson and Sir H. Jeï¬€reys brought Laplace transforms to the increasing
attention of scientists and engineers. (Portrait courtesy of the Royal Society of London.)
This line integral, the Bromwich integral,6 runs along the line x = c parallel to the imaginary
axis and c units to the right of it, the so-called Bromwich contour. We select the value of c
suï¬ƒciently large so that the integral, Equation 2.2.2, exists; subsequent analysis shows that
this occurs when c is larger than the real part of any of the singularities of F(z).
We must now evaluate the contour integral.
Because of the power of the residue
theorem in complex variables, the contour integral is usually transformed into a closed
contour through the use of Jordanâ€™s lemma. See Section 2.1, Equations 2.1.4 and Equation
2.1.5. The following examples will illustrate the proper use of Equation 2.2.5.
â€¢ Example 2.2.1
Let us invert
F(s) =
eâˆ’3s
s2(s âˆ’1).
(2.2.6)
6 Bromwich, T. J. Iâ€™A., 1916: Normal coordinates in dynamical systems. Proc. London Math. Soc.,
Ser. 2, 15, 401â€“448.

94
Advanced Engineering Mathematics: A Second Course
t > 3
(c,0)
t < 3
Figure 2.2.1: Contours used in the inversion of Equation 2.2.6.
From Bromwichâ€™s integral,
f(t) =
1
2Ï€i
Z c+âˆi
câˆ’âˆi
e(tâˆ’3)z
z2(z âˆ’1) dz =
1
2Ï€i
I
C
e(tâˆ’3)z
z2(z âˆ’1) dz âˆ’
1
2Ï€i
Z
CR
e(tâˆ’3)z
z2(z âˆ’1) dz,
(2.2.7)
where CR is a semicircle of inï¬nite radius in either the right or left half of the z-plane and
C is the closed contour that includes CR and Bromwichâ€™s contour. See Figure 2.2.1.
Our ï¬rst task is to choose an appropriate contour so that the integral along CR vanishes.
By Jordanâ€™s lemma, this requires a semicircle in the right half-plane if t âˆ’3 < 0 and
a semicircle in the left half-plane if t âˆ’3 > 0.
Consequently, by considering these two
separate cases, we force the second integral in Equation 2.2.7 to zero and the inversion
simply equals the closed contour.
Consider the case t < 3 ï¬rst. Because Bromwichâ€™s contour lies to the right of any
singularities, there are no singularities within the closed contour and f(t) = 0.
Consider now the case t > 3. Within the closed contour in the left half-plane, there is
a second-order pole at z = 0 and a simple pole at z = 1. Therefore,
f(t) = Res
 e(tâˆ’3)z
z2(z âˆ’1); 0

+ Res
 e(tâˆ’3)z
z2(z âˆ’1); 1

,
(2.2.8)
where
Res
 e(tâˆ’3)z
z2(z âˆ’1); 0

= lim
zâ†’0
d
dz

z2 e(tâˆ’3)z
z2(z âˆ’1)

= lim
zâ†’0
(t âˆ’3)e(tâˆ’3)z
z âˆ’1
âˆ’e(tâˆ’3)z
(z âˆ’1)2

= 2 âˆ’t,
(2.2.9)
and
Res
 e(tâˆ’3)z
z2(z âˆ’1); 1

= lim
zâ†’1 (z âˆ’1) e(tâˆ’3)z
z2(z âˆ’1) = etâˆ’3.
(2.2.10)
Taking our earlier results into account, the inverse equals
f(t) =

etâˆ’3 âˆ’(t âˆ’3) âˆ’1

H(t âˆ’3),
(2.2.11)

Advanced Transform Methods
95
-
(c,0)
Ï€i/a
Ï€i/a
2Ï€i/a
2Ï€i/a
3Ï€i/a
3Ï€i/a
-
-
Figure 2.2.2: Contours used in the inversion of Equation 2.2.12.
which we would have obtained from the second shifting theorem and tables.
âŠ“âŠ”
â€¢ Example 2.2.2
For our second example of the inversion of Laplace transforms by complex integration,
let us ï¬nd the inverse of
F(s) =
1
s sinh(as),
(2.2.12)
where a is real. From Bromwichâ€™s integral,
f(t) =
1
2Ï€i
Z c+âˆi
câˆ’âˆi
etz
z sinh(az) dz.
(2.2.13)
Here c is greater than the real part of any of the singularities in Equation 2.2.12. Using the
inï¬nite product for the hyperbolic sine,7
etz
z sinh(az) =
etz
az2[1 + a2z2/Ï€2][1 + a2z2/(4Ï€2)][1 + a2z2/(9Ï€2)] Â· Â· Â·.
(2.2.14)
Thus, we have a second-order pole at z = 0 and simple poles at zn = Â±nÏ€i/a, where
n = 1, 2, 3, . . ..
We can convert the line integral Equation 2.2.13, with the Bromwich contour lying
parallel and slightly to the right of the imaginary axis, into a closed contour using Jordanâ€™s
lemma through the addition of an inï¬nite semicircle joining iâˆto âˆ’iâˆ, as shown in Figure
2.2.2. We now apply the residue theorem. For the second-order pole at z = 0,
7 Gradshteyn, I. S., and I. M. Ryzhik, 1965: Table of Integrals, Series and Products. Academic Press,
Section 1.431, Formula 2.

96
Advanced Engineering Mathematics: A Second Course
Res

etz
z sinh(az); 0

= 1
1! lim
zâ†’0
d
dz
(z âˆ’0)2etz
z sinh(az)

= lim
zâ†’0
d
dz

zetz
sinh(az)

(2.2.15)
= lim
zâ†’0

etz
sinh(az) +
ztetz
sinh(az) âˆ’az cosh(az)etz
sinh2(az)

= t
a
(2.2.16)
after using sinh(az) = az + O(z3). For the simple poles zn = Â±nÏ€i/a,
Res

etz
z sinh(az); zn

= lim
zâ†’zn
(z âˆ’zn)etz
z sinh(az) = lim
zâ†’zn
etz
sinh(az) + az cosh(az)
(2.2.17)
= exp(Â±nÏ€it/a)
(âˆ’1)n(Â±nÏ€i) ,
(2.2.18)
because cosh(Â±nÏ€i) = cos(nÏ€) = (âˆ’1)n. Thus, summing up all of the residues gives
f(t) = t
a +
âˆ
X
n=1
(âˆ’1)n exp(nÏ€it/a)
nÏ€i
âˆ’
âˆ
X
n=1
(âˆ’1)n exp(âˆ’nÏ€it/a)
nÏ€i
(2.2.19)
= t
a + 2
Ï€
âˆ
X
n=1
(âˆ’1)n
n
sin(nÏ€t/a).
(2.2.20)
âŠ“âŠ”
In addition to computing the inverse of Laplace transforms, Bromwichâ€™s integral places
certain restrictions on F(s) in order that an inverse exists. If Î± denotes the minimum value
that c may possess, the restrictions are threefold.8
First, F(z) must be analytic in the
half-plane x â‰¥Î±, where z = x + iy. Second, in the same half-plane it must behave as zâˆ’k,
where k > 1. Finally, F(x) must be real when x â‰¥Î±.
â€¢ Example 2.2.3
Is the function sin(s)/(s2 + 4) a proper Laplace transform? Although the function
satisï¬es the ï¬rst and third criteria listed in the previous paragraph on the half-plane x > 2,
the function becomes unbounded as y â†’Â±âˆfor any ï¬xed x > 2. Thus, sin(s)/(s2 + 4)
cannot be a Laplace transform.
âŠ“âŠ”
â€¢ Example 2.2.4
An additional beneï¬t of understanding inversion by the residue method is the ability
to qualitatively anticipate the inverse by knowing the location of the poles of F(s). This
intuition is important because many engineering analyses discuss stability and performance
entirely in terms of the properties of the systemâ€™s Laplace transform. In Figure 2.2.3 we
have graphed the location of the poles of F(s) and the corresponding f(t). The student
should go through the mental exercise of connecting the two pictures.
8 For the proof, see Churchill, R. V., 1972: Operational Mathematics. McGraw-Hill, Section 67.

Advanced Transform Methods
97
t
f(t)
f(t)
s-plane
s-plane
t
t
f(t)
f(t)
s-plane
s-plane
t
t
f(t)
f(t)
s-plane
s-plane
t
Figure 2.2.3: The correspondence between the location of the simple poles of the Laplace transform F(s)
and the behavior of f(t).

98
Advanced Engineering Mathematics: A Second Course
Problems
Use Bromwichâ€™s integral to invert the following Laplace transforms F(s):
1.
s + 1
(s + 2)2(s + 3)
2.
1
s2(s + a)2
3.
1
s(s âˆ’2)3
4.
1
s(s + a)2(s2 + b2)
5.
eâˆ’s
s2(s + 2)
6. Use Bromwichâ€™s integral to invert
F(s) =
1
s(1 + eâˆ’as).
Step 1: Show that the singularities are all simple poles and are located at z = 0 and
zn = Â±(2n âˆ’1)Ï€i/a, where n = 1, 2, 3, . . ..
Step 2: Show that the corresponding residues are
Res

etz
z(1 + eâˆ’az); 0

= 1
2,
and
Res

etz
z(1 + eâˆ’az); zn

= Â±exp[Â±(2n âˆ’1)Ï€it/a]
(2n âˆ’1)Ï€i
.
Step 3: Show that the inverse Laplace transform equals
f(t) = 1
2 + 2
Ï€
âˆ
X
n=1
sin[(2n âˆ’1)Ï€t/a]
(2n âˆ’1)Ï€
.
7. Use Bromwichâ€™s integral to invert
F(s) =
1
(s + b) cosh(as).
Step 1: Show that the singularities are all simple poles and are located at z = âˆ’b and
zn = Â±(2n âˆ’1)Ï€i/(2a), where n = 1, 2, 3, . . . because cosh(az) = cos(iaz) = 0.
Step 2: Show that the corresponding residues are
Res

etz
(z + b) cosh(az); âˆ’b

=
eâˆ’bt
cosh(ab),
and
Res

etz
(z + b) cosh(az); zn

= Â±
exp[Â±(2n âˆ’1)Ï€it/(2a)]
ai[b Â± (2n âˆ’1)Ï€i/(2a)] sin[(2n âˆ’1)Ï€/2].
Step 3: Show that the inverse Laplace transform equals
f(t) =
eâˆ’bt
cosh(ab) âˆ’8ab
âˆ
X
n=1
(âˆ’1)n sin[(2n âˆ’1)Ï€t/(2a)]
4a2b2 + (2n âˆ’1)2Ï€2
+ 4
âˆ
X
n=1
(âˆ’1)n (2n âˆ’1)Ï€ cos[(2n âˆ’1)Ï€t/(2a)]
4a2b2 + (2n âˆ’1)2Ï€2
.

Advanced Transform Methods
99
8. Use Bromwichâ€™s integral to invert
F(s) =
1
s(1 âˆ’eâˆ’as).
Step 1: Show that the singularities are all simple poles and are located at z = 0 and
zn = Â±2nÏ€i/a, where n = 1, 2, 3, . . ..
Step 2: Show that the corresponding residues are
Res

etz
z(1 âˆ’eâˆ’az); 0

= t
a + 1
2,
and
Res

etz
z(1 âˆ’eâˆ’az); zn

= Â±exp(Â±2nÏ€it/a)
2nÏ€i
.
Hint: Near z = 0, show that
etz
z(1 âˆ’eâˆ’az) =
1 + tz + Â· Â· Â·
az2(1 âˆ’az/2 + Â· Â· Â·) =
1
az2

1 + tz + az
2 + Â· Â· Â·

and read oï¬€the residue from the Laurent expansion.
Step 3: Show that the inverse Laplace transform equals
f(t) = t
a + 1
2 + 1
Ï€
âˆ
X
n=1
sin(2nÏ€t/a)
n
.
9. Consider a function f(t) that has the Laplace transform F(z), which is analytic in the
half-plane â„œ(z) > s0. Can we use this knowledge to ï¬nd g(t), whose Laplace transform
G(z) equals F[Ï•(z)], where Ï•(z) is also analytic for â„œ(z) > s0? The answer to this question
leads to the Schouten9â€“Van der Pol10 theorem.
Step 1: Show that the following relationships hold true:
G(z) = F[Ï•(z)] =
Z âˆ
0
f(Ï„)eâˆ’Ï•(z)Ï„ dÏ„,
and
g(t) =
1
2Ï€i
Z c+âˆi
câˆ’âˆi
F[Ï•(z)]etz dz.
Step 2: Using the results from Step 1, show that
g(t) =
Z âˆ
0
f(Ï„)
 1
2Ï€i
Z c+âˆi
câˆ’âˆi
eâˆ’Ï•(z)Ï„etz dz

dÏ„.
This is the Schouten-Van der Pol theorem.
Step 3: If G(z) = F(âˆšz ) show that
g(t) =
1
2
âˆš
Ï€t3
Z âˆ
0
Ï„f(Ï„) exp

âˆ’Ï„ 2
4t

dÏ„.
9 Schouten, J. P., 1935: A new theorem in operational calculus together with an application of it.
Physica, 2, 75â€“80.
10 Van der Pol, B., 1934: A theorem on electrical networks with applications to ï¬lters.
Physica, 1,
521â€“530.

100
Advanced Engineering Mathematics: A Second Course
Hint: Do not evaluate the contour integral. Instead, ask yourself: What function of time
has a Laplace transform that equals eâˆ’Ï•(z)Ï„, where Ï„ is a parameter? Then use tables.
2.3 INTEGRAL EQUATIONS
An integral equation contains the dependent variable under an integral sign. The convo-
lution theorem provides an excellent tool for solving a very special class of these equations,
the Volterra equation of the second kind:11
f(t) âˆ’
Z t
0
K[t, x, f(x)] dx = g(t),
0 â‰¤t â‰¤T.
(2.3.1)
These equations appear in history-dependent problems, such as epidemics,12 vibration prob-
lems,13 and viscoelasticity.14
â€¢ Example 2.3.1
Let us ï¬nd f(t) from the integral equation
f(t) = 4t âˆ’3
Z t
0
f(x) sin(t âˆ’x) dx.
(2.3.2)
The integral in Equation 2.3.2 is such that we can use the convolution theorem to ï¬nd
its Laplace transform. Then, because L[sin(t)] = 1/(s2 +1), the convolution theorem yields
L
Z t
0
f(x) sin(t âˆ’x) dx

= F(s)
s2 + 1.
(2.3.3)
Therefore, the Laplace transform converts Equation 2.3.2 into
F(s) = 4
s2 âˆ’3F(s)
s2 + 1.
(2.3.4)
Solving for F(s),
F(s) = 4(s2 + 1)
s2(s2 + 4).
(2.3.5)
By partial fractions, or by inspection,
F(s) = 1
s2 +
3
s2 + 4.
(2.3.6)
11 Fock, V., 1924: Â¨Uber eine Klasse von Integralgleichungen. Math. Z., 21, 161â€“173; Koizumi, S., 1931:
On Heavisideâ€™s operational solution of a Volterraâ€™s integral equation when its nucleus is a function of (xâˆ’Î¾).
Philos. Mag., Ser. 7, 11, 432â€“441.
12 Wang, F. J. S., 1978: Asymptotic behavior of some deterministic epidemic models. SIAM J. Math.
Anal., 9, 529â€“534.
13 Lin, S. P., 1975: Damped vibration of a string. J. Fluid Mech., 72, 787â€“797.
14 Rogers, T. G., and E. H. Lee, 1964: The cylinder problem in viscoelastic stress analysis. Q. Appl.
Math., 22, 117â€“131.

Advanced Transform Methods
101
Therefore, inverting term by term,
f(t) = t + 3
2 sin(2t).
(2.3.7)
Note that the integral equation
f(t) = 4t âˆ’3
Z t
0
f(t âˆ’x) sin(x) dx
(2.3.8)
also has the same solution.
âŠ“âŠ”
â€¢ Example 2.3.2
Let us solve the equation
f â€²(t) + Î±2
Z t
0
f(Ï„) dÏ„ = B âˆ’C cos(Ï‰t),
f(0) = 0.
(2.3.9)
Again the integral is one of the convolution type; it diï¬€ers from the previous example
in that it includes a derivative. Taking the Laplace transform of Equation 2.3.9,
sF(s) âˆ’f(0) + Î±2F(s)
s
= B
s âˆ’
sC
s2 + Ï‰2 .
(2.3.10)
Because f(0) = 0, Equation 2.3.10 simpliï¬es to
(s2 + Î±2)F(s) = B âˆ’
Cs2
s2 + Ï‰2 .
(2.3.11)
Solving for F(s),
F(s) =
B
s2 + Î±2 âˆ’
Cs2
(s2 + Î±2)(s2 + Ï‰2).
(2.3.12)
Using partial fractions to invert Equation 2.3.12,
f(t) =
B
Î± +
Î±C
Ï‰2 âˆ’Î±2

sin(Î±t) âˆ’
Ï‰C
Ï‰2 âˆ’Î±2 sin(Ï‰t).
(2.3.13)
âŠ“âŠ”
â€¢ Example 2.3.3
Let us solve15 the integral equation
f(t) =
a
2(1 + 2a)
Z t
0
f(t âˆ’x)f(x) dx + eâˆ’t.
(2.3.14)
15 Hounslow, M. J., 1990: A discretized population balance for continuous systems at steady state.
AICHE J., 36, 106â€“116.

102
Advanced Engineering Mathematics: A Second Course
Taking the Laplace transform of Equation 2.3.14, we obtain
F(s) =
a F 2(s)
2(1 + 2a) +
1
s + 1.
(2.3.15)
Solving for F(s) so that F(s) â†’0 as s â†’âˆ, we have
F(s) = 2a + 1
a
âˆ’2a + 1
a
s
(2a + 1)(s + 1) âˆ’2a
(2a + 1)(s + 1)
= 2a + 1
a
âˆ’
âˆš2a + 1
a
r
(2a + 1)s + 1
s + 1
.
(2.3.16)
Taking the inverse of Equation 2.3.16,
f(t) = 2a + 1
a
Î´(t) âˆ’
âˆš2a + 1
a
g(t),
(2.3.17)
where g(t) is the inverse of the Laplace transform G(s),
G(s) =
r
(2a + 1)s + 1
s + 1
=
âˆš
2a + 1
s + 1/(1 + 2a)
âˆšs + 1
p
s + 1/(2a + 1)
(2.3.18)
=
âˆš
2a + 1 sH(s) +
H(s)
âˆš2a + 1
(2.3.19)
and
H(s) =
1
âˆšs + 1
p
s + 1/(2a + 1)
.
(2.3.20)
Taking the inverse of H(s), we ï¬nd that
h(t) = exp

âˆ’a + 1
2a + 1t

I0

at
2a + 1

(2.3.21)
and
hâ€²(t) = âˆ’a + 1
2a + 1 exp

âˆ’a + 1
2a + 1t

I0

at
2a + 1

+
a
2a + 1 exp

âˆ’a + 1
2a + 1t

I1

at
2a + 1

,
(2.3.22)
where I0(Â·) and I1(Â·) are modiï¬ed Bessel functions of the ï¬rst kind.
Because sH(s) = L[hâ€²(t)]+h(0) and h(0) = 1, hâ€²(t) = Lâˆ’1[sH(s)]âˆ’Î´(t) or Lâˆ’1[sH(s)]
= hâ€²(t) + Î´(t). Then,
g(t) =
âˆš
2a + 1

hâ€²(t) +
h(t)
2a + 1 + Î´(t)

(2.3.23)
=
âˆš
2a + 1

Î´(t) +
a
2a + 1 exp

âˆ’a + 1
2a + 1t

I1

at
2a + 1

âˆ’
a
2a + 1 exp

âˆ’a + 1
2a + 1t

I0

at
2a + 1

.
(2.3.24)
Finally, substituting Equation 2.3.24 into Equation 2.3.17,
f(t) = exp

âˆ’a + 1
2a + 1t
 
I0

at
2a + 1

âˆ’I1

at
2a + 1

.
(2.3.25)

Advanced Transform Methods
103
Problems
Solve the following integral equations:
1. f(t) = 1 + 2
Z t
0
f(t âˆ’x)eâˆ’2x dx
2. f(t) = 1 +
Z t
0
f(x) sin(t âˆ’x) dx
3. f(t) = t +
Z t
0
f(t âˆ’x)eâˆ’x dx
4. f(t) = 4t2 âˆ’
Z t
0
f(t âˆ’x)eâˆ’x dx
5. f(t) = t3 +
Z t
0
f(x) sin(t âˆ’x) dx
6. f(t) = 8t2 âˆ’3
Z t
0
f(x) sin(t âˆ’x) dx
7. f(t) = t2 âˆ’2
Z t
0
f(t âˆ’x) sinh(2x) dx
8. f(t) = 1 + 2
Z t
0
f(t âˆ’x) cos(x) dx
9. f(t) = e2t + 2
Z t
0
f(t âˆ’x) cos(x) dx
10. f(t) = t2 +
Z t
0
f(x) sin(t âˆ’x) dx
11. f(t) = eâˆ’t âˆ’2
Z t
0
f(x) cos(t âˆ’x) dx
12. f(t) +
Z t
0
f(x)(t âˆ’x) dx = t
13. f(t) = 6t + 4
Z t
0
f(x)(t âˆ’x)2 dx
14. f(t) = a
âˆš
t âˆ’
Z t
0
f(t âˆ’x)
âˆšx
dx
15. Solve the following equation for f(t) with the condition that f(0) = 4:
f â€²(t) = t +
Z t
0
f(t âˆ’x) cos(x) dx.
16. Solve the following equation for f(t) with the condition that f(0) = 0:
f â€²(t) = sin(t) +
Z t
0
f(t âˆ’x) cos(x) dx.
17. During a study of nucleation involving idealized active sites along a boiling surface,
Marto and Rohsenow16 solved the integral equation
A = B
âˆš
t + C
Z t
0
xâ€²(Ï„)
âˆšt âˆ’Ï„ dÏ„
to ï¬nd the position x(t) of the liquid/vapor interface. If A, B, and C are constants and
x(0) = 0, ï¬nd the solution for them.
18. Solve the following equation for x(t) with the condition that x(0) = 0:
x(t) + t =
1
câˆšÏ€
Z t
0
xâ€²(Ï„)
âˆšt âˆ’Ï„ dÏ„,
where c is constant.
Step 1: Show that
X(s) = âˆ’
c
s2(c âˆ’âˆšs ) = âˆ’c(c + âˆšs )
s2(c2 âˆ’s) .
16 Marto, P. J., and W. M. Rohsenow, 1966: Nucleate boiling instability of alkali metals.
J. Heat
Transfer, 88, 183â€“193.

104
Advanced Engineering Mathematics: A Second Course
Step 2: Use partial fractions to show that
X(s) = âˆ’1
c2

1 +
âˆšs
c
 c2
s2 + 1
s âˆ’
1
s âˆ’c2

.
Step 3: Show that
x(t) = 1
c2
(
ec2t h
1 + erf

c
âˆš
t
i
âˆ’c2t âˆ’1 âˆ’2c
r
t
Ï€
)
.
19. During a study of the temperature f(t) of a heat reservoir attached to a semi-inï¬nite
heat-conducting rod, Huber17 solved the integral equation
f â€²(t) = Î± âˆ’Î²
âˆšÏ€
Z t
0
f â€²(Ï„)
âˆšt âˆ’Ï„ dÏ„,
where Î± and Î² are constants and f(0) = 0. Find f(t) for him.
Step 1: Show that
F(s) =
Î±
s3/2  s1/2 + Î²
 =
Î±
s (s âˆ’Î²2) âˆ’
Î±Î²
s3/2 (s âˆ’Î²2)
=
Î±
Î²2(s âˆ’Î²2) âˆ’Î±
Î²2s âˆ’
Î±Î²
s3/2 (s âˆ’Î²2).
Step 2: Taking the inverse term by term, show that
f(t) = Î±
Î²2
 
eÎ²2t âˆ’1 âˆ’4eÎ²2t
âˆšÏ€
Z Î²
âˆš
t
0
eâˆ’x2x2 dx
!
= Î±
Î²2
 
eÎ²2t âˆ’1 + 2Î²
âˆš
t
âˆšÏ€
âˆ’2eÎ²2t
âˆšÏ€
Z Î²
âˆš
t
0
eâˆ’x2 dx
!
.
20.
During the solution of a diï¬€usion problem, Zhdanov, Chikhachev, and Yavlinskii18
solved an integral equation similar to
Z t
0
f(Ï„)

1 âˆ’erf
 a
âˆš
t âˆ’Ï„

dÏ„ = at,
where erf(x) =
2
âˆšÏ€
Z x
0
eâˆ’y2 dy is the error function. What should they have found?
Step 1: Show that
F(s) = a
s + a3
s2 +
a2
s
âˆš
s + a2 +
a4
s2 âˆš
s + a2 .
17 Huber, A., 1934: Eine Methode zur Bestimmung der WÂ¨arme- und TemperaturleitfÂ¨ahigkeit. Monatsh.
Math. Phys., 41, 35â€“42.
18 Zhdanov, S. K., A. S. Chikhachev, and Yu. N. Yavlinskii, 1976: Diï¬€usion boundary-value problem for
regions with moving boundaries and conservation of particles. Sov. Phys. Tech. Phys., 21, 883â€“884.

Advanced Transform Methods
105
Step 2: Show that
L

t erf(a
âˆš
t ) âˆ’
1
2a2 erf(a
âˆš
t ) +
âˆš
t
a âˆšÏ€ eâˆ’a2t

= âˆ’d
ds

a
s
âˆš
s + a2

âˆ’
1
2as
âˆš
s + a2 +
1
2a (s + a2)3/2 =
a
s2 âˆš
s + a2 .
Step 3: Taking the inverse of Step 1 term by term, show that
f(t) = a + a2t + 1
2a erf(
âˆš
at ) + a3t erf(
âˆš
at ) + a2âˆš
t
âˆšÏ€ eâˆ’a2t.
21. The Laguerre polynomial19
y(t) = Ln(t) = et
n!
dn
dtn
 tneâˆ’t
,
n = 0, 1, 2, 3, . . .
satisï¬es the ordinary diï¬€erential equation
tyâ€²â€² + (1 âˆ’t)yâ€² + ny = (tyâ€²)â€² âˆ’tyâ€² + ny = 0,
with y(0) = 1 and yâ€²(0) = âˆ’n.
Step 1: Using the properties that L[f (n)(t)] = snF(s) âˆ’snâˆ’1f(0) âˆ’Â· Â· Â· âˆ’sf (nâˆ’2)(0) âˆ’
f (nâˆ’1)(0) and Equation L[t f(t)] = âˆ’F â€²(s), show that the Laplace transformed version of
this diï¬€erential equation is
Y â€²(s) = n + 1 âˆ’s
s(s âˆ’1) Y (s) =
n
s âˆ’1Y (s) âˆ’n + 1
s
Y (s),
where Y (s) is the Laplace transform of y(t).
Step 2: Using the property that L[t f(t)] = âˆ’F â€²(s) and the convolution theorem, show that
Laguerre polynomials are the solution to the integral equation
ty(t) = (n + 1)
Z t
0
y(Ï„) dÏ„ âˆ’net
Z t
0
y(Ï„) eâˆ’Ï„ dÏ„.
2.4 THE SOLUTION OF THE WAVE EQUATION BY USING LAPLACE TRANSFORMS
The solution of linear partial diï¬€erential equations by Laplace transforms is the most
commonly employed analytic technique after separation of variables. Because the transform
consists solely of an integration with respect to time, the transform U(x, s) of the solution
of the wave equation u(x, t) is
U(x, s) =
Z âˆ
0
u(x, t)eâˆ’st dt,
(2.4.1)
19 See Section 5.3 in Andrews, L. C., 1985: Special Functions for Engineers and Applied Mathematicians.
MacMillan, 357 pp.

106
Advanced Engineering Mathematics: A Second Course
assuming that the wave equation only varies in a single spatial variable x and time t.
Partial derivatives involving time have transforms similar to those that we encountered
in the case of functions of a single variable. They include
L[ut(x, t)] = sU(x, s) âˆ’u(x, 0),
(2.4.2)
and
L[utt(x, t)] = s2U(x, s) âˆ’su(x, 0) âˆ’ut(x, 0).
(2.4.3)
These transforms introduce the initial conditions via u(x, 0) and ut(x, 0). On the other
hand, derivatives involving x become
L[ux(x, t)] = d
dx{L[u(x, t)]} = dU(x, s)
dx
,
(2.4.4)
and
L[uxx(x, t)] = d2
dx2 {L[u(x, t)]} = d2U(x, s)
dx2
.
(2.4.5)
Because the transformation eliminates the time variable, only U(x, s) and its derivatives
remain in the equation. Consequently, we transform the partial diï¬€erential equation into a
boundary-value problem involving an ordinary diï¬€erential equation. Because this equation
is often easier to solve than a partial diï¬€erential equation, the use of Laplace transforms
considerably simpliï¬es the original problem. Of course, the Laplace transforms must exist
for this technique to work.
The following schematic summarizes the Laplace transform method:
In the following examples, we illustrate transform methods by solving the classic equa-
tion of telegraphy as it applies to a uniform transmission line. The line has a resistance R,
an inductance L, a capacitance C, and a leakage conductance G per unit length. We denote
the current in the direction of positive x by I; V is the voltage drop across the transmission
line at the point x. The dependent variables I and V are functions of both distance x along
the line and time t.
To derive the diï¬€erential equations that govern the current and voltage in the line,
consider the points A at x and B at x + âˆ†x in Figure 2.4.1. The current and voltage at
A are I(x, t) and V (x, t); at B, I + âˆ‚I
âˆ‚xâˆ†x and V + âˆ‚V
âˆ‚x âˆ†x. Therefore, the voltage drop
from A to B is âˆ’âˆ‚V
âˆ‚x âˆ†x and the current in the line is I + âˆ‚I
âˆ‚xâˆ†x. Neglecting terms that are
proportional to (âˆ†x)2,

Lâˆ‚I
âˆ‚t + RI

âˆ†x = âˆ’âˆ‚V
âˆ‚x âˆ†x.
(2.4.6)
The voltage drop over the parallel portion HK of the line is V while the current in this
portion of the line is âˆ’âˆ‚I
âˆ‚xâˆ†x. Thus,

C âˆ‚V
âˆ‚t + GV

âˆ†x = âˆ’âˆ‚I
âˆ‚xâˆ†x.
(2.4.7)
Therefore, the diï¬€erential equations for I and V are
Lâˆ‚I
âˆ‚t + RI = âˆ’âˆ‚V
âˆ‚x ,
(2.4.8)
and
C âˆ‚V
âˆ‚t + GV = âˆ’âˆ‚I
âˆ‚x.
(2.4.9)

Advanced Transform Methods
107
x
K
H
L
R
B
V+
C
A
x
x
âˆ†x
1 âˆ†x
âˆ†x
âˆ†
I
+âˆ†
x
Î´ I
Î´ x
Î´ V
Î´ x
+
I
Î´
Î´ x
I
x
âˆ†x
âˆ†
G
V
Figure 2.4.1: Schematic of a uniform transmission line.
Turning to the initial conditions, we solve these simultaneous partial diï¬€erential equa-
tions with the initial conditions
I(x, 0) = I0(x),
(2.4.10)
and
V (x, 0) = V0(x)
(2.4.11)
for 0 < t. There are also boundary conditions at the ends of the line; we will introduce
them for each speciï¬c problem. For example, if the line is short-circuited at x = a, V = 0
at x = a; if there is an open circuit at x = a, I = 0 at x = a.
To solve Equation 2.4.8 and Equation 2.4.9 by Laplace transforms, we take the Laplace
transform of both sides of these equations, which yields
(Ls + R)I(x, s) = âˆ’dV (x, s)
dx
+ LI0(x),
(2.4.12)
and
(Cs + G)V (x, s) = âˆ’dI(x, s)
dx
+ CV0(x).
(2.4.13)
Eliminating I gives an ordinary diï¬€erential equation in V
d2V
dx2 âˆ’q2V = LdI0(x)
dx
âˆ’C(Ls + R)V0(x),
(2.4.14)
where q2 = (Ls + R)(Cs + G). After ï¬nding V , we may compute I from
I = âˆ’
1
Ls + R
dV
dx + LI0(x)
Ls + R.
(2.4.15)
At this point we treat several classic cases.
â€¢ Example 2.4.1: The semi-inï¬nite transmission line
We consider the problem of a semi-inï¬nite line 0 < x with no initial current and charge.
The end x = 0 has a constant voltage E for 0 < t.
In this case,
d2V
dx2 âˆ’q2V = 0,
0 < x.
(2.4.16)

108
Advanced Engineering Mathematics: A Second Course
The boundary conditions at the ends of the line are
V (0, t) = E,
0 < t,
(2.4.17)
and V (x, t) is ï¬nite as x â†’âˆ. The transform of these boundary conditions is
V (0, s) = E/s,
and
lim
xâ†’âˆV (x, s) â†’0.
(2.4.18)
The general solution of Equation 2.4.16 is
V (x, s) = Aeâˆ’qx + Beqx.
(2.4.19)
The requirement that V remains ï¬nite as x â†’âˆforces B = 0. The boundary condition at
x = 0 gives A = E/s. Thus,
V (x, s) = E
s exp
h
âˆ’
p
(Ls + R)(Cs + G) x
i
.
(2.4.20)
We discuss the general case later. However, for the so-called â€œlosslessâ€ line, where R = G =
0,
V (x, s) = E
s exp(âˆ’sx/c),
(2.4.21)
where c = 1/
âˆš
LC. Consequently,
V (x, t) = EH

t âˆ’x
c

,
(2.4.22)
where H(t) is Heavisideâ€™s step function. The physical interpretation of this solution is as
follows: V (x, t) is zero up to the time x/c, at which time a wave traveling with speed c from
x = 0 would arrive at the point x. V (x, t) has the constant value E afterwards.
For the so-called â€œdistortionlessâ€ line,20 R/L = G/C = Ï,
V (x, t) = Eeâˆ’Ïx/cH

t âˆ’x
c

.
(2.4.23)
In this case, the disturbance not only propagates with velocity c but also attenuates as we
move along the line.
Suppose now, that instead of applying a constant voltage E at x = 0, we apply a
time-dependent voltage, f(t). The only modiï¬cation is that in place of Equation 2.4.20,
V (x, s) = F(s)eâˆ’qx.
(2.4.24)
In the case of the distortionless line, q = (s + Ï)/c, this becomes
V (x, s) = F(s)eâˆ’(s+Ï)x/c
(2.4.25)
and
V (x, t) = eâˆ’Ïx/cf

t âˆ’x
c

H

t âˆ’x
c

.
(2.4.26)
20 Prechtl and SchÂ¨urhuber (Prechtl, A., and R. SchÂ¨urhuber, 2000: Nonuniform distortionless transmission
lines. Electr. Eng. [Berlin], 82, 127â€“134) generalized this problem to nonuniform transmission lines.

Advanced Transform Methods
109
(l-x)/c
t
E
V(x,t)
(3l-x)/c
(3l+x)/c
direct
once
twice
thrice reflected
(l+x)/c
Figure 2.4.2: The voltage within a lossless, ï¬nite transmission line of length l as a function of time t.
Thus, our solution shows that the voltage at x is zero up to the time x/c. Afterwards V (x, t)
follows the voltage at x = 0 with a time lag of x/c and decreases in magnitude by eâˆ’Ïx/c.âŠ“âŠ”
â€¢ Example 2.4.2: The ï¬nite transmission line
We now discuss the problem of a ï¬nite transmission line 0 < x < l with zero initial
current and charge. We ground the end x = 0 and maintain the end x = l at constant
voltage E for 0 < t.
The transformed partial diï¬€erential equation becomes
d2V
dx2 âˆ’q2V = 0,
0 < x < l.
(2.4.27)
The boundary conditions are
V (0, t) = 0,
and
V (l, t) = E,
0 < t.
(2.4.28)
The Laplace transform of these boundary conditions is
V (0, s) = 0,
and
V (l, s) = E/s.
(2.4.29)
The solution of Equation 2.4.27 that satisï¬es the boundary conditions is
V (x, s) = E sinh(qx)
s sinh(ql) .
(2.4.30)
Let us rewrite Equation 2.4.30 in a form involving negative exponentials and expand the
denominator by the binomial theorem,
V (x, s) = E
s eâˆ’q(lâˆ’x) 1 âˆ’eâˆ’2qx
1 âˆ’eâˆ’2ql
(2.4.31)
= E
s eâˆ’q(lâˆ’x)(1 âˆ’eâˆ’2qx)
 1 + eâˆ’2ql + eâˆ’4ql + Â· Â· Â·

(2.4.32)
= E
s

eâˆ’q(lâˆ’x) âˆ’eâˆ’q(l+x) + eâˆ’q(3lâˆ’x) âˆ’eâˆ’q(3l+x) + Â· Â· Â·

.
(2.4.33)
In the special case of the lossless line where q = s/c,
V (x, s) = E
s

eâˆ’s(lâˆ’x)/c âˆ’eâˆ’s(l+x)/c + eâˆ’s(3lâˆ’x)/c âˆ’eâˆ’s(3l+x)/c + Â· Â· Â·

,
(2.4.34)
or
V (x, t) = E

H

t âˆ’l âˆ’x
c

âˆ’H

t âˆ’l + x
c

+ H

t âˆ’3l âˆ’x
c

âˆ’H

t âˆ’3l + x
c

+ Â· Â· Â·

.
(2.4.35)

110
Advanced Engineering Mathematics: A Second Course
1
10
100
1000
x
0.0
0.2
0.4
0.6
0.8
1.0
V(x,t)/E
Îºt=10
100
1000
10000
Figure 2.4.3: The voltage within a submarine cable as a function of distance for various values of Îºt.
We illustrate Equation 2.4.35 in Figure 2.4.2. The voltage at x is zero up to the time
(l âˆ’x)/c, at which time a wave traveling directly from the end x = l would reach the point
x. The voltage then has the constant value E up to the time (l+x)/c, at which time a wave
traveling from the end x = l and reï¬‚ected back from the end x = 0 would arrive. From this
time up to the time of arrival of a twice-reï¬‚ected wave, it has the value zero, and so on. âŠ“âŠ”
â€¢ Example 2.4.3: The semi-inï¬nite transmission line reconsidered
In the ï¬rst example, we showed that the transform of the solution for the semi-inï¬nite
line is
V (x, s) = E
s eâˆ’qx,
(2.4.36)
where q2 = (Ls + R)(Cs + G). In the case of a lossless line (R = G = 0), we found traveling
wave solutions.
In this example, we shall examine the case of a submarine cable,21 where L = G = 0.
In this special case,
V (x, s) = E
s eâˆ’xâˆš
s/Îº,
(2.4.37)
where Îº = 1/(RC).
From a table of Laplace transforms,22 we can immediately invert
Equation 2.4.37 and ï¬nd that
V (x, t) = E erfc

x
2
âˆš
Îºt

,
(2.4.38)
where erfc(Â·) is the complementary error function. Unlike the traveling wave solution, the
voltage diï¬€uses into the cable as time increases. We illustrate Equation 2.4.38 in Figure
2.4.3.
âŠ“âŠ”
21 First solved by Thomson, W., 1855: On the theory of the electric telegraph. Proc. R. Soc. London,
Ser. A, 7, 382â€“399.
22 See Churchill, R. V., 1972: Operational Mathematics. McGraw-Hill Book, Section 27.

Advanced Transform Methods
111
â€¢ Example 2.4.4: A short-circuited, ï¬nite transmission line
Let us ï¬nd the voltage of a lossless transmission line of length l that initially has the
constant voltage E. At t = 0, we ground the line at x = 0 while we leave the end x = l
insulated.
The transformed partial diï¬€erential equation now becomes
d2V
dx2 âˆ’s2
c2 V = âˆ’sE
c2 ,
(2.4.39)
where c = 1/
âˆš
LC. The boundary conditions are
V (0, s) = 0,
(2.4.40)
and
I(l, s) = âˆ’1
Ls
dV (l, s)
dx
= 0
(2.4.41)
from Equation 2.4.15.
The solution to this boundary-value problem is
V (x, s) = E
s âˆ’E cosh[s(l âˆ’x)/c]
s cosh(sl/c)
.
(2.4.42)
The ï¬rst term on the right side of Equation 2.4.42 is easy to invert and the inversion equals
E. The second term is much more diï¬ƒcult to handle. We will use Bromwichâ€™s integral.
In Section 2.2 we showed that
Lâˆ’1
cosh[s(l âˆ’x)/c]
s cosh(sl/c)

=
1
2Ï€i
Z c+âˆi
câˆ’âˆi
cosh[z(l âˆ’x)/c]etz
z cosh(zl/c)
dz.
(2.4.43)
To evaluate this integral, we must ï¬rst locate and then classify the singularities. Using the
product formula for the hyperbolic cosine,
cosh[z(l âˆ’x)/c]
z cosh(zl/c)
= [1 + 4z2(lâˆ’x)2
c2Ï€2
][1 + 4z2(lâˆ’x)2
9c2Ï€2
] Â· Â· Â·
z[1 + 4z2l2
c2Ï€2 ][1 + 4z2l2
9c2Ï€2 ] Â· Â· Â·
.
(2.4.44)
This shows that we have an inï¬nite number of simple poles located at z = 0, and zn =
Â±(2n âˆ’1)Ï€ci/(2l), where n = 1, 2, 3, . . .. Therefore, Bromwichâ€™s contour can lie along, and
just to the right of, the imaginary axis. By Jordanâ€™s lemma we close the contour with a
semicircle of inï¬nite radius in the left half of the complex plane. Computing the residues,
Res
cosh[z(l âˆ’x)/c]etz
z cosh(zl/c)
; 0

= lim
zâ†’0
cosh[z(l âˆ’x)/c]etz
cosh(zl/c)
= 1,
(2.4.45)
and
Res
cosh[z(l âˆ’x)/c]etz
z cosh(zl/c)
; zn

= lim
zâ†’zn
(z âˆ’zn) cosh[z(l âˆ’x)/c]etz
z cosh(zl/c)
(2.4.46)
= cosh[(2n âˆ’1)Ï€(l âˆ’x)i/(2l)] exp[Â±(2n âˆ’1)Ï€cti/(2l)]
[(2n âˆ’1)Ï€i/2] sinh[(2n âˆ’1)Ï€i/2]
(2.4.47)
=
2(âˆ’1)n
(2n âˆ’1)Ï€ cos
(2n âˆ’1)Ï€(l âˆ’x)
2l

exp

Â±(2n âˆ’1)Ï€cti
2l

.
(2.4.48)

112
Advanced Engineering Mathematics: A Second Course
Summing the residues and using the relationship that cos(t) = (eti + eâˆ’ti)/2,
V (x, t) = E âˆ’E

1 âˆ’4
Ï€
âˆ
X
n=1
(âˆ’1)n+1
2n âˆ’1 cos
(2n âˆ’1)Ï€(l âˆ’x)
2l

cos
(2n âˆ’1)Ï€ct
2l

(2.4.49)
= 4E
Ï€
âˆ
X
n=1
(âˆ’1)n+1
2n âˆ’1 cos
(2n âˆ’1)Ï€(l âˆ’x)
2l

cos
(2n âˆ’1)Ï€ct
2l

.
(2.4.50)
An alternative to contour integration is to rewrite Equation 2.4.42 as
V (x, s) = E
s
(
1 âˆ’eâˆ’sx/c 
1 + eâˆ’2s(lâˆ’x)/c
1 + eâˆ’2sl/c
#
(2.4.51)
= E
s
h
1 âˆ’eâˆ’sx/c âˆ’eâˆ’s(2lâˆ’x)/c + eâˆ’s(2l+x)/c + Â· Â· Â·
i
(2.4.52)
so that
V (x, t) = E

1 âˆ’H

t âˆ’x
c

âˆ’H

t âˆ’2l âˆ’x
c

+ H

t âˆ’2l + x
c

+ Â· Â· Â·

. (2.4.53)
âŠ“âŠ”
â€¢ Example 2.4.5: The general solution of the equation of telegraphy
In this example we solve the equation of telegraphy without any restrictions on R, C,
G, or L. We begin by eliminating the dependent variable I(x, t) from the set of equations,
Equation 2.4.8 and Equation 2.4.9. This yields
CLâˆ‚2V
âˆ‚t2 + (GL + RC)âˆ‚V
âˆ‚t + RG V = âˆ‚2V
âˆ‚x2 .
(2.4.54)
We next take the Laplace transform of Equation 2.4.54 assuming that V (x, 0) = f(x), and
Vt(x, 0) = g(x). The transformed version of Equation 2.4.54 is
d2V
dx2 âˆ’[CLs2 + (GL + RC)s + RG]V = âˆ’CLg(x) âˆ’(CLs + GL + RC)f(x),
(2.4.55)
or
d2V
dx2 âˆ’(s + Ï)2 âˆ’Ïƒ2
c2
V = âˆ’g(x)
c2
âˆ’
 s
c2 + 2Ï
c2

f(x),
(2.4.56)
where c2 = 1/LC, Ï = c2(RC + GL)/2, and Ïƒ = c2(RC âˆ’GL)/2.
We solve Equation 2.4.56 by Fourier transforms with the requirement that the solution
dies away as |x| â†’âˆ. The most convenient way of expressing this solution is the convolution
product
V (x, s) =
g(x)
c
+
s
c + 2Ï
c

f(x)

âˆ—exp[âˆ’|x|
p
(s + Ï)2 âˆ’Ïƒ2/c]
2
p
(s + Ï)2 âˆ’Ïƒ2
.
(2.4.57)
From a table of Laplace transforms,
Lâˆ’1
"
exp
 âˆ’b
âˆš
s2 âˆ’a2 
âˆš
s2 âˆ’a2
#
= I0

a
p
t2 âˆ’b2

H(t âˆ’b),
(2.4.58)

Advanced Transform Methods
113
where b > 0 and I0(Â·) is the zeroth-order modiï¬ed Bessel function of the ï¬rst kind. There-
fore, by the ï¬rst shifting theorem,
Lâˆ’1
ï£±
ï£²
ï£³
exp
h
âˆ’|x|
p
(s + Ï)2 âˆ’Ïƒ2/c
i
p
(s + Ï)2 âˆ’Ïƒ2
ï£¼
ï£½
ï£¾= eâˆ’ÏtI0
h
Ïƒ
p
t2 âˆ’(x/c)2
i
H

t âˆ’|x|
c

.
(2.4.59)
Using Equation 2.4.59 to invert Equation 2.4.57, we have that
V (x, t) =
1
2ceâˆ’Ïtg(x) âˆ—I0
h
Ïƒ
p
t2 âˆ’(x/c)2
i
H(t âˆ’|x|/c)
+ 1
2ceâˆ’Ïtf(x) âˆ—âˆ‚
âˆ‚t
n
I0[Ïƒ
p
t2 âˆ’(x/c)2]
o
H(t âˆ’|x|/c)
+ Ï
ceâˆ’Ïtf(x) âˆ—I0
h
Ïƒ
p
t2 âˆ’(x/c)2
i
H(t âˆ’|x|/c)
+ 1
2eâˆ’Ït[f(x + ct) + f(x âˆ’ct)].
(2.4.60)
The last term in Equation 2.4.60 arises from noting that sF(s) = L[f(t)] + f(0). If we
explicitly write out the convolution, the ï¬nal form of the solution is
V (x, t) = 1
2eâˆ’Ït[f(x + ct) + f(x âˆ’ct)]
+ 1
2ceâˆ’Ït
Z x+ct
xâˆ’ct
[g(Î·) + 2Ïf(Î·)]I0

Ïƒ
p
c2t2 âˆ’(x âˆ’Î·)2

c

dÎ·
+ 1
2ceâˆ’Ït
Z x+ct
xâˆ’ct
f(Î·) âˆ‚
âˆ‚t

I0

Ïƒ
p
c2t2 âˆ’(x âˆ’Î·)2

c

dÎ·.
(2.4.61)
There is a straightforward physical interpretation of the ï¬rst line of Equation 2.4.61.
It represents damped progressive waves; one is propagating to the right and the other to
the left. In addition to these progressive waves, there is a contribution from the integrals,
even after the waves pass. These integrals include all of the points where f(x) and g(x)
are nonzero within a distance ct from the point in question. This eï¬€ect persists through all
time, although dying away, and constitutes a residue or tail. Figure 2.4.4 illustrates this for
Ï = 0.1, Ïƒ = 0.2, and c = 1. This ï¬gure was obtained using the MATLAB script:
% initialize parameters in calculation
clear; dx = 0.1; dt = 0.5; rho over c = 0.1; sigma over c = 0.2;
X=[-10:dx:10]; T = [0:dt:10]; % compute locations of x and t
for j=1:length(T); t = T(j);
for i=1:length(X); x = X(i);
XX(i,j) = x; TT(i,j) = t; deta i = 0.05 % set up grid
% compute characteristics x+ct and x-ct
characteristic 1 = x - t; characteristic 2 = x + t;
% compute first term in Equation 2.4.61
F = inline(â€™stepfun(x,-1.0001)-stepfun(x,1.0001)â€™);
u(i,j ) = F(characteristic 1) + F(characteristic 2);
% find the upper and lower limits of the integration
upper = characteristic 2; lower = characteristic 1;
if t > 0 & upper > -1 & lower < 1
if upper > 1 upper = 1; end
if lower < -1 lower = -1; end

114
Advanced Engineering Mathematics: A Second Course
âˆ’10
âˆ’5
0
5
10
0
5
10
0
0.2
0.4
0.6
0.8
1
1.2
TIME
DISTANCE
SOLUTION
Figure 2.4.4: The evolution of the voltage with time given by the general equation of telegraphy for initial
conditions and parameters stated in the text.
% set up parameters needed for integration
interval = upper-lower;
NN = interval / deta i;
if mod(NN,2) > 0 NN = NN + 1; end;
deta = interval / NN;
% compute integrals in Equation 2.4.61 by Simpsonâ€™s rule
% sum1 deals with the first integral while sum2 is the second
sum1 = 0; sum2 = 0; eta = lower;
for k = 0:2:NN-2
arg = sigma over c * sqrt(t*t-(x-eta)*(x-eta));
sum1 = sum1 + besseli(0,arg);
if (arg == 0)
sum2 = sum2 + 0.5 * sigma over c * t;
else
sum2 = sum2 + t * besseli(1,arg) / arg; end
eta = eta + deta;
arg = sigma over c * sqrt(t*t-(x-eta)*(x-eta));
sum1 = sum1 + 4*besseli(0,arg);
if (arg == 0)
sum2 = sum2 + 4 * 0.5 * sigma over c * t;
else
sum2 = sum2 + 4 * t * besseli(1,arg) / arg; end
eta = eta + deta;
arg = sigma over c * sqrt(t*t-(x-eta)*(x-eta));
sum1 = sum1 + besseli(0,arg);
if (arg == 0)
sum2 = sum2 + 0.5 * sigma over c * t;

Advanced Transform Methods
115
else
sum2 = sum2 + t * besseli(1,arg) / arg; end
end
u(i,j) = u(i,j) + 2 * rho over c * deta * sum1 / 3 ...
+ sigma over c * deta * sum2 / 3;
end
% multiply final answer by damping coefficient
u(i,j) = 0.5 * exp(-rho over c * t) * u(i,j);
end;end;
% plot results
mesh(XX,TT,real(u)); colormap spring;
xlabel(â€™DISTANCEâ€™,â€™Fontsizeâ€™,20); ylabel(â€™TIMEâ€™,â€™Fontsizeâ€™,20)
zlabel(â€™SOLUTIONâ€™,â€™Fontsizeâ€™,20)
We evaluated the integrals by Simpsonâ€™s rule for the initial conditions f(x) = H(x + 1) âˆ’
H(x âˆ’1), and g(x) = 0. If there was no loss, then two pulses would propagate to the left
and right. However, with resistance and leakage the waves leave a residue after their leading
edge has passed.
Problems
1. Use transform methods to solve the wave equation
âˆ‚2u
âˆ‚t2 = âˆ‚2u
âˆ‚x2 ,
0 < x < 1,
0 < t,
with the boundary conditions u(0, t) = u(1, t) = 0, 0 < t, and the initial conditions u(x, 0) =
0, ut(x, 0) = 1, 0 < x < 1.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
âˆ’s2U(x, s) = âˆ’1,
0 < x < 1,
subject to the boundary conditions U(0, s) = U(1, s) = 1.
Step 2: Show that the solution to the previous step is
U(x, s) = 1 âˆ’cosh(sx)
s2
+ [cosh(s) âˆ’1] sinh(sx)
s2 sinh(s)
.
Step 3: Show that U(x, s) has simple poles at sn = Â±nÏ€i with n = 1, 2, 3, . . . and a
removable pole at s = 0.
Step 4: Use Bromwichâ€™s integral and show that
u(x, t) = 4
Ï€2
âˆ
X
m=1
sin[(2m âˆ’1)Ï€x] sin[(2m âˆ’1)Ï€t]
(2m âˆ’1)2
.
2. Use transform methods to solve the wave equation
âˆ‚2u
âˆ‚t2 = âˆ‚2u
âˆ‚x2 ,
0 < x < 1,
0 < t,

116
Advanced Engineering Mathematics: A Second Course
with the boundary conditions u(0, t) = ux(1, t) = 0, 0 < t, and the initial conditions
u(x, 0) = 0, ut(x, 0) = x, 0 < x < 1.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
âˆ’s2U(x, s) = âˆ’x,
0 < x < 1,
with the boundary condition U(0, s) = U â€²(1, s) = 0.
Step 2: Show that the solution to the previous step is
U(x, s) = xs cosh(s) âˆ’sinh(sx)
s3 cosh(s)
.
Step 3: Show that U(x, s) has simple poles at sn = Â±(2n âˆ’1)Ï€i/2 with n = 1, 2, 3, . . . and
a removable pole at s = 0.
Step 4: Use Bromwichâ€™s integral and show that
u(x, t) = 16
Ï€3
âˆ
X
n=1
(âˆ’1)n+1
(2n âˆ’1)3 sin
(2n âˆ’1)Ï€x
2

sin
(2n âˆ’1)Ï€t
2

.
3. Use transform methods to solve the wave equation
âˆ‚2u
âˆ‚t2 = âˆ‚2u
âˆ‚x2 ,
0 < x < 1,
0 < t,
with the boundary conditions u(0, t) = u(1, t) = 0, 0 < t, and the initial conditions u(x, 0) =
sin(Ï€x), ut(x, 0) = âˆ’sin(Ï€x), 0 < x < 1.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
âˆ’s2U(x, s) = âˆ’s sin(Ï€x) + sin(Ï€x),
0 < x < 1,
with the boundary conditions U(0, s) = U(1, s) = 0.
Step 2: Show that the solution to the previous step is U(x, s) = (s âˆ’1) sin(Ï€x)/(s2 + Ï€2).
Step 3: Inverting by inspection, show that u(x, t) = sin(Ï€x) cos(Ï€t) âˆ’sin(Ï€x) sin(Ï€t)/Ï€.
4. Use transform methods to solve the wave equation
âˆ‚2u
âˆ‚t2 = c2 âˆ‚2u
âˆ‚x2 ,
0 < x < a,
0 < t,
with the boundary conditions u(0, t) = sin(Ï‰t), u(a, t) = 0, 0 < t, and the initial conditions
u(x, 0) = ut(x, 0) = 0, 0 < x < a. Assume that Ï‰a/c is not an integer multiple of Ï€. Why?
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
âˆ’s2
c2 U(x, s) = 0,
0 < x < a,

Advanced Transform Methods
117
with the boundary condition U(0, s) = Ï‰/(s2 + Ï‰2) and U(a, s) = 0.
Step 2: Show that the solution to the previous step is
U(x, s) =
Ï‰ sinh[s(a âˆ’x)/c]
(s2 + Ï‰2) sinh(sa/c).
Step 3: Show that U(x, s) has simple poles at s = Â±Ï‰i and sn = Â±nÏ€ci/a with n =
1, 2, 3, . . . and a removable pole at s = 0.
Step 4: Use Bromwichâ€™s integral and show that
u(x, t) = sin[Ï‰(a âˆ’x)/c]
sin(Ï‰a/c)
sin(Ï‰t) âˆ’2Ï‰a
c
âˆ
X
n=1
sin(nÏ€x/a)
n2Ï€2 âˆ’a2Ï‰2/c2 sin
nÏ€ct
a

.
5. Use transform methods to solve the wave equation
âˆ‚2u
âˆ‚t2 = c2 âˆ‚2u
âˆ‚x2 ,
0 < x < L,
0 < t,
with the boundary conditions ux(0, t) = âˆ’f(t), ux(L, t) = 0, 0 < t, and the initial conditions
u(x, 0) = ut(x, 0) = 0, 0 < x < L.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
âˆ’s2
c2 U(x, s) = 0,
0 < x < L,
with the boundary conditions U â€²(0, s) = âˆ’F(s) and U â€²(L, s) = 0.
Step 2: Show that the solution to the previous step is
U(x, s) = cF(s) cosh[s(L âˆ’x)/c]
s sinh(sL/c)
.
Step 3: Replacing sinh and cosh by their deï¬nitions and expanding the denominator as a
geometric series, show that
U(x, s) = cF(s)
s
h
eâˆ’sx/c + eâˆ’s(2Lâˆ’x)/ci 
1 + eâˆ’2sL/c + eâˆ’4sL/c + Â· Â· Â·

.
Step 4: Multiplying everything out and inverting term by term, show that
u(x, t) = c
âˆ
X
n=0
f(t âˆ’x/c âˆ’2nL/c)H(t âˆ’x/c âˆ’2nL/c)
+ c
âˆ
X
m=1
f(t + x/c âˆ’2mL/c)H(t + x/c âˆ’2mL/c).

118
Advanced Engineering Mathematics: A Second Course
6. Use transform methods to solve the wave equation
âˆ‚2u
âˆ‚t2 = c2 âˆ‚2u
âˆ‚x2 âˆ’qâ€²(t),
a < x < b,
0 < t,
with the boundary conditions u(a, t) = ux(b, t) = 0, 0 < t, and the initial conditions
u(x, 0) = 0, ut(x, 0) = âˆ’q(0), a < x < b.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
c2 d2U(x, s)
dx2
âˆ’s2U(x, s) = sQ(s),
a < x < b,
with the boundary conditions U(a, s) = U â€²(b, s) = 0.
Step 2: Show that the eigenfunctions sin[kn(x âˆ’a)], where kn = (2n + 1)Ï€/[2(b âˆ’a)] and
n = 0, 1, 2, . . ., satisfy the boundary conditions.
Step 3: Expand the right side of the diï¬€erential equation using an eigenfunction expansion
consisting of sin[kn(x âˆ’a)]. Show that
sQ(s) = 4sQ(s)
Ï€
âˆ
X
n=0
1
2n + 1 sin
(2n + 1)Ï€(x âˆ’a)
2(b âˆ’a)

.
Step 4: Assuming that
U(x, s) =
âˆ
X
n=0
An sin
(2n + 1)Ï€(x âˆ’a)
2(b âˆ’a)

,
show by direct substitution that
An = âˆ’4sc2Q(s)
Ï€(2n + 1)

s2 + (2n + 1)2Ï€2c2
4(b âˆ’a)2
âˆ’1
.
Step 5: Invert U(x, s) term by term and show that
u(x, t) = âˆ’4c2
Ï€
âˆ
X
n=0
1
2n + 1 sin
(2n + 1)Ï€(x âˆ’a)
2(b âˆ’a)
 Z t
0
q(Ï„) cos
(2n + 1)Ï€c(t âˆ’Ï„)
2(b âˆ’a)

dÏ„.
7. Use transform methods to solve the wave equation
âˆ‚2u
âˆ‚t2 âˆ’âˆ‚2u
âˆ‚x2 = teâˆ’x,
0 < x < âˆ,
0 < t,
with the boundary conditions u(0, t) = 1 âˆ’eâˆ’t, limxâ†’âˆ|u(x, t)| âˆ¼xn, n ï¬nite, 0 < t, and
the initial conditions u(x, 0) = 0, ut(x, 0) = x, 0 < x < âˆ.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
âˆ’s2U(x, s) = âˆ’x âˆ’eâˆ’x
s2 ,
0 < x < âˆ,

Advanced Transform Methods
119
with the boundary conditions
U(0, s) = 1
s âˆ’
1
s + 1
and
lim
xâ†’âˆ|U(x, s)| âˆ¼xn.
Step 2: Show that the solution to the previous step is
U(x, s) =
1
s âˆ’
1
s + 1 + 1
s2 âˆ’
1
s2 âˆ’1

eâˆ’sx + x
s2 âˆ’eâˆ’x
s2 +
eâˆ’x
s2 âˆ’1.
Step 3: Inverting term by term, show that
u(x, t) = xt âˆ’teâˆ’x + sinh(t)eâˆ’x +
h
1 âˆ’eâˆ’(tâˆ’x) + t âˆ’x âˆ’sinh(t âˆ’x)
i
H(t âˆ’x).
8. Use transform methods to solve the wave equation
âˆ‚2u
âˆ‚t2 âˆ’âˆ‚2u
âˆ‚x2 = xeâˆ’t,
0 < x < âˆ,
0 < t,
with the boundary conditions u(0, t) = cos(t), limxâ†’âˆ|u(x, t)| âˆ¼xn, n ï¬nite, 0 < t, and
the initial conditions u(x, 0) = 1, ut(x, 0) = 0, 0 < x < âˆ.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
âˆ’s2U(x, s) = âˆ’s âˆ’
x
s + 1,
0 < x < âˆ,
with the boundary conditions U(0, s) = s/(s2 + 1) and limxâ†’âˆ|U(x, s)| âˆ¼xn.
Step 2: Show that the solution to the previous step is
U(x, s) =

s
s2 + 1 âˆ’1
s

eâˆ’sx + 1
s + x
s2 âˆ’x
s +
x
s + 1.
Step 3: Inverting term by term, show that u(x, t) = 1+xtâˆ’x+xeâˆ’t+[cos(tâˆ’x)âˆ’1]H(tâˆ’x).
9. Use transform methods to solve the wave equation
âˆ‚2u
âˆ‚t2 = âˆ‚2u
âˆ‚x2 ,
0 < x < L,
0 < t,
with the boundary conditions
u(0, t) = 0,
âˆ‚2u(L, t)
âˆ‚t2
+ k
m
âˆ‚u(L, t)
âˆ‚x
= g,
0 < t,
and the initial conditions u(x, 0) = ut(x, 0) = 0, 0 < x < L, where k, m, and g are constants.

120
Advanced Engineering Mathematics: A Second Course
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
âˆ’s2U(x, s) = 0,
0 < x < L,
with the boundary conditions U(0, s) = 0 and s2U(L, s) + Ï‰2U â€²(L, s) = g/s, where Ï‰2 =
k/m.
Step 2: Show that the solution to the previous step is
g sinh(sx)
U(x, s) =
.
s2[s sinh(sL) + Ï‰2 cosh(sL)]
Step 3: Show that U(x, s) has simple poles at s = 0 and sn = Â±Î»ni, where Î»n = Ï‰2 cot(Î»nL)
with n = 1, 2, 3, . . ..
Step 4: Using Bromwichâ€™s integral, show that
gx
2gÏ‰2
âˆ
sin(Î»nx) cos(Î»nt)
u(x, t) = Ï‰2 âˆ’
L
X
.
Î»2n(Ï‰4 + Ï‰2/L + Î»2n) sin(Î»nL)
n=1
10. Use transform methods23 to solve the wave equation
âˆ‚2u
âˆ‚
= c2
 âˆ‚u
x

,
0 < x < 1,
0 < t,
âˆ‚t2
âˆ‚x
âˆ‚x
with the boundary conditions limx
0 |u(x, t)| < âˆand u(1, t) = A sin(Ï‰t), 0 < t, and
â†’
the initial conditions u(x, 0) = ut(x, 0) = 0, 0 < x < 1. Assume that 2Ï‰ = cÎ²n, where
J0(Î²n) = 0.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d  dU(x, s)
x

s2
âˆ’
U(x, s) = 0,
0 < x < 1.
dx
dx
c2
with the boundary conditions lim
2
2
xâ†’0 |U(x, s)| < âˆand U(1, s) = AÏ‰/(s + Ï‰ ).
Step 2: Show that the solution to the previous step is
AÏ‰
I0(2sâˆšx/c)
U(x, s) =
.
s2 + Ï‰2
I0(2s/c)
Step 3: Show that U(x, s) has simple poles at s = Â±Ï‰i and sn = Â±cÎ²ni/2, where J0(Î²n) = 0,
n = 1, 2, 3, ...
23 Suggested by a problem solved by Brown, J., 1975: Stresses in towed cables during re-entry. J. Spacecr.
Rockets, 12, 524â€“527.
Ì¸

Advanced Transform Methods
121
Step 4: Using Bromwichâ€™s integral, show that
J0(2Ï‰âˆšx/c)
X
âˆJ0(Î²n
âˆšx ) sin(Î²nct/2)
u(x, t) = A
sin(Ï‰t) + AcÏ‰
.
J0(2Ï‰/c)
(Ï‰2
c
=
âˆ’
2Î²2n/4)J1(Î²n)
n
1
11. A lossless transmission line of length â„“has a constant voltage E applied to the end
x = 0 while we insulate the other end [Vx(â„“, t) = 0]. Find the voltage at any point on the
line if the initial current and charge are zero.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2U(x, s)
s2
âˆ’
U(x, s) = 0,
0 < x < â„“,
dx2
c2
with the boundary conditions U(0, s) = E/s and U â€²(â„“, s) = 0.
Step 2: Show that the solution to the previous step is
E cosh[s(â„“âˆ’x)/c]
U(x, s) =
.
s cosh(sâ„“/c)
Step 3: Show that U(x, s) has simple poles at s = 0 and sn = Â±(2n âˆ’1)cÏ€i/(2â„“) with
n = 1, 2, 3, . . .
Step 4: Using Bromwichâ€™s integral, show that
4E
âˆ
1
(2n
1)Ï€x
(2n
1)cÏ€t
u(x, t) = E
in

âˆ’
âˆ’
X
s

cos
âˆ’
Ï€
2n
1
2â„“
n=1
âˆ’

2â„“

.
Step 5: An alternative approach is to replace the hyperbolic functions with their exponential
deï¬nitions. Then,
E
U(x, s) =
h
eâˆ’sx/c âˆ’eâˆ’s(x+2â„“)/c + eâˆ’s(x+4â„“)/c
s
âˆ’Â· Â· Â·
E
+
h
eâˆ’s(2â„“âˆ’x)/c âˆ’eâˆ’s(4â„“âˆ’x)/c + eâˆ’s(6â„“âˆ’x)/c
i
s
âˆ’Â· Â· Â·
i
after using the summation rule for the geometric series. Take the inverse by inspection and
show that
âˆ
u(x, t) = E
n
X
x + 2nâ„“
(
=0
âˆ’1)nH

t âˆ’
c

+ E
n
X
âˆ
n

[(2n + 2)â„“
(
=0
âˆ’1) H
t
âˆ’x]
âˆ’
c

.
12. Solve the equation of telegraphy without leakage
âˆ‚2u
âˆ‚u
âˆ‚2u
= CR
+ CL
,
0 < x < â„“,
0 < t,
âˆ‚x2
âˆ‚t
âˆ‚t2

122
Advanced Engineering Mathematics: A Second Course
subject to the boundary conditions u(0, t) = 0, u(â„“, t) = E, 0 < t, and the initial conditions
u(x, 0) = ut(x, 0) = 0, 0 < x < â„“. Assume that 4Ï€2L/CR2â„“2 > 1. Why?
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2U(x, s)
s2
âˆ’
U(x, s) = 0,
0 < x < â„“,
dx2
c2
with the boundary conditions U(0, s) = E/s and U â€²(â„“, s) = 0.
Step 2: Show that the solution to the previous step is
E cosh[s(â„“âˆ’x)/c]
U(x, s) =
.
s cosh(sâ„“/c)
Step 3: Show that U(x, s) has simple poles at s = 0 and sn = Â±(2n âˆ’1)cÏ€i/(2â„“) with
n = 1, 2, 3, . . .
Step 4: Using Bromwichâ€™s integral, show that
u(x, t)
x
2
=
E
â„“âˆ’
eâˆ’t/2T
Ï€
n
X
âˆ(âˆ’1)n
i
2T )
s n
nÏ€x s n(t
âˆš
n2Î´2
i
âˆ’1/
âˆš
+cos
n
â„“
n2Î´2
1
âˆ’1
=

t
p
n2Î´2 âˆ’1/2T

.
13. The pressure and velocity oscillations from water hammer in a pipe without friction24
are given by the equations
âˆ‚p
2 âˆ‚u
âˆ‚u
1 âˆ‚p
=
c
t
âˆ’Ï
,
and
=
âˆ‚
âˆ‚x
âˆ‚t
âˆ’
,
Ï âˆ‚x
where p(x, t) denotes the pressure perturbation, u(x, t) is the velocity perturbation, c is
the speed of sound in water, and Ï is the density of water. These two ï¬rst-order partial
diï¬€erential equations can be combined to yield
âˆ‚2p
p
= c2 âˆ‚2
,
0 < x < L,
0 < t.
âˆ‚t2
âˆ‚x2
Find the solution to this partial diï¬€erential equation if p(0, t) = p0, and u(L, t) = 0, and
the initial conditions are p(x, 0) = p0, pt(x, 0) = 0, and u(x, 0) = u0.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2P(x, s)
s2
s
âˆ’
P(x, s) = âˆ’
p0,
0 < x < L,
dx2
c2
c2
with the boundary conditions P(0, s) = p0/s and P â€²(L, s) = Ïu0.
Step 2: Show that the solution to the previous step is
p0
Ïu0c sinh(sx/c)
P(x, s) =
+
.
s
s cosh(sL/c)
24 See Rich, G. R., 1945: Water-hammer analysis by the Laplace-Mellin transformation. Trans. ASME,
67, 361â€“376.

Advanced Transform Methods
123
Step 3: Show that P(x, s) has simple poles at sn = Â±(2n âˆ’1)cÏ€i/(2L) with n = 1, 2, 3, . . .
and a removable singularity at s = 0.
Step 4: Using Bromwichâ€™s integral, show that
p(x, t) = p0 âˆ’4Ïu0c
Ï€
âˆ
X
n=1
(âˆ’1)n
2n âˆ’1 sin
(2n âˆ’1)Ï€x
2L

sin
(2n âˆ’1)cÏ€t
2L

.
14. Use Laplace transforms to solve the wave equation25
âˆ‚2u
âˆ‚t2 = c2
âˆ‚2u
âˆ‚r2 + 2
r
âˆ‚u
âˆ‚r âˆ’2u
r2

,
a < r < âˆ,
0 < t,
subject to the boundary conditions that
u(a, t) = A

1 âˆ’eâˆ’ct/a
H(t),
lim
râ†’âˆu(r, t) â†’0,
0 < t,
and the initial conditions that u(r, 0) = ut(r, 0) = 0, a < r < âˆ.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2U(r, s)
dr2
+ 2
r
dU(r, s)
dr
âˆ’2
r2 U(r, s) âˆ’s2
c2 U(r, s) = 0,
a < r < âˆ,
with the boundary condition
U(a, s) = A
s âˆ’
A
s + c/a,
lim
râ†’âˆ|U(r, s)| < âˆ.
Step 2: Show that the solution to the previous step is
U(r, s) = A
 a2
sr2 âˆ’
a2
r2(s + c/a) âˆ’c
a
a2
r2 âˆ’a
r

1
(s + c/a)2

eâˆ’s(râˆ’a)/c.
Step 3: Use tables and the second shifting theorem to show that
u(r, t) = A
a2
r2 âˆ’
a2
r2 + cÏ„
a
a2
r2 âˆ’a
r

eâˆ’cÏ„/a

H(Ï„),
where Ï„ = t âˆ’(r âˆ’a)/c.
15. Use Laplace transforms to solve the wave equation26
âˆ‚2(ru)
âˆ‚t2
= c2 âˆ‚2(ru)
âˆ‚r2
,
a < r < âˆ,
0 < t,
25 Wolf, J. P., and G. R. Darbre, 1986: Time-domain boundary element method in visco-elasticity with
application to a spherical cavity. Soil Dynam. Earthq. Eng., 5, 138â€“148.
26 Originally solved using Fourier transforms by Sharpe, J. A., 1942: The production of elastic waves by
explosion pressures. I. Theory and empirical ï¬eld observations. Geophysics, 7, 144â€“154.

124
Advanced Engineering Mathematics: A Second Course
subject to the boundary conditions that
âˆ’Ïc2
âˆ‚2u
âˆ‚r2 + 2
3r
âˆ‚u
âˆ‚r

r=a
= p0eâˆ’Î±tH(t),
lim
râ†’âˆu(r, t) â†’0,
0 < t,
where Î± > 0, and the initial conditions that u(r, 0) = ut(r, 0) = 0, a < r < âˆ.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2[rU(r, s)]
dr2
âˆ’s2
c2 [rU(r, s)] = 0,
a < r < âˆ.
with the boundary condition
âˆ’Ïc2
d2U(a, s)
dr2
+ 2
3a
dU(a, s)
dr

=
p0
s + Î±
and
lim
râ†’âˆ|U(r, s)| < âˆ.
Step 2: Show that the solution to the previous step is
U(r, s) = âˆ’
ap0 exp[âˆ’s(r âˆ’a)/c]
Ïr(s + Î±)[s2 + 4sc/(3a) + 4c2/(3a2)].
Step 3: Show that U(r, s) has three simple poles, s = âˆ’Î± and s = âˆ’Î²/
âˆš
2 Â± Î²i, where
Î² = 2
âˆš
2c/(3a).
Step 4: Use Bromwichâ€™s integral and show that
u(r, t) =
ap0
Ïr[(Î²/
âˆš
2 âˆ’Î±)2 + Î²2]

eâˆ’Î²Ï„/
âˆš
2
 1
âˆš
2 âˆ’Î±
Î²

sin(Î²Ï„) + cos(Î²Ï„)

âˆ’eâˆ’Î±Ï„

H(Ï„),
where Ï„ = t âˆ’(r âˆ’a)/c.
16. Consider a vertical rod or column of length L that is supported at both ends. The
elastic waves that arise when the support at the bottom is suddenly removed are governed
by the wave equation27
âˆ‚2u
âˆ‚t2 = c2 âˆ‚2u
âˆ‚x2 + g,
0 < x < L,
0 < t,
where g denotes the gravitational acceleration, c2 = E/Ï, E is Youngâ€™s modulus, and Ï is the
mass density. Find the wave solution if the boundary conditions are ux(0, t) = ux(L, t) = 0,
0 < t, and the initial conditions are
u(x, 0) = âˆ’gx2
2c2 ,
âˆ‚u(x, 0)
âˆ‚t
= 0,
0 < x < L.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
âˆ’s2
c2 U(x, s) = sgx2
2c4 âˆ’g
sc2 ,
0 < x < L,
27 See Hall, L. H., 1953: Longitudinal vibrations of a vertical column by the method of Laplace transform.
Am. J. Phys., 21, 287â€“292.

Advanced Transform Methods
125
with U â€²(0, s) = U â€²(L, s) = 0.
Step 2: Show that the solution to the previous step is
U(x, s) = gL cosh(sx/c)
cs2 sinh(sL/c) âˆ’gx2
2sc2 .
Step 3: Show that U(x, s) has poles that are located at s = 0 and sn = Â±nÏ€ci/L, where
n = 1, 2, 3, . . ..
Step 4: Use Bromwichâ€™s integral and show that
u(x, t) = gt2
2 âˆ’gL2
6c2 âˆ’2gL2
c2Ï€2
âˆ
X
n=1
(âˆ’1)n
n2
cos
nÏ€x
L

cos
nÏ€ct
L

.
17. Use Laplace transforms to solve the hyperbolic equation
âˆ‚2u
âˆ‚t2 âˆ’âˆ‚2u
âˆ‚x2 + 1 = 0,
0 < x < 1,
0 < t,
subject to the boundary conditions that ux(0, t) = 0, ux(1, t) = 1, 0 < t, and the initial
conditions that u(x, 0) = ut(x, 0) = 0, 0 < x < 1.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
âˆ’s2U(x, s) = 1
s + x2 âˆ’1,
0 < x < 1,
with U â€²(0, s) = 0 and U â€²(1, s) = 1/s.
Step 2: Show that the solution to the previous step is
U(x, s) = 1 âˆ’x2
s2
âˆ’1
s3 âˆ’2
s4 + cosh(sx)
s2 sinh(s) + 2 cosh(sx)
s3 sinh(s) .
Step 3: Show that U(x, s) has poles that are located at s = 0 and zn = Â±nÏ€i, where
n = 1, 2, 3, . . ..
Step 4: Use Bromwichâ€™s integral and show that
u(x, t) = 2t
3 + x2
2 âˆ’1
6 âˆ’2
âˆ
X
n=1
(âˆ’1)n cos(nÏ€x)
cos(nÏ€t)
n2Ï€2
+ 2 sin(nÏ€t)
n3Ï€3

.
18. Solve the telegraph-like equation28
âˆ‚2u
âˆ‚t2 + k âˆ‚u
âˆ‚t = c2
âˆ‚2u
âˆ‚x2 + Î±âˆ‚u
âˆ‚x

,
0 < x < âˆ,
0 â‰¤t,
28 See Abbott, M. R., 1959: The downstream eï¬€ect of closing a barrier across an estuary with particular
reference to the Thames. Proc. R. Soc. London, Ser. A, 251, 426â€“439.

126
Advanced Engineering Mathematics: A Second Course
subject to the boundary conditions ux(0, t) = âˆ’u0Î´(t), limxâ†’âˆu(x, t) â†’0, 0 â‰¤t, and the
initial conditions u(x, 0) = u0, ut(x, 0) = 0, 0 < x < âˆ, with Î±c > k. Here Î´(t) denotes the
Dirac delta function.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
+ Î±dU(x, s)
dx
âˆ’
s2 + ks
c2

U(x, s) = âˆ’
s + k
c2

u0,
0 < x < âˆ,
with U â€²(0, s) = âˆ’u0, and limxâ†’âˆU(x, s) â†’0.
Step 2: Show that the solution to the previous step is
U(x, s) = u0
s + u0eâˆ’Î±x/2
exp

âˆ’x
q s + k
2
2 + a2/c

Î±
2 +
q
(s + k
2)2 + a2/c
,
where 4a2 = Î±2c2 âˆ’k2 > 0.
Step 3: Using the ï¬rst and second shifting theorems and the property that
F
p
s2 + a2

= L
"
f(t) âˆ’a
Z t
0
J1
 a
âˆš
t2 âˆ’Ï„ 2 
âˆš
t2 âˆ’Ï„ 2
Ï„f(Ï„) dÏ„
#
,
show that
u(x, t) = u0 + u0ceâˆ’kt/2H(t âˆ’x/c)
"
eâˆ’Î±ct/2 âˆ’a
Z t
x/c
J1
 a
âˆš
t2 âˆ’Ï„ 2 
âˆš
t2 âˆ’Ï„ 2
Ï„eâˆ’Î±cÏ„/2dÏ„
#
.
19. As an electric locomotive travels down a track at the speed V , the pantograph (the
metallic framework that connects the overhead power lines to the locomotive) pushes up
the line with a force P. Let us ï¬nd the behavior29 of the overhead wire as a pantograph
passes between two supports of the electrical cable that are located a distance L apart. We
model this system as a vibrating string with a point load:
âˆ‚2u
âˆ‚t2 = c2 âˆ‚2u
âˆ‚x2 + P
ÏV Î´

t âˆ’x
V

,
0 < x < L,
0 < t.
Let us assume that the wire is initially at rest [u(x, 0) = ut(x, 0) = 0 for 0 < x < L] and
ï¬xed at both ends [u(0, t) = u(L, t) = 0 for 0 < t].
Step 1: Take the Laplace transform of the partial diï¬€erential equation and show that
s2U(x, s) = c2 d2U(x, s)
dx2
+ P
ÏV eâˆ’xs/V ,
0 < x < L.
29 See Oda, O., and Y. Ooura, 1976: Vibrations of catenary overhead wire. Q. Rep., (Tokyo) Railway
Tech. Res. Inst., 17, 134â€“135.

Advanced Transform Methods
127
Step 2: Solve the ordinary diï¬€erential equation in Step 1 as a Fourier half-range sine series
U(x, s) =
âˆ
X
n=1
Bn(s) sin
nÏ€x
L

,
where
Bn(s) =
2PÎ²n
ÏL(Î²2n âˆ’Î±2n)

1
s2 + Î±2n
âˆ’
1
s2 + Î²2n
 h
1 âˆ’(âˆ’1)neâˆ’Ls/V i
,
Î±n = nÏ€c/L and Î²n = nÏ€V/L. This solution satisï¬es the boundary conditions.
Step 3: By inverting the solution in Step 2, show that
u(x, t) = 2P
ÏL
âˆ
X
n=1
sin(Î²nt)
Î±2n âˆ’Î²2n
âˆ’V
c
sin(Î±nt)
Î±2n âˆ’Î²2n

sin
nÏ€x
L

âˆ’2P
ÏL H

t âˆ’L
V
 âˆ
X
n=1
(âˆ’1)n sin
nÏ€x
L
 sin[Î²n(t âˆ’L/V )]
Î±2n âˆ’Î²2n
âˆ’V
c
sin[Î±n(t âˆ’L/V )]
Î±2n âˆ’Î²2n

,
or
u(x, t) = 2P
ÏL
âˆ
X
n=1
sin(Î²nt)
Î±2n âˆ’Î²2n
âˆ’V
c
sin(Î±nt)
Î±2n âˆ’Î²2n

sin
nÏ€x
L

âˆ’2P
ÏL H

t âˆ’L
V
 âˆ
X
n=1
sin
nÏ€x
L
 sin(Î²nt)
Î±2n âˆ’Î²2n
âˆ’V
c (âˆ’1)n sin[Î±n(t âˆ’L/V )]
Î±2n âˆ’Î²2n

.
The ï¬rst term in both summations represents the static uplift on the line; this term dis-
appears after the pantograph passes. The second term in both summations represents the
vibrations excited by the traveling force. Even after the pantograph passes, they continue
to exist.
20. Solve the wave equation
1
c2
âˆ‚2u
âˆ‚t2 âˆ’âˆ‚2u
âˆ‚r2 âˆ’1
r
âˆ‚u
âˆ‚r + u
r2 = Î´(r âˆ’Î±)
Î±2
,
0 â‰¤r < a,
0 < t,
where 0 < Î± < a, subject to the boundary conditions limrâ†’0 |u(r, t)| < âˆ, ur(a, t) +
h u(a, t)/a = 0, 0 < t, and the initial conditions u(r, 0) = ut(r, 0) = 0, 0 â‰¤r < a.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and show that
d2U(r, s)
dr2
+ 1
r
dU(r, s)
dr
âˆ’
s2
c2 + 1
r2

U(r, s) = âˆ’Î´(r âˆ’Î±)
sÎ±2
,
0 â‰¤r < a,
with limrâ†’0 |U(r, s)| < âˆand U â€²(a, s) + h
aU(a, s) = 0.
Step 2: Show that the Dirac delta function can be reexpressed as the Fourier-Bessel series
Î´(r âˆ’Î±) = 2Î±
a2
âˆ
X
n=1
Î²2
n J1(Î²nÎ±/a)
(Î²2n + h2 âˆ’1)J2
1(Î²n)J1(Î²nr/a),
0 â‰¤r < a,
where Î²n is the nth root of Î²Jâ€²
1(Î²) + h J1(Î²) = Î²J0(Î²) + (h âˆ’1)J1(Î²) = 0 and J0(Â·), J1(Â·)
are the zeroth and ï¬rst-order Bessel functions of the ï¬rst kind, respectively.

128
Advanced Engineering Mathematics: A Second Course
Step 3: Show that the solution to the ordinary diï¬€erential equation in Step 1 is
U(r, s) = 2
Î±
âˆ
X
n=1
J1(Î²nÎ±/a)J1(Î²nr/a)
(Î²2n + h2 âˆ’1) J2
1(Î²n)
1
s âˆ’
s
s2 + c2Î²2n/a2

.
Note that this solution satisï¬es the boundary conditions.
Step 4: Taking the inverse of the Laplace transform in Step 3, show that the solution to
the partial diï¬€erential equation is
u(r, t) = 2
Î±
âˆ
X
n=1
J1(Î²nÎ±/a)J1(Î²nr/a)
(Î²2n + h2 âˆ’1) J2
1(Î²n)

1 âˆ’cos
cÎ²nt
a

.
21. Solve the hyperbolic equation
âˆ‚2u
âˆ‚xâˆ‚t + u = 0,
0 < x, t,
subject to the boundary conditions u(0, t) = eâˆ’t, limxâ†’âˆu(x, t) â†’0, 0 < t, and u(x, 0) = 1,
limtâ†’âˆ|u(x, t)| < Mekt, 0 < k, M, x, t.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and show that
sdU(x, s)
dx
+ U = 0,
0 < x < âˆ,
with U(0, s) = 1/(s + 1) and limxâ†’âˆU(x, s) â†’0.
Step 2: Show that
U(x, s) = eâˆ’x/s
s + 1 = eâˆ’x/s
s
âˆ’
eâˆ’x/s
s(s + 1).
Step 3: Using tables and the convolution theorem, show that the solution is
u(x, t) = J0(2
âˆš
xt ) âˆ’eâˆ’t
Z t
0
eÏ„J0(2âˆšxÏ„ ) dÏ„,
where J0(Â·) is the Bessel function of the ï¬rst kind and order zero.
22. Solve the hyperbolic equation
âˆ‚2u
âˆ‚xâˆ‚t + aâˆ‚u
âˆ‚t + bâˆ‚u
âˆ‚x = 0,
0 < a, b, x, t,
subject to the boundary conditions u(0, t) = ect, limxâ†’âˆu(x, t) â†’0, 0 < t, and the initial
conditions u(x, 0) = 1, limtâ†’âˆ|u(x, t)| < Mekt, 0 < k, M, t, x.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and show that
(s + b)dU(x, s)
dx
+ asU = a,
0 < x < âˆ,
with U(0, s) = 1/(s âˆ’c) and limxâ†’âˆU(x, s) â†’0.

Advanced Transform Methods
129
Step 2: Show that
U(x, s) = 1
s + c eâˆ’ax
s(s âˆ’c) exp
 bx
s + b

.
Step 3: Using tables, the ï¬rst shifting theorem, and the convolution theorem, show that
the solution is
u(x, t) = 1 + c ectâˆ’ax
Z t
0
eâˆ’(b+c)Ï„I0

2
âˆš
bxÏ„

dÏ„,
where I0(Â·) is the modiï¬ed Bessel function of the ï¬rst kind and order zero.
2.5 THE SOLUTION OF THE HEAT EQUATION BY USING LAPLACE TRANSFORMS
In the previous section we showed that we can solve the wave equation by the method
of Laplace transforms. This is also true for the heat equation. Once again, we take the
Laplace transform with respect to time. From the deï¬nition of Laplace transforms,
L[u(x, t)] = U(x, s),
(2.5.1)
L[ut(x, t)] = sU(x, s) âˆ’u(x, 0),
(2.5.2)
and
L[uxx(x, t)] = d2U(x, s)
dx2
.
(2.5.3)
We next solve the resulting ordinary diï¬€erential equation, known as the auxiliary equation,
along with the corresponding Laplace transformed boundary conditions. The initial condi-
tion gives us the value of u(x, 0). The ï¬nal step is the inversion of the Laplace transform
U(x, s). We typically use the inversion integral.
â€¢ Example 2.5.1
To illustrate these concepts, we solve a heat conduction problem30 in a plane slab of
thickness 2L. Initially the slab has a constant temperature of unity. For 0 < t, we allow
both faces of the slab to radiatively cool in a medium that has a temperature of zero.
If u(x, t) denotes the temperature, a2 is the thermal diï¬€usivity, h is the relative emis-
sivity, t is the time, and x is the distance perpendicular to the face of the slab and measured
from the middle of the slab, then the governing equation is
âˆ‚u
âˆ‚t = a2 âˆ‚2u
âˆ‚x2 ,
âˆ’L < x < L,
0 < t,
(2.5.4)
with the initial condition
u(x, 0) = 1,
âˆ’L < x < L,
(2.5.5)
and boundary conditions
âˆ‚u(L, t)
âˆ‚x
+ hu(L, t) = 0,
âˆ‚u(âˆ’L, t)
âˆ‚x
+ hu(âˆ’L, t) = 0,
0 < t.
(2.5.6)
30 Goldstein, S., 1932: The application of Heavisideâ€™s operational method to the solution of a problem
in heat conduction. Z. Angew. Math. Mech., 12, 234â€“243.

130
Advanced Engineering Mathematics: A Second Course
Taking the Laplace transform of Equation 2.5.4 and substituting the initial condition,
a2 d2U(x, s)
dx2
âˆ’sU(x, s) = âˆ’1.
(2.5.7)
If we write s = a2q2, Equation 2.5.7 becomes
d2U(x, s)
dx2
âˆ’q2U(x, s) = âˆ’1
a2 .
(2.5.8)
From the boundary conditions, U(x, s) is an even function in x and we may conveniently
write the solution as
U(x, s) = 1
s + A cosh(qx).
(2.5.9)
From Equation 2.5.6,
qA sinh(qL) + h
s + hA cosh(qL) = 0,
(2.5.10)
and
U(x, s) = 1
s âˆ’
h cosh(qx)
s[q sinh(qL) + h cosh(qL)].
(2.5.11)
The inverse of U(x, s) consists of two terms. The ï¬rst term is simply unity. We will
invert the second term by contour integration.
We begin by examining the nature and location of the singularities in the second term.
Using the product formulas for the hyperbolic cosine and sine functions, the second term
equals
h

1 + 4q2x2
Ï€2
 
1 + 4q2x2
9Ï€2

Â· Â· Â·
s

q2L

1 + q2L2
Ï€2
 
1 + q2L2
4Ï€2

Â· Â· Â· + h

1 + 4q2L2
Ï€2
 
1 + 4q2L2
9Ï€2

Â· Â· Â·
.
(2.5.12)
Because q2 = s/a2, Equation 2.5.12 shows that we do not have any âˆšs in the transform
and we need not concern ourselves with branch points and cuts. Furthermore, we have only
simple poles: one located at s = 0 and the others where
q sinh(qL) + h cosh(qL) = 0.
(2.5.13)
If we set q = iÎ», Equation 2.5.13 becomes
h cos(Î»L) âˆ’Î» sin(Î»L) = 0,
or
Î»L tan(Î»L) = hL.
(2.5.14)
From Bromwichâ€™s integral,
Lâˆ’1

h cosh(qx)
s[q sinh(qL) + h cosh(qL)]

=
1
2Ï€i
I
C
h cosh(qx)etz
z[q sinh(qL) + h cosh(qL)] dz,
(2.5.15)

Advanced Transform Methods
131
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
0
0.2
0.4
0.6
0.8
1
1.2
distance
time
U(X,T)
Figure 2.5.1: The temperature within the portion of a slab 0 < x/L < 1 at various times a2t/L2 if the
faces of the slab radiate to free space at temperature zero and the slab initially has the temperature 1. The
parameter hL = 1.
where q = z1/2/a and the closed contour C consists of Bromwichâ€™s contour plus a semicircle
of inï¬nite radius in the left half of the z-plane. The residue at z = 0 is 1 while at zn = âˆ’a2Î»2
n,
Res

h cosh(qx)etz
z[q sinh(qL) + h cosh(qL)]; zn

= lim
zâ†’zn
h(z + a2Î»2
n) cosh(qx)etz
z[q sinh(qL) + h cosh(qL)]
(2.5.16)
= lim
zâ†’zn
h cosh(qx)etz
z[(1 + hL) sinh(qL) + qL cosh(qL)]/(2a2q)
(2.5.17)
=
2ha2Î»ni cosh(iÎ»nx) exp(âˆ’Î»2
na2t)
(âˆ’a2Î»2n)[(1 + hL)i sin(Î»nL) + iÎ»nL cos(Î»nL)]
(2.5.18)
= âˆ’
2h cos(Î»nx) exp(âˆ’a2Î»2
nt)
Î»n[(1 + hL) sin(Î»nL) + Î»nL cos(Î»nL)].
(2.5.19)
Therefore, the inversion of U(x, s) is
u(x, t) = 1 âˆ’

1 âˆ’2h
âˆ
X
n=1
cos(Î»nx) exp(âˆ’a2Î»2
nt)
Î»n[(1 + hL) sin(Î»nL) + Î»nL cos(Î»nL)]

,
(2.5.20)
or
u(x, t) = 2h
âˆ
X
n=1
cos(Î»nx) exp(âˆ’a2Î»2
nt)
Î»n[(1 + hL) sin(Î»nL) + Î»nL cos(Î»nL)].
(2.5.21)
We can further simplify Equation 2.5.21 by using h/Î»n = tan(Î»nL). This yields hL =
Î»nL tan(Î»nL). Substituting these relationships into Equation 2.5.21 and simplifying,
u(x, t) = 2
âˆ
X
n=1
sin(Î»nL) cos(Î»nx) exp(âˆ’a2Î»2
nt)
Î»nL + sin(Î»nL) cos(Î»nL)
.
(2.5.22)
Figure 2.5.1 illustrates Equation 2.5.22. It was created using the MATLAB script

132
Advanced Engineering Mathematics: A Second Course
clear
hL = 1; m = 0; M = 100; dx = 0.05; dt = 0.05;
% create initial guess at zero n
zero = zeros(length(M));
for n = 1:10000
k1 = 0.1*n; k2 = 0.1*(n+1);
prod = k1 * tan(k1); y1 = hL - prod; y2 = hL - k2 * tan(k2);
if (y1*y2 <= 0 & prod < 2 & m < M) m = m+1; zero(m) = k1; end;
end;
% use Newton-Raphson method to improve values of zero n
for n = 1:M; for k = 1:10
f = hL - zero(n) * tan(zero(n));
fp = - tan(zero(n)) - zero(n) * sec(zero(n))^2;
zero(n) = zero(n) - f / fp;
end; end;
% compute Fourier coefficients
for m = 1:M
a(m) = 2 * sin(zero(m)) / (zero(m) + sin(zero(m))*cos(zero(m)));
end
% compute grid and initialize solution
X = [0:dx:1]; T = [0:dt:2];
u = zeros(length(T),length(X));
XX = repmat(X,[length(T) 1]); TT = repmat(Tâ€™,[1 length(X)]);
% compute solution from Equation 2.5.22
for m = 1:M
u = u + a(m) * cos(zero(m)*XX) .* exp(-zero(m)*zero(m)*TT);
end
surf(XX,TT,u)
xlabel(â€™distanceâ€™,â€™Fontsizeâ€™,20); ylabel(â€™timeâ€™,â€™Fontsizeâ€™,20)
zlabel(â€™U(X,T)â€™,â€™Fontsizeâ€™,20)
âŠ“âŠ”
â€¢ Example 2.5.2: Heat dissipation in disc brakes
Disc brakes consist of two blocks of frictional material known as pads that press against
each side of a rotating annulus, usually made of a ferrous material. In this problem we deter-
mine the transient temperatures reached in a disc brake during a single brake application.31
If we ignore the errors introduced by replacing the cylindrical portion of the drum by a
rectangular plate, we can model our disc brakes as a one-dimensional solid, which friction
heats at both ends. Assuming symmetry about x = 0, the boundary condition there is
ux(0, t) = 0. To model the heat ï¬‚ux from the pads, we assume a uniform disc deceleration
that generates heat from the frictional surfaces at the rate N(1 âˆ’Mt), where M and N are
experimentally determined constants.
If u(x, t), Îº, and a2 denote the temperature, thermal conductivity, and diï¬€usivity of
the rotating annulus, respectively, then the heat equation is
âˆ‚u
âˆ‚t = a2 âˆ‚2u
âˆ‚x2 ,
0 < x < L,
0 < t,
(2.5.23)
31 From Newcomb, T. P., 1958: The ï¬‚ow of heat in a parallel-faced inï¬nite solid. Brit. J. Appl. Phys.,
9, 370â€“372. See also Newcomb, T. P., 1958/59: Transient temperatures in brake drums and linings. Proc.
Inst. Mech. Eng., Auto. Div., 227â€“237; Newcomb, T. P., 1959: Transient temperatures attained in disk
brakes. Brit. J. Appl. Phys., 10, 339â€“340.

Advanced Transform Methods
133
with the boundary conditions
âˆ‚u(0, t)
âˆ‚x
= 0,
Îºâˆ‚u(L, t)
âˆ‚x
= N(1 âˆ’Mt),
0 < t.
(2.5.24)
The boundary condition at x = L gives the frictional heating of the disc pads.
Introducing the Laplace transform of u(x, t), deï¬ned as
U(x, s) =
Z âˆ
0
u(x, t)eâˆ’st dt,
(2.5.25)
the equation to be solved becomes
d2U
dx2 âˆ’s
a2 U = 0,
(2.5.26)
subject to the boundary conditions that
dU(0, s)
dx
= 0,
and
dU(L, s)
dx
= N
Îº
1
s âˆ’M
s2

.
(2.5.27)
The solution of Equation 2.5.26 is
U(x, s) = A cosh(qx) + B sinh(qx),
(2.5.28)
where q = s1/2/a. Using the boundary conditions, the solution becomes
U(x, s) = N
Îº
1
s âˆ’M
s2
 cosh(qx)
q sinh(qL).
(2.5.29)
It now remains to invert the transform, Equation 2.5.29.
We will invert cosh(qx)/
[sq sinh(qL)]; the inversion of the second term follows by analog.
Our ï¬rst concern is the presence of s1/2 because this is a multivalued function. However,
when we replace the hyperbolic cosine and sine functions with their Taylor expansions,
cosh(qx)/[sq sinh(qL)] contains only powers of s and is, in fact, a single-valued function.
From Bromwichâ€™s integral,
Lâˆ’1
 cosh(qx)
sq sinh(qL)

=
1
2Ï€i
Z c+âˆi
câˆ’âˆi
cosh(qx)etz
zq sinh(qL) dz,
(2.5.30)
where q = z1/2/a. Just as in the previous example, we replace the hyperbolic cosine and
sine with their product expansion to determine the nature of the singularities. The point
z = 0 is a second-order pole. The remaining poles are located where z1/2
n
L/a = nÏ€i, or
zn = âˆ’n2Ï€2a2/L2, where n = 1, 2, 3, . . .. We have chosen the positive sign because z1/2
must be single-valued; if we had chosen the negative sign, the answer would have been the
same. Our expansion also shows that the poles are simple.
Having classiï¬ed the poles, we now close Bromwichâ€™s contour, which lies slightly to the
right of the imaginary axis, with an inï¬nite semicircle in the left half-plane, and use the

134
Advanced Engineering Mathematics: A Second Course
residue theorem. The values of the residues are
Res
cosh(qx)etz
zq sinh(qL) ; 0

= 1
1! lim
zâ†’0
d
dz
(z âˆ’0)2 cosh(qx)etz
zq sinh(qL)

(2.5.31)
= lim
zâ†’0
d
dz
z cosh(qx)etz
q sinh(qL)

(2.5.32)
= a2
L lim
zâ†’0
d
dz
z
h
1 + zx2
2!a2 + Â· Â· Â·
i h
1 + tz + t2z2
2! + Â· Â· Â·
i
z + L2z2
3!a2 + Â· Â· Â·

(2.5.33)
= a2
L lim
zâ†’0
d
dz

1 + tz + zx2
2a2 âˆ’zL2
3!a2 + Â· Â· Â·

(2.5.34)
= a2
L

t + x2
2a2 âˆ’L2
6a2

,
(2.5.35)
and
Res
cosh(qx)etz
zq sinh(qL) ; zn

=

lim
zâ†’zn
cosh(qx)
zq
etz

lim
zâ†’zn
z âˆ’zn
sinh(qL)

(2.5.36)
= lim
zâ†’zn
cosh(qx)etz
zq cosh(qL)L/(2a2q)
(2.5.37)
= cosh(nÏ€xi/L) exp(âˆ’n2Ï€2a2t/L2)
(âˆ’n2Ï€2a2/L2) cosh(nÏ€i)L/(2a2)
(2.5.38)
= âˆ’2L(âˆ’1)n
n2Ï€2
cos(nÏ€x/L)eâˆ’n2Ï€2a2t/L2.
(2.5.39)
When we sum all of the residues from both inversions, the solution is
u(x, t) = a2N
ÎºL

t + x2
2a2 âˆ’L2
6a2

âˆ’2LN
ÎºÏ€2
âˆ
X
n=1
(âˆ’1)n
n2
cos(nÏ€x/L)eâˆ’n2Ï€2a2t/L2
âˆ’a2NM
ÎºL
t2
2 + tx2
2a2 âˆ’tL2
6a2 +
x4
24a4 âˆ’x2L2
12a4 + 7L4
360a4

âˆ’2L3NM
a2ÎºÏ€4
âˆ
X
n=1
(âˆ’1)n
n4
cos(nÏ€x/L)eâˆ’n2Ï€2a2t/L2.
(2.5.40)
Figure 2.3.2 shows the temperature in the brake lining at various places within the
lining [xâ€² = x/L] if a2 = 3.3 Ã— 10âˆ’3 cm2/sec, Îº = 1.8 Ã— 10âˆ’3 cal/(cm secâ—¦C), L = 0.48 cm,
and N = 1.96 cal/(cm2 sec). Initially the frictional heating results in an increase in the disc
brakeâ€™s temperature. As time increases, the heating rate decreases and radiative cooling
becomes suï¬ƒciently large that the temperature begins to fall.
âŠ“âŠ”
â€¢ Example 2.5.3
In the previous example we showed that Laplace transforms are particularly useful
when the boundary conditions are time dependent. Consider now the case when one of the
boundaries is moving.

Advanced Transform Methods
135
Figure 2.5.2: Typical curves of transient temperature at diï¬€erent locations in a brake lining.
Circles
denote computed values while squares are experimental measurements. (From Newcomb, T. P., 1958: The
ï¬‚ow of heat in a parallel-faced inï¬nite solid. Brit. J. Appl. Phys., 9, 372 with permission.)
We wish to solve the heat equation
âˆ‚u
âˆ‚t = a2 âˆ‚2u
âˆ‚x2 ,
Î²t < x < âˆ,
0 < t,
(2.5.41)
subject to the boundary conditions
u(x, t)

x=Î²t = f(t),
and
lim
xâ†’âˆu(x, t) â†’0,
0 < t,
(2.5.42)
and the initial condition
u(x, 0) = 0,
0 < x < âˆ.
(2.5.43)
This type of problem arises in combustion problems where the boundary moves due to the
burning of the fuel.
We begin by introducing the coordinate Î· = x âˆ’Î²t. Then the problem can be refor-
mulated as
âˆ‚u
âˆ‚t âˆ’Î² âˆ‚u
âˆ‚Î· = a2 âˆ‚2u
âˆ‚Î·2 ,
0 < Î· < âˆ,
0 < t,
(2.5.44)
subject to the boundary conditions
u(0, t) = f(t),
lim
Î·â†’âˆu(Î·, t) â†’0,
0 < t,
(2.5.45)
and the initial condition
u(Î·, 0) = 0,
0 < Î· < âˆ.
(2.5.46)

136
Advanced Engineering Mathematics: A Second Course
Taking the Laplace transform of Equation 2.5.44, we have that
d2U(Î·, s)
dÎ·2
+ Î²
a2
dU(Î·, s)
dÎ·
âˆ’s
a2 U(Î·, s) = 0,
(2.5.47)
with
U(0, s) = F(s),
and
lim
Î·â†’âˆU(Î·, s) â†’0.
(2.5.48)
The solution to Equation 2.5.47 and Equation 2.5.48 is
U(Î·, s) = F(s) exp
 
âˆ’Î²Î·
2a2 âˆ’Î·
a
r
s + Î²2
4a2
!
.
(2.5.49)
Because
L[Î¦(Î·, t)] = exp
 
âˆ’Î·
a
r
s + Î²2
4a2
!
,
(2.5.50)
where
Î¦(Î·, t) = 1
2

eâˆ’Î²Î·/2a2erfc

Î·
2a
âˆš
t âˆ’Î²
âˆš
t
2a

+ eÎ²Î·/2a2erfc

Î·
2a
âˆš
t + Î²
âˆš
t
2a

,
(2.5.51)
and
erfc(x) = 1 âˆ’
2
âˆšÏ€
Z x
0
eâˆ’Î·2 dÎ·,
(2.5.52)
we have by the convolution theorem that
u(Î·, t) = eâˆ’Î²Î·/2a2 Z t
0
f(t âˆ’Ï„)Î¦(Î·, Ï„) dÏ„,
(2.5.53)
or
u(x, t) = eâˆ’Î²(xâˆ’Î²t)/2a2 Z t
0
f(t âˆ’Ï„)Î¦(x âˆ’Î²Ï„, Ï„) dÏ„.
(2.5.54)
Problems
1. Solve the heat equation
âˆ‚u
âˆ‚t = âˆ‚2u
âˆ‚x2 âˆ’a2(u âˆ’T0),
0 < x < 1,
0 < t,
subject to the boundary conditions ux(0, t) = ux(1, t) = 0, 0 < t, and the initial condition
u(x, 0) = 0, 0 < x < 1.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
âˆ’(s + a2)U(x, s) = âˆ’a2T0
s
,
0 < x < 1,
subject to the boundary conditions U â€²(0, s) = U â€²(1, s) = 0.

Advanced Transform Methods
137
Step 2: Show that the solution to the previous step is
U(x, s) = T0
1
s âˆ’
1
s + a2

.
Step 3: Invert U(x, s) and show that u(x, t) = T0

1 âˆ’eâˆ’a2t
.
2. Solve
âˆ‚u
âˆ‚t = âˆ‚2u
âˆ‚x2 ,
0 < x < 1,
0 < t,
subject to the boundary conditions ux(0, t) = 0, u(1, t) = t, 0 < t, and the initial condition
u(x, 0) = 0, 0 < x < 1.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
âˆ’sU(x, s) = 0,
0 < x < 1,
subject to the boundary conditions U â€²(0, s) = 0 and U(1, s) = 1/s2.
Step 2: Show that the solution to the previous step is
U(x, s) = cosh(xâˆšs )
s2 cosh(âˆšs ).
Step 3: Show that U(x, s) has a second-order pole at s = 0 and simple poles at sn =
âˆ’(2n âˆ’1)2Ï€2/4 with âˆšzn = (2n âˆ’1)Ï€i/2, where n = 1, 2, 3, . . ..
Step 4: Use Bromwichâ€™s integral and show that
u(x, t) = t + 1
2(x2 âˆ’1) âˆ’16
Ï€3
âˆ
X
n=1
(âˆ’1)n
(2n âˆ’1)3 cos
(2n âˆ’1)Ï€x
2

exp

âˆ’(2n âˆ’1)2Ï€2t
4

.
3. Solve
âˆ‚u
âˆ‚t = âˆ‚2u
âˆ‚x2 ,
0 < x < 1,
0 < t,
subject to the boundary conditions u(0, t) = 0, u(1, t) = 1, 0 < t, and the initial condition
u(x, 0) = 0, 0 < x < 1.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
âˆ’sU(x, s) = 0,
0 < x < 1,
subject to the boundary conditions U(0, s) = 0 and U(1, s) = 1/s.
Step 2: Show that the solution to the previous step is U(x, s) = sinh(xâˆšs ) /[s sinh(âˆšs )].
Step 3: Show that U(x, s) has simple poles at s = 0 and sn = âˆ’n2Ï€2 with âˆšzn = nÏ€i,
where n = 1, 2, 3, . . ..

138
Advanced Engineering Mathematics: A Second Course
Step 4: Use Bromwichâ€™s integral and show that
u(x, t) = x + 2
Ï€
âˆ
X
n=1
(âˆ’1)n
n
sin(nÏ€x)eâˆ’n2Ï€2t.
4. Solve
âˆ‚u
âˆ‚t = âˆ‚2u
âˆ‚x2 ,
âˆ’1
2 < x < 1
2,
0 â‰¤t,
subject to the boundary conditions ux
 âˆ’1
2, t

= 0, ux
  1
2, t

= Î´(t), 0 â‰¤t, and the initial
condition u(x, 0) = 0, âˆ’1
2 < x < 1
2. Here Î´(t) is the Dirac delta function.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
âˆ’sU(x, s) = 0,
âˆ’1
2 < x < 1
2,
subject to the boundary conditions U â€² âˆ’1
2, s

= 0 and U â€²  1
2, s

= 1.
Step 2: Show that the solution to the previous step is
U(x, s) = cosh

(x + 1
2)âˆšs

âˆšs sinh(âˆšs )
.
Step 3: Replacing the hyperbolic functions by their exponential deï¬nition, show that
U(x, s) = 1
âˆšs

exp
 x âˆ’1
2
 âˆšs

+ exp

âˆ’
 x + 3
2
 âˆšs
	 
1 + eâˆ’2âˆšs + eâˆ’4âˆšs + Â· Â· Â·

.
Step 4: Taking the inverse of U(x, s) term by term, show that
u(x, t) =
1
âˆš
Ï€ t
âˆ
X
n=0
(
exp
"
âˆ’
 2n + 1
2 âˆ’x
2
4t
#
+ exp
"
âˆ’
 2n + 3
2 + x
2
4t
#)
.
Step 5: Show that U(x, s) has simple poles at s = 0 and sn = âˆ’n2Ï€2 with âˆšzn = nÏ€i,
where n = 1, 2, 3, . . ..
Step 6: Use Bromwichâ€™s integral and show that
u(x, t) = 1 + 2
âˆ
X
n=1
(âˆ’1)n cos

nÏ€
 x + 1
2

eâˆ’n2Ï€2t.
5. Solve
âˆ‚u
âˆ‚t âˆ’âˆ‚2u
âˆ‚x2 = 1,
0 < x < 1,
0 < t,
subject to the boundary conditions u(0, t) = u(1, t) = 0, 0 < t, and the initial condition
u(x, 0) = 0, 0 < x < 1.

Advanced Transform Methods
139
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
âˆ’sU(x, s) = âˆ’1
s,
0 < x < 1,
subject to the boundary conditions U(0, s) = U(1, s) = 0.
Step 2: Show that the solution to the previous step is
U(x, s) = 1 âˆ’cosh(xâˆšs )
s2
âˆ’[1 âˆ’cosh(âˆšs )] sinh(xâˆšs )
s2 sinh(âˆšs )
.
Step 3: Show that U(x, s) has a second-order pole at s = 0 and simple poles at sn = âˆ’n2Ï€2
with âˆšzn = nÏ€i, where n = 1, 2, 3, . . ..
Step 4: Use Bromwichâ€™s integral and show that
u(x, t) = x(1 âˆ’x)
2
âˆ’4
Ï€3
âˆ
X
m=1
sin[(2m âˆ’1)Ï€x]
(2m âˆ’1)3
eâˆ’(2mâˆ’1)2Ï€2t.
6. Solve32
âˆ‚u
âˆ‚t = a2 âˆ‚2u
âˆ‚x2 ,
0 < x < âˆ,
0 < t,
subject to the boundary conditions u(0, t) = 1, limxâ†’âˆu(x, t) â†’0, 0 < t, and the initial
condition u(x, 0) = 0, 0 < x < âˆ.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
âˆ’s
a2 U(x, s) = 0,
0 < x < âˆ,
subject to the boundary conditions U(0, s) = 1/s and limxâ†’âˆ|U(x, s)| < âˆ.
Step 2: Show that the solution to the previous step is U(x, s) = eâˆ’xâˆšs/a/s.
Step 3: From an extensive table of inverses, show that u(x, t) = erfc

x/(2a
âˆš
t )

, where
erfc(Â·) is the complementary error function.
7. Solve
âˆ‚u
âˆ‚t = âˆ‚2u
âˆ‚x2 ,
0 < x < âˆ,
0 < t,
subject to the boundary conditions ux(0, t) = 1, limxâ†’âˆu(x, t) â†’0, 0 < t, and the initial
condition u(x, 0) = 0, 0 < x < âˆ.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
âˆ’sU(x, s) = 0,
0 < x < âˆ,
32 If u(x, t) denotes the Eulerian velocity of a viscous ï¬‚uid in the half space x > 0 and parallel to the wall
located at x = 0, then this problem was ï¬rst solved by Stokes, G. G., 1850: On the eï¬€ect of the internal
friction of ï¬‚uids on the motions of pendulums. Proc. Cambridge Philos. Soc., 9, Part II, [8]â€“[106].

140
Advanced Engineering Mathematics: A Second Course
subject to the boundary conditions U â€²(0, s) = 1/s and limxâ†’âˆ|U(x, s)| < âˆ.
Step 2: Show that the solution to the previous step is U(x, s) = âˆ’eâˆ’xâˆšs/s3/2.
Step 3: From an extensive table of inverses, show that
u(x, t) = x erfc
 x
2
âˆš
t

âˆ’2
r
t
Ï€ exp

âˆ’x2
4t

,
where erfc(Â·) is the complementary error function.
8. Solve
âˆ‚u
âˆ‚t = âˆ‚2u
âˆ‚x2 ,
0 < x < âˆ,
0 < t,
subject to the boundary conditions u(0, t) = 1, limxâ†’âˆu(x, t) â†’0, 0 < t, and the initial
condition u(x, 0) = eâˆ’x, 0 < x < âˆ.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
âˆ’sU(x, s) = âˆ’eâˆ’x,
0 < x < âˆ,
subject to the boundary conditions U(0, s) = 1/s and limxâ†’âˆ|U(x, s)| < âˆ.
Step 2: Show that the solution to the previous step is
U(x, s) = eâˆ’x
s âˆ’1 +
1
s âˆ’
1
s âˆ’1

eâˆ’xâˆšs.
Step 3: From an extensive table of inverses, show that
u(x, t) = etâˆ’x + erfc
 x
2
âˆš
t

âˆ’1
2et

eâˆ’xerfc
 x
2
âˆš
t âˆ’
âˆš
t

+ exerfc
 x
2
âˆš
t +
âˆš
t

,
where erfc(Â·) is the complementary error function.
9. Solve
âˆ‚u
âˆ‚t = a2
âˆ‚2u
âˆ‚x2 + (1 + Î´)âˆ‚u
âˆ‚x + Î´u

,
0 < x < âˆ,
0 < t,
where Î´ is a constant, subject to the boundary conditions u(0, t) = u0, limxâ†’âˆu(x, t) â†’0,
0 < t, and the initial condition u(x, 0) = 0, 0 < x < âˆ.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
+ (1 + Î´)dU(x, s)
dx
+

Î´ âˆ’s
a2

U(x, s) = 0,
0 < x < âˆ,
subject to the boundary conditions U(0, s) = u0/s and limxâ†’âˆ|U(x, s)| < âˆ.
Step 2: Show that the solution to the previous step is
U(x, s) = u0
s exp
"
âˆ’(1 + Î´)x
2
âˆ’x
a
r
a2(1 âˆ’Î´)2
4
+ s
#
.

Advanced Transform Methods
141
Step 3: From an extensive table of inverses, show that
u(x, t) = u0
2 eâˆ’Î´xerfc

x
2a
âˆš
t + a(1 âˆ’Î´)
âˆš
t
2

+ u0
2 eâˆ’xerfc

x
2a
âˆš
t âˆ’a(1 âˆ’Î´)
âˆš
t
2

.
10. During their modeling of a chemical reaction with a back reaction, Agmon et al.33
solved
âˆ‚u
âˆ‚t = a2 âˆ‚2u
âˆ‚x2 ,
0 < x < âˆ,
0 < t,
subject to the boundary conditions
Îºd + a2ux(0, t) + a2Îºd
Z t
0
ux(0, Ï„) dÏ„ = Îºru(0, t),
lim
xâ†’âˆu(x, t) â†’0,
0 < t,
and the initial condition u(x, 0) = 0, 0 < x < âˆ, where Îºd and Îºr denote the intrinsic
dissociation and recombination rate coeï¬ƒcients, respectively. What should they have found?
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
âˆ’s
a2 U(x, s) = 0,
0 < x < âˆ,
subject to the boundary conditions limxâ†’âˆ|U(x, s)| < âˆand
Îºd + (s + Îºd)a2U â€²(0, s) = sÎºrU(0, s).
Step 2: Show that the solution to the previous step is
U(x, s) = 2Îºd exp(âˆ’xâˆšs/a)
aâˆ†âˆšs

1
2aâˆšs + Îºr âˆ’âˆ†âˆ’
1
2aâˆšs + Îºr + âˆ†

,
where âˆ†â‰¡
p
Îº2r âˆ’4a2Îºd.
Step 3: From an extensive table of inverses, show that
u(x, t) = Îºd
âˆ†eâˆ’x2/(4a2t) h
ex2
âˆ’erfc(xâˆ’) âˆ’ex2
+erfc(x+)
i
,
where xÂ± = [x + (Îºr Â± âˆ†)a2t]/(2a
âˆš
t ) and erfc(Â·) is the complementary error function.
11. Solve34
âˆ‚u
âˆ‚t = âˆ‚2u
âˆ‚x2 âˆ’Î²u,
0 < x < âˆ,
0 < t,
subject to the boundary conditions Ïu(0, t)âˆ’ux(0, t) = e(Ïƒ2âˆ’Î²)t, limxâ†’âˆu(x, t) â†’0, 0 < t,
and the initial condition u(x, 0) = 0, 0 < x < âˆ, where Î², Ï, and Ïƒ are constants and
Ïƒ Ì¸= Ï.
33 Agmon, N., E. Pines, and D. Huppert, 1988: Germinate recombination in proton-transfer reactions.
II. Comparison of diï¬€usional and kinetic schemes. J. Chem. Phys., 88, 5631â€“5638.
34 Saidel, G. M., E. D. Morris, and G. M. Chisolm, 1987: Transport of macromolecules in arterial wall
in vivo: A mathematical model and analytic solutions. Bull. Math. Biol., 49, 153â€“169.

142
Advanced Engineering Mathematics: A Second Course
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
âˆ’(s + Î²)U(x, s) = 0,
0 < x < âˆ,
subject to the boundary conditions
ÏU(0, s) âˆ’U â€²(0, s) =
1
s + Î² âˆ’Ïƒ2 ,
lim
xâ†’âˆ|U(x, s)| < âˆ.
Step 2: Show that the solution to the previous step is
U(x, s) =
exp(âˆ’xâˆšs + Î² )
(s + Î² âˆ’Ïƒ2)(Ï + âˆšs + Î² ).
Step 3: Using partial fractions, show that
U(x, s) =
eâˆ’x
âˆš
sâ€²
(sâ€² + Ïƒ2)(
âˆš
sâ€² + Ï)
=
eâˆ’x
âˆš
sâ€²
(
âˆš
sâ€² + Ïƒ)(
âˆš
sâ€² âˆ’Ïƒ)(
âˆš
sâ€² + Ï)
=
eâˆ’x
âˆš
sâ€²
(Ï2 âˆ’Ïƒ2)(
âˆš
sâ€² + Ï)
+
eâˆ’x
âˆš
sâ€²
2Ïƒ(Ï + Ïƒ)(
âˆš
sâ€² âˆ’Ïƒ)
âˆ’
eâˆ’x
âˆš
sâ€²
2Ïƒ(Ï âˆ’Ïƒ)(
âˆš
sâ€² + Ïƒ)
,
where sâ€² = s + Î².
Step 4: Using the ï¬rst shifting theorem and the fact that
Lâˆ’1
 
eâˆ’kâˆšs
a + âˆšs
!
=
1
âˆš
Ï€t exp

âˆ’k2
4t

âˆ’aeakea2terfc

a
âˆš
t +
k
2
âˆš
t

,
show that
u(x, t) = 1
2eÏƒ2tâˆ’Î²t
 eâˆ’Ïƒx
Ï + Ïƒ erfc
 x
2
âˆš
t âˆ’Ïƒ
âˆš
t

+ eÏƒx
Ï âˆ’Ïƒ erfc
 x
2
âˆš
t + Ïƒ
âˆš
t

âˆ’
Ï
Ï2 âˆ’Ïƒ2 eÏx+Ï2tâˆ’Î²terfc
 x
2
âˆš
t + Ï
âˆš
t

.
12. Solve
âˆ‚u
âˆ‚t = a2 âˆ‚2u
âˆ‚x2 + Aeâˆ’kx,
0 < x < âˆ,
0 < t,
subject to the boundary conditions ux(0, t) = 0, limxâ†’âˆu(x, t) = u0, 0 < t, and the initial
condition u(x, 0) = u0, 0 < x < âˆ.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
âˆ’s
a2 U(x, s) = âˆ’u0
a2 âˆ’A
a2 seâˆ’kx,
0 < x < âˆ,
subject to the boundary conditions U â€²(0, s) = 0 and limxâ†’âˆU(x, s) = u0/s.

Advanced Transform Methods
143
Step 2: Show that the solution to the previous step is
U(x, s) = u0
s + Aeâˆ’kx
a2k2

1
s âˆ’a2k2 âˆ’1
s

+ Aeâˆ’qx
aksâˆšs âˆ’
Aeâˆ’qx
akâˆšs(s âˆ’a2k2),
where q = âˆšs/a.
Step 3: Using the convolution theorem,
u(x, t) = u0 + Aeâˆ’kx
a2k2

ea2k2t âˆ’1

+ A
ak
"
2
r
t
Ï€ exp

âˆ’x2
4a2t

âˆ’x
aerfc

x
2a
âˆš
t
#
âˆ’Aea2k2t
ak
Z t
0
eâˆ’a2k2Ï„ exp

âˆ’x2
4a2Ï„
 dÏ„
âˆšÏ€Ï„ .
13. Solve
âˆ‚u
âˆ‚t = a2 âˆ‚2u
âˆ‚x2 âˆ’P,
0 < x < L,
0 < t,
subject to the boundary conditions u(0, t) = t, u(L, t) = 0, 0 < t, and the initial condition
u(x, 0) = 0, 0 < x < L.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
âˆ’s
a2 U(x, s) = P
sa2 ,
0 < x < L,
subject to the boundary conditions U(0, s) = 1/s2 and U(L, s) = 0.
Step 2: Show that the solution to the previous step is
U(x, s) = P
s2
 sinh(qx)
sinh(qL) âˆ’1

+ (P + 1)sinh[q(L âˆ’x)]
s2 sinh(qL) ,
where q = âˆšs/a.
Step 3: Show that U(x, s) has a second-order pole at s = 0 and simple poles at sn =
âˆ’n2Ï€2a2/L2 and âˆšsn = nÏ€ai/L, where n = 1, 2, 3, . . ..
Step 4: Use Bromwichâ€™s integral and show that
u(x, t) = t(L âˆ’x)
L
+ Px(x âˆ’L)
2a2
âˆ’x(x âˆ’L)(x âˆ’2L)
6a2L
âˆ’2PL2
a2Ï€3
âˆ
X
n=1
(âˆ’1)n
n3
sin
nÏ€x
L

exp

âˆ’a2n2Ï€2t
L2

+ 2(P + 1)L2
a2Ï€3
âˆ
X
n=1
1
n3 sin
nÏ€x
L

exp

âˆ’a2n2Ï€2t
L2

.
14. Solve
âˆ‚u
âˆ‚t = a2 âˆ‚2u
âˆ‚x2 + ku,
0 < x < L,
0 < k, t,

144
Advanced Engineering Mathematics: A Second Course
subject to the boundary conditions u(0, t) = u(L, t) = T0, 0 < t, and the initial condition
u(x, 0) = T0, 0 < x < L.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
âˆ’s âˆ’k
a2
U(x, s) = âˆ’T0
a2 ,
0 < x < L,
subject to the boundary conditions U(0, s) = U(L, s) = T0/s.
Step 2: Show that the solution to the previous step is
U(x, s) =
T0
s âˆ’k âˆ’
kT0
s(s âˆ’k)
sinh(qx) + sinh[q(L âˆ’x)]
sinh(qL)
.
where q =
âˆš
s âˆ’k/a.
Step 3: Show that U(x, s) has simple poles at s = 0, s = k, and sn = k âˆ’n2Ï€2a2/L2, where
n = 1, 2, 3, . . .. Note here that qn = nÏ€i/L.
Step 4: Use Bromwichâ€™s integral and show that
u(x, t) = T0 cos[(L/2 âˆ’x)
p
k/a2]
cos(L
p
k/a2/2)
+ 4kT0
Ï€
âˆ
X
m=1
sin[(2m âˆ’1)Ï€x/L]
(2m âˆ’1)[k âˆ’(2m âˆ’1)2Ï€2a2/L2]ektâˆ’(2mâˆ’1)2Ï€2a2t/L2
= 4T0
Ï€
âˆ
X
m=1
1
2m âˆ’1

Îºm
Îºm âˆ’k âˆ’

k
Îºm âˆ’k

ektâˆ’Îºmt

sin
(2m âˆ’1)Ï€x
L

,
where Îºm = (2m âˆ’1)2Ï€2a2/L2.
15. An electric fuse protects electrical devices by using resistance heating to melt an en-
closed wire when excessive current passes through it. A knowledge of the distribution of
temperature along the wire is important in the design of the fuse. If the temperature rises
to the melting point only over a small interval of the element, the melt will produce a
small gap, resulting in an unnecessary prolongation of the fault and a considerable release
of energy. Therefore, the desirable temperature distribution should melt most of the wire.
For this reason, Guile and Carne35 solved the heat conduction equation
âˆ‚u
âˆ‚t = a2 âˆ‚2u
âˆ‚x2 + q(1 + Î±u),
âˆ’L < x < L,
0 < t,
to understand the temperature structure within the fuse just before meltdown. The second
term on the right side of the heat conduction equation gives the resistance heating, which is
assumed to vary linearly with temperature. If the terminals at x = Â±L remain at a constant
temperature, which we can take to be zero, the boundary conditions are u(âˆ’L, t) = u(L, t) =
0, 0 < t. The initial condition is u(x, 0) = 0, âˆ’L < x < L. Find the temperature ï¬eld as a
function of the parameters a, q, and Î±.
35 Guile, A. E., and E. B. Carne, 1954: An analysis of an analogue solution applied to the heat conduction
problem in a cartridge fuse. AIEE Trans., Part 1, 72, 861â€“868.

Advanced Transform Methods
145
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
+ Î±q âˆ’s
a2
U(x, s) = âˆ’q
a2s,
0 < x < L,
subject to the boundary conditions U(âˆ’L, s) = U(L, s) = 0.
Step 2: Show that the solution to the previous step is
sU(x, s) =
q
s âˆ’Î±q âˆ’
q cosh(xâˆšs âˆ’Î±q/a)
(s âˆ’Î±q) cosh(Lâˆšs âˆ’Î±q/a).
Step 3: Show that U(x, s) has a removable singularity at s = Î±q and simple poles at
sn = Î±q âˆ’(2n âˆ’1)2Ï€2a2/4L2 with âˆšsn âˆ’Î±q = (2n âˆ’1)Ï€ai/2L, where n = 1, 2, 3, . . ..
Step 4: Use Bromwichâ€™s integral and show that
ut(x, t) = âˆ’4q
Ï€
âˆ
X
n=1
(âˆ’1)n
2n âˆ’1 cos[(2n âˆ’1)Ï€x/2L] exp[Î±qt âˆ’(2n âˆ’1)2Ï€2a2t/4L2].
Step 5: Integrate ut(x, t) with respect to time and obtain
u(x, t) = 4q
Ï€
âˆ
X
n=1
(âˆ’1)n cos[(2n âˆ’1)Ï€x/2L]
(2n âˆ’1)[Î±q âˆ’(2n âˆ’1)2Ï€2a2/4L2]

1 âˆ’exp[Î±qt âˆ’(2n âˆ’1)2Ï€2a2t/4L2]
	
,
where the constant of integration ensures that u(x, 0) = 0.
16. Solve36
âˆ‚u
âˆ‚t = âˆ‚2u
âˆ‚r2 + 2
r
âˆ‚u
âˆ‚r ,
0 â‰¤r < 1,
0 < t,
subject to the boundary conditions limrâ†’0 |u(r, t)| < âˆ, ur(1, t) = 1, 0 < t, and the initial
condition u(r, 0) = 0, 0 â‰¤r < 1.
Step 1: Introduce the new variable v(r, t) = r u(r, t) and show that the problem becomes
âˆ‚v
âˆ‚t = âˆ‚2v
âˆ‚r2 ,
0 â‰¤r < 1, 0 < t,
with the boundary conditions limrâ†’0 v(r, t) â†’0 and
âˆ‚v(1, t)
âˆ‚r
âˆ’v(1, t) = 1,
t > 0
and the initial condition v(r, 0) = 0, 0 â‰¤r < 1.
Step 2: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2V (r, s)
dr2
âˆ’sV (r, s) = 0,
0 â‰¤r < 1,
36 See Reismann, H., 1962: Temperature distribution in a spinning sphere during atmospheric entry. J.
Aerosp. Sci., 29, 151â€“159.

146
Advanced Engineering Mathematics: A Second Course
subject to the boundary conditions limrâ†’0 V (r, s) â†’0 and V â€²(1, s) âˆ’V (1, s) = 1/s.
Step 3: Show that the solution to the previous step is
V (r, s) =
sinh(râˆšs )
s [âˆšs cosh(âˆšs ) âˆ’sinh(âˆšs )].
Step 4: Show that V (x, s) has a second-order pole at s = 0 and simple poles where âˆšsn =
iÎ»n, sn = âˆ’Î»2
n and tan(Î»n) = Î»n, n = 1, 2, 3, . . ..
Step 5: Use Bromwichâ€™s integral and show that
u(r, t) = r2
2 + 3t âˆ’3
10 âˆ’2
r
âˆ
X
n=1
sin(Î»nr)
Î»2n sin(Î»n)eâˆ’Î»2
nt,
where tan(Î»n) = Î»n.
17. Solve37
âˆ‚u
âˆ‚t = a2
âˆ‚2u
âˆ‚r2 + 2
r
âˆ‚u
âˆ‚r

+ q(t) = a2
r
âˆ‚2(ru)
âˆ‚r2
+ q(t),
b < r < âˆ,
0 < t,
subject to the boundary conditions
âˆ‚u(b, t)
âˆ‚r
= u(b, t),
lim
râ†’âˆu(r, t) = u0 +
Z t
0
q(Ï„) dÏ„,
0 < t,
and the initial condition u(r, 0) = u0, b < r < âˆ.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2[rU(r, s)]
dr2
âˆ’s
a2 rU(r, s) = âˆ’r[Q(s) + u0]
a2
,
b < r < âˆ,
subject to the boundary conditions U â€²(b, s) = U(b, s) and limrâ†’âˆU(r, s) = [u0 + Q(s)]/s.
Step 2: Show that the solution to the previous step is
U(r, s) = u0 + Q(s)
s
âˆ’
b Q(s)
s(q + 1/Î²)
eâˆ’q(râˆ’b)
r
âˆ’
b u0
s(q + 1/Î²)
eâˆ’q(râˆ’b)
r
,
where q = âˆšs/a and Î² = b/(1 + b).
Step 3: Because
Lâˆ’1
 eâˆ’âˆšs (râˆ’b)/a
s(âˆšs/a + 1/Î²)

= Î²

erfc
 r âˆ’b
2a
âˆš
t

âˆ’exp
r âˆ’b
Î²
+ a2t
Î²2

erfc
a
âˆš
t
Î²
+ r âˆ’b
2a
âˆš
t

,
37 See Frisch, H. L, and F. C. Collins, 1952: Diï¬€usional processes in the growth of aerosol particles. J.
Chem. Phys., 20, 1797â€“1803.

Advanced Transform Methods
147
show that
u(r, t) = u0

1 âˆ’b âˆ’Î²
r
f(r, t)

+
Z t
0

1 âˆ’b âˆ’Î²
r
f(r, t âˆ’Ï„)

q(Ï„) dÏ„,
where
f(r, t) = erfc
 r âˆ’b
2a
âˆš
t

âˆ’exp
r âˆ’b
Î²
+ a2t
Î²2

erfc
a
âˆš
t
Î²
+ r âˆ’b
2a
âˆš
t

.
18. Consider38 a viscous ï¬‚uid located between two ï¬xed walls x = Â±L. At x = 0 we
introduce a thin, inï¬nitely long rigid barrier of mass m per unit area and let it fall under
the force of gravity, which points in the direction of positive x. We wish to ï¬nd the velocity
of the ï¬‚uid u(x, t). The ï¬‚uid is governed by the partial diï¬€erential equation
âˆ‚u
âˆ‚t = Î½ âˆ‚2u
âˆ‚x2 ,
0 < x < L,
0 < t,
subject to the boundary conditions u(L, t) = 0 and ut(0, t) âˆ’2Âµux(0, t)/m = g, 0 < t, and
the initial condition u(x, 0) = 0, 0 < x < L.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
âˆ’s
Î½ U(x, s) = 0,
0 < x < L,
subject to the boundary conditions U(L, s) = 0 and sU(0, s) âˆ’2ÂµU â€²(0, s)/m = g/s.
Step 2: Show that the solution to the previous step is
U(x, s) =
g sinh[(L âˆ’x)
p
s/Î½ ]
s[s sinh(L
p
s/Î½ ) + 2Âµâˆšs cosh(L
p
s/Î½ )/(mâˆšÎ½ )]
.
Step 3: Show that U(x, s) has simple poles at s = 0 and sn = âˆ’Î½Î»2
n/L2, where Î»n tan(Î»n) =
2ÂµL/(mÎ½) â‰¡k and n = 1, 2, 3, . . ..
Step 4: Use Bromwichâ€™s integral and show that
u(x, t) = mg(L âˆ’x)
2Âµ
âˆ’4gÂµL3
mÎ½2
âˆ
X
n=1
sin[Î»n(L âˆ’x)/L] exp(âˆ’Î½Î»2
nt/L2)
Î»2n[Î»2n + k(1 + k)] sin(Î»n)
.
19. Consider39 a viscous ï¬‚uid located between two ï¬xed walls x = Â±L. At x = 0 we
introduce a thin, inï¬nitely long rigid barrier of mass m per unit area. The barrier is acted
upon by an elastic force in such a manner that it would vibrate with a frequency Ï‰ if the
38 See Havelock, T. H., 1921: The solution of an integral equation occurring in certain problems of viscous
ï¬‚uid motion. Philos. Mag., Ser. 6, 42, 620â€“628.
39 See Havelock, T. H., 1921: On the decay of oscillation of a solid body in a viscous ï¬‚uid. Philos. Mag.,
Ser. 6, 42, 628â€“634.

148
Advanced Engineering Mathematics: A Second Course
liquid were absent. We wish to ï¬nd the barrierâ€™s deviation from equilibrium, y(t). The ï¬‚uid
is governed by the partial diï¬€erential equation
âˆ‚u
âˆ‚t = Î½ âˆ‚2u
âˆ‚x2 ,
0 < x < L,
0 < t.
The boundary conditions are u(L, t) = myâ€²â€²(t) âˆ’2Âµux(0, t) + mÏ‰2y(t) = 0, yâ€²(t) = u(0, t),
0 < t, and the initial conditions are u(x, 0) = 0, 0 < x < L, y(0) = A, and yâ€²(0) = 0.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
âˆ’s
Î½ U(x, s) = 0,
0 < x < L,
subject to the boundary conditions U(L, s) = 0 and ms2Y (s) âˆ’2ÂµU â€²(0, s) + mÏ‰2Y (s) =
âˆ’msA and sY (s) âˆ’A = U(0, s).
Step 2: Show that U(x, s) = B sinh
hp
s/Î½(L âˆ’x)
i
.
Step 3: Show that at x = 0,
ms2Y (s) + 2ÂµB
r s
Î½ cosh

L
r s
Î½

+ mÏ‰2Y (s) = msA
and
sY (s) âˆ’A = B sinh

L
r s
Î½

.
Step 4: Eliminating B in Step 3, show that
Y (s) = A
ms + 2Âµ
p
s/Î½ coth

L
p
s/Î½

ms2 + 2Âµs
p
s/Î½ coth

L
p
s/Î½

+ mÏ‰2 .
Step 5: Show that Y (s) has simple poles at Î»n which are the roots of
Î»2
n + 2ÂµÎ»3/2
n
coth

L
p
Î»n/Î½

/(mâˆšÎ½ ) + Ï‰2 = 0,
n = 1, 2, 3, . . . .
Step 6: Use Bromwichâ€™s integral and show that
y(t) = 4ÂµAÏ‰2
mL
âˆ
X
n=1
Î»neÎ»nt
Î»4n âˆ’( 2Âµ
mL)(1 + 2ÂµL
mÎ½ )Î»3n + 2Ï‰2Î»2n + 6Ï‰2Âµ
mL Î»n + Ï‰4 .
20. Solve40
âˆ‚u
âˆ‚t = a2 âˆ‚2u
âˆ‚x2 ,
0 < x < L,
0 < t,
40 See McCarthy, T. A., and H. J. Goldsmid, 1970: Electro-deposited copper in bismuth telluride. J.
Phys. D, 3, 697â€“706.

Advanced Transform Methods
149
subject to the boundary conditions ux(0, t) = 0, a2ux(L, t) + Î±u(L, t) = F, 0 < t, and the
initial condition u(x, 0) = 0, 0 < x < L.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
âˆ’s
a2 U(x, s) = 0,
0 < x < L,
subject to the boundary conditions U â€²(0, s) = 0 and a2U â€²(L, s) + Î±U(L, s) = F/s.
Step 2: Show that the solution to the previous step is
U(x, s) =
F cosh(qx)
s[a2q sinh(qL) + Î± cosh(qL)],
where q = âˆšs/a.
Step 3: Show that U(x, s) has simple poles at s = 0 and sn = âˆ’a2Î»2
n/L2, where Î»n is the
nth root of Î» tan(Î») = Î±L/a2, qn = iÎ»n/L, and n = 1, 2, 3, . . ..
Step 4: Use Bromwichâ€™s integral and show that
u(x, t) = F
Î±
(
1 âˆ’2hL
âˆ
X
n=1
cos(Î»nx/L) exp(âˆ’a2Î»2
nt/L2)
[hL(1 + hL) + Î»2n) cos(Î»n)
)
,
where h = Î±/a2.
21. Solve
âˆ‚u
âˆ‚t = âˆ‚2u
âˆ‚x2 ,
0 â‰¤x < 1,
0 â‰¤t,
subject to the boundary conditions u(0, t) = 0 and 3a [ux(1, t) âˆ’u(1, t)] + ut(1, t) = Î´(t),
0 â‰¤t, and the initial condition u(x, 0) = 0, 0 â‰¤x < 1. Here Î´(t) denotes the Dirac delta
function.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
âˆ’sU(x, s) = 0,
0 < x < 1,
subject to the boundary conditions U(0, s) = 0 and 3a[U â€²(1, s) âˆ’U(1, s)] + sU(1, s) = 1.
Step 2: Show that the solution to the previous step is
U(x, s) =
sinh(xâˆšs )
3a [âˆšs cosh(âˆšs ) âˆ’sinh(âˆšs )] + s sinh(âˆšs ).
Step 3: Show that U(x, s) has simple poles at s = 0 and sn = âˆ’Î»2
n where Î»n cot(Î»n) =
(3a + Î»2
n)/3a, n = 1, 2, 3, . . ..
Step 4: Use Bromwichâ€™s integral and show that
u(x, t) =
x
a + 1 + 2
âˆ
X
n=1
sin(Î»nx) exp(âˆ’Î»2
nt)
[3a + 3 + Î»2n/(3a)] sin(Î»n).

150
Advanced Engineering Mathematics: A Second Course
22. Solve41 the partial diï¬€erential equation
âˆ‚u
âˆ‚t + V âˆ‚u
âˆ‚x = âˆ‚2u
âˆ‚x2 ,
0 < x < 1,
0 < t,
where V is a constant, subject to the boundary conditions u(0, t) = 1, ux(1, t) = 0, 0 < t,
and the initial condition u(x, 0) = 0, 0 < x < 1.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
âˆ’V dU(x, s)
dx
âˆ’sU(x, s) = 0,
0 < x < 1,
subject to the boundary conditions U(0, s) = 1/s and U â€²(1, s) = 0.
Step 2: Show that the solution to the previous step is
U(x, s) = eV x/2 Âµ cosh[Âµ(1 âˆ’x)] + (V/2) sinh[Âµ(1 âˆ’x)]
s [Âµ cosh(Âµ) + (V/2) sinh(Âµ)]
= eV x/2
âˆš
sâ€² cosh
h
(1 âˆ’x)
âˆš
sâ€²
i
+ (V/2) sinh
h
(1 âˆ’x)
âˆš
sâ€²
i
(sâ€² âˆ’V 2/4)
hâˆš
sâ€² cosh
âˆš
sâ€²

+ (V/2) sinh
âˆš
sâ€²
i,
where Âµ =
p
s + V 2/4 and sâ€² = s + V 2/4.
Step 3: Show that U(x, s) has simple poles at sâ€² = V 2/4 and sâ€²
n = âˆ’Î»2
n with Î»n cot(Î»n) =
âˆ’V/2, where n = 1, 2, 3, . . ..
Step 4: Using Bromwichâ€™s integral, show that
u(x, t) = 1 âˆ’2eV x/2âˆ’V 2t/4
Ã—
âˆ
X
n=1
Î»n{(V/2) sin[Î»n(1 âˆ’x)] + Î»n cos[Î»n(1 âˆ’x)]}eâˆ’Î»2
nt
(Î»2n + V 2/4)[Î»n sin(Î»n) âˆ’(1 + V/2) cos(Î»n)]
.
23. Solve42 the partial diï¬€erential equation
âˆ‚2u
âˆ‚xâˆ‚t + aâˆ‚u
âˆ‚t + bâˆ‚u
âˆ‚x = 0,
0 < x < âˆ,
0 < a, b, t,
subject to the boundary conditions u(0, t) = 1, limxâ†’âˆu(x, t) â†’0, 0 < t, and the initial
condition ux(x, 0) + au(x, 0) = 0, 0 < x < âˆ.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and
(s + b)U â€²(x, s) + asU(x, s) = 0,
0 < x < âˆ,
41 See Yoo, H., and E.-T. Pak, 1996: Analytical solutions to a one-dimensional ï¬nite-domain model for
stratiï¬ed thermal storage tanks. Sol. Energy, 56, 315â€“322.
42 See Liaw, C. H., J. S. P. Wang, R. A. Greenhorn, and K. C. Chao, 1979: Kinetics of ï¬xed-bed
absorption: A new solution. AICHE J., 25, 376â€“381.

Advanced Transform Methods
151
subject to the boundary conditions limxâ†’0 |U(x, s)| < âˆand U(0, s) = 1/s.
Step 2: Show that the solution to the previous step is
U(x, s) = 1
s exp

âˆ’asx
s + b

.
Step 3: Because
eâˆ’cÎ¾ = 1 âˆ’c
Z Î¾
0
eâˆ’cÎ· dÎ·,
show that
U(x, s) = 1
s âˆ’
Z ax
0
eâˆ’Î· exp
 bÎ·
s + b

dÎ·
s + b.
Step 4: By inverting U(x, s) term by term and using the ï¬rst shifting theorem,
u(x, t) = 1 âˆ’eâˆ’bt
Z ax
0
eâˆ’Î·I0

2
p
btÎ·

dÎ·.
24. Solve
1
r
âˆ‚
âˆ‚r

râˆ‚u
âˆ‚r

âˆ’âˆ‚u
âˆ‚t = Î´(t),
0 â‰¤r < a,
0 â‰¤t,
subject to the boundary conditions limrâ†’0 |u(r, t)| < âˆ, u(a, t) = 0, 0 < t, and the initial
condition u(r, 0) = 0, 0 â‰¤r < a, where Î´(t) is the Dirac delta function.
Note that
Jn(iz) = inIn(z) and In(iz) = inJn(z) for all complex z.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
1
r
d
dr

rdU(r, s)
dr

âˆ’sU(r, s) = 1,
0 â‰¤r < a,
subject to the boundary conditions limrâ†’0 |U(r, s)| < âˆand U(a, s) = 0.
Step 2: Show that the solution to the previous step is
U(r, s) = I0(râˆšs ) âˆ’I0(aâˆšs )
s I0(aâˆšs )
.
Step 3: Show that U(r, s) has a removable singularity at s = 0 and simple poles at sn =
âˆ’k2
n/a2, where J0(kn) = 0 and n = 1, 2, 3, . . ..
Step 4: Using Bromwichâ€™s integral, show that
u(r, t) = âˆ’2
âˆ
X
n=1
J0(knr/a)
kn J1(kn) eâˆ’k2
nt/a2.
25. Solve
âˆ‚u
âˆ‚t = 1
r
âˆ‚
âˆ‚r

râˆ‚u
âˆ‚r

+ H(t),
0 â‰¤r < a,
0 < t,

152
Advanced Engineering Mathematics: A Second Course
subject to the boundary conditions limrâ†’0 |u(r, t)| < âˆ, u(a, t) = 0, 0 < t, and the initial
condition u(r, 0) = 0, 0 â‰¤r < a. Note that Jn(iz) = inIn(z) and In(iz) = inJn(z) for all
complex z.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
1
r
d
dr

rdU(r, s)
dr

âˆ’sU(r, s) = âˆ’1
s,
0 â‰¤r < a,
subject to the boundary conditions limrâ†’0 |U(r, s)| < âˆand U(a, s) = 0.
Step 2: Show that the solution to the previous step is
U(r, s) = I0(aâˆšs ) âˆ’I0(râˆšs )
s2 I0(aâˆšs)
.
Step 3: Show that U(r, s) has simple poles at s = 0 and sn = âˆ’k2
n/a2 where J0(kn) = 0
and n = 1, 2, 3, . . ..
Step 4: Using Bromwichâ€™s integral, show that
u(r, t) = a2 âˆ’r2
4
âˆ’2a2
âˆ
X
n=1
J0(knr/a)
k3n J1(kn) eâˆ’k2
nt/a2.
26. Solve
âˆ‚u
âˆ‚t = 1
r
âˆ‚
âˆ‚r

râˆ‚u
âˆ‚r

,
0 â‰¤r < a,
0 < t,
subject to the boundary conditions limrâ†’0 |u(r, t)| < âˆ, u(a, t) = eâˆ’t/Ï„0, 0 < t, and the
initial condition u(r, 0) = 1, 0 â‰¤r < a. Note that Jn(iz) = inIn(z) and In(iz) = inJn(z)
for all complex z.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
1
r
d
dr

rdU(r, s)
dr

âˆ’sU(r, s) = âˆ’1,
0 â‰¤r < a,
subject to the boundary conditions limrâ†’0 |U(r, s)| < âˆand U(a, s) = 1/(s + 1/Ï„0).
Step 2: Show that the solution to the previous step is
U(r, s) = 1
s +

1
s + 1/Ï„0
âˆ’1
s
 I0(râˆšs )
I0(aâˆšs ).
Step 3: Show that U(r, s) has simple poles at s = 0, s = âˆ’1/Ï„0, and sn = âˆ’k2
n/a2, where
J0(kn) = 0 and n = 1, 2, 3, . . ..
Step 4: Using Bromwichâ€™s integral, show that
u(r, t) = J0(r
p
1/Ï„0 )
J0(a
p
1/Ï„0 )
eâˆ’t/Ï„0 + 2a2
âˆ
X
n=1
J0(knr/a)
kn(a2 âˆ’k2nÏ„0) J1(kn)eâˆ’k2
nt/a2
= eâˆ’t/Ï„0 + 2a2
âˆ
X
n=1
J0(knr/a)
kn(a2 âˆ’k2nÏ„0) J1(kn)

eâˆ’k2
nt/a2 âˆ’eâˆ’t/Ï„0
.

Advanced Transform Methods
153
27. Solve
âˆ‚u
âˆ‚t = a2
âˆ‚2u
âˆ‚r2 + 1
r
âˆ‚u
âˆ‚r

,
0 â‰¤r < b,
0 < t,
subject to the boundary conditions
lim
râ†’0 |u(r, t)| < âˆ,
u(b, t) = kt,
0 < t,
and the initial condition u(r, 0) = 0, 0 â‰¤r < b. Note that Jn(iz) = inIn(z) and In(iz) =
inJn(z) for all complex z.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
1
r
d
dr

rdU(r, s)
dr

âˆ’s
a2 U(r, s) = 0,
0 â‰¤r < b,
subject to the boundary conditions limrâ†’0 |U(r, s)| < âˆand U(b, s) = k/s2.
Step 2: Show that the solution to the previous step is
U(r, s) = k I0(râˆšs/a)
s2 I0(bâˆšs/a).
Step 3: Show that U(r, s) has a second-order pole at z = 0 and simple poles at iÎºn = âˆšzn/a
or zn = âˆ’a2Îº2
n, where J0(Îºnb) = 0 and n = 1, 2, 3, . . ..
Step 4: Using Bromwichâ€™s integral, show that
u(r, t) = k
"
t âˆ’b2 âˆ’r2
4a2
+ 2
a2b
âˆ
X
n=1
J0(Îºnr)
Îº3nJ1(Îºnb)
#
.
28. Solve the nonhomogeneous heat equation for the spherical shell43
âˆ‚u
âˆ‚t = a2
âˆ‚2u
âˆ‚r2 + 2
r
âˆ‚u
âˆ‚r + A
r4

,
Î± < r < Î²,
0 < t,
subject to the boundary conditions ur(Î±, t) = u(Î², t) = 0, 0 < t, and the initial condition
u(r, 0) = 0, Î± < r < Î².
Step 1: By introducing v(r, t) = r u(r, t), show that the problem simpliï¬es to
âˆ‚v
âˆ‚t = a2
âˆ‚2v
âˆ‚r2 + A
r3

,
Î± < r < Î²,
0 < t,
subject to the boundary conditions vr(Î±, t) âˆ’v(Î±, t)/Î± = v(Î², t) = 0, 0 < t, and the initial
condition v(r, 0) = 0, Î± < r < Î².
43 See Malkovich, R. Sh., 1977: Heating of a spherical shell by a radial current. Sov. Phys. Tech. Phys.,
22, 636.

154
Advanced Engineering Mathematics: A Second Course
Step 2: Taking the Laplace transform of the diï¬€erential equation and boundary conditions
in Step 1, show that
d2V (r, s)
dr2
âˆ’s
a2 V (r, s) = âˆ’A
sr3 ,
Î± < r < Î²,
along with V â€²(Î±, s) + V (Î±, s)/Î± = V (Î², s) = 0.
Step 3: Using the method of variation of parameters, show that the particular solution to
Step 2 is Vp(r, s) = u1(r, s) cosh(qr) + u2(r, s) sinh(qr), where
u1(r, s) = A
sq
Z r
Î²
sinh(qÏ„)
Ï„ 3
dÏ„,
u2(r, s) = âˆ’A
sq
Z r
Î²
cosh(qÏ„)
Ï„ 3
dÏ„,
and
q = âˆšs/a.
Step 4: Show that the general solution to Step 2 is
V (r, s) = C sinh[q(r âˆ’Î²)] âˆ’A
sq
Z r
Î²
sinh[q(r âˆ’Ï„)]
Ï„ 3
dÏ„.
This solution satisï¬es V (Î², s) = 0.
Step 5: Use the remaining boundary condition and show that
U(r, s) = A
srq

sinh[q(Î² âˆ’r)]
Î±q cosh(qâ„“) + sinh(qâ„“)
Z â„“
0
Î±q cosh(qÎ·) + sinh(qÎ·)
(Î± + Î·)3
dÎ·âˆ’
Z Î²âˆ’r
0
sinh(qÎ·)
(r + Î·)3 dÎ·

,
where â„“= Î² âˆ’Î±. Note: V (r, s) = r U(r, s).
Step 6: Show that U(r, s) has simple poles at s = 0 and sn = âˆ’a2Î³2
n, where Î³n is the nth
root of Î±Î³n cos(Î³nâ„“) + sin(Î³nâ„“) = 0, n = 1, 2, 3, . . ..
Step 7: Use Bromwichâ€™s integral and show that
u(r, t) = A
1
r âˆ’1
Î²
  1
Î± âˆ’1
2
1
r + 1
Î²

âˆ’2Î±2
râ„“2
âˆ
X
n=0
sin[Î³n(Î² âˆ’r)] exp(âˆ’a2Î³2
nt)
sin2(Î³nâ„“)(Î² + Î±2â„“Î³2n)
Z 1
0
sin(Î³nâ„“Î·)
(Î´ âˆ’Î·)3 dÎ·

,
where Î³n is the nth root of Î±Î³ + tan(â„“Î³) = 0, and Î´ = 1 + Î±/â„“.
2.6 THE SOLUTION OF LAPLACEâ€™S EQUATION BY LAPLACE TRANSFORMS
Laplace transforms are useful in solving Laplaceâ€™s or Poissonâ€™s equation over a semi-
inï¬nite strip. The following problem illustrates this technique.
Let us solve Poissonâ€™s equation within a semi-inï¬nite circular cylinder
1
r
âˆ‚
âˆ‚r

râˆ‚u
âˆ‚r

+ âˆ‚2u
âˆ‚z2 = 2
b n(z)Î´(r âˆ’b),
0 â‰¤r < a,
0 < z < âˆ,
(2.6.1)
subject to the boundary conditions
u(r, 0) = 0,
lim
zâ†’âˆ|u(r, z)| < âˆ,
0 â‰¤r < a,
(2.6.2)

Advanced Transform Methods
155
and
u(a, z) = 0,
0 < z < âˆ,
(2.6.3)
where 0 < b < a. This problem gives the electrostatic potential within a semi-inï¬nite cylin-
der of radius a that is grounded and has the charge density of n(z) within an inï¬nitesimally
thin shell located at r = b.
Because the domain is semi-inï¬nite in the z direction, we introduce the Laplace trans-
form
U(r, s) =
Z âˆ
0
u(r, z) eâˆ’sz dz.
(2.6.4)
Thus, taking the Laplace transform of Equation 2.6.1, we have that
1
r
d
dr

rdU(r, s)
dr

+ s2U(r, s) âˆ’su(r, 0) âˆ’uz(r, 0) = 2
b N(s)Î´(r âˆ’b).
(2.6.5)
Although u(r, 0) = 0, uz(r, 0) is unknown and we denote its value by f(r).
Therefore,
Equation 2.6.5 becomes
1
r
d
dr

rdU(r, s)
dr

+ s2U(r, s) = f(r) + 2
b N(s)Î´(r âˆ’b),
0 â‰¤r < a,
(2.6.6)
with limrâ†’0 |U(r, s)| < âˆ, and U(a, s) = 0.
To solve Equation 2.6.6 we ï¬rst assume that we can rewrite f(r) as the Fourier-Bessel
series
f(r) =
âˆ
X
n=1
AnJ0(knr/a),
(2.6.7)
where kn is the nth root of the J0(k) = 0, and
An =
2
a2J2
1(kn)
Z a
0
f(r) J0(knr/a) r dr.
(2.6.8)
Similarly, the expansion for the delta function is
Î´(r âˆ’b) = 2b
a2
âˆ
X
n=1
J0(knb/a)J0(knr/a)
J2
1(kn)
,
(2.6.9)
because
Z a
0
Î´(r âˆ’b)J0(knr/a) r dr = b J0(knb/a).
(2.6.10)
Why we chose this particular expansion will become apparent shortly.
Thus, Equation 2.6.6 may be rewritten as
1
r
d
dr

rdU(r, s)
dr

+ s2U(r, s) = 2
a2
âˆ
X
n=1
2N(s)J0(knb/a) + ak
J2
1(kn)
J0(knr/a),
(2.6.11)
where ak =
R a
0 f(r) J0(knr/a) r dr.
The form of the right side of Equation 2.6.11 suggests that we seek solutions of the
form
U(r, s) =
âˆ
X
n=1
BnJ0(knr/a),
0 â‰¤r < a.
(2.6.12)

156
Advanced Engineering Mathematics: A Second Course
We now understand why we rewrote the right side of Equation 2.6.6 as a Fourier-Bessel
series; the solution U(r, s) automatically satisï¬es the boundary condition U(a, s) = 0. Sub-
stituting Equation 2.6.12 into Equation 2.6.11, we ï¬nd that
U(r, s) = 2
a2
âˆ
X
n=1
2N(s)J0(knb/a) + ak
(s2 âˆ’k2n/a2)J2
1(kn) J0(knr/a),
0 â‰¤r < a.
(2.6.13)
We have not yet determined ak. Note, however, that in order for the inverse of Equation
2.6.13 not to grow as eknz/a, the numerator must vanish when s = kn/a and s = kn/a is a
removable pole. Thus,
ak = âˆ’2N(kn/a)J0(knb/a),
(2.6.14)
and
U(r, s) = 4
a2
âˆ
X
n=1
[N(s) âˆ’N(kn/a)]J0(knb/a)
(s2 âˆ’k2n/a2)J2
1(kn)
J0(knr/a),
0 â‰¤r < a.
(2.6.15)
The inverse of U(r, s) then follows directly from simple inversions, the convolution theorem,
and the deï¬nition of the Laplace transform. The complete solution is
u(r, z) = 2
a
âˆ
X
n=1
J0(knb/a)J0(knr/a)
kn J2
1(kn)
Ã—
Z z
0
n(Ï„)ekn(zâˆ’Ï„)/a dÏ„ âˆ’
Z z
0
n(Ï„)eâˆ’kn(zâˆ’Ï„)/a dÏ„
âˆ’
Z âˆ
0
n(Ï„)eâˆ’knÏ„/aeknz/a dÏ„ +
Z âˆ
0
n(Ï„)eâˆ’knÏ„/aeâˆ’knz/a dÏ„

(2.6.16)
= 2
a
âˆ
X
n=1
J0(knb/a)J0(knr/a)
kn J2
1(kn)
(2.6.17)
Ã—
Z âˆ
0
n(Ï„)eâˆ’kn(z+Ï„)/a dÏ„ âˆ’
Z z
0
n(Ï„)eâˆ’kn(zâˆ’Ï„)/a dÏ„ âˆ’
Z âˆ
z
n(Ï„)eâˆ’kn(Ï„âˆ’z)/a dÏ„

.
Problems
1. Use Laplace transforms to solve
âˆ‚2u
âˆ‚x2 + âˆ‚2u
âˆ‚y2 = 0,
0 < x < âˆ,
0 < y < a,
subject to the boundary conditions u(0, y) = 1, limxâ†’âˆ|u(x, y)| < âˆ, 0 < y < a, and
u(x, 0) = u(x, a) = 0, 0 < x < âˆ.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
d2U
dy2 + s2U = s + f(s, y),
subject to the boundary conditions U(s, 0) = U(s, a) = 0.

Advanced Transform Methods
157
Step 2: Because
1 = 2
Ï€
âˆ
X
n=1
[1 âˆ’(âˆ’1)n]
n
sin
nÏ€y
a

and expanding f(s, y) in a half-range sine expansion:
f(s, y) =
âˆ
X
n=1
An sin
nÏ€y
a

, where An = 2
a
Z a
0
f(s, y) sin
nÏ€y
a

dy,
show that the diï¬€erential equations in Step 1 can be rewritten
d2U
dy2 + s2U =
âˆ
X
n=1
2s[1 âˆ’(âˆ’1)n]
nÏ€
+ An

sin
nÏ€y
a

,
Step 3: Show that the solution of the diï¬€erential equation in Step 2 is
U(s, y) =
âˆ
X
n=1
2s[1 âˆ’(âˆ’1)n] + nÏ€An
nÏ€(s2 âˆ’n2Ï€2/a2)
sin
nÏ€y
a

.
Step 4: For the solution to remain ï¬nite as x â†’âˆ, s = nÏ€/a cannot be a pole of the
transform U(s, y). Show that An = âˆ’2[1 âˆ’(âˆ’1)n]/a and
U(s, y) =
âˆ
X
n=1
2[1 âˆ’(âˆ’1)n]
nÏ€(s + nÏ€/a) sin
nÏ€y
a

.
Step 5: Take the inverse of U(s, y) term by term and show that
u(x, y) = 4
Ï€
âˆ
X
m=1
1
2m âˆ’1 exp

âˆ’(2m âˆ’1)Ï€x
a

sin
(2m âˆ’1)Ï€y
a

.
2. Use Laplace transforms to solve
1
r
âˆ‚
âˆ‚r

râˆ‚u
âˆ‚r

+ âˆ‚2u
âˆ‚z2 = 0,
0 â‰¤r < a,
0 < z < âˆ,
subject to the boundary conditions u(r, 0) = 1, limzâ†’âˆ|u(r, z)| < âˆ0 â‰¤r < a, and
limrâ†’0 |u(r, z)| < âˆ, u(a, z) = 0, 0 < z < âˆ.
Step 1: Take the Laplace transform of the partial diï¬€erential equation and boundary con-
ditions and show that
1
r
d
dr

rdU(r, s)
dr

+ s2U(r, s) = s + f(r),
0 â‰¤r < a,
with |U(0, s)| < âˆand U(a, s) = 0.
Step 2: Rewrite f(r) as the Fourier-Bessel series:
f(r) =
âˆ
X
n=1
AnJ0(knr/a),

158
Advanced Engineering Mathematics: A Second Course
where kn is the nth root of the J0(k) = 0 and
An =
2
a2J2
1(kn)
Z a
0
f(r) J0(knr/a) r dr.
Step 3: Because
1 = 2
âˆ
X
n=1
J0(knr/a)
kn J1(kn) ,
0 â‰¤r < a,
show that the diï¬€erential equation in Step 1 becomes
1
r
d
dr

rdU(r, s)
dr

+ s2U(r, s) = 2
a2
âˆ
X
n=1
sa2J1(kn) âˆ’knak
kn J2
1(kn)
J0
knr
a

,
where ak =
R a
0 f(r)J0(knr/a) r dr.
Step 4: Show that the solution to the diï¬€erential equation is
U(r, s) = 2
a2
âˆ
X
n=1
sa2J1(kn) âˆ’knak
(s2 âˆ’k2n/a2)knJ2
1(kn)J0(knr/a),
0 â‰¤r < a.
Step 5: Because s = kn/a cannot be a pole of U(r, s), ak = aJ1(kn). Therefore,
U(r, s) = 2
âˆ
X
n=1
J0(knr/a)
kn(s + kn/a)J1(kn),
0 â‰¤r < a.
Step 6: Find the inverse of U(r, s) and show that
u(r, z) = 2
âˆ
X
n=1
J0(knr/a)
kn J1(kn) eâˆ’knz/a.
Further Readings
Debnath, L., and D. Bhatta, 2015: Integral Transforms and Their Applications.
CRC
Press, 792 pp. A book that covers Laplace, Fourier, z-, Hankel, Mellin, Hilbert and Stieltjes
transforms and their application.
Duï¬€y, D. G., 2015: Transform Methods for Solving Partial Diï¬€erential Equations. CRC
Press, 728 pp. This book covers the material of this chapter in greater depth.

0.0
1.0
2.0
3.0
Ï‰Î¤
0.1
1.0
10.0
Ratio of quadrature amplitudes to ideal integration      
Trapezoidal
Simpsonâ€™s
3/8âˆ’rule
Rule
Simpsonâ€™s
1/3âˆ’rule
Ideal Rule
Chapter 3
The Z-Transform
Since the Second World War, the rise of digital technology has resulted in a corre-
sponding demand for designing and understanding discrete-time (data sampled) systems.
These systems are governed by diï¬€erence equations in which members of the sequence yn
are coupled to each other.
One source of diï¬€erence equations is the numerical evaluation of integrals on a digital
computer.
Because we can only have values at discrete time points tk = kT for k =
0, 1, 2, . . ., the value of the integral y(t) =
R t
0 f(Ï„) dÏ„ is
y(kT) =
Z kT
0
f(Ï„) dÏ„ =
Z (kâˆ’1)T
0
f(Ï„) dÏ„ +
Z kT
(kâˆ’1)T
f(Ï„) dÏ„
(3.0.1)
= y[(k âˆ’1)T] +
Z kT
(kâˆ’1)T
f(Ï„) dÏ„ = y[(k âˆ’1)T] + Tf(kT),
(3.0.2)
because
R kT
(kâˆ’1)T f(Ï„) dÏ„ â‰ˆTf(kT). The right side of Equation 3.0.2 is an example of a
ï¬rst-order diï¬€erence equation because the numerical scheme couples the sequence value
y(kT) directly to the previous sequence value y[(k âˆ’1)T]. If Equation 3.0.2 had contained
y[(k âˆ’2)T], then it would have been a second-order diï¬€erence equation, and so forth.
Although we could use the conventional Laplace transform to solve these diï¬€erence
equations, the use of z-transforms can greatly facilitate the analysis, especially when we
only desire responses at the sampling instants. Often the entire analysis can be done using
only the transforms and the analyst does not actually ï¬nd the sequence y(kT).
In this chapter we will ï¬rst deï¬ne the z-transform and discuss its properties. Then we
will show how to ï¬nd its inverse. Finally, we shall use them to solve diï¬€erence equations.
159

160
Advanced Engineering Mathematics: A Second Course
f(t)
t
t
t
T
2T
3T
T
2T
3T
0
0
0
Îµ
f  (t)
S
f  (t)
S*
Figure 3.1.1: Schematic of how a continuous function f(t) is sampled by a narrow-width pulse sampler
fâˆ—
S(t) and an ideal sampler fS(t).
3.1 THE RELATIONSHIP OF THE Z-TRANSFORM TO THE LAPLACE TRANSFORM1
Let f(t) be a continuous function that an instrument samples every T units of time.
We denote this data-sampled function by f âˆ—
S(t). See Figure 3.1.1. Taking Ç«, the duration of
an individual sampling event, to be small, we may approximate the narrow-width pulse in
Figure 3.1.1 by ï¬‚at-topped pulses. Then f âˆ—
S(t) approximately equals
f âˆ—
S(t) â‰ˆ1
Ç«
âˆ
X
n=0
f(nT) [H(t âˆ’nT + Ç«/2) âˆ’H(t âˆ’nT âˆ’Ç«/2)] ,
(3.1.1)
if Ç« â‰ªT.
Clearly the presence of Ç« is troublesome in Equation 3.1.1; it adds one more parameter
to our problem. For this reason we introduce the concept of the ideal sampler, where the
sampling time becomes inï¬nitesimally small so that
fS(t) = lim
Ç«â†’0
âˆ
X
n=0
f(nT)
H(t âˆ’nT + Ç«/2) âˆ’H(t âˆ’nT âˆ’Ç«/2)
Ç«

=
âˆ
X
n=0
f(nT)Î´(t âˆ’nT).
(3.1.2)
Let us now ï¬nd the Laplace transform of this data-sampled function. From the linearity
property of Laplace transforms,
FS(s) = L[fS(t)] = L
"
âˆ
X
n=0
f(nT)Î´(t âˆ’nT)
#
=
âˆ
X
n=0
f(nT)L[Î´(t âˆ’nT)].
(3.1.3)
1 Gera (Gera, A. E., 1999: The relationship between the z-transform and the discrete-time Fourier
transform.
IEEE Trans.
Auto.
Control, AC-44, 370â€“371) explored the general relationship between
the one-sided discrete-time Fourier transform and the one-sided z-transform. See also NaumoviÂ´c, M. B.,
2001: Interrelationship between the one-sided discrete-time Fourier transform and one-sided delta transform.
Electr. Engng., 83, 99â€“101.

The Z-Transform
161
Because L[Î´(t âˆ’nT)] = eâˆ’nsT , Equation 3.1.3 simpliï¬es to
FS(s) =
âˆ
X
n=0
f(nT)eâˆ’nsT .
(3.1.4)
If we now make the substitution that z = esT , then FS(s) becomes
F(z) = Z(fn) =
âˆ
X
n=0
fnzâˆ’n,
(3.1.5)
where F(z) is the one-sided z-transform2 of the sequence f(nT), which we shall now denote
by fn. Here Z denotes the operation of taking the z-transform while Zâˆ’1 represents the
inverse z-transformation. We will consider methods for ï¬nding the inverse z-transform in
Section 3.3.
Just as the Laplace transform was deï¬ned by an integration in t, the z-transform is
deï¬ned by a power series (Laurent series) in z. Consequently, every z-transform has a region
of convergence that must be implicitly understood if not explicitly stated. Furthermore,
just as the Laplace integral diverged for certain functions, there are sequences where the
associated power series diverges and its z-transform does not exist.
Consider now the following examples of how to ï¬nd the z-transform.
â€¢ Example 3.1.1
Given the unit sequence fn = 1, n â‰¥0, let us ï¬nd F(z). Substituting fn into the
deï¬nition of the z-transform leads to
F(z) =
âˆ
X
n=0
zâˆ’n =
z
z âˆ’1,
(3.1.6)
because Pâˆ
n=0 zâˆ’n is a complex-valued geometric series with common ratio zâˆ’1. This series
converges if |zâˆ’1| < 1 or |z| > 1, which gives the region of convergence of F(z).
MATLABâ€™s symbolic toolbox provides an alternative to the hand computation of the
z-transform. In the present case, the command
>> syms z; syms n positive
>> ztrans(1,n,z)
yields
ans =
z/(z-1)
âŠ“âŠ”
â€¢ Example 3.1.2
Let us ï¬nd the z-transform of the sequence
fn = eâˆ’anT ,
n â‰¥0,
(3.1.7)
2 The standard reference is Jury, E. I., 1964: Theory and Application of the z-Transform Method. John
Wiley & Sons, 330 pp.

162
Advanced Engineering Mathematics: A Second Course
for a real and a imaginary.
For a real, substitution of the sequence into the deï¬nition of the z-transform yields
F(z) =
âˆ
X
n=0
eâˆ’anT zâˆ’n =
âˆ
X
n=0
 eâˆ’aT zâˆ’1n.
(3.1.8)
If u = eâˆ’aT zâˆ’1, then Equation 3.1.8 is a geometric series so that
F(z) =
âˆ
X
n=0
un =
1
1 âˆ’u.
(3.1.9)
Because |u| = eâˆ’aT |zâˆ’1|, the condition for convergence is that |z| > eâˆ’aT . Thus,
F(z) =
z
z âˆ’eâˆ’aT ,
|z| > eâˆ’aT .
(3.1.10)
For imaginary a, the inï¬nite series in Equation 3.1.8 converges if |z| > 1, because
|u| = |zâˆ’1| when a is imaginary. Thus,
F(z) =
z
z âˆ’eâˆ’aT ,
|z| > 1.
(3.1.11)
Although the z-transforms in Equation 3.1.10 and Equation 3.1.11 are the same in these
two cases, the corresponding regions of convergence are diï¬€erent. If a is a complex number,
then
F(z) =
z
z âˆ’eâˆ’aT ,
|z| > |eâˆ’aT |.
(3.1.12)
Checking our work using MATLAB, we type the commands:
>> syms a z; syms n T positive
>> ztrans(exp(-a*n*T),n,z);
>> simplify(ans)
which yields
ans =
z*exp(a*T)/(z*exp(a*T)-1)
âŠ“âŠ”
â€¢ Example 3.1.3
Let us ï¬nd the z-transform of the sinusoidal sequence
fn = cos(nÏ‰T),
n â‰¥0.
(3.1.13)
Substituting Equation 3.1.13 into the deï¬nition of the z-transform results in
F(z) =
âˆ
X
n=0
cos(nÏ‰T)zâˆ’n.
(3.1.14)
From Eulerâ€™s formula,
cos(nÏ‰T) = 1
2(einÏ‰T + eâˆ’inÏ‰T ),
(3.1.15)

The Z-Transform
163
so that Equation 3.1.14 becomes
F(z) = 1
2
âˆ
X
n=0

einÏ‰T zâˆ’n + eâˆ’inÏ‰T zâˆ’n

,
(3.1.16)
or
F(z) = 1
2

Z(einÏ‰T ) + Z(eâˆ’inÏ‰T )

.
(3.1.17)
From Equation 3.1.11,
Z(eÂ±inÏ‰T ) =
z
z âˆ’eÂ±iÏ‰T ,
|z| > 1.
(3.1.18)
Substituting Equation 3.1.18 into Equation 3.1.17 and simplifying yields
F(z) =
z[z âˆ’cos(Ï‰T)]
z2 âˆ’2z cos(Ï‰T) + 1,
|z| > 1.
(3.1.19)
âŠ“âŠ”
â€¢ Example 3.1.4
Let us ï¬nd the z-transform for the sequence
fn =
 1,
0 â‰¤n â‰¤5,
( 1
2)n,
6 â‰¤n.
(3.1.20)
From the deï¬nition of the z-transform,
Z(fn) = F(z) =
5
X
n=0
zâˆ’n +
âˆ
X
n=6
 1
2z
n
.
(3.1.21)
= 1 + 1
z + 1
z2 + 1
z3 + 1
z4 + 1
z5 +
2z
2z âˆ’1
âˆ’1 âˆ’1
2z âˆ’
1
4z2 âˆ’
1
8z3 âˆ’
1
16z4 âˆ’
1
32z5
(3.1.22)
=
2z
2z âˆ’1 + 1
2z +
3
4z2 +
7
8z3 +
15
16z4 +
31
32z5 .
(3.1.23)
We could also have obtained Equation 3.1.23 via MATLAB by typing the commands:
>> syms z; syms n positive
>> ztrans(â€™1+((1/2)^n-1)*Heaviside(n-6)â€™,n,z)
which yields
ans =
2*z/(2*z-1)+1/2/z+3/4/z^2+7/8/z^3+15/16/z^4+31/32/z^5
âŠ“âŠ”

164
Advanced Engineering Mathematics: A Second Course
We summarize some of the more commonly encountered sequences and their transforms
in Table 3.1.1 along with their regions of convergence.
â€¢ Example 3.1.5
In many engineering studies, the analysis is done entirely using transforms without
actually ï¬nding any inverses. Consequently, it is useful to compare and contrast how various
transforms behave in very simple test problems.
Consider the time function f(t) = aeâˆ’atH(t), a > 0. Its Laplace and Fourier transform
are identical, namely a/(a + iÏ‰), if we set s = iÏ‰. In Figure 3.1.2 we illustrate its behavior
as a function of positive Ï‰.
Let us now generate the sequence of observations that we would measure if we sampled
f(t) every T units of time apart: fn = aeâˆ’anT . Taking the z-transform of this sequence, it
equals az/
 z âˆ’eâˆ’aT 
. Recalling that z = esT = eiÏ‰T , we can also plot this transform as
a function of positive Ï‰. For small Ï‰, the transforms agree, but as Ï‰ becomes larger they
diverge markedly. Why does this occur?
Recall that the z-transform is computed from a sequence comprised of samples from a
continuous signal. One very important ï¬‚aw in sampled data is the possible misrepresenta-
tion of high-frequency eï¬€ects as lower-frequency phenomena. It is this aliasing or folding
eï¬€ect that we are observing here. Consequently, the z-transform of a sampled record can
diï¬€er markedly from the corresponding Laplace or Fourier transforms of the continuous
record at frequencies above one half of the sampling frequency. This also suggests that care
should be exercised in interpolating between sampling instants. Indeed, in those applica-
tions where the output between sampling instants is very important, such as in a hybrid
mixture of digital and analog systems, we must apply the so-called â€œmodiï¬ed z-transform.â€
Problems
From the fundamental deï¬nition of the z-transform, ï¬nd the transform of the following
sequences, where n â‰¥0. Then check your answer using MATLAB.
1. fn =
1
2
n
2. fn = einÎ¸
3. fn =
 1,
0 â‰¤n â‰¤5,
0,
5 < n
4. fn =
(   1
2
n ,
n = 0, 1, . . . , 10
  1
4
n ,
n â‰¥11
5. fn =
( 0,
n = 0,
âˆ’1,
n = 1,
an,
n â‰¥2
3.2 SOME USEFUL PROPERTIES
In principle we could construct any desired transform from the deï¬nition of the z-
transform. However, there are several general theorems that are much more eï¬€ective in
ï¬nding new transforms.

The Z-Transform
165
Table 3.1.1: Z-Transforms of Some Commonly Used Sequences
fn, n â‰¥0
F(z)
Region of
convergence
1.
f0 = k = const.
k
|z| > 0
fn = 0, n â‰¥1
2.
fm = k = const.
kzâˆ’m
|z| > 0
fn = 0, for all values
of n Ì¸= m
3.
k = constant
kz/(z âˆ’1)
|z| > 1
4.
kn
kz/(z âˆ’1)2
|z| > 1
5.
kn2
kz(z + 1)/(z âˆ’1)3
|z| > 1
6. keâˆ’anT , a complex
kz/
 z âˆ’eâˆ’aT 
|z| > |eâˆ’aT |
7. kneâˆ’anT , a complex
kzeâˆ’aT
(zâˆ’eâˆ’aT )2
|z| > |eâˆ’aT |
8.
sin(Ï‰0nT)
z sin(Ï‰0T )
z2âˆ’2z cos(Ï‰0T )+1
|z| > 1
9.
cos(Ï‰0nT)
z[zâˆ’cos(Ï‰0T )]
z2âˆ’2z cos(Ï‰0T )+1
|z| > 1
10.
eâˆ’anT sin(Ï‰0nT)
zeâˆ’aT sin(Ï‰0T )
z2âˆ’2zeâˆ’aT cos(Ï‰0T )+eâˆ’2aT
|z| > eâˆ’aT
11.
eâˆ’anT cos(Ï‰0nT)
zeâˆ’aT [zeaT âˆ’cos(Ï‰0T )]
z2âˆ’2zeâˆ’aT cos(Ï‰0T )+eâˆ’2aT
|z| > eâˆ’aT
12.
Î±n , Î± constant
z/(z âˆ’Î±)
|z| > |Î±|
13.
nÎ±n
Î±z/(z âˆ’Î±)2
|z| > |Î±|
14.
n2Î±n
Î±z(z + Î±)/(z âˆ’Î±)3
|z| > |Î±|
15.
sinh(Ï‰0nT)
z sinh(Ï‰0T )
z2âˆ’2z cosh(Ï‰0T )+1
|z| > cosh(Ï‰0T)
16.
cosh(Ï‰0nT)
z[zâˆ’cosh(Ï‰0T )]
z2âˆ’2z cosh(Ï‰0T )+1
|z| > sinh(Ï‰0T)
17.
an/n!
ea/z
|z| > 0
18.
[ln(a)]n/n!
a1/z
|z| > 0

166
Advanced Engineering Mathematics: A Second Course
0.0
2.5
5.0
7.5
10.0
12.5
Ï‰
10
âˆ’4
10
âˆ’3
10
âˆ’2
10
âˆ’1
10
0
amplitude of transform
a = 0.1
a = 0.01
a = 0.001
Figure 3.1.2: The amplitude of the Laplace or Fourier transform (solid line) for the function aeâˆ’atH(t)
and the z-transform (dashed line) for the sequence fn = aeâˆ’anT as a function of frequency Ï‰ for various
positive values of a and T = 1.
Linearity
From the deï¬nition of the z-transform, it immediately follows that
if
hn = c1fn + c2gn,
then
H(z) = c1F(z) + c2G(z),
(3.2.1)
where F(z) = Z(fn), G(z) = Z(gn), H(z) = Z(hn), and c1, c2 are arbitrary constants.
Multiplication by an expo-
nential sequence
If
gn = eâˆ’anT fn,
n â‰¥0,
then
G(z) = F(zeaT ).
(3.2.2)
This follows from
G(z) = Z(gn) =
âˆ
X
n=0
gn zâˆ’n =
âˆ
X
n=0
eâˆ’anT fn zâˆ’n =
âˆ
X
n=0
fn(zeaT )âˆ’n = F(zeaT ).
(3.2.3)
This is the z-transform analog to the ï¬rst shifting theorem in Laplace transforms.
Shifting
The eï¬€ect of shifting depends upon whether it is to the right or to the left, as Table
3.2.1 illustrates. For the sequence fnâˆ’2, no values from the sequence fn are lost; thus,
we anticipate that the z-transform of fnâˆ’2 only involves F(z). However, in forming the

The Z-Transform
167
Table 3.2.1: Examples of Shifting Involving Sequences
n
fn
fnâˆ’2
fn+2
0
1
0
4
1
2
0
8
2
4
1
16
3
8
2
64
4
16
4
128
...
...
...
...
sequence fn+2, the ï¬rst two values of fn are lost, and we anticipate that the z-transform of
fn+2 cannot be expressed solely in terms of F(z) but must include those two lost pieces of
information.
Let us now conï¬rm these conjectures by ï¬nding the z-transform of fn+1, which is a
sequence that has been shifted one step to the left. From the deï¬nition of the z-transform,
it follows that
Z(fn+1) =
âˆ
X
n=0
fn+1zâˆ’n = z
âˆ
X
n=0
fn+1zâˆ’(n+1)
(3.2.4)
or
Z(fn+1) = z
âˆ
X
k=1
fkzâˆ’k + zf0 âˆ’zf0,
(3.2.5)
where we added zero in Equation 3.2.5. This algebraic trick allows us to collapse the ï¬rst
two terms on the right side of Equation 3.2.5 into one and
Z(fn+1) = zF(z) âˆ’zf0.
(3.2.6)
In a similar manner, repeated applications of Equation 3.2.6 yield
Z(fn+m) = zmF(z) âˆ’zmf0 âˆ’zmâˆ’1f1 âˆ’. . . âˆ’zfmâˆ’1,
(3.2.7)
where m > 0. This shifting operation transforms fn+m into an algebraic expression involv-
ing m. Furthermore, we introduced initial sequence values, just as we introduced initial
conditions when we took the Laplace transform of the nth derivative of f(t). We will make
frequent use of this property in solving diï¬€erence equations in Section 3.4.
Consider now shifting to the right by the positive integer k,
gn = fnâˆ’kHnâˆ’k,
n â‰¥0,
(3.2.8)
where Hnâˆ’k = 0 for n < k and 1 for n â‰¥k. Then the z-transform of Equation 3.2.8 is
G(z) = zâˆ’kF(z),
(3.2.9)
where G(z) = Z(gn), and F(z) = Z(fn). This follows from
G(z) =
âˆ
X
n=0
gnzâˆ’n =
âˆ
X
n=0
fnâˆ’kHnâˆ’kzâˆ’n = zâˆ’k
âˆ
X
n=k
fnâˆ’kzâˆ’(nâˆ’k) = zâˆ’k
âˆ
X
m=0
fmzâˆ’m = zâˆ’kF(z).
(3.2.10)

168
Advanced Engineering Mathematics: A Second Course
This result is the z-transform analog to the second shifting theorem in Laplace transforms.
In symbolic calculations involving MATLAB, the operator Hnâˆ’k can be expressed by
Heaviside(n-k).
Initial-value theorem
The initial value of the sequence fn, f0, can be computed from F(z) using the initial-
value theorem:
f0 = lim
zâ†’âˆF(z).
(3.2.11)
From the deï¬nition of the z-transform,
F(z) =
âˆ
X
n=0
fnzâˆ’n = f0 + f1zâˆ’1 + f2zâˆ’2 + . . . .
(3.2.12)
In the limit of z â†’âˆ, we obtain the desired result.
Final-value theorem
The value of fn, as n â†’âˆ, is given by the ï¬nal-value theorem:
fâˆ= lim
zâ†’1 (z âˆ’1)F(z),
(3.2.13)
where F(z) is the z-transform of fn.
We begin by noting that
Z(fn+1 âˆ’fn) = lim
nâ†’âˆ
n
X
k=0
(fk+1 âˆ’fk)zâˆ’k.
(3.2.14)
Using the shifting theorem on the left side of Equation 3.2.14,
zF(z) âˆ’zf0 âˆ’F(z) = lim
nâ†’âˆ
n
X
k=0
(fk+1 âˆ’fk)zâˆ’k.
(3.2.15)
Applying the limit as z approaches 1 to both sides of Equation 3.2.15:
lim
zâ†’1 (z âˆ’1)F(z) âˆ’f0 = lim
nâ†’âˆ
n
X
k=0
(fk+1 âˆ’fk)
(3.2.16)
= lim
nâ†’âˆ

(f1 âˆ’f0) + (f2 âˆ’f1) + . . . + (fn âˆ’fnâˆ’1) + (fn+1 âˆ’fn) + . . .

(3.2.17)
= lim
nâ†’âˆ(âˆ’f0 + fn+1) = âˆ’f0 + fâˆ.
(3.2.18)
Consequently,
fâˆ= lim
zâ†’1 (z âˆ’1)F(z).
(3.2.19)

The Z-Transform
169
Note that this limit has meaning only if fâˆexists. This occurs if F(z) has no second-order
or higher poles on the unit circle and no poles outside the unit circle.
Multiplication by n
Given
gn = nfn,
n â‰¥0,
(3.2.20)
this theorem states that
G(z) = âˆ’z dF(z)
dz
,
(3.2.21)
where G(z) = Z(gn), and F(z) = Z(fn).
This follows from
G(z) =
âˆ
X
n=0
gnzâˆ’n =
âˆ
X
n=0
nfnzâˆ’n = z
âˆ
X
n=0
nfnzâˆ’nâˆ’1 = âˆ’z dF(z)
dz
.
(3.2.22)
Periodic sequence theorem
Consider the N-periodic sequence:
fn = {f0f1f2 . . . fNâˆ’1
|
{z
}
ï¬rst period
f0f1 . . .},
(3.2.23)
and the related sequence:
xn =

fn,
0 â‰¤n â‰¤N âˆ’1,
0,
N â‰¤n.
(3.2.24)
This theorem allows us to ï¬nd the z-transform of fn if we can ï¬nd the z-transform of xn
via the relationship
F(z) =
X(z)
1 âˆ’zâˆ’N ,
|zN| > 1,
(3.2.25)
where X(z) = Z(xn).
This follows from
F(z) =
âˆ
X
n=0
fnzâˆ’n =
Nâˆ’1
X
n=0
xnzâˆ’n +
2Nâˆ’1
X
n=N
xnâˆ’Nzâˆ’n +
3Nâˆ’1
X
n=2N
xnâˆ’2Nzâˆ’n + Â· Â· Â· .
(3.2.26)
Application of the shifting theorem in Equation 3.2.26 leads to
F(z) = X(z) + zâˆ’NX(z) + zâˆ’2NX(z)+ = X(z)

1 + zâˆ’N + zâˆ’2N + Â· Â· Â·

.
(3.2.27)

170
Advanced Engineering Mathematics: A Second Course
Equation 3.2.27 contains an inï¬nite geometric series with common ratio zâˆ’N, which con-
verges if |zâˆ’N| < 1. Thus,
F(z) =
X(z)
1 âˆ’zâˆ’N ,
|zN| > 1.
(3.2.28)
Convolution
Given the sequences fn and gn, the convolution product of these two sequences is
wn = fn âˆ—gn =
n
X
k=0
fkgnâˆ’k =
n
X
k=0
fnâˆ’kgk.
(3.2.29)
Given F(z) and G(z), we then have that W(z) = F(z)G(z).
This follows from
W(z) =
âˆ
X
n=0
"
n
X
k=0
fkgnâˆ’k
#
zâˆ’n =
âˆ
X
n=0
âˆ
X
k=0
fkgnâˆ’kzâˆ’n,
(3.2.30)
because gnâˆ’k = 0 for k > n. Reversing the order of summation and letting m = n âˆ’k,
W(z) =
âˆ
X
k=0
âˆ
X
m=âˆ’k
fkgmzâˆ’(m+k) =
" âˆ
X
k=0
fkzâˆ’k
# "
âˆ
X
m=0
gmzâˆ’m
#
= F(z)G(z).
(3.2.31)
We can use MATLABâ€™s command conv( ), which multiplies two polynomials to perform
discrete convolution as follows:
>>x = [1 1 1 1 1 1 1];
>>y = [1 2 4 8 16 32 64];
>>z = conv(x,y)
produces
z =
1 3 7 15 31 63 127 126 124 120 112 96 64
The ï¬rst seven values of z contain the convolution of the sequence x with the sequence y.
Consider now the following examples of the properties discussed in this section.
â€¢ Example 3.2.1
From
Z(an) =
1
1 âˆ’azâˆ’1 ,
(3.2.32)
for n â‰¥0 and |z| > |a| , we have that
Z
 einx
=
1
1 âˆ’eixzâˆ’1 ,
(3.2.33)

The Z-Transform
171
and
Z
 eâˆ’inx
=
1
1 âˆ’eâˆ’ixzâˆ’1 ,
(3.2.34)
if n â‰¥0 and |z| > 1. Therefore, the sequence fn = cos(nx) has the z-transform
F(z) = Z[cos(nx)] = 1
2Z
 einx
+ 1
2Z
 eâˆ’inx
(3.2.35)
= 1
2
1
1 âˆ’eixzâˆ’1 + 1
2
1
1 âˆ’eâˆ’ixzâˆ’1 =
1 âˆ’cos(x)zâˆ’1
1 âˆ’2 cos(x)zâˆ’1 + zâˆ’2 .
(3.2.36)
âŠ“âŠ”
â€¢ Example 3.2.2
Using the z-transform,
Z(an) =
1
1 âˆ’azâˆ’1 ,
n â‰¥0,
(3.2.37)
we ï¬nd that
Z(nan) = âˆ’z d
dz
h 1 âˆ’azâˆ’1âˆ’1i
= (âˆ’z)(âˆ’1)
 1 âˆ’azâˆ’1âˆ’2 (âˆ’a)(âˆ’1)zâˆ’2
(3.2.38)
=
azâˆ’1
(1 âˆ’azâˆ’1)2 =
az
(z âˆ’a)2 .
(3.2.39)
âŠ“âŠ”
â€¢ Example 3.2.3
Consider F(z) = 2azâˆ’1/(1 âˆ’azâˆ’1)3, where |a| < |z| and |a| < 1. Here we have that
f0 = lim
zâ†’âˆF(z) = lim
zâ†’âˆ
2azâˆ’1
(1 âˆ’azâˆ’1)3 = 0
(3.2.40)
from the initial-value theorem. This agrees with the inverse of F(z):
fn = n(n + 1)an,
n â‰¥0.
(3.2.41)
If the z-transform consists of the ratio of two polynomials, we can use MATLAB to ï¬nd
f0. For example, if F(z) = 2z2/(z âˆ’1)3, we can ï¬nd f0 as follows:
>>num = [2 0 0];
>>den = conv([1 -1],[1 -1]);
>>den = conv(den,[1 -1]);
>>initialvalue = polyval(num,1e20) / polyval(den,1e20)
initialvalue =
2.0000e-20
Therefore, f0 = 0.
âŠ“âŠ”

172
Advanced Engineering Mathematics: A Second Course
â€¢ Example 3.2.4
Given the z-transform F(z) = (1 âˆ’a)z/[(z âˆ’1)(z âˆ’a)], where |z| > 1 > a > 0, then
from the ï¬nal-value theorem we have that
lim
nâ†’âˆfn = lim
zâ†’1(z âˆ’1)F(z) = lim
zâ†’1
1 âˆ’a
1 âˆ’azâˆ’1 = 1.
(3.2.42)
This is consistent with the inverse transform fn = 1 âˆ’an with n â‰¥0.
âŠ“âŠ”
â€¢ Example 3.2.5
Using the sequences fn = 1 and gn = an, where a is real, verify the convolution
theorem.
We ï¬rst compute the convolution of fn with gn, namely
wn = fn âˆ—gn =
n
X
k=0
ak =
1
1 âˆ’a âˆ’an+1
1 âˆ’a.
(3.2.43)
Taking the z-transform of wn,
W(z) =
z
(1 âˆ’a)(z âˆ’1) âˆ’
az
(1 âˆ’a)(z âˆ’a) =
z2
(z âˆ’1)(z âˆ’a) = F(z)G(z),
(3.2.44)
and the convolution theorem holds true for this special case.
Problems
Use the properties of z-transforms and Table 3.1.1 to ï¬nd the z-transform of the following
sequences. Then check your answer using MATLAB.
1. fn = nTeâˆ’anT
2. fn =

0,
n = 0
nanâˆ’1,
n â‰¥1
3. fn =

0,
n = 0
n2anâˆ’1,
n â‰¥1
4. fn = an cos(n)
[Hint : Use cos(n) = 1
2(ein + eâˆ’in)]
5. fn = cos(n âˆ’2)Hnâˆ’2
6. fn = 3 + eâˆ’2nT
7. fn = sin(nÏ‰0T + Î¸),
8. fn =
ï£±
ï£´
ï£²
ï£´
ï£³
0,
n = 0
1,
n = 1
2,
n = 2
1,
n = 3,
fn+4 = fn
9. fn = (âˆ’1)n
(Hint: fn is a periodic sequence.)
10. Using the property stated in Equation 3.2.20 and Equation 3.2.21 twice, ï¬nd the z-
transform of n2 = n[n(1)n]. Then verify your result using MATLAB.

The Z-Transform
173
11. Verify the convolution theorem using the sequences fn = gn = 1. Then check your
results using MATLAB.
12. Verify the convolution theorem using the sequences fn = 1 and gn = n. Then check
your results using MATLAB.
13. Verify the convolution theorem using the sequences fn = gn = 1/(n!). Then check
your results using MATLAB. Hint: Use the binomial theorem with x = 1 to evaluate the
summation.
14. If a is a real number, show that Z(anfn) = F(z/a), where Z(fn) = F(z).
3.3 INVERSE Z-TRANSFORMS
In the previous two sections we dealt with ï¬nding the z-transform. In this section we
ï¬nd fn by inverting the z-transform F(z). There are four methods for ï¬nding the inverse:
(1) power series, (2) recursion, (3) partial fractions, and (4) the residue method. We will
discuss each technique individually. The ï¬rst three apply only to those functions F(z) that
are rational functions while the residue method is more general. For symbolic computations
with MATLAB, you can use iztrans.
Power series
By means of the long-division process, we can always rewrite F(z) as the Laurent
expansion:
F(z) = a0 + a1zâˆ’1 + a2zâˆ’2 + Â· Â· Â· .
(3.3.1)
From the deï¬nition of the z-transform,
F(z) =
âˆ
X
n=0
fnzâˆ’n = f0 + f1zâˆ’1 + f2zâˆ’2 + Â· Â· Â· ,
(3.3.2)
the desired sequence fn is given by an.
â€¢ Example 3.3.1
Let
F(z) = z + 1
2z âˆ’2 = N(z)
D(z) .
(3.3.3)
Using long division, N(z) is divided by D(z) and we obtain
F(z) = 1
2 + zâˆ’1 + zâˆ’2 + zâˆ’3 + zâˆ’4 + Â· Â· Â· .
(3.3.4)
Therefore,
a0 = 1
2, a1 = 1, a2 = 1, a3 = 1, a4 = 1, etc.,
(3.3.5)
which suggests that f0 = 1
2 and fn = 1 for n â‰¥1 is the inverse of F(z).
âŠ“âŠ”

174
Advanced Engineering Mathematics: A Second Course
â€¢ Example 3.3.2
Let us ï¬nd the inverse of the z-transform:
F(z) =
2z2 âˆ’1.5z
z2 âˆ’1.5z + 0.5.
(3.3.6)
By the long-division process, we have that
2
+
1.5zâˆ’1
+
1.25zâˆ’2
+
1.125zâˆ’3
+
Â· Â· Â·
z2 âˆ’1.5z + 0.5
2z2
âˆ’
1.5z
2z2
âˆ’
3z
+
1
1.5z
âˆ’
1
1.5z
âˆ’
2.25
+
0.750zâˆ’1
1.25
âˆ’
0.750zâˆ’1
1.25
âˆ’
1.870zâˆ’1
+
Â· Â· Â·
1.125zâˆ’1
+
Â· Â· Â·
Thus, f0 = 2, f1 = 1.5, f2 = 1.25, f3 = 1.125, and so forth, or fn = 1 + ( 1
2)n. In general,
this technique only produces numerical values for some of the elements of the sequence.
Note also that our long division must always yield the power series Equation 3.3.1 in order
for this method to be of any use.
To check our answer using MATLAB, we type the commands:
syms z; syms n positive
iztrans((2*z^2 - 1.5*z)/(z^2 - 1.5*z + 0.5),z,n)
which yields
ans =
1 + (1/2)^n
âŠ“âŠ”
Recursive method
An alternative to long division was suggested3 several years ago. It obtains the inverse
recursively.
We begin by assuming that the z-transform is of the form
F(z) = a0zm + a1zmâˆ’1 + a2zmâˆ’2 + Â· Â· Â· + amâˆ’1z + am
b0zm + b1zmâˆ’1 + b2zmâˆ’2 + Â· Â· Â· + bmâˆ’1z + bm
,
(3.3.7)
where some of the coeï¬ƒcients ai and bi may be zero and b0 Ì¸= 0. Applying the initial-value
theorem,
f0 = lim
zâ†’âˆF(z) = a0/b0.
(3.3.8)
3 Jury, E. I., 1964: Theory and Application of the z-Transform Method. John Wiley & Sons, p. 41;
Pierre, D. A., 1963:
A tabular algorithm for z-transform inversion.
Control Engng., 10(9), 110â€“111;
Jenkins, L. B., 1967: A useful recursive form for obtaining inverse z-transforms. Proc. IEEE, 55, 574â€“575.

The Z-Transform
175
Next, we apply the initial-value theorem to z[F(z) âˆ’f0] and ï¬nd that
f1 = lim
zâ†’âˆz[F(z) âˆ’f0]
(3.3.9)
= lim
zâ†’âˆz (a0 âˆ’b0f0)zm + (a1 âˆ’b1f0)zmâˆ’1 + Â· Â· Â· + (am âˆ’bmf0)
b0zm + b1zmâˆ’1 + b2zmâˆ’2 + Â· Â· Â· + bmâˆ’1z + bm
(3.3.10)
= (a1 âˆ’b1f0)/b0.
(3.3.11)
Note that the coeï¬ƒcient a0 âˆ’b0f0 = 0 from Equation 3.3.8. Similarly,
f2 = lim
zâ†’âˆz[zF(z) âˆ’zf0 âˆ’f1]
(3.3.12)
= lim
zâ†’âˆz (a0 âˆ’b0f0)zm+1 + (a1 âˆ’b1f0 âˆ’b0f1)zm + (a2 âˆ’b2f0 âˆ’b1f1)zmâˆ’1 + Â· Â· Â· âˆ’bmf1
b0zm + b1zmâˆ’1 + b2zmâˆ’2 + Â· Â· Â· + bmâˆ’1z + bm
(3.3.13)
= (a2 âˆ’b2f0 âˆ’b1f1)/b0
(3.3.14)
because a0 âˆ’b0f0 = a1 âˆ’b1f0 âˆ’f1b0 = 0. Continuing this process, we ï¬nally have that
fn = (an âˆ’bnf0 âˆ’bnâˆ’1f1 âˆ’Â· Â· Â· âˆ’b1fnâˆ’1) /b0,
(3.3.15)
where an = bn â‰¡0 for n > m.
â€¢ Example 3.3.3
Let us redo Example 3.3.2 using the recursive method. Comparing Equation 3.3.7 to
Equation 3.3.6, a0 = 2, a1 = âˆ’1.5, a2 = 0, b0 = 1, b1 = âˆ’1.5, b2 = 0.5, and an = bn = 0 if
n â‰¥3. From Equation 3.3.15,
f0 = a0/b0 = 2/1 = 2,
(3.3.16)
f1 = (a1 âˆ’b1f0)/b0 = [âˆ’1.5 âˆ’(âˆ’1.5)(2)]/1 = 1.5,
(3.3.17)
f2 = (a2 âˆ’b2f0 âˆ’b1f1)/b0
(3.3.18)
= [0 âˆ’(0.5)(2) âˆ’(âˆ’1.5)(1.5)]/1 = 1.25,
(3.3.19)
and
f3 = (a3 âˆ’b3f0 âˆ’b2f1 âˆ’b1f2)/b0
(3.3.20)
= [0 âˆ’(0)(2) âˆ’(0.5)(1.5) âˆ’(âˆ’1.5)(1.25)]/1 = 1.125.
(3.3.21)
âŠ“âŠ”
Partial fraction expansion
One of the popular methods for inverting Laplace transforms is partial fractions. A
similar, but slightly diï¬€erent, scheme works here.

176
Advanced Engineering Mathematics: A Second Course
â€¢ Example 3.3.4
Given F(z) = z/
 z2 âˆ’1

, let us ï¬nd fn. The ï¬rst step is to obtain the partial fraction
expansion of F(z)/z.
Why we want F(z)/z rather than F(z) will be made clear in a
moment. Thus,
F(z)
z
=
1
(z âˆ’1)(z + 1) =
A
z âˆ’1 +
B
z + 1,
(3.3.22)
where
A = (z âˆ’1) F(z)
z

z=1
= 1
2,
and
B = (z + 1) F(z)
z

z=âˆ’1
= âˆ’1
2.
(3.3.23)
Multiplying Equation 3.3.22 by z,
F(z) = 1
2

z
z âˆ’1 âˆ’
z
z + 1

.
(3.3.24)
Next, we ï¬nd the inverse z-transform of z/(z âˆ’1) and z/(z + 1) in Table 3.1.1. This
yields
Zâˆ’1

z
z âˆ’1

= 1,
and
Zâˆ’1

z
z + 1

= (âˆ’1)n.
(3.3.25)
Thus, the inverse is
fn = 1
2 [1 âˆ’(âˆ’1)n] , n â‰¥0.
(3.3.26)
âŠ“âŠ”
From this example it is clear that there are two steps: (1) obtain the partial fraction
expansion of F(z)/z, and (2) ï¬nd the inverse z-transform by referring to Table 3.1.1.
â€¢ Example 3.3.5
Given F(z) = 2z2/[(z + 2)(z + 1)2], let us ï¬nd fn. We begin by expanding F(z)/z as
F(z)
z
=
2z
(z + 2)(z + 1)2 =
A
z + 2 +
B
z + 1 +
C
(z + 1)2 ,
(3.3.27)
where
A = (z + 2) F(z)
z

z=âˆ’2
= âˆ’4,
B = d
dz

(z + 1)2 F(z)
z

z=âˆ’1
= 4,
(3.3.28)
and
C = (z + 1)2 F(z)
z

z=âˆ’1
= âˆ’2,
(3.3.29)
so that
F(z) =
4z
z + 1 âˆ’
4z
z + 2 âˆ’
2z
(z + 1)2 ,
(3.3.30)
or
fn = Zâˆ’1
 4z
z + 1

âˆ’Zâˆ’1
 4z
z + 2

âˆ’Zâˆ’1

2z
(z + 1)2

.
(3.3.31)

The Z-Transform
177
From Table 3.1.1,
Zâˆ’1

z
z + 1

= (âˆ’1)n,
Zâˆ’1

z
z + 2

= (âˆ’2)n,
(3.3.32)
and
Zâˆ’1

z
(z + 1)2

= âˆ’Zâˆ’1

âˆ’z
(z + 1)2

= âˆ’n(âˆ’1)n = n(âˆ’1)n+1.
(3.3.33)
Applying Equation 3.3.32 and Equation 3.3.33 to Equation 3.3.31,
fn = 4(âˆ’1)n âˆ’4(âˆ’2)n + 2n(âˆ’1)n, n â‰¥0.
(3.3.34)
âŠ“âŠ”
â€¢ Example 3.3.6
Given F(z) = (z2 + z)/(z âˆ’2)2, let us determine fn. Because
F(z)
z
=
z + 1
(z âˆ’2)2 =
1
z âˆ’2 +
3
(z âˆ’2)2 ,
(3.3.35)
fn = Zâˆ’1

z
z âˆ’2

+ Zâˆ’1

3z
(z âˆ’2)2

.
(3.3.36)
Referring to Table 3.1.1,
Zâˆ’1

z
z âˆ’2

= 2n,
and
Zâˆ’1

3z
(z âˆ’2)2

= 3
2n2n.
(3.3.37)
Substituting Equation 3.3.37 into Equation 3.3.36 yields
fn =
  3
2n + 1

2n, n â‰¥0.
(3.3.38)
âŠ“âŠ”
Residue method
The power series, recursive, and partial fraction expansion methods are rather limited.
We now prove that fn may be computed from the following inverse integral formula:
fn =
1
2Ï€i
I
C
znâˆ’1F(z) dz,
n â‰¥0,
(3.3.39)
where C is any simple curve, taken in the positive sense, that encloses all of the singularities
of F(z). It is readily shown that the power series and partial fraction methods are special
cases of the residue method.

178
Advanced Engineering Mathematics: A Second Course
Proof : Starting with the deï¬nition of the z-transform
F(z) =
âˆ
X
n=0
fnzâˆ’n,
|z| > R1,
(3.3.40)
we multiply Equation 3.3.40 by znâˆ’1 and integrating both sides around any contour C that
includes all of the singularities:
1
2Ï€i
I
C
znâˆ’1F(z) dz =
âˆ
X
m=0
fm
1
2Ï€i
I
C
znâˆ’m dz
z .
(3.3.41)
Let C be a circle of radius R, where R > R1. Then, changing variables to z = R eiÎ¸, and
dz = iz dÎ¸,
1
2Ï€i
I
C
znâˆ’m dz
z = Rnâˆ’m
2Ï€
Z 2Ï€
0
ei(nâˆ’m)Î¸dÎ¸ =
 1,
m = n,
0,
otherwise.
(3.3.42)
Substituting Equation 3.3.42 into Equation 3.3.41 yields the desired result that
1
2Ï€i
I
C
znâˆ’1F(z) dz = fn.
(3.3.43)
âŠ“âŠ”
We can easily evaluate the inversion integral, Equation 3.3.39, using Cauchyâ€™s residue
theorem.
â€¢ Example 3.3.7
Let us ï¬nd the inverse z-transform of
F(z) =
1
(z âˆ’1)(z âˆ’2).
(3.3.44)
From the inversion integral,
fn =
1
2Ï€i
I
C
znâˆ’1
(z âˆ’1)(z âˆ’2) dz.
(3.3.45)
Clearly the integral has simple poles at z = 1 and z = 2. However, when n = 0 we also have
a simple pole at z = 0. Thus the cases n = 0 and n > 0 must be considered separately.
Case 1: n = 0. The residue theorem yields
f0 = Res

1
z(z âˆ’1)(z âˆ’2); 0

+ Res

1
z(z âˆ’1)(z âˆ’2); 1

+ Res

1
z(z âˆ’1)(z âˆ’2); 2

.
(3.3.46)
Evaluating these residues,
Res

1
z(z âˆ’1)(z âˆ’2); 0

=
1
(z âˆ’1)(z âˆ’2)

z=0
= 1
2,
(3.3.47)

The Z-Transform
179
Res

1
z(z âˆ’1)(z âˆ’2); 1

=
1
z(z âˆ’2)

z=1
= âˆ’1,
(3.3.48)
and
Res

1
z(z âˆ’1)(z âˆ’2); 2

=
1
z(z âˆ’1)

z=2
= 1
2.
(3.3.49)
Substituting Equation 3.3.47 through Equation 3.3.49 into Equation 3.3.46 yields f0 = 0.
Case 2: n > 0. Here we only have contributions from z = 1 and z = 2.
fn = Res

znâˆ’1
(z âˆ’1)(z âˆ’2); 1

+ Res

znâˆ’1
(z âˆ’1)(z âˆ’2); 2

,
n > 0,
(3.3.50)
where
Res

znâˆ’1
(z âˆ’1)(z âˆ’2); 1

= znâˆ’1
z âˆ’2

z=1
= âˆ’1,
(3.3.51)
and
Res

znâˆ’1
(z âˆ’1)(z âˆ’2); 2

= znâˆ’1
z âˆ’1

z=2
= 2nâˆ’1, n > 0.
(3.3.52)
Thus,
fn = 2nâˆ’1 âˆ’1,
n > 0.
(3.3.53)
Combining our results,
fn =

0,
n = 0,
1
2 (2n âˆ’2) ,
n > 0.
(3.3.54)
âŠ“âŠ”
â€¢ Example 3.3.8
Let us use the inversion integral to ï¬nd the inverse of
F(z) = z2 + 2z
(z âˆ’1)2 .
(3.3.55)
The inversion theorem gives
fn =
1
2Ï€i
I
C
zn+1 + 2zn
(z âˆ’1)2
dz = Res
zn+1 + 2zn
(z âˆ’1)2
; 1

,
(3.3.56)
where the pole at z = 1 is second order. Consequently, the corresponding residue is
Res
zn+1 + 2zn
(z âˆ’1)2
; 1

= d
dz

zn+1 + 2zn

z=1
= 3n + 1.
(3.3.57)
Thus, the inverse z-transform of Equation 3.3.55 is
fn = 3n + 1,
n â‰¥0.
(3.3.58)
âŠ“âŠ”

180
Advanced Engineering Mathematics: A Second Course
â€¢ Example 3.3.9
Let F(z) be a z-transform whose poles lie within the unit circle |z| = 1. Then
F(z) =
âˆ
X
n=0
fnzâˆ’n,
|z| > 1,
(3.3.59)
and
F(z)F(zâˆ’1) =
âˆ
X
n=0
f 2
n +
âˆ
X
n=0
âˆ
X
m=0
nÌ¸=m
fmfnzmâˆ’n.
(3.3.60)
We now multiply both sides of Equation 3.3.60 by zâˆ’1 and integrate around the unit circle
C. Therefore,
I
|z|=1
F(z)F(zâˆ’1)zâˆ’1 dz =
âˆ
X
n=0
I
|z|=1
f 2
nzâˆ’1 dz +
âˆ
X
n=0
âˆ
X
m=0
nÌ¸=m
fmfn
I
|z|=1
zmâˆ’nâˆ’1 dz, (3.3.61)
after interchanging the order of integration and summation. Performing the integration,
âˆ
X
n=0
f 2
n =
1
2Ï€i
I
|z|=1
F(z)F(zâˆ’1)zâˆ’1 dz,
(3.3.62)
which is Parsevalâ€™s theorem for one-sided z-transforms. Recall that there are similar theo-
rems for Fourier series and transforms.
âŠ“âŠ”
â€¢ Example 3.3.10: Evaluation of partial summations4
We begin by noting that
SN =
N
X
n=1
fn =
1
2Ï€i
I
C
F(z)
N
X
n=1
znâˆ’1 dz.
(3.3.63)
Here we employed the inversion integral to replace fn and reversed the order of integration
and summation. This interchange is permissible since we only have a partial summation.
Because the summation in Equation 3.3.63 is a geometric series, we have the ï¬nal result
that
SN =
1
2Ï€i
I
C
F(z)(zN âˆ’1)
z âˆ’1
dz.
(3.3.64)
Therefore, we can use the residue theorem and z-transforms to evaluate partial summations.
4 See Bunch, K. J., W. N. Cain, and R. W. Grow, 1990: The z-transform method of evaluating partial
summations in closed form. J. Phys. A, 23, L1213â€“L1215.

The Z-Transform
181
Let us ï¬nd SN = PN
n=1 n3.
Because fn = n3, F(z) = z(z2 + 4z + 1)/(z âˆ’1)4.
Consequently
SN = Res
z(z2 + 4z + 1)(zN âˆ’1)
(z âˆ’1)5
; 1

= 1
4!
d4
dz4

z(z2 + 4z + 1)(zN âˆ’1)

z=1
(3.3.65)
= 1
4!
d4
dz4
 zN+3 + 4zN+2 + zN+1 âˆ’z3 âˆ’4z2 âˆ’z

z=1
= 1
4(N + 1)2N 2.
(3.3.66)
âŠ“âŠ”
â€¢ Example 3.3.11
An additional beneï¬t of understanding inversion by the residue method is the ability
to qualitatively anticipate the inverse by knowing the location of the poles of F(z). This
intuition is important because many engineering analyses discuss stability and performance
entirely in terms of the properties of the systemâ€™s z-transform. In Figure 3.3.1 we graphed
the location of the poles of F(z) and the corresponding fn. The student should go through
the mental exercise of connecting the two pictures.
Problems
Use the power series or recursive method to compute the ï¬rst few values of fn of the
following z-transforms. Then check your answers with MATLAB.
1. F(z) = 0.09z2 + 0.9z + 0.09
12.6z2 âˆ’24z + 11.4
2. F(z) =
z + 1
2z4 âˆ’2z3 + 2z âˆ’2
3. F(z) =
1.5z2 + 1.5z
15.25z2 âˆ’36.75z + 30.75
4. F(z) =
6z2 + 6z
19z3 âˆ’33z2 + 21z âˆ’7
Use partial fractions to ï¬nd the inverse of the following z-transforms. Then verify your
answers with MATLAB.
5. F(z) =
z(z + 1)
(z âˆ’1)(z2 âˆ’z + 1/4)
6. F(z) =
(1 âˆ’eâˆ’aT )z
(z âˆ’1)(z âˆ’eâˆ’aT )
7. F(z) =
z2
(z âˆ’1)(z âˆ’Î±)
8. F(z) = (2z âˆ’a âˆ’b)z
(z âˆ’a)(z âˆ’b)
9. Using the property that the z-transform of gn = fnâˆ’kHnâˆ’k if n â‰¥0 is G(z) = zâˆ’kF(z),
ï¬nd the inverse of
F(z) =
z + 1
z10(z âˆ’1/2).
Then check your answer with MATLAB.
Use the residue method to ï¬nd the inverse z-transform of the following z-transforms. Then
verify your answer with MATLAB.
10. F(z) =
z2 + 3z
(z âˆ’1/2)3
11. F(z) =
z
(z + 1)2(z âˆ’2)
12. F(z) =
z
(z + 1)2(z âˆ’1)2
13. F(z) = ea/z

182
Advanced Engineering Mathematics: A Second Course
n
|z|=1
n
n
|z|=1
n
f
f
n
|z|=1
n
n
|z|=1
n
f
f
n
|z|=1
n
n
|z|=1
n
f
f
Figure 3.3.1: The correspondence between the location of the simple poles of the z-transform F(z) and
the behavior of fn.

The Z-Transform
183
3.4 SOLUTION OF DIFFERENCE EQUATIONS
Having reached the point where we can take a z-transform and then ï¬nd its inverse,
we are ready to use it to solve diï¬€erence equations. The procedure parallels that of solving
ordinary diï¬€erential equations by Laplace transforms. Essentially we reduce the diï¬€erence
equation to an algebraic problem. We then ï¬nd the solution by inverting Y (z).
â€¢ Example 3.4.1
Let us solve the second-order diï¬€erence equation
2yn+2 âˆ’3yn+1 + yn = 5 3n, n â‰¥0,
(3.4.1)
where y0 = 0 and y1 = 1.
Taking the z-transform of both sides of Equation 3.4.1, we obtain
2Z(yn+2) âˆ’3Z(yn+1) + Z(yn) = 5 Z(3n).
(3.4.2)
From the shifting theorem and Table 3.1.1,
2z2Y (z) âˆ’2z2y0 âˆ’2zy1 âˆ’3[zY (z) âˆ’zy0] + Y (z) =
5z
z âˆ’3.
(3.4.3)
Substituting y0 = 0 and y1 = 1 into Equation 3.4.3 and simplifying yields
(2z âˆ’1)(z âˆ’1)Y (z) = z(2z âˆ’1)
z âˆ’3
,
or
Y (z) =
z
(z âˆ’3)(z âˆ’1).
(3.4.4)
To obtain yn from Y (z) we can employ partial fractions or the residue method. Applying
partial fractions gives
Y (z)
z
=
A
z âˆ’1 +
B
z âˆ’3,
(3.4.5)
where
A = (z âˆ’1) Y (z)
z

z=1
= âˆ’1
2,
and
B = (z âˆ’3) Y (z)
z

z=3
= 1
2.
(3.4.6)
Thus,
Y (z) = âˆ’1
2
z
z âˆ’1 + 1
2
z
z âˆ’3,
or
yn = âˆ’1
2Zâˆ’1

z
z âˆ’1

+ 1
2Zâˆ’1

z
z âˆ’3

. (3.4.7)
From Equation 3.4.7 and Table 3.1.1,
yn = 1
2 (3n âˆ’1) ,
n â‰¥0.
(3.4.8)
An alternative to this hand calculation is to use MATLABâ€™s ztrans and iztrans to
solve diï¬€erence equations. In the present case, the MATLAB script would read
clear
% define symbolic variables
syms z Y; syms n positive

184
Advanced Engineering Mathematics: A Second Course
% take z-transform of left side of difference equation
LHS = ztrans(2*sym(â€™y(n+2)â€™)-3*sym(â€™y(n+1)â€™)+sym(â€™y(n)â€™),n,z);
% take z-transform of right side of difference equation
RHS = 5 * ztrans(3^n,n,z);
% set Y for z-transform of y and introduce initial conditions
newLHS = subs(LHS,â€™ztrans(y(n),n,z)â€™,â€™y(0)â€™,â€™y(1)â€™,Y,0,1);
% solve for Y
Y = solve(newLHS-RHS,Y);
% invert z-transform and find y(n)
y = iztrans(Y,z,n)
This script produced
y =
-1/2+1/2*3^n
Two checks conï¬rm that we have the correct solution. First, our solution must satisfy
the initial values of the sequence. Computing y0 and y1,
y0 = 1
2(30 âˆ’1) = 1
2(1 âˆ’1) = 0,
and
y1 = 1
2(31 âˆ’1) = 1
2(3 âˆ’1) = 1.
(3.4.9)
Thus, our solution gives the correct initial values.
Our sequence yn must also satisfy the diï¬€erence equation. Now
yn+2 = 1
2(3n+2âˆ’1) = 1
2(9 3nâˆ’1),
and
yn+1 = 1
2(3n+1âˆ’1) = 1
2(3 3nâˆ’1). (3.4.10)
Therefore,
2yn+2 âˆ’3yn+1 + yn =
 9 âˆ’9
2 + 1
2

3n âˆ’1 + 3
2 âˆ’1
2 = 5 3n
(3.4.11)
and our solution is correct.
Finally, we note that the term 3n/2 is necessary to give the right side of Equation 3.4.1;
it is the particular solution. The âˆ’1/2 term is necessary so that the sequence satisï¬es the
initial values; it is the complementary solution.
âŠ“âŠ”
â€¢ Example 3.4.2
Let us ï¬nd the yn in the diï¬€erence equation
yn+2 âˆ’2yn+1 + yn = 1,
n â‰¥0
(3.4.12)
with the initial conditions y0 = 0 and y1 = 3/2.
From Equation 3.4.12,
Z(yn+2) âˆ’2Z(yn+1) + Z(yn) = Z(1).
(3.4.13)
The z-transform of the left side of Equation 3.4.13 is obtained from the shifting theorem,
and Table 3.1.1 yields Z(1). Thus,
z2Y (z) âˆ’z2y0 âˆ’zy1 âˆ’2zY (z) + 2zy0 + Y (z) =
z
z âˆ’1.
(3.4.14)
Substituting y0 = 0 and y1 = 3/2 in Equation 3.4.14 and simplifying gives
Y (z) = 3z2 âˆ’z
2(z âˆ’1)3
or
yn = Zâˆ’1
 3z2 âˆ’z
2(z âˆ’1)3

.
(3.4.15)

The Z-Transform
185
We ï¬nd the inverse z-transform of Equation 3.4.15 by the residue method, or
yn =
1
2Ï€i
I
C
3zn+1 âˆ’zn
2(z âˆ’1)3 dz = 1
2!
d2
dz2
3zn+1
2
âˆ’zn
2

z=1
= 1
2n2 + n.
(3.4.16)
Thus,
yn = 1
2n2 + n,
n â‰¥0.
(3.4.17)
Note that n2/2 gives the particular solution to Equation 3.4.12, while n is there so that
yn satisï¬es the initial conditions.
This problem is particularly interesting because our
constant forcing produces a response that grows as n2, just as in the case of resonance in
a time-continuous system when a ï¬nite forcing such as sin(Ï‰0t) results in a response whose
amplitude grows as tm.
âŠ“âŠ”
â€¢ Example 3.4.3
Let us solve the diï¬€erence equation
b2yn + yn+2 = 0,
(3.4.18)
where the initial conditions are y0 = b2 and y1 = 0.
We begin by taking the z-transform of each term in Equation 3.4.18. This yields
b2Z(yn) + Z(yn+2) = 0.
(3.4.19)
From the shifting theorem, it follows that
b2Y (z) + z2Y (z) âˆ’z2y0 âˆ’zy1 = 0.
(3.4.20)
Substituting y0 = b2 and y1 = 0 into Equation 3.4.20,
b2Y (z) + z2Y (z) âˆ’b2z2 = 0,
or
Y (z) =
b2z2
z2 + b2 .
(3.4.21)
To ï¬nd yn we employ the residue method or
yn =
1
2Ï€i
I
C
b2zn+1
(z âˆ’ib)(z + ib) dz.
(3.4.22)
Thus,
yn = b2zn+1
z + ib

z=ib
+ b2zn+1
z âˆ’ib

z=âˆ’ib
= bn+2in
2
+ bn+2(âˆ’i)n
2
(3.4.23)
= bn+2einÏ€/2
2
+ bn+2eâˆ’inÏ€/2
2
= bn+2 cos
nÏ€
2

,
(3.4.24)
because cos(x) = 1
2
 eix + eâˆ’ix
. Consequently, we obtain the desired result that
yn = bn+2 cos
nÏ€
2

for n â‰¥0.
(3.4.25)
âŠ“âŠ”

186
Advanced Engineering Mathematics: A Second Course
0
5
10
15
20
25
30
0
1
2
3
4
5
6
7
8
9
10
number of conversion periods
amount left in account (K$)
Figure 3.4.1: The amount in a savings account as a function of an annual conversion period when interest
is compounded at the annual rate of 12% and $1000 is taken from the account every period starting with
period 10.
â€¢ Example 3.4.4: Compound interest
Diï¬€erence equations arise in ï¬nance because the increase or decrease in an account
occurs in discrete steps. For example, the amount of money in a compound interest savings
account after n + 1 conversion periods (the time period between interest payments) is
yn+1 = yn + ryn,
(3.4.26)
where r is the interest rate per conversion period. The second term on the right side of
Equation 3.4.32 is the amount of interest paid at the end of each period.
Let us ask a somewhat more diï¬ƒcult question of how much money we will have if we
withdraw the amount A at the end of every period starting after the period â„“. Now the
diï¬€erence equation reads
yn+1 = yn + ryn âˆ’AHnâˆ’â„“âˆ’1.
(3.4.27)
Taking the z-transform of Equation 3.4.27,
zY (z) âˆ’zy0 = (1 + r)Y (z) âˆ’Az2âˆ’â„“
z âˆ’1
(3.4.28)
after using Equation 3.2.9 or
Y (z) =
y0z
z âˆ’(1 + r) âˆ’
Az2âˆ’â„“
(z âˆ’1)[z âˆ’(1 + r)].
(3.4.29)
Taking the inverse of Equation 3.4.29,
yn = y0(1 + r)n âˆ’A
r

(1 + r)nâˆ’â„“+1 âˆ’1

Hnâˆ’â„“.
(3.4.30)
The ï¬rst term in Equation 3.4.30 represents the growth of money by compound interest
while the second term gives the depletion of the account by withdrawals.
Figure 3.4.1 gives the values of yn for various starting amounts assuming an annual
conversion period with r = 0.12, â„“= 10 years, and A = $1000. These computations were
done in two ways using MATLAB as follows:

The Z-Transform
187
% load in parameters
clear; r = 0.12; A = 1; k = 0:30;
y = zeros(length(k),3); yanswer = zeros(length(k),3);
% set initial condition
for m=1:3
y(1,m) = m;
% compute other y values
for n = 1:30
y(n+1,m) = y(n,m)+r*y(n,m);
y(n+1,m) = y(n+1,m)-A*stepfun(n,11);
end
% now use Equation 3.4.30
for n = 1:31
yanswer(n,m) = y(1,m)*(1+r)^(n-1);
yanswer(n,m) = yanswer(n,m)-A*((1+r)^(n-10)-1)
*stepfun(n,11)/r;
end; end;
plot(k,y,â€™oâ€™); hold; plot(k,yanswer,â€™sâ€™);
axis([0 30 0 10])
xlabel(â€™number of conversion periodsâ€™,â€™Fontsizeâ€™,20)
ylabel(â€™amount left in account (K$)â€™,â€™Fontsizeâ€™,20)
Figure 3.4.1 shows that if an investor places an initial amount of $3000 in an account
bearing 12% annually, after 10 years he can withdraw $1000 annually, essentially forever.
This is because the amount that he removes every year is replaced by the interest on the
funds that remain in the account.
âŠ“âŠ”
â€¢ Example 3.4.5
Let us solve the following system of diï¬€erence equations:
xn+1 = 4xn + 2yn,
and
yn+1 = 3xn + 3yn,
(3.4.31)
with the initial values of x0 = 0 and y0 = 5.
Taking the z-transform of Equation 3.4.31,
zX(z) âˆ’x0z = 4X(z) + 2Y (z),
zY (z) âˆ’y0z = 3X(z) + 3Y (z),
(3.4.32)
or
(z âˆ’4)X(z) âˆ’2Y (z) = 0,
3X(z) âˆ’(z âˆ’3)Y (z) = âˆ’5z.
(3.4.33)
Solving for X(z) and Y (z),
X(z) =
10z
(z âˆ’6)(z âˆ’1) =
2z
z âˆ’6 âˆ’
2z
z âˆ’1,
(3.4.34)
and
Y (z) =
5z(z âˆ’4)
(z âˆ’6)(z âˆ’1) =
2z
z âˆ’6 +
3z
z âˆ’1.
(3.4.35)
Taking the inverse of Equation 3.4.34 and Equation 3.4.35 term by term,
xn = âˆ’2 + 2 6n,
and
yn = 3 + 2 6n.
(3.4.36)

188
Advanced Engineering Mathematics: A Second Course
We can also check our work using the MATLAB script
clear
% define symbolic variables
syms X Y z; syms n positive
% take z-transform of left side of differential equations
LHS1 = ztrans(sym(â€™x(n+1)â€™)-4*sym(â€™x(n)â€™)-2*sym(â€™y(n)â€™),n,z);
LHS2 = ztrans(sym(â€™y(n+1)â€™)-3*sym(â€™x(n)â€™)-3*sym(â€™y(n)â€™),n,z);
% set X and Y for the z-transform of x and y
%
and introduce initial conditions
newLHS1 = subs(LHS1,â€™ztrans(x(n),n,z)â€™,â€™ztrans(y(n),n,z)â€™,...
â€™x(0)â€™,â€™y(0)â€™,X,Y,0,5);
newLHS2 = subs(LHS2,â€™ztrans(x(n),n,z)â€™,â€™ztrans(y(n),n,z)â€™,...
â€™x(0)â€™,â€™y(0)â€™,X,Y,0,5);
% solve for X and Y
[X,Y] = solve(newLHS1,newLHS2,X,Y);
% invert z-transform and find x(n) and y(n)
x = iztrans(X,z,n)
y = iztrans(Y,z,n)
This script yields
x =
2*6^n-2
y =
2*6^n+3
Problems
Solve the following diï¬€erence equations using z-transforms, where n â‰¥0. Check your answer
using MATLAB.
1. yn+1 âˆ’yn = n2,
y0 = 1.
2. yn+2 âˆ’2yn+1 + yn = 0,
y0 = y1 = 1.
3. yn+2 âˆ’2yn+1 + yn = 1,
y0 = y1 = 0.
4. yn+1 + 3yn = n,
y0 = 0.
5. yn+1 âˆ’5yn = cos(nÏ€),
y0 = 0.
6. yn+2 âˆ’4yn = 1,
y0 = 1, y1 = 0.
7. yn+2 âˆ’1
4yn = ( 1
2)n, y0 = y1 = 0.
8. yn+2 âˆ’5yn+1 + 6yn = 0, y0 = y1 = 1.
9. yn+2 âˆ’3yn+1 + 2yn = 1,
y0 = y1 = 0.
10. yn+2 âˆ’2yn+1 + yn = 2,
y0 = 0, y1 = 2.
11. xn+1 = 3xn âˆ’4yn, yn+1 = 2xn âˆ’3yn,
x0 = 3, y0 = 2.
12. xn+1 = 2xn âˆ’10yn, yn+1 = âˆ’xn âˆ’yn,
x0 = 3, y0 = âˆ’2.
13. xn+1 = xn âˆ’2yn, yn+1 = âˆ’6yn,
x0 = âˆ’1, y0 = âˆ’7.
14. xn+1 = 4xn âˆ’5yn, yn+1 = xn âˆ’2yn,
x0 = 6, y0 = 2.

The Z-Transform
189
3.5 STABILITY OF DISCRETE-TIME SYSTEMS
When we discussed the solution of ordinary diï¬€erential equations by Laplace trans-
forms, we introduced the concept of transfer function and impulse response. In the case of
discrete-time systems, similar considerations come into play.
Consider the recursive system
yn = a1ynâˆ’1Hnâˆ’1 + a2ynâˆ’2Hnâˆ’2 + xn,
n â‰¥0,
(3.5.1)
where Hnâˆ’k is the unit step function. It equals 0 for n < k and 1 for n â‰¥k. Equation
3.5.1 is called a recursive system because future values of the sequence depend upon all of
the previous values. At present, a1 and a2 are free parameters that we shall vary.
Using Equation 3.2.7,
z2Y (z) âˆ’a1zY (z) âˆ’a2Y (z) = z2X(z),
(3.5.2)
or
G(z) = Y (z)
X(z) =
z2
z2 âˆ’a1z âˆ’a2
.
(3.5.3)
As in the case of Laplace transforms, the ratio Y (z)/X(z) is the transfer function. The
inverse of the transfer function gives the impulse response for our discrete-time system.
This particular transfer function has two poles, namely
z1,2 = a1
2 Â±
r
a2
1
4 + a2.
(3.5.4)
At this point, we consider three cases.
Case 1: a2
1/4 + a2 < 0. In this case z1 and z2 are complex conjugates. Let us write them
as z1,2 = reÂ±iÏ‰0T . Then
G(z) =
z2
(z âˆ’reiÏ‰0T )(z âˆ’reâˆ’iÏ‰0T ) =
z2
z2 âˆ’2r cos(Ï‰0T)z + r2 ,
(3.5.5)
where r2 = âˆ’a2, and Ï‰0T = cosâˆ’1(a1/2r). From the inversion integral,
gn = Res

zn+1
z2 âˆ’2r cos(Ï‰0T)z + r2 ; z1

+ Res

zn+1
z2 âˆ’2r cos(Ï‰0T)z + r2 ; z2

,
(3.5.6)
where gn denotes the impulse response. Now
Res

zn+1
z2 âˆ’2r cos(Ï‰0T)z + r2 ; z1

= lim
zâ†’z1
(z âˆ’z1)zn+1
(z âˆ’z1)(z âˆ’z2)
(3.5.7)
= rn exp[i(n + 1)Ï‰0T]
eiÏ‰0T âˆ’eâˆ’iÏ‰0T
= rn exp[i(n + 1)Ï‰0T]
2i sin(Ï‰0T)
.
(3.5.8)
Similarly,
Res

zn+1
z2 âˆ’2r cos(Ï‰0T)z + r2 ; z2

= âˆ’rn exp[âˆ’i(n + 1)Ï‰0T]
2i sin(Ï‰0T)
,
(3.5.9)

190
Advanced Engineering Mathematics: A Second Course
and
gn = rn sin[(n + 1)Ï‰0T]
sin(Ï‰0T)
.
(3.5.10)
A graph of sin[(n + 1)Ï‰0T]/ sin(Ï‰0T) with respect to n gives a sinusoidal envelope.
More importantly, if |r| < 1 these oscillations vanish as n â†’âˆand the system is stable.
On the other hand, if |r| > 1 the oscillations grow without bound as n â†’âˆand the system
is unstable.
Recall that |r| > 1 corresponds to poles that lie outside the unit circle while |r| < 1 is
exactly the opposite. Our example suggests that for discrete-time systems to be stable, all
of the poles of the transfer function must lie within the unit circle while an unstable system
has at least one pole that lies outside of this circle.
Case 2: a2
1/4 + a2 > 0. This case leads to two real roots, z1 and z2. From the inversion
integral, the sum of the residues gives the impulse response
gn = zn+1
1
âˆ’zn+1
2
z1 âˆ’z2
.
(3.5.11)
Once again, if the poles lie within the unit circle, |z1| < 1 and |z2| < 1, the system is stable.
Case 3: a2
1/4 + a2 = 0. This case yields z1 = z2,
G(z) =
z2
(z âˆ’a1/2)2
and
gn =
1
2Ï€i
I
C
zn+1
(z âˆ’a1/2)2 dz =
a1
2
n
(n + 1).
(3.5.12)
This system is obviously stable if |a1/2| < 1 and the pole of the transfer function lies within
the unit circle.
In summary, ï¬nding the transfer function of a discrete-time system is important in
determining its stability. Because the location of the poles of G(z) determines the response
of the system, a stable system has all of its poles within the unit circle. Conversely, if
any of the poles of G(z) lie outside of the unit circle, the system is unstable. Finally, if
limnâ†’âˆgn = c, the system is marginally stable. For example, if G(z) has simple poles,
some of the poles must lie on the unit circle.
â€¢ Example 3.5.1
Numerical methods of integration provide some of the simplest, yet most important,
diï¬€erence equations in the literature. In this example,5 we show how z-transforms can be
used to highlight the strengths and weaknesses of such schemes.
Consider the trapezoidal integration rule in numerical analysis.
The integral yn is
updated by adding the latest trapezoidal approximation of the continuous curve. Thus, the
integral is computed by
yn = 1
2T(xn + xnâˆ’1Hnâˆ’1) + ynâˆ’1Hnâˆ’1,
(3.5.13)
where T is the interval between evaluations of the integrand.
5 See Salzer, J. M., 1954: Frequency analysis of digital computers operating in real time. Proc. IRE,
42, 457â€“466.

The Z-Transform
191
We ï¬rst determine the stability of this rule because it is of little value if it is not stable.
Using Equation 3.2.7, the transfer function is
G(z) = Y (z)
X(z) = T
2
z + 1
z âˆ’1

.
(3.5.14)
To ï¬nd the impulse response, we use the inversion integral and ï¬nd that
gn = T
4Ï€i
I
C
znâˆ’1 z + 1
z âˆ’1 dz.
(3.5.15)
At this point, we must consider two cases: n = 0 and n > 0. For n = 0,
g0 = T
2 Res
 z + 1
z(z âˆ’1); 0

+ T
2 Res
 z + 1
z(z âˆ’1); 1

= T
2 .
(3.5.16)
For n > 0,
g0 = T
2 Res
znâˆ’1(z + 1)
z âˆ’1
; 1

= T.
(3.5.17)
Therefore, the impulse response for this numerical scheme is g0 = T
2 and gn = T for n > 0.
Note that this is a marginally stable system (the solution neither grows nor decays with n)
because the pole associated with the transfer function lies on the unit circle.
Having discovered that the system is not unstable, let us continue and explore some
of its properties. Recall now that z = esT = eiÏ‰T if s = iÏ‰. Then the transfer function
becomes
G(Ï‰) = T
2
1 + eâˆ’iÏ‰T
1 âˆ’eâˆ’iÏ‰T = âˆ’iT
2 cot
Ï‰T
2

.
(3.5.18)
On the other hand, the transfer function of an ideal integrator is 1/s or âˆ’i/Ï‰. Thus, the
trapezoidal rule has ideal phase but its shortcoming lies in its amplitude characteristic; it
lies below the ideal integrator for 0 < Ï‰T < Ï€. We show this behavior, along with that for
Simpsonâ€™s one-third rule and Simpsonâ€™s three-eighths rule, in Figure 3.5.1.
Figure 3.5.1 conï¬rms the superiority of Simpsonâ€™s one-third rule over his three-eighths
rule. The ï¬gure also shows that certain schemes are better at suppressing noise at higher
frequencies, an eï¬€ect not generally emphasized in numerical calculus but often important in
system design. For example, the trapezoidal rule is inferior to all others at low frequencies
but only to Simpsonâ€™s one-third rule at higher frequencies. Furthermore, the trapezoidal
rule might actually be preferred, not only because of its simplicity but also because it
attenuates at higher frequencies, thereby counteracting the eï¬€ect of noise.
âŠ“âŠ”
â€¢ Example 3.5.2
Given the transfer function
G(z) =
z2
(z âˆ’1)(z âˆ’1/2),
(3.5.19)
is this discrete-time system stable or marginally stable?
This transfer function has two simple poles. The pole at z = 1/2 gives rise to a term
that varies as ( 1
2)n in the impulse response, while the z = 1 pole gives a constant. Because
this constant neither grows nor decays with n, the system is marginally stable.
âŠ“âŠ”

192
Advanced Engineering Mathematics: A Second Course
0.0
1.0
2.0
3.0
Ï‰Î¤
0.1
1.0
10.0
Ratio of quadrature amplitudes to ideal integration      
Trapezoidal
Simpsonâ€™s
3/8âˆ’rule
Rule
Simpsonâ€™s
1/3âˆ’rule
Ideal Rule
Figure 3.5.1: Comparison of various quadrature formulas by ratios of their amplitudes to that of an ideal
integrator. (From Salzer, J. M., 1954: Frequency analysis of digital computers operating in real time. Proc.
IRE, 42, p. 463.)
â€¢ Example 3.5.3
In most cases the transfer function consists of a ratio of two polynomials.
In this
case we can use the MATLAB function filter to compute the impulse response as follows:
Consider the Kronecker delta sequence, x0 = 1, and xn = 0 for n > 0. From the deï¬nition
of the z-transform, X(z) = 1. Therefore, if our input into filter is the Kronecker delta
sequence, the output yn will be the impulse response since Y (z) = G(z). If the impulse
response grows without bound as n increases, the system is unstable. If it goes to zero as
n increases, the system is stable. If it remains constant, it is marginally stable.
To illustrate this concept, the following MATLAB script ï¬nds the impulse response
corresponding to the transfer function, Equation 3.5.19:
% enter the coefficients of the numerator
%
of the transfer function, Equation 3.5.19
num = [1 0 0];
% enter the coefficients of the denominator
%
of the transfer function, Equation 3.5.19
den = [1 -1.5 0.5];
% create the Kronecker delta sequence
x = [1 zeros(1,20)];
% find the impulse response
y = filter(num,den,x);
% plot impulse response
plot(y,â€™oâ€™), axis([0 20 0.5 2.5])
xlabel(â€™n+1â€™,â€™Fontsizeâ€™,20)
ylabel(â€™impulse responseâ€™,â€™Fontsizeâ€™,20)
Figure 3.5.2 shows the computed impulse response. The asymptotic limit is two, so the
system is marginally stable, as we found before.
We note in closing that the same procedure can be used to ï¬nd the inverse of any
z-transform that consists of a ratio of two polynomials. Here we simply set G(z) equal to
the given z-transform and perform the same analysis.

The Z-Transform
193
0
5
10
15
20
0.5
1
1.5
2
2.5
n+1
impulse response
Figure 3.5.2: The impulse response for a discrete system with a transform function given by Equation
3.5.19.
Problems
For the following time-discrete systems, ï¬nd the transfer function and determine whether
the systems are unstable, marginally stable, or stable. Check your answer by graphing the
impulse response using MATLAB.
1. yn = ynâˆ’1Hnâˆ’1 + xn
2. yn = 2ynâˆ’1Hnâˆ’1 âˆ’ynâˆ’2Hnâˆ’2 + xn
3. yn = 3ynâˆ’1Hnâˆ’1 + xn
4. yn = 1
4ynâˆ’2Hnâˆ’2 + xn
Further Readings
Jury, E. I., 1964: Theory and Application of the z-Transform Method. John Wiley & Sons,
330 pp. The classic text on z-transforms.
LePage, W. R., 1980: Complex Variables and the Laplace Transform for Engineers. Dover,
483 pp. Chapter 16 is on z-transforms.


âˆ’15
âˆ’10
âˆ’5
0
5
10
15
âˆ’1
âˆ’0.8
âˆ’0.6
âˆ’0.4
âˆ’0.2
0
0.2
0.4
0.6
0.8
1
t
exact Hilbert transform
computed Hilbert transform
Chapter 4
The Hilbert Transform
In addition to the Fourier, Laplace, and z-transforms, there are many other linear
transforms that have their own special niche in engineering.
Examples include Hankel,
Walsh, Radon, and Hartley transforms. In this chapter we consider the Hilbert transform,
which is a commonly used technique for relating the real and imaginary parts of a spectral
response, particularly in communication theory.
We begin our study of Hilbert transforms by ï¬rst deï¬ning them and then exploring
their properties. Next, we develop the concept of the analytic signal. Finally, we explore
a property of Hilbert transforms that is frequently applied to data analysis: the Kramers-
Kronig relationship.
4.1 DEFINITION
In Chapter 3 we motivated the development of z-transforms by exploring the concept
of the ideal sampler. In the case of Hilbert transforms, we introduce another fundamental
operation, namely quadrature phase shifting or the ideal Hilbert transformer. This proce-
dure does nothing more than shift the phase of all input frequency components by âˆ’Ï€/2.
Hilbert transformers are frequently used in communication systems and signal processing;
examples include the generation of single-sideband modulated signals and radar and speech
signal processing.
Because a âˆ’Ï€/2 phase shift is equivalent to multiplying the Fourier transform of a
signal by eâˆ’iÏ€/2 = âˆ’i, and because phase shifting must be an odd function of frequency,1
1 For a real function the phase of its Fourier transform must be an odd function of Ï‰.
195

196
Advanced Engineering Mathematics: A Second Course
the transfer function of the phase shifter is G(Ï‰) = âˆ’i sgn(Ï‰), where sgn(Â·) is deï¬ned by
sgn(t) =
( 1,
t > 0,
0,
t = 0,
âˆ’1,
t < 0.
In other words, if X(Ï‰) denotes the input spectrum to the phase shifter, the output spec-
trum must be âˆ’i sgn(Ï‰)X(Ï‰). If the process is repeated, the total phase shift is âˆ’Ï€, a
complete phase reversal of all frequency components. The output spectrum then equals
[âˆ’i sgn(Ï‰)]2X(Ï‰) = âˆ’X(Ï‰). This agrees with the notion of phase reversal because the
output function is âˆ’x(t).
Consider now the impulse response of the quadrature phase shifter, g(t) = Fâˆ’1[G(Ï‰)].
From the deï¬nition of Fourier transforms,
dG
dÏ‰ = âˆ’i
Z âˆ
âˆ’âˆ
t g(t)eâˆ’iÏ‰t dt,
(4.1.1)
and
g(t) = i
tFâˆ’1
dG
dÏ‰

.
(4.1.2)
Since Gâ€²(Ï‰) = âˆ’2iÎ´(Ï‰), the corresponding impulse response is
g(t) = i
tFâˆ’1[âˆ’2iÎ´(Ï‰)] = 1
Ï€t.
(4.1.3)
Consequently, if x(t) is the input to a quadrature phase shifter, the superposition integral
gives the output time function as
bx(t) = x(t) âˆ—1
Ï€t = 1
Ï€
Z âˆ
âˆ’âˆ
x(Ï„)
t âˆ’Ï„ dÏ„.
(4.1.4)
We shall deï¬ne bx(t) as the Hilbert transform of x(t), although some authors use the negative
of Equation 4.1.4 corresponding to a +Ï€/2 phase shift. The transform bx(t) is also called
the harmonic conjugate of x(t).
In similar fashion, bbx(t) is the Hilbert transform of the Hilbert transform of x(t) and
corresponds to the output of two cascaded phase shifters. However, this output is known
to be âˆ’x(t), so bbx(t) = âˆ’x(t), and we arrive at the inverse Hilbert transform relationship
that
x(t) = âˆ’bx(t) âˆ—1
Ï€t = âˆ’1
Ï€
Z âˆ
âˆ’âˆ
bx(Ï„)
t âˆ’Ï„ dÏ„.
(4.1.5)
Taken together, x(t) and bx(t) are called a Hilbert pair.
Hilbert pairs enjoy the unique
property that x(t) + ibx(t) is an analytic function.2
2 For the proof, see Titchmarsh, E. C., 1948: Introduction to the Theory of Fourier Integrals. Oxford
University Press, p. 125.

The Hilbert Transform
197
Descended from a Prussian middle-class family, David Hilbert (1862â€“1943) would make signiï¬cant
contributions in the ï¬elds of algebraic form, algebraic number theory, foundations of geometry,
analysis, mathematical physics, and the foundations of mathematics.
Hilbert transforms arose
during his study of integral equations (Hilbert, D., 1912: GrundzÂ¨uge einer allgemeinen Theorie der
linearen Integralgleichungen. Teubner, p. 75). (Portrait courtesy of Photo AKG, London, with
permission.)
Because of the singularity at Ï„ = t, the integrals in Equation 4.1.4 and Equation 4.1.5
must be taken in the Cauchy principal value sense by approaching the singularity point
from both sides, namely
Z âˆ
âˆ’âˆ
f(Ï„) dÏ„ = lim
Ç«â†’0
Z tâˆ’Ç«
âˆ’âˆ
f(Ï„) dÏ„ +
Z âˆ
t+Ç«
f(Ï„) dÏ„

,
(4.1.6)
so that the inï¬nities to the right and left of Ï„ = t cancel each other. See Section 1.10.
We also note that the Hilbert transform is basically a convolution and does not produce a
change of domain; if x is a function of time, then bx is also a function of time. This is quite
diï¬€erent from what we encountered with Laplace or Fourier transforms.
From its origin in phase shifting, Hilbert transforms of sinusoidal functions are trivial.
Some examples are
d
cos(Ï‰t + Ï•) = cos
 Ï‰t + Ï• âˆ’Ï€
2

= sgn(Ï‰) sin(Ï‰t + Ï•).
(4.1.7)
Similarly,
d
sin(Ï‰t + Ï•) = âˆ’sgn(Ï‰) cos(Ï‰t + Ï•),
(4.1.8)

198
Advanced Engineering Mathematics: A Second Course
and
d
eiÏ‰t+iÏ• = âˆ’i sgn(Ï‰)eiÏ‰t+iÏ•.
(4.1.9)
Thus, Hilbert transformation does not change the amplitude of sine or cosine but does
change their phase by Â±Ï€/2.
â€¢ Example 4.1.1
Let us apply the integral deï¬nition of the Hilbert transform, Equation 4.1.4, to ï¬nd
the Hilbert transform of sin(Ï‰t), Ï‰ Ì¸= 0.
From the deï¬nition,
H[sin(Ï‰t)] = 1
Ï€
Z âˆ
âˆ’âˆ
sin(Ï‰Ï„)
t âˆ’Ï„
dÏ„.
(4.1.10)
If x = t âˆ’Ï„, then
H[sin(Ï‰t)] = âˆ’cos(Ï‰t)
Ï€
Z âˆ
âˆ’âˆ
sin(Ï‰x)
x
dx = âˆ’cos(Ï‰t) sgn(Ï‰).
(4.1.11)
âŠ“âŠ”
â€¢ Example 4.1.2
Let us compute the Hilbert transform of x(t) = sin(t)/(t2 + 1) from the deï¬nition of
the Hilbert transform, Equation 4.1.4.
From the deï¬nition,
bx(t) = 1
Ï€ PV
Z âˆ
âˆ’âˆ
sin(Ï„)
(t âˆ’Ï„)(Ï„ 2 + 1) dÏ„ = 1
Ï€ â„‘

PV
Z âˆ
âˆ’âˆ
eiÏ„
(t âˆ’Ï„)(Ï„ 2 + 1) dÏ„

.
(4.1.12)
Because of the singularity on the real axis at Ï„ = t, we treat the integrals in Equation 4.1.12
in the sense of Cauchy principal value.
To evaluate Equation 4.1.12, we convert it into a closed contour integration by in-
troducing a semicircle CR of inï¬nite radius in the upper half-plane. This yields a closed
contour C, which consists of the real line plus this semicircle. Therefore, Equation 4.1.12
can be rewritten
PV
Z âˆ
âˆ’âˆ
eiÏ„
(t âˆ’Ï„)(Ï„ 2 + 1) dÏ„ = PV
I
C
eiz
(t âˆ’z)(z2 + 1) dzâˆ’
Z
CR
eiz
(t âˆ’z)(z2 + 1) dz. (4.1.13)
The second integral on the right side of Equation 4.1.13 vanishes by Equation 1.9.7.
The evaluation of the closed integral in Equation 4.1.13 follows from the residue theo-
rem. We have that
Res

eiz
(t âˆ’z)(z2 + 1); t

= lim
zâ†’t
(z âˆ’t) eiz
(t âˆ’z)(z2 + 1) = âˆ’
eit
t2 + 1,
(4.1.14)
and
Res

eiz
(t âˆ’z)(z2 + 1); i

= lim
zâ†’i
(z âˆ’i) eiz
(t âˆ’z)(z2 + 1) =
eâˆ’1
2i(t âˆ’i).
(4.1.15)
We do not have a contribution from z = âˆ’i because it lies outside of the closed contour.

The Hilbert Transform
199
The Hilbert Transform of Some Common Functions
function, x(t)
Hilbert transform, bx(t)
1.

1,
a < t < b
0,
otherwise
1
Ï€ ln

t âˆ’a
t âˆ’b

2.
sin(Ï‰t + Ï•)
âˆ’sgn(Ï‰) cos(Ï‰t + Ï•)
3.
cos(Ï‰t + Ï•)
sgn(Ï‰) sin(Ï‰t + Ï•)
4.
eiÏ‰t+Ï•i
âˆ’i sgn(Ï‰)eiÏ‰t+Ï•i
5.
1
t
âˆ’Ï€Î´(t)
6.
1
t2 + a2 ,
0 < â„œ(a)
t
a(t2 + a2)
7.
Î»t + Âµa
t2 + a2 ,
0 < â„œ(a)
Âµt âˆ’Î»a
t2 + a2
8.
1
1 + t4
t(1 + t2)
âˆš
2 (1 + t4)
9.
sin(at)
t
,
0 < a
1 âˆ’cos(at)
t
10.
sin(t)
1 + t2
eâˆ’1 âˆ’cos(t)
1 + t2
11.
sin(at)J1(at),
0 < a
âˆ’cos(at)J1(at)
12.
sin(at)Jn(bt),
0 < b < a
âˆ’cos(at)Jn(bt)
13.
cos(at)J1(at),
0 < a
sin(at)J1(at)
14.
cos(at)Jn(bt),
0 < b < a
sin(at)Jn(at)
15.
 âˆš
a2 âˆ’t2,
âˆ’a < t < a
0,
otherwise
ï£±
ï£²
ï£³
t +
âˆš
t2 âˆ’a2,
âˆ’âˆ< t < âˆ’a
t,
âˆ’a < t < a
t âˆ’
âˆš
t2 âˆ’a2,
a < t < âˆ
16.
sin
 a
âˆš
t

H(t),
0 < a
(
âˆ’eâˆ’aâˆš
|t|,
âˆ’âˆ< t < 0
âˆ’cos
 a
âˆš
t

,
0 < t < âˆ

200
Advanced Engineering Mathematics: A Second Course
Therefore,
PV
Z âˆ
âˆ’âˆ
eiÏ„
(t âˆ’Ï„)(Ï„ 2 + 1) dÏ„ = âˆ’Ï€i eit
t2 + 1 + Ï€ eâˆ’1(t + i)
t2 + 1
.
(4.1.16)
Only one half of the value of the residue at z = t was included; this reï¬‚ects the semicircular
indentation around the singularity there. Substituting Equation 4.1.16 into Equation 4.1.12,
we obtain the ï¬nal result that
H
 sin(t)
t2 + 1

= eâˆ’1 âˆ’cos(t)
t2 + 1
.
(4.1.17)
âŠ“âŠ”
â€¢ Example 4.1.3
Let us employ the relationship that the Fourier transform of bx(t) equals âˆ’i sgn(Ï‰) times
the Fourier transform of x(t) to ï¬nd the Hilbert transform of x(t) = eâˆ’t2.
Because F(eâˆ’t2) = âˆšÏ€eâˆ’Ï‰2/4,
b
X(Ï‰) = âˆ’iâˆšÏ€ sgn(Ï‰)eâˆ’Ï‰2/4.
(4.1.18)
Therefore,
bx(t) =
i
2âˆšÏ€
Z 0
âˆ’âˆ
eitÏ‰âˆ’Ï‰2/4 dÏ‰ âˆ’
i
2âˆšÏ€
Z âˆ
0
eitÏ‰âˆ’Ï‰2/4 dÏ‰
(4.1.19)
=
i
âˆšÏ€
Z 0
âˆ’âˆ
e2itÎ·âˆ’Î·2 dÎ· âˆ’
i
âˆšÏ€
Z âˆ
0
e2itÎ·âˆ’Î·2 dÎ·
(4.1.20)
= eâˆ’t2
âˆšÏ€
Z t
âˆ’iâˆ
eâˆ’s2 ds âˆ’eâˆ’t2
âˆšÏ€
Z iâˆ
t
eâˆ’s2 ds = 2eâˆ’t2
âˆšÏ€
Z t
0
eâˆ’s2 ds,
(4.1.21)
where s = t + Î·i. The integral in Equation 4.1.21 is the well-known Dawsonâ€™s integral.3 See
Gautschi and Waldvogel4 for an alternative derivation.
âŠ“âŠ”
â€¢ Example 4.1.4: Numerical computation of the Hilbert transform
Recently AndrÂ´e Weideman5 devised a particularly eï¬ƒcient method for numerically
computing the Hilbert transform when x(t) is known exactly for any real t and enjoys the
property that
Z âˆ
âˆ’âˆ
|x(t)|2 dt < âˆ.
(4.1.22)
3 Press, W. H., S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery, 1992: Numerical Recipes in
Fortran: The Art of Scientiï¬c Computing. Cambridge University Press, Section 6.10.
4 Gautschi, W., and J. Waldvogel, 2000: Computing the Hilbert transform of the generalized Laguerre
and Hermite weight functions. BIT, 41, 490â€“503.
5 Weideman, J. A. C., 1995: Computing the Hilbert transform on the real line. Math. Comput., 64,
745â€“762.

The Hilbert Transform
201
Given Equation 4.1.22, the function x(t) can be represented by the rational expansion
x(t) =
âˆ
X
n=âˆ’âˆ
anÏn(t),
(4.1.23)
where Ïn(t) is the set of rational functions
Ïn(t) =
(1 + it)n
(1 âˆ’it)n+1 ,
n = 0, Â±1, Â±2, Â· Â· Â· ,
(4.1.24)
and
an = 1
Ï€
Z âˆ
âˆ’âˆ
x(t)Ïâˆ—
n(t) dt
(4.1.25)
or
an = 1
2Ï€
Z Ï€
âˆ’Ï€

1 âˆ’i tan
  1
2Î¸

x

tan
  1
2Î¸

eâˆ’inÎ¸ dÎ¸,
(4.1.26)
if we introduce the substitution t = tan(Î¸/2).
Why is Equation 4.1.23 useful? Taking the Hilbert transform of both sides of this
equation,
bx(t) =
âˆ
X
n=âˆ’âˆ
anbÏn(t).
(4.1.27)
Using contour integration, we ï¬nd that
bÏn(t) = 1
Ï€ PV
Z âˆ
âˆ’âˆ
(1 + iÏ„)n
(1 âˆ’iÏ„)n+1(t âˆ’Ï„) dÏ„ = âˆ’i sgn(n)Ïn(t),
(4.1.28)
where sgn(t) is the signum function with sgn(0) = 1. Therefore,
bx(t) = âˆ’i
âˆ
X
n=âˆ’âˆ
sgn(n) an Ïn(t).
(4.1.29)
We must now approximate Equation 4.1.29 so that we can evaluate it numerically. We
do this by introducing the following truncated version:
bxN(t) = âˆ’i
Nâˆ’1
X
n=âˆ’N
sgn(n) An Ïn(t).
(4.1.30)
This particular truncation was chosen because Ïn(t) and Ïâˆ’nâˆ’1(t) are a conjugate pair. The
coeï¬ƒcient an has become An, which equals
An = 1
N
Nâˆ’1
X
j=âˆ’N+1

1 âˆ’i tan
  1
2Î¸j

x

tan
  1
2Î¸j

eâˆ’inÎ¸j,
(4.1.31)
where Î¸j = Ï€j/N. The terms corresponding to j = Â±N have been set to zero because it
is assumed that x(t) vanishes rapidly with t â†’Â±âˆ. Finally, we substitute Î¸ for t and
transform Equation 4.1.30 into
bxN(tj) = âˆ’
i
1 âˆ’i tan(Î¸j)
Nâˆ’1
X
n=âˆ’N
sgn(n)AneinÎ¸j.
(4.1.32)

202
Advanced Engineering Mathematics: A Second Course
âˆ’15
âˆ’10
âˆ’5
0
5
10
15
âˆ’1
âˆ’0.8
âˆ’0.6
âˆ’0.4
âˆ’0.2
0
0.2
0.4
0.6
0.8
1
t
exact Hilbert transform
computed Hilbert transform
Figure 4.1.1: The Hilbert transform for x(t) = 1/(1 + t4) computed from Weidemanâ€™s algorithm.
The advantage of Equation 4.1.31 and Equation 4.1.32 is that they can be evaluated
using fast Fourier transforms. For example, the following MATLAB script devised by Wei-
deman illustrates his methods for x(t) = 1/(1 + t4):
% initialize parameters used in computation
b = 1; N = 8; n = [-N:N-1]â€™;
% set up collocation points and evaluate function there
t = b * tan(pi*(n+1/2)/(2*N)); F = 1./(1+t.^4);
% evaluate Equation 4.1.31
an = fftshift(fft(F.*(b-i*t)));
% compute Hilbert transform via Equation 4.1.32
hilbert = ifft(fftshift(i*(sign(n+1/2).*an)))./(b-i*t);
hilbert = -real(hilbert);
% find points at which we will compute exact answer
tt = [-12:0.02:12];
% compute exact answer
answer = tt.*(1+tt.^2)./(1+tt.^4)./sqrt(2);
fzero = zeros(size(tt));
% plot both computed Hilbert transform and exact answer
plot(tt,answer,â€™-â€™,t,hilbert,â€™oâ€™,tt,fzero,â€™--â€™)
xlabel(â€™tâ€™,â€™Fontsizeâ€™,20)
legend(â€™exact Hilbert transformâ€™,â€™computed Hilbert transformâ€™)
legend boxoff
Figure 4.1.1 illustrates Weidemanâ€™s algorithm for numerically computing the Hilbert
transform of 1/(1 + t4).
There are two important points concerning Weidemanâ€™s implementation of his algo-
rithm. First, the collocation points originally given by tj = tan[Ï€j/(2N)], j = âˆ’N, . . . , Nâˆ’1
have changed to tj = tan[(j + 1
2)Ï€/(2N)], j = âˆ’N, . . . , N âˆ’1. This change replaces the
trapezoidal rule discretization for the Fourier coeï¬ƒcients with a midpoint rule. The advan-
tages are twofold: First, it avoids the nuisance of dealing with a collocation point at inï¬nity.
Second, it actually yields more accurate results in many cases.

The Hilbert Transform
203
The discerning student will also notice that Weideman introduced a free parameter b,
which we set to one. This rescaling parameter can have a major inï¬‚uence on the accuracy.
The interested student is referred to the bottom of page 756 in Weidemanâ€™s paper for further
details.
âŠ“âŠ”
â€¢ Example 4.1.5: Discrete Hilbert transform
Quite often the function is given as discrete data points. How do we ï¬nd the Hilbert
transform in this case? We will now prove6 that the equivalent discrete Hilbert transform
is
H(fn) = bfk =
ï£±
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£³
2
Ï€
X
n odd
fn
k âˆ’n,
k even,
2
Ï€
X
n even
fn
k âˆ’n,
k odd,
(4.1.33)
where fn denotes a set of discrete data values that are sampled at t = nT and both k and
n run from âˆ’âˆto âˆ. The corresponding inverse is
fn =
ï£±
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£³
2
Ï€
X
k odd
bfk
k âˆ’n,
n even,
2
Ï€
X
k even
bfk
k âˆ’n,
n odd.
(4.1.34)
We begin our proof by inserting Equation 4.1.33 into Equation 4.1.34. For n even,
fn = 2
Ï€
X
k odd
1
k âˆ’n
 
2
Ï€
X
p even
fp
k âˆ’p
!
= 4
Ï€2
X
p even
X
k odd
fp
(k âˆ’p)(k âˆ’n)
(4.1.35)
= 4
Ï€2
X
k odd
fn
(k âˆ’n)2 + 4
Ï€2
X
p even,pÌ¸=n
X
k odd
(n âˆ’p)fp

1
k âˆ’n âˆ’
1
k âˆ’p

.
(4.1.36)
The term within the curly brackets equals zero as k runs through all of its values. Therefore,
Equation 4.1.36 reduces to
fn = 8
Ï€2 fn

1 + 1
32 + 1
52 + + 1
72 + Â· Â· Â·

.
(4.1.37)
However, the term in the brackets of Equation 4.1.37 equals Ï€2/8. Therefore, Equation
4.1.33 and Equation 4.1.34 is proved for n even. An identical proof follows for n odd.
A popular alternative7 to Equation 4.1.33 involves the (fast) Fourier transform and
the relationship that b
X(Ï‰) = âˆ’i sgn(Ï‰)X(Ï‰), where X(Ï‰) and b
X(Ï‰) denote the Fourier
transform of x(t) and bx(t), respectively. In this technique, a fast Fourier transform is taken
of the data.
This transformed dataset is then multiplied by âˆ’i sgn(Ï‰) and then back
transformed to give the Hilbert transform.
6 See Kak, S. C., 1970: The discrete Hilbert transform. Proc. IEEE, 58, 585â€“586. For an alternative
derivation, see Kress, R., and E. Martensen, 1970: Anwendung der Rechteckregel auf die reelle Hilbert-
transformation mit unendlichem Intervall. Z. Angew. Math. Mech., 50, T61â€“T64.
7 Ë‡CÂ´Ä±Ë‡zek, V., 1970: Discrete Hilbert transform. IEEE Trans. Audio Electroacoust., AU-18, 340â€“343.

204
Advanced Engineering Mathematics: A Second Course
Let x(t) be a real, even function. Then X(Ï‰), the Fourier transform of x(t), is also an
even function. Consequently,
bx(t) = 1
2Ï€
Z âˆ
âˆ’âˆ
b
X(Ï‰)eiÏ‰t dÏ‰ = 1
2Ï€
Z âˆ
âˆ’âˆ
âˆ’i sgn(Ï‰)X(Ï‰) [cos(Ï‰t) + i sin(Ï‰t)] dÏ‰
(4.1.38)
= âˆ’i
2Ï€
Z âˆ
âˆ’âˆ
sgn(Ï‰)X(Ï‰) cos(Ï‰t) dÏ‰ + 1
2Ï€
Z âˆ
âˆ’âˆ
sgn(Ï‰)X(Ï‰) sin(Ï‰t) dÏ‰
(4.1.39)
= 1
Ï€
Z âˆ
0
X(Ï‰) sin(Ï‰t) dÏ‰.
(4.1.40)
Note that the Hilbert transform in this case is an odd function. Similarly, if x(t) is a real,
odd function,
bx(t) = âˆ’i
Ï€
Z âˆ
0
X(Ï‰) cos(Ï‰t) dÏ‰,
(4.1.41)
and the Hilbert transform is an even function.
Problems
1. Show that the Hilbert transform of a constant function is zero.
2. Use Equation 4.1.4 to compute the Hilbert transform of cos(Ï‰t), Ï‰ Ì¸= 0.
3. Use Equation 4.1.4 to show that the Hilbert transform of the Dirac delta function Î´(t)
is 1/(Ï€t).
4. Use Equation 4.1.4 to show that the Hilbert transform of 1/(t2 + 1) is t/(t2 + 1).
5. The output y(t) from an ideal lowpass ï¬lter can be expressed by the convolution integral
y(t) = x(t) âˆ—sin(2Ï€Ï‰t)
Ï€t
,
where x(t) is the input signal. Show that this expression can also be expressed in terms of
Hilbert transforms as
y(t) = H[x(t) cos(2Ï€Ï‰t)] sin(2Ï€Ï‰t) âˆ’H[x(t) sin(2Ï€Ï‰t)] cos(2Ï€Ï‰t).
Following Example 4.1.3, ï¬nd the Hilbert transforms of
6. x(t) =
1
1 + t2
7. x(t) =
 1,
âˆ’a < t < a
0,
otherwise
8. Using the commutative and associate properties of convolution, f(t) âˆ—g(t) = g(t) âˆ—f(t)
and [f(t) âˆ—g(t)] âˆ—v(t) = f(t) âˆ—[g(t) âˆ—v(t)], respectively, and the deï¬nition of the Hilbert
transform, Equation 4.1.4, show8 that
H[f(t) âˆ—g(t)] = bf(t) âˆ—g(t) = f(t) âˆ—bg(t).
8 For an application, see Sakai, H., and G. A. Vanasse, 1966: Hilbert transform in Fourier spectroscopy.
J. Opt. Soc. Am., 56, 131â€“132.

The Hilbert Transform
205
Using MATLAB, test Weidemanâ€™s algorithm for the following cases. Why does the algorithm
do well or not?
9.
 1,
âˆ’1 < t < 1
0,
otherwise
10. sin(t)
11.
1
t2 + 1
12. sin(t)
1 + t4
For Problem 12, you will need
H
 sin(t)
t4 + 1

= eâˆ’1/
âˆš
2[cos(1/
âˆš
2 ) + sin(1/
âˆš
2 )t2] âˆ’cos(t)
t4 + 1
.
4.2 SOME USEFUL PROPERTIES
In principle, we could construct any desired transform from the deï¬nition of the Hilbert
transform. However, there are several general theorems that are much more eï¬€ective in
ï¬nding new transforms.
Linearity
From the deï¬nition of the Hilbert transform, it immediately follows that if z(t) =
c1x(t) + c2y(t), where c1 and c2 are arbitrary constants, then bz(t) = c1bx(t) + c2by(t).
The energy in a signal and its Hilbert
transform are the same.
Consider the energy spectral densities at input and output of a quadrature phase shifter.
The output equals
| b
X(Ï‰)|2 =
F[bx(t)]
2 = | âˆ’i sgn(Ï‰)|2|X(Ï‰)|2 = |X(Ï‰)|2.
(4.2.1)
Because the energy spectral density at input and output are the same, so are the total
energies.
A signal and its Hilbert transform
are orthogonal.
From Parsevalâ€™s theorem,
Z âˆ
âˆ’âˆ
x(t)bx(t) dt =
Z âˆ
âˆ’âˆ
X(Ï‰) b
Xâˆ—(Ï‰) dÏ‰,
(4.2.2)
where b
X(Ï‰) = F[bx(t)]. Then,
Z âˆ
âˆ’âˆ
X(Ï‰) b
Xâˆ—(Ï‰) dÏ‰ =
Z âˆ
âˆ’âˆ
i sgn(Ï‰)|X(Ï‰)|2 dÏ‰ = 0,
(4.2.3)

206
Advanced Engineering Mathematics: A Second Course
because the integrand in the middle expression of Equation 4.2.3 is odd. Thus,
Z âˆ
âˆ’âˆ
x(t)bx(t) dt = 0.
(4.2.4)
The reason why a function and its Hilbert transform are orthogonal to each other follows
from the fact that a Hilbert transformation of a function shifts the phase of each Fourier
component of the function forward by Ï€/2 for positive frequencies and backward for negative
frequencies.
â€¢ Example 4.2.1
Let us verify the orthogonality condition for Hilbert transforms using x(t) = 1/(1+t2).
Because bx(t) = t/(1 + t2),
Z âˆ
âˆ’âˆ
x(t)bx(t) dt =
Z âˆ
âˆ’âˆ
t
(1 + t2)2 dt = 0,
(4.2.5)
since the integrand is an odd function.
âŠ“âŠ”
Shifting
Let us ï¬nd the Hilbert transform of x(t + a) if we know bx(t). From the deï¬nition of
Hilbert transforms,
H[x(t + a)] = 1
Ï€
Z âˆ
âˆ’âˆ
x(Î· + a)
t âˆ’Î·
dÎ· = 1
Ï€
Z âˆ
âˆ’âˆ
x(Ï„)
(t + a) âˆ’Ï„ dÏ„ = bx(t + a)
(4.2.6)
or H[x(t + a)] = bx(t + a).
Time scaling
Let a > 0. Then,
H[x(at)] = 1
Ï€
Z âˆ
âˆ’âˆ
x(aÎ·)
t âˆ’Î· dÎ· = 1
Ï€
Z âˆ
âˆ’âˆ
x(Ï„)
at âˆ’Ï„ dÏ„ = bx(at).
(4.2.7)
On the other hand, if a < 0,
H[x(at)] = 1
Ï€
Z âˆ
âˆ’âˆ
x(aÎ·)
t âˆ’Î· dÎ· = âˆ’1
Ï€
Z âˆ
âˆ’âˆ
x(Ï„)
at âˆ’Ï„ dÏ„ = âˆ’bx(at).
(4.2.8)
Thus, we have that H[x(at)] = sgn(a) bx(at).

The Hilbert Transform
207
Some General Properties of Hilbert Transforms
function, x(t)
Hilbert transform, bx(t)
1.
bx(t)
âˆ’x(t)
2.
x(t) + y(t)
bx(t) + by(t)
3.
x(t + a),
a real
bx(t + a)
4.
dnx(t)
dtn
dnbx(t)
dtn
5.
x(at)
sgn(a) bx(at)
6.
tx(t)
tbx(t) + 1
Ï€
R âˆ
âˆ’âˆx(Ï„) dÏ„
7.
(t + a)x(t)
(t + a)bx(t) + 1
Ï€
R âˆ
âˆ’âˆx(Ï„) dÏ„
Derivatives
Let us ï¬nd the relationship between the nth derivative of x(t) and its Hilbert transform.
Using the derivative rule as it applies to Fourier transforms,
H

F
dnx
dtn

= âˆ’i sgn(Ï‰)(iÏ‰)nX(Ï‰) = (iÏ‰)n[âˆ’i sgn(Ï‰)X(Ï‰)] = (iÏ‰)n b
X(Ï‰) = F
dnbx
dtn

.
(4.2.9)
Taking the inverse Fourier transforms, we have that
H
dnx
dtn

= dnbx
dtn .
(4.2.10)
Convolution
Hilbert transforms enjoy a similar, but not identical, property with Fourier transforms
with respect to convolution. If
w(t) = u(t) âˆ—v(t) =
Z âˆ
âˆ’âˆ
u(Ï„)v(t âˆ’Ï„) dÏ„ =
Z âˆ
âˆ’âˆ
u(t âˆ’Ï„)v(Ï„) dÏ„,
(4.2.11)
then
bw(t) = v(t) âˆ—bu(t).
(4.2.12)
Proof : From the convolution theorem for Fourier transforms, W(Ï‰) = V (Ï‰)U(Ï‰). Multi-
plying both sides of the equation by âˆ’i sgn(Ï‰),
c
W(Ï‰) = âˆ’i sgn(Ï‰)W(Ï‰) = V (Ï‰)[âˆ’i sgn(Ï‰)U(Ï‰)] = V (Ï‰)bU(Ï‰).
(4.2.13)

208
Advanced Engineering Mathematics: A Second Course
Again, using the convolution theorem as it applies to Fourier transforms, we arrive at the
ï¬nal result.
âŠ“âŠ”
â€¢ Example 4.2.2
Given the functions u(t) = cos(t) and v(t) = 1/(1 + t4), let us verify the convolution
theorem as it applies to Hilbert transforms.
With u(t) = cos(t) and v(t) = 1/(1 + t4),
w(t) = u(t) âˆ—v(t) =
Z âˆ
âˆ’âˆ
cos(t âˆ’x)
1 + x4
dx
(4.2.14)
=
Z âˆ
âˆ’âˆ
cos(t) cos(x)
1 + x4
dx +
Z âˆ
âˆ’âˆ
sin(t) sin(x)
1 + x4
dx
(4.2.15)
= Ï€
âˆš
2eâˆ’1/
âˆš
2

cos
 1
âˆš
2

+ sin
 1
âˆš
2

cos(t)
(4.2.16)
so that
bw(t) = Ï€
âˆš
2eâˆ’1/
âˆš
2

cos
 1
âˆš
2

+ sin
 1
âˆš
2

sin(t).
(4.2.17)
Because bv(t) = t(1 + t2)/[
âˆš
2 (1 + t4)],
u(t) âˆ—bv(t) =
1
âˆš
2
Z âˆ
âˆ’âˆ
cos(t âˆ’x)x(1 + x2)
1 + x4
dx
(4.2.18)
=
1
âˆš
2
Z âˆ
âˆ’âˆ
cos(t) cos(x)x(1 + x2)
1 + x4
dx + 1
âˆš
2
Z âˆ
âˆ’âˆ
sin(t) sin(x)x(1 + x2)
1 + x4
dx
(4.2.19)
=
1
âˆš
2 sin(t)
Z âˆ
âˆ’âˆ
x(1 + x2) sin(x)
1 + x4
dx
(4.2.20)
= Ï€
âˆš
2eâˆ’1/
âˆš
2

cos
 1
âˆš
2

+ sin
 1
âˆš
2

sin(t),
(4.2.21)
and the convolution theorem for Hilbert transforms holds true in this case.
âŠ“âŠ”
Product theorem
Let f(t) and g(t) denote complex functions with Fourier transforms F(Ï‰) and G(Ï‰),
respectively. If
1) F(Ï‰) vanishes for |Ï‰| > a, and G(Ï‰) vanishes for |Ï‰| < a, where a > 0,
or
2) f(t) and g(t) are analytic functions (their real and imaginary parts are Hilbert pairs),
then the Hilbert transform of the product of f(t) and g(t) is
H[f(t)g(t)] = f(t)bg(t).
(4.2.22)

The Hilbert Transform
209
F(u) = 0, u<0
(b)
(a) 
G(u) = 0, |v| < a
G(u) = 0, v<0
v
v
u+v = 0
u+v = 0
F(u) = 0, |u| > a
u
u
a
-a
a
-a
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      




























                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    





























Figure 4.2.1: Region of integration in the proof of the product theorem.
Proof :9 The product f(t)g(t) can be expressed as
f(t)g(t) =
1
4Ï€2
Z âˆ
âˆ’âˆ
Z âˆ
âˆ’âˆ
F(u)G(v)ei(u+v)t dv du.
(4.2.23)
Because H(eibt) = i sgn(b)eibt,
H[f(t)g(t)] =
i
4Ï€2
Z âˆ
âˆ’âˆ
Z âˆ
âˆ’âˆ
F(u)G(v) sgn(u + v)ei(u+v)t dv du.
(4.2.24)
The shaded regions of Figure 4.2.1 are those in which the product F(u)G(v) is nonvanishing
for the conditions of the theorem. In Figure 4.2.1(a) the nonoverlapping Fourier transforms
yield two semi-inï¬nite strips in which the product is nonvanishing.
In Figure 4.2.1(b),
for analytic functions, the Fourier transforms vanish for negative arguments10 so that the
product is nonvanishing only in the ï¬rst quadrant. In both cases sgn(u + v) = sgn(v) over
the regions of integration in which the integrand is nonvanishing. Thus,
H[f(t)g(t)] =
i
4Ï€2
Z âˆ
âˆ’âˆ
Z âˆ
âˆ’âˆ
F(u)G(v) sgn(v)ei(u+v)t dv du
(4.2.25)
= f(t) i
2Ï€
Z âˆ
âˆ’âˆ
G(v) sgn(v)eivt dv = f(t)bg(t).
(4.2.26)
âŠ“âŠ”
â€¢ Example 4.2.3: Hilbert transforms of band-pass functions
In communications, we have the double-sideband, amplitude-modulated signal given
by a(t) cos(Ï‰t + Ï•), where Ï• is constant. From the product theorem, its Hilbert transform
equals a(t) sin(Ï‰t + Ï•), Ï‰ > 0, provided that the highest frequency component in a(t) is
less than Ï‰. Paradoxically, the Hilbert transform of more general a(t) cos[Ï‰t + Ï•(t)], which
equals a(t) sin[Ï‰t + Ï•(t)], has no such restriction.
9 See Bedrosian, E., 1963: A product theorem for Hilbert transforms. Proc. IEEE, 51, 868â€“869. This
theorem has been extended to functions of n-dimensional real vectors by Stark, H., 1971: An extension of
the Hilbert transform product theorem. Proc. IEEE, 59, 1359â€“1360.
10 Titchmarsh, E. C., 1948: Introduction to the Theory of Fourier Integrals. Oxford University Press,
p. 128.

210
Advanced Engineering Mathematics: A Second Course
Problems
Verify the orthogonality property of Hilbert transforms using
1. x(t) = 1/(1 + t4)
2. x(t) = sin(t)/(1 + t2)
3. x(t) =
 1,
0 < t < a
0,
otherwise
Verify the convolution theorem for Hilbert transforms using
4. u(t) =
 1,
0 < t < a,
0,
otherwise,
v(t) = sin(t)
5. u(t) = cos(t),
v(t) =
1
1 + t2
6. Use the product theorem to show that
H[sin(at)Jn(bt)] = âˆ’cos(at)Jn(bt),
0 < b < a,
if n = 0, 1, 2, 3, . . . .
Hint:
F[Jn(bt)] = 2(âˆ’1)m
âˆš
b2 âˆ’Ï‰2 Tn
|Ï‰|
b

H(b âˆ’|Ï‰|),
where Tn(Â·) is a Chebyshev polynomial of the ï¬rst kind and m = n/2 or (nâˆ’1)/2, depending
upon which deï¬nition gives an integer.
7. Given cosine and sine integrals:
Ci(x) = âˆ’
Z âˆ
x
cos(t)
t
dt,
Si(x) = âˆ’
Z âˆ
x
sin(t)
t
dt,
and
H[Ci(a|t|)] = âˆ’sgn(t)Si(a|t|),
0 < a,
use the product rule to show that
H[sin(bt)Ci(a|t|)] = âˆ’sgn(t) sin(bt)Si(a|t|),
0 < b < a.
Hint:
F[Ci(a|t|)] =

0,
0 < |Ï‰| < a,
âˆ’Ï€/|Ï‰|,
a < |Ï‰| < âˆ,
0 < a.
8. Prove that
H[tx(t)] = tbx(t) âˆ’1
Ï€
Z âˆ
âˆ’âˆ
x(Ï„) dÏ„.
Hint:
Ï„x(Ï„)
t âˆ’Ï„ = tx(Ï„)
t âˆ’Ï„ âˆ’x(Ï„).

The Hilbert Transform
211
4.3 ANALYTIC SIGNALS
The monochromatic signal A cos(Ï‰0t + Ï•) appears in many physical and engineering
applications. It is common to represent this signal by the complex representation Aei(Ï‰0t+Ï•).
These two representations are related to each other by
A cos(Ï‰0t + Ï•) = â„œ
h
Aei(Ï‰0t+Ï•)i
= 1
2

Aei(Ï‰0t+Ï•) + Aeâˆ’i(Ï‰0t+Ï•)
.
(4.3.1)
Furthermore, the Fourier transform of A cos(Ï‰0t + Ï•) is
F[A cos(Ï‰0t + Ï•)] = 1
2

AeiÏ•Î´(Ï‰ âˆ’Ï‰0) + Aeâˆ’iÏ•Î´(Ï‰ + Ï‰0)

,
(4.3.2)
while the Fourier transform of Aei(Ï‰0t+Ï•) is
F
h
Aei(Ï‰0t+Ï•)i
= AeiÏ•Î´(Ï‰ âˆ’Ï‰0).
(4.3.3)
As Equation 4.3.2 and Equation 4.3.3 clearly show, in passing from the real signal to its
complex representation, we double the strength of the positive frequencies and remove
entirely the negative frequencies.
Let us generalize these concepts to nonmonochromatic signals. For the real signal x(t)
with Fourier transform X(Ï‰) and the complex signal z(t) with Fourier transform Z(Ï‰), the
previous paragraph shows that our generalization must have the property:
Z(Ï‰) = X(Ï‰) + sgn(Ï‰)X(Ï‰)
(4.3.4)
or
Z(Ï‰) =
ï£±
ï£²
ï£³
2X(Ï‰),
Ï‰ > 0,
X(Ï‰),
Ï‰ = 0,
0,
Ï‰ < 0.
(4.3.5)
Taking the inverse of Equation 4.3.4, we have the deï¬nition of an analytic signal as
z(t) = x(t) + ibx(t),
(4.3.6)
where x(t) is a real signal and bx(t) is its Hilbert transform.
â€¢ Example 4.3.1
In Figure 4.3.1 the amplitude spectrum of the analytic signal is graphed when x(t) is
the rectangular pulse,
x(t) =

1,
|t| < a,
0,
|t| > a.
Note that the amplitude spectrum equals zero for Ï‰ < 0 and twice the amplitude spectrum
for Ï‰ > 0.
âŠ“âŠ”
â€¢ Example 4.3.2
Let us ï¬nd the energy of an analytic signal.
The energy of an analytic signal is
Z âˆ
âˆ’âˆ
|z(t)|2 dt =
Z âˆ
âˆ’âˆ
x2(t) dt +
Z âˆ
âˆ’âˆ
bx2(t) dt = 2
Z âˆ
âˆ’âˆ
x2(t) dt = 2
Z âˆ
âˆ’âˆ
|X(Ï‰)|2 dÏ‰ (4.3.7)

212
Advanced Engineering Mathematics: A Second Course
âˆ’20.0
âˆ’10.0
0.0
10.0
20.0
 a
âˆ’1.0
0.0
1.0
2.0
3.0
4.0
phase (radians)
âˆ’1.0
0.0
1.0
2.0
3.0
4.0
5.0
amplitude / a
Ï‰
Figure 4.3.1: The spectrum of the analytic signal when x(t) is the rectangular pulse given in Example
4.3.1.
(b)
(d)
(a)
(c)
X(  )
Ï‰
Ï‰
Ï‰
âˆ’Ï‰
Ï‰
âˆ’Ï‰
Z(  )
0
0
Ï‰
0
0
Ï‰
Ï‰
âˆ’Ï‰
max
max
Ï‰max
Ï‰
Ï‰
Figure 4.3.2: Given a function x(t) with an amplitude spectrum shown in (a), frame (b) shows the
amplitude spectrum of the amplitude-modulated signal x(t) cos(Ï‰0t) while frames (c) and (d) give the
amplitude spectrum of the analytic signal z(t) and x(t) cos(Ï‰0t) âˆ’bx(t) sin(Ï‰0t), respectively.
by Parsevalâ€™s theorem. Thus, the analytic signal has twice the energy of the corresponding
real signal.
âŠ“âŠ”
Consider the function x(t) whose amplitude spectrum is shown in Figure 4.3.2(a). If
we were to amplitude modulate x(t) with cos(Ï‰0t), then the amplitude spectrum of this
modulated signal would appear as pictured in Figure 4.3.2(b).
Consider now the signal
y(t) = x(t) cos(Ï‰0t) âˆ’bx(t) sin(Ï‰0t) = â„œ

[x(t) + ibx(t)]eiÏ‰0t	
(4.3.8)
= â„œ

z(t)eiÏ‰0t	
= 1
2

z(t)eiÏ‰0t + zâˆ—(t)eâˆ’iÏ‰0t
,
(4.3.9)
where z(t) is the analytic signal of x(t). We have plotted the amplitude spectrum |Z(Ï‰)| in
Figure 4.3.2(c). If we computed the amplitude spectrum of y(t), we would ï¬nd that
Y (Ï‰) = 1
2Z(Ï‰ âˆ’Ï‰0) + 1
2Z(âˆ’Ï‰ âˆ’Ï‰0)
(4.3.10)

The Hilbert Transform
213
Y (Ï‰) =
ï£±
ï£²
ï£³
X(Ï‰ âˆ’Ï‰0),
Ï‰0 â‰¤Ï‰ â‰¤Ï‰0 + Ï‰max,
Xâˆ—(âˆ’Ï‰ âˆ’Ï‰0),
âˆ’Ï‰0 âˆ’Ï‰max â‰¤Ï‰ â‰¤Ï‰0,
0,
otherwise.
(4.3.11)
We have sketched this amplitude spectrum |Y (Ï‰)| in Figure 4.3.2(d). Each triangular part
is called the single sideband signal because it contains the upper frequencies (|Ï‰| > Ï‰0) of
the modulated signal x(t) cos(Ï‰0t). Similarly, if we had used x(t) cos(Ï‰0t) + bx(t) sin(Ï‰0t),
we would only have obtained the lower sidebands. Consequently, a communication system
using x(t) cos(Ï‰0t)âˆ’bx(t) sin(Ï‰0t) or x(t) cos(Ï‰0t)+bx(t) sin(Ï‰0t) would realize a 50% savings
in its frequency bandwidth over one transmitting x(t) cos(Ï‰0t).
Problems
1. Find the analytic signal corresponding to x(t) = cos(Ï‰t), Ï‰ > 0.
2. Show that the polar form of an analytic signal can be written
z(t) = |z(t)|eiÏ•(t),
where
|z(t)|2 = x2(t) + bx2(t),
Ï•(t) = tanâˆ’1
 bx(t)
x(t)

.
3. Analytic signals are often used with narrow-band waveforms with carrier frequency Ï‰0.
If Ï•(t) = Ï‰0t + Ï•â€²(t), show that the analytic signal can be written z(t) = r(t)eiÏ‰0t, where
r(t) = |z(t)|eiÏ•â€²(t). The function r(t) is called the complex envelope or the phasor amplitude;
this is a generalization of the phasor idea beyond pure alternating currents.
4.4 CAUSALITY: THE KRAMERS-KRONIG RELATIONSHIP
Causality is the physical principle which states that an event cannot proceed its cause.
In this section we explore what eï¬€ect this principle has on Hilbert transforms.
We begin by introducing the concept of causal functions. A causal function is a function
that equals zero for all t < 0. As with all functions we can write it in terms of an even xe(t)
and an odd xo(t) part as x(t) = xe(t) + xo(t). Because x(t) is causal, xo(t) = sgn(t)xe(t)
and
x(t) = xe(t) + sgn(t)xe(t).
(4.4.1)
Taking the Fourier transform of Equation 4.4.1, we ï¬nd that the Fourier transform of all
causal functions are of the form
X(Ï‰) = Xe(Ï‰) âˆ’i b
Xe(Ï‰),
(4.4.2)
where
b
Xe(Ï‰) = 1
Ï€
Z âˆ
âˆ’âˆ
Xe(Ï„)
Ï‰ âˆ’Ï„ dÏ„,
and
Xe(Ï‰) = âˆ’1
Ï€
Z âˆ
âˆ’âˆ
b
Xe(Ï„)
Ï‰ âˆ’Ï„ dÏ„,
(4.4.3)
because
2Ï€F[xe(t)sgn(t)] = 2
iÏ‰ âˆ—Xe(Ï‰) = 2
i
Z âˆ
âˆ’âˆ
Xe(Ï„)
Ï‰ âˆ’Ï„ dÏ„.
(4.4.4)

214
Advanced Engineering Mathematics: A Second Course
Equation 4.4.3 ï¬rst arose in dielectric theory and, taken together, are called the Kramers11
and Kronig12 relation after their discoverers, who derived these relationships during their
work on the dispersion of light by gaseous atoms or molecules.
â€¢ Example 4.4.1
Let us verify the Kramers-Kronig relation using the causal time function x(t) = H(t).
Because xe(t) = 1
2 and Xe(Ï‰) = Ï€Î´(Ï‰),
b
Xe(Ï‰) = 1
Ï€
Z âˆ
âˆ’âˆ
Ï€ Î´(Ï„)
Ï‰ âˆ’Ï„ dÏ„ = âˆ’1
Ï‰ .
(4.4.5)
Consequently, by the Kramers-Kronig relation,
F[H(t)] = Xe(Ï‰) âˆ’i b
Xe(Ï‰) = Ï€Î´(Ï‰) + i
Ï‰ .
(4.4.6)
âŠ“âŠ”
â€¢ Example 4.4.2
A simple example of a causal function is the impulse response or Greenâ€™s function intro-
duced in earlier chapters. From Equation 4.4.2 we have the result that the transfer function
G(Ï‰), the Fourier transform of the impulse response, must yield the Hilbert transform pair
Ge(Ï‰) âˆ’i bGe(Ï‰).
For example, if g(t) = eâˆ’tH(t), then G(Ï‰) = 1/(1 + iÏ‰). Because
1
1 + iÏ‰ =
1
Ï‰2 + 1 âˆ’i
Ï‰
Ï‰2 + 1,
(4.4.7)
we have the Hilbert transform pair of
x(t) =
1
t2 + 1
and
bx(t) =
t
t2 + 1.
(4.4.8)
âŠ“âŠ”
â€¢ Example 4.4.3
Let us verify the Kramers-Kronig relation for the Hilbert transform pair
x(t) =
1
t4 + 1
and
bx(t) =
t(t2 + 1)
âˆš
2(t4 + 1)
(4.4.9)
by direct integration.
11 Kramers, H. A., 1929: Die Dispersion und Absorption von RÂ¨ontgenstrahlen. Phys. Z., 30, 522â€“523.
12 Kronig, R. de L., 1926: On the theory of dispersion of x-rays. J. Opt. Soc. Am., 12, 547â€“551.

The Hilbert Transform
215
From Equation 4.4.3, we have that
Ï‰(Ï‰2 + 1)
âˆš
2(Ï‰4 + 1) = 1
Ï€
Z âˆ
âˆ’âˆ
dÏ„
(Ï„ 4 + 1)(Ï‰ âˆ’Ï„).
(4.4.10)
Applying the residue theorem to the right side of Equation 4.4.10, we obtain
Ï‰(Ï‰2 + 1)
âˆš
2(Ï‰4 + 1) = i Res

1
(z4 + 1)(Ï‰ âˆ’z); Ï‰

+ 2i Res

1
(z4 + 1)(Ï‰ âˆ’z); eÏ€i/4

+ 2i Res

1
(z4 + 1)(Ï‰ âˆ’z); e3Ï€i/4

.
(4.4.11)
We only include one half of the value of the residue at Ï„ = Ï‰ because the singularity lies
on the path of integration and we must treat this integration along the lines of a Cauchy
principal value. Evaluating the residues, we ï¬nd
Res

1
(z4 + 1)(Ï‰ âˆ’z); Ï‰

= âˆ’
1
Ï‰4 + 1,
(4.4.12)
Res

1
(z4 + 1)(Ï‰ âˆ’z); eÏ€i/4

=
âˆš
2 âˆ’(1 + i)Ï‰
4
âˆš
2

Ï‰ âˆ’
1
âˆš
2
2
+ 1
2
,
(4.4.13)
and
Res

1
(z4 + 1)(Ï‰ âˆ’z); e3Ï€i/4

=
âˆš
2 + (1 âˆ’i)Ï‰
4
âˆš
2

Ï‰ +
1
âˆš
2
2
+ 1
2
.
(4.4.14)
Substituting Equation 4.4.12 through Equation 4.4.14 into the right side of Equation 4.4.11,
we obtain the left side.
Problems
1. For a causal function x(t), prove that xo(t) = sgn(t)xe(t) and xe(t) = sgn(t)xo(t).
2. Redo our analysis if x(t) is a negative time function, i.e., x(t) = 0 if t > 0. Verify your
result using x(t) = etH(âˆ’t).
3. Using g(t) = teâˆ’tH(t), ï¬nd the corresponding Hilbert transform pairs.
4. Using g(t) = eâˆ’t cos(Ï‰t)H(t), ï¬nd the corresponding Hilbert transform pairs.
5. Verify the Kramers-Kronig relation for the Hilbert transform pair:
x(t) =
1
t2 + 1
and
bx(t) =
t
t2 + 1
by direct integration.
Further Reading
Hahn, S. L., 1996: Hilbert Transforms in Signal Processing. Artech House, 442 pp. Covers
the basic theory and gives some practical applications.


Chapter 5
Greenâ€™s Functions
An important aspect of engineering mathematics is the solution of linear ordinary and
partial diï¬€erential equations. As an undergraduate you were probably introduced to the
method of separation of variables, which leads to a solution in terms of an eigenfunction
expansion. However, this method is not the only one; there is Duhamelâ€™s principle which
uses the superposition integral. Here we expand upon this idea and illustrate how a solution,
called a Greenâ€™s function, to a diï¬€erential equation forced by the Dirac delta function can
be used in an integral representation of a solution when the forcing is arbitrary.
5.1 WHAT IS A GREENâ€™S FUNCTION?
The following examples taken from engineering show how Greenâ€™s functions naturally
appear during the solution of initial-value and boundary-value problems. We also show that
the solution u(x) can be expressed as an integral involving the Greenâ€™s function and f(x).
Circuit theory
In electrical engineering, one of the simplest electrical devices consists of a voltage
source v(t) connected to a resistor with resistance R and an inductor with inductance L.
See Figure 5.1.1. Denoting the current by i(t), the equation that governs this circuit is
Ldi
dt + Ri = v(t).
(5.1.1)
Consider now the following experiment: With the circuit initially dead, we allow the
voltage to suddenly become V0/âˆ†Ï„ during a very short duration âˆ†Ï„ starting at t = Ï„.
217

218
Advanced Engineering Mathematics: A Second Course
v(t)
-
+
L
i(t)
R
Figure 5.1.1: The RL electrical circuit driven by the voltage v(t).
Then, at t = Ï„ + âˆ†Ï„, we again turn oï¬€the voltage supply. Mathematically, for t > Ï„ + âˆ†Ï„,
the circuitâ€™s performance obeys the homogeneous diï¬€erential equation:
Ldi
dt + Ri = 0,
t > Ï„ + âˆ†Ï„,
(5.1.2)
whose solution is
i(t) = I0eâˆ’Rt/L,
t > Ï„ + âˆ†Ï„,
(5.1.3)
where I0 is a constant and L/R is the time constant of the circuit. Because the voltage v(t)
during Ï„ < t < Ï„ + âˆ†Ï„ is V0/âˆ†Ï„, then
Z Ï„+âˆ†Ï„
Ï„
v(t) dt = V0.
(5.1.4)
Therefore, over the interval Ï„ < t < Ï„ + âˆ†Ï„, Equation 5.1.1 can be integrated to yield
L
Z Ï„+âˆ†Ï„
Ï„
di + R
Z Ï„+âˆ†Ï„
Ï„
i(t) dt =
Z Ï„+âˆ†Ï„
Ï„
v(t) dt,
(5.1.5)
or
L [i(Ï„ + âˆ†Ï„) âˆ’i(Ï„)] + R
Z Ï„+âˆ†Ï„
Ï„
i(t) dt = V0.
(5.1.6)
If i(t) remains continuous as âˆ†Ï„ becomes small, then
R
Z Ï„+âˆ†Ï„
Ï„
i(t) dt â‰ˆ0.
(5.1.7)
Finally, because
i(Ï„) = 0
and
i(Ï„ + âˆ†Ï„) = I0eâˆ’R(Ï„+âˆ†Ï„)/L â‰ˆI0eâˆ’RÏ„/L,
(5.1.8)
for small âˆ†Ï„, Equation 5.1.6 reduces to
LI0eâˆ’RÏ„/L = V0,
or
I0 = V0
L eRÏ„/L.
(5.1.9)
Therefore, Equation 5.1.3 can be written as
i(t) =
(
0,
t < Ï„,
V0eâˆ’R(tâˆ’Ï„)/L/L,
Ï„ â‰¤t,
(5.1.10)

Greenâ€™s Functions
219
t
Ï„+âˆ†Ï„
Ï„
V  /L
0
i(t)
Figure 5.1.2: The current i(t) within an RL circuit when the voltage V0/âˆ†Ï„ is introduced between the
times Ï„ < t < Ï„ + âˆ†Ï„.
after using Equation 5.1.9. Equation 5.1.10 is plotted in Figure 5.1.2.
Consider now a new experiment with the same circuit where we subject the circuit
to N voltage impulses, each of duration âˆ†Ï„ and amplitude Vi/âˆ†Ï„ with i = 0, 1, . . . , N,
occurring at t = Ï„i. See Figure 5.1.3. The current response is then
i(t) =
ï£±
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£³
0,
t < Ï„0,
V0eâˆ’R(tâˆ’Ï„0)/L/L,
Ï„0 < t < Ï„1,
V0eâˆ’R(tâˆ’Ï„0)/L/L + V1eâˆ’R(tâˆ’Ï„1)/L/L,
Ï„1 < t < Ï„2,
...
...
N
X
i=0
Vieâˆ’R(tâˆ’Ï„i)/L/L,
Ï„N < t < Ï„N+1.
(5.1.11)
Finally, consider our circuit subjected to a continuous voltage source v(t). Over each
successive interval dÏ„, the step change in voltage is v(Ï„) dÏ„. Consequently, from Equation
5.1.11 the response i(t) is now given by the superposition integral
i(t) =
Z t
Ï„
v(Ï„)
L eâˆ’R(tâˆ’Ï„)/L dÏ„,
or
i(t) =
Z t
Ï„
v(Ï„)g(t|Ï„) dÏ„,
(5.1.12)
where
g(t|Ï„) = eâˆ’R(tâˆ’Ï„)/L
L
,
Ï„ < t.
(5.1.13)
Here we have assumed that i(t) = v(t) = 0 for t < Ï„. In Equation 5.1.13, g(t|Ï„) is called the
Greenâ€™s function. As this equation shows, given the Greenâ€™s function to Equation 5.1.1, the
response i(t) to any voltage source v(t) can be obtained by convolving the voltage source
with the Greenâ€™s function.
We now show that we could have found the Greenâ€™s function, Equation 5.1.13, by
solving Equation 5.1.1 subject to an impulse- or delta-forcing function. Mathematically,
this corresponds to solving the following initial-value problem:
Ldg
dt + Rg = Î´(t âˆ’Ï„),
g(0|Ï„) = 0.
(5.1.14)

220
Advanced Engineering Mathematics: A Second Course
Ï„
Ï„
Ï„2
1
0
Ï„N
t
i(t)
Figure 5.1.3: The current i(t) within an RL circuit when the voltage is changed at t = Ï„0, t = Ï„1, and so
forth.
Taking the Laplace transform of Equation 5.1.14, we ï¬nd that
G(s|Ï„) =
eâˆ’sÏ„
Ls + R,
or
g(t|Ï„) = eâˆ’R(tâˆ’Ï„)/L
L
H(t âˆ’Ï„),
(5.1.15)
where H(Â·) is the Heaviside step function. As our short derivation showed, the most direct
route to ï¬nding a Greenâ€™s function is solving the diï¬€erential equation when its forcing
equals the impulse or delta function. This is the technique that we will use throughout this
chapter.
Statics
Consider a string of length L that is connected at both ends to supports and is subjected
to a load (external force per unit length) of f(x). We wish to ï¬nd the displacement u(x) of
the string. If the load f(x) acts downward (negative direction), the displacement u(x) of
the string is given by the diï¬€erential equation:
T d2u
dx2 = f(x),
(5.1.16)
where T denotes the uniform tensile force of the string. Because the string is stationary at
both ends, the displacement u(x) satisï¬es the boundary conditions u(0) = u(L) = 0.
Instead of directly solving for the displacement u(x) of the string subject to the load
f(x), let us ï¬nd the displacement that results from a load Î´(xâˆ’Î¾) concentrated at the point
x = Î¾. See Figure 5.1.4. For this load, the diï¬€erential equation, Equation 5.1.16, becomes
T d2g
dx2 = Î´(x âˆ’Î¾),
(5.1.17)
subject to the boundary conditions g(0|Î¾) = g(L|Î¾) = 0.
In Equation 5.1.17, g(x|Î¾) denotes the displacement of the string when it is subjected
to an impulse load at x = Î¾. In line with our circuit theory example, it gives the Greenâ€™s
function for our statics problem. Once found, the displacement u(x) of the string subject to

Greenâ€™s Functions
221
Î¾
f(
Î¾ )
Î¾)
g(x
x
L
Figure 5.1.4: The response, commonly called a Greenâ€™s function, of a string ï¬xed at both ends to a point
load at x = Î¾.
any arbitrary load f(x) can be found by convolving the load f(x) with the Greenâ€™s function
g(x|Î¾) as we did earlier.
Let us now ï¬nd this Greenâ€™s function. At any point x Ì¸= Î¾, Equation 5.1.17 reduces to
the homogeneous diï¬€erential equation:
d2g
dx2 = 0,
(5.1.18)
which has the solution
g(x|Î¾) =

ax + b,
0 â‰¤x < Î¾,
cx + d,
Î¾ < x â‰¤L.
(5.1.19)
Applying the boundary conditions, Equation 5.1.19, we ï¬nd that
g(0|Î¾) = a Â· 0 + b = b = 0,
and
g(L|Î¾) = cL + d = 0,
or
d = âˆ’cL.
(5.1.20)
Therefore, we can rewrite Equation 5.1.19 as
g(x|Î¾) =

ax,
0 â‰¤x < Î¾,
c(x âˆ’L),
Î¾ < x â‰¤L,
(5.1.21)
where a and c are undetermined constants.
At x = Î¾, the displacement u(x) of the string must be continuous; otherwise, the string
would be broken. Therefore, the Greenâ€™s function given by Equation 5.1.21 must also be
continuous there. Thus,
aÎ¾ = c(Î¾ âˆ’L),
or
c =
aÎ¾
Î¾ âˆ’L.
(5.1.22)
From Equation 5.1.13 the second derivative of g(x|Î¾) must equal the impulse function.
Therefore, the ï¬rst derivative of g(x|Î¾), obtained by integrating this equation, must be
discontinuous by the amount 1/T or
lim
Ç«â†’0
dg(Î¾ + Ç«|Î¾)
dx
âˆ’dg(Î¾ âˆ’Ç«|Î¾)
dx

= 1
T ,
(5.1.23)
in which case
dg(Î¾+|Î¾)
dx
âˆ’dg(Î¾âˆ’|Î¾)
dx
= 1
T ,
(5.1.24)

222
Advanced Engineering Mathematics: A Second Course
where Î¾+ and Î¾âˆ’denote points lying just above or below Î¾, respectively. Using Equation
5.1.24, we ï¬nd that
dg(Î¾âˆ’|Î¾)
dx
= a,
and
dg(Î¾+|Î¾)
dx
= c =
aÎ¾
Î¾ âˆ’L.
(5.1.25)
Thus, Equation 5.1.25 leads to
aÎ¾
Î¾ âˆ’L âˆ’a = 1
T =
aL
Î¾ âˆ’L,
or
a = Î¾ âˆ’L
LT ,
(5.1.26)
and the Greenâ€™s function is
g(x|Î¾) =
1
TL(x> âˆ’L)x<,
(5.1.27)
where x< = min(x, Î¾) and x> = max(x, Î¾). To ï¬nd the displacement u(x) subject to the
load f(x), we proceed as we did in the previous example. The result of this analysis is
u(x) =
Z L
0
f(Î¾)g(x|Î¾) dÎ¾ = x âˆ’L
TL
Z x
0
f(Î¾) Î¾ dÎ¾ + x
TL
Z L
x
f(Î¾) (Î¾ âˆ’L) dÎ¾,
(5.1.28)
since Î¾ < x in the ï¬rst integral and x < Î¾ in the second integral of Equation 5.1.28.
Integral Equations
Consider the Sturm-Liouville problem
yâ€²â€² + Î»y = 0,
y(0) = y(L) = 0.
(5.1.29)
From its general theory, nontrivial solutions exist only if
Î»n = n2Ï€2
L2 ,
yn(x) = sin
nÏ€x
L

,
(5.1.30)
where n = 1, 2, 3, . . ..
Consider now a new boundary-value problem:
d2y
dx2 = âˆ’f(x),
y(0) = y(L) = 0.
(5.1.31)
In the next section (Equation 5.2.76), we will show that we can write its solution by
y(x) =
Z L
0
f(Î¾)g(x|Î¾) dÎ¾,
(5.1.32)
where the Greenâ€™s function g(x|Î¾) is given by
d2g
dx2 = âˆ’Î´(x âˆ’Î¾),
g(0|Î¾) = g(L|Î¾) = 0,
or
g(x|Î¾) = (L âˆ’x>)x</L,
(5.1.33)
where x> = max(x, Î¾) and x< = min(x, Î¾).

Greenâ€™s Functions
223
We can now use Equation 5.1.29 to rewrite Equation 5.1.31 as Î»y(Î¾) = f(Î¾). Multi-
plying this equation by g(x|Î¾) and integrating from 0 to L, we ï¬nd that
Z L
0
f(Î¾)g(x|Î¾) dÎ¾ = Î»
Z L
0
y(Î¾)g(x|Î¾) dÎ¾,
(5.1.34)
or
y(x) âˆ’Î»
Z L
0
y(Î¾)g(x|Î¾) dÎ¾ = 0.
(5.1.35)
Because of the equivalence of Equation 5.1.29 and Equation 5.1.35, the solutions to the
integral equation, Equation 5.1.35, are Î»n = n2Ï€2/L2 with yn(x) = sin(nÏ€x/L). Direct
substitution veriï¬es this result. Thus, we can use Greenâ€™s functions to construct integral
equations that have known solutions. Indeed, it was the use of Greenâ€™s functions to solve
Fredholm integral equations that drew the attention of mathematicians at the turn of the
twentieth century.1
5.2 ORDINARY DIFFERENTIAL EQUATIONS
Second-order diï¬€erential equations are ubiquitous in engineering. In electrical engi-
neering, many electrical circuits are governed by second-order, linear ordinary diï¬€erential
equations. In mechanical engineering they arise during the application of Newtonâ€™s second
law.
One of the drawbacks of solving ordinary diï¬€erential equations with a forcing term is
its lack of generality. Each new forcing function requires a repetition of the entire process.
In this section we give some methods for ï¬nding the solution in a somewhat more gen-
eral manner for stationary systems where the forcing, not any initially stored energy (i.e.,
nonzero initial conditions), produces the total output. Unfortunately, the solution must be
written as an integral.
Consider the linear diï¬€erential equation
yâ€²â€² + 2yâ€² + y = f(t),
(5.2.1)
subject to the initial conditions y(0) = yâ€²(0) = 0. Solving this equation by Laplace trans-
forms, we can write the Laplace transform of y(t), Y (s), as the product of two Laplace
transforms:
Y (s) =
1
(s + 1)2 F(s).
(5.2.2)
One drawback in using Equation 5.2.2 is its dependence upon an unspeciï¬ed Laplace trans-
form F(s). Is there a way to eliminate this dependence and yet retain the essence of the
solution?
One way of obtaining a quantity that is independent of the forcing is to consider the
ratio:
Y (s)
F(s) = G(s) =
1
(s + 1)2 .
(5.2.3)
This ratio is called the transfer function because we can transfer the input F(s) into the
output Y (s) by multiplying F(s) by G(s).
It depends only upon the properties of the
system.
1 See Section 36 in Kneser, A., 1911: Integralgleichungen und ihre Anwendungen in der mathematischen
Physik. Braunschweig, 293 pp.

224
Advanced Engineering Mathematics: A Second Course
Let us now consider a problem related to Equation 5.2.1, namely
gâ€²â€² + 2gâ€² + g = Î´(t),
t > 0,
(5.2.4)
with g(0) = gâ€²(0) = 0. Because the forcing equals the Dirac delta function, g(t) is called
the impulse response or Greenâ€™s function.2 Computing G(s),
G(s) =
1
(s + 1)2 .
(5.2.5)
From Equation 5.2.3 we see that G(s) is also the transfer function. Thus, an alternative
method for computing the transfer function is to subject the system to impulse forcing and
the Laplace transform of the response is the transfer function.
From Equation 5.2.3,
Y (s) = G(s)F(s),
(5.2.6)
or
y(t) = g(t) âˆ—f(t).
(5.2.7)
That is, the convolution of the impulse response with the particular forcing gives the re-
sponse of the system. Thus, we may describe a stationary system in one of two ways: (1)
in the transform domain we have the transfer function, and (2) in the time domain there is
the impulse response.
Despite the fundamental importance of the impulse response or Greenâ€™s function for a
given linear system, it is often quite diï¬ƒcult to determine, especially experimentally, and a
more convenient practice is to deal with the response to the unit step H(t). This response
is called the indicial admittance or step response, which we shall denote by a(t). Because
L[H(t)] = 1/s, we can determine the transfer function from the indicial admittance because
L[a(t)] = G(s)L[H(t)] or sA(s) = G(s). Furthermore, because
L[g(t)] = G(s) = L[a(t)]
L[H(t)],
(5.2.8)
then
g(t) = da(t)
dt ,
(5.2.9)
since L[f â€²(t)] = sF(s) âˆ’f(0+).
â€¢ Example 5.2.1
Let us ï¬nd the transfer function, impulse response, and step response for the system
yâ€²â€² âˆ’3yâ€² + 2y = f(t),
(5.2.10)
with y(0) = yâ€²(0) = 0. To ï¬nd the impulse response, we solve
gâ€²â€² âˆ’3gâ€² + 2g = Î´(t âˆ’Ï„),
(5.2.11)
2 For the origin of the Greenâ€™s function, see Farina, J. E. G., 1976: The work and signiï¬cance of George
Green, the miller mathematician, 1793â€“1841. Bull. Inst. Math. Appl., 12, 98â€“105.

Greenâ€™s Functions
225
with g(0) = gâ€²(0) = 0. We have generalized the problem to an arbitrary forcing at t = Ï„
and now denote the Greenâ€™s function by g(t|Ï„). We have done this so that our discussion
will be consistent with the other sections in the chapter.
Taking the Laplace transform of Equation 5.2.11, we ï¬nd that
G(s|Ï„) =
eâˆ’sÏ„
s2 âˆ’3s + 2,
(5.2.12)
which is the transfer function for this system when Ï„ = 0. The impulse response or Greenâ€™s
function equals the inverse of G(s|Ï„) or
g(t|Ï„) =
h
e2(tâˆ’Ï„) âˆ’etâˆ’Ï„i
H(t âˆ’Ï„).
(5.2.13)
To ï¬nd the step response, we solve
aâ€²â€² âˆ’3aâ€² + 2a = H(t),
(5.2.14)
with a(0) = aâ€²(0) = 0. Taking the Laplace transform of Equation 5.2.14,
A(s) =
1
s(s âˆ’1)(s âˆ’2),
(5.2.15)
and the indicial admittance is given by the inverse of Equation 5.2.15, or
a(t) = 1
2 + 1
2e2t âˆ’et.
(5.2.16)
Note that aâ€²(t) = g(t|0).
âŠ“âŠ”
â€¢ Example 5.2.2
MATLABâ€™s control toolbox contains several routines for the numerical computation
of impulse and step responses if the transfer function can be written as the ratio of two
polynomials. To illustrate this capacity, let us redo the previous example where the transfer
function is given by Equation 5.2.12 with Ï„ = 0. The transfer function is introduced by
loading in the polynomial in the numerator num and in the denominator den followed by
calling tf. The MATLAB script
clear
% load in coefficients of the numerator and denominator
%
of the transfer function
num = [0 0 1]; den = [1 -3 2];
% create the transfer function
sys = tf(num,den);
% find the step response, a
[a,t] = step(sys);
% plot the indicial admittance
subplot(2,1,1), plot(t, a, â€™oâ€™)
ylabel(â€™indicial responseâ€™,â€™Fontsizeâ€™,20)
% find the impulse response, g
[g,t] = impulse(sys);
% plot the impulse response

226
Advanced Engineering Mathematics: A Second Course
0
0.2
0.4
0.6
0.8
1
1.2
0
1
2
3
indicial response
0
0.2
0.4
0.6
0.8
1
1.2
0
2
4
6
8
impulse response
time
Figure 5.2.1: The impulse and step responses corresponding to the transfer function, Equation 5.2.12,
with Ï„ = 0.
subplot(2,1,2), plot(t, g, â€™oâ€™)
ylabel(â€™impulse responseâ€™,â€™Fontsizeâ€™,20)
xlabel(â€™timeâ€™,â€™Fontsizeâ€™,20)
shows how the impulse and step responses are found. Both of them are shown in Figure
5.2.1.
âŠ“âŠ”
â€¢ Example 5.2.3
There is an old joke about a man who took his car into a garage because of a terrible
knocking sound. Upon his arrival the mechanic took one look at it and gave it a hefty
kick.3 Then, without a momentâ€™s hesitation he opened the hood, bent over, and tightened
up a loose bolt. Turning to the owner, he said, â€œYour car is ï¬ne. Thatâ€™ll be $50.â€ The
owner felt that the charge was somewhat excessive, and demanded an itemized account.
The mechanic said, â€œThe kicking of the car and tightening one bolt, cost you a buck. The
remaining $49 comes from knowing where to kick the car and ï¬nding the loose bolt.â€
Although the moral of the story may be about expertise as a marketable commodity,
it also illustrates the concept of transfer function.4 Let us model the car as a linear system
where the equation
an
dny
dtn + anâˆ’1
dnâˆ’1y
dtnâˆ’1 + Â· Â· Â· + a1
dy
dt + a0y = f(t)
(5.2.17)
governs the response y(t) to a forcing f(t). Assuming that the car has been sitting still, the
initial conditions are zero and the Laplace transform of Equation 5.2.17 is
K(s)Y (s) = F(s),
(5.2.18)
where
K(s) = ansn + anâˆ’1snâˆ’1 + Â· Â· Â· + a1s + a0.
(5.2.19)
3 This is obviously a very old joke.
4 Originally suggested by Stern, M. D., 1987: Why the mechanic kicked the car - A teaching aid for
transfer functions. Math. Gaz., 71, 62â€“64.

Greenâ€™s Functions
227
Hence,
Y (s) = F(s)
K(s) = G(s)F(s),
(5.2.20)
where the transfer function G(s) clearly depends only on the internal workings of the car.
So if we know the transfer function, we understand how the car vibrates because
y(t) =
Z t
0
g(t âˆ’x)f(x) dx.
(5.2.21)
But what does this have to do with our mechanic? He realized that a short sharp kick
mimics an impulse forcing with f(t) = Î´(t) and y(t) = g(t). Therefore, by observing the
response of the car to his kick, he diagnosed the loose bolt and ï¬xed the car.
âŠ“âŠ”
In the previous examples, we used Laplace transforms to solve for the Greenâ€™s functions.
However, there is a rich tradition of using Fourier transforms rather than Laplace transforms.
In these particular cases, the Fourier transform of the Greenâ€™s function is called frequency
response or steady-state transfer function of our system when Ï„ = 0. Consider the following
examples.
â€¢ Example 5.2.4: Spectrum of a damped harmonic oscillator
In mechanics the damped oscillations of a mass m attached to a spring with a spring
constant k and damped with a velocity-dependent resistance are governed by the equation
myâ€²â€² + cyâ€² + ky = f(t),
(5.2.22)
where y(t) denotes the displacement of the oscillator from its equilibrium position, c denotes
the damping coeï¬ƒcient, and f(t) denotes the forcing.
Assuming that both f(t) and y(t) have Fourier transforms, let us analyze this system
by ï¬nding its frequency response.
We begin by solving for the Greenâ€™s function g(t|Ï„),
which is given by
mgâ€²â€² + cgâ€² + kg = Î´(t âˆ’Ï„),
(5.2.23)
because the Greenâ€™s function is the response of a system to a delta function forcing. Taking
the Fourier transform of both sides of Equation 5.2.23, the frequency response is
G(Ï‰|Ï„) =
eâˆ’iÏ‰Ï„
k + icÏ‰ âˆ’mÏ‰2 =
eâˆ’iÏ‰Ï„/m
Ï‰2
0 + icÏ‰/m âˆ’Ï‰2 ,
(5.2.24)
where Ï‰2
0 = k/m is the natural frequency of the system. The most useful quantity to plot
is the frequency response or
|G(Ï‰|Ï„)| =
Ï‰2
0
k
p
(Ï‰2 âˆ’Ï‰2
0)2 + Ï‰2Ï‰2
0(c2/km)
(5.2.25)
=
1
k
p
[(Ï‰/Ï‰0)2 âˆ’1]2 + (c2/km)(Ï‰/Ï‰0)2 .
(5.2.26)
In Figure 5.2.2 we plotted the frequency response as a function of c2/(km). Note that as the
damping becomes larger, the sharp peak at Ï‰ = Ï‰0 essentially vanishes. As c2/(km) â†’0,

228
Advanced Engineering Mathematics: A Second Course
Ï‰/Ï‰ 0
k
0.0
0.5
1.0
1.5
2.0
0.0
1.0
2.0
3.0
4.0
5.0
6.0
7.0
8.0
9.0
10.0
11.0
c /km = 0.01
2
c /km = 0.1
c /km = 1
2
2
|
)
G(Ï‰ |
Figure 5.2.2: The variation of the frequency response for a damped harmonic oscillator as a function of
driving frequency Ï‰. See the text for the deï¬nition of the parameters.
we obtain a very ï¬nely tuned response curve. Let us now ï¬nd the Greenâ€™s function. From
the deï¬nition of the inverse Fourier transform,
mg(t|Ï„) = âˆ’1
2Ï€
Z âˆ
âˆ’âˆ
eiÏ‰t
Ï‰2 âˆ’icÏ‰/m âˆ’Ï‰2
0
dÏ‰ = âˆ’1
2Ï€
Z âˆ
âˆ’âˆ
eiÏ‰t
(Ï‰ âˆ’Ï‰1)(Ï‰ âˆ’Ï‰2) dÏ‰,
(5.2.27)
where
Ï‰1,2 = Â±
q
Ï‰2
0 âˆ’Î³2 + Î³i,
(5.2.28)
and Î³ = c/(2m) > 0. We can evaluate Equation 5.2.27 by residues. Clearly the poles always
lie in the upper half of the Ï‰-plane. Thus, if t < Ï„ in Equation 5.2.27 we can close the line
integration along the real axis with a semicircle of inï¬nite radius in the lower half of the
Ï‰-plane by Jordanâ€™s lemma. Because the integrand is analytic within the closed contour,
g(t|Ï„) = 0 for t < Ï„. This is simply the causality condition,5 the impulse forcing being the
cause of the excitation. Clearly, causality is closely connected with the analyticity of the
frequency response in the lower half of the Ï‰-plane.
If t > Ï„, we close the line integration along the real axis with a semicircle of inï¬nite
radius in the upper half of the Ï‰-plane and obtain
mg(t|Ï„) = 2Ï€i

âˆ’1
2Ï€
 
Res

eiz(tâˆ’Ï„)
(z âˆ’Ï‰1)(z âˆ’Ï‰2); Ï‰1

+ Res

eiz(tâˆ’Ï„)
(z âˆ’Ï‰1)(z âˆ’Ï‰2); Ï‰2

(5.2.29)
=
âˆ’i
Ï‰1 âˆ’Ï‰2
h
eiÏ‰1(tâˆ’Ï„) âˆ’eiÏ‰2(tâˆ’Ï„)i
=
eâˆ’Î³(tâˆ’Ï„) sin
h
(t âˆ’Ï„)
p
Ï‰2
0 âˆ’Î³2
i
p
Ï‰2
0 âˆ’Î³2
H(t âˆ’Ï„).
(5.2.30)
Let us now examine the damped harmonic oscillator by describing the migration of
the poles Ï‰1,2 in the complex Ï‰-plane as Î³ increases from 0 to âˆ. See Figure 5.2.3. For
Î³ â‰ªÏ‰0 (weak damping), the poles Ï‰1,2 are very near to the real axis, above the points
Â±Ï‰0, respectively. This corresponds to the narrow resonance band discussed earlier and
we have an underdamped harmonic oscillator.
As Î³ increases from 0 to Ï‰0, the poles
5 The principle stating that an event cannot precede its cause.

Greenâ€™s Functions
229
Ï„
Ï„
x
y
âˆ’Ï‰0
0
Ï‰
t > 
t < 
Figure 5.2.3: The migration of the poles of the frequency response of a damped harmonic oscillator as a
function of Î³.
approach the positive imaginary axis, moving along a semicircle of radius Ï‰0 centered at
the origin. They coalesce at the point iÏ‰0 for Î³ = Ï‰0, yielding repeated roots, and we have
a critically damped oscillator. For Î³ > Ï‰0, the poles move in opposite directions along
the positive imaginary axis; one of them approaches the origin, while the other tends to
iâˆas Î³ â†’âˆ. The solution then has two purely decaying, overdamped solutions. During
the early 1950s, a similar diagram was invented by Evans6 where the movement of closed-
loop poles is plotted for all values of a system parameter, usually the gain. This root-locus
method is very popular in system control theory for two reasons. First, the investigator can
easily determine the contribution of a particular closed-loop pole to the transient response.
Second, he can determine the manner in which open-loop poles or zeros should be introduced
or their location modiï¬ed so that he will achieve a desired performance characteristic for
his system.
âŠ“âŠ”
â€¢ Example 5.2.5: Low-frequency ï¬lter
Consider the ordinary diï¬€erential equation
Ryâ€² + y
C = f(t),
(5.2.31)
where R and C are real, positive constants. If y(t) denotes current, then Equation 5.2.31
would be the equation that gives the voltage across a capacitor in an RC circuit. Let us ï¬nd
the frequency response and Greenâ€™s function for this system. We begin by writing Equation
5.2.31 as
Rgâ€² + g
C = Î´(t âˆ’Ï„),
(5.2.32)
where g(t|Ï„) denotes the Greenâ€™s function. If the Fourier transform of g(t|Ï„) is G(Ï‰|Ï„), the
frequency response G(Ï‰|Ï„) is given by
iÏ‰RG(Ï‰|Ï„) + G(Ï‰|Ï„)
C
= eâˆ’iÏ‰Ï„,
(5.2.33)
6 Evans, W. R., 1948: Graphical analysis of control systems. Trans. AIEE, 67, 547â€“551; Evans, W.
R., 1954: Control-System Dynamics. McGraw-Hill, 282 pp.

230
Advanced Engineering Mathematics: A Second Course
p
Ï‰/Ï‰
|
0.0
1.0
2.0
3.0
4.0
5.0
0.0
0.2
0.4
0.6
0.8
1.0
G(Ï‰
/C
|)
Figure 5.2.4: The variation of the frequency response, Equation 5.2.35, as a function of driving frequency
Ï‰. See the text for the deï¬nition of the parameters.
or
G(Ï‰|Ï„) =
eâˆ’iÏ‰Ï„
iÏ‰R + 1/C =
Ceâˆ’iÏ‰Ï„
1 + iÏ‰RC ,
(5.2.34)
and
|G(Ï‰|Ï„)| =
C
âˆš
1 + Ï‰2R2C2 =
C
q
1 + Ï‰2/Ï‰2p
,
(5.2.35)
where Ï‰p = 1/(RC) is an intrinsic constant of the system.
In Figure 5.2.4 we plotted
|G(Ï‰|Ï„)| as a function of Ï‰. From this ï¬gure, we see that the response is largest for small
Ï‰ and decreases as Ï‰ increases.
This is an example of a low-frequency ï¬lter because relatively more signal passes through
at lower frequencies than at higher frequencies. To understand this, let us drive the system
with a forcing function that has the Fourier transform F(Ï‰). The response of the system
will be G(Ï‰, 0)F(Ï‰). Thus, that portion of the forcing functionâ€™s spectrum at the lower
frequencies is relatively unaï¬€ected because |G(Ï‰, 0)| is near unity.
However, at higher
frequencies where |G(Ï‰, 0)| is smaller, the magnitude of the output is greatly reduced.
âŠ“âŠ”
â€¢ Example 5.2.6
During his study of tumor growth, Adam7 found the particular solution to an ordinary
diï¬€erential equation which, in its simplest form, is
yâ€²â€² âˆ’Î±2y =

|x|/L âˆ’1,
|x| < L,
0,
|x| > L,
(5.2.36)
by the method of Greenâ€™s functions. Let us retrace his steps and see how he did it.
The ï¬rst step is ï¬nding the Greenâ€™s function. We do this by solving
gâ€²â€² âˆ’Î±2g = Î´(x),
(5.2.37)
subject to the boundary conditions lim|x|â†’âˆg(x) â†’0. Taking the Fourier transform of
Equation 5.2.37, we obtain
G(Ï‰) = âˆ’
1
Ï‰2 + Î±2 .
(5.2.38)
7 Adam, J. A., 1986: A simpliï¬ed mathematical model of tumor growth. Math. Biosci., 81, 229â€“244.

Greenâ€™s Functions
231
The function G(Ï‰) is the frequency response for our problem. Straightforward inversion
yields the Greenâ€™s function
g(x) = âˆ’eâˆ’Î±|x|
2Î±
.
(5.2.39)
Therefore, by the convolution integral, y(x) = g(x) âˆ—f(x),
y(x) =
Z L
âˆ’L
g(x âˆ’Î¾) (|Î¾|/L âˆ’1) dÎ¾ = 1
2Î±
Z L
âˆ’L
(1 âˆ’|Î¾|/L) eâˆ’Î±|xâˆ’Î¾| dÎ¾.
(5.2.40)
To evaluate Equation 5.2.40 we must consider four separate cases: âˆ’âˆ< x < âˆ’L,
âˆ’L < x < 0, 0 < x < L, and L < x < âˆ. Turning to the âˆ’âˆ< x < âˆ’L case ï¬rst, we have
y(x) = 1
2Î±
Z L
âˆ’L
(1 âˆ’|Î¾|/L) eÎ±(xâˆ’Î¾) dÎ¾
(5.2.41)
= eÎ±x
2Î±
Z 0
âˆ’L
(1 + Î¾/L) eâˆ’Î±Î¾ dÎ¾ + eÎ±x
2Î±
Z L
0
(1 âˆ’Î¾/L) eâˆ’Î±Î¾ dÎ¾
(5.2.42)
= eÎ±x
2Î±3L
 eÎ±L + eâˆ’Î±L âˆ’2

.
(5.2.43)
Similarly, for x > L,
y(x) = 1
2Î±
Z L
âˆ’L
(1 âˆ’|Î¾|/L) eâˆ’Î±(xâˆ’Î¾) dÎ¾
(5.2.44)
= eâˆ’Î±x
2Î±
Z 0
âˆ’L
(1 + Î¾/L) eÎ±Î¾ dÎ¾ + eâˆ’Î±x
2Î±
Z L
0
(1 âˆ’Î¾/L) eÎ±Î¾ dÎ¾
(5.2.45)
= eâˆ’Î±x
2Î±3L
 eÎ±L + eâˆ’Î±L âˆ’2

.
(5.2.46)
On the other hand, for âˆ’L < x < 0, we ï¬nd that
y(x) = 1
2Î±
Z x
âˆ’L
(1 âˆ’|Î¾|/L) eâˆ’Î±(xâˆ’Î¾) dÎ¾ + 1
2Î±
Z L
x
(1 âˆ’|Î¾|/L) eÎ±(xâˆ’Î¾) dÎ¾
(5.2.47)
= eâˆ’Î±x
2Î±
Z x
âˆ’L
(1 + Î¾/L) eÎ±Î¾ dÎ¾ + eÎ±x
2Î±
Z 0
x
(1 + Î¾/L) eâˆ’Î±Î¾ dÎ¾ + eÎ±x
2Î±
Z L
0
(1 âˆ’Î¾/L) eâˆ’Î±Î¾ dÎ¾
(5.2.48)
=
1
Î±3L

eâˆ’Î±L cosh(Î±x) + Î±(x + L) âˆ’eÎ±x 
.
(5.2.49)
Finally, for 0 < x < L, we have that
y(x) = 1
2Î±
Z x
âˆ’L
(1 âˆ’|Î¾|/L) eâˆ’Î±(xâˆ’Î¾) dÎ¾ + 1
2Î±
Z L
x
(1 âˆ’|Î¾|/L) eÎ±(xâˆ’Î¾) dÎ¾
(5.2.50)
= eâˆ’Î±x
2Î±
Z 0
âˆ’L
(1 + Î¾/L) eÎ±Î¾ dÎ¾ + eâˆ’Î±x
2Î±
Z x
0
(1 âˆ’Î¾/L) eÎ±Î¾ dÎ¾ + eÎ±x
2Î±
Z L
x
(1 âˆ’Î¾/L) eâˆ’Î±Î¾ dÎ¾
(5.2.51)
=
1
Î±3L

eâˆ’Î±L cosh(Î±x) + Î±(L âˆ’x) âˆ’eâˆ’Î±x 
.
(5.2.52)

232
Advanced Engineering Mathematics: A Second Course
These results can be collapsed down into
y(x) =
1
Î±3L
h
eâˆ’Î±L cosh(Î±x) + Î±(L âˆ’|x|) âˆ’eâˆ’Î±|x| i
(5.2.53)
if |x| < L, and
y(x) = eâˆ’Î±|x|
2Î±3L
 eÎ±L + eâˆ’Î±L âˆ’2

(5.2.54)
if |x| > L.
âŠ“âŠ”
Superposition integral
So far we showed how the response of any system can be expressed in terms of its
Greenâ€™s function and the arbitrary forcing. Can we also determine the response using the
indicial admittance a(t)?
Consider ï¬rst a system that is dormant until a certain time t = Ï„1. At that instant we
subject the system to a forcing H(t âˆ’Ï„1). Then the response will be zero if t < Ï„1 and will
equal the indicial admittance a(t âˆ’Ï„1) when t > Ï„1 because the indicial admittance is the
response of a system to the step function. Here tâˆ’Ï„1 is the time measured from the instant
of change.
Next, suppose that we now force the system with the value f(0) when t = 0 and hold
that value until t = Ï„1. We then abruptly change the forcing by an amount f(Ï„1) âˆ’f(0)
to the value f(Ï„1) at the time Ï„1 and hold it at that value until t = Ï„2. Then we again
abruptly change the forcing by an amount f(Ï„2) âˆ’f(Ï„1) at the time Ï„2, and so forth (see
Figure 5.2.5). From the linearity of the problem, the response after the instant t = Ï„n equals
the sum
y(t) = f(0)a(t) + [f(Ï„1) âˆ’f(0)]a(t âˆ’Ï„1) + [f(Ï„2) âˆ’f(Ï„1)]a(t âˆ’Ï„2)
+ Â· Â· Â· + [f(Ï„n) âˆ’f(Ï„nâˆ’1)]a(t âˆ’Ï„n).
(5.2.55)
If we write f(Ï„k) âˆ’f(Ï„kâˆ’1) = âˆ†fk and Ï„k âˆ’Ï„kâˆ’1 = âˆ†Ï„k, Equation 5.2.55 becomes
y(t) = f(0)a(t) +
n
X
k=1
a(t âˆ’Ï„k)âˆ†fk
âˆ†Ï„k
âˆ†Ï„k.
(5.2.56)
Finally, proceeding to the limit as the number n of jumps becomes inï¬nite, in such a manner
that all jumps and intervals between successive jumps tend to zero, this sum has the limit
y(t) = f(0)a(t) +
Z t
0
f â€²(Ï„)a(t âˆ’Ï„) dÏ„.
(5.2.57)
Because the total response of the system equals the weighted sum (the weights being a(t))
of the forcing from the initial moment up to the time t, we refer to Equation 5.2.57 as
the superposition integral, or Duhamelâ€™s integral,8 named after the French mathematical
8 Duhamel, J.-M.-C., 1833: MÂ´emoire sur la mÂ´ethode gÂ´enÂ´erale relative au mouvement de la chaleur dans
les corps solides plongÂ´es dans des milieux dont la tempÂ´erature varie avec le temps. J. Â´Ecole Polytech., 22,
20â€“77.

Greenâ€™s Functions
233
t
Ï„
Ï„
Ï„
1
2
3
âˆ†Ï„
f(t)
Ï„n
Figure 5.2.5: Diagram used in the derivation of Duhamelâ€™s integral.
physicist Jean-Marie-Constant Duhamel (1797â€“1872), who ï¬rst derived it in conjunction
with heat conduction.
We can also express Equation 5.2.57 in several diï¬€erent forms. Integration by parts
yields
y(t) = f(t)a(0) +
Z t
0
f(Ï„)aâ€²(t âˆ’Ï„) dÏ„ = d
dt
Z t
0
f(Ï„)a(t âˆ’Ï„) dÏ„

.
(5.2.58)
â€¢ Example 5.2.7
Suppose that a system has the step response of a(t) = A[1 âˆ’eâˆ’t/T ], where A and T
are positive constants. Let us ï¬nd the response if we force this system by f(t) = kt, where
k is a constant.
From the superposition integral, Equation 5.2.57,
y(t) = 0 +
Z t
0
kA[1 âˆ’eâˆ’(tâˆ’Ï„)/T ] dÏ„ = kA[t âˆ’T(1 âˆ’eâˆ’t/T )].
(5.2.59)
âŠ“âŠ”
Boundary-value problem
One of the purposes of this book is the solution of a wide class of nonhomogeneous
ordinary diï¬€erential equations of the form
d
dx

p(x)dy
dx

+ s(x)y = âˆ’f(x),
a â‰¤x â‰¤b,
(5.2.60)
with
Î±1y(a) + Î±2yâ€²(a) = 0,
Î²1y(b) + Î²2yâ€²(b) = 0.
(5.2.61)
This is an example of a Sturm-Liouville-like equation
d
dx

p(x)dy
dx

+ [q(x) + Î»r(x)]y = âˆ’f(x),
a â‰¤x â‰¤b,
(5.2.62)

234
Advanced Engineering Mathematics: A Second Course
where Î» is a parameter. Here we wish to develop the Greenâ€™s function for this class of
boundary-value problems.
We begin by determining the Greenâ€™s function for the equation
d
dx

p(x)dg
dx

+ s(x)g = âˆ’Î´(x âˆ’Î¾),
(5.2.63)
subject to yet undetermined boundary conditions. We know that such a function exists for
the special case p(x) = 1 and s(x) = 0, and we now show that this is almost always true
in the general case. Presently we construct Greenâ€™s functions by requiring that they satisfy
the following conditions:
â€¢ g(x|Î¾) satisï¬es the homogeneous equation f(x) = 0 except at x = Î¾,
â€¢ g(x|Î¾) satisï¬es certain homogeneous conditions, and
â€¢ g(x|Î¾) is continuous at x = Î¾.
These homogeneous boundary conditions for a ï¬nite interval (a, b) will be
Î±1g(a|Î¾) + Î±2gâ€²(a|Î¾) = 0,
Î²1g(b|Î¾) + Î²2gâ€²(b|Î¾) = 0,
(5.2.64)
where gâ€² denotes the x derivative of g(x|Î¾) and neither a nor b equals Î¾. The coeï¬ƒcients
Î±1 and Î±2 cannot both be zero; this also holds for Î²1 and Î²2. These conditions include the
commonly encountered Dirichlet, Neumann, and Robin boundary conditions.
What about the value of gâ€²(x|Î¾) at x = Î¾? Because g(x|Î¾) is a continuous function of
x, Equation 5.2.63 dictates that there must be a discontinuity in gâ€²(x|Î¾) at x = Î¾. We now
show that this discontinuity consists of a jump in the value gâ€²(x|Î¾) at x = Î¾. To prove this,
we begin by integrating Equation 5.2.63 from Î¾ âˆ’Ç« to Î¾ + Ç«, which yields
p(x)dg(x|Î¾)
dx

Î¾+Ç«
Î¾âˆ’Ç«
+
Z Î¾+Ç«
Î¾âˆ’Ç«
s(x)g(x|Î¾) dx = âˆ’1.
(5.2.65)
Because g(x|Î¾) and s(x) are both continuous at x = Î¾,
lim
Ç«â†’0
Z Î¾+Ç«
Î¾âˆ’Ç«
s(x)g(x|Î¾) dx = 0.
(5.2.66)
Applying the limit Ç« â†’0 to Equation 5.2.65, we have that
p(Î¾)
dg(Î¾+|Î¾)
dx
âˆ’dg(Î¾âˆ’|Î¾)
dx

= âˆ’1,
(5.2.67)
where Î¾+ and Î¾âˆ’denote points just above and below x = Î¾, respectively. Consequently,
our last requirement on g(x|Î¾) will be that
â€¢ dg/dx must have a jump discontinuity of magnitude âˆ’1/p(Î¾) at x = Î¾.

Greenâ€™s Functions
235
Similar conditions hold for higher-order ordinary diï¬€erential equations.9
Consider now the region a â‰¤x < Î¾. Let y1(x) be a nontrivial solution of the homo-
geneous diï¬€erential equation satisfying the boundary condition at x = a; then Î±1y1(a) +
Î±2yâ€²
1(a) = 0. Because g(x|Î¾) must satisfy the same boundary condition, Î±1g(a|Î¾)+Î±2gâ€²(a|Î¾)
= 0. Since the set Î±1, Î±2 is nontrivial, then the Wronskian of y1 and g must vanish at x = a
or y1(a)gâ€²(a|Î¾) âˆ’yâ€²
1(a)g(a|Î¾) = 0. However, for a â‰¤x < Î¾, both y1(x) and g(x|Î¾) satisfy
the same diï¬€erential equation, the homogeneous one. Therefore, their Wronskian is zero
at all points and g(x|Î¾) = c1y1(x) for a â‰¤x < Î¾, where c1 is an arbitrary constant. In
a similar manner, if the nontrivial function y2(x) satisï¬es the homogeneous equation and
the boundary conditions at x = b, then g(x|Î¾) = c2y2(x) for Î¾ < x â‰¤b. The continuity
condition of g and the jump discontinuity of gâ€² at x = Î¾ imply
c1y1(Î¾) âˆ’c2y2(Î¾) = 0,
c1yâ€²
1(Î¾) âˆ’c2yâ€²
2(Î¾) = 1/p(Î¾).
(5.2.68)
We can solve Equation 5.2.68 for c1 and c2 provided the Wronskian of y1 and y2 does not
vanish at x = Î¾, or
y1(Î¾)yâ€²
2(Î¾) âˆ’y2(Î¾)yâ€²
1(Î¾) Ì¸= 0.
(5.2.69)
In other words, y1(x) must not be a multiple of y2(x). Is this always true? The answer is
â€œgenerally yes.â€ If the homogeneous equation admits no nontrivial solutions satisfying both
boundary conditions at the same time,10 then y1(x) and y2(x) must be linearly independent.
On the other hand, if the homogeneous equation possesses a single solution, say y0(x), which
also satisï¬es Î±1y0(a)+ Î±2yâ€²
0(a) = 0 and Î²1y0(b)+ Î²2yâ€²
0(b) = 0, then y1(x) will be a multiple
of y0(x) and so is y2(x). Then they are multiples of each other and their Wronskian vanishes.
This would occur, for example, if the diï¬€erential equation is a Sturm-Liouville equation, Î»
equals the eigenvalue, and y0(x) is the corresponding eigenfunction. No Greenâ€™s function
exists in this case.
â€¢ Example 5.2.8
Consider the problem of ï¬nding the Greenâ€™s function for gâ€²â€² +k2g = âˆ’Î´(xâˆ’Î¾), 0 < x <
L, subject to the boundary conditions g(0|Î¾) = g(L|Î¾) = 0 with k Ì¸= 0. The corresponding
homogeneous equation is yâ€²â€² + k2y = 0. Consequently, g(x|Î¾) = c1y1(x) = c1 sin(kx) for
0 â‰¤x â‰¤Î¾, while g(x|Î¾) = c2y2(x) = c2 sin[k(L âˆ’x)] for Î¾ â‰¤x â‰¤L.
Let us compute the Wronskian. For our particular problem,
W(x) = y1(x)yâ€²
2(x) âˆ’yâ€²
1(x)y2(x)
(5.2.70)
= âˆ’k sin(kx) cos[k(L âˆ’x)] âˆ’k cos(kx) sin[k(L âˆ’x)]
(5.2.71)
= âˆ’k sin[k(x + L âˆ’x)] = âˆ’k sin(kL),
(5.2.72)
and W(Î¾) = âˆ’k sin(kL). Therefore, the Greenâ€™s function will exist as long as kL Ì¸= nÏ€. If
kL = nÏ€, y1(x) and y2(x) are linearly dependent with y0(x) = c3 sin(nÏ€x/L), the solution
to the regular Sturm-Liouville problem yâ€²â€² + Î»y = 0, and y(0) = y(L) = 0.
âŠ“âŠ”
9 Ince, E. L., 1956: Ordinary Diï¬€erential Equations. Dover Publications, Inc. See Section 11.1.
10 In the theory of diï¬€erential equations, this system would be called incompatible: one that admits no
solution, save y = 0, which is also continuous for all x in the interval (a, b) and satisï¬es the homogeneous
boundary conditions.

236
Advanced Engineering Mathematics: A Second Course
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
âˆ’0.2
âˆ’0.15
âˆ’0.1
âˆ’0.05
0
0.05
0.1
0.15
0.2
 x/L
 Î¾/L
 L g(x|Î¾)
Figure 5.2.6: The Greenâ€™s function, Equation 5.2.75, divided by L, as functions of x and Î¾ when kL = 10.
Let us now proceed to ï¬nd g(x|Î¾) when it does exist. The system, Equation 5.2.68, has
the unique solution
c1 = âˆ’
y2(Î¾)
p(Î¾)W(Î¾),
and
c2 = âˆ’
y1(Î¾)
p(Î¾)W(Î¾),
(5.2.73)
where W(Î¾) is the Wronskian of y1(x) and y2(x) at x = Î¾. Therefore,
g(x|Î¾) = âˆ’y1(x<)y2(x>)
p(Î¾)W(Î¾)
.
(5.2.74)
Clearly g(x|Î¾) is symmetric in x and Î¾. It is also unique. The proof of the uniqueness is as
follows: We can always choose a diï¬€erent y1(x), but it will be a multiple of the â€œoldâ€ y1(x),
and the Wronskian will be multiplied by the same factor, leaving g(x|Î¾) the same. This is
also true if we modify y2(x) in a similar manner.
â€¢ Example 5.2.9
Let us ï¬nd the Greenâ€™s function for gâ€²â€² + k2g = âˆ’Î´(x âˆ’Î¾), 0 < x < L, subject to
the boundary conditions g(0|Î¾) = g(L|Î¾) = 0.
As we showed in the previous example,
y1(x) = c1 sin(kx), y2(x) = c2 sin[k(L âˆ’x)], and W(Î¾) = âˆ’k sin(kL). Substituting into
Equation 5.2.74, we have that
g(x|Î¾) = sin(kx<) sin[k(L âˆ’x>)]
k sin(kL)
,
(5.2.75)
where x< = min(x, Î¾) and x> = max(x, Î¾). Figure 5.2.6 illustrates Equation 5.2.75.
âŠ“âŠ”
So far, we showed that the Greenâ€™s function for Equation 5.2.63 exists, is symmetric,
and enjoys certain properties (see the material in the boxes after Equation 5.2.63 and
Equation 5.2.67). But how does this help us solve Equation 5.2.63? We now prove that
y(x) =
Z b
a
g(x|Î¾)f(Î¾) dÎ¾
(5.2.76)

Greenâ€™s Functions
237
is the solution to the nonhomogeneous diï¬€erential equation, Equation 5.2.63, and the ho-
mogeneous boundary conditions, Equation 5.2.64.
We begin by noting that in Equation 5.2.76 x is a parameter while Î¾ is the dummy
variable. As we perform the integration, we must switch from the form for g(x|Î¾) for Î¾ â‰¤x
to the second form for Î¾ â‰¥x when Î¾ equals x; thus,
y(x) =
Z x
a
g(x|Î¾)f(Î¾) dÎ¾ +
Z b
x
g(x|Î¾)f(Î¾) dÎ¾.
(5.2.77)
Diï¬€erentiation yields
d
dx
Z x
a
g(x|Î¾)f(Î¾) dÎ¾ =
Z x
a
dg(x|Î¾)
dx
f(Î¾) dÎ¾ + g(x|xâˆ’)f(x),
(5.2.78)
and
d
dx
Z b
x
g(x|Î¾)f(Î¾) dÎ¾ =
Z b
x
dg(x|Î¾)
dx
f(Î¾) dÎ¾ âˆ’g(x|x+)f(x).
(5.2.79)
Because g(x|Î¾) is continuous everywhere, we have that g(x|x+) = g(x|xâˆ’) so that
dy
dx =
Z x
a
dg(x|Î¾)
dx
f(Î¾) dÎ¾ +
Z b
x
dg(x|Î¾)
dx
f(Î¾) dÎ¾.
(5.2.80)
Diï¬€erentiating once more gives
d2y
dx2 =
Z x
a
d2g(x|Î¾)
dx2
f(Î¾) dÎ¾+dg(x|xâˆ’)
dx
f(x)+
Z b
x
d2g(x|Î¾)
dx2
f(Î¾) dÎ¾âˆ’dg(x|x+)
dx
f(x). (5.2.81)
The second and fourth terms on the right side of Equation 5.2.81 will not cancel in this
case; on the contrary,
dg(x|xâˆ’)
dx
âˆ’dg(x|x+)
dx
= âˆ’1
p(x).
(5.2.82)
To show this, we note that the term dg(x|xâˆ’)/dx denotes a diï¬€erentiation of g(x|Î¾) with
respect to x using the x > Î¾ form and then letting Î¾ â†’x. Thus,
dg(x|xâˆ’)
dx
= âˆ’lim
Î¾â†’x
Î¾<x
yâ€²
2(x)y1(Î¾)
p(Î¾)W(Î¾) = âˆ’yâ€²
2(x)y1(x)
p(x)W(x) ,
(5.2.83)
while for dg(x|x+)/dx we use the x < Î¾ form or
dg(x|x+)
dx
= âˆ’lim
Î¾â†’x
Î¾>x
yâ€²
1(x)y2(Î¾)
p(Î¾)W(Î¾) = âˆ’yâ€²
1(x)y2(x)
p(x)W(x) .
(5.2.84)
Upon introducing these results into the diï¬€erential equation
p(x)d2y
dx2 + pâ€²(x)dy
dx + s(x)y = âˆ’f(x),
(5.2.85)
we have
Z x
a
[p(x)gâ€²â€²(x|Î¾) + pâ€²(x)gâ€²(x|Î¾) + s(x)g(x|Î¾)]f(Î¾) dÎ¾
(5.2.86)
+
Z b
x
[p(x)gâ€²â€²(x|Î¾) + pâ€²(x)gâ€²(x|Î¾) + s(x)g(x|Î¾)]f(Î¾) dÎ¾ âˆ’p(x)f(x)
p(x) = âˆ’f(x).

238
Advanced Engineering Mathematics: A Second Course
Because
p(x)gâ€²â€²(x|Î¾) + pâ€²(x)gâ€²(x|Î¾) + s(x)g(x|Î¾) = 0,
(5.2.87)
except for x = Î¾, Equation 5.2.86, and thus Equation 5.2.63, is satisï¬ed. Although Equation
5.2.87 does not hold at the point x = Î¾, the results are still valid because that one point
does not aï¬€ect the values of the integrals. As for the boundary conditions,
y(a) =
Z b
a
g(a|Î¾)f(Î¾) dÎ¾,
yâ€²(a) =
Z b
a
dg(a|Î¾)
dx
f(Î¾) dÎ¾,
(5.2.88)
and Î±1y(a) + Î±2yâ€²(a) = 0 from Equation 5.2.64. A similar proof holds for x = b.
Finally, let us consider the solution for the nonhomogeneous boundary conditions
Î±1y(a) + Î±2yâ€²(a) = Î±, and Î²1y(b) + Î²2yâ€²(b) = Î². The solution in this case is
y(x) =
Î±y2(x)
Î±1y2(a) + Î±2yâ€²
2(a) +
Î²y1(x)
Î²1y1(b) + Î²2yâ€²
1(b) +
Z b
a
g(x|Î¾)f(Î¾) dÎ¾.
(5.2.89)
A quick check shows that Equation 5.2.89 satisï¬es the diï¬€erential equation and both non-
homogeneous boundary conditions.
Eigenfunction expansion
We just showed how Greenâ€™s functions can be used to solve the nonhomogeneous linear
diï¬€erential equation. The next question is how do you ï¬nd the Greenâ€™s function? Here we
present the most common method: series expansion. This is not surprising given its success
in solving the Sturm-Liouville problem.
Consider the nonhomogeneous problem
yâ€²â€² = âˆ’f(x),
with
y(0) = y(L) = 0.
(5.2.90)
The Greenâ€™s function g(x|Î¾) must therefore satisfy
gâ€²â€² = âˆ’Î´(x âˆ’Î¾),
with
g(0|Î¾) = g(L|Î¾) = 0.
(5.2.91)
Because g(x|Î¾) vanishes at the ends of the interval (0, L), this suggests that it can be
expanded in a series of suitably chosen orthogonal functions such as, for instance, the
Fourier sine series
g(x|Î¾) =
âˆ
X
n=1
Gn(Î¾) sin
nÏ€x
L

,
(5.2.92)
where the expansion coeï¬ƒcients Gn are dependent on the parameter Î¾. Although we chose
the orthogonal set of functions sin(nÏ€x/L), we could have used other orthogonal functions
as long as they vanish at the endpoints.

Greenâ€™s Functions
239
Because
gâ€²â€²(x|Î¾) =
âˆ
X
n=1

âˆ’n2Ï€2
L2

Gn(Î¾) sin
nÏ€x
L

,
(5.2.93)
and
Î´(x âˆ’Î¾) =
âˆ
X
n=1
An(Î¾) sin
nÏ€x
L

,
(5.2.94)
where
An(Î¾) = 2
L
Z L
0
Î´(x âˆ’Î¾) sin
nÏ€x
L

dx = 2
L sin
nÏ€Î¾
L

,
(5.2.95)
we have that
âˆ’
âˆ
X
n=1
n2Ï€2
L2

Gn(Î¾) sin
nÏ€x
L

= âˆ’2
L
âˆ
X
n=1
sin
nÏ€Î¾
L

sin
nÏ€x
L

,
(5.2.96)
after substituting Equation 5.2.93 through Equation 5.2.95 into the diï¬€erential equation,
Equation 5.2.91. Since Equation 5.2.96 must hold for any arbitrary x,
n2Ï€2
L2

Gn(Î¾) = 2
L sin
nÏ€Î¾
L

.
(5.2.97)
Thus, the Greenâ€™s function is
g(x|Î¾) = 2L
Ï€2
âˆ
X
n=1
1
n2 sin
nÏ€Î¾
L

sin
nÏ€x
L

.
(5.2.98)
How might we use Equation 5.2.98? We can use this series to construct the solution of
the nonhomogeneous equation, Equation 5.2.90, via the formula
y(x) =
Z L
0
g(x|Î¾) f(Î¾) dÎ¾.
(5.2.99)
This leads to
y(x) = 2L
Ï€2
âˆ
X
n=1
1
n2 sin
nÏ€x
L
 Z L
0
f(Î¾) sin
nÏ€Î¾
L

dÎ¾,
(5.2.100)
or
y(x) = L2
Ï€2
âˆ
X
n=1
an
n2 sin
nÏ€x
L

,
(5.2.101)
where an are the Fourier sine coeï¬ƒcients of f(x).
â€¢ Example 5.2.10
Consider now the more complicated boundary-value problem
yâ€²â€² + k2y = âˆ’f(x),
with
y(0) = y(L) = 0.
(5.2.102)
The Greenâ€™s function g(x|Î¾) must now satisfy
gâ€²â€² + k2g = âˆ’Î´(x âˆ’Î¾),
and
g(0|Î¾) = g(L|Î¾) = 0.
(5.2.103)

240
Advanced Engineering Mathematics: A Second Course
Once again, we use the Fourier sine expansion
g(x|Î¾) =
âˆ
X
n=1
Gn(Î¾) sin
nÏ€x
L

.
(5.2.104)
Direct substitution of Equation 5.2.104 and Equation 5.2.94 into Equation 5.2.103 and
grouping by corresponding harmonics yields
âˆ’n2Ï€2
L2 Gn(Î¾) + k2Gn(Î¾) = âˆ’2
L sin
nÏ€Î¾
L

,
(5.2.105)
or
Gn(Î¾) = 2
L
sin(nÏ€Î¾/L)
n2Ï€2/L2 âˆ’k2 .
(5.2.106)
Thus, the Greenâ€™s function is
g(x|Î¾) = 2
L
âˆ
X
n=1
sin(nÏ€Î¾/L) sin(nÏ€x/L)
n2Ï€2/L2 âˆ’k2
.
(5.2.107)
Examining Equation 5.2.107 more closely, we note that it enjoys the symmetry property
that g(x|Î¾) = g(Î¾|x).
âŠ“âŠ”
â€¢ Example 5.2.11
Let us ï¬nd the series expansion for the Greenâ€™s function for
xgâ€²â€² + gâ€² +

k2x âˆ’m2
x

g = âˆ’Î´(x âˆ’Î¾),
0 < x < L,
(5.2.108)
where m â‰¥0 and is an integer. The boundary conditions are
lim
xâ†’0 |g(x|Î¾)| < âˆ,
and
g(L|Î¾) = 0.
(5.2.109)
To ï¬nd this series, consider the Fourier-Bessel series
g(x|Î¾) =
âˆ
X
n=1
Gn(Î¾)Jm(knmx),
(5.2.110)
where knm is the nth root of Jm(knmL) = 0. This series enjoys the advantage that it satisï¬es
the boundary conditions and we will not have to introduce any homogeneous solutions so
that g(x|Î¾) satisï¬es the boundary conditions.
Substituting Equation 5.2.110 into Equation 5.2.108 after we divide by x and using the
Fourier-Bessel expansion for the delta function, we have that
(k2 âˆ’k2
nm)Gn(Î¾) = âˆ’2k2
nmJm(knmÎ¾)
L2[Jm+1(knmL)]2 = âˆ’
2Jm(knmÎ¾)
L2[Jâ€²m(knmL)]2 ,
(5.2.111)
so that
g(x|Î¾) = 2
L2
âˆ
X
n=1
Jm(knmÎ¾)Jm(knmx)
(k2nm âˆ’k2)[Jâ€²m(knmL)]2 .
(5.2.112)

Greenâ€™s Functions
241
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
âˆ’2
âˆ’1
0
1
2
3
4
 x/L
 Î¾/L
 L g(x|Î¾)
Figure 5.2.7: The Greenâ€™s function, Equation 5.2.112, as functions of x/L and Î¾/L when kL = 10 and
m = 1.
Equation 5.2.112 is plotted in Figure 5.2.7.
âŠ“âŠ”
We summarize the expansion technique as follows: Suppose that we want to solve the
diï¬€erential equation
Ly(x) = âˆ’f(x),
(5.2.113)
with some condition By(x) = 0 along the boundary, where L now denotes the Sturm-
Liouville diï¬€erential operator
L = d
dx

p(x) d
dx

+ [q(x) + Î»r(x)],
(5.2.114)
and B is the boundary condition operator
B =
ï£±
ï£´
ï£´
ï£²
ï£´
ï£´
ï£³
Î±1 + Î±2
d
dx,
at x = a,
Î²1 + Î²2
d
dx,
at x = b.
(5.2.115)
We begin by seeking a Greenâ€™s function g(x|Î¾), which satisï¬es
Lg = âˆ’Î´(x âˆ’Î¾),
Bg = 0.
(5.2.116)
To ï¬nd the Greenâ€™s function, we utilize the set of eigenfunctions Ï•n(x) associated with the
regular Sturm-Liouville problem
d
dx

p(x)dÏ•n
dx

+ [q(x) + Î»nr(x)]Ï•n = 0,
(5.2.117)

242
Advanced Engineering Mathematics: A Second Course
where Ï•n(x) satisï¬es the same boundary conditions as y(x). If g exists and if the set {Ï•n}
is complete, then g(x|Î¾) can be represented by the series
g(x|Î¾) =
âˆ
X
n=1
Gn(Î¾)Ï•n(x).
(5.2.118)
Applying L to Equation 5.2.118,
Lg(x|Î¾) =
âˆ
X
n=1
Gn(Î¾)L[Ï•n(x)] =
âˆ
X
n=1
Gn(Î¾)(Î» âˆ’Î»n)r(x)Ï•n(x) = âˆ’Î´(x âˆ’Î¾),
(5.2.119)
if Î» does not equal any of the eigenvalues Î»n. Multiplying both sides of Equation 5.2.119
by Ï•m(x) and integrating over x,
âˆ
X
n=1
Gn(Î¾)(Î» âˆ’Î»n)
Z b
a
r(x)Ï•n(x)Ï•m(x) dx = âˆ’Ï•m(Î¾).
(5.2.120)
If the eigenfunctions are orthonormal,
Z b
a
r(x)Ï•n(x)Ï•m(x) dx =
 1,
n = m,
0,
n Ì¸= m,
and
Gn(Î¾) = Ï•n(Î¾)
Î»n âˆ’Î».
(5.2.121)
This leads directly to the bilinear formula:
g(x|Î¾) =
âˆ
X
n=1
Ï•n(Î¾)Ï•n(x)
Î»n âˆ’Î»
,
(5.2.122)
which permits us to write the Greenâ€™s function at once if the eigenvalues and eigenfunctions
of L are known.
Problems
For the following initial-value problems, ï¬nd the transfer function, impulse response, Greenâ€™s
function, and step response. Assume that all of the necessary initial conditions are zero and
Ï„ > 0. If you have MATLABâ€™s control toolbox, use MATLAB to check your work.
1. gâ€² + kg = Î´(t âˆ’Ï„)
2. gâ€²â€² âˆ’2gâ€² âˆ’3g = Î´(t âˆ’Ï„)
3. gâ€²â€² + 4gâ€² + 3g = Î´(t âˆ’Ï„)
4. gâ€²â€² âˆ’2gâ€² + 5g = Î´(t âˆ’Ï„)
5. gâ€²â€² âˆ’3gâ€² + 2g = Î´(t âˆ’Ï„)
6. gâ€²â€² + 4gâ€² + 4g = Î´(t âˆ’Ï„)
7. gâ€²â€² âˆ’9g = Î´(t âˆ’Ï„)
8. gâ€²â€² + g = Î´(t âˆ’Ï„)
9. gâ€²â€² âˆ’gâ€² = Î´(t âˆ’Ï„)
Find the Greenâ€™s function and the corresponding bilinear expansion using eigenfunctions
from the regular Sturm-Liouville problem Ï•â€²â€²
n + k2
nÏ•n = 0 for
gâ€²â€² = âˆ’Î´(x âˆ’Î¾),
0 < x, Î¾ < L,

Greenâ€™s Functions
243
which satisfy the following boundary conditions:
10. g(0|Î¾) âˆ’Î±gâ€²(0|Î¾) = 0, Î± Ì¸= 0, âˆ’L,
g(L|Î¾) = 0,
11. g(0|Î¾) âˆ’gâ€²(0|Î¾) = 0,
g(L|Î¾) âˆ’gâ€²(L|Î¾) = 0,
12. g(0|Î¾) âˆ’gâ€²(0|Î¾) = 0,
g(L|Î¾) + gâ€²(L|Î¾) = 0.
Find the Greenâ€™s function11 and the corresponding bilinear expansion using eigenfunctions
from the regular Sturm-Liouville problem Ï•â€²â€²
n + k2
nÏ•n = 0 for
gâ€²â€² âˆ’k2g = âˆ’Î´(x âˆ’Î¾),
0 < x, Î¾ < L,
which satisfy the following boundary conditions:
13. g(0|Î¾) = 0,
g(L|Î¾) = 0,
14. gâ€²(0|Î¾) = 0,
gâ€²(L|Î¾) = 0,
15. g(0|Î¾) = 0,
g(L|Î¾) + gâ€²(L|Î¾) = 0,
16. g(0|Î¾) = 0,
g(L|Î¾) âˆ’gâ€²(L|Î¾) = 0,
17. a g(0|Î¾) + gâ€²(0|Î¾) = 0,
gâ€²(L|Î¾) = 0,
18. g(0|Î¾) + gâ€²(0|Î¾) = 0,
g(L|Î¾) âˆ’gâ€²(L|Î¾) = 0.
5.3 JOINT TRANSFORM METHOD
In the previous section an important method for ï¬nding Greenâ€™s function involved either
Laplace or Fourier transforms. In the following sections we wish to ï¬nd Greenâ€™s functions
for partial diï¬€erential equations. Again, transform methods play an important role. We will
always use the Laplace transform to eliminate the temporal dependence. However, for the
spatial dimension we will use either a Fourier series or Fourier transform. Our choice will
be dictated by the domain: If it reaches to inï¬nity, then we will employ Fourier transforms.
On the other hand, a domain of ï¬nite length calls for an eigenfunction expansion. The
following two examples illustrate our solution technique for domains of inï¬nite and ï¬nite
extent.
â€¢ Example 5.3.1: One-dimensional Klein-Gordon equation
The Klein-Gordon equation is a form of the wave equation that arose in particle physics
as the relativistic scalar wave equation describing particles with nonzero rest mass. In this
example, we ï¬nd its Greenâ€™s function when there is only one spatial dimension:
âˆ‚2g
âˆ‚x2 âˆ’1
c2
âˆ‚2g
âˆ‚t2 + a2g

= âˆ’Î´(x âˆ’Î¾)Î´(t âˆ’Ï„),
(5.3.1)
11 Problem 18 was used by Chakrabarti, A., and T. Sahoo, 1996: Reï¬‚ection of water waves by a nearly
vertical porous wall. J. Austral. Math. Soc., Ser. B, 37, 417â€“429.

244
Advanced Engineering Mathematics: A Second Course
where âˆ’âˆ< x, Î¾ < âˆ, 0 < t, Ï„, c is a real, positive constant (the wave speed), and a is a
real, nonnegative constant. The corresponding boundary conditions are
lim
|x|â†’âˆg(x, t|Î¾, Ï„) â†’0,
(5.3.2)
and the initial conditions are
g(x, 0|Î¾, Ï„) = gt(x, 0|Î¾, Ï„) = 0.
(5.3.3)
We begin by taking the Laplace transform of Equation 5.3.1 and ï¬nd that
d2G
dx2 âˆ’
s2 + a2
c2

G = âˆ’Î´(x âˆ’Î¾)eâˆ’sÏ„.
(5.3.4)
Applying Fourier transforms to Equation 5.3.4, we obtain
G(x, s|Î¾, Ï„) = c2
2Ï€ eâˆ’sÏ„
Z âˆ
âˆ’âˆ
eik(xâˆ’Î¾)
s2 + a2 + k2c2 dk = c2
Ï€ eâˆ’sÏ„
Z âˆ
0
cos[k(x âˆ’Î¾)]
s2 + a2 + k2c2 dk.
(5.3.5)
Inverting the Laplace transform and employing the second shifting theorem,
g(x, t|Î¾, Ï„) = c2
Ï€ H(t âˆ’Ï„)
Z âˆ
0
sin

(t âˆ’Ï„)
âˆš
a2 + k2c2 
cos[k(x âˆ’Î¾)]
âˆš
a2 + k2c2
dk.
(5.3.6)
Equation 5.3.6 represents a superposition of homogeneous solutions (normal modes) to
Equation 5.3.1. An intriguing aspect of Equation 5.3.6 is that this solution occurs every-
where after t > Ï„. If |x âˆ’Î¾| > c(t âˆ’Ï„), these wave solutions destructively interfere so that
we have zero there while they constructively interfere at those times and places where the
physical waves are present.
Applying integral tables to Equation 5.3.6, the ï¬nal result is
g(x, t|Î¾, Ï„) = c
2J0
h
a
p
(t âˆ’Ï„)2 âˆ’(x âˆ’Î¾)2/c2
i
H[c(t âˆ’Ï„) âˆ’|x âˆ’Î¾|].
(5.3.7)
Figure 5.3.1 illustrates this Greenâ€™s function. Thus, the Greenâ€™s function for the Klein-
Gordon equation yields waves that propagate to the right and left from x = 0 with the
wave front located at x = Â±ct. At a given point, after the passage of the wave front, the
solution vibrates with an ever-decreasing amplitude and at a frequency that approaches a,
the so-called cutoï¬€frequency, at t â†’âˆ.
Why is a called a cutoï¬€frequency? From Equation 5.3.5, we see that, although the
spectral representation includes all of the wavenumbers k running from âˆ’âˆto âˆ, the
frequency Ï‰ =
âˆš
c2k2 + a2 is restricted to the range Ï‰ â‰¥a from Equation 5.3.6. Thus, a is
the lowest possible frequency that a wave solution to the Klein-Gordon equation may have
for a real value of k.
âŠ“âŠ”

Greenâ€™s Functions
245
âˆ’10
âˆ’5
0
5
10
0
2
4
6
8
10
âˆ’0.4
âˆ’0.2
0
0.2
0.4
0.6
 a(xâˆ’Î¾)/c
 a(tâˆ’Ï„)
 g(x,t|Î¾,Ï„)/c
Figure 5.3.1: The free-space Greenâ€™s function g(x, t|Î¾, Ï„)/c for the one-dimensional Klein-Gordon equation
at diï¬€erent distances a(x âˆ’Î¾)/c and times a(t âˆ’Ï„).
â€¢ Example 5.3.2: One-dimensional wave equation on the interval 0 < x < L
One of the classic problems of mathematical physics involves ï¬nding the displacement
of a taut string between two supports when an external force is applied. The governing
equation is
âˆ‚2u
âˆ‚t2 âˆ’c2 âˆ‚2u
âˆ‚x2 = f(x, t),
0 < x < L,
0 < t,
(5.3.8)
where c is the constant phase speed.
In this example, we ï¬nd the Greenâ€™s function for this problem by considering the
following problem:
âˆ‚2g
âˆ‚t2 âˆ’c2 âˆ‚2g
âˆ‚x2 = Î´(x âˆ’Î¾)Î´(t âˆ’Ï„),
0 < x, Î¾ < L,
0 < t, Ï„,
(5.3.9)
with the boundary conditions
Î±1g(0, t|Î¾, Ï„) + Î²1gx(0, t|Î¾, Ï„) = 0,
0 < t,
(5.3.10)
and
Î±2g(L, t|Î¾, Ï„) + Î²2gx(L, t|Î¾, Ï„) = 0,
0 < t,
(5.3.11)
and the initial conditions
g(x, 0|Î¾, Ï„) = gt(x, 0|Î¾, Ï„) = 0,
0 < x < L.
(5.3.12)
We start by taking the Laplace transform of Equation 5.3.9 and ï¬nd that
d2G
dx2 âˆ’s2
c2 G = âˆ’Î´(x âˆ’Î¾)
c2
eâˆ’sÏ„,
0 < x < L,
(5.3.13)
with
Î±1G(0, s|Î¾, Ï„) + Î²1Gâ€²(0, s|Î¾, Ï„) = 0,
(5.3.14)

246
Advanced Engineering Mathematics: A Second Course
and
Î±2G(L, s|Î¾, Ï„) + Î²2Gâ€²(L, s|Î¾, Ï„) = 0.
(5.3.15)
Problems similar to Equation 5.3.13 through Equation 5.3.15 were considered in the previous
section. There, solutions were developed in terms of an eigenfunction expansion. Applying
the same technique here,
G(x, s|Î¾, Ï„) = eâˆ’sÏ„
âˆ
X
n=1
Ï•n(Î¾)Ï•n(x)
s2 + c2k2n
,
(5.3.16)
where Ï•n(x) is the nth orthonormal eigenfunction to the regular Sturm-Liouville problem
Ï•
â€²â€²(x) + k2Ï•(x) = 0,
0 < x < L,
(5.3.17)
subject to the boundary conditions
Î±1Ï•(0) + Î²1Ï•â€²(0) = 0,
(5.3.18)
and
Î±2Ï•(L) + Î²2Ï•â€²(L) = 0.
(5.3.19)
Taking the inverse of Equation 5.3.16, we have that the Greenâ€™s function is
g(x, t|Î¾, Ï„) =
( âˆ
X
n=1
Ï•n(Î¾)Ï•n(x)sin[knc(t âˆ’Ï„)]
knc
)
H(t âˆ’Ï„).
(5.3.20)
Let us illustrate our results to ï¬nd the Greenâ€™s function for
âˆ‚2g
âˆ‚t2 âˆ’c2 âˆ‚2g
âˆ‚x2 = Î´(x âˆ’Î¾)Î´(t âˆ’Ï„),
(5.3.21)
with the boundary conditions
g(0, t|Î¾, Ï„) = g(L, t|Î¾, Ï„) = 0,
0 < t,
(5.3.22)
and the initial conditions
g(x, 0|Î¾, Ï„) = gt(x, 0|Î¾, Ï„) = 0,
0 < x < L.
(5.3.23)
For this example, the Sturm-Liouville problem is
Ï•â€²â€²(x) + k2Ï•(x) = 0,
0 < x < L,
(5.3.24)
with the boundary conditions Ï•(0) = Ï•(L) = 0. The nth orthonormal eigenfunction for
this problem is
Ï•n(x) =
r
2
L sin
nÏ€x
L

.
(5.3.25)

Greenâ€™s Functions
247
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
âˆ’0.6
âˆ’0.4
âˆ’0.2
0
0.2
0.4
0.6
 x/L
 c(tâˆ’Ï„)/L
 c g(x,t|Î¾,Ï„)
Figure 5.3.2: The Greenâ€™s function cg(x, t|Î¾, Ï„) given by Equation 5.3.26 for the one-dimensional wave
equation over the interval 0 < x < L as a function of location x/L and time c(t âˆ’Ï„)/L with Î¾/L = 0.2.
The boundary conditions are g(0, t|Î¾, Ï„) = g(L, t|Î¾, Ï„) = 0.
Consequently, from Equation 5.3.20, the Greenâ€™s function is
g(x, t|Î¾, Ï„) = 2
Ï€c
( âˆ
X
n=1
1
n sin
nÏ€Î¾
L

sin
nÏ€x
L

sin
nÏ€c(t âˆ’Ï„)
L
)
H(t âˆ’Ï„).
(5.3.26)
See Figure 5.3.2.
5.4 WAVE EQUATION
In Section 5.2, we showed how Greenâ€™s functions could be used to solve initial- and
boundary-value problems involving ordinary diï¬€erential equations. When we approach par-
tial diï¬€erential equations, similar considerations hold, although the complexity increases.
In the next three sections, we work through the classic groupings of the wave, heat, and
Helmholtzâ€™s equations in one spatial dimension. All of these results can be generalized to
three dimensions.
Of these three groups, we start with the wave equation
âˆ‚2u
âˆ‚x2 âˆ’1
c2
âˆ‚2u
âˆ‚t2 = âˆ’q(x, t),
(5.4.1)
where t denotes time, x is the position, c is the phase velocity of the wave, and q(x, t) is
the source density. In addition to Equation 5.4.1 it is necessary to state boundary and
initial conditions to obtain a unique solution. The condition on the boundary can be either
Dirichlet or Neumann or a linear combination of both (Robin condition). The conditions
in time must be Cauchy, that is, we must specify the value of u(x, t) and its time derivative
at t = t0 for each point of the region under consideration.
We begin by proving that we can express the solution to Equation 5.4.1 in terms of
boundary conditions, initial conditions, and the Greenâ€™s function, which is found by solving
âˆ‚2g
âˆ‚x2 âˆ’1
c2
âˆ‚2g
âˆ‚t2 = âˆ’Î´(x âˆ’Î¾)Î´(t âˆ’Ï„),
(5.4.2)

248
Advanced Engineering Mathematics: A Second Course
where Î¾ denotes the position of a source that is excited at t = Ï„. Equation 5.4.2 expresses
the eï¬€ect of an impulse as it propagates from x = Î¾ as time increases from t = Ï„. For
t < Ï„, causality requires that g(x, t|Î¾, Ï„) = gt(x, t|Î¾, Ï„) = 0 if the impulse is the sole source
of the disturbance. We also require that g satisï¬es the homogeneous form of the boundary
condition satisï¬ed by u.
Our derivation starts with the equations
âˆ‚2u(Î¾, Ï„)
âˆ‚Î¾2
âˆ’1
c2
âˆ‚2u(Î¾, Ï„)
âˆ‚Ï„ 2
= âˆ’q(Î¾, Ï„),
(5.4.3)
and
âˆ‚2g(x, t|Î¾, Ï„)
âˆ‚Î¾2
âˆ’1
c2
âˆ‚2g(x, t|Î¾, Ï„)
âˆ‚Ï„ 2
= âˆ’Î´(x âˆ’Î¾)Î´(t âˆ’Ï„),
(5.4.4)
where we obtain Equation 5.4.4 from a combination of Equation 5.4.2 plus reciprocity,
namely g(x, t|Î¾, Ï„) = g(Î¾, âˆ’Ï„|x, âˆ’t). Next we multiply Equation 5.4.3 by g(x, t|Î¾, Ï„) and
Equation 5.4.4 by u(Î¾, Ï„) and subtract. Integrating over Î¾ from a to b, where a and b are
the endpoints of the spatial domain, and over Ï„ from 0 to t+, where t+ denotes a time
slightly later than t so that we avoid ending the integration exactly at the peak of the delta
function, we obtain
Z t+
0
Z b
a

g(x, t|Î¾, Ï„)âˆ‚2u(Î¾, Ï„)
âˆ‚Î¾2
âˆ’u(Î¾, Ï„)âˆ‚2g(x, t|Î¾, Ï„)
âˆ‚Î¾2
+ 1
c2

u(Î¾, Ï„)âˆ‚2g(x, t|Î¾, Ï„)
âˆ‚Ï„ 2
âˆ’g(x, t|Î¾, Ï„)âˆ‚2u(Î¾, Ï„)
âˆ‚Ï„ 2

dÎ¾ dÏ„
= u(x, t) âˆ’
Z t+
0
Z b
a
q(Î¾, Ï„) g(x, t|Î¾, Ï„) dÎ¾ dÏ„.
(5.4.5)
Because
g(x, t|Î¾, Ï„)âˆ‚2u(Î¾, Ï„)
âˆ‚Î¾2
âˆ’u(Î¾, Ï„)âˆ‚2g(x, t|Î¾, Ï„)
âˆ‚Î¾2
= âˆ‚
âˆ‚Î¾

g(x, t|Î¾, Ï„)âˆ‚u(Î¾, Ï„)
âˆ‚Î¾

âˆ’âˆ‚
âˆ‚Î¾

u(Î¾, Ï„)âˆ‚g(x, t|Î¾, Ï„)
âˆ‚Î¾

,
(5.4.6)
and
g(x, t|Î¾, Ï„)âˆ‚2u(Î¾, Ï„)
âˆ‚Ï„ 2
âˆ’u(Î¾, Ï„)âˆ‚2g(x, t|Î¾, Ï„)
âˆ‚Ï„ 2
= âˆ‚
âˆ‚Ï„

g(x, t|Î¾, Ï„)âˆ‚u(Î¾, Ï„)
âˆ‚Ï„

âˆ’âˆ‚
âˆ‚Ï„

u(Î¾, Ï„)âˆ‚g(x, t|Î¾, Ï„)
âˆ‚Ï„

,
(5.4.7)
we ï¬nd that
Z t+
0

g(x, t|Î¾, Ï„)âˆ‚u(Î¾, Ï„)
âˆ‚Î¾
âˆ’u(Î¾, Ï„)âˆ‚g(x, t|Î¾, Ï„)
âˆ‚Î¾
Î¾=b
Î¾=a
dÏ„
+ 1
c2
Z b
a

u(Î¾, Ï„)âˆ‚g(x, t|Î¾, Ï„)
âˆ‚Ï„
âˆ’g(x, t|Î¾, Ï„)âˆ‚u(Î¾, Ï„)
âˆ‚Ï„
Ï„=t+
Ï„=0
dÎ¾
+
Z t+
0
Z b
a
q(Î¾, Ï„) g(x, t|Î¾, Ï„) dÎ¾ dÏ„ = u(x, t).
(5.4.8)

Greenâ€™s Functions
249
The integrand in the ï¬rst integral is speciï¬ed by the boundary conditions. In the second
integral, the integrand vanishes at t = t+ from the initial conditions on g(x, t|Î¾, Ï„). The
limit at t = 0 is determined by the initial conditions. Hence,
u(x, t) =
Z t+
0
Z b
a
q(Î¾, Ï„)g(x, t|Î¾, Ï„) dÎ¾ dÏ„
+
Z t+
0

g(x, t|Î¾, Ï„)âˆ‚u(Î¾, Ï„)
âˆ‚Î¾
âˆ’u(Î¾, Ï„)âˆ‚g(x, t|Î¾, Ï„)
âˆ‚Î¾
Î¾=b
Î¾=a
dÏ„
âˆ’1
c2
Z b
a

u(Î¾, 0)âˆ‚g(x, t|Î¾, 0)
âˆ‚Ï„
âˆ’g(x, t|Î¾, 0)âˆ‚u(Î¾, 0)
âˆ‚Ï„

dÎ¾.
(5.4.9)
Equation 5.4.9 gives the complete solution of the nonhomogeneous problem. The ï¬rst two
integrals on the right side of this equation represent the eï¬€ect of the source and the boundary
conditions, respectively. The last term involves the initial conditions; it can be interpreted
as asking what sort of source is needed so that the function u(x, t) starts in the desired
manner.
â€¢ Example 5.4.1
Let us apply the Greenâ€™s function technique to solve
âˆ‚2u
âˆ‚t2 = âˆ‚2u
âˆ‚x2 ,
0 < x < 1,
0 < t,
(5.4.10)
subject to the boundary conditions u(0, t) = 0 and u(1, t) = t, 0 < t, and the initial
conditions u(x, 0) = x and ut(x, 0) = 0, 0 < x < 1.
Because there is no source term and c = 1, Equation 5.4.9 becomes
u(x, t) =
Z t
0
[g(x, t|1, Ï„)uÎ¾(1, Ï„) âˆ’u(1, Ï„)gÎ¾(x, t|1, Ï„)] dÏ„
âˆ’
Z t
0
[g(x, t|0, Ï„)uÎ¾(0, Ï„) âˆ’u(0, Ï„)gÎ¾(x, t|0, Ï„)] dÏ„
âˆ’
Z 1
0
[u(Î¾, 0)gÏ„(x, t|Î¾, 0) âˆ’g(x, t|Î¾, 0)uÎ¾(Î¾, 0)] dÎ¾.
(5.4.11)
Therefore we must ï¬rst compute the Greenâ€™s function for this problem. However, we have
already done this in Example 5.3.2 and it is given by Equation 5.3.26 with c = L = 1.
Next, we note that g(x, t|1, Ï„) = g(x, t|0, Ï„) = 0 and u(0, Ï„) = uÏ„(Î¾, 0) = 0. Consequently,
Equation 5.4.11 reduces to only two nonvanishing integrals:
u(x, t) = âˆ’
Z t
0
u(1, Ï„)gÎ¾(x, t|1, Ï„) dÏ„ âˆ’
Z 1
0
u(Î¾, 0)gÏ„(x, t|Î¾, 0) dÎ¾.
(5.4.12)
If we now substitute for g(x, t|Î¾, Ï„) and reverse the order of integration and summation,
Z t
0
u(1, Ï„)gÎ¾(x, t|1, Ï„) dÏ„ = 2
âˆ
X
n=1
(âˆ’1)n sin(nÏ€x)
Z t
0
Ï„ sin[nÏ€(t âˆ’Ï„)] dÏ„
(5.4.13)
= 2t
âˆ
X
n=1
(âˆ’1)n sin(nÏ€x)
Z t
0
sin[nÏ€(t âˆ’Ï„)] d(t âˆ’Ï„)

250
Advanced Engineering Mathematics: A Second Course
âˆ’2
âˆ
X
n=1
(âˆ’1)n sin(nÏ€x)
Z t
0
(t âˆ’Ï„) sin[nÏ€(t âˆ’Ï„)] d(t âˆ’Ï„)
(5.4.14)
Z t
0
u(1, Ï„)gÎ¾(x, t|1, Ï„) dÏ„ = âˆ’2t
âˆ
X
n=1
(âˆ’1)n sin(nÏ€x) cos[nÏ€(t âˆ’Ï„)]
nÏ€

t
0
(5.4.15)
âˆ’2
âˆ
X
n=1
(âˆ’1)n sin(nÏ€x)
sin[nÏ€(t âˆ’Ï„)]
n2Ï€2
âˆ’(t âˆ’Ï„)cos[nÏ€(t âˆ’Ï„)]
nÏ€

t
0
= âˆ’2t
Ï€
âˆ
X
n=1
(âˆ’1)n
n
sin(nÏ€x) + 2
Ï€2
âˆ
X
n=1
(âˆ’1)n
n2
sin(nÏ€x) sin(nÏ€t),
(5.4.16)
and
Z 1
0
u(Î¾, 0)gÏ„(x, t|Î¾, 0) dÎ¾ = âˆ’2
âˆ
X
n=1
sin(nÏ€x) cos(nÏ€t)
Z 1
0
Î¾ sin(nÏ€Î¾) dÎ¾
(5.4.17)
= âˆ’2
âˆ
X
n=1
sin(nÏ€x) cos(nÏ€t)
sin(nÏ€Î¾)
n2Ï€2
âˆ’Î¾ cos(nÏ€Î¾)
nÏ€

1
0
(5.4.18)
= 2
Ï€
âˆ
X
n=1
(âˆ’1)n
n
sin(nÏ€x) cos(nÏ€t).
(5.4.19)
Substituting Equation 5.4.16 and Equation 5.4.19 into Equation 5.4.12, we ï¬nally obtain
u(x, t) = âˆ’2t
Ï€
âˆ
X
n=1
(âˆ’1)n
n
sin(nÏ€x) âˆ’2
Ï€
âˆ
X
n=1
(âˆ’1)n
n
sin(nÏ€x) cos(nÏ€t)
+ 2
Ï€2
âˆ
X
n=1
(âˆ’1)n
n2
sin(nÏ€x) sin(nÏ€t).
(5.4.20)
The ï¬rst summation in Equation 5.4.20 is the Fourier sine expansion for f(x) = x over the
interval 0 < x < 1. Indeed, a quick check shows that the particular solution up(x, t) = xt
satisï¬es the partial diï¬€erential equation and boundary conditions.
The remaining two
summations are necessary so that u(x, 0) = x and ut(x, 0) = 0.
âŠ“âŠ”
To apply Equation 5.4.9 to other problems, we must now ï¬nd the Greenâ€™s function for
a speciï¬c domain. In the following examples we illustrate how this is done using the joint
transform method introduced in the previous section. Note that both examples given there
were for the wave equation.
â€¢ Example 5.4.2: One-dimensional wave equation in an unlimited domain
The simplest possible example of Greenâ€™s functions for the wave equation is the one-
dimensional vibrating string problem.12 In this problem the Greenâ€™s function is given by
the equation
âˆ‚2g
âˆ‚t2 âˆ’c2 âˆ‚2g
âˆ‚x2 = c2Î´(x âˆ’Î¾)Î´(t âˆ’Ï„),
(5.4.21)
12 See also Graï¬€, K. F., 1991: Wave Motion in Elastic Solids. Dover Publications, Inc., Section 1.1.8.

Greenâ€™s Functions
251
where âˆ’âˆ< x, Î¾ < âˆ, and 0 < t, Ï„.
If the initial conditions equal zero, the Laplace
transform of Equation 5.4.21 is
d2G
dx2 âˆ’s2
c2 G = âˆ’Î´(x âˆ’Î¾)eâˆ’sÏ„,
(5.4.22)
where G(x, s|Î¾, Ï„) is the Laplace transform of g(x, t|Î¾, Ï„). To solve Equation 5.4.22 we take
its Fourier transform and obtain the algebraic equation
G(k, s|Î¾, Ï„) = exp(âˆ’ikÎ¾ âˆ’sÏ„)
k2 + s2/c2
.
(5.4.23)
Having found the joint Laplace-Fourier transform of g(x, t|Î¾, Ï„), we must work our way
back to the Greenâ€™s function. From the deï¬nition of the Fourier transform, we have that
G(x, s|Î¾, Ï„) = eâˆ’sÏ„
2Ï€
Z âˆ
âˆ’âˆ
eik(xâˆ’Î¾)
k2 + s2/c2 dk.
(5.4.24)
To evaluate the Fourier-type integral, Equation 5.4.24, we apply the residue theorem. See
Section 2.1. Performing the calculation,
G(x, s|Î¾, Ï„) = c exp(âˆ’sÏ„ âˆ’s|x âˆ’Î¾|/c)
2s
.
(5.4.25)
Finally, taking the inverse Laplace transform of Equation 5.4.25,
g(x, t|Î¾, Ï„) = c
2H(t âˆ’Ï„ âˆ’|x âˆ’Î¾|/c) ,
(5.4.26)
or
g(x, t|Î¾, Ï„) = c
2H[c(t âˆ’Ï„) + (x âˆ’Î¾)] H[c(t âˆ’Ï„) âˆ’(x âˆ’Î¾)] .
(5.4.27)
We can use Equation 5.4.26 and the method of images to obtain the Greenâ€™s function
for
âˆ‚2g
âˆ‚x2 âˆ’1
c2
âˆ‚2g
âˆ‚t2 = Î´(x âˆ’Î¾)Î´(t âˆ’Ï„),
0 < x, t, Î¾, Ï„,
(5.4.28)
subject to the boundary condition g(0, t|Î¾, Ï„) = 0.
We begin by noting that the free-space Greenâ€™s function,13 Equation 5.4.26, is the
particular solution to Equation 5.4.28. Therefore, we need only ï¬nd a homogeneous solution
f(x, t|Î¾, Ï„) so that
g(x, t|Î¾, Ï„) = c
2H(t âˆ’Ï„ âˆ’|x âˆ’Î¾|/c) + f(x, t|Î¾, Ï„)
(5.4.29)
13 In electromagnetic theory, a free-space Greenâ€™s function is the particular solution of the diï¬€erential
equation valid over a domain of inï¬nite extent, where the Greenâ€™s function remains bounded as we approach
inï¬nity, or satisï¬es a radiation condition there.

252
Advanced Engineering Mathematics: A Second Course
0
2
4
6
8
10
0
2
4
6
8
10
0
0.1
0.2
0.3
0.4
0.5
 x/Î¾
 c(tâˆ’Ï„)/Î¾
 g(x,t|Î¾,Ï„)/c
Figure 5.4.1: The Greenâ€™s function g(x, t|Î¾, Ï„)/c given by Equation 5.4.30 for the one-dimensional wave
equation for x > 0 at diï¬€erent distances x/Î¾ and times c(tâˆ’Ï„) subject to the boundary condition g(0, t|Î¾, Ï„) =
0.
satisï¬es the boundary condition at x = 0.
To ï¬nd f(x, t|Î¾, Ï„), let us introduce a source at x = âˆ’Î¾ at t = Ï„. The corresponding
free-space Greenâ€™s function is H(t âˆ’Ï„ âˆ’|x + Î¾|/c). If, along the boundary x = 0 for any
time t, this Greenâ€™s function destructively interferes with the free-space Greenâ€™s function
associated with the source at x = Î¾, then we have our solution. This will occur if our new
source has a negative sign, resulting in the combined Greenâ€™s function
g(x, t|Î¾, Ï„) = c
2 [H(t âˆ’Ï„ âˆ’|x âˆ’Î¾|/c) âˆ’H(t âˆ’Ï„ âˆ’|x + Î¾|/c)] .
(5.4.30)
See Figure 5.4.1. Because Equation 5.4.30 satisï¬es the boundary condition, we need no
further sources.
In a similar manner, we can use Equation 5.4.26 and the method of images to ï¬nd the
Greenâ€™s function for
âˆ‚2g
âˆ‚x2 âˆ’1
c2
âˆ‚2g
âˆ‚t2 = Î´(x âˆ’Î¾)Î´(t âˆ’Ï„),
0 < x, t, Î¾, Ï„,
(5.4.31)
subject to the boundary condition gx(0, t|Î¾, Ï„) = 0.
We begin by examining the related problem
âˆ‚2g
âˆ‚x2 âˆ’1
c2
âˆ‚2g
âˆ‚t2 = Î´(x âˆ’Î¾)Î´(t âˆ’Ï„) + Î´(x + Î¾)Î´(t âˆ’Ï„),
(5.4.32)
where âˆ’âˆ< x, Î¾ < âˆ, and 0 < t, Ï„. In this particular case, we have chosen an image that
is the mirror reï¬‚ection of Î´(x âˆ’Î¾). This was dictated by the fact that the Greenâ€™s function
must be an even function of x along x = 0 for any time t. In line with this argument,
g(x, t|Î¾, Ï„) = c
2 [H(t âˆ’Ï„ âˆ’|x âˆ’Î¾|/c) + H(t âˆ’Ï„ âˆ’|x + Î¾|/c)] .
(5.4.33)
âŠ“âŠ”

Greenâ€™s Functions
253
â€¢ Example 5.4.3: Equation of telegraphy
When the vibrating string problem includes the eï¬€ect of air resistance, Equation 5.4.21
becomes
âˆ‚2g
âˆ‚t2 + 2Î³ âˆ‚g
âˆ‚t âˆ’c2 âˆ‚2g
âˆ‚x2 = c2Î´(x âˆ’Î¾)Î´(t âˆ’Ï„),
(5.4.34)
where âˆ’âˆ< x, Î¾ < âˆ, and 0 < t, Ï„, with the boundary conditions
lim
|x|â†’âˆg(x, t|Î¾, Ï„) â†’0
(5.4.35)
and the initial conditions
g(x, 0|Î¾, Ï„) = gt(x, 0|Î¾, Ï„) = 0.
(5.4.36)
Let us ï¬nd the Greenâ€™s function.
Our analysis begins by introducing an intermediate dependent variable w(x, t|Î¾, Ï„),
where g(x, t|Î¾, Ï„) = eâˆ’Î³tw(x, t|Î¾, Ï„). Substituting for g(x, t|Î¾, Ï„), we now have
âˆ‚2w
âˆ‚t2 âˆ’Î³2w âˆ’c2 âˆ‚2w
âˆ‚x2 = c2Î´(x âˆ’Î¾)Î´(t âˆ’Ï„)eÎ³Ï„.
(5.4.37)
Taking the Laplace transform of Equation 5.4.37, we obtain
d2W
dx2 âˆ’
s2 âˆ’Î³2
c2

W = âˆ’Î´(x âˆ’Î¾)eÎ³Ï„âˆ’sÏ„.
(5.4.38)
Using Fourier transforms as in Example 5.3.1, the solution to Equation 5.4.38 is
W(x, s|Î¾, Ï„) = exp[âˆ’|x âˆ’Î¾|
p
(s2 âˆ’Î³2)/c2 + Î³Ï„ âˆ’sÏ„]
2
p
(s2 âˆ’Î³2)/c2
.
(5.4.39)
Employing tables to invert the Laplace transform and the second shifting theorem, we have
that
w(x, t|Î¾, Ï„) = c
2eÎ³Ï„I0
h
Î³
p
(t âˆ’Ï„)2 âˆ’(x âˆ’Î¾)2/c2
i
H[c(t âˆ’Ï„) âˆ’|x âˆ’Î¾|],
(5.4.40)
or
g(x, t|Î¾, Ï„) = c
2eâˆ’Î³(tâˆ’Ï„)I0
h
Î³
p
(t âˆ’Ï„)2 âˆ’(x âˆ’Î¾)2/c2
i
H[c(t âˆ’Ï„) âˆ’|x âˆ’Î¾|].
(5.4.41)
Figure 5.4.2 illustrates Equation 5.4.41 when Î³ = 1.
âŠ“âŠ”
â€¢ Example 5.4.4
Let us solve14 the one-dimensional wave equation on an inï¬nite domain:
âˆ‚2u
âˆ‚t2 âˆ’âˆ‚2u
âˆ‚x2 = cos(Ï‰t)Î´[x âˆ’X(t)],
(5.4.42)
14 See Knowles, J. K., 1968: Propagation of one-dimensional waves from a source in random motion. J.
Acoust. Soc. Am., 43, 948â€“957.

254
Advanced Engineering Mathematics: A Second Course
âˆ’10
âˆ’5
0
5
10
0
2
4
6
8
10
0
0.1
0.2
0.3
0.4
0.5
0.6
 (xâˆ’Î¾)/c
 tâˆ’Ï„
 g(x,t|Î¾,Ï„)/c
Figure 5.4.2: The free-space Greenâ€™s function g(x, t|Î¾, Ï„)/c for the one-dimensional equation of telegraphy
with Î³ = 1 at diï¬€erent distances (x âˆ’Î¾)/c and times t âˆ’Ï„.
subject to the boundary conditions
lim
|x|â†’âˆu(x, t) â†’0,
0 < t,
(5.4.43)
and initial conditions
u(x, 0) = ut(x, 0) = 0,
âˆ’âˆ< x < âˆ.
(5.4.44)
Here Ï‰ is a constant and X(t) is some function of time.
With the given boundary and initial conditions, only the ï¬rst integral in Equation 5.4.9
does not vanish. Substituting the source term q(x, t) = cos(Ï‰t)Î´[x âˆ’X(t)] and the Greenâ€™s
function given by Equation 5.4.26, we have that
u(x, t) =
Z t
0
Z âˆ
âˆ’âˆ
q(Î¾, Ï„)g(x, t|Î¾, Ï„) dÎ¾ dÏ„
(5.4.45)
= 1
2
Z t
0
Z âˆ
âˆ’âˆ
cos(Ï‰Ï„)Î´[Î¾ âˆ’X(Ï„)]H(t âˆ’Ï„) âˆ’|x âˆ’Î¾|) dÎ¾ dÏ„
(5.4.46)
= 1
2
Z t
0
H[t âˆ’Ï„ âˆ’|X(Ï„) âˆ’x|] cos(Ï‰Ï„) dÏ„,
(5.4.47)
since c = 1.
Problems
1. By direct substitution, show15 that
g(x, t|0, 0) = J0(
âˆš
xt )H(x)H(t)
15 First proven by Picard, Â´E., 1894: Sur une Â´equation aux dÂ´erivÂ´ees partielles de la thÂ´eorie de la propa-
gation de lâ€™Â´electricitÂ´e. Bull. Soc. Math., 22, 2â€“8.

Greenâ€™s Functions
255
is the free-space Greenâ€™s function governed by
âˆ‚2g
âˆ‚xâˆ‚t + 1
4g = Î´(x)Î´(t),
âˆ’âˆ< x, t < âˆ.
2.
Use Equation 5.3.20 to construct the Greenâ€™s function for the one-dimensional wave
equation
âˆ‚2g
âˆ‚t2 âˆ’âˆ‚2g
âˆ‚x2 = Î´(x âˆ’Î¾)Î´(t âˆ’Ï„),
0 < x, Î¾ < L,
0 < t, Ï„,
subject to the boundary conditions g(0, t|Î¾, Ï„) = gx(L, t|Î¾, Ï„) = 0, 0 < t, and the initial
conditions that g(x, 0|Î¾, Ï„) = gt(x, 0|Î¾, Ï„) = 0, 0 < x < L.
3.
Use Equation 5.3.20 to construct the Greenâ€™s function for the one-dimensional wave
equation
âˆ‚2g
âˆ‚t2 âˆ’âˆ‚2g
âˆ‚x2 = Î´(x âˆ’Î¾)Î´(t âˆ’Ï„),
0 < x, Î¾ < L,
0 < t, Ï„,
subject to the boundary conditions gx(0, t|Î¾, Ï„) = gx(L, t|Î¾, Ï„) = 0, 0 < t, and the initial
conditions that g(x, 0|Î¾, Ï„) = gt(x, 0|Î¾, Ï„) = 0, 0 < x < L.
4. Use the Greenâ€™s function given by Equation 5.3.26 to write down the solution to the
wave equation utt = uxx on the interval 0 < x < L with the boundary conditions u(0, t) =
u(L, t) = 0, 0 < t, and the initial conditions u(x, 0) = cos(Ï€x/L) and ut(x, 0) = 0, 0 < x <
L.
5. Use the Greenâ€™s function given by Equation 5.3.26 to write down the solution to the wave
equation utt = uxx on the interval 0 < x < L with the boundary conditions u(0, t) = eâˆ’t
and u(L, t) = 0, 0 < t, and the initial conditions u(x, 0) = sin(Ï€x/L) and ut(x, 0) = 1,
0 < x < L.
6. Use the Greenâ€™s function that you found in Problem 2 to write down the solution to the
wave equation utt = uxx on the interval 0 < x < L with the boundary conditions u(0, t) = 0
and ux(L, t) = 1, 0 < t, and the initial conditions u(x, 0) = x and ut(x, 0) = 1, 0 < x < L.
7. Use the Greenâ€™s function that you found in Problem 3 to write down the solution to
the wave equation utt = uxx on the interval 0 < x < L with the boundary conditions
ux(0, t) = 1 and ux(L, t) = 0, 0 < t, and the initial conditions u(x, 0) = 1 and ut(x, 0) = 0,
0 < x < L.
8. Find the Greenâ€™s function16 governed by
âˆ‚2g
âˆ‚t2 + 2âˆ‚g
âˆ‚t âˆ’âˆ‚2g
âˆ‚x2 = Î´(x âˆ’Î¾)Î´(t âˆ’Ï„),
0 < x, Î¾ < L,
0 < t, Ï„,
subject to the boundary conditions
gx(0, t|Î¾, Ï„) = gx(L, t|Î¾, Ï„) = 0,
0 < t,
16 Â¨OziÂ¸sik, M. N., and B. Vick, 1984: Propagation and reï¬‚ection of thermal waves in a ï¬nite medium.
Int.
J. Heat Mass Transfer, 27, 1845â€“1854; Tang, D.-W., and N. Araki, 1996:
Propagation of non-
Fourier temperature wave in ï¬nite medium under laser-pulse heating (in Japanese). Nihon Kikai Gakkai
Rombumshu (Trans. Japan Soc. Mech. Engrs.), Ser. B, 62, 1136â€“1141.

256
Advanced Engineering Mathematics: A Second Course
and the initial conditions
g(x, 0|Î¾, Ï„) = gt(x, 0|Î¾, Ï„) = 0,
0 < x < L.
Step 1: If the Greenâ€™s function can be written as the Fourier half-range cosine series
g(x, t|Î¾, Ï„) = 1
LG0(t|Ï„) + 2
L
âˆ
X
n=1
Gn(t|Ï„) cos
nÏ€x
L

,
so that it satisï¬es the boundary conditions, show that Gn(t|Ï„) is governed by
Gâ€²â€²
n + 2Gâ€²
n + n2Ï€2
L2 Gn = cos
nÏ€Î¾
L

Î´(t âˆ’Ï„),
0 â‰¤n.
Step 2: Show that
G0(t|Ï„) = eâˆ’(tâˆ’Ï„) sinh(t âˆ’Ï„)H(t âˆ’Ï„),
and
Gn(t|Ï„) = cos
nÏ€Î¾
L

eâˆ’(tâˆ’Ï„) sin[Î²n(t âˆ’Ï„)]
Î²n
H(t âˆ’Ï„),
1 â‰¤n,
where Î²n =
p
(nÏ€/L)2 âˆ’1.
Step 3: Combine the results from Steps 1 and 2 and show that
g(x, t|Î¾, Ï„) = eâˆ’(tâˆ’Ï„) sinh(t âˆ’Ï„)H(t âˆ’Ï„)/L
+ 2eâˆ’(tâˆ’Ï„)H(t âˆ’Ï„)/L
Ã—
âˆ
X
n=1
sin[Î²n(t âˆ’Ï„)]
Î²n
cos
nÏ€Î¾
L

cos
nÏ€x
L

.
5.5 HEAT EQUATION
In this section we present the Greenâ€™s function17 for the heat equation
âˆ‚u
âˆ‚t âˆ’a2 âˆ‚2u
âˆ‚x2 = q(x, t),
(5.5.1)
where t denotes time, x is the position, a2 is the diï¬€usivity, and q(x, t) is the source den-
sity. In addition to Equation 5.5.1, boundary conditions must be speciï¬ed to ensure the
uniqueness of solution; the most common ones are Dirichlet, Neumann, and Robin (a linear
combination of the ï¬rst two). An initial condition u(x, t = t0) is also needed.
The heat equation diï¬€ers in many ways from the wave equation and the Greenâ€™s function
must, of course, manifest these diï¬€erences. The most notable one is the asymmetry of the
17 See also Carslaw, H. S., and J. C. Jaeger, 1959: Conduction of Heat in Solids.
Clarendon Press,
Chapter 14; Beck, J. V., K. D. Cole, A. Haji-Sheikh, and B. Litkouhi, 1992: Heat Conduction Using
Greenâ€™s Functions. Hemisphere Publishing Corp., 523 pp.; Â¨OziÂ¸sik, M. N., 1993: Heat Conduction. John
Wiley & Sons, Inc., Chapter 6.

Greenâ€™s Functions
257
heat equation with respect to time. This merely reï¬‚ects the fact that the heat equation
diï¬€erentiates between past and future as entropy continually increases.
We begin by proving that we can express the solution to Equation 5.5.1 in terms of
boundary conditions, the initial condition, and the Greenâ€™s function, which is found by
solving
âˆ‚g
âˆ‚t âˆ’a2 âˆ‚2g
âˆ‚x2 = Î´(x âˆ’Î¾)Î´(t âˆ’Ï„),
(5.5.2)
where Î¾ denotes the position of the source. From causality18 we know that g(x, t|Î¾, Ï„) = 0
if t < Ï„. We again require that the Greenâ€™s function g(x, t|Î¾, Ï„) satisï¬es the homogeneous
form of the boundary condition on u(x, t). For example, if u satisï¬es a homogeneous or
nonhomogeneous Dirichlet condition, then the Greenâ€™s function will satisfy the correspond-
ing homogeneous Dirichlet condition. Although we will focus on the mathematical aspects
of the problem, Equation 5.5.2 can be given the physical interpretation of the temperature
distribution within a medium when a unit of heat is introduced at Î¾ at time Ï„.
We now establish that the solution to the nonhomogeneous heat equation can be ex-
pressed in terms of the Greenâ€™s function, boundary conditions, and the initial condition.
We begin with the equations
a2 âˆ‚2u(Î¾, Ï„)
âˆ‚Î¾2
âˆ’âˆ‚u(Î¾, Ï„)
âˆ‚Ï„
= âˆ’q(Î¾, Ï„),
(5.5.3)
and
a2 âˆ‚2g(x, t|Î¾, Ï„)
âˆ‚Î¾2
+ âˆ‚g(x, t|Î¾, Ï„)
âˆ‚Ï„
= âˆ’Î´(x âˆ’Î¾)Î´(t âˆ’Ï„).
(5.5.4)
As we did in the previous section, we multiply Equation 5.5.3 by g(x, t|Î¾, Ï„) and Equation
5.5.4 by u(Î¾, Ï„) and subtract. Integrating over Î¾ from a to b, where a and b are the endpoints
of the spatial domain, and over Ï„ from 0 to t+, where t+ denotes a time slightly later than
t so that we avoid ending the integration exactly at the peak of the delta function, we ï¬nd
a2
Z t+
0
Z b
a

u(Î¾, Ï„)âˆ‚2g(x, t|Î¾, Ï„)
âˆ‚Î¾2
âˆ’g(x, t|Î¾, Ï„)âˆ‚2u(Î¾, Ï„)
âˆ‚Î¾2

dÎ¾ dÏ„
+
Z t+
0
Z b
a

u(Î¾, Ï„)âˆ‚g(x, t|Î¾, Ï„)
âˆ‚Ï„
+ g(x, t|Î¾, Ï„)âˆ‚u(Î¾, Ï„)
âˆ‚Ï„

dÎ¾ dÏ„
=
Z t+
0
Z b
a
q(Î¾, Ï„)g(x, t|Î¾, Ï„) dÎ¾ dÏ„ âˆ’u(x, t).
(5.5.5)
Applying Equation 5.4.6 and performing the time integration in the second integral, we
ï¬nally obtain
u(x, t) =
Z t+
0
Z b
a
q(Î¾, Ï„)g(x, t|Î¾, Ï„) dÎ¾ dÏ„
+ a2
Z t+
0

g(x, t|Î¾, Ï„)âˆ‚u(Î¾, Ï„)
âˆ‚Î¾
âˆ’u(Î¾, Ï„)âˆ‚g(x, t|Î¾, Ï„)
âˆ‚Î¾
Î¾=b
Î¾=a
dÏ„
+
Z b
a
u(Î¾, 0)g(x, t|Î¾, 0) dÎ¾,
(5.5.6)
18 The principle stating that an event cannot precede its cause.

258
Advanced Engineering Mathematics: A Second Course
where we used g(x, t|Î¾, t+) = 0. The ï¬rst two terms in Equation 5.5.6 represent the familiar
eï¬€ects of volume sources and boundary conditions, while the third term includes the eï¬€ects
of the initial data.
â€¢ Example 5.5.1: One-dimensional heat equation in an unlimited domain
The Greenâ€™s function for the one-dimensional heat equation is governed by
âˆ‚g
âˆ‚t âˆ’a2 âˆ‚2g
âˆ‚x2 = Î´(x âˆ’Î¾)Î´(t âˆ’Ï„),
âˆ’âˆ< x, Î¾ < âˆ,
0 < t, Ï„,
(5.5.7)
subject to the boundary conditions lim|x|â†’âˆg(x, t|Î¾, Ï„) â†’0, and the initial condition
g(x, 0|Î¾, Ï„) = 0. Let us ï¬nd g(x, t|Î¾, Ï„).
We begin by taking the Laplace transform of Equation 5.5.7 and ï¬nd that
d2G
dx2 âˆ’s
a2 G = âˆ’Î´(x âˆ’Î¾)
a2
eâˆ’sÏ„.
(5.5.8)
Next, we take the Fourier transform of Equation 5.5.8 so that
(k2 + b2)G(k, s|Î¾, Ï„) = eâˆ’ikÎ¾eâˆ’sÏ„
a2
,
(5.5.9)
where G(k, s|Î¾, Ï„) is the Fourier transform of G(x, s|Î¾, Ï„) and b2 = s/a2.
To ï¬nd G(x, s|Î¾, Ï„), we use the inversion integral
G(x, s|Î¾, Ï„) = eâˆ’sÏ„
2Ï€a2
Z âˆ
âˆ’âˆ
ei(xâˆ’Î¾)k
k2 + b2 dk.
(5.5.10)
Transforming Equation 5.5.10 into a closed contour via Jordanâ€™s lemma, we evaluate it by
the residue theorem and ï¬nd that
G(x, s|Î¾, Ï„) = eâˆ’|xâˆ’Î¾|âˆšs/aâˆ’sÏ„
2a âˆšs
.
(5.5.11)
From a table of Laplace transforms we ï¬nally obtain
g(x, t|Î¾, Ï„) =
H(t âˆ’Ï„)
p
4Ï€a2(t âˆ’Ï„)
exp

âˆ’(x âˆ’Î¾)2
4a2(t âˆ’Ï„)

,
(5.5.12)
after applying the second shifting theorem.
âŠ“âŠ”
The primary use of the fundamental or free-space Greenâ€™s function19 is as a particular
solution to the Greenâ€™s function problem. For this reason, it is often called the fundamental
19 In electromagnetic theory, a free-space Greenâ€™s function is the particular solution of the diï¬€erential
equation valid over a domain of inï¬nite extent, where the Greenâ€™s function remains bounded as we approach
inï¬nity, or satisï¬es a radiation condition there.

Greenâ€™s Functions
259
0
0.5
1
1.5
2
0
0.5
1
1.5
2
0
0.1
0.2
0.3
0.4
0.5
0.6
 x
 a2(tâˆ’Ï„)
 [a2(tâˆ’Ï„)]1/2 g(x,t|Î¾,Ï„)
Figure 5.5.1: The Greenâ€™s function, Equation 5.5.17, for the one-dimensional heat equation on the semi-
inï¬nite domain 0 < x < âˆ, and 0 â‰¤t âˆ’Ï„, when the left boundary condition is gx(0, t|Î¾, Ï„) = 0 and
Î¾ = 0.5.
heat conduction solution. Consequently, we usually must ï¬nd a homogeneous solution so
that the sum of the free-space Greenâ€™s function plus the homogeneous solution satisï¬es any
boundary conditions. The following examples show some commonly employed techniques.
â€¢ Example 5.5.2
Let us ï¬nd the Greenâ€™s function for the following problem:
âˆ‚g
âˆ‚t âˆ’a2 âˆ‚2g
âˆ‚x2 = Î´(x âˆ’Î¾)Î´(t âˆ’Ï„),
0 < x, Î¾ < âˆ,
0 < t, Ï„,
(5.5.13)
subject to the boundary conditions g(0, t|Î¾, Ï„) = 0, limxâ†’âˆg(x, t|Î¾, Ï„) â†’0, and the initial
condition g(x, 0|Î¾, Ï„) = 0. From the boundary condition g(0, t|Î¾, Ï„) = 0, we deduce that
g(x, t|Î¾, Ï„) must be an odd function in x over the open interval (âˆ’âˆ, âˆ). We ï¬nd this
Greenâ€™s function by introducing an image source of âˆ’Î´(x + Î¾) and resolving Equation 5.5.7
with the source Î´(xâˆ’Î¾)Î´(tâˆ’Ï„)âˆ’Î´(x+Î¾)Î´(tâˆ’Ï„). Because Equation 5.5.7 is linear, Equation
5.5.12 gives the solution for each delta function and the Greenâ€™s function for Equation 5.5.13
is
g(x, t|Î¾, Ï„) =
H(t âˆ’Ï„)
p
4Ï€a2(t âˆ’Ï„)

exp

âˆ’(x âˆ’Î¾)2
4a2(t âˆ’Ï„)

âˆ’exp

âˆ’(x + Î¾)2
4a2(t âˆ’Ï„)

(5.5.14)
=
H(t âˆ’Ï„)
p
Ï€a2(t âˆ’Ï„)
exp

âˆ’x2 + Î¾2
4a2(t âˆ’Ï„)

sinh

xÎ¾
2a2(t âˆ’Ï„)

.
(5.5.15)
In a similar manner, if the boundary condition at x = 0 changes to gx(0, t|Î¾, Ï„) = 0,
then Equation 5.5.14 through Equation 5.5.15 become
g(x, t|Î¾, Ï„) =
H(t âˆ’Ï„)
p
4Ï€a2(t âˆ’Ï„)

exp

âˆ’(x âˆ’Î¾)2
4a2(t âˆ’Ï„)

+ exp

âˆ’(x + Î¾)2
4a2(t âˆ’Ï„)

(5.5.16)
=
H(t âˆ’Ï„)
p
Ï€a2(t âˆ’Ï„)
exp

âˆ’x2 + Î¾2
4a2(t âˆ’Ï„)

cosh

xÎ¾
2a2(t âˆ’Ï„)

.
(5.5.17)
Figure 5.5.1 illustrates Equation 5.5.17 for the special case when Î¾ = 0.5.
âŠ“âŠ”

260
Advanced Engineering Mathematics: A Second Course
â€¢ Example 5.5.3: One-dimensional heat equation on the interval 0 < x < L
Here we ï¬nd the Greenâ€™s function for the one-dimensional heat equation over the in-
terval 0 < x < L associated with the problem
âˆ‚u
âˆ‚t âˆ’a2 âˆ‚2u
âˆ‚x2 = f(x, t),
0 < x < L,
0 < t,
(5.5.18)
where a2 is the diï¬€usivity constant.
To ï¬nd the Greenâ€™s function for this problem, consider the following problem:
âˆ‚g
âˆ‚t âˆ’a2 âˆ‚2g
âˆ‚x2 = Î´(x âˆ’Î¾)Î´(t âˆ’Ï„),
0 < x, Î¾ < L,
0 < t, Ï„,
(5.5.19)
with the boundary conditions
Î±1g(0, t|Î¾, Ï„) + Î²1gx(0, t|Î¾, Ï„) = 0,
0 < t,
(5.5.20)
and
Î±2g(L, t|Î¾, Ï„) + Î²2gx(L, t|Î¾, Ï„) = 0,
0 < t,
(5.5.21)
and the initial condition
g(x, 0|Î¾, Ï„) = 0,
0 < x < L.
(5.5.22)
We begin by taking the Laplace transform of Equation 5.5.19 and ï¬nd that
d2G
dx2 âˆ’s
a2 G = âˆ’Î´(x âˆ’Î¾)
a2
eâˆ’sÏ„,
0 < x < L,
(5.5.23)
with
Î±1G(0, s|Î¾, Ï„) + Î²1Gâ€²(0, s|Î¾, Ï„) = 0,
(5.5.24)
and
Î±2G(L, s|Î¾, Ï„) + Î²2Gâ€²(L, s|Î¾, Ï„) = 0.
(5.5.25)
Problems similar to Equation 5.5.23 through Equation 5.5.25 were considered in Section
5.2. Applying this technique of eigenfunction expansions, we have that
G(x, s|Î¾, Ï„) = eâˆ’sÏ„
âˆ
X
n=1
Ï•n(Î¾)Ï•n(x)
s + a2k2n
,
(5.5.26)
where Ï•n(x) is the nth orthonormal eigenfunction to the regular Sturm-Liouville problem
Ï•
â€²â€²(x) + k2Ï•(x) = 0,
0 < x < L,
(5.5.27)
subject to the boundary conditions
Î±1Ï•(0) + Î²1Ï•â€²(0) = 0,
(5.5.28)
and
Î±2Ï•(L) + Î²2Ï•â€²(L) = 0.
(5.5.29)

Greenâ€™s Functions
261
Taking the inverse of Equation 5.5.26, we have that
g(x, t|Î¾, Ï„) =
" âˆ
X
n=1
Ï•n(Î¾)Ï•n(x)eâˆ’k2
na2(tâˆ’Ï„)
#
H(t âˆ’Ï„).
(5.5.30)
For example, let us ï¬nd the Greenâ€™s function for the heat equation on a ï¬nite domain
âˆ‚g
âˆ‚t âˆ’a2 âˆ‚2g
âˆ‚x2 = Î´(x âˆ’Î¾)Î´(t âˆ’Ï„),
0 < x, Î¾ < L,
0 < t, Ï„,
(5.5.31)
with the boundary conditions g(0, t|Î¾, Ï„) = g(L, t|Î¾, Ï„) = 0, 0 < t, and the initial condition
g(x, 0|Î¾, Ï„) = 0, 0 < x < L.
The Sturm-Liouville problem is
Ï•â€²â€²(x) + k2Ï•(x) = 0,
0 < x < L,
(5.5.32)
with the boundary conditions Ï•(0) = Ï•(L) = 0. The nth orthonormal eigenfunction to
Equation 5.5.32 is
Ï•n(x) =
r
2
L sin
nÏ€x
L

.
(5.5.33)
Substituting Equation 5.5.33 into Equation 5.5.30, we ï¬nd that
g(x, t|Î¾, Ï„) = 2
L
( âˆ
X
n=1
sin
nÏ€Î¾
L

sin
nÏ€x
L

eâˆ’a2n2Ï€2(tâˆ’Ï„)/L2
)
H(t âˆ’Ï„).
(5.5.34)
On the other hand, the Greenâ€™s function for the heat equation on a ï¬nite domain
governed by
âˆ‚g
âˆ‚t âˆ’a2 âˆ‚2g
âˆ‚x2 = Î´(x âˆ’Î¾)Î´(t âˆ’Ï„),
0 < x, Î¾ < L,
0 < t, Ï„,
(5.5.35)
with the boundary conditions
gx(0, t|Î¾, Ï„) = 0,
gx(L, t|Î¾, Ï„) + hg(L, t|Î¾, Ï„) = 0,
0 < t,
(5.5.36)
and the initial condition g(x, 0|Î¾, Ï„) = 0, 0 < x < L, yields the Sturm-Liouville problem
that we must now solve:
Ï•â€²â€²(x) + Î»Ï•(x) = 0,
Ï•â€²(0) = 0,
Ï•â€²(L) + hÏ•(L) = 0.
(5.5.37)
The nth orthonormal eigenfunction for Equation 5.5.37 is
Ï•n(x) =
s
2(k2n + h2)
L(k2n + h2) + h cos(knx),
(5.5.38)

262
Advanced Engineering Mathematics: A Second Course
where kn is the nth root of k tan(kL) = h. We also used the identity that (k2
n+h2) sin2(knh)
= h2. Substituting Equation 5.5.38 into Equation 5.5.30, we ï¬nally obtain
g(x, t|Î¾, Ï„) = 2
L
( âˆ
X
n=1
[(knL)2 + (hL)2] cos(knÎ¾) cos(knx)
(knL)2 + (hL)2 + hL
eâˆ’a2k2
n(tâˆ’Ï„)
)
H(t âˆ’Ï„).
(5.5.39)
âŠ“âŠ”
â€¢ Example 5.5.4
Let us use Greenâ€™s functions to solve the heat equation
âˆ‚u
âˆ‚t = a2 âˆ‚2u
âˆ‚x2 ,
0 < x < L,
0 < t,
(5.5.40)
subject to the boundary conditions
u(0, t) = 0,
u(L, t) = t,
0 < t,
(5.5.41)
and the initial condition
u(x, 0) = 0,
0 < x < L.
(5.5.42)
Because there is no source term, Equation 5.5.6 simpliï¬es to
u(x, t) = a2
Z t
0
[g(x, t|L, Ï„)uÎ¾(L, Ï„) âˆ’u(L, Ï„)gÎ¾(x, t|L, Ï„)] dÏ„
(5.5.43)
âˆ’a2
Z t
0
[g(x, t|0, Ï„)uÎ¾(0, Ï„) âˆ’u(0, Ï„)gÎ¾(x, t|0, Ï„)] dÏ„ +
Z L
0
u(Î¾, 0)g(x, t|Î¾, 0) dÎ¾.
The Greenâ€™s function that should be used here is the one given by Equation 5.5.34. Further
simpliï¬cation occurs by noting that g(x, t|0, Ï„) = g(x, t|L, Ï„) = 0 as well as u(0, Ï„) =
u(Î¾, 0) = 0. Therefore we are left with the single integral
u(x, t) = âˆ’a2
Z t
0
u(L, Ï„)gÎ¾(x, t|L, Ï„) dÏ„.
(5.5.44)
Upon substituting for g(x, t|L, Ï„) and reversing the order of integration and summation,
u(x, t) = âˆ’2Ï€a2
L2
âˆ
X
n=1
(âˆ’1)nn sin
nÏ€x
L
 Z t
0
Ï„ exp
a2n2Ï€2
L2
(Ï„ âˆ’t)

dÏ„
(5.5.45)
= âˆ’2L2
a2Ï€3
âˆ
X
n=1
(âˆ’1)n
n3
sin
nÏ€x
L

exp
a2n2Ï€2
L2
(Ï„ âˆ’t)
 a2n2Ï€2Ï„
L2
âˆ’1

t
0
(5.5.46)
= âˆ’2L2
a2Ï€3
âˆ
X
n=1
(âˆ’1)n
n3
sin
nÏ€x
L
 a2n2Ï€2t
L2
âˆ’1 + exp

âˆ’a2n2Ï€2t
L2

.
(5.5.47)
Figure 5.5.2 illustrates Equation 5.5.47. This solution could also have been found using
Duhamelâ€™s integral.
âŠ“âŠ”

Greenâ€™s Functions
263
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.5
1
 x/L
 a2t/L2
 a2u(x,t)/L2
Figure 5.5.2: The temperature distribution within a bar when the temperature is initially at zero and
then the ends are held at zero at x = 0 and t at x = L.
â€¢ Example 5.5.5: Heat equation within a cylinder
In this example, we ï¬nd the Greenâ€™s function for the heat equation in cylindrical
coordinates
âˆ‚g
âˆ‚t âˆ’a2
r
âˆ‚
âˆ‚r

râˆ‚g
âˆ‚r

= Î´(r âˆ’Ï)Î´(t âˆ’Ï„)
2Ï€r
,
0 < r, Ï < b,
0 < t, Ï„,
(5.5.48)
subject to the boundary conditions limrâ†’0 |g(r, t|Ï, Ï„)| < âˆ, g(b, t|Ï, Ï„) = 0, and the initial
condition g(r, 0|Ï, Ï„) = 0.
As usual, we begin by taking the Laplace transform of Equation 5.5.48, or
1
r
d
dr

rdG
dr

âˆ’s
a2 G = âˆ’eâˆ’sÏ„
2Ï€a2rÎ´(r âˆ’Ï).
(5.5.49)
Next we re-express Î´(r âˆ’Ï)/r as the Fourier-Bessel expansion
Î´(r âˆ’Ï)
2Ï€r
=
âˆ
X
n=1
AnJ0(knr/b),
(5.5.50)
where kn is the nth root of J0(k) = 0, and
An =
2
b2J2
1(kn)
Z b
0
Î´(r âˆ’Ï)
2Ï€r
J0(knr/b) r dr = J0(knÏ/b)
Ï€b2J2
1(kn)
(5.5.51)
so that
1
r
d
dr

rdG
dr

âˆ’s
a2 G = âˆ’eâˆ’sÏ„
Ï€a2b2
âˆ
X
n=1
J0(knÏ/b)J0(knr/b)
J2
1(kn)
.
(5.5.52)

264
Advanced Engineering Mathematics: A Second Course
0
0.5
1
0
0.05
0.1
0.15
0.2
0
1
2
3
4
5
 r/b
 a2(tâˆ’Ï„)/b2
 b2 g(r,t|Ï,Ï„)
Figure 5.5.3: The Greenâ€™s function, Equation 5.5.54, for the axisymmetric heat equation, Equation 5.5.48,
with a Dirichlet boundary condition at r = b. Here Ï/b = 0.3 and the graph starts at a2(t âˆ’Ï„)/b2 = 0.001
to avoid the delta function at t âˆ’Ï„ = 0.
The solution to Equation 5.5.52 is
G(r, s|Ï, Ï„) = eâˆ’sÏ„
Ï€
âˆ
X
n=1
J0(knÏ/b)J0(knr/b)
(sb2 + a2k2n)J2
1(kn) .
(5.5.53)
Taking the inverse of Equation 5.5.53 and applying the second shifting theorem,
g(r, t|Ï, Ï„) = H(t âˆ’Ï„)
Ï€b2
âˆ
X
n=1
J0(knÏ/b)J0(knr/b)
J2
1(kn)
eâˆ’a2k2
n(tâˆ’Ï„)/b2.
(5.5.54)
See Figure 5.5.3.
If we modify the boundary condition at r = b so that it now reads
gr(b, t|Ï, Ï„) + hg(b, t|Ï, Ï„) = 0,
(5.5.55)
where h â‰¥0, our analysis now leads to
g(r, t|Ï, Ï„) = H(t âˆ’Ï„)
Ï€b2
âˆ
X
n=1
J0(knÏ/b)J0(knr/b)
J2
0(kn) + J2
1(kn)
eâˆ’a2k2
n(tâˆ’Ï„)/b2,
(5.5.56)
where kn are the positive roots of k J1(k) âˆ’hb J0(k) = 0. If h = 0, we must add 1/(Ï€b2) to
Equation 5.5.56.
Problems
1. Find the free-space Greenâ€™s function for the linearized Ginzburg-Landau equation20
âˆ‚g
âˆ‚t + v âˆ‚g
âˆ‚x âˆ’ag âˆ’bâˆ‚2g
âˆ‚x2 = Î´(x âˆ’Î¾)Î´(t âˆ’Ï„), âˆ’âˆ< x, Î¾ < âˆ, 0 < t, Ï„,
20 See Deissler, R. J., 1985: Noise-sustained structure, intermittency, and the Ginzburg-Landau equation.
J. Stat. Phys., 40, 371â€“395.

Greenâ€™s Functions
265
with b > 0.
Step 1: Taking the Laplace transform of the partial diï¬€erential equation, show that it
reduces to the ordinary diï¬€erential equation
bd2G
dx2 âˆ’v dG
dx + aG âˆ’sG = âˆ’Î´(x âˆ’Î¾)eâˆ’sÏ„.
Step 2: Using Fourier transforms, show that
G(x, s|Î¾, Ï„) = eâˆ’sÏ„
2Ï€
Z âˆ
âˆ’âˆ
eik(xâˆ’Î¾)
s + ikv + bk2 âˆ’a dk,
or
g(x, t|Î¾, Ï„) = ea(tâˆ’Ï„)
Ï€
H(t âˆ’Ï„)
Z âˆ
0
eâˆ’b(tâˆ’Ï„)k2 cos{k[x âˆ’Î¾ âˆ’v(t âˆ’Ï„)]} dk.
Step 3: Evaluate the second integral and show that
g(x, t|Î¾, Ï„) = ea(tâˆ’Ï„)H(t âˆ’Ï„)
2
p
Ï€b(t âˆ’Ï„)
exp

âˆ’[x âˆ’Î¾ âˆ’v(t âˆ’Ï„)]2
4b(t âˆ’Ï„)

.
2. Use Greenâ€™s functions to show that the solution21 to
âˆ‚u
âˆ‚t = a2 âˆ‚2u
âˆ‚x2 ,
0 < x, t,
subject to the boundary conditions
u(0, t) = 0,
lim
xâ†’âˆu(x, t) â†’0,
0 < t,
and the initial condition
u(x, 0) = f(x),
0 < x < âˆ,
is
u(x, t) = eâˆ’x2/(4a2t)
a
âˆš
Ï€t
Z âˆ
0
f(Ï„) sinh
 xÏ„
2a2t

eâˆ’Ï„ 2/(4a2t) dÏ„.
3.
Use Equation 5.5.30 to construct the Greenâ€™s function for the one-dimensional heat
equation gt âˆ’gxx = Î´(x âˆ’Î¾)Î´(t âˆ’Ï„) for 0 < x < L, 0 < t, with the initial condition that
g(x, 0|Î¾, Ï„) = 0, 0 < x < L, and the boundary conditions that g(0, t|Î¾, Ï„) = gx(L, t|Î¾, Ï„) = 0
for 0 < t. Assume that L Ì¸= Ï€.
4.
Use Equation 5.5.30 to construct the Greenâ€™s function for the one-dimensional heat
equation gt âˆ’gxx = Î´(x âˆ’Î¾)Î´(t âˆ’Ï„) for 0 < x < L, 0 < t, with the initial condition that
21 See Gilev, S. D., and T. Yu. MikhaË˜Ä±lova, 1996: Current wave in shock compression of matter in a
magnetic ï¬eld. Tech. Phys., 41, 407â€“411.

266
Advanced Engineering Mathematics: A Second Course
g(x, 0|Î¾, Ï„) = 0, 0 < x < L, and the boundary conditions that gx(0, t|Î¾, Ï„) = gx(L, t|Î¾, Ï„) = 0
for 0 < t.
5.
Use Equation 5.5.43 and the Greenâ€™s function given by Equation 5.5.34 to ï¬nd the
solution to the heat equation ut = uxx for 0 < x < L, 0 < t, with the initial data
u(x, 0) = 1, 0 < x < L, and the boundary conditions u(0, t) = eâˆ’t and u(L, t) = 0 when
0 < t.
6. Use Equation 5.5.43 and the Greenâ€™s function that you found in Problem 3 to ï¬nd the
solution to the heat equation ut = uxx for 0 < x < L, 0 < t, with the initial data u(x, 0) = 1,
0 < x < L, and the boundary conditions u(0, t) = sin(t) and ux(L, t) = 0 when 0 < t.
7. Use Equation 5.5.43 and the Greenâ€™s function that you found in Problem 4 to ï¬nd the
solution to the heat equation ut = uxx for 0 < x < L, 0 < t, with the initial data u(x, 0) = 1,
0 < x < L, and the boundary conditions ux(0, t) = 1 and ux(L, t) = 0 when 0 < t.
8. Find the Greenâ€™s function for
âˆ‚g
âˆ‚t âˆ’a2 âˆ‚2g
âˆ‚x2 + a2k2g = Î´(x âˆ’Î¾)Î´(t âˆ’Ï„),
0 < x, Î¾ < L,
0 < t, Ï„,
subject to the boundary conditions
g(0, t|Î¾, Ï„) = gx(L, t|Î¾, Ï„) = 0,
0 < t,
and the initial condition
g(x, 0|Î¾, Ï„) = 0,
0 < x < L,
where a and k are real constants.
5.6 HELMHOLTZâ€™S EQUATION
In the previous sections, we sought solutions to the heat and wave equations via Greenâ€™s
functions. In this section, we turn to the reduced wave equation
âˆ‚2u
âˆ‚x2 + âˆ‚2u
âˆ‚y2 + Î»u = âˆ’f(x, y).
(5.6.1)
Equation 5.6.1, generally known as Helmholtzâ€™s equation, includes the special case of Pois-
sonâ€™s equation when Î» = 0. Poissonâ€™s equation has a special place in the theory of Greenâ€™s
functions because George Green (1793â€“1841) invented his technique for its solution.
The reduced wave equation arises during the solution of the harmonically forced wave
equation22 by separation of variables. In one spatial dimension, the problem is
âˆ‚2u
âˆ‚x2 âˆ’1
c2
âˆ‚2u
âˆ‚t2 = âˆ’f(x)eâˆ’iÏ‰t.
(5.6.2)
Equation 5.6.2 occurs, for example, in the mathematical analysis of a stretched string over
some interval subject to an external, harmonic forcing. Assuming that u(x, t) is bounded
22 See, for example, Graï¬€, K. F., 1991: Wave Motion in Elastic Solids. Dover Publications, Inc., Section
1.4.

Greenâ€™s Functions
267
everywhere, we seek solutions of the form u(x, t) = y(x)eâˆ’iÏ‰t.
Upon substituting this
solution into Equation 5.6.2 we obtain the ordinary diï¬€erential equation
yâ€²â€² + k2
0y = âˆ’f(x),
(5.6.3)
where k2
0 = Ï‰2/c2. This is an example of the one-dimensional Helmholtz equation.
Let us now use Greenâ€™s functions to solve the Helmholtz equation, Equation 5.6.1,
where the Greenâ€™s function is given by the Helmholtz equation
âˆ‚2g
âˆ‚x2 + âˆ‚2g
âˆ‚y2 + Î»g = âˆ’Î´(x âˆ’Î¾)Î´(y âˆ’Î·).
(5.6.4)
The most commonly encountered boundary conditions are
â€¢ the Dirichlet boundary condition, where g vanishes on the boundary,
â€¢ the Neumann boundary condition, where the normal gradient of g vanishes on the bound-
ary, and
â€¢ the Robin boundary condition, which is the linear combination of the Dirichlet and Neu-
mann conditions.
We begin by multiplying Equation 5.6.1 by g(x, y|Î¾, Î·) and Equation 5.6.4 by u(x, y), sub-
tract and integrate over the region a < x < b, c < y < d. We ï¬nd that
u(Î¾, Î·) =
Z d
c
Z b
a

g(x, y|Î¾, Î·)
âˆ‚2u(x, y)
âˆ‚x2
+ âˆ‚2u(x, y)
âˆ‚y2

âˆ’u(x, y)
âˆ‚2g(x, y|Î¾, Î·)
âˆ‚x2
+ âˆ‚2g(x, y|Î¾, Î·)
âˆ‚y2

dx dy
+
Z d
c
Z b
a
f(x, y)g(x, y|Î¾, Î·) dx dy
(5.6.5)
=
Z d
c
Z b
a
 âˆ‚
âˆ‚x

g(x, y|Î¾, Î·)âˆ‚u(x, y)
âˆ‚x

âˆ’âˆ‚
âˆ‚x

u(x, y)âˆ‚g(x, y|Î¾, Î·)
âˆ‚x

dx dy
+
Z d
c
Z b
a
 âˆ‚
âˆ‚y

g(x, y|Î¾, Î·)âˆ‚u(x, y)
âˆ‚y

âˆ’âˆ‚
âˆ‚y

u(x, y)âˆ‚g(x, y|Î¾, Î·)
âˆ‚y

dx dy
+
Z d
c
Z b
a
f(x, y)g(x, y|Î¾, Î·) dx dy
(5.6.6)
=
Z d
c

g(x, y|Î¾, Î·)âˆ‚u(x, y)
âˆ‚x
âˆ’u(x, y)âˆ‚g(x, y|Î¾, Î·)
âˆ‚x
x=b
x=a
dy
+
Z b
a

g(x, y|Î¾, Î·)âˆ‚u(x, y)
âˆ‚y
âˆ’u(x, y)âˆ‚g(x, y|Î¾, Î·)
âˆ‚y
y=d
y=c
dx
+
Z d
c
Z b
a
f(x, y)g(x, y|Î¾, Î·) dx dy.
(5.6.7)
Because (Î¾, Î·) is an arbitrary point inside the rectangle, we denote it in general by (x, y).
Furthermore, the variable (x, y) is now merely a dummy integration variable that we now
denote by (Î¾, Î·).
Upon making these substitutions and using the symmetry condition

268
Advanced Engineering Mathematics: A Second Course
g(x, y|Î¾, Î·) = g(Î¾, Î·|x, y), we have that
u(x, y) =
Z d
c

g(x, y|Î¾, Î·)âˆ‚u(Î¾, Î·)
âˆ‚Î¾
âˆ’u(Î¾, Î·)âˆ‚g(x, y|Î¾, Î·)
âˆ‚Î¾
Î¾=b
Î¾=a
dÎ·
+
Z b
a

g(x, y|Î¾, Î·)âˆ‚u(Î¾, Î·)
âˆ‚Î·
âˆ’u(Î¾, Î·)âˆ‚g(x, y|Î¾, Î·)
âˆ‚Î·
Î·=d
Î·=c
dÎ¾
+
Z d
c
Z b
a
f(Î¾, Î·)g(x, y|Î¾, Î·) dÎ¾ dÎ·.
(5.6.8)
Equation 5.6.8 shows that the solution of Helmholtzâ€™s equation depends upon the sources
inside the rectangle and values of u(x, y) and (âˆ‚u/âˆ‚x, âˆ‚u/âˆ‚y) along the boundary. On the
other hand, we must still ï¬nd the particular Greenâ€™s function for a given problem; this
Greenâ€™s function depends directly upon the boundary conditions. At this point, we work
out several special cases.
1. Nonhomogeneous Helmholtz equation
and homogeneous Dirichlet boundary conditions
In this case, let us assume that we can ï¬nd a Greenâ€™s function that also satisï¬es the
same Dirichlet boundary conditions as u(x, y). Once the Greenâ€™s function is found, then
Equation 5.6.8 reduces to
u(x, y) =
Z d
c
Z b
a
f(Î¾, Î·)g(x, y|Î¾, Î·) dÎ¾dÎ·.
(5.6.9)
A possible source of diï¬ƒculty would be the nonexistence of the Greenâ€™s function. From
our experience in Section 5.2, we know that this will occur if Î» equals one of the eigenvalues
of the corresponding homogeneous problem. An example of this occurs in acoustics when
the Greenâ€™s function for the Helmholtz equation does not exist at resonance.
2. Homogeneous Helmholtz equation
and nonhomogeneous Dirichlet boundary conditions
In this particular case, f(x, y) = 0. For convenience, let us use the Greenâ€™s function
from the previous example so that g(x, y|Î¾, Î·) = 0 along all of the boundaries. Under these
conditions, Equation 5.6.8 becomes
u(x, y) = âˆ’
Z b
a
u(Î¾, Î·)âˆ‚g(x, y|Î¾, Î·)
âˆ‚Î·

Î·=d
Î·=c
dÎ¾ âˆ’
Z d
c
u(Î¾, Î·)âˆ‚g(x, y|Î¾, Î·)
âˆ‚Î¾

Î¾=b
Î¾=a
dÎ·.
(5.6.10)
Consequently, the solution is determined once we compute the normal gradient of the
Greenâ€™s function along the boundary.
3. Nonhomogeneous Helmholtz equation
and homogeneous Neumann boundary conditions
If we require that u(x, y) satisï¬es the nonhomogeneous Helmholtz equation with homo-
geneous Neumann boundary conditions, then the governing equations are Equation 5.6.1

Greenâ€™s Functions
269
and the boundary conditions ux = 0 along x = a and x = b, and uy = 0 along y = c and
y = d. Integrating Equation 5.6.1, we have that
Z d
c
âˆ‚u(b, y)
âˆ‚x
âˆ’âˆ‚u(a, y)
âˆ‚x

dy +
Z b
a
âˆ‚u(x, d)
âˆ‚y
âˆ’âˆ‚u(x, c)
âˆ‚y

dx
+ Î»
Z d
c
Z b
a
u(x, y) dx dy = âˆ’
Z d
c
Z b
a
f(x, y) dx dy.
(5.6.11)
Because the ï¬rst two integrals in Equation 5.6.11 must vanish in the case of homogeneous
Neumann boundary conditions, this equation cannot be satisï¬ed if Î» = 0 unless
Z d
c
Z b
a
f(x, y) dx dy = 0.
(5.6.12)
A physical interpretation of Equation 5.6.12 is as follows: Consider the physical process
of steady-state heat conduction within a rectangular region. The temperature u(x, y) is
given by Poissonâ€™s equation
âˆ‚2u
âˆ‚x2 + âˆ‚2u
âˆ‚y2 = âˆ’f(x, y),
(5.6.13)
where f(x, y) is proportional to the density of the heat sources and sinks. The boundary
conditions ux(a, y) = ux(b, y) = 0 and uy(x, c) = uy(x, d) = 0 imply that there is no heat
exchange across the boundary. Consequently, no steady-state temperature distribution can
exist unless the heat sources are balanced by heat sinks. This balance of heat sources and
sinks is given by Equation 5.6.12.
Having provided an overview of how Greenâ€™s functions can be used to solve Poisson
and Helmholtz equations, let us now determine several of them for commonly encountered
domains.
â€¢ Example 5.6.1: Free-space Greenâ€™s function for the one-dimensional Helmholtz equation
Let us ï¬nd the Greenâ€™s function for the one-dimensional Helmholtz equation
gâ€²â€² + k2
0g = âˆ’Î´(x âˆ’Î¾),
âˆ’âˆ< x, Î¾ < âˆ.
(5.6.14)
If we solve Equation 5.6.14 by piecing together homogeneous solutions, then
g(x|Î¾) = Aeâˆ’ik0(xâˆ’Î¾) + Beik0(xâˆ’Î¾),
(5.6.15)
for x < Î¾, while
g(x|Î¾) = Ceâˆ’ik0(xâˆ’Î¾) + Deik0(xâˆ’Î¾),
(5.6.16)
for Î¾ < x.
Let us examine Equation 5.6.15 more closely. The solution represents two propagating
waves. Because x < Î¾, the ï¬rst term is a wave propagating out to inï¬nity, while the second
term gives a wave propagating in from inï¬nity. This is seen most clearly by including the
eâˆ’iÏ‰t term into Equation 5.6.15, or
g(x|Î¾)eâˆ’iÏ‰t = Aeâˆ’ik0(xâˆ’Î¾)âˆ’iÏ‰t + Beik0(xâˆ’Î¾)âˆ’iÏ‰t.
(5.6.17)
Because we have a source only at x = Î¾, solutions that represent waves originating at inï¬nity
are nonphysical and we must discard them. This requirement that there are only outwardly

270
Advanced Engineering Mathematics: A Second Course
0
x
x
iy
k
-k
-k
k
iy
(a) Contour for x >
(b) Contour for x < 
0
0
Î¾
Î¾
0
Figure 5.6.1: Contour used to evaluate Equation 5.6.21.
propagating wave solutions is commonly called Sommerfeldâ€™s radiation condition.23 Similar
considerations hold for Equation 5.6.16 and we must take C = 0.
To evaluate A and D, we use the continuity conditions on the Greenâ€™s function:
g(Î¾+|Î¾) = g(Î¾âˆ’|Î¾),
and
gâ€²(Î¾+|Î¾) âˆ’gâ€²(Î¾âˆ’|Î¾) = âˆ’1,
(5.6.18)
or
A = D,
and
ik0D + ik0A = âˆ’1.
(5.6.19)
Therefore,
g(x|Î¾) =
i
2k0
eik0|xâˆ’Î¾|.
(5.6.20)
We can also solve Equation 5.6.14 by Fourier transforms. Assuming that the Fourier
transform of g(x|Î¾) exists and denoting it by G(k|Î¾), we ï¬nd that
G(k|Î¾) =
eâˆ’ikÎ¾
k2 âˆ’k2
0
,
and
g(x|Î¾) = 1
2Ï€
Z âˆ
âˆ’âˆ
eik(xâˆ’Î¾)
k2 âˆ’k2
0
dk.
(5.6.21)
Immediately we see that there is a problem with the singularities lying on the path of
integration at k = Â±k0. How do we avoid them?
There are four possible ways that we might circumvent the singularities. One of them
is shown in Figure 5.6.1. Applying Jordanâ€™s lemma to close the line integral along the real
axis (as shown in Figure 5.6.1),
g(x|Î¾) = 1
2Ï€
I
C
eiz(xâˆ’Î¾)
z2 âˆ’k2
0
dz.
(5.6.22)
23 Sommerfeld, A., 1912: Die Greensche Funktion der Schwingungsgleichung.
Jahresber.
Deutschen
Math.- Vereinung, 21, 309â€“353.

Greenâ€™s Functions
271
Free-Space Greenâ€™s Function for the Poisson and Helmholtz Equations
Dimension
Poisson Equation
Helmholtz Equation
One
no solution
g(x|Î¾) =
i
2k0
eik0|xâˆ’Î¾|
Two
g(x, y|Î¾, Î·) = âˆ’ln(r)
2Ï€
g(x, y|Î¾, Î·) = i
4H(1)
0 (k0r)
r =
p
(x âˆ’Î¾)2 + (y âˆ’Î·)2
Note: For the Helmholtz equation, we have taken the temporal forcing to be eâˆ’iÏ‰t and k0 = Ï‰/c.
For x < Î¾,
g(x|Î¾) = âˆ’i Res
eiz(xâˆ’Î¾)
z2 âˆ’k2
0
; âˆ’k0

=
i
2k0
eâˆ’ik0(xâˆ’Î¾),
(5.6.23)
while
g(x|Î¾) = i Res
eiz(xâˆ’Î¾)
z2 âˆ’k2
0
; k0

=
i
2k0
eik0(xâˆ’Î¾),
(5.6.24)
for x > Î¾. A quick check shows that these solutions agree with Equation 5.6.20. If we try
the three other possible paths around the singularities, we obtain incorrect solutions.
âŠ“âŠ”
â€¢ Example 5.6.2: Free-space Greenâ€™s function for the two-dimensional Helmholtz equation
At this point, we have found two forms of the free-space Greenâ€™s function for the one-
dimensional Helmholtz equation. The ï¬rst form is the analytic solution, Equation 5.6.20,
while the second is the integral representation, Equation 5.6.21, where the line integration
along the real axis is shown in Figure 5.6.1.
In the case of two dimensions, the Greenâ€™s function24 for the Helmholtz equation sym-
metric about the point (Î¾, Î·) is the solution of the equation
d2g
dr2 + 1
r
dg
dr + k2
0g = âˆ’Î´(r)
2Ï€r ,
(5.6.25)
where r =
p
(x âˆ’Î¾)2 + (y âˆ’Î·)2.
The homogeneous form of Equation 5.6.25 is Besselâ€™s
diï¬€erential equation of order zero. Consequently, the general solution in terms of Hankel
functions is
g(r|r0) = A H(1)
0 (k0r) + B H(2)
0 (k0r).
(5.6.26)
Why have we chosen to use Hankel functions rather than J0(Â·) and Y0(Â·)? As we argued
earlier, solutions to the Helmholtz equation must represent outwardly propagating waves
(the Sommerfeld radiation condition).
If we again assume that the temporal behavior
is eâˆ’iÏ‰t and use the asymptotic expressions for Hankel functions, we see that H(1)
0 (k0r)
represents outwardly propagating waves and B = 0.
24 For an alternative derivation, see Graï¬€, K. F., 1991: Wave Motion in Elastic Solids. Dover Publica-
tions, Inc., pp. 284â€“285.

272
Advanced Engineering Mathematics: A Second Course
What is the value of A? Integrating Equation 5.6.26 over a small circle around the
point r = 0 and taking the limit as the radius of the circle vanishes, A = i/4 and
g(r|r0) = i
4H(1)
0 (k0r).
(5.6.27)
If a real function is needed, then the free-space Greenâ€™s function equals the Neumann
function Y0(k0r) divided by âˆ’4.
âŠ“âŠ”
â€¢ Example 5.6.3: Free-space Greenâ€™s function for the two-dimensional Laplace equation
In this subsection, we ï¬nd the free-space Greenâ€™s function for Poissonâ€™s equation in two
dimensions. This Greenâ€™s function is governed by
1
r
âˆ‚
âˆ‚r

râˆ‚g
âˆ‚r

+ 1
r2
âˆ‚2g
âˆ‚Î¸2 = âˆ’Î´(r âˆ’Ï)Î´(Î¸ âˆ’Î¸â€²)
r
.
(5.6.28)
If we now choose our coordinate system so that the origin is located at the point source,
r =
p
(x âˆ’Î¾)2 + (y âˆ’Î·)2 and Ï = 0. Multiplying both sides of this simpliï¬ed Equation
5.6.28 by r dr dÎ¸ and integrating over a circle of radius Ç«, we obtain âˆ’1 on the right side
from the surface integration over the delta functions. On the left side,
Z 2Ï€
0
râˆ‚g
âˆ‚r

r=Ç«
dÎ¸ = âˆ’1.
(5.6.29)
The Greenâ€™s function g(r, Î¸|0, Î¸â€²) = âˆ’ln(r)/(2Ï€) satisï¬es Equation 5.6.29.
To ï¬nd an alternative form of the free-space Greenâ€™s function when the point of exci-
tation and the origin of the coordinate system do not coincide, we ï¬rst note that
Î´(Î¸ âˆ’Î¸â€²) = 1
2Ï€
âˆ
X
n=âˆ’âˆ
ein(Î¸âˆ’Î¸â€²).
(5.6.30)
This suggests that the Greenâ€™s function should be of the form
g(r, Î¸|Ï, Î¸â€²) =
âˆ
X
n=âˆ’âˆ
gn(r|Ï)ein(Î¸âˆ’Î¸â€²).
(5.6.31)
Substituting Equation 5.6.30 and Equation 5.6.31 into Equation 5.6.29, we obtain the or-
dinary diï¬€erential equation
1
r
d
dr

rdgn
dr

âˆ’n2
r2 gn = âˆ’Î´(r âˆ’Ï)
2Ï€r
.
(5.6.32)
The homogeneous solution to Equation 5.6.32 is
g0(r|Ï) =

a,
0 â‰¤r â‰¤Ï ,
b ln(r),
Ï â‰¤r < âˆ,
(5.6.33)

Greenâ€™s Functions
273
and
gn(r|Ï) =

c (r/Ï)n,
0 â‰¤r â‰¤Ï ,
d (Ï/r)n,
Ï â‰¤r < âˆ,
(5.6.34)
if n Ì¸= 0.
At r = Ï, the gnâ€™s must be continuous, in which case,
a = b ln(Ï),
and
c = d.
(5.6.35)
On the other hand,
Ïdgn
dr

r=Ï+
r=Ïâˆ’
= âˆ’1
2Ï€ ,
(5.6.36)
or
a = âˆ’ln(Ï)
2Ï€ ,
b = âˆ’1
2Ï€ ,
and
c = d =
1
4Ï€n.
(5.6.37)
Therefore,
g(r, Î¸|Ï, Î¸â€²) = âˆ’ln(r>)
2Ï€
+ 1
2Ï€
âˆ
X
n=1
1
n
r<
r>
n
cos[n(Î¸ âˆ’Î¸â€²)],
(5.6.38)
where r> = max(r, Ï) and r< = min(r, Ï).
We can simplify Equation 5.6.38 by noting that
ln

1 + Ï2 âˆ’2Ï cos(Î¸ âˆ’Î¸â€²)

= âˆ’2
âˆ
X
n=1
Ïn cos[n(Î¸ âˆ’Î¸â€²)]
n
,
(5.6.39)
if |Ï| < 1. Applying this relationship to Equation 5.6.38, we ï¬nd that
g(r, Î¸|Ï, Î¸â€²) = âˆ’1
4Ï€ ln

r2 + Ï2 âˆ’2rÏ cos(Î¸ âˆ’Î¸â€²)

.
(5.6.40)
Note that when Ï = 0 we recover g(r, Î¸|0, Î¸â€²) = âˆ’ln(r)/(2Ï€).
âŠ“âŠ”
â€¢ Example 5.6.4: Two-dimensional Poisson equation over a rectangular domain
Consider the two-dimensional Poisson equation
âˆ‚2u
âˆ‚x2 + âˆ‚2u
âˆ‚y2 = âˆ’f(x, y).
(5.6.41)
This equation arises in equilibrium problems, such as the static deï¬‚ection of a rectangular
membrane. In that case, f(x, y) represents the external load per unit area, divided by the
tension in the membrane. The solution u(x, y) must satisfy certain boundary conditions.
For the present, let us choose u(0, y) = u(a, y) = 0, and u(x, 0) = u(x, b) = 0.
To ï¬nd the Greenâ€™s function for Equation 5.6.41 we must solve the partial diï¬€erential
equation
âˆ‚2g
âˆ‚x2 + âˆ‚2g
âˆ‚y2 = âˆ’Î´(x âˆ’Î¾)Î´(y âˆ’Î·),
0 < x, Î¾ < a,
0 < y, Î· < b,
(5.6.42)
subject to the boundary conditions
g(0, y|Î¾, Î·) = g(a, y|Î¾, Î·) = g(x, 0|Î¾, Î·) = g(x, b|Î¾, Î·) = 0.
(5.6.43)

274
Advanced Engineering Mathematics: A Second Course
From Equation 5.6.9,
u(x, y) =
Z a
0
Z b
0
g(x, y|Î¾, Î·)f(Î¾, Î·) dÎ· dÎ¾.
(5.6.44)
One approach to ï¬nding the Greenâ€™s function is to expand it in terms of the eigenfunc-
tions Ï•(x, y) of the diï¬€erential equation
âˆ‚2Ï•
âˆ‚x2 + âˆ‚2Ï•
âˆ‚y2 = âˆ’Î»Ï•,
(5.6.45)
and the boundary conditions, Equation 5.6.43. The eigenvalues are
Î»nm = n2Ï€2
a2
+ m2Ï€2
b2
,
(5.6.46)
where n = 1, 2, 3, . . ., m = 1, 2, 3, . . ., and the corresponding eigenfunctions are
Ï•nm(x, y) = sin
nÏ€x
a

sin
mÏ€y
b

.
(5.6.47)
Therefore, we seek g(x, y|Î¾, Î·) in the form
g(x, y|Î¾, Î·) =
âˆ
X
n=1
âˆ
X
m=1
Anm sin
nÏ€x
a

sin
mÏ€y
b

.
(5.6.48)
Because the delta functions can be written
Î´(x âˆ’Î¾)Î´(y âˆ’Î·) = 4
ab
âˆ
X
n=1
âˆ
X
m=1
sin
nÏ€Î¾
a

sin
mÏ€Î·
b

sin
nÏ€x
a

sin
mÏ€y
b

,
(5.6.49)
we ï¬nd that
n2Ï€2
a2
+ m2Ï€2
b2

Anm = 4
ab sin
nÏ€Î¾
a

sin
mÏ€Î·
b

,
(5.6.50)
after substituting Equations 5.6.48 and 5.6.49 into Equation 5.6.42, and setting the corre-
sponding harmonics equal to each other. Therefore, the bilinear formula for the Greenâ€™s
function of Poissonâ€™s equation is
g(x, y|Î¾, Î·) = 4
ab
âˆ
X
n=1
âˆ
X
m=1
sin
nÏ€x
a

sin
nÏ€Î¾
a

sin
mÏ€y
b

sin
mÏ€Î·
b

n2Ï€2/a2 + m2Ï€2/b2
.
(5.6.51)
Thus, solutions to Poissonâ€™s equation can now be written as
u(x, y) =
âˆ
X
n=1
âˆ
X
m=1
anm
n2Ï€2/a2 + m2Ï€2/b2 sin
nÏ€x
a

sin
mÏ€y
b

,
(5.6.52)

Greenâ€™s Functions
275
where anm are the Fourier coeï¬ƒcients for the function f(x, y):
anm = 4
ab
Z a
0
Z b
0
f(x, y) sin
nÏ€x
a

sin
mÏ€y
b

dy dx.
(5.6.53)
Another form of the Greenâ€™s function can be obtained by considering each direction
separately. To satisfy the boundary conditions along the edges y = 0 and y = b, we write
the Greenâ€™s function as the Fourier series
g(x, y|Î¾, Î·) =
âˆ
X
m=1
Gm(x|Î¾) sin
mÏ€y
b

,
(5.6.54)
where the coeï¬ƒcients Gm(x|Î¾) are left as undetermined functions of x, Î¾, and m. Substi-
tuting this series into the partial diï¬€erential equation for g, multiplying by 2 sin(nÏ€y/b)/b,
and integrating over y, we ï¬nd that
d2Gn
dx2 âˆ’n2Ï€2
b2 Gn = âˆ’2
b sin
nÏ€Î·
b

Î´(x âˆ’Î¾).
(5.6.55)
This diï¬€erential equation shows that the expansion coeï¬ƒcients Gn(x|Î¾) are one-dimensional
Greenâ€™s functions; we can ï¬nd them, as we did in Section 5.2, by piecing together homo-
geneous solutions to Equation 5.6.55 that are valid over various intervals. For the region
0 â‰¤x â‰¤Î¾, the solution to this equation that vanishes at x = 0 is
Gn(x|Î¾) = An sinh
nÏ€x
b

,
(5.6.56)
where An is presently arbitrary. The corresponding solution for Î¾ â‰¤x â‰¤a is
Gn(x|Î¾) = Bn sinh
nÏ€(a âˆ’x)
b

.
(5.6.57)
Note that this solution vanishes at x = a. Because the Greenâ€™s function must be continuous
at x = Î¾,
An sinh
nÏ€Î¾
b

= Bn sinh
nÏ€(a âˆ’Î¾)
b

.
(5.6.58)
On the other hand, the appropriate jump discontinuity of Gâ€²
n(x|Î¾) yields
âˆ’nÏ€
b Bn cosh
nÏ€(a âˆ’Î¾)
b

âˆ’nÏ€
b An cosh
nÏ€Î¾
b

= âˆ’2
b sin
nÏ€Î·
b

.
(5.6.59)
Solving for An and Bn,
An = 2
nÏ€ sin
nÏ€Î·
b
 sinh[nÏ€(a âˆ’Î¾)/b]
sinh(nÏ€a/b)
,
(5.6.60)
and
Bn = 2
nÏ€ sin
nÏ€Î·
b
 sinh(nÏ€Î¾/b)
sinh(nÏ€a/b).
(5.6.61)
This yields the Greenâ€™s function
g(x, y|Î¾, Î·) = 2
Ï€
âˆ
X
n=1
sinh[nÏ€(a âˆ’x>)/b] sinh(nÏ€x</b)
n sinh(nÏ€a/b)
sin
nÏ€Î·
b

sin
nÏ€y
b

,
(5.6.62)

276
Advanced Engineering Mathematics: A Second Course
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
 x/b
 y/b
 g(x,y|Î¾,Î·)
Figure 5.6.2: The Greenâ€™s function, Equation 5.6.62 or Equation 5.6.63, for the planar Poisson equation
over a rectangular area with Dirichlet boundary conditions on all sides when a = b and Î¾/b = Î·/b = 0.3.
where x> = max(x, Î¾) and x< = min(x, Î¾). Figure 5.6.2 illustrates Equation 5.6.62 in the
case of a square domain with Î¾/b = Î·/b = 0.3.
If we began with a Fourier expansion in the y-direction, we would have obtained
g(x, y|Î¾, Î·) = 2
Ï€
âˆ
X
m=1
sinh[mÏ€(b âˆ’y>)/a] sinh(mÏ€y</a)
m sinh(mÏ€b/a)
sin
mÏ€Î¾
a

sin
mÏ€x
a

,
(5.6.63)
where y> = max(y, Î·) and y< = min(y, Î·).
âŠ“âŠ”
â€¢ Example 5.6.5: Two-dimensional Helmholtz equation over a rectangular domain
The problem to be solved is
âˆ‚2g
âˆ‚x2 + âˆ‚2g
âˆ‚y2 + k2
0g = âˆ’Î´(x âˆ’Î¾)Î´(y âˆ’Î·),
(5.6.64)
where 0 < x, Î¾ < a, and 0 < y, Î· < b, subject to the boundary conditions that
g(0, y|Î¾, Î·) = g(a, y|Î¾, Î·) = g(x, 0|Î¾, Î·) = g(x, b|Î¾, Î·) = 0.
(5.6.65)
We use the same technique to solve Equation 5.6.64 as we did in the previous example
by assuming that the Greenâ€™s function has the form
g(x, y|Î¾, Î·) =
âˆ
X
m=1
Gm(x|Î¾) sin
mÏ€y
b

,
(5.6.66)
where the coeï¬ƒcients Gm(x|Î¾) are undetermined functions of x, Î¾, and Î·. Substituting this
series into Equation 5.6.64, multiplying by 2 sin(nÏ€y/b)/b, and integrating over y, we ï¬nd
that
d2Gn
dx2 âˆ’
n2Ï€2
b2
âˆ’k2
0

Gn = âˆ’2
b sin
nÏ€Î·
b

Î´(x âˆ’Î¾).
(5.6.67)

Greenâ€™s Functions
277
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
âˆ’1
âˆ’0.5
0
0.5
1
 x/a
 y/a
 g(x,y|Î¾,Î·)
Figure 5.6.3: The Greenâ€™s function, Equation 5.6.72, for Helmholtzâ€™s equation over a rectangular region
with a Dirichlet boundary condition on the sides when a = b, k0a = 10, and Î¾/a = Î·/a = 0.35.
The ï¬rst method for solving Equation 5.6.67 involves writing
Î´(x âˆ’Î¾) = 2
a
âˆ
X
m=1
sin
mÏ€Î¾
a

sin
mÏ€x
a

,
(5.6.68)
and
Gn(x|Î¾) = 2
a
âˆ
X
m=1
anm sin
mÏ€x
a

.
(5.6.69)
Upon substituting Equations 5.6.68 and 5.6.69 into Equation 5.6.67, we obtain
âˆ
X
m=1

k2
0âˆ’m2Ï€2
a2
âˆ’n2Ï€2
b2

anm sin
mÏ€x
a

= âˆ’4
ab
âˆ
X
m=1
sin
nÏ€Î·
b

sin
mÏ€Î¾
a

sin
mÏ€x
a

.
(5.6.70)
Matching similar harmonics,
anm =
4 sin(mÏ€Î¾/a) sin(nÏ€Î·/b)
ab(m2Ï€2/a2 + n2Ï€2/b2 âˆ’k2
0),
(5.6.71)
and the bilinear form of the Greenâ€™s function is
g(x, y|Î¾, Î·) = 4
ab
âˆ
X
n=1
âˆ
X
m=1
sin(mÏ€Î¾/a) sin(nÏ€Î·/b) sin(mÏ€x/a) sin(nÏ€y/b)
m2Ï€2/a2 + n2Ï€2/b2 âˆ’k2
0
.
(5.6.72)
See Figure 5.6.3. The bilinear form of the Greenâ€™s function for the two-dimensional Helm-
holtz equation with Neumann boundary conditions is left as Problem 8.

278
Advanced Engineering Mathematics: A Second Course
As in the previous example, we can construct an alternative to the bilinear form of the
Greenâ€™s function, Equation 5.6.72, by writing Equation 5.6.67 as
d2Gn
dx2 âˆ’k2
nGn = âˆ’2
b sin
nÏ€Î·
b

Î´(x âˆ’Î¾),
(5.6.73)
where k2
n = n2Ï€2/b2 âˆ’k2
0. The homogeneous solution to Equation 5.6.73 is now
Gn(x|Î¾) =

An sinh(knx),
0 â‰¤x â‰¤Î¾,
Bn sinh[kn(a âˆ’x)],
Î¾ â‰¤x â‰¤a.
(5.6.74)
This solution satisï¬es the boundary conditions at both endpoints.
Because Gn(x|Î¾) must be continuous at x = Î¾,
An sinh(knÎ¾) = Bn sinh[kn(a âˆ’Î¾)].
(5.6.75)
On the other hand, the jump discontinuity involving Gâ€²
n(x|Î¾) yields
âˆ’knBn cosh[kn(a âˆ’Î¾)] âˆ’knAn cosh(knÎ¾) = âˆ’2
b sin
nÏ€Î·
b

.
(5.6.76)
Solving for An and Bn,
An =
2
bkn
sin
nÏ€Î·
b
 sinh[kn(a âˆ’Î¾)]
sinh(kna)
,
(5.6.77)
and
Bn =
2
bkn
sin
nÏ€Î·
b
 sinh(knÎ¾)
sinh(kna).
(5.6.78)
This yields the Greenâ€™s function
g(x, y|Î¾, Î·) = 2
b
N
X
n=1
sin[Îºn(a âˆ’x>)] sin(Îºnx<)
Îºn sin(Îºna)
sin
nÏ€Î·
b

sin
nÏ€y
b

+ 2
b
âˆ
X
n=N+1
sinh[kn(a âˆ’x>)] sinh(knx<)
kn sinh(kna)
sin
nÏ€Î·
b

sin
nÏ€y
b

,
(5.6.79)
where x> = max(x, Î¾) and x< = min(x, Î¾). Here N denotes the largest value of n such that
k2
n < 0, and Îº2
n = k2
0 âˆ’n2Ï€2/b2. If we began with a Fourier expansion in the y direction,
we would have obtained
g(x, y|Î¾, Î·) = 2
a
M
X
m=1
sin[Îºm(b âˆ’y>)] sin(Îºmy<)
Îºm sin(Îºmb)
sin
mÏ€Î¾
a

sin
mÏ€x
a

+ 2
a
âˆ
X
m=M+1
sinh[km(b âˆ’y>)] sinh(kmy<)
km sinh(kmb)
sin
mÏ€Î¾
a

sin
mÏ€x
a

,
(5.6.80)
where M denotes the largest value of m such that k2
m < 0, k2
m = m2Ï€2/a2 âˆ’k2
0, Îº2
m =
k2
0 âˆ’m2Ï€2/a2, y< = min(y, Î·), and y> = max(y, Î·).
âŠ“âŠ”

Greenâ€™s Functions
279
â€¢ Example 5.6.6: Two-dimensional Helmholtz equation over a circular disk
In this example, we ï¬nd the Greenâ€™s function for the Helmholtz equation when the
domain consists of the circular region 0 < r < a. The Greenâ€™s function is governed by the
partial diï¬€erential equation
1
r
âˆ‚
âˆ‚r

râˆ‚g
âˆ‚r

+ 1
r2
âˆ‚2g
âˆ‚Î¸2 + k2
0g = âˆ’Î´(r âˆ’Ï)Î´(Î¸ âˆ’Î¸â€²)
r
,
(5.6.81)
where 0 < r, Ï < a, and 0 â‰¤Î¸, Î¸â€² â‰¤2Ï€, with the boundary conditions
lim
râ†’0 |g(r, Î¸|Ï, Î¸â€²)| < âˆ,
g(a, Î¸|Ï, Î¸â€²) = 0,
0 â‰¤Î¸, Î¸â€² â‰¤2Ï€.
(5.6.82)
The Greenâ€™s function must be periodic in Î¸.
We begin by noting that
Î´(Î¸ âˆ’Î¸â€²) = 1
2Ï€ + 1
Ï€
âˆ
X
n=1
cos[n(Î¸ âˆ’Î¸â€²)] = 1
2Ï€
âˆ
X
n=âˆ’âˆ
cos[n(Î¸ âˆ’Î¸â€²)].
(5.6.83)
Therefore, the solution has the form
g(r, Î¸|Ï, Î¸â€²) =
âˆ
X
n=âˆ’âˆ
gn(r|Ï) cos[n(Î¸ âˆ’Î¸â€²)].
(5.6.84)
Substituting Equation 5.6.83 and Equation 5.6.84 into Equation 5.6.81 and simplifying, we
ï¬nd that
1
r
d
dr

rdgn
dr

âˆ’n2
r2 gn + k2
0gn = âˆ’Î´(r âˆ’Ï)
2Ï€r
.
(5.6.85)
The solution to Equation 5.6.85 is the Fourier-Bessel series
gn(r|Ï) =
âˆ
X
m=1
AnmJn
knmr
a

,
(5.6.86)
where knm is the mth root of Jn(k) = 0. Upon substituting Equation 5.6.86 into Equation
5.6.85 and solving for Anm, we have that
(k2
0 âˆ’k2
nm/a2)Anm = âˆ’
1
Ï€a2Jâ€²2
n(knm)
Z a
0
Î´(r âˆ’Ï)Jn
knmr
a

dr,
(5.6.87)
or
Anm =
Jn(knmÏ/a)
Ï€(k2nm âˆ’k2
0a2)Jâ€²2
n(knm).
(5.6.88)
Thus, the Greenâ€™s function25 is
g(r, Î¸|Ï, Î¸â€²) = 1
Ï€
âˆ
X
n=âˆ’âˆ
âˆ
X
m=1
Jn(knmÏ/a)Jn(knmr/a)
(k2nm âˆ’k2
0a2)Jâ€²2
n(knm) cos[n(Î¸ âˆ’Î¸â€²)].
(5.6.89)
25 For an example of its use, see Zhang, D. R., and C. F. Foo, 1999: Fields analysis in a solid magnetic
toroidal core with circular cross section based on Greenâ€™s function. IEEE Trans. Magnetics, 35, 3760â€“3762.

280
Advanced Engineering Mathematics: A Second Course
âˆ’1
âˆ’0.5
0
0.5
1
âˆ’1
âˆ’0.5
0
0.5
1
âˆ’1
âˆ’0.5
0
0.5
1
 x/a
 y/a
 g(r,Î¸|Ï,Î¸â€²)
Figure 5.6.4: The Greenâ€™s function, Equation 5.6.89, for Helmholtzâ€™s equation within a circular disk with
a Dirichlet boundary condition on the rim when k0a = 10, Ï/a = 0.35
âˆš
2, and Î¸â€² = Ï€/4.
See Figure 5.6.4.
Problems
1. Using a Fourier sine expansion in the x-direction, construct the Greenâ€™s function governed
by the planar Poisson equation
âˆ‚2g
âˆ‚x2 + âˆ‚2g
âˆ‚y2 = âˆ’Î´(x âˆ’Î¾)Î´(y âˆ’Î·),
0 < x, Î¾ < a,
âˆ’âˆ< y, Î· < âˆ,
with the Dirichlet boundary conditions
g(0, y|Î¾, Î·) = g(a, y|Î¾, Î·) = 0,
âˆ’âˆ< y < âˆ,
and the conditions at inï¬nity that
lim
|y|â†’âˆg(x, y|Î¾, Î·) â†’0,
0 < x < a.
2. Using a generalized Fourier expansion in the x-direction, construct the Greenâ€™s function
governed by the planar Poisson equation
âˆ‚2g
âˆ‚x2 + âˆ‚2g
âˆ‚y2 = âˆ’Î´(x âˆ’Î¾)Î´(y âˆ’Î·),
0 < x, Î¾ < a,
âˆ’âˆ< y, Î· < âˆ,
with the Neumann and Dirichlet boundary conditions
gx(0, y|Î¾, Î·) = g(a, y|Î¾, Î·) = 0,
âˆ’âˆ< y < âˆ,
and the conditions at inï¬nity that
lim
|y|â†’âˆg(x, y|Î¾, Î·) â†’0,
0 < x < a.

Greenâ€™s Functions
281
3. Using a Fourier sine expansion in the y-direction, show that the Greenâ€™s function governed
by the planar Poisson equation
âˆ‚2g
âˆ‚x2 + âˆ‚2g
âˆ‚y2 = âˆ’Î´(x âˆ’Î¾)Î´(y âˆ’Î·),
0 < x, Î¾ < a,
0 < y, Î· < b,
with the boundary conditions
g(x, 0|Î¾, Î·) = g(x, b|Î¾, Î·) = 0,
and
g(0, y|Î¾, Î·) = gx(a, y|Î¾, Î·) + Î² g(a, y|Î¾, Î·) = 0,
Î² â‰¥0,
is
g(x, y|Î¾, Î·) =
âˆ
X
n=1
sinh(Î½x<) {Î½ cosh[Î½(a âˆ’x>)] + Î² sinh[Î½(a âˆ’x>)]}
Î½2 cosh(Î½a) + Î²Î½ sinh(Î½a)
sin
nÏ€Î·
b

sin
nÏ€y
b

,
where Î½ = nÏ€/b, x> = max(x, Î¾), and x< = min(x, Î¾).
4. Using the Fourier series representation of the delta function in a circular domain:
Î´(Î¸ âˆ’Î¸â€²) = 1
2Ï€ + 1
Ï€
âˆ
X
n=1
cos[n(Î¸ âˆ’Î¸â€²)],
0 â‰¤Î¸, Î¸â€² â‰¤2Ï€,
construct the Greenâ€™s function governed by the planar Poisson equation
1
r
âˆ‚
âˆ‚r

râˆ‚g
âˆ‚r

+ 1
r2
âˆ‚2g
âˆ‚Î¸2 = âˆ’Î´(r âˆ’Ï)Î´(Î¸ âˆ’Î¸â€²)
r
,
where a < r, Ï < b, and 0 â‰¤Î¸, Î¸â€² â‰¤2Ï€, subject to the boundary conditions g(a, Î¸|Ï, Î¸â€²) =
g(b, Î¸|Ï, Î¸â€²) = 0 and periodicity in Î¸.
5. Construct the Greenâ€™s function governed by the planar Poisson equation
1
r
âˆ‚
âˆ‚r

râˆ‚g
âˆ‚r

+ 1
r2
âˆ‚2g
âˆ‚Î¸2 = âˆ’Î´(r âˆ’Ï)Î´(Î¸ âˆ’Î¸â€²)
r
,
where 0 < r, Ï < âˆ, and 0 < Î¸, Î¸â€² < Î², subject to the boundary conditions that g(r, 0|Ï, Î¸â€²)
= g(r, Î²|Ï, Î¸â€²) = 0 for all r. Hint:
Î´(Î¸ âˆ’Î¸â€²) = 2
Î²
âˆ
X
n=1
sin
nÏ€Î¸â€²
Î²

sin
nÏ€Î¸
Î²

.
6. Construct the Greenâ€™s function governed by the planar Poisson equation
1
r
âˆ‚
âˆ‚r

râˆ‚g
âˆ‚r

+ 1
r2
âˆ‚2g
âˆ‚Î¸2 = âˆ’Î´(r âˆ’Ï)Î´(Î¸ âˆ’Î¸â€²)
r
,

282
Advanced Engineering Mathematics: A Second Course
where 0 < r, Ï < a, and 0 < Î¸, Î¸â€² < Î², subject to the boundary conditions g(r, 0|Ï, Î¸â€²) =
g(r, Î²|Ï, Î¸â€²) = g(a, Î¸|Ï, Î¸â€²) = 0. Hint:
Î´(Î¸ âˆ’Î¸â€²) = 2
Î²
âˆ
X
n=1
sin
nÏ€Î¸â€²
Î²

sin
nÏ€Î¸
Î²

.
7. Using a Fourier sine series in the z-direction and the fact that
Î´(x âˆ’b) = 2b
a2
âˆ
X
k=1
J0(Âµkb/a)J0(Âµkx/a)
J2
1(Âµk)
,
0 < x, b < a,
where Âµk is the kth positive root of J0(Âµ) = 0, ï¬nd the Greenâ€™s function governed by the
axisymmetric Poisson equation
âˆ‚2g
âˆ‚r2 + 1
r
âˆ‚g
âˆ‚r + âˆ‚2g
âˆ‚z2 = âˆ’Î´(r âˆ’Ï)Î´(z âˆ’Î¶)
2Ï€r
,
where 0 < r, Ï < a, and 0 < z, Î¶ < L, subject to the boundary conditions
g(r, 0|Ï, Î¶) = g(r, L|Ï, Î¶) = 0,
0 < r < a,
and
lim
râ†’0 |g(r, z|Ï, Î¶)| < âˆ,
g(a, z|Ï, Î¶) = 0,
0 < z < L.
8. Following Example 5.6.5 except for using Fourier cosine series, construct the Greenâ€™s
function26 governed by the planar Helmholtz equation
âˆ‚2g
âˆ‚x2 + âˆ‚2g
âˆ‚y2 + k2
0g = âˆ’Î´(x âˆ’Î¾)Î´(y âˆ’Î·),
0 < x, Î¾ < a,
0 < y, Î· < b,
subject to the Neumann boundary conditions
gx(0, y|Î¾, Î·) = gx(a, y|Î¾, Î·) = 0,
0 < y < b,
and
gy(x, 0|Î¾, Î·) = gy(x, b|Î¾, Î·) = 0,
0 < x < a.
9. Using Fourier sine transforms,
g(x, y|Î¾, Î·) = 2
Ï€
Z âˆ
0
G(k, y|Î¾, Î·) sin(kx) dk,
where
G(k, y|Î¾, Î·) =
Z âˆ
0
g(x, y|Î¾, Î·) sin(kx) dx,
26 Kulkarni et al. (Kulkarni, S., F. G. Leppington, and E. G. Broadbent, 2001: Vibrations in several
interconnected regions: A comparison of SEA, ray theory and numerical results. Wave Motion, 33, 79â€“96)
solved this problem when the domain has two diï¬€erent, constant k2
0â€™s.

Greenâ€™s Functions
283
ï¬nd the Greenâ€™s function governed by
âˆ‚2g
âˆ‚x2 + âˆ‚2g
âˆ‚y2 = âˆ’Î´(x âˆ’Î¾)Î´(y âˆ’Î·),
for the quarter space 0 < x, y, with the boundary conditions
g(0, y|Î¾, Î·) = g(x, 0|Î¾, Î·) = 0,
and
lim
x,yâ†’âˆg(x, y|Î¾, Î·) â†’0.
Step 1: Taking the Fourier sine transform in the x direction, show that the partial diï¬€er-
ential equation reduces to the ordinary diï¬€erential equation
d2G
dy2 âˆ’k2G = âˆ’sin(kÎ¾)Î´(y âˆ’Î·),
with the boundary conditions
G(k, 0|Î¾, Î·) = 0,
and
lim
yâ†’âˆG(k, y|Î¾, Î·) â†’0.
Step 2: Show that the particular solution to the ordinary diï¬€erential equation in Step 1 is
Gp(k, y|Î¾, Î·) = sin(kÎ¾)
2k
eâˆ’k|yâˆ’Î·|.
You may want to review Example 5.2.8.
Step 3: Find the homogeneous solution to the ordinary diï¬€erential equation in Step 1 so
that the general solution satisï¬es the boundary conditions. Show that the general solution
is
G(k, y|Î¾, Î·) = sin(kÎ¾)
2k
h
eâˆ’k|yâˆ’Î·| âˆ’eâˆ’k(y+Î·)i
.
Step 4: Taking the inverse, show that
g(x, y|Î¾, Î·) = 1
Ï€
Z âˆ
0
h
eâˆ’k|yâˆ’Î·| âˆ’eâˆ’k(y+Î·)i
sin(kÎ¾) sin(kx) dk
k .
Step 5: Performing the integration,27 show that
g(x, y|Î¾, Î·) = âˆ’1
4Ï€ ln
[(x âˆ’Î¾)2 + (y âˆ’Î·)2][(x + Î¾)2 + (y + Î·)2]
[(x âˆ’Î¾)2 + (y + Î·)2][(x + Î¾)2 + (y âˆ’Î·)2]

.
27 Gradshteyn, I. S., and I. M. Ryzhik, 1965: Table of Integrals, Series, and Products. Academic Press,
Section 3.947, Formula 1.

284
Advanced Engineering Mathematics: A Second Course
10. Find the free-space Greenâ€™s function28 governed by
âˆ‚2g
âˆ‚x2 + âˆ‚2g
âˆ‚y2 âˆ’g = âˆ’Î´(x âˆ’Î¾)Î´(y âˆ’Î·),
âˆ’âˆ< x, y, Î¾, Î· < âˆ.
Step 1: Introducing the Fourier transform
g(x, y|Î¾, Î·) = 1
2Ï€
Z âˆ
âˆ’âˆ
G(k, y|Î¾, Î·)eikx dk,
where
G(k, y|Î¾, Î·) =
Z âˆ
âˆ’âˆ
g(x, y|Î¾, Î·)eâˆ’ikx dx,
show that the governing partial diï¬€erential equation can be transformed into the ordinary
diï¬€erential equation
d2G
dy2 âˆ’
 k2 + 1

G = âˆ’eâˆ’ikÎ¾Î´(y âˆ’Î·).
Step 2: Introducing the Fourier transform in the y-direction,
G(k, y|Î¾, Î·) = 1
2Ï€
Z âˆ
âˆ’âˆ
G(k, â„“|Î¾, Î·)eiâ„“y dâ„“,
where
G(k, â„“|Î¾, Î·) =
Z âˆ
âˆ’âˆ
G(k, y|Î¾, Î·)eâˆ’iâ„“y dy,
solve the ordinary diï¬€erential equation in Step 1 and show that
G(k, y|Î¾, Î·) = eâˆ’ikÎ¾
2Ï€
Z âˆ
âˆ’âˆ
eiâ„“(yâˆ’Î·)
k2 + â„“2 + 1 dâ„“.
Step 3: Complete the problem by showing that
g(x, y|Î¾, Î·) =
1
4Ï€2
Z âˆ
âˆ’âˆ
Z âˆ
âˆ’âˆ
eik(xâˆ’Î¾)eiâ„“(yâˆ’Î·)
k2 + â„“2 + 1
dâ„“dk
=
1
4Ï€2
Z âˆ
0
Z 2Ï€
0
eirÎº cos(Î¸âˆ’Ï•)
Îº2 + 1
Îº dÎ¸ dÎº
= 1
2Ï€
Z âˆ
0
J0(rÎº)
Îº2 + 1 Îº dÎº = K0(r)
2Ï€
,
where r =
p
(x âˆ’Î¾)2 + (y âˆ’Î·)2, k = Îº cos(Î¸), â„“= Îº sin(Î¸), x âˆ’Î¾ = r cos(Ï•), and y âˆ’Î· =
r sin(Ï•). You will need to use integral tables29 to obtain the ï¬nal result.
28 For its use, see Geisler, J. E., 1970: Linear theory of the response of a two layer ocean to a moving
hurricane. Geophys. Fluid Dyn., 1, 249â€“272.
29 Gradshteyn and Ryzhik, op. cit., Section 6.532, Formula 6.

Greenâ€™s Functions
285
11. Find the free-space Greenâ€™s function governed by
âˆ‚2g
âˆ‚x2 + âˆ‚2g
âˆ‚y2 âˆ’âˆ‚g
âˆ‚x = âˆ’Î´(x âˆ’Î¾)Î´(y âˆ’Î·),
âˆ’âˆ< x, y, Î¾, Î· < âˆ.
Step 1: By introducing Ï•(x, y|Î¾, Î·) such that
g(x, y|Î¾, Î·) = ex/2Ï•(x, y|Î¾, Î·),
show that the partial diï¬€erential equation for g(x, y|Î¾, Î·) becomes
âˆ‚2Ï•
âˆ‚x2 + âˆ‚2Ï•
âˆ‚y2 âˆ’Ï•
4 = âˆ’eâˆ’Î¾/2Î´(x âˆ’Î¾)Î´(y âˆ’Î·).
Step 2: After taking the Fourier transform with respect to x of the partial diï¬€erential
equation in Step 1, show that it becomes the ordinary diï¬€erential equation
d2Î¦
dy2 âˆ’
 k2 + 1
4

Î¦ = âˆ’eâˆ’Î¾/2âˆ’ikÎ¾Î´(y âˆ’Î·).
Step 3: Introducing the same transformation as in Step 3 of the previous problem, show
that
Î¦(k, y|Î¾, Î·) = eâˆ’Î¾/2âˆ’ikÎ¾
2Ï€
Z âˆ
âˆ’âˆ
eiâ„“(yâˆ’Î·)
k2 + â„“2 + 1
4
dâ„“,
and
Ï•(x, y|Î¾, Î·) = eâˆ’Î¾/2
2Ï€ K0( 1
2r),
where r =
p
(x âˆ’Î¾)2 + (y âˆ’Î·)2.
Step 4: Using the transformation introduced in Step 1, show that
g(x, y|Î¾, Î·) = e(xâˆ’Î¾)/2
2Ï€
K0( 1
2r).
5.7 GALERKIN METHOD
In the previous sections we developed various analytic expressions for Greenâ€™s functions.
We close this chapter by showing how to construct a numerical approximation.
Finite elements can be used to solve diï¬€erential equations by introducing subdomains
known as ï¬nite elements rather than a grid of nodal points. The solution is then represented
within each element by an interpolating polynomial. Unlike ï¬nite diï¬€erence schemes that are
constructed from Taylor expansions, the theory behind ï¬nite elements introduces concepts
from functional analysis and variational methods to formulate the algebraic equations.
There are several paths that lead to the same ï¬nite element formulation. The two most
common techniques are the Galerkin and collocation methods. In this section we focus on
the Galerkin method. This method employs a rational polynomial, called a basis function,
that satisï¬es the boundary conditions.

286
Advanced Engineering Mathematics: A Second Course
We begin by considering the Sturm-Liouville problem governed by
d2Ïˆn
dx2 + Î»nÏˆn = 0,
0 < x < L,
(5.7.1)
subject to the boundary conditions Ïˆn(0) = Ïˆn(L) = 0.
Although we could solve this
problem exactly, we will pretend that we cannot.
Rather, we will assume that we can
express it by
Ïˆn(x) =
N
X
j=1
Î±njfj(x),
(5.7.2)
where fj(x) is our basis function. Clearly, it is desirable that fj(0) = fj(L) = 0.
How do we compute Î±nj? We begin by multiplying Equation 5.7.1 by fi(x) and inte-
grating the resulting equation from 0 and L. This yields
Z L
0
fi(x)d2Ïˆn
dx2 dx + Î»n
Z L
0
fi(x)Ïˆn(x) dx = 0,
(5.7.3)
where i = 1, 2, 3, . . . , N. Next, we substitute Equation 5.7.2 and ï¬nd that
N
X
j=1
"Z L
0
fi(x)f â€²â€²
j (x) dx + Î»n
Z L
0
fi(x)fj(x) dx
#
Î±nj = 0.
(5.7.4)
We can write Equation 5.7.4 as
(A + Î»nB)d = 0,
(5.7.5)
where
aij =
Z L
0
fi(x)f â€²â€²
j (x) dx = âˆ’
Z L
0
f â€²
i(x)f â€²
j(x) dx,
(5.7.6)
bij =
Z L
0
fi(x)fj(x) dx,
(5.7.7)
and the vector d contains the elements Î±nj.
There are several obvious choices for fj(x):
â€¢ Example 5.7.1
The simplest choice for fj(x) = sin(jÏ€x/L). If we select N = 2, Equation 5.7.2 becomes
Ïˆn(x) = Î±n1 sin
Ï€x
L

+ Î±n2 sin
2Ï€x
L

.
(5.7.8)
From Equation 5.7.6 and Equation 5.7.7,
aij = âˆ’
jÏ€
L
2 Z L
0
sin
iÏ€x
L

sin
jÏ€x
L

dx,
i = 1, 2, j = 1, 2;
(5.7.9)
and
bij =
Z L
0
sin
iÏ€x
L

sin
jÏ€x
L

dx,
i = 1, 2, j = 1, 2.
(5.7.10)

Greenâ€™s Functions
287
Performing the integrations, a12 = a21 = b12 = b21 = 0, a11 = âˆ’Ï€2/(2L), a22 = âˆ’2Ï€2/L,
and b11 = b22 = L/2.
Returning to Equation 5.7.5, it becomes

âˆ’Ï€2/2 + Î»nL2/2
0
0
âˆ’2Ï€2 + Î»nL2/2
 
Î±n1
Î±n2

=

0
0

.
(5.7.11)
In order for Equation 5.7.11 to have a unique solution,

âˆ’Ï€2/2 + Î»nL2/2
0
0
âˆ’2Ï€2 + Î»nL2/2
 = 0.
(5.7.12)
Equation 5.7.12 yields 4Î»1 = Î»2 = 4Ï€2/L2.
In summary,
Ïˆ1(x) = sin(Ï€x/L),
Î»1 = Ï€2/L2;
(5.7.13)
and
Ïˆ2(x) = sin(2Ï€x/L),
Î»2 = 4Ï€2/L2,
(5.7.14)
with Î±12 = Î±21 = 0. Here we have chosen that Î±11 = Î±22 = 1.
âŠ“âŠ”
â€¢ Example 5.7.2
Another possible choice for fj(x) involves polynomials of the form (1 âˆ’x/L)(x/L)j
with j = 1, 2. Unlike the previous example, we have nonorthogonal basis functions here.
Note that fj(0) = fj(L) = 0. Therefore, Equation 5.7.2 becomes
Ïˆn(x) = Î±n1(1 âˆ’x/L)(x/L) + Î±n2(1 âˆ’x/L)(x/L)2.
(5.7.15)
From Equation 5.7.6 and Equation 5.7.7,
aij = âˆ’1
L2
Z L
0

1 âˆ’x
L
  x
L
i 
j(j âˆ’1)
 x
L
jâˆ’2
âˆ’j(j + 1)
 x
L
jâˆ’1
dx
(5.7.16)
= 1
L
 j(j âˆ’1)
i + j âˆ’1 âˆ’j(j âˆ’1)
i + j
âˆ’j(j + 1)
i + j
+ j(j + 1)
i + j + 1

,
(5.7.17)
with i = 1, 2 and j = 1, 2. Similarly,
bij =
Z L
0

1 âˆ’x
L
  x
L
i 
1 âˆ’x
L
  x
L
j
dx
(5.7.18)
= L

1
i + j + 1 âˆ’
2
i + j + 2 +
1
i + j + 3

.
(5.7.19)
Performing the computations, a11 = âˆ’1/(3L), a12 = a21 = âˆ’1/(6L), a22 = âˆ’2/(15L),
b11 = L/30, b12 = b21 = L/60, and b22 = L/105.
Returning to Equation 5.7.5, it becomes

âˆ’1/3 + Î»nL2/30
âˆ’1/6 + Î»nL2/60
âˆ’1/6 + Î»nL2/60
âˆ’2/15 + Î»nL2/105
 
Î±n1
Î±n2

=

0
0

.
(5.7.20)

288
Advanced Engineering Mathematics: A Second Course
In order for Equation 5.7.20 to have a unique solution,

âˆ’1/3 + Î»nL2/30
âˆ’1/6 + Î»nL2/60
âˆ’1/6 + Î»nL2/60
âˆ’2/15 + Î»nL2/105
 = 0.
(5.7.21)
Equation 5.7.21 yields Î»1L2 = 10 and Î»2L2 = 42. Note how close these values of Î» are to
those found in the previous example. Returning to Equation 5.7.20, we ï¬nd that Î±11 = 1,
Î±12 = 0, Î±22 = 1, and Î±21 = âˆ’1/2.
In summary,
Ïˆ1(x) =

1 âˆ’x
L
 x
L,
Î»1 = 10
L2 ;
(5.7.22)
and
Ïˆ2(x) = âˆ’1
2

1 âˆ’x
L
 x
L +

1 âˆ’x
L
  x
L
2
,
Î»2 = 42
L2 .
(5.7.23)
Because fj(x) are linearly independent, their use in the Galerkin expansion is quite ac-
ceptable. However, because these functions are not particularly orthogonal, their usefulness
will become more diï¬ƒcult as N increases. Consequently, the choice of orthogonal functions
is often best.
âŠ“âŠ”
How do we employ the Galerkin technique to approximate Greenâ€™s functions? We begin
by considering the inhomogeneous heat conduction problem:
âˆ‚u
âˆ‚t âˆ’âˆ‚2u
âˆ‚x2 = F(x, t),
0 < x < L,
0 < t,
(5.7.24)
with the boundary conditions
u(0, t) = u(L, t) = 0,
0 < t,
(5.7.25)
and the initial condition u(x, 0) = 0, 0 < x < L.
Let us write the solution to this problem as
u(x, t) =
N
X
n=1
cn(t)Ïˆn(x)eâˆ’Î»nt.
(5.7.26)
Direct substitution of Equation 5.7.26 into Equation 5.7.24, followed by multiplying the
resulting equation by fi(x) and integrating from 0 to L, gives
N
X
n=1
cn(t)eâˆ’Î»nt
Z L
0
fi(x)d2Ïˆn
dx2 dx âˆ’
N
X
n=1
dcn
dt âˆ’Î»ncn

eâˆ’Î»nt
Z L
0
fi(x)Ïˆn(x) dx
= âˆ’
Z L
0
fi(x)F(x, t) dx.
(5.7.27)
Because
d2Ïˆn
dx2 + Î»nÏˆn = 0,
(5.7.28)
Equation 5.7.27 simpliï¬es to
N
X
n=1
dcn
dt eâˆ’Î»nt
Z L
0
fi(x)Ïˆn(x) dx =
Z L
0
fi(x)F(x, t) dx = F âˆ—
i (t),
(5.7.29)

Greenâ€™s Functions
289
where i = 1, 2, . . . , N.
We must now ï¬nd cn. We can write Equation 5.7.29 as
N
X
n=1
eineâˆ’Î»nt dcn
dt dx = F âˆ—
i (t),
(5.7.30)
where
ein =
N
X
j=1
Î±njbji.
(5.7.31)
Using linear algebra, we ï¬nd that
eâˆ’Î»nt dcn
dt =
N
X
i=1
pniF âˆ—
i (t),
(5.7.32)
where pni are the elements of an array P = Eâˆ’1 and E = (DB)T . The arrays D and B
consist of elements Î±ij and bij, respectively. Solving Equation 5.7.32, we ï¬nd that
cn(t) = An +
N
X
i=1
pni
Z t
0
F âˆ—
i (Î·)eÎ»nÎ· dÎ·.
(5.7.33)
Because u(x, 0) = 0, cn(0) = 0 and An = 0.
We are now ready to ï¬nd our Greenâ€™s function. Let us set F(x, t) = Î´(x âˆ’Î¾)Î´(t âˆ’Ï„).
Then F âˆ—
i (t) = fi(Î¾)Î´(t âˆ’Ï„) and
cn(t) = H(t âˆ’Ï„)
N
X
i=1
pnifi(Î¾)eÎ»nÏ„.
(5.7.34)
From Equation 5.7.2, Equation 5.7.26, and Equation 5.7.34, we obtain the ï¬nal result that
g(x, t|Î¾, Ï„) = H(t âˆ’Ï„)
N
X
n=1
N
X
j=1
N
X
i=1
Î±njpnifi(Î¾)fj(x)eâˆ’Î»n(tâˆ’Ï„).
(5.7.35)
âŠ“âŠ”
â€¢ Example 5.7.3
In Example 5.5.2, we solved the Greenâ€™s function problem:
âˆ‚g
âˆ‚t âˆ’âˆ‚2g
âˆ‚x2 = Î´(x âˆ’Î¾)Î´(t âˆ’Ï„),
(5.7.36)
with the boundary condition
g(0, t|Î¾, Ï„) = g(L, t|Î¾, Ï„) = 0,
(5.7.37)
and the initial condition g(x, 0|Î¾, Ï„) = 0. There we found the solution (Equation 5.5.34):
g(x, t|Î¾, Ï„) = H(t âˆ’Ï„)
âˆ
X
n=1
Ïˆn(Î¾)Ïˆn(x)eâˆ’k2
n(tâˆ’Ï„),
(5.7.38)

290
Advanced Engineering Mathematics: A Second Course
where we have the orthonormal eigenfunctions
Ïˆn(x) =
p
2/L sin(knx),
kn = nÏ€/L.
(5.7.39)
Let us use the basis function fj(x) = (1 âˆ’x/L)(x/L)j to ï¬nd the approximate Greenâ€™s
function to Equation 5.7.36. Here j = 1, 2, 3, . . . , N,
For N = 2, we showed in Example 5.7.2 that
B = L

1/30
1/60
1/60
1/105

,
D =

1
0
âˆ’1/2
1

.
(5.7.40)
Consequently,
BD =
L
840

28
14
0
1

,
E =
L
840

28
0
14
1

.
(5.7.41)
Using Gaussian elimination,
P = Eâˆ’1 = 1
L

30
0
âˆ’420
840

.
(5.7.42)
Therefore, the two-term approximation to the Greenâ€™s function, Equation 5.7.38, is
g(x, t|Î¾, Ï„) = 30
L
x
L

1 âˆ’x
L
 Î¾
L

1 âˆ’Î¾
L

exp

âˆ’10(t âˆ’Ï„)
L2

H(t âˆ’Ï„)
+
210
L
x
L

1 âˆ’x
L
 Î¾
L

1 âˆ’Î¾
L

âˆ’420
L
x
L

1 âˆ’x
L
  Î¾
L
2 
1 âˆ’Î¾
L

âˆ’420
L
 x
L
2 
1 âˆ’x
L
 Î¾
L

1 âˆ’Î¾
L

+ 840
L
 x
L
2 
1 âˆ’x
L
  Î¾
L
2 
1 âˆ’Î¾
L

Ã— exp

âˆ’42(t âˆ’Ï„)
L2

H(t âˆ’Ï„).
(5.7.43)
For N > 2, hand computations are very cumbersome and numerical computations are
necessary. For a speciï¬c N, we ï¬rst compute the arrays A and B via Equation 5.7.17 and
Equation 5.7.19.
for j = 1:N
for i = 1:N
A(i,j) = j*(j-1)/(i+j-1) - j*(j-1)/(i+j) ...
+ j*(j+1)/(i+j+1) - j*(j+1)/(i+j) ;
B(i,j) = 1/(i+j+1) - 2/(i+j+2) + 1/(i+j+3);
end; end
Next we compute the Î»nâ€™s and corresponding eigenfunctions: [v,d] = eig(A,-B).
Table 5.7.1 gives L2Î»n for several values of N.
Once we found the eigenvalues and eigenvectors, we now compute the matrices D, E,
and P. For convenience we have reordered the eigenvalues so that their numerical value
increases with n. Furthermore, we have set Î±nn equal to one for n = 1, 2, . . . , N.
[lambda,ix] = sort(temp);
for i = 1:N
for j = 1:N
D(i,j) = v(j,ix(i));

Greenâ€™s Functions
291
Table 5.7.1: The Value of L2Î»n for n = 1, 2, . . . , N as a Function of N.
n
Exact
N = 2
N = 3
N = 4
N = 6
N = 8
N = 10
1
9.87
10.00
9.87
9.87
9.87
9.87
9.87
2
39.48
42.00
42.00
39.50
39.48
39.48
39.48
3
88.83
102.13
102.13
89.17
88.83
88.83
4
157.91
200.50
159.99
157.96
157.91
5
246.74
350.96
254.42
247.04
6
355.31
570.53
376.47
356.65
7
483.61
878.88
531.55
8
631.65
1298.03
725.34
9
799.44
1850.98
10
986.96
2548.73
end; end
for i = 1:N
denom = D(i,i);
for j = 1:N
D(i,j) = D(i,j) / denom;
end; end
E = transpose(D*B);
P = inv(E);
Having computed the matrices D and P, our ï¬nal task is the computation of the
Greenâ€™s function using Equation 5.7.35. The MATLAB code is:
phi i(1) = (1-xi)*xi;
for i = 2:N
phi i(i) = xi*phi i(i-1);
end
for ii = 1:idim
x = (ii-1)*dx;
phi j(1) = (1-x)*x;
for j = 2:N
phi j(j) = x*phi j(j-1);
end
for n = 1:N
for j = 1:N
for i = 1:N
g(ii) = g(ii) + D(n,j).*P(n,i).*phi j(j).*phi i(i) ...
.*exp(-lambda(n)*time);
end; end; end
end
In this code the parameter time denotes the quantity (tâˆ’Ï„)/L2. Figure 5.7.1 compares
this approximate Greenâ€™s function for various N against the exact solution. One of the

292
Advanced Engineering Mathematics: A Second Course
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
âˆ’2
0
2
4
6
8
10
x/L
L g(x,t|Î¾,Ï„)
exact
N = 3
N = 6
N = 9
N = 12
Figure 5.7.1: Comparison of the exact Greenâ€™s function Lg(x, t|Î¾, Ï„) for a one-dimensional heat equation
given by Equation 5.7.38 (solid line) and approximate Greenâ€™s functions found by the Galerkin method for
diï¬€erent values of N. Here (t âˆ’Ï„)/L2 = 0.001 and Î¾ = 0.4.
problems with this method is ï¬nding the inverse of the array E.
As N increases, the
accuracy of the inverse becomes poorer.
Further Readings
Beck, J. V., K. D. Cole, A. Haji-Sheikh, and B. Litkouhi, 1992: Heat Conduction Using
Greenâ€™s Functions. Hemisphere Publishing Corp., 523 pp. Detailed study of solving heat
conduction problems via Greenâ€™s functions.
Carslaw, H. S., and J. C. Jaeger, 1959: Conduction of Heat in Solids. Oxford University
Press, Chapter 14. An early classic for ï¬nding the Greenâ€™s function for the heat equation.
Duï¬€y, D. G., 2015: Greenâ€™s Functions with Applications. Chapman & Hall/ CRC, 464 pp.
A source book.
Â¨OziÂ¸sik, M. N., 1993: Heat Conduction. John Wiley & Sons, Chapter 6. A book of how to
solve partial diï¬€erential equations of heat conduction.
Stakgold, I., 1997: Greenâ€™s Functions and Boundary Value Problems. Wiley-Interscience,
720 pp. A systematic approach to boundary-value problems.

âˆ’4
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
4
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
 x
Estimated and true PDF
Chapter 6
Probability
So far in this book we presented mathematical techniques that are used to solve deter-
ministic problemsâ€”problems in which the underlying physical processes are known exactly.
In this and the next chapter we turn to problems in which uncertainty is key.
Although probability theory was ï¬rst developed to explain the behavior of games of
chance,1 its usefulness in the physical sciences and engineering became apparent by the late
nineteenth century. Consider, for example, the biological process of birth and death. If b
denotes the birth rate and d is the death rate, the size of the population P(t) at time t is
P(t) = P(0)e(bâˆ’d)t.
(6.0.1)
Let us examine the situation when P(0) = 1 and b = 2d so that a birth is twice as likely
to occur as a death. Then, Equation 6.0.1 predicts exponential growth with P(t) = edt.
But the ï¬rst event may be a death, a one-in-three chance since d/(b + d) = 1/3, and this
would result in the population immediately becoming extinct. Consequently we see that
for small populations, chance ï¬‚uctuations become important and a deterministic model is
inadequate.
The purpose of this and the next chapter is to introduce mathematical techniques that
will lead to realistic models where chance plays an important role, and show under what
conditions deterministic models will work. In this chapter we present those concepts that
we will need in the next chapter to explain random processes.
1 Todhunter, I., 1949: A History of the Mathematical Theory of Probability from the Time of Pascal
to That of Laplace. Chelsea, 624 pp.; Hald, A., 1990: A History of Probability and Statistics and Their
Applications before 1750. John Wiley & Sons, 586 pp.
293

294
Advanced Engineering Mathematics: A Second Course
6.1 REVIEW OF SET THEORY
Often we must count various objects in order to compute a probability. Sets provide a
formal method to aid in these computations. Here we review important concepts from set
theory.
Sets are collections of objects, such as the number of undergraduate students at a
college. We deï¬ne a set A either by naming the objects or describing the objects. For
example, the set of natural numbers can be either enumerated:
A = {1, 2, 3, 4, . . .},
(6.1.1)
or described:
A = {I : I is an integer and I â‰¥1}.
(6.1.2)
Each object in set A is called an element and each element is distinct. Furthermore, the
ordering of the elements within the set is not important.
Two sets are said to be equal if they contain the same elements and are written A = B.
An element x of a set A is denoted by x âˆˆA. A set with no elements is called a empty
or null set and denoted by âˆ…. On the other hand, a universal set is the set of all elements
under consideration.
A set B is subset of a set A, written B âŠ‚A, if every element in B is also an element
of A. For example, if A = {x : 0 â‰¤x < âˆ} and S = {x : âˆ’âˆ< x < âˆ}, then A âŠ‚S.
We can also use this concept to deï¬ne the equality of sets A and B. Equality occurs when
A âŠ‚B and B âŠ‚A.
The complement of A, written A, is the set of elements in S but not in A. For example,
if A = {x : 0 â‰¤x < âˆ} and S = {x : âˆ’âˆ< x < âˆ}, then A = {x : âˆ’âˆ< x < 0}.
Two sets can be combined together to form a new set. This union of A and B, written
A âˆªB, creates a new set that contains elements that belong to A and/or B. This deï¬nition
can be extended to multiple sets A1, A2, . . . , AN so that the union is the set of elements for
which each element belongs to at least one of these sets. It is written
A1 âˆªA2 âˆªA3 âˆªÂ· Â· Â· âˆªAN =
N
[
i=1
Ai.
(6.1.3)
The intersection of sets A and B, written A âˆ©B, is deï¬ned as the set of elements that
belong to both A and B. This deï¬nition can also be extended to multiple sets A1, A2, . . . , AN
so that the intersection is the set of elements for which each element belongs to all of these
sets. It is written
A1 âˆ©A2 âˆ©A3 âˆ©Â· Â· Â· âˆ©AN =
N
\
i=1
Ai.
(6.1.4)
If two sets A and B have no elements in common, they are said to be disjoint and Aâˆ©B = âˆ….
A popular tool for visualizing set operations is the Venn diagram.2 For sets A and B
Figure 6.1.1 pictorially illustrates A âˆªB, A âˆ©B, A, and A âˆ©B.
With these deï¬nitions a number of results follow: A = A, A âˆªA = S, A âˆ©A = âˆ…,
A âˆªâˆ…= A, A âˆ©âˆ…= âˆ…, A âˆªS = S, A âˆ©S = A, S = âˆ…, and âˆ…= S. Here S denotes the
universal set.
Sets obey various rules similar to those encountered in algebra. They include:
2 Venn, J., 2008: Symbolic Logic. Kessinger, 492 pp.

Probability
295
                                                                                                                                                         
















                                                                                                                                















                                                                                                                                                                                              


















                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                























                        











S
S
S
S
A
B
A
B
A
B
Shaded area:
Shaded area:
B
U
A
Shaded area:
B
A
Shaded area:
A
B
U
U
A
A
Figure 6.1.1: Examples of Venn diagrams for various conï¬gurations of sets A and B. Note that in the
case of the lower right diagram, B âŠ‚A.
1. Commutative properties: A âˆªB = B âˆªA,
A âˆ©B = B âˆ©A.
2. Associate properties: A âˆª(B âˆªC) = (A âˆªB) âˆªC,
A âˆ©(B âˆ©C) = (A âˆ©B) âˆ©C.
3. Distributive properties: Aâˆ©(BâˆªC) = (Aâˆ©B)âˆª(Aâˆ©C),
Aâˆª(Bâˆ©C) = (AâˆªB)âˆ©(AâˆªC).
4. De Morganâ€™s law: A âˆªB = A âˆ©B.
Finally we deï¬ne the size of a set.
Discrete sets may have a ï¬nite number of el-
ements or countably inï¬nite number of elements.
By countably inï¬nite we mean that
we could in theory count the number of elements in the sets. Two simple examples are
A = {1, 2, 3, 4, 5, 6} and A = {1, 4, 16, 64, . . .}. Discrete sets lie in opposition to continuous
sets where the elements are inï¬nite in number and cannot be counted. A simple example
is A = {x : 0 â‰¤x â‰¤2}.
Problems
1. If B âŠ‚A, use Venn diagrams to show that A = B âˆª(B âˆ©A) and B âˆ©(B âˆ©A) = âˆ…. Hint:
Use the Venn diagram in the lower right frame of Figure 6.1.1.
2. Using Venn diagrams, show that A âˆªB = A âˆª(A âˆ©B) and B = (A âˆ©B) âˆª(A âˆ©B). Hint:
For A âˆ©B, use the upper right frame from Figure 6.1.1.
6.2 CLASSIC PROBABILITY
All questions of probability begin with the concept of an experiment where the governing
principle is chance. The set of all possible outcomes of a random experiment is called the
sample space (or universal set); we shall denote it by S. An element of S is called a sample
point. The number of elements in S can be ï¬nite as in the ï¬‚ipping of a coin twice, inï¬nite
but countable such as repeatedly tossing a coin and counting the number of heads, or inï¬nite
and uncountable, as measuring the lifetime of a light bulb.

296
Advanced Engineering Mathematics: A Second Course
Any subset of the sample set S is called an event. If this event contains a single point,
then the event is elementary or simple.
â€¢ Example 6.2.1
Consider an experiment that consists of two steps. In the ï¬rst step, a die is tossed. If
the number of dots on the top of the die is even, a coin is ï¬‚ipped; if the number of dots on
the die is odd, a ball is selected from a box containing blue and green balls. The sample
space is S = {1B, 1G, 2H, 2T, 3B, 3G, 4H, 4T, 5B, 5G, 6H, 6T}. The event A of obtaining a
blue ball is A = {1B, 3B, 5B}, of obtaining a green ball is B = {1G, 3G, 5G}, and obtaining
an even number of dots when the die is tossed is C = {2H, 2T, 4H, 4T, 6H, 6T}. The simple
event of obtaining a 1 on the die and a blue ball is D = {1B}.
âŠ“âŠ”
Equally likely outcomes
An important class of probability problems consists of those whose outcomes are equally
likely. The expression â€œequally likelyâ€ is essentially an intuitive one. For example, if a coin
is tossed it seems reasonable that the coin is just as likely to fall â€œheadsâ€ as to fall â€œtails.â€
Probability seeks to quantify this common sense.
Consider a sample space S of an experiment that consists of ï¬nitely many outcomes
that are equally likely. Then the probability of an event A is
P(A) = Number of points in A
Number of points in S .
(6.2.1)
With this simple deï¬nition we are ready to do some simple problems. An important aid
in our counting is whether we can count a particular sample only once (sampling without
replacement) or repeatedly (sampling with replacement). The following examples illustrate
both cases.
â€¢ Example 6.2.2: Balls drawn from urns with replacement
Imagine the situation where we have an urn that has k red balls and N âˆ’k black balls.
A classic problem asks: What is the chance of two balls being drawn, one after another
with replacement, where the ï¬rst ball is red and the second one is black?
We begin by labeling the k red balls with 1, 2, 3, . . . , k and black balls are numbered
k + 1, k + 2, . . . , N. The possible outcomes of the experiment can be written as a 2-tuple
(z1, z2), where z1 âˆˆ1, 2, 3, . . . , N and z2 âˆˆ1, 2, 3, . . . , N. A successful outcome is a red ball
followed by a black one; we can express this case by E = {(z1, z2) : z1 = 1, 2, . . . , k; z2 =
k + 1, k + 2, . . . , N}. Now the total number of 2-tuples in the sample space is N 2 while the
total number of 2-tuples in E is k(N âˆ’k). Therefore, the probability is
P(E) = k(N âˆ’k)
N 2
= p(1 âˆ’p),
(6.2.2)
where p = k/N.
âŠ“âŠ”

Probability
297
0
10
20
30
40
50
60
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Number of people in crowd
Probability of shared birthdays
Figure 6.2.1: The probability that a pair of individuals in a crowd of n people share the same birthday.
â€¢ Example 6.2.3: Balls drawn from urns without replacement
Let us redo the previous example where the same ball now cannot be chosen twice. We
can express this mathematically by the condition z1 Ì¸= z2. The sample space has N(N âˆ’1)
balls and the number of successful 2-tuples is again k(N âˆ’k). The probability is therefore
given by
P(E) = k(N âˆ’k)
N(N âˆ’1) = k
N
N âˆ’k
N
N
N âˆ’1 = p(1 âˆ’p)
N
N âˆ’1.
(6.2.3)
The restriction that we cannot replace the original ball has resulted in a higher probability.
Why? We have reduced the number of red balls and thereby reduced the chance that we
again selected another red ball while the situation with the black balls remains unchanged.
âŠ“âŠ”
â€¢ Example 6.2.4: The birthday problem3
A classic problem in probability is: What is the chance that at least two individuals
share the same birthday in a crowd of n people? Actually it is easier to solve the comple-
mentary problem: What is the chance that no one in a crowd of n individuals shares the
same birthday?
For simplicity let us assume that there are only 365 days in the year. Each individual
then has a birthday on one of these 365 days. Therefore, there are a total of (365)n possible
outcomes in a given crowd.
Consider now each individual separately. The ï¬rst person has a birthday on one of 365
days. The second person, who cannot have the same birthday, has one of the remaining
364 days. Therefore, if A denotes the event that no two people have the same birthday and
each outcome is equally likely, then
P(A) = n(A)
n(S) = (365)(364) Â· Â· Â· (365 âˆ’n + 1)
(365)n
.
(6.2.4)
To solve the original question, we note that P(A) = 1 âˆ’P(A) where P(A) denotes the
probability that at least two individuals share the same birthday.
3 First posed by von Mises, R., 1939: Â¨Uber Aufteilungs- und Besetzungswahrscheinlichkeiten. Rev. Fac.
Sci. Istambul, 4, 145â€“163.

298
Advanced Engineering Mathematics: A Second Course
t
t
T
T
t
Ï„
2
1
Figure 6.2.2: The graphical solution of whether two fellows can chat online between noon and T minutes
after noon. The shaded area denotes the cases when the two will both be online whereas the rectangle gives
the sample space.
If n = 50, P(A) â‰ˆ0.03 and P(A) â‰ˆ0.97. On the other hand, if n = 23, P(A) â‰ˆ0.493
and P(A) â‰ˆ0.507. Figure 6.2.1 illustrates P(A) as a function of n. Nymann4 computed
the probability that in a group of n people, at least one pair will have the same birthday
with at least one such pair among the ï¬rst k people.
âŠ“âŠ”
In the previous examples we counted the objects in sets A and S. Sometimes we can
deï¬ne these sets only as areas on a graph. This graphical deï¬nition of probability is
P(A) = Area covered by set A
Area covered by set S .
(6.2.5)
The following example illustrates this deï¬nition.
â€¢ Example 6.2.5
Two friends, Joe and Dave, want to chat online but they will log on independently
between noon and T minutes after noon. Because of their schedules, Joe can only wait
t minutes after his log-on while Dave can only spare Ï„ minutes. Neither fellow can stay
beyond T minutes after noon. What is the chance that they will chat?
Let us denote Joeâ€™s log-on time by t1 and Daveâ€™s log-on time by t2. Joe and Dave will
chat if 0 < t2 âˆ’t1 < t and 0 < t1 âˆ’t2 < Ï„. In Figure 6.2.2 we show the situation where
these inequalities are both satisï¬ed. The area of the sample space is T 2. Therefore, from
the geometrical deï¬nition of probability, the probability P(A) that they will chat is
P(A) = T 2 âˆ’(T âˆ’t)2/2 âˆ’(T âˆ’Ï„)2/2
T 2
.
(6.2.6)
âŠ“âŠ”
So far there has been a single event that interests us and we have only had to compute
P(A). Suppose we now have two events that we wish to follow. How are the probabilities
P(A) and P(B) related?
4 Nymann, J. E., 1975: Another generalization of the birthday problem. Math. Mag., 53, 111â€“125.

Probability
299
                                                                                                                                















                        











S
B
A
B
S
B
A
Shaded area:
B
A
Shaded area:
U
U
A
Figure 6.2.3: The Venn diagram used in the derivation of Property 5.
Consider the case of ï¬‚ipping a coin. We could deï¬ne event A as obtaining a head, A =
{head}. Event B could be obtaining a tail, B = {tail}. Clearly A âˆªB = {head, tail} = S,
the sample space. Furthermore, Aâˆ©B = âˆ…and A and B are mutually exclusive. We already
know that P(A) = P(B) = 1
2. But what happens if A âˆ©B is not an empty set?
From our deï¬nition of probability and previous examples, we introduce the following
three basic axions:
Axion 1: P(A), P(B) â‰¥0,
Axion 2: P(S) = 1,
Axion 3: P(A âˆªB) = P(A) + P(B) if A âˆ©B = âˆ….
The ï¬rst two axions are clearly true from the deï¬nition of probability and sample space.
It is the third axion that needs some attention. Here we have two mutually exclusive events
A and B in the sample space S. Because the number of points in A âˆªB equals the number
of points in A plus the number of points in B, n(AâˆªB) = n(A)+n(B). Dividing both sides
of this equation by the number of sample points and applying Equation 6.2.1, we obtain
Axion 3 when A âˆ©B = âˆ….
From these three axioms, the following properties can be written down:
1. P(A) = 1 âˆ’P(A)
2. P(âˆ…) = 0
3. P(A) â‰¤P(B) if A âŠ‚B
4. P(A) â‰¤1
5. P(A âˆªB) + P(A âˆ©B) = P(A) + P(B).

300
Advanced Engineering Mathematics: A Second Course
                                                                                                                                                                                                                                                                                                                                                                                                                                    





























S
A
B
A
B
U
U
A
B
Figure 6.2.4: The Venn diagram that shows that A = (A âˆ©B) âˆª(A âˆ©B).
All of these properties follow readily from our deï¬nition of probability except for Prop-
erty 5 and this is an important one. To prove this property from Axion 3, consider the
Venn diagram shown in Figure 6.2.3. From this ï¬gure we see that
A âˆªB = A âˆª(A âˆ©B)
and
B = (A âˆ©B) âˆª(A âˆ©B).
(6.2.7)
From Axion 3, we have that
P(A âˆªB) = P(A) + P(A âˆ©B),
(6.2.8)
and
P(B) = P(A âˆ©B) + P(A âˆ©B).
(6.2.9)
Eliminating P(A âˆ©B) between Equation 6.2.8 and Equation 6.2.9, we obtain Property 5.
The following example illustrates a probability problem with two events A and B.
â€¢ Example 6.2.6
Consider Figure 6.2.4. From this ï¬gure, we see that A = (A âˆ©B) âˆª(A âˆ©B). Because
A âˆ©B and A âˆ©B are mutually exclusive, then from Axion 3 we have that
P(A) = P(A âˆ©B) + P(A âˆ©B).
(6.2.10)
âŠ“âŠ”
â€¢ Example 6.2.7
A company has 400 employees. Every quarter, 100 of them are tested for drugs. The
companyâ€™s policy is to test everyone at random, whether they have been previously tested
or not. What is the chance that someone is not tested?
The chance that someone will be tested is 1/4. Therefore, the chance that someone
will not be tested is 1 âˆ’1/4 = 3/4.
âŠ“âŠ”
Permutations and combinations
By now it should be evident that your success at computing probabilities lies in correctly
counting the objects in a given set. Here we examine two important concepts for systemic
counting: permutations and combinations.

Probability
301
A permutation consists of ordering n objects without any regard to their order. For
example, the six permutations of the three letters a, b, and c are abc, acb, bac, bca, cab, and
cba. The number of permutations equals n!.
In a combination of given objects, we select one or more objects without regard to
their order. There are two types of combinations: (1) n diï¬€erent objects, taken k at a time,
without repetition, and (2) n diï¬€erent objects, taken k at a time, with repetitions. In the
ï¬rst case, the number of sets that can be made up from n objects, each set containing k
diï¬€erent objects and no two sets containing exactly the same k things, equals
number of diï¬€erent combinations =

n
k

â‰¡
n!
k!(n âˆ’k)!.
(6.2.11)
Using the three letters a, b, and c, there are three combinations, taken two letters at a time,
without repetition: ab, ac, and bc.
In the second case, the number of sets, consisting of k objects chosen from the n objects
and each being used as often as desired, is
number of diï¬€erent combinations =

n + k âˆ’1
k

.
(6.2.12)
Returning to our example using three letters, there are six combinations with repetitions:
ab, ac, bc, aa, bb, and cc.
â€¢ Example 6.2.8
An urn contains r red balls and b blue balls. If a random sample of size m is chosen,
what is the probability that it contains exactly k red balls?
If we choose a random sample of size m, we obtain

r + b
m

possible outcomes. The
number of samples that includes k red balls and mâˆ’k blue balls is

r
k
 
b
m âˆ’k

. There-
fore, the probability that a sample of size m contains exactly k red balls is

r
k
 
b
m âˆ’k


r + b
m

.
âŠ“âŠ”
â€¢ Example 6.2.9
A dog kennel has 50 dogs, including 5 German shepherds. (a) What is the probability of
choosing 3 German shepherds if 10 dogs are randomly selected? (b) What is the probability
of choosing all of the German shepherds in a group of 10 dogs that is chosen at random?
Let S denote the sample space of groups of 10 dogs. The number of those groups
is n(S) = 50!/(10!40!).
Let Ai denote the set of 10 dogs that contain i German shep-
herds. Then the number of groups of 10 dogs that contain i German shepherds is n(Ai) =
10!/[i!(10 âˆ’i)!]. Therefore, the probability that out of 50 dogs, we can select at random 10
dogs that include i German shepherds is
P(Ai) = n(Ai)
n(S) =
10!10!40!
i!(10 âˆ’i)!50!.
(6.2.13)

302
Advanced Engineering Mathematics: A Second Course
Thus, P(A3) = 1.1682 Ã— 10âˆ’8 and P(A5) = 2.453 Ã— 10âˆ’8.
âŠ“âŠ”
â€¢ Example 6.2.10
Consider an urn with n red balls and n blue balls inside. Let R = {r1, r2, . . . , rn} and
B = {b1, b2, . . . , bn}. Then the number of subsets of R âˆªB with n elements is

2n
n

. On
the other hand, any subset of RâˆªB with n elements can be written as the union of a subset
of R with i elements and a subset of B with n âˆ’i elements for some 0 â‰¤i â‰¤n. Because,
for each i, there are

n
i
 
n
n âˆ’i

such subsets, the total number of subsets of red and
blue balls with n elements equals Pn
i=0

n
i
 
n
n âˆ’i

. Since both approaches must be
equivalent,

2n
n

=
n
X
i=0

n
i
 
n
n âˆ’i

=
n
X
i=0

n
i
2
(6.2.14)
because

n
n âˆ’i

=

n
i

.
âŠ“âŠ”
Conditional probability
Often we are interested in the probability of an event A provided event B occurs.
Denoting this conditional probability by P(A|B), its probability is given by
P(A|B) = P(A âˆ©B)
P(B)
,
P(B) > 0,
(6.2.15)
where P(A âˆ©B) is the joint probability of A and B. Similarly,
P(B|A) = P(A âˆ©B)
P(A)
,
P(A) > 0.
(6.2.16)
Therefore,
P(A âˆ©B) = P(A|B)P(B) = P(B|A)P(A),
(6.2.17)
and we obtain the famous Bayesâ€™ rule
P(A|B) = P(B|A)P(A)
P(B)
.
(6.2.18)
â€¢ Example 6.2.11
Consider a box containing 10 pencils. Three of the pencils are defective with broken
lead. If we draw 2 pencils out at random, what is the chance that we will have selected
nondefective pencils?
There are two possible ways of selecting our two pencils: with and without replacement.
Let Event A be that the ï¬rst pencil is not defective and Event B be that the second pencil
is not defective. Regardless of whether we replace the ï¬rst pencil or not, P(A) =
7
10 because

Probability
303
each pencil is equally likely to be picked. If we then replace the ï¬rst pencil, we have the
same situation before any selection was made and P(B|A) = P(A) =
7
10. Therefore,
P(A âˆ©B) = P(A)P(B|A) = 0.49.
(6.2.19)
On the other hand, if we do not replace the ï¬rst selected pencil, P(B|A) = 6
9 because
there is one fewer nondefective pencils. Consequently,
P(A âˆ©B) = P(A)P(B|A) = 7
10 Ã— 6
9 = 14
30 < 0.49.
(6.2.20)
Why do we have a better chance of obtaining defective pencils if we donâ€™t replace the
ï¬rst one? Our removal of that ï¬rst, nondefective pencil has reduced the uncertainty because
we know that there are relatively more defective pencils in the remaining 9 pencils. This
reduction in uncertainty must be reï¬‚ected in a reduction in the chances that both selected
pencils will be nondefective.
âŠ“âŠ”
Law of total probability
Conditional probabilities are useful because they allow us to simplify probability cal-
culations. Suppose we have n mutually exclusive events A1, A2, . . . , An whose probabilities
sum to unity, then
P(B) = P(B|A1)P(A1) + P(B|A2)P(A2) + Â· Â· Â· + P(B|An)P(An),
(6.2.21)
where B is an arbitrary event, and P(B|Ai) is the conditional probability of B assuming
Ai. In other words, the law (or formula) of total probability expresses the total probability
of an outcome that can be realized via several distinct events.
â€¢ Example 6.2.12
There are three boxes, each containing a diï¬€erent number of light bulbs. The ï¬rst box
has 10 bulbs, of which 4 are dead. The second has 6 bulbs, of which one is dead. Finally,
there is a third box of eight bulbs, of which 3 bulbs are dead. What is the probability of
choosing a dead bulb if a bulb is randomly chosen from one of the three boxes?
The probability of choosing a dead bulb is
P(D) = P(D|B1)P(B1) + P(D|B2)P(B2) + P(D|B3)P(B3)
(6.2.22)
=
1
3
  4
10

+
1
3
 1
6

+
1
3
 3
8

= 113
360.
(6.2.23)
If we had only one box with a total 24 bulbs, of which 8 were dead, then our chance of
choosing a dead bulb would be 1/3 > 113/360.
âŠ“âŠ”
Independent events
If events A and B satisfy the equation
P(A âˆ©B) = P(A)P(B),
(6.2.24)

304
Advanced Engineering Mathematics: A Second Course
they are called independent events. From Equation 6.2.15 and Equation 6.2.16, we see that
if Equation 6.2.24 holds, then
P(A|B) = P(A),
P(B|A) = P(B),
(6.2.25)
assuming that P(A) Ì¸= 0 and P(B) Ì¸= 0. Therefore, the term â€œindependentâ€ refers to the
fact that the probability of A does not depend on the occurrence or non-occurrence of B,
and vice versa.
â€¢ Example 6.2.13
Imagine some activity where you get two chances to be successful (for example, jumping
for fruit still on a tree or shooting basketballs). If each attempt is independent and the
probability of success 0.6 is the same for each trial, what is the probability of success after
(at most) two tries?
There are two ways of achieving success. We can be successful in the ï¬rst attempt
with P(S1) = 0.6 or we can fail and then be successful on the second attempt: P(F1 âˆ©
S2) = P(F1)P(S2) = (0.4)(0.6) = 0.24, since each attempt is independent.
Therefore,
the probability of achieving success in two tries is 0.6 + 0.24 = 0.84. Alternatively, we
can compute the probability of failure in two attempts: P(F1 âˆ©F2) = 0.16.
Then the
probability of success with two tries would be the complement of the probability of two
failures: 1 âˆ’0.16 = 0.84.
âŠ“âŠ”
â€¢ Example 6.2.14
Consider the tossing of a fair die. Let event A denote the tossing of a 2 or 3. Then
P(A) = P({2, 3}) = 1
3. Let event B denote tossing an odd number, B = {1, 3, 5}. Then
P(B) = 1
2.
Now A âˆ©B = {3} and P(A âˆ©B) = 1
6. Because P(A âˆ©B) = P(A)P(B), events A and
B are independent.
âŠ“âŠ”
Often we can characterize each outcome of an experiment consisting of n experiments
as either a â€œsuccessâ€ or a â€œfailure.â€
If the probability of each individual success is p,
then the probability of k successes and n âˆ’k failures is pk(1 âˆ’p)nâˆ’k. Because there are
n!/[k!(n âˆ’k)!] ways of achieving these k successes, the probability of an event having k
successes in n independent trials is
Pn(k) =
n!
k!(n âˆ’k)!pk(1 âˆ’p)nâˆ’k,
(6.2.26)
where p is the probability of a success during one of the independent trials.
â€¢ Example 6.2.15
What is the probability of having two boys in a four-child family?
Let us assume that the probability of having a male is 0.5. Taking the birth of one
child as a single trial,
P4(2) = 4!
2!2!
1
2
4
= 3
8.
(6.2.27)
Note that this is not 0.5, as one might initially guess.

Probability
305
Problems
1. For the following experiments, describe the sample space:
(a) ï¬‚ipping a coin twice
(b) selecting two items out of three items {a, b, c} without replacement
(c) selecting two items out of three items {a, b, c} with replacement
(d) selecting three balls, one by one, from a box that contains four blue balls and ï¬ve green
balls without replacement
(e) selecting three balls, one by one, from a box that contains four blue balls and ï¬ve green
balls with replacement.
2. Consider two fair dice. What is the probability of throwing them so that the dots sum
to seven?
3. In throwing a fair die, what is the probability of obtaining a one or two on the top side
of the cube?
4. What is the probability of getting heads exactly (a) twice or (b) thrice if you ï¬‚ip a fair
coin 6 times?
5.
An urn contains six red balls, three blue balls, and two green balls.
Two balls are
randomly selected.
What is the sample space for this experiment?
Let X denote the
number of green balls selected. What are the possible values of X? Calculate P(X = 1).
6. Consider an urn with 30 blue balls and 50 red balls in it. These balls are identical except
for their color. If they are well mixed and you draw 3 balls without replacement, what is
the probability that the balls are all of the same color?
7. A deck of cards has 52 cards, including 4 jacks and 4 tenâ€™s. What is the probability of
selecting a jack or ten?
8. Two boys and two girls take their place on a stage to receive an award. What is the
probability that the boys take the two end seats?
9. A lottery consists of posting a 3-digit number given by selecting 3 balls from 10 balls,
each ball having the number from 1 to 10. The balls are not replaced after they are drawn.
What are your chances of winning the lottery if the order does not matter? What are your
chances of winning the lottery if the order does matter? Write a short MATLAB code and
verify your results. You may want to read about the MATLAB intrinsic function randperm.
10. A circle of radius 1 is inscribed in a square with sides of length 2. A point is selected
at random in the square in such a manner that all the subsets of equal area of the square
are equally likely to contain the point. What is the probability that it is inside the circle?
11. In a rural high school, 20% of the students play football and 10% of them play football
and wrestle. If Ed, a randomly selected student of this high school, played football, what
is the probability that he also wrestles for his high school?

306
Advanced Engineering Mathematics: A Second Course
12. You have a well-shuï¬„ed card deck. What is the probability the second card in the deck
is an ace?
13. We have two urns: One has 4 red balls and 6 green balls, the other has 6 red and 4
green. We toss a fair coin. If heads, we pick a random ball from the ï¬rst urn, if tails from
the second. What is the probability of getting a red ball? How do your results compare
with the probability of getting a red ball if all of the red and green balls had been placed
into a single urn?
14. A customer decides between two dinners: a â€œcheapâ€ one and an â€œexpensiveâ€ one. The
probability that the customer chooses the expensive meal is P(E) = 0.2. A customer who
chooses the expensive meal likes it with a 80% probability P(L|E) = 0.8. A customer who
chooses the cheap meal dislikes it with 70% probability P(D|C) = 0.7.
(a) Compute the probability that a customer (1) will choose a cheap meal, (2) will be
disappointed with an expensive meal, and (3) will like the cheap meal.
(b) Use the law of total probability to compute the probability that a customer will be
disappointed.
(c) If a customer found his dinner to his liking, what is the probability that he or she chose
the expensive meal? Hint: Use Bayesâ€™ theorem.
15. Suppose that two points are randomly and independently selected from the interval
(0, 1). What is the probability the ï¬rst one is greater than 1/4, and the second one is less
than 3/4? Check your result using rand in MATLAB.
16. A certain brand of electronics chip is found to fail prematurely in 1% of all cases. If
three of these chips are used in three independent sets of equipment, what is the probability
that (a) all three will fail prematurely, (b) that two will fail prematurely, (c) that one will
fail prematurely, and (d) that none will fail?
Project: Experimenting with MATLABâ€™s Intrinsic Function rand
The MATLAB function rand can be used in simulations where sampling occurs with
replacement. If we write X = rand(1,100), the vector X contains 100 elements whose values
vary between 0 and 1. Therefore, if you wish to simulate a fair die, then we can set up the
following table:
0 < X < 1/6
die with one dot showing
1/6 < X < 1/3
die with two dots showing
1/3 < X < 1/2
die with three dots showing
1/2 < X < 2/3
die with four dots showing
2/3 < X < 5/6
die with ï¬ve dots showing
5/6 < X < 1
die with six dots showing.
We can then write MATLAB code that counts the number of times that we obtain a one or
two. Call this number n. Then the probability that we would obtain one or two dots on a
fair die is n/100. Carry out this experiment and compare your answer with the result from
Problem 2. What occurs as you do more and more experiments?

Probability
307
Table 6.2.1: The Probability of a Male (Female) Freshman Having Always Had New Male
(Female) Roommates from a Pool of m Other Male (Female) Freshmen after n Random
Reassignments during His (Her) Freshman Year. The Numerator Is the Probability for a
Two-Person Room; the Denominator Is the Probability for a Three-Person Room.
Total Number of Freshmen
n
6
12
18
24
30
36
42
48
2
0.8000
0.3000
0.9091
0.6545
0.9412
0.7721
0.9565
0.8300
0.9655
0.8645
0.9714
0.8874
0.9756
0.9037
0.9787
0.9158
4
0.1920
0.0000
0.5409
0.4550
0.6839
0.1792
0.7594
0.3015
0.8059
0.3981
0.8374
0.4729
0.8601
0.5325
0.8773
0.5801
6
0.0000
0.0000
0.1878
0.0000
0.3692
0.0073
0.4910
0.0385
0.5750
0.0867
0.6357
0.1407
0.6815
0.1943
0.7172
0.2450
8
0.0000
0.0000
0.0310
0.0000
0.1405
0.0000
0.2524
0.0012
0.3459
0.0075
0.4214
0.0212
0.4825
0.0411
0.5325
0.0658
Project: Experimenting with
MATLABâ€™s Intrinsic Function randperm
MATLABâ€™s intrinsic function randperm(m) creates a random ordering of the numbers
from 1 to m. If you execute perm = randperm(365), this would produce a vector of length
365 and each element has a value lying between 1 and 365. If you repeat the process, you
would obtain another list of 365 numbers but they would be in a diï¬€erent order.
Let us simulate the birthday problem. Invoking the randperm command, use the ï¬rst
element to simulate the birthday of student 1 in a class of N students. Repeatedly invoking
this command, create vector birthdays that contains the birthdays of the N students. Then
ï¬nd out if any of the days are duplicates of another. (Hint: You might want to explore the
MATLAB command unique.) Repeating this experiment many times, compute the chance
that a class of size N has at least two students that have the same birthday. Compare your
results with Equation 6.2.4. What occurs as the number of experiments increases?
Project: The Roommate Problem
You are a freshman at a small all-male (all-female) college with m other freshmen. For
esprit de corps, the administration requires that n times during your freshman year, you
are randomly (with equal probability) assigned new roommates. The administration does
not, however, require that you have never roomed with any of them previously.
(a) Assuming that there are 2 freshmen per room (so that m + 1 is even), what is the
probability that all of your roommates during the year have never roomed with you before?
Verify your answer by writing a MATLAB script that simulates this housing practice. I used
the MATLAB intrinsic functions randi(m,1,n), unique and length and ran the simulation
10 million times.
(b) Assuming that there are 3 freshmen per room (so that m + 1 is a multiple of 3),
what is the probability that all of your roommates during the year have never roomed with

308
Advanced Engineering Mathematics: A Second Course
you before? Verify your answer by writing a MATLAB script that simulates this housing
practice. I used the MATLAB intrinsic functions randperm, unique and length and ran the
simulation 10 million times.
6.3 DISCRETE RANDOM VARIABLES
In the previous section we presented the basic concepts of probability. In high school
algebra you were introduced to the concept of a variableâ€”a quantity that could vary unlike
constants and parameters. Here we extend this idea to situations where the variations are
due to randomness.
A random variable is a single-valued real function that assigns a real number, the value,
to each sample point t of S. The variable can be discrete, such as the ï¬‚ipping of a coin,
or continuous, such as the lifetime of a light bulb. The sample space S is the domain of
the random variable X(t), and the collection of all numbers X(t) is the range. Two or
more sample points can give the same value of X(t), but we will never allow two diï¬€erent
numbers in the range of X(t) for a given t.
The term â€œrandom variableâ€ is probably a poor one. Consider the simple example of
tossing a coin. A random variable that describes this experiment is
X[si] =

1,
s1 = head,
0,
s2 = tail.
(6.3.1)
An obvious question is: What is random about Equation 6.3.1? If a head is tossed, we
obtain the answer one; if a tail is tossed, we obtain a zero. Everything is well deï¬ned;
there is no element of chance here. The randomness arises from the tossing of the coin.
Until the experiment (tossing of the coin) is performed, we do not know the outcome of
the experiment and the value of the random variable. Therefore, a random variable is a
variable that may take diï¬€erent values if a random experiment is conducted and its value is
not known in advance.
We begin our study of random variables by focusing on those arising from discrete
events. If X is discrete, X assumes only ï¬nitely many or countably many values: x1, x2, x3,
. . .. For each possible value of xi, there is a corresponding positive probability pX[x1] =
P(X = x1), pX[x2] = P(X = x2), . . . given by the probability mass function. For values of
x diï¬€erent from xi, say x1 < x < x2, the probability mass function equals zero. Therefore,
we have that
pX[xi] =
n pi,
x = xi,
0,
otherwise,
(6.3.2)
where i = 1, 2, 3, . . ..
At this point it is convenient to introduce several special classes or types of random
variables. First we have independent random variables where the realization of one does
not aï¬€ect the probability distribution of the other. Of equal importance are identically
distributed random variables where the random variables have the same probability dis-
tribution. Finally we can combine both properties into independent identically distributed
(i.i.d.) random variables. This last class occurs repeatedly in common applications.
â€¢ Example 6.3.1
Consider a fair die. We can describe the results from rolling this fair die via the discrete
random variable X, which has the possible values xi = 1, 2, 3, 4, 5, 6 with the probability
pX[xi] = 1
6 each. Note that 0 â‰¤pX[xi] < 1 here. Furthermore,
6
X
i=1
pX[xi] = 1.
(6.3.3)

Probability
309
1
2
3
4
5
6
1
2
3
4
5
6
x
x
F  (x)
X
X
1/6
1/6
1/3
1/2
2/3
5/6
1
p  [x]
Figure 6.3.1: The probability mass function for a fair die.
Figure 6.3.1 illustrates the probability mass function.
âŠ“âŠ”
â€¢ Example 6.3.2
Let us now modify Example 6.3.1 so that
X[si] =
( 1,
si = 1, 2,
2,
si = 3, 4,
3,
si = 5, 6.
(6.3.4)
The probability mass function becomes
pX[1] = pX[2] = pX[3] = 1
3.
(6.3.5)
âŠ“âŠ”
â€¢ Example 6.3.3
Consider the probability mass function:
pX[xn] =

k(1/2)n,
n = 0, 1, 2, . . . ,
0,
otherwise.
(6.3.6)
Let us (a) ï¬nd the value of k, (b) ï¬nd P(X = 2), (c) ï¬nd P(X â‰¤2), and (d) P(X â‰¥1).
From the properties of probability mass function,
k
âˆ
X
n=0
1
2
n
= k
1
1 âˆ’1
2
= 2k = 1.
(6.3.7)
Therefore, k = 1
2. Note that 0 â‰¤pX[xn] â‰¤1.
Having found k, we immediately have
P(X = 2) = pX[x2] = 1
8,
(6.3.8)
P(X â‰¤2) = pX[x0] + pX[x1] + pX[x2] = 7
8,
(6.3.9)
and
P(X â‰¥1) = 1 âˆ’P(X = 0) = 1
2.
(6.3.10)
âŠ“âŠ”

310
Advanced Engineering Mathematics: A Second Course
Some Properties of the Probability Mass Function pX[xi]
0 â‰¤pX[xk] < 1,
pX[x] = 0
if
x Ì¸= xk,
k = 1, 2, . . .
X
n
pX[xn] = 1
FX(x) = P(X â‰¤x) =
X
xkâ‰¤x
pX[xk]
P(a < x â‰¤b) =
X
a<xkâ‰¤b
pX[xk]
Having introduced the probability mass function, an alternative means of describing the
probabilities of a discrete random variable is the cumulative distribution function.
It is
deï¬ned as
FX(x) = P(X â‰¤x),
âˆ’âˆ< x < âˆ.
(6.3.11)
It is computed via
FX(x) =
X
xiâ‰¤x
pX[xi] =
X
xiâ‰¤x
pi.
(6.3.12)
Consequently, combining Equation 6.3.11 and Equation 6.3.12, we obtain
P(a < x â‰¤b) =
X
a<xiâ‰¤b
pi.
(6.3.13)
Equation 6.3.13 gives the probability over the interval (a, b].
â€¢ Example 6.3.4
A Bernoulli experiment is a random experiment, the outcome of which is a success
or failure. Consider now a sequence of independent Bernoulli trials with probability p of
success from trial to trial. This sequence is observed until the ï¬rst success occurs. Let X
denote a random variable that equals the trial number on which the ï¬rst success occurs.
The probability mass function is then
pX[xn] = (1 âˆ’p)nâˆ’1p,
n = 1, 2, 3, . . . .
(6.3.14)
Let us compute the cumulative distribution function.
For geometric series, we begin by noting that
âˆ
X
n=0
arn =
âˆ
X
n=1
arnâˆ’1 =
a
1 âˆ’r,
|r| < 1.
(6.3.15)
Next we check Equation 6.3.14 and determine whether it is a valid probability mass function.
It is because
âˆ
X
n=1
pX[xn] =
âˆ
X
n=1
(1 âˆ’p)nâˆ’1p =
p
1 âˆ’(1 âˆ’p) = 1,
(6.3.16)

Probability
311
1
F  (x)
1âˆ’p
x
p
X
1
Figure 6.3.2: The cumulative distribution function for a Bernoulli random variable.
where we used Equation 6.3.15. Next, we note that
P(X > m) =
âˆ
X
n=m+1
(1 âˆ’p)nâˆ’1p = (1 âˆ’p)mp
1 âˆ’(1 âˆ’p) = (1 âˆ’p)m.
(6.3.17)
Therefore,
FX(x) = P(X â‰¤m) = 1 âˆ’P(X > m) = 1 âˆ’(1 âˆ’p)m,
(6.3.18)
where m = 1, 2, 3, . . . .
âŠ“âŠ”
â€¢ Example 6.3.5: Generating discrete random variables via MATLAB
In this example we show how to generate a discrete random variable using MATLABâ€™s
intrinsic function rand. This MATLAB command produces random, uniformly distributed
(equally probable) reals over the interval (0, 1). How can we use this function, when in the
case of discrete random variables, we have only integer values, such as k = 1, 2, 3, 4, 5, 6, in
the case of tossing a die?5
Consider the Bernoulli random variable X = k, k = 0, 1. As you will show in your
homework, it has the cumulative distribution function of
FX(x) =
( 0,
x < 0,
1 âˆ’p,
0 â‰¤x < 1,
1,
1 â‰¤x.
(6.3.19)
See Figure 6.3.2.
Imagine now a program that includes the MATLAB function rand, which yields the
value t. Then, if 0 < t â‰¤1 âˆ’p, Figure 6.3.2 gives us that X = 0. On the other hand, if
1 âˆ’p < t < 1, then X = 1. Thus, to obtain M realizations of the Bernoulli random variable
X, the MATLAB code would read for a given p:
clear;
5 This technique is known as the inverse transform sampling method. See pages 85â€“102 in Devroye, L.,
1986: Non-Uniform Random Variable Generation. Springer-Verlag, 843 pp.

312
Advanced Engineering Mathematics: A Second Course
0
5
10
15
20
25
30
35
40
45
50
âˆ’0.5
0
0.5
1
1.5
 k
 xk
(a)
0
20
40
60
80
100
120
140
160
180
200
0
0.2
0.4
0.6
0.8
1
 M
 est. pX[1]
(b)
Figure 6.3.3: (a) Outcomes of the Bernoulli random variable generated by the MATLAB function rand. (b)
The computed value of the probability mass function pX[1] as a function of M realization of the Bernoulli
random variable. The dashed line is the line for the exact answer p = 0.4.
for i = 1:M
t = rand(1,1);
if (t <= 1-p) X(i,1) = 0;
else
X(i,1) = 1;
end; end
The end product of this code creates a vector X of length M consisting of a random variable
with either zeros or ones. This is shown in Figure 6.3.3(a) when p = 0.4.
Once we have generated this random variable, we can use its relative frequency to
compute its probability mass function and cumulative distribution function from
Ë†pX[xk] = Number of outcomes equal to k
M
,
(6.3.20)
and
Ë†FX(x) = Number of outcomes â‰¤x
M
.
(6.3.21)
In Figure 6.3.3(b) we have computed the value of Ë†pX[1]. Clearly it should equal p. As this
ï¬gure shows, we obtain poor results when M is small, with Ë†pX[1] moving randomly above
and below the correct answer. As M becomes larger, our estimate improves.
Problems
1. The Bernoulli distribution has the probability mass function
pX[xk] = P(X = k) = pk(1 âˆ’p)1âˆ’k,
k = 0, 1,
where 0 â‰¤p â‰¤1. (a) Show that this distribution is a valid probability mass function. (b)
Find its cumulative distribution function.

Probability
313
x
x
p  (x)
X
dx
Figure 6.4.1: A probability density function.
2.
An experiment is performed where a digit, ranging from 0 to 9, is repeatedly and
randomly chosen. If X denotes the times that this experiment must be repeated until the
digit 0 is selected, ï¬nd P(X).
3. A scientiï¬c company needs a programmer who knows an unusual programming language.
If only 5% of programmers know this language, how many programmers should the company
interview to have a 75% chance of ï¬nding such a programmer?
6.4 CONTINUOUS RANDOM VARIABLES
In the previous section we examined random variables that can assume only certain
discrete values. Here we extend the concept of random variables so that they can take on
values over a continuous interval. Typical examples of continuous random variables include
the noisy portion of the voltage within an ampliï¬er, the phase of a propagating wave, and
the amount of precipitation.
An important quantity that we introduced in the previous section was the probability
mass function. What is the corresponding function for continuous random variables? From
the fundamental concepts of probability, we know that the probability of a continuous
variable assuming one speciï¬c value out of its possible range values equals zero; it is merely
one point out of an inï¬nite number of points in the sample space. On the other hand, there
is a ï¬nite probability that the value assumed by the random variable X will lie within an
arbitrarily small interval dx and this probability will depend on the length of the interval.
Another factor that should inï¬‚uence the probability is the value of x. There is no
reason why the probability of X should be independent of x. Consequently, an equation
for probability in the interval x < X â‰¤x + dx requires a function pX(x), which acts as a
weighting function and models the relative frequency behavior of X. For these reasons, the
probability that a continuous random variable X will assume a value lying between x and
x + dx is given by
P(x < X â‰¤x + dx) = pX(x) dx.
(6.4.1)
Figure 6.4.1 illustrates a possible example of pX(x) where the shaded area equals the prob-
ability P(x < X â‰¤x + dx). Clearly the function pX(x) = P(x < X â‰¤x + dx)/dx has the
dimension of probability per inï¬nitesimal interval dx and is called, for that reason, the prob-
ability density. Furthermore, although pX(x) dx â‰¤1, this does not mean that pX(x) â‰¤1.
A family of random variables having the same probability density is identically distributed.
The function pX(x) must also satisfy several additional conditions. Because probability
cannot be negative, pX(x) â‰¥0 of all x. Furthermore, as Figure 6.4.1 suggests, if we add

314
Advanced Engineering Mathematics: A Second Course
Some Properties of the Probability Density Function pX(x)
pX(x) â‰¥0,
Z âˆ
âˆ’âˆ
pX(x) dx = 1
P(a < X â‰¤b) =
Z b
a
pX(x) dx
up all of the possible values of x, then we have a certain event.
We can express this
mathematically by
Z âˆ
âˆ’âˆ
pX(x) dx = 1.
(6.4.2)
Thus, a probability density has the properties given by Equation 6.4.1 and Equation 6.4.2.
It must also be a single-valued function of x. Note that these conditions do not require that
pX(x) is a continuous function of x.
Let us now consider the probability P(a < X â‰¤b) where a and b are constants. If
we subdivide the range of x between a and b into inï¬nitesimal intervals (x, x + dx), the
probability that the random variable will assume a value from one such interval is given by
Equation 6.4.1. The probability that the variable will assume a value in the interval (a, b)
equals the sum of the probabilities from each subinterval between a and b and is given by
the area under the curve p(x) between x = a and x = b. Therefore,
P(a < X â‰¤b) =
Z b
a
pX(x) dx.
(6.4.3)
If a = âˆ’âˆ, we have that
P(X â‰¤b) =
Z b
âˆ’âˆ
pX(x) dx.
(6.4.4)
Alternatively, setting b = âˆ,
P(a < X) =
Z âˆ
a
pX(x) dx.
(6.4.5)
From Equation 6.4.3 we also have
P(X > a) = 1 âˆ’P(X < a) = 1 âˆ’
Z a
âˆ’âˆ
pX(x) dx =
Z âˆ
a
pX(x) dx.
(6.4.6)
From Equation 6.4.4 we now deï¬ne
FX(x) = P(X â‰¤x) =
Z x
âˆ’âˆ
pX(Î¾) dÎ¾.
(6.4.7)
This function FX(x) is called the cumulative distribution function, or simply the distribution
function, of the random variable X. Clearly,
pX(x) = F â€²
X(x).
(6.4.8)
Therefore, from the properties of pX(x), we have that (1) FX(x) is a nondecreasing function
of x, (2) FX(âˆ’âˆ) = 0, (3) FX(âˆ) = 1, and (4) P(a < X â‰¤b) = FX(b) âˆ’FX(a).

Probability
315
â€¢ Example 6.4.1
The continuous random variable X has the probability density function
pX(x) =

k
 x âˆ’x2
,
0 < x < 1,
0,
otherwise.
(6.4.9)
What must be the value of k? What is the cumulative distribution function? What is
P(X < 1/2)?
From Equation 6.4.2, we have that
Z âˆ
âˆ’âˆ
pX(x) dx = k
Z 1
0
 x âˆ’x2
dx = k x2
2 âˆ’x3
3

1
0
= k
6.
(6.4.10)
Therefore, k must equal 6.
Next, we note that
FX(x) = P(X â‰¤x) =
Z x
âˆ’âˆ
pX(Î¾) dÎ¾.
(6.4.11)
If x < 0, FX(x) = 0. For 0 < x < 1, then
FX(x) = 6
Z x
0
 Î¾ âˆ’Î¾2
dÎ¾ = 6
Î¾2
2 âˆ’Î¾3
3

x
0
= 3x2 âˆ’2x3.
(6.4.12)
Finally, if x > 1,
FX(x) = 6
Z 1
0
 Î¾ âˆ’Î¾2
dÎ¾ = 1.
(6.4.13)
In summary,
FX(x) =
(
0,
0 â‰¤x,
3x2 âˆ’2x3,
0 < x â‰¤1,
1,
1 < x.
(6.4.14)
Because P(X â‰¤x) = FX(x), we have that P(X < 1
2) = 1
2 and P(X > 1
2) = 1 âˆ’P(X <
1
2) = 1
2.
âŠ“âŠ”
â€¢ Example 6.4.2: Generating continuous random variables via MATLAB6
In the previous section we showed how the MATLAB function rand can be used to gen-
erate outcomes for a discrete random variable. Similar considerations hold for a continuous
random variable.
Consider the exponential random variable X. Its probability density function is
pX(x) =

0,
x < 0,
Î»eâˆ’Î»x,
0 < x,
(6.4.15)
where Î» > 0. For homework you will show that the corresponding cumulative distribution
function is
FX(x) =

0,
x â‰¤0,
1 âˆ’eâˆ’Î»x,
0 < x.
(6.4.16)
6 This technique is known as the inverse transform sampling method. See pages 27â€“39 in Devroye, L.,
1986: Non-Uniform Random Variable Generation. Springer-Verlag, 843 pp.

316
Advanced Engineering Mathematics: A Second Course
F  (x)
X
1
2
1
x
3
Figure 6.4.2: The cumulative distribution function for an exponential random variable.
Figure 6.4.2 illustrates this cumulative density function when Î» = 1. How can we use these
results to generate a MATLAB code that produces an exponential random variable?
Recall that both MATLAB function rand and the cumulative distribution function pro-
duce values that vary between 0 and 1. Given a value from rand, we can compute the
corresponding X = x, which would give the same value from the cumulative distribution
function. In short, we are creating random values for the cumulative distribution function
and using those values to give the exponential random variable via
X = x = âˆ’ln(1 âˆ’rand) /Î»,
(6.4.17)
where we have set FX(x) = rand. Therefore, the MATLAB code to generate exponential
random variables for a particular lambda is
clear;
for i = 1:M
t = rand(1,1);
X(i,1) = -log(1-t) / lambda;
end
where M is the number of experiments that we run. In Figure 6.4.3(a) we illustrate the ï¬rst
200 outcomes from our numerical experiment to generate an exponential random variable.
To compute the probability density function we use the ï¬nite diï¬€erence approximation
of Equation 6.4.1, or
Ë†p(x0) = Number of outcomes in [x0 âˆ’âˆ†x/2, x0 + âˆ†x/2]
Mâˆ†x
,
(6.4.18)
where âˆ†x is the size of the bins into which we collect the various outcomes. Figure 6.4.3(b)
illustrates this numerical estimation of the probability density function in the case of an
exponential random variable. The function Ë†pX(x) was created from the MATLAB code:
clear;
delta x = 0.2; lambda = 1; M = 1000; % Initialize âˆ†x, Î» and M
% sample M outcomes from the uniformly distributed distribution
t = rand(M,1);
% generate the exponential random variable
x = - log(1-t)/lambda;

Probability
317
0
5
10
15
20
25
30
35
40
45
50
0
1
2
3
4
5
6
 k
 xk
(a)
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
0
0.2
0.4
0.6
0.8
1
 x
 est. pX(x)
(b)
Figure 6.4.3: (a) Outcomes of a numerical experiment to generate an exponential random variable using
the MATLAB function rand. (b) The function Ë†pX(x) given by Equation 6.4.18 as a function of x for an
exponential random variable with M = 1000. The dashed black line is the exact probability density function.
% create the various bins [x0 âˆ’âˆ†x/2, x0 + âˆ†x/2]
bincenters=[delta x/2:delta x:5];
bins=length(bincenters); % count the number of bins
% now bin the M outcomes into the various bins
[n,x out] = hist(x,bincenters);
n = n / (delta x*M); % compute the probability per bin
bar h = bar(x out,n); % create the bar graph
bar child = get(bar h,â€™Childrenâ€™);
set(bar child,â€™CDataâ€™,n);
colormap(Autumn);
Problems
1. The probability density function for the exponential random variable is
pX(x) =

0,
x < 0,
Î»eâˆ’Î»x,
0 < x,
with Î» > 0. Find its cumulative distribution function.
2. Given the probability density function
pX(x) =

kx,
0 < x < 2,
0,
otherwise,

318
Advanced Engineering Mathematics: A Second Course
S
0.35
0.4
0.45
0.5
0.55
0.6
0.65
probability density
0
2
4
6
8
10
12
14
Figure 6.4.4: Computed probability density function for the sum S = (X1 + X2 + X3 + Â· Â· Â· + X100)/100,
where Xi is the ith sample from a uniform distribution.
where k is a constant, (a) compute the value of k, (b) ï¬nd the cumulative density function
FX(x), and (c) ï¬nd the P(1 < X â‰¤2).
3. Given the probability density function
pX(x) =

k (1 âˆ’|x|) ,
|x| < 1,
0,
|x| > 1,
where k is a constant, (a) compute the value of k and (b) ï¬nd the cumulative density
function FX(x).
Project: Central Limit Theorem
Consider the sum S = (X1 + X2 + X3 + Â· Â· Â· + X100)/100, where Xi is the ith sample
from a uniform distribution.
Step 1: Write a MATLAB program to compute the probability density function of S. See
Figure 6.4.4.
Step 2: The central limit theorem states the distribution of the sum (or average) of a
large number of independent, identically distributed random variables will be approximately
normal, regardless of the underlying distribution. Do your numerical results agree with this
theorem?
6.5 MEAN AND VARIANCE
In the previous two sections we explored the concepts of the random variable and
distribution.
Here we introduce two parameters, mean and variance, that are useful in
characterizing a distribution.

Probability
319
The mean ÂµX is deï¬ned by
ÂµX = E(X) =
ï£±
ï£´
ï£´
ï£²
ï£´
ï£´
ï£³
X
k
xk pX[xk],
X discrete,
Z âˆ
âˆ’âˆ
x pX(x) dx,
X continuous.
(6.5.1)
The mean provides the position of the center of the distribution. The operator E(X), which
is called the expectation of X, gives the average value of X that one should expect after many
trials.
Two important properties involve the expectation of the sum and product of two ran-
dom variables X and Y . The ï¬rst one is
E(X + Y ) = E(X) + E(Y ).
(6.5.2)
Second, if X and Y are independent random variables, then
E(XY ) = E(X)E(Y ).
(6.5.3)
The proofs can be found elsewhere.7
The variance provides the spread of a distribution. It is computed via
Ïƒ2
X = Var(X) = E{[X âˆ’E(X)]2},
(6.5.4)
or
Ïƒ2
X =
ï£±
ï£´
ï£´
ï£²
ï£´
ï£´
ï£³
X
k
(xk âˆ’ÂµX)2pX[xk],
X discrete,
Z âˆ
âˆ’âˆ
(x âˆ’ÂµX)2pX(x) dx,
X continuous.
(6.5.5)
If we expand the right side of Equation 6.5.4, an alternative method for ï¬nding the variance
is
Ïƒ2
X = Var(X) = E(X2) âˆ’[E(X)]2,
(6.5.6)
where
E(Xn) =
ï£±
ï£´
ï£´
ï£²
ï£´
ï£´
ï£³
X
k
xn
k pX[xk],
X discrete,
Z âˆ
âˆ’âˆ
xnpX(x) dx,
X continuous.
(6.5.7)
â€¢ Example 6.5.1: Mean and variance of M equally likely outcomes
Consider the random variable X = k where k = 1, 2, . . . , M. If each event has an
equally likely outcome, pX[xk] = 1/M. Then the expected or average or mean value is
ÂµX = 1
M
M
X
k=1
xk = M(M + 1)
2M
= M + 1
2
.
(6.5.8)
7 For example, Kay, S. M., 2006: Intuitive Probability and Random Processes Using MATLAB. Springer,
833 pp. See Sections 7.7 and 12.7.

320
Advanced Engineering Mathematics: A Second Course
Note that the mean does not equal any of the possible values of X. Therefore, the expected
value need not equal a value that will be actually observed.
Turning to the variance,
Var(X) = (M + 1) [(2M + 1)/6 âˆ’(M + 1)/4]
(6.5.9)
= (M + 1) [4M + 2 âˆ’3M âˆ’3] /12
(6.5.10)
= (M + 1)(M âˆ’1)/12 = (M 2 âˆ’1)/12,
(6.5.11)
because
E(X2) = 1
M
M
X
k=1
x2
k = M(M + 1)(2M + 1)
6M
= (M + 1)(2M + 1)
6
.
(6.5.12)
We used Equation 6.5.6 to compute the variance.
âŠ“âŠ”
â€¢ Example 6.5.2
Let us ï¬nd the mean and variance of the random variable X whose probability density
function is
pX(x) =

kx,
0 < x < 1,
0,
otherwise.
(6.5.13)
From Equation 6.5.1, we have that
ÂµX = E(X) =
Z 1
0
x(kx) dx = kx3
3

1
0
= k
3.
(6.5.14)
From Equation 6.5.6, the variance of X is
Ïƒ2
X = Var(X) = E(X2) âˆ’[E(X)]2 =
Z 1
0
x2(kx) dx âˆ’k2
9 = kx4
4

1
0
âˆ’k2
9 = k
4 âˆ’k2
9 .(6.5.15)
âŠ“âŠ”
â€¢ Example 6.5.3: Characteristic functions
The characteristic function of a random variable is deï¬ned by
Ï†X(Ï‰) = E[exp(iÏ‰X)].
(6.5.16)
If X is a discrete random variable, then
Ï†X(Ï‰) =
âˆ
X
k=âˆ’âˆ
pX[xk]eikÏ‰.
(6.5.17)
On the other hand, if X is a continuous random variable,
Ï†X(Ï‰) =
Z âˆ
âˆ’âˆ
pX(x)eiÏ‰x dx,
(6.5.18)
the inverse Fourier transform (times 2Ï€) of the Fourier transform, pX(x).

Probability
321
Characteristic functions are useful for computing various moments of a random variable
via
E(Xn) = 1
in
dnÏ•X(Ï‰)
dÏ‰n

Ï‰=0
.
(6.5.19)
This follows by taking repeated diï¬€erentiation of Equation 6.5.16 and then evaluating the
diï¬€erentiation at Ï‰ = 0.
Consider, for example, the exponential probability density function pX(x) = Î»eâˆ’Î»x
with x, Î» > 0. A straightforward calculation gives
Ï†X(Ï‰) =
Î»
Î» âˆ’Ï‰i.
(6.5.20)
Substituting Equation 6.5.20 into Equation 6.5.19 yields
E(Xn) = n!
Î»n .
(6.5.21)
In particular,
E(X) = 1
Î»
and
E(X2) = 2
Î»2 .
(6.5.22)
Consequently, ÂµX = 1/Î» and Var(X) = E(X2) âˆ’Âµ2
X = 1/Î»2.
âŠ“âŠ”
â€¢ Example 6.5.4: Characteristic function for a Gaussian distribution
Let us ï¬nd the characteristic function for the Gaussian distribution and then use that
characteristic function to compute the mean and variance.
Because
pX(x) =
1
âˆš
2Ï€ Ïƒ eâˆ’(xâˆ’Âµ)2/(2Ïƒ2),
(6.5.23)
the characteristic function equals
Ï†X(Ï‰) =
1
âˆš
2Ï€ Ïƒ
Z âˆ
âˆ’âˆ
eâˆ’(xâˆ’Âµ)2/(2Ïƒ2)+iÏ‰x dx
(6.5.24)
= eiÏ‰Âµâˆ’Ïƒ2Ï‰2/2

1
âˆš
2Ï€ Ïƒ
Z âˆ
âˆ’âˆ
exp

âˆ’(x âˆ’Âµ âˆ’iÏ‰Ïƒ2)2
2Ïƒ2

dx

(6.5.25)
= eiÏ‰Âµâˆ’Ïƒ2Ï‰2/2
(6.5.26)
because the quantity within the wavy brackets equals one.
Given this characteristic function, Equation 6.5.26, we have that
Ï†â€²
X(Ï‰) = (iÂµ âˆ’Ïƒ2Ï‰)eiÏ‰Âµâˆ’Ïƒ2Ï‰2/2.
(6.5.27)
Therefore, Ï†â€²
X(0) = iÂµ and from Equation 6.5.19, ÂµX = E(X) = Âµ. Furthermore,
Ï†â€²â€²
X(Ï‰) = (iÂµ âˆ’Ïƒ2Ï‰)2eiÏ‰Âµâˆ’Ïƒ2Ï‰2/2 âˆ’Ïƒ2eiÏ‰Âµâˆ’Ïƒ2Ï‰2/2.
(6.5.28)
Consequently, Ï†â€²â€²
X(0) = âˆ’Âµ2 âˆ’Ïƒ2 and Var(X) = E(X2) âˆ’Âµ2
X = Ïƒ2.

322
Advanced Engineering Mathematics: A Second Course
â€¢ Example 6.5.5: (Weak) law of large numbers
One of the reasons why independent identically distributed (i.i.d.) random variables
play such a large role in probability and statistics lies in the (weak) law of large numbers. If
X1, X2, X3, . . . , Xn denote i.i.d. random variables and An = 1
n
Pn
i=1 Xi, then P(|An âˆ’Âµ| â‰¥
Ç«) â†’0, as n â†’âˆfor any Ç« > 0. How is this law useful in daily life? Let us observe some
random variable many times and take the average of these observations. The law of large
numbers predicts that this average will converges to a single value, namely the mean.
Problems
1. Let X(s) denote a discrete random variable associated with a fair coin toss. Then
X(s) =

0,
s = tail,
1,
s = head.
Find the expected value and variance of this random variable.
2. The geometric random variable X has the probability mass function:
pX[xk] = P(X = k) = p(1 âˆ’p)kâˆ’1,
k = 1, 2, 3, . . . .
Find its mean and variance. Hint:
âˆ
X
k=1
krkâˆ’1 =
1
(1 âˆ’r)2 ,
âˆ
X
k=2
k(k âˆ’1)rkâˆ’2 =
2
(1 âˆ’r)3 ,
|r| < 1,
and E(X2) = E[X(X âˆ’1)] + E(X).
3. Given
pX(x) =

kx(2 âˆ’x)
0 < x < 2,
0,
otherwise,
(a) ï¬nd k and (b) its mean and variance.
4. Given the probability density
pX(x) = (a2 âˆ’x2)Î½âˆ’1
2 ,
Î½ > âˆ’1
2,
ï¬nd its characteristic function using integral tables.
For the following distributions, ï¬rst ï¬nd their characteristic functions. Then compute the
mean and variance using Equation 6.5.19.
5. Binomial distribution:
pX[xk] =

n
k

pkqnâˆ’k,
0 < p < 1,
where q = 1 âˆ’p. Hint: Use the binomial theorem to simplify Equation 6.5.17.

Probability
323
6. Poisson distribution:
pX[xk] = eâˆ’Î» Î»k
k! ,
0 < Î».
7. Geometric distribution:
pX[xk] = qkp,
0 < p < 1,
where q = 1 âˆ’p.
8. Uniform distribution:
pX(x) = H(x âˆ’a) âˆ’H(x âˆ’b)
b âˆ’a
,
b > a > 0.
Project: MATLABâ€™s Intrinsic Function mean and var
MATLAB has the special commands mean and var to compute the mean and variance,
respectively, of the random variable X.
Use the MATLAB command randn to create a
random variable X(n) of length N. Then, ï¬nd the mean and variance of X(n). How do these
parameters vary with N?
Project: Monte Carlo Integration and Importance Sampling
Consider the integral I =
R 1
0
âˆš
1 âˆ’x2 dx = Ï€/4. If we were to compute it numerically
by the conventional midpoint rule, the approximate value is given by
IN = 1
N
N
X
n=1
f(xn),
(1)
where f(x) =
âˆš
1 âˆ’x2 and xn = (n âˆ’1/2)/N. For N = 10, 50, 100, and 500, the absolute
value of the relative error is 2.7 Ã— 10âˆ’3, 2.4 Ã— 10âˆ’4, 8.6 Ã— 10âˆ’5, and 7.7 Ã— 10âˆ’6, respectively.
Monte Carlo integration is a simple alternative method for doing the numerical inte-
gration using random sampling. It is a particularly powerful technique for approximating
complicated integrals. Here you will explore a simple one-dimensional version of this scheme.
Consider the random variable:
IM = 1
M
M
X
m=1
f(xm),
(2)
where xm is the mth sample point taken from the uniform distribution. IM is a random
variable because it is a function of the random variable xm. Therefore,
E(IM) = 1
M
M
X
m=1
E[f(xm)] = 1
M
M
X
m=1
Z 1
0
f(x)p(x) dx
= 1
M
M
X
m=1
Z 1
0
f(x) dx =
Z 1
0
f(x) dx = I,
because p(x), the probability of the uniform distribution, equals 1. Furthermore, as we
increase the number of samples M, IM approaches I. By the strong law of large numbers,

324
Advanced Engineering Mathematics: A Second Course
âˆ’0.3
âˆ’0.2
âˆ’0.1
0
0.1
0.2
0
1
2
3
4
5
6
M = 10
probability density
âˆ’0.1
âˆ’0.05
0
0.05
0.1
0
2
4
6
8
10
12
14
M = 50
âˆ’0.08
âˆ’0.06
âˆ’0.04
âˆ’0.02
0
0.02
0.04
0.06
0.08
0
5
10
15
20
M = 100
approximate âˆ’ exact value of the integral
probability density
âˆ’0.04
âˆ’0.03
âˆ’0.02
âˆ’0.01
0
0.01
0.02
0.03
0.04
0
10
20
30
40
50
M = 500
approximate âˆ’ exact value of the integral
Figure 6.5.1: The probability density function arising from using Monte Carlo integration to compute
R 1
0
âˆš
1 âˆ’x2 dx for various values of M.
this limit is guaranteed to converge to the exact solution: P(limMâ†’âˆIM âˆ’I) = 1. Equation
(2) is not the midpoint rule because the uniform grid xn has been replaced by randomly
spaced grid points.
Step 1: Write a MATLAB program that computes IM for various values of M when xm is
selected from a uniform distribution. By running your code thousands of times, ï¬nd the
probability density as a function of the diï¬€erence between IM and I. Compute the mean
and variance of IM. How does the variance vary with M? See Figure 6.5.1.
The reason why standard Monte Carlo integration is not particularly good is the fact
that we used a uniform distribution. A better idea would be to sample from regions where
the integrand is larger. This is the essence of the concept of importance sampling: That
certain values of the input random variable xm in a simulation have more impact on the
parameters being estimated than others.
We begin by noting that
I =
Z 1
0
f(x) dx =
Z 1
0
f(x)
p1(x)p1(x) dx,
where p1(x) is a new probability density function that replaces the uniform probability
distribution and is relatively larger when f(x) is larger and relatively smaller when f(x) is
smaller.
The question now becomes how to compute p1(x). We shall use the VEGAS algo-
rithm, which constructs p1(x) by sampling f(x) K times, where K < M. Within each kth
subinterval we assume that there are M/K uniformly distributed points. Therefore,
p1(xm) =
K f(sm)
PK
k=1 f(sk)
,
where sk is the center point of the kth subinterval within which the mth point is located.
For each m, we must ï¬nd xm. This is done in two steps: First we randomly choose the kth

Probability
325
âˆ’0.04
âˆ’0.03
âˆ’0.02
âˆ’0.01
0
0.01
0.02
0.03
0
10
20
30
40
50
K = 5
probability density
âˆ’0.03
âˆ’0.02
âˆ’0.01
0
0.01
0.02
0
10
20
30
40
50
60
70
80
K = 10
âˆ’0.02 âˆ’0.015 âˆ’0.01 âˆ’0.005
0
0.005
0.01
0.015
0
20
40
60
80
100
120
140
K = 20
approximate âˆ’ exact value of the integral
probability density
âˆ’0.015
âˆ’0.01
âˆ’0.005
0
0.005
0.01
0
50
100
150
200
250
300
350
K = 50
approximate âˆ’ exact value of the integral
Figure 6.5.2: The probability density function arising from using importance sampling with Monte Carlo
integration to compute R 1
0
âˆš
1 âˆ’x2 dx for various values of K and M = 100.
subinterval using a uniform distribution. Then we randomly choose the point xm within
that subinterval using a uniform distribution. Therefore, our modiï¬ed integration scheme
becomes
IM = 1
M
M
X
m=1
f(xm)
p1(xm).
(3)
Now,
E(IM) = 1
M
M
X
m=1
E
 f(xm)
p1(xm)

= 1
M
M
X
m=1
Z 1
0
f(x)
p1(x)p1(x)p2(x) dx
= 1
M
M
X
m=1
Z 1
0
f(x) dx =
Z 1
0
f(x) dx = I,
because p2(x) = 1.
Step 2: Write a MATLAB program that computes IM for various values of K for a ï¬xed value
of M. Recall that you must ï¬rst select the subdivision using the MATLAB function rand
and then the value of xm within the subdivision using a uniform distribution. By running
your code thousands of times, ï¬nd the probability density as a function of the diï¬€erence
between IM and I. Compute the mean and variance of IM. How does the variance vary
with M? See Figure 6.5.2.
6.6 SOME COMMONLY USED DISTRIBUTIONS
In the previous sections we introduced the concept of probability distributions and their
description via mean and variance. In this section we focus on some special distributions,
both discrete and continuous, that appear often in engineering.

326
Advanced Engineering Mathematics: A Second Course
Bernoulli distribution
Consider an experiment where the outcome can be classiï¬ed as either a success or
failure. The probability of a success is p and the probability of a failure is 1âˆ’p. Then these
â€œBernoulli trialsâ€ have a random variable X associated with them where the probability
mass function is given by
pX[xk] = P(X = k) = pk(1 âˆ’p)1âˆ’k,
k = 0, 1,
(6.6.1)
where 0 â‰¤p â‰¤1. From Equation 6.3.12 the cumulative density function of the Bernoulli
random variable X is
FX(x) =
(
0,
x < 0,
1 âˆ’p,
0 â‰¤x < 1,
1,
1 â‰¤x.
(6.6.2)
The mean and variance of the Bernoulli random variable X are
ÂµX = E(X) = p,
and
Ïƒ2
X = Var(X) = p(1 âˆ’p).
(6.6.3)
â€¢ Example 6.6.1
A simple pass and fail process is taking a ï¬nal exam, which can be modeled by a
Bernoulli distribution. Suppose a class passed a ï¬nal exam with the probability of 0.75. If
X denotes the random variable that someone passed the exam, then
E(X) = p = 0.75,
and
Var(X) = p(1 âˆ’p) = (0.75)(0.25) = 0.1875. (6.6.4)
âŠ“âŠ”
Geometric distribution
Consider again an experiment where we either have success with probability p or failure
with probability 1âˆ’p. This experiment is repeated until the ï¬rst success occurs. Let random
variable X denote the trial number on which this ï¬rst success occurs. Its probability mass
function is
pX[xk] = P(X = k) = p(1 âˆ’p)kâˆ’1,
k = 1, 2, 3, . . . .
(6.6.5)
From Equation 6.3.12 the cumulative density function of this geometric random variable X
is
FX(x) = P(X â‰¤x) = 1 âˆ’(1 âˆ’p)k.
(6.6.6)
The mean and variance of the geometric random variable X are
ÂµX = E(X) = 1
p,
and
Ïƒ2
X = Var(X) = 1 âˆ’p
p2
.
(6.6.7)

Probability
327
â€¢ Example 6.6.2
A particle within an accelerator has the probability 0.01 of hitting a target material.
(a) What is the probability that the ï¬rst particle to hit the target is the 50th? (b) What is
the probability that the target will be hit by any particle?
P(ï¬rst particle to hit is the 50th) = 0.01(0.99)49 = 0.0061.
(6.6.8)
P(target hit by any of ï¬rst 50th particles) =
50
X
n=1
0.01(0.99)nâˆ’1 = 0.3950. (6.6.9)
âŠ“âŠ”
â€¢ Example 6.6.3
The police ticket 5% of parked cars. Assuming that the cars are ticketed independently,
ï¬nd the probability of 1 ticket on a block with 7 parked cars.
Each car is a Bernoulli trial with P(ticket) = 0.05. Therefore,
P(1 ticket on block) = P(1 ticket in 7 trials) =

7
1

(0.95)6(0.05) = 0.2573.
(6.6.10)
âŠ“âŠ”
Binomial distribution
Consider now an experiment in which n independent Bernoulli trials are performed and
X represents the number of successes that occur in the n trials. In this case the random
variable X is called binomial with parameters (n, p) with a probability mass function given
by
pX[xk] = P(X = k) =

n
k

pk(1 âˆ’p)nâˆ’k,
k = 0, 1, . . . , n,
(6.6.11)
where 0 â‰¤p â‰¤1, and

n
k

=
n!
k!(n âˆ’k)!,
(6.6.12)
the binomial coeï¬ƒcient. The term pk arises from the k successes while (1 âˆ’p)nâˆ’k is due
to the failures. The binomial coeï¬ƒcient gives the number of ways that we pick those k
successes from the n trials.
The corresponding cumulative density function of X is
FX(x) =
n
X
k=0

n
k

pk(1 âˆ’p)nâˆ’k,
n â‰¤x < n + 1.
(6.6.13)
The mean and variance of the binomial random variable X are
ÂµX = E(X) = np,
and
Ïƒ2
X = Var(X) = np(1 âˆ’p).
(6.6.14)

328
Advanced Engineering Mathematics: A Second Course
A Bernoulli random variable is the same as a binomial random variable when the parameters
are (1, n).
â€¢ Example 6.6.4
Let us ï¬nd the probability of rolling the same side of a die (say, the side with N dots
on it) at least 3 times when a fair die is rolled 4 times.
During our 4 tosses, we could obtain no rolls with N dots on the side (k = 0), one roll
with N dots (k = 1), two rolls with N dots (k = 2), three rolls with N dots (k = 3), or
four rolls with N dots (k = 4). If we deï¬ne A as the event of rolling a die so that the side
with N dots appears at least three times, then we must add the probabilities for k = 3 and
k = 4. Therefore,
P(A) = pX[x3] + pX[x4] =

4
3

p3(1 âˆ’p)1 +

4
4

p4(1 âˆ’p)0
(6.6.15)
= 4!
3!1!p3(1 âˆ’p)1 + 4!
4!0!p4(1 âˆ’p)0 = 0.0162
(6.6.16)
because p = 1
6.
âŠ“âŠ”
â€¢ Example 6.6.5
If 10 random binary digits are transmitted, what is the probability that more than
seven 1â€™s are included among them?
Let X denote the number of 1â€™s among the 10 digits. Then
P(X > 7) = P(X = 8) + P(X = 9) + P(X = 10) = pX[x8] + pX[x9] + pX[x10]
(6.6.17)
=

10
8
 1
2
8 1
2
2
+

10
9
 1
2
9 1
2
1
+

10
10
 1
2
10 1
2
0
(6.6.18)
= (45 + 10 + 1)
1
2
10
=
56
1024.
(6.6.19)
âŠ“âŠ”
Poisson distribution
The Poisson probability distribution arises as an approximation for the binomial dis-
tribution as n â†’âˆand p â†’0 such that np remains ï¬nite. To see this, let us rewrite the
binomial distribution as follows:
P(X = k) =
n!
k!(n âˆ’k)!
Î»
n
k 
1 âˆ’Î»
n
nâˆ’k
= n(n âˆ’1)(n âˆ’2) Â· Â· Â· (n âˆ’k + 1)
nk
Î»n
n!
(1 âˆ’Î»/n)n
(1 âˆ’Î»/n)k ,
(6.6.20)
if Î» = np. For ï¬nite Î»,
lim
nâ†’âˆ

1 âˆ’Î»
n
k
â†’1,
lim
nâ†’âˆ

1 âˆ’Î»
n
n
â†’eâˆ’Î»,
(6.6.21)
and
lim
nâ†’âˆ
n(n âˆ’1)(n âˆ’2) Â· Â· Â· (n âˆ’k + 1)
nk
â†’1.
(6.6.22)

Probability
329
Therefore, for large n, small p and moderate Î», we can approximate the binomial distribution
by the Poisson distribution:
pX[xk] = P(X = k) = eâˆ’Î» Î»k
k! ,
k = 0, 1, . . . .
(6.6.23)
The corresponding cumulative density function of X is
FX(x) = eâˆ’Î»
n
X
k=0
Î»k
k! ,
n â‰¤x < n + 1.
(6.6.24)
The mean and variance of the Poisson random variable X are
ÂµX = E(X) = Î»,
and
Ïƒ2
X = Var(X) = Î».
(6.6.25)
In addition to this approximation, the Poisson distribution is the probability distribution
for a Poisson process. But that has to wait for the next chapter.
â€¢ Example 6.6.6
Consider a student union on a campus. On average 3 persons enter the union per
minute. What is the probability that, during any given minute, 3 or more persons will
enter the union?
To make use of Poissonâ€™s distribution to solve this problem, we must have both a large
n and a small p with the average Î» = np = 3. Therefore, we divide time into a large number
of small intervals so that n is large while the probability that someone will enter the union
is small. Assuming independence of events, we have a binomial distribution with large n.
Let A denote the event that 3 or more persons will enter the union, then
P(A) = pX[0] + pX[1] + pX[2] = eâˆ’3
30
0! + 31
1! + 32
2!

= 0.423.
(6.6.26)
Therefore, P(A) = 1 âˆ’P(A) = 0.577.
âŠ“âŠ”
Uniform distribution
The continuous random variable X is called uniform if its probability density function
is
pX(x) =

1/(b âˆ’a),
a < x < b,
0,
otherwise.
(6.6.27)
The corresponding cumulative density function of X is
FX(x) =
(
0,
x â‰¤a,
(x âˆ’a)/(b âˆ’a),
a < x < b,
1,
b â‰¤x.
(6.6.28)
The mean and variance of a uniform random variable X are
ÂµX = E(X) = 1
2(a + b),
and
Ïƒ2
X = Var(X) = (b âˆ’a)2
12
.
(6.6.29)

330
Advanced Engineering Mathematics: A Second Course
Uniform distributions are used when we have no prior knowledge of the actual probability
density function and all continuous values in some range appear equally likely.
Exponential distribution
The continuous random variable X is called exponential with parameter Î» > 0 if its
probability density function is
pX(x) =

Î»eâˆ’Î»x,
x > 0,
0,
x < 0.
(6.6.30)
The corresponding cumulative density function of X is
FX(x) =

1 âˆ’eâˆ’Î»x,
x â‰¥0,
0,
x < 0.
(6.6.31)
The mean and variance of an exponential random variable X are
ÂµX = E(X) = 1/Î»,
and
Ïƒ2
X = Var(X) = 1/Î»2.
(6.6.32)
This distribution has the interesting property that is â€œmemoryless.â€ By memoryless,
we mean that for a nonnegative random variable X, then
P(X > s + t|X > t) = P(X > s),
(6.6.33)
where x, t â‰¥0. For example, if the lifetime of a light bulb is exponentially distributed, then
the light bulb that has been in use for some hours is as good as a new light bulb with regard
to the amount of time remaining until it fails.
To prove this, from Equation 6.2.4, Equation 6.6.33 becomes
P(X > s + t and X > t)
P(X > t)
= P(X > s),
(6.6.34)
or
P(X > s + t and X > t) = P(X > t)P(X > s),
(6.6.35)
since P(X > s + t and X > t) = P(X > s + t). Now, because
P(X > s + t) = 1 âˆ’
h
1 âˆ’eâˆ’Î»(s+t)i
= eâˆ’Î»(s+t),
(6.6.36)
P(X > s) = 1 âˆ’
 1 âˆ’eâˆ’Î»s
= eâˆ’Î»s,
(6.6.37)
and
P(X > t) = 1 âˆ’
 1 âˆ’eâˆ’Î»t
= eâˆ’Î»t.
(6.6.38)
Therefore, Equation 6.6.35 is satisï¬ed and X is memoryless.
â€¢ Example 6.6.7
A component in an electrical circuit has an exponentially distributed failure time with
a mean of 1000 hours. Calculate the time so that the probability of the time to failure is
less than 10âˆ’3.

Probability
331
Let the exponential random variable X = k have the units of hours. Then Î» = 10âˆ’3.
From the deï¬nition of the cumulative density function,
FX(xt) = P(X â‰¤xt) = 0.001,
and
1 âˆ’exp(âˆ’Î»xt) = 0.001.
(6.6.39)
Solving for xt,
xt = âˆ’ln(0.999)/Î» = 1.
(6.6.40)
âŠ“âŠ”
â€¢ Example 6.6.8
A computer contains a certain component whose time (in years) to failure is given by
the random variable T distributed exponentially with Î» = 1/5. If 5 of these components
are installed in diï¬€erent computers, what is the probability that at least 2 of them will still
work at the end of 8 years?
The probability that a component will last 8 years or longer is
P(T > 8) = eâˆ’8/5 = 0.2019,
(6.6.41)
because Î» = 1/5.
Let X denote the number of components functioning after 8 years. Then,
P(X â‰¥2) = 1 âˆ’P(X = 0) âˆ’P(X = 1)
(6.6.42)
= 1 âˆ’

5
0

(0.2019)0(0.7981)5 âˆ’

5
1

(0.2019)1(0.7981)4
(6.6.43)
= 0.2666.
(6.6.44)
âŠ“âŠ”
Normal (or Gaussian) distribution
The normal distribution is the most important continuous distribution. It occurs in
many applications and plays a key role in the study of random phenomena in nature.
A random variable X is called a normal random variable if its probability density
function is
pX(x) = eâˆ’(xâˆ’Âµ)2/(2Ïƒ2)
âˆš
2Ï€ Ïƒ
,
(6.6.45)
where the mean and variance of a normal random variable X are
ÂµX = E(X) = Âµ,
and
Ïƒ2
X = Var(X) = Ïƒ2.
(6.6.46)
The distribution is symmetric with respect to x = Âµ and its shape is sometimes called
â€œbell shaped.â€ For small Ïƒ2 we obtain a high peak and steep slope while with increasing Ïƒ2
the curve becomes ï¬‚atter and ï¬‚atter.
The corresponding cumulative density function of X is
FX(x) =
1
âˆš
2Ï€ Ïƒ
Z x
âˆ’âˆ
eâˆ’(Î¾âˆ’Âµ)2/(2Ïƒ2) dÎ¾ =
1
âˆš
2Ï€
Z (xâˆ’Âµ)/Ïƒ
âˆ’âˆ
eâˆ’Î¾2/2 dÎ¾.
(6.6.47)

332
Advanced Engineering Mathematics: A Second Course
The integral in Equation 6.6.46 must be evaluated numerically. It is convenient to introduce
the probability integral:
Î¦(z) =
1
âˆš
2Ï€
Z z
âˆ’âˆ
eâˆ’Î¾2/2 dÎ¾.
(6.6.48)
Note that Î¦(âˆ’z) = 1 âˆ’Î¦(z). Therefore,
FX(x) = Î¦
x âˆ’Âµ
Ïƒ

.
(6.6.49)
and
P(a < X â‰¤b) = FX(b) âˆ’FX(a).
(6.6.50)
Consider now the intervals consisting of one Ïƒ, two Ïƒ, and three Ïƒ around the mean Âµ.
Then, from Equation 6.6.50,
P(Âµ âˆ’Ïƒ < X â‰¤Âµ + Ïƒ) = 0.68,
(6.6.51)
P(Âµ âˆ’2Ïƒ < X â‰¤Âµ + 2Ïƒ) = 0.955,
(6.6.52)
and
P(Âµ âˆ’3Ïƒ < X â‰¤Âµ + 3Ïƒ) = 0.997.
(6.6.53)
Therefore, approximately
2
3 of the values will be distributed between Âµ âˆ’Ïƒ and Âµ + Ïƒ,
approximately 95% of the values will be distributed between Âµâˆ’2Ïƒ and Âµ+2Ïƒ, and almost
all values will be distributed between Âµ âˆ’3Ïƒ and Âµ + 3Ïƒ. For most uses, then, all values
will lie between Âµ âˆ’3Ïƒ and Âµ + 3Ïƒ, the so-called â€œthree-sigma limits.â€
As stated earlier, the mean and variance of a normal random variable X are
ÂµX = E(X) = Âµ,
and
Ïƒ2
X = Var(X) = Ïƒ2.
(6.6.54)
The notation N(Âµ; Ïƒ) commonly denotes that X is normal with mean Âµ and variance
Ïƒ2. The special case of a normal random variable Z with zero mean and unit variance,
N(0, 1), is called a standard normal random variable.
Problems
1.
Four coins are tossed simultaneously.
Find the probability function for the random
variable X that gives the number of heads. Then compute the probabilities of (a) obtaining
no heads, (b) exactly one head, (c) at least one head, and (d) not less than four heads.
2. A binary source generates the digits 1 and 0 randomly with equal probability. (a) What
is the probability that three 1â€™s and three 0â€™s will occur in a six-digit sequence? (b) What
is the probability that at least three 1â€™s will occur in a six-digit sequence?
3. Show that the probability of exactly n heads in 2n tosses of a fair coin is
pX[xn] = 1 Â· 3 Â· 5 Â· Â· Â· 2n âˆ’1
2 Â· 4 Â· 6 Â· Â· Â· 2n
.
4. If your cell phone rings, on average, 3 times between noon and 3 P.M., what is the
probability that during that time period you will receive (a) no calls, (b) 6 or more calls, and
(c) not more than 2 calls? Assume that the probability is given by a Poisson distribution.

Probability
333
5. A company sells blank DVDs in packages of 10. If the probability of a defective DVD is
0.001, (a) what is the probability that a package contains a defective DVD? (b) what is the
probability that a package has two or more defective DVDs?
6. A school plans to oï¬€er a course on probability in a classroom that contains 20 seats.
From experience they know that 95% of the students who enroll actually show up. If the
school allows 22 students to enroll before the session is closed, what is the probability of
the class being oversubscribed?
7. The lifetime (in hours) of a certain electronic device is a random variable T having a
probability density function pT (t) = 100H(tâˆ’100)/t2. What is the probability that exactly
3 of 5 such devices must be replaced within the ï¬rst 150 hours of operation? Assume that
the events that the ith device must be replaced within this time are independent.
6.7 JOINT DISTRIBUTIONS
In the previous sections we introduced distributions that depended upon a single ran-
dom variable. Here we generalize these techniques for two random variables. The range of
the two-dimensional random variable (X, Y ) is RXY = {(x, y); Î¾ âˆˆS and X(Î¾) = x, Y (Î¾) =
y}.
Discrete joint distribution
Let X and Y denote two discrete random variables deï¬ned on the same sample space
(jointly distributed). The function pXY [xi, yj] = P[X = xi, Y = yj] is the joint probability
mass function of X and Y . As one might expect, pXY [xi, yj] â‰¥0.
Let the sets of possible values of X and Y be A and B. If xi Ì¸âˆˆA or yj Ì¸âˆˆB, then
pXY [xi, yj] = 0. Furthermore,
X
xiâˆˆA,yjâˆˆB
pXY [xi, yj] = 1.
(6.7.1)
The marginal probability functions of X and Y are deï¬ned by
pX[xi] =
X
yjâˆˆB
pXY [xi, yj],
(6.7.2)
and
pY [yj] =
X
xiâˆˆA
pXY [xi, yj].
(6.7.3)
If X and Y are independent random variables, then pXY [xi, yj] = pX[xi]Â· pY [yj].
â€¢ Example 6.7.1
A joint probability mass function is given by
pXY [xi, yj] =

k(xi + 2yj),
xi = 1, 2, 3, yj = 1, 2;
0,
otherwise.
(6.7.4)

334
Advanced Engineering Mathematics: A Second Course
Let us ï¬nd the value of k, pX[xi], and pY [yj].
From Equation 6.7.1, we have that
k
3
X
xi=1
2
X
yj=1
(xi + 2yj) = 1,
(6.7.5)
or
k

(1 + 2) + (1 + 4) + (2 + 2) + (2 + 4) + (3 + 2) + (3 + 4)

= 1.
(6.7.6)
Therefore, k = 1/30.
Turning to pX[xi] and pY [yj],
pX[xi] = k
2
X
yj=1
(xi + 2yj) = k(xi + 2) + k(xi + 4) = k(2xi + 6) = (xi + 3)/15,
(6.7.7)
where xi = 1, 2, 3, and
pY [yj] = k
3
X
xi=1
(xi + 2yj) = k(1 + 2yj) + k(2 + 2yj) + k(3 + 2yj) = k(6 + 6yj) = (1 + yj)/5,
(6.7.8)
where yj = 1, 2.
âŠ“âŠ”
â€¢ Example 6.7.2
Consider an urn that contains 1 red ball, 2 blue balls, and 2 green balls. Let (X, Y )
be a bivariate random variable where X and Y denote the number of red and blue balls,
respectively, chosen from the urn. There are 18 possible ways that three balls can be drawn
from the urn: rbb, rbg, rgb, rgg, brb, brg, bbr, bbg, bgr, bgb, bgg, grb, grg, gbr, gbb, gbg, ggr,
and ggb.
The range of X and Y in the present problem is RXY ={(0, 1), (0, 2), (1, 0), (1, 1), (1, 2)}.
The joint probability mass function of (X, Y ) is given by pXY [xi, yj] = P(X = i, Y = j),
where xi = 0, 1 and yj = 0, 1, 2. From our list of possible drawings, we ï¬nd that pXY [0, 0] =
0, pXY [0, 1] = 1/6, pXY [0, 2] = 1/6, pXY [1, 0] = 1/6, pXY [1, 1] = 1/3, and pXY [1, 2] = 1/6.
Note that all of these probabilities sum to one.
Given these probabilities, the marginal probabilities are pX[0] = 1/3, pX[1] = 2/3,
pY [0] = 1/3, pY [1] = 1/2, and pY [2] = 1/3. Because pXY [0, 0] Ì¸= pX[0]pY [0], X and Y are
not independent variables.
âŠ“âŠ”
â€¢ Example 6.7.3
Consider a community where 50% of the families have a pet. Of these families, 60%
have one pet, 30% have 2 pets, and 10% have 3 pets. Furthermore, each pet is equally
likely (and independently) to be a male or female. If a family is chosen at random from
the community, then we want to compute the joint probability that his family has M male
pets and F female pets.
These probabilities are as follows:
P{F = 0, M = 0} = P{no pets} = 0.5,
(6.7.9)

Probability
335
P{F = 1, M = 0} = P{1 female and total of 1 pet}
(6.7.10)
= P{1 pet}P{1 female|1 pet}
(6.7.11)
= (0.5)(0.6) Ã— 1
2 = 0.15,
(6.7.12)
P{F = 2, M = 0} = P{2 females and total of 2 pets}
(6.7.13)
= P{2 pets}P{2 females|2 pets}
(6.7.14)
= (0.5)(0.3) Ã—
1
2
2
= 0.0375,
(6.7.15)
and
P{F = 3, M = 0} = P{3 females and total of 3 pets}
(6.7.16)
= P{3 pets}P{3 females|3 pets}
(6.7.17)
= (0.5)(0.1) Ã—
1
2
3
= 0.00625.
(6.7.18)
The remaining probabilities can be obtained in a similar manner.
âŠ“âŠ”
Continuous joint distribution
Let us now turn to the case when we have two continuous random variables. In analog
with the deï¬nition given in Section 6.4, we deï¬ne the two-dimensional probability density
pXY (x, y) by
P(x < X â‰¤x + dx, y < Y â‰¤y + dy) = pXY (x, y) dx dy.
(6.7.19)
Here, the comma in the probability parentheses means â€œand also.â€
Repeating the same analysis as in Section 6.4, we ï¬nd that pXY (x, y) must be a single-
valued function with pXY (x, y) â‰¥0, and
Z âˆ
âˆ’âˆ
Z âˆ
âˆ’âˆ
pXY (x, y) dx dy = 1.
(6.7.20)
The joint distribution function of X and Y is
FXY (x, y) = P(X â‰¤x, Y â‰¤y) =
Z x
âˆ’âˆ
Z y
âˆ’âˆ
pXY (Î¾, Î·) dÎ¾ dÎ·.
(6.7.21)
Therefore,
P(a < X â‰¤b, c < Y â‰¤d) =
Z b
a
Z d
c
pXY (Î¾, Î·) dÎ¾ dÎ·.
(6.7.22)
The marginal probability density functions are deï¬ned by
pX(x) =
Z âˆ
âˆ’âˆ
pXY (x, y) dy,
and
pY (y) =
Z âˆ
âˆ’âˆ
pXY (x, y) dx.
(6.7.23)

336
Advanced Engineering Mathematics: A Second Course
An important distinction exists upon whether the random variables are independent or not.
Two variables X and Y are independent if and only if
pXY (x, y) = pX(x)pY (y),
(6.7.24)
and conversely.
â€¢ Example 6.7.4
The joint probability density function of bivariate random variables (X, Y ) is
pXY (x, y) =

kxy,
0 < y < x < 1,
0,
otherwise,
(6.7.25)
where k is a constant. (a) Find the value of k. (b) Are X and Y independent?
The range RXY for this problem is a right triangle with its sides given by x = 1, y = 0,
and y = x. From Equation 6.7.20,
Z âˆ
âˆ’âˆ
Z âˆ
âˆ’âˆ
pXY (x, y) dx dy = k
Z 1
0
x
Z x
0
y dy

dx = k
Z 1
0
x y2
2

x
0
dx
(6.7.26)
= k
2
Z 1
0
x3 dx = k
8 x41
0 = k
8.
(6.7.27)
Therefore, k = 8.
To check for independence we must ï¬rst compute pX(x) and pY (y). From Equation
6.7.23 and holding x constant,
pX(x) = 8x
Z x
0
y dy = 4x3,
0 < x < 1;
(6.7.28)
pX(x) = 0 otherwise. From Equation 6.7.23 and holding y constant,
pY (y) = 8y
Z 1
y
x dx = 4y(1 âˆ’y2),
0 < y < 1.
(6.7.29)
Because pXY (x, y) Ì¸= pX(x)pY (y), X and Y are not independent.
âŠ“âŠ”
â€¢ Example 6.7.5: Buï¬€onâ€™s needle problem
A classic application of joint probability distributions is the solution of Buï¬€onâ€™s needle
problem:8 Consider an inï¬nite plane with an inï¬nite series of parallel lines spaced a unit
distance apart. A needle of length L < 1 is thrown upward and we want to compute the
probability that the stick will land so that it intersects one of these lines. See Figure 6.7.1.
There are two random variables that determine the needleâ€™s orientation: X, the distance
from the lower end O of the needle to the nearest line above and Î˜, the angle from the
vertical to the needle. Of course, we assume that the position where the needle lands is
random; otherwise, it would not be a probability problem.
8 First posed in 1733, its solution is given on pages 100â€“104 of Buï¬€on, G., 1777: Essai dâ€™arithmÂ´etique
morale. Histoire naturelle, gÂ´enÂ´erale et particuli`ere, SupplÂ´ement, 4, 46â€“123.

Probability
337
L
cos
Î˜
L 
(Î¸)
(b) no intersection
Î˜
LL cos(Î¸)
O
X
X
O
(a) intersection
Figure 6.7.1: Schematic of Buï¬€onâ€™s needle problem showing the random variables X and Î˜.
Let us deï¬ne X ï¬rst. Its possible values lie between 0 and 1. Second, X is uniformly
distributed on (0, 1) with the probability density
pX(x) =
 1,
0 â‰¤x â‰¤1,
0,
otherwise.
(6.7.30)
Turning to Î˜, its value lies between âˆ’Ï€/2 to Ï€/2 and is uniformly distributed between these
values. Therefore, the probability density is
pÎ˜(Î¸) =

1/Ï€,
âˆ’Ï€/2 < Î¸ < Ï€/2,
0,
otherwise.
(6.7.31)
The probability p that we seek is
p = P{needle intersects line} = P{X < L cos(Î˜)}.
(6.7.32)
Because X and Î˜ are independent, their joint density equals the product of the densities
for X and Î˜: pXÎ˜(x, Î¸) = pX(x)pÎ˜(Î¸).
The ï¬nal challenge is to use pXÎ˜(x, Î¸) to compute p. In Section 6.2 we gave a geometric
deï¬nition of probability. The area of the sample space is Ï€ because it consists of a rectangle
in (X, Î˜) space with 0 < x < 1 and âˆ’Ï€/2 < Î¸ < Ï€/2.
The values of X and Î˜ that
lead to the intersection with a parallel line is 0 < x < L cos(Î¸) where âˆ’Ï€/2 < Î¸ < Ï€/2.
Consequently, from Equation 6.2.5,
p =
Z Ï€/2
âˆ’Ï€/2
Z L cos(Î¸)
0
pXÎ˜(x, Î¸) dx dÎ¸ =
Z Ï€/2
âˆ’Ï€/2
Z L cos(Î¸)
0
pX(x)pÎ˜(Î¸) dx dÎ¸
(6.7.33)
= 1
Ï€
Z Ï€/2
âˆ’Ï€/2
Z L cos(Î¸)
0
dx dÎ¸ = 1
Ï€
Z Ï€/2
âˆ’Ï€/2
L cos(Î¸) dÎ¸ = 2L
Ï€ .
(6.7.34)
Consequently, given L, we can perform the tossing either physically or numerically, measure
p, and compute the value of Ï€.
âŠ“âŠ”

338
Advanced Engineering Mathematics: A Second Course
Convolution
It is often important to calculate the distribution of X + Y from the distribution of
X and Y when X and Y are independent. We shall derive the relationship for continuous
random variables and then state the result for X and Y discrete.
Let X have a probability density function pX(x) and Y has the probability density
pY (y). Then the cumulative distribution function of X + Y is
GX+Y (a) = P(x + y â‰¤a) =
Z Z
x+yâ‰¤a
pX(x)pY (y) dx dy
(6.7.35)
=
Z âˆ
âˆ’âˆ
Z aâˆ’y
âˆ’âˆ
pX(x)pY (y) dx dy =
Z âˆ
âˆ’âˆ
Z aâˆ’y
âˆ’âˆ
pX(x) dx

pY (y) dy
(6.7.36)
=
Z âˆ
âˆ’âˆ
FX(a âˆ’y)pY (y) dx.
(6.7.37)
Therefore,
pX+Y (a) = d
da
Z âˆ
âˆ’âˆ
FX(a âˆ’y)pY (y) dy

=
Z âˆ
âˆ’âˆ
pX(a âˆ’y)pY (y) dy.
(6.7.38)
In the case when X and Y are discrete,
pX+Y [ak] =
âˆ
X
i=âˆ’âˆ
pX[xi]pY [ak âˆ’xi].
(6.7.39)
Covariance
In Section 6.5 we introduced the concept of variance of a random variable X. There
we showed that this quantity measures the dispersion, or spread, of the distribution of X
about its expectation. What about the case of two jointly distributed random numbers?
Our ï¬rst attempt might be to look at Var(X) and Var(Y ). But this would simply
display the dispersions of X and Y independently rather than jointly.
Indeed, Var(X)
would give the spread along the x-direction while Var(Y ) would measure the dispersion
along the y-direction.
Consider now Var(aX +bY ), the joint spread of X and Y along the (ax+by)-direction
for two arbitrary real numbers a and b. Then
Var(aX + bY ) = E[(aX + bY ) âˆ’E(aX + bY )]2
(6.7.40)
= E[(aX + bY ) âˆ’E(aX) âˆ’E(bY )]2
(6.7.41)
= E{a[X âˆ’E(X)] + b[Y âˆ’E(Y )]}2
(6.7.42)
= E{a2[X âˆ’E(X)]2 + b2[Y âˆ’E(Y )]2 + 2ab[X âˆ’E(X)][Y âˆ’E(Y )]}
(6.7.43)
= a2Var(X) + b2Var(Y ) + 2abE{[X âˆ’E(X)][Y âˆ’E(Y )]}.
(6.7.44)

Probability
339
Thus, the joint spread or dispersion of X and Y in any arbitrary direction ax + by depends
upon three parameters: Var(X), Var(Y ), and E{[X âˆ’E(X)][Y âˆ’E(Y )]}. Because Var(X)
and Var(Y ) give the dispersion of X and Y separately, it is the quantity E{[X âˆ’E(X)][Y âˆ’
E(Y )]} that measures the joint spread of X and Y . This last quantity,
Cov(X, Y ) = E{[X âˆ’E(X)][Y âˆ’E(Y )]},
(6.7.45)
is called the covariance and is usually denoted by Cov(X, Y ) because it determines how X
and Y covary jointly. It only makes sense when we have two diï¬€erent random variables
because in the case of a single random variable, Cov(X, X) = Ïƒ2
X = Var(X). Furthermore,
Cov(X, Y ) â‰¤ÏƒXÏƒY . In summary,
Var(aX + bY ) = a2 Var(X) + b2 Var(Y ) + 2ab Cov(X, Y ).
(6.7.46)
An alternative method for computing the covariance occurs if we recall that ÂµX = E(X)
and ÂµY = E(Y ). Then
Cov(X, Y ) = E[(X âˆ’ÂµX)(Y âˆ’ÂµY )] = E(XY âˆ’ÂµXY âˆ’ÂµY X + ÂµXÂµY )
(6.7.47)
= E(XY ) âˆ’ÂµXE(Y ) âˆ’ÂµY E(X) + ÂµXÂµY
(6.7.48)
= E(XY ) âˆ’ÂµXÂµY âˆ’ÂµY ÂµX + ÂµXÂµY
(6.7.49)
= E(XY ) âˆ’ÂµXÂµY = E(XY ) âˆ’E(X)E(Y ),
(6.7.50)
where
E(XY ) =
ï£±
ï£´
ï£´
ï£²
ï£´
ï£´
ï£³
X
xiâˆˆA,yjâˆˆB
xiyj pXY [xi, yj],
X discrete,
Z âˆ
âˆ’âˆ
Z âˆ
âˆ’âˆ
xy pXY (x, y) dx dy,
X continuous.
(6.7.51)
Therefore,
Cov(X, Y ) = E(XY ) âˆ’E(X)E(Y ).
(6.7.52)
It is left as a homework assignment to show that
Cov(aX + b, cY + d) = ac Cov(X, Y ).
(6.7.53)
In general, Cov(X, Y ) can be positive, negative, or zero.
For it to be positive, X
and Y decrease together or increase together.
For a negative value, X would increase
while Y decreases, or vice versa. If Cov(X, Y ) > 0, X and Y are positively correlated. If
Cov(X, Y ) < 0, X and Y are negatively correlated. Finally, if Cov(X, Y ) = 0, X and Y are
uncorrelated.
â€¢ Example 6.7.6
The following table gives a discrete joint density function:

340
Advanced Engineering Mathematics: A Second Course
xi
pXY [xi, yj]
0
1
2
pY [yj]
0
3
28
9
28
3
28
15
28
yj
1
3
14
3
14
0
3
7
2
1
28
0
0
1
28
pX[xi]
5
14
15
28
3
28
Because
E(XY ) =
2
X
i=0
2
X
j=0
xi yj pXY [xi, yj] = 3
14,
(6.7.54)
ÂµX = E(X) =
2
X
i=0
xi pX[xi] = 3
4,
and
ÂµY = E(Y ) =
2
X
j=0
yj pY [yj] = 1
2,
(6.7.55)
then
Cov(X, Y ) = E(XY ) âˆ’E(X)E(Y ) = 3
14 âˆ’3
4 Â· 1
2 = âˆ’9
56.
(6.7.56)
Therefore, X and Y are negatively correlated.
âŠ“âŠ”
â€¢ Example 6.7.7
The random variables X and Y have the joint probability density function
pXY (x, y) =
 x + y,
0 < x < 1, 0 < y < 1,
0,
otherwise.
(6.7.57)
Let us compute the covariance.
First, we must compute pX(x) and pY (y). We ï¬nd that
pX(x) =
Z 1
0
pXY (x, y) dy =
Z 1
0
(x + y) dy = x + 1
2
(6.7.58)
for 0 < x < 1, and
pY (y) =
Z 1
0
pXY (x, y) dx =
Z 1
0
(x + y) dx = y + 1
2
(6.7.59)
for 0 < y < 1.
Because
E(XY ) =
Z 1
0
Z 1
0
xy(x + y) dx dy =
Z 1
0
 
y x3
3

1
0
+ y2 x2
2

1
0
!
dy
(6.7.60)
=
Z 1
0
y
3 + y2
2

dy = y2
6 + y2
6

1
0
= 1
3,
(6.7.61)

Probability
341
ÂµX = E(X) =
Z 1
0
x pX(x) dx =
Z 1
0

x2 + x
2

dx = 7
12,
(6.7.62)
ÂµY = E(Y ) =
Z 1
0
y pY (y) dy =
Z 1
0

y2 + y
2

dy = 7
12,
(6.7.63)
then
Cov(X, Y ) = E(XY ) âˆ’E(X)E(Y ) = 1
3 âˆ’49
144 = âˆ’1
144.
(6.7.64)
Therefore, X and Y are negatively correlated.
âŠ“âŠ”
Correlation
Although the covariance tells us how X and Y vary jointly, it depends upon the same
units in which X and Y are measured. It is often better if we free ourselves of this nuisance,
and we now introduce the concept of correlation.
Let X and Y be two random variables with 0 < Ïƒ2
X < âˆand 0 < Ïƒ2
Y < âˆ. The
correlation coeï¬ƒcient Ï(X, Y ) between X and Y is given by
Ï(X, Y ) = Cov
X âˆ’E(X)
ÏƒX
, Y âˆ’E(Y )
ÏƒY

= Cov(X, Y )
ÏƒXÏƒY
.
(6.7.65)
It is noteworthy that |Ï(X, Y )| â‰¤1.
Random Vectors
It is often useful to express our two random variables X and Y as a two-dimensional
random vector V = (X Y )T . Then, the covariance can be written as a 2 Ã— 2 covariance
matrix, given by

cov(X, X)
cov(X, Y )
cov(Y, X)
cov(Y, Y )

.
These considerations can be generalized into the n-dimensional random vector consisting of
n random variables that are all associated with the same events.
â€¢ Example 6.7.7
Using MATLAB, let us create two random variables by invoking X = randn(N,1) and Y
= randn(N,2), where N is the sample size. If N = 10, we would ï¬nd that using the MATLAB
command cov(X,Y) would yield
>> ans =
3.1325
0.9748
0.9748
1.4862
.
(If you do this experiment, you will also obtain a symmetric matrix but with diï¬€erent
elements.) On the other hand, if N = 1000, we ï¬nd that cov(X,Y) equals
>> ans =

342
Advanced Engineering Mathematics: A Second Course
X
-4
-2
0
2
4
Y
-4
-2
0
2
4
Figure 6.7.2: Scatter plot of points (Xi, Yi) given by the random vector V in Example 6.7.7 when N =
1000.
0.9793
âˆ’0.0100
âˆ’0.0100
0.9927 .
The interpretation of the covariance matrix is as follows: The variance (or spread)
of data given by X and Y is (essentially) unity.
The correlation between X and Y is
(essentially) zero. These results are conï¬rmed in Figure 6.7.2 where we have plotted X and
Y as the data points (Xi, Yi) when N = 1000. We can see the symmetric distribution of
data points.
Problems
1. A search committee of 5 is selected from a science department that has 7 mathematics
professors, 8 physics professors, and 5 chemistry professors. If X and Y denote the number
of mathematics and physics professors, respectively, that are selected, calculate the joint
probability function.
2. In an experiment of rolling a fair die twice, let Z denote a random variable that equals
the sum of the results. What is pZ[zi]? Hint: Let X denote the result from the ï¬rst toss
and Y denote the result from the second toss. What you must ï¬nd is Z = X + Y .
3. Show that Cov(aX + b, cY + d) = ac Cov(X, Y ).
Project: Convolution
Consider two independent, uniformly distributed random variables (X, Y ) that are summed
to give Z = X + Y with
pX(x) =
 1,
0 < x < 1,
0,
otherwise,
and
pY (y) =
 1,
0 < y < 1,
0,
otherwise.

Probability
343
âˆ’0.5
0
0.5
1
1.5
2
2.5
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
 z
Estimated  pZ(z)
Convolution Project
Show that
pZ(z) =
(
z,
0 < z â‰¤1,
2 âˆ’z,
1 < z â‰¤2,
0,
otherwise.
Then conï¬rm your results using MATLABâ€™s intrinsic function rand to generate {xi} and {yj}
and computing pZ(z). You may want to review Example 6.5.1 in my Advanced Engineering
Mathematics with MATLAB to see how to compute a convolution analytically.
Further Readings
Beckmann, P., 1967: Probability in Communication Engineering. Harcourt, Brace & World,
511 pp. A presentation of probability as it applies to problems in communication engineer-
ing.
Ghahramani, S., 2000: Fundamentals of Probability. Prentice Hall, 511 pp. Nice introduc-
tory text on probability with a wealth of examples.
Hsu, H., 1997: Probability, Random Variables, & Random Processes. McGraw-Hill, 306 pp.
Summary of results plus many worked problems.
Kay, S. M., 2006: Intuitive Probability and Random Processes Using MATLAB. Springer,
833 pp. A well-paced book designed for the electrical engineering crowd.
Ross, S. M., 2007: Introduction to Probability Models. Academic Press, 782 pp. An intro-
ductory undergraduate book in applied probability and stochastic processes.
Tuckwell, H. C., 1995: Elementary Applications of Probability Theory. Chapman & Hall,
292 pp. This book presents applications using probability theory, primarily from biology.


âˆ’2
âˆ’1.5
âˆ’1
âˆ’0.5
0
0.5
1
1.5
2
âˆ’1
âˆ’0.5
0
0.5
1
1.5
2
2.5
x
y
Chapter 7
Random Processes
In the previous chapter we introduced the concept of a random variable X. There X
assumed various values of x according to a probability mass function pX[k] or probability
density function pX(x). In this chapter we generalize the random variable so that it is also
a function of time t. As before, the values of x assumed by the random variable X(t) at a
certain time is still unknown beforehand and unpredictable.
Our random, time-varying variable X(t; Î¾) is often used to describe a stochastic or
random process. In that case, X(t) is the state of the process at time t. The process can
be either discrete or continuous in t.
A random process is not one function but a collection or family of functions, called
sample functions, with some probability assigned to each. When we perform an experiment,
we observe only one of these functions that is called a realization or sample path
of the
process. To observe more than a single function, we must repeat the experiment.
The state space of a random process is the set of all possible values that the random
variable X(t) can assume.
We can view random processes from many perspectives. First, it is a random function of
time. This perspective is useful when we wish to relate an evolutionary physical phenomenon
to its probabilistic model. Second, we can focus on its aspect as a random variable. This is
useful in developing mathematical methods and tools to analyze random processes.
Another method for characterizing a random process examines its behavior as t and Î¾
vary or are kept constant. For example, if we allow t and Î¾ to vary, we obtain a family or
ensemble of X(t). If we allow t to vary while Î¾ is ï¬xed, then X(t) is simply a function of
time and gives a sample function or realization for this particular random process. On the
other hand, if we ï¬x t and allow Î¾ to vary, X(t) is a random variable equal to the state of
the random process at time t. Finally, if we ï¬x both t and Î¾, then X(t) is a number.
345

346
Advanced Engineering Mathematics: A Second Course
0
20
40
60
80
100
âˆ’1
âˆ’0.5
0
0.5
1
 t
 X(t)/a
Figure 7.0.1: A realization of the random telegraph signal.
â€¢ Example 7.0.1
Consider a random process X(t) = A, where A is uniformly distributed in the interval
[0, 1]. A plot of sample functions of X(t) (a plot of X(t) as a function of t) consists of
horizontal straight lines that would cross the ordinate somewhere between 0 and 1.
âŠ“âŠ”
â€¢ Example 7.0.2
Consider the coin tossing experiment where the outcomes are either heads H or tails
T. We can introduce the random process deï¬ned by
X(t; H) = sin(t),
and
X(t; T) = cos(t).
(7.0.1)
Note that the sample functions here are continuous functions of time.
âŠ“âŠ”
â€¢ Example 7.0.3: Random telegraph signal
Consider a signal that switches between âˆ’a and +a at random times. Suppose the
process starts (at time t = 0) in the âˆ’a state. It then remains in that state for a time
interval T1 at which point it switches to the state X(t) = a. The process remains in that
state until t = T2, then switches back to X(t) = âˆ’a. The switching time is given by a
Poisson process, a random process that we discuss in Section 7.6. Figure 7.0.1 illustrates
the random telegraph signal.
âŠ“âŠ”
Of all the possible random processes, a few are so useful in engineering and the physical
sciences that they warrant special names. Some of them are:
â€¢ Bernoulli process
Imagine an electronics ï¬rm that produces electronic devices that either work (a success
denoted by â€œSâ€ ) or do not work (a failure or denoted â€œFâ€). We can model the production
line as a series of independent, repeated events where p denotes the probability of producing
a working device and q = 1 âˆ’p is the probability of producing a faulty device. Thus, the
production line can be modeled as a random process, called a Bernoulli process, which has
discrete states and parameter space.

Random Processes
347
0
5
10
15
20
25
30
âˆ’0.5
0
0.5
1
1.5
 x[k,s]
Realization 1
0
5
10
15
20
25
30
âˆ’0.5
0
0.5
1
1.5
 x[k,s]
Realization 2
0
5
10
15
20
25
30
âˆ’0.5
0
0.5
1
1.5
 k
 x[k,s]
Realization 3
Figure 7.0.2: Three realization or sample functions of a Bernoulli random process with p = 0.4. The
realization starts at k = 0 and continues forever.
The dashed box highlights the values of the random
variable X[21, s].
If we denote each discrete trial by the integer k, a Bernoulli process generates successive
outcomes at times k = 0, 1, 2, . . .. Mathematically we can express this discrete random
process by X[k, s] where k denotes the time and s denotes the number of the realization
or sample function.
Furthermore, this random process maps the original experimental
sample space {(F, F, S, . . .), (S, F, F, . . .), (F, F, F, . . .), . . .} to the numerical sample space
{(0, 0, 1, . . .), (1, 0, 0, . . .), (0, 0, 0, . . .), . . .}. Unlike the Bernoulli trial that we examined in
the previous chapter, each simple event now becomes an inï¬nite sequence of Sâ€™s and Fâ€™s.
Figure 7.0.2 illustrates three realizations or sample functions for a Bernoulli random
variable when p = 0.4. In each realizations s = 1, 2, . . ., the abscissa denotes time where
each successive trial occurs at times k = 0, 1, 2, . . .. When we ï¬x the value of k, the quantity
X[k, s] is a random variable with a probability mass function of a Bernoulli random variable.
â€¢ Markov process
Communication systems transmit either the digits 0 or 1. Each transmitted digit often
must pass through several stages. At each stage there is a chance that the digit that enters
one stage will be changed by the time when it leaves.
A Markov process is a stochastic process that describes the probability that the digit
will or will not be changed. It does this by computing the conditional distribution of any
future state Xn+1 by considering only the past states X0, X1, . . . , Xnâˆ’1 and the present
state Xn.
In Section 7.4 we examine the simplest possible discrete Markov process, a
Markov chain, when only the present and previous stages are involved. An example is the
probabilistic description of birth and death, which is given in Section 7.5.
â€¢ Poisson process
The prediction of the total number of â€œeventsâ€ that occur by time t is important to
such diverse ï¬elds as telecommunications and banking. The most popular of these counting
processes is the Poisson process. It occurs when:

348
Advanced Engineering Mathematics: A Second Course
1. the events occur â€œrarely,â€
2. the events occur in nonoverlapping intervals of time that are independent of each other,
3. the events occur at a constant rate Î».
In Section 7.6 we explore this random process.
â€¢ Wiener process
A Wiener process Wt is a random process that is continuous in time and possesses the
following three properties:
1. W0 = 0,
2. Wt is almost surely continuous, and
3. Wt has independent increments with a distribution Wtâˆ’Ws âˆ¼N(0, tâˆ’s) for 0 â‰¤s â‰¤t.
As a result of these properties, we have that
1. the expectation is zero, E(Wt) = 0,
2. the variance is E(W 2
t ) âˆ’E2(Wt) = t, and
3. the covariance is cov(Ws, Wt) = min(s, t).
Norbert Wiener (1894â€“1964) developed this process to rigorously describe the physical
phenomena of Brownian motionâ€”the apparent random motion of particles suspended in
a ï¬‚uid. In a Wiener process the distances traveled in Brownian motion are distributed
according to a Gaussian distribution and the path is continuous but consists entirely of
sharp corners.
Project: Gamblerâ€™s Ruin Problem
Pete and John decide to play a coin-tossing game. Pete agrees to pay John 10 cents
whenever the coin yields a â€œheadâ€ and John agrees to pay Pete 10 cents whenever it
is a â€œtail.â€
Let Sn denote the amount that John earns in n tosses of a coin.
This
game is a stochastic process with discrete time (number of tosses).
The state space is
{0, Â±10, Â±20, Â· Â· Â·} cents. A realization occurs each time that they play a new game.
Step 1: Create a MATLAB code to compute a realization of Sn. Plot several realizations
(sample functions) of this random process. See Figure 7.0.3.
Step 2: Suppose Pete has 10 dimes. Therefore, there is a chance he will run out of dimes at
some n = N. Modify your MATLAB code to construct a probability density function that
gives the probability Pete will run out of money at time n = N. See Figure 7.0.3.
This problem is often formulated in terms of a gambler versus casino and called the
gamblerâ€™s ruin problem: A gambler enters a casino with $n in cash and starts playing a
game where he wins with probability p and loses with probability 1 âˆ’q. The gambler plays
the game repeatedly, betting $1 in each round. He leaves the casino if his total fortune
reaches $N or he runs out of money.
The gamblerâ€™s ruin problem is also particularly popular because it a simple exam-
ple of a important stochastic process called a martingale.
In discrete time the martin-
gale requires that the sequence X1, X2, X3, . . . satisï¬es two conditions: E(|Xn|) < âˆand
E(Xn+1|X1, X2, . . . , Xn) = Xn for any time n, where E(Â·) denotes the expectation opera-
tor. If Xn is an observation, then we have a martingale if the conditional expected value
of the next observation, given all the past observations, equals the most recent observation.
To see that the gamblerâ€™s run problem is a martingale, we compute
E(Xn+1|xn) = 1
2(Xn + 1) + 1
2(Xn âˆ’1) = Xn,

Random Processes
349
0
200
400
600
800
1000
âˆ’50
0
50
100
number of tosses
gain/loss (in dimes)
0
200
400
600
800
1000
0
1
2
3
4x 10
âˆ’3
N
Estimated PDF
Figure 7.0.3: (a) Top frame: Johnâ€™s gains or losses as the result of the three diï¬€erent coin tossing games.
(b) The probability density function for Johnâ€™s winning 10 dimes as a function of the number of tosses that
are necessary to win 10 dimes.
where we denote the gamblerâ€™s bankroll by Xn.
7.1 FUNDAMENTAL CONCEPTS
In Section 6.5 we introduced the concepts of mean (or expectation) and variance as
they apply to discrete and continuous random variables. These parameters provide useful
characterizations of a probability mass function or probability density function. Similar
considerations hold in the case of random processes and we introduce them here.
Mean and variance
We deï¬ne the mean of the random process X(t) as the expected value of the processâ€”
that is, the expected value of the random variable deï¬ned by X(t) for a ï¬xed instant of
time. Note that when we take the expectation, we hold the time as a nonrandom parameter
and average only over the random quantities. We denote this mean of the random process
by ÂµX(t), since, in general, it may depend on time. The deï¬nition of the mean is just the
expectation of X(t):
ÂµX(t) = E[X(t)] =
Z âˆ
âˆ’âˆ
x pX(t)(t; x) dx.
(7.1.1)
In a similar vein, we can generalize the concept of variance so that it applies to random
processes. Here variance also becomes a time-dependent function deï¬ned by
Ïƒ2
X(t) = Var[X(t)] = E
n
[X(t) âˆ’ÂµX(t)]2o
.
(7.1.2)

350
Advanced Engineering Mathematics: A Second Course
â€¢ Example 7.1.1: Random linear trajectories
Consider the random process deï¬ned by
X(t) = A + Bt,
(7.1.3)
where A and B are uncorrelated random variables with means ÂµA and ÂµB. Let us ï¬nd the
mean of this random process.
From the linearity property of expectation, we have that
ÂµX(t) = E[X(t)] = E(A + Bt) = E(A) + E(B)t = ÂµA + ÂµBt.
(7.1.4)
âŠ“âŠ”
â€¢ Example 7.1.2: Random sinusoidal signal
A random sinusoidal signal is one governed by X(t) = A cos(Ï‰0t + Î˜), where A and
Î˜ are independent random variables, A has a mean ÂµA and variance Ïƒ2
A, and Î˜ has the
probability density function pÎ˜(x) that is nonzero only over the interval (0, 2Ï€).
The mean of X(t) is given by
ÂµX(t) = E[X(t)] = E[A cos(Ï‰0t + Î˜)] = E[A]E[cos(Ï‰0t + Î˜)].
(7.1.5)
We have used the property that the expectation of two independent random variables equals
the product of the expectation of each of the random variables. Simplifying Equation 7.1.5,
ÂµX(t) = ÂµA
Z 2Ï€
0
cos(Ï‰0t + x)pÎ˜(x) dx.
(7.1.6)
A common assumption is that pÎ˜(x) is uniformly distributed in the interval (0, 2Ï€), namely
pÎ˜(x) = 1
2Ï€ ,
0 < x < 2Ï€.
(7.1.7)
Substituting Equation 7.1.7 into Equation 7.1.6, we ï¬nd that
ÂµX(t) = ÂµA
2Ï€
Z 2Ï€
0
cos(Ï‰0t + x) dx = 0.
(7.1.8)
âŠ“âŠ”
â€¢ Example 7.1.3: Wiener random process or Brownian motion
A Wiener (random) process is deï¬ned by
X(t) =
Z t
0
U(Î¾) dÎ¾,
t â‰¥0,
(7.1.9)
where U(t) denotes white Gaussian noise. It is often used to model Brownian motion. To
ï¬nd its mean, we have that
E[X(t)] = E
Z t
0
U(Î¾) dÎ¾

=
Z t
0
E[U(Î¾)] dÎ¾ = 0,
(7.1.10)

Random Processes
351
because the mean of white Gaussian noise equals zero.
âŠ“âŠ”
Autocorrelation function
When a random process is examined at two time instants t = t1 and t = t2, we obtain
two random variables X(t1) and X(t2). A useful relationship between these two random
variables is found by computing their correlation as a function of time instants t1 and t2.
Because it is a correlation between the values of the same process sampled at two diï¬€erent
instants of time, we shall call it the autocorrelation function of the process X(t) and denote
it by RX(t1, t2). It is deï¬ned in the usual way for expectations by
RX(t1, t2) = E[X(t1)X(t2)].
(7.1.11)
Just as in the two random variables case, we can deï¬ne the covariance and correlation
coeï¬ƒcient, but here the name is slightly diï¬€erent. We deï¬ne the autocovariance function
as
CX(t1, t2) = E{[X(t1) âˆ’ÂµX(t1)][X(t2) âˆ’ÂµX(t2)]}
(7.1.12)
= RX(t1, t2) âˆ’ÂµX(t1)ÂµX(t2).
(7.1.13)
Note that the variance of the process and its average power (the names used for the average
of [X(t)âˆ’ÂµX(t)]2 and [X(t)]2, respectively) can be directly obtained for the autocorrelation
and the autocovariance functions, by simply using the same time instants for both t1 and
t2:
E{[X(t)]2} = RX(t, t),
(7.1.14)
and
Ïƒ2
X(t) = E{[X(t) âˆ’ÂµX(t)]2} = CX(t, t) = RX(t, t) âˆ’Âµ2
X(t).
(7.1.15)
Therefore, the average power, Equation 7.1.14, and the variance, Equation 7.1.15, of the
process follows directly from the deï¬nition of the autocorrelation and autocovariance func-
tions.
â€¢ Example 7.1.4: Random linear trajectories
Let us continue Example 7.1.1 and ï¬nd the autocorrelation of a random linear trajectory
given by X(t) = A + Bt. From the deï¬nition of the autocorrelation,
RX(t1, t2) = E[X(t1)X(t2)] = E{[A + Bt1][A + Bt2]}
(7.1.16)
= E(A2) + E(AB)(t1 + t2) + E(B2)t1t2
(7.1.17)
= (Ïƒ2
A + Âµ2
A) + ÂµAÂµB(t1 + t2) + (Ïƒ2
B + Âµ2
B)t1t2,
(7.1.18)
where Ïƒ2
A and Ïƒ2
B are the variances of the random variables A and B. We can easily ï¬nd
the autocovariance by
CX(t1, t2) = RX(t1, t2) âˆ’ÂµX(t1)ÂµX(t2) = Ïƒ2
A + Ïƒ2
Bt1t2.
(7.1.19)
âŠ“âŠ”

352
Advanced Engineering Mathematics: A Second Course
â€¢ Example 7.1.5: Random sinusoidal signal
We continue to examine the random sinusoidal signal given by X(t) = A cos(Ï‰0t + Î˜).
The autocorrelation function is
RX(t1, t2) = E[X(t1)X(t2)] = E[A cos(Ï‰0t1 + Î˜)A cos(Ï‰0t2 + Î˜)]
(7.1.20)
= 1
2E(A2)E[cos(Ï‰0t2 âˆ’Ï‰0t1) + cos(Ï‰0t2 + Ï‰0t1 + 2Î˜)]
(7.1.21)
= 1
2(Ïƒ2
A + Âµ2
A)

cos[Ï‰0(t2 âˆ’t1)] +
Z 2Ï€
0
cos[Ï‰0(t2 + t1) + 2x]pÎ˜(x) dx

. (7.1.22)
In our derivation we used (1) the property that the expectation of A2 equals the sum of
the variance and the square of the mean, and (2) the ï¬rst term involving the cosine is
not random because it is a function of only the time instants and the frequency. From
Equation 7.1.22 we see that autocorrelation function may depend on both time instants if
the probability density function of the phase angle is arbitrary. On the other hand, if pÎ˜(x)
is uniformly distributed, then the last term in Equation 7.1.22 vanishes because integrating
the cosine function over the interval of one period is zero. In this case we can write the
autocorrelation function as a function of only the time diï¬€erence. The process also becomes
wide-sense stationary with
RX(Ï„) = E[X(t)X(t + Ï„)] = 1
2(Ïƒ2
A + Âµ2
A) cos(Ï‰0Ï„).
(7.1.23)
âŠ“âŠ”
Wide-sense stationary processes
The mathematical analysis of a random or stochastic process would appear to be hope-
less because of the uncertainty of its time-dependent behavior at any instant of time. To
circumvent this diï¬ƒculty we will examine only those processes that have certain statistical
properties at any instant. A wide-sense stationary process is one of the most popular.
A process is strictly stationary if its distribution and density functions do not depend
on the absolute values of the time instants t1 and t2, but only on the diï¬€erence of the time
instants, |t1 âˆ’t2|. However, this is a very rigorous condition. If we are concerned only with
the mean and autocorrelation function, then we can soften our deï¬nition of a stationary
process to a limited form, and we call such processes wide-sense stationary processes. A
wide-sense stationary process has a constant mean, and its autocorrelation function depends
only on the time diï¬€erence:
ÂµX(t) = E[X(t)] = ÂµX,
(7.1.24)
and
RX(t1, t2) = E[X(t1)X(t2)] = RX(t2 âˆ’t1).
(7.1.25)
Because time does not appear in the mean, we simply write it as a constant mean value ÂµX.
Similarly, because the autocorrelation function is a function only of the time diï¬€erence, we
can write it as a function of a single variable, the time diï¬€erence Ï„:
RX(Ï„) = E[X(t)X(t + Ï„)].
(7.1.26)

Random Processes
353
We can obtain similar expressions for the autocovariance function, which in this case de-
pends only on the time diï¬€erence as well:
CX(Ï„) = E{[X(t) âˆ’ÂµX][X(t + Ï„) âˆ’ÂµX]} = RX(Ï„) âˆ’Âµ2
X.
(7.1.27)
Finally, the average power and variance for a wide-sense stationary process are
E{[X(t)]2} = RX(0),
and
Ïƒ2
X = CX(0) = RX(0) âˆ’Âµ2
X,
(7.1.28)
respectively. Therefore, a wide-sense stationary process has a constant average power and
constant variance.
Problems
1. Find ÂµX(t) and Ïƒ2
X(t) for the random process given by X(t) = A cos(Ï‰t), where Ï‰ is
a constant and A is a random variable with the Gaussian (or normal) probability density
function
pX(x) =
1
âˆš
2Ï€ eâˆ’x2/2.
2. Consider a sine-wave random process X(t) = A cos(Ï‰t + Î˜), âˆ’Ï€ < t < Ï€, where A and
Ï‰ are constants with A > 0. The phase function Î˜ is a random, uniform variable on the
interval [âˆ’Ï€, Ï€]. Find the mean, variance and autocorrelation for this random function. Is
this process wide-sense stationary?
3.
Consider a countably inï¬nite sequence {Xn, n = 0, 1, 2, 3, . . .} of a random variable
deï¬ned by
Xn =

1,
for success in the nth trial,
0,
for failure in the nth trial,
with the probabilities P(Xn = 0) = 1 âˆ’p and P(Xn = 1) = p. Thus, Xn is a Bernoulli
process. For this process, E(Xn) = p and Var(Xn) = p(1âˆ’p). Show that the autocorrelation
is
RX(t1, t2) =
 p,
t1 = t2,
p2,
t1 Ì¸= t2;
and the autocovariance is
CX(t1, t2) =

p(1 âˆ’p),
t1 = t2,
0,
t1 Ì¸= t2.
Project: Computing the Autocorrelation Function
In most instances you must compute the autocorrelation function numerically. The
purpose of this project is to explore this computation using the random telegraph signal.
The exact solution is given by Equation 7.2.24. You will compute the autocorrelation two
ways:
Step 1: Using Example 7.6.1, create MATLAB code that generates 500 realizations of the
random telegraph signal.

354
Advanced Engineering Mathematics: A Second Course
0
0.2
0.4
0.6
0.8
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Ï„
 RX
0
0.2
0.4
0.6
0.8
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Ï„
(a)
(b)
Figure 7.1.1: The autocorrelation function RX(Ï„) for the random telegraph signal as a function of Ï„ when
Î» = 2. The dashed line gives the exact solution. In frame (a) Xk(tS)Xk(tS + Ï„) has been averaged over
500 realizations when tS = 2. In frame (b) X200(mâˆ†t)X200(mâˆ†t + Ï„) has been averaged with M = 1200
and âˆ†t = 0.01.
Step 2: Choosing an arbitrary time tS, compute Xk(tS)Xk(tS + Ï„) for 0 â‰¤0 â‰¤Ï„max and
k = 1, 2, 3, . . . , 500. Then ï¬nd the average value of Xk(tS)Xk(tS + Ï„). Plot RX(Ï„) as a
function of Ï„ and include the exact answer for comparison. Does it matter how many sample
functions you use?
Step 3: Now introduce a number of times tm = mâˆ†t, where m = 0, 1, 2, . . . , M. Using only
a single realization k = K of your choice, compute XK(mâˆ†t) Ã— XK(mâˆ†t + Ï„). Then ï¬nd
the average value of XK(mâˆ†t)XK(mâˆ†t + Ï„) and plot this result as a function of Ï„. On the
same plot, include the exact solution. Does the value of âˆ†t matter? See Figure 7.1.1
7.2 POWER SPECTRUM
In earlier chapters we provided two alternative descriptions of signals, either in the
time domain, which provides information on the shape of the waveform, or in the frequency
domain, which provides information on the frequency content. Because random signals do
not behave in any predictable fashion nor are they represented by a single function, it is
unlikely that we can deï¬ne the spectrum of a random signal by taking its Fourier transform.
On the other hand, the autocorrelation of random signals describes in some sense whether
the signal changes rapidly or slowly. In this section we explain and illustrate the concept
of power spectrum of random signals.
For a wide-sense stationary random signal X(t) with autocorrelation function RX(Ï„),
the power spectrum SX(Ï‰) of the random signal is the Fourier transform of the autocorre-
lation function:
SX(Ï‰) =
Z âˆ
âˆ’âˆ
RX(Ï„)eâˆ’iÏ‰Ï„ dÏ„.
(7.2.1)
Consequently, the autocorrelation can be obtained from inverse Fourier transform of the
power spectrum, or
RX(Ï„) = 1
2Ï€
Z âˆ
âˆ’âˆ
SX(Ï‰)eiÏ‰Ï„ dÏ‰.
(7.2.2)

Random Processes
355
As with any Fourier transform, it enjoys certain properties. They are:
1. The power spectrum is real and even: SX(âˆ’Ï‰) = SX(Ï‰) and Sâˆ—
X(Ï‰) = SX(Ï‰), where
Sâˆ—
X(Ï‰) denotes the complex conjugate value of SX(Ï‰).
2. The power spectrum is nonnegative: SX(Ï‰) â‰¥0.
3. The average power of the random signal is equal to the integral of the power spectrum:
E{[X(t)]2} = RX(0) = 1
Ï€
Z âˆ
0
SX(Ï‰) dÏ‰.
(7.2.3)
4. If the random signal has nonzero mean ÂµX, then its power spectrum contains an impulse
at zero frequency of magnitude 2Ï€Âµ2
X.
5. The Fourier transform of the autocovariance function of the random process is itself
also a power spectrum and usually does not contain an impulse component in zero
frequency.
Consider the following examples of the power spectrum:
â€¢ Example 7.2.1: Random sinusoidal signal
The sinusoidal signal is deï¬ned by
X(t) = A cos(Ï‰0t + Î˜),
(7.2.4)
where the phase is uniformly distributed in the interval [0, 2Ï€]. If the amplitude A has a
mean of zero and a variance of Ïƒ2, then the autocorrelation function is
RX(Ï„) = 1
2Ïƒ2 cos(Ï‰0Ï„) = RX(0) cos(Ï‰0Ï„).
(7.2.5)
The power spectrum of this signal is then
SX(Ï‰) =
Z âˆ
âˆ’âˆ
RX(0) cos(Ï‰0Ï„)eâˆ’iÏ‰Ï„ dÏ„ = RX(0)Ï€ [Î´(Ï‰ âˆ’Ï‰0) + Î´(Ï‰ + Ï‰0)] .
(7.2.6)
Because this signal contains only one frequency Ï‰0, its power spectrum is just two impulses,
one at Ï‰0 and one at âˆ’Ï‰0. Since the negative frequency appears due only to the even
property of the power spectrum, it is clear that all power is concentrated at the frequency
of the sinusoidal signal. While this is a very simple example, it does illustrate that the
power spectrum indeed represents the way the power in the random signal is distributed
among the various frequencies.
We shall see later that if we also use linear systems in
order to amplify or attenuate certain frequencies, the results mirror what we expect in the
deterministic case.
âŠ“âŠ”
â€¢ Example 7.2.2: Modulated signal
Let us now examine a sinusoidal signal modulated by another random signal that
contains low frequencies. This random process is described by
Y (t) = X(t) cos(Ï‰0t + Î˜),
(7.2.7)

356
Advanced Engineering Mathematics: A Second Course
where the phase angle in Equation 7.2.7 is a random variable that is uniformly distributed
in the interval [0, 2Ï€] and is independent of X(t). Then the autocorrelation function of Y (t)
is given by
RY (Ï„) = E[Y (t)Y (t + Ï„)] = E{X(t) cos(Ï‰0t + Î˜)X(t + Ï„) cos[Ï‰0(t + Ï„) + Î˜]}
(7.2.8)
= E[X(t)X(t + Ï„)]E{cos(Ï‰0t + Î˜) cos[Ï‰0(t + Ï„) + Î˜]} = 1
2RX(Ï„) cos(Ï‰0t).
(7.2.9)
Let us take RX(Ï„) = RX(0)eâˆ’2Î»|Ï„|, the autocorrelation function for a random telegraph
signal (see Equation 7.2.22). In this case,
RY (Ï„) = 1
2RX(0)eâˆ’2Î»|Ï„| cos(Ï‰0t).
(7.2.10)
Turning to the power spectrum, the deï¬nition gives
SY (Ï‰) =
Z âˆ
âˆ’âˆ
1
2RX(Ï„) cos(Ï‰0t)eâˆ’iÏ‰Ï„ dÏ„
(7.2.11)
= 1
4
Z âˆ
âˆ’âˆ
RX(Ï„)
 eiÏ‰0Ï„ + eâˆ’iÏ‰0Ï„
eâˆ’iÏ‰Ï„ dÏ„
(7.2.12)
= 1
4 [SX(Ï‰ âˆ’Ï‰0) + SX(Ï‰ + Ï‰0)] .
(7.2.13)
Thus, the resulting power spectrum is shifted to the modulating frequency Ï‰0 and its
negative value, with peak values located at both Ï‰ = Ï‰0 and Ï‰ = âˆ’Ï‰0.
âŠ“âŠ”
â€¢ Example 7.2.3: White noise
There are instances when we want to approximate random signals where the autocor-
relation function is very narrow and very large about Ï„ = 0. In those cases we construct an
idealization of the autocorrelation function by using the impulse or delta function Î´(Ï„).
In the present case when RX(Ï„) = C Î´(Ï„), the power spectrum is
SX(Ï‰) =
Z âˆ
âˆ’âˆ
C Î´(Ï„)eâˆ’iÏ‰Ï„ dÏ„ = C.
(7.2.14)
Thus, the power spectrum here is a ï¬‚at spectrum whose value is equal to C. Because the
power spectrum is ï¬‚at for all frequencies, it is often called â€œwhite noiseâ€ since it contains
all frequencies with equal weight.
An alternative derivation involves the random telegraph that we introduced in Example
7.0.3. As the switching rate becomes large and the rate Î» approaches inï¬nity, its amplitude
increases as
âˆš
Î».
Because RX(0) increases linearly with Î», the autocorrelation function
becomes
RX(Ï„) = CÎ» exp(âˆ’2Î»|Ï„|).
(7.2.15)
The resulting power spectrum equals
SX(Ï‰) = lim
Î»â†’âˆ
4CÎ»2
Ï‰2 + 4Î»2 = lim
Î»â†’âˆ
C
1 + [Ï‰/(2Î»)]2 = C.
(7.2.16)
The power spectrum is again ï¬‚at for all frequencies.
The autocorrelation for white noise is an idealization because it has inï¬nite average
power. Obviously no real signal has inï¬nite power since in practice the power spectrum

Random Processes
357
decays eventually.
Nevertheless, white noise is still quite useful because the decay usu-
ally occurs at such high frequencies that we can tolerate the errors of introducing a ï¬‚at
spectrum.
âŠ“âŠ”
â€¢ Example 7.2.4: Random telegraph signal
In Example 7.0.3 we introduced the random telegraph signal: X(t) equals either +h or
âˆ’h, changing its value from one to the other in Poisson-distributed moments of time. The
probability of n changes in a time interval Ï„ is
PÏ„(n) = (Î»Ï„)n
n!
eâˆ’Î»Ï„,
(7.2.17)
where Î» denotes the average frequency of changes.
To compute the power spectrum, we must ï¬rst compute the correlation function via
the product X(t)X(t + Ï„). This product equals h2 or âˆ’h2, depending on whether X(t) =
X(t + Ï„) or X(t) = âˆ’X(t + Ï„), respectively.
These latter relationships depend on the
number of changes during the time interval. Now,
P[X(t) = X(t + Ï„)] = PÏ„(n even) = eâˆ’Î»Ï„
âˆ
X
n=1
(Î»Ï„)2n
(2n)! = eâˆ’Î»Ï„ cosh(Î»Ï„),
(7.2.18)
and
P[X(t) = âˆ’X(t + Ï„)] = PÏ„(n odd) = eâˆ’Î»Ï„
âˆ
X
n=1
(Î»Ï„)2n+1
(2n + 1)! = eâˆ’Î»Ï„ sinh(Î»Ï„).
(7.2.19)
Therefore,
E[X(t)X(t + Ï„)] = h2PÏ„(n even) âˆ’h2PÏ„(n odd)
(7.2.20)
= h2eâˆ’Î»Ï„[cosh(Î»Ï„) âˆ’sinh(Î»Ï„)]
(7.2.21)
= h2eâˆ’2Î»|Ï„|.
(7.2.22)
We have introduced the absolute value sign in Equation 7.2.24 because our derivation was
based on t2 > t1 and the absolute value sign takes care of the case t2 < t1.
Using Problem 1, we have that
SX(Ï‰) = 2h2
Z âˆ
0
eâˆ’2Î»Ï„ cos(Î»Ï„) dÏ„ =
4h2Î»
Ï‰2 + 4Î»2 .
(7.2.23)
Problems
1. Show that
SX(Ï‰) = 2
Z âˆ
0
RX(Ï„) cos(Ï‰Ï„) dÏ„.
7.3 TWO-STATE MARKOV CHAINS
A Markov chain is a probabilistic model in which the outcomes of successive trials
depend only on its immediate predecessors. The mathematical description of a Markov

358
Advanced Engineering Mathematics: A Second Course
chain involves the concepts of states and state transition. If Xn = i, then we have a process
with state i and time n. Given a process in state i, there is a ï¬xed probability Pij that state
i will transition into state j. In this section we focus on the situation of just two states.
Imagine that you want to predict the chance of rainfall tomorrow.1 From close obser-
vation you note that the chance of rain tomorrow depends only on whether it is raining
today and not on past weather conditions. From your observations you ï¬nd that if it rains
today, then it will rain tomorrow with probability Î±, and if it does not rain today, then
the chance it will rain tomorrow is Î². Assuming that these probabilities of changes are
stationary (unchanging), you would like to answer the following questions:
1. Given that it is raining (or not raining), what are the chances of it raining in eight
days?
2. Suppose the day is rainy (or dry). How long will the current weather remain before it
changes for the ï¬rst time?
3. Suppose it begins to rain during the week. How long does it take before it stops?
If the weather observation takes place at noon, we have a discrete parameter process;
the two possible states of the process are rain and no rain. Let these be denoted by 0 for no
rain and 1 for rain. The four possible transitions are (0 â†’0), (0 â†’1), (1 â†’0), and (1 â†’1).
Let Xn be the state of the process at the nth time point. We have Xn = 0, 1. Clearly,
{Xn, n = 0, 1, 2, . . .} is a two-state Markov chain. Therefore, questions about precipitation
can be answered if all the properties of the two-state Markov chains are known. Let
P (m,n)
i,j
= P(Xn = j|Xm = i),
i, j = 0, 1;
m â‰¤n.
(7.3.1)
P (m,n)
i,j
denotes the probability that the state of the process at the nth time point is j given
that it was at state i at the mth time point. Furthermore, if this probability is larger for
i = j than when i Ì¸= j, the system prefers to stay or persist in whatever state it is. When
n = m + 1, we have that
P (m,m+1)
i,j
= P(Xm+1 = j|Xm = i).
(7.3.2)
This is known as the one-step transition probability, given that the process is at i at time
m.
There are two possibilities: either P (m,m+1)
ij
depends on m or P (m,m+1)
ij
is independent
of m, where m is the initial value of the time parameter. Our precipitation model is an
example of a second type of process in which the one-step transition probabilities do not
change with time.
Such processes are known as time homogeneous.
Presently we shall
restrict ourselves only to these processes. Consequently, without loss of generality we can
use the following notation for the probabilities:
Pij = P(Xm+1 = j|Xm = i)
for all m,
(7.3.3)
and
P (n)
ij
= P(Xm+n = j|Xm = i)
for all m.
(7.3.4)
1 See, for example, Gabriel, K. R., and J. Neumann, 1962: A Markov chain model for daily rainfall
occurrence at Tel Aviv. Quart. J. R. Met. Soc., 88, 90â€“95.

Random Processes
359
Chapman-Kolmogorov equation
The Chapman2-Kolmogorov3 equations provide a mechanism for computing the tran-
sition probabilities after n steps. The n-step transition probabilities P (n)
ij
denote the prob-
ability that a process in state i will be in state j after n transitions, or
P (n)
ij
= P[Xn+k = j|Xk = i],
n â‰¥0,
i, j â‰¥0.
(7.3.5)
Therefore, P (1)
ij
= Pij. The Chapman-Kolmogorov equations give a method for computing
these n-step transition probabilities via
P (n+m)
ij
=
âˆ
X
k=0
P (n)
ik P (m)
kj ,
n, m â‰¥0,
(7.3.6)
for all i and j. Here P (n)
ik P (m)
kj
represents the probability that the ith starting process will
go to state j in n + m transitions via a path that takes it into state k at the nth transition.
Equation 7.3.6 follows from
P (n+m)
ij
= P[Xn+m = j|X0 = i] =
âˆ
X
k=0
P[Xn+m = j, Xn = k|X0 = i]
(7.3.7)
=
âˆ
X
k=0
P[Xn+m = j|Xn = k, X0 = i]P[Xn = k|X0 = i] =
âˆ
X
k=0
P (n)
ik P (m)
kj .
(7.3.8)
Transmission probability matrix
Returning to the task at hand, we have that
P (2) = P (1+1) = P Â· P = P 2,
(7.3.9)
and by induction
P (n) = P (nâˆ’1+1) = P (nâˆ’1) Â· P = P n,
(7.3.10)
where P (n) denotes the transition matrix after n steps.
From our derivation, we see the following: (1) The one-step transition probability
matrix completely deï¬nes the time-homogeneous two-state Markov chain. (2) All transition
probability matrices show the important property that the elements in any of their rows add
up to one. This follows from the fact that the elements of a row represent the probabilities
of mutually exclusive and exhaustive events on a sample space.
2 Chapman, S., 1928: On the Brownian displacements and thermal diï¬€usion of grains suspended via
non-uniform ï¬‚uid. Proc. R. Soc. London, Ser. A, 119, 34â€“54.
3 Kolmogorov, A. N., 1931: Â¨Uber die analytischen Methoden in der Wahrscheinlichkeitsrechnung. Math.
Ann., 104, 415â€“458.

360
Advanced Engineering Mathematics: A Second Course
Table 7.3.1: The Probability of Rain on the nth Day.
n
P00
P10
P01
P11
1
0.7000
0.2000
0.3000
0.8000
2
0.5500
0.3000
0.4500
0.7000
3
0.4750
0.3500
0.5250
0.6500
4
0.4375
0.3750
0.5625
0.6250
5
0.4187
0.3875
0.5813
0.6125
6
0.4094
0.3938
0.5906
0.6063
7
0.4047
0.3969
0.5953
0.6031
8
0.4023
0.3984
0.5977
0.6016
9
0.4012
0.3992
0.5988
0.6008
10
0.4006
0.3996
0.5994
0.6004
âˆ
0.4000
0.4000
0.6000
0.6000
For two-state Markov processes, this means that
P (n)
00 + P (n)
01 = 1,
and
P (n)
10 + P (n)
11 = 1.
(7.3.11)
Furthermore, with the one-step transmission probability matrix:
P =

1 âˆ’a
a
b
1 âˆ’b

,
0 â‰¤a, b â‰¤1,
|1 âˆ’a âˆ’b| < 1,
(7.3.12)
then the n-step transmission probability matrix is

P (n)
00
P (n)
01
P (n)
10
P (n)
11

=
 
b
a+b + a (1âˆ’aâˆ’b)n
a+b
a
a+b âˆ’a (1âˆ’aâˆ’b)n
a+b
b
a+b âˆ’b (1âˆ’aâˆ’b)n
a+b
a
a+b + b (1âˆ’aâˆ’b)n
a+b
!
.
(7.3.13)
This follows from the Chapman-Kolmogorov equation that
P (1)
00 = 1 âˆ’a,
(7.3.14)
and
P (n)
00 = (1 âˆ’a)P (nâˆ’1)
00
+ bP (nâˆ’1)
01
,
n > 1,
(7.3.15)
= b + (1 âˆ’a âˆ’b)P (nâˆ’1)
00
,
(7.3.16)
since P (n)
01 = 1âˆ’P (n)
00 . Solving these equations recursively for n = 1, 2, 3, . . . and simplifying,
we obtain Equation 7.3.13 as long as both a and b do not equal zero.
â€¢ Example 7.3.1
Consider a precipitation model where the chance for rain depends only on whether it
rained yesterday. If we denote the occurrence of rain by state 0 and state 1 denotes no rain,
then observations might give you a transition probability that looks like:
P =

0.7
0.3
0.2
0.8

.
(7.3.17)
Given that the atmosphere starts today with any one of these states, the probability of
ï¬nding that it is raining on the nth day is given by P n. Table 7.3.1 illustrates the results
as a function of n. Thus, regardless of whether it rains today or not, in ten days the chance
for rain is 0.4 while the chance for no rain is 0.6.
âŠ“âŠ”

Random Processes
361
Limiting behavior
As Table 7.3.1 suggests, as our Markov chain evolves, it reaches some steady state. Let
us explore this limit of n â†’âˆbecause it often provides a simple and insightful represen-
tation of a Markov process.
For large values of n it is possible to show that the limiting probability distribution of
states is independent of the initial value. In particular, for |1 âˆ’a âˆ’b| < 1, we have that
lim
nâ†’âˆP (n) =

b
a+b
a
a+b
b
a+b
a
a+b

.
(7.3.18)
This follows from limnâ†’âˆ(1 âˆ’a âˆ’b)n â†’0 since |1 âˆ’a âˆ’b| < 1. From Equation 7.3.13 the
second term in each of the elements of the matrix tends to zero as n â†’âˆ.
Let us denote these limiting probabilities by Ï€j = limnâ†’âˆP (n)
ij . Then, from Equation
7.3.18,
Ï€00 = Ï€10 =
b
a + b = Ï€0,
and
Ï€01 = Ï€11 =
a
a + b = Ï€1,
(7.3.19)
and these limiting distributions are independent of the initial state.
Number of visits to a certain state
When a random process visits several states, we would like to know the number of
visits to a certain state. Let N (n)
ij
denote the number of visits the two-state Markov chain
{Xn} makes to state j, starting initially at state i, in n time periods. If Âµ(n)
ij
denotes the
expected number of visits that the process makes to state j in n steps after it originally
started at state i, and the transition probability matrix P of the two-state Markov chain is
P =

1 âˆ’a
a
b
1 âˆ’b

(7.3.20)
with |1 âˆ’a âˆ’b| < 1, then
||Âµ(n)
ij || =
 
nb
a+b + a(1âˆ’aâˆ’b)[1âˆ’(1âˆ’aâˆ’b)n]
(a+b)2
na
a+b âˆ’a(1âˆ’aâˆ’b)[1âˆ’(1âˆ’aâˆ’b)n]
(a+b)2
nb
a+b âˆ’b(1âˆ’aâˆ’b)[1âˆ’(1âˆ’aâˆ’b)n]
(a+b)2
na
a+b + b(1âˆ’aâˆ’b)[1âˆ’(1âˆ’aâˆ’b)n]
(a+b)2
!
(7.3.21)
To prove Equation 7.3.21, we introduce a random variable Y (k)
ij , where
Y (n)
ij
=

1,
if Xk = j and X0 = i,
0,
otherwise,
(7.3.22)
for i, j = 0, 1. This random variable Y (n)
ij
gives the time at which the process visits state j.
The probability distribution of Y (n)
ij
for ï¬xed k is
Y (n)
ij
0
1

362
Advanced Engineering Mathematics: A Second Course
Probability
1 âˆ’P (n)
ij
P (n)
ij
Thus, we have that
E[Y (k)
ij ] = P (k)
ij ,
i, j = 0, 1;
k = 1, 2, . . . , n.
(7.3.23)
Because Y (k)
ij
equals 1 whenever the process is in state j and 0 when it is not in j, the
number of visits to j, starting originally from i, in n steps is
N (n)
ij
= Y (1)
ij
+ Y (2)
ij
+ Â· Â· Â· + Y (n)
ij .
(7.3.24)
Taking the expected values and using the property that the expectation of a sum is the sum
of expectations,
Âµ(n)
ij
= E
h
N (n)
ij
i
= P (1)
ij
+ P (2)
ij
+ Â· Â· Â· + P (n)
ij
=
n
X
k=1
P (k)
ij .
(7.3.25)
From Equation 7.3.13, we substitute for each P (k)
ij
and ï¬nd
Âµ(n)
00 =
n
X
k=1
P (k)
00 =
n
X
k=1

b
a + b + a(1 âˆ’a âˆ’b)k
a + b

,
(7.3.26)
Âµ(n)
01 =
n
X
k=1
P (k)
01 =
n
X
k=1

a
a + b âˆ’a(1 âˆ’a âˆ’b)k
a + b

,
(7.3.27)
Âµ(n)
10 =
n
X
k=1
P (k)
10 =
n
X
k=1

b
a + b âˆ’b(1 âˆ’a âˆ’b)k
a + b

,
(7.3.28)
and
Âµ(n)
11 =
n
X
k=1
P (k)
11 =
n
X
k=1

a
a + b + b(1 âˆ’a âˆ’b)k
a + b

,
(7.3.29)
ï¬nally, noting that
n
X
k=1
b
a + b =
nb
a + b,
(7.3.30)
and
n
X
k=1
a(1 âˆ’a âˆ’b)k
a + b
=
a
a + b
n
X
k=1
(1 âˆ’a âˆ’b)k
(7.3.31)
=
a
a + b

(1 âˆ’a âˆ’b) + (1 âˆ’a âˆ’b)2 + Â· Â· Â· + (1 âˆ’a âˆ’b)n
(7.3.32)
= a(1 âˆ’a âˆ’b)
a + b

1 + (1 âˆ’a âˆ’b) + Â· Â· Â· + (1 âˆ’a âˆ’b)nâˆ’1
(7.3.33)
= a(1 âˆ’a âˆ’b) [1 âˆ’(1 âˆ’a âˆ’b)n]
(a + b) [1 âˆ’(1 âˆ’a âˆ’b)]
.
(7.3.34)
Here we used the property of a geometric series that
nâˆ’1
X
k=0
xk = 1 âˆ’xn
1 âˆ’x ,
|x| < 1.
(7.3.35)

Random Processes
363
â€¢ Example 7.3.2
Let us continue with our precipitation model that we introduced in Example 7.3.1. If
we wish to know the expected number of days within a week that the atmosphere will be
in a given state, we have from Equation 7.3.21 that
Âµ(7)
00 =
7b
a + b + a(1 âˆ’a âˆ’b)[1 âˆ’(1 âˆ’a âˆ’b)7]
(a + b)2
= 3.3953,
(7.3.36)
Âµ(7)
10 =
7b
a + b âˆ’b(1 âˆ’a âˆ’b)[1 âˆ’(1 âˆ’a âˆ’b)7]
(a + b)2
= 2.4031,
(7.3.37)
Âµ(7)
01 =
7a
a + b âˆ’a(1 âˆ’a âˆ’b)[1 âˆ’(1 âˆ’a âˆ’b)7]
(a + b)2
= 3.6047,
(7.3.38)
and
Âµ(7)
11 =
7a
a + b + b(1 âˆ’a âˆ’b)[1 âˆ’(1 âˆ’a âˆ’b)7]
(a + b)2
= 4.5969,
(7.3.39)
since a = 0.3 and b = 0.2.
âŠ“âŠ”
Duration of stay
In addition to computing the number of visits to a certain state, it would also be useful
to know the fraction of the discrete time that a process stays in state j out of n when the
process started in state i. These fractions are:
lim
nâ†’âˆ
Âµ(n)
00
n
= lim
nâ†’âˆ
Âµ(n)
10
n
= Ï€0,
(7.3.40)
and
lim
nâ†’âˆ
Âµ(n)
01
n
= lim
nâ†’âˆ
Âµ(n)
11
n
= Ï€1.
(7.3.41)
Thus, the limiting probabilities also give the fraction of time that the process spends in the
two states in the long run.
If the process is in state i (i = 0, 1) at some time, let us compute the number of
additional time periods it stays in state i until it moves out of that state. We now want to
show that this probability distribution Î±i, i = 0, 1, is
P(Î±0 = n) = a(1 âˆ’a)n,
(7.3.42)
and
P(Î±1 = n) = b(1 âˆ’b)n,
(7.3.43)
where n = 1, 2, 3, . . .. Furthermore,
E(Î±0) = (1 âˆ’a)/a,
E(Î±1) = (1 âˆ’b)/b,
(7.3.44)
and
Var(Î±0) = (1 âˆ’a)/a2,
Var(Î±1) = (1 âˆ’b)/b2,
(7.3.45)

364
Advanced Engineering Mathematics: A Second Course
where the transition probability matrix P of the Markov chain {Xn} equals
P =

1 âˆ’a
a
b
1 âˆ’b

(7.3.46)
with |1 âˆ’a âˆ’b| < 1. Clearly a or b cannot equal zero.
To prove this we note that at every step, the process has two choices: either to stay in
the same state or to move to the other state. Suppose the process is in state 0 at some time.
The probability of a sequence of outcomes of the type {0 0 Â· Â· Â· 0
| {z }
n
1} is required. Because of the
property of Markov-dependence, we therefore have the realization of a Bernoulli process with
n consecutive outcomes of one type followed by an outcome of the other type. Therefore,
the probability distribution of Î±0 is geometric with (1 âˆ’a) as the probability of â€œfailure,â€
and the distribution of Î±1 is geometric with (1âˆ’b) as the probability of failure. Thus, from
Equation 6.6.5, we have that
P(Î±0 = n) = a(1 âˆ’a)n,
(7.3.47)
and
P(Î±1 = n) = b(1 âˆ’b)n,
(7.3.48)
where n = 0, 1, 2, . . .. The expressions for the mathematical expectation and variance of Î±0
and Î±1 easily follow from the corresponding expressions for the geometric distribution.
â€¢ Example 7.3.3
Let us illustrate our expectation and variance expressions for our precipitation model.
From Equation 7.3.44 and Equation 7.3.45, we have that
E(Î±0) = (1 âˆ’a)/a = 2.3333,
E(Î±1) = (1 âˆ’b)/b = 4,
(7.3.49)
and
Var(Î±0) = (1 âˆ’a)/a2 = 7.7777,
Var(Î±1) = (1 âˆ’b)/b2 = 20,
(7.3.50)
since a = 0.3 and b = 0.2.
âŠ“âŠ”
â€¢ Example 7.3.4: Gamblerâ€™s ruin problem
At the beginning of this chapter we introduced the gamblerâ€™s ruin problem as an ex-
ample of a random process. Here we wish to redo that problem as a Markov chain
Our particular version of the game is as follows: A gambler plays a game involving the
ï¬‚ipping of a coin. The probability of the coin coming up heads is p while the probability of
the coin coming up tails is q = 1âˆ’p. He enters the game with some initial amount of money
and plays until (1) he has lost all of his money or (2) he has gained N units of money. We
would like to describe this game at the j ï¬‚ip of the coin.
Let xi denote the probability that the gambler has i units of money. At the j ï¬‚ip,
these probabilities will be aï¬€ected by the states xi+1 and xiâˆ’1 according to
xj+1
i
= pxj
i+1 + qxj
iâˆ’1,
i = 1, 2, . . . , N âˆ’1.
(7.3.51)
Equation 7.3.51 does not describe the states i = 0 and i = N, the absorbing states. State
i = 0 corresponds to the gambler losing all of his money and quitting the game while state
i = N corresponds to the gambler winning N units of money and calling it a night. Once

Random Processes
365
q
p
p
p
q
q
0
1
N
2
Nâˆ’2
Nâˆ’1
1
1
Figure 7.3.1: Markov chain diagram for the gamblerâ€™s ruin problem.
these absorbing states are attained, there is no way of going to another state: xj+1
0
= xj
0
and xj+1
N
= xj
N. Since these absorbing states can be eventually reached from any other
state, the game will eventually reach a steady state. We have illustrated this Markov chain
in Figure 7.3.1.
The most convenient way of computing x is via matrix algebra. Using matrix notation,
we can compute the probabilities from:
xj+1 = xjP,
(7.3.52)
where
P =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
0
0
0
0
Â· Â· Â·
0
0
0
0
q
0
p
0
0
Â· Â· Â·
0
0
0
0
0
q
0
p
0
Â· Â· Â·
0
0
0
0
...
...
...
...
...
...
...
...
...
...
0
0
0
0
0
Â· Â· Â·
q
0
p
0
0
0
0
0
0
Â· Â· Â·
0
q
0
p
0
0
0
0
0
Â· Â· Â·
0
0
0
1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
(7.3.53)
and x is the row vector [x0 x1 Â· Â· Â· xNâˆ’1xN].
To illustrate the evolution of the gamblerâ€™s ruin problem, let us set N = 3, p = q = 0.5,
and x0 = [0 1 0 0]T . Then.
x1 = x0P = [0.5 0 0.5 0]
(7.3.54)
x2 = x0P 2 = [0.5 0.25 0 0.25]
(7.3.55)
x3 = x0P 3 = [0.625 0 0.125 0.25]
(7.3.56)
x10 = x0P 10 = [0.66601562 0.00097656 0 0.33300781]
(7.3.57)
x100 = x0P 100 = [0.6660666 0.00000000 0.00000000 0.3333333].
(7.3.58)
The interpretation of these results is straightforward. After 100 games, the probability
that the gambler, with an initial bankroll of one unit of money, will lose all his money is
2/3 while the chance that he will go home with 3 units of money is 1/3. There are no other
outcomes to the game.
Problems
1. Given
P =

3/4
1/4
1/2
1/2

,
(a) compute P n and (b) ï¬nd limnâ†’âˆP n.
2. Suppose you want to model how your dog learns a new trick. Let Fido be in state 0 if
he learns the new trick and in state 1 if he fails to learn the trick. Suppose that if he learns

366
Advanced Engineering Mathematics: A Second Course
the trick, he will retain the trick. If he has yet to learn the trick, there is a probability Î±
of him learning it with each training session. (a) Write down the transition matrix. (b)
Compute P (n) where n is the number of training sessions. (c) What is the steady-state
solution? Interpret your result. (d) Compute the expected amount of time that Fido will
spend in each state during n training sessions.
7.4 BIRTH AND DEATH PROCESSES
In the previous section we considered two-state Markov chains that undergo n steps.
As the time interval between steps tends to zero, the Markov process becomes continuous
in time. In this section and the next, we consider two independent examples of continuous
Markov processes.
We began Chapter 6 by showing that the deterministic description of birth and death
is inadequate to explain the extinction of species. Here we will ï¬ll out the details of our
analysis and extend them to population dynamics and chemical kinetics.
Deterministic
models lead to ï¬rst-order ordinary diï¬€erential equations, and this description fails when
the system initially contains a small number of particles.
Consider a population of organisms that multiply by the following rules:
1. The sub-populations generated by two co-existing individuals develop completely in-
dependently of one another;
2. an individual existing at time t has a chance Î» dt+o(dt) of multiplying by binary ï¬ssion
during the following time interval of length dt;
3. the â€œbirth rateâ€ Î» is the same for all individuals in the population at any time t;
4. an individual existing at time t has a chance Âµ dt + o(dt) of dying in the following time
interval of length dt; and
5. the â€œdeath rateâ€ Âµ is the same for all individuals at any time t.
Rule 3 is usually interpreted in the sense that in each birth, just one new member is added
to the population, but of course mathematically (and because the age structure of the
population is being ignored) it is not possible to distinguish between this and an alternative
interpretation in which one of the parents dies when the birth occurs and is replaced by
two children.
Let n0 be the number of individuals at the initial time t = 0 and let pn(t) denote the
probability that the population size N(t) has the value n at the time t. Then
dpn
dt = (n âˆ’1)Î»pnâˆ’1 âˆ’n(Î» + Âµ)pn + Âµ(n + 1)pn+1,
n â‰¥1,
(7.4.1)
and
dp0(t)
dt
= Âµp1(t),
(7.4.2)
subject to the initial condition that
pn(0) =
 1,
n = n0,
0,
n Ì¸= n0.
(7.4.3)
Equation 7.4.1 through Equation 7.4.3 constitute a system of linear ordinary equations.
The question now turns on how to solve them most eï¬ƒciently. To this end we introduce a
probability-generating function:
Ï†(z, t) =
âˆ
X
n=0
znpn(t).
(7.4.4)

Random Processes
367
Summing Equation 7.4.1 from n = 1 to âˆafter we multiplied it by zn and using Equation
7.4.2, we obtain
âˆ
X
n=0
zn dpn
dt = Î»
âˆ
X
n=1
(nâˆ’1)znpnâˆ’1(t)âˆ’(Î»+Âµ)
âˆ
X
n=1
nznpn(t)+Âµ
âˆ
X
n=0
(n+1)znpn+1(t). (7.4.5)
Because
âˆ
X
n=0
zn dpn
dt = âˆ‚Ï†
âˆ‚t ,
(7.4.6)
âˆ
X
n=1
nznpn(t) = z
âˆ
X
n=0
nznâˆ’1pn(t) = z âˆ‚Ï†
âˆ‚z ,
(7.4.7)
âˆ
X
n=1
(n âˆ’1)znpnâˆ’1(t) =
âˆ
X
k=0
kzk+1pk(t) = z2
âˆ
X
k=0
kzkâˆ’1pk(t) = z2 âˆ‚Ï†
âˆ‚z ,
(7.4.8)
and
âˆ
X
n=0
(n + 1)znpn+1(t) =
âˆ
X
k=1
kzkâˆ’1pk(t) =
âˆ
X
k=0
kzkâˆ’1pk(t) = âˆ‚Ï†
âˆ‚z ,
(7.4.9)
Equation 7.4.5 becomes the ï¬rst-order partial diï¬€erential equation
âˆ‚Ï†
âˆ‚t = (Î»z âˆ’Âµ)(z âˆ’1)âˆ‚Ï†
âˆ‚z ,
(7.4.10)
subject to the initial condition
Ï†(z, 0) = zn0.
(7.4.11)
Equation 7.4.10 is an example of a ï¬rst-order partial diï¬€erential equation of the general
form
P(x, y)âˆ‚u
âˆ‚x + Q(x, y)âˆ‚u
âˆ‚y = 0.
(7.4.12)
This equation has solutions4 of the form u(x, y) = f(Î¾) where f(Â·) is an arbitrary function
that is diï¬€erentiable and Î¾(x, y) = constant are solutions to
dx
P(x, y) =
dy
Q(x, y).
(7.4.13)
In the present case,
dt
1 = âˆ’
dz
(Î»z âˆ’Âµ)(z âˆ’1) = âˆ’
dz
(Î» âˆ’Âµ)(z âˆ’1) +
dz
(Î» âˆ’Âµ)(z âˆ’Âµ/Î»).
(7.4.14)
Integrating Equation 7.4.14,
âˆ’(Î» âˆ’Âµ)t + ln[Ïˆ(z)] = ln(Î¾),
(7.4.15)
or
Î¾(z, t) = Ïˆ(z)eâˆ’(Î»âˆ’Âµ)t,
(7.4.16)
4 See Webster, A. G., 1966: Partial Diï¬€erential Equations of Mathematical Physics. Dover, 446 pp.
See Section 22.

368
Advanced Engineering Mathematics: A Second Course
where
Ïˆ(z) = Î»z âˆ’Âµ
z âˆ’1 .
(7.4.17)
Therefore, the general solution is
Ï†(z, t) = f
h
Ïˆ(z)eâˆ’(Î»âˆ’Âµ)ti
.
(7.4.18)
Our remaining task is to ï¬nd f(Â·). From the initial condition, Equation 7.4.11, we have
that
Ï†(z, 0) = f[Ïˆ(z)] = zn0.
(7.4.19)
Because z = [Âµ âˆ’Ïˆ(z)]/[Î» âˆ’Ïˆ(z)], then
f(Ïˆ) =
Âµ âˆ’Ïˆ
Î» âˆ’Ïˆ
n0
.
(7.4.20)
Therefore,
Ï†(z, t) =
Âµ âˆ’Ïˆ(z)eâˆ’(Î»âˆ’Âµ)t
Î» âˆ’Ïˆ(z)eâˆ’(Î»âˆ’Âµ)t
n0
.
(7.4.21)
Once we ï¬nd Ï†(z, t), we can compute the probabilities of each of the species from the
probability generating function. For example,
P{N(t) = 0|N(0) = n0} = p0(t) = Ï†(0, t).
(7.4.22)
From Equation 7.4.17 we have Ïˆ(0) = Âµ and
Ï†(0, t) =
(
Âµ

1 âˆ’eâˆ’(Î»âˆ’Âµ)t
Î» âˆ’Âµeâˆ’(Î»âˆ’Âµ)t
)n0
,
Î» Ì¸= Âµ,
(7.4.23)
and
Ï†(0, t) =

Î»t
1 + Î»t
n0
,
Î» = Âµ.
(7.4.24)
An important observation from Equation 7.4.23 and Equation 7.4.24 is that
lim
tâ†’âˆp0(t) = 1,
Î» â‰¤Âµ,
(7.4.25)
and
lim
tâ†’âˆp0(t) =
Âµ
Î»
n0
,
Î» > Âµ.
(7.4.26)
This limit can be interpreted as the probability of extinction of the population in a ï¬nite
time.
Consequently, there will be â€œalmost certainâ€ extinction whenever Î» â‰¤Âµ.
These
results, which are true whatever the initial number of individuals may be, show very clearly
the inadequacy of the deterministic description of population dynamics.
Finally, let us compute the mean and variance for the birth and death process. The
expected number of individuals at time t is
m(t) = E[N(t)] =
âˆ
X
n=0
n pn(t) =
âˆ
X
n=1
n pn(t).
(7.4.27)

Random Processes
369
Now
dm
dt =
âˆ
X
n=1
ndpn
dt =
âˆ
X
n=1
n [(n âˆ’1)Î»pnâˆ’1 âˆ’n(Î» + Âµ)pn + Âµ(n + 1)pn+1]
(7.4.28)
= Î»
âˆ
X
n=1
(n âˆ’1)2pnâˆ’1 + Î»
âˆ
X
n=1
(n âˆ’1)pnâˆ’1 âˆ’(Î» + Âµ)
âˆ
X
n=1
n2pn + Âµ
âˆ
X
n=1
(n + 1)2pn+1
âˆ’Âµ
âˆ
X
n=1
(n + 1)pn+1
(7.4.29)
= âˆ’(Î» + Âµ)
âˆ
X
n=1
n2pn + Î»
âˆ
X
i=0
i2pm + Âµ
âˆ
X
k=2
k2pk + Î»
âˆ
X
i=0
i pi âˆ’Âµ
âˆ
X
k=2
k pk.
(7.4.30)
In the ï¬rst three sums in Equation 7.4.30, terms from i, k, n = 2, and onward cancel and
leave âˆ’(Î» + Âµ)p1 + Î»p1 = âˆ’Âµp1. Therefore,
dm
dt = âˆ’Âµp1 + Î»
âˆ
X
i=0
i pi âˆ’Âµ
âˆ
X
k=2
k pk = (Î» âˆ’Âµ)
âˆ
X
n=0
n pn, = (Î» âˆ’Âµ)m.
(7.4.31)
If we choose the initial condition m(0) = n0, the solution is
m(t) = n0e(Î»âˆ’Âµ)t.
(7.4.32)
This is the same as the deterministic result with the birth rate b replaced by Î» and the death
rate d replaced by Âµ. Furthermore, if Î» = Âµ, the mean size of the population is constant.
The second moment of N(t) is
M(t) =
âˆ
X
n=0
n2pn(t).
(7.4.33)
Proceeding as before, we have that
dM
dt =
âˆ
X
n=1
n2 dpn
dt =
âˆ
X
n=1
n2 [Î»(n âˆ’1)pnâˆ’1 âˆ’(Î» + Âµ)n pn + Âµ(n + 1)pn+1]
(7.4.34)
= Î»
âˆ
X
n=1
(n âˆ’1)3pnâˆ’1 + 2Î»
âˆ
X
n=1
(n âˆ’1)2pnâˆ’1 + Î»
âˆ
X
n=1
(n âˆ’1)pnâˆ’1 âˆ’(Î» + Âµ)
âˆ
X
n=1
n3pn
+ Âµ
âˆ
X
n=1
(n + 1)3pn+1 âˆ’2Âµ
âˆ
X
n=1
(n + 1)2pn+1 + Âµ
âˆ
X
n=1
(n + 1)pn+1
(7.4.35)
= Î»
âˆ
X
k=1
k3pk + 2Î»
âˆ
X
k=1
k2pk + Î»
âˆ
X
k=1
k pk âˆ’(Î» + Âµ)
âˆ
X
n=1
n3pn
+ Âµ
âˆ
X
i=2
i3pi âˆ’2Âµ
âˆ
X
i=2
i2pi + Âµ
âˆ
X
i=2
i pi.
(7.4.36)
The three sums, which contain either i3 or k3 or n3 in them, cancel when i, k, n = 2 and
onward; these three sums reduce to âˆ’Âµp1. The sums that involve i2 or k2 can be written

370
Advanced Engineering Mathematics: A Second Course
in terms of M(t). Finally, the sums involving i and k can be expressed in terms of m(t).
Therefore, Equation 7.4.36 becomes the ï¬rst-order ordinary diï¬€erential equation
dM
dt âˆ’2(Î» âˆ’Âµ)M = (Î» + Âµ)m(t) = (Î» + Âµ)n0e(Î»âˆ’Âµ)t,
(7.4.37)
with M(0) = n2
0.
Equation 7.4.37 can be solved exactly using the technique of integrating factors. Its
solution is
M(t) = n2
0e2(Î»âˆ’Âµ)t + Î» + Âµ
Î» âˆ’Âµn0e(Î»âˆ’Âµ)t h
e(Î»âˆ’Âµ)t âˆ’1
i
.
(7.4.38)
From the deï¬nition of variance, Equation 6.6.5, the variance of the population in the birth
and death process equals
Var[N(t)] = n0
(Î» + Âµ)
(Î» âˆ’Âµ)e(Î»âˆ’Âµ)t h
e(Î»âˆ’Âµ)t âˆ’1
i
,
Î» Ì¸= Âµ,
(7.4.39)
or
Var[N(t)] = 2Î»n0t,
Î» = Âµ.
(7.4.40)
â€¢ Example 7.4.1: Chemical kinetics
The use of Markov processes to describe birth and death has become quite popular.
Indeed, it can be applied to any phenomena where something is being created or destroyed.
Here we illustrate its application in chemical kinetics.
Let the random variable X(t) be the number of A molecules in a unimolecular reaction
A â†’B (such as radioactive decay) at time t. A stochastic model that describes the decrease
of A can be constructed from the following assumptions:
1. The probability of transition from n to n âˆ’1 in the time interval (t, t + âˆ†t) is nÎ»âˆ†t +
o(âˆ†t) where Î» is a constant and o(âˆ†t) denotes that o(âˆ†t)/âˆ†t â†’0 as âˆ†t â†’0.
2. The probability of a transition from n to n âˆ’j, j > 1, in the time interval (t, t + âˆ†t) is
at least o(âˆ†t) because the time interval is so small that only one molecule undergoes
a transition.
3. The reverse reaction occurs with probability zero.
The equation that governs the probability that X(t) = n is
pn(t + âˆ†t) = (n + 1)Î»âˆ†tpn+1(t) + (1 âˆ’Î»nâˆ†t)pn(t) + o(âˆ†t).
(7.4.41)
Transposing pn(t) from the right side, dividing by âˆ†t, and taking the limit âˆ†t â†’0, we
obtain the diï¬€erential-diï¬€erence equation5
dpn
dt = (n + 1)Î»pn+1(t) âˆ’nÎ»pn(t).
(7.4.42)
Equation 7.4.42 is frequently called the stochastic master equation. The ï¬rst term on the
right side of this equation vanishes when n = n0.
5 McQuarrie, D. A., 1963: Kinetics of small systems. I. J. Chem. Phys., 38, 433â€“436.

Random Processes
371
The solution of Equation 7.4.42 once again involves introducing a generating function
for pn(t), namely
F(z, t) =
n0
X
n=0
pn(t)zn,
|z| < 1.
(7.4.43)
Summing Equation 7.4.42 from n = 0 to n0 after multiplying it by zn, we ï¬nd
n0
X
n=0
zn dpn
dt = Î»
n0âˆ’1
X
n=0
(n + 1)znpn+1(t) âˆ’Î»
n0
X
n=1
nznpn(t).
(7.4.44)
Because
n0
X
n=0
zn dpn
dt = âˆ‚F
âˆ‚t ,
(7.4.45)
n0
X
n=0
nznpn(t) = z
n0
X
n=0
nznâˆ’1pn(t) = z âˆ‚F
âˆ‚z ,
(7.4.46)
and
n0âˆ’1
X
n=0
(n + 1)znpn+1(t) =
n0
X
k=1
kzkâˆ’1pk(t) =
n0
X
k=0
kzkâˆ’1pk(t) = âˆ‚F
âˆ‚z ,
(7.4.47)
Equation 7.4.44 becomes the ï¬rst-order partial diï¬€erential equation
âˆ‚F
âˆ‚t = Î»(1 âˆ’z)âˆ‚F
âˆ‚z .
(7.4.48)
The solution of Equation 7.4.48 follows the method used to solve Equation 7.4.10. Here
we ï¬nd Î¾(z, t) via
dt
1 =
dz
Î»(z âˆ’1),
(7.4.49)
or
Î¾(z, t) = (z âˆ’1)eâˆ’Î»t.
(7.4.50)
Therefore,
F(z, t) = f

(z âˆ’1)eâˆ’Î»t
.
(7.4.51)
To ï¬nd f(Â·), we use the initial condition that F(z, 0) = zn0. This yields f(y) = (1+y)n0
and
F(z, t) =

1 + (z âˆ’1)eâˆ’Î»tn0 .
(7.4.52)
Once again, we can compute the mean and variance of this process. Because
âˆ‚F
âˆ‚z

z=1
=
n0
X
n=0
npn(t),
(7.4.53)
the mean is given by
E[X(t)] = âˆ‚F(1, t)
âˆ‚z
.
(7.4.54)
To compute the variance, we ï¬rst compute the second moment. Since
z âˆ‚F
âˆ‚z =
n0
X
n=0
nznpn(t),
(7.4.55)

372
Advanced Engineering Mathematics: A Second Course
and
âˆ‚
âˆ‚z

z âˆ‚F
âˆ‚z

=
n0
X
n=0
n2znâˆ’1pn(t),
(7.4.56)
we have that
n0
X
n=0
n2pn(t) = âˆ‚2F(1, t)
âˆ‚z2
+ âˆ‚F(1, t)
âˆ‚z
.
(7.4.57)
From Equation 6.6.5, the ï¬nal result is
Var[X(t)] = âˆ‚2F(1, t)
âˆ‚z2
+ âˆ‚F(1, t)
âˆ‚z
âˆ’
âˆ‚F(1, t)
âˆ‚z
2
.
(7.4.58)
Upon substituting Equation 7.4.52 into Equations 7.4.54 and 7.4.58, the mean and variance
for this process are
E[X(t)] = n0eâˆ’Î»t,
and
Var[X(t)] = n0eâˆ’Î»t  1 âˆ’eâˆ’Î»t
.
(7.4.59)
Because the expected value of the stochastic representation also equals the deterministic
result, the two representations are â€œconsistent in the mean.â€ Further study shows that this
is true only for unimolecular reactions. Upon expanding Equation 7.4.52, we ï¬nd that
pn(t) =

n0
n

eâˆ’nÎ»t  1 âˆ’eâˆ’Î»tn0âˆ’n .
(7.4.60)
An alternative method to the generating function involves Laplace transforms.6 To
illustrate this method, we again examine the reaction A â†’B.
The stochastic master
equation is
dpn
dt = (n âˆ’1)Î»pnâˆ’1(t) âˆ’nÎ»pn(t),
n0 â‰¤n < âˆ,
(7.4.61)
pn(t) = 0 for 0 < n < n0, where pn(t) denotes the probability that we have n particles of
B at time t. The initial condition is that pn0(0) = 1 and pm(0) = 0 for m Ì¸= n0 where n0
denotes the initial number of molecules of B.
Taking the Laplace transform of Equation 7.4.61, we ï¬nd that
sPn(s) = (n âˆ’1)Î»Pnâˆ’1(s) âˆ’nÎ»Pn(s),
n0 < n < âˆ,
(7.4.62)
and
sPn0(s) âˆ’1 = âˆ’nÎ»Pn0(s).
(7.4.63)
Therefore, solving for Pn(s),
Pn(s) = (n âˆ’1)Î»
s + nÎ» Pnâˆ’1(s) = Î»nâˆ’n0(n âˆ’1)!
(n0 âˆ’1)!
n
Y
k=n0
(s + kÎ»)âˆ’1.
(7.4.64)
From partial fractions,
Pn(s) = (n âˆ’1)!
(n0 âˆ’1)!
n
X
k=n0
(âˆ’1)kâˆ’n0
(k âˆ’n0)!(n âˆ’k)!(s + kÎ»).
(7.4.65)
6 Ishida, K., 1969: Stochastic model for autocatalytic reaction. Bull. Chem. Soc. Japan, 42, 564â€“565.

Random Processes
373
Taking the inverse Laplace transform,
pn(t) =
(n âˆ’1)!
(n0 âˆ’1)!(n âˆ’n0)!
n
X
k=n0
(âˆ’1)kâˆ’n0(n âˆ’n0)!
(k âˆ’n0)!(n âˆ’k)! eâˆ’Î»kt
(7.4.66)
=
(n âˆ’1)!eâˆ’Î»n0t
(n0 âˆ’1)!(n âˆ’n0)!
nâˆ’n0
X
j=0
(âˆ’1)j(n âˆ’n0)!
j!(n âˆ’n0 âˆ’j)!eâˆ’Î»jt
(7.4.67)
=
(n âˆ’1)!eâˆ’Î»n0t
(n0 âˆ’1)!(n âˆ’n0)!
 1 âˆ’eâˆ’Î»tnâˆ’n0 ,
(7.4.68)
where we introduced j = k âˆ’n0 and eliminated the summation via the binomial theorem.
Equation 7.4.68 is identical to results7 given by DelbrÂ¨uck using another technique.
âŠ“âŠ”
â€¢ Example 7.4.2
In the chemical reaction rA
Î»
â†’
â†
Âµ B, r molecules of A combine to form one molecule of B.
If X(t) = n is the number of B molecules, then the probability pn(t) = P{X(t) = n} of
having n molecules of B is given by
dpn
dt = âˆ’[nÂµ + (N âˆ’n)Î»] pn + (N âˆ’n + 1)Î»pnâˆ’1 + (n + 1)Âµpn+1,
(7.4.69)
where 0 â‰¤n â‰¤N, rN is the total number of molecules of A, Î» is the rate at which r
molecules of A combine to produce B, and Âµ is the rate at which B decomposes into A.
Multiplying Equation 7.4.69 by zn and summing from n = âˆ’1 to N + 1,
N+1
X
n=âˆ’1
zn dpn
dt = âˆ’NÎ»
N+1
X
n=âˆ’1
znpn + (Î» âˆ’Âµ)
N+1
X
n=âˆ’1
nznpn + NÎ»
N+1
X
n=âˆ’1
znpnâˆ’1
âˆ’Î»
N+1
X
n=âˆ’1
(n âˆ’1)znpnâˆ’1 + Âµ
N+1
X
n=âˆ’1
(n + 1)znpn+1.
(7.4.70)
Deï¬ning
F(z, t) =
N+1
X
n=âˆ’1
pn(t)zn,
|z| < 1,
(7.4.71)
with pâˆ’1 = pN+1 = 0, we have that
âˆ‚F
âˆ‚t =
N+1
X
n=âˆ’1
zn dpn
dt ,
(7.4.72)
âˆ‚F
âˆ‚z =
N+1
X
n=âˆ’1
nznâˆ’1pn =
N
X
i=âˆ’2
(i + 1)zipi+1 =
N+1
X
i=âˆ’1
(i + 1)zipi+1,
(7.4.73)
7 DelbrÂ¨uck, M., 1940: Statistical ï¬‚uctuations in autocatalytic reactions. J. Chem. Phys., 8, 120â€“124.
See his Equation 7.

374
Advanced Engineering Mathematics: A Second Course
z âˆ‚F
âˆ‚z =
N+1
X
n=âˆ’1
nznpn,
(7.4.74)
z2 âˆ‚F
âˆ‚z =
N+1
X
n=âˆ’1
nzn+1pn =
N+2
X
i=0
(i âˆ’1)zipiâˆ’1 =
N+1
X
i=âˆ’1
(i âˆ’1)zipiâˆ’1,
(7.4.75)
and
F =
N+1
X
n=âˆ’1
zn+1pn =
N+2
X
i=0
zipiâˆ’1 =
N+1
X
i=âˆ’1
zipiâˆ’1.
(7.4.76)
Therefore, the diï¬€erential-diï¬€erence equation, Equation 7.4.70, can be replaced by
âˆ‚F
âˆ‚t = NÎ»(z âˆ’1)F +

Âµ âˆ’(Âµ âˆ’Î»)z âˆ’Î»z2 âˆ‚F
âˆ‚z .
(7.4.77)
Using the same technique as above, this partial diï¬€erential equation can be written as
dt
âˆ’1 =
dz
(1 âˆ’z)(Âµ + Î»z) =
dF
âˆ’NÎ»(z âˆ’1).
(7.4.78)
Equation 7.4.78 yields the independent solutions
1 âˆ’z
Âµ + Î»z eâˆ’(Âµ+Î»)t = Î¾(z, t) = constant,
(7.4.79)
and
(Âµ + Î»)âˆ’NF(z, t) = Î·(z, t) = another constant,
(7.4.80)
where f(Â·) is an arbitrary, diï¬€erentiable function.
If there are m units of B at t = 0,
0 â‰¤m â‰¤N, the initial condition is F(z, 0) = zm. Then,
f
 1 âˆ’z
Âµ + Î»z

=
zm
(Âµ + Î»z)N ,
(7.4.81)
or
f(x) = (1 âˆ’Âµx)m
(Âµ + Î»)N (1 + Î»x)Nâˆ’m.
(7.4.82)
After some algebra, we ï¬nally ï¬nd that
F(z, t) =
1
(Âµ + Î»)N
n
Âµ
h
1 âˆ’eâˆ’(Âµ+Î»)ti
+ z
h
Î» + Âµeâˆ’(Âµ+Î»)tiom
Ã—
n
Âµ + Î»eâˆ’(Âµ+Î»)t + Î»z
h
1 âˆ’eâˆ’(Âµ+Î»)tioNâˆ’m
.
(7.4.83)
Computing the mean and variance, we obtain
E(X) =
m
Âµ + Î»
h
Î» + Âµeâˆ’(Âµ+Î»)ti
+ (N âˆ’m)Î»
Âµ + Î»
h
1 âˆ’eâˆ’(Âµ+Î»)ti
,
(7.4.84)
and
Var(X) =
mÂµ
(Âµ + Î»)2
h
Î» + Âµeâˆ’(Âµ+Î»)ti h
1 âˆ’eâˆ’(Âµ+Î»)ti
+ (N âˆ’m)Î»
(Âµ + Î»)2
h
Âµ + Î»eâˆ’(Âµ+Î»)ti h
1 âˆ’eâˆ’(Âµ+Î»)ti
.
(7.4.85)

Random Processes
375
Problems
1. During their study of growing cancerous cells (with growth rate Î±), BartoszyÂ´nski et
al.8 developed a probabilistic model of a tumor that has not yet metastasized. In their
mathematical derivation a predictive model gives the probability pn(t) that certain nth
type of cells (out of N) will develop. This probability can change in two ways: (1) Each of
the existing cells has the probability Î»nâˆ†t + o(âˆ†t) of mutating to another type between t
and t+âˆ†t. (2) The probability that cells in state n at time t will shed a metastasis between
t and t + âˆ†t is Âµncet/Î±âˆ†t + o(âˆ†t), where Âµ is a constant and c is the size of a single cell.
Setting Ï = Î»c/N and Î½ = Âµc, the governing equations for pn(t) are
dpn
dt = âˆ’(Ï + Î½)net/Î±pn + Ï(n + 1)et/Î±pn+1,
n = 0, 1, 2, . . . , N âˆ’1,
and
dpN
dt
= âˆ’(Ï + Î½)Net/Î±pN,
with the initial conditions pN(0) = 1 and pn(0) = 0 if n Ì¸= N.
Step 1: Introducing the generating function
Ï†(z, t) =
N
X
n=0
znpn(t),
0 â‰¤z â‰¤1,
show that our system of linear diï¬€erential-diï¬€erence equations can be written as the ï¬rst-
order partial diï¬€erential equation
âˆ‚Ï†
âˆ‚t = [Ï âˆ’(Ï + Î½)z]et/Î± âˆ‚Ï†
âˆ‚z
with Ï†(z, 0) = zN.
Step 2: Solve the partial diï¬€erential equation in Step 1 and show that
Ï†(z, t) =

Ï
Ï + Î½
N 
1 âˆ’

1 âˆ’Ï + Î½
Ï
z

exp
h
âˆ’Î±(Ï + Î½)

et/a âˆ’1
iN
.
Project: Stochastic Simulation of Chemical Reactions
Most stochastic descriptions of chemical reactions cannot be attacked analytically and
numerical simulation is necessary. The purpose of this project is to familiarize you with
some methods used in the stochastic simulation of chemical reactions. In particular, we will
use the Lokta reactions given by the reaction equations:
A + X
k1
âˆ’â†’2X,
(1)
X + Y
k2
âˆ’â†’2Y,
(2)
Y
k3
âˆ’â†’Z.
(3)
8 BartoszyÂ´nski, R., B. F. Jones, and J. P. Klein, 1985: Some stochastic models of cancer metastases.
Commun. Statist.-Stochastic Models, 1, 317â€“339.

376
Advanced Engineering Mathematics: A Second Course
0
5
10
15
20
25
30
0
500
1000
1500
2000
2500
3000
 time
 number of x molecules
0
5
10
15
20
25
30
0
500
1000
1500
2000
2500
3000
 time
 number of y molecules
Figure 7.4.1: The temporal variation of the molecules in a Lokta reaction when âˆ†t = 10âˆ’5, k1a = 10,
k2 = 0.01, k3 = 10, and x(0) = y(0) = 1000.
Surprisingly, simple numerical integration of the master equation is not fruitful. This occurs
because of the number and nature of the independent variables; there is only one master
equation but N reactants and time for independent variables.
An alternative to integrating the master equation is a direct stochastic simulation. In
this approach, the (transition) probability for each reaction is computed: p1 = k1axâˆ†t,
p2 = k2xyâˆ†t, and p3 = k3yâˆ†t, where âˆ†t is the time between each consecutive state and
a is the constant number of molecules of A.
The obvious question is: Which of these
probabilities should we use?
Our ï¬rst attempt follows Nakanishi:9 Assume that âˆ†t is suï¬ƒciently small so that
p1 + p2 + p3 < 1.
Using a normalized uniform distribution, such as MATLABâ€™s rand,
compute a random variable r for each time step. Then march forward in time. At each
time step, there are four possibilities. If 0 < r â‰¤p1, then the ï¬rst reaction occurs and
x(t + âˆ†t) = x(t) + 1, y(t + âˆ†t) = y(t). If p1 < r â‰¤p1 + p2, then the second reaction occurs
and x(t + âˆ†t) = x(t) âˆ’1, y(t + âˆ†t) = y(t) + 1. If p1 + p2 < r â‰¤p1 + p2 + p3, then the third
reaction occurs and x(t + âˆ†t) = x(t), y(t + âˆ†t) = y(t) âˆ’1. Finally, if p1 + p2 + p3 < r â‰¤1,
then no reaction occurs and x(t + âˆ†t) = x(t), y(t + âˆ†t) = y(t).
For the ï¬rst portion of this project, create MATLAB code to simulate our chemical
reaction using this simulation technique. Explore how your results behave as you vary x(0),
y(0) and especially âˆ†t. See Figure 7.4.1.
One of the diï¬ƒculties in using Nakanishiâ€™s method is the introduction of âˆ†t. What
value should we choose to ensure that p1 + p2 + p3 < 1? Several years later, Gillespie10
9 This is the technique used by Nakanishi, T., 1972: Stochastic analysis of an oscillating chemical
reaction. J. Phys. Soc. Japan, 32, 1313â€“1322.
10 Gillespie, D. T., 1976: A general method for numerically simulating the stochastic time evolution
of coupled chemical reactions. J. Comput. Phys., 22, 403â€“434; Gillespie, D. T., 1977: Exact stochastic

Random Processes
377
0
5
10
15
20
25
30
0
1000
2000
3000
4000
5000
 time
 number of x molecules
0
5
10
15
20
25
30
0
1000
2000
3000
4000
5000
 time
 number of y molecules
Figure 7.4.2: Same as Figure 7.4.1 except that Gillespieâ€™s method has been used.
developed a similar algorithm. He introduced three parameters, a1 = k1ax, a2 = k2xy, and
a3 = k3y, along with a0 = a1 + a2 + a3. These parameters a1, a2, and a3 are similar to the
probabilities p1, p2, and p3. Similarly, he introduced a random number r2 that is chosen
from a normalized uniform distribution. Then, if 0 < r2a0 â‰¤a1, the ï¬rst reaction occurs
and x(t + âˆ†t) = x(t) + 1, y(t + âˆ†t) = y(t). If a1 < r2a0 â‰¤a1 + a2, then the second reaction
occurs and x(t + âˆ†t) = x(t) âˆ’1, y(t + âˆ†t) = y(t) + 1. If a1 + a2 < r2a0 â‰¤a0, then the third
reaction occurs and x(t + âˆ†t) = x(t), y(t + âˆ†t) = y(t) âˆ’1. Because of his selection criteria
for the reaction that occurs during a time step, one of the three reactions must take place.
See Figure 7.4.2.
The most radical diï¬€erence between the Nakanishi and Gillespie schemes involves the
time step. It is no longer constant but varies with time and equals âˆ†t = ln(1/r1)/a0, where
r1 is a random variable selected from a normalized uniform distribution. The theoretical
justiï¬cation for this choice is given in Section III of his paper.
For the second portion of this project, create MATLAB code to simulate our chemical
reaction using Gillespieâ€™s technique. You might like to plot x(t) vs y(t) and observe the
patterns that you obtain.
Finally, for a speciï¬c time, compute the probability density function that gives the
probability that x and y molecules exist. See Figure 7.4.3.
7.5 POISSON PROCESSES
The Poisson random process is a counting process that counts the number of occur-
rences of some particular event as time increases. In other words, for each value of t, there
simulation of coupled chemical reactions. J. Phys. Chem., 81, 2340â€“2361

378
Advanced Engineering Mathematics: A Second Course
0
1000
2000
3000
0
0.2
0.4
0.6
0.8
1
1.2x 10
âˆ’3
number of  x  molecules
Estimated PDF
0
1000
2000
3000
0
0.2
0.4
0.6
0.8
1
1.2x 10
âˆ’3
number of  y  molecules
Figure 7.4.3: The estimated probability density function for the chemical reactions given by Equations
(1) through (3) (for X on the left, Y on the right) at time t = 10. Five thousand realizations were used in
these computations.
is a number N(t), which gives the number of events that occurred during the interval [0, t].
For this reason N(t) is a discrete random variable with the set of possible values {0, 1, 2, . . .}.
Figure 7.5.1 illustrates a sample function. We can express this process mathematically by
N(t) =
âˆ
X
n=0
H(t âˆ’T[n]),
(7.5.1)
where T[n] is the time to the nth arrival, a random sequence of times. The question now
becomes how to determine the values of T[n]. The answer involves three rather physical
assumptions. They are:
1. N(0) = 0.
2. N(t) has independent and stationary increments. By stationary we mean that for any
two equal time intervals âˆ†t1 and âˆ†t2, the probability of n events in âˆ†t1 equals the
probability of n events in âˆ†t2. By independent we mean that for any time interval
(t, t + âˆ†t) the probability of n events in (t, t + âˆ†t) is independent of how many events
have occurred earlier or how they have occurred.
3.
P[N(t + âˆ†t) âˆ’N(t) = k ] =
( 1 âˆ’Î»âˆ†t,
k = 0,
Î»âˆ†t,
k = 1,
0,
k > 1,
(7.5.2)
for all t. Here Î» equals the expected number of events in an interval of unit length of
time. Because E[N(t)] = Î», it is the average number of events that occur in one unit
of time and in practice it can be measured experimentally.
We begin our analysis of Poisson processes by ï¬nding P[N(t) = 0 ] for any t > 0. If
there are no arrivals in [0, t], then there must be no arrivals in [0, tâˆ’âˆ†t] and also no arrivals

Random Processes
379
4
3
2
1
N(t)
t
t
t
t
t
t
1
2
4
5
3
Figure 7.5.1: Schematic of a Poisson process.
in (t âˆ’âˆ†t, t]. Therefore,
P[N(t) = 0 ] = P[N(t âˆ’âˆ†t) = 0, N(t) âˆ’N(t âˆ’âˆ†t) = 0 ].
(7.5.3)
Because N(t) is independent,
P[N(t) = 0 ] = P[N(t âˆ’âˆ†t) = 0 ]P[N(t) âˆ’N(t âˆ’âˆ†t) = 0 ].
(7.5.4)
Furthermore, since N(t) is stationary,
P[N(t) = 0 ] = P[N(t âˆ’âˆ†t) = 0 ]P[N(t + âˆ†t) âˆ’N(t) = 0 ].
(7.5.5)
Finally, from Equation 7.5.2,
P[N(t) = 0 ] = P[N(t âˆ’âˆ†t) = 0 ](1 âˆ’Î»âˆ†t).
(7.5.6)
Let us denote P[N(t) = 0 ] by P0(t). Then,
P0(t) = P0(t âˆ’âˆ†t)(1 âˆ’Î»âˆ†t),
(7.5.7)
or
P0(t) âˆ’P0(t âˆ’âˆ†t)
âˆ†t
= âˆ’Î»P0(t âˆ’âˆ†t).
(7.5.8)
Taking the limit as âˆ†t â†’0, we obtain the (linear) diï¬€erential equation
dP0(t)
dt
= âˆ’Î»P0(t).
(7.5.9)
The solution of Equation 7.5.9 is
P0(t) = Ceâˆ’Î»t,
(7.5.10)
where C is an arbitrary constant. To evaluate C, we have the initial condition P0(0) =
P[N(0) = 0 ] = 1 from Axion 1. Therefore,
P[N(t) = 0 ] = P0(t) = eâˆ’Î»t.
(7.5.11)

380
Advanced Engineering Mathematics: A Second Course
Next, let us ï¬nd P1(t) = P[N(t) = 1 ]. We either have no arrivals in [0, t âˆ’âˆ†t] and one
arrival in (t âˆ’âˆ†t, t] or one arrival in [0, t âˆ’âˆ†t] and no arrivals in (t âˆ’âˆ†t, t]. These are the
only two possibilities because there can be at most one arrival in a time interval âˆ†t. The
two events are mutually exclusive. Therefore,
P[N(t) = 1 ] = P[N(t âˆ’âˆ†t) = 0, N(t) âˆ’N(t âˆ’âˆ†t) = 1 ]
+ P[N(t âˆ’âˆ†t) = 0, N(t) âˆ’N(t âˆ’âˆ†t) = 0 ]
(7.5.12)
= P[N(t âˆ’âˆ†t) = 0 ]P[N(t) âˆ’N(t âˆ’âˆ†t) = 1 ]
+ P[N(t âˆ’âˆ†t) = 1 ]P[N(t) âˆ’N(t âˆ’âˆ†t) = 0 ]
(7.5.13)
= P[N(t âˆ’âˆ†t) = 0 ]P[N(t + âˆ†t) âˆ’N(t) = 1 ]
+ P[N(t âˆ’âˆ†t) = 1 ]P[N(t + âˆ†t) âˆ’N(t) = 0 ].
(7.5.14)
Equation 7.5.13 follows from independence while Equation 7.5.14 follows from stationarity.
Introducing P1(t) in Equation 7.5.14 and using Axion 3,
P1(t) = P0(t âˆ’âˆ†t)Î»âˆ†t + P1(t âˆ’âˆ†t)(1 âˆ’Î»âˆ†t),
(7.5.15)
or
P1(t) âˆ’P1(t âˆ’âˆ†t)
âˆ†t
= âˆ’Î»P1(t âˆ’âˆ†t) + Î»P0(t âˆ’âˆ†t).
(7.5.16)
Taking the limit as âˆ†t â†’0, we obtain
dP1(t)
dt
+ Î»P1(t) = Î»P0(t).
(7.5.17)
In a similar manner, we can prove that
dPk(t)
dt
+ Î»Pk(t) = Î»Pkâˆ’1(t),
(7.5.18)
where k = 1, 2, 3, . . . and Pk(t) = P[N(t) = k].
This set of simultaneous linear equations can be solved recursively. Its solution is
Pk(t) = exp(âˆ’Î»t)(Î»t)k
k!
,
k = 0, 1, 2, . . . ,
(7.5.19)
which is the Poisson probability mass function. Here Î» is the average number of arrivals
per second.
In the realization of a Poisson process, one of the important quantities is the arrival
time, tn, shown in Figure 7.5.1. Of course, the arrival time is also a random process and
will change with each new realization. A related quantity Zi = ti âˆ’tiâˆ’1, the time intervals
between two successive occurrences (interoccurrence times) of Poisson events. We will now
show that the random variables Z1, Z2, etc., are independent and identically distributed
with
P(Zn â‰¤x) = 1 âˆ’eâˆ’Î»x,
x â‰¥0,
n = 1, 2, 3, . . .
(7.5.20)
We begin by noting that
P(Z1 > t) = P[N(t) = 0] = eâˆ’Î»t
(7.5.21)

Random Processes
381
from Equation 7.5.19. Therefore, Z1 has an exponential distribution.
Let us denote its probability density by pZ1(z1). From the joint conditional density
function,
P(Z2 > t) =
Z Î¾1
0
P(Z2 > t|Z1 = z1)pZ1(z1) dz1,
(7.5.22)
where 0 < Î¾1 < t. If Z1 = z1, then Z2 > t if and only if N(t + z1) âˆ’N(z1) = 0. Therefore,
using the independence and stationary properties,
P{Z2 > t|Z1 = P[N(t + z1) âˆ’N(z1) = 0]} = P[N(t) = 0] = eâˆ’Î»t.
(7.5.23)
Consequently,
P(Z2 > t) = eâˆ’Î»t,
(7.5.24)
showing that Z2 is also exponential. Also, Z2 is independent of Z1. Now, let us introduce
pZ2(z2) as the probability density of Z1 + Z2. By similar arguments we can show that Z3
is also exponential. The ï¬nal result follows by induction.
â€¢ Example 7.5.1: Random telegraph signal
We can use the fact that interoccurrence times are independent and identically dis-
tributed to realize the Poisson process. An important application of this is in the generation
of the random telegraph signal: X(t) = (âˆ’1)N(t). However, no one uses this deï¬nition to
compute the signal; they use the arrival times to change the signal from +1 to âˆ’1 or vice
versa.
We begin by noting that Ti = Tiâˆ’1 + Zi, with i = 1, 2, . . ., T0 = 0, and Ti is the ith
arrival time. Each Zi has the same exponent probability density function. From Equation
6.4.17,
Zi = 1
Î» ln

1
1 âˆ’Ui

,
(7.5.25)
where the Uiâ€™s are from a uniform distribution. The realization of a random telegraphic
signal is given by the MATLAB code:
clear
N = 100; % number of switches in realization
lambda = 0.15; % switching rate
X = [ ];
% generate N uniformly distributed random variables
S = rand(1,N);
% transform S into an exponential random variable
T = - log(S)/lambda;
V = cumsum(T); % compute switching times
t = [0.01:0.01:100]; % create time array
icount = 1; amplitude = -1; % initialize X(t)
for k = 1:10000
if ( t(k) >= V(icount) ) % at each switching point
icount = icount + 1;
amplitude = - amplitude; % switch sign
end
X(k) = amplitude; % generate X(t)
end

382
Advanced Engineering Mathematics: A Second Course
plot(t,X) % plot results
xlabel(â€™\it tâ€™,â€™FontSizeâ€™,25);
ylabel(â€™\it X(t)/aâ€™,â€™FontSizeâ€™,25);
axis([0 max(t) -1.1 1.1])
This was the MATLAB code that was used to generate Figure 7.5.2.
âŠ“âŠ”
â€¢ Example 7.0.2
It takes a workman an average of one hour to put a widget together. Assuming that
the task can be modeled as a Poisson process, what is the probability that a workman can
build 12 widgets during an eight-hour shift?
The probability that n widgets can be constructed by time t is
P[N(t) = n] = eâˆ’Î»t (Î»t)n
n!
.
(7.5.26)
Therefore, the probability that 12 or more widgets can be constructed in eight hours is
P[N(t) â‰¥12] = eâˆ’8
âˆ
X
n=12
8n
n! = 0.1119,
(7.5.27)
since Î» = 1.
We could have also obtained our results by creating 12 exponentially distributed time
periods and summed them together using MATLAB:
t uniform = rand(1,12);
T = - log(1-t uniform);
total time = sum(T);
Then, by executing this code a large number N of times and counting the number icount
of times that total time <= 8, the probability equals icount / N.
Problems
1. Use the generating function
F(z, t) =
âˆ
X
n=0
pn(t)zn,
|z| < 1,
with F(z, 0) = 1 to solve Equation 7.5.18 by showing that F(z, t) = eÎ»t(zâˆ’1). Then, by
expanding F(z, t), recover Equation 7.5.19.

Random Processes
383
âˆ’2
âˆ’1.5
âˆ’1
âˆ’0.5
0
0.5
1
1.5
2
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
 y
 P(y)
Figure 7.5.1: The probability density P(y) of the output from an ideal integrator with ï¬nite memory
when the input is a random telegraphic signal when âˆ†t = 0.01, Î» = 2, and Ï„1 = 10.
Project: Output from a Filter
When the Input Is a Random Telegraphic Signal 11
In the study of many systems, such as linear ï¬lters, the output y(Â·) can be written as
y(t) =
Z t
âˆ’âˆ
W(t âˆ’Ï„)x(Ï„) dÏ„,
where W(Â·) is the weight function and x(Â·) is the input. The purpose of this project is
to explore the probability density P(y) of the output when x(t) is the random telegraphic
signal, a Poisson random process. You will ï¬lter this input two ways: (1) ideal integrator
with ï¬nite memory: W(t) = H(t) âˆ’H(t âˆ’Ï„1), Ï„1 > 0, and (2) simple RC = 1 low-pass
ï¬lter W(t) = eâˆ’tH(t).
Step 1: Use MATLAB to code x(t) where the expected time between the zeros is Î».
Step 2: Develop MATLAB code to compute y(t) for each of the weight functions W(t).
Step 3: Compute P(y) for both ï¬lters. How do your results vary as Î» varies?
Further Readings
Beckmann, P., 1967: Probability in Communication Engineering. Harcourt, Brace & World,
511 pp. A presentation of probability as it applies to problems in communication engineer-
ing.
Gillespie, D. T., 1991: Markov Processes: An Introduction for Physical Scientists. Academic
Press, 592 pp. For the scientist who needs an introduction to the details of the subject.
Hsu, H., 1997: Probability, Random Variables, & Random Processes. McGraw-Hill, 306 pp.
Summary of results plus many worked problems.
11 Suggested by a paper by McFadden, J. A., 1959: The probability density of the output of a ï¬lter when
the input is a random telegraphic signal: Diï¬€erential-equation approach. IRE Trans. Circuit Theory, 6,
228â€“233.

384
Advanced Engineering Mathematics: A Second Course
âˆ’1
âˆ’0.5
0
0.5
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
 y
 P(y)
Figure 7.5.2: The probability density P(y) of the output from a simple RC = 1 ï¬lter, when the input is
a random telegraphic signal, when Î» = 1, and âˆ†t = 0.05.
Kay, S. M., 2006: Intuitive Probability and Random Processes Using MATLAB. Springer,
833 pp. A well-paced book designed for the electrical engineering crowd.
Ross, S. M., 2007: Introduction to Probability Models. Academic Press, 782 pp. An intro-
ductory undergraduate book in applied probability and stochastic processes.
Tuckwell, H. C., 1995: Elementary Applications of Probability Theory. Chapman & Hall,
292 pp. This book presents applications using probability theory, primarily from biology.

Rt/L
0
0.5
1
1.5
2
2.5
3
3.5
4
I(t)
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
Chapter 8
ItË†oâ€™s Stochastic Calculus
In elementary diï¬€erential equation classes, students study the solution to ï¬rst-order
ordinary diï¬€erential equations
dx
dt = a(t, x),
x(0) = x0.
(8.0.1)
There we showed that Equation 8.0.1 has the solution
x(t) = x(0) +
Z t
0
a[Î·, x(Î·)] dÎ·.
(8.0.2)
Consider now the analogous stochastic diï¬€erential equation:
dX(t) = a[t, X(t)] dt,
X(0) = X0.
(8.0.3)
Although Equations 8.0.1 and 8.0.3 formally appear the same, an immediate question is
what is meant by dX(t). In elementary calculus, the concept of the inï¬nitesimal involves
limits, continuity, and so forth. As we shall see in Section 8.2, Brownian motion, a very
common stochastic process, is nowhere diï¬€erentiable. Here we can merely say that dX(t) =
X(t + dt) âˆ’X(t).
Consider now a modiï¬cation of Equation 8.0.3 where we introduce a random forcing:
dX(t) = a[t, X(t)] dt + b[t, X(t)] dB(t),
X(0) = X0.
(8.0.4)
Here dB(t) = B(t + dt) âˆ’B(t), B(t) denotes Brownian motion and a[t, X(t)] and b[t, X(t)]
are deterministic functions. Consequently, changes to X(t) result from (1) the eï¬€ects of the
385

386
Advanced Engineering Mathematics: A Second Course
initial conditions and (2) noise generated by Brownian motion (the driving force). Stochastic
processes governed by Equation 8.0.4 are referred to as ItË†o processes.
Following the methods used to derive 8.0.2, we can formally write the solution to
Equation 8.0.4 as
X(t) = X0 +
Z t
0
a[Î·, X(Î·)] dÎ· +
Z t
0
b[Î·, X(Î·)] dB(Î·).
(8.0.5)
The ï¬rst integral in Equation 8.0.5 is the conventional Riemann integral from elementary
calculus and is well understood. The second integral, however, is new and must be treated
with care. It is called ItË†oâ€™s stochastic integral and treated in Section 8.3.
In summary, a simple analog to ï¬rst-order ordinary diï¬€erential equations for a single
random variable X(t) raises several important questions. What is meant by the inï¬nitesimal
and the integral in stochastic calculus? In this chapter we will focus on ItË†o processes and
the associated calculus. Although ItË†oâ€™s calculus is an important discipline, it is not the only
form of stochastic calculus. The interested student is referred elsewhere for further study.
Problems
1. The Poisson random process N(t) is deï¬ned by
N(t) =
âˆ
X
n=1
H(t âˆ’tn),
where tn is a sequence of independent and identically distributed inter-arrival times tn.
A graphical representation of N(t) would consist of ever-increasing steps with the edges
located at t = tn. Use the deï¬nition of dN(t) = N(t + dt) âˆ’N(t) to show that
dN(t) =

1,
for t = tn,
0,
otherwise.
2. The telegraph signal is deï¬ned by X(t) = (âˆ’1)N(t), where N(t) is given by the Poisson
random distribution in Problem 1. Show1 that
dX(t) = X(t + dt) âˆ’X(t) = (âˆ’1)N(t) h
(âˆ’1)dN(t) âˆ’1
i
= âˆ’2X(t) dN(t).
Hint: Consider dN(t) at various times.
3. If X(t) and Y (t) denote two stochastic processes, use the deï¬nition of the derivative to
show that (a) d[cX(t)] = c dX(t), where c is a constant, (b) d[X(t)Â±Y (t)] = dX(t)Â±dY (t),
and (c) d[X(t)Y (t)] = X(t) dY (t) + Y (t) dX(t) + dX(t) dY (t).
8.1 RANDOM DIFFERENTIAL EQUATIONS
A large portion of this book has been devoted to solving diï¬€erential equations. Here
we examine the response of diï¬€erential equations to random forcing where the diï¬€erential
1 Taken from Janaswamy, R., 2013: On random time and on the relation between wave and telegraph
equation. IEEE Trans. Antennas Propag., 61, 2735â€“2744.

ItË†oâ€™s Stochastic Calculus
387
equation describes a nonrandom process. This is an important question in the sciences and
engineering because noise, a random phenomenon, is ubiquitous in nature.
Because the solution to random diï¬€erential equations can be found by conventional
techniques, we can use them to study the eï¬€ect of randomness on the robustness of a
solution to a diï¬€erential equation subject to small changes of the initial condition. Although
this may be of considerable engineering interest, it is really too simple to develop a deep
understanding of stochastic diï¬€erential equations.
â€¢ Example 8.1.1: LR circuit
One of the simplest diï¬€erential equations involves the mathematical model for an LR
electrical circuit:
LdI
dt + RI = E(t),
(8.1.1)
where I(t) denotes the current within an electrical circuit with inductance L and resistance
R, and E(t) is the mean electromotive force. If we solve this ï¬rst-order ordinary diï¬€erential
equation using an integrating factor, its solution is
I(t) = I(0) exp

âˆ’Rt
L

+ 1
L exp

âˆ’Rt
L
 Z t
0
F(Ï„) exp
RÏ„
L

dÏ„.
(8.1.2)
Clearly, if the electromotive forcing is random, so is the current.
In the previous chapter we showed that the mean and variance were useful parameters
in characterizing a random variable. This will also be true here. If we ï¬nd the mean of the
solution,
E[I(t)] = I(0) exp

âˆ’Rt
L

(8.1.3)
provided E[F(t)] = 0. Thus, the mean of the current is the same as that for an ideal LR
circuit.
Turning to the variance,
Ïƒ2
X(t) = E[I2(t)] âˆ’{E[I(t)]}2
(8.1.4)
= E

I2(0) exp

âˆ’2Rt
L

+ 2I(0)
L
exp

âˆ’2Rt
L
 Z t
0
E[F(Ï„)] exp
RÏ„
L

dÏ„
+ 1
L2 exp

âˆ’2Rt
L
 Z t
0
Z t
0
E[F(Ï„)F(Ï„ â€²)] exp
R(Ï„ + Ï„ â€²)
L

dÏ„ â€² dÏ„
(8.1.5)
âˆ’I2(0) exp

âˆ’2Rt
L

= 1
L2 exp

âˆ’2Rt
L
 Z t
0
Z t
0
E[F(Ï„)F(Ï„ â€²)] exp
R(Ï„ + Î¾)
L

dÏ„ â€² dÏ„.
(8.1.6)
To proceed further we need the autocorrelation E[F(Ï„)F(Ï„ â€²)]. In papers by Ornstein
et al.2 and Jones and McCombie,3 they adopted a random process with the autocorrelation
2 Ornstein, L. S., H. C. Burger, J. Taylor, and W. Clarkson, 1927: The Brownian movement of a
galvanometer and the inï¬‚uence of the temperature of the outer circuit. Proc. Roy. Soc. London, Ser. A,
115, 391â€“406.
3 Jones, R. V., and C. W. McCombie, 1952: Brownian ï¬‚uctuations in galvanometer and galvanometer
ampliï¬ers. Phil. Trans. Roy. Soc. London, Ser. A, 244, 205â€“230.

388
Advanced Engineering Mathematics: A Second Course
function
E[F(Ï„)F(Ï„ â€²)] = 2DÎ´(Ï„ âˆ’Ï„ â€²).
(8.1.7)
The advantage of this process is that it is mathematically the simplest because it possesses a
white power spectrum. Unfortunately this random process can never be physically realized
because it would possess inï¬nite mean square power. All physically realizable processes
involve a power spectrum that tends to zero at suï¬ƒciently high frequencies. If Î¦(Ï‰) denotes
the power spectrum, this condition can be expressed as
Z âˆ
0
Î¦(Ï‰) dÏ‰ < âˆ.
(8.1.8)
In view of these considerations, let us adopt the autocorrelation
RX(Ï„ âˆ’Ï„ â€²) =
Z âˆ
0
Î¦(Ï‰) cos[Ï‰(Ï„ âˆ’Ï„ â€²)] dÏ‰,
(8.1.9)
where Î¦(Ï‰) is the power spectrum of F(Ï„). Therefore, the variance becomes
Ïƒ2
X(t) = 1
L2
Z t
0
Z t
0
Z âˆ
0
Î¦(Ï‰) exp

âˆ’R(t âˆ’Ï„)
L

exp

âˆ’R(t âˆ’Ï„ â€²)
L

cos[Ï‰(Ï„ âˆ’Ï„ â€²)] dÏ‰ dÏ„ dÏ„ â€².
(8.1.10)
Reversing the ordering of integration,
Ïƒ2
X(t) = 1
L2
Z âˆ
0
Î¦(Ï‰)
Z t
0
Z t
0
exp

âˆ’R(2t âˆ’Ï„ âˆ’Ï„ â€²)
L

cos[Ï‰(Ï„ âˆ’Ï„ â€²)] dÏ„ dÏ„ â€² dÏ‰.
(8.1.11)
We can evaluate the integrals involving Ï„ and Ï„ â€² exactly. Equation 8.1.11 then becomes
Ïƒ2
X(t) =
Z âˆ
0
Î¦(Ï‰)
Ï‰2 + R2/L2
h
1 + eâˆ’2Rt/L âˆ’2eâˆ’Rt/L cos(Ï‰t)
i
dÏ‰.
(8.1.12)
Let us now consider some special cases. As t â†’0, Ïƒ2
X(t) â†’0 and the variance is
initially small. On the other hand, as t â†’âˆ,
Ïƒ2
X(t) =
Z âˆ
0
Î¦(Ï‰)
Ï‰2 + R2/L2 dÏ‰.
(8.1.13)
Thus, the variance grows to a constant value, which we would have found by using Fourier
transforms to solve the diï¬€erential equation.
Consider now the special case Î¦(Ï‰) = 2D/Ï€, a forcing by white noise. Ignoring the
defects in this model, we can evaluate the integrals in Equation 8.1.13 exactly and ï¬nd that
Ïƒ2
X(t) = DL
R

1 âˆ’eâˆ’2Rt/L
.
(8.1.14)
These results are identical to those found by Uhlenbeck and Ornstein4 in their study of a
free particle in Brownian motion.
âŠ“âŠ”
4 Uhlenbeck, G. E., and L. S. Ornstein, 1930: On the theory of the Brownian motion. Phys. Review,
36, 823â€“841. See the top of their page 828.

ItË†oâ€™s Stochastic Calculus
389
â€¢ Example 8.1.2: Damped harmonic motion
Another classic diï¬€erential equation that we can excite with a random process is the
damped harmonic oscillator:
yâ€²â€² + 2Î¾Ï‰0yâ€² + Ï‰2
0y = F(t),
(8.1.15)
where 0 â‰¤Î¾ < 1, y denotes the displacement, t is time, Ï‰2
0 = k/m, 2Î¾Ï‰0 = Î²/m, m is
the mass of the oscillator, k is the linear spring constant, and Î² denotes the constant of a
viscous damper. The solution to this second-order ordinary diï¬€erential equation is
y(t) = y(0)eâˆ’Î¾Ï‰0t

cos(Ï‰1t) + Î¾Ï‰0
Ï‰1
sin(Ï‰1t)

+ yâ€²(0)
Ï‰1
eâˆ’Î¾Ï‰t sin(Ï‰1t) +
Z t
0
h(t âˆ’Ï„)F(Ï„) dÏ„,
(8.1.16)
where Ï‰1 = Ï‰0
p
1 âˆ’Î¾2, and
h(t) = eâˆ’Î¾Ï‰0t
Ï‰1
sin(Ï‰1t)H(t).
(8.1.17)
Again we begin by ï¬nding the mean of Equation 8.1.16. It is
E[y(t)] = y(0)eâˆ’Î¾Ï‰0t

cos(Ï‰1t) + Î¾Ï‰0
Ï‰1
sin(Ï‰1t)

+yâ€²(0)
Ï‰1
eâˆ’Î¾Ï‰t sin(Ï‰1t)+
Z t
0
h(tâˆ’Ï„)E[F(Ï„)] dÏ„.
(8.1.18)
If we again choose a random process where E[F(t)] = 0, the integral vanishes and the
stochastic mean of the motion only depends on the initial conditions.
Turning to the variance,
Ïƒ2
X(t) = E[y2(t)] âˆ’{E[y(t)]}2 =
Z t
0
Z t
0
h(t âˆ’Ï„)h(t âˆ’Ï„ â€²)E[F(Ï„)F(Ï„ â€²)] dÏ„ dÏ„ â€².
(8.1.19)
If we again adopt the autocorrelation function
RX(Ï„ âˆ’Ï„ â€²) =
Z âˆ
0
Î¦(Ï‰) cos[Ï‰(Ï„ âˆ’Ï„ â€²)] dÏ‰,
(8.1.20)
where Î¦(Ï‰) is the power spectrum of F(Ï„), then
Ïƒ2
X(t) =
Z âˆ
0
Î¦(Ï‰)
Ï‰2
1
Z t
0
Z t
0
eâˆ’Î¾Ï‰0(2tâˆ’Ï„âˆ’Ï„ â€²) sin[Ï‰1(tâˆ’Ï„)] sin[Ï‰1(tâˆ’Ï„ â€²)] cos[Ï‰(Ï„ âˆ’Ï„ â€²)] dÏ„ dÏ„ â€² dÏ‰.
(8.1.21)
Carrying out the integrations in Ï„ and Ï„ â€², we ï¬nally obtain
Ïƒ2
X(t) =
Z âˆ
0
Î¦(Ï‰)
|â„¦(Ï‰)|2

1 + eâˆ’2Î¾Ï‰0t

1 + 2Î¾Ï‰0
Ï‰1
sin(Ï‰1t) cos(Ï‰1t)
âˆ’eÏ‰0Î¾t

2 cos(Ï‰1t) + 2Î¾Ï‰0
Ï‰1
sin(Ï‰1t)

cos(Ï‰t) âˆ’eÎ¾Ï‰0t 2Ï‰
Ï‰1
sin(Ï‰1t) sin(Ï‰t)
+ Î¾2Ï‰2
0 âˆ’Ï‰2
1 + Ï‰2
Ï‰2
1
sin2(Ï‰1t)

dÏ‰,
(8.1.22)
where |â„¦(Ï‰)|2 = (Ï‰2
0 âˆ’Ï‰2)2 + 4Ï‰2Ï‰2
0Î¾2.

390
Advanced Engineering Mathematics: A Second Course
0
1
2
3
4
âˆ’0.02
âˆ’0.01
0
0.01
0.02
0.03
mean of forcing
0
1
2
3
4
0
0.2
0.4
0.6
0.8
1
mean of response
0
1
2
3
4
0.95
1
1.05
t
variance of forcing
0
1
2
3
4
0
2
4
6x 10
âˆ’3
t
variance of response
Figure 8.1.1: The mean and variance of the response for the diï¬€erential equation yâ€² + y = f(t) when
forced by Gaussian random noise. The parameters used are y(0) = 1 and âˆ†Ï„ = 0.01.
As in the previous example, Ïƒ2
X(t) â†’0 as t â†’0 and the variance is initially small.
The steady-state variance now becomes
Ïƒ2
X(t) =
Z âˆ
0
Î¦(Ï‰)
|â„¦(Ï‰)|2 dÏ‰.
(8.1.23)
Finally, for the special case Î¦(Ï‰) = 2D/Ï€, the variance is
Ïƒ2
X(t) =
D
2Î¾Ï‰2
0

1 âˆ’eâˆ’2Î¾Ï‰0t
Ï‰2
1

Ï‰2
1 + Ï‰0Ï‰1Î¾ sin(2Ï‰1t) + 2Î¾2Ï‰2
0 sin2(Ï‰1t)

.
(8.1.24)
These results are identical to those found by Uhlenbeck and Ornstein5 in their study of a
harmonically bound particle in Brownian motion.
Project: Low-Pass Filter with Random Input
Consider the initial-value problem
yâ€² + y = f(t),
y(0) = y0.
It has the solution
y(t) = y0eâˆ’t + eâˆ’t
Z t
0
eÏ„f(Ï„) dÏ„.
This diï¬€erential equation is identical to that governing an RC electrical circuit. This circuit
has the property that it ï¬lters out high-frequency disturbances. Here we explore the case
when f(t) is a random process.
5 Ibid. See their pages 834 and 835.

ItË†oâ€™s Stochastic Calculus
391
âˆ’0.2 âˆ’0.1
0
0.1
0.2
0
5
10
15
 t = 0.1
Estimated PDF
âˆ’0.2 âˆ’0.1
0
0.1
0.2
0
2
4
6
8
 t = 0.5
Estimated PDF
âˆ’0.2 âˆ’0.1
0
0.1
0.2
0
2
4
6
8
 t = 1.5
 y
Estimated PDF
âˆ’0.2 âˆ’0.1
0
0.1
0.2
0
2
4
6
 t = 3
Estimated PDF
y
Figure 8.1.2: The probability density function for the response to the diï¬€erential equation yâ€² + y = f(t)
when f(t) is a Gaussian distribution.
Twenty thousand realizations were used to compute the density
function. Here the parameters used are y(0) = 0 and âˆ†Ï„ = 0.01.
Step 1: Using the MATLAB intrinsic function randn, generate a stationary white noise
excitation of length N. Let deltat denote the time interval âˆ†t between each new forcing so
that n = 1 corresponds to t = 0 and n = N corresponds to the end of the record t = T.
Step 2: Using the Gaussian random forcing that you created in Step 1, develop a MATLAB
code to compute y(t) given y(0) and f(t).
Step 3: Once you have conï¬dence in your code, modify it so that you can generate many
realizations of y(t). Save your solution as a function of t and realization. Use MATLABâ€™s
intrinsic functions mean and var to compute the mean and variance as a function of time.
Figure 8.1.1 shows the results when 2000 realizations were used. For comparison the mean
and variance of the forcing have also been included. Ideally this mean and variance should
be zero and one, respectively. We have also included the exact mean and variance, given
by Equation 8.1.3 and Equation 8.1.14, when we set L = R = 1 and D = âˆ†t/2.
Step 4: Now generalize your MATLAB code so that you can compute the probability density
function of ï¬nding y(t) lying between y and y + dy at various times. Figure 8.1.2 illustrates
four times when y(0) = 0 and âˆ†Ï„ = 0.01.
Step 5: Modify your MATLAB code so that you can compute the autocovariance. See Figure
8.1.3.
Project: First-Passage Problem with Random Vibrations 6
In the design of devices, it is often important to know the chance that the device will
exceed its design criteria. In this project you will examine how often the amplitude of a
6 Based on a paper by Crandall, S. H., K. L. Chandiramani, and R. G. Cook, 1966: Some ï¬rst-passage
problems in random vibration. J. Appl. Mech., 33, 532â€“538.

392
Advanced Engineering Mathematics: A Second Course
0
1
2
3
0
1
2
3
0
2
4
6
x 10
âˆ’3
 t1
 t2
autocovariance
Figure 8.1.3: The autocovariance function for the diï¬€erential equation yâ€² + y = f(t) when f(t) is a
Gaussian distribution. Twenty thousand realizations were used. The parameters used here are y(0) = 0
and âˆ†Ï„ = 0.01.
simple, slightly damped harmonic oscillator
yâ€²â€² + 2Î¶Ï‰0yâ€² + Ï‰2
0y = f(t),
0 < Î¶ â‰ª1,
(8.1.25)
will exceed a certain magnitude when forced by white noise. In the physical world this
transcending of a barrier or passage level leads to â€œbottomingâ€ or â€œshort circuiting.â€
Step 1: Using the MATLAB command randn, generate a stationary white noise excitation
of length N. Let deltat denote the time interval âˆ†t between each new forcing so that n =
1 corresponds to t = 0 and n = N corresponds to the end of the record t = T.
Step 2: The exact solution to Equation 8.1.25 is
y(t) = y(0)eâˆ’Î¶Ï‰0t
"
cos
p
1 âˆ’Î¶2 Ï‰0t

+
Î¶
p
1 âˆ’Î¶2 sin
p
1 âˆ’Î¶2 Ï‰0t
#
+
yâ€²(0)
Ï‰0
p
1 âˆ’Î¶2 eâˆ’Î¶Ï‰0t sin
p
1 âˆ’Î¶2 Ï‰0t

(8.1.26)
+
Z t
0
eâˆ’Î¶Ï‰0(tâˆ’Ï„) sin
hp
1 âˆ’Î¶2 Ï‰0(t âˆ’Ï„)
i
p
1 âˆ’Î¶2
f(Ï„)
Ï‰2
0
d(Ï‰0Ï„)
= y(0)eâˆ’Î¶Ï‰0t
"
cos
p
1 âˆ’Î¶2 Ï‰0t

+
Î¶
p
1 âˆ’Î¶2 sin
p
1 âˆ’Î¶2 Ï‰0t
#
+
yâ€²(0)
Ï‰0
p
1 âˆ’Î¶2 eâˆ’Î¶Ï‰0t sin
p
1 âˆ’Î¶2 Ï‰0t

(8.1.27)
+ eâˆ’Î¶Ï‰0t sin
p
1 âˆ’Î¶2 Ï‰0t

p
1 âˆ’Î¶2
Z t
0
eÎ¶Ï‰0Ï„ cos
p
1 âˆ’Î¶2 Ï‰0Ï„
 f(Ï„)
Ï‰2
0
d(Ï‰0Ï„)

ItË†oâ€™s Stochastic Calculus
393
0
20
40
60
80
100
âˆ’4
âˆ’2
0
2
4
forcing
0
20
40
60
80
100
âˆ’2
âˆ’1
0
1
2
Ï‰0t
y(t)
Figure 8.1.4: A realization of the random function y(t) governed by Equation (1) when forced by the
Gaussian random forcing shown in the top frame. The parameters used here are y(0) = 1, yâ€²(0) = 0.5,
Î¶ = 0.1, and Ï‰0âˆ†Ï„ = 0.02.
âˆ’eâˆ’Î¶Ï‰0t cos
p
1 âˆ’Î¶2 Ï‰0t

p
1 âˆ’Î¶2
Z t
0
eÎ¶Ï‰0Ï„ sin
p
1 âˆ’Î¶2 Ï‰0Ï„
 f(Ï„)
Ï‰2
0
d(Ï‰0Ï„).
Because you will be computing numerous realizations of y(t) for diï¬€erent f(t)â€™s, an eï¬ƒcient
method for evaluating the integrals must be employed. Equation 8.1.27 is more eï¬ƒcient
than Equation 8.1.26.
Using the Gaussian random forcing that you created in Step 1, develop a MATLAB code
to compute y(t) given y(0), yâ€²(0), Î¶ and f(t). Figure 8.1.4 illustrates a realization where
the trapezoidal rule was used to evaluate the integrals in Equation 8.1.27.
Step 3: Now that you can compute y(t) or y(n) for a given Gaussian random forcing,
generalize your code so that you can compute irun realizations and store them in y(n,m)
where m = 1:irun. For a speciï¬c n or Ï‰0t, you can use MATLABâ€™s commands mean and var
to compute the mean ÂµX(t) and the variance Ïƒ2
X(t). Figure 8.1.5 shows the results when
1000 realizations were used. For comparison the mean and variance of the forcing have also
been included. Ideally this mean and variance should be zero and one, respectively. The
crosses give the exact results that
ÂµX(t) = y(0)eâˆ’Î¶Ï‰0t
"
cos
p
1 âˆ’Î¶2 Ï‰0t

+
Î¶
p
1 âˆ’Î¶2 sin
p
1 âˆ’Î¶2 Ï‰0t
#
+
yâ€²(0)
Ï‰0
p
1 âˆ’Î¶2 eâˆ’Î¶Ï‰0t sin
p
1 âˆ’Î¶2 Ï‰0t

and Equation 8.1.24 when D = Ï‰0âˆ†t/2.
Step 4: Finally, generalize your MATLAB code so that you store the time T(m) that the
solution y(n) exceeds a certain amplitude b > 0 for the ï¬rst time during the realization m.

394
Advanced Engineering Mathematics: A Second Course
0
10
20
30
40
50
âˆ’0.15
âˆ’0.1
âˆ’0.05
0
0.05
0.1
0.15
mean of forcing
0
10
20
30
40
50
âˆ’1
âˆ’0.5
0
0.5
1
1.5
mean of response
0
10
20
30
40
50
0.8
0.9
1
1.1
1.2
Ï‰0t
variance of forcing
0
10
20
30
40
50
0
0.02
0.04
0.06
0.08
0.1
0.12
Ï‰0t
variance of response
Figure 8.1.5: The mean ÂµX(t) and variance Ïƒ2
X(t) of a slightly damped simple harmonic oscillator when
forced by the Gaussian random noise. The parameters used here are y(0) = 1, yâ€²(0) = 0.5, Î¶ = 0.1, and
Ï‰0âˆ†Ï„ = 0.02.
Of course, you can do this for several diï¬€erent bâ€™s during a particular realization. Once you
have this data you can estimate the probability density function using histc. Figure 8.1.6
illustrates four probability density functions for b = 0.4, b = 0.8, b = 1.2, and b = 1.6.
Project: Wave Motion Generated by Random Forcing 7
In the previous projects we examined ordinary diï¬€erential equations that we forced
with a random process. Here we wish to extend our investigation to the one-dimensional
wave equation
âˆ‚2u
âˆ‚t2 âˆ’âˆ‚2u
âˆ‚x2 = cos(Ï‰t)Î´[x âˆ’X(t)],
subject to the boundary conditions
lim
|x|â†’âˆu(x, t) â†’0,
0 < t,
and initial conditions
u(x, 0) = ut(x, 0) = 0,
âˆ’âˆ< x < âˆ.
Here Ï‰ is a constant and X(t) is a stochastic process.
In Example 5.4.4 we show that the solution to this problem is
u(x, t) = 1
2
Z t
0
H[t âˆ’Ï„ âˆ’|X(Ï„) âˆ’x|] cos(Ï‰Ï„) dÏ„.
7 Based on a paper by Knowles, J. K., 1968: Propagation of one-dimensional waves from a source in
random motion. J. Acoust. Soc. Am., 43, 948â€“957.

ItË†oâ€™s Stochastic Calculus
395
0
10
20
30
40
50
0
0.02
0.04
0.06
0.08
0.1
 b = 0.4
Estimated PDF
0  
50
100
150
200
0
0.005
0.01
0.015
0.02
0.025
 b = 0.8
Estimated PDF
0   
250
500 
750
1000
0
1
2
3
4
5x 10
âˆ’3
 b = 1.2
Ï‰0T
Estimated PDF
0   
500
1000
1500
2000
0
2
4
6
8x 10
âˆ’4
 b = 1.6
Ï‰0T
Estimated PDF
Figure 8.1.6: The probability density function that a slightly damped oscillator exceeds b at the time
Ï‰0T. Fifty thousand realizations were used to compute the density function. The parameters used here are
y(0) = 0, yâ€²(0) = 0, Î¶ = 0.05, and Ï‰0âˆ†Ï„ = 0.05. The mean value of Ï‰0T is 10.7 when b = 0.4, 41.93 when
b = 0.8, 188.19 when b = 1.2, and 1406.8 when b = 1.6.
When the stochastic forcing is absent X(t) = 0, we can evaluate the integral and ï¬nd that
u(x, t) = 1
2Ï‰ H(t âˆ’|x|) sin[Ï‰(t âˆ’|x|)].
Step 1: Invoking the MATLAB command randn, use this Gaussian distribution to numeri-
cally generate an excitation X(t).
Step 2: Using the Gaussian distribution from Step 1, develop a MATLAB code to com-
pute u(x, t). Figure 8.1.7 illustrates one realization where the trapezoidal rule was used to
evaluate the integral.
Step 3: Now that you can compute u(x, t) for a particular random forcing, generalize your
code so that you can compute irun realizations. Then, for particular values of x and t, you
can compute the corresponding mean and variance from the irun realizations. Figure 8.1.8
shows the results when 10,000 realizations were used.
Step 4: Redo your calculations but use a sine wave with random phase: X(t) = A sin(â„¦t+Î¾),
where A and â„¦are constants and Î¾ is a random variable with a uniform distribution on
[0, 2Ï€].
8.2 RANDOM WALK AND BROWNIAN MOTION
In 1827 the Scottish botanist Robert Brown (1773â€“1858) investigated the fertilization
process in a newly discovered species of ï¬‚ower. Brown observed under the microscope that
when the pollen grains from the ï¬‚ower were suspended in water, they performed a â€œrapid

396
Advanced Engineering Mathematics: A Second Course
âˆ’6
âˆ’4
âˆ’2
0
2
4
6
0
1
2
3
4
5
âˆ’0.25
âˆ’0.2
âˆ’0.15
âˆ’0.1
âˆ’0.05
0
0.05
0.1
0.15
0.2
0.25
 t
 x
 u(x,t)
âˆ’6
âˆ’4
âˆ’2
0
2
4
6
0
1
2
3
4
5
âˆ’0.25
âˆ’0.2
âˆ’0.15
âˆ’0.1
âˆ’0.05
0
0.05
0.1
0.15
0.2
0.25
 t
 x
 u(x,t)
(a)
(b)
Figure 8.1.7: The solution (realization) of the wave equation when forced by a Gaussian distribution and
Ï‰ = 2. In frame (a), there is no stochastic forcing X(t) = 0. Frame (b) shows one realization.
âˆ’6
âˆ’4
âˆ’2
0
2
4
60
1
2
3
4
5
âˆ’0.2
âˆ’0.15
âˆ’0.1
âˆ’0.05
0
0.05
0.1
0.15
0.2
 t
 x
 mean
âˆ’6
âˆ’4
âˆ’2
0
2
4
60
1
2
3
4
5
0
0.2
0.4
0.6
0.8
1
x 10
âˆ’3
 t
 x
 variance
Figure 8.1.8: The mean and variance when the wave equation is forced by the stochastic forcing cos(Ï‰t)Î´[x
âˆ’X(t)], where Ï‰ = 2 and X(t) is a Gaussian distribution.
oscillation motion.â€ This motion, now known as Brownian motion, results from the random
kinetic strikes on the pollen grain by water molecules. Brownian motion is an example of
a random process known as random walk. This process has now been discovered in diverse
disciplines, from biology8 to ï¬nance. In this section we examine its nature.
Consider a particle that moves along a straight line in a series of steps of equal length.
Each step is taken, either forwards or backwards, with equal probability 1
2. After taking
N steps, the particle could be at any one (let us denote it m) of the following points:
8 Codling, E. A., M. J. Plank, and S. Benhamou, 2008: Random walk models in biology. J. R. Soc.
Interface, 5, 813â€“834.

ItË†oâ€™s Stochastic Calculus
397
0
10
20
30
40
50
âˆ’10
âˆ’5
0
5
10
15
 N
 m
Figure 8.2.1: Three realizations of a one-dimensional random walk where N = 50.
âˆ’N, âˆ’N + 1, . . . , âˆ’1, 0, 1, . . . , N âˆ’1 and N. Here m is a random variable.
We can generate realizations of one-dimensional Brownian motion using the MATLAB
code:
clear
NN = 50; % select the number of steps for the particle to take
t = (0:1:NN); % create â€˜â€˜timeâ€™â€™ as the particle moves
% create an array to give the position at each time step
m = zeros(size(t));
m(1) = 0; % initialize the position of particle
for N = 1:NN % now move the particle
x = rand(1); % generate a random variable lying between [0, 1]
if (x <= 0.5) step = 1; % if less then 0.5, make it a â€˜â€˜headâ€™â€™
else step = -1; end % otherwise it is a â€˜â€˜tailâ€™â€™
% move the particle one step to the right or left
m(N+1) = m(N) + step;
end
%
plot the results
hold on
plot(t,m,â€™--koâ€™,â€™LineWidthâ€™,2,â€™MarkerSizeâ€™,8)
xlabel(â€™Nâ€™,â€™FontSizeâ€™,25); ylabel(â€™mâ€™,â€™FontSizeâ€™,25)
grid on % add a grid to axes
Figure 8.2.1 illustrates three such realizations.
A natural question would now be: What are the quantitative properties of random
walk? In particular, what is the probability P(m, N) that the particle is at point m after N
displacements? We begin by noting the probability of any given sequence of N steps is
  1
2
N.
The desired probability P(m, N) equals
  1
2
N times the number of distinct sequences of steps

398
Advanced Engineering Mathematics: A Second Course
that will lead to the point m after N steps. To reach m, we must take (N +m)/2 steps in the
positive direction and (N âˆ’m)/2 in the negative direction since (N +m)/2âˆ’(N âˆ’m)/2 = m.
(Note both m and N must be even or odd.) The number of these distinct sequences is
N!
 1
2(N + m)

!
 1
2(N âˆ’m)

!.
(8.2.1)
Therefore,
P(m, N) =
N!
 1
2(N + m)

!
 1
2(N âˆ’m)

!
1
2
N
.
(8.2.2)
Comparing these results with Equation 6.6.14, we see that P(m, N) is simply a binomial
distribution. For this reason, we immediately know E(m) = 0 and Var(m) = N. That
is, the average position is the origin and the spread of the Brownian motion occurs as the
square root of steps taken increases.
The case of greatest interest arises when N is large and m â‰ªN. Then we can approx-
imate P(m, N) by the Poisson distribution,
P(m, N) â‰ˆ
r
2
Ï€N exp

âˆ’m2
2N

.
(8.2.3)
Let us reexpress Equation 8.2.3 in terms of x and t where x = mâˆ†x and t = Nâˆ†t. Using
these deï¬nitions, our equation becomes
P(x, t) =
1
2
âˆš
Ï€Dt
exp

âˆ’x2
4Dt

,
(8.2.4)
if we deï¬ne D = (âˆ†x)2/(2âˆ†t). The attentive student will note that P(x, t) is the Greenâ€™s
function for the heat function, Example 5.5.1.
An alternative approach to this problem would be to compute many random walks and
then calculate the probability density function from these computations. We can construct
a MATLAB code to do this. First we would realize many random walks (here 2000) and
count the number of times that they end at position m:
clear
NN = 100; % set the end point of the random walks
% introduce intermediate positions along the random walk
t = (0:1:NN);
% initialize array â€˜â€˜mâ€™â€™ which gives the position at any time
m = zeros(size(t));
for icount = 1:2000 % now perform many random walks
m(1) = 0; % initial position of particle in each walk
for N = 1:NN
x = rand(1); % create a random variable between [0, 1]
% if â€™â€™xâ€™â€™ less than 0.5, we have a â€™â€™headsâ€™â€™
if (x <= 0.5) step = 1;
else step = -1; end % otherwise we have a â€™â€™tailâ€™â€™

ItË†oâ€™s Stochastic Calculus
399
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
10
20
30
40
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
 m
Estimated  P(m,100)
Figure 8.2.2: Numerical computation of P(m, 100) using 2000 random walks. The black line gives Equation
8.2.3.
m(N+1) = m(N) + step; % now take a step forward or backward
end
% set up array that tracks of the final position of the particle
location(icount) = m(N+1);
end
xx = -40:1:40;
% now count the particles that ended somewhere
%
between -40 and 40
[n,xout] = hist(location,xx)
% for comparison, compute Equation 8.2.3
w exact = sqrt(2/(pi*NN))*exp(-xout.*xout/(2*NN));
n = n / 2000; % now compute the mass probability function
%
plot the results
bar h = bar(xout,n)
bar child = get(bar h,â€™Childrenâ€™)
set(bar child,â€™CDataâ€™,n)
colormap(Autumn)
hold on
plot(xout,w exact,â€™-kâ€™,â€™LineWidthâ€™,3)
xlabel(â€™\it mâ€™,â€™FontSizeâ€™,25)
ylabel(â€™Estimated \it P(m,100)â€™,â€™FontSizeâ€™,25)
Figure 8.2.2 illustrates the results of simulating random walk.
â€¢ Example 8.2.1: On the probability of striking a barrier
An important question in engineering is what is the probability that a given random sys-
tem will exceed its design constraints. Here we ask a similar question about one-dimensional

400
Advanced Engineering Mathematics: A Second Course
t
Î¾
N
m
Figure 8.2.3: Several random walks from the origin to point (Î¾, N). All of these walks would be excluded
from our calculations because they either cross or touch the line m = Î¾ before the ï¬nal step.
Brownian motion: What is the probability that after taking N steps the particle arrives at
Î¾ without ever having touched or crossed the line m = Î¾ at any earlier step? We will do it
exactly and then conï¬rm our results using MATLAB.
The arrival of the particle at Î¾ after N steps implies that its position after N âˆ’1 steps
must have been either Î¾ âˆ’1 or Î¾ + 1. However, a trajectory from (Î¾ + 1, N âˆ’1) to (Î¾, N) is
not allowed because it must have crossed the line m = Î¾ earlier. On the other hand, not all
trajectories arriving at (Î¾, N) from (Î¾ âˆ’1, N âˆ’1) are acceptable because a certain number
will have touched or crossed the line m = Î¾ earlier than its last step. See Figure 8.2.3. Thus
the number of permitted ways of arriving at Î¾ for the ï¬rst time after N steps equals all
possible ways of arriving at Î¾ minus any arrivals from (Î¾ âˆ’1, N âˆ’1) and any arrivals that
crossed or touched the line m = Î¾ earlier than the N âˆ’1.
From our previous work, the number of possible ways from the origin to (Î¾, N) is
N!
 1
2(N + Î¾)

!
 1
2(N âˆ’Î¾)

!.
(8.2.5)
The number of possible ways from the origin to (Î¾ + 1, N âˆ’1) is
(N âˆ’1)!
 1
2(N + Î¾)

!
 1
2(N âˆ’Î¾ âˆ’2)

!.
(8.2.6)
Finally, the number of trajectories arriving at (Î¾ âˆ’1, N âˆ’1) but having an earlier contact
with, or a crossing of, the line m = Î¾ is also
(N âˆ’1)!
 1
2(N + Î¾)

!
 1
2(N âˆ’Î¾ âˆ’2)

!,
(8.2.7)
since it equals the number of trajectories that arrive at (Î¾ + 1, N âˆ’1). From Figure 8.2.3
we see that, due to symmetry, the trajectory that leads to (Î¾ + 1, N âˆ’1) also leads to

ItË†oâ€™s Stochastic Calculus
401
One of the great mathematicians of the twentieth century, Norbert Wiener (1894â€“1964) graduated
from high school at the age of 11 and Tufts at 14. Obtaining a doctorate in mathematical logic
at 18, he repeatedly traveled to Europe for further education. His work extends over an extremely
wide range from stochastic processes to harmonic analysis to cybernetics. (Photo courtesy of the
MIT Museum with permission.)
(Î¾ âˆ’1, N âˆ’1). Consequently the number of trajectories from the origin to (Î¾, N) that have
never touched or crossed m = Î¾ is
N!
 1
2(N + Î¾)

!
 1
2(N âˆ’Î¾)

! âˆ’2
(N âˆ’1)!
 1
2(N + Î¾)

!
 1
2(N âˆ’Î¾ âˆ’2)

!,
(8.2.8)
or
Î¾
N
N!
 1
2(N + Î¾)

!
 1
2(N âˆ’Î¾)

!.
(8.2.9)
The probability P(Î¾, N) that we are seeking is
P(Î¾, N) = Î¾
N
N!
 1
2(N + Î¾)

!
 1
2(N âˆ’Î¾)

!
1
2
N
.
(8.2.10)
For large N, P(Î¾, N) is approximately given by
P(Î¾, N) = Î¾
N
r
2
Ï€N exp

âˆ’Î¾2
2N

.
(8.2.11)
We can also compute this probability using the MATLAB code given above. In this
code we replace the counting process location(icount) = m(N+1); by
b = sort(m);
if ( (m(NN+1) == b(NN+1)) & (b(NN+1)>b(NN)) )
jcount = jcount + 1;
location(jcount) = m(NN+1);
end
where we initialize jcount = 0 at the beginning. The idea behind this code is as follows:
For each of the icount trajectories, we use the MATLAB routine sort to arrange them from

402
Advanced Engineering Mathematics: A Second Course
âˆ’5
0
5
10
15
20
25
30
35
40
45
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5x 10
âˆ’3
Î¾
Estimated  P(Î¾,100)
Figure 8.2.4: The probability P(Î¾, N) that a particle will reach the point m = Î¾ without the particle ever
crossing or touching the line m = Î¾ earlier than N = 100. The solid line is the theoretical probability given
by Equation 8.2.11. Here 50, 000 random walks were taken.
the left-most to the right-most position. To be included in the count of particles reaching
m = Î¾ at step N, the last position of the particle must be (Î¾, N) and it may never have
reached or crossed m = Î¾.
The if condition ensures that both conditions are met.
If
they are, that particular walk is accepted. Once again, the various right-most positions are
binned and the probability is computed. Figure 8.2.4 illustrates this process using 5000
random walks and this result is compared with the probability given by Equation 8.2.11. âŠ“âŠ”
â€¢ Example 8.2.2: Wiener process
Consider the time interval (0, t] and let us subdivide it into subintervals of length âˆ†t
so that there are t/âˆ†t subintervals. Suppose now that a particle, initially at x = 0, takes a
step (in one space dimension) at the times âˆ†t, 2âˆ†t, . . . and that the size of the step is either
âˆ†x or âˆ’âˆ†x, with a probability of 1
2 that the step is to the left or right. The position of
the particle X(t) at time t is a random walk, which has executed t/âˆ†t steps. Because the
position depends on the choice of âˆ†t and âˆ†x, X(t) depends upon t, âˆ†t and âˆ†x.
Mathematically we can describe this random process by
X(t) =
t/âˆ†t
X
n=1
Zi,
(8.2.12)
where the Ziâ€™s are independent and identically distributed with
P(Zi = âˆ†x) = P(Zi = âˆ’âˆ†x) = 1
2,
(8.2.13)
and n = 1, 2, . . .. For each Zi,
E(Zi) = 0,
and
Var(Zi) = E(Z2
i ) = (âˆ†x)2.
(8.2.14)
From Equation 8.2.12, we see that
E[X(t)] = 0,
and
Var[X(t)] = E(Z2
i ) =
t
âˆ†tVar(Zi) = t(âˆ†x)2
âˆ†t
.
(8.2.15)

ItË†oâ€™s Stochastic Calculus
403
Presently we have said nothing about the relationship between âˆ†t and âˆ†x except
that both are small. However, we cannot have just any relationship between them because
the variance would be either zero or inï¬nite. The only reasonable choice is âˆ†x =
âˆš
âˆ†t,
which makes Var[X(t)] = t for all values of âˆ†t. In the limit âˆ†t â†’0 the random variable
X(t) converges into a random variable, hereafter denoted by B(t), with the properties that
E[B(t)] = 0 and Var[B(t)] = t.
The collection of random variables {B(t), t > 0} is a
continuous process in time and called a Wiener process.
âŠ“âŠ”
Our previous example shows that Brownian motion and the Wiener process are very
closely linked. Because Brownian motion occurs in so many physical and biological pro-
cesses, we shall focus on that motion (and the corresponding Wiener process) exclusively
from now on.
We deï¬ne the standard Brownian motion (or Wiener process) B(t) as a
stochastic process that has the following properties:
1. It starts at zero: B(0) = 0.
2. Noting that B(t)âˆ’B(s) âˆ¼N(0, tâˆ’s), E{[B(t)âˆ’B(s)]2} = tâˆ’s and Var{[B(t)âˆ’
B(s)]2} = 2(t âˆ’s)2.
Replacing t with t + dt and s with t, we ï¬nd that
E{[dB(t)]2} = dt.
3. It has stationary and independent increments.
Stationary increments means
that B(t + h) âˆ’B(Î· + h) = B(t) âˆ’B(Î·) for all h. An independent increment
means B(t2) âˆ’B(t1), . . . , B(tn) âˆ’B(tnâˆ’1) are independent random variables.
4. Because increments of Brownian motion on adjacent intervals are independent
regardless of the length of the interval, the derivative will oscillate wildly as
âˆ†x â†’0 and never converge. Consequently, Brownian motion is nowhere diï¬€er-
entiable.
5. It has continuous sample paths, i.e., â€œno jumps.â€
6. The expectation values for the moments are given by
E[B2n(t)] = (2n)!tn
n!2n ,
and
E[B2nâˆ’1(t)] = 0,
(8.2.16)
where n > 0. See Problem 1 at the end of Section 8.4.
Problems
1. Show that E{sin[aB(t)]} = 0, where a is a real.
2. Show that
E{cos[aB(t)]} =
âˆ
X
n=0
(âˆ’1)n
2nn! (a2t)n,
where a is a real.

404
Advanced Engineering Mathematics: A Second Course
3. Show that E{exp[aB(t)]} = exp(a2t/2), where a is a real.
Project: Probabilistic Solutions of Laplaceâ€™s Equation
Laplaceâ€™s equation can be solved using ï¬nite diï¬€erence or ï¬nite element methods, re-
spectively.
During the 1940s, the apparently unrelated ï¬elds of random processes and
potential theory were shown to be in some sense mathematically equivalent.9 As a result, it
is possible to use Brownian motion to solve Laplaceâ€™s equation, as you will discover in this
project. The present numerical method is useful for the following reasons: (1) the entire
region need not be solved in order to determine potentials at relatively few points, (2) com-
putation time is not lengthened by complex geometries, and (3) a probabilistic potential
theory computation is more topologically eï¬ƒcient than matrix manipulations for problems
in two and three spatial dimensions.
To understand this technique,10 consider the following potential problem:
âˆ‚2u
âˆ‚x2 + âˆ‚2u
âˆ‚y2 = 0,
0 < x < 1,
0 < y < 1,
(8.2.17)
subject to the boundary conditions
u(x, 0) = 0,
u(x, 1) = x,
0 < x < 1,
(8.2.18)
and
u(0, y) = u(1, y) = 0,
0 < y < 1.
(8.2.19)
If we introduce a uniform grid with âˆ†x = âˆ†y = âˆ†s, then the ï¬nite diï¬€erence method yields
the diï¬€erence equation:
4u(i, j) = u(i + 1, j) + u(i âˆ’i, j) + u(i, j + 1) + u(i, j âˆ’1),
(8.2.20)
with i, j = 1, N âˆ’1 and âˆ†s = 1/N.
Consider now a random process of the Markov type in which a large number N1 of non-
interacting particles are released at some point (x1, y1) and subsequently perform Brownian
motion in steps of length âˆ†s each unit of time. At some later time, when a few arrive at
point (x, y), we deï¬ne a probability P(i, j) of any of them reaching the boundary y = 1
with potential uk at any subsequent time in the future. Whenever one of these particles
does (later) arrive on y = 1, it is counted and removed from the system. Because P(i, j) is
deï¬ned over an inï¬nite time interval of the diï¬€usion process, the probability of any parti-
cles leaving (x, y) and arriving along some other boundary (where the potential equals 0)
at some future time is 1 âˆ’P(i, j). Whenever a particle arrives along these boundaries it is
also removed from the square.
Having deï¬ned P(i, j) for an arbitrary (x, y), we now compute it in terms of the proba-
bilities of the neighboring points. Because the process is Markovian, where a particle jumps
from a point to a neighbor with no memory of the past,
P(i, j) = p(i + 1, j|i, j)P(i + 1, j) + p(i âˆ’1, j|i, j)P(i âˆ’1, j)
+ p(i, j + 1|i, j)P(i, j + 1) + p(i, j âˆ’1|i, j)P(i, j âˆ’1),
(8.2.21)
9 See Hersh, R., and R. J. Griego, 1969: Brownian motion and potential theory. Sci. Amer., 220,
67â€“74.
10 For the general case, see Bevensee, R. M., 1973: Probabilistic potential theory applied to electrical
engineering problems. Proc. IEEE, 61, 423â€“437.

ItË†oâ€™s Stochastic Calculus
405
0
2
4
6
8
10
0
1
2
3
4
5
6
7
8
9
10
 i
 j
Figure 8.2.5: Four Brownian motions within a square domain with âˆ†x = âˆ†y. All of the random walks
begin at grid point i = 4 and j = 6.
where p(i + 1, j|i, j) is the conditional probability of jumping to (x + âˆ†s, y), given that the
particle is at (x, y). Equation 8.2.21 evaluates P(i, j) as the sum of the probabilities of
reaching y = 1 at some future time by various routes through the four neighboring points
around (x, y). The sum of all the pâ€™s is exactly one because a particle at (x, y) must jump
to a neighboring point during the next time interval.
Let us now compare Equation 8.2.20 and Equation 8.2.21. The potential u(i, j) in
Equation 8.2.20 and P(i, j) becomes an identity if we take the conditional probabilities as
p(i + 1, j|i, j) = p(i âˆ’1, j|i, j) = p(i, j + 1|i, j) = p(i, j âˆ’1|i, j) = 1
4,
and if we also force u(i, N) = P(i, N) = i, u(i, 0) = P(i, 0) = 0, u(0, j) = P(0, j) = 0,
and u(N, j) = P(N, j) = 0. Both the potential u and the probability P become continuous
functions in the space as âˆ†s â†’0, and both are well behaved as (x, y) approaches a boundary
point. A particle starting along y = 1, where the potential is uk, has a probability uk of
arriving there; a particle starting on the remaining boundaries, where the potential is zero,
is immediately removed with no chance of arriving along y = 1. From these considerations,
we have
u(i, j) â‰¡P(i, j) = lim
Nâ†’âˆ
1
N
X
k
Nkuk,
where N is the number of particles starting at (x, y) and Nk equals the number of particles
that eventuallyâ€”after inï¬nite timeâ€”arrive along the entire boundary at potential uk. This
sum includes the boundary y = 1 and (trivially) the remaining boundaries.
Step 1: Develop a MATLAB code to perform two-dimensional Brownian motion. Let U
be a uniformly distributed random variable lying between 0 and 1. You can use rand. If
0 < U â‰¤1
4, take one step to the right; 1
4 < U â‰¤1
2, take one step to the left; if 1
2 < U â‰¤3
4,
take one step downward; and if 3
4 < U â‰¤1, take one step upward. For the arbitrary point
i,j located on a grid of N Ã—N points with 2 â‰¤i, j â‰¤N âˆ’1, repeatedly take a random step

406
Advanced Engineering Mathematics: A Second Course
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
 x
 50 realizations 
 y
 u(x,y)
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
 x
 200 realizations 
 y
 u(x,y)
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
 x
 800 realizations 
 y
 u(x,y)
0
0.5
1
0
0.5
1
0
0.2
0.4
0.6
0.8
1
 x
 3200 realizations 
 y
 u(x,y)
Figure 8.2.6: Solution to Equation 8.2.17 through Equation 8.2.19 using the probabilistic solution method.
until you reach one of the boundaries. Record the value of the potential at the boundary
point. Let us call this result u k(1). Figure 8.2.5 illustrates four of these two-dimensional
Brownian motions.
Step 2: Once you have conï¬dence in your two-dimensional Brownian motion code, generalize
it to solve Equation 8.2.17 through Equation 8.2.19 using runs realizations at some interior
grid point. Then the solution u(i,j) is given by
u(i, j) =
1
runs
runs
X
n=1
u k(n).
Step 3: Finally, plot your results. Figure 8.2.6 illustrates the potential ï¬eld for diï¬€erent
values of runs. What are the possible sources of error in using this method?
8.3 ITË†Oâ€™S STOCHASTIC INTEGRAL
In the previous section we noted that Brownian motion (the Wiener process) is nowhere
diï¬€erentiable. An obvious question is what is meant by the integral of a stochastic variable.
Consider the interval [a, b], which we subdivide so that a = t0 < t1 < t2 < Â· Â· Â· < tn = b.
The earliest and simplest deï¬nition of the integral is
Z b
a
f(t) dt = lim
âˆ†tâ†’0
n
X
i=1
f(Ï„i) âˆ†ti,
(8.3.1)

ItË†oâ€™s Stochastic Calculus
407
where tiâˆ’1 â‰¤Ï„i â‰¤ti and âˆ†ti = ti âˆ’tiâˆ’1. In the case of the classic integral, the integration
is with regards to the increment dt.
ItË†oâ€™s integral is an integral where the inï¬nitesimal increment involves Brownian motion
dB(t), which is a random variable. Before we can deï¬ne this integral, we must introduce
two important concepts. The ï¬rst one is nonanticipating processes: A process F(t) is a
nonanticipating process if F(t) is independent of any future increment B(s) âˆ’B(t) for any
s and t where s > t. Nonanticipating processes are important because ItË†oâ€™s integral applies
only to them.
The second important concept is convergence in the mean square sense. It is deï¬ned
by
lim
nâ†’âˆE
ï£±
ï£²
ï£³
"
Sn âˆ’
Z b
a
F(t) dB(t)
#2ï£¼
ï£½
ï£¾= 0,
(8.3.2)
where Sn is the partial sum
Sn =
n
X
i=1
F(tiâˆ’1) [B(ti) âˆ’B(tiâˆ’1)] .
(8.3.3)
We are now ready to deï¬ne the ItË†o integral: It is the limit of the partial sum Sn:
msâˆ’lim
nâ†’âˆ
Sn =
Z b
a
F(t) dB(t),
(8.3.4)
where we denoted the limit in the mean square sense by msâˆ’lim. Combining Equation 8.3.3
and Equation 8.3.4 together, we ï¬nd that
Z b
a
f[t, B(t)] dB(t) = lim
âˆ†tâ†’0
n
X
i=1
f[tiâˆ’1, B(tiâˆ’1)] [B(ti) âˆ’B(tiâˆ’1)] ,
(8.3.5)
where ti = iâˆ†t and âˆ†t = (b âˆ’a)/N. As one might suspect,
Z b
a
dB(t) = B(b) âˆ’B(a).
(8.3.6)
Because F(t) and dB(t) are random variables, so is ItË†oâ€™s integral.
The results from Equation 8.3.6 would be misunderstood if we think about them as we
do in conventional calculus. We cannot evaluate the right side of Equation 8.3.6 by looking
up B(t) in some book entitled â€œTables of Brownian Motion.â€ This equation only holds true
for a particular realization (sample path).
â€¢ Example 8.3.1
Let us use the deï¬nition of the ItË†o integral to evaluate ItË†o integral
R t
0 B(x) dB(x). In
the present case,
Sn =
n
X
i=1
B(xiâˆ’1) [B(xi) âˆ’B(xiâˆ’1)] ,
(8.3.7)

408
Advanced Engineering Mathematics: A Second Course
where xi = it/n. Because 2a(b âˆ’a) = b2 âˆ’a2 âˆ’(b âˆ’a)2,
Sn = 1
2
n
X
i=1
B2(xi) âˆ’1
2
n
X
i=1
B2(xiâˆ’1) âˆ’1
2
n
X
i=1
[B(xi) âˆ’B(xiâˆ’1)]2
(8.3.8)
= 1
2B2(t) âˆ’1
2
n
X
i=1
[B(xi) âˆ’B(xiâˆ’1)]2 .
(8.3.9)
Therefore,
msâˆ’lim
nâ†’âˆ
Sn = 1
2B2(t) âˆ’1
2 msâˆ’lim
nâ†’âˆ
n
X
i=1
[B(xi) âˆ’B(xiâˆ’1)]2
(8.3.10)
= 1
2B2(t) âˆ’t
2.
(8.3.11)
As a consequence,
Z t
0
B(Î·) dB(Î·) = 1
2B2(t) âˆ’t
2,
(8.3.12)
or
Z b
a
B(t) dB(t) = 1
2[B2(b) âˆ’B2(a)] âˆ’b âˆ’a
2
.
(8.3.13)
Consider now the derivative of B2(t),
d[B2(t)] = [B(t + dt) âˆ’B(t)]2 = 2B(t) dB(t) + dB(t) dB(t).
(8.3.14)
In order for Equation 8.3.12 and Equation 8.3.14 to be consistent, we arrive at the very
important result that
[dB(t)]2 = dt
(8.3.15)
in the mean square sense. We will repeatedly use this result in the remaining portions of
the chapter.
âŠ“âŠ”
Because the ItË†o integral is a random variable, two important quantities are its mean
and variance. Let us turn ï¬rst to the computation of the expectation of
R b
a f[t, B(t)] dB(t).
From Equation 8.3.5 we ï¬nd that
E
(Z b
a
f[t, B(t)] dB(t)
)
= lim
âˆ†tâ†’0 E
( n
X
i=1
f[tiâˆ’1, B(tiâˆ’1)]âˆ†Bi
)
(8.3.16)
= lim
âˆ†tâ†’0
n
X
i=1
E{f[tiâˆ’1, B(tiâˆ’1)]}E[âˆ†Bi] = 0.
(8.3.17)
Therefore
E
(Z b
a
f[t, B(t)] dB(t)
)
= 0.
(8.3.18)

ItË†oâ€™s Stochastic Calculus
409
To compute the variance, we begin by noting that
(Z b
a
f[t, B(t)] dB(t)
)2
= lim
âˆ†tâ†’0
( n
X
i=1
f[tiâˆ’1, B(tiâˆ’1)]âˆ†Bi
)2
(8.3.19)
= lim
âˆ†tâ†’0
n
X
i=1
f 2[tiâˆ’1, B(tiâˆ’1)](âˆ†Bi)2
(8.3.20)
+ 2
n
X
i=1
n
X
j=1
iÌ¸=j
f[tiâˆ’1, B(tiâˆ’1)]âˆ†Bi f[tjâˆ’1, B(tjâˆ’1)]âˆ†Bj.
Taking the expectation of both sides of Equation 8.3.20, we have that
E
Z b
a
f[t, B(t)] dB(t)
2
= lim
âˆ†tâ†’0
n
X
i=1
E{f 2[tiâˆ’1, B(tiâˆ’1)]}E[(âˆ†Bi)2]
(8.3.21)
+ 2 lim
âˆ†tâ†’0
n
X
i=1
n
X
j=1
iÌ¸=j
E{f(tiâˆ’1, B(tiâˆ’1)]}E[âˆ†Bi] E{f(tjâˆ’1, B(tjâˆ’1)]}E[âˆ†Bj]
= lim
âˆ†tâ†’0
n
X
i=1
E{f 2[tiâˆ’1, B(tiâˆ’1)]}(ti+1 âˆ’ti).
(8.3.22)
The double summation vanishes because of the independence of Brownian motion. There-
fore, the ï¬nal result is
E
(Z b
a
f[t, B(t)] dB(t)
)2
=
Z b
a
E{f 2[t, B(t)]} dt.
(8.3.23)
âŠ“âŠ”
â€¢ Example 8.3.2
Consider the random number X =
R b
a
âˆš
t sin[B(t)] dB(t). Let us ï¬nd E(X) and E(X2).
From Equation 8.3.18, we have that E(X) = 0. For that reason, var(X) = E(X2) and
Var(X) = E(X2) =
Z b
a
E
nâˆš
t sin[B(t)]
2o
dt =
Z b
a
t E

sin2[B(t)]
	
dt
(8.3.24)
=
Z b
a
(t/2)E{1 âˆ’cos[2B(t)]} dt =
Z b
a
t
2
"
1 âˆ’
âˆ
X
n=0
(âˆ’1)n22ntn
2nn!
#
dt
(8.3.25)
= âˆ’1
2
Z b
a
âˆ
X
n=1
(âˆ’1)n2n
n!
tn+1 dt = 1
2
âˆ
X
n=1
(âˆ’1)n+12n
(n + 2)n!
 bn+2 âˆ’an+2
.
(8.3.26)
The value of E{cos[2B(t)]} follows from Problem 2 at the end of the last section.
âŠ“âŠ”

410
Advanced Engineering Mathematics: A Second Course
Table 8.3.1 gives a list of ItË†o stochastic integrals. Most of these results were not derived
from the deï¬nition of the ItË†o stochastic integral but from ItË†o lemma, to which we now turn.
Problems
Consider the random variable X =
R b
a f[t, B(t)] dB. Find E(X) and Var(X) for the follow-
ing f[t, B(t)]:
1. f[t, B(t)] = t
2. f[t, B(t)] = tB(t)
3. f[t, B(t)] = |B(t)|
4. f[f, B(t)] =
âˆš
t exp[B(t)]
5. If X =
R b
a f(t){sin[B(t)] + cos[B(t)]} dB(t), show that var(X) =
R b
a f 2(t) dt, if f(t) is a
real function.
Project: Numerical Integration of ItË†oâ€™s Integral
Equation 8.3.5 is useful for numerically integrating the ItË†o integral
Z t
0
f[x, B(x)] dB(x).
Write a MATLAB script to check Example 8.3.1 for various values of n when t = 1. How
does the error vary with n?
Project: Numerical Check of Equations 8.3.18 and 8.3.23
Using the script from the previous project, develop MATLAB code to compute Equation
8.3.18 and Equation 8.3.23.
Using a million realizations (sample paths), compare your
numerical results with the exact answer when a = 1, b = 1, and f[t, B(t)] =
âˆš
t sin[B(t)].
8.4 ITË†Oâ€™S LEMMA
Before we can solve stochastic diï¬€erential equations, we must derive a key result in
stochastic calculus: ItË†oâ€™s formula or lemma. This is stochastic calculusâ€™s version of the
chain rule.
Consider a function f(t) that is twice diï¬€erentiable. Using Taylorâ€™s expansion,
df(B) = f(B + dB) âˆ’f(B) = f â€²(B) dB + 1
2f â€²â€²(B) (dB)2 + Â· Â· Â· ,
(8.4.1)
where B(t) denotes Brownian motion. Integrating Equation 8.4.1 from s to t, we ï¬nd that
Z t
s
df(B) = f[B(t)] âˆ’f[B(s)] =
Z t
s
f â€²(B) dB + 1
2
Z t
s
f â€²â€²(B) dx + Â· Â· Â· ,
(8.4.2)
because [dB(x)]2 = dx. The ï¬rst integral on the right side of Equation 8.4.2 is an ItË†oâ€™s
stochastic integral while the second one can be interpreted as the Riemann integral of f â€²â€²(B).
Therefore, ItË†oâ€™s lemma or formula is
f[B(t)] âˆ’f[B(s)] =
Z t
s
f â€²(B) dB + 1
2
Z t
s
f â€²â€²(B) dx
(8.4.3)

ItË†oâ€™s Stochastic Calculus
411
Table 8.3.1: A Table of ItË†o Stochastic Integrals with t > 0 and b > a > 0
1.
Z b
a
dB(t) = B(b) âˆ’B(a)
2.
Z t
0
B(Î·) dB(Î·) = 1
2[B2(t) âˆ’t]
3.
Z t
0
[B2(Î·) âˆ’Î·] dB(Î·) = 1
3B2(t) âˆ’tB(t)
4.
Z t
0
Î· dB(Î·) = tB(t) âˆ’
Z t
0
B(Î·) dÎ·
5.
Z t
0
B2(Î·) dB(Î·) = 1
3B3(t) âˆ’
Z t
0
B(Î·) dÎ·
6.
Z t
0
eÎ»2Î·/2 cos[Î»B(Î·)] dB(Î·) = 1
Î»eÎ»2t/2 sin[Î»B(t)]
7.
Z t
0
eÎ»2Î·/2 sin[Î»B(Î·)] dB(Î·) = 1
Î»
n
1 âˆ’eÎ»2t/2 cos[Î»B(t)]
o
8.
Z t
0
exp

âˆ’1
2Î»2Î· Â± Î»B(Î·)

dB(Î·) = Â± 1
Î»

exp

âˆ’1
2Î»2t Â± Î»B(t)

âˆ’1
	
9.
Z b
a
B(Î·) exp
B2(Î·)
2Î·
 dB(Î·)
Î·3/2
= bâˆ’1/2 exp
B2(b)
2b

âˆ’aâˆ’1/2 exp
B2(a)
2a

10.
Z b
a
f(Î·) dB(Î·) = f(t)B(t)

b
a
âˆ’
Z b
a
f â€²(Î·)B(Î·) dÎ·
11.
Z b
a
gâ€²[B(Î·)] dB(Î·) = g[B(t)]

b
a
âˆ’1
2
Z b
a
gâ€²â€²[B(Î·)] dÎ·

412
Advanced Engineering Mathematics: A Second Course
for t > s.
â€¢ Example 8.4.1
Consider the case when f(t) = t2 and s = 0. Then, ItË†oâ€™s formula yields
B2(t) âˆ’B2(0) = 2
Z t
0
B(x) dB(x) âˆ’
Z t
0
dx.
(8.4.4)
Evaluating the second integral and noting that B(0) = 0, we again obtain Equation 8.3.12,
that
Z t
0
B(x) dB(x) = 1
2[B2(t) âˆ’t].
(8.4.5)
âŠ“âŠ”
â€¢ Example 8.4.2
Consider the case when f(t) = eat and s = 0. Then, ItË†oâ€™s formula yields
eaB(t) âˆ’1 = a
Z t
0
eaB(x) dB(x) + a2
2
Z t
0
eaB(x) dx.
(8.4.6)
Computing the expectation of both sides,
E
h
eaB(t)i
âˆ’1 = a2
2
Z t
0
E
h
eaB(x)i
dx.
(8.4.7)
Solving this integral equation, we ï¬nd that E[eaB(t)] = ea2t/2, a result that we found earlier
in Problem 2, Section 8.2.
âŠ“âŠ”
â€¢ Example 8.4.3
If f(t) = sin(Î»t), Î» > 0, then ItË†oâ€™s formula gives
sin[Î»B(t)] = Î»
Z t
0
cos[Î»B(Î·)] dB(Î·) âˆ’1
2Î»2
Z t
0
sin[Î»B(Î·)] dÎ·.
(8.4.8)
Taking the expectation of both sides of Equation 8.4.8, we ï¬nd that
E{sin[Î»B(t)]} = âˆ’1
2Î»2
Z t
0
E{sin[Î»B(Î·)]} dÎ·.
(8.4.9)
Setting g(t) = E{sin[Î»B(t)]}, then
g(t) = âˆ’1
2Î»2
Z t
0
g(Î·) dÎ·.
(8.4.10)
The solution to this integral equation is g(t) = 0. Therefore, E{sin[Î»B(t)]} = 0.
âŠ“âŠ”

ItË†oâ€™s Stochastic Calculus
413
Educated at the Imperial University of Tokyo, Kiyoshi ItË†o (1915â€“2008) applied the techniques of
diï¬€erential and integral to stochastic processes. Much of ItË†oâ€™s original work from 1938 to 1945 was
done while he worked for the Japanese National Statistical Bureau. After receiving his doctorate,
ItË†o became a professor at the University of Kyoto from 1952 to 1979. (Author: Konrad Jacobs,
Source: Archives of the Mathematisches Forschungsinstitut Oberwolfach.)
The second version of ItË†oâ€™s lemma begins with the second-order Taylor expansion of
the function f(t, x):
f[t + dt, B(t + dt)] âˆ’f[t, B(t)] = ft[t, B(t)] dt + fx[t, B(t)] dB(t)
+ 1
2

ftt[t, B(t)] (dt)2 + fxt[t, B(t)] dt dB(t)
(8.4.11)
+ fxx[t, B(t)] [dB(t)]2	
+ Â· Â· Â· .
Here we assume that f[t, B(t)] has continuous partial derivatives of at least second order.
Neglecting higher-order terms in Equation 8.4.11, which include the terms with factors such
as (dt)2 and dt dB(t) but not [dB(t)]2 because [dB(t)]2 = dt, our second version of ItË†oâ€™s
lemma is
f[t, B(t)] âˆ’f[s, B(s)] =
Z t
s

ft[Î·, B(Î·)] + 1
2fxx[Î·, B(Î·)]
	
dÎ· +
Z t
s
fx[Î·, B(Î·)] dB(Î·)
(8.4.12)
if t > s.

414
Advanced Engineering Mathematics: A Second Course
â€¢ Example 8.4.4
Consider the function f(t, x) = exâˆ’t/2. Then,
ft(t, x) = âˆ’1
2exâˆ’t/2,
fx(t, x) = exâˆ’t/2,
and
fxx(t, x) = exâˆ’t/2.
(8.4.13)
Therefore, from ItË†oâ€™s lemma, we have that
eB(t)âˆ’t/2 âˆ’eB(s)âˆ’s/2 =
Z t
s
eâˆ’Î·/2eB(Î·) dB(Î·).
(8.4.14)
âŠ“âŠ”
â€¢ Example 8.4.5: Integration by parts
Consider the case when F(t, x) = f(t)g(x). The ItË†o formula gives
d[f(t)g(x)] =

f â€²(t)g[B(t)] + 1
2f(t)gâ€²â€²[B(t)]
	
dt + f(t)gâ€²[B(t)] dB(t).
(8.4.15)
Integrating both sides of Equation 8.4.15, we ï¬nd that
Z b
a
f(t)gâ€²[B(t)] dB(t) = f(t)g[B(t)]

b
a
âˆ’
Z b
a
f â€²(t)g[B(t)] dt âˆ’1
2
Z b
a
f(t)gâ€²â€²[B(t)] dt,
(8.4.16)
which is the stochastic version of integration by parts.
For example, let us choose f(t) = eÎ±t and g(x) = sin(x). Equation 8.4.16 yields
Z t
0
eÎ±Î· cos[B(Î·)] dB(Î·) = eÎ±Î· sin[B(Î·)]

t
0
âˆ’Î±
Z t
0
eÎ±Î· sin[B(Î·)] dÎ· + 1
2
Z t
0
eÎ±Î· sin[B(Î·)] dÎ·
(8.4.17)
= eÎ±t sin[B(t)] âˆ’
 Î± âˆ’1
2
 Z t
0
eÎ±Î· sin[B(Î·)] dÎ·.
(8.4.18)
In the special case of Î± = 1
2, Equation 8.4.18 simpliï¬es to
Z t
0
eÎ±Î· cos[B(Î·)] dB(Î·) = et/2 sin[B(t)].
(8.4.19)
âŠ“âŠ”
An important extension of ItË†oâ€™s lemma involves the function f[t, X(t)] where X(t) is no
longer simply Brownian motion but is given by the ï¬rst-order stochastic diï¬€erential equation
dX(t) = cX(t) dt + ÏƒX(t) dB(t),
(8.4.20)
where c and Ïƒ are real. The second-order Taylor expansion of the function f[t, X(t)] becomes
f[t + dt,X(t + dt)] âˆ’f[t, X(t)] = ft[t, X(t)] dt + fx[t, X(t)] dX(t)
(8.4.21)
+ 1
2

ftt[t, X(t)] (dt)2 + fxt[t, X(t)] dt dX(t) + fxx[t, X(t)] [dX(t)]2	
+ Â· Â· Â· .

ItË†oâ€™s Stochastic Calculus
415
Next, we substitute for dX(t) using Equation 8.4.20, neglect terms involving (dt)2 and
dt dB(t), and substitute [dB(t)]2 = dt. Consequently,
df = f[t + dt, X(t + dt)] âˆ’f[t, X(t)]
(8.4.22)
= ÏƒX(t)fx[t, X(t)] dB(t) +

ft[t, X(t)] + cX(t)fx[t, X(t)] + 1
2Ïƒ2X2(t)fxx[t, X(t)]

dt.
(8.4.23)
The present extension of ItË†oâ€™s lemma reads
f[t, X(t)] âˆ’f[s, X(s)] =
Z t
s

ft[Î·, X(Î·)] + cX(Î·)fx[Î·, X(Î·)] + 1
2Ïƒ2X2(Î·)fxx[Î·, X(Î·)]

dÎ·
+
Z t
s
ÏƒX(Î·)fx[Î·, X(Î·)] dB(Î·)
(8.4.24)
=
Z t
s

ft[Î·, X(Î·)] + 1
2Ïƒ2X2(Î·)fxx[Î·, X(Î·)]

dÎ·
+
Z t
s
fx[Î·, X(Î·)] dX(Î·),
(8.4.25)
where
dX(Î·) = cX(Î·) dÎ· + ÏƒX(Î·) dB(Î·)
(8.4.26)
and t > s.
We can ï¬nally generalize ItË†oâ€™s formula to the case of several ItË†o processes with respect
to the same Brownian motion. For example, let X(t) and Y (t) denote two ItË†o processes
governed by
dX(t) = A(1,1)(t) dt + A(2,1)(t) dB(t),
(8.4.27)
and
dY (t) = A(1,2)(t) dt + A(2,2)(t) dB(t).
(8.4.28)
For stochastic process f[t, X(t), Y (t)], the Taylor expansion is
df[t, X(t),Y (t)] = ft[t, X(t), Y (t)] dt + fx[t, X(t), Y (t)] dX(t)
+ fy[t, X(t), Y (t)] dY (t)
(8.4.29)
+ 1
2fxx[t, X(t), Y (t)]A(2,1)(t)A(2,1)(t) dt + 1
2fxy[t, X(t), Y (t)]A(2,1)(t)A(2,2)(t) dt
+ 1
2fyx]t, X(t), Y (t)]A(2,2)(t)A(2,1)(t) dt + 1
2fyy[t, X(t), Y (t)]A(2,2)(t)A(2,2)(t) dt.
â€¢ Example 8.4.6: Product rule
Consider the special case f(t, x, y) = xy. Then ft = 0, fx = y, fy = x, fxx = fyy = 0,
and fxy = fyx = 1. In this case, Equation 8.4.29 simpliï¬es to
d[X(t)Y (t)] = Y (t) dX(t) + X(t) dY (t) + A(2,1)[t, X(t), Y (t)]A(2,2)[t, X(t), Y (t)] dt.
(8.4.30)
A very important case occurs when A(2,1)[t, X(t), Y (t)] = 0 and X(t) = g(t) is purely
deterministic. In this case,
d[g(t)Y (t)] = Y (t) dg(t) + g(t) dY (t).
(8.4.31)

416
Advanced Engineering Mathematics: A Second Course
This is exactly the product rule from calculus.
Problems
1. (a) Use Equation 8.4.3 and f(t) = tn to show that
Bn(t) = n
Z t
0
Bnâˆ’1(x) dB(x) + n(n âˆ’1)
2
Z t
0
Bnâˆ’2(x) dx.
(b) Show that
E[Bn(t)] = n(n âˆ’1)
2
Z t
0
E[Bnâˆ’2(x)] dx.
(c) Because E[B(t)] = 0 and E[B2(t)] = t, show that
E

B2k+1(t)

= 0,
and
E

B2k(t)

= (2k)!
2kk! tk.
2. Let f(t, x) = x2t and use ItË†oâ€™s formula to show that
Z t
0
B2(Î·) dt + 2
Z t
0
Î·B(Î·) dB(Î·) = tB2(t) âˆ’t2/2.
3. Let f(t, x) = x3/2 and use ItË†oâ€™s formula to show that
Z t
0
B1/2(Î·) dB(Î·) = 2
3B3/2(t) âˆ’1
4
Z t
0
Bâˆ’1/2(Î·) dt.
4. Let f(t, x) = x3/3 âˆ’tx and use ItË†oâ€™s formula to show that
Z t
0
[B2(Î·) âˆ’Î·] dB(Î·) = 1
3B3(t) âˆ’t B(t).
5. If f(x) is any continuously diï¬€erentiable function, use Equation 8.4.29 to show that
Z t
0
f(Î·) dB(Î·) = f(t)B(t) âˆ’
Z t
0
f â€²(Î·)B(Î·) dÎ·.
6. If f(t) = et, use the previous problem to show that
Z t
0
eÎ· dB(Î·) = etB(t) âˆ’
Z t
0
eÎ·B(Î·) dÎ·.
7. Let G(x) denote the antiderivative of g(x). Use Equation 8.4.3 to show that
Z b
a
g[B(t)] dB(t) = G[B(t)]

b
a
âˆ’1
2
Z b
a
gâ€²[B(t)] dt.

ItË†oâ€™s Stochastic Calculus
417
8. (a) If g(x) = xex, use Problem 7 to show that
Z t
0
B(Î·)eB(Î·) dB(Î·) = [B(t) âˆ’1]eB(t) + 1 âˆ’1
2
Z t
0
[B(Î·) + 1]eB(Î·) dÎ·.
(b) Use Equation 8.3.18 to show that
E
h
B(t)eB(t)i
= E
h
eB(t)i
âˆ’1 + 1
2
Z t
0
n
E
h
B(Î·)eB(Î·)i
+ E
h
eB(Î·)io
dÎ·
= et/2 âˆ’1 + 1
2
Z t
0
n
eÎ·/2 + E
h
B(Î·)eB(Î·)io
dÎ·.
(c) Setting g(t) = E

B(t)eB(t)
, use Laplace transforms to show that
E
h
B(t)eB(t)i
= tet/2.
9. (a) If g(x) = 1/(1 + x2), use Problem 7 to show that
Z t
0
dB(Î·)
1 + B2(Î·) = arctan[B(t)] +
Z t
0
B(Î·)
[1 + B2(Î·)]2 dÎ·.
(b) Use Equation 8.3.18 to show that
Z t
0
E

B(Î·)
[1 + B2(Î·)]2

dÎ· = âˆ’E{arctan[B(t)]} .
(c) Because
âˆ’3
âˆš
3
16
â‰¤
x
(1 + x2)2 â‰¤3
âˆš
3
16 ,
or
âˆ’3
âˆš
3
16 t â‰¤
Z t
0
B(Î·)
[1 + B2(Î·)]2 dÎ· â‰¤3
âˆš
3
16 t,
show that
âˆ’3
âˆš
3
16 t â‰¤E{arctan[B(t)]} â‰¤3
âˆš
3
16 t.
10. If g(x) = x/(1 + x2), use Problem 7 to show that
Z t
0
B(Î·)
1 + B2(Î·) dB(Î·) = 1
2 log[1 + B2(t)] âˆ’1
2
Z t
0
1 âˆ’B2(Î·)
[1 + B2(Î·)]2 dÎ·.
11. Use integration by parts with f(t) = eÎ²t and g(x) = âˆ’cos(x) to show that
Z t
0
eÎ²Î· sin[B(Î·)] dB(Î·) = 1 âˆ’eÎ²t cos[B(t)] +
 Î² âˆ’1
2
 Z t
0
eÎ²Î· cos[B(Î·)] dÎ·.

418
Advanced Engineering Mathematics: A Second Course
Then, take Î² = 1
2 and show that
Z t
0
eÎ·/2 sin[B(Î·)] dB(Î·) = 1 âˆ’et/2 cos[B(t)].
12. Redo Example 8.4.3 and show that E{cos[Î»B(t)]} = eâˆ’Î»2t/2, Î» > 0.
13. Use trigonometric double angle formulas to show that
(a)
E{sin[t + Î»B(t)]} = eâˆ’Î»2t/2 sin(t),
and
(b)
E{cos[t + Î»B(t)]} = eâˆ’Î»2t/2 cos(t),
when Î» > 0.
14. Following Example 8.4.4 with f(t, x) = Â±Î» exp
 Â±Î»x âˆ’Î»2t/2

, Î» > 0, show that
Z t
0
exp

Â±Î»B(Î·) âˆ’Î»2Î·
2

dB(Î·) = Â± 1
Î»

exp

Â±Î»B(t) âˆ’Î»2t
2

âˆ’1

.
15. Following Example 8.4.4 with f(t, x) = exp(Î»2t/2) sin(Î»x), Î» > 0, show that
Z t
0
exp
Î»2Î·
2

cos[Î»B(Î·)] dB(Î·) = 1
Î» exp
Î»2t
2

sin[Î»B(t)].
16. Following Example 8.4.4 with f(t, x) = âˆ’exp(Î»2t/2) cos(Î»x), Î» > 0, show that
Z t
0
exp
Î»2Î·
2

sin[Î»B(Î·)] dB(Î·) = 1
Î»

1 âˆ’exp
Î»2t
2

cos[Î»B(t)]

.
17. Following Example 8.4.4 with f(t, x) = tâˆ’1/2 exp[x2/(2t)], show that
Z b
a
B(t) exp
B2(t)
2t
 dB(t)
t3/2
= bâˆ’1/2 exp
B2(b)
2b

âˆ’aâˆ’1/2 exp
B2(a)
2a

.
18. The average of geometric Brownian motion on [0, t] is deï¬ned by
G(t) = 1
t
Z t
0
eB(Î·) dÎ·.
Use the product rule to ï¬nd dG(t). Hint: Take the time derivative of tG(t) =
R t
0 eB(Î·) dÎ·.

ItË†oâ€™s Stochastic Calculus
419
8.5 STOCHASTIC DIFFERENTIAL EQUATIONS
We have reached the point where we can examine stochastic diï¬€erential equations. Of
all the possible stochastic diï¬€erential equations, we will focus on Langevinâ€™s equation11â€”a
model of the velocity of Brownian particles. We will employ this model in a manner similar
to that played by simple harmonic motion in the study of ordinary diï¬€erential equations.
It illustrates many of the aspects of stochastic diï¬€erential equations without being overly
complicated.
â€¢ Example 8.5.1
Before we consider the general stochastic diï¬€erential equation, consider the following
cases where we can make clever use of the product rule. For example, let us solve
dX(t) = [t + B2(t)] dt + 2tB(t) dB(t),
X(0) = X0.
(8.5.1)
In the present case, we can ï¬nd the solution by noting that
dX(t) = B2(t) dt + t[2B(t) dB(t) + dt] = B2(t) dt + t d[B2(t)] = d[tB2(t)].
(8.5.2)
Integrating both sides of Equation 8.5.2, we ï¬nd that the solution to Equation 8.5.1 is
X(t) = tB2(t) + X0.
(8.5.3)
Similarly, let us solve the stochastic diï¬€erential equation
dX(t) = b âˆ’X(t)
1 âˆ’t
dt + dB(t),
0 â‰¤t < 1,
(8.5.4)
with X(0) = X0.
We begin by writing Equation 8.5.4 as
d[b âˆ’X(t)]
1 âˆ’t
+ b âˆ’X(t)
(1 âˆ’t)2 dt = âˆ’dB(t)
1 âˆ’t .
(8.5.5)
Running the product rule backwards,
d
b âˆ’X(t)
1 âˆ’t

= âˆ’dB(t)
1 âˆ’t .
(8.5.6)
Integrating both sides of Equation 8.5.6 from 0 to t, we ï¬nd that
b âˆ’X(t)
1 âˆ’t
= b âˆ’X(0) âˆ’
Z t
0
dB(Î·)
1 âˆ’Î· .
(8.5.7)
Solving for X(t), we obtain the ï¬nal result that
X(t) = b âˆ’[b âˆ’X(0)](1 âˆ’t) + (1 âˆ’t)
Z t
0
dB(Î·)
1 âˆ’Î· .
(8.5.8)
11 Langevin, P., 1908: Sur la thÂ´eorie du mouvement brownien. C. R. Acad. Sci. Paris, 146, 530â€“530.
English translation: Langevin, P., 1997: On the theory of Brownian motion. Am. J. Phys., 65, 1079â€“1081.

420
Advanced Engineering Mathematics: A Second Course
In the present case we cannot simplify the integral in Equation 8.5.8 and must apply nu-
merical quadrature if we wish to have numerical values.
âŠ“âŠ”
In the introduction we showed that the solution to Langevinâ€™s equation:
dX(t) = cX(t) dt + Ïƒ dB(t),
X(0) = X0,
(8.5.9)
is
X(t) = X0 + c
Z t
0
X(Î·) dÎ· + Ïƒ
Z t
0
dB(Î·).
(8.5.10)
An obvious diï¬ƒculty in understanding this solution is the presence of X(s) in the ï¬rst
integral on the right side of Equation 8.5.21.
Let us approach its solution by considering the function f(t, x) = eâˆ’ctx. Then, by ItË†oâ€™s
lemma, Equation 8.4.16,
f[t, X(t)] âˆ’X(0) =
Z t
0

ft[Î·, X(Î·)] + cX(Î·)fx[Î·, X(Î·)] + 1
2Ïƒ2fxx[Î·, X(Î·)]
	
dÎ·
+
Z t
0
Ïƒfx[Î·, X(Î·)] dB(Î·),
(8.5.11)
because f[0, X(0)] = X(0). Direct substitution of f(t, x) into Equation 8.5.11 yields
eâˆ’ctX(t) âˆ’X0 = Ïƒ
Z t
0
eâˆ’cÎ· dB(Î·).
(8.5.12)
Finally, solving for X(t), we obtain
X(t) = X(0)ect + Ïƒect
Z t
0
eâˆ’cÎ· dB(Î·),
(8.5.13)
an explicit expression for X(t). For the special case when X0 is constant, X(t) is known as
an Ornstein-Uhlenbeck process.12
An alternative derivation begins by multiplying Equation 8.5.9 by the integrating factor
eâˆ’ct so that the equation now reads
eâˆ’ct dX(t) âˆ’ceâˆ’ctX(t) dt = Ïƒeâˆ’ct dB(t).
(8.5.14)
Running the product rule, Equation 8.4.23, backwards, we have that
d

eâˆ’ctX(t)

= Ïƒeâˆ’ct dB(t).
(8.5.15)
Integrating both sides of Equation 8.5.15, we obtain Equation 8.5.12.
â€¢ Example 8.5.2: Exact stochastic diï¬€erential equation
Consider the stochastic diï¬€erential equation
X(t) = X(0) + c
Z t
0
X(s) ds + Ïƒ
Z t
0
X(s) dB(s),
(8.5.16)
12 Uhlenbeck and Ornstein, op. cit.

ItË†oâ€™s Stochastic Calculus
421
time
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
X(t)
0.5
1
1.5
2
2.5
3
3.5
Figure 8.5.1: Ten realizations (sample paths) of geometric Brownian motion when c = 0.1, Ïƒ = 0.5, and
X(0) = 1. The heavy line is the mean of X(t).
with c, Ïƒ > 0.
If X(t) = f[t, B(t)], then by ItË†oâ€™s lemma, Equation 8.4.9,
X(t) = X(0) +
Z t
0

ft[s, B(s)] + 1
2fxx[s, B(s)]
	
ds +
Z t
0
fx[s, B(s)] dB(s).
(8.5.17)
Comparing Equation 8.5.16 and Equation 8.5.17, we ï¬nd that
cf(t, x) = ft(t, x) + 1
2fxx(t, x),
(8.5.18)
and
Ïƒf(t, x) = fx(t, x).
(8.5.19)
From Equation 8.5.19,
fxx(t, x) = Ïƒfx(t, x) = Ïƒ2f(t, x).
(8.5.20)
Therefore, Equation 8.5.18 can be replaced by
 c âˆ’1
2Ïƒ2
f(t, x) = ft(t, x).
(8.5.21)
Equation 8.5.19 and Equation 8.5.21 can be solved using separation of variables, which
yields
f(t, x) = f(0, 0) exp
 c âˆ’1
2Ïƒ2
t + Ïƒx

,
(8.5.22)
or
X(t) = f[t, B(t)] = X(0) exp
 c âˆ’1
2Ïƒ2
t + ÏƒB(t)

.
(8.5.23)
Thus, a stochastic diï¬€erential equation can sometimes be solved as the solution of a deter-
ministic partial diï¬€erential equation. In the present case, this solution is called geometric
Brownian motion. For its solution numerically, see Example 8.6.1. See Figure 8.5.1.
âŠ“âŠ”

422
Advanced Engineering Mathematics: A Second Course
â€¢ Example 8.5.3: Homogeneous linear equation
Consider the homogeneous linear stochastic diï¬€erential equation
dX(t) = c1(t)X(t) dt + Ïƒ1(t)X(t) dB(t).
(8.5.24)
Let us introduce f(t, x) = ln(x). Then by ItË†oâ€™s lemma, Equation 8.4.21,
df = d[ln(X)] =

c1(t) âˆ’1
2Ïƒ2
1(t)

dt + Ïƒ1(t) dB(t),
(8.5.25)
because ft = 0, fx = 1/x and fxx = âˆ’1/x2. Integrating both sides of Equation 8.5.25 and
exponentiating the resulting expression, we obtain
X(t) = X(0) exp
Z t
0

c1(Î·) âˆ’1
2Ïƒ2
1(Î·)

dÎ· +
Z t
0
Ïƒ1(Î·) dB(Î·)

.
(8.5.26)
âŠ“âŠ”
â€¢ Example 8.5.4: General case
Consider the homogeneous linear stochastic diï¬€erential equation
dX(t) = [c1(t)X(t) + c2(t)] dt + [Ïƒ1(t)X(t) + Ïƒ2(t)] dB(t).
(8.5.27)
Our analysis begins by considering the homogeneous linear stochastic diï¬€erential equa-
tion
dY (t) = c1(t)Y (t) dt + Ïƒ1(t)Y (t) dB(t),
Y (0) = 1.
(8.5.28)
From the previous example,
Y (t) = exp
Z t
0

c1(Î·) âˆ’1
2Ïƒ2
1(Î·)

dÎ· +
Z t
0
Ïƒ1(Î·) dB(Î·)

.
(8.5.29)
Next, let us introduce two random variables, X1 = 1/Y and X2 = X. Using ItË†o lemma
f(t, x) = 1/x, then
dX1 = df(t, Y ) = d
 1
Y

=

Ïƒ2
1(t) âˆ’c1(t)
 dt
Y âˆ’Ïƒ1(t)dB(t)
Y
(8.5.30)
=

Ïƒ2
1(t) âˆ’c1(t)

X1(t) dt âˆ’Ïƒ1(t)X1(t) dB(t),
(8.5.31)
since ft = 0, fx = âˆ’1/x2 and fxx = 2/x3.
Using Equation 8.4.30, where X1 is governed by Equation 8.5.31 and X2 is governed
by 8.5.27 because X2 = X,
d(X1X2) = [c2(t) âˆ’Ïƒ1(t)Ïƒ2(t)] X1(t) dt + Ïƒ2(t)X1(t) dB(t).
(8.5.32)
Upon integrating both sides of Equation 8.5.32, we have
X1X2 âˆ’X1(0) =
Z t
0
[c2(Î·) âˆ’Ïƒ1(Î·)Ïƒ2(Î·)]
dÎ·
Y (Î·) +
Z t
0
Ïƒ2(Î·) dB(Î·)
Y (Î·) .
(8.5.33)

ItË†oâ€™s Stochastic Calculus
423
Consequently, our ï¬nal result is
X(t) = Y (t)

X(0) +
Z t
0
[c2(Î·) âˆ’Ïƒ1(Î·)Ïƒ2(Î·)]
dÎ·
Y (Î·) +
Z t
0
Ïƒ2(Î·) dB(Î·)
Y (Î·)

,
(8.5.34)
where Y (t) is given by Equation 8.5.29.
âŠ“âŠ”
â€¢ Example 8.5.5: Stochastic Verhulst equation
The stochastic Verhulst equation is
dX(t) = aX(t)[M âˆ’X(t)] dt + bX(t) dB(t),
X(0) = X0.
(8.5.35)
We begin its solution by introducing Î¦(t) = 1/X(t). Then by ItË†oâ€™s lemma, Equation
8.4.21 with f(x) = 1/x,
dÎ¦(t) = âˆ’Î¦(t)[(aM âˆ’b2) dt + b dB(t)] + a dt,
Î¦(0) = 1/X0.
(8.5.36)
To solve Equation 8.5.36, we use the results from Example 8.5.4 with c1(t) = b2 âˆ’aM,
c2(t) = a, Ïƒ1(t) = âˆ’b, and Ïƒ2(t) = 0. Denoting Ç«(t) = (aM âˆ’b2/2)t + bB(t), we can write
Equation 8.5.34 as
Î¦(t)eÎ¾(t) âˆ’Î¦(0) = a
Z t
0
eÎ¾(Î·) dÎ·,
(8.5.37)
or
eÎ¾(t)
X(t) âˆ’1
X0
= a
Z t
0
eÎ¾(Î·) dÎ·.
(8.5.38)
Solving for X(t), we obtain the ï¬nal result that
X(t) =
X0 exp[Î¾(t)]
1 + aX0
R t
0 exp[Î¾(Î·)] dÎ·
.
(8.5.39)
Problems
1. Solve the stochastic diï¬€erential equation
dX(t) = 1
2et/2B(t) dt + et/2 dB(t),
X(0) = X0,
by running the product rule backwards.
2. Solve the stochastic diï¬€erential equation
dX(t) = e2t[1 + 2B2(t)] dt + 2e2tB(t) dB(t),
X(0) = X0,
by running the product rule backwards. Hint: Rewrite the diï¬€erential equation dX(t) =
e2t[2B(t) dB(t) + dt] + (2e2t dt)B2(t).
3. Solve the stochastic diï¬€erential equation
dX(t) = [1 + B(t)] dt + [t + 2B(t)] dB(t),
X(0) = X0,

424
Advanced Engineering Mathematics: A Second Course
by running the product rule backwards. Hint: Rewrite the diï¬€erential equation dX(t) =
2B(t) dB(t) + dt + B(t) dt + t dB(t).
4. Solve the stochastic diï¬€erential equation
dX(t) = [3t2 + B(t)] dt + t dB(t),
X(0) = X0,
by running the product rule backwards. Hint: Rewrite the diï¬€erential equation dX(t) =
3t2 dt + [B(t) dt + t dB(t)].
5. Solve the stochastic diï¬€erential equation
dX(t) = B2(t) dt + 2tB(t) dB(t),
X(0) = X0,
by running the product rule backwards. Hint: Rewrite the diï¬€erential equation dX(t) =
t[2B(t) dB(t) + dt] + B2(t) dt âˆ’t dt.
6. Find the integrating factor and solution to the stochastic diï¬€erential equation
dX(t) = [Î² âˆ’Î±X(t)] dt + Ïƒ dB(t),
X(0) = X0,
where B(t) is Brownian motion and Î±, Î² and Ïƒ are constants.
7. Find the integrating factor and solution to the stochastic diï¬€erential equation
dX(t) = [1 + 2X(t)] dt + e2t dB(t),
X(0) = X0,
where B(t) is Brownian motion.
8. Find the integrating factor and solution to the stochastic diï¬€erential equation
dQ(t) + Q(t)
RC dt = V (t)
R
dt + Î±(t)
R
dB(t),
Q(0) = Q0,
where R and C are real, positive constants, and B(t) is Brownian motion.
9. Find the integrating factor and solution to the stochastic diï¬€erential equation13
dX(t) = 2tX(t) dt + eâˆ’t dt + dB(t),
t âˆˆ[0, 1],
with X(0) = X0, and B(t) is Brownian motion.
10. Find the integration factor and solution to the stochastic diï¬€erential equation
dX(t) = [4X(t) âˆ’1] dt + 2 dB(t),
X(0) = X0,
where B(t) is Brownian motion.
13 Khodabin, M., and M. Rostami, 2015: Mean square numerical solution of stochastic diï¬€erential equa-
tions by fourth order Runge-Kutta method and its applications in the electric circuits with noise. Adv.
Diï¬€. Eq., 2015, 62.

ItË†oâ€™s Stochastic Calculus
425
11. Find the integration factor and solution to the stochastic diï¬€erential equation
dX(t) = [2 âˆ’X(t)] dt + eâˆ’tB(t) dB(t),
X(0) = X0,
where B(t) is Brownian motion.
12. Find the integration factor and solution to the stochastic diï¬€erential equation
dX(t) = [1 + X(t)] dt + etB(t) dB(t),
X(0) = X0,
where B(t) is Brownian motion.
13. Find the integration factor and solution to the stochastic diï¬€erential equation
dX(t) =
 1
2X(t) + 1

dt + et cos[B(t)] dB(t),
X(0) = X0,
where B(t) is Brownian motion.
14. Find the integration factor and solution to the stochastic diï¬€erential equation
dX(t) =

t + 1
2X(t)

dt + et sin[B(t)] dB(t),
X(0) = X0,
where B(t) is Brownian motion.
15. Following Example 8.5.2, solve the exact stochastic diï¬€erential equation:
dX(t) = et 
1 + B2(t)

dt +

1 + 2etB(t)

dB(t),
X(0) = X0.
Step 1: Show that ft + 1
2fxx = et(1 + x2), and fx = 1 + 2etx.
Step 2: Show that f(t, x) = x + etx2 + g(t).
Step 3: Show that g(t) = X0 and X(t) = B(t) + etB2(t) + X0.
16. Following Example 8.5.2, solve the exact stochastic diï¬€erential equation:
dX(t) =

2tB2(t) + 3t2 [1 + B(t)]
	
dt +

1 + 3t2B2(t)

dB,
X(0) = X0.
Step 1: Show that ft + 1
2fxx = 2tx3 + 3t2(1 + x), and fx = 3t2x2 + 1.
Step 2: Show that f(t, x) = t2x3 + x + g(t).
Step 3: Show that gâ€²(t) = 3t2.
Step 4: Show that X(t) = t2[B3(t) + t] + B(t) + X0.
Using Equation 8.5.26, solve the following stochastic diï¬€erential equations:
17. dX(t) = t2X(t) dt + tX(t) dB(t),
X(0) = X0
18. dX(t) = cos(t)X(t) dt + sin(t)X(t) dB(t),
X(0) = X0
19. dX(t) = ln(t + 1)X(t) dt +
p
ln(t + 1) X(t) dB(t),
X(0) = X0

426
Advanced Engineering Mathematics: A Second Course
20. dX(t) = ln(t + 1)X(t) dt + tX(t) dB(t),
X(0) = X0
21. Following Example 8.5.5, solve the stochastic diï¬€erential equation
dX(t) = [aXn(t) + bX(t)] dt + cX(t) dB(t),
X(0) = X0,
where n > 1.
Step 1: Setting Î¦(t) = X1âˆ’n(t), use ItË†oâ€™s lemma Equation 8.4.21 with f(x) = 1/xnâˆ’1 to
show that
dÎ¦(t) = (1 âˆ’n)Î¦(t)
 b âˆ’1
2nc2
dt + c dB(t)

+ (1 âˆ’n)a dt.
Step 2: Setting c1(t) = (1 âˆ’n)b âˆ’n(1 âˆ’n)c2/2, c2(t) = (1 âˆ’n)a, Ïƒ1(t) = (1 âˆ’n)c, and
Ïƒ2(t) = 0, show that
exp[(n âˆ’1)Î¾(t)]
Xnâˆ’1(t)
âˆ’
1
Xnâˆ’1
0
= (1 âˆ’n)a
Z t
0
exp[(n âˆ’1)Î¾(Î·)] dÎ·,
or
exp[(n âˆ’1)Î¾(t)]
Xnâˆ’1(t)
=
1
Xnâˆ’1
0
+ (1 âˆ’n)a
Z t
0
exp[(n âˆ’1)Î¾(Î·)] dÎ·,
where Î¾(t) = (b âˆ’c2/2)t + cB(t).
22. Following Example 8.5.5, solve the stochastic Ginzburg-Landau equation:
dX(t) =
h
aecX(t) + b
i
dt + Ïƒ dB(t),
X(0) = X0.
Step 1: Setting Î¦(t) = exp[âˆ’cX(t)], use ItË†oâ€™s lemma Equation 8.4.21 with f(x) = eâˆ’cx to
show that
dÎ¦(t) = âˆ’
 bc âˆ’1
2Ïƒ2c2
Î¦(t) dt âˆ’ÏƒcÎ¦(t) dB(t) âˆ’ac dt.
Step 2: Setting c1(t) = Ïƒ2c2/2 âˆ’bc, c2(t) = âˆ’ac, Ïƒ1(t) = âˆ’Ïƒc, and Ïƒ2(t) = 0, show that
X(t) = X0 + bt + ÏƒB(t) âˆ’1
c ln

1 âˆ’ac
Z t
0
exp[cX0 + bcÎ¾ + ÏƒcB(Î¾)] dÎ¾

.
23. Following Example 8.5.5, solve the stochastic diï¬€erential equation:
dX(t) =

[1 + X(t)][1 + X2(t)]

dt + [1 + X2(t)] dB(t),
X(0) = X0.
Step 1: Setting Î¦(t) = tanâˆ’1[X(t)], use ItË†oâ€™s lemma Equation 8.4.21 with f(x) = tanâˆ’1(x)
to show that dÎ¦(t) = dt + dB(t).
Step 2: Solving the stochastic diï¬€erential equation in Step 1, show that
X(t) = tan[tanâˆ’1(X0) + t + B(t)].

ItË†oâ€™s Stochastic Calculus
427
8.6 NUMERICAL SOLUTION OF STOCHASTIC DIFFERENTIAL EQUATIONS
In this section we construct numerical schemes for integrating the stochastic diï¬€erential
equation
dX(t) = a[X(t), t] dt + b[X(t), t] dB(t)
(8.6.1)
on t0 â‰¤t â‰¤T with the initial-value X(t0) = X0.
Our derivation begins by introducing the grid t0 < t1 < t2 < Â· Â· Â· < tn < Â· Â· Â· < tN = T.
For simplicity we assume that all of the time increments are the same and equal to 0 <
âˆ†t < 1 although our results can be easily generalized when this is not true. Now
Xn+1 = Xn +
Z tn+1
tn
a[X(Î·), Î·] dÎ· +
Z tn+1
tn
b[X(Î·), Î·] dB(Î·).
(8.6.2)
The crudest approximation to the integrals in Equation 8.6.2 is
Z tn+1
tn
a[X(Î·), Î·] dÎ· â‰ˆa[X(tn), tn]âˆ†tn,
(8.6.3)
and
Z tn+1
tn
b[X(Î·), Î·] dB(Î·) â‰ˆb[X(tn), tn]âˆ†Bn.
(8.6.4)
Substituting these approximations into Equation 8.6.2 yields the Euler-Marugama approx-
imation.14 For the ItË†o process X(t) = {X(t), t0 â‰¤t â‰¤T}:
Xn+1 = Xn + a(tn, Xn) (tn+1 âˆ’tn) + b(tn, Xn)
 Btn+1 âˆ’Btn

(8.6.5)
for n = 0, 1, 2, . . . , N âˆ’1 with the initial value X0.
When b = 0, the stochastic iterative scheme reduces to the conventional Euler scheme
for ordinary diï¬€erential equations. When b Ì¸= 0, we have an extra term generated by the
random increment âˆ†Bn = B(tn+1) âˆ’B(tn) where n = 0, 1, 2, . . . , N âˆ’1 for Brownian
motion (the Wiener process) B(t) = B(t), t â‰¥0. Because these increments are independent
Gaussian random variables, the mean equals E(âˆ†Bn) = 0 while the variance is E[(âˆ†Bn)2] =
âˆ†t. We can generate âˆ†Bn using the MATLAB function randn.
An important consideration in the use of any numerical scheme is the rate of conver-
gence. During the numerical simulation of a realization, at time t there will be a diï¬€erence
between the exact solution X(t) and the numerical approximation Y (t). This diï¬€erence
e(t) = X(t)âˆ’Y (t) will also be a random variable. A stochastic diï¬€erential equation scheme
converges strongly with order m, if for any time t, E(|e(t)|) = O[(âˆ†t)m] for suï¬ƒciently small
time step âˆ†t. The strong order for the Euler-Marugama method can be proven to be 1
2.
To construct a strong order 1 approximation to Equation 8.6.1, we return to Equation
8.6.2. Using Equation 8.4.12, we have
Xn+1 âˆ’Xn =
Z tn+1
tn

a[Xn(Î·), Î·] +
Z Î·
tn
 aax + 1
2b2axx

dÎ¾ +
Z Î·
tn
bax dB(Î¾)

dÎ·
+
Z tn+1
tn

b[Xn(Î·), Î·] +
Z Î·
tn
 abx + 1
2b2bxx

dÎ¾ +
Z Î·
tn
bbx dB(Î¾)

dÎ·
(8.6.6)
= a[X(tn), tn]âˆ†t + b[X(tn), tn)âˆ†Bn + Rn,
(8.6.7)
14 Maruyama, G., 1955: Continuous Markov processes and stochastic equations. Rend. Circ. Math.
Palermo, Ser. 2,, 4, 48â€“90.

428
Advanced Engineering Mathematics: A Second Course
where
Rn =
Z tn+1
tn
Z Î·
tn
bbx dB(Î¾)

dB(Î·) + higher-order terms.
(8.6.8)
Dropping the higher-order terms,
Rn â‰ˆb[X(tn), tn]bx[X(tn), tn]
Z tn+1
tn
Z Î·
tn
dB(Î¾)

dB(Î·).
(8.6.9)
Consider now the double integrals
(âˆ†Bn)2 =
Z tn+1
tn
dB(Î·)
 Z tn+1
tn
dB(Î·)

=
Z tn+1
tn
Z tn+1
tn
dB(Î¾)

dB(Î·).
(8.6.10)
Now,
Z tn+1
tn
Z tn+1
tn
dB(Î¾)

dB(Î·) =
Z tn+1
tn
Z Î·
tn
dB(Î¾)

dB(Î·) +
Z tn+1
tn
Z tn+1
Î·
dB(Î¾)

dB(Î·)
+
Z tn+1
tn
[dB(Î·)]2
(8.6.11)
= 2
Z tn+1
tn
Z Î·
tn
dB(Î¾)

dB(Î·) +
Z tn+1
tn
[dB(Î·)]2
(8.6.12)
= 2
Z tn+1
tn
Z Î·
tn
dB(Î¾)

dB(Î·) + âˆ†t,
(8.6.13)
because
Z tn+1
tn
[dB(Î·)]2 =
Z tn+1
tn
dÎ· = âˆ†t.
(8.6.14)
Combining Equation 8.6.9, Equation 8.6.10, and Equation 8.6.13 yields
Rn â‰ˆb[X(tn), tn]bx[X(tn), tn]

(âˆ†Bn)2 âˆ’âˆ†t

.
(8.6.15)
Finally, substituting Equation 8.6.15 into Equation 8.6.7 gives the ï¬nal result, the Milstein
method:15
Xn+1 = Xn + a(Xn, tn) âˆ†tn + b(Xn, tn) âˆ†Bn + 1
2b(Xn, tn)âˆ‚b(Xn, tn)
âˆ‚x

(âˆ†Bn)2 âˆ’âˆ†t

.
(8.6.16)
â€¢ Example 8.6.1
Consider the ItË†o process X(t) deï¬ned by the linear stochastic diï¬€erential equation
dX(t) = aX(t) dt + bX(t) dB(t),
(8.6.17)
15 Milstein, G., 1974: Approximate integration of stochastic diï¬€erential equations. Theory Prob. Applic.,
19, 557â€“562.

ItË†oâ€™s Stochastic Calculus
429
0
0.25
0.5
0.75
1
X(t n),Yn
0
1
2
3
4
5
6
h = 0.2
0
0.25
0.5
0.75
1
0
5
10
15
20
25
30
h = 0.1
t
0
0.25
0.5
0.75
1
X(t n),Yn
0
2.5
5
7.5
10
12.5
15
h = 0.05
t
0
0.25
0.5
0.75
1
0
1
2
3
4
5
6
h = 0.02
Figure 8.6.1: The numerical solution of the stochastic diï¬€erential equation, Equation 8.6.17, using the
Euler-Marugama (crosses) and the Milstein (circles) methods for various time steps h. The dashed line
gives the exact solution.
for t âˆˆ[0, T]. If this ItË†o process has the drift a(x, t) = ax and the diï¬€usion coeï¬ƒcient
b(x, t) = bx, the exact solution (see Equation 8.5.16) is
X(t) = X0 exp

a âˆ’b2
2

t + bB(t)

(8.6.18)
for t âˆˆ[0, T]. Figure 8.6.1 compares the numerical solution of this stochastic diï¬€erential
equation using the Euler-Marugama and Milstein method against the exact solution. Note
that each frame has a diï¬€erent solution because the Brownian forcing changes with each
realization.
âŠ“âŠ”
Although a plot of various realizations can give an idea of how the stochastic processes
aï¬€ect the solution, two more useful parameters are the sample mean and standard deviation
at time tn:
X(tn) = 1
J
J
X
j=1
Xj(tn),
(8.6.19)
and
Ïƒ2(tn) =
1
J âˆ’1
J
X
j=1

Xj(tn) âˆ’X(tn)
2 ,
(8.6.20)
where J are the number of realizations and Xj(tn) is the value of the random variable at
time tn of the jth realization.
In many physical problems, â€œnoiseâ€ is the origin of the stochastic process and we suspect
that we have a normal distribution N(Âµ, Ïƒ2) where Âµ and Ïƒ are the population mean and
standard deviation, respectively. Then, using the sample statistics, Equations 8.6.20 and

430
Advanced Engineering Mathematics: A Second Course
Rt/L
0
0.5
1
1.5
2
2.5
3
3.5
4
I(t)
-2
-1
0
1
2
3
4
5
Figure 8.6.2: Eleven realizations as a function of the nondimensional time Rt/L of the numerical solution
of Equation 8.6.24 using the Euler-Marugama method when h = 0.02, Î±/L = 1, Î²/L = 0, I0 = 0, and
v(t) = R. The mean and 95% conï¬dence interval (here tstudent = 2.228) are given by the heavy solid and
dashed lines, respectively. Finally, the crosses (+) give the deterministic solution.
8.6.21, a two-sided conï¬dence interval can be determined as

X(tn) âˆ’Ï„student
Ïƒ(tn)
âˆš
J
, X(tn) + Ï„student
Ïƒ(tn)
âˆš
J

based on the student-Ï„ distribution with J âˆ’1 degrees of freedom.
Project: RL Electrical Circuit with Noise
An important component of contemporary modeling is the mixture of deterministic
and stochastic aspects of a physical system. In this project you will see how this is done
using a simple electrical system.16
Consider a simple electrical circuit consisting of a resistor with resistance R and an
inductor with inductance L. If the circuit is driven by a voltage source v(t), the current
I(t) at a given time t is given by the ï¬rst-order ordinary diï¬€erential equation
LdI
dt + RI = v(t),
I(0) = I0.
(8.6.21)
Step 1: Using classical methods, show that the deterministic solution to Equation 8.6.21 is
I(t) = I0eâˆ’Rt/L + 1
L
Z t
0
exp
R
L (Ï„ âˆ’t)

v(Ï„) dÏ„.
(8.6.22)
16 See KolÂ´aË‡rovÂ´a, E., 2005: Modeling RL electrical circuits by stochastic diï¬€erential equations. Proc. Int.
Conf. Computers as Tool, Belgrade (Serbia and Montenegro), IEEE R8, 1236â€“1238.

ItË†oâ€™s Stochastic Calculus
431
time
0
100
200
300
400
500
600
700
800
900
1000
E[x(t)]
-1.5
-1
-0.5
0
0.5
1
Ïƒ = 0
Ïƒ = 0.01
Ïƒ = 0.1
Figure 8.6.3: Plot of E[x(t)] versus time for the FitzHugh-Nagamo model for three values of Ïƒ. The value
of the parameters are a = 0.8, m = 1.2, and Ï„ = 100. The Euler method was used with a time step of 0.1.
There are two possible ways that randomness can enter this problem. First, the power
supply could introduce some randomness so that the right side of Equation 8.6.21 could
read v(t) + Î± dB2(t)/dt.
Second, some physical process within the resistor could cause
randomness so that the resistance would now equal R + Î² dB1(t)/dt. Here B1(t) and B2(t)
denote two independent white noise processes and Î±, Î² are nonnegative constants. In this
case the governing diï¬€erential equation would now read
dI
dt + 1
L

R + Î±dB1
dt

= 1
L

v(t) + Î² dB2
dt

,
I(0) = I0.
(8.6.23)
Converting Equation 8.6.23 into the standard form of a stochastic ordinary diï¬€erential
equation, we have that
dI = 1
L [v(t) âˆ’RI(t)] dt âˆ’Î±
LI(t) dB1(t) + Î²
L dB2(t),
I(0) = I0.
(8.6.24)
Step 2: Using MATLAB, create a script to numerically integrate Equation 8.6.24 for a given
set of Î±, Î², I0 = 0, R, L, and v(t). Plot I(t) as a function of the nondimensional time Rt/L
for many realizations (say 20). See Figure 8.6.2.
Step 3: Although some idea of the eï¬€ect of randomness is achieved by plotting several
realizations, a better way would be to compute the mean and standard deviation at a given
time. On the plot from the previous step, plot the mean and standard deviation of your
solution as a function of nondimensional time. How does it compare to the deterministic
solution?
Project: Relaxation Oscillator with Brownian Motion Forcing
The FitzHugh-Nagamo17 model describes excitable systems such as a neuron. We will
modify it so that the forcing is due to Brown motion. The governing equations are
dx = âˆ’x(x2 âˆ’a2) dt âˆ’y dt + Ïƒ dB1(t),
17 FitzHugh, R., 1961:
Impulses and physiological states in theoretical models of nerve membrane.
Biophys. J., 1, 445-466; Nagumo, J., S. Arimoto, and S. Yoshizawa, 1962: An active pulse transmission
line simulating nerve axon. Proc. IRE, 50, 2061â€“2070.

432
Advanced Engineering Mathematics: A Second Course
time
0
5
10
15
20
25
30
E[x(t)]
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
Euler method
Heun method
leapfrog method
Figure 8.6.4: Plot of E[x(t)] versus time for the damped harmonic oscillator forced by Brownian motion.
The value of the parameters are k = 1, Î³ = 0.25, and alpha = âˆ†t = 0.1. Five thousand realization were
performed.
and
dy = (x âˆ’my) dt/Ï„ + Ïƒ dB2(t),
where a, m, Ïƒ, and Ï„ are parameters.
Write a MATLAB script to numerically integrate this modiï¬ed FitzHugh-Nagamo model
for various values of Ïƒ. Using many simulations, compute E[x(t)] as a function of time t.
See Figure 8.6.3. What is the eï¬€ect of the Brownian motion forcing?
Project: Stochastically Damped Harmonic Oscillator
The damped stochastic harmonic oscillator is governed by the stochastic diï¬€erential
equations:
dv(t) = âˆ’Î³v(t) dt âˆ’k2x(t) dt âˆ’Î±x(t) dB(t),
and
dx(t) = v(t) dt,
where k, Î± and Î³ are real constants. This system of equations is of interest for two reasons:
(1) The system is forced by Brownian motion. (2) The noise is multiplicative rather than
additive because the forcing term is x(t) dB(t) rather than just dB(t).
We could solve both equations numerically using Eulerâ€™s method.18 The purpose of
this project is to introduce you to the Heun method. In the Heun method we ï¬rst compute
an estimate of the solution xâˆ—and vâˆ—by taking a Euler-like time step:
xâˆ—= xi + viâˆ†t,
and
vâˆ—= vi âˆ’Î³viâˆ†t âˆ’k2xiâˆ†t âˆ’Î±xiâˆ†Bi,
where xi and vi denote the displacement and velocity at time ti = iâˆ†t, âˆ†t is the time step,
and i = 0, 1, 2, . . .. With these estimates we compute the value for xi+1 and vi+1 using
xi+1 = xi + 1
2(vi + vâˆ—)âˆ†t,
and
vi+1 = vi âˆ’1
2Î³(vi + vâˆ—)âˆ†t âˆ’1
2k2(xi + xâˆ—)âˆ†t âˆ’Î±xiâˆ†Bi.
18 For further details, see Greiner, A., W. Strittmatter, and J. Honerkamp, 1988: Numerical integration
of stochastic diï¬€erential equations. J. Stat. Phys., 51, 95â€“108.

ItË†oâ€™s Stochastic Calculus
433
h1/2
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
0.2
mean time
0.7
0.8
0.9
1
1.1
1.2
1.3
Figure 8.6.5: The mean time that it takes a particle to travel from X(0) = âˆ’1 to X = 0 in the double-well
potential stated in the text. Sixty thousand realizations were used with a time step h. Two diï¬€erential
numerical schemes were used: the Euler-Marugama (crosses) and the Milstein (circles) methods. The curves
are linear least-squares ï¬ts through the results.
Qiang and Habib19 developed a leapfrog algorithm to solve this problem. Because the
algorithm is rather complicated, the interested student is referred to their paper.
Write a MATLAB script to use the Euler and Huen methods to numerically integrate
the stochastic harmonic oscillator when 10Î± = 4Î³ = k = 1 and x(0) = v(0) = 0. Using
many simulations, compute E[x(t)] as a function of time t. See Figure 8.6.4. What happens
to the accuracy of the solution for larger values of âˆ†t?
Project: Mean First Passage Time
The stochastic diï¬€erential equation
dX(t) = [X(t) âˆ’X3(t)] dt +
4
X2(t) + 1 dB(t)
describes the motion of a particle in a double-well potential V (x) = x4/4 âˆ’x2/2, subject
to a spatially dependent random forcing when the acceleration Xâ€²â€²(t) can be neglected.
An important question is what is the average (mean) time that it takes a particle initially
located at a minimum X(0) = âˆ’1 to reach the local maximum X(t) = 0.
Write MATLAB code that computes X(t) as a function of time t.
Using this code
and creating N realizations, compute the length of time that it takes the particle to reach
X(t) = 0 in each realization. Then compute the mean from those times and plot the results
as a function
âˆš
h, the square root of the time step. See Figure 8.6.5. We used
âˆš
h rather
than h following the suggestions of SeeÃŸelberg and Petruccione.20
19 Qiang, J., and S. Habib, 2000: Second-order stochastic leapfrog algorithm for multiplicative noise
Brownian motion. Phys. Review, 62, 7430â€“7437.
20 SeeÃŸelberg, M., and F. Petruccione, 1993: An improved algorithm for the estimation of the mean ï¬rst
passage of ordinary stochastic diï¬€erential equations. Comput. Phys. Commun., 74, 247â€“255.

434
Advanced Engineering Mathematics: A Second Course
interest (%/yr)
0
1
2
3
4
5
6
7
8
9
10
probability of bankruptcy
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Ïƒ = 1.5
Ïƒ = 2.0
Ïƒ = 2.5
Figure 8.6.6: The probability of bankruptcy over a three-year period as a function of interest rate of a
ï¬rm with initial wealth X0 = 500 and debt D = 100. Other parameters are h = 0.01 yr and Âµ = 1.001/year.
The units on Ïƒ is yearâˆ’1/2. Five hundred thousand realizations were used to compute the probability.
Project: Bankruptcy of a Company
The stochastic diï¬€erential equation21
dX(t) = [ÂµX(t) âˆ’iD] dt + ÏƒX(t) dB(t),
0 < t < T,
with X(0) = X0, describes the evolution with time t of the wealth X(t) of a ï¬rm. Here Âµ
and Ïƒ denote the deterministic and stochastic evolution of the ï¬rmâ€™s wealth, respectively,
X0 is the initial wealth of the ï¬rm, and iD gives the amount of payment to a ï¬nancier (bank)
who initially loaned the ï¬rm the amount D at the interested rate i. Write a MATLAB code
to simulate the wealth of a ï¬rm during its lifetime T given a known D, i and X0 with
Âµ = 1.001/year, and various values of Ïƒ.
During the simulation there is a chance that the ï¬rm goes bankrupt at time t = Ï„ <
T. This occurs when the stochastic process hits the barrier X(Ï„) = 0. If n denotes the
number of times that bankruptcy occurs in N simulations, the probability of bankruptcy
is P[X(Ï„) = 0] = n/N.
Using your code for simulating a ï¬rmâ€™s wealth, compute the
probability of bankruptcy as a function of interest rate for a small (D = 20, X0 = 100),
medium (D = 100, X0 = 500), and large (D = 200, X0 = 1000) ï¬rm. See Figure 8.6.6.
How does the average value of Ï„ vary with interest rate?
Further Readings
Kloeden, P. E., and E. Platen, 1992: Numerical Solution of Stochastic Diï¬€erential Equa-
tions. Springer-Verlag, 632 pp. A solid book covering numerical schemes for solving stochas-
tic diï¬€erential equations.
Mikosch, T., 1998: Elementary Stochastic Calculus with Finance in View. World Scientiï¬c,
212 pp. Very well-crafted book on stochastic calculus.
21 See Cerqueti, R., and A. G. Quaranta, 2012:
The perspective of a bank in granting credit:
An
optimization model. Optim. Lett., 6, 867â€“882.

Answers
to the Odd-Numbered Problems
Section 1.1
1. 1 + 2i
3. âˆ’2/5
5. 2 + 2i
âˆš
3
7. z = e3Ï€i/2
9. z = 4eÏ€i/3
11. z = 2
âˆš
2e7Ï€i/4
Section 1.2
1. Â±
âˆš
2,
Â±
âˆš
2
 
1
2 +
âˆš
3i
2
!
,
Â±
âˆš
2
 
âˆ’1
2 +
âˆš
3i
2
!
3. i,
Â±
âˆš
3
2 âˆ’i
2
5. Â± 1
âˆš
2

âˆ’
qp
a2 + b2 + a + i
qp
a2 + b2 + a

7. Â±(1 + i),
Â±2(1 âˆ’i)
Section 1.3
1. u = 2 âˆ’y, v = x
3. u = x3 âˆ’3xy2, v = 3x2y âˆ’y3
5. f â€²(z) = 3z(1 + z2)1/2
7. f â€²(z) = 2(1 + 4i)z âˆ’3
9. f â€²(z) = âˆ’3i(iz âˆ’1)âˆ’4
11. âˆ’1/4
13. (âˆ’1)n/Ï€
15. v(x, y) = 2xy + constant
17. v(x, y) = x sin(x)eâˆ’y + yeâˆ’y cos(x) + constant
Section 1.4
1. 0
3. 2i
5. 14/15 âˆ’i/3
435

436
Advanced Engineering Mathematics: A Second Course
Section 1.5
1. (eâˆ’2 âˆ’eâˆ’4)/2
3. Ï€/2
5. 17/6 + 52i/3
7. âˆ’sinh(1)i/3
Section 1.6
1. Ï€i/32
3. Ï€i/2
5. âˆ’2Ï€i
7. 2Ï€i
9. âˆ’6Ï€
11. 2Ï€i/3
Section 1.7
1.
âˆ
X
n=0
(n + 1)zn
3. f(z) = z10 âˆ’z9 + z8
2 âˆ’z7
6 + Â· Â· Â· âˆ’
1
11!z + Â· Â· Â·
We have an essential singularity and the residue equals âˆ’1/11!
5. f(z) = 1
2! + z2
4! + z4
6! + Â· Â· Â·
We have a removable singularity where the value of the residue equals zero.
7. f(z) = âˆ’2
z âˆ’2 âˆ’7z
6 âˆ’z2
2 âˆ’Â· Â· Â·
We have a simple pole and the residue equals âˆ’2.
9. f(z) = 1
2
1
z âˆ’2 âˆ’1
4 + z âˆ’2
8
âˆ’Â· Â· Â·
We have a simple pole and the residue equals 1/2.
Section 1.8
1. âˆ’3Ï€i/4
3. âˆ’2Ï€i.
5. 2Ï€i
7. 2Ï€i
9. âˆ’2i
Section 1.11
3. z = CâˆšÏ„ âˆ’Ï€ + Ï€ i
5. z = Ï„ 7/4 or Ï„ = z4/7
7. z = a coshâˆ’1(Ï„)/Ï€,
0 â‰¤â„‘[coshâˆ’1(Ï„)] â‰¤Ï€
Section 2.1
1. f(t) = eâˆ’a|t|/(2a)
3. f(t) = iteâˆ’a|t|/(4a)
5. f(t) = âˆ’2eâˆ’3t/2 sin
 âˆš
3 t/2

H(t)/
âˆš
3
7.
f(t) =
ï£±
ï£²
ï£³
eâˆ’a|t| cosh(
âˆš
a2âˆ’1|t|)
4a
âˆ’
eâˆ’a|t| sinh(
âˆš
a2âˆ’1|t|)
4
âˆš
a2âˆ’1
,
a > 1,
eâˆ’a|t| cos(
âˆš
1âˆ’a2|t|)
4a
âˆ’
eâˆ’a|t| sin(
âˆš
1âˆ’a2|t|)
4
âˆš
1âˆ’a2
,
0 < a < 1.
Section 2.2
1. f(t) = (2 âˆ’t)eâˆ’2t âˆ’2eâˆ’3t
3. f(t) =
 t2/4 âˆ’t/4 + 1/8

e2t âˆ’1/8
5. f(t) =

(t âˆ’1)/2 âˆ’1/4 + eâˆ’2(tâˆ’1)/4

H(t âˆ’1)

Answers to the Odd-Numbered Problems
437
Section 2.3
1. f(t) = 1 + 2t
3. f(t) = t + t2/2
5. f(t) = t3 + t5/20
7. f(t) = t2 âˆ’t4/3
9. f(t) = 5e2t âˆ’4et âˆ’2tet
11. f(t) = (1 âˆ’t)2eâˆ’t
13. f(t) = e2t âˆ’eâˆ’t 
cos
 âˆš
3 t

) +
âˆš
3 sin
 âˆš
3 t

15. f(t) = 4 + 5t2/2 + t4/24
17. x(t) = 2A
âˆš
t/(Ï€C) âˆ’Bt/(2C)
Section 3.1
1. F(z) = 2z/(2z âˆ’1) if |z| > 1/2
3. F(z) = (z6 âˆ’1)/(z6 âˆ’z5) if |z| > 0
5. F(z) = (a2 + a âˆ’z)/[z(z âˆ’a)] if |z| > a.
Section 3.2
1. F(z) = zTeaT /(zeaT âˆ’1)2
3. F(z) = z(z + a)/(z âˆ’a)3
5. F(z) = [z âˆ’cos(1)]/{z[z2 âˆ’2z cos(1) + 1]}
7. F(z) = z[z sin(Î¸) + sin(Ï‰0T âˆ’Î¸)]/[z2 âˆ’2z cos(Ï‰0T) + 1]
9. F(z) = z/(z + 1)
11. fn âˆ—gn = n + 1
13. fn âˆ—gn = 2n/n!
Section 3.3
1. f0 = 0.007143, f1 = 0.08503, f2 = 0.1626, f3 = 0.2328
3. f0 = 0.09836, f1 = 0.3345, f2 = 0.6099, f3 = 0.7935
5. fn = 8 âˆ’8
  1
2
n âˆ’6n
  1
2
n
7. fn = (1 âˆ’Î±n+1)/(1 âˆ’Î±)
9. fn =
  1
2
nâˆ’10 Hnâˆ’10 +
  1
2
nâˆ’11 Hnâˆ’11
11. fn = 1
9(6n âˆ’4)(âˆ’1)n + 4
9
  1
2
n
13. fn = an/n!
Section 3.4
1. yn = 1 + 1
6n(n âˆ’1)(2n âˆ’1)
3. yn = 1
2n(n âˆ’1)
5. yn = 1
6 [5n âˆ’(âˆ’1)n]
7. yn = (2n âˆ’1)
  1
2
n +
 âˆ’1
2
n
9. yn = 2n âˆ’n âˆ’1
11. xn = 2 + (âˆ’1)n; yn = 1 + (âˆ’1)n
13. xn = 1 âˆ’2(âˆ’6)n; yn = âˆ’7(âˆ’6)n
Section 3.5
1. marginally stable
3. unstable
Section 4.1
7. bx(t) = 1
Ï€ ln

t + a
t âˆ’a


438
Advanced Engineering Mathematics: A Second Course
Section 4.2
5. w(t) = u(t) âˆ—v(t) = Ï€eâˆ’1 sin(t)
Section 4.3
1. z(t) = eiÏ‰t
Section 4.4
3. x(t) =
1 âˆ’t2
(1 + t2)2 ;
bx(t) =
2t
(1 + t2)2
Section 5.2
1. G(s) = 1/(s + k)
g(t|0) = eâˆ’kt
g(t|Ï„) = eâˆ’k(tâˆ’Ï„)H(t âˆ’Ï„)
a(t) =
 1 âˆ’eâˆ’kt
/k
3. G(s) = 1/(s2 + 4s + 3)
g(t|0) = 1
2(eâˆ’t âˆ’eâˆ’3t)
g(t|Ï„) = 1
2

eâˆ’(tâˆ’Ï„) âˆ’eâˆ’3(tâˆ’Ï„)
H(t âˆ’Ï„)
a(t) = 1
6eâˆ’3t âˆ’1
2eâˆ’t + 1
3
5. G(s) = 1/[(s âˆ’2)(s âˆ’1)]
g(t|0) = e2t âˆ’et
g(t|Ï„) =

e2(tâˆ’Ï„) âˆ’etâˆ’Ï„
H(t âˆ’Ï„)
a(t) = 1
2 + 1
2e2t âˆ’et
7. G(s) = 1/(s âˆ’9)2
g(t|0) = 1
3 sinh(3t)
g(t|Ï„) = 1
3 sinh[3(t âˆ’Ï„)]H(t âˆ’Ï„)
a(t) = 1
9 [cosh(3t) âˆ’1]
9. G(s) = 1/[s(s âˆ’1)]
g(t|0) = et âˆ’1
g(t|Ï„) = [etâˆ’Ï„ âˆ’1] H(t âˆ’Ï„)
a(t) = et âˆ’t âˆ’1
11.
g(x|Î¾) = (1 + x<)(L âˆ’1 âˆ’x>)
L
,
and
g(x|Î¾) = âˆ’2ex+Î¾
e2L âˆ’1 + 2L3
Ï€2
âˆ
X
n=1
Ï•n(Î¾)Ï•n(x)
n2(n2Ï€2 + L2),
where Ï•n(x) = sin(nÏ€x/L) + nÏ€ cos(nÏ€x/L)/L.
13.
g(x|Î¾) = sinh(kx<) sinh[k(L âˆ’x>)]
k sinh(kL)
,
and
g(x|Î¾) = 2L
âˆ
X
n=1
sin(nÏ€Î¾/L) sin(nÏ€x/L)
n2Ï€2 + k2L2
.
15.
g(x|Î¾) = sinh(kx<){k cosh[k(x> âˆ’L)] âˆ’sinh[k(x> âˆ’L)]}
k sinh(kL) + k2 cosh(kL)
,
and
g(x|Î¾) = 2
âˆ
X
n=1
(1 + k2
n) sin(knÎ¾) sin(knx)
[1 + (1 + k2n)L](k2n + k2) ,

Answers to the Odd-Numbered Problems
439
where kn is the nth root of tan(kL) = âˆ’k.
17.
g(x|Î¾) = [a sinh(kx<) âˆ’k cosh(kx<)] cosh[k(L âˆ’x>)]
k[a cosh(kL) âˆ’k sinh(kL)]
,
and
g(x|Î¾) = 2
âˆ
X
n=1
(a2 + k2
n) cos[kn(Î¾ âˆ’L)] cos[kn(x âˆ’L)]
[(a2 + k2n)L âˆ’a](k2n + k2)
,
where kn is the nth root of k tan(kL) = âˆ’a.
Section 5.4
3.
g(x, t|Î¾, Ï„) = t âˆ’Ï„
L
H(t âˆ’Ï„) + 2
Ï€ H(t âˆ’Ï„)
âˆ
X
n=1
1
n cos
nÏ€Î¾
L

cos
nÏ€x
L

sin
nÏ€(t âˆ’Ï„)
L

5.
u(x, t) = 2
âˆ
X
n=1
sin
nÏ€x
L
 
nÏ€
L2 + n2Ï€2

eâˆ’t âˆ’cos
nÏ€t
L

+
L
L2 + n2Ï€2 sin
nÏ€t
L

+ 2 sin
Ï€x
L

cos
Ï€t
L

+ 4L
Ï€2
âˆ
X
m=1
1
(2m âˆ’1)2 sin
(2m âˆ’1)Ï€x
L

sin
(2m âˆ’1)Ï€t
L

7.
u(x, t) = 1 âˆ’t2
2L âˆ’2L
Ï€2
âˆ
X
n=1
1
n2 cos
nÏ€x
L
 
1 âˆ’cos
nÏ€t
L

Section 5.5
3.
g(x, t|Î¾, Ï„) = 2
L
 âˆ
X
n=1
sin
(2n âˆ’1)Ï€Î¾
2L

sin
(2n âˆ’1)Ï€x
2L

exp

âˆ’(2n âˆ’1)2Ï€2(t âˆ’Ï„)
4L2

Ã— H(t âˆ’Ï„)
5.
u(x, t) = 2Ï€
âˆ
X
n=1
n
n2Ï€2 âˆ’L2 sin
nÏ€x
L
 
eâˆ’t âˆ’exp

âˆ’n2Ï€2t
L2

+ 4
Ï€
âˆ
X
m=1
1
2m âˆ’1 sin
(2m âˆ’1)Ï€x
L

exp

âˆ’(2m âˆ’1)2Ï€2t
L2


440
Advanced Engineering Mathematics: A Second Course
7.
u(x, t) = 1 âˆ’t
L âˆ’2L
Ï€2
âˆ
X
n=1
1
n2 cos
nÏ€x
L
 
1 âˆ’exp

âˆ’n2Ï€2t
L2

Section 5.6
1.
g(x, y|Î¾, Î·) = 1
Ï€
âˆ
X
n=1
1
n exp

âˆ’nÏ€
a |y âˆ’Î·|

sin
nÏ€Î¾
a

sin
nÏ€x
a

5.
g(r, Î¸|Ï, Î¸â€²) = 1
Ï€
âˆ
X
n=1
1
nrnÏ€/Î²
<
râˆ’nÏ€/Î²
>
sin
nÏ€Î¸â€²
Î²

sin
nÏ€Î¸
Î²

7.
g(r, z|Ï, Î¶) =
2
Ï€a2L
âˆ
X
n=1
âˆ
X
m=1
J0(kmÏ/a)J0(kmr/a)
Ï€a2LJ2
1(km)(k2m/a2 + n2Ï€2/L2) sin
nÏ€Î¶
L

sin
nÏ€z
L

Section 6.2
1. (a) S = {HH, HT, TH, TT}
(b) S = {ab, ac, ba, bc, ca, cb}
(c) S = {aa, ab, ac, ba, bb, bc, ca, cb, cc}
(d) S = {bbb, bbg, bgb, bgg, ggb, ggg, gbb, gbg}
(e) S = {bbb, bbg, bgb, bgg, ggb, ggg, gbb, gbg}
3. 1/3
5. 1/3
7. 2/13
9. 1/720, 1/120
11. 1/2
13. 1/2
15. 9/16
Section 6.3
1.
FX(x) =
( 0,
x < 0,
1 âˆ’p,
0 â‰¤x < 1,
1,
1 â‰¤x.
3.
27
Section 6.4
1. FX(x) =

0,
x â‰¤0,
1 âˆ’eâˆ’Î»x,
0 < x.
3. FX(x) =
ï£±
ï£´
ï£²
ï£´
ï£³
0,
x < âˆ’1,
(1 + x)2/2,
âˆ’1 â‰¤x < 0,
1 âˆ’(x âˆ’1)2/2,
0 â‰¤x < 1,
1,
1 â‰¤x.
Section 6.5
1. E(X) = 1
2, and Var(X) = 1
4
3. k = 3/4, E(X) = 1, and Var(X) = 1
5

Answers to the Odd-Numbered Problems
441
5. Ï†X(Ï‰) =
 peiÏ‰ + q
n, ÂµX = np, Var(X) = npq
7. Ï†X(Ï‰) = p/(1 âˆ’qeÏ‰i), ÂµX = q/p, Var(X) = q/p2
Section 6.6
1. (a) 1/16, (b) 1/4, (c) 15/16, (d) 1/16
5. P(X > 0) = 0.01, and P(X > 1) = 9 Ã— 10âˆ’5
7. P(T < 150) = 1
3, and P(X = 3) = 0.1646
Section 6.7
1.
pXY [xi, yj] =

7
xi
 
8
yj
 
5
5 âˆ’xi âˆ’yj


20
5

,
where xi = 0, 1, 2, 3, 4, 5, yj = 0, 1, 2, 3, 4, 5 and 0 â‰¤xi + yj â‰¤5.
Section 7.1
1. ÂµX(t) = 0, and Ïƒ2
X(t) = cos(Ï‰t)
3. For t1 = t2, RX(t1, t2) = p; for t1 Ì¸= t2, RX(t1, t2) = p2. For t1 = t2, CX(t1, t2) = p(1âˆ’p);
for t1 Ì¸= t2, CX(t1, t2) = 0.
Section 7.4
1.
P n =

2/3 + (1/3)(1/4)n
1/3 âˆ’(1/3)(1/4)n
2/3 âˆ’(2/3)(1/4)n
1/3 + (2/3)(1/4)n

.
P âˆ=

2/3
1/3
2/3
1/3

.
Section 8.3
1. E(X) = 0,
Var(X) = E(X2) = (b3 âˆ’a3)/3
3. E(X) = 0,
Var(X) = E(X2) = (b2 âˆ’a2)/2
Section 8.5
1. X(t) = et/2B(t) + X0
3. X(t) = B2(t) + tB(t) + X0
5. X(t) = tB2(t) âˆ’t2/2 + X0
7. X(t) = X(0)e2t + 1
2
 e2t âˆ’1

+ e2tB(t)
9. X(t) = et2X(0) + et2 R t
0 eâˆ’Î·2âˆ’Î· dÎ· + et2 R t
0 eâˆ’Î·2 dB(Î·)
11. X(t) = X0eâˆ’t + 2(1 âˆ’eâˆ’t) + 1
2eâˆ’t[B2(t) âˆ’t]
13. X(t) = X0et/2 + 2
 et/2 âˆ’1

+ et sin[B(t)]
17. X(t) = X0 exp
h
t3
6 +
R t
0 Î· dB(Î·)
i
19. X(t) = X0 exp
h
1
2t ln(t + 1) + 1
2 ln(t + 1) âˆ’1
2t +
R t
0
p
ln(Î· + 1) dB(Î·)
i


Index
absolute value of a complex number, 2
addition of a complex numbers, 1
amplitude of a complex number, 2
analytic complex function, 9
derivative of, 10
analytic signals, 211â€“212
argument of a complex number, 2
band-pass functions, 209
basis function, 285
Bayesâ€™ rule, 302
branches
of a complex function, 9
principal, 3
Bromwich contour, 93
Bromwich integral, 93
Bromwich, Thomas John Iâ€™Anson, 93
Brownian motion, 396â€“403
Buï¬€onâ€™s needle problem, 336â€“337
Cauchy
integral formula, 24â€“26
principal value, 51â€“53
residue theorem, 34â€“36
Cauchy, Augustin-Louis, 11
Cauchy-Goursat theorem, 20
Cauchy-Riemann equations, 11
central limit theorem, 318
Chapman-Kolmogorov equation, 359
characteristic function, 320â€“321
chemical reaction, 373â€“375
closed contour integral, 19
combinations, 301
complex
-valued function, 8â€“10
conjugate, 1
number, 1
plane, 2
variable, 1
compound interest, 186
conformal mapping, 59â€“75
contour integrals, 16â€“20
convolution theorem
for Hilbert transforms, 207â€“209
for z-transforms, 170
curve, simply closed, 20
cutoï¬€frequency, 244
damped harmonic motion, 389
de Moivreâ€™s theorem, 3
deformation principle, 22
diï¬€erence equations, 159
diï¬€erential equations, stochastic, 419â€“423
division of complex numbers, 1
Duhamelâ€™s theorem
for ordinary diï¬€erential equation, 233
entire complex function, 9
essential singularity, 30
443

444
Advanced Engineering Mathematics: A Second Course
Eulerâ€™s formula, 2
evaluation of partial sums
using z-transform, 180
ï¬nal-value theorem
for z-transforms, 168
ï¬nite element, 285
ï¬rst-passage problem, 391â€“393
Fourier transform, 78â€“91, 228â€“232
inverse of, 78â€“87
frequency response, 227
frequency spectrum,
for a damped harmonic
oscillator, 227â€“229
for low-frequency ï¬lter, 229â€“230
function
multiplied complex, 8
single-valued complex, 8
Galerkin method, 285â€“292
gamblerâ€™s ruin problem, 348, 364
Greenâ€™s function, 217â€“233
for a damped harmonic oscillator, 228
for heat equation, 256â€“266
for Helmholtzâ€™s equation, 266â€“285
for low-frequency ï¬lter, 230
for ordinary diï¬€erential eqn, 223â€“243
for wave equation, 247â€“256
harmonic functions, complex, 15
heat dissipation in disc brakes, 132â€“134
heat equation, 129â€“153
Hilbert pair, 196
Hilbert transform, 195â€“215
and convolution, 207â€“208
and derivatives, 206â€“207
and shifting, 206
and time scaling, 206
discrete, 203â€“204
linearity of, 205
product theorem, 208â€“209
Hilbert, David, 197
holomorphic complex function, 9
ideal Hilbert transformer, 195
ideal sampler, 160
imaginary part of a complex number, 1
importance sampling, 324
impulse function
see (Dirac) delta function
impulse response, 224
indicial admittance
for ordinary diï¬€erential eqns, 224â€“225
initial-value theorem
for z-transforms, 168
integral equation, 222â€“223
of convolution type, 100â€“101
integrals
complex contour, 16â€“20
Fourier type, evaluation of, 81
real, evaluation of, 37â€“43
interest rate, 186
inverse
Fourier transform, 78â€“87
Hilbert transform, 196
Laplace transform, 92â€“96
z-transform, 173â€“181
inversion formula
for the Hilbert transform, 196
for the Laplace transform, 92â€“96
for the z-transform, 173â€“181
inversion of Fourier transform
by contour integration, 77â€“87
inversion of Laplace transform
by contour integration, 93â€“96
inversion of z-transform
by contour integration, 177â€“181
by partial fractions, 175â€“177
by power series, 173â€“175
by recursion, 174â€“175
isolated singularities, 12
ItË†o process, 386
ItË†oâ€™s integral, 406â€“411
ItË†oâ€™s lemma, 410â€“418
ItË†o, Kiyhosi, 413
joint transform method, 243
Jordan curve, 20
Jordanâ€™s lemma, 78
Kramers-Kronig relationship, 213â€“215
Lagrangeâ€™s trigonometric identities, 5
Laguerre polynomial, 105
Laplace transform, 92â€“101
in solving
heat equation, 129â€“136
integral equations, 100â€“101
Laplace equation, 154â€“156
wave equation, 105â€“115

Index
445
Laplace transform (contd.)
inverse of, 92â€“96
Schouten-van der Pol theorem for, 99
Laplaceâ€™s equation,
solution by Laplace transforms, 154â€“156
Laurent expansion, 29
law of large numbers, 322
line integral, 16â€“20
linearity
of Hilbert transform, 205
of z-transform, 166
low-frequency ï¬lter, 229â€“230
low-pass ï¬lter, 390â€“392
Markov chain
state, 358
state transition, 358
time homogeneous, 358
martingale, 349
mean, 318â€“230
meromorphic function, 12
method of partial fractions
for Fourier transform, 77
for z-transform, 175â€“177
modulus of a complex number, 2
Monte Carlo integration, 324
multiplication of complex numbers, 1
multivalued complex function, 8
not simply connected, 20
numerical solution
of stochastic diï¬€erential eqn, 427â€“434
order of a pole, 30
Parsevalâ€™s identity
for z-transform, 180
partial fraction expansion
for z-transform, 175â€“177
path in complex integrals, 17
path independence in complex integrals, 22
permutation, 301
phase of the complex number, 2
phasor amplitude, 213
Poisson process, 377â€“382
arrival time, 380
polar form of a complex number, 2
pole of order n, 30
population growth and decay, 366â€“375
positively oriented curve, 23
power spectrum, 354â€“357
principal branch, 3
probability
Bernoulli distribution, 326
Bernoulli trials, 310
binomial distribution, 327
characteristic function, 320
combinations, 301
conditional, 302
continuous joint distribution, 335
correlation, 341
covariance, 339
cumulative distribution, 310
distribution function, 314
event, 296
elementary, 296
simple, 296
expectation, 319
experiment, 295
exponential distribution, 330
Gaussian distribution, 331
geometric distribution, 326
independent events, 303â€“304
joint probability mass function, 333
law of total probability, 303
marginal probability functions, 333
mean, 319
normal distribution, 331
permutation, 301
Poisson distribution, 328
probability integral, 332
probability mass function, 308
random variable, 308
sample point, 295
sample space, 295
standard normal distribution, 332
uniform distribution, 329
variance, 319
quadrature phase shifting, 195
radius of convergence, 27
random diï¬€erential equation, 387â€“388
random process, 345â€“383
autocorrelation function, 351
Bernoulli process, 346
Brownian motion, 395â€“403
chemical kinetics, 370
counting process, 347
mean, 349

446
Advanced Engineering Mathematics: A Second Course
random process (contd.)
power spectrum, 354
realization, 345
sample function, 345
sample path, 345
state, 345
state space, 345
variance, 349
wide-sense stationary process, 352
Wiener process, 402â€“403
random variable, 308
discrete, 308
domain, 308
identically distributed, 308
independent, 308
independent identically distributed, 308
range, 308
real deï¬nite integrals
evaluation of, 37â€“43
real part of a complex number, 1
regular complex function, 9
removable singularity, 30
residue, 29
residue theorem, 33â€“36
Riemann, Georg Friedrich Bernhard, 12
root locus method, 229
roots of a complex number, 5â€“7
Schouten-Van der Pol theorem, 99
Schwarz-Christoï¬€el transformation, 66â€“75
set, 294
complement, 294
disjoint, 294
element, 294
empty, 294
intersection, 294
null, 294
subset, 294
union, 294
universal, 294
simple pole, 30
simply closed curve, 20
single side-band signal, 213
single-valued complex function, 8
singularity
essential, 30
isolated, 30
pole of order n, 30
removable, 30
solution of ordinary diï¬€erential equations
by Fourier transform, 227â€“232
steady-state transfer function, 227
step response, 224â€“225
stochastic calculus, 385â€“431
Brownian motion, 396â€“403
damped harmonic motion, 389
derivative, 385
diï¬€erential equations, 419â€“423
ï¬rst-passage problem, 391â€“393
integrating factor, 420
ItË†o process, 385
ItË†oâ€™s integral, 406â€“411
ItË†oâ€™s lemma, 410â€“418
low-pass ï¬lter, 390â€“392
nonlinear oscillator, 431
numerical solution, 427â€“430
Euler-Marugama method, 427
Milstein method, 428
product rule, 416, 419
random diï¬€erential equations, 387â€“388
RL electrical circuit with noise, 430
wave motion due to random
forcing, 394â€“396
Wiener process, 403
stochastic process, 345
subtraction of complex numbers, 1
superposition integral, 219
for ordinary diï¬€erential equations, 233
Taylor expansion, 27
telegraph equation, 106â€“115
telegraph signal, 346, 353
transfer function, 223
transform
Fourier, 78â€“91, 228â€“232
Hilbert, 195â€“215
Laplace, 92â€“101
z-, 159â€“193
transmission line, 106â€“115
transmission probability matrix, 360
variance, 318â€“320
Venn diagram, 294
Volterra equation of the second kind, 100
wave equation, 100â€“127
wave motion due to random forcing, 394â€“396
Wiener process, 350, 403
Wiener, Norbert, 401

Index
447
z-transform, 159â€“193
basic properties of, 164â€“172
convolution for, 170
ï¬nal-value theorem for, 168â€“169
for solving diï¬€erence equations, 183â€“188
initial-value theorem for, 168
inverse of, 173â€“181
linearity of, 166
multiplication by n, 169
of a sequence multiplied by an
exponential sequence, 166
of a shifted sequence, 166â€“168
of periodic sequences, 169â€“170
their use in determining
stability, 189â€“192


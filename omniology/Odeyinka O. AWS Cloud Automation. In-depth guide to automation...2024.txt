

AWS
Cloud
Automation
In-depth guide to automation using
Terraform infrastructure as code
solutions
Oluyemi James Odeyinka

www.bpbonline.com

Copyright © 2024 BPB Online
All rights reserved. No part of this book may be reproduced, stored in a retrieval system, or
transmitted in any form or by any means, without the prior written permission of the publisher,
except in the case of brief quotations embedded in critical articles or reviews.
Every effort has been made in the preparation of this book to ensure the accuracy of the information
presented. However, the information contained in this book is sold without warranty, either express
or implied. Neither the author, nor BPB Online or its dealers and distributors, will be held liable for
any damages caused or alleged to have been caused directly or indirectly by this book.
BPB Online has endeavored to provide trademark information about all of the companies and
products mentioned in this book by the appropriate use of capitals. However, BPB Online cannot
guarantee the accuracy of this information.
First published: 2024
Published by BPB Online
WeWork
119 Marylebone Road
London NW1 5PU
UK | UAE | INDIA | SINGAPORE
ISBN 978-93-55516-534
www.bpbonline.com

Dedicated to
My beloved wife,
Memunat
&
My daughter Ayobola & my son Damola

About the Author
Oluyemi James Odeyinka has over a decade of experience serving as a
cloud solution architect and cybersecurity leader. Throughout his career, he
has played pivotal roles in numerous cloud projects, demonstrating both
leadership and hands-on engineering expertise. His project delivery
encompasses the deployment of resources for major companies, employing
Infrastructure as Code methodologies. Notably, he has achieved success
with high-profile projects in the United States.
Currently holding the position of Senior Solution Architect at Walgreens
and serving as the Founder and President of Cloudall Technologies,
Odeyinka is a multifaceted professional. His academic background includes
the attainment of a postgraduate MBA degree in Analysis, coupled with in-
depth knowledge of Agile Methods.
As an accomplished information technology leader, Odeyinka has amassed
significant experience in Cyber Security, Hybrid Cloud, Product
Management, Data Analytics, Governance, IoT, and Data Protection and
Data Management. He has actively contributed to the field by authoring and
leading numerous cloud and cybersecurity training sessions on a global
scale.
A sought-after speaker, Odeyinka has addressed audiences at various
conferences, such as IoT World in 2020 & 2021, Edge Computing 2021,
Embedded Devices 2022, and the latest Infosec World 2022. His expertise
is further solidified by his CISSP certification and proud membership in
ISC2, a leading IT Security organization. In recognition of his substantial
contributions to technical communities worldwide, he was also honored
with the Outstanding Leadership Award at the 2022 Internet 2.0
Conference.

About the Reviewers
❖ Divit Gupta, a seasoned IT professional with 20 years of industry
expertise, excels in driving strategic architecture initiatives and
providing leadership in multi-pillar sales cycles. With a global
impact, he spearheads technical partnerships, defines team vision,
and champions new strategic endeavors.
As the host of popular podcasts like 'Tech Talk with Divit,’ ʻLive
Labs with Divit,’ and ʻCloud Bites with Divit,’ he showcases
technological initiatives and leadership.
In 2022-23, he served as Oracle TV’s correspondent for Cloud
World. A recognized expert, Divit presented on Oracle Database
technology at Oracle Cloud World FY 2023.
His passion for knowledge sharing extends to international
conference talks, technical blogs, and multiple books on emerging
technologies. Divit has been featured in several prominent
newspapers and technology magazines worldwide.
Holding over 40 certifications from Microsoft, Oracle, AWS, and
Databricks, he remains at the forefront of technology.
❖ Mariano Geuzi Karaian is a seasoned technical delivery leader
with expertise in Architecture, Networking, and Security,
complemented by a deep passion for Cloud technologies. He
leverages AWS and Azure to assist companies in migrating their
technology stack to the cloud and developing cloud-native solutions,
prioritizing automation, throughout the process. Additionally, he
possesses experience in multi-cloud environments and is adept at
formulating resilient multi-cloud strategies.

Acknowledgement
I wish to convey my heartfelt appreciation to my family for their constant
support and encouragement during the creation of this book, with special
mention to my wife, Memunat, and my two children, Ayobola, my daughter,
and Damola, my son.
I want to also express my gratitude to BPB Publications for their invaluable
guidance and expertise in the successful completion of this book. The
process of revising the manuscript was a lengthy journey, enriched by the
valuable contributions and collaborative efforts of reviewers, technical
experts, and editors.
I would also want to express my gratitude for the significant contributions
made by my colleagues and co-workers throughout the years in the tech
industry. Their wealth of knowledge and constructive feedback have been
instrumental in my professional growth.
Finally, I would like to express my gratitude to every reader who has shown
interest in my book and supported its realization. Your encouragement has
been immensely valuable.

Preface
In the relentless pursuit of technological advancement, the landscape of
managing and deploying cloud infrastructure has undergone a profound
transformation. This metamorphosis is not merely a shift in methodology
but a revolution in the way we conceive, build, and maintain the digital
foundations of our interconnected world.
As we stand on the precipice of a new era in computing, this book embarks
on a journey into the heart of AWS IaC—a paradigm that transcends
traditional boundaries and challenges the very fabric of conventional
infrastructure management. In these pages, we will explore the profound
impact of treating infrastructure not as a static set of hardware and software
components but as dynamic, version-controlled code.
The genesis of this book lies in the realization that AWS IaC is not just a
buzzword or a fleeting trend. It is a fundamental shift that empowers
engineers, architects, and organizations to sculpt their digital landscapes
with unprecedented precision and agility. Gone are the days of manual,
error-prone configurations and the labyrinthine processes of provisioning
and scaling infrastructure. In their place emerges a landscape where AWS
infrastructure is expressed as code, a language understood by both machines
and humans.
Our exploration will traverse the foundational principles of IaC, delving
into the core philosophies that underpin its effectiveness. We will demystify
the orchestration tools and platforms that breathe life into code,
orchestrating intricate symphonies of servers, networks, and services with
the precision of a maestro.
The chapters that follow are not just a compilation of technical insights but
a narrative that unfolds the stories of pioneers who have harnessed the
power of IaC to reshape industries, break barriers, and fuel innovation.
From the streamlined efficiency of continuous delivery pipelines to the

resilience of infrastructure that adapts to the ever-changing demands of
modern applications, each page is a testament to the transformative
potential that IaC holds.
As you embark on this journey, whether you are a seasoned AWS engineer
seeking to deepen your understanding or a newcomer eager to grasp the
AWS fundamentals, I invite you to immerse yourself in the philosophy,
practices, and promises of Infrastructure as Code. Let this book be your
guide through the intricate tapestry of AWS code-driven infrastructure,
where the future unfolds with every line written and every deployment
executed. May it inspire you to not only embrace the tools and techniques
but to forge a mindset that embraces change, values collaboration, and
champions the evolution of our digital world.
Chapter 1: AWS DevOps and Automation Tools Set – This chapter
explains the skills to set up AWS CLI, AWS CDK, AWS CloudFormation,
and CodeCommit. Additionally, the reader will gain proficiency in
configuring CodeBuild, CodeDeploy, CodePipeline, and CodeArtifact to
automate AWS builds. Lastly, the reader will understand the process of
creating an S3 Bucket using CloudFormation.
Chapter 2: AWS Terraform Setup – This chapter presents an introduction
to Infrastructure as Code (IaC), elucidating its significance in contemporary
software development and operations. It offers an overview of Terraform,
highlighting its merits as an IaC tool. The chapter subsequently presents a
detailed, step-by-step guide on initiating work with Terraform,
encompassing aspects like installation, configuration, infrastructure
deployment, and management. Furthermore, the chapter addresses common
challenges and considerations inherent in the utilization of Terraform,
including constraints, security aspects, and the scalability of infrastructure.
By doing so, it aims to equip readers with a comprehensive understanding
of both the advantages and potential pitfalls associated with implementing
Terraform in the realm of Infrastructure as Code.
Chapter 3: IAM, Governance and Policies Administration – This
chapter covers AWS Identity and Access Management (IAM), governance,
and policy is presented to provide readers with essential knowledge and
skills for proficiently overseeing access control, establishing a governance
framework, and enforcing security measures within their AWS

infrastructure. Readers will gain insight into the core elements of AWS
IAM, such as users, groups, roles, and policies, and grasp the importance of
governance in the AWS environment. The chapter guides readers in
creating account structures and implementing Identity lifecycle
management through Terraform Infrastructure as Code (IaC). This chapter
introduces readers to the implementation of role-based access control
(RBAC), adherence to least privilege principles, resource-level permissions,
and the adoption of a zero-trust approach.
Chapter 4: Automating AWS Storage Deployment and Configuration –
This chapter aims to offer a thorough insight into the automation of
deployment and configuration processes for different Amazon storage
services through the use of Terraform. Upon completing this chapter,
readers will have acquired the expertise and hands-on proficiency required
to define, provision, and oversee storage deployment in a streamlined and
reliable manner using Terraform. The content of this chapter will provide
readers with a comprehensive understanding of employing Terraform
Infrastructure as Code (IaC) to deploy Amazon S3, EBS volumes, and EFS
file systems. Practical examples and hands-on exercises will be woven
throughout the chapter, guiding readers in the practical application of
Terraform for automating various AWS storage services.
Chapter 5: VPC and Network Security Tools Automation – This chapter
explores the complex domain of AWS Virtual Private Cloud (VPC) to
provide readers with a deep comprehension of its structure, elements, and
functionalities. This chapter guide readers through the intricacies of
automating the creation, setup, and administration of VPCs, enabling them
to proficiently design isolated network environments, establish secure
communication between resources, and seamlessly integrate with various
AWS services. By demystifying the intricacies of VPC security,
connectivity choices, and advanced configurations, it aims to cultivate
expertise in leveraging the full potential of AWS VPC for crafting robust,
scalable, and flexible cloud infrastructures.
Chapter 6: Automating EC2 Deployment of various Workloads – This
chapter explore the domain of EC2 deployment automation using the robust
infrastructure-as-code tool. Emphasizing efficiency and reproducibility,
navigating through contemporary cloud deployment practices, illustrating
how Terraform adeptly orchestrates the provisioning and management of

Amazon EC2 instances. By the conclusion of this chapter, readers will
acquire a comprehensive grasp of both the essential principles of Terraform
and EC2, as well as the strategic approaches to automate and enhance their
infrastructure deployment workflows. Through practical examples, best
practices, and real-world insights, the chapter empower readers to leverage
the capabilities of Terraform, enabling them to establish a sturdy foundation
for deploying EC2 instances while embracing the agility and reliability that
automation brings to cloud environments.
Chapter 7: Automating ELB Deployment and Configurations – This
chapter explains Elastic Load Balancer (ELB) foundational service
provided by Amazon Web Services (AWS), leading the way in load
balancing solutions. Amazon ELB serves as a versatile tool, empowering
developers and system administrators to efficiently distribute incoming
traffic across multiple instances, ensuring seamless and dependable user
experiences. As reader progress through this chapter, the reader will not
only gain a solid understanding of Amazon ELB but also the confidence to
design and deploy resilient, scalable, and fault-tolerant applications
utilizing this potent AWS service.
Chapter 8: AWS Route53 Policy and Routing Automation – This
chapter is dedicated to AWS Route 53 Policy and Routing Automation
seeks to optimize and automate the management of domains and traffic
routing within the Amazon Web Services (AWS) ecosystem. Its primary
goal is to streamline domain name management, enabling efficient traffic
routing and improved application performance. By employing automated
policies, this service ensures that domain requests adhere to defined rules
and conditions, directing them to the appropriate resources. Furthermore,
the reader learn how to use AWS Route 53 service to facilitates rapid
failover and disaster recovery by automating the redirection of traffic away
from unhealthy endpoints. This ensures uninterrupted service delivery and
minimal downtime during system failures or maintenance activities.
Ultimately, the goal is to provide a resilient and high-performance DNS
management solution that contributes to an optimal end-user experience.
Chapter 9: AWS EKS and Fargate Deployments – This chapter is
dedicated to Amazon Elastic Kubernetes Service (EKS) and AWS Fargate.
It covers essential concepts, deployment strategies, and provides hands-on
experience to equip readers with the knowledge and skills needed for

efficient management of containerized applications in a cloud-native
environment. AWS EKS, also known as Elastic Kubernetes Service, is a
fully managed Kubernetes service designed to streamline the deployment,
management, and scaling of containerized applications using Kubernetes on
the Amazon Web Services platform. The key goal of AWS EKS is to
furnish users with a dependable, highly available, and secure Kubernetes
environment, facilitating seamless orchestration and management of
containerized workloads. By eliminating the need for manual setup and
management of Kubernetes clusters, EKS allows organizations to focus on
their applications and business logic, with AWS handling the Kubernetes
infrastructure.
Chapter 10: Databases and Backup Services Automation – This chapter
this chapter is to explore the fundamental concepts underlying AWS
database services, comprehending their distinctive features, applications,
and advantages. Reader will learn how these services contribute to
optimizing organizational data architecture, ensuring data integrity,
achieving cost-effectiveness, and fostering innovation through insights
derived from data. Furthermore, the chapter delves deeply into the
intricacies of AWS Backup Services, a robust and versatile suite of tools
crafted to ensure the resilience and recoverability of your invaluable data.
Chapter 11: Automating and Bootstrapping Monitoring Service – This
chapter is dedicated to AWS Monitoring Service and reliability for
applications and infrastructure hosted on the Amazon Web Services (AWS)
platform. Monitoring plays a crucial role in proactively identifying and
addressing issues, preventing potential downtimes, and improving overall
operational efficiency. By continuously monitoring aspects such as resource
utilization, application performance, and system health, the AWS
Monitoring Service aims to provide actionable insights and data-driven
decisions for efficient resource allocation and management. The monitoring
tools and services within AWS enable organizations to analyze trends,
forecast growth, and make informed decisions about resource provisioning.
This ensures that applications and services can seamlessly handle increased
workloads while maintaining optimal performance.

Code Bundle and Coloured Images
Please follow the link to download the
Code Bundle and the Coloured Images of the book:
https://rebrand.ly/8wyxaqf
The code bundle for the book is also hosted on GitHub at
https://github.com/bpbpublications/AWS-Cloud-Automation. In case
there's an update to the code, it will be updated on the existing GitHub
repository.
We have code bundles from our rich catalogue of books and videos
available at https://github.com/bpbpublications. Check them out!
Errata
We take immense pride in our work at BPB Publications and follow best
practices to ensure the accuracy of our content to provide with an indulging
reading experience to our subscribers. Our readers are our mirrors, and we
use their inputs to reflect and improve upon human errors, if any, that may
have occurred during the publishing processes involved. To let us maintain
the quality and help us reach out to any readers who might be having
difficulties due to any unforeseen errors, please write to us at :
errata@bpbonline.com
Your support, suggestions and feedbacks are highly appreciated by the BPB
Publications’ Family.
Did you know that BPB offers eBook versions of every book published, with PDF and ePub files
available? You can upgrade to the eBook version at www.bpbonline.com and as a print book
customer, you are entitled to a discount on the eBook copy. Get in touch with us at :
business@bpbonline.com for more details.

At www.bpbonline.com, you can also read a collection of free technical articles, sign up for a
range of free newsletters, and receive exclusive discounts and offers on BPB books and eBooks.
Piracy
If you come across any illegal copies of our works in any form on the internet, we would be
grateful if you would provide us with the location address or website name. Please contact us at
business@bpbonline.com with a link to the material.
If you are interested in becoming an author
If there is a topic that you have expertise in, and you are interested in either writing or
contributing to a book, please visit www.bpbonline.com. We have worked with thousands of
developers and tech professionals, just like you, to help them share their insights with the global
tech community. You can make a general application, apply for a specific hot topic that we are
recruiting an author for, or submit your own idea.
Reviews
Please leave a review. Once you have read and used this book, why not leave a review on the site
that you purchased it from? Potential readers can then see and use your unbiased opinion to make
purchase decisions. We at BPB can understand what you think about our products, and our
authors can see your feedback on their book. Thank you!
For more information about BPB, please visit www.bpbonline.com.
Join our book’s Discord space
Join the book’s Discord Workspace for Latest updates, Offers, Tech
happenings around the world, New Release and Sessions with the Authors:
https://discord.bpbonline.com

Table of Contents
1. AWS DevOps and Automation Tools Set
Introduction
Structure
Objectives
Overview of Amazon Web Services tool set
Lists of AWS automation tool set
Infrastructure as code
How to setup CodeCommit repository
AWS CloudFormation
AWS CloudFormation template anatomy
CloudFormation change set
AWS CloudDevelopment Kit
Continuous Delivery
Continuous Integration
End-to-End view of AWS deployment tools
Creating end-to-end deployment pipeline using CloudFormation
Conclusion
Multiple choice questions
Answers
2. AWS Terraform Setup
Introduction
Structure
Objectives

Overview of Terraform
Main parts of Terraform
Benefits of Terraform
Key features and capabilities of Terraform
Understanding Terraform Infrastructure as Code
Installation architecture overview
Getting started with Terraform
Setup and configuration of Terraform on Linux Debian OS Family
Setup and configuration of Terraform on RHEL OS Family
Setup and configuration of Terraform on MacOS
Setup and configuration of Terraform on Windows
Installing and setup Sentinel on Linux
Installing Sentinel on MacOS
Installing Sentinel on Windows
Common terminologies in Terraform
End-2-End view of our deployed AWS CICD pipeline
Enterprise Terraform deployment using AWS CICD pipeline
Local Code Deployment directory structure details
Step-by-step deployment
Creating state file remote storage using s3
Validate deployment using AWS Console
Challenges and considerations with Terraform
Conclusion
Multiple choice questions
Answers
3. IAM, Governance and Policies Administration
Introduction
Structure
Objectives
Overview of AWS IAM

What is IAM
IAM Architecture
AWS IAM key components
AWS IAM user
AWS IAM group
Establishing governance frameworks
AWS account structure
AWS account structure segments
Implementing identity Life Cycle Management
Automating service control policies
Bash Script to create your own organization structure
Types of AWS organization policies
Bash script to enable your organization policies
Policy-Based Access Control using Terraform IaC
Create AWS user, group, and role using Terraform
Securing resources using AWS Policy
AWS governance policies
Backup policies
Tag policies
Service control policies
Creating SCP with Terraform
AWS Compliance and Auditing using Terraform IaC
Users’ automation using Terraform
Best practices for IAM using Terraform IaC
Access planning matrix table
Create IAM group using Terraform
AWS IAM role
Create IAM role using Terraform
Conclusion
Multiple choice questions
Answers

4. Automating AWS Storage Deployment and Configuration
Introduction
Structure
Objectives
Overview of AWS Storage
Overview of key AWS storage types
Benefit of AWS storage Services
Amazon Simple Storage Service
Understanding of S3 bucket, objects, and access controls
Use cases
Automation using Terraform
Amazon Elastic Block Storage
Use cases
Automation using Terraform
Amazon Elastic File System
Use cases
Automation using Terraform
Amazon FSx
Use cases
Automation using Terraform
Amazon Glacier
Use cases
Automation using Terraform
Amazon S3 Glacier
Use cases
Automation using Terraform
AWS Storage Gateway
AWS Storage Gateway options
Use cases
Automation using Terraform

Conclusion
Multiple choice questions
Answers
5. VPC and Network Security Tools Automation
Introduction
Structure
Objectives
AWS VPC overview
AWS VPC components
Subnet
Internet Gateway
Elastic IP
Network Address Translation Gateway
AWS VPC security group
AWS Network Access Control List
AWS VPC endpoint
AWS VPC peering connection
2-Tier Subnet Deployment Architecture overview
2-Tier VPC automation using terraform
3-Tier Subnet Deployment Architecture overview
3-Tier VPC Automation using Terraform
AWS VPC Peering
AWS VPC peering architecture
AWS VPC Peering Automation using Terraform
AWS VPC Monitoring and Auditing
Conclusion
Multiple choice questions
Answers
6. Automating EC2 Deployment of various Workloads

Introduction
Structure
Objectives
Overview of AWS Elastic Compute
Overview of AWS Amazon Machine image
AMI Automation using Terraform
Defining AMIs in Terraform
Advance EC2 instance types and their use cases
Different EC2 instance types and their use cases
Configuring network and security for EC2
Deploy web application on Single EC2 General Purpose
Deploying web application multi-EC2 Web Server General Purpose
using Terraform
Deploying EC2 Availability Group for application
Deploying EC2 Auto Scaling Group
Conclusion
Multiple choice questions
Answers
7. Automating ELB Deployment and Configurations
Introduction
Structure
Objectives
Types of Elastic Load Balancers
Introduction to Application Load Balancers
Automating Web Server with ALB
Introduction to Network Load Balancers (NLB)
Automating Web Server with NLB
Introduction to Classic Load Balancers
Automating Web Server with CLB

Introduction to Gateway Load Balancers
Conclusion
Multiple choice questions
Answers
8. AWS Route53 Policy and Routing Automation
Introduction
Structure
Objectives
Introduction to AWS Route 53
Features and benefits
Understanding Route 53 basics
Exploring Route 53 policies
Implementing simple routing policies
Simple Routing Automation using Terraform
Advanced routing strategies
AWS Route 53 Blue-Green alias weighted Routing Automation using
Terraform
AWS Route 53 Failover Automation using Terraform
Conclusion
Multiple choice questions
Answers
9. AWS EKS and Fargate Deployments
Introduction
Structure
Objectives
Understanding Kubernetes fundamentals
Exploring AWS EKS and Fargate
Introduction to containerization

Overview of EKS
Setting-up AWS EKS and Fargate
Access and security in AWS EKS and Fargate
Monitoring and logging in AWS EKS and Fargate
Deploying applications on AWS EKS and Fargate
Managing and updating AWS EKS and Fargate
Conclusion
Multiple choice questions
Answers
10. Databases and Backup Services Automation
Introduction
Structure
Objectives
Introduction to AWS Database Services
Exploring AWS Database offerings
Relational Database Services (RDS)
NoSQL Database Services
Amazon Redshift
ElastiCache: In Memory data store
Amazon Aurora
AWS DynamoDB
AWS DynamoDB Architecture Diagram
AWS DynamoDB Setup
Introduction to AWS backup and recovery
AWS Backup architecture and components
Backup and restore strategies
Backup integration with AWS Services
Backup services monitoring and reporting
Backup services, security, and compliance

Disaster recovery and business continuity
Conclusion
Multiple choice questions
Answers
11. Automating and Bootstrapping Monitoring Service
Introduction
Structure
Objectives
Overview of AWS Monitoring Services
End-to-end monitoring architecture diagram
Amazon CloudWatch: The foundation of AWS Monitoring
Amazon CloudTrail: Enabling comprehensive AWS activity
monitoring
Cloud auditing and observability automation
AWS X-Ray for application performance monitoring
Setting-up AWS X-Ray using Terraform
AWS Lambda monitoring
Monitoring AWS infrastructure with AWS Inspector
Setting up AWS inspector assessments
Best practices for effective AWS Monitoring
Conclusion
Multiple choice questions
Answers
Index

CHAPTER 1
AWS DevOps and Automation Tools
Set
Introduction
Amazon Web Services (AWS-https://aws.amazon.com/what-is-aws/ ) DevOps is
a set of practices and tools that are used by AWS to automate and orchestrate the
process of cloud infrastructure deployment and software development. DevOps
stands for Development and Operations, and it is a software deployment and
development methodology that combines the best practices of software
development with the best practices of IT operations. The main purpose of
DevOps is to improve the speed and quality of software delivery by creating a
culture of collaboration between development teams and operations teams.
AWS provides a wide range of services and automation tools that are designed to
help businesses automate various tasks, from deployment to monitoring and
management. In the context of tools set, businesses can use automation tools like
AWS CodeCommit, AWS CodePipeline, AWS CodeBuild, AWS OpsWorks, and
AWS CodeDeploy. AWS CodeStart enables you to quickly develop, build, and
deploy applications on AWS.
Finally, it should be noted that Site Reliability Engineering (SRE) is now
replacing the operation part of DevOps. SRE is a methodology for managing
large-scale software systems that prioritizes reliability, availability, and
automation to reduce the risk of downtime or other issues.
Structure
In this chapter, we will go through the following topics:

Overview of Amazon Web services automation tool set
AWS CloudDevelopment Kit
End-to-End view of AWS deployment tools
Objectives
In this chapter, you will learn how to setup AWS CLI, AWS CDK, AWS
CloudFormation, and CodeCommit. You will also learn how to configure
CodeBuild, CodeDeploy, CodePipeline, and Code Artifact for automating AWS
builds. And lastly, you will learn how to create S3 Bucket using CloudFormation.
Overview of Amazon Web Services tool set
The emergence of digital economy has drastically changed how Companies
deliver software. DevOps and Automation have championed the transformation
journey, and there is no way to mention DevOps without mentioning its twin
assistant, known as Automation, and vice versa. Amazon Web Services (AWS)
has a wide range of automation tool sets, some of which can be easily adapted for
Infrastructure as Code (IaC) while others are used for software development
codes. This chapter introduces you to the tool set available within the AWS
ecosystem, but before we look at this toolset in detail, let us understand what this
toolset is primarily used to accomplish:
Deployments: Includes pre-provisioning and post-provisioning definitions
Provisioning: Includes definition of desired state settings and goals
Configuration: Includes set of standards in parameterized format
Orchestration: Zero-touch deployment or zero-manual intervention from
users
The principle behind IaC is to treat infrastructure deployment the same way the
developers treat software code. The list of AWS tools set below will be used to
make our automation journey effective, smoother, and efficient to deliver
enterprise-grade IaC and software automation.
Lists of AWS automation tool set
Following is the list of AWS automation tool set:
AWS CodeCommit

AWS CodeBuild
AWS CodeArtifact
AWS Code Deploy
AWS CodeStar
AWS CodePipeline
AWS CloudFormation
AWS Cloud Development Kit (AWS CDK)
AWS Cloud Development Kit for Kubernetes
AWS Device Farm
Before we dive into the details of the toolset and its configuration, let us divide
the above list into three segments with details.
Infrastructure as code
AWS CodeCommit is a private or managed GitHub version of AWS; it is a private
source control service within AWS eco-system that hosts your git repositories.
CodeCommit allows you to host your IaC code within AWS services. You can still
use GitHub with CodeCommit if you choose to, you can migrate your existing code
to CodeCommit, and you can still collaborate with developers across the globe.
How to setup CodeCommit repository
AWS CodeCommit can be setup through AWS Management Console or AWS
CLI, we are going to use AWS CLI to setup our repository and Open CMD if you
are using Windows or Terminal if you are using Linux or MAC OS.
TIP: It is assumed you already have AWS Account, you already installed
GIT, AWS CLI, and you already configured AWS CLI to connect to your
AWS Account with the existing IAM user. Please use this link if you need
help 
setting 
up 
CodeCommit
https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up.html.
AWS CloudFormation
Follow these steps:
1. Check CodeCommit help feature:

Policy $ aws codecommit help
If everything is setup right, you should get the following output:
    codecommit
^^^^^^^^^^
Description
***********
This is the *AWS CodeCommit API Reference* . This 
reference provides
descriptions of the operations and data types for 
AWS CodeCommit API
along with usage examples.
You can use the AWS CodeCommit API to work with the following
objects.
2. Create your first CodeCommit repository:
$ aws codecommit create-repository --repository-
name awsca-chapter1-repo --repository-description 
"This is chapter1 demo repo"
3. Verify the repository that was actually created:
$ aws codecommit list-repositories
Output:
         {
    "repositories": [
        {
            "repositoryName": "aws-cloud-
automation",
            "repositoryId": "dbd5e1f2-860e-4c32-
9f4d-7a56229730fa"
        }

    ]
}
4. Now clone the repository you just created:
$ git clone https://git-codecommit.us-east-
2.amazonaws.com/v1/repos/awsca-chapter1-repo
5. Now check your cloned repository:
ls
Output:
awsca-chapter1-repo/
You have now finished setting up your CodeCommit repository, we will use this
repo later to hold all chapter1 codes including our CLI commands we have used
so far.
AWS CloudFormation is a service that provides developers and system
administrator an automated way to create, deploy, provision, and manage AWS
cloud resources in a consistent and predictable manner. CloudFormation templates
are written in JSON or YAML to describe the standard of AWS stack.
AWS CloudFormation template anatomy
Following is the AWS CloudFormation template anatomy:
Template Format Version
Description
Metadata
Parameters
Mappings
Conditions
Transform
Resources
Outputs
AWS CloudFormation which is used to create a new CodeCommit:
AWSTemplateFormatVersion: "2010-09-09"

Description: "This is codecommit repository created 
with cloud formation"
Resources:
  CodeCommitRepository:
    Type: "AWS::CodeCommit::Repository"
    Properties:
      RepositoryName: "awsca-chapter1-cloudFormation-
repo"
Parameters: {}
Metadata: {}
Conditions: {}
Verify if the repository was created:
$ aws codecommit list-repositories
{
    "repositories": [
        {
            "repositoryName": "aws-cloud-automation",
            "repositoryId": "dbd5e1f2-860e-4c32-9f4d-
7a56229730fa"
        }
    ]
}
Finally, if you read the code of topic Name the file
awsCloudAutomationCodePipelineDemo.json, you will notice we used some of the
CloudFormation Anatomy mentioned on page 10 such as Description, Resources,
Parameters, Metadata, and Condition. The CloudFormation can use all the
Template Anatomy list or the subset of it. CloudFormation deployment can be
modified at any time if needed, by applying what is known as change set.

CloudFormation change set
Change Set is a terminology used when developer modified the template by
adding resource to be deployed to the CloudFormation definition. Change Set can
be applied to the template at any given time.
AWS CloudDevelopment Kit
This is an open source software development kit that is used to model, deploy and
provision cloud resources. The use of CDK allows developers with existing
experience with other programming languages such as Python, TypeScript, Java
and .NET to accelerate their IaC practices.
How to setup your first AWS CDK:
1. Check if npm application is installed:
$npm --version
Output:
9.2.0
If everything is setup right, you should get the version in the output.
2. Install AWS CDK using npm package:
$ npm install -g aws-cdk
If everything is setup right, you should get the output after you checked the
version:
$ cdk --version
Output:
2.60.0 (build 2d40d77)
3. Initialize CDK environment for development needs:
$cdk init --language typescript
4. Bootstrap your AWS Account to the CDK environment by running
following commands:
Get your account identity:
$aws sts get-caller-identity
Get the default region:

$aws configure get region
5. Now run the command below to bootstrap your account:
$cdk bootstrap aws://ACCOUNT-NUMBER/REGION
npm test
Output:
> aws-cdk@0.1.0  
> jest  
PASS test/aws-cdk.test.ts (8.503 s)
 
SQS Queue Created (1 ms)
Test Suites: 1 passed, 1 total
Test:        1 passed, 1 total
Snapshots:   0 total
Time:        9.074 s
Ran all test suites.
Note: Helm is mostly used today to deploy pods and applications.
AWS Cloud Development Kit for Kubernetes (CDK8s) is similar to AWS CDK
in the sense that it is also an open-source software development kit that is used to
define kubernetes application and resources. The use of CDK8s allows developers
with existing experience with other programming languages such as Python and
TypeScript to accelerate their Kubernetes deployments. It should be noted that all
CDK8s will be converted to YML files before it can be consumed by Kubernetes
clusters.
Continuous Delivery
Continuous Delivery (CD) is the heartbeat of modern software development,
orchestrating a symphony of seamless processes that propel software from
conception to deployment with unparalleled efficiency. In this dynamic dance of
development, every line of code is a note, and CD ensures that these notes
harmonize flawlessly, producing a symphony of innovation. It transcends the
conventional barriers of traditional software release cycles. CD is not merely a
methodology; it is a philosophy that embraces adaptability and thrives on the
relentless pursuit of perfection in the software delivery lifecycle.

Within the spectrum of continuous delivery, AWS provides two pivotal services:
AWS CodeDeploy: Is a managed service provided by AWS to aide
deployment of pre-built application different endpoints such as EC2,
Lambda, and On-Premises servers. CodeDeploy runs on a serverless engine
and customers are only charged when it is used, it is typically known as
pay-as-you-run.
AWS CodePipeline: Is a managed service provided by AWS to automate
your application release pipeline and infrastructure updates. CodePipeline
makes the automation of build, test, and deployment of software release
phases seamless when there’s code change.
Continuous Integration
Continuous Integration (CI) is the architectural cornerstone of software
craftsmanship, akin to the intricate interlocking of bricks in a well-constructed
edifice. It is the vigilant guardian of code quality, constantly surveying the
landscape to detect and rectify any structural weaknesses. Like a vigilant
lighthouse guiding ships through treacherous waters, CI ensures that the software
journey is safe and reliable. With each integration, it fortifies the codebase, laying
a foundation that can withstand the tempests of bugs and glitches. In this
architectural dance, CI becomes the silent but omnipresent architect, shaping the
digital skyline with resilience and precision, ensuring that the software structure
stands tall against the winds of change.
In the domain of continuous integration, AWS offers robust solutions:
AWS CodeBuild: Is a managed service provided by AWS to compile source
code which produces software packages that can be used to deploy
applications. CodeBuild is powerhouse with the ability to process multiple
builds in parallel.
AWS CodeArtifact: Is a managed service provided by AWS to allow
organization to securely share, store, and publish software packages used in
software development process. AWS CodeArtifact works with most
common used package managers and common build tools and promotes
flexibility in fetching packages as they are made available.
End-to-End view of AWS deployment tools

In the following Figure 1.1, end-to-end tool of AWS development pipeline eco-
system is depicted:
Figure 1.1: End-to-End tool of AWS development pipeline eco-system
Creating end-to-end deployment pipeline using CloudFormation
Following are the steps for creating end-to-end deployment pipeline using
CloudFormation:
1. Create S3 Bucket to be used for our deployment and create a file named
myS3-awsca-demo.yaml:
AWSTemplateFormatVersion: 2010-09-09
Description: S3 Bucket to Host Our Simple 
CodePipeline CloudFormation Templates
Resources:
  S3Bucket:
    DeletionPolicy: Retain
    Type: 'AWS::S3::Bucket'
    Description: Creating S3 Bucket to Host Our 
Simple CodePipeline CloudFormation Templates

    Properties:
      BucketName: s3-awsloudutomation-demo
      AccessControl: Private
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      VersioningConfiguration:
        Status: Enabled
Outputs:
  S3Bucket:
    Description: Bucket Created using this 
template.
    Value: !Ref S3Bucket
2. Run the template using CloudFormation Studio using AWS Console.
3. Confirm that your Bucket is deployed (refer to Figure 1.2):

Figure 1.2: Full details of s3-bucket created with CloudFormation
4. Create the CodeBuild, CodeDeploy, and CodePipeline using CloudFormation
template as follows:
Name the file awsCloudAutomationCodePipelineDemo.json:
{
  "AWSTemplateFormatVersion": "2010-09-09",
  "Resources": {
    "awsCloudAutomationDemoCodePipeline": {
      "Type": "AWS::CodePipeline::Pipeline",
      "Properties": {
        "ArtifactStore": {
          "Type": "S3",
          "Location": "s3-awscloudautomation-demo"
        },
        "Name": 
"awscloudautomationdemocodepipeline",
        "RoleArn": "arn:aws:iam::account-
id:role/codepipeline_service_role",
        "Stages": [
          {

            "Name": "Source",
            "Actions": [
              {
                "Name": "Source",
                "ActionTypeId": {
                  "Category": "Source",
                  "Owner": "AWS",
                  "Provider": "S3",
                  "Version": "1"
                },
                "Configuration": {
                  "PollForSourceChanges": "false",
                  "S3Bucket": "s3-
awscloudautomation-demo",
                  "S3ObjectKey": 
"builds/awscloudautomationDemo.zip"
                },
                "OutputArtifacts": [
                  {
                      "Name": "SourceArtifact"
                  }
                ]              
              }
            ]
          }, 
          {

            "Name": "Build",
            "Actions": [
                {
                    "Name": "Build",
                    "ActionTypeId": {
                        "Category": "Build",
                        "Owner": "AWS",
                        "Provider": "CodeBuild",
                        "Version": "1"
                    },
                    "RunOrder": 1,
                    "Configuration": {
                        "BatchEnabled": "false",
                        "ProjectName": { 
"Fn::GetAtt" : [ "awscloudautomationDemoCodeBuild" 
,"Arn" ] }                                           
                    },
                    "OutputArtifacts": [
                        {
                            "Name": 
"BuildArtifact"
                        }
                    ],
                    "InputArtifacts": [
                        {
                            "Name": 
"SourceArtifact"

                        }
                    ],
                    "Namespace": "BuildVariables"    
                }
            ]           
          },       
          {
            "Name": "Deploy",
            "Actions": [
              {
                "Name": "create-changeset",
                "ActionTypeId": {
                  "Category": "Deploy",
                  "Owner": "AWS",
                  "Provider": "CloudFormation",
                  "Version": "1"
                },
                "RunOrder": 1,
                "Configuration": {
                  "ActionMode": 
"CHANGE_SET_REPLACE",
                  "Capabilities": 
"CAPABILITY_NAMED_IAM,CAPABILITY_AUTO_EXPAND",
                  "ChangeSetName": 
"awscloudautomationdemo-changeset",
                  "RoleArn": 
"arn:aws:iam::758678122039:role/AWSCloudFormationF

ullAccess",
                  "StackName": 
"awscloudautomationdemo",
                  "TemplatePath": 
"BuildArtifact::outputtemplate.yml"
                },
                "OutputArtifacts": [],
                "InputArtifacts": [
                  {
                    "Name": "BuildArtifact"
                  }
                ],
                "Namespace": "DeployVariables"
              },
              {
                "Name": "execute-changeset",
                "ActionTypeId": {
                  "Category": "Deploy",
                  "Owner": "AWS",
                  "Provider": "CloudFormation",
                  "Version": "1"
                },
                "RunOrder": 2,
                "Configuration": {
                  "ActionMode": 
"CHANGE_SET_EXECUTE",

                  "ChangeSetName": 
"awscloudautomationdemo-changeset",
                  "StackName": 
"awscloudautomation-demo"
                },
                "OutputArtifacts": [],
                "InputArtifacts": [
                  {
                    "Name": "BuildArtifact"
                  }
                ]
              }
            ]
          }
        ]
      }
    },
    "awscloudautomationDemoCodeBuild": {
      "Type" : "AWS::CodeBuild::Project",
      "Properties" : {
          "Name": "NodeJSBuild",
          "Source": {
              "Type": "CODEPIPELINE"               
          },
          "Artifacts": {
              "Type": "CODEPIPELINE",

              "Name": 
"awscloudautomationdemocodepipeline"                
          },
          "Environment": {
              "Type": "LINUX_CONTAINER",
              "Image": 
"aws/codebuild/standard:4.0",
              "ComputeType": 
"BUILD_GENERAL1_SMALL",
              "ImagePullCredentialsType": 
"CODEBUILD"
          },
          "ServiceRole": "arn:aws:iam::account-
id:role/codebuild_service_role",
          "TimeoutInMinutes": 60,
          "QueuedTimeoutInMinutes": 480,
          "Tags": [],
          "LogsConfig": {
              "CloudWatchLogs": {
                  "Status": "ENABLED"
              },
              "S3Logs": {
                  "Status": "DISABLED",
                  "EncryptionDisabled": false
              }
          }        
      }

    }
  }
}
5. Run the following command from the command line or use
CloudFormation studio using console:
AWS CLI COMMAND:
aws cloudformation create-stack --stack-name 
awsCloudAutomationCodePipelineDemo --template-body 
file://awsCloudAutomationCodePipelineDemo.json
6. Validate the service deploy via the CloudFormation console (refer to Figure
1.3):
Figure 1.3: Out of deployed services through CloudFormation
Note: You may run into issue during deployment due to CODEBUILD
and CODEPIPELINE Role. Please follow the steps below to correct
the error.
7. Create the codebuild_service_role using the template below:
{
  "Version": "2012-10-17",
  "Statement": [
    {

      "Effect": "Allow",
      "Principal": {
        "Service": "codebuild.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
8. Create the codepipeline_service_role using the template below:
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "codepipeline.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
TIP: If you already have existing CodeBuild role, you can edit line 12 and
139 in step 4.
Conclusion

It is very obvious that big organizations need to adopt multiple tools before they
successfully practice IaC and DevOps. CICD plays a very important role in
transforming source codes into a repeatable, reliable, and continuous deployment.
AWS tools such as CodeCommit, CodePipeline, CodeBuild, CodeGuru, Cloud9,
CloudFormation, CloudShell, CDK, and CodeDeploy are part of cloud automation
tools that are used often to deliver quality software which reduce or eliminates
human errors.
By completing this chapter, you have completed the following so far, and we will
be using some of this setup in the subsequent chapters:
Created CodeCommit
Created Repository and push sample code
Created AWS CDK
Created CodeCommit Repository using CloudFormation
Created CodeDeploy
Created CodeBuild
Created Artifacts
Created CodePipeline and Ran Build
By leveraging some or all these tools and technologies will accelerate
organization DevOps practices and automation journey. Organizations that
invested in automation are poised to deliver quicker than their counterparts.
In the next chapter, you will learn how to setup and configure terraform for
provisioning and deploying of cloud infrastructure. You will also learn about the
inner working engine of the terraform.
Multiple choice questions
1. AWS CodeCommit is similar to GIT:
a. True
b. False
2. AWS Deployment tools includes:
a. CodeCommit

b. CodeDeploy
c. Code Build
d. CodePipeline
e. All of the above
Answers
1. a
2. e
Join our book’s Discord space
Join the book’s Discord Workspace for Latest updates, Offers, Tech happenings
around the world, New Release and Sessions with the Authors:
https://discord.bpbonline.com

CHAPTER 2
AWS Terraform Setup
Introduction
Terraform is an open-source Infrastructure as Code (IaC) tool developed by
HashiCorp. It allows the users to define, provision, and manage infrastructure in a
declarative manner, allowing developers to specify the desired state of
infrastructure and then have Terraform make the necessary changes by
provisioning the infrastructure into the defined state.
IaC is a paradigm shift in the way infrastructure is managed and deployed. Before
the age of IaC, infrastructure deployment was a manual and error-prone process,
requiring human intervention and often resulting in inconsistencies and
configuration drift. Terraform addresses most of these challenges by providing a
method to codify infrastructure requirements in a configuration file. The IaC
configuration files describe the desired state of the infrastructure, which comprises
EC2, storage, VPC, applications, and more.
Finally, we will learn, how to provision resources instead of writing scripts.
Terraform embraces a declarative approach and uses desired end state of the
infrastructure. Terraform determines the necessary methodology and steps to
achieve that state and applies them automatically. This approach ensures that the
infrastructure remains in sync with the desired configuration, and any changes can
be made through version-controlled files.
Structure
This chapter covers the following topics:
Overview of Terraform

Getting started with Terraform
Common terminologies in Terraform
Creating state file remote storage using s3
Objectives
This chapter introduces the concept of IaC and explains its significance in modern
software development and operations. Provides an overview of Terraform
highlights the benefits and strengths of Terraform as an IaC tool. Hence, it
provides a step-by-step guide to getting started with Terraform, including
installation, configuration, deployment, and management of infrastructure. It
discusses the best practices and guidelines for writing Terraform code, version
control, collaboration with other developers, git repository, and deployment
pipeline. Lastly, this chapter familiarizes the readers with the high-level workflow
of infrastructure as code and how Terraform fits into that workflow. It also
addresses common challenges and considerations when using Terraform, such as
limitations, security considerations, and scaling of infrastructures.
Overview of Terraform
Terraform was developed by HashiCorp in 2014. Terraform emerged as one of the
pioneering tools in the IaC space, aiming to provide a unified approach to
managing diverse cloud and on-premises infrastructure. Terraform has evolved
significantly as both an open-source and commercial support tool to define and
manage infrastructure in a declarative manner. Hence, this chapter also explores
the inner working engine of Terraform, best practices, and how to call other
programming languages such as Bash Script and other languages within
Terraform.
Main parts of Terraform
Terraform is a powerful open-source Infrastructure as Code (IaC) tool
developed by HashiCorp that allows you to define, provision and manage your
infrastructure using a declarative configuration language.
Provider configuration: This section defines the cloud or service provider
you will be using. It specifies the necessary credentials and endpoints. For
example, if you are using AWS, you would configure the AWS provider:
provider "aws" {

region = "us-west-2"
} 
Resource blocks: Resource blocks are where you define the infrastructure
components you want to create or manage. Resources could be virtual
machines, databases, storage buckets, networks, or any other service
provided by the chosen provider. Each resource has a unique type and a set
of configuration parameters.
resource "aws_instance" "aws-cloud-automation-
book" {
  ami           = "ami-0c55b159cbfafe1f0"
  instance_type = "t2.micro"
}
Data blocks: Data blocks are used to fetch information from the provider,
like the state of existing resources. This is useful for referencing existing
resources or for making decisions based on data fetched from the provider:
data "aws_ami" " aws-cloud-automation-book " {
most_recent = true
filter {
name   = "name"
values = ["ubuntu/images/hvm-ssd/ubuntu-bionic-
18.04-amd64-server-*"]
}
}
Variables: Variables allow you to parameterize your configuration. You can
define input variables to make your configurations more flexible and
reusable. Variables can have default values or be set externally when you
run Terraform:
variable "instance_count" {

type    = number
default = 2
}
Output blocks: Output blocks define what information you want to expose
after running terraform apply. This is useful for showing, sharing, or using
the result of your infrastructure provisioning:
output "instance_ip" {
value = aws_instance.example[*].public_ip
}
Modules: Modules allow you to encapsulate and reuse parts of your
configuration. You can create custom modules to manage common patterns
or sets of resources. This promotes modularity and reusability in your
configurations:
module "web_server" {
source = "./modules/web_server"
 instance_count = var.instance_count
}
State management: Terraform keeps track of the state of your
infrastructure, often named terraform.tfstate. This is essential for
knowing what resources exist and their current configuration. State files can
be managed remotely in a backend like AWS S3 or locally in a file.
Provisioners: Provisioners allow you to run scripts or commands on the
resources you are creating. They can be used for tasks like installing
software, configuring instances, or running custom initialization scripts:
provisioner "file" {
source      = "init_script.sh"
destination = "/tmp/init_script.sh"
}

provisioner "remote-exec" {
inline = ["chmod +x /tmp/init_script.sh", 
"/tmp/init_script.sh"]
}
Lifecycle blocks: The lifecycle block can be used to control aspects of how
Terraform manages resources. For example, you can prevent certain
resources from being destroyed or updated:
lifecycle {
prevent_destroy = true
}
Terraform core: This provides terraform Command-Line-Interface
(CLI), which is used to initialize, plan, apply, and manage your
infrastructure using Terraform configurations. Common commands include
terraform init, terraform plan, terraform apply, and terraform destroy.
Terraform Plugins: This provides communication between core and
plugins using Remote Procedure Calls (RPC) to make a call to providers
and provisioners. Terraform plugin allows the full life cycle management of
its provider resources.
Terraform Cloud/Enterprise (Optional): Terraform Cloud and Terraform
Enterprise are optional services provided by HashiCorp for remote state
storage, collaboration, and additional features like remote execution runs,
access control, and more.
Benefits of Terraform
The following are the benefits of Terraform:
Automation: Enables the management of the entire lifecycle of resources,
from deployment to decommissioning.
Consistency: Promotes consistency in deployment through code and
versioning processes.
Reliability: The deployment becomes more reliable because it is less
manual.

Repeatability: Code and versioning processes make the process repeatable
and less error-prone.
Modular: Deployment can modularize to promote an easy understanding of
the structure of the deployment. Modularization also allows the use of code
for multiple deployments.
Open source: Terraform is open source and is used by major enterprises.
Multi-Cloud Provider support (flexibility: one technology to manage
multiple cloud providers.
Support for on-premise infrastructure: It allows the management of on-
premise resources.
Multi-language 
support: 
Supports 
Python, 
HCL 
and 
JSON
(CloudFormation only supports JSON and YAML).
Supports dynamic variables: This can be useful for automating the
provisioning of infrastructure. CloudFormation does not support dynamic
variables.
Supports conditionals: This can be useful for creating complex
infrastructure deployments. CloudFormation does not support conditionals.
Key features and capabilities of Terraform
Following are the key features and capabilities of terraform:
Declarative Language: Terraform uses a declarative language called
HashiCorp Configuration Language (HCL): Allows users to define
infrastructure resources, dependencies, and configurations in a human-
readable format, including Python and JSON.
Cloud agnostic: Terraform supports multiple cloud providers, enabling
users to manage infrastructure across different platforms.
Infrastructure as Code (IaC): Terraform treats infrastructure as code,
providing versioning, code reuse, and collaboration capabilities.
Ecosystem: Terraform has a large and active community, which contributes
immensely to the development of provider plugins, modules, and sharing
best practices.
State management: Terraform maintains a state file to track the current
state of deployed infrastructure, enabling updates and management of

existing resources.
Agentless: Terraform uses API as a mode of interaction with infrastructure.
It supports the deployment of infrastructure without the installation of
agents on devices.
Modular and extensible: Terraform allows users to create reusable
modules, enabling code sharing. Terraform is known to have extensive
plugin systems with dynamic Variables, conditionals and support for on-
premises infrastructure.
Understanding Terraform Infrastructure as Code
The IaC workflow is a systematic approach to managing and provisioning
infrastructure resources using code. It entails defining infrastructure
configurations in a declarative or imperative language, such as JSON or YAML,
and then using tools to automate the deployment and management of those
configurations. The workflow typically consists of several key steps. First, the
infrastructure requirements are defined and translated into code. This code is then
version-controlled and stored in a git or similar repository. Next, an infrastructure
provisioning tool, such as Terraform, creates, and manages the infrastructure
resources based on the code. The tool orchestrates the deployment and
configuration of resources across different environments using a code pipeline.
Lastly, the infrastructure code is continuously tested, reviewed, and updated as
needed, ensuring that the infrastructure remains consistent and scalable
throughout its lifecycle.
Terraform is a widely used infrastructure provisioning tool that fits seamlessly
into the IaC workflow. It allows organizations to define their infrastructure as
code using a declarative language called HCL. Terraform provides a flexible and
scalable approach to managing infrastructure resources across multiple cloud
providers, data centers, and service providers. It enables infrastructure teams to
create, modify, and destroy resources in a controlled and reproducible manner.
Terraform acts as the orchestrator in the IaC workflow by interpreting the
infrastructure code and communicating with the underlying cloud or technology
APIs to provision and manage resources. It provides several features, such as
dependency tracking, parallel resource creation, and state management, ensuring
that the desired infrastructure state is achieved and maintained. With Terraform,
organizations can achieve infrastructure automation, version control,
infrastructure drift detection, and drift prevention, improving efficiency and
reliability in managing their infrastructure.

Installation architecture overview
This section delves into the core components of Terraform’s installation
architecture. From the command-line interface that serves as our gateway to the
vast possibilities, of the state management system that maintains the desired
infrastructure’s state, we will uncover the layers that form Terraform’s robust
foundation. This section further explores the intricacies of Terraform’s
installation, unravelling the inner workings of this remarkable technology. From
the initial setup to the orchestration of complex multi-cloud environments.
The following are the core foundations of technology for all the 3-major operating
systems (Figure 2.1):
Figure 2.1: Terraform Installation Architecture Overview
Getting started with Terraform
Follow the below-mentioned steps, by installing Terraform on your local machine
and configuring the necessary environment variables for your cloud provider.
Setup and configuration of Terraform on Linux Debian OS Family
Following are the steps to setup Terraform on Linux Debian OS Family:
1. Install Curl:
sudo apt install curl
2. Add GPG key:
curl -f sSL https://apt.release.hashicorp.com/gpg 
| sudo apt-key add -

3. Add a repository for Terraform and do system update:
sudo apt-add-repository "deb [arch=amd64] 
https://apt.release.hashicorp.com $(lsb_release -
cs) main"
sudo apt update
4. Install Terraform and validate the installation:
sudo apt-get install terraform
terraform -v
Output:
Terraform v1.3.7
on linux_amd64
Setup and configuration of Terraform on RHEL OS Family
Following are the steps to setup Terraform on Linux Debian OS Family:
1. Install Curl:
sudo yum install -y yum-utils
2. Add Hashicorp Repo:
sudo yum-config-manager --add-repo 
https://rpm.release.hashicorp.com/RHEL/hashicorp.r
epo
3. If on Amazon EC2, Add this Repo:
sudo yum-config-manager --add-repo 
https://rpm.release.hashicorp.com/AmazonLinux/hash
icorp.repo
4. Install Terraform and validate the installation:
sudo yum -y install terraform
terraform -V
Output:
Terraform v1.3.7

on linux_amd64
Setup and configuration of Terraform on MacOS
Following are the steps to setup Terraform on MacOS:
1. Install using Homebrew:
% /bin/bash -c "$(curl -fsSL 
https://raw.githubusercontent.com/Homebrew/install
/master/install.sh)"
 terraform -v
2. Install using Brew:
% brew install terraform
               OR
% brew tap install Hashicorp/tap
% brew install Hashicorp/tap/terraform
3. Verify installation:
terraform -v
Output:
Terraform v1.1.2
on darwin_amd64
Setup and configuration of Terraform on Windows
To install terraform on Windows, you need to perform the following steps:
1. Download terraform executable from the GitHub repository and select 32-
bit or 64-bit.
2. Extract the .zip file folder that contains the executable to a folder. Please
use this PATH:"C:\Program Files (x86)\Terraform"
3. Set an Environment Variable by using CORTANA and search for Edit the
system environment variables:
Below is the Cortana search window (refer to Figure 2.2):

Figure 2.2: Windows OS Search
4. Below in Figure 2.3 is the system environment variables:
Figure 2.3: Output of environment variables.
5. Click Environment, select path, and click EDIT on the pop-up.
6. Click New, Paste the path, and click OK (refer to Figure 2.4):

Figure 2.4: Configuration window for path
7. Now open the Command prompt and type the following command:
terraform -v
Output:
Terraform v1.3.7
on windows_amd64
Installing and setup Sentinel on Linux
Following are the steps to install Sentinel on Linux:
1. Install jq using the command below:
sudo apt-get install jq
2. Download the sentinel installer:

curl -LO 
https://raw.github.com/robertpeteuil/sentinel-
installer/master/sentinel-install.sh
3. Change the script permission to executable:
chmod +x sentinel-install.sh
4. Run the installer script:
./sentinel-install.sh
5. Validate installation:
sentinel
Installing Sentinel on MacOS
Following are the steps to install Sentinel on MacOS:
1. Install jq using brew command below:
brew install jq
2. Install Sentinel using brew command below:
brew install sentinel
3. Validate installation:
sentinel
Installing Sentinel on Windows
To install terraform on Windows, you need to perform the following steps:
1. Download terraform executable from the GitHub repository and select 32-
bit or 64-bit.
2. Extract the .zip file folder that contains the executable to a folder. Please
use this PATH: "C:\Program Files (x86)\Sentinel"
3. Set an environment variable by using COTANA and search for Edit the
system environment variables.
4. Click Environment, select path, and click EDIT on the pop-up.
5. Click New, paste the path, and click ok.
6. Now open the Command prompt and type the command below:

sentinel
Output:
Usage: sentinel [--version] [--help] <command> 
[<args>] 
Available commands are:
    apply      Execute a policy and output the 
result
    fmt        Format Sentinel policy to a 
canonical format
    test       Test policies
    version    Prints the sentinel runtime version
Now that we have installed Terraform and sentinel on your workstation, we need
to go through some other setup to get ready for Chapter 3, IAM, Governance and
Policy Administration and subsequent chapters. This chapter is main foundation
for using terraform for all our automation needs. We will, from time-to-time
reference or use some of the tools we created in Chapter 1, AWS DevOps &
Automation Tool Set kit, for example we will be using CodeCommit more with
every chapter, and we will also be using CodeBuild, CodeDeploy, and
CodePipeline to get things in place.
Common terminologies in Terraform
To get started, you need to create a working directory containing all terraform and
configuration files. Below are major commands that will be used time to time
when we need to apply terraform to build AWS cloud infrastructure:
Terraform initialize and plan: Run the terraform init command to
initialize your project and download any necessary provider plugins. Once
initialized, execute terraform plan to generate an execution plan. Review
the plan to understand Terraform’s changes to your infrastructure.
Apply changes: Execute terraform apply to apply the changes defined in
your configuration file. Terraform will communicate with the cloud
provider’s API and provision the required resources. Confirm the changes
by typing "yes" when prompted.
Infrastructure management: As your infrastructure evolves or requires
changes, update your Terraform configuration file accordingly. When

changes are made, re-run terraform plan to preview the modifications
before applying them using terraform.
Destroy infrastructure: To tear down the infrastructure, use terraform
destroy command. This ensures that all the resources managed by
Terraform are properly terminated, preventing unwanted charges or
lingering resources.
The following is the Table 2.1 with high-level and most used Terraform
commands:
COMMAND
DESCRIPTION OF COMMAND
init
This is to prepare your working directory for terraform commands
validate
This checks your configuration files for validity
plan
This shows changes that will be applied through the configuration changes
apply
This updates or creates infrastructure
destroy
This destroys the infrastructure that was previously created
Table 2.1: Terraform most used commands.
End-2-End view of our deployed AWS CICD pipeline
The AWS CICD Pipeline architecture empowers developers to focus on crafting
impeccable code while automating the tedious tasks of building, testing, and
deploying applications. This architectural marvel harnesses the power of AWS
services and tools to create a robust and highly customizable pipeline, tailored to
meet the unique needs of your organization. Whether you are a startup looking to
establish a scalable infrastructure or an enterprise striving for consistent software
delivery, AWS CICD Pipeline architecture provides a flexible and scalable
solution.
By leveraging AWS services like AWS CodeCommit, AWS CodeBuild, AWS
CodeDeploy, and AWS CodePipeline, the architecture seamlessly integrates
source code management, automated build processes, comprehensive testing
frameworks, and reliable deployment mechanisms.
Lastly, staying ahead requires embracing modern software delivery practices in
this ever-evolving digital landscape. The AWS CICD Pipeline architecture
empowers you to unlock the full potential of your development team, delivering
applications with speed, agility, and confidence (refer to Figure 2.5):

Figure 2.5: End-to-End infrastructure diagram for CICD pipeline
Enterprise Terraform deployment using AWS CICD pipeline
This section familiarizes you with creating enterprise deployment using Terraform
and AWS CICD Pipeline. This deployment includes using terraform modules,
which allows the creation and usage of abstraction on top of the resources set.
Following is the local deployment directory structure:
.
└── Root Folder/
├── Modules/
│ ├── codecommit/
│ ├── codebuild/
│ ├── codepipeline/
│ ├── iam-role/
│ ├── kms/
│ └── s3/
├

├── templates/
│ └── scripts/
└── sampleParameterFile/
Local Code Deployment directory structure details
The details of Local Code Deployment directory structure are:
Root folder: This is the root folder for all our deployments. It contains
several files, folders, and sub-folders that is needed for our code and
deployment to work. The folders inside the root folders are, Modules,
sampleParameterFiles, and templates, while the files in the root folder are
data, locals, main, variables, and the hidden file called gitignore.
Modules folder: This subfolder contains the abstract folders that are needed
to help our deployment scale. Using a module in Terraform is more
advanced because it is like calling a function within a function that handles
other layers of infrastructure deployments. The folder inside the modules,
CodeCommit, CodeBuild, CodePipeline, IAM Role, KMS, and S3, has
Terraform configuration files to create abstract resources. You can read
more 
about 
terraform 
modules 
using 
this 
link
https://aws.amazon.com/solutions/partners/terraform-modules/?
quickstart-all.sort-by=item.additionalFields.sortDate&quickstart-
all.sort-order=desc
Template folder: Template folder is also one of the crucial folders because
it contains all our buildspec that handles all the CICD workflow. Hence, a
script folder contains a bash script used to make a call to our buildspec.
Sample parameter folder: The parameter file folder contains our
parameter for our first deployment. This file must be edited to fit your
deployment criteria and called when the Terraform apply command is used.
Step-by-step deployment
Following are the steps for deploying and configuring AWS CICD Pipeline
resources mentioned AWS CICD pipeline:
1. Create folder named chapter2 cd to the folder you just created:
mkdir chapter2

cd chapter2
2. Create three folders (modules, template, and sampleParameters) in chapter2
folder and confirm the folders are created using ls:
mkdir modules template sampleParameters
ls
3. Create 
main.tf, 
variables.tf, 
data.tf, 
locals.tf, 
variables.tf,
outputs.tf, and .gitignore files in chapter2 folder. The following files are
placed in the code repository for chapter2:
.gitignore 
data.tf 
locals.tf
main.tf 
variables.tf
output.tf
CD to the sampleParameter folder you created in Step 1.
4. Create cicdParams.tfvar:
The file cicdParams.tfvars is in the sampleParameter folder in chapter2
folder.
CD to the template folder you created in Step 1.
5. Create a new folder and name it scripts:
cd template/
mkdir scripts
6. Create Buildspec Validate YAML file buildspec_validate.yml.
The file buildspec_validate.yml is in the template folder in chapter2
folder.
7. Create Buildspec Plan YAML file buildspec_plan.yml.
The file buildspec_plan.yml is in the template folder in chapter2 folder.
8. Create Buildspec Destroy YAML file buildspec_destroy.yml.
The file buildspec_destroy.yml is in the template folder in chapter2 folder.

9. Create Buildspec Apply YAML file buildspec_apply.yml.
The file buildspec_apply.yml is in the template folder in chapter2 folder.
10. Create the BASH SCRIPT needed to drive the stages inside the pipeline.
Use the following command to create the file inside the scripts folder you
created in Step 5.
Name the file validation_script.sh:
nano scripts/validation_scripts.sh
The file validation_script.sh is in the template/script folder in chapter2
folder.
CD to modules folder you created in step 1.
11. Create the subfolders we need using the following command:
cd modules/
mkdir codebuild codecommit codepipeline iam-role 
kms s3
You can use the command below to create the file inside each folder in Step
7. This part is the last part, and you must be careful to avoid unnecessary
mistakes.
12. Create all the files needed inside the codebuild folder using the following
command.
All files below are created inside modules/codebuild/ folder in chapter2
folder:
main.tf
     nano codebuild/main.tf
variables.tf
nano codebuild/variables.tf
outputs.tf
cat codebuild/outputs.tf
13. Create all the files needed inside the codecommit folder using the following
command.

All files below are created inside modules/codecommit/ folder in chapter2
folder:
data.tf
nano codecommit/data.tf
main.tf
nano codecommit/main.tf
variables.tf
nano codecommit/variables.tf
outputs.tf
nano codecommit/output.tf
14. Create all the files needed inside the codepipeline folder using the
following command.
All files below are created inside modules/codepipeline/ folder in chapter2
folder:
main.tf
nano codepipeline/main.tf
variables.tf
nano codepipeline/variables.tf
outputs.tf
nano codepipeline/outputs.tf
15. Create all the files needed inside the iam-role folder using the following
command.
All files below are created inside modules/iam-role/ folder in chapter2
folder:
data.tf
 nano iam-role/data.tf
main.tf
nano iam-role/main.tf 

variables.tf
nano iam-role/variables.tf 
outputs.tf
nano iam-role/outputs.tf 
16. Create all the files needed inside the kms folder using the following
command.
All files below are created inside modules/kms/ folder in chapter2 folder:
data.tf
nano kms/data.tf
main.tf
nano kms/main.tf
variables.tf
nano kms/variables.tf 
outputs.tf
nano kms/outputs.tf
Creating state file remote storage using s3
Storing Terraform state files for continuous improvement and continuous
deployment:
1. Create all the files needed inside the s3 folder using the command below:
All files below are created inside modules/s3/ folder in chapter2 folder.
main.tf
nano s3/main.tf 
provider.tf
nano s3/provider.tf
variables.tf
nano s3/variables.tf 

outputs.tf
nano s3/outputs.tf
2. Now that you have created all the files and folders, you must run the
following commands from the root folder. If you receive any error, you
need to go back and validate that all the steps are followed.
From the terraform root folder, run the following command:
terraform init
terraform plan -var-
file=./sampleParameters/cicdParams.tfvars
terraform apply -var-
file=./sampleParameters/cicdParams.tfvars –auto-
approve
Validate deployment using AWS Console
After running the above command, you need to validate that all the resources were
deployed based on the terraform definition. The following bullets denote each
resource deployed with our codes:
CodeCommit
CodeBuild
CodePipeline
KMS
S3
IAM Roles
Note: Your AWS account is very important and may cause an error if you do
not configure your parameter files correctly.
Clean up the deployment:
terraform destroy -var-
file=./sampleParameters/cicdParams.tfvars –auto-approve
Challenges and considerations with Terraform

While Terraform offers a wide range of robust infrastructure provisioning and
management capabilities, it also has some limitations and constraints that users
should be aware of. One limitation is the lack of support for a number of certain
cloud provider features. At times, as cloud providers or vendors continually
update and release new features and services, Terraform may not immediately
support them. Additionally, complex, and provider-specific configurations may
also require custom workarounds or extensions, which can introduce additional
maintenance overhead.
Other Terraform limitations include learning curve. Terraform relies on specific
syntax, and this low-level syntax of the Terraform language is known as HCL.
Moreover, understanding Terraform’s inner workings, including state
management, resource dependencies, and remote backend for storing state files,
may require additional learning and experience.
Lastly, organizations with strict compliance and security requirements may also
face constraints when securely managing and storing Terraform state files, as they
may contain sensitive information about the resources.
Conclusion
Terraform is a defacto IaC programming language that works with multiple cloud
providers using the declarative and HCL on cloud API. Terraform integrates
seamlessly with other programming languages like Bash Script and Ansible.
CICD pipeline is very crucial to deploying code in a consistent and repeatable
manner. This chapter is the foundation for the automation journey, and you can
use this setup to create a production and enterprise pipeline on AWS.
After going through this chapter, you have completed the following so far, and we
will be using some of this setup in Chapter 3, IAM, Governance and Policies
Administration, and subsequent chapters:
Created CodeCommit
Created CodeBuild
Created CodePipeline
KMS
S3
IAM Roles

In the next chapter, you will learn about Identity Access Management (IAM),
governance, and policy administration. The chapter delves into setting up and
configuring IAM and policy using Terraform.
Multiple choice questions
1. What is the name of the AWS Code Repository?
a. CodeCommit
b. CodeDeploy
c. Code Build
d. CodePipeline
e. All of the Above
2. What storage can be used to store Terraform state files?
a. EBS
b. EFS
c. S3
d. Code Commit
e. All of the above
Answers
1. a
2. c

CHAPTER 3
IAM, Governance and Policies
Administration
Introduction
In the age of cloud computing, security and access management are critical
components to ensure the Confidentiality, Integrity, And Availability
(CIA) of resources. Amazon Web Services (AWS) offers a robust set of
tools and services to assist organizations manage access control, establish
governance policies, and enforce security measures. This chapter delves into
the AWS Identity and Access Management (IAM), governance, and
policy, providing a comprehensive understanding of these foundational
concepts and their practical application using Terraform as Infrastructure
as Code (IaC).
Meanwhile, governance is crucial for organizations to maintain control over
their AWS infrastructure, ensure compliance with regulations, and mitigate
potential risks. Using Terraform to implement strong governance practices,
organizations can achieve greater control, visibility, and accountability over
their AWS environment.
Finally, to meet various regulatory requirements and industry standards,
organizations must establish strong compliance and auditing capabilities
within their AWS infrastructure. IAM facilitates compliance management
and auditing through the integration with other AWS services such as
CloudTrail. Whether you are a citizen developer or a professional developer,

the insights and techniques presented in this chapter will empower you to
harness the full potential of AWS IAM and effectively govern your cloud
resources.
Structure
This chapter covers the following topics:
Overview of AWS IAM
Implementing identity Life Cycle
Creating SCP with Terraform
Objectives
This chapter provides a comprehensive understanding of AWS Identity and
Access Management (IAM), governance, and policy, and to equip readers
with the key knowledge and skill necessary to effectively manage access
control, establish governance framework, and enforce security measures
within their AWS infrastructure. The readers will understand the
fundamentals of AWS IAM, including its key components such as users,
groups, roles, and policies, comprehend the significance of governance in
AWS and learn how to establish account structures and implementing
Identity lifecycle management using Terraform IaC. As a reader you will be
able to develop, create, and manage IAM policies, including understanding
the policy language, writing policies using JSON syntax, and applying
conditions and functions. Lastly, this chapter introduces the reader to
implementing Role-Based Access Control (RBAC), least privilege
principles, resource-level permission, and zero-trust.
Overview of AWS IAM
AWS Identity Access Management (IAM) is a comprehensive web service
provided by Amazon Web Services (AWS) that allows Organization to
manage access to their AWS resources securely. IAM enables the creation
and management of users, groups, roles, along with the assignment of
permissions to control the level of access each entity has to various AWS
services and resources. IAM allows administrators to set up fine-grained

access controls and easily manage user authentication and authorization,
ensuring the security and integrity of their AWS environment.
IAM plays a vital role in ensuring the security, compliance, and efficient
access management of AWS resources. IAM enables the principle of least
privilege, allowing IAM administrators to grant users and resources the
permission they require to perform their tasks, reducing the risk of accidental
or intentional misuse. In addition, IAM provides centralized control over
user authentication and access management, simplifying the administration
process and ensuring consistent security policies across the entire AWS
services.
What is IAM
IAM is a web service from Amazon Web Services that provides centralized
control of AWS access. It allows you to manage users, security credentials
such as access keys, and permissions that control user access to AWS
resources.
AWS IAM allows creation and management of AWS users and groups and
use permissions to allow and deny their access to resources. This helps to
secure your AWS infrastructure and ensure that only authorized users have
access to the needed resources.
AWS IAM integrates with other AWS services, allowing you to use the same
AWS identity across a range of services, and to apply consistent access
policies. AWS IAM make it easier to manage and maintain security for your
AWS resources and helps you to comply with regulatory requirements.
IAM Architecture
In the next Figure 3.1, the IAM architecture is showcased:

Figure 3.1: IAM architecture diagram
AWS IAM key components
The key components of AWS IAM are as follows:
Users: Users are individuals who are assigned unique credentials to
access AWS cloud resources. Each user has specific permissions
assigned to them, which determine what actions they can perform on
AWS resources.
Groups: Groups are a collection of users; you can create groups and
assign permissions to the group. Group simplifies the management of
users and permissions.
Roles: Roles are synonymous to users, but they can be only assigned
to AWS resources, such as EC2 instances, VPC, or Lambda functions.
Roles is used to define the permissions that the resource can have and
assumed by trusted entities to access AWS resources.

Policies: Policies are JSON documents that define the permissions for
users, groups, and roles. They specify what actions denied and
allowed on specific AWS resources. Policies can be attached to IAM
entities or inherited through a role or group membership.
Access keys: Access keys consist of an access key ID and a secret
access key. They are used to authenticate applications or
programmatic access to AWS services using the AWS Command
Line (CLI), Terraform, or AWS SDKs.
Multi-Factor Authentication (MFA): MFA adds an extra layer of
security to IAM user’s account by requiring an additional
authentication factor, such as a one-time password generated by a
virtual or hardware MFA devices.
Following is the IAM user and group definition:
AWS IAM user
IAM user is unique identity within an AWS account, with specific
permissions to access AWS services and resources. For example, an IAM
user can be granted permission to perform specific actions in AWS, such as
launching an EC2 instance or S3 bucket. IAM users can be granted access to
resources through policies attached to the user, or by being added to groups
with specific permissions.
AWS IAM group
IAM Group is a collection of IAM users within an AWS account. IAM
group allows you to manage access and permissions for multiple IAM users
as a single unit.
When a group is created, you can assign IAM policies to the group that
specify what actions the group members are allowed to perform. The use of
IAM group simplifies the process of managing permission for multiple IAM
users and make it easier to maintain consistency in your security policies.
Establishing governance frameworks
Below are some significances of governance in AWS:

Control and compliance: Governance helps ensure that AWS
resources and services are used in a controlled and compliant manner.
It is used to establish policies, procedures, and controls to ensure
adherence to regulatory requirements, and internal policies.
Security: Governance helps enforce security and ensure that resources
are configured securely. It involves implementing measures such as
identity, encryption, access management, network security, and
monitoring to protect data and infrastructure from unauthorized access
and potential threats.
Risk mitigation: Governance helps in identifying and mitigating risks
associated with deployed AWS infrastructure. It involves conducting
risk assessments and implementing controls to address vulnerabilities.
Governance frameworks also facilitates regular audits and compliance
checks to ensure ongoing risk management.
Collaboration 
and 
accountability: 
Governance 
encourages
collaboration and accountability within organizations. It establishes
roles and responsibilities, promotes transparency, and provides
methodology for tracking and documenting actions.
AWS account structure
AWS enterprise account must be structured in a certain way to ensure proper
control and standards. The structure below is a typical account structure that
is created for most fortune 500 companies in the cloud, and for us to create a
governance needed to be compliant with various regulation.
The more reason why most companies structure their AWS account into this
format is because of the following reasons.
Security and regulations: If you are a company that deal with HIPAA
and PCI data, you are expected to meet certain security standards
outlined by the government and the regulation body.
Separation of resources: Companies with larger teams must group
their resources into logical order such as business unit or department.
Cost control: Billing is one of the reasons many companies create an
organization structure for their AWS cloud account.

AWS account structure segments
AWS account structure segments are:
AWS Account: This is the master account, and the company that hired
you already have this account.
Organizations: This is the root account.
Organization Unit: This is sub-account such as divisions or
departments. This is where we will apply Service Control Policies
(SCP).
Accounts: This is where we will have a different team managing their
environment at the granular level (Figure 3.2):

Figure 3.2: AWS Account Structure
Implementing identity Life Cycle Management
The IAM lifecycle management encompasses various stages, including user
provisioning, access management, and deprovisioning. Here are the key
aspects of IAM lifecycle management:

User provisioning: Creation: IAM allows you to create and provide
users with specific access permissions. You can define user attributes,
such as username, password, and optional metadata. Credential
Management: IAM supports the management of user credentials,
including passwords and access keys. Users can be assigned
permanent passwords or required to reset their passwords upon initial
login.
Multi-Factor 
Authentication 
(MFA): 
IAM 
facilitates 
the
configuration and enforcement of MFA, adding an extra layer of
security for user authentication.
Access management: Policies: IAM utilizes policies to define and
manage access permissions. You can create custom policies or use
predefined policies to grant or restrict access to AWS resources. Roles:
IAM enables the creation of roles to define sets of permissions that
can be assumed by trusted entities, such as AWS services or federated
users. Group Management: IAM allows you to organize users into
logical groups, simplifying the assignment and management of
permissions.
Permissions and authorization: Least Privilege: IAM encourages the
principle of least privilege by granting users only the permissions
necessary to perform their tasks. This minimizes the risk of
unauthorized access or accidental misuse. Fine-Grained Permissions:
IAM supports fine-grained access control using policies, allowing you
to define specific permissions at the level of individual AWS resources
or actions.
Account and resource management: Account-level Access: IAM
provides mechanisms for managing access to AWS account-level
resources and services. This includes managing permissions for
billing, support, and other administrative tasks.
Resource lagging: IAM supports the use of tags to categorize and
manage AWS resources. Tags can be used in policies to control access
based on resource attributes.

Deprovisioning and user lifecycle: Suspension and Deactivation:
IAM allows you to suspend or deactivate user accounts temporarily or
permanently. Suspended users cannot access AWS resources, while
deactivated accounts can be reactivated if needed.
Removal of access: When a user no longer requires access, IAM
provides the ability to remove or modify their permissions and revoke
their access keys.
Account Removal: IAM allows you to delete user accounts entirely,
including all associated permissions and access keys.
Automating service control policies
Before you can automate the policy, you need to plan and understand the
structure of how the organization functions. Following Figure 3.3 is the
account structure of AWS Cloud Automation LLC organization.

Figure 3.3: AWS Organization structure
Bash Script to create your own organization structure
As mentioned earlier, planning, and architecting organization’s structure is
one of the success criteria when organizations move to the cloud. Use the
following steps to create your own organization structure:
1. Create a file named create_OU.sh.
2. Copy the following content to the file:

#!/bin/bash
aws organizations create-organizational-unit -
-parent-id <Your-Account_id-Here> --name 
PCI_PLATFORM_OU
aws organizations create-organizational-unit -
-parent-id <Your-Account_id-Here> --name 
HIPAA_PLATFORM_OU
aws organizations create-organizational-unit -
-parent-id <Your-Account_id-Here> --name 
GDPR_PLATFORM_OU
aws organizations create-organizational-unit -
-parent-id <Your-Account_id-Here> --name 
GENERAL_PLATFORM_OU
aws organizations create-organizational-unit -
-parent-id <Your-Account_id-Here> --name 
IDENTITY_ENGINEERING_OU
aws organizations create-organizational-unit -
-parent-id <Your-Account_id-Here> --name 
SECURITY_ENGINEERING_OU
aws organizations create-organizational-unit -
-parent-id <Your-Account_id-Here> --name 
INFORMATION_TECHNOLOGY_OU
aws organizations create-organizational-unit -
-parent-id <Your-Account_id-Here> --name 
DATA_SCIENCE_ML_OU
aws organizations create-organizational-unit -
-parent-id <Your-Account_id-Here> --name 
ISO_27001_OU
aws organizations create-organizational-unit -
-parent-id <Your-Account_id-Here> --name 
SOC2_TYPEII_OU

3. Change the file ownership using this following command:
chmod u+x create_OU.sh
4. Execute the script using the following command:
./create_OU.sh
Types of AWS organization policies
Types of AWS organization policies are given as follows:
AI service opt-out policies: This should be enabled if you have
Intellectual Property (IP), confidential data, and secret data or
process that you do not want Amazon to use for their service
enhancement. Discuss this with your privacy team first before you
enable this policy.
Backup policies: This policy allows you to set up your organization
backup plan. You can have different backup plans for your databases,
middleware, and your web servers. You can also have separate backup
plans for production vs non-production AWS resources.
Service control policies: This policy is the most widely used policy
because it is used across many services used by organizations. Using
SCP policy helps an organization to govern its organization activities,
helps the organization stay within the guidelines of its access control.
Tag policies: Tag policies are used to standardize tags across resources
in your AWS Organization. Tags are used for a variety of things such
as billing, environment, and command execution. Please read more
about policies on AWS:
https://docs.aws.amazon.com/organizations/latest/userguide/orgs_
manage_policies.html
Bash script to enable your organization policies
The following bash script can be used to enable your own organization
policies. As mentioned above, all four types of AWS policies can be enabled
using the following bash script:

1. Create a file named enable_policy.sh.
2. Copy the following content to the file:
#!/bin/bash
aws organizations enable-policy-type --root-id 
<Your-Root-ID-HERE> --policy-type 
BACKUP_POLICY
aws organizations enable-policy-type --root-id 
<Your-Root-ID-HERE> --policy-type 
AISERVICES_OPT_OUT_POLICY
aws organizations enable-policy-type --root-id 
<Your-Root-ID-HERE> --policy-type 
SERVICE_CONTROL_POLICY
aws organizations enable-policy-type --root-id 
<Your-Root-ID-HERE> --policy-type TAG_POLICY
3. Change the file ownership using this following command:
chmod u+x enable_policy.sh
4. Execute the script using the following command:
./enable_policy.sh
Note: The User, Group, and Role created below is for the preparation
of our Organization Polices. There’s a detailed section created for IAM.
Policy-Based Access Control using Terraform IaC
Policy-based for IAM is a set of rules and syntax used to define and enforce
access controls within an organization’s digital ecosystem. Policy provides a
framework for management. Let us start creating the access control using
Terraform.
Create AWS user, group, and role using Terraform

This section is for you to use terraform to create several governance policy
as best practices for several OU within your AWS organization. Before you
get started, you need to create a user, group, and role before you can create
the policies. Please follow the steps below to implement the creation of user,
group, and a role that we need for our policies using terraform:
1. Create a file called foundation_iam.tf in chapter 2 folder.
2. Copy the following content to the file:
# Config
terraform {
  required_version = ">=0.12.0"
  required_providers {
    aws = ">= 2.0"
  }
}
provider "aws" {
  region = var.region
}
# Variables
variable "name" {
  description = "Name for prefixes (including 
tailing dash)"
  type        = string
  default     = "awscademo-"
}
variable "region" {
  description = "Region for AWS resources"

  type        = string
  default     = "us-east-1"
}
variable "tags-to-value" {
  type = map(string)
  default = {
    contact     = "odeyinka2001@yahoo.com"
    LifeCycle   = "infinity"
    deployed-by = "terraform"
    date-deployed = "02-01-2023"
  }
  description = "Map of mandatory tags"
}
# IAM
## Default Group
resource "aws_iam_group" "demo-user-group" {
  name = format("%sdemo-users", var.name)
}
resource "aws_iam_policy" "self-management-
policy" {
  name_prefix = substr(format("%suser-self-
management-policy", var.name), 0, 31)
  description = format(
    "Policy for user self management of pwd, 
mfa device for %s",

    var.name,
  )
  ### Remark: During copy paste the policy 
json from AWS policy generator
  ### one has to escape the HCL reserved "${" 
by a second $ to indicate the use of AWS 
variables
  ### Better way would be to use fucntions 
file() and templatefile with external stored 
policy json files
  policy = <<EOP
{
    "Statement": [
      {
        "Action": [
          "iam:ChangePassword",
          "iam:*LoginProfile",
          "iam:*AccessKey*",
          "iam:*SSHPublicKey*",
          "iam:ListUserPolicies",
          "iam:ListAttachedUserPolicies",
          "iam:ListGroupsForUser"
        ],
        "Effect": "Allow",
        "Resource": 
"arn:aws:iam:::user/$${aws:username}",

        "Sid": "AllowCredentialManagement"
      },
      {
        "Action": [
  "iam:ListAccount*",
          "iam:GetAccountSummary",
          "iam:GetAccountPasswordPolicy",
          "iam:ListUsers"
        ],
        "Effect": "Allow",
        "Resource": "*",
        "Sid": "AllowConsoleUsage"
      },
      {
        "Action": [
          "iam:CreateVirtualMFADevice",
          "iam:EnableMFADevice",
          "iam:ResyncMFADevice"
        ],
        "Effect": "Allow",
        "Resource": [
          
"arn:aws:iam:::mfa/$${aws:username}",

          
"arn:aws:iam:::user/$${aws:username}"
        ],
        "Sid": "AllowMfaCreateEnableResync"
      },
      {
        "Action": [
          "iam:ListMFADevices",
          "iam:ListVirtualMFADevices"
        ],
        "Effect": "Allow",
        "Resource": "*",
        "Sid": "AllowMfaFromConsole"
      },
      {
        "Action": [
          "iam:ListAccountAliases",
    "iam:ListUsers",
          "iam:ListVirtualMFADevices",
          "iam:GetAccountPasswordPolicy",
          "iam:GetAccountSummary"
        ],
        "Effect": "Allow",
        "Resource": "*",

        "Sid": "AllowAllUsersToListAccounts"
      },
      {
        "Action": [
          "iam:ChangePassword",
          "iam:CreateAccessKey",
          "iam:CreateLoginProfile",
          "iam:DeleteAccessKey",
          "iam:DeleteLoginProfile",
          "iam:GetLoginProfile",
          "iam:ListAccessKeys",
          "iam:UpdateAccessKey",
          "iam:UpdateLoginProfile",
          "iam:ListSigningCertificates",
          "iam:DeleteSigningCertificate",
          "iam:UpdateSigningCertificate",
          "iam:UploadSigningCertificate",
          "iam:ListSSHPublicKeys",
          "iam:GetSSHPublicKey",
          "iam:DeleteSSHPublicKey",
          "iam:UpdateSSHPublicKey",
          "iam:UploadSSHPublicKey"
        ],

        "Effect": "Allow",
        "Resource": 
"arn:aws:iam::*:user/$${aws:username}",
        "Sid": 
"AllowIndividualUserToSeeAndManageOnlyTheirOwn
AccountInformation"
      },
   {
        "Action": [
          "iam:CreateVirtualMFADevice",
          "iam:DeleteVirtualMFADevice",
          "iam:EnableMFADevice",
          "iam:ListMFADevices",
          "iam:ResyncMFADevice"
        ],
        "Effect": "Allow",
        "Resource": [
          
"arn:aws:iam::*:mfa/$${aws:username}",
          
"arn:aws:iam::*:user/$${aws:username}"
        ],
        "Sid": 
"AllowIndividualUserToViewAndManageTheirOwnMFA
"
      },

      {
        "Action": [
          "iam:DeactivateMFADevice"
        ],
        "Condition": {
          "Bool": {
            "aws:MultiFactorAuthPresent": 
"true"
          }
        },
        "Effect": "Allow",
        "Resource": [
          
"arn:aws:iam::*:mfa/$${aws:username}",
          
"arn:aws:iam::*:user/$${aws:username}"
        ],
        "Sid": 
"AllowIndividualUserToDeactivateOnlyTheirOwnMF
AOnlyWhenUsingMFA"
      }
    ],
    "Version": "2012-10-17"
  }
EOP

}
resource "aws_iam_group_policy_attachment" 
"self-management-attach" {
  group      = aws_iam_group.demo-user-
group.name
  policy_arn = aws_iam_policy.self-management-
policy.arn
  depends_on = [
    aws_iam_group.demo-user-group,
    aws_iam_policy.self-management-policy,
  ]
}
## Read only Group
resource "aws_iam_group" "readonly-user-group" 
{
  name = format("%sreadonly-users", var.name)
}
data "aws_iam_policy" "readonly-access-
managed-policy" {
  arn = 
"arn:aws:iam::aws:policy/ReadOnlyAccess"
}
resource "aws_iam_group_policy_attachment" 
"readonly-attach" {
  group      = aws_iam_group.readonly-user-
group.name

  policy_arn = data.aws_iam_policy.readonly-
access-managed-policy.arn
  depends_on = [
    aws_iam_group.readonly-user-group,
    data.aws_iam_policy.readonly-access-
managed-policy,
  ]
}
## Admin group
resource "aws_iam_group" "mfa-superadmin-
group" {
  name = format("%smfa-superadmins", var.name)
}
resource "aws_iam_role" "admin-role" {
  name_prefix          = substr(format("%smfa-
superadmins-role", var.name), 0, 31)
  assume_role_policy   = 
data.aws_iam_policy_document.admin-assume-
role-document.json
  max_session_duration = 43200 # 43200 seconds 
is the supported maximum, i.e. 12 hours
}
data "aws_iam_policy_document" "admin-policy-
document" {
  statement {
    actions = [
      "*",

    ]
    effect = "Allow"
    resources = [
      "*",
    ]
    condition {
        test     = "BoolIfExists"
        variable = 
"aws:MultiFactorAuthPresent"
        values = [
        "true"
        ]
      }
  }
}
resource "aws_iam_policy" "admin-policy" {
  name_prefix = substr(format("%sadmin-access-
policy", var.name), 0, 31)
  description = format("Permits full admin 
access for %s", var.name)
  path        = "/"
  policy      = 
data.aws_iam_policy_document.admin-policy-
document.json
}

resource "aws_iam_role_policy_attachment" 
"admin-role-attach" {
  role       = aws_iam_role.admin-role.name
  policy_arn = aws_iam_policy.admin-policy.arn
  depends_on = [
    aws_iam_role.admin-role,
    aws_iam_policy.admin-policy,
  ]
}
resource "aws_iam_policy" "assume-role-mfa-
policy" {
  depends_on  = [aws_iam_group.mfa-superadmin-
group]
  name_prefix = substr(format("%sassume-admin-
role-with-mfa-policy", var.name), 0, 31)
  path        = "/"
  description = format("Assume admin role with 
MFA only policy for %s", var.name)
  policy = <<EOP
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": "sts:AssumeRole",

            "Resource": "${aws_iam_role.admin-
role.arn}",
            "Condition": {
                "BoolIfExists": {
                    
"aws:MultiFactorAuthPresent": "true"
                }
            }
        }
    ]
}
EOP
}
resource "aws_iam_group_policy_attachment" 
"admin-assume-role-mfa-attach" {
  group      = aws_iam_group.mfa-superadmin-
group.name
  policy_arn = aws_iam_policy.assume-role-mfa-
policy.arn
  depends_on = [
    aws_iam_group.mfa-superadmin-group,
    aws_iam_policy.assume-role-mfa-policy,
  ]
}
data "aws_caller_identity" "current" {

}
data "aws_iam_policy_document" "admin-assume-
role-document" {
  statement {
    actions = [
      "sts:AssumeRole",
    ]
    effect = "Allow"
    principals {
      type        = "AWS"
      identifiers = 
["arn:aws:iam::${data.aws_caller_identity.curr
ent.account_id}:root"]
    }
  }
}
 
# Outputs
output "mfa-superadmin-group" {
  description = "The name of the IAM group for 
admins"
  value       = "${aws_iam_group.mfa-
superadmin-group.id}"
}
output "read-only-group" {

  description = "The name of the IAM group for 
read only access permissions"
  value       = "${aws_iam_group.readonly-
user-group.id}"
}
output "default-group" {
  description = "The name of the IAM group for 
everyone"
  value       = "${aws_iam_group.demo-user-
group.id}"
}  
3. The foundation_iam.tf will create the following groups (refer to
Figure 3.4):
Figure 3.4: Output of AWS iam.tf
4. The foundation_iam.tf will create the following roles:
Please note the following role because we are going to use it to
override any policies we create (refer to Figure 3.5):
Figure 3.5: Output of AWS MFA role
5. The foundation_iam.tf will create the following policies (refer to
Figure 3.6):

Figure 3.6: Output of AWS policy
Now that you have learnt how to enable different kinds of polices, we are
going to create several polices to help you govern you AWS Cloud Account.
Note: Your AWS account is very important and may cause an error if
you do not prepare your parameter files correctly.
Securing resources using AWS Policy
AWS policy serves as a powerful shield, enabling customers to fortify their
assets and protect them from potential vulnerabilities and unauthorized
access. The fine-grained access control can be used to establish
comprehensive security baseline as implemented below using all the types of
AWS governance policy.
AWS governance policies
When your company sign up for an AWS account, there are many reasons to
govern the usage of your cloud account. Billing, Standard, Security, and
regulation are top reasons why governance must be in place to manage the
usage of your cloud, one additional important reason is to put control and
standard in place to avoid chaos and instability that accompany lack of
governance.
AWS cloud governance includes people, process, and technology associated
with cloud resources. Below are the most common types of polies that will
get you started. Policy can be created in multiple ways, but for the sake of
this book, we are going to use both Terraform and JSON files to implement
our organization policy.
Backup policies
An AWS Backup policy is a set of guidelines that determine how you create,
store, and manage backups of your AWS resources. AWS Backup is a

critical component of your disaster recovery and data protection strategy and
helps ensure that you can quickly recover from data loss or corruption due to
hardware failures, software bugs, human error, or other unexpected
incidents.
AWS Backup Policy should contain the followings:
Data to be backed up: This is resources that need to be backed up,
such as EC2, RDS, etc.
Backup frequency: How often should the backup be taken, daily,
weekly, or monthly.
Data retention: Define how long the backup should be kept, and
when it should be deleted.
Backup window: What time of the day do you want the backup to run
for and how long do you want it to run for.
Restore and recovery: Determine who can restore from the backup.
By having a standard policy and well-defined backup policy in place, you
can ensure that your critical data is protected and that you have a reliable
disaster recovery plan in place. Following is the JSON file for you to setup
the backup policy at organization level. This can be used in your production
environment.
Create prod-db-backup-policy.json file
{
    "plans": {
        "prod-db-backup-plan": {
            "regions": {
                "@@assign": [
                    "us-east-2",
                    "us-west-1",
                    "us-west-2",

                    "us-east-1"
                ]
            },
            "rules": {
                "prod-db-daily-backup-rule": {
                    "lifecycle": {
                        
"move_to_cold_storage_after_days": {
                            "@@assign": "90"
                        }
                    },
                    "target_backup_vault_name": {
                        "@@assign": "prod-db-daily-
backup-vault"
                    },
                    "recovery_point_tags": {
                        "Restore": {
                            "tag_key": {
                                "@@assign": 
"Restore"
                            },
                            "tag_value": {
                                "@@assign": "True"
                            }

                        },
                        "Env": {
                            "tag_key": {
                                "@@assign": "Env"
                            },
                            "tag_value": {
                                "@@assign": "Prod"
                            }
                        },
                        "App": {
                            "tag_key": {
                                "@@assign": "App"
                            },
                            "tag_value": {
                                "@@assign": "DB"
                            }
                        }
                    },
                    "copy_actions": {
                        "arn:aws:backup:us-west-
2:$account:backup-vault:prod-db-daily-backup-
replication-vault": {
                            
"target_backup_vault_arn": {

                                "@@assign": 
"arn:aws:backup:us-west-2:$account:backup-
vault:prod-db-daily-backup-replication-vault"
                            },
                            "lifecycle": {}
                        }
                    }
                },
                "prod-db-monthly-backup-rule": {
                    "schedule_expression": {
                        "@@assign": "cron(0 5 25 * 
? *)"
                    },
                    "lifecycle": {
                        
"move_to_cold_storage_after_days": {
                            "@@assign": "180"
                        }
                    },
                    "target_backup_vault_name": {
                        "@@assign": "prod-db-daily-
backup-vault"
                    },
                    "recovery_point_tags": {
                        "App": {

                            "tag_key": {
                                "@@assign": "App"
                            },
                            "tag_value": {
                                "@@assign": "DB"
                            }
                        },
                        "Env": {
                            "tag_key": {
                                "@@assign": "Env"
                            },
                            "tag_value": {
                                "@@assign": "Prod"
                            }
                        },
                        "Restore": {
                            "tag_key": {
                                "@@assign": 
"Restore"
                            },
                            "tag_value": {
                                "@@assign": "True"
                            }

                        }
                    },
                    "copy_actions": {
                        "arn:aws:backup:us-west-
2:$account:backup-vault:prod-db-monthly-
replication-vault": {
                            
"target_backup_vault_arn": {
                                "@@assign": 
"arn:aws:backup:us-west-2:$account:backup-
vault:prod-db-monthly-replication-vault"
                            },
                            "lifecycle": {}
                        }
                    }
                }
            },
            "backup_plan_tags": {
                "App": {
                    "tag_key": {
                        "@@assign": "App"
                    },
                    "tag_value": {
                        "@@assign": "DB"
                    }

                },
                "Env": {
                    "tag_key": {
                        "@@assign": "Env"
                    },
                    "tag_value": {
                        "@@assign": "Prod"
                    }
                },
                "Backup": {
                    "tag_key": {
                        "@@assign": "Backup"
                    },
                    "tag_value": {
                        "@@assign": "True"
                    }
                }
            },
            "selections": {
                "tags": {
                    "prod-db-backup-resources": {
                        "iam_role_arn": {

                            "@@assign": 
"arn:aws:iam::$account:role/AWSBackupServiceRole"
                        },
                        "tag_key": {
                            "@@assign": "Backup"
                        },
                        "tag_value": {
                            "@@assign": [
                                "True"
                            ]
                        }
                    }
                }
            }
        }
    }
}
Create the backup policy for your organization using prod-db-backup-
policy.json file using the following command:
aws organizations create-policy --content 
file://prod-nondb-backup-policy.json --description 
"This is production for all servers with the 
exception of database governance policy for this 
organization" --name prod-nondb-backup-policy --
type BACKUP_POLICY

Attach the backup policy to the OU in your AWS organization using the
following command:
aws organizations attach-policy --policy-id 
<YOUR_POLICY_ID> --target-id <OU_ORGANIZATION-ID>
Verify the policy is created:
aws organizations list-policies-for-target --
target-id <OU_ORGANIZATION-ID> --filter 
BACKUP_POLICY
Note: I have created three additional backup policy for you to get you
started. dev-db-backup-policy.json, prod-nondb-backup policy.json,
dev-nondb-backup-policy.json.
Tag policies
AWS Tag policies are set of guidelines for creating, managing, and using
tags in AWS environment. These policies help organize and categorize AWS
resources and make it to track and manage AWS costs.
Tag policy can include mandatory tag requirements, tag values and formats,
and the key length restrictions. Tags can also be used to specify the roles and
permissions required to create, manage, and delete tags. AWS tags makes it
easier to manage tagging across AWS environment, and ensures that all
resources are properly tagged, making it easier to manage cost, identify cost,
track usage, and enforce security and compliance policies.
Creating and enforcing an AWS tag policy is an important step in optimizing
AWS environment, and can help you better manage your costs, security, and
compliance requirements.
Below is the JSON file for you to set up the tagging policy at organization
level. This can be used in your production environment.
Create general-tag-policy.json file:
{
    "tags": {

        "Env": {
            "tag_value": {
                "@@assign": [
                    "Prod",
                    "Prd",
                    "Production",
                    "DEV",
                    "Dev",
                    "dev",
                    "Staging",
                    "STAGING",
                    "QA",
                    "qa",
                    "pre-prod",
                    "PRE-PROD",
                    "preprod",
                    "PREPROD",
                    "TEST",
                    "Test",
                    "test",
                    "PoC",
                    "POC",
                    "Poc",

                    "poc",
                    "NPROD",
                    "nPROD",
                    "nProd",
                    "nprod",
                    "Nprod"
                ]
            }
        },
        "App": {
            "tag_value": {
                "@@assign": [
                    "DB",
                    "db",
                    "nondb",
                    "NONDB"
                ]
            }
        },
        "Backup": {
            "tag_value": {
                "@@assign": [
                    "True",

                    "true",
                    "TRUE",
                    "Yes",
                    "YES",
                    "yes"
                ]
            }
        },
        "CostCode": {
            "tag_value": {
                "@@assign": [
                    "hr001",
                    "mamashipper1",
                    "cloudalltechnologies",
                    "eng001",
                    "aiml001"
                ]
            }
        }
    }
 }
Create the tag policy for your organization using general-tag-policy.json
file using the following command:

aws organizations create-policy --content file://general-tag-
policy.json --description "This is general tag policy" --name 
general-tag-policy --type TAG_POLICY
Attach the tag policy to the OU in your AWS organization using the
following command:
aws organizations attach-policy --policy-id <YOUR_POLICY_ID> --
target-id <OU_ORGANIZATION-ID>
Verify the policy is created.
aws organizations list-policies-for-target --
target-id <OU_ORGANIZATION-ID> --filter TAG_POLICY
Service control policies
AWS Service Control Policy (SCP) is a policy that you can use in your
AWS organization to set fine-grained access controls for your AWS account
and the resources within. SCP defines the maximum permission that an
account can have within your organization. SCP can be used to restrict
access to AWS services, actions, and resources. For example, you can use
SCP to limit access to specific regions, VPCs or S3 bucket, or to set
password policy for your AWS organization.
SCP is enforced at the root level of your AWS organization and applies to all
member accounts in your AWS organization. SCP can be attached to
Organization Unit (OU) to define the policies for the accounts within that
OU, or you can attach it directly to member account to define the policies for
that specific account. SCP and other policies help you and your organization
to meet compliance requirements and reduce the risk of unauthorized access
to your AWS resources.
Note: SCP Policy is the most widely used policies among AWS policies.
Creating SCP with Terraform
In the following section, we will create SCP with Terraform:

1. Create a policy file to require MFA to stop EC2 and name it
require_mfa_to_stop_ec2.tf:
resource "aws_organizations_policy" "ec2Mfa-
ScpPolicy" {
  name = "require_ec2_tostop_mfa"
  description = "This SCP policy requires that 
multi-factor authentication (MFA) is enabled 
before a principal or root user can stop an 
Amazon EC2 instance. "
  type = "SERVICE_CONTROL_POLICY"
  content = <<POLICY
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Action": [
        "ec2:StopInstances",
        "ec2:TerminateInstances"
      ],
      "Resource": "*",
      "Effect": "Deny",
      "Condition": {
        "BoolIfExists": {
          "aws:MultiFactorAuthPresent": false
        }

      }
    }
  ]
}
POLICY
}
2. Create a policy file to prevent anyone from deleting KMS keys and
name it deny_kms_delete.tf:
resource "aws_organizations_policy" 
"DenyKmsDelete-ScpPolicy" {
  name = "deny_kms_delete_keys"
  description = "This SCP policy prevents 
users or roles in any affected account from 
deleting KMS keys, either directly as a 
command or through the console. "
  type = "SERVICE_CONTROL_POLICY"
  content = <<POLICY
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Action": [
        "kms:ScheduleKeyDeletion",
        "kms:Delete*"
      ],
      "Resource": "*",

      "Effect": "Deny"
    }
  ]
}
POLICY
}
Run the following command:
terraform init
Run the following command:
terraform plan
Run the following command:
terraform apply –auto-approve
AWS Compliance and Auditing using Terraform IaC
AWS Compliance and Auditing are crucial aspects of maintaining data
security and regulatory adherence in the cloud. AWS offers a comprehensive
set of compliance offerings and tools to help organizations meet their
regulatory requirements. AWS provides a secure infrastructure and a wide
range of services that are designed to address various compliance needs,
including data protection, privacy, and industry-specific regulations. With
AWS Policy automation, and least privilege through IAM, organizations can
benefit from a robust framework that includes built-in IAM security
controls, regular auditing of user access, and continuous monitoring to
ensure data integrity and confidentiality.
Auditing is an essential process for verifying and validating compliance in
AWS. AWS provides organizations with detailed audit trails and logs that
capture activity and changes within their cloud environment. These logs
enable organizations to track and review user actions, system events, and
resource modifications, allowing for comprehensive auditing and
accountability. Furthermore, AWS offers services such as AWS CloudTrail

and Amazon CloudWatch that provide real-time monitoring and logging
capabilities, enabling organizations to detect and respond to security
incidents promptly.
AWS compliance encompasses a wide range of regulations and standards,
including but not limited to HIPAA, GDPR, ISO 27001, PCI DSS, and
FedRAMP. To achieve compliance with these regulations, AWS employs a
shared responsibility model, where AWS is responsible for the security of
the cloud infrastructure, while customers are responsible for implementing
and configuring security controls within their own applications and
environments. AWS also offers various resources and compliance
documentation, including whitepapers, compliance reports, and reference
architectures, to assist organizations in understanding and implementing
necessary controls to meet their compliance obligations.
Following is the user creation policy automation:
Users’ automation using Terraform
Use the Terraform code below to automate the creation of the user. The file
is name create_iam_user.tf:
provider "aws" {
  region = "us-east-1"
}
locals {
  iam_users = [
    {
      name     = "user1",
   name     = "user1@gmail.com",
      password = "password1"
    },
    {

      name     = "user2",
   name     = "user2@gmail.com",
      password = "password2"
    },
    {
      name     = "user3",
   name     = "user3@gmail.com",
      password = "password3"
    }
  ]
}
resource "aws_iam_user" "newusers" {
  count = length(local.iam_users)
  name = local.iam_users[count.index].name
  tags = {
    Terraform   = "true"
    Environment = "dev"
  }
}
resource "aws_iam_access_key" "newusers" {
  count = length(local.iam_users)
  user = aws_iam_user.newusers[count.index].name
}

Best practices for IAM using Terraform IaC
Unlocking the true potential of the cloud requires more than just powerful
infrastructure; it requires a robust security framework that safeguards your
organization’s digital assets. In this rapidly evolving digital landscape, where
the magnitude of data breaches continues to grow, AWS IAM stands as
stalwart guardian of your cloud environment. Navigating the intricacies of
IAM can be akin to exploring a labyrinth of permission and policies. For
within this labyrinth lie the invaluable best practices that will fortify your
cloud fortress and empower your organization to wield the full might of
AWS security.
Following are the realm of AWS IAM best practices:
Access planning matrix table
Following Table 3.1 shows typical groups for a fortune 500 companies:
Account Group
Name
Account
Type
Comments
Automation
Engineer
Human
Acct.
Storage Engineer
Human
Acct.
Storage Admin
Human
Acct.
Security
Engineer
Human
Acct.
Security Admin
Human
Acct.
VPC Engineer
Human
Acct.
VPC Admin
Human
Acct.
EC2 Engineer
Human
Acct.
EC2 Admin
Human
Acct.
Network
Engineer
Human
Acct.

Account Group
Name
Account
Type
Comments
Network Admin
Human
Acct.
Identity Engineer
Human
Acct.
Identity Admin
Human
Acct.
EKS Engineer
Human
Acct.
EKS Admin
Human
Acct.
Database
Engineer
Human
Acct.
Database Admin
Human
Acct.
Data Engineer
Human
Acct.
Data Admin
Human
Acct.
Integration
Engineer
Human
Acct.
Integration
Admin
Human
Acct.
Application
Developer
Human
Acct.
Application
Support
Human
Acct.
Analytics
Engineer
Human
Acct.
Data Science
Engineer
Human
Acct.
SaaS Engineer
Human
Acct.
Big Data
Engineer
Human
Acct.
DevOps
Engineer
Human
Acct.
SA Automation
Service
Acct.
Service Account for Automation.

Account Group
Name
Account
Type
Comments
SA Developer
Service
Acct.
Service Account for Developer. The developer can use this for
interactive login.
Billing Engineer
Human
Acct.
Backup Engineer
Human
Acct.
SR Engineer
Human
Acct.
Cloud Engineer
Human
Acct.
Cloud Admin
Human
Acct.
Super Admin
Human
Acct.
This is the only group that can DELETE any Object, Services,
and Resources.
Table 3.1: Access Control Matrix
Create IAM group using Terraform
Use the Terraform code below to automate the creation of the IAM group.
The file is name create_iam_group.tf:
provider "aws" {
  region = "us-east-1"
}
resource "aws_iam_group" "storage-eng-group" {
  name = "Storage-Engineering-Group"
}
resource "aws_iam_group" "vpc-eng-group" {
  name = "VPC-Engineering-Group"
}
resource "aws_iam_group" "identity-eng-group" {

  name = "Identity-Engineering-Group"
}
resource "aws_iam_group" "eks-eng-group" {
  name = "EKS-Engineering-Group"
}
resource "aws_iam_group" "database-engineering-
group" {
  name = "Database-Engineering-Group"
}
resource "aws_iam_group_policy_attachment" 
"storage-eng-group_policy" {
  group = "${aws_iam_group.storage-eng-group.name}"
  policy_arn = 
"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess"
}
resource "aws_iam_group_policy_attachment" "vpc-
eng-group_policy" {
  group = "${aws_iam_group.vpc-eng-group.name}"
  policy_arn = 
"arn:aws:iam::aws:policy/AmazonEC2FullAccess"
}
resource "aws_iam_group_policy_attachment" 
"identity-eng-group_policy" {
  group = "${aws_iam_group.identity-eng-
group.name}"

  policy_arn = 
"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess"
}
resource "aws_iam_group_policy_attachment" "eks-
eng-group_policy" {
  group = "${aws_iam_group.eks-eng-group.name}"
  policy_arn = 
"arn:aws:iam::aws:policy/AmazonEC2FullAccess"
}
resource "aws_iam_group_policy_attachment" 
"Database-Engineering-Group_policy" {
  group = "${aws_iam_group.database-engineering-
group.name}"
  policy_arn = 
"arn:aws:iam::aws:policy/AmazonEC2FullAccess"
}
AWS IAM role
AWS IAM role is a method to grant permissions to AWS services,
applications, or to users that don’t normally have direct access to your AWS
resources. IAM role predefine what actions are allowed but does not have
any specific credentials associated with it.
IAM roles are useful for situations where you need to grant access to AWS
resources to a trusted entity, but you do not want to share your AWS
credentials.
Create IAM role using Terraform
Use the following Terraform code to automate the creation of the IAM
group. The file is name create_iam_role.tf:

provider "aws" {
  region = "us-east-2"
}
resource "aws_iam_role" "ec2-eng-role" {
  name = "ec2-eng-role"
  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Action = "sts:AssumeRole",
        Effect = "Allow",
        Principal = {
          Service = "ec2.amazonaws.com"
        }
      }
    ]
  })
}
resource "aws_iam_role" "lambda-eng-role" {
  name = "lambda-eng-role"
  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [

      {
        Action = "sts:AssumeRole",
        Effect = "Allow",
        Principal = {
          Service = "lambda.amazonaws.com"
        }
      }
    ]
  })
}
resource "aws_iam_role_policy_attachment" "ec2-eng-
role_policy" {
  role = "${aws_iam_role.ec2-eng-role.name}"
  policy_arn = 
"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess"
}
resource "aws_iam_role_policy_attachment" "lambda-
eng-role_policy" {
  role = "${aws_iam_role.lambda-eng-role.name}"
  policy_arn = 
"arn:aws:iam::aws:policy/AmazonEC2FullAccess"
}
Conclusion

AWS Organization Account is used to structure, organize, and standardize
AWS Organization Account. Organization Unit (OU) is used to manage
AWS account between multiple departments, the OU are used to simplify
Billing and control with AWS organization account.
IAM is a web service that provides secure control of access to AWS
resources, its used to manage user accounts and permissions for accessing
AWS services and resources.
Organization policies are used to set permissions at the root level to enforce
control and standards across AWS organization units. Polices are written in
JSON and can be created and managed through the AWS management
console or through the AWS CLI or API.
By completing Chapter 3 you have completed the following so far, and we
will be using some of this setup in subsequent chapters.
Organization Policy
Backup Policy
Tag Policy
Service Control Policy (SCP)
Config
IAM Users
IAM Groups
IAM Roles
It is obvious that big organizations need to adopt multiple tools before they
successfully practice IaC and DevOps. CICD plays a very important role in
transforming source code into a repeatable, reliable, and continuous
deployment.
AWS tools such as CodeCommit, CodePipeline, CodeBuild, CodeGuru,
Cloud9, CloudFormation, CloudShell, CDK, and CodeDeploy are part of
cloud automation tools that are used often to deliver quality software which
reduce or eliminates human errors.
By completing this chapter, you have completed the following so far, and we
will be using some of this setup in the subsequent chapters.

Leveraging some or all of these tools and technologies will accelerate
organization DevOps practices and automation journey. Organizations that
invested in automation are poised to deliver more quickly than their
counterparts.
In the next chapter, you will learn how to setup and configure most used
AWS storage using terraform as IaC provisioning tool.
Multiple choice questions
1. What is the purpose of AWS IAM?
a. Provide Secure Access to AWS Services
b. Provide Access to Websites
c. To manage Databases in AWS
d. To create AI for AWS Cloud
e. All of the above
2. What is the default policy attached to the root user in AWS
Account?
a. Administrative Policy
b. Backup Policy
c. Power User Access
d. Full Access Policy
e. All of the above
Answers
1. a
2. d

Join our book’s Discord space
Join the book’s Discord Workspace for Latest updates, Offers, Tech
happenings around the world, New Release and Sessions with the Authors:
https://discord.bpbonline.com

CHAPTER 4
Automating AWS Storage
Deployment and Configuration
Introduction
Automating AWS storage deployment and configuration using Terraform
Infrastructure as Code (IaC) is a crucial step towards achieving efficient
and scalable cloud infrastructure management. With the rapid growth of
cloud computing and the increasing complexity of storage requirements, it
has become important for organizations to automate their storage
provisioning and configuration processes.
Manual storage management in AWS can be time-consuming and error-
prone process. Setting up storage resources manually involves navigating
through the AWS Management Console, clicking through various menus,
and configuring settings and features individually. This approach is not only
tedious but also full of human prone errors, such as misconfigurations and
inconsistencies. Automating storages deployment and configuration using
Terraform can alleviate these challenges and provide a more efficient and
reliable solution.
Hence, by leveraging Terraform capabilities, organizations can automate
AWS storage provisioning and configuration, ensuring consistent and
reliable deployments. This chapter focuses on Amazon S3, Amazon EBS
volumes, and EFS deployment and configuration using Terraform IaC.

Structure
This chapter will have the following topics:
Overview of AWS Storage
Amazon Simple Storage Service
Amazon Elastic Block Store
Amazon Elastic File System
Amazon FSx
Amazon Glacier
Amazon S3 Glacier
Amazon Storage Gateway
Objectives
The objective of this chapter is to provide a comprehensive understanding of
how to automate the deployment and configuration of various Amazon
storage services using Terraform. By the end of this chapter, readers will
have gained the knowledge and practical skills necessary to define,
provision, and manage storage deployment efficiently and consistently using
Terraform. Readers will gain a thorough understanding of deploying
Amazon S3, EBS volume, and EFS file system using Terraform IaC.
Throughout this chapter, readers will follow practical examples and hands-
on exercises that showcase the automation of various AWS storage services
using Terraform.
Overview of AWS Storage
Amazon Web Services (AWS) provides a wide range of storage solutions
tailored to meet diverse business storage needs. AWS offers a
comprehensive suite of storage services, from simple file storage to high-
performance databases. Each storage type has its own unique characteristics,
enabling users to effectively store, retrieve, and manage data in the cloud.
Overview of key AWS storage types

AWS storage types offer a wide array of options to meet the diverse storage
needs of modern applications, making it easier for businesses to select the
right storage solution that aligns with their specific requirements and cost
considerations.
Amazon S3 (Simple Storage Service): Amazon S3 is an object
storage service designed for scalability, durability, and high
availability. It allows users to store and retrieve data from anywhere
on the web. S3 is ideal for storing unstructured data, such as images,
videos, backups, log files, and static web content. It also offers
different storage classes, including Standard, Intelligent-Tiering,
Glacier, and others, each catering to specific access and cost
requirements.
Amazon EBS (Elastic Block Store): Amazon EBS provides
persistent block-level storage volumes that can be attached to Amazon
EC2 instances. EBS volumes are suitable for transactional databases,
applications requiring low-latency access to data, and boot volumes.
Users can provision and choose between different volume types, such
as General Purpose SSD, Provisioned IOPS SSD, and Magnetic, based
on performance and cost needs.
Amazon EFS (Elastic File System): Amazon EFS offers scalable and
fully managed file storage for EC2 instances. It supports the Network
File System (NFS) protocol, enabling multiple instances to access the
same file system simultaneously. EFS is suitable for workloads that
require shared access to files, like content management systems, web
servers, and big data analytics.
Amazon RDS (Relational Database Service): Amazon RDS
simplifies the deployment and management of relational databases,
including popular ones like MySQL, PostgreSQL, Oracle, and SQL
Server. It provides automated backups, automatic software patching,
and high availability options, allowing developers to focus on
application development rather than database administration.
Amazon DynamoDB: DynamoDB is a fully managed NoSQL
database service that offers fast and predictable performance with

seamless scalability. It is ideal for applications that require low-latency
data access at any scale, such as gaming, IoT, and real-time analytics.
Amazon Redshift: Amazon Redshift is a fully managed data
warehousing service designed for high-performance analysis of large
datasets. It uses columnar storage and parallel query execution to
deliver fast query performance on massive amounts of data, making it
suitable for data warehousing, business intelligence, and reporting
workloads.
Amazon Glacier: Amazon Glacier is a secure, durable, and low-cost
storage service designed for long-term data archival. It is ideal for data
that is infrequently accessed but requires long-term retention for
compliance or regulatory reasons.
AWS Storage Gateway: AWS Storage Gateway is a hybrid cloud
storage service that enables seamless integration between on-premises
environments and AWS cloud storage. It supports file, volume, and
tape gateways, allowing businesses to extend their existing IT
infrastructure to the cloud.
Amazon FSx (File System): Amazon FSx provides fully managed file
systems compatible with popular file protocols like Windows File
Server and Lustre for high-performance computing workloads. It
simplifies the deployment and management of shared file storage in
the AWS environment, reducing administrative overheads.
AWS Snow Family: The AWS Snow Family consists of physical
devices designed to transfer large amounts of data into and out of the
AWS cloud securely and efficiently. This family includes Snowcone,
Snowball, and Snowmobile, each offering different storage capacities
to address diverse data migration needs.
AWS DataSync: AWS DataSync is a data transfer service that
simplifies and accelerates moving data between on-premises storage
and AWS storage services. It can handle large-scale data transfers and
works with Amazon S3, Amazon EFS, and Amazon FSx.
Lastly, AWS’s storage offerings provide flexibility, scalability, and cost-
effectiveness, empowering organizations to build robust and reliable cloud-

based storage solutions. Each service has its unique set of features,
performance characteristics, and use cases, enabling businesses to select the
most suitable storage solution based on their specific requirements.
Benefit of AWS storage Services
AWS offers a wide range of storage services that cater to different needs and
use cases. Here are some benefits of AWS storage services:
Scalability: AWS storage services are designed to be highly scalable,
allowing you to easily accommodate growing storage requirements.
Durability and reliability: AWS provides high durability and
reliability for your data through replication and redundancy across
multiple data centers. This ensures that your data is safe from
hardware failures or disasters, reducing the risk of data loss.
Data security: AWS employs various security measures to protect
your data, including encryption at rest and in transit, access controls,
and network firewalls. This helps you comply with data privacy
regulations and safeguards your data from unauthorized access.
Cost-effectiveness: With AWS storage services, you only pay for
what you use, which means you can avoid upfront capital expenses.
Additionally, AWS offers various pricing models, such as pay-as-you-
go or reserved capacity, allowing you to optimize costs based on your
storage needs.
Global accessibility: AWS has data centers spread across multiple
geographic regions, enabling you to store and access data from
locations around the world. This distributed infrastructure also
improves data access latency, ensuring a better user experience for
your customers.
Elasticity: Many AWS storage services, such as Amazon S3, offer
elasticity, meaning they automatically adjust to accommodate
changing workloads. This makes it easier to handle sudden spikes in
demand without manual intervention.
Integration with other AWS services: AWS storage services
seamlessly integrate with a vast ecosystem of other AWS services,

such as computing, analytics, and databases. This integration
simplifies application development and enables you to build
sophisticated solutions efficiently.
Versatility: AWS offers a variety of storage options, including object
storage (Amazon S3), block storage (Amazon EBS), file storage
(Amazon EFS), and more. This flexibility allows you to choose the
right storage service for each specific use case.
Easy data transfer and migration: AWS provides tools and services
that make it easy to transfer data to and from AWS storage services, as
well as perform data migrations between different storage options.
Automated backup and archiving: AWS storage services often come
with built-in features for automated backups and long-term archiving.
This simplifies data management and ensures your data is protected
and accessible for compliance or historical purposes.
Content delivery: AWS offers Amazon CloudFront, a content
delivery network (CDN) that works in conjunction with storage
services like S3, enabling you to deliver content to users with low
latency and high data transfer speeds.
Amazon Simple Storage Service
Amazon Simple Storage Service (S3), the epitome of object storage
solutions! Unveil the marvels of this versatile and scalable cloud-based
service that revolutionizes data management in the digital age.
At its core, Amazon S3 operates on the principle of simplicity. It offers a
seamless experience, providing a straightforward interface that makes
interactions as smooth as a summer breeze. Uploading, storing, and
managing data has become a breeze, even for those new to the realm of
cloud storage.
Amazon S3 breaks the shackles of conventional storage constraints.
Embracing the power of metadata, every object in S3 gains a new dimension
- it becomes "self-aware." This self-descriptive nature empowers users to
attach custom metadata, making data management smarter, more efficient,
and tailored to specific applications.

In conclusion, Amazon S3 rises as an iconic figure in the realm of object
storage. It combines simplicity, adaptability, and security to present an
unrivalled cloud storage solution.
Understanding of S3 bucket, objects, and access controls
Amazon S3 is a widely used cloud storage service provided by Amazon
Web Services (AWS).
S3 Buckets: Virtual Containers of Objects: In the context of Amazon
S3, imagine buckets as virtual containers or directories that can hold
an unlimited number of objects. Just like real-world buckets, they
provide a means to organize and store data. However, unlike
traditional file systems, S3 buckets are globally unique, identified by
their names and are part of a flat namespace. Each AWS account can
have multiple buckets, making them excellent for segregating data and
facilitating access control.
S3 Objects: Immutable Data Entities S3 objects are individual data
entities that reside within S3 buckets. These objects can store any type
of data, such as images, videos, documents, and more. One unique
aspect of S3 objects is their immutability. Once an object is created
and stored in S3, it cannot be modified or changed directly. Instead, to
“update” an object, a new version with a different key or version ID is
created, promoting data integrity and versioning.
S3 Access Controls: Principle of Least Privilege Access controls in
S3 are a critical aspect of securing data. A unique understanding of S3
access controls is the “Principle of Least Privilege.” This principle
dictates that each entity, whether it’s an IAM (Identity and Access
Management) user or an AWS service, should only be granted the
minimum permissions necessary to perform its intended tasks. This
approach reduces the attack surface and mitigates the risk of data
breaches or accidental data exposure.
AWS offers various mechanisms for controlling access to S3 objects and
buckets, including:
Bucket Policies: JSON-based policies attached to buckets to manage
access for multiple users and services at the bucket level.

Access Control Lists (ACLs): Fine-grained permissions attached
directly to individual objects for specific users or groups.
IAM Policies: IAM policies are attached to IAM users, groups, or
roles to grant them permissions to interact with S3 resources.
S3 Object Ownership: By default, the creator of an S3 object is its
owner, and they have full control over the object, but ownership can
be transferred.
Use cases
Amazon S3 (Simple Storage Service) is a highly versatile and scalable
object storage service offered by Amazon Web Services. Its flexibility makes
it suitable for a wide range of use cases across various industries.
Here are some Amazon S3 use cases:
Data Backup and Archiving: S3 is commonly used for secure and
reliable data backup and archiving. Businesses can store their critical
data in S3 buckets, ensuring durability and easy retrieval when
needed. It provides an affordable and efficient solution for long-term
data retention.
Big Data Analytics and Data Lake: S3 is a popular choice as a data
lake storage for big data analytics. Organizations can ingest, store, and
analyze vast amounts of data using tools like AWS Glue, Amazon
Athena, or Amazon Redshift Spectrum, without the need to manage
the underlying storage infrastructure.
Static website hosting: S3 can be used to host static websites by
storing HTML, CSS, JavaScript, and media files in S3 buckets. It
offers cost-effective and scalable hosting solutions for websites that do
not require server-side processing.
Content Delivery Network (CDN) Origin: S3 can serve as an origin
for a Content Delivery Network like Amazon CloudFront. By storing
static assets in S3, organizations can distribute content globally with
low-latency and high-speed access.

Media storage and distribution: S3 is well-suited for storing and
distributing media content like images, videos, and audio files. It
provides the necessary scalability and reliability required for media
streaming applications.
Data sharing and collaboration: S3 buckets can be used as a
centralized repository for data sharing and collaboration across teams
or even between different organizations. Access controls and policies
ensure data security and restrict access as needed.
IoT data storage: Internet of Things (IoT) devices can generate
massive amounts of data. S3 serves as a reliable storage option for
storing IoT-generated data, which can later be analyzed or used for
real-time processing.
Disaster recovery: S3 can be part of a comprehensive disaster
recovery strategy. Organizations can replicate critical data to S3
buckets in different AWS regions to ensure data availability in case of
a disaster.
Machine Learning model storage: Data scientists and machine
learning engineers use S3 to store and version machine learning
models, datasets, and training artifacts. This facilitates collaboration
and reproducibility in machine learning workflows.
Application data storage: S3 can serve as a storage backend for
applications, especially those that involve user-generated content like
photo-sharing apps, document management systems, and social media
platforms.
Log storage and analysis: AWS services like Amazon CloudTrail and
Amazon CloudWatch can store logs directly in S3. Organizations can
then use tools like Amazon Athena or Amazon QuickSight for log
analysis and monitoring.
Compliance and regulatory requirements: S3’s robust security
features, encryption options, and access controls make it a suitable
choice for industries with strict compliance and regulatory
requirements, such as healthcare and finance.

Automation using Terraform
When it comes to provisioning S3 buckets with Terraform, it means defining
the configuration for an S3 bucket in the Terraform code and then applying
that code to create and set up the bucket within the cloud provider. Migration
is one of the big use cases of cloud computing and S3 buckets play an
important role in website or data migration.
1. Below is the step by step to automate website migration using S3
buckets, and we also hosted the website in S3. Only host low traffic
website in S3 bucket and not a medium or heavy traffic website. Use
the code below to create provider.tf file and run terraform init
command:
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 4.0"
    }
  }
}
provider "aws" {
  region = "us-east-2"
}
2. Use the following code to create s3.tf file. and run terraform init
command. We used the code below to setup two buckets. The first
bucket is used to setup the root domain, and the other bucket is for the
subdomain with www in front:
resource "aws_s3_bucket" 
"awscloudautomationbook_com" {

  bucket = "awscloudautomationbook.com"
}
resource "aws_s3_bucket" 
"www_awscloudautomationbook_com" {
  bucket = "www.awscloudautomationbook.com"
}
resource "aws_s3_bucket_cors_configuration" 
"awscloudautomationbook_com" {
  bucket = 
aws_s3_bucket.awscloudautomationbook_com.id
  cors_rule {
    allowed_headers = ["Authorization", 
"Content-Length"]
    allowed_methods = ["GET", "POST"]
    allowed_origins = 
["https://awscloudautomationbook.com"]
    max_age_seconds = 3000
  }
}
resource "aws_s3_bucket_website_configuration" 
"awscloudautomationbook_com" {
  bucket = 
aws_s3_bucket.awscloudautomationbook_com.bucke
t
  index_document {
    suffix = "index.html"
  }

  error_document {
    key = "404/error.html"
  }
}
resource "aws_s3_bucket_website_configuration" 
"www_awscloudautomationbook_com" {
  bucket = 
aws_s3_bucket.www_awscloudautomationbook_com.b
ucket
  redirect_all_requests_to {
    host_name = "awscloudautomationbook.com"
  }
}
resource "aws_s3_bucket_policy" 
"awscloudautomationbook_com_allow_public_acces
s" {
  bucket = 
aws_s3_bucket.awscloudautomationbook_com.bucke
t
  policy = 
data.aws_iam_policy_document.awscloudautomatio
nbook_com_allow_public_access.json
}
resource "aws_s3_bucket_policy" 
"www_awscloudautomationbook_com_allow_public_a
ccess" {
  bucket = 
aws_s3_bucket.www_awscloudautomationbook_com.b

ucket
  policy = 
data.aws_iam_policy_document.www_awscloudautom
ationbook_com_allow_public_access.json
}
data "aws_iam_policy_document" 
"awscloudautomationbook_com_allow_public_acces
s" {
  statement {
    principals {
      type        = "AWS"
      identifiers = ["*"]
    }
    actions   = ["s3:GetObject"]
    resources = 
["${aws_s3_bucket.awscloudautomationbook_com.a
rn}/*"]
  }
}
data "aws_iam_policy_document" 
"www_awscloudautomationbook_com_allow_public_a
ccess" {
  statement {
    principals {
      type        = "AWS"
      identifiers = ["*"]
    }

    actions   = ["s3:GetObject"]
    resources = 
["${aws_s3_bucket.www_awscloudautomationbook_c
om.arn}/*"]
  }
}
3. Use the following code to create .gitignore file.
4. The .gitignore is to prevent you from uploading the terraform state
to our code commit when we push the code to the repository.
# Compiled files
*.tfstate
*.tfstate.backup
# Module directory
.terraform
.idea
*.iml
*.ini
5. Create a config folder using mkdir config command.
6. Create the following four config files and save it in the config folder.
We should aim to create separate state files for different environment
for easy management:
demo.confg
bucket  = "s3-awscloudautomation-demo"
dev.confg
bucket  = "s3-awscloudautomation-dev"

staging.confg
bucket  = "s3-awscloudautomation-staging"
prod.confg
bucket  = "s3-awscloudautomation-prod"
7. Create a remote_state.tf file with the code below: State file knows
what bucket to use at the backend based on the config folder. Now use
this 
command 
terraform 
init 
--backend-
config=config/demo.config to re initialize the folder:
terraform {
  backend "s3" {
  region = "us-east-2"
  key    = "demo-statefiles"
  }
}
8. Create and setup the domain in AWS Route 53 using terraform code
below. See the AWS instruction to learn more on how to transfer
domain:
resource "aws_route53_zone" 
"awscloudautomationbook_com" {
  name = "awscloudautomationbook.com"
}
resource "aws_route53_record" 
"awscloudautomationbook_com-a" {
  zone_id = 
aws_route53_zone.awscloudautomationbook_com.zo
ne_id

  name    = "awscloudautomationbook.com"
  type    = "A"
  alias {
    name                   = 
aws_cloudfront_distribution.root_s3_distributi
on.domain_name
    zone_id                = 
aws_cloudfront_distribution.root_s3_distributi
on.hosted_zone_id
    evaluate_target_health = false
  }
}
resource "aws_route53_record" 
"www_awscloudautomationbook_com-a" {
  zone_id = 
aws_route53_zone.awscloudautomationbook_com.zo
ne_id
  name    = "www.awscloudautomationbook.com"
  type    = "A"
  alias {
    name                   = 
aws_cloudfront_distribution.www_s3_distributio
n.domain_name
    zone_id                = 
aws_cloudfront_distribution.www_s3_distributio
n.hosted_zone_id
    evaluate_target_health = false
  }

}
You already have website files awscloudautomation.com and they
comprise of two files index.html and error.html:
terraform plan
terraform apply –approve-auto
9. Run the command below to move the files to both S3:
aws s3 sync awscloudautomation.com/ 
s3://awscloudautomationbook.com –delete
aws s3 sync awscloudautomation.com/ 
s3://www.awscloudautomationbook.com –delete
10. Create and setup the SSL Certificates using the terraform code below:
Create ssl.tf. Validation Method could be Email or DNS.
resource "aws_acm_certificate" 
"ssl_certificate" {
  domain_name               = 
"awscloudautomationbook.com"
  subject_alternative_names = 
["*.awscloudautomationbook.com"]
  validation_method         = "EMAIL"
  lifecycle {
    create_before_destroy = true
  }
}
resource "aws_acm_certificate_validation" 
"cert_validation" {
  certificate_arn = 
aws_acm_certificate.ssl_certificate.arn

}
11. Create and setup the CloudFront using the terraform code below:
Create cloudfront.tf
resource "aws_cloudfront_distribution" 
"root_s3_distribution" {
  origin {
    domain_name = 
aws_s3_bucket.awscloudautomationbook_com.websi
te_endpoint
    origin_id   = 
"S3-.awscloudautomationbook.com"
    custom_origin_config {
      http_port              = 80
      https_port             = 443
      origin_protocol_policy = "http-only"
      origin_ssl_protocols   = ["TLSv1", 
"TLSv1.1", "TLSv1.2"]
    }
  }
  enabled             = true
  is_ipv6_enabled     = true
  default_root_object = "index.html"
  aliases             = 
["awscloudautomationbook.com"]
  custom_error_response {
    error_caching_min_ttl = 0

    error_code            = 404
    response_code         = 200
    response_page_path    = "/404/error.html"
  }
  default_cache_behavior {
    allowed_methods  = ["GET", "HEAD"]
    cached_methods   = ["GET", "HEAD"]
    target_origin_id = 
"S3-.awscloudautomationbook.com"
    forwarded_values {
      query_string = false
      cookies {
        forward = "none"
      }
    }
    viewer_protocol_policy = "redirect-to-
https"
    min_ttl                = 31536000
    default_ttl            = 31536000
    max_ttl                = 31536000
    compress               = true
  }
  restrictions {
    geo_restriction {

      restriction_type = "none"
    }
  }
  viewer_certificate {
    acm_certificate_arn      = 
aws_acm_certificate_validation.cert_validation
.certificate_arn
    ssl_support_method       = "sni-only"
    minimum_protocol_version = "TLSv1.1_2016"
  }
}
resource "aws_cloudfront_distribution" 
"www_s3_distribution" {
  origin {
    domain_name = 
aws_s3_bucket.www_awscloudautomationbook_com.w
ebsite_endpoint
    origin_id   = "S3-
www.awscloudautomationbook.com"
    custom_origin_config {
      http_port              = 80
      https_port             = 443
      origin_protocol_policy = "http-only"
      origin_ssl_protocols   = ["TLSv1", 
"TLSv1.1", "TLSv1.2"]
    }

  }
  enabled         = true
  is_ipv6_enabled = true
  aliases         = 
["www.awscloudautomationbook.com"]
  default_cache_behavior {
    allowed_methods  = ["GET", "HEAD"]
    cached_methods   = ["GET", "HEAD"]
    target_origin_id = "S3-
www.awscloudautomationbook.com"
    forwarded_values {
      query_string = true
      cookies {
        forward = "none"
      }
      headers = ["Origin"]
    }
    viewer_protocol_policy = "allow-all"
    min_ttl                = 0
    default_ttl            = 86400
    max_ttl                = 31536000
  }
  restrictions {
    geo_restriction {

      restriction_type = "none"
    }
  }
  viewer_certificate {
    acm_certificate_arn      = 
aws_acm_certificate_validation.cert_validation
.certificate_arn
    ssl_support_method       = "sni-only"
    minimum_protocol_version = "TLSv1.1_2016"
  }
}
12. Now run the two commands below to finish the process of setup after
you have validated the domain transfer via the email:
terraform plan
terraform apply –approve-auto
13. Push your code to a new branch in our CodeCommit in chapter4. git
push 
https://git-codecommit.us-east-
1.amazonaws.com/v1/repos/awsca-demo-repo.
Amazon Elastic Block Storage
Amazon Elastic Block Storage is a highly flexible and scalable block-level
storage service offered by Amazon Web Services. It is designed to provide
durable and persistent storage for Amazon Elastic Compute Cloud
(Amazon EC2) instances. EBS allows users to create and attach storage
volumes to EC2 instances, enabling them to store data independent of the
EC2 instance’s lifecycle.
Below is the key features of EBS:
Highly reliable: EBS volumes are designed for durability and
availability. Data is automatically replicated within the same

Availability Zone (AZ) to protect against hardware failures.
Additionally, snapshots can be created to back up the data, offering
point-in-time recovery options.
Flexible storage types: EBS offers a range of volume types to cater to
different workloads. These include:
General Purpose (SSD): Suitable for a broad range of workloads,
providing a balance of price and performance.
Provisioned IOPS (SSD): Optimized for I/O-intensive applications
that require high and consistent performance.
Cold HDD: Ideal for infrequently accessed, throughput-intensive
workloads.
Throughput Optimized HDD: Suitable for big data and data
warehousing applications requiring frequent data access.
EBS Snapshots: Users can create point-in-time snapshots of their
EBS volumes, allowing for data backup, replication, and migration.
These snapshots are stored in Amazon S3, providing an additional
layer of data protection.
Scalability: EBS volumes can be easily resized to meet changing
storage requirements. Users can scale up or down without any
disruption to their applications.
High Performance: Depending on the chosen volume type, EBS can
provide low-latency and high-throughput storage, ensuring optimal
performance for various workloads.
Encryption: EBS volumes can be encrypted using AWS Key
Management Service (KMS) to ensure data security and compliance
with industry regulations.
Elasticity: EBS volumes can be attached and detached from EC2
instances on-the-fly, giving users the flexibility to adapt to changing
infrastructure needs.
Integration with AWS Services: EBS seamlessly integrates with
other AWS services, such as Amazon RDS, Amazon EMR, and AWS

Lambda, enhancing the capabilities and performance of these services.
Snapshot-Based Replication: EBS snapshots can be used to replicate
data across different regions, enabling disaster recovery and reducing
downtime in case of failures.
Cost-Effective: EBS offers a pay-as-you-go pricing model, where
users are charged based on the storage resources consumed, making it
cost-effective for both small-scale and enterprise applications.
Use cases
Amazon Elastic Block Store (EBS) is a highly versatile and scalable block
storage service provided by Amazon Web Services (AWS). It offers several
unique use cases that cater to various requirements.
Here are some distinctive and innovative Amazon EBS use cases:
High-performance databases: EBS can be utilized as storage for
high-performance databases, such as Oracle, Microsoft SQL Server, or
MySQL. By using Provisioned IOPS (Input/Output Operations Per
Second) volumes, you can ensure low-latency and consistent I/O
performance, which is crucial for demanding database workloads.
Real-time analytics: For businesses requiring real-time data
processing and analytics, EBS can play a significant role. It enables
you to efficiently store, access, and process large volumes of data
generated by analytics platforms like Amazon Redshift or Apache
Spark.
Machine Learning model training: Training machine learning
models often requires large-scale storage with high I/O throughput.
EBS can serve as a reliable and scalable storage solution to store
training datasets and models. The data can be accessed and processed
by instances running ML frameworks like TensorFlow, PyTorch, or
Apache MXNet.
Content Management Systems (CMS): EBS can be used to store
media files, documents, and other assets in content management
systems. This ensures that the content remains persistent and

accessible across different instances, allowing for easy scaling and
content distribution.
High-Performance Computing (HPC) workloads: EBS can be
integrated into HPC environments to provide fast and scalable storage
for applications like computational fluid dynamics simulations,
seismic analysis, and molecular modeling. EBS offers high bandwidth
and throughput to handle the massive data requirements of HPC
workloads.
Financial applications: In the finance sector, low-latency and high-
throughput storage are crucial for processing vast amounts of real-time
data. EBS can be used to power trading platforms, risk analysis
systems, and financial databases, where data availability and reliability
are paramount.
Disaster Recovery (DR) solutions: Amazon EBS snapshots can be
used effectively in disaster recovery scenarios. By taking regular
snapshots of your EBS volumes and storing them in Amazon S3, you
can create point-in-time backups and restore your data quickly in case
of a failure or disaster.
Video editing and rendering: EBS can be used to store video files in
video editing applications and facilitate smooth video rendering
processes. As video editing often requires working with large files,
using EBS can provide the necessary storage capacity and
performance.
Application versioning and testing: When developing and testing
applications, you may need to maintain different versions of your
application and its associated data. EBS snapshots can serve as a
convenient way to create and manage versioned backups, making it
easier to roll back or test different configurations.
Software development environments: For software development
teams, EBS can be utilized to create and manage development
environments. By using Amazon Machine Images (AMIs) and EBS
volumes, developers can quickly spin up instances with specific
configurations and tools.

Automation using Terraform
As defined above, EBS Volumes are elastic block storage devices that can be
attached to EC2 Instances. We are going to do the following:
Create EBS volume
Attach the EBS volume to an existing EC2 instance
Following in Figure 4.1, is the architecture diagram of the following
terraform execution:
Figure 4.1: EBS to EC2 Architectural diagram
Now follow the step-by-step below to create EBS volume and attach it to an
existing EC2 Instance:
1. The following code is used to create EBS volume with 20GB size.
Create ebs.tf with the following code:
resource "aws_ebs_volume" "ebs-volume" {
  availability_zone = "us-east-2a"
  size = 20

  encrypted = false
  tags = {
    name = "ebs-demo",
        CreatedBy = "James Odeyinka",
        DateCreated = "02-25-2023",
        Env = "PoC"
  }
}
resource "aws_volume_attachment" "mount-ebs-
volume" {
  device_name = "/dev/sdd"
  instance_id = "PUT-YOUR-INSTANCE-ID-HERE"
  volume_id = "${aws_ebs_volume.ebs-
volume.id}"
}
2. Create provider.tf with the following code:
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 4.0"
    }
  }
}

provider "aws" {
  region = "us-east-2"
}
3. Use the following code to create .gitignore file. The .gitignore is to
prevent you from uploading the terraform state to our code commit
when we push the code to the repository:
# Compiled files
*.tfstate
*.tfstate.backup
# Module directory
.terraform
.idea
*.iml
*.ini
4. Create a config folder using mkdir config command. Create the four
config files below and save it in the config folder. We should aim to
create separate state files for different environment for easy
management:
demo.confg
bucket  = "s3-awscloudautomation-demo"
key     = "ebs-demo-statefiles"
dev.confg
bucket  = "s3-awscloudautomation-dev" key     
= "ebs-dev-statefiles"
staging.confg

bucket  = "s3-awscloudautomation-staging" key    
= "ebs-staging-statefiles"
prod.confg
bucket  = "s3-awscloudautomation-prod" key     
= "ebs-prod-statefiles"
5. Create a remote_state.tf file with the following code. State file
knows what bucket to use at the backend based on the config folder:
terraform {
  backend "s3" {
  region = "us-east-2"
  }
}
6. Run the following commands to implement the attachment of EBS.
You can have multiple EBS attached to one EC2 Instance:
 terraform init --backend-
config=config/demo.config
 terraform plan
 terraform apply –auto-approve
7. Push you code to a new branch in our CodeCommit for chapter4:
git push https://git-codecommit.us-east-
1.amazonaws.com/v1/repos/awsca-demo-repo 
Amazon Elastic File System
Amazon Elastic File System (Amazon EFS) stands tall as an innovative
managed file storage service provided by Amazon Web Services (AWS).
Designed to cater to the diverse needs of modern applications, Amazon EFS
offers an unparalleled experience in data accessibility, scalability, and
reliability. As a fully managed, scalable, and elastic Network File System
(NFS) designed to provide shared access to files across multiple Amazon

EC2 instances. EBS has a lot of limitations which makes EFS a perfect
choice for enterprise-related projects. EBS can only be used in 1 region or
the region where the EC2 is created while EFS can be created in any region
and attach to instance in any region, EBS can only be attached to one EC2
instance at a time while EFS can be attached to multiple EC2 and even on-
premises server at a time.
Following are key features:
Unraveling the virtues of elasticity: In the dynamic world of cloud
computing, the elasticity of storage resources is paramount. Amazon
EFS sets itself apart with its automatic and near-instantaneous
scalability. Irrespective of workload fluctuations, it dynamically
adjusts storage capacity and throughput to accommodate growing
demands, eliminating the need for manual intervention. This
empowers businesses to focus on innovation and growth, without
being constrained by traditional storage limitations.
Multi-region and multi-AZ redundancy: To fortify data resilience,
Amazon EFS boasts built-in support for multi-region and multi-
Availability Zone (AZ) redundancy. This robust architecture ensures
that your data remains secure and accessible, even in the face of
regional outages or unexpected disasters. By replicating data across
multiple locations, Amazon EFS provides a reliable safety net that
instills confidence in the face of uncertainties.
Cost-Efficiency through Pay-as-you-Go: In the pursuit of optimal
resource allocation, cost plays a crucial role. Amazon EFS aligns itself
with the pay-as-you-go pricing model, which means you only pay for
the storage you consume. This flexible approach eliminates upfront
costs and offers substantial savings for businesses of all sizes. With no
minimum commitments, Amazon EFS is a cost-effective solution that
adapts to your financial requirements.
The synchronization symphony: Collaboration is at the heart of
success, and Amazon EFS understands the significance of real-time
synchronization. Its shared file system architecture allows multiple
instances to access and modify data simultaneously, fostering seamless

collaboration and ensuring that all stakeholders stay in sync,
irrespective of their geographic locations.
Security that never compromises: Recognizing the significance of
data security, Amazon EFS adheres to stringent encryption standards,
both in transit and at rest. This guarantees that your data remains
shielded from unauthorized access, giving you peace of mind in
today’s increasingly data-centric world.
Simplicity and automation: Amazon EFS prioritizes user-
friendliness 
by 
automating 
routine 
tasks 
and 
streamlining
administrative processes. Its intuitive interface and well-documented
APIs empower users to configure, deploy, and manage file systems
with remarkable ease. This allows organizations to focus on
innovation and productivity instead of dealing with complex setup and
maintenance procedures.
Use cases
As a network file system, it can be easily shared with multiple EC2
instances, with a capacity that can grow up to petabytes with parallel access
through EC2 instances. You are going to create enterprise grade.
Big Data Analytics and Machine Learning Workloads: EFS can
serve as a centralized storage solution for data analytics and machine
learning workloads. The data generated by different instances running
data processing or ML algorithms can be stored in EFS, making it
accessible to all instances simultaneously, allowing for real-time data
processing, model training, and result sharing.
Media and entertainment collaboration: In media production
environments, teams often need to collaborate on large video, audio,
or graphics files. Amazon EFS provides a highly scalable and shared
file system that allows multiple artists, editors, and designers to work
on the same project files concurrently, boosting productivity and
creative collaboration.
Content Management Systems (CMS): EFS can be utilized as the
shared file storage for websites and content management systems. This

enables seamless sharing of assets, templates, and configurations
across multiple instances, providing a consistent experience for users
accessing the website or application.
DevOps Continuous Integration/Continuous Deployment (CI/CD)
Pipelines: When you have a CI/CD pipeline with multiple build and
deployment instances, Amazon EFS can serve as a shared storage
system to store artifacts, configuration files, and intermediate build
data, ensuring consistent results across all pipeline stages.
Software development environments: For teams of software
developers, EFS can be used to host source code repositories and
dependencies shared across development instances. This simplifies
version control and ensures all developers have access to the same
codebase, fostering collaboration.
Hybrid cloud storage: In hybrid cloud environments, where on-
premises and cloud resources coexist, EFS can act as a bridge for file
storage. Applications running both on-premises and in the cloud can
access and share data seamlessly, providing a unified experience.
Genomic data analysis: In bioinformatics and genomics research,
analyzing large-scale genomic datasets requires a high-performance
and shared storage solution. EFS can be used to store and share
genomic data across multiple instances, facilitating collaboration and
accelerating research efforts.
Multi-tier web applications: For web applications with different tiers
(web servers, application servers, and databases), Amazon EFS can
serve as the shared file system, enabling data and code sharing
between these components and ensuring data consistency.
Containerized applications: Amazon EFS can be used as a storage
backend for containerized applications orchestrated by tools like
Amazon ECS or Kubernetes. Containers running on different
instances can access and update shared data via EFS, facilitating
stateful applications.
IoT Data Aggregation and Analytics: Internet of Things (IoT)
devices generate vast amounts of data that need to be collected,

analyzed, and processed. EFS can serve as the storage solution for IoT
data, enabling real-time data analysis and insights across multiple
instances.
Note: EFS requires pre-requisites such as existing VPC, Subnets, EC2,
and GitHub repo which I already provided as a complement to this
book.
Automation using Terraform
EFS is high availability file system, it can be mounted to any server in any
region, it can be mounted to multiple EC2 instances, and the EFS can be
mounted to cloud and on-premises servers (refer to Figure 4.2):
Figure 4.2: EFS to Multiple EC2 Architecture diagram
Now follow the step-by-step process below to create EFS volume and attach
it to an existing EC2 Instance:
1. Keypair is used to execute some commands in the provisioned VP .
Create keypair.tf with the following code:
# Create Keypair
resource "tls_private_key" "efs-tfdemo-
keypair"  {

  algorithm = "RSA"
}
# locally store
resource "local_file" "efstfdemokey" {
    content     = tls_private_key.efs-tfdemo-
keypair.public_key_openssh
    filename = "efs-demo.pem"
    //file_permission = 0400
}
resource "aws_key_pair" "efs-tfdemo-key" {
  key_name    = "efs-tfdemo-key"
  public_key = tls_private_key.efs-tfdemo-
keypair.public_key_openssh
}
2. Create efs.tf with the following code. The following code is used to
create EFS file system. The inline code has git clone that clones the
code from my GitHub repo for Chapter 4, Automating AWS Storage:
# Creating efs
resource "aws_efs_file_system" "efs-demo"{
  creation_token = "efs_file_token"
  tags = {
    Name = "efs_demo_file_system"
    CreatedBy = "James Odeyinka",
    DateCreated = "02-25-2023",
    Env = "Demo"

  }
}
resource "aws_efs_mount_target" "efs_mount" {
  file_system_id  = aws_efs_file_system.efs-
demo.id
  subnet_id       = "${aws_subnet.efs-demo-
subnet.id}"
  security_groups = 
["${aws_security_group.efs-demo-sgroup.id}"]
}
output "myip" {
  value = aws_instance.efsdemo-
instance1.public_ip
}
resource "null_resource" "nullip" {
  depends_on = [
    aws_efs_mount_target.efs_mount,
  ]
  connection {
    agent    = "false"
    type     = "ssh"
    user     = "ubuntu"
    private_key = "${tls_private_key.efs-
tfdemo-keypair.private_key_pem}"
    host     = "${aws_instance.efsdemo-
instance1.public_ip}"

  }
provisioner "remote-exec"{
       inline = [
          "sudo apt-get update"
          "sudo apt install httpd php git -y",
          "sudo systemctl start httpd",
          "sudo systemctl enable httpd",
          "sudo mkfs.ext4  /dev/xvdf",
          "sudo rm -rf /var/www/html/*",
          "sudo mount  /dev/xvdf  
/var/www/html",
          "sudo git clone 
https://github.com/awscloudautomationbook/chap
ter4-efs-filetrans.git /var/www/html"
         ]
    }
}
3. Create provider.tf with the following code:
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 4.0"
    }
  }

}
provider "aws" {
  region = "us-east-2"
}
4. Use the following code to create .gitignore file. The .gitignore is to
prevent you from uploading the terraform state to our code commit
when we push the code to the repository:
# Compiled files
*.tfstate
*.tfstate.backup
# Module directory
.terraform
.idea
*.iml
*.ini
5. Create a config folder using mkdir config command. Create the four
config files below and save it in the config folder. We should aim to
create separate state files for different environment for easy
management.
demo.confg
bucket  = "s3-awscloudautomation-demo"
key     = "efs-demo-statefiles"
dev.confg
bucket  = "s3-awscloudautomation-dev"
key     = "efs-dev-statefiles"
staging.confg

bucket  = "s3-awscloudautomation-staging"
key     = "efs-staging-statefiles"prod.confg
bucket  = "s3-awscloudautomation-prod"
key     = "efs-prod-statefiles"
6. Create a remote_state.tf file with the code below. State file knows
what bucket to use at the backend based on the config folder:
terraform {
  backend "s3" {
  region = "us-east-2"
  }
}
7. Create a ec2.tf file with the following code. Run the commands
below to implement the attachment of EFS. You can have single EFS
attached to multiple EC2 Instance. EFS is used widely in Big Data
because of its portability:
resource "aws_instance" "efsdemo-instance1" {
  ami           = "ami-00eeedc4036573771"
  instance_type = "t2.micro"
  availability_zone = "us-east-2b"
  key_name      = "efs-tfdemo-key"
  security_groups = [ 
"${aws_security_group.efs-demo-sgroup.id}" ]
  subnet_id = "${aws_subnet.ec2-demo-
subnet.id}"
   connection {
    type     = "ssh"

    user     = "ubuntu"
    private_key =  tls_private_key.efs-tfdemo-
keypair.private_key_pem
    host     = "${aws_instance.efsdemo-
instance1.public_ip}"
 }
 provisioner "remote-exec" {
    inline = [
      "sudo apt-get update && apt-get 
upgrade",
      "sudo apt install httpd  php git -y",
      "sudo systemctl restart httpd",
      "sudo systemctl enable httpd"
    ]
  }
tags = {
       Name = "efs-demo-svr1"
       CreatedBy = "James Odeyinka",
       DateCreated = "02-25-2023",
       Env = "PoC"
  }
 }
resource "aws_instance" "efsdemo-instance2" {
  ami           = "ami-00eeedc4036573771"
  instance_type = "t2.micro"

  availability_zone = "us-east-2b"
  key_name      = "efs-tfdemo-key"
  security_groups = [ 
"${aws_security_group.efs-demo-sgroup.id}" ]
  subnet_id = "${aws_subnet.ec2-demo-
subnet.id}"
   connection {
    type     = "ssh"
    user     = "ubuntu"
    private_key =  tls_private_key.efs-tfdemo-
keypair.private_key_pem
    host     = "${aws_instance.efsdemo-
instance2.public_ip}"
 }
 provisioner "remote-exec" {
    inline = [
      "sudo apt-get update && apt-get 
upgrade",
      "sudo apt install httpd  php git -y",
      "sudo systemctl restart httpd",
      "sudo systemctl enable httpd"
    ]
  }
tags = {
       Name = "efs-demo-svr2"

       CreatedBy = "James Odeyinka",
       DateCreated = "02-25-2023",
       Env = "Demo"
  }
 }
8. The following code also created EC2 that you need to mount the EFS.
You can have multiple EFS attached to one EC2 Instance. Run the
following commands to implement the creation and attachment of
EFS:
terraform init --backend-
config=config/demo.config terraform plan 
terraform apply –auto-approve
9. Run the following commands from the command line to mount the
EFS to any files of your choice and move files freely and across
multiple EC2 Instances:
# mount -t nfs4 -o 
nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans
=2,noresvport fs-31337er3.efs.us-east-1.amazonaws.com:/ 
/mnt/efs  # df -kh
10. Push your code to a new branch in our codecommit chapter4-efs git
push, on the mentioned GitHub repository.
Amazon FSx
Amazon FSx is a fully managed file storage service provided by AWS. It is
designed to simplify and accelerate the deployment of file systems, making
it easier for organizations to run and scale file-based applications in the
cloud. FSx offers compatibility with popular file systems, along with high
availability, durability, and seamless integration with other AWS services,
making it an excellent choice for various workloads. As a third-party
managed file system that provides native file system access to applications
running on EC2 instances. FSx files are usually SSD storage to provide fast

performance with low latency. There are FSx for Windows File Server, and
FSx for Lustre.
FSx key features and benefits:
Fully managed service: Amazon FSx takes care of all aspects of file
system administration, such as provisioning, setup, maintenance, and
backups. This allows users to focus on their applications and data
without worrying about the underlying infrastructure.
Compatibility with multiple file systems: FSx supports two main file
systems: Amazon FSx for Windows File Server and Amazon FSx for
Lustre. The former is compatible with Microsoft Windows
applications, providing Windows NTFS file permissions and Active
Directory integration. The latter is optimized for high-performance
computing and analytics workloads.
High performance: With Amazon FSx, users can achieve high
throughput and low-latency access to their file systems. It is ideal for
applications that require fast data access, such as data analytics,
content repositories, and media processing.
Seamless integration with AWS services: FSx integrates seamlessly
with other AWS services, allowing users to combine it with various
tools like AWS Directory Service, AWS Backup, AWS DataSync, and
more. This integration enhances data management and disaster
recovery capabilities.
Scalability and elasticity: As the storage demands of applications
grow, FSx can easily scale up or down, providing the necessary
storage capacity and performance. This ensures that organizations can
adapt to changing workloads without disruptions.
Data durability and backup: Amazon FSx ensures data durability by
automatically replicating data across multiple Availability Zones
within an AWS Region. Additionally, it allows users to create
automatic backups and schedule snapshots for point-in-time data
recovery.
Cost-effective: FSx offers a pay-as-you-go pricing model, enabling
organizations to optimize costs by paying only for the storage and

throughput they use. This flexibility is particularly beneficial for
applications with varying workloads.
Security and compliance: Amazon FSx implements various security
features, such as data encryption at rest and in transit, integration with
AWS Identity and Access Management for access control, and
compliance with industry standards and regulations.
Use cases
Amazon File System (FSx) is a fully managed file storage service offered
by Amazon Web Services. It provides scalable, high-performance file
storage that is compatible with popular industry-standard file systems.
Here are some unique use cases for Amazon FSx:
Media and entertainment workloads: Amazon FSx can be used for
managing and sharing media files in the media and entertainment
industries. Post-production studios, animation companies, and video
editing teams can store and collaborate on large video files, audio
files, and graphics using Amazon FSx. The service’s high throughput
and low latency are crucial for handling large media workloads
efficiently.
Genomics and bioinformatics: In the field of genomics and
bioinformatics, massive datasets need to be stored, accessed, and
analyzed. Amazon FSx can serve as a centralized storage solution for
genomic data, enabling researchers and scientists to process, share,
and collaborate on genome sequencing and analysis projects. The
service’s ability to scale and handle heavy workloads makes it suitable
for genomics research.
High-Performance Computing (HPC): HPC environments often
require shared storage to enable multiple computer instances to access
data simultaneously. Amazon FSx provides a fully managed, high-
performance file system that can be integrated seamlessly with HPC
clusters. This allows researchers, engineers, and data scientists to work
on complex simulations, modeling, and data-intensive scientific
computations efficiently.

Autonomous vehicle data processing: With the rise of autonomous
vehicles, massive amounts of data are generated from sensors,
cameras, and other devices. Amazon FSx can be used to store and
process this data, making it accessible to machine learning algorithms
and data analytics tools. By using FSx, companies in the automotive
industry can accelerate the development and testing of autonomous
vehicle systems.
CAD and CAM collaboration: In engineering and manufacturing
environments, Computer-Aided Design (CAD) and Computer-
Aided Manufacturing (CAM) files often need to be shared and
accessed by multiple users simultaneously. Amazon FSx provides a
robust and scalable file system that can handle the performance
requirements of CAD/CAM applications. This enables efficient
collaboration among design and engineering teams.
Financial services applications: Financial institutions deal with large
amounts of data and have strict requirements for data security and
compliance. Amazon FSx can be used to store financial data, run risk
modeling applications, and manage data used for financial analysis.
The service’s encryption features and integration with AWS Identity
and Access Management ensure data remains secure and accessible to
authorized personnel only.
Real-time analytics and big data: Amazon FSx can be leveraged as
shared storage for real-time analytics and big data processing. It
enables data analytics tools like Apache Spark, Apache Hadoop, or
Amazon EMR to access and process data efficiently. The high
throughput and low latency of Amazon FSx enhance the performance
of these analytics workloads, allowing organizations to gain valuable
insights from their data faster.
Education and research collaboration: In educational institutions
and research organizations, collaborative projects involve sharing and
accessing large datasets and research materials. Amazon FSx can
serve as a reliable, scalable storage solution to facilitate collaboration
among researchers, students, and faculty members. This enables

seamless sharing and access to educational resources and research
data.
Automation using Terraform
Creating an Amazon FSx automation using Terraform can be achieved
through a series of steps. In this example, we’ll create an Amazon FSx for
Windows File Server.
Here is a step-by-step guide to set up the automation:
1. Create provider.tf with the following code:
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 4.0"
    }
  }
}
provider "aws" {
  region = "us-east-2"
}
2. Use the following code to create .gitignore file. The .gitignore is to
prevent you from uploading the terraform state to our code commit
when we push the code to the repository:
# Compiled files
*.tfstate
*.tfstate.backup

# Module directory
.terraform
.idea
*.iml
*.ini
3. Create an Amazon FSx for Windows File Server:
resource "aws_fsx_windows_file_system" 
"example" {
subnet_ids          = ["subnet-12345678"]  # 
Replace with your desired subnet IDs
security_group_ids  = ["sg-12345678"]      # 
Replace with your desired security group IDs
storage_capacity    = 300
throughput_capacity = 32
storage_type        = "HDD"
kms_key_id          = "alias/aws/fsx"     # 
Replace with your desired KMS key ARN or alias
tags = {
Name = "ExampleFSx"
}
}
4. Create a directory for your Windows instances:
resource "aws_directory_service_directory" 
"example" {
  name                 = "example.com"

  password             = "P@ssw0rd"        # 
Replace with your desired password
  size                 = "Small"
  type                 = "MicrosoftAD"
  vpc_settings {
    subnet_ids = ["subnet-12345678"]       # 
Replace with your desired subnet IDs
    vpc_id     = "vpc-12345678"           # 
Replace with your desired VPC ID
  }
}
5. Create a Windows EC2 instances to test the FSx file system:
resource "aws_instance" "example" {
  ami           = "ami-0abc123def456"     # 
Replace with your desired Windows AMI ID
  instance_type = "t2.micro"              # 
Replace with your desired instance type
  key_name      = "example-key"          # 
Replace with your desired SSH key pair
  security_groups = 
[aws_security_group.example.id]
  tags = {
    Name = "ExampleInstance"
  }
}
6. Mount the FSx file system on the Windows instance. Use PowerShell
Script to mount the FSx file system on the Windows Instance:

resource "aws_security_group" "example" {
  name_prefix = "example-sg"
}
provisioner "remote-exec" {
  inline = [
    "Add-WindowsFeature FS-FileServer",
    "New-Item -ItemType Directory -Path 
'C:\\data'",
    "New-SmbGlobalMapping -ShareName 
${aws_fsx_windows_file_system.example.dns_name
} -LocalPath 'C:\\data'",
    "net use Z: 
\\\\${aws_fsx_windows_file_system.example.dns_
name}\\share P@ssw0rd /user:FSxUser",
  ]
}
7. Run the following commands to implement the creation and
attachment of FSx. The code below also created Windows EC2 which
you need to mount the FSx:
 terraform init --backend-
config=config/demo.config
 terraform plan
 terraform apply –auto-approve
Amazon Glacier
Amazon Glacier is a secure, durable, and low-cost cloud storage service
offered by Amazon Web Services. It is designed to provide long-term
archival and backup storage for data that is infrequently accessed but

requires retention for extended periods. The service is particularly suited for
data archiving, regulatory compliance, and disaster recovery purposes.
Launched in 2012, Amazon Glacier is one of the prominent components of
the AWS cloud ecosystem.
Amazon Glacier key features are:
Data durability and availability: Amazon Glacier ensures high
durability and availability of stored data by replicating it across
multiple geographically distinct AWS data centers. This redundancy
minimizes the risk of data loss and provides reliable access to archives
when needed.
Cost-effective storage: One of the most significant advantages of
Amazon Glacier is its cost-effectiveness. It offers exceptionally low
storage costs, making it ideal for organizations that need to retain large
volumes of data but infrequently access it. The pricing model is based
on the amount of data stored, and the retrieval options chosen.
Data retention and archival: Glacier is designed for long-term data
retention, allowing businesses to meet regulatory requirements and
comply with industry standards. It is particularly suitable for storing
historical records, financial documents, medical records, media
archives, and other data that must be preserved for extended periods.
Data security: Amazon Glacier employs a variety of security
measures to protect data at rest and in transit. Data is encrypted using
AES-256 encryption, and SSL/TLS protocols are used for secure
communication during data transfer. Additionally, AWS Identity and
Access Management controls access to the Glacier vaults and retrieval
policies.
Data lifecycle policies: Glacier supports data lifecycle management
through user-defined policies. These policies enable automatic data
archival, deletion, or transfer to other storage classes based on
predefined rules and timeframes. This feature simplifies data
management and reduces administrative overhead.
Expedited, standard, and bulk retrieval options: Amazon Glacier
provides three retrieval options, each with different costs and

timeframes. Expedited retrieval offers data access within minutes,
standard retrieval takes a few hours, and bulk retrieval is suitable for
non-urgent retrieval at a lower cost but may take several hours to
complete.
Integration with other AWS Services: Amazon Glacier seamlessly
integrates with other AWS services, such as Amazon S3 (Simple
Storage Service), to create a comprehensive storage and archival
strategy. Organizations can use Amazon S3 as a staging area for data
before transferring it to Glacier for long-term storage.
Vault lock: Glacier offers a Vault Lock feature, which allows users to
set data retention policies and enforce data preservation for regulatory
and compliance requirements. Once the Vault Lock policy is applied,
it cannot be altered or deleted until the specified retention period
expires.
Use cases
Amazon Glacier is a secure, durable, and cost-effective cloud storage service
designed for long-term data archiving and backup. Its unique features and
characteristics make it well-suited for various use cases.
Here are some unique Amazon Glacier use cases:
Digital preservation for cultural institutions: Museums, libraries,
and archives can use Amazon Glacier to preserve digital copies of
historical artifacts, manuscripts, artworks, and other culturally
significant materials. Glacier’s long-term storage capabilities and low-
cost structure ensure that valuable data remains accessible for future
generations.
Genomic data storage: The field of genomics generates massive
amounts of data, such as DNA sequencing data. Researchers and
organizations in genomics can use Glacier to store and archive these
data sets securely over the long term, enabling access and analysis
whenever needed.
Satellite and remote sensing data archiving: Satellites and remote
sensing technologies generate vast amounts of data related to Earth

observation. Glacier provides a reliable and scalable solution for
organizations and researchers to archive this data for analysis,
research, and historical reference.
Legal and compliance data retention: Certain industries have strict
regulatory requirements for data retention, such as financial services
and healthcare. Amazon Glacier offers a compliant and cost-effective
storage solution for businesses to meet their legal obligations by
archiving sensitive data for extended periods.
Digital evidence and forensics: Law enforcement agencies and legal
departments often require long-term storage of digital evidence for
investigations and legal proceedings. Glacier’s secure and immutable
storage helps preserve the integrity of the evidence and ensures it
remains tamper-proof.
Media and entertainment archives: Media companies, studios, and
broadcasters can use Glacier to store and manage large libraries of
digital assets, including video footage, audio recordings, and high-
resolution images. Glacier’s scalability accommodates the ever-
growing volume of media content.
Scientific research data preservation: Scientists and researchers
generate massive datasets in fields like astronomy, climate research,
and environmental studies. Amazon Glacier enables them to store and
preserve these valuable datasets, facilitating reproducibility and long-
term research continuity.
Historical document and newspaper archives: Publishers and
organizations with extensive collections of historical documents and
newspapers can utilize Glacier to digitize, store, and preserve these
records, making them easily accessible for historical research and
public access.
Disaster recovery data backup: In addition to regular backups,
organizations can use Glacier as a cost-effective, off-site disaster
recovery solution. By replicating critical data to Glacier, businesses
can quickly recover their data in the event of a catastrophic data loss
event.

Video surveillance footage retention: Surveillance systems in public
places, businesses, and homes generate a massive amount of video
footage. Amazon Glacier can serve as an economical and secure
solution to retain surveillance data for an extended period, as required
by security and compliance regulations.
Automation using Terraform
To create Amazon Glacier automation using Terraform, we can set up a
simple workflow to demonstrate how to automate the process of archiving
data to Glacier. In this example, we will assume you already have an S3
bucket where you want to store your data before archiving it to Glacier.
Here are the steps we will follow in our automation:
1. Create an S3 bucket to store the data.
2. Set up an S3 lifecycle policy to automatically move data to Glacier
after a specific period.
3. Create an IAM role with the necessary permissions for S3 and Glacier
actions.
4. Create a Lambda function that will trigger when objects are uploaded
to the S3 bucket.
5. Configure the Lambda function to move objects to the Glacier vault.
6. Create provider.tf with the following code:
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 4.0"
    }
  }
}

provider "aws" {
  region = "us-east-2"
}
7. Use the following code to create .gitignore file. The .gitignore is to
prevent you from uploading the terraform state to our code commit
when we push the code to the repository:
# Compiled files
*.tfstate
*.tfstate.backup
# Module directory
.terraform
.idea
*.iml
*.ini
8. Create main.tf with the following code:
resource "aws_s3_bucket" "data_bucket" {
  bucket = "your-data-bucket-name" # Replace 
with your desired S3 bucket name
}
resource 
"aws_s3_bucket_lifecycle_configuration" 
"data_bucket_lifecycle" {
  rule {
    id      = "glacier_rule"
    status  = "Enabled"
    prefix  = ""

    enabled = true
    transition {
      days          = 30 # Adjust the number 
of days as per your requirements
      storage_class = "GLACIER"
    }
  }
}
resource "aws_iam_role" "lambda_role" {
  name = "glacier_lambda_role"
  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Principal = {
          Service = "lambda.amazonaws.com"
        }
      }
    ]
  })
}
resource "aws_iam_policy" "lambda_policy" {

  name        = "glacier_lambda_policy"
  description = "Policy for Glacier Lambda 
Function"
  policy      = 
data.aws_iam_policy_document.glacier_lambda.js
on
}
data "aws_iam_policy_document" 
"glacier_lambda" {
  statement {
    actions = [
      "logs:CreateLogGroup",
      "logs:CreateLogStream",
      "logs:PutLogEvents",
    ]
    effect = "Allow"
    resources = ["*"]
  }
  statement {
    actions = [
      "s3:GetObject",
      "s3:PutObject",
      "s3:DeleteObject",
    ]
    effect = "Allow"

    resources = 
[aws_s3_bucket.data_bucket.arn, 
"${aws_s3_bucket.data_bucket.arn}/*"]
  }
  statement {
    actions = [
      "glacier:UploadArchive",
      "glacier:InitiateMultipartUpload",
     "glacier:CompleteMultipartUpload",
    ]
    effect = "Allow"
    resources = ["*"]
  }
}
resource "aws_iam_role_policy_attachment" 
"lambda_policy_attachment" {
  policy_arn = 
aws_iam_policy.lambda_policy.arn
  role       = aws_iam_role.lambda_role.name
}
resource "aws_lambda_function" 
"glacier_lambda" {
  filename      = 
"glacier_lambda_function.zip" # Create this 
zip file containing your Lambda code
  function_name = "glacier_lambda_function"

  role          = aws_iam_role.lambda_role.arn
  handler       = "index.handler"
  runtime       = "nodejs14.x" # Replace with 
the desired runtime if needed
}
resource "aws_lambda_permission" 
"glacier_lambda_permission" {
  statement_id  = "AllowExecutionFromS3"
  action        = "lambda:InvokeFunction"
  function_name = 
aws_lambda_function.glacier_lambda.function_na
me
  principal     = "s3.amazonaws.com"
  source_arn    = 
aws_s3_bucket.data_bucket.arn
}
9. Prepare the Lambda function code. Create a file named index.js with
the following content:
const AWS = require('aws-sdk');
exports.handler = async (event) => {
  const s3 = new AWS.S3();
  const glacier = new AWS.Glacier();
 
  const srcBucket = 
event.Records[0].s3.bucket.name;
  const srcKey = 
event.Records[0].s3.object.key;

 
  try {
    const params = {
      accountId: 'YOUR_AWS_ACCOUNT_ID', // 
Replace with your AWS account ID
      vaultName: 'YOUR_GLACIER_VAULT_NAME', // 
Replace with your Glacier vault name
      body: s3.getObject({ Bucket: srcBucket, 
Key: srcKey }).createReadStream()
    };
 
    const response = await 
glacier.uploadArchive(params).promise();
    console.log('Successfully uploaded to 
Glacier:', response);
 
    return {
      statusCode: 200,
      body: JSON.stringify('Successfully 
uploaded to Glacier'),
    };
  } catch (err) {
    console.error('Error uploading to 
Glacier:', err);
    throw err;
  }

};
10. Run the followng commands to implement the creation:
terraform init --backend-config=config/demo.config
terraform plan
terraform apply –auto-approve
Whenever you upload an object to the S3 bucket, the Lambda function will
automatically move it to the Glacier vault specified in the index.js code.
Amazon S3 Glacier
Amazon S3 Glacier is a secure, durable, and cost-effective cloud storage
service offered by Amazon Web Services. It is designed to provide long-
term data archiving and backup solutions for businesses and individuals.
Unlike Amazon S3, which offers immediate access to data, S3 Glacier is
optimized for infrequently accessed data that requires long-term retention,
offering lower storage costs while maintaining data durability.
Key features and benefits:
Cost-effective storage: S3 Glacier provides a highly economical
storage solution, making it ideal for organizations that need to store
large volumes of data without incurring high expenses. The pricing
model is based on a pay-as-you-go approach, enabling users to pay
only for the storage capacity they utilize.
Data durability and security: Data stored in S3 Glacier is replicated
across multiple AWS Availability Zones, ensuring high durability and
protection against hardware failures. Additionally, AWS employs
robust security measures, including server-side encryption, to
safeguard data at rest.
Data retrieval options: Retrieving data from Glacier is designed to be
a cost-effective process. However, it’s important to note that there are
different retrieval options with varying time frames and costs.
Expedited retrieval allows for immediate access at a higher cost, while
standard retrieval is suitable for most data retrieval needs. Bulk
retrieval is the most economical option but requires longer lead times.

Archiving capabilities: S3 Glacier enables users to store a wide range
of data types, including backups, archives, and regulatory compliance
data. This makes it an excellent choice for organizations dealing with
vast amounts of historical or seldom-accessed information.
Lifecycle policies: To optimize storage costs, users can set up
lifecycle policies to automatically transition data from Amazon S3 to
Glacier based on specified rules. This feature allows seamless
movement of data between storage classes without manual
intervention.
Integration with AWS ecosystem: Amazon S3 Glacier seamlessly
integrates with other AWS services, enabling users to create
comprehensive data management workflows. It can be combined with
AWS Lambda, Amazon S3, and Amazon S3 Glacier Deep Archive,
providing a powerful ecosystem for data archival, retrieval, and
processing.
Compliance and regulatory support: S3 Glacier adheres to various
compliance standards, making it suitable for industries with strict data
retention and governance requirements. These standards include
HIPAA, GDPR, and SEC Rule 17a-4.
Use cases
Amazon S3 Glacier is a secure, durable, and low-cost storage service
designed for data archiving and long-term backup.
Here are some unique Amazon S3 Glacier use cases:
Long-term data archiving: S3 Glacier is ideal for organizations that
need to store large amounts of data for extended periods without
frequent retrieval. This could include regulatory data retention,
historical records, or backups that need to be retained for years.
Digital preservation: Libraries, museums, and cultural institutions
can leverage S3 Glacier to preserve digital assets such as rare
manuscripts, historical photos, and audiovisual recordings. The service
ensures the data’s long-term preservation with durability and
encryption.

Compliance and regulatory requirements: Companies in industries
with strict compliance requirements, such as healthcare, finance, and
government, can use S3 Glacier to store sensitive data while meeting
regulatory obligations.
Media and entertainment content archiving: Media companies
generate vast amounts of content daily. S3 Glacier can serve as a
secure and cost-effective solution for archiving raw footage, project
files, and completed media assets that may need to be accessed
occasionally but retained indefinitely.
Disaster recovery and business continuity: Organizations can use S3
Glacier to store critical backup data as part of their disaster recovery
and business continuity strategies. It acts as an off-site backup
solution, ensuring data integrity and availability in case of a disaster.
Genomic data storage: The field of genomics generates massive
amounts of data. Researchers and institutions can use S3 Glacier for
long-term storage of genomic data, such as DNA sequences, research
findings, and experimental results.
Scientific data archiving: Scientific research often generates vast
datasets that need to be retained for reference and future analysis. S3
Glacier can store experimental data, climate records, astronomical
observations, and more.
Digital forensics and legal evidence: Law enforcement agencies and
legal firms can use S3 Glacier to archive digital evidence and case-
related data for future reference or presentation in court.
Artificial Intelligence and Machine Learning Model Storage: AI
and ML developers can use S3 Glacier to archive older versions of
trained models, datasets, and experiment results, which may be needed
for comparisons or audits.
Satellite image archiving: Companies or agencies that collect satellite
imagery for various purposes, such as environmental monitoring or
urban planning, can use S3 Glacier to store large volumes of historical
satellite images.

Automation using Terraform
As mentioned earlier, planning, and architecting organization’s structure is
one of the success criteria when organizations move to the cloud. Use the
step below to create your own organization structure.
1. Copy config folder to the terraform folder for the storage gateway.
Copy provider.tf, .gitignore, provider.tf, and remote_state.tf to
the terraform folder file the storage gateway.
2. Create s3_glacier_bucket.tf with the code below. The code below is
used to create S3 glacier storage:
# Creating the s3 glacier bucket
resource "aws_s3_bucket" "s3glacier-demo-
bucket" {
  bucket = "s3glacier-demo-bucket"
  acl    = "private"
  lifecycle_rule {
    id      = "archive"
    enabled = true
    prefix = "archive/"
    tags = {
       rule = "archive"
       CreatedBy = "James Odeyinka",
       DateCreated = "02-25-2023",
       Env = "Demo"
    }
    transition {
      days          = 30

      storage_class = "ONEZONE_IA"
    }
    transition {
      days          = 90
      storage_class = "GLACIER"
    }
    expiration {
      days = 120
    }
  }
}
resource "aws_s3_bucket_object" "prod-
lifecycle-mgt" {
  bucket = "${aws_s3_bucket.s3glacier-demo-
bucket.id}"
  acl    = "private"
  key    = "prod/"
  source = "/dev/null"
}
resource "aws_s3_bucket_object" "lifecycle-
archive-mgt" {
  bucket = "${aws_s3_bucket.s3glacier-demo-
bucket.id}"
  acl    = "private"
  key    = "archive/"

  source = "/dev/null"
}
3. Run the following commands to implement the creation and
attachment of SGW:
terraform init --backend-
config=config/demo.config  terraform plan  
terraform apply –auto-approve
Push your code to a new branch in our CodeCommit chapter4-sgw git push in
the mentioned GitHub repository.
AWS Storage Gateway
AWS Storage Gateway is a versatile and seamless hybrid cloud storage
service offered by Amazon Web Services. It enables businesses to integrate
their on-premises environments with the AWS Cloud, extending the power
of cloud storage to their existing infrastructure. The service acts as a bridge
between on-premises data and the AWS cloud, allowing organizations to
store, manage, and retrieve their data across multiple locations securely and
efficiently.
Storage gateway key features:
Hybrid cloud storage: AWS Storage Gateway facilitates a hybrid
storage model, where on-premises applications can access cloud
storage without any disruption. It seamlessly integrates with existing
environments, enabling businesses to leverage the benefits of cloud
storage while still retaining their on-premises infrastructure.
Multiple storage protocols: Storage Gateway supports a range of
industry-standard storage protocols, making it highly compatible with
existing applications and systems. It offers access to data through file-
based protocols like Network File System (NFS) and Server Message
Block (SMB) and block-based protocols like iSCSI.
Tape Gateway- Virtual Tape Library (VTL): For businesses still
relying on tape backups, AWS Storage Gateway’s Tape Gateway
allows them to replace their physical tape infrastructure with a virtual

tape library in the cloud. This simplifies backup and archive processes,
reducing costs and improving scalability.
Volume Gateway (stored and cached volumes): Volume Gateway
provides block storage volumes that can be mounted as iSCSI devices.
It offers two modes: Stored Volumes, where data is stored locally on-
premises and asynchronously backed up to AWS, and Cached
Volumes, where most data is stored in the cloud, minimizing on-
premises storage requirements.
File gateway: File Gateway presents a file interface to objects stored
in Amazon S3. It enables seamless integration of cloud storage into
existing file-based applications, providing scalable and cost-effective
file storage solutions.
Data synchronization and migration: Storage Gateway efficiently
synchronizes data between on-premises systems and AWS cloud
storage. It also supports data migration to and from the cloud,
simplifying the process of moving large datasets to AWS.
Data encryption and security: Data transferred between on-premises
environments and AWS cloud is encrypted using industry-standard
protocols, ensuring data security and compliance. AWS IAM (Identity
and Access Management) allows granular control over access to the
gateway resources.
Cost-effective solution: With AWS Storage Gateway, businesses can
avoid upfront hardware investments and pay only for the storage used
in the AWS cloud. This pay-as-you-go model makes it a cost-effective
storage solution for various use cases.
AWS Storage Gateway options
The following are AWS Storage Gateway options:
VMware ESXi: To use the Storage Gateway on an ESXi host
platform, you will need to verify that your ESXi host platform is
compatible with AWS Storage Gateway. You can check the
compatibility of your ESXi host platform with AWS Storage Gateway
by visiting the AWS Storage Gateway User Guide.

Download and install the AWS Storage Gateway Virtual Machine
(VM) image. You can download the VM image from the AWS
Management Console. Deploy the AWS Storage Gateway VM image
on your ESXi host platform using VMware vSphere.
After the VM image has been deployed, you will need to configure the
AWS Storage Gateway. You can configure the Gateway using the
AWS Management Console. Once the Gateway has been configured,
you can start using it to integrate on-premises storage securely and
seamlessly with cloud storage.
It is important to note that AWS Storage Gateway is a service
provided by AWS, while ESXi host platform is a product provided by
VMware.
Microsoft Hyper-V: The process for setting up AWS Storage
Gateway on a Hyper-V host platform is to verify that your Hyper-V
host platform is compatible with AWS Storage Gateway. You can
check the compatibility of your Hyper-V host platform with AWS
Storage Gateway by visiting the AWS Storage Gateway User Guide.
Download and install the AWS Storage Gateway Virtual Machine
(VM) image. You can download the VM image from the AWS
Management Console. Deploy the AWS Storage Gateway VM image
on your Hyper-V host platform using Hyper-V Manager.
After the VM image has been deployed, you will need to configure the
AWS Storage Gateway. You can configure the Gateway using the
AWS Management Console. Once the Gateway has been configured,
you can start using it to integrate on-premises storage securely and
seamlessly with cloud storage.
Linux KVM: To use the Storage Gateway on an ESXi host platform,
you will need to verify that your Linux KVM host platform is
compatible with AWS Storage Gateway. You can check the
compatibility of your Linux KVM host platform with AWS Storage
Gateway by visiting the AWS Storage Gateway User Guide.
Download and install the AWS Storage Gateway virtual VM image.
You can download the VM image from the AWS Management

Console. Deploy the AWS Storage Gateway VM image on your Linux
KVM host platform using the KVM command line tools or Graphical
User Interface (GUI).
After the VM image has been deployed, you will need to configure the
AWS Storage Gateway. You can configure the Gateway using the
AWS Management Console. Once the Gateway has been configured,
you can start using it to integrate on-premises storage securely and
seamlessly with cloud storage.
Amazon EC2: To use the Storage Gateway on Amazon EC2 instance,
you will need to verify that your Linux KVM host platform is
compatible with AWS Storage Gateway by launching an Amazon EC2
instance that meets the minimum system requirements for running
AWS Storage Gateway.
Download and install the AWS Storage Gateway software on the
Amazon EC2 instance. After the software has been installed, you will
need to configure the AWS Storage Gateway. You can configure the
Gateway using the AWS Management Console.
Once the Gateway has been configured, you can start using it to
integrate on-premises storage securely and seamlessly with cloud
storage.
Hardware appliance: To use the Storage Gateway hardware
appliance, you must order the AWS Storage Gateway hardware
appliance from the AWS Management Console.
When the appliance arrives, connect it to your on-premises network.
Use the AWS Management Console to configure the AWS Storage
Gateway Hardware Appliance.
Once the Gateway has been configured, you can start using it to
integrate on-premises storage securely and seamlessly with cloud
storage.
Use cases
AWS Storage Gateway is a hybrid cloud storage service that enables on-
premises applications to seamlessly and securely access data stored in the

AWS Cloud. It acts as a bridge between on-premises environments and
cloud storage, offering various use cases to suit different business needs.
Here are some unique use cases for AWS Storage Gateway:
Disaster Recovery (DR) and backup: AWS Storage Gateway can be
used for disaster recovery and backup purposes. You can set up a
backup of your on-premises data to AWS S3, allowing you to recover
your critical data and applications in the event of a disaster or data
loss. By using Storage Gateway, you can achieve cost-effective,
automated backups without the need for extensive infrastructure on-
premises.
Data archiving and long-term retention: For organizations that need
to archive large volumes of data for compliance or regulatory
requirements, AWS Storage Gateway can be used to seamlessly
offload older, less frequently accessed data from on-premises storage
to Amazon S3 Glacier or Amazon S3 Glacier Deep Archive. This
helps save on storage costs while ensuring data integrity and
accessibility when needed.
Cloud bursting: Cloud bursting is a scenario where on-premises
applications experience spikes in demand and require additional
compute and storage resources temporarily. With AWS Storage
Gateway, you can extend your on-premises storage capacity by using
Amazon S3 as an extension of your data center, enabling you to
handle increased workloads efficiently during peak periods.
File sharing and collaboration: Storage Gateway supports the File
Gateway mode, which enables seamless integration between on-
premises file servers and Amazon S3. This makes it an excellent
solution for file sharing and collaboration across multiple offices or
remote teams, providing a centralized storage location accessible by
all authorized users.
Data migration to the cloud: AWS Storage Gateway can facilitate a
smooth and incremental migration of on-premises applications and
data to the cloud. By using the Volume Gateway mode, you can stage
your data in AWS before fully transitioning to cloud-native solutions.

This approach minimizes downtime and allows for a gradual and
controlled migration process.
Hybrid cloud application development: For organizations building
hybrid cloud applications that require a mix of on-premises and cloud-
based resources, AWS Storage Gateway can act as a seamless bridge.
The Gateway-cached volumes mode enables low-latency access to
frequently accessed data on-premises while leveraging the cost-
effectiveness and scalability of AWS S3 in the cloud.
Media and entertainment workflows: Media and entertainment
companies often deal with large video files that require fast access and
secure backup. AWS Storage Gateway can be utilized to move large
video assets from on-premises storage to the cloud for archiving,
transcoding, or content distribution, while maintaining low-latency
access for video editing workstations.
IoT data ingestion and processing: For Internet of Things
deployments, AWS Storage Gateway can play a role in ingesting and
processing data generated by IoT devices. By using the Volume
Gateway mode, IoT data can be collected on-premises and then stored
and analyzed in the cloud using AWS services like Amazon IoT Core
and AWS Lambda.
Automation using Terraform
As mentioned earlier, planning, and architecting organization’s structure is
one of the success criteria when organizations move to the cloud. Use the
step below to create your own organization structure.
1. Copy config folder to the terraform folder for the storage gateway.
Copy provider.tf,.gitignore, provider.tf, and remote_state.tf to
the terraform folder file the storage gateway.
2. Create main.tf with the code below. The code below is used to create
file share storage gateway using ESXi platform:
resource "aws_storagegateway_gateway" "s3-
demo-sgw" {
  # (resource arguments)

  activation_key            = "Put-Your-
Activation-Key-Here"
  gateway_arn               = 
aws_storagegateway_cache.s3-demo-
sgw.gateway_arn
  endpoint_type             = "STANDARD"
  gateway_name              = "s3-demo-sgw"
  gateway_timezone          = "GMT-6:00"
  gateway_type              = "FILE_S3"
  gateway_network_interface = "YOUR-GATEWAY-
IP-ADDRESS"
  host_environment          = "VMWARE"
  id                        = 
"aws_storagegateway_cache.s3-demo-
sgw.gateway_arn.id"
  smb_file_share_visibility = false
  smb_security_strategy     = 
"MandatoryEncryption"
  kms_encrypted             = "true"
  kms_key_arn               = "YOUR-KMS-ARN"
 
  tags                      = {
          "CreatedBy"   = "James Odeyinka"
          "DateCreated" = "02-04-2023"
        }
 

  maintenance_start_time {
          day_of_month   = "25"
          day_of_week    = "1"
          hour_of_day    = "1"
          minute_of_hour = "53"
        }
 
}
resource "aws_storagegateway_gateway" "fxs-
demo-sgw" {
  # (resource arguments)
  activation_key            = "Put-Your-
Activation-Key-Here"
  gateway_arn               = 
aws_storagegateway_cache.fsx-demo-
sgw.gateway_arn
  endpoint_type             = "STANDARD"
  gateway_name              = "fsx-demo-sgw"
  gateway_timezone          = "GMT-6:00"
  gateway_type              = "FILE_FSX_SMB"
  kms_encrypted         = "true"
  kms_key_arn           = "YOUR-KMS-ARN"
  smb_active_directory_settings {
    domain_name = "corp.example.com"
    password    = "avoid-plaintext-passwords"

    username    = "Admin"
  }
  tags                      = {
          "CreatedBy"   = "James Odeyinka"
          "DateCreated" = "03-04-2023"
        }
 
  maintenance_start_time {
          day_of_month   = "25"
          day_of_week    = "1"
          hour_of_day    = "1"
          minute_of_hour = "53"
        }
 
}
resource "aws_storagegateway_gateway" "volume-
cache-demo-sgw" {
  # (resource arguments)
  activation_key            = "Put-Your-
Activation-Key-Here"
  gateway_arn               = 
aws_storagegateway_cache.volume-demo-
sgw.gateway_arn
  endpoint_type             = "STANDARD"

  gateway_name              = "volume-demo-
sgw"
  gateway_timezone          = "GMT-6:00"
    gateway_type            = "CACHED
  kms_encrypted             = "true"
  kms_key_arn               = "YOUR-KMS-ARN"
  smb_active_directory_settings {
    domain_name = "corp.example.com"
    password    = "avoid-plaintext-passwords"
    username    = "Admin"
  }
  tags                      = {
          "CreatedBy"   = "James Odeyinka"
          "DateCreated" = "03-04-2023"
        }
 
  maintenance_start_time {
          day_of_month   = "25"
          day_of_week    = "1"
          hour_of_day    = "1"
          minute_of_hour = "53"
        }
 
}

resource "aws_storagegateway_gateway" "volume-
stored-demo-sgw" {
  # (resource arguments)
  activation_key            = "Put-Your-
Activation-Key-Here"
  gateway_arn               = 
aws_storagegateway_stored.volume-stored-demo-
sgw.gateway_arn
  endpoint_type             = "STANDARD"
  gateway_name              = "volume-demo-
sgw"
  gateway_timezone          = "GMT-6:00"
    gateway_type            = "STORED
  kms_encrypted             = "true"
  kms_key_arn               = "YOUR-KMS-ARN"
  smb_active_directory_settings {
    domain_name = "corp.example.com"
    password    = "avoid-plaintext-passwords"
    username    = "Admin"
  }
  tags                      = {
          "CreatedBy"   = "James Odeyinka"
          "DateCreated" = "03-04-2023"
        }
 

  maintenance_start_time {
          day_of_month   = "25"
          day_of_week    = "1"
          hour_of_day    = "1"
          minute_of_hour = "53"
        }
 
}
resource "aws_storagegateway_gateway" "tape-
demo-sgw" {
  # (resource arguments)
  activation_key            = "Put-Your-
Activation-Key-Here"
  gateway_arn               = 
aws_storagegateway_stored.volume-stored-demo-
sgw.gateway_arn
  endpoint_type             = "STANDARD"
  gateway_name              = "volume-demo-
sgw"
  gateway_timezone          = "GMT-6:00"
  gateway_type              = "VTL"
  medium_changer_type       = "AWS-Gateway-
VTL"
  tape_drive_type           = "IBM-ULT3580-
TD5"
  tags                      = {

          "CreatedBy"   = "James Odeyinka"
          "DateCreated" = "03-04-2023"
        }
 
  maintenance_start_time {
          day_of_month   = "25"
          day_of_week    = "1"
          hour_of_day    = "1"
          minute_of_hour = "53"
        }
 
}
3. Run the following commands to implement the creation and
attachment of SGW:
terraform init --backend-config=config/demo.config  terraform 
plan  terraform apply –auto-approve
4. Push you code to a new branch in our codecommit chapter4-sgw git
push in the mentioned GitHub repository.
Conclusion
In this comprehensive chapter, we delved into the diverse world of AWS
Storage Services, exploring an array of solutions designed to meet varying
storage requirements and cater to a wide spectrum of businesses. AWS,
being a pioneer in cloud computing, offers an impressive line-up of storage
services that empower organizations with flexible, scalable, and cost-
effective storage solutions.
This chapter on AWS Storage Services also presents a panoramic view of the
cloud giant’s storage offerings. Readers are equipped with a deep

understanding of Amazon S3, EBS, EFS, Glacier, DynamoDB, and RDS,
enabling them to architect and optimize storage solutions that align
seamlessly with their organizational needs. Whether building simple
applications or complex data-driven systems, AWS Storage Services
provides the key to unlocking the full potential of cloud storage and database
management.
Multiple choice questions
1. S3 File Objects are only accessible from only within a region.
a. True
b. False
2. Which type of storage service is Amazon S3?
a. Block
b. Simple
c. Secure
d. Object
Answers
1. b
2. a
Join our book’s Discord space
Join the book’s Discord Workspace for Latest updates, Offers, Tech
happenings around the world, New Release and Sessions with the Authors:
https://discord.bpbonline.com


CHAPTER 5
VPC and Network Security Tools
Automation
Introduction
Amazon Virtual Private Cloud (VPC) is a fundamental service in Amazon
Web Services (AWS) that allows users to create isolated virtual networks
within the AWS cloud. With the use of combined power of Terraform,
organizations can leverage its declarative language to automate the
provisioning and configuration of their VPC while ensuring robust network
security.
AWS VPC is composed of components such as subnets, route tables,
security groups, and network Access Control Lists (ACLs). It also covers
different connectivity options available, such as Virtual Private Network
(VPN) and Direct Connect, and the role that each component plays to
establish secure network connections. By grasping the fundamentals
concepts of Amazon VPC, readers will be better equipped to implement
network security measures using Terraform.
Terraform IaC brings reproducibility and version control to cloud
infrastructure management. Readers will learn how to setup and configure
VPC, setup of network access control, security group rules, subnet-level
security, public and private subnet, configuration of route tables, internet
gateways, network gateways, and implementation of bastion hosts using
Terraform.

Finally, AWS VPC also supports a variety of features that enable you to
extend other network infrastructure to the cloud, such as VPN connections,
Direct Connect, and AWS Transit Gateway. In short, AWS VPC provides a
highly customizable and secure networking environment for your
applications and resources in the AWS cloud.
Structure
This chapter covers the following topics:
AWS VPC overview
2-tier Subnet Deployment Architecture overview
3-tier Subnet Deployment Architecture overview
AWS VPC Peering
AWS VPC Monitoring and Auditing
Objectives
In this chapter, we delve into the intricate realm of Amazon Web Services
Virtual Private Cloud (AWS VPC), aiming to equip readers with a
profound understanding of its architecture, components, and capabilities.
Our objective is to navigate the complexities of automating VPC creation,
configuration, and management, empowering readers to confidently design
isolated network environments, establish secure communication between
resources, and seamlessly integrate with other AWS services. By unraveling
the layers of VPC security, connectivity options, and advanced
configurations, this chapter endeavors to foster expertise in harnessing AWS
VPC’s potential to architect resilient, scalable, and adaptable cloud
infrastructures.
By the end of this chapter, you will have a solid understanding of:
VPC fundamentals: Gain a comprehensive overview of what an
Amazon VPC is and its role in establishing a secure and isolated
network environment within AWS.
VPC architecture: Discover the key components that comprise an
Amazon VPC, including subnets, route tables, security groups, and

Network Access Control Lists (NACLs), and understand how they
work together to create a versatile and flexible network infrastructure.
IP addressing and CIDR blocks: Learn about IP addressing within
an Amazon VPC, how CIDR blocks are used for IP range allocation,
and explore strategies for effective IP address management.
Subnet configuration: Explore the concept of subnets and subnetting
within an Amazon VPC and grasp the significance of public and
private subnets in differentiating between resources with public
internet access and those with restricted access.
Connectivity options: Dive into the various connectivity options
available for linking your Amazon VPC with your on-premises
infrastructure, 
including 
Virtual 
Private 
Network 
(VPN)
connections, AWS Direct Connect, and Transit Gateways.
Security in VPC: Understand the fundamental principles of network
security within an Amazon VPC, including the use of security groups
and NACLs to control inbound and outbound traffic and enforce
access policies.
VPC Peering: Explore the concept of VPC peering and how it enables
communication between different VPCs while maintaining isolation,
facilitating 
resource 
sharing, 
and 
streamlining 
inter-VPC
communication.
Hybrid Cloud Architectures: Gain insights into designing and
implementing hybrid cloud architectures, combining the flexibility of
AWS with on-premises infrastructure using technologies like VPN and
Direct Connect.
Lastly, put your knowledge into practice with automation that guides you
through the process of creating a custom Amazon VPC, configuring subnets,
setting up security groups, and establishing connectivity with resources. As
you progress through this chapter, you will gain the knowledge and skills
necessary to create and manage your own isolated network environments
within AWS, empowering you to architect robust and secure cloud-based
solutions that meet the demands of modern businesses.

AWS VPC overview
AWS VPC is a customizable, isolated virtual network environment that
offers unparalleled flexibility and scalability. By seamlessly blending the
benefits of the cloud with the control of an on-premises network, AWS VPC
enables organizations to craft a tailored ecosystem to suit their specific
needs. With AWS VPC, businesses can carve out virtual slices of the AWS
cloud, partitioning resources into subnets, and even creating multiple VPCs
to maintain a multi-tiered architecture. Whether you’re launching a small
web application or architecting a complex enterprise system,
Security is paramount in the cloud domain, and AWS VPC goes above and
beyond to ensure the sanctity of your data. Employing a rich assortment of
security features such as Network Access Control Lists (NACLs), Security
Groups, and Virtual Private Gateways, AWS VPC constructs an impregnable
fortress around your virtual landscape.
But the beauty of AWS VPC lies not just in its rigid security and intricate
architecture. It is a living, breathing ecosystem that adapts to the changing
needs of your business. With Elastic IP addresses and Dynamic Host
Configuration Protocol (DHCP) options, AWS VPC remains ever flexible,
accommodating growth and metamorphosis. AWS VPC lets you establish
encrypted, high-speed connections through Virtual Private Network (VPN)
and AWS Direct Connect, bridging the gap between the old and the new.
In this landscape of endless possibilities, the sky’s not even the limit. With
AWS Transit Gateway, you can create hub-and-spoke architectures that
connect thousands of VPCs, enabling an intergalactic network spanning
across accounts and regions. As your creation thrives, you can monitor and
manage it with ease. AWS provides an array of tools like Amazon
CloudWatch, AWS Config, and AWS CloudTrail, acting as your vigilant
guards, ensuring your VPC’s health, compliance, and auditing requirements
are well attended to.
So, let’s venture forth into the boundless realm of AWS VPC, where dreams
take shape, and innovation knows no bounds. Unlock the potential of a
private cloud within the public cloud and behold the magic that transpires
when security, scalability, and customizability merge harmoniously.
AWS VPC components

Amazon Virtual Private Cloud (VPC) is a fundamental component of
Amazon Web Services (AWS) that allows users to create a private network
in the cloud. It provides a virtual network environment that closely
resembles a traditional data center network, allowing users to launch AWS
resources in a defined virtual network.
Below are some of well-known components of AWS VPC:
Subnet
A VPC can be divided into subnets, which are logical segments of the VPC
that can be isolated from one another. Each subnet can be associated with a
specific Availability Zone (AZ), which is an isolated location within an
AWS region. AZ allows you to create available and fault-tolerant
architectures.
Internet Gateway
An Internet Gateway (IGW) is a horizontally scaled, redundant, and
available VPC component that allows communication between instances in
your VPC and the Internet. It, therefore, imposes no availability risks or
bandwidth constraints on your network traffic.
When VPCs are created, it does not automatically include an IGW. You must
create and attach one to the VPC to enable connectivity to the internet. Once
the IGW is attached to your VPC, your instances have access to the internet
via IPv4 or IPv6.
Elastic IP
AWS Elastic IP is a static, public IPv4 address that can be assigned to an
Amazon Web Services (AWS) resource, such as an Amazon Elastic
Compute Cloud (EC2) instance or a Network Load Balancer.
With Elastic IP, you can associate a persistent IP address with your AWS
resources, even if they are stopped and restarted. This allows you to maintain
the same public IP address, which can be useful for various scenarios such as
hosting a website or application that requires a static IP address.
Network Address Translation Gateway

The AWS NAT Gateway is a managed Network Address Translation
(NAT) service that enables resources within a private subnet to connect to
the Internet or other AWS services, without exposing their private IP
addresses to the public network. It provides an available, scalable, and
secure way for resources in private subnets to access the Internet.
A NAT gateway operates at the network level and translates the private IP
addresses of resources in a private subnet to a public IP address. The NAT
gateway uses an Elastic IP address (EIP) as its public IP address, which
remains static even if the gateway is replaced. The EIP can be associated
with a NAT gateway, network interface, or instance in a VPC.
AWS VPC security group
AWS VPC Security Gateway is a managed service provided by AWS that
allows you to add an additional layer of security to your VPC environment.
The security gateway is designed to protect your VPC from unauthorized
access, malicious attacks, and data breaches.
The security gateway is implemented using a variety of security tools,
including firewalls, intrusion detection and prevention systems, and Security
Information and Event Management (SIEM) systems. It can be used to
monitor and control network traffic, restrict access to specific resources, and
prevent unauthorized access to your VPC.
AWS Network Access Control List
An AWS Network Access Control List (NACL) is a security feature in
AWS that acts as a firewall for controlling inbound and outbound traffic to
and from subnets in a VPC.
NACLs are stateless and operate at the subnet level, meaning they apply to
all the resources within a subnet. They can be used to create rules that allow
or deny traffic based on IP addresses, protocols, and ports. Unlike security
groups, which are stateful and operate at the instance level, NACLs can be
used to block entire protocols, such as Internet Control Message Protocol
(ICMP) or User Datagram Protocol (UDP).
NACLs are numbered, and the rules are processed in order, with the lowest
numbered rule being processed first. Each rule can either allow or deny
traffic; if no rule matches, the default action is to deny traffic.

AWS VPC endpoint
An AWS VPC Endpoint is a connection between a VPC and another AWS
service or a supported AWS Marketplace partner service. It enables you to
privately access AWS services or Marketplace partner services without
requiring an internet gateway, NAT device, VPN connection, or firewall
proxy server.
VPC endpoints are a logical entity that allows you to connect to a service
within your VPC without exposing it to the public internet. It provides an
alternative to using a public IP address or using a NAT gateway, which can
help to improve security, reduce latency, and reduce data transfer costs.
There are two types of VPC endpoints:
Interface endpoints: This type of endpoint is an elastic network
interface with a private IP address that serves as an entry point for
traffic destined for a supported service. It uses private Domain Name
Services (DNS) to resolve the service endpoint and supports security
groups for fine-grained access control.
Gateway endpoints: This type of endpoint is a route table entry that
directs traffic to a specific service. It is used for S3 and DynamoDB
and is simpler to configure than interface endpoints.
AWS VPC peering connection
AWS VPC Peering is a feature that allows you to connect two VPCs in the
same or different regions if they belong to the same AWS account. With
VPC peering, you can create a secure and private connection between VPCs,
and resources in the peered VPCs can communicate with each other as if
they are on the same network.
Here are some key points to keep in mind about AWS VPC Peering:
VPC peering is a regional service, and you can only peer VPCs in the
same region or in different regions that are connected through VPC
peering inter-region connection.
You cannot peer VPCs that have overlapping Classless Inter-Domain
Routing (CIDR) blocks.

VPC peering does not support transitive peering, which means that
you cannot use a VPC peering connection to route traffic to a VPC
that is not directly peered with your VPC.
VPC peering does not allow traffic to transit through an internet
gateway or a VPN connection.
2-Tier Subnet Deployment Architecture overview
A two-tier subnet architecture in Amazon Web Services (AWS) provides a
versatile and efficient way to manage network resources while enhancing
security and scalability. In the two-tier configuration, the public subnet and
the private subnet. The public subnet is designed to host resources that
require direct internet access, such as load balancers or web servers. On the
other hand, the private subnet is shielded from direct internet exposure,
housing sensitive components like application servers or databases. This
separation not only bolsters security by limiting direct access to critical
components but also optimizes performance by allowing traffic to be routed
more efficiently.
One of the key benefits of the AWS two-tier subnet architecture lies in its
heightened security posture. By isolating public-facing and private resources
into separate subnets, the attack surface is significantly reduced. Public
resources can interact with the internet while maintaining a minimal
exposure to potential threats, as the private subnet acts as an additional layer
of defense. Furthermore, network ACLs and Security Groups can be applied
independently to each subnet, enabling granular control over inbound and
outbound traffic flows. This segregation of resources contributes to a more
controlled and secure environment, a crucial consideration in today’s ever-
evolving threat landscape.
Scalability is also a notable advantage of the AWS two-tier subnet
architecture. By distributing resources across multiple subnets, organizations
can allocate resources and manage traffic spikes with greater ease. This
architecture supports the implementation of Auto Scaling groups, allowing
applications to dynamically adjust their capacity based on demand.
Additionally, the modular nature of the two-tier subnet setup simplifies the
process of introducing new components or services. This elasticity
empowers businesses to respond swiftly to changing requirements, whether

it involves accommodating increased user activity or deploying new
functionalities, ultimately fostering a more agile and responsive IT
infrastructure.
Below is the two-tier architecture diagram (see Figure 5.1):
Figure 5.1: Two Tier VPC Subnet Architecture Diagram
2-Tier VPC automation using terraform
Automating the creation of a Two-Tier VPC architecture with Terraform
symbolizes a transformative leap in infrastructure deployment.as well.
In Figure 5.2, the structure of our terraform files is shown. You are expected
to copy the Config folder and remote_state.tf from previous chapters and
put them in the terraform folder you created for this chapter:

Figure 5.2: Deployment Structure for two tier architecture
Please download the codes from our GitHub repository before attempting the
automation steps below:
1. Copy config folder to the folder created for this chapter, chapter5.
2. Copy provider.tf, .gitignore, and remote_state.tf to the chapter5
folder.
3. Create vpc.tf with the following code:
# Creating VPC
resource "aws_vpc" "chapter5-demo-vpc" {
  cidr_block       = "${var.chapter5-demo-
vpc_cidr}"
  instance_tenancy = "default"
  tags = {
    Name = "Chapter5 Demo VPC"

    CreatedBy = "James Odeyinka",
    DateCreated = "02-25-2023",
    Env = "Demo"
  }
}
4. When creating 2-tier subnet. You divide your address space into 2.
Use the first half for public subnet where you host your web servers.
Create subnet.tf with the following code:
# Creating 1st web subnet
resource "aws_subnet" "two-tier-public-subnet-
1a" {
  vpc_id                  = 
"${aws_vpc.chapter5-demo-vpc.id}"
  cidr_block             = "${var.pub-
subnet1a_cidr}"
  map_public_ip_on_launch = true
  availability_zone = "us-east-2a"
  tags = {
    Name = "Demo Subnet Public1A"
        CreatedBy = "James Odeyinka",
    DateCreated = "02-25-2023",
    Env = "Demo"
  }
}
resource "aws_subnet" "two-tier-public-subnet-
1b" {

  vpc_id                  = 
"${aws_vpc.chapter5-demo-vpc.id}"
  cidr_block             = "${var.pub-
subnet1b_cidr}"
  map_public_ip_on_launch = true
  availability_zone = "us-east-2a"
  tags = {
    Name = "Demo Subnet Public1B"
    CreatedBy = "James Odeyinka",
    DateCreated = "02-25-2023",
    Env = "Demo"
  }
}
resource "aws_subnet" "two-tier-private-
subnet-1a" {
  vpc_id                  = 
"${aws_vpc.chapter5-demo-vpc.id}"
  cidr_block             = "${var.pri-
subnet1a_cidr}"
  map_public_ip_on_launch = true
  availability_zone = "us-east-2a"
tags = {
    Name = "Demo Subnet Private1A"
        CreatedBy = "James Odeyinka",
    DateCreated = "02-25-2023",

    Env = "Demo"
  }
}
resource "aws_subnet" "two-tier-private-
subnet-1b" {
  vpc_id                  = 
"${aws_vpc.chapter5-demo-vpc.id}"
  cidr_block             = "${var.pri-
subnet1b_cidr}"
  map_public_ip_on_launch = true
  availability_zone = "us-east-2a"
  tags = {
    Name = "Demo Subnet Private1B"
        CreatedBy = "James Odeyinka",
    DateCreated = "02-25-2023",
    Env = "Demo"
  }
}
5. The following code is for creating Internet Gateway. IGW is an
appliance that can be used to connect a target to internet traffic. It also
allows communication between instances within your VPC. Now
create igw.tf with the code below: # Creating Internet
Gateway
resource "aws_internet_gateway" "chapter5-
demo-internet-gateway" {
  vpc_id = "${aws_vpc.chapter5-demo-vpc.id}"
  tags = {

    Name = "Chapter5 Demo VPC IGW"
    CreatedBy = "James Odeyinka",
    DateCreated = "02-25-2023",
    Env = "Demo"
  }
}
6. Network Access Control List is a layer of security for your VPC that
acts as a firewall for controlling traffic in and out of subnets.
Port 22, 80, and port 1024 to 65535 is allowed into subnets.
Priority numbers are also used to give open ports a priority:
Create nacl.tf with the code below:
# create VPC Network access control list
resource "aws_network_acl" "chapter5-demo-
vpc-nacl" {
  vpc_id = aws_vpc.chapter5-demo-vpc.id
  subnet_ids = [ aws_subnet.two-tier-public-
subnet-1a.id, aws_subnet.two-tier-public-
subnet-1b.id, aws_subnet.two-tier-private-
subnet-1a.id, aws_subnet.two-tier-private-
subnet>  ingress {
    protocol   = "tcp"
    rule_no    = 100
    action     = "allow"
    cidr_block = var.destinationCIDRblock
    from_port  = 22

    to_port    = 22
  }
  # allow ingress port 80
  ingress {
    protocol   = "tcp"
    rule_no    = 200
    action     = "allow"
    cidr_block = var.destinationCIDRblock
    from_port  = 80
    to_port    = 80
  }
  # allow ingress ephemeral ports
  ingress {
    protocol   = "tcp"
    rule_no    = 300
    action     = "allow"
    cidr_block = var.destinationCIDRblock
    from_port  = 1024
    to_port    = 65535
  }
  # allow egress port 22
  egress {
    protocol   = "tcp"

 
rule_no    = 100
    action     = "allow"
    cidr_block = var.destinationCIDRblock
    from_port  = 22
    to_port    = 22
  }
  # allow egress port 80
  egress {
    protocol   = "tcp"
    rule_no    = 200
    action     = "allow"
    cidr_block = var.destinationCIDRblock
    from_port  = 80
    to_port    = 80
  }
  # allow egress ephemeral ports
  egress {
    protocol   = "tcp"
    rule_no    = 300
    action     = "allow"
    cidr_block = var.destinationCIDRblock
    from_port  = 1024
    to_port    = 65535

  }
  tags = {
    Name = "Chapter5 Demo VPC NACL"
    CreatedBy = "James Odeyinka",
    DateCreated = "02-25-2023",
    Env = "Demo"
}
}
7. The following code is for creating Nat Gateway. Nat gateway is an
appliance that can be used to connect a target to internet traffic. Nat
gateway only allows outbound traffic.
Create nat_gw.tf with the following code:
# NAT gateway
resource "aws_nat_gateway" "chapter5-demo-nat-
gateway1a" {
  depends_on = [
    aws_subnet.two-tier-public-subnet-1a,
#       aws_subnet.two-tier-public-subnet-1b,
    aws_eip.chapter5-demo-elastic-ip,
  ]
  allocation_id = aws_eip.chapter5-demo-
elastic-ip.id
  subnet_id     = aws_subnet.two-tier-public-
subnet-1a.id
  tags = {

    Name = "James Demo NAT GW"
    CreatedBy = "James Odeyinka",
    DateCreated = "02-25-2023",
    Env = "Demo"
  }
}
# NAT gateway
resource "aws_nat_gateway" "chapter5-demo-nat-
gateway1b" {
  depends_on = [
#    aws_subnet.two-tier-public-subnet-1a,
        aws_subnet.two-tier-public-subnet-1b,
    aws_eip.chapter5-demo-elastic-ip,
  ]
allocation_id = aws_eip.chapter5-demo-elastic-
ip.id
#  subnet_id     = aws_subnet.two-tier-public-
subnet-1a.id
  subnet_id     = aws_subnet.two-tier-public-
subnet-1b.id
  tags = {
    Name = "James Demo NAT GW"
    CreatedBy = "James Odeyinka",
    DateCreated = "02-25-2023",
    Env = "Demo"

  }
}
8. Elastic IP address is a static public IP address associated with your
AWS resources, EIP is dedicated to customer account until it’s
released:
Create eip.tf with the code below:
# Elastic ip
resource "aws_eip" "chapter5-demo-elastic-ip" 
{
  vpc      = true
  tags = {
    Name = "James Demo NAT GW EIP"
    CreatedBy = "James Odeyinka",
    DateCreated = "02-25-2023",
    Env = "Demo"
  }
}
9. Route tables is used to control where network traffic is directed. Each
subnet can have its routable or multiple route tables.
Create nat_routetable.tf with the following code:
# route table with target as NAT gateway
resource "aws_route_table" "chapter5-demo-nat-
gateway-routetable" {
  depends_on = [
    aws_vpc.chapter5-demo-vpc,

    aws_nat_gateway.chapter5-demo-nat-
gateway1a,
    #aws_nat_gateway.chapter5-demo-nat-
gateway1b,
  ]
  vpc_id = aws_vpc.chapter5-demo-vpc.id
  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_nat_gateway.chapter5-
demo-nat-gateway1a.id
    #gateway_id = aws_nat_gateway.chapter5-
demo-nat-gateway1b.id
  }
  tags = {
    Name = "Chapter5 Demo NAT Gateway Route 
Table "
    CreatedBy = "James Odeyinka",
    DateCreated = "02-25-2023",
    Env = "Demo"
  }
}
# Associate route table to private subnet1a
resource "aws_route_table_association" 
"associate_routetable_to_private_subnet1a" {
  depends_on = [
    aws_subnet.two-tier-private-subnet-1a,

    aws_route_table.chapter5-demo-nat-gateway-
routetable,
  ]
  subnet_id      = aws_subnet.two-tier-
private-subnet-1a.id
  route_table_id = aws_route_table.chapter5-
demo-nat-gateway-routetable.id
}
# Associate route table to private subnet1b
resource "aws_route_table_association" 
"associate_routetable_to_private_subnet1b" {
  depends_on = [
    aws_subnet.two-tier-private-subnet-1b,
    aws_route_table.chapter5-demo-nat-gateway-
routetable,
  ]
  subnet_id      = aws_subnet.two-tier-
private-subnet-1b.id
  route_table_id = aws_route_table.chapter5-
demo-nat-gateway-routetable.id
}
10. We will now create a private subnet security group where the data and
database is hosted.
Let us create private_subnet_sg.tf with the following code:
# Create Database Security Group
resource "aws_security_group" "chapter5-demo-
private-sg" {

  name        = "Private Subnet SG"
  description = "Allow inbound traffic from 
application layer"
  vpc_id      = aws_vpc.chapter5-demo-vpc.id
  ingress {
    description     = "Allow traffic from 
application layer"
    from_port       = 443
    to_port         = 443
    protocol        = "tcp"
  }
  ingress {
    description     = "Allow traffic from 
application layer"
        from_port       = 22
    to_port         = 22
    protocol        = "tcp"
  }
  egress {
    from_port   = 32768
    to_port     = 65535
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
  tags = {

    CreatedBy = "James Odeyinka",
    DateCreated = "02-25-2023",
    Env = "Demo"
  }
}
11. Create a public subnet security group where the web server is hosted.
Let us create public_subnet_sg.tf with the following code:
# Creating Security Group
resource "aws_security_group" "chapter5-demo-
public-sg" {
  name        = "Public Subnet SG"
  vpc_id = "${aws_vpc.chapter5-demo-vpc.id}"
  # Inbound Rules
  # HTTP access from James House
  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["69.47.200.180/32"]
  }
  # HTTPS access from James House
  ingress {
    from_port   = 443
    to_port     = 443

    protocol    = "tcp"
    cidr_blocks = ["69.47.200.180/32"]
  }
  # GoPhish Port access from James House
  ingress {
    from_port   = 3333
    to_port     = 3333
    protocol    = "tcp"
    cidr_blocks = ["69.47.200.180/32"]
  }
  # SSH access from James House
  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["69.47.200.180/32"]
}
  # Outbound Rules
  # Internet access to James House
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"

    cidr_blocks = ["0.0.0.0/0"]
  }
  tags = {
    CreatedBy = "James Odeyinka",
    DateCreated = "02-25-2023",
    Env = "Demo"
  }
}
Route tables are used to control where network traffic is directed.
12. Create vpc_routetable.tf with the following code:
# Creating Route Table
resource "aws_route_table" "route" {
    vpc_id = "${aws_vpc.chapter5-demo-vpc.id}"
    route {
        cidr_block = "0.0.0.0/0"
        gateway_id = 
"${aws_internet_gateway.chapter5-demo-
internet-gateway.id}"
    }
    tags = {
    Name = "James Demo Public Route Table"
        CreatedBy = "James Odeyinka",
    DateCreated = "02-25-2023",
    Env = "Demo"
  }

}
# Associating Route Table
resource "aws_route_table_association" 
"chapter5-demo-public-route1a" {
    subnet_id = "${aws_subnet.two-tier-public-
subnet-1a.id}"
    route_table_id = 
"${aws_route_table.route.id}"
}
# Associating Route Table
resource "aws_route_table_association" 
"chapter5-demo-public-route1b" {
    subnet_id = "${aws_subnet.two-tier-public-
subnet-1b.id}"
    route_table_id = 
"${aws_route_table.route.id}"
}
13. Run the following commands to implement the creation of the VPC.
Do not forget to name your config file for the demo or your
environment properly:
 terraform init --backend-
config=config/demo.config
 terraform plan
 terraform apply –auto-approve
14. Push your code to a new branch in our CodeCommit shared with you
for chapter5-vpc git push in the mentioned GitHub repository.
3-Tier Subnet Deployment Architecture overview

An AWS 3-tier subnet architecture offers a robust foundation for building
scalable and highly available applications within the Amazon Web Services
(AWS) cloud environment. This architecture is structured into three distinct
tiers: the presentation tier, the application tier, and the data tier. Each tier is
deployed within separate subnets to enhance security, isolate components,
and optimize performance.
The presentation tier, also known as the web tier, hosts web servers that
handle user requests and present data. By placing this tier in a dedicated
subnet, organizations can implement security groups and Network Access
Control Lists (NACLs) to control incoming traffic and protect against
potential threats. Furthermore, the separation of tiers allows for targeted
scaling, enabling businesses to allocate resources specifically where they’re
needed most.
Sitting in the middle, the application tier processes business logic and serves
as a bridge between the presentation and data tiers. Here, application servers
execute complex tasks, transforming user inputs into meaningful actions and
retrieving data from the data tier. By isolating the application tier in its own
subnet, AWS users can configure security groups to manage incoming and
outgoing traffic, minimizing the attack surface, and ensuring that only
necessary communication occurs between components.
At the base of the architecture, the data tier stores and manages the
application’s data. Databases, both relational and non-relational, reside
within this tier to provide data storage and retrieval services. Placing the data
tier in a separate subnet helps control access to sensitive information, while
also optimizing data traffic for efficient retrieval and storage operations. This
layered approach not only enhances security but also simplifies maintenance
and scalability by allowing each tier to be individually scaled and updated as
needed.
Finally, the AWS 3-tier subnet architecture exemplifies a well-structured
approach to designing applications that prioritize security, scalability, and
performance. By segregating presentation, application, and data layers into
distinct subnets, organizations can fine-tune security settings, minimize
attack vectors, and ensure efficient resource utilization. This approach
contributes to the creation of robust, high-performance applications within
the AWS cloud ecosystem.

Following is the three-tier architecture Figure 5.3:
Figure 5.3: Three Tier VPC Subnet Architecture Diagram
3-Tier VPC Automation using Terraform
Follow all the steps in 3-Tier VPC Automation and make the following
modifications to the code:
1. Modify subnet.tf created for 2-tier with the following code:
The modification below will turn the subnet into 3-Tier by creating
two private subnets and one public subnet.
# Creating 1st web subnet
resource "aws_subnet" "three-tier-public-
subnet-1a" {

  vpc_id                  = 
"${aws_vpc.chapter5-demo-vpc.id}"
  cidr_block             = "${var.pub-
subnet1a_cidr}"
  map_public_ip_on_launch = true
  availability_zone = "us-east-2a"
  tags = {
    Name = "Demo Subnet Public1A"
        CreatedBy = "James Odeyinka",
    DateCreated = "02-25-2023",
    Env = "Demo"
  }
}
resource "aws_subnet" "three-tier-public-
subnet-1b" {
  vpc_id                  = 
"${aws_vpc.chapter5-demo-vpc.id}"
  cidr_block             = "${var.pub-
subnet1b_cidr}"
  map_public_ip_on_launch = true
  availability_zone = "us-east-2a"
  tags = {
    Name = "Demo Subnet Public1B"
    CreatedBy = "James Odeyinka",
    DateCreated = "02-25-2023",

    Env = "Demo"
  }
}
resource "aws_subnet" "three-tier-private-
subnet-1a" {
  vpc_id                  = 
"${aws_vpc.chapter5-demo-vpc.id}"
  cidr_block             = "${var.pri-
subnet1a_cidr}"
  map_public_ip_on_launch = true
  availability_zone = "us-east-2a"
 
tags = {
    Name = "Demo Subnet Private1A"
        CreatedBy = "James Odeyinka",
    DateCreated = "02-25-2023",
    Env = "Demo"
  }
}
resource "aws_subnet" "three-tier-private-
subnet-1b" {
  vpc_id                  = 
"${aws_vpc.chapter5-demo-vpc.id}"
  cidr_block             = "${var.pri-
subnet1b_cidr}"
  map_public_ip_on_launch = true

  availability_zone = "us-east-2a"
  tags = {
    Name = "Demo Subnet Private1B"
        CreatedBy = "James Odeyinka",
    DateCreated = "02-25-2023",
    Env = "Demo"
  }
}
resource "aws_subnet" "three-tier-private-
subnet-2a" {
  vpc_id                  = 
"${aws_vpc.chapter5-demo-vpc.id}"
  cidr_block             = "${var.pri-
subnet2a_cidr}"
  map_public_ip_on_launch = true
  availability_zone = "us-east-2a"
 
tags = {
    Name = "Demo Subnet Private2A"
        CreatedBy = "James Odeyinka",
    DateCreated = "02-25-2023",
    Env = "Demo"
  }
}

resource "aws_subnet" "three-tier-private-
subnet-2b" {
  vpc_id                  = 
"${aws_vpc.chapter5-demo-vpc.id}"
  cidr_block             = "${var.pri-
subnet2b_cidr}"
  map_public_ip_on_launch = true
  availability_zone = "us-east-2a"
  tags = {
    Name = "Demo Subnet Private2B"
        CreatedBy = "James Odeyinka",
    DateCreated = "02-25-2023",
    Env = "Demo"
  }
}
2. Create nacl.tf with the following code:
# create VPC Network access control list
resource "aws_network_acl" "chapter5-demo-vpc-
nacl" {
  vpc_id = aws_vpc.chapter5-demo-vpc.id
  subnet_ids = [ aws_subnet.three-tier-public-
subnet-1a.id, aws_subnet.three-tier-public-
subnet-1b.id, aws_subnet.three-tier-private-
subnet-1a.id, aws_subnet.three-tier-private-
subnet1b.id, aws_subnet.three-tier-private-
subnet-2a.id, aws_subnet.three-tier-private-
subnet2b.id ]  ingress {

    protocol   = "tcp"
    rule_no    = 100
    action     = "allow"
    cidr_block = var.destinationCIDRblock
    from_port  = 22
    to_port    = 22
  }
  # allow ingress port 80
  ingress {
    protocol   = "tcp"
    rule_no    = 200
    action     = "allow"
    cidr_block = var.destinationCIDRblock
    from_port  = 80
    to_port    = 80
  }
  # allow ingress ephemeral ports
  ingress {
    protocol   = "tcp"
    rule_no    = 300
    action     = "allow"
    cidr_block = var.destinationCIDRblock
    from_port  = 1024

    to_port    = 65535
  }
  # allow egress port 22
  egress {
    protocol   = "tcp"
 
rule_no    = 100
    action     = "allow"
    cidr_block = var.destinationCIDRblock
    from_port  = 22
    to_port    = 22
  }
  # allow egress port 80
  egress {
    protocol   = "tcp"
    rule_no    = 200
    action     = "allow"
    cidr_block = var.destinationCIDRblock
    from_port  = 80
    to_port    = 80
  }
  # allow egress ephemeral ports
  egress {
    protocol   = "tcp"

    rule_no    = 300
    action     = "allow"
    cidr_block = var.destinationCIDRblock
    from_port  = 1024
    to_port    = 65535
  }
  tags = {
    Name = "Chapter5 Demo VPC NACL"
    CreatedBy = "James Odeyinka",
    DateCreated = "02-25-2023",
    Env = "Demo"
}
}
3. Modify vpc_routetable.tf with the following code:
# Creating Route Table
resource "aws_route_table" "route" {
    vpc_id = "${aws_vpc.chapter5-demo-vpc.id}"
    route {
        cidr_block = "0.0.0.0/0"
        gateway_id = 
"${aws_internet_gateway.chapter5-demo-
internet-gateway.id}"
    }
    tags = {

    Name = "James Demo Public Route Table"
        CreatedBy = "James Odeyinka",
    DateCreated = "02-25-2023",
    Env = "Demo"
  }
}
# Associating Route Table
resource "aws_route_table_association" 
"chapter5-demo-public-route1a" {
    subnet_id = "${aws_subnet.three-tier-
public-subnet-1a.id}"
    route_table_id = 
"${aws_route_table.route.id}"
}
# Associating Route Table
resource "aws_route_table_association" 
"chapter5-demo-public-route1b" {
    subnet_id = "${aws_subnet.three-tier-
public-subnet-1b.id}"
    route_table_id = 
"${aws_route_table.route.id}"
}
4. Modify nat_routetable.tf with the following code:
# route table with target as NAT gateway
resource "aws_route_table" "chapter5-demo-nat-
gateway-routetable" {

  depends_on = [
    aws_vpc.chapter5-demo-vpc,
    aws_nat_gateway.chapter5-demo-nat-
gateway1a,
    #aws_nat_gateway.chapter5-demo-nat-
gateway1b,
  ]
  vpc_id = aws_vpc.chapter5-demo-vpc.id
  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_nat_gateway.chapter5-
demo-nat-gateway1a.id
    #gateway_id = aws_nat_gateway.chapter5-
demo-nat-gateway1b.id
  }
  tags = {
    Name = "Chapter5 Demo NAT Gateway Route 
Table "
    CreatedBy = "James Odeyinka",
    DateCreated = "02-25-2023",
    Env = "Demo"
  }
}
# Associate route table to private subnet1a
resource "aws_route_table_association" 
"associate_routetable_to_private_subnet1a" {

  depends_on = [
    aws_subnet.three-tier-private-subnet-1a,
    aws_route_table.chapter5-demo-nat-gateway-
routetable,
  ]
  subnet_id      = aws_subnet.three-tier-
private-subnet-1a.id
  route_table_id = aws_route_table.chapter5-
demo-nat-gateway-routetable.id
}
# Associate route table to private subnet1b
resource "aws_route_table_association" 
"associate_routetable_to_private_subnet1b" {
  depends_on = [
    aws_subnet.three-tier-private-subnet-1b,
    aws_route_table.chapter5-demo-nat-gateway-
routetable,
  ]
  subnet_id      = aws_subnet.three-tier-
private-subnet-1b.id
  route_table_id = aws_route_table.chapter5-
demo-nat-gateway-routetable.id
}
# Associate route table to private subnet2a
resource "aws_route_table_association" 
"associate_routetable_to_private_subnet2a" {
  depends_on = [

    aws_subnet.three-tier-private-subnet-2a,
    aws_route_table.chapter5-demo-nat-gateway-
routetable,
  ]
  subnet_id      = aws_subnet.three-tier-
private-subnet-2a.id
  route_table_id = aws_route_table.chapter5-
demo-nat-gateway-routetable.id
}
# Associate route table to private subnet2b
resource "aws_route_table_association" 
"associate_routetable_to_private_subnet2b" {
  depends_on = [
    aws_subnet.three-tier-private-subnet-2b,
    aws_route_table.chapter5-demo-nat-gateway-
routetable,
  ]
  subnet_id      = aws_subnet.three-tier-
private-subnet-2b.id
  route_table_id = aws_route_table.chapter5-
demo-nat-gateway-routetable.id
}
5. Modify nat_gw.tf with the following code:
# NAT gateway
resource "aws_nat_gateway" "chapter5-demo-nat-
gateway1a" {
  depends_on = [

    aws_subnet.three-tier-public-subnet-1a,
#       aws_subnet.three-tier-public-subnet-
1b,
    aws_eip.chapter5-demo-elastic-ip,
  ]
  allocation_id = aws_eip.chapter5-demo-
elastic-ip.id
  subnet_id     = aws_subnet.three-tier-
public-subnet-1a.id
  tags = {
    Name = "James Demo NAT GW"
    CreatedBy = "James Odeyinka",
    DateCreated = "02-25-2023",
    Env = "Demo"
  }
}
# NAT gateway
resource "aws_nat_gateway" "chapter5-demo-nat-
gateway1b" {
  depends_on = [
#    aws_subnet.three-tier-public-subnet-1a,
        aws_subnet.three-tier-public-subnet-
1b,
    aws_eip.chapter5-demo-elastic-ip,
  ]

allocation_id = aws_eip.chapter5-demo-elastic-
ip.id
#  subnet_id     = aws_subnet.three-tier-
public-subnet-1a.id
  subnet_id     = aws_subnet.three-tier-
public-subnet-1b.id
  tags = {
    Name = "James Demo NAT GW"
    CreatedBy = "James Odeyinka",
    DateCreated = "02-25-2023",
    Env = "Demo"
  }
}
6. Modify var.tf with the following code:
# Defining CIDR Block for VPC
variable "chapter5-demo-vpc_cidr" {
  default = "10.20.0.0/20"
}
variable "destinationCIDRblock" {
    default = "0.0.0.0/0"
}
# Defining CIDR Block for 1st Subnet
variable "pub-subnet1a_cidr" {
  default = "10.20.1.0/24"
}

# Defining CIDR Block for 2nd Subnet
variable "pub-subnet1b_cidr" {
  default = "10.20.2.0/24"
}
# Defining CIDR Block for 1st Subnet
variable "pri-subnet1a_cidr" {
  default = "10.20.3.0/24"
}
# Defining CIDR Block for 2nd Subnet
variable "pri-subnet1b_cidr" {
  default = "10.20.4.0/24"
}
# Defining CIDR Block for 1st Subnet
variable "pri-subnet2a_cidr" {
  default = "10.20.5.0/24"
}
# Defining CIDR Block for 2nd Subnet
variable "pri-subnet1b_cidr" {
  default = "10.20.6.0/24"
}
7. You have now modified all the necessary codes, and you should have
all the files needed. Run the commands below to implement the
creation of the 3-Tier VPC. Do not forget to name your config file for
the demo or your environment properly:

terraform init --backend-config=config/demo.config
 terraform plan
 terraform apply –auto-approve
8. Push your code to a new branch in our CodeCommit chapter5-vpc-
3Tier: git push in the mentioned GitHub repository.
AWS VPC Peering
AWS VPC peering is a networking connection between two or more VPCs
that enables them to communicate with each other as if they are in the same
network. VPC peering is a way to connect two VPCs within the same AWS
region. It allows instances in one VPC to communicate directly with
instances in the other VPC using private IP addresses, without traversing the
public internet.
VPC peering is established between two VPCs using private IP addresses.
The IP address ranges of the VPCs must not overlap, and both VPCs must be
in the same AWS region. Once VPC peering is established, you can create
route tables to route traffic between the two VPCs. You can also use security
groups and network Access Control Lists (ACLs) to control access to
resources in the peered VPC.
VPC peering is a one-to-one relationship between two VPCs. To connect
more than two VPCs, you need to establish separate peering connections for
each pair of VPCs. VPC peering is a cost-effective way to connect VPCs
within the same region, as it does not require any additional hardware or
software. However, there may be additional data transfer costs depending on
the amount of traffic between the VPCs. Overall, VPC peering is a
convenient way to connect two VPCs within the same AWS region, allowing
them to communicate with each other securely and efficiently.
AWS VPC peering architecture
AWS VPC peering is a networking connection between two or more VPCs
that enables them to communicate with each other as if they are in the same
network.
VPC peering is a way to connect two VPCs within the same AWS region. It
allows instances in one VPC to communicate directly with instances in the

other VPC using private IP addresses, without traversing the public internet.
VPC peering is established between two VPCs using private IP addresses.
The IP address ranges of the VPCs must not overlap, and both VPCs must be
in the same AWS region.
Once VPC peering is established, you can create route tables to route traffic
between the two VPCs. You can also use security groups and network ACLs
to control access to resources in the peered VPC. Peering is a one-to-one
relationship between two VPCs. To connect more than two VPCs, you need
to establish separate peering connections for each pair of VPCs.
VPC peering is a cost-effective way to connect VPCs within the same
region, as it does not require any additional hardware or software. However,
there may be additional data transfer costs depending on the amount of
traffic between the VPCs.
Overall, VPC peering is a convenient way to connect two VPCs within the
same AWS region, allowing them to communicate with each other securely
and efficiently (see Figure 5.4):
Figure 5.4: VPC Peering Architecture Diagram
AWS VPC Peering Automation using Terraform
Create vpc_peering.tf with the below code:
1. It is assumed that you already have two VPCs running in your AWS
Account. You can deploy 2-tier subnet codes shared with you above.

You must change the address space of one of the deployments
because 2 VPC that need to peer cannot have same address:
# Create the VPC Connection
resource "aws_vpc_peering_connection" 
"chapter5-demo-vpc2chapter5-demo-vpcb" {
  # Main VPC ID.
  vpc_id = "${aws_vpc.chapter5-demo-vpc.id}"
  # AWS Account ID. This can be dynamically 
queried using the
  # aws_caller_identity data resource.
  # 
https://www.terraform.io/docs/providers/aws/d/
caller_identity.html
  peer_owner_id = 
"${data.aws_caller_identity.current.account_id
}"
  # Secondary VPC ID.
  peer_vpc_id = "${aws_vpc.chapter5-demo-
vpcb.id}"
  # Flags that the peering connection should 
be automatically confirmed. This
  # only works if both VPCs are owned by the 
same account.
  auto_accept = true
}
2. Create vpc_peering_primary_vpc_route.tf with the below code:
resource "aws_route" "chapter5-demo-
vpc2chapter5-demo-vpcb-route" {

  # ID of VPC 1 main route table.
  route_table_id = "${aws_vpc.chapter5-demo-
vpc.main_route_table_id}"
  # CIDR block / IP range for VPC 2.
  destination_cidr_block = 
"${aws_vpc.chapter5-demo-vpcb.cidr_block}"
  # ID of VPC peering connection.
  vpc_peering_connection_id = 
"${aws_vpc_peering_connection.chapter5-demo-
vpc2chapter5-demo-vpcb.id}"
}
3. Create vpc_peering_secondary_vpc_route.tf with the following
code:
resource "aws_route" "chapter5-demo-
vpcb2chapter5-demo-vpc-route" {
  # ID of VPC 1 main route table.
  route_table_id = "${aws_vpc.chapter5-demo-
vpcb.main_route_table_id}"
  # CIDR block / IP range for VPC 2.
  destination_cidr_block = 
"${aws_vpc.chapter5-demo-vpc.cidr_block}"
  # ID of VPC peering connection.
  vpc_peering_connection_id = 
"${aws_vpc_peering_connection.chapter5-demo-
vpcb2chapter5-demo-vpc.id}"
}
4. You have now modified all the necessary codes, and you should have
all the files. Run the following commands to implement the creation

of the 3-Tier VPC. Do not forget to name your config file for the
demo or your environment properly:
terraform init –backend-config=config/demo.config terraform
plan terraform apply –auto-approve
5. Push your code to a new branch in our CodeCommit chapter5-vpc-
peering in the mentioned GitHub repository.
AWS VPC Monitoring and Auditing
AWS Virtual Private Cloud (VPC) Monitoring is an indispensable aspect
of managing a secure and optimized cloud environment. As organizations
increasingly adopt cloud infrastructure, the need to monitor VPCs becomes
paramount for maintaining network health and ensuring data integrity. VPC
monitoring involves tracking network traffic, resource utilization, and
security events within the VPC ecosystem. VPC can be monitored through
tools like Amazon CloudWatch, administrators gain insights into network
flows, latency metrics, and the overall health of resources within the VPC.
By proactively identifying bottlenecks, unusual patterns, or potential security
breaches, VPC monitoring empowers businesses to take swift actions to
enhance performance, enforce compliance, and fortify their cloud
architecture.
In addition to monitoring, auditing is a critical component of ensuring the
robustness and compliance of AWS Virtual Private Clouds. Auditing
involves a systematic examination of VPC configurations, access controls,
and network policies to validate adherence to security standards and
regulatory requirements. By conducting regular audits, organizations can
identify and rectify misconfigurations, unauthorized access, or potential
vulnerabilities within their VPC environments. AWS CloudTrail is a
valuable tool for auditing VPC activities, providing a detailed log of API
calls and changes to resources. This comprehensive approach to VPC
auditing enhances the overall security posture, demonstrating a commitment
to proactive governance and risk management in the cloud infrastructure.
Conclusion

AWS Virtual Private Cloud is a service that enables customers to launch
their own logically isolated cloud infrastructure within AWS. Here is a
summary of what you have accomplished with AWS VPC:
You created VPC which is a virtual network dedicated to a single
AWS account. Which enables you to launch AWS resources, such as
Amazon Elastic Compute Cloud instances, into a virtual network that
you define.
You defined a VPC’s IP address range, created subnets, and
configured route tables, network gateways, and security settings. This
gives you complete control over your virtual network.
You can also connect your VPC to your on-premises data center using
a VPN connection or AWS Direct Connect. This allows you to extend
your data center into the cloud while keeping it isolated from other
VPCs and the public internet.
AWS VPC provides several security features to help protect your
resources. For example, you can create security groups and network
ACLs to control inbound and outbound traffic to your instances, and
you can use AWS Identity and Access Management to control access
to your resources.
AWS VPC also integrates with other AWS services, such as AWS
Elastic Load Balancing, AWS Auto Scaling, and AWS Elastic Block
Store, enabling you to build scalable and highly available applications.
Overall, AWS VPC provides a secure and flexible way to launch your
own virtual network in the cloud, with full control over IP addressing,
subnets, and routing.
Multiple choice questions
1. VPC cannot be peered across regions:
a. True
b. False
2. How many Internet gateways can be attached to a VPC:

a. 1
b. 2
c. 3
d. 4
Answers
1. b
2. a

CHAPTER 6
Automating EC2 Deployment of
various Workloads
Introduction
In today’s rapidly evolving digital landscape, deploying and managing
workloads in the cloud has become a critical aspect of modern business
operations. Amazon Elastic Compute Cloud (EC2) is a powerful and
scalable cloud computing service offered by Amazon Web Services (AWS)
that enables organizations to deploy and manage virtual servers in the
cloud.
This chapter guides the readers through a step-by-step process of defining
the infrastructure as code, provisioning the necessary EC2 instances, and
configuring additional resources as required. The readers will learn how to
deploy web application, high performing computing, and serverless
architecture using terraform IaC.
Amazon EC2 is a cornerstone service in the AWS ecosystem, offering on-
demand virtual servers in the cloud. EC2 provides a wide range of instance
types, storage options, and networking capabilities to meet diverse
workload requirements. With EC2, organizations can scale their compute
resources needs up or down based on demand, pay only for what they have
consumed, and leverage the global reach and reliability of AWS data
centres.

Structure
In this chapter, we will go through the following topics:
Overview of AWS Elastic Compute
Overview of AWS Amazon Machine image
Advance EC2 instances types and their use cases
Deploying Web Application multi-EC2 Web Server General Purpose
using Terraform
Deploying EC2 Availability Group for application
Deploying EC2 Auto Scaling Group
Objectives
In this chapter, our primary objective is to delve into the realm of EC2
deployment automation using the powerful infrastructure-as-code tool,
Terraform. With a focus on efficiency and repeatability, we will navigate
the landscape of modern cloud deployment practices, showcasing how
Terraform can seamlessly orchestrate the provisioning and management of
Amazon EC2 instances. By the end of this chapter, readers will gain a
comprehensive understanding of not only the fundamental concepts of
Terraform and EC2 but also the strategic methodologies to automate and
optimize their infrastructure deployment processes. Through hands-on
examples, best practices, and real-world insights, we will empower readers
to harness Terraforms capabilities, enabling them to establish a robust
foundation for deploying EC2 instances while embracing the agility and
reliability that automation brings to cloud environments.
Overview of AWS Elastic Compute
AWS Elastic Compute Cloud (EC2) stands as a cornerstone within
Amazon Web Services’ comprehensive suite of cloud computing offerings.
Renowned for its scalability and flexibility, EC2 empowers businesses and
individuals to rent virtualized computing resources on-demand. Through
this service, users can effortlessly launch and manage virtual servers, also

known as instances, tailored to their specific needs. Whether the
requirement is for a single instance or a complex cluster of instances
working together, EC2 provides the infrastructure necessary to
accommodate computing demands ranging from basic applications to
resource-intensive workloads.
One of the most striking features of AWS EC2 is its remarkable
adaptability. Users can select instance types optimized for various use
cases, such as general-purpose, memory-intensive, compute-intensive, and
GPU-powered instances. This versatility ensures that no matter the task at
hand, EC2 can offer the appropriate blend of processing power, memory,
and storage. This agility extends to the ability to choose the operating
system, networking configurations, security settings, and even pricing
models, allowing users to finely calibrate their environment to align with
performance, security, and budgetary considerations.
EC2’s significance is further elevated by its integration with other AWS
services. Seamlessly combining with services like Amazon Elastic Block
Store (EBS) for scalable storage, Amazon VPC for networking, and
Amazon CloudWatch for monitoring and management, EC2 becomes a
pivotal component within a holistic cloud ecosystem. This interconnectivity
simplifies the deployment of robust, reliable, and high-performance
applications by enabling users to leverage a comprehensive suite of tools
and services that complement EC2’s computational capabilities.
Lastly, AWS Elastic Compute Cloud emerges as a linchpin in the realm of
cloud computing, revolutionizing the way businesses and individuals’
access and manage computational resources. Its dynamic nature allows
users to tailor virtual instances to their specific requirements, while its
integration with a myriad of AWS services forms a cohesive and potent
cloud environment. By virtue of EC2’s unrivaled scalability and flexibility,
the possibilities for innovation and growth within the cloud computing
landscape become virtually limitless.
Overview of AWS Amazon Machine image
An Amazon Machine Image (AMI) within the Amazon Web Services
ecosystem serves as a cornerstone for streamlined and efficient cloud
computing solutions. AMIs can be envisioned as pre-configured blueprints,

encompassing an entire operating system, application stack, and associated
configurations. These images act as the building blocks for virtual server
instances, expediting the process of deployment while ensuring consistency
and repeatability. Ranging from fundamental OS variations to complex
software frameworks, AWS offers an extensive library of public AMIs. This
enables users to effortlessly replicate environments, scale applications, and
even collaborate by sharing customized AMIs, culminating in an
accelerated and agile cloud computing experience.
At its essence, an Amazon Machine Image represents the encapsulation of
computational power within the Amazon Web Services ecosystem. It
embodies the quintessential formula for rapidly and reliably reproducing
instances of virtual servers. Think of AMIs as digital snapshots – frozen
moments in time containing the entirety of an operating system, software
applications, data, and even unique configurations. This snapshot can be
duplicated into multiple virtual machines, facilitating the creation of
identical environments for development, testing, and production. AWS
users can harness the prowess of AMIs not only to minimize setup overhead
but also to establish security and governance measures consistently across
the cloud infrastructure. From foundational templates to intricate
architectures, AMIs stand as the catalyst for fortifying the efficiency and
consistency of cloud-based operations.
In the intricate tapestry of Amazon Web Services, Amazon Machine Images
weave a tale of computational empowerment. An AMI essentially embodies
the digital DNA of a virtual server, encompassing an array of genetic
information, from the chosen operating system to the installed software
packages and configured settings. This encapsulation is pivotal in
catapulting the efficiency of cloud deployment and scaling processes.
Organizations can craft and curate personalized AMIs tailored to their
unique requirements, thus shortening the time-to-market for new
applications. Beyond mere convenience, the ability to share custom-crafted
AMIs among team members fosters collaboration, while the option to
secure and control access to proprietary configurations enhances the
robustness of the cloud infrastructure. In the grand narrative of AWS, AMIs
stand as the elemental building blocks of innovation, enabling businesses to
paint their computational canvases with unprecedented efficiency and
dynamism.

AMI Automation using Terraform
AMIs serve as templates for creating virtual machines, making it easier to
replicate and scale your infrastructure. To streamline the process of
managing and deploying AMIs, Terraform, an infrastructure as code tool,
can be employed.
Defining AMIs in Terraform
Following is how to automate AMI management using Terraform on AWS:
1. Use the following code to create provider.tf file and run terraform
init command:
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 4.0"
    }
  }
}
provider "aws" {
  region = "us-east-2"
}
2. Use the following code to create .gitignore file:
#  Local .terraform directories
**/.terraform/*
# .tfstate files
*.tfstate

*.tfstate.*
# .tfvars files
*.tfvars
.vscode
*.pem
# Crash log files
crash.log
# Ignore any .tfvars files that are generated 
automatically for each Terraform run. Most
# .tfvars files are managed as part of 
configuration and so should be included in
# version control.
#
*.tfvars
# Ignore override files as they are usually 
used to override resources locally and so
# are not checked in
override.tf
override.tf.json
*_override.tf
*_override.tf.json
# Include override files you do wish to add to 
version control using negated pattern
#
# !example_override.tf

# Include tfplan files to ignore the plan 
output of command: terraform plan -out=tfplan
*tfplan*
3. Create a config folder inside Chapter6 using mkdir config command.
4. Then create a remote_state.tf file with the code below. State file
knows what bucket to use at the backend based on the config folder.
Now 
use 
this 
command 
terraform 
init 
--backend-
config=config/demo.config to re-initialize the folder:
terraform {
  backend "s3" {
  region = "us-east-2"
  }
}
5. Create 
and 
setup 
the 
domain 
in 
AWS 
Route 
53-
https://aws.amazon.com/route53/ using terraform code below. See
the AWS instruction-
https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/do
main-transfer-to-route-53.html to learn more on how to transfer
domain.
6. Use the following code to create "s3.tf" file. and run terraform
init command:
resource "aws_s3_bucket_object" "this" {
  for_each = fileset("/files/", "*")
  bucket = var.aws_s3_bucket
  key    = "/files/${each.value}"
  source = "/files/${each.value}"
  # If the md5 hash is different it will re-
upload

  etag = filemd5("/files/${each.value}")
}
data "aws_kms_key" "image_builder" {
  key_id = "alias/image-builder"
}
# Amazon Cloudwatch agent component
resource "aws_imagebuilder_component" 
"cw_agent" {
  name       = "amazon-cloudwatch-agent-linux"
  platform   = "Linux"
  uri        = 
"s3://${var.aws_s3_bucket}/files/amazon-
cloudwatch-agent-linux.yml"
  version    = "1.0.1"
  kms_key_id = 
data.aws_kms_key.image_builder.arn
  depends_on = [
    aws_s3_bucket_object.this
  ]
  lifecycle {
    create_before_destroy = true
  }
}
7. Use the following code to create component.tf file:
resource "aws_s3_bucket_object" "this" {

  for_each = fileset("/files/", "*")
  bucket = var.aws_s3_bucket
  key    = "/files/${each.value}"
  source = "/files/${each.value}"
  # If the md5 hash is different it will re-
upload
  etag = filemd5("/files/${each.value}")
}
data "aws_kms_key" "image_builder" {
  key_id = "alias/image-builder"
}
# Amazon Cloudwatch agent component
resource "aws_imagebuilder_component" 
"cw_agent" {
  name       = "amazon-cloudwatch-agent-linux"
  platform   = "Linux"
  uri        = 
"s3://${var.aws_s3_bucket}/files/amazon-
cloudwatch-agent-linux.yml"
  version    = "1.0.1"
  kms_key_id = 
data.aws_kms_key.image_builder.arn
  depends_on = [
    aws_s3_bucket_object.this
  ]

  lifecycle {
    create_before_destroy = true
  }
}
8. Use the following code to create dist_config.tf file:
resource 
"aws_imagebuilder_distribution_configuration" 
"this" {
  name = "local-distribution"
  distribution {
    ami_distribution_configuration {
      ami_tags = {
        CostCenter = "IT"
      }
      name = "example-{{ 
imagebuilder:buildDate }}"
      launch_permission {
        # user_ids = ["123456789012"]
      }
    }
    region = var.aws_region
  }
}
9. Use the following code to create image_builder_policy.tf file:

data "aws_iam_policy_document" "image_builder" 
{
  statement {
    effect = "Allow"
    actions = [
      "ssm:DescribeAssociation",
      
"ssm:GetDeployablePatchSnapshotForInstance",
      "ssm:GetDocument",
      "ssm:DescribeDocument",
      "ssm:GetManifest",
      "ssm:GetParameter",
      "ssm:GetParameters",
      "ssm:ListAssociations",
      "ssm:ListInstanceAssociations",
      "ssm:PutInventory",
      "ssm:PutComplianceItems",
      "ssm:PutConfigurePackageResult",
      "ssm:UpdateAssociationStatus",
      "ssm:UpdateInstanceAssociationStatus",
      "ssm:UpdateInstanceInformation",
      "ssmmessages:CreateControlChannel",
      "ssmmessages:CreateDataChannel",
      "ssmmessages:OpenControlChannel",

      "ssmmessages:OpenDataChannel",
      "ec2messages:AcknowledgeMessage",
      "ec2messages:DeleteMessage",
      "ec2messages:FailMessage",
      "ec2messages:GetEndpoint",
      "ec2messages:GetMessages",
      "ec2messages:SendReply",
      "imagebuilder:GetComponent",
    ]
    resources = ["*"]
  }
  statement {
    effect = "Allow"
    actions = [
      "s3:List",
      "s3:GetObject"
    ]
    resources = ["*"]
  }
  statement {
    effect = "Allow"
    actions = [
      "s3:PutObject"

    ]
    resources = 
["arn:aws:s3:::${var.aws_s3_log_bucket}/image-
builder/*"]
  }
  statement {
    effect = "Allow"
    actions = [
      "logs:CreateLogStream",
      "logs:CreateLogGroup",
      "logs:PutLogEvents"
    ]
    resources = ["arn:aws:logs:*:*:log-
group:/aws/imagebuilder/*"]
  }
  statement {
    effect = "Allow"
    actions = [
      "kms:Decrypt"
    ]
    resources = ["*"]
    condition {
      test     = "ForAnyValue:StringEquals"
      variable = "kms:EncryptionContextKeys"

      values = [
        "aws:imagebuilder:arn"
      ]
    }
    condition {
      test     = "ForAnyValue:StringEquals"
      variable = "aws:CalledVia"
      values = [
        "imagebuilder.amazonaws.com"
      ]
    }
  }
}
10. Use the following code to create main.tf file:
# Create the EC2 IAM role to use for the image
module "image_builder_role" {
  ec2_iam_role_name  = var.ec2_iam_role_name
  policy_description = "IAM ec2 instance 
profile for the Image Builder instances."
  assume_role_policy = file("files/assumption-
policy.json")
  policy             = 
data.aws_iam_policy_document.image_builder.jso
n
}

data "aws_region" "current" {}
data "aws_partition" "current" {}
data "aws_subnet" "this" {
  filter {
    name   = "tag:Name"
    values = ["default-public-1d"]
  }
}
data "aws_security_group" "this" {
  filter {
    name   = "tag:Name"
    values = ["default-sec-group"]
  }
}
11. Now, use the following code to create pipeline.tf file:
resource "aws_imagebuilder_image_pipeline" 
"this" {
  name                             = 
var.ami_name_tag
  status                           = "ENABLED"
  description                      = "Creates 
an AMI."
  image_recipe_arn                 = 
aws_imagebuilder_image_recipe.this.arn

  infrastructure_configuration_arn = 
aws_imagebuilder_infrastructure_configuration.
this.arn
  distribution_configuration_arn   = 
aws_imagebuilder_distribution_configuration.th
is.arn
  schedule {
    schedule_expression = "cron(0 8 ? * tue)"
    # This cron expressions states every 
Tuesday at 8 AM.
    pipeline_execution_start_condition = 
"EXPRESSION_MATCH_AND_DEPENDENCY_UPDATES_AVAIL
ABLE"
  }
  # Test the image after build
  image_tests_configuration {
    image_tests_enabled = true
    timeout_minutes     = 60
  }
  tags = {
    "Name" = "${var.ami_name_tag}-pipeline"
  }
  depends_on = [
    aws_imagebuilder_image_recipe.this,
    
aws_imagebuilder_infrastructure_configuration.
this

  ]
}
12. You can now use the following code to create recipe.tf file:
resource "aws_imagebuilder_image" "this" {
  distribution_configuration_arn   = 
aws_imagebuilder_distribution_configuration.th
is.arn
  image_recipe_arn                 = 
aws_imagebuilder_image_recipe.this.arn
  infrastructure_configuration_arn = 
aws_imagebuilder_infrastructure_configuration.
this.arn
  depends_on = [
    
data.aws_iam_policy_document.image_builder,
    aws_imagebuilder_image_recipe.this,
    
aws_imagebuilder_distribution_configuration.th
is,
    
aws_imagebuilder_infrastructure_configuration.
this
  ]
}
resource "aws_imagebuilder_image_recipe" 
"this" {
  block_device_mapping {
    device_name = "/dev/xvdb"

    ebs {
      delete_on_termination = true
      volume_size           = 
var.ebs_root_vol_size
      volume_type           = "gp3"
    }
  }
  component {
    component_arn = 
aws_imagebuilder_component.cw_agent.arn
  }
  name         = "amazon-linux-recipe"
  parent_image = 
"arn:${data.aws_partition.current.partition}:i
magebuilder:${data.aws_region.current.name}:aw
s:image/amazon-linux-2-x86/x.x.x"
  version      = var.image_receipe_version
  lifecycle {
    create_before_destroy = true
  }
  depends_on = [
    aws_imagebuilder_component.cw_agent
  ]
}
13. Now use the following code to create svr_config.tf file:

resource 
"aws_imagebuilder_infrastructure_configuration
" "this" {
  description                   = "Simple 
infrastructure configuration"
  instance_profile_name         = 
var.ec2_iam_role_name
  instance_types                = ["t2.micro"]
  key_pair                      = 
var.aws_key_pair_name
  name                          = "amazon-
linux-infr"
  security_group_ids            = 
[data.aws_security_group.this.id]
  subnet_id                     = 
data.aws_subnet.this.id
  terminate_instance_on_failure = true
  logging {
    s3_logs {
      s3_bucket_name = var.aws_s3_log_bucket
      s3_key_prefix  = "image-builder"
    }
  }
  tags = {
    Name = "amazon-linux-infr"
  }

}
14. Use the following code to create terraform.tfvars file:
ec2_iam_role_name = "svc-image-builder-role"
ebs_root_vol_size = 10
aws_key_pair_name = "image-builder-kp"
ami_name_tag      = "amzn-linux2"
15. Use the following code to create tf_config.tf file:
terraform {
  required_providers {
    aws = {
      source = "hashicorp/aws"
    }
  }
}
provider "aws" {
  region  = var.aws_region
  profile = var.aws_cli_profile
}
terraform {
  backend "s3" {}
}
16. Now follow code to create tf_config.tf file.
variable "aws_region" {
  type        = string

  description = "The AWS region."
}
variable "aws_cli_profile" {
  type        = string
  description = "The AWS CLI profile name."
}
variable "ec2_iam_role_name" {
  type        = string
  description = "The EC2's IAM role name."
}
variable "aws_s3_log_bucket" {
  type        = string
  description = "The S3 bucket name to send 
logs to."
}
variable "aws_s3_bucket" {
  type        = string
  description = "The S3 bucket name that 
stores the Image Builder componeent files."
}
variable "ebs_root_vol_size" {
  type = number
}
variable "aws_key_pair_name" {

  type = string
}
variable "image_receipe_version" {
  type = string
}
variable "ami_name_tag" {
  type = string
}
17. Now, run the following two commands to finish the process of setup
after you have validated the domain transfer via the email:
terraform plan
terraform apply –approve-auto
18. Push your code to a new branch in our codecommit chapter6 git push
in the mentioned GitHub repository.
Note: You can download Chapter6 code file and change the necessary
parameters to fit your needs and run the command in #16 and #17.
Advance EC2 instance types and their use cases
Amazon Web Services EC2 instances have evolved to push the boundaries
of computing capabilities, catering to diverse workloads with remarkable
efficiency. The advanced EC2 instance types represent a pinnacle of
innovation, delivering unparalleled processing power, memory, and
specialized resources. These instances are meticulously crafted to address
specific use cases, from compute-intensive tasks like High-Performance
Computing (HPC) and data analytics, to memory-intensive applications
such as in-memory databases and real-time analytics. By seamlessly
merging technological prowess with targeted design, advanced EC2

instance types empower businesses to scale their operations, accelerate
innovation, and conquer new frontiers in the digital landscape.
Different EC2 instance types and their use cases
Amazon Elastic Compute Cloud offers a diverse range of advanced instance
types, engineered to meet the demands of diverse workloads and
applications. These instances combine cutting-edge hardware, optimized
performance, and specialized features to empower businesses with the
flexibility and scalability needed to succeed in the modern digital
landscape.
This overview delves into a selection of these advanced EC2 instance types,
highlighting their unique attributes and strategic use cases:
Compute-optimized instances: These instances, such as the C7g and
C7gn families, excel in delivering high compute power for CPU-
bound workloads. Ideal for applications requiring substantial
processing capabilities, like data analytics, batch processing, and
scientific simulations.
Memory-optimized instances: Instances like the R7g and R7iz
families are designed to cater to memory-intensive workloads,
including large-scale databases, in-memory caching, and real-time
big data analytics. They provide ample memory resources to prevent
bottlenecks and ensure smooth performance.
Storage-optimized instances: Suited for data-intensive tasks,
instances like the I4g and Im4gn families offer exceptional storage
capacity and high I/O throughput. Use cases encompass NoSQL
databases, data warehousing, and applications requiring rapid access
to large datasets.
Accelerated computing instances: Tailored for tasks demanding
massive parallel processing, instances like the P5 and P4 families are
equipped with GPUs for accelerated computational power. These are
indispensable for machine learning, high-performance computing
(HPC), and rendering applications.

Network-optimized instances: Designed for applications reliant on
fast and consistent network performance, instances like the N6g and
N5 families ensure minimal latency and high network throughput.
Use cases include content delivery, video streaming, and online
gaming.
Burstable performance instances: Instances such as the T4g family
are built for workloads that exhibit variable resource demands. These
instances offer baseline performance with the ability to burst when
needed, 
making 
them 
suitable 
for 
development 
and 
test
environments, as well as small-scale applications.
AI-optimized instances: Specialized for artificial intelligence and
machine learning workloads, instances like the Inf1 family are
equipped with AWS Inferential chips. They provide cost-effective
and efficient inference processing for AI models.
High Performance Computing (HPC) instances: Instances such as
the HPc7g and HPc7ga families cater to computationally demanding
scientific and engineering simulations. They provide high-core counts
and significant memory, enabling rapid simulations and data analysis.
Instance families with local NVMe storage: Instances like the I3en
family offer local NVMe storage, making them ideal for applications
needing low-latency, high-speed storage. Use cases encompass data
warehousing, analytics, and high-performance databases.
Secure instances with EC2 Nitro Enclave: Instances featuring the
EC2 Nitro Enclave technology provide isolated and secure execution
environments for sensitive workloads. This is critical for industries
dealing with sensitive data like financial services and healthcare.
In conclusion, Amazon EC2’s advanced instance types are meticulously
crafted to accommodate a wide spectrum of business requirements. By
choosing the right instance type for specific workloads, enterprises can
optimize performance, reduce costs, and bolster their ability to innovate
across various industries and domains.
Configuring network and security for EC2

When creating EC2 instances, configuring network and security settings for
your EC2 instances is a critical aspect of ensuring a secure and well-
performing environment in the cloud. By following best practices for VPC
design, subnet configuration, security groups, ACLs, and other networking
components, you can create a robust and protected infrastructure to support
your applications and services on Amazon EC2.
Deploy web application on Single EC2 General Purpose
The General Purpose EC2 instances stand as a testament to versatility in the
digital realm. Seamlessly melding computational power with flexibility,
these instances embody the essence of adaptability in the ever-evolving
landscape of cloud services. Their prowess lies in their ability to cater to a
diverse array of workloads, from web hosting to app deployment, all while
maintaining a harmonious balance between CPU, memory, and network
resources.
Note: You can download Chapter6/single_web_ec2 code file and
change the necessary parameters to fit your needs and run the
commands in #16 and #17.
Following are the steps to automate EC2 deployment using Terraform:
1. Copy config folder from chapter6/single_web_ec2 code file to your
own chapter6/single_web_ec2 folder.
2. Copy modules folder from chapter6/single_web_ec2 code file to your
own chapter6/single_web_ec2 folder.
3. Copy .ignore file from chapter6/single_web_ec2 folder to your own
chapter6/single_web_ec2 folder or use the following code:
#  Local .terraform directories
**/.terraform/*
# .tfstate files
*.tfstate
*.tfstate.*

# .tfvars files
*.tfvars
.vscode
*.pem
4. Copy variable.tf file from chapter6/single_web_ec2 folder to your
own chapter6/single_web_ec2 folder or use the following code:
               variable "namespace" {
  description = "The project namespace to use 
for unique resource naming"
  default     = "Single-EC2-Demo"
  type        = string
}
variable "region" {
  description = "AWS region"
  default     = "us-east-2"
  type        = string
}
5. Copy remote_state.tf file from chapter6/single_web_ec2 folder to
your own chapter6/single_web_ec2 folder or use the following code:
terraform {
  backend "s3" {
  region = "us-east-2"
  }
}

6. Copy provider.tf file from chapter6/single_web_ec2 folder to your
own chapter6/single_web_ec2 folder or use the following code:
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 4.0"
    }
  }
}
provider "aws" {
  region = var.region
}
7. Copy output.tf file from chapter6/single_web_ec2 folder to your
own chapter6/single_web_ec2 folder or use the following code:
output "public_connection_string" {
  description = "Copy/Paste/Enter - You are in 
the Public Server"
  value       = "ssh -i ${module.ssh-
key.key_name}.pem ec2-
user@${module.ec2.public_ip}"
}
output "private_connection_string" {
  description = "Copy/Paste/Enter - You are in 
the Private Server"

  value       = "ssh -i ${module.ssh-
key.key_name}.pem ec2-
user@${module.ec2.private_ip}"
}
8. Copy main.tf file from chapter6/single_web_ec2 folder to your own
chapter6/single_web_ec2 folder or use the following code:
module "vpc" {
  source    = "./modules/vpc"
  namespace = var.namespace
}
module "ssh-key" {
  source    = "./modules/ssh-key"
  namespace = var.namespace
}
module "ec2" {
  source     = "./modules/ec2"
  namespace  = var.namespace
  vpc        = module.vpc.vpc
  sg_pub_id  = module.vpc.sg_pub_id
  sg_priv_id = module.vpc.sg_priv_id
  key_name   = module.ssh-key.key_name
}
Note: Ensure that your .gitignore have an entry to exempt .pem
files. This will avoid you from mistakenly pushing your ssh
private key.

9. Run terraform commands below to deploy the infrastructure. You
should have 23 resources to deploy if everything runs correctly:
terraform init --backend-config=config/demo.config
terraform plan
terraform apply –approve-auto
10. Validate the output and login to the server to validate your
deployments. Your output should look like below:
Output:
Apply complete! Resources: 23 added, 0 
changed, 0 destroyed.
Outputs:
private_connection_string = "ssh -i EC2-Demo-
key.pem ec2-user@10.10.2.45"
public_connection_string = "ssh -i EC2-Demo-
key.pem ec2-user@3.133.128.173"
root@web-
lnxnginx:/home/jodeyinka/WORKSPACE/awsca-demo-
repo/chapter6/terraform/single_ec2#            
Now login to the public server with the following command:
ssh -i EC2-Demo-key.pem ec2-user@3.133.128.173
You are now in the public EC2 server as shown as follows:
root@web-
lnxnginx:/home/jodeyinka/WORKSPACE/awsca-demo-
repo/chapter6/terraform/single_ec2# ssh -i 
EC2-Demo-key.pem ec2-user@3.133.128.173
The authenticity of host '3.133.128.173 
(3.133.128.173)' can't be established.
ECDSA key fingerprint is 
SHA256:4h+i3mTz0GDkUrY2nnx0pqvgYgyTR0/6ZfOR+1a

T3gE.
Are you sure you want to continue connecting 
(yes/no/[fingerprint])? yes
Warning: Permanently added '3.133.128.173' 
(ECDSA) to the list of known hosts.
Last login: Sun Sep 10 00:02:38 2023 from 
207.172.40.22
       __|  __|_  )
       _|  (     /   Amazon Linux 2 AMI
      ___|\___|___|
https://aws.amazon.com/amazon-linux-2/
package(s) needed for security, out of 23 
available
11. Run sudo yum update to apply all updates.
12. Push you code to a new branch in our codecommit chapter6-single-
web-ec2 git push in the mentioned GitHub repository.
Deploying web application multi-EC2 Web Server General
Purpose using Terraform
Implementing load balancing is a strategic imperative in establishing a
robust framework for high availability. By intelligently distributing
incoming network traffic or computational workloads across multiple
servers or resources, load balancing ensures optimal utilization and prevents
any single point of failure from disrupting the system. Whether through
traditional hardware load balancers or modern software-defined solutions,
achieving high availability mandates an orchestrated equilibrium that
dynamically adapts to fluctuations in demand. This seamless distribution
not only enhances reliability but also minimizes response times, thus
cultivating an uninterrupted user experience. As digital landscapes evolve,
integrating load balancing mechanisms becomes not only a technical

necessity but a cornerstone for building resilient and unwaveringly
dependable systems.
Note: You can download Chapter6/single_web_ec2 code file and
change the necessary parameters to fit your needs, and run the
commands in #16 and #17.
Below are the steps to automate multi-EC2 with load balancer deployment
using Terraform:
1. Copy config folder from chapter6/multi_web_ec2_alb code file to
your own chapter6/multi_web_ec2_alb folder.
2. Copy modules folder from chapter6/multi_web_ec2_alb code file to
your own chapter6/multi_web_ec2_alb folder.
3. Copy .ignore file from chapter6/single_web_ec2 folder to your own
chapter6/multi_web_ec2_alb folder or use the following code:
#  Local .terraform directories
**/.terraform/*
# .tfstate files
*.tfstate
*.tfstate.*
# .tfvars files
*.tfvars
.vscode
*.pem
4. Copy variable.tf file from chapter6/multi_web_ec2_alb folder to
your own chapter6/multi_web_ec2_alb folder or use the following
code:
  variable "namespace" {

  description = "The project namespace to use 
for unique resource naming"
  default     = "Multi-EC2-Demo"
  type        = string
}
variable "region" {
  description = "AWS region"
  default     = "us-east-2"
  type        = string
}
5. Copy remote_state.tf file from chapter6/multi_web_ec2_alb folder
to your own chapter6/multi_web_ec2_alb folder or use the following
code:
terraform {
  backend "s3" {
  region = "us-east-2"
  }
}
6. Now, copy provider.tf file from chapter6/multi_web_ec2_alb
folder to your own chapter6/multi_web_ec2_alb folder or use the
following code:
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"

      version = "~> 4.0"
    }
  }
}
provider "aws" {
  region = var.region
}
7. Copy output.tf file from chapter6/multi_web_ec2_alb folder to
your own chapter6/multi_web_ec2_alb folder or use the following
code:
output "public_connection_string" {
  description = "Copy/Paste/Enter - You are in 
the Public Server"
  value       = "ssh -i ${module.ssh-
key.key_name}.pem ec2-
user@${module.ec2.public_ip}"
}
output "public_connection_string2" {
  description = "Copy/Paste/Enter - You are in 
the Public Server 2"
  value       = "ssh -i ${module.ssh-
key.key_name}.pem ec2-
user@${module.ec2.public_ip2}"
}
output "alb" {
  description = "Copy/Paste/Enter - You are in 
the Public Server 2"

  value       = "ssh -i ${module.ssh-
key.key_name}.pem ec2-
user@${module.ec2.public_ip2}"
}
8. Copy main.tf file from chapter6/multi_web_ec2_alb folder to your
own chapter6/multi_web_ec2_alb folder or use the following code:
module "vpc" {
  source    = "./modules/vpc"
  namespace = var.namespace
}
module "ssh-key" {
  source    = "./modules/ssh-key"
  namespace = var.namespace
}
module "ec2" {
  source     = "./modules/ec2"
  namespace  = var.namespace
  vpc        = module.vpc.vpc
  sg_pub_id  = module.vpc.sg_pub_id
  sg_priv_id = module.vpc.sg_priv_id
  key_name   = module.ssh-key.key_name
}
9. Copy alb.tf file from chapter6/multi_web_ec2_alb folder to your
own chapter6/multi_web_ec2_alb folder or use the following code:
# Create target group

resource "aws_lb_target_group" "multiec2-demo-
alb-target-group" {
  name     = "Multi EC2 Demo ALB Target Group"
  port     = 80
  protocol = "HTTP"
  vpc_id   = aws_vpc.vpc.id
  health_check {
    enabled             = true
    healthy_threshold   = 3
    interval            = 10
    matcher             = 200
    path                = "/"
    port                = "traffic-port"
    protocol            = "HTTP"
    timeout             = 3
    unhealthy_threshold = 2
  }
}
# Attach the EC2 to the Load Balancer
resource "aws_lb_target_group_attachment" 
"attach-app1" {
  #count            = length(aws_instance.app-
server)
   count           = 
length(aws_instance.ec2_public)

  target_group_arn = 
aws_lb_target_group.multiec2-demo-alb.arn
  #target_id        = 
element(aws_instance.app-server.*.id, 
count.index)
  target_id        = 
element(aws_instance.ec2_public.*.id, 
count.index)
  port             = 80
}
# Create the Load Balancer Listener
resource "aws_lb_listener" "multiec2-demo-alb-
listener" {
  load_balancer_arn = aws_lb.multiec2-demo-
alb.arn
  port              = "80"
  protocol          = "HTTP"
  default_action {
    type             = "forward"
    target_group_arn = 
aws_lb_target_group.multiec2-demo-alb.arn
  }
}
# Create the Load Balancer
resource "aws_lb" "multiec2-demo-alb" {
  name               = "Multi EC2 Demo ALB"

  internal           = false
  load_balancer_type = "application"
  #security_groups    = 
[aws_security_group.allow_ssh_pub.id]
   security_gtroups  = [var.sg_pub_id]
  #subnets            = [for subnet in 
aws_subnet.private : subnet.id]
  subnet_id          = 
var.vpc.public_subnets[0]
  enable_deletion_protection = false
  tags = {
    CreatedBy   = "<Your-Name-Here>"
    DateCreated = "$date"
    Environment = "Demo"
  }
}
Note: Ensure that your .gitignore have an entry to exempt .pem
files. This will avoid you from mistakenly pushing your ssh
private key.
10. Run terraform commands below to deploy the infrastructure. You
should have 23 resources to deploy if everything runs correctly:
terraform init --backend-config=config/demo.config
terraform plan
terraform apply –approve-auto
Validate the output and login to the server to validate your
deployments. Your output should look like below:

Output:
Apply complete! Resources: 23 added, 0 
changed, 0 destroyed.
Outputs:
private_connection_string = "ssh -i EC2-Demo-
key.pem ec2-user@10.10.2.45"
public_connection_string = "ssh -i EC2-Demo-
key.pem ec2-user@3.133.128.173"
root@web-
lnxnginx:/home/jodeyinka/WORKSPACE/awsca-demo-
repo/chapter6/terraform/single_ec2#            
Now login to the public server with the following command:
ssh -i EC2-Demo-key.pem ec2-user@3.133.128.173
11. You are now in the public EC2 server as shown below:
root@web-
lnxnginx:/home/jodeyinka/WORKSPACE/awsca-demo-
repo/chapter6/terraform/single_ec2# ssh -i 
EC2-Demo-key.pem ec2-user@3.133.128.173
The authenticity of host '3.133.128.173 
(3.133.128.173)' can't be established.
ECDSA key fingerprint is 
SHA256:4h+i3mTz0GDkUrY2nnx0pqvgYgyTR0/6ZfOR+1a
T3gE.
Are you sure you want to continue connecting 
(yes/no/[fingerprint])? yes
Warning: Permanently added '3.133.128.173' 
(ECDSA) to the list of known hosts.
Last login: Sun Sep 10 00:02:38 2023 from 
207.172.40.22

       __|  __|_  )
       _|  (     /   Amazon Linux 2 AMI
     ___|\___|___|
https://aws.amazon.com/amazon-linux-2/
6 package(s) needed for security, out of 26 
available
Run "sudo yum update" to apply all updates
Push you code to a new branch in our codecommit chapter6-multi-
ec2-alb 
git 
push 
https://git-codecommit.us-east-
1.amazonaws.com/v1/repos/awsca-demo-repo.
Deploying EC2 Availability Group for application
An essential facet of modern cloud architecture, the EC2 Availability Group
stands as a resilient fortress for applications, ensuring unwavering service
even in the face of adversity. Like the vigilant guardians of digital realms,
this group dynamically orchestrates the distribution of computational
workloads across multiple virtual bastions. Thus, applications hosted within
its fortified embrace gain the power to gracefully navigate the tumultuous
seas of server failures and fluctuations, offering users an experience as
seamless as the gentlest breeze despite the underlying storms. Just as a
conductor masterfully guides an orchestra, the EC2 Availability Group
conducts a symphony of reliability, where each virtual instance plays its
part, harmonizing together to deliver a crescendo of uninterrupted
performance, echoing across the digital landscape.
Note: You can download Chapter6/availability_group_ec2 code file
and change the necessary parameters to fit your needs.
Following are the steps to automate Availability Group EC2 with load
balancer deployment using Terraform:

1. Copy config folder from chapter6/availability_group_ec2_alb
code file to your own chapter6/availability_group_ec2_alb folder.
2. Copy modules folder from chapter6/availability_group_ec2_alb
code file to your own chapter6/availability_group_ec2_alb folder.
3. Copy .ignore file from chapter6/single_web_ec2 folder to your own
chapter6/availability_group_ec2_alb folder or use the following
code:
#  Local .terraform directories
**/.terraform/*
# .tfstate files
*.tfstate
*.tfstate.*
# .tfvars files
*.tfvars
.vscode
*.pem
4. Copy variable.tf file from chapter6/availability_group_ec2_alb
folder to your own chapter6/availability_group_ec2_alb folder or
use the following code:
variable "namespace" {
  description = "The project namespace to use 
for unique resource naming"
  default     = "Availability Group EC2-Demo"
  type        = string
}
variable "region" {

  description = "AWS region"
  default     = "us-east-2"
  type        = string
}
5. Now 
copy 
provider.tf 
file 
from
chapter6/availability_group_ec2_alb 
folder 
to 
your 
own
chapter6/availability_group_ec2_alb folder or use the following
code:
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 4.0"
    }
  }
}
provider "aws" {
  region = "us-east-2"
}
6. Copy output.tf file from chapter6/availability_group_ec2_alb
folder to your own chapter6/availability_group_ec2_alb folder or
use the following code:
output "public_connection_string" {
  description = "Copy/Paste/Enter - You are in 
the Public Server"

  value       = "ssh -i ${module.ssh-
key.key_name}.pem ec2-
user@${module.ec2.public_ip}"
}
output "public_connection_string1" {
  description = "Copy/Paste/Enter - You are in 
the Public Server"
  value       = "ssh -i ${module.ssh-
key.key_name}.pem ec2-
user@${module.ec2.public_ip1}"
}
output "public_connection_string2" {
  description = "Copy/Paste/Enter - You are in 
the Public Server"
  value       = "ssh -i ${module.ssh-
key.key_name}.pem ec2-
user@${module.ec2.public_ip2}"
}
output "private_connection_string" {
  description = "Copy/Paste/Enter - You are in 
the Private Server"
  value       = "ssh -i ${module.ssh-
key.key_name}.pem ec2-
user@${module.ec2.private_ip}"
}
output "private_connection_string1" {
  description = "Copy/Paste/Enter - You are in 
the Private Server"

  value       = "ssh -i ${module.ssh-
key.key_name}.pem ec2-
user@${module.ec2.private_ip1}"
}
7. Copy main.tf file from 
chapter6/availability_group_ec2_alb
folder to your own chapter6/availability_group_ec2_alb folder or
use the following code:
module "vpc" {
  source    = "./modules/vpc"
  namespace = var.namespace
}
module "ssh-key" {
  source    = "./modules/ssh-key"
  namespace = var.namespace
}
module "ec2" {
  source     = "./modules/ec2"
  namespace  = var.namespace
  vpc        = module.vpc.vpc
  sg_pub_id  = module.vpc.sg_pub_id
  sg_priv_id = module.vpc.sg_priv_id
  key_name   = module.ssh-key.key_name
}
8. Run terraform commands below to deploy the infrastructure. You
should have 23 resources to deploy if everything runs correctly:
terraform init --backend-config=config/demo.config

terraform plan
terraform apply –approve-auto
9. Validate the output and login to the server to validate your
deployments. Your output should look similar to below:
Apply complete! Resources: 23 added, 0 
changed, 0 destroyed.
Outputs:
private_connection_string = "ssh -i EC2-Demo-
key.pem ec2-user@10.10.2.45"
public_connection_string = "ssh -i EC2-Demo-
key.pem ec2-user@3.133.128.173"
root@web-
lnxnginx:/home/jodeyinka/WORKSPACE/awsca-demo-
repo/chapter6/terraform/single_ec2#             
Now login to the public server with the 
following command:
 
ssh -i EC2-Demo-key.pem ec2-
user@3.133.128.173
10. You are now in the public EC2 server as shown below:
root@web-
lnxnginx:/home/jodeyinka/WORKSPACE/awsca-demo-
repo/chapter6/terraform/single_ec2# ssh -i 
EC2-Demo-key.pem ec2-user@3.133.128.173
The authenticity of host '3.133.128.173 
(3.133.128.173)' can't be established.
ECDSA key fingerprint is 
SHA256:4h+i3mTz0GDkUrY2nnx0pqvgYgyTR0/6ZfOR+1a
T3gE.
Are you sure you want to continue connecting 
(yes/no/[fingerprint])? yes

Warning: Permanently added '3.133.128.173' 
(ECDSA) to the list of known hosts.
Last login: Sun Sep 10 00:02:38 2023 from 
207.172.40.22
       __|  __|_  )
       _|  (     /   Amazon Linux 2 AMI
      ___|\___|___|
https://aws.amazon.com/amazon-linux-2/
6 package(s) needed for security, out of 27 
available
11. Run sudo yum update to apply all updates.
12. Push your code to a new branch in our codecommit chapter6-
availability-group-ec2 git push in the mentioned GitHub repository.
Deploying EC2 Auto Scaling Group
An EC2 Auto Scaling group orchestrates the dynamic expansion and
contraction of Amazon Elastic Compute Cloud instances with an
unparalleled finesse. This ingenious service ensures that your application’s
availability remains unwavering, deftly adapting to traffic fluctuations by
seamlessly adding or removing instances as needed. The Auto Scaling
group operates as an agile conductor of computational resources,
conducting a symphony of elasticity that harmonizes with your
application’s demand crescendos. With its configuration simplicity and
automated lifecycle management, it imbues your infrastructure with a self-
healing resilience, adeptly navigating failures and rebalancing the
ensemble. In the ever-changing landscape of cloud computing, the EC2
Auto Scaling group stands as a monument to operational fluidity, allowing
your architecture to not only weather surges but also dance gracefully with
the rhythm of demand.

Note: You can download Chapter6/auto_scaling_group_ec2 code file
and change the necessary parameters to fit your needs.
Following are the steps to automate Availability Group EC2 with load
balancer deployment using Terraform:
1. Copy the config folder from chapter6/auto_scaling_group_ec2 code
file to your own chapter6/auto_scaling_group_ec2 folder.
2. Copy the .ignore file from chapter6/ auto_scaling_group_ec2 folder
to your own chapter6/auto_scaling_group_ec2 folder or use the
code below:
#  Local .terraform directories
**/.terraform/*
# .tfstate files
*.tfstate
*.tfstate.*
# .tfvars files
*.tfvars
.vscode
*.pem
# Crash log files
crash.log
# Ignore any .tfvars files that are generated 
automatically for each Terraform run. Most
# .tfvars files are managed as part of 
configuration and so should be included in
# version control.

#
*.tfvars
# Ignore override files as they are usually 
used to override resources locally and so
# are not checked in
override.tf
override.tf.json
*_override.tf
*_override.tf.json
# Include override files you do wish to add to 
version control using negated pattern
#
# !example_override.tf
# Include tfplan files to ignore the plan 
output of command: terraform plan -out=tfplan
*tfplan*
3. Copy provider.tf file from chapter6/auto_scaling_group_ec2 folder
to your own chapter6/auto_scaling_group_ec2 folder or use the
following code:
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 4.0"
    }

  }
}
provider "aws" {
  region = "us-east-2"
}
4. Copy remote_state.tf file from chapter6/ auto_scaling_group_ec2
folder to your own chapter6/auto_scaling_group_ec2 folder or use
the following code:
terraform {
  backend "s3" {
  region = "us-east-2"
  }
}
5. Copy variable.tf file from chapter6/ auto_scaling_group_ec2
folder to your own chapter6/auto_scaling_group_ec2 folder or use
the following code:
# Defining Public Key
variable "public_key" {
  default = "autoscaling-demo.pub"
}
# Defining Private Key
variable "private_key" {
  default = "autoscaling-demo.pem"
}
# Definign Key Name for connection

variable "key_name" {
  default = "autoscaling-demo"
  description = "Name of AWS key pair"
}
# Defining CIDR Block for VPC
variable "vpc_cidr" {
  default = "10.10.0.0/20"
}
# Defining CIDR Block for Subnet
variable "subnet_cidr" {
  default = "10.10.1.0/24"
}
# Defining CIDR Block for 2d Subnet
variable "subnet1_cidr" {
  default = "10.10.2.0/24"
}
6. Copy vpc.tf file from chapter6/ auto_scaling_group_ec2 folder to
your own chapter6/auto_scaling_group_ec2 folder or use the code
below.
resource "aws_vpc" "autoscaling-demo-vpc" {
  cidr_block       = "${var.vpc_cidr}"
  instance_tenancy = "default"
tags = {
    Name = "Auto Scaling Demo VPC"

  }
}
7. Copy subnet.tf file from chapter6/auto_scaling_group_ec2 folder
to your own chapter6/auto_scaling_group_ec2 folder or use the
following code:
# Creating public subnet 
resource "aws_subnet" "autoscaling-demo-
public-subnet" {
  vpc_id                  = 
"${aws_vpc.autoscaling-demo-vpc.id}"
  cidr_block             = 
"${var.subnet_cidr}"
  map_public_ip_on_launch = true
  availability_zone = "us-east-2a"
tags = {
    Name = "Autoscaling Demo Public Subnet"
  }
}
# Creating Public subnet 1 
resource "aws_subnet" "autoscaling-demo-
public-subnet1" {
  vpc_id                  = 
"${aws_vpc.autoscaling-demo-vpc.id}"
  cidr_block             = 
"${var.subnet1_cidr}"
  map_public_ip_on_launch = true

  availability_zone = "us-east-2b"
tags = {
    Name = "Autoscaling Demo Public Subnet 1"
  }
}
8. Copy route_table.tf file from chapter6/auto_scaling_group_ec2
folder to your own chapter6/auto_scaling_group_ec2 folder or use
the following code:
#Creating Route Table
resource "aws_route_table" "route" {
    vpc_id = "${aws_vpc.autoscaling-demo-
vpc.id}"
route {
        cidr_block = "0.0.0.0/0"
        gateway_id = 
"${aws_internet_gateway.autoscaling-demo-
igw.id}"
    }
tags = {
        Name = "Route to internet"
    }
}
resource "aws_route_table_association" "rt1" {
    subnet_id = "${aws_subnet.autoscaling-
demo-public-subnet.id}"

    route_table_id = 
"${aws_route_table.route.id}"
}
resource "aws_route_table_association" "rt2" {
    subnet_id = "${aws_subnet.autoscaling-
demo-public-subnet1.id}"
    route_table_id = 
"${aws_route_table.route.id}"
}
9. Copy igw.tf file from chapter6/ auto_scaling_group_ec2 folder to
your own chapter6/auto_scaling_group_ec2 folder or use the
following code:
resource "aws_internet_gateway" "autoscaling-
demo-igw" {
  vpc_id = "${aws_vpc.autoscaling-demo-
vpc.id}"
}
10. Copy elb_sg.tf file from chapter6/ auto_scaling_group_ec2 folder
to your own chapter6/auto_scaling_group_ec2 folder or use the
following code:
# Creating Security Group for ELB
resource "aws_security_group" "autoscaling-
elb-demo-sg1" {
  name        = "Autoscaling Demo Security 
Group"
  description = "Demo Module"
  vpc_id      = "${aws_vpc.autoscaling-demo-
vpc.id}"

# Inbound Rules
  # HTTP access from anywhere
  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
  # HTTPS access from anywhere
  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
  # SSH access from anywhere
  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
# Outbound Rules

  # Internet access to anywhere
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}
11. Copy elb.tf file from chapter6/ auto_scaling_group_ec2 folder to
your own chapter6/auto_scaling_group_ec2 folder or use the
following code:
resource "aws_elb" "autoscaling-demo-web-elb" 
{
  name = "web-elb"
  security_groups = [
    "${aws_security_group.autoscaling-elb-
demo-sg1.id}"
  ]
  subnets = [
    "${aws_subnet.autoscaling-demo-public-
subnet.id}",
    "${aws_subnet.autoscaling-demo-public-
subnet1.id}"
  ]
cross_zone_load_balancing   = true
health_check {

    healthy_threshold = 2
    unhealthy_threshold = 2
    timeout = 3
    interval = 30
    target = "HTTP:80/"
  }
listener {
    lb_port = 80
    lb_protocol = "http"
    instance_port = "80"
    instance_protocol = "http"
  }
}
12. Now, copy ec2_sg.tf file from chapter6/ auto_scaling_group_ec2
folder to your own chapter6/auto_scaling_group_ec2 folder or use
the following code:
# Creating Security Group for EC2
resource "aws_security_group" "autoscaling-
demo-ec2-sg1" {
  name        = "Autoscaling Ec2 Demo Security 
Group"
  description = "Demo Module"
  vpc_id      = "${aws_vpc.autoscaling-demo-
vpc.id}"
# Inbound Rules

  # HTTP access from anywhere
  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
# HTTPS access from anywhere
  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
# SSH access from anywhere
  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
# Outbound Rules
  # Internet access to anywhere

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}
13. Copy ec2.tf file from chapter6/ auto_scaling_group_ec2 folder to
your own chapter6/auto_scaling_group_ec2 folder or use the
following code:
resource "aws_launch_configuration" "web" {
  name_prefix = "web-"
image_id = "ami-02238ac43d6385ab3" 
  instance_type = "t2.micro"
  key_name = "jodeyinka_cyber_macaws_sshkey"
security_groups = [ 
"${aws_security_group.autoscaling-demo-ec2-
sg1.id}" ]
  associate_public_ip_address = true
  user_data = "${file("data.sh")}"
lifecycle {
    create_before_destroy = true
  }
}

14. Copy asg.tf file from chapter6/ auto_scaling_group_ec2 folder to
your own chapter6/auto_scaling_group_ec2 folder or use the
following code:
resource "aws_autoscaling_policy" 
"web_policy_up" {
  name = "web_policy_up"
  scaling_adjustment = 1
  adjustment_type = "ChangeInCapacity"
  cooldown = 300
  autoscaling_group_name = 
"${aws_autoscaling_group.web.name}"
}
resource "aws_cloudwatch_metric_alarm" 
"web_cpu_alarm_up" {
  alarm_name = "web_cpu_alarm_up"
  comparison_operator = 
"GreaterThanOrEqualToThreshold"
  evaluation_periods = "2"
  metric_name = "CPUUtilization"
  namespace = "AWS/EC2"
  period = "120"
  statistic = "Average"
  threshold = "70"
dimensions = {
    AutoScalingGroupName = 
"${aws_autoscaling_group.web.name}"

  }
alarm_description = "This metric monitor EC2 
instance CPU utilization"
  alarm_actions = [ 
"${aws_autoscaling_policy.web_policy_up.arn}" 
]
}
resource "aws_autoscaling_policy" 
"web_policy_down" {
  name = "web_policy_down"
  scaling_adjustment = -1
  adjustment_type = "ChangeInCapacity"
  cooldown = 300
  autoscaling_group_name = 
"${aws_autoscaling_group.web.name}"
}
resource "aws_cloudwatch_metric_alarm" 
"web_cpu_alarm_down" {
  alarm_name = "web_cpu_alarm_down"
  comparison_operator = 
"LessThanOrEqualToThreshold"
  evaluation_periods = "2"
  metric_name = "CPUUtilization"
  namespace = "AWS/EC2"
  period = "120"
  statistic = "Average"

  threshold = "30"
dimensions = {
    AutoScalingGroupName = 
"${aws_autoscaling_group.web.name}"
  }
alarm_description = "This metric monitor EC2 
instance CPU utilization"
  alarm_actions = [ 
"${aws_autoscaling_policy.web_policy_down.arn}
" ]
}
15. Copy asg.tf file from chapter6/ auto_scaling_group_ec2 folder to
your own chapter6/auto_scaling_group_ec2 folder or use the
following code:
resource "aws_autoscaling_group" "web" {
  name = 
"${aws_launch_configuration.web.name}-asg"
  min_size             = 2
  desired_capacity     = 2
  max_size             = 10
 
  health_check_type    = "ELB"
  load_balancers = [
    "${aws_elb.autoscaling-demo-web-elb.id}"
  ]

launch_configuration = 
"${aws_launch_configuration.web.name}"
enabled_metrics = [
    "GroupMinSize",
    "GroupMaxSize",
    "GroupDesiredCapacity",
    "GroupInServiceInstances",
    "GroupTotalInstances"
  ]
metrics_granularity = "1Minute"
vpc_zone_identifier  = [
    "${aws_subnet.autoscaling-demo-public-
subnet.id}",
    "${aws_subnet.autoscaling-demo-public-
subnet1.id}"
  ]
# Required to redeploy without an outage.
  lifecycle {
    create_before_destroy = true
  }
tag {
    key                 = "Name"
    value               = "web"
    propagate_at_launch = true

  }
}
16. Create a file name data.sh using the following code:
sudo yum update -y
sudo amazon-linux-extras install docker -y
sudo service docker start
sudo usermod -a -G docker ec2-user
sudo chkconfig docker on
sudo chmod 666 /var/run/docker.sock
docker pull dhruvin30/dhsoniweb:v1
docker run -d -p 80:80 
dhruvin30/dhsoniweb:latest
17. Now use this command:
terraform init --backend-config=config/demo.config
terraform plan
terraform apply --auto–approve
18. Push you code to a new branch in our codecommit chapter6-
autoscaling git push in the mentioned GitHub repository.
Conclusion
In this chapter, we dive into the world of cloud computing and uncover the
remarkable capabilities of Amazon Elastic Compute Cloud. Serving as the
backbone of Amazon Web Services, EC2 revolutionizes the way we
conceptualize, deploy, and manage computational resources.
The chapter embarks on a journey through the foundational principles of
EC2, elucidating its role as a highly scalable and flexible Infrastructure as
a Service (IaaS) solution. We explore the concept of virtualization,

shedding light on how EC2 ingeniously abstracts physical hardware,
allowing users to conjure virtual instances tailored to their specific needs.
Moreover, the chapter delves into the extensive selection of instance types,
ranging from the nimble and efficient to the robust and graphics-intensive.
Each instance type is dissected to illuminate the vCPUs, memory, storage,
and GPU provisions, empowering readers to make astute choices aligned
with their project requisites.
As security emerges as a paramount concern in today’s digital landscape, a
dedicated section navigates the robust security measures ingrained in EC2.
The Secure Shell (SSH) key pairs, Virtual Private Clouds (VPCs), and
Network Access Control Lists (NACLs) weave an intricate web of
safeguards, fostering an environment where data integrity and
confidentiality reign supreme.
Additionally, the chapter dissects the orchestration of EC2 instances,
illuminating the seamless integration with complementary AWS services
like Elastic Load Balancing, Auto Scaling, and Amazon Elastic Block
Store. This intricate symphony of services ensures high availability,
scalability, and fault tolerance for applications hosted on EC2.The narrative
culminates in a visionary projection of EC2’s future trajectory. From
embracing the latest processor technologies to pioneering advancements in
GPU-accelerated computing, Amazon EC2 continues to push the
boundaries of innovation. As cloud computing cements its position as the
bedrock of modern IT infrastructure, EC2 stands tall as a beacon of virtual
prowess, perpetually reshaping the contours of computation.
Multiple choice questions
1. What are the different types of EC2 instances?
a. General purpose
b. Computer optimized
c. Storage optimized
d. All of the above

e. None of the above
2. What are the merits of auto-scaling?
a. Better availability
b. Fault tolerance
c. Cost management
d. All of the above
e. None of the above
Answers
1. d
2. d
Join our book’s Discord space
Join the book’s Discord Workspace for Latest updates, Offers, Tech
happenings around the world, New Release and Sessions with the Authors:
https://discord.bpbonline.com

CHAPTER 7
Automating ELB Deployment and
Configurations
Introduction
Elastic Load Balancer (ELB) is a fundamental service offered by Amazon Web
Services (AWS) that stands at the forefront of load balancing solutions. Amazon
ELB is a versatile tool that empowers developers and system administrators to
optimize the distribution of incoming traffic across multiple instances, ensuring
smooth and reliable user experiences. Service provided by Amazon Web Services
that automatically distributes inbound traffic across multiple servers or instances.
ELB helps improve the availability and scalability of applications by spreading
the traffic across servers and ensuring that no single server is overwhelmed.
By the end of this chapter, you will not only have a firm grasp of Amazon ELB
but also the confidence to architect and deploy robust, scalable, and fault-tolerant
applications using this powerful AWS service. So, let us embark on this
enlightening journey into the heart of Amazon ELB and unleash its potential to
optimize your applications for success in the cloud.
Structure
We will go through the following topics in this chapter:
Types of Elastic Load Balancers
Introduction to Application Load Balancer
Introduction to Network Load Balancer

Introduction to Gateway Load Balancer
Objectives
This chapter provides a comprehensive exploration of Amazon Web Services
Elastic Load Balancers. Beginning with an overview of ELB types, including
Application Load Balancer (ALB), Network Load Balancer (NLB), and
Gateway Load Balancer (GLB), readers gain a nuanced understanding of their
unique functionalities. The focus shifts to an in-depth introduction to each load
balancer type, highlighting their specific use cases and benefits. By the end,
participants will possess a thorough knowledge of AWS ELBs, enabling informed
decision-making and effective implementation of load balancing solutions tailored
to diverse application and network requirements.
Types of Elastic Load Balancers
An Elastic Load Balancer (ELB) is a pivotal component in modern web
infrastructure, orchestrating the seamless distribution of incoming traffic across
multiple servers or instances. Its "elastic" nature stems from its ability to
dynamically adjust to varying workloads, effortlessly scaling resources up or
down as demand fluctuates. ELBs enhance system performance and availability
by effectively mitigating overloads on any single server, ensuring optimal
responsiveness and reliability for end-users.
Followng are the types of commonly used Elastic Load Balancers offered by
AWS:
Application Load Balancer (ALB)
Network Load Balancer (NLB)
Classic Load Balancer (CLB)
Gateway Load Balancer (GWLB)
Introduction to Application Load Balancers
An Application Load Balancer (ALB) is designed to efficiently distribute
incoming network traffic across a group of servers or resources. Unlike traditional
load balancers that operate at the transport layer, ALBs function at the application
layer of the OSI model. This means they have a deeper understanding of the
content of the packets they are distributing, allowing for intelligent routing based
on the application-level content, such as HTTP headers, URLs, or cookies.

One of the standout features of an ALB is its ability to provide advanced routing
and traffic management based on various factors. It can direct traffic based on the
path in the URL, which is invaluable in microservices architectures where
different services may handle specific API endpoints. Additionally, ALBs can
route traffic based on hostnames, enabling the use of a single ALB for multiple
domains or subdomains.
Finally, ALBs often support content-based routing using flexible rules. This
allows for dynamic forwarding of requests to different sets of resources based on
the content of the request. For example, requests for static content like images or
CSS files can be directed to a different set of servers optimized for serving such
content, improving overall system performance and user experience.
Automating Web Server with ALB
Automating Application Load Balancers is a pivotal step in modernizing and
optimizing application deployment and performance. Leveraging automation tools
and techniques not only streamlines the deployment process but also enhances
scalability, reliability, and efficiency. Through automation, you can dynamically
manage ALB configurations, adapt to changing traffic patterns, and respond
promptly to fluctuating demand.
One of the key benefits of automating ALBs is achieving consistency across
environments. Using Terraform, you can define the ALB configuration as code,
ensuring that the setup is identical across development, testing, and production
environments. This consistency mitigates the risk of misconfigurations and
simplifies troubleshooting and debugging.
Furthermore, automation can enable advanced traffic routing and management.
Through integrating with traffic management solutions or leveraging
programmable ALB features, you can implement intelligent routing rules, A/B
testing, and canary releases. This level of automation empowers you to optimize
user experience and ensure smooth deployments without manual intervention.
Before we delve deeper into the infrastructure code, let us look at the architecture
diagram of ALB with three web servers (refer to Figure 7.1):

Figure 7.1: ALB with Three Web Server Architecture Diagram
1. This step copy config folder and remote state folder we have used in the
previous chapters. Same file is provided in chapter 7 code for your usage.
You also need to create the variable files for creating application subnet,
availability zone.
a. Copy config folder to the root of your chapter7/application_lb.
b. Copy remote_state.tf and provider.tf files to the root of
chapter7/application_lb.
c. Create variables.tf with the following code:
  variable "subnet_cidr_private" {
  description = "cidr blocks for the private 
subnets"
  default     = ["10.10.1.0/24", 
"10.10.2.0/24", "10.10.3.0/24"]
  type        = list(any)
}
variable "availability_zone" {
  description = "availability zones for the 
private subnets"

  default     = ["us-east-2a", "us-east-2b", 
"us-east-2c"]
  type        = list(any)
}
2. This step defines the output files. The output will display on your screen,
the output contains public DNS of the load balancer.
Create output.tf with the following code:
output "instance_app-server_public_dns" {
  value = aws_instance.app-server.*.public_dns
}
output "load_balancer_dns_name" {
  value = aws_lb.front.dns_name
}
3. This is the file that defines the main code of the deployment. This includes
the definition, declaration, and configuration of VPC, subnets, route table,
and the gateway. Create main.tf with the following code:
resource "aws_vpc" "alb-demo-vpc" {
  cidr_block = "10.10.0.0/20"
  enable_dns_support = true
  enable_dns_hostnames = true
  tags = {
    "Name" = "Application-1"
  }}
resource "aws_subnet" "private" {
  count             = 
length(var.subnet_cidr_private)
  vpc_id            = aws_vpc.alb-demo-vpc.id

  cidr_block        = 
var.subnet_cidr_private[count.index]
  availability_zone = 
var.availability_zone[count.index]
  tags = {
    "Name" = "Application-1-private"}}
resource "aws_route_table" "alb-demo-vpc-rt" {
  vpc_id = aws_vpc.alb-demo-vpc.id
  tags = {
    "Name" = "Application-1-route-table"}}
resource "aws_route_table_association" "private" {
  count          = length(var.subnet_cidr_private)
  subnet_id      = 
element(aws_subnet.private.*.id, count.index)
  route_table_id = aws_route_table.alb-demo-vpc-
rt.id
}
resource "aws_internet_gateway" "alb-demo-vpc-igw" 
{
  vpc_id = aws_vpc.alb-demo-vpc.id
  tags = {
    "Name" = "Application-1-gateway"}}
resource "aws_route" "internet-route" {
  destination_cidr_block = "0.0.0.0/0"
  route_table_id         = aws_route_table.alb-
demo-vpc-rt.id
  gateway_id             = 
aws_internet_gateway.alb-demo-vpc-igw.id

}
4. This section creates the three web servers that serves as the backend to the
ALB. This ALB uses port 80 to server the application, you must port 443 in
the production environment. Create ec2.tf with the following code:
  resource "aws_security_group" "http-sg" {
  name        = "allow_http_access"
  description = "allow inbound http traffic"
  vpc_id      = aws_vpc.alb-demo-vpc.id
  ingress {
    description = "from my ip range"
    from_port   = "80"
    to_port     = "80"
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
  egress {
    cidr_blocks = ["0.0.0.0/0"]
    from_port   = "0"
    protocol    = "-1"
    to_port     = "0"
  }
  tags = {
    "Name" = "Application-1-sg" }}
data "aws_ami" "amazon_ami" {
  filter {
    name   = "name"

    values = ["amzn2-ami-hvm-2.0.20220606.1-
x86_64-gp2"]
  }
  filter {
    name   = "virtualization-type"
    values = ["hvm"]
  }
  most_recent = true
  owners      = ["amazon"]
}
resource "aws_instance" "app-server" {
  count                  = 
length(var.subnet_cidr_private)
  instance_type          = "t2.micro"
  ami                    = 
data.aws_ami.amazon_ami.id
  vpc_security_group_ids = 
[aws_security_group.http-sg.id]
  subnet_id              = 
element(aws_subnet.private.*.id, count.index)
  # associate public ip address
  associate_public_ip_address = true
  tags = {
    Name = "app-server-${count.index + 1}"
  }
  user_data = file("data.sh")}
5. This section creates the ALB, the target group, and the listeners.

Create a "alb.tf" file with the following code:
 
resource "aws_lb_target_group" "front" {
  name     = "application-front"
  port     = 80
  protocol = "HTTP"
  vpc_id   = aws_vpc.alb-demo-vpc.id
  health_check {
    enabled             = true
    healthy_threshold   = 3
    interval            = 10
    matcher             = 200
    path                = "/"
    port                = "traffic-port"
    protocol            = "HTTP"
    timeout             = 3
    unhealthy_threshold = 2}}
# Attaching the lb_target_group to the load 
balancer
resource "aws_lb_target_group_attachment" "attach-
app1" {
  count            = length(aws_instance.app-
server)
  target_group_arn = aws_lb_target_group.front.arn
  target_id        = element(aws_instance.app-
server.*.id, count.index)
  port             = 80

}
# This is the load balancer listener
resource "aws_lb_listener" "front_end" {
  load_balancer_arn = aws_lb.front.arn
  port              = "80"
  protocol          = "HTTP"
  default_action {
    type             = "forward"
    target_group_arn = 
aws_lb_target_group.front.arn}}
# Creating the lb
resource "aws_lb" "front" {
  name               = "front"
  internal           = false
  load_balancer_type = "application"
  security_groups    = [aws_security_group.http-
sg.id]
  subnets            = [for subnet in 
aws_subnet.private : subnet.id]
  enable_deletion_protection = false
  tags = {
    Environment = "front"}}
6. This step installs and configures the Apache web server, and also copy the
web files to the serving folder:
Create "data.sh" file with the code below:           
#!/bin/bash
sudo su

yum update -y
yum install -y httpd.x86_64
yum install -y jq
REGION_AV_ZONE=`curl -s 
http://169.254.169.254/latest/dynamic/instance-
identity/document | jq .availabilityZone -r`
systemctl start httpd.service
systemctl enable httpd.service
echo "Hello and Hello from $(hostname -f) from the 
availability zone: $REGION_AV_ZONE"> 
/var/www/html/index.html
7. Now run the following commands to finish the deployment:
terraform init --backend-config=config/demo.config
terraform plan
terraform apply --approve-auto
8. Put the Load Balancer URL output in your browser to see the result.
Click the Browser Refresh to see the load balancer go to another instance
behind the lb (refer to Figure 7.2):
Figure 7.2: Browser Refresh
9. Push you code to a new branch in our codecommit chapter7-alb git push
https://git-codecommit.us-east-1.amazonaws.com/v1/repos/awsca-
demo-repo.
Introduction to Network Load Balancers (NLB)
An Elastic Network Load Balancer (NLB) is a dynamic and adaptive
networking solution designed to distribute incoming network traffic across

multiple servers or resources efficiently. Unlike traditional load balancers, an NLB
utilizes an elastic and scalable architecture, allowing it to adapt to changing
workloads and traffic patterns in real-time. This makes it an ideal choice for
modern applications and services hosted in cloud environments.
At its core, an NLB employs advanced algorithms to intelligently route traffic
based on various factors such as server health, available resources, and
geographical proximity. This enables optimal distribution of incoming requests,
ensuring high availability, fault tolerance, and improved overall performance of
the applications it serves. The elastic nature of an NLB allows it to automatically
scale up or down in response to demand, optimizing resource usage and cost-
efficiency.
One of the standout features of an NLB is its ability to seamlessly integrate with
cloud platforms and take advantage of their scalability and resilience. It can be
easily configured and managed through cloud provider interfaces, enabling users
to fine-tune load balancing settings and adapt to the specific needs of their
applications. This adaptability, combined with robust security features and
monitoring capabilities, empowers organizations to deliver a superior user
experience even during high traffic or surges in demand.
Automating Web Server with NLB
Automating Elastic Network Load Balancers (NLBs) is a critical step in
ensuring efficient and scalable network traffic management within a cloud
infrastructure. Leveraging automation allows for streamlined operations,
improved responsiveness to changing traffic patterns, and enhanced reliability.
One approach to automate NLBs involves utilizing Terraform infrastructure as
code principles and integrating with cloud orchestration tools.
By adopting Terraform, network engineers can define the desired state of the NLB
configuration using code, typically in a declarative manner. Tools like Terraform
provide powerful mechanisms to define, manage, and automate the provisioning
and configuration of NLBs. Engineers can create templates that outline the
necessary NLB properties, such as protocols, ports, health checks, and target
groups, allowing for consistent and repeatable deployments.
Machine learning and AI algorithms can be integrated into the automation process
to predict and optimize NLB configurations based on historical and real-time
traffic patterns. These algorithms can adjust the load balancing strategies, ensuring
optimal distribution of traffic and resource utilization, ultimately enhancing the
performance and reliability of the application (refer to Figure 7.3):

Figure 7.3: NLB with Three Web Server Architecture Diagram
1. Since you already deployed ALB above, you are going to reuse some of the
codes used when you deployed the ALB. Copy all the files used in
Application Load Balancer (ALB) above with the exception of alb.tf
file.
2. This step defines the NLB code, which includes the target group.
3. Create a file named nlb.tf with the following codes:
resource "aws_lb_target_group" "front" {
  name     = "application-front"
  port     = 80
  target_type = "instance"
  protocol = "TCP"
  vpc_id   = aws_vpc.nlb-demo-vpc.id
  health_check {
    enabled             = true
    healthy_threshold   = 3
    interval            = 10
    matcher             = 200

    path                = "/"
    port                = 80
    protocol            = "HTTP"
    timeout             = 3
    unhealthy_threshold = 2}}
# Attaching the lb_target_group to the load 
balancer
resource "aws_lb_target_group_attachment" "attach-
app1" {
  count            = length(aws_instance.app-
server)
  target_group_arn = aws_lb_target_group.front.arn
  target_id        = element(aws_instance.app-
server.*.id, count.index)
  port             = 80
  #cross_zone_load_balancing   = true
}
# This is the load balancer listener
resource "aws_lb_listener" "front_end" {
  load_balancer_arn = aws_lb.front.arn
  port              = "80"
  protocol          = "TCP"
  default_action {
    type             = "forward"
    target_group_arn = 
aws_lb_target_group.front.arn}}
# Creating the lb

resource "aws_lb" "front" {
  name               = "demo-nlb"
  internal           = false
  load_balancer_type = "network"
  enable_cross_zone_load_balancing = true
  subnets            = [for subnet in 
aws_subnet.private : subnet.id]
  enable_deletion_protection = false
  tags = {
    Environment = "Demo"}}
4. Modify all the files you used for the ALB, any files that has alb must be
changes to nlb.
5. Now run the following commands to finish the deployment:
terraform init --backend-config=config/demo.config
terraform plan
terraform apply --approve-auto
6. Put the Load Balancer URL output in your browser to see the result.
7. Click the Browser Refresh to see the load balancer go to another instance
behind the lb (refer to Figure 7.4):
Figure 7.4: Click on Browser Refresh
8. Push you code to a new branch in our codecommit chapter7-nlb git push in
the mentioned GitHub repository.
Introduction to Classic Load Balancers

Elastic Classic Load Balancer (CLB) is a vital component in the realm of cloud
computing and web services, designed to efficiently distribute incoming
application traffic across multiple targets. Employed within Amazon Web
Services infrastructure, this load balancer dynamically adjusts and allocates
requests to maintain optimal performance and reliability of applications hosted in
the AWS cloud. The classic in its name signifies its longstanding presence and
essential role in AWS, catering to a diverse range of applications and workloads.
At its core, the Elastic Classic Load Balancer facilitates seamless scaling by
distributing incoming traffic across a fleet of instances, enhancing fault tolerance,
and enabling high availability. Its robust and elastic nature allows it to adapt to
fluctuating demand and traffic patterns, ensuring a consistent user experience even
during peak usage. By effectively managing traffic, this load balancer optimizes
resource utilization, reduces latency, and mitigates the risk of overloading specific
instances.
Moreover, Elastic Classic Load Balancer boasts features such as health checks,
which actively monitor the health of instances and automatically route traffic
away from unhealthy ones, promoting a resilient architecture. Its compatibility
with various protocols and session persistence options further amplifies its
versatility, making it a versatile tool for diverse applications, including web,
mobile, and enterprise applications.
Automating Web Server with CLB
Automating Elastic Classic Load Balancers is a crucial step in achieving a more
efficient and scalable infrastructure. With the increasing complexity of modern
applications, automation helps streamline the management and configuration of
ELBs, ensuring optimal performance and reliability. One approach to automating
CLBs involves leveraging Infrastructure as Code using Terraform.
By utilizing Terraform, you can define your CLB configurations. This template
include specifications for the load balancer, listeners, health checks, and any other
necessary settings. Automating with Terraform enables you to version control the
configuration, easily replicate it across environments, and seamlessly update or
rollback changes. Additionally, integrating terraform with Continuous
Integration/Continuous Deployment (CI/CD) pipelines allows for a streamlined
and automated deployment process.
1. This step copy config folder and remote state folder we have used in the
previous chapters. Same file is provided in chapter 7 code for your usage.
You also need to create the variable files for creating VPC, subnet,
availability zone:

a. Copy config folder to the root of your chapter7/classic_lb.
b. Copy remote_state.tf and provider.tf files to the root of
chapter7/classic_lb.
c. Create variables.tf with the following code:
# Defining Public Key
variable "public_key" {
  default = "autoscaling-demo.pub"
}
# Defining Private Key
variable "private_key" {
  default = "autoscaling-demo.pem"
}
# Definign Key Name for connection
variable "key_name" {
  default = "autoscaling-demo"
  description = "Name of AWS key pair"
}
# Defining CIDR Block for VPC
variable "vpc_cidr" {
  default = "10.10.0.0/20"
}
# Defining CIDR Block for Subnet
variable "subnet_cidr" {
  default = "10.10.1.0/24"
}

# Defining CIDR Block for 2d Subnet
variable "subnet1_cidr" {
  default = "10.10.2.0/24"}
2. Create vpc.tf with the following code:
resource "aws_vpc" "autoscaling-demo-vpc" {
  cidr_block       = "${var.vpc_cidr}"
  instance_tenancy = "default"
tags = {
    Name = "Auto Scaling Demo VPC"}}
3. This step created the private and public subnet configuration and
declaration files needed.
4. Create subnet.tf with the following code:
# Creating public subnet
resource "aws_subnet" "autoscaling-demo-public-
subnet" {
  vpc_id                  = 
"${aws_vpc.autoscaling-demo-vpc.id}"
  cidr_block             = "${var.subnet_cidr}"
  map_public_ip_on_launch = true
  availability_zone = "us-east-2a"
tags = {
    Name = "Autoscaling Demo Public Subnet"}}
# Creating Public subnet 1
resource "aws_subnet" "autoscaling-demo-public-
subnet1" {
  vpc_id                  = 
"${aws_vpc.autoscaling-demo-vpc.id}"

  cidr_block             = "${var.subnet1_cidr}"
  map_public_ip_on_launch = true
  availability_zone = "us-east-2b"
tags = {
    Name = "Autoscaling Demo Public Subnet 1" }}
5. This step created the route table configuration and the declaration files
needed.
6. Create route_table.tf with the following code:
#Creating Route Table
resource "aws_route_table" "route" {
    vpc_id = "${aws_vpc.autoscaling-demo-vpc.id}"
route {
        cidr_block = "0.0.0.0/0"
        gateway_id = 
"${aws_internet_gateway.autoscaling-demo-igw.id}"
    }
tags = {
        Name = "Route to internet" }}
resource "aws_route_table_association" "rt1" {
    subnet_id = "${aws_subnet.autoscaling-demo-
public-subnet.id}"
    route_table_id = "${aws_route_table.route.id}"
}
resource "aws_route_table_association" "rt2" {
    subnet_id = "${aws_subnet.autoscaling-demo-
public-subnet1.id}"

    route_table_id = 
"${aws_route_table.route.id}"}
7. This step created the internet gateway configuration and declaration files
needed.
8. Create igw.tf with the following code:
resource "aws_internet_gateway" "autoscaling-demo-
igw" {
  vpc_id = "${aws_vpc.autoscaling-demo-vpc.id}"
}
9. This step created the route table configuration and declaration files needed.
10. Create elb.tf with the following code:
resource "aws_elb" "autoscaling-demo-web-elb" {
  name = "web-elb"
  security_groups = [
    "${aws_security_group.autoscaling-elb-demo-
sg1.id}"
  ]
  subnets = [
    "${aws_subnet.autoscaling-demo-public-
subnet.id}",
    "${aws_subnet.autoscaling-demo-public-
subnet1.id}"
  ]
cross_zone_load_balancing   = true
health_check {
    healthy_threshold = 2
    unhealthy_threshold = 2
    timeout = 3

    interval = 30
    target = "HTTP:80/"
  }
listener {
    lb_port = 80
    lb_protocol = "http"
    instance_port = "80"
    instance_protocol = "http" }}
11. This step created the load balancer security group configuration and
declaration files needed.
12. Create elb_sg.tf with the following code:
# Creating Security Group for ELB
resource "aws_security_group" "autoscaling-elb-
demo-sg1" {
  name        = "Autoscaling Demo Security Group"
  description = "Demo Module"
  vpc_id      = "${aws_vpc.autoscaling-demo-
vpc.id}"
# Inbound Rules
  # HTTP access from anywhere
  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]}
  # SSH access from anywhere
  ingress {

    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]}
# Outbound Rules
  # Internet access to anywhere
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"] }}
13. This step created the route ec2 configuration and declaration files needed.
14. Create ec2.tf with the following code:
resource "aws_launch_configuration" "web" {
  name_prefix = "web-"
image_id = "ami-02238ac43d6385ab3"
  instance_type = "t2.micro"
  key_name = "jodeyinka_cyber_macaws_sshkey"
security_groups = [ 
"${aws_security_group.autoscaling-demo-ec2-
sg1.id}" ]
  associate_public_ip_address = true
  user_data = "${file("data.sh")}"
lifecycle {
    create_before_destroy = true}}
15. This step created the ec2 security group configuration and declaration files
needed. Als it also attaches the ssh key to the EC2 for access.

16. Create ec2_sg.tf with th following code:
# Creating Security Group for ELB
resource "aws_security_group" "autoscaling-demo-
ec2-sg1" {
  name        = "Autoscaling Ec2 Demo Security 
Group"
  description = "Demo Module"
  vpc_id      = "${aws_vpc.autoscaling-demo-
vpc.id}"
# Inbound Rules
  # HTTP access from anywhere
  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]}
# SSH access from anywhere
  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]}
# Outbound Rules
  # Internet access to anywhere
  egress {
    from_port   = 0

    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"] }}
17. This step created the auto scaling configuration and declaration files
needed.
18. Create asg.tf with the following code:
resource "aws_autoscaling_group" "web" {
  name = "${aws_launch_configuration.web.name}-
asg"
  min_size             = 2
  desired_capacity     = 2
  max_size             = 10
  health_check_type    = "ELB"
  load_balancers = [
    "${aws_elb.autoscaling-demo-web-elb.id}"
  ]
launch_configuration = 
"${aws_launch_configuration.web.name}"
enabled_metrics = [
    "GroupMinSize",
    "GroupMaxSize",
    "GroupDesiredCapacity",
    "GroupInServiceInstances",
    "GroupTotalInstances"
  ]
metrics_granularity = "1Minute"
vpc_zone_identifier  = [

    "${aws_subnet.autoscaling-demo-public-
subnet.id}",
    "${aws_subnet.autoscaling-demo-public-
subnet1.id}"
  ]
# Required to redeploy without an outage.
  lifecycle {
    create_before_destroy = true
  }
tag {
    key                 = "Name"
    value               = "web"
    propagate_at_launch = true }}
19. This step created the auto scaling configuration and declaration files
needed.
20. Create asg_policy.tf with the following code:
resource "aws_autoscaling_policy" "web_policy_up" 
{
  name = "web_policy_up"
  scaling_adjustment = 1
  adjustment_type = "ChangeInCapacity"
  cooldown = 300
  autoscaling_group_name = 
"${aws_autoscaling_group.web.name}"}
resource "aws_cloudwatch_metric_alarm" 
"web_cpu_alarm_up" {
  alarm_name = "web_cpu_alarm_up"

  comparison_operator = 
"GreaterThanOrEqualToThreshold"
  evaluation_periods = "2"
  metric_name = "CPUUtilization"
  namespace = "AWS/EC2"
  period = "120"
  statistic = "Average"
  threshold = "70"
dimensions = {
    AutoScalingGroupName = 
"${aws_autoscaling_group.web.name}" }
alarm_description = "This metric monitor EC2 
instance CPU utilization"
  alarm_actions = [ 
"${aws_autoscaling_policy.web_policy_up.arn}" ]}
resource "aws_autoscaling_policy" 
"web_policy_down" {
  name = "web_policy_down"
  scaling_adjustment = -1
  adjustment_type = "ChangeInCapacity"
  cooldown = 300
  autoscaling_group_name = 
"${aws_autoscaling_group.web.name}"}
resource "aws_cloudwatch_metric_alarm" 
"web_cpu_alarm_down" {
  alarm_name = "web_cpu_alarm_down"
  comparison_operator = 
"LessThanOrEqualToThreshold"

  evaluation_periods = "2"
  metric_name = "CPUUtilization"
  namespace = "AWS/EC2"
  period = "120"
statistic = "Average"
  threshold = "30"
dimensions = {
    AutoScalingGroupName = 
"${aws_autoscaling_group.web.name}"}
alarm_description = "This metric monitor EC2 
instance CPU utilization"
  alarm_actions = [ 
"${aws_autoscaling_policy.web_policy_down.arn}" ]}
21. Now run the following commands to finish the deployment:
terraform init --backend-config=config/demo.config
terraform plan
terraform apply --approve-auto
22. Push your code to a new branch in our codecommit chapter7-clb git push
in the mentioned GitHub repository.
Introduction to Gateway Load Balancers
AWS Gateway Load Balancer (GWLB) is a robust and scalable solution
designed to enable you to deploy virtual third-party appliances, such as Next
Generation Firewalls (NGFW), Intrusion Prevention Systems (IPS), firewall
appliances, and Intrusion Detection System (IDS) by distributing network traffic
across multiple targets within and across AWS Virtual Private Clouds (VPCs).
Unlike traditional load balancers, GWLB operates at the network layer (Layer 3)
and supports both Transmission Control Protocol (TCP) and User Datagram
Protocol (UDP) traffic. This unique approach allows GWLB to efficiently handle
a diverse range of workloads, making it a versatile tool for modern application
architectures.

Furthermore, AWS Gateway Load Balancer supports the use of Elastic Network
Interfaces (ENIs) for both internal and internet-facing configurations, providing
flexibility in network design. This makes it well-suited for a variety of use cases,
including multi-tier applications, microservices architectures, and hybrid cloud
setups. As organizations increasingly adopt distributed and complex
infrastructures, AWS Gateway Load Balancer stands out as a reliable and efficient
solution for managing network traffic with ease.
Following architecture illustrates the deployment of appliances behind GWLB,
which enterprise customers utilize to enhance visibility, perform service triage,
conduct network analysis, facilitate service discovery, and safeguard against
application-layer attacks directed at destinations like web servers, SIP voice, and
BGP. The GWLB effectively extends application visibility within AWS (refer to
Figure 7.5):
Figure 7.5: GWLB with ALB, application, and virtual appliance Architecture Diagram
Finally, to deploy the Gateway Load Balancer with terraform, follow the same
step used in Application Load Balancer, but with the following definition for the
gwlb.tf:
# Create a Gateway Load Balancer

resource "aws_lb" "example" {
    name               = "gateway-lb"
    internal           = false
    load_balancer_type = "gateway"
    tags = {
        Name = "gateway-lb"
    }
}
Conclusion
Typically, ALBs are used to improve the availability and scalability of
applications running in the cloud. They help ensure that the traffic is distributed
evenly across multiple backend servers.
While NLBs are typically used for applications that require high levels of
throughput. It is also used to distribute traffic across multiple availability zones in
a cloud environment.
Lastly, CLBs works at layer 4 by distributing incoming traffic to the backend
servers, it uses round robin, least connection, and IP hash algorithm to manage the
traffic distribution.
In this chapter, we delved into the fundamental concepts and advanced
functionalities of Elastic Load Balancers, a crucial component in modern cloud
computing and web application architecture. ELBs are essential for efficiently
distributing traffic across multiple resources, ensuring optimal performance,
scalability, and reliability.
We began by discussing the primary purpose of load balancers, emphasizing their
role in evenly distributing incoming requests among a group of servers or
instances. ELBs play a vital role in enhancing system performance, fault
tolerance, and handling sudden spikes in traffic.
Next, we explored the types of Elastic Load Balancers: Application Load
Balancers, Network Load Balancers, and Classic Load Balancers. Each type caters
to specific use cases, providing distinct features and benefits based on the
application’s requirements.

We examined the key features of Elastic Load Balancers, including SSL
termination, content-based routing, health checks, and integration with other AWS
services. Understanding these features is essential for configuring and optimizing
load balancing for diverse workloads.
Multiple choice questions
1. You can point External Network Load Balancer to ALB and On-
Premises Load Balancer.
a. True
b. False
2. Which of the following is not the type of AWS ELB?
a. Classic Load Balancer
b. Network Load Balancer
c. Application Load Balancer
d. F5 Load Balancer
Answers
1. a
2. d
Join our book’s Discord space
Join the book’s Discord Workspace for Latest updates, Offers, Tech happenings
around the world, New Release and Sessions with the Authors:
https://discord.bpbonline.com

CHAPTER 8
AWS Route53 Policy and Routing
Automation
Introduction
In today’s digital landscape, managing and optimizing the flow of internet traffic
to your applications and services is paramount. AWS Route 53, a highly scalable
and reliable Domain Name System (DNS) web service, plays a pivotal role in
ensuring seamless and efficient routing of traffic across your infrastructure.
This chapter delves into the intricacies of AWS Route 53, focusing on policy-
driven routing and automation strategies. We will uncover how you can leverage
Route 53 to customize and automate the way traffic is directed to your various
resources, optimizing performance, enhancing fault tolerance, and increasing
overall reliability.
By understanding the principles of policy-based routing and automation, you will
gain the ability to tailor routing decisions based on your unique business
requirements and use cases. Whether it is managing traffic between regions,
implementing geolocation-based routing, or automating failover scenarios, this
chapter will equip you with the knowledge and tools to design robust and efficient
routing policies.
We will start by discussing the core concepts of AWS Route 53 and its role in
managing DNS. Next, we’ll explore the various routing policies available,
highlighting their distinct features and optimal use cases. Then, we will dive into
automation strategies, enabling you to automate the application of routing policies
based on changing conditions and events within your AWS environment.

Throughout this chapter, we will provide practical examples, best practices, and
real-world use cases to illustrate how Route 53 can be harnessed effectively,
empowering you to architect a highly available and performant application
infrastructure. So, let’s embark on this insightful journey into AWS Route 53
Policy and Routing Automation, unlocking the potential to enhance your
application’s reliability and performance in the cloud.
Some of the key features of Route 53 you should know before we continue with
this chapter are:
High available and scalable
Domain registration
Routing policies
Health checks
Structure
The chapter will cover the following topics:
Introduction to AWS Route 53
Simple Routing Automation using Terraform
Advanced routing strategies
AWS Route 53 Blue-Green alias weighted routing automation using
Terraform
AWS Route 53 Failover Automation using Terraform
Objectives
AWS Route 53 Policy and Routing Automation is aimed at optimizing and
automating domain management and traffic routing within the Amazon Web
Services (AWS) ecosystem. The primary objective is to streamline and enhance
the management of domain names, allowing for efficient traffic routing and
improved application performance. Through automated policies, this service
ensures that domain requests are directed to the appropriate resources based on
defined rules and conditions, optimizing the user experience, and minimizing
latency. The goal is to achieve a dynamic and flexible routing infrastructure that
adapts to changing conditions, enabling rapid updates and adjustments to DNS
configurations to support evolving application needs and traffic patterns.

AWS Route 53 Policy and Routing Automation empower organizations to
implement granular control over their domain routing strategies. By leveraging
intelligent routing policies, this service allows for customized traffic distribution
based on various parameters such as geographic location, health of endpoints,
latency, and more. The objective is to enhance application availability, reliability,
and responsiveness by dynamically directing users to the closest and healthiest
resources. Additionally, it aims to facilitate rapid failover and disaster recovery by
automating the redirection of traffic away from unhealthy endpoints, ensuring
uninterrupted service delivery and minimal downtime during system failures or
maintenance activities. Ultimately, the aim is to provide a resilient and high-
performance DNS management solution that contributes to an optimal end-user
experience.
Introduction to AWS Route 53
Amazon S3 is a highly scalable, secure, and reliable cloud-based Domain Name
System (DNS) services that provides businesses with a method to route end users
to internet applications by translating domain names to IP addresses. As AWS
developer you should have knowledge of how Route 53 can be used to service
business needs.
Features and benefits
Amazon Route 53 is a highly reliable and scalable Domain Name System web
service provided by Amazon Web Services. It is designed to route users to the
optimal resources, ensuring high availability and performance for applications and
websites. Here are unique points outlining the features of AWS Route 53:
Global DNS resolution: AWS Route 53 provides global DNS resolution
services, allowing businesses to route users to the nearest endpoints
worldwide. It leverages a vast network of DNS servers strategically
distributed across different regions, ensuring low latency and improved user
experiences.
Traffic flow management: Route 53 allows users to manage traffic flow
effectively through features like weighted routing, latency-based routing,
geolocation-based routing, and failover routing. This level of control
enables businesses to optimize their application’s performance and
reliability based on specific criteria.
Health checking and monitoring: Route 53 continuously monitors the
health and availability of resources, automatically routing traffic away from

unhealthy endpoints. This feature ensures that users are directed to
functioning instances, enhancing overall application reliability and uptime.
Scalability and flexibility: AWS Route 53 seamlessly scale to handle
varying levels of traffic, making it suitable for small websites to enterprise-
level applications. It accommodates changes in traffic patterns and supports
rapid scalability to adapt to evolving business needs.
DNSSEC (DNS Security Extensions): Route 53 supports DNSSEC, a suite
of security extensions that add an additional layer of security to the DNS
protocol. This feature helps protect against DNS-related attacks and ensures
the integrity and authenticity of DNS data.
Here are unique points outlining the benefits of AWS Route 53:
High availability and reliability: AWS Route 53 offer a highly available
and reliable DNS infrastructure, minimizing downtime and providing users
with uninterrupted access to applications and services. Its distributed
architecture reduces the risk of service disruptions.
Improved website performance: With features like latency-based routing
and geolocation-based routing, Route 53 directs users to the closest and
most responsive servers. This results in reduced latency and improved
website loading times, enhancing the user experience.
Cost-effective and Pay-as-You-Go pricing: AWS Route 53 follows a pay-
as-you-go pricing model, ensuring cost-effectiveness by allowing users to
pay only for the DNS queries they receive. There are no upfront
commitments or fixed costs, making it a cost-efficient choice for businesses
of all sizes.
Seamless integration with AWS services: Route 53 integrates seamlessly
with other AWS services, enabling easy management of DNS settings
alongside other cloud resources. This integration simplifies operations and
ensures a cohesive cloud infrastructure.
Global scalability and redundancy: AWS Route 53’s global infrastructure
and redundancy options enable businesses to scale their applications
globally without concerns about performance or availability. It distributes
traffic efficiently across diverse geographic locations, maintaining a robust
and responsive user experience across the globe.
In conclusion, AWS Route 53 provides a robust set of features and benefits that
contribute to efficient traffic management, improved application performance,

enhanced security, and cost-effectiveness for businesses leveraging DNS services
in the AWS cloud environment.
Understanding Route 53 basics
Amazon Route 53 is a powerful and versatile Domain Name System (DNS) web
service provided by Amazon Web Services (AWS). It enables users to manage
domain names and route internet traffic efficiently. At its core, Route 53 translates
human-readable domain names into IP addresses that computers use to locate web
servers and other resources on the internet. This process is vital for the seamless
functioning of websites and online applications.
In the world of Route 53, understanding the fundamental components is crucial.
Hosted zones are a cornerstone; they contain all the records that define how traffic
is routed for a specific domain. These records include information like the
domain’s name servers, which are responsible for directing inquiries to the
appropriate resources. Additionally, Route 53 supports various record types, each
serving distinct purposes such as mapping domain names to IP addresses (A
records), pointing to mail servers (MX records), and enabling secure
communication (TLSA records).
Routing policies within Route 53 determine how traffic is distributed among
different resources, allowing for efficient load balancing, failover management,
and traffic steering based on geographical location. Simple routing directs traffic
to a single resource, while weighted routing enables traffic distribution based on
predefined weights. Geolocation and latency-based routing focus on optimizing
the user experience by directing traffic to the nearest or fastest available resource,
respectively.
Furthermore, health checks play a vital role in ensuring the reliability of the
resources. Route 53 allows users to configure health checks to monitor the status
of their endpoints and automatically route traffic away from unhealthy or failing
resources, enhancing the overall resilience of the system. Overall, comprehending
these Route 53 basics is fundamental to effectively manage and optimize domain
routing for a seamless online presence.
Exploring Route 53 policies
Amazon Route 53 is a highly versatile and powerful Domain Name System
(DNS) web service that allows for efficient domain management. Within this
platform, Route 53 Policies serve as crucial tools to manage and control domain
routing, ensuring optimal traffic distribution and failover handling. These policies

enable the user to tailor routing decisions based on specific criteria such as
geographic location, latency, health checks, or weighted distribution.
Geographic-based routing policies in Route 53 are particularly valuable for global
enterprises. By segmenting traffic based on the geographical location of users,
businesses can direct users to the nearest data center, thereby reducing latency and
enhancing user experience. This policy provides the flexibility to define different
responses for users from various regions, contributing to a seamless and optimized
browsing experience across the globe.
Another essential policy type is latency-based routing, which allows for traffic
redirection based on the lowest network latency. This means users are directed to
the endpoint that offers the quickest response time, optimizing the overall
performance and responsiveness of applications. This strategy is especially useful
for applications sensitive to latency, ensuring swift and reliable access for users.
Additionally, Route 53 provides health check-based routing policies, enabling
automatic failover to healthy endpoints. In the event of an endpoint or resource
failure, traffic is redirected to alternative health resources, enhancing the
availability and reliability of the application. These policies are crucial for critical
applications that demand high availability and fault tolerance.
Weighted routing policies offer yet another layer of flexibility by allowing users to
distribute traffic in specific proportions among different endpoints. This is
beneficial during testing, where a small portion of traffic can be directed to a new
version of the application to assess performance before full-scale deployment. It
provides a controlled environment for testing and gradual deployment while
minimizing risks and ensuring a smooth transition.
In conclusion, Amazon Route 53 policies empower businesses to fine-tune their
DNS routing strategies, optimizing the performance, reliability, and availability of
their applications. Each policy type offers unique capabilities, allowing for
tailored routing decisions based on various factors, ultimately enhancing the end-
user experience, and ensuring seamless operations in diverse scenarios.
Implementing simple routing policies
One of the fundamental routing policies offered by Route 53 is Simple Routing.
Simple Routing policy allows you to associate one or more DNS records with a
single endpoint, such as an IP address or an AWS resource. This straightforward
approach is perfect for scenarios where you have a single resource or need to
distribute traffic evenly among a few endpoints.

To implement the Simple Routing policy using Route 53, you start by logging into
the AWS Management Console and navigating to the Route 53 dashboard. From
there, create a hosted zone and add the appropriate DNS records for the desired
domain. For example, if you have a web application hosted on an Elastic Load
Balancer, you will associate the DNS record with the ELB endpoint.
Once you’ve set up the hosted zone and added the necessary DNS records, you
can adjust the routing policy to Simple. When using Simple Routing, all defined
DNS records are given equal weight, and the traffic is evenly distributed among
them. This is ideal for scenarios where you have multiple resources that can
handle the incoming traffic equally, ensuring high availability and fault tolerance.
It is important to regularly monitor the performance and health of your resources
associated with the Simple Routing policy. AWS provides various monitoring
tools and features like CloudWatch, which can help you gain insights into the
traffic distribution and the performance of your endpoints. Additionally, you can
make real-time adjustments to the DNS records and their respective weights
within the hosted zone to fine-tune the traffic distribution based on your
requirements.
Simple Routing Automation using Terraform
Before you start the automation, you must either purchase a new domain from
Amazon here https://us-east-
1.console.aws.amazon.com/route53/home#DomainRegistration or transfer
your existing domain here:
https://us-east-1.console.aws.amazon.com/route53/home#DomainTransfer.
Most companies that uses Route 53 use transfer services than purchasing a new
domain directly from AWS.
You are creating the following using terraform as the IAC:
Zone
www Record
A Record
TXT/SPF Record
MX Record
Name Server Record
DKIM Record

Follow the below steps:
1. This step starts with cloning the repository we have used in previous
chapter. This activity reduced the time it takes to complete this project.
Clone this from the mentioned GitHub repository to deploy multiple EC2
in availability set to be used for this deployment.
2. In this step you need to create folder to hold all the terraform files, and
create a configuration file such as provider.tf, remote_state.tf, and the
.gitignore files.
a. Create a folder named chapter8-simple-routing.
b. Copy folder config from previous chapter to chapter8-simple-
routing folder.
c. Copy file provider.tf, remote_state.tf to chapter8-simple-routing
folder.
d. Copy file .gitignore to chapter8-simple-routing folder.
3. Create a file named a_record.tf with the following code:
resource "aws_route53_record" "demo-server-
arecord" {
zone_id = "${aws_route53_zone.route53-demo-
zone.zone_id}"
name = "yourservername.yourdomain" # e.g. 
server1.awscloudautomationbook.com
type = "A"
ttl = "300"
records = ["IP-Address-Of-Server1","IP-Address-Of-
Server2"] # Put the IP Addresses of the two 
servers you deployed in step 1
}
4. Create a file named dkim_record.tf with the following code:
resource "aws_route53_record" 
"awscloudautomation_dkim_1" {

  zone_id = aws_route53_zone.route53-demo-
zone.zone_id
  name = "awscloudautomation1._domainkey"
  type = "CNAME"
  ttl = 1200
  records = [
    "domain_key1"
  ]
}
resource "aws_route53_record" 
"awscloudautomation_dkim_2" {
  zone_id = aws_route53_zone.route53-demo-
zone.zone_id
  name = "awscloudautomation2._domainkey"
  type = "CNAME"
  ttl = 1200
  records = [
    "domain_key2"
  ]
}
resource "aws_route53_record" 
"awscloudautomation_dkim_3" {
  zone_id = aws_route53_zone.route53-demo-
zone.zone_id
  name = "awscloudautomation3._domainkey"
  type = "CNAME"
  ttl = 1200

  records = [
    "domain_key3"
  ]
}
5. Create a file named mail_record.tf with the following code:
resource "aws_route53_record" "mail-arecord" {
zone_id = "${aws_route53_zone.route53-demo-
zone.zone_id}"
name = "Your-Root-Domain-Here" # Use your root 
domain here
type = "MX"
ttl = "300"
records = [
"server1.Your-Root-Domain-Here", # Your main 
server 1
"server2.Your-Root-Domain-Here", # Your main 
server 2
"server3.Your-Root-Domain-Here", # Your main 
server 3
]
}
6. Then create a file named www_record.tf with the following code:
resource "aws_route53_record" "demo-server-
wwwrecord" {
zone_id = "${aws_route53_zone.route53-demo-
zone.zone_id}"
name = "www.<your-root-domain-here>" # Substitute 
your domain here

type = "A"
ttl = "300"
records = ["IP-Address-Of-Server1","IP-Address-Of-
Server2"]
}
7. Now create a file named ns_record.tf with the following code:
resource "aws_route53_record" "demo-name-server" {
zone_id = "${aws_route53_zone.route53-demo-
zone.zone_id}"
name = "" # Your root domain here
type = "NS"
ttl = "300"
records = [aws_route53_zone.route53-demo-
zone.name_server]
}
8. You will then create a file named txt_record.tf with the following code:
resource "aws_route53_record" "demo-txt-record" {
zone_id = "${aws_route53_zone.route53-demo-
zone.zone_id}"
name = "example-demo-routetAble"
type = "TXT"
ttl = "300"
records = [
"demodomain-verification=123987", # Substitute 
your own number here
"v=spf1 include:_spf.demo mx ~all"
]

}
9. Now craete a file named zone.tf with the following code:
resource "aws_route53_zone" "route53-demo-zone" {
name = "Your-Root-Domain-Here" # Your root domain 
here
}
10. Now run the following commands:
Now 
use 
this 
command: 
"terraform 
init 
--backend-
config=config/demo.config" 
terraform 
plan 
terraform 
apply 
--
approve-auto.
11. Validate your deployment via AWS Console (see Figure 8.1):
Figure 8.1: AWS Console shows route configuration we just created
12. Push your code to a new branch in our codecommit chapter8-route53-
simple git push in the mentioned GitHub repository.
Advanced routing strategies
One essential routing strategy is weighted routing, allowing you to distribute
traffic across multiple resources in proportions you specify. For instance, you

could direct a higher percentage of users to a new version of your application for
testing purposes while maintaining the majority on the stable version. This
ensures a smooth transition and minimizes disruptions during updates.
Another advanced routing strategy offered by Route 53 is latency-based routing,
ideal for applications hosted in multiple AWS regions. By directing users to the
region with the lowest latency, you ensure optimal performance and
responsiveness, crucial for latency-sensitive applications. This strategy is
particularly beneficial for global applications where reducing response times is a
top priority.
Geolocation-based routing is yet another powerful tool provided by Route 53. It
allows you to direct traffic based on the geographical location of your users. For
instance, you can route users from Europe to servers located in a European data
center, ensuring compliance with regional data privacy regulations, and
optimizing performance for that specific user base.
Furthermore, Route 53 offers failover routing, enabling automatic failover to a
standby resource in the event of an outage. This is essential for applications that
require high availability and need to minimize downtime. By configuring health
checks and setting up primary and secondary resources, you ensure that traffic is
seamlessly redirected to a healthy resource when an issue occurs, maintaining a
smooth user experience.
In summary, Amazon Route 53’s advanced routing strategies provide the tools
necessary to optimize application performance, increase availability, and enhance
user experience by strategically managing traffic based on various parameters like
weights, latency, geolocation, and failover, making it a powerful solution for
modern, scalable web applications.
AWS Route 53 Blue-Green alias weighted Routing Automation using
Terraform
Blue-Green deployment is a Cadillac technique used in software development that
involves creating two identical infrastructure and software environments, referred
to as blue and green which involves switching traffic between these two
environments. Blue-Green is mostly used when deploying newer version of
application in production environment, the traffic can be partially or fully directed
to a particular region.
Before you start the automation, you must either purchase a new domain or
transfer you existing domain from Amazon

https://console.aws.amazon.com/route53/. Most companies that use Route 53
use transfer services than purchasing a new domain directly from AWS.
You are creating the following using terraform as the IAC:
Blue API Record
Green API Record
1. Clone this repository to deploy multiple EC2 in availability set to be used
for 
this 
deployment:
https://github.com/awscloudautomationbook/chapter8-route53-ec2.
2. This step creates the root folder for this chapter. It also creates config
folder, and the two files named provider.tf, and .gitignore.
a. Create a folder named chapter8-weighted-routing.
b. Copy folder config from previous chapter to chapter8-weighted-
routing folder.
c. Copy file provider.tf, remote_state.tf to chapter8-weighted-
routing folder.
d. Copy file .gitignore to chapter8-weighted-routing folder.
3. Copy all the files you used in simple routing above to chapter8-weighted-
routing folder. Create a file named api_blue_record.tf with the following
code:
resource "aws_route53_record" "demo-api-blue" {
zone_id = "${aws_route53_zone.route53-demo-
zone.zone_id}"
name = "" # Server1 FQDN (e.g. server1.Your-Root-
Domain-Here)
type = "CNAME"
ttl = "300"
records = ["Your-Server1-IP-Address-Here"] # Your 
Server1 IP Address here
weighted_routing_policy {

    weight = 10 # This is the percentage of 
traffic to this server
  }
  set_identifier = "api-blue"
}
4. Create a file named api_green_record.tf with the following code:
resource "aws_route53_record" "demo-api-green" {
zone_id = "${aws_route53_zone.route53-demo-
zone.zone_id}"
name = "" # Server1 FQDN (e.g. server2.Your-Root-
Domain-Here)
type = "CNAME"
ttl = "300"
records = ["Your-Server3-IP-Address-Here "] # Your 
Server2 IP Address here
weighted_routing_policy {
    weight = 90 # This is the percentage of 
traffic to this server
  }
  set_identifier = "api-green"
}
5. Now run the following commands:
terraform init --backend-config=config/demo.config
terraform plan
terraform apply --approve-auto
Validate your deployment via AWS Console (see Figure 8.2):

Figure 8.2: AWS Console shows CNAME for weighted routing protocol we just created
6. Push your code to a new branch in our codecommit chapter8-route53-
weighted git push in the mentioned GitHub repository.
AWS Route 53 Failover Automation using Terraform
Route 53 Failover Is a feature that allows business to setup a backup or secondary
resources for internet applications. The feature prevents total failure or outage of
the primary resources or region, the traffic is automatically redirected to the
secondary resources.
Before you start the automation, you must either purchase a new domain from
Amazon https://us-east-
1.console.aws.amazon.com/route53/home#DomainRegistrationor. Transfer
your existing domain https://us-east-
1.console.aws.amazon.com/route53/home#DomainTransfer. Most companies
that use Route 53 use transfer services than purchasing a new domain directly
from AWS.
You are creating the following using terraform as the IAC:
Primary record
Secondary record
1. Clone the repository linked, to deploy multiple EC2 in availability set to be
used for this deployment.
2. This step creates the root folder for this chapter. It also creates config
folder, and the two files named provider.tf, and .gitignore.
a. Create a folder named chapter8-failover-route53.
b. Copy folder config from previous chapter to chapter8-failover-
route53 folder.
c. Copy file provider.tf, remote_state.tf to chapter8-failover-
route53 folder.
d. Copy file .gitignore to chapter8-failover-route53 folder.

3. Create a file named variable.tf with the following code:
variable "primary_awscloudautomationbook_com" {}
variable "secondary_awscloudautomationbook_com" {}
4. Create a file named zone.tf with the following code:
resource "aws_route53_zone" "route53-demo-zone" {
    name = "Your-Root-Domain-Here" # Your root 
domain here
tags = {
    name = "Demo zone"
    CreatedBy = "Your-Name-Here",
    DateCreated = "Date-Created",
    Env = "Demo"
  }
}
5. Create a file named secondary_record.tf with the following code:
resource "aws_route53_record" "demo-secondary-
record" {
    zone_id = "${aws_route53_zone.route53-demo-
zone.zone_id}"
    name    = "${aws_route53_zone.route53-demo-
zone.name}"
    type    = "A"
    ttl     = "60"
    records = 
["${var.secondary_awscloudautomation_com}"]
    failover_routing_policy {
        type = "SECONDARY"

    }
    set_identifier  = "demo-secondary-record"
    health_check_id = 
"${aws_route53_health_check.demo-secondary-
record.id}"
}
resource "aws_route53_health_check" "demo-
secondary-record" {
    ip_address        = 
"${var.secondary_awscloudautomation_com}"
    port              = "80"
    type              = "HTTP"
    resource_path     = "/"
    failure_threshold = "3"
    request_interval  = "30"
}
6. Create a file named primary_record.tf with the following code:
resource "aws_route53_record" "demo-primary-
record" {
    zone_id = "${aws_route53_zone.route53-demo-
zone.zone_id}"
    name    = "${aws_route53_zone.route53-demo-
zone.name}"
    type    = "A"
    ttl     = "60"
    records = ["${var.primary_domain_com}"]
    failover_routing_policy {
        type = "PRIMARY"

    }
    set_identifier  = "demo-primary-record"
    health_check_id = 
"${aws_route53_health_check.demo-primary-
record.id}"
}
resource "aws_route53_health_check" "demo-primary-
record" {
    ip_address        = 
"${var.primary_domain_com}"
    port              = "80"
    type              = "HTTP"
    resource_path     = "/"
    failure_threshold = "3"
    request_interval  = "30"
}
7. Create a file named terraform.tfvars with the following code:
primary_awscloudautomation_com = "" # Your 
Instance 1 IP Address
secondary_awscloudautomation_com = "" # Your 
Instance 2 IP Address
8. Now run the following commands:
Now 
use 
this 
command: 
terraform 
init 
--backend-
config=config/demo.config terraform plan terraform apply --approve-auto
9. Validate your deployment via AWS Console (see Figure 8.3):

Figure 8.3: AWS Console shows CNAME for failover routing protocol we just created
10. Push you code to a new branch in our codecommit chapter8-route53-
failover git push in the mentioned GitHub repository.
Note: Read more on AWS Route 35 here:
https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/Welcome.html
Conclusion
In this chapter, you have learned how to use Route 53 to create a DNS service in
AWS web services, you have learned how to create zone file, A record, txt record
etc. The simple routing you created includes all the DNS service that can be used
for root domain and sub domain.
You have also learnt how to use DNS to test new software deployments known as
blue-green methodology. The blue green allows the business to send traffic into
two different environments based on percentage.
Finally, this chapter also shows you how to use Route 53 to create a standby
environment in case there’s failure in a particular zone. The failover feature
remediates a failure of service in a region by automatically switching to the
secondary region.
In the next chapter, we will embark on a journey into the world of AWS Elastic
Kubernetes Service (EKS) and Fargate deployments. we will explore how these
cutting-edge technologies come together to revolutionize container orchestration
and application deployment, opening new doors for modernization in the cloud.
We will begin by understanding the core concepts of AWS EKS and Fargate, and
then progress to hands-on examples that illustrate how these technologies can
simplify and optimize your application deployments. Along the way, you will
learn about the advantages, best practices, and real-world use cases that
demonstrate why AWS EKS and Fargate are rapidly becoming the go-to choice
for organizations seeking to modernize their container workloads.
Multiple choice questions
1. AWS Route 53 routing policy type includes:
a. Simple

b. Weighted
c. Failover
d. All of the above
e. None of the above
2. Route 53 is AWS global service:
a. True
b. False
Answers
1. d
2. a
Join our book’s Discord space
Join the book’s Discord Workspace for Latest updates, Offers, Tech happenings
around the world, New Release and Sessions with the Authors:
https://discord.bpbonline.com

CHAPTER 9
AWS EKS and Fargate
Deployments
Introduction
Amazon Elastic Kubernetes Service (EKS) is a cloud-based managed
Kubernetes service offered by Amazon Web Services (AWS). It simplifies
the process of deploying, managing, and scaling containerized applications
using Kubernetes. Kubernetes is an open-source container orchestration
platform that automates the deployment, scaling, and management of
application containers.
AWS EKS provides a highly available and scalable Kubernetes control
plane, which eliminates the need for you to manage and maintain your own
Kubernetes control plane. This allows you to focus on deploying and
managing your applications while AWS takes care of the underlying
infrastructure and Kubernetes control plane management.
One of the key advantages of using AWS EKS is its seamless integration
with other AWS services, enabling you to build a comprehensive and
scalable application architecture. It provides features such as auto-scaling,
security and compliance controls, and high-performance networking to
ensure that your applications run reliably and efficiently.
With AWS EKS, you can easily deploy, manage, and scale containerized
applications across multiple clusters and regions. It offers a flexible and

consistent way to manage your container workloads, making it an ideal
choice for organizations looking to leverage the power of Kubernetes
without the operational overhead. Whether you are running microservices,
batch processing, or machine learning workloads, AWS EKS provides a
robust and scalable platform to meet your needs. Moreover, you will also
find that we have continued the steps through the sections in this chapter, as
that would give us a better understanding of AWS EKS and the steps that
come with it.
Structure
This chapter covers the following topics:
Understanding Kubernetes fundamentals
Exploring AWS EKS and Fargate
Access and security in AWS EKS and Fargate
Deploying applications on AWS EKS and Fargate
Managing and updating AWS EKS and Fargate
Objectives
The purpose of this chapter is to provide a comprehensive introduction to
Amazon Elastic Kubernetes Service (EKS) and AWS Fargate, covering
fundamental concepts, deployment strategies, and hands-on experience to
equip participants with the knowledge and skills necessary to efficiently
manage containerized applications in a cloud-native environment. AWS
EKS, or Elastic Kubernetes Service, is a managed Kubernetes service that
simplifies the deployment, management, and scaling of containerized
applications using Kubernetes on the Amazon Web Services platform. The
primary objective of AWS EKS is to provide users with a reliable, highly
available, and secure Kubernetes environment, allowing seamless
orchestration and management of containerized workloads. EKS eliminates
the need for manual setup and management of Kubernetes clusters, enabling
organizations to focus on their applications and business logic while AWS
handles the Kubernetes infrastructure. EKS facilitates the integration of

various AWS services and tools, empowering users to build, deploy, and
operate containerized applications at scale with ease.
We also delve into the fundamental concepts of AWS Fargate in this chapter
by providing a comprehensive understanding of its architecture,
components, and how it integrates with Kubernetes, gain insights into the
core principles. AWS Fargate is an integral part of Amazon Elastic
Container Service (ECS) and EKS, designed to simplify serverless
container management. The objective of AWS Fargate is to enable users to
run containers without the need to manage the underlying EC2 instances or
Kubernetes clusters. It abstracts the infrastructure layer, allowing
developers to focus solely on building, deploying, and scaling their
applications in containers. With Fargate, users pay only for the resources
their containers use, optimizing cost-efficiency and resource utilization. The
primary aim of Fargate is to provide a seamless and scalable environment
for running containers, ensuring agility and flexibility for modern
application development and deployment workflows on the AWS cloud
platform.
Understanding Kubernetes fundamentals
Kubernetes, often referred to as K8s, is a powerful container orchestration
platform that automates the deployment, scaling, and management of
containerized applications. At its core, Kubernetes simplifies the
complexity of managing multiple containers by providing a centralized
control plane that orchestrates the deployment and maintenance of these
containers. It abstracts away the underlying infrastructure, allowing
developers and operators to focus on application logic and scalability.
Kubernetes enables automated scaling, load balancing, and self-healing,
ensuring that applications run reliably and efficiently.
One fundamental concept of Kubernetes is the pod. A pod is the basic unit
of deployment and scheduling in Kubernetes. It represents a group of one or
more containers that share networking and storage resources. Containers
within a pod can communicate with each other using localhost, making
them a cohesive unit. Pods are scheduled onto nodes (physical or virtual
machines) within a Kubernetes cluster, and Kubernetes ensures that the

desired number of pods are always running, restarting them if they fail or
are terminated.
Another critical concept is containers. Containers are lightweight, portable,
and self-contained units that package software and its dependencies,
allowing for consistent deployment across various environments.
Kubernetes leverages containerization technologies like Docker to
encapsulate application components into containers. This approach
simplifies deployment, scaling, and management, making it easier to
maintain consistency and reproducibility across different environments.
Services in Kubernetes facilitate communication and networking between
different parts of an application or between applications. A Kubernetes
service acts as an abstraction layer that provides a stable endpoint for
accessing a set of pods. This decouples the application logic from the
network, allowing for easy scaling and deployment without impacting the
accessibility of the services.
Understanding Kubernetes fundamentals, including pods, containers, and
services, is crucial for effectively utilizing this platform to deploy and
manage modern, cloud-native applications. With its robust and flexible
architecture, Kubernetes empowers developers to build scalable, resilient
applications while providing operators with powerful tools to automate and
streamline the management of these applications in a dynamic and rapidly
evolving IT landscape.
Exploring AWS EKS and Fargate
Amazon Elastic Kubernetes Service and AWS Fargate are two prominent
services offered by Amazon Web Services that cater to the dynamic world
of container orchestration and management. AWS EKS is a fully managed
Kubernetes service, simplifying the deployment and operation of
Kubernetes clusters. On the other hand, AWS Fargate is a serverless
compute engine for containers, abstracting away the need to manage the
underlying infrastructure.
When delving into AWS EKS, one immediately appreciates its seamless
integration with other AWS services. It allows for effortless scaling of
applications and workloads, ensuring they run efficiently and reliably. EKS

abstracts the complexities of managing the Kubernetes control plane,
enabling developers and operators to focus on their applications rather than
the underlying infrastructure. Its support for Kubernetes-native tooling and
plugins facilitates smoother deployment and operation of containerized
applications.
On the flip side, AWS Fargate offers an intriguing proposition for those
seeking to optimize resource utilization and minimize operational overhead.
By leveraging Fargate, users can run containers without managing the
underlying EC2 instances, enhancing agility and reducing administrative
burden. Fargate provisions the exact resources needed for each task,
optimizing costs, and enabling a pay-as-you-go model. This serverless
approach to containers liberates developers to concentrate solely on
designing and deploying their applications.
Together, AWS EKS and Fargate present a powerful combination for
modern container orchestration. EKS provides the robust Kubernetes
platform, while Fargate streamlines the operational aspects, ultimately
enhancing productivity and efficiency. This amalgamation empowers
organizations to build, deploy, and scale containerized applications
seamlessly, thus embracing the benefits of both Kubernetes and serverless
computing within the AWS ecosystem.
Introduction to containerization
Containerization is a ground breaking technology that has revolutionized
how applications are developed, deployed, and managed. Amazon Web
Services offer a robust and comprehensive platform for hosting and
managing containers, providing a highly efficient and scalable environment
for running containerized applications. In essence, containerization allows
developers to package their applications and all their dependencies into a
standardized unit called a container. These containers can run consistently
across various environments, making it easier to manage, scale, and deploy
applications seamlessly.
AWS offers a range of services and tools to support containerization, the
most prominent being Amazon Elastic Container Service and Amazon
Elastic Kubernetes Service. Amazon ECS allows users to easily deploy,
manage, and scale containerized applications using Docker containers,

providing a highly available and secure environment. On the other hand,
Amazon EKS simplifies the orchestration and management of containerized
applications using Kubernetes, an open-source container orchestration
platform. With these services, developers can leverage the benefits of
containerization, including rapid deployment, resource efficiency, and
enhanced portability, while seamlessly integrating with the broader AWS
ecosystem.
The benefits of containerization on AWS extend beyond simplified
application deployment. Containers enable organizations to achieve better
resource utilization, faster development cycles, and increased application
scalability. Additionally, AWS offers a variety of integrations with other
services such as AWS Fargate for serverless container orchestration and
AWS Lambda for event-driven serverless computing, further enhancing the
versatility and power of containerized applications. As organizations
continue to embrace modern software development practices,
containerization on AWS emerges as a pivotal technology, empowering
businesses to build, deploy, and manage applications with unparalleled
efficiency and agility.
Overview of EKS
EKS allows developers to focus on building and scaling their applications
while offloading the complexities of Kubernetes cluster management to
AWS. This service is designed to provide a reliable and secure environment
for running production-grade workloads using containers. EKS leverages
the power of Kubernetes to enable automated scaling, self-healing, and
efficient management of containerized applications. It integrates seamlessly
with AWS services and provides tight integration with AWS Identity and
Access Management (IAM), allowing for fine-grained access control and
enhanced security. EKS also supports Kubernetes features such as native
VPC networking, allowing you to use familiar AWS networking constructs
within your Kubernetes clusters.
One of the significant advantages of using AWS EKS is its ability to
provide a highly available and reliable platform for running applications at
scale. EKS automatically manages the Kubernetes control plane, ensuring
high availability and redundancy across multiple availability zones. It also

allows for quick and efficient scaling of clusters to handle changes in
workload demands, providing agility and flexibility to businesses.
In addition to this, AWS EKS offers seamless integration with other AWS
services, such as Amazon EC2, Amazon RDS, and Amazon S3. This
enables a streamlined development and deployment process, allowing
developers to leverage a wide range of AWS services to enhance their
applications. EKS provides comprehensive monitoring, logging, and
troubleshooting capabilities through integrations with AWS CloudWatch
and AWS CloudTrail, giving you insights into the performance and
behavior of your applications and clusters.
In summary, AWS EKS is a powerful managed Kubernetes service that
simplifies the deployment and management of containerized applications. It
offers reliability, scalability, and seamless integration with other AWS
services, making it an excellent choice for organizations looking to run
production workloads using Kubernetes in a cloud-native environment.
Setting-up AWS EKS and Fargate
Setting up Amazon Elastic Kubernetes Service (EKS) and leveraging
AWS Fargate for container orchestration is a powerful combination that
streamlines the deployment and management of containerized applications.
To embark on this journey, you begin by creating an EKS cluster through
the AWS Management Console or using the AWS CLI. Select the desired
AWS region and configure the necessary security groups and IAM roles to
ensure smooth communication and access control within the cluster.
Once the EKS cluster is established, the integration with AWS Fargate
begins. Fargate eliminates the need to manage and provision underlying
EC2 instances, enabling you to focus solely on deploying and running your
containers. Define Kubernetes pods and deployments to leverage Fargate as
the launch type, specifying the required CPU and memory resources. This
integration ensures optimal resource utilization and cost efficiency, as
Fargate allocates resources dynamically based on your application’s needs.
Furthermore, AWS Fargate provides seamless scaling capabilities,
automatically adjusting resources to match the demands of your workload.
Utilize Kubernetes Horizontal Pod Autoscaler (HPA) and Cluster
Autoscaler to efficiently manage scaling based on metrics and cluster

resource availability. This dynamic scaling enhances application
performance and ensures a consistent user experience, even during peak
usage periods.
Incorporating AWS EKS with Fargate optimizes your application
deployment pipeline. Implement and utilizing tools like GitLab CI/CD or
GitHub Actions, to automate the deployment process. Define declarative
manifests for your Kubernetes resources and Git triggers to streamline the
application deployment and ensure version control, thus promoting a robust
and agile development workflow.
Moreover, leverage AWS managed services like Amazon RDS for
databases and Amazon S3 for storage, seamlessly integrating them with
your EKS and Fargate setup. This enables your applications to utilize
scalable, reliable, and secure storage and database solutions while
maintaining high availability and durability, further enhancing the overall
resilience and performance of your architecture.
In conclusion, combining AWS EKS and Fargate unlocks a plethora of
benefits, including efficient resource management, seamless scaling,
automated deployments, and integration with other AWS managed services.
This integration empowers you to build and deploy resilient, cost-effective,
and highly performant containerized applications in a streamlined and
efficient manner.
Carefully follow the step-by-step below to automate EKS and deploy
various services:
1. We will be introduced to some new providers in this step, and also
explore the latest EKS version. EKS version is at 1.25 as of the time
of writing this book:
a. Create a folder named chapter9-eks-fargate for this chapter.
b. Copy the config folder from previous chapters to chapter9-eks-
fargate folder.
c. Copy file remote_state.tf from previous chapters to chapter9-
eks-fargate folder.

d. Copy file .gitignore from previous chapters to chapter9-eks-
fargate folder.
e. Create a file provider.tf with the following code:
  terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 4.15.0"
    }
    random = {
      source  = "hashicorp/random"
      version = "3.1.0"
    }
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = ">= 2.0.1"
    }
  }
}
provider "aws" {
  region = "us-east-1"
}
variable "cluster_name" {

  default = "demoeks"
}
variable "cluster_version" {
  default = "1.25"
}
2. In this step, you will create the VPC file, and enable DNS support for
the hostnames, and in anticipation of EFS file system. It should be
noted that if the DNS is not enabled, the CSI driver will fail.
Create a file vpc.tf with the following code:
resource "aws_vpc" "eks-vpc" {
  cidr_block = "10.0.0.0/16"
# This is required for EFS
  enable_dns_support   = true
  enable_dns_hostnames = true
  tags = {
    Name         = "eks-efs"
    CreatedBy    = "Your-Name-Here"
    DateCreated  = "Date-Created-Here"
    Env          = "Demo"
  }
}
3. This file created four subnets, it created two private subnets and two
public subnets in availability zones:
Create a file "subnets.tf" with the code 
below: 

resource "aws_subnet" "private-us-east-1a" {
  vpc_id            = aws_vpc.eks-vpc.id   
cidr_block        = "10.0.0.0/19"   
availability_zone = "us-east-1a"
  tags = {
    "Name"                                      
= "private-us-east-1a-subnet"
    "ubernetes.io/role/internal-elb"           
= "1"
    "ubernetes.io/cluster/${var.cluster_name}" 
= "owned"
    DateCreated                                 
= "Your-Name-Here"
    CreatedBy                                   
= "Date-Created-Here"
    Env                                         
= "Demo"
  }
}
resource "aws_subnet" "private-us-east-1b" {
  vpc_id            = aws_vpc.eks-vpc.id
  cidr_block        = "10.0.32.0/19"
  availability_zone = "us-east-1b"
  tags = {
    "Name"                                      
= "private-us-east-1b-subnet"

    "ubernetes.io/role/internal-elb"           
= "1"
    "ubernetes.io/cluster/${var.cluster_name}" 
= "owned"
    DateCreated                                 
= "Your-Name-Here"
    CreatedBy                                   
= "Date-Created-Here"
    Env                                         
= "Demo"
  }
}
resource "aws_subnet" "public-us-east-1a" {
  vpc_id                  = aws_vpc.eks-vpc.id
  cidr_block              = "10.0.64.0/19"
  availability_zone       = "us-east-1a"
  map_public_ip_on_launch = true
  tags = {
    "Name"                                      
= "public-us-east-1a-subnet"
    "ubernetes.io/role/elb"                    
= "1"
    "ubernetes.io/cluster/${var.cluster_name}" 
= "owned"
    DateCreated                                 
= "Your-Name-Here"

    CreatedBy                                   
= "Date-Created-Here"
    Env                                         
= "Demo"
  }
}
resource "aws_subnet" "public-us-east-1b" {
  vpc_id                  = aws_vpc.eks-vpc.id
  cidr_block              = "10.0.96.0/19"
  availability_zone       = "us-east-1b"
  map_public_ip_on_launch = true
  tags = {
    "Name"                                      
= "public-us-east-1b-subnet"
    "ubernetes.io/role/elb"                    
= "1"
    "ubernetes.io/cluster/${var.cluster_name}" 
= "owned"
    DateCreated                                 
= "Your-Name-Here"
    CreatedBy                                   
= "Date-Created-Here"
    Env                                         
= "Demo"
  }
}

4. This file is used to create the route table.
Create a file routetable.tf with the following code:
resource "aws_route_table" "private-
routetable" {
  vpc_id = aws_vpc.eks-vpc.id
  route {
    cidr_block     = "0.0.0.0/0"
    nat_gateway_id = aws_nat_gateway.nat-gw.id
  }
  tags = {
    Name               = "Private Route Table"
    CreatedBy          = "Your-Name-Here"
    DateCreated        = "Date-Created-Here"
    Env                = "Demo"
  }
}
resource "aws_route_table" "public-routetable" 
{
  vpc_id = aws_vpc.eks-vpc.id
  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.eks-vpc-
igw.id
  }

  tags = {
    Name = "Public Route Table"
    CreatedBy          = "Your-Name-Here"
    DateCreated        = "Date-Created-Here"
    Env                = "Demo"
  }
}
resource "aws_route_table_association" 
"private-us-east-1a" {
  subnet_id      = aws_subnet.private-us-east-
1a.id   route_table_id = 
aws_route_table.private-routetable.id
}
resource "aws_route_table_association" 
"private-us-east-1b" {
  subnet_id      = aws_subnet.private-us-east-
1b.id   route_table_id = 
aws_route_table.private-routetable.id
}
resource "aws_route_table_association" 
"public-us-east-1a" {
  subnet_id      = aws_subnet.public-us-east-
1a.id   route_table_id = 
aws_route_table.public-routetable.id
}
resource "aws_route_table_association" 
"public-us-east-1b" {

  subnet_id      = aws_subnet.public-us-east-
1b.id   route_table_id = 
aws_route_table.public-routetable.id
}
5. This file is used to create the Network Address Translation (NAT)
gateway.
Create a file nat_gw.tf with the following code:
resource "aws_eip" "nat-gw-eip" {
  vpc = true
  tags = {
    Name = "NAT GW EIP"     CreatedBy   = 
"Your-Name-Here"     DateCreated = "Date-
Created-Here"     Env         = "Demo"
  }
}
resource "aws_nat_gateway" "nat-gw" {
  allocation_id = aws_eip.nat-gw-eip.id   
subnet_id     = aws_subnet.public-us-east-
1a.id
  tags = {
    Name        = "NAT GW"     CreatedBy   = 
"Your-Name-Here"     DateCreated = "Date-
Created-Here"     Env         = "Demo"
  }
  depends_on = [aws_internet_gateway.eks-vpc-
igw]
}

6. This file is used to create the Internet Gateway (IGW).
Create a file igw.tf with the following code:
resource "aws_internet_gateway" "eks-vpc-igw" 
{
  vpc_id = aws_vpc.eks-vpc.id
  tags = {
    Name          = "EKS VPC IGW"     
CreatedBy     = "Your-Name-Here"     
DateCreated   = "Date-Created-Here"     Env     
= "Demo"
  }
}
7. This step will help you create the EKS control plane. The control
plane is use for a variety of things but has been created specifically
for creating AWS Fargate profiles. In addition, you will create IAM
assume role you needed to manage the EKS services, manage node
pools, and to make API calls.
Create a file eks.tf with the following code:
  resource "aws_iam_role" "eks-cluster" {
  name = "eks-cluster-${var.cluster_name}"
  assume_role_policy = <<POLICY
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {

        "Service": "eks.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
POLICY
}
resource "aws_iam_role_policy_attachment" 
"amazon-eks-cluster-policy" {
  policy_arn = 
"arn:aws:iam::aws:policy/AmazonEKSClusterPolic
y"
  role       = aws_iam_role.eks-cluster.name
}
resource "aws_eks_cluster" "cluster" {
  name     = var.cluster_name
  version  = var.cluster_version
  role_arn = aws_iam_role.eks-cluster.arn
 
    endpoint_private_access = false
    endpoint_public_access  = true
    public_access_cidrs     = ["0.0.0.0/0"]
    subnet_ids = [

      aws_subnet.private-us-east-1a.id,
      aws_subnet.private-us-east-1b.id,
      aws_subnet.public-us-east-1a.id,
      aws_subnet.public-us-east-1b.id
    ]
  }
  depends_on = 
[aws_iam_role_policy_attachment.amazon-eks-
cluster-policy]
}
Before we go to Step 8, let us look at the main IAM and security of AWS
EKS and AWS Fargate.
Access and security in AWS EKS and Fargate
In AWS EKS, access control is a critical aspect of ensuring the security of
the Kubernetes cluster. EKS integrates with AWS Identity and Access
Management, allowing administrators to define fine-grained permissions
for users and services. IAM roles and policies can be configured to grant or
restrict access to specific resources within the EKS environment. Utilizing
IAM, organizations can implement the principle of least privilege, ensuring
that users and applications have only the necessary permissions to perform
their respective tasks. Additionally, EKS supports Role-Based Access
Control (RBAC) within Kubernetes, enabling granular control over access
to Kubernetes resources at the cluster level.
Security in AWS Fargate revolves around the principle of a serverless,
managed compute environment for containers. Fargate abstracts away the
underlying infrastructure, reducing the attack surface and providing a
secure runtime environment for applications. AWS manages and maintains
the Fargate infrastructure, applying security patches and updates to the
underlying hosts. Furthermore, Fargate offers encryption options for data in
transit and at rest, ensuring data security throughout the application

lifecycle. By default, Fargate tasks run in an isolated environment, offering
strong process and workload isolation, enhancing overall system security.
AWS provides additional security measures such as Amazon Virtual
Private Cloud (VPC) integration, which allows users to isolate and control
network traffic between Fargate tasks and other resources within the VPC.
Security groups and Network Access Control Lists (NACLS) can be
configured to control inbound and outbound traffic, providing an additional
layer of security. AWS Key Management Service (KMS) integration
offers robust encryption capabilities for sensitive data, ensuring secure
storage and transmission of information within the Fargate environment.
Overall, AWS EKS and Fargate empower organizations to maintain a strong
balance between accessibility and security, allowing efficient deployment
and operation of containerized applications while implementing
comprehensive access control and security measures to safeguard critical
data and resources. Let us continue with our setup with from the previous
steps, from Step 8 to Step 10 that is primarily on IAM and security of AWS
EKS and Fargate:
8. This step is for creating IAM role for Kubernetes service account.
The Kubernetes service account need permissions to access the
containers in any of the pods that uses the service account. By
creating this IAM role, all the pods do not extra permission to call
the Kubernetes node APIs.
Create a file oidc_iam.tf with the following code:
  data "tls_certificate" "eks-cert" {
  url = 
aws_eks_cluster.cluster.identity[0].oidc[0].is
suer
}
resource "aws_iam_openid_connect_provider" 
"eks-oidc" {
  client_id_list  = ["sts.amazonaws.com"]

  thumbprint_list = [data.tls_certificate.eks-
cert.certificates[0].sha1_fingerprint]
  url             = 
aws_eks_cluster.cluster.identity[0].oidc[0].is
suer
}
9. This file uses JSON file we created later to create AWS Load
Balancer IAM role and permissions that help with the assume role.
Create a file lb_controller_iam.tf with the following code:
  data "aws_iam_policy_document" 
"aws_load_balancer_controller_assume_role_poli
cy" {
  statement {
    actions = 
["sts:AssumeRoleWithWebIdentity"]
    effect  = "Allow"
    condition {
      test     = "StringEquals"
      variable = 
"${replace(aws_iam_openid_connect_provider.eks
-oidc.url, "https://", "")}:sub"
      values   = ["system:serviceaccount:kube-
system:aws-load-balancer-controller"]
    }
    principals {
      identifiers = 
[aws_iam_openid_connect_provider.eks-oidc.arn]
      type        = "Federated"

    }
  }
}
resource "aws_iam_role" 
"aws_load_balancer_controller" {
  assume_role_policy = 
data.aws_iam_policy_document.aws_load_balancer
_controller_assume_role_policy.json
  name               = "aws-load-balancer-
controller"
}
resource "aws_iam_policy" 
"aws_load_balancer_controller" {
  policy = 
file("./AWSLoadBalancerController.json")
  name   = "AWSLoadBalancerController"
} 
resource "aws_iam_role_policy_attachment" 
"aws_load_balancer_controller_attach" {
  role       = 
aws_iam_role.aws_load_balancer_controller.name
  policy_arn = 
aws_iam_policy.aws_load_balancer_controller.ar
n
}
output "aws_load_balancer_controller_role_arn" 
{

  value = 
aws_iam_role.aws_load_balancer_controller.arn
}
10. This JSON file below is used to describe the IAM permission for the
controller IAM. This JSON file is available online for download, but
the skill will be where to use.
Create a file AWSLoadBalancerController.json with the following
code:
  {
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "iam:CreateServiceLinkedRole"
            ],
            "Resource": "*",
            "Condition": {
                "StringEquals": {
                    "iam:AWSServiceName": 
"elasticloadbalancing.amazonaws.com"
                }
            }
        },
        {

            "Effect": "Allow",
            "Action": [
                
"ec2:DescribeAccountAttributes",
                "ec2:DescribeAddresses",
                
"ec2:DescribeAvailabilityZones",
                
"ec2:DescribeInternetGateways",
                "ec2:DescribeVpcs",
                
"ec2:DescribeVpcPeeringConnections",
                "ec2:DescribeSubnets",
                "ec2:DescribeSecurityGroups",
                "ec2:DescribeInstances",
                
"ec2:DescribeNetworkInterfaces",
                "ec2:DescribeTags",
                "ec2:GetCoipPoolUsage",
                "ec2:DescribeCoipPools",
                
"elasticloadbalancing:DescribeLoadBalancers",
                
"elasticloadbalancing:DescribeLoadBalancerAttr
ibutes",
                
"elasticloadbalancing:DescribeListeners",

                
"elasticloadbalancing:DescribeListenerCertific
ates",
                
"elasticloadbalancing:DescribeSSLPolicies",
                
"elasticloadbalancing:DescribeRules",
                
"elasticloadbalancing:DescribeTargetGroups",
                
"elasticloadbalancing:DescribeTargetGroupAttri
butes",
                
"elasticloadbalancing:DescribeTargetHealth",
                
"elasticloadbalancing:DescribeTags"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "cognito-
idp:DescribeUserPoolClient",
                "acm:ListCertificates",
                "acm:DescribeCertificate",
                "iam:ListServerCertificates",

                "iam:GetServerCertificate",
                "waf-regional:GetWebACL",
                "waf-
regional:GetWebACLForResource",
                "waf-
regional:AssociateWebACL",
                "waf-
regional:DisassociateWebACL",
                "wafv2:GetWebACL",
                "wafv2:GetWebACLForResource",
                "wafv2:AssociateWebACL",
                "wafv2:DisassociateWebACL",
                "shield:GetSubscriptionState",
                "shield:DescribeProtection",
                "shield:CreateProtection",
                "shield:DeleteProtection"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                
"ec2:AuthorizeSecurityGroupIngress",

                
"ec2:RevokeSecurityGroupIngress"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "ec2:CreateSecurityGroup"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "ec2:CreateTags"
            ],
            "Resource": 
"arn:aws:ec2:*:*:security-group/*",
            "Condition": {
                "StringEquals": {
            "ec2:CreateAction": 
"CreateSecurityGroup"
                },

                "Null": {
                    
"aws:RequestTag/elbv2.k8s.aws/cluster": 
"false"
                }
            }
        },
        {
            "Effect": "Allow",
            "Action": [
                "ec2:CreateTags",
                "ec2:DeleteTags"
            ],
            "Resource": 
"arn:aws:ec2:*:*:security-group/*",
            "Condition": {
                "Null": {
                    
"aws:RequestTag/elbv2.k8s.aws/cluster": 
"true",
                    
"aws:ResourceTag/elbv2.k8s.aws/cluster": 
"false"
                }
            }
        },

        {
            "Effect": "Allow",
            "Action": [
                
"ec2:AuthorizeSecurityGroupIngress",
                
"ec2:RevokeSecurityGroupIngress",
                "ec2:DeleteSecurityGroup"
            ],
            "Resource": "*",
            "Condition": {
                "Null": {
                    
"aws:ResourceTag/elbv2.k8s.aws/cluster": 
"false"
                }
            }
        },
        {
            "Effect": "Allow",
            "Action": [
                
"elasticloadbalancing:CreateLoadBalancer",
                
"elasticloadbalancing:CreateTargetGroup"
            ],

            "Resource": "*",
            "Condition": {
                "Null": {
                    
"aws:RequestTag/elbv2.k8s.aws/cluster": 
"false"
                }
            }
        },
        {
            "Effect": "Allow",
            "Action": [
                
"elasticloadbalancing:CreateListener",
                
"elasticloadbalancing:DeleteListener",
                
"elasticloadbalancing:CreateRule",
                
"elasticloadbalancing:DeleteRule"
            ],
            "Resource": "*"
        },
        {
        "Effect": "Allow",
            "Action": [

                
"elasticloadbalancing:AddTags",
                
"elasticloadbalancing:RemoveTags"
            ],
            "Resource": [
                
"arn:aws:elasticloadbalancing:*:*:targetgroup/
*/*",
                
"arn:aws:elasticloadbalancing:*:*:loadbalancer
/net/*/*",
                
"arn:aws:elasticloadbalancing:*:*:loadbalancer
/app/*/*"
            ],
            "Condition": {
                "Null": {
                    
"aws:RequestTag/elbv2.k8s.aws/cluster": 
"true",
                    
"aws:ResourceTag/elbv2.k8s.aws/cluster": 
"false"
                }
            }
        },
        {

            "Effect": "Allow",
            "Action": [
                
"elasticloadbalancing:AddTags",
                
"elasticloadbalancing:RemoveTags"
            ],
            "Resource": [
                
"arn:aws:elasticloadbalancing:*:*:listener/net
/*/*/*",
                
"arn:aws:elasticloadbalancing:*:*:listener/app
/*/*/*",
                
"arn:aws:elasticloadbalancing:*:*:listener-
rule/net/*/*/*",
                
"arn:aws:elasticloadbalancing:*:*:listener-
rule/app/*/*/*"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                
"elasticloadbalancing:ModifyLoadBalancerAttrib
utes",

                
"elasticloadbalancing:SetIpAddressType",
                
"elasticloadbalancing:SetSecurityGroups",
                
"elasticloadbalancing:SetSubnets",
                
"elasticloadbalancing:DeleteLoadBalancer",
                
"elasticloadbalancing:ModifyTargetGroup",
                
"elasticloadbalancing:ModifyTargetGroupAttribu
tes",
                
"elasticloadbalancing:DeleteTargetGroup"
            ],
            "Resource": "*",
            "Condition": {
                "Null": {
                    
"aws:ResourceTag/elbv2.k8s.aws/cluster": 
"false"
                }
            }
        },
        {
            "Effect": "Allow",

            "Action": [
                
"elasticloadbalancing:RegisterTargets",
                
"elasticloadbalancing:DeregisterTargets"
            ],
            "Resource": 
"arn:aws:elasticloadbalancing:*:*:targetgroup/
*/*"
        },
        {
            "Effect": "Allow",
            "Action": [
                
"elasticloadbalancing:SetWebAcl",
         
"elasticloadbalancing:ModifyListener",
                
"elasticloadbalancing:AddListenerCertificates"
,
                
"elasticloadbalancing:RemoveListenerCertificat
es",
                
"elasticloadbalancing:ModifyRule"
            ],
            "Resource": "*"

        }
    ]
}
Before we go to Step 11, let us look at the main monitoring and logging of
AWS EKS and AWS Fargate.
Monitoring and logging in AWS EKS and Fargate
Monitoring and logging are essential components of managing and
optimizing the performance, security, and reliability of applications and
infrastructure in AWS Elastic Kubernetes Service (EKS) and AWS
Fargate. In EKS, a managed Kubernetes service, and Fargate, a serverless
compute engine for containers, effective monitoring and logging play a
critical role in understanding and managing the containerized workloads.
In AWS EKS, monitoring involves leveraging AWS CloudWatch, a
comprehensive monitoring service. CloudWatch allows for the collection
and tracking of various metrics, such as CPU and memory utilization,
network performance, and application-level metrics. Additionally, custom
metrics specific to the application or workload can be logged and monitored
using CloudWatch. Integration with tools like Prometheus and Grafana is
also possible to enhance monitoring capabilities, enabling advanced
querying and visualization of metrics.
Logging in AWS EKS involves utilizing Amazon CloudWatch Logs to
centralize and retain logs from various components of the EKS cluster. This
includes logs from applications running in containers, as well as logs from
the Kubernetes control plane and worker nodes. AWS also provides the
option to integrate with AWS X-Ray for distributed tracing, allowing for
deep insights into application performance and dependencies. Proper
configuration and management of log groups, log streams, and log retention
policies ensure efficient logging and analysis for troubleshooting, security
auditing, and compliance purposes.
Similarly, in AWS Fargate, monitoring and logging are vital for gaining
insights into containerized workloads. AWS provides integration with AWS
CloudWatch and AWS X-Ray for monitoring and tracing container
performance and application behavior. With CloudWatch, it is possible to

set up alarms based on specific thresholds to proactively respond to
performance issues or unexpected behaviors. AWS Fargate also supports
native integration with AWS FireLens, enabling the aggregation and
redirection of logs to various destinations, including Amazon CloudWatch
Logs, Amazon S3, and Amazon Elasticsearch for comprehensive log
analysis and insights.
In summary, effective monitoring and logging in AWS EKS and Fargate
involve utilizing AWS CloudWatch for metric collection, setting up alarms,
and integrating with third-party monitoring tools for enhanced capabilities.
Centralized logging through Amazon CloudWatch Logs and integration
with AWS X-Ray for tracing ensure comprehensive monitoring and
visibility into containerized workloads, aiding in performance optimization,
troubleshooting, and ensuring the overall health of the applications and
infrastructure. Now continue to next step in our setup with Step 11 on
monitoring and logging of AWS EKS and Fargate.
11. This step is for creating metric server for monitoring the Pod’s CPU
and Memory usage.
The metric server is deployed with HELM. You can read more about
HELM https://helm.sh/docs/topics/charts/ . We also make a call to
Kubernetes repository on github.io.
Create a file metric_server.tf with the following code:
  provider "helm" {
  kubernetes {
    host                   = 
aws_eks_cluster.cluster.endpoint
    cluster_ca_certificate = 
base64decode(aws_eks_cluster.cluster.certifica
te_authority[0].data)
    exec {
      api_version = 
"client.authentication.k8s.io/v1beta1"

      args        = ["eks", "get-token", "--
cluster-name", aws_eks_cluster.cluster.id]
      command     = "aws"
    }
  }
}
resource "helm_release" "metrics-server" {
  name = "metrics-server"
  repository = "https://kubernetes-
sigs.github.io/metrics-server/"
  chart      = "metrics-server"
  namespace  = "kube-system"
  version    = "3.8.2"
  set {
    name  = "metrics.enabled"
    value = false
  }
  depends_on = [aws_eks_fargate_profile.kube-
system]
}
Before we go to Step 12, let us learn more about deploying
application on AWS EKS and AWS Fargate.
Deploying applications on AWS EKS and Fargate
When deploying applications on AWS EKS, you start by provisioning and
configuring your Kubernetes cluster. EKS takes care of the heavy lifting,

such as managing the control plane and ensuring high availability. Once the
cluster is set up, you can define your application workloads using
Kubernetes manifests. These manifests describe the desired state of your
application, including the number of replicas, resource requirements, and
networking configurations.
To deploy an application on AWS EKS, you first create Docker images of
your application components and store them in a container registry like
Amazon ECR (Elastic Container Registry). Then, using Kubernetes
manifests, you define your application’s deployment, services, and any
necessary configurations. EKS manages the orchestration of these
components, ensuring they run, and scale as specified.
Deploying on Fargate involves creating a Fargate profile within your EKS
cluster and specifying the namespace or workload for which you want to
use Fargate. When you deploy your application, Kubernetes scheduler
places the pods onto the Fargate instances based on the defined Fargate
profile. Fargate then manages the underlying compute resources and scales
your application based on demand, providing a seamless serverless
experience.
In summary, AWS EKS and Fargate offer powerful options for deploying
applications in a scalable and efficient manner, allowing you to choose the
level of control and management that best fits your application’s needs.
Whether you prefer the flexibility of EKS or the serverless simplicity of
Fargate, AWS provides a comprehensive ecosystem to support your
deployment requirements. Let us continue to step 12 to step 21 which
focuses on application deployment on AWS EKS and Fargate.
12. This step is for creating kube-system which contains service
accounts that are used to run the Kubernetes controllers. These
service accounts are granted permissions to create pods.
13. Create a file kube_system_profile.tf with the following code:
  resource "aws_iam_role" "eks-fargate-
profile" {
  name = "eks-fargate-profile"
  assume_role_policy = jsonencode({

    Statement = [{
      Action = "sts:AssumeRole"
      Effect = "Allow"
      Principal = {
        Service = "eks-fargate-
pods.amazonaws.com"
      }
    }]
    Version = "2012-10-17"
  })
}
resource "aws_iam_role_policy_attachment" 
"eks-fargate-profile" {
  policy_arn = 
"arn:aws:iam::aws:policy/AmazonEKSFargatePodEx
ecutionRolePolicy"
  role       = aws_iam_role.eks-fargate-
profile.name
}
resource "aws_eks_fargate_profile" "kube-
system" {
  cluster_name           = 
aws_eks_cluster.cluster.name
  fargate_profile_name   = "kube-system"
  pod_execution_role_arn = aws_iam_role.eks-
fargate-profile.arn

  subnet_ids = [
    aws_subnet.private-us-east-1a.id,
    aws_subnet.private-us-east-1b.id
  ]
  selector {
    namespace = "kube-system"
  }
} 
data "aws_eks_cluster_auth" "eks" {
  name = aws_eks_cluster.cluster.id
}
resource "null_resource" "k8s_patcher" {
  depends_on = [aws_eks_fargate_profile.kube-
system]
  triggers = {
    endpoint = 
aws_eks_cluster.cluster.endpoint
    ca_crt   = 
base64decode(aws_eks_cluster.cluster.certifica
te_authority[0].data)
    token    = 
data.aws_eks_cluster_auth.eks.token
  }
  provisioner "local-exec" {
    command = <<EOH

cat >/tmp/ca.crt <<EOF
${base64decode(aws_eks_cluster.cluster.certifi
cate_authority[0].data)}
EOF
kubectl \
  --
server="${aws_eks_cluster.cluster.endpoint}" \
  --certificate_authority=/tmp/ca.crt \
  --
token="${data.aws_eks_cluster_auth.eks.token}" 
\
  patch deployment coredns \
  -n kube-system --type json \
  -p='[{"op": "remove", "path": 
"/spec/template/metadata/annotations/eks.amazo
naws.com~1compute-type"}]'
EOH
  }
  lifecycle {
    ignore_changes = [triggers]
  }
}
14. This step is for creating EFS for persistent volume to support pod’s
file system. AWS Fargate does not support EBS for obvious reason,
however, it supports EFS and the ReadWriteMany mode. It can be
mount automatically to multiple pods and the configuration of the
EFS support bursting which makes EFS expands as needed, and this
EFS configuration supports encryption as well.

15. Create a file kube_system_profile.tf with the following code:
  resource "aws_efs_file_system" "eks-efs" {
  creation_token = "eks"
  performance_mode = "generalPurpose"
  throughput_mode  = "bursting"
  encrypted        = true
   lifecycle_policy {
     transition_to_ia = "AFTER_30_DAYS"
   }
  tags = {
    Name          = "eks-efs"
    CreatedBy     = "Your-Name-Here"
    DateCreated   = "Date-Created-Here"
    Env           = "Demo"
  }
}
resource "aws_efs_mount_target" "zone-a" {
  file_system_id  = aws_efs_file_system.eks-
efs.id
  subnet_id       = aws_subnet.private-us-
east-1a.id
  security_groups = 
[aws_eks_cluster.cluster.vpc_config[0].cluster
_security_group_id]
}

resource "aws_efs_mount_target" "zone-b" {
  file_system_id  = aws_efs_file_system.eks-
efs.id
  subnet_id       = aws_subnet.private-us-
east-1b.id
  security_groups = 
[aws_eks_cluster.cluster.vpc_config[0].cluster
_security_group_id]
}
16. This step is to prep create a namespace for our pod’s deployment.
Namespace is used to organize cluster of pods into virtual sub-
clusters for organization and management. You can read more about
namespace –
https://kubernetes.io/docs/concepts/overview/working-with-
objects/namespaces/ .
Create a file staging_profile.tf with the following code:
  resource "aws_eks_fargate_profile" "staging" 
{
  cluster_name           = 
aws_eks_cluster.cluster.name
  fargate_profile_name   = "staging"
  pod_execution_role_arn = aws_iam_role.eks-
fargate-profile.arn
  subnet_ids = [
    aws_subnet.private-us-east-1a.id,
    aws_subnet.private-us-east-1b.id
  ]
  selector {

    namespace = "staging"
  }
}
17. This step deploys application in our Kubernetes cluster. You must
use HELM Charts or use traditional YAML configuration files. This
step deploys Apache pod in our Kubernetes.
a. Create a folder named apps.
b. Create a file basic_deployment.yaml with the following code
inside the apps folder:
  ---
apiVersion: v1
kind: Namespace
metadata:
  name: staging
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: php-apache
  namespace: staging
spec:
  selector:
    matchLabels:
      run: php-apache

  replicas: 1
  template:
    metadata:
      labels:
        run: php-apache
    spec:
      containers:
      - name: php-apache
        image: k8s.gcr.io/hpa-example
        ports:
        - containerPort: 80
        resources:
          limits:
            cpu: 200m
            memory: 256Mi
          requests:
             cpu: 200m
            memory: 256Mi
18. This step deploys applications in our Kubernetes cluster. You must
use HELM Charts or use traditional YAML configuration files. This
step also deploys EFS pod in our Kubernetes.
a. Create a folder named apps.
b. Create a file efs.yaml with the following code inside the apps
folder.

       ---
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: efs-sc
provisioner: efs.csi.aws.com
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: efs-pv
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
  - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  storageClassName: efs-sc
  csi:
    driver: efs.csi.aws.com
    volumeHandle: fs-<your-id>
---

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: efs-claim
  namespace: staging
spec:
  accessModes:
  - ReadWriteMany
storageClassName: efs-sc
  resources:
    requests:
      storage: 5Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: app
  namespace: staging
spec:
  containers:
  - name: app1
    image: busybox
    command: ["/bin/sh"]

    args: ["-c", "while true; do echo 
$(date -u) >> /data/out1.txt; sleep 5; 
done"]
    volumeMounts:
    - name: persistent-storage
      mountPath: /data
  volumes:
  - name: persistent-storage
    persistentVolumeClaim:
      claimName: efs-claim
19. This step deploys applications in our Kubernetes cluster. You must
use HELM Charts or use traditional YAML configuration files. This
step deploys autoscaling pod in our Kubernetes:
a. Create a folder named apps.
b. Create a file hpa.yaml with the following code inside the apps
folder.
 ---
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: php-apache
  namespace: staging
spec:
  minReplicas: 1
  maxReplicas: 6

  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: php-apache
  targetCPUUtilizationPercentage: 50
20. This step deploys the application in our Kubernetes cluster. You must
use HELM Charts or use traditional YAML configuration files. This
step deploys ingress pod in our Kubernetes.
a. Create a folder named apps.
b. Create a file ingress.yaml with the following code inside the
apps folder:
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: php-apache
  namespace: staging
  annotations:
    alb.ingress.kubernetes.io/scheme: 
internet-facing
    alb.ingress.kubernetes.io/target-type: 
ip
    alb.ingress.kubernetes.io/certificate-
arn: arn:aws:acm:us-east-1:<your-
id>:certificate/6b2831b8-3fcc-4b4b-81e8-
e7325dfbca84

    alb.ingress.kubernetes.io/listen-ports: 
'[{"HTTP": 80}, {"HTTPS":443}]'
    alb.ingress.kubernetes.io/ssl-redirect: 
'443'
spec:
  ingressClassName: alb
  rules:
    - host: php-apache.devopsbyexample.io
      http:
        paths:
          - path: /
            pathType: Exact
            backend:
              service:
                name: php-apache
                port:
                  number: 80
21. This step deploys application in our Kubernetes cluster. You must
use HELM Charts or use traditional YAML configuration files. This
step deploys autoscaling pod in our Kubernetes.
a. Create a folder named apps.
b. Create a file lb.yaml with the following code inside the apps
folder:
---
apiVersion: v1

kind: Service
metadata:
  name: php-apache-lb
  namespace: staging
  annotations:
    service.beta.kubernetes.io/aws-load-
balancer-type: external
    service.beta.kubernetes.io/aws-load-
balancer-nlb-target-type: ip
    service.beta.kubernetes.io/aws-load-
balancer-scheme: internet-facing
# service.beta.kubernetes.io/aws-load-
balancer-proxy-protocol: "*"
spec:
  type: LoadBalancer
  ports:
  - port: 80
  selector:
    run: php-apache
22. This step deploys application in our Kubernetes cluster. You must
use HELM Charts or use traditional YAML configuration files. This
step also deploys pod distribution budget pod in our Kubernetes:
a. Create a folder named apps.
b. Create a file pdb.yaml with the following code inside the apps
folder.
---

apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: php-apache
  namespace: staging
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      run: php-apache
23. You must use HELM Charts or use traditional YAML configuration
files. This step deploys pod distribution budget pod in our
Kubernetes:
a. Create a folder named apps.
b. Create a file service.yaml with the following code inside the
apps folder.
---
apiVersion: v1
kind: Service
metadata:
  name: php-apache
  namespace: staging
spec:
  ports:

  - port: 80
  selector:
    run: php-apache
24. Run the following command to deploy the Infrastructure:
       terraform plan
       terraform apply –approve-auto
25. This step is for you to validate your deployment and perform initial
configurations. After the deployment, you need to deploy some pods
we defined with YAML You need to cd to the apps folder and run the
following commands.
Check deployment using the AWS CLI by running the following
commands:
             aws eks update-kubeconfig --name eks-demo-
cluster --region us-east-2
             kubectl get nodes
             kubectl get namespaces
             kubectl apply -f eks-apps/ingress.yaml
             kubectl apply -f eks-apps/lb.yaml
             kubectl apply -f eks-apps/service.yaml
             kubectl apply -f eks-apps/pdb.yaml
             kubectl apply -f eks-apps/hpa.yaml
             kubectl apply -f eks-apps/efs.yaml
             kubectl apply -f eks-apps/basic_deployment.yaml
             kubectl get pods --namespace staging
26. Push your code to a new branch in our codecommit chapter9 git
push in the mentioned GitHub repository.
Managing and updating AWS EKS and Fargate
Managing and updating AWS EKS and Fargate is crucial to ensure optimal
performance, security, and cost-efficiency for your containerized
applications. Amazon Elastic Kubernetes Service (EKS) and AWS
Fargate are powerful tools that simplify the orchestration and management

of containerized workloads. Here are some key strategies for effective
management and updating of AWS EKS and Fargate:
Continuous monitoring and optimization: Regularly monitors the
performance, resource utilization, and health of your EKS clusters
and Fargate tasks. Utilize AWS monitoring tools like Amazon
CloudWatch to gather metrics and set up alarms to notify you of any
abnormal behavior or potential issues. Continuously optimize
resource allocation based on usage patterns to improve cost
efficiency.
Automated updates and patching: Leverages EKS managed
updates to automatically apply patches and updates to your
Kubernetes control plane. This helps ensure that your EKS clusters
are running the latest security patches and enhancements.
Additionally, use AWS Systems Manager for automating the patching
of underlying EC2 instances in Fargate.
Version control and rollbacks: Implements version control
mechanisms for your applications and configurations. Use tools like
Git to manage changes and versions of your Kubernetes manifests,
Docker images, and any other related files. Implement a rollback
strategy to quickly revert to a previous version in case of issues
during updates.
Regular backups and disaster recovery: Establishes a backup
strategy for your EKS clusters and Fargate workloads, including
critical data and configurations. Automate backups and test the
restoration process to ensure a quick recovery in case of a failure or
disaster.
Security best practices: Adheres to AWS security best practices,
including regularly updating and patching your operating systems,
applications, and Kubernetes versions. Utilize AWS Identity and
Access Management (IAM) for access control and ensure that
permissions are least privileged. Regularly scan for vulnerabilities
and apply security patches promptly.

Scale responsively: Continuously monitors the load on your
applications and scale your EKS clusters and Fargate tasks
accordingly. Utilize auto-scaling based on metrics like CPU and
memory usage to handle traffic spikes efficiently and reduce costs
during low-traffic periods.
Documentation and training: Maintains comprehensive and up-to-
date documentation on the architecture, configurations, and processes
related to managing and updating EKS and Fargate. Provide training
to your team to ensure they are well-equipped to handle day-to-day
operations and updates effectively.
Testing and staging environments: Implements testing and staging
environments to validate updates and changes before deploying them
to production. Use tools like AWS CodePipeline to automate the
deployment pipeline and promote a controlled and reliable
deployment process.
Regular health checks and maintenance: Conducts regular health
checks on your EKS clusters and Fargate tasks to identify and address
any performance degradation or potential issues proactively. Perform
routine maintenance tasks, such as cleaning up unused resources and
optimizing configurations.
Stay informed and engage with the community: Stays informed
about the latest features, updates, and best practices for EKS and
Fargate by actively participating in AWS forums, webinars, and
community events. Engage with the community to learn from others’
experiences and gather valuable insights into effective management
and updates.
By following these strategies and best practices, you can effectively manage
and update your AWS EKS and Fargate environments, ensuring a robust,
secure, and well-optimized container orchestration platform for your
applications.
Conclusion

AWS Fargate is a serverless compute engine for Amazon EKS. You have
also deployed EKS Fargate in this chapter along with other services to help
with your knowledge. Fargate allows the user to run EKS without managing
the underlining infrastructure such servers or the clusters.
In conclusion, the introduction to Amazon Web Services Elastic Kubernetes
Service within a Virtual Private Cloud (VPC) provides a robust
foundation for managing and orchestrating containerized applications. EKS
leverages the power of Kubernetes, a leading container orchestration tool,
to simplify the deployment, scaling, and management of applications. By
utilizing a VPC, organizations can enhance security and isolation, ensuring
that their EKS clusters operate within a dedicated network environment.
This level of isolation not only bolsters security but also allows for fine-
grained control over networking configurations, enabling seamless
integration with other AWS services while maintaining a secure and
efficient infrastructure.
Furthermore, AWS EKS VPC introduction underscores the flexibility and
scalability that AWS offers to enterprises of varying sizes and requirements.
With EKS, organizations can efficiently manage their containerized
workloads, utilizing auto-scaling features to adapt to fluctuating demands.
The seamless integration of EKS with AWS services like Amazon EC2 and
AWS Identity and Access Management further simplifies the management
and access control of resources within the VPC, streamlining the
deployment process and ensuring compliance with organizational policies
and standards.
In summary, AWS EKS in a VPC introduces a powerful and flexible
solution for container orchestration, empowering businesses to effectively
deploy and manage their applications while leveraging the security,
scalability, and seamless integration capabilities that AWS provides. This
introduction lays the groundwork for building a robust and efficient cloud-
native ecosystem that can adapt to the dynamic needs of modern
enterprises.
In the next chapter, we will journey through the mystical realms of AWS
databases, discovering the potent magic they wield. Whether you are
seeking the flexibility of NoSQL databases like Amazon DynamoDB, the

relational strength of Amazon RDS, or the analytical prowess of Amazon
Redshift, we will uncover the perfect database solution to meet your needs.
However, a kingdom’s history is only as secure as its fortifications. Data is
the lifeblood of modern enterprises, and safeguarding it is paramount. That
is where AWS Backup Services come into play. We will explore how you
can create a formidable fortress around your data, ensuring its availability
and resilience in the face of unforeseen challenges.
Get ready to dive into the enchanting world of AWS Databases and Backup
Services, where data transforms into the currency of the future and where
every bit and byte carries the potential to shape your digital destiny.
Multiple choice questions
1. EKS is the same as AKS and ECS.
a. True
b. False
2. You can use EBS and EFS on AWS Fargate.
a. True
b. False
Answers
1. b
2. b

CHAPTER 10
Databases and Backup Services
Automation
Introduction
In this chapter, we delved into the robust suite of AWS Database Services
and AWS Backup Services, exploring their functionalities and how they
contribute to efficient and secure data management within the AWS
ecosystem.
The AWS Database Services section provides a comprehensive overview of
various database options offered by Amazon Web Services (AWS). We
also explored Amazon RDS, a fully managed relational database service,
and Amazon DynamoDB, a NoSQL database service known for its
scalability and performance. Additionally, we also discuss Amazon Aurora,
a high-performance relational database engine, and Amazon Redshift, a
fully managed data warehousing solution. Understanding the strengths and
use cases of each of these services is crucial for making informed decisions
in designing and managing databases on AWS.
Moving on to AWS Backup Services, we emphasized the criticality of
implementing an effective backup and recovery strategies for data stored on
AWS. AWS Backup is a centralized, fully managed backup service that
simplifies backup management across AWS services, ensuring data
resilience and compliance with organizational policies. We explored
features such as cross-region replication, lifecycle policies, and automated

backups, highlighting how these aspects enhance data protection and
recovery.
Overall, this chapter will provide valuable insights into optimizing database
performance and reliability, while also emphasizing the significance of
proactive backup practices to safeguard valuable data assets.
Structure
The chapter will cover the following topics:
Introduction to AWS Database Services
Amazon Redshift
ElastiCache: In-Memory data store
Amazon Aurora
AWS DynamoDB
Introduction to AWS backup and recovery
AWS Backup architecture and components
Disaster recovery and business continuity
Objectives
This chapter’s objective is to delve into the core concepts of AWS database
services, understanding their unique features, use cases, and benefits. We
will explore how these services help organizations optimize their data
architecture, ensure data integrity, achieve cost-effectiveness, and embark
on a path of innovation fueled by the insights derived from their data.
As you may already know that data is the lifeblood of organizations,
empowering decision-making, enhancing customer experiences, and driving
business growth. AWS, a pioneer in cloud computing, offers a rich array of
database services designed to cater to diverse data management needs,
whether you are a startup, a growing enterprise, or an established industry
leader. From scalable and highly available relational databases to purpose-
built, fully managed NoSQL databases, AWS provides a comprehensive
suite of database services that enable you to store, retrieve, analyze, and

manage data with unparalleled efficiency, security, and performance.
Whether you are dealing with structured, semi-structured, or unstructured
data, AWS has a solution that aligns with your requirements.
In addition, this chapter also delves deep into the intricacies of AWS
Backup Services, a robust and versatile suite of tools designed to ensure the
resilience and recoverability of your invaluable data. With AWS Backup
Services, you can rest assured that your digital fortress is fortified against
unforeseen disasters and disruptions.
Hence, AWS Backup services safeguard businesses from fortifying their
digital assets against the unpredictable tides of data loss. As businesses and
organizations navigate the dynamic landscape of modern technology,
securing and preserving critical data is no longer an option—it is an
imperative.
Introduction to AWS Database Services
Amazon Web Services offers a comprehensive suite of database services
designed to meet the diverse and evolving needs of modern applications and
businesses. These services provide a scalable, flexible, and cost-effective
solution for managing and storing data in the cloud. AWS database services
encompass various options, ranging from traditional relational databases to
NoSQL databases and in-memory caching systems. These services are
crucial for organizations seeking to harness the power of data, enabling
them to innovate, analyze, and make informed business decisions.
One of the key offerings in the AWS database services portfolio is Amazon
Relational Database Service (RDS). RDS simplifies the management of
relational databases by automating common administrative tasks such as
backups, software patching, and failover, allowing developers and database
administrators to focus on building and optimizing applications. It supports
popular databases like MySQL, PostgreSQL, Oracle, SQL Server, and
MariaDB, providing a familiar environment with the added benefits of
scalability, high availability, and security.
For applications that require flexible, schema-less data models and high
performance at scale, AWS offers Amazon DynamoDB, a fully managed
NoSQL database service. DynamoDB is designed for low-latency, high-

throughput applications and is ideal for use cases such as gaming, e-
commerce, and IoT, where quick and reliable access to data is critical. Its
seamless scaling capabilities and fully managed infrastructure make it a
popular choice for modern, high-growth applications.
Amazon Redshift, another notable service in the AWS database ecosystem,
is a fast and fully managed data warehousing solution. It allows
organizations to analyze large datasets and generate insights for business
intelligence and data analytics purposes. Redshift is optimized for Online
Analytical Processing (OLAP) workloads, enabling complex queries
across vast amounts of data with high performance.
Furthermore, AWS provides Amazon ElastiCache, a managed in-memory
caching service, which helps applications improve performance by caching
frequently accessed data in memory. This service supports popular open-
source caching engines such as Redis and Memcached, enabling
applications to achieve sub-millisecond response times and reduce the load
on underlying databases.
In summary, AWS offers a diverse array of database services to cater to a
wide range of application needs, ensuring that businesses can leverage the
power of data effectively and efficiently. Whether it is relational databases,
NoSQL databases, or caching services, AWS database offerings play a
crucial role in enabling organizations to innovate and deliver exceptional
user experiences.
Exploring AWS Database offerings
Amazon Web Services offers a comprehensive suite of database services
that cater to a wide range of needs, from small-scale startups to large
enterprises. One of the standout offerings is Amazon Aurora, a fully
managed relational database engine designed for high performance and
availability. Aurora combines the speed and availability of high-end
commercial databases with the simplicity and cost-effectiveness of open-
source databases. Its compatibility with MySQL and PostgreSQL makes it a
seamless choice for businesses looking to migrate or scale their existing
applications.
Another noteworthy AWS database offering is Amazon DynamoDB, a
highly-scalable NoSQL database that provides low-latency performance at

any scale. DynamoDB’s flexible data model, automatic scaling, and
seamless replication across multiple regions make it ideal for applications
that require high availability and fast response times. Additionally, AWS
offers Amazon Neptune, a purpose-built, fully managed graph database
service that makes it easy to build and run applications that work with
highly connected datasets, such as social networks, recommendation
engines, and fraud detection.
For those dealing with large volumes of unstructured or semi-structured
data, Amazon Redshift is a powerful data warehousing solution that offers
high-performance analytics and integrates with popular business
intelligence tools. It allows businesses to analyze vast datasets quickly and
derive actionable insights to drive strategic decisions. Furthermore, AWS
provides Amazon DocumentDB, a fully managed MongoDB-compatible
document database, offering developers a familiar interface and seamless
integration while benefitting from the scalability, performance, and
durability provided by AWS.
AWS also addresses the need for real-time data streaming and processing
through Amazon Kinesis. Kinesis allows users to ingest, process, and
analyze streaming data in real-time, enabling applications to react swiftly to
new information and make informed decisions on the fly. With the evolving
landscape of big data and analytics, AWS continues to innovate and expand
its database offerings, ensuring that businesses have the right tools to
manage, analyze, and derive value from their data effectively.
Relational Database Services (RDS)
Relational Database Services (RDS) represent a cornerstone of modern
data management solutions, providing a scalable and flexible platform for
organizing, storing, and retrieving structured data. RDS offers a managed
environment for popular relational databases like MySQL, PostgreSQL,
SQL Server, and Oracle, relieving businesses of the administrative burden
associated with database management. This allows organizations to focus
more on their applications and data-driven strategies, accelerating
development cycles and enhancing overall efficiency.
One significant advantage of RDS is its automatic backups and point-in-
time recovery capabilities. With automated backups, critical data is

safeguarded, and in the unfortunate event of data loss or corruption, point-
in-time recovery enables restoration to a specific point in history. This
ensures data integrity and business continuity, mitigating potential
disruptions caused by unforeseen circumstances or human error.
In addition to robust backup features, RDS offers scalability options,
allowing businesses to adapt to changing workloads and growing data
volumes. With just a few clicks, organizations can upscale or downscale
their database resources, optimizing performance and cost-efficiency. This
flexibility is vital in meeting fluctuating demand, ensuring that the database
can handle peak loads during high-traffic periods and minimize resource
usage during lulls.
Security is a paramount concern in the realm of databases, and RDS
addresses this by implementing various security measures. Encryption at
rest and in transit, fine-grained access control, and integration with Identity
and Access Management (IAM) help safeguard sensitive information and
maintain compliance with industry regulations. Additionally, RDS offers
auditing capabilities, allowing organizations to track and monitor database
activities, further enhancing security and accountability.
The global reach and reliability of RDS are key factors that make it an
attractive choice for businesses operating on a global scale. RDS offers
multi-region replication, enabling data distribution across various
geographic locations for improved latency, disaster recovery, and
compliance with data residency requirements. This geographical diversity
ensures that data is always accessible and minimizes the impact of potential
outages in a particular region.
In summary, Relational Database Services (RDS) combine the power of
popular relational databases with the benefits of a managed service, offering
automatic backups, scalability, robust security, and global accessibility. By
leveraging RDS, organizations can streamline their database management
processes and focus on leveraging their data to drive innovation and
business growth.
NoSQL Database Services
Amazon Web Services offers a diverse range of NoSQL database services
tailored to meet various data storage and management needs. One

prominent service is Amazon DynamoDB, a fully managed, scalable
NoSQL database that provides consistent, single-digit millisecond latency
at any scale. DynamoDB is designed to handle high-traffic applications and
can automatically scale up or down based on demand, ensuring optimal
performance and cost efficiency. Its key-value and document data models
make it versatile for a wide array of use cases, from web and mobile
applications to gaming and IoT applications.
Another notable AWS NoSQL service is Amazon DocumentDB, a
compatible MongoDB database that provides the performance, scalability,
and availability of a fully managed service. It’s designed to handle JSON-
style document data and is an excellent choice for applications that already
use or are compatible with MongoDB, providing seamless migration
options and integration capabilities.
Amazon Neptune is AWS’s fully managed graph database service,
supporting two popular graph models: Property Graph and RDF. It’s
designed for highly connected data and is ideal for social networking, fraud
detection, knowledge graphs, and recommendation engines. Neptune offers
high performance and availability, enabling efficient querying and analysis
of complex relationships in your data.
Amazon Keyspaces is a fully managed, serverless, Apache Cassandra-
compatible NoSQL database service. It allows you to run Cassandra
workloads on AWS with virtually unlimited scale, ensuring high availability
and performance for your critical applications. Keyspaces is suitable for
applications that require a wide-column store with high write and read
throughput, such as IoT data, time-series data, and more.
In summary, AWS provides a range of NoSQL database services, each with
its unique capabilities to cater to diverse data storage and management
requirements. These services empower developers and organizations to
choose the best fit for their specific use cases, ensuring efficient, scalable,
and high-performance data solutions.
Amazon Redshift
Amazon Redshift is a powerful cloud-based data warehousing solution
offered by Amazon Web Services. It is designed to handle large-scale

analytics workloads and deliver high-performance querying and reporting
capabilities. Redshift utilizes a columnar storage approach, which stores
data in columns rather than rows, optimizing query performance and
reducing I/O overhead.
One of the key advantages of Amazon Redshift is its scalability. It allows
businesses to start with a small data warehouse and scale up to petabytes of
data seamlessly as their needs grow. This scalability is essential for
organizations dealing with massive amounts of data and needing to process
and analyze it efficiently.
Another remarkable feature of Amazon Redshift is its MPP (Massively
Parallel Processing) architecture. It distributes and processes data across
multiple nodes, enabling parallel execution of queries and significantly
accelerating query response times. This architecture enhances the speed and
efficiency of complex analytical queries, making it an ideal solution for data
analytics and business intelligence applications.
Additionally, Amazon Redshift offers integration with various AWS
services, facilitating easy and seamless data ingestion and transformation. It
supports data loading from various sources, including Amazon S3, Amazon
DynamoDB, Amazon EMR, and more. This integration simplifies the data
pipeline and enables organizations to ingest and analyze diverse datasets
from different sources.
Moreover, Amazon Redshift provides built-in security features, including
encryption at rest and in transit, IAM-based authentication, and fine-grained
access control. These security measures help organizations ensure the
confidentiality and integrity of their data while meeting compliance
requirements and industry standards.
In conclusion, Amazon Redshift is a versatile and scalable data
warehousing solution that empowers organizations to analyze massive
datasets efficiently and make data-driven decisions. Its MPP architecture,
seamless integration with other AWS services, and robust security features
position it as a leading choice for businesses seeking high-performance
analytics and data warehousing capabilities.
ElastiCache: In Memory data store

Amazon ElastiCache is a powerful in-memory data store service offered by
Amazon Web Services, designed to enhance the performance and
scalability of applications by allowing them to seamlessly cache and
retrieve data in real-time. This service enables businesses to operate at high
speed and low latency by leveraging the benefits of an in-memory data
store. ElastiCache supports popular open-source in-memory engines like
Redis and Memcached, providing flexibility and versatility to developers.
Redis, one of the supported engines, offers advanced data structures and
various caching strategies, making it ideal for use cases that require real-
time analytics, session management, and high-performance applications. Its
support for complex data types and sophisticated commands allows
developers to create efficient and dynamic caching solutions. On the other
hand, Memcached is another widely used engine that is renowned for its
simplicity and speed. It excels in straightforward key-value caching use
cases, making it a popular choice for applications that prioritize rapid data
access and retrieval.
One of the key advantages of Amazon ElastiCache is its seamless
integration with other AWS services, enabling developers to build efficient
and high-performing applications within the AWS ecosystem. ElastiCache
works seamlessly with Amazon EC2 instances, making it easier to set up,
manage, and scale caching environments. It also provides automated
backup and restoration features, ensuring data durability and reliability.
Moreover, ElastiCache allows for the creation of highly available and fault-
tolerant caching clusters, minimizing downtime and enhancing application
reliability.
By utilizing Amazon ElastiCache, businesses can significantly reduce the
load on their databases and improve application response times, ultimately
delivering a superior user experience. With its scalable and cost-effective
solutions, ElastiCache empowers organizations to handle increasing
amounts of data and traffic, making it an essential tool for modern, high-
performance applications.
Amazon Aurora
Amazon Aurora is a cloud-native, MySQL and PostgreSQL-compatible
relational database service offered by Amazon Web Services. It is

engineered for performance, scalability, and high availability, making it an
attractive choice for organizations with demanding database needs. One of
its standout features is its ability to automatically replicate data across
multiple Availability Zones (AZs), providing fault tolerance and
eliminating the need for complex manual setup. This design ensures that
even in the event of a failure in one AZ, data remains accessible, and
applications can continue running smoothly.
Furthermore, Amazon Aurora utilizes a distributed architecture that allows
it to deliver up to five times the throughput of standard MySQL databases
and up to two times that of standard PostgreSQL databases. This is
achieved through an intelligent storage subsystem that dynamically divides
the database volume into 10GB segments, each replicated six ways across
multiple AZs. In addition, Aurora continuously backs up data to Amazon
S3, providing durability and reliability. The storage is also self-healing,
detecting, and repairing failures transparently, ensuring minimal downtime
and maintaining performance.
Amazon Aurora is designed to be compatible with MySQL and
PostgreSQL, allowing for easy migration of existing applications and tools.
It supports common MySQL and PostgreSQL features, functions, and
extensions, making it familiar to developers and database administrators.
This compatibility also allows for a smooth transition to Aurora, without
requiring extensive rewrites of code or changes to the existing
infrastructure.
In terms of security, Amazon Aurora offers several layers of protection. It
encrypts data in transit using industry-standard SSL, and at rest using AWS
Key Management Service (KMS). Fine-grained access control and VPC
isolation further enhance security, enabling organizations to define who can
access the database and from where, minimizing potential security threats.
In conclusion, Amazon Aurora stands as a powerful and innovative solution
for organizations seeking a highly performant and reliable relational
database service in the cloud. Its compatibility with MySQL and
PostgreSQL, scalability, self-healing storage, and robust security features
make it an attractive choice for a wide range of applications and use cases,
from small startups to large enterprises with demanding database
workloads.

AWS DynamoDB
Amazon DynamoDB is a fully managed NoSQL database service provided
by Amazon Web Services (AWS). It offers seamless scalability and high
performance, making it an ideal choice for applications that require low-
latency access to data with varying workloads. DynamoDB operates on the
principles of a key-value store, allowing for efficient and fast retrieval of
data using a primary key. It also supports secondary indexes, enabling
flexible querying options.
One of the standouts features of DynamoDB is its automatic scaling. As
your application’s demand grows or shrinks, DynamoDB can automatically
adjust its capacity to handle the load. This eliminates the need for manual
provisioning and helps in cost optimization by ensuring that you only pay
for the resources you use. Whether you are dealing with a sudden spike in
traffic or experiencing a lull in activity, DynamoDB scales accordingly to
maintain optimal performance.
DynamoDB provides strong data consistency and durability. It achieves this
through synchronous replication of data across multiple Availability Zones
within a region. This design ensures that even in the event of a hardware
failure or outage, your data remains available and durable. Additionally,
DynamoDB offers encryption at rest and in transit, providing a secure
environment for sensitive data.
Another compelling aspect of DynamoDB is its seamless integration with
other AWS services. Developers can easily combine DynamoDB with AWS
Lambda, Amazon S3, Amazon Kinesis, or Amazon Redshift, among others,
to create powerful and scalable applications. This integration allows for
real-time analytics, event-driven processing, and efficient data pipelines,
contributing to a more robust and versatile architecture.
In conclusion, Amazon DynamoDB stands out as a high-performing, fully
managed NoSQL database service that seamlessly scales to match your
application’s needs. Its strong consistency, automatic scaling, and
integration capabilities make it a valuable choice for a wide range of use
cases, from mobile apps and gaming to IoT and enterprise-level
applications.

AWS DynamoDB Architecture Diagram
We are going to deploy DynamoDB that uses API Gateway as the frontend,
and when the user upload file to s3 bucket, that will trigger a lambda
function which will then upload the file to the DynamoDB, (refer to the
following figure):
Figure 10.1: AWS DynamoDB Deployment Architecture
AWS DynamoDB Setup
Setting up AWS DynamoDB using Terraform involves defining the
infrastructure as code to provision and configure the necessary resources.
First, create a Terraform configuration file specifying the AWS provider and
DynamoDB table details. Define the AWS provider with appropriate access
credentials and region. Then, define the DynamoDB table configuration,
including its schema, read and write capacity units, and other relevant
attributes. Leverage Terraform’s declarative syntax to ensure the desired
state of the DynamoDB table aligns with the configuration. Once the
configuration is defined, run terraform init to initialize the working
directory and download the required provider plugins. Follow this with
terraform plan to preview the changes Terraform will make. Finally, execute
terraform apply to provision the DynamoDB table based on the defined

configuration, allowing for automated and consistent management of your
DynamoDB resources. Terraform streamlines the process, promoting
efficiency and reproducibility in managing AWS DynamoDB infrastructure.
Following is the step by step of the architecture above:
1. This step creates the folder for this chapter, created the config file,
.gitinore file, provider file, and the apiGateway file.
a. Create Chapter10 Folder and move config folder to the Chapter
10 folder.
b. Move provider.tf and remote_state.tf, .gitignore to the
Chapter 10 folder.
c. Create apiGateway.tf file with the following codes:
  resource "aws_api_gateway_rest_api" 
"rest_api" {
  name        = "${var.function_name}-
serverless-api"
  description = "Terraform Gateway 
Serverless API"
}
resource "aws_api_gateway_resource" "proxy" 
{
  rest_api_id = 
aws_api_gateway_rest_api.rest_api.id
  parent_id   = 
aws_api_gateway_rest_api.rest_api.root_reso
urce_id
  path_part   = "{proxy+}"
}
resource "aws_api_gateway_method" "proxy" {

  rest_api_id      = 
aws_api_gateway_rest_api.rest_api.id
  resource_id      = 
aws_api_gateway_resource.proxy.id
  http_method      = "ANY"
  authorization    = "NONE"
  api_key_required = true
}
resource "aws_api_gateway_integration" 
"lambda" {
  rest_api_id = 
aws_api_gateway_rest_api.rest_api.id
  resource_id = 
aws_api_gateway_method.proxy.resource_id
  http_method = 
aws_api_gateway_method.proxy.http_method
  integration_http_method = "POST"
  type                    = "AWS_PROXY"
  uri                     = 
aws_lambda_function.rest_api.invoke_arn
}
resource "aws_api_gateway_method" 
"proxy_root" {
  rest_api_id      = 
aws_api_gateway_rest_api.rest_api.id
  resource_id      = 
aws_api_gateway_rest_api.rest_api.root_reso

urce_id
  http_method      = "ANY"
  authorization    = "NONE"
  api_key_required = true
}
resource "aws_api_gateway_integration" 
"lambda_root" {
  rest_api_id = 
aws_api_gateway_rest_api.rest_api.id
  resource_id = 
aws_api_gateway_method.proxy_root.resource_
id
  http_method = 
aws_api_gateway_method.proxy_root.http_meth
od
  integration_http_method = "POST"
  type                    = "AWS_PROXY"
  uri                     = 
aws_lambda_function.rest_api.invoke_arn
}
resource "aws_api_gateway_deployment" 
"endpoint" {
  depends_on = [
    aws_api_gateway_integration.lambda,
    
aws_api_gateway_integration.lambda_root,
  ]

  rest_api_id = 
aws_api_gateway_rest_api.rest_api.id
  stage_name  = "test"
}
resource "aws_api_gateway_usage_plan" 
"usageplan" {
  name = "${var.function_name}_usage_plan"
  api_stages {
    api_id = 
aws_api_gateway_rest_api.rest_api.id
    stage  = 
aws_api_gateway_deployment.endpoint.stage_n
ame
  }
}
resource "aws_api_gateway_api_key" 
"api_key" {
  name = "my_key"
}
resource "aws_api_gateway_usage_plan_key" 
"main" {
  key_id        = 
aws_api_gateway_api_key.api_key.id
  key_type      = "API_KEY"
  usage_plan_id = 
aws_api_gateway_usage_plan.usageplan.id
}

2. Create items.json file with the following codes:
{
    "firewallLogs": [
        {
            "PutRequest": {
                "Item": {
                    "Id": {
                "N": "1"
            },
            "firstName": {
                "S": "John"
            },
            "lastName": {
                "S": "Area"
            },
            "contact": {
                "N": "99999999999"
            }
                }
            }
        },
        {
            "PutRequest": {

                "Item": {
                    "Id": {
                "N": "2"
            },
            "firstName": {
                "S": "Peter"
            },
            "lastName": {
                "S": "Pan"
            },
            "contact": {
                "N": "88888888888"
            }
                }
            }
        },
        {
            "PutRequest": {
                "Item": {
                    "Id": {
                "N": "3"
            },
            "firstName": {

                "S": "Bill"
            },
            "lastName": {
                "S": "Anderson"
            },
            "contact": {
                "N": "77777777777"
            }
                }
            }
        },
        {
            "PutRequest": {
                "Item": {
                   "Id": {
                "N": "4"
            },
            "firstName": {
                "S": "James"
            },
            "lastName": {
                "S": "Odeyinka"
            },

            "contact": {
                "N": "66666666666"
            }
                }
            }
        },
        {
            "PutRequest": {
                "Item": {
                  "Id": {
                "N": "5"
            },
            "firstName": {
                "S": "Bill"
            },
            "lastName": {
                "S": "Clinton"
            },
            "contact": {
                "N": "55555555555"
            }
                }
            }

        },
        {
            "PutRequest": {
                "Item": {
               "Id": {
                "N": "6"
            },
            "firstName": {
                "S": "Joel"
            },
            "lastName": {
                "S": "Gervengen"
            },
            "contact": {
                "N": "44444444444"
            }
                }
            }
        }
    ]
}
3. Create lambda.tf with the following code:
resource "aws_dynamodb_table" "firewall-logs-
table" {

  name           = "firewallLogs"
  billing_mode   = "PROVISIONED"
  read_capacity  = 10
  write_capacity = 10
  hash_key       = "Id"
  attribute {
    name = "Id"
    type = "N"
  }
provisioner "local-exec" {
    command = "bash populate_dynamodb.sh"
  }
}
resource "aws_iam_role" "lambda_iam" {
  name = "${var.function_name}-lambda-role"
  assume_role_policy = <<EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Action": "sts:AssumeRole",
      "Principal": {
        "Service": "lambda.amazonaws.com"

      },
      "Effect": "Allow"
    }
  ]
}
EOF
}
resource "aws_iam_role_policy_attachment" 
"attachPolicy" {
  role       = aws_iam_role.lambda_iam.name
  policy_arn = 
"arn:aws:iam::aws:policy/AdministratorAccess"
}
resource "aws_s3_bucket" 
"dynamodb_lambda_bucket" {
  bucket = "${var.function_name}-bucket"
}
resource "aws_lambda_function" "func" {
  filename      = "lambda_csv_2_dynamodb.zip"
  function_name = "${var.function_name}-
lambda-func"
  role          = aws_iam_role.lambda_iam.arn
  handler       = 
"lambda_csv_2_dynamodb.lambda_handler"
  runtime       = "python3.9"

  timeout       = 180
  depends_on    = 
[aws_dynamodb_table.firewall-logs-table]
}
resource "aws_lambda_function" "rest_api" {
  filename      = "lambda_rest_api.zip"
  function_name = "${var.function_name}-rest-
api"
  role          = aws_iam_role.lambda_iam.arn
  handler       = 
"lambda_rest_api.lambda_handler"
  runtime       = "python3.9"
  depends_on    = [aws_lambda_function.func]
}
resource "aws_lambda_permission" 
"allow_bucket" {
  statement_id  = "AllowExecutionFromS3Bucket"
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.func.arn
  principal     = "s3.amazonaws.com"
  source_arn    = 
aws_s3_bucket.dynamodb_lambda_bucket.arn
}
resource "aws_lambda_permission" "allow_apigw" 
{
  statement_id  = "AllowAPIGatewayInvoke"

  action        = "lambda:InvokeFunction"
  function_name = 
aws_lambda_function.rest_api.arn
  principal     = "apigateway.amazonaws.com"
  source_arn    = 
"${aws_api_gateway_rest_api.rest_api.execution
_arn}/*/*"
  depends_on    = 
[aws_api_gateway_rest_api.rest_api]
}
resource "aws_s3_bucket_notification" 
"bucket_notification" {
  bucket = 
aws_s3_bucket.dynamodb_lambda_bucket.id
  lambda_function {
    lambda_function_arn = 
aws_lambda_function.func.arn
    events              = 
["s3:ObjectCreated:*"]
    filter_suffix       = ".csv"
  }
  depends_on = 
[aws_lambda_permission.allow_bucket]
}
4. Create a file populate_dynamodb.sh with the following code:
#!/usr/bin/env bash

aws dynamodb batch-write-item --request-items 
file://items.json
5. Create a file variables.tf file with the following code:
variable "function_name" {
  type        = string
  description = "Lambda function name"
}
6. Create a file "terraform.tfvars" with the following code:
function_name = "Your-Function-Name-Here" 
#e.g. myChapterTenLambda
7. Create a folder named zip_files and sample_csv in Chapter10
folder.
Create a file sample_data.csv inside sample_csv folder with the
following code:
7,John,Akoi,9999999999
8,Peter,Oyon,9999999999
9,Steve,Benzilla,9999999999
10,Josh,Burger,9999999999
11,Tim,Balogun,9999999999
8. Create a file lambda_rest_api.py inside zip_files folder with the
following code:
import json, boto3
region = 'us-east-1'
def lambda_handler(event, context):
    try:
        statusCode = 200

        result = None
        db = boto3.client('dynamodb', 
region_name = region)
        httpMethod = event['httpMethod']
        body = {}
        params = {}
        if 'body' in event.keys() and 
event['body']:
            body = json.loads(event['body'])
        if 'queryStringParameters' in 
event.keys():
            params = 
event['queryStringParameters']
# Read an item by id
        if httpMethod == 'GET':
            if 'id' not in params.keys():
                statusCode = 400
                result = 'Id parameter not 
specified in query string'
            else:
                id = params['id']
                response = db.get_item(
                    Key={
                        'Id': {
                            'N': str(id)

                        }
                    },
                    TableName = 'Customers')
                result = response['Item']
# Create item
        elif httpMethod == 'POST':
            if body.keys() < {'id', 
'firstname', 'lastname', 'contact'}:
                statusCode = 400
                result = 'Ensure that id, 
firstname, lastname, contact are present in 
the request body'
            else:
                id = body['id']
                firstname = body['firstname']
                lastname = body['lastname']
                contact = body['contact']
                result = db.put_item(
                    Item={
                        'Id': {
                            'N': str(id)
                        },
                        'Firstname': {
                            'S': 
str(firstname)

                        },
                        'Lastname': {
                            'S': str(lastname)
                        },
                        'Contact': {
                            'N': str(contact)
                        },
                    },
                    TableName = 'Customers')
                result = 'Item created 
successfully'
# Update item, this will create new item if 
the key does not exist
        elif httpMethod == 'PUT':
            if body.keys() < {'id', 
'firstname', 'lastname', 'contact'}:
                statusCode = 400
                result = 'Ensure that id, 
firstname, lastname, contact are present in 
the request body'
            else:
                id = body['id']
                firstname = body['firstname']
                lastname = body['lastname']
                contact = body['contact']

                db.put_item(
                    Item={
                        'Id': {
                            'N': str(id)
                        },
                        'Firstname': {
                            'S': 
str(firstname)
                        },
                        'Lastname': {
                            'S': str(lastname)
                        },
                        'Contact': {
                            'N': str(contact)
                        },
                    },
                    TableName = 'Customers')
                result = 'Item updated 
successfully'
# Delete an item by id
        elif httpMethod == 'DELETE':
            if 'id' not in params.keys():
                statusCode = 400

                result = 'Id parameter not 
specified in query string'
            else:
                id = params['id']
                db.delete_item(
                    Key={
                        'Id': {
                            'N': str(id)
                        }
                    },
                    TableName = 'Customers')
                result = 'Item deleted 
successfully'
    except Exception as ex:
        statusCode = 500
        result = str(ex)
    return {
        'statusCode': statusCode,
        'body': json.dumps(result)
    }
9. Create a file lambda_csv_2_dynamodb.py inside zip_files folder with
the following code:
import json, boto3, csv
region = 'us-east-1'
def lambda_handler(event, context):

    try:
        s3 = boto3.client('s3')
        db = boto3.client('dynamodb', 
region_name = region)
# Get the uploaded file details
        bucketName = event['Records'][0]['s3']
['bucket']['name']
        fileName = event['Records'][0]['s3']
['object']['key']
        print('Uploaded file name: ', 
fileName)
# Fetch the data from uploaded file
        csvFile = s3.get_object(Bucket = 
bucketName, Key = fileName)
        dataSet = 
csvFile['Body'].read().decode('utf-
8').split('\n')
        csvReader = csv.reader(dataSet, 
delimiter = ',', quotechar = '"')
        rows = 0
        for row in csvReader:
            rows += 1
            id = row[0]
            firstName = row[1]
            lastName = row[2]
            contact = row[3]

# Insert record in DynamoDB table
            response = db.put_item(TableName = 
'Customers',
                Item = {
                    'Id': {
                        'N': str(id)
                    },
                    'Firstname': {
                        'S': str(firstName)
                    },
                    'Lastname': {
                        'S': str(lastName)
                    },
                    'Contact': {
                        'N': str(contact)
                    },
                })
        print('Successfully added ', rows, ' 
in Customers Table')
    except Exception as e:
        print(str(e))
    return {
        'statusCode': 200,

        'body': json.dumps('Function execution 
completed!')
    }
10. Run the following command to make the bash file executable:
chmod u+x populate_dynamodb.sh
11. Run the following commands:
terraform init --backend-config=config/demo.config terraform
plan terraform apply --approve-auto
12. Run the following command to make the bash file executable:
aws s3 cp sample_csv/sample_data.csv s3://dynamodb-lambda-
bucket/sample_data.csv
./populate_dynamodb.sh
aws dynamodb scan --table-name Customers
13. Push you code to a new branch in our codecommit “Chapter10”: git
push 
https://git-codecommit.us-east-
1.amazonaws.com/v1/repos/awsca-demo-repo.
Introduction to AWS backup and recovery
AWS Backup and Recovery is a vital aspect of managing and securing your
data within the Amazon Web Services ecosystem. In the fast-paced digital
landscape, data is at the heart of every organization, making effective
backup and recovery strategies crucial for maintaining business continuity,
ensuring compliance, and mitigating risks associated with data loss or
corruption.
AWS Backup is a centralized, fully managed backup service provided by
AWS that enables you to protect your data and applications across AWS
services and on-premises environments. It simplifies the backup process,
allowing you to automate and manage backups, set retention policies, and
perform data recovery operations seamlessly. AWS Backup supports a wide
range of AWS services, including Amazon RDS, Amazon EBS, Amazon

EC2, Amazon DynamoDB, and more, providing a unified solution for your
backup needs.
Recovery is an integral part of the AWS Backup and Recovery paradigm,
ensuring that in case of data loss, accidental deletion, or system failures,
you can efficiently restore your data to a previous, consistent state. AWS
offers flexible recovery options, including point-in-time recovery, snapshot
restoration, and cross-region replication, giving you the flexibility to choose
the most appropriate recovery strategy based on your specific use cases and
requirements.
In this digital age where data is invaluable, understanding AWS Backup and
Recovery is paramount for businesses looking to protect their assets and
maintain operations in the face of potential disruptions. This introduction
sets the stage for delving deeper into the mechanisms, best practices, and
strategies associated with AWS Backup and Recovery, empowering
organizations to develop robust data protection strategies within the AWS
environment.
AWS Backup architecture and components
Amazon Web Services Backup is a comprehensive and scalable solution
designed to simplify and centralize the backup and recovery of data across
various AWS services and on-premises environments. The architecture of
AWS Backup comprises several key components that work together to
ensure reliable, efficient, and secure backup and restore operations.
Backup console and APIs: At the core of AWS Backup is the
Backup Console, a centralized management interface that enables
users to define, configure, and manage backup policies, schedules,
and targets. Additionally, AWS Backup provides APIs that allow
programmatic access and integration with other applications and
services, facilitating automation and customization of backup
processes.
Backup plans: AWS Backup allows users to create backup plans,
which serve as a blueprint for defining how backups should be
conducted. A backup plan outlines backup rules, retention policies,
and lifecycle management, enabling users to tailor the backup

strategy to meet specific requirements for different workloads and
applications.
Backup vaults: Backup Vaults are logical containers that organize
and manage backups based on the configured backup plans. Each
vault is associated with a specific AWS region, making it easier to
organize and segregate backups geographically or based on other
criteria. Backup Vaults provide a centralized location for storing and
accessing backups, allowing for efficient management and retrieval
when needed.
Backup selections: AWS Backup supports backup selections, which
are user-defined sets of resources that need to be backed up. These
can include Amazon EC2 instances, Amazon RDS databases,
Amazon EBS volumes, and more. Backup selections are specified
within the backup plans, allowing users to define what data and
resources should be included in the backup process.
Lifecycle policies: Lifecycle policies define the retention and
deletion rules for backups stored in a vault. Users can configure
policies to specify how long backups should be retained and when
they should be transitioned to colder storage classes or deleted. This
feature allows for efficient management of storage costs and
compliance with data retention requirements.
Backup jobs: Backup jobs are instances of backups being initiated
based on defined backup plans and selections. These jobs are
scheduled and executed according to the configured backup policies,
ensuring that data is consistently backed up and protected. AWS
Backup monitors and manages these jobs to ensure successful
backups and provides detailed status and progress information.
In summary, AWS Backup’s architecture and components provide a robust
and flexible framework for orchestrating backup operations across diverse
AWS services and resources, ensuring data protection, compliance, and ease
of management for businesses and organizations.
Backup and restore strategies

Amazon Web Services Backup and Restore Strategies are essential
components of any organization’s data management and disaster recovery
plan. These strategies ensure that critical data and applications are
protected, easily recoverable, and resilient to potential disruptions. AWS
offers a range of tools and services that facilitate effective backup and
restore strategies, tailored to meet specific business requirements.
One fundamental aspect of an effective AWS Backup and Restore Strategy
is understanding the importance of regular, automated backups. AWS
provides services like Amazon S3 for scalable, durable, and secure object
storage, making it an excellent choice for storing backup data.
Implementing automated backup schedules ensures that data is captured
consistently, minimizing the risk of data loss due to human error or
unforeseen incidents.
In addition to automated backups, versioning and retention policies play a
vital role in the comprehensive backup strategy
(https://docs.aws.amazon.com/prescriptive-guidance/latest/security-
best-practices/strategy.html). AWS allows organizations to define custom
retention policies for their backups, enabling them to retain multiple
versions of their data over time. This capability is crucial for compliance,
auditing, and restoring to a specific point in time, enhancing data
availability and reliability.
Moreover, utilizing AWS Disaster Recovery services like AWS Backup and
AWS Disaster Recovery (DR) services, organizations can design robust
disaster recovery strategies. AWS Backup provides centralized backup
management for various AWS services, simplifying backup management
across multiple accounts and regions. Implementing geographic redundancy
and replicating backups across different AWS regions ensures high
availability and resilience to regional outages, further enhancing disaster
recovery capabilities.
Furthermore, employing encryption and access controls is imperative to
ensure data security and compliance with privacy regulations. AWS offers
features like AWS Key Management Service (KMS) to manage
encryption keys securely, ensuring that backup data remains protected
throughout its lifecycle. Implementing granular access controls to limit

access to backup data to authorized personnel only enhances the security
posture of the backup and restore process.
In summary, an effective AWS Backup and Restore Strategy combines
automated backups, versioning, retention policies, disaster recovery
planning, encryption, and access controls to create a robust and reliable data
protection framework. By leveraging AWS’s comprehensive suite of
services and adhering to best practices, organizations can safeguard their
critical data, enhance resilience, and ensure business continuity in the face
of disruptions.
Backup integration with AWS Services
Amazon Web Services Backup is a comprehensive and versatile service that
offers seamless integration with various AWS services, enhancing data
protection and disaster recovery strategies for businesses. One of the
notable integrations is with Amazon Elastic Block Store (EBS), providing
an efficient way to back up EBS volumes. AWS Backup simplifies the
process by allowing centralized management and automation of EBS
snapshot creation and retention policies. Through this integration,
organizations can ensure critical data stored in EBS volumes is backed up
regularly, meeting compliance requirements and reducing the risk of data
loss.
Furthermore, AWS Backup seamlessly integrates with Amazon Relational
Database Service (RDS), a vital component for many applications. This
integration enables automated backup and retention of RDS databases,
ensuring that valuable data is safeguarded against accidental deletion,
corruption, or other unforeseen incidents. AWS Backup supports point-in-
time restores, giving organizations the ability to recover databases to
specific states, providing flexibility in data recovery and minimizing
downtime in case of issues.
In addition to EBS and RDS, AWS Backup can be integrated with AWS
Storage Gateway, facilitating efficient backup and recovery of on-premises
applications and data. This integration extends the reach of AWS Backup to
hybrid cloud architectures, allowing businesses to back up and manage data
seamlessly across both on-premises and cloud environments. It promotes a

unified approach to data protection, essential for maintaining business
continuity and ensuring data availability.
Moreover, AWS Backup integrates with AWS Lambda, enabling automated
backup operations triggered by events or defined schedules. With this
integration, businesses can automate backup workflows, making the backup
process more efficient and reducing administrative overhead. AWS
Lambda’s serverless architecture ensures cost-effectiveness and scalability,
aligning with the agility and scalability goals of modern enterprises.
In conclusion, AWS Backup’s seamless integration with various AWS
services like EBS, RDS, Storage Gateway, and Lambda demonstrates its
versatility and adaptability within the AWS ecosystem. These integrations
enhance the capabilities of AWS Backup, enabling organizations to design
comprehensive and efficient data protection strategies that meet their
specific needs and compliance requirements.
Backup services monitoring and reporting
AWS Backup Services Monitoring and Reporting play a critical role in
ensuring the reliability and security of data stored within the Amazon Web
Services environment. Monitoring involves constant surveillance of backup
processes, assessing the health of backups, and promptly identifying any
issues that could compromise data integrity. AWS Backup provides a
centralized platform for managing and monitoring backups across various
AWS services like Amazon EBS, Amazon RDS, Amazon DynamoDB, and
more. Real-time monitoring allows for timely detection of failed or
incomplete backups, ensuring that data is protected and recoverable when
needed.
The reporting aspect of AWS Backup Services is equally essential,
providing insights into backup performance, compliance, and adherence to
defined backup policies. Detailed reports can include information such as
backup success rates, backup duration, and the amount of data protected.
These reports help organizations assess their backup strategy’s
effectiveness, identify trends, and make informed decisions to optimize
their backup processes. Moreover, compliance reports assist in meeting
regulatory requirements by showcasing adherence to data protection

standards and ensuring that backup activities align with industry-specific
mandates.
AWS Backup Services Monitoring and Reporting leverage advanced
analytics and automation to enhance the efficiency and effectiveness of data
backup and recovery. Intelligent monitoring tools can proactively identify
anomalies, predict potential issues, and trigger alerts to notify
administrators of any deviations from established backup policies. In-depth
reporting not only facilitates performance evaluation but also aids in
capacity planning, resource allocation, and budget forecasting, enabling
organizations to optimize their backup strategy and costs effectively.
By combining robust monitoring and insightful reporting, AWS Backup
Services empower organizations to maintain a reliable and compliant
backup infrastructure. This ensures data resiliency, accelerates recovery
processes, and ultimately contributes to a secure and reliable AWS cloud
environment, giving businesses the confidence that their critical data is
safeguarded and accessible when needed most.
Backup services, security, and compliance
Amazon Web Services places a strong emphasis on ensuring robust security
measures and compliance standards for their backup services. AWS Backup
Services incorporate a multi-layered security approach, starting with
physical data center security and extending to advanced encryption
protocols and access controls.
At the physical level, AWS data centers are equipped with state-of-the-art
security measures, including biometric access controls, 24/7 monitoring,
and redundant power and networking to ensure data integrity and
availability. These measures create a secure foundation for the AWS
Backup Services infrastructure.
In terms of data protection, AWS Backup Services use encryption at rest
and in transit. Data at rest is encrypted using industry-standard algorithms
like Advanced Encryption Standard (AES) with strong key management
policies. This ensures that even if unauthorized access occurs, the data
remains unintelligible and secure. Additionally, AWS provides customers
with the option to manage their own encryption keys for added control and
security.

AWS Backup Services also adhere to various compliance standards, such as
HIPAA, GDPR, SOC 2, and PCI DSS, among others. These compliance
certifications demonstrate AWS’s commitment to maintaining stringent
security and privacy controls, making the services suitable for a wide range
of regulated industries, and ensuring the protection of sensitive data.
Access controls and permissions are pivotal components of AWS Backup
Services security. AWS offers Identity and Access Management (IAM) to
manage user access and permissions effectively. With IAM, organizations
can define and enforce granular access policies, ensuring that only
authorized individuals can access, manage, and restore backup data, further
enhancing the security posture of the backup environment.
In conclusion, AWS Backup Services prioritize security and compliance by
implementing robust physical security measures, encryption protocols, and
stringent access controls. Compliance with industry standards and the
provision of advanced encryption options demonstrate AWS’s dedication to
maintaining the highest levels of security and privacy for backup services.
Disaster recovery and business continuity
Amazon Web Services Disaster Recovery and Business Continuity
solutions are integral components of modern IT strategies, ensuring
seamless operations and data protection even in the face of unforeseen
disruptions. AWS offers a range of services and best practices that enable
businesses to mitigate risks, recover quickly, and maintain operations
during challenging times.
In the realm of Disaster Recovery, AWS provides an array of options to
create resilient systems. AWS Disaster Recovery as a Service (DRaaS)
allows organizations to replicate and store critical data and applications in
multiple AWS regions. This redundancy ensures that in the event of a
disaster, systems can quickly failover to a secondary location, minimizing
downtime and data loss. AWS provides tools like AWS Backup and AWS
CloudEndure that aids in automated backup, replication, and recovery
processes, allowing businesses to tailor their disaster recovery strategies to
their specific needs.

Moreover, AWS Business Continuity strategies emphasize the importance
of maintaining operations without disruption. AWS offers a globally
distributed infrastructure, allowing organizations to distribute workloads
and services across multiple AWS regions. This geographic diversity
ensures that in case of localized outages or disasters, operations can
continue seamlessly from unaffected regions. AWS also offers load
balancing and auto-scaling features to dynamically manage traffic and
ensure continuous availability, even during unexpected surges in demand.
A key advantage of AWS solutions is the ability to run critical workloads in
the cloud, reducing reliance on on-premises infrastructure. This flexibility
enables businesses to swiftly adapt to changing circumstances, scaling
resources up or down based on demand, and redirecting traffic to
operational regions to maintain business continuity. AWS also emphasizes
regular testing and automation to validate disaster recovery and business
continuity plans, ensuring they are effective and up to date.
In conclusion, AWS Disaster Recovery and Business Continuity solutions
empower organizations to create robust, flexible, and scalable strategies for
preserving operations and safeguarding data. By leveraging AWS’s cloud
infrastructure and services, businesses can confidently navigate disruptions,
enabling them to swiftly recover and continue providing essential services
to their customers.
Conclusion
In this chapter, we explored two crucial aspects of Amazon Web Services
are Database Services and AWS Backup Services. First, we delved into
AWS Database Services, which provide a range of managed database
options to cater to diverse business needs. AWS offers fully managed
relational databases, NoSQL databases (Amazon DynamoDB), in-memory
databases (Amazon ElastiCache), and more. The chapter highlighted the
key features, advantages, and use cases of these services, emphasizing their
scalability, reliability, and ease of management.
Next, we examined AWS Backup Services, focusing on the importance of
data backup and recovery in the cloud environment. AWS Backup
simplifies and centralizes the backup process for various AWS resources,
including databases, storage volumes, and file systems. The chapter

discussed the functionalities, benefits, and best practices for implementing
AWS Backup, stressing the significance of regular backups, data retention
policies, and compliance requirements.
Overall, this chapter provided a comprehensive understanding of AWS
Database Services and AWS Backup Services, empowering readers to make
informed decisions for their database management and data protection
strategies within the AWS ecosystem.
In the next chapter, we will look through the ever-evolving landscape of
AWS Monitoring Services. We will embark on an expedition into the heart
of CloudWatch Metrics, discovering how to harness their potential to
monitor, troubleshoot, and optimize your AWS resources. We will also
discuss the increasingly critical world of CloudWatch Logs, where logs are
not just records but beacons guiding you through the labyrinth of your
cloud environment.
We will traverse through AWS CloudTrail, exploring its role as the keeper
of truth in your AWS environment. We will also unlock the power of
forensic investigation and compliance, revealing the tales hidden within the
logs, and understanding the who, what, and why behind every AWS API
call.
Multiple choice questions
1. What are the data types supported by DynamoDB?
a. Number
b. String
c. Boolean
d. All of the above
e. A and B only
2. DynamoDB Supports Query and scans API calls.
a. True

b. False
Answers
1. d
2. a
Join our book’s Discord space
Join the book’s Discord Workspace for Latest updates, Offers, Tech
happenings around the world, New Release and Sessions with the Authors:
https://discord.bpbonline.com

CHAPTER 11
Automating and Bootstrapping
Monitoring Service
Introduction
Monitoring is a fundamental aspect of maintaining a robust and reliable
AWS environment. AWS offers a suite of comprehensive monitoring
services designed to provide deep insights into the health and performance
of your AWS resources, enabling you to proactively identify and address
issues before they impact your users and business operations.
In this chapter, we will explore the AWS monitoring landscape, highlighting
the key AWS monitoring services available to businesses and how they can
be utilized to monitor various components of your AWS infrastructure. From
the monitoring of compute instances and databases to application
performance and infrastructure health, AWS monitoring services equip you
with the tools and capabilities needed to optimize your operations and
deliver a superior user experience.
Structure
This chapter covers the following topics:
Overview of AWS Monitoring Services
End-to-end monitoring Architecture Diagram

Amazon CloudWatch: The foundation of AWS Monitoring
Amazon 
CloudTrail: 
Enabling 
comprehensive 
AWS 
activity
monitoring
AWS X-Ray for application performance monitoring
AWS Lambda Monitoring
Monitoring AWS Infrastructure with AWS Inspector
Best practices for effective AWS Monitoring
Objectives
One of the primary objectives of AWS Monitoring Service is to ensure the
optimal performance and reliability of applications and infrastructure hosted
on the Amazon Web Services (AWS) platform. Monitoring is essential for
identifying and addressing issues proactively, preventing potential
downtimes, and enhancing overall operational efficiency. By monitoring
various aspects such as resource utilization, application performance, and
system health, AWS Monitoring Service aims to provide actionable insights
and data-driven decisions for efficient resource allocation and management.
Another critical objective of AWS Monitoring Service is to support capacity
planning and scalability. AWS enables organizations to scale their resources
up or down based on demand, and effective monitoring is vital for
understanding usage patterns and predicting future requirements accurately.
Monitoring tools and services within AWS allow organizations to analyze
trends, forecast growth, and make informed decisions regarding resource
provisioning, ensuring that applications and services can seamlessly handle
increased workloads while maintaining optimal performance.
Lastly, AWS Monitoring Service aims to optimize costs and resource
utilization. Monitoring tools provide insights into resource consumption and
expenditure, helping organizations identify unused or underutilized
resources and make cost-saving adjustments. By optimizing resource
allocation and eliminating unnecessary costs, organizations can achieve
significant cost efficiencies while maintaining the required performance
levels for their applications and services hosted on AWS.

Overview of AWS Monitoring Services
Amazon Web Services offers a robust suite of monitoring services designed
to help businesses effectively manage their cloud infrastructure, optimize
performance, and ensure the reliability and security of their applications.
AWS monitoring services empower organizations to gain insights into their
environment, detect anomalies, and make informed decisions to enhance
operational efficiency.
One key AWS monitoring service is Amazon CloudWatch, a comprehensive
observability solution that collects and analyzes data from various AWS
resources and applications. CloudWatch provides real-time monitoring,
customizable dashboards, and alarms to monitor the health of applications
and infrastructure. It allows tracking of metrics, logs, and traces, enabling
proactive identification of performance issues and immediate response to
ensure smooth operations.
Another essential monitoring service is AWS X-Ray, which offers
distributed tracing to help identify and troubleshoot performance bottlenecks
and latency issues in microservices-based applications. X-Ray provides a
visual representation of application architecture and traces requests as they
traverse different components, making it easier to pinpoint areas for
optimization and enhancement.
AWS also offers AWS Trusted Advisor, a service that provides tailored
recommendations to optimize AWS infrastructure. It offers insights into cost
optimization, security, fault tolerance, performance improvement, and
service limits. Trusted Advisor assists in aligning AWS resources with best
practices, reducing costs, and enhancing the overall efficiency of AWS
deployments.
For log analysis and insights, AWS offers Amazon CloudWatch Logs and
Amazon Elasticsearch. CloudWatch Logs enables centralized logging,
storing, and monitoring of logs, providing visibility into application, system,
and custom logs. Amazon Elasticsearch, on the other hand, offers a powerful
search and analytics engine, allowing for deep log analysis and visualization,
aiding in troubleshooting and operational intelligence.
In summary, AWS monitoring services offer a comprehensive suite of tools
and capabilities to monitor, analyze, and optimize cloud infrastructure and

applications. These services play a crucial role in ensuring high availability,
reliability, and cost-effectiveness for businesses leveraging AWS for their
operations.
End-to-end monitoring architecture diagram
End-to-end monitoring architecture is the backbone of modern businesses
and organizations, ensuring seamless operations and optimal performance
across various interconnected components. At its core, the architecture
diagram represents a comprehensive visual representation of the entire
system, from the initial user interaction to the backend infrastructure.
Starting with the CloudTrail, the diagram delineates the various layers and
modules that make up the service. This includes CloudWatch, user
authentication, and data input mechanisms, all of which play a crucial role in
shaping the user experience.
Moving deeper into the architecture, the diagram illustrates the middleware
layer, showcasing the intricate SNS, and SNS topics. Monitoring tools and
agents are strategically placed at key junctures to collect data on latency,
response times, error rates, and resource utilization. This data is then
centralized in a monitoring system, providing real-time insights into the
system’s health and performance.
Furthermore, the end-to-end monitoring architecture diagram highlights
integration points. These integrations are vital for seamless collaboration and
data exchange between various entities, and monitoring their performance is
essential for ensuring the system’s overall reliability. Overall, the end-to-end
monitoring architecture diagram provides a holistic view of the system’s
design, operation, and resilience, empowering organizations to make
informed decisions and proactively address any potential issues.
The following Figure 11.1 depicts E2E architecture of monitoring services.
The architecture describes our implementation of shipping the CloudTrail
log files stored in an S3 bucket and CloudWatch, and into a Kinesis stream
for additional processing in real-time (refer to the following Figure 11.1):

Figure 11.1: E2E Monitoring Architecture Diagram
Amazon CloudWatch: The foundation of AWS Monitoring
Amazon CloudWatch stands at the forefront of AWS monitoring services,
serving as the foundational monitoring and observability service.
CloudWatch allows you to collect and track metrics, collect, and monitor log
files, and set alarms. These capabilities enable you to gain real-time
operational insights, troubleshoot issues, and keep an eye on the
performance of your applications and AWS resources.
With CloudWatch, you can monitor a wide range of AWS resources,
including Amazon EC2 instances, Amazon RDS databases, Amazon S3
buckets, and more. It provides customizable dashboards, allowing you to
visualize your metrics and create meaningful visual representations of your
system’s health and performance. Additionally, CloudWatch Alarms
empower you to set thresholds and trigger actions based on predefined
criteria, ensuring timely notification and automated responses to potential
issues.
Amazon CloudTrail: Enabling comprehensive AWS activity
monitoring
Amazon CloudTrail is another crucial AWS monitoring service that provides
detailed logs of all API calls made on your AWS account. These logs offer
valuable insights into who accessed your resources, what actions were

performed, and when they occurred. This information is indispensable for
security and compliance, enabling you to maintain an audit trail of AWS
account activity and investigate any unauthorized or suspicious actions.
CloudTrail logs are stored securely in Amazon S3, and you can easily query
and analyze them using Amazon Athena or other analytics tools. This
service plays a crucial role in enhancing security and governance, making it
an essential part of your AWS monitoring strategy.
Cloud auditing and observability automation
Cloud auditing and observability automation have emerged as critical
components in modern cloud computing environments, ensuring robust
monitoring, analysis, and compliance adherence. Cloud auditing involves the
systematic examination of cloud infrastructure and services to verify
compliance with predefined policies, regulations, and security standards.
Automating this process streamlines data collection, analysis, and reporting,
enhancing efficiency and accuracy.
Through automation, cloud auditing tools can continuously monitor various
aspects of cloud infrastructure, such as access controls, data encryption, and
configuration settings. Automated audits can detect and alert on deviations
from established security and compliance policies in real-time, enabling
prompt corrective actions. By automating the audit process, organizations
can reduce human error and ensure a consistent and reliable auditing
mechanism across complex cloud ecosystems.
In parallel, observability automation focuses on enhancing visibility into
cloud systems and applications. It involves collecting and analyzing vast
amounts of data generated by cloud services, applications, and infrastructure.
Automated observability tools use advanced analytics and machine learning
algorithms to process this data and generate actionable insights, such as
performance trends, anomaly detection, and predictive analysis.
Automation in observability allows for the proactive identification of
potential issues and performance bottlenecks. It enables automatic scaling of
resources, optimizing resource allocation based on real-time demand and
usage patterns. This ensures the optimal performance and reliability of cloud
applications, ultimately enhancing user experience.

Moreover, integrating automation into cloud observability facilitates the
creation of dashboards and visualizations that provide a comprehensive view
of the entire cloud environment. These insights empower organizations to
make informed decisions, optimize resource allocation, and improve the
overall operational efficiency of their cloud infrastructure.
In summary, automation in cloud auditing and observability is fundamental
for organizations striving to maintain compliance, security, and optimal
performance within their cloud environments. By leveraging automated tools
and processes, businesses can achieve a more efficient, accurate, and
proactive approach to monitoring and managing their cloud-based
operations:
1. Create a folder named Chapter11.
2. Copy the config folder from previous chapter10 to the Chapter11
folder.
3. Copy remote_state.tf, provider.tf, and .gitignore from previous
chapters.
4. Use the following code to create cloudtrail.tf file in chapter 11:
       resource "aws_cloudtrail" "cloudtrail" 
{
  name                          = "infra-
cloud-trail"
  s3_bucket_name                = "cloudtrail-
infra-prod"
  s3_key_prefix                 = "trail-key"
  include_global_service_events = true
  is_multi_region_trail         = true
  enable_logging                = true
  enable_log_file_validation    = true
 event_selector {

    read_write_type = "WriteOnly"
    data_resource {
      type   = "AWS::S3::Object"
      values = ["arn:aws:s3:::cloudtrail-
infra-prod/*"]
    }
  }
}
5. Use the following code to create kms.tf file in chapter11 folder. KMS
is used to encrypt the log files received from CloudTrail:
resource "aws_kms_key" "sns-cloudtrail-kms" {
  description         = "KMS key used to 
encrypt SNS topic containing events of newly 
delivery CloudTrail log files"
  key_usage           = "ENCRYPT_DECRYPT"
  enable_key_rotation = true
  policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Sid    = "AllowS3ToEncryptMessages"
        Effect = "Allow"
        Principal = {
          Service = "s3.amazonaws.com"
        },

        Action   = ["kms:GenerateDataKey*", 
"kms:Decrypt"],
        Resource = "*"
      },
      {
# to allow authorized principals to update the 
key in the future
        Sid    = "DefaultKeyPolicy"
        Effect = "Allow"
        Principal = {
          AWS = "arn:aws:iam::${local.account-
id}:root"
        },
        Action   = "kms:*",
        Resource = "*"
      }
    ]
  })
}
6. Use the following code to create kinesis.tf file in chapter11 folder.
Kinesis stream is used for further and real time analysis for the
CloudTrail logs:
resource "aws_kinesis_stream" "cloudtrail-
logs" {
  name             = var.kinesis-stream-name
  shard_count      = var.kinesis-num-shards

  retention_period = var.kinesis-retention-
time-days * 24
  encryption_type  = "KMS"
  kms_key_id       = var.kinesis-stream-kms-
key-id
}
7. Use the following code to create kinesis.tf file in chapter11 folder.
Kinesis stream is used for further and real time analysis for the
CloudTrail logs:
resource "aws_iam_role" "lambda-cloudtrail" {
  name        = "lambda-cloudtrail-role"
  description = "Execution role of the Lambda 
function used to ship CloudTrail logs from a 
S3 bucket to a Kinesis stream"
  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Effect = "Allow"
      Action = "sts:AssumeRole"
      Principal = {
        Service = "lambda.amazonaws.com"
      }
    }]
  })
}

resource "aws_iam_policy" "lambda-cloudtrail" 
{
  name        = "lambda-policy"
  path        = "/"
  description = "Role policy of the Lambda 
function used to ship CloudTrail logs from a 
S3 bucket to a Kinesis stream"
  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
# Allow reading from CloudTrail S3 bucket
        Effect   = "Allow",
        Action   = ["s3:GetObject"],
        Resource = 
"arn:aws:s3:::${var.cloudtrail-bucket}/*"
      },
      {
# Allow writing to Kinesis stream
        Effect   = "Allow",
        Action   = ["kinesis:PutRecord", 
"kinesis:PutRecords"],
        Resource = 
aws_kinesis_stream.cloudtrail-logs.arn
      },
      {

# Allow writing logs and metrics to CloudWatch
        Effect = "Allow",
        Action = [
          "logs:CreateLogGroup",
          "logs:CreateLogStream",
          "logs:PutLogEvents",
          "cloudwatch:PutMetricData"
        ],
        Resource = "*"
      }
    ]
  })
}
resource "aws_iam_role_policy_attachment" 
"lambda-cloudtrail" {
  role       = aws_iam_role.lambda-
cloudtrail.name
  policy_arn = aws_iam_policy.lambda-
cloudtrail.arn
}
resource "aws_cloudwatch_log_group" "lambda-
cloudtrail" {
  name              = 
"/aws/lambda/${aws_lambda_function.lambda-
cloudtrail.function_name}"

  retention_in_days = var.cloudwatch-logs-
retention-time-days
}
# Zip lambda source code on the fly
locals {
  lambda-src-path = "${path.module}/lambda"
}
data "archive_file" "lambda-cloudtrail" {
  type        = "zip"
  source_dir  = local.lambda-src-path
  output_path = "lambda.zip"
}
resource "aws_lambda_function" "lambda-
cloudtrail" {
  filename         = "lambda.zip"
  source_code_hash = data.archive_file.lambda-
cloudtrail.output_base64sha256
  function_name    = "lambda-cloudtrail"
  role             = aws_iam_role.lambda-
cloudtrail.arn
  handler          = "main.lambda_entrypoint"
  runtime          = "python3.8"
  timeout          = 900         # seconds
  memory_size      = var.lambda-memory
  tracing_config {

    mode = "Active"
  }
  environment {
    variables = {
      KINESIS_STREAM_NAME       = 
aws_kinesis_stream.cloudtrail-logs.name
      KINESIS_STREAM_NUM_SHARDS = 
aws_kinesis_stream.cloudtrail-logs.shard_count
    }
  }
}
8. Use the following code to create main.tf file in chapter11 folder:
data "aws_caller_identity" "current" {}
locals {
  account-id = 
data.aws_caller_identity.current.account_id
  region      = "us-east-2"
}
9. Use the following code to create output.tf file in chapter11 folder.
The output files are straight forward, and are used to display the
output of implementation:
output "kinesis-stream-name" {
  description = "Name of the newly created 
Kinesis stream"
  value       = aws_kinesis_stream.cloudtrail-
logs.name

}
output "kinesis-stream-arn" {
  description = "ARN of the newly created 
Kinesis stream"
  value       = aws_kinesis_stream.cloudtrail-
logs.arn
}
output "sns-topic-name" {
  description = "Name of the newly creates SNS 
topic"
  value       = aws_sns_topic.cloudtrail.name
}
output "sns-topic-arn" {
  description = "ARN of the newly creates SNS 
topic"
  value       = aws_sns_topic.cloudtrail.arn
}
10. Use the following code to create sns.tf file in chapter11 folder.
11. The SNS file is very detailed with the topic name, bucket event
queue. Since we will be using cloud trail to report everything in our
cloud, we will need to manage the queue for all the message received.
We also create the policy for the SNS topic:
locals {
  cloudtrail_sns_topic_name = 
"${var.cloudtrail-bucket}-event-notification-
topic"
  cloudtrail_sqs_queue_name = 
"${var.cloudtrail-bucket}-event-notification-

queue"
}
# Bucket notification
resource "aws_s3_bucket_notification" "bucket-
notification" {
  bucket = var.cloudtrail-bucket
  topic {
    events        = ["s3:ObjectCreated:*"]
    filter_suffix = ".json.gz"
    topic_arn     = 
aws_sns_topic.cloudtrail.arn
  }
}
# SNS topic
data "aws_iam_policy_document" "sns-topic-
policy-document" {
  statement {
    effect    = "Allow"
    actions   = ["SNS:Publish"]
    resources = 
["arn:aws:sns:${var.region}:${local.account-
id}:${local.cloudtrail_sns_topic_name}"]
    principals {
      type        = "Service"
      identifiers = ["s3.amazonaws.com"]

    }
    condition {
      variable = "aws:SourceArn"
      test     = "ArnLike"
      values   = 
["arn:aws:s3:*:*:${var.cloudtrail-bucket}"]
    }
    condition {
      variable = "aws:SourceAccount"
      test     = "StringEquals"
      values   = [local.account-id]
    }
  }
}
resource "aws_sns_topic" "cloudtrail" {
  name              = 
local.cloudtrail_sns_topic_name
  kms_master_key_id = aws_kms_key.sns-
cloudtrail-kms.id
  policy            = 
data.aws_iam_policy_document.sns-topic-policy-
document.json
}
12. Use the following code to create sns_to_lambda.tf file in chapter11
folder. The SNS to Lambda file is used by lambda function to monitor
the s3 bucket for activity:

resource "aws_lambda_permission" "cloudtrail-
sns-to-lambda" {
  statement_id  = "AllowExecutionFromSnsTopic"
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.lambda-
cloudtrail.arn
  principal     = "sns.amazonaws.com"
  source_arn    = aws_sns_topic.cloudtrail.arn
}
resource "aws_sns_topic_subscription" 
"cloudtrail-sns-to-lambda" {
  topic_arn = aws_sns_topic.cloudtrail.arn
  protocol  = "lambda"
  endpoint  = aws_lambda_function.lambda-
cloudtrail.arn
  depends_on = 
[aws_lambda_permission.cloudtrail-sns-to-
lambda]
}
13. Use the following code to create variables.tf file in chapter11
folder. The variables file is very straight forward and does not require
too much explanation. Some variables have already been set within
the variable; you can change some values:
variable "cloudtrail-bucket" {
  description = "Name of the S3 bucket in 
which CloudTrail logs are stored (must exist 
and properly configured to receive CloudTrail 
logs prior to calling this module)"

  type        = string
}
variable "region" {}
variable "cloudtrail-sns-topic-name" {
  description = "Name of the SNS topic where 
information about newly shipped CloudTrail log 
files are sent"
  type        = string
  default     = "organization-trail-event-
notification-topic"
}
variable "kinesis-stream-name" {
  description = "Name of the Kinesis stream 
used for aggregation"
  type        = string
  default     = "cloudtrail-logs-stream"
}
variable "kinesis-stream-kms-key-id" {
  description = "ID of the KMS key to use for 
encrypting the Kinesis stream"
  type        = string
  default     = "alias/aws/kinesis"
}
variable "kinesis-num-shards" {
  description = "Number of shards to use in 
the Kinesis stream"

  type        = number
  default     = 4
}
variable "kinesis-retention-time-days" {
  description = "Retention period of the 
Kinesis stream (in days)"
  type        = number
  default     = 7
}
variable "lambda-memory" {
  description = "Memory to allocate to the 
Lambda function"
  type        = number
  default     = 512
}
variable "cloudwatch-logs-retention-time-days" 
{
  description = "Retention period for the 
CloudWatch logs of the Lambda function (in 
days)"
  type        = number
  default     = 7
}
14. Create a folder named scripts inside the chapter 11 folder.
15. Use the following code to create a Python kinesis.py file in
chapter11 folder:

"""
Utility script to continuously read data from 
a Kinesis stream, from multiple shards, and 
decode the CloudTrail logs it contains
Sample usage:
$ python scripts/cat-kinesis.py some-kinesis-
stream
"""
import boto3
import sys
import time
import base64
import json
if len(sys.argv) != 2:
    sys.stderr.write("Usage: python 
scripts/cat-kinesis.py stream-name\n")
    exit(1)
stream_name = sys.argv[1]
kinesis = boto3.client('kinesis')
stream = 
kinesis.describe_stream(StreamName=stream_name
)
shard_ids = [ shard['ShardId'] for shard in 
stream['StreamDescription']['Shards'] ]
shard_iterators = [ 
kinesis.get_shard_iterator(StreamName=stream_n
ame, ShardId=shard_id, 

ShardIteratorType='LATEST')['ShardIterator'] 
for shard_id in shard_ids]
print(f'Found {len(shard_ids)} shards')
while len(shard_iterators) > 0:
    new_iterators=[]
    for iterator in shard_iterators:
        records = 
kinesis.get_records(ShardIterator=iterator)
        for record in records.get('Records'):
            
print(json.dumps(json.loads(record.get('Data')
.decode('utf8')), indent=2))
        if records.get('NextShardIterator'):
            
new_iterators.append(records.get('NextShardIte
rator'))
    time.sleep(1)
16. Create a folder named lambda inside the chapter 11 folder.
Use the following code to create a Python main.py file in lambda
folder:
import json
import logging
import os
import sys
import telemetry
import transform

from extract import get_cloudtrail_file
from load import write_to_kinesis
from transform import 
read_cloudtrail_events_file
logger = logging.getLogger()
MAX_CLOUDTRAIL_ENTRIES_PER_KINESIS_RECORD = 
100
def setup_logging():
    root = logging.getLogger()
    if root.handlers:
        for handler in root.handlers:
            root.removeHandler(handler)
    logging.basicConfig(format='%(asctime)s %
(message)s', level=logging.INFO)
def lambda_entrypoint(event, context):
    setup_logging()
    # 1) Read the 'new file' notification sent 
by S3 and extract the file name
    bucket, path = get_cloudtrail_file(event)
    # 2) Read the CloudTrail log file from the 
S3 bucket
    cloudtrail_event_payload = 
read_cloudtrail_events_file(bucket, path)
    cloudtrail_event = 
json.loads(cloudtrail_event_payload)
    num_events = 
len(cloudtrail_event.get("Records"))

    logging.info(f'CloudTrail log file has 
{num_events} log entries')
# 3) Send to Kinesis
    kinesis_messages = 
transform.chunk_cloudtrail_events(cloudtrail_e
vent,
                                                 
chunk_size=MAX_CLOUDTRAIL_ENTRIES_PER_KINESIS_
RECORD)
    kinesis_messages = map(lambda message: 
json.dumps(message, separators=(',', ':')), 
kinesis_messages)
    write_to_kinesis(list(kinesis_messages), 
os.environ['KINESIS_STREAM_NAME'],
                     
int(os.environ['KINESIS_STREAM_NUM_SHARDS']))
    # 4) Telemetry
    telemetry.log_event(num_events)
def cli_entrypoint():
    if len(sys.argv) != 4:
        sys.stderr.write("CLI usage: python 
lambda_handler.py bucket path kinesis-stream")
        exit(1)
    setup_logging()
    _, bucket, path, target_kinesis_stream = 
sys.argv
    cloudtrail_event_payload = 
read_cloudtrail_events_file(bucket, path)

    cloudtrail_event = 
json.loads(cloudtrail_event_payload)
    kinesis_messages = 
transform.chunk_cloudtrail_events(cloudtrail_e
vent,
                                                 
chunk_size=MAX_CLOUDTRAIL_ENTRIES_PER_KINESIS_
RECORD)
    kinesis_messages = map(json.dumps, 
kinesis_messages)
    write_to_kinesis(list(kinesis_messages), 
target_kinesis_stream)
if __name__ == '__main__':
    cli_entrypoint()
17. Use the following code to create a python load.py file in lambda
folder:
import logging
import random
from typing import List
import boto3
# 
https://docs.aws.amazon.com/kinesis/latest/API
Reference/API_PutRecords.html
KINESIS_MAX_NUM_RECORDS = 500
KINESIS_MAX_RECORDS_PER_BATCH = 10
def write_to_kinesis(payload: List[str], 
stream_name: str, num_shards: int = 1):
    kinesis = boto3.client('kinesis')

    # Write at most 
KINESIS_MAX_RECORDS_PER_BATCH records at a 
time
    batches = [payload[i:i + 
KINESIS_MAX_RECORDS_PER_BATCH] for i in
               range(0, len(payload), 
KINESIS_MAX_RECORDS_PER_BATCH)]
    logging.info(f'Writing CloudTrail log 
payload to Kinesis ({len(payload)} records in 
{len(batches)} batches)')
    for batch in batches:
        result = kinesis.put_records(
            StreamName=stream_name,
            Records=[{'Data': message, 
'PartitionKey': str(random.randint(1, 
num_shards))} for message in batch],
        )
    logging.info(f'Successfully shipped 
{len(payload)} records to Kinesis')
18. Use the following code to create a Python extract.py file in lambda
folder:
import json
# Extracts the CloudTrail log file name from a 
SNS notification
# Returns a tuple (bucket, object name)
def get_cloudtrail_file(sns_event: object):
    records = sns_event.get('Records', [])
    if len(records) != 1:

        raise ValueError(f'Expected 1 record, 
got {len(records)}')
    record = records[0].get('Sns')
    eventName = record.get('Subject')
    if eventName != 'Amazon S3 Notification':
        raise ValueError(f'Ignoring invalid 
SNS notification type "{eventName}"')
    s3_notification = 
json.loads(record.get('Message'))
    return 
_get_cloudtrail_file_from_s3_notification(s3_n
otification)
# Private method
# Extracts the CloudTrail log file name from a 
S3 notification
# Returns a tuple (bucket, object name)
def 
_get_cloudtrail_file_from_s3_notification(s3_n
otification: object):
    records = s3_notification.get('Records', 
[])
    if len(records) != 1:
        raise ValueError(f'Expected 1 record, 
got {len(records)}')
    record = records[0]
    eventName = record.get('eventName')
    if eventName != 'ObjectCreated:Put':

        raise ValueError(f'Ignoring invalid 
event type "{eventName}"')
    return record['s3']['bucket']['name'], 
record['s3']['object']['key']
19. Use the following code to create a Python telemetry.py file in
lambda folder:
from datetime import datetime
import boto3
METRIC_NAME = 
'NumberOfCloudTrailRecordsShippedToKinesis'
METRIC_NAMESPACE = 
'CloudtrailS3ToKinesisShipper'
def log_event(num_cloudtrail_records: int):
    cloudwatch = boto3.client('cloudwatch')
    metric_data = [{
        'MetricName': METRIC_NAME,
        'Dimensions': [],
        'Timestamp': datetime.now(),
        'Value': num_cloudtrail_records,
        'Unit': 'Count'
    }]
    
cloudwatch.put_metric_data(Namespace=METRIC_NA
MESPACE, MetricData=metric_data)
20. Use the following code to create a Python transform.py file in
lambda folder:
import gzip

import logging
from typing import List
import boto3
def read_cloudtrail_events_file(bucket: str, 
path: str):
    s3 = boto3.resource('s3')
    logging.info(f'Reading CloudTrail log file 
s3://{bucket}/{path}')
    raw_bytes = s3.Object(bucket, path).get()
['Body'].read()
    return gzip.decompress(raw_bytes)
def chunk_cloudtrail_events(cloudtrail_event: 
object, chunk_size) -> List[object]:
    records = cloudtrail_event.get('Records')
    record_chunks = [records[i:i + chunk_size] 
for i in range(0, len(records), chunk_size)]
    logging.info(f'Chunked CloudTrail records 
in {len(record_chunks)} chunks')
    return [{'Records': record_chunk} for 
record_chunk in record_chunks]
21. Use the following code to create a text requirement.txt file in
lambda folder:
boto3==1.17.48
22. Now use this following command initialize the folder:
terraform init --backend-config=config/demo.config
terraform plan
terraform apply --approve-auto

23. Push your code to a new branch in our codecommit "chapter11" git
push in the mentioned GitHub repository.
Note: When you run the code, there will be a file called lambda.zip file.
You will need to wait for about 4-8 hours for the cloudwatch to pick it
up.
AWS X-Ray for application performance monitoring
AWS X-Ray is a powerful tool designed to provide comprehensive insights
into the performance of applications running in the Amazon Web Services
(AWS) cloud environment. It enables developers to identify and troubleshoot
performance bottlenecks, errors, and latency issues in distributed
applications, making it an invaluable tool for optimizing application
performance.
One of the key features of AWS X-Ray is its ability to trace requests as they
flow through various components of a distributed application. This tracing
capability allows developers to visualize the entire request lifecycle,
providing a clear and detailed picture of how different services and resources
contribute to the overall performance. By understanding the end-to-end
journey of a request, developers can pinpoint areas for optimization and
enhance the overall user experience.
Furthermore, AWS X-Ray offers insightful analytics and visualizations,
including service maps and trace views, which help in identifying patterns
and anomalies in application behavior. These visualizations enable
developers to understand the dependencies and interactions between various
components, making it easier to optimize performance, troubleshoot issues,
and enhance the application’s architecture.
In addition to performance monitoring, AWS X-Ray also plays a crucial role
in enhancing application security. By tracing requests and identifying
potential vulnerabilities or abnormal patterns of behavior, developers can
proactively address security concerns, ensuring a more robust and secure
application environment. This proactive approach to security aligns with
modern development practices that emphasize security from the outset.

Overall, AWS X-Ray is an indispensable tool for monitoring and optimizing
application performance in AWS environments. Its tracing capabilities,
analytics, and visualization features empower developers to enhance the
performance, reliability, and security of their applications, ultimately leading
to a better user experience and increased operational efficiency in the cloud.
Setting-up AWS X-Ray using Terraform
Setting up AWS X-Ray using Terraform involves automating the
provisioning and configuration of AWS X-Ray resources using
infrastructure-as-code principles. AWS X-Ray is a distributed tracing service
that helps developers analyze and debug production, distributed applications.
Firstly, you will need to define your Terraform configuration files. Begin by
specifying the provider and region to work with AWS. Then, create the
necessary AWS X-Ray resources, such as the X-Ray group, sampling rules,
and encryption configuration. You will also want to define IAM roles and
permissions for X-Ray to access other AWS services.
Next, incorporate the AWS X-Ray resources into your Terraform
configuration. Define a Terraform module for AWS X-Ray, encapsulating
the necessary resources and their configurations. Within this module, set up
the X-Ray group, specifying the group name, filter expressions, and any
additional settings.
To ensure effective tracing and monitoring, consider configuring sampling
rules based on your application’s requirements. This involves defining rules
for sampling requests based on criteria such as HTTP methods, URLs, or
user agents.
Additionally, implement encryption settings for sensitive data within X-Ray
traces. Configure encryption using AWS Key Management Service (KMS)
keys to protect your trace data at rest.
Finally, validate and apply your Terraform configuration to provision and
configure the AWS X-Ray resources. Run terraform init and terraform plan
to verify the changes and dependencies. Once validated, execute terraform
apply to provision the X-Ray resources according to your defined
configuration.

By utilizing Terraform to automate the setup of AWS X-Ray, you can
efficiently manage and maintain a consistent and well-configured tracing
environment for your distributed applications, aiding in performance
optimization and debugging.
AWS Lambda monitoring
AWS Lambda monitoring is an essential aspect of maintaining a robust
serverless architecture. With AWS Lambda, monitoring provides insights
into the performance, availability, and health of your serverless functions,
enabling timely detection and resolution of issues. Utilizing AWS
CloudWatch, you can gather metrics, set alarms, and visualize data to ensure
your applications are running smoothly.
One of the key advantages of AWS Lambda monitoring is the real-time
tracking of invocation metrics. Metrics such as invocation count, duration,
error count, and concurrent executions offer valuable insights into the
function’s behavior and performance. By closely monitoring these metrics,
you can optimize the function’s resource allocation and make informed
decisions to enhance efficiency.
Moreover, AWS Lambda monitoring allows for proactive identification of
potential issues. Setting up CloudWatch Alarms based on predefined
thresholds for metrics empowers you to receive notifications when certain
thresholds are breached. This ensures prompt action to address performance
degradation or errors, guaranteeing a seamless user experience.
In addition to metrics and alarms, AWS Lambda monitoring offers logging
capabilities for enhanced visibility. Leveraging AWS CloudWatch Logs, you
can collect and analyze logs generated by your Lambda functions. These
logs provide valuable information for troubleshooting, performance
optimization, and auditing, allowing you to maintain a high level of system
integrity and reliability.
Furthermore, AWS Lambda monitoring supports custom metrics, enabling
you to tailor the monitoring to your specific application requirements. You
can create and publish custom metrics related to business specific KPIs,
enabling a comprehensive understanding of the function’s impact on your
application’s overall performance and user satisfaction.

In conclusion, AWS Lambda monitoring is a critical component of a well-
architected serverless application. By leveraging AWS CloudWatch’s robust
monitoring capabilities, you can gain real-time insights, proactively address
potential issues, and optimize the performance of your serverless functions
for a seamless user experience.
Monitoring AWS infrastructure with AWS Inspector
AWS Inspector is a valuable tool for monitoring the security and compliance
of your AWS infrastructure. Leveraging AWS Inspector allows you to
proactively identify potential security vulnerabilities and compliance
deviations in your environment. The service employs a range of security
assessment tests to evaluate the security posture of your resources, providing
you with insightful findings and recommendations to mitigate identified
risks.
Using AWS Inspector, you can schedule and automate security assessments
on your EC2 instances, ECS, and other AWS resources. These assessments
delve into various aspects of security, such as network exposure, operating
system vulnerabilities, and application vulnerabilities. The tool not only
helps you understand your system’s security status but also guides you in
prioritizing and addressing the most critical issues first, ensuring a focused
and efficient approach to security improvements.
One of the significant benefits of AWS Inspector is its integration with AWS
services like AWS CloudWatch and AWS Systems Manager. This integration
allows for seamless monitoring, alerting, and remediation based on the
Inspector findings. By setting up custom Amazon CloudWatch Alarms, you
can receive real-time alerts for specific security vulnerabilities or
compliance violations, enabling immediate action to mitigate potential risks
and maintain a robust security posture.
Furthermore, AWS Inspector simplifies compliance reporting by generating
detailed assessment reports that can be shared with stakeholders, auditors, or
regulatory bodies. These reports provide a comprehensive overview of
security assessment results, including identified vulnerabilities, their severity
levels, and recommended remediation steps. This streamlined reporting
process facilitates compliance with industry standards and regulations,

giving you peace of mind and enhancing the credibility of your
infrastructure.
In conclusion, AWS Inspector offers a powerful and comprehensive solution
for monitoring and enhancing the security of your AWS infrastructure. Its
automated assessment capabilities, integration with AWS services, and
detailed reporting features make it an indispensable tool for maintaining a
secure and compliant environment in the ever-evolving landscape of cloud
computing.
Setting up AWS inspector assessments
Setting up AWS Inspector using Terraform allows for efficient and
automated vulnerability assessment and security compliance monitoring
within your AWS infrastructure. The rules used in the code below is located
in:
https://docs.aws.amazon.com/inspector/v1/userguide/inspector_rules-
arns.html#us-east-2
1. Use one of the EC2 Solution we already created in previous chapters.
2. Add the tag below to one or two instance you would like to use as
Inspector server:
tags { Name = "InspectInstances" }
3. This step initializes resource group that will be used by Amazon
Inspector.
4. Define the assessment template based on the Amazon region:
resource "aws_inspector_resource_group" 
"OS_Hardening_Scan" {
tags {
Name = "${aws_instance.inspector-
instance.tags.Name}"
  }
}

resource "aws_inspector_assessment_target" 
"inspect-assessment" {
  name = "inspector-instance-assessment"
  resource_group_arn = 
"${aws_inspector_resource_group.bar.arn}"
}
resource "aws_inspector_assessment_template" 
"scanrules" {
  name       = "inspector template"
  target_arn = 
"${aws_inspector_assessment_target.OS_Hardenin
g_Scan.arn}"
  duration   = 3600
  rules_package_arns = [
    "arn:aws:inspector:us-east-
1:316112463485:rulespackage/0-gEjTy7T7",
    "arn:aws:inspector:us-east-
1:316112463485:rulespackage/0-rExsr2X8",
    "arn:aws:inspector:us-east-
1:316112463485:rulespackage/0-R01qwB5Q",
    "arn:aws:inspector:us-east-
1:316112463485:rulespackage/0-gBONHN9h"
  ]
}
output "inspect-assessment-template-arn" {
  value = 
aws_instector_assessment_template.scanrules.ar
n

}
5. Now use this following command to initialize the folder:
terraform init --backend-config=config/demo.config terraform
plan terraform apply --approve-auto
6. Push you code to a new branch in our codecommit "chapter11-B" git
push in the mentioned GitHub repository.
7. Run AWS CLI with the ARN output from terraform. We also replace
the assessment run name:
aws 
inspector 
start-assessment-run 
--assessment-run-name
Hardeningrun --assessment-template-arn "output ARN value from
terraform apply"
8. Go to AWS inspector console after 1-hour.
Best practices for effective AWS Monitoring
Effective monitoring is crucial for maintaining optimal performance,
security, and reliability in an AWS environment. Here are some best
practices to ensure effective AWS monitoring:
Comprehensive monitoring strategy: It develops a comprehensive
monitoring strategy that aligns with your business objectives and
technical requirements. Clearly define the key performance indicators
(KPIs), metrics, and events you need to monitor across your AWS
infrastructure.
Utilize AWS CloudWatch: It leverages AWS CloudWatch, a
powerful monitoring service that allows you to collect and track
metrics, set alarms, and automatically respond to changes in your
AWS resources. Use CloudWatch to monitor EC2 instances, Lambda
functions, S3 buckets, and other AWS services.
Custom metrics and alarms: It creates custom metrics and alarms
based on specific thresholds and patterns relevant to your applications
and services. Tailor alerts to notify the appropriate personnel or trigger
automated actions when thresholds are breached.

Resource tagging for organization: It implements consistent
resource tagging practices. Tagging resources allows for organized
monitoring by providing metadata that can be used to filter and group
metrics. This aids in efficient monitoring, cost allocation, and resource
management.
Utilize AWS CloudTrail: It integrates AWS CloudTrail for
monitoring and auditing API calls made on your AWS account.
CloudTrail provides insights into user activity, allowing you to track
actions, detect unauthorized access, and ensure compliance with
security policies.
Integrate AWS Config: It integrates AWS Config to maintain an
inventory of AWS resources and configurations. This helps track
changes, assess compliance, and identify potential security risks or
drifts from desired configurations.
Monitoring Automation with AWS Lambda: It automates
monitoring tasks using AWS Lambda functions. For instance, trigger
Lambda functions in response to specific CloudWatch alarms,
enabling automated actions and remediation based on defined criteria.
Performance optimization with AWS Trusted Advisor: It leverages
AWS Trusted Advisor to receive recommendations on cost
optimization, performance improvements, security enhancements, and
fault tolerance based on AWS best practices. Implement these
recommendations to optimize your AWS environment.
Implement anomaly detection: It utilizes anomaly detection
algorithms to identify unusual patterns or deviations in your metrics.
Implement machine learning-based approaches or statistical models to
automatically detect anomalies, enabling proactive responses to
potential issues.
Regular performance reviews and optimization: It conducts regular
reviews of your monitoring setup and performance data. Use the
insights gained to optimize your infrastructure, improve application
performance, enhance cost efficiency, and refine your monitoring
strategy based on evolving needs.

Disaster recovery and high availability monitoring: It implements
thorough monitoring for disaster recovery and high availability setups.
Continuously monitor failover mechanisms, replication status, and
backup systems to ensure business continuity in case of failures.
Collaborative monitoring and feedback: It encourages collaboration
and feedback from all relevant teams involved in managing and
monitoring AWS resources. Foster a culture of continuous
improvement, where feedback and lessons learned from monitoring
are shared and used to enhance the overall monitoring strategy.
Conclusion
Amazon Web Services offers a comprehensive suite of monitoring services
designed to help businesses effectively monitor and manage their cloud
infrastructure and applications. AWS Monitoring Service encompasses a
range of tools and features that enable real-time monitoring, efficient
resource utilization, and proactive issue detection, ensuring optimal
performance and reliability for AWS-hosted environments.
One core component of AWS Monitoring Service is Amazon CloudWatch, a
monitoring and observability service that collects and processes data from
various sources such as AWS resources, custom applications, and third-party
integrations. CloudWatch provides detailed insights into system metrics,
application performance, and operational health, allowing users to set
alarms, visualize data through customizable dashboards, and generate
meaningful insights through advanced analytics.
AWS also offers Amazon CloudTrail, a service that records API calls made
on AWS resources, providing a comprehensive audit trail for compliance,
security, and operational troubleshooting. This service helps organizations
track user activity and detect any unauthorized or unusual actions within
their AWS environment.
For in-depth application monitoring and troubleshooting, AWS X-Ray is
available, offering a distributed tracing service that allows developers to
analyze and optimize the performance of their applications. X-Ray provides
a visual representation of the entire application architecture, helping identify
bottlenecks and areas for optimization.

Moreover, AWS provides AWS Config, a service that helps users assess,
audit, and evaluate configurations of AWS resources. It offers a centralized
view of resource configurations and changes, aiding in compliance
management, security assessment, and resource governance.
Additionally, AWS Insights provides automated anomaly detection and
actionable insights by leveraging machine learning to analyze operational
data, helping users identify unusual behavior and potential issues early on,
leading to faster incident response and resolution.
AWS Monitoring Service encompasses a suite of tools like Amazon
CloudWatch, Amazon CloudTrail, AWS X-Ray, AWS Config, and AWS
Insights. These tools work in harmony to provide comprehensive
monitoring, auditing, and troubleshooting capabilities, empowering
businesses to optimize their AWS infrastructure, enhance performance, and
ensure a secure and reliable cloud environment.
Multiple choice questions
1. What AWS service allows the system administrator to receive
notifications:
a. Lambda
b. CloudWatch
c. CloudTrail
d. SNS
e. All of the above
2. The external auditor requested your company to provide the
deployment logs.
a. CloudWatch
b. Lambda
c. CloudTrail

d. SNS
e. None of the above
Answers
1. d
2. c

Index
A
Access Control Lists (ACLs) 82, 131
Advanced Encryption Standard (AES) 309
advanced routing strategies 238, 239
advance EC2 instance types 177
accelerated computing instances 178
AI-optimized instances 178
burstable performance instances 178
compute-optimized instances 177
High Performance Computing (HPC) instances 178
instance families with local NVMe storage 178
memory-optimized instances 177
network and security, configuring 178
network-optimized instances 178
securing, with EC2 Nitro Enclave 178
storage-optimized instances 177
types 177
use cases 177
Amazon Aurora 289, 290
Amazon CloudTrail 317
cloud auditing and observability automation 317-331
Amazon CloudWatch 316
Amazon DynamoDB 79, 286
Amazon EBS use cases
application versioning and testing 94
Content Management Systems (CMS) 93
Disaster Recovery (DR) solutions 93
financial applications 93
High-Performance Computing (HPC) workloads 93
high-performance databases 93
machine Learning model training 93
real-time analytics 93
software development environments 94
video editing and rendering 94
Amazon EFS (Elastic File System) 79
Amazon EFS use cases
Big Data Analytics and Machine Learning Workloads 98
containerized applications 99
Content Management Systems (CMS) 98
DevOps CI/CD Pipelines 98

genomic data analysis 99
hybrid cloud storage 98
IoT Data Aggregation and Analytics 99
media and entertainment collaboration 98
multi-tier web applications 99
software development environments 98
Amazon ElastiCache 289
Amazon Elastic Block Storage (EBS) 79, 91
automation, using Terraform 94-96
features 92
flexible storage types 92
use cases 93, 94
Amazon Elastic Block Store (EBS) 165
Amazon Elastic Compute Cloud (Amazon EC2) instances 91
Amazon Elastic Compute Cloud (EC2) 123, 163
overview 164, 165
Amazon Elastic File System (Amazon EFS) 97, 98
automation, using Terraform 99-105
use cases 98, 99
Amazon Elastic Kubernetes Service (EKS) 247-251
Amazon FSx 79, 105
features 105, 106
use cases 106
Amazon FSx use cases
automation, using Terraform 107-110
autonomous vehicle data processing 107
CAD and CAM collaboration 107
education and research collaboration 107
financial services applications 107
genomics and bioinformatics 106
High-Performance Computing (HPC) 107
media and entertainment workloads 106
real-time analytics and big data 107
Amazon Glacier 79, 110
automation, using Terraform 112-117
features 110, 111
use cases 111, 112
Amazon Glacier use cases
digital evidence and forensics 112
digital preservation for cultural institutions 111
disaster recovery data backup 112
genomic data storage 111
historical document and newspaper archives 112
legal and compliance data retention 112
media and entertainment archives 112
satellite and remote sensing data archiving 112
scientific research data preservation 112
video surveillance footage retention 112

Amazon Keyspaces 288
Amazon Machine Image (AMI) 94
automation, using Terraform 166
defining, in Terraform 166-177
overview 165, 166
Amazon Neptune 288
Amazon RDS 79
Amazon Redshift 79, 288, 289
Amazon S3 Glacier 117
automation using Terraform 119, 120
features 117, 118
use cases 118, 119
Amazon S3 Glacier use cases
AI and ML model storage 119
compliance and regulatory requirements 118
digital forensics and legal evidence 119
digital preservation 118
disaster recovery and business continuity 118
genomic data storage 119
long-term data archiving 118
media and entertainment content archiving 118
satellite image archiving 119
scientific data archiving 119
Amazon S3 use cases
application data storage 84
Big Data Analytics and Data Lak 83
compliance and regulatory requirements 84
Content Delivery Network (CDN) Origin 83
Data Backup and Archiving 83
data sharing and collaboration 83
disaster recovery 83
IoT data storage 83
log storage and analysis 84
Machine Learning model storage 83
media storage and distribution 83
Static website hosting 83
Amazon Simple Storage Service (S3) 78-81
automation, using Terraform 84-91
S3 access controls 82
S3 buckets 82
S3 objects 82
use cases 83, 84
Amazon Web Services (AWS) 1, 93, 131
Amazon Web Services tool set
automation tool set 2, 3
AWS CloudFormation 3
AWS CodeCommit 3
overview 2

Amazon Web Services Virtual Private Cloud (AWS VPC) 132
AWS Network Access Control List (NACL) 135
overview 133, 134
Security Gateway 135
Application Load Balancer (ALB) 206, 207
web server, automating with 207-213
Availability Zone (AZ) 92, 134, 290
AWS account structure 43
segments 44
AWS Backup and Recovery 304, 305
AWS Backup architecture
backup and restore strategies 306, 307
backup integration, with AWS Services 307, 308
backup services 309
backup services monitoring and reporting 308, 309
compliance 309
components 305, 306
security 309
AWS Backup policy 59, 63
AWS CICD Pipeline architecture
deployment 33-35
End-2-End view 31
enterprise Terraform deployment 32
Local Code Deployment directory structure 32
AWS CloudDevelopment Kit 6, 7
Continuous Delivery (CD) 7
Continuous Integration (CI) 8
AWS CloudFormation 3, 4
change set 6
template anatomy 5, 6
AWS CodeArtifact 8
AWS CodeBuild 8
AWS CodeDeploy 7
AWS CodePipeline 8
AWS Database Services 285
exploring 286
NoSQL Database Services 287, 288
Relational Database Service (RDS) 286, 287
AWS DataSync 80
AWS deployment tools
end-to-end deployment pipeline, creating with CloudFormation 9-16
end-to-end view 8
AWS DynamoDB 290, 291
architecture diagram 291
setting up 292-304
AWS EKS and Fargate
access and security 259-268
applications, deploying 270-279

containerization 250, 251
exploring 249, 250
logging 268, 269
managing 280, 281
monitoring 268, 269
setting up 252-259
updating 280, 281
AWS governance policies 58
AWS IAM role 73
creating, with Terraform 73
AWS Identity and Access Management (IAM) 40, 41, 39, 251
architecture 41
governance frameworks, establishing 43
group 42
key components 42
user 42
AWS Inspector 333
assessments, setting up 334, 335
for monitoring AWS infrastructure 333, 334
AWS Lambda monitoring 333
AWS monitoring
best practices 336, 337
AWS Monitoring Services 314, 315
AWS Network Access Control List (NACL) 135
AWS organization policies
bash script, using for enabling 48
types 47, 48
AWS Route 53 87, 231
basics 232, 233
benefits 232
features 231
policies, exploring 233, 234
simple routing policies, implementing 234
AWS Route 53 Blue-Green
routing automation, using Terraform 239-241
AWS Route 53 Failover 241
automation using Terraform 241-244
AWS Route 53 Policy and Routing Automation 230
AWS Service Control Policy (SCP) 67
AWS Snow Family 80
AWS Storage
Amazon DynamoDB 79
Amazon EBS (Elastic Block Store) 79
Amazon EFS (Elastic File System) 79
Amazon FSx 79
Amazon Glacier 79
Amazon RDS 79
Amazon Redshift 79

Amazon S3 78
AWS DataSync 80
AWS Snow Family 80
AWS Storage Gateway 79
benefits 80, 81
overview 78
types 78
AWS Storage Gateway 79
AWS Storage Gateway use cases
cloud bursting 124
data archiving and long-term retention 124
data migration to cloud 124
Disaster Recovery (DR) and backup 123
file sharing and collaboration 124
hybrid cloud application development 124
IoT data ingestion and processing 124
media and entertainment workflows 124
AWS Tag policies 64, 66
AWS Virtual Private Clouds (VPCs) 225
AWS Virtual Private Cloud (VPC) Auditing 161
AWS Virtual Private Cloud (VPC) Monitoring 161
AWS VPC components 134
Elastic IP 134
Internet Gateway (IGW) 134
Network Address Translation Gateway 135
subnet 134
AWS VPC Endpoint 135, 136
gateway endpoints 136
interface endpoints 136
AWS VPC peering 136, 158, 159
architecture 159
automation using Terraform 160, 161
AWS X-Ray 331
for application performance monitoring 331, 332
setting up, using Terraform 332
B
Blue-Green deployment 239
Bucket Policies 82
C
Classic Load Balancer (CLB) 216, 217
web server, automating with 217-224
CodeCommit 3
repository, setting up 3
Command-Line-Interface (CLI) 22
Computer-Aided Design (CAD) 107

Computer-Aided Manufacturing (CAM) 107
Confidentiality, Integrity, And Availability (CIA) 39
containerization 250, 251
Continuous Delivery (CD) 7
Continuous Integration (CI) 8
Continuous Integration/Continuous Deployment (CI/CD) pipelines 217
D
Disaster Recovery and Business Continuity 310
Disaster Recovery as a Service (DRaaS) 310
Domain Name System (DNS) 232
Dynamic Host Configuration Protocol (DHCP) 133
E
EC2 Auto Scaling group
deploying 192-202
EC2 Availability Group
deploying, for application 188-191
ElastiCache 289
Elastic Load Balancer (ELB) 205, 206
Application Load Balancer (ALB) 206, 207
Classic Load Balancers (CLB) 216, 217
Gateway Load Balancers (GWLB) 224-226
Network Load Balancer (NLB) 213
Elastic Network Interfaces (ENIs) 225
end-to-end monitoring architecture 315, 316
F
flexible storage types, EBS
Cold HDD 92
General Purpose (SSD) 92
Provisioned IOPS (SSD) 92
Throughput Optimized HDD 92
G
Gateway Load Balancer (GWLB) 224-226
Graphical User Interface (GUI) 123
H
HashiCorp Configuration Language (HCL) 23
High-Performance Computing (HPC) 177
Horizontal Pod Autoscaler (HPA) 252
I
IAM lifecycle management

AWS organization policies 47
Bash Script, to create your own organization structure 46, 47
implementing 45
resources, securing with AWS Policy 58
service control policies, automating 46
IAM policies 82
Identity and Access Management (IAM) 287, 309
Infrastructure as Code (IaC) 2, 3
Internet Control Message Protocol (ICMP) 135
Internet Gateway (IGW) 257
Intrusion Detection System (IDS) 225
Intrusion Prevention Systems (IPS) 225
K
Key Management Service (KMS) 290
Kubernetes fundamentals 249
L
Local Code Deployment directory structure
modules folder 32
root folder 32
sample parameter folder 33
template folder 33
N
Network Address Translation (NAT) gateway 257
Network Load Balancer (NLB) 213
web server, automating with 214-216
Next Generation Firewalls (NGFW) 224
NoSQL Database Services 287, 288
O
Online Analytical Processing (OLAP) 285
P
Policy-Based Access Control
AWS user, creating 49
role, creating 49
user group, creating 49
using Terraform IaC 48
R
Relational Database Service (RDS) 285-287
Remote Procedure Calls (RPC) 22
Role-Based Access Control (RBAC) 259

S
S3 Object Ownership 82
SCP creation, with Terraform 67, 68
access planning matrix table 71
AWS Compliance and Auditing, using Terraform 69
AWS IAM best practices 70, 71
AWS IAM role 73
IAM group, creating 72
users' automation, using Terraform 69
Sentinel
installing, on Linux 28
installing, on MacOS 29
installing, on Windows 29, 30
Simple Routing Automation
using Terraform 234-238
Site Reliability Engineering (SRE) 1
state file remote storage
creating, with s3 36
deployment, validating with AWS Console 36
T
Terraform 19
architecture overview 25
benefits 23
challenges 37
commands 30
common terminologies 30
considerations 37
data blocks 21
infrastructure 20, 21
key features and capabilities 23, 24
lifecycle block 22
modules 21
output blocks 21
overview 20
provider configuration 20
provisioners 22
resource blocks 21
setting up, on Linux Debian OS Family 25
setting up, on MacOS 26, 27
setting up, on RHEL OS Family 26
setting up, on Windows 27, 28
state file remote storage, creating with s3 36
state management 22
Terraform Cloud/Enterprise (Optional) 22
Terraform core 22
Terraform Plugins 22

variables 21
Terraform Infrastructure as Code 24
terraform init command 84
three-tier subnet deployment architecture
3-Tier VPC Automation, using Terraform 150-158
overview 148, 149
Transmission Control Protocol (TCP) 225
two-tier subnet deployment architecture
2-Tier VPC automation, using terraform 138-148
overview 136, 137
U
User Datagram Protocol (UDP) 135, 225
V
Virtual Machine (VM) image 122
Virtual Private Cloud (VPC) 131
Virtual Private Network (VPN) 131, 133
W
web application
deploying, on multi-EC2 Web Server General Purpose using Terraform 182-188
deploying, on single EC2 General Purpose 179-182
WS Storage Gateway 121
Amazon EC2 123
automation using Terraform 125-129
features 121
hardware appliance 123
Linux KVM 122, 123
Microsoft Hyper-V 122
options 122
use cases 123
VMware ESXi 122


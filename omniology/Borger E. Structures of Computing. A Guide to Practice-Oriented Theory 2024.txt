Egon Börger
Vincenzo Gervasi
Structures 
of Computing
A Guide to Practice-Oriented Theory

Structures of Computing

EgonBörger • Vincenzo Gervasi
Structures of Computing
A Guide to Practice-Oriented Theory

Egon Börger 
Vincenzo Gervasi 
Dipartimento di Informatica 
Dipartimento di Informatica 
University of Pisa 
University of Pisa 
Pisa, Italy 
Pisa, Italy 
ISBN 978-3-031-54357-9 
ISBN 978-3-031-54358-6 (eBook) 
https://doi.org/10.1007/978-3-031-54358-6 
 
© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland 
AG 2024 
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, 
whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, 
reuse of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, 
and transmission or information storage and retrieval, electronic adaptation, computer software, or by 
similar or dissimilar methodology now known or hereafter developed. 
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication 
does not imply, even in the absence of a specific statement, that such names are exempt from the relevant 
protective laws and regulations and therefore free for general use. 
The publisher, the authors, and the editors are safe to assume that the advice and information in this 
book are believed to be true and accurate at the date of publication. Neither the publisher nor the 
authors or the editors give a warranty, expressed or implied, with respect to the material contained herein 
or for any errors or omissions that may have been made. The publisher remains neutral with regard to 
jurisdictional claims in published maps and institutional affiliations. 
 
This Springer imprint is published by the registered company Springer Nature Switzerland AG 
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland 
 
Paper in this product is recyclable. 

Preface: What the Book is About
Io stimo più il trovar un vero, benché di cosa leggiera,
che ’l disputar lungamente delle massime questioni
senza conseguir verità nissuna.
—Galileo Galilei1
The Theme of the Book
”Structures of Computing” aims to explain the behavioural meaning of fun-
damental concepts of computing without referring to any specific computing
device or programming language. By defining these concepts in more general
and at the same time simpler mathematical terms the book is focused on
cooperating agents that process dynamic structures
where the data—stored in abstract memory forming abstract states—are ar-
guments and values of arbitrary functions and relations between objects.
These computational structures are dynamic, subject to change due to rule-
based actions that are performed on the data by algorithm-equipped agents
under various forms of cooperation control, including in particular the reac-
tion to environmental stimuli.
The book uses the conceptual computational framework of Abstract State
Machines (ASMs)—an intuitive form of pseudo-code that operates directly
in abstract states (computational structures) and comes with a precise eas-
ily defined rule-based meaning—but it does not assume knowledge of ASMs.
We explain the small number of basic abstract but operational terms of that
framework which suﬀice to explain the common core of a great multitude of
operational concepts and constructs of computing in a uniform way, facili-
tating their understanding and analysis.
1 I value finding the truth about something, even if inconsequential, more than endlessly
speculating about deep questions without reaching anything true.
See [82, pg. 738, footnote 2]. The context of this quote is well described in [63].
V

VI
Preface: What the Book is About
A Short Survey of the Book
Roughly speaking, to compute means to execute algorithmic processes by a
set of communicating agents each of which executes in a given background
structure an algorithm assigned to it, interacting with other agents in the
given process environment. This calls for an explanation, which we provide
from scratch in Part 1, of the triple data-action-control, the conceptual con-
stituents of interactive processes:
• Data the algorithmic processes operate upon, abstractly described as
structured elements, i.e. objects with associated properties, relations and
functions (Ch. 1).
• Actions (basic algorithmic operations) that affect—create and delete,
access (read) and change (write), receive and send—the data in single
execution steps (Ch. 2).
• Control patterns that determine the combination of single (in adaptable
systems even run-time-modifiable) steps in
single-agent sequential or multi-agent concurrent or mixed runs
(also called ‘executions’ or ‘computations’), where the algorithm-equipped
agents interact with each other and with their environment via commu-
nication or other forms of data sharing (Ch. 3-6).
Part 2 investigates the following three themes:
• Fundamental analysis methods for the investigation of computations
in abstract states at different levels of abstraction/refinement: model in-
spection, validation and verification (Ch. 7).
• Classification of principal computational paradigms, leading from
classes of single-agent sequential algorithms to a variety of classes of
multi-agent processes with asynchronous or mixed (concurrent or recur-
sive) runs (Ch. 8). This extends the characterizations in Ch. 3-6 of self-
modifying reflective algorithms, of synchronous or bulk-synchronous al-
gorithms, of various classes of streaming machines (illustrated by models
for spreadsheets or the TCP/IP computer network protocol), etc.
• Complexity analysis of the power of computations in abstract states
and their intrinsic computational limits (Ch. 9).
We expect the emerging
Model Theory of Structures of Computing
to contribute to a solid foundation for the practice of computing.

Preface: What the Book is About
VII
Target Audience and How to Use the Book
The book is addressed to persons who want to understand the conceptual
foundation of computing. It does not assume any specific programming ex-
perience but only a basic understanding of what are mechanically executable
processes and their descriptions. Think of system-control mechanisms, au-
tomated service procedures, administrative or business processes and the
like which accompany our daily life. To make the book also accessible for
non-experts of the theory of computing we avoid any unnecessary formal-
ism and formulate our definitions as much as possible in natural language,
using common mathematical notation only where needed to avoid ambigui-
ties.2 Examples and exercises (coming with solutions in the Appendix) serve
as comprehension checkpoints; we insert references to the literature for the
reader who is interested in further developments.
The book has been written for self-study but can also be used in teaching,
in particular to accompany undergraduate courses or seminars for students
who want to understand how the many concepts they learn are intimately
linked to each other by a common foundation.
Acknowledgements
We thank the following colleagues who have helped with comments on draft
chapters of this book: Giuseppe Del Castillo, Paolo Dini, Flavio Ferrarotti,
Uwe Glässer, Albert Fleischmann, Fatemeh Movafagh, Giuseppe Prencipe,
Alexander Raschke, Klaus-Dieter Schewe.
Last but not least we thank Ralf Gerstner from Springer for his trust and
patience.
Egon Börger and Vincenzo Gervasi
Pisa, December 2023
2 We recall in Ch. 1 the few basic concepts we use from classical logic.

Contents
Part I Computational Paradigms: Definition
1
Computational Data, Operations, States . . . . . . . . . . . . . . . . . .
7
1.1
Data and Operations on Data . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
1.1.1
Naming Objects and Operations (Syntax of Terms) . . .
9
1.1.2
Generic Notion of States . . . . . . . . . . . . . . . . . . . . . . . . . .
10
1.1.3
Interpretation (Semantics) of Terms in States
. . . . . . .
13
1.2
State Properties (Syntax and Semantics of Statements). . . . . .
14
2
Logical Structure of State-Change Actions . . . . . . . . . . . . . . . .
17
2.1
Logical Memory Structure (Locations and Updates). . . . . . . . .
17
2.1.1
Interaction Type of Functions and Locations . . . . . . . . .
20
2.2
Logical Structure of One-Step Actions. . . . . . . . . . . . . . . . . . . . .
22
2.2.1
Single-Agent Algorithmic Steps: Syntax . . . . . . . . . . . . .
23
2.2.2
Single-Agent Algorithmic Steps: Semantics . . . . . . . . . .
31
2.3
Domain-Specific One-Step Actions . . . . . . . . . . . . . . . . . . . . . . . .
33
2.3.1
Call Step (by Value, by Name) . . . . . . . . . . . . . . . . . . . . .
34
2.3.2
Workspace Increase Operation . . . . . . . . . . . . . . . . . . . . .
35
2.4
Core Actions on Structures (Recap) . . . . . . . . . . . . . . . . . . . . . .
37
3
Control Structures of Single-Process Runs . . . . . . . . . . . . . . . .
39
3.1
Definition of Single-Process Runs . . . . . . . . . . . . . . . . . . . . . . . . .
40
3.1.1
Relative Computability (Background Concept) . . . . . . .
41
3.2
Sequential Imperative Procedural Program Runs . . . . . . . . . . .
42
3.2.1
Imperative Constructs (JavaI) . . . . . . . . . . . . . . . . . . . . .
44
3.2.2
Procedural Constructs (Class Model JavaC) . . . . . . . . .
47
3.3
Input Driven Sequential Runs . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
3.3.1
Computable Functions (Input/Output Machines) . . . . .
53
3.4
Nondeterminism and Interleaving . . . . . . . . . . . . . . . . . . . . . . . . .
60
3.5
Synchronous Parallel Step Control . . . . . . . . . . . . . . . . . . . . . . . .
68
IX

X
Contents
4
Dynamic Sequential Step Control . . . . . . . . . . . . . . . . . . . . . . . . .
73
4.1
Partial Updates of Structured Objects. . . . . . . . . . . . . . . . . . . . .
74
4.2
Intuitive Meaning of Reflectivity . . . . . . . . . . . . . . . . . . . . . . . . .
76
4.3
Reflective PGAs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
78
4.4
Backtracking and Reflection (Prolog) . . . . . . . . . . . . . . . . . . . . .
82
4.4.1
Interpreter for Pure Prolog . . . . . . . . . . . . . . . . . . . . . . . .
82
4.4.2
Interpreter for reflective Prolog . . . . . . . . . . . . . . . . . . . . .
86
4.5
Reflection in LISP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
4.6
Reflective RAM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
4.6.1
Reflectivity at Work (Indirect Addressing) . . . . . . . . . . . 103
4.7
Reflectivity Styles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
5
Control Structures of Multi-Process Runs . . . . . . . . . . . . . . . . 111
5.1
What are Concurrent Runs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
5.2
Communicating Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
5.2.1
Web Browser Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
5.3
Concurrency, Communication, Choice (Occam) . . . . . . . . . . . . . 130
5.3.1
Occam Ground Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
5.3.2
Refining Sync to Async Communication . . . . . . . . . . . . . 133
5.3.3
Optimized Communication . . . . . . . . . . . . . . . . . . . . . . . . 138
5.3.4
Sequential Implementation . . . . . . . . . . . . . . . . . . . . . . . . 140
5.3.5
Time-Slicing Refinement . . . . . . . . . . . . . . . . . . . . . . . . . . 143
5.4
Concurrency, Parameterization, Context Awareness . . . . . . . . . 144
5.4.1
Context Aware Processes (Ambient ASMs) . . . . . . . . . . 145
5.4.2
From Sequential to Concurrent Runs (Java/C#) . . . . . 146
6
Mixed Synchronous/Asynchronous Control Structures . . . . 151
6.1
Bulk Synchronous Parallel (BSP) Control . . . . . . . . . . . . . . . . . 151
6.2
Streaming (Data Flow) Control . . . . . . . . . . . . . . . . . . . . . . . . . . 153
6.2.1
High-Level Streaming Model . . . . . . . . . . . . . . . . . . . . . . . 154
6.2.2
Refined Streaming Variants . . . . . . . . . . . . . . . . . . . . . . . . 156
6.2.3
Artificial Neural Network . . . . . . . . . . . . . . . . . . . . . . . . . . 161
6.2.4
Spreadsheet Streaming Machine . . . . . . . . . . . . . . . . . . . . 162
6.2.5
TCP/IP Streaming in Computer Networks . . . . . . . . . . 169
Part II Computational Paradigms: Analysis
7
Correctness Analysis: Inspection, Validation, Verification . 185
7.1
Ground Models (Inspection) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186
7.2
Model Refinements (Validation and Verification) . . . . . . . . . . . 189
7.3
The Role of Documentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192

Contents
XI
8
Characterization of Process Families . . . . . . . . . . . . . . . . . . . . . . 195
8.1
Single-Agent Sequential Algorithms . . . . . . . . . . . . . . . . . . . . . . . 196
8.1.1
From Turing Machines to PGAs . . . . . . . . . . . . . . . . . . . . 197
8.1.2
The PGA Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200
8.2
Arithmetical Algorithms and Turing’s Thesis. . . . . . . . . . . . . . . 203
8.3
Concurrent Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
8.4
Recursive Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
8.4.1
Recursion Postulates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
8.4.2
Recursive ASMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211
8.4.3
Proof of the Recursive ASM Theorem . . . . . . . . . . . . . . . 214
9
Complexity Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
9.1
Power and Limits of Computation . . . . . . . . . . . . . . . . . . . . . . . . 221
9.1.1
Undecidable Computational Problems (Diagonalization)224
9.1.2
Universality and Recursion . . . . . . . . . . . . . . . . . . . . . . . . 226
9.1.3
Undecidable Logic Problems (Reduction Method) . . . . 231
9.2
Complexity of Computing over Structures . . . . . . . . . . . . . . . . . 236
9.2.1
Universal ASM over Fixed Signature . . . . . . . . . . . . . . . . 239
9.2.2
Look-Compute-Move Algorithms
. . . . . . . . . . . . . . . . . . 250
A
ASM Behaviour in a Nutshell . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
B
Solutions of Selected Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279

List of Figures
2.1
Interaction type of functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
2.2
Control State ASM Diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
2.3
Transaction Control Structure of Ta(M) (M transformed by
Ta). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
2.4
Structured Iterated Control State ASM (while Cond M) . . . . .
31
3.1
Internal and external steps of an iterate that terminates in
success . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
3.2
Choose Control State ASM Diagram . . . . . . . . . . . . . . . . . . . . . . .
65
3.3
Occam alt (G1 S1 . . . Gn Sn) statement diagram . . . . . . . . . . . . .
65
3.4
Successive generations of the CellRuleDiamond automata . .
72
4.1
Tree representation of assignment instructions f (t1, . . . , tn) := t
80
4.2
Tree representation of update instructions
f (t1, . . . , tn) :=action t . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
80
4.3
Tree representation of if cond then M and par M1, . . . , Mn
rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
80
4.4
Tree representation of let x = t in M and import x do M rules 80
4.5
Program representation in LISP . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
4.6
Sharing in the internal representation of a LISP program . . . . .
94
4.7
Architectures of RAM and RASP machine compared. . . . . . . . . 102
5.1
ConcurStep: Component Steps in Concurrent Runs . . . . . . . . 115
5.2
Occam par (S1 . . . Sn) statement diagram . . . . . . . . . . . . . . . . . . 117
5.3
Occam1 alt (G1 S1 . . . Gn Sn) statement diagram . . . . . . . . . . . . 135
6.1
Writing through channels from source to target . . . . . . . . . . . . . 152
6.2
Control State ASM Diagram for agents of a streaming system . 156
6.3
A Deep Neural Network as a streaming system . . . . . . . . . . . . . . 161
6.4
Possible paths in the execution of Compute(r,c) . . . . . . . . . . . . 167
XIII

XIV
List of Figures
6.5
Handling of an input event by the user interface in the
spreadsheet model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
6.6
Handling of incoming myValue messages by a cell in the
spreadsheet model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
6.7
Handling of incoming newText messages by a cell in the
spreadsheet model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
6.8
Overview of the networking model . . . . . . . . . . . . . . . . . . . . . . . . . 171
6.9
Realization of TcpSend(host, payload, buﬀer) in the
networking model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
7.1
The ASM refinement scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
7.2
Process Structure of the ASM Development Method . . . . . . . . . 193
8.1
Recursive call nesting in state S with successor state S′ . . . . . . 214
9.1
Richard’s paradox . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
9.2
Diagonal Machine D for Halt (with Input Machines M) . . . . . . 225
9.3
Fixpoint for Rice’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226
9.4
UniversalAsm(Σ) Component Structure . . . . . . . . . . . . . . . . . . 241
9.5
CopyParseTree Component . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
9.6
ParseTree for PGA Constructs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243
9.7
Parse Tree for Variables and Imports . . . . . . . . . . . . . . . . . . . . . . 243
9.8
Parse Tree Extension PT(M + m) . . . . . . . . . . . . . . . . . . . . . . . . . 244
9.9
ParseTreeEval Component. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
9.10 ApplyUpdates Component . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
9.11 LcmSyncRound simulator for LcmGraphPgm . . . . . . . . . . . . 254
B.1
ConcurStep with Local Computation Segments. . . . . . . . . . . . 266

Symbols and Notations
Notations from Logic
not (¬), and (∧), or (∨)
-- negation, conjunction, disjunction
iff (⇔)
-- logical equivalence: if and only if
forall (∀)
-- universal quantifier
forsome (∃), thereissome (∃)
-- existential quantifier
thereisno (¬∃)
-- negated existential quantifier
ζ
-- interpretation of individual variables, see pg. 13
eval(exp, A, ζ)
-- value of exp in state A with variable assignment ζ
eval(formula, A, ζ)
-- formula interpretation, see pg. 15
exp(x/t)
-- result of replacing each free occurrence of x in exp by t
Set/Multiset/List Notation
Nat
-- set of natural numbers 0, 1, 2, . . .
Integer
-- set of integers 0, 1, −1, 2, −2, . . .
|A|
-- cardinality of A
x ∈A
-- x is an element of A
A \ B = {a ∈A | a ̸∈B}
-- difference set
A ∩B, A ∪B, A × B
-- intersection, union, cross product
A ⊆B
-- A is a subset of B
P(X) = {Y | Y ⊆X}
-- Power set of X
{x ∈X | P(x)}
-- set of all elements of X which satisfy P
charR
-- characteristic function of R
f : A →B
-- function f from domain A to range B
f [a 7→b]
-- denotes f ′ where f = f ′ except for f ′(a) = b
f b
a = f [a 7→b]
-- equivalent notation, common in logic
f (_) = constant
-- abbreviates forall x f (x) = constant
ϵx(P(x))
-- some x which satisfies P (Hilbert’s choice operator)
ιx(P(x))
-- the unique x that satisfies P (Hilbert’s ι operator)
{| |}
-- empty multiset/bag
{|p1, p1, p2, . . . , pn|}
-- a multiset of n + 1 elements
[ ]
-- empty list (or sequence, stream, queue)
XV

XVI
Symbols and Notations
[p1, . . . , pn]
-- list etc. of n elements, in the given order
List(Domain)
-- a list etc. of elements of Domain
head(a)
-- the first element of a list etc. a
tail(a)
-- the list etc. a, except its first element
concatenate([p1, . . . , pn], [q1, . . . , qm]) = [p1, . . . , pn, q1, . . . , qm]
[l | l′] = concatenate(l, l′)
a < b
-- order relation: a comes before b
a > b
-- order relation: a comes after b
ASM Notation
| A |
-- superuniverse of A, see Def. 2
Upd(M, A, env)
-- update set M computes in A with env, see pg. 32
eval(M, A, env)
-- functional notation for Upd(M, A, env)
evalCand(M, A, env, U)
-- nondeterministic relation of eval
Locs(U)
-- set of locations of updates in an update set U
A + U
-- sequel state, result of applying to state A the updates in U
A ⇒M A′
-- M can make a move from state A to A′, see Def. 14
A
n⇒M A′
-- M moves in n steps from state A to A′, see Def. 14
chooseOneOf (Rules)
-- bounded choice choose R ∈Rules do R
M or N
-- chooseOneOf (M, N)
import
-- provides fresh memory, see pg. 36
M seq N
-- atomic sequence operator, see pg. 53
U ⊕V
-- sequential update set merge, see pg. 53
undef
-- see Sect. 1.1.2
∆(M, A) = stepM(A) −A-- difference set (of fresh updates) see pg. 33
∆(M, A) = {∆M(A, A′) | (A, A′) ∈stepM}
-- see pg. 202
A ↓Σ
-- restriction of state A to signature Σ
[M]
-- the function computed by M, see Sect. 9.1.1

Part I
Computational Paradigms: Definition


Introduction and Survey of Part 1
The intuitive meaning of the concept of computing refers to3 the
mechanical execution of precisely and completely described
procedures
by agents which may interact with each other and their environment.
Procedures are also called programs, agents are often viewed as actors that
perform the role defined by the program they execute. Computational pro-
cesses are given by what is called a ‘process description’ and an entity
that performs the ‘process execution’. This distinction simplifies dealing with
multi-agent processes that come with a large variety of (possibly concrete
real-world) process components, executed by corresponding agents. The dis-
tinction allows one to use as agents a variety of execution entities: specific
physical devices, program interpreters, persons (think about a business pro-
cess where an employee has to execute precise instructions that may be for-
mulated in natural language, or think about code inspection where the code
executor is a person), virtual machines, etc. The distinction also serves to
speak about multi-agent systems where different agents may execute simul-
taneously each a different instance of the same program. When dealing with
traditional (single-agent) algorithms we usually suppress the agent.
Instead of mechanical execution also the term algorithmic execution (or
simply execution) is used and process descriptions are also called algorithms
or algorithmic specifications. By a common abuse of language often the (com-
putational)4 process descriptions are simply called processes, thereby hiding
the underlying execution mechanism where its specifics do not matter. Usu-
ally each actor in an algorithmic process can be assumed to execute one al-
gorithm (which may prescribe to perform a finite set of multiple elementary
actions simultaneously—in parallel—or in some order).
3 The technical terms that appear in this introductory survey are explained in the main
text of the book.
4 In this book ‘process’ stands for ‘computational process’, the only processes we are
interested in here.
3

4
Introduction and Survey of Part 1
The execution of an algorithmic process is assumed to depend only on
what is specified precisely (without ambiguity) in the process description.
In other words, the procedure of an algorithmic process must be ‘precise’
and its execution performed under the procedure’s ‘complete’ control (at the
desired level of abstraction). This implies that the elementary actions the
procedure requires an agent to perform are completely determined by the
process description and the current information the agent possesses. This in-
formation (also called data) may be shared by the agent with its environment
(read: other cooperating computing agents). In Ch. 1-6 we investigate these
three conceptual constituents of (interactive)5 algorithms—data, elementary
actions and process control—and illustrate them by characteristic examples.
• Ch. 1 describes the logical structure of the data algorithmic processes
handle. For the sake of generality and simplicity, to refer to computa-
tional data, i.e. objects of computations and their current properties and
relations, we use logical terms (expressions built up by function symbol
applications) and logical combinations of relational statements. In this
way an agent’s state, i.e. its current memory during an execution (read:
its data basis or information status), is represented by a mathematical
structure (more precisely a first-order structure, i.e. a set with finitely
many functions and relations defined on its elements, also called Tarski
structure). Technically this means that states are defined by an inter-
pretation of the structure’s function and relation symbols over a base
set; consequently state changes result from assigning new values to some
arguments of some functions or relations.
• Ch. 2 describes the logical structure of the elementary actions (also
called operations) an algorithmic process performs in a single execution
step on its data. Due to the most general concept of state introduced in
Ch. 1, the elementary actions to create or delete, access (read) or change
(write), receive or send data can all be expressed by (conditional) ab-
stract assignments of the form f (t1, ..., tn) := t which (if the given
condition is true in the current state) change the value of the function
or relation f for the current values of its argument terms t1, ..., tn to the
current value of the term t. In every step an agent may execute simultane-
ously (we also say ‘in parallel’) finitely many such conditioned (also called
‘guarded’) assignments that constitute its program. The single-agent ver-
sion of such Parallel Guarded Assignment processes (PGAs) gives rise to
the classical concept of sequential run as iteration of single-agent steps,
well known from Finite State Machine (FSM) or Turing Machine (TM)
5 For simplicity of wording, in case of a process with only one actor we still consider
the process to be interactive; in fact it will interact at least with the environment, e.g.
by receiving some input at the start of an execution (the case of Turing machines) or
in the simplest case by receiving only a start signal that causes the execution to start.
When dealing with a single-agent process (ag, algorithm) we often suppress mentioning
the executing agent ag and speak only about the algorithm and the effect of its execution
by an agent.

Introduction and Survey of Part 1
5
computations. In analogy to FSMs and TMs the PGAs (and various ex-
tensions introduced below) are called Abstract State Machines (ASMs).
• Ch. 3-6 describe the logical structure of process control, i.e. the way
single steps are combined in runs of multiple interacting agents. These
control structures distinguish different run concepts we illustrate for the
following major software control patterns:
– Sequential deterministic and nondeterministic computations, in-
cluding multiple-agent interleaved or synchronous parallel computa-
tions (Ch. 3). Standard examples are runs of imperative procedural
(Pascal-like,6 Sect. 3.2) or structured programs (Sect. 3.3) and of
cellular automata (Example 12, pg. 69).
– Reflective algorithmic process runs of dynamic programs, i.e. of pro-
grams that can change their definition at run-time (Ch.4). These are
processes that during the execution of their algorithm can change
this algorithm and continue to execute the modified procedure. We
formulate an abstract form of reflective machines and illustrate its
refinements for three major programming paradigms: logical (Pro-
log), functional (Lisp) and imperative (Random Access Machine with
Stored Program RASP).
– Concurrent computations (Ch. 5) with their typical context aware-
ness and state separation. We illustrate them by runs of communi-
cating agents (Sect. 5.2), of web browsers (Sect. 5.2.1), of Occam
processes (Sect. 5.3) and by multithreaded Java runs (Sect. 5.4.2).
– Mixed sequential/concurrent computations, explained in Ch. 6
by two characteristic examples: Bulk Synchronous Parallel algorithms
(Sect. 6.1) and streaming machines with three outstanding instances,
namely neuronal networks, spreadsheets, and the TCP/IP protocol
(Sect. 6.2). Another example with mixed sequential and concurrent
executions are recursive computations we investigate in Sect. 8.4.
These together with various other outstanding classes of Abstract State
Machines of the kind we explain in Ch. 8 pave the way for a general, Turing
machine model independent, ASM-based abstract theory of computation over
structures (outlined in Ch. 9) that has the potential to become relevant for
the practice of computing.
6 The language Pascal has been created last century by Niklaus Wirth to support good
programming practice using structuring of programs and data [165].

Chapter 1
Computational Data, Operations, States
In this chapter we fix the notation we use for first-order logic terms and
formulae and their interpretation. We use a common notation so that most
readers may immediately jump to Ch. 2 and come back here only should a
notational question arise.
Typically, the execution of an algorithmic step implies to perform some
operations (actions) on some objects of computation (that come with their
properties and relations, also called data). These fundamental data man-
agement items can be represented in full generality, of whatever particular
type the data may be, by treating them abstractly, independently of their
particular computational nature:1
• objects as elements of whatever ‘universe of discourse’ (a set),2
• operations as functions defined over the universe of discourse,
• properties and relations as subsets of the universe of discourse.
Due to algorithmic actions some properties, relations and functions are sub-
ject to change so that they are called dynamic, in contrast to static ones that
never change. One therefore has to provide syntactic names to denote the
items via some name interpretation. This interpretation is subject to change
when an algorithmic action is performed on the involved item.
For naming we adopt the usual syntactic terms from classical predicate
(first-order) logic. We introduce them in Sect. 1.1 together with their inter-
1 The foundational character of the triple (algorithm, action, object) can be seen from Al-
bert Fleischmann’s epistemological observation that the Subject-Predicate-Object struc-
ture of simple statements ‘Subj PerformsActionOn Obj’ in indo-european languages is
closely related to the structure of elementary steps of (single-agent) algorithmic pro-
cesses, namely AlgorithmicAgent-PerformsActionOn-Objects; it shows the algorithm-
executing agents as the subjects of computational steps, a view that appears in nuce
already in [78, Sect. 19.7] and explicitly in [79, Sect. 2.6]. The Subject-Oriented Busi-
ness Process Modeling approach exploits this analogy to smoothly integrate persons as
subjects (read: executing agents) of digital business-processing steps.
2 In the area of conceptual modeling the wording ‘entities’ is used for objects, see https:
//conceptualmodeling.org/
7
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
E. Börger, V. Gervasi, Structures of Computing, 
https://doi.org/10.1007/978-3-031-54358-6_1 
 
 
 
 
 

8
1 Computational Data, Operations, States
pretations (also called their semantics), describing how the names denote
computational items that constitute what is called the state of a computa-
tion. To specify conditions under which an algorithmic action is requested
to be executed in a state we use mainly standard first-order formulae which
for the sake of completeness and to explain the used notation are defined in
Sect. 1.2.
1.1 Data and Operations on Data
Functions are the principal operations dealt with in computing. Functions
f in the mathematical understanding come mainly with a fixed length of
arguments, called their arity, and associate with every argument (a1, . . . , an)
(of a given set A1 ×. . . An of arguments) a unique element b (of a given set B
of values); b is called the value of the function for the argument (a1, . . . , an).
The standard notation is f : A1 × . . . An →B. Functions can be partial,
meaning that for some arguments (a1, . . . , an) there may be no value defined;
this is often expressed by writing f (a1, . . . , an) = undef . A function that is
not partial is called total. For a partial function f its domain is defined as
the set of arguments (a1, . . . , an) with value f (a1, . . . , an) ̸= undef , its range
as the set of its values.
In mathematics functions usually come with a fixed definition which is
independent of any computational action, reason for which they are called
static. In computing the argument/value association of a function may change
for some arguments, due to computational actions that affect the definition
of the function values for the involved arguments. This is why such functions
are called dynamic.
Notational convention for relations. A relation R (also called predi-
cate) is treated in this book as a function with range true, false, undef . You
may think of R(a1, . . . , an) = undef as expressing that the type of the ar-
guments is not correct. A relation R is often written as a characteristic
function charR to mark that it is treated as a partial function. A relation
can be seen also as a set, namely the set of all elements (a1, . . . , an) where
R(a1, . . . , an) = true. Other standard notations for R(a1, . . . , an) = true are
a relational ‘statement’ R(x1, . . . , xn) or R(x1, . . . , xn) is true or R(x1, . . . , xn)
holds. The same holds for properties P which we view as relations of arity 1
or sets {a | P(a) = true}.
To simplify the notation we also use the special case of a function with zero
arguments as denoting some element, an object that is associated with this
function. In logic 0-ary functions are called constants, in contrast to logical
variables which are used as auxiliary temporary names to denote any not
furthermore specified elements of some set. But since also 0-ary functions may
change their value due to computational actions we stick to the wording 0-ary

1.1 Data and Operations on Data
9
function. We may omit the parentheses for 0-ary functions, writing c instead
of c(). A dynamic 0-ary function corresponds to a variable of programming.
We now define the syntax (Sect. 1.1.1) and semantics of terms (Sect. 1.1.3)
in computational states (Sect. 1.1.2), thereby specifying the logical structure
we use to describe data and operations on data.
1.1.1 Naming Objects and Operations (Syntax of
Terms)
With the preceding stipulations one can describe any algorithmic action in-
volving any computational item in a uniform way by using terms to name
computational objects (read: data) on which some operations are performed
(by an active entity). Terms are built up from 0-ary function symbols by
appropriate function application (Def. 1). To provide the possibility to tem-
porarily name some usually arbitrarily chosen but fixed element of a set we
include also individual variables as syntactical names for objects. The seman-
tical interpretation of terms and individual variables assigns values to them,
thus constituting what is called an algorithmic state (Def. 2). The interpreta-
tion of individual variables (Def. 3) appears there as a variable environment
in which the computation (also called evaluation) of functional terms in the
state takes place (Def. 4).
Definition 1. (Terms over a Signature) A finite set of function symbols
is called a signature or vocabulary. Since we deal with relations as func-
tions with truth values true, false, undef we usually assume without further
mention that three special 0-ary function symbols (which we again write
as true, false, undef ) are present in every signature and interpreted in the
standard way.
We write f , g, h for function symbols, possibly indexed f1, f2, . . ., and indi-
cate their arity n by f n or f (n) only where necessary to avoid any ambiguity;
usually the arity is clear from the context. Sometimes we write c, c1, . . . for
0-ary function symbols. For a frugal notation we consider f n and f m with
n ̸= m as different function names, the same for versions of f with different
argument types. We write Σ (possibly indexed) for signatures.
We write x, y, z (possibly indexed) for individual variables which are special
identifiers, used as auxiliary temporary names for arbitrary not furthermore
specified elements of a set.
For any signature Σ the terms (over Σ) are constructed starting with
individual variables x and 0-ary function symbols f 0 ∈Σ and successively
applying any function symbol g ∈Σ of positive arity n to any already con-
structed terms t1, . . . , tn. Formally the steps of this inductive definition read
as follows:

10
1 Computational Data, Operations, States
• Every individual variable x and 0-ary function symbol g0 ∈Σ is a term
(over Σ).
• If t1, . . . , tn are terms (over Σ) and f is an n-ary function symbol (in Σ)
for n > 0, then f (t1, . . . , tn) is a term (over Σ).
A term which does not contain any individual variable is called a ground term
or closed. We write s, t (possibly indexed) for terms. In computer science they
are also called expressions, a name which in logic usually refers to formulae
as defined below.
Example 1 (Mealy Automaton Signature). As our first example we consider
Mealy automata, a frequently used type of machines introduced in [127]. A
Mealy automaton consists of a 0-ary function in where the input is read, a 0-
ary function out where the output is written, and a control unit ctl (denoting
what we call control states) by which the behaviour (read: the execution
of specific actions in single steps) of the automaton is managed following a
program pgm. The program determines how the automaton, reading in its
current control state ctl some input, produces some output and proceeds to
its next control state (for further input reading). The three 0-ary function
symbols in, out, ctl belong to the signature of a Mealy automaton and denote
its architectural components. Their precise meaning (as memory locations)
and some more signature elements will become visible when Mealy automaton
states and actions are described (see below Example 2, pg. 12).
1.1.2 Generic Notion of States
Definition 2. (States over a Signature) For a given signature Σ a state
A (over Σ) is defined by a non-empty set A and an interpretation of each
function symbol f n ∈Σ by a total function f A : An →A which is also called
the denotation of f . We assume Σ to contain three elements true, false,
and undef whose denotations are assumed to be fixed (i.e. static) pairwise
different elements and different from all other elements in A. We assume
Boolean functions and equality to be interpreted in the usual way (see Def. 6,
pg. 15).
A is called the superuniverse or base set of the state A and denoted |A|.
We call the elements of the superuniverse also the elements of the state. The
convention to treat properties and relations by their characteristic functions
makes them interpreted in a state as subsets of the superuniverse. Such sub-
sets are also called subuniverses or simply universes.
Comprehensiveness of states. Structures are the most general means
mathematical language offers to represent states of affairs of the (part of
interest of the) world. As the result of interpretations of given predicate,
relation and function symbols they are known in logic as Tarski structures

1.1 Data and Operations on Data
11
[161, 11] or simply structures and in universal algebra as algebraic structures
or simply algebras (with static functions). In theoretical computer science
they are known as (axiomatically defined) abstract data types. Structures as
states support also the view of databases as relational structures where every
argument for which the relation holds corresponds to a row in the database
table for the relation. Allowing some functions in Tarski structures to be dy-
namic as we exploit in this book provides for the practice of computing a most
general but conceptually simple form of computational states. The elements
of the superuniverse can be any kind of dynamic structured objects—think
about sets, lists, trees, graphs, but also programs (that may even be trans-
formed at runtime), networks of communicating agents, etc.—which can be
transformed with the help of corresponding functions that operate on those
structures, at whatever desired level of abstraction. In addition, as is further
explained in Ch. 2 and exploited throughout in this book:
• functions represent an abstract easy-to-grasp form of memory: every ar-
gument of f appears as memory location for f whose content is the value
of f for that argument, so that
• any algorithmic action can be described as changing the value of some
memory locations, namely the interpretation of those terms that are char-
acteristic for the algorithmic action (read: are at the level of abstraction
of that action).
An intuitive representation of functions is by function tables which support
the two main views of memory: the abstract view as given by tables and the
concrete view as given by single locations (read: table entries), a feature that
can be useful for program documentation (see [134, 32]).
A notational convention for 0-ary functions. By Definition 2 dy-
namic 0-ary functions c play the role of programming variables (or registers)
since the denoted element cA ∈A (or register content) may change from state
to state due to some algorithmic action. If c is used as a constant, i.e. as a
static function that denotes the same element in every state under consider-
ation, say a natural number cA = 7, then we sometimes identify the function
symbol c with its denotation cA = 7 and simply write 7 instead of c. This
holds in particular for the static constants true, false and undef . We extend
this convention by writing f instead of f A when it is clear from the context
that not the 0-ary function symbol f , but its interpretation f A is meant.
A note on undef . By undef we denote an undetermined object. It plays
the role of the default value of the superuniverse. We use undef to handle
partial functions as follows. For logical reasons not explained here we require
by Definition 2 that the interpretation of a function symbol f in a state
associates a value with each element of the state so that it denotes a total
function. But we want to be able to deal also with partial functions that
appear frequently in computing. Therefore we permit functions to have undef
as value and treat the domain of functions as the set of all and only those
arguments (a1, . . . , an) ∈|A|n for which the function has a ‘defined’ value

12
1 Computational Data, Operations, States
f A(a1, . . . , an) ̸= undef . As a consequence, by a slight abuse of language we
call a function a partial function also in case it has the value undef for some
argument. Since we treat relations by their characteristic functions, relations
too can be partial so that their range is the set {true, false, undef }.
Example 2 (Mealy Automaton States). We continue the previous example of
a Mealy automaton (see Example 1 above) by modeling a transformation of
an input sequence (read from a tape) into an output sequence (written to the
same tape). To this end, we modify the signature by refining the functions in
and out to tape(head), where a 0-ary function head represents the position
on the tape from which the automaton reads the input symbol and to which
it writes the output symbol.3
More precisely stated: a tape is (interpreted as) a (possibly infinite) se-
quence of squares each of which can bear a letter, constrained by the condi-
tion that only finitely many squares are non-empty, i.e. bear a letter that is
different from a special value that we identify with undef (traditionally called
a ‘blank’). The segment of a tape that contains all non-empty squares repre-
sents a word Mealy automata are interested in, i.e. a finite sequence a1 . . . an
of elements (called ‘letters’) of a non-empty finite set (called ‘alphabet’). Ini-
tially all non-empty squares contain a symbol from some InAlphabet. As the
transformation progresses, such input symbols are replaced by symbols from
some OutAlphabet (which may or may not coincide with InAlphabet).
head : Square
-- 0-ary function with values in Square
tape : Square →InAlphabet ∪OutAlphabet
At any time, the control unit is in a control state held by a 0-ary function ctl;
the possible control states are the elements of a non-empty finite set CtlState:
ctl : CtlState
-- a 0-ary function with values in Ctl
The Mealy automaton program pgm determines the reaction of the machine
when in its current control state ctl = i it reads an input letter tape(head) =
a: it writes a letter b into tape(head), moves the head (namely to the next
square to the right of the currently visited square), and switches to its next
control state j. For simplicity of exposition of the moves of the read-and-write
head we choose Square to be (an initial segment of) the set Nat of natural
numbers so that ‘the next square to the right of ctl’ is ctl + 1.
This means that pgm is furthermore specified by the following two func-
tions (plus the successor function +1 of Nat to determine the next square to
the right of head):4
3 In the literature Mealy automata come with separate input and output tapes. To show
below in an explicit manner the subtle relation between Mealy automata and Turing
machines we use one tape from where input is read and to which output is written.
4 Alternatively, one can specify pgm by one single function pgm : CtlState×InAlphabet →
OutAlphabet × CtlState × Square. Here, Square is a static successor function that always
yields the next Square on the right. By omitting Square, we obtain the usual notation
of pgm as a set of instructions of form (i, a, b, j) where the assumed right-move of the
head remains implicit.

1.1 Data and Operations on Data
13
write : CtlState × InAlphabet →OutAlphabet
nextCtl : CtlState × InAlphabet →CtlState
Thus the superuniverse is the union of Nat, the alphabets InAlphabet,
OutAlphabet and the CtlState (plus the always assumed 0-ary fixed functions
true, false and undef ).
The example shows that the further specification of Mealy automata im-
poses certain constraints on the interpretation of the Mealy automaton sig-
nature elements introduced in Example 1 (pg. 10), for example turning the
monitored function in and the output function out into one refined function
tape. It also adds new signature elements, like for the static program functions
write, nextCtl (usually denoted λ, δ) and the successor function +1 together
with head. In Example 4 (pg. 26) we will conclude the behavioural definition
of Mealy automata by specifying also the logical structure of the elementary
actions Mealy automata perform in every single step of their computations.
1.1.3 Interpretation (Semantics) of Terms in States
One can compute in every state for each syntactical term a value if an in-
terpretation of the individual variables that occur in the term is given. Such
an interpretation consists of an assignment of elements of the superuniverse
to finitely many individual variables; such an interpretation is also called
(individual variable) environment of the state.
Definition 3 (Variable environment). Let A be a state. A variable envi-
ronment for A is a function ζ which binds each of a finite number of individual
variables to name (‘denote’) an element of the superuniverse of A. We write
ζ[x 7→a] for the variable environment that coincides with ζ except that it
binds x to denote the element a. range(ζ) is defined as the set of all elements
that occur in the bindings of ζ. Note that the function symbol ζ is not in
the signature of an ASM but appears in the ASM framework with the role to
keep track of variable bindings where defining the meaning of terms, formulae
and machines.
In every state A one can compute for every term t a value if a variable
environment ζ is given that has a binding for each individual variable that
occurs in the term. This term evaluation follows the inductive definition of
terms t, evaluating individual variables x as defined by the variable environ-
ment and 0-ary functions c as defined by the state. We skip mentioning ζ
where the considered terms are without free variables.
Definition 4 (Interpretation of terms in states). Let A be a state and
t a term of Σ, ζ a variable environment for A that defines for each individual
variable of t a value.

14
1 Computational Data, Operations, States
• eval(x, A, ζ) = ζ(x) and eval(c, A, ζ) = cA
• eval(f (t1, . . . , tn), A, ζ) = f A(eval(t1, A, ζ), . . . , eval(tn, A, ζ))
We say that in state A with variable environment ζ the term t denotes
(‘names’) the value eval(t, A, ζ) which is also called the meaning of t or its
semantics. By definition only the subfunction of ζ that binds to every indi-
vidual variable of t a value influences the interpretation of t, a property we
formulate for future reference as a lemma. So the evaluation of the variable-
free ground terms t depends only on the interpretation of the state signature
and one can write simply eval(t, A).
Lemma 1 (Coincidence Lemma for Term Evaluation).
If ζ and η
are two variable environments that coincide over t, i.e. satisfy ζ(x) = η(x)
for all variables x of t, then also the evaluations of t with those variable
environments coincide, i.e. yield the same result eval(t, A, ζ) = eval(t, A, η).
1.2 State Properties (Syntax and Semantics of
Statements)
We definitely want to use any standard mathematical expressions that help
to describe the analysis of the behaviour of a computational system, but to
formulate the conditions by which typically the execution of any algorithmic
action in a state is triggered we need only formulae of predicate logic which
we review in this section. As with terms one has to define the syntax and the
semantics of formulae.
Since we treat properties and relations by their characteristic functions
the only atomic formulae we need to consider are equations between terms.
Starting with them arbitrary formulae are syntactically constructed using
boolean connectives and quantifiers.
Definition 5 (Formula). Let Σ be a signature. The formulae over Σ are
constructed inductively:
• Every equation s = t between terms s and t over Σ is a formula, also
called an atomic formula.
• Every Boolean composition of formulae φ, ψ is a formula. Boolean com-
positions are negations not (φ), conjunctions (φ and ψ), disjunctions
(φ or ψ), equivalences (φ iff ψ), etc.
• The universal and existential quantifications forall x (φ) and forsome
x (φ) are formulas if φ is a formula and x an individual variable.
For writing formulae we use the usual notational conventions, in particular
when omitting parentheses for better readability. s ̸= t stands for not (s =
t). As usual an individual variable x is called bound by the quantifier in a
quantification forall x (φ) or forsome x (φ) and its scope is the formula φ.

1.2 State Properties (Syntax and Semantics of Statements)
15
An occurrence of an individual variable x is free in a formula if it is not
in the scope of a quantifier forall x or forsome x. By φ(x/t) or if x is
clear from context φ(t) (substitution) we denote the result of replacing all
free occurrences of the individual variable x in φ by the term t; individual
variables that are bound in φ but also occur in t are assumed to be renamed
by fresh individual variables.
The interpretation of terms in a state with respect to a variable environ-
ment can be extended to formulae, resulting in a truth value that indicates
whether what the formula asserts is true in the given state or false. The names
of the logical connectives and quantifiers refer to their usual meaning that is
used for the evaluation of formulae where the equality sign is interpreted as
identity.
Definition 6 (Interpretation of Formulae).
Let A be a state and φ a
formula of Σ and ζ a variable environment that defines a value for every
individual variable with some free occurence in φ. By induction on φ we
define:
eval(s = t, A, ζ) =
 true if eval(s, A, ζ) = eval(t, A, ζ)
false
otherwise
eval(not φ, A, ζ) =
true if eval(φ, A, ζ) = false
false
otherwise
eval(φ and ψ, A, ζ) =
 true if eval(φ, A, ζ) = eval(ψ, A, ζ) = true
false
otherwise
eval(forall x φ, A, ζ) =



true if eval(φ, A, ζ[x 7→a]) = true
for everya ∈|A|
false
otherwise
etc. as usual
Note that for conceptual economy we use the same name eval for the
two functions that evaluate terms respectively formulae and thus are dis-
tinguished by the type of their arguments. Remember that relations and
predicates are treated as functions so that the only atomic formulae we con-
sider are equations. By the preceding definition in every state every formula
built up from equations is either true or false. If in a state A a formula φ
evaluates to true for all variable environments ζ (for φ), this state A is called
a model of φ. As for terms also the interpretation of a formula φ depends
only on a subset of any variable environment ζ, namely the values of ζ for the
individual variables that have some free occurence in φ. For further reference
we formulate this property as a lemma.
Lemma 2 (Coincidence Lemma for Formula Evaluation). If ζ and η
are two variable environments for φ that coincide over φ, i.e. satisfy ζ(x) =
η(x) for all individual variables x with some free occurence in φ, then also
the evaluations of φ with those variable environments coincide, i.e. yield the
same result eval(φ, A, ζ) = eval(φ, A, η).

16
1 Computational Data, Operations, States
For further reference we also formulate here as a lemma that the meaning
of any formula φ where an individual variable x has been substituted by a
term t can also be obtained by changing the variable environment for the
evaluation of the formula to assign the meaning of t to x.
Lemma 3 (Substitution Lemma). Formula evaluation treats variable
substitution by terms as variable environment change. To express this more
precisely, let a = eval(t, A, ζ). Then
eval(φ(x/t), A, ζ) = eval(φ, A, ζ[x 7→a]).
Given the above lemma, we will also write φ[x 7→t] for φ(x/t).

Chapter 2
Logical Structure of State-Change Actions
In this chapter we define an abstract form of elementary state-change actions
that can be instantiated to describe in a direct, encoding-free manner any
concrete atomic computational step, at whatever level of abstraction. In
Sect. 2.1 we define the underlying logical memory structure of states,
namely by sets of locations (i.e. function table entries), and classify the inter-
action type of locations and functions. In Sect. 2.2 we define a generic form of
elementary state-change by updates of locations via abstract assignments.
Iteration of such steps yields a general concept of single-agent computations
we investigate further in Ch. 3. In Sect. 2.3 we provide some simple exam-
ples how to tailor atomic computational steps to specific abstraction goals,
including the important action of increasing the workspace.1
2.1 Logical Memory Structure (Locations and Updates)
We consider computational states as Tarski structures, i.e. finite sets of func-
tion tables (Def. 2, pg. 10), an abstract form of memory: every function table
entry represents a location (Def. 7, pg. 18) where the value of the function
for that entry is stored. We explain in this section how every state change
(Def. 10), at whatever level of abstraction2, can be uniformly described by up-
dates (Def. 8) whose application changes the values of the involved locations
(Def. 9).
The methodological role of locations and updates. The two crucial
contributions of the concept of abstract memory locations are abstraction and
localization. The content of locations are data of whatever desired type and
can be changed directly (i.e. without any encoding) by operations of what-
1 The reader who knows the concept of ASMs may proceed directly to Sect. 2.3.2 where
we explain a constructive Workspace Increase Operation we will use in later chapters of
the book.
2 This will be further expanded upon in Sect. 4.1.
17
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
E. Börger, V. Gervasi, Structures of Computing, 
https://doi.org/10.1007/978-3-031-54358-6_2 
 
 
 
 
 

18
2 Logical Structure of State-Change Actions
ever appropriate type; this data and operation abstraction that is provided
by the definition of locations strips away any implementation details that are
irrelevant at the involved level of abstraction. By localization one can iden-
tify the locations at the heart of the algorithmic state where it all happens,
stripping away any parts of a state that are irrelevant for a computational
step; locations offer to define and analyze any algorithmic step focussing on
those and only those data and operations that are relevant for the intended
effect of the step. The concept of abstract memory locations offers a tool to
realize the major concern of conceptual modeling:3
provide the right set of modeling constructs at the right level of abstraction.
The important issue here is not whether a description is declarative (i.e.
equational or axiomatic) or operational4 but at which level of abstraction
it is given, using updates that directly represent the state change action,
concentrating on the computational aspects of data and updates that are
relevant for a mathematical analysis of dynamic behaviour that changes a
state.
Definition 7 (Locations). A location of a state A is a pair (f , (a1, . . . , an)),
where f is an n-ary function symbol and a1, . . . , an are elements of the supe-
runiverse of A. The value f A(a1, . . . , an) is called the content of the location
in A. The elements of the set {a1, . . . , an} are called the elements or the ar-
guments of the location. Every location (f , (a1, . . . , an)) is called a location
for f or an f -location. Locs(A) denotes the set of locations of A. For 0-ary
dynamic functions c we usually write c for the location (c, ()); usually it is
clear from the context whether c is meant to denote the location (c, ()) or
the value cA of the function in a given state A.
This definition offers to view an entire state A as a function that maps
every location of A to its content so that one can write A(l) for the content
of the location l in A. Consequently a state change can be described in a
simple and uniform way, whatever items are handled by the investigated
computations, by modifying this function A(l) (and thereby the functions of
A) via updates of the content of some of its locations l to new values.
Definition 8 (Updates). An update for a state A is a pair (l, v), where l
is a location of A and v is an element of the superuniverse of A. We call
the update trivial if its value v does not change A, i.e. if v is identical to
the content of l in A. The elements of an update ((f , (a1, . . . , an)), v) are the
elements of the set {a1, . . . , an, v}. It has become common usage to call a
set of updates an update set. If an update set contains some updates with
different values for a same location we say that these updates clash and that
3 Quoted from https://conceptualmodeling.org
4 Although we share the experience that the “...idea of describing behavior in terms
of mathematical equations works well...where the behavior is fairly simple, it almost
inevitably fails whenever the behavior is more complex”[167, pg.3].

2.1 Logical Memory Structure (Locations and Updates)
19
the set is inconsistent (because it does not determine in an unambiguous way
how to change A); otherwise it is called consistent. By Locs(U) for an update
set U we denote the set of locations in U.
If an update set U is consistent, it describes in a precise way how to change
the given state A. The application of U (we also say to fire U) results in a
unique new state we denote by A + U.
In the new state, the interpretations of those and only those function
symbols which appear in Locs(U) may be changed.
Definition 9 (Application of updates). The result of applying a consis-
tent update set U to a state A is a new state A + U with the same superuni-
verse as A such that for every location l of A
(A + U)(l) =
(
v
if (l, v) ∈U
A(l)
otherwise
A + U is called the sequel (or successor state or next state) of A with respect
to U. If U is inconsistent A + U is not defined.
The generality of the concept of state change as application of update
sets results also from the following observation that justifies the concept of
difference of two states.
Definition 10 (Difference of states).
Let A and B be two states with
the same superuniverse and signature. Then B can be obtained from A by
replacing all A-values A(l) that differ from the B-values B(l) by those B-
values B(l), in other words by applying to A the set of all updates with value
B(l) that differs from the value A(l). This update set {(l, B(l)) | B(l) ̸= A(l)}
is called the difference of B and A and written as B−A. By definition B−A
is consistent and A + (B −A) = B holds.
Remark on isomorphisms. The definition of updates of memory loca-
tions captures the logical structure of state change actions and can be used
to uniformly describe state changes at whatever level of abstraction, indepen-
dently of the specific type of functions and location contents in a state. Fur-
thermore, in a state as defined in Def. 2 (pg. 10) the internal representation of
the elements of its superuniverse should play no role, only the functions than
can be applied to the elements should count. This can be technically expressed
by the algebraic concept of isomorphism. Two states A and B with the same
signature are called isomorphic if their superuniverses can be mapped to each
other one-to-one (say by a function α) in such a way that the interpretations
of the functions agree on corresponding elements. This means that for every
location l = (f , (a1, . . . , an)) of A its content A(l) is mapped to the content
that B assigns to the corresponding location (f , (b1, . . . , bn)) (where ai is
mapped by α to bi). This homomorphy condition—first mapping and then
interpreting yields the same result as first interpreting and then mapping—is
expressed and visualized by the following commutative diagram:

20
2 Logical Structure of State-Change Actions
(f , args)
α
=⇒(f , α(args))
forall l ∈Locs(A) holds
⇓A
⇓B
α(A(l)) = B(α(l))
f A(args)
α
=⇒
f B(args)
Homomorphisms are extended to locations loc = (f , (a1, . . . , an)) and update
sets U by α(loc) = (f , (α(a1), . . . , α(an))) and α(U) = {(α(l), α(v)) | (l, v) ∈
U}.
It is reasonable to identify isomorphic states A and B because every iso-
morphic mapping between them maps every consistent update set U of A to
a consistent update set U ′ of B and establishes also an isomorphism between
the corresponding successor states A + U and B + U ′.
Exercise 1 (Homomorphic Term Value Preservation). Show by induc-
tion on terms that every homomorphism α from a state A to a state B pre-
serves the value of terms t in the sense that α(eval(t, A, ζ)) = eval(t, B, α◦ζ)
holds, where α ◦ζ denotes the binding of variables x defined in ζ to α(ζ(x)).
For ground terms this implies α(eval(t, A)) = eval(t, B).
Exercise 2 (Isomorphic State Updates). Check your understanding of
update sets and isomorphic states by proving that every isomorphism α be-
tween two states A, B is also an isomophism between the updated states
A + U, B + α(U) for every consistent update set U for A. For a proof see
[58, Lemma 2.4.2].
2.1.1 Interaction Type of Functions and Locations
A practical approach to system analysis and design has to provide means
to reflect the important principles of separation of concerns, modulariza-
tion, information hiding, abstraction and stepwise refinement. The view of
states as sets of functions or memory locations allows us to analyze the dif-
ferent roles played by the functions which appear during the evaluation of
terms f (t1, . . . , tn) in a given state. Some function may be specifiable in-
dependently of the considered computations, e.g. background functions the
given algorithm may use in some step but never modify, or input functions
the algorithm can only read. Some functions may be accessible for read/write
actions only of the considered algorithm and nowhere else in its environment.
In this section we define the interaction type of functions and locations which
classifies their computational role. This interaction type is defined with re-
spect to an algorithm or program M an agent executes and is illustrated by
Fig. 2.1.
• A static function is defined independently of any computation (of M) so
that its argument/value relation does not depend on any states (of M)
and therefore never changes during any run (of M). Examples are math-
ematical functions like addition, multiplication, etc. The definition and

2.1 Logical Memory Structure (Locations and Updates)
21
analysis of static functions can be separated from the description of the
system dynamics and may exploit different techniques, depending on the
degree of information-hiding the specifier wants to realize, e.g. explicit or
recursive definitions, abstract specifications using axiomatic constraints,
definitions by a separate program, etc.
• A dynamic function may change its argument/value relation as a con-
sequence of some computational action (of M or of any other program-
equipped agent in the considered computational context). In other words
the function’s argument/value pairs may depend on the computational
states (of M). Dynamic functions generalize array variables. For dynamic
0-ary functions c we often identify notationally their name c with their
interpretation cA in a state A if the context makes clear what is meant.
• A controlled function of M is a dynamic function that can be accessed in
the considered computational context where M is executed only for read
and function-changing write actions of M. Controlled functions represent
the part of the states that is internally and exclusively controlled by M.
• A monitored function for M is a dynamic function than can be read
but not changed by actions of M and is directly updatable only by the
computational environment of M (e.g. an input device or other program
executing agents).
• An output function (also called out function) for M is a dynamic function
that can be changed but not read by actions of M and is monitored by the
computational environment of M (e.g. an output device or other program
executing agents).
Fig. 2.1 Interaction type of functions.
© 2003 Springer-Verlag GmbH Berlin Heidelberg, reprinted with permission
function / relation / location
basic
static
dynamic
in
(monitored)
controlled
shared
out
derived

22
2 Logical Structure of State-Change Actions
• A shared function is a dynamic function that can be read and changed by
multiple executing agents. To guarantee the consistency of simultaneous
function changes by different agents appropriate interaction (communi-
cation) protocols are needed. This helps if one wants to separate internal
computation concerns from communication concerns.
This classification of functions extends naturally to the corresponding lo-
cations and function symbols. We call functions external for M if they are
either static or monitored for M. We call shared and input/output functions
also interaction functions.
For pragmatic reasons we also distinguish basic and derived functions.
Derived functions are dynamic but come with a fixed definition—e.g. by an
equation, an abstract specification, a separate algorithm—in terms of other
static and/or dynamic functions (read: with an implicit state parameter)
so that they are not monitored and cannot be changed but can be read
by the considered computational agent. Derived functions are often used to
separately describe the behaviour of some system component. Often they are
part of what is called the background of sets and functions that can be used
to perform state change actions but cannot be modified themselves.
Example 3 (Interaction Type of Mealy Automaton Functions). In Example 2
(pg. 12) write, nextCtl, +1, CtlState, Nat, InAlphabet, OutAlphabet are static;
ctl and head are controlled. Furthermore, the set InAlphabet∗of input words
is derived (in fact, it is derived from InAlphabet, and since the latter is static,
InAlphabet∗is also static). The tape function is shared with the environment
that generates the input (read: the initial tape with blank squares except
those with letters from InAlphat) and receives the output (the non-blank
part of the tape when the automaton has read all its input.
2.2 Logical Structure of One-Step Actions
We begin the section with a naming convention.
Definition 11. We call an algorithmic process a single-agent process if its
execution involves only one agent (not counting the environment); otherwise
we speak of multi-agent (interacting) processes we analyze in Ch. 5. The
program of a single-agent process is called a single-agent program.
In this section we describe the syntax and the semantics of three ‘elemen-
tary state-change actions’ agents execute in their single steps.5 The definition
comes in a computer and programming language independent abstract form.
5 We say ‘execution by a single agent’ to distinguish different instances of the same
process as executed by different agents with the same process but possibly different
data.

2.2 Logical Structure of One-Step Actions
23
It includes the beneficial simultaneous (synchronous parallel) execution of
atomic-step actions that are independent of each other. This avoids any un-
necessary sequentialization of single-step actions in single-agent programs.
As a consequence the scheme we define in Def. 12 mentions no sequential
control construct.
2.2.1 Single-Agent Algorithmic Steps: Syntax
The view of states as sets of functions or memory locations implies that
every single computation step that changes a given state turns out to consist
of changing some functions of the state, in practice by changing in one blow
the value of some (in practice finitely many) of its memory locations. The
standard semantics defined below for an
assignment instruction: f (t1, . . . , tn) := t
with any (unless otherwise stated ground) terms ti, t directly describes the ef-
fect of such updates of memory locations (f , (eval(t1, A, ζ), . . . , eval(tn, A, ζ))
by eval(t, A, ζ) in states A with given variable environment ζ. It is the ab-
stract nature of the terms in an assignment that permits to work directly,
without encoding, on arbitrary locations and their contents. Assignment in-
structions are usually simply called assignments; they represent the elemen-
tary state-changing components of structured algorithmic processes that are
executed by a single agent.
A basic algorithmic control structure allows one to restrict the execution of
an algorithmic process P to states which satisfy a condition (called a ‘guard’
of P). The resulting single-agent process (or instruction or program) is called
conditional process: if cond then P
(or guarded process, instruction, program) where cond is a logical (unless
otherwise stated quantifier-free) formula (Def. 5, pg. 14).
Often various subprocesses Pi of an algorithmic process are independent
of each other so that they can be executed in any order without changing
the overall behaviour. In such cases it is often convenient to think of these
processes as executed simultaneously, in parallel, in one step, by one agent.6
The resulting process is called a
parallel process: par (P1, . . . , Pn)
or parallel instruction or program.
6 Note that Turing in his epochal paper [162] considered some possible parallelism in a
step (called ‘move’) of his machines from one to the next state (called ‘complete config-
uration’), namely printing, shifting the scanned square to the right/left, and changing
the control state (called ‘m-configuration’).

24
2 Logical Structure of State-Change Actions
Definition 12 (Parallel Guarded Assignments).
We define Parallel
Guarded Assignment rules (abbreviated PGA rules or simply PGAs) by in-
duction:
• Every assignment instruction is a PGA rule.
• If P is a PGA rule and cond is a quantifier free formula, then also if
cond then P is a PGA rule.
• If P1, . . . , Pn are PGA rules, then also par (P1, . . . , Pn) is a PGA rule.
By PGA we denote the class of PGA programs.
Each PGA is a restricted form of what in the literature is called an Ab-
stract State Machine (ASM) [58, 50]. We use here the acronym PGA to draw
the attention on the three underlying computational core concepts and to
distinguish PGAs from the more general ASMs. Abstract State Machines
extend PGAs by further computational constructs that are widely used in
system design and analysis and exploited in the following chapters of this
book. The particular role of PGAs results from a practical experience that
eventually became also supported by a theorem:
• A great variety of experiments with well-known, real-world, single-agent
sequential computational systems produced the practical experience that
such sequential systems can be faithfully modeled, simulated, analyzed
and refined to code by specific PGAs. A similar experience has been made
with PGAs as sequential components of concurrent systems. For a survey
see [58, Ch. 7,9] and https://abz-conf.org/method/asm.
• A theorem we explain in Sect. 8.1 proves that every computational pro-
cess that satisfies three natural postulates for sequential algorithms is in
a strong sense computationally equivalent to a PGA.7
Terminology and notation. Every PGA can be written in the following
theoretically interesting normal form (see the proof of Theorem 7, pg. 200).
Definition 13 (PGA Normal Form).
if φ1 then f1(s1,1, . . . , s1,n1) := t1
...
if φk then fk(sk,1, . . . , sk,nk) := tk
where the terms are ground terms and the guards φi are boolean combi-
nations of equations between ground terms. The vertical notation hides the
par operator and indicates the intended simultaneous (synchronous parallel)
execution of the horizontally displayed rules (here guarded assignments) in
each step.
7 Due to this theorem, in the literature PGAs are somehow misleadingly called ‘sequen-
tial ASMs’ (Gurevich’s wording in [93]). But the reader will see below that a) there
are forms of sequential algorithms that are not captured by the three postulates of
the theorem (Ch. 4), and b) not every PGA is a sequential algorithm in the intuitive
understanding of the term (Sect. 3.1.1 and Sect. 8.2).

2.2 Logical Structure of One-Step Actions
25
Sometimes it is technically useful to have a name for a process that per-
forms an empty step. For this purpose we use a special PGA rule skip.
The intuitive understanding of PGAs (and generally of ASMs) as pseudo-
code is supported by their rigorous semantics defined below (Sect. 2.2.2,
pg. 31). Therefore we use throughout and without further ado commonly
used notational variations and extensions, as long as their meaning is clear
and can be expressed by precisely defined core constructs.
For example, if an ASM (for example a PGA) M contains two rules if
φ then M1 and if not φ then M2 we write them also in the shorter form:
if φ then M1 else M2
and call this again a conditional ASM respectively PGA rule. Similarly, we
write:
case exp of
value1 →rule1
...
valuen →rulen
to stand for
if exp = value1 then rule1
else if . . .
...
else if exp = valuen then rulen
There are other kinds of basic algorithmic concepts and corresponding
ASM constructs that extend PGAs, in particular call mechanisms (call by
value and call by name, see Sect. 2.3.1) to support modular algorithmic struc-
ture and two fundamental logical operations (quantification and choice, see
Sect. 3.5 and Sect. 3.4). But when it is clear from the context or it does not
matter which kind of ASM is considered we use the term ASM rule instead
of a more specific name, like PGA rule. An ASM rule is also called transition
rule or simply ASM although an ASM besides its rule comes with additional
features like its signature with interaction type definition, an input/output
management for initial/final states, etc. (see Ch. 3). The signature of M con-
tains (and unless otherwise stated is understood to be) the set of all function
symbols that occur in a rule of M. For parsimony of names we usually use
the same name for an ASM and for any of its instances obtained by different
signature interpretations (and possibly associated to different agents when it
comes to execution).
To simplify the understanding of rules we often use where clauses to
separate the specification of particular constraints we impose on some terms
that are used in a rule.
We illustrate the definition of PGAs by two well-known examples of se-
quential algorithmic processes, namely Mealy automata (with the Finite State

26
2 Logical Structure of State-Change Actions
Machines variation) and Turing machines. Their control structure suggests
a generalization to the class of control state ASMs. Although we define the
semantics of PGAs only in the next Sect. 2.2.2, the reader will neverthe-
less understand the intended meaning of the rules below by reading them as
pseudo-code (reading that provides the correct understanding).
Example 4. Mealy Automata Interpreter. Example 2 (pg. 12) states in
natural language what a Mealy automaton is requested to do in one step; this
is formally expressed by the following PGA rule. We indicate some auxiliary
functions as parameters of the rule name so that one can define variations
of the machine simply by refining (read: specifying furthermore or imposing
additional constraints on) those parameters. Remember that for a Mealy
automaton +1 as specified in Example 2 is the right-move function.
MealyAutomaton(write, nextCtl) =
tape(head) := write(ctl, tape(head))
-- output function λ
ctl := nextCtl(ctl, tape(head))
-- internal state transition function δ
shift head to right
-- only one move direction
where
shift head to right = (head := head + 1)
This Mealy automaton rule is an abstract Mealy automaton interpreter:
once a concrete Mealy automaton program pgm = write × nextCtl and input
tape are given, together with an initial value for the head position (say 0)
and for ctl, the interpreter executes for this input the steps required by pgm.
The abstract Mealy automaton PGA rule stands for the infinite set of all its
Mealy automaton instances obtained by instantiating write and nextCtl by
concrete finite functions.
Exercise 3. A Finite State Machine (Fsm) is a Mealy automaton without
output behavior. The variant of a two-way FSM (which is computationally
equivalent to one-way FSMs, see [140]) can move its reading head to the right
or left on the tape, depending on current control state and input symbol.
What has to be changed in the above definition of the MealyAutomaton
and its signature to refine it to a (two-way) Fsm?
Example 5. Turing Machine Interpreter. In Sect. 8.1-8.2 we explain the
epochal character, for the whole of computer science, of the definition Turing
gave of his machines [162]. Here we use them to show how they can be de-
fined by slightly refining the definition given for Mealy automata (which were
defined two decades later). In fact, there is a technically rather thin though
conceptually crucial line that separates Turing machines (TMs) from Mealy
automata. TMs and Mealy automata share the same control structure: the
first two rules of MealyAutomaton are the same for the TuringMachine.
However, Mealy machines have only one moveDirection, namely to move on
the tape at every step to the right, and thus can only examine each input
symbol once and never re-read what has been written. In contrast, TMs have

2.2 Logical Structure of One-Step Actions
27
an additional moveDirection left that is determined by the current ctl and
tape(head). As a consequence, InAlphabet and OutAlphabet collapse into a
single set Alphabet. To this end, we extend the set of moveDirections by:
moveDir(ctl, tape(head)) ∈{right, left}
Then we can define:
TuringMachine(write, nextCtl, move) =
tape(head) := write(ctl, tape(head))
-- as for MealyAutomaton
ctl := nextCtl(ctl, tape(head))
-- as for MealyAutomaton
shift head to moveDir(ctl, tape(head)) -- move in indicated direction
where
shift head to right = (head := head + 1)
shift head to left = (head := head −1)
In this rule it is crucial that the three updates are executed simulta-
neously (in parallel), in one machine step, sharing the same values for
ctl, head, tape(head) before their updates. Note that we define TM programs
by the three static functions write, moveDirection, nextCtl.
Look-Compute-Action Pattern. TM instructions incorporate a cycle
of three basic sequentially executed actions, namely:
• Look up for the values of the state and the environment parameters that
are needed to perform the next step, here: read ctl and tape(head),
• Compute the values of the terms that determine the to-be-executed ac-
tion, using the parameter values read in the preceding step, here: perform
a program table look up to find the to-be-printed letter b, the to-be-
performed move m(head) of the reading head and the to-be-entered next
ctl state j,
• Action: perform as last step of the cycle the required proper action with
the computed term values, here: perform the three assignments in the
rule TuringMachine above.
Instances of this computational Look-Compute-Action pattern (also called
Read-Compute-Write pattern) appear in numerous (classes of) algorithms.
Prominently the atomic view of ASM steps incorporates this pattern, as
becomes visible in the sequential implementation of concurrent ASM steps in
Fig. 5.1 (pg. 115). For other examples of the pattern see cellular automata
(pg. 69), streaming algorithms (Fig. 6.2 on pg. 156), and Look-Compute-Move
algorithms (Sect. 9.2.2).
Exercise 4 (2-dimensional TM). What is needed to extend the TM-
interpreter to interpret 2-dimensional Turing Machines? See Appendix B.
Control state ASMs. The control structure of Turing machines under-
lies a large number of algorithmic processes and is widely used in connection
with FSMs and their flowchart visualization. For high-level system design it

28
2 Logical Structure of State-Change Actions
is useful to raise the level of abstraction from an FSM-notation to an ab-
stract machine notation so that a complex machine M can be composed out
of separately specifiable, implementable, testable and verifiable component
submachines Mi, one per mode or phase i. Such a modular structure is easily
realized by the concept of control state ASMs, machines where every com-
ponent machine Mi splits again into separate guarded submachines, i.e. each
one of the following form visualized by the Control State Diagram in Fig. 2.2
(with i, j1, . . . , jn denoting control states and assuming for consistency rea-
sons that the rule guards condj are pairwise disjoint)8 :
if ctl = i then
if cond1 then
rule1
ctl := j1
. . .
if condn then
rulen
ctl := jn
Fig. 2.2 Control State ASM Diagram.
© 2003 Springer-Verlag GmbH Berlin Heidelberg, reprinted with permission
i
cond1
rule1
j1
. . .
condn
rulen
jn
The degree of possible interaction of the guarded submachines of Mi can be
controlled by specifying the interaction types of the functions and locations
they use. The guarded submachines are often textually written as follows:9
FSM(i, if cond then rule, j) =
if ctl = i and cond then
rule
ctl := j
8 Sometimes instead of pairwise disjoint guards a sequential order is imposed so that
each time the first (in the given order) subrule with true guard is executed. An example is
the semantics of if B1 S1 . . . Bn Sn statements in the programming language Occam [69]:
the guards are evaluated in the order B1, B2, . . . in which they appear in the statement
and for the first Bi that evaluates to true the substatement Si is executed.
9 The reader who would like to see a precise mathematical (metamodel) definition of
the class of Control State Diagrams (CSD) and a compilation of CSDs to ASMs that
define their semantics may consult [50, Ch. 9].

2.2 Logical Structure of One-Step Actions
29
A control state ASM can equivalently be written in the form of a PGA
if the component machines rulej are equivalent to PGAs. But the definition
works well also for generic ASMs.
Example 6. Control structure of a transaction operator.
We illustrate the decomposition potential of control state ASMs by a non-
trivial example from transaction control design; to understand the interaction
of the given machine M with the transaction control components it should
suﬀice if the reader has some rough idea about the concept of transactional
control. In [56] an abstract (programming language independent) transaction
controller TaCtl is defined together with an operator Ta which applied to a
family of programs enables them to realize a transactional behaviour in con-
current runs together with TaCtl. The transactional control behaviour that
results from the application of the operator Ta to a program M of the given
family appears as control state computation specified in Fig. 2.3: M tamed
by Ta calls separately specifiable and analyzable (here not further described)
components of TaCtl for assistance to guarantee that when M Fires a step
(or a sequence of steps) that computation segment is transactional.
Fig. 2.3 Transaction Control Structure of Ta(M) (M transformed by Ta)
Register(M)
TA-ctl
RecoveryVictim(M)
WaitForRecovery
Recovered(M)
Terminated(M)
CallCommit(M)
NeedsNewLock(M)
CallLockHandler(M)
WaitForLocks
GrantedLocks(M)
CanGo
CanFire(M)
CallAbort(M)
Fire(M)
no
no
no
yes
yes
yes
yes
yes
no
no

30
2 Logical Structure of State-Change Actions
We now explain the control state ASM defined by Fig. 2.3, assuming a
rough understanding of the behavior of the diagram components. The di-
agram shows that once a program M is Registered with the transaction
controller TaCtl, its steps are constrained by a series of transactional con-
ditions inserted by the transaction operator Ta and to be satisfied by calling
the corresponding TaCtl components:
• If M has been ‘victimized’ by the DeadlockHandler component
(which we do not furthermore specify here), then it must wait until the
Recovery component resolves the deadlock and makes M Recovered.
The Recovery component essentially backtracks some deadlocked pro-
grams. Both Recovery and DeadlockHandler are independently de-
finable components that do not appear in the diagram, but their interac-
tion with M behaviour becomes visible in the checks whether M became
a RecoveryVictim respectively has been Recovered.
• If M has Terminated its current transactional computation segment—
the success case—it calls the Commit component, thus completing the
current transactional computation segment.
• If M to Fire its next step needs locks (on shared, monitored or output
locations it wants to access) it calls the LockHandler and can proceed
from there only when the LockHandler has GrantedLocks for M.
• If M needs no locks or has been GrantedLocks to Fire its next step but
a failure occurred, then M calls the Abort component to end the failed
transactional segment of computation.
• When M does Fire a step it also executes a RecoveryRecording pro-
gram (inserted into M by the Ta operator) which enables Undoing the
current step upon a Recovery step by TaCtl.
The interaction structure of the components with the Ta-transformed ma-
chine M is completely defined by the control state ASM in Fig. 2.3 and per-
mits to specify and analyze those components independently of each other
in terms of their behaviour in reaction to input from or output to other
components.
For the sake of completeness we define here the component structure of
TaCtl and refer the reader for their detailed definition and the correctness
statement and proof for TaCtl to [56].
TaCtl =
LockHandler
Commit
Abort
DeadlockHandler
Recovery
Example 7. Structured Iterated Control State ASMs. An interest-
ing class of control state ASMs consists of those one can compose in a

2.2 Logical Structure of One-Step Actions
31
Fig. 2.4 Structured Iterated Control State ASM (while Cond M)
start
Cond
startM
. . . dgm(M) . . .
done
yes
no
structured way out of flowchart diagrams dgm(M) for basic FSM-like rules
M = FSM(i, if cond then rule, j) (see Fig. 2.2, pg. 28).
In such diagrams every transition from a current control state i to a suc-
cessor control state j is considered as one internal machine step, after which
the environment can make a step before the next rule is fired in control state
j, leading to a next current control state, etc. We illustrate this by Fig. 2.4
where it is assumed that the already defined structured flowchart diagram
dgm(M) of M has exactly one initial control state startM and that each of
its final control states (i.e. not connected to any control state in dgm(M)) is
connected to the initial control state start of the while construct. This con-
straint catches jumps that would lead out of M and leads them to the initial
state of the iteration machine. Such a governed goto-discipline, as is found
in various programming languages, helps to avoid what are called spaghetti
control structures. To forbid gotos tout court appears as an exaggeration.
Exercise 5 (Register Machine Interpreter). Check your understanding
of the above interpreter rules for Mealy automata, Turing machines and con-
trol state ASMs by writing a control state ASM which interprets register
machines. A register machine (RM) M has a finite number of registers rj to
store arbitrary natural numbers and has a program of two kinds of labeled
instructions (say labeled by natural numbers): (i, j, op, i +1) (in control state
i perform operation op on register rj and go to the next control state i + 1,
where op ∈{+1, −1}) and (i, j, test, l) (in control state i test whether the
value of rj is 0: if it is, go to control state l, else go to control state i + 1).
For a solution see Appendix B.
Exercise 6. Write a control state ASM (e.g. with 2 control states halt-
ing,moving) for a lift control program. For a solution see [58, Sect. 2.3].
2.2.2 Single-Agent Algorithmic Steps: Semantics
In this section we define the effect—also called the behavioural meaning or
the semantics—of the three PGA steps. We formulate the definition in a way
that applies also to extensions of PGAs to ASMs with additional one-step

32
2 Logical Structure of State-Change Actions
actions. Such a generic notion of atomic steps that are executed repeatedly by
single-agent algorithmic processes forms the basis for the concept of various
(single- or multi-agent) algorithmic process runs investigated in Ch. 3-5.
Definition 14 (Semantics of ASM Rules). In a given state A with vari-
able environment ζ an ASM rule M in one atomic step computes (we also
say yields) an update set U = eval(M, A, ζ) and—if U is consistent—applies
it to A to obtain the successor state B = A + U of A as defined in Def. 9
(pg. 19). The evaluation of M is defined by induction on the rules, its result
is also called the yield of M.
• The evaluation of skip yields the empty set: eval(skip, A, ζ) = ∅.
• The evaluation of an assignment instruction yields a 1-element update
set for the location corresponding to the evaluated argument with value
obtained by evaluating the right-hand side of the assignment instruction:
eval(f (t1, . . . , tn) := t, A, ζ) = {((f , (t1, . . . , tn)), t)}
where x = eval(x, A, ζ).
Note that for frugal naming we use the same name for the three evaluation
functions of terms (Def. 4, pg. 13) and formulae (Def. 6, pg. 15) and here
of machines, distinguishing them by the type of their arguments.
• The evaluation of a conditional ASM yields the update set of the com-
ponent ASM that must be executed depending on whether the condition
is true or not:
eval(if φ then M else N, A, ζ) =
(
M
if φ = true
N
otherwise
where ¯x = eval(x, A, ζ).
Note that we treat the conditional guards φ as formulae so that by the
interpretation of formulae in Def. 6 (pg. 15) their meaning is always
defined (either true or false).
• Simultaneous evaluation of two ASMs yields the union of their update
sets:
eval(par (M1, M2), A, ζ) = eval(M1, A, ζ) ∪eval(M2, A, ζ).
If B = A + eval(M, A, ζ) is defined we say that M can make a move (or a
step) from state A (with variable environment ζ) to B and write A
1⇒M B
(or A
1⇒B when M and ζ are clear from the context). We also say that
M is (or can be) fired in state A. The updates in eval(M, A, ζ) are called
internal updates and are distinguished from updates of monitored locations or
from updates of shared locations by the environment (read: by other agents,
see Def. 22, pg. 40). The state B resulting from internal updates is called
successor state, value of the partial step function stepM(A) = B of M on its
states, also called transition function. When interactive runs are considered,
where also the environment may add further changes of monitored or shared

2.3 Domain-Specific One-Step Actions
33
locations, successor states are more precisely called next internal states (see
below Def. 22 on pg. 40).
Notation. Note that here again we use eval as name for an evaluation
function, here of machines in a given state. To emphasize that eval(M, A)
yields an update set sometimes also the notation Upd(M, A) is used. To
prepare the ground for non-deterministic ASMs with a choose construct
(where in a state more than one update set could be generated, see Sect. 3.4)
we write U = eval(M, A, ζ) also as a relation evalCand(M, A, ζ, U). In the
literature the relation name yields is used instead of evalCand. Where the
variable environment ζ is empty and the successor state B = A + U is
defined, it has become customary to denote a generated update set U with
eval(M, A, U) as difference set ∆(M, A) = (A + U) −A.10
Exercise 7 (Isomorphic Yields). If M yields U in A under a variable
assignment ζ, then in every isomorphic B M yields α(U) under α ◦ζ where
α is the isomorphism from A to B. Prove this statement.
2.3 Domain-Specific One-Step Actions
The general one-step action concept defined in terms of ASMs in Sect. 2.2 pro-
vides the foundation for the ASM refinement method described in Sect. 7.2.
In fact, to apply in one step a set of assignments f (t1, . . . , tn) := t implies
that the computation of every argument term ti, t is considered as (part of)
one step. Here computing a term can represent any domain-specific com-
plex but well-understood action for given arguments. Furthermore, the ASM
update concept makes it easy to define specialized one-step actions where
their atomic execution is a helpful abstraction in terms of the given appli-
cation domain. We illustrate this here for two widely used procedure calling
mechanisms (by value and by name, Sect. 2.3.1)) and for workspace cre-
ation (Sect. 2.3.2). Other examples of useful one-step action constructs are
explained in the following chapters.
10 There is an underlying assumption for the ∆notation. Differently from ∆(M, A),
the update set U computed by M in A may contain a trivial update, i.e. some (loc, v)
where v is the current value of loc in A. U is inconsistent if it contains also a non-trivial
update for the same location, in which case U does not define a successor state B. If U
contains a trivial update without a conflicting non-trivial one, then this trivial update
can be disregarded without loss of generality when applying U to generate the (new
part of the) successor state.

34
2 Logical Structure of State-Change Actions
2.3.1 Call Step (by Value, by Name)
The let construct is often used with advantage to compute once the value of
some term t and then use this value in different places, without recomputing
the term evaluation in each place. Its usual syntactical form is let x = t in M
(where M denotes any ASM) and its semantics as an ASM rule is defined for
any variable environment ζ and state A as follows:
Definition 15 (Semantics of let Rules (Call by Value)).
eval(let x = t in M, A, ζ) = eval(M, A, ζ[x →a])
where a = eval(t, A, ζ)
Remark. The rule let x = t in M is equivalent to M(x/t) only as long
as for each occurrence of x in M that is replaced by t the value of t that
is associated with this occurrence of x is the same. This property does not
necessarily hold.11
The Call by Name mechanism applies to a rule M that is defined with
some individual variable parameters x1, . . . , xn by a rule body N. When M
is called with concrete parameters t1, . . . , tn, then the defining rule body N
is executed with each xi replaced by the term (not its value) ti. Those terms
are evaluated when they are used in the rule body N to compute an update
set, not necessarily in the state where the rule M is called.
Therefore we extend the syntax of ASMs by rules of the form M(t1, . . . , tn)
where M (a rule name of arity n) has been defined by a rule declaration, an
expression of the form
M(x1, . . . , xn) = N
where N is an ASM rule whose free variables are among the xi.
Definition 16 (Semantics of Call (by Name) Rules). Let a rule decla-
ration M(x1, . . . , xn) = N be given. To execute a call M(t1, . . . , tn) execute
the rule body N with the call parameters:
eval(M(t1, . . . , tn), A, ζ) = eval(N(x1/t1, . . . , xn/tn), A, ζ)
Note that calls M(t1, . . . , tn) represent a notational abbreviation, they can
always be equivalently replaced by a copy of the corresponding body instance
N(x1/t1, . . . , xn/tn). If the terms ti are static, then first substituting the terms
ti for xi in M and then evaluating the machine by eval(M(t1, . . . , tn), A, ζ)
generates the same update set as the one obtained by first evaluating the
terms ti for an environment extension ζ[xi →ai] and then evaluating M
under the new environment by eval(M, A, ζ[x1 →a1, . . . , xn →an]) where
ai = eval(ti, A, ζ).
11 It may fail for example in the presence of a sequencing operator like seq (Def. 24,
pg. 53).

2.3 Domain-Specific One-Step Actions
35
Remark. The call by name scheme is sometimes used in a more general
form that permits location and rule variables besides the individual variables
xi in Def. 16. At times, by abuse of notation, we use the same scheme to
express arbitrary text substitution.
2.3.2 Workspace Increase Operation
To describe how algorithmic processes increase their workspace we introduce
an infinite static subset of the superuniverse, a Heap of elements machines
can import to increase their workspace. We treat the Heap as 0-ary function
symbol in Σ but require that it does not appear in rules because it is only
used to define the semantics of ASMs with the import construct. We also
require that for every machine in each of its initial states A the Workspace(A)
and the Heap are disjoint.12 Workspace(A) is defined as follows:
Definition 17 (Workspace). In every state A Workspace(A) contains be-
sides true, false and undef the elements that for some f ̸= Heap appear in
state A as content f A(a1, . . . , an) ̸=undef of a location (f , (a1, . . . , an)) or as
element ai of a location (f , (a1, . . . , an)) whose content is not undef .
In the presence of parallelism it is reasonable to require for an import
operation that two or more simultaneous imports yield pairwise distinct ele-
ments from the Heap. Furthermore, the behaviour of a machine that uses an
import should not depend on which element of Heap is imported, otherwise
stated it should not depend on permutations of the Heap.
A simple way to achieve this13 is to guarantee that when computing the
updates generated by a parallel rule M = par (M1, M2) (Def. 14, pg. 32)
every import that occurs in M1 or M2 is evaluated separately from the
others, in whatever order; keeping track of each element that is imported
during the evaluation of M permits to select upon the next import a free
element from Heap, i.e. one that is distinct from those imported before and
neither in the current Workspace nor in the range(ζ) of the current variable
environment. This means that for each machine M and state A its evaluation
procedure eval(M, A) is refined by an additional parameter η (besides ζ) to
generate a pair (U, η) of an update set U (as by Def. 14) together with a set
η of those Heap elements that during this evaluation procedure for M have
been committed for an import operation. This evaluation starts with an
initially empty import parameter η = ∅and an empty variable environment
parameter ζ (if M is ground). Each time an import subrule of M requests
12 Workspace(A) and Heap are not complement of each other; elements imported from
Heap are intended to become workspace elements, but by further machine steps they
could loose this property. What does hold is the relation Workspace(A)∪Heap⊆|A|.
13 We define here a constructive evaluation procedure that simplifies the import man-
agement defined in [58, Sect. 2.4.4] as well as its correctness proof.

36
2 Logical Structure of State-Change Actions
to import an element we select a free element a ∈Heap to evaluate M and
add it to the current value of η.
We formulate the import operation with the syntax import x do M.
We define its semantics as generating for every given M with some import
subrules an update set together with a set η of elements imported from Heap,
starting the evaluation procedure of M with an initially empty η and empty
variable environment ζ (if M has no free variables).
Definition 18 (Semantics of ASM import rule).
Let an ASM M be
given together with a state A and two parameters for a variable environment
ζ respectively a subset η ⊆Heap. We define:
eval(import x do M, A, ζ, η) = eval(M, A, ζ[x 7→α], η ∪{α})
where
-- select a free element
α ∈Heap \ (Workspace(A) ∪range(ζ) ∪η)
The formulation of the semantics of the other PGA rules on pg. 32 is eas-
ily adapted. In the rules M for skip and assignment η is passed as fourth
argument of the eval function and returned unchanged in the generated pair
eval(M, A, ζ, η) = (U, η) of an update set U and η; analogously, for condi-
tional instructions we have:
eval(if Cond then M, A, ζ, η) =

eval(M, A, ζ, η) if CondA
(∅, η)
otherwise
where CondA = (eval(Cond, A, ζ) = true)
A change appears only for par (M1, M2) rules and the definition of successor
states.
Definition 19 (par semantics with import).
eval(par (M1, M2), A, ζ, η) = (U1 ∪U2, η2)
where
(U1, η1) = eval(M1, A, ζ, η)
-- first evaluate M1 adding imports (if any) to η
(U2, η2) = eval(M2, A, ζ, η1)}
-- then evaluate M2 adding imports (if any) to η1
Definition 20 (Successor state for ASMs with import). For every state
A its successor state B computed by a ground ASM M with import rules is
defined as follows by a refinement of Def. 14, pg. 32. Note that initially (when
the evaluation of M starts) there is no imported element and the variable
environment is empty (if M is ground).
if eval(M, A, ∅, ∅) = (U, η) and Consistent(U)
then nextStateM(A) = A + U
else nextStateM(A) = undef

2.4 Core Actions on Structures (Recap)
37
Notational conventions. We define an operator new used in the form
let x = new (X) in M as an alternative notation for import x do M.
We write let x1, . . . , xn = new (X) in M for
let x1 = new (X) in
...
let xn = new (X) in M
If new does not occur in a formula α(x) and x has only free occurrences
in α(x) we write α(new (X)) for let x = new (X) in α(x).
For conciseness we also write c :=new X for let x = new (X) in c := x.
With a slight abuse of terminology, we will at times assume that when
a new element is obtained by new for a background that is provided with
an algebra of operations, then this new element is initialized so that the
operations can be applied to the new element with sane results. For exam-
ple, q =new (Queue) would initially satisfy IsEmpty(q), head(q) =undef ,
size(q) = 0, etc. This applies in particular to membership in the domain, i.e.
by charQueue(q) = true.
We skip mentioning the eval-parameters ζ and/or η where they play no
role or where they are clear from the context.
2.4 Core Actions on Structures (Recap)
Given their fundamental character and for an easy reference we recapitulate
here the three core actions (partial operations) on computational structures
in terms of which we characterize in the rest of the book the major compu-
tational paradigms for processing abstract states.
apply(A, U) = A + U
This partial function applies to any state A and any given con-
sistent update set U to compute a new B. A = B is possible.
See pg. 19.
eval(M, A, ζ)
This function computes the update set machines M without
import subrule generate in state A with variable environment
ζ. This set may be inconsistent. See pg. 32.
stepM(A) = apply(A, eval(M, A))
For every ground M without import subrules this partial func-
tion is defined in a state A if M generates an update set
U = eval(M, A) that is consistent; in this case M can make

38
2 Logical Structure of State-Change Actions
a move in A and the function computes what we also call the
nextStateM(A) = A+U by applying the update set U to A. See
pg. 32.
eval(M, A, ζ, η)
For machines that may have some import subrule this function
computes a pair (U, η′) consisting of the (modulo a permutation
of Heap) same update set U as eval(M, A, ζ) and an extension
η′ of η by elements imported by M as part of its step (in state
A with variable environment ζ and set η of already imported
elements). See Sect. 2.3.2.
stepM(A) = apply(A, upd(eval(M, A, ∅, ∅)))
We write upd(U, η) = U. For every ground M with import sub-
rules this partial function is defined if eval(M, A, ∅, ∅) = (U, η)
holds for some update set U—in fact the same as eval(M, A)—
with a set η of elements imported during the evaluation of M and
if U is consistent; when defined the function applies upd(U, η)
to A to determine the successor state. See Def. 20, pg. 36.
Where we are only interested in computed update sets to illustrate a com-
putational issue or where the parameter η is passed unchanged from the
argument of an evaluation to its value eval(M, . . . , η) = (U, η) for some U
(as is the case for skip, assignment and conditional instructions) we omit
mentioning the import of elements from Heap during a machine evaluation
and formulate the investigated issue using eval(M, A, ζ) or eval(M, A) instead
of eval(M, A, ζ, η).

Chapter 3
Control Structures of Single-Process Runs
In this chapter we define and investigate in generic terms sequential process
runs, also called sequential algorithmic computations. They are performed
stepwise by a single agent that is equipped with an algorithm it executes and
that may interact with an abstract environment (Sect. 3.1). The possible run-
time interaction with the environment extends the classical notion of Turing-
machine-like algorithmic computations which are input-triggered.1 Here we
illustrate our definition first by runs of imperative procedural Java programs
(known as Pascal-like programs, see Sect. 3.2) and then instantiate it to the
following widely used specific paradigms of sequential computations.
• In input-driven output-generating single-agent sequential runs (Sect. 3.3)
the role of the environment is reduced to define by the input the initial
state of a computation and to possibly receive some output in a final
state.2 We illustrate this concept of input/output machines (abbreviated
i/o-machines) by runs of structured ASMs which compute all Turing-
computable functions over the natural numbers. Another characteristic
example are the components of recursive algorithms we investigate in
Sect. 8.4.
• In nondeterministic single-agent sequential runs (Sect. 3.4) a step may
depend upon some choice that is performed by the executing agent (using
a choose operator) or by the process environment.3 We illustrate the
use of choose for an explanation of the interleaving model of parallel
computations.
• To describe truly parallel runs of multiple synchronized sequential pro-
cesses (Sect. 3.5) we let one agent execute simultaneously the steps of all
1 In Sect. 8.1.1 this relation is investigated in more detail.
2 Remember that for single-agent processes, where the attention is focused on a given
program to be executed, we do not count the environment as an agent (Definition 11,
pg. 22).
3 Turing [162] already considered ‘choice machines’ which at certain points ‘cannot go
on until some arbitrary choice has been made by an external operator’.
39
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
E. Börger, V. Gervasi, Structures of Computing, 
https://doi.org/10.1007/978-3-031-54358-6_3 
 
 
 
 
 

40
3 Control Structures of Single-Process Runs
involved synchronized agents, expressed by a forall construct borrowed
from first-order logic.
In Chapter 4 we extend the notion of sequential processes by reflection,
an ability of algorithms to change their program at runtime.
3.1 Definition of Single-Process Runs
We view sequential runs of algorithmic processes as executed by a single
agent in an abstract process environment. The environment may provide
input and receive output via the correspondingly declared interaction type
of functions and locations (see Sect. 2.1.1). Such an ongoing interaction of
an ASM with its environment turns the machine into a reactive process and
requires a protocol to avoid any conflict between environment actions and
internal actions the agent of the process performs. We adopt here the widely
used scheme where environment and process steps alternate. Furthermore,
usually computational processes are started in some initial state. For this
reason we extend Definition 12 as follows.
Definition 21 (Abstract State Machine). An Abstract State Machine M
consists of a signature Σ,4 a set of initial states over Σ, and a ground ASM
rule (i.e. a rule without free variables), called the main rule of M, which
comes with a finite (possibly empty) set of declarations of rules the main
rule may call by name. The main rule and the declared rules are also called
machine components. The signature often comes with additional stipulations,
e.g. an interaction type classification.
We often use M as name for its main rule and identify the entire machine
with this rule if the signature, the declarations and the initial states are clear
from the context. The states of M are the Tarski structures of M’s signature
Σ. We say PGA instead of ASM if the components are PGA rules. To avoid
repetitions, definitions which mutatis mutandis apply to every type of ASM
are given in terms of not furthermore specified ASMs.
A computation of a single process starts in one of its initial states and
repeats (iterates) as long as possible making a move (one step), reacting
to interspersed steps of the environment (read: of other agents) which may
update some monitored or shared locations.
Definition 22 (Single-Agent Process Run). For an ASM M with signa-
ture Σ a single-agent run or computation is a sequence A0, A1, . . . of states
over Σ where A0 is an initial state of M and for each natural number n the
following holds:
4 In some cases we permit the signature to be infinite. Usually it is requested to be
finite.

3.1 Definition of Single-Process Runs
41
• either the main rule of M can make a move (see Sect. 2.4, pg. 37) from
An to a next internal state A′
n and the environment produces a consistent
(possibly empty) update set U of monitored or shared locations such that
An+1 = A′
n + U
• or M in An cannot make any move in which case the run terminates and
An is the last state in the run.
Note that by this definition the environment may override some internal
updates of shared locations. In specific applications other protocols can be
used to avoid inconsistent updates of shared locations by the involved agents.
We adopt the usual wording of run or sequential run instead of single-agent
run where the context prevents any confusion with multi-agent runs. By
A
n⇒M B we denote that a single-agent run of M started in A leads in n
moves to B. By A ⇒M B or A ⇒B we denote that in a finite number of
steps M moves from state A to B. We call a run internal if it generates only
internal updates (i.e., the environment always generates empty update sets),
otherwise it is called an interactive run.
In the next sections we illustrate the definition by some characteristic
examples. Note that standard Turing machines (Example 5, pg. 26) during
their computations do not interact with the environment.
Exercise 8 (Interactive Turing Machine). Refine the concept of TM
in such a way that in each step the machine may receive (and react to)
some input from the environment and provide an output to the environment.
Write an Interactive TM interpreter, refining the TuringMachine defined
in Example 5. For a solution see Appendix B.
3.1.1 Relative Computability (Background Concept)
The reader may have wondered that in the signature of an ASM M any func-
tion is allowed to appear, even if not computable. This feature is crucial for
the freedom of abstraction to design high-level abstract models, in particular
to model multi-agent computations. In fact, when using Tarski structures as
machine states one can make various constraints or assumptions depending
on the considered class of machines. Some are always present to avoid trivial
states, e.g. the condition that every state comprises the denotations of true,
false and undef which are pairwise different elements and different from all
other elements in the state (Definition 2). Others relegate the specification of
some well-known and auxiliary features (think about operations on sets, lists,
sequences, trees, etc.) or of modules to be done elsewhere. For modularization
purposes one can include into the signature of an ASM M some elsewhere
defined functions f so that when computing terms of M these functions can
just be used without having to compute their values via updates of M.

42
3 Control Structures of Single-Process Runs
In logical terms this phenomenon is known as relative computability: every
step of M is an effective computation step relative to the set of such functions
f so that the runs of M represent an algorithmic computation only relative
to the computation of such functions f .
A function f may even be non computable but have a precise definition of
its values f (t) that are used in the description of the updates generated by
a machine. For example, see the one-step ASM computation out := f (TM)
where f is the characteristic (total) function of the algorithmically unde-
cidable Halting Problem of Turing machines TM (Sect. 9.1.1). Nobody will
consider this computation to provide an algorithmic solution of the Halting
Problem of Turing machines.
In general, a function symbol f in the signature of an ASM M is a back-
ground function of M if it appears in (some term in the rules of components
of) M but never as outer function symbol f in an assignment f (s) := t; this
includes all static and monitored functions of M.
Therefore, ASMs can serve for a foundation of relative computability, in-
tensively investigated in logic. But be aware that an ASM describes an algo-
rithm in the intuitive sense of the term only if all of its background functions
are algorithmically computable. This holds even for PGAs so that (contrary
to what is suggested in [93]) not every PGA is a sequential algorithm in
the intuitive sense of the term, although it satisfies Gurevich’s ‘Sequential
Postulates’ (Def. 45, pg. 199). Background restrictions are needed where ef-
fectiveness of a computation is an issue.5
3.2 Sequential Imperative Procedural Program Runs
Imperative procedural programs constitute a set of sequential processes. Their
basic constructs are assignment and sequential control constructs (sequenc-
ing, IfThenElse, While, Goto, Call). It is easy to provide a succinct description
of their behaviour by a PGA, a program interpreter that reflects directly the
intuitions the programming language associates with the program instruc-
tions.
However, to make the example convincing for the reader we want to also
document its extendability, using the same specification method, to a full-
fledged conceptually richer programming language that in particular offers
also concurrency (the case of full Java, see Sect. 5.4.2).
For this reason we define an interpreter for the sublanguage of Java that
consists of imperative and procedural (neither object-oriented nor multi-
threaded) programs. We follow the instructionwise (refinement supporting)
platform-independent specification as it has been described and analyzed for
5 Two examples illustrate this phenomenon in this book: the reduction of the background
to some undisputedly computable functions for the computation of partial recursive
functions (Def. 26, pg. 55) or for the characterization of Turing’s thesis (Sect. 8.2).

3.2 Sequential Imperative Procedural Program Runs
43
the entire Java language in [159] and reused in [45] for the description of the
ECMA standard of C#. We concentrate the attention here on the dynamics
of program execution and refer for the underlying static part of the language
(syntax, types, constraints, etc.) to [159, 45] (work that was based on the
language manual of that time [89]).
To reflect the traditional distinction of basic imperative method constructs
from procedural ones (like call, return) concerning procedures in modules
(the class methods in Java), we arrange the instructions in two groups JavaI
and JavaC. Furthermore each group is splitted into a subgroup JavaExpI
respectively JavaExpC dealing with the computation of expressions and a
subgroup JavaStmI respectively JavaStmC dealing with the semantics of
statements.
JavaI =
JavaExpI
JavaStmI
JavaC =
JavaExpC
JavaStmC
The four components are independent of each other and therefore can be de-
veloped separately but put in parallel execution because imperative programs
describe sequential processes where the program counter enforces to deal at
any moment with only one program element. Note that the complete Java
interpreter can be defined in a similar way as parallel composition of stepwise
extended machines JavaI, JavaC, JavaO (describing Java’s object-oriented
features), JavaE (describing Java’s exception handling), JavaT (describing
concurrency features of Java threads), see [159] .
We adopt the intuitive representation of programs by an annotated ab-
stract syntax tree so that program execution corresponds to a walk through
the syntax tree of the current method body. A 0-ary function pos : Pos de-
notes an abstract program counter: the current position (view it as a node in
the tree or as a place in the source code) where the interpreter must compute
an expression or a statement. Initially pos = ﬁrstPos, the ﬁrstPosition in the
body of the given method to compute. We write αexp or αstm to denote an
expression or a statement stm associated with position α. At each node the
associated pending subcomputation is recorded using a function
restbody : Pos →Phrase
where initially restbody(ﬁrstPos) = body. We write also restbody/pos for
restbody(pos) denoting the currently to be computed phrase at this position.
The set of Phrases contains besides expressions and block statements—the
initial values of restbody(pos) for every position—also semi-computed expres-
sions and statements which may contain computed Values, some goto in-
formation (element of a set of possible reasons for Abruption of the normal

44
3 Control Structures of Single-Process Runs
control flow) or the word Norm denoting that the computation of a state-
ment terminated successfully (read: has been completed without abruption).
The interpreter from expressions and statements walks down the syntax tree
to first compute the values of the corresponding subexpressions or substate-
ments. Once the pending subcomputation is completed (whether normally
or abruptly) the control passes to the parent position we denote by up(pos)
so that the context of the completed subcomputation becomes the remaining
parent’s (sub)computation restbody(up(pos)).
up : Pos →Pos
-- parent function of syntax tree
context(pos) =
restbody(pos)
if pos = ﬁrstPos or restbody(pos) ∈Bstm ∪Exp
restbody(up(pos)) else
where Bstm and Exp are respectively block statements and expressions, as
defined more precisely in Sect. 3.2.1 below.
To record the result of a subcomputation we write:
yield(result) = (restbody(pos) := result)
To pass to the parent position the result of the subcomputation(s) (or what
has to be computed from them at the parent position) we write:
yieldUp(result) = restbody(up(pos)) := result
pos := up(pos)
Notational convention. For a succinct and intuitive notation we indicate
by f (. . .▷t . . . ) that the current pos is on the subword t of a structured
word (sequence of letters) f (. . . t . . . ). Similarly we write s = phrase(▷t)
as abbreviation for s = phrase(t) and pos = ▷and restbody(pos) = t.
We exploit the flexibility of the ASM framework by adopting here pattern
matching to write down and execute the rules for JavaI and JavaC—which
anyway are sequential rules— so that they are tried out for execution one by
one, in the order they are written down.
3.2.1 Imperative Constructs (JavaI)
The expression evaluation machine JavaExpI. The definition follows
the inductive syntactical definition of the set of expressions of the imperative
sublanguage of Java.
Exp = Lit | Loc | Uop Exp | Exp Bop Exp | Exp ? Exp : Exp | Asgn
Asgn = (Loc = Exp)
Using pattern matching the expression interpreter is defined by the follow-
ing four sequences described one by one below of case descriptions e →rules
with rules to evaluate the currently visited expression context e:

3.2 Sequential Imperative Procedural Program Runs
45
JavaExpI = case context(pos) of
LitVar
-- evaluation of literals and variables
FctExp
-- evaluation of functional expressions
VarAssign
-- evaluation of variable assignments
CondExp
-- evaluation of conditional expressions
The submachines are defined and executed instructionwise in the order of the
text. For the meaning of literals and functional expressions we use the defi-
nitions provided by the language specification function JLS in the reference
manual [88, Sect. 3.10]. The value of local variables is given by a function
locals (that is assumed to be initially undefined):
locals : Loc →Val
Therefore LitVar has the following two rules:
lit →yield(JLS(lit))
-- record the meaning of the literal
loc →yield(locals(loc))
-- record the current variable value
FctExp consists of five rules to compute functional expressions formed by
unary or binary operators. Note that the Arithmetic Exception division-by-0
case is dealt with in the exception handling component JavaExpE we do not
show here.
uop αexp →pos := α
-- walk down to compute the exp
uop ▷val →yieldUp(JLS(uop, val))
-- pass computed value to parent
αexp1 bop βexp2 →pos := α
-- walk down to compute the left exp1
▷val bop βexp2 →pos := β
-- continue to compute the right exp2
val1 bop ▷val2 →
-- continue at parent passing computed val
if not (bop ∈divMod and isZero(val2))
-- exclude division-by-0 case
then yieldUp(JLS(bop, val1, val2))
The variable assignment interpreter VarAssign is defined by two rules:
loc = αexp →pos := α
loc = ▷val →locals(loc) := val
-- bind val to loc in local env
yieldUp(val)
The conditional expression interpreter CondExp is defined by four rules:
αexp1 ? βexp2 : γexp3 →pos := α
-- first compute the condition
if ▷val ? βexp2 : γexp3 →if val then pos := β else pos := γ
-- depending on val move to compute exp2 or exp3
α true ? ▷val : γexp3 →yieldUp(val)
-- continue with val at parent
α false ? βexp : ▷val →yieldUp(val)
The imperative statement computing machine JavaStmI. The def-
inition follows the inductive syntactical definition of the sublanguage of im-
perative statements of Java.

46
3 Control Structures of Single-Process Runs
Stm = ; | Asgn; | Lab : Stm | break Lab; | continue Lab;
| if (Exp) Stm else Stm | while (Exp) Stm | Block
Block = {Bstm1 . . . Bstmn}
Bstm = Type Loc | Stm
Phrase = Exp | Bstm | Val | Abr | Norm
We use pattern matching also for the imperative statement interpreter
defined by the following submachines:
JavaStmI = case context(pos) of
ExpStm
-- computation of empty and expression stms
BreakContinueStm
-- computation of goto stms
BlockStm
-- computation of block stms
CondStm
-- computation of conditional stms
WhileStm
-- computation of while stms
These machines walk through the syntax tree, moving from any structured
statement down to compute its substatements and back to the parent state-
ment until the statement has been computed Normally or an Abruption
showed up.
The interpreter ExpStm for expression statements is defined by four rules:
;
→yield(Norm)
-- executing the empty stm terminates normally
Type x; →yield(Norm)
-- a type declaration terminates normally
αexp; →pos := α
-- an expression stm...
▷val; →yieldUp(Norm)
-- ...terminates normally evaluating exp
The interpreter BlockStm for block statements is defined by four rules:
{ } →yield(Norm)
-- an empty block terminates normally
{α1stm1 . . . αnstmn} →pos := α1
-- left-to-right execution strategy
{α1Norm . . . ▷Norm
αi+1stmi+1 . . . αnstmn} →pos := αi+1
{α1Norm . . . ▷Norm} →yieldUp(Norm)
-- moves to parent reporting Norm if all stmi terminated Normally
The interpreter CondStm for conditional statements is defined by four rules:
if (αexp) βstm1 else γstm2 →pos := α
-- first evaluate the condition
if (▷val) βstm1 else γstm2 →if val then pos := β else pos := γ
if (αtrue) ▷Norm else γstm2 →yieldUp(Norm)
if (αfalse) βstm1 else ▷Norm →yieldUp(Norm)
The interpreter WhileStm for while statements is defined by:
while (αexp) βstm →pos := α
-- first evaluate the condition
while (▷val) βstm →if val then pos := β else yieldUp(Norm)
while (αtrue) ▷Norm →yieldUp(bdy)
-- iterate the while stm
where bdy is the program body(up(pos)) of the while statement

3.2 Sequential Imperative Procedural Program Runs
47
In the imperative sublanguage of Java a statement execution can be
abrupted before its normal completion, namely due to the jump statements
break lab or continue lab which are syntactically constrained to appear in
a statement labeled by lab. When such an Abruption of the normal control
flow happens at a node pos, the interpreter yields Break(lab) or Continue(lab)
and with this information walks up the syntax tree through all the enclosing
statements until it finds a statement labeled with lab. All the statements it
passes through terminate abruptly. Reaching the statement with continua-
tion label labc the execution continutes with the corresponding method body
(which is avaiable at pos). The concept of abruption propagation is defined in
such a way that it can be easily refined to other forms of abruption (like re-
turn from procedure calls in JavaC and handling exceptions in JavaE). Here
it means that every statement except labeled ones propagate the constrained
jump statement abruptions in Java programs.
propagatesAbr(phrase) iff phrase ̸= lab : stm
This explains the definition of the BreakContinueStm interpreter:
lab : αstm →pos := α
-- labels do not influence normal execution
lab : ▷Norm →yieldUp(Norm)
break lab; →yield(Break(lab))
-- record abruption of normal ctl flow
continue lab; →yield(Continue(lab))
lab : ▷Break(labb) →if lab = labb
then yieldUp(Norm)
-- jump target stm terminates normally
else yieldUp(Break(labb))
-- propagate abruption upwards
lab : ▷Continue(labc) →if lab = labc
then yield(body(pos))
-- proceed iterating body
else yieldUp(Continue(labc))
-- propagate abruption upwards
phrase(▷abr) →if pos ̸= ﬁrstPos and propagatesAbr(restbody(up(pos)))
then yieldUp(abr)
-- up(pos) is not a labeled stm
3.2.2 Procedural Constructs (Class Model JavaC)
JavaC is the extension of JavaI by class (also called static) fields, methods
and initializers, traditionally called global variables, procedures (subroutines)
and module initializers. Correspondingly the syntax of JavaC adds to the
syntax of JavaI (indicated by . . .) expressions with global variables and pro-
cedure calls (method invocations), as statements method return and trigger
of static initializers.
Exp = . . . | Class.Field | Invk
Asgn = . . . | Class.Field = Exp

48
3 Control Structures of Single-Process Runs
Invk = Class.meth(Exps)
Exps = Exp1 . . . Expn
Stm = . . . | Invk; | return Exp; | return;
Phrase = . . . | static Block
To describe the dynamics of these class concepts we refine JavaI by enrich-
ing the state for JavaC and adding corresponding new expression evaluation
and statement execution rules grouped in JavaExpC and JavaStmC.
Class related state elements for JavaC. As for JavaI we refer for the
static part of JavaC to the literature and mention here only those static state
functions that appear directly in the new rules. One of them is the function
super(C) which yields the direct superclass of a given class. A function body
associates with each method C/msig of a given signature in a given class
its body. C/MSig denotes the set of method signatures declared in C, i.e.
method names with arity and argument types.
super : Class →Class
body : Class/MSig →Block
globals : Class/Field →Val records the current value of class variables.
The currently executed method is denoted by meth with values in C/MSig.
We use a stack of method frames: when the current method makes a call, its
frame—consisting of meth, the restbody, the current position and locals—
is pushed on the stack and will be resumed once the called method has
terminated.
Frame = (Class/MSig, Phrase, Pos, Locals)
frames ∈Frame∗
Concerning its initialization a class can be in one of four states: the class is
Linked but its initialization did not start yet, its initialization is InProgress,
the class is Initialized or Unusable (due to an initialization error). The ini-
tialization must be performed at the first use of a class (or interface). Being
InProgress is considered as being initialized because during the execution of
a static initializer the access to class variables or calls of class methods do
not count as a first use of the class.
ClassState = {Linked, InProgress, Initialized, Unusable}
classState : Class →ClassState
initialized(c) iff classState(c) ∈{Initialized, InProgress}
We now apply to JavaC the definition scheme introduced above for JavaI.
Class expressions evaluation machine JavaExpC. The new expres-
sion evaluation rules are defined instructionwise:
JavaExpC = case context(pos) of
GlobalVar
-- evaluation of global variables
GlobalAssign
-- assignments to global variables
MethCall
-- execution of method calls
SeqExp
-- evaluation of sequences of expressions

3.2 Sequential Imperative Procedural Program Runs
49
GlobalVar differs from recording values of local variables in the LitVar
subrule of JavaExpI not only by using globals instead of locals, but also
because the recording takes place only when the class of the global variable
is initialized. If it is not, first the interpreter must Initialize(c), using the
submachine defined below. This explains the following rule GlobalVar.
c.f →if initialized(c) then yields(globals(c/f )) else Initialize(c)
GlobalAssign refines VarAssign in JavaExpI the same way.
c.f = αexp →pos := α
c.f = ▷val →if initialized(c) then globals(c/f ) := val
yieldUp(val)
else Initialize(c)
When executing MethCall first the sequence of arguments must be
evaluated. Only then can the interpreter proceed to InvokeMethod with
the computed parameter values, preparing the return from that call to the
parent position up(pos) in the caller method. In case the method’s class is
not yet initialized, first the initialization must be performed. This explains
MethCall.
c.mα(exps) →pos := α
c.m▷(vals) →if initialized(c)
then InvokeMethod(up(pos), c/m, vals)
else Initialize(c)
The rules in SeqExp to evaluate a sequence of expressions follow the left-to-
right strategy used in Java.
() →yield([ ])
-- no parameters
(α1exp1 . . .αn expn) →pos := α1
-- left-to-right evaluation order
(α1val1 . . .▷vali αi+1expi+1 . . .αn expn) →pos := αi+1
(α1val1 . . .▷valn) →yieldUp([val1 . . . valn])
-- pass vals to parent
It remains to define the submachine Initialize(c). It calls the static ini-
tializer c/ < clinit > (without parameters and with return position pos, to
proceed upon return where the initialization was triggered), assigns the de-
fault values to the global variables and sets classState(c) to InProgress, value
that is replaced by Initialized at the exit from the static initializer method.
Initialize(c) =
if classState(c) = Linked then
classState(c) := InProgress
forall f ∈staticFields(c) globals(f ) := defaultVal(type(f ))
InvokeMethod(pos, c/ < clinit >, [ ])

50
3 Control Structures of Single-Process Runs
The submachine InvokeMethod pushes the current frame on the frame
stack frames and defines a new current frame for the execution of the invoked
method’s body. The argument values are bound in locals to the method’s
formal parameters.6
InvokeMethod(nextPos, c/m, values) =
frames := push(frames, (meth, restbody, nextPos, locals))
meth := c/m
restbody := body(c/m)
-- syntax tree initialization
pos := ﬁrstPos(body(c/m))
-- start at the root of the syntax tree
locals := ((x1, v1) . . . (xn, vn))
-- binding values to params
where (x1, . . . , xn) = argNames(c/m) and (v1, . . . , vn) = values
Notice that in this refinement, ﬁrstPos — which has been so far a pre-
initialized static value — becomes a dynamic function; previous occur-
rences can be interpreted as ﬁrstPos(body(meth)) with meth initialized to
initialClass/main.
Class statements execution machine JavaStmC. The new proce-
dural statements are triggering a StaticInitializer to be executed and
ReturnStms, including the report of normal method body execution to the
caller.
JavaStmC = case context(pos) of
StaticInitializer
ReturnStm
Before executing a static initialization statement the StaticInitializer
must check whether the superclass of the current class c (if it exists, i.e. if
c ̸= Object) is already initialized. If not, the initialization of the superclass
must be called. The return from the static initializer triggers a new kind of
abruption, namely Return which will trigger to ExitMethod (see below).
This explains the following definition of StaticInitializer.
static αstm →let c = className(meth)
if c = Object or initialized(super(c)) then pos := α
else Initialize(super(c))
static αReturn →yieldUp(Return)
There are two new reasons for abruption, namely Return and Return(val)
occurring upon execution of return statements. Therefore the Abruptions
set now contains four kinds of abruption:
Abr = Break(lab) | Continue(lab) | Return | Return(val)
The return from a method triggers an abruption to exit from the cur-
rent method. Instead of transfering the control directly back to the caller
6 We skip here the submachine InvokeNative that plays a role only in the concurrent
extension JavaT.

3.2 Sequential Imperative Procedural Program Runs
51
of the method we refine the abruption mechanism of JavaI and propa-
gate the Return abruption up to the ﬁrstPos of the current method body
to ExitMethod there. This prepares a simple refinement of the propaga-
tion mechanism to include the execution of what is called finally code which
may be present in the method and is dealt with in the exception handling
module JavaE of Java we do not show here. Return from class initializers is
dealt with by a StaticInitializer rule and therefore is excluded from this
propagation. Thus propagatesAbr is refined as follows:
propagatesAbr(phrase) iff phrase ̸= lab : stm and phrase ̸= static stm
ReturnStm =
return αexp; →pos := α
-- evaluate returned exp
return ▷val; →yieldUp(Return(val))
-- throw abruption to parent
return; →yield(Return)
lab :▷Return →yieldUp(Return)
-- propagate abruption upwards
lab :▷Return(val) →yieldUp(Return(val))
Return →if pos = ﬁrstPos(body(meth)) and frames ̸= [ ]
-- meth terminated normally and there is a caller
then ExitMethod(Norm)
Return(val) →if pos = ﬁrstPos(body(meth)) and frames ̸= [ ]
then ExitMethod(val)
▷Norm; →yieldUp(Norm)
-- pass normal termination upwards
It remains to define the ExitMethod(result) submachine. It reestablishes
the invoker’s frame popped from the frames stack passing also the result in
case the terminated method is not a class initialization method, otherwise it
updates the classState of the initialized class to Initialized.
ExitMethod(result) =
let (oldMeth, oldRestBdy, oldPos, oldLocals) = top(frames)
method := oldMethod
pos : oldPos
locals := oldLocals
if methName(meth) = “ < clinit > ” and result = Norm then
restbody := oldRestBdy
classState(className(meth)) := Initialized
else
restbody := oldRestBdy[oldPos/result]
frames := pop(frames)
where oldRestBdy[oldPos/result] is obtained by replacing
the value of oldRestBdy at argument oldPos by result
This ends the instructionswise platform-independent definition of the pro-
cedural sublanguage of Java, refining the imperative component JavaI (with
assignment and sequential control constructs seq, if, while, goto) by adding

52
3 Control Structures of Single-Process Runs
JavaC rules which define the semantics of procedural language concepts.
Using the same method this model has been extended in [159] by model-
ing object-oriented features (JavaO), exception handling (JavaE) and con-
currency (thread model JavaT), covering the entire Java language of the
time. This model Java has been reused in [45] to formulate a rigorous model
for the ECMA standard of C#. In Sect. 5.4.2 we explain a general context-
sensitivity-based method to pass from single-agent sequential to multi-process
concurrent runs. We illustrate it by defining the multithreaded JavaT com-
ponent and using it to pass from the single-thread interpreter Java to a
multi-core-inspired model with truly concurrent runs of Java threads.
3.3 Input Driven Sequential Runs
Definition 22 (pg. 40) of sequential runs makes it possible to use ASMs di-
rectly as reactive processes. If in a sequential run of a machine the interaction
with the environment is reduced to define the initial state we say that the
run (or the machine) is input-driven. In such runs, once they are started,
the machine does not interact any more with the environment (except for
possibly generating some output the environment can read).
The number computations performed by Turing’s original machines in
[162] are input-driven runs: they all start with the empty tape and in a non-
terminating run output bitwise each an infinite 0-1-sequence that represents
the computed real number in binary decimal notation. The huge variety of al-
gorithms to compute functions perform input-driven output-generating runs,
coming with a notion of final state where their computations may terminate
and output the function value for the input. We call them i/o-algorithms or
machines.
Definition 23 (i/o-algorithm). An input/output algorithm A (or machine)
is an input-driven output-generating sequential algorithm, meaning that the
role of the environment in runs of A is reduced to define the initial state of a
computation—dynamically by assigning input to monitored locations and/or
statically via the definition of a class of initial states—and to possibly receive
some output (usually if and when the computation terminates in some final
state). We call also such runs to be input-driven and output-generating. The
signature of such algorithms A is the disjoint union Σ = Σin ∪Σctl ∪Σout
of monitored functions that are used only for the initial input, of controlled
(read: local) functions and of output functions. We say that initial states are
input-dependent if they are defined by the input in monitored locations (with
content undef in the other locations).
To follow standard notation, when investigating i/o-machines we some-
times use as locations also variables which appear as input resp. output pa-
rameters, although the parameters are not (0-ary) function symbols.

3.3 Input Driven Sequential Runs
53
We now illustrate runs of i/o algorithms by runs of structured ASMs
which compute the computable number-theoretic (also called partial recur-
sive) functions.
3.3.1 Computable Functions (Input/Output Machines)
It is often claimed that using equations to compute functional values is less
error prone than computing these values by (programs executed by) ma-
chines. This seems to be a question about the structure of the terms that are
involved in the computation steps and about the level of abstraction where
the computation is formulated. We define in this section a class of i/o ASMs
whose compositional structure reflects a widely used ordering for the compu-
tation of subterms of functional terms that purely equational definitions do
not show. In fact this order appears as ‘natural’ to us because we have learnt
it at school and have been drilled to follow it.
We use two well-known structured sequential programming operators seq
and while and apply them to inductively define structured i/o ASMs, starting
with assignment instructions. To illustrate the flexibility for choosing the
desired level of abstraction of a specification we define seq and while hiding
all internals—all the intermediate steps which appear explicitly in the control
state ASM flowchart notation for the machine components, see Fig. 2.4 of
Example 7, pg. 31—so that executing M seq N appears in a black-box view
as one (called a macro-) step; the same for while cond do M. It implies that
during a macro step the environment is assumed to make no step.7
Definition 24 (ASM Operator seq). For ASMs M, N with signature Σ
we define M seq N so that for every state A over Σ, first M computes
its update set U in A to construct its successor state B = A + U (if U is
consistent) and then N computes its update set V in state B to construct
the M seq N-successor state of A (if V is consistent). Since N may overwrite
some updates of M, the update set M seq N computes consists of V plus all
those updates in U that are not overwritten by N. It is the update set which
appears for the outside world as produced by M seq N in one atomic step.8
eval(M seq N, A) = eval(M, A) ⊕eval(N, A + eval(M, A))
where
U ⊕V =
(
U
if U is inconsistent
V ∪{(loc, val) ∈U | loc ̸∈Locs(V )}
otherwise
7 Similarly we assume for logical reasons not explained here (but see [58, Sect. 8.1.4])
that these machines are deterministic, i.e. that they do not use the choose construct
added to ASMs in Sect. 3.4.
8 We skip mentioning the variable environments ζ because the ASMs we consider here
are without free variables.

54
3 Control Structures of Single-Process Runs
By this definition when U is inconsistent, neither A + U nor A + (U ⊕V )
(because of U ⊕V = U) are defined so that the computation gets stuck.
Exercise 9. Convince yourself that the seq operator is associative. This
property justifies the parenthesis-free notation of machines composed by seq.
Consider now iterations (read: successive firings) of an ASM M, starting
in some state A. We write M n for n iterations of M, i.e. M 0 = skip and
M n+1 = M n seq M.
Definition 25 (ASM Operators iterate and while).
The iteration of
M started in a state A is computed by the sequence of states
B0 = A
Bi+1 = Bi + eval(M, Bi)
that is said
• to terminate at the first integer n where eval(M, Bn) is either ∅(in which
case the iteration is said to terminate with success) or inconsistent (in
which case the iteration is said to terminate with failure), if there is such
an n.9 If the iteration terminates, then
eval(iterate M, A) = (Bn −A) ⊕eval(M, Bn)
• to diverge else.
In the success case we say that Bn is the successor state of A computed by
iterate M.
The iterate operator has the effect of executing M repeatedly across mul-
tiple steps, until a step produces either an empty or an inconsistent update
set. However, such steps and corresponding intermediate states Bi are in-
ternal to the execution of the iterate, whereas from the point of view of
the outer machine, only a single step is performed, which produces all the
updates needed to reach the final state of the iteration (see Fig. 3.1 where
we have Ui = eval(M, Bi)). Notice that this cumulative update set will be
consistent if the iteration terminated in success, or inconsistent if it termi-
nated in failure. If the iteration diverges, then the result of its evaluation is
undefined (and in fact, there is no successor state).
The while operator can then be defined as an instance of iterate:
while cond do M = iterate (if cond then M)
Remark. Such while computations terminate with success upon the first
encounter of a state where the condition is false. However, the computation
9 Including the case of an update set that does not change the given state, i.e. Bn +
eval(M, Bn) = Bn, is an alternative termination criterion we do not use here. When
using ASMs to compute functions a natural alternative Stop(S) criterion is that an
initially undefined output location is for the first time defined in state S. We use this
criterion below for structured input/output ASMs.

3.3 Input Driven Sequential Runs
55
also succeeds if in some of the iteration steps M yields an empty update
set; furthermore the computation fails if in some of the iteration steps M
yields an inconsistent update set. So to obtain while steps with the usual
meaning the iteration body M should be designed not to yield an empty or
any inconsistent update set.
Fig. 3.1 Internal and external steps of an iterate that terminates in success
A
B0
B1
· · ·
Bi
· · ·
Bn
Bn+1 = Bn
A′
M
U0
M
U1
M
Ui−1
M
Ui
M
Un−1
M
Un = ∅
M n
(Bn −A) ⊕Un
Exercise 10. Consider Fig. 3.1, which depicts the case of a iterate termi-
nating with success. A′ denotes the state that results from a move of M n in
state A. Convince yourself that the definition of eval(iterate M, A) is sound
also in the case in which Un is inconsistent (the case for termination with
failure), even if in that case the states Bn+1 and A′ are of course not defined.
Exercise 11. Check your understanding of while: determine whether the
following ASMs started in a state A terminate with or without success or
diverge. See Appendix B for the answer.
while cond do skip
while false do M
while true do a := a
while true do a := 1
a := 2
In the following definition we tailor the signature of ASMs for computing
partial recursive functions.10 The encapsulation properties of seq and while
are then exploited to literally paraphrase each application of an equation
defining a partial recursive function by one step of those machines.11
Definition 26 (Structured Input/Output ASMs). A structured in-
put/output ASM M (used to compute computable functions over Nat and
10 The definition comes from [58, Ch. 4] and will be reused for Def. 46 (pg. 203) of the
Arithmetical State Postulate.
11 In [58, Sect. 4.1.3] it is shown that each step of such a ‘turbo’-ASM M can be
computed by an ASM MicroStep(M) which (starting with M and an empty update
set) step-by-step computes the update set accumulated so far by M and the still to be
executed rules of M.

56
3 Control Structures of Single-Process Runs
therefore a deterministic ASM) is defined as an ASM with a signature consist-
ing of only one monitored (a 0-ary) function inM (to store input sequences
of natural numbers), only one output (a 0-ary) function outM (to store a
natural number), a finite number of controlled functions and of some clearly
computable so called initial functions of recursion theory (see below). In its
initial states every dynamic function except inM is undefined. Its rule is built
from unguarded assignment instructions applying finitely often seq, while
and par.
M computes a partial function f : Natn →Nat if for every argument
x ∈Natn the machine, starting an input-driven run in the initial state with
inM = x, reaches after a finite number of steps a state where outM gets
updated for the first time, namely to outM = f (x), if and only if f (x) is
defined.
The initial functions of recursion theory are the following functions of nat-
ural numbers: +1, the projection functions U n
i (x1, . . . , xn) = xi, the constant
functions C n
i (x1, . . . , xn) = i and the characteristic functions of the predicate
= 0 and of the smaller-than relation <.
Theorem 1 (Structured Programming). Every partial recursive function
f on natural numbers can be computed by a structured input/output ASM
StructInOut(f ).
Proof. We use Kleene’s [112] equational definition of the partial recur-
sive functions which starts from the initial functions of recursion theory
and applies finitely often equations for simultaneous substitution, primi-
tive recursion and the minimal-zero-point operator.12 So one can define
StructInOut(f ) by induction on the definition of f .
Basis of the induction. Each initial function f is computed by an assign-
ment instruction that reflects the defining equation for f :13
StructInOut(f ) = (outf := f (inf ))
Inductive step. By the functional notation M(x) we indicate the program
we intend to use for an input-driven run of M to compute the functional
value for input x:
Abbreviation. M(x) = ((inM := x) seq M)
To transfer the output value computed by M(x) from outM to a different
location result we write:
Abbreviation. result ←M(x) = (M(x) seq result := outM)
12 In recursion theory this operator is called µ-operator and partial recursive functions
are also called µ-recursive functions.
13 As shorthand we index the in/out functions with the function name f to stand for
the longer rule name StructInOut(f ).

3.3 Input Driven Sequential Runs
57
Case 1: substitution. Let f be defined by simultaneous substitution of
functions hi into a function g, i.e. f (x) = g(h1(x), . . . , hm(x)). Then f (x) can
be computed by first computing for every hi its value for input x = inf ,
independently of each other so that these subcomputations can be done
in parallel,14 and then the value of g for the computed intermediate val-
ues (outH1, . . . , outHm). This order of using the given equations to compute
f is made explicit by the following definition of a structured input/out-
put ASM StructInOut(f ). Let G, H1, . . . , Hm be structured input/output
ASMs which compute respectively g, h1, . . . , hm by induction hypothesis.
StructInOut(f ) =
par (H1(inf ), . . . , Hm(inf )) seq outf ←G(outH1, . . . , outHm)
The structure of StructInOut(f ) reflects the structure of the term on
the right side of the defining equation for f : simultaneous substitution reveals
its nature of a parallel computation step followed by one ‘equational’ step.
Case 2: primitive recursion. Let f be defined by primitive recursion
from g, h, i.e.
f (x, 0) = g(x), f (x, y + 1) = h(x, y, f (x, y))
Let G, H be structured input/output ASMs which compute respectively g, h
by induction hypothesis. To computate f (x, y) one can start with computing
f (x, 0) as first intermediate value ival, using the first equation for f , and
setting the recursor to 0; then one can iterate the computation of f (x, rec+1),
using the second equation for f and increasing the recursor, until rec = y:
in this moment ival = f (x, y) so that the result can be copied to the output
location.
StructInOut(f ) =
let (x, y) = inf
par (ival ←G(x), rec := 0)
-- basis: run G, initialize recursor
seq while rec < y do
par (ival ←H(x, rec, ival), rec := rec + 1)
-- a recursion step
seq outf := ival
-- output final intermediate value
Note that it is only for better readability that we use the let-notation
instead of writing U n+1
i
(inf ) for xi with (1 ≤i ≤n) and U n+1
n+1 (inf ) for y
where inf = (x1, . . . , xn, y).
The structure of StructInOut(f ) makes this order for applying the defin-
ing equations for f explicit: first perform the basis step—computing the right-
hand side of the equation with recursion parameter 0—and then use step-by-
step the right-hand side of the equation with positive recursion parameter
until this parameter reaches the recursor value y of the input of f .
14 For simplicity of exposition but without loss of generality assume that the function
computing submachines have pairwise disjoint signature.

58
3 Control Structures of Single-Process Runs
Exercise 12. Define a structured input/output ASM for the case of primitive
recursion where the computation of f (x, y) uses the defining equations in the
following order:
f (x, y)
= h(x, y −1, f (x, y −1))
...
= h(x, y −1, h(x, y −2, . . . h(x, 0, f (x, 0)) . . .))
= h(x, y −1, h(x, y −2, . . . h(x, 0, g(x)) . . .))
Compare this order with the simpler sequence (ivali)i≤y that is computed by
the above machine StructInOut(f )) to compute f (x, y):
ival0 = g(x)
ivali+1 = h(x, i, ivali)
Case 3: minimal-zero-point operator. Let f be defined by the µ-
operator from g, i.e. f (x) is the smallest y such that g(x, y) = 0. The equa-
tional definition suggests to perform the computation of f (x) as follows: start
with computing g(x, 0) and setting the recursor to 0; then iterate the com-
putation of values g(x, rec + 1) until the value 0 shows up in which case the
current recursor value rec = f (x) can be copied to the output location.
The structure of the following definition of StructInOut(f ) makes this
order explicit. Let G be a structured input/output ASM which computes g
by induction hypothesis.
StructInOut(f ) =
par (G(inf , 0), rec := 0)
seq while outG ̸= 0 do par (G(inf , rec + 1), rec := rec + 1)
seq outf := rec
Remark. The definitions of StructInOut represent patterns
FctSubst(G, H1, . . . , Hm), PrimRecursion(G, H), MinZeroPoint(G)
to compose structured input/output ASMs out of components that appear
as parameters G, H, H1, . . . , Hm. An occurrence of par indicates where any
sequentialization of the involved simultaneous steps yields the same result.
In purely functional notations this operator is suppressed and it is implicitly
assumed that some sequentialization of the involved independent steps is
chosen for their execution. Note that the PGA normal form (Def. 13, pg. 24)
is structurally flat. Nevertheless, in a sequential context it is easy to simulate
seq and while machines by PGAs.
Exercise 13 (PGAs for seq and while). Define PGAs that simulate
M seq N and while cond do M. Use this to write PGA rules for each of
the three ASM schemes FctSubst(G, H1, . . . , Hm), PrimRecursion(G, H),
and MinZeroPoint(G) to see what is gained by the abstraction. See Ap-
pendix B for a solution.

3.3 Input Driven Sequential Runs
59
Exercise 14 (Random Access Machine). The RAM is a refinement of
the register machine model (pg. 261), equipped with an explicit input/out-
put mechanism. It comes close to the Harvard computer architecture model
and is often used as reference model to investigate complexity properties of
arithmetical algorithms.
Data from an input tape can be copied into registers and the content of
any register (or some integer) can be written to an output tape. All other
data (not ctl) manipulations happen only involving a special register called
the accumulator: register transfers—from other registers into the accumulator
and from there into other registers applying to its content some operation—
and operations involving as one argument the content of the accumulator.
The second argument is either an integer or taken from other indirectly ad-
dressable registers reg in main memory. Those registers serve only as con-
tainers of data that can be loaded into the accumulator, be modified there
and stored from acc into reg.
The main memory is a (potentially infinite) set of registers rn (with
n ∈Nat and acc = r0) whose contents are integers, in initial states assumed
to be 0 (unless otherwise stated). The finite instruction memory where the
to-be-executed program pgm is stored is separated from the (potentially in-
finite) main memory in the sense that during the execution of the program
its instructions can be read and executed but not modified. In other words
pgm is static, i.e. the execution of the program cannot change the program. A
RAM program is a finite sequence of labeled instructions instr with pairwise
different labels lab(instr), an opCode(instr) and (except the Halt instruction)
an operand(instr) on whose interpretation—together with the accumulator
content as first argument (except for jump instructions)— the assembly-like
operation code is executed.
To simplify the presentation but without loss of generality we use as labels
natural numbers 0 ≤l ≤m for some natural number m; 0,1 are usually writ-
ten and interpreted as start respectively halt label. We write instrl or instr(l)
for the instruction with label l so that the currently executed instruction can
be defined by instr(ctl). The commands that can appear as opCode(instr)
are of the following kinds:
• Load and Store for data transfer (between the accumulator and any other
register),
• Add, Sub, Mult, Div for the basic functions +, −, ×, / of integers to be
applied to the content of acc and a second operand (an integer or the
content of another register),
• Jump, Jump(> 0), Jump(= 0) and Halt for the management of an in-
struction counter ctl,
• Read (from the finite input tape into a register) and Write (an integer or
a register content to the potentially infinite output tape).
The operands of instructions are of three types:

60
3 Control Structures of Single-Process Runs
• A Halt instruction has no operand.
• For jump instructions the operand is a label indicating where to jump to
if the jump condition (if any) is satisfied.
• For memory transfer or functional opCode the operand and its interpre-
tation operandVal is defined for the following three cases:
– int i: denotes an integer constant i (except for Store)15
– reg n (for n ∈Nat): denotes the content of register rn
– addr n: denotes the content of the register whose address (also called
index) i is the content of register rn. If i < 0 the machine halts.
operandVal(int i) = i
operandVal(reg n) = content(rn)
operandVal(addr n) =
(
undef
if content(rn) < 0
content(rcontent(rn))
else
The definition of operandVal(addr n) reflects that only natural numbers
are used as register index. If in a computation step an operand addr n appears
where content(rn) < 0, then the computation is required to halt (due to a
register address error). The same for divisions by 0. Note that the function
operandVal has an implicit state parameter, referring to register contents.
Define a PGA rule that interprets the behaviour of the RAM, refining the
RM-rule of the solution for Exercise 5 (pg. 261). For a solution see pg. 263
in Appendix B.
3.4 Nondeterminism and Interleaving
For a computational or a proof step it often suﬀices to perform the step for an
arbitrary element a belonging to some set A, without exploiting any specific
property of the chosen element or any relation it shares with other elements
of the set. Such a selection can be expressed using a choice function ϵ which
applied to a set X provides an element ϵx(X) of X if there is at least one. If
there is none ϵx(X) is undefined so that the function is a partial function; as
an alternative a default value undef can be used.
Hilbert [99] and Ackermann [4] (see also [101, vol. II]) defined and axiom-
atized this choice operator and used the resulting epsilon-calculus in a foun-
dational context. For example, first-order logic can be dealt with in terms of
quantifier-free reasoning involving the ϵ operator:16
forsome x P(x) = P(ϵx(P(x)))
forall x P(x) = P(ϵx(not P(x))).
15 For Store a number operand int i makes no sense.
16 See [8] for foundational and proof-theoretic work using the ϵ operator.

3.4 Nondeterminism and Interleaving
61
The quantifier elimination capability of Hilbert’s and Ackermann’s ϵ operator
is exploited in some contemporary proof engines. In the extensional version of
the epsilon-calculus the epsilon operator assigns the same witness to equiv-
alent formulae, i.e. if forall
x (α(x) iff β(x)) then ϵx(α) = ϵx(β) holds.
But there are other possibilities to correlate different choices, in particular in
computing where multiple choices made in a dynamic context appear. In a
run a choice operator can be used to express an interface condition φ under
which the next step is performed, e.g. an abstract property an element is
required to satisfy to be used for the next step in the run, abstracting from
how such elements are computed. We follow that approach and define the
syntax of the choose construct by the following ASM rule, called choose
rule:
choose x with φ do M
Its meaning is to select—due to the presence of import rules not in the su-
peruniverse |A| but in the current Workspace(A)—an element a that satisfies
the choice condition phi and to execute M with this element a, bound by the
variable environment to x. In this way the choose rule offers to continue the
computation with any one among possibly many update sets M computes in
state A with individual variable environment ζ[x 7→a] for any chosen element
a ∈X satisfying φ. Technically, to extend the semantics of ASMs by a se-
mantics for choose rules it suﬀices to replace the evaluation function, which
provides a unique update set U = eval(M, A, ζ), by an evaluation candidate
relation
evalCand(M, A, ζ, U)
expressing that the evaluation of M in state A with variable environment ζ
offers U as one update set candidate to continue the computation.
Definition 27 (Semantics of choose rules). We extend the class of ASMs
inductively by choose rules, i.e. rules of the form choose x with φ do M
(where M is an ASM). We define their semantics by extending Def. 14 (pg. 32)
as follows: replace in Def. 14 eval(M, A, ζ) = U by the evaluation candidate
relation evalCand(M, A, ζ, U) and add the following inductive clause to define
this relation for choose rules :
if range(x, φ, A, ζ) = ∅
-- there is no element to choose
then evalCand(choose x with φ do M, A, ζ, ∅) holds
-- generates empty update set
else
forall a ∈range(x, φ, A, ζ) and forall U
if evalCand(M, A, ζ[x 7→a], U) holds
then evalCand(choose x with φ do M, A, ζ, U) holds
where
range(x, φ, A, ζ) =
{a ∈Workspace(A) | eval(φ, A, ζ[x 7→a]) = true}

62
3 Control Structures of Single-Process Runs
In the presence of imports import parameters are passed without change
from evalCand(M, . . . , η, (U, η′)) to evalCand(choose x with φ do M, . . .).
ASMs with choose rules are nondeterministic machines. In their sequen-
tial runs each time they execute a choose rule this may offer multiple up-
date sets U (to continue the run by one more step if U is consistent),
namely all those update sets Ua that for an element a ∈range(x, φ, A, ζ)
satisfy the relation evalCand(M, A, ζ[x 7→a], Ua). Thus the corresponding
run can have multiple continuations A + Ua, each computable in single M-
steps A
1⇒M A + Ua. This means that the stepM function of deterministic
ASMs is refined to a relation of state pairs (A, A+Ua) where a is an element
of range(x, φ, A, ζ) and Ua is a consistent update set for which the relation
evalCand(M, A, ζ[x 7→a], Ua) holds. We summarize this by the following
definition.
Definition 28 (Successor State Relation for ASMs with choose).
For every state A its (possibly multiple) successor states computed by any M
in the presence of choose (and import) rules are defined as follows, refining
the stepM function in Def. 14 (pg. 32) to a relation:
forall U, η
if evalCand(M, A, ∅, ∅, (U, η)) holds and Consistent(U)
then nextStateM(A, A + U) holds
In most applications the choose construct is used only with non-empty
sets to choose from. To catch error situations where there is no a ∈
Workspace(A) satisfying φ a good discipline is to use the construct always
together with a dedicated ifnone N clause; if such a clause is provided, and
there is no (a, U) satisfying the conditions above, then for every update set
U ′ such that evalCand(N, A, ζ, U ′) holds also
evalCand(choose x with φ do M ifnone N, A, ζ, U ′) holds.
Notational remark. When the choice condition φ is restricted to being
an element of a set X we simply write choose x ∈X do M. We use other
familiar notational shorthands without further explanation; for example, with
power sets we write choose A ⊆B instead of choose A in P(B).
Example 8. Nondeterministic Turing Machine. A nondeterministic Tur-
ing machine differs from its deterministic version by some choice, for example
in updating the control state ctl. Refining the function nextCtl in Example 5
(pg. 26) to a function with range PowerSet(Ctl) instead of Ctl yields the
following behavioural description of nondeterministic TMs:
NonDeterministicTuringMachine(write, nextCtl, move) =
tape(head) := write(ctl, tape(head))
-- read and write on the tape
head := move(ctl, tape(head))(head)
-- move reading device
choose c ∈nextCtl(ctl, tape(head)) do
-- choose next control state
ctl := c
-- go to the chosen control state

3.4 Nondeterminism and Interleaving
63
Note that this choice in Turing machines is bounded, i.e. it offers only a finite
number of choices that depends only on the program and the alphabet.
Exercise 15. How can a NonDeterministicTuringMachine be config-
ured and specified to become able to choose also for the writing and move
actions?
Non determinism occurs frequently in computing, but it not necessarily
increases the computational power. For example, deterministic Finite State
Machines (see Exercise 3, pg. 26) and their non-deterministic variant com-
pute the same class of languages [140]; also every non-deterministic Turing
machine can easily be simulated by a deterministic TM (Exercise 16). But
restricting TM computations by a polynomial time bound leads to two classes
P respectively NP of sets—those decidable by polynomial-time-bounded de-
terministic respectively non-deterministic Turing machines—for which it is a
longstanding open problem whether they coincide. See Sect. 9.1 for more on
this theme.
Exercise 16. Explain how every non-deterministic Turing machine can be
simulated by a deterministic Turing machine.
As one justification for the definition of PGA rules (Def.12, pg. 24) we
mentioned the axiomatic sequential process characterization that leads to
an ASM version of Turing’s thesis, as we will explain in Sect. 8.1. With
some small changes the characterization remains valid if a ‘bounded choice’ is
allowed in sequential process descriptions. To capture this we define bounded-
choice PGAs.
Definition 29 (Bounded-choice PGAs). Bounded-choice PGAs are de-
fined by adding to the inductive definition of PGAs (Def. 12, pg. 24) a clause
that applies choose to a finite number of (already defined bounded-choice
PGA) rules Mi:
choose M ∈{M1, . . . , Mn} do M
Notationally we write such a rule also as chooseOneOf {M1, . . . , Mn} or
M1 or . . . or Mn. Bounded-choice PGAs have the following normal form:17
if φ1 then choose rule ∈{rule1,1, . . . , rule1,l1} do rule
...
if φk then choose rule ∈{rulek,1, . . . , rulek,lk} do rule
where
rulei,j consists of finitely many parallel assignment instructions
17 See the proof of Corollary 1 (pg. 202).

64
3 Control Structures of Single-Process Runs
Example 9. Alternative and Repetitive Guarded Commands. Dijkstra
proposed in [67] what he called an ‘alternative construct’ if G0 →S0 |
G1 →S1... | Gn →Sn ﬁand a ‘repetitive construct’ do G0 →S0 |
G1 →S1... | Gn →Sn od of sequential program statements Si whose
execution is guarded by boolean expressions Gi. Both constructs describe
some non-deterministic sequential program behaviour. Their semantics can
be easily defined by non-deterministic PGA-ASMs as follows, bypassing Di-
jkstra’s weakest pre-condition approach. Let condi be Boolean expressions,
Mi PGA-ASMs and Abort a description of Dijkstra’s abort command.
AlternativeCmd((condi, Mi)i≤n) =
choose i ∈{0, . . . , n} with condi do Mi
ifnone then Abort
where
ifnone = if thereisno i ∈{0, . . . , n} with condi
The repetitive construct repeats the choice until a state is reached where
no more choice is possible.
RepetitiveCmd((condi, Mi)i≤n) =
while forsome i ∈{0, . . . , n} condi do
choose i ∈{0, . . . , n} with condi do Mi
Note that the behaviour of RepetitiveCmd in case ifnone is that of skip
(by Def. 27, pg. 61), as required by Dijkstra’s definition. The same holds if
instead of the macro-step while operator the control state while cond M
construct of Example 7 (pg. 30) is used.
We generalize the choice among different actions from Dijkstra’s guarded
commands to arbitrary ASMs Mi with the following notation:
chooseOneOf ({M0, . . . , Mn}) =
choose i ∈{0, . . . , n}
Mi
Correspondingly we adapt the graphical representation of control state
ASMs from Fig. 2.2 (pg. 28) to the following Fig. 3.2 (where for the sake of
notational succinctness we write choose instead of chooseOneOf):
Example 10. Alternative Statement in Occam.
The programming language Occam [69] has an alternative command alt
(G1 S1 . . . Gn Sn) that instantiates the if-part of Dijkstra’s AlternativeCmd
but replaces the else-part Abort by a waiting mechanism; this mechanism
is specified in the guards to wait for a communication with other agents to
happen or for a state property or a time constraint to be satisfied.
Consider Occam programs as executed by agents a which walk through
the flowchart representation of their program; each of them carries its own
environment (internal state in which to evaluate expressions eval(exp, env(a))

3.4 Nondeterminism and Interleaving
65
Fig. 3.2 Choose Control State ASM Diagram
choose
M0
. . .
Mn
and to bind(id, env(a)) identifiers id to a variable or channel) and is posi-
tioned at each moment on its currently visited node pos(a). We abbrevi-
ate bind(id, env(a)) to id when env(a) is clear from the context. Each node
is decorated by a to-be-executed command cmd(node). A node with alter-
native (guarded) command alt (G1, . . . , Gn) has possible successor nodes
next(node, i) decorated by command Si, see Fig. 3.3.
Fig. 3.3 Occam alt (G1 S1 . . . Gn Sn) statement diagram
alt(G1, . . . , Gn)
· · ·
choose one
done
S1
Sn
Guards come in three forms:
• A StateGuard Gi consists of a StateCondition condi that is requested to
evaluate to true.
• A TimeGuard has the form Gi = condi : time?after t requesting besides
the StateCondition also a TimeCondition the clock of the executing agent
a has to satisfy, namely timer(a) > eval(t, env(a)).
• A ComGuard has the form Gi = condi : chani ? vi with an additional
communication request chani ? vi to be satisfied by a ComCondition (in
addition to the StateCondition), expressing that the executing agent a

66
3 Control Structures of Single-Process Runs
wishes to receive for its variable vi some data via the channel chani (to
which chani is bound) and that there is another agent b with command
c!t ready to communicate its value of t via the channel c = chani.
To execute an alternative command for some i the StateCondition condi
must evaluate to true in the requestor’s environment. If Gi is a StateGuard,
the truth of condi suﬀices to let agent a proceed to execute Si. Similarly
in case Gi is a TimeGuard for which timer(a) > eval(t, env(a)) is satisfied.
To satisfy in addition the ComCondition, in case Gi is a ComGuard, agent
a and some other agent b must agree on the channel (i.e. both must bind
the channel identifier chani respectively c in their respective environment to
the same channel). If this is true for some i and b, the data transmission (if
chosen) can take place, agent a proceeds to execute Si and the communication
partner b proceeds to execute its next statement.
This explains the following definition of alternative commands in Occam.18
For i = 1, . . . , n let Gi be condi or condi : chani ? vi or condi : time?after t.
Alt0(a, (G1, . . . , Gn)) =
if a does alt (G1, . . . , Gn) then
choose i ∈{1, . . . , n} with Ready(Gi) do
pos(a) := next(pos(a), i)
-- a proceeds with i-th alternative
if ComGuard(Gi) then
-- com partner requested
let b, c, s with b does c!s and chani = c
vi := eval(s, env(b))
-- data transfer to the variable
pos(b) := next(pos(b))
-- b proceeds with next command
where
Ready(Gi) =
(StateGuard(Gi) and StateCond(Gi))
or (ComGuard(Gi) and StateCond(Gi) and ComCond(Gi))
or (TimeGuard(Gi) and StateCond(Gi) and TimeCond(Gi))
StateCond(Gi) = (eval(condi, env(a)) = true)
ComCond(Gi) = forsome b, c, s
b does c!s and chani = c
TimeCond(Gi) = timer(a) > eval(t, env(a))
a does instr =
a ∈Agent and mode(a) = running and cmd(pos(a)) = instr
Example 11. Interleaving. Often, in particular for machine assisted proofs
of properties of multiprocess algorithms, as computational model interleaving
is used where in each step an arbitrary process among those which can make
a step is chosen to be executed on one processor (read: by one agent). For
18 Note the usual assumption on Occam programs that any two agents are connected by
a parent chain (see Example 13 on pg. 116 with the Occam Par rule) if in any states they
could both input or both output on the same channel (Occam Channel Assumption).
This implies that in a same state they cannot be both in running mode both trying to
communicate with a same agent on the same channel.

3.4 Nondeterminism and Interleaving
67
every set of Processes and a notion of being ReadyToExecute—that is delib-
erately left abstract to be refinable by scheduling schemes of interest—one
can describe this interleaving paradigm by the following scheme:
Interleav(Process) =
choose M ∈Process with ReadyToExec(M) do M
The Interleaving machine specifies by its nondeterministic sequential
runs the multitasking view of runs of multiple sequential processes on one pro-
cessor: the processor switches between the multiple tasks to execute them.
Interleav(Process) as formulated here switches step by step, but it suf-
fices to refine the choice condition ReadyToExec to obtain the usual time-
sharing versions and also sophisticated cooperative or preemptive multitask-
ing schemes as they appear in operating systems. Since we explain below in
detail the relation between the states of concurrent processes (Sect. 5.4) we
do not enter here the issue that every process switch typically implies also a
context switch whereby the state of the to-be-switched process is saved and
the state of the next to-be-executed process is reloaded.
In case the interleaved processes do not work on completely separated
states but interact via input/output and shared locations the question is
which of the ‘sequential multiprocess runs’ of Interleav(Process) should
be considered as sequentially consistent, given that the interaction creates an
access order to shared locations which may result in dependencies between the
component processes. Here are two natural constraints (discovered in [116]):
• in the sequential multiprocess run for each agent a its moves appear in
the order in which they appear in its given sequential run (property called
preservation of sequentiality of the component processes);
• each time in the sequential multiprocess run a component process reads
a location, the value read is the most recently written value in the mul-
tiprocess run (property called read-freshness).
For a generalization of sequential consistency to ASM runs the read-freshness
property must include the effect of environment moves which by Def. 22
(pg. 40) alternate with internal moves. This means that one can allow any
interleaving where between two successive moves mn, mn+1 of an agent a,
the moves of other agents can be arranged in any sequentiality preserving
order as long as their execution computes the effect of the environment move
between mn and mn+1, as perceived by agent a. This leads to the following
definition.
Definition 30 (Sequentially Consistent Interleavings). Let Process be
a set of pairs (a, asm(a)) of agents a ∈A each equipped with a PGA asm(a).
A run R of Interleav(Process) is called sequentially consistent if it preserves
the sequentiality of the component processes (a, asm(a)) and satisfies the
read-freshness property.

68
3 Control Structures of Single-Process Runs
More precisely stated, for every agent a ∈A its moves mn(a), coupled in
its given single-agent sequential run A0(a), A1(a), . . . with the environment
moves mn(environ(a)), appear in the same order in R (restricted to the sig-
nature Σa of a). Furthermore, for each location a reads in state An(a)—to
make its move mn(a) leading to its next internal state A′
n(a) with successor
state An+1(a)—the value a reads is the last-written value into that location
in R.
Note that the states An in a sequentially consistent run are not states any
single agent a can see. a sees only the restriction An ↓Σa to its own signature.
It is for reasons of analysis that we speak about viewing Interleav(Process)
as a multiprocess system. For a truly concurrent (not only multitasking)
computation model the single processor which manages the interleaving must
be replaced by multiple processors (see Ch. 5).
Internal versus external choice. Often the choice performed by the
choose x with φ in M construct is called an internal choice, to be distin-
guished from an external choice let x = select({y | φ(y)}) in M expressed
by a background function select that is assumed to provide an element that
satisfies φ (if there exists one). External selection functions may be static (in
which case the run of the agent becomes deterministic) or dynamic (thus pre-
serving non-determinism, in a way that may be constrained by background
stipulations as usual).
3.5 Synchronous Parallel Step Control
The par construct for ASMs expresses a bounded parallelism, namely the
simultaneous one-step execution of a fixed finite number of processes. For
high-level specifications, in particular in connection with modeling the be-
haviour of hardware or other synchronous processes, it is often very useful to
work with an unbounded parallelism the universal quantifier from logic helps
to express. The ASM construct forall x with φ do M, called forall rule, gen-
erates in A the union of all update sets (one per instance) which are generated
in state A by the instances of M that interpret x by an a ∈Workspace(A)
satisfying φ.
Definition 31 (Semantics of forall rules). We extend the class of ASMs
(with choose and import rules) inductively by forall rules, i.e. rules of the
form M ′ = forall x with φ do M (where M is an ASM). Its semantics
is defined (in mathematical terms, borrowing some notation from the ASM
framework) by the following clause that generalizes the semantics of par rules
in the presence of import and choose rules (Def. 19, pg. 36).
Let ζ, η0 be two parameters given in the context where the forall rule is
evaluated, and let the set of elements x satisfying φ be defined as

3.5 Synchronous Parallel Step Control
69
range(x, φ, A, ζ) = {a ∈Workspace(A) | eval(φ, A, ζ[x 7→a]) = true}.
We define:
if range(x, φ, A, ζ) = ∅then
evalCand(forall x with φ do M, A, ζ, η0, (∅, η0)) holds
else
evalCand(forall x with φ do M, A, ζ, η0, (U, η)) holds
where
{a1, a2, . . .} is any enumeration of range(x, φ, A, ζ)19
evalCand(M, A, ζ[x 7→an], ηn−1, (Un, ηn))
holds for every an with some (Un, ηn)
U = S
n Un and η = S
n ηn
Why we restrict choose/forall to quantify over the current Workspace,
besides for implementation reasons, can be explained here: without the re-
striction a rule like forall x with true do f (x) := 1 would turn each Heap
element into an argument of the function f with value 1 ̸=undef and thereby
into a Workspace element. That would exhaust the capacity of Heap to pro-
vide free elements for further imports.
With the forall construct one can specify the synchronous parallel control
behaviour paradigm for any set of Processes as follows:
SyncPar(Process) = forall P ∈Process do P
An interesting combination of interleaving with synchronous parallelism is
implemented in CoreASM [64]—an ASM interpreter—where in each step not
only one but a finite number of agents can be chosen to simultaneously make a
step. We call this computational paradigm parallel interleaving. It provides an
interesting sequential implementation of concurrent runs (Ch. 5) and at the
same time coincides with concurrency of communicating processes (Sect. 5.2)
where the information exchange between agents is provided by a commu-
nication medium with an appropriate protocol so that there are no shared
locations that could create update conflicts in concurrent runs. Another ex-
ample appears in runs of Look-Compute-Move algorithms (Sect. 9.2.2.3).
Definition 32 (Parallel Interleaving Paradigm).
ParInterleav(Process) =
choose P ⊆Process with forall M ∈P ReadyToExec(M)
forall M ∈P do M
Example 12. Cellular Automata are an intensively studied computing de-
vice (see [109] for a survey) that uses the forall construct for its behavioural
definition. These automata can be defined by refining the Finite State Ma-
chine rule Fsm—that is executed by a single agent (see Exercise 3, pg. 26)—to
19 We assume that every superuniverse |A| is enumerable.

70
3 Control Structures of Single-Process Runs
an even simpler CellRule of just one control state assignment that is exe-
cuted in every step simultaneously by all agents of a discrete d-dimensional
space Cell, say of all (or a finite subset of) d-tuples of integers viewed as ac-
tors. So the overall behaviour of a CellAutomaton working on a (possibly
infinite) structure of connected agents is defined by the following ASM (not
any more a PGA!) rule:
CellAutomaton =
forall c ∈Cell do CellRule(c)
The refinement of the Fsm rule to the CellRule is obtained by replacing
input reading by reading at each cell c the ctl state of each neighbour of c.
More precisely:
• in the Fsm rule ctl := nextCtl(ctl, in(head)) replace reading the input
in(head) by reading at each cell c the ctl states of its finitely many neigh-
bour cells neighbi(c) (0 ≤i ≤n for some n > 0) where for notational
convenience we include c as one of its neighbours, say neighb0(c) = c.
We call each n-tuple (ctl(c), ctl(neighb1(c)), . . . , ctl(neighbn(c))) a neigh-
bourhood configuration of c.
This implies to delete the FSM reading head (together with its assignment
rule and move function) and to refine the dynamic 0-ary FSM function ctl to
a unary function ctl : Cell →CtlState where CtlState is the usual finite set
of FSM control states.
Thus each CellRule performs an atomic action that represents an in-
stance of the Look-Compute-Action paradigm (pg. 27) of the following form.
As usual self denotes the currently considered cell (the executing agent).
CellRule =
ctl(self) := nextCtl(ctl(neighb0(self)), . . . , ctl(neighbn(self))).
For every CellAutomaton a state consists of the dynamic function
ctl : Cell →CtlState plus the space Cell with static neighbi and nextCtl func-
tions. Runs are ASM runs of an agent executing program CellAutomaton
without further interaction with the environment (see Definition 22, pg. 40),
i.e. a synchronizer of all local agents c each of which computes its instance
CellRule(c) of the CellRule. Note that in every step the update set of
all updates computed for every location (ctl, c) by the local CellRule is
obviously consistent.
The multitude of CellRules is provided by different rule specific inter-
pretations of the finite and static local-update function nextCtl that specifies
for every cell c and for each of its finitely many possible neighbourhood con-
figurations the next value of ctl(c).
The constituents of a cellular automaton—the local rule and function in-
stances (CellRule(c), neighbi(c), ctl(c)) and nextCtl—are rather simple.
This holds in particular for what in the literature is known as elementary

3.5 Synchronous Parallel Step Control
71
cellular automata, the 1-dimensional case with integer cells (Cell = Z) and
only two control states 0, 1 where every cell c has only its direct leftNeighbour
and rightNeighbour; so per cell there are only 23 = 8 neighbourhood config-
urations (ctl(c −1), ctl(c), ctl(c + 1)) resulting in 28 = 256 possible nextCtl
functions in CellRules. Despite of this simplicity some elementary cellular
automata A are computation universal in the sense that for every Turing
machine computation C there is an initial state of A such that A starting
its run in this initial state simulates C [167, Ch. 11: Sect. 4-6]. This reflects
the power of cooperation that results from connecting elementary component
processes in nets that provide specific interaction structures. In Sect. 9.2.2 we
illustrate a similar example: the distributed Look-Compute-Move automata
whose components are connected in an underlying discrete graph or Euclidean
space where they can also change their position in the structure, moving along
an edge in the graph or following a computed trajectory in the given space.
The universality phenomenon is explained in more detail in Sect. 9.1.2 and
Sect. 9.2.1; here we point only to the following rule of a concrete computation
universal elementary cellular automaton, rule 110 in Wolfram’s notation with
ctl represented by color, control state 1 by black and 0 by white (see [167,
Ch. 11: Sect. 8]):
CellRule110 =
if color(self) = color(leftNeighb(self)) = color(rightNeighb(self))
or (color(leftNeighb(self)) = black and
color(rightNeighb(self)) = color(self) = white)
then color(self) := white
else color(self) := black
Computations of cellular automata with small dimension (for example 1
or 2), a small number of control states and (possibly near) neighbors when
started with not too complicated initial states can be eﬀiciently visualized
in the plane representing control states by colors. This has been exploited
intensively in investigations of the behaviour of cellular automata and re-
vealed a new kind of behavioural complexity, namely the complexity of (ei-
ther static or dynamically evolving) structural patterns that are generated
by (segments of) cellular automata computations started in specific initial
states. See for example [166, 167] and the rich catalogues [132] of patterns
generated from initial states by the best known particular cellular automaton:
Conway’s (computation universal 2-dimensional) Game of Life.
Exercise 17 (Conway’s Game of Life). Consider the 2-dimensional Game
of Life where a cell with 3 alive neighbours becomes (or remains) alive and a
cell with less than 2 or more than 3 neighbours becomes (or remains) dead.
Have a look at [132] for interesting patterns this simple rule generates from
initial states.
As a further example of 2-dimensional cellular automata, with neighbours
consisting of the cell itself plus the 4 cells directly above, below, left or right
of it, consider the following CellRule:

72
3 Control Structures of Single-Process Runs
CellRuleDiamond =
if thereissome x ∈neighbours(self) with color(x) = black then
color(self) := black
which, starting from a single black cell, produces an ever-growing diamond
shape of black cells. Figure 3.4 shows the first few generations of the automata
(with the new black cells at each step highlighted in a darker shade).
Fig. 3.4 Successive generations of the CellRuleDiamond automata

Chapter 4
Dynamic Sequential Step Control
In this chapter we explain the basic ingredient of self-modifying (also called
adaptive) sequential algorithms, namely their ability to change their own
behaviour at runtime. This reflectivity feature extends the classical notion of
Turing-machine-like algorithmic computations which are input-triggered and
follow a fixed static program.
In Sect. 4.2 we analyse the intuitive concept of reflectivity which directs
our definition of an abstract notion of sequential deterministic reflective al-
gorithms in Sect. 4.3. As characteristic examples for the practice of reflective
programming we specify the reflective behaviour of programs in three con-
crete (logical, functional and imperative) languages: Prolog (Section 4.4),
LISP (Section 4.5), and the reflective version RASP (Random Access Stored
Program Machine) of the RAM (Section 4.6). In Sect. 4.6.1 we show an ex-
ample of a reflective machine at work, namely a RASP simulation of indirect
addressing. The simplicity and the expressive power of our generic concept of
reflective machines are also highlighted by using reflectivity to define a com-
putationally universal abstract machine (Sect. 9.1.2.1, pg. 228). In Sect. 4.7
we compare the reflectivity features of the mentioned languages with our def-
inition of reflective ASMs and relate them with the closely related concept
of reification of programming features, a different reflectivity style offered for
example by Java.
In this chapter by reflective algorithms we mean deterministic sequential
algorithms with reflection and use as abstract model an extension of PGAs
(unless otherwise stated). We use the occasion to illustrate a practical and
rather general mechanism to handle domain-specific extensions of a given
language or machine. As example we use an extension of PGAs by a con-
ceptually simple but general scheme for program change instructions. This
scheme allows one to modify (at run time) a complex structured object (here:
a program) by an appropriate combination into one ‘global’ update of mul-
tiple ‘local’ updates of parts of this object. The generic character of this
construct relies upon a partial update technique we explain in Section 4.1.
73
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
E. Börger, V. Gervasi, Structures of Computing, 
https://doi.org/10.1007/978-3-031-54358-6_4 
 
 
 
 
 

74
4 Dynamic Sequential Step Control
The hurried reader may skip this section and go directly to the discussion of
reflectivity in Sect. 4.2.
4.1 Partial Updates of Structured Objects
When operating on structured objects one might wish to permit the inde-
pendent specification of two or more ‘local’ updates in parts of the structure
that are then combined (‘aggregated’) into one ‘global’ update of the struc-
ture, possibly also mediating conflicts that could occur among the given local
updates. This consideration applies in full generality to multiple so-called
partial updates for any structured objects, like sets, functions, graphs, trees,
programs, and also to multiple assignments to shared locations in concurrent
runs, e.g. a counter where updates by different agents may be intended to be
applied cumulatively (see Example 13, pg. 116).
The update-based ASM framework lends itself to deal with partial up-
dates computed by appropriately tailored update instructions (to be dis-
tinguished from assignment instructions, see pg. 23), also called partial as-
signment instructions or partial assignments. They extend regular updates
of a given location that has structured content in such a way that one
can specify the desired update of loc by a set of updates of parts of the
structure of the content of loc, ‘partial’ updates that will be aggregated us-
ing some action to become executable as one regular update of the (struc-
ture in the given) location. For example, one may wish to handle two si-
multaneous requests of a counter increase, say count := count + 1 and
count := count + 2, not as generating in any state A two inconsistent regular
updates (count, countA+1) and (count, countA+2) but as an action that will
addCumulatively the two addition requests, expressed by two partial updates
(count, countA + 1, addCumulative) and (count, countA + 2, addCumulative)
that are combined to result in one regular counter update (count, countA+3).
The syntax we use for partial updates adds to assignment instructions
f (t1, . . . , tn) := t the action by which the value in the generated partial
update (loc, val, action) is combined with the values of possibly further gen-
erated (occurrences of) partial updates of the same location, possibly also
with a different action (for example (count, countA −1, subtractCumulative):
f (t1, . . . , tn) :=action t
Correspondingly, the semantics of ASMs is extended to compute two kinds
of updates:
• Usual updates we continue to simply call updates (or also regular or
genuine updates), i.e. location/value pairs (l, v) which form an update
set U as described in Definition 14 (pg. 32).
• Possibly multiple occurrences of partial updates (loc, val, action). They
result from the evaluation of a partial assignment instruction, i.e. with

4.1 Partial Updates of Structured Objects
75
loc = (f , (eval(t1, A, ζ), . . . , eval(tn, A, ζ))), val = eval(t, A, ζ) and the in-
dicated action. For every location these partial updates must be trans-
formed together with a computed regular update (loc, v) (if any) into a
unique update of loc.
The semantics of partial updates is therefore defined as follows:
Definition 33 (Semantics of update instructions). It suﬀices to extend
the basis of the induction in Definition 14 (pg. 32), the inductive cases remain
the same (but use partial update multisets instead of update sets, given that
possibly multiple occurrences of one update are intended to be taken into
account, for example two occurrences of (count, countA + 1, addCumulative)
could be treated to yield the update (count, countA + 2)).
eval(f (t1, . . . , tn) :=action t, A, ζ) = {|(loc, val, action)|}
where
loc = (f , (eval(t1, A, ζ), . . . , eval(tn, A, ζ)))
val = eval(t, A, ζ)
The triples (loc, val, act) generated in state A form a partial-update mul-
tiset PartUpdloc. For each location, from those triples together with a regular
update (loc, v) generated in state A (if any) a unique update (loc, newVal)
has to be computed which contributes to form the successor state A + U (if
any) as described in Def. 9 (pg. 19).1 This computation is performed by a
separately specified machine Aggregate that depends on the (type of the)
structure of the content of the involved location (e.g. a set), on the used
actions (e.g. insertion or deletion of some elements), and on the value2 that
contributes to the update. Therefore here we have to leave this machine ab-
stract, its specification is possible once the structure of the content of loc and
the corresponding kinds of action are known.
For notational economy, in the following we assume that the + operator
(the application of updates to a state, Def. 9 pg. 19) is extended to support
partial updates.
Remark. This treatment of partial updates is supported by the ASM in-
terpreter CoreASM, see [128, 73, 74] and [50, Ch. 8]. If there are dependencies
among updates of different locations—e.g. when in a structured object the
value of one location l1 determines the value of another location l2—the spec-
ification of the Aggregate machine must include a compatibility analysis.3
1 Note that if one does not want to combine a regular update (loc, v) generated in state
A with the partial updates generated for the same location in state A, then the update
(loc, v′) resulting from the combination of those partial updates in PartUpdloc must be
consistent with the regular update (loc, v) to be applicable to form the successor state.
One could also give priority either to the regular (a ‘global’) update or to the update
resulting from the combination of the partial (‘local’) updates.
2 This is without loss of generality; if multiple parameters are needed to specify a certain
action, we can assume the value to be a tuple of such parameters.
3 For the analysis of some sophisticated compatibility criteria investigated in terms of
an alternative treatment of partial updates see [154].

76
4 Dynamic Sequential Step Control
Partial Update Instruction Notation. Instead of partial assignments
container :=addCumulative container + c we often use an equivalent update
instruction notation (preferring a mixfix natural-language based syntax for
readability) such as
add c to container
(or AddCumulative(c, container)) with an indication of the partial updates
the instruction is intended to generate in a state A. For example, we may
stipulate that add c to container generates triples (container, c∗, addCumu-
lative) where c∗does not stand for the new value that would result for the
container by the assignment container := container +c; instead it stands for
the element eval(c, A, ζ) that is intended to-be-added to the container (maybe
together with other elements, depending on what Aggregate will compute
to combine the partial container updates). The container may stand for say
a (0-ary function) counter or set or list or stack with structured content as
indicated by the names. The computation of the Aggregate machine for the
addCumulative action depends on this structure of the container elements:
addCumulative is interpreted as the cumulative version (possibly using some
ordering of the local operations) of the following operations:
• number operation + if the container is a counter with values in say Nat,
• set union ∪if the container contains sets,
• insert (at an indicated position) if the container contains lists,
• push if the container contains stacks, etc.
Similarly for update instructions remove c from container describing the
effect of container :=remove c (as it is often the case that such update instruc-
tions come in families, providing various operations of a coherent algebra of
partial updates).
4.2 Intuitive Meaning of Reflectivity
Before proceeding to the definition of the language ReflectPGA of reflective
PGAs we investigate the intuitive concept of reflective algorithms which di-
rects our definition.
Reflective processes support a double view on the functionality of their
algorithm: an execution view and a data view.
• At the ‘object-level’ an algorithm is seen as description of the updates the
algorithm performs in a given state S on some of its data, excluding the
algorithm itself. The instructions of the algorithm are read, interpreted as
actions the algorithm requires to perform, and executed on the involved
data, but they are not changed (cannot be written).
• At the ‘meta-level’ instructions of an algorithm are seen as data that can
be modified by runtime writes, i.e. as location(s) (data container(s)) where

4.2 Intuitive Meaning of Reflectivity
77
to store at runtime a possibly new value which determines the execution
step that will be performed in the successor state S′. For this view the
algorithm must be included in its modifiable data space.
This is the main characteristic of reflexive algorithms: they can be both
executed and updated by the algorithm. Therefore, to find a general but pre-
cise reflectivity concept that can be instantiated to explain the behaviour of
reflective programs of concrete programming languages we have to investigate
two questions:
• Can one define in a general, precise and simple form how to incorporate
into states A an algorithm that is executed in A such that the execution
step may include to perform some updates that modify the algorithm
itself, resulting in a new algorithm that is executed in the next state (if
there is a next state)?
• Can one define a precise, general and simple form for instructions that
manipulate reflective programs?
To answer these two questions for sequential and deterministic algorithms
we use PGAs. The first question has a simple answer: introduce into the state
of a PGA a dynamic program location pgm where to store the current
value of the PGA rule, more precisely a 0-ary dynamic function pgm whose
value pgmA in a given state A first of all defines the step function step(pgmA),
i.e. it drives the execution of the PGA rule to compute the successor state
steppgmA(A) = A′ (if it is defined). But in addition this step of the PGA with
program pgmA may also generate an update (pgm, rule) whose execution
modifies pgmA to a new PGA rule rule = pgmA′ that is executed in the
successor state A′ to compute the next steppgmA′(A′).
To answer the second question we must analyze the structure of programs
which determines which parts of this structure are appropriate for being
updated. For example, in the logic programming language Prolog a program
is a sequence of instructions (called clauses) that can be inserted and deleted.
In the functional programming language LISP a program is an expression all
of whose terms can be modified. In the imperative RASP (Random Access
Stored Program) language a program is a sequence of instructions that come
with two components, an operation code and an operand both of which are
stored in registers that can be modified when the current program executes
a step. In the structure of PGA rules we find all of these three features: a
rule can be inserted or deleted from the program, a logical expression that
appears as condition of a guarded rule can be modified, and all the terms
that appear in an assignment can be modified.
This suggests to consider pgm update instructions at the level of PGA
rules, of conditional expressions and of terms as candidate for a general form
of instructions tailored to manipulate reflective programs.
A question that remains concerns the signature: in a reflective algorithm
the fixed signature Σ of PGAs must become dynamic to reflect that in up-
dates of pgm new functions may be introduced, maybe together with some

78
4 Dynamic Sequential Step Control
initialization code. This is easily specified by the import construct of ASMs
which allows one to introduce new functions and specify their initialization.
4.3 Reflective PGAs
As explained in the preceding section we start with the following simple
definition.4
Definition 34 (Reflective PGA). We call an ASM M a reflective PGA if
it is an extension of a PGA (Def. 12, pg. 24) by the following two properties:
• The signature of M contains a 0-ary dynamic function pgm together with
a 0-ary dynamic signature function Σ such that the runs of M start in
initial states where the value of pgm is the main rule of M and in every
state A the executed step is a step of pgmA computing the successor state
step(pgmA)(A). pgm is required to have the same value in every initial state
of M, called its initial program.
• Besides the original PGA rules M uses program update instructions
(Def. 35 below) and also the import construct, added to the inductive
definition of PGA rules to express extensions of the signature Σ.
By ReflectPGA we denote the class of reflective PGAs.
For simplicity of exposition but without loss of generality, in this definition
we assume every call-by-name N(t1, . . . , tn) in M to be replaced by the corre-
sponding body instance N ′(x1/t1, . . . , xn/tn) of the declaration of N so that
we have to consider for the pgm only the program of the main component of
M.
The structure of PGAs suggests three kinds of specific program up-
date instructions, namely to insert/delete rules (read: instructions) and to
modify formulae (which occur as guards of conditional instructions) or terms
(which occur in assignment instructions). This can be expressed by corre-
sponding update instructions that contribute to modify the program value of
pgm.
Rule replacement. To express that a particular occurrence of a PGA rule
in a given PGA program should be replaced by some PGA rule′ we label all
occurrences of the to-be-replaced rule in the program, say by rule1, . . . , rulen
(using number indices as labels). This allows us to formulate the desired rule
replacement by an update instruction of the following form:
4 The section is inspired by [153] where the authors provide an axiomatic definition of
reflective sequential algorithms with a proof that the latter are captured (in the sense
defined in Ch. 8, pg. 195) by an appropriate class of sequential ASMs. These ASMs use for
the program manipulation a tree algebra background we replace by a conceptually simple
partial program assignment construct that provides a practical definition of reflective
ASMs—in terms of Sect. 7.1 a ground model for the above reflectivity requirements.

4.3 Reflective PGAs
79
ReplaceRule rulei by rule′ in program
The effect of this ReplaceRule is defined to generate a partial update
(program, (rulei, rule′, program), replaceRule)
the AggregatePgmUpd machine will try to perform by replacing in the given
program the indicated rulei occurrence by rule′. Note that the labeling is
assumed to be made (in the background) for a given program where needed
to name a rule one wants to replace.
Rule deletion. The deletion of a rule occurrence rulei in a program is a
special case of rule replacement so that it can be defined as follows:
DeleteRule rulei from program =
ReplaceRule rulei by skip in program
Rule insertion. The insertion of a rule makes sense only in case rule
becomes a par companion of some rule occurrence in program. Therefore it
can be defined as follows:
InsertRule rule′ at rulei into program =
ReplaceRule rulei by par (rulei, rule′) in program
Formula and term update instructions. The same technique can be
used for update instructions that modify rule guards (i.e. logical formulae)
or terms in a PGA program:
• ReplaceFormula condi by cond′ in program is defined to generate the
partial update triple (program, (condi, cond′, program), replaceFormula).
• ReplaceTerm ti by t′ in program is defined to generate the partial
update triple (program, (ti, t′, program), replaceTerm).
Definition 35 (Program Update Instructions). We call ReplaceRule,
InsertRule, DeleteRule, ReplaceFormula, and ReplaceTerm pgm
update instructions of reflexive PGAs.
Note that the AggregatePgmUpd machine for the handling of partial
program updates is tailored to try to combine partial updates generated to
modify rules, guards and terms of program into a regular update of program
(if the partial updates are compatible). In this way the detailed specification
and implementation of how to apply multiple partial updates to a program is
pushed for a refinement (i.e. a more detailed specification or an implementa-
tion) to the background (in CoreASM to a plugin), an example of separation
of concerns. Also the update of the labeling of the items of interest (rules,
guards, terms) in the updated program is computed in the background.
Remark on program representation. The abstract program update
instructions of reflexive PGAs hide the details of program representation,
access and change and indicate the place for their refinement and imple-
mentation in the AggregatePgmUpd machine. These update instructions
work not only for a representation of ASMs as code (as obtained by their

80
4 Dynamic Sequential Step Control
inductive definition, see pg. 24) but also for other widely used program rep-
resentations, including visual ones. A standard example is the definition by
annotated parse trees, see Fig. 4.1, Fig. 4.2, Fig. 4.3 and Fig. 4.4, by which
program manipulation is realized by operations on (sub)trees.
Fig. 4.1 Tree representation of assignment instructions f (t1, . . . , tn) := t
update
id
f
terms
tree(t1)
. . .
tree(tn)
term
tree(t)
Fig. 4.2 Tree representation of update instructions f (t1, . . . , tn) :=action t
partial
id
f
terms
tree(t1)
. . .
tree(tn)
id
action
term
tree(t)
Fig. 4.3 Tree representation of if cond then M and par M1, . . . , Mn rules
if
tree(cond)
tree(M)
par
tree(M1)
. . .
tree(Mn)
Fig. 4.4 Tree representation of let x = t in M and import x do M rules
let
id
x
term
tree(t)
tree(M)
import
id
x
tree(M)

4.3 Reflective PGAs
81
The visualization often helps to find easy-to-comprehend definitions of
intricate program behaviour. A typical example is the definition of a Java
interpreter in [159] the procedural part of which we have used in Sect. 3.2 to
illustrate the sequential imperative programming paradigm. Using that tech-
nique the labeling of rule occurrences—introduced above to have a distinct
name rulei for every occurrence of rule in program—comes for free: in the
parseTree(program) representation use the labeling of tree nodes by α, β, . . .
to refer to rulei by the unique α that labels the root of the subtree that
represents rulei (in parseTree(program)). Therefore the corresponding pro-
gram update instruction becomes (for example) ReplaceRule α by rule′ in
parseTree(program). For another example where the parse tree structure helps
see Fig. 9.6–Fig. 9.8 in Sect. 9.2.1 for the construction of a universal ASM
over fixed signature.
However, for the formulation of partial updates of reflective programs the
two representations are equivalent. The important point here is the abstrac-
tion obtained by the Program Update Instructions defined above (Def. 35)
for reflective PGAs (Def. 34). These instructions abstract from the complex
technical details that are involved when parse trees (or program texts) for
ASMs are treated as terms to which the basic term evaluation function is
applied, as done in [153] to define reflectivity.
Extensions of ReflectPGA. To let the essential part of the computational
reflectivity notion come out explicitly we have defined it by a technically
simple extension of deterministic PGAs, namely by letting the given program
that computes the step function be value of part of the memory, for simplicity
value of a location pgm. In terms of PGAs the execution view and the data
view of a reflective pgm stand out explicitly as follows:
• pgm appears as parameter of the step function whose application to
a state A executes the step that leads from A to the successor state
steppgmA(A).
• pgm can appear in updates of part of the memory where pgm is stored,
namely in (partial updates that are aggregated into) an update of pgmA
to pgmA′.
Technically this is obtained by using the eval(M, A, ζ) function to deter-
mine the update set deterministic PGAs apply to compute the succes-
sor state of the given state A. Using instead the nondeterministic version
evalCand(M, A, ζ, U) (Def. 27) integrates reflectivity in a natural way into
nondeterministic ASMs. It provides reflectivity also in the presence of other
constructs or for multi-agent concurrent ASMs we investigate in the next
chapter.5
5 Also the characterization of reflexivity in [153] starts with PGAs. Whether this charac-
terization can be extended to work for ASMs with unbounded choice and/or parallelism
has not yet been investigated.

82
4 Dynamic Sequential Step Control
4.4 Backtracking and Reflection (Prolog)
The logic programming language Prolog offers dedicated instructions to
change the program at runtime. We explain in this section from scratch an
abstract precise definition of an interpreter for dynamic Prolog programs; the
definition is given in ASM (even PGA) terms, the way that has been used
to formulate the entire ISO Prolog standard [66] and to perform a rigorous
analysis of the various proposals to decide upon for the specification of the
ISO standard semantics of program updates [30, 42].
We use the occasion to illustrate by a circumscribed but concerning the
reflectivity phenomenon complete example how to construct a language inter-
preter out of components for groups of instructions, using the same instruc-
tion wise composition method as in Sect. 3.2. The interpreter is defined as a
parallel composition of independent but interacting modules which directly
reflect the organization of Prolog instructions into the following groups:
• Pure Prolog, consisting of the cut-free part without built-in predicates.
This group has been proven to be correct but not complete with respect
to SLD resolution.
• Logic Control, including cut and catch-and-throw.
• Dynamic Program Management
• Solution Collecting Predicates, including findall, bagof, setof.
• Error Handling.
• Box Model Debugger.
We concentrate our attention here on the core machine for Pure Prolog—
which explains the by itself interesting computational backtracking paradigm
by four simple rules—and its extension by a machine for the management of
dynamic code. For the remaining components and their proven to be correct
implementation to virtual machine code by a dozen of proven to be correct
stepwise refinement steps the interested reader is refered to the literature
[51, 52] and [149]. The last mentioned work is an interesting and for the
ASM framework typical example for implementing a detailed proof-engine
version of a mathematical proof, obtained by splitting the proof details into
a series of independently verifiable stepwise ASM refinements (see Sect. 7.2).
4.4.1 Interpreter for Pure Prolog
A Prolog computation represents a systematic search through a space of pos-
sibly multiple solutions to an initially given problem, called initialQuery,
where each time the currently tried out solution alternative fails a back-
tracking mechanism directs the search engine to try the next alternative (if
available). The search is naturally modeled using a dynamically built tree
structure of a dynamic set of Nodes:

4.4 Backtracking and Reflection (Prolog)
83
parent : Node \ {root} →Node
root : Node
Each node (except the root and its initialQuery-node child, see the initial-
ization condition below) represents computation states that correspond to
one solution-computing alternative provided by the pgm. The search engine
is a sequential machine so that in every moment it visits a unique node we
denote by a 0-ary dynamic function currnode. Each node is associated with
a sequence of its to-be-resolved Goals that come with (we say are ‘decorated’
by) their backtracking information in presence of the Prolog-specific crucial
tree-pruning operator cut. Each goal is a sequence of user-defined or built-in
terms. This explains the following functions that serve to record state infor-
mation (we skip the explanation of the role of Marks because they are needed
only for some built-in predicates we do not explain here):
Goal = Term∗
Decgoal = Goal × Node
decgoalseq : Node →(Decgoal ∪Mark)∗
currnode : Node
currdecgoalseq = decgoalseq(currnode)
currgoal = ﬁrst(ﬁrst(currdecgoalseq))
At currnode with a user-defined to-be-resolved term ﬁrst(currgoal), called
activator, the search engine can do two things:
• LayOutAlternatives: if in mode = layoutAlts lay out the possible
solution alternatives for the current activator by extending the tree by a
list cands(currnode) of children of currnode, one candidate for each to-
be-tried-out code alternative provided by the pgm, and switch to mode =
tryNextAlt,
• TryNextAlternative: in mode = tryNextAlt, try to Resolve the
activator by one of the code alternatives if there is still some candidate
available, otherwise Backtrack.
The user-defined terms in elements of Goal are called literals. When a
literal appears as activator at currnode the Prolog engine retrieves via a
function procdef from its current program pgm the Code sequence of all oc-
currences of Clauses of form Head ←Body whose Head is formed by the same
predicate symbol as lit (so that it may be unifiable with lit); in other words
procdef (lit, pgm) retrieves from pgm the candidate clause occurrences for lit.6
lit can be successfully computed by first unifying it with the Head of the al-
ternative clause(clauseOcc(child)) of at least one child in cands(currnode)
and then computing with success the subgoal represented by the Body of
such a clause. We use a dynamic function clauseOcc to record the association
of one code alternative with each new child node in cands(currnode).
Since Prolog has an error handling mechanism and supports termination
of programs the rules we explain here have in their guard the following OK
condition:
6 nil in the range of this function is needed for the PrologDbModule defined below
which handles dynamic changes of Prolog pgms.

84
4 Dynamic Sequential Step Control
OK = (stop = 0 and error = 0)
This explains the following four rules for Prolog’s basic backtracking mech-
anism. The logical unification-based Resolve procedure is explained below.
LayOutAlternatives =
if UserDeﬁned(activator) and mode = layoutAlts and OK then
ExtendTreeBy node1, . . . , nodem with
-- state alternatives
forall i = 1, . . . , m clauseOcc(nodei) := ci
-- one per to-be-tried-out solution alternative
mode := tryNextAlt
-- switch to compute the alternatives
where [c1, . . . , cm] = procdef (activator, pgm)
-- solution alternatives
The signature elements and the auxiliary macro are defined as follows:
activator = ﬁrst(currgoal)
-- currently to-be-resolved term
procdef : Lit × Program →Code∗∪{nil}
-- yields solution alternatives
clauseOcc : Node →Code
-- records associated code line
-- candidate clause occurrence of candidate child node
clause : Code →Clause
-- clause at code line
ExtendTreeBy node1, . . . , nodem with M =
let n1, . . . , nm = new (Node) in
forall i = 1, . . . , m do
parent(ni) = currnode
cands(currnode) := [n1, . . . , nm]
M
TryNextAlternative =
if UserDeﬁned(activator) and mode = tryNextAlt and OK
then if cands(currnode) = [ ]
-- no alternative resolved activator
then Backtrack
else Resolve
where Backtrack =
if parent(currnode) = root then stop := Failure
else currnode := parent(currnode)
Observe that when the engine Backtracks to parent(currnode) it finds
there all necessary state information (attached to the node by the functions
that record the state information) to continue the computation with possibly
remaining alternatives that may resolve the activator.
The GoalSuccess rule expresses that once the currgoal has been suc-
cessfully computed the engine Proceeds to compute the rest of its current
decgoalsequence, popping the call stack. 7
7 Proceed is refined by an additional rule to express special actions that have to be
taken for certain built-in predicates we do not explain here.

4.4 Backtracking and Reflection (Prolog)
85
GoalSuccess =
if currgoal = [ ] and OK then Proceed
where
Proceed = (decgoalseq(currnode) := rest(currdecgoalseq))
The QuerySuccess rule expresses that once all goals have been computed
the computation terminates with success.
QuerySuccess =
if currdecgoalseq = [ ] and OK then stop := success
These four rules describe the backtracking-based computational paradigm of
Prolog. For the initialization of Prolog computation trees we stipulate that
initially the tree consists only of two nodes: the root node (on which no
function is defined) and its unique child currnode; the latter comes with the
one-element-list with initialQuery goal and with the backtracking node root,
matching the condition parent(currnode) = root.
PureProlog =
LayOutAlternatives
TryNextAlternative
GoalSuccess
QuerySuccess
Exercise 18 (BacktrackScheme). Abstract from the particular Prolog
features in PureProlog and extract a generic BacktrackScheme for al-
ternative machines whose execution is supposed to terminate with or without
success and may recursively call alternative submachines to be tried out for
execution. See Appendix B for a solution.
Prolog programs are sequential and deterministic. In fact at each mo-
ment at most one PureProlog rule can be applied at the uniquely de-
termined currnode because the rules have pairwise disjoint guards: if the
UserDeﬁned(activator) guard is true, then currgoal and with it currgoalseq
are not the empty list. currgoal = [ ] implies that currgoalseq is not empty;
if currgoalseq is empty then currgoal =undef .
Exercise 19 (Prolog rule NextSolution). Define an additional stop rule
that continues the search for another solution if requested (by the user) in
case stop = success. See Appendix B for a solution.
It remains to specify the Resolve mechanism.8 It involves the logical
concepts of term unification (most general unifier mgu of terms) and substi-
tution (of free variables by terms) we assume the reader to know. We use the
following functions:
8 The reader who is not interested in the logical details of the resolution procedure may
proceed directly to the interpreter rules for reflective Prolog.

86
4 Dynamic Sequential Step Control
mgu : Term × Term →Substitution ∪{nil}
-- most general unifier
t ◦sub
-- postfix notation for application of sub to t
sub1 ◦sub2
-- postfix notation for composition of substitutions
sub : Node →Substitution
-- current substitution in a state
term′ = rename(term, vi)
-- variables renamed at variable index vi
Resolve tries to unify the given UserDeﬁned(activator) with the renamed
Head′ of the clause Head ←Body that is associated with the first candidate
fstcand = ﬁrst(cands(currnode)). Whether the unification fails or not, the en-
gine updates cands to the remaining candidates (having tried the alternative
ﬁrstcand). But if there is a unifying substitution s = mgu(activator, Head′),
then the machine in addition updates at fstcand the decgoalseq to a new
one—obtained by replacing in the current one the activator by the renamed
Body′ (with cutpoint information) and applying the substitution s, extends
the current sub(currnode) (to continue with at fstcand) by s, increases the
variable renaming index vi, switches to mode = layoutAlts and moves to
fstcand to continue the computation there by trying to resolve the new first
goal defined by Body′ ◦s.
Resolve =
let fstcand = ﬁrst(cands(currnode))
-- look at next alternative
let Hd ←Bd = clause(clauseOcc(fstcand))
-- find its clause
let s = mgu(activator, Hd′) in
-- try to compute a unifier
cands(currnode) := rest(cands(currnode))
-- that is all if s = nil
if s ̸= nil then
-- if unification is possible
decgoalseq(fstcand) := newdecgoalseq
sub(fstcand) := sub(currnode) ◦s
-- extend current substitution
vi := vi + 1
-- increase variable renaming index
mode := layoutAlts
-- move to try out goal Bd′ under s
currnode := fstcand
-- at next candidate child node
where
newdecgoalseq = [(Bd′, parent(currnode)) | continuation] ◦s
continuation = [(rest(currgoal), cutpoint) | rest(currdecgoalseq)]
cutpoint = snd(ﬁrst(currdecgoalseq))
4.4.2 Interpreter for reflective Prolog
Prolog has six so-called database operations9 four of which are instructions
to modify and the other two to read the pgm at runtime. asserta(clause) and
assertz(clause) are deterministic instructions that simply inserta (at the be-
ginning) respectively insertz (at the end) of the current pgm a new occurrence
9 The in the Prolog community usual wording database db refers to what we denote by
pgm.

4.4 Backtracking and Reflection (Prolog)
87
of their argument clause. retract(clause) and abolish(P) allow one to (try to)
delete a clause in the pgm which via unification matches clause respectively
all clauses whose head is a literal with predicate symbol P. In addition Pro-
log also offers a read version clause of retract and currentPredicate(P) of
abolish(P) both of which coincide with their pgm updating version except
for the deletion operation. We therefore first analyse the behaviour of retract
and abolish.
Since in every state a Prolog program can execute at most one of its pgm
related instructions no partial updates are needed to specify the semantics
of those instructions. Furthermore, the tree structure we considered for the
general case of reflective pgms (Sect. 4.3) is reduced here to a list with corre-
sponding list functions. So the two assert instructions immediately succeed,
using the corresponding list insertion function to insert a new occurrence of
the given clause Head ←Body at the begin respective the end of pgm. Prolog
operations on pgm are applied only if the user-defined predicate symbol that
occurs in the head of the involved clauses is declared as Dynamic. This re-
quirement implies that when an assert instruction is executed the predicate
symbol P in the clause head must have been declared as Dynamic.10
Assert =
if activator = assert(H, B) and Dynamic(H) and OK then
pgm := insert(H, B, pgm)
decgoalseq(currnode) := continuation-- assert succeeds immediately
where
continuation = [(rest(currgoal), cutpoint) | rest(currdecgoalseq)]
cutpoint = snd(ﬁrst(currdecgoalseq))
Prolog has the two instances Asserta and Assertz of Assert, defined with
the corresponding functions inserta and insertz.
The behaviour of retract and abolish has the same structure as the
behaviour of user-defined predicates described in the two PureProlog
rules LayOutAlternatives and TryNextAlternatives. For the Lay-
Out retract(Head, Body) rule it suﬀices to refine LayOutAlternatives by
two replacements which constitute a pure data refinement:
• in the guard replace UserDeﬁned(activator) by the two conditions
activator = retract(Head, Body) and Dynamic(Head),
• replace procdef (activator, pgm) by clauseList(Head, Body, pgm).
The clauseList(Head, Body, p) of program p for clause terms Head, Body
is the restriction of procdef (Head, p) to occurrences in p of candidate clauses
Hd ←Bd whose predicate symbols in their literals coincide with the corre-
sponding ones in Head ←Body (so that these clauses might be unifiable with
the clause Head ←Body. This explains the first retract rule.
10 So the signature extension rule Extend(Σ, P) boils down to a declaration statement.

88
4 Dynamic Sequential Step Control
LayOutRetractAlternatives =
if activator = retract(Head, Body) and Dynamic(Head)
and mode = layoutAlts and OK
then
ExtendTreeBy node1, . . . , nodem with
-- solution alternatives
forall i = 1, . . . , m clauseOcc(nodei) := ci
mode := tryNextAlt
where
[c1, . . . , cm] = clauseList(Head, Body, pgm)
clauseList : Term × Term × Program →Code∗∪{nil}
Similarly we can define the TryNext retract(Head, Body) rule as a refine-
ment of TryNextAlternatives. It suﬀices to make the same guard refine-
ment as for the LayOut retract(Head, Body) rule and two further refinements,
an operation refinement of Resolve and a data refinement:
• add to Resolve the update of pgm by deleting the clauseOcc(fstcand),
• refine the unification mgu(activator, Hd′) to unify the entire activator
clause Head′ ←Body′ with the entire clause(clauseOcc(fstcand)) (instead
of only its Hd′),
• refine newdecgoalseq to the continuation◦s (because a retraction succeeds
immediately without introducing any new subgoal).
TryNextRetractAlternative =
if activator = retract(Head, Body) and Dynamic(Head)
and mode = tryNextAlt and OK
then if cands(currnode) = [ ] then Backtrack
else
Resolve
if s ̸= nil then pgm := deleteClauseOcc(clauseOcc(fstcand), pgm)
where
s = mgu(Head′ ←Body′, clause(clauseOcc(fstcand)))
newdecgoalseq = continuation ◦s
deleteClauseOcc : Code × Program →Program
The function deleteClauseOcc deletes a code line c ∈Code in pgm so that
after the deletion c does not appear any more as member of any clauseList
(with whatever terms) of the modified program deleteClauseOcc(c, pgm).
The clause reading rules LayOut/TryNextClauseAlternatives—
with clause instead of retract—are identical to the corresponding retract rules
except for the missing clause deletion update of pgm.
The structural identity of retract rules to the corresponding rules for user-
defined predicates holds also for the abolish(P) instruction of Prolog and its
read version currentPredicate(P). abolish(P) deletes from the current pgm all
clause occurrences with the predicate symbol P in their head. It works the
same way as retract but using functions predicateList (instead of clauseList)

4.4 Backtracking and Reflection (Prolog)
89
and deletePredOcc (instead of deleteClauseOcc). Similarly for the reading
version currentPredicate(P) of abolish(P).
The function predicateList : Program →PI ∗yields for a program p
the sequence of code lines in p that are associated with a clause whose
head is formed by a predicate symbol, code lines we call predicate indica-
tors which form a subset PI of Code. Thus for an n-ary predicate symbol
Pn ∈predicateList(p) if and only if the clauseList of p contains a code line
with associated clause with head P(t1, . . . , tn) for some terms ti and some
body B. For n-ary predicate symbols P the function predicateList(P, p) is
the restriction of predicateList(p) to clauses whose head is formed with P.
LayOutAbolishAlternatives =
if activator = abolish(P) and Dynamic(P)
and mode = layoutAlts and OK
then
ExtendTreeBy node1, . . . , nodem with
-- solution alternatives
forall i = 1, . . . , m clauseOcc(nodei) := ci
mode := tryNextAlt
where
[c1, . . . , cm] = predicateList(P, pgm)
predicateList : PI × Program →PI ∗
Similarly the TryNext abolish(P) rule is structurally identical to the
TryNext retract(Head, Body) rule. A pure data refinement replaces delete-
ClauseOcc by deletePredOcc and unifies the activator parameter P with
the (predicate symbol of the clause associated with the) code line clause-
Occ(fstcand).11
TryNextAbolishAlternative =
if activator = abolish(P) and Dynamic(P)
and mode = tryNextAlt and OK
then if cands(currnode) = [ ] then Backtrack
else
Resolve
if s ̸= nil then pgm := deletePredOcc(clauseOcc(fstcand), pgm)
where
s = mgu(P, clauseOcc(fstcand))
newdecgoalseq = continuation ◦s
deletePredOcc : PI × Program →Program
The function deletePredOcc deletes a code line c ∈PI in pgm that is
associated with a predicate symbol so that after the deletion this predicate
symbol (here P) does not appear any more as element of the predicateList
(see the Definition above) of the modified program deletePredOcc(c, pgm).
11 The word unification is used for notational uniformity reasons. What is checked here
is the equality of the predicate symbols in two predicate symbol occurrences.

90
4 Dynamic Sequential Step Control
P ̸∈predicateList(deletePredOcc(c, pgm))
Thus all clauses whose head is formed with P are deleted from pgm.
The two rules for the reading version currentPredicate(P) of abolish(P)
are identical to the corresponding abolish(P) rules except for the missing
predicate-symbol-deletion update of pgm.
The rules presented in this section constitute the reflective Prolog-
DbManagmt module, obtained by refining LayOutAlternatives and
TryNextAlternative of PureProlog to structurally identical rules for
runtime pgm management. PureProlog and PrologDbManagmt inter-
act with each other without conflicts, given that their rules have pairwise
disjoint rule guards.
PrologDbManagmt =
Assert
LayOutDbAlternatives
TryNextDbAlternative
where
Assert =
Asserta
Assertz
LayOutDbAlternatives =
LayOutRetractAlternatives
LayOutClauseAlternatives
LayOutAbolishAlternatives
LayOutCurrentPredicateAlternatives
TryNextDbAlternative =
TryNextRetractAlternative
TryNextClauseAlternative
TryNextAbolishAlternative
TryNextCurrentPredicateAlternative
Historical remark. The PrologDbManagmt rules guided the detailed
comparative ASM-model-based analysis (see [30, 42]) of various proposals
for the ISO Prolog standard [66] and led to the decision to define the so-
called logical view [122] of Prolog pgm operations as the ISO standard view,
recognized as the best one concerning comprehension, rigorous design and
eﬀicient implementation.
Remark on separation of concerns. Our Prolog interpreter illustrates
two natural and expressive ways by which the ASM framework supports the
important concept of separation of concerns in system design: one consists
in decomposing a system into independent modules, characterized by mak-
ing the relevant parameters together with the operations on them explicit in
an abstract reusable form; another one is the introduction of appropriately
parameterized functions to independently specify various actions within a

4.5 Reflection in LISP
91
module. An example is the definition of Resolve with differently instan-
tiatable parameters, namely mgu, newdecgoalseq and the deletion functions
deleteClauseOcc and deletePredOcc. Another example is the separation of
layout and try phase to compute an activator, coupled with the introduction
of procdef , clauseList and predicateList to operate on user-defined or retract
or abolish activators; this yields the independent but structurally isomorphic
(thereby reusable) rules that lay out and successively try out the alternatives
for computing the given activator in the corresponding module. The intro-
duction of deleteClauseOcc and deletePredOcc with appropriate constraints
illustrates how to separate the definition of their effect from the specification
of the rules where they are used.
4.5 Reflection in LISP
The handling of reflectivity in Reflective ASMs (Sect. 4.3) and in Prolog
(Sect. 4.4) is based on replacing the current value of pgm with a new value,
typically obtained by manipulating the previous one. A different and more
localized approach applies to languages which have a uniform by-reference
semantics.
The LISt Processing language LISP [126] is a prime example of what
is called a homoiconic language, in that LISP programs are LISP data
structures—namely lists made up from atoms by a pairing function—which
can be dynamically manipulated in the language and executed. In this section
we describe the part of LISP that suﬀices to explain its distinctive reflective
list manipulation and execution features.
A LISP program is a sequence of Expressions (also called S-expressions
for “symbolic expressions”) rather than statements; consequently to execute
the program means to EvaluateExpressions of such a sequence (that is
technically defined as a list, see below). Depending on the execution envi-
ronment, the sequence can be provided interactively (e.g., by a user typing
in a terminal) or as one or more textual source file (or, in certain implemen-
tations, as compiled file). The result of the evaluation of the last expression
in a program is considered to be the result of the whole program. Therefore,
to distill the reflective properties of LISP programs we can concentrate our
attention on what a LISP interpreter EvaluateExpr does to execute one
expression, abstracting from how the execution environment organizes the
execution (i.e. evaluation) of the entire sequence of expressions.
The set of LISP Expressions is inductively defined from Atoms and
Constructs.
The elements of Atom are individual values: numbers, strings denoted by
a sequence of characters enclosed in double quotation marks, or symbols.
Each string literal produces a new instance of the string, whereas number
and symbol literals produce a reference to a unique instance. Symbols can be

92
4 Dynamic Sequential Step Control
alphanumeric sequences (e.g., if or let), but can also use a wide variety of
characters from the available character set (e.g., + or pair? are valid symbols
and thus valid identifiers).
Constructs are equipped with two functions head and tail in terms of which
complex data structures can be defined. We call the elements of the abstract
set Cons simply cons and write them with lower case ‘c’.
Atom = Number ∪String ∪Symbol
-- disjoint union
Expr = Atom ∪Cons
-- disjoint union
head : Cons →Expr
tail : Cons →Expr
We will freely use predicates such as isAtom, isSymbol, isCons to indicate
the characteristic functions of the various sorts.
Using head and tail one can express a List element by a cons representing
the structure of a finite sequence of given cons ci for 1 ≤i ≤n, in particular
singly-linked lists defined from a cons l by the equations (∗) and denoted as
usual by (e1 . . . en); the special atom NIL indicates the end of a list (an empty
list) and tail is required to be defined on List with values in List ∪{NIL}.
(∗)
forall i = 1, . . . , n, ei = head(taili−1(l))
and
tailn(l) = NIL.
To access the elements of such a list l = (e1 . . . en) we use the notations
item(l, i) = ei, length(l) = n, range(l) = {1, . . . , length(l)}.
The use of parentheses to denote lists, and the prefix notation with func-
tion symbols preceding the arguments, give LISP its instantly recognizable
and remarkably regular syntax. As an example,
(if (= (mod (read) 2) 1)
(print "odd")
(print "even")
)
is an expression which reads a value (hopefully a number) from whatever
input is provided, computes the modulo 2 of that value, compares the results
for equality to the atom 1, and prints “odd” or “even” depending on the
outcome. The Cons-based representation of the above program S is given in
Figure 4.5 (where × stands for a reference to NIL). In the figure, each cons
c is represented as a pair of cells, with the left one representing head(c) and
the right one representing tail(c).
At any given moment our interpreter will be evaluating a specific expres-
sion pointed to by a 0-ary position function, which we will consider initialized
by an unspecified top-level loop12 to the top-level expression to evaluate. The
results of the evaluation will be given as the value of a function value(·) which
is initially undefined everywhere; we use a predicate done(·) to check whether
12 Such a loop can be of the form (loop (print (eval (read)))), hence the acronym
REPL for read-eval-print-loop.

4.5 Reflection in LISP
93
Fig. 4.5 Program representation in LISP
S
×
if
×
print
"even"
×
"odd"
×
=
1
×
mod
2
×
read
value(·) is defined. Two other auxiliary functions, return(·) and result(·) will
hold, respectively, the continuation that is to be taken once the evaluation
at a given pos is completed, and the result returned from that evaluation,
which will be then accessed via value(·).
It should be noted that, in general, LISP programs may be graphs, not
necessarily trees. An example is Figure 4.5, where the symbol print is refer-
enced by multiple cons, but the same is true more generally: while the parser
will always generate a tree up to symbol sharing from a textual representation
of a program, as in Figure 4.6(a), the program itself may modify its contents
— as we will show below — and produce arbitrary program structures such
as the one in Figure 4.6(b), which requires the same sub-expression (read)
to be evaluated twice. In addition, a program may request the evaluation of
expressions that are dynamically constructed (e.g. via cons) or read from
I/O (e.g., via read), and which are thus not part of any “source file”. To

94
4 Dynamic Sequential Step Control
support these features, as well as to implement recursion, each evaluation of
an expression takes place in a corresponding context. We model a context
by a value of sort Ctx; a variable (dynamic 0-ary function) ctx will hold at
any moment the current context, with context nesting expressed via a func-
tion parentCtx : Ctx →Ctx. The initial value for ctx is a distinguished ctx0
element of the sort, corresponding to the top-level evaluation.
Fig. 4.6 (a) LISP program structure as produced by the parser for the pro-
gram (print (read) (read)). Notice how the arguments to print are two
distinct lists. (b) Another structure with the same effect: Here, the arguments
to print are the same list (read), referenced twice.
×
print
×
×
read
×
print
×
read
(a)
(b)
For navigating the expression being evaluated, we define two macros:
ProceedAt(p, k) instructs the interpreter to proceed to evaluating the ex-
pression at (that is, referenced by) p, after preparing a new context nc for
the evaluation. The new context establishes where to store the result (in
result at key k of the invoking context) and where to continue the evalua-
tion (via return of the invoked context). Different keys will be used to store
results from possibly many subexpressions of the same expression (e.g. in
FullEvalForm defined below). Yield(v) instructs the interpreter to store
the computed value v in the appropriate result of the invoking context, and
continue executing from the associated continuation in return.
Accordingly, we define our signature and basic navigation operations as
follows:
pos : Expr initialized by pos0
-- current position of the interpreter
ctx : Ctx initialized by ctx0
-- current context of the interpreter
parentCtx : Ctx →Ctx
key : Ctx →Key
-- key used to store intermediate value
result : Ctx × Key →Expr
-- storage for intermediate values
return : Ctx →Expr
-- continuation
ProceedAt(p, k) =
-- evaluate p, store result in k
let nc = new Ctx in
parentCtx(nc) := ctx

4.5 Reflection in LISP
95
key(nc) := k
return(nc) := pos
ctx := nc
pos := p
Yield(v) =
-- evaluation complete, return v as result
let pc = parentCtx(ctx) in
result(pc, key(ctx)) := v
pos := return(ctx)
ctx := pc
We also use the following two derived functions:
value(k) = result(ctx, k)
done(k) = (value(k) ̸=undef )
To EvaluateExpr LISP uses various forms of global or local bind-
ings (e.g. let). We model a set of bindings as a function that in a given
Environment interprets given Symbols by some Expressions. To model nest-
ing of scoping, each environment has a parentEnvironment, and one distin-
guished environment env0 is assumed to hold all pre-defined (library) bindings
in the system. At any moment, the 0-ary global function env refers to the
current environment; we assume env to be initialized by env0. This explains
the following functions:
env : Env initialized by env0
parentEnv : Env →Env
binding : Env × Symbol →Expr
-- a partial function
It should be noted that the same symbol can be bound to different values
in different environments. As usual in programming languages, a shadowing
convention prescribes that more recent bindings take precedence over previ-
ous ones. We model this convention by defining the following function:
envVal : Env × Symbol →Expr
envVal(e, s) =









binding(e, s)
if binding(e, s) ̸= undef
envVal(parentEnv(e), s)
if binding(e, s) = undef and
parentEnv(e) ̸= undef
undef
otherwise
LISP dialects differ in whether they implement dynamic or static scoping;
some use both forms. Our definition above models dynamic scoping (as in
Emacs Lisp). In any given implementation, the top-level scope will include
a number of well-known symbols (e.g., +) that are bound to corresponding
well-known functions (e.g., addition). Some of these well-known functions are
part of the definition of the language (the historical term for such functions

96
4 Dynamic Sequential Step Control
is special forms), while others are provided by a given implementation as a
standard library, for the convenience of programmers.
The evaluation of a LISP expression consists in the repeated execution
of EvaluateExpr in the state established by an appropriate invocation
ProceedAt (to indicate the pos to evaluate and the key k where the result
will be stored).
The EvaluateExpr procedure includes the following cases we formalize
below by PGA rules:
1. If an expression is a number or a string, the result is the number or string
itself.
2. If an expression is a symbol for which a binding exists in the current
scope, the result is the value bound to the symbol.
3. If an expression is a list, then:
a. If the first element of the list is a special form symbol, corresponding
special processing EvaluateSpecialForm is performed;
b. otherwise, all elements in the list are evaluated by EvaluateList
and the function indicated by the first element of the list is invoked.
4. Other cases may vary depending on the specific dialect, but are generally
regarded as an error.
The corresponding PGA rules are thus:
EvaluateExpr =
if isNumber(pos) or isString(pos) then
Yield(pos)
-- numeric and string literals evaluate to themselves
if isSymbol(pos) then
Yield(envVal(env, pos))
-- symbols looked up in the environment
if isCons(pos) then
if isSpecialForm(head(pos)) then
EvaluateSpecialForm
-- forms with special semantics
else
EvaluateList
-- any other list
EvaluateList handles the evaluation of lists that are not special forms.
It evaluates all items in the list (in whatever order) and Applys the function
expressed by the first item to the arguments provided by the remaining items.
The result of the function invocation is the result of the evaluation.
EvaluateList =
choose i ∈range(pos) with not done(i)
ProceedAt(item(pos, i), i)
-- evaluate i-th element
ifnone
-- if all list elements are evaluated
Apply
-- apply function

4.5 Reflection in LISP
97
The Apply macro describes the process of invoking a function (given by
the first item) with the actual parameters provided by the subsequent items.
Note that the usual mathematical notation f (x1, . . . , xn) is written in LISP as
the list (f x1 . . . xn). We expect13 the function to be a lambda expression
which is itself a special form. Most commonly, the first element will be a
symbol that has been bound to a lambda expression, but it could also be
a literal lambda expression, or any expression that evaluates to a lambda
expression. To bind formal parameters (named in the lambda definition) to
actual parameters (provided by the invoking form), we create a new local
environment. Notice this is a form of dynamic scoping, as e.g. in newLisp
or Emacs Lisp, whereas other dialects (e.g. Scheme) prefer static scoping.
Finally, the body of the lambda is evaluated in the new environment, and the
result of the evaluation of the body (as stored in the key call) provides the
result of the invocation.
Apply =
let f = value(1),
-- will be f = (lambda (formals) body)
formals = item(f , 2),
-- list of formal arguments symbols
body = item(f , 3) in
-- body of function
if not done(call) then
-- pass arguments to body
let ne = new Env in
forall i ∈range(formals) do
binding(ne, item(formals, i)) := value(i + 1)
-- bind args
parentEnv(ne) := env
-- nesting of env
env := ne
-- make callee env current
ProceedAt(body, call)
-- compute body in callee env
else
env := parentEnv(env)
-- reestablish invocation env
Yield(value(call))
-- return computed function value
Special forms start with one of a set of well-known symbols (e.g., if, let,
car), and they have a semantics specified by the language; most importantly,
they do not follow the usual rules for evaluation of lists. The need for, and
utility of, special forms in LISP has been analyzed in detail in [136]. We focus
here only on those special forms that are essential to provide the capabilities
for reflection.
We distinguish two classes of special forms: those that evaluate only part of
the arguments before applying the function expressed by the initial symbols
(PartialEvalForms) and those that evaluate all arguments before appli-
cation (FullEvalForms). A prime example of the first class is if, whose
13 We choose to not model error handling since it contributes nothing specific to the
reflectivity features of LISP. The original LISP did check that the head of the function
definition was the literal symbol lambda; later versions used symbol properties instead,
to better integrate with compiled code.

98
4 Dynamic Sequential Step Control
entire point is to evaluate either the second or the third argument, depend-
ing on whether the first argument evaluated to true or false. Evaluating both
the “then” and the “else” part before checking the condition would of course
defeat the purpose. Similar reasoning applies to other forms, e.g. and and
or which guarantee short-circuit evaluation. An example of the second class
is cons, which evaluates both the second and third argument (the head and
tail of the new cons to be built), but then applies special processing (i.e.,
allocating new memory from the machine’s heap) that cannot be expressed
via standard LISP. Most LISP primitive operations, including input/output
primitives, belong to this class.
EvaluateSpecialForm =
if isPartial(head(pos)) then
PartialEvalForm
else
FullEvalForm
The evaluation of FullEvalForms consists in evaluating all arguments
(e.g., the second to last element in the list), and then applying specific se-
mantics according to the symbol (the first element). The order of evaluation
is irrelevant so that we leave it unspecified using the choose construct. Some
LISP dialect (e.g. CommonLISP) specify leftmost-innermost order, whereas
others (e.g. Scheme) explicitly leave the order unspecified.
FullEvalForm =
choose i ∈range(pos) with i > 1 and not done(i)
ProceedAt(item(pos, i), i)
-- evaluate chosen unevaluated item
ifnone
-- if argument list fully evaluated
case item(pos, 1) of
cons →
let c = new Cons in
-- construct a new pair
head(c) := value(2)
-- assign head value
tail(c) := value(3)
-- assign tail value
Yield(c)
-- return new cons
car →Yield(head(value(2)))
-- access head value
cdr →Yield(tail(value(2)))
-- access tail value
setcar →
-- set head value of a cons and return it
head(value(2)) := value(3)
Yield(value(3))
setcdr →
-- set tail value of a cons and return it
tail(value(2)) := value(3)
Yield(value(3))
set →
-- mutate a binding
binding(env, value(2)) := value(3)
Yield(value(3))
eval →
-- evaluates the value of the argument

4.5 Reflection in LISP
99
if not done(eval) then
ProceedAt(value(2), eval)
else
Yield(value(eval))
. . .
We need to specify FullEvalForms such as cons to show how new parts
of the program can be dynamically constructed at runtime, and eval to show
how to execute such a part. We also need a way to prevent the immediate
execution of parts of a program, which is obtained by the quote special
form. quote is handled by PartialEvalForm, and in fact its entire purpose
is to return its argument unevaluated. So, for example, (quote (+ 1 2))
evaluates to (+ 1 2) — which can be used to build a new part of the program
— whereas an evaluating form such as (print (+ 1 2)) would print 3. The
special form if needs no further discussion, except to observe that its rather
cumbersome specification is in part due to the need to accommodate both
the if-then and if-then-else variant.
It is more interesting to comment the let special form. It has two argu-
ments; the first is a list of declarations, with each declaration being a pair
(symbol expr), whereas the second is the body, i.e. an expression to be eval-
uated in an environment where the declarations are in effect. To evaluate a
let, we evaluate all the exprs and bind them to the corresponding symbols
in the declaration list, and finally evaluate the body in the environment by
establishing the new bindings. The result of a let is the result of its body.
The last remaining capability we need is that of defining lambda. This is
particularly simple, because in our model the value of a lambda expression
is the lambda expression itself (in other words, lambdas are always implicitly
quoted, and a lambda expression is essentially a literal constant). Notice that
the actual application (i.e., invocation) of a lambda has been described in
Apply above.
PartialEvalForm =
case item(pos, 1) of
quote →Yield(item(pos, 2))
-- return arg unevaluated
lambda →Yield(pos)
-- return (implicitly quoted) lambda-exp
if →
if not done(2) then
ProceedAt(item(pos, 2), 2)
-- evaluate guard
else
if value(2) ̸= NIL14 then
-- if guard true
if not done(3) then
ProceedAt(item(pos, 3), 3)
-- compute the then-case
14 Historically, the global symbol T or *T* is used in LISP to denote the canonical true,
but any non-NIL value is also considered as true. NIL denotes not only the empty list,
but also the canonical false.

100
4 Dynamic Sequential Step Control
else
Yield(value(3))
-- return then-case result
elseif item(pos, 4) ̸= undef
-- there is an else-case
if not done(4) then
ProceedAt(item(pos, 4), 4)
-- compute the else-case
else
Yield(value(4))
-- return else-case result
else
Yield(NIL)
-- if guard false and no else-case
let →
-- will be (let ((x1 e1) . . . (xn en)) body)
let decls = item(pos, 2), body = item(pos, 3) in
choose i ∈range(decls) with not done(initi)
ProceedAt(item((item(decls, i), 2), initi)
ifnone
-- all declarations evaluated
if not done(3) then
let ne =new Env in
forall i ∈range(decls)
-- bind xi to computed values
binding(ne, item(item(decls, i), 1)) := value(initi)
parentEnv(ne) := env
-- record env
env := ne
-- continue with new env
ProceedAt(body, 3)
-- evaluate body
else
-- evaluation of body terminated
env := parentEnv(env)
-- reestablish parent env
Yield(value(3))
-- return computed body value
. . .
We now have all the ingredients LISP provides for reflectivity in a ho-
moiconic language:
1. symbols, bound via let or lambda to lists which are valid program frag-
ments, provide a way to identify and refer to parts of the existing pro-
gram;
2. cons and various list manipulations provide a way to construct new parts
and modify existing parts of the program;
3. application of a lambda, or using eval, provide a way to cause the exe-
cution of such parts;
4. quote provides a way to prevent the immediate evaluation of parts of the
program.
Of course, a practical language may well provide additional facilities, such
as defun (defining a function and binding it to a name), defparameter and
defconstant (binding variable or constant symbols in env0), print and
read (input/output operations), setf and setq (modifying the value of a
binding), do and loop (iterative statements), etc. However, these are ei-
ther primitives that must be implemented in native code, or short-hands
for more complex forms that can be defined on the basis of the special

4.6 Reflective RAM
101
forms we have specified. For example, (setq x 3) is an abbreviation for
(set (quote x) 3), whereas (defun f (x y) (+ x y)) is an abbreviation
for (set (quote f) (lambda (x y) (+ x y))) (the specification of set is
found in FullEvalForm above). The same goes for more advanced features
(e.g., class/object systems, namespaces, macros) that have been implemented
in various concrete LISP variants [103] but do not contribute additional ele-
ments to the reflectivity features of LISP.
4.6 Reflective RAM
As third concrete example we investigate reflectivity in an imperative lan-
guage. As characteristic example we consider the language of Random Access
Machine programs. The RAM model has been described on pg. 59 and by the
RamInterpreter (pg. 263). The Random Access Stored Program Machine
RASP is a reflective version of the RAM, with the same instructions and
their meaning, but without indirect addressing (because indirect addressing
can be programmed on the RASP, see Sect. 4.6.1 below).
To become reflective the RASP needs that its program instructions are
part of its state. The RASP has no explicitly named pgm memory, but for
simplicity, in its usual definition (see for example [6, Ch. 1.4]), the pgm the
machine executes is contained in a known part of the main memory, say in
the consecutive registers r2 r3 . . . rp for some program space number p ∈
Nat. Technically this is achieved via an instruction Gödelization15 instr(l) =
r2l+2 r2l+3 using two adjacent registers to contain (a Gödel number of) the
operation code respectively the operand of the instruction. Given the labeling
convention that the label l of an instruction instrl is a natural number l ≤m
(for some natural number m) this Gödelization occupies the first registers
r2 r3 . . . rp (where p = 2m + 3) (see Fig. 4.7).
More precisely, pgm is defined as a sequence instr(0) . . . instr(m) of pairs
of adjacent registers instr(l) = r2l+2 r2l+3 that contain a Gödel number of
the instruction’s command (called opCode) respectively of (the address of)
its argument (called operand):
let instr(l) = (l, cmd, arg) in
if arg = int i then r2l+2 = cmd int
r2l+3 = i
if arg = reg n then r2l+2 = cmd reg
r2l+3 = n
if arg = label then r2l+2 = cmd
r2l+3 = label
-- jump cmd
if cmd = Halt then r2l+2 = cmd
15 The method of Gödelization was invented in [84] to algorithmically relate logical
concepts and operations (e.g. deductions) in axiomatic systems to computations on
numbers. We use it here to encode non-numeric algorithmic instructions by natural
numbers.

102
4 Dynamic Sequential Step Control
Fig. 4.7 Architectures of RAM and RASP machine compared
RAM
RASP
instr0
instr1
instr2
...
instrm
program
memory
pgm
r0 = acc
r1
r2
...
data
memory
x1 x2
· · ·
xs
read-only input tape
y1 y2
· · ·
write-only output tape
r0 = acc
r1
r2 = opC0
r3 = opd0
r4 = opC1
r5 = opd1
...
r2m+2 = Halt
r2m+3
...
memory
pgm
(dynamic m)
data
x1 x2
· · ·
xs
read-only input tape
y1 y2
· · ·
write-only output tape
Without loss of generality we assume that the Halt instruction is the last
one in any given program (with label m) so that it marks the end of the pgm
Gödelization in the main memory. In this way it is easy to decode in each state
of a RASP computation the currently executed program pgm from the content
of the registers r2, r3, . . . , r2m+3 where r2m+2 contains the Gödel number of
(the opCode of) the Halt instruction. The program counter ctl in the RASP
model takes values in the set of register addresses {2, 4, . . . , 2m + 2}: in each
state it is the address of the first of the two consecutive Gödelization registers
of the currently executed instruction. Therefore the next function of the RAM
interpreter is refined for its RASP interpreter version to next(l) = l + 2.
Having RASP programs in the updatable and executable part of the mem-
ory permits to program in RASP the insertion, deletion, or modification of
instructions. In the specific case here we have the sequence pgm of instruction
registers as modifiable description—by updating, deleting or inserting adja-
cent register pairs r2x+2 r2x+3— of the currently executed RASP program
pgm. pgm is conceptually similar to the db memory of Prolog.

4.6 Reflective RAM
103
4.6.1 Reflectivity at Work (Indirect Addressing)
As noted in the preceding section there is no indirect addressing in RASP
instructions. The reason is that RAM instructions with indirect addressing
can be programmed by small RASP programs that exploit the reflectivity
capability, modifying some instruction registers in some execution step. We
show this in this section by an ASM refinement (of mixed types ((1, 1) and
(1, 6))) of any RAM program M by a RASP program M ∗defined as follows:
• First we refine each RAM instr(l) of M that does not use indirect address-
ing by an equivalent RASP instruction instr(l)∗. This is a step-by-step
refinement, a pure data refinement of refinement type (1,1).
• Then we refine each instruction instr(l) with indirect addressing in its
operand by an equivalent RASP program instr(l)∗. This is an ASM re-
finement of type (1,6) where one M-step of the RAM is refined by a
program of 6 successive M ∗-steps of the RASP.
By RAM/RASP steps we refer to the steps of the RamInterpreter with
program M respectively its RaspInterpreter version with program M ∗.
The latter is the same interpreter but without the indirect addressing
operands addr n in the instructions and without the rules that interpret
them. The (dynamic part of) states of a RAM/RASP program consists of
the current values of the instruction counter and the registers:
(ctlM, RegM) with RegM = r0,M, r1,M, . . . the same for M ∗.
For notational uniformity, in RegM we do not list the static RAM registers
(for example r0, . . . , rm) where the program M is stored. To simplify the
notation we write ctl and rn without the index M or M ∗where from the
context it is clear to which machine the location belongs.
A pair (S, S∗) of corresponding M and M ∗states consist of a state S
where M makes a step and a state S∗where M ∗starts the simulation of
that M-step. We define the to-be-defined simulation to be correct if in runs
of the two programs corresponding states satisfy the correspondence of their
locations of interest in the following sense:
• Accumulator correspondence: accM in state S and accM∗in S∗have the
same content.
• Register correspondence: for every n the registers rn,M in state S and
rs+n,M∗in state S∗have the same content. The offset s is defined to
satisfy that the encoding of the RASP program M ∗fits in the RASP
registers r0, r1, . . . , rs. Remember that for the simulation of M-runs we
do not need to consider the static RAM memory registers.
• Instruction counter correspondence: ctlM = l in state S and ctlM∗in
state S∗correspond to each other as follows: if M executes in state S
the instr(l) with label ctlM = l, then M ∗in state S∗starts to exe-
cute the simulation code instr(l)∗at its start address, so that ctlM∗=

104
4 Dynamic Sequential Step Control
startAddrSimCode(l). The function startAddrSimCode(l) defined below
denotes the address of the first register of the register segment that
Gödelizes the simulation code instr(l)∗.
For the initialization we define startAddrSimCode(0) = 2 (i.e. the Gödeliza-
tion of the first instruction of M ∗begins in the RASP register r2). We stip-
ulate that the two accumulators accM and accM∗contain 0, the same for all
M-registers rn and for all M ∗registers rn with n > s.16 We assume that the
two programs start with the same input tape and a blank output tape.
The computation of the simulation code instr(l)∗uses the RASP accu-
mulator. Therefore we must reserve one register (say r1,M∗) where M ∗when
starting the execution of the simulation code instr(l)∗can temporarily save
its current accumulator content.
We define instr(l)∗by induction on the labels of M-instructions instr(l).
There are two cases to distinguish.
Case 1: instr(l) does not involve indirect addressing, i.e. its operand is
not of type addr n. In this case M ∗is defined to execute the same instruc-
tion with parameters adapted. More precisely, instr(l)∗is the instruction
with the same opCode(instr(l)) and the same type (int or reg or jump la-
bel k) of the operand(instr(l)) but with appropriately incremented address
s + n in case the operand is reg n (for some n ∈Label) and with jump
target label startAddrSimCode(k) in case the operand(instr(l)) is a jump
label k. Initially (where l = 0)—the same in case 1 of an induction step—
this instr(l)∗is Gödelized in the two consecutive RASP registers with index
startAddrSimCode(l) + 2 and startAddrSimCode(l) + 3. Consequently, if M
has a next instruction (with label l + 1) we define startAddrSimCode(l + 1)
as startAddrSimCode(l) + 2.
Case 2: instr(l) does involve indirect addressing so that it has an
operand(instr(l)) = addr n. Let cmd be the opCode(instr(l)). To simulate
such an instr(l) by RASP instructions the idea is to supply the content of
rn in the RAM, say the index iM ∈Nat, as operand reg iM to the execution
of the cmd by M ∗. This involves a code manipulation and is achieved by
letting M ∗execute sequentially the following instruction sequence. We define
this sequence by its Gödelization (in the RASP registers starting with the
register labeled l∗= startAddrSimCode(l)) and explain for each instruction
the behaviour of M ∗when it executes the considered instruction.
16 The RASP registers r2, . . . rs are occupied by the simulation code M ∗, the RAM
registers r0, . . . , rm by the static to-be-simulated M-code.

4.6 Reflective RAM
105
label
opcode
operand behaviour
l∗
Store reg
save acc into register 1
l∗+ 1
1
r1 := acc
l∗+ 2
Load reg
load address iM from rn,M
l∗+ 3
s + n
acc := rs+n
using rn,M = rs+n,M∗
l∗+ 4
Add int
compute M ∗-address iM∗
l∗+ 5
s
acc := acc + s
adding M ∗-offset to iM
l∗+ 6
Store reg
store iM∗as operand for
l∗+ 7
l∗+ 11
rl∗+11 := acc
the (cmd reg)-instruction
l∗+ 8
Load reg
retrieve the stored acc
l∗+ 9
1
acc := r1
l∗+ 10 cmd reg
RaspInterpreter
l∗+ 11
rl∗+11
where rl∗+11,M∗= rn,M
• Step 1. r1 := acc: save the current accumulator content by placing it into
register r1.
• Step 2. acc := rs+n: load the address of the indirectly addressed RAM-
register M operates with when executing its instr(l). This address is the
current content of rn,M and can be retrieved by M ∗from register rs+n,M∗.
• Step 3. acc := acc + s: add to the retrieved address used by M the offset
s to obtain the address used by M ∗to execute the cmd of instr(l) by the
code sequence instr(l∗).
• Step 4. rl∗+11 := acc: store the computed address in the address register
rl∗+11 of the M ∗-instruction defined by the encoding registers rl∗+10 and
rl∗+11. This is the step where M ∗makes use of the possibility to modify
at run time the (encoding of the) program it is currently executing.
• Step 5. acc := r1: retrieve the content of the accumulator saved in Step
1 of the simulation code.
• Step 6. Execute cmd of instr(l) (with the computed parameter iM∗). Due
to Step 4, M ∗performs this cmd with the operand reg iM∗where the
number iM∗that is stored in the RASP register rl∗+11 equals the content
of the RAM register rn in the state where M executes the instr(l).
Given that M ∗needs 6 steps to simulate the given instr(l)-step of M, starting
in l∗= startAddrSimCode(l) we define startAddrSimCode(l +1) as l∗+12 (if
M has a next instruction with label l + 1).
Exercise 20. Show that every RASP program can be simulated by a RAM
program. Use indirect addressing to decode and simulate RASP instructions
that are stored in the RAM-memory. For a solution see [6, Sect. 1.4].

106
4 Dynamic Sequential Step Control
4.7 Reflectivity Styles
In Section 4.3 we tried to find a precise, conceptually simple but general
definition that captures the intuitive concept of sequential reflective pro-
cesses. Given the characterization of a large class of sequential algorithms
by PGAs we searched for a natural extension of the latter by some generic
easily refinable construct that makes them reflective. This led us to describe
self-modifying ASM programs as executable content of a dedicated updat-
able part of memory. For the sake of generality we have chosen to represent
the latter by a 0-ary dynamic function pgm whose content in each step is
read and evaluated for execution and whose function can be easily refined
to concrete reflection mechanisms in various programming languages. Note
that the parallelism of ASMs permits to nevertheless perform meta-level pgm
updates simultaneously with ‘ordinary’ object-level updates. When searching
for appropriate generic and easily refinable operations on ASM rules their
well-known parse tree structure offered a useful intuitive support, namely to
use without loss of generality just one abstract but well-known background
operation: rule-tree replacement. This operation covers the full spectrum from
single assignment rules s := t—an abstract version of the single instruction
view in programming languages—to arbitrarily complex ASM rules, including
a replacement of the entire current program. In addition, to permit reflection
also in a parallel (or even concurrent) context we wanted simultaneous re-
placements in different parts of the program to be supported, a requirement
that could be easily satisfied using the partial update technique of ASMs.
Therefore, in the ASM ground model for reflectivity a program can manipu-
late its own structure at any point, from term manipulations in assignment
instructions to a complete program change.
A background that supports this approach is a tree structure where one can
define a rather generic single operation to replace a program by another one.
We could have chosen a different program representation. We have chosen the
parse tree representation mainly because the tree substitution can be easily
refined to many forms of concrete program updates, covering a wide spectrum
of different realizations of reflectivity using different program structures and
different (computed and/or background) operations on them.
Above we have illustrated concrete versions of reflection for three char-
acteristic programming paradigms: logical (Prolog), functional (Lisp) and
imperative (RAM) programming. In this section we compare the different
styles and their relation to the abstract concept of reflective PGAs.
Prolog (Sect. 4.4) has an explicit separation of meta-level and object-
level (‘ordinary’) steps. In object-level steps a Prolog program performs only
operations that do not affect the given program, a list of clauses that is
called database db. In each meta-level step a Prolog program applies either
one of two kinds of db-read instructions (which trigger no update of db) or
one out of four kinds of dedicated, at the level of clauses atomic operations

4.7 Reflectivity Styles
107
that do change db. Each such step refines a reflective ASM pgm assignment
step of form pgm := replace(. . .) so that db directly instantiates the pgm of
a reflective ASM. No partial updates are needed because in each meta-level
step a Prolog program performs at most one (an atomic) db operation. In this
way the abstract replace operation is instantiated by the respective concrete
clause insertion, deletion (or inspection). New predicate symbols must be
declared as Dynamic. So the Prolog reflectivity model is an instance of the
ASM reflectivity model.
Lisp (Sect. 4.5) program states are given by the set of Expressions (with
its subsets Atom, Cons and functions head, tail), the Envvironment (with its
associated functions) and the graph traversal functions pos, result, return.
There is no explicitly defined separate pgm memory. Instead, the program
the LISP interpreter EvaluateExpr executes currently (in a program graph
traversal, see Figure 4.5) is distinguished as pos. Note that every syntacti-
cally correct list expression represents a program the LISP interpreter can
execute (read: evaluate). The only functions that can modify a given list
expression are head and tail so that one can view these two functions as rep-
resenting a global reflective pgm; otherwise stated, the meta-level operations
on list expressions are those that update or inspect head or tail or both, like
(setcar c2 c3), (setcdr c2 c3), (cons c2 c3) for updating, and (car c2),
(cdr c2) for inspecting.
The execution of an expression that modifies head or tail, resulting in a
modified expression that is going to be evaluated, can be seen as a refinement
of the updates to pgm in our reflective ASM model; in fact, one way to
make the refinement explicit would be to assume that pgm maps to the pair
(head, tail).
No partial updates are needed since due to the sequential character of LISP
computations a LISP program executes in each step exactly one (whether
object or meta level) operation.
RASP, the reflective version of the RAM, has no explicitly named pgm
memory, but in its usual definition (see for example [6, Ch. 1.4]) the pgm the
machine executes is contained in its (updatable) registers r2 r3 . . . rs for some
s ∈Nat (via a Gödelization instr(l) as described in Sect. 4.6). Therefore, as
long as the register labeling of this encoding is respected when instructions
are added or deleted one can consider the sequence pgm of those first registers
r2 r3 . . . rs as description of the current RASP pgm memory, similarly to the
db memory of Prolog. In this way a RASP program is in main memory and
can be changed during a run by updating, deleting or adding register pairs
r2x r2x+1 containing instruction data.
Clearly, there is a price to pay for a precise comprehensive explanation
of the intuitive program reflectivity phenomenon by a conceptually simple
and epistemologically satisfying extension (of a provably generic model) of
sequential algorithms. If one is interested in the detailed reflectivity features
of a particular programming language one is well advised to construct an

108
4 Dynamic Sequential Step Control
ASM ground model directly for the given language, instead of investigating
it as refinement of reflective PGAs. We illustrated this for the three tech-
nically different cases Prolog (for logic programming), Lisp (for functional
programming) and the reflective RAM (RASP for imperative programming).
The resulting ASM ground models come with specific reflectivity backgrounds
which can serve as basis for a direct investigation and comparison of the re-
flectivity model of the given languages. They all show that there is nothing
magic in reflectivity, contrary to a widespread belief: whatever new program
shows up at run time is result of a computation that has been programmed
(and hopefully well thought through!) before.
This is the place to mention another practical approach, exemplified by
the Java language, where the program itself cannot be manipulated, but
the language provides a set of utilities that afford reification of program-
ming concepts. Thus, in Java, one could have for example an instance of the
library class java.lang.reflect.Method whose properties describe a con-
crete method of some concrete class. However, that Method instance is not
the method itself, but only a concrete representation (i.e., a reification) of
the method, as a data object. The Method may well provide its own methods
to cause actions on the method it represents (e.g., invoke()), but these rely
on private conventions of the virtual machine. Java also provides ways to dy-
namically create new classes, e.g. by implementing a custom ClassLoader,
but that entails providing the bytecode for the body of the method. Essen-
tially, the programmer is required to implement a compiler in order to modify
(in limited17 ways) the running program.
Such forms of reification are different from genuine reflection in the sense
explored in this chapter, as from a computational point of view they do not
substantially differ from the case in which a program writes a text file with
some source code S, then causes the external compilation of S and thereafter
the execution of the compiled program C(S). An external compilation that
occurs concurrently (or possibly interleaved) with the execution of the main
program is also the case of Just-In-Time (JIT) compilers, such as those im-
plemented in most virtual machines (including many of those available for
executing Java, LISP and Prolog). In JIT compilation, the running program is
reflectively modified by an embedded compiler, which only uses two reflective
operations: (1) inspecting the source code, in order to compile it into native
executable, and (2) replacing the first instruction in the body of a method
by a jump to the first instruction of the compiled version of the body. This
process could be modeled by a minor change to the various interpreters we
have specified in this chapter. For example, to specify the typical behaviour
of a function-level JIT compiler it suﬀices to refine in the Apply macro of
our LISP model (pg. 97) the line
17 Among the limitations: while new classes can be generated on the fly, they are isolated
“inside” their original class loader for security reasons; moreover, entire new classes can
be created and added to the program, but existing classes cannot be removed from the
running program, nor dynamically modified.

4.7 Reflectivity Styles
109
ProceedAt(body, call)
by the conservative extension
if compiled(body) ̸=undef then
ProceedWithRASP(compiled(body), env, call)
else
if runCount(body) > JITThreshold
compiled(body) := JITCompile(body)
runCount(body) := runCount(body) + 1
ProceedAt(body, call)
The interested reader may want to consider the following rather diﬀicult
Exercise 21. Define an ASM model for the JITCompile(expr) function and
the ProceedWithRASP(memory, env, key) macro used above. The com-
piler would need to traverse the expression, but instead of applying the eval-
uation to the interpreter state immediately, it would emit RASP instructions
to the same effect. It can be assumed that in the RASP program, jumping
to certain well-known labels will cause the execution of specific rules of the
LISP interpreter (i.e., the RASP program can call back on the LISP virtual
machine if needed). The ProceedWithRASP rule would start the execu-
tion of the code produced by the JITCompiler, establishing some convention
on the use of registers to implement parameter passing and to collect and
return the result of the execution (to be stored in the provided key as usual).

Chapter 5
Control Structures of Multi-Process Runs
In this chapter we explain the concept of true concurrency (Sect. 5.1), i.e. of
runs of multi-agent interacting algorithmic processes where the single (usually
sequential) agents compute their steps under a genuine form of concurrent
asynchronous control. As first illustrative examples we use in Sect. 5.2 compu-
tations of communicating processes and describe a stepwise refined high-level
model of a web browser running JavaScript programs. In Sect. 5.3 we explain
Occam program runs where concurrency comes together with communication
and choice. We illustrate some typical provably correct stepwise refinements
of such runs towards an implementation.1
In Sect. 5.4 we study the context awareness property of concurrent pro-
cesses. For this we explain in Sect. 5.4.1 a simple framework to parameterize
ambient-sensitive processes. We apply it in Sect. 5.4.2 to define a precise
model of concurrent multithreaded Java runs that has been used for con-
currency analysis for Java and mutatis mutandis for C# programs. Another
interesting application is in Sect. 9.2.2 where we use parameterization to de-
fine teams of communicating Look-Compute-Move robots which operate over
graphs to solve various classical graph problems, each robot executing its
instance of a same ambient-sensitive protocol.
5.1 What are Concurrent Runs
Many reactive processes consist of multiple sequential processes that run
asynchronously, truly concurrently (not in synchronous parallelism and not
in interleaving manner), each at its own pace, and interact autonomously with
each other via interaction locations, i.e. input/output and shared locations,
without any designated controlling agent (read: there is no global clock and
1 For these rather simple proofs the precise definition of the general ASM-refinement
concept is not needed but it is defined in Sect. 7.
111
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
E. Börger, V. Gervasi, Structures of Computing, 
https://doi.org/10.1007/978-3-031-54358-6_5 
 
 
 
 
 

112
5 Control Structures of Multi-Process Runs
no agent has a full global view of the decentralized system). This breaks
the synchrony feature of parallel actions in atomic steps. In Def. 22 (pg. 40)
we could enrich the merely input-driven interaction of sequential processes
with their environment (see Sect. 3.3) by a simple alternation scheme for
updates of interactive locations that is rather useful in practice, but already
for multitasking as described in the interleaving Example 11 (pg. 66) the
notion of interaction with the environment is more complex, though still
based upon synchronous atomic read-and-write steps of a single processor
(see Def. 30, pg. 67 of sequentially consistent interleavings).
The crucial question is how to obtain, without introducing any explicit syn-
chronous control, read-cleanness for interaction locations, those whose values
become ‘globally visible’ in the concurrent view of runs of multiple agents.
Read-cleanness means that when a reading and a writing of a location by
different agents overlap, still the reading yields a well-defined value, either
taken before the writing starts or after the writing is finished [115, 117]. The
ASM framework offers a solution by splitting in concurrent runs S0, S1, . . . the
atomic read-and-write of interaction locations, at whatever level of abstrac-
tion, into a pair of two separate atomic actions, a read in a state Sn (needed
to compute an update set without applying it already) followed in some later
state Sm with m > n by a write, which is an application of the computed
update set. The case m = n describes the sequential atomic read-and-write
step semantics of ASMs (Def. 14, pg. 32). The case n < m reflects the con-
current refinement of this behaviour where each agent reads and writes its
locations at its own pace.
We therefore require2 that in every state of a concurrent run every agent
that is ready to make an interaction step can be in global or in local mode:
in global mode it can choose whether to GoGlobal—i.e. perform an atomic
read/write GlobalStep and stay in global mode—or to move into local
mode, locally recording (say by a subprogram CopyInteractData) the
current values of the interaction locations. From local mode the agent may
switch back to global mode, contributing (say by a WriteBack subprogram)
its computed updates for interaction locations to the global interaction state.
Definition 36 (Concurrent ASM Runs). Let A be a finite set of pairs
(a, pgm(a)) of agents a ∈A with PGA rule pgm(a), called a multi-agent
ASM. A concurrent run (or computation) of A is a sequence S0, S1, . . . of
states (started in an initial state S0) together with a sequence A0, A1, . . . of
subsets of A such that3 each state Sm+1 is obtained from Sm by applying
to Sm simultaneously for each agent a ∈Am that chooses to GoGlobal the
update set computed by pgm(a) in state Sm. An agent a ∈Am \ GoGlobal in
mode = global that does not want to GoGlobal is required to switch to local
mode, recording the current interaction location values (by a subprogram
2 The idea is taken from [54]
3 One can permit the set A to be (potentially) infinite. This includes the case of dy-
namically increasing sets of agents.

5.1 What are Concurrent Runs
113
CopyInteractData). In local mode pgm(a) starts a local subcomputation
(a submachine LocalStep); in this subcomputation it may eventually enter
a write back mode = wb (and from there again the global mode), executing
a subprogram that does WriteBack to the interaction state sequence the
values the submachine of pgm(a) has computed for (some of) its interaction
locations. The run terminates in state Sm if the updates computed by the
agents in Am are inconsistent.
This is expressed as follows (see also Fig. 5.1). The equation is a refinement
of the equation for deterministic single-agent steps (Def. 22, pg. 40):
Sm+1 = Sm +
[
a∈GoGlobal
eval(pgm(a), Sm)
GoGlobal = {a ∈Am |
(mode(a) = global and (mode, local) ̸∈Upd(pgm(a), Sm))
or mode(a) = wb
-- write back mode
if a ∈Am \ GoGlobal and mode(a) = global then
Upd(pgm(a), Sm) = {(mode, local)} ∪CopyInteractData
where
CopyInteractData = the updates generated by CopyInteractData
Def. 36 is a scheme we use often with other single-agent component ma-
chines instead of PGAs. For example, with non-deterministic PGAs (Def. 29,
pg. 63) evalCand is used instead of eval so that in a state S there may be
different continuations Sm+1 of the concurrent run. We leave it as an exercise
to define Sm+1 for nondeterministic PGAs.
Exercise 22. Define Sm+1 for nondeterministic ASMs. See Appendix B.
Remark on the pgm refinement. Note that with multi-agent machines
the 0-ary pgm function we used to define reflective machines (Def. 34, pg. 78)
is refined to a unary function pgm(a) whose arguments are agents. To make
these machines reflective it suﬀices to treat pgm(a) as a location so that in
each state A the program pgm(a)A is executed and (possibly) modified. This
is at times used to alter during execution the program associated to an agent;
see Sect. 5.2.1 for an example of application. In executable implementations
the pgm function is often called program.
Making single-agent steps concurrency-aware. Def. 36 above cap-
tures asynchronous behavior in the ASM framework by allowing every agent
to compute and apply at its own pace the update sets it contributes to a
concurrent run. The pairs of read-only and successive write-only steps each
agent makes in a concurrent run for its interaction locations can be imple-
mented by refined single-agent steps. The refinement preserves the—via the
par construct—synchronous parallel state-based ASM computation model at
the level of single autonomous agents. In fact, by separating reads of inter-
action locations from writing them every agent a can be viewed to perform
a globally visible read-and-write step of its pgm(a) in a concurrency-aware

114
5 Control Structures of Multi-Process Runs
manner by first a) only reading (and locally recording) in some state Sn the
current values of its interaction locations, then b) locally emulating at its
own pace its update computation step triggered by those interaction data,
and eventually c) writing back its own new interaction data. WriteBack(a)
means that a submits the locally computed updates of interaction locations
to a global state Sm where they become ‘globally visible’ (i.e. readable by
other agents) in the successive state Sm+1, with n ≤m (n < m reflecting
the asynchrony). Such a sequential three-steps-refinement ConcurStep(a)
of the one-step pgm(a) relates the abstract concurrent view of pgm(a)-steps
in a given concurrent run to a sequential implementation view, making the
underlying Look-Compute-Action Pattern (pg. 27) explicit, in the following
way (see Fig. 5.1 below):
• In global mode agent a can choose to perform one atomic GlobalStep(a)
by executing its pgm(a) as a normal atomic ASM read-write-step.
• In global mode agent a may also choose to perform three successive (and
further refinable, see Exercise 24, pg. 116) local substeps which emulate
the concurrent step of its pgm(a):
– first CopyInteractData(a), coupled with a switch to local mode,
– then emulate GlobalStep(a) by
· executing a LocalStep(a) on copies of interaction locations,
· followed by a WriteBack(a) and switching back to global mode.
The ConcurStep(a) components implement the concurrent run steps of
the given pgm(a) as expressed by the following proposition.
Proposition 1 (Implementing Single-Agents’ Concurrent Steps). Let
A be a multi-agent ASM of pairs (a, pgm(a)) of agents a ∈A with bounded-
choice PGA rule pgm(a). Each concurrent run R of A, say with states
S0, S1, . . . and subsets A0, A1, . . . of A, can be implemented by a parallel inter-
leaving run R′ where each pgm(a)-step in R is simulated in R′ either directly
by an atomic combined read-and-write GlobalStep(a) or by a sequence of
moves CopyInteractData, LocalStep (local emulation of the write-step
of pgm(a)), WriteBack agent a performs in R′ at the pace and with the
same choices as in R.
ConcurStep(a) =
if mode = global then
choose R ∈{GoLocal, StayGlobal} do R
if mode = local then
LocalStep(a)
-- emulate pgm(a)-step
mode := wb
if mode = wb then
WriteBack(a)
mode := global

5.1 What are Concurrent Runs
115
Fig. 5.1 ConcurStep: Component Steps in Concurrent Runs
global
choose
GlobalStep
CopyInteractData
WriteBack
local
LocalStep
wb
where
StayGlobal = GlobalStep(a) = pgm(a)
GoLocal =
CopyInteractData(a)
mode := local
• CopyInteractData(a) executes for all interaction locations (f , args) of
pgm(a) in the given state (with shared or monitored function symbol f )
an update ((f ′, args), f (args)).
• LocalStep(a) is a local copy of GlobalStep(a), namely the result of
replacing in pgm(a) every interaction function symbol f by a new function
symbol f ′ controlled by pgm(a).
• WriteBack(a) is defined similarly by copying back to f the new locally
computed values f ′(args) for every shared or output function f .
Exercise 23. Check your understanding of the proposition by proving it via
an induction on concurrent runs.
Concurrent versus Parallel Interleaved Runs. Def. 36 of concurrent
runs leaves it to scheduling refinements to determine how the subsets Am
of agents are formed that happen to be ready to simultaneously execute
a write step in state Sm. If in Def. 36 in every state Sm all involved agents
always perform an atomic read-and-write step in Sm, then the concurrent run
coincides with a parallel interleaved run (as defined at the end of Sect. 3.5).
A concrete example are runs of communicating processes (Sect. 5.2) or more
generally of processes with disjoint states (Sect. 5.4).
Note that in Definition 36 every state and every state transition from Sm
to Sm+1 represents a ‘snapshot of the run’ provided by those agents which
we see to interact by simultaneously applying to Sm the update set they
computed (maybe earlier) as read part of the concurrent implementation of
a read-and-write pgm(a)-step. Obviously the states of major interest for an

116
5 Control Structures of Multi-Process Runs
analysis of concurrent run behaviour are those where some agents perform
a GlobalStep(a) or a WriteBack(a) step because in those steps the in-
teraction locations are updated, whereas the other steps are of purely local
single-agent nature. The components GlobalStep(a) and WriteBack(a)
are also the only ones that may generate an inconsistent update set due to up-
date disagreements of different agents that become visible in the concurrent
state view; to CopyInteractData from global to local state view and to
perform LocalSteps does not become visible in the concurrent state view.
Exercise 24 (ConcurStep with local loop). Refine ConcurStep such
that agent a in local mode can decide whether it wants to make further inter-
nal steps before performing the WriteBack. The solution in Appendix B
(pg. 266) exhibits a scheme for implementing atomic actions by a sequence
of micro-steps, e.g. functional specifications by state-based behavioural de-
scriptions.
Remark on Atomicity. As stated above, to not loose the expressiveness
coming with atomic one-step abstract machine actions we deliberately treat
reading and writing in concurrent runs as possible atomic operations which
provide a clear result for every read or write operation. Doing this we trade
the read-freshness property for read-cleanness: when in local mode, an agent
may use a value of an interaction location that has already been updated
by another process. This means that in particular we abstract from possibly
overlapping reads/writes as they occur in relaxed memory models. Modeling
such features is a question of further detailing the atomic view at refined
levels of abstraction. For an example see the Location Consistency Memory
Model in [164].
Example 13. Partial Updates in Concurrent Runs. This example illus-
trates the use of update instructions to handle simultaneous cumulative up-
dates of shared locations by different agents in concurrent computations in
Occam.
As already explained in Example 10 (pg. 64), in an Occam run the agents
walk through the flowchart representation of their program. To execute a
par(S1 . . . Sn) statement an agent a creates n new agents ai (i = 1, . . . , n) as
its children and initializes them to enter the computation of Si, see Fig. 5.2.
Furthermore a sets a counter to the number of created children and goes to
sleep at its nextCtl node. Every child agent ai works in an asynchronous man-
ner, independently of the other agents; upon the termination of its subcom-
putation it decreases its parent’s counter and deletes itself. When the counter
becomes 0 all children have terminated their program and their parent agent
a becomes running again. This explains the following rule; remember that it
may happen that some agents terminate simultaneously.4
4 For conceptual simplicity we use the forall construct though the parallelism in Occam
programs is bounded so that an explicit description without forall could be given.

5.1 What are Concurrent Runs
117
Fig. 5.2 Occam par (S1 . . . Sn) statement diagram
par n
. . .
do all
all done
S1
Sn
Par(a, n) =
if a does par n then
forall i = 1, . . . , n do
let ai = new (Agent) in
env(ai) := env(a)
-- initialize the state of ai
pos(ai) := next(pos(a), i)
-- assign the subprogram Si to ai
parent(ai) := a
mode(ai) := running
-- set ai to run
count(a) := n
-- set running-children counter
pos(a) := next(pos(a))
-- move to next node
mode(a) := sleeping
-- wait for children to terminate
This rule needs two auxiliary rules: one describing the last step of a child
agent and one wakeup rule for the parent agent. The ChildTermination
may be executed simultaneously by more than one child b of a same par-
ent agent a, each of them wishing to subtract one from count(a). Therefore
we use an update instruction with an accumulate action that turns k oc-
currences of partial updates ((count, a), count(a) −1, accumulate) generated
simultaneously by children b of a into one update ((count, a), count(a) −k).
ChildTermination(a) =
-- simultaneous termination
choose agents5 ⊆{b ∈children(a) |
-- of some children
mode(b) = running and cmd(pos(b)) = end}
forall b ∈agents do
-- report termination and terminate
count(a) :=accumulate count(a) −1
Delete(b, Agent)
-- garbage collection: back to Reserve
5 This abbreviates choose x in P(children(a) with forall b ∈x holds mode(b) =
running and cmd(pos(b)) = end.

118
5 Control Structures of Multi-Process Runs
ParentWakeup(a) =
if mode(a) = sleeping and count(a) = 0 then
count(a) :=undef
mode(a) := running
5.2 Communicating Processes
In the definition of concurrent runs (pg. 112) the interaction locations (in-
put/output or shared) provide a direct form of process communication that
is independent of any specific communication medium. However this abstrac-
tion makes it diﬀicult to specify forms of process communication that delib-
erately avoid shared memory and rely upon an underlying communication
mechanism for the design and analysis of message passing, corruption, fail-
ure, security, etc. We therefore define here a class of multi-agent processes
which come with disjoint states and perform besides local actions only some
inter-process communication. We leave this communication scheme abstract
so that it can serve as refinable model for the major message passing systems
used in computing.6
Concretely we distinguish in communicating processes two types of single
computation steps, excluding any other external steps:
• internal steps which involve only internal data (controlled locations),
• communication steps we treat as atomic update instructions to Send,
Receive or Consume messages. Their interpretation involves a commu-
nication medium through which messages are sent and for each commu-
nicating agent a mailbox where messages are Received.
Definition 37 (Communicating ASMs). A system of communicating
ASMs is a multi-agent ASM A with triples (a, pgm(a), mailbox(a)) of agents
a ∈A, PGA (or more generally ASM) rule pgm(a) and a mailbox(a) satisfying
the following conditions:
• the agents’ signatures are pairwise disjoint so that each agent a has its
own internal state, also called its local state, where to be distinguishable
every function symbol f may be (implicitly) parameterized as fa,
• each program pgm(a) of agent a can contain the abstract communica-
tion actions (technically atomic one-step actions) Send(msg), Receive,
Consume(msg) and a Received(msg) predicate. Using Received(msg) is
a means to leave any particular feature of the Receive action for further
refinement.
With the communication constructs we deliberately abstract from details
like communication channels, constraints on messages (e.g. their sender, type,
6 A well-known example the reader may wish to look at are actor systems [98, 5] that
are closely related to Communicating ASMs.

5.2 Communicating Processes
119
content, sending time, multitude of sent/received messages), the way they are
transmitted (reliably or not) and delivered (corrupted or not), the way the
destination Receives them, etc. The constructs are used with the following
intended (refinable) interpretation:
• Send(m, to a) transfers the message (m, from self, to a) to the com-
munication medium which is supposed to deliver it to mailbox(a) (note
that there is no handshake requirement between sender and receiver of a
message),
• Received(msg) = (msg ∈mailbox(self)) means that the msg has been
delivered to the mailbox of the destination, where unless otherwise stated
mailbox is treated as a set,
• Receive = choose msg ∈mailbox do HandleMsg. This rule may
include Consume(msg)
• Consume(msg) = Delete(msg, mailbox(self)).
A complete message has the form (m, from a, to b) with message content
m (also called payload), sender a and receiver b. When it is clear from the
context, we omit notationally the sender or receiver of messages or write
(m, a, b) instead of (m, from a, to b).
Definition 36 (pg. 112) of concurrent runs applies to communicating ASMs
where—since there are no interaction locations but only local states—one can
assume that lastGlobalRead(a, m) = m holds for every write move of every
agent a ∈Am in state Sm, so that these runs are also parallel interleaved runs.
One of the best known examples of a system of communicating processes is
WhatsApp. It illustrates well that the components (the registered cellular
phones) work autonomously, each at the pace of its user, performing each
send or receive step possibly simultaneously with a step of some other users
but without being synchronized by any global clock. Due to their abstract
character concurrent runs of communicating ASMs can be refined to describe
frequently used communication patterns and serve for a rigorous analysis of
distributed algorithms; for some characteristic examples see [50, Ch. 3.3,
4.4], [124, 36].
5.2.1 Web Browser Model
To show a concrete Send/Receive pattern example we sketch in this sec-
tion the structure of a simple browser model with three refinements (by
layers describing successively environment, transport and stream details)
which describe the concurrent behaviour triggered by a web browser that
runs Javascript programs.7
7 The specification is taken from [83] where in a similar way the Request/Reply pattern
of a web server is specified at various levels of abstraction.

120
5 Control Structures of Multi-Process Runs
5.2.1.1 Browser User Layer
The top level of a browser model is where the interaction between the user
and the browser takes place, e.g. when the user opens or closes the browser
(or a new window or tab) and when the browser shows some documents to
the user. Windows or tabs within windows visualize a browser environment
we call browser context. Most browsers can work with a set of multiple inde-
pendent contexts which appear in an (initially empty) subset currContexts
of an abstract set Context. We describe only three basic user commands, just
to illustrate by a few characteristic concrete examples the kind of rules that
appear at this level in a complete model. We use two submachines StartBc
and StopBc to start or stop a browser context. These submachines together
with the notion of Context are refined in the context layer below.
Browser =
StartContext
-- includes start of the browser
CloseContext
CloseBrowser
The three submachines are triggered by userInput. We keep it abstract
here, together with the related Consume concept. There are various possible
refinements, notably by events in an eventQueue (provided by the window
manager or a graphical user interface tool or the operating system). See the
context layer below.
StartContext =
if userInput = startContext then
let newContext = new (Context)
StartBc(newContext)
Insert(newContext, currContexts)
Consume(userInput)
-- event consumption
CloseContext =
if userInput = (closeContext, context) then
StopBc(context)
Delete(context, currContexts)
Consume(userInput)
CloseBrowser =
if userInput = closeBrowser then
forall context ∈currContexts do
StopBc(context)
-- stop every element of currContexts
Delete(context, currContexts) -- empty entire set currContexts
Consume(userInput)
As a side note, most implemented browsers in fact execute an initial
StartContext on startup (this is equivalent to assuming that userInput is
startContext in the initial state).

5.2 Communicating Processes
121
5.2.1.2 Browser Context Layer
Here we define the notion of browser Context and the meaning of start-
ing/stopping a browser context to run. A browser context is characterized by
the following five elements:
• the current document that is presented to the user, a tree called DOM
(Document Object Model),
• the stack of documents the user has visited (in the given browsing con-
text), called session history,
• a window where the current document is presented and where the inter-
action with the user takes place,
• a Renderer that produces the user-visible graphical representation of
the current document,
• an EventLoop that receives and processes the local browser input (read:
operating system applied events).
Since we want to illustrate here the concurrent browser behaviour we con-
centrate our attention on the EventLoop, leaving the window and the work
of the Renderer abstract and assuming that the user interaction with the
window is handled by the operating system. For the same reason we skip
modeling the session history.
For the definition of StartBc(k) and StopBc(k) note the context pa-
rameter k which identifies the corresponding request/response pair. Upon
starting a new browsing context the DOM(k) is initialized by an initialDOM
(whose definition is provided by the implementation) and two agents are cre-
ated to execute independently the Renderer respectively the EventLoop
program for the given browsing context. These two agents contribute to the
concurrent browser run.
StartBc(k) =
let a, b =new (Agent), q =new (Queue) in
pgm(a) := Renderer(k)
pgm(b) := EventLoop(k)
DOM(k) := initialDOM
eventQueue(k) := q
-- initially empty
agents(k) := {a, b}
To stop a running browsing context its agents and DOM are canceled:
StopBc(k) =
forall agent ∈agents(k)
pgm(agent) :=undef
DOM(k) :=undef
agents(k) := ∅
A stated above we keep the Renderer abstract. So we only require that
there is a (system dependent) renderingTime(k) at which the Renderer will
GenerateUi(DOM(k), k) if no other agent has a lock on the DOM(k).

122
5 Control Structures of Multi-Process Runs
Renderer(k) =
if renderingTime(k) and not Locked(DOM(k)) then
GenerateUi(DOM(k), k)
The EventLoop(k) agent is created when the browsing context k is cre-
ated. Its role is to handle the events in the eventQueue of the browsing con-
text k.8 For the sake of illustration we define the behaviour for two typical
request/response pattern events, a new url request and a form submission
request. Other event types can be specified similarly.
EventLoop(k) =
if eventQueue(k) ̸= empty then
let e = head(eventQueue(k)) in
Dequeue(e, eventQueue(k))
if UrlRequestFromUser(e) then
HandleUrlReq(e, k)
if FormSubmission(e) then
HandleFormSubmissionReq(e, k)
...
-- rules for other event types
If the user provides a new url (e.g. by typing it in the browser’s address bar
or by selecting a bookmark), by a PageLoad that starts an asynchronous
GET Transfer the browsing context is navigated to that url and a new
DOM is created to keep the data of the requested document. PageLoad
comes with three parameters besides k: the GET method—other methods are
POST, PUT, HEAD, and some more we do not discuss—an url and no data
(some other components come with data, e.g. HandleFormSubmissionReq
below). The Transfer machine used by PageLoad has an additional call-
back parameter proc denoting the process that should handle the expected
response to the given request, namely the HtmlProcessor defined in the
stream layer below. It contains an HtmlParser component whose mode
must be set to Parsing. This explains the following definitions. The Transfer
machine is defined at the transport layer below.
HandleUrlReq(e, k) = PageLoad(GET, url(e), ⟨⟩, k)
where
PageLoad(method, url, data, k) = 9
Transfer(method, url, data, HtmlProc, k)
htmlParserMode(k) := Parsing
let d = new (DOM) in
DOM(k) := d
curNode(k) := root(d)
8 Therefore in the browser layer userInput = x is treated as an event, i.e. it is re-
fined to head(eventQueue(k)) = (userInput = x) and Consume(userInput) is refined to
Dequeue(userInput = x, eventQueue).
9 Here is the place for a session history rule.

5.2 Communicating Processes
123
If an event e is about the submission of a form, the machine computes
from it the relevant data and an url to send out a request to the new url
(MutateUrl) or to submit the data computed for the form (SubmitBody).
This explains the following definition where the vertical dots refer to GET
and POST methods of other schemas like ftp, javascript, data, mailto we do
not describe here.
HandleFormSubmissionReq(e, k) =
let f = formElement(e), m = method(f ), a = action(f ),
data = encodeFormData(f ), u = resolveUrl(f , a) in
case (schema(u), m) of
(http, GET) : MutateUrl(u, data, k)
(http, POST) : SubmitBody(u, data, k)
...
where
MutateUrl(u, data, k) =
let u′ = u · “?” · data in
-- compute new url
PageLoad(GET, u′, ⟨⟩, k)
SubmitBody(u, data, k) = PageLoad(POST, u, data, k)
5.2.1.3 Browser Transport Layer
This layer describes the HTTP Request/Response data exchange structure
between agents residing on different hosts. We define here the Transfer ma-
chine to initiate a complete HTTP request/response data exchange, as used
for example in the context layer PageLoad machine above. The Transfer
machine uses a submachine Send which does the eventual TcpSending of a
request but also activates a Receive machine instance which processes the
expected response stream. These three machines are defined at this level of
abstraction.
As we have seen in the context layer, Transfer comes with five param-
eters method, url, data, callBackProc, k (the latter being the Context of the
transfer). It records the first three parameters in corresponding context loca-
tions and then performs the specific actions forseen by the specific protocol
of the given url. We only describe the actions for the HTTP protocol: first
the cookies that match the url—they are stored by the Receive compo-
nent below—are retrieved using the function cookiesFor(url) to compute by
the makeHeader function the HTTP header of the message to be sent (that
depends also on method and url). Then the request body is formatted ap-
plying the function makeData to the parameter data and the destination
host address is computed by the function addressFor(url). This explains the
following definition.10
10 Note that we leave it to further refinements to specify aspects concerning transmission
reliability or quality of received data (e.g. being virus or advertisement free).

124
5 Control Structures of Multi-Process Runs
Transfer(method, url, data, callBackProc, k) =
meth(k) := method
url(k) := url
data(k) := data
if protocol(url) = http then
let cookies = cookiesFor(url),
header = makeHeader(method, url, cookies),
bodyData = makeData(data),
host = addressFor(url) in
Send(host, header, bodyData, callBackProc, k)
...
-- other transfer forms, e.g. file, ftp, etc.
To send request data to a host using the HyperText Transfer Protocol
(HTTP, which we will later refine as HTTP over TCP/IP [60]) some for-
mat constraints must be satisfied which we represent by an abstract header
concept; it includes a request line (indicating a method, protocol versioning
information, etc.) and a possibly empty header sequence of (key, value) pairs
followed by an empty line followed by a (possibly empty) data body to be pro-
cessed by the receiver of the request. To record the communication data we
use Buﬀers. The Send machine besides constructing the HTTP request and
sending it to the destination host (using the actual TcpSend) also creates an
agent a—which contributes to the concurrent browser run—and a new buﬀer
managed by this agent to process the response, using the Receive program
with the callback process determined by the initiator of the Transfer in
the given context k. This explains the following definition.
Send(host, header, bodyData, callBackProc, k) =
let buﬀer = new (Buﬀer), a = new (Agent) in
ag(k) := a
buf (k) := buﬀer
TcpSend(host, header · emptyline · bodyData, buﬀer)
mode(k) := expectStatus
pgm(a) := Receive(callBackProc, k)
We specify the Receive(callBackProc, k) machine only with a character-
istic frequent example for headers, namely a cookie header.
Receive(callBackProc, k) =
if mode(k) = expectStatus then
if IsLine(head(buf (k))) then
let l = head(buf (k)) in
status(k) := l
-- status determines type of response code
mode(k) := expectHeader
Dequeue(l, buf (k))
if mode(k) = expectHeader then
if IsLine(head(buf (k))) then

5.2 Communicating Processes
125
let l = head(buf (k)) in
Dequeue(l, buf (k))
if EmptyLine(l) then mode(k) := expectData
if SetCookie(l) then
forall cookie ∈l Record(cookie, url(k))
...
-- processing other headers
if mode(k) = expectData then
callBackProc(k)
Remark on locks in concurrent browser runs. Since in a concurrent
browser run multiple tranfers may occur simultaneously, the access to cook-
ies must be constrained by a locking mechanism, as we assume here using
abstract functions to access and modify cookies.11
Remark on program switch. In the last if-clause of Receive, one
could also have written pgm(self) := callBackProc(k). However, in that case
we would not have been able to change mode(k) after entering expectData,
which could have been useful to handle, say, network errors — which we omit
here.
5.2.1.4 Browser Stream Layer
At this level of abstraction we describe three typical stream processors that
are used for the call back processes invoked by the Transfer browser com-
ponent, namely for HTML, script and image streams.
We start with the HtmlProcessor. We specify three typical cases. De-
pending on the response code status in the given context the processor
parses an HTML document and builds the corresponding DOM (in case of
SuccessCode) or will RestartTransfer (with a new url provided by the
response in case of RedirectCode) or in case of ErrorCode handles the error
(notifying the user that a requested page loading failed).
HtmlProc(k) =
if SuccessCode(status(k)) then HtmlParser(k)
if ErrorCode(status(k)) then HandleHtmlError(k)
if RedirectCode(status(k)) then RestartTransfer(k)
...
-- rules for other return codes
We concentrate our attention on three (among many other) aspects of
the HTML parsing process which are important for the execution of web
applications, namely to build the DOM tree, to LoadResources and to
11 One could also apply the method described in [56] to embed the critical browser
components into an abstract control scheme that turns their concurrent behavior into
a transactional one. See Fig. 2.3. A similar remark applies in the stream layer below
where we specify details of operations on the DOM.

126
5 Control Structures of Multi-Process Runs
ExecScripts. The structure of this little parser is determined by three com-
ponents detailed below. For brevity we write in the following again k to denote
any context.
HtmlParser(k) =
if not paused(k) then
ParseText(k)
-- if head(buf (k)) is a piece of text
ParseTag(k)
-- if head(buf (k)) is a tag
FinishParsing(k)
-- if buf (k) is Finished
If the head of the buf (k) is a piece of text (if the buffer is not empty), this
text is dequeued and appended to the contents of curNode(k) in the DOM,
the parent node of the content that is currently parsed. This describes the
behaviour of the ParseText(k) parser component where we use a controlled
function curText(node) to store the accumulated text at the node.
ParseText(k) =
if buf (k) ̸= [ ] then let t = head(buf (k)) in
if IsText(t) then
AppendText(t, curNode(k))
-- write to DOM
Dequeue(t, buf (k))
where
AppendText(text, node) =
if curText(node) =undef then curText(node) := text
else curText(node) := curText(node) · text
The ParseTag(k) component is triggered if head(buf (k)) is a tag. It has
a subrule to OpenTags and a subrule to CloseTags. Both are applied if
the tag in question is both opening and closing.
• In case of an opening tag the DOM tree is extended by a new child node of
curNode(k); the curNode(k) moves there if the tag is only opening and not
also closing. If the tag requires background loading of further resources—
as characteristic examples we consider three cases: a script, an image
or a stylesheet—OpenTag will StartLoadResource calling the corre-
sponding Transfer component with the appropriate callback process (in
our examples ScriptProc, ImageProc or StyleSheetProc). These
background transfers run concurrently with the transfer of the main
HTML page, managed by the agent created to Receive the expected
resource (see the Send component defined together with Transfer in
the transport layer).
• In case of a closing tag that is not also opening the curNode(k) moves
back to its parent node. At this point some post-processing may be re-
quired. We consider the example of a closing SCRIPT tag </SCRIPT>.
In this case a script execution component ScriptExec triggers the (syn-
chronous immediate, asynchronous or deferred) execution of the loaded
code. This explains the following definition of ParseTag(k) and its com-
ponents whose behaviour we describe below.

5.2 Communicating Processes
127
ParseTag(k) =
let t = head(buf (k)) in
if IsTag(t) then
OpenTag(t, k)
CloseTag(t, k)
Dequeue(t, buf (k))
The components of ParseTag(k) are defined as follows. Other cases we do
not consider here can be specified in an analogous way.
OpenTag(t, k) =
if IsOpening(t) then
let n = new (Node) in
AddChild(n, curNode(k))
if not IsClosing(t) then curNode(k) := n
StartLoadResource(t, n)
where
AddChild(n, node) =
parent(n) := node
if ﬁrstChild(node) = undef
then ﬁrstChild(node) := n
else nextSibling(lastChild(node)) := n
For StartLoadResource(t, n) we consider only the cases of script code,
images and style sheets with their call back processes we describe in more
detail below.
StartLoadResource(t, n) =
case t of
<SCRIPT src=url> :
Transfer(GET, url, ⟨⟩, ScriptProc, n)
<IMG src=url> :
Transfer(GET, url, ⟨⟩, ImageProc, n)
<LINK rel=rl src=url> :
if “stylesheet” ∈rl then
Transfer(GET, url, ⟨⟩, StylesheetProc, n)
...
-- other cases requiring background transfer
For the possible post-processing to be done upon closing a tag we con-
sider only the script execution case. The invoked ScriptExecution com-
ponent manages the three kinds of execution of loaded code: immediate
(synchronous) execution, asynchronous execution (that is also started im-
mediately) and execution that is deferred (with information enqueued in
Deferred(k)) to be performed for each element in Deferred(k) when the com-
ponent FinishParsing is activated.

128
5 Control Structures of Multi-Process Runs
CloseTag(t, k) =
if IsClosing(t) then
if not IsOpening(t) then curNode(k) := parent(curNode(k))
case t of
</SCRIPT> : ScriptExec
...
-- other cases requiring post-processing
where
ScriptExec =
if IsAsync(curNode(k)) then
StartAsync(curNode(k), k)
elseif IsDeferred(curNode(k)) then
Enqueue((curNode(k), k), Deferred(k))
-- store for later exec
else
RunImmediate(curNode(k), k)
-- start exec now
FinishParsing(k) =
if Finished(buf (k)) then
-- loading and parsing finished
if Deferred(k) ̸= [ ] then
RunDeferred(k)
-- see below
else
FinalizeLoading(k)
pgm(self) := undef
-- parser program can be deleted
It remains to specify the execution of script components encountered in a
page. The synchronous (also called immediate) execution pauses the HTML
parser and inserts into the interpreter call parameters also the current context
with a callback component RunCompleted to restart the parsing once the
immediate code execution terminates. The reason for pausing the parser is
that HTML content can be injected into the page under construction from a
script, typically using the document.write() method. We leave the detailed
specification of this for a further refinement step and assume here that output
generated by document.write() or document.writeln() during a script
execution is collected in an ordered documentWriteBuﬀer(k) and prepended
to buf (k) just before resuming the parsing. Accordingly, we define:
RunImmediate(node, k) =
paused(k) := true
case type(node) of
text/javascript :
EcmaScriptInterpreter(node, RunCompleted, k)
...
-- other language interpreters
where
RunCompleted(k) =
buf (k) := documentWriteBuﬀer(k) · buf (k)
paused(k) := false

5.2 Communicating Processes
129
The main difference of asynchronous with respect to immediate execution
is that the asynchronous script execution is concurrent to parsing (subject
to locking for shared data access, typically concerning the DOM) so that the
HTML parser is not paused and no callback is performed when the called
interpreter terminates.
StartAsync(node, k) =
case type(node) of
text/javascript :
EcmaScriptInterpreter(node, NoOp, k)
...
-- other language interpreters
where
NoOp(k) =skip
As the name suggests deferred execution postpones the execution of a
script, namely until the entire page is loaded. Therefore a queue Deferred(k)
is used to store the information about the pending scripts and to execute
them managed by RunDeferred before executing FinalizeLoading. We
skip specifying FinalizeLoading that is about some final events fired by
the parser (see [163, Sect. 8.2.6]) and further operations for malformed doc-
uments.
RunDeferred(k) =
if Deferred(k) ̸= [ ] then
let node = head(Deferred(k)) in
RunImmediate(node, k)
Dequeue(node, Deferred(k))
It remains to specify the processors which process script and image streams.
The rules that describe their behaviour are structurally the same as the rule
specifying HtmlProc.
Script streams. Script streams appear when the browser loads script
code from a remote url; this happens when the HTML parser compo-
nent StartLoadResource defined above processes an element of form
<SCRIPT src=url>. When an error occurs the ScriptProcessor does not
notify the user but assigns an empty programText.
ScriptProc(k) =
if SuccessCode(status(k)) then ScriptParser(k)
if ErrorCode(status(k)) then programText(k) := “ ”
if RedirectCode(status(k)) then RestartTransfer(k)
...
-- rules for other return codes
For the sake of simplicity of exposition we formulate the parsing of script
source code in terms of text, using the same abstract functions used already
in the ParseText(k) component above. When the parsing of the content

130
5 Control Structures of Multi-Process Runs
of buf (k) is Finished the context is notified that the to-be-executed code is
Complete and the parser program can be deleted.
ScriptParser(k) =
if not Finished(buf (k)) then
let t = head(buf (k)) in
programText(k) := programText(k) · t
-- build the script code
Dequeue(t, buf (k))
if Finished(buf (k)) then
Complete(k) := true
-- signal that code is complete
pgm(self) := undef
Note that the execution of script code can start while the script is still
loading, but the interpreter can terminate only when the full program text
has been loaded and parsed (i.e. when Complete(k) has become true.)
Image streams. ImageProcessor and ImageParser have a similar rule
structure to that of the HTML and script processors and parsers. Therefore
we do not formulate it here although the incremental loading behaviour has
interesting image specific features (e.g. progressive image rendering with dif-
ferent quality).
5.3 Concurrency, Communication, Choice (Occam)
In this section we illustrate concurrent runs—and some of their proven to be
correct stepwise refinements towards the Transputer implementation—by ex-
ecutions of programs of the programming language Occam where concurrency
comes together with communication and choice. Such refinement correctness
proofs are methodologically important to produce reliable software-intensive
systems (see Sect. 7.2), although in this book we can only sketch the proof
method and refer for detailed proofs to [44].
5.3.1 Occam Ground Model
The Alt0 and Par commands (Examples 10, pg. 64 and 13, pg. 116) together
with the Communication instruction we add here describe already the con-
ceptual core of an Occam ground model (in the sense explained in Sect. 7.1).
Channel communication requires one reader and one writer. At this level of
abstraction we assume the synchronous view of instantaneous channel com-
munication: if only one communication partner is ready it stands still until
also the other partner becomes ready.12
12 For a more detailed definition and analysis of such bilateral synchronous communica-
tion models as used in operating systems see [40, 65]. In Sect. 6.2.1 (pg. 154) the reader

5.3 Concurrency, Communication, Choice (Occam)
131
Com(a, c, v; b, d, t) =
if a does c?v and b does d!t and c = d then
write eval(t, env(b)) to a at v
Proceed(a)
Proceed(b)
where
write α to a at v = (v := α)
v = bind(v, env(a))
c = bind(c, env(a))
d = bind(d, env(b))
For the sake of completeness we add the following rules for the set
SeqCmds of basic sequential commands of Occam.
Assignment(a) = if a does v := t then
write eval(t, env(a)) to a at v
Proceed(a)
Time(a) = if a does time?v then
-- time request
write timer(a) to a at v
-- record the current local time
Proceed(a)
If(a, cond) = if a does cond then
if eval(cond, env(a)) = true
then pos(a) := yes(pos(a)) else pos(a) := no(pos(a))
Skip(a) = if a does skip then Proceed(a)
Stop(a) = if a does stop then mode(a) := sleeping
Putting these rules together we obtain the following ground model for Oc-
cam (the rules ChildTermination and ParentWakeup have been defined
together with the Par rule, see pg. 117):
Occam0 =
SeqCmds
Com
Alt0
Par
ChildTermination
ParentWakeup
where
SeqCmds =par (Assignment, Time, If, Skip, Stop)
finds an asynchronous communication model where each communication channel has a
writer and a reader which access the channel independently of each other. Every bilat-
eral Send/Receive pattern can be defined as composition of refinements (see Sect. 7.2) of
two abstract ASM communication models SendPattern(m) and ReceivePattern(m),
similarly for multilateral communication models with ASMs OneToManySend and
OneFromManyReceive and their composition, see [10] and [50, Sect. 4.4].

132
5 Control Structures of Multi-Process Runs
It is called ground model because it tries to capture the intuitive understand-
ing of the semantics of the programming language. Consequently inspection
of this model must establish that it correctly reflects the design goals for Oc-
cam (and its Transputer implementation [105, 104]) because a mathematical
verification of the desired language properties is possible only with respect to
a mathematical definition of the language. Here we illustrate some first refine-
ment steps, moving towards the implementation. Since the ground model and
its refinements represent mathematical objects, properties of the refinement
steps which show their correctness can be mathematically proved, adding to
the reliability of the implementation. These two issues are explained further
in Ch. 7.
The successive refinement steps we are going to explain now replace the
synchronous by an asynchronous communication (via external Transputer
channels) (Sect. 5.3.2), optimize the communication by shared locations (in-
ternal Transputer channels) (Sect. 5.3.3), then proceed to a sequential imple-
mentation (Sect. 5.3.4) and prevent divergence (Sect. 5.3.5).
To simplify the exposition of the refinement steps we split the rule
Alt0 into three rules AltRule0(Gi), one per type of the guards: type
StateCondition for Gi = condi, type TimeCondition for Gi = condi :
time?after ti and type ComCondition for Gi = condi : chani ? vi. So Alt0
assumes the following form:
Alt0(a, (G1, . . . , Gn)) =
if a does alt (G1, . . . , Gn) then
choose i ∈{1, . . . , n} do AltRule0(Gi)
where
forall i ∈{1, . . . , n}
Gi ∈{condi, condi : time?after ti, condi : chani ? vi}
We simply collect for each guard parameter type (element of {StateCond,
TimeCond, ComCond}) the conditions for the rule to be fireable and the
effect of its application, resulting in three different parameterized rules
AltRule0(guard), one per type of guard.
AltRule0(condi) =
if eval(condi, env(a)) = true then
-- StateCond
pos(a) := next(pos(a), i)
AltRule0(condi : time?after ti) =
if eval(condi, env(a)) = true
-- StateCond
and timer(a) > eval(ti, env(a))
-- TimeCond
then
pos(a) := next(pos(a), i)
AltRule0(condi : chani ? vi) =
if eval(condi, env(a)) = true and ComCond(Gi) then
pos(a) := next(pos(a), i)

5.3 Concurrency, Communication, Choice (Occam)
133
let b, c, t with b does c!t and chani = c
write eval(t, env(b)) to a at vi
-- data transfer
Proceed(b)
where
ComCond(Gi) = forsome b, c, t b does c!t and chani = c
5.3.2 Refining Sync to Async Communication
The idea of this refinement step is to split the synchronization of input request
c?v and output offer d!t via channel condition c = d into 3 independent
asynchronous steps. This needs some new auxiliary functions:
• agents first register as reader/writer at a (now active) Channel:
reader, writer : Channel →Agent ∪{nil}
• then they wait (go to sleep)
• until the channel agent fires to transmit its mssg to the indicated place
(a variable identifier):
mssg : Channel →Value
place : Channel →Id
We therefore define a reader rule In, a writer rule Out and a Chan rule
to asynchronously simulate the synchronized ground model communication
Com(a, c, v; b, d, t). For reasons explained below such a communication must
be distinguished from a communication via an AltRule0 (with communica-
tion parameters (condi : chani ? vi)). Therefore we introduce a communica-
tion version c_mode of the mode function
c_mode : Agent →{altRunning, altSleeping, input}
so that a refined ‘ordinary’ communication (not via an alternative choice)
can be distinguished by c_mode input of the reader.13
In(a, c, v) = if a does c?v then
reader(c) := a
place(¯c) := v
c_mode(a) := input
put a asleep at next(pos(a))
Out(b, d, t) = if b does d!t then
writer(¯d) := b
mssg(¯d) := eval(t, env(b))
put b asleep at next(pos(b))
13 Rules In and Out need no check that when an agent accesses the involved channel
there is not yet a defined reader resp. writer. This follows from Occam’s Channel As-
sumption we do not explain further here: if two agents in some states both access a same
channel by rules In or Out then they are connected by a parent chain.

134
5 Control Structures of Multi-Process Runs
Chan(C) =
-- executed by channel agent
if Ready(C) and c_mode(reader(C)) = input then
write mssg(C) to reader(C) at place(C)
Wakeup(reader(C))
Wakeup(writer(C)
Clear(C)
where
Ready(C) iff reader(C) ̸= nil and writer(C) ̸= nil
Clear(C) =
reader(C) := nil
writer(C) := nil
Wakeup(a) = (mode(a) := running)
For the refined AltRule1 we must guarantee that choosing an alter-
native with communication parameter condi : chani ? vi can result in
a data exchange step only when both a reader and a writer are regis-
tered at chani. Therefore when an agent a starts to execute an alternative
command with some communication alternative, it can only announce at
the relevant channel its own readiness to choose that communication al-
ternative. So a registers itself at all involved channels as potential reader
with true condi (Register) and then will have to wait (in communication
c_mode = altSleep) for a writer. This case implies the need to wake up
a (switch to c_mode(a) = altRunning) when TimeCond (TimeWakeup)
or ComCond (ChanWakeup) become true. In case upon Registering at
some channel already a writer is registered and ComCond is true or the
minimal TimeCondition and StateCondition is already satisfied, a directly
Proceeds in c_mode = altRunning. This explains the following refinement
of the flowchart for an alternative command and the AltRegistration rule.
For simplicity of exposition we assume for the flowchart and the formu-
lation of the AltRegistration rule that G = . . . Gi . . . is grouped into
com/time/state segments:
• condi : chani ? vi for i = 1, . . . , k and 0 ≤k
• condi : time?after ti for i = k + 1, . . . , k + l and 0 ≤l
• condi for i = k + l + 1, . . . , k + l + m and 0 ≤m
AltRegistration(a, G) =
if a does altRegister(G) then
forall 1 ≤i ≤k Register(chani, condi, a)
-- where condi holds
minTime(a) := tmin
if StateCond or TimeCond or ComCond then
Proceed(a)
-- to execute the selection cmd altSelect
c_mode(a) := altRunning
else
put a asleep at next(pos(a))
c_mode(a) := altSleep
-- wait for TimeCond or ComCond

5.3 Concurrency, Communication, Choice (Occam)
135
Fig. 5.3 Occam1 alt (G1 S1 . . . Gn Sn) statement diagram
altRegister(G1, . . . , Gn)
altSelect(G1, . . . , Gn)
Sk+1
. . .
c1?v1
S1
. . .
Sn
where
Register(C, cond, a) =
if eval(cond, env(a)) then reader(C) := a
tmin =
(
min(T)
if T ̸= ∅
∞
otherwise
where
T = {eval(tj, env(a)) | k + 1 ≤j ≤k + l and
-- minimal time
eval(condj, env(a)) = true}
-- with condj = true
StateCond = (m > 0)
-- some guard has no time or channel cond14
TimeCond = (timer(a) > tmin)
ComCond = forsome 1 ≤i ≤k
eval(condi, env(a)) = true and writer(chani) ̸= nil
Two new rules are needed to wakeup agents waiting in c_mode(a) =
altSleep for ComCond or TimeCond to become true. ChanWakeup(C)
serves to wake up an agent registered at a channel and waiting for a writer
at that channel to satisfy ComCond. TimeWakeup(a) serves to wake up an
agent that is waiting for a TimeCond to become satisfied.
14 Whether a pure state condition condi, i.e. without additional time or channel re-
quirements (k + l + 1 ≤i ≤k + l + m), is true is checked when executing the selection
command altSelect. Obviously to be altSelectable such a guard must become true.

136
5 Control Structures of Multi-Process Runs
ChanWakeup(C) =
-- channel agent rule
if Ready(C) and c_mode(reader(C)) = altSleep then
Wakeup(reader(C))
c_mode(reader(C)) := altRunning
TimeWakeup(a) =
-- Occam agent rule
if timer(a) > minTime(a) and c_mode(a) = altSleep then
Wakeup(a)
c_mode(a) = altRunning
where Wakeup(a) = (mode(a) := running)
For the refinement Alt1 of Alt0(a, (G1, . . . , Gn)) there are three subrules
AltRule1 each of which refines the corresponding subrule AltRule0 of
Alt0.
Alt1(a, (G1, . . . , Gn)) =
-- same as Alt0 for cmd altSelect
if a does altSelect (G1, . . . , Gn) then
choose i ∈{1, . . . , n} do AltRule1(Gi)
where forall i ∈{1, . . . , n}
Gi ∈{condi, condi : time?after ti, condi : chani ? vi}
The state or time conditioned subrules AltRule1 are structurally iden-
tical to their abstract companions AltRule0: the alt command is replaced
by altSelect and the agent is required to be in c_mode(a) = altRunning.
AltRule1(condi) =
if a does altSelect (G1, . . . , Gn) and Gi = condi
and c_mode(a) = altRunning then
-- waiting for condi
if eval(condi, env(a)) = true then
-- StateCond satisfied
pos(a) := next(pos(a), i)
-- enter the choice
AltRule1(condi : time?after ti) =
if a does altSelect (G1, . . . , Gn) and Gi = condi : time?after ti
and c_mode(a) = altRunning then
if eval(condi, env(a)) = true
-- StateCond satisfied
and timer(a) > eval(ti, env(a))
-- TimeCond Satisfied
then
pos(a) := next(pos(a), i)
-- enter the choice
The communication conditioned AltRule1 differs from its abstract com-
panion by the fact that the data transfer cannot happen directly but only
after the choice, via an execution of a communication command ci?vi by
the reader (see Fig. 5.3 where the command ci ? vi is placed before Si for
i = 1, . . . , k). Therefore to execute the altSelect command with channel
parameter chani ? vi the agent only enters the alternative command and
UnRegisters from the channel. UnRegister makes sure that the commu-
nication (and therefore becoming reader at the involved channel) takes place
only after the choice (with parameters of the chosen alternative, redoing rule
In).

5.3 Concurrency, Communication, Choice (Occam)
137
AltRule1(condi : chani ? vi) =
if a does altSelect (G1, . . . , Gn) and Gi = condi : chani ? vi
and c_mode(a) = altRunning then
if eval(condi, env(a)) = true and ComCond(Gi) then
pos(a) := next(pos(a), i)
-- enter the choice
forall i = 1, . . . k UnRegister(chani, condi, a)
where
ComCond(Gi) = (writer(chani) ̸= nil)
UnRegister(C, cond, a) =
if eval(cond, env(a)) then reader(C) := nil
The result of this refinement of Occam0 (by introducing the external
Transputer channels [137]) is the following machine Occam1. To make the
refinement explicit we indent the new rules that replace the Occam0 rule
Com respectively Alt0.
Occam1 =
SeqCmds


In
Out
Chan
-- 3 parallel rules that refine Com


AltRegistration
ChanWakeup
TimeWakeup
Alt1
-- 4 parallel rules that refine Alt0
Par
ChildTermination
ParentWakeup
where
SeqCmds = par (Assignment, Time, If, Skip, Stop)
Theorem 2 (Refinement By External Channels). Occam1 is a correct
and complete refinement of Occam0 by external channels.
Proof. The explanations of the refined rules show that and how each ab-
stract step in a concurrent Occam0 run can be simulated by corresponding
concrete steps in a concurrent Occam1 run (completeness). For the correct-
ness proof we map Occam1 runs to equivalent Occam0 runs as follows:
• If in state Si the rule R an agent a ∈Ai executes is the last step in a run
segment containing all concrete steps that simulate an abstract R∗, then
we map R to R∗(using R = R∗for homonymous rules).
State and time conditioned Occam1 rules are mapped to their homonymous
Occam0 companion. Denote by AltStatei the rule AltRulei with param-
eter cond (for i = 0, 1), analogously for AltTimei and AltComi. We map
AltState1 to AltState0 and AltTime1 to AltTime0. For applications
of the Chan rule there are two cases.

138
5 Control Structures of Multi-Process Runs
• Case 1. If Chan is reached in the given run via corresponding rules In
and Out (in any order) in states before Si, this application of Chan is
mapped to Com.
• Case 2. If otherwise Chan is reached in the given run via some of the
AltComRules, where
AltComRules =
{In, Out, AltRegistration, AltCom1}
∪{ChanWakeup, TimeWakeup}
then this application of Chan is mapped to an application of a corre-
sponding communication conditioned AltCom0.
• All other rules that are not in AltComRules∪{AltState/Time1, Chan}
are mapped to themselves.
For a detailed proof see [44, Sect. 4.1].
5.3.3 Optimized Communication
The Transputer supports concurrent execution of multiprocesses on one pro-
cessor. To reflect this we add to the Par rule a processor placement update
processor(ai) := proc. Concentrating the attention on one processor, com-
munication can be optimized via internal channels which are implemented in
the Transputer by shared locations. Shared channels permit to let the com-
municating agents (instead of a Channel agent) directly perform the data
transfer when accessing an internal not idle channel.
To achieve this by a refinement of Occam1 we refine the signature and
some rules. For the signature:
• we merge the two functions reader, writer into one function agent,
• we replace the domain Channel of the functions msg, place by Agent.
For the operational refinement we refine the In/Out rules to the corre-
sponding rules InIdle/OutIdle for accessing idle internal channels and add
a companion rule InReady respectively OutReady which when accessing a
not idle internal channel immediately does the data exchange work of Chan.
Furthermore, we incorporate the work of the ChanWakeup rule into a new
rule AltOutWakeup.
To define the refined rules for internal channels we let InIdle(a, c, v) do
the work of In(a, c, v) for internal channels where no writer showed up yet:
a makes itself waiting in c_mode(a) = input for input from a writer.
InIdle(a, c, v) = if a does c?v and Internal(¯c) and Idle(¯c) then
agent(¯c) := a
place(a) := v
c_mode(a) := input
put a asleep at next(pos(a))

5.3 Concurrency, Communication, Choice (Occam)
139
InReady(a, c, v) does the work of Chan(¯c) for internal channels where a
writer is waiting to communicate its data to a reader.
InReady(a, c, v) =
if a does c?v and Internal(¯c) and not Idle(¯c) then
write mssg(agent(¯c)) to a at v
Wakeup(agent(¯c))
Proceed(a)
Clear(¯c)
Analogously OutIdle(a, c, t) does the work of Out(a, c, t) for internal
channels where no reader showed up yet: a records its data and makes itself
waiting for a reader to communicate with.
OuIdle(a, c, t) = if a does c!t and Internal(¯c) and Idle(¯c) then
agent(¯c) := a
mssg(a) := eval(t, env(a))
put a asleep at next(pos(a))
OutReady(a, c, t) does the work of Chan(¯c) for internal channels where a
reader is waiting in c_mode(a) = input to receive input from a writer:
OutReady(a, c, t) =
if a does c?v and Internal(¯c) and not Idle(¯c)
and c_mode(agent(¯c)) = input then
write eval(t, env(a)) to agent(¯c) at place(agent(¯c))
Wakeup(agent(¯c))
Proceed(a)
Clear(¯c)
When the ComCondition a reader is waiting for becomes true one must
wake up the reader. In Occam1 this is done by ChanWakeup. Here the
writer must do it and wait that the AltCom2 rule in question will be chosen.
So we incorporate ChanWakeup into the rule OutAltWakeup to make
the writer ready for the communication (if chosen).
OutAltWakeup(a, c, t) =
if a does c!t and Internal(¯c) and not Idle(¯c)
and c_mode(agent(¯c)) ̸= input then
-- a reader is waiting
if c_mode(agent(¯c)) = altSleep then
-- ChanWakeup
Wakeup(agent(¯c))
-- wake up reader
c_mode(agent(¯c)) := altRunning
mssg(a) := eval(t, env(a))
-- record data to transmit
agent(¯c) := a
-- record writer
put a asleep at next(pos(a))
-- wait for com choice
As a result we obtain the refinement Occam2 of Occam1.

140
5 Control Structures of Multi-Process Runs
Occam2 =
SeqCmds
Par
ChildTermination
ParentWakeup
 InIdle
OutIdle
-- rephrased In/Out

InReady
OutReady
-- simulating work of Chan
 OutAltWakeup
-- simulating work of ChanWakeup
AltRegistration
TimeWakeup
Alt1
Theorem 3 (Refinement by Internal Channels). Occam2 with internal
channels is a correct and complete refinement of Occam1.
Proof. The explanations of the refined rules show that and how each ab-
stract step in a concurrent Occam1 run can be simulated by corresponding
concrete steps in a concurrent Occam2 run (completeness). For the cor-
rectness proof we map Occam2 runs to equivalent Occam1 runs as follows,
using the same technique as for the correctntess of Occam1 with respect to
Occam0:
(In/Out)Idle are mapped respectively to In/Out
(In/Out)Ready are mapped respectively to
In/Out followed by Chan
OutAltWakeup is mapped to Out followed by ChanWakeup
Every other Occam2 rule R is mapped to the homonymous rule R from our
previous Occam1 model.
NB. This mapping preserves the set of agents, their states (environment,
position), communication traces, termination, deadlock, divergence. For a
detailed proof see [44, Sect. 4.2].
5.3.4 Sequential Implementation
Consider any sequential Processor with an agentQueue of active Agents a
processed by P = processor(a) which share the external timer(processor(a))
(a data refinement of timer(a)) and are executed by the unique currAgent
whose value is at any time either some of the agents placed on processor P
or nil.
The introduction of a unique active agent whose task is to serve an
agentQueue of processes subject to some time requirements requires three

5.3 Concurrency, Communication, Choice (Occam)
141
kinds of data and operation refinements we are going to explain in this sec-
tion, concerning mode (becoming a question of being in the agentQueue),
concerning minTime (which becomes a list of time conditions tmin of agents
in the agentQueue), and concerning the sequential implementation of par-
allelism (refined rules for Par, ChildTermination and the removal of the
ParentWakeup rule).
Refinement concerning mode. The crucial data refinement is the refine-
ment of (a does C)2—the guard (a does C) as defined in Occam2 to guar-
antee that only the currAgent can execute a step:
(a does C)3 = (a = currAgent and cmd(pos(a)) = C)
There are three more refinements concerning mode.
• mode(a) = running is refined to: a is in agentQueue.
• put a asleep at n is refined to put currAgent asleep at n that is defined
as follows:
put currAgent asleep at n =
pos(currAgent) := n
currAgent := nil
Note that this update will usually be followed by a Dequeue step which
activates the next queue element:
Dequeue =
if currAgent = nil and not agentQueue = [ ] then
currAgent := ﬁrst(agentQueue)
agentQueue := rest(agentsQueue)
• Correspondingly Stop and Wakeup(a) are refined to:
Stop3 = if a does stop then a := nil
Wakeup3(a) = Enqueue(a, agentQueue)
Refinement concerning minTime. minTime is refined to a list of agents
with their waiting time (in the order of the time values).
minTime : Processor →(Agent × Nat)∗sorted by time
Since we are concerned here with only one processor we simply write minTime
instead of minTime(proc). The definition requires a refinement of three rules
where minTime appears:
• In AltRegistration3 minTime(a) := tmin is refined to an order-
respecting insertion of (a, tmin) into the list minTime.
minTime := insert((a, tmin), minTime)
where insert is assumed to respect the time order

142
5 Control Structures of Multi-Process Runs
• Refinement of TimeWakeup:
TimeWakeup3 =
if minTime ̸= [ ] then
let minWait = minimal waiting time in minTime
if forsome (a, minWait) ∈minTime
ReadyFor(altRunning, a)
then choose
(a, minWait) ∈minTime with ReadyFor(altRunning, a)
Wakeup(a)
c_mode(a) := altRunning
minTime := delete((a, minWait), minTime)
where
ReadyFor(altRunning, a) =
timer > minWait and c_mode(a) = altSleep
• In AltRule1 with time parameter add
minTime := delete((a, −), minTime)
to prevent a from being chosen once more (by Dequeue)
Refinement concerning sequential implementation of parallelism.
This refinement concerns the three rules related to Occam’s parallelism.
The refinement of Par (see Example 13, pg. 116) is part of the mode
refinement in the sense that putting the created children into running mode
becomes now to Enqueue them.
Par3(a, n) = if (a does PAR n)3 then
forall i = 1, . . . , n let ai = new (Agent) in
Enqueue(ai, agentQueue)
-- in any order
-- mode update disappears
env(ai) := env(a)
-- initialize the state of ai
pos(ai) := next(pos(a), i)
-- assign the subprogram Si to ai
parent(ai) := a
count(a) := n
-- set running-children counter
put a asleep at next(pos(a))
The refinement of ChildTermination is characterized by three features:
• at each moment only one agent a = currAgent executes the rule
• no partial updates of count(parent(a)) are needed any more
• the ParentWakeup rule is optimized away by including it into the
ChildTermination3 rule:
ChildTermination3(a) =
if (a does end)3 then
-- only one agent a = currAgent
count(parent(a)) := count(parent(a)) −1
-- no partial updates
if count(parent(a)) = 1 then Wakeup(parent(a))
-- the last one terminates the Par implementation
Delete(a, Agent)

5.3 Concurrency, Communication, Choice (Occam)
143
Thus we arrive at the sequential implementation model Occam3. To let them
stand out we indent the modified rules.
Occam3 =
SeqCmds with refined Stop3
InIdle
OutIdle
InReady
OutReady
OutAltWakeup
Par3
-- put children into agentQueue
ChildTermination3
-- deleting ParentWakeup
AltRegistration3
-- insert (a, tmin) into minTime sequence
TimeWakeup3
-- minTime lookup for some minWait < timer
AltRule3
-- when selecting delete (a, t) from minTime
Dequeue
-- to let currAgent = nil fetch the next a to execute
Theorem 4 (Sequential Implementation of Occam). Occam3 is a cor-
rect refinement of Occam2 and also a complete one if there is no divergence
(where one agent runs alone forever).
Proof. As above we concentrate on the correctness part mapping Occam3
runs to Occam2 runs.
ChildTermination3 is mapped to ChildTermination2
if count(parent(a)) > 1
ChildTermination3 is mapped to
ChildTermination2 followed by ParentWakeup
if count(parent(a)) = 1
TimeWakeup3 is mapped to the set of TimeWakeup2(b)
forall b with (b, minWait) ∈minTime
Each other Occam3 rule R is mapped to the homonymous Occam2 rule R.
For proof details see [44, Sect. 4.3].
5.3.5 Time-Slicing Refinement
Time-slicing eliminates the divergence phenomenon from Occam3. The sim-
ple refinement idea is that every agent a can be a = currAgent only for a fixed
period. When this time is Elapsed a is Enqueued by a new rule TimeSlice.
TimeSlice = if Elapsed and currAgent ̸= nil then
Enqueue(currAgent, agentQueue)
currAgent := nil
where Elapsed = timer −start > period

144
5 Control Structures of Multi-Process Runs
The start time is updated when an agent becomes Active, so that this
update is added to Dequeue3:
Dequeue4 =
if currAgent = nil and not agentQueue = [ ] then
currAgent := ﬁrst(agentQueue)
agentQueue := rest(agentsQueue)
start := timer
-- refinement by begin of execution time
Executing a command is once more constrained, here by the time not yet
being Elapsed:
(a does C)4 = (a does C)3 and not Elapsed
The correctness and completeness of this model follow immediately.
Theorem 5 (Sequential Occam Model with Time-Slicing). The re-
finement Occam4 of Occam3 by time-slicing is correct and complete.
Methodological Remark. The first steps to go from Occam4 via compi-
lation to a proven-to-be-correct Transputer code model consist in linearizing
the flowchart-based nondeterministic model, followed by a refinement to se-
quential search of the alternatives. For the details we must point the reader
to [43]; here we could only illustrate by a real-life example how such stepwise
refinements of complex models help the practitioner to build, check (validate
and verify), modify, understand and document those models for reuse (see
Sect. 7.2).
5.4 Concurrency, Parameterization, Context Awareness
Parameterization is a fundamental technique to define an abstract entity
that can be instantiated into a variety of concrete instances by replacing the
abstract parameters by concrete values. A well-known example is provided
by the class concept in object-oriented programming languages. Abstracting
from any language specific particularities one can view a class as given by an
ASM pgm that defines a multi-agent ASM (agi, pgm)i∈I with a set of agents
agi (i ∈I for some index set I) with same program pgm, same signature,
maybe even same initialization but different evolution of their state, for ex-
ample due to different environmental influences. Such a parameterization of
pgm as pgm(a) is a special case of pgm(params) with arbitrary parameters,
as used for example in the Call by Name mechanism defined in Sect. 2.3.
Parameterization offers also a clean and simple way to explicitly define pro-
gram environments that influence the execution of programs. This provides
in particular a technique to eﬀiciently design and analyze concurrent systems
constructed out of single-agent subsystems. We illustrate this in Sect. 5.4.2 by
a model for multithreaded Java, using the concept of context aware ambient
ASMs defined in Sect. 5.4.1.

5.4 Concurrency, Parameterization, Context Awareness
145
5.4.1 Context Aware Processes (Ambient ASMs)
In this section we sketch a general method to design and analyze context sen-
sitive processes. We illustrate in Sect. 5.4.2 a typical application that refines
the single-agent Java program interpreter from Sect. 3.2 to a truly concurrent
thread model. Another application is in Sect. 8.4.1.1 where in recursive calls
of an algorithm we must provide fresh instances with encapsulated states
for the execution of the called algorithm. The reader might also recognize in
the usage of a context k to parameterize operations of the Browser model
(Sect. 5.2.1) an example of this common requirement.
One can specify and manage context sensitivity by introducing environ-
ment parameters with respect to which terms and rules are evaluated. For
the sake of generality we do this via an extension of ASMs by rules of form
amb exp in M where M is an ASM rule and exp an arbitrary expression. exp
determines the level of abstraction of the considered environment.
Definition 38 (Semantics of Ambient ASMs). The behaviour of an am-
bient ASM amb exp in M consists in evaluating exp and use the obtained
value as environment (passed by value) for the execution of M.
To define the meaning of executing M in an environment we parameterise
each context sensitive function f in the signature of M implicitly by env
so that evaluating f (t) in a state is turned into an evaluation of f (env, t).
Everything else in the behavioural definition of ASMs remains unchanged.
f (env, t) is also denoted by fenv(t) to stress that a context sensitive func-
tion f is turned into a family of independent functions fenv, separated one
from the other by the specific environment. This feature permits to precisely
describe widely used encapsulation techniques (for states, computations, pat-
terns, etc.), as is illustrated in Sect. 5.4.2 by separating the states of inde-
pendent threads in concurrent runs.
Therefore the interaction type definition of functions (Sect. 2.1.1) is ex-
tended by what we call context sensitive or environment dependent functions.
Exercise 25 (Reduction of Flat Ambient ASMs). In the general hier-
archical form of the ambient ASM concept one can record environments in
a stack; the whole stack then becomes an additional parameter to context-
sensitive functions. In many applications the flat version of Ambient ASM
we explain and use here suﬀices where at each moment only the most recent
environment is used. Show how flat ambient ASMs can be transformed to
equivalent ASMs without occurrences of amb exp in M rules. For a solution
see [50, pg. 137].

146
5 Control Structures of Multi-Process Runs
5.4.2 From Sequential to Concurrent Runs (Java/C#)
In this section we first extend the imperative Java component JavaI and its
procedural extension JavaC (Sect. 3.2) with the thread-handling component
JavaT, which in the sequence of stepwise refined models of [159] completes
the definition of a single-agent Java interpreter:
Java =
JavaI
-- imperative instructions, Sect. 3.2.1
JavaC
-- procedural (class) instructions, Sect. 3.2.2
JavaO
-- object-oriented instructions, not shown here
JavaE
-- exception handling instructions, not shown here
JavaT
-- thread handling instructions
Then we use the concept of context sensitive processes (ambient ASMs) to
specify truly concurrent Java program runs, namely as runs of the following
Java program instances:
amb a in
Synchronize
Java
where
Java is the above single-agent interpreter of Java programs
a ∈Thread
-- see below the definition of Thread
The component Synchronize guarantees that the agents a ∈Thread of the
instances respect the Java synchronization discipline.
The thread instructions model JavaT. In Java threads are indepen-
dent concurrently running processes within a single program. We view them
as agents a equipped with a Java pgm(a) to execute. They are represented
as Ref erences in the heap—defined in JavaO, it records the classOf (ref ) of
Objects together with the values of instanceFields(C)—and exchange data
with other threads via the heap and the globals function of their class. The
independence results from the fact that every thread executes an instance of
the Java interpreter Java with its own frame stack and current frame, which
we denote by a function localState. This function is local to the thread and
cannot be accessed by other threads, a fact we model below by the ambient
concept.
localState : Thread →(Frame∗, Frame)
The thread extension (of JavaE) consists only of a rule for synchronized
statements which allow one thread a at a time to exclusively execute a pro-
tected code area. To guarantee this, Java associates a unique lock to the
target reference which must be grabbed by the thread that wants to execute
the protected code and must be released upon completion of that code. A

5.4 Concurrency, Parameterization, Context Awareness
147
function sync(a) records the sequence of all references grabbed by a thread
a. Since a thread may hold multiple locks for the same object also a lock
counter locks is needed.
sync : Thread →Ref ∗
locks : Ref →Nat
To execute a synchronized (exp) stm statement first the expression is
evaluated: if the computed value ref is null, a NullPointerException is thrown
(modeled in JavaE); otherwise it is checked whether the current thread al-
ready holds the lock of object ref . If it does, one more lock is grabbed and
execution of the (protected) statement can be started. Upon return from the
stm execution (whether Normally or with an abruption) the thread must
ReleaseLock and passes control to the parent node. If the current thread
does not yet hold the lock of ref it enters execution mode Synchronizing
(from the normal exec(thread) = Active).15 Thereby it becomes subject to be
chosen by the synchronization management to grab the lock on its syncObject
ref and to perform its next execution step in a given run. This explains the
following definition of JavaT. For the special case of ThreadAbruption see
below.
JavaT = case context(pos) of
synchronized (αexp) βstm →pos := α
-- first evaluate exp
synchronized (▷ref ) βstm →
if ref = null then failUp(NullPointerException)
-- see JavaE
else
if ref ∈sync(thread) then
-- thread already holds a lock on ref
sync(thread) := [ref ] · sync(thread)
-- grab another lock
locks(ref ) := locks(ref ) + 1
-- counter update
pos := β
-- enter protected stm
else
-- become subject to being scheduled
exec(thread) := Synchronizing
syncObj(thread) := ref
-- record synchronization object
synchronized (αref )▷Norm →ReleaseLock(Norm)
synchronized (αref )▷abr →ReleaseLock(abr)
ThreadAbrupt
where
ReleaseLock(phrase) =
let [r] · rest = synch(thread)
synch(thread) := rest
locks(r) := locks(r) −1
yieldUp(phrase)
15 Since we do not explain here the thread methods offered by Java (but see [159,
Sect. 7.2.2]) we neglect here the other four exec mode values: NotStarted (when a thread
is created, value that remains unchanged until the call of the thread’s start method
whereby the thread becomes Active), Waiting (entered by execution of Java’s wait
method), Notiﬁed (entered by execution of Java’s notify method), and Dead (entered
by killing the thread).

148
5 Control Structures of Multi-Process Runs
The rule ThreadAbrupt refers to the abruption and the class initializa-
tion mechanism in Java. If during the execution of a synchronized statement
an abruption occurs it is passed up by ReleaseLock(phrase) except at a
synchronized statement for which there is a special rule. That rule takes
into account a constraint on class initialization explained below. Therefore
propagatesAbr has to be refined as follows (we include the try/finally cases
from JavaE).
propagatesAbr(phrase) iff phrase is none of the following statements:
labeled, static initializer, try, finally, synchronized stm
If an abruption reaches the beginning of a thread’s program and its frame
stack frames is null, then the thread is killed (after updating some wait
mechanism data we are not going to explain here). So ThreadAbrupt is
defined as follows.
ThreadAbrupt =
static ▷abr →notifyThreadsWaitingForInitialization
abr →if pos = ﬁrstPos and frames = [ ] then Kill(thread)
where
Kill(t) =
forall t′ ∈waitSet(t) exec(t′) := Notiﬁed
waitSet(t) := ∅
exec(t) := Dead
To explain what happens when a thread abrupts an initialization method we
have to refine the Initialize(c) machine: it must insert the condition that
only one thread should execute the class initialization. Therefore we record
it as initThread when it changes classState(c) from Linked to InProgress and
calls the class initialization method. Other threads that try to initialize the
class when its initialization is InProgress are suspended (put into execution
mode Waiting so that they are not Runnable) and registered in a set initWait
of all such threads by the following rule:
if classState(c) = InProgress and initThread(c) ̸= thread then
exec(thread) := Waiting
-- exclude thread from Runnable ones
Insert(thread, initWait(c))
Upon abruption of the initialization method the current thread must make all
these waiting threads Active again; it sets initThread to undef and empties
the initWait set.
notifyThreadsWaitingForInitialization =
let c = classNm(meth)
-- the current class initialization meth
forall t ∈initWait(c) exec(t) := Active
initWait(c) := ∅
initThread(c) :=undef

5.4 Concurrency, Parameterization, Context Awareness
149
The concurrent Java thread model ConcurJavaThread. The ma-
chine Java interpretes any Java program as executed by a single agent. Ap-
plying the concept of ambient ASM it is easy to use the interpreter to de-
fine a model for truly concurrent execution of a Java program with threads
(a, pgm(a)) where pgm(a) essentially is the parameterization of Java by a.
More precisely each thread a is equipped with the following program. The
Synchronize component serves to grab the lock a has computed on a new
syncObj(a) and expects to be granted to continue its execution in the given
run.
amb a in
Synchronize
Java
where Synchronize =
if exec(a) = Synchronising then
sync(a) := [syncObj(a)] · sync(a)
-- add the new lock
locks(syncObj(a)) := 1
-- initialize the new lock counter
exec(a) := Active
The environment sensitive functions are the frame stack function frames
and the frame component functions meth, restbody, pos, locals. It means that
these functions define the local state of a and are not accessible to any other
thread. A concurrent run of threads (a, pgm(a))a∈A is then defined by Def. 36
(pg. 112) with the following additional synchronization management con-
straint on the set Am of agents which make a step in state Sm:
• every chosen a ∈Am is Runnable (i.e. Active or Synchronising),16
• no chosen thread a ∈Am is Synchronizing on a syncObj(a) that is locked
by some other agent,
• if a thread a ∈Am is Synchronizing no other thread a′ ∈Am is
Synchronizing on the same syncObj(a).
For a mathematical analysis of an interleaving version of this thread model
see [159] and its analogue for C# in [158].
16 We disregard here the Waiting mechanism implemented by Java’s wait method. See
[159] for the details about this and other thread methods of Java.

Chapter 6
Mixed Synchronous/Asynchronous
Control Structures
In this section we explain two classes of computational processes with a mixed
asynchronous/synchronous form of run control: bulk synchronous parallel and
streaming algorithms (with neural networks, spreadsheets and the TCP/IP
protocol as examples). Another outstanding example are recursive algorithms
we analyze in Sect. 8.4.
6.1 Bulk Synchronous Parallel (BSP) Control
The bulk synchronous parallel control model1 is a computation paradigm
that is widely used for the analysis of parallel algorithms. The concurrent
runs of processes connected via a communication network consist of alternat-
ing segments of either only local process steps or only communication steps
and where the alternation is controlled by a synchronizer. We explain the
Bulk Synchronous Parallel (BSP) paradigm of computing as an interesting
refinement of the concept of concurrent computations (Def. 36, pg. 112).
A BSP system is a multi-agent system of finitely many sequential processes
(ai, Mi) with 1 ≤i ≤k, to be executed asynchronously on k processors.
Their local signatures Σlocal
i
are disjoint. The processes can exchange data
only via communication channels ci,j (1 ≤i ̸= j ≤k). Such channels can be
introduced as functions where for Mi each ci,j is declared as output function
(to pass data from agent ai to agent aj) and each cj,i as monitored function
(to read data passed by agent aj to ai), as depicted in Fig. 6.1.
Furthermore, phases where (in modei = local) all processes perform only
local actions of their processPgmi are separated from phases where (in
modei = comm) they all just exchange data following their commPgmi. A
synchronizer agent a0 Switches between the phases in such a way that if its
mode0 is local and coincides with the modei of all processes, they can perform
1 The model we explain here is taken from [76].
151
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
E. Börger, V. Gervasi, Structures of Computing, 
https://doi.org/10.1007/978-3-031-54358-6_6 
 
 
 
 
 

152
6 Mixed Synchronous/Asynchronous Control Structures
Fig. 6.1 Writing through channels from source to target
ai
aj
ci,j
write
read
cj,i
write
read
local processPgmi-steps; the same way, if for every i mode0 = modei = comm,
the processes can communicate using their commPgmi. This is expressed by
the following definition of BSP programs Mi the BSP agents are equipped
with.
Definition 39 (BSP Process Rule). A BSP process rule is a rule of the
following form:
if modei = local = mode0 then processPgmi
if modei = comm = mode0 then commPgmi
The local BSP process rule processPgmi is a PGA rule defined inductively
starting from assignment instructions of one of the following two forms:
• modei := comm, terminating the current local process phase of the i-th
process, or
• f (t1, . . . , tn) := t with f ∈Σlocal
i
\ {modei}
The communicating BSP process rule commPgmi is a PGA rule defined
inductively starting from assignment instructions of one of the two following
two forms:
• modei := local, terminating the current communication phase of the i-th
process, or
• ci,j(t1, . . . , tn) := t.
In a run the synchronizer a0 opens the next communication phase when
the processes have terminated their current local computation phase and (via
updates modei := comm) are all ready for a communication phase; a0 closes
this communication phase (and reopens the next local process phase) when
every process has terminated its communication steps for this phase (via an
update modei := local). Therefore the Switch action of the synchronizer is
defined as follows:2
2 Note that the quantifier stands for a conjunction of fixed length.

6.2 Streaming (Data Flow) Control
153
Switch =
if mode0 = local and forall i = 1, . . . , k modei = comm then
mode0 := comm
if mode0 = comm and forall i = 1, . . . , k modei = local then
mode0 := local
Summarizing we define:
Definition 40 (BSP-ASM). A BSP-ASM is a multi-agent ASM B =
(ai, Mi)0≤i≤k where the synchronizer has the program M0 = Switch and
each process agent has as program Mi a BSP Process Rule with local BSP
process rule processPgmi and communicating BSP process rule commPgmi.
Every B-computation leading from modei = local (for all i = 0, . . . , k)
after a Switch step to a state with modei = comm (for all i = 0, . . . , k)
and then by another Switch step back to a state with modei = local (for
all i = 0, . . . , k) is called a BSP-superstep. It consists of a concurrent local
computation segment of Switch and processPgmi (i = 1, . . . , k) followed by a
concurrent communication segment of Switch and commPgmi (i = 1, . . . , k);
both segments are terminated by a synchronization step. Runs of the BSP-
ASM are sequences of such BSP-supersteps.
6.2 Streaming (Data Flow) Control
As for the BSP case in the preceding section, a streaming system is a multi-
agent system but without central synchronizing agent a0. The streaming (also
called dataflow) computation paradigm is based on the idea that the agents
are autonomous—meaning that potentially they can execute at any time—
and that it is the availability of data which controls the execution of their
otherwise asynchronous computation steps. Which agents are observed in any
state Sm of a concurrent run to simultaneously perform a globally observable
step—i.e. the elements of the set Am in Def. 36 (pg. 112)— is not determined
by program counters, but rather by the availability of the input data needed
for the operation of each agent, in other words by the data flow rather than
(or, in addition to) a control flow. Given that input data are provided by
the environment or by other system components at maybe unpredictable
times, streaming systems exhibit naturally the non-deterministic behaviour
of concurrent systems. On the other hand, by the characteristic scheduling
constraint to execute a computation step as soon as all input data is available,
and no later, streaming systems provide eﬀicient computation, which can be
an advantage for certain classes of problems.
The streaming paradigm has been used in hardware implementations op-
timized to the processing of streams of data, e.g. in digital signal processors
(DSP), including the common case of real-time encoding and decoding of mul-
timedia data, as well as in specialized hardware for computer networks. It has

154
6 Mixed Synchronous/Asynchronous Control Structures
also been used in software architectures, to eﬀiciently process series of actions
(e.g., transactions in an e-commerce system) or events (e.g., event-oriented
programming in user interfaces). The streaming paradigm is also at the core
of several computational models and languages, including Kahn’s Process
Networks [108], Hoare’s Communicating Sequential Processes (CSP) [102]
and later derivatives such as Occam (Section 5.3).
In this section we first define a stream control scheme in terms of three
basic components Read, Compute, Write with corresponding predicates
CanRead, DoneRead, CanWrite, DoneWrite, DoneCompute (Sect. 6.2.1).
The scheme is an example of the Look-Compute-Action pattern (pg. 27)
which we instantiate in Sect. 6.2.2 by appropriate refinements of the compo-
nents. The resulting modules can then be used to build concrete streaming
machines by feature composition of basic components. We illustrate this by
three well-known streaming machines: neural networks (Sect. 6.2.3), spread-
sheets (Sect. 6.2.4), and streaming in computer networks, here illustrated by
defining a rigorous high-level model of the TCP/IP protocol (Sect. 6.2.5). The
last two examples exhibit a dynamic topological communication structure.
6.2.1 High-Level Streaming Model
A streaming system (also called streaming machine) is a multi-agent system of
finitely many sequential processes (ai, Mi) (i = 1, . . . k) that are connected by
communication lines called Channels. Every channel behaves as FIFO buffer
and can be connected to at most two agents, one which writes (its output) to
the buffer and one which reads (its input) from the buffer, asynchronously.
Obviously only what has been written to a channel can then be read from
that channel, but in streaming systems these two actions do not happen
at the same time, differently from the instantaneous communication concept
Com(a, c, v; b, d, t) of Occam explained in Sect. 5.3. Here, we extend the BSP
concept of channel ci,j by defining structured objects which provide queueing
and addressing through three functions on a set Chan we leave abstract here:
buﬀer : Chan →Buﬀer
reader : Chan →Agent
writer : Chan →Agent
When an agent ai is connected as reader to a channel we say that this chan-
nel is an input channel of ai and denote it by ini,k (for some k), denoting
the finite set of input channels of ai by Ini = {ini,1, . . . , ini,ki} (for some ki);
analogously, when an agent ai is connected as writer to a channel we say that
this channel is an output channel of ai and denote it by outi,l (for some l),
denoting the finite set of output channels of ai by Outi = {outi,1, . . . , outi,li}
(for some li). We say that Connected(ai, aj) via chan holds—so that commu-
nication can happen between the agents ai, aj through this channel—if ai is

6.2 Streaming (Data Flow) Control
155
the writer of chan and aj its reader. We write Connected(ai, aj) if for some
channel Connected(ai, aj) via chan holds.
We abstract from various channel details such as whether the buffers are
bounded or unbounded, what their maximum capacity may be, etc. by as-
suming that on each channel c we have two predicates IsReadable(c) and
IsWriteable(c) that indicate whether the channel is readable by an agent
(i.e., it contains some data) and whether it is writable (i.e., if the buffer is
unbounded, or it is bounded and has not reached full capacity). In addition,
we will assume that agents can write a value to a channel (thus enqueuing
the value on the channel buffer) and read from a channel (thus removing a
value from the queue of the channel buffer, which is returned as result of the
read operation). Using the buffer representation of channel data we abstract
from the finite but arbitrarily large transmission time of data from an output
channel of one agent to an input channel of another agent. We assume that
conflicting buffer updates are reconciled as to implement an atomic queue.3
We abstract here also from the size of the concrete in-memory representa-
tions of a data item, and consider buffer positions as always holding a single
logical data item.
Each streaming machine component Mi has the following Read-Compute-
Write program structure (see also Fig. 6.2 below). It is an instance of the
Look-Compute-Action pattern (pg. 27) with to-be-detailed stage components
(PGA rules) Readi, Computei, Writei, and for each stage Start/Termi-
nation predicates CanReadi/DoneReadi, CanWritei/DoneWritei and Done-
Computei. It is possible that different agents have different components and
different predicates.
Mi =
if modei = reading and CanReadi then
Readi
if DoneReadi then modei := computing
if modei = computing then
Computei
if DoneComputei then modei := writing
if modei = writing and CanWritei then
Writei
if DoneWritei then modei := reading
Each agent in every round goes sequentially over its three stage modes,
which are not synchronized with those of other agents. In reading mode, an
agent waits for the needed data to be available in its input channels, and if so
reads the data. In computing mode the agent performs whatever computation
is necessary to obtain the output values. In writing mode these values are
written to output channels when possible, and the agent goes back to reading
3 The operations can be seen as partial updates on the buﬀer(c) associated to a channel
c, see Section 4.1.

156
6 Mixed Synchronous/Asynchronous Control Structures
Fig. 6.2 Control State ASM Diagram for agents of a streaming system
reading
CanReadi
Readi
DoneReadi
computing
Computei
DoneComputei
writing
CanWritei
Writei
DoneWritei
yes
yes
yes
no
no
no
the next set of data. Each stage may consist of a single or of multiple steps, as
controlled by the various Done predicates. Notice that for reading and writing
(I/O operations which may depend on external events) we have symmetric
controlling predicates CanRead and CanWrite, whereas for computing, which
is a purely internal operation, there is no corresponding guard CanCompute.
6.2.2 Refined Streaming Variants
In this section we show how different refinements of the Readi, Computei,
Writei components and of the corresponding predicates, together with ad-
ditional restrictions on how agents are connected through channels, yield
well-known classes of streaming machines. We begin with a widely used
functional refinement FctalDataFlow that involves all components. Then
we define a class StatefulDataFlow of streaming machines by a re-
finement of the Computei component, a class SelectDataFlow of non-
deterministic streaming machines by a refinement of the Readi component,
and a class NonSysDataFlow of non-systolic streaming machines by a re-
finement of the Writei component. We illustrate a further refinement to
the class GuardedSelectDataFlow where in addition to data availabil-
ity also other conditions can be imposed on reading data. Then we briefly
discuss systems with dynamic channel structure.
Functional Data Flow. The simplest case is purely functional and con-
cerns all components. To trigger a Read-Compute-Write round all input ar-
guments (inputi,1, . . . , inputi,ki) must be defined, so that all ki input channels
must be ready to be read together in one step (so that for the functional case

6.2 Streaming (Data Flow) Control
157
no termination test DoneReadi is needed and can be assumed to be always
true)4. Then all output locations outputi,j are computed in parallel, simulta-
neously, as a function fi,j of the read input channel values (so that also the
termination test DoneComputei is not needed). Finally, all li output channels
are required to be ready to write the functional output outputi,1, . . . , outputi,li
of this round in one step (so that also the termination test DoneWritei is not
needed).
This yields the following refinement FctalDataFlowi of program Mi:
CanReadi = forall c ∈Ini IsReadable(c)
Readi = forall 1 ≤k ≤ki do inputi,k := read from ini,k
Computei = forall 1 ≤l ≤li do outputi,l := fi,l(inputi,1, . . . , inputi,ki)
CanWritei = forall c ∈Outi IsWriteable(c)
Writei = forall 1 ≤l ≤li do write outputi,l to outi,l
By the synchronous parallelism in the component rules and predicates
of FctalDataFlow machines this computational paradigm realizes what
is called a systolic behaviour, its results proceed in waves along the net-
work. Kahn’s Process Networks (KPN) [108] are of this form with unbounded
channels (so that CanWritei will always be true). This variant also mod-
els computations by feedforward neural networks [107], with appropriate fi,j
(Sect. 6.2.3).
Stateful Data Flow. With a refinement of the Computei component
one can support stateful dataflow graphs. Allow a refinement LocCompi of
Computei to maintain across different rounds a persistent local state, say
of a finite set localsi of variables (0-ary functions). We can assume that the
variables in localsi are appropriately initialized in the initial state of the agent,
and can be both used and assigned during the computation (potentially,
across multiple steps, depending on DoneComputei).
We also assume as before that by the time DoneComputei becomes true,
LocCompi stores in outputi,l any final results (which may depend on the
value of localsi) that should be written on output channels; for a stateful
refinement of the functional machine FctalDataFlow this implies that the
functions fi,l have the local state localsi as further argument. The resulting
refinement defines a class of StatefulDataFlow machines.
Non-deterministic Data Flow. With a refinement of the Readi com-
ponent one can enable agents to choose reading a selection of channels with
available input. We call such a set P of input channels a selection pattern,
and write IsReadable(P) to mean that all channels in P are readable.
When a selection pattern P IsReadable, a round can be started with the
data from the channels in P (i.e. without waiting for all other input channels
to be readable).5 To model this let SelectPatterni be a set of subsets Pi,s ⊆
4 This comes up to delete the stage termination test in Fig. 6.2
5 Note that if rounds can be started with partial input without any fairness constraint
among patterns, then some input channels may never be read.

158
6 Mixed Synchronous/Asynchronous Control Structures
{1, . . . , ki} of indices k of input channels ini,k (for 1 ≤s ≤si). In the reading
phase an agent waits until some selection pattern P IsReadable and then
performs its local computation (and writes corresponding results) depending
on which pattern has been selected to activate the Readi component. This
leads to a class SelectDataFlow of streaming machines with the following
refinement of Readi and CanReadi. Note that, as for the data flow pattern,
after a step of Readi for a chosen pattern the read step is done so that no
further termination test DoneReadi is needed and DoneReadi can be assumed
to be always true in this model.
CanReadi = forsome P ∈SelectPatterni IsReadable(P)
Readi =
choose P ∈SelectPatterni with IsReadable(P)
forall k ∈P do inputi,k := read from ini,k
actPatterni := P
In addition, if so desired, one can enable Computei to take advantage
from the information on the chosen actPatterni by performing different com-
putations depending on the value of actPatterni. For the refinement of the
functional model it suﬀices to provide actPatterni as an additional argument
to the fi,l functions; in the general case one can explicitly distinguish in the
Computei program what to do in the various cases of actPatterni.
Similarly, at the writing stage one could select which channels to write to,
based upon the information on the value of actPatterni or of the computed
outputi,l locations, as done for example in the non-systolic variant described
below.
Exercise 26 (Ordered selection patterns). Replace the choice among
multiple competing selection patterns by a rule that tests selection patterns
in a fixed order, say Pi,1, . . . , Pi,si. See the Appendix B.
It may be noted that the pure functional variant FctalDataFlowi de-
fined above is a special case of the non-deterministic activation pattern vari-
ant, with SelectPatterni = {Pi,1} and Pi,1 = {1, . . . , ki}. At the other ex-
treme, we have the special case with si = ki and Pi,s = {s}, i.e. input is
handled separately for each channel as soon as it is available. This latter case
is not frequently encountered in practical systems, although there are notable
exceptions.6
Non-Systolic Data Flow. A non-systolic behaviour is characterized by
a refinement of the Writei component, writing all defined outputs with
writable corresponding channel as soon as there is some such output. This
6 For example, the select or poll system calls in UNIX, used to suspend a process while
waiting for at least one of a set of file descriptors to become ready. Another example is
an agent merging individual data items from a number of input channels, in order of
arrival, into a single output channel.

6.2 Streaming (Data Flow) Control
159
can be obtained by the following refinement NonSysWritei of Writei and
yields a class NonSysDataFlow of streaming machines.
CanWritei = true
Writei =
forall 1 ≤l ≤li with outputi,l ̸=undef and IsWriteable(outi,l) do
write outputi,l to outi,l
outputi,l :=undef
DoneWritei = forall 1 ≤l ≤li outputi,l =undef
Non-systolic systems tend to be more eﬀicient (in fact, computations which
need only some of the results of an agent can proceed immediately after the
result is available), at the cost of loosing temporal coherence between the
different results from the same computation. Streaming systems for which
temporal coherence is important (e.g., multimedia processors) will usually im-
plement systolic behaviour. Conversely, asynchronous systems (e.g., a packet
switching network component) will implement non-systolic behaviour.
Exercise 27. Refine the non-systolic streaming model described above by
introducing the concept of output pattern, so that partial results are written
out to channels only when all outputs identified by one of a set of patterns can
be written simultaneously (notice that this requires both the output variable
to be defined, and the corresponding out channel to be writeable, for each
output of the pattern).
Guarded Selection of Data Flow. Some programming and specification
languages, e.g. the Language Of Temporal Ordering Specification (LOTOS)
[24], or libraries that implement the streaming paradigm provide affordances
to test additional conditions or guards (in addition to data being available)
before proceeding with reading from a channel. For our models, it suﬀices
to assume an additional function on channels, peek(c), to return the same
value that read from c would return if IsReadable(c) = true, and undef
otherwise. Guards can then be added in a refinement of CanReadi and Readi
by a predicate Guardi which can examine the contents of channels via peek
and, in the case of stateful systems, the local state via localsi.
To show a concrete example we formulate this for a SelectDataFlow
refinement by a machine we call GuardedSelectDataFlow. It suﬀices to
refine CanReadi and correspondingly Readi by adding the Guardi to the
readability test (in CanReadi) and to the selection condition (in Readi) as
follows:
CanReadi =
forsome P ∈SelectPatterni
IsReadable(P) and Guardi(P, localsi)

160
6 Mixed Synchronous/Asynchronous Control Structures
Readi =
choose P ∈SelectPatterni with
IsReadable(P) and Guardi(P, localsi) in
forall k ∈P do inputi,k := read from ini,k
actPatterni := P
In the case of non-systolic systems, partially read input (in inputi,k) and
partially computed outputs (in outputi,l) may also be available for testing
by Guardi. This is akin to the alternative statement in Occam (Example 10
pg. 64).
Static vs. Dynamic Network. In a streaming system the topology of
the network of agents is determined by how channels are set up, with the
same channel c appearing both in the Out set of one agent (writer(c)), and
in the In set of another agent (reader(c)); the two agents thus share the same
buﬀer(c).
Most streaming systems (e.g., those implemented in hardware) have a
static topology, where the set of agents and the structure of their connec-
tions is established at the beginning of the computation by having appro-
priate reader(), writer() and buﬀer() in the initial state, and is immutable
thenceforth.
A streaming system with a dynamic topology can be modeled by allowing
assignments to reader(c) and writer(c) during the computation, thus treat-
ing Ini and Outi as dynamic sets. They can then be updated by each agent
autonomously, based upon knowledge about other agents in the network,
possibly by introducing in Fig. 6.2 (pg. 156) a fourth control state recon-
figure between writing and reading, where communication structure updates
can be programmed by a Reconfigurei component. This is, for example,
the case of a computer network where each host can dynamically open or
close connections to other hosts it knows in the network (see the example in
Section 6.2.5). For another example see the UserInterface component of
the spreadsheet machine (Sect. 6.2.4) which creates and deletes connections
dynamically (but leaves the set of agents static). Note that with dynamic
communication lines also the SelectPattern concept becomes dynamic.
Modular ASM refinements and Feature-Oriented-Programming.
The reader will have noticed that the ASM refinements defined in this sec-
tion are modular, in that they can be combined in different ways to obtain
composite behaviours. This is an example of how modularity in ASM re-
finements supports Feature-Oriented-Programming (FOP) techniques to de-
fine software product lines and prove for them behavioural properties (see
[14, 13]).

6.2 Streaming (Data Flow) Control
161
6.2.3 Artificial Neural Network
Some particularly simple examples of streaming systems with however im-
portant practical applications are various classes of artificial neural networks
(ANN). In Deep Neural Networks (DNN), there are multiple layers of com-
putation elements called neurons (agents); each neuron applies a function to
the values of its inputs, and produces a single value that is then provided as
output. The values flow according to an interconnection structure (channels)
that is fixed as part of the specific architecture.
Neurons in the first or input layer receive values from the environment;
neurons in the output layer provide values to the environment; neurons in
intermediate or “hidden” layers receive the output of preceding layers and
provide input to the subsequent layers. In each layer, the result of the function
computed by each neuron is provided to all connected nodes in the next layer.
An example of DNN is depicted in Fig. 6.3; the network in this concrete
case computes a function (y1, y2) = DNNf (x1, x2, x3, x4), which is an ap-
proximation (learned from known individual examples presented to the DNN
during training) of some unknown function f (·). Notice that while the net-
work depicted in Fig. 6.3 is a complete layered acyclic directed graph (a
common case in ANNs), this is not a requirement of the architecture. Other
common cases include Recurrent Neural Networks (RNNs) which have cy-
cles, and Convolutional Neural Networks (CNNs) in which each neuron is
only connected to a limited number of neighbours in adjacent layers. Differ-
ent arrangement of channels in a streaming system can of course model such
architectural variants.
The kind of computation performed by a DNN can be expressed as a direct
refinement of FctlDataFlow, with the computation step refined as follows:
Computei =
forall 1 ≤l ≤li do
outputi,l := tanh(Pki
k=1 wi,k · inputi,k)
Fig. 6.3 A Deep Neural Network as a streaming system
x1
x2
x3
x4
y1
y2

162
6 Mixed Synchronous/Asynchronous Control Structures
where wi,k are the weights associated to the various inputs of the i-th neu-
ron; we use in the example the hyperbolic tangent function tanh(x) as
the activation function of the neuron (a popular choice), but different ap-
plications might require different functions, e.g. ReLU(x) = max(0, x) or
sigmoid(x) =
1
1+e−x . When executing a pre-trained model (a “forward” com-
putation), the wi,k are constants, and thus the computation is purely func-
tional.7 The resulting behaviour is that of a functional, systolic, static stream-
ing system.
If we want to also model the training phase, where weights are adjusted
by gradient-controlled “backward” propagation of errors (that is, difference
between the produced output and the expected output), then the wi,j weights
become part of the local state of a neuron, which is preserved (and adjusted)
across repeated forward and backward stages during the training. Hence, the
complete trainable model is a stateful, systolic, static streaming system.
6.2.4 Spreadsheet Streaming Machine
In a typical spreadsheet application, Cells—arranged in a 2-dimensional ma-
trix of R rows and C columns—can contain either literals, or formulas. For-
mulas are written in an expression syntax8 which, among other things, per-
mits to reference the value of other cells. The localValue of a cell is the result
of evaluating its literal respectively formula, based on the current localValue
of all referenced cells. Notice that it is possible to create circular references
(e.g., when two cells contain formulas which reference each other); such cases
are sometimes treated as an error.9
The user can directly enter a (new) formula or a (new) literal in any cell
at any moment. When this happens at a cell c the localValue of c must be
recomputed because the localValue of some cell b that is newly referenced by
c (or the localValue of the cell c with the newly entered literal) may yield a
value change.
We model the spreadsheet as a streaming control system, with an agent
a(r,c) and program Cell(r,c) for each cell (r, c) (where r and c represent the
row and column coordinates of the cell), and a single agent aUI with program
UserInterface that implements the user interface of the spreadsheet. Ini-
tially, each cell is Connected only to the user interface, i.e. In(r,c) = {ui(r,c)}
and Out(r,c) = {}, with the channel ui(r,c) modeling the connection between
the user interface (the writer of the channel) and the individual cell (the
reader of the channel).
7 For some more details see the training refinement model in [57, Sect. 4].
8 In most spreadsheet applications, a formula has to start with the character =.
9 In some system, the user has the option of requesting a fixed point computation
instead, providing additional parameters.

6.2 Streaming (Data Flow) Control
163
The user interface program. The operations of the user interface are
as follows. An inputEvent signals when the user has committed a new text t
into a particular cell c, where t can represent a formula or a literal. The set of
cells referenced in t is obtained by a function cellsReferencedIn(t). This will
be the empty set in case t is a literal or a formula without references. Note
also that this set may include some elements that are already connected to
the cell c, because they were referenced in the previously entered text for c.
Channels among cells are then reconfigured: by Disconnect all channels
are removed that connect c to previously referenced cells that are no longer
cellsReferencedIn t; for the cells that are referenced in the new formula but
not yet Connected with c, new channels are added to Connect them to c.
When a formula is entered in a cell c, UserInterface also instructs
(via their interface channels uib) all cells b referenced in the formula—those
already Connected to c and those newly Connected to c—to send their
localValueb (even if unchanged) to c. Here again, if a literal was entered in c
then no such cell b exists, and no message is sent. In c, the values received by
the referenced b cells (if any) serve to update the localValuec; this happens
when in the computing phase c will DoRecalculation of localValuec, see
Compute below.
In addition, UserInterface informs c (via the interface channel uic)
that its localTextc has changed and must be replaced by the newText t. The
inputEvent is Dismissed (read: becomes undef ) by firing UserInterface.
UserInterface =
if inputEvent ̸= undef then
let c = cell(inputEvent), t = text(inputEvent),
Refs = cellsReferencedIn(t) in
forall a ∈Cell \ Refs do
Disconnect(a, c)
-- drop chans from no-longer ref’d cells
forall b ∈Refs do
Connect(b, c)
-- add chans from newly ref’d cells
write (update, c) to uib
-- ask b to send localValueb to c
write (newText, t) to uic
-- ask c to set localTextc to t
Dismiss(inputEvent)
-- event trigger done
where
Disconnect(a, c) =
forall ch ∈Chan with writer(ch) = a and reader(ch) = c do
writer(ch) :=undef
reader(ch) :=undef
Connect(b, c) =
if not Connected(b, c) then
let ch = new Chan in
writer(ch) := b
reader(ch) := c
buﬀer(ch) :=new Buﬀer
-- initially empty buffer

164
6 Mixed Synchronous/Asynchronous Control Structures
In the above UserInterface model, we assume that the uic channels are
unbound, i.e. always writeable. In practice, this is not a limitation as the
rate at which the user can submit input events is much slower than that of
the spreadsheet updates, hence the reader is much faster than the writer on
those channels.
The cell program. The program Cell(r,c) (the same for all cells) is
defined as in Fig. 6.2 (pg. 156) for generic streaming systems but instantiated
with the following components Read(r,c), Compute(r,c), Write(r,c) and
respective guards CanRead/Write, DoneRead/Write, DoneCompute.
The Read(r,c) component distinguishes two different activation modes:
the first (UIevent) indicates that at the given cell new input is available from
the user interface due to some user action, while the second (newValue) in-
dicates that one or more cells on whose value the value of the given cell
depends, may have a new value – so that the cell’s own value may need to be
updated. In both cases, after reading the corresponding input and recording
the activation mode, the agent has DoneRead(r,c) and transitions to comput-
ing mode.
CanRead(r,c) = forsome ch ∈In(r,c) IsReadable(ch)
Read(r,c) =
if IsReadable(ui(r,c)) then
-- Read input from UserInterface
inputUI(r,c) := read from ui(r,c)
activation(r,c) := UIevent
else
-- Read input from cells
forall ch ∈In(r,c) with IsReadable(ch) do
input(r,c),writer(ch) := read from ch
activation(r,c) := newValue
DoneRead(r,c) = true
It should be noted that in the else case ch cannot be ui(r,c), by the very
case distinction.
The Compute(r,c) component distinguishes three input cases: an update
or a newText instruction from the UserInterface (two cases ⃝
1 and ⃝
2 that
are handled by the component HandleUIEvent below) or the arrival (at
some non-interface input channels of a cell (r, c)) of some newValues from
some cells referenced by the formula in cell (r, c) (case ⃝
3 the component
HandleNewValue below handles by triggering DoRecalc). DoRecalc
tries to compute the new localValue(r,c) that must replace the previous (not
any more valid) one. So we have the following definition for Compute(r,c).
Compute(r,c) =
HandleUIEvent
-- cases ⃝
1 and ⃝
2
HandleNewValue
-- case ⃝
3
DoRecalc
-- case ⃝
4

6.2 Streaming (Data Flow) Control
165
It remains to define the submachine behaviour of these Compute(r,c)
components. We distinguish the three input cases and the work of the
DoRecalculation machine.
⃝
1 Input of a dependent-cell-update request. If a request inputUI(r,c) =
(update, d) to update a dependent cell d arrives from UserInterface,
the agent executes HandleUIEvent: it sets output(r,c),d to a mes-
sage with its own current localValue(r,c) and (due to the definition of
DoneCompute(r,c) below) proceeds to writing. This is expressed by the
following subrule of the HandleUIEvent below:
if activation(r,c) = UIevent then
if kind(inputUI(r,c)) = update then
let d = targetCell(inputUI(r,c)) in
output(r,c),d := (myValue, localValue(r,c))
⃝
2 Input of a newText. If the UserInterface requests to enter a new-
Text into the cell (r, c), this text is extracted from the inputUI(r,c)
and stored by HandleUIEvent in the local state (in localText(r,c)).
A TriggerRecalc is executed, which causes the current (the to-
be-updated) localValue(r,c) of the cell to be invalidated; the value of
activation(r,c) is also invalidated so that in the next step Compute(r,c)
tries to DoRecalc and (by the definition of DoneCompute below) enters
mode writing. This is expressed by the second subrule of the following
HandleUIEvent rule:
HandleUIEvent =
if activation(r,c) = UIevent then
if kind(inputUI(r,c)) = update then -- ⃝
1 dependant cell update
let d = targetCell(inputUI(r,c)) in
output(r,c),d := (myValue, localValue(r,c))
if kind(inputUI(r,c)) = newText then -- ⃝
2 new text for this cell
localText(r,c) := enteredText(inputUI(r,c))
TriggerRecalc
⃝
3 Input of a newValue. If by some non-ui-input channel ch of a cell (r, c)
some newValue arrives (from some cell that is referenced in the localText
of (r, c)), then as in the preceding case the agent will TriggerRecalc,
i.e. invalidate the current localValuer,c (to-be-updated by the DoRecalc
component below) and invalidate activationr,c to trigger DoRecalc.
This explains the following rule HandleNewValue.
Note that the new value that arrives at (r, c) as input via a non-ui channel
ch is stored in the corresponding input location input(r,c),writer(ch) where
DoRecalc will check that it is available.
HandleNewValue =
if activation(r,c) = newValue then -- ⃝
3 new referenced cell value
TriggerRecalc

166
6 Mixed Synchronous/Asynchronous Control Structures
⃝
4 The DoRecalc component. In cases ⃝
2 and ⃝
3 , the current localValue(r,c)
of the cell is no longer valid and has to be updated. This update consists
in checking whether all input values for referenced cells are available. If so,
the new localValue is computed—by evaluating the localText with all the
various input(r,c),w for each referenced cell writer w (we denote this set as
input(r,c) for brevity). The resulting value is assigned to the output vari-
ables output(r,c),reader(ch) for the reader(ch) of any ch ∈Out(r,c). In this
way upon reaching the writing stage, all dependent cells will be updated
(by receiving a newValue activation). In any case, the agent proceeds to
writing (since activation(r,c) is undef and thus DoneCompute(r,c) is true)
so that any pending outputs are written out. Then the agent proceeds to
reading to wait for more argument values or input events to arrive.
DoRecalc =
if activation(r,c) = undef then
-- ⃝
4 attempt recalc this cell
if allArgsAvail then
let result = eval(localText(r,c), args) in
-- evaluate text
localValue(r,c) := result
forall ch ∈Out(r,c) do
-- update dependants
output(r,c),reader(ch) := (myValue, result)
UpdateDisplay((r, c), result)
where the allArgsAvail predicate checks whether all the arguments needed
for the evaluation of the value of the current cell are indeed available; it
is defined as follows:
allArgsAvail =
forall ch ∈In(r,c) \ {ui(r,c)} input(r,c),writer(ch) ̸= undef
and args is a map of cells mentioned in localText(r,c) (if any) to their
current values, defined as
args = {ref 7→v | ∃ch ∈In(r,c) with writer(ch) = ref
and input(r,c),ref = (myValue, v)
}
TriggerRecalc causes a recalculation (in the next step) of the current
cell value by the DoRecalc rule; it is defined as
TriggerRecalc =
localValue(r,c) := undef
activation(r,c) := undef
The termination predicate for the compute stage is refined as follows:
DoneCompute(r,c) =
(activation(r,c) = UIevent and kind(inputUI(r,c)) = update) -- case ⃝
1
or (activation(r,c) = undef )
-- case ⃝
4

6.2 Streaming (Data Flow) Control
167
Fig. 6.4 Possible paths in the execution of Compute(r,c)
compute
1
2
3
4
write
Notice that our definition of DoneCompute(r,c) guarantees that the compute
stage will span exactly one step (in case ⃝
1 ) or two steps (in cases ⃝
2 and ⃝
3 ,
both followed by ⃝
4 ), as depicted in Fig. 6.4.
Finally, Write(r,c) is a pure data refinement (here just a renaming)10 of
the NonSysWritei streaming component defined above (pg. 159):
CanWrite(r,c) = true
Write(r,c) =
forall ch ∈Out(r,c) with output(r,c),reader(ch) ̸= undef do
if IsWriteable(ch) then
write output(r,c),reader(ch) to ch
output(r,c),reader(ch) := undef
DoneWriter,c = forall ch ∈Out(r,c) (output(r,c),reader(ch) = undef )
Thus, our spreadsheet computation is an example of a stateful, non-
deterministic, systolic, dynamic streaming system.
The combined behaviour of UserInterface and Cell is depicted below
as UML sequence diagrams. Fig. 6.5 shows the handling of an inputEvent
generated by the user u. Fig. 6.6 and 6.7 show the handling of the remaining
messages by any cell c.
10 Define agent indeces i as matrix points (r, c), channels outi,l as channels ch ∈Out(r,c),
and the output locations outputi,l as output(r,c),reader(ch).

168
6 Mixed Synchronous/Asynchronous Control Structures
Fig. 6.5 Handling of an input event by the user interface in the spreadsheet
model
Disconnect(a, c)
Connect(b, c)
(myValue, v)
(update, c)
(newText, t)
inputEvent(c, t)
u:User
ui:UserInterface
a:Cell
b:Cell
c:Cell
Fig. 6.6 Handling of incoming myValue messages by a cell in the spreadsheet
model
save arg v
r = eval(text, args)
(myValue, r)
(myValue, v)
ref:Cell
c:Cell
dep:Cell
opt
[allArgsAvail]

6.2 Streaming (Data Flow) Control
169
Fig. 6.7 Handling of incoming newText messages by a cell in the spreadsheet
model
save text t
r = eval(text, args)
(myValue, r)
(newText, t)
ui:UserInterface
c:Cell
dep:Cell
opt
[allArgsAvail]
6.2.5 TCP/IP Streaming in Computer Networks
TCP/IP (the Transmission Control Protocol / Internet Protocol) [60] is one of
the protocols used to implement host-to-host communication on the Internet.
In its basic form, it implements two-way communication between a pair of
hosts, one of which is designated as server (the one waiting for connection re-
quests) and the other as client (the one initiating a connection request). Each
host is identified by a unique address11; furthermore, it provides a number
of ports12 to indicate specific connection points. The pair ⟨host, port⟩deter-
mines which agent, among all those executed by a given host, should handle
the data transmitted over the connection. A connection is uniquely identified
by two such pairs, one for each end of the communication channel. From the
point of view of one of the participating agents, its own ⟨host, port⟩address
is the local one, whereas the other is the remote one. From the point of view
of the other agent, these roles are switched. The roles of source (sender or
writer) and destination (receiver or reader) are instead fixed, and depend on
the direction of the data flow.
Following the style used in the Java networking library, we describe the
workings of TCP/IP through operations grouped in two classes (in the sense
of Section 3.2.2): Socket and ServerSocket. The latter provides operations
used by a server when setting up to accept a connection request (open, accept,
11 In IP implementation, Internet host addresses are encoded as 32 bit integers (for IP
version 4) or 128 bit integers (for IP version 6), with certain ranges reserved or assigned
to specific users.
12 In TCP implementation, ports are encoded as 16 bit integers, with the first 1024
reserved for admin-level usage, and the remaining ones freely available to applications.

170
6 Mixed Synchronous/Asynchronous Control Structures
close), whereas the former provides operations used to establish a connection
and transmit data (open, read, write, close).
The actions corresponding to the various operations are taken by agents
of a streaming system. Agents residing on different hosts cannot share any
state; but on each host h, a designated agent dh which is part of the host’s op-
erating system and represents the driver handling the networking hardware,
has access to a special pair of channels through which it can send and receive
messages via the physical networking infrastructure. dh can share some state
with other agents ah,i on the same host; these agents are mutually isolated
and don’t share state among themselves. Moreover, each agent ah,i can in-
voke operations of the network driver via some here not furthermore specified
Call / Return mechanism.
The network driver (dh, Dh) is defined according to our general model for
streaming system agents, with the refinements below. We assume that in the
initial state, Inh = {ihwh} and Outh = {ohwh}, where ihwh and ohwh are
the I/O channels connecting the driver to the networking hardware. Agents
(ah,i, Ah,i), representing OS processes running on h, will use other channels
connected to dh as needed (see Fig. 6.8, pg. 171). Thus at every moment the
network driver dh on host h is connected to the networking hardware (through
the pair of channels ihwh/owhn) and to each agent ah,i corresponding to the
OS processes on h that are accessing the networking functionality.
6.2.5.1 Networking driver
The networking driver can receive messages from the networking hardware
of h or from any of the local processes on h; these messages are handled sepa-
rately by the HandleNetwork and HandleLocal components below. In
keeping with common practice, we give priority to the handling of networking
hardware (a shared resource) over that of local agents.
CanReadh = forsome c ∈Inh
IsReadable(c)
Readh =
choose c ∈Inh with IsReadable(c) do
inputh,c := read from c
DoneReadh = true
Computeh =
if inputh,ihwh ̸=undef then
HandleNetwork
-- handle messages from network
else
HandleLocal
-- handle messages from local agents
DoneComputeh = true

6.2 Streaming (Data Flow) Control
171
Fig. 6.8 Overview of the networking model
Physical Network Infrastructure
Host h
Host h′
Network Driver dh
ah,1
ah,2
ah,nh
. . .
Network Driver dh′
ah′,1
ah′,2
ah′,nh′
. . .
. . .
c1
c2
ohwh
ihwh
ohwh′
ihwh′
Incoming packets from networking hardware are of four kinds: inbound
connection requests and corresponding connres responses, data packets to be
delivered to some process on h, and disconnection signals. In all cases, the
incoming data packet will carry additional information, that we will extract
through appropriate background functions, abstracting from the details of
the data format. In particular, beside addressing information (source and
destination addresses, each composed of host and port numbers), a packet
may carry a payload with arbitrary data (that is exchanged between processes
on the network).
HandleNetwork =
let pkt = inputh,ihwh, lh = dsthost(pkt), lp = dstport(pkt),
rh = srchost(pkt), rp = srcport(pkt) in
case kind(pkt) of
connect →HNConnect
-- incoming connection request
connres →HNConnRes
-- outgoing connection response
data →HNData
-- incoming data packet
disconnect →HNDisconnect
-- connection closed

172
6 Mixed Synchronous/Asynchronous Control Structures
Each of the operations to be performed upon receiving the different kind
of packets are detailed in the following. In case a connection request from
a process on another host is incoming, the local agent responsible for the
destination port (as established by ServerAccept from Sect. 6.2.5.3) is
identified, and if present a new pair of channels is set up13 between that agent
and the network driver, over which communications will be carried on (by
HandleLocal). In addition, a mapping is established linking the channels
and the connection identity (as given by the two pairs of host, port addresses).
Finally, confirmation that the connection is now established is sent, through
the network, to the requesting host and any pending ServerAccept call is
completed.
HNConnect =
let a = acceptingh(lp) in
if a =undef then
-- no process waiting on port
outputh,ohwh := (rh, rp, lh, lp, connres, ko, connection refused)
else
let c1 = new Channel, c2 = new Channel in
Connect(c1, a, dh)
Connect(c2, dh, a)
outboundh(c1) := (rh, rp, lh, lp)
inboundh(rh, rp, lh, lp) := c2
outputh,ohwh := (rh, rp, lh, lp, connres, ok, connection accepted)
Return(pendingCallh(lp), (success, (c1, c2)))
-- complete call
Dually, the connres packet that can be received when requesting an out-
going connection is handled by the following macro:
HNConnRes =
if result(pkt) = ok then
-- connection accepted
let a = callee(pendingCallh(lp)),
c1 = new Channel, c2 = new Channel in
Connect(c1, a, dh)
Connect(c2, dh, a)
outboundh(c1) := (rh, rp, lh, lp)
inboundh(rh, rp, lh, lp) := c2
Return(pendingCallh(lp), (success, (c1, c2))
else
-- connection refused
Return(pendingCallh(lp), (fail, connection refused))
Notice that in the macro above, as elsewhere, we omit for brevity any check
about the legality of the incoming packet (e.g., we do not explicitly check
that there is indeed a pending ClientOpen call from Sect. 6.2.5.4 for the
13 Via Connect and Disconnect operations that we specify below.

6.2 Streaming (Data Flow) Control
173
given lp, which could be false if a forged connres packet is crafted and sent
via the physical network infrastructure by a malicious actor).
If a data packet has been received, the payload is extracted and forwarded
to the appropriate channel. Notice that the payload function is assumed to
be defined in such a way that the receiving agent will only receive the data as
posted by the sender agent (as if the two agents were communicating locally
via channels).
HNData =
let c = inboundh(rh, rp, lh, lp) in
if c = undef then
-- no process waiting on port
outputh,ohwh := (rh, rp, lh, lp, connres, ko, connection refused)
else
outputh,c := payload(pkt)
A disconnection request causes the destruction of the corresponding chan-
nels, and the removal of mapping information. It an attempt is made to close
a connection that had never been opened, the attempt is silently ignored.
HNDisconnect =
let a = acceptingh(lp) in
if a = undef then
-- no process waiting on port
skip
else
forall c ∈Chan with outboundh(c) = (rh, rp, lh, lp) do
-- one c
Disconnect(c)
outboundh(c) :=undef
Disconnect(inbound(rh, rp, lh, lp))
inbound(rh, rp, lh, lp) :=undef
In the preceding macros, to set up and dispose of channels we use the
following macros, variants of the Connect and Disconnect macros used
in the UserInterface machine above (pg. 163):
Connect(ch, a, b) =
writer(ch) := a
reader(ch) := b
buﬀer(ch) :=new Buﬀer
Disconnect(ch) =
writer(ch) :=undef
reader(ch) :=undef
buﬀer(ch) :=undef
In the model above, it should be remarked that a new pair of channels
⟨c1, c2⟩(providing for two-way communication) is established between the
network driver and an agent for each network connection established by that
agent. Each agent can in fact maintain an arbitrary number of connections
open at the same time, and using separate channels ensures that the corre-
sponding buﬀers are kept distinct. The functions inboundh and outboundh are
used to map (local) addresses from incoming network packets to channels,

174
6 Mixed Synchronous/Asynchronous Control Structures
and data to send from local channels to (remote) addresses to be inserted in
outgoing network packets.
Any data written by a process to the outbound channel of a connection
should be wrapped in a packet (as the packet’s payload) and completed14
with source and destination routing information. The packet is then sent
to the networking hardware for physical transmission. The HandleLocal
macro below expresses these operations.
HandleLocal =
choose c ∈Inh with inputh,c ̸=undef do
let (rh, rp, lh, lp) = outboundh(c),
pkt = (rh, rp, lh, lp, data, inputh,c) in
outputh,ohwh := pkt
This completes the computing stage. The writing stage of the network
driver is a typical non-systolic variant, with a minor modification to accom-
modate the separate handling of the channel connected to the networking
hardware from that of channels connected to local OS processes.
CanWriteh = true
Writeh =
if outputh,ohwh ̸=undef and IsWriteable(ohwh) then
write outputh,ohwh to ohwh
outputh,ohwh :=undef
forall c ∈Outh with c ̸= ohwh do
if outputh,c ̸=undef and IsWriteable(c) then
write outputh,c to c
outputh,c :=undef
DoneWriteh = forall c ∈Outh
outputh,c =undef
6.2.5.2 Networking operations
As for the communication between agents ah,i (representing processes on host
h) and the network driver dh on the same host, we assume that some OS-
provided mechanism allows the processes to cause the execution of certain
rules in the context of dh. Such rules correspond to operations that the pro-
cesses request to dh, and would be executed in alternative to the steps of the
compute stage detailed above.
14 We omit here other technical details, such as adding a checksum to ensure that data
is not corrupted in transit.

6.2 Streaming (Data Flow) Control
175
One such mechanism could be the exchange of control messages between
processes and the network driver via an additional per-process pair of chan-
nels, to be implemented as a separate pattern and activation mode. An accu-
rate model need however be OS-dependant, as it should reflect the way user
processes interact with system components on a particular environment.
Some of the operations can be completed in a single step, but others may
have to wait on external events (e.g., waiting for a connection to arrive), and
are thus naturally asynchronous. We do not specify here a particular call con-
vention; depending on the language and operating system, these operations
could be implemented as blocking calls, or as asynchronous invocations, with
results to be fetched at some later time via some form of await operation.
Here, we simply assume that each invocation of a method includes an ad-
ditional parameter: a unique call value that is used to associate invocations
with corresponding results. A function caller : Call →Agent lets us identify
the agent performing the call.
In particular, we assume that a new Call is generated for each invoca-
tion, and provided to both the callee (the network driver) and the caller
(the user process). When a call is completed, either synchronously or asyn-
chronously, a Return macro referencing the corresponding call will pro-
vide the user process with the operation’s results. We also assume that a
Return(call, returnValue) executed by dh, in addition to any OS-specific way
of providing the returnValue to caller(call), also removes the call from the
range of pendingCallh(·) (by setting the corresponding locations to undef ).
Using the conventions above, the effect of the various operations can be ex-
pressed succinctly as described in the next subsections.
Exercise 28. The OS-specific mechanism could be represented via moni-
tored or shared variables, or via additional channels, or by using oracle func-
tions, etc. Choose a particular mechanism, and then define the following three
concepts accordingly.
1. Write a model for an asynchronous realization of the Call/Return
mechanism. Hints: define state : Call →{new, pending, succeeded, failed}
and result : Call →Values (valid only when the call is settled, i.e.
state(call) is either succeeded or failed).
2. Define an await call operation that suspends the executing agent until
the call is settled.
3. Define a synchronous realization of the Call/Return mechanism. You
can combine the asynchronous version with await, or write a different
model.
You may want to compare your resulting models with the operations of
Promises, Futures and await/async in popular programming languages such
as Java, C++, JavaScript.

176
6 Mixed Synchronous/Asynchronous Control Structures
6.2.5.3 Server operations
A server on host h can open a specific port (provided that the port is not
already in use by another agent), thus establishing ownership over that port:
ServerOpen(call, port) =
if acceptingh(port) =undef then
acceptingh(port) := caller(call)
Return(call, (success))
else
Return(call, (fail, port already in use))
A server can invoke the ServerAccept operation on a port it owns to
indicate that it intends to receive connection requests for other agents on the
network. The invocation of this operation can result in an immediate error, or
leave the call suspended until a connection request arrives (the corresponding
Return will then be performed by HNConnect):
ServerAccept(call, port) =
if acceptingh(port) = caller(call) then
if pendingCallh(port) =undef then
pendingCallh(port) := call
-- defer till a connect arrives
else
Return(call, (fail, already accepting))
else
Return(call, (fail, not owner))
Closing the port has the effect of relinquishing ownership and interrupting
any pending accept operation (notice how in this case two calls are com-
pleted at the same time; the pending ServerAccept fails, while the current
ServerClose succeeds):
ServerClose(call, port) =
if acceptingh(port) = caller(call) then
acceptingh(port) :=undef
if pendingCallh(port) ̸=undef then
-- pending accept
Return(pendingCallh(port), (fail, port closed))
Return(call, (success))
else
Return(call, (fail, not owner))
6.2.5.4 Client operations
As for client operations, only the opening and closing of the connection need
to be specified, since reading and writing are realized by the native read
from and write to operations on channels. We thus have,

6.2 Streaming (Data Flow) Control
177
ClientOpen(call, rh, rp) =
choose lp in availablePortsh do
outputh,ohwh := (rh, rp, h, lp, connect)
pendingCallh(lp) := call
-- defer till a connres arrives
ifnone
Return(call, (fail, no free ports))
where availablePortsh is the set of all ports that are available to the agent
(depending on whether it is a user-mode process or a privileged process) that
are not already in use, i.e. they do not appear in the domain of inboundh and
the codomain of outboundh.
Closing a connection is straightforward, as it only involves sending a dis-
connect packet over the network, aborting a possibly pending ClientOpen
call, and closing the corresponding channels:
ClientClose(call, lp, rh, rp) =
outputh,ohwh := (rh, rp, h, lp, disconnect)
forall c ∈Chan with outboundh(c) = (rh, rp, h, lp) do
-- unique c
Disconnect(c)
outboundh(c) :=undef
Disconnect(inbound(rh, rp, h, lp))
inbound(rh, rp, h, lp) :=undef
if pendingCallh(lp) ̸=undef then
-- pending ClientOpen
Return(pendingCallh(lp), (fail, closed while connecting))
Return(call, success)
-- we ignore errors
This completes our example. From the point of view of the individual
processes, once a connection is established via a successful ServerAccept
on host server by agent s and a corresponding ClientOpen on host client
by agent c, any data written on each agent’s outbound channel will appear
(through the physical network and the operations of the network driver) on
the other agent’s inbound channel, and vice versa, despite the two agents
residing on different hosts and sharing no state and no direct channels.
Our computer network is an example of a stateful, non-deterministic, non-
systolic, dynamic streaming system.
6.2.5.5 Completing the browser model
In Sect. 5.2.1.3 we presented a model for the Browser Transport Layer, where
a TcpSend macro was used in conjunction with abstract Buﬀers to express
the actual network operations used to fetch data from a web server. We
can now provide a possible realization of TcpSend based on the networking
model we just introduced, where we use an abstract await syntax to express
asynchronous invocation of the client operations from Section 6.2.5.4.

178
6 Mixed Synchronous/Asynchronous Control Structures
Fig. 6.9 Realization of TcpSend(host, payload, buﬀer) in the networking
model
call := ClientOpen(host, 80)
opening
await call
result
failed
writing
chout
write payload to chout
reading
chin
read line from chin
append line to buﬀer
done
response complete
ClientClose(lp, host, 80)
returned
pending
ko
ok
writeable
not writeable
destroyed
readable
destroyed
not readable
no
yes

6.2 Streaming (Data Flow) Control
179
Our TcpSend is implemented as a Control State ASM, represented in
Fig. 6.9, that can be executed in parallel with the whole browser model,
enqueuing fragments of the server response to the corresponding Buﬀer as
they arrive from the physical infrastructure.
Notice that, although not detailed in the figure, we freely use return values
from the ClientOpen call once it has returned (namely: the success indicator
result and the channel pair ⟨chout, chin⟩from HNConnRes, and the allocated
local port lp from ClientOpen).

Part II
Computational Paradigms: Analysis


Introduction and Survey of Part 2
Part 2 is devoted to three themes.
• Ch. 7 explains three fundamental complementary methods for the anal-
ysis of computations at different levels of abstraction: mathematical pro-
gram verification (by proofs of properties), experimental program val-
idation (by test suites), and professional program inspection (a direct
comparison of the terms, properties and behavior of the program to the
real world scenario they are intended to describe). These methods are
applied in a hierarchical process development:
– it starts with building precise requirements models, called ground
models one can submit to inspection (Sect. 7.1),
– it leads through verified and/or validated model refinements (which
piecemeal implement the abstractions) to compilable code (Sect. 7.2).
Such refinements yield pairs of stepwise refined process families which
make implementation patterns explicit.
• Ch. 8 classifies some historically or conceptually outstanding classes of
algorithmic processes, defined by instantiating the specific structure of
their data, elementary actions and run control features. This makes the
characteristic ingredients of each of these families explicit and ready for
being studied as computational paradigm in a model theory of computing.
We illustrate this for the historically first sequential (Turing machine or
finite automata like) abstract model of computation with fixed program
(Sect. 8.1) and its arithmetical instance (Sect. 8.2), as distinguished
from multi-agent models of computation for concurrent (Sect. 8.3) and
recursive (Sect. 8.4) algorithms.
• In Ch. 9 we illustrate the possibility ASMs offer to develop a practically
useful rich algorithmic model theory that measures the complexity of runs
in terms of items in the computational structure of abstract machines
(Sect. 9.2). In Sect. 9.1 we use ASMs to succintly formulate and prove
some classical intrinsic limits of computing.
183

Chapter 7
Correctness Analysis: Inspection,
Validation, Verification
The two major properties of interest for algorithms are their correctness and
their complexity. The complexity issue is addressed in Ch. 9. In this chapter
we investigate the correctness problem with a focus on the code correctness
problem: to make sure and to document that the executions of a proposed
computer program perform what the program has been built for. It is char-
acteristic for such system developments to involve descriptions at numerous
levels of abstraction that have to be linked in an appropriate way, leading
from what is called requirements capture through high-level design to pro-
gramming and system maintenance. We explain here how the modeling con-
cepts described in the previous chapters contribute to a reliable and practical
design and analysis method for software system engineering.1
In Sect. 7.1 we describe how one can exploit the abstraction potential
of ASMs to rigorously define the requirements (read: an elaborated con-
sistent, correct and complete version of them) by an executable abstract
model—called ground model2—that can be objectively checked for its ade-
quacy (‘domain-specific correctness’). In Sect. 7.2 we explain how using the
ASM refinement method enables to rigorously formulate and apply every de-
sign and implementation decision taken on a stepwise-refinement-path from
the abstract requirements model to the targeted eﬀiciently runnable or com-
pilable code and to check the refinement correctness.
For ground models as well as for their ASM refinements there are two
complimentary methods to document the correctness of a component respec-
tively of a refinement step (implementation of an abstraction) with respect
to the design intentions:
• Model validation by experimental testing and comparison of runs of
the abstract and the refined model, checking that the test results are as
expected and related in the intended way.
1 This chapter is based on [31, 34, 33, 35, 37, 38].
2 In [31] they were called primary models.
185
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
E. Börger, V. Gervasi, Structures of Computing, 
https://doi.org/10.1007/978-3-031-54358-6_7 
 
 
 
 
 

186
7 Correctness Analysis: Inspection, Validation, Verification
• Model verification by proving properties of interest for abstract and
concrete runs and proving that the properties correspond to each other
to establish correctness.
For this reason the abstract models must not only be executable but also come
with a mathematically analyzable behavioural definition, as is the case for
ASMs. Note that for every scientific discipline experiments and mathematical
proofs must go together, as we have learnt from Galilei. In computer science
this combination comes in the form of test suites put together with proofs of
properties of model runs.
However, for the requirements model there remains a gap that for rea-
sons of principle cannot be closed by mere programming constructs or model
transformations. It concerns the relation between the ground model and the
part of the real-world it describes. The characteristic (non-mathematical but
scientifically rigorous) method to check the adequacy of the model with re-
spect to its intended behaviour in the world is model inspection, a method
that resembles the traditional code inspection (Sect. 7.1) but proceeds at an
application-domain-determined level of abstraction. We explain below how
ASMs by their abstract and rigorous character eﬀiciently support model in-
spection.
In Sect. 7.3 we point out that the development of stepwise refined models
provides a documentation that not only facilitates (and not only for new-
comers!) the understanding of the final code but also adds to its reliability as
well as resilience and enhances considerably the code maintenance (support
for reuse and design for change).
7.1 Ground Models (Inspection)
To define what the software for a computer-based system is supposed to do is
the role of the requirements which must provide the conceptual algorithmic
core of the system prior to its implementation. Methodologically speaking
this role is an epistemological one and resembles the role Aristotle assigned
to axioms to ground science in reality, avoiding infinite regress. The require-
ments must relate real-world items and behavior (objects, events, actions) to
corresponding descriptions of what in philosophical terms could be called the
essence of the software, its process kernel. Unfortunately, too often the formu-
lation of requirements is incomplete or on the contrary detailed beyond ne-
cessity, ambiguous, or even inconsistent. In addition, when the requirements
are precise, complete and consistent they still may not reflect correctly the
intended behaviour of the system to be built so that often the running code
is proposed as the true definition of the system. But as we explain below this
does not provide the epistemologically and pragmatically needed appropriate
‘grounding’ of the code in the real world where the system will work.

7.1 Ground Models (Inspection)
187
An eﬀicient and reliable development method for software-intensive sys-
tems needs precise high-level models of (system and software) requirements—
we call them ground models—that tackle three major problems in software
engineering we describe in more detail below:
• The ground model must provide an adequate understanding by all
stakeholders (customers, users, software experts) of what the system to-
be-built should consist of. This involves a language and communication
problem: domain experts and software developers must be enabled via the
ground model to reach a common understanding of the desired behaviour
of the system prior to its coding.
• Domain experts must be enabled to check—by a repeatable and well-
documented inspection procedure applied to the ground model—that
the requirements describe in an appropriate way what the to-be-designed
system is supposed to do. This is also expressed by saying that the re-
quirements are ‘correct’ and ‘complete’ (read: correspond to the desired
system behaviour). This is an epistemological, not a mathematical prob-
lem the ground model must resolve, namely to directly and appropriately
relate the conceptual linguistic terms of the model to the intended real
world items or affaires. Leibniz called this proportio quaedam inter cha-
racteres (‘symbols’) et res (‘things’) and considered it as the foundation
of truth: ‘Et haec proportio sive relatio est fundamentum veritatis’ [119].
• For system maintenance the ground model must enable the system de-
velopers to cope with ever-changing requirements by faithfully capturing
and tracing them (in synchrony with the code) via well-documented
modeling-for-change so that the models can be reused to extend them
together with their implementation by new features (a process called hor-
izontal refinement that supports also software product lines [14]).
Ground Model Language. To build a ground model—a precise system
blueprint as used by architects—from given requirements usually consists not
only in translating the possibly ambiguous natural-language terms by precise
expressions but also in making the requirements consistent, correct, complete
and minimal (abstracting from unnecessary implementation details). This is
a diﬀicult mediation task between domain experts and software experts and
needs a description language both of them understand so that it solves the
communication problem. In this language it must be possible to calibrate
the degree of precision of expressions to describe the relevant domain-specific
terms without ambiguity in a way the software experts understand. On the
other side the language must come with a general conceptual data and op-
eration model the application domain experts can grasp without becoming
software experts themselves. Clearly, the language of ASMs is an easily ex-
tendable, not formal but rigorous language that satisfies these conditions,
what explains its successful use to build ground models.
Ground Model Inspection. Given that a ground model starts with
transforming informally-expressed requirements into something precise there

188
7 Correctness Analysis: Inspection, Validation, Verification
are no mathematical means to prove the correctness (and analogously the
completeness) of such a transformation, due to the lack of yet another math-
ematically precise model of the requirements the ground model could be
compared to. To check the ‘correctness’ and ‘completeness’ of a ground model
the only way to proceed is by model inspection. It resembles traditional code
inspection,3 where however the ground model inspection performs a direct
comparison of model terms to the intentions the domain experts associate to
the corresponding constructs in the real world (as described by the initially
given informally expressed requirements) and to what is conveyed by the
ground model to the software experts. This very serious and widely neglected
problem is of epistemological (not mathematical) nature and is resolved if the
inspection provides an appropriate ‘evidence’ for the desired correspondence
between model terms and matters in the real world. This phenomenon is
characteristic for scientific (not purely mathematical) theories and has been
observed already by Aristotle in his Analytica Posteriora: to provide a foun-
dation for a scientific theory no infinite regress is possible; in every chain of
theories the first one must be justified by evident axioms (see [9]). Ground
model inspection must provide such evidence that the terms and rules of the
ground model are ‘correct’ and ‘complete’.
Clearly, model inspection is enhanced by the two main complimentary
and precise correctness checking methods, namely experimental validation by
testing runs of a model and mathematical verification by proving model prop-
erties of interest. Therefore ground models must be not only mathematically
precise, but also executable, as ASM ground models are (see [7] for tool sup-
port). The executability of ASM ground models offers to run use cases to
systematically try to falsify the ground model in the Popperian sense [138].
It also permits to define, independently of the code, a system-acceptance
test plan where executions of the still to-be-defined code are matched against
runs of the ground model. Furthermore, it supports prototyping as well as the
definition of reference implementations. Last but not least it permits early
conceptual correctness checks and to establish explicit links between the tra-
ditional levels of testing: of the entire system, of its modules and of its code
components (by unit tests). Concerning the verifiability of model properties,
to verify ASMs covers the full range from proof sketches to mathematical
proofs to formal deductions in logic calculi to computer-assisted proofs. No-
tably it permits to integrate domain-specific forms of reasoning that are not
limited to formal deductions.
3 We cannot do better than quote here Parnas’ code inspection guidelines: ‘the key
to inspection of any complex product is a policy of divide and conquer, i. e., having
the inspector examine small parts of the product in isolation, while making sure that 1)
nothing is overlooked and 2) that the correctness of all inspected components implies the
correctness of the whole product. The decomposition of the inspection into discrete steps
must assure that each step is simple enough that it can be carried out reliably and that
one inspection step can be carried out without detailed knowledge of the others.’ [135]

7.2 Model Refinements (Validation and Verification)
189
There is a great variety of ASM ground models in the literature, some
of them not surprisingly constructed to define various industrial standards.4
The full value of ASM ground models becomes visible when they are linked
by a chain of ASM refinements to preserve the behavioral correctness of the
ground models in their implementation. This is the theme of the next section.
7.2 Model Refinements (Validation and Verification)
Refinement is a well-known general methodological principle to manage com-
plexity by decomposing systems into parts whose details can be designed and
analysed step-by-step. Refinement is intimately related to its inverse: abstrac-
tion. Thus it should not surprise that the abstract operational character of
ASMs can be exploited to define a stepwise refinement concept that allows
one to link ground model runs—piecemeal and in an effectively controllable
manner—to executions of compilable code, offering at any intermediate level
of abstraction to formulate, analyse, implement, and document in a precise
way the underlying design decision for the further detailing of some abstrac-
tion. In this short section we explain the basic idea of the notion of ASM
refinement and refer for small but characteristic examples to the machines
we define in this book. For further extensions and more involved practical
applications of the concept we provide references to the literature.
The ASM models in the literature and in this book illustrate how the
abstraction potential of ASMs leads directly to a component-based system
development approach. Furthermore, this potential permits to tailor abstrac-
tion/refinement pairs of components to meet any given design idea. In terms
of ASMs that is a question of accurately linking not only abstract to cor-
responding refined data, but also corresponding abstract and refined oper-
ations in the runs of the to-be-related components. This is the reason why
ASM-refinements offer more expressivity and flexibility than other refinement
notions in the literature: an ASM-refinement step typically refines both data
and flow of control (read: execution of rules) and thereby can be tailored
to achieve the desired effect of corresponding actions on the values of corre-
sponding locations in runs at the abstract and the refined level of abstraction.
More precisely stated, to define an ASM refinement M ∗of a given ASM
M the system designer is free to determine the following items to realize his
design idea (see the illustration by Fig. 7.1).
• The definition of a refined state and a refined program M ∗. The state
refinement (change of signature) expresses the data part of the refinement
4 Some examples are language standards for ISO Prolog [66, 51], IEEE VHDL93 [46, 47],
ITU SDL2000 [106, 71], Java/JVM (2000 de facto) [159], Ecma C# [45], OMG BPMN
2.0 [113, 114].

190
7 Correctness Analysis: Inspection, Validation, Verification
Fig. 7.1 The ASM refinement scheme.
© 2003 Springer-Verlag GmbH Berlin Heidelberg, reprinted with permission
σ1 · · · σn
|
{z
}
n steps of M ∗
State S∗
S′∗
≡
≡
State S
S′
m steps of M
z
}|
{
τ1 · · · τm
With an equivalence notion ≡between data in
locations of interest in corresponding states.
step (see below the required equivalence of locations of interest) whereas
the rule refinement takes care of the (possibly) refined flow of control.
• The definition of corresponding computation segments: abstract segments
τ1, . . . , τm where each τi represents a single M-step, and refined segments
σ1, . . . , σn of single M ∗-steps σj. These segments reflect the refinement
of the flow of control. Their initial and final state pairs S, S∗and S′, S′∗
are called corresponding states of interest because they are those the de-
signer will compare to check the correctness of the refinement. In given
runs these computation segments lead from corresponding states of inter-
est to corresponding states of interest. The resulting diagrams are called
(m, n)-diagrams and the refinements (m, n)-refinements. It is a distinctive
feature of ASM-refinements—crucial for their practicality to decompose
complex actions into simpler ones—that any pairs (m, n), (∗, n), (m, ∗)
(where usually ∗is some function of n or m) may appear, including opti-
mization pairs (n, m) where n > m, not only the typical data refinements
(of type (1, 1)) or procedural refinements (of type (1, n) with n > 1) as
one finds in compiler design where single source code instructions are
compiled (read: refined) to target code [168, 87, 41][159, Sect. 14.2].
• The definition of corresponding locations of interest, i.e. pairs of (usually
sets of) abstract and corresponding refined locations that must be checked
(validated or verified) in corresponding states of interest to match the
desired data refinement condition.

7.2 Model Refinements (Validation and Verification)
191
• The definition of the intended preservation of the meaning of data in
abstract locations of interest by the data in the corresponding refined
locations. Usually this defines an equivalence of the local data and these
equivalences accumulate to the notion of the equivalence of corresponding
states of interest.
It remains to define the precise meaning of refinement correctness, i.e.
what it means if a refined run correctly simulates an abstract run.
Definition 41 (ASM Refinement Correctness). Choose any notion ≡
of equivalence of states and of initial and final states. An ASM M ∗is a
correct refinement of an ASM M iff each concrete run S∗
0 , S∗
1 , . . . (of M ∗) has
a corresponding abstract run S0, S1, . . . (of M) such that
• their corresponding states of interest Sik and S∗
jk are equivalent
– where i0 < i1 < . . . and j0 < j1 < . . . are index sequences with
i0 = j0 = 0
• and either both runs terminate and their final states are equivalent,
• or both runs and both sequences i0 < i1 < . . ., j0 < j1 < . . . are infinite.
Remark on design and verification. Perhaps already the few small but
characteristic examples for ASM refinements in this book (Sect. 3.2.2 (Java),
4.4 (Prolog), 4.6 (RAM to reflective RASP), Sect. 5.3 (Occam), 6.2.2 (stream-
ing machines), 9.2.1 (universal machine)) let the reader grasp the potential
of the concept. Numerous examples in the literature show the practicality of
the ASM refinement method, a result of the freedom it offers the practitioner
to choose an appropriate abstraction level to implement a design idea
and an adequate reasoning method for its verification coupled with ex-
perimental validation. We cannot show it here but want to point out that
this freedom helps to not only construct stepwise refined design chains, but to
accompany these definitions with (even machine-assisted) correctness-proof
chains (see for example [52, 148, 149, 43, 159, 59, 14]). Furthermore, the above
defined notion of ASM refinement has been extended and implemented in KIV
(Karlsruhe Interactive Verifier) by Schellhorn [143, 144, 146, 145, 147, 150]
and is used by the KIV group in Augsburg for challenging applications, e.g.
security critical protocols in e-commerce [94], an electronic purse [96, 152, 95]
with a verified Java implementation [90] and an underlying Java calculus
[160] that has been influenced by the ASM models in the Jbook [159], a
verified flash file system [151, 70, 23], and much more. Also in the Verifix
project on compiler verification [85, 86, 87] refinement-correctness theorems
have been proved using the interactive theorem prover of the PVS system
(https://pvs.csl.sri.com/).

192
7 Correctness Analysis: Inspection, Validation, Verification
7.3 The Role of Documentation
Too often when a software project ends there is no interest any more to pro-
vide a documentation of what has been done. But this makes it diﬀicult a)
to maintain the code (including bug correction) or b) to reuse the code to
respond to requirements changes that show up during the maintenance pe-
riod. In both cases a refinement chain that links in a provably correct way a
ground model to compilable code is of intellectual help and reduces the cost of
code maintenance. Such a documented refinement chain provides what in re-
quirements engineering is called traceability, both between requirements and
implementation, and at different levels of implementation. Practical experi-
ence shows that such a documentation makes it possible to quickly localise the
conceptual origin of a bug, probably at some higher level of abstraction than
that of the final code, to repair the bug there and to insert the change along
the refinement chain from there to the final code. A similar situation arises
when for a running system additional requirements show up: find their level
of abstraction in the refinement hierarchy, introduce the new requirements
there by a model refinement that is followed by an appropriate refinement
in the lower-level models down to the compilable code. Consider that at the
level of abstraction of requirements it is usually easier than in eﬀicient code
to detect inconsistency issues among different requirements.
The basic reason in both cases a) and b) is that a model refinement chain
facilitates the understanding of the system behaviour,5 providing the de-
signer with handles that are intimately associated with the design steps that
were taken to implement the requirements and can be used to massage the
implementation when some changes have to be made.
It is important to realize that ASM models—pseudo-code with a precise
meaning—if used in the right way are not unrelated to the writing of final
code but represent parts of it. During the software development process ev-
ery design decision does appear anyway (even if not explicitly formulated
in rigorous terms), whether on the blackboard or articulated during a team
meeting or described by a prototype or inserted as comment into the code.
What an ASM definition of a design idea does is a) to formulate the idea
precisely as a piece of pseudo-code that later refinement steps fill up, and
b) to systematically keep such information in a digital document instead of
erasing it when the corresponding chunk of code has been written. It has been
proven that this can be done in practical cases, and at the scale of real-life
systems6 and it helps (also economically to reduce maintenance expenses and
system failures, see [49] for an industrial example).
5 Note that even the quality of manuals can be improved if the manual writers can
access a good documentation.
6 See for example the reuse of the Prolog/WAM models and of their compilation correct-
ness proof [52, 51] for the compilation of CLP(R) programs to code running on IBM’s
Constraint Logic Arithmetical Machine (CLAM) [53] or for the compilation of PROTOS-
L programs to code running on IBM’s Protos Abstract Machine (PAM) [16, 15] or the

7.3 The Role of Documentation
193
Fig. 7.2 Process Structure of the ASM Development Method.
© 2003 Springer-Verlag GmbH Berlin Heidelberg, reprinted with permission
TEST
CASES
domains
transition system
stepwise
refinement
reflecting
design
dynamic functions
external functions
decisions
manual
mechanized
PROVER
adding assumptions
adding definitions
SIMULATOR
using data from
application domain
Verification
Application Domain Knowledge
Ground Model
Informal Requirements
Code
Validation
+
Remark on the modeling process. A refinement chain leading from
a ground model to compilable code has often a sequential character, but
the development process is by no means linear. Typically the construction
is a back and forth because frequently during the design and programming
phases issues show up that belong to higher levels of abstraction but had been
overseen. This is illustrated by Fig. 7.2. In a sense the final ground model is
defined only once the coding has been finished and no further change is to be
expected. But as explained above, in the maintenance phase the game starts
again because to let the method work the models and the final code must
remain in sync.
reuse of the Java interpreter ground model in [58] for an interpreter ground model of
C# programs in [45, 59].

Chapter 8
Characterization of Process Families
In this chapter we characterize some historically or conceptually outstanding
classes P of algorithmic processes in terms of the structures over which they
operate: their architectural background, the type of their data, operations,
control constructs and runs. As examples we consider sequential (Sect. 8.1)
and in particular arithmetical algorithms (Sect. 8.2) and their runs for the
case of single-agent (thereby input-driven) processes; for multi-agent pro-
cesses we investigate the case of concurrent algorithms (Sect. 8.3) and for
mixed synchronous/asynchronous systems the case of recursive algorithms
(Sect. 8.4).1
The epistemological claim for each of these axiomatically defined classes
P to represent a fundamental intuitive computational paradigm is formu-
lated as a thesis and supported by a characterization theorem. For each
characterization theorem a class ASM(P) of concrete ASMs is defined and
proved to capture the computational paradigm; by this it is expressed
that the following two two properties hold:
• completeness property: for each axiomatically described process in P
with its runs one can construct a machine in ASM(P) with computa-
tionally equivalent runs,
• correctness property: each machine in ASM(P) with its runs satisfies
the axioms for machines in P with their runs and thus is an element of
P.2
Such characterizations of algorithmic processes over abstract data types
have an ancestor in Turing’s epistemological (but not axiomatic) analysis in
[162] of processes that can be carried out (in principle) by a single (human
or mechanical) calculator using paper and pencil, investigation that led to
1 For a recent survey of various other computational paradigm characterizations and
open questions (e.g. concerning unbounded nondeterminism or bounded nondeterminism
with unbounded parallelism) see [153, Sect. 7].
2 In the literature the term ‘plausibility’ is used for the correctness property and ‘char-
acterization theorem’ for the completeness property.
195
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
E. Börger, V. Gervasi, Structures of Computing, 
https://doi.org/10.1007/978-3-031-54358-6_8 
 
 
 
 
 

196
8 Characterization of Process Families
Turing’s model of computation and to what in the literature is called Turing’s
Thesis (for computable numerical functions)3. See Sect. 8.2 for more details
on Turing’s Thesis.
Why ASMs. As concrete machines over abstract structures we use in
this book ASMs instead of descriptions in other equally comprehensive state-
based specification frameworks (notably (Event-) B [2, 3] or TLA [118]). The
main reason is the greater flexibility ASMs offer, in two ways:
• ASM design is focused on the dynamic state-change aspects—lifting them
in a natural way (see Sect. 8.1.1) and term-for-term (see pg. 237) from
Turing’s tape cells with symbol content to sets of locations containing
arbitrary structured data.
• ASM design is not constrained by (though compatible with) a particular
verification framework.
A second pragmatic reason is that ASMs come with a simple and intu-
itive behavioural semantics that provides rigour to pseudo-code (see Def. 14,
pg. 32), the prevailing form in which algorithms are described in the literature
and in textbooks on algorithms and data types.
Furthermore, as we will see in the next chapter, the update-based ASM
computation model enriches the set of possible complexity measures, sim-
plifies numerous classical recursion-theoretic arguments and by investigating
computation directly over structures opens the door for a complexity disci-
pline which is based on machine concepts that are relevant for the practice of
computing and not blurred by coding or proof system details (see Sect. 9.2).
8.1 Single-Agent Sequential Algorithms
We first explain (in Sect. 8.1.1) how Turing machines (TMs) as mathematical
model for ‘sequential’ computations of intuitively computable functions—i.e.
algorithmic input-driven processes executed by a single agent that stepwise
executes a fixed program—are naturally extended to the Parallel Guarded
Assignment (PGA) rules. In doing this we also show how some of the in-
tuitive arguments Turing provides in [162]—to justify the thesis that TMs
are an epistemologically reasonable mathematical substitute for computing
numerical functions by single-agent algorithms in the intuitive sense of the
term—can be extended by three precise Sequential Postulates that Gure-
vich [93] proposed to mathematically characterize single-agent input-driven
stepwise algorithmic processing of abstract data by abstract operations using
a fixed program. In fact the class PGA of PGA rules—the historically first
form of ASMs—captures the paradigm of single-agent input-driven stepwise
data processing using a fixed-program that is described by those Sequential
Postulates:
3 We disregard here the distinction between functions over Nat respectively strings.

8.1 Single-Agent Sequential Algorithms
197
Theorem 6 (A Characterization of a Class of Sequential Algo-
rithms). Let Seq be the class of algorithms that satisfy the Sequential Pos-
tulates defined below. Seq is captured by the class PGA of Parallel Guarded
Assignment rules (Def. 12, pg. 24) with respect to input-driven runs of single-
agents which stepwise execute a fixed program.
Proof: for the correctness property see the end of Sect. 8.1.1, for the com-
pleteness property see Sect. 8.1.2.
The theorem also holds for non-deterministic sequential algorithms and
bounded-choice PGAs (Def. 29, pg. 63), see Corollary 1 (pg. 202). As
Corollary 2 we obtain in Sect. 8.2 an axiomatic characterization of Turing-
Computability.
Remark Thesis versus Characterization Theorem. It remains for
the reader to accept or to disagree with the various theses discussed in this
chapter. For example, Ch. 4 (on reflective algorithms) and Sect. 8.4 (on re-
cursive algorithms) investigate two classes of algorithms one would probably
want to consider as sequential, but they are not captured by the Sequential
Postulates. In this book we stick to the technical content of the charac-
terization theorems and leave the interesting epistemological evaluation of
axiomatic descriptions of computational paradigms for another occasion.
8.1.1 From Turing Machines to PGAs
In [162] Turing investigated which data, operations and forms of control are
needed for a single agent—a human or mechanical calculator—to compute
number theoretic functions using paper and pencil. The signature of Ex-
ample 5 (pg. 26) shows the computational ingredients Turing identified: a
tape of (at least potentially infinitely many) squares (read: memory loca-
tions (tape, int) where without loss of generality the tape is mathematically
described as function tape : Integer →Alphabet) containing a (almost every-
where the blank) symbol tape(int) of a finite Alphabet, a ‘scanned square’
(read: 0-ary function) head that is currently observed (but can be moved) by
the machine to read or modify the square’s current content tape(head), and a
current control (read: a 0-ary function) ctl with finitely many possible values
(named by Turing condition or machine-configuration or state of mind). Thus
a TM-state consists of the values of the dynamic functions tape, head, ctl to-
gether with the static functions write, nextCtl, moveDirection (with +1, −1
for shifting the head to the right/left) and in the background the set Integer
of integer numbers and the Alphabet.4 The pair (ctl, tape(head)) determines
which state-changing actions (read: assignments) the machine can possibly
perform simultaneously in one step: to read and update the content of the
4 Remember that we treat sets via their characteristic function.

198
8 Characterization of Process Families
scanned square, to move its head to the right or left of the scanned square,
and to change its current ctl value.
These TM-state functions together with the TM-step transition function
defined by the 3 parallel assignments determine the level of abstraction of
TM-computations. If one wants to express algorithmic steps at different (any
higher or lower) levels of abstraction it comes natural to generalize the TM-
items (TM-state and TM-step function) in two ways for computations over
abstract data:
• Allow any memory for the definition of abstract states, i.e. any set of
locations (f , args) containing any objects as their value, where f belongs
to a finite set of dynamic or static functions. From Sect. 1.1.2 we know
that mathematically such states are Tarski structures.
• Allow for the one-step transition executed by a single agent any state-
transition function; formulated in terms of machines and memory loca-
tions this comes up to generalize the three simultaneous (synchronous
parallel) TM-step assignments to the simultaneous execution (as one-
step) of any finite set of guarded assignments (PGAs).
To guarantee that the level of abstraction at which an algorithm operates is
fixed it is required that the algorithm does not distinguish between isomorphic
states. This leads to the first two postulates for single-agent (also called
sequential) algorithmic processes.
Definition 42 (Sequential Step Postulate). A single-agent (also called
sequential) algorithmic process P is a state-transition system, i.e. it comes
with:
• a set State(P) of states,
• a subset Init(P) ⊆State(P) of initial states,
• a one-step transition function stepP : State(P) →State(P)
This suﬀices to define single-agent sequential process runs as a sequence of
discrete computational steps (see Def. 22, pg. 40).
Definition 43 (Abstract State Postulate). For a single-agent algorithmic
process P
• the states are Tarski structures (see Def. 2, pg. 10), all of the same finite
signature Σ,
• the one-step transition function does not change the superuniverse (so
that in a given run all states have the same superuniverse),
• isomorphy constraint: the sets of states resp. initial states are closed under
isomorphism and every isomorphism between two states S, S′ is also an
isomorphism between their next states stepP(S), stepP(S′) (if they are
defined).
By Definition 10 (pg. 19) the difference between a successor state stepP(S)
(if defined) and its predecessor state S—i.e. the (as effect of the step) fresh

8.1 Single-Agent Sequential Algorithms
199
part of the successor state—is a consistent set of non-trivial5 updates denoted
by ∆(P, S) = stepP(S)−S. When it is fired in S the successor state stepP(S)
is obtained: S + ∆(P, S) = stepP(S).
To make the steps of a sequential process ‘effective’ it remains to re-
quire an appropriate finiteness condition. In [162] Turing requires for effective
processes (to be reducible to TMs) a finite set of symbols (to appear in 2-
dimensional memory or on the tape), a finite set of ‘states of mind’ and a
bound on the number of symbols or squares which the process can ‘imme-
diately recognize’ (observe and possibly modify) at any one moment. This
makes every step depending only on an a priori bounded (algorithm-specific
but run-independent) number of state elements.
Following Turing’s analysis an algorithmic process can access elements of
the superuniverse only via ground terms, like tape(head). One can obtain a
bound on the number of elements an algorithm can examine in each step by
bounding the number of ground terms on which every step of the algorithm
depends. This leads to the following Definition.
Definition 44 (Bounded Exploration Postulate). For every single-agent
algorithmic process P there is a finite set T of so-called critical terms such
that if two states S, S′ agree on6 the evaluation eval(t, S) = eval(t, S′) of
every critical term t ∈T, then the process computes for both states the
same ∆-set ∆(P, S) = ∆(P, S′) leading to their successor states.
This postulate implies that the elements a1, . . . , an, a of non-trivial updates
((f , (a1, . . . , an)), a) in ∆(P, S)—involved in the state change from S to its
successor state S′—are values of critical terms t1, . . . , tn, t respectively; there-
fore ∆(P, S) can be calculated as yield of a difference calculator rule ruleS
P,
hence ∆(P, S) = ∆(ruleS
P, S), of all those assignments f (t1, . . . , tn) := t with
state-change-involved critical terms (see Lemma 4 below).
Example 14. Critical TM terms are the following: head, write(ctl, tape(head)),
move(ctl, tape(head))(head), nextCtl(ctl, tape(head)) (see Example 5, pg. 26).
Definition 45 (Sequential Thesis). Following [93] we call the above three
postulates the Sequential Postulates and call Sequential Thesis the statement
that every single-agent algorithmic process in the intuitive meaning of the
term satisfies the Sequential Postulates.
The above formulated Characterization Theorem 6 (pg. 197) we are go-
ing to prove now is presented in [93] to support the thesis. Note that the
mathematical proof of the theorem is independent of whether one accepts
the epistemological thesis or not.
Proof of the PGA correctness property. By the semantics of PGA
rules (Def. 14, pg. 32) each PGA rule (equipped with a set of initial states)
5 See the footnote to the definition of ∆(P, S) on pg. 33.
6 We also say ‘coincide for’.

200
8 Characterization of Process Families
satisfies the Sequential Step Postulate (with stepP(S) = S + eval(P, S))
and the Abstract State Postulate; for the isomorphy constraint see Exer-
cise 2 (isomorphy of state updates, pg. 20) and Exercise 7 (isomorphic yields,
pg. 33). For the Bounded Exploration Postulate use as set of critical terms
the set of all (ground) terms that appear in the PGA rule in an assignment
f (t1, . . . , tn) := t as argument or value term ti or t or in a formula bi of a
guard if bi.7
8.1.2 The PGA Theorem
The PGA completeness property can be formulated as follows:
Theorem 7 (PGA Theorem). If a process P satisfies the Sequential Pos-
tulates, then its step function can be computed by a PGA (with the same
signature, the same sets of states and initial states as P) in normal form (see
Def. 12, pg. 24):
if φ1 then f1(s1,1, . . . , s1,n1) := t1
...
if φk then fk(sk,1, . . . , sk,nk) := tk
where the guards φi are Boolean combinations of term equations.
Proof. The proof is done in three steps. Let T be a set of critical terms of P.
Step 1. For every state S with well-defined successor state S′ the difference
set ∆(P, S) = S′ −S is shown to depend only on the values of critical terms
so that it can be computed by an ASM as its update set in S, here a PGA
ruleS
P consisting of a parallel composition of assignments with critical terms
(Critical Terms Lemma 4). Step 2. The difference calculators ruleS
P for S are
shown to depend only on which equations between critical terms hold in state
S (Critical-Terms-Similarity Lemma 5). Step 3. The finitely many Critical-
Terms-Similarity types can be axiomatized as guards φSi—conjunctions of
equations and inequalities between critical terms in some state Si—which
yield finitely many guarded difference calculators if φSi then ruleSi
P whose
parallel combination PgaP computes ∆(P, S) for every state S (Similarity
Type Formalization Lemma 6).
Lemma 4 (Critical Terms). 1. For every state S of P and for every (non-
trivial) to-be-executed update ((f , (a1, . . . , an)), a0) in ∆(P, S) the arguments
a1, . . . , an read and the computed new value a0 are values of critical terms.
7 If the seq operator (Def. 24, pg. 53) and the let construct (Def. 15, pg. 34) are added
to PGAs the resulting class of ASMs still satisfies the bounded exploration postulate,
but this does not hold any more if the forall construct is added. See [58, Sect. 7.2.3] for
a proof.

8.1 Single-Agent Sequential Algorithms
201
2. Therefore every upd ∈∆(P, S) can be computed by an assignInstrupd =
f (t1, . . . , tn) := t with critical terms ti, t of P so that all together ∆(P, S)
can be computed by the parallel combination ruleS
P of all assignInstrupd for
upd ∈∆(P, S), i.e. ∆(P, S) = ∆(ruleS
P, S). ruleS
P is called an ASM difference
calculator for state S.
Proof. The first claim is proved by contradiction. Suppose one of the el-
ements of the given update u, say x = ai, is not the value of a critical
term. Map S by an isomorphism α to a new state S′ (by the isomorphy clo-
sure) that is identical to S except for x which is replaced by and mapped
by α to a new fresh element y. By the isomorphy constraint α is also an
isomorphism between the successor states stepP(S), stepP(S′) so that the im-
age ((f , (α(a1), . . . , α(an))), α(a)) of the update u in ∆(P, S) is an update
in ∆(P, S′). Now, S and S′ coincide on critical terms since only x and no
critical term value v is mapped by α to y, so that α(v) = v. By the bounded
exploration postulate it follows that ∆(P, S) = ∆(P, S′) so that y which oc-
curs in the elements of the update image of u in ∆(P, S′) occurs also in an
update of ∆(P, S): a contradiction to the freshness of y.
The second claim follows from the definitions.
Lemma 5 (Critical-Terms-Similarity). If two states S, S′ satisfy the
same equations between critical terms—called critical-terms-similarity—then
their ASM difference calculators ruleS
P and ruleS′
P are the same.
Proof. First construct an isomorphism α from S to a state S∗that agrees
with S′ on the evaluation of critical terms (see below). By the isomorphy con-
straint in the Abstract State Postulate it follows that α(∆(P, S)) = ∆(P, S∗)
so that (see Exercise 1) ruleS
P = ruleS∗
P . Since S∗and S′ agree on the
evaluation of critical terms the bounded exploration postulate implies that
∆(P, S∗) = ∆(P, S′) so that ruleS∗
P
= ruleS′
P and thereby ruleS
P = ruleS∗
P
=
ruleS′
P .
To construct such a state S∗assume that the base sets of S and S′ are
disjoint (otherwise take an isomorphic copy of S without elements from S′).
Map by a function α the value eval(t, S) of every critical term in S to its
corresponding value eval(t, S′) in S′ and every other element (of the base set
of S) to itself. By the disjointness of the base sets of S, S′ and by the critical-
terms-similarity of the two states the function is well-defined and bijective.
The isomorphic α-image say S∗of S agrees with S′ on the evaluation of crit-
ical terms t because by the homomorphy property eval(t, S∗) = α(eval(t, S))
holds and by definition of α also α(eval(t, S)) = eval(t, S′).
Lemma 6 (Similarity Type Formalization). Each of the finitely many
Critical-Terms-Similarity types can be formalized as property of some state Si
(1 ≤i ≤n) and used as guard φSi of ruleSi
P such that the parallel combination
of these guarded difference calculators computes stepP.

202
8 Characterization of Process Families
Proof. By finiteness of T there are only finitely many Critical-Terms-
Similarity types. Each of them is described by a Boolean combination of
equations between critical terms as follows: for a state S let φS be the con-
junction of all equalities s = t and all inequalities s ̸= t that hold in S
between critical terms s, t. There are finitely many states S1, . . . Sn of P such
that every state S of P satisfies the same equations between critical terms as
one of the Si and thus satisfies φSi. Therefore to compute the step function
stepP we define:
PgaP =
if φS1 then ruleS1
P
...
if φSn then ruleSn
P
Let S be any state of P. By Lemma 4 on Critical Terms ∆(P, S) =
∆(ruleS
P, S). Since by the Critical-Terms-Similarity Lemma 5 ruleS
P = ruleSi
P
(for some i where S satisfies φSi) it follows that PgaP executing ruleSi
P in
state S yields the update set ∆(P, S), simulating the move of P from S to
the successor state stepP(S) = S + ∆(P, S).
Corollary 1 (Bounded-Choice PGA Theorem). If a process P satisfies
the postulates for non-deterministic sequential algorithms, then its step rela-
tion can be computed by a bounded-choice PGA (with the same signature, the
same sets of states and initial states as P) in normal form as described in
Def. 29 (pg. 63).
Proof. The nd-Sequential Postulates for non-deterministic sequential algo-
rithmic processes P are the Sequential Postulates with the following adapta-
tions:
• In the Sequential Step Postulate (Def. 42, pg. 198) stepP is a relation
so that a state S may have more than one successor state S′ (i.e. with
(S, S′) ∈stepP).
• In the Abstract State Postulate (Def. 43, pg. 198) the second condition
in the isomorphy constraint is changed to require that every isomorphism
α from a state S1 to a state S2 maps every pair (S1, S′
1) in the successor
relation stepP to a pair (S2, S′
2) in the successor relation such that α is
an isomorphism from S′
1 to S′
2.
Correspondingly instead of one update set ∆(P, S) we have to consider
the set ∆(P, S) of all such difference sets ∆P(S, S′) = S′ −S between every
successor state S′ of S and S, i.e.
∆(P, S) = {S′ −S | (S, S′) ∈stepP}.
The Bounded Exploration Postulate is formulated with ∆(P, S) instead of
∆(P, S).

8.2 Arithmetical Algorithms and Turing’s Thesis
203
If P satisfies these postulates for non-deterministic sequential algorithms,
then there exists a bound for the number of possible successor states of any
state S of P. In fact, by the Critical Terms Lemma every element that occurs
in an update of an update set in ∆(P, S) is value of a critical term so that by
the finiteness of the set of critical terms there is a bound for the number of
update sets in ∆(P, S) that depends only on P and not on S. Therefore the
same arguments as for the PGA Theorem apply if in the definition of PgaP
the difference calculators ruleSi
P are replaced by the following bounded-choice
PGA rule:
choose rule ∈{rulei,1, . . . , rulei,li} do rule
where
rulei,j consists of finitely many parallel assignment instructions
8.2 Arithmetical Algorithms and Turing’s Thesis
Since in Sect. 8.1.1 we have described algorithmic single-agent processes with
abstract data and operations as generalization of numerical algorithms, the
latter appear as instances of the former. Therefore it comes natural to refine
the Sequential Thesis to an Arithmetical Thesis for computations over Nat.
Doing this one obtains via the PGA Theorem an axiomatic characterization
of Turing machine computability, as we are going to show in this section.
Call a process numerical if its operations apply arithmetical functions to
natural numbers. The Arithmetical State Postulate we are going to define
requires a numerical algorithmic process to use as auxiliary functions only
some clearly computable static background functions. This is what we used
already for Definition 26 of Structured Input/Output ASMs (pg. 55).
Definition 46 (Arithmetical State Postulate). A single-agent algorith-
mic numerical process P satisfies the Arithmetical State Postulate if it has
as base set the set Nat plus the two truth values and undef . Furthermore, as
background functions it has only the following ones: the usual Boolean oper-
ations, finitely many initial functions of recursion theory (see Def. 26, pg. 55)
including C 1
0 (the unary constant 0 function), +1 (the successor function),
and the characteristic function of the < relation. It also has a 0-ary function
inP (whose value is a finite input sequence of natural numbers, to be pro-
vided as arguments to P) and a dynamic function outP (for the computation
result, in case of i/o-algorithms). P has exactly one initial state per input
value and in each initial state only inP and none of its dynamic functions are
defined (i.e. ̸= undef ).
Corollary 2 (Axiomatic characterization of Turing computability).
Every function that is computed by a single-agent algorithmic numerical
process that satisfies the Sequential Postulates and the Arithmetical State
Postulate is Turing machine computable.

204
8 Characterization of Process Families
Proof. Let a function f be computed by a single-agent algorithmic numeri-
cal process P that satisfies the Sequential Postulates and is therefore equiva-
lent to some PgaP (by the PGA Theorem, pg. 200). Since by the Arithmetical
State Postulate P, and therefore PgaP, works over natural numbers using
only µ-recursive static background functions, the PgaP-computations can be
Gödelized in a standard way8 implying that PgaP computes a µ-recursive
function. In Sect. 3.3.1 we have proved these functions to be computable by
structured i/o-ASMs (Structured Programming Theorem 1, pg. 56). By a
simple exercise this proof can be rephrased in terms of structured function-
computing Turing machine programs.9
Exercise 29 (Characterization of Arithmetical Algorithms). Denote
by ArithAlg the class of algorithms that satisfy the Sequential Postulates
and the Arithmetical State Postulate. Explain why the restriction ArithPGA
of PGA to PGAs that satisfy the Arithmetical State Postulate captures
ArithAlg.
Epistemological status of TM/ASM. Obviously there can be an epis-
temological disagreement on whether a specific set of mathematical axioms
(e.g. the Sequential Postulates) ‘captures’ the targeted intuitive concept (e.g.
sequential algorithm) and thus justifies a corresponding thesis. Turing used
three kinds of argument for his widely shared answer (known as Turing’s
Thesis) to the question “What are the possible processes which can be car-
ried out in computing a number?” (intended with pencil and paper) [162,
pg. 249]:
(a) A direct appeal to intuition.
(b) A proof of the equivalence of two definitions (in case the new definition has a
greater intuitive appeal).
(c) Giving examples of large classes of numbers which are computable.
Gurevich [91] asked a more general question for any “processes which can
be carried out in computing”:
Every computational device can be simulated by an appropriate dynamic structure
[read: ASM] – of appropriately the same size – in real time; a uniform family of
computational devices can be uniformly simulated by an appropriate family of
dynamic structures [read: ASMs] in real time.
8 For example, formalize instead of register machine steps (see [29, pg. 52]) the update set
computation by eval (Def. 14) and the apply function A+U (Def. 9) of PGAs (Def. 12),
exploiting that by the Arithmetical State Postulate in initial states A only the input
location has a defined value so that in each state of the computation started with A
there are only finitely many dynamic locations with defined content. For a comparison
see the Gödelization of RAMs and their runs in Fig. 4.7, pg. 102.
9 For a technically very elegant definition of such structured TM programs see Rödding’s
Turing operators in [29, pg. 20].

8.3 Concurrent Algorithms
205
The large variety of system descriptions by ASMs in the literature10 pro-
vides an argument of kind (c) for various instances of an ASM thesis, typically
extensions of PGAs by specific further constructs—for example more or less
restricted forms of selection functions, of quantifiers (e.g. in BSP-ASMs, see
Sect. 6.1) and more generally of other logical operators (e.g. Hilbert’s ϵ and
ι operators), of memory management (e.g. by import), of dynamic program
change constructs (see Ch. 4), of application-domain functions, etc.
Various theorems (some of them explained in this chapter) that character-
ize well-known computational paradigms in terms of corresponding instances
of ASMs provide an argument of kind (b), where the ‘greater intuitive appeal’
stems from the combination of simplicity and generality of the ASM concept.
Concerning argument (a) it looks like we should not expect just one new
thesis that is supported by ‘a direct appeal to intuition’. Even in the restricted
sequential case the claim in [93] that PGAs—misleadingly called there se-
quential ASMs—capture sequential algorithms tout court does not match
different intuitive understandings of sequentiality; a fortiori such disagree-
ments can be observed when it comes to capture more involved concepts,
for example unbounded parallelism (see the discussion in [77]), recursion
([130, 20, 39, 55]), or reflectivity (see [153] and Ch. 4 in this book). Another
interesting example is the discussion in [156] about whether Turing’s Thesis
can be derived as a corollary from the PGA Theorem (Sect. 8.1.2) or whether
we should content ourselves with the proof that the class ArithPGA captures
ArithAlg. What we see showing up are various classes of ASMs each of which
can be proved to provide practical mathematical machine models for a spe-
cific class of (possibly axiomatically described) computing devices; in the case
of PGA those which satisfy the Sequential Postulates. This belongs to a new
computation theory over structures we discuss in Ch. 9.
8.3 Concurrent Algorithms
In this section we investigate whether Def. 5.1 (pg. 111) of multi-agent ASMs
and their concurrent runs captures an epistemologically satisfying machine-
independent axiomatic description of the concurrency paradigm. We build
upon the classification of the interaction type of locations (Sect. 2.1.1) and
the characterization of sequential algorithms by PGAs (Theorem 6, pg. 197)
to formulate the following conditions proposed in [54] as an extension of
PGAs for practical use in design and analysis of concurrent processes:
Definition 47 (Concurrency Postulate).
1. The actors of a concurrent process P are finitely many agents a, each
equipped with a fixed sequential algorithm alg(a) and its own clock.
10 There are more than seven hundred references in [58, 50] and https://abz-conf.
org/method/asm/.

206
8 Characterization of Process Families
2. Concurrent runs of P are sequences of interaction states, started in some
initial state.
3. In each interaction state Sm the agents which are ready to execute a step
in Sm form a subset Am ⊆A. Each a ∈Am performs one of the following
actions, simultaneously with the other agents in Am.
• a can be in global or local mode; in global mode it has read/write access
to the interaction data (shared or monitored locations), in local mode
it can access only its own local locations.
• In global mode a can do two things:
– it performs a simultaneous read/write step, remaining in global
mode,
– it can decide to switch to local mode, making a local copy of the
current value of the interaction data.
• In local mode a can perform a subcomputation that may end up in a
write back mode = wb in which a switches back to global mode and
the evaluation of pgm(a) contributes to the next interaction state
by an update of (some) interaction locations. For this reason Am is
required to be a subset of agents with mode ∈{global, wb}.
Theorem 8 (A Characterization of Concurrency). Concurrent ASMs
with concurrent runs (Def. 36, pg. 112) capture the computational paradigm
of concurrency.
Proof. For the algorithms executed by the agents of a concurrent run one
cannot simply invoke the characterization of sequential single-agent input-
driven algorithms by PGAs (Theorem 6, pg. 197), a problem that is due
to the presence of input/output and shared locations in concurrent runs.
Such locations make it possible that in a concurrent run some states become
reachable for an agent although in its isolated mono-agent runs they are
unreachable. One can however strengthen the proof arguments to include the
presence of input/output and shared locations when describing by updates
of an ASM rule the update sets ∆(alg(a), S) a given sequential algorithm
generates in a concurrent run. This exploits that those ASM rules depend only
on the updates that establish the relation between a state and its successor
state via the algorithmic transition function. To simplify the exposition we
restrict here the attention to deterministic component computations (which
satisfy the Sequential Postulates).
Concurrent ASMs and their runs satisfy the conditions of the Concurrency
Postulate due to the definition of concurrent ASM runs (Def. 36, pg. 112)
and the characterization Theorem 6 for sequential algorithms (pg. 197). This
establishes the correctness property. Therefore it remains to check the fol-
lowing:
Completeness property: Let P be a concurrent process with a finite set
A of agents a ∈A each equipped with an algorithm alg(a) that satisfies the
Sequential Postulates. If P satisfies the Concurrency Postulate, then its runs

8.3 Concurrent Algorithms
207
can be simulated step-by-step by corresponding runs of a concurrent ASM
Concur(P) = {(a, pga(a)) | a ∈A} with the same sets Am of agents and the
same states and initial states.
We paraphrase the proof of Theorem 7 (pg. 200) to define for each agent a
the desired PGA component pga(a) of Concur(P) to simulate the behaviour
of the given component algorithm alg(a) of P. The signature Σalg(a), the
states, the initial states, the stepalg(a) function and a set Ta of critical terms
are determined for every agent a by the Sequential Postulates. Let S0, S1, . . .
be the interaction state sequence with agent sequence A0, A1, . . . of an arbi-
trary concurrent P-run where (by the Concurrency Postulate) for every m
the following equation holds (if the successor state of Sm is defined):11
Sm+1 = Sm +
[
a∈GoGlobal
∆(alg(a), Sm) ↓Σalg(a)).
S ↓Σ denotes the restriction of state S to the signature Σ.
Paraphrasing the proof of the PGA Theorem 7 (Sect. 8.1.2) we show below:
Lemma 7 (PGA Components of Concur(P)). For each agent a ∈A there
is a PGA rule pga(a) such that
forall a, S holds ∆(alg(a), S) = ∆(pga(a), S).
The lemma extends the above equation for P-runs to Concur(P)-runs:
Sm+1
= Sm + S
a∈GoGlobal ∆(alg(a), Sm) ↓Σalg(a))
= Sm + S
a∈GoGlobal ∆(pga(a), Sm) ↓Σalg(a))
where Concur(P) is defined by (a, pga(a))a∈A.
Proof of the lemma. We invite the reader to read once more the proof in
Sect. 8.1.2 with alg(a) in mind in place of the process P. The construction of
a PGA ruleS
alg(a) (consisting of assignments with critical terms) to compute
∆(alg(a), S) = ∆(ruleS
alg(a), S) in the proof for the Critical Terms Lemma 4
(pg. 200) goes through verbatim12 for all states S of a, which includes shared,
monitored and output locations with corresponding critical terms. Also the
proof that ruleS
alg(a) works unchanged for every critical-term-similar state S′
(Lemma 5, pg. 201) goes through verbatim for all states S of a. Therefore,
by the Similarity Type Formalization Lemma 6 (pg. 201)—exploiting the
finiteness of the set Ta of critical terms and thereby of similarity types—the
PgaP defined for the PGA Theorem works also for pga(a), replacing ruleSi
P
by ruleSi
alg(a).
11 In the non-deterministic case one has to consider here a relation between state and
successor state instead of an equation and the corresponding set ∆(alg(a), S) of update
sets alg(a) can yield in S.
12 Just replace P by alg(a) and T by Ta.

208
8 Characterization of Process Families
8.4 Recursive Algorithms
Recursion plays a major role in computing, but the concept is not well cap-
tured by the single-agent sequential computation model because a recursive
call may involve multiple agents to independently perform some subcomputa-
tions, segments of computation that can be single-agent sequential or multi-
agent concurrent runs. On the other side, the interaction between a caller
and a callee of a recursive call is restricted to passing input and returning a
result, which is more restrictive than the interaction between components of
concurrent processes. So the paradigm of recursive computations stands in
the middle between sequential and concurrent computations and the ques-
tion arises whether there is a mathematical definition that closely expresses
the intuitive meaning of recursion and is captured by an appropriate class of
abstract machines.
In this section we explain a positive answer proposed in [55]. In Sect. 8.4.1
we distill what we consider as essential properties of the intuitive notion of
recursive algorithms into two Recursion Postulates, a Call Step Postulate and
a Recursive Run Postulate. They support a conservative extension of non-
deterministic sequential algorithms and their runs into recursive algorithms
and their runs. Guided by this intuition and trying to turn it into an adequate
machine concept we extend in Sect. 8.4.2 bounded-choice PGAs by a Call rule
to recursive ASMs with recursive runs. This allows us to prove in Sect. 8.4.3 an
extension of the characterization theorem from non-deterministic sequential
to recursive algorithms.
Theorem 9 (A Characterization of Recursive Algorithms).
Recur-
sive algorithms—those satisfying the non-deterministic Sequential and the
Recursion Postulates—are captured by recursive ASMs.
8.4.1 Recursion Postulates
The characteristic property of a recursive algorithm is that it can call (some
instances of) itself or some other (recursive) algorithms—triggering the callees
to perform some input/output subcomputation—and remain waiting until
the callee returns the output computed for the given input. Furthermore,
in one step a caller may trigger finitely many independent i/o-algorithms,
for example mergesort calls two copies of itself to independently sort each
one half of a given list. Due to the callees’ independence (which must be
guaranteed by an appropriate state encapsulation, in particular for different
calls of a same algorithm but with different input), the called component
processes can be executed in any sequential order or in parallel or even asyn-
chronously. This illuminates the position of recursive algorithms as multi-
agent algorithms between single-agent sequential and concurrent algorithms.

8.4 Recursive Algorithms
209
Furthermore, it lets it appear as reasonable to consider recursive algorithms
with non-deterministic component processes.
In a recursive call we view the caller as parent algorithm and the callees
as child algorithms. Caller and callee are used as i/o-algorithms (Def. 23,
pg. 52); their input/output locations for the parent/child interaction must
satisfy the following (recursive) call relationship.
Definition 48 (Call Relationship of Interaction Functions). A (re-
cursive) call relationship holds between the interaction functions of two (in-
stances of) i/o-algorithms Ap (parent) and Ac (child) if they satisfy the
following:
• The signature of the caller—the parent—comprises the input and out-
put functions of the callee—the child—but none of the child’s controlled
functions.
• The parent algorithm, when calling the child algorithm and only then,
assigns some input to monitored locations of the child and thereby defines
an initial state of the child, but it never reads the child’s input locations.
• The parent algorithm never updates but can read the output locations of
its child algorithm, namely when the latter has Terminated its computa-
tion (read: has reached a final state with defined output).
• To simplify the exposition but without loss of generality we assume that
in the state where a callee by its last move reaches its final state it does
not call a new child.13
Remember that by the function classification (Sect. 2.1.1) monitored loca-
tions of A are read by A, but never updated by A, whereas output locations
of A are never read by A, but can be written by it. By the Call Relation-
ship the initial states of a callee are input-dependent. This reflects a common
understanding of parameter passing in recursive calls.
The following postulate describes the characteristic properties of call steps
by which sequential algorithms become recursive algorithms.
Definition 49 (Recursive-Call-Step Postulate).
When an i/o-algorithm p (the caller, viewed as parent algorithm) calls a finite
number of i/o-algorithms c1, . . . , cn (the callees, viewed as child algorithms
CalledBy(p)), then the Call Relationship of Interaction Functions holds be-
tween the caller and each callee. For each callee ci the caller activates a fresh
instance of the algorithm and updates the input, thereby defining the initial
state from where the requested subcomputation can start. These subcompu-
tations are independent of each other and the caller remains waiting—i.e.
performs no step—until each of its callees has terminated its computation.
To simplify the exposition we assume, as is done usually, that every called
i/o-algorithm eventually terminates its run and updates as its last move its
13 In other words we require that a Terminated and therefore no longer Active algorithm
is not Waiting for computations of still Active child algorithms.

210
8 Characterization of Process Families
output; so this output becomes available to the caller in the callee’s final
state.
Definition 50 (Recursive Algorithm). A (sequential) recursive algorithm
R is a finite set of i/o-algorithms which satisfy the non-deterministic Sequen-
tial Postulates and whose transition relation may comprise call steps all of
which satisfy the Recursive Call Step Postulate. One of these i/o-algorithms
is distinguished as main algorithm. The elements of R are also called com-
ponents of R.
Since a recursive call may trigger multiple component algorithms to inde-
pendently perform some subcomputations, runs of recursive algorithms are
multi-agent runs: not sequential single-agent but also not concurrent runs.
The called component processes can be executed in any sequential order or in
parallel or even asynchronously. So we need a definition of what constitutes
runs of a recursive algorithm. Whether an instance of a component algorithm
can participate in a run step depends on whether
• it is still Active, i.e. has been Called but has not yet Terminated its run
• and it is not Waiting for a callee to return.
We therefore collect in a dynamic set Called the instances of component
algorithms that are called in a run, similarly we collect in CalledBy(P) the
children called by process P, so that in every state of the run Called =
S
P CalledBy(P) holds. Obviously in initial states only main is Called and
CalledBy(P) is empty for every P. We define below a precise meaning of
‘instance’ of an i/o-algorithm we use in the Call Step and the Recursive Run
postulates.
Definition 51 (Recursive-Run Postulate). For a recursive algorithm R
with main component main a recursive run is a sequence S0, S1, S2, . . . of
states together with a sequence C0, C1, C2, . . . of sets of instances of compo-
nents of R which satisfy the Recursive Run Constraint:
Recursive-Run Constraint.
• the run starts with an instance Main of main, i.e. C0 = {Main},
• every Ci is a finite set of instances of components of R that are Active
and not Waiting in Si, where in Si we define:
Active(P) iff P ∈Called and not Terminated(P)
Waiting(P) iff forsome c c ∈CalledBy(P) and Active(c)
• every Si+1 is obtained in one R-step by performing in Si simultane-
ously one step of each i/o-algorithm in Ci. Such an R-step is also
called a recursive step of R.

8.4 Recursive Algorithms
211
8.4.1.1 Fresh Instances of i/o-Algorithms
In the Call Step and the Recursive Run Postulates we refer to ‘fresh instances’
of i/o-algorithms to guarantee the independence of recursively called subcom-
putations. This notion can easily be described more precisely using parame-
terization, as explained for ambient ASMs in Sect. 5.4.1. Here it suﬀices to
equip a called i/o-algorithm A and its functions with a new parameter (read:
a child executing agent) c so that the following holds:
• The interpretation of a function symbol f of A yields different specimens
of functions fc, fc′ for different agents c, c′.
• In the run of Ac the child interpretation fc of each input or output func-
tion f is the same as the parent interpretation fc = fp (due to the call
relationship of interaction functions).
Therefore one can express the meaning of “activate a fresh instance of each
callee ci” more precisely as follows: if in a state S with successor state S′ a
parent i/o-algorithm instance Ap calls i/o-algorithms A1, . . . , An, then create
for each i a fresh child instance Ai
ci of Ai with input locations inputi so that
in the successor state S′ the child algorithm instance Ai
ci is CalledBy the
caller instance Ap and is Initialized(Ai
ci, inputi) so that it is ready to start its
computation, whereas the parent algorithm instance Ap becomes Waiting.14
For every algorithm B, Initialized(B, inputi) expresses that the restriction
S′ ↓ΣB of S′ to the signature of B is an initial state of B determined by
inputi.
8.4.2 Recursive ASMs
We now define recursive ASMs as a multi-agent extension of bounded-choice
PGAs by a recursive ASM call construct of form t0 ←M(t1, . . . , tn). The
construct is used as an i/o-machine and therefore called ASM i/o-rule. For
notational convenience we include into the definition the let construct, with-
out loss of generality (see footnote 7 on pg. 200).
Definition 52 (Recursive ASM). A recursive ASM R is a finite set of
recursive ASM rules, called components of R, with one rule declared as main
rule. Recursive ASM rules are defined by adding to the inductive definition
of bounded-choice PGAs (including the let construct) a clause for ASM i/o-
rules t0 ←M(t1, . . . , tn) for given recursive ASM rule M. In an ASM i/o-rule
the outermost function symbol of t0 is an output function for M (an element
of Σout) and the sequence ⟨t1, . . . , tn⟩is the value of an input function for M
(an element of Σin).
14 Except the trivial case that all Ai
ci are already Terminated when Called in S′.

212
8 Characterization of Process Families
Since recursive ASMs are particular recursive algorithms (as we will prove
in Sect. 8.4.3), their runs have the structure we have distilled into the Recur-
sive Run Postulate for recursive algorithms (Def. 51, pg. 210). To guarantee
the independence of component runs we use agents a equipped with ambient
ASM programs amb a in rule (Def. 38, pg. 145) so that each agent executes
its rule instance in its own state space, independently of the other agents.
Definition 53 (Recursive ASM run). A recursive ASM run—i.e. a run of
a recursive ASM R with main component main—is a sequence S0, S1, S2, . . .
of states together with a sequence A0, A1, A2, . . . of subsets of a dynamic
set of Agents, where each a ∈Agent is equipped with a pgm(a) that is the
instance amb a in r of a rule r ∈R, such that the following holds:
• The run starts with an instance of main, i.e. in the initial state S0 there
is only one agent (A0 = {root} = Agent). It is initially Active and not
Waiting; it is equipped with pgm(root) = amb root in main.
• Every Ai is a finite set of in Si Active and not Waiting agents, where we
define in Si:
Active(a) iff a ∈Agent and not Terminated(pgm(a))
Waiting(a) iff forsome a′ ∈CalledBy(a) Active(a′)
• Every Si+1 is obtained in one R-step by performing in Si for each agent
a ∈Ai one step of its pgm(a). Such an R-step is called a recursive step
of R.
Called and CalledBy(parent) are defined as for recursive algorithms. The
input/output interaction between a caller and a callee satisfies the following
Recursive Call Parameter Constraints:
• The caller can update but never read any input location of the callee.
• The caller can read but never update any output location of the callee.
• Caller and callee have no other common locations and the initial state of
a callee computation is determined by the value of its input locations.
It remains to define the semantics of single ASM i/o-rules r = t0 ←
M(t1, . . . , tn). Let M(x1, . . . , xn) = q be the declaration of M, with all free
variables of q among x0, x1, . . . , xn. The caller program r triggers the cre-
ation of a new agent c and an instance pgm(c) of the body q of M and its
initialization by the values of the input terms t1, . . . , tn, so that the initial
state is input-dependent. For the callee the sequence ⟨t1, . . . , tn⟩of the input
terms ti (1 ≤i ≤n) is classified as value of the input function and the outer
function symbol of the output term t0 as output function. So to establish
the Call Relationship for caller and callee (Def. 48, pg. 209) we define the
interaction function types of the caller correspondingly and make the corre-
sponding input/output locations interpreted in the same way in both state
spaces. Caller and callee have no other functions in common since both are

8.4 Recursive Algorithms
213
defined as ambient ASMs. The new callee agent c becomes Active15 whereas,
by Inserting it into CalledBy(p), the parent agent p becomes Waiting.
Definition 54 (Semantics of i/o-rule). The semantics of the i/o-rule t0 ←
M(t1, . . . , tn) is the singleton set ∆(r, S)16 containing the unique update
set that is computed in state S by the following deterministic ASM rule
Call(t0 ←M(t1, . . . , tn)).17
Call(t0 ←M(t1, . . . , tn)) =
let c = new (Agent)
pgm(c) :=
let x1 = t1, . . . , xn = tn
-- evaluate input passed by value
amb c in P
-- equip callee with its initialized pgm instance
Insert(c, CalledBy(self))
-- child becomes Active, parent Waiting
CalledBy(c) := ∅
where M(x1, . . . , xn) = P
-- declaration of M
Note that during the entire computation of the callee the value of the input
terms ti does not change: the caller does not update them because it is
Waiting, the callee cannot update them because they are monitored for the
callee.
It remains to check that the Recursive Call Parameter Constraints are
satisfied. In pgm(c) the outer function symbols of ti for 1 ≤i ≤n are
classified as input functions (which are not read but can be written by the
caller program); the outer function symbol f of t0 is classified as output
function (which is not updated but can be read by the caller program). The
in/out-function symbols can be considered as belonging to the signature of
caller and callee and to be interpreted the same way in the states of caller and
callee. Therefore the first two of the Recursive Call Parameter Constraints can
be assumed without loss of generality for caller and callee programs. The third
condition too is satisfied, because the ambient ASM mechanism separates the
state spaces of different agents. It turns each local function symbol f of arity n
in a program instance implicitly into an (n + 1)-ary function symbol, namely
by adding the additional agent as environment parameter for the evaluation
of terms with f as leading function symbol. Therefore, each local function of
the callee resolves into locations that are different from those of each local
function of the caller.
Remark on Call Trees. Starting with the initial tree Agent = {root},
every recursive run begins with a segment of steps that is under the control
15 Since the initial state of the callee is input-dependent, initially the predicate
Terminated is completely undef ined.
16 Remember that ∆(P, S) defines in the non-deterministic case the set of update sets
which change state S into a successor state S′.
17 The new construct can be avoided by introducing instead a counter c which for
each recursive call is incremented by 1 in a partial update (sequentially, one by one, if
there are finitely many simultaneous calls each of which needs a different pgm(c), see
Sect. 4.1).

214
8 Characterization of Process Families
of a single agent, namely of root with an instance of program main. We call
such a segment a sequential run segment. If in a step of a sequential run
segment the executing agent makes exactly one recursive call (Fig. 8.1), this
triggers a new sequential run segment that is under the control of the callee.
If in a step of a sequential run segment the executing agent makes more
than one recursive call, this triggers a finite branching of the call tree. It
creates new leaf nodes of Active and not Waiting agents, and passes control
from the caller to a multi-agent concurrent run segment of Active and not
Waiting agents.18 If an Active but Waiting agent a is the last node of a
sequence root, a1, . . . , an of Waiting agents in the caller/callee relation and
the computations of all children of this node a are Terminated, control goes
back (by the definition of Active) from the concurrent run segment of the
callees to their caller a, starting a new sequential run segment. In this way
from a Terminated (and thereby not any more Active) callee control goes
back to its caller when all its siblings are also Terminated. When the run
terminates the tree will be without Active agents.
Fig. 8.1 Recursive call nesting in state S with successor state S′
S
S′
Sc0
0
Sc0
i
Sc0
i+1
Sc0
ﬁn(c0)
Sc1
0
Sc1
j
Sc1
j+1
Sc1
ﬁn(c1)
Scn
0
Scn
ﬁn(cn)
call
call
call
return
return
return
8.4.3 Proof of the Recursive ASM Theorem
In this section we prove Theorem 9 (see statement on pg. 208). The first
half of it is the correctness property: each recursive ASM M defines a
recursive algorithm Malg such that the terminating recursive runs of M can
be step-by-step simulated by the runs of Malg.
Proof. First we show that every terminating recursive ASM M satisfies
the non-deterministic Sequential Postulates and the Recursion Postulates.
18 As with communicating ASMs, due to the separate states of the agents these concur-
rent run segments can also be interpreted as parallel interleaving runs (Sect. 5.2).

8.4 Recursive Algorithms
215
The signature of the PGA and the i/o-components of M define the states
as required by the non-deterministic Abstract State Postulate. The non-
deterministic Sequential Step Postulate is satisfied by the definition of re-
cursive ASM runs. For the Bounded Exploration Postulate use that each
PGA component of M and the body of each of the finitely many i/o-rules of
M components (together with their input parameters) have a set of critical
terms. Their union provides a set of critical terms for M. The Recursive-
Call-Step Postulate is satisfied (for terminating runs) due to Def. 54 of the
recursive ASM Call(i/o-rule). For the Recursive Run Postulate the recursive
run constraint is satisfied by Def. 53 of recursive ASM runs.
Let Malg be the recursive algorithm that shows up by the analysis of
M. Their recursive runs as defined by Def. 51 and Def. 53 are structurally
identical, step by step, only the wording (‘instances of components’ versus
‘agents associated with component program instances’) is different. Further-
more, each single step of Malg in any state S is defined by an update set
computed by the rules of M to define a successor state S′.
For the other half of Theorem 9 we must prove the completeness prop-
erty: for each recursive algorithm R one can construct a recursive ASM
ASM(R) such that the (terminating) recursive runs of R are step-by-step
simulated by (terminating) recursive runs of ASM(R).
Proof. By the Recursive-Run Postulate every step of R leading from any
state S to a successor state S′ is the result of one step of each of finitely many
Active and not Waiting R-component instances Aa. By the Recursive-Call-
Step Postulate these instances are fresh and come with disjoint signatures Σa
and subrun states S ↓Σa, S′ ↓Σa with difference set ∆Aa(S ↓Σa, S′ ↓Σa).
Consider any Aa and let A be the R-component of which Aa is an instance.
By definition of fresh instances (see Sect. 8.4.1.1), modulo the parameteri-
zation by agent a, a fresh instance Aa and its original A perform the same
steps. Therefore it suﬀices to show that every A-step leading from S to S′ via
a difference set ∆A(S, S′) ∈∆(A, S) in a given run of R can be described as
equivalent step of a recursive ASM rule rA. This is expressed by the following
lemma.
Lemma 8 (Construction of Recursive ASM Components). For each
R-component A one can construct a recursive ASM rule rA which computes
∆(A, S), i.e. such that ∆(rA, S) = ∆(A, S) for all states S appearing in a
recursive run of R where (any given instance Aa of) A makes a step.
Due to the lemma the required recursive ASM ASM(R) can be defined
by replacing in R the (instances of) i/o-components A by (instances of) rA.
Proof of the lemma. We adapt the arguments used for sequential algo-
rithms in the proof of the PGA Theorem (Sect. 8.1.2, Steps 1–3) to the case of
sequential algorithms extended by call rules. Let T be a set of critical terms
for A. We distinguish the following three cases with arbitrary given states
S and some successor state S′: in state S A either makes no recursive call

216
8 Characterization of Process Families
and no return step or a call step or a return step. In each case we construct
in step 1 a bounded-choice PGA ruleS
A (instead of a deterministic ruleS
P)
that computes a set of update sets (not only one set of updates) satisfying
∆(ruleS
A, S) = ∆(A, S). In step 2 we show that ruleS
A depends only on the
equations between the critical terms in S. In step 3 we apply the similarity
type argument to turn ruleS
A into a rule rA that works for every state S.
Case 1: In state S of the given run A makes no recursive call and no final
(return value outputting) step. In this case no update of A’s input or output
function appears in any ∆A(S, S′) ∈∆(A, S). Therefore, for A the first claim
of the Critical Terms Lemma 4 holds (pg. 200) for every update in any update
set ∆A(S, S′) ∈∆(A, S). Then to define rA one can paraphrase as follows
the three steps explained for the proof of Theorem 7 and Corollary 1.
Step 1: By the Critical Terms Lemma for every update set ∆A(S, S′) ∈
∆(A, S) we obtain a parallel combination ruleS,S′
A
of all assignment instruc-
tions with critical terms for updates in ∆A(S, S′). Then the bounded choice
composition of these rules for all (nota bene finitely many) successor states
S′ defines a rule ruleS
A with ∆(ruleS
A, S) = ∆(A, S).
Step 2: We adapt the proof for the Critical-Terms-Similarity Lemma
(pg. 201) to ruleS
A, applying the arguments in the lemma to every differ-
ence set in ∆(ruleS
A, S) and ∆(A, S) showing that ruleS
A depends only on the
equations between critical terms in S.
Step 3: We apply the proof for the Similarity Type Formalization lemma
to the rules ruleS
A to establish the lemma with rA defined as follows:
rA =
if φ1 then ruleS1
A
...
if φn then ruleSn
A
Case 2: in state S of the given run A makes some recursive calls. Again
in a first step we construct a bounded-choice PGA ruleS
A that computes a set
of update sets satisfying ∆(ruleS
A, S) = ∆(A, S).
To do this, consider any update (l, v0) ∈∆A(S, S′) ∈∆(A, S) at any
location l = (f , (v1, . . . , vn)). Then the update cannot be a final value return
move (i.e. updating an output function) of A because otherwise in state S′
A would be Terminated but also Waiting (having just called some child),
contradicting the Recursive-Call Relationship (pg. 209). But f could be the
input function of a called child. So we have two subcases to consider.
• Case 2.1. f is not an input function of some child algorithm.
Then all elements vi of the given update are critical values and there exist
critical terms ti of A with eval(ti, S) = vi (as in Case 1) so that we can use
the assignment rule f (t1, . . . , tn) := t0 to yield the update (l, v0) A generates
in state S.

8.4 Recursive Algorithms
217
• Case 2.2. f is an input function of a child algorithm C.
In this case we construct an i/o-PGA rule whose call yields the input defining
update ((f , (v1, . . . .vn)), v0) and creates a new child ASM to perform the
subcomputation. To do this we proceed as follows.
By the Recursive-Call-Step Postulate (pg. 209) with the Call Relationship,
A in state S is just calling the child algorithm C so that C becomes Active
and A becomes Waiting. Let SC
i with i = 1, . . . , k for some k be the subcom-
putation triggered by C (including all its subcomputations) with final state
SC
k . Consider the recursive calls made in this subcomputation: their nesting
depth is smaller (by 1) than the nesting depth of the recursive calls made
in the run segment from S to S′. This is illustrated in Fig. 8.1 (pg. 214):
in state S the algorithm calls c0 = C and defines its initial state Sc0
0 .The
subcomputation of c0 ends in state Sc0
ﬁn(c0) and during this subcomputation
the depth of nesting of recursive calls is one less than the nesting depth of
recursive calls between S and S′.
Therefore by induction on the depth of nesting of recursive calls there
is a recursive ASM component rule rC that simulates C’s subcomputation,
i.e. satisfies ∆(rC, SC
i ) = ∆(C, SC
i ) for every subcomputation state SC
i where
(i = 1, . . . , k) for some k with final state SC
k of C. With this rule we can define
the desired recursive i/o-rule output ←M(t1, . . . , tn) and its parameters as
follows.
• The body of M is the recursive rule rC, declared by M(x1, . . . , xn) = rC
where xi are the input variables of rC.
• By the Recursive-Call-Step Postulate the input value v0 at location l =
(f , (v1, . . . , vn)) for C is passed to the child but defined by an update made
by the parent so that the vi appear in S as values of critical terms ti of
A.
• The output locations of C are shared with A as critical read-only terms
of A (by the Call Relationship).
Therefore the recursive ASM i/o-rule out ←M(t1, . . . , tn) (where out is an
output location of C) yields the update ((f , (v1, . . . , vn)), v0) in state S (and
creates the new child). Nota bene that by the CALL semantics of recursive
i/o-rules (Def. 54, pg. 213) the input assignment f (t1, . . . , tn) := t0 (which
yields the update (l, v0)) is integrated into the clauses let xi = ti (for 1 ≤
i ≤n) that pass the input to the body rC of the child.
The parallel composition of all the assignment instructions and i/o-rules
defined in cases 2.1 or 2.2 for each (l, v0) ∈∆A(S, S′) yields ∆A(S, S′) in state
S. Therefore the bounded-choice composition of these rules for all successor
state S′ of S defines a recursive ASM ruleS
A with ∆(ruleS
A, S) = ∆(A, S).
Then we apply step 2 (introducing critical terms similarity) and step 3
(exploiting finiteness of critical terms) as in Case 1 to refine ruleS
A to rA
satisfying ∆(rA, S) = ∆(A, S) for every state S.

218
8 Characterization of Process Families
Case 3. In state S A makes a final step to return the value computed by its
subcomputation. In step 1 consider any update (l, v0) ∈∆A(S, S′) ∈∆(A, S)
at any location l = (f , (v1, . . . , vn)). There are again two cases to consider.
• Case 3.1. f is an output function symbol of A. The values vi have been
computed by the subcomputation of A, therefore they are values of criti-
cal terms ti of A so that the assignment rule f (t1, . . . , tn) := t0 yields the
output update l = ((f , (v1, . . . , vn)), v0).
• Case 3.2. Otherwise. Since A in state S makes its final move, it cannot
simultaneously make a new recursive call (because otherwise in its final
state, when Terminated with defined output, it would have some Active
children, something we excluded by the Call Relationship (pg. 209).
Therefore f is not an input function and the Critical Terms Lemma pro-
vides critical terms t0, t1, . . . , tn of A with values v0, v1, . . . , vn (as in case
1) such that the assignment f (t1, . . . , tn) := t0 yields the given update
(l, v0).
The parallel composition of the assignment instructions defined in cases 3.1
and 3.2 for each (l, v0) ∈∆A(S, S′) yields ∆A(S, S′) in state S. Therefore
the bounded-choice composition of these rules for all successor state S′ of S
defines a recursive ASM ruleS
A with ∆(ruleS
A, S) = ∆(A, S).
Then we apply again step 2 (introducing critical terms similarity) and step
3 (exploiting finiteness of critical terms) as in Case 1 to refine ruleS
A to rA
satisfying ∆(rA, S) = ∆(A, S) for every state S.
Remark on termination. Theorem 9 characterizes a wide class of re-
cursive algorithms, but there are other classes of interest that wait for an
abstract characterization, for example recursive algorithms whose compu-
tations are intended to not terminate. A famous example is the Sieve of
Eratosthenes; using ASMs it can be defined in three lines: starting with
sieve = {n ∈Nat | n ≥2} it inserts step by step the current minimum
p of sieve into the set of Primes and deletes any multiple of p from sieve.
SieveOfEratosthenes =
let p = min(sieve)
-- initially min(sieve) = 2
Insert(p, Prime)
sieve := {n ∈sieve | p does not divide n}
Remark on structures. In keeping with the traditional view on recur-
sion, we have formulated the postulates in Section 8.4.1 in functional terms,
i.e. for the case when the callee computes a function based on the provided
arguments and returns the resulting value to the caller.
The underlying computational model of ASMs would however lend itself
well to defining a more general notion, where entire parts of the state—
possibly expressing complex structures with all the corresponding algebraic
properties—are exchanged between caller and callee, either as arguments (to
define the initial state of the callee) or as results (to extract from the final
state of the callee those parts that are of interest to the caller).

8.4 Recursive Algorithms
219
Such an approach would provide a characterization of encoding-free recur-
sive computations on structures, thus simplifying the investigation of prop-
erties in those practical cases which are not naturally framed as computation
of recursive functions.

Chapter 9
Complexity Analysis
This chapter is devoted to the second important property of algorithms (be-
sides their correctness, theme we investigated in Ch. 7), namely their com-
plexity. Different algorithms may compute the same desired result but differ
in eﬀiciency. This triggers the analysis of the complexity of algorithmic be-
haviour in terms of various complexity measures. In Sect. 9.1 we illustrate
the power of universal (programmable) machines and of recursion, but also
some intrinsic computational limits and their impact on fundamental issues
in computation theory and logic; these limits separate a) algorithmically un-
solvable (computational or logical) problems from those that have an algo-
rithmic solution, and similarly b) problems that cannot be solved eﬀiciently
from those that have eﬀicient solutions. In Sect. 9.2 we point to the role of a
modern complexity theory that investigates algorithms which work on appro-
priate structures (not only numbers or strings) and are executed by networks
of corresponding abstract machines (not Turing machines), measuring com-
plexity in terms of a great variety of concrete structural parameters that are
of practical relevance.
9.1 Power and Limits of Computation
In this section we explain four fundamental concepts that determine the
power as well as the intrinsic limits of algorithms: (un)decidability (with
its recursive enumerability companion), the diagonalization and reduction
method, and universal (programmable) machines. The reader will find here
no new result but can experience the simplifying effect of dealing with com-
putational issues at appropriate levels of abstraction, in particular dealing
directly with programs instead of their numerical encoding (Gödelization).
Decidability and Recursive Enumerability. Once one has a notion of
algorithms a ∈Alg and of algorithmically computable functions f ∈AlgFct
one can define the fundamental concept of decidability and of recursive enu-
221
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
E. Börger, V. Gervasi, Structures of Computing, 
https://doi.org/10.1007/978-3-031-54358-6_9 
 
 
 
 
 

222
9 Complexity Analysis
merability of sets or relations. A set X (or its defining property) is called
Algorithmically decidable—i.e. decidable by algorithms in Alg—if its char-
acteristic function ξX is an Alg-computable total function ξX ∈AlgFct;
it is called Alg-undecidable (or unsolvable) if it is not Alg-decidable. X is
called recursively enumerable by algorithms in Alg if it is empty or coincides
with the range f (0), f (1), . . . of an Alg-computable (possibly partial) func-
tion f ∈AlgFct. The terms decidable or recursively enumerable are normally
used without further specification if they are intended to relate to the in-
tuitive meaning of the underlying sets Alg and AlgFct or if these sets are
clear from the context, as happens in classical recursion theory that is about
partial recursive (i.e. Turing computable) functions over Nat. The idea for
these definitions can be traced back to Leibniz’ definition of ars iudicandi (a
method to decide the truth of given scientific statements) and ars inveniendi
(a method to list all true scientific statements) (see [97]).
Exercise 30 (Aristoteles). Show that if a set and its complement are both
recursively enumerable, then they are decidable.1 Below we will see that not
every recursively enumerable set is recursive.
Diagonalization. A property of the intuitive notion of algorithm is that
any algorithm comes with a finite (usually thought to be textual) description.
As a consequence there are only denumerably many algorithms so that for
set-theoretic cardinality reasons there are more problems (sets) than
decidable problems. In fact, Cantor’s diagonal argument shows that there
is no enumeration C0, C1, . . . of all subsets of the Natural numbers because
otherwise the diagonal set D = {n ∈Nat | n ̸∈Cn} would be one set Ck of
the list which implies the contradiction
k ∈Ck
iff
(by Ck = D) k ∈D
iff
(by Def. of D) k ̸∈Ck.
This argument is visualized (in the form known as Richard’s paradox [142])
by Fig. 9.1 where for every list of monadic functions (say 0-1-valued charac-
teristic functions c0, c1, . . . of subsets Ci of Nat) one obtains a new diagonal
function d (the characteristic function of the diagonal set D) by defining
d(n) = 1 −cn(n) ̸= cn(n).
From a constructive point of view Cantor’s diagonalization argument de-
fines for each denumerable list of objects of a given type—here sequences
C0, C1, . . . of sets Ci ⊆Nat or c0, c1, . . . of functions ci : Nat →{0, 1}—
some new object of the given type (here D ̸= Ci because i ∈D iff i ̸∈Ci
respectively d because d(i) ̸= ci(i)), showing thereby that such lists cannot
1 Aristoteles was the first to develop a calculus which enumerates the set of valid syllo-
gisms together with a calculus to enumerate the set of fallacies; the latter is not complete
but has been completed in [157, 123] showing that the Aristotelian syllogistic is decid-
able. See [26, 27] with further references there.

9.1 Power and Limits of Computation
223
Fig. 9.1 Richard’s paradox. © 1985 (Vieweg-Verlag now) Springer-Verlag
GmbH Berlin Heidelberg, reprinted with permission
fct
arg
0
1
2
. . .
n
. . .
c0
c0(0)
. . .
. . .
. . .
. . .
. . .
c1
...
c1(1)
c2
...
c2(2)
...
...
...
cn
...
cn(n) ̸= 1 −cn(n)
...
...
...
exhaust the class of considered objects. A similar use of diagonalization can
be made for various decision procedures or computing machines, establishing
some limits for them: given a (maybe hypothetical) problem solving machine
P use it to construct another machine D that inverts the result of the prob-
lem solver, thus generating a contradictory behaviour if D is applied to itself.
We show below a few examples that establish by diagonalization some funda-
mental limits of computational methods (Sect. 9.1.1). On the positive side,
the diagonalization method is frequently used in complexity theory to prove
hierarchy theorems.2 It also provides a theoretical foundation for the mean-
ing of recursion which is related to universal (programmable) machines we
explain in general ASM terms in Sect. 9.1.2.
Reduction Method. Once one has an undecidable (or a lower bound
for the computational complexity of a) property P one can use it to show
the undecidability (or a lower complexity bound) of other problems Q by re-
ducing Q algorithmically to P instead of producing a specialized diagonaliza-
tion argument for Q. The reduction method has innumerable applications for
proving undecidability and more generally complexity limits of given problem
classes. We illustrate as outstanding example an undecidability proof for the
Entscheidungsproblem (the decision problem of first order logic) (Sect. 9.1.3)
that has a complexity theoretic analogue, namely that the satisfiability prob-
lem of propositional logic is NP-complete (Sect. 9.1.3.2), where NP is the
class of problems that can be decided by a nondeterministic Turing machine
in polynomial time.
2 For an example see the universal ASM in Sect. 9.2.1: applying to it a diagonalization
establishes a linear time hierarchy for RAMs and a class of ASMs.

224
9 Complexity Analysis
9.1.1 Undecidable Computational Problems
(Diagonalization)
In [162] Turing used a diagonalization argument to prove the undecidabil-
ity (by Turing Machines) of various TM properties (e.g. whether a TM is
circle-free or not, whether it ever prints a particular symbol, etc.). In this
section we illustrate the method by two examples: proving the undecidability
of the Halting problem of arithmetical PGAs (see Exercise 29, pg. 204) and
more generally of any other non-trivial computational problem for them, as
expressed by the Theorem of Rice below. The proof of this theorem uses the
Recursion Theorem we explain in Sect. 9.1.2.2.
To remain close to the formulation of Cantor’s and Richard’s diagonal
argument we define
Halt = {M ∈ArithPGA | M(M) ↓}
to be the set of (Gödel numbers of)3 arithmetical PGAs M which, started
with their own description M as input inM = M, eventually terminate their
input-driven computation, property usually denoted by M(M) ↓. As usual for
ASMs we use as termination criterion whether a state is reached that yields
an empty update set. The following theorem shows that Halt is undecidable
(by arithmetical PGAs or computationally equivalent algorithms).
Theorem 10 (Halting Problem). It is undecidable by arithmetical PGAs
whether any arithmetical PGA M started with its own description M as input
eventually terminates its input-driven computation.
Proof by contradiction. Assume that there is an arithmetical PGA H that
decides Halt, i.e. started in any initial state with input M ∈ArithPGA, H
eventually reaches a state with defined output, namely yes or no, answering
the question whether M ∈Halt or not. Then H is easily changed to an
input-driven arithmetical PGA D as defined by Fig. 9.2. The machine D first
uses H to compute the (initially undefined output) answer to the question
whether M ∈Halt is yes or no. Then the control flow is changed to invert the
answer: if answ = no holds (i.e. M ̸∈Halt) we trigger the termination of D
by letting D in ctl = halt generate the empty update set; if answ = yes holds
(i.e. M ∈Halt) we trigger a not terminating computation of D by letting D in
ctl = loop continue ad infinitum to generate some update without changing
the state (namely the update (ctl, loop) by the definition of the FSM scheme
with cond = true and rule =skip, see pg. 28).
As a result, letting D work with itself as input implies a contradiction:
D ∈Halt
iff
(by Def. of Halt) D(D) ↓
iff
(by Def. of D) D ̸∈Halt.
3 To let the algorithmic features discussed in this chapter stand out expressis verbis, we
consider programs as numbers, abstracting from the particular Gödelization of programs
by natural numbers that remains implicit.

9.1 Power and Limits of Computation
225
Fig. 9.2 Diagonal Machine D for Halt (with Input Machines M)
comp
H
answ = undef
answ = no
loop
halt
no
yes
yes
no
In [141] it has been shown that undecidability holds not only for Halt but
for every class of numerical algorithms that compute a non-trivial algorithmic
property or relation. We prove this here for arithmetical PGAs. We borrow
the recursion-theoretic notation [M] to denote the function computed by (the
ASM with Gödel number) M (Def. 26, pg. 55).
Theorem 11 (Theorem of Rice). Let C be a non-trivial class of ArithPGA-
computable functions, where non-trivial means that C ̸= ∅and C is not the
class of all ArithPGA-computable functions. It is undecidable by arithmetical
PGAs whether an arithmetical PGA M computes a function in C, formally
stated: {M ∈ArithPGA | [M] ∈C} is ArithPGA-undecidable.
Proof by contradiction. The proof uses the Recursion Theorem 15 (proved
in Sect. 9.1.2 below) that every computable function f has a ‘fixpoint’ pro-
gram M in the sense that M computes the same function as f (M), i.e. such
that [M] = [f (M)] holds. We apply this to a recursive diagonalization func-
tion d that changes the behaviour of a decision procedure we assume to be
given for the set {M ∈ArithPGA | [M] ∈C}.
Let M0 be some arithmetical PGA with computed function [M0] ̸∈C and
M1 some arithmetical PGA with computed function [M1] ∈C. Define the
diagonalization d as follows (see Fig. 9.3):
• Map every arithmetical PGA M with [M] ∈C to the machine M0—which
computes a function that is not in C.
• Map every arithmetical PGA M with [M] ̸∈C to the machine M1—which
computes a function that is in C.
This function d is (by the assumption) clearly ArithPGA-computable.
d(M) =
(
M0
if [M] ∈C
M1
else
Therefore by the Recursion Theorem 15 d has a ‘fixpoint’ program M satis-
fying [M] = [d(M)]. Then the function computed by M must be element of
C or not.

226
9 Complexity Analysis
Fig. 9.3 Fixpoint for Rice’s Theorem. © 1985 (Vieweg-Verlag now) Springer-
Verlag GmbH Berlin Heidelberg, reprinted with permission
Compl(C)
C
[M0]
[M1]
d
[M] = [d(M)]
• Case 1: [M] ∈C. Then d(M) = M0 so that [M] = [d(M)] = [M0] ̸∈C
and thus [M] ̸∈C.
• Case 2: [M] ̸∈C. Then d(M) = M1 so that [M] = [d(M)] = [M1] ∈C
and thus [M] ∈C.
So [M] ∈C iff [M] ̸∈C, a contradiction. Therefore the assumption that the
set {M ∈ArithPGA | [M] ∈C} has a decision procedure is false and the
theorem is proved.
Exercise 31. To prepare the ground for the next exercise we remind the
reader that every n-registers machine M can be simulated by a register ma-
chine M ′ with only two registers [129, 155]. Hint: using prime number encod-
ing of register contents define M ′ such that for control states i, j, i′, j′ and
register contents xl, yl the following holds:
(i, x1, . . . , xn)
1⇒M (j, y1, . . . , yn) iff
(i′, ⟨x1, . . . , xn⟩, 0) ⇒M′ (j′, ⟨y1, . . . , yn⟩, 0)
Exercise 32 (2-RM Halting Problem). Apply the proof for Theorem 10
above to the following halting problem for 2-register machines (see the pre-
ceding Exercise 31). Call a control state halting state of M if M has no
RM-instruction for mode = halt (so that M yields the empty update set once
it entered this mode): Halt2−RM = {M ∈2-RM | (0, 0, 0) ⇒M (halt, 0, 0)}.
9.1.2 Universality and Recursion
Turing’s discovery in [162] of the existence of universal programmable ma-
chines brought the stored-program digital computer model that changed our
world. The recursion theorem is a cornerstone of classical recursion theory. In
this section we formulate these two fundamental concepts in terms of ASMs

9.1 Power and Limits of Computation
227
so that the recursion-theoretic results stand out directly, in their algorithmic
nature, abstracting where possible from the details of a number encoding of
computational features of algorithms that work on non-numeric objects of
algorithmic nature.
9.1.2.1 Universal Machine and Enumeration Function
To formulate directly and in some detail a universal Turing machine that
can simulate every TM computation is technically rather involved, given the
(intended!) rudimentary signature of TMs (see [162]). But on a higher level
of abstraction the intuitive concept of simulation of a class of machines can
be expressed in a rather simple yet concise way. We illustrate this here with
ASMs, exploiting that their comprehensive state concept (if we abstract from
finiteness and implementation concerns) permits to incorporate the states and
the program of any ASM directly, without encoding, into the current states
of a universal machine. In Sect. 9.2.1 we show how this abstract concept of
a UniversalAsm can be refined (in the sense explained in Sect. 7.2) to a
universal machine for ASMs of a fixed finite signature.
Here is the classical definition Turing gave of the concept of a “universal
computing machine” U:
If this machine U is supplied with a tape on the beginning of which is written the
S.D [Standard Description] of some computing machine M, then U will compute
the same sequence as M. [162, pg. 241]
In other words a (single-agent) universal ASM needs as input (say in a 0-ary
function simPgm) a description of the to-be-simulated machine M together
with a description (say in a 0-ary function simState) of an initial state where
to start the M-sequence (read: M-computation) and its simulation. We pro-
vide this by an initial step of the universal machine: in mode = init it loads
the pgm(M) into simPgm and one of M’s InitialStates into simState and
goes into mode = simulate. For simplicity of exposition we make the nota-
tional assumption made already for reflective ASMs (Sect. 4.3), namely that
all declared submachines of M are put into its main program, replacing every
occurrence of a call rule N(t1, . . . , tn) in the main(M) program by the rule
body N ′(x1/t1, . . . , xn/tn) where N(x1, . . . , xn) = N ′ is the rule declaration.
Therefore pgm(M) = main(M).
For the simulation steps let us restrict our attention for the moment to
steps of deterministic ASMs (without choose construct), as done by Tur-
ing for TMs; for the extension to nondeterministic ASMs see Exercise 33.
In mode = simulate the universal machine uses the function eval (Def. 14,
pg. 32) to compute the update set U = eval(simPgm, simState), the very
same update set the to-be-simulated (deterministic) machine computes in
its state simState. If U is consistent the universal machine uses the apply
function (Def. 9, pg. 19) to apply U to the current simState.

228
9 Complexity Analysis
This explains the following definition of a UniversalAsm rule that sim-
ulates computations of deterministic ASMs. Obviously we assume without
loss of generality that mode ̸∈Σ(M) for every simulated M.
UniversalAsm =
if mode = init then
-- initialize the simulation
choose M ∈{M | M is a deterministic ASM}
choose S ∈InitialState(M)
simPgm := main(M)
-- load M’s main program
simState := S
-- load an initial state of M
mode := simulate
-- start the simulation
if mode = simulate then
let U = eval(simPgm, simState)
-- compute M’s updates
if Consistent(U) then simState := apply(simState, U)
where apply(S, U) = S + U
Remark on unbounded choice. To express which program (with an
arbitrary signature!) and which initial state are selected to be loaded for the
simulation by the universal machine we make use of the unbounded choose
operator. Sure the resulting machine UniversalAsm is not what in the
literature is called a sequential ASM (a PGA in our terminology). For a
refinement to a machine UniversalAsm(Σ) that simulates every ASM over
the given fixed signature Σ see Sect. 9.2.1.
Exercise 33 (Universality for Nondeterminism). Refine the machine
UniversalAsm to work for nondeterministic ASMs. For a solution see Ap-
pendix B.
A theme that is about an instance of ‘universal’ machines is that of com-
putable enumeration functions e, i.e. computable functions where every com-
putable (without loss of generality 1-argument) function f satisfies for some
k the equation f (x) = e(k, x) for all x. Kleene [112] investigated such enu-
meration functions in terms of µ-recursive functions4 which led to a nor-
mal form for such functions we shortly present here abstractly, in terms of
ASM-computable functions (think about k as some ASM rule). The normal
form is used in the Recursion Theorem in Sect. 9.1.2.2. For the notion of
ASM-computability with arbitrary (not necessarily structured) deterministic
i/o-ASMs M over Nat we refer to Def. 23 (pg. 52) and Def. 26 (pg. 55).
Theorem 12 (Kleene Normal Form Theorem). There exists a binary
enumeration function e of all unary ArithPGA-computable functions that
is ArithPGA-computable, i.e. for every ArithPGA-computable f there is an
arithmetical PGA M such that for all arguments x holds f (x) = e(M, x).
Traditionally the function e(M, x) of x computed by M is denoted by [M](x).
4 Note that Kleene submitted his article 2 years before Turing’s article [162] was pub-
lished.

9.1 Power and Limits of Computation
229
Proof. We define four ArithPGA-computable functions whose composition
specifies an interpreter model for arithmetical PGAs and thus yields the
desired enumeration function e. The four functions specify the interpreter
components, each of them corresponding to a fundamental conceptual ingre-
dient of computing functions. We remind the reader that by definition the
machines to compute functions are i/o-machines that interact with the envi-
ronment only to determine the initial state and to output the result of their
computation.
• An input function input(M, x) defines the initial state of M with input
x, i.e. the state where in = x and pgm is (the program of) M.
• A 1-step transition function steppgm whose iteration iterate(pgm, n) =
stepn
pgm describes the state reached (if defined) after n steps of pgm.
• A stop criterion Stop(S) expresses a termination property, for example
that in state S the pgm generates an empty update set.
Exercise 34. Show that one can equivalently use the criterion that in the
given computation the initially undefined output location out is for the
first time defined in state S (formally that for the first time outS ̸= undef
holds). For a hint see the Appendix B.
• An output function output(S) extracts the value outS.
These four functions imply that the following equation holds for all x if f is
computable by an arithmetical i/o-PGA M. Kleene’s termination predicate
T(pgm, x, n) expresses that starting in the initial state S0 (with in = x and
loaded program pgm) after n pgm-steps a state S is reached (i.e. S0
n⇒pgm S)
in which the Stop(S) predicate is satisfied:
(∗)
f (x) = output( iterate (pgm, µn(T(pgm, x, n))) (input(M, x)) )
Intuitively, these four functions and the T-predicate are clearly ArithPGA-
computable (in fact µ-recursive, see Theorem 1, pg. 56). That the composition
(here e) of ArithPGA-computable functions is ArithPGA-computable has been
shown already in Theorem 1. We recall the iterative definition of stepn
pgm =
iterate(pgm, n) (see Def. 25, pg. 54).
step0
pgm(S) = S
stepn+1
pgm (S) =









stepn
pgm(S)
if eval(pgm, stepn
pgm(S)) = ∅
not defined
if eval(pgm, stepn
pgm(S)) is
inconsistent
steppgm(stepn
pgm(S))
otherwise
Remark on Universality. The composition of four functions for in-
put, output, a 1-step transition and a termination criterion to a universal
(read: a computable enumeration) function defined by (∗) represents ab-
stractly Kleene’s normal form for computable numeric functions and can

230
9 Complexity Analysis
be rephrased in terms of any standard computation model. As an interesting
example see Moschovaki’s iterator definition in terms of in/output, transition
and termination functions to formally define the meaning of algorithms [131,
pg. 11]. An instance of the equation (∗) for the register machine model can
be found in [29, Ch. AII1]: using an explicit Gödelization just instantiate
the definition of the four functions in terms of RM-programs working with
Natural number registers (see Exercise 5). For Buchberger’s analysis of those
functions that can play the role of one of the four components of ‘universal’
computation models and for references on this theme see [29, Ch. BIII3].
9.1.2.2 Recursion Theorem
The Recursion Theorem of classical recursion theory is based upon a simple
program transformation technique known as Sm
n −Theorem in logic, as Pa-
rameterization Lemma in computing where it found an interesting application
for partial evaluation of programs. We first describe this parameterization
technique.
The partial evaluation interpretation of the parameterization lemma reads
as follows: consider programs M to be given with a sequence y of m input
variables for static input and a sequence x of n input variables for dynamic
input. The idea is that the static input is the part of the input data that
can be precomputed because known at compile time. For every m, n one can
compute a program called Sm
n that takes as input any program M together
with any values y1, . . . , ym for its static input variables and generates a hope-
fully more eﬀicient ‘residual program’ Sm+1
n
(M, y1, . . . , ym) that computes
for every dynamic input x1, . . . , xn the same result as M for the entire input
y1, . . . , ym, x1, . . . , xn. A particularly interesting case is where M is a pro-
gramming language interpreter ([81]). In abstract terms this specification of
the program transformer Sm
n is easily stated and proved to work correctly.
Theorem 13 (Sm
n −Theorem). For every m, n one can construct an arith-
metical PGA Sm+1
n
such that for every arithmetical PGA M and every input
sequence y, x (with y of length m and x of length n) the following equation
holds:
[M](y, x) = [Sm+1
n
(M, y)](x).
Usually the indices m, n are omitted if they are clear from the context.
Proof. Define Sm+1
n
(M, y) by in := (y, in) seq M, an ASM which for every
input x (of length n) first copies y (of length m) into the input in and then
executes M with in = y, x.
The recursion theorem is easily proved using the Sm
n −theorem. We first
prove a form of the theorem for monadic ArithPGA-computable functions.
Theorem 14 (Fixpoint Theorem). Each ArithPGA-computable function
f has a program fixpoint M such that [f (M)] = [M].

9.1 Power and Limits of Computation
231
Proof. Let f be ArithPGA-computable. Then also the function [f (S(j, j))](x)
of j, x is ArithPGA-computable so that for some arithmetical i/o-PGA M and
every j, x holds
[M](j, x) = [f (S(j, j))](x)
so that by this equation for [f (S(j, j))](x) and the Sm
n −theorem holds
[f (S(M, M))](x) = [M](M, x) = [S(M, M)](x)
establishing the fixpoint [f (S(M, M))] = [S(M, M)].
Slightly modifying this proof one obtains a proof for the recursion theorem.
Theorem 15 (Recursion Theorem). For each ArithPGA-computable func-
tion f (of arity n+1) there is an i/o-ArithPGA M such that [M](x) = f (M, x)
holds for every argument tuple x of length n.
Proof. For ArithPGA-computable function f also the function f (S(j, j), x)
is ArithPGA-computable so that for some arithmetical i/o-PGA M and all
j, x holds
[M](j, x) = f (S(j, j), x).
By the Sm
n −theorem and the definition of the function f (S(j, j), x) it follows
that for every x the following equation holds:
[S(M, M)](x) = [M](M, x) = f (S(M, M), x).
Remark. The recursion theorem allows one to define a computable func-
tion g of n arguments by an equation f (M, x) = t for an n+1-ary computable
function f where in the defining term t occurrences of [M](x) (read: of recur-
sive calls of M) are allowed to stand for the intended g(x).
9.1.3 Undecidable Logic Problems (Reduction Method)
In this section we illustrate the reduction method to derive impossibility
results for a set P of problems by translating into P in an effectively com-
putable manner a set of problems Q which are already know to share this
impossibility property.
As application examples we prove the complexity of two outstanding log-
ical decision problems: the undecidability of the Entscheidungsproblem (the
decision problem of first-order predicate logic, Sect. 9.1.3.1) and the NP-
completeness of the satisfiability problem of propositional logic (Sect. 9.1.3.2).
We prove both results by refinements of Turing’s method [162] to describe
the semantics of machine programs by logical formulae, used as reduction
method for establishing complexity relations between logic and computing.
There is a great variety of reducibility concepts (see [29]). The following
standard notion from recursion theory suﬀices for our illustrative purposes
here.

232
9 Complexity Analysis
Definition 55 (Many-One Reduction). A set A is called m-reducible5 to
a set B, denoted A ≤m B, if there is an algorithmically computable function
f (called a reduction function) such that for all x holds x ∈A iff f (x) ∈B.
Lemma 9 (Many-One Reduction Property). If A ≤m B and A is algo-
rithmically undecidable, then B is algorithmically undecidable.
9.1.3.1 Entscheidungsproblem
A century ago Hilbert presented the Entscheidungsproblem as ‘the main
problem of mathematical logic’ [100, pg. 74], hoping that one would find
an algorithm for deciding the truth or falsity of any mathematical state-
ment.6 Church and Turing proved independently that this is not possible (see
[62, 162]). Turing’s proof introduced a method to effectively reduce machine
problems—specifically an unsolvable word problem for TMs—to logical de-
cision problems for the class of formulae that describe the machine problem.
It has numerous variations and became a fundamental method of complexity
theory.
For the decision problem of first-order logic we illustrate the method by a
frugal and particularly simple logical description of the semantics of 2-register
machines.7
Theorem 16 (Logical Interpretation of 2-Register Machines). One
can define the semantics of 2-register machines M by predicate logic program
formulae StepM with atomic formulae C representing states C of M such
that the following equivalence holds:
Lemma 10 (2-RM Simulation). For all states C, D of any 2-RM M holds
C ⇒M D iff C ∧∀x∀yStepM →D is a logically valid formula
where logical validity means that the formula is true for every (not only for
the intended) interpretation of its predicate and function symbols.8
5 Many-One refers to the fact that the reduction function is not necessarily injective
(one-to-one).
6 That logic would be of help to define problem solving methods was known for cen-
turies, but Leibniz seems to be the first who understood the need of a comprehensive
language (characteristica universalis) and a mechanical procedure (calculus ratiocina-
tor) to resolve mechanically any question formulated in the characteristica universalis.
It was Frege who defined a concrete universal (symbolic) language for mathematics on
the basis of which Russell and Whitehead could formalize the mathematics of their time.
7 This logical encoding has been developed in [1, 25] with the goal to obtain classes of
syntactically severely restricted formulae with unsolvable decision problem.
8 When writing logical formulae we use the traditional symbols ∀instead of forall, ∃
instead of forsome, ∧instead of and, →for implication, etc.

9.1 Power and Limits of Computation
233
Proof. 2-RM states are triples (i, m, n) of control state i and register con-
tent m, n one can describe by an atomic logical formula Kimn, denoted as
(i, m, n). Due to a theorem of Skolem one can identify numbers m with the
logical term built up from 0 by applying m times the successor function ′
to 0. The intended interpretation of the atomic formulae Kimn is that from
some start state (see StartM below) M can reach the state (i, m, n). Each
machine step C
1→M D—determined by the execution of an M-instruction—
can be ‘simulated’ as a logical deduction step: it deduces (read: applies the
rules of logic to derive) D from C and a program formula ∀x∀yStepM. To
achieve this define StepM as the conjunction of the following implications
for each instruction-determined possible step of M.
• For addition instructions (i, 1, +1, l) concerning the first register define
Kixy →Klx′y, similarly for addition instructions concerning the second
register (inverting the role of x and y).
• For subtraction instructions (i, 2, −1, l) concerning the second register
define Kiyx′ →Klyx and Kiy0 →Kly0 (remember that for natural
numbers the function x −1 is defined to satisfy 0 −1 = 0), similarly for
subtraction instructions concerning the first register (inverting the role
of x and y).
• For 0-test instructions (i, 1, test, l) concerning the first register define
Ki0y →Kl0y and Kix′y →Ki+1x′y, similarly for subtraction instruc-
tions concerning the second register (inverting the role of x and y).
Then the simulation lemma follows by an induction on M-runs. As a corollary
one obtains the Church-Turing theorem.
Theorem 17 (Church, Turing). The Entscheidungsproblem—even the de-
cision problem of the class of (syntactically rather restricted) reduction for-
mulae αM of 2-Register Machines M—is algorithmically undecidable.
Proof. Take in the 2-RM simulation as StartM and StopM formulae
K000 respectively Khalt00. This provides an m-reduction of Halt2−RM to the
class of reduction formulae αM = K000 ∧∀x∀yStepM →Khalt00 for 2-RM
machines M; in fact (0, 0, 0) ⇒M (halt, 0, 0) iff αM is valid. This proves the
theorem because the halting problem Halt2−RM is known to be algorithmi-
cally undecidable (see Exercise 32 on pg. 226 and [129, 155]).
9.1.3.2 NP-Completeness of Propositional Logic
In this section we explain a frugal parameterized scheme (discovered in [28])
for logical descriptions of time-restricted or space-restricted Turing machine
runs that establish strong links between computational complexity of ma-
chines and expressibility of logical formulae. We illustrate the method by a

234
9 Complexity Analysis
proof for the NP-completeness of SAT, the well-known satisfiability problem
of propositional logic.9
First of all we must refine the reducibility concept to take time restrictions
for the computation of reduction functions into account.
Definition 56 (Polynomial Time Reduction). A set A is called p-time
(or polynomial-time) reducible to a set B, denoted A ≤p B, if A ≤m B via a
reduction function f that is computable by a deterministic Turing machine
in polynomial time (in the length of the input).
For the logical interpretation of 2-RMs in Theorem 16 above, given the
simple structure of 2-RMs and the goal to obtain syntactically rather re-
stricted reduction formulae, we defined for each M a fixed program formula
StepM and for each state C a fixed atomic state formulae C. For (in this
section possibly non-deterministic) Turing machines M we define a scheme
for composing in different logics program and state formulae StepM and C
to the intended reduction formula out of the following parameterized basic
formulae with the indicated intended interpretation:
• H(t, x) to express that the head position at time t is the tape cell x. For
notational simplicity the tape is assumed to consist of cells numbered
0,1,….
• Tj(t, x) to express that at time t the tape cell x contains the letter j.
• Ii(t) to express that at time t the control state is i.
• S(t, t′) to express that the successor of t is t′.
Note the time parameter t that is needed in general to express time con-
straints (but was not needed to describe the not time-restricted 2-RM halting
problem). Note also the presence of the successor relation that is needed in
general (but in the case of register machines became a function symbol S to
represent numbers as Sm0).
Assume any state Ct of M at time t to be expressed in terms of basic
formulae by a formula Ct. Then one can define the program formula StepM
in terms of basic formulae such that the following holds.
Lemma 11 (TM Simulation). Let A be a model that satisfies C0∧StepM.
If M started in C0 can make t steps and A provides enough time and space
parameters to describe those steps—i.e. in A there are S-chains of length t
of time and tape cell parameters—then for some Ct holds C0
t⇒M Ct along
those chains and A satisfies Ct.
StepM is defined as (finite or infinite, depending on the time and space
resources) conjunction of the following formulae for every M-instruction
(i, j, k, move, l) (see Example 5 on pg. 26 and the tuple-notation for instruc-
tions on pg. 12) for all parameters i, l (for control states), j, k (for tape
9 Numerous instantiations of the method are applied in [48] for a systematic investigation
of the complexity of logical decision problems.

9.1 Power and Limits of Computation
235
letters) and all t, t′, x, x′, y concerned according to the underlying time and
space bounds. Without loss of generality assume that instructions with same
pair i, j make the same move.
• for right move instructions (i, j, k, +1, l) define:
Ii(t) ∧H(t, x) ∧Tj(t, x) ∧S(t, t′) ∧S(x, x′)
→W
(i,j,k,+1,l)∈M Tk(t′, x) ∧H(t′, x′) ∧Il(t′)
• the same with H(t′, x) in the conclusion for no-move instructions
• for left move instructions (i, j, k, −1, l) define:
Ii(t) ∧H(t, x′) ∧Tj(t, x′) ∧S(t, t′) ∧S(x, x′)
→W
(i,j,k,−1,l)∈M Tk(t′, x′) ∧H(t′, x) ∧Il(t′)
• for no change outside the current head position define:
H(t, x) ∧y ̸= x ∧Tj(t, y) ∧S(t, t′) →Tj(t′, y))
Exercise 35. Convince yourself by an induction on run time t that the TM-
simulation holds. Consider halting instructions with control state halt to be
of form (halt, j, j, 0, halt) (stay idle).
A propositional logic instantiation of program and state formulae StepM, C
proves the following theorem.
Theorem 18 (Cook, Levin). SAT, the satisfiability problem of proposi-
tional logic, is NP-complete, i.e. it is an element of NP and every X ∈NP
is p-time reducible to it.
Proof. SAT ∈NP holds because for some nondeterministic TM M, after
guessing a truth value assignment to the propositional variables occurring
in a given input formula α in conjunctive normal form, M computes the
resulting truth value v of α in polynomial time in the length of the input and
accepts α if and only if v = 1.
To polynomial-time reduce the acceptance problem of any nondeterminis-
tic p-time bounded TM M it suﬀices to appropriately instantiate StepM to a
propositional formula in conjunctive normal form and to add two state formu-
lae for Start and an Accepting halt such that the following time-restricted
reduction property follows from the above TM-Simulation Lemma 11 (where
for brevity we write s for the polynomial bound of the number of steps in
the length of the input and use 1 as accepting control state). Clearly the
reduction function is TM-computable in polynomial time.
Time-Restricted Reduction Property:
M accepts input v0 . . . vn−1 ∈{0, 1}n in s steps iff
(Startn,s∧M-Step∧Accepts,1)(x0/v0, . . . , xn−1/vn−1) is satisfiable

236
9 Complexity Analysis
M-Step is defined by an instantiation of the schema StepM: interpret the
basic formulae (deleting those with symbol S) as pairwise different proposi-
tional variables with natural number parameters t, t′, x, x′, y ≤s for time and
space. S(t, t′) is interpreted as t′ = t +1 (the same for S(x, x′)) and is treated
not as part of the formula but as condition on the admitted parameters, the
same for y ̸= x.
The initial M-states are (0, v0)v1 . . . vn−1b . . . b (of length s) where b is the
blank tape symbol (say b = 2), 0 the initial control state and the initial head
position. Therefore the start formula is defined by:
Startn,s =
I0(0) ∧H(0, 0) ∧V
0≤j<n Inputj ∧V
n≤j≤s Tb(0, j)
where
Inputj = (T1(0, j) ↔xj) ∧(T0(0, j) ↔¬xj)
Note that due to the formulation of Inputj the start formula depends only
on the length n of the input, not on the concrete input values v0v1 . . . vn−1.
The stop formula Accepts,1 must express that the final control state,
the one at time s, is the accepting control state 1. Since the TM-simulation
lemma only guarantees that in the final state at least one control state i is
reached we formulate the Acceptance condition by stating that in the final
state no control state formula Ii(s) different from I1(s) is true.
Accept =
^
i̸=1
¬Ii(s)
Exercise 36. Instantiate the above TM formalization scheme by first-order
formulae to derive an alternative proof for the unsolvability of the Entschei-
dungsproblem.
9.2 Complexity of Computing over Structures
In this section we explain how replacing Turing Machines by Abstract State
Machines leads to a complexity theory for computations on structures (not
only numbers or strings) that has the potential to become useful also for the
practice of today’s computing. Such investigations are related to the classical
study of the relations between algorithms and data structures with respect
to their complexity [6, 133] as well as to a major theme in finite model the-
ory, namely the relations between logic languages and complexity classes
they capture [68, 121]. Given the extraordinary richness of the ASM world
compared to Turing’s world [12] one would have expected a huge theoretical
interest to explore this new world,10 but up to today there is only a compara-
tively small number of papers in this challenging area of research, to mention
10 Already in 1982, long before the final definition of ASMs in [92], it had been asked in
[61] whether the complexity class P can be characterized by a model of computation over

9.2 Complexity of Computing over Structures
237
some of the first ones [21, 22, 18, 19]. Here we illustrate the potential of such
ASM-based complexity studies by two examples: the construction of a uni-
versal ASM over a fixed signature which leads to a linear hierarchy for ASMs
and RAMs (Sect. 9.2.1) and the investigation of distributed Look-Compute-
Move algorithms (Sect. 9.2.2) which operate and move in discrete graphs or
in Euclidean spaces.
We have explained in Sect. 8.1.1 how Turing’s model of computation is
generalized in a natural way by a simple class of ASMs, namely Parallel
Guarded Assignments. In fact, not only are TM programs literally PGAs (as
shown by Example 5, pg. 26), but the normal form of PGAs
if φ1 then f1(s1,1, . . . , s1,n1) := t1
...
if φk then fk(sk,1, . . . , sk,nk) := tk
(see Theorem 7, pg. 200)—a parallel composition of guarded assignments
where the guards φi are Boolean combinations of term equations—generalizes
directly, term-by-term, the usual behavioural description of TM-instructions
(i, a, b, m, j):
if ctl = i and tape(head) = a then tape(head) := b
-- print
if ctl = i and tape(head) = a then head := m(head)
-- move
if ctl = i and tape(head) = a then ctl := j
-- proceed
Remember that each TM-step can be seen as a condensed form of the om-
nipresent Look-Compute-Action pattern (pg. 27). Furthermore, as we know
from Theorem 7, the PGA model captures at any level of abstraction the com-
putational paradigm of the Sequential Postulates, i.e. of single-agent input-
driven stepwise data processing using a fixed program.
This means that TMs are a particular subclass of PGAs so that nothing
is lost in passing the investigation from TMs to PGAs and more generally to
ASMs, but a lot can be gained. Already the few but characteristic examples
in Sect. 9.1.1 (Halting Problem and Theorem of Rice) and Sect. 9.1.2 (Kleene
Normal Form Theorem and the Recursion Theorem) illustrate how the high
level of abstraction ASMs support lets algorithmic phenomena and arguments
stand out explicitly in a technically simple manner. This holds also for the
idealized computer models in terms of which traditional complexity theory
works, in particular the TM model, the Random Access Machine (Exercise 14,
pg. 59) with its reflective RASP version (Sect. 4.6, pg. 101), the Bulk Syn-
chronous Parallel Control model (Sect. 6.1) for the design and analysis of
parallel algorithms, and many others most of which satisfy the postulates for
a class of ASMs that captures those idealized paradigms of computing (see
the examples illustrated in Ch. 8).
structures (instead of Turing machines working on strings). In view of the successful use
of ASMs in the practice of computing the theme of a computation theory over structures
has been proposed again in [58, pg. 310] (Problem 26), see also [125].

238
9 Complexity Analysis
There is a large variety of concepts to measure the computational com-
plexity of (classes of) algorithms by the amount of resources required to run
the algorithms. In the traditional complexity theory mostly used are bounds
on computation time and space, e.g.:
• the number of steps or of basic (arithmetical, communication, load/store,
read/write, move, etc.) operations that are performed in an appropriate
unit of time,
• the amount of memory required to store the arguments on which opera-
tions are applied.
Other types of resources have recently become important in practical cases,
e.g. energy consumed by the execution of algorithms on battery-powered
devices or in very large data centers.
Since such measures of complexity depend on the underlying model of data,
operations and memory, it is to be expected that investigating computations
directly over the structures of interest opens the door for a rich computational
complexity discipline with underlying machine concepts that are relevant
for the practice of today’s computing, whether generic or domain-specific.
Practitioners typically measure concrete data and operations directly, at the
genuine level of abstraction of the investigated algorithms; to mention two
among a myriad of examples in the area of algorithms and data structures:
the number of comparisons of keys and the number of shifts of to-be-sorted
data in sorting algorithms [133] or the structural complexity of non-numeric
objects of computation (e.g. syntactically restricted forms of logical formulae
when analysing their decision problems [48]).
In terms of the ASM framework this comes rather natural: it means to take
into account the signature (data structure) and in particular the background
of algorithms or classes of machines when investigating their complexity prop-
erties. In Sect. 3.1.1 we have observed that the ASM-computability concept
is first of all a notion of relative computability where, logically speaking,
the background plays the role of an oracle so that even the impressive re-
search on degrees of unsolvability [120] can be uniformly expressed in terms
of ASMs. What is important for the practice of computing is that one can
tailor the concept of ASM-computation by appropriate constraints on the
background (for an example see the Arithmetical State Postulate in Def. 46,
pg. 203) which result in corresponding parameters for complexity measures.
We list here some typical complexity measures ASMs do support explicitly
and directly:
• cost of data access
• cost of background operations
• cost of term evaluation
• cost of computing updates (assignments) and more generally of instruc-
tions (e.g. ASM rules in terms of the size of their parse tree)
• cost of single computation steps or of tasks involving objects and actions

9.2 Complexity of Computing over Structures
239
• maximum number and size of the content of used locations in a run
• cost of runs (depending on the underlying control structure or scheduling
scheme)
• cost of refinement, measuring the complexity of macro versus micro steps
(i.e. abstract versus refined and in particular implemented steps on given
hardware)
Note that the listed parameters refer to structural features and not to their
encoding by computable numbers. It might be worth to stress that since the
cost of encoding and decoding is in itself variable, and comes with its own
complexity, when discussing the complexity of an algorithm (or problem)
with that encoding in mind it may be diﬀicult or impossible to separate
the complexity which is proper of the problem, from that which comes from
the encoding. With ASMs, and more general when computing on structures
directly, it is natural to skip the encoding altogether, so it is easier to focus
on the proper complexity of the solution.
9.2.1 Universal ASM over Fixed Signature
An interesting application of ASMs in complexity theory appeared in [17]
where a linear time hierarchy theorem is proved for (a class of) ASMs and
analogously for RAMs. The result is obtained by applying the diagonaliza-
tion technique to the runs of a universal ASM with fixed signature Σ and
fixed program. This construction is a refinement UniversalAsm(Σ) of the
UniversalAsm in Sect. 9.1.2.1. It implements the choice of a to-be-simulated
machine M (with an initial state S0) by starting with a given encoding of M
(and S0) to first construct a copy that is then used to directly read and write
the data during the stepwise simulation, without modifying the initial encod-
ing of M. In this section we explain such a simulator U = UniversalAsm(Σ)
for a class of ASMs with fixed signature Σ.
Signature. The ASMs we consider in this section are input/output control
state PGAs M (i.e. interacting with the environment only for input/output),
extended by import rules. The latter are used to build a data copy of the
to-be-simulated machines M. For simplicity of exposition the machines M
are assumed to have no static functions (except the logical ones true, false,
undef ), but they may have any finite number of dynamic functions. The sig-
nature Σ is assumed to contain in particular the following dynamic functions
that are used to construct and traverse a parse tree representation of any M
(the analogue of the standard description of a TM program on the TM tape
in Turing’s definition of universal machines, pg. 227):
• Functions parent, fstChild, nxtSibl (to build the parse tree) and tree la-
beling functions type, nomen we call PT-functions.

240
9 Complexity Analysis
• Five PT-traversal functions:
– a cursor function to denote the currently visited parse tree node,
– a function val used to assign computed results to nodes,
– two functions lastUpdNode and nxtUpdNode to construct linked lists
of visited nodes in the parse tree where some updates have been
computed that will be applied at the end of the parse tree traversal,
– a 0-ary Boolean-valued error function assumed to be initially false
and to become true when during the apply phase the simulator detects
an update inconsistency.11
• Functions ctl, 0, succ. Initially the simulator is in ctl-state 1 and finds an
encoding of the parse tree of the given program M in an initial segment
[0, n) of Nat, so that besides 0 there is the standard successor function
restricted by succ(n −1) = undef .
Architectural Component Structure. The strategy of the simulation is
to first CopyParseTree of M to provide space (the analogue of the TM
tape) for the universal machine U to record the intermediate data M needs
during the computation of its updates for a step. Each M-step (as long as
the Simulation is not Finished) is performed by the ParseTreeEvaluation
component. ParseTreeEval starts in a given simulation state S (where
ctl = SimStep) a traversal of M’s parse tree in mode ctl = down. U moves
down in depth-first manner and then moves up again, recording (where
needed) at the currently visited node cursor the computed subtree eval-
uation result as val(cursor). When eventually the cursor comes back to
the parse tree root and U enters mode ctl = apply, the third component
ApplyUpdates finds at the UPDATE nodes the values computed for the
update elements: it checks them for consistency and applies them if they are
consistent. This finishes the current simulation step and lets U enter the
computed successor state S′ of S (if defined) with mode ctl = SimStep.
The just described architectural view of U = UniversalAsm(Σ) is rigor-
ously defined by the control state ASM in Fig. 9.4 and the definition below
of its components.
Here we anticipate the definition for the termination condition and the
initialization of the linked list of update nodes. This list will be extended by
nxtUpdNode upon encountering during the parse tree traversal a node with
label type(node) = Update. The Gödelization function ordinal defined below
is used to describe the termination condition.
11 Since we explain here only the universal machine and not its diagonalization feature
that serves to prove the linear time hierarchy, we disregard the i/o-functions of the
simulated machines and the particular role assigned to the 0-ary output function out
in [17] concerning the diagonalization. A similar role is played by the error function we
introduce as element of Σ for the consistency check after the parse tree evaluation. The
input function in is also not mentioned explicitly as element of Σ, but its functionality
is present via the parse tree encoding, see below.

9.2 Complexity of Computing over Structures
241
Fig. 9.4 UniversalAsm(Σ) Component Structure
CopyParseTree
initial
FinishedSim
SimStep
final
InitUpdNode
ApplyUpdates
down
apply
ParseTreeEval
yes
no
FinishedSim iff val(ordinal(ctl)) = final
InitUpdNode = (lastUpdNode := 0)
It remains to define and explain the three components CopyParseTree
(a preprocessor that constructs the initial simulation state of U provid-
ing a copy of the parse tree where to record step-simulation data), the
ParseTreeEval to compute the updates of M in the current simulation
state, and the final application of the computed update set Upd (if consis-
tent) by ApplyUpdates. Note the precisely defined splitting of the simulator
into components which interact as expressed by the architectural design of
the main component UniversalAsm(Σ); the splitting satisfies the princi-
ple that to be satisfactorily inspectable for a correctness check, a component
(specification or piece of code) should fit a page or two.
Path Tree and CopyParseTree Preprocessor. The role of the subma-
chine CopyParseTree is to produce a copy of the (Gödelization of the)
input parse tree formed by the initial state elements in [0, n) together with
succ and the PT-functions. The elements of initial states are called original
elements of the simulation states, i.e. of those states where ctl = SimStep.
The Gödelization we describe below is done in such a way that the copies
of the first nodes succi(0) with i = 0, 1, 2, . . . , m are reserved to contain the
current values of the i-th 0-ary non-logical function nullaryi, listing that by
definition starts with succ0(0) = 0, succ1(0) = ctl = initial.
0
succ
−→1
succ
−→2 . . . succi(0) . . . (n −1)
val ↓
↓
↓
↓
↓
0′ succ
−→1′ succ
−→2′ . . . nullaryi . . . (n −1)′
This explains the definition in Fig. 9.5 with the following definition of the
macros.

242
9 Complexity Analysis
Fig. 9.5 CopyParseTree Component
initial
InitializeCursor
Copy(Universe)
FinishedCopy
Copy(Universe)
ResetCursor
FinishedCopy
ResetCursor
SimStep
Copy(ParseTree)
Copy(ParseTree)
no
yes
yes
no
InitializeCursor = ResetCursor = (cursor := 0)
FinishedCopy iff cursor = undef -- cursor reached succ(n −1) = undef
Copy(Universe) =
import node do val(cursor) := node -- clone the visited node cursor
cursor := succ(cursor)
-- visit next node
Copy(ParseTree)12 =
forall f ∈{succ, parent, fstChild, nxtSibl, type, nomen}
f (val(cursor)) := val(f (cursor))
-- copy parse tree structure
cursor := succ(cursor)
-- visit next node
The definition of the parse tree PT(M) of M by induction on M follows
the usual pattern (see the figures at the beginning of Sect. 4.3) but with
two particular labeling functions type and nomen. Fig. 9.6 shows the trees
for terms f (t1, . . . , tr) (with type FUN and nomen f for the outer function
symbol) which includes the case of 0-ary functions (r = 0), for assignment
instructions f (t1, . . . , tr) := tr+1 (with type UPDATE and nomen f for the
outer function symbol), conditional instructions if cond then N (with type
COND) and parallel instructions par M1, . . . , Mn (with type PAR).
Fig. 9.7 shows the trees for the import v do N construct and for variables.
The parse tree for a variable v is a one-node tree with type VAR and nomen
v.
To construct PT(import v do N) with variable v a new node x is intro-
duced as parent of the root of PT(N); its type is defined as IMPORT and its
12 The use of forall stands here as abbreviation for six parallel PGA assignments.

9.2 Complexity of Computing over Structures
243
Fig. 9.6 ParseTree for PGA Constructs
COND
PT(cond)
PT(N)
PAR
PT(M1)
. . .
PT(Mr)
UPDATE: f
PT(t1)
. . .
PT(tr+1)
FUN: f
PT(t1)
. . .
PT(tr)
Fig. 9.7 Parse Tree for Variables and Imports
IMPORT: x
x =
PT(N)
VAR: x
x =
update binding nomen(node) := x
wherever nomen(node) = v in PT(N)
for rule import v in N
N.B. x is the root node for
the two constructs.
nomen as x. In addition, for every occurrence of a node with nomen(node) = v
in PT(N) the binding is updated to nomen(node) = x.
Initial states I(M, Σ) of U are defined as states that satisfy the following
conditions.
• ctl = initial = succ(0),
• the regular elements (i.e. those that are in Workspace, see Sect. 2.3.2) form
an initial segment [0, n) of Nat with n ≥2 and standard interpretation of
0 and succ (including succ(n −1) = undef ) the simulator will not change,
• the regular elements together with the restriction of the PT-functions to
regular elements form a (Gödelization defined below for the) parse tree
PT(M + m) of a notationally slightly modified but equivalent program
M + m (where m is the number of non-logical 0-ary function symbols
in Σ); also this parse tree will only be read but never be changed by U
during the simulation,

244
9 Complexity Analysis
• every f ∈Σ (except 0, succ, ctl and the PT-functions) is interpreted as
undef resp. false (for every argument). This comes up to assume (without
loss of geneality) that M starts in a blank state.
The program extension M +m = par par . . . par M (m times) has the effect
to create in the parse tree m initial nodes, as illustrated by Fig. 9.8, that are
used only for recording the current value of the non-logical 0-ary functions.
Fig. 9.8 Parse Tree Extension PT(M + m)
PAR
PAR
...
m times
PAR
PT(M)
For this reason the Gödelization of PT(M + m) is defined as follows:
• Starting with 0 perform a depth-first numbering (Gödelization) of all
parse tree nodes and replace them by their Gödel number. Note that as
a consequence, in the parse tree for M + m every succi(0) for i < m is a
node of type PAR.
• Number (Gödelize) all function names f ∈Σ as fi where all non-logical
0-ary function names nullaryi come first and in particular 0 receives num-
ber 0 and ctl receives number 1. Represent fi by the term succi(0), de-
noted as ordinal(f ). Thus the initial PAR nodes in PT(M + m) can
be used to record the current value of a 0-ary non-logical function as
val(ordinal(nullaryi)) = val(succi(0)).
• To complete the Gödelization replace the labels PAR, COND, FUN, IM-
PORT, UPDATE, VAR in the type function by the respective number
0,1,...,5, replace every value f of the nomen function by ordinal(f ), and
replace the ctl-values of U by some numbers.
Component ParseTreeEval. We are now ready to explain the component
for the ParseTreeEvaluation of the universal machine U. Starting in mode
ctl = down with cursor = 0 (due to the last ResetCursor step of the
CopyParseTree component) the machine moves down in the depth-first
manner to reach a leaf. At each node of type(cursor) = IMPORT it records
a new element as val(cursor); at each node of type(cursor) = UPDATE it
extends the linked list of encountered update nodes by

9.2 Complexity of Computing over Structures
245
nxtUpdNode(lastUpdNode) := cursor
lastUpdNode := cursor
(list that had been initialized in mode SimStep when entering mode down).
When U reaches a leaf (i.e. a node without child so that fstChild(cursor) =
undef ) it switches to ctl := up.
This and the following definition of the used macros explain the moving-
down part of the ParseTreeEvaluator in Fig. 9.9.
Fig. 9.9 ParseTreeEval Component
down
RecordImpUpd
AtLeaf
ToNxtChild
up
EvalVar
EvalFun
AtFalseGuard
ToParent
SomeSibl
ToNxtSibl
AtRoot
InitUpd
apply
no
yes
yes
no
yes
no
no
yes
RecordImpUpd =
if type(cursor) = IMPORT then
import node do val(cursor) := node
-- record new node
if type(cursor) = UPDATE then
-- extend list of update nodes
nxtUpdNode(lastUpdNode) := cursor
lastUpdNode := cursor
ToNxtChild = (cursor := fstChild(cursor))
-- depth-first traversal
AtLeaf iff fstChild(cursor) = undef
-- there is no child

246
9 Complexity Analysis
As to the moving up segments of the tree traversal, in mode up the sim-
ulator visits for each node the next sibling except when it is AtFalseGuard,
i.e. at the root of the guard child of a COND node with guard evaluated to
false; in that case U can move to the parent node because is not necessary to
evaluate the body of that conditional rule. Otherwise U moves ToNxtSibling,
assuming there is some, and reenters mode ctl = down. If there is no sibling
U moves to the parent node unless it reaches the root of the parse tree. In the
latter case U enters ctl = apply (and initializes for the consistency check the
auxiliary set Upd of already applied updates). This calls the ApplyUpdate
component to check the computed updates for consistency and in the positive
case to apply them to the current M-state.
Wherever in a moving up step the cursor visits a VAR: x or a FUN: f
node, the simulator must EvalVariable respectively EvalFunction.
EvalVar finds (via nomen(cursor) in the parse tree definition for vari-
ables and import constructs in Fig. 9.7) the IMPORT node x where the cur-
rent M-value of x is recorded as val(x). Therefore one can define EvalVar
as follows:
EvalVar =
if type(cursor) = VAR then val(cursor) := val(nomen(cursor))
EvalFun is a PAR rule of (finitely many) rules for f ∈Σ, so it is a PGA
though for notational convenience we formulate it using the forall construct.
Clearly at each FUN node only the rule guarded by the ordinal number of
its nomen applies.
EvalFun =
if type(cursor) = FUN then
forall f ∈Σ
if nomen(cursor) = ordinal(f ) then Evaluatef
To define Evaluatef there are two cases, depending on whether f is 0-ary
or not. Let r = arity(f ).
Case 1: r > 0 or f is a 0-ary logic function.13 Then we define:
Evaluatef =
-- record current f -value on computed arguments
val(cursor) := f (val(child1), . . . , val(childr))
where
child1 = fstChild(cursor)
childi+1 = nxtSibl(childi)
forall i < r −1
Case 2: f is a non-logic 0-ary function. Then the current M-value of f is
kept as val(ordinal(f )) at ordinal(f ) = succi(0) for some i < m so that we
define:
13 We do not need to consider error here since it is used by the simulator only once and
only to get a defined value, namely if and when during the apply phase an inconsistent
update set is discovered.

9.2 Complexity of Computing over Structures
247
Evaluatef = (val(cursor) := val(ordinal(f ))
-- record current f -value
Together with the following definition of the other macros this explains
the moving-up part of Fig. 9.9.
ToParent = (cursor := parent(cursor))
-- go to parent node
SomeSibl iff nxtSibl(cursor) ̸= undef
-- visited node has some sibling
ToNxtSibl = (cursor := nxtSibl(cursor))
AtRoot iff cursor = 0
-- entire parse tree has been traversed
InitUpd = (Upd := ∅)
-- initialize set of applied updates
Exercise 37 (AtFalseGuard). Define the (intuitively clear but verbose)
property AtFalseGuard. Hint: reformulate part of the definition above for
Evaluatef . For a definition see the Appendix B.
Component ApplyUpdates. In ctl = apply mode the machine, starting
with cursor = 0, checks at each nxtUpdNode(cursor) in the computed list
whether the update recorded there is consistent with those already applied
in the (initially empty) set Upd. In case U detects an inconsistency it sets
error := true so that FinishedApply becomes true and the simulation stops
with ctl = final. Otherwise U will Apply the update and add it to Upd (if
it is new) so that it can go ToNxtUpdNode (if there is one) or otherwise
enter mode ctl = SimStep and reset the cursor for the next simulation step.
This explains Fig. 9.10 and its macros (where again for notational conve-
nience we use forall instead of the bounded par-notation with finally many
arguments).14 The submachine CheckApplyUpdf of CheckApplyUpd is
explained below.
Fig. 9.10 ApplyUpdates Component
apply
CheckApplyUpd
check
FinishedApply
ToNxtUpdNode
error
ResetCursor
SimStep
final
no
yes
no
yes
14 Note that when initially cursor = 0, none of the CheckApplyUpd rules fires since 0
is not an UPDATE node.

248
9 Complexity Analysis
CheckApplyUpd =
forall non-logical f ∈Σ with f ̸= error
if nomen(cursor) = ordinal(f ) then CheckApplyUpdf
FinishedApply iff nxtUpdNode(cursor) = undef or error = true
ToNxtUpdNode =
cursor := nxtUpdNode(cursor)
nxtUpdateNode(cursor) := undef
ResetCursor = (cursor := 0)
For CheckApplyUpdf we must distinguish whether the arity(f ) is posi-
tive or 0 to correctly identify the location for the computed and to-be-checked
and possibly applied update. The elements of the update have been computed
by the subtrees ti of the current node and are recorded there as val(root(ti)).
CheckApplyUpdf =
let child1 = fstChild(cursor)
-- access subtrees for upd elements
if arity(f ) > 0 then
forall 1 ≤i ≤r let childi+1 = nxtSibl(childi)
let arg = (val(child1), . . . , val(childr))
-- computed arguments
let upd = ((f , arg), val(childr+1))
-- retrieve upd elements computed by subtrees
CheckApply(upd)
if arity(f ) = 0 then
CheckApply((val, ordinal(f )), val(child1))
where
CheckApply((g, a), b) =
if ((g, a), b) ∈Upd then skip
-- same update already applied
elseif forsome c ̸= b ((g, a), c) ∈Upd then
error := true
-- two inconsistent updates detected
else
-- new update is consistent with those already made
g(a) := b
-- apply the update
Insert(((g, a), b), Upd)
-- record applied update
Analysis of Simulation as Refinement. To prove the correctness of the
simulation of runs of ASMs M with signature Σ by runs of the simulator U =
UniversalAsm(Σ) we describe the relation between the computations using
the refinement scheme of Sect. 7.2, an interesting and frequently encountered
(1, c) refinement type with a small constant c. Such simulations are called
lock-step simulation.
For every M-computation S0, S1, . . . there is an U-computation with cor-
responding states S0, S1, . . . of interest such that the M-locations in Si (the
locations of interest) have the same value as their corresponding U-locations.
We call these corresponding states of interest Si simulation states (where
ctl = SimStep). The first one of them is reached by the preprocessing U-
computation segment by which CopyParseTree leads from U’s initial state
I(M, Σ) described above to the first state with ctl = SimStep, the state S0.

9.2 Complexity of Computing over Structures
249
For S0 and S0 the correspondence between the locations of interest (initially
the blank M-locations and their counterpart in U) holds by definition of
I and the fact that the preprocessor CopyParseTree does not touch the
M-locations.
For each M-step leading from Si to Si+1 U starts the three-phase-
simulation of this step in Si with cursor = 0. In a first phase it moves
down from the parse tree root to the leafs, recording at each IMPORT: x
node the newly imported element and extending at each UPDATE: f node
the list of encountered UPDATE nodes (that is needed for the application of
the computed updates performed at the end of this simulation round by the
ApplyUpdates component). We visualize these actions as follows:
IMPORT : x
val
−→
import new do val(cursor) := new
UPDATE : f
val
−→
nxtUpdNode(lastUpdNode) := cursor
lastUpdNode := cursor
In a second phase U moves up from the leafs to the root of the parse
tree visiting however sibling nodes it encounters (from where it may again
move down to the leafs and then return up again). In the ascending phase U
records at each VAR: x node the Si-value of x (where by the binding defined
by IMPORT nodes in the parse tree, x denotes an IMPORT: x node). At each
FUN: f node it records the Si-value of f applied to the Si-values computed
at the node’s children. We visualize these actions as follows:
VAR : x
val
−→
val(cursor) := val(nomen(cursor))
FUN : f
val
−→
val(cursor) :=
(
val(ordinal(f ))
if f is 0-ary
f (val(children))
otherwise
In phase 3 by the ApplyUpdates component U traverses the list of the
visited updates nodes to check at each node of the list whether the update
recorded there is consistent with the already applied updates (those in the
set Upd initialized by the empty set) and in the positive case to apply that
update, thus updating the concerned M-locations to their value in state Si+1;
in case of an encountered inconsistency U stops its computation.
The reader will have observed that this rather typical ASM refinement
scheme not only allows one to prove runtime properties by induction on runs
(here of M and its simulation by U), but the component structure of the
refined machine (here of U) permits to split the major proof steps into a
series of lemmas, one per component, fighting this way complexity (of both
design and verification).
Remark on completeness. In the presentation above we have closely
followed the approach presented in [17], where the theorem establishing the
linear time hierarchy was first stated and proved. While that approach suﬀices
to prove the theorem, some aspect of the construction might appear rather

250
9 Complexity Analysis
artificial (e.g. creating additional PAR nodes in the syntax tree to store parts
of the state, or the somewhat contrived treatment of VAR and IMPORT).
Moreover, the construction only considers the most basic ASM constructs.
For a complete model of an ASM interpreter with variable signature, which
more directly express the operations of an ASM run by means of appropriate
structures, rather than by making recourse to unnatural encoding, the reader
may refer to [75] where also non-determinism, partial updates and arbitrary
backgrounds are allowed. The model of the interpreter in [75], which is itself
specified as an ASM, constitutes a full specification of the universal ASM
concept.
9.2.2 Look-Compute-Move Algorithms
Distributed systems of mobile agents that operate in complex topological
environments provide an interesting example for modern complexity investi-
gations of computations over structures. In this section we explain the basic
concepts of one such example, surveyed in [80]: the class of Look-Compute-
Move (LCM) algorithms which operate in a spatial universe—a graph (a
discrete version) or an Euclidean space (a continuous version)—using var-
ious resources and forms of communication and interaction that lead to a
variety of models of computation with rich structural complexity properties.
9.2.2.1 Signature of LCM algorithms
An LCM algorithm is a pair (Ag, LcmPgm) of a team Ag of finitely many
agents (also called robots) and a program LcmPgm each agent executes. The
structure where an LcmPgm operates is typically either a finite connected
undirected graph G (with a universe V of vertices and a set E of edges) or an
Euclidean space (with a universe U ⊆Realn for some finite dimension n). At
each moment every agent is positioned in a point pos of the universe (called
its current position). In rounds, based on an observation of its current envi-
ronment, each agent computes a destination point and then (if dest ̸= pos)
moves towards this destination—each time the agent is activated to execute
such an LCM round (also called LCM cycle). The LCM control structure is
similar to the one for streaming systems (see Fig. 6.2, pg. 156).
The LcmPgm describes the sequential execution of the three steps Look,
Compute and Move an agent performs in the corresponding mode ∈
{look, compute, move} once it has been activated for an LCM round. Ac-
tivation of agents to make them run requires some Start mechanism we
formulate using an additional mode value sleep. LCM algorithms come with
a variety of notions of run we analyze in more detail in Sect. 9.2.2.3. In [80,
pg. 4-7] they are formulated in terms of scheduling policies where agents are

9.2 Complexity of Computing over Structures
251
activated by a scheduler (expressed as setting Active for the considered agent
to true). We reflect this by a Start step that wakes up (read: calls) the
activated agent to sequentially execute its three LCM cycle steps.15
At this point we can define LcmPgm by the following abstract scheme of
four round components that are guarded by their mode value so that they
are executed in sequential steps (see below). They are open to refinements by
adding details characterizing various classes of LCM algorithms we explain
below.
LcmPgm =
Start
Look
Compute
Move
To equip each agent with its own separate state we use the ambient nota-
tion (see Sect. 5.4.1) declaring the agent-sensitive locations (like pos, Active,
mode, etc.) as implicitly instantiated per agent. Thus each agent is equipped
with the following program:
pgm(a) = amb a in LcmPgm
9.2.2.2 Discrete LCM Algorithms over Graphs (LcmGraphPgm)
To illustrate in more detail the four LcmPgm components we consider the
case of graph algorithms where the robots interact only through (global or
local) shared memory or by face-to-face communication (of pairs of robots po-
sitioned on the same node and both reading the partner’s memory) or by wire-
less communication and can move only to neighbouring nodes of their current
position. We focus the attention on algorithms with local shared memory that
is accessible for agents on a mutual exclusion basis, say as nodeInfo(pos) at
their current position where they can read, record and update the informa-
tion. We denote this LcmPgm refinement by LcmGraphPgm.
To abstract from the details of mutual exclusion we let the Start com-
ponent RequestAccess(nodeInfo(pos)) and guard the Look component
by a HasAccessTo(nodeInfo(pos)) predicate. Robots also may use some own
workMemory (that is ReSet at the beginning of a new round) and have some
privateMemory that is persistent (i.e. it does not change when mode = sleep).
15 Concerning scheduling it is assumed (see [139]) that once the execution of an LCM-
cycle by an agent is started (triggered by an activation of the agent) its three program
components Look, Compute and Move will immediately be executed and their exe-
cutions will terminate regularly, except in the continuous model where moves of agents
(differently from the graph model) are not atomic and a mobility scheduler may stop
a moving agent. But it is the scheduler’s role to deactivate an agent (by an update
Active(ag) := false) where desired.

252
9 Complexity Analysis
This explains in more detail the role of the Start and Look component for
LCM graph algorithms. When an agent becomes Active it Starts one LCM
cycle: it requests a mutual exclusion access to the local memory nodeInfo(pos)
that is available at its current position, refreshes its work memory and goes
to mode = look (where by the fairness assumption it will eventually receive
access to the local nodeInformation).
Start =
if Active and mode = sleep then
ReSet(workMem)
-- clean working memory
RequestAccess(nodeInfo(pos))
-- require exclusive access
mode := look
-- wake up the agent
In look mode a robot, when it HasAccessTo the local memory nodeInfo
at its current position, takes a snapshot of it, updates it, Releases the lock
and enters the compute mode.
Look =
if mode = look then
if HasAccessTo(nodeInfo(pos)) then
snapshot := nodeInfo(pos)
-- record current observation
WriteTo(nodeInfo(pos))
-- interact with env
Release(nodeInfo(pos))
-- release lock
mode := compute
With LCM algorithms working in an Euclidean space the observation in look
mode of the environment does not manage any data (so that the Start rule
contains no RequestAccess) but only takes a snapshot of the positions of
those other robots it can observe (determined in terms of its specific local co-
ordinate system with origin = pos) within its visibility range. This operation
is considered as instantaneous.
Details on local memory. A frequently used refinement of the local
memory function nodeInfo is to a whiteboard with mutual-exclusion constraint
for reads and writes on this whiteboard by any agent visiting the node in
question. Analogously, nodeInfo can be refined to communication via a token-
count in the mutual-exclusion-constrained Petri net fashion: nodes and agents
can hold tokens; when an agent visits a node it records in info the number
of tokens found on the node and may change nodeInfo by adding some of its
own tokens to those found on the node and/or vice versa by picking up some
of the tokens from the node.
Exercise 38 (LCM with face-to-face communication). Refine Look
for face-to-face communication. See the Appendix B.
The Compute component executes a Protocol we do not specify further
here. It is assumed to be the same for each agent but it is called with the

9.2 Complexity of Computing over Structures
253
agent’s current snapshot value16 and calculates in particular whether and
where to move to in the next step. We assume the result of that computation
to be recorded in a location dest; in the graph model it is either a neighbouring
node of pos in the graph—i.e. the destination for the next move to perform—
or undef , indicating that the decision is to not move. In the Euclidean model
the result may include also a trajectory to the destination and the robots are
capable to perform infinite precision real arithmetic operations.
Compute =
if mode = compute then
Protocol(snapshot)
-- compute dest where to move to
mode := move
In the graph model, differently from an Euclidean model, “The Move
operation is considered instantaneous” [80, pg. 12] so that it can be de-
scribed by an update of the agent’s position to the computed destination—if
the Protocol decided to request a move from pos to dest. If however the
Protocol has computed dest = undef the agent remains in its current
position. In both cases the agent is set to mode = sleep (cycle termination)
so that it becomes ready to be activated for a next LCM cycle. In general,
for backtracking purposes an agent records the information on the port from
which it came (described by labeling the edges incident to a node).
Move =
if mode = move then
if dest ∈V then
pos := dest
-- perform the move to vertex dest
RecordBacktrack(Info(pos, dest))
elseif dest =undef then skip
-- stand still
mode := sleep
-- terminate the LCM cycle
Remark on termination. “At the end of a cycle a robot may either start
a new cycle or become inactive (sleep)” [80, pg. 9], but “The choice of which
robots are activated ... is assumed to be made by the activation scheduler”
[80, pg. 6]. This is the reason why in the graph model at the end of a cycle
we let the agent go to sleep (from where it may Start immediately a new
LCM cycle if it is still (or again) Active).17
Note that in an Euclidean model the Move rule is more complex. There
the robots can turn and move in any direction and use these motorial capabil-
ities to move towards the computed destination (possibly along a computed
trajectory). Furthermore, a mobility scheduler controls the move speed, may
16 Obviously Protocol is assumed to be designed such that two executions by different
agents generate no inconsistency.
17 If one wants to allow that the agent can try to deactivate itself it suﬀices to add a
partial update (see Def. 33) Active :=schedPriority false where to resolve a possible update
conflict the action schedPriority gives priority to the scheduler’s update of the location
Active.

254
9 Complexity Analysis
impose for moves a maximal length γ if the distance to the destination is
greater than this constant γ and may interrupt the move. Therefore, instead
of updating the computed destination the mode is updated for the move phase
to motile (“not necessarily moving, but capable of moving” [111, pg. 9]) with
an additional rule for the termination of the move phase we do not formalize
here.
9.2.2.3 LCM Runs and Complexity
In applications, for each robot each round, each component computation and
each inactivity segment (where mode = sleep) may have its particular dura-
tion (and in case of an Euclidean structure also its rate of motion in every
move phase). Any constraint made on their timing can affect the compu-
tational capabilities of LCM algorithms. In this section we use the control
structures explained in Ch. 3–5 to describe the major notions of run that
are used for LCM graph algorithms, essentially variations of synchronous or
asynchronous protocol executions.
Synchronous LCM runs come with a specific robot activation schedule
and a specific timing of the robot components. Formally these runs are se-
quences R1, R2, . . . of rounds; in each round R the activated agents execute
one by one each of their sequential components in parallel (simultaneously)
Fig. 9.11 LcmSyncRound simulator for LcmGraphPgm
start
syncround
ScheduleRobots
start
stage
SyncPar(Robots)
SyncPar(Robots)
look
stage
StillLooking
compute
stage
SyncPar(Robots)
move
stage
ParInterleav(LookingRobots)
yes
no

9.2 Complexity of Computing over Structures
255
and terminate their activity at the end of the round. The role of the activation
scheduler to determine in a fair way18 for each round the team of Active agents
becomes explicit in the synchronous LCM run simulator LcmSyncRound
defined by Fig. 9.11 for LCM algorithms with LcmGraphPgm (where each
component can be viewed as an atomic action).
LcmSyncRound is a single-agent sequential ASM. In each round it first
schedules a set Robots of pgm(a) (for some agents a ∈Ag that form the
RoundTeam). Then in its start stage the simulator executes simultaneously
(in parallel) one step for every pgm(a) ∈Robots, using for the synchronization
the SyncPar module defined on pg. 68, and proceeds to its look stage. As-
suming that initially all agents are in mode = sleep, after the first simulation
step the RoundTeam agents are all in mode = look. In the look stage where
(due to the mutual exclusion scheme for the local communication) in each
moment at each node at most one team member can make a step, instead
of SyncPar the parallel interleaving paradigm ParInterleav(Process) is
applied (see pg. 69) with ReadyToExec(pgm(a)) meaning that the robot a
HasAccessTo(nodeInfo(pos(a))). Assuming a fair mutual exclusion, eventu-
ally there will be no more RoundTeam agent with mode(a) = look so that
also the simulator enters its compute stage. In that stage the simulator ex-
ecutes again simultaneously for every pgm(a) ∈Robots one step, the same
in its move stage, namely the Compute respectively the Move component
of pgm(a), applying again the SyncPar operator to Robots. This terminates
the round. The macros for Fig. 9.11 are defined as follows.
ScheduleRobots =
choose A ⊆Ag19
-- select round team
forall a ∈A Active(a) := true
RoundTeam := A
Robots := {pgm(a) | a ∈A}
LookingRobots = {pgm(a) | a ∈RoundTeam and mode(a) = look}
StillLooking iff LookingRobots ̸= ∅
ReadyToExec(pgm(a)) iff HasAccessTo(nodeInfo(pos(a)))
The definition in Fig. 9.11 satisfies the requirement that only at the end of a
round execution a new round can be started. The two well-known border cases
of synchronous runs are where the activation scheduler selects for each round
the RoundTeam of all agents respectively for each round a RoundTeam of
only one agent, called in [80, pg. 6] fully synchronous respectively centralized
LCM activation model.
Asynchronous LCM runs. For LCM graph algorithms with atomic se-
quentially executed components one can specify an asynchronous execution
model by the notion of concurrent ASM runs (see Definition 36, pg. 112).
18 Fairness is meant as usual that in an infinite run every robot is activated infinitely
many times.
19 This stands for choose A in P(Ag)

256
9 Complexity Analysis
To show this we define a concurrent run simulator LcmAsyncRound for
LcmGraphPgms which in each step activates a (constrained to be fairly
selected) set of robots for one round execution and at the same time executes
for some already activated robots their current component (as determined by
their current mode).20 In the concurrent ASM run each simulated component
step can be interpreted as a GlobalStep (see Fig. B.1, pg. 266).21 Initially
all robots are assumed to be in mode = sleep and not Active.
LcmAsyncRound =
-- simulator for LcmGraphPgm
ActivateSomeRobots
ExecSomeActivatedRobots
where
ActivateSomeRobots =
choose A ⊆SleepingNotActiveAgents
forall a ∈A Active(a) := true
ExecSomeActivatedRobots =
choose 22A ⊆{a ∈Ag | Active(a)}
forall a ∈A do pgm(a)
where
SleepingNotActiveAgents =
{ag ∈Ag | mode(ag) = sleep and not Active(ag)}
In this way every robot can be activated independently of other robots
and the executions of components by different activated robots may happen
at arbitrary (same or different) moments.
Exercise 39 (Eager LCM Round Execution). If one wants the compo-
nents of every activated robot to be executed without delay (as is expressed
in [139], see footnote at the end of Sect. 9.2.2.1) it suﬀices to constrain the
selection in ExecSomeActivatedRobots by stipulating that if in a con-
current run a robot is activated at time t by ActivateSomeRobots, then
its program instance will be selected by ExecSomeActivatedRobots in
the next four steps of the concurrent run to execute the robot’s components.
Show that the resulting LcmAsyncRound program is equivalent to the fol-
lowing scheduler:
ActivateSomeRobots
SyncPar({pgm(a) | a ∈Ag and Active(a)})
Bounded asynchronous LCM runs. In LCM algorithms working in
an Euclidean space the intrinsically durative Move component requires an
AsyncLcm run concept where robots can be activated independently and
20 For simplicity but without loss of generality we assume here that Active agents remain
Active at least until their current round is terminated.
21 A durative component version can be captured by a sequence of LocalSteps.
22 This stands for choose A in P({a ∈Ag | Active(a)}).

9.2 Complexity of Computing over Structures
257
the duration of their component executions and inactivity periods is finite
but arbitrary and may even change from round to round and may differ from
robot to robot. As a consequence the activity rounds of different robots may
overlap arbitrarily; for example during one robot’s round another robot could
execute any finite number of rounds. This leads to the interesting subclass of
bounded asynchronous LCM runs: a k −AsyncLcm run is an AsyncLcm
run where during the time interval of any round execution by any robot r
every other robot r′ can be activated at most k times. In [111] it is shown
that LCM algorithms running under k −AsyncLcm scheduling can solve
certain problems LCM algorithms executed under unbounded AsyncLcm
scheduling cannot solve.
The coordination of teams of agents is but one example of interesting com-
putational complexity parameters which come into view when investigating
computations over structures instead of numerical computations. For LCM
structures various intensively investigated parameters to measure the com-
plexity of LCM algorithms concern background functions and operations, for
Euclidean spaces for example the local coordinate system of robots, visibility
features (visibility range, visibility obstructions, multiplicity detection, colli-
sion handling), forms of measurement inaccuracy (when computing distances
or angles), motorial capabilities (mobility scheduling), etc. (see [80, 111]).

Appendix A
ASM Behaviour in a Nutshell
Definition of: N generates (U, η′) (in A with parameters ζ, η)
• Assignment rule: N = (f (t1, . . . , tn) := t0). N generates (U, η) with
update set U = {((f , (t1, . . . , tn)), t0)} and ti = eval(ti, A, ζ)
• Import rule: N = import x do M. N generates some (Uα, η′)
– if M generates it in A with parameters ζ[x 7→α] and η ∪{α}
· for an arbitrary element α ∈Heap \(Workspace(A)∪range(ζ)∪η)
• Conditional rule: N = if Cond then M.
– Case 1. N generates (∅, η) if eval(Cond, A, ζ) = false
– Case 2. otherwise, N generates some (U, η′) if M does it
• Parallel rule: N = par (M1, M2). N generates (U1 ∪U2, η2) if
– M1 generates some (U1, η1) and
– M2 generates some (U2, η2) in A with parameters ζ, η1
• Let range(x, φ, A, ζ) be {a ∈Workspace(A) | eval(φ, A, ζ[x 7→a]) = true}
– Choose rule: N = choose x with φ do M
· Case 1: N generates (∅, η) if range(x, φ, A, ζ) = ∅
· Case 2: otherwise N generates some (Ua, η′) if M does it with
ζ[x 7→a], η for any a ∈range(x, φ, A, ζ)
– Forall rule: N = forall x with φ do M
· Case 1: N generates (∅, η) if range(x, φ, A, ζ) = ∅
· Case 2: otherwise N generates (U, η′) if the following holds:
{a1, a2, . . .} is any enumeration of range(x, φ, A, ζ)
M generates (Un, ηn) with parameters ζ[x 7→an], ηn−1
for every an with some (Un, ηn) and η0 = η
U = S
n Un and η′ = S
n ηn
• Let rule: N = (let x = t in M). N generates some (U, η′)
– if M does it with parameters ζ[x 7→eval(t, A, ζ)], η
259
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
E. Börger, V. Gervasi, Structures of Computing, 
https://doi.org/10.1007/978-3-031-54358-6 

260
A ASM Behaviour in a Nutshell
In this list we use for single-agent ASMs the expression
M generates a pair (U, η′)
(in A with parameters ζ, η) to stand for evalCand(M, A, ζ, η, (U, η′)) = true
used in the book. The wording should help to support a quick but correct
intuitive understanding of the procedure M performs to compute for every
given state A with initially empty parameters ζ, η every update set candidate
(each of which—if consistent—can then be used to transform the given A into
a possible successor state).
Technically this means the following. Denote by UpdSets(M, A) the set of
update sets U = Upd(M, A) in pairs (U, η′) generated by M in A started
with empty parameters ζ and η. The definition of
M can make a move from state A to B
from pg. 32 reads now that there is a consistent update set U ∈UpdSets(M, A)
so that A+U is one of the possibly multiple successor states B. The wording
of the corresponding extended definition of single-agent ASM runs (Def. 22,
pg. 40) remains unchanged.

Appendix B
Solutions of Selected Exercises
4
Solution of exercise 4 from pg. 27. 2-dimensional Turing Machine
interpreter. Just replace Nat by Plane (Integer × Integer), correspondingly
replace tape by plane : Plane →Alphabet (so that the head positions become
pairs (x, y) ∈Plane) and extend the allowable moveDirections by ‘up’ and
‘down’:
moveDir(ctl, tape(head)) ∈{right, left, up, down}
The rule TuringMachine defined in the main text remains unchanged
(modulo the new interpretation of Square)
2-DimensionalTM(write, nextCtl, move) =
plane(head) := write(ctl, plane(head))
-- tape replaced by plane
ctl := nextCtl(ctl, plane(head))
shift head to moveDir(ctl, plane(head))
-- four move directions
where
let head = (x, y) in
shift head to right = (head := (x + 1, y)
shift head to left = (head := (x −1, y)
shift head to up = (head := (x, y + 1)
shift head to down = (head := (x, y −1)
5
Solution of exercise 5 from pg. 31. Register Machine interpreter. Use
indexed registers rj as dynamic 0-ary functions (with values in Nat).
RM(reg, oper, nextCtl) =
let i = ctl, j = reg(ctl), op = oper(ctl), l = nextCtl(ctl)
if op = test then
if rj = 0 then ctl := l else ctl := i + 1
if op ̸= test then
rj := op(rj)
ctl := i + 1
261
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
E. Börger, V. Gervasi, Structures of Computing, 
https://doi.org/10.1007/978-3-031-54358-6 

262
B Solutions of Selected Exercises
Since a register machine is supposed to work on natural numbers it uses the
modified subtraction operation where 0 −n is defined as 0.
The three parameters reg, oper, nextCtl (here static functions of control
states) define the pgm of register machine instructions. They determine for
each instruction label i (e.g. i ∈Nat) a register (which we denote by rreg(i)),
an operation to be performed on that register, and a label of the next to-be-
executed instruction (which may depend on the current content of register
rreg(i)).
This formulation of the RM-rule is intended to make it explicit that vari-
ations of RM-machines can be defined depending a) on the register data
(natural numbers, real numbers, bit-sequences, words, trees, etc.) and the
corresponding set of operations on register contents and b) on the way to ac-
cess registers, whether statically or dynamically. The above RM-interpreter
has finitely many registers that are provided by a static function r(reg(ctl)).
To determine a register dynamically one can define its index k as the content
of some register rj, writing rk where k = content(rj) (assuming that indices
are natural numbers). This is called indirect register addressing and is sup-
ported by the view of registers as 0-ary dynamic functions. A widely used
example is provided by the Random Access Machine model (see Exercise 14,
pg. 59).
8
Solution of exercise 8 from pg. 41. Interactive Turing Machine inter-
preter. Introduce a monitored location envInput and an additional program
function output to write on an output location outputToEnv read by the
environment.
TuringMachine(write, nextCtl, move, envInput) =
in(head) := write(ctl, in(head), envInput)
-- param for envInput
head := move(ctl, in(head), envInput)(head)
ctl := nextCtl(ctl, in(head), envInput)
outputToEnv := output(ctl, in(head), envInput)
where
move(ctl, in(head), envInput) ∈{right-move, left-move, no-move}

B Solutions of Selected Exercises
263
11
Solution of exercise 11 from pg. 55. Understanding of while.
while cond do skip
If cond is false, the while terminates immediately
with success. If cond is true, then the skip is ex-
ecuted, which produces an empty update set and
again the while terminates with success.
while false do M
Since the condition is false, the while terminates
immediately with success.
while true do a := a
Diverges, since the update set produced by the as-
signment is never empty nor inconsistent.
while true do a := 1
a := 2
Terminates (with failure) since the two updates pro-
duced by the assignments are mutually inconsistent.
13
Hint for exercise 13 from pg. 58. PGA programs for structured
machines. Use control-state ASMs. For M1 seq M2 the PGA M defines a
(1,2)-refinement (see Fig. 7.1). Assume M1 and M2 to have suﬀiciently disjoint
signatures.
M =
-- triggered by mode = startM
if mode = startM then
M1
mode := startM2
-- switch to M2
if mode = startM2 then
M2
mode := stopM
-- finished executing seq
For while cond do N the PGA M defines a (1, ∗)-refinement where ∗is
determined by the length of the simulating computation.
M =
-- triggered by mode = startM
if mode = startM and cond then
-- cond is true, proceed to N
mode := startN
if mode = startM and not cond then
-- cond is false, stop
mode := stopM
if mode = startN then
-- execute N and repeat
N
mode := startM
14
Solution of exercise 14 from pg. 59. Interpreter for Random Ac-
cess Machine programs. Let pgm be the given program of instructions
instr(l), use ctl as instruction counter (initialized by ctl = 0) with the ab-
breviation currInstr = instr(ctl). For the arithmetical operations denote by
meaning(opCode) the corresponding arithmetical function (where for division
x/y is cut off to its integer part, i.e. the greatest integer equal to or less than
x/y). Represent registers rn as 0-ary dynamic functions.

264
B Solutions of Selected Exercises
RamInterpreter =
let cmd = opCode(currInstr), arg = operand(currInstr) in
if cmd ∈{Load, Store, Add, Sub, Mult, Div}
-- check failures
and WrongAddr(arg) then Halt(WrongAddr)
if cmd = DIV and operandVal(arg) = 0 then Halt(DivBy0)
else
-- regular behaviour
if cmd = Load then acc := operandVal(arg)
if cmd = Store then
if arg = reg n then rn := acc
if arg = addr n let k = rn in rk := acc
if cmd ∈{Add, Sub, Mult, Div} then
acc := meaning(cmd)(acc, operandVal(arg))
if cmd = Jump or
-- Jump instructions
(cmd = Jump(> 0) and acc > 0) or
(cmd = Jump(= 0) and acc = 0)
then ctl := arg
-- Jump to label arg
if cmd ̸∈{Jump, Jump(> 0), Jump(= 0), Halt}
or (cmd = Jump(> 0) and not acc > 0)
or (cmd = Jump(= 0) and not acc = 0)
then ctl := next(ctl)
-- Go to next instruction
if cmd = Read then
if arg = reg n then rn := inTape(headIn)
if arg = addr n then let k = rn in rk := inTape(headIn)
Move(headIn, inTape)
-- move to next square
if cmd = Write then
outTape(headOut) := operandVal(arg)
Move(headOut, outTape)
-- move to next square
where
currInstr = instr(ctl)
WrongAddr(arg) iff
arg = addr n and rn < 0
Halt(Reason) =
-- Reason ∈{WrongAddr, DivBy0}
ctl := halt
-- enter control state halt = 1
ReportError(Reason)
next(l) = l + 1
-- compute the label of the next instruction
meaning(Add) = +
meaning(Sub) = −
meaning(Mult) = ×
meaning(Div) = /
-- modified to yield integers
operandVal(int i) = i
operandVal(reg n) = rn
operandVal(addr n) =
(
undef
if rn < 0
rrn
else

B Solutions of Selected Exercises
265
For uniformity of notation we consider Labels as operands of jump instruc-
tions. If the machine reaches ctl = halt, by definition the command of
instr(halt) is Halt, but the RamInterpreter has no rule with cmd = Halt
so that the machine halts and the RamInterpreter generates the empty
update set.
18
Solution of exercise 18 from pg. 85. Backtracking Scheme. Let a set
M of machines be given with a function alternatives(M) that assigns to
machines M a subset of alternative machines in M. Unexplained macros are
identical to those of PureProlog.
BacktrackScheme =
LayOutAlts
TryNextAlt
AltTermination
The component machines are defined as follows:
LayOutAlts =
if mode = layoutAlts and OK then
ExtendTreeBy node1, . . . , nodem with
-- machine alternatives
forall i = 1, . . . , m pgm(nodei) := Mi
-- one node per to-be-tried-out machine alternative
mode := tryNextAlt
-- switch to try the alternatives
where
[M1, . . . , Mm] = alternatives(pgm(currnode))
TryNextAlt =
if mode = tryNextAlt and OK then
if cands(currnode) = [ ] then
-- no candidate left
Backtrack
else
TryNxtCand
TryNxtCand =
currnode := ﬁrst(cands(currnode))
mode := execute
-- start next machine
cands(currnode) := rest(cands(currnode))
-- update candidates
AltTermination =
-- upon termination of a machine
if mode = successful then
stop := success
if mode = unsuccessful then
Backtrack
mode := tryNextAlt

266
B Solutions of Selected Exercises
19
Solution of exercise 19 from pg. 85. Prolog rule asking for one more
solution.
NextSolution =
if stop = success and AnotherSolutionWanted then
Backtrack
stop := 0
AnotherSolutionWanted is a monitored 0-ary predicate, typically representing
a user request for an alternative solution.
22
Solution of exercise 22 from pg. 113.
Sm+1 = Sm +
[
a∈GoGlobal
Ua
where Ua is an update set candidate generated by agent a in this run in state
Sm.
24
Hint for exercise 24 from pg. 116. Concurrent Step with Local Com-
putation Segments. The program is defined by Fig. B.1 below.
Fig. B.1 ConcurStep with Local Computation Segments
global
choose
GlobalStep
CopyInteractData
WriteBack
local
choose
wb
LocalStep
26
Solution of exercise 26 from pg. 158. Ordered Selection Patterns.
Denote by Pi,1, . . . , Pi,si the given static selection pattern order. Denote by
Actioni,s the body of the Readi component in the main text:
Actioni,s =
forall k ∈Pi,s do inputi,k := read from ini,k
actPatterni := Pi,s
Then refine Readi to the following rule:

B Solutions of Selected Exercises
267
Readi =
if IsReadable(Pi,1) then Actioni,1
elseif
...
elseif IsReadable(Pi,si) then Actioni,si
33
Solution of exercise 33 from pg. 228. Universal ASM for nondeter-
ministic ASMs. It suﬀices to replace in UniversalAsm the line
let U = eval(simPgm, simState)
by the line
choose U ∈UpdSet with evalCand(simPgm, simState, U)
34
Solution of exercise 34 from pg. 229. Stop Criterion. Modify the given
i/o-pgm to a program that generates in every state an update (e.g. an initially
undefined signal to still be active) unless out is defined. Then pgm reaches a
stop state S in terms of outS ̸= undef if and only if the program
while out = undef do
pgm
stillActive := 1
reaches a stop state S in terms of generating in S an empty update set.
37
Solution of exercise 37 from pg. 247. Definition of AtFalseGuard.
Compare the description below with the definition of Evaluatef in the
ParseTreeEval component (pg. 246).
AtFalseGuard iff
cursor = fstChild(parent(cursor)) and
type(cursor) = FUN and type(parent(cursor)) = COND and
let f = nomen(cursor), r = arity(f )
((r > 0 or f is a nullary logic function )
and f (val(child1), . . . , val(childr)) = false)
or (f is a non-logic nullary function
and val(ordinal(f )) = false)
where
child1 = fstChild(cursor)
childi+1 = nxtSibl(childi)
forall i < r −1
38
Hint for exercise 38 from pg. 252. Denote by robotInfo(a) the informa-
tion an agent obtains from any other agents a at the same node pos(ag). This
notation implies that in face-to-face communication an agent can see and dis-
tinguish the other agents a that currently have the same pos(a) = pos(ag).

268
B Solutions of Selected Exercises
Look =
if mode = look then
snapshot := {(a, robotInfo(a)) | a ∈Ag and pos(a) = pos(self)}
mode := compute
Obviously for face-to-face communication there is no memory access request
in the Start component.

References
1. Aanderaa, S.: On the decision problem for formulas in which all disjunctions are
binary. In: 2nd Scandinavian Logic Colloquium Symp., pp. 1–18 (1971) 232
2. Abrial, J.R.: The B-Book. Cambridge University Press, Cambridge (1996) 196
3. Abrial, J.R.: Modeling in Event-B: System and Software Engineering. Cambridge
University Press, Cambridge (2010) 196
4. Ackermann, W.: Begründung des “tertium non datur” mittels der Hilbertschen
Theorie der Widerspruchsfreiheit. Mathematische Annalen 93, 1–36 (1924) 60
5. Agha, G.: Actors: a model of concurrent computation in distributed systems. MIT
Press (1986) 118
6. Aho, A.V., Hopcroft, J.E., Ullman, J.D.: The design and analysis of computer
algorithms. Addison-Wesley (1974) 101, 105, 107, 236
7. ASM method: tool support. https://abz-conf.org/method/asm/ (2023) 188
8. Avigad, J., Zach, T.: The Epsilon Calculus.
In: E.N. Zalta (ed.) The Stanford
Encyclopedia of Philosophy, Fall 2020 edn. Metaphysics Research Lab, Stanford
University (2020) 60
9. Barnocchi, D.: L’“evidenza” nell’assiomatica aristotelica.
Proteus 5, 133–144
(1971) 188
10. Barros, A., Börger, E.: A compositional framework for service interaction patterns
and communication flows. In: K.K. Lau, R. Banach (eds.) Formal Methods and
Software Engineering. Proc. 7th International Conference on Formal Engineering
Methods (ICFEM 2005), LNCS, vol. 3785, pp. 5–35. Springer (2005) 131
11. Barwise, J., Etchemendy, J.: Tarski’s World. CSLI, Stanford (CA) (1993). See
2008 edition by D. Barker-Plummer and J. Barwise and J. Etchemendy 11
12. Barwise, J., Etchemendy, J.: Turing’s World 3.0 for Windows. Cambridge Univer-
sity Press (2001) 236
13. Batory, D.: Science of Software Product Lines (in preparation). ACM publishers
(2025) 160
14. Batory, D., Börger, E.: Modularizing theorems for software product lines: The
Jbook case study. J. Universal Computer Science 14(12), 2059–2082 (2008). URL
http://www.jucs.org/jucs_14_12/modularizing_theorems_for_software. Ex-
tended abstract “Coupling Design and Verification in Software Product Lines” of
FoIKS 2008 Keynote in: S. Hartmann and G. Kern-Isberner (Eds): FoIKS 2008
(Proc. of The Fifth International Symposium on Foundations of Information and
Knowledge Systems), Springer LNCS 4932, p.1–4, 2008 160, 187, 191
15. Beierle, C., Börger, E.: Refinement of a typed WAM extension by polymorphic
order-sorted types. Formal Aspects of Computing 8(5), 539–564 (1996) 192
16. Beierle, C., Börger, E.: Specification and correctness proof of a WAM extension
with abstract type constraints. Formal Aspects of Computing 8(4), 428–462 (1996)
192
269
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
E. Börger, V. Gervasi, Structures of Computing, 
https://doi.org/10.1007/978-3-031-54358-6 

270
References
17. Blass, A., Gurevich, Y.: The linear time hierarchy theorems for Abstract State
Machines. J. Universal Computer Science 3(4), 247–278 (1997) 239, 240, 249
18. Blass, A., Gurevich, Y.: The logic of choice. J. Symbolic Logic 65(3), 1264–1310
(2000) 237
19. Blass, A., Gurevich, Y.: New zero-one law and strong extension axioms.
Bull.
EATCS 72, 103–122 (2000) 237
20. Blass, A., Gurevich, Y.: Algorithms vs. machines. Bull. EATCS 74, 96–118 (2002).
205
21. Blass, A., Gurevich, Y., Shelah, S.: Choiceless polynomial time. Annals of Pure
and Applied Logic 100, 141–187 (1999) 237
22. Blass, A., Gurevich, Y., Shelah, S.: On polynomial time computation over un-
ordered structures. J. Symbolic Logic 67(3), 1093–1125 (2001) 237
23. Bodenmüller, S., Schellhorn, G., Bitterlich, M., Reif, W.: Flashic: Modular verifica-
tion of a concurrent and crash-safe flash file system. In: A. Raschke, E. Riccobene,
K.D. Schewe (eds.) Börger Festschrift, vol. LNCS 12750, pp. 239–265. Springer
(2021) 191
24. Bolognesi, T., Brinksma, E.: Introduction to the ISO specification language LO-
TOS. Computer Networks and ISDN Systems 14(1), 25–59 (1987) 159
25. Börger, E.: Reduktionstypen in Krom- und Hornformeln. Ph.D. thesis, Institut
für Mathematische Logik und Grundlagenforschung der Universität Münster i.W.
(1971) 232
26. Börger, E.: Per una teoria delle fallacie dal punto di vista della logica simbolica.
Proteus III(7), 11–23 (1972) 222
27. Börger, E.: Überlegungen zur Aristotelischen Irrtumslehre vom Standpunkt
der mathematischen Logik.
In: E. Börger, D. Barnocchi, F. Kaulbach (eds.)
Zur Philosophie der mathematischen Erkenntnis. Königshausen und Neumann
(Würzburg) (1981). ISBN: 978-3-8847-9022-9 222
28. Börger, E.: Decision problems in predicate logic.
In: Logic Colloquium 82, pp.
263–301 (1984). Elsevier (North Holland) 233
29. Börger, E.: Computability, Complexity, Logic (English translation of “Berechen-
barkeit, Komplexität, Logik”, Vieweg-Verlag 1985), Studies in Logic and the Foun-
dations of Mathematics, vol. 128. North-Holland (1989). Italian Translation “Com-
putabilità Complessità Logica”, Vol. 1 Teoria della Computazione. Bollati Bor-
inghieri 1989 204, 230, 231
30. Börger, E.: A logical operational semantics of full Prolog: Part 2. Built-in predi-
cates for database manipulations. In: B. Rovan (ed.) Mathematical Foundations
of Computer Science 1990, pp. 1–14 (1990). Springer LNCS 452 82, 90
31. Börger, E.: Logic programming: The Evolving Algebra approach. In: B. Pehrson,
I. Simon (eds.) IFIP 13th World Computer Congress, vol. I: Technology/Founda-
tions, pp. 391–395. Elsevier, Amsterdam (1994) 185
32. Börger, E.: Evolving Algebras and Parnas tables.
In: H. Ehrig, F. von Henke,
J. Meseguer, M. Wirsing (eds.) Specification and Semantics. Dagstuhl Seminar
No. 9626, Schloss Dagstuhl, Int. Conf. and Research Center for Computer Science
(1996) 11
33. Börger, E.: The ASM ground model method as a foundation of requirements en-
gineering. In: N. Dershowitz (ed.) Verification: Theory and Practice, LNCS, vol.
2772, pp. 145–160. Springer-Verlag (2003) 185
34. Börger, E.: The ASM refinement method. Formal Aspects of Computing 15, 237–
257 (2003) 185
35. Börger, E.: Construction and analysis of ground models and their refinements as a
foundation for validating computer based systems. Formal Aspects of Computing
19, 225–241 (2007) 185
36. Börger, E.: Modeling distributed algorithms by Abstract State Machines compared
to Petri Nets. In: M.B. et al (ed.) ABZ 2016, Lecture Notes in Computer Science,
vol. 9675, pp. 3–34. Springer-Verlag (2016). DOI: 10:1007/978-3-319-33600-8.1 119

References
271
37. Börger, E.: Why programming must be supported by modeling and how.
In:
T. Margaria, B. Steffen (eds.) ISOLA 2018, Lecture Notes in Computer Science,
vol. 11244, pp. 89–110. Springer-Verlag (2018). https://doi.org/10.1007/978-
3-030-03418-4_6 185
38. Börger, E.: The role of executable abstract programs in software development and
documentation. http://arxiv.org/abs/2209.06546 (2022). Originally prepared
and accepted for presentation to (but then withdrawn from) ISOLA 2022, track
Programming - What is Next: The Role of Documentation 185
39. Börger, E., Bolognesi, T.: Remarks on turbo ASMs for computing functional equa-
tions and recursion schemes. In: E. Börger, A. Gargantini, E. Riccobene (eds.)
Abstract State Machines 2003 – Advances in Theory and Applications, Lecture
Notes in Computer Science, vol. 2589, pp. 218–228. Springer-Verlag (2003) 205
40. Börger, E., Craig, I.: Modeling an operating system kernel. In: V. Diekert, K. We-
icker, N. Weicker (eds.) Informatik als Dialog zwischen Theorie und Anwendung,
pp. 199–216. Vieweg+Teubner, Wiesbaden (2009) 130
41. Börger, E., Del Castillo, G.: A formal method for provably correct composition of
a real-life processor out of basic components (The APE100 Reverse Engineering
Study). In: B. Werner (ed.) Proc. 1st IEEE Int. Conf. on Engineering of Complex
Computer Systems (ICECCS’95), pp. 145–148. IEEE (1995) 190
42. Börger, E., Demoen, B.: A framework to specify database update views for Prolog.
In: J. Maluszynski, M. Wirsing (eds.) Programming Language Implementation and
Logic Programming, pp. 147–158. Springer LNCS 528 (1991) 82, 90
43. Börger, E., Durdanović, I.: Correctness of compiling Occam to Transputer code.
Computer Journal 39(1), 52–92 (1996) 144, 191
44. Börger, E., Durdanović, I., Rosenzweig, D.: Occam: Specification and compiler
correctness. Part I: Simple mathematical interpreters.
In: U. Montanari, E.R.
Olderog (eds.) Proc. PROCOMET’94 (IFIP Working Conf. on Programming Con-
cepts, Methods and Calculi), pp. 489–508. North-Holland (1994) 130, 138, 140,
143
45. Börger, E., Fruja, G., Gervasi, V., Stärk, R.: A high-level modular definition of the
semantics of C#. Theoretical Computer Science 336(2–3), 235–284 (2005) 43, 52,
189, 193
46. Börger, E., Glässer, U., Müller, W.: The semantics of behavioral VHDL’93 descrip-
tions. In: J. Mermet (ed.) EURO-DAC’94. European Design Automation Confer-
ence with EURO-VHDL’94, ISBN 0-89791-685-9, pp. 500–505. IEEE Computer
Society Press, Los Alamitos, California (1994) 189
47. Börger, E., Glässer, U., Müller, W.: Formal definition of an abstract VHDL’93 sim-
ulator by ea-machines. In: C. Delgado Kloos, P.T. Breuer (eds.) Formal Semantics
for VHDL, pp. 107–139. Kluwer Academic Publishers (1995) 189
48. Börger, E., Grädel, E., Gurevich, Y.: The Classical Decision Problem. Perspectives
in Mathematical Logic. Springer (1997) 234, 238
49. Börger, E., Päppinghaus, P., Schmid, J.: Report on a practical application of ASMs
in software design. In: Y. Gurevich, P. Kutter, M. Odersky, L. Thiele (eds.) Ab-
stract State Machines: Theory and Applications, Lecture Notes in Computer Sci-
ence, vol. 1912, pp. 361–366. Springer-Verlag (2000) 192
50. Börger, E., Raschke, A.: Modeling Companion for Software Practitioners. Springer
(2018). ISBN 978-3-662-56641-1. For Corrigenda and lecture material on themes
treated in the book see http://modelingbook.informatik.uni-ulm.de 24, 28, 75,
119, 131, 145, 205
51. Börger, E., Rosenzweig, D.: A mathematical definition of full Prolog. Science of
Computer Programming 24, 249–286 (1995) 82, 189, 192
52. Börger, E., Rosenzweig, D.: The WAM – definition and compiler correctness. In:
C. Beierle, L. Plümer (eds.) Logic Programming: Formal Methods and Practi-
cal Applications, Studies in Computer Science and Artificial Intelligence, vol. 11,
chap. 2, pp. 20–90. North-Holland (1995) 82, 191, 192

272
References
53. Börger, E., Salamone, R.: CLAM specification for provably correct compilation of
CLP(R) programs. In: E. Börger (ed.) Specification and Validation Methods, pp.
97–130. Oxford University Press (1995) 192
54. Börger, E., Schewe, K.D.: Concurrent Abstract State Machines. Acta Informatica
53(5) (2016). http://link.springer.com/article/10.1007/s00236-015-0249-
7, DOI 10.1007/s00236-015-0249-7. Listed as Notable Article in ACM 21th Annual
BEST OF COMPUTING, see www.computingreviews.com/recommend/bestof/
notableitems.cfm?bestYear=2016. 112, 205
55. Börger, E., Schewe, K.D.: A behavioural theory of recursive algorithms. Funda-
menta Informatica 177(1), 1–37 (2020) 205, 208
56. Börger, E., Schewe, K.D., Wang, Q.: Serialisable multi-level transaction con-
trol:a specification and verification. Science of Computer Programming 131, 42–
85 (2016). http://authors.elsevier.com/sd/article/S0167642316300041, DOI
10.1016/j.scico.2016.03.008 29, 30, 125
57. Börger, E., Sona, D.: A neural abstract machine. J. Universal Computer Science
7(11), 1007–1024 (2001) 162
58. Börger, E., Stärk, R.F.: Abstract State Machines. A Method for High-Level System
Design and Analysis. Springer (2003) 20, 24, 31, 35, 53, 55, 193, 200, 205, 237
59. Börger, E., Stärk, R.F.: Exploiting Abstraction for Specification Reuse. The
Java/C# Case Study. In: M. Bonsangue (ed.) Formal Methods for Components
and Objects: Second International Symposium (FMCO 2003 Leiden), Lecture Notes
in Computer Science (ISBN 3-540-22942-6, ISSN 0302-9743), vol. 3188, pp. 42–76.
Springer (2004). 191, 193
60. Cerf, V.G., Dalal, Y., Sunshine, C.: RFC675: Specification of internet Transmission
Control Program (Dec 1974) 124, 169
61. Chandra, A.K., Harel, D.: Structure and complexity of relational queries.
J.Comp.Syst.Sci. 25(1), 99–128 (1982) 236
62. Church, A.: A note on the Entscheidungsproblem. J.Symbolic Logic 1, 40–41 (1936)
232
63. Claverini, C.: Tommaso Campanella in difesa di Galileo Galilei. “Libertas philoso-
phandi” e concordanza dei libri di Dio. Giornale Critico di Storia delle Idee 12/13
(2014-2015) V
64. CoreASM Contributors: ASM interpreter: The CoreASM Project.
https://
github.com/coreasm/ (since 2005) 69
65. Craig, I., Börger, E.: Synchronous message passing and semaphores: An equivalence
proof. In: M. Frappier, U. Glässer, S. Khurshid, R. Laleau, S. Reeves (eds.) Abstract
State Machines, Alloy, B and Z, Lecture Notes in Computer Science, vol. 5977, pp.
20–33. Springer-Verlag (2010) 130
66. Daessler, K.: Prolog. Part 1. General Core (1992). ISO IEC JTCI WG17 Committee
Draft 1.0, N.92. 82, 90, 189
67. Dijkstra, E.W.: Guarded commands, non-determinacy and formal derivation of
programs. Commun. ACM 18(8), 453–457 (1975). Also available as EWD 472. 64
68. Ebbinghaus, H.D., Flum, J.: Finite Model Theory. Perspectives in Mathematica
Logic. Springer (1995) 236
69. Ericsson-Zenith, S.: Occam 2 reference manual (1988). Prentice-Hall. ISBN 0-13-
629312-3 28, 64
70. Ernst, G., Pfähler, J., Schellhorn, G., Reif, W.: Modular, crash-safe refinement for
ASMs with submachines. Science of Computer Programming 131, 3–21 (2016) 191
71. Eschbach, R., Glässer, U., Gotzhein, R., Prinz, A.: On the formal semantics of SDL-
2000: A compilation approach based on an abstract SDL machine. In: Y. Gurevich,
P. Kutter, M. Odersky, L. Thiele (eds.) Abstract State Machines: Theory and
Applications, Lecture Notes in Computer Science, vol. 1912, pp. 242–265. Springer-
Verlag (2000) 189

References
273
72. Farahbod, R.: CoreASM: An extensible modeling framework & tool environment
for high-level design and analysis of distributed systems. Ph.D. thesis, Simon Fraser
University, Burnaby, Vancouver, Canada (2009) 273
73. Farahbod, R.: Design and Specification of the CoreASM Execution Engine and Plu-
gins.
https://github.com/CoreASM/coreasm.core/raw/master/org.coreasm.
engine/rsc/doc/CoreASM-DesignDocumentation.pdf (2010). See also [72]. 75
74. Farahbod,
R.,
Dausend,
M.:
CoreASM
Language
User
Manual.
https:
//github.com/CoreASM/coreasm.core/raw/master/org.coreasm.engine/rsc/
doc/user_manual/CoreASM-UserManual.pdf (2016) 75
75. Farahbod, R., Gervasi, V., Glaesser, U.: CoreASM: An extensible ASM execution
engine. Fundamenta Informaticae pp. 71–103 (2007) 250
76. Ferrarotti, F., Gonzalez, S., Schewe, K.D.: Bsp abstract state machines capture
bulk synchronous parallel computations. Science of Computer Programming 184
(2019) 151
77. Ferrarotti, F., Schewe, K.D., Tec, L., Wang, Q.: A new thesis concerning synchro-
nised parallel computing – simplified parallel ASM thesis. Theor. Comp. Sci. 649,
25–53 (2016) 205
78. Fleischmann, A.: Distributed Systems. Software Design and Implementation.
Springer-Verlag, Berlin Heidelberg New York (1994) 7
79. Fleischmann, A., Schmidt, W., Stary, C., Obermeier, S., Börger, E.: Subject-
Oriented Business Process Management.
Springer, Heidelberg (2012).
www.
springer.com/978-3-642-32391-1 (Open Access Book) 7
80. Flocchini, P., Prencipe, G., Santoro, N.: Distribute Computing by Mobile Entities.
Current Research in Moving and Computing. Springer-Verlag (2019). LNCS 11340
250, 253, 255, 257
81. Futamura, Y.: Partial evaluation of computation process–an approach to a
compiler-compiler. Systems, Computers, Controls 2, 721–728 (1971) 230
82. Galilei, G.: Le opere di Galileo Galilei. In: Volume IV. Edizione Nazionale, Firenze
(1894) V
83. Gervasi, V., Börger, E., Cisternino, A.: Modeling web applications infrastructure
with ASMs. Science of Computer Programming 94(2), 69–92 (2014). DOI http:
//dx.doi.org/10.1016/j.scico.2014.02.025. URL http://www.sciencedirect.com/
science/article/pii/S0167642314000926 119
84. Gödel, K.: Über formal unentscheidbare Sätze der Principia Matematica und ver-
wandter Systeme I. Monatshefte Math.Physik 38, 173–198 (1931) 101
85. Goerigk, W., Dold, A., Gaul, T., Goos, G., Heberle, A., von Henke, F.W., Hoff-
mann, U., Langmaack, H., Pfeifer, H., Ruess, H., Zimmermann, W.: Compiler
correctness and implementation verification: The Verifix approach. In: P. Fritzson
(ed.) Int. Conf. on Compiler Construction, Proc. Poster Session of CC’96. IDA
Technical Report LiTH-IDA-R-96-12, Linköping, Sweden (1996) 191
86. Goos, G., Zimmermann, W.: Verification of compilers. In: E.R. Olderog, B. Steffen
(eds.) Correct System Design, Lecture Notes in Computer Science, vol. 1710, pp.
201–230. Springer-Verlag (1999). https://doi.org/10.1007/3-540-48092-7_10
191
87. Goos, G., Zimmermann, W.: Verifying compilers and ASMs.
In: Y. Gurevich,
P. Kutter, M. Odersky, L. Thiele (eds.) Abstract State Machines: Theory and
Applications, Lecture Notes in Computer Science, vol. 1912, pp. 177–202. Springer-
Verlag (2000) 190, 191
88. Gosling, J., Joy, B., Steele, G.: The Java™Language Specification. Addison Wesley
(1996) 45
89. Gosling, J., Joy, B., Steele, G., Bracha, G.: The Java™Language Specification,
2nd edn. Addison Wesley (2000) 43
90. Grandy, H., Bischof, M., Schellhorn, G., Reif, W., Stenzel, K.: Verification of Mon-
dex Electronic Purses with KIV: From a Security Protocol to Verified Code. In:

274
References
FM 2008: 15th Int. Symposium on Formal Methods. Springer LNCS 5014 (2008)
191
91. Gurevich, Y.: A new thesis. Abstracts, American Mathematical Society 6(4), 317
(1985) 204
92. Gurevich, Y.: Evolving algebras 1993: Lipari Guide. In: E. Börger (ed.) Specifica-
tion and Validation Methods, pp. 9–36. Oxford University Press (1995) 236
93. Gurevich, Y.: Sequential Abstract State Machines capture sequential algorithms.
ACM Trans. Computational Logic 1(1), 77–111 (2000) 24, 42, 196, 199, 205
94. Haneberg, D., Grandy, H., Reif, W., Schellhorn, G.: Verifying smart card appli-
cations: An ASM approach. In: Proc. Conference on Integrated Formal Methods
(iFM 2007), LNCS, vol. 4591. Springer (2007). URL http://www.informati.uni-
augsburg.de/lehrstuehle/swt/se/projects/gocard 191
95. Haneberg, D., Moebius, N., Reif, W., Schellhorn, G., Stenzel, K.: Mondex: Engi-
neering a provable secure electronic purse. International Journal of Software and
Informatics 5(1), 159–184 (2011). http://www.ijsi.org 191
96. Haneberg, D., Schellhorn, G., Grandy, H., Reif, W.: Verification of mondex elec-
tronic purses with KIV: From transactions to a security protocol. Formal Aspects
of Computing 20(1) (2008) 191
97. Hermes, H.: Ideen von Leibniz zur Grundlagenforschung: die ars inveniendi und
die ars iudicandi.
In: Studia Leibnitiana. Akten des Internationalen Leibniz-
Kongresses, Hannover, pp. 78–88 (1966) 222
98. Hewitt, C., Bishop, P., Steiger, R.: A universal modular ACTOR formalism for
artificial intelligence. In: Proceedings of the 3rd International Joint Conference on
Artificial Intelligence San Francisco/CA (IJCAI’73), pp. 235–245. Morgan Kauf-
mann (1973) 118
99. Hilbert, D.: Die logischen Grundlagen der Mathematik. Mathematische Annalen
88, 151–165 (1923) 60
100. Hilbert, D., Ackermann, W.: Grundzüge der theoretischen Logik. Springer (1928,
1938) 232
101. Hilbert, D., Bernays, P.: Grundlagen der Mathematik I,II. Springer (1934, 1939)
60
102. Hoare, C.A.R.: Communication sequential processes. Communications of the ACM
21(8), 666–677 (1978). DOI doi:10.1145/359576.359585 154
103. Hoyte, D.: Let Over Lambda: 50 Years of Lisp. Doug Hoyte/HCSW and Hoytech
production (2008). URL https://books.google.it/books?id=vbTONwAACAAJ 101
104. Inmos: Transputer instruction set—a compiler writer’s guide (1988). INMOS doc-
ument 72TRN 119 05, Prentice-Hall, Englewood Cliffs, NJ. 132
105. Inmos: Transputer implementation of Occam (1989). In: Communication Process
Architecture, Prentice-Hall, Englewood Cliffs, NJ. 132
106. ITU-T: SDL formal semantics definition. ITU-T Recommendation Z.100 Annex F
http://www.sdl-forum.org, International Telecommunication Union (2000) 189
107. Ivakhnenko, A.G., Lapa, V.G.: Cybernetic Predicting Devices.
U.S. Depart-
ment of Commerce, Join Publications Research Service, Washington, D.C., USA
(1966). (English version of the original: Kiberneticheskiye Predskazyvayushchiye
Ustroystva, Kiev, USSR, 1965) 157
108. Kahn, G.: The semantics of a simple language for parallel programming. In: J.L.
Rosenfeld (ed.) Proceedings of the IFIP Congress on Information Processing, pp.
471–475. North Holland Publishing Company (1974) 154, 157
109. Kari, J.: Theory of cellular automata: A survey. Theoretical Computer Science
334, 3–33 (2005) 69
110. Kirkpatrick, D., Kostitsyna, I., Navarra, A., Prencipe, G., Santoro, N.: Separating
bounded and unbounded asynchrony for autonomous robots: Point convergence
with limited visibility.
In: Proceedings of the 2021 ACM Symposium on Prin-
ciples of Distributed Computing, PODC’21, p. 9–19. Association for Computing

References
275
Machinery, New York, NY, USA (2021).
DOI 10.1145/3465084.3467910.
URL
https://doi.org/10.1145/3465084.3467910 275
111. Kirkpatrick, D., Kostitsyna, I., Navarra, A., Prencipe, G., Santoro, N.: On the
power of bounded asynchrony: convergence by autonomous robots with limited
visibility (2023). Submitted, extended version of [110] 254, 257
112. Kleene, S.C.: General recursive functions of natural numbers. Mathematische An-
nalen 112, 727–742 (1936) 56, 228
113. Kossak, F., Illibauer, C., Geist, V., Kubovy, J., Natschläger, C., Ziebermayr, T.,
Kopetzky, T., Freudenthaler, B., Schewe, K.D.: A Rigorous Semantics for BPMN
2.0 Process Diagrams. Springer (2015) 189
114. Kossak, F., Illibauer, C., Geist, V., Natschläger, C., Ziebermayr, T., Freudenthaler,
B., Kopetzky, T., Schewe, K.D.: Hagenberg Business Process Modelling Method.
Springer (2016) 189
115. Lamport, L.: A new solution of Dijkstra’s concurrent programming problem. Com-
mun. ACM 17(8), 453–455 (1974) 112
116. Lamport, L.: How to make a multiprocessor computer that correctly executes mul-
tiprocess programs. IEEE Trans. Computers 28(9), 690–691 (1979) 67
117. Lamport, L.: On interprocess communication. Part I: Basic formalism. Part II:
Algorithms. Distributed Computing 1, 77–101 (1986) 112
118. Lamport, L.: Specifying Systems: The TLA+ Language and Tools for Hardware
and Software Engineers. Addison-Wesley (2003). Availabel at http://lamport.org
196
119. Leibniz, G.W.: Dialogus de connexione inter res et verba.
G. W. Leibniz:
Philosophische Schriften (1677). Edited by Leibniz-Forschungsstelle der Univer-
sität Münster, Vol.4 A, n.8. Akademie Verlag 1999 187
120. Lerman, M.: Degrees of Unsolvability: Local and Global Theory. Cambridge Uni-
versity Press (2017). DOI 10.1017/9781316717059. Perspectives in Logic 238
121. Libkin, L.: Elements of Finite Model Theory. Springer (2012) 236
122. Lindholm, T.G., O’Keefe, R.A.: Eﬀicient implementation of a defensible semantics
for dynamic prolog code. In: Proceedings of the Fourth International Conference
on Logic Programming, pp. 21–39 (1987) 90
123. Lukasiewicz, J.: Aristotle’s Syllogistic. From the Standpoint of Modern Formal
Logic. Clarendon Press (Oxford) (1954) 222
124. Lynch, N.: Distributed Algorithms. Morgan Kaufmann (1996). ISBN 978-1-55860-
348-6 119
125. Makowski, J.A.: Some thoughts on computational models: from massive human
computing to Abstract State Machines. In: A. Raschke, E. Riccobene, K.D. Schewe
(eds.) Logic, Computation and Rigorous Methods (Börger Festschrift), LNCS, vol.
12750, pp. 172–186. Springer (2021) 237
126. McCarthy, J.: Recursive functions of symbolic expressions and their computation
by machine, part I. Communications of the ACM 3(4) (1960) 91
127. Mealy, G.H.: A method for synthesizing sequential circuits. Bell System Technical
J. 34, 1045–1079 (1955). DOI 10.1002/j.1538-7305.1955.tb03788.x 10
128. Memon, M.: Specification language design concepts: Aggregation and extensibility
in coreasm. Master thesis, Simon Fraser University (2006) 75
129. Minsky, M.: Recursive unsolvability of Post’s problem of ‘tag’ and other topics in
the theory of Turing machines. Annals of Mathematics 74, 437–455 (1961) 226,
233
130. Moschovakis, Y.N.: What is an algorithm?
In: B. Engquist, W. Schmid (eds.)
Mathematics Unlimited – 2001 and beyond, pp. 919–936. Springer-Verlag (2001)
205
131. Moschovakis, Y.N.: On founding the theory of algorithms (2002). URL https:
//www.math.ucla.edu/~ynm/papers/foundalg.pdf. Preprint, February 12, 2002
230

276
References
132. N., N.: Some catalogues of Conway’s Game of Life patterns. https://conwaylife.
com/wiki/, https://catagolue.appspot.com/home, https://conwaylife.com/
ref/lexicon/lex_home.htm. Consulted on May 31, 2023. 71
133. Ottmann, T., Widmayer, P.: Algorithmen und Datenstrukturen.
BI Wis-
senschaftsverlag (1993). Reihe Informatik Band 70 236, 238
134. Parnas, D.L.: Tabular representation of relations (1992). CRL Report 260, Mc-
Master University 11
135. Parnas, D.L., Lawford, M.: The role of inspection in software quality assurance.
IEEE Transactions on Software engineering 29(8), 674–676 (2003) 188
136. Pitman, K.M.: Special forms in Lisp. In: Proceedings of the 1980 ACM Confer-
ence on LISP and Functional Programming, LFP ’80, p. 179–187. Association for
Computing Machinery, New York, NY, USA (1980). DOI 10.1145/800087.802804
97
137. P.Michell, D.A., Thompson, J.A.: Inside the Transputer. Blackwell Scientific Pub-
lications (1990) 137
138. Popper, K.: Logik der Forschung. Zur Erkenntnistheorie der modernen Naturwis-
senschaft. Springer-Verlag Wien, Wien (1935) 188
139. Prencipe, G.: e-mail to Egon Börger (13.3.2023). Cc to pflocchi@uottawa.ca and
santoro@scs.carleton.ca 251, 256
140. Rabin, M., Scott, D.: Finite automata and their decision problems. IBM J.Research
3, 114–125 (1959) 26, 63
141. Rice, H.G.: Classes of recursively enumerable sets and their decision problems.
Trans.AMS 89, 25–59 (1953) 225
142. Richard, J.: Les principes des mathématiques et le problème des ensembles. Revue
Générale des Sciences Pures Et Appliquées 12(16), 541–543 (1905) 222
143. Schellhorn, G.: Verification of ASM refinements using generalized forward simula-
tion. J. Universal Computer Science 7(11), 952–979 (2001) 191
144. Schellhorn, G.: ASM refinement and generalizations of forward simulation in data
refinement: A comparison. J. of Theoretical Computer Science 336(2-3), 403–436
(2005) 191
145. Schellhorn, G.: ASM refinement preserving invariants. J. UCS 14(12), 1929–1948
(2008) 191
146. Schellhorn, G.: Completeness of ASM refinement. Electr. Notes Theor. Comput.
Sci. 214, 25–49 (2008) 191
147. Schellhorn, G.: Completeness of fair ASM refinement. Sci. of Computer Program-
ming 76(9), 756–773 (2011). URL https://kiv.isse.de/projects/Refinement/
ASM-Refinement-complete.html 191
148. Schellhorn, G., Ahrendt, W.: Reasoning about Abstract State Machines: The WAM
case study. J. Universal Computer Science 3(4), 377–413 (1997) 191
149. Schellhorn, G., Ahrendt, W.: The WAM case study: Verifying compiler correctness
for Prolog with KIV. In: W. Bibel, P. Schmitt (eds.) Automated Deduction – A Ba-
sis for Applications, vol. III: Applications, pp. 165–194. Kluwer Academic Publish-
ers (1998).
URL http://www.informatik.uni-augsburg.de/lehrstuehle/swt/
se/publications 82, 191
150. Schellhorn, G., Ernst, G., Pfähler, J., Bodenmüller, S., Reif, W.: Symbolic execu-
tion for a clash-free subset of ASMs. Science of Computer Programming 158, 21–40
(2018). DOI 10.1016/j.scico.2017.08.014. URL https://www.sciencedirect.com/
science/article/pii/S0167642317301739 191
151. Schellhorn, G., Ernst, G., Pfähler, J., Haneberg, D., Reif, W.: Development of a
verified flash file system. In: ABZ 2014, LNCS, vol. 8477, pp. 9–24. Springer (2014)
191
152. Schellhorn, G., Grandy, H., Haneberg, D., Moebius, N., Reif, W.: A System-
atic Verification Approach for Mondex Electronic Purses Using ASMs. In: J.R.
Abrial, U. Glässer (eds.) Rigorous Methods for Software Construction and Anal-
ysis (Börger Festschrift), LNCS, vol. 5115, pp. 93–110. Springer (2009).
URL
http://www.informatik.uni-augsburg.de/swt/projects/mondex.html 191

References
277
153. Schewe, K.D., Ferrarotti, F.: Behavioural theory of reflective algorithms I: Reflec-
tive sequential algorithms. Science of Computer Programming 223 (2022) 78, 81,
195, 205
154. Schewe, K.D., Wang, Q.: Partial updates in complex-value databases. In: A. Heim-
bürger, et al. (eds.) Information and Knowledge Bases XXII, pp. 37–56. IOS Press
(2011). Vol. 225 of Frontiers in Artificial Intelligence and Applications 75
155. Shepherdson, J., Sturgis, H.: Computability of recursive functions. J. Association
for Computing Machinery 10, 217–255 (1963) 226, 233
156. Sieg, W.: Axioms for computability: Do they allow a proof of Church’s thesis? In:
H. Zenil (ed.) Computation in nature and the nature of computation, pp. 332–341.
College Publications (2012) 205
157. Slupecki, J.: Z badan nad Sylogistyka Arystotelesa.
Travaux de la Société des
Sciences et des Lettres B6, 5–30 (1948) 222
158. Stärk, R.F.: Formal specification and verification of the C# thread model. Theo-
retical Computer Science 343, 482–508 (2005) 149
159. Stärk, R.F., Schmid, J., Börger, E.: Java and the Java Virtual Machine: Definition,
Verification, Validation. Springer-Verlag (2001) 43, 52, 81, 146, 147, 149, 189, 190,
191
160. Stenzel, K.: A formally verified calculus for full Java card. In: C. Rattray, S. Ma-
haraj, C. Shankland (eds.) AMAST 2004, LNCS, vol. 3116, pp. 491–505. Springer
(2004) 191
161. Tarski, A.: Der Wahrheitsbegriff in den formalisierten Sprachen. Studia Philosoph-
ica 1, 261–405 (1936) 11
162. Turing, A.M.: On computable numbers, with an application to the Entschei-
dungsproblem. Proceedings of the London Mathematical Society s2-42(1), 230–
–265 (1937). https://doi.org/10.1112/plms/s2-42.1.230 23, 26, 39, 52, 195,
196, 197, 199, 204, 224, 226, 227, 228, 231, 232
163. W3C Consortium: HTML 5: A vocabulary and associated APIs for HTML and
XHTML (2010). URL http://www.w3.org/TR/html5. W3C Working Draft, Oc-
tober 19 129
164. Wallace, C., Tremblay, G., Amaral, J.N.: An Abstract State Machine specification
and verification of the location consistency memory model and cache protocol.
J. Universal Computer Science 7(11), 1089–1113 (2001) 116
165. Wirth, N.: The development of procedural programming languages personal contri-
butions and perspectives. In: W. Weck, J. Gutknecht (eds.) Modular Programming
Languages, pp. 1–10. Springer Berlin Heidelberg, Berlin, Heidelberg (2000) 5
166. Wolfram, S.: Cellular Automata And Complexity: Collected Papers.
CRC
Press (2002).
ISBN-10: 0201626640, ISBN-13: 978-0201626643, https://www.
stephenwolfram.com/publications/cellular-automata-complexity/ 71
167. Wolfram, S.: A new kind of science. Wolfram Media (2002). ISBN: 1-57955-008-8,
https://www.wolframscience.com/nks/ 18, 71
168. Zimmerman, W., Gaul, T.: On the construction of correct compiler back-ends: An
ASM approach. J. Universal Computer Science 3(5), 504–567 (1997) 190

Index
A
alphabet
12
apply(state,U)
37
architecture
Harvard
59
argument
of a location
18
ArithAlg
204
ArithPGA
204
ASM
ambient
145
communicating
118
concurrent
112, 206
control state
28
i/o-rule
211
multi-agent
112
sequential
24
structured
55
turbo
55
universal
227
assignment
23
partial
74
automaton
cellular
69
elementary cellular
71
Mealy
10, 12
B
background
22, 41
base set
10
browser
119
BSP
151
ASM
153
process rule
152
C
capture
195
choice
bounded
63
coincidence
14, 15
computation
concurrent
112
sequential
39, 196
concurrency
205
content
18
control state ASM
28
D
declarative description
18
Delta
33, 199, 202
denotation
10
diagonalization
222, 225
diagram
(m, n)
190
difference
B −A of states
19
E
element
of a location
18
of state
10
Entscheidungsproblem
232
environment
variable
13
eval
evalCand
61, 68
of ASM rules
32
of formulae
15
of import
36
of terms
13
expression
10
F
formula
atomic
14
FSM
26
279
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
E. Börger, V. Gervasi, Structures of Computing, 
https://doi.org/10.1007/978-3-031-54358-6 

280
Index
function
8
arity
8
characteristic
8
context sensitive
145
controlled
21
derived
22
domain
8, 11
dynamic
7, 8, 21
enumeration
228
environment dependent
145
external
22
initial
56
interaction
22
monitored
21
out
21
partial
8, 12
range
8
reduction
232
shared
22
static
7, 8, 20
step
32
total
8, 11
transition
32
G
Gödelization
101, 104, 204, 227, 244
Game of Life
71
ground model
187
guard
23
guarded command
alternative
64
repetitive
64
H
halting problem
224
heap
35
homomorphism
19
I
inspection
187
instruction
assignment
23
partial assignment
74
update
74
interleaving
66
parallel
69
isomorphism
19
iterate
54
L
location
18
f -location
18
argument of a
18
content of
18
element of a
18
Locs(U)
19
Look Compute Action
27, 155
algorithm
250
run
254
M
machine
finite state
26
input/output
52
Mealy
26
register
31
Turing
26
minimization µ
58
multitasking
67
N
new
37
non-systolic
158
NP-completeness
233
O
Occam
alt cmd
64
ground model
130
par cmd
116
operational description
18
P
Pascal
5
PGA
4, 24
reflective
78
postulate
arithmetical state
203
recursion
208
sequential
199
predicate
8
process
3
multi-agent
22
reactive
40
single-agent
22
property
8
R
RAM
59, 101, 263
RASP
101
reflective
101
read-cleanness
112

Index
281
recursion
primitive
57
recursive
µ-recursive
56
reduction
method
223, 231
polynomial time
234
refinement
(m, n)
190
correct
191
reflective
algorithm
76
LISP
91
PGA
78
Prolog
82
RAM
101
relation
8
RM
31
rule
declaration
34
ground
40
main
40
run
async LCM
255
concurrent
112
input-driven
52
k-async LCM
256
single-agent
40
sync LCM
254
S
scope
14
Seq
197
seq
53
sequel
19
signature
9
state
10
difference
19
elements of
10
internal
118
isomorphic
19
next internal
33
successor
32, 36, 62
streaming
abstract model
154
functional
156
guarded selection
159
non-deterministic
157
spreadsheet
162
stateful
157
TCP/IP
170
structure
Tarski
4
substitution
15, 16
simultaneous of functions
57
superuniverse
10
T
tape
12
Tarski
structure
4
TCP/IP
169
term
9
closed
10
evaluation
13
ground
10
theorem
characterization
195, 197, 203, 204,
206, 208
thesis
sequential
199
Turing’s
204
transaction control
29
Turing Machine TM
27, 196
U
universal machine
227
with fixed signature
239
universe
10
update
18
consistent update set
19
elements of
18
genuine
74
instruction
74
internal
32
partial
74
regular
74
set
18
trivial
18, 33
V
variable
9
bound
14
environment
13
scope
14
vocabulary
9
W
while
54
Workspace
35



Designing Software 
Architectures
2nd Edition

T
he SEI Series in Software Engineering is a collaborative undertaking of 
the Carnegie Mellon Software Engineering Institute (SEI) and Addison-
Wesley to develop and publish books on software engineering and related 
topics. The common goal of the SEI and Addison-Wesley is to provide the 
most current information on these topics in a form that is easily usable by 
practitioners and students.
Titles in the series describe frameworks, tools, methods, and technologies 
designed to help organizations, teams, and individuals improve their technical 
or management capabilities. Some books describe processes and practices for 
developing higher-quality software, acquiring programs for complex systems, 
or delivering services more effectively. Other books focus on software and 
system architecture and product-line development. Still others, from the 
SEI’s CERT Program, describe technologies and practices needed to manage 
software and network security risk. These and all titles in the series address 
critical problems in software engineering for which practical solutions are 
available.
Visit informit.com/sei for a complete list of available publications.
The SEI Series in Software Engineering
Make sure to connect with us! 
informit.com/socialconnect

Designing Software 
Architectures
2nd Edition
A Practical Approach
Humberto Cervantes 
Rick Kazman

SEI Series in Software Engineering
Figures 7.1, 7.2, 7.3, 9.8b: Amazon Web Services, Inc.
Figures 9.5, 9.6: Litmus Automation Inc.
Figures 9.9, 9.10: SoftServe Inc.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks. 
Where those designations appear in this book, and the publisher was aware of a trademark claim, the designations 
have been printed with initial capital letters or in all capitals.
The authors and publisher have taken care in the preparation of this book, but make no expressed or implied war-
ranty of any kind and assume no responsibility for errors or omissions. No liability is assumed for incidental or 
consequential damages in connection with or arising out of the use of the information or programs contained herein.
For information about buying this title in bulk quantities, or for special sales opportunities (which may include elec-
tronic versions; custom cover designs; and content particular to your business, training goals, marketing focus, or 
branding interests), please contact our corporate sales department at corpsales@pearsoned.com or (800) 382-3419.
For government sales inquiries, please contact governmentsales@pearsoned.com. 
For questions about sales outside the U.S., please contact intlcs@pearson.com. 
Visit us on the Web: informit.com/aw
Library of Congress Control Number: 2024937507 
Copyright © 2024 Pearson Education, Inc.
Hoboken, NJ
All rights reserved. This publication is protected by copyright, and permission must be obtained from the publisher 
prior to any prohibited reproduction, storage in a retrieval system, or transmission in any form or by any means, elec-
tronic, mechanical, photocopying, recording, or likewise. For information regarding permissions, request forms and 
the appropriate contacts within the Pearson Education Global Rights & Permissions Department, please visit www.
pearsoned.com/permissions/. Please contact us with concerns about any potential bias at https://www.pearson.com/
report-bias.html. 
ISBN-13: 978-0-13-810802-1
ISBN-10: 0-13-810802-1 
$PrintCode

I would like to dedicate this book to my parents, Ilse and Humberto; to my wife Gabriela; 
and to my sons Julian and Alexis. Thank you for all your love, support, and inspiration.
Humberto Cervantes
I would like to dedicate this book to my wife Hong-Mei, and to my kids Jonah, Mia, and Sam, 
who have always supported me in my endeavors and tolerated my space cadet moments.
Rick Kazman

This page intentionally left blank 

vii
Contents
Preface    xi
Acknowledgments    xiii
About the Authors    xv
CHAPTER 1	
Introduction 
1
1.1  Motivations    1
1.2  Software Architecture    2
1.3  The Role of the Architect    6
1.4  A Brief History of ADD    7
1.5  Summary    8
1.6  Further Reading    8
1.7  Discussion Questions    9
CHAPTER 2	
Architectural Design 
11
2.1  Design in General    11
2.2  Design in Software Architecture    12
2.3  Why Is Architectural Design So Important?    16
2.4  Architectural Drivers    17
2.5  Summary    26
2.6  Further Reading    26
2.7  Discussion Questions    27
CHAPTER 3	
Making Design Decisions 
29
3.1  Making Design Decisions    29
3.2  Design Concepts: The Building Blocks for Creating  
         Structures    32

viii  Contents
3.3  Design Concepts to Support Performance    39
3.4  Design Concepts to Support Availability    41
3.5  Design Concepts to Support Modifiability    46
3.6  Design Concepts to Support Security    49
3.7  Design Concepts to Support Integrability    53
3.8  Summary    57
3.9  Further Reading    57
3.10  Discussion Questions    58
CHAPTER 4	
The Architecture Design Process 
59
4.1  The Need for a Principled Method    59
4.2  Attribute-Driven Design 3.0    60
4.3  Applying ADD to Different System Contexts    66
4.4  Identifying and Selecting Design Concepts    70
4.5  Producing Structures    74
4.6  Defining Interfaces    77
4.7  Creating Preliminary Documentation During Design    81
4.8  Tracking Design Progress    85
4.9  Summary    87
4.10  Further Reading    88
4.11  Discussion Questions    89
CHAPTER 5	
API-centric Design 
91
5.1  Business Agility    91
5.2  API-centric Design    93
5.3  API-centric Design and ADD    107
5.4  Summary    110
5.5  Further Reading    110
5.6  Discussion Questions    111
CHAPTER 6	
Designing for Deployability 
113
6.1  Deployability Principles and Architectural Design    113
6.2  Design Decisions to Support Deployability    119
6.3  Deployability and ADD    128

Contents  ix
6.4  Summary    129
6.5  Further Reading    130
6.6  Discussion Questions    130
CHAPTER 7	
Designing Cloud-Based Solutions 
133
7.1  Introduction to the Cloud    133
7.2  Drivers and the Cloud    136
7.3  Cloud-Based Design Concepts    139
7.4  ADD in the Design of Cloud-Based Solutions    147
7.5  Summary    148
7.6  Further Reading    148
7.7  Discussion Questions    149
CHAPTER 8	
Case Study: Hotel Pricing System 
151
8.1  Business Case    151
8.2  System Requirements    153
8.3  Development and Operations Requirements    156
8.4  The Software Design Process    158
8.5  Summary    188
8.6  Further Reading    189
8.7  Discussion Questions    189
CHAPTER 9	
Case Study: Digital Twin Platform 
191
9.1  Business Case    192
9.2  System Requirements    193
9.3  The Design Process    195
9.4  Summary    223
9.5  Further Reading    224
9.6  Discussion Questions    225
CHAPTER 10	
Technical Debt in Architectural Design 
227
10.1  Technical Debt    227
10.2  The Roots of Technical Debt in Design    228
10.3  Refactoring and Redesign    231
10.4  Technical Debt and ADD    233

x  Contents
10.5  Summary    235
10.6  Further Reading    236
10.7  Discussion Questions    237
CHAPTER 11	
Analysis in the Design Process  239
11.1  Analysis and Design    239
11.2  Why Analyze?    242
11.3  Analysis Techniques    243
11.4  Tactics-Based Analysis    244
11.5  Reflective Questions    246
11.6  Scenario-Based Design Reviews    247
11.7  Summary    250
11.8  Further Reading    251
11.9  Discussion Questions    251
CHAPTER 12	
The Architecture Design Process in the Organization 
253
12.1  Architecture Design and the Development Life Cycle    253
12.2  Architecture Design and the Organization    263
12.3  Summary    268
12.4  Further Reading    269
12.5  Discussion Questions    270
CHAPTER 13	
Final Thoughts 
273
13.1  On the Need for Methods    273
13.2  Future Directions    275
13.3  Next Steps    276
13.4  Further Reading    277
13.5  Discussion Questions    277
Appendix  Tactics-Based Questionnaires 
279
Index    301

xi
Preface
It has now been nearly 8 years since the first edition of Designing Software Architectures 
appeared. Much has changed in the world of technology since then—cloud architectures, 
Internet of Things (IoT) architectures, DevOps, the rise of artificial intelligence/machine 
learning (AI/ML), containers, microservices, and much more. Is our advice from 8 years ago 
still relevant? Well, yes and no.
The good news, from our perspective, is that the principles and practices of designing 
software architectures have not changed. The Attribute-Driven Design (ADD) method, which 
provides the scaffolding for the entire book, did not change at all. Over this 8-year span, it has 
been taught to thousands of practitioners and used successfully in many industrial projects. 
In all this time, the feedback has been positive and we have not had requests to change the 
method. That was reassuring. What did change, though, was the technical environment and the 
contexts in which we design today.
It is rare today that someone designs a standalone system. At the least, you will be build-
ing on top of existing frameworks and toolkits, and incorporating off-the-shelf components, 
some of which are likely open source. But you may be building a system that is interacting 
with other systems in real time, possibly sharing resources with them. It is likely that you are 
building a system using some kind of Agile methodology, and that your system will be fre-
quently modified, with regular updates and releases. It is possible that the architectures you 
build will include IoT devices, will be mobile, will be cloud-based and containerized, and will 
be adaptive. And it is also possible that the software you are working with is old, and has accu-
mulated technical debt over the years.
For all these reasons, we felt that it was time to write a second edition, one that acknowl-
edges the many new contexts in which architectural decisions are made today. In this new 
edition, we now have chapters on supporting business agility through API-centric design, on 
deployability, on cloud-based solutions, and on technical debt in design.
We also created two new case studies for this edition of the book, which build upon and 
explore the challenges in these new contexts. The Hotel Pricing System case study in Chapter 
8 implements much of its functionality in microservices, is deployed to a cloud infrastruc-
ture, is meant to be portable across environments, and was designed to be highly available.  
The digital twin platform case study presented in Chapter 9 explores even more challenges: 

xii  Preface
IoT, cloud computing, Big Data and analytics, AI/ML, extended reality (XR), simulation, 
advanced automation, and often robotics. This is a very large, complex system and requires 
the coordinated efforts of not just a single architect, but a large architecture team with varied 
expertise. In each case we show how the ADD method helps to transform design challenges 
into reality in a disciplined way. 
We sincerely hope that this book gives you the confidence you need to tackle any design 
challenge, no matter how large!
Register your copy of Designing Software Architectures, 2nd Edition, on the InformIT 
site for convenient access to updates and/or corrections as they become available. To start 
the registration process, go to informit.com/register and log in or create an account. Enter 
the product ISBN (9780138108021) and click Submit. Look on the Registered Products tab 
for an Access Bonus Content link next to this product, and follow that link to access any 
available bonus materials.

xiii
Acknowledgments
Almost nothing worth doing can or should be done alone. It definitely took a small village of 
supporters to get this book done. But in particular we want to acknowledge and thank the fol-
lowing people who really made a substantial contribution to the book.
Marty Barrett, Mario Benitez, Jeff Gitter, James Ivers, Andrew Kotov, Stefan Malich, and 
Ipek Ozkaya kindly reviewed and provided feedback on this new edition of the book. Thank 
you very much for your valuable and insightful comments.
Thank you Serge Haziyev, Yaroslav Pidstryhach, Rodion Myronov, Taras Bachynskyy, 
Lyubomyr Demkiv, and Martin Vesper, who kindly provided a new case study for this book. 
The team from SoftServe have been long-time supporters and contributors of ideas and feed-
back on our work, and we are very grateful for that.
I (Humberto) wish to thank Ricardo Ivison and Luis Castro for all the opportunities they 
have provided me in recent years to be a practicing architect. Thank you also to the architects, 
developers, and managers in different companies with whom I have had the opportunity to 
collaborate and exchange ideas. I have learned, and continue to learn, a lot from them. I also 
want to give recognition to my university, Universidad Autónoma Metropolitana Iztapalapa, 
as it has always supported my work. Thanks to my colleagues and students who have accom-
panied me across the years in this architectural journey. I also wish to thank my co-author 
Rick Kazman for being such a nice person and colleague; it is always a pleasure to work and 
exchange opinions with him. Finally, I want to thank all the people outside of the software 
development world who lovingly support, motivate, and inspire me in many different ways. 
Your presence in my life is enriching and invaluable.
I (Rick) wish to thank James Ivers and his group at the Software Engineering Institute. 
They have given me many opportunities to engage with real-world projects and I have always 
benefited greatly from these. I would like to thank my wife, who has always been supportive 
of this work, even though it often means I am sitting at my desk, staring into space, lost in 
thought! And I would like to thank my co-author Humberto, who has always been energetic, 
positive, and a true pleasure to work with.
Finally, we want to extend a special thank you to Serge Haziyev, who has always been so 
supportive of our work.

This page intentionally left blank 

xv
About the Authors
Humberto Cervantes
Humberto Cervantes is a practicing software architect and a professor at Universidad 
Autónoma Metropolitana Iztapalapa in Mexico City. His primary research interest is software 
architecture and, more specifically, the development of methods and tools to aid in the design 
process. He is active in promoting the adoption of these methods and tools in the software 
industry. Since 2006, Cervantes has collaborated with several software development compa-
nies as a consultant and a software architect. He has worked in several projects for differ-
ent industries, including telecommunications, hospitality, finance, and retail. He has authored 
numerous research papers and popularization articles, and has also co-authored one of the few 
books in Spanish on the topic of software architecture.
Cervantes received a master’s degree and a PhD from Université Joseph Fourier (now 
Université Grenoble Alpes) in France. He holds the Software Architecture Professional and 
ATAM Evaluator certificates from the Software Engineering Institute. Besides software engi-
neering, Cervantes enjoys spending time with his loved ones, playing with his dogs, exercising, 
and traveling.
Rick Kazman
Rick Kazman is the Danny and Elsa Lui Distinguished Professor of Information Technology 
Management at the University of Hawaii and a Visiting Researcher at the Software Engineering 
Institute of Carnegie Mellon University. His primary research interests are software architec-
ture, design and analysis tools, software visualization, and technical debt. Kazman has been 
involved in the creation of several highly influential methods and tools for architecture anal-
ysis, including the ATAM (Architecture Tradeoff Analysis Method) and the Titan and DV8 
tools. He is the author of more than 250 publications, co-author of three patents and nine 
books, including Software Architecture in Practice, Technical Debt: How to Find It and Fix It, 
Designing Software Architectures: A Practical Approach, Evaluating Software Architectures: 
Methods and Case Studies, and Ultra-Large-Scale Systems: The Software Challenge of the 
Future. His methods and tools have been adopted by many Fortune 1000 companies and has 
been cited more than 30,000 times, according to Google Scholar. He is currently a member 
of the IEEE Computer Society’s Board of Governors, and a member of the ICSE Steering 
Committee.

xvi  About the Authors
Kazman received a B.A. (English/music) and M.Math (computer science) from the 
University of Waterloo, an M.A. (English) from York University, and a Ph.D. (computational 
linguistics) from Carnegie Mellon University. How he ever became a software engineering 
researcher is anybody’s guess. When not doing architecture things, Kazman may be found 
cycling, singing acapella music, gardening, playing the piano, or practicing Tae Kwon Do.

1
1
Introduction
In this chapter we provide an introduction to the topic of software architecture and architecture  
design. We briefly discuss what architecture is and why it is fundamental to take it into account 
when developing software systems. We also discuss the activities that are associated with the 
development of software architecture so that architectural design—which is the primary topic 
of this book—can be understood in the context of these activities. We also briefly discuss the 
role of the architect, who is the person responsible for creating the design. Finally, we intro-
duce the Attribute-Driven Design (ADD) method, the architecture design method that we will 
discuss extensively in this book.
1.1  Motivations
Our goal in this book is to teach you how to design software architecture in a systematic, pre-
dictable, repeatable, and cost-effective way. If you are reading this book, then presumably you 
already have an interest in architecture and aspire to be an architect. The good news is that this 
goal is within your grasp. To convince you of that point, we will spend a few moments talking 
about the idea of design—the design of anything—and we will see how and why architectural 
design relates to that broader concept. In most fields, “design” involves the same sorts of chal-
lenges and considerations—meeting stakeholder needs, adhering to budgets and schedules, 
dealing with constraints, working with available materials, and so forth. While the primitives 
and tools of design may vary from field to field, the goals and steps of design do not.
This is encouraging news, because it means that design is not the sole province of wizards 
or highly skilled craftsmen. That is, design can be taught, and it can be learned. Most design, 
particularly in engineering, consists of putting known design primitives together in (sometimes 
innovative) ways that achieve predictable outcomes. Of course, the devil is in the details—and 
that is why we have methods. It may seem difficult at first to imagine that a creative endeavor 
such as design can be captured in a step-by-step method; in fact, this is not only possible but 
also valuable, as Parnas and Clements discussed in their paper “A Rational Design Process: 
How and Why to Fake It”. Of course, not everyone can be a great designer, just as not everyone 
can be a Thomas Edison or a LeBron James or a Frank Gehry. What we do claim is that every-
one can be a much better designer, and that structured methods supported by reusable chunks 

2  Chapter 1  Introduction
of design knowledge, which we provide in this book, can help pave the road from mediocrity 
to excellence and from a craft to a discipline of engineering.
Our goal in writing this book is to provide a practical method that can be enacted by any 
competent software engineer, and also (and just as important) to provide a set of rich case stud-
ies that realize the method. Albert Einstein was reputed to have said, “Example isn’t another 
way to teach, it is the only way to teach”. We firmly believe that. Most of us learn better from 
examples than from sets of rules or steps or principles. Of course, we need the steps and rules 
and principles to structure what we do and to create the examples, but the examples speak to 
our day-to-day concerns and help us by making the steps concrete. Experienced architects may 
not need to follow these steps precisely, but it is important to have them detailed so that novice 
architects can be guided in the design process.
This is not to say that architecture design will ever be simple. If you are building a com-
plex system, the chances are that you are trying to balance many competing demands—time 
to market, cost, performance, evolvability, usability, availability, security, and so on. If you are 
pushing the boundaries in any of these dimensions, then your job as an architect will be even 
more complex. This is true in any engineering discipline, not just software. If you examine the 
history of building large ships or skyscrapers or any other complex “system”, you will see how 
the architects of those systems struggled with making the appropriate decisions and tradeoffs. 
No, architecture design may never be easy, but our aim is to make it tractable and achievable 
by well-trained, well-educated software engineers.
1.2  Software Architecture
Much has been written about what software architecture is. We adopt the definition of  
software architecture from Software Architecture in Practice (fourth edition):
The software architecture of a system is the set of structures needed to reason about  
the system. These structures comprise software elements, relations among them, and 
properties of both.
As you will see, our design method embodies this definition and helps to guide the 
designer in creating an architecture that has the desired properties.
1.2.1  The Importance of Software Architecture
Much has also been written about why architecture is important. Again, following Software 
Architecture in Practice, we note that architecture is important for a wide variety of reasons, 
and a similarly wide variety of consequences stem from those reasons:
■
■An architecture will inhibit or enable a system’s driving quality attributes.
■
■The decisions made in an architecture allow you to reason about and manage change as 
the system evolves.

1.2  Software Architecture  3
■
■The analysis of an architecture enables early prediction of a system’s qualities.
■
■A documented architecture enhances communication among stakeholders.
■
■The architecture is a carrier of the earliest and hence most fundamental, hardest- 
to-change design decisions.
■
■An architecture defines a set of constraints on subsequent implementation.
■
■The architecture influences the structure of an organization, and vice versa.
■
■An architecture can provide the basis for evolutionary, or even throwaway, prototyping.
■
■An architecture is the key artifact that allows the architect and the project manager to 
reason about cost and schedule.
■
■An architecture can be created as a transferable, reusable model that forms the heart of a 
product line.
■
■Architecture-centric development focuses attention on the assembly of components, 
rather than simply on their creation.
■
■By restricting design alternatives, architecture channels the creativity of developers, 
reducing design and system complexity.
■
■An architecture can be the foundation for training a new team member.
If an architecture is important for all of these reasons—if it affects the structure of  
the organization, and the qualities of the system, and the people involved in its creation and 
evolution—then surely great care must be taken in designing this crucial artifact. Sadly, that 
is most often not the case. Architectures often “evolve” or “emerge”. While evolution is typi-
cally a given and while emergence might be necessary as requirements congeal, and while we 
emphatically are not arguing for “big design up front”, doing no architecture at all is often too 
risky for anything but the simplest projects or products. Would you want to drive over a bridge 
or ride in a jet that had not been carefully designed? Of course not. But you use software every 
day that is buggy, costly, insecure, unreliable, fault prone, and slow—and many of these unde-
sirable characteristics can be easily avoided!
The core message of this book is that architecture design does not need to be difficult or 
scary; it is not the sole province of wizards; and it does not have to be costly and all done up 
front. Our job is to show you how and convince you that it is within your reach.
1.2.2  Life-Cycle Activities
Software architecture design is one of the software architecture life-cycle activities  
(Figure 1.1). As in any software project life cycle, this activity is concerned with the translation 
of requirements into a design, and from there into an implementation. Specifically, the archi-
tect needs to worry about the following aspects:
■
■Architectural requirements. Among all the requirements, a few will have a particular 
importance with respect to the software architecture. These architecturally significant 
requirements (ASRs) include not only the most important functionality of the system and 
the constraints that need to be taken into account, but also—and most importantly—
quality attributes such as high performance, high availability, ease of evolution, and 

4  Chapter 1  Introduction
iron-clad security. These requirements, along with a clear design purpose and other 
architectural concerns that may never be written down or may be invisible to external 
stakeholders, will guide you to choose one set of architectural structures and compo-
nents over another. We will refer to these ASRs, constraints, and concerns as drivers, as 
they can be said to drive the design.
■
■Architectural design. Design is a translation, from the world of needs (requirements) to 
the world of solutions, in terms of structures composed of modules, frameworks, and 
components. A good design is one that satisfies the drivers. This topic is the focus of  
the book.
■
■Architectural documentation. Some level of preliminary documentation (or sketches) of 
the structures should be created as part of architectural design. This activity, however, 
refers to the creation of a more formal document from these sketches. If the project or 
product is small and well understood, then architecture documentation may be minimal. 
In contrast, if the project is large, if distributed teams are collaborating, if it is expected 
to survive for a long time, or if significant technical challenges exist, then architectural 
documentation will repay the effort invested in this activity. Although documentation is 
often avoided and derided by programmers, it is a standard, non-negotiable deliverable in 
almost every other engineering discipline. If your system is big enough and if it is mis-
sion critical, it should be documented. In other engineering disciplines, a “blueprint”—
some sort of documented design—is an absolutely essential step in moving toward 
implementation and the commitment of resources.
■
■Architectural evaluation. As with documentation, if your project is nontrivial, then 
you owe it to yourself and to your stakeholders to evaluate it—that is, to ensure that 
the architectural decisions that have been made are appropriate to address the critical 
requirements. Would you deliver code without testing it? Of course not. Similarly, why 
would you commit enormous resources to fleshing out an architecture if you had not 
first “tested” the design? You might want to do this when first creating the system, or 
when evolving it, or when putting it through a major refactoring. Most commonly, evalu-
ation is done informally and internally, but for truly important projects it is advisable to 
have a formal evaluation done by an external team.
■
■Architectural implementation/conformance checking. Finally, you need to implement 
the architecture that you have created (and evaluated). As an architect, you may need 
to tweak the design as the system grows and as requirements emerge and evolve. This 
is normal. In addition to this tweaking, your major responsibility during implementa-
tion is to ensure conformance of the code to the design. If developers are not faithfully 
implementing the architecture, they may be undermining the qualities that you have 
carefully designed in. Again, consider what is done in other fields of engineering. When 
a concrete foundation for a new building is poured, the building that rests on top of that 
foundation is not constructed until the foundation has first been tested, typically via a 
core sample, to ensure that it is strong enough, dense enough, sufficiently impermeable 
to water and gases, and so forth. Without conformance checking, we have no way of 
ensuring the quality of what is being subsequently constructed.

1.2  Software Architecture  5
Architectural
Requirements
Architectural
Design
Architectural
Documentation
Architectural
Evaluation
<<precedes>>
<<precedes>>
<<precedes>>
<<precedes>>
<<precedes>>
<<inﬂuences>>
Architectural
Implementation
Focus of the
book
FIGURE 1.1  Software architecture life-cycle activities
Note that we are not proposing a specific life-cycle model in Figure 1.1. The stereotype 
<<precedes>> simply means that some effort in an activity must be performed, and hence pre-
cede, effort in a later activity. For example, you cannot (or at least should not) perform design 
activities if you have no idea about the requirements, and you cannot evaluate an architecture 
if you have not first made some design decisions.
Today most commercial software is developed using some form of Agile methodology. 
None of these architecture activities is incompatible with Agile principles and practices. The 
question for a software architect is not “Should I do Agile or architecture?”, but rather “How 
much architecture should I do up front versus how much should I defer until the project’s 
requirements have solidified somewhat?” and “How much of the architecture should I for-
mally document, and when?” Agile and architecture are happy companions for many software 
projects.
We will discuss the relationship between architecture design and business agility in 
Chapter 5. In addition, we will discuss the place of architecture design in an organizational 
context in Chapter 12.

6  Chapter 1  Introduction
1.3  The Role of the Architect
An architect is much more than “just” a designer. This role, which may be filled by one or 
more individuals, has a long list of duties, skills, and knowledge that must be satisfied if it is to 
be successful. These prerequisites include the following:
■
■Leadership: mentoring, team-building, establishing a vision, coaching
■
■Communication: both technical and nontechnical, encouraging collaboration
■
■Negotiation: dealing with internal and external stakeholders and their conflicting needs 
and expectations
■
■Technical skills: life-cycle skills, expertise with technologies, continuous learning, 
coding
■
■Project skills: budgeting, personnel, schedule management, risk management (although 
these duties are often shared with a project manager)
■
■Analytical skills: architectural analysis, general analysis mindset for project management 
and measurement (see the sidebar “The Meaning of Analysis”)
A successful design is not a static document that is “thrown over the wall”. That is, archi-
tects must not only design well, but must also be intimately involved in every aspect of the 
product or project, from conception and business justification to design and creation, through 
to operation, maintenance, and eventually retirement. 
The Meaning of Analysis
In the Longman Dictionary, the word analysis is defined as follows:
■
■
A careful examination of something in order to understand it better
■
■
The way in which someone describes a situation or problem, and says what causes  
it to happen
In this book we use the word analysis for different purposes, and both of these defi-
nitions apply. For instance, as part of the architectural evaluation activity, an existing 
architecture is analyzed to gauge if it is appropriate to satisfy its associated drivers. 
During the design process, the inputs are analyzed to make design decisions. The 
creation of prototypes is also a form of analysis. In fact, analysis is so important to the 
design process that we devote an entire chapter (Chapter 11) to just this topic.
In this book, we focus primarily on the design activity, its associated technical skills, 
and its integration into the development life cycle. We will discuss the many roles of the 
architect in Chapter 12. For a fuller treatment of the other aspects of an architect’s life, 
we invite you to read a more general book on software architecture, such as Software 
Architecture in Practice.

1.4  A Brief History of ADD  7
1.4  A Brief History of ADD
Although an architect has many duties and responsibilities, in this book we focus on what 
is perhaps the single most important skill that a software engineer must master to be called 
“architect”: the process of design. To make architectural design more tractable and repeatable, 
in this book we focus most of our attention on the Attribute-Driven Design (ADD) method, 
which provides step-by-step guidance on how to iteratively perform the design activity shown 
in Figure 1.1. Chapter 4 describes the most recent version of ADD, version 3.0, in detail, so 
here we provide a bit of background for those who are familiar with previous versions of ADD. 
The first version of ADD (ADD 1.0, originally called ABD, for “Architecture-Based Design”) 
was published in January 2000, and the second version (ADD 2.0) was published in November 
2006. Version 3.0 was published in 2016, in the first edition of this book.
ADD is, to our knowledge, the most comprehensive and most widely used documented 
architecture design method. When ADD appeared, it was the first design method to focus spe-
cifically on quality attributes and their achievement through the creation of architectural struc-
tures and their representation through views. Another important contribution of ADD is that it 
includes architecture analysis and documentation as an integral part of the design process. In 
ADD, design activities include refining the sketches created during early design iterations to 
produce a more detailed architecture, and continuously evaluating the design.
Although ADD 2.0 was useful for linking quality attributes to design choices, it had sev-
eral shortcomings that needed to be addressed. For example, it did not clearly relate patterns 
and tactics to implementation technologies (such as components and frameworks), it did not 
offer architects guidance on how to weave together architectural design and Agile practices, 
and it gave little indication about how to begin the design process.
ADD 3.0 addresses these shortcomings. To be sure, ADD 3.0 is evolutionary, not revolu-
tionary. It was catalyzed by the creation of ADD 2.5,1 which was itself a reaction to attempting 
to use ADD in the real world, in many different contexts.
We published ADD 2.5 in 2013. In that work, we advocated the use of application 
frameworks, such as Spring or Hibernate, as first-class design concepts. This guidance was 
intended to address ADD 2.0’s shortcoming of being too abstract to apply easily. ADD starts 
with drivers, systematically links them to design decisions, and then links those decisions to 
the available implementation options, including externally developed components. For Agile 
development, ADD 3.0 promotes quick design iterations in which a small number of design 
decisions are made, potentially followed by an implementation effort. In addition, ADD 3.0 
explicitly promotes the (re)use of reference architectures and promotes the adoption of a 
“design concepts catalog”, which includes a broad selection of tactics, patterns, frameworks, 
reference architectures, and technologies.
Since the first version of this book was published, the field of architectural design has 
continued to evolve. During this time, ADD 3.0 has been taught and used extensively in indus-
try. Throughout this experience we have not found any compelling reason to change it. This is 
a good thing: Methods should be general and stable.
1.  This is our own coding notation; the 2.5 number is not used elsewhere.

8  Chapter 1  Introduction
And so, while we are not updating the steps of the method itself, we felt the need to update 
the first edition of the book to reflect the changes surrounding ADD. What has changed are 
the ways that ADD can be used with contemporary architectural practices. Architects design-
ing systems today are focusing on aspects such as the cloud, DevOps, and technical debt.  
In addition, in this edition we have emphasized the design of APIs for distributed systems.
1.5  Summary
Having covered our motivations and background, we now move on to the heart and soul of 
this book. In the next few chapters, we describe what we mean by design and by architectural 
design in particular, we discuss ADD, and we provide two case studies showing in detail how 
ADD can be used in the real world. We also discuss the use of ADD in contemporary practices 
including cloud development, API-centric design agility, and DevOps. Moreover, we describe 
the critical role that analysis plays in the design process and provide examples of how analy-
sis can be performed on design artifacts and how the design process fits in an organizational 
context.
1.6  Further Reading
Fred Brooks has written a thoughtful series of essays on the nature of design, reflecting his 
50 years of experience as a designer and researcher: F. P. Brooks, Jr., The Design of Design: 
Essays from a Computer Scientist, Addison-Wesley, 2010.
The usefulness of having a documented process for design and other development 
activities has been recognized for decades. This is discussed in D. Parnas and P. Clements, 
“A Rational Design Process: How and Why to Fake It”. IEEE Transactions on Software 
Engineering, SE-12, 2, February 1986.
The definition of software architecture used here, as well as the arguments for the  
importance of architecture and the role of the architect, are derived from L. Bass, P. Clements, 
and R. Kazman, Software Architecture in Practice, 4th ed., Addison-Wesley, 2021.
An early reference for the first version of ADD can be found in F. Bachmann, L. Bass, 
G. Chastek, P. Donohoe, and F. Peruzzi, The Architecture Based Design Method, CMU/
SEI-2000-TR-001. The second version of ADD was described in R. Wojcik, F. Bachmann, 
L. Bass, P. Clements, P. Merson, R. Nord, and W. Wood, Attribute-Driven Design (ADD), 
Version 2.0, CMU/SEI-2006-TR-023. The version of ADD that we refer to here as ADD 2.5 
was published in H. Cervantes, P. Velasco-Elizondo, and R. Kazman, “A Principled Way of 
Using Frameworks in Architectural Design”, IEEE Software, 46–53, March/April 2013.

1.7  Discussion Questions  9
1.7  Discussion Questions
1.	
Consider the architecture of buildings and software architecture. What are similarities 
and differences between these disciplines in terms of how they elicit requirements, the 
quality attributes they focus on, and how they make design decisions?
2.	
If you were to design the architecture of a software system, what steps would you follow? 
Would these steps change based on the complexity of that system?
3.	
Consider a system that is built by a team of developers, none of whom has knowledge of 
architectural design. They build the system as best they can. Does this system have an 
architecture?
4.	
Consider the definition of a software architecture given in Section 1.4. What type of 
elements and relations can form the structures of a software system?

This page intentionally left blank 

11
2
Architectural Design
We now dive into our discussion of architecture design, a complex and much misunderstood 
topic. We will spend time talking about what it is, why it is important, how it works (at an 
abstract level), and which major concepts and activities it involves. We will also discuss archi-
tectural drivers: the various factors that “drive” design decisions, some of which may be docu-
mented as requirements, but many of which will not be. 
Why Read This Chapter?
Design is the core concept of this book. Designs can be arbitrarily complex but you 
can’t possibly deal with all of the complexity all of the time. This chapter gives you a way 
to think about design decisions at different levels of abstraction, which is key to taming 
the complexity. Also, you may not intuitively consider all dimensions of drivers. Here we 
give you a framework for thinking about drivers; we will continue to use this framework 
throughout the rest of the book.
2.1  Design in General
Design is both a verb and a noun. Design is a process, an activity, and hence a verb. This 
process results in the creation of a design—a description of a desired state. The output of the 
design process is the thing, the noun, the artifact that you will eventually implement. Designing 
means making decisions to achieve goals and to satisfy requirements and constraints. The out-
puts of the design process are a direct reflection of those goals, requirements, and constraints. 
Think about houses, for example. Why do traditional houses in China look different from 
those in Switzerland or Algeria? Why does a yurt look like a yurt, which is different from an 
igloo or a chalet or a longhouse?
The architectures of these styles of houses have evolved over the centuries to reflect their 
unique sets of goals, requirements, and constraints. Houses in China feature symmetric enclo-
sures, sky wells to increase ventilation, south-facing courtyards to collect sunlight and provide 
protection from cold north winds, and so forth. A-frame houses have steep pitched roofs that 

12  Chapter 2  Architectural Design
extend to the ground, meaning minimal painting and protection from heavy snow loads (which 
just slide off to the ground). Igloos are built of ice, reflecting the availability of ice, the relative 
poverty of other building materials, and the constraints of time (a small one can be built in an 
hour).
In each case, the process of design involves the selection and adaptation of a number of 
solution approaches. For example, igloo designs can vary. Some are small and meant to serve 
as a temporary travel shelter. Others are large, often connecting several structures, meant for 
entire communities to meet. Some are simple unadorned snow huts. Others are lined with furs, 
with ice “windows” and doors made of animal skin.
The process of design, in each case, balances the various “forces” facing the designer. 
Some designs require considerable skill to execute (such as carving and stacking snow blocks 
in such a way that they produce a self-supporting dome). Others require relatively little skill—a 
simple lean-to can be constructed from branches and bark by almost anyone. But the quali-
ties that these structures exhibit may also vary considerably. Lean-tos provide little protection 
from the elements and are easily destroyed by high winds or fire, whereas an igloo can with-
stand Arctic storms and support the weight of a person standing on the roof.
Is design “hard”? Well, yes and no. Novel design is hard. It is pretty clear how to design 
a conventional bicycle, but the designs for the Segway and the OneWheel broke new ground. 
Fortunately, most design is not novel, because most of the time our requirements are not novel. 
Most people want a bicycle that will reliably convey them from place to place. Most people 
living in Phoenix want a house that can be easily and economically kept cool, whereas most 
people in Edmonton are primarily concerned with a house that can be kept warm. In contrast, 
people living in Tokyo and Mexico City are often concerned with buildings that can withstand 
serious earthquakes.
The good news for you, the architect, is that there are ample proven designs and design 
fragments, or building blocks that we call design concepts, that can be reused and combined to 
reliably achieve these goals. If your design is truly novel—if you are designing the next Sydney 
Opera House—then the design process will likely be “hard”. The Sydney Opera House, for 
example, cost nearly 14 times its original budget estimate and was delivered 10 years late. So, 
too, with the design of software architectures.
2.2  Design in Software Architecture
Architectural design for software systems is no different than design in general: It involves 
making decisions, working with available skills and materials, to satisfy requirements and 
constraints. In architectural design, we make decisions to transform our design purpose, pri-
mary functional requirements, quality attribute requirements, constraints, and architectural 
concerns—what we call the architectural drivers—into structures, as shown in Figure 2.1. 
These structures, which are mentioned in the definition of software architecture that we gave 
in Chapter 1, are then used to guide the project. They guide analysis and construction, and 
serve as the foundation for educating a new project member. They also guide cost and schedule 
estimation, team formation, risk analysis and mitigation, and, of course, implementation.

2.2  Design in Software Architecture  13
Architectural Drivers
<<uses>>
<<produces>>
<<selects and 
instantiates>>
The Architect
Candidate 
design
decisions
Design Concepts
Primary Functionality
Constraints
Architectural Concerns
Quality Attributes
Design Purpose
Structures
Module
Uses
Decomposition
Layered
Class/Generalization
...
Component-and-Connector
Client-Server
Concurrency
Process
Shared-Data
...
Allocation
Work Assignment
Deployment
Implementation
...
FIGURE 2.1  Overview of the architecture design activity
Architectural design is, therefore, a key step to achieving your product and project goals. 
Some of these goals are technical (e.g., achieving low and predictable latency in a video game 
or an e-commerce website), and some are nontechnical (e.g., supporting multiple lines of busi-
ness, keeping your existing team employed, entering a new market, meeting a deadline). The 
decisions that you, as an architect, make will have implications for the achievement of these 
goals and may, in some cases, be in conflict. The choice of a particular reference architecture 
(e.g., the Lambda architecture) may provide a good foundation for achieving your latency and 
throughput goals and will keep your workforce employed because they are already familiar 
with that reference architecture and its supporting technology stacks. But this choice will not 
help you enter a new market—mobile games, for example.
In general, during the design process, a change in some structure to achieve one quality 
attribute will have negative effects on other quality attributes. These tradeoffs are a fact of 
life for every practicing architect in every domain. We will see this over and over again in 
the examples and case studies provided in this book. In essence, the architect’s job is not one 
of finding an optimal solution, but rather one of satisficing—searching through a potentially 
large space of design alternatives and decisions until an acceptable solution is found.
2.2.1  Architectural Design
Grady Booch has said, “All architecture is design, but not all design is architecture”. What 
makes a decision “architectural”? A decision is architectural if it has non-local consequences 
and those consequences matter to the achievement of an architectural driver. No decision is, 
therefore, inherently architectural or non-architectural. The choice of a buffering strategy 

14  Chapter 2  Architectural Design
within a single element may have little effect on the rest of the system, in which case it is an 
implementation detail that is of no concern to anyone except the implementer or maintainer of 
that element. In contrast, the buffering strategy may have enormous implications for overall 
system performance (if the buffering affects the achievement of latency or throughput or jitter 
goals) or availability (if the buffers might not be large enough and information gets lost) or 
modifiability (if we wish to flexibly change the buffering strategy in different deployments 
or contexts). The choice of a buffering strategy, like most design choices, is neither inherently 
architectural nor inherently non-architectural. Instead, this distinction is completely dependent 
on the current and anticipated architectural drivers.
2.2.2  Element Interaction Design
Architectural design identifies and defines the major elements and important relationships that 
make up the overall structure of a system. The major elements are often abstract or existing 
design concepts (such as architectural patterns) that address primary functional requirements, 
quality attribute requirements, constraints, and architectural concerns. Architectural design 
generally results in the identification of only a subset of the elements that are part of the sys-
tem’s structure. This is to be expected because, during initial architectural design, the architect 
will focus on the primary functionality of the system.
What makes a use case primary? A combination of business importance, risk, and com-
plexity considerations feed into this designation. Of course, to the typical user almost every-
thing will be urgent and the number one priority. More realistically, a small number of user 
stories provide the most fundamental business value or represent the greatest risk (if they are 
done wrong), so these are deemed primary. Every system has many more user stories, beyond 
the primary ones, that need to be satisfied. The elements that support these nonprimary user 
stories and their interfaces are part of what we call element interaction design. This level of 
design usually follows the overall architectural design. The precise locations and relationships 
of these elements, however, are constrained by the decisions that were made during architec-
tural design. These elements can be units of work (i.e., modules or services) assigned to an 
individual or to a team, so this level of design is important not only for defining how non-
primary functionality is allocated, but also for planning purposes (e.g., team formation and 
communication, budgeting, outsourcing, release planning, unit and integration test planning).
Depending on the scale and complexity of the system, the architect should be involved in 
element interaction design, either directly or in an auditing role. This involvement ensures that 
the system’s important quality attributes are not compromised—for example, that the elements 
are not defined, located, or connected incorrectly. It also helps the architect spot opportunities 
for generalization.
2.2.3  Element Internals Design
A third level of design follows element interaction design, which we call element internals 
design. In this level of design, which is usually addressed as part of the element development 

2.2  Design in Software Architecture  15
activities, the internals of the elements identified in the previous design level are established so 
as to satisfy the element’s interface.
2.2.4  Decisions and Design Levels
Architectural decisions can and do occur at the three levels of design. Moreover, during archi-
tectural design, the architect may need to delve as deeply as element internals design to achieve 
a particular architectural driver. An example is the design of the internals of a microservice 
following a layered architecture. The data layer of the microservice may need to be carefully 
designed so that it does not contribute excessive latency or lock important parts of its database. 
In this sense, architectural design can, at times, involve considerable detail, which explains 
why we do not like to talk about it in terms of “high-level design” or “detailed design” (see the 
sidebar “High-Level vs. Detailed Design?”).
Architectural design precedes element interaction design, which precedes element inter-
nals design. This is logically necessary: You cannot design an element’s internals until you 
have defined the elements themselves, and you cannot usefully reason about interactions until 
you have defined several elements and some patterns of interactions among them. But as proj-
ects or products grow and evolve, there is, in practice, considerable iteration between these 
activities. 
High-Level vs. Detailed Design?
The term “detailed design” is often used to refer to the design of the internals of mod-
ules. Although it is widely used, we really don’t like this term, which is presented as 
somehow in opposition to “high-level design”. We prefer the more precise terms “archi-
tectural design”, “element interaction design”, and “element internals design”.
After all, architectural design may be quite detailed, if your system is complex. And 
some design “details” will turn out to be architectural. For the same reason, we also 
don’t like the terms “high-level design” and “low-level design”. Who can really know what 
these terms mean? Clearly, “high-level design” should be somehow “higher” or more 
abstract, and cover more of the architectural landscape than “low-level design”, but 
beyond that we are at a loss to imbue these terms with any precise meaning.
So here is what we recommend: Just avoid using terms such as “high”, “low”, or 
“detailed” altogether. There is always a better, more precise choice, such as “architec-
tural”, “element interaction”, or “element internals” design!
Think carefully about the impact of the decisions you are making, the information that 
you are trying to convey in your design documentation, and the likely audience for that 
information, and then give that process an appropriate, meaningful name.

16  Chapter 2  Architectural Design
2.3  Why Is Architectural Design So Important?
Although premature commitment can be wasteful, and sometimes “you ain’t gonna need 
it”, there is a very high cost to a project of not making important design decisions, or of not 
making them early enough. This manifests itself in many different ways. Early on, an initial 
architecture is critical for estimation in project proposals. Without doing some architectural 
thinking and some early design work, you cannot confidently predict project cost, schedule, 
and quality. Even at this early stage, an initial architecture will determine the key approaches 
for achieving architectural drivers, the gross work-breakdown structure, and the choices of 
tools, skills, and technologies needed to realize the system.
In addition, architecture is a key enabler of agility, as we will discuss in Chapter 12. 
Whether your organization has embraced Agile processes or not, it is difficult to imagine any-
one who would willingly choose an architecture that is brittle and hard to change or extend 
or tune—and yet it happens all the time. This kind of technical debt (which we discuss in 
Chapter 10) occurs for a variety of reasons, but paramount among these is the combination of 
a focus on features—typically driven by stakeholder demands—and the inability of architects 
and project managers to measure the return on investment of good architectural practices. 
Features provide immediate benefit. Architectural improvement provides immediate costs 
and, if done right, long-term benefits. Put this way, why would anyone ever “invest” in archi-
tecture? The answer is simple: Without architecture, the benefits that the system is supposed to 
bring will be far harder to realize; velocity will slow and the system will become increasingly 
more difficult to debug, fix, scale, and evolve.
Simply put, if you do not make some key architectural decisions early on, and if you 
allow your architecture to degrade, you will be unable to maintain sprint velocity, because 
you cannot easily respond to change requests. We vehemently disagree with what the original 
creators of the Agile Manifesto claimed: “The best architectures, requirements, and designs 
emerge from self-organizing teams”. Indeed, our demurral with this point is precisely why we 
have written this book. Good architectural design is difficult (and still rare), and it does not 
just “emerge”. This opinion mirrors a growing consensus within the Agile community. More 
and more, we see techniques such as “disciplined agility at scale”, the “walking skeleton”, 
and the “Scaled Agile Framework” embraced by Agile thought leaders and practitioners alike. 
Each of these techniques advocates some architectural thinking and design prior to much, if 
any, development. To reiterate, architecture enables agility, and not the other way around.
Furthermore, the architecture will influence, but not determine, other decisions that are 
not in and of themselves design decisions. These decisions do not influence the achievement of 
quality attributes directly, but still need to be made by the architect. For example, such deci-
sions may include selection of tools, structuring the development environment, and making 
work assignments.
Finally, a well-designed, properly communicated architecture is key to achieving agree-
ments that will guide the team. The most important kinds of consensus to reach are agree-
ments on interfaces and on shared resources. Agreeing on interfaces early is important for 
component-based development, and critically important for distributed development using 
approaches such as microservices. These decisions will be made sooner or later. If you don’t 

2.4  Architectural Drivers  17
make them early in the design process, the system will be much more difficult to integrate. 
In Section 4.6, we will discuss how to define interfaces as part of architectural design—both 
the external interfaces to other systems and the internal interfaces that mediate your element 
interactions.
2.4  Architectural Drivers
Before commencing design with ADD (or with any other design approach, for that matter), you 
need to think about what you are doing and why. Although this statement may seem blindingly 
obvious, the devil is, as usual, in the details. We categorize these “what” and “why” questions 
as architectural drivers. As shown in Figure 2.1, these drivers include a design purpose, qual-
ity attributes, primary functional requirements, architectural concerns, and constraints. These 
considerations are critical to the success of the system and, as such, they drive and shape the 
architecture.
As with any other important requirements, architectural drivers need to be baselined and 
managed throughout the development life cycle.
2.4.1  Design Purpose
First, you need to be clear about the purpose of the design that you want to achieve. When and 
why are you doing this architecture design? Which business goals is the organization most 
concerned about at this time?
1.	
You may be doing architecture design as part of a project proposal (for the estimation 
process in a consulting organization, or for internal project selection and prioritization in 
a company, as discussed in Section 12.1.1). It is not uncommon that, as part of determin-
ing project feasibility, schedule, and budget, an initial architecture is created. Such an 
architecture would not be very detailed; the goal is to understand and break down the 
architecture in sufficient detail that the units of work are understood and hence may be 
estimated.
2.	
You may be doing architecture design as part of the process of creating an exploratory 
prototype. In this case, the purpose of the architecture design process is not so much to 
create a releasable or reusable system, but rather to explore the domain, to explore new 
technology, to place something executable in front of a customer to elicit rapid feed-
back, or to explore some quality attribute (such as latency or performance scalability or 
failover for availability).
3.	
You may be designing your architecture during development. This could be for a 
complete new system, for a substantial portion of a new system, or for a portion of an 
existing system that is being refactored or replaced. In this case, the purpose is to do just 
enough design work to satisfy requirements, guide system construction and work assign-
ments, and prepare for an eventual release.

18  Chapter 2  Architectural Design
In a mature domain, the estimation process might be relatively straightforward; the architect  
can reuse existing systems as examples and confidently make estimates based on analogy. In 
novel domains, the estimation process will be far more complex and risky, and may have highly 
variable results. In these circumstances, a prototype of the system, or a key part of the system, 
may need to be created to mitigate risk and reduce uncertainty. In many cases, this architecture 
may also need to be quickly adapted as new requirements are learned and embraced. In brown-
field systems, where the requirements are better understood, the existing system is itself a com-
plex artifact that must be well understood for planning to be accurate.
The development organization’s goals during development or maintenance may also affect 
the architecture design process. For example, the organization might be interested in design-
ing for reuse, designing for future extension or subsetting, designing for scalability, designing 
a product for continuous delivery, designing to best utilize existing project capabilities and 
team member skills, and so forth. Or the organization might have a strategic relationship with 
a vendor. Or the CTO might have a specific technology preference and wants to impose it on 
your project.
Why do we bother to list these considerations? Because they will affect both the pro-
cess of design and the outputs of design. Architectures exist to help achieve business goals. 
The architect should be clear about these goals and should communicate them (and negotiate 
them!) and establish a clear design purpose before beginning the design process.
2.4.2  Quality Attributes
In the book Software Architecture in Practice, quality attributes are defined as being measur-
able or testable properties of a system that are used to indicate how well the system satisfies 
the needs of its stakeholders. Because the term quality, as it is informally used, tends to be a 
squishy term, these properties allow quality to be expressed succinctly and objectively.
Among the drivers, quality attributes are the ones that most strongly shape the architec-
ture. The critical choices that you make when you are doing architectural design determine, in 
large part, the ways that your system will or will not meet these driving quality attribute goals.
Given their importance, you must worry about eliciting, specifying, prioritizing, and 
validating quality attributes. Given that so much depends on getting these drivers right, this 
sounds like a daunting task. Fortunately, a number of well-understood, widely disseminated 
practices can help you here (see the sidebar “The Quality Attribute Workshop and the Utility 
Tree” for a more detailed discussion of the first two techniques):
■
■A Utility Tree, the simplest technique, can be used by the architect to quickly prioritize 
quality attribute requirements according to their technical difficulty and risk.
■
■The Quality Attribute Workshop (QAW), a bit more costly and complex, is a facilitated 
brainstorming session involving a group of system stakeholders that covers the bulk of 
the activities geared toward eliciting, specifying, prioritizing, and achieving consensus 
on quality attributes.
■
■The Mission Thread Workshop serves the same purpose as QAW, but for a system of sys-
tems. It is typically the most costly, most complex method. However, if you are building 
a system of systems, the greater effort and expense are probably worth it.

2.4  Architectural Drivers  19
The best way to discuss, document, and prioritize quality attribute requirements is as a set 
of scenarios. A scenario, in its most basic form, describes the system’s response to some stimulus. 
Why are scenarios the best approach? Because all other approaches are worse! Endless time may 
be wasted in defining terms such as “performance” or “scalability” or “modifiability” or “configu-
rability”, even though these discussions tend to shed little light on the real system. It is meaningless 
to say that a system will be “modifiable”, because every system is modifiable with respect to some 
changes and not modifiable with respect to others. You can, however, specify the modifiability 
response measure you would like to achieve as the result of a specific change request, perhaps 
in terms of elapsed time or total effort. For example, you might want to specify that “a request to 
update shipping rates is processed in less than 50 milliseconds”—an unambiguous criterion.
The heart of a quality attribute scenario, therefore, is the pairing of a stimulus with a 
response. Suppose that you are building a video game and you have a functional requirement 
like this: “The game shall change view modes when the user presses the <C> button”. This 
functional requirement, if it is important, needs to be associated with quality attribute require-
ments. For example:
■
■How fast should the function be?
■
■How secure should the function be?
■
■How modifiable should the function be?
To address this problem, we use a scenario to describe a quality attribute requirement. 
A quality attribute scenario is a short description of how a system is required to respond to 
some stimulus. For example, we might annotate the functional requirement given earlier as 
follows: “The game shall change view modes in less than 500 ms when the user presses the 
<C> button”. A scenario associates a stimulus (in this case, pressing the <C> button) with a 
response (changing the view mode) that is measured using a response measure (less than 500 
ms). A complete quality attribute scenario adds three other parts: the source of the stimulus (in 
this case, the user), the artifact affected (in this case, because we are dealing with end-to-end 
latency, the artifact is the entire system), and the environment (are we in normal operation, 
startup, degraded mode, or some other mode?). In total, then, there are six parts of a com-
pletely well-specified scenario, as shown in Figure 2.2.
Stimulus
Response
Response
Measure
Source
of Stimulus
Artifact
Environment
3
2
1
4
FIGURE 2.2  The six parts of a quality attribute scenario

20  Chapter 2  Architectural Design
Scenarios are testable, falsifiable hypotheses about the quality attribute behavior of the 
system under consideration. Because they have explicit stimuli and responses, we can evaluate 
a design in terms of how likely it is to support the scenario, and we can take measurements and 
test a prototype or fully fleshed-out system for whether it satisfies the scenario in practice. If 
the analysis (or prototyping results) indicates that the scenario’s response goal likely won’t be 
met, then the hypothesis is not supported.
We often hear the objections “I don’t know precisely what that requirement is” and “I 
don’t know what that response measure should be”. You should still collect and document 
scenarios, even in the face of uncertainty, for two reasons: (1) just struggling with this ques-
tion, and perhaps discussing it, will help to clarify the requirement, and (2) even if the precise 
requirements are not known, you probably know the value of the response measure to within 
an order of magnitude. If the system should respond “quickly” to a user’s request for a report, 
you may not know whether 2 seconds or 5 seconds or 10 seconds is the best number to use as 
the cutoff, but you certainly know that 100 seconds doesn’t count as “quickly” in most people’s 
books. This kind of order-of-magnitude requirement is typically enough to guide architectural 
decisions.
As with all other requirements, scenarios should be prioritized. This can be achieved by 
considering two dimensions that are associated with each scenario:
■
■The first dimension corresponds to the importance of the scenario with respect to the 
success of the system. This is ranked by the customer.
■
■The second dimension corresponds to the degree of technical risk associated with the 
scenario. This is ranked by the architect.
A low/medium/high (L/M/H) scale is used to rank both dimensions; in practice, most 
people don’t have too much trouble making these coarse distinctions. Once the dimensions 
have been ranked, scenarios are prioritized by selecting those that have a (H, H) ranking. If 
there aren’t enough of those, then we look at scenarios with (H, M), or (M, H) rankings. Why 
these ones? If a scenario involves high business importance and high risk, it is definitely a 
driver! And it certainly deserves your attention.
In addition, some traditional requirements elicitation techniques can be modified slightly 
to focus on quality attribute requirements. Those techniques include Joint Requirements 
Planning (JRP), Joint Application Design (JAD), discovery prototyping, and user stories.
No matter which technique you use, do not start design without a prioritized list of 
measurable quality attributes! Although stakeholders might plead ignorance (“I don’t know 
how fast it needs to be; just make it fast!”), you can always elicit at least a range of possible 
responses. Instead of accepting the requirement that the system should be “fast”, ask the stake-
holder if a 10-second response time is acceptable. If that is unacceptable, ask if 5 seconds is 
OK, or, failing that, 1 second. You will find that, in most cases, users know more than they 
realize about their requirements, and you can at least “box them in” to a range. 

2.4  Architectural Drivers  21
The Quality Attribute Workshop and the Utility Tree
The Quality Attribute Workshop (QAW)
The QAW is a facilitated, stakeholder-focused method that can be used to generate,  
prioritize, and refine quality attribute scenarios. A QAW meeting is ideally enacted 
before the software architecture has been defined, although, in practice, we have seen 
the QAW being used at all points in the software development life cycle. The QAW 
focuses on system-level concerns and specifically the role that software will play in the 
system. The steps of the QAW are as follows:
1.	 QAW Presentation and Introductions
The QAW facilitators describe the motivation for the QAW and explain each step of 
the method.
2.	Business Goals Presentation
A stakeholder representing the project’s business concerns presents the system’s 
business context, broad functional requirements, constraints, and known quality 
attribute requirements. The quality attributes that will be refined in later QAW steps 
will be derived from, and should be traceable to, the business goals presented in this 
step. For this reason, these business goals must be prioritized.
3.	Architectural Plan Presentation
The architect presents the system architectural plans as they currently exist. 
Although the architecture may not have been fully defined yet (particularly for green-
field systems), the architect often knows quite a lot about it even at this early stage. 
For example, the architect might already know about technologies that are mandated, 
other systems that this system must interact with, standards that must be followed, 
subsystems or components that could be reused, and so forth.
4.	Identification of Architectural Drivers
The facilitators share the list of key architectural drivers that they assembled during 
steps 2 and 3 and ask the stakeholders for clarifications, additions, deletions, and 
corrections. The idea here is to reach a consensus on a distilled list of architectural 
drivers that covers major functional requirements, business drivers, constraints, and 
quality attributes.
5.	Scenario Brainstorming
Given this context, each stakeholder now has the opportunity to express a scenario 
representing their needs and desires with respect to the system. The facilitators 
ensure that each scenario has an explicit stimulus and response. The facilitators also 
ensure traceability and completeness: At least one representative scenario should 
exist for each architectural driver listed in step 4, and collectively the scenarios 
should cover all of the business goals listed in step 2.
6.	Scenario Consolidation
Similar scenarios are consolidated where reasonable. In step 7, the stakeholders 
vote for their favorite scenarios, and consolidation helps to prevent votes from being 
spread across several scenarios that are expressing essentially the same concern.

22  Chapter 2  Architectural Design
7.	 Scenario Prioritization
Prioritization of the scenarios is accomplished by allocating to each stakeholder a 
number of votes equal to 30 percent of the total number of scenarios. The stakehold-
ers can distribute these votes to any scenario or scenarios. Once all the stakeholders 
have voted, the results are tallied and the scenarios are sorted in order of popularity.
8.	Scenario Refinement
The highest-priority scenarios are refined and elaborated. The facilitators help the 
stakeholders express these in the form of six-part scenarios: source, stimulus,  
artifact, environment, response, and response measure.
In summary, the output of the QAW is a prioritized list of scenarios, aligned with 
business goals, where the highest-priority scenarios have been explored and refined. 
A QAW can be conducted in as little as 2–3 hours for a simple system or as part of 
an iteration, but may last as long as 2 days for a complex system where requirements 
completeness is a goal.
The Utility Tree
If no stakeholders are readily available to consult, you still need to decide what to do 
and how to prioritize the many challenges facing the system. One way to organize your 
thoughts is to create a Utility Tree. A Utility Tree, such as the one shown in Figure 2.3, 
helps to articulate your quality attribute goals in detail, and then to prioritize them.
Performance
Latency
Peak load
Uptime
Availability
Guarantee
SW failure
Utility
Reliability
Exception
management
Authorization
Security
Audit trail
Deployability
Rollback
deployment
Base price is changed during normal operation.
Derived prices can be queried in less than 100 ms.
Price change request is received during peak load,
the request is processed in at most 30 ms.
Pricing queries uptime SLA must be 99.9% outside
of maintenance windows.
Error occurs in query service which brings it down.
Query service is available again in at most 30s.
Multiple price changes performed on a hotel. 100%
of the price changes are published successfully.
Exception occurs in the system. 100% of the
exceptions are logged.
Non administrative user logs into the system, no
administrative tasks can be performed.
User performs change operations on the system.
100% of the operations are logged.
Environment
transfer
Application moved between nonproduction
environments. No code changes are needed.
Error occurs after deployment to production,
previous version can be successfully rolled back in
at most 10 minutes.
(M,H)
(L,M)
(H,L)
(M,L)
(L,M)
(H,L)
(M,M)
(H,H)
(H,M)
(H,H)
FIGURE 2.3  A Utility Tree

2.4  Architectural Drivers  23
It works as follows. First write the word “Utility” on a sheet of paper. Then write the 
various quality attributes that constitute utility for your system. For example, you might 
know, based on the business goals for the system, that the most important qualities are 
that the system is fast, secure, and easy to modify. In turn, you would write these words 
underneath “Utility”. Next, because you don’t really know what any of those terms actu-
ally means, you describe the aspect of the quality attribute that you are most concerned 
with. For example, “performance” is vague, but “latency of price changes” is a bit less 
vague. Likewise, “deployability” is vague, but “rollback deployment” is a bit less vague.
The leaves of the tree are expressed as scenarios, which provide concrete exam-
ples of the quality attribute considerations that you just enumerated. For example, for 
“latency of price changes”, you might create a scenario such as “Base price is changed 
during normal operation. Derived prices can be queried in less than 100 ms”. For 
“deployability”, you might create a scenario such as “Error occurs after deployment to 
production, previous version can be successfully rolled back in at most 10 minutes”.
Finally, prioritize the scenarios that you have created. We perform this prioritization by 
using the technique of ranking across two dimensions, resulting in a priority matrix such 
as the following (where the numbers in the cells are from a set of system scenarios).
BUSINESS IMPORTANCE/
TECHNICAL RISK
L
M
H
L
5, 6, 17, 20, 22
1, 14
12, 19
M
9, 12, 16
8, 20
3, 13, 15
H
10, 18, 21
4, 7
2, 11
Our job, as architects, is to focus on the lower-right-hand portion of this table (H, H): 
those scenarios that have high business importance and high risk. Once we have satis-
factorily addressed those scenarios, we can move to the (M, H) or (H, M) ones, and then 
move up and to the left until all of the system’s scenarios are addressed (or perhaps 
until we run out of time or budget, as is often the case).
The QAW and the Utility Tree are different techniques that share the same goal—
eliciting and prioritizing the most important quality attribute requirements, which will be 
some of your most critical architectural drivers. There is no reason, however, to choose 
between these techniques—both are useful and valuable. In our experience, they have 
complementary strengths: The QAW tends to focus more on the requirements of exter-
nal stakeholders, whereas the Utility Tree tends to excel at eliciting the requirements of 
internal stakeholders. Making all of these stakeholders happy will go a long way toward 
ensuring the success of your architecture.
2.4.3  Primary Functionality
Functionality is the ability of the system to do the work for which it was intended. As opposed 
to quality attributes, the way the system is structured does not normally influence functional-
ity. You can have all of the functionality of a given system coded in a single enormous module 
(a god class, for example), or you can have it neatly distributed across many smaller, highly 
cohesive modules. Externally the system will look and work the same way if you consider 
only functionality. What matters, though, is what happens when you want to make changes to 
such a system. In the former case, changes will be difficult and costly; in the latter case, they 

24  Chapter 2  Architectural Design
should be much easier and cheaper to perform. In terms of architectural design, allocation of  
functionality to elements, rather than the functionality per se, is what matters. A good archi-
tecture is one in which the most common changes are localized in a single or a few elements 
(as a consequence of high cohesion), and hence easy to make.
When designing an architecture, you need to consider at least the primary functionality. 
Primary functionality is usually defined as functionality that is critical to achieve the busi-
ness goals that motivate the development of the system. These are typically captured as user 
stories or use cases (we will use these terms interchangeably throughout this book). Primary 
functionality is often focused on the “happy” paths, where everything works as planned and 
the system delivers some useful outcome. Other criteria for primary functional requirements 
might be that it implies a high level of technical difficulty or requires the interaction of many 
architectural elements. As a rule of thumb, approximately 10 percent of your user stories or use 
cases are likely to be primary.
There are two important reasons why you need to consider primary functionality when 
designing an architecture:
1.	
You need to think how functionality will be allocated to elements (usually modules) to 
promote modifiability or reusability, and also to plan work assignments.
2.	
Some quality attribute scenarios are directly connected to the primary functionality in 
the system. For example, in a movie streaming application, one of the primary use cases 
is, of course, to watch a movie. This use case is associated with a performance quality 
attribute scenario such as “Once the user clicks ‘Play’, the movie should begin stream-
ing in no more than 5 seconds”. In this case, the quality attribute scenario is directly 
associated with the primary user story, so making decisions to support this scenario also 
requires making decisions about how its associated functionality will be supported. This 
is not the case for all quality attributes, however. For example, an availability scenario 
can involve recovery from a system failure, and this failure may occur when any of the 
system’s functions is being executed.
Decisions regarding the allocation of functionality that are made during architectural 
design establish a precedent for how the rest of the functionality should be allocated to modules 
as development progresses. This is usually not the work of the architect; instead, this activity is 
typically performed as part of the element interaction design process described in Section 2.2.2.
Finally, bad decisions about the allocation of functionality result in the accumulation of 
technical debt. This debt can be paid through the use of refactoring, although this impacts the 
project’s rate of progress, or velocity (see Chapter 10).
2.4.4  Architectural Concerns
Architectural concerns encompass additional aspects that need to be considered as part  
of architectural design but are not expressed as traditional requirements. There are several 
different types of concerns:
■
■General concerns. These are “broad” issues that you deal with in creating the architec-
ture, such as establishing the overall system structure, the allocation of functionality to 
modules, the allocation of modules to teams, and the organization of the code base.

2.4  Architectural Drivers  25
■
■Specific concerns. These are more detailed, system-internal issues such as exception 
management, dependency management, configuration, logging, authentication, autho-
rization, caching, and so forth that are common across large numbers of applications. 
Some specific concerns are addressed in reference architectures (see Section 3.2.1), but 
others will be unique to your system. Specific concerns may also result from previous 
design decisions. For example, you may need to address API versioning if you previously 
decided to use a REST API.
■
■Internal requirements. These requirements are usually not specified explicitly in  
traditional requirement documents, as customers usually seldom express them. Internal 
requirements may address aspects that facilitate development, deployment, operation, 
or maintenance of the system. They are sometimes called “derived requirements”. An 
example would be the use of a particular data type to manage currencies, or the require-
ment to store all timestamps in UTC.
■
■Issues. These result from analysis activities, such as a design review (see Section 11.6), 
so they may not be present initially. For instance, an architectural evaluation may 
uncover a security risk that requires some changes to the current design.
Some of the decisions surrounding architectural concerns might be trivial or obvious. 
For example, your deployment structure might be a single processor for an embedded system, 
or a single cell phone for an app. Your reference architecture might be constrained by com-
pany policy. Your authentication and authorization policies might be dictated by your enter-
prise architecture and realized in a shared framework. In other cases, however, the decisions 
required to satisfy particular concerns may be less obvious—for example, decisions involving 
exception management or input validation or structuring the code base.
From their past experience, wise architects are usually aware of the concerns that are 
associated with a particular type of system and the need to make early design decisions to 
address them. Inexperienced architects are usually less aware of such concerns; because these 
concerns tend to be tacit rather than explicit, they may not consider them as part of the design 
process, which often results in problems later on.
Architectural concerns frequently result in the introduction of new quality attribute sce-
narios. The general concern of “supporting logging”, for example, is too vague and needs to 
be made more specific. Like the quality attribute scenarios that are provided by the customer, 
these scenarios need to be prioritized. For these scenarios, however, the customer is the devel-
opment team, operations, or other members of the organization. During design, the architect 
must consider both the quality attribute scenarios that are provided by the customer and the 
scenarios that are derived from architectural concerns.
One of our goals when revising the ADD method was to elevate the importance of archi-
tectural concerns as explicit inputs to the architecture design process. That issue will be high-
lighted in our examples and case studies in Chapters 8 and 9.
2.4.5  Constraints
As part of the architectural design process, you need to catalog the constraints on development. 
These constraints may take the form of mandated technologies, other systems with which your 
system needs to interoperate or integrate, laws and standards that the system must comply with, 

26  Chapter 2  Architectural Design
the abilities and availability of your developers, deadlines that are non-negotiable, backward 
compatibility with older versions of systems, and so on. An example of a technical constraint is 
the use of open source technologies, whereas a nontechnical constraint is that the system must 
obey the Sarbanes-Oxley Act or that it must be delivered by December 15.
A constraint is a decision over which you have little or no control as an architect. Your job 
is, as we mentioned in Chapter 1, to satisfice: to design the best system that you can, despite 
the constraints you face. Sometimes you might be able to argue for loosening a constraint, but 
in most cases you have no choice but to design around the constraints.
2.5  Summary
In this chapter, we introduced the idea of design as a set of decisions to satisfy requirements 
and constraints. We also introduced the notion of “architectural” design and showed that it 
does not differ from design in general, other than that it addresses the satisfaction of architec-
tural drivers: the purpose, primary functional requirements, quality attribute requirements, 
architectural concerns, and constraints. What makes a decision “architectural”? A decision  
is architectural if it has non-local consequences and those consequences matter to the achieve-
ment of an architectural driver.
We also discussed why architectural design is so important: because it is the embodi-
ment of early, far-reaching, hard-to-change decisions. These decisions will help you meet your 
architectural drivers, will determine much of your project’s work-breakdown structure, and 
will affect the tools, skills, and technologies needed to realize the system. Thus, architectural 
design decisions should be scrutinized closely, as their consequences are profound. In addi-
tion, architecture is a key enabler of agility.
2.6  Further Reading
A more in-depth treatment of scenarios and architectural drivers can be found in L. Bass,  
P. Clements, and R. Kazman, Software Architecture in Practice, 4th ed., Addison-Wesley, 
2021. Also found in this book is an extensive discussion of architectural tactics, which are 
useful in guiding an architecture to achieve quality attribute goals. Likewise, this book  
contains an extensive discussion of QAW and Utility Trees.
A discussion of technical debt, including design/architecture debt, can be found in  
N. Ernst, J. Delange, and R. Kazman, Technical Debt in Practice: How to Find It and Fix It, 
MIT Press, 2021.
The Mission Thread Workshop is discussed in R. Kazman, M. Gagliardi, and  
W. Wood, “Scaling Up Software Architecture Analysis”, Journal of Systems and Software, 85,  

2.7  Discussion Questions  27
1511–1519, 2012; and in M. Gagliardi, W. Wood, and T. Morrow, Introduction to the Mission 
Thread Workshop, Software Engineering Institute Technical Report CMU/SEI-2013-TR-003, 
2013.
An overview of discovery prototyping, JRP, JAD, and accelerated systems analysis can 
be found in any competent book on systems analysis and design, such as J. Whitten and  
L. Bentley, Systems Analysis and Design Methods, 7th ed., McGraw-Hill, 2007.
An extensive collection of architectural design patterns for the construction of distrib-
uted systems can be found in F. Buschmann, K. Henney, and D. Schmidt, Pattern-Oriented 
Software Architecture, Volume 4: A Pattern Language for Distributed Computing, Wiley, 
2007. Other books in the POSA (Patterns of Software Architecture) series provide additional 
pattern catalogs. Many other pattern catalogs specializing in particular application domains 
and technologies exist. A few examples are listed here:
■
■
E. Gamma, R. Helm, R. Johnson, and J. Vlissides, Design Patterns: Elements of 
Reusable Object-Oriented Software, Addison-Wesley, 1995.
■
■
E. Fernandez-Buglioni, Security Patterns in Practice: Designing Secure 
Architectures Using Software Patterns, Wiley, 2013.
■
■
J. Gilbert, Software Architecture Patterns for Serverless Systems: Architecting 
for Innovation with Events, Autonomous Services, and Micro Frontends, Packt 
Publishing, 2021.
■
■
H. Percival and B. Gregory, Architecture Patterns with Python: Enabling Test-
Driven Development, Domain-Driven Design, and Event-Driven Microservices, 
O’Reilly, 2020.
The “bible” for software architecture documentation is P. Clements, F. Bachmann, 
L. Bass, D. Garlan, J. Ivers, R. Little, P. Merson, R. Nord, and J. Stafford, Documenting 
Software Architectures: Views and Beyond, 2nd ed., Addison-Wesley, 2011.
A chapter devoted to software architecture is now included in Version 4.0 of the 
SWEBOK: Hironori Washizaki, eds., Guide to the Software Engineering Body of Knowledge 
(SWEBOK), Version 4.0, IEEE Computer Society, 2024; www.swebok.org.
2.7  Discussion Questions
1.	
Can you find an example of an application that, at the time of its development, was  
considered to have a novel design? Why was its design novel?
2.	
What risks arise when an architect does all of the design of the system (including  
element interaction design and element internals design)? What happens if all of this 
design is performed initially, before any development begins?
3.	
Consider an application that you are familiar with. It might be one that you have worked 
on, or one you have used, such as a social network, a video streaming app, a productivity 
tool, or a game. What are drivers for this application?

28  Chapter 2  Architectural Design
4.	
Which functional requirement would be primary in the application that you selected? 
Why?
5.	
Which quality attribute would have the highest priority? Why?
6.	
Can you identify a constraint for this system?
7.	
Can you identify an obvious tradeoff that was made in the design of the application that 
you selected?

29
3
Making Design Decisions
Design is the process of making decisions. Which decisions? Ideally, the ones that will lead 
to project success! But decision-making is hard. It is fraught with uncertainty. We are often 
uncertain about our project requirements and project resources. We are uncertain about the 
evolutionary trajectories of the technologies we build upon. We are uncertain about our plan-
ning horizon. We may be uncertain about the costs of the choices we are making. We may 
even be uncertain about our own knowledge (although architects and others often fail in the 
other direction, being overconfident rather than underconfident—this is known as overconfi-
dence bias).
As architects we are responsible for the decisions that we make. This is as it should be. 
So which practices are available to us to ensure that the decisions that we make are the best 
ones, given our abilities? That is the subject of this chapter. We begin by reviewing a set of 
decision-making techniques. Then we turn to the body of knowledge that we can employ, as 
architects, to make better, more confident decisions. 
Why Read This Chapter?
You come to this chapter, we are certain, with some preconceived ideas about what 
design is and what role it should play. But very few people have the depth and breadth 
of experience to help them make design decisions confidently. In this chapter, we 
provide a sampling of the kinds of knowledge that experienced designers possess, and 
we show how this knowledge forms the basis for making good design decisions with 
confidence. Reading this chapter won’t make you an expert in design concepts—that is 
too much to expect of a single chapter—but you’ll understand this essential tool that will 
be used throughout the book.
3.1  Making Design Decisions
We are seldom explicitly taught how to make decisions. It is somehow assumed that, given a 
proper education, we will make prudent choices. But this is not necessarily so. The field of 

30  Chapter 3  Making Design Decisions
design is littered with examples of “What were they thinking?!” The architects who are really 
good decision makers are often ones with decades of experience. They achieved their status 
through years of trial and error, by making mistakes and learning from their failures. This is 
clearly a suboptimal form of training—expensive and inefficient and not very scalable. Can we 
do any better? We think that we can!
3.1.1  Decision-Making Principles
In what follows we present nine general decision-making principles. By following these princi-
ples, or strategies, you are not guaranteed to make a perfect decision. That is clearly too high a 
bar. But you can certainly increase the likelihood that the decision you are making is a sound, 
well-reasoned one—which is why we recommend these principles. And while all of them are 
“common sense”, we see them violated over and over again in practice. The result of these 
(frequently avoidable) violations is projects with high technical debt, poor performance, poor 
scalability, stability issues, poor usability, and so on.
Principle 1: Use Facts
A fact is something that we believe to be true. Facts, and the evidence for them, are the founda-
tions of logical reasoning. Incorrect or incomplete information may lead to invalid conclusions. 
An architect may hear praise for a new technology and base a decision on that, instead of test-
ing it themselves. For example, a colleague of ours chose a NoSQL database in the Query side 
of a CQRS-based system (see Section 3.3.2) he was designing because he felt it was a good fit; 
he had not done any prototyping or analysis, but made this decision based on hearsay, experi-
ence, and gut feelings. Was this the right choice? Time will tell. To check the facts, he could 
have consciously considered: “What evidence supports the assumption that NoSQL will meet 
the system’s requirements now and in the future?” When we cannot have all the facts, we make 
assumptions, as we will discuss next. We make a similar decision, as you will see, in Chapter 8 
(the case study where this decision was made). Was this the right decision?
Principle 2: Check Assumptions
In the absence of facts, we often make assumptions so that we can continue with the process 
of design. For instance, we may not know if a technology can perform adequately until we 
prototype the software. When we consciously make assumptions, we are saying that these 
assumptions will be good ones. Explicit assumptions can be checked. Implicit assumptions, by 
comparison, are more devious. We may even be unaware we have made them. For example, if 
we build our application with node.js based on an existing three-tier client-server architecture 
(which contains a monolithic database) without assessing the compatibility with the database’s 
performance characteristics, then this implicit assumption would not be checked, and that 
might create a performance risk for the system.
Principle 3: Explore Contexts
Contexts are conditions that influence software decisions. There are many contextual factors, 
such as development resources, financial pressures, legal obligations, industry norms, user 

3.1  Making Design Decisions  31
expectations, and past decisions. For example, we might want to implement a scalable and 
highly reliable database system, but have a limited budget. Even though the budget is not a sys-
tem requirement, it affects our decision on database license procurement. Some contexts will 
end up being constraints on design, such as team experience or expectations. Design contexts 
shape our decisions implicitly, yet are not necessarily technology related. Exploring contextual 
factors can broaden our design considerations. To check that we have considered contexts we 
may ask: “What are the contexts that could influence X?”, “Have I missed any contexts?”, and 
“Does the team have the experience in implementing X?”
Principle 4: Anticipate Risks
A risk is the possibility of an undesirable outcome. A documented risk contains an estimate of 
the size of the loss and the probability of the loss. There are many risks that an architect needs 
to estimate, such as extreme spikes of demand and security attacks. Anticipating and quantify-
ing risks is the process of exploring the unknowns, including estimating the possibility of risks 
occurring and, if they occur, their impacts. Architects may reflect on risks by asking questions 
such as “What are the potential undesirable outcomes?” or “Is there a chance that X would not 
work?”
Principle 5: Assign Priorities
Priorities quantify the relative importance of choices, choices such as which requirement to 
implement or which solution to use. If we can afford to implement only one of the two require-
ments, which one is more important? Prioritization is required when the things that we desire 
are competing for the same limited resource (e.g., time, money, developer skills, CPU, mem-
ory, network bandwidth). Some of these priorities emanate from contextual factors. To sort out 
our priorities, we can ask: “Which requirement is more important?”, “What can we do with-
out?”, and “What should we use this resource for?”
Principle 6: Define Time Horizon
The time horizon defines the time period relevant to a decision and its impacts. Risks, bene-
fits, costs, needs, and impacts can change over time, and we want to anticipate how they might 
change. For example, we might estimate that the system processing load will reach 85 percent 
capacity in 3 years, or we might choose to deploy the system on premises in the short term while 
keeping alive the option to deploy it on a public cloud if that becomes more cost-effective.
Defining the time horizon allows architects to explicitly state and evaluate the pros and 
cons of specific actions (and nonactions) in terms of their short- and long-term impacts. Without 
explicit considerations of the time horizon and the reasoning that goes along with it, long-term 
considerations may be undermined, or short-term needs may be ignored. To define the time hori-
zon, the architect may ask questions such as “What would be affected in the short and long term 
if I decide on X?” or “What needs to be considered in different time horizons for X?”
Principle 7: Generate Multiple Solution Options
Some architects will go with the first solution they think of without considering further options. 
If the architect is experienced and the problem is well understood and low risk, this approach 

32  Chapter 3  Making Design Decisions
may be ideal. But in more challenging contexts, considering only a single solution may be 
risky; the first solution is not necessarily the best one, especially when an architect is inexperi-
enced or faces an unfamiliar situation. This behavior may be due to anchoring bias—a refusal 
to let go of the first idea. Generating multiple solution options helps an architect broaden the 
menu of solution choices and stimulate creativity. We can create a larger set of solution ideas 
by asking ourselves questions such as “Are there other solutions to this problem?” or “Can I 
find a better solution than X?”
Principle 8: Design Around Constraints
Constraints are limitations that set the boundaries of our design options. Constraints may 
come from requirements, contexts, technologies, and existing design choices. For instance, a 
CPU can compute only W instructions per second; the budget of the project is $X; the number 
of concurrent users supported by a software license is Y; platform Z doesn’t support a cer-
tain protocol; developers have no experience with some technology; and so forth. In software 
development, we often find interconnected sets of constraints: If we choose component A, 
we must also use component B. When there are no apparent solutions, architects must design 
around constraints and introduce novel solutions, relax constraints, or manipulate the contexts. 
An architect can check on constraints by asking, “If I choose X, would it limit other design 
options?” or “Are there any constraints that could impede this design choice?”
Principle 9: Weigh the Pros and Cons
Pros and cons represent the arguments for and against each of the choices in a selection. 
Weighing the pros and cons is heavily bound up with examining tradeoffs. The evaluation of 
the pros and cons allows architects to decide what to choose and what to avoid. A quantitative 
evaluation can be based on measurable elements such as costs, benefits, priorities, immedi-
acy (i.e., time horizon), and risks. However, some pros and cons cannot be easily quantified. 
Consider the navigation menu design of a mobile app: How can one quantify the pros and 
cons of a hamburger menu versus a set of tabs? In this case, qualitative arguments such as 
ease of access, ease of learning, and effort to implement can be marshaled. Weighing the pros 
and cons offers architects the chance to think about the relative benefits and drawbacks and 
who they might affect. To check tradeoffs, the architect can ask questions such as “Are there 
more benefits from solution X than from solution Y?” or “Have we done a thorough tradeoff 
evaluation?”
Now that we have enumerated a set of principles, let us turn our attention to the design 
concepts that we use to structure the system and to support our most important quality 
attributes.
3.2  Design Concepts: The Building Blocks for Creating Structures
Design is not random, but rather is planned, intentional, rational, and directed. The process 
of design might seem daunting at first. When facing the “blank page” at the beginning of any 

3.2  Design Concepts: The Building Blocks for Creating Structures  33
design iteration, the space of possibilities might seem impossibly huge and complex; however, 
there is some help here. Over the course of decades, the software architecture community has 
created and evolved a body of generally accepted design principles that can guide us to create 
high-quality designs with predictable outcomes.
For example, some well-documented design principles are oriented toward the achieve-
ment of specific quality attributes:
■
■To help achieve high modifiability, aim for good modularity, which means high cohesion 
and low coupling.
■
■To help achieve high availability, avoid having any single point of failure.
■
■To help achieve scalability, avoid having any hard-coded limits for critical resources.
■
■To help achieve security, limit the points of access to critical resources.
■
■To help achieve testability, externalize state.
■
■. . . and so forth.
In each case, these principles have been evolved over decades of dealing with those qual-
ity attributes in practice. But these principles are rather abstract. To make them more usable 
in practice, a set of design concepts have been cataloged. These concepts serve as the building 
blocks of architectures. Different types of design concepts exist, and in the following subsec-
tions we discuss some of the most commonly used, including reference architectures, patterns, 
tactics, and externally developed components (such as frameworks). Whereas the first three 
are conceptual in nature, the last one is concrete.
3.2.1  Reference Architectures
Reference architectures are blueprints that provide an overall logical structure for particular 
classes of applications. A reference architecture is a reference model mapped onto one or more 
architectural patterns. Its utility has been proven in business and technical contexts, and it typ-
ically comes with a set of supporting artifacts that facilitates its use.
An example of a reference architecture for the development of service applications 
(such as microservices) is shown in Figure 3.1. This reference architecture establishes the 
main layers for this type of application—service, business, and data—as well as the types 
of elements that occur within the layers and the responsibilities of these elements, such as 
service interfaces, business components, data access components, service agents, and so 
on. Also, this reference architecture introduces cross-cutting concerns, such as security and 
communication, that need to be addressed. As this example shows, when you select a ref-
erence architecture for your application, you also adopt a set of concerns that you need to 
address during design. You may not have an explicit requirement related to communications 
or security, but the fact that these elements are part of the reference architecture guides you 
to make design decisions about them.

34  Chapter 3  Making Design Decisions
External System
Server
Service Consumers
(from Services Layer)
Message Types
Service Interfaces
Cross-Cutting
(from Business Layer)
Application Facade*
Business
Workﬂow
Business
Logic
Business
Entities
(from Data Layer)
Security
Service
Agents
Operational Management
Communication
Data
Access
Helpers
and
Utilities
* = optional component
Other
systems
Dats Sources
FIGURE 3.1  Service application reference architecture

3.2  Design Concepts: The Building Blocks for Creating Structures  35
Reference architectures may be confused with architectural styles, but these two concepts 
are different. Architectural styles (such as “pipe and filter” and “client-server”) are patterns 
that define types of components and connectors in a specified topology that are useful for 
structuring an application either logically or physically. Reference architectures, in contrast, 
provide a more detailed structure for entire applications, and they may embody multiple pat-
terns. Reference architectures are preferred by practitioners—which is also why we favor them 
in our list of design concepts.
Many reference architectures are available, but there is no single authoritative catalog of 
them. In the context of cloud development, providers have created a number of useful catalogs 
that collect reference architectures focused on cloud resources. In addition, many catalogs of 
patterns have been created across the years, and new ones appear periodically.
3.2.2  Patterns
Design/architectural patterns are conceptual solutions to recurring design problems that exist 
in a defined context. Although design patterns originally focused on decisions at the object 
scale, including instantiation, structuring, and behavior, today there are catalogs of patterns 
that address decisions at varying levels of granularity. In addition, there are specific patterns to 
address quality attributes such as security, availability, performance, and integrability.
Some people might differentiate between what they consider to be architectural patterns 
and the more fine-grained design patterns. However, we believe there is no principled differ-
ence that can be solely attributed to scale. We consider a pattern to be architectural when its 
use directly and substantially influences the satisfaction of some of a system’s architectural 
drivers.
Another important type of pattern that has significant implications for the architectural 
design process is deployment patterns. These patterns package key decisions that shape the 
system’s infrastructure and have profound implications for quality attributes such as avail-
ability, performance, modifiability, and usability. Such decisions are often bound early in the 
product life cycle. We will discuss deployment patterns in Chapter 6.
We provide examples of patterns for a number of quality attributes in Sections 3.3 through 
3.8. Note that these are just examples—many more patterns exist for each quality attribute, 
and for other qualities.
3.2.3  Tactics
Architects can also employ fundamental design techniques to help achieve a response goal for 
a particular quality attribute. We call these architectural design primitives tactics. Tactics, like 
patterns, are techniques that architects have been using for years. We do not invent tactics, but 
simply capture what architects have done in actual practice, over the decades, to manage qual-
ity attribute response goals.
Tactics are design decisions that influence the control of a quality attribute response. For 
example, if you want to design a system to have good usability, you would need to make a set 
of design decisions that support this quality attribute, as represented in Figure 3.2.

36  Chapter 3  Making Design Decisions
User interaction
User given appropriate
feedback and assistance
Tactics
to Control
Response
FIGURE 3.2  Usability tactics mediate user interactions and responses
Tactics are simpler than patterns. They focus on the control of a single quality attribute 
response (although they may, of course, trade off this response with other quality attribute 
goals). Patterns, in contrast, typically focus on resolving and balancing multiple forces—that 
is, multiple quality attribute goals. By way of analogy, we can say that a tactic is an atom, 
whereas a pattern is a molecule.
Tactics provide a top-down way of thinking about design. A tactics categorization begins 
with a set of design objectives related to the achievement of a quality attribute, and presents the 
architect with a set of options from which to choose.
For example, in Figure 3.3, the design objectives for usability are “Support user initia-
tive” and “Support system initiative”. An architect who wants to create a system with “good” 
usability needs to choose one or more of these options. That is, the architect needs to decide 
if they want to support a user in their tasks by allowing for features such as undo, cancel (for 
long-running or hung operations), aggregation (of similar UI objects so that a user-issued com-
mand applies to all of them), or pause/resume (again, typically used for long-running opera-
tions). The architect may also want to support system initiatives, such as maintaining a model 
of the task (so that, for example, context-appropriate help and guidance can be given), main-
taining a model of the user (so that user-appropriate feedback is offered; for example, a novice 
user might want far more guidance, whereas an expert user might want shortcuts for common 
operations), or maintaining a system model (for example, being able to accurately estimate 
time remaining for long-running operations).
Each of these tactics is an option for the architect. They may be instantiated via coding, 
via patterns, or via external components such as frameworks—but they are architectural. To 
offer undo capabilities, the system must maintain a set of transactions, representing changes to 
the system state, and be able to roll back any of those transactions to undo—that is, to return 
the system to its prior state. To be able to offer cancel capabilities, the system must be able to 
revert its state to whatever it was before the operation was initiated. These sorts of capabilities 
require architectural thinking, and tactics provide a starting point for this decision-making 
process. As we will see in Chapter 4, the choice, combination, and tailoring of tactics and  
patterns are some of the key steps of the ADD process.

3.2  Design Concepts: The Building Blocks for Creating Structures  37
Usability Tactics
Support User Initiative
Support System Initiative
Cancel
Undo
Pause/Resume
Aggregate
MaintainTask Model
Maintain User Model
Maintain System Model
FIGURE 3.3  Usability tactics categorization
Tactics categorizations have been established for the quality attributes of availability, 
deployability, energy efficiency, interoperability, modifiability, performance, safety, secu-
rity, testability, and usability. Collectively, these categorizations cover the vast majority of the 
design decisions that an architect needs to make in practice.
3.2.4  Externally Developed Components
Both patterns and tactics are abstract in nature. When you are designing a software archi-
tecture, however, you need to make these design concepts concrete and closer to the actual 
implementation. There are two ways to achieve this: You can code the elements obtained from 
tactics and patterns, or you can associate technologies with one or more of these elements in 
the architecture. This “buy versus build” choice is one of the most important decisions you 
will make as an architect.
We consider technologies to be externally developed components, because they are not 
created as part of the development project. Several types of externally developed components 
exist:
■
■Technology families. A technology family represents a class of technologies with common 
functional purposes. It can serve as a placeholder until a specific product or framework is 
selected. Examples include a relational database management system (RDBMS) and an 
object-oriented to relational mapper (ORM).
■
■Products. A product (or software package) comprises a self-contained piece of software 
that can be integrated into the system that is being designed and that requires only minor 
configuration or coding. An example is a relational database management system, such 

38  Chapter 3  Making Design Decisions
as Oracle or PostgreSQL, which belongs to the RDBMS technology family. APIs  
(application programming interfaces) from external systems are one commonly encoun-
tered type of product; we discuss them in Chapter 5. We discuss another type of product, 
cloud capabilities made available by cloud provider platforms, in Chapter 7.
■
■Application frameworks. An application framework (or just framework) is a reusable 
software element, constructed out of patterns and tactics, that provides generic function-
ality addressing recurring domain and quality attribute concerns across a broad range of 
applications. Frameworks, when carefully chosen and properly implemented, increase 
the productivity of programmers. They do so by enabling programmers to focus on 
business logic and end-user value, rather than on the underlying technologies and their 
implementations. As opposed to products, framework functions are generally invoked 
from the application code or are “injected” using some type of dependency injection 
approach. Frameworks usually require extensive configuration, typically through XML 
files or other approaches such as annotations in Java. One example of a framework 
is Hibernate, which is used to perform object-oriented to relational mapping in Java. 
Several types of frameworks are available: Full-stack frameworks, such as Spring, are 
usually associated with reference architectures and address general concerns across the 
different elements of the reference architecture, whereas non-full-stack frameworks, such 
as JPA (Java Persistence API), address specific functional or quality attribute concerns.
■
■Platforms. A platform provides a complete infrastructure upon which to build and 
execute applications. Examples of current cloud platforms include Amazon Elastic 
Beanstalk, Google App Engine, and Azure App Service, all of which are Platform as a 
Service (PaaS) offerings.
The selection of externally developed components, which is a key aspect of the design 
process, can be a challenging task because of their extensive number. Here are a few criteria 
you should consider when selecting externally developed components:
■
■Problem that it addresses. Is it something specific, such as a framework for object- 
oriented to relational mapping, or is it something more generic, such as a complete 
platform?
■
■Cost. What is the cost of the license? If it is free, what is the cost of support and 
education?
■
■Type of license. Does it have a license that is compatible with the project goals?
■
■Vendor lock-in. Will the inclusion of the technology result in a dependency on the  
organization that produces it?
■
■Learning curve. Is this a technology that is difficult to learn? Is it difficult to find  
developers who have some expertise with it?
■
■Community support. Is this technology well supported by a robust community or by its 
vendor?
In the following sections, we focus our attention on two categories of design concepts: 
tactics and patterns. We intentionally omit externally developed components, even though the 
choice of these components is clearly an important design decision. However, the rapid evolu-
tion of technologies makes externally developed components prone to becoming quickly out-
dated, which is why we do not discuss them extensively here.

3.3  Design Concepts to Support Performance  39
3.3  Design Concepts to Support Performance
Performance is about time and the software system’s ability to meet timing requirements. 
Generally speaking, faster is better!
When events occur—interrupts, messages, requests from users or other systems, or clock 
events—the system must respond to them in time. Characterizing the events that can occur 
(and when they can occur) and the system’s response to those events is the starting point for 
discussing performance. All systems have performance requirements, even if they are not 
explicitly expressed.
Performance is often linked to scalability—increasing the system’s capacity, while ensur-
ing that it still performs well. Often, performance is considered after you have constructed 
something and found its performance to be inadequate. You can fix this by architecting your 
system consciously with performance in mind.
3.3.1  Performance Tactics
The performance tactics categorization is shown in Figure 3.4. This set of tactics, like all 
tactics, helps an architect reason about the quality attribute. There are two major categories of 
performance tactics: Control Resource Demand and Manage Resources.
Performance Tactics
Control Resource Demand
Manage Resources
Manage Work Requests
Limit Event Response
Prioritize Events
Reduce Computational Overhead
Bound Execution Times
Increase Efﬁciency
Increase Resources
Introduce Concurrency
Maintain Multiple Copies of Computations
Maintain Multiple Copies of Data
Bound Queue Sizes
Schedule Resources
FIGURE 3.4  Performance tactics categorization

40  Chapter 3  Making Design Decisions
Within the Control Resource Demand category, the tactics are:
■
■Manage work requests. One way to reduce work is to reduce the number of requests 
coming into the system. Ways to do that include managing work requests (i.e., limiting 
the number of requests the system will accept in a given time period) and managing the 
sampling rate (e.g., switching to a lower frame rate for streaming video).
■
■Limit event response. When events arrive too rapidly to be processed, they must be 
queued until they can be processed, or until they are discarded. You may choose to 
process events only up to a set maximum rate, thereby ensuring predictable processing 
for other events.
■
■Prioritize events. If not all events are equally important, you can impose a priority 
scheme that ranks events according to how urgently you want to service them.
■
■Reduce computational overhead. For events that make it into the system, you can reduce 
the amount of work in handling each event by reducing indirection (making direct calls 
rather than going through an intermediary, for example), by co-locating communicating 
resources, and by periodic cleaning (such as garbage collection).
■
■Bound execution times. You can place a limit on how much execution time is used to 
respond to an event.
■
■Increase efficiency of resource usage. Improving the efficiency of algorithms and data 
structures (adding an index to database, for example) in critical areas can decrease 
latency and improve throughput and resource consumption.
Within the Manage Resources category, the tactics are:
■
■Increase resources. Faster processors, additional processors, additional memory, and 
faster networks all have the potential to improve performance.
■
■Introduce concurrency. If requests can be processed in parallel, blocked time can be 
reduced.
■
■Maintain multiple copies of computations. This tactic reduces the contention that would 
occur if all requests were allocated to a single instance.
■
■Maintain multiple copies of data. Two common examples of maintaining multiple copies 
of data are data replication and caching.
■
■Bound queue sizes. This tactic controls the maximum number of queued arrivals and 
consequently the resources used to process the arrivals.
■
■Schedule resources. Whenever contention for a resource occurs, the resource should be 
scheduled. Your concern is to understand the characteristics of each resource’s use and 
choose an appropriate scheduling strategy.
These tactics cover the spectrum of architectural concerns with respect to performance. 
Now we turn our attention to more complex design structures, patterns.
3.3.2  Performance Patterns
In what follows we provide a small selection of architectural patterns that address performance 
concerns. We make no attempt here to provide a comprehensive catalog; that is not the purpose 

3.4  Design Concepts to Support Availability  41
of this book. In-depth coverage of performance patterns can be found in many other resources. 
We merely provide a few patterns here to stimulate thinking and to provide examples of the 
kinds of resources that are available to support the architect in reasoning about design for 
performance.
3.3.2.1  Load Balancer Pattern
A load balancer is an intermediary that handles messages from clients and determines which 
instance of a service should respond. The load balancer serves as a single point of contact for 
incoming messages and farms out requests to a pool of (typically stateless) providers.
By sharing the load among a pool of providers, latency can be kept lower and more pre-
dictable for clients. It is a simple matter to add more resources to the resource pool, and no 
client needs to be aware of this event (an instance of the increase resources, maintain multiple 
copies of computations, and introduce concurrency tactics). Moreover, any failure of a server 
is invisible to clients, assuming there are still some remaining processing resources. Although 
this is an availability benefit, we note once again that patterns often address multiple quality 
attributes, whereas tactics address just a single quality attribute.
The load balancing algorithm must be very fast; otherwise, it may itself contribute to per-
formance problems. The load balancer is a potential bottleneck or single point of failure, so it 
is often replicated (and even load balanced).
3.3.2.2  Throttling Pattern
The throttling pattern packages the manage work requests tactic. It is used to limit access to 
an important service. In this pattern, an intermediary—a throttler—monitors the service and 
determines whether an incoming request can be serviced. By throttling incoming requests, 
you can gracefully handle variations in demand. In doing so, services never become over-
loaded; they can be kept in a performance “sweet spot” where they handle requests efficiently.
But, once again, there are tradeoffs to consider: The throttling logic must be very fast; 
otherwise, it may itself contribute to performance problems. If client demand regularly exceeds 
capacity, buffers will need to be large, or you may lose requests. Also, this pattern can be diffi-
cult to add to an existing system where clients and servers are tightly coupled.
3.4  Design Concepts to Support Availability
Availability is a system property. A highly available system is compliant with its  
specifications—always ready to carry out its tasks when you need it to be. A failure in a sys-
tem occurs when the system no longer delivers a service consistent with its specification. A 
fault has the potential to cause a failure. Availability encompasses the ability of a system to 
mask or repair faults so they do not become failures. It builds on the quality attribute of reli-
ability by adding the notion of recovery (repair). The goal is to minimize service outage time 
by mitigating faults.

42  Chapter 3  Making Design Decisions
3.4.1  Availability Tactics
Availability tactics enable a system to endure faults. The tactics keep faults from becoming 
failures (or bound the effects of the fault and make repairs). A failure’s cause is a fault. A fault 
can be internal or external to the system. Faults can be prevented, tolerated, removed, or fore-
cast. Through these actions, a system can become “resilient” to faults.
Figure 3.5 shows the availability tactics categorization. There are three major categories 
of availability tactics: Detect Faults, Recover from Faults, and Prevent Faults. It is difficult 
to envision a system that could achieve high availability if it does not address all of these 
categories.
Detect Faults
Prevent Faults
Monitor
Ping/Echo
Heartbeat
Timestamp
Condition Monitoring
Sanity Checking
Voting
Exception Detection
Self-Test
Redundant Spare
Rollback
Exception Handling
Software Upgrade
Retry
Ignore Faulty Behavior
Graceful Degradation
Reconﬁguration
Shadow
State Resynchronization
Escalating Restart
Nonstop Forwarding
Removal from Service
Transactions
Predictive Model
Exception Prevention
Increase Competence Set
Recover from Faults
Preparation
and Repair
Reintroduction
Availability Tactics
FIGURE 3.5  Availability tactics categorization
When designing for availability, we are concerned with how faults are detected, how fre-
quently they occur, what happens when they occur, how long a system may be out of operation, 
how faults or failures can be prevented, and which notifications are required when a failure 
occurs. Each of these concerns can be addressed architecturally, through the appropriate  
selection of availability tactics.

3.4  Design Concepts to Support Availability  43
Within the Detect Faults category, the tactics are:
■
■Monitor: a component used to monitor the state of health of other parts of the system.  
A system monitor can detect failure or congestion in the network or other shared 
resources, such as from a denial-of-service attack.
■
■Ping/echo: asynchronous request/response message pair exchanged between nodes, used 
to determine reachability and the round-trip delay through the associated network path.
■
■Heartbeat: a periodic message exchange between a system monitor and a process being 
monitored.
■
■Timestamp: used to detect incorrect sequences of events, primarily in distributed  
message-passing systems.
■
■Condition monitoring: checks conditions in a process or device, or validates assumptions 
made during the design.
■
■Sanity checking: checks the validity or reasonableness of a component’s operations or 
outputs; typically based on a knowledge of the internal design, the state of the system, or 
the nature of the information under scrutiny.
■
■Voting: checks that replicated components are producing the same results. Comes in 
various flavors: replication, functional redundancy, and analytic redundancy.
■
■Exception detection: detection of a system condition that alters the normal flow of  
execution (e.g., system exception, parameter fence, parameter typing, timeout).
■
■Self-test: procedure for a component to test itself for correct operation.
Within the Recover from Faults category, there are two subcategories: “Preparation and 
Repair” and “Reintroduction”. Let us first look at the tactics for “Preparation and Repair”:
■
■Redundant spare: a configuration in which one or more duplicate components can step in 
and take over the work if the primary component fails. (This tactic is at the heart of the hot 
spare, warm spare, and cold spare patterns, which differ primarily in how up-to-date the 
backup component is at the time of its takeover, as we will discuss in Section 3.4.2.)
■
■Rollback: revert to a previous known good state, referred to as the “rollback line”.
■
■Exception handling: dealing with the exception by reporting it or handling it, potentially 
masking the fault by correcting the cause of the exception and retrying.
■
■Software upgrade: in-service upgrades to executable code images in a non-service- 
affecting manner.
■
■Retry: where a failure is transient, retrying the operation may lead to success.
■
■Ignore faulty behavior: ignoring messages sent from a source when it is determined that 
those messages are spurious.
■
■Graceful degradation: maintains the most critical system functions in the presence of 
component failures, dropping less critical functions.
■
■Reconfiguration: reassigning responsibilities to the resources left functioning, while 
maintaining as much functionality as possible.
Within the “Reintroduction” subcategory, the tactics are:
■
■Shadow: running a previously failed/upgraded component in “shadow mode” prior to 
reverting it to an active role.

44  Chapter 3  Making Design Decisions
■
■State resynchronization: partner to the redundant spare tactic, where state is sent from 
active to standby components.
■
■Escalating restart: recover from faults by varying the granularity of the component(s) 
restarted.
■
■Nonstop forwarding: functionality is split into supervisory and data components. If a 
supervisor fails, a router continues forwarding packets along known routes while  
protocol information is recovered.
The final category of availability tactics is Prevent Faults. The tactics within this  
category are:
■
■Removal from service: temporarily placing a system component in an out-of-service 
state for the purpose of mitigating potential failures.
■
■Transactions: bundling state updates so that messages exchanged between components 
are atomic, consistent, isolated, and durable.
■
■Predictive model: monitoring the state of a process to ensure that the system is operating 
properly; taking corrective action when conditions are predictive of likely future faults.
■
■Exception prevention: preventing system exceptions from occurring by masking a 
fault, or preventing it via mechanisms such as smart pointers, abstract data types, and 
wrappers.
■
■Increase competence set: designing a component to handle more cases—faults—as part 
of its normal operation.
3.4.2  Availability Patterns
A number of redundancy patterns are commonly used to achieve high availability. In this  
section, we will look at them as a group, as this helps us understand their strengths and weak-
nesses, and the scope of the design space around this issue. These patterns deploy a group of 
active components and a group of redundant spare components. In general, the greater the 
level of redundancy, the higher the availability and the cost and complexity will be.
The benefit of using a redundant spare is that the system continues to function cor-
rectly with only a brief delay after a failure. The alternative is a system that stops functioning  
correctly (or altogether) until the failed component is repaired.
All of these patterns incur additional cost and complexity from providing a spare. The 
tradeoff among the three alternatives is the time to recover from a failure versus the runtime 
cost incurred to keep a spare up-to-date. A hot spare carries the highest cost but leads to the 
fastest recovery time, for example.
3.4.2.1  Hot Spare (Active Redundancy)
In the hot spare configuration, all of the components belong to the active group and receive and 
process identical inputs in parallel, allowing the redundant spares to maintain a synchronous 
state with the active components. Because the redundant spare possesses an identical state to 
the active component, it can take over from a failed component in a matter of milliseconds.

3.4  Design Concepts to Support Availability  45
3.4.2.2  Warm Spare (Passive Redundancy)
In the warm spare variant, only the members of the active group process input. One of their 
duties is to provide the redundant spares with periodic state updates. Because this state is 
loosely coupled with the active components, the redundant components are referred to as 
warm spares.
Passive redundancy achieves a balance between the more highly available but more  
compute-intensive (and expensive) hot spare pattern and the less available but less complex 
(and cheaper) cold spare pattern.
3.4.2.3  Cold Spare (Spare)
In the cold spare configuration, spares remain out of service until a failure occurs, at which 
point a power-on-reset is initiated on the spare prior to it being placed in service. Due to its 
poor recovery performance, and hence its high MTTR (mean time to recovery), this pattern is 
poorly suited to systems with high availability requirements.
3.4.2.4  Tri-Modular Redundancy
Tri-modular redundancy (TMR) is a widely used implementation in which the voting tactic is 
combined with the hot spare pattern; it employs three identical components (or, more generally, 
N identical components in N-modular redundancy). In the example shown in Figure 3.6, each 
component receives the same inputs and forwards its output to the voter. If the voter detects any 
inconsistency, it reports a fault. For this reason, the value of N is usually set to an odd number, 
to avoid ties in the voting. The voter must also decide which output to use. Typical choices are 
letting the majority rule or choosing a computed average of the outputs.
Processor 1
Processor 2
Processor 3
Voter
Input
Output
Legend:
Processor
Data ﬂow
FIGURE 3.6  Tri-modular redundancy pattern

46  Chapter 3  Making Design Decisions
The TMR pattern is simple to understand and implement. It is independent of what might 
be causing disparate results and is only concerned with making a choice so that the system can 
continue to function.
As with the other redundancy patterns, there is a tradeoff between increasing the level 
of replication, which raises the cost, and the resulting availability. The statistical likelihood 
of two or more components failing is small, and three components is a common sweet spot 
between availability and cost.
3.4.2.5  Circuit Breaker
The Circuit Breaker pattern is sometimes seen as a performance pattern, but actually addresses 
both performance and availability concerns. (As this example shows, the label on a pattern is 
less important than the response measures that it helps you to achieve.) This pattern does not 
use redundancy, but it is important for achieving high availability in networked contexts, such 
as microservice architectures. A common availability tactic in networked systems is retry. For 
example, in the event of a timeout or fault when invoking a service, an invoker may try again 
(and again and again). A circuit breaker keeps the invoker from retrying infinitely many times, 
waiting for a response that may never come. This breaks the endless retry cycle when the 
circuit breaker deems that the system is dealing with a fault. Until the circuit is “reset”, subse-
quent invocations will return immediately without requesting the service.
One important benefit of this pattern is that it removes from individual components the 
policy of how many retries to allow before declaring a failure. The circuit breaker, in conjunc-
tion with software that listens to it and begins recovery, prevents cascading failures.
But care must be taken in choosing the timeout (or retry) values. If the timeout is too long, 
then unnecessary latency is added. If the timeout is too short, then the circuit breaker will trip 
when it does not need to, which can lower the availability and performance of services.
3.5  Design Concepts to Support Modifiability
Modifiability is about change. As architects, we need to consider and plan for the cost and risk 
of making anticipated changes. To be able to plan for modifiability, an architect has to con-
sider three big questions: (1) What can change? (2) What is the likelihood of the change? and 
(3) When is the change made and who makes it?
Change is so prevalent in the life of software systems that special names have been given 
to specific flavors of modifiability. Some of the common ones are scalability, variability,  
portability, and location independence, but there are many others.
3.5.1  Modifiability Tactics
Architects need to worry about modifiability to make the system easy to understand, debug, 
and extend. Tactics can help address these concerns.

3.5  Design Concepts to Support Modifiability  47
The modifiability tactics categorization is shown in Figure 3.7. There are three major  
categories of modifiability tactics: Increase Cohesion, Reduce Coupling, and Defer Binding. 
We will examine these in turn.
Increase Cohesion
Reduce Coupling
Defer Binding
Component Replacement
Compile-Time Parameterization
Aspects
Conﬁguration-Time Binding
Resource Files
Discovery
Interpret Parameters
Shared Repositories
Polymorphism
Encapsulate
Use an Intermediary
Abstract Common Services
Restrict Dependencies
Split Module
Redistribute Responsibilities
Modiﬁability Tactics
FIGURE 3.7  Modifiability tactics categorization
Within the Increase Cohesion category, the tactics are:
■
■Split module. If the module being modified includes responsibilities that are not  
cohesive, modification costs will likely be high. Refactoring the module to separate the 
responsibilities should reduce the average cost of future changes.
■
■Redistribute responsibilities. If (related) responsibilities A, A′, and A″ are sprinkled 
across several distinct modules, they should be grouped together into a single module.
Within the Reduce Coupling category, the tactics are:
■
■Encapsulate. Encapsulation introduces an explicit interface to an element, hiding its 
implementation details. This ensures that changes to the implementation do not affect 
the clients, as long as the interface remains stable. All access to the element must then 
pass through this interface.
■
■Use an intermediary. Intermediaries, such as proxies, bridges, or adapters, are used for 
breaking dependencies between a set of components Ci or between Ci and the system S.

48  Chapter 3  Making Design Decisions
■
■Abstract common services. When multiple elements provide services that are similar, it 
may be useful to hide them behind a common abstraction. The resulting encapsulation 
hides the details of the elements from other components in the system.
■
■Restrict dependencies. This tactic restricts which modules a given module interacts with 
or depends on.
And within the Defer Binding category, we distinguish the tactics according to how late 
in the life cycle functionality is bound to the system:
■
■Tactics to bind values at compile or build time: component replacement, compile-time 
parameterization, aspects.
■
■Tactics to bind values at deployment, startup, or initialization time: configuration-time 
binding, resource files.
■
■Tactics to bind values at runtime: discovery, interpret parameters, shared repositories, 
polymorphism.
3.5.2  Modifiability Patterns
In Section 3.2.3, we said that patterns differ from tactics in that tactics typically have just a 
single goal—the control of a quality attribute response—whereas patterns often attempt to 
achieve multiple goals and balance multiple competing forces. You might not think of some 
of the patterns that we present in this section as modifiability patterns at all. For example, we 
present the client-server pattern in this section. But first we begin with the uber modifiability 
pattern—layers.
3.5.2.1  Layered Pattern
In complex systems, there is a need to develop and evolve portions independently. For this rea-
son, developers need a clear and well-documented separation of concerns, so that modules may 
be independently developed and maintained. The software therefore needs to be segmented in 
such a way that the modules can be evolved separately with little interaction among the parts. 
To achieve this separation of concerns, we divide the software into units called layers. Each 
layer is a grouping of modules that offers a cohesive set of services. Layers completely parti-
tion the software, and each partition is exposed through a public interface. There is, ideally, a 
unidirectional allowed-to-use relation among the layers.
But layering has its costs and tradeoffs. The addition of layers adds up-front effort and 
complexity to a system. Layers also add runtime overhead; there is a performance penalty to 
every interaction that crosses multiple layers. In Section 5.2.3, we will see how this pattern 
may be realized in terms of layered APIs.
3.5.2.2  Strategy
The Strategy pattern allows the selection of an algorithm, typically from a family of related 
algorithms, at runtime. This pattern is a realization of the defer binding tactic. It makes it easy 
to define a rich set of behaviors or policies and switch between them, typically at runtime, 

3.6  Design Concepts to Support Security  49
as needed. This pattern eliminates the need to clutter the code with complex switch or if  
statements to provide for the various options. It also eases the burden for programmers who 
might later want to evolve the system, adding even more options. The only drawback to this 
pattern is that it adds some up-front complexity. Thus, it should not be used if only a small and 
stable number of alternative algorithms or policies need to be implemented.
3.5.2.3  Client-Server Pattern
The Client-Server pattern consists of a server providing services simultaneously to multiple 
distributed clients. The most common example is a web server providing information to mul-
tiple browser clients. You might not think of this pattern as a modifiability pattern at all. But 
patterns are complex and they can serve multiple purposes. Indeed, the Client-Server pattern 
offers many benefits:
■
■The connection between a server and its clients is established dynamically.
■
■There is no coupling among the clients.
■
■The number of clients can easily scale and is constrained only by the capacity of the server.
■
■Server functionality can be scaled if needed with no impact on the clients.
■
■Clients and servers can evolve independently as long as their APIs remain stable.
■
■Common services can be shared among multiple clients.
■
■For interactive systems, the interaction with a user is isolated to the client.
Not all of these benefits relate to modifiability, but certainly the aspects of this pattern  
that limit coupling and defer binding are obvious instantiations of modifiability tactics.  
A server typically has no prior knowledge of its clients.
The tradeoffs that this pattern introduces primarily relate to performance, availabil-
ity, and security. When this pattern is implemented, communication occurs over a network. 
Messages may be delayed by network congestion, leading to degradation (or at least unpre-
dictability) of performance. For clients that communicate with servers over a network shared 
by other applications, provisions must be made for security. In addition, servers may be single 
points of failure.
3.6  Design Concepts to Support Security
Security is a measure of the system’s ability to protect data and resources from unautho-
rized access while still providing legitimate access to authorized actors. The most common 
approaches to characterizing and analyzing security focus on three important characteristics: 
confidentiality, integrity, and availability (CIA):
■
■Confidentiality is the property that data or services are protected from unauthorized 
access.
■
■Integrity is the property that data or services are not subject to unauthorized manipulation.
■
■Availability is the property that the system will be there for legitimate use, consistent 
with its specifications.

50  Chapter 3  Making Design Decisions
The security tactics provide strategies for achieving confidentiality and integrity. We 
have already discussed availability.
3.6.1  Security Tactics
Secure facilities in the physical world permit only limited access to certain resources (e.g., 
by building walls, having locked doors and windows, and establishing security checkpoints), 
have means of detecting intruders (e.g., requiring visitors to wear badges, using motion detec-
tors), have deterrence mechanisms (e.g., armed guards, razor wire), have reaction mechanisms 
(e.g., automatic locking of doors), and have recovery mechanisms (e.g., off-site backups). These 
strategies are all relevant to computer-based systems and so lead to our four categories of 
security tactics: Detect Attacks, Resist Attacks, React to Attacks, and Recover from Attacks, as 
shown in Figure 3.8.
Revoke Access
Restrict Login
Inform Actors
Audit
Nonrepudiation
Detect Intrusion
Detect Service Denial
Verify Message Integrity
Detect Message Delivery Anomalies
Detect Attacks
Resist Attacks
React to
Attacks
Recover from
Attacks
Security Tactics
Identify Actors
Authenticate Actors
Authorize Actors
Limit Access
Limit Exposure
Encrypt Data
Separate Entities
Validate Input
Change Credential Settings
FIGURE 3.8  Security tactics categorization
Within the Detect Attacks category, the tactics are:
■
■Detect intrusion. This tactic compares network traffic or service request patterns within 
a system to a set of signatures or known patterns of malicious behavior stored in a 
database.

3.6  Design Concepts to Support Security  51
■
■Detect service denial. This tactic compares the pattern or signature of network traffic 
coming into a system to historical profiles of known denial-of-service (DoS) attacks.
■
■Verify message integrity. This tactic employs techniques such as checksums or hash 
values to verify the integrity of messages, resource files, deployment files, and  
configuration files.
■
■Detect message delivery anomalies. This tactic is used to detect man-in-the-middle 
attacks. If message delivery times are normally stable, then by checking the amount of 
time that it takes to deliver or receive a message, it becomes possible to detect suspicious 
timing behavior. Similarly, abnormal numbers of connections and disconnections may 
indicate such an attack.
Within the Resist Attacks category, the tactics are:
■
■Identify actors. Identifying actors (users or remote computers) focuses on identifying the 
source of any external input to the system. Users are typically identified through user 
IDs. Other systems may be identified through access keys, IP addresses, protocols, ports, 
or some other means.
■
■Authenticate actors. Authentication means ensuring that an actor is actually who or what 
it purports to be. Passwords, digital certificates, two-factor authentication, and biometric 
identification provide means for authentication.
■
■Authorize actors. Authorization means ensuring that an authenticated actor has the 
rights to access and modify either data or services. This mechanism is usually enabled 
by providing some access control mechanisms within a system.
■
■Limit access. This tactic involves limiting access to computer resources. Limiting access 
might mean restricting the number of access points to resources, or restricting the type 
of traffic that can go through the access points. Both kinds of limits minimize the attack 
surface of a system.
■
■Limit exposure. This tactic focuses on minimizing the effects of damage caused by a 
hostile action. It is a passive defense since it does not proactively prevent attackers from 
doing harm. Limiting exposure is typically realized by reducing the amount of data or 
services that can be accessed through a single access point, and hence compromised in a 
single attack.
■
■Encrypt data. Confidentiality is usually achieved by applying some form of encryption 
to data and to communication. Encryption provides extra protection to persistently main-
tained or in-transit data beyond that available from authorization.
■
■Separate entities. Separating different entities limits the scope of an attack. Separation 
within the system can be done through physical separation on different servers attached 
to different networks, the use of virtual machines, or an “air gap”—that is, by having no 
electronic connection between different portions of a system.
■
■Validate input. Cleaning and checking input as it is received by a system, or portion of 
a system, is an important early line of defense in resisting attacks. This is often imple-
mented by using a security framework or validation class to perform actions such as 
filtering, canonicalization, and sanitization of input.

52  Chapter 3  Making Design Decisions
■
■Change credential settings. Many systems have default security settings assigned when 
the system is delivered. Forcing the user to change those settings will prevent attackers 
from gaining access to the system through settings that may be publicly available.
Within the React to Attacks category, the tactics are:
■
■Revoke access. If the system or an administrator believes that an attack is under way, 
then access can be limited to sensitive resources, even for normally legitimate users  
and uses.
■
■Restrict login. Repeated failed login attempts may indicate a potential attack. Many 
systems limit access from a particular computer if there are repeated failed attempts to 
access an account from that computer.
■
■Inform actors. Ongoing attacks may require action by operators, other personnel, or 
cooperating systems. Such personnel or systems—the set of relevant actors—must be 
notified when the system has detected an attack.
Finally, within the Recover from Attacks category, the tactics are:
■
■Audit. We audit systems—that is, keep a record of user and system actions and their 
effects—to help trace the actions of, and to identify, an attacker. We may analyze audit 
trails to attempt to prosecute attackers or to create better defenses in the future.
■
■Nonrepudiation. This tactic guarantees that the sender of a message cannot later deny 
having sent the message and that the recipient cannot deny having received the message.
In addition, all of the availability tactics aid in recovering from attacks.
3.6.2  Security Patterns
Many security patterns have been developed over the years. Here we touch on just two of them, 
as examples: Intercepting Validator and Intrusion Detection.
3.6.2.1  Intercepting Validator
This pattern inserts a software element—an adapter—between the source and the destination 
of messages. This approach assumes greater importance when the source of the messages is 
outside the system.
The most common responsibility of this pattern is to implement the verify message integ-
rity tactic, but it can also incorporate tactics such as detect intrusion and detect service denial, 
or detect message delivery anomalies.
The benefit of this pattern is that, depending on the specific validator that you create and 
deploy, this pattern can cover most of the waterfront of the “detect attack” category of tactics, 
all in one package. The tradeoffs are that, as always, introducing an intermediary exacts a 
performance price.
Attack vectors change and evolve over time, so this component must be kept up-to-date 
to ensure that it maintains its effectiveness. This imposes a maintenance obligation on the 

3.7  Design Concepts to Support Integrability  53
organization responsible for the system. Of course, this maintenance cost must be paid in any 
case, if the system is to maintain its level of security.
3.6.2.2  Intrusion Detection
An intrusion prevention system (IPS) is a standalone element whose main purpose is to iden-
tify and analyze any suspicious activity. If the activity is deemed acceptable, it is allowed. 
Conversely, if it is suspicious, the activity is prevented and reported. These systems often 
implement most of the “detect attacks” and “react to attacks” tactics.
There are some tradeoffs associated with this pattern, however. For example, the patterns 
of activity that an IPS looks for change and evolve over time, so the patterns database must be 
constantly updated. Also, systems employing an IPS incur a performance cost. In addition, 
IPSs are typically created from commercial off-the-shelf components, which might mean that 
they are not tailored for your specific application.
3.7  Design Concepts to Support Integrability
Software architects need to be concerned about more than just making separately developed 
components cooperate: They must also consider the costs and technical risks of integration 
tasks. These risks may be related to schedule, performance, or technology. Consider that a 
project needs to integrate a set of components C1, C2, . . . , Cn into a system S. The task, then, 
is to design for, and analyze the costs and technical risks of, integrating additional, but not yet 
specified, components {Cn + 1, . . . , Cm}. We assume we have control over S, but the develop-
ment of the unspecified {Ci} components may be outside our control.
Integration difficulty—the costs and the technical risks—can be thought of as a function 
of the size of and the “distance” between the interfaces of {Ci} and S: Size is the number of 
potential dependencies between {Ci} and S, and distance is the difficulty of resolving differ-
ences in each of the dependencies. Distance may be any of the following: syntactic distance, 
data semantic distance, behavioral semantic distance, temporal distance, or even resource dis-
tance. Although we may not capture size and distance as precise metrics, this understanding 
creates a frame of reference for thinking about integrability.
3.7.1  Integrability Tactics
The goals for the integrability tactics are to reduce the costs and risks of adding new  
components, reintegrating changed components, and integrating sets of components to fulfill 
evolutionary requirements. These tactics help to reduce size and distance attributes.
There are three categories of integrability tactics: Limit Dependencies, Adapt, and 
Coordinate, as shown in Figure 3.9.

54  Chapter 3  Making Design Decisions
Integrability Tactics
Limit Dependencies
Adapt
Coordinate
Orchestrate
Manage Resources
Choreograph
Discover
Tailor Interface
Conﬁgure Behavior
Encapsulate
Use an Intermediary
Restrict Communication Paths
Adhere to Standards
Abstract Common Service
FIGURE 3.9  Integrability tactics categorization
The first category, Limit Dependencies, may remind you of the Reduce Coupling category 
of modifiability tactics. This is not surprising because limiting dependencies can be achieved, 
in large part, by reducing coupling. Within the Limit Dependencies category, the tactics are:
■
■Encapsulate. Encapsulation introduces an explicit interface to an element and ensures 
that all access to the element passes through this interface. Dependencies on the element 
internals are eliminated, because all dependencies must flow through the interface.
■
■Use an intermediary. Intermediaries are used for breaking dependencies between a set 
of components Ci or between Ci and the system S. Intermediaries can be used to resolve 
different types of dependencies (e.g., syntactic, behavior, data semantic).
■
■Restrict communication paths. This tactic restricts the set of elements with which a 
given element can communicate. In practice, this tactic is implemented by restricting an 
element’s visibility (when developers cannot see an interface, they cannot employ it) and 
by authorization (i.e., restricting access to only authorized elements).
■
■Adhere to standards. Standardization in system implementations is a primary enabler 
of integrability and interoperability, across both platforms and vendors. Some standards 
focus on defining syntax and data semantics. Others include richer descriptions, such as 
those describing protocols that include behavioral and temporal semantics.
■
■Abstract common services. Where multiple elements provide services that are similar, 
it may be useful to hide them behind a common abstraction. This abstraction might be 
realized as a common interface implemented by them all, or it might involve an inter-
mediary that translates requests for the abstract service to more specific requests. The 
resulting encapsulation hides the details of the elements from other components in the 
system.

3.7  Design Concepts to Support Integrability  55
Within the Adapt category, the tactics are:
■
■Discover. A discovery service is the mechanism by which applications and services 
locate each other. Entries in a discovery service are there because they were registered. 
This registration can happen statically, or it can happen dynamically when a service is 
instantiated.
■
■Tailor interface. Tailoring an interface is a tactic that adds capabilities to, or hides 
capabilities in, an existing interface without changing the API or implementation. 
Capabilities such as translation, buffering, and data smoothing can be added to an inter-
face without changing it.
■
■Configure behavior. The behavior of a component can be configured during the build 
phase (recompile with a different flag), during system initialization (read a configuration 
file or fetch data from a database), or during runtime (specify a protocol version as part 
of your requests).
Finally, within the Coordinate category, the tactics are:
■
■Orchestrate. This tactic uses a control mechanism to coordinate and manage the invoca-
tion of services so that they can remain unaware of each other. Orchestration helps with 
the integration of a set of loosely coupled reusable services to create a system that meets 
a new need.
■
■Choreograph. Choreography is an alternative control mechanism in which a process is 
implemented as steps in a flow, each triggered by an event message upon completion of 
the previous step and unaware of the other steps in the flow. This results in composable, 
loosely coupled services without an orchestrator. Orchestration is centralized, whereas 
choreography is distributed.
■
■Manage resources. A resource manager is a form of intermediary that governs access to 
computing resources; it is similar to the restrict communication paths tactic. With this 
tactic, software components are not allowed to directly access some computing resources 
(e.g., threads or blocks of memory), but instead request those resources from a manager.
3.7.2  Integrability Patterns
Many patterns exist to support integrability. In this section, we briefly describe just a few of 
them. The first three are related—adapters, bridges, and mediators.
3.7.2.1  Adapters, Bridges, and Mediators
An adapter is a form of encapsulation whereby some component is encased within an alter-
native abstraction. An adapter is the only element allowed to use that component; every other 
piece of software uses the component’s services by going through the adapter. The adapter 
transforms the data or control information for the component it wraps.
A bridge translates some “requires” assumptions of one arbitrary component to some 
“provides” assumptions of another component. The key difference between a bridge and an 
adapter is that a bridge is independent of any particular component. Also, the bridge must 

56  Chapter 3  Making Design Decisions
be explicitly invoked by some external agent—possibly, but not necessarily, by one of the  
components the bridge spans.
Mediators exhibit properties of both bridges and adapters. The major distinction between 
bridges and mediators is that mediators incorporate a planning function that results in runtime 
determination of the translation, whereas bridges establish this translation at bridge construc-
tion time.
All three of these patterns allow access to an element without forcing a change to the 
element or its interface. But these benefits, of course, come at a cost. Creating any of these 
patterns requires up-front development work. Moreover, all of the patterns introduce some 
performance overhead while accessing the element, although typically this overhead is small.
As a side note, a fourth pattern with a similar intent is Facade. We will discuss an  
example of the Facade pattern in Chapter 5, where it is instantiated as an API gateway.
3.7.2.2  Services
Another pattern often used to ease integrability concerns is services. A service (whether 
“micro” or not) is an independent decoupled software component that can be developed, 
deployed, and scaled independently. The Services pattern describes a collection of distributed 
components that provide and consume services. Components have interfaces that describe the 
services that they request and that they provide. A service’s quality attributes can be specified 
and guaranteed with a service level agreement (SLA). Components perform their computa-
tions by requesting services from one another.
The benefits of the Services pattern are that services are designed to be used by a variety 
of clients, leading them to be more generic. Also, services are independent: The only method 
for accessing a service is through its interface and through asynchronous messages over a net-
work. This means that services typically have loose coupling with other services and with their 
environment. Finally, services can be implemented heterogeneously, using whatever languages 
and technologies are most appropriate.
However, service-based architectures, because of their heterogeneity and distinct own-
ership, come with many interoperability mechanisms such as Web Services Description 
Language (WSDL) and Simple Object Access Protocol (SOAP). This adds some up-front com-
plexity and overhead. (We will discuss APIs and API-centric design in Chapter 5.)
3.7.2.3  Dynamic Discovery
Dynamic discovery applies the discover tactic to support finding service providers at runtime. 
A dynamic discovery capability sets the expectation that the system will advertise the services 
available for integration and the information that will be available for each service.
The main benefit of this pattern is that it allows for flexibility in binding services together 
into a cooperating whole. For example, services may be chosen at startup or dynamically at 
runtime based on their pricing or availability or other properties. The cost of this flexibility is 
that dynamic discovery registration and de-registration must be automated, and tools for this 
purpose must be acquired or generated.

3.9  Further Reading  57
3.8  Summary
This chapter, though quite long, barely scratched the surface of making design decisions. Even 
so, it identified some key principles for making design decisions—a way to reason rigorously 
about decision-making. In addition, we provided several examples of the design concepts (pat-
terns and tactics) that serve as the building blocks of design. We hope that you are now pre-
pared to examine other quality attributes, or other patterns for the quality attributes discussed 
here, in exactly the same way as demonstrated in this chapter. The ones we presented here are 
merely the most common ones, but they are not special in any other way.
Technologies (externally developed components) are also design concepts, though we did 
not discuss these extensively here. In subsequent chapters of this book (for example, Chapter 7, 
where we discuss cloud-based solutions), we will explore other specific design concerns such 
as technology choices.
This is all good news for the architect. Design, it turns out, is not mysterious. The design 
decisions that you can make for quality attributes encompass a substantial body of knowledge, 
but it is both tractable and learnable.
3.9  Further Reading
The nine decision-making principles presented here were first described in A. Tang and 
R. Kazman, “Decision-Making Principles for Better Software Design Decisions”, IEEE 
Software, 38, November–December 2021.
There are many references for design and architecture patterns; we identified a few 
of these in Chapter 2. Among the best known are the “original” design patterns book: E. 
Gamma, R. Helm, R. Johnson, and J. Vlissides, Design Patterns: Elements of Reusable 
Object-Oriented Software by Addison-Wesley, 1994; and the book by F. Buschmann,  
R. Meunier, H. Rohnert, P. Sommerlad, and M. Stal, Pattern-Oriented Software Architecture 
Volume 1: A System of Patterns, Wiley, 1996. The latter book was the beginning of a series 
that eventually totaled five volumes. But the set of patterns is never complete; new patterns 
continue to be created all the time. For example, you can find a catalog of microservice  
patterns here: https://microservices.io/patterns/.
You can read about the tactics presented here, and other tactics, in greater detail, by  
L. Bass, P. Clements, and R. Kazman, Software Architecture in Practice, 4th edition, 
Addison-Wesley, 2021.
The SWEBOK summarizes a set of nine software design principles: Hironori Washizaki, 
ed., Guide to the Software Engineering Body of Knowledge (SWEBOK), Version 4.0, IEEE 
Computer Society, 2024; www.swebok.org.

58  Chapter 3  Making Design Decisions
Several online catalogs of reference architectures have been developed by different cloud 
providers. They can be found at the following links:
■
■
https://learn.microsoft.com/en-us/azure/architecture/browse/
■
■
https://aws.amazon.com/architecture/
■
■
https://cloud.google.com/architecture
3.10  Discussion Questions
1.	
Humans are inevitably biased. How do the nine decision-making principles help to  
combat bias? How might you use these principles in practice?
2.	
If you had to design an architecture for a quality attribute driver that was not listed in 
this chapter, how would you go about determining (and validating) the tactics and  
patterns, including their costs, benefits, and tradeoffs?
3.	
How could you use a tactic to modify and improve a design pattern? Give a concrete 
example.
4.	
What is the relationship between tactics, patterns, and externally developed components? 
How can you use this knowledge to select and analyze components? Give an example.
5.	
Tactics are foundational, but the categorization can never be said to be “done”. New 
designs and design approaches continually emerge over time. Can you identify a tactic 
that was not included in the categorizations presented here?

59
4
The Architecture Design 
Process
In this chapter, we provide a detailed discussion of ADD, the design method that is at the heart 
of this book. We begin with an overview of the method and of each of its steps. This overview 
is followed by more detailed discussions of different aspects that need to be considered when 
performing these steps. We also discuss the identification and selection of design concepts, the 
production of structures from these design concepts, the definition of interfaces, the produc-
tion of preliminary documentation, and, finally, a technique to track design progress. 
Why Read This Chapter?
Architectural design is the foundation for any complex software system. Unfortunately, 
it is most often done in an ad hoc way, if at all. It is our belief, and our experience, that 
having a structured way of doing design results in better, more predictable outcomes.  
In this chapter, we introduce a method that allows design to be performed in a struc-
tured way; we hope to convince you that this approach truly leads to better outcomes.
4.1  The Need for a Principled Method
In Chapter 2, we discussed the various concepts associated with design. The question is, how 
do you actually perform design? Performing design to ensure that the drivers are satisfied 
requires a principled method. By “principled”, we refer to a method that takes into account all 
of the relevant aspects that are needed to produce an adequate design. Such a method provides 
guidance that is necessary to guarantee that your drivers are satisfied. To achieve this goal in 
a cost-effective, repeatable way, you need a method that guides you in combining and incorpo-
rating reusable design concepts.

60  Chapter 4  The Architecture Design Process
Performing design adequately is important because architecture design decisions have 
significant consequences at different points in a project’s lifetime. For example, during the 
estimation phase, an appropriate design will allow for a better estimation of cost, scope, and 
schedule. During development, an appropriate design will be helpful to avoid later rework and 
facilitate development and deployment. Finally, a clear understanding of what architectural 
design involves is necessary to better manage aspects of technical debt.
4.2  Attribute-Driven Design 3.0
Architecture design is performed in a series of rounds across the development of a software 
project. Each design round may take place within a project increment such as a sprint. Within 
these rounds, a number of design iterations are performed. Perhaps the most important charac-
teristic of the ADD method is that it provides detailed, step-by-step guidance on the tasks that 
have to be performed during the design iterations.
ADD has been used successfully for more than 20 years. Version 2.0 was published in 
2006, and version 3.0 was published in 2016, in the first edition of this book. While dramatic 
changes have occurred in the development landscape since the publication of version 3.0, the 
method itself remains unchanged. That attests to the general applicability of ADD.
We have added chapters in the book that explain how ADD can be used with the software 
development techniques that are widely used today, but the steps and artifacts of the method, 
which are shown in Figure 4.1 remain the same. This is good news for you as an architect!
In the following subsections, we provide an overview of the activities in each of the ADD 
steps. Note that we present them as linear steps, but there may be significant interactions 
among them, resulting in micro-iterations. This is the nature of design; it is not linear.
4.2.1  Step 1: Review Inputs
Before starting a design round, you need to make sure that the inputs to the design process 
are available and correct. First, you need to ensure that you are clear about the purpose for 
the design activities that will ensue. The purpose may be, for example, to produce a design 
for early estimation, to refine an existing design to build a new increment of the system, or to 
design and generate a prototype to mitigate certain technical risks (see Section 2.4.1 for a dis-
cussion of the design purpose). Also, you need to make sure that the other drivers needed for 
the design activity are available. These include primary functional requirements, quality attri-
bute scenarios, architectural constraints, and concerns. Finally, if this is not the first design 
round, or if this is not greenfield development, an additional input that you need to consider is 
the existing architecture design.

4.2  Attribute-Driven Design 3.0  61
Design
Purpose
Primary
Functionality
Quality
Attributes
Architectural
Concerns
Constraints
Step 1: Review inputs
Step 2: Establish iteration goal 
by selecting drivers
Step 3: Choose one or more elements of the system to
reﬁne
Step 4: Choose one or more design concepts that satisfy
the selected drivers
Step 5: Instantiate architectural elements, allocate
responsibilities, and deﬁne interfaces
Step 6: Sketch views and record design decisions
From previous round of iterations or from existing
system (brownﬁeld development)
Key
Driver
Architecture design
Process step
Key design step
Precedence
Artifact ﬂow
Iterate if necessary
Step 7: Perform analysis of current design and review
iteration goal and achievement of design purpose
(Reﬁned) Software
Architecture Design
FIGURE 4.1  Steps and artifacts of ADD version 3.0
At this point, we assume that the primary functionality and quality attribute scenarios 
have been prioritized, ideally by your most important project stakeholders. (If not, you can 
employ various techniques to elicit and prioritize them, as discussed in Sections 2.4.2 and 
2.4.3.) You, as the architect, must now “own” these drivers. You need to check, for exam-
ple, whether any important stakeholders were overlooked in the original requirements elici-
tation process, or whether any business conditions have changed since the prioritization was 

62  Chapter 4  The Architecture Design Process
performed. These drivers really do “drive” design, so getting them right and getting their 
priority right is crucial. We cannot stress this point strongly enough. Software architecture 
design, like most activities in software engineering, is a “garbage in, garbage out” process. 
The results of ADD cannot be good if the inputs are poorly formed.
As a rule of thumb, you should be able to start designing if, besides the design purpose, 
constraints, and initial architectural concerns, you have established the primary use cases and 
the most important quality attribute scenarios. This, of course, does not mean you will make 
decisions focusing on only these drivers: You still need to address other quality attribute sce-
narios, use cases, and architectural concerns, but these can be treated later on.
The drivers become part of an architectural design backlog that you should use to  
perform the different design iterations. We discuss this idea in more depth in Section 4.8.1.
4.2.2  Step 2: Establish the Iteration Goal by Selecting Drivers
A design round comprises the architecture design activities performed within a development 
cycle if an iterative development model is used, or the whole set of architecture design activi-
ties if a waterfall model is used. Through one or more rounds, you produce an architecture that 
suits the established design purpose.
A design round generally takes the form of a series of design iterations, where each iter-
ation focuses on achieving a particular goal. Such a goal typically involves designing to sat-
isfy a subset of the drivers. For example, an iteration goal could be to create structures from 
elements that will support a particular performance scenario, or that will enable a use case to 
be achieved. For this reason, when performing design, you need to establish a goal before you 
start a particular design iteration.
Selecting an adequate goal for a design iteration can sometimes be challenging. In some 
cases, the goal may be too small (a unique and relatively simple driver is selected) or too big 
(too many drivers are selected). The following guidelines can help you establish a “right-sized” 
iteration goal:
■
■The goal may be to address a single important driver—for example, the architectural 
concern of decomposing the system or addressing a challenging quality attribute 
scenario.
■
■The goal may be to make decisions to satisfy a set of similar drivers—for example, 
making design decisions to address a family of use cases or related quality attribute 
scenarios.
■
■The goal may be to make decisions to satisfy a set of related drivers—for example, mak-
ing design decisions to address a user story and an associated quality attribute scenario.
As we will discuss in Section 4.3, depending on the type of system whose architecture is 
being designed, there may be a “best”—or at least strongly suggested—ordering of the itera-
tion goals that need to be addressed. For example, for a greenfield system in a mature domain, 
your initial goal is typically to identify an overall structure for the system by choosing a refer-
ence architecture.

4.2  Attribute-Driven Design 3.0  63
4.2.3  Step 3: Choose One or More Elements of the System to Refine
This step is where the core design activities start. Satisfying drivers requires you to produce 
one or more architectural structures. These structures are composed of interrelated elements, 
and those elements are generally obtained by refining other elements that you previously iden-
tified in an earlier iteration. Refinement can mean decomposition into finer-grained elements 
(top-down approach), combination of elements into coarser-grained elements (bottom-up 
approach), or improvement of previously identified elements. For greenfield development, you 
can start by establishing the system context and then selecting the only available element—
that is, the system itself—for refinement by decomposition. For existing systems or for later 
design iterations in greenfield systems, you would normally choose to refine elements that 
were identified in prior iterations.
The elements that you will select are the ones that are involved in the satisfaction of 
specific drivers. For this reason, when design is performed for an existing system, you need 
to have a good understanding of the elements that are part of the as-built architecture of the 
system. This may involve some “detective work”, reverse engineering, or discussions with 
developers.
We have presented steps 2 and 3 in the order they appear in the ADD method. That is to 
say, step 2 precedes step 3. However, in some cases you may need to reverse this order. For 
example, when you are designing a greenfield system or fleshing out certain types of reference 
architectures, you will—at least in the early stages of design—focus on elements of the system 
and start the iteration by selecting a particular element; only then will you consider the drivers 
that you want to address.
4.2.4  Step 4: Choose One or More Design Concepts That Satisfy the 
Selected Drivers
Choosing the design concepts is probably the most difficult decision you will face in the 
design process, because it requires you to identify alternatives among design concepts that 
can be used to achieve your iteration goal, and to select one of these alternatives. As we saw 
in Section 3.2, different types of design concepts exist, and, for each type, there may be many 
options. This can result in a considerable number of alternatives that need to be analyzed to 
make a choice, though the existence of architecture standards may help to constrain the num-
ber of viable alternatives. In Section 4.4, we discuss the identification and selection of design 
concepts in more detail.
4.2.5  Step 5: Instantiate Architectural Elements, Allocate Responsibilities, 
and Define Interfaces
Once you have selected one or more design concepts, you must make another design decision, 
which involves instantiating elements out of the design concepts that you selected. Instantiation 
means adjusting the selected design concepts to the problem at hand. For example, if you 

64  Chapter 4  The Architecture Design Process
selected the Layers pattern as a design concept, you must decide how many layers will be 
used, since the pattern itself does not prescribe a specific number. In this example, the layers 
are the elements that are instantiated. If you selected a particular availability tactic, such as 
“redundant spare”, instantiation implies deciding how this particular tactic will be achieved, 
such as by adding replication to a particular element. In certain cases, instantiation can mean 
configuration. For example, you may have dedicated an iteration to selecting technologies and 
associating them with the elements in your design. In further iterations, you might refine these 
elements by making finer-grained decisions about how they should be configured to support a 
particular driver, such as a quality attribute.
After instantiating the elements, you need to allocate responsibilities to each of them. For 
example, in a typical back-end of a web-based enterprise system, at least three layers are usu-
ally present: the API layer, the business layer, and the data layer. The responsibilities of these 
layers differ: The responsibilities of the API layer include exposing endpoints, whereas the 
responsibilities of the data layer include managing the persistence of data. An additional utility 
layer may be considered as well.
Instantiating elements is just one of the tasks you need to perform to create structures 
that satisfy a driver or a concern. The elements that have been instantiated also need to be 
connected, thereby enabling them to collaborate with one another. This requires the existence 
of relationships between the elements and the exchange of information through some kind of 
interface. The interface is a contractual specification of how information should flow between 
the elements. Section 4.5 provides more details on how the different types of design concepts 
are instantiated and how structures are created, and Section 4.7 discusses how interfaces can 
be defined.
The activities that are performed in this step are frequently performed by creating some 
type of diagram, either on a whiteboard or with some diagramming tool. Finally, it should be 
noted there may be a lot of back and forth (micro-iterations) among steps 3 and 5, as these are 
the central steps of the process in which design decisions are made.
4.2.6  Step 6: Sketch Views and Record Design Decisions
At this point, you have completed the design activities for the iteration. Nevertheless, you 
may not have taken any actions to ensure that the views—the representations of the structures 
you created—are preserved. For instance, if you performed the previous step in a conference 
room, you probably ended up with a series of diagrams on a whiteboard. This information is 
essential, and you need to capture it so that you can later analyze and communicate it to other 
stakeholders.
The views that you have created are almost certainly incomplete, so these diagrams may 
need to be revisited and refined in a subsequent iteration. This is typically done to accommo-
date elements resulting from other design decisions that you will make to support additional 
drivers. This factor explains why we speak of “sketching” the views in ADD—that is, creating 
a preliminary type of documentation. The more formal, more fully fleshed-out documenta-
tion of these views—should you choose to produce it—occurs only after a number of design 
iterations have been finished (as part of the architectural documentation activity discussed in 
Section 1.2.2).

4.2  Attribute-Driven Design 3.0  65
In addition to storing the sketches of the views, you should record the significant  
decisions that are made in the design iteration, and the reasons that led to these decisions 
(i.e., the rationale), to facilitate later analysis and understanding of the decisions. For example, 
decisions about important tradeoffs might be recorded at this time. During a design iteration, 
decisions are primarily made in steps 4 and 5. Section 4.7 provides further information on how 
to create preliminary documentation during design, including creating sketches and recording 
design decisions and their rationales.
4.2.7  Step 7: Perform Analysis of Current Design and Review Iteration Goal 
and Achievement of Design Purpose
By the time you reach step 7, you should have created a partial design that addresses the goal 
established for the iteration. Making sure that this is actually the case is a good idea, because it 
helps you avoid unhappy stakeholders and later rework. You can perform the analysis yourself 
by reviewing the sketches of the views and design decisions that you recorded, but an even 
better idea is to have someone else help you review this design. We recommend this practice 
for the same reason that organizations frequently establish a separate testing/quality assurance 
group: Another person will not share your assumptions, and will have a different experience 
base and a different perspective. Pulling in someone with a different point of view can help 
you find flaws, in both code and architecture. We discuss analysis in more depth in Chapter 11.
Once the design performed in the iteration has been analyzed, you should review the 
state of your architecture in terms of the established design purpose. This means considering 
whether you have performed enough design iterations to satisfy the drivers that are associated 
with the design round, as well as whether the design purpose has been achieved or if additional 
design rounds are needed in future project increments. Section 4.8 describes simple techniques 
that allow you to keep track of design progress.
4.2.8  Iterate If Necessary
Ideally, you should perform additional iterations and repeat steps 2 to 7 for every driver that 
was considered as part of the input. More often than not, such iterations are not possible 
because of time or resource constraints that force you to stop the design activities and move on 
to the next activities in the development process—typically implementation.
What are the criteria for evaluating whether more design iterations are necessary? We let 
risk be our guide. You should at least have addressed the drivers with the highest priorities. 
Ideally, you should have assured that the critical drivers are satisfied or, at least, that the design 
is “good enough” to satisfy them. Finally, when performing iterative development, you can 
choose to perform one design round in every project iteration. The initial rounds should focus 
on addressing the drivers with the highest priorities, while subsequent rounds focus on making 
design decisions for other drivers with lower priorities or on new drivers that appear as devel-
opment progresses.

66  Chapter 4  The Architecture Design Process
4.3  Applying ADD to Different System Contexts
When writing a paper or an essay, you might have experienced the much-dreaded “fear of the 
blank page”. Similarly, when you start designing an architecture, you may face a situation in 
which you ask yourself, “How do I begin designing?” To answer this question, you need to 
consider which type of system you are designing.
Design of software systems falls into three broad categories: (1) design of a greenfield 
system for a mature (i.e., well-known) domain; (2) design of a greenfield system for a domain 
that is novel (i.e., a domain that has a less established infrastructure and knowledge base); 
and (3) design for making changes to an existing system (brownfield). Each of these catego-
ries involves different uses of ADD and different ways that ADD can be applied to guide the 
design process.
4.3.1  Design of Greenfield Systems for Mature Domains
The design of a greenfield system for a mature domain occurs when you are designing an 
architecture for a system that is built from “scratch” and when this type of system is well 
known and understood—that is, when there is an established portfolio of tools and technolo-
gies, and an associated knowledge base. Examples of mature domains include the following:
■
■Traditional desktop applications
■
■Interactive applications that run on a mobile device
■
■Enterprise applications accessed from a web browser, which store information in a 
relational database, and which provide support for partially or fully automating business 
processes
■
■A microservice that manages a single domain entity
Because these types of applications are relatively common, some general architectural 
concerns associated with their design are well known, well supported, and well documented. 
If you are designing a new system that falls into this category, we recommend following the 
roadmap shown in Figure 4.2.
The goal of your initial design iteration(s) should be to address the general architectural 
concern of establishing an initial overall system structure. Is this to be a three-tier client-server 
application, a peer-to-peer application, a mobile app connecting to a Big Data back-end, or 
something else? Each of these options will lead you to different architectural solutions, and 
these solutions will help you to achieve your drivers. To achieve this iteration goal, you will 
select some design concepts. Specifically, you will typically choose one or more reference 
architectures and patterns, including patterns for deployment of the system (see Section 6.2). 
You may also select some externally developed components, such as frameworks. The types 
of frameworks that are typically chosen in early iterations are either “full-stack” frameworks 
that are associated with the selected reference architectures, or more specific frameworks 
that are associated with elements established by the reference architecture (see Section 3.2.1).  

4.3  Applying ADD to Different System Contexts  67
In this first iteration, you should review all of your drivers when selecting the design concepts, 
but you will probably pay more attention to the constraints and to quality attributes that are 
not associated with specific functionalities and that favor particular reference architectures or 
require particular deployment configurations. Consider an example: If you select a reference 
architecture for Big Data systems, you have presumably chosen a quality attribute such as low 
latency with high data volumes as your most important driver. Of course, you will make many 
subsequent decisions to flesh out this early choice, but this driver has already exerted a great 
influence on your design, such as the selection of a particular reference architecture.
Iteration
Goal
Design Concepts
Establishing
an initial
overall system
structure
Identifying
structures to
support
primary
functionality
3 ... N
Reﬁning
previously
created
structures to
fully address
the
remaining
drivers
Reference Architectures
1
2
Deployment Patterns
(tiers)
Externally Developed
Components
Architectural Patterns
(Domain objects/components)
Externally Developed
Components
Tactics
Architectural Patterns
Deployment Patterns
Externally Developed
Components
Legend:
Design concept
Design concept
(optional)
“Followed by”
FIGURE 4.2  Design concept selection roadmap for greenfield systems

68  Chapter 4  The Architecture Design Process
The goal of your next design iteration(s) should be to identify structures that support the 
primary functionality. As noted in Section 2.4.3, allocation of functionality (i.e., use cases 
or user stories) to elements is an important part of architectural design because it has critical 
downstream implications for modifiability and allocation of work to teams. Furthermore, once 
functionality has been allocated, the elements that support it can be refined in later iterations 
to support the quality attributes associated with these functionalities. For example, a perfor-
mance scenario may be associated with a particular use case. Achieving the performance goal 
may require making design decisions across all of the elements that participate in the satisfac-
tion of this use case. To allocate functionality, you usually refine the elements that are associ-
ated with the reference architecture by decomposing them. A particular use case may require 
the identification of multiple elements. For example, if you have selected a web application 
reference architecture, supporting a use case will probably require you to identify modules 
across the different layers associated with this reference architecture. Finally, at this point you 
should start thinking about allocating functionality—associated with modules—to (teams of) 
developers.
The goal of your subsequent design iterations should be to refine the structures you have 
created so far to fully address the remaining drivers. Addressing these drivers, and espe-
cially quality attributes, will likely require you to use the three major categories of design  
concepts—tactics, patterns, and externally developed components such as frameworks—as 
well as commonly accepted design best practices such as modularity, low coupling, and high 
cohesion. For example, to (partially) satisfy a performance requirement for the search use case 
in a web application, you might select the “maintain multiple copies of data” tactic and imple-
ment this tactic by configuring a cache in a framework that is used inside an element responsi-
ble for persisting data.
This roadmap is appropriate for the initial project iterations, but it is also extremely useful 
for early project estimation activities (see the discussion of designing to support estimation in 
Section 12.1.1). Why have we created such a roadmap? First, because the process of starting an 
architectural design is always complex. Second, because many of the steps in this roadmap are 
frequently overlooked or done in an intuitive and ad hoc way, rather than in a well-considered, 
reflective way. Third, because different types of design concepts exist, and it is not always 
clear at which point in the design they should be used. This roadmap encapsulates best prac-
tices that we have observed in the most competent architecture organizations. Simply put, the 
use of a roadmap results in better architectures, particularly for less mature architects.
4.3.2  Design of Greenfield Systems for Novel Domains
In the case of novel domains, it is more challenging to establish a precise roadmap, because 
reference architectures may not exist and there may be few, if any, externally developed com-
ponents that you can use. You are, more than likely, working from first principles and creating 
your own home-grown solutions. Even in this case, however, general-purpose design concepts 
such as tactics and patterns can guide you, aided by strategic prototyping. In essence, your 
iteration goals will mostly be to continuously refine the previously created structures to fully 
address the drivers.

4.3  Applying ADD to Different System Contexts  69
Many times, your design goal will focus on the creation of prototypes so that you can 
explore possible solutions to the challenge that you are facing. In particular, you may need to 
focus on quality attributes and design challenges oriented toward issues such as performance, 
scalability, or security. We discuss the creation of prototypes in Section 4.4.2.
Of course, the notion of “novel” is fluid. Mobile application development was a novel 
domain 15 or 20 years ago, but now it is a well-established field.
4.3.3  Design for an Existing System (Brownfield)
Architecture design for an existing system may occur for different purposes. The most obvious 
is maintenance—that is, when you need to satisfy new requirements or correct issues, and 
doing so requires changes to the architecture of an existing system. You may also be mak-
ing architectural changes to an existing system for the purpose of refactoring (see Chapter 
10). When refactoring, you change the architecture of an existing system, without altering its 
functions, to reduce technical debt, to introduce technology updates, or to fix quality attribute 
problems (e.g., the system is too slow, or insecure, or crashes frequently).
To choose which elements to decompose as part of the design process (step 3 of ADD), 
you need to first identify which elements are present in the architecture of the existing system. 
In this sense, before starting the design iterations, your first goal should be to ensure that you 
have a clear understanding of the existing architecture of the system.
Once you understand the elements, properties, and relationships that constitute the sys-
tem’s architecture, and the characteristics of the existing code base, the design steps can be 
performed similarly to what is done for greenfield systems after the initial design iteration. 
Your design iteration goals here will be to identify and refine structures to satisfy architectural 
drivers, including new functionality and quality attributes, and to address specific architec-
tural concerns. These design iterations will typically not involve establishing a new overall 
system structure unless you are dealing with a major refactoring.
4.3.4  Design to Replace a Legacy Application
As systems age, many of the technologies they were built upon become increasingly obsolete, 
such that their replacement can be both complex and expensive. Also, some of these systems 
have accumulated so much technical debt that its repayment becomes infeasible (see Chapter 
10). When these situations occur, replacement of the system often emerges as the only feasible 
choice. But complex systems are difficult to replace all at once; instead, a gradual replacement 
approach is often less risky and more palatable to management.
Patterns such as the Strangler Fig offer some help in this task. This pattern allows an 
existing application to be replaced gradually while it keeps operating. The application to be 
replaced is typically moved behind a proxy, which initially delegates all of the service calls 
from clients to the legacy application. Parts of the legacy application then are redesigned and 
implemented—for example, following a microservices approach if the existing application is 
a monolith. Once the new parts are deployed, the proxy begins to route calls to them, without 
affecting existing clients. This approach proceeds gradually until all of the invocations that 

70  Chapter 4  The Architecture Design Process
were originally processed by the legacy application are now processed by newly developed 
components. At that point, the legacy application has been “strangled” and can be removed 
from service; the replacement is complete. The name “Strangler Fig” was chosen as an  
analogy to its botanical counterpart: This fig plant slowly surrounds an existing tree, which 
eventually dies once it is strangled.
When performing this kind of gradual replacement, design iteration goals focus on 
replacing one or more services provided by the legacy application. Each design iteration hones 
in on new architectural choices for those replacement services.
4.4  Identifying and Selecting Design Concepts
Freeman Dyson, the English physicist, once said, “A good scientist is a person with origi-
nal ideas. A good engineer is a person who makes a design that works with as few original 
ideas as possible”. This remark is particularly relevant in the context of software architecture 
design: Most of the time you don’t need to, and shouldn’t, reinvent the wheel. Rather, your 
major design activities are to identify and select design concepts to address the challenges and 
drivers that you encounter across the design iterations. Design is still an original and creative 
endeavor, but the creativity resides in the appropriate identification of these existing solutions 
and then on combining and adapting them to the problem at hand.
4.4.1  Identification of Design Concepts
The identification of design concepts can appear to be daunting, because of the vast number 
of design concepts that exist. There are likely dozens of design patterns, tactics, and exter-
nally developed components that you could use to address any particular issue. Making things 
even more complicated, these design concepts are scattered across many different sources: 
in the popular press, in research literature, in books, and on the Internet. Moreover, in many 
cases, there is no canonical definition of a concept. Different sites, for example, will define 
the Broker pattern in different, largely informal ways. Finally, once you have identified the 
alternatives that might potentially help you achieve the design goals of the iteration, you need 
to select among them.
To identify which design concepts you need at a particular point, recall our earlier dis-
cussion of design for different system contexts. Different points in the design process usually 
require different types of design concepts. For example, when you are designing a greenfield 
system in a mature domain, the types of design concepts that will help you initially structure 
the system are reference architectures and deployment patterns. As you progress in the design 
process, you will use all of the categories of design concepts: tactics, architecture and design 
patterns, and externally developed components (see Chapter 3). Keep in mind that to address 

4.4  Identifying and Selecting Design Concepts  71
a specific design problem, you can and often will use and combine different types of design  
concepts. For example, when addressing a security driver, you might employ a security  
pattern, a security tactic, a security framework, or some combination of these.
Once you have more clarity regarding the types of design concepts that you wish to use, 
you still need to identify alternatives—that is, design candidates. There are several ways to do 
so, although you will probably use a combination of these techniques rather than just a single 
method:
■
■Leverage existing best practices. You can identify alternatives for your required design 
concepts by making use of catalogs that are available in printed or online form. Some 
design concepts, such as patterns, are extensively documented; others, such as externally 
developed components, are documented in a less thorough way. The benefits of this 
approach are twofold: You can identify many alternatives, and you can leverage the con-
siderable knowledge and experience of others. The downsides are that searching for and 
studying the information can require a considerable amount of time, the quality of the 
documented knowledge is often unknown, and the assumptions and biases of the authors 
are unknown.
■
■Leverage your own knowledge and experience. If the system you are designing is similar 
to other systems you have designed in the past, you will probably want to begin with 
some of the design concepts that you have used before. The benefit of this approach is 
that the identification of alternatives is performed rapidly and confidently. The downside 
is that you may end up using the same ideas repeatedly, even if they are not the most 
appropriate for all the design problems that you are facing, and even if they have been 
superseded by newer, better approaches. As the saying goes, “If you give a small child a 
hammer, all the world looks like a nail”.
■
■Leverage the knowledge and experience of others. As an architect, you have a wealth 
of background information and knowledge that you have gained through the years. This 
foundation varies from person to person, especially if the types of design problems 
they have addressed in the past differ. You can leverage this information by perform-
ing the identification and selection of design concepts with some of your peers through 
brainstorming.
4.4.2  Selection of Design Concepts
Once you have identified a list of alternative design concepts, you need to select which one is 
the most appropriate to solve the design problem at hand. You can achieve this in a relatively 
simple way—by creating a table that lists the pros and cons associated with each alternative, 
and selecting one of the alternatives based on those criteria and your drivers. Table 4.1 shows 
a simplified example of such a table used to support the selection of different deployment 
paradigms.

72  Chapter 4  The Architecture Design Process
TABLE 4.1  Example of a Table to Support the Selection of Alternatives
Name of Alternative Pros
Cons
Monolithic application
Easier to manage and deploy.
Updates require redeployment of the whole 
application.
Horizontal scaling requires creating a 
replica of the entire application.
Microservices-based 
application
Services can be replicated  
and updated independently.
More complex management.
You may also need to perform a more in-depth analysis to select the best alternative. 
Methods such as CBAM (cost/benefit analysis method) or SWOT (strengths, weaknesses, 
opportunities, threats) can help you to perform this analysis (see the sidebar “The Cost/Benefit 
Analysis Method”). 
The Cost/Benefit Analysis Method
CBAM is a method that guides the selection of design alternatives using a quantitative 
approach. This method is based on the idea that architectural strategies (i.e., combina-
tions of design concepts) affect quality attribute responses, and that the level of each 
response in turn provides system stakeholders with some benefit called utility. Each 
architectural strategy provides a different level of utility, but also has a cost and takes 
time to implement. The idea behind CBAM is that by studying levels of utility and costs 
of implementation, particular architectural strategies can be selected based on their 
associated return on investment (ROI). CBAM was conceived to be performed after 
ATAM (architecture tradeoff analysis method), but it is possible to use CBAM during 
design—that is, prior to the moment where the architectural evaluation is performed.
CBAM takes as its input a collection of prioritized traditional quality attribute scenar-
ios, which are then analyzed and refined with additional information. The addition is to 
consider several levels of response for each scenario:
■
■
The worst-case scenario, which represents the minimum threshold at which a system 
must perform (utility = 0)
■
■
The best-case scenario, which represents the level after which stakeholders foresee 
no further utility (utility = 100)
■
■
The current scenario, which represents the level at which the system is already  
performing (the utility of the current scenario is estimated by stakeholders)
■
■
The desired scenario, which represents the level of response that the stakeholders 
are hoping to achieve (the utility of the desired scenario is estimated by stakeholders)
Using these data points, we can draw a utility–response curve, like that shown in 
Figure 4.3. After the utility–response curve is mapped for each of the different scenar-
ios, a number of design alternatives may be considered, and their expected response 
values can be estimated. For example, if we are concerned about mean time to failure, 
we might consider three different architectural strategies (i.e., redundancy options)—for 
example, no redundancy, cold spare, and hot spare. For each of these strategies, we 

4.4  Identifying and Selecting Design Concepts  73
could estimate their expected responses (i.e., their expected mean times to failure).  
In the graph shown in Figure 4.3, point “e” represents one such option, which is placed 
on the curve based on its expected response measure.
Response
Utility
w
c
e
d
b
b: best
c: current
e: expected
d: desired
w: worst
1
2
3
0
100
FIGURE 4.3  Utility–response curve
Using these response estimates, the utility values of each architectural strategy can 
now be determined via interpolation, which provides its expected benefit. The costs of 
each architectural strategy are also elicited: We would expect hot spare to be the most 
costly option, followed by cold spare and no redundancy.
Given all of this information, architectural strategies can then be selected based on 
their expected value for cost.
CBAM might seem relatively complex and time-consuming at first. However, some 
design decisions can have enormous economic consequences—in terms of their costs, 
their benefits, and their effects on project schedule. You must decide if you are willing 
to take the chance of making these decisions solely using a gut-feeling approach, or if it 
would be wiser to apply the more rational and systematic approach offered by CBAM.
If the aforementioned analysis techniques do not guide you to make an appropriate selec-
tion, you may need to create throwaway prototypes and collect measurements from them. The 
creation of early throwaway prototypes is a useful technique that can facilitate the selection 
of externally developed components. This type of prototype is usually created in a “quick and 
dirty” fashion without too much consideration of its maintainability or reuse. For these rea-
sons, throwaway prototypes should not be used as a basis for further development.

74  Chapter 4  The Architecture Design Process
Although the creation of prototypes can be costly compared to analysis (the ratio of costs 
is between 10 and 5 to 1, according to our sources), certain scenarios strongly motivate the cre-
ation of prototypes. Aspects that you should consider when deciding whether you will create a 
prototype include the following:
■
■Does the project incorporate emerging technologies?
■
■Is the technology new to the company?
■
■Are there certain drivers, particularly quality attributes, whose satisfaction using 
the selected technology presents risks (i.e., it is not understood whether they can be 
satisfied)?
■
■Is there a lack of trusted information, internal or external, that provides some degree of 
certainty that the selected technology will be useful to satisfy the project drivers?
■
■Are there configuration options associated with the technology that need to be tested or 
understood?
■
■Is it unclear whether the selected technology can be integrated with other technologies 
that are used in the project?
If most of your answers to these questions are “yes”, then you should strongly consider 
creating a throwaway prototype.
When identifying and selecting design concepts, you need to keep in mind the constraints 
that are part of the architectural drivers, because some constraints will restrict you from select-
ing particular alternatives. For example, a constraint might require that all libraries and frame-
works in the system do not use the GPL license; thus, even if you have found a framework that 
could be useful for your needs, you may need to discard it if it has a GPL license. Also, the 
design concepts you selected in previous iterations may restrict the design concepts that you 
can select in the future because of incompatibilities. For example, if you selected a web appli-
cation reference architecture for use in an initial iteration, you cannot select a user interface 
framework intended for local applications in a subsequent iteration.
Finally, you need to recognize that even though ADD provides guidance on how to per-
form the design process, it cannot ensure that you will make appropriate design decisions. 
Thorough reasoning and considering different alternatives (not just the first thing that comes 
to mind) are the best means to improve your odds of finding a good solution. We discuss doing 
“analysis in the design process” in Chapter 11.
4.5  Producing Structures
Design concepts per se won’t help you satisfy your drivers unless you produce structures; that 
is, you need to identify and connect elements that are derived from the selected design con-
cepts. This process constitutes the instantiation of architectural elements in ADD: creating 
elements and relationships between them, and associating responsibilities with these elements. 

4.5  Producing Structures  75
It is important to remember that the architecture of a software system is composed of a set of 
structures, which can be grouped into three major categories:
■
■Module structures: composed of logical and static elements that exist at development 
time, such as files, modules, and classes
■
■Component and connector (C&C) structures: composed of dynamic elements that exist 
at runtime, such as processes and threads
■
■Allocation structures: composed of both software elements (from a module or a C&C 
structure) and non-software elements that may exist both at development time and at 
runtime, such as file systems, infrastructure, and development teams
When you instantiate a design concept, you may actually produce more than one struc-
ture. For example, in a particular iteration you might instantiate the Layers pattern, which 
results in a module structure. As part of instantiating this pattern, you will need to choose the 
number of layers, their relationships, and the specific responsibilities of each layer. As part of 
the iteration, you might also study how a scenario is supported by the elements that you have 
just identified. For example, you could create instances of the logical elements in a C&C struc-
ture and model how they exchange messages (see Section 4.6.2). Finally, you might decide 
who will be responsible for implementing the modules inside each of the layers, which is an 
allocation decision.
4.5.1  Instantiating Elements
The instantiation of architectural elements depends on the type of design concept that you are 
working with:
■
■Reference architectures. In the case of reference architectures, instantiation typically 
means that you perform some sort of customization. As part of this work, you will add 
or remove elements that are part of the structure that is defined by the reference architec-
ture. For example, if you are designing a microservice, you will probably need a service 
reference architecture that includes service, business, and data layers, as we showed in 
Chapter 3.
■
■Architectural and design patterns. These patterns provide a generic structure composed 
of elements, their relationships, and their responsibilities. As this structure is generic, 
you will need to adapt it to your specific problem. Instantiation usually involves trans-
forming the generic structure defined by the pattern into a specific one that is adapted to 
the needs of the problem that you are solving. For example, consider the Pipe and Filters 
architectural pattern. It establishes the basic elements of computation (filters) and their 
relationships (pipes), but does not specify how many filters you should use for your prob-
lem or what their relationships should be. You instantiate this pattern by defining how 
many pipes and filters are needed to solve your problem, by establishing the specific 
responsibilities of each of the filters, and by defining their topology.
■
■Deployment patterns. Similar to the case with architectural and design patterns, the 
instantiation of deployment patterns generally involves the identification and specifica-
tion of infrastructure elements. For example, if you are using a Load-Balanced Cluster 

76  Chapter 4  The Architecture Design Process
pattern, instantiation may involve identifying the number of replicas to be included in 
the cluster, the load-balancing algorithm, and the physical location of the replicas. When 
deploying to the cloud, instantiation of these patterns involves selecting resources offered 
by the chosen cloud provider.
■
■Tactics. This design concept does not prescribe a particular structure, so you will need to 
use other design concepts to instantiate a tactic. For example, you might select a security 
tactic of authenticating actors and instantiate it by creating a custom-coded ad hoc solu-
tion, or by using a security pattern, or by using an externally developed component such 
as a security framework.
■
■Externally developed components. The instantiation of these components may or may 
not imply the creation of new elements. For example, in the case of object-oriented 
frameworks, instantiation may require you to create specific classes that inherit from 
the base classes defined in the framework. This will result in new elements. Other 
approaches, which do not involve the creation of new elements, might include choosing 
a specific technology from a technology family that was identified in a previous itera-
tion, associating a particular framework to elements that were identified in a previous 
iteration, or specifying configuration options for an element associated with a particular 
technology (such as a number of threads in a thread pool).
4.5.2  Associating Responsibilities and Identifying Properties
When you are creating elements by instantiating design concepts, you need to consider the 
responsibilities that are allocated to these elements. For example, if you instantiate the Layers 
pattern and decide to use the traditional three-layer structure, you might decide that one of 
the layers will be responsible for managing the interactions with the users (typically known as 
the presentation layer). When instantiating elements and allocating responsibilities, you should 
keep in mind the high cohesion/low coupling design principle: Elements should have high 
cohesion (internally), defined by a narrow set of responsibilities, and low coupling (externally), 
defined by a lack of knowledge of the implementation details of other elements.
One additional aspect that you need to consider when instantiating design concepts is 
the properties of the elements. This may involve aspects such as the configuration options, 
statefulness, resource management, priority, or even hardware characteristics (if the elements 
that you created are physical nodes) of the chosen technologies. Identifying these properties 
supports analysis and the documentation of the design rationale.
4.5.3  Establishing Relationships Between the Elements
The creation of structures also requires making decisions with respect to the relationships that 
exist between the elements and their properties. Once again, consider the Layers pattern. You 
may decide that two layers are connected, but these layers will eventually be allocated to com-
ponents that are, in turn, allocated to hardware. In such a case, you need to decide how com-
munication will take place between these layers, as they have been allocated to components: 
Is the communication synchronous or asynchronous? Does it involve some type of network 

4.6  Defining Interfaces  77
communication? Which type of protocol is used? How much information is transferred and 
at what rate? These design decisions can have a significant impact with respect to achieving 
certain quality attributes such as performance.
4.6  Defining Interfaces
Interfaces are the externally visible properties of elements that establish a contractual specifi-
cation that allows elements to collaborate and exchange information. There are two categories 
of interfaces: external and internal.
4.6.1  External Interfaces
External interfaces include application programming interfaces (APIs) from other systems that 
are required by the system that you are developing and interfaces that are provided by your 
system to other systems. Required APIs are part of the constraints for your system, as you usu-
ally cannot influence their specification. Chapter 5 provides an extensive discussion of APIs, 
including mechanisms for defining the APIs that are necessary when your system is responsi-
ble for providing them.
Establishing a system context at the beginning of the design process is useful to identify 
external interfaces. This context can be represented using a system context diagram, as shown in 
Figure 4.4. Given that external entities and the system under development interact via interfaces, 
there should be at least one external interface per external system (each relationship in the figure).
Channel Management
System
Other Systems...
User Identity Service
Commercial Analysis
System
Property Management
System
Prices
Prices
Prices
Prices
User
Credentials
System under development
External system
Data ﬂow
Price Changes
End User
Hotel Pricing
System
FIGURE 4.4  A system context diagram

78  Chapter 4  The Architecture Design Process
4.6.2  Internal Interfaces
Internal interfaces are interfaces between the elements that result from the instantiation of 
design concepts. To identify the relationships and the interface details, you generally need 
to understand how the elements exchange information at runtime. You can achieve this with 
the help of modeling tools such as UML sequence diagrams (Figure 4.5), which allow you to 
model the information that is exchanged between elements during execution to support use 
cases or quality attribute scenarios. This type of analysis is also useful for identifying rela-
tionships between elements: If two elements need to exchange information directly, then a 
relationship between these elements must exist. The information that is exchanged becomes 
part of the specification of the interface.  
Figure 4.5 is an initial sequence diagram for HPS-2 (Change Prices) on the com-
mand side. The diagram shows how the client application invokes the POST method 
that triggers a change in prices. The PriceService retrieves the hotel and price that 
must be changed and asks the hotel object to calculate additional prices. To increase 
performance, an index must be added to the database (increase efficiency of 
resource usage tactic). Once prices are calculated, an event is created and sent to the 
PriceChangeEventProcessor, which in turn sends it to Kafka.
Key: UML
From this interaction, initial methods for the interfaces of the interacting elements can 
be identified:
PriceServices:
Method name
Description
updatePriceForPeriod
Update prices for a hotel and produce a price change event.
Parameters:
■
■
Hotel identifier
■
■
Range of dates of price changes
■
■
Amount
Returns:
■
■
True if the update is performed successfully.
Throws:
■
■
PriceChangeEventException in case price change fails.
Note: More details of this example are presented in Chapter 8.

4.6  Defining Interfaces  79
:HotelController
:PricesService
validateBusinessRules()
@Transactional
:HotelRepository
:PricesRepository
updatePricesForPeriod(hotelCode, date, amount)
true
ﬁndById(hotelCode)
hotel
ﬁndByHotelAndDate(hotel, date)
price
hotel:Hotel
calculateAdditionalPrices(price)
price:Price
setAmount(amount)
true
prices
evt:
PriceChangeEvent
new(prices)
evt
:PriceChange
EventProcessor
Event sent to
Kafka
publishEvent(evt)
true
POST /v1/hotels/{id}/prices
204 NO CONTENT
FIGURE 4.5  A sequence diagram used to identify interfaces

80  Chapter 4  The Architecture Design Process
Interfaces typically consist of a set of operations (such as methods) with specified param-
eters, return values, and, possibly, exceptions and pre and post conditions. Some interfaces, 
however, may involve other information exchange mechanisms, such as a component that 
writes information to a file or database and another component that then accesses this infor-
mation. Interfaces may also establish quality of service agreements. For example, the execu-
tion of an operation specified in the interface may be time constrained to satisfy a performance 
quality attribute scenario.
Identification of interfaces is usually not carried out to the same extent across all design 
iterations. When you are starting the design of a greenfield system, for example, your first 
iterations will produce only abstract elements such as layers, with these elements then being 
refined in later iterations. The interfaces of abstract elements such as layers are typically under-
specified. For example, in an early iteration you might simply specify that the UI layer sends 
“commands” to the business logic layer, with the business logic layer then sending “results” 
back. As the design process advances, and particularly when you create structures to address 
specific use cases and quality attribute scenarios, you will need to refine the interfaces of the 
specific elements that participate in this interaction.
In some special cases, identification of interfaces is greatly simplified. For example, 
when you use a persistence framework that provides modules in the data layer, interfaces are 
already defined by the technology. The specification of interfaces then becomes a relatively 
trivial task, as the chosen technologies are designed to interoperate and hence have already 
“baked in” many interface assumptions and decisions.
Finally, you need to consider that not all of the internal interfaces of the system element 
will be identified as part of the design process (see the sidebar “Identifying Interfaces in 
Element Interaction Design”). 
Identifying Interfaces in Element Interaction Design
Although defining interfaces is an essential part of the architecture design process, it is 
important to recognize that not all of the internal interfaces are identified during archi-
tectural design. As part of the architecture design process, you typically consider the 
primary use cases as part of the architectural drivers, and you identify elements (usually 
modules) that support this primary functionality along with the other drivers. This pro-
cess will, however, not uncover all of the elements and interfaces for the system that 
are required to support the entire set of use cases. This lack of specificity is intended: 
Architecture is about abstraction, so some information is necessarily less important, 
particularly in the earliest stages of design.
Identifying the modules that support the nonprimary use cases is often necessary for 
estimation or work-assignment purposes. Identifying their interfaces is also necessary 
to support the individual development and integration of the modules and to perform 
unit testing. This identification of modules may be done early in the project life cycle, but 
it must not be confused with a big design up front (BDUF) approach. But in certain con-
texts, such as early estimation or iteration planning, this kind of work is hard to avoid.
As an architect, you may identify the set of modules that support the complete set of 
use cases for the system or for a particular release of the system. However, the identi-
fication of the interfaces associated with the modules that support the nonprimary use 
cases is typically not your responsibility, as it would require a significant amount of your 

4.7  Creating Preliminary Documentation During Design  81
time and rarely has a major architectural impact. This task, which we call element  
interaction design (see Section 2.2.2), is usually performed after architectural design 
ends but before the development of (most of) the modules begins. Although this task 
should be performed by other members in the development team, as an architect you 
will play a critical role in shaping it, since these interfaces must adhere to the architec-
tural design that you established. You, as the architect, must communicate the architec-
ture to the engineers who are responsible for identifying the interfaces and ensure that 
they understand the rationale for the existing design decisions.
A good way to achieve this communication is to use the active reviews for interme-
diate design (ARID) method. In this method, the architecture design (or part of it) is 
presented to a group of reviewers—in this case, the engineers who will make use of this 
design. After the design presentation, the participants in the review select a set of sce-
narios. These scenarios are used for the core of the exercise, where the reviewers use 
the elements present in the architecture to satisfy them. In standard ARID, the review-
ers are asked to write code or pseudo-code for the purpose of identifying interfaces. 
Alternatively, the architect can present the architecture, select a nonprimary functional 
scenario, and ask the participants to identify the interfaces of the components that  
support the scenario using sequence diagrams or a similar method.
Aside from the fact that the architectural design is reviewed in this exercise, the ARID 
approach offers additional benefits. Specifically, in a single meeting, the architecture 
design (or part of it) is presented to the entire team, and agreements can be reached 
with respect to how the interfaces should be defined (e.g., the level of detail or aspects 
such as parameter passing, data types, or exception management).
4.7  Creating Preliminary Documentation During Design
A software architecture is typically documented as a set of views, which represent the different 
structures that compose the architecture. The formal documentation of these views is not part 
of the design process. Structures, however, are produced as part of design. Capturing them, 
even in an informal manner (i.e., as sketches), along with the design decisions that led you to 
create these structures, is a task that should be performed as part of normal design activities.
4.7.1  Recording Sketches of the Views
When you produce structures by instantiating the design concepts that you have selected to 
address a particular design problem, you typically will not produce these structures in your 
mind, but rather will create some sketches of them. In the simplest case, you will produce 
these sketches on a whiteboard, a flip-chart, or even a piece of paper. Alternatively, you might 
use a modeling tool in which you will draw the structures. The sketches that you produce are 
the initial documentation for your architecture that you should capture and can flesh out later, 
if necessary. When you create sketches, you don’t need to always use a more formal language 
such as UML. If you use some informal notation, you should at least be careful in maintaining 
consistency in the use of symbols. Eventually, you will need to add a legend to your diagrams 
to provide clarity and avoid ambiguity.

82  Chapter 4  The Architecture Design Process
You should develop discipline in writing down the responsibilities that you allocate to 
the elements as you create the structures. The reasons for this are simple: As you identify an 
element, you are determining some responsibilities for that element in your mind. Writing 
it down at that moment ensures that you won’t have to remember it later. Also, it is easier to 
write down the responsibilities associated with your elements gradually, instead of compiling 
all of them at a later time.
Creating this preliminary documentation as you design requires some discipline. But the 
benefits are worth the effort—you will be able to produce the more detailed architecture docu-
mentation relatively easily and quickly at a later point. One simple way that you can document 
responsibilities if you are using a whiteboard, a flip-chart, or a PowerPoint slide is to take a 
photo of the sketch that you have produced and paste it in a document, along with a table that 
summarizes the responsibilities of every element depicted in the diagram (Figure 4.6 provides an 
example). If you are using a computer-aided software engineering (CASE) tool, you can select an 
element to create and use the text area that usually appears in the properties sheet of the element 
to document its responsibilities, and then generate the documentation automatically. 
Figure 4.6 is a sketch of a module view depicting the overall system structure for a 
CQRS-based system. The diagram is complemented with a table that describes the 
responsibilities of the various elements.
Frontend
Other App
Command
Microservice
Query
Microservice
Kafka
PostgreSQL
MongoDB
API Gateway
FIGURE 4.6  Sample preliminary documentation

4.7  Creating Preliminary Documentation During Design  83
Key: UML
Element
Responsibility
Command microservice
This microservice is responsible for making changes to the domain entities 
and sending an event when changes are made.
Query microservice
This microservice receives events sent by the command microservice and 
persists the information in the database to support querying.
. . .
. . .
Of course, it is not necessary to document everything. The three purposes of documen-
tation are analysis, construction, and education. At the moment you are designing, you should 
choose a documentation purpose and then document to fulfill that purpose, based on your risk 
mitigation concerns. For example, if you have a critical quality attribute scenario that your 
architecture design needs to satisfy, and if you will need to prove this requirement is met in an 
analysis, then you must take care to document the information that is relevant for the analysis 
to be satisfactory. Alternatively, if you anticipate having to train new team members, then you 
should make a sketch of a C&C view of the system, showing how it operates and how the ele-
ments interact at runtime, and perhaps construct a crude module view of the system, showing 
at least the major layers or subsystems.
Finally, it is a good idea to remember, as you are documenting, that your design may 
eventually be analyzed. Consequently, you need to think about which information should be 
documented to support this analysis (see the sidebar “Scenario-Based Documentation”). 
Scenario-Based Documentation
The analysis of an architecture design is based on your most important use cases and 
quality attribute scenarios. Simply put, a scenario is selected and you must explain to 
reviewers how the architecture supports the scenario, and justify your decisions.  
To start preparing for the analysis while you design, it is useful to produce and doc-
ument structures that contain the elements that are involved in the satisfaction of a 
scenario. These artifacts should come naturally given that the design process is  
guided by scenarios, but keeping this point firmly in mind is always helpful.
During the design process, you should at least try to capture the following elements 
in a single document:
■
■
The primary presentation: the diagram that represents the structure that you produced
■
■
The element responsibilities table: to help you record the responsibilities of the  
elements that are present in the structure
■
■
The relevant design decisions, and their rationales (see Section 4.7.2)
You might also capture two other pieces of information:
■
■
A runtime representation of the element’s interaction—for example, a sequence diagram
■
■
The initial interface specifications (which can also be captured in a separate 
document)

84  Chapter 4  The Architecture Design Process
As you can see, all of this information needs to be produced as part of the design 
process. One way or another, you need to decide which elements are present in the 
system and how they interact. The only question is whether you bother to write this 
information down, or whether its sole representation is in the code.
If you follow the approach that we advocate here, at the end of the design you will 
have a set of preliminary views documented, with each of the views being associated 
with a particular scenario, and you will have constructed this documentation at little 
cost. This preliminary documentation can be used “as is” to analyze the design, and 
particularly through scenario-based evaluations.
4.7.2  Recording Design Decisions
In each design iteration, you make important design decisions to achieve your iteration goal. 
As we saw previously, these design decisions include the following:
■
■Selecting a design concept from several alternatives
■
■Creating structures by instantiating the selected design concept
■
■Establishing relationships between elements and defining interfaces
■
■Allocating resources (e.g., people, hardware, computation)
■
■Others
When you study a diagram that represents an architecture, you are seeing the end prod-
uct of a thought process, but it may not be easy to understand the decisions that were made 
to achieve this result. Recording design decisions beyond the representation of the chosen 
elements, relationships, and properties is fundamental to help clarify how you arrived at the 
result: the design rationale.
When your iteration goal involves satisfying a specific quality attribute scenario, some 
of the decisions that you make will significantly influence your ability to achieve the scenario 
response measure. These are, therefore, the decisions that you should take the greatest care in 
recording. You should record these decisions because they are essential to facilitate analysis of 
the design you created; then to facilitate implementation; and, still later, to aid in understand-
ing the architecture (e.g., during maintenance). Also every design decision is “good enough” 
but seldom optimal, so you need to justify the decisions made, and possibly revisit the remain-
ing risks later.
You might think that recording design decisions is a tedious task. In reality, depending on 
the criticality of the system being developed, you can adjust the amount of information that is 
recorded. For example, to record a minimum of information, you can use a simple table such 
as that shown in Table 4.2. If you decide to record more than this minimum, the following 
information can prove useful:
■
■What evidence was produced to justify decisions?
■
■Who did what?
■
■Why were shortcuts taken?
■
■Why were tradeoffs made?
■
■What assumptions did you make?

4.8  Tracking Design Progress  85
TABLE 4.2  Example of a Table to Document Design Decisions
Design Decisions and Location
Rationale and Assumptions
Use a relational database for the 
Command microservice
Given that the Command microservice manages the 
different entities of the domain model (Hotels, Rates, 
Room Types, etc.), and given that these entities are 
related to each other, the most natural and appropriate 
mechanism for storing them is a relational database.
Discarded alternatives are nonrelational databases or 
other types of storage.
Use a nonrelational database for the 
Query microservice
Because the database on the query side does not need 
to handle transactions and relationships between entities, 
and because the prices that are stored have a variable 
structure depending on the hotel, it was decided that 
the use of a nonrelational database is more appropriate. 
PriceChangeEvents can be appropriately stored in a 
document database.
Discarded alternatives include other types of NoSQL, 
relational databases or other types of storage.
Much in the same way that you record responsibilities as you identify elements, you 
should record the design decisions as you make them. The reason for this is simple: If you 
leave it until later, you may not remember why you did things.
4.8  Tracking Design Progress
Although ADD provides clear guidelines to help you perform design systematically, it does 
not provide a mechanism to help you track design progress. When you are performing design, 
however, you will want to answer several questions:
■
■How much design do we need to do?
■
■How much design has been done so far?
■
■Are we finished?
Agile practices such as the use of backlogs and Kanban boards can help you track the 
design progress and answer these questions. These techniques are not limited to Agile meth-
ods, of course. Any development project using any methodology should track progress.
4.8.1  Use of an Architectural Backlog
The concept of an architecture (or design) backlog has been proposed by several authors. This 
is similar to the product backlog that is found in Agile development methods such as Scrum, 
but the focus here is not on feature development. Instead, the basic idea is that you need to cre-
ate a list of the pending actions that still need to be performed as part of the architecture design 
process. These actions are called “enablers” in methods such as the Scaled Agile Framework.

86  Chapter 4  The Architecture Design Process
Initially, you should populate the design backlog with tasks focused on directly satisfying 
drivers. Then, other activities that support the design of the architecture can also be included. 
For example:
■
■Creation of a prototype to test a particular technology or to address a specific quality 
attribute risk
■
■Exploration and understanding of existing assets (possibly requiring reverse engineering)
■
■Issues uncovered in a review of the design
■
■Review of a partial design that was performed on a previous iteration
For example, when using Scrum, the sprint backlog and the design backlog are not inde-
pendent: Some features in the sprint backlog may require architecture design to be performed, 
so they will generate items that go into the architectural design backlog. While these two 
backlogs can be managed separately, it may be more practical to combine the architectural 
tasks with features and other elements in the backlog (see Figure 10.1, “Value and visibility: 
the dimensions of entries in your project backlog”).
Also, additional architectural concerns may arise as decisions are made. For example, if 
you choose a reference architecture, you will probably need to add specific architectural con-
cerns, or quality attribute scenarios derived from them, to the architectural design backlog. An 
example of such a concern is the establishment of a mechanism to support observability.
4.8.2  Use of a Design Kanban Board
As design is performed in rounds and as a series of iterations within these rounds, you need to 
have a way of tracking the design’s progress. You must also decide whether you need to con-
tinue making more design decisions (i.e., performing additional iterations). One tool that can 
be used to facilitate this task is a Kanban board, like the one shown in Figure 4.7.
At the beginning of a design round, the inputs to the design process become entries in the 
backlog. Initially, that activity occurs in step 1 of ADD; the different entries in your backlog for 
this design round should be added to the “Not Yet Addressed” column of the board (unless you 
have some entries that were not concluded in previous design rounds that you wish to address 
here). When you begin a design iteration, in step 2 of ADD, the backlog entries that correspond 
to the drivers that you plan to address as part of the design iteration goal should be moved to 
the “Partially Addressed” column. Finally, once you finish an iteration and the analysis of your 
design decisions reveals that a particular driver has been addressed (step 7 of ADD), the entry 
should be moved to the “Completely Addressed” column of the board. It is important to estab-
lish clear criteria that will allow a driver to be moved to the “Completely Addressed” column 
(think of this as the “Definition of Addressed” criteria, similar to the “Definition of Done” cri-
teria used in Scrum). A criterion may be, for example, that the driver has been analyzed or that 
it has been implemented in a prototype. Also, drivers that are selected for a particular iteration 
may not be completely addressed in that particular iteration, in which case they should remain 
in the “Partially Addressed” column. In preparation for subsequent iterations, you should con-
sider how they can be allocated to the elements that exist at this point.

4.9  Summary  87
Not Yet Addressed 
6
Partially Addressed 
7
Completely Addressed 
1
Discarded
High Priority
High Priority
Medium Priority
Medium Priority
Medium Priority
Low Priority
High Priority
High Priority
High Priority
High Priority
High Priority
QA-8 Test code coverage should
be at least 85% for each CI
QA-3 External user credentials are
veriﬁed against user registry
UC10-
CT-1 MVP release of the solution
to the selected consultants,
customers, and prospective
licensees in 9 months, release
in 1.5 years
QA-5 Data center infrastructure
has uptime 99.95%
QA-4 User facing parts are
available 99.9% - 4 hours in
months (maintenance window)
QA-1 User credentials are veriﬁed
against corporate AD
UC4 - As sales person, prepare
proposal plan
CT-8 Infrastructure team is not
able to support large-scale
SaaS setup
CN-2 Choose architecture style
QAScenario
QAScenario
QAScenario
QAScenario
QAScenario
Constraint
Constraint
UseCase
Concern
CN-1 Codebase (reuse legacy
code if possible)
FIGURE 4.7  An example of a Kanban board used to track design progress
It can be useful to select a technique that will allow you to differentiate the entries in the 
Kanban board according to their priority. For example, you might use different colors of Post-it 
notes depending on the priority.
With a Kanban board, it is easy to visually track the advancement of design, as you can 
quickly see how many of the (most important) drivers are being or have been addressed in the 
design round. This technique also helps you decide whether you need to perform additional 
iterations as, ideally, the design round is terminated when a majority of your drivers (or at least 
the ones with the highest priority) are located under the “Completely Addressed” column.
4.9  Summary
In this chapter, we presented a detailed walk-through of the Attribute-Driven Design method, 
version 3.0. We also discussed several important aspects that need to be considered in the 

88  Chapter 4  The Architecture Design Process
various steps of the design process. These aspects include the use of a backlog, the various 
ways that ADD can be applied to different system contexts, the identification and selection 
of design concepts and their use in producing structures, the definition of interfaces, and the 
production of preliminary documentation.
Even though the overall architecture development life cycle includes documenting and 
analyzing architecture as activities that are separate from design, we have argued that a clean 
separation of these activities is artificial and harmful. We urge you to regularly perform at 
least some lightweight documentation and analysis activities and to consider them as integral 
parts of the design process.
4.10  Further Reading
Some of the concepts of ADD 3.0 were first introduced in H. Cervantes, P. Velasco, and  
R. Kazman, “A Principled Way of Using Frameworks in Architectural Design”, IEEE 
Software, 46–53, March/April 2013.
Version 2.0 of ADD was first documented in the SEI Technical Report: R. Wojcik,  
F. Bachmann, L. Bass, P. Clements, P. Merson, R. Nord, and B. Wood, Attribute-Driven 
Design (ADD), Version 2.0, SEI/CMU Technical Report CMU/SEI-2006-TR-023, 2006.  
An extended example of using ADD 2.0 was documented in W. Wood, A Practical Example 
of Applying Attribute-Driven Design (ADD), Version 2.0, SEI/CMU Technical Report: CMU/
SEI-2007-TR-005.
The concept of an architecture backlog is discussed in C. Hofmeister, P. Kruchten, 
R. Nord, H. Obbink, A. Ran, and P. America, “A General Model of Software Architecture 
Design Derived from Five Industrial Approaches”, Journal of Systems and Software, 80, 
106–126, 2007.
The ARID method is discussed in P. Clements, R. Kazman, and M. Klein, Evaluating 
Software Architectures: Methods and Case Studies, Addison-Wesley, 2002.
The CBAM method is presented in L. Bass, P. Clements, and R. Kazman, Software 
Architecture in Practice, 3rd ed., Addison-Wesley, 2013.
The ways in which an architecture can be documented are covered extensively in  
P. Clements et al. Documenting Software Architectures: Views and Beyond, 2nd ed.,  
Addison-Wesley, 2011. More Agile approaches to documenting are discussed in books  
such as S. Brown, Software Architecture for Developers, Lean Publishing, 2015.
The importance and challenges of capturing design rationale are discussed in A. Tang, 
M. Ali Babar, I. Gorton, and J. Han, “A Survey of Architecture Design Rationale”, Journal 
of Systems and Software, 79(12), 1792–1804, 2007. A minimalistic technique for capturing 
rationale is discussed in U. Zdun, R. Capilla, H. Tran, and O. Zimmermann, “Sustainable 
Architectural Design Decisions”, IEEE Software, 30(6), 46–53, 2013.

4.11  Discussion Questions  89
4.11  Discussion Questions
1.	
In step 2 of ADD, what can happen if the goal that is selected for the iteration involves a 
scope that is too big?
2.	
In the design of greenfield systems for mature domains (Section 4.3.1), the recommen-
dation is to first identify structures that support primary functionality before addressing 
other drivers, including quality attributes. Why is this recommendation made? Could you 
design to support quality attributes before considering how primary functionality will be 
supported?
3.	
What are the consequences of the architect deciding that they want to perform ADD 
iterations for all of the system’s use cases?
4.	
What are the risks of not performing step 6 of ADD and leaving all documentation activ-
ities for later?
5.	
What are the benefits and downsides of using an architectural backlog and its associated 
Kanban board versus just adding architectural tasks to the standard product backlog 
used in Scrum?

This page intentionally left blank 

91
5
API-centric Design
The concept of agility in software development started to become popular with the publication 
of the Manifesto for Agile Software Development in 2001. Today, a large number of com-
panies have adopted agile practices and techniques, particularly inside software development 
teams. Currently many companies are going through an “agile transformation”, which involves 
supporting agile practices not only at the software development team level, but across the 
whole organization. In this chapter, we discuss concepts of agility at business scale and how 
API-centric design is a key enabler to achieve them. 
Why Read This Chapter?
Having read this chapter you should not expect to become an expert on any given API 
technology. Instead, we hope you will understand the motivations and principles behind 
service-based and API-centric design, how they can be related to ADD, and how this 
can drive business agility.
5.1  Business Agility
The Agile Alliance defines business agility as follows:
Business agility is the ability of an organization to sense changes internally or  
externally and respond accordingly in order to deliver value to its customers.
While agility at the software team level means that the development team can quickly 
adjust to changes in the backlog and quickly deliver value, agility at the business scale means 
that the whole company is capable of adjusting quickly to changes in the market and to new 
opportunities. One does not imply the other because applying agile practices only at the team 
level is not sufficient to achieve agility at the business level, as organizations that still practice 
a water-Scrum-fall approach can attest. Achieving business agility requires that agile practices 
are used not only by software development teams, but also used in other areas in the company, 
including leadership and strategy, operations, finance, marketing, and others.

92  Chapter 5  API-centric Design
5.1.1  Moving from Projects to Products
Associated with these changes at the organization level, there has been a shift in the way of 
thinking about development. Instead of thinking in terms of projects, many agile companies 
think in terms of products. The focus is on bringing value to the customer rather than on the 
execution of a project. This approach also changes the perspective of teams, which used to be 
formed to work on specific projects, and then disbanded at the end of the project. When the 
organization moves to a product focus, its teams become responsible for specific products in 
the long term and across their life cycle, from inception to removal. This requires teams to be 
cross-functional to support all of the activities necessary for the development and operation 
of the product. In this approach, products have an associated backlog, which is continuously 
updated. The teams that are responsible for these products work on putting the items in the 
backlog into production in the quickest way possible. Agile and deployability practices are 
fundamental to support this paradigm shift.
5.1.2  Drivers for Business Agility
Larger organizations typically manage a set of products that share information about the  
common domain entities of the company. Consider, for example, a ride-sharing company 
that provides not only mobility services, but also food delivery and shipping services. This 
company manages multiple mobile applications for different users, including riders, drivers, 
food service providers, end customers, and multiple websites for these same types of users. In 
addition, websites might be added for other types of users, such as managers, as the business 
evolves. A customer that creates an account with this company is able to use the same account 
(including their payment details) in the different products oriented to the customer. The differ-
ent products share common forms of information.
At some point, this company might identify a new market opportunity that requires a 
completely new product to address it. The company might want to put this new product into 
production quickly, even if it is only in the shape of a Minimum Viable Product (MVP)—that 
is, a version of the product with just enough features to be usable. This new product must 
access existing information, such as customer details. It might also reuse business processes 
from other products, such as calculating a route for delivery or adding a new payment method. 
This example illustrates the concepts of business agility and product orientation. The new 
product should be created and put into production in a short time frame. Once in production, it 
will continuously evolve to rapidly incorporate changes in response to market forces.
Scenarios such as this one are common in large companies nowadays. These require-
ments mean that certain key quality attributes must be considered, such as composability and 
deployability.
Composability
According to Wikipedia, composability “is a system design principle that deals with the 
inter-relationships of components. A highly composable system provides components that 

5.2  API-centric Design  93
can be selected and assembled in various combinations to satisfy specific user requirements”. 
Composability and reusability are closely related. Reusability is a measure of the capability of 
an artifact to be used in multiple contexts. Composability requires the existence of reusable 
assets and attention to integrability.
In the ride-sharing example given previously, the new product might be composed of sev-
eral services that are reused. For instance, the services that provide access to user information 
or the services that manage business processes might be part of this new product.
Being able to reuse and reassemble these assets from the company is key to supporting 
the rapid development of new products. Consider, for instance, a service that calculates routes 
for delivery, which is part of the core business of the company and which has been in devel-
opment for a significant amount of time. The ability to reuse and incorporate existing services 
such as this one into a new product can reduce costs (measured in terms of both time and 
effort) and give business a competitive advantage.
Composability is such an important quality attribute that recently it has been used to 
qualify organizations as being “composable businesses”. However, composability involves 
certain tradeoffs. For instance, composable services are more complex to design and become 
harder to change once they are being used by many clients.
Deployability
While being able to quickly assemble a new product is necessary to support business agility, a 
second fundamental quality attribute is deployability. In this context, deployability constitutes 
the capability to continuously deliver initial or updated versions of the system typically in the 
shortest possible time. This quality attribute is associated not just with specific products, but 
also with the reusable services that are part of those products.
We will discuss deployability in detail in Chapter 6—what it is, how to achieve it, and 
how to design for it. Composability and deployability are the cornerstones of any organiza-
tion’s design approach for achieving business agility.
Other Quality Attributes
API-centric design, as you will see, is a key enabler to achieving many other quality attri-
butes, such as high availability. For example, location abstraction, scalability, and platform and  
technology independence are all benefits of a well-designed architecture based on services 
communicating via APIs.
5.2  API-centric Design
The acronym API—“application programming interface”—has existed for a long time. 
Typically, it has been used to refer to an interface, or contract, that is used by two or more 
programs to interact with each other and that hides implementation details. Historically, APIs 

94  Chapter 5  API-centric Design
have often been designed for specific situations where two systems must be integrated without 
much intention for future reuse by other clients, and without regard to evolution. However, in 
the context of business agility, we are interested in APIs that are designed to be reused and 
consumed by different clients over a network connection (web APIs). When APIs are designed 
in this way, they can serve as the building blocks that support composability. In fact, many 
thought leaders now argue that APIs are the cornerstone of modern, distributed, service- 
oriented architectures. If you accept this premise, then, as an architect, you should be thinking 
about API-centric design, which includes many of the considerations discussed in the rest of 
this chapter.
5.2.1  APIs and Composability
In the context of business agility, appropriately abstract APIs are the building blocks that enable 
composability. One difference between these APIs and the “traditional” integration-oriented 
APIs is that the former are less technical in nature and more focused on business aspects; for 
that reason, they may be more accurately called “business APIs”. Business APIs are frequently 
oriented around services and entities and represent business assets from the problem domain—
for example, customers, trips, or payments in the context of the ride-sharing company. APIs can 
also represent business processes such as the act of performing a trip. Ideally, they will follow 
the principles of high cohesion and low coupling. A highly cohesive API provides methods that 
belong together and that are focused on a specific service or resource. An API with low coupling 
does not expose implementation details, and changes in the way it is implemented do not impact 
the consumers of the API.
An API is a contract that describes a set of operations associated with the business asset. 
Examples of operations include creating, retrieving, and updating instances of the entities; 
triggering the execution of a particular business process; and performing a calculation such as 
a currency conversion. For example, many products in the ride-sharing company require infor-
mation about a customer, and they can obtain this information by accessing a customer API. 
APIs can be private or public. Private APIs are typically designed for internal consumption in 
the company, whereas public APIs are designed for external consumption.
An important aspect of APIs is that they should support self-service. In essence, devel-
opers should be capable of finding an API and making use of it by reading its documentation. 
Good documentation of an API is thus essential to facilitate composability. Discoverability 
and usability are also essential aspects, as we will discuss in Section 5.2.4.2.
A composable business infrastructure requires a collection of APIs to support the oper-
ation of its different products and potentially to provide access to its information to business 
partners or other external consumers. This collection of APIs is frequently referred to as an 
API platform, which constitutes a back-end for the different products in the organization. 
Figure 5.1 shows an API platform and a set of internally developed applications built on 
top of it. The figure also shows an external application that is consuming APIs from the 
platform.

5.2  API-centric Design  95
API
API
API
API
API
API
API
API
API
API
API
API
API
API
API
API
API
Internal
Application
1
Internal
Application
2
Internal
Application
n
External
Application
API Platform
External APIs
FIGURE 5.1  An API platform and a set of applications built on top of it
5.2.2  API-First Design
When an organization adopts a composable API platform, APIs become essential assets for 
the company and foundations for architectural planning and reasoning. API-first design refers 
to the idea that APIs are designed from the start as reusable assets. The design of the APIs 
must be planned carefully to support composability. Once an API is published, making break-
ing changes to it can be complicated, especially if the API is consumed by a large number of 
clients. In this section, we discuss important aspects that need to be considered when design-
ing and specifying APIs to support API-first design.
5.2.2.1  Architectural Concerns for APIs
Besides ensuring that an API delivers its intended functionality, several other concerns need to 
be considered in the design of APIs.
Establishing Intended Purpose and Adequate Granularity
Designing an API requires considering the needs of the consumers of this API. For example, 
APIs that are expected to be widely used by different products must be designed for reus-
ability. In contrast, APIs that are specifically designed to support a single front-end, as in the 
Back-End for Front-End pattern, must support usability of the client but do not really need to 
be reusable.

96  Chapter 5  API-centric Design
One particular challenge with API design involves determining the right level of abstrac-
tion for the API. APIs that are too fine grained or too coarse grained may have limited reus-
ability, or may result in excessive requests, or may result in requests that return too much 
information. Web APIs, for example, are often organized around domains and resources, 
which are representations of concepts and which can be identified using approaches such as 
domain-driven design.
Following Established Design Principles According to the Type of API
Different styles of APIs (see Section 5.2.3.1) obey different principles in their design. For 
example, for an API to be considered RESTful, several considerations must be satisfied. 
Richardson’s Maturity Model classifies web APIs into four levels (from 0 to 3) according to 
their degree of adherence to the principles of REST. Only APIs that are at level 3 may be con-
sidered to be RESTful. Furthermore, APIs may need to conform to both industry standards 
and company standards for naming and design conventions.
Designing the endpoints and the resource representations is also an important concern 
that needs to be addressed. This is accomplished by carefully considering aspects such as 
naming, use of data types, relationships, and parameters.
Supporting API Evolution
Changes to an API are almost inevitable, and can be categorized as breaking changes or 
nonbreaking changes. Breaking changes typically involve a modification to the contract that 
requires clients to be updated to continue using the API. In contrast, nonbreaking changes do 
not require updates to the clients. Nonbreaking changes usually involve adding characteristics 
to an API in such a way that clients that are not aware of these new characteristics can con-
tinue functioning normally without them, and clients that are aware of the new characteristics 
can leverage them. Breaking changes can be especially problematic for public APIs that a 
large number of consumers depend on. Prior to making changes to an API, an impact analysis 
should be done to determine what consumers need to change.
API versioning is necessary to manage changes to the contract. It should be noted that 
the version of the contract is different from the version of its implementation mechanism (see 
Section 5.2.4). Different strategies for API versioning may be selected. For REST APIs, a 
popular approach involves including the API version as part of the resources path (e.g., /v1/
products). Versioning also affects the resource representations that are transferred by the API 
endpoints.
Managing Errors
Error management is an important consideration when designing APIs. It is necessary to have 
a consistent way of returning errors across all APIs. Furthermore, it is necessary to define 
mechanisms for propagating errors when several APIs are involved in an invocation (e.g., 
across API layers). For REST-based APIs, errors may include specific codes in addition to 
HTTP status codes, human-readable messages, and additional information. Security aspects 
must also be considered when designing error responses, as compromising implementation 
details may be revealed if, for example, a stack trace is included as part of the error response.

5.2  API-centric Design  97
Securing an API
API security is an aspect that needs to be considered early on in the design of an API—whether 
it is a public API or a private API. Authentication and authorization are fundamental concerns 
that need to be addressed when securing an API. Security approaches such as OAuth 2.0 are 
industry standards to support authentication and authorization through the use of tokens and 
scopes. An authorization server generates access tokens, which are then used by API clients 
when invoking the operations of the API. Furthermore, communication between the client and 
the API provider usually occurs over a secure channel. API security is an extensive topic, and 
additional considerations related to authorization and authentication need to be addressed as 
part of this concern—for example, request schema validation, prevention of data exfiltration, 
and certificate management.
Other Concerns
Other general concerns include monitoring API usage and establishing mechanisms to limit 
consumption (e.g., throttling). More specific concerns include managing the size of responses, 
which is typically achieved through pagination mechanisms. Internationalization is another 
important concern, including aspects such as the management of time zones, currencies, and 
languages. Finally, debugging capabilities must be considered; adding mechanisms to support 
distributed tracing can be useful to identify different kinds of problems during execution.
5.2.2.2  Specifying an API
The design of an API does not require code to be written early on, since the API is a con-
tract. Notably, specifications such as OpenAPI define languages that facilitate the modeling of 
REST APIs. Listing 5.1 shows an excerpt from the specification of an API using the OpenAPI 
specification. This example shows how the specification includes general information about 
the API, including versioning, security mechanisms, paths including parameters, return and 
error values, schemas for the objects that are returned and received by the API, and server 
information that allows the API to be used and tested. Note that while the fine details in the 
example might change over time, the intent will probably remain stable, as the information 
described in this specification needs to be communicated somehow in this or other specifica-
tion languages.
LISTING 5.1   Sample API using the OpenAPI specification language
openapi: 3.0.1
info:
  title: Products API
  version: '1.0'
  description: Sample Open API specification.
  contact:
    email: john.doe@gmail.com
    name: John Doe

98  Chapter 5  API-centric Design
security:
  - BearerAuth: []
paths:
  /v1/products:
    get:
      summary: Allows a product to be retrieved using an optional filter
      operationId: getAllProducts
      parameters:
        - name: productType
          in: query
          description: Product type filter
          required: false
          schema:
            type: string
          example: Toys
      responses:
        '200':
          description: The list of products was retrieved successfully
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/productListDto'
        '400':
          description: Invalid filter parameter
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/errorDto'
        '500':
          description: Internal error during query
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/errorDto'
      tags:
        - products
…
components:
  schemas:
    productListDto':
      type: object
      properties:
        products:

5.2  API-centric Design  99
          type: array
          items:
            $ref: '#/components/schemas/productDto'
…
servers:
  - url: https://products-system.mocklab.io/
    variables: {}
    description: Mock server
  - url: http://localhost:8080/
    variables: {}
    description: Dev server
Defining the specification of the API before writing code, as part of the API-first philos-
ophy, offers several benefits. This should come as no surprise to anyone who believes in the 
value of architecture. The first benefit is that the use of API specification languages allows 
specialized tools to validate the contract and to generate API documentation on the fly, which 
is essential to support self-service. For example, an API specification using the OpenAPI spec-
ification language can be rendered into an interactive web page. This web page allows potential 
users to quickly understand the characteristics of the API, such as the paths, parameters, and 
error values. Potential users may include API consumers, producers, and testers of the API, 
since the specification should contain enough information to generate test cases for the API. 
An additional benefit is that once interfaces are defined, they support parallel development; 
that is, they allow the developers of the implementations and clients of these interfaces to work 
independently. Client, server, and mock implementations can be easily created from the API 
specification, which can greatly accelerate both the development and testing processes. API 
specifications must include sufficient information to support interaction with other services.
Information about the API
The documentation of an API must include general information about the API, which should 
help users to quickly identify whether the particular API might be useful to them. An exam-
ple of the information that can be included, as defined by the OpenAPI specification, is the 
following:
■
■Title: The title of the API
■
■Description: A short description of the API
■
■Terms of Service: A URL to the terms of service for the API
■
■Contact: Contact information for the API
■
■License: License information about the API
■
■Version: Version of the document
Methods or Operations
An essential component of an API is the list of methods, or operations, that can be invoked 
on the API. Methods also include parameters and return values, and the data that is passed 

100  Chapter 5  API-centric Design
or returned has a well-defined format. Methods typically must be described through the  
following elements:
■
■Name of the method
■
■Description of the method: A textual description of the purpose of the method;  
a summary may also be present
■
■Parameters: Data that is passed when the method is invoked (discussed later in this 
section)
■
■Return values: Data that is returned when the method is invoked (discussed later in this 
section)
Parameters
Parameters are the data that is passed to the method when it is invoked. Depending on the 
type of API, this data can be passed in different ways. For example, in the case of web APIs, 
parameters can be passed as part of the path (e.g., /products/{id}), as part of a query (e.g.,  
/products/?id=1234), or as part of the header (e.g., Content-Type: application/json). Parameters 
can also be passed as part of the message body. Parameters can have additional properties, 
such as the following:
■
■Name
■
■Description (may include examples)
■
■Type
■
■Whether it is required or optional
■
■Allowed and default values
■
■Content type (for parameters passed as message body)
Return Values
Return values are the data that is returned as a result of the invocation of the method. Similar 
to parameters, depending on the type of API, data can be returned in different ways. In the 
case of web APIs, responses typically include an HTTP response code and, optionally, a pay-
load. Responses can have additional properties, including these:
■
■Description
■
■Type
■
■A content type (in case a payload is returned)
Other Characteristics
An API may define additional characteristics, including aspects such as the following:
■
■Security schemes
■
■Location of servers where the API resides

5.2  API-centric Design  101
5.2.3  Design Concepts for API-centric Design
As mentioned previously, designing an API is a task that must be performed with care because 
the cost of changing published APIs, especially those that are widely consumed, can be high. 
In this section, we present different design concepts associated with APIs.
5.2.3.1  Styles of APIs
When working with APIs, an important design decision is the selection of the type of API. 
Here we discuss some common API styles.
REST-Oriented APIs
REST—that is, “Representational State Transfer”—is an architectural style for distributed 
hypermedia systems. REST includes a set of architectural constraints and features:
■
■Client-server: This constraint enforces the principle of separation of concerns and allows 
these two components to evolve independently.
■
■Statelessness: Communication between the client and the server must be stateless in 
nature. Session state should not be stored internally. Either the client sends all of the 
session state in each request, or the server stores the session state externally.
■
■Cacheable: Data within a response to a request is implicitly or explicitly labeled as 
cacheable or non-cacheable so that clients can behave accordingly.
■
■Layered system: There can be different layers between the client and the server to 
address different concerns (e.g., load balancing); however, the client is unaware of this 
layering.
■
■Uniform interface: Components must have a uniform interface. This requires four  
interface constraints:
■
■Identification of resources: Resources are individually identifiable.
■
■Manipulation of resources through representations: A particular resource can be  
represented in different ways (e.g., JSON or XML).
■
■Self-descriptive messages: Messages returned to the client provide sufficient  
information for the client on how to process the messages.
■
■Hypermedia as the engine of application state: A resource representation contains 
hyperlinks that allow the discovery of other actions that can be taken on the resource.
■
■Code-on-demand: With this optional feature, client functionality can be extended by 
downloading and executing code in the form of applets or scripts.
Regardless of whether they are RESTful or not, today REST-based APIs are the de facto 
standard for web APIs.
RPC-Oriented APIs
Remote Procedure Call (RPC) APIs allow a client to invoke a procedure remotely in the same 
manner as if the procedure was being invoked locally. An RPC API is typically described 
using an interface description language (IDL). This description is used to generate a stub and 

102  Chapter 5  API-centric Design
a skeleton that facilitate communication between the client and the server sides. The client 
communicates with the stub as if it were a local component, and the stub is responsible for for-
warding the call to the skeleton on the server side. The skeleton then invokes the real code that 
needs to be executed and returns any response to the client side. The response data is received 
by the stub, which returns it to the client. Many standards for RPC have been proposed; one 
that is popular today is gRPC, which was created by Google and which also supports stream-
ing data.
The choice of RPC APIs is typically driven by performance needs, as use of such an API 
involves tighter coupling between client and server. This approach is more commonly used for 
APIs that are internal to an organization, rather than for APIs that are exposed for consump-
tion to the outside world. RPCs can also be useful for prototyping or rapid development, but 
the downside is that this protocol may be less familiar to developers and less human-readable, 
making it more difficult to debug and maintain.
SOAP and WSDL appeared as a way to implement features made available by older RPC 
technologies on top of HTTP. SOAP and WSDL are, however, complex to use. Although they 
are still in use today, they have slowly been replaced by REST-oriented APIs.
Query-Oriented APIs
Query-oriented APIs seek to solve some problems that occur with other API approaches 
such as REST-oriented APIs. In REST-oriented APIs, making flexible queries to an endpoint 
with different combinations of parameters can be complex to implement. Also, obtaining the 
desired information may require combining results from multiple invocations to different  
endpoints, which often results in the retrieval of superfluous information in each of the invoca-
tions. This interaction model can be inefficient and lead to unsatisfactory performance.
In query-oriented APIs, such as GraphQL, each client specifies exactly which informa-
tion it is interested in. The types that are exposed by the API are described using a schema 
description language. This schema establishes a contract for how the client can access the data 
obtained from a single endpoint.
Queries are executed on the server side by a specialized component that can retrieve data 
from different sources of information, and the results are returned to the client. Only the data 
specified in the query is returned—unlike the case with standard REST-oriented APIs, which 
tend to return unnecessary data. Furthermore, changes in the query do not require the server 
side to be modified, as long as they are supported by the query schema. In addition, GraphQL 
supports changes to the data on the server, achieved by using mutations instead of queries.
Query-oriented APIs are a relatively new technology. Although they solve some of the 
problems that can occur with other types of APIs, they present new challenges and, possibly, 
steeper learning curves. This type of API is used primarily in front-ends and can be appro-
priate in scenarios involving limited bandwidth and situations where complex domain models 
with many entities and relationships need to be queried.
Asynchronous APIs
The approaches that were previously discussed are all based on a synchronous request–
response interaction pattern in which the interaction is initiated by the client. Although 

5.2  API-centric Design  103
this approach is useful for many types of scenarios, it is inadequate in some situations. For 
instance, if a client needs to be notified about changes that occur in a resource when these 
changes occur, the request–response approach does not work. While this functionality can be 
implemented through some type of periodic polling, this is a cumbersome and often inefficient 
approach. A better approach would be for the server that hosts the resource to initiate the inter-
action and to notify clients when changes occur in the resource.
Asynchronous APIs realize this approach. Events or commands can be sent by an event 
publisher and received by one or more consumers. These events or commands can be sent 
using different strategies, such as message distribution to one or more consumers or stream-
ing, and using technologies such as web hooks or server-side events. There are also ongoing 
efforts to specify asynchronous APIs in a similar manner to request–response APIs, such as 
the AsyncAPI specification. Currently, asynchronous APIs are not as widespread as request–
response APIs.
WebSocket APIs
WebSockets is a bidirectional stateful protocol that allows clients and servers to send messages 
to each other; contrast that with the HTTP protocol, which is stateless and unidirectional. The 
WebSockets protocol is useful for continuous data delivery and is widely used in the develop-
ment of real-time applications, such as gaming and real-time sports updates.
5.2.3.2  Patterns
As with other areas of design, a number of patterns (and anti-patterns) focus on solving prob-
lems related to API-centric development. Here we describe a sample of relevant patterns.
Back-End for Front-End
The Back-End for Front-End pattern is used to avoid the problems that occur when a single 
back-end supports different types of frontends (e.g., mobile, web). These problems include 
bloating and difficulty maintaining the common back-end. The main idea with this pattern is 
that each front-end has a specific back-end associated with it; hence the name “back-end for 
front-end” (BFF). The BFF exposes an API that is tailored to the specific needs of the partic-
ular front-end, which can help improve user experience, performance, security, and maintain-
ability. When this pattern is used, a user-facing application becomes the combination of the 
front-end and its back-end. The BFF typically acts as a façade to other services that are not 
exposed directly to the front-end. It may query several other services and aggregate the results 
in a response that is returned in a format that is efficient to the front-end. This approach helps 
improve performance, as only the needed data is returned. It also improves security, as APIs 
other than the ones from the BFFs are not publicly accessible. Also, additional concerns such 
as logging can be handled by the BFF. The downside of this pattern is that several BFFs need 
to be developed, depending on the number of front-ends that must be supported.
BFF and query-oriented APIs are related, in that the query-oriented API returns a 
response that is tailored to the needs of the client. As with BFFs, this response may contain 
information collected from several other services by the query server.

104  Chapter 5  API-centric Design
API Layering
As described in Section 3.5.2.1, layering is commonly used to organize modules into groupings 
with specific responsibilities so as to facilitate independent evolution. Modules are grouped 
into layers, and typically depend only on other modules located in the same layer or in a lower 
layer.
Layering can also be applied to APIs; that is, APIs with different types of responsibili-
ties can be grouped into specific layers. API implementation components (e.g., microservices) 
may depend only on APIs that are on the same or on a lower layer. For example, the API-Led 
Connectivity pattern structures APIs across three different layers, as shown in Figure 5.2:
■
■System APIs: These APIs are used to access data from back-end systems. Residing on 
the lowest layer, they are responsible for transforming the data obtained from these sys-
tems into a format appropriate for the consumers of the API. System APIs are typically 
not publicly accessible.
■
■Process APIs: These APIs are responsible for orchestrating system APIs and implement-
ing business processes and logic. They are typically not publicly accessible.
■
■Experience APIs: These APIs, located on the uppermost layer, implement the Back-End 
for Front-End pattern. The API typically supports one type of end-user application and is 
responsible for delivering data so that an optimal user experience can be achieved. The 
implementation consumes APIs from lower layers and must not contain business logic. 
These APIs are typically accessible to the outside world, so security is an important 
concern for them.
Invocations between API implementation components generally involve network calls. 
In this architecture, implementation components from an upper layer (e.g., the experience API 
layer) may invoke APIs from both the process or system API layers to avoid delays from tra-
versing the layers. As noted in Section 3.5.2.1, this is a modifiability/performance tradeoff.
This pattern is useful to support specialization in APIs. It also helps avoid the creation of 
tangled webs of API implementation components where a call to an API results in a chain of 
requests that traverses many components.
API Gateway
The API Gateway pattern is a type of façade pattern for APIs. The API gateway exposes a 
single point of entry to clients for other (typically internal) APIs. Such a gateway can handle 
both functional and quality concerns. Functionally, it can be used to expose an API that deliv-
ers data in a way that is optimal for a particular type of client. An example is the Back-End 
for Front-End Pattern. An API gateway can also be used to manage quality attribute concerns 
such as security, monitoring, or limiting traffic. API gateways from cloud providers typically 
fall in to this category.
Other Patterns
Several other patterns address the concerns discussed in Section 5.2.2.1. For example, there 
are patterns to support versioning, such as Version Identifier, and to support error manage-
ment, such as Error Report (see the “Further Reading” recommendations).

5.2  API-centric Design  105
API
System of record
API implementation (e.g., microservice)
Backend boundary
Layer boundary
System
API Layer
Process
API Layer
Experience
API Layer
Mobile Application
Web Application
Web Application
FIGURE 5.2  Example of an API layers architecture
5.2.3.3  Implementing APIs
Several different mechanisms can be used to implement APIs and patterns, but the specific 
mechanism that is selected should be invisible to the API consumers. Implementation mech-
anisms are key to supporting deployability, and are described in more detail in Section 6.2.2.
5.2.4  API Management
Once an organization engages in the journey of creating an API-based infrastructure to sup-
port business agility, it needs to establish a set of management practices at the organizational 

106  Chapter 5  API-centric Design
level to ensure success. These management practices, which are collectively known as API 
management, can be supported by specialized tools. API management tools may cover either 
part of or the full development life of APIs. API management models include practices such 
as those shown in Figure 5.3. These components are discussed in more detail in the following 
subsections.
Developer
Portal
API
Gateway
Policy
Management
and Analytics
API
Management
Design and
Development
Testing
FIGURE 5.3  Practices of API management
5.2.4.1  API Design and Development
API design and development requires tools to support both the design of APIs and the imple-
mentation of APIs. We discussed definition mechanisms in Section 5.2.2. Tools that support 
these activities include specification editors and validators, along with code generators that 
take the specification and produce skeletons of implementations.
5.2.4.2  Developer Portal
Self-service is an essential aspect of API-based development, so portals that allow developers 
to find APIs (development-time discoverability) and to understand how these APIs can be used 
(usability) are essential. If developers are unable to discover APIs or if they cannot understand 
how to use them, they may end up creating new interfaces that address similar concerns as the 
interfaces that already exist, resulting in an undesirable replication of previous work.

5.3  API-centric Design and ADD  107
Various tools have been created to allow specifications to be visualized in a human- 
readable form and to allow direct interaction with the API by making calls to it. Furthermore, 
developer portals may offer code snippets to facilitate writing API clients.
5.2.4.3  API Testing
API testing is an essential activity in API development. Tools are available that allow mock 
implementations (online or offline) to be generated directly from API specifications, facili-
tating the development of tests and parallel development. Automated API tests suites are also 
essential during development; tools such as Postman are useful in this task. Quality aspects 
such as performance can be tested as well, and some online tools can perform stress testing  
of APIs.
5.2.4.4  API Gateway
We introduced API gateways in Section 5.2.3.3 as a pattern, but they also play a fundamen-
tal role in API management strategies because they are the point of access to the back-end 
platform. Since all requests must go through the API gateway, it can be used as a proxy that 
manages many quality aspects such as security, rate throttling, analytics and monitoring, and 
monetization. Modern API gateways from cloud providers can use OpenAPI specifications as 
a starting point for configuration.
5.2.4.5  Policy Management and Analytics
Policy management supports fine-grained management of policies associated with operational 
aspects such as traffic management or security. Changes in policies must be made carefully, as 
they can have important impacts on system qualities or can affect clients. 
Analytics is also an essential part of API management. Usage monitoring information 
is essential to support activities such as monetization or making decisions such as allowing 
changes to published APIs or removal of unused APIs.
5.3  API-centric Design and ADD
In this chapter, we have covered a wide spectrum of considerations that need to be addressed 
to support API-centric design. We now turn our attention to how ADD can support design in 
API-centric environments. API-centric design is essentially architectural design but with a 
focus on APIs. Thus, our attention here is on the setting and structures of the APIs.
5.3.1  ADD and API Specification Design
The iterative design process promoted by ADD can be very useful in the design of APIs. Here 
we discuss how the steps of ADD can be mapped to the design of APIs, assuming that the 
decision to follow an API-centric approach has already been made.

108  Chapter 5  API-centric Design
Step 1: Review Inputs
As with any design, there should be clarity with respect to the drivers. Is this an API that will 
be consumed internally or externally? What is the purpose of this API? Is it an API that sup-
ports a particular front-end or an API that will be reused by many clients? Who are the con-
sumers of this API? What are the data types and anticipated data volume? Are there specific 
quality attributes that need to be considered in the design of the API, such as extensibility or 
security? What are the operations that need to be supported by the API (only queries, or also 
modifications to the resources)? Are there existing guidelines—for example, at the organiza-
tional level—regarding API design that need to be followed?
Step 2: Establish Iteration Goal by Selecting Drivers
The goal may be to create an initial version of the API, or it may be to refine a previous version 
of the API. When creating an initial version of an API, input from API consumers should be 
obtained. Also, it is important to identify which APIs to start with first. One possibility when 
constructing APIs over existing components is to start with common functions that can easily 
be reused before moving to more specialized APIs.
Step 3: Choose One or More Elements of the System to Refine
An initial design iteration of an API involves considering the API as a whole. Further itera-
tions may focus on specific parts of the API that would be the elements, such as the paths of a 
given resource or a specific operation.
Step 4: Choose One or More Design Concepts That Satisfy the Drivers
Initial design decisions for the design of the API involve selecting the type of API (e.g., REST, 
GraphQL, Async). Other important early decisions include satisfying concerns by selecting 
design concepts to address resource representation, versioning, authorization and authentica-
tion, and error management. Further decisions may involve selecting approaches to address 
specific drivers, such as supporting future extensions of the API or parts of it without breaking 
clients, or selecting approaches for optimizing the amount of data that is returned in a call.
Step 5: Instantiate Architectural Elements, Allocate Responsibilities, and Define 
Interfaces
Once the design concept has been chosen, it needs to be applied to the API that is being 
designed. This instantiation activity involves, for example, deciding in a REST API if the  
versioning scheme will reside on the paths or be part of a header.
The main activity, though, is the actual design of the API, which corresponds to defin-
ing interfaces. As discussed in Section 4.6, sequence diagrams are a very useful tool to sup-
port this activity. Once operations are identified, responsibilities can be allocated to them. 
For instance, certain operations will be responsible for retrieving data, whereas others will 
be responsible for creating or modifying data. Great care should be taken in the design of the 
operations and the data that is exchanged through the API operations in accordance with the 
API style and to facilitate aspects such as usability. Identifying the resource representations, 
their properties, and their relationships is also an essential activity during this step.

5.3  API-centric Design and ADD  109
During instantiation, API prototypes can be created. For example, a specification of the 
API that is being designed can be created using a specification language such as OpenAPI. 
This specification can be used to generate human-readable documentation pages, or mock 
implementations, which are useful for obtaining feedback.
Step 6: Sketch Views and Record Design Decisions
In the context of API design, documentation is fundamental; however, that documentation 
should be part of the API and not something that is captured in an external document. API 
design decisions can be recorded by adding comments to the schemas or specifications.
Step 7: Perform Analysis of the Current Design and Review the Iteration Goal and 
Achievement of the Design Purpose
At the end of the iteration, it is necessary to evaluate whether the current design is sufficient 
to proceed to implementation. The API may be reviewed by peers or using automated tools 
to check conformance to standards and identify design flaws. Complex APIs may require  
multiple design iterations.
5.3.2  Using ADD in Other Areas of API-centric Design
ADD can also be used in other areas of API-centric design discussed in this chapter.
5.3.2.1  Design of an Implementation Mechanism
The design of an implementation mechanism for the API is a task where ADD can be applied. 
For example, once an API has been specified, it may be implemented using a microservice. 
The standard steps of ADD described in Chapter 4 can be followed to design the microservice 
in question. However, in this case the API specification becomes an important driver in addi-
tion to the factors that were mentioned in Section 2.4.
5.3.2.2  Design of a Greenfield API Platform
The design of a greenfield API platform is not a common situation, but when it presents itself, 
certain design decisions related to API-centric design must be made early on. Ideally, early 
decisions will be made regarding how to structure the APIs across layers, how to identify their 
correct levels of granularity, and how to manage versioning, error management, and security, 
among other concerns. There is a risk of inconsistency if these decisions are not made and 
disseminated at the beginning of the design process, and a similar fate befalls many APIs: 
They are subsequently developed without clear guidelines. Furthermore, the cost of refactor-
ing inconsistent APIs may be high if the APIs are already being consumed by many clients.
One possible approach is to start with the design of a specific product or application that 
involves APIs that cross the different layers of the platform. Perhaps a front-end consumes an 
API that itself depends on other APIs. This kind of design can be completed in a few ADD 
iterations and can prove helpful in establishing the foundational design decisions for the API 
platform as a whole.

110  Chapter 5  API-centric Design
5.3.2.3  Moving from Monolith to API Platform
Many companies are facing the prospect of moving from an IT infrastructure centered on a 
monolith to an API platform that allows the company to support business agility. In such cases, 
patterns such as the Strangler Fig (introduced in Chapter 4) can be helpful.
One possible approach to strangle a monolith and convert it to an API platform is to 
establish a design for the platform based on API layers. A number of APIs (typically system 
and process APIs) are built so that they are implemented by the monolith (see the discussion of 
monoliths as implementation mechanisms in Section 6.2.2). New applications consume these 
APIs and are decoupled from the legacy monolith, and the monolith itself is slowly replaced 
through the Strangler Fig approach. In such a situation, ADD can be used for both the design 
of the platform and the design of the APIs.
5.4  Summary
In this chapter, we presented an overview of API-centric design and discussed its role as an 
enabler of business agility through composability. API-centric design involves not only the 
design and specification of particular APIs, but also decisions on how to implement the APIs 
and how to organize them as a platform. Also, aspects related to managing the APIs are essen-
tial to support self-service and other important governance aspects. Finally, we discussed how 
ADD can be applied in different areas of API-centric design.
5.5  Further Reading
The definition of business agility is taken from the Agile Alliance’s website:  
www.agilealliance.org/glossary/business-agility/.
Current trends in adoption of agile practices are discussed in the Annual State of Agile 
Report: https://digital.ai/resource-center/analyst-reports/state-of-agile-report/.
Documentation of the Scaled Agile Framework (SAfe), including the definition and 
a discussion about business agility, is available at https://scaledagileframework.com/
business-agility/.
The REST architectural style was defined by Roy Fielding in 2000 in his PhD  
dissertation, Architectural Styles and the Design of Network-Based Software Architectures, 
University of California, Irvine, 2000. An explanation of Richardson’s Maturity Model can be 
found online at https://martinfowler.com/articles/richardsonMaturityModel.html.

5.6  Discussion Questions  111
Numerous books and online resources cover the topic of API design, including the 
following:
■
■
A. Lauret, The Design of Web APIs, Manning Publications, 2019.
■
■
O. Zimmerman, M. Stocker, D. Lübke, U. Zdun, and C. Pautasso, Patterns for 
API Design: Simplifying Integration with Loosely Coupled Message Exchanges, O’Reilly, 
November 2022.
■
■
Google Cloud/Apigee team, “Web API Design: The Missing Link, Best Practices for 
Crafting Interfaces that Developers Love”, https://cloud.google.com/files/apigee/apigee-web-
api-design-the-missing-link-ebook.pdf.
In this chapter, we mentioned several technologies that support API-centric development. 
More information about them can be found in the following links:
■
■
OpenAPI: www.openapis.org/
■
■
AsyncAPI: www.asyncapi.com/
■
■
OAuth: https://oauth.net/specs/
■
■
gRPC: https://grpc.io/
■
■
SOAP: www.w3.org/TR/soap/
■
■
GraphQL: https://graphql.org/
■
■
WebSockets: www.w3.org/TR/2021/NOTE-websockets-20210128/
The API-Led Connectivity pattern from Mulesoft defines three different API layers. 
More information can be found online at www.mulesoft.com/resources/api/types-of-apis.
API management is discussed in Brajesh De’s book An Architect’s Guide to Developing 
and Managing APIs for Your Organization, Apress, 2017.
5.6  Discussion Questions
1.	
What is different about the API-centric approach discussed in this chapter versus  
previous approaches such as SOA?
2.	
What is a “Death Star architecture”, and how can this problem be avoided by following 
the concepts discussed in this chapter?
3.	
Currently the OpenAPI specification provides only constructs to specify aspects related 
to security; other quality attributes are not supported. What do you think is the reason 
for this? Propose constructs to support other quality attributes such as performance or 
availability.
4.	
Find an example of a GraphQL API and compare it with an example of an API docu-
mented using OpenAPI. What are the pros and cons of Swagger documentation versus 
GraphQL documentation?

112  Chapter 5  API-centric Design
5.	
What are the risks of not following API management practices in an organization that 
wishes to build an API platform?
6.	
Under what circumstances would you favor API-centric design over a more traditional 
approach? What business goals or other drivers would lead you to favor this approach?
7.	
Find an example of a published API and invoke it using Postman. Did the API documen-
tation provide enough information to properly consume the API? What more could have 
been provided to ensure a better experience for a developer unfamiliar with the selected 
API?
8.	
Consider defining an API using OpenAPI. How difficult is it to define this API thor-
oughly to support self-service by other developers? Compare the usability of the API you 
defined with the one you consumed in question 7.

113
6
Designing for Deployability
In Chapter 5, we focused on the importance of composability and how API-centric design 
supports business agility. But just designing your infrastructure and systems with APIs and 
separation of concerns in mind may not be enough. A focus on composability, if done well, 
allows you to change your code base with relative ease, and can help you avoid accumulating 
technical debt. But composability, by itself, doesn’t get those changes out to market. For that 
we need deployability. In this chapter, we describe deployability as a quality attribute, present 
some design concepts associated with it, and discuss it in the context of ADD.
Why Read This Chapter?
Today, DevOps practices have become widespread and are key to many business  
models, shortening the development life cycle. Deployability is an essential quality  
attribute related to these practices. In this chapter, you’ll become familiar with this  
quality attribute and how to design to achieve it at scale using ADD.
6.1  Deployability Principles and Architectural Design
Deployability, like any other important system quality, needs to be consciously designed and 
managed. How do we do this? First, let’s understand what we mean by this term.
6.1.1  Defining Deployability
Deployability refers to a property of software indicating that it may be deployed—allocated 
to an environment for execution—with predictable time and effort. If the deployment pro-
cess is fully automated—that is, if there is no human intervention—then it is called contin-
uous deployment. An architect who is designing for deployability needs to reason about the 
deployment pipeline: the sequence of tools and activities that begin when a developer checks 

114  Chapter 6  Designing for Deployability
in some code and ends when some application functionality has been deployed to an execution  
environment. Deployability is not simply a quality attribute that is associated with products; it 
is also important for the components that are used in those products (and which might imple-
ment the APIs discussed previously).
Deployment means more than just delivering the system or its components to the final 
environment. As part of the development process, the system needs to be deployed to dif-
ferent environments in addition to the one where the system is developed. Different sets of 
tests are performed in each environment, expanding the testing scope from unit testing of a 
single module in the development environment, to functional testing of all the components that 
make up a service in the integration environment, and ending with broad quality testing in the 
staging environment and usage monitoring in the production environment. The major environ-
ments that typically exist and the tests that are performed in each one of them are as follows  
(Figure 6.1):
■
■Code is written in a development environment for a module, where it is subject to unit 
tests. After it passes these tests, and potentially after appropriate review, the code is 
pushed to a version control system that triggers the build activities in the integration 
environment.
■
■An integration environment is a pre-production environment that builds an execut-
able version of the component or service. A continuous integration server compiles the 
changed code, along with the latest compatible versions of code for other portions of the 
service, and constructs a deployable executable image. Tests in the integration envi-
ronment may include the unit tests for the various modules (now run against the built 
system) as well as integration tests designed specifically for the entire system or system 
part being deployed. When these tests are passed, the built service is promoted to the 
staging environment.
■
■A staging environment is a pre-production environment that tests for various qualities 
of the total system. These include performance testing, security testing, license confor-
mance checks, and possibly acceptance testing. A system that passes all staging environ-
ment tests—which may include field testing—is deployed to the production environment, 
typically using either a blue/green model or a rolling upgrade. In some cases, partial 
deployments are used for quality control or to test the market response to a proposed 
change or offering.
■
■Once in the production environment, the service is monitored until all parties have  
confidence in its quality. At that point, it is considered a normal part of the system.
These environments, along with the tools to provision them, to move artifacts from one 
environment to another, to test them, and to monitor them, form the backbone of the deploy-
ment pipeline.

6.1  Deployability Principles and Architectural Design  115
Local
Environment
A
B
A + B
+ C
All
Features
All
Features
Pre-production
Environments
Final
Environment
Development
Integration
Staging
Production
Legend:
Code associated
with features
Database with
relative size
Flow of deployment
FIGURE 6.1  Environments where the system is deployed
Table 6.1 outlines the general scenario for deployability. Part of this scenario involves 
specifying which environment is of concern. But, of course, other concerns need to be 
reflected as well: what causes the trigger for a deployment, what needs to be changed, what 
should happen in response to the trigger, and how we measure success.
TABLE 6.1  General Scenario for Deployability
Portion of 
Scenario
Description
Possible Values
Source
The trigger for the 
deployment
End user, developer, system administrator, operations 
personnel, component marketplace, product owner.
Stimulus
What causes the 
trigger
A new element is available to be deployed. This is 
typically a request to replace a software element 
with a new version (e.g., fix a defect, apply a security 
patch, upgrade to the latest release of a component or 
framework, upgrade to the latest version of an internally 
produced element).
A new element is approved for incorporation.
An existing element/set of elements needs to be rolled 
back. 
Artifacts
What is to be changed Specific components or modules, the system’s platform, 
its user interface, its environment, or another system with 
which it interoperates. Thus the artifact might be a single 
software element, multiple software elements, or the 
entire system.
Environment
Where the artifacts 
are being deployed
Integration environment.
Staging environment.
Production environment.
continues

116  Chapter 6  Designing for Deployability
Portion of 
Scenario
Description
Possible Values
Response
What should happen
Full deployment.
Subset deployed to a specified portion of: users, virtual 
machines (VMs), containers, servers, platforms.
Monitor the new components.
Roll back a previous deployment. 
Response 
measure
How effective is 
the deployment, in 
terms of cost, time, or 
process effectiveness
Cost in terms of:
■
■
Number, size, complexity of affected artifacts
■
■
Average/worst-case effort
■
■
Elapsed time
■
■
Money (direct outlay or opportunity cost)
■
■
New defects introduced
Extent to which this deployment/rollback affects other 
functions or quality attributes.
Number of failed deployments.
Repeatability of the process.
Traceability of the process.
Note that while we refer to the quality attribute as deployability, a set of practices that 
have grown up around the concerns of deployability have become known as DevOps, because 
they bridge the concerns of development and operations. DevOps includes additional concerns 
such as monitoring the system in production.
6.1.2  Continuous Integration, Deployment, and Delivery
Frequently delivering new versions of the system, or its components, is necessary to support 
agile development. This means that a change that is made in the development environment 
needs to be deployed as quickly as possible to the production environment and, consequently, 
needs to move across the different pre-production environments discussed previously. This is 
a foundational principle of DevOps, and a key aspect to support it is to automate this process 
as much as possible.
When the process is fully automated—that is, if there is no human intervention—then 
it is called continuous integration (CI) or continuous deployment (CD), depending on how 
far automation takes the system across the environments. If the process is automated up to 
the point of placing system components into production, but human intervention is required 
(perhaps due to regulations or policies) for this final step, then the process is called continuous 
delivery.
In many systems, deliveries can occur at any time—possibly multiple ones per day—and 
each can be instigated by a different team. Being able to deliver frequently means that new 
features, bug fixes, and security patches do not have to wait until the next scheduled release, 
but can be developed and released as soon as the feature is tested and ready, or as a bug is  
discovered and fixed.
Continuous deployment is not desirable, or even possible, in all domains. If the software 
exists in a complex ecosystem with many dependencies, it may not be possible to release just 

6.1  Deployability Principles and Architectural Design  117
one part of it without coordinating that release with the other parts. In addition, many embed-
ded systems, safety-critical systems, systems residing in hard-to-access locations, and sys-
tems that are not networked are poor candidates for a continuous deployment mindset. In such 
cases, a strategy of frequent deployment to staging, with less frequent deployments to produc-
tion, could be employed.
If the new deployment is not meeting its specifications, it can be (and in some cases must 
be) rolled back. This “rolling back” returns the system to its prior state, within a predictable 
and acceptable amount of time and effort. As the world moves increasingly toward virtualiza-
tion and cloud infrastructures (see Chapter 7), and as the scale of deployed software-intensive 
systems increases, it is one of the architect’s responsibilities to make design decisions that sup-
port efficient and predictable deployment. In particular, the ability to roll back deployments 
quickly and efficiently is key to minimizing overall system risk.
6.1.3  Designing for Deployability
To design for deployability, decisions need to be made about both (1) the architecture of the 
system being deployed and (2) the architecture of the deployment infrastructure. While these 
architectures are related, they are distinct and may be separately designed. We will discuss the 
deployment infrastructure in Section 6.1.3.2.
6.1.3.1  Designing the System to Support Deployability
To support deployability, an architect needs to make decisions about how an executable is 
updated on a host platform, as well as how it is subsequently invoked, measured, monitored, 
and controlled. Mobile systems in particular present a challenge for deployability in terms of 
how they are updated because of bandwidth and data volume constraints. Some of the issues 
involved in deploying software are as follows:
■
■How does the new executable arrive at its host (e.g., push, where updates deployed are 
unbidden, or pull, where users or administrators must explicitly request updates)?
■
■How does it interact with any existing systems? Can this interaction be switched over 
while the existing systems are executing?
■
■What is the medium, such as USB drive or Internet delivery?
■
■What is the packaging (e.g., executable, container image, app, plug-in)?
■
■What is the resulting integration into an existing system?
■
■What is the efficiency of executing the process?
■
■How controllable and automated is the process? For example, can new versions be 
swapped in and out, and potentially rolled back, with minimal human intervention and 
without disrupting system operations?
To address all of these concerns, the architect must be able to assess the associated risks. 
Architects are primarily concerned with the degree to which the architecture supports deploy-
ments that are:

118  Chapter 6  Designing for Deployability
■
■Granular. Deployments can be of the whole system or of elements within a system. If the 
architecture provides options for finer granularity of deployment, then certain risks can 
be reduced.
■
■Controllable. The architecture should provide the capability to deploy at varying levels 
of granularity, monitor the operation of the deployed units, and roll back unsuccessful 
deployments.
■
■Efficient. The architecture should support rapid deployment (and, if needed, rollback) 
with a reasonable level of effort.
Architectural choices (tactics and patterns) profoundly affect deployability. For example, 
when the microservice architecture pattern (see Section 6.2.2) is employed, each team respon-
sible for a microservice can package an executable with all its runtime dependencies and hence 
can make their own technology choices. This removes incompatibility problems that would 
previously have been discovered at integration time (e.g., incompatible choices of which ver-
sion of a library to use). Because microservices are independent services, such choices do not 
usually cause problems.
Similarly, a continuous deployment mindset forces you to think about the infrastructure 
for testing, and you are wise to think about this early in the development process. This “shift 
left” is necessary because designing for continuous deployment requires continuous automated 
testing. In addition, the need to be able to roll back or disable features leads to architectural 
decisions about mechanisms, such as feature toggles and backward compatibility of interfaces. 
These decisions are best taken early on.
6.1.3.2  Designing the Deployment Infrastructure
Deployability requires the support of a deployment infrastructure, which is primarily consti-
tuted by a deployment pipeline. A deployment pipeline is the sequence of tools and activities 
that begin when you check your code into a version control system and end when your system 
has been deployed so that users or other systems can send it requests.
In between those endpoints, a series of tools build, integrate, and automatically test the 
newly committed code; test the integrated code for functionality; test the system for concerns 
such as performance under load, security, and license compliance; and, when quality gates are 
met, push the new services into production.
Of course, things do not always go according to plan. If you find problems after the  
software is in its production environment, perhaps by monitoring or perhaps through user- 
submitted bug reports, it may be necessary to roll back to a previous version while the defect 
is being addressed. Ideally, we want this rollback to be done at the push of a button, so this 
capability must also be part of the deployment pipeline.
Thus, tools to realize a deployment pipeline include code analysis tools, test automation 
tools, continuous integration servers, deployment automation tools, monitoring tools, and arti-
fact repositories. It should be noted that the design of the deployment infrastructure is primar-
ily a task of DevOps (or DevSecOps) engineers in collaboration with the architects. In smaller 
organizations, these might be the same person; in large organizations, they will be separate 
individuals or even separate teams.

6.2  Design Decisions to Support Deployability  119
6.2  Design Decisions to Support Deployability
To design for deployability, we need to think about all of the bottlenecks that might prevent us 
from deploying quickly and with minimal labor. We want to ensure that our software is:
■
■Easy to change, with few ripple effects, and easy for developers to work in parallel. This 
increases delivery velocity because one developer’s changes are less likely to affect 
another and, in turn, testing and deployment are likely to be less complex.
■
■Easy to test with automated tests. If tests are not fully automated, the pipeline will 
require human intervention, potentially reducing velocity.
■
■Easy to monitor, so that defects introduced by the release, such as performance problems 
or security flaws, can be spotted quickly.
■
■Easy to roll back if, for example, the monitoring revealed a problem.
■
■Easy to scale, up or down, so that a gradual replacement of instances can be made  
without affecting the performance of the running application.
■
■Robust to failures of individual parts, so that a failed update in a single service will not 
cause the entire system to fail, for example.
Examining this list, it should come as no surprise that these are all qualities— 
modifiability, performance, availability, and testability—that architects have striven to achieve 
for decades. They are all desirable for many complex systems. The only aspect that is particu-
lar to deployability is that we want all of them, all the time! In this chapter, we focus on only 
the quality attributes that are specific to deployability—but keep in mind that if you do not 
worry about the other quality dimensions, you will likely not achieve your deployability goals.
6.2.1  Deployability Tactics
A deployment may be triggered by the release of one or more new or modified software  
elements. The deployment is considered successful if these elements are deployed within 
acceptable time, cost, and quality constraints.
Figure 6.2 shows the deployability tactics categorization. There are two major categories 
of deployability tactics: Manage Deployment Pipeline and Manage Deployed System. The first 
category focuses on design decisions that apply to the deployment infrastructure, and the sec-
ond focuses on design decisions that apply to the functionality being deployed. Let us examine 
these in turn.

120  Chapter 6  Designing for Deployability
Deployability Tactics
Manage Deployment Pipeline
Manage Deployment System
Scale Rollouts
Script Deployment Commands
Rollback
Manage Service Interactions
Package Dependencies
Toggle Features
Externalize Conﬁgurations
FIGURE 6.2  Deployability tactics categorization
Within the Manage Deployment Pipeline category, the tactics are:
■
■Scale rollouts. Rather than deploying to the entire user base, scaled rollouts deploy a new 
version of a service gradually, to subsets of the user base. By releasing gradually, the 
effects of new deployments can be monitored and measured and, if necessary, rolled back.
■
■Roll back. If a deployment has defects, it can be “rolled back” to its prior state. Since 
deployments may involve multiple updates of multiple services, the rollback mechanism 
must be able to keep track of these, or reverse the consequences of any update, ideally in 
a fully automated fashion.
■
■Script deployment commands. The complex steps to be carried out and precisely orches-
trated in a deployment should be scripted. This is part of a general movement toward 
“infrastructure as code”.
Within the Manage Deployed System category, the tactics are:
■
■Package dependencies. Elements are packaged with their internal dependencies (those 
required for the element to execute) so that they get deployed together.
■
■Manage service interactions. Deployment of versions of services that are interacting 
with external entities needs to be mediated so that incompatibilities are avoided.
■
■Feature toggle. A “kill switch” (feature toggle) can be used for new features to automat-
ically disable a feature at runtime, without forcing a new deployment. It is often the case 
that issues arise after deploying new features. Feature toggles reduce the risk associated 
with such bugs.
■
■Externalize configurations. The system should not have any hardcoded configurations 
that limit the possibility of moving it from one environment to another (e.g., from testing 
to production). This is typically achieved through the use of environment variables 
whose values are stored in an external repository.

6.2  Design Decisions to Support Deployability  121
6.2.2  Deployability Patterns
Architecture patterns for deployability can be organized into two major categories, similar to 
the two categories of tactics. The first category contains patterns for structuring services to 
be deployed. The second category contains patterns for how to deploy services, which can be 
parsed into two broad subcategories: all-or-nothing or partial deployment. The two main cate-
gories for deployability are not completely independent of each other because certain deploy-
ment patterns depend on certain structural properties of the services.
We begin by discussing patterns for structuring the services to be deployed. With respect 
to deployability concerns, there are a few common options for structuring a system: as a set of 
microservices, as a set of tiers, as a load-balanced cluster, or as a (possibly modular) monolith. 
We briefly sketch each of these architectural patterns.
6.2.2.1  Microservice Architecture
The microservice architecture pattern structures the system as a collection of independently 
deployable services that communicate only via messages through service interfaces. These 
services are meant to be:
■
■Highly maintainable and testable
■
■Loosely coupled
■
■Independently deployable
■
■Organized around business capabilities
■
■Owned by a small team
No other form of interprocess communication is allowed: no direct linking, no direct 
reads of another service’s data store, no shared-memory model, no back-doors whatsoever. 
Services are usually stateless, and (because they are developed by a single small team) are 
relatively small—hence the term microservice. Service dependencies should be acyclic. An 
integral part of this pattern is a discovery service so that messages can be appropriately routed.
Microservices provide APIs and may also need to consume APIs to satisfy the contract 
that they provide. These APIs are generally of the request–response type, but they may also 
be asynchronous (sending and receiving events). Consumed APIs, especially of the request–
response type, become dependencies of the microservice.
One of the biggest challenges when using microservices is to find the right granularity of 
the microservices and the APIs they provide. A number of drivers for granularity disintegra-
tion and integration are available. Granularity disintegration means that a service should be 
broken apart, whereas granularity integration means that a service should be integrated from 
separate services. Granularity disintegration drivers include the following:
■
■Service scope and function: Is the service doing unrelated things?
■
■Code volatility: Are changes isolated to only one part of the service?
■
■Scalability and throughput: Do parts of the service need to scale differently?
■
■Security: Do some parts of the service need higher security levels than others?
■
■Extensibility: Is the service always expanding to add new contexts?
Granularity integration drivers include the following:

122  Chapter 6  Designing for Deployability
■
■Database transactions: Is an ACID transaction required between separate services?
■
■Workflow and choreography: Do services need to talk to one another, and are there 
services that interact frequently with each other?
■
■Shared code: Do services need to share code between each other?
■
■Data relationships: A service can be broken apart, but can the data it uses be broken 
apart as well?
Management of concerns that arise because of dependencies between microservices can 
be challenging. For example, consumed APIs, both external and internal, may require creden-
tials to be used, so design decisions must be made about how to provide these credentials to 
the consumed APIs. Several different token delegation patterns can be used to address this sit-
uation. Another challenging concern is error management. If an invocation to a consumed API 
fails, this may result in an error in the provider of the API. Propagating the error information 
across dependent microservices so that the error can be tracked adequately, and so that the end 
user can obtain useful information, is also a challenge. Failures in the consumption of an API 
can be problematic as well, but can be addressed by using patterns such as the Circuit Breaker 
pattern. Moreover, transaction management can present challenges if an API endpoint requires 
the implementing microservice to perform a transaction that involves two or more consumed 
APIs; these are usually addressed using patterns such as SAGA.
Many other concerns need to be dealt with when using microservices, including service 
discovery, load balancing, scaling, monitoring, and management of configuration, among oth-
ers. One approach to dealing with these concerns is the introduction of sidecars as proxies 
between dependent microservices. With this strategy, code that supports these concerns is 
placed in the sidecar and is not part of the microservice. These sidecars intercept data accord-
ing to rules defined by a controller; this is the basis of a service mesh.
6.2.2.2  Monoliths and Modular Monoliths
A monolithic architecture is one in which all of the modules of the system are compiled into a 
single executable, which is then deployed as a single unit. Any code change typically requires a 
re-deployment of the entire system. This was the way that most systems were structured before 
the advent of microservice architectures. Like any architectural pattern, monoliths have both 
costs and benefits. In recent years, many organizations have moved away from this approach. 
Some of the reasons for this migration away from monoliths is that they allow for less control 
over deployment and fewer options for scalability.
One possible benefit of using a monolith is that, because it is not a distributed system, 
there should be fewer problems related to distributed transactions that occur frequently with 
microservice architectures. Also, managing the deployment of a single unit of code is sim-
pler than managing the deployment of multiple units of code. The complexity associated with 
microservice architectures has motivated some organizations to move back to monoliths in 
recent years.
One variant of traditional monoliths that has become popular recently is modular mono-
liths. In modular monoliths, the modules contain all of the parts needed to provide particu-
lar features, and the data associated with the module is isolated from the data belonging to 
other modules, similar to how functionality and data are packaged together in microservices. 
Modular monoliths are thus similar to microservices, except that the modules are not deployed 

6.2  Design Decisions to Support Deployability  123
as independent executables, they do not result in a distributed system, and the data they store 
is usually isolated (Figure 6.3). Although modular monoliths suffer from the same limitations 
as traditional monoliths in terms of deployment and scalability, one benefit of this approach is 
that the internal modularization of these monoliths should facilitate their separation into dif-
ferent deployment units in case this is required later on.
Key:
Component and
associated data store
Monolith
Modular Monolith
Microservices
Node
Dependency
FIGURE 6.3  Monolith versus modular monolith versus microservices
Now we turn our attention to a different class of patterns—those for managing the deploy-
ment of services. The first two patterns presented in this section, microservices and monoliths, 
are options for how to structure a system or a service. The patterns discussed next focus on 
how to deploy a system or service, irrespective of how it is internally structured.
Suppose there are N instances of Service A and you wish to replace them with N instances 
of a new version of Service A, leaving no instances of the original version. You wish to do this 
with no reduction in quality of service to the clients of the service, so there must always be  
N instances of the service running.
6.2.2.3  N-Tier Deployment
Another common form of deployment, often chosen to facilitate performance scalability, is 
n-tier deployment. The creation of tiers involves additional costs, in terms of added network 
latency, complexity, and deployment effort. However, it also offers many benefits, such as the 
ability to optimize and scale different tiers independently, to isolate faults, and to promote 
security, by placing sensitive resources in inner tiers that are not directly accessible by external 

124  Chapter 6  Designing for Deployability
clients. In addition, different security policies may be applied according to the tier, and fire-
walls may be added between the tiers. Three common alternatives of distributed deployment 
are 2-tier, 3-tier, and 4-tier.
2-Tier Deployment (Client-Server)
Two-tier deployment is the most basic layout for distributed deployment. The client and the 
server are usually deployed on different physical tiers, as shown in Figure 6.4.
Client
Client Tier
Web /
Application /
Data Server
Server Tier
FIGURE 6.4  An example 2-tier deployment
3-Tier Deployment
In this pattern, shown in Figure 6.5, the application is deployed in a tier that is separate from 
the one that hosts the database. This is a very common physical layout for web applications.
Web / App
Server
Client
Database
Web / App Tier
Database Tier
Client Tier
FIGURE 6.5  An example 3-tier deployment

6.2  Design Decisions to Support Deployability  125
4-Tier Deployment
In this variant, shown in Figure 6.6, the web server and the application server are deployed in 
different tiers. This separation is usually implemented to improve security, as the web server 
may reside in a publicly accessible network while the application resides in a protected net-
work. Additionally, firewalls may be placed between the tiers.
Web
Server
App
Server
Client
Client Tier
Web Tier
Database
Database Tier
Business Logic Tier
FIGURE 6.6  An example 4-tier deployment
6.2.2.4  Load-Balanced Cluster
In this deployment pattern, an application is deployed onto multiple servers that share the 
workload (Figure 6.7). Client requests are received by a load balancer, which redirects them to 
one of the servers according to their current load. The application server instances can process 
requests concurrently, thereby improving performance.
Firewall*
Firewall*
* = optional element
Load
Balancer
Client Tier
Application Tier
Data Tier
Database Server
Instance 2:
AppServer
Client
Instance n:
AppServer
Instance 1:
AppServer
FIGURE 6.7  A load-balanced cluster deployment

126  Chapter 6  Designing for Deployability
The preceding patterns all offer ways to structure the hardware and software onto which 
applications are deployed. Now we turn our attention to a different class of patterns, which 
allow for different modes of deployment and rolling back deployments. Two patterns for com-
plete replacement are well known, both of which are realizations of the scale rollouts tactic: 
blue/green and rolling upgrade.
6.2.2.5  Blue/Green Pattern
In a blue/green deployment, N new instances of a Service A are created; these are termed the 
“green” instances. After the N instances of new Service A are deployed, the DNS server, load 
balancer, or discovery service is changed to point to the new version. Once it is determined 
that the new instances are working satisfactorily, then—and only then—are the N instances of 
original Service A removed.
Before this cutoff point, if a problem is found in the new version, perhaps via monitoring, 
it is a simple matter of switching back to the original (the blue services) with little or no inter-
ruption to the service clients.
6.2.2.6  Rolling Upgrade
A rolling upgrade replaces instances of Service A with instances of the new version one at a 
time (or a small fraction at a time). The steps of the rolling upgrade are as follows:
1.	
Allocate resources for a new instance of Service A.
2.	
Deploy the new instance.
3.	
Begin to direct requests to the new instance.
4.	
Choose an instance of the old Service A, allow it to complete any active processing.
5.	
Destroy that instance.
6.	
Repeat the preceding steps until all instances of the old version have been replaced.
The benefit of these two patterns is the ability to completely replace deployed versions of 
services without having to take the system out of service, thereby increasing the system’s avail-
ability. The cost is resources. On this dimension, the two patterns differ: The peak resource 
utilization for a blue/green approach is 2N instances, whereas the peak utilization for a rolling 
upgrade can be as small as N + 1 instances.
There are other issues to consider as well. If you use blue/green deployment, then at any 
time a client is using either the new version or the old version. If you are performing a rolling 
upgrade, both versions are active simultaneously. This introduces two potential problems:
■
■Temporal inconsistency. In a sequence of requests by Client C to Service A, some may 
be served by the old version of the service and some may be served by the new version. 
If the versions behave differently, this may cause Client C to produce erroneous, or at 
least inconsistent, results. (However, this can be prevented by using the manage service 
interactions tactic.)
■
■Interface mismatch. If the interface to the new version of Service A is different from 
the old version, then invocations by clients that have not been updated to reflect the new 
interface will produce unpredictable results. This can be prevented by extending (but 

6.2  Design Decisions to Support Deployability  127
not modifying) the existing interface, or by using an adapter pattern to translate the old 
interface to the new one.
What if you do not wish to completely replace your services? Sometimes changing all 
instances of a service is undesirable. Partial-deployment patterns aim to provide multiple ver-
sions of a service simultaneously for different user groups; they are used for purposes such as 
quality control (canary testing) and marketing tests (A/B testing).
6.2.2.7  Canary Testing
Before rolling out a new release, it is prudent to test it in the production environment, but with 
a limited set of users. Canary testing is the continuous deployment analog of beta testing. This 
approach is named after the 19th-century practice of bringing canaries into coal mines. Coal 
mining releases gases that are explosive and poisonous. Because canaries are more sensitive to 
these gases than are humans, coal miners brought the birds into the mines and watched them 
for signs of reaction to the gases. The canaries acted as early warning devices for the miners, 
indicating an unsafe environment.
Canary testing designates a small set of users who will test the new release. Sometimes, 
these testers are so-called power users or preview-stream users from outside your organization 
who are more likely to exercise code paths and edge cases than the typical users, who may 
use the system less intensively. Users may or may not know that they are serving as guinea 
pigs—er, that is, canaries. Another approach is to use testers from within the organization that 
is developing the software. For example, Google employees almost never use the release that 
external users would be using, but instead act as testers for upcoming releases.
In both cases, the users are designated as canaries and routed to the appropriate version of 
a service through DNS or load-balancer settings or through discovery-service configuration. 
After testing is complete, users are all directed to either the new version or the old version, and 
instances of the deprecated version may be destroyed. Rolling upgrade or blue/green deploy-
ment could be used to deploy the new version. If the canary versions are implemented using 
feature toggles, then the rollout (or rollback) is even simpler—simply toggling the feature in 
the desired direction.
6.2.2.8  A/B Testing
A/B testing is used by marketers to perform an experiment with real users to determine which 
of several alternatives yields the best business results. A small but meaningful number of users 
receive a different treatment from the remainder of the users. The difference can be minor, 
such as a change to the font size or form layout, or it can be more significant. The “winner” 
is kept, the “loser” discarded, and another contender designed and deployed. An example of 
A/B testing is a bank offering different promotions to customers to encourage them to open 
new accounts. An oft-repeated story is that Google tested 41 different shades of blue to decide 
which shade to use to report search results.
Similar to canary testing, load balancers, DNS servers, and discovery-service configura-
tions can be set to send client requests to different versions. In A/B testing, the different ver-
sions are monitored to see which one provides the best response from a business perspective.

128  Chapter 6  Designing for Deployability
Like the patterns for complete replacement of services, patterns for partial replacement 
work best when the services being replaced are built with the tactics of encapsulation and 
externalization of state in mind.
6.3  Deployability and ADD
Now that we have a solid understanding of the architectural impacts of deployability, and 
the design options at an architect’s disposal, we turn our attention to the process of design. 
Specifically, we look at how ADD can be used to support design for deployability.
In the following sequence of steps, we assume that deployability has already been deter-
mined to be an architectural driver. This discussion of ADD focuses on the specific concerns 
of deployment, as they relate to the steps of the method.
Step 1. Review inputs
	
In this initial step, one or more deployability scenarios should have been collected 
as part of the architectural drivers. The general scenario for deployability (as shown 
in Table 6.1) can help here during reflection or in eliciting such scenarios from 
stakeholders.
Step 2. Establish the iteration goal by selecting drivers
	
At this point, we have established that deployability is important to the success of the 
system and have elicited one of more deployability scenarios. For this iteration, one of 
these scenarios will be the primary driver. Of course, other iterations could consider 
other deployability scenarios.
Step 3. Choose one or more elements of the system to refine
	
In Section 6.1.3, we stated that to design for deployability, the architect needs to think 
about the architecture of the application being deployed as well as the architecture of 
the deployment infrastructure. With respect to the architecture of the application being 
deployed, the granularity of deployment is one of the most important decisions that you 
can make and significantly influences the overall system structure. In choosing one 
or more elements of the system to refine, you are choosing a granularity. Do you want 
to deploy big-bang style, where the entire system is a monolith that is redeployed each 
time? Do you want to make each function its own microservice? Or do you want some-
thing in between? This decision can be tricky, as it may be affected by multiple types of 
requirements and concerns. If you are working in a highly regulated domain, perhaps 
deploying safety-critical software, then you will likely opt for few releases per year and 
these releases will have a low granularity; perhaps you will release a single monolith 
each time. If you are in a highly competitive, fast-moving, minimally regulated business 
area (say, e-commerce or entertainment), then you might choose a much finer granular-
ity so that individual features can be deployed or redeployed with little impact on the 
rest of the system. As you increase the granularity, the complexity of your deployment 
infrastructure will necessarily increase, and you will typically want most or all of the 
deployment process to be automated.

6.4  Summary  129
Step 4. Choose one or more design concepts that satisfy the drivers
	
Having chosen one or more drivers and a granularity, you now need to think about which 
design concepts will satisfy those requirements. At this point you will be selecting and 
tailoring tactics for deployability as well as patterns for deploying services, patterns for 
partial replacement of services, or patterns for complete replacement of services.
Step 5. Instantiate architectural elements, allocate responsibilities, and define interfaces
	
As with any design choice, the design concepts chosen in step 4 now need to be instanti-
ated. The instantiation decisions will be of several forms. For example, if you have cho-
sen microservices, you need to decide how many microservices, how they are packaged, 
and where they are deployed to (e.g., embedded device, cloud infrastructure, hosted 
servers). And if you wish to automate your deployments, then you will need an automa-
tion tool, such as Jenkins or GitLab.
Step 6. Sketch views and record design decisions
	
Deployment decisions have enormous impacts on maintainability, testability, availability, 
security, and many other key aspects of system success. So, these decisions and the ratio-
nale for them should be documented. All high-maturity DevOps projects include a way 
to capture how their systems are deployed and to keep track of the state of the deploy-
ment, possibly in a configuration management database (CMDB). Deployment diagrams 
could be captured using UML, but are more commonly represented using informal 
notations such as cloud provider icons. Of course, your deployment scripts are software, 
too—so they also deserve to be carefully designed, as they may outlive individuals and 
may become quite complex over time. Each component or service should have a deploy-
ment process. This process must define at least the following elements:
■
■Which artifacts are being deployed
■
■Which action is being done to initiate the deployment
■
■Which metrics should be monitored during the deployment (to spot problems and to 
potentially stop the deployment)
■
■How to stop the deployment
■
■How to roll the deployment back
Step 7. Perform analysis of the current design, and review the iteration goal and  
achievement of design purpose
	
Finally, your deployment decisions, because they have profound implications for many 
important system qualities, should be scrutinized carefully. This might take the form of 
design reviews or code reviews (of the deployment scripts). Any risks or issues discov-
ered as a result of these reviews should be added to the backlog.
6.4  Summary
In this chapter, we described the quality attribute of deployability and explored how you can 
design for deployability in a disciplined, repeatable way. This involves two major sets of deci-
sions: a set of decisions about how to structure your system (e.g., monolith, microservices) and 

130  Chapter 6  Designing for Deployability
a set of decisions about how to structure your deployment infrastructure (primarily achieved 
through the design of a deployment pipeline and patterns for all-or-nothing deployment or  
partial deployment). Deployability, along with API-centric design, forms the foundation of 
business agility today.
6.5  Further Reading
This technical report contains many of the ideas about patterns found in this chapter: R. 
Kazman, S. Echeverria, and J. Ivers, “Extensibility”, CMU/SEI-2022-TR-002, 2022.
This book contains a chapter devoted to the quality attribute of deployability: L. Bass,  
P. Clements, and R. Kazman, Software Architecture in Practice, 4th ed., Addison-Wesley, 2021.
A nice discussion of the principles behind microservice design, and many other  
related topics including an extensive catalog of design patterns, can be found at:  
https://microservices.io/.
The drivers for granularity disintegration and integration come from N. Ford, M. Richards, 
P. Sadalage, and Z. Dehghani, Software Architecture: The Hard Parts, O’Reilly, 2021.
For a slightly different take on deployability, this book contains a chapter devoted to the 
problems of deployment debt, focusing on how to identify it, how to manage it, and how to 
avoid it: N. Ernst, J. Delange, and R. Kazman, Technical Debt in Practice: How to Find It 
and Fix It, MIT Press, 2021.
This book was an early treatment of the discipline of continuous engineering as that 
field was emerging and becoming mainstream: Jez Humble and David Farley, Continuous 
Delivery: Reliable Software Releases through Build, Test, and Deployment Automation, 
Addison-Wesley, 2010.
A discussion of the architectural implications of DevOps can be found in L. Bass,  
I. Weber, and L. Zhu, DevOps: A Software Architect’s Perspective, Addison-Wesley, 2015.
6.6  Discussion Questions
1.	
In Section 6.1 we mentioned that to design for deployability, decisions need to be made 
about (1) the architecture of the system being deployed and (2) the architecture of the 
deployment infrastructure. Can ADD be used to design the deployment infrastructure?  
If so, how would this differ from using ADD to design the system itself?
2.	
Name three kinds of systems for which continuous delivery is a bad idea. Which of the 
tactics and patterns would you not use for such systems and why?
3.	
In Section 6.1.3, we claimed that an architect worrying about deployability needs to 
be concerned with the degree to which an architecture is granular, controllable, and 

6.6  Discussion Questions  131
efficient. How would you go about determining the “right” granularity for the pieces of 
an architecture that you wished to deploy?
4.	
Which business drivers would lead you to choose a blue/green deployment pattern versus 
a rolling upgrade deployment pattern? What are the pros and cons of each approach?
5.	
Which business drivers would lead you to choose canary testing versus A/B testing? 
What are the pros and cons of each approach?
6.	
What challenges arise when replicating a microservices architecture across pre- 
production environments? Consider that there may be many dependencies, on either 
third-party applications or infrastructure resources.

This page intentionally left blank 

133
7
Designing Cloud-Based 
Solutions
In this book, we talk about software architecture and how it allows drivers and quality  
attributes to be satisfied. For many software-intensive systems, certain quality attributes can-
not be satisfied purely through software design decisions. They can only be satisfied through 
combinations of software and infrastructure design decisions. In this chapter, we discuss how 
designing the infrastructure—and more specifically, a cloud infrastructure—is also an essen-
tial part of the design of the architecture. 
Why Read This Chapter?
Having read this chapter, you should not expect to become an expert on cloud-based 
architectural solutions, as this is an extensive set of technologies, and its precise details 
are covered elsewhere. Instead, we hope you will understand the types of drivers and 
the spectrum of design concepts associated with cloud development. Here we focus on 
how these design concepts can be employed in the context of ADD. As our case studies 
both employ cloud-based infrastructures, a knowledge of these design concepts is key 
to understanding them.
7.1  Introduction to the Cloud
The term cloud computing was first defined in 1997, but it was not until 2006 that a cloud 
provider started offering resources to the public. Since then, cloud adoption has become 
widespread.

134  Chapter 7  Designing Cloud-Based Solutions
7.1.1  What Is Cloud Computing?
Cloud computing allows computer system resources to be used without the need to manage 
hardware. Prior to the emergence of the cloud, setting up an infrastructure typically meant 
interacting with physical hardware, which might involve considerable effort. Today, setting 
up a cloud infrastructure means selecting and configuring virtual hardware resources without 
having to worry about physical hardware.
Several different deployment models for cloud computing are possible: private, public, and 
hybrid. A private cloud consists of an infrastructure that is owned and managed by an organi-
zation for internal usage. Private clouds are typically used to comply with security and privacy 
regulations or to leverage existing infrastructure. A public cloud consists of an infrastructure 
in which resources can be consumed by any customer. Hybrid clouds combine private and one 
or more public cloud resources. In this chapter, we focus primarily on resources offered by 
public cloud providers such as Amazon (AWS), Google (GCP), and Microsoft (Azure).
The cloud has been adopted widely because consuming virtual resources, rather than 
actually buying and installing physical hardware, offers many benefits. Some of the benefits 
are the following:
■
■Having your own dedicated spaces to host hardware infrastructure is costly, but is no 
longer necessary with the cloud.
■
■Resources can be obtained and paid for only when they are needed (pay-per-use), and 
can be scaled rapidly in a manual or automatic way (elastic scaling).
■
■Replicating the infrastructure to support the different deployment environments  
(e.g., integration, staging, production; see Section 6.1.1) can be achieved in a simpler way.
■
■Reconfiguring the infrastructure becomes simple, as compared with a physical 
reconfiguration.
■
■Cloud providers employ experts in areas such as security and physical infrastructure that 
small companies could not afford if they were to have their systems hosted on premises.
■
■When using resources managed by the cloud provider, updating the hardware or the 
operating system is no longer the user’s responsibility (or worry).
■
■Outsourcing infrastructure needs enables a business to focus resources and investments 
on key differentiators.
7.1.2  Service Models
Cloud providers offer different models of resource provision, which are commonly called  
service models. Each one is characterized by the amount and type of resources that are  
managed by the customer. Notably, each of these models builds upon the previous one:
■
■Infrastructure as a Service (IaaS): In this model, the cloud provider manages aspects 
such as virtualization, physical servers, storage, and networking. The customer obtains 
access to the virtual hardware and is responsible for managing many aspects, such as the 
operating system and everything that is executed on top of it, including runtime environ-
ments, middleware, data, and applications.

7.1  Introduction to the Cloud  135
■
■Platform as a Service (PaaS): In this model, the cloud provider manages the infrastruc-
ture as well as additional aspects including the operating system, runtime environments, 
and middleware. The customer obtains access to the platform, but still manages their 
own data and applications. These environments can handle concerns such as elastic  
scaling, fault tolerance, and health monitoring.
■
■Function as a Service (FaaS): In this model, the cloud manages most aspects and pro-
vides environments where functions (but not whole applications, as in PaaS), developed 
by the customer, are executed. This model is considered a variant of serverless comput-
ing, as billing accounts for only the actual time during which the functions are executed. 
As in PaaS, the execution environment supports aspects such as elastic scaling and fault 
tolerance.
■
■Software as a Service (SaaS): In this model, the cloud provider manages all aspects, 
including applications, and the customer pays to use the application. An application can 
be accessed from its user interface or can be integrated and used by calling its APIs. 
Third parties may offer applications as SaaS offerings; the application is managed by the 
third party but is deployed on a cloud provider’s infrastructure.
Each of these models of resource provision involves differing levels of costs, effort that 
the resource consumer must devote to managing the resources, and dependence on the cloud 
provider (vendor lock-in). Any given system may combine several of these approaches. For 
instance, some parts of the system might be implemented using FaaS capabilities, and other 
parts might use SaaS and PaaS capabilities.
7.1.3  Managed Resources
Resources that are managed by the cloud provider (we will refer to them as managed resources) 
do not require the consumer to launch a virtual machine or to install software associated with 
the desired resource such as a database engine or a message queue. Managed resources simply 
need to be selected and instantiated in the cloud environment. Some of these capabilities exist 
across cloud providers, such as a MySQL database engine, while others are specific to the pro-
vider, such as a user management capability.
Managed resources provide a variety of benefits, including the fact that they can be 
launched in a simple manner and do not need to be updated manually. Also, they are well inte-
grated with other tools from the cloud provider that support capabilities such as monitoring, 
logging, security, and backups. Furthermore, every cloud provider includes mechanisms that 
allow these resources to be scaled elastically to support performance needs, and replicated to 
support availability. Note that the software that runs on elastic managed resources must be 
designed to support this capability (e.g., by considering aspects such as statelessness).
However, using managed resources also has some downsides. The most important are 
that the application is tied to the particular cloud provider, which limits the ability to move it to 
a different provider (vendor lock-in), and that these managed resources are typically costlier, in 
terms of billing, than using open source solutions directly on virtual machines. Furthermore, 
managed resources may be limited to certain options only. For example, a relational database 

136  Chapter 7  Designing Cloud-Based Solutions
resource may be instantiated from only a limited selection of database engine types and  
versions, although these tend to be the most recent and popular.
Cloud providers expose APIs that allow resources to be managed and provisioned by 
interacting with the API. This approach enables the infrastructure to be treated as code, as 
resources can be managed and provisioned through definition files that are processed by a 
computer, instead of having to configure them manually. Referred to as infrastructure as code, 
this model facilitates, for example, the replication of the infrastructure across the staging and 
production environments.
7.2  Drivers and the Cloud
In this section, we describe the architectural drivers—more specifically, the quality attributes 
and constraints—that have a direct relationship with the use of cloud infrastructures.
7.2.1 Quality Attributes
Certain quality attributes cannot be satisfied purely through software decisions. Here, we 
describe the most common quality attributes that require a combination of software and infra-
structure decisions. In Section 7.3.1, we explore the capabilities offered by cloud providers in 
more detail. In Section 7.3.2, we describe how tactics associated with these quality attributes 
can be realized by these capabilities.
7.2.1.1  Performance and Scalability
The cloud facilitates the modification of compute resources to increase factors such as CPU, 
memory, or storage to achieve vertical scaling as well as the replication of resources to achieve 
horizontal scaling. Scaling can be performed manually or automatically (the latter is called 
elastic scaling), depending on the model of resource provision. Furthermore, resources can 
be placed in close geographic proximity or even in the same data center to reduce latency in 
communications (see Section 7.3.1.1).
7.2.1.2  Availability
One of the key decisions to support availability focuses on the replication of resources. Cloud 
infrastructure facilitates the replication of resources as well as their instantiation in different 
geographic locations, as a means of avoiding failures due to natural disasters. Cloud provid-
ers also supply tools that facilitate the monitoring of resources so that remediation actions 
can be taken in case of failures. Of course, there is inevitably a tradeoff between distributing 
resources across geographic locations to increase availability and performance.

7.2  Drivers and the Cloud  137
7.2.1.3  Security
Security is a quality attribute that requires many measures to be taken at the infrastructure 
level, and often these measures can be quite complex to create and maintain. Cloud providers 
typically offer mechanisms to create virtual private networks and also offer capabilities such 
as firewalls, authorization and authentication servers, distributed denial-of-service (DDoS) 
attack protection, gateways, and many others to address this quality attribute.
7.2.1.4  Deployability
Deployability, as we saw in Chapter 6, is the capability of being able to deliver new versions of 
a system or system component continuously, predictably, and with minimal effort and human 
intervention. Delivery involves deploying the code across the different pre-production envi-
ronments until it is ultimately deployed into the final production environment. Cloud infra-
structures provide capabilities to set up and execute deployment pipelines, to replace services 
partially or completely, and to roll back deployments if problems arise.
7.2.1.5  Operability
Operability is the ability to keep a system in a safe and reliable functioning condition, accord-
ing to predefined operational requirements. An important part of this quality attribute is the 
capacity of the system to support observability and monitoring, to detect and react when prob-
lems occur. Cloud providers offer capabilities to monitor resources so that they can operate 
adequately.
7.2.2  Constraints
In the following subsections, we discuss important constraints associated with the use of cloud 
infrastructures.
7.2.2.1  Cost Optimization
Using cloud resources has an associated cost. Each additional capability that is used con-
tributes to the cost of the infrastructure. Furthermore, the characteristics of the capabilities 
contribute to the overall cost (e.g., CPU, memory and storage sizes, managed or not, and the 
pay-per-use model).
Cost is always a constraint in development, so it is important to be able to estimate how 
much a solution will cost and to evaluate the costs of alternative solutions. Cloud providers 
offer calculators (Figure 7.1) that allow costs to be estimated for a given infrastructure. These 
tools can be extremely useful during the design process to help make decisions that optimize 
costs.
Once deployed, the actual costs of operation need to be monitored and optimized if nec-
essary. Some measures can be taken to reduce costs, such as shutting down integration envi-
ronments on the weekends or using less expensive resources than the ones used in the staging 
or production environments.

138  Chapter 7  Designing Cloud-Based Solutions
FIGURE 7.1  Cloud provider cost estimation calculator
7.2.2.2  Cloud Native and Vendor Lock-in
In the last decade, many companies have started to migrate their enterprise IT infrastructure to 
the cloud. This can be a major effort, as many of the applications that are used by the company 
may have been designed in an era when the cloud did not exist or was not so prevalent. One 
relatively simple approach for moving to the cloud is “lift and shift”, which involves moving 
an on-premises application to the cloud, without making significant changes to the application. 
This approach is usually associated with the IaaS model of resource provision, as it typically 
involves obtaining cloud infrastructure resources similar to the ones used on-premises. “Lift 
and shift” is a relatively simple strategy, but it does not make as effective use of the cloud 
resources as possible. Even so, it can be a helpful first step in moving to the cloud.
A different approach involves rewriting parts or the whole of the application to make 
effective use of cloud resources. This results in a “cloud native” application. Cloud native 
applications are built using a cloud-specific approach; that is, they use managed resources 
or a platform specific to a cloud provider. Choosing a cloud native strategy can be a project 
constraint that simplifies the choices of resources, as only the resources offered by the cloud 
provider need to be considered.

7.3  Cloud-Based Design Concepts  139
An important constraint may be the desire to avoid vendor lock-in by following a cloud- 
agnostic approach. The customer may require that the system is not tied to a particular cloud 
provider and that it can easily be moved to a different provider if necessary. This constraint 
limits the kinds of resources that can be consumed and the models of resource provision that 
can be employed. When this constraint exists, the typical solution is to avoid vendor-specific 
managed resources, which may be replaced by open source products deployed on the cloud, or 
to use IaaS as the model of resource provision.
7.2.2.3  Legal Considerations
The data centers where the cloud resources reside are likely to be scattered across the globe. 
However, there may not be a data center for a given cloud provider in every country. This 
means that your application and its data may not always reside in your country. This may vio-
late legal requirements about sensitive information from one country being hosted in a differ-
ent one or being hosted by a different company.
Furthermore, some mistrust may arise regarding the handling of information and security 
measures by cloud providers, which further limits the desirability of hosting an application in 
a public cloud.
7.3  Cloud-Based Design Concepts
The design concepts used in the construction of cloud-based solutions include a number of 
capabilities along with tactics and patterns realized by these capabilities. In this section, we 
present these design concepts.
7.3.1  Externally Developed Components: Cloud Capabilities
While a variety of cloud providers exist, most of them offer a similar set of fundamental cloud 
capabilities. Cloud capabilities can therefore be considered as one type of externally developed 
component. The most common capabilities are presented in the following subsections.
7.3.1.1  Regional Capabilities
As noted earlier, cloud resources are hosted in data centers that may be scattered around the 
globe. Cloud providers typically organize resources hierarchically as follows:
■
■Regions: These are separate geographic areas, typically grouping one or more data  
centers. An example of a region could be the western United States.
■
■Availability zones: These typically correspond to specific data centers within a region. 
Applications can be partitioned across availability zones so that a failure in a data center 
does not result in a failure of the application.
■
■Localized capabilities: These correspond to resources that are hosted in geographic 
locations that are close to the users of those resources—for example, on the edge of a 
telecommunication carrier’s networks or even at on-premises locations.

140  Chapter 7  Designing Cloud-Based Solutions
7.3.1.2  Computing Capabilities
The most essential capability that is supplied by cloud providers is a set of computing resources:
■
■Virtual machines: These virtual servers in the cloud are configured with a particular 
operating system and hardware configuration (e.g., memory, CPUs, hard drive capacity). 
They are typically associated with the IaaS resource provision model.
■
■Web application execution environments: These are execution environments for  
applications written in different programming languages. Configuration options typi-
cally include the platform (e.g., Java or .NET) and some specific options for the given 
platform. These environments are typically associated with the PaaS resource provision 
model.
■
■Serverless execution environments: These are execution environments for functions that 
are executed as a service. As opposed to virtual machines, these environments do not 
require a particular hardware configuration to be guaranteed.
■
■Container execution environments: These are execution environments for containers 
(which include container orchestrators). The container image specifies the operating 
system, and certain hardware configurations can be established, such as the amount of 
CPU and memory that are provided to the container. These environments can support 
concerns such as automatic provisioning, scaling, and health monitoring.
Some of these execution environments—specifically, those that are not IaaS—support 
automatic scaling and other features such as integration with security and logging capabilities.
7.3.1.3  Networking Capabilities
Cloud providers also supply networking capabilities that are required to connect other  
capabilities such as computing and storage. Examples of these networking capabilities include 
the following:
■
■Virtual private networks (VPNs): Cloud capabilities used in the application reside in 
these virtual networks. A VPN can be associated with a region. Subnets are ranges of 
IP addresses that are created inside VPNs and that can be associated with availability 
zones. Subnets can be public (for resources that are connected to the Internet) or private 
(for resources that are not connected to the Internet).
■
■Domain name servers (DNS): Resources that are created inside VPNs are associated 
with names that are resolved using these servers.
■
■Gateways: These connect VPNs to other networks, including the Internet. API gateways 
(see Section 5.2.4.4) are a specific type of gateway that allows APIs to be published, 
secured, monitored, and maintained.
■
■Content delivery networks (CDNs): These servers are distributed geographically; they 
help deliver data to users with reduced latency by delivering the data from a server that 
is close and well connected to the end user.
■
■Service meshes: This infrastructure layer manages communication between microser-
vices. The service mesh introduces proxies called sidecars that handle aspects such as 
communication or monitoring.

7.3  Cloud-Based Design Concepts  141
7.3.1.4  Storage Capabilities
Cloud providers supply capabilities that allow information to be stored inside file systems. 
Examples of these capabilities include the following:
■
■Instance storage: This storage capability is associated with a virtual machine, similar to 
a hard drive for the virtual machine.
■
■Network file systems (NFS): These managed file systems can be shared across multiple 
instances and are able to scale on demand.
■
■Object storage: This scalable mechanism stores objects, albeit not hierarchically  
(contrast this approach to file systems).
■
■Backup services: These services create and manage backups for different resources.
7.3.1.5  Database Capabilities
Information storage inside databases is another important capability that is universally  
provided. Examples of these capabilities include the following:
■
■Managed relational databases: These relational databases are managed by the cloud 
provider, using different engines (such as MySQL or PostgreSQL). Hardware, availabil-
ity, reliability, backups, security, and monitoring characteristics can be configured.
■
■Managed NoSQL databases: These nonrelational databases come in several different 
varieties: Document, Key-Value, Graph, Column, and Time series.
■
■In-memory caches and databases: These are in-memory data stores and cache services.
7.3.1.6  Development and DevOps Capabilities
Cloud providers also supply capabilities to support software development. These range from 
code repositories to continuous integration and continuous deployment or delivery (CI/CD) 
pipelines that facilitate DevOps. Examples of these capabilities include the following:
■
■Code repositories: These are managed version control systems, most commonly Git 
repositories.
■
■Artifact repositories: These managed repositories for artifacts facilitate the storage, 
publication, and sharing of software packages.
■
■Pipelines: These managed, continuous integration and deployment/delivery servers  
automate the build, test, and release process.
■
■Deployment services: These services manage deployments to different environments.
■
■Analysis and debugging tools: These tools facilitate analysis and debugging of  
distributed systems—for example, using distributed tracing.
■
■Infrastructure as code services: These services allow physical architectures to be 
described using a domain-specific language. The descriptions can be used to easily set 
up the infrastructure across environments, without the need to instantiate and configure 
all of the cloud resources manually.
■
■Monitoring services: These services collect data from a variety of sources, including 
logs and events. They also provide mechanisms to visualize and analyze logged data 
using dashboards, and to set up alarms and actions based on thresholds.

142  Chapter 7  Designing Cloud-Based Solutions
7.3.1.7  Security Capabilities
Security is a fundamental quality attribute that requires design decisions to be made at  
both the software and the hardware level. At the hardware level, cloud providers supply many 
security capabilities, such as the following:
■
■Authentication and authorization services: These services allow users to be managed 
and provide access tokens to grant access.
■
■Firewalls: These network protection mechanisms are deployed to protect VPNs.
■
■Certificate managers: These services allow SSL/TLS certificates to be provisioned, 
managed, and deployed.
■
■Secret managers: These services allow sensitive information (e.g., passwords) to be 
managed.
■
■Threat detection services: These services identify threats such as malicious usage of 
compute capabilities or suspicious activity in storage capabilities.
■
■DDoS protection services: These services are specifically designed to protect against 
this type of attack.
7.3.1.8  Application Integration Capabilities
Application integration is essential to support the construction of distributed systems and to 
allow different applications to communicate with each other. Cloud providers offer several 
services to support application integration, such as the following:
■
■Message queues: These services allow message brokers to be provisioned and operated.
■
■Notification services: These services allow publisher/subscriber messaging mechanisms 
to be provisioned and operated.
■
■Event streaming services: These services allow event streaming platforms to be  
provisioned and operated.
7.3.1.9  Analytics Capabilities
Operational data is the data that supports the day-to-day operation of the business; it is  
typically stored in databases. Analytical data, which is often derived from operational data, 
has a different purpose: it is used to support business intelligence to make long-term strategic 
decisions and give direction. Some capabilities that support analytics are:
■
■Data lakes: These centralized repositories store raw structured and unstructured data. 
Data is ingested and cleansed before being stored securely.
■
■Data warehouses: These central repositories contain curated structured information that 
is used for analysis. Data is received from databases, transactional systems, and other 
sources and stored securely.
■
■Search and query services: These services facilitate performing searches and queries on 
large volumes of information.

7.3  Cloud-Based Design Concepts  143
7.3.1.10  Additional Capabilities
Cloud providers offer additional capabilities that support common development needs.  
The following are some of the most popular options:
■
■Machine learning capabilities: These services facilitate the construction, training, and 
deployment of machine learning models.
■
■Blockchain capabilities: These services allow blockchains to be created and managed.
■
■Internet of Things (IoT) capabilities: These services support the development of IoT 
applications, including deployment and execution of code on devices, connection of 
devices to the cloud, and device management.
■
■Game development: These services facilitate the development of games, specifically 
large multi-player games.
7.3.2  Tactics
In this section, we review the tactics that are associated with the quality attributes presented in 
Section 7.2.1 and show how they can be achieved using the capabilities discussed previously.
7.3.2.1  Performance and Scalability
Some performance tactics (see Section 3.3.1) that are supported by cloud capabilities fall 
within the resource management group. They include the following tactics:
■
■Maintain multiple copies of data/Maintain multiple copies of computation: Cloud 
capabilities such as virtual machines and databases can easily be replicated and capabil-
ities such as load balancers can be used to distribute requests across replicas. Managed 
databases typically handle replication of data automatically.
■
■Increase Resources: Several cloud capabilities can be scaled both horizontally and  
vertically. For example, a virtual machine with a given CPU and memory configuration 
can easily be changed to a more powerful CPU with a larger memory allocation.
7.3.2.2  Availability
Availability tactics (see Section 3.4.1) that are supported by cloud capabilities include the 
following:
■
■Condition monitoring: Certain cloud capabilities can be monitored by the cloud  
infrastructure—for example, a container execution environment. In case of failures, a 
new container instance can be launched automatically.
■
■Redundant spare: Replicating capabilities across availability zones or geographic 
regions supports the realization of this tactic. The cloud infrastructure provides  
mechanisms to ensure that the replicas maintain consistency (e.g., in databases).
■
■Software upgrade: The cloud infrastructure allows non-service-affecting upgrades to 
be performed. New compute capabilities with updated versions of the software can be 
launched while previous versions continue to process requests. Once the updated  
versions are running, traffic can be directed to them.

144  Chapter 7  Designing Cloud-Based Solutions
■
■Reintroduction: Reintroducing faulty components is also facilitated by the cloud  
infrastructure. For example, launching a container after a failure is relatively easy and 
can be handled by the execution environment.
■
■Rollback: Certain types of rollbacks, such as deployments, can be handled by the cloud 
infrastructure.
7.3.2.3  Security
Security is a quality attribute that is extensively supported by cloud capabilities. Security 
tactics (see Section 3.6.1) that can be partially or fully implemented using cloud capabilities 
include the following:
■
■Detect intrusion: Cloud providers offer services to detect intrusions and malicious usage 
of resources.
■
■Detect service denial: DDoS protection services are specifically designed to guard 
against this type of attack.
■
■Authenticate actors: Authentication and authorization services support the implementa-
tion of this tactic.
■
■Limit access: Limiting access to resources can be achieved through mechanisms such as 
the use of Virtual Private Clouds (VPC)s or API gateways.
■
■Encrypt data: Capabilities such as databases offer the possibility of encrypting data at 
rest, while data that is being transferred can also be encrypted by using protocols such as 
TLS.
■
■Inform actors: Notification mechanisms support informing actors when malicious  
activity is detected by capabilities such as threat detection services.
■
■Audit: Monitoring and storage capabilities support the collection, storage, and further 
analysis of information including audit logs.
7.3.2.4  Deployability
Deployability tactics (see Section 6.2.1) include tactics related to managing a deployment pipe-
line and managing the deployed system. Cloud providers offer a number of capabilities that 
allow the deployment pipeline to be managed.
■
■Scale rollouts: Cloud providers offer services that support the capability of scaling 
rollouts.
■
■Script deployment commands: Infrastructure as code capabilities are offered by cloud 
providers, either through proprietary mechanisms or by supporting popular open source 
solutions.
■
■Rollback: As with scaling rollouts, services support rollbacks of deployments.
7.3.2.5  Operability
While there is not a catalog of tactics that support operability, aspects such as monitoring and 
observability are supported by capabilities offered by cloud providers. Monitoring services, 

7.3  Cloud-Based Design Concepts  145
analysis and debugging tools, and dashboards allow the collection of operational information, 
including logs, and its visualization to support health analysis. Alarms and notifications can be 
configured to identify problematic situations. The information that is collected is also essential 
for optimizing costs by identifying, for example, unused or underused resources. Supporting 
observability requires certain design decisions to be made in the software architecture—for 
example, the integration of libraries that allow metrics to be collected.
7.3.3  Patterns
Numerous resources provide information on patterns associated with cloud-based development.
7.3.3.1  Reference Architectures
Reference architectures provide blueprints for developing specific types of applications. Some 
of them specifically employ cloud capabilities to solve development and operation problems. 
Reference architectures for cloud-based applications are most commonly associated with a 
specific cloud provider’s capabilities, but can easily be translated to a different cloud provider 
with equivalent capabilities. Figure 7.2 illustrates an example of a typical microservices appli-
cation on AWS.
VPC
AWS Cloud
Cognito
Authorization Service
CloudFront
(Content Delivery)
API Gateway
Application Load
Balancer
Relational Database
Service
Fargate
(Container Exection)
Devices
FIGURE 7.2  A simple microservice application on AWS
7.3.3.2  Design Patterns
Many patterns can be employed to help achieve the quality attributes supported by the use of 
the cloud—performance, scalability, availability, and security, among others.

146  Chapter 7  Designing Cloud-Based Solutions
An example of these patterns is Command and Query Responsibility Segregation (CQRS). 
In this pattern, the update (command) and read (query) operations are separated and handled 
by different components. Changes that result from updates are propagated from the command 
side to the query side using mechanisms such as events. The command and query components 
can use different data stores (e.g., a relational database on the command side and a docu-
ment database on the query side). Furthermore, the command and query sides can be scaled 
independently. This pattern promotes availability, performance, and security. Availability is 
enhanced because a failure on the command side does not affect the query side, and vice 
versa. Performance is enhanced because the query side can receive data from multiple sources, 
allowing data from different system or domain boundaries to be queried efficiently. Also, the 
query side can be scaled independently from the command side, and querying the data does 
not affect the performance of the command side. Finally, security is enhanced because access 
to the command side can be restricted and clients that have access to the query side cannot per-
form updates. The disadvantage of the CQRS pattern is that its implementation is associated 
with some complexity; also, the separation of the databases brings up consistency concerns.
Although the CQRS pattern is cloud provider agnostic, it can be implemented using capa-
bilities from a specific cloud provider, as illustrated in Figure 7.3.
AWS Cloud
API
Gateway
Command-side
Container running
on EKS
Query-side
Container running
on EKS
Document DB
with Mongo
Compatibility
Relational
Database Service
Managed Kafka
Service
Commands
Queries
Events
Events
Write
Read
Client
FIGURE 7.3  Implementation of CQRS using AWS capabilities

7.4  ADD in the Design of Cloud-Based Solutions  147
Other resources, including several books, offer more details on design patterns for cloud 
computing. Please see the “Further Reading” section of this chapter for additional references.
7.4  ADD in the Design of Cloud-Based Solutions
In this section, we discuss each of the ADD steps in relation to the use of a cloud infrastructure.
Step 1. Review inputs
In this step, it is essential to pay attention to the constraints associated with the use of 
a cloud infrastructure. These constraints, which may be defined by an organization’s 
enterprise architecture team, may mandate the use of resources from a particular pro-
vider, or the need to avoid vendor lock-in, or legal restrictions that limit the geographic 
region where the application and its data can reside. Also, clarity with respect to budget 
constraints for implementation and operation is essential, as this consideration may guide 
design decisions along the way.
Step 2. Establish the iteration goal by selecting drivers
One common early iteration goal is to design the infrastructure of the application; in this 
case, the drivers selected should be quality attributes that must be satisfied through combi-
nations of software and infrastructure decisions as well as previously identified constraints.
In addition, as products are continuously evolved, changes in the existing infrastructure 
may be required. In this case, the iteration goal may be to refine a previously defined 
infrastructure.
Step 3. Choose one or more elements of the system to refine
The elements to refine are typically software elements, and the refinement involves the iden-
tification of infrastructure elements where these software elements will reside. For example, a 
previous iteration may have produced a structure that involves microservices. In this iteration, 
these microservices are identified and the refinement will involve the selection of capabilities 
where these microservices will be executed and where data will be stored.
If the iteration goal is to refine a previously defined infrastructure, the elements that are 
selected can be infrastructure components that were identified in this previous iteration.
Step 4. Choose one or more design concepts that satisfy the drivers
During initial iterations, design decisions may involve choosing the type of cloud (public, 
private, hybrid), selecting the resource provision model (or models) that can be used (e.g., 
IaaS, PaaS), and selecting a reference architecture. These decisions can be guided by the 
approach used to structure services (e.g., monolith, microservices), the workload that is 
expected in the application, the type of data that needs to be managed, and how these 
are expected to scale in the future. Subsequent iterations will involve the selection of 
capabilities that align with the decisions made previously.
Design concepts selected at the infrastructure level must be aligned with the decisions 
made at the software level. For example, selecting a managed elastic compute capability 
requires that the software that runs on these compute capabilities can be replicated (by 
decoupling the application state from the life cycle of the compute resources, for example).

148  Chapter 7  Designing Cloud-Based Solutions
One benefit of using the cloud is that it is easy to instantiate capabilities to perform 
experiments and prototypes. These activities can be useful in the selection process. This 
process will usually be constrained by cost limitations; in such a case, it can be helpful 
to make use of cost calculators to guide decision-making.
Step 5. Instantiate architectural elements, allocate responsibilities, and define 
interfaces
The instantiation of cloud capabilities involves deciding on aspects that are specific to 
a provider’s configurations. For example, for a database capability, instantiation may 
involve the selection of a database engine (e.g., MySQL or Postgres) and the selection of 
values for its configuration, such as the size of the database (e.g., small, medium, or large 
instance) and configurations to support quality attributes such as security or availability.
Step 6. Sketch views and record design decisions
Cloud infrastructure diagrams are usually represented using informal notations that 
employ cloud provider-specific icons, like those shown in Figure 7.2. Design decisions 
may be recorded separately using a template for architectural decisions.
Step 7. Perform analysis of the current design, and review the iteration goal and 
achievement of design purpose
As is usual in ADD, the last step of the process involves analyzing whether the design 
decisions that have been made up to this point are sufficient. This analysis may lead to 
further iterations to refine the design.
7.5  Summary
In this chapter, we discussed the design of solutions that involve an infrastructure hosted 
in a public cloud. Today, cloud providers offer a wide assortment of capabilities that can be 
extremely useful in accelerating development and satisfying quality attributes that involve 
software and infrastructure decisions.
Using the cloud offers many benefits, as its widespread adoption attests. However, design 
decisions when using a cloud infrastructure must be carefully made because of their associated 
costs and because the use of the cloud may not be acceptable in all contexts.
7.6  Further Reading
Cloud provider websites provide a wealth of information regarding the capabilities they offer. 
Major cloud provider sites include:
■
■Amazon Web Services: https://aws.amazon.com/
■
■Google Cloud: https://cloud.google.com/
■
■Microsoft Azure: https://azure.microsoft.com/en-us/

7.7  Discussion Questions  149
An online catalog of cloud design patterns is available at https://learn.microsoft.com/
en-us/azure/architecture/patterns/. This catalog is hosted by Microsoft, but the patterns are 
independent from any specific provider.
This book describes many important concepts associated with the development of appli-
cations that are designed as collections of microservices hosted in the cloud: K. Indrasiri and 
S. Suhothayan, Design Patterns for Cloud Native Applications, O’Reilly, 2021.
This book discusses many concerns that must be addressed in the construction of distrib-
uted architectures that are typically deployed in the cloud and provides guidance on the design 
decisions used to address them: N. Ford, M. Richards, P. Sadalage, and Z. Dehghani, Software 
Architecture: The Hard Parts: Modern Trade-Off Analyses for Distributed Architectures, 
O’Reilly, 2021.
7.7  Discussion Questions
1.	
Use of the IaaS resource provision model may lead to lower billing costs from the cloud 
provider. What other costs, not billed by the cloud provider, can be incurred by the 
resource customers when using this model?
2.	
When using the infrastructure as code model, the infrastructure can easily be replicated 
in the integration, test, and production environments. Should these three infrastructures 
be exactly the same?
3.	
What are some measures that can be taken to reduce costs when establishing different 
execution and test environments?
4.	
Can you think of a situation that involves (or requires) mixing resources from two or 
more cloud providers?
5.	
Which important architectural decisions are orthogonal to (i.e., unaffected by) the choice 
of cloud provider?
6.	
Which other quality attributes besides the ones discussed in this chapter can be 
addressed as combinations of software and infrastructure design decisions?
7.	
A company wants to move a monolithic web-based application to the cloud. What 
are some of the challenges it may face in trying to make optimal use of the cloud 
infrastructure?

This page intentionally left blank 

151
8
Case Study: Hotel  
Pricing System
In this chapter, we offer a case study in which ADD is used to design a greenfield system in 
a mature domain. This case study presents the early design rounds, which include four iter-
ations, and is based on a real-world example. We first describe the business context for the 
system, then summarize its requirements. This is followed by a step-by-step summary of the 
activities that are performed during the ADD iterations. 
Why Read This Chapter?
People learn best from examples. This chapter will help you to contextualize and  
visualize how the many concepts that have been previously introduced can be put 
together in an actual setting. We do this with a detailed example of addressing a  
real-world problem—a hotel pricing system—with an architectural solution.
8.1  Business Case
AD&D Hotels is a mid-sized business hotel chain (currently around 300 hotels) that has been 
experiencing robust growth in recent years. The IT infrastructure of the company is com-
posed of many different applications, such as a Property Management System, a Commercial 
Analysis System, an Enterprise Reservation System, and a Channel Management System. At 
the center of this system of systems is the Hotel Pricing System, as shown in the context dia-
gram in Figure 8.1. Although this system was already deployed in the cloud, the approach fol-
lowed was “lift and shift”; thus, this system was not making full use of cloud resources.

152  Chapter 8  Case Study: Hotel Pricing System 
Channel Management
System
Other Systems...
User Identity Service
Commercial Analysis
System
Property Management
System
Prices
Prices
Prices
Prices
User
credentials
System under development
External system
Data ﬂow
Price changes
End User
Hotel Pricing
System
FIGURE 8.1  Context diagram for the Hotel Pricing System
The Hotel Pricing System (HPS) is used by sales managers and commercial represen-
tatives to establish prices for rooms on specific dates for the different hotels in the company. 
Prices are associated with different rates (e.g., a public rate, a discount rate), and most of the 
prices for the different rates are calculated by taking a base rate and applying business rules 
to it (although some of the rates can also be fixed and not depend on the base rate). Managers 
typically change prices of the base and fixed rates. Then, using this information, the HPS 
calculates the prices of all the rates for all rooms in all hotels, which also vary according to 
the types of rooms available in each hotel. Prices that are calculated by the HPS are used by 
other systems in the company to make reservations, and are also sent to different online travel 
agencies through the Channel Management System (CMS). The company’s systems are hosted 
with a cloud provider that offers a user identity service that manages users and provides single 
sign-on functionalities.
AD&D Hotels wants to modernize its IT infrastructure. The first step will be the com-
plete replacement of the existing pricing system, which was developed several years ago and 
is suffering from reliability, performance, availability, and maintainability issues that have 
resulted in financial losses. Furthermore, the company has experienced difficulties because 
many of its systems are connected using traditional SOAP and REST request–response end-
points: Changes to one application frequently impact other applications and complicate the 
deployment of individual updates to specific applications. Also, the failure of an applica-
tion can propagate through the entire system. Furthermore, some of the applications interact 

8.2  System Requirements  153
using what are now well-known anti-patterns, such as integration through a shared database. 
Recently revised enterprise architecture principles within the organization mandate a migra-
tion of the system toward a more decoupled model. The remainder of this chapter describes the 
first three iterations of an architectural design for this system, created using ADD.
8.2  System Requirements
Requirement elicitation activities were performed earlier by the company. This section  
summarizes the most important requirements collected.
8.2.1  Primary Functionality
The HPS’s functionality is conceptually simple. The main user stories for this system are 
shown in the use case diagram in Figure 8.2.
Hotel Pricing System
HPS-1: Log In
HPS-2: Change Prices
HPS-3: Query Prices
HPS-4: Manage Hotels
HPS-5: Manage Rates
HPS-6: Manage Users
Channel
Management
System
External System
Key: UML
Administrator
Commercial
FIGURE 8.2  Initial use case diagram for the Hotel Pricing System
Each of these user cases is described in Table 8.1.

154  Chapter 8  Case Study: Hotel Pricing System 
TABLE 8.1  Use Cases for the Hotel Pricing System
Use Case
Description
HPS-1: Log In
A user (commercial or administrator) provides their credentials in a login 
window. The system checks these credentials against a user identity service 
and, if successful, provides access to the system. Once logged in, a user 
can only make queries and changes to the hotels for which they have been 
authorized.
HPS-2: Change 
Prices
A user selects a specific hotel for which they are authorized to change prices, 
and selects dates where they want to make price changes to either a base 
rate or a fixed rate. All of the prices for the rates that are calculated from the 
base rate are calculated at that point. The system allows price changes to be 
simulated before they are actually changed. When the prices are changed, 
they are pushed to the Channel Management System and become available 
for querying by external systems.
HPS-3: Query Prices
A user or an external system queries prices for a given hotel through the user 
interface or a query API.
HPS-4: Manage 
Hotels
An administrator adds, changes, or modifies hotel information. This includes 
editing the hotel’s tax rates, available rates, and room types.
HPS-5: Manage 
Rates 
An administrator adds, changes, or modifies rates. This includes defining the 
calculation business rules for the different rates.
HPS-6: Manage 
Users
An administrator changes permissions for a given user.
8.2.2  Quality Attribute Scenarios
In addition to these use cases, a number of quality attribute scenarios were elicited and doc-
umented. The seven most relevant and important ones are presented in Table 8.2. For each 
scenario, we also identify the use case that it is associated with. 
TABLE 8.2  Quality Attribute Scenarios for the Hotel Pricing System
ID
Quality 
Attribute
Scenario
Associated 
Use Case
QA-1
Performance
A base rate price is changed for a specific hotel and 
date during normal operation; the prices for all the rates 
and room types for the hotel are published (ready for 
query) in less than 100 ms.
HPS-2
QA-2
Reliability
A user performs multiple price changes on a given 
hotel; 100% of the price changes are published 
(available for query) successfully and are also received 
by the Channel Management System.
HPS-2
QA-3
Availability
Pricing queries uptime SLA must be 99.9% outside of 
maintenance windows.
All
QA-4
Scalability
The system will initially support a minimum of 100,000 
price queries per day through its API and should be 
capable of handling up to 1,000,000 without decreasing 
average latency by more than 20%.
HPS-3

8.2  System Requirements  155
ID
Quality 
Attribute
Scenario
Associated 
Use Case
QA-5
Security
A user logs into the system through the front-end. 
The credentials of the user are validated against 
the User Identity Service and, once logged in, they 
are presented with only the functions that they are 
authorized to use.
All
QA-6
Modifiability
Support for a price query endpoint with a different 
protocol than REST (e.g., gRPC) is added to the 
system. The new endpoint does not require changes to 
be made to the core components of the system.
All
QA-9
Testability
100% of the system and its elements should support 
integration testing independently of the external 
systems.
All
8.2.3  Constraints
Finally, a set of constraints on the system and its implementation were collected. These are 
presented in Table 8.3.
TABLE 8.3  Constraints for the Hotel Pricing System
ID
Constraint
CON-1
Users must interact with the system through a web browser in different platforms 
(Windows, OSX, and Linux, and different devices).
CON-4
The initial release of the system must be delivered in 6 months, but an initial version of 
the system (MVP) must be demonstrated to internal stakeholders in at most 2 months.
CON-5
The system must interact initially with existing systems through REST APIs but may 
need to later support other protocols.
8.2.4  Architectural Concerns
Since this is a greenfield development, only a few general concerns were identified initially 
and these are shown in Table 8.4.
TABLE 8.4  Architectural Concerns for the Hotel Pricing System
ID
Concern
CRN-1
Establish an overall initial system structure.
CRN-2
Leverage the team’s knowledge about Java technologies, the Angular framework, and 
Kafka.
CRN-3
Allocate work to members of the development team.
CRN-4
Avoid introducing technical debt (see Chapter 10).

156  Chapter 8  Case Study: Hotel Pricing System 
8.3  Development and Operations Requirements
As part of the modernization effort, AD&D Hotels wants to integrate agile (specifically 
Scrum) and DevOps practices into the development of the HPS. Besides the previously  
discussed system requirements, other development and operational requirements need to be 
taken into account.
Artifacts in the development process move through four different environments, as 
depicted in Figure 8.3. The different environments are the following:
Development
Integration
Staging
Production
FIGURE 8.3  Different environments during the development process
■
■Development: This is a local environment on the developers’ computers.
■
■Integration: This is an environment in the cloud where an integrated version of the HPS 
is tested. In this environment, the system is not connected with all of the external  
systems, so some of these external systems are substituted by mock-ups.
■
■Staging: This is an environment in the cloud where the system’s final tests (including 
load testing) are performed prior to deployment. Here the system is connected to test 
versions of all the external systems. At the end of a sprint, the system is typically demon-
strated from this environment.
■
■Production: This is the real-world execution environment.
The company is just starting to implement sophisticated deployability practices, so only 
support for continuous deployment to the integration and staging environments is currently 
required. These new requirements, specific to development and operations, result in additional 
constraints, quality attribute scenarios, and concerns, which are presented next.
8.3.1  Quality Attribute Scenarios
In addition to the quality attribute scenarios that are part of the initial set of system requirements, 
several scenarios result from development and operations considerations. These scenarios are 
captured in Table 8.5. 

8.3  Development and Operations Requirements  157
TABLE 8.5  Development and Operations Quality Attribute Scenarios for the Hotel Pricing 
System 
ID
Quality 
Attribute
Scenario
Associated 
Use Case
QA-7
Deployability
The application is moved between nonproduction 
environments as part of the development process.  
No changes in the code are needed.
All
QA-8
Monitorability
A system operator wishes to measure the performance 
and reliability of price publication during operation.  
The system provides a mechanism that allows 100% of 
these measures to be collected as needed.
HPS-2
8.3.2  Constraints
The additional constraints on the system and its implementation due to considerations of  
development and operations were collected. These are presented in Table 8.6.
TABLE 8.6  Development and Operations Constraints for the Hotel Pricing System 
ID
Constraint
CON-2
Manage users through cloud provider identity service and host resources in the cloud.
CON-3
Code must be hosted on a proprietary Git-based platform that is already in use by other 
projects in the company.
CON-6
A cloud-native approach should be favored when designing the system.
8.3.3  Architectural Concerns
Development and operations requirements introduce a new concern, shown in Table 8.7.
TABLE 8.7  Development and Operations Architectural Concerns for the Hotel Pricing System
ID
Concern
CRN-5
Set up a continuous deployment infrastructure.
Given these sets of inputs, we are now ready to describe the design process, following 
the steps outlined in Chapter 4. In this chapter, we present only the final results of the require-
ments collection process. The job of collecting these requirements is nontrivial, but beyond the 
scope of this chapter.

158  Chapter 8  Case Study: Hotel Pricing System 
8.4  The Software Design Process
We are now ready to make the leap from the world of requirements and business concerns 
to the world of design. This is perhaps the most important job for an architect—translating 
requirements into design decisions. Of course, many other decisions and duties are important, 
but this is the core of what it means to be an architect: making design decisions with far- 
reaching consequences.
8.4.1  ADD Step 1. Review Inputs
The first step of the ADD method involves reviewing the inputs and identifying which 
requirements will be considered as architectural drivers (which will be included in the design  
backlog). The inputs are summarized in Table 8.8.
TABLE 8.8  Inputs for the Hotel Pricing System
Category
Details
Design purpose
This project can be considered greenfield development, as it involves the 
complete replacement of an existing system. The purpose of the design 
activity is to make initial decisions to support the construction of the system 
from scratch.
Primary functional 
requirements
From the user stories presented in Section 8.2.1, the primary ones were 
determined to be:
■
■
HPS-2: Change Prices—Because it directly supports the core business
■
■
HPS-3: Query Prices—Because it directly supports the core business
■
■
HPS-4: Manage Hotels—Because it establishes a basis for many other 
user stories
Quality attribute 
scenarios
The scenarios for the HPS have now been prioritized (as discussed in  
Section 2.4.2) as follows:
Scenario ID
Importance to 
the Customer
Difficulty of Implementation  
According to the Architect
QA-1: Performance
High
High
QA-2: Reliability
High
High
QA-3: Availability
High
High
QA-4: Scalability 
High
High
QA-5: Security
High
Medium
QA-6: Modifiability
Medium
Medium
QA-7: Deployability
Medium
Medium
QA-8: Monitorability
Medium
Medium
QA-9: Testability
Medium
Medium
From this list, QA-1, QA-2, QA-3, QA-4, and QA-5 are selected as primary 
drivers.

8.4  The Software Design Process  159
Category
Details
Constraints
All of the previously discussed constraints are included as drivers.
Architectural 
concerns
All of the previously discussed architectural concerns associated with the 
system are included as drivers. 
8.4.2  Iteration 1: Establishing an Overall System Structure
This section presents the results of the activities that are performed in each of the steps of 
ADD in the first iteration of the design process.
8.4.2.1  Step 2. Establish Iteration Goal by Selecting Drivers
This is the first iteration in the design of a greenfield system, so the iteration goal is to achieve 
concern CRN-1, establishing an overall system structure.
Although this first iteration is driven by a general architectural concern, the architect 
must keep in mind all of the drivers that may influence the general structure of the system.  
In particular, the architect must be mindful of the following:
■
■QA-1: Performance
■
■QA-2: Reliability
■
■QA-3: Availability
■
■QA-5: Security
■
■CON-1: Access to the system via web browser
■
■CON-2: Cloud provider
■
■CON-6: Develop cloud-native solution
■
■CRN-2: Leverage team’s knowledge about Java technologies
■
■CRN-4: Avoid introducing technical debt
Accounting for security is particularly important early in the design process so this  
quality attribute is given special consideration, even though the architect does not consider it to 
be too difficult to implement.
8.4.2.2  Step 3. Choose Elements of the System to Refine
Since this project involves the complete replacement of an existing system, the first element 
to refine is the whole HPS system (shown in Figure 8.1). In this case, refinement is performed 
through decomposition.
8.4.2.3  Step 4. Choose Design Concepts That Satisfy the Selected Drivers
In this initial iteration, given the goal of structuring the entire system, design concepts are 
selected according to the roadmap presented in Section 4.3.1. Table 8.9 summarizes the selec-
tion of design decisions. Also, security is considered from the beginning of the design process. 
The words in bold in the table refer to the design concepts that are selected.

160  Chapter 8  Case Study: Hotel Pricing System 
TABLE 8.9  Design Concept Selection Decisions for the Hotel Pricing System: Iteration 1
Design Decision 
and Location
Rationale
Structure the  
back-end part of the 
system following 
the CQRS pattern 
and separate the 
command and 
query components 
that communicate 
through events.
CQRS (Command and Query Responsibility Segregation) is a pattern that 
separates changes to data from queries to this data. This pattern is useful 
to support performance, scalability, availability, and security, albeit at the 
cost of added complexity (see Section 7.3.3.2). The command and query 
components communicate using events. This type of communication using 
events also supports the decision by the company to move away from 
connecting applications using traditional request–response invocations. 
Furthermore, this will allow other applications to connect to the event channel 
in the future to perform tasks such as analysis or event storage.
Discarded alternatives include developing the back-end as a monolithic 
application. The pricing system that is being replaced was structured as a 
monolith. Although this was not the reason for its problems, it is believed 
that starting with a monolith is not aligned with the enterprise architecture 
principles of the company and current best practices. Furthermore, if the 
application is structured as a monolith, scaling it to support future growth will 
require creating multiple instances of the whole application. Although CQRS 
could be implemented in a monolith, the separation of the application into 
different components allows the query side of the application to be scaled 
independently of the command side.
Implement the 
command and 
query components 
as microservices.
The command and query parts of the system are developed and deployed as 
independent microservices that expose service APIs, making this a service-
oriented application. Multiple replicas of the query side can be set up to support 
larger query volumes and higher availability. Security is enhanced because 
clients accessing the query side cannot make changes to important aspects 
of the system such as business rule calculations. In addition to the benefits 
in terms of supporting quality attributes, the use of this pattern in this context 
is appropriate because all prices do not need to be recalculated with each 
query, and because changes in the calculations of business rules do not affect 
published prices prior to the date of change of the business rule calculation.
The use of microservices also allows CRN-3 to be addressed, as each 
microservice can be implemented by a small team.
Secure the APIs 
exposed by the 
microservices by 
implementing the 
authentication and 
authorization of 
actors tactics and 
by implementing 
the limit access 
and encrypt data 
tactics.
The front-end of the system along with external systems will interact with the 
microservices through APIs. The access to the functionalities provided by the 
APIs should be secured.
Furthermore, access to other resources such as databases needs to be 
limited because all interactions are made through APIs or through events.
All of the information that is exchanged through APIs is encrypted.
Logically structure 
the client 
application using 
the Rich Internet 
Application 
reference 
architecture.
The system must be accessed from a web browser (CON-1). The front-end 
part of the system is developed as a Rich Internet Application that consumes 
APIs on the back-end side.
Discarded alternatives include developing the application as a rich client or a 
mobile application. 

8.4  The Software Design Process  161
8.4.2.4  Step 5. Instantiate Architectural Elements, Allocate Responsibilities, and 
Define Interfaces
The instantiation design decisions considered and made are summarized in Table 8.10.
TABLE 8.10  Instantiation Decisions for the Hotel Pricing System: Iteration 1
Design Decision 
and Location
Rationale
Use Apache Kafka 
as the event bus.
Kafka is a durable message broker that enables applications to process, 
persist, and reprocess streamed data. Events are published and consumed 
from “topics”. Kafka supports message ordering through the use of keys, 
which is necessary because changes to the hotel prices for a given day must 
be ordered. Kafka also has the benefit of retaining all of the messages in a log, 
which acts as the “source of truth” for prices.
Discarded alternatives are RabbitMQ and other messaging alternatives that 
are less familiar to the team (CRN-2).
Create an additional 
microservice 
to handle price 
change events 
and deliver them 
to the Channel 
Management 
System.
The Channel Management System is a legacy application that cannot be 
changed to listen to the CQRS events. Instead, an additional microservice is 
created so that when it receives events from the command side, these events 
are then pushed to the CMS.
This microservice will not require a database, because messages are pulled 
from the log (Kafka) and sent to the CMS. In case the CMS is not available, 
messages are put back in the log and delivery is retried.
In the future, if the CMS is updated or replaced, its integration with the pricing 
system will require no changes to the HPS.
Use Angular to 
implement the front-
end component.
Angular is selected because the team has experience with this framework 
and because it supports the development of responsive front-ends. This is 
necessary to support CON-1.
Discarded alternatives are other web frameworks such as React and Vue.js, 
mainly because the team is not familiar with them.
Use Docker 
containers 
to promote 
portability across 
environments.
Because the application has to be moved across environments (QA-7), 
deploying the application as a set of container images is preferred. Each 
microservice is deployed in a container. This ensures that the application will 
run in the same execution environment from development to production.
A discarded alternative is the use of virtual machines, as the size of VM 
images is typically considerably larger than container images; also VMs 
require more resources (memory, CPU) than containers.
Use TLS (HTTPS) 
to encrypt data in 
transit.
All of the data in transit—that is, the data that is exchanged with the front-end 
and external systems—is encrypted. At this point no decisions have been 
made regarding data at rest (in the databases). 
Secure the API 
through the use of 
JSON web tokens.
Access tokens are a proven authentication and authorization mechanism for 
APIs. These tokens are provided by the User Identity Service once a user has 
successfully provided their credentials.
Use an API 
gateway to limit 
access to the 
microservices that 
expose the APIs.
Access to the microservices is not performed directly but rather through an 
API gateway, which can perform various functions including securing and 
monitoring the APIs. The calls that are received by the API are routed to the 
microservices, which are protected inside a private network in the cloud.
The results of these instantiation decisions are recorded in the next step. In this initial 
iteration, it is typically too early to precisely define functionality and interfaces. In the next 
iteration, which is dedicated to defining functionality in more detail, interfaces will begin to 
be defined.

162  Chapter 8  Case Study: Hotel Pricing System 
8.4.2.5  Step 6. Sketch Views and Record Design Decisions
Figure 8.4 shows the sketch of a component view of the architecture adapted according to the 
design decisions we have made.
This sketch is created using an informal representation. A short description of the respon-
sibilities of the elements is also captured. The fact that each component has a specific function 
promotes high cohesion and thus helps avoid technical debt (CRN-4). Note that the descrip-
tions at this point are quite crude, just indicating major functional responsibilities, with no 
details. Table 8.11 summarizes the information that is captured.
Legend
Front-End
(Angular)
User Identity
Service
API Gateway
Command
Microservice
Channel
Manager
System
Property
Management
System
External
System
Event Bus (Kafka)
price change
events
price change
events
price change
events
Export
Microservice
Query
Microservice
API
API
API
Microservice
External system
Other component
Private network
Event
<<HTTPS>>
<<HTTPS>>
<<HTTPS>>
<<HTTPS>>
Remote invocation
FIGURE 8.4  Main components of the Hotel Pricing System

8.4  The Software Design Process  163
TABLE 8.11  Elements of the Hotel Pricing System: Iteration 1
Element
Responsibility/Description
Front-End
Represents the client-side application built using the Rich Internet 
application reference architecture and implemented using Angular.
API Gateway
The API gateway sits between the clients of the API and the microservices 
that expose them.
Command  
Microservice
This microservice encapsulates the logic for the command part of the 
CQRS pattern. This includes managing the different hotels, rates, room 
types, and price calculation business rules.
Query Microservice
This microservice’s primary responsibility is to serve price requests. It exposes 
two APIs: one used by the front-end and one used by legacy internal systems.
Export Microservice
This microservice’s primary responsibility is to consume price change 
events and publish them on the Channel Management System.
Event Bus
This event bus is used by the Command microservice to publish price change 
events, and by the Query and Export microservices to consume these events.
Property Management 
System
This element represents the legacy Property Management System, which 
needs to query prices.
Channel Management 
System
This element represents the legacy Channel Management System, which 
must be notified of changes in the prices. The Export microservice is in 
charge of notifying this system.
External System
This element represents any other system that needs to query prices.
User Identity Service
Service from the cloud provider that manages users and provides access 
tokens for the APIs.
In this iteration, initial decisions about how to deploy the application are made. The micro-
services and the event bus are deployed inside a private network and are only accessible through 
the API gateway. Decisions about specific cloud resources to be used still need to be made.
8.4.2.6  Step 7. Perform Analysis of the Current Design and Review the Iteration 
Goal and Achievement of Design Purpose
The decisions made during this iteration result in new concerns that need to be addressed:
■
■CRN-6: Select specific cloud technologies
Table 8.12 summarizes the design progress using the Kanban technique discussed in 
Section 4.8.2. 
TABLE 8.12  Design Progress for the Hotel Pricing System: Iteration 1
Not 
Addressed
Partially 
Addressed 
Completely 
Addressed
Design Decisions Made During the Iteration
HPS-1
Login will be performed using the cloud provider 
identity management service, but additional decisions 
need to be made.
HPS-2
The overall structure of the application was decided 
based on the need to support this use case, but many 
details have not yet been addressed.
continues

164  Chapter 8  Case Study: Hotel Pricing System 
Not 
Addressed
Partially 
Addressed 
Completely 
Addressed
Design Decisions Made During the Iteration
HPS-3
The use of CQRS and the creation of a specific 
microservice to support queries was made to support 
this use case, but many details have not yet been 
addressed.
HPS-4
No relevant decisions made yet.
HPS-5
No relevant decisions made yet.
HPS-6
No relevant decisions made yet.
QA-1
Separation of command and query sides will promote 
performance, but many details have not yet been 
addressed.
QA-2
The selection of Kafka, a highly reliable message 
broker, supports this scenario, but many decisions 
(e.g., the configuration of Kafka itself) have not yet 
been addressed.
QA-3
The use of CQRS, Kafka, microservices, and a 
container management service in the cloud provider 
support this scenario, but many details have not yet 
been addressed.
QA-4
Scalability on the query side will be supported by 
creating multiple replicas of the Query microservice. 
Additional decisions to make this microservice scalable 
need to be made.
QA-5
Use of an API gateway, HTTPS, and access tokens 
contribute to the satisfaction of this driver.
QA-6
No relevant decisions made yet.
QA-7
The use of container technologies is a starting point to 
support this driver, but additional decisions need to be 
made.
QA-8
No relevant decisions made yet.
CON-1
The front-end will run in a web browser; the Angular 
framework is well supported by modern browsers.
CON-2
While this constraint has been considered, a new 
concern has appeared (CRN-6) regarding the selection 
and configuration of products from the cloud provider.
CON-3
No relevant decisions made yet.
CON-4
Initial decisions, such as the identification of 
microservices, that can help estimate development 
time have been made, but additional information is 
needed.
CON-6
No decisions about specific cloud provider resources 
have been made yet.
CRN-1
Establishing an overall initial system structure was the 
goal of this iteration.
CRN-2
Decisions regarding the implementation of the 
microservices have not yet been made, but Angular 
has been chosen as the implementation framework for 
the front-end.
CRN-3
The decomposition of the system into different 
microservices will allow work to be allocated to 
members of the development team (one small team 
per microservice).

8.4  The Software Design Process  165
Not 
Addressed
Partially 
Addressed 
Completely 
Addressed
Design Decisions Made During the Iteration
CRN-4
Microservices have specific functions, promoting high 
cohesion and modifiability.
CRN-5
No relevant decisions made yet.
CRN-6
No relevant decisions made yet.
8.4.3  Iteration 2: Identifying Structures to Support Primary Functionality
This section presents the results of the activities that are performed in each of the steps of 
ADD in the second iteration of the design process for the HPS. In this iteration, we move 
from the relatively abstract view of the architecture developed in iteration 1 to more detailed  
decisions that will drive implementation.
This movement from the generic to the specific is intentional, and is built into the ADD 
method. We can’t design everything up front, so we need to be disciplined about which deci-
sions we make, and when, to ensure that the design is done in a systematic way, addressing the 
biggest risks first and moving from there to ever finer details. Our goal for the first iteration 
was to establish an overall system structure. Now that this goal has been met, our goal for this 
second iteration is to reason about the units of implementation, which affect team formation, 
interfaces, and how development tasks may be distributed, outsourced, and implemented in 
sprints.
8.4.3.1  Step 2. Establish Iteration Goal by Selecting Drivers
The goal of this iteration is identifying structures to support primary functionality. Identifying 
these elements is useful for understanding how functionality is supported and in providing a 
better estimate for development.
In this second iteration, the architect considers the system’s primary user stories:
■
■HPS-2: Change prices
■
■HPS-3: Query prices
Note that in this iteration, the design is focused on the back-end part of the application.
8.4.3.2  Step 3. Choose Elements of the System to Refine
The elements that will be refined in this iteration are the Command, Query, and Export  
microservices derived from the CQRS pattern used in the first iteration.
8.4.3.3  Step 4. Choose Design Concepts That Satisfy the Selected Drivers
Table 8.13 summarizes the design decisions made. The words in bold refer to the design  
concepts that are selected.

166  Chapter 8  Case Study: Hotel Pricing System 
TABLE 8.13  Design Concept Selection Decisions for the Hotel Pricing System: Iteration 2
Design Decision and 
Location
Rationale
Create a domain model 
for the application.
Before starting a functional decomposition, it is necessary to create 
an initial domain model for the system, including identifying the 
major entities in the domain and their relationships. While this is not 
an architectural decision per se, it is important and interacts with all 
subsequent architectural choices.
There are no shortcuts or good alternatives for this step. A domain model 
must eventually be created, or it will emerge in a suboptimal fashion 
leading to an ad hoc architecture that is hard to understand and maintain.
Logically structure the 
microservices using the 
Service Application 
reference architecture. 
Microservices expose 
APIs.
Service applications (see Section 3.2.1) do not provide a user interface 
but rather expose services that are consumed by other applications.
This reference architecture is suitable to logically structure each 
microservice that exposes an API. These APIs will be consumed by the 
client application and by other legacy systems (CON-5).
There are no discarded alternatives with respect to the reference 
architecture.
8.4.3.4  Step 5. Instantiate Architectural Elements, Allocate Responsibilities, and 
Define Interfaces
The instantiation design decisions made in this iteration are summarized in Table 8.14.
TABLE 8.14  Instantiation Decisions for the Hotel Pricing System: Iteration 2 
Design Decision 
and Location
Rationale
Create a domain 
model for the 
command-side 
microservice. 
The domain model that is to be created is not a domain model for the entire 
company, but rather a model that is useful in the context of hotel prices. This 
model resides on the command-side microservice.
The query- and export-side microservices do not have their own specific 
domain models, as there was not a compelling reason to create them. In the 
case of the query side, price change events are stored directly in the data 
store. In the case of the export side, no information is stored (as we will see 
later).
Create a 
PriceChangeEvent 
and use a single 
topic and Kafka 
keys to ensure 
temporal ordering  
of price changes.
Kafka was chosen in the previous iteration, and in iteration 2 further decisions 
are made:
■
■
A single topic is used for price change events; these events contain the 
prices for a given hotel and date.
■
■
A key that guarantees price ordering is defined. This key allows log 
compaction to be supported.
Use a relational 
database for 
the Command 
microservice.
Given that the Command microservice manages the entities of the domain 
model (e.g., hotel prices, rates, room types), and given that these entities are 
related to each other, the most natural and appropriate mechanism for storing 
them is a relational database.
Discarded alternatives are nonrelational databases and other types of storage.

8.4  The Software Design Process  167
Design Decision 
and Location
Rationale
Use a 
nonrelational 
database for the 
Query microservice.
Because the database on the query side does not need to handle transactions 
and relationships between entities, and because the prices that are stored 
have a variable structure depending on the hotel, it was decided that the use 
of a nonrelational database is more appropriate. PriceChangeEvents can be 
appropriately stored in a document database.
Discarded alternatives include other types of NoSQL, relational databases, 
and other types of storage.
Use the Spring 
Framework 
technology stack 
to implement the 
microservices and 
map the system 
user stories to the 
Service Application 
reference 
architecture 
modules. 
The Spring Framework technology stack is selected because the team 
is familiar with it (CRN-2). Because of this, no alternative frameworks for 
languages other than Java are considered.
The Service Application reference architecture and Spring Framework provide 
categories of modules for the different layers: controllers to expose APIs, 
services to manage business logic, entities to manage business entities, and 
repositories to persist them, among others.
User stories can typically be mapped to modules following well-established 
rules: One entity can be mapped to at least one controller, one service, and 
one repository. This technique ensures that modules that support all of the 
functionalities and entities are identified.
The architect will perform this task only for the primary use cases. This allows 
another team member to identify the rest of the modules to allocate work 
among team members (CRN-3).
Having established the set of modules, the architect realizes the need to test 
these modules, so a new architectural concern is identified here:
■
■
CRN-7: Ensure that all non-generated modules can be unit tested
The reason that only “non-generated” modules are covered by this concern 
is that some modules, such as repositories, are generated by Spring and do 
not need to be unit tested. The use of Spring facilitates testability because it is 
built upon the Inversion of Control (IoC) pattern. 
Define and expose 
REST APIs to 
the front-end and 
document them 
using Swagger/
OpenAPI.
The API exposed by the microservices for consumption by the front-end 
will follow the REST paradigm and exchange JSON data. REST is selected 
because it is commonly used to communicate with Rich Internet front-ends; 
in this case, there are no requirements that would constrain the use of this 
paradigm. Furthermore, JSON is selected as the data exchange format for 
similar reasons.
A new concern arises from this decision:
■
■
CRN-8: Document the API
This new concern is immediately addressed by selecting Swagger/OpenAPI 
as the framework used to document the API. Swagger is selected because it 
is a mature framework that integrates easily with the Spring Framework. 
Apply the 
externalize 
configurations 
tactic to promote 
portability. 
To support moving from one execution environment to another without 
changing code or repackaging, configuration is externalized and environment 
variables are used to configure aspects such as database connections.
Use cloud 
provider managed 
services for 
databases, 
container 
orchestration, and 
Kafka.
After evaluating the alternative products available from the cloud provider 
and considering factors such as cost, licensing, ease of installation, and 
administration, a decision was made to select products directly managed by 
the cloud provider for the databases, container orchestration, and Kafka.
continues

168  Chapter 8  Case Study: Hotel Pricing System 
Design Decision 
and Location
Rationale
There are no anticipated needs to port this application to a different cloud 
provider once deployed, so open source solutions that can be deployed on 
the cloud but that require more management are discarded. Although these 
solutions could be cheaper than managed services, the time required to set 
them up is considerable and an economic analysis of the costs of managed 
services has convinced management that they are acceptable (CON-6).
Although the products are now selected, configuring them adequately is still 
pending. A new concern is raised:
■
■
CRN-9: Configure managed cloud services
Although the structures and interfaces are identified in this step of the ADD method, they 
are only captured in the next step. For this reason, they are not shown here.
8.4.3.5  Step 6. Sketch Views and Record Design Decisions
Figure 8.5 shows an initial domain model to be used in the command-side microservice.
1
*
1...*
1
1..*
*
1
1
1..*
*
Currency
Hotel
*
0..1
BaseRate
1
1
0..*
RoomType
Rate
1
Tax
CalculationRule
Price
FIGURE 8.5  Domain model for the command side of the Hotel Pricing System (Key: UML)

8.4  The Software Design Process  169
The entities of the domain model are described in Table 8.15.
TABLE 8.15  Elements of the Domain Model for the Hotel Pricing System
Element
Responsibility/Description
Hotel
A hotel, which is the main entity.
Currency
The primary currency associated with the hotel. 
RoomType
A particular type of room. One hotel may have different types of rooms.
Tax
Taxes associated with the hotels.
Rate
Different room rates that are used to calculate prices.
CalculationRule
Rules that are applied to rates to calculate prices.
Price
The price in the base rate (or a fixed rate) for a particular date.
Figure 8.6 shows a sketch of a module view of the command-side microservice with  
modules derived from the entities that are associated with the primary use case (HPS-2).
HotelController
HotelService
HotelRepository
PricesRepository
PricesService
Exposed Services Layer
Business Layer
Data Layer
Hotel
Rate
RoomType
Domain Model
Data Transfer Objects
HotelDto
PriceChangeEventProcessor
EnvironmentVariable
Manager
Cross Cutting
Price
FIGURE 8.6  Module view of the command-side microservice (Key: UML)
The responsibilities for the elements identified in Figure 8.6 are summarized in Table 8.16.

170  Chapter 8  Case Study: Hotel Pricing System 
TABLE 8.16  Responsibilities of the Command-Side Microservice Modules
Element
Responsibility/Description
HotelController
Exposes API endpoint for CRUD operations for the Hotel resource and 
contains annotations that produce Swagger documentation.
DataTransferObjects
Contains objects that are returned or received by the API endpoint.
HotelService
Validates certain business rules that do not fit inside entities and coordinates 
objects from the domain model and data layer to support operations exposed 
by the controller. Also manages certain transactional operations.
PricesService
Coordinates price changes.
Domain Model
Contains the business entities. These entities are “rich” in the sense that they 
contain business logic.
HotelRepository
Manages persistence of Hotel objects.
PricesRepository
Stores public rate prices for given days. Prices for other rates are not stored 
because they are calculated and sent as events to the query side.
PriceChangeEvent­­
Processor
Generates events related to price changes.
Environmental­­
VariableManager
Manages externalized configuration values (environment variables).
Figure 8.7 shows a sketch of a module view of the query-side microservice.
PriceChangeEventsConsumer
DailyPricesService
DailyPricesRepository
Exposed Services Layer
Business Layer
Data Layer
DailyPrices
Domain Model
Data Transfer Objects
DailyPricesDto
EnvironmentVariable
Manager
Cross Cutting
DailyPricesController
FIGURE 8.7  Module view of the query-side microservice (Key: UML)

8.4  The Software Design Process  171
The responsibilities of the elements identified in Figure 8.7 are summarized in Table 8.17.
TABLE 8.17  Responsibilities of the Query-Side Microservice Modules
Element
Responsibility/Description
PriceChangeEventsConsumer
Receives price change events that are published in Kafka.
DailyPricesController
Exposes APIs to query prices.
DailyPricesService
Prepares price change events to be stored.
DailyPricesRepository
Stores events in the (nonrelational) database.
Data Transfer Objects
Prices that are returned as JSON or XML objects.
Domain Model
Contains a class that stores the information from the price 
change events.
EnvironmentalVariableManager
Manages externalized configuration values (environment 
variables).
Figure 8.8 shows a sketch of a module view of the export-side microservice.
PriceChangeEventsConsumer
PricesService
CMSClient
Exposed Services Layer
Business Layer
Data Layer
EnvironmentVariable
Manager
Cross Cutting
FIGURE 8.8  Module view of the export-side microservice (Key: UML)

172  Chapter 8  Case Study: Hotel Pricing System 
The responsibilities of the elements identified in Figure 8.8 are summarized in Table 8.18.
TABLE 8.18  Responsibilities of the Export-Side Microservice Modules
Element
Responsibility/Description
PriceChangeEventsConsumer
Receives price change events that are published in Kafka.
DailyPricesService
Prepares payload to be sent to the CMS.
CMSClient
Communicates with the CMS.
EnvironmentalVariableManager
Manages externalized configuration values (environment 
variables).
Next, we show the sequence diagrams for HPS-2, which are used to define interfaces (as 
discussed in Section 4.6).
8.4.3.5.1  HPS-2 Command Side
Figure 8.9 shows an initial sequence diagram for HPS-2 (Change prices) on the command 
side. The client application invokes the POST method that triggers a change in prices. The 
PriceService retrieves the hotel and price that must be changed, and asks the hotel object to cal-
culate the additional types of prices. To increase performance, an index is added to the database. 
Once prices are calculated, an event is created and sent to the PriceChangeEventProcessor, 
which sends it to Kafka.
Note that the PriceChangeEventProcessor must use an adequate topic and keys to send 
the message to ensure ordering of the events. The schema for the event payload remains to 
be defined. Also, it is necessary to ensure that a failure in the broker does not result in a 
situation in which a price is modified but an event is not sent (the method is annotated as  
@Transactional, but configuration of the transaction manager is pending). Finally, the event 
broker must be configured to support adequate performance and reliability. All of these 
aspects can be considered as part of the additional concern that is created in this iteration: 
CRN-9: Configure managed cloud services.
8.4.3.5.2  HPS-2 Query Side
Figure 8.10 shows the interaction that occurs on the query side when the event is received. 
Here, the event payload is simply stored in the database.

8.4  The Software Design Process  173
:HotelController
:PricesService
@Transactional
:HotelRepository
:PricesRepository
true
ﬁndById(hotelCode)
hotel
hotel:Hotel
price:Price
setAmount(amount)
true
new(prices)
evt
:PriceChange
EventProcessor
evt:
PriceChangeEvent
Event sent
to Kafka
publishEvent(evt)
true
204 NO CONTENT
POST /v1/hotels/{id}/prices
updatePricesForPeriod(hotelCode, date, amount)
validateBusinessRules()
ﬁndByHotelAndDate(hotel, date)
price
calculateAdditionalPrices(price)
prices
FIGURE 8.9  Sequence diagram for HPS-2 (Change prices) on the command side (Key: UML)

174  Chapter 8  Case Study: Hotel Pricing System 
:Kafka
:PriceChangeEventConsumer
event:PriceChangeEvent
:DailyPricesService
:DailyPricesRepository
processPriceChangeEvent(event)
prices
prices:DailyPrices
return
true
save(prices)
true
ﬁndByHotelCodeAndDate(event.getHotelCode(), event.getDate())
setValues(event.getPrices())
FIGURE 8.10  Sequence diagram illustrating the reception of events in the query side (Key: UML)
8.4.3.5.3  HPS-2 Export Side
Figure 8.11 shows the interaction that occurs on the export side when the event is received. In 
this scenario, the event is consumed and the event contents are used to prepare the payload that 
is sent to the CMS. After the event is sent, a success message is returned to Kafka.
:Kafka
:PriceChangeEventConsumer
:PricesService
prepare payload
:CMSClient
processPriceChangeEvent(event)
true
true
event:PriceChangeEvent
:Channel
Management
System
export(payload)
true
send message
OK
FIGURE 8.11  Sequence diagram illustrating the reception of events in the export side (Key: UML)

8.4  The Software Design Process  175
From the interactions illustrated in the sequence diagrams, the initial interfaces can be 
defined. In this case, both an external interface (API) and internal interfaces are identified. 
Regarding the API, endpoint paths are defined from the domain model. Likewise, API ver-
sioning (an additional concern) is considered in the name of the path. A more detailed descrip-
tion of the methods will eventually need to be be established, but here we provide a summary 
of only the most relevant methods.
8.4.3.5.4  HPS-2 Interfaces
HotelController
Method Name
Description
POST /v1/hotels/{id}/
prices
Allows prices for a hotel to be updated; returns HTTP NO CONTENT if 
change is successful, or HTTP BAD REQUEST if parameters are incorrect.
Parameters:
■
■
Hotel Identifier
■
■
Range of dates of price changes
■
■
Amount
The following interfaces from the modules provide a basis for defining internal inter-
faces. Only a sample of methods from the command side are described here.
PriceServices
Method Name
Description
updatePriceForPeriod
Update prices for a hotel and produce a price change event.
Parameters:
■
■
Hotel Identifier
■
■
Range of dates of price changes
■
■
Amount
Returns:
■
■
Boolean: true if successful, false otherwise
Throws:
■
■
PriceChangeEventException in case price change fails.
PriceRepository
Method Name
Description
findByHotelAndDate
Retrieves a price object associated with a hotel and a particular date.
Parameters:
■
■
Hotel object
■
■
Date
Returns:
■
■
Price object or null

176  Chapter 8  Case Study: Hotel Pricing System 
Hotel
Method Name
Description
calculateAdditionalPrices
Calculates prices associated with all the rates and room types for this 
hotel. Returns a data structure with the prices.
Parameters:
■
■
Price for base room
Returns:
■
■
Prices for other rooms and rates
PriceChangeEventProcessor
Method Name
Description
publishEvent
Publishes an event to the broker for the appropriate topic with a key that 
ensures correct ordering.
Parameters:
■
■
PriceChangeEvent
Returns:
■
■
Prices for other rooms and rates
In regard to deployment, the design decisions have resulted in a clearer vision of the solu-
tion in the cloud environment. At this point, specific products from the cloud provider have 
been selected, but they still need to be configured appropriately to support the quality attribute 
goals (CRN-9). Figure 8.12 presents an allocation view of the system using cloud-managed 
resources.
The responsibilities of the elements identified in Figure 8.12 are summarized in  
Table 8.19.
TABLE 8.19  Deployment Elements for the Hotel Pricing System: Iteration 2
Element
Responsibility/Description
User Identity 
Management Service
Manages users; provides and validates access tokens used to access APIs.
API Gateway
Restricts direct access to the microservices and secures the APIs. Also 
responsible for other aspects such as monitoring.
Load Balancer
Forwards and balances the requests to the microservices inside the private 
cloud network.
Container 
Orchestration Service
Provides an execution environment for the containers associated with 
the different microservices. This service also monitors the health of the 
containers and, if needed, restarts them in case of failures, contributing to 
improved availability (QA-3).
Managed Relational 
Database
Database service managed by the cloud provider.
Managed 
Nonrelational 
Database
Database service managed by the cloud provider.
Managed Kafka 
Service
Kafka service managed by the cloud provider.

8.4  The Software Design Process  177
Managed
Relational DB
Managed
Kafka
Service
Container
Orchestration
Service
(Command)
Private
Cloud
API
Gateway
User Identity
Management
Service
Load
Balancer
Container
Orchestration
Service
(Query)
Container
Orchestration
Service
(Export)
Managed
Nonrelational
DB
FIGURE 8.12  Initial deployment diagram of the system (Key: UML)
8.4.3.6  Step 7. Perform Analysis of the Current Design and Review the Iteration 
Goal and Achievement of Design Purpose
The decisions made in iteration 2 provided an initial understanding of how functionality is 
supported in the system. Furthermore, the design decisions made at this point are sufficient 
to produce a Minimum Viable Product (MVP) for the system, which can be demonstrated to 
internal stakeholders and which supports price changes and queries to these changes (CON-4).

178  Chapter 8  Case Study: Hotel Pricing System 
New architectural concerns have also been added as a result of the decisions made in this 
iteration:
■
■CRN-7: Ensure that all non-generated modules can be unit tested
■
■CRN-8: Document the APIs
■
■CRN-9: Configure cloud provider managed services
Table 8.20 summarizes the design progress using the Kanban technique discussed in 
Section 4.8.2. Note that drivers that were completely addressed in the previous iteration are 
removed from the table.
TABLE 8.20  Design Progress for the Hotel Pricing System: Iteration 2
Not 
Addressed
Partially 
Addressed 
Completely
Addressed
Design Decisions Made During the Iteration
HPS-1
No additional decisions were made during the 
iteration.
HPS-2
This iteration was dedicated to the identification of the 
modules that support this use case on the back-end, 
but the front-end part is not yet designed.
HPS-3
Querying through the API has been addressed during 
this iteration.
HPS-4
No relevant decisions have been made yet, but rules 
for module identification used for HPS-2 can also be 
applied for this use case.
HPS-5
same as for HPS-4
HPS-6
same as for HPS-4
QA-1
Adding an index to the database will accelerate the 
retrieval of the information required by HPS-2, but 
additional decisions need to be made.
QA-2
Kafka is highly reliable, and the use of a transaction 
manager to ensure that an event is sent when prices 
are changed will promote reliability.
QA-3
No additional decisions were made during this 
iteration.
QA-4
No additional decisions were made during this 
iteration.
QA-5
Use of an API gateway, HTTPS, and access tokens 
contribute to the satisfaction of this driver.
QA-6
The way the Query microservice is structured, based 
on the Service Application reference architecture, will 
help add other types of query endpoints.
QA-7
The use of containers and externalized configurations 
(through EnvironmentalVariableManager) should 
allow this requirement to be satisfied. There are 
still some decisions to be made regarding the 
way configurations will be externalized and how 
containers will be built.
QA-8
No relevant decisions made yet. 
CON-3
No relevant decisions made yet. 

8.4  The Software Design Process  179
Not 
Addressed
Partially 
Addressed 
Completely
Addressed
Design Decisions Made During the Iteration
CON-4
The level of decomposition achieved in this iteration 
provides sufficient information to estimate and 
understand if the time constraint can be supported. 
The result of this iteration can already support the 
primary functionality and can be demoed.
CON-6
Cloud-managed resources have been selected.
CRN-2
The Spring framework was selected to implement the 
microservices.
CRN-4
The decisions made in this iteration promote high 
cohesion and modifiability.
CRN-5
No relevant decisions made yet.
CRN-6
Specific products from the cloud provider have been 
identified. Other products will need to be selected as 
design progresses.
CRN-7
The use of Spring, which is based on the IoC pattern, 
facilitates unit testing using frameworks such as JUnit 
and Mockito.
CRN-8
Swagger/OpenAPI was chosen to document the APIs.
CRN-9
No relevant decisions made yet.
8.4.4  Iteration 3: Addressing Reliability and Availability Quality Attributes
This section presents the results of the activities that are performed in each of the ADD steps 
in the third iteration of the design process. While the decisions made in the previous iteration 
are sufficient to support price changes and querying these price changes from a functional 
perspective on the back-end side, several quality attributes have not yet been addressed. These 
are the focus of this iteration.
8.4.4.1  Step 2. Establish Iteration Goal by Selecting Drivers
The goal of this iteration is to identify additional structures to support quality attributes— 
particularly reliability, availability, and scalability:
■
■QA-2: Reliability—Guaranteeing that 100% of the price changes are published and 
exported successfully
■
■QA-3: Availability—Uptime SLA
■
■QA-4: Scalability—Increase query volume without decreasing average latency
In the previous iteration, a new concern was identified. This concern plays an important 
role with respect to the quality attributes:
■
■CRN-9: Configuration of cloud provider managed services
8.4.4.2  Step 3. Choose Elements of the System to Refine
For this iteration, the elements that are refined are mainly located in the infrastructure part 
of the solution. Furthermore, additional refinements are made to the export microservice to 
support failure scenarios.

180  Chapter 8  Case Study: Hotel Pricing System 
8.4.4.3  Step 4. Choose Design Concepts That Satisfy the Selected Drivers
The design concepts used in this iteration are summarized in Table 8.21.
TABLE 8.21  Design Concept Selection Decisions for the Hotel Pricing System: Iteration 3
Design Decision and Location
Rationale
Apply tactics for performance: increase 
resources and maintain multiple copies of 
computations and data.
Replication is an essential strategy to address both 
reliability and performance.
Apply tactics for availability: exception 
handling.
When one of the microservices fails, the problem 
needs to be detected and addressed so that the 
system as a whole continues operating normally.
Apply tactics for availability: redundant spare.
There may be a more generalized failure occurring 
in the cloud’s availability zone where the various 
microservices are located. This situation also 
needs to be addressed.
8.4.4.4  Step 5. Instantiate Architectural Elements, Allocate Responsibilities, and 
Define Interfaces
The instantiation design decisions are summarized in Table 8.22.
TABLE 8.22  Instantiation Decisions for the Hotel Pricing System: Iteration 3
Design Decision and Location
Rationale
Use Kafka as the data storage 
mechanism instead of a database  
for the Export microservice.
As described in iteration 2, price change events that are 
received by the export microservice are forwarded to the CMS.
In case of communication failures with the CMS, there is no 
need to store the events to later retry: Kafka can be used as the 
storage mechanism (this is described in more detail in step 6).
A discarded alternative is to add a database to the Export 
microservice, but this would incur additional costs and would 
not provide additional benefits.
Configure the container 
orchestration service to restart 
microservices in case of failures.
Container orchestration services from the cloud provider 
support restarting containers in case a failure is detected.
A discarded alternative is to use an open source solution to 
orchestrate containers. However, it was previously decided 
that managed services are preferred for this project.
Establish an infrastructure 
configuration to support passive 
redundancy.
The design of the system allows it to be replicated. This 
involves creating a duplicate of the infrastructure hosted in a 
different availability zone or region. When a failure is detected, 
this replica needs to be started.
Configurations of the managed services that support 
redundancy are therefore selected.
A discarded alternative is to manually replicate the whole 
infrastructure in different availability zones or regions. 
However, the work involved in setting up and managing such 
an approach makes it undesirable.
Replicate the Query service.
To increase performance, the Query microservice is replicated 
and requests are load balanced.
The results of these instantiation decisions are recorded in the next step.

8.4  The Software Design Process  181
8.4.4.5  Step 6. Sketch Views and Record Design Decisions
Several decisions are made during this iteration to address the drivers listed in step 2. Here we 
summarize these decisions.
Communication Failure with the Channel Management System
In this iteration, a scenario involving failed communication between the export microservice 
and the CMS is considered (associated with QA-2). Figure 8.13 shows the interaction that 
occurs with the Export microservice when the event is received. In this scenario, the event is 
sent to the CMS but the invocation does not succeed (e.g., there is a timeout). When this situa-
tion occurs, the event that was consumed from the event log is sent back to Kafka. This means 
that the next time the export microservice polls the event log, it will recover the event that was 
not sent successfully.
prepare payload
export(payload)
send message
return (failure)
event requeued
event:PriceChangeEvent
Events recovered using a
polling approach
After a delay, the event log
is polled again
:Kafka
:PriceChangeEventConsumer
:PricesService
:CMSClient
:Channel
Management
System
processPriceChangeEvent(event)
t(eve
FIGURE 8.13  Sequence diagram of failure scenario on the export side (Key: UML)
Changes in the Infrastructure to Support Quality Attributes
Figure 8.14 shows the managed services with the introduction of replication.

182  Chapter 8  Case Study: Hotel Pricing System 
Managed
Kafka
Service
<<HA>>
Container
Orchestration
Service
(Command)
Private
Cloud
API
Gateway
Load
Balancer
Container
Orchestration
Service
(Query)
<<replicated>>
Container
Orchestration
Service
(Export)
Managed
Nonrelational DB
<<HA>>
Managed
Relational DB
<<HA>>
API
Gateway
DNS and
Monitoring
Service
User Identity
Management
Service
<<HA>>
Managed
Kafka
Service
<<HA>>
Container
Orchestration
Service
(Command)
Private
Cloud
Load
Balancer
Container
Orchestration
Service
(Query)
<<replicated>>
Container
Orchestration
Service
(Export)
Managed
Nonrelational DB
<<HA>>
Managed
Relational DB
<<HA>>
Active
Passive
FIGURE 8.14  Refined deployment diagram (Key: UML)

8.4  The Software Design Process  183
The responsibilities for the elements identified in the previous diagram are summarized 
in Table 8.23 (only the elements that were updated are described).
TABLE 8.23  Deployment Elements for the Hotel Pricing System: Iteration 3
Element
Responsibility/Description
DNS and Monitoring 
Service
Monitor the availability of the services in the active replica. If a failure is 
detected, start up the containers on the passive replica and redirect traffic to 
the passive replica.
User Identity 
Management Service
Manage users, and provide and validate access tokens used to access 
APIs. A high availability configuration is selected where the service is 
replicated across availability zones.
Container 
Orchestration Service
Provides an execution environment for the containers associated with 
the different microservices. This service also monitors the health of the 
containers and, if needed, restarts them in case of failures, contributing to 
improved availability (QA-3).
Managed Relational 
Database
A database service managed by the cloud provider. A high availability 
configuration is selected where the database is replicated across availability 
zones. 
Managed 
Nonrelational 
Database
A database service managed by the cloud provider. A high availability 
configuration is selected where the database is replicated across availability 
zones.
Managed Kafka 
Service
A Kafka service managed by the cloud provider. A high availability 
configuration is selected where Kafka is replicated across availability zones.
8.4.4.6  Step 7. Perform Analysis of the Current Design and Review the Iteration 
Goal and Achievement of Design Purpose
Table 8.24 summarizes the status of the different drivers and the decisions that were made 
during iteration 3. Note that drivers that were completely addressed in the previous iteration 
have been removed from the table.
TABLE 8.24  Design Progress for the Hotel Pricing System: Iteration 3
Not 
Addressed
Partially 
Addressed 
Completely
Addressed
Design Decisions Made During the Iteration
HPS-1
No additional decisions were made during the iteration.
HPS-2
No additional decisions were made during the iteration.
HPS-4
No additional decisions were made during the iteration.
HPS-5
No additional decisions were made during the iteration.
HPS-6
No additional decisions were made during the iteration.
QA-1
Replication of the Query microservice improves 
performance and scalability. Tests to validate the 
measures associated with the quality attributes remain 
to be done.
QA-2
Kafka is highly reliable, and the use of a transaction 
manager to ensure that an event is sent when prices 
are changed will promote reliability.
continues

184  Chapter 8  Case Study: Hotel Pricing System 
Not 
Addressed
Partially 
Addressed 
Completely
Addressed
Design Decisions Made During the Iteration
QA-3
Container orchestration addresses failures in 
containers; replication across availability zones 
addresses failures in an availability zone.
QA-4
Tests to validate the measures associated with the 
quality attributes remain to be done.
QA-5
No additional decisions have been made.
QA-7
No additional decisions have been made.
QA-8
No relevant decisions made yet. 
CON-3
No relevant decisions made yet. 
CRN-4
A careful analysis considering growth scenarios and 
cost helps to select technologies that are more future-
proof, thereby avoiding technical debt.
CRN-5
No relevant decisions made yet.
CRN-6
Specific products from the cloud provider have been 
identified. Other products will need to be selected as 
the design progresses.
CRN-9
Initial configurations have been established for the 
container orchestration service and other managed 
services (database and Kafka).
8.4.5  Iteration 4: Addressing Development and Operations Requirements
At some point during the design process, decisions need to be made to satisfy the development 
and operations requirements. This iteration focuses on decisions made to satisfy QA-7 and 
CRN-5, which are these types of requirements.
8.4.5.1  Step 2. Establish Iteration Goal by Selecting Drivers
For this iteration, the architect focuses on satisfying the following requirements:
■
■QA-7 (Deployability): The application is moved between environments as part of the 
development process. Moving is performed successfully in at most 4 hours.
■
■CRN-5: Set up continuous deployment infrastructure.
Certain constraints are also considered:
■
■CON-3: Code must be hosted on a proprietary Git-based platform.
8.4.5.2  Step 3. Choose Elements of the System to Refine
For this scenario, the elements that will be refined are the various microservices. However, 
elements that haven’t been identified previously will be added.
8.4.5.3  Step 4. Choose Design Concepts That Satisfy the Selected Drivers
The design concepts used in this iteration are summarized in Table 8.25.

8.4  The Software Design Process  185
TABLE 8.25  Design Concept Selection Decisions for the Hotel Pricing System: Iteration 4
Design Decision and Location
Rationale
Apply the tactic of managing a 
deployment pipeline.
The company wants to automate the tasks needed to 
put the application in the pre-production and production 
environments. This can be achieved by setting up a CI/CD 
pipeline.
Apply the tactic of managing the 
deployed system.
Once built, a container image needs to be uploaded to a 
container registry so that it can be pulled and instantiated 
in the different environments. This is necessary for initial 
deployment and for updates.
Apply the tactic of scripting 
deployment commands to  
automate the instantiation of the 
infrastructure (infrastructure as code).
It is possible to manually set up all the infrastructure in the 
cloud provider environment, but this is tedious and error-
prone. Furthermore, the infrastructures need to be similar 
in the different environments. Instead of setting them up 
manually, they are described in a deployment configuration 
script that can be used to easily instantiate an environment. 
Also, changes in the configuration of the infrastructure are 
easily replicated among the different environments.
8.4.5.4  Step 5. Instantiate Architectural Elements, Allocate Responsibilities, and 
Define Interfaces
The instantiation design decisions are summarized in Table 8.26.
TABLE 8.26  Instantiation Decisions for the Hotel Pricing System: Iteration 4
Design Decision and 
Location
Rationale
Use proprietary Git-based 
platform support for CI/CD 
pipelines.
Constraint CON-3 states that code must be hosted on a proprietary 
Git-based platform. This platform’s support for CI/CD pipelines can 
be leveraged. Each microservice is managed as an independent 
project, and each one has to include an individual pipeline.
Discarded alternatives included setting up a separate CI server 
using tools such as Jenkins.
Use cloud provider’s 
container registry to host 
Docker images.
Because CON-6 states that a cloud-native approach should be 
favored, it makes sense to leverage the support that the selected 
cloud provider offers for hosting container images.
Discarded alternatives include Docker Hub, because it would 
require the payment of an additional subscription that is more 
expensive than the costs of the cloud provider’s container registry.
Use cloud provider’s support 
for infrastructure as code 
to automate instantiation of 
environments.
Because CON-6 states that a cloud-native approach should be 
favored, it makes sense to leverage the support that the selected 
cloud provider offers for describing infrastructure as code.
Discarded alternatives include Terraform, because it requires 
managing external tools; furthermore, the costs incurred by the 
use of the cloud provider’s infrastructure as code features is not 
significant.
The results of these instantiation decisions are recorded in the next step.

186  Chapter 8  Case Study: Hotel Pricing System 
8.4.5.5  Step 6. Sketch Views and Record Design Decisions
Figure 8.15 shows how each microservice is structured as an individual project that is hosted 
in an individual repository. Table 8.27 describes the elements depicted in the figure.
Source and Test Code + Resources
Maven POM
Dockerﬁle
Pipeline + Cloud Descriptors
Microservice Project Repository
FIGURE 8.15  Package diagram showing structure of repositories (Key: UML)
TABLE 8.27  Repository Elements for the Hotel Pricing System: Iteration 4
Element
Responsibility
Microservice project repository A project for one microservice, which is hosted in a single repository.
Source and test code + 
resources
Source and test code, along with resources.
Maven POM
Maven descriptor used to automate the build process.
Dockerfile
Used to build Docker container images.
Pipeline + cloud descriptors
Descriptors that provide information for the CI/CD pipeline steps 
and for configuring the cloud resources where the microservice is 
executed.

8.4  The Software Design Process  187
Figure 8.16 shows the sequence of tasks that triggers the update of a microservice.
passed
Merge request
Test and static
code analysis
Container image
generation and
uploading
Container update
Git-based
Platform
Cloud
Environment
not
passed
FIGURE 8.16  Activity diagram for the integration and deployment process (Key: UML)
A merge request of a branch with new changes to the main branch triggers the execution 
of tests and static code analysis (the pipeline descriptor triggers Maven tasks). If the previ-
ous step is completed successfully, a container image is generated using the Dockerfile and 
uploaded to the container registry (this is where the transition from the Git-based platform to 
the cloud environment occurs). Once uploaded, the container is updated by the orchestrator in 
the cloud provider’s environment.
8.4.5.6  Step 7. Perform Analysis of the Current Design and Review the Iteration 
Goal and Achievement of Design Purpose
In this iteration, important design decisions have been made to address QA-7 and CRN-5. 
Table 8.28 summarizes the status of the different drivers and the decisions that were made 
during the iteration. Note that drivers that were completely addressed in the previous iteration 
have been removed from the table.
TABLE 8.28  Design Progress for the Hotel Pricing System: Iteration 4
Not 
Addressed
Partially 
Addressed 
Completely
Addressed
Design Decisions Made During the Iteration
HPS-1
No additional decisions were made.
HPS-2
No additional decisions were made.
HPS-4
No additional decisions were made.
continues

188  Chapter 8  Case Study: Hotel Pricing System 
Not 
Addressed
Partially 
Addressed 
Completely
Addressed
Design Decisions Made During the Iteration
HPS-5
No additional decisions were made.
HPS-6
No additional decisions were made.
QA-1
No additional decisions were made.
QA-2
No additional decisions were made.
QA-3
No additional decisions were made.
QA-4
No additional decisions were made.
QA-5
No additional decisions were made.
QA-7
The use of Docker and externalization of the 
configuration values plus the establishment of a CI/CD 
pipeline allows this requirement to be satisfied.
QA-8
No relevant decisions made yet. 
CON-3
The code is now hosted on a Git-based platform.
CRN-4
No relevant decisions were made during this iteration.
CRN-5
An infrastructure to support continuous integration and 
deployment has been established. Further decisions 
need to be made to manage scenarios such as 
rollbacks.
CRN-9
Additional configurations to cloud provider-managed 
services were made, particularly to the container 
registry.
At this point, it was determined that sufficient decisions had been made to proceed 
directly to implementation, without needing further ADD iterations for this initial release of 
the system. As described in CON-4, the system was slated to have a working MVP within two 
months, so it was important to transition quickly to an initial implementation.
8.5  Summary
This chapter presented an example of using ADD to design a greenfield system in a mature 
domain. We first illustrated three iterations with different foci: addressing the general con-
cern of structuring the application, addressing functionality, and addressing quality attribute 
scenarios. We then illustrated a fourth iteration focused on supporting development and oper-
ations requirements.
This example follows the roadmap discussed in Section 4.3.1. It is interesting to note that 
the application is structured using the CQRS pattern implemented as a set of microservices, 
but that the microservices themselves are structured internally using a reference architecture. 
Also, the selection of externally developed components—in this case, frameworks and prod-
ucts, both open source and from a cloud platform—was performed across the different itera-
tions. The example also illustrates how the design results in a system that is functional early in 
the development process, which aligns with agile development practices. Finally, the example 

8.7  Discussion Questions  189
illustrates that new architectural concerns may appear as the design progresses. CRN-4, the 
concern associated with avoiding introducing technical debt, is never completely addressed, as 
the design decisions in each iteration may introduce additional debt.
This example was structured to show how concerns, primary user stories, and quality 
attribute scenarios can be addressed as part of architectural design. Also, it illustrates that 
architectural design sometimes requires a certain amount of detail and is more than just a 
“high level” design. The design was not completed with the iterations described here, and 
additional iterations would be necessary to satisfy the drivers that were not fully addressed. 
However, the decisions made within the four iterations were sufficient to demonstrate how 
ADD is conducted to generate the initial system MVP. Finally, it must be noted that prototypes 
or analyses need to be created to ensure that the measures associated with the quality attribute 
scenarios are satisfied.
8.6  Further Reading
Many of the design concepts used in this chapter are discussed in chapters 5, 6 and 7 of this 
book and also in the online design concepts catalog.
8.7  Discussion Questions
1.	
A confirmation mechanism for price publication needs to be added to the HPS so that 
the user interface can show some icons that signal whether a price change is ready to be 
queried or has been exported to the Channel Management System. Which changes would 
you make to the architecture to accommodate that requirement?
2.	
How would you modify the design to support QA-8 (Monitorability)?
3.	
Several quality attributes were addressed by using cloud-managed solutions. Can all 
quality attributes be satisfied in this way?
4.	
Suppose that new hotels are added to the system, and this information needs to be 
communicated to other systems in the company. Which changes would you make to the 
architecture to accommodate this requirement?
5.	
Considering the design decisions that were made during the four iterations, can you 
think of a different ordering for them? For example, can the deployment and operations 
design decisions be made prior to the ones made in the earlier iterations?
6.	
Consider CON-5: The system must interact initially with existing systems through REST 
APIs but may need to later support other protocols. Which changes would you make to 
the design of the system, if any, to accommodate this constraint?

This page intentionally left blank 

191
9
Case Study: Digital Twin 
Platform
With Serge Haziyev, Yaroslav Pidstryhach, and Rodion Myronov
We now present an extended design example of using ADD in a greenfield system for an 
emerging domain, that of digital twins in an Industry 4.0 context. At the time of writing, this 
domain was still relatively new and rapidly evolving. This domain requires a combination of 
multiple disciplines such as the Internet of Things (IoT), cloud computing, Big Data and ana-
lytics, artificial intelligence/machine learning (AI/ML), extended reality (XR), simulation, 
advanced automation, and often robotics. Such a vast number of domains is beyond the exper-
tise of a single architect, and this case study exemplifies how a team of architects from dif-
ferent disciplines can participate in the design of a system (as discussed in Chapter 12). Here, 
none of the architects can solely rely on their experience alone to guide them. Instead, they 
have to cooperate and leverage design concepts and best practices from each discipline. 
Why Read This Chapter?
In the last chapter, we said that people learn best from examples. But having more than 
one example is even better! In this chapter, we offer another example of design using 
ADD in a very different, and challenging, context. The scale of the system here is much 
larger than the one presented in Chapter 8, and this introduces the need to consciously 
align the specific considerations and design choices made in each iteration with the 
overall system drivers. In this chapter, we hope to convince you that, despite this added 
complexity, ADD works for architectural design at all scales.

192  Chapter 9  Case Study: Digital Twin Platform
9.1  Business Case
The client in this case study is a large food producer with dozens of production plants, many of 
them containing several production lines. The company is committed to developing innovative 
solutions as part of its mission to ensure the advancement of food security and promotion of 
sustainability. For this company, creating a smart factory and automating industrial processes 
is crucial in addressing the shortage of skilled workers and maintaining affordable prices for 
consumers.
One of the key investments of the company is creation of a digital twin platform—a 
computational platform with digital representations of assets and processes. This platform is 
expected to enable a range of use cases, from simple ones like remote monitoring of industrial 
processes across plants, to more complex ones such as process simulation and autonomous 
decision-making to adapt to a changing business environment as quickly as possible. The plat-
form should provide the capabilities to implement various use cases that contribute to the goal 
of creating a smart factory.
The marketecture diagram (an informal depiction of the system’s structure) shown in 
Figure 9.1 represents the desired solution from a functional perspective.
FIGURE 9.1  Marketecture diagram for the digital twin platform

9.2  System Requirements  193
9.2  System Requirements
For requirements elicitation activities, the Digital Twin Maturity Model (DTMM), shown in 
Figure 9.2, was used to identify use cases corresponding to each of the five digital twin matu-
rity stages.
Status Twin
- Device connectivity
(input)
- Data ingestion, storing,
processing and analytics
- Dashboard and
visualization
- Math and physical
modeling
- Industrial process and
asset behavior simulation
- ML model training and inference
- MLOps infrastructure
- Device connectivity (output)
- Control and automation
Operational Twin
Simulation Twin
Autonomy Twin
FIGURE 9.2  Digital Twin Maturity Model
Each stage in this model represents a group of capabilities that the platform should sup-
port to provide value to its users. During the requirements elicitation process, base use cases 
were chosen for each stage, as this technique allows for a holistic view of the system’s capa-
bilities rather than focusing on just a part of it. Some of the use cases chosen are abstract and 
require further development and elicitation to create more concrete forms, as innovative solu-
tions typically require exploration and experimentation. Therefore, the platform’s goal is to 
enable both the identified use cases and similar future ones.
The following subsections summarize the most important requirements collected. These 
are a set of primary use cases, a set of quality attribute scenarios, and a set of constraints.
9.2.1  Use Case Model
Table 9.1 presents the primary use cases for the system and their relationship with the DTMM.

194  Chapter 9  Case Study: Digital Twin Platform
TABLE 9.1  Primary Use Cases for the Digital Twin Platform
Use Case
Description
DTMM Stage
UC-1: Real-time 
monitoring and 
historical data 
insights
As a Global Operator, I can see information such as 
overall equipment effectiveness (OEE) for any industrial 
process on any plant. The data should be collected from 
equipment in real time and stored in the cloud.
Monitoring
UC-2: Quality 
inspection  
automation
As a Quality Control Operator, I can control the quality of 
production via sensors (including optical sensors). The 
system should immediately generate alerts in case of 
detection of anomalies.
Analyzing
UC-3: Predictive 
maintenance and 
XR for maintenance 
activities
As a Maintenance Engineer, I can get predictions for 
equipment failures and diagnose them by accessing 
historical and real-time information. The information 
should be accessible through a hands-free extended 
reality device.
Predicting
UC-4: Process 
simulation
As a Process Engineer, I can find optimal parameters for 
equipment settings based on raw materials and desired 
quality of output. The parameters should be derived from 
simulation.
Prescribing
UC-5: Advanced 
automation
As an Automation Engineer, I want to automate 
operational decision-making, so that the plant can adapt 
to changing conditions (such as orders, raw materials, or 
power supply) without human intervention.
Adapting
9.2.2  Quality Attribute Scenarios
Table 9.2 is a list of quality attribute scenarios for the system.
TABLE 9.2  Quality Attribute Scenarios for the Digital Twin Platform
ID
Quality 
Attribute
Scenario
Associated 
Use Case
QA-1
Availability
Critical system functions for the production floor 
must remain 100% available in the event of a cloud 
or network outage. Should such an event occur, data 
should be cached at the edge for at least 8 hours.
UC-2
QA-2
Security
An unauthorized access attempt is made. The attempt 
is denied and logged, and the system administrator is 
alerted within 15 minutes of the attempted breach.
All use cases
QA-3
Deployability
A system deployment is initiated. The deployment is 
fully automated and supports at least development, 
test, and production environments.
All use cases
QA-4
Performance
100,000 data points (where a data point is 120 bytes in 
size, on average) arrive at the system per second. The 
system processes them all within 1 second.
All use cases
QA-5
Performance
The real-time dashboard for Global Operators is 
refreshed automatically, at 15-second intervals, with 
less than 15 seconds data latency.
UC-1

9.3  The Design Process  195
ID
Quality 
Attribute
Scenario
Associated 
Use Case
QA-6
Performance
Industrial processes, coupled with real-time anomaly 
detection, are continuously visually inspected. 
Inspection results are received within 5 seconds. 
UC-2
QA-7
Performance
When a data analyst requests ad hoc SQL-like queries 
for raw and aggregated historical data (multirelational 
data that can include various equipment from different 
plants), 95% of queries return results within 30 
seconds.
UC-3, 4, 5
QA-8
Extensibility
New devices and sensors can be added to the system 
at runtime, with no interruption of ongoing data 
collection and system functionality.
All use cases
QA-9
Extensibility
New applications (e.g., Production, Quality, Supply 
Chain, Sales) can be created by building on existing 
services without the need to re-architect or change the 
system’s main components (such as Edge, Data, and 
AI Platforms).
All use cases
QA-10
Observability
Key performance metrics and error logs are collected, 
aggregated, and visualized in a monitoring dashboard 
with a maximum delay of 5 seconds.
All use cases
9.2.3  Constraints
Table 9.3 is a list of constraints that were placed on the system.
TABLE 9.3  Constraints on the Digital Twin Platform
ID
Constraint
CON-1
The system shall use existing company cloud infrastructure on AWS.
CON-2
Use a cloud-native strategy (prefer microservices, containers, and managed services) 
for building scalable applications.
CON-3
When selecting cloud services, give preference to the services that have the lowest 
and most predictable costs.
CON-4
NIST’s IoT cybersecurity capabilities must be implemented to manage cybersecurity 
risks.
9.3  The Design Process
Now that we have enumerated the architecturally significant requirements, we are ready to 
begin the first iteration of ADD.

196  Chapter 9  Case Study: Digital Twin Platform
9.3.1  ADD Step 1. Review Inputs
The first step of the method involves reviewing the inputs. They are summarized in Table 9.4.
TABLE 9.4  Inputs for the Digital Twin Platform
Category
Details
Design purpose
This is a greenfield system in a novel domain. The organization will perform 
development following an agile process with short iterations to quickly receive 
feedback and continue modifying the system. At the same time, an architectural 
design is needed to make conscious decisions to satisfy architectural drivers 
and avoid unnecessary rework.
Primary functional 
requirements
The use cases outlined in Section 9.2.1 have been reviewed. Of these,  
UC-1, UC-2, and UC-3 have been selected as primary. The other use cases, 
specifically UC-4 and UC-5, are slated for inclusion in future versions of the 
platform.
Quality attribute 
scenarios
The following table illustrates the priority of the quality attribute scenarios, as 
ranked by the customer and the architect (as discussed in Section 2.4.2). Of 
course, quality attribute scenarios with lower priorities exist, but they are not 
shown here.
Scenario ID
Importance to 
Customer
Difficulty of  
Implementation 
According to 
Architect
QA-1
High
Medium
QA-2
High
High
QA-3
Medium
Medium
QA-4
High
Medium
QA-5
Medium
Medium
QA-6
High
High
QA-7
Medium
Medium
QA-8
High
Medium
QA-9
High
Medium
QA-10
Medium
High
Constraints
See Section 9.2.3
9.3.2  Iteration 1: Reference Architecture and Overall System Structure
This section presents the results of the activities that are performed in each of the steps of the 
ADD method in the first iteration of the design process.

9.3  The Design Process  197
9.3.2.1  Step 2. Establish Iteration Goal by Selecting Drivers
Given that this is the first iteration in the design of a greenfield system, the iteration goal is 
to establish an initial overall structure for the system. The architect must keep in mind all of 
the drivers, but primarily the constraints and high-priority quality attributes. Note that in this 
chapter we follow a variant of the roadmap we presented in Chapter 4, modified to accommo-
date the very large scale of this system.
9.3.2.2  Step 3. Choose Elements of the System to Refine
As this is the initial iteration, we must refine the entire system. This system, as it has been 
envisioned, is expressed in the marketecture diagram (Figure 9.1), which suggests two major 
components to refine: Factory Floor (Edge) and Cloud.
9.3.2.3  Step 4. Choose Design Concepts That Satisfy the Selected Drivers
In this iteration, design concepts are selected based on the major components of the system: 
Edge and Cloud. They are summarized in Table 9.5.
TABLE 9.5  Design Concept Selection Decisions for the Digital Twin Platform: Iteration 1
Design Decision and 
Location
Rationale
Select a public cloud 
provider.
Based on CON-1, the system should leverage existing cloud 
infrastructure on AWS.
Choose the PaaS and 
FaaS service models. 
In the case of the platform as a service (PaaS) and function as a service 
(FaaS) resource consumption models, the cloud provider manages 
the infrastructure, facilitating the adoption of a cloud-native strategy 
(CON-2).
The system can still utilize infrastructure as a service (IaaS; i.e., virtual 
machines) for components that are not supported by PaaS or FaaS 
services.
Refer to Section 7.1.2 for more details.
Use edge computing. 
The following considerations are taken into account:
Network reliability: When dealing with mission-critical applications, 
network reliability becomes a crucial factor to allow local processing 
even when there is intermittent or no connectivity to the cloud. At 
least two use cases—UC-2 (quality inspection automation) and UC-5 
(advanced automation)—imply mission-criticality.
Latency reduction: Edge computing processes data closer to the 
source, resulting in lower latency. This is particularly important in 
applications that demand real-time data processing and decision-
making, such as computer vision (UC-2).
Bandwidth optimization: By processing data at the edge, only 
relevant information is sent to the cloud, reducing the amount of 
bandwidth required. This can help optimize network usage and decrease 
operational costs (UC-1, UC-2).
The alternative—connecting the devices directly to the cloud—
compromises reliability and is typically not used in manufacturing.
continues

198  Chapter 9  Case Study: Digital Twin Platform
Design Decision and 
Location
Rationale
Use the Digital Twin 
Maturity Model (DTMM) 
as a reference model to 
identify capabilities that 
can be matched with 
architectural elements.
The DTMM (see Figure 9.1) can serve as a reference model that guides 
the mapping of desired platform capabilities to architectural elements. 
Here is a breakdown of capabilities associated with the types of digital 
twins and stages of the DTMM:
Status Twin—Monitoring stage capabilities (UC-1):
■
■
Input and output device connectivity
■
■
Data ingestion, storing, processing, and analytics
■
■
Dashboard and visualization
Operational Twin—Analyzing (UC-2) and Predicting (UC-3) stages 
capabilities:
■
■
ML model training and inference
■
■
MLOps infrastructure
Simulation Twin—Prescribing (UC-4) stage capabilities:
■
■
Math and physical modeling
■
■
Industrial process and asset behavior simulation
Autonomous Twin—Autonomy (UC-5) stage capabilities:
■
■
Output device connectivity
■
■
Control and automation
Cross-cutting capabilities:
■
■
Access control (QA-2)
■
■
Future custom applications (shown in the marketecture diagram)
9.3.2.4  Step 5. Instantiate Architectural Elements, Allocate Responsibilities, and 
Define Interfaces
The instantiation design decisions considered and made are summarized in Table 9.6.
TABLE 9.6  Instantiation Decisions for the Digital Twin Platform: Iteration 1
Design Decision and 
Location
Rationale
Instantiate Edge 
elements based on 
selected capabilities 
from Step 4
From the list of capabilities in the DTMM identified in step 4, a subset of 
capabilities deemed to be most important is selected and instantiated as 
Edge elements (components and subsystems): 
Selected Capabilities
Edge Element
Future custom applications
Apps
Access control
Access Control
Control and automation
Control & Automation

9.3  The Design Process  199
Design Decision and 
Location
Rationale
Selected Capabilities
Edge Element
Data ingestion, storing,  
processing, and analytics
Data Store & Analytics
ML inference
ML Models & Inference
Input and output device  
connectivity
Input/Output
Instantiate Cloud 
elements based on 
selected capabilities 
from step 4.
A subset of the capabilities identified in step 4 are instantiated as Cloud 
elements (components and subsystems):
Selected Capabilities
Cloud Element
Access control
Centralized Access Control
Device management
Device Management
Connectivity with edge
IoT Message Broker
Data ingestion, storing,  
processing, and analytics
Data Platform 
Dashboard and visualization
Dashboard
ML model training and inference on 
cloud, MLOps infrastructure
AI Platform
ML model training and inference on 
cloud, MLOps infrastructure
AI Models 
Industrial process and asset 
behavior simulation
Simulator
Math and physical modeling
Math & Physical Models
Future custom applications
Apps
Due to the complexity of this system, defining the precise functionality and interfaces is 
not possible in this early iteration; it is left for later iterations.
9.3.2.5  Step 6. Sketch Views and Record Design Decisions
The diagram in Figure 9.3 shows the resulting reference architecture based on these instantia-
tion decisions.
A summary of each element’s responsibilities in the Platform presented in Figure 9.3 is 
recorded as shown in Table 9.7.

200  Chapter 9  Case Study: Digital Twin Platform
FIGURE 9.3  Digital twin platform reference architecture
TABLE 9.7  Edge elements of the Digital Twin Platform Reference Architecture
Edge Element
Responsibility
Apps
Applications for the employees on the manufacturing floor. This is available to 
users irrespective of connectivity with the cloud.
Access Control
Manage and enforce the policies that determine which users and devices are 
authorized to access and interact with the system at the plant level.
Control & 
Automation
Execute industrial control algorithms such as MPC (Model Predictive Control) 
or PID (Proportional Integral Derivative); orchestrate and automate industrial 
processes with decision-making on the edge device.
Data Store & 
Analytics
Local data storage on the edge device, allowing for the storage of both raw 
and processed data. Real-time data processing, such as filtering, aggregation, 
and transformation.
ML Models & 
Inference
Deployment and execution of machine learning models directly on the edge 
device.
Input / Output
Gather data from different sources such as industrial equipment, PLC 
(Programmable Logic Controller), SCADA (Supervisory Control and Data 
Acquisition) systems, sensors, and other devices using industrial protocols 
like OPC UA (Open Platform Communications United Architecture—a data 
exchange standard for industrial communication).
Send commands to actuators or other control elements within the industrial 
environment to enable automation of industrial processes.
Table 9.8 describes the elements and their responsibilities for the Cloud part.

9.3  The Design Process  201
TABLE 9.8  Cloud Elements of the Digital Twin Platform Reference Architecture
Element
Responsibility
Centralized Access Control Manage and enforce the policies that determine which users and 
devices are authorized to access and interact with the system globally 
at the organization level.
Device Management
A single point of control for all edge deployments: device provisioning, 
configuring, monitoring, and updating edge software.
IoT Message Broker
A high-throughput pub/sub messaging component that securely 
transmits messages to and from Edge instances with low latency.
Data Platform 
Ingest, store, process, and analyze data from various sources, 
including sensors, edge devices, and third-party software such as 
ERP (Enterprise Resource Planning), MES (Manufacturing Execution 
System), etc.
Dashboard
A user interface that visualizes and consolidates real-time and historical 
data; provides means for data analytics and insights.
AI Platform
A set of tools to build, train, test, and deploy AI models (using mostly 
ML methods).
AI Models 
Trained AI models for various tasks (e.g., computer vision, predictive 
maintenance, optimization, automation), for Cloud or Edge inference.
Simulator
Realistic virtual environments and “what-if” scenarios that accurately 
represent real-world conditions, enabling testing and optimization of 
processes and strategies without impacting actual physical assets.
Math & Physical Models
Representation of the underlying physics, dynamics, and behavior 
of assets, processes, or systems utilizing physical laws and domain-
specific knowledge.
Apps
Applications for company departments (and potentially for business 
partners) that leverage the platform capabilities.
9.3.2.6  Step 7. Perform Analysis of the Current Design and Review the Iteration 
Goal and Achievement of Design Purpose
The decisions made in this iteration address important early considerations affecting the over-
all system structure. Additional design decisions will need to be made in later iterations to 
further decompose the elements, select candidate technologies, and provide more details on 
how use cases and quality attributes will be supported.
Table 9.9 summarizes the design progress in iteration 1 using the Kanban board  
technique discussed in Section 4.8.2.
TABLE 9.9  Design Progress for the Digital Twin Platform: Iteration 1
Not 
Addressed
Partially 
Addressed
Completely 
Addressed
Design Decisions Made During the Iteration
UC-1
Introduced Edge Platform, Data Platform, and 
Dashboard elements. No detailed decisions on 
technologies to use have been made.
UC-2
Use Edge computing with ML inference. No detailed 
decisions on the technology have been made.
continues

202  Chapter 9  Case Study: Digital Twin Platform
Not 
Addressed
Partially 
Addressed
Completely 
Addressed
Design Decisions Made During the Iteration
UC-3
Introduced Data Platform, AI Platform, AI Models 
elements. No detailed decisions on technologies to 
use and XR device integration have been made.
UC-4
Introduced Simulator, Math & Physical Models 
elements. No detailed decisions on the technology 
have been made.
UC-5
Introduced Edge (Control & Automation), AI Platform, 
AI Models elements. No detailed decisions on 
technologies have been made.
QA-1
Use Edge computing for critical functions. No detailed 
decisions on the technology have been made.
QA-2
Introduced Access Control on the Edge and 
Centralized Access Control on the Cloud. No detailed 
decisions on the technology have been made.
QA-3
QA-4
QA-5
To be addressed in the following iterations.
QA-6
Use Edge computing with ML inference. No detailed 
decisions on the technology have been made.
QA-7
Introduced the Data Platform element. No detailed 
decisions on technologies to use have been made.
QA-8
To be addressed in the following iterations.
QA-9
Introduced Apps elements on the Edge and Cloud. No 
detailed decisions on the technology have been made.
QA-10
To be addressed in the following iterations.
CON-1
Confirmed that the type of Cloud is Public.
CON-2
The primary Cloud resource consumption models and 
PaaS and FaaS.
CON-3
CON-4
No relevant decisions made.
9.3.3  Iteration 2: Refinement of IIoT Elements
The second iteration of this design is focused on the IIoT (Industrial IoT) aspects of the system 
and the selection of technologies to support system requirements.
9.3.3.1  Step 2. Establish Iteration Goal by Selecting Drivers
Table 9.10 identifies the chosen drivers, based on the information identified in Section 9.2, to 
support the goal of this iteration. Additionally, for each driver, specific considerations with 
respect to the focus of the iteration are discussed.

9.3  The Design Process  203
TABLE 9.10  Drivers for the Digital Twin Platform: Iteration 2
Primary Use Cases
Architectural Driver
Specific IIoT Considerations
UC-1: Real-time monitoring 
and historical data insights
Although this use case pertains to the Global Operator, the data 
originates from, and is forwarded by, an edge computing system.
UC-2: Quality inspection 
automation
At least some components of the quality inspection automation system 
must be deployed on the edge, such as the sensing device.
Quality Attribute Scenarios
Architectural Driver
Specific IIoT Considerations
QA-1: Availability
The availability of the system in an offline mode, or in the event of 
limited Internet connectivity, is a key aspect of edge solutions, and 
a critical architectural driver for the edge segment. It produces a 
host of indirect requirements, such as sufficient local storage, data 
synchronization algorithms, and the ability to make local decisions on 
the edge that are critical for the operation of the industrial system.
QA-2: Security
In IoT systems, security through hardware is critical, and is typically 
achieved with components like Trusted Platform Modules or Hardware 
Security Modules. We assume the hardware aspects are covered and 
will be focusing on the software aspects of the architecture.
QA-3: Deployability
The importance of this driver can be understood when one considers 
that an industrial company may have dozens of plants, each with 
dozens of edge devices. The manual deployment of production 
systems will be either infeasible or extremely error prone.
QA-5: Performance  
(real-time dashboard)
Whereas this driver seems more directly pertinent to other 
architectural elements (such as dashboards), the capability of the IIoT 
system to deliver data sufficiently quickly is critical for these systems 
to achieve this goal. Therefore the IIoT architect should aim to achieve 
latencies much smaller than 15 seconds.
QA-6: Performance  
(visual inspection)
Latency on the order of hours could permit some cloud-based and 
batch-oriented approaches. But when we are dealing with seconds, it 
becomes evident that the driver will affect an edge architecture decision.
QA-8: Extensibility  
(new devices and sensors)
The ability to extend devices and sensors from a wide multitude of 
vendors is a key capability of all IIoT systems.
QA-9: Extensibility  
(new applications)
This driver is selected for the IIoT iteration because it is cross-
cutting. Indeed, it’s best for the boundaries of an application to be 
defined by the use cases and the domain knowledge required for its 
implementation, rather than the context of its execution (e.g., edge 
device, cloud IoT pipeline, ML pipeline).
QA-10: Observability
This cross-cutting driver will manifest itself in the perceived latency 
of a system monitoring dashboard, which is not a direct concern of 
this iteration. However, the dashboard cannot be responsive unless 
the IIoT backbone is performant. Therefore, this scenario must be 
considered in this iteration.
continues

204  Chapter 9  Case Study: Digital Twin Platform
Constraints
Architectural Driver
Specific IIoT Considerations
CON-4: NIST’s IoT cybersecurity 
capabilities must be implemented 
to manage cybersecurity risks.
This is a constraint that is specifically focused on edge, IoT, and 
security. (See the “Further Reading” section.)
9.3.3.2  Step 3. Choose Elements of the System to Refine
For this step, we choose the elements highlighted in blue in Figure 9.4 to be refined.
FIGURE 9.4  Digital twin platform reference architecture annotated for iteration 2
9.3.3.3  Step 4. Choose Design Concepts That Satisfy the Selected Drivers
Table 9.11 summarizes the design decisions made in this iteration regarding the selection of 
design concepts.

9.3  The Design Process  205
TABLE 9.11  Design Concept Selection Decisions for the Digital Twin Platform: Iteration 2
Design Decision and 
Location
Rationale
Perform computer vision 
on the edge.
This design concept pertains to performing computer vision on a 
dedicated piece of hardware in close proximity to the production line.  
The following quality attributes are affected by this decision:
1. Performance: By processing the video closer to the source (as 
opposed to processing it in the cloud), latency is significantly reduced. 
IIoT applications often require real-time or near-real-time analysis and 
response, so moving the processing closer to the data allows for quicker 
decision-making. Faster availability of these decisions means faster 
responses, which enhances operational efficiency and improves overall 
system performance.
2. Congestion avoidance: High-volume data generated by IIoT  
devices, especially visual sensors, can strain network bandwidth,  
if all the data is transmitted to a central location (such as a cloud 
component) for processing. By performing processing tasks closer  
to the source, we can avoid sending high-volume data over long network 
segments, thereby ensuring that we transmit only relevant information. 
This avoids network congestion, which can have adverse effects not  
only on this data streaming application, but also on other processes  
in the plant.
3. Security: Video streamed from production lines might contain trade 
secrets or other sensitive information. If we were to process the data in 
the cloud, the data stream would b exposed to the public Internet and 
hence many intermediaries. By processing data locally, near the data 
source, we can reduce the exposure to potential security breaches.
4. Resiliency: In case of an Internet link failure, data processing in 
the cloud will cease, but local data processing can continue. The 
accumulated data will be much easier to store and transmit to the cloud 
later, if needed.
In addition to these quality attributes, a benefit associated with this design 
decision is that it should be less costly.
Alternatives considered: Computer vision in the cloud.
Reason for discarding: Many quality attributes, such as performance, 
would be affected negatively if computer vision would be performed in the 
cloud.
continues

206  Chapter 9  Case Study: Digital Twin Platform
Design Decision and 
Location
Rationale
Adopt the Litmus Edge 
reference architecture.
The Litmus Edge reference architecture, depicted in Figure 9.5, focuses 
on the design of IIoT and incorporates many design concepts that are 
useful in the design of edge solutions.
 
FIGURE 9.5  Litmus Edge reference architecture
Relevant design concepts embodied by the Litmus Edge reference 
architecture are listed here, and mapped to elements in Figure 9.5. 
Design Concept
Associated Element
Object model for digital twins and 
meta-modeling: A model that can 
describe the physical world as a 
collection of interrelated objects 
with attributes that have a value 
at certain points in time. Meta-
modeling refers to the process of 
creating a description of the model 
itself.
Digital twins (Data)

9.3  The Design Process  207
Design Decision and 
Location
Rationale
Design Concept
Associated Element
Connectors: Components that 
are created to reliably link or join 
elements that do not have prior 
knowledge about each other. In 
our case, this provides extensible 
connectivity between IIoT devices 
and the Edge Platform.
DeviceHub and drivers (Southbound 
Integration)
Pub-sub architecture: Facilitates 
communication between 
components in a distributed system, 
where the actors in the system do 
not need to know about each other 
in advance.
Message broker (Routing)
Pipes and filters: Focuses on 
the flow of data through a series 
of processing steps. Each filter 
performs a specific task on the 
input data and produces an output. 
The filters are connected together 
through pipes, which act as the 
transport for data transfer between 
filters.
Flows manager (Applications 
Framework)
Central edge management: If a 
client has a vast fleet of IoT devices, 
located in a large number of 
factories, it cannot afford to create 
and maintain deployments at the 
level of individual edge devices. 
There is a need for an element that 
has a “one to many” relation to the 
edge devices.
Edge manager (external devices)
Alternatives considered: AWS Greengrass reference architecture.
Reason for discarding: AWS Greengrass is a lower-level product. It 
provides building blocks for creating IoT solutions, whereas Litmus Edge 
is a ready-made IoT platform. AWS Greengrass would be suitable in 
projects where a highly customized solution is desired; in our case, the 
IoT requirements are typical and straightforward, and readily satisfied by 
an existing generic solution.
9.3.3.4  Step 5. Instantiate Architectural Elements, Allocate Responsibilities, and  
Define Interfaces
In this iteration, instantiation is performed by choosing technologies to implement the selected 
design concepts. The instantiation design decisions considered and made are summarized in 
Table 9.12.

208  Chapter 9  Case Study: Digital Twin Platform
TABLE 9.12  Instantiation Decisions for the Digital Twin Platform: Iteration 2
Design Decision and 
Location
Rationale
Establish a contract 
between “IoT” and  
“Big Data and AI”.
In step 3 of iteration 1, we divided the system into two tiers: Cloud and 
Edge. The Cloud tier contains elements pertaining to Big Data and AI. 
In iteration 2, these are treated as black boxes. These elements will be 
further fleshed out in iteration 3. But before proceeding with architectural 
decisions, it is necessary to establish a contract between the “IoT” and “Big 
Data and AI” elements.
The responsibilities of these elements can overlap. For example, it is 
possible to do analytics and ML operations using IoT products on the edge 
as well as on the cloud. To reduce the ambiguity of responsibilities, the 
following decisions have been made.
For the implementation of analytic and ML use cases, the “Big Data and AI” 
technology stack will be prioritized, even where relevant capabilities can be 
developed on the edge.
For the implementation of IoT use cases, the “IoT” technology stack will be 
prioritized, even where relevant capabilities can, in principle, be developed 
using cloud services.
We also decided that the data flow from the “IoT” elements to the “Big 
Data and AI” elements will be based on the Kafka protocol. This protocol is 
widely supported and is consistent with the “pub-sub architecture” design 
concept selected in the previous step.
Use a smart camera to 
perform computer  
vision on the edge.
A smart camera is an edge device capable of real-time inference of custom 
computer vision models. The use of a smart camera is the instantiation 
of the “Computer vision on the edge” design concept. This addresses the 
requirement of using optical sensing (UC-2, QA-6). Locating the sensor 
and the computation for ML inference within the same device ensures that 
the required data bandwidth is available, and reduces the latency between 
obtaining the image and detecting the defect (for rapid alerting). As the smart 
camera supports custom ML models, it is possible to train and refine models 
that are specific to the product being monitored and its common defects.
The implementation of smart camera solutions is based on NVIDIA Jetson 
edge-GPU computers, which provide the necessary computational power 
to enable real-time ML inferencing for scenarios from image classification 
and object detection to image segmentation. With the open source NVIDIA 
TAO Toolkit, these solutions can be developed faster by leveraging 
pretrained AI models.
Instantiate the Litmus 
Edge reference 
architecture by using 
the Litmus Edge 
platform.
Litmus Edge is a ready-to-use implementation of the Litmus reference 
architecture, which was selected as a design concept. This decision 
is compatible with the previous design decision, because Litmus Edge 
supports custom applications; thus, it can host an application that drives 
the connected smart camera.
Litmus Edge is not free. However, in this case, it is more economical to 
buy a commercial product than to build the functionality in-house. The 
client is not in the business of writing software; for this company, the IoT 
platform is a means to achieve its business goal, and not a profit center. 
Therefore it has no incentive to invest in the development of a new system. 
Additionally, security is difficult and expensive to get right. And because 
security concerns are front and center in IoT, it is usually safer to rely on a 
battle-tested solution.
Use the Litmus Edge 
Manager product 
for the central edge 
management aspect 
of the reference 
architecture.
Litmus Edge Manager is a centralized management platform for edge 
devices and applications. It is used to set up and manage all aspects of 
multiple Litmus Edge deployments. This instantiates the “Central edge 
management” design concept from the reference architecture. Litmus 
Edge Manager is part of the Litmus product suite and integrates directly 
with Litmus Edge.

9.3  The Design Process  209
9.3.3.5  Step 6. Sketch Views and Record Design Decisions
The diagram in Figure 9.6 shows the result of the prior instantiation design decisions. In this 
diagram we highlight a number of elements.
FIGURE 9.6  Instantiation of the Litmus Edge reference architecture

210  Chapter 9  Case Study: Digital Twin Platform
Data Sources
The data sources are collections of industrial equipment on the factory floor, referred to as 
“Things” in Figure 9.6, that generate data and telemetry. They are summarized in Table 9.13.
TABLE 9.13  Data Sources for the Digital Twin Platform
Litmus Edge Elements
Element
Corresponding 
Element Before 
Refinement  
(Figure 9.4)
Responsibility/Description
Southbound Integration
DeviceHub
Input/Output
Allows data connections between devices and 
Litmus Edge. DeviceHub manages and uses 
“drivers” that implement connectivity with devices 
from major manufacturers.
Data
Digital twins
Data Store & Analytics Provides a representation of industrial assets, where 
an industrial asset is an object associated with 
industrial processes. Includes modeling that allows 
us to assign object attributes, define transformations 
of input data, and represent a hierarchy of attributes.
Normalization
Input/Output
Normalizes the measurement units and 
representation of data sent by different devices, 
ensuring a unified view to data consumers.
Application Framework
Custom containerized 
app support
Marketplace support
OTA updates
Apps
Control & Automation
Provides the ability to install, update, and launch 
Docker-based applications from public or private 
container image registries or marketplaces over the 
air (OTA).
Flows manager
Apps
Input/Output
Graphically defines the data connections and 
transformations between different components 
of the solution, such as devices, databases, 
information systems, and static files.
Northbound Integrations
Kafka integration
IoT Message Broker
Sends messages to clients that implement the Kafka 
TCP protocol (to be defined in iteration 3).
Litmus Edge Manager 
agent
Device Management
Centralized Access 
Control
Enables communication with Litmus Edge Manager.

9.3  The Design Process  211
Other Elements
Element
Corresponding 
Element(s) Before 
Refinement
Responsibility/Description
Applications
Quality inspection 
application
Apps
Control & Automation
ML Models & 
Inference
Implements an administrative and operational front-
end for the Quality Control Operator. Generates alerts 
about quality control failure events, per application 
requirements. Ensures the smart camera updates are 
compatible with, and can be received by, the cloud 
applications.
AWS
Big Data & AI 
elements
See iteration 3
A placeholder that will be instantiated during iteration 3.
9.3.3.6  Step 7. Perform Analysis of the Current Design and Review the Iteration  
Goal And achievement of Design Purpose
Table 9.14 summarizes the design progress and the decisions made during the second iteration.
TABLE 9.14  Design Progress for the Digital Twin Platform: Iteration 2
Not 
Addressed
Partially 
Addressed
Completely 
Addressed
Design Decisions Made During the Iteration
UC-1
 
Real-time pipeline of data has been organized from the 
IIoT devices to the cloud.
Pending: OEE monitoring will be designed in Iteration 3.
 
 
UC-2
Quality inspection is achieved using a smart camera 
that performs ML processing on the edge.
UC-3
UC-4
UC-5
Out of scope for this iteration.
 
QA-1
Neither Litmus Edge nor any of the applications affects 
the critical system functions for the production floor.
 
QA-2
 
Litmus Edge: Authentication, authorization, and RBAC 
(Role-Based Access Control) policies suitable for 
enterprise identity management and access control. 
Access control for all interfaces, including web UI. 
Quality control application: integration with Litmus 
Edge security facilities.
The Cloud part is pending in the subsequent iteration.
 
QA-3
Satisfied by using Litmus Edge + Litmus Edge 
Manager.
The Cloud part is pending in the subsequent iteration.
continues

212  Chapter 9  Case Study: Digital Twin Platform
Not 
Addressed
Partially 
Addressed
Completely 
Addressed
Design Decisions Made During the Iteration
QA-5
QA-6 
 
Performance was partially addressed during this 
iteration, especially QA-6, with the design decision 
of using a smart camera to perform computer vision 
on the edge, which decreases latency. Additional 
performance testing and analysis can be accomplished 
by creating a trivial skeleton implementation that 
involves the selected elements (as a proof of concept) 
and seeing how it fares under the projected load. 
QA-7
Out of scope for this iteration.
 
QA-8
Satisfied by using Litmus Edge (DeviceHub and device 
driver library). 
The Cloud part is pending in the next iteration.
 
QA-9
Satisfied by using Litmus Edge (application support). 
The Cloud part is pending in the next iteration.
 
QA-10
Satisfied by using Litmus Edge Manager (admin 
console dashboard, digital twins, Grafana dashboards, 
alerts with multiple event sinks).
The Cloud part is pending in the next iteration.
CON-1
CON-2
Out of scope for this iteration.
CON-3
Out of scope for this iteration.
CON-4 
 
Addressing all of the constraints related to NIST’s IoT 
cybersecurity capabilities is deferred to a subsequent 
iteration.
9.3.4  Iteration 3: Refinement of Big Data and AI Elements
The goal of this iteration is to address the Big Data and AI/ML aspects of the system and to 
select technologies to support system requirements. As usual in ADD, we start by considering 
architectural drivers in step 2, to determine how relevant they are for the Big Data and AI 
aspect of the system.
9.3.4.1  Step 2. Establish Iteration Goal by Selecting Drivers
All five of the use cases introduce requirements for the design of Big Data & AI/ML compo-
nents. We will review the use cases (in Table 9.15) and note which system elements should be 
considered as we move forward with element decomposition in step 3.
TABLE 9.15  Drivers for the Digital Twin Platform: Iteration 3
Primary Use Cases
Architectural Driver
Specific Big Data & AI/ML Considerations
UC-1: Real-time monitoring and 
historical data insights
Requires a centralized, cloud-based data repository. Requires 
a streaming data ingestion interface between the cloud and the 
plants, streaming data processing in the cloud, and visualization 
in the cloud.

9.3  The Design Process  213
Architectural Driver
Specific Big Data & AI/ML Considerations
UC-2: Quality inspection 
automation
Requires real-time sensor data collection and processing. 
Requires real-time AI/ML models for anomaly detection. Image 
processing for optical sensors can introduce channel throughput 
requirements if done in the cloud; this was one reason for 
selecting the “computer vision on the edge” option in the previous 
iteration.
UC-3: Predictive maintenance 
and XR for maintenance  
activities
Requires a historical data repository and near-real-time 
streaming and AI/ML inference capabilities similar to those for 
UC-1 and UC-2. Inference results should be consumable by 
extended reality devices; thus, they should be available via an 
API access layer.
UC-4: Process simulation
Historical data for raw materials, equipment settings, and output 
quality should be collected and available for manual or ML-based 
analysis for creation of simulation models.
UC-5: Advanced automation
The results of AI/ML-model inference and simulations should be 
consumable by automation agents. This may require API data 
access or the requirement to push messages from the data layer 
to components controlling equipment parameters. The control 
itself (and any ML-related decision-making for autonomy) should 
be executed on the edge device.
Quality Attribute Scenarios
Architectural Driver
Specific Big Data & AI/ML Considerations
QA-1: Critical system functions 
for the production floor must be 
available in the event of a cloud 
or network outage. (Applicable to 
UC-2)
Streaming sensor data collection, processing, and AI/ML 
anomaly detection models should be running on the Edge tier, 
not in the cloud.
QA-2: An unauthorized access 
attempt is made. The attempt 
is denied and logged, and the 
system administrator is alerted 
within 15 minutes of the attempted 
breach. (Applicable to all use 
cases)
Data and AI/ML components should be protected by 
authentication, authorization, access logging, and access alerting 
capabilities. Implementing those for every component or centrally 
is a matter of further tradeoff analysis. On-premises and cloud 
components might require different implementations.
With multiple system components, it might be better to 
introduce a centralized access management system rather than 
implementing authorization, authentication, and logging in each 
component. 
QA-3: A system deployment is 
initiated. The deployment is fully 
automated and supports at least 
development, test, and production 
environments. (Applicable to all 
use cases)
Deployment automation should be implemented for all Big Data 
and AI/ML components.
QA-4: 100,000 data points (where 
data point is 120 bytes in size, on 
average) arrive at the system per 
second. The system processes 
them all within 1 second. 
(Applicable to all use cases)
This amount of data should be accounted for in throughput 
planning for networking components and data processing 
components. Together with historical data retention requirements, 
it should be used to define storage capacity.
continues

214  Chapter 9  Case Study: Digital Twin Platform
Architectural Driver
Specific Big Data & AI/ML Considerations
QA-5: The real-time dashboard 
for Global Operators is refreshed 
automatically, at 15-second 
intervals, with less than 15 
seconds data latency. (Applicable 
to UC-1)
A streaming data processing solution design and near-real-time 
data repository query response time requirements need to be 
accommodated. The visualization layer should automatically 
refresh dashboards. 
QA-6: Industrial processes, 
coupled with real-time anomaly 
detection, are continuously 
visually inspected. Inspection 
results are received within 5 
seconds. (Applicable to UC-2)
The streaming data processing design for UC-2 (ergo on-
premises), including AI/ML inference, must be handled. We 
should clarify why the requirement is “within 5 seconds”; it is not 
obvious for anomaly detection, as “anomalies” can mean a series 
of sensor data that is longer than 5 seconds.
QA-7: When a data analyst 
requests ad hoc SQL-like queries 
for raw and aggregated historical 
data (multirelational data that can 
include various equipment from 
different plants), 95% of queries 
return results within 30 seconds. 
(Applicable to UC-3, 4, 5)
Considering the anticipated data volume, this creates response 
time and scalability requirements for the SQL engine on 
top of Cloud-based data repositories. This also introduces 
data modeling requirements such as proper partitioning and 
pre-aggregation, which in turn might require scheduled ETL 
processing and execution. 
QA-8: New devices and sensors 
can be added to the system at 
runtime, with no interruption 
of ongoing data collection and 
system functionality. (Applicable 
to all use cases)
The data model and technologies used for data repositories 
should be flexible enough to support new devices and new types 
of devices without data model modification. This might require a 
metadata repository to store the IoT asset hierarchy.
QA-9: New applications (e.g., 
Production, Quality, Supply 
Chain, Sales) can be created 
building on existing services 
without the need to re-architect 
or change the system’s main 
components (such as Edge, Data, 
and AI Platforms).
No influence on the design decisions for Big Data & AI/ML. 
Review at iteration 3, step 7 to ensure that no limitations were 
introduced by the design.
QA-10: Key performance metrics 
and error logs are collected, 
aggregated, and visualized in 
a monitoring dashboard with a 
maximum delay of 5 seconds.
This introduces observability requirements to Big Data and AI/
ML services. Most likely it should be implemented centrally, with 
all system components providing metrics/logs in near real time. 
Implementing this at the individual component level will most 
likely be inefficient.
Constraints
Architectural Driver
Specific Big Data & AI/ML Considerations
CON-1: Public cloud (AWS).
Services and technologies used in the solution should be specific 
to or compatible with the selected public cloud vendor. Particular 
thought must be given to the integration of the Edge layer and the 
AWS cloud. 
CON-2: Use cloud-native 
strategy (promote microservices, 
containers, and managed 
services) for building scalable 
applications. 
Cloud-native services and technologies should take priority when 
making tradeoffs. 

9.3  The Design Process  215
Architectural Driver
Specific Big Data & AI/ML Considerations
CON-3: Cost economy and cost 
predictability—specifically in 
selecting among cloud service 
alternatives.
Cost predictability is important for solutions using managed 
services with a pay-as-you-go (consumption-based) billing 
model.
To satisfy the cost predictability requirement, the solution should 
explicitly document each component’s/service’s cost model and 
factors influencing it, prioritizing those with less dependency on 
system workload.
CON-4: NIST’s IoT cybersecurity 
capabilities must be implemented 
to manage cybersecurity risks
Follow the recommendations of the Data Protection section of 
the NIST standard (see the “Further Reading” section), which 
focuses on the ability to encrypt the data. 
9.3.4.2  Step 3. Choose Elements of the System to Refine
Based on the system structure sketch (Figure 9.3) we defined in Section 9.3.2.5, four system 
elements (shown in Figure 9.7) are related to Big Data & AI/ML processing and should be 
refined in iteration 3.
FIGURE 9.7  Digital twin platform reference architecture annotated for iteration 3
The following Cloud elements will be refined in iteration 3:
■
■Data Platform
■
■Dashboard
■
■AI Models
■
■AI Platform

216  Chapter 9  Case Study: Digital Twin Platform
9.3.4.3  Step 4. Choose Design Concepts That Satisfy the Selected Drivers
Table 9.16 outlines the design decisions regarding design concepts selection performed in this 
iteration.
TABLE 9.16  Design Concept Selection Decisions for the Digital Twin Platform: Iteration 3
Design Decision and 
Location
Rationale
Introduce message 
queues for IoT sensor 
data transferring between 
Edge and Cloud layers, 
as well as between 
components of the cloud 
layer.
A message queue provides an asynchronous communication channel 
between the system elements. It simplifies the design by reducing 
the number of cross-component dependencies, as well as by 
reducing potential performance and scalability issues in synchronous 
communications. A message queue also acts as a message buffer in 
case of temporary unavailability of message consumers.
The decision to use Litmus Edge made in the previous iteration implies 
message queue usage on the Edge layer. Following UC-1 and QA-5, the 
Global Operator should see the changes from the factory floor within 15 
seconds of the event, which means that batching IoT sensor data on the 
edge and transmitting this data to the cloud on a schedule is not a viable 
option. The alternative of pushing data from the Edge layer directly to 
some cloud-based data repository is viable, but reduces the options for 
streaming data processing, which is required for UC-1 and QA-5 as well. 
Thus, introducing message queues for IoT sensor data at both Edge and 
Cloud layers is an optimal architectural decision.
Discarded Alternatives: Direct data ingestion from the Edge layer to a 
data storage service in the cloud, such as S3 object storage or one of the 
AWS database services. Such an approach would introduce undesirable 
strong coupling between solution components.
Use the Streaming Data 
Processing pattern to 
perform data processing 
operation on individual 
records.
The near-real-time monitoring and AI/ML inference requirements  
(UC-1, UC-2, QA-2, QA-5) do not allow for batch data processing. At the 
same time, the raw data obtained from IoT sensors requires additional 
cleansing, contextualization, and transformation for visual and AI-based 
analytics. Those calculations should be performed against individual 
messages from the message queue, which makes streaming data 
processing a good architectural choice.
Discarded Alternatives: Process data in batches according to a 
predefined schedule.
Separate data serving 
layers into near-real-time 
and historical data layers.
Many implementations of the popular Lambda architecture suggest 
combining historical and near-real-time data, and making all of this data 
available to a visualization layer. This masks the complexity of different data 
repositories and technologies used for each type of data, simplifying data 
consumption. However, the Lambda architecture comes with implementation 
complexities and technology choice limitations. In our use case, historical 
(UC-3, UC-4) and near-real-time (UC-1, UC-2) analytics are clearly 
separated. So, it makes sense to keep these types of analytics separate and 
allow for different technologies and services (e.g., data repositories, data 
access engines, visualization tools) to be used for each type. Thus, despite 
both historical and real-time data paths being present, we are not combining 
their data at the visualization or data virtualization layers.
It is worth mentioning that even with this decision, we can still satisfy UC-3 
(“predictions for equipment failures and diagnose them by accessing 
historical and real-time information”). UC-3 can be decomposed into two sub-
use cases: using historical data to train AI/ML models and using near-real-
time data to get predictions and detect failures. Each of those sub-use cases 
can work with its own data time scope.

9.3  The Design Process  217
Design Decision and 
Location
Rationale
Use different Edge and 
Cloud technologies for 
Big Data and AI/ML.
Having a minimal number of services and technologies in an architecture 
is, other things being equal, a desirable goal. It helps avoid duplication 
of implementation effort, skills acquisition effort, maintenance effort, 
and cost. In addition, CON-2 calls for cloud-native service usage, which 
requires us to consider technical stack unification based on those 
services.
However, we have two strong arguments against the use of a unified 
technology stack across the Edge and Cloud tiers:
■
■
The set of cloud-native services available in an on-premises 
environment is extremely limited. In fact, it includes only third-
party (often open source) technologies adopted by cloud providers 
and implemented as services or cloud-vendor-specific hardware 
appliances, which are not economically viable for installation across 
multiple factory environments.
■
■
The Litmus technical stack includes multiple services in Data Analytics 
& AI/ML, including message queues, a time-series database, a 
streaming data processing framework, visualization dashboards, and 
an ML model runtime environment. Using an alternative technology 
stack for the Edge tier would contradict the decision already made in 
the previous iteration.
The decision, therefore, is to separate the two technical stacks—for Data 
Analytics & AI/ML—utilizing Litmus Edge services on the Edge tier and 
following the CON-2 requirement for cloud-native services for the Cloud tier.
Discarded Alternatives: Use a similar set of open source technologies 
on the edge and in the cloud. While doable, such a decision contradicts 
CON-2 and would limit the choice of technologies by having a suboptimal 
common denominator for the Edge and Cloud tier options.
9.3.4.4  Step 5. Instantiate Architectural Elements, Allocate Responsibilities, and  
              Define Interfaces
The first step in making the instantiation decisions involves identifying the elements associ-
ated with the design concepts. Once these elements are identified, we can decide which AWS 
capabilities we will be using to implement them. These design decisions, as well as the ratio-
nale behind them, are listed in Table 9.17. Although the technology selection for some ele-
ments is predefined by the cloud provider requirement or by decisions made in the previous 
iteration, some still require additional tradeoff analysis.
TABLE 9.17  Instantiation Decisions for the Digital Twin Platform: Iteration 3
Design Decision 
and Location
Rationale
Create a message 
queue service 
and implement it 
with Amazon MSK 
(Managed Streaming 
for Apache Kafka).
The message queue service is responsible for data exchange between 
system components and message buffering, including those between the 
Edge and Cloud layers. This is an instantiation of the message queue design 
concept, related to use cases UC-1 and UC-5. It should expose at least an 
API interface to publishers and subscribers. It should also integrate with the 
near-real-time data repository and streaming data processing service.
Two primary alternatives for the message queue service in AWS are 
Amazon’s native Kinesis Data Streams and Amazon MSK. While both would 
be an adequate choice for this element, only MSK (Kafka) integration is 
supported by Litmus out of the box. 
continues

218  Chapter 9  Case Study: Digital Twin Platform
Design Decision 
and Location
Rationale
Create a near-real-
time data repository 
and implement it with 
Amazon Timestream.
This is an instantiation of the “No shared data service layer” design concept, 
and is related to use cases UC-1 and UC-2. It is responsible for storing and 
serving near-real-time data in the cloud. It should be capable of working with 
a comparatively short window of the most recent 24 hours of data. It should 
also integrate with the message queue service and the visualization service.
The near-real-time data repository contains a relatively small volume of 
information, limited by the sliding time window. It requires frequent and fast 
individual record updates. Because it is used for streaming data processing, it 
should also exhibit low latency for individual row operations (inserts, updates, 
row retrieval). The data in the repository will follow a typical time-series data 
pattern, without the need for complex documents or interconnected tables.
Such a combination of requirements, combined with the CON-2, “Use cloud-
native strategy” priority, leaves two major contenders: Amazon DynamoDB 
and Amazon Timestream.
Although the required data repository can be implemented with either of 
these technologies, Amazon Timestream has the advantage of supporting 
complex SQL queries and offering native support for time-series data 
analysis functions. This allows us to rely on SQL for aggregations that in 
DynamoDB would require additional data structures and additional coding to 
implement.
With cost and performance (for the amount of data that fits in-memory 
storage of Timestream) being comparable, Timestream appears to be a 
better choice. 
Create a streaming 
data processing 
service and 
implement it with 
Kinesis Data 
Analytics (Managed 
Apache Flink).
The streaming data processing service is an instantiation of the “Streaming 
data processing” design concept and supports UC-1, UC-2, QA-2, and  
QA-5. It is responsible for message transformations such as aggregations 
and deriving higher-level KPIs from lower-level device data. It should integrate 
with the message queue service, and should be able to access the asset 
hierarchy service data.
At the time of writing, the choice of technologies for streaming data 
processing (ignoring proprietary solutions) was between Apache Spark, 
Apache Flink, and Apache Beam. CON-2 (managed service usage) 
and CON-1 (AWS) narrows this choice down to the first two, which are 
represented in AWS by two managed services: AWS Glue (can leverage 
Apache Spark) and Kinesis Data Analytics.
As they offer closely matched capabilities, the choice between Spark and 
Flink often comes down to the skills and preferences of the development 
team. Having no such preferences documented, we will go with Apache Flink 
as the more streaming-oriented processing framework.
Create a visualization 
service and 
implement it with 
Amazon QuickSight.
The visualization service is not directly an implementation of the design 
concepts previously identified; however, it is necessary to support UC-1,  
UC-4, and QA-5. This service is responsible for data visualization for 
monitoring and analysis purposes. It should integrate with the near-real-time 
data repository and historical data repository.
The choice of visualization service technology is mostly predefined by 
CON-1 (AWS as a cloud provider) and CON-2 (managed service usage). In 
the absence of specific requirements for visualization types, security model, 
interactive capabilities, and so on, the native QuickSight service is a safe 
choice that can be revised later if needed. 

9.3  The Design Process  219
Design Decision 
and Location
Rationale
Create a historical 
data repository and 
implement it with 
Amazon Athena and 
S3.
The historical data repository is also an instantiation of the “No shared data 
service layer” design concept that supports QA-4 and QA-7. This repository 
is responsible for storing and serving historical data. It should be capable 
of working with large amounts of data, reflecting multiple years of system 
operations. It should integrate with the visualization service and the AI 
platform service, and should also provide ad hoc data querying capabilities to 
end users.
The two alternatives for large analytics data repositories are Amazon Redshift 
(a columnar MPP database) and Amazon Athena (AWS’s version of the 
Apache Presto SQL engine, running on top of S3 data and additional services 
like the Glue Data Catalog).
The basic use case for Amazon Redshift is a data-warehouse-like solution, 
with a well-defined data model and a set of predefined reports running on top 
of it. The use case for Amazon Athena is a data-lake-like solution, with ad hoc 
queries on top of raw or slightly preprocessed data.
As the second use case is closer to the patterns of historical data processing 
defined by UC-1, QA-7, and the need to train ML models, we will go with 
Amazon Athena.
Create an asset 
hierarchy service 
and implement it with 
Amazon DynamoDB.
This service is an instantiation of the “Different technologies on the edge 
and in the cloud” design concept, which directly supports QA-8. The service 
is responsible for storing, managing, and serving metadata on IoT devices, 
from individual sensors to machines to production lines to the factory level. 
Unlike the Edge service of the same name, it will be used for managing 
only the higher levels of hierarchy, starting from the factory and going up. It 
should consume and replicate lower levels from the Edge instances, provide 
capabilities to define and change the hierarchy to end users, and provide a 
data access interface to the streaming data processing service.
The asset hierarchy is a comparatively small dataset with a straightforward 
data model and without specific performance requirements or supported 
data access patterns. As such, a wide variety of technologies can be used 
to implement it. The selection of Amazon DynamoDB here is due to the fact 
that this database is used for asset hierarchy by Amazon’s IMC (Industrial 
Machine Connectivity) framework, which can be partially reused for our 
solution as well. 
Create an AI 
platform service and 
implement it with 
Amazon Sagemaker.
This service is also an instantiation of the “Different technologies on the 
edge and in the cloud” design concept that supports UC-2 and UC-3. It is 
responsible for AI/ML model development, training, quality control, evolution, 
and execution. It should provide capabilities to develop and evolve models 
to end users, have access to a historical data repository, and provide 
capabilities to export developed models for deployment on the edge.
Amazon Sagemaker is chosen because it provides a rich set of services for 
ML model development. 
continues

220  Chapter 9  Case Study: Digital Twin Platform
Design Decision 
and Location
Rationale
Use generative AI 
for maintenance 
and diagnostics 
(UC-3) with Amazon 
Bedrock.
Recently, a new service appeared on the AWS platform based on generative 
AI: Amazon Bedrock. After a thorough analysis of UC-3, the team conceived 
the idea of implementing an AI-powered Co-Pilot for maintenance engineers. 
The Co-Pilot’s purpose is to process equipment technical documentation and 
provide answers to users’ questions in natural language.
It was decided to integrate the Co-Pilot into the XR application for field 
maintenance by leveraging speech-to-text headset functionality, thereby 
saving time that might otherwise be spent on searching and reading 
documentation. AWS Bedrock enables this “talk to documents” scenario by 
providing a large language model (LLM) as a fully managed service. The 
user’s query, submitted as text, is received via an API. The corresponding 
equipment documentation is then retrieved and processed by the LLM to 
provide a reply that addresses the user’s question.
The previous decision to connect the XR device with the cloud, bypassing the 
edge, allows this scenario to be implemented with limited modifications.
9.3.4.5  Step 6. Sketch Views and Record Design Decisions
Figure 9.8 presents a simplified data flow diagram that shows the results of the instantiation 
step. The Cloud tier also includes the cloud resources used in each of the elements.
FIGURE 9.8  Simplified data flow diagram for the digital twins system

9.3  The Design Process  221
The data flow at the Cloud layer starts with Kafka integration passing the data collected 
on the edge to the message queue service. The messages are then picked up by the asset hier-
archy service (to build and store equipment inventory from individual messages) and by the 
streaming data processing service for cleansing, enrichment, aggregation, and other processing 
purposes. Next, the data is stored in two separate repositories for near-real-time and historical 
analysis. Finally, the data is used by the AI platform service for AI/ML model development 
and evolution, as well as by the visualization service to present the data to end users.
Note that the diagram in Figure 9.8 simplifies the real data flow for the sake of read-
ability and architecture clarity. For example, in practice the flow between the message queue 
service and the streaming data processing service is almost always bidirectional: Raw data is 
being processed and then put into the message queue again for the next layer of the processing 
pipeline to pick up.
Figure 9.9 provides a mock-up of a real-time dashboard for UC-1.
FIGURE 9.9  Real-time dashboard mock-up
A mock-up of the extended reality (XR) user interface for the maintenance activities 
described in UC-3 is shown in Figure 9.10.

222  Chapter 9  Case Study: Digital Twin Platform
FIGURE 9.10  The extended reality (XR) user interface for maintenance activities
9.3.4.6  Step 7. Perform Analysis of the Current Design and Review the Iteration  
              Goal and Achievement of Design Purpose
As in the previous iteration, we now can summarize the design progress and review the 
addressed architecture drivers. Table 9.18 outlines the progress made so far.
TABLE 9.18  Design Progress for the Digital Twin Platform: Iteration 3
Not 
Addressed
Partially 
Addressed
Completely 
Addressed
Design Decisions Made During the Iteration
UC-1
All the related elements are established and 
associated with cloud provider services. 
 
UC-2
UC-3
Although the design decisions provided the 
environment for AI/ML model development, this 
iteration did not cover AI/ML algorithm selection, which 
needs to be addressed in future iterations.
 
UC-4 
UC-5
Out of scope for this iteration.
QA-1
Covered in the previous iteration.
 
QA-2
Although not discussed explicitly, data access control 
is present in all the selected services and technologies. 
It is also integrated with AWS IAM (Identity & Access 
Management service) and Cloud Watch (logging and 
monitoring service).

9.4  Summary  223
Not 
Addressed
Partially 
Addressed
Completely 
Addressed
Design Decisions Made During the Iteration
QA-3
It is left for future iterations to make a selection 
between Amazon CloudFormation and Terraform as 
two primary IaaC approaches in AWS, as well as to 
design the CI/CD pipeline for the whole solution.
QA-4
Both Amazon Timestream and Amazon Athena with 
S3 storage are capable of storing and processing this 
amount of data. The selected data processing services 
scale horizontally and will be able to manage this 
amount of data as well. 
 
QA-5
Although Amazon QuickSight itself does not provide 
automatic dashboard refresh capabilities, it is 
often used for embedding reports into custom web 
interfaces. If that is the case, such a custom interface 
is also able to control the dashboard refresh frequency. 
 
QA-6
Out of scope for this iteration.
 
QA-7
Amazon Athena provides a SQL interface to query 
historical data. 
QA-8
Partially addressed by the introduction of the Asset 
Hierarchy architectural element. This should be further 
developed to define specific data models and edge 
messages used to extract equipment information. 
QA-9
Satisfied by the decisions made in this iteration (such 
as the selected AWS services) as well as the decision 
from the previous iteration (Litmus Edge).
QA-10
Addressed by inherent AWS capabilities and services 
integrability with Amazon CloudWatch as well as the 
decision from the previous iteration (Litmus Edge 
Manager).
CON-1
Fully addressed by using AWS as cloud provider of 
choice.
CON-2
All the specified cloud services are managed services. 
CON-3
This is only partially addressed, as a number of 
managed services were chosen in this iteration. A 
cost model based on workload parameters should be 
introduced later to address CON-3. This may result in 
some changes to the technologies that were selected.
CON-4
Out of scope for this iteration.
9.4  Summary
In this chapter, we explored an example of using ADD in the emerging realm of digital twins 
within an Industry 4.0 context. The complex nature of this domain blends areas like IoT, cloud 
computing, Big Data, AI/ML, XR, simulation, advanced automation, and robotics, present-
ing a multifaceted challenge. Such a broad spectrum surpasses the capabilities of any single 
architect. Each of the iterations was led by a different architect, but their decisions needed to 

224  Chapter 9  Case Study: Digital Twin Platform
be carefully aligned. After the first iteration, which focused on the overall structure of the  
system, iterations 2 and 3 were performed by architects with different backgrounds, who ana-
lyzed the considerations for each chosen driver. This was a step that was omitted in the case 
study in Chapter 8, but it was deemed to be essential to managing complexity and aligning 
decisions between the iterations for this project.
The decisions made in this case study were shaped by the diverse design concepts, pat-
terns, and technologies from each discipline. The rapidly evolving nature of this domain means 
that established information sources, such as reference architectures and vendor documenta-
tion, cannot possibly offer direct guidance for satisfying all architectural drivers. Thus, the 
architects needed to perform experiments and build prototypes to arrive at their architectural 
decisions.
Digital transformation programs, as described in this case study, are typically multi-year 
journeys. The architects involved should be prepared to periodically review their decisions, as 
new technologies or updated versions are bound to emerge during the implementation, pre-
senting promising business and architectural opportunities. This case illustrates how, within 
a relatively short period of time, architects can witness the emergence of new technologi-
cal capabilities, necessitating a proactive approach to continuously integrate technological 
advancements into their designs.
9.5  Further Reading
You can read about Industry 4.0 here: https://en.acatech.de/publication/industrie-4-0-maturity- 
index-update-2020/.
A short essay entitled “The Rise of Digital Twins: How Software Is Eating the Real 
Economy” discusses digital twins and their many use cases: https://info.softserveinc.com/
hubfs/files/the-rise-of-digital-twins.pdf.
More information about the Litmus Edge IoT platform can be found at https://litmus.io/
litmus-edge/.
The AWS Architecture blog includes a very interesting “Manufacturing” category: https://
aws.amazon.com/blogs/architecture/category/industries/manufacturing/.
The NIST standard describes IoT cybersecurity capabilities including recommendations 
for data protection: https://pages.nist.gov/IoT-Device-Cybersecurity-Requirement-Catalogs/.
A discussion of how and when to prototype as part of the architecture design process can 
be found in H.-M. Chen, R. Kazman, and S. Haziyev, “Strategic Prototyping for Developing 
Big Data Systems”, IEEE Software, 2016.
A design concepts catalog that includes some of the reference architectures and technol-
ogies mentioned in this case study is part of the Smart Decisions Game, which is described 
in H. Cervantes, S. Haziyev, O. Hrytsay, and R. Kazman, “Smart Decisions: An Architectural 
Design Game”, Proceedings of the International Conference on Software Engineering (ICSE) 
2016, (Austin, TX), May 2016. More information can be found at http://smartdecisionsgame.
com.

9.6  Discussion Questions  225
9.6  Discussion Questions
1.	
What differences do you see in the application of ADD to a very large system such as the 
one described in this chapter compared to the case study in the previous chapter?
2.	
In this chapter, architects with different areas of expertise collaborate. What are the 
challenges with ensuring that the decisions made by different architects in different parts 
of the system are consistent and compatible?
3.	
How can ADD be applied in domains such as IoT, ML or Big Data, as used in this case 
study? Consider aspects such as drivers, designing using iterations, and domain concepts. 
Do they exist in these different domains?

This page intentionally left blank 

227
10
Technical Debt in 
Architectural Design
In this chapter, we focus on design debt—and particularly on cases where that design debt 
is unintentional debt. We will look at the ways in which properly thought-out architectural 
design decisions can help to avoid or remediate this debt. We start with an introduction to tech-
nical debt, followed by a discussion of its roots, and then a consideration of how debt can be 
addressed through refactoring and redesign. We also discuss how ADD can be used to design 
with technical debt in mind. 
Why Read This Chapter?
You have almost certainly heard about technical debt, and you may already have some 
notion of what it is and why it is bad. But all too often, there is little discussion of how to 
design to avoid debt, or how to redesign, via refactoring, to reduce debt. In this chapter, 
you will learn about design debt and how it can be managed using ADD.
10.1  Technical Debt
Technical debt is the often unintended burden of complexity that every software project accu-
mulates over time. We do not intend to add debt to a system as we develop, maintain, debug, 
and extend it, but it happens just the same. It happens because software is complex, with lots 
of moving parts; because dependencies among the parts are often indirect and invisible; and 
because our understanding of this complexity is limited in scope and imperfect. Sometimes 
we (again, unintentionally) add to this complexity because we are focused on our task at hand, 
fixing a bug or implementing a feature, and we don’t realize that our efforts are decreas-
ing the conceptual integrity of the system. Perhaps we implement “nearly but not quite the 
same” functionality as already exists in some other part of the system. Perhaps we make a 
direct dependency between two parts of the code base without realizing that we should have 
employed an abstraction, via a published API. These kinds of degradations of an architecture 

228  Chapter 10  Technical Debt in Architectural Design
happen all the time, typically without developers being aware they have contributed to the 
system’s technical debt.
At other times, we consciously choose to add technical debt—doing some expeditious 
hack in pursuit of meeting a deadline. And perhaps we do this with the intention of going back 
later and cleaning it up and “paying back” the debt. This is a completely reasonable strategy, 
particularly for startups or exploratory prototypes or when you are faced with a hard deadline 
and missing that deadline has serious business consequences.
You can acquire many different kinds of technical debt in a complex software project—
code debt, documentation debt, deployment debt, testing debt, and so on.
10.2  The Roots of Technical Debt in Design
Design debt—that is, technical debt in design—can have its roots in many sources. It can 
originate from drivers that are not well defined, particularly quality attributes. It can emerge as 
a by-product of the pressure to deliver quickly, so not much of the project schedule is devoted 
to design. In such cases, developers feel as if they are making more progress by delivering 
features, irrespective of the quality of the underlying architecture. This attitude is reinforced 
in most projects because backlogs typically contain only features and bugs, but not “enablers” 
of architecture.
Even when design is a priority, unintentional debt can accumulate because the design pro-
cess is complex: There is an abundance of design concepts, with no clearly superior options, 
and making good choices among them may require a great deal of analysis and prototyping 
(which is frequently omitted, as it may seem to “get in the way” of progress). And even when 
the design concepts are chosen wisely, there may be a lack of understanding of the architec-
ture by developers, resulting in instantiation and implementation mistakes that undermine the 
architectural vision. For example, we have seen many cases where the “as implemented” archi-
tecture diverges wildly from the “as designed” architecture.
And design is not always done well. A survey of 1831 software practitioners (developers 
and architects) done by the Software Engineering Institute found that bad architectural choices 
were the greatest, most problematic sources of debt in their projects. Furthermore, as a system 
evolves, there are many ways in which design debt can be introduced and can accumulate. 
These all involve inadequate attention to coupling and cohesion, in some form:
■
■“Improper separation of concerns”: Multiple responsibilities are implemented in the 
same class or module, making the class or module bloated, difficult to understand, test, 
and modify.
■
■“Clone and own”: A piece of code is copied (cloned) and then modified, typically 
because the programmer does not understand the dependencies on the original code and 
so fears to modify it or take time to understand it.
■
■“Tangled dependencies” (sometimes called “big ball of mud”): Little thought is given to 
architectural integrity, and dependencies are added (unintentionally) over time, eventu-
ally resulting in unmanageable complexity.

10.2  The Roots of Technical Debt in Design  229
■
■“Unplanned evolution”: New features and bug fixes are added without any consideration 
of how these changes will affect the overall structure and conceptual integrity of the  
system. This is rather common today in many teams following an Agile methodology 
where effort is devoted to addressing the immediate, pressing issue—implementing a 
customer-facing feature or fixing a bug. Although this customer focus makes perfect 
sense in the short run, developers typically pay little attention to the impact of such 
changes in terms of the maintainability, understandability, and modifiability of the code 
base over time.
This list is not exhaustive, but these are some of the most common causes of design debt 
in the maintenance phase of a system. Now you might ask, why do these practices lead to debt, 
and how do we know this? Quite simply, they lead to debt because they violate the principles of 
good design: These practices tend to increase coupling or decrease cohesion. They degrade the 
modular structure of a system, which makes the system harder to understand and modify, and 
because systems continuously evolve, this degradation of modifiability is highly problematic.
To understand why this is the case, let’s take a quick look at the SOLID principles. These 
principles have been around for more than two decades and have come to represent the high-
est ideals of good design. The SOLID acronym derives from the first letter of each principle: 
Single-responsibility, Open-closed, Liskov substitution, Interface segregation, and Dependency 
inversion. These principles were originally devised with a focus on object-oriented program-
ming, but actually transcend any particular technology or design approach. Let’s look at these 
principles in terms of their implications and their consequences:
■
■Single-responsibility principle: There should never be more than one reason for a mod-
ule to change. This means that a module should have high cohesion.
■
■Open–closed principle: Software entities should be open for extension, but closed for 
modification. A module is “open” if it can be extended—for example, by inheritance—
which means that the original module is cohesive, doing just one thing. For a module to 
be closed for modification, it must expose a well-defined interface, which limits coupling 
opportunities.
■
■Liskov substitution principle: Functions that use references to base classes must be able 
to use objects of derived classes without knowing it. This again means that a function 
can depend only on the class’s interface, which limits coupling.
■
■Interface segregation principle: Clients should not be forced to depend on interfaces 
that they do not use. This implies that large, complex interfaces should be split up into 
smaller, more focused ones. This is management of cohesion.
■
■Dependency inversion principle: Code should depend on abstractions, and not concre-
tions. Following this principle naturally leads one to design software with low coupling.
In each case, the five principles that encompass SOLID are specific ways to think about 
and to achieve low coupling and high cohesion. David Parnas wrote about this issue more than 
50 years ago in his seminal paper, “On the Criteria to Be Used in Decomposing Systems into 
Modules”. He was talking about coupling and cohesion back then, although he did not use 
those terms. Five decades later, the software industry is still struggling to achieve these goals. 
Why is this?

230  Chapter 10  Technical Debt in Architectural Design
There are many possible answers to this question, but we boil it down to two major issues: 
incentives and lack of measurements.
Let us consider a similar question to put this into perspective. Why is the population of 
the United States the most obese out of all the world’s major economies? Nearly 70 percent of 
Americans are obese or overweight. And why, according to the Centers for Disease Control 
and Prevention, does the U.S. population keep getting more overweight with each passing 
year? This epidemic of obesity costs many billions of dollars each year. Our claim is that the 
real problem is incentives. Quite simply, we humans love our cakes and our ice cream and our 
sweet drinks. Sweets trigger a dopamine reaction in our brain. We can quite easily become 
addicted to them. We are genetically incentivized to seek out sweets. And while we can con-
trol such responses—our genetics do not determine our destiny—without good feedback and 
direction, we are unlikely to do so. Our nature is to seek them out.
What is the connection with software development? Developers are measured by out-
comes such as number of features shipped or bugs fixed in a unit of time, number of commits 
or lines of code committed per unit time, or more global measures such as cycle or sprint time. 
Each of these outcomes—fixing a bug, making a commit, implementing a feature—is a short-
term behavior that is rapidly rewarded; it is the software equivalent of a shot of dopamine. 
Why wouldn’t we want more of this stuff? Some organizations track each developer’s software 
velocity and reward high achievers and penalize low achievers. Naturally, people will respond 
to such incentives.
Furthermore, humans respond to short-term, immediate gratification over longer-term, 
broader goals. We all know we should eat better and get more exercise. But the couch looks 
comfy and those cookies are yummy. Cookies and the couch win more of the time, sadly.
In fact, the situation is even worse in software development than in our analogy  
with human health. At least with our own bodies we can see ourselves in the mirror, we can 
stand on the scale and see if the numbers have gone up, we can feel our clothes becoming too 
tight, and we can acknowledge that we are out of breath after climbing a flight of stairs. What 
are the equivalents in software development that might give us similar feedback? Yes, some 
organizations—rare ones—do measure their software development velocity, as we just said. 
And that’s great—it’s a start. These organizations can use this data to incentivize (and punish) 
developers, but they could also use such data to view and track the accumulated consequences 
of their bad software development habits. But even those organizations that do track their pro-
ductivity seldom understand exactly how they have gotten into this mess and even less often 
have a concrete plan for getting back into shape.
Although organizations may measure the consequences of their technical debt, they are 
not measuring the underlying causes of their problems: their increased coupling and decreased 
cohesion, or the lack of time or knowledge to make adequate design decisions and good archi-
tectural choices. And these are design issues. They occur due to a lack of attention to design, 
perhaps when an architecture is first conceived, or perhaps as the architecture evolves. But 
technical debt is, to a large extent, a design problem. How do we deal with this problem? 
First, technical debt needs to be recognized up front as a design driver. If so—if we have 
collected some maintainability or modifiability scenarios—or if it is recognized as a concern 
by the architect, then we can justifiably design an architecture to have low coupling and high 

10.3  Refactoring and Redesign  231
cohesion, or we can provide adequate support for making appropriate design decisions. We do 
this through the application of appropriate patterns and tactics and by following a systematic 
design process such as ADD, with enough time to perform it adequately.
Second, to maintain an architecture with low coupling and high cohesion—that is, one 
with a lower probability of accumulating technical debt—we need to monitor it for signs of 
debt and take appropriate action when the architecture shows signs of deterioration. You can 
think of this as your annual checkup with the doctor. The doctor may take your blood pressure, 
do blood tests, listen to your heart, and so forth. Each of these tests is done to determine the 
early warning signs of a potential problem. These tests are done so that appropriate remedial 
measures can be adopted. In the case of our bodies, the appropriate remedial measures might 
be diet or exercise, or cholesterol-lowering medication. For our software systems, the appropri-
ate measures are typically refactoring and redesign.
10.3  Refactoring and Redesign
Kruchten and colleagues have written that a project’s backlog—and hence the investments and 
effort that will be allocated to software in the future—may have positive or negative value, and 
may be visible or invisible. These possibilities are depicted in Figure 10.1. If some part of the 
software has positive value and is visible, we call this a feature. If it has negative value and is 
visible, we call this a bug. So far, so good. But the invisible parts of software are trickier. If the 
software has positive value and it is invisible, that is architecture and infrastructure: We need 
it, but no end user is ever aware of it. It is simply part of the cost of doing business. And if the 
software is both invisible and has negative value, that is technical debt. 
Product
Feature
Architecture/
Infrastructure
Technical
Debt
Bug/
Defect
Value
Visibility
high
high
low
low
FIGURE 10.1  Value and visibility: the dimensions of entries in your project backlog

232  Chapter 10  Technical Debt in Architectural Design
Everyone understands the difference between bugs and features, and there is little  
discussion that we should minimize the former and maximize the latter. By comparison, the 
tension between architecture and technical debt is more subtle and fraught. Why is this? To 
examine this question, let us return to the earlier discussion about incentives.
All software systems accumulate debt over time. This is one of Lehman’s Laws of 
Software Evolution: “As [a system] evolves, its complexity increases unless work is done to 
maintain or reduce it”. Systems always trend toward disorder—you can think of this as a kind 
of “entropy”. This can be counteracted, as Lehman pointed out. To counteract architectural 
degradation, we need to refactor, in an effort to restore order (see the sidebar “Refactoring”). 
This refactoring can take many forms: removing unwise dependencies, reorganizing function-
ality to make it more cohesive, splitting large modules into smaller ones, creating abstractions 
with published APIs, changing a technology that was previously selected incorrectly, and so 
forth. Each of these refactorings comes at a cost, of course. 
Refactoring
If you refactor a software architecture (or part of one), what you are doing is maintain-
ing the same functionality but changing some quality attribute that you care about. 
Architects often choose to refactor because a portion of the system is difficult to  
understand, debug, and maintain. Alternatively, they may refactor because part of the 
system is slow, or prone to failure, or insecure.
The goal of the refactoring in each case is not to change the functionality, but  
rather to change the quality attribute response. (Of course, additions to functionality are 
sometimes lumped together with a refactoring exercise, but that is not the core intent of 
the refactoring.)
Clearly, if we can maintain the same functionality but change the architecture to 
achieve different quality attribute responses, these requirement types are orthogonal to 
each other—that is, they can vary independently.
Take a hypothetical project that is humming along, implementing features and swatting 
bugs. Perhaps the development velocity per developer is going down, but the project manager 
can often combat this with hiring or better testing or techniques such as code reviews. This 
manager is almost certainly incentivized to produce more features, and quickly. Now suppose 
that an enlightened developer looks at the project’s history, notes that the feature velocity (per 
developer) has gone down, and both bug rates and bug resolution time have gone up. So the 
developer comes to their manager and offers to “pay down” some of the accumulated debt 
via refactoring. How does the manager react? The manager has just been given a proposal to 
spend precious developer resources on the implementation of zero new features. Most manag-
ers, faced with this proposal, will say, “Hell no! Get back to work, implementing features and 
swatting bugs!” Why? Again, because of incentives and the lack of appropriate measures. Few 
managers are incentivized to have a cleaner, more coherent, better organized code base.

10.4  Technical Debt and ADD  233
In contrast, if the developer had been able to present a business case to their manager, 
showing the return on investment of the refactoring, perhaps then the discussion would 
have gone differently. If the developer had been able to say, “With this investment of 3  
person-months of effort, we will increase our development velocity by 30 percent, and achieve 
a 300 percent ROI in the first year alone”, they might have gotten the green light. Few proj-
ects actually have this discipline, and few projects collect the data necessary to make such an 
assessment. But they should. We have, in fact, been part of some engagements that did exactly 
this, with the help of tools to help make the business case, such as DV8 (see the “Further 
Reading” section).
Let us now turn our attention to the relationship between technical debt and ADD. In 
essence, we need to ask ourselves: How can we design to “future proof” our systems as much 
as possible, so that they accumulate less debt? And, as it turns out, ADD can help.
10.4  Technical Debt and ADD
The ADD method is not aimed at any single quality attribute or concern; however, its activities 
can be easily focused on specific concerns. In this section, we walk through some of the steps 
of ADD and discuss how these steps can be adapted so that they specifically address technical 
debt concerns. In what follows we will focus on the quality attribute of modifiability, but these 
techniques can be applied to other quality attributes as well.
Step 1: Review Inputs
In this step, we need to make sure that one of the concerns included in the inputs to the 
design process is to avoid debt. In fact, we believe that this should be a concern in almost 
every complex software project. We will achieve this in our design process, as we will 
see, by making design decisions carefully. For example, we deliberately choose tactics 
and patterns so that coupling is minimized and cohesion is maximized. These are nec-
essary but not sufficient conditions. If a beautiful design is created but developers do not 
follow it, if the design is not adequately communicated and socialized, if the design is 
undermined by subsequent maintenance activities, or if design decisions are not docu-
mented, then any goodness baked into the original design will be undermined.
For example, in the Hotel Pricing System (see Chapter 8), a future requirement 
involves doing analytics with respect to price changes. It is critical that this evolutionary 
scenario is collected as part of this step.
Step 2: Establish Iteration Goal
In this step, drivers are chosen that will be the focus of the current iteration. An  
architect who is concerned about technical debt should, at this point, elicit or create 
modifiability scenarios that characterize the ways in which a system is expected to 
evolve. When eliciting or generating scenarios, we should always consider three condi-
tions: (1) expected uses of the system; (2) expected, planned growth cases for the system; 
and (3) exploratory scenarios, where more speculative aspects of the system’s evolution 

234  Chapter 10  Technical Debt in Architectural Design
can be considered. Although the future state of the system can never be fully predicted, 
the mere consideration of such scenarios will steer the design process in the right 
direction. To put it another way, when such possible states are not considered, it is highly 
unlikely that design decisions will be put into place to “future-proof” the system. For 
example, in the Hotel Pricing System (Chapter 8), an evolutionary scenario that focuses 
on the introduction of an analytics component that helps users in changing prices may be 
created (Table 10.1).
TABLE 10.1  Modifiability Scenario for the Hotel Pricing System 
ID
Quality 
Attribute
Scenario
Associated 
Use Case
QA-10
Modifiability
A component that performs different types of analyses 
on price changes is introduced to the system. The 
component is added successfully without requiring 
changes in the code or interruptions in operation.
HPS-2
Steps 3–5: Choose and Instantiate Elements
In choosing and instantiating architectural elements, the wise architect will consider 
modifiability tactics and patterns (see section 3.5). One of the most common ways that 
technical debt takes root in a complex code base is through unbridled dependencies. 
Most of the modifiability tactics exist to combat this—to reduce coupling and to increase 
cohesion. An example of a modifiability tactic that increases cohesion is “split module”, 
where a large, non-cohesive module is split into smaller, more cohesive ones. This is 
a typical activity undertaken in refactoring but can also be done proactively in design. 
Several modifiability tactics, as shown in Figure 3.7, directly address reducing coupling, 
such as encapsulate, use an intermediary, abstract common services, and restrict depen-
dencies. Thus, these tactics, and their related patterns, should be highly prioritized when 
choosing architectural elements. For example, the Client-Server, Publish-Subscribe, and 
Microservices patterns all serve to reduce binding among architectural elements.
Also, when choosing and instantiating elements, a wise architect will carefully study 
the pros and cons of the alternatives that are considered before making any decisions. 
This may involve, for example, devoting time to investigating a particular technology, 
building prototypes that allow a hypothesis to be tested, or doing lightweight architec-
ture analysis, so that a decision can be made based on well-founded arguments.
For the scenario described in step 2, the decision was to architect the system around 
the event-based CQRS pattern (see Section 7.3.3.2).
Step 6: Sketch Views
When we sketch views, we usually include the rationale for the design decisions taken. 
This rationale can help others in assessing why a design choice was made—perhaps it 
was an arbitrary choice, or one that was rushed. In such a case, it might be revisited,  
particularly if it is causing problems and incurring debt. Furthermore, the ratio-
nale might include assumptions about the evolutionary path of the system, or about 

10.5  Summary  235
non-obvious dependencies. By consciously adding this kind of information, debt can be 
minimized or appropriately dealt with in the future. This rationale will also help guide 
and constrain implementation choices.
Step 7: Perform Analysis
If this project comprises greenfield development, the analysis undertaken can be  
scenario-based, where modifiability scenarios are mapped onto the architectural descrip-
tion to determine how well they will be satisfied. Scenario-based analysis is relatively 
low cost and easy to perform (see Section 11.6). It requires no tooling more sophisticated 
than a whiteboard. If this is brownfield development, however, in addition to perform-
ing the preceding analyses, the team can mine the project’s existing data. For example, 
project members can mine the revision history to pinpoint the areas of the architecture 
that are incurring the greatest effort—that is, the greatest numbers of changes and churn. 
They can also run analysis tools over the existing code base to identify architectural 
flaws, such as cyclic dependencies, modularity violations, unstable interfaces, and so 
forth. These identified problem areas can then be made the focus of further design (and  
perhaps refactoring) activities.
Considering the scenario described in step 2, the decision of architecting the system 
around the event-based CQRS pattern is deemed appropriate to support QA-10. The price 
change events can be fed to an analytics tool from the cloud provider, or an additional micro-
service focused on this task can be created. The introduction of either of these two solutions 
does not impact the existing components in the system. Furthermore, it does not require the 
system to be shut down, as the analytics component can read historical events from the log.  
A scenario-based walkthrough describing this approach convinced the stakeholders that this 
was a good solution.
10.5  Summary
In this chapter, we provided a brief overview of technical debt and zoomed in on the kind of 
debt that we are most concerned about: design debt. This debt is ubiquitous in software sys-
tems and can, over time, cause a code base to collapse under its own weight. We argued that 
this “entropy” is caused, albeit often unintentionally, by bad design choices and by developers’ 
actions that undermine the coupling and cohesion of the software. We further argued that 
technical debt can be counteracted only by an investment in refactoring. But it is often difficult 
to get management approval for such refactoring because the costs of technical debt are usu-
ally invisible, and most projects do not have the discipline, and do not collect the data, to make 
the business case for paying down the debt.
Finally, we showed how ADD can be used to proactively design with technical debt 
avoidance or mitigation in mind. All it takes is the right mindset. The steps of ADD can be 
tailored, with little effort, to focus on avoiding and remediating debt. That is the good news.

236  Chapter 10  Technical Debt in Architectural Design
10.6  Further Reading
A brief introduction to the metaphor of technical debt and its consequences can be found  
in Ipek Ozkaya, Philippe Kruchten and Robert L. Nord, “Technical Debt: From Metaphor to 
Theory and Practice”, IEEE Software, 29(6), 2012, 18–21.
Comprehensive overviews of technical debt in its many dimensions and manifestations 
can be found in N. Ernst, J. Delange, and R. Kazman, Technical Debt in Practice—How to 
Find It and Fix It, MIT Press, 2021; and in P. Kruchten, R. Nord, and I. Ozkaya, Managing 
Technical Debt: Reducing Friction in Software Development, Addison-Wesley, 2019.
A discussion of the survey of practitioners on technical debt carried out by the Software 
Engineering Institute can be found at https://insights.sei.cmu.edu/blog/a-field-study-of- 
technical-debt/.
The tactics and patterns mentioned in this chapter, and many more as well, can be found 
in L. Bass, P. Clements, and R. Kazman, Software Architecture in Practice, 4th ed., Addison-
Wesley, 2021.
Manny Lehman’s Laws of Software Evolution are described in M. Lehman, “On 
Understanding Laws, Evolution, and Conservation in the Large-Program Life Cycle”, Journal 
of Systems and Software, 1, 1980, 213–221.
A case study of an organization successfully refactoring to pay down technical debt can 
be found in M. Nayebi, Y. Cai, R. Kazman, G. Ruhe, Q. Feng, C. Carlson, and F. Chew, “A 
Longitudinal Study of Identifying and Paying Down Architectural Debt”, Proceedings of the 
International Conference on Software Engineering (ICSE), May 2019.
A discussion of architectural flaws, their costs, and their detection can be found in L. 
Xiao, R. Kazman, Y. Cai, R. Mo, and Q. Feng, “Detecting the Locations and Predicting the 
Costs of Compound Architectural Debts”, IEEE Transactions on Software Engineering, 48(9), 
September 2022; and in R. Mo, Y. Cai, R. Kazman, L. Xiao, and Q. Feng, “Architecture Anti-
patterns: Automatically Detectable Violations of Design Principles”, IEEE Transactions on 
Software Engineering, 47(5), May 2021.
The DV8 design analysis tool was described in Y. Cai and R. Kazman, “DV8: Automated 
Architecture Analysis Tool Suites”, Proceedings of TechDebt 2019 International Conference 
on Technical Debt (Tools Track), Montreal, Canada, May 2019. A trial version is available 
from archdia.com.
An analysis of technical debt detection tools, and how they report startlingly inconsis-
tent results, can be found in J. Lefever, Y. Cai, H. Cervantes, R. Kazman, and H. Fang, “On 
the Lack of Consensus Among Technical Debt Detection Tools”, Proceedings of the 43rd 
International Conference on Software Engineering (ICSE), 2021.

10.7  Discussion Questions  237
10.7  Discussion Questions
1.	
Technical debt can come in many forms—requirements debt, code debt, testing debt, 
design debt, deployment debt, and documentation debt—which can affect many aspects 
of a project. Which of these do you think is the most important and most problematic for 
a software project? Why?
2.	
When is it a good time to incur design debt? When is it a bad time?
3.	
Should you document your design debt? If so, what would this documentation look like? 
When should you create it?
4.	
What kinds of stakeholders should you engage so that you can get the best scenarios for 
your system, in terms of scenarios that will have an impact on your future design debt?
5.	
When should you analyze your design debt? Every release? Every commit? Some other 
frequency?

This page intentionally left blank 

239
11
Analysis in the Design 
Process
Design is the process of making decisions; analysis is the process of understanding those deci-
sions, so that the design may be evaluated. To reflect on this intimate relationship, we now turn 
our attention to questions of why, when, and how to analyze architectural decisions during 
the design process. We contend that it is impossible to do design without doing some analysis. 
But frequently this analysis is ad hoc, or purely intuitive. In many cases that is good enough, 
but not always. It makes sense to be clear about if, when, and where we need to put a bit more 
thought and a bit more analysis into the decisions we make, as we are making them. In this 
chapter, we will take a quick look at various techniques for analysis, discuss when they can be 
enacted, and consider their costs and benefits. 
Why Read This Chapter?
Although this book is focused on architectural design, we have always believed that 
design and analysis are two sides of the same coin. Besides performing design, another 
essential task for architects is to perform analysis. We hope to convince you in this 
chapter that this is not an onerous duty or one that will greatly increase the time and 
effort devoted to design. The benefits of analysis greatly outweigh the costs, in terms of 
early problem resolution and greater confidence in the outputs of the design process.
11.1  Analysis and Design
Analysis is the process of breaking a complex entity into its constituent parts as a means of 
understanding it. The opposite of analysis is synthesis. Analysis and design are therefore 

240  Chapter 11  Analysis in the Design Process
intertwined activities. During the design process, the activity of analysis can refer to several 
aspects:
■
■Studying the inputs to the design process to understand the problem whose solution you 
are about to design. This includes giving priorities to the drivers as discussed in Section 
4.2.2. This type of analysis is performed in steps 1 and 2 of ADD.
■
■Studying the alternative design concepts that you identified to solve a design problem to 
select the most appropriate one. In this situation, analysis leads you to provide concrete 
evidence for your choices. This is performed in step 4 of ADD and was discussed in 
Section 4.2.4.
■
■Ensuring that the key decisions that have been made during an iteration of the design 
process are appropriate ones. This is the type of analysis that you perform in step 7 of 
ADD.
The decisions that you make when designing the architecture are not just critical to 
achieve the quality attribute responses. Frequently, the cost associated with correcting them 
at a later time can be significant, because they may affect many parts of the system. For these 
reasons, it is necessary to perform analysis during the design process, so that problems can 
be identified, possibly quantified, and corrected quickly. Remember, being too confident and 
following your gut feelings may not be the best idea (see the sidebar “‘I Believe’ Isn’t Good 
Enough”). Fortunately, if you have followed the recommendations that we have given up to this 
point, you should be able to conduct analysis—either by yourself or with the help of peers—by 
using the sketches and rationale that you have been capturing as you make design decisions. 
“I Believe” Isn’t Good Enough
Even if you are following a systematic approach to designing your architecture, you 
are using design concepts from well-established sources, and you have nice-looking 
diagrams where you have represented your structures, nothing really guarantees that 
the decisions you are making will actually satisfy a particular architectural driver. Certain 
quality attributes are critical to the success of your system—and particularly for these, 
the justification of “I believe” is not good enough.
Studies of practicing software architects have shown that most follow an “adequacy” 
approach to making design decisions—that is, they adopt the first decision that appears 
to meet their needs. Architects, like all people, are subject to confirmation bias and 
availability bias. And they often have no strong rationale to substantiate those decisions 
other than their gut feelings, their beliefs. This means that important decisions are fre-
quently made after insufficient reasoning, which can add risk to a system.
For drivers that are truly critical to your system, you owe it to yourself and to your 
organization to perform a more detailed analysis rather than just trusting your gut feel-
ing, relying on analogy and history, or performing a couple of superficial tests to show 
that your drivers are satisfied. There are several options that you can choose from to 
deepen your analysis and support your rationale for the decisions you’ve made:

11.1  Analysis and Design  241
■
■
Analytic models: These well-established mathematical models allow quality attributes 
such as performance or availability to be studied. They include Markov and statistical 
models for availability, and queuing and real-time scheduling theory for performance. 
These models are highly mature but may require considerable education and training 
to be used adequately.
■
■
Checklists: Checklists are useful tools that allow you to ensure, in a systematic way, 
that important decisions and considerations are not forgotten. Checklists for particu-
lar quality attributes can be found in the public domain, such as the OWASP checklist 
that can serve as guidance for performing black-box security testing of web applica-
tions. Also, your organization may develop proprietary checklists that are specific to 
the application domains that you are developing. Later in this chapter, we will present 
tactics-based questionnaires, a type of checklist for the most critical system quality 
attributes, based on a knowledge of tactics.
■
■
Thought experiments, reflective questions, and back-of-the-envelope analyses: 
Thought experiments are informal analyses performed by a small group of designers 
in which important scenarios are studied to identify potential problems. For example, 
you might use a sequence diagram produced in step 5 of ADD and perform a walk-
through of the interaction of the objects that support the scenario modeled in the 
diagram with a colleague. Reflective questions (discussed in depth in Section 11.5) 
are questions that challenge the assumptions underlying a decision. Back-of-the-
envelope analyses are rough calculations that are less precise than analytic models, 
but that can be performed quickly. These calculations, which are frequently based on 
analogy with other similar systems or on prior experience, are useful to obtain ball-
park estimates for desired quality attribute responses. For example, by summing the 
latencies of a number of processes in a pipeline, you can derive a first-order estimate 
of the end-to-end latency.
■
■
Prototypes, simulations, and experiments: Purely conceptual techniques for analyz-
ing a design are sometimes inadequate to accurately understand if certain design 
decisions are appropriate, or whether you should favor one particular technology over 
another. In situations like these, the creation of prototypes, simulations, or experi-
ments can be an invaluable option to obtain a better understanding. For example, in 
the back-of-the-envelope estimate of latency just described, you may not have taken 
into account that several of the processes are sharing (and hence competing for) the 
same resources, so you cannot simply sum their individual latencies and expect to 
get accurate results. Prototypes and simulations can provide a deeper understanding 
of system dynamics, but may require a significant effort that the project plan needs to 
take into account.
As always, there is no one technique that is consistently better than the others. 
Thought experiments and back-of-the-envelope calculations are inexpensive and can 
be done early in the design process, but their validity may be questionable. Prototypes, 
simulations, and experiments typically produce much higher-fidelity results, but at a 
far greater cost and with a bigger impact on your schedule. The choice of which to do 
depends on the context, the risk involved, and the priorities of your quality attributes.
However, keep in mind that applying any of these techniques will be helpful in going 
from “I believe” (that my design is appropriate) to an approach that is backed by docu-
mented evidence and argumentation.

242  Chapter 11  Analysis in the Design Process
11.2  Why Analyze?
As we said earlier, analysis and design are two sides of the same coin. Design is (the process  
of) making decisions. Analysis is (the process of) understanding the consequences of those 
decisions in terms of cost, schedule, and quality. No sensible architect would make any deci-
sion, or at least any nontrivial decision, without first attempting to understand the implications 
of that decision: its near-term and possibly long-term consequences. Architects may make 
thousands of decisions in the course of designing a large project, and clearly not all of them 
matter. Furthermore, not all of the decisions that matter are carriers of quality attributes. Some 
may be about which vendor to select, or which coding convention to follow, or which pro-
grammer to hire or fire, or which IDE to use—important decisions, to be sure, but not directly 
linked to a quality attribute outcome.
But, just as clearly, some of these decisions will affect the achievement of quality attri-
butes. When the architect breaks down the development into a system of layers or modules, or 
both, this decision will affect how a change will ripple through the code base, who needs to 
talk to whom when modifying a feature or fixing a bug, how easy or difficult it is to distribute 
or outsource some of the development, how easy it is to port the software to a different plat-
form, and so forth. When the architect chooses a distributed resource management system, 
how it determines which services are leaders and which are followers, how it detects failures, 
and how it detects resource starvation are all factors that will affect the availability of the 
system.
When and why do we analyze during the design process? First, we analyze because we 
can. An architecture specification, whether it is just a whiteboard sketch or something that 
has been more formally documented and circulated, is the first artifact supporting an analysis 
that offers insights into quality attributes. Yes, we can analyze requirements, but we mainly 
analyze them for consistency and completeness. Until we translate those requirements into 
structures resulting from design decisions, we have little to say about the actual consequences 
of those decisions, their costs and benefits, and the tradeoffs among them.
Second, and more to the point, we analyze because it is a prudent way of informing deci-
sions and managing risk. No design is completely without risk, but we want to ensure that the 
risks that we are taking on are commensurate with our stakeholders’ expectations and toler-
ances. For a banking application or a military application, we would expect our stakeholders 
to demand low levels of risk, and they should be willing to pay accordingly for higher levels of 
assurance. For a startup company, where time to market is of the essence and budgets are tight, 
we might be prepared to accept far higher levels of risk. As with every important decision in 
software engineering: It depends.
Finally, analysis is the key to evaluation. Evaluation is the process of determining the 
value of something. Companies are evaluated to determine their share price. A company’s 
employees are evaluated annually to determine their raises. In each case, the evaluation is built 
upon an analysis of the properties of the company or employee.

11.3  Analysis Techniques  243
11.3  Analysis Techniques
Different projects will deserve (and possibly demand) different responses to risk. Fortunately, 
we, as architects, have a wide variety of practices and methods at our disposal to analyze 
architectures. With a bit of planning, we can match our risk tolerance with a set of analysis 
techniques that both meet our budget and schedule constraints and provide reasonable levels 
of assurance. The point here is that analysis does not have to be costly or complex. Just asking 
thoughtful questions is a form of analysis, and that is pretty inexpensive. Building a simple 
prototype is more expensive, but in the context of a large project this analysis technique may 
be worth the additional expense, in terms of how it explores and mitigates risks.
Examples of (relatively economical, relatively low ceremony) analysis already in wide-
spread use include design reviews and scenario-based analyses, code reviews, pair program-
ming, and Scrum retrospective meetings. Other commonly used analysis techniques, albeit 
somewhat more costly, include prototypes (throw-away or evolutionary) and simulations.
At the high end of expense and complexity, we can build formal models of our systems 
and analyze them for properties such as latency or security or safety. And, finally, when there 
is a candidate implementation or a fielded system, we can perform experiments, including 
instrumenting running systems and collecting data, ideally from executions of the system that 
reflect realistic usages.
As indicated in Table 11.1, the cost of these techniques typically increases as you pro-
ceed through the software development life cycle. A prototype or experiment is more expen-
sive than a checklist, which is more expensive than experience-based analogy. However, this 
expected cost is strongly correlated with the confidence that you have in the analysis results. 
Unfortunately, there is no free lunch!
TABLE 11.1  Analysis at Different Stages of the Software Life Cycle
Life-Cycle Stage
Form of Analysis
Cost
Confidence
Requirements
Experience-based analogy
Low
Low–high
Requirements
Back-of-the-envelope analysis
Low
Low–medium
Architecture
Thought experiment/reflective 
questions
Low
Low–medium
Architecture
Checklist-based analysis
Low
Medium
Architecture
Tactics-based analysis
Low
Medium
Architecture
Scenario-based analysis
Low
Medium
Architecture
Analytic model
Low–medium
Medium
Architecture
Simulation
Medium
Medium
Architecture
Prototype
Medium
Medium–high
Implementation
Experiment
Medium–high
Medium–high
Fielded system
Instrumentation
Medium–high
High

244  Chapter 11  Analysis in the Design Process
11.4  Tactics-Based Analysis
Architectural tactics (discussed in Chapter 3) have been presented thus far as design prim-
itives. However, as these taxonomies are intended to cover the entire space of architectural 
design possibilities for managing a quality attribute, we can use them in an analysis setting 
as well. Specifically, we can use them as guides for interviews or questionnaires. These inter-
views help you, as an analyst, to gain rapid insight into the architectural approaches taken, or 
not taken.
Consider, for example, the tactics for availability (refer back to Figure 3.5). Each of these 
tactics is a design option for the architect who wants to design a highly available system. But, 
in hindsight, they represent a taxonomy of the entire design space for availability and hence 
can be a way of gaining insight into the decisions made, and not made, by the architect. To do 
this, we simply turn each tactic into an interview question. For example, consider the (partial) 
set of tactics-inspired availability questions in Table 11.2.
TABLE 11.2  Example Tactics-Based Availability Questions
Tactics 
Group
Tactics Question
Supported? 
(Y/N)
Risk
Design 
Decisions  
and Location
Rationale and 
Assumptions
Detect faults Does the system use ping/
echo to detect a failure of a 
component or connection, or 
network congestion?
Does the system use a 
component to monitor the 
state of health of other parts 
of the system? A system 
monitor can detect failure or 
congestion in the network or 
other shared resources, such 
as from a denial-of-service 
attack.
Does the system use a 
heartbeat—a periodic 
message exchange 
between a system monitor 
and a process—to detect 
a failure of a component 
or connection, or network 
congestion?
Does the system use a time 
stamp to detect incorrect 
sequences of events in 
distributed systems?

11.4  Tactics-Based Analysis  245
Tactics 
Group
Tactics Question
Supported? 
(Y/N)
Risk
Design 
Decisions  
and Location
Rationale and 
Assumptions
Does the system use voting 
to check that replicated 
components are producing 
the same results? The 
replicated components 
may be identical replicas, 
functionally redundant, or 
analytically redundant.
Do you use exception 
detection to detect a system 
condition that alters the 
normal flow of execution (e.g., 
system exception, parameter 
fence, parameter typing, 
timeout)?
Can the system do a self-
test to test itself for correct 
operation?
Recover 
from faults 
(preparation 
and repair)
Does the system employ a 
redundant spare? That is, 
does it use a configuration in 
which one or more duplicate 
components can step in 
and take over the work if the 
primary component fails?
Does the system employ 
exception handling to 
deal with faults? Typically, 
the handling involves either 
reporting the fault or handling 
it, potentially masking the 
fault by correcting the cause 
of the exception and retrying.
Does the system employ 
rollback, so that it can revert 
to a previously saved good 
state (the “rollback line”) in 
the event of a fault?
When these questions are used in an interview setting, we can record whether each tactic 
is supported by the system’s architecture, according to the opinions of the architect. If we are 
analyzing an existing system we can also investigate the following issues:
■
■Whether there are any obvious risks from the use (or nonuse) of this tactic. If the tactic 
has been used, we can record here how it is realized in the system (e.g., via custom code, 
generic frameworks, externally produced components, or something else). For example, 
we might note that the redundant spare tactic has been employed by replicating the query 
microservice in the case study in Chapter 8.

246  Chapter 11  Analysis in the Design Process
■
■The specific design decisions made to realize the tactic and where in the code base the 
implementation (realization) may be found. This is useful for auditing and architecture 
reconstruction purposes. Continuing the example from the previous bullet, we might 
probe how many replicas of the microservice have been created, and where these  
replicas are located (i.e., in different regions or availability zones).
■
■Any rationale or assumptions made in the realization of this tactic. For example, we 
might assume that there will be no common-mode failure, so it is acceptable that the 
replicas are identical microservices, running on identical execution environments.
While this interview-based approach might sound simplistic, it can actually be quite pow-
erful and insightful. In your daily activities as an architect, you likely do not take the time to 
step back and consider the bigger picture. A set of interview questions such as those shown in 
Table 11.2 force you to do just that. And this approach is quite efficient: A typical interview for 
a single quality attribute takes between 30 and 90 minutes.
A set of tactics-based questionnaires, covering the ten most important system quality 
attributes—availability, deployability, energy efficiency, integrability, modifiability, perfor-
mance, safety, security, testability, and usability—can be found in Appendix.
11.5  Reflective Questions
Similar to the tactics-based interviews, a number of researchers have advocated the practice 
of asking (and answering) reflective questions to augment the design process. The idea behind 
this process is that we actually think differently when we are problem-solving versus when we 
are reflecting. For this reason, these researchers have advocated a separate “reflection” activ-
ity in design that challenges the decisions made, and that challenges us to examine our biases.
Architects, like all humans, are subject to bias. For example, we are subject to  
confirmation bias—the tendency to interpret new information in a way that confirms our pre-
conceptions—and we are subject to anchoring bias—the tendency to rely too heavily on the 
first piece of information that we receive when investigating a problem and using this informa-
tion to filter and judge any subsequent information. Reflective questions help to uncover and 
counter such biases in a systematic way, which can lead us to revise our assumptions and hence 
our designs.
Architects can and should reflect on a system’s context and requirements (are the contexts 
and requirements identified relevant, complete, and accurate?), design problems (have they 
been properly and fully articulated?), design solutions (are they appropriate given the require-
ments?), and design decisions (are they principled and justified?). Examples of reflective ques-
tions that researchers propose include the following:
■
■What assumptions are made? Do the assumptions affect the design problem? Do the 
assumptions affect the solution option? Is an assumption acceptable in a decision?

11.6  Scenario-Based Design Reviews  247
■
■What are the risks that certain events would happen? How do the risks cause design 
problems? How do the risks affect the viability of a solution? Is the risk of a decision 
acceptable? What can be done to mitigate the risks?
■
■What are the constraints imposed by the contexts? How do the constraints cause design 
problems? How do the constraints limit the solution options? Can any constraints be 
relaxed when making a decision?
■
■What are the contexts and the requirements of this system? What does this context 
mean? What are the design problems? Which are the important problems that need to be 
solved? What does this problem mean? What potential solutions can solve this problem? 
Are there other problems to follow up in this decision?
■
■What contexts can be compromised? Can a problem be framed differently? What are the 
solution options? Can a solution option be compromised? Are the pros and cons of each 
solution treated fairly? What is an optimal solution after considering tradeoffs?
Of course, all of these questions might not be relevant for a system, and you would not 
employ this technique for every decision that you make. But, used judiciously, these kinds of 
questions are fast and economical, and can help you to reflect mindfully on the decisions that 
you are making.
11.6  Scenario-Based Design Reviews
Comprehensive scenario-based design reviews, such as the architecture tradeoff analysis 
method (ATAM), have typically been conducted outside the design process. The ATAM is an 
example of a comprehensive architecture evaluation (see the sidebar “The ATAM”).
An ATAM review, as it was initially conceived, was a “milestone” review. When an 
architect or other key stakeholder believed that there was enough of an architecture or archi-
tecture description to analyze, an ATAM meeting could be convened. This might occur when 
an architectural design had been done but before much, if any, implementation had been com-
pleted. Or, more commonly, it occurred when an existing system was in place and some stake-
holders wanted an objective evaluation of the risks of the architecture before committing to it, 
evolving it, acquiring it, and so forth. 
The ATAM
The ATAM—architecture tradeoff analysis method—is an established method for  
analyzing architectures, driven by scenarios. Its purpose is to assess the consequences 
of architectural decisions in light of quality attribute requirements and business goals.
The ATAM brings together three groups in an evaluation:
■
■
A trained evaluation team
■
■
An architecture’s “decision makers”
■
■
Representatives of the architecture’s stakeholders

248  Chapter 11  Analysis in the Design Process
The ATAM helps stakeholders ask the right questions to discover potentially problem-
atic architectural decisions—that is, risks. These discovered risks can then be made the 
focus of mitigation activities such as further design, further analysis, prototyping, and 
implementation. In addition, design tradeoffs are often identified—hence the name of 
the method. The purpose of the ATAM is not to provide precise analyses: The method 
is typically applied during a pair of two-day meetings and this (relatively) short time 
frame does not permit a deep dive into any specific concern. Those kinds of analyses 
are, however, appropriate as part of the risk mitigation activities that could follow and be 
guided by an ATAM.
The ATAM can be used throughout the software development life cycle. It can be 
used in the following situations:
■
■
After an architecture has been specified but when there is little or no code as yet
■
■
To evaluate potential architectural alternatives
■
■
To evaluate the architecture of an existing system
The outputs of the ATAM are as follows:
■
■
A concise presentation of the architecture. The architecture is presented in one hour.
■
■
A concise articulation of the business goals for the system under scrutiny. Frequently, 
the business goals presented in the ATAM are being seen by some of the assembled 
participants for the first time; these are captured in the outputs.
■
■
A set of prioritized quality attribute requirements, expressed as scenarios.
■
■
A mapping of architectural decisions to quality requirements. For each quality 
attribute scenario examined, the architectural decisions that help to achieve it are 
identified and recorded.
■
■
A set of sensitivity and tradeoff points. These architectural decisions have a marked 
effect on one or more quality attributes.
■
■
A set of risks and non-risks. A risk is defined as an architectural decision that may 
lead to undesirable consequences in light of quality attribute requirements. A non-risk 
is an architectural decision that, upon analysis, is deemed safe. The identified risks 
form the basis for an architectural risk mitigation plan.
■
■
A set of risk themes. The evaluation team examines the full set of discovered risks to 
identify overarching themes that reveal systemic weaknesses in the architecture (or 
even in the architecture process and team). If left untreated, these will threaten the 
project’s business goals.
An ATAM-based evaluation also produces some intangible results. These include 
a sense of community on the part of the stakeholders, open communication channels 
between the architect and the stakeholders, and a better overall understanding of the 
architecture and its strengths and weaknesses. Although difficult to measure, these 
results are no less important than the others and often are the longest-lasting.
A full ATAM takes place in four phases. The first phase (phase 0) and the final phase 
(phase 3) are managerial: setting up the evaluation at the start and reporting results and 
follow-on activities at the end. The middle phases (phase 1 and phase 2) are when the 
actual analysis takes place. The steps of the ATAM enacted in phases 1 and 2 are as 
follows:
1.	 Present the ATAM
2.	 Present business drivers

11.6  Scenario-Based Design Reviews  249
3.	 Present the architecture
4.	 Identify architectural approaches
5.	 Generate a quality attribute utility tree
6.	 Analyze architectural approaches
7.	 Brainstorm and prioritize scenarios
8.	 Analyze architectural approaches
9.	 Present the results
In phase 1, we enact steps 1–6 with a small, internal group of stakeholders, typically 
just the architect, project manager, and perhaps one or two senior developers. In phase 
2, we invite a larger group of stakeholders to attend—all the people who attended 
phase 1 plus external stakeholders, such as customer representatives, end-user repre-
sentatives, quality assurance, operations, and so forth. In phase 2, we review steps 1–6 
and enact steps 7–9.
The actual analysis takes place in step 6, where we analyze architectural approaches 
by asking the architect to map the highest-priority scenarios, one at a time, onto the 
architectural approaches that have been described. During this step, the analysts ask 
probing questions, motivated by a knowledge of the quality attributes, and risks are 
discovered and documented.
The idea of carrying out a distinct evaluation activity once the architecture is “done” is 
a poor fit with the way that most organizations operate today. Today, most software organiza-
tions are practicing some form of agile or iterative development. There is no distinct mono-
lithic “architecture phase” in agile processes. Rather, the system architecture and development 
are co-created in a series of sprints. For example, as we will discuss in Section 12.1, many 
agile thought leaders are promoting practices such as “disciplined agility at scale”, the “walk-
ing skeleton”, and the “scaled agile framework”—all of which embrace the idea that archi-
tectures continuously evolve in relatively small increments, addressing the most critical risks. 
This may be aided by developing a small proof-of-concept or minimum viable product (MVP), 
or by doing strategic prototyping.
To better align with this view of software development, a lightweight scenario-based peer 
review method, based on the ATAM, has been created. The lightweight ATAM follows all the 
steps of the full ATAM but limits their scope, thereby shortening the time allotted to each step. 
This modified method consists of just a single analysis phase; by comparison, the full ATAM 
has two phases with two groups of stakeholders, one internal and one external. As a conse-
quence, a lightweight ATAM can be conducted in a half-day meeting. Also, you can conduct 
this method internally, using just project members. Of course, an external review gives more 
objectivity and may produce better results, but it may be too costly or infeasible due to sched-
ule or intellectual property (IP) constraints. A lightweight ATAM provides a reasonable mid-
dle ground between the costly but objective and comprehensive ATAM and doing no analysis 
whatsoever, or only doing ad hoc analysis.
Table 11.3 outlines an example schedule for a lightweight ATAM conducted by project 
members on their own project.

250  Chapter 11  Analysis in the Design Process
TABLE 11.3   A Typical Agenda for a Lightweight ATAM
Step
Time
Allotted
Notes
1: Present business  
    drivers
0.25 hour
The participants are expected to understand the system and its 
business goals and their priorities. Fifteen minutes is allocated 
for a brief review to ensure that these are fresh in everyone’s 
mind and that there are no surprises.
2: Present architecture 0.5 hour
All participants are expected to be familiar with the system, 
so a brief overview of the architecture is presented and 1–2 
scenarios are traced through the documented architecture 
views.
3: Identify architectural  
    approaches
0.25 hour
The architecture approaches for specific quality attribute 
concerns are identified by the architect. This may be done as a 
portion of step 2.
4: Generate quality  
    attribute utility tree
0.5 hour 
Scenarios might already exist; if so, use them. A utility tree 
might already exist; if so, the team reviews this and updates it, 
if needed.
5: Analyze architectural  
    approaches
2.0 hours
This step—mapping the highly ranked scenarios onto the 
architecture—consumes the bulk of the time and can be 
expanded or contracted as needed.
6: Present results
0.5 hour
At the end of the evaluation, the team reviews the existing and 
newly discovered risks and tradeoffs and discusses priorities.
Total
4 hours
A half-day review such as this is similar, in terms of effort, to other quality assurance 
efforts that are typically conducted in a development project, such as code reviews, inspec-
tions, and walk-throughs. For this reason, it is easy to schedule a lightweight ATAM in a 
sprint, particularly in those sprints where architectural decisions are being made, challenged, 
or changed.
11.7  Summary
No responsible programmer would field code that they had not tested. But architects and pro-
grammers regularly commit to (implement) architectural decisions that have not been ana-
lyzed. Why the dichotomy? Surely, if testing code is important, then “testing” the design 
decisions you have made is an order of magnitude more important, as these decisions often 
have long-term, system-wide, significant impacts.
The most important message of this chapter is that design and analysis are not sepa-
rate activities. Every important design decision that you make should be analyzed. Techniques 
have been developed for doing this kind of analysis continuously, in a relatively disruption-free 
manner, as part of the process of designing and evolving a system.
The interesting questions are not whether to analyze, but rather how much to analyze and 
when. Analysis is inherent in doing good design, and it should be a continuous process.

11.9  Discussion Questions  251
11.8  Further Reading
The sets of architectural tactics used here have been documented in L. Bass, P. Clements, and 
R. Kazman, Software Architecture in Practice, 4th ed., Addison-Wesley, 2021. The availabil-
ity tactics were first created in J. Scott and R. Kazman, Realizing and Refining Architectural 
Tactics: Availability, CMU/SEI-2009-TR-006, 2009.
The idea of reflective questions was first introduced in M. Razavian, A. Tang, R. Capilla, 
and P. Lago, “In Two Minds: How Reflections Influence Software Architecture Design 
Thinking”, VU University Amsterdam, Tech. Rep. 2015-001, April 2015. The idea that soft-
ware designers satisfice—that is, they look for a “good enough”, as opposed to an optimal, 
solution—has been discussed in A. Tang and H. van Vliet, “Software Designers Satisfice”, 
European Conference on Software Architecture (ECSA 2015), 2015.
The ATAM was comprehensively described in P. Clements, R. Kazman, and M. Klein, 
Evaluating Software Architectures: Methods and Case Studies, Addison-Wesley, 2001. The 
lightweight ATAM is explained in L. Bass, P. Clements, and R. Kazman, Software Architecture 
in Practice, 4th ed., Addison-Wesley, 2021. In addition, ATAM-style peer reviews have been 
described in F. Bachmann, “Give the Stakeholders What They Want: Design Peer Reviews the 
ATAM Style”, Crosstalk, November/December 2011.
A discussion of how to use analysis models for reasoning about the quality attribute of 
robustness can be found in this technical report: R. Kazman, P. Bianco, S. Echeverria, and  
J. Ivers, Robustness, CMU/SEI-2022-TR-004, 2022.
Reviews have social as well as technical aspects. A discussion of some of the nontechnical 
(social, psychological, managerial) aspects of reviews can be found in R. Kazman and L. Bass, 
“Making Architecture Reviews Work in the Real World”, IEEE Software, 19(1), 67–73, 2002.
11.9  Discussion Questions
1.	
What is the difference between requirements analysis and analysis during the design 
process?
2.	
What are the benefits of using a checklist-based approach, such as tactics-based analysis, 
versus a more open analysis approach, such as scenario-based design reviews?
3.	
Consider the case study presented in Chapter 9. Propose some reflective questions that might 
be relevant to it. These reflective questions could be associated with the requirements or with 
the design decisions. Which questions would be most useful to the case in this chapter?
4.	
Consider the case study presented in Chapter 8 and select a quality attribute category 
from the ones listed in Section 8.2.2. Use the tactics-based questionnaire associated with 
this category and identify the tactics that are supported or not supported. Also fill in the 
remaining columns of the questionnaire. What insights does this give you into the design 
decisions made or not made?
5.	
Consider the case study presented in Chapter 8. Choose the most important scenarios 
and perform a scenario-based analysis of the design. Can you identify at least three risks 
in the decisions that were made by the architect? Justify why they are risks.

This page intentionally left blank 

253
12
The Architecture Design 
Process in the Organization
In the previous chapters of this book, we focused our attention on designing architectures, and 
the concerns surrounding architectural design. In this chapter, we consider how architectural 
design is integrated both within the development life cycle and with the organization as a 
whole. 
Why Read This Chapter?
An architect’s job is not purely technical. An architect is the fulcrum between business 
requirements on the one hand, and technical solutions on the other hand. Architectures 
exist within an organization. The effective architect is aware of this context—such as 
life-cycle concerns and existing team structures—and creates architectures that are 
well aligned with their contexts. Having read this chapter, you should be aware of the 
many forces and concerns that an architecture influences and is influenced by, and you 
should be better able to be a positive force for good in your organization.
12.1  Architecture Design and the Development Life Cycle
As we saw in Section 2.4.1, architecture design can serve different purposes. Here we are 
interested in four contexts in which designs and design decisions are typically made. The first 
context is before the development of the software system begins, when an architectural design 
is created to support estimation of project cost, effort, and schedule. The second and third con-
texts are when an architectural design is being created to support agile or sequential develop-
ment. The final context is when architectural decisions need to be made to support a project’s 
testing strategy.

254  Chapter 12  The Architecture Design Process in the Organization
12.1.1  Designing to Support Estimation
In many types of development projects, organizations typically need to provide an initial  
estimate of the cost, effort, and schedule of the project. In the context of product-oriented 
development, the word project used here refers to the delivery of a particular milestone of the 
product. This estimation phase frequently determines whether the project will actually go for-
ward, as shown in Figure 12.1. The estimation phase can be done both for internal and external 
projects.
Preliminary
Architecture Design
+ Estimates
Architecture Design
<<Produces>>
<<Derived from>>
<<Produces>>
[Rejected]
[Accepted]
Estimation
Development and
Operations
FIGURE 12.1  The relationship between estimation and project development
Frequently, estimation activities must be performed in a short time period, and the infor-
mation that is available to inform this process is always limited. For example, typically only 
high-level requirements or features (rather than detailed user stories) are available at this 
phase. Also, quality attributes are frequently not detailed with precise measurements; only a 
rough idea of the important quality attributes is available.
The problem with limited information is that the estimate that is produced frequently 
has a high level of variance, as illustrated by the cone of uncertainty depicted in Figure 12.2. 
This cone covers the uncertainty surrounding estimates in a project, typically those of cost and 
schedule, but also risk. All of these estimates get better as a project progresses, so the cone 
narrows. When the project is done, uncertainty is zero. The issue for any development method-
ology is how to narrow the cone of uncertainty earlier in the project’s life cycle.

12.1  Architecture Design and the Development Life Cycle  255
Initial
Project
Deﬁnition
Project
Schedule
Approved
Project
Deﬁnition
Requirements
Speciﬁcation
Product
Design
Speciﬁcation
Detailed
Design
Speciﬁcation
Accepted
Software
1.6x
1.25x
1.15x
1.1x
0.9x
0.85x
0.8x
0.6x
x
FIGURE 12.2  Example cone of uncertainty
Architectural practices can be applied in the estimation phase to gain understanding and 
help reduce the variance in the estimates:
■
■Architectural drivers can be identified in this phase. Even if it may be complicated to 
describe detailed quality attribute scenarios at this point, the most important quality 
attributes with initial measures and constraints should be identified. Following the 
principles from QAW (see Section 2.4.2), quality attributes that are important to different 
stakeholders should be identified.
■
■ADD can be used to produce an initial architecture, which is then used as the basis for 
early cost and schedule estimates.
■
■Sketches of this initial architecture are useful for communication with the customer. 
They are also useful as a basis to perform lightweight evaluations of this initial design.

256  Chapter 12  The Architecture Design Process in the Organization
Generating an initial architecture allows estimation to be performed using the “standard 
components” technique. Standard components are a type of proxy; they include front-ends, 
components associated with specific layers, services, or microservices, among other things. 
When estimating based on standard components, companies typically build databases that 
contain historical measurements of size, effort, and data volumes for components that have 
been built and incorporated into previously developed systems. To estimate with standard 
components, you need to identify the components that will be required for the problem that 
you are trying to solve, and then use historical data (or some other technique such as Wideband 
Delphi) to estimate the size of these components. The total size can then be translated into 
effort, and these estimates can be rolled up to produce a project-level time and cost estimate.
Identification of the components that are required to create estimates with this technique 
can be achieved in a short time frame through the use of ADD. This approach is similar to 
what we recommended for the design of greenfield systems in Section 4.3.1:
■
■The goal of your first design iteration should be to address the concern of establishing an 
initial overall structure for the application. The reference architecture, if you employ one, 
dictates the types of standard components that will be used in the estimation. At this 
point, the most relevant technologies to use in the project can also be selected, particu-
larly if your historical data is tied to specific technologies.
■
■The goal of your second design iteration should be to identify components to support 
all of the functionality that needs to be considered for the estimation. As opposed to 
what we discussed for the design of greenfield systems, when designing to produce an 
estimate, you need to consider more than just the primary functionality. To identify the 
standard components, you need to consider all of the important functional requirements 
that are part of the scope and map them to the structure that you defined in the first iter-
ation. Doing so ensures that you identify a majority of components needed to estimate, 
thus producing a more accurate estimation.
This technique will help you estimate costs and schedule for meeting the most import-
ant functional requirements. At this point, however, you will likely not have taken quality 
attributes into account. As a consequence, you should perform a few more iterations focusing 
on where you will make design decisions to address the driving quality attributes. If the time 
available to perform the estimation process is limited, you will not be able to design it in 
much detail, so the decisions that you should take here are the ones that will have a signifi-
cant impact on the estimate. In the case of systems that will be deployed in the cloud, at least 
one iteration should be devoted to designing the cloud architecture of the system, considering 
aspects such as redundancy, performance, and security where possible. Cloud provider calcu-
lators, such as the one discussed in Section 7.2.2.1, can be used to estimate the costs associated 
with this initial infrastructure.
When this technique is used in the estimation process, an initial architecture design is 
produced—the preliminary architecture design (see Figure 12.1). If the project proposal is 
accepted by the customer and the project proceeds, this initial architecture can become one 
of the bases for a contract. This architecture should be used as a starting point in the subse-
quent architecture design activities that are performed during the development and operation 

12.1  Architecture Design and the Development Life Cycle  257
phase of the project. In this case, the approach suggested for designing brownfield systems 
(discussed in Section 4.3.3) can be used.
Furthermore, the preliminary documentation produced for this initial architecture can 
be included as part of the technical proposal that is provided to the customer. Finally, this ini-
tial architecture design can be evaluated, preferably before estimation occurs. This evaluation 
can be performed using a technique such as the lightweight ATAM, which was introduced in 
Section 11.6.
12.1.2  Designing to Support Agile Development
The relationship between software architecture and agility has been the subject of some debate 
over the past decade. Although we believe—and much research has shown—that architec-
tural practices and Agile practices are actually well aligned, this position has not always been 
universally accepted. Here we discuss different aspects that are relevant when designing to 
support agile development.
12.1.2.1  Approaches to Design
Agile practices, according to the original Agile Manifesto, emphasize “Individuals and inter-
actions over processes and tools, working software over comprehensive documentation, cus-
tomer collaboration over contract negotiation, and responding to change over following a 
plan”. None of these values is inherently in conflict with architectural practices. So why has 
the belief arisen—at least in some circles—that the two sets of practices are somehow incom-
patible? The crux of the matter is the one principle on which Agile practices and architectural 
practices differ.
The creators of the Agile Manifesto originally described 12 principles behind the man-
ifesto. While 11 of these principles are fully compatible with architectural practices, one of 
them is not: “The best architectures, requirements, and designs emerge from self-organizing 
teams”. Although this principle may have held true for small and perhaps even medium-sized 
projects, we are unaware of any cases where it has been successful in large projects, particu-
larly those with complex or novel requirements and distributed development. The heart of the 
problem is this: Software architecture design is “up-front” work. You could always just start 
a project by coding and doing minimal or no up-front analysis or design. This is what we call 
the emergent approach, as shown in Figure 12.3b. In some cases—small systems, throw-away 
prototypes, systems where you have little idea of the customer’s requirements—this may, in 
fact, be the optimal decision. At the opposite extreme, you could attempt to collect all the 
requirements up front, and from that synthesize the ideal architecture, which you would then 
implement, test, and deploy. This big design up front approach (BDUF; Figure 12.3a) is usu-
ally associated with the classic waterfall model of software development. The waterfall model 
has fallen out of favor over the past decades due to its complexity and rigidity, which led to 
many well-documented cases of cost overruns, schedule overruns, and customer dissatisfac-
tion. With respect to architectural design, the downside of the BDUF approach is that it can 
end up producing an extensively documented but untested design that may not be appropriate. 
This occurs because problems in the design are often discovered late and may require a lot 

258  Chapter 12  The Architecture Design Process in the Organization
of rework, or the original design may end up being ignored by developers in their attempts to  
satisfy system requirements, and the true architecture is thus not documented.
Design Effort
Development
Cycles
Time
Time
Design Effort
a) BDUF Approach
Design Effort
Development
Cycles
Time
b) Emergent Approach
c) Iteration 0 + Spikes
Approach
FIGURE 12.3  Three approaches to architectural design
Clearly, neither of these extremes makes sense for most real-world projects, where some 
(but not all) of the requirements are well understood up front, but there is also a risk of doing 
too much too soon and hence becoming locked into a solution that will inevitably need to be 
modified, at significant cost. So, the truly interesting question is this: How much up-front 
work, in terms of requirements analysis, risk mitigation, and architecture, should a project 
do? Boehm and Turner have presented evidence arguing that there is no single right answer to 
this question, but that you can find a “sweet spot” for any given project. The “right” amount 
of up-front work depends on several factors, with the most dominant being project size. Other 
important factors include requirements complexity, requirements volatility (related to the  
novelty of the domain), and degree of distribution of development.
So how do architects achieve the right amount of agility? How do they find the right bal-
ance between up-front work and technical debt leading to rework? For small, simple projects, 
no up-front work on architecture is a justifiable decision. It is easy and relatively inexpen-
sive to turn on a dime and refactor. In more complex or more mature projects, where there is 
already some understanding of the requirements, begin by performing a few ADD iterations. 
These iterations can focus on aspects such as the following concerns:
■
■Identifying the major architectural patterns, particularly a reference architecture, if one 
is appropriate
■
■Identifying infrastructure elements where the elements from the architectural patterns 
will execute or store data (operational and analytical) and communicate with each other

12.1  Architecture Design and the Development Life Cycle  259
Furthermore, if a DevOps approach is desired, some iterations should be focused on 
designing for deployability (see Chapter 6). Some additional tasks should also be performed, 
including these:
■
■Establishing a testing strategy and setting up pre-production and production  
environments (see Section 12.1.4)
■
■Setting up build automation infrastructures, including continuous integration and 
deployment pipelines that will automate the deployment of the artifacts to the execution 
environments
■
■Setting up monitoring infrastructures, including dashboards that allow the system to be 
observed at runtime
This is the iteration 0 + spikes approach depicted in Figure 12.3c. It can help to structure 
the project, define work assignments and team formation, address the most critical quality 
attributes, and prepare for early and continuous delivery, if needed. If and when requirements 
change—particularly if these are driving quality attribute requirements—adopt a practice of 
Agile experimentation, where spikes are used to address new requirements. A spike is a time-
boxed task that is created to answer a technical question or gather information associated with 
important architectural decisions; it is not intended to lead to a finished product. Spikes are 
developed in a separate branch and, if successful, merged into the main branch of the code.  
In this way, emerging requirements can be welcomed and managed without being too disrup-
tive to the overall process of development.
Agile architecture practices, however, can help to tame some of the complexity, narrow-
ing the cone of uncertainty and hence reducing project risk. A reference architecture defines 
families of technology components and their relationships. It guides integration and indicates 
where abstraction should be built into the architecture, to help reduce rework when a new 
technology (from within a family) replaces an existing one. Agile spikes allow prototypes to 
be built quickly and to “fail fast”, thereby guiding the eventual selection of technologies to be 
included on the main development branch.
12.1.2.2  Tension Between Emergence and Planning
In an organizational context where multiple teams develop different products and reuse  
composable assets, it is highly unlikely that a successful architecture will simply “emerge”, 
particularly if multiple products share common assets. Models that support scaling agile 
approaches, such as the Scaled Agile Framework (SAFe), consider the existence of two types 
of architectures—one that is “intentional” and one that is “emergent”. SAFe describes them in 
the following way:
■
■Intentional architecture: Defines a set of purposeful, planned architectural strategies 
and initiatives, which enhance solution design, performance, and usability and provide 
guidance for interteam design and implementation synchronization.
■
■Emergent design: Provides the technical basis for a fully evolutionary and incremental 
implementation approach. This helps developers and designers respond to immediate 
user needs, allowing the design to evolve as the system is built and deployed.

260  Chapter 12  The Architecture Design Process in the Organization
In a context of business agility (see Chapter 5), there is a constant tension between emer-
gence and planning in terms of architectural design. As the different products continuously 
evolve and perhaps pivot from the original vision as new features are added to the backlog, 
new architectural decisions may potentially need to be made. Consider, for example, a product 
that was originally conceived for a national market, but which at some point in time must cater 
to an international audience. This product may not have been originally designed with interna-
tionalization or with a high volume of users in mind, and new design decisions may be needed 
to support these quality attribute scenarios. In such a situation, parts of the architecture may 
emerge as the product’s original architecture is adjusted to support a higher volume of users 
and multiple user interfaces.
This product, however, may reuse shared common concerns and components, which must 
be designed in ways that satisfy quality attributes that are important not just to this specific 
product, but potentially to many other products developed in the organization. Examples of 
these quality attributes and concerns include security (such as rules and services for authen-
tication or authorization), internationalization, logging, and error management. Changes that 
occur in the course of design decisions and are common to multiple reusable assets can have 
a high impact in terms of the effort required to achieve them. For these types of assets, emer-
gent architectural decisions can be very costly. Thus, it is preferable to make these costly 
design decisions intentionally and early on, before many reusable composable assets are built. 
Planning promotes risk management and a mitigation mindset; for this reason, important 
architectural decisions that can have important impacts should be made early and not left to 
emerge at a later time. There is constant pressure on the architect to decide how much general-
ity to build into architectural decisions, and a balance must be found between more immediate 
and longer-term decisions.
12.1.2.3  Continuous Architecture Design and Refactoring
In the context of business agility and product orientation, systems evolve continuously until 
they are retired. The backlog for these products changes constantly to account for new fea-
tures, but also with the addition of new architectural drivers such as the need for supporting a 
new quality attribute requirement or addressing an architectural concern.
This implies that architectural design is an activity that is performed continuously as the 
product evolves. It does not mean, however, that at every sprint the architecture will change; 
rather, it suggests that changes should be expected and welcome at any time. Furthermore, 
refactoring is an activity that is performed continuously as changes in the product potentially 
result in technical debt that needs to be repaid. Tasks associated with architectural design and 
with technical debt should be part of the product backlog, as discussed in Section 10.3.
12.1.3  Designing to Support Sequential Development
Certain projects are still developed using a sequential phase approach, although this approach 
is not nearly as widely used as in the past. Although not a waterfall model, the Rational Unified 
Process (RUP) proposed that development projects should be divided into four major phases, 

12.1  Architecture Design and the Development Life Cycle  261
which are carried out sequentially; within these phases, a number of iterations are performed. 
The four phases of the RUP are as follows:
1.	
Inception. In this first phase, the goal is to achieve concurrence among project stake-
holders. During this phase, the scope of the project and a business architecture are 
defined. Also, a candidate architecture is established. This phase is equivalent to the 
estimation phase discussed previously.
2.	
Elaboration. In the second phase, the goal is to baseline the architecture of the system 
and to produce architectural prototypes. It should be noted that RUP places a strong 
emphasis on architecture.
3.	
Construction. In the third phase, the goal is to incrementally develop the system from 
the architecture that was defined in the previous phase.
4.	
Transition. In the fourth phase, the goal is to ensure that the system is ready for delivery. 
The system is transitioned from the development environment to its final operational 
environment.
We could argue that, from the elaboration phase until the end of the project, RUP intrin-
sically follows the iteration 0 + spikes approach described earlier. RUP also provides some 
guidance with respect to architectural design, although this guidance is far less detailed than 
that offered by ADD. Consequently, ADD can be used as a complement to the RUP. ADD iter-
ations can be performed during inception to establish the candidate architecture by following 
the approach described in Section 12.1.1. Furthermore, during the elaboration phase, the initial 
architecture is taken as a starting point for performing additional design iterations until an 
architecture that can be baselined is produced. During construction, additional ADD iterations 
may be performed as part of the development iterations.
A sequential phase approach also occurs in organizations that are transitioning from 
sequential development approaches to more agile ones. During the transition period, the orga-
nization performs what is often referred to as water-Scrum-fall development. In this context, 
the traditional planning phase is followed by a development phase that is performed using 
Scrum. Once the development team finishes a sprint, the product increment is passed on to a 
quality assurance (QA) team for testing. After tests are concluded, corrections are made, and 
user stories are accepted, the system is passed to an operations team for moving to produc-
tion. Although this type of development is not conducive to agility because it involves rela-
tively static requirements and a slow delivery, it can still benefit from ADD. Similar to what 
was discussed in the context of RUP, initial ADD iterations can be performed in the planning 
phase to produce an initial architecture, and an iteration 0 + spikes approach can be followed 
in the subsequent development. We recommend that teams or organizations that are stuck in 
the water-Scrum-fall approach introduce DevOps practices and architectural support for these 
practices, to reduce the time needed to move the system from development to production and 
achieve a more agile approach.

262  Chapter 12  The Architecture Design Process in the Organization
12.1.4  Architecture Design and the Testing Strategy
One important aspect of software development is planning how testing for a given system will 
be performed. Depending on the type of system being developed, a specific testing strategy 
needs to be established.
The architecture of a software system has a direct impact on how the system can be tested. 
This is particularly salient for complex distributed systems where many components (such as 
microservices) compose an application and where these components depend on middleware 
resources, databases, and external third-party systems. Establishing an effective testing strat-
egy requires an architecture to be defined, and must involve architects, quality engineers, and 
people who manage infrastructure.
Several concerns must be considered when developing the testing strategy, including 
these:
■
■How many pre-production environments are needed? As we saw in Section 6.1.1, there 
may be a number of pre-production environments where the system is integrated and 
tested before it is released to a production environment.
■
■How are dependencies handled in the pre-production environments? To function 
correctly, the system may require the presence of shared reusable services, middleware 
components, databases, and third-party systems. In an integration environment, some 
of these dependencies may be represented by mock-ups; however, in a staging environ-
ment, it may be necessary to have access to testing versions of these components (e.g., 
a sandbox of a third-party system). In the case of shared reusable services, such as the 
ones that are present when the application is built on top of an API platform, additional 
difficulties arise. These are associated with ensuring that tests are conducted using the 
correct versions of the services that are reused.
■
■How is infrastructure replicated across environments? A complex system may involve 
many moving parts, and it may be necessary to replicate its infrastructure across 
pre-production and production environments. The infrastructure as code approach can 
be very helpful here, as an infrastructure descriptor can be used to easily replicate the 
infrastructure across environments.
■
■How to reduce costs? Replicating the infrastructure across pre-production environments 
can be costly, especially in cloud environments. Approaches to reduce costs might 
include requesting more limited cloud resources in integration environments (e.g., non-
replicated resources, smaller databases) and turning off environments during nontesting 
times. This can be challenging when dealing with shared services such as the ones that 
exist in API platforms, as a shared service may be used by multiple teams.
Even relatively simple systems, such as the one discussed in the Hotel Pricing System case 
study in Chapter 8, present the challenges discussed here. For instance, the HPS is composed 
of four microservices with two different types of databases and a messaging system, besides a 
number of other components to support the front-end. Furthermore, it interacts with a number 
of external systems. For the HPS, the testing strategy involved the use of limited resources and 
mock-ups in the integration environment (including a version of Kafka and databases deployed 

12.2  Architecture Design and the Organization  263
in a container). This strategy limited the possibility of using infrastructure as code to set up 
the integration environment, but reduced costs. However, infrastructure as code was used to 
set up the staging and production environments.
12.2  Architecture Design and the Organization
In 1967, Melvin Conway wrote a paper in which he stated an observation that has become 
widely known as Conway’s law: “Organizations which design systems (defined broadly) are 
constrained to produce designs which are copies of the communication structures of these 
organizations”. Colloquially, this means that the structure of a software system will reflect the 
structure of the organization that designed it. For example, a colleague of ours reported on an 
architectural analysis that he did for which the system included three separate databases. Why 
three databases, you ask? Because there were three major contractors working on this large 
system and each of them had a database team, and they found something for these database 
teams to do! The result was likely a suboptimal architecture, but it was driven by Conway’s 
law, not by technical merit.
In the following subsections, we discuss aspects related to the organization that have a 
direct impact on the way that architectures are designed.
12.2.1  Designing as an Individual or as a Team
In large and complex projects, it seems straightforward that an architecture team should be 
responsible for performing the design. Even in smaller projects, however, you may find that 
having more than one person participate in the design process yields important advantages. 
You can decide if only one person is the architect and the others are observers (as in the prac-
tice of pair programming), or if the group actively collaborates on design decisions (although 
even here we recommend that you have one lead architect).
The team approach offers multiple benefits:
■
■Two (or more) heads can be better than one, particularly if the design problem that you 
are trying to solve is different from ones that you have addressed before.
■
■Different people can have different areas of expertise that are useful in the design of the 
architecture, as we will discuss in the next section.
■
■Design decisions are reflected upon and reviewed as they are being made and, as a con-
sequence, can be corrected immediately.
■
■It is well known that individuals have cognitive biases, and they can be unaware of these 
biases. Having multiple designers and reviewers means that such biases are more likely 
to be flagged.
■
■Less experienced people can participate in the design process, which can be an excellent 
mentoring practice.

264  Chapter 12  The Architecture Design Process in the Organization
Unfortunately, there are also certain difficulties with this approach:
■
■Design by committee can be complicated if agreement is not achieved in a reasonable 
time frame. The search for consensus can lead to “analysis paralysis”.
■
■The cost of design increases—and, in many cases, the time for design also increases.
■
■Managing the logistics can be complex, because this approach requires the regular avail-
ability of the group of people.
■
■You may encounter personality and political conflicts, resulting in resentment or hurt 
feelings, or in design decisions being heavily influenced by the person who shouts  
longest and loudest (“design by bullying”).
12.2.2  The Many Roles of the Architect
In most of this book, we have described the role of architect as the primary actor in the design 
activities. In smaller projects, a single person may fulfill this role and make design decisions 
across different fronts. In larger organizations, the role of architect may be split across many 
people, each of whom has a different area of expertise. This makes sense because it can be dif-
ficult for a single person to have expertise across all the areas where design decisions need to 
be made, especially on large and complex systems (see Chapter 9 for a good example of this). 
Here, we review some of the most common specializations of the architect role.
12.2.2.1  Software Architect
The software architect focuses on making design decisions at the software level. They are 
responsible for defining the structure of the software system, including its modules and their 
relationships—principally interfaces—to ensure that drivers are satisfied. The design of the 
software architecture also frequently involves the selection of technologies (i.e., externally 
developed components). The activities performed by this main role are, of course, discussed 
throughout this book.
12.2.2.2  Infrastructure Architect
Infrastructure architects focus on designing the underlying infrastructure that supports soft-
ware systems. They are responsible for defining the physical or virtual infrastructure com-
ponents, such as servers, networks, storage, and cloud services, and ensuring that these 
components are selected and configured to meet the performance, security, and scalability 
requirements of the system. Cloud architects are one type of infrastructure architects who 
specialize in the design of cloud-based infrastructures. They frequently specialize in the offer-
ings of a particular cloud provider and are knowledgeable about the resources that the cloud 
provider offers and how they can be used together. When following an infrastructure as code 
approach, this role may participate not only in the design decisions but also in the selection of 
technologies to support this approach and, possibly, the definition of source code files where 
the infrastructure is described.
As we have discussed elsewhere in this book (such as Chapter 6 on deployability and 
Chapter 7 on cloud-based solutions), quality attributes frequently require a combination of 
design decisions at the software and infrastructure levels. Thus, when different people occupy 

12.2  Architecture Design and the Organization  265
the roles of software and infrastructure architects, they need to collaborate closely to ensure 
that the software and the infrastructure work together optimally.
12.2.2.3  Security Architect
Security is a quality attribute of utmost importance in many of today’s systems. It is also a 
quality attribute that requires extensive knowledge in many different areas, including the types 
of threats and vulnerabilities that can occur in a system and the different design concepts 
(including tactics, patterns, and externally developed components) that can be used to address 
this quality attribute. Security architects must be experts on these topics, as they are responsi-
ble for designing and implementing secure information systems and infrastructures. Their role 
is to identify potential security risks, threats, and vulnerabilities, and to design and implement 
appropriate security controls and solutions to mitigate those risks.
12.2.2.4  Data Architect
Design decisions about data are the realm of the data architect. Data architects are responsible 
for designing mechanisms by which data can be stored and retrieved securely, reliably, and 
efficiently. They work closely with business analysts, data scientists, and other stakeholders to 
understand the organization’s data needs and to develop solutions that meet those needs. For 
instance, when designing enterprise applications, decisions about storage and management of 
operational versus analytical data must be made. Important aspects that need to be considered 
include the establishment of a data model and the life cycle of its abstractions, the management 
of metadata, the organization of data into different types of data stores, ways to secure access 
to the data, retention policies, and so forth.
12.2.2.5  Other Architect Roles
Many areas in software development require design decisions to be made; hence,  architec-
ture design may be performed in a number of additional domains besides the ones that we 
have already mentioned. An example is the design of the infrastructure required to automate 
deployments, which involves making design decisions on aspects such as the selection of tech-
nologies for the execution of pipelines as well as the definition of the steps of the pipeline 
itself. Like any other architectural decisions, the design of this infrastructure is associated with 
various drivers. For example, it has to map to the repository branching model (e.g., Gitflow) 
that is used in the organization. There may be performance requirements regarding the time it 
takes to execute the pipeline and specific constraints, such as not using proprietary solutions.
Another example of a specialized architecture role would be that of a blockchain archi-
tect. Similar to other architect roles, this person must have very deep expertise, in this case 
on matters related to blockchain. Design decisions made by this role include aspects such as 
the selection of the type of blockchain network, the type of consensus mechanism, the type of 
smart contracts, and the specific technologies to be used.
Another type of specialized architecture role is filled by AI or machine learning archi-
tects. AI/ML architects are experts on making design decisions in their field, such as the 
selection of algorithms and learning methods, online versus offline learning, centralized ver-
sus distributed approaches, model retraining frequencies, and so forth.

266  Chapter 12  The Architecture Design Process in the Organization
While the previous examples of specialized architect roles are on a similar level in the 
organizational chart, some organizations or models also have vertical hierarchies of architects. 
In this kind of hierarchy, enterprise architects are typically “above” the software architects. 
Whereas software architects are typically focused on the design of a particular system, enter-
prise architects are focused on making design decisions at the organizational level (i.e., the 
intentional architecture). They are responsible for designing and managing the overall archi-
tecture of an organization’s IT systems, standards, processes, and technologies. They work to 
align the organization’s technology strategy with its overall business objectives, and help to 
ensure that all IT systems are integrated, efficient, and effective. The enterprise architecture 
group generally establishes a governance body that provides guidance for the activities per-
formed by the other architects. When doing so, they define constraints (in the context of the 
architectural design drivers) that should be considered by the other architect roles like soft-
ware and infrastructure architects.
12.2.2.6  Architect Roles and ADD
Integrating different architect roles into ADD can be performed by considering the goals of 
specific design iterations (step 2 of ADD). For example, an iteration whose goal is a driver 
focused on producing an initial structure of the system may require only the expertise of a 
software architect. However, an iteration whose goal is a driver associated with creating an 
efficient catalog of products for a large e-commerce system that will be hosted in the cloud 
may involve not only a software architect but also a cloud and a data architect, and perhaps 
even an enterprise architect. Each architect role plays an important part in the key design steps 
of ADD (steps 3–5), as each is an expert on the design concepts associated with their particu-
lar area and also in instantiating these design concepts to address the driver that is the goal of 
the iteration.
These architect roles must collaborate, because a decision made by the software archi-
tect to store information may be complemented by a recommendation from the data architect 
regarding the type of database to be used. This, in turn, must be validated with the cloud 
architect to discuss the database capabilities that the cloud provider offers to see which one is 
the best fit. The software architect may perform the design iteration and consult with the other 
architects when their point of view is required, or all the architects may participate in perform-
ing the key design steps. As mentioned in Section 12.2.1, while the design of the system may 
be performed by a team of experts, one person should be responsible for leading the design 
process and its iterations; ideally, that is the responsibility of the software architect.
12.2.3  Architecture Guidelines for the Organization
In larger organizations, a number of different teams may work on the development of appli-
cations or a platform that supports these applications (such as the API platform we presented 
in Chapter 5). Within such organizations, it is often desirable that teams solve certain design 
aspects in a common way. Here we discuss some approaches that are helpful to achieve this.

12.2  Architecture Design and the Organization  267
12.2.3.1 Addressing Specific Architectural Concerns
While teams usually focus on solving specific design problems associated with their particular 
projects, there are some common design aspects that all teams must address. These aspects are 
typically associated with making decisions related to the architectural concerns discussed in 
Section 2.4.4, and they involve considerations such as the following:
■
■The selection of a deployment approach (monolithic versus non-monolithic)
■
■The selection of mechanisms and standards for communication
■
■The selection of security mechanisms to address aspects such as authorization and 
authentication, and encryption of data
■
■The selection of an approach for versioning APIs
■
■The selection of mechanisms for logging both for auditing and for debugging
■
■The selection of mechanisms to support observability and debugging, including error 
management and distributed tracing
■
■The selection of appropriate data storage options (e.g., how long data needs to be stored), 
considering aspects such as compression and encryption
■
■The selection of mechanisms to support internationalization, including the management 
of time, currencies, and languages
■
■The selection of mechanisms to support privacy, including protecting sensitive informa-
tion from exposure and preparation of data for testing environments
■
■The selection of development tools and platforms, including programming languages, 
repositories, coding conventions, and standards
Within an organization, it is frequently desirable to establish uniform mechanisms to 
address a number of these specific concerns, to simplify support or hiring and training of 
development resources, among other things. A good example is logging. If every team decides 
on a particular way to address logging, issues might arise if the company decides to centralize 
the storage, analysis, and comparison of logs. As an alternative, the organization can provide 
guidelines and mechanisms, such as libraries or services to handle this specific architectural 
concern, which facilitate the centralized approach to storage and analysis. The definition of 
these guidelines and common design decisions to address these architectural concerns can be 
performed by a governance body such as the enterprise architecture group.
12.2.3.2  Design Concepts Catalogs
The guidelines discussed in the previous section focus on addressing common architectural 
concerns. It may also be useful for the organization to provide support for identifying design 
concepts to solve recurring problems.
As we saw in Section 4.4, the selection of concepts is one of the most challenging aspects 
of the design process. This problem is exacerbated by the fact that information is scattered 
across many locations: Architects usually need to consult several pattern and tactics catalogs 
and to perform extensive research to find the design concepts that can be considered and used.
One possible way to resolve this issue is to create design concepts catalogs. These cata-
logs group collections of design concepts for particular application domains. Such catalogs are 

268  Chapter 12  The Architecture Design Process in the Organization
intended to facilitate the identification and selection of concepts when performing design. They 
are also useful in enhancing consistency in the designs across the organization. For example, 
designers may be required to use the technologies in a particular catalog as much as possible 
because this facilitates estimation, reduces learning curves, reduces costs and risks, facilitates 
evaluation of proposed architectures, and may lead to opportunities for reuse. Catalogs can 
also be useful for training purposes. An example of a design concepts catalog can be found 
online in the companion site to this book.
The creation of these catalogs requires considerable effort. Once created, they should 
be maintained as new design concepts, and particularly new technologies, are introduced or 
removed in the organization. This effort is worthwhile, however, as these catalogs are a valu-
able organizational asset.
12.2.4  Architecture Groups
Large companies frequently employ a number of architects. In that situation, it is desirable to 
establish architecture groups that promote the collaboration of architects in activities such as 
the following:
■
■Sharing knowledge: Architects should be incentivized to present new approaches and 
tools to the group, for learning purposes or for evaluating whether to incorporate them 
into the set of technologies used in the company. Contributing to the company’s design 
concepts catalog, if it exists, is another effective form of sharing knowledge.
■
■Discussing particular issues: Architects can present particular design challenges or 
solutions to the group to obtain feedback or opinions from other points of view. This can 
also highlight opportunities for sharing and reuse.
■
■Evaluating proposed solutions: In Section 11.6, we discussed an analysis technique 
called “scenario-based design reviews” and the steps to perform it. This technique 
requires a team of evaluators who review and question the design to uncover risks. Other 
architects in the company are frequently ideal candidates to participate in this evaluation 
team.
These activities can be performed periodically—for example, through a weekly architec-
ture group meeting. Note that architectural evaluations usually require a dedicated meeting.
12.3  Summary
In this chapter, we discussed how ADD can be used in relation to several organizational 
aspects. ADD can be used from the project’s inception to facilitate estimation using standard 
components. As the project evolves, it can be used in conjunction with both modern and tra-
ditional software development life-cycle methods. In general, ADD is a valuable complement 
to life-cycle methods that do not provide detailed guidance on how to perform architectural 
design.

12.4  Further Reading  269
We also discussed aspects related to supporting architectural design at the organizational 
level, including the composition of the design team, the different architect roles, the creation 
of guidelines at the organizational level to address architectural concerns, and the collection 
of design concepts into catalogs. Finally, we discussed the benefits of creating architecture 
groups in organizations where there are a number of architects in the IT department.
12.4  Further Reading
Organizational structure and its influences on software architecture are addressed in the field 
of enterprise architecture management. Enterprise architecture frameworks are discussed in 
F. Ahlemann et al. (eds.), Strategic Enterprise Architecture Management: Challenges, Best 
Practices, and Future Developments, Springer-Verlag, 2012.
The book Architecture Centric Project Management: A Practical Guide by D. Paulish, 
Addison-Wesley Professional, 2002, discusses many aspects of project management and con-
nects them to software architecture.
The definitions of intentional architecture and emergent design are © by Scaled Agile, 
Inc. Additional details about the Scaled Agile Framework can be found online: https://scaled-
agileframework.com/.
The concept of the cone of uncertainty has existed in the project management domain 
for decades. Barry Boehm employed this concept to software projects in his seminal work 
Software Engineering Economics, Prentice Hall, 1981.
A nice set of articles looking at the relationship between architecture and Agile methods 
can be found in the April 2010 special issue of IEEE Software on this topic.
A number of studies have looked at how architecture and agility methods complement 
and support each other, such as S. Bellomo, I. Gorton, and R. Kazman, “Insights from 15 
Years of ATAM Data: Towards Agile Architecture”, IEEE Software, 2015; and S. Bellomo, R. 
Nord, and I. Ozkaya, “A Study of Enabling Factors for Rapid Fielding: Combined Practices to 
Balance Speed and Stability”, Proceedings of ICSE 2013, 982–991, 2013.
Barry Boehm and Richard Turner have taken an empirical look at the topic of the rela-
tionship between agility and “discipline” (not just architecture) in their book Balancing Agility 
and Discipline: A Guide for the Perplexed, Addison-Wesley, 2004.
The practice of creating architectural “spikes” as a means of resolving uncertainty in 
Agile sprints is discussed in T. C. N. Graham, R. Kazman, and C. Walmsley, “Agility and 
Experimentation: Practical Techniques for Resolving Architectural Tradeoffs”, Proceedings of 
the 29th International Conference on Software Engineering (ICSE 29), (Minneapolis, MN), 
May 2007. A general discussion of spikes can be found at www.scrumalliance.org/community/
articles/2013/march/spikes-and-the-effort-to-grief-ratio.

270  Chapter 12  The Architecture Design Process in the Organization
Many practitioners and researchers have thought deeply about how Agile methods and 
architectural practices fit together. Some of the best examples of this thinking can be found in 
the following sources:
■
■S. Brown, Software Architecture for the Developers, LeanPub, 2013.
■
■J. Bloomberg, The Agile Architecture Revolution, Wiley CIO, 2013.
■
■A. Cockburn, “Walking Skeleton”, http://alistair.cockburn.us/Walking+skeleton. 
■
■“Manifesto for Agile Software Development”, http://agilemanifesto.org/.
■
■Scott ambler and mark lines, “Scaling Agile Software Development: Disciplined 
Agility at Scale”,  http://disciplinedagileconsortium.org/Resources/Documents/
ScalingAgileSoftwareDevelopment.pdf.
■
■The Agile Architecting Collection in the SEI Digital Library: https://resources.sei.cmu.
edu/library/asset-view.cfm?assetid=483941.
An extensive treatment of estimation techniques, including estimation using standard 
components, is given in S. McConnell, Software Estimation: Demystifying the Black Art, 
Microsoft Press, 2006.
The integration of ADD 2.0 (as well as other architecture development methods) with 
RUP is discussed in R. Kazman, P. Kruchten, R. Nord, and J. Tomayko, Integrating Software-
Architecture-Centric Methods into the Rational Unified Process, Technical Report CMU/SEI-
2004-TR-011, July 2004.
Considerable attention has been given to the problem of architecture knowledge represen-
tation and management. For a good overview of this area, see P. Kruchten, P. Lago, and H. Van 
Vliet, “Building Up and Reasoning About Architectural Knowledge”, in Quality of Software 
Architectures, Springer, 2006. For a perspective on tools for architecture knowledge manage-
ment, see A. Tang, P. Avgeriou, A. Jansen, R. Capilla, and M. Ali Babar, “A Comparative 
Study of Architecture Knowledge Management Tools”, Journal of Systems and Software, 
83(3), 352–370, 2010.
12.5  Discussion Questions
1.	
Which additional actions relative to architectural design would you suggest to undertake 
in the estimation phase to further reduce the uncertainty in estimates? Consider that 
there are contexts where the work performed in this phase is not compensated, and will 
be compensated only when the project is approved.
2.	
Considering an iteration 0 + spikes approach, what can happen if the team doesn’t devote 
time to design for deployability (i.e., they do not design and set up the execution environ-
ments and build automation infrastructures)? Once the project is running, how can they 
remedy this situation?
3.	
What other architect roles can you identify besides the ones mentioned in Section 
12.2.2?

12.5  Discussion Questions  271
4.	
Considering the role of an AI architect, what kind of drivers and design concepts can be 
associated with this particular architect role? Provide examples.
5.	
What problems can happen in an organization where multiple teams develop APIs but 
there are no company-wide guidelines on how to address authorization and authentica-
tion, versioning, or error management?
6.	
Similarly, what problems can occur in an organization that develops microservices,  
but there are no company-wide guidelines on what microservices to create, at what  
granularity, and accessing what data?
7.	
Given Conway’s law, it is obvious that an architect needs to “shepherd” both the tech-
nical and social dimensions of a project. What are some techniques for monitoring 
a project’s sociotechnical alignment, and what are some techniques for remedying 
misalignments?

This page intentionally left blank 

273
13
Final Thoughts
In this chapter we reflect, once again, on the nature of design and why we need methods for 
design. This is, after all, the major point of this book! And we leave you with a few words 
about where to go with the information and skills that you have gleaned from reading this 
book.
13.1  On the Need for Methods
Given that you have prevailed and reached this final chapter, we can assume that you are com-
mitted to being a professional software architect. Being a professional means that you can per-
form consistently at a high level in all sorts of business and technical contexts. To achieve this 
level of consistent performance, everyone needs methods. This is why standardized methods 
are employed in manufacturing, science, healthcare, education, IT, and many other domains.
We all need methods when we are performing complex tasks that have serious conse-
quences if we get them wrong. Consider this: Jet pilots and surgeons are two of the most highly 
trained groups of professionals in the world, and yet they use checklists and standardized pro-
cedures for every important task that they perform. Why? Because the consequences of mak-
ing a mistake are serious. You probably will not be designing the architectures for systems 
that have life-and-death consequences. Even so, the systems that you do design, particularly if 
they are large and complex, may very well have consequences for the health and well-being of 
your organization. If you are designing a throwaway prototype or a trivial system, perhaps an 
explicit architecture design step may be omitted. If you are designing the nth variant of a sys-
tem that you have created over and over in the past, perhaps architecture design is little more 
than a cut-and-paste from your prior experiences.
But if the system you are charged with creating or evolving is nontrivial and if there is 
risk associated with its creation, then you owe it to yourself, you owe it to your organization, 
and you owe it to your profession to do the best job that you can in this most critical step in the 
software development life cycle. To achieve that goal, you probably need a method. Methods 
help to ensure uniformity, consistency, and completeness. Methods help you take the right 
steps and ask the right questions.

274  Chapter 13  Final Thoughts
Of course, no method can substitute for proper training and education. No one would 
trust a novice pilot at the controls of a 787 or a first-year medical student wielding a scalpel in 
an operating theater, armed only with a method or a checklist. A method, however, is a key to 
producing high-quality results repeatedly. And this is, after all, what we all desire as software 
engineering professionals.
Fred Books, writing about the design process, said:
Any systematization of the design process is a great step forward compared to “Let’s 
just start coding, or building.” It provides clear steps for planning a design project. It 
furnishes clearly definable milestones for planning a schedule and for judging prog-
ress. It suggests project organization and staffing. It helps communication within the 
design team, giving everyone a single vocabulary for the activities. It wonderfully 
helps communication between the team and its manager, and between the manager 
and other stakeholders. It is readily teachable to novices. It tells novices facing their 
first design assignments where to begin.
Design is just too important to be left to chance. And there needs to be a better way of 
getting good at design than to “shoot yourself in the foot repeatedly”. As the Nobel Prize- 
winning scientist Herbert Simon wrote in 1969, “Design . . . is the core of all professional 
training; it is the principal mark that distinguishes the professions from the sciences. Schools 
of engineering, as well as schools of architecture, business, education, law, and medicine, are 
all centrally concerned with the process of design”. Simon went on to say that lack of profes-
sional competence is caused by the relative neglect of design in universities’ curricula. This 
trend is, we are happy to note, gradually reversing. Even so, more than 50 years later we cannot 
claim victory; this is still a cause for concern and attention.
In this book we have provided you with a road-tested method—ADD—for doing archi-
tectural design. Methods are useful in that they provide guidance for the novice and reas-
surance for the expert. Like any good method, ADD has a set of steps. And while ADD has 
evolved over the years, the current set of steps has remained stable for the past decade, even 
as our focus has broadened from single systems deployed on your own hardware to collections 
of services deployed on the cloud, from big-bang deployments to continuous delivery, from 
design of modules to design of microservices and APIs.
But just as important, we have focused on the broader architecture life cycle and shown how 
some changes to the design process can help make your life as an architect better, and provide 
you with better outcomes. For example, we have expanded the set of inputs that you need to think 
about to include things like design purpose and architectural concerns. This broader view helps 
you create an architecture that not only meets your customer’s requirements, but also is aligned 
with the business needs of your team and your organization. In addition, we have shown that 
design can and should be guided by a “design concepts catalog”—a corpus of reusable architec-
tural knowledge consisting of reference architectures, patterns, tactics, and externally developed 
components such as frameworks and technology families. By cataloging these concepts, design 
can be made more predictable, efficient, and repeatable. Finally, we have argued that design 
should be documented, perhaps informally in sketches with an associated rationale, and should 
be accompanied by a consistent practice of analyzing the decisions made.

13.2  Future Directions  275
If we are to conceive of ourselves as software engineers, we need to take the title of 
“engineer” seriously. No mechanical or electrical or structural engineer would commit signifi-
cant resources to a design that was not based on sound principles and components, or that was 
not analyzed and documented. We think that software engineering in general, and software 
architecture specifically, should strive for similar goals. We are not “artistes”, for whom cre-
ativity is paramount; we are engineers, so predictability and repeatability should be our most 
cherished goal.
13.2  Future Directions
Although technologies have evolved significantly from the first edition of the book, the prin-
ciples of software architecture haven’t changed. The process of designing an architecture 
remains the same and, as we mentioned in Chapter 4, we did not modify ADD from the ver-
sion that we published in the first edition of the book. While this gives us comfort—knowing 
that design is design and how we do it does not depend on the latest technologies—recent 
developments in artificial intelligence may actually have an impact on the way we architect in 
the future. Specialized large language models (LLMs) may be useful in several areas associ-
ated with software architecture, including for design:
■
■Generating or analyzing requirements and architectural drivers
■
■Assisting in the selection of design concepts and their instantiation
■
■Automating the generation of diagrams and documentation
■
■Generating test cases associated with the architectural design
■
■Generating key code snippets, such as interfaces, that manifest an architect’s design 
rules
and for refactoring and maintenance:
■
■Explaining the concepts employed in existing code
■
■Suggesting code changes to remove bad smells
■
■Updating documentation to match what is actually implemented
As we write this book, it is still too early to consistently perform these tasks effectively 
and with high confidence. But this situation is changing rapidly and proofs-of-concept are 
already feasible; developers the world over are embracing LLMs. The future use of this tech-
nology is exciting, as properly trained LLMs can be based on a broader knowledge base and 
have less bias than humans. Also, these tools can free up architects and developers to focus 
on the more interesting and challenging aspects of design, rather than the mundane stuff (like 
documentation) that people often resist or avoid. We will surely witness exciting developments 
around this topic in the coming years.

276  Chapter 13  Final Thoughts
13.3  Next Steps
Where should you go from here? We see four answers to this question. One answer focuses on 
what you can do as an individual to hone your skills and experience as an architect. The sec-
ond answer revolves around how you might engage your colleagues to think more consciously 
about architecture design. The third answer is where your organization can go with a more 
explicit commitment to architecture design. And the fourth answer is about how you can con-
tribute to your community, and to the larger community of software architects.
Our advice to you, as an individual, about how to proceed is simple: practice. Like any 
other complex skill worth having, your skill as an architect will not come immediately, but 
your confidence should increase steadily. “Fake it till you make it” is the best advice that we 
can give. Having a method that you can consult, and a ready supply of common design con-
cepts, gives you a solid foundation from which to “fake it” and learn.
To help you practice your skills and to engage your colleagues, we have developed an 
architecture game. This game, which is called “Smart Decisions”, can be found at www.smart-
decisionsgame.com. It simulates the architecture design process using ADD and promotes 
learning about it in a fun, pressure-free way. The game is focused on a specific application 
domain, but it can be easily adapted to other application domains.
You might also think about next steps to be taken in your organization. You can be an 
agent for change. Even if your company does not “believe in” architecture, you can still prac-
tice many of the ideas embodied in this book and in ADD. Ensure that your quality attri-
bute requirements are clear by insisting on concrete response measures. Even when you face 
tight deadlines and schedule pressures, try to get agreement on the major architectural pat-
terns being employed. Do periodic, quick, informal design reviews with colleagues, huddled 
around a whiteboard, and ask yourself reflective questions. None of these “next steps” needs 
to be daunting or hugely time-consuming. And we believe—and our industrial experience has 
shown—that they will be self-reinforcing. Better designs will lead to better outcomes, which 
will lead you and your group and your organization to want to do more of the same.
Finally, you can contribute to your local software engineering community, and even to 
the worldwide community of software architects. You could, for example, play the architecture 
game in a local software engineering meetup and then share your experiences. You could con-
tribute case studies about your successes and failures as an architect with real-world projects. 
We strongly believe that examples are the best way to teach. While we have provided two case 
studies in this book, more is always better.
Happy architecting!

13.5  Discussion Questions  277
13.4  Further Reading
The long quotation by Fred Books in this chapter comes from his thought-provoking book  
The Design of Design: Essays from a Computer Scientist, Pearson, 2013.
Many of the ideas in this chapter, in this book, and in the field of software architec-
ture in general can be traced back to Herbert Simon’s seminal book on the science of design:  
The Sciences of the Artificial, MIT Press, 1969.
13.5  Discussion Questions
1.	
Experienced architects are sometimes reluctant to accept the idea that design can be 
performed using a systematic method. Why do you think this is the case? What are the 
benefits of using a method such as ADD?
2.	
Can you think of other areas of software architecture where large language models can 
be used?
3.	
What benefits or drawbacks can you identify of using artificial intelligence tools to assist 
in the design process?
4.	
As architects and developers rely more on existing frameworks and components,  
especially the ones from cloud providers, does their role get more or less important?

This page intentionally left blank 

279
Appendix
Tactics-Based Questionnaires
This appendix provides a set of tactics-based questionnaires for the ten most important and 
widely used quality attributes: availability, deployability, energy efficiency, interoperability, 
modifiability, performance, safety, security, testability, and usability. These questionnaires 
could be used by an analyst, who poses each question, in turn, to the architect and records the 
responses, as a means of conducting a lightweight architecture review. Alternatively, the ques-
tionnaires could be employed as a set of reflective questions that you could, on your own, use 
to examine your architectural choices.
In either case, to use these questionnaires, simply follow these four steps:
1.	
For each tactics question, fill the “Supported” column with Y if the tactic is supported in 
the architecture and with N otherwise. The tactic name in the “Tactics Question” column 
appears in bold.
2.	
If the answer in the “Supported” column is Y, then in the “Design Decisions and 
Location” column, describe the specific design decisions made to support the tactic and 
enumerate where these decisions are manifested (located) in the architecture. For exam-
ple, indicate which code modules, frameworks, or packages implement this tactic.
3.	
In the “Risk” column, indicate the anticipated/experienced difficulty or risk of imple-
menting the tactic using a (H = high, M = medium, L = low) scale. For example, a tactic 
that was of medium difficulty or risk to implement (or which is anticipated to be of 
medium difficulty, if it has not yet been implemented) would be labeled M.
4.	
In the “Rationale” column, describe the rationale for the design decisions made (includ-
ing a decision to not use this tactic). Briefly explain the implications of this decision. For 
example, you might explain the rationale and implications of the decision in terms of the 
effort on cost, schedule, evolution, and so forth.

280  Appendix  Tactics-Based Questionnaires
A.1  Availability
Tactics  
Group
Tactics Question
Supported? 
(Y/N)
Risk Design 
Decisions  
and Location
Rationale 
and 
Assumptions
Detect  
faults
Does the system use ping/
echo to detect a failure of a 
component or connection, or 
network congestion? 
Does the system use a 
component to monitor the state 
of health of other parts of the 
system? A system monitor can 
detect failure or congestion in 
the network or other shared 
resources, such as from a 
denial-of-service attack.
Does the system use a 
heartbeat—a periodic message 
exchange between a system 
monitor and a process—to 
detect a failure of a component 
or connection, or network 
congestion?
Does the system use a time 
stamp to detect incorrect 
sequences of events in 
distributed systems?
Does the system do any sanity 
checking—checking the 
validity or reasonableness of 
a component’s operations or 
outputs?
Does the system do condition 
monitoring—checking 
conditions in a process 
or device, or validating 
assumptions made during the 
design?
Does the system use voting 
to check that replicated 
components are producing the 
same results? The replicated 
components may be identical 
replicas, functionally redundant, 
or analytically redundant.
Do you use exception 
detection to detect a system 
condition that alters the normal 
flow of execution (e.g., system 
exception, parameter fence, 
parameter typing, timeout)?
Can the system do a self-test to 
test itself for correct operation?

A.1  Availability  281
Tactics  
Group
Tactics Question
Supported? 
(Y/N)
Risk Design 
Decisions  
and Location
Rationale 
and 
Assumptions
Recover 
from faults 
(preparation 
and repair)
Does the system employ 
redundant spares?
Is a component’s role as active 
versus spare fixed, or does it 
change in the presence of a 
fault? What is the switchover 
mechanism? What is the trigger 
for a switchover? How long does 
it take for a spare to assume its 
duties?
Does the system employ 
exception handling to deal with 
faults? Typically, the handling 
involves either reporting the 
fault or handling it, potentially 
masking the fault by correcting 
the cause of the exception and 
retrying.
Does the system employ 
rollback, so that it can revert to 
a previously saved good state 
(the “rollback line”) in the event 
of a fault?
Can the system perform in-
service software upgrades to 
executable code images in a 
non-service-affecting manner?
Does the system systematically 
retry in cases where the 
component or connection failure 
may be transient?
Can the system simply ignore 
faulty behavior (e.g., ignore 
messages sent from a source 
when it is determined that those 
messages are spurious)?
Does the system have a policy 
of degradation when resources 
are compromised, maintaining 
the most critical system 
functions in the presence 
of component failures, and 
dropping less critical functions?
Does the system have 
consistent policies 
and mechanisms for 
reconfiguration after failures, 
reassigning responsibilities to 
the resources left functioning, 
while maintaining as much 
functionality as possible?
continues

282  Appendix  Tactics-Based Questionnaires
Tactics  
Group
Tactics Question
Supported? 
(Y/N)
Risk Design 
Decisions  
and Location
Rationale 
and 
Assumptions
Recover 
from faults 
(reintroduction)
Can the system operate a 
previously failed or in-service 
upgraded component in a 
“shadow mode” for a predefined 
time prior to reverting the 
component back to an active 
role?
If the system uses active 
or passive redundancy, 
does it also employ state 
resynchronization, to send 
state information from active to 
standby components?
Does the system employ 
escalating restart—that is, 
does it recover from faults by 
varying the granularity of the 
component(s) restarted and 
minimizing the level of service 
affected?
Can message processing and 
routing portions of the system 
employ nonstop forwarding, 
where functionality is split into 
supervisory and data planes? 
In this case, if a supervisor fails, 
a router continues forwarding 
packets along known routes 
while protocol information is 
recovered and validated.
Prevent faults
Can the system remove 
components from service, 
temporarily placing a system 
component in an out-of-
service state, for the purpose 
of mitigating potential system 
failures? 
Does the system employ 
transactions—bundling state 
updates so that asynchronous 
messages exchanged between 
distributed components are 
atomic, consistent, isolated, and 
durable?

A.2  Deployability  283
Tactics  
Group
Tactics Question
Supported? 
(Y/N)
Risk Design 
Decisions  
and Location
Rationale 
and 
Assumptions
Does the system use a 
predictive model to monitor the 
state of health of a component 
to ensure that the system 
is operating within nominal 
parameters? When conditions 
are detected that are predictive 
of likely future faults, the model 
initiates corrective action.
Does the system prevent 
exceptions from occurring by, 
for example, masking a fault, 
using smart pointers, abstract 
data types, or wrappers?
Has the system been designed 
to increase its competence 
set, for example, by designing 
a component to handle more 
cases—faults—as part of its 
normal operation?
A.2  Deployability
Tactics  
Group
Tactics Question
Supported? 
(Y/N)
Risk
Design 
Decisions 
and Location
Rationale and 
Assumptions
Manage 
deployment 
pipeline
Can the system scale 
rollouts of a new version 
of a service gradually, to 
subsets of the user base, 
rather than deploying to the 
entire user base at once?
Is there a standard and 
broadly used way to script 
deployment commands 
for the complex steps 
to be carried out and 
precisely orchestrated in a 
deployment?
If a deployment has defects, 
can it be rolled back to 
its prior state, in a fully 
automated fashion?
continues

284  Appendix  Tactics-Based Questionnaires
Tactics  
Group
Tactics Question
Supported? 
(Y/N)
Risk
Design 
Decisions 
and Location
Rationale and 
Assumptions
Manage 
deployed 
system
Does the system rigorously 
manage service 
interactions among 
deployed versions of 
interacting services so 
that incompatibilities are 
avoided?
Does the system package 
dependencies among 
elements so that they are 
deployed together with all 
dependencies required to 
execute?
Does the system implement 
feature toggle—a “kill 
switch” that can be used for 
new features to automatically 
disable them without forcing 
a new deployment?
Does the system 
externalize configurations, 
avoiding any hardcoded 
configurations that limit the 
possibility of moving it from 
one environment to another?
A.3  Energy Efficiency
Tactics  
Group
Tactics Question
Supported? 
(Y/N)
Risk
Design 
Decisions 
and Location
Rationale and 
Assumptions
Resource 
monitoring
Does your system 
meter the use of 
energy? That is, does 
the system collect 
data about the actual 
energy consumption 
of computational 
devices, via a sensor 
infrastructure, in near 
real time?

A.3  Energy Efficiency  285
Tactics  
Group
Tactics Question
Supported? 
(Y/N)
Risk
Design 
Decisions 
and Location
Rationale and 
Assumptions
Does the system 
statically classify 
devices and 
computational 
resources? That is, 
does the system have 
reference values to 
estimate the energy 
consumption of a 
device or resource (in 
cases where real-time 
metering is infeasible 
or too computationally 
expensive)? 
Does the system 
dynamically 
classify devices 
and computational 
resources? In 
cases where static 
classification is not 
accurate due to varying 
load or environmental 
conditions, does the 
system use dynamic 
models, based on 
prior data collected, to 
estimate the varying 
energy consumption of 
a device or resource at 
runtime?
Resource 
allocation
Does the system reduce 
usage to scale down 
resource usage? That 
is, can the system 
deactivate resources 
when demands no 
longer require them, 
to save energy? This 
may involve spinning 
down hard drives, 
darkening displays, 
turning off CPUs or 
servers, running CPUs 
at a slower clock rate, or 
shutting down memory 
blocks of the processor 
that are not being used.
continues

286  Appendix  Tactics-Based Questionnaires
Tactics  
Group
Tactics Question
Supported? 
(Y/N)
Risk
Design 
Decisions 
and Location
Rationale and 
Assumptions
Does the system 
schedule resources 
to more effectively 
utilize energy, given 
task constraints 
and respecting task 
priorities, switching 
computational resources 
(e.g., service providers) 
to ones that offer better 
energy efficiency or 
lower energy costs? Is 
scheduling based on 
data collected (using 
one or more resource 
monitoring tactics) about 
the state of the system? 
Does the system make 
use of a discovery 
service to match service 
requests to service 
providers? In the context 
of energy efficiency, a 
service request could be 
annotated with energy 
requirement information, 
allowing the requestor 
to choose a service 
provider based on its 
(possibly dynamic) 
energy characteristics.
Reduce 
resource 
demand
Do you consistently 
attempt to reduce 
resource demand? 
Here, you may insert 
the questions in 
this category from 
the Tactics-Based 
Questionnaire for 
Performance  
(Chapter 9).

A.4  Integrability  287
A.4  Integrability
Tactics  
Group
Tactics Question
Supported? 
(Y/N)
Risk
Design 
Decisions 
and 
Location
Rationale and 
Assumptions
Limit 
dependencies
Does the system 
encapsulate the 
functionality of each 
element by introducing 
explicit interfaces and 
requiring that all access 
to the elements passes 
through these interfaces?
Does the system broadly 
use intermediaries for 
breaking dependencies 
between components—for 
example, removing a data 
producer’s knowledge of its 
consumers?
Does the system abstract 
common services, 
providing a general, 
abstract interface for similar 
services?
Does the system provide 
a means to restrict 
communication paths 
between components?
Does the system adhere to 
standards in terms of how 
components interact and 
share information with each 
other?
Adapt
Does the system provide 
the ability to statically 
(i.e., at compile time) 
tailor interfaces—that 
is, the ability to add or 
hide capabilities of a 
component’s interface 
without changing its API or 
implementation?
Does the system 
provide a discovery 
service, cataloguing and 
disseminating information 
about services?
continues

288  Appendix  Tactics-Based Questionnaires
Tactics  
Group
Tactics Question
Supported? 
(Y/N)
Risk
Design 
Decisions 
and 
Location
Rationale and 
Assumptions
Does the system provide 
a means to configure the 
behavior of components 
at build, initialization, or 
runtime?
Coordinate
Does the system include an 
orchestration mechanism 
that coordinates and 
manages the invocation of 
components so they can be 
unaware of each other?
Does the system provide 
a resource manager 
that governs access to 
computing resources?
A.5  Modifiability
Tactics 
Group
Tactics Question
Supported? 
(Y/N)
Risk
Design 
Decisions 
and 
Location
Rationale and 
Assumptions
Increase 
cohesion
Do you make modules more 
cohesive by splitting the 
module? For example, if you have 
a large, complex module, can you 
split it into two (or more) more 
cohesive modules?
Do you make modules more 
cohesive by redistributing 
responsibilities? For example, 
if responsibilities in a module 
do not serve the same purpose, 
they should be placed in other 
modules.
Reduce 
coupling
Does the system consistently 
encapsulate functionality? This 
typically involves isolating the 
functionality under scrutiny and 
introducing an explicit interface 
to it.

A.6  Performance  289
Tactics 
Group
Tactics Question
Supported? 
(Y/N)
Risk
Design 
Decisions 
and 
Location
Rationale and 
Assumptions
Does the system consistently 
use an intermediary to keep 
modules from being too tightly 
coupled? For example, if A calls 
concrete functionality C, you might 
introduce an abstraction B that 
mediates between A and C. 
Do you restrict dependencies 
between modules in a systematic 
way? Or is any system module 
free to interact with any other 
module?
Does the system abstract 
common services, in cases 
where you are providing several 
similar services? For example, this 
technique is often used when you 
want your system to be portable 
across operating systems, 
hardware, or other environment 
variations.
Defer 
binding
Does the system regularly defer 
binding of important functionality 
so that it can be replaced later 
in the life cycle, perhaps even 
by end users? For example, 
do you use plug-ins, add-ons, 
or user scripting to extend the 
functionality of the system?
A.6  Performance
Tactics 
Group
Tactics Question
Supported? 
(Y/N)
Risk
Design 
Decisions 
and 
Location
Rationale and 
Assumptions
Control 
resource 
demand
Can you manage work 
requests? For example, do 
you have in place a service 
level agreement (SLA) that 
specifies the maximum event 
arrival rate that you are willing 
to support? Can you manage 
the rate at which you sample 
events arriving at the system?
continues

290  Appendix  Tactics-Based Questionnaires
Tactics 
Group
Tactics Question
Supported? 
(Y/N)
Risk
Design 
Decisions 
and 
Location
Rationale and 
Assumptions
Does the system monitor and 
limit its event response? 
Does the system limit the 
number of events it responds 
to in a time period, to ensure a 
predictable response?
Given that you may have more 
requests for service than 
available resources, does the 
system prioritize events?
Does the system reduce 
the overhead of responding 
to service requests by, 
for example, removing 
intermediaries or co-locating 
resources?
Does the system monitor 
and bound execution time? 
More generally, do you bound 
the amount of any resource 
(e.g., memory, CPU, storage, 
bandwidth, connections, locks) 
expended in response to 
requests for services? 
Do you increase resource 
efficiency? For example, 
do you regularly improve 
the efficiency of algorithms 
in critical areas to decrease 
latency and improve 
throughput?
Manage 
resources
Can you allocate more 
resources to the system or its 
components?
Are you employing 
concurrency? If requests can 
be processed in parallel, the 
blocked time can be reduced. 
Are there computations that 
can be replicated on different 
processors?
Is there data that can be 
cached (to maintain a local 
copy that can be quickly 
accessed) or replicated (to 
reduce contention)?

A.7  Safety  291
Tactics 
Group
Tactics Question
Supported? 
(Y/N)
Risk
Design 
Decisions 
and 
Location
Rationale and 
Assumptions
Can queue sizes be bounded 
to place an upper bound on the 
resources needed to process 
stimuli?
Have you ensured that the 
scheduling strategies you 
are using are appropriate for 
your performance concerns?
A.7  Safety
Tactics  
Group
Tactics Question
Supported? 
(Y/N)
Risk Design 
Decisions 
and 
Location
Rationale and 
Assumptions
Unsafe state 
avoidance
Do you employ substitution—
that is, safer, often hardware-
based protection mechanisms 
for potentially dangerous 
software design features?
Do you use a predictive model 
to predict the state of health of 
system processes, resources, 
or other properties—based on 
monitored information—not 
only to ensure that the system 
is operating within its nominal 
operating parameters, but also 
to provide early warning of a 
potential problem?
Unsafe state 
detection
Do you use timeouts to 
determine whether the operation 
of a component meets its timing 
constraints?
Do you use time stamps to 
detect incorrect sequences of 
events?
Do you employ condition 
monitoring to check 
conditions in a process or 
device, particularly to validate 
assumptions made during 
design?
continues

292  Appendix  Tactics-Based Questionnaires
Tactics  
Group
Tactics Question
Supported? 
(Y/N)
Risk Design 
Decisions 
and 
Location
Rationale and 
Assumptions
Is sanity checking employed 
to check the validity or 
reasonableness of specific 
operation results, or of inputs or 
outputs of a component? 
Does the system employ 
comparison to detect unsafe 
states, by comparing the 
outputs produced by a number 
of synchronized or replicated 
elements?
Containment 
(redundancy)
Do you use replication—
clones of a component—to 
protect against random failures 
of hardware?
Do you use functional 
redundancy, to address 
the common-mode failures 
by implementing diversely 
designed components? 
Do you use analytic 
redundancy—functional 
“replicas” that include high 
assurance/high performance 
and low assurance/low 
performance alternatives—to 
be able to tolerate specification 
errors?
Containment 
(limit 
consequences)
Can the system abort an 
operation that is determined to 
be unsafe before it can cause 
damage? 
Does the system provide 
controlled degradation, 
where the most critical system 
functions are maintained in the 
presence of component failures, 
while dropping or degrading less 
critical functions?
Does the system mask a fault 
by comparing the results of 
several redundant components 
and employ a voting procedure 
in case one or more of the 
components differ? 

A.8  Security  293
Tactics  
Group
Tactics Question
Supported? 
(Y/N)
Risk Design 
Decisions 
and 
Location
Rationale and 
Assumptions
Containment 
(barrier)
Does the system support 
limiting access to critical 
resources (e.g., processors, 
memory, network connections) 
through a firewall?
Does the system control access 
to protected components and 
protect against failures arising 
from incorrect sequencing of 
events through interlocks?
Recovery
Is the system able to roll 
back—that is, revert to a 
previous known good state—
upon the detection of a failure?
Can the system repair a state 
determined to be erroneous, 
without failure, and then 
continue execution? 
Can the system reconfigure 
resources, in the event of 
failures, by remapping the 
logical architecture onto the 
resources left functioning?
A.8  Security
Tactics 
Group
Tactics Question
Supported? 
(Y/N)
Risk
Design 
Decisions 
and Location
Rationale and 
Assumptions
Detecting 
attacks
Does the system 
support the detection of 
intrusions? An example is 
comparing network traffic 
or service request patterns 
within a system to a set 
of signatures or known 
patterns of malicious 
behavior stored in a 
database.
Does the system support 
the detection of denial-
of-service attacks? An 
example is the comparison 
of the pattern or signature 
of network traffic coming 
into a system to historical 
profiles of known denial-of-
service attacks.
continues

294  Appendix  Tactics-Based Questionnaires
Tactics 
Group
Tactics Question
Supported? 
(Y/N)
Risk
Design 
Decisions 
and Location
Rationale and 
Assumptions
Does the system support 
the verification of 
message integrity? 
An example is the use 
of techniques such as 
checksums or hash values 
to verify the integrity of 
messages, resource files, 
deployment files, and 
configuration files.
Does the system support 
the detection of message 
delays? An example is 
checking the time that it 
takes to deliver a message.
Resisting 
attacks
Does the system support 
the identification of 
actors? An example is 
identifying the source of 
any external input to the 
system.
Does the system support 
the authentication of 
actors? An example is 
ensuring that an actor (a 
user or a remote computer) 
is actually who or what it 
purports to be.
Does the system support 
the authorization of 
actors? An example 
is ensuring that an 
authenticated actor has the 
rights to access and modify 
either data or services. 
Does the system support 
limiting access? An 
example is controlling what 
and who may access which 
parts of a system, such as 
processors, memory, and 
network connections.
Does the system support 
limiting exposure? An 
example is reducing the 
probability of a successful 
attack, or restricting 
the amount of potential 
damage, by concealing 
facts about a system 
(“security by obscurity”) or 
by dividing and distributing 
critical resources (“don’t 
put all your eggs in one 
basket”).

A.8  Security  295
Tactics 
Group
Tactics Question
Supported? 
(Y/N)
Risk
Design 
Decisions 
and Location
Rationale and 
Assumptions
Does the system support 
data encryption? An 
example is applying some 
form of encryption to data 
and to communication.
Does the system design 
consider the separation 
of entities? An example is 
the physical separation of 
different servers attached 
to different networks, the 
use of virtual machines, or 
an “air gap.”
Does the system support 
changing credential 
settings? An example is 
forcing the user to change 
settings assigned by 
default.
Does the system validate 
input in a consistent, 
system-wide way? An 
example is the use of a 
security framework or 
validation class to perform 
actions such as filtering, 
canonicalization, and 
escaping of external input.
Reacting to 
attacks
Does the system support 
revoking access? An 
example is limiting access 
to sensitive resources, 
even for normally legitimate 
users and uses, if an attack 
is suspected.
Does the system support 
restricting login in 
instances such as multiple 
failed login attempts?
Does the system support 
informing actors? An 
example is notifying 
operators, other personnel, 
or cooperating systems 
when an attack is 
suspected or detected. 
continues

296  Appendix  Tactics-Based Questionnaires
Tactics 
Group
Tactics Question
Supported? 
(Y/N)
Risk
Design 
Decisions 
and Location
Rationale and 
Assumptions
Recovering 
from attacks
Does the system support 
maintaining an audit trail? 
An example is keeping a 
record of user and system 
actions and their effects, 
to help trace the actions of, 
and to identify, an attacker.
Does the system 
guarantee the property 
of nonrepudiation, 
which ensures that the 
sender of a message 
cannot later deny having 
sent the message and 
that the recipient cannot 
deny having received the 
message?
Have you checked the 
“recover from faults” 
category of tactics from the 
Availability checklist?
A.9  Testability
Tactics 
Group
Tactics Question
Supported? 
(Y/N)
Risk
Design 
Decisions 
and 
Location
Rationale and 
Assumptions
Control and 
observe 
system state
Does the system or the 
system components provide 
specialized interfaces 
to facilitate testing and 
monitoring?
Does the system provide 
mechanisms that allow 
information that crosses an 
interface to be recorded so 
that it can be used later for 
testing purposes (record/
playback)?
Is the state of the system, 
subsystem, or modules 
stored in a single place to 
facilitate testing (localized 
state storage)?

A.10  Usability  297
Tactics 
Group
Tactics Question
Supported? 
(Y/N)
Risk
Design 
Decisions 
and 
Location
Rationale and 
Assumptions
Can you abstract data 
sources—for example, 
by abstracting interfaces? 
Abstracting the interfaces 
lets you substitute test data 
more easily.
Can the system be 
executed in isolation (a 
sandbox) to experiment 
or test it without worrying 
about having to undo 
the consequences of the 
experiment?
Are executable assertions 
used in the system code to 
indicate when and where a 
program is in a faulty state?
Limit 
complexity
Is the system designed in 
such a way that structural 
complexity is limited? 
Examples include avoiding 
cyclic dependencies, 
reducing dependencies, 
and using techniques such 
as dependency injection.
Does the system 
include few or no (i.e., 
limited) sources of 
nondeterminism? This 
helps to limit the behavioral 
complexity that comes with 
unconstrained parallelism, 
which in turn simplifies 
testing.
A.10  Usability
Tactics 
Group
Tactics Question
Supported? 
(Y/N)
Risk
Design 
Decisions 
and Location
Rationale and 
Assumptions
Supporting 
user initiative
Does the system support 
operation canceling?
Does the system support 
operation undoing?
continues

298  Appendix  Tactics-Based Questionnaires
Tactics 
Group
Tactics Question
Supported? 
(Y/N)
Risk
Design 
Decisions 
and Location
Rationale and 
Assumptions
Does the system allow 
operations to be paused 
and later resumed? 
Examples are pausing the 
download of a file in a web 
browser and allowing the 
user to retry an incomplete 
(and failed) download.
Does the system support 
operations to be applied 
to groups of objects 
(aggregation)? For 
example, does it allow 
you to see the cumulative 
size of a number of files 
that are selected in a file 
browser window?
Support 
system 
initiative
Does the system provide 
assistance to users  
based on the tasks that 
they are performing  
(by maintaining a task 
model)? Examples 
include:
■
■
Validation of input data
■
■
Drawing user attention 
to changes in the UI
■
■
Maintaining UI 
consistency
■
■
Adding toolbars and 
menus to help users 
find functionality 
provided by the UI
■
■
Using wizards or other 
techniques to guide 
users in performing key 
user scenarios
Does the system support 
adjustments to the UI with 
respect to the class of 
users (by maintaining a 
user model)? Examples 
include supporting UI 
customization (including 
localization) and 
supporting accessibility.

A.11  Further Reading  299
Tactics 
Group
Tactics Question
Supported? 
(Y/N)
Risk
Design 
Decisions 
and Location
Rationale and 
Assumptions
Does the system provide 
appropriate feedback to 
the user based on the 
system characteristics  
(by maintaining a system 
model)? Examples 
include:
■
■
Avoiding blocking the 
user while handling 
long-running requests
■
■
Providing feedback on 
action progress  
(i.e., progress bars)
■
■
Displaying user-friendly 
errors without exposing 
sensitive data by 
managing exceptions
■
■
Adjusting the UI with 
respect to screen size 
and resolution 
A.11  Further Reading
The tactics catalog on which the questionnaires presented here are based can be found in 
L. Bass, P. Clements, and R. Kazman, Software Architecture in Practice (4th ed.), Addison-
Wesley, 2021.
An analysis of quality attribute data from SEI ATAMs, showing which qualities are the 
most common in practice, can be found in I. Ozkaya, L. Bass, R. Sangwan, and R. Nord, 
“Making Practical Use of Quality Attribute Information”, IEEE Software, March/April 2008; 
and in a later study by S. Bellomo, I. Gorton, and R. Kazman, “Insights from 15 Years of 
ATAM Data: Towards Agile Architecture”, IEEE Software, 32(5), 38–45, September/October 
2015.

This page intentionally left blank 

301
Index
Numerics
2-tier deployment, 124
4-tier deployment, 125
A
abstraction, common, 48, 54
active redundancy, 44
AD&D Hotels. See HPS (Hotel Pricing System)
Adapt tactics, 55
adapter pattern, 55
ADD (Attribute-Driven Design), 7, 165, 274.  
See also digital twinplatform; HPS  
(Hotel Pricing System); performance
allocate responsibilities, 64
analyze the current design, 65, 163–165
API-centric design, 109
applying to different system contexts, 66
and architect roles, 266
associating responsibilities, 76
brownfield, 69
choose design concepts that satisfy the 
selected drivers, 63
API-centric design, 108
digital twin platform, 197–198, 216–217
HPS (Hotel Pricing System), 159–160
choose elements of the system to refine, 63
API-centric design, 108
HPS (Hotel Pricing System), 159
cloud-based solutions, 147–148
decisions, recording, 65
and deployability, 128–129
design concepts
alternatives, 71
identifying, 70–71
selection, 71–72, 73–74
design to replace a legacy application, 69–70
elements, instantiation, 75–76
establish iteration goal by selecting  
drivers, 62
API-centric design, 108
digital twin platform, 197
HPS (Hotel Pricing System), 159
establishing relationships between  
elements, 76–77
greenfield
mature domain, 66–68
novel domain, 68–69
instantiate architectural elements, 63–64, 
160–161
API-centric design, 108
digital twin platform, 198–199
interface
defining, 64, 77, 108
external, 77
identification, 80–81
internal, 78, 80
iteration, 65
Kanban board, 86–87
properties, identifying, 76
recording design decisions, 84–85
review inputs, 60–62
API-centric design, 108
digital twin platform, 196
HPS (Hotel Pricing System), 158–159
review the design goals and purpose, 65
roadmap, 68
sketch views and record design decisions, 64
API-centric design, 109
digital twin platform, 220–222
HPS (Hotel Pricing System), 162–163, 
168–172
structures, producing, 74–75
and technical debt, 233–235
tracking design progress, 85
version 2.0, 7
version 2.5, 7
version 3.0, 7
Agile/agility, 5, 16, 91
in architecture, 258
business, 91
drivers. See drivers, business agility
focus on products, 92
design approaches, 257–259
Manifesto, 16

302  Index
agreements, 16–17
AI, 208
architect, 265
digital twin platform
constraints, 213–214
design concepts, 216–217
use case/s, 212
generative, 220
algorithm, selection, 48–49
allocation structures, 75
alternatives, design concept, 71
Amazon QuickSight, 218
Amazon Timestream, 218
analysis, 239
ATAM (architecture tradeoff analysis 
method), 247–249
back-of-the-envelope, 241
cost/benefit, 72–73
current design, 65
definition, 6
and design, 239–241
lightweight ATAM, 247–249
reasons for, 242
reflective questions, 246–247
scenario-based design reviews, 247
tactics-based, 244–246
techniques, 243
analytical skills, 6
analytic/s
cloud capabilities, 142
models, 241
anchoring bias, 31–32
Apache Kafka, 161, 166
API/s
asynchronous, 102–103
business, 94
-centric design, 93–94, 109
Back-End for Front-End pattern, 103
choose design concepts that satisfy the 
selected drivers, 108
choose one or more elements of the system 
to refine, 108
design of an implementation  
mechanism, 109
establish iteration goal by selecting 
drivers, 108
implementing APIs, 105
instantiate architectural element, 108
interface, defining, 108
layering, 103–104
review inputs, 108
sketch views and record design  
decisions, 109
and composability, 94–95
experience, 104
-first design, 95
documentation, 99
error management, 96
granularity, 96
methods, 99–100
operations, 99–100
parameters, 100
principles, 96
purpose of API, 95
return values, 100
security, 97
specifying an API, 97–99
supporting API evolution, 96
gateway, 107, 161
Gateway pattern, 104
management, 105–106
analytics, 107
API testing, 107
developer portal, 106–107
policies, 107
platform, 94, 109–110
process, 104
prototype, 109
query-oriented, 102
REST (Representational State Transfer),  
101, 167
RPC (Remote Procedure Call), 101–102
security, 161
system, 104
versioning, 96
WebSocket, 103
application
cloud-native, 138
framework, 38
legacy, ADD (Attribute-Driven Design), 
69–70
platform, 38
Rich Internet, 160
service, 33
web, 140
architect, 265–266

Index  303
AI/ML, 265
blockchain, 265
data, 265
infrastructure, 264–265
role of, 6, 266
security, 265
software, 264
architecture/architectural, 2–3, 11–12.  
See also element/s; microservice/s
agility, 258–259
agreements, 16–17
backlog, 85–86
concerns, 24–25
HPS, development and operations 
requirements, 157
HPS, system requirements, 155
constraints, 25–26
decisions, 13–14, 15, 16
deployability, 117
design, 3–4, 13
element interaction, 14
element internals, 14–15
importance of, 16–17
iterations, 60
prototype, 17, 18
rounds, 60
documentation, 4
drivers, 12, 17, 21, 61–62
architectural concerns, 24–25
constraints, 25–26
design purpose, 17–18
primary functionality, 23–24
quality attributes, 18–20
evaluation, 4, 249
groups, 268
implementation, 4
intentional, 259
Lambda, 216
microservice, 121–122
monolithic, 122
patterns, 35
performance, 40–41
throttling, 41
plan, 21
refactoring, 69
reference, 13, 25, 33–35, 75
cloud-based, 35, 145
digital twin platform, 196–202
requirements, 3–4
structures, 63
style, 35
tension between emergence and planning, 
259–260
ARID (active reviews for intermediate design), 81
artifact, 19, 141
ASR (architecturally significant requirements), 
3–4
asset hierarchy service, 219
assumptions
explicit, 30
implicit, 30
ATAM (architecture tradeoff analysis method), 
247–250
attack/s
man-in-the-middle, 51
reacting to, 52
recovery, 52
resisting, 51–52
authentication, 51
authorization, 51
automation, continuous deployment (CD), 
113–114
availability, 41, 49, 179
cloud, 136
high, 44
patterns, 44
Circuit Breaker, 46
cold spare, 45
hot spare, 44
tri-modular redundancy, 45–46
warm spare, 45
tactics, 42
-based questions, 244–246, 280–283
cloud-based, 143–144
Detect Faults, 43
Prevent Faults, 44
Recover from Faults, 43–44
B
A/B testing, 127–128
backlog
architectural, 85–86
project, 231
back-of-the-envelope analysis, 241
BDUF (big design up front), 80, 257–258
behavior, component, 55

304  Index
best-case scenario, 72
BFF (back-end for front-end) pattern, 103
bias, anchoring, 31–32
Big Data, 208
blockchain architect, 265
blue/green pattern, 126
blueprint, 4
Booch, Grady, 13
Books, Fred, 274
brainstorming, scenario, 21
bridge pattern, 55–56
brownfield, ADD (Attribute-Driven Design), 69
brownfield development, 66
bug, 231
business
agility, 91
drivers. See drivers, business agility
focus on products, 92
APIs, 94
goals, 18
C
C&C (component and connector) structures, 75
canary testing, 127
CASE (computer-aided software engineering), 82
CBAM (cost/benefit analysis method), 72–73
CD (continuous deployment), 113–114,  
116–117, 118
CDN (content delivery network), 140
checklists, 241
choreography, 55
CI (continuous integration), 116
CIA (confidentiality, integrity, and  
availability), 49
Circuit Breaker pattern, 46
Clements, Paul, “A Rational Design Process: 
How and Why to Fake It”, 1–2
Client-Server pattern, 49
“clone and own”, 228
cloud/cloud computing, 134
ADD (Attribute-Driven Design), 147–148
analytics capabilities, 142
computing capabilities, 139
constraints
cloud native and vendor lock-in, 138–139
cost optimization, 137
legal, 139
database capabilities, 141
development and DevOps  
capabilities, 141
elements, digital twin platform, 201
FaaS (function as a service), 135
hybrid, 134
IaaS (infrastructure as a service), 134
managed resources, 135–136
networking capabilities, 140
PaaS (platform as a service), 135
patterns
design, 145–147
reference architecture, 145
private, 134
provider managed services, 167–168
public, 134
quality attributes
availability, 136
deployability, 137
operability, 137
performance and scalability, 136
security, 137
reference architecture, 35
regional capabilities, 139
SaaS (software as a service), 135
security capabilities, 142
service models, 134–135
storage capabilities, 141
tactics
availability, 143–144
deployability, 144
operability, 144–145
performance and scalability, 143
security, 144–145
code
“clone and own”, 228
repository, 141
cold spare, 45
communication, 6, 54
complexity, 2
component/s
behavior, 55
externally developed, 37–38, 76
interface, 56
composability, 92–93, 94–95
computational overhead, 40
computing capabilities, cloud, 139

Index  305
concepts
cloud capability
analytics, 142
computing, 140
database, 141
development and DevOps, 141
networking, 140
regional, 139
security, 142
storage, 141
design, 12, 33
choosing, 63, 71–72, 73–74
digital twin platform, 197–198
externally developed components, 37–38
HPS (Hotel Pricing System), 166, 185
IIoT (Industrial IoT), 204–207
instantiation, 75
patterns, 35
reference architecture, 33–35
tactics, 35–37
concerns
cross-cutting, 33
general, 24
improper separation of, 228
specific, 25
concurrency, 40
condition monitoring, 43, 143
cone of uncertainty, 254–255
confidentiality, 49, 51
conformance checking, 4
constraints, 32
architectural, 25–26, 74
cloud computing
cloud native and vendor lock-in, 138–139
cost optimization, 137
legal, 139
development and operations requirement, 
HPS (Hotel Pricing System), 157
digital twin platform
Big Data & AI/ML, 213–214
IIoT, 204
system requirement
digital twin platform, 195
HPS (Hotel Pricing System), 155
container, 140, 161
contexts, 30–31, 70–71
continuous deployment, 113–114
Control Resource Demand tactics, 40
Conway’s law, 263
Coordinate tactics, 55
cost optimization, cloud computing, 137
CQRS (Command and Query Responsibility 
Segregation), 146, 160
credentials, 52
cross-cutting concerns, 33
D
data architect, 265
data lake, 142
data sources, digital twin platform, 210–211
data warehouse, 142
database
cloud-based, 141
nonrelational, 167
relational, 166
decisions/decision making, 29
“adequacy” approach, 240
architectural, 15, 16
“buy versus build” choice, 37
choosing externally developed  
components, 38
deployability, 119
functionality, 24
instantiation, 160–161, 180
digital twin platform, 198–199, 208
HPS (Hotel Pricing System), 185
principles
anticipate risks, 31
assign priorities, 31
check assumptions, 30
define time horizon, 31
design around constraints, 32
explore contexts, 30–31
generate multiple solution options, 31–32
use facts, 30
weigh the pros and cons, 32
recording, 65, 84–85
tactics, 35–37
decomposition, 159, 166
Defer Binding tactics, 48
dependency/ies, 48
inversion principle, 229
limiting, 54
microservice, 122

306  Index
package, 120
tangled, 228
deployability, 93
and ADD, 128–129
cloud, 137
defining, 113–116
design decisions, 119
designing the system to support, 117–118
DevOps, 116
general scenario, 115–116
patterns, 121
A/B testing, 127–128
blue/green, 126
canary testing, 127
load-balanced cluster, 125–126
microservice architecture, 121–122
monoliths and modular monoliths, 
122–123
rolling upgrade, 126–127
tactics, 119–120
-based questions, 283–284
Manage Deployment Pipeline, 120
deployment
continuous, 113–114
infrastructure, 118
N-tier, 123–125
patterns, 35, 75–76
pipeline, 113–114, 118
rollback, 117
design, 1–2, 11, 239. See also ADD 
(Attribute-Driven Design); concept/s, 
design;performance
Agile approaches, 257–259
and analysis, 239–241. See also analysis
API-centric, 93–94, 109
asynchronous APIs, 102–103
choose design concepts that satisfy the 
selected drivers, 108
greenfield API platform, 109–110
implementing APIs, 105
interface, defining, 108
layering, 103–104
queries, 102
REST (Representational State Transfer), 101
RPC APIs, 101–102
WebSockets, 103
API-first, 95
documentation, 99
error management, 96
granularity, 96
methods, 99–100
operations, 99–100
parameters, 100
purpose of API, 95
return values, 100
security, 97
specifying an API, 97–99
supporting API evolution, 96
architectural, 3–4, 13, 15, 16–17
Attribute-Driven. See ADD  
(Attribute-Driven Design)
Books on, 274
complex, 2
concepts, 12, 33
catalog, 267–268
choosing, 63, 71–72, 73–74
cloud-based, 139–143
digital twin platform, 197–198
HPS (Hotel Pricing System), 166, 185
identifying, 70–71
IIoT (Industrial IoT), 204–207
instantiation, 75
constraints, 25–26, 32, 74
contexts, 30–31
debt, 228–231
decision making, 29
“adequacy” approach, 240
anticipate risks, 31
check assumptions, 30
constraints, 32
define time horizon, 31
explore contexts, 30–31
generate multiple solution options, 31–32
use facts, 30
weigh the pros and cons, 32
detailed, 15
difficulty, 12
element interaction, 14
element internals, 14–15
emergent, 259
goals, 62
Herbert on, 274
high-level, 15
house, 11–12

Index  307
integrability, 53–55
iteration, 60, 62, 64
modifiability, 46. See also modifiability
novel, 12, 18
patterns, 35, 36, 145–147
performing, 59–60
principles, 33
priority, 31
progress
digital twin platform, 201–202, 211–212, 
222–223
HPS (Hotel Pricing System), 187–188
tracking, 85
purpose, 17–18, 60, 65
reference architecture, 33–35
role of the architect, 6
round, 60, 62
skill, 7, 12
software architecture, 12–13
tactics, 35–37, 120
team approach, 263–264
tradeoffs, 13
detailed design, 15
Detect Attacks tactics, 50–51
Detect Faults tactics, 43
developer portal, 106–107
development and operations requirements. See 
also software development
HPS (Hotel Pricing System), 156, 184–188
architectural concerns, 157
quality attribute scenarios, 156–157
DevOps, 116
CD (continuous deployment), 116–117, 118
cloud capabilities, 141
iteration 0 + spikes approach, 258–259
pipeline, 141
production environment, 116
diagram, 64
package, 186
sequence, 78–79, 173–174
difficulty, integration, 53
digital twin platform
analysis of current design, 222–223
Big Data & AI/ML
asset hierarchy service, 219
constraints, 213–214
design concepts, 216–217
edge and cloud technologies, 217
elements to refine, 215
generative AI, 220
historical data repository, 219
instantiation decisions, 217–220
use case/s, 212–213
business case, 192
constraints, 195
data sources, 210–211
design concepts, 197–198
design progress, 201–202, 211–212, 222–223
IIoT (Industrial IoT)
constraints, 204
design concepts, 204–207
drivers, 202–203
elements to refine, 204
instantiating architectural elements, 
207–208
inputs, 196
instantiation decisions, 198–199
quality attribute scenarios, 194–195, 213–214
reference architecture and overall system 
structure, 196–202
cloud elements, 201
drivers, 197
edge elements, 200
system requirements, 193
use cases, 193–194
dimensions, scenario, 20
discovery
dynamic, 49
prototyping, 20
service, 55
DNS (domain name server), 140
Docker containers, 161
documentation
API, 99
architectural, 4
purpose, 83
scenario-based, 83–84
views, 81–83
domain model, HPS (Hotel Pricing System),  
166, 168–169
drivers
architectural, 12, 17, 21, 61–62
design purpose, 17–18
digital twin platform, 197

308  Index
primary functionality, 23–24
quality attributes, 18–20
business agility, 92
composability, 92–93
deployability, 93
HPS (Hotel Pricing System), 159, 165
iteration, 65
dynamic discovery, 49
E
edge elements, digital twin platform, 200
element/s
HPS (Hotel Pricing System),  
175–176, 183
instantiation, 63–64, 75–76, 108, 160–161
interaction design, 14
interface
external, 77
identification, 80–81
internal, 78, 80
internals design, 14–15
refactoring, 69
refinement, 159
relationships between, 64, 76–77
responsibilities, 82
allocating, 64
associating, 76
emergence, 3, 257–258, 259–260
encapsulation, 47, 54
encryption, 51
energy efficiency, tactics-based questions, 
284–286
enterprise architect, 266
environments
execution, 140
HPS (Hotel Pricing System), 156
integration, 114
production, 114, 116
staging, 114
escalating restart, 44
estimation process, 18, 80, 254
cone of uncertainty, 254–255
standard components technique, 256–257
evaluation, 4, 242, 249
event/s
priority, 40
response, 40
evolution
API, 96
software, 232
unplanned, 229
exception
detection, 43
handling, 43
prevention, 44
execution environments, 140
execution time, 40
experience
API, 104
leveraging, 71
experiments, 241, 259
explicit assumptions, 30
export-side microservice modules, HPS  
(Hotel Pricing System), 172
external interface, 77
externally developed components, 37–38, 76
F
FaaS (function as a service), 135
facts, 30
failure
retry, 43
system, 41
falsifiable hypotheses, 20
faults, 41
features, 16, 120
firewall, 142
framework, 38, 66–67
front-end, HPS (Hotel Pricing System), 160
full-stack framework, 38, 66–67
functional requirement, 19
functionality
decisions, 24
primary, 23–24, 153–154
G
gateway
API, 161
network, 140
general concerns, 24
generative AI, 220
goals, 13
business, 18
design, 62, 66–68

Index  309
iteration, 159
reviewing, 65
short-term, 230
graceful degradation, 43
granularity
disintegration, 121
integration, 121–122
greenfield design/development, 63, 66
API platform, 109–110
mature domain, 66–68
novel domain, 68–69
H
heartbeat, 43
Hibernate, 38
high availability, 44
high-level design, 15
historical data repository, digital twin  
platform, 219
hot spare, 44
HPS (Hotel Pricing System)
addressing reliability and availability quality 
attributes, 179
analyze the current design, 183–184
changes in the infrastructure to support 
quality attributes, 181–183
communication failure with the channel 
management system, 181–183
design concepts, 180–183
drivers, 179
refinement, 179
business case, 151–153
deployment elements, 183
design concepts, 166, 185
design progress, 187–188
development and operations requirements, 
156, 184–188
architectural concerns, 157
constraints, 157
quality attribute scenarios, 156–157
domain model, 166
environments, 156
establishing an overall system structure
analyze the current design, 163–165
choose design concepts that satisfy the 
selected drivers, 159–160
choose elements of the system to refine, 159
drivers, 159
front-end, 160
instantiate architectural element, 160–161
sketch views and record design decisions, 
162–163
identify structures to support primary 
functionality, 165
analysis of current design, 177–179
choose elements of the system to  
refine, 165
drivers, 165
instantiate architectural element, 166–168
sketch views and record design decisions, 
168–172
inputs, 158–159
instantiation decisions, 180, 185
methods, 175–176
modules
command-side, 170
query-side, 171
quality attribute scenarios, 154–155
software design process, 158
system requirements
architectural concerns, 155
constraints, 155
primary functionality, 153–154
quality attribute scenarios, 154–155
hybrid cloud, 134
I
IaaS (infrastructure as a service), 134
IDL (interface design language), 101–102
IIoT (Industrial IoT), digital twin platform
constraints, 204
design concepts, 204–207
drivers, 202–203
elements to refine, 204
instantiating architectural elements, 207–208
implementation
API, 109
architectural, 4
implicit assumptions, 30
improper separation of concerns, 228
Increase Cohesion tactics, 47
infrastructure
architect, 264–265
as code services, 141

310  Index
deployment, 118
as a service, 134
inputs, digital twin platform, 196
instantiation
decisions
digital twin platform, 198–199, 208, 217–220
HPS (Hotel Pricing System), 180, 185
design concept, 75
element, 63–64, 75–76, 108, 160–161, 166–168
integrability/integration, 53
difficulty, 53
environment, 114
granularity, 121–122
patterns
adapter, 55
bridge, 55–56
dynamic discovery, 49
mediator, 56
services, 56
tactics, 53–54
Adapt, 55
Coordinate, 55
Limit Dependencies, 54
questionnaire, 287–288
integrity, 49, 51
intentional architecture, 259
Intercepting Validator pattern, 52–53
interface, 56
defining, 64, 77, 108
design language, 101–102
external, 77
identification, 80–81
internal, 78, 80
mismatch, 126–127
operations, 80
segregation principle, 229
tailoring, 55
intermediary, 47, 54
internal interface, 78, 80
internal requirements, 25
interview, tactics-based, 244–246
Intrusion Detection pattern, 53
issues, 25
iteration, 62, 65
0 + spikes approach, 258–259
goals, 62, 159
micro-, 64
J-K
JAD (Joint Application Design), 20
JRP (Joint Requirements Planning), 20
Kanban board, 86–87
Kinesis Data Analytics, 218
L
Lambda architecture, 216
layered pattern, 48
layers, 48, 64, 104
leadership, 6
learning from example, 1–2
legacy application, ADD (Attribute-Driven 
Design), 69–70
legal constraints, cloud computing, 139
Lehman’s Laws of Software Evolution, 232
leveraging, experience, 71
life-cycle activities, software  
architecture, 3–5
lightweight ATAM (architecture tradeoff 
analysis method), 249–250
Liskov substitution principle, 229
Litmus Edge reference architecture, 206–207, 208
LLM (large language model), 275
load balanced cluster pattern, 125–126
load balancer pattern, 41
M
Manage Deployed System tactics, 120
Manage Deployment Pipeline tactics, 120
Manage Resources tactics, 40
managed relational database, 141
man-in-the-middle attack, 51
mature domain, greenfield system  
design, 66–68
mediator pattern, 56
in-memory cache, 141
message
integrity, 51
queue, 216, 217
methodology
ADD (Attribute-Driven Design).  
See ADD (Attribute-Driven Design)
Agile, 5, 16
cost/benefit analysis, 72–73
principled, 59

Index  311
methods
API, 99–100
HPS (Hotel Pricing System), 175–176
need for, 273–275
micro-iteration, 64
microservice/s, 123, 160, 161
architecture, 121–122
container, 161
dependencies, 122
element internals design, 15
Mission Thread Workshop, 18
modifiability, 46
patterns, 48
Client-Server, 49
layered, 48
strategy, 48–49
tactics, 46–47
Defer Binding, 48
Increase Cohesion, 47
questionnaire, 288–289
Reduce Coupling, 47–48
modular monolith, 122–123
module/s
HPS (Hotel Pricing System)
command-side, 170
export-side, 172
query-side, 171
interface, identification, 80–81
structures, 75
view. sketching, 82
monitoring
condition, 43, 143
services, 141
monolithic architecture, 122
MVP (Minimum Viable Product), 92
N
negotiation, 6
networking capabilities, cloud, 140
non-full-stack framework, 38
nonprimary use case, 80
nonrelational database, 167
nonstop forwarding, 44
NoSQL database, 141
novel design, 12, 18
N-tier deployment, 123–125
O
OpenAPI specification, 97–99, 167
open-closed principle, 229
operability, cloud, 137, 144–145
operations, 80, 99–100
orchestration, 55
organizations/organizational
architecture groups, 268
architecture guidelines, 267
design concept catalog, 267–268
goals, 18
P
PaaS (platform as a service), 135
package dependencies, 120
parameters, API, 100
Parnas, David, “A Rational Design Process: How 
and Why to Fake It”, 1–2
passive redundancy, 45
pattern/s, 36, 75–76
API Gateway, 104
API Layering, 104
availability, 44
Circuit Breaker, 46
cold spare, 45
hot spare, 44
tri-modular redundancy, 45–46
warm spare, 45
Back-End for Front-End, 103
cloud/cloud computing
design, 145–147
reference architecture, 145
CQRS (Command and Query Responsibility 
Segregation), 160
deployability/deployment, 35, 121
A/B testing, 127–128
blue/green, 126
canary testing, 127
load-balanced cluster, 125–126
microservice architecture, 121–122
monoliths and modular monoliths, 
122–123
N-tier deployment, 123–125
rolling upgrade, 126–127
design/architectural, 35

312  Index
integrability
adapter, 55
bridge, 55–56
dynamic discovery, 49
mediator, 56
services, 56
modifiability, 48
Client-Server, 49
layered, 48
strategy, 48–49
performance, 40–41
load balancer, 37–41
throttling, 41
security
Intercepting Validator, 52–53
Intrusion Detection, 53
Strangler Fig, 69–70
performance, 39. See also availability
patterns, 40–41
load balancer, 41
throttling, 41
quality attribute, 136
tactics, 39
-based questions, 289–291
cloud-based, 143
Control Resource Demand, 40
Manage Resources, 40
performing design, 59–60
ping/echo, 43
planning, tension with emergence, 259–260
platform, 38. See also digital twin platform
API, 94
greenfield API, 109–110
as a service, 135
policies, 25, 107
portal, developer, 106–107
predictive model, 44
Prevent Faults tactics, 44
primary functionality, 23–24, 153–154
primary use case, 14
principles
Agile, 257–258
API-first design, 96
decision making
anticipate risks, 31
assign priorities, 31
check assumptions, 30
define time horizon, 31
design around constraints, 32
explore contexts, 30–31
generate multiple solution options, 31–32
use facts, 30
weigh the pros and cons, 32
design, 32–33
SOLID, 229
priority/ies
assigning, 31
scenario, 20, 22, 23
private cloud, 134
process API, 104
producing, structures, 74–75
production environment, 114, 116
products, 37–38, 92
project/s
backlog, 231
versus products, 92
proposal, 16
properties, identifying, 76
pros and cons, weighing, 32
prototype/prototyping, 17, 18, 241
API, 109
discovery, 20
throwaway, 73–74
public cloud, 134
purpose
API, 95
design, 17–18, 60, 65
of documentation, 83
Q
QAW (Quality Attribute Workshop), 18, 21–22
quality attribute/s, 18–20
availability, 136, 179
composability, 92–93
deployability, 93, 113–116
design principles, 33
digital twin platform, 194–195
falsifiable hypotheses, 20
HPS (Hotel Pricing System), 154–155
operability, 137
performance, 136
reliability, 179
scalability, 136, 179
scenarios, 19–20, 24, 25
digital twin platform, 213–214
HPS (Hotel Pricing System), 154–155

Index  313
security, 137
query-oriented APIs, 102
query-side modules, HPS  
(Hotel Pricing System), 171
questions
reflective, 241, 246–247
tactics-based, 244–246
availability, 280–283
deployability, 283–284
energy efficiency, 284–286
integrability, 287–288
modifiability, 288–289
performance, 289–291
safety, 291–293
security, 293–296
testability, 296–297
usability, 297–299
R
reconfiguration, 43
recording design decisions, 84–85
Recover from Faults tactics, 43–44
recovery, 41, 52
Reduce Coupling tactics, 47–48
redundancy
active, 44
passive, 45
tri-modular, 45–46
redundant spare, 43, 44
refactoring, 69, 232–233, 260
reference architecture, 13, 25, 33–35, 75
cloud-based, 35, 145
digital twin platform, 196–202
Litmus Edge, 206–207, 208
refinement, element, 63, 68, 159
reflective questions, 241, 246–247
regional capabilities, cloud, 139
relational database, 141, 166
reliability, 179
requirement/s. See also development and 
operations requirements; system 
requirements
architecturally significant, 3–4
development and operations, HPS (Hotel 
Pricing System), 156–157, 184–188
elicitation techniques, 20
functional, 19
internal, 25
scenario, 20
system
digital twin platform, 193
HPS (Hotel Pricing System), 153–155
Resist Attacks tactics, 51–52
resource/s
managed, 135–136
manager, 55
scheduling, 40
responsibilities
associating, 76
command-side microservices module, 170
element, 76, 80, 82
improper separation of concerns, 228
redistribution, 47
REST (Representational State Transfer), 101
REST API, 167
restart, escalating, 44
retry, 43
return values, API, 100
reusability, 92–93
Rich Internet Application, 160
risk/s
anticipating, 31
managing, 242
ROI (return on investment), 72
roles, architect, 6
rollback, 43, 117, 120, 144
rolling upgrade, 126–127
round, design, 60, 62
RPC APIs, 101–102
RUP (Rational Unified Process), 260–261
S
SaaS (software as a service), 135
safety, tactics questionnaire, 291–293
sanity checking, 43
satisfice, 26
scalability, 39, 136, 179
scaled rollout, 120
scenario, 19
-based design review, 247
-based documentation, 83–84
best-case, 72
brainstorming, 21
consolidation, 21
dimensions, 20
priority, 20, 22, 23

314  Index
quality attribute, 19–20, 24, 25
digital twin platform, 213–214
HPS (Hotel Pricing System), 154–155, 
156–157
requirements, 20
worst-case, 72
scheduling, resources, 40
script deployment commands, 120
search and query services, 142
security, 49–50
API, 97, 161
architect, 265
cloud, 137, 142
patterns
Intercepting Validator, 52–53
Intrusion Detection, 53
tactics, 50
-based questions, 293–296
cloud-based, 144–145
Detect Attacks, 50–51
React to Attacks, 52
Recover from Attacks, 52
Resist Attacks, 51–52
self-service, 94, 106–107
self-test, 43
sequence diagram, 78–79
sequential development, 260–261
serverless execution environment, 140
service/s
application, 33
asset hierarchy, 219
cloud provider-managed, 167–168
discovery, 55
-level agreement (SLA), 56
mesh, 140
micro-, 160, 161. See also module/s
architecture, 121–122
versus monoliths, 117–123
monitoring, 141
pattern, 56
search and query, 142
visualization, 218
shadow mode, 43
Simon, Herbert, 274
single-responsibility principle, 229
sketching, views, 64, 81–83, 234
digital twin platform, 220–222
HPS (Hotel Pricing System), 162–163, 168–172
skill/s
architect, 6
design, 7, 12
smart camera, 208
“Smart Decisions” game, 276
software
bug, 231
evolution, 232
upgrade, 43
software architecture, 2, 264
design, 12–13
importance of, 2–3
layers, 48
life-cycle activities, 3–5
LLM (large language model), 275
refactoring, 232–233
Software Architecture in Practice, 2, 18
software development
Agile, 91, 257–259
deployability, 113–116
environments, 114
outcomes, 230
sequential phase approach, 260–261
testing strategy, 262–263
velocity, 230
water-Scrum-fall, 261
SOLID principles, 229
solution/s, 31–32
specific concerns, 25
specification, API, 97–99
split module, 47
Spring Framework, 167
staging environment, 114
standard components technique,  
256–257
standardization, 54
state resynchronization, 44
stimulus/response, 19–20
storage capabilities, cloud, 141
Strangler Fig pattern, 69–70
Strategy pattern, 48–49
structures
allocation, 75
C&C, 75
module, 75
producing, 74–75

Index  315
style, architectural, 35
Swagger, 167
system
API, 104
availability, 41
composability, 92–93
contexts, 70–71
element interaction design, 14
failure, 41
greenfield
mature domain, 66–68
novel domain, 68–69
scenario, 19
separating entities, 51
structures, producing, 74–75
system requirements
digital twin platform, 193
HPS (Hotel Pricing System)
architectural concerns, 155
constraints, 155
primary functionality, 153–154
quality attribute scenarios, 154–155
T
table, design decision, 85
tactics, 76. See also pattern/s
availability, 42
cloud-based, 143–144
Detect Faults, 43
Prevent Faults, 44
questionnaire, 280–283
Recover from Faults, 43–44
-based analysis, 244–246
deployability, 119–120
cloud-based, 144
Manage Deployment Pipeline, 120
questionnaire, 283–284
design, 35–37
energy efficiency, questionnaire, 284–286
integrability, 53–54
Adapt, 55
Coordinate, 55
Limit Dependencies, 54
questionnaire, 287–288
modifiability, 46–47
Defer Binding, 48
Increase Cohesion, 47
questionnaire, 288–289
Reduce Coupling, 47–48
operability, cloud-based, 144–145
performance, 39
cloud-based, 143
Control Resource Demand, 40
questionnaire, 289–291
safety, questionnaire, 291–293
security, 50
cloud-based, 144–145
Detect Attacks, 50–51
questionnaire, 293–296
React to Attacks, 52
Recover from Attacks, 52
Resist Attacks, 51–52
testability, questionnaire, 296–297
usability, 36–37, 297–299
tailoring an interface, 55
tangled dependencies, 228
team approach to design, 263–264
technical debt, 16, 227–228
ADD and, 233–235
in design, 228–231
technical skills, 6
temporal inconsistency, 126
test/ing
API, 107
A/B, 127–128
canary, 127
self-, 43
strategy, 262–263
tactics questionnaire, 296–297
thought experiment, 241
throttling pattern, 41
throwaway prototype, 73–74
time horizon, 31
timestamp, 43
tools
analysis and debugging, 141
API design and development, 106
API management, 105–106
CASE (computer-aided software 
engineering), 82
deployment pipeline, 118
Kanban board, 86–87
tracking, design progress, 85
tradeoffs, design, 13
transactions, 44
tri-modular redundancy, 45–46

316  Index
U
unplanned evolution, 229
up-front work, 257–258
upgrade, rolling, 126–127
usability
questionnaire, 297–299
tactics, 36–37
use case/s
digital twin platform, 192, 193–194
HPS (Hotel Pricing System), 192
nonprimary, 80
primary, 14
user stories, 20, 167
utility, 72–73
Utility Tree, 18, 22–23
V
velocity, software, 230
vendor lock-in, 139
versioning, API, 96
views, 81
sketching, 64, 81–83, 168–172, 234
digital twin platform, 220–222
HPS (Hotel Pricing System), 162–163
virtual machine, 140
visualization service, 218
voting, 43
VPN (virtual private network), 140
W-X-Y-Z
warm spare, 45
waterfall model, 257–258
water-Scrum-fall development, 261
web application execution environment, 140
WebSockets, 103
work
requests, 40
up-front, 257–258
workshop
Mission Thread, 18
Quality Attribute, 18
worst-case scenario, 72

1
Online Appendix of Designing Software Architectures: A Practical Approach, 
Second Edition by Humberto Cervantes & Rick Kazman / ISBN: 9780138108021 / 
Copyright 2024 / All Rights Reserved
From Ch. 4 of Designing Software  
Architectures, First Edition
Case Study: FCAPS System
We now present a case study of using ADD 3.0 for a greenfield system in a mature domain. 
This case study details an initial design round composed of three iterations and is based on a 
real-world example. We first present the business context, and then we summarize the require-
ments for the system. This is followed by a step-by-step summary of the activities that are 
performed during the ADD iterations.
4.1  Business Case
In 2006, a large telecommunications company wanted to expand its Internet Protocol (IP) 
network to support “carrier-class services”, and more specifically high-quality voice over IP 
(VOIP) systems. One important aspect to achieve this goal was synchronization of the VOIP 
servers and other equipment. Poor synchronization results in low quality of service (QoS), de-
graded performance, and unhappy customers. To achieve the required level of synchronization, 
the company wanted to deploy a network of time servers that support the Network Time Pro-
tocol (NTP). Time servers are formed into groups that typically correspond to geographical re-
gions. Within these regions, time servers are organized hierarchically in levels or strata, where 
time servers placed in the upper level of the hierarchy (stratum 1) are equipped with hardware 
(e.g., Cesium Oscillator, GPS signal) that provides precise time. Time servers that are lower in 
the hierarchy use NTP to request time from servers in the upper levels or from their peers.

2 
Case Study: FCAPS System
Many pieces of equipment depend on the time provided by time servers in the network, so 
one priority for the company was to correct any problems that occur on the time servers. Such 
problems may require dispatching a technician to perform physical maintenance on the time 
servers, such as rebooting. Another priority for the company was to collect data from the time 
servers to monitor the performance of the synchronization framework.
In the initial deployment plans, the company wanted to field 100 time servers of a par-
ticular model. Besides NTP, time servers support the Simple Network Management Protocol 
(SNMP), which provides three basic operations:
§ set() operations: change configuration variables (e.g., connected peers)
§ get() operations: retrieve configuration variables or performance data
§ trap() operations: notifications of exceptional events such as the loss or restoration of 
the GPS signal or changes in the time reference
To achieve the company’s goals, a management system for the time servers needed to be 
developed. This system needed to conform to the FCAPS model, which is a standard model for 
network management. The letters in the acronym stand for:
§ Fault management. The goal of fault management is to recognize, isolate, correct, and log 
faults that occur in the network. In this case, these faults correspond to traps generated by 
time servers or other problems such as loss of communication between the management 
system and the time servers.
§ Configuration management. This includes gathering and storing configurations from net-
work devices, thereby simplifying the configuration of devices and tracking changes that 
are made to device configurations. In this system, besides changing individual configura-
tion variables, it is necessary to be able to deploy a specific configuration to several time 
servers.
§ Accounting. The goal here is to gather device information. In this context, this includes 
tracking device hardware and firmware versions, hardware equipment, and other compo-
nents of the system.
§ Performance management. This category focuses on determining the efficiency of the 
current network. By collecting and analyzing performance data, the network health can 
be monitored. In this case, delay, offset, and jitter measures are collected from the time 
servers.
§ Security management. This is the process of controlling access to assets in the network. 
In this case, there are two important types of users: technicians and administrators. 
Technicians can visualize trap information and configurations but cannot make changes; 
administrators are technicians who can visualize the same information but can also make 
changes to configurations, including adding and removing time servers from the network.
Once the initial network was deployed, the company planned to extend it by adding time 
servers from newer models that might potentially support management protocols other than 
SNMP.
The remainder of this chapter describes a design for this system, created using ADD 3.0.

4.2 System Requirements 3
4.2 System Requirements
Requirement elicitation activities had been previously performed, and the following is a sum-
mary of the most relevant requirements collected.
4.2.1 Use Case Model
The use case model in Figure 4.1 presents the most relevant use cases that support the FCAPS 
model in the system. Other use cases are not shown.
FIGURE 4.1 Use case model for the FCAPS system

4 
Case Study: FCAPS System
Each of these use cases is described in the following table:
Use Case
Description
UC-1: Monitor 
network status 
 
A user monitors the time servers in a hierarchical representation of the whole 
network. Problematic devices are highlighted, along with the logical regions 
where they are grouped. The user can expand and collapse the network 
representation. This representation is updated continuously as faults are 
detected or repaired.
UC-2: Detect 
fault
Periodically the management system contacts the time servers to see if they 
are “alive”. If a time server does not respond, or if a trap that signals a problem 
or a return to a normal state of operation is received, the event is stored and the 
network representation observed by the users is updated accordingly.
UC-3: Display 
event history
Stored events associated with a particular time server or group of time servers 
are displayed. These can be filtered by various criteria such as type or severity.
UC-4: Manage 
time servers
The administrator adds a time server to, or removes a time server from, the 
network. 
UC-5: Configure 
time server
An administrator changes configuration parameters associated with a particular 
time server. The parameters are sent to the device and are also stored locally.
UC-6: Restore 
configuration
A locally stored configuration is sent to one or more time servers.
UC-7: Collect 
performance 
data
Network performance data (delay, offset, and jitter) is collected periodically from 
the time servers.
UC-8: Display 
information
The user displays stored information about the time server—configuration 
values and other parameters such as the server name.
UC-9: Visualize 
performance 
data
The user displays network performance measures (delay, offset, jitter) in a 
graphical way to view and analyze network performance.
UC-10: Log in
A user logs into the system through a login/password screen. Upon successful 
login, the user is presented with different options according to their role.
U-11: Manage 
users
The administrator adds or removes a user or modifies user permissions.
4.2.2  Quality Attribute Scenarios
In addition to these use cases, a number of quality attribute scenarios were elicited and docu-
mented. The six most relevant ones are presented in the following table. For each scenario, we 
also identify the use case that it is associated with. 

4.2  System Requirements  5
ID
Quality 
Attribute
Scenario
Associated 
Use Case
QA-1
Performance
Several time servers send traps to the management 
system at peak load; 100% of the traps are successfully 
processed and stored.
UC-2
QA-2
Modifiability
A new time server management protocol is introduced  
to the system as part of an update. The protocol is added 
successfully without any changes to the core components 
of the system.
UC-5
QA-3
Availability
A failure occurs in the management system during  
normal operation. The management system resumes 
operation in less than 30 seconds.
All
QA-4
Performance
The management system collects performance data  
from a time server during peak load. The management 
system collects all performance data within 5 minutes, 
while processing all user requests, to ensure no loss of 
data due to CON-5.
UC-7
QA-5
Performance, 
usability
A user displays the event history of a particular time 
server during normal operation. The list of events from  
the last 24 hours is displayed within 1 second.
UC-3
QA-6
Security
A user performs a change in the system during normal 
operation. It is possible to know who performed the 
operation and when it was performed 100% of the time.
All
4.2.3  Constraints
Finally, a set of constraints on the system and its implementation were collected. These are 
presented in the following table.
ID
Constraint
CON-1
A minimum of 50 simultaneous users must be supported.
CON-2
The system must be accessed through a web browser (Chrome V3.0+, Firefox V4+, IE8+) 
in different platforms: Windows, OSX, and Linux.
CON-3
An existing relational database server must be used. This server cannot be used for other 
purposes than hosting the database.
CON-4
The network connection to user workstations can have low bandwidth but is generally 
reliable.
CON-5
Performance data needs to be collected in intervals of no more than 5 minutes, as higher 
intervals result in time servers discarding data.
CON-6
Events from the last 30 days must be stored.

6 
Case Study: FCAPS System
4.2.4  Architectural Concerns
Given that this is greenfield development, only a few general architectural concerns are identi-
fied initially, as shown in the following table.
ID
Concern
CRN-1
Establishing an overall initial system structure.
CRN-2
Leverage the team’s knowledge about Java technologies, including Spring, JSF, Swing, 
Hibernate, Java Web Start and JMS frameworks, and the Java language.
CRN-3
Allocate work to members of the development team.
Given these sets of inputs, we are now ready to proceed to describe the design process, as 
described in Section 3.2. In this chapter, we present only the final results of the requirements 
collection process. The job of collecting these requirements is nontrivial, but is beyond the 
scope of this chapter.
4.3  The Design Process
We now ready to make the leap from the world of requirements and business concerns to the 
world of design. This is perhaps the most important job for an architect—translating require-
ments into design decisions. Of course, many other decisions and duties are important, but 
this is the core of what it means to be an architect: making design decisions with far-reaching 
consequences.
4.3.1  ADD Step 1: Review Inputs
The first step of the ADD method involves reviewing the inputs and identifying which require-
ments will be considered as drivers (i.e., which will be included in the design backlog). The 
inputs are summarized in the following table.
Category
Details
Design purpose
This is a greenfield system from a mature domain. The purpose is to produce a 
sufficiently detailed design to support the construction of the system.
Primary functional 
requirements
From the use cases presented in Section 4.2.1, the primary ones were 
determined to be:
UC-1: Because it directly supports the core business
UC-2: Because it directly supports the core business
UC-7: Because of the technical issues associated with it  
(see QA-4)

4.3  The Design Process  7
Quality attribute 
scenarios
The scenarios were described in Section 4.2.2. They have now been 
prioritized (as discussed in Section 2.4.2) as follows:
Scenario 
ID
Importance to 
the Customer
Difficulty of Implementation According to 
the Architect
QA-1
High
High
QA-2
High
Medium
QA-3
High
High
QA-4
High
High
QA-5
Medium
Medium
QA-6
Medium
Low
From this list, only QA-1, QA-2, QA-3, and QA-4 are selected as drivers.
Constraints
All of the constraints discussed in Section 4.2.3 are included as drivers.
Architectural 
concerns
All of the architectural concerns discussed in Section 4.2.4 are included as 
drivers.
4.3.2  Iteration 1: Establishing an Overall System Structure
This section presents the results of the activities that are performed in each of the steps of ADD 
in the first iteration of the design process.
4.3.2.1  Step 2: Establish Iteration Goal by Selecting Drivers
This is the first iteration in the design of a greenfield system, so the iteration goal is to achieve 
the architectural concern CNR-1 of establishing an overall system structure (see Section 3.3.1).
Although this iteration is driven by a general architectural concern, the architect must 
keep in mind all of the drivers that may influence the general structure of the system. In partic-
ular, the architect must be mindful of the following:
§ QA-1: Performance
§ QA-2: Modifiability
§ QA-3: Availability
§ QA-4: Performance
§ CON-2: System must be accessed through a web browser in different platforms—
Windows, OSX, and Linux
§ CON-3: A relational database server must be used
§ CON-4: Network connection to users workstations can have low bandwidth and be 
unreliable
§ CRN-2: Leverage team’s knowledge about Java technologies

8 
Case Study: FCAPS System
Time server
Legend:
FIGURE 4.2 Context diagram for the FCAPS system 
4.3.2.2 Step 3: Choose One or More Elements of the System to Refi ne
This is a greenfi eld development eff ort, so in this case the element to refi ne is the entire 
FCAPS system, which is shown in Figure 4.2. In this case, refi nement is performed through 
decomposition.
4.3.2.3 Step 4: Choose One or More Design Concepts That Satisfy the Selected Drivers
In this initial iteration, given the goal of structuring the entire system, design concepts are 
selected according to the roadmap presented in Section 3.3.1. The following table summarizes 
the selection of design decisions. Note that all of the design concepts used in this case study are 
also described in Appendix A.
Design Decisions 
and Location
Rationale
Logically structure 
the client part of the 
system using the Rich 
Client Application 
reference architecture
The Rich Client Application (RCA) reference architecture (see Section 
A.1.2) supports the development of applications that are installed in the 
users’ PC. These applications support rich user interface capabilities that 
are needed for displaying the network topology and performance graphs 
(UC-1). These capabilities are also helpful in achieving QA-5, even if this 
design decision is not a driver. Although these types of applications do not 
run in a web browser (CON-2), they can be installed from a web browser 
using a technology such as Java Web Start.

4.3  The Design Process  9
Design Decisions 
and Location
Rationale
Discarded alternatives:
Alternative
Reason for Discarding
Rich Internet 
applications 
(RIA)
This reference architecture (see Section A.1.3) is oriented 
toward the development of applications with a rich user 
interface that runs inside a web browser.
Although this type of application supports a rich user inter-
face and can be upgraded easily, this option was discard-
ed because it was believed that plugins for executing RIA 
were less broadly available than the Java Virtual Machine.
Web 
applications
This reference architecture (see Section A.1.1) is oriented 
toward the development of applications that are accessed 
from a web browser. Although this reference architecture 
facilitates deployment and updating, it was discarded 
because it is difficult to provide a rich user interface 
experience.
Mobile 
applications
This reference architecture (see Section A.1.4) is oriented 
toward the development of applications that are deployed 
in handheld devices. This alternative was discarded 
because this type of device was not considered for 
accessing the system.
Logically structure 
the server part of 
the system using the 
Service Application 
reference architecture
Service applications (see Section A.1.5) do not provide a user interface but 
rather expose services that are consumed by other applications.
No other alternatives were considered and discarded, as the architect was 
familiar with this reference architecture and considered it fully adequate to 
meet the requirements.
Physically structure 
the application 
using the three-tier 
deployment pattern
Since the system must be accessed from a web browser (CON-2) and 
an existing database server must also be used (CON-3), a three-tier 
deployment is appropriate (see  
Section A.2.2).
At this point, it is clear that some type of replication will be needed on both 
the web/app tier and the database tier to support QA-3, but this will be 
addressed later (in iteration 3).
Discarded alternatives include other n-tier patterns with n != 3. The two-tier 
alternative is discarded because an existing legacy database server needs 
to be incorporated into the system and this cannot be used for any other 
purpose, according to CON-3. All n > 3 alternatives are discarded because 
at this point no other servers are necessary for the solution.
(continues)

10 
Case Study: FCAPS System
Design Decisions 
and Location
Rationale
Build the user 
interface of the client 
application using 
the Swing Java 
framework and other 
Java technologies
The standard framework for building Java Rich Clients ensures portability 
(CON-2) and it is what the developers were already familiar with (CRN-3).
Discarded alternatives: The Eclipse SWT (Standard Widget Toolkit) 
framework was considered, but the developers were not as familiar with it. 
Deploy the application 
using the Java Web 
Start technology
Access to the application is obtained via a web browser, which launches 
the installer (CON-2).
This technology also facilitates updating because client code is reloaded 
only when a new version is available. As updates are not expected to occur 
frequently, this is beneficial for low-bandwidth situations (CON-4).
The alternative would be the use of applets, but they need to be reloaded 
every time the web page is loaded, which increases the bandwidth 
requirements.
4.3.2.4  Step 5: Instantiate Architectural Elements, Allocate Responsibilities, and Define 
Interfaces
The instantiation design decisions considered and made are summarized in the following table:
Design Decision and 
Location
Rationale
Remove local data sources 
in the rich client application
It is believed that there is no need to store data locally, as the network 
connection is generally reliable.
Also, communication with the server is handled in the data layer. 
Internal communication between components in the client is 
managed through local method calls and does not need particular 
support.
Create a module dedicated 
to accessing the time 
servers in the data layer 
of the Service Application 
reference architecture
The service agents component from the reference architecture is 
adapted to abstract the access to the time servers. This will further 
facilitate the achievement of QA-2 and will play a critical role in the 
achievement of UC-2 and UC-7.
The results of these instantiation decisions are recorded in the next step. In this initial 
iteration, it is typically too early to precisely define functionality and interfaces. In the next 
iteration, which is dedicated to defining functionality in more detail, interfaces will begin to be 
defined.
4.3.2.5  Step 6: Sketch Views and Record Design Decisions
The diagram in Figure 4.3 shows the sketch of a module view of the two reference architec-
tures that were selected for the client and server applications. These have now been adapted 
according to the design decisions we have made.

4.3  The Design Process  11
«Layer»
Presentation CS
«Layer»
Business logic CS
«Layer»
Data CS
«Swing»
UI Modules
UI Process Modules
Client Side
Server Side
«Layer»
Cross-cutting CS
Business Modules CS
Business Entities CS
«Module»
Communication Modules
Security Module CS
Op. Mgmt. Module CS
«Layer»
Services SS
Service Interfaces
«Layer»
Business Logic SS
Business Modules SS
Business Entities SS
«Layer»
Data SS
DB Access Module
Time Server Access Module
«Layer»
Cross-cutting SS
Security Module SS
Op. Mgmt. Module SS
Communication Module SS
FIGURE 4.3  Modules obtained from the selected reference architectures (Key: UML)

12 
Case Study: FCAPS System
This sketch was created using a CASE tool. In the tool, each element is selected and a short 
description of its responsibilities is captured. Note that the descriptions at this point are quite crude, 
just indicating major functional responsibilities, with no details. The following table summarizes 
the information that is captured:
Element
Responsibility
Presentation client side (CS)
This layer contains modules that control user interaction and use 
case control flow.
Business logic CS
This layer contains modules that perform business logic 
operations that can be executed locally on the client side.
Data CS
This layer contains modules that are responsible for 
communication with the server.
Cross-cutting CS
This “layer” includes modules with functionality that goes across 
different layers, such as security, logging, and I/O. This is helpful 
in achieving QA-6, even if it is not one of the drivers.
UI modules
These modules render the user interface and receive user inputs.
UI process modules
These modules are responsible for control flow of all the system 
use cases (including navigation between screens).
Business modules CS
These modules either implement business operations that can be 
performed locally or expose business functionality from the server 
side.
Business entities CS
These entities make up the domain model. They may be less 
detailed than those on the server side.
Communication modules CS
These modules consume the services provided by the application 
running on the server side.
Services server side (SS)
This layer contains modules that expose services that are 
consumed by the clients.
Business Logic SS
This layer contains modules that perform business logic 
operations that require processing on the server side.
Data SS
This layer contains modules that are responsible for data 
persistence and for communication with the time servers.
Cross-cutting SS
These modules have functionality that goes across different 
layers, such as security, logging, and I/O.
Service interfaces SS
These modules expose services that are consumed by the clients.
Business modules SS
These modules implement business operations.
Business entities SS
These entities make up the domain model. 
DB access module
This module is responsible for persistence of business entities 
(objects) into the relational database. It performs object-oriented 
to relational mapping and shields the rest of the application from 
persistence details.

4.3  The Design Process  13
Element
Responsibility
Time server access module
This module is responsible for communication with the time 
servers. It isolates and abstracts operations with the time servers 
to support communication with different types of time servers 
(see QA-2).
The deployment diagram in Figure 4.4 sketches an allocation view that illustrates where 
the components associated with the modules in the previous diagram will be deployed.
The responsibilities of the elements are summarized here:
Element
Responsibility
User workstation
The user’s PC, which hosts the client side logic of the application
Application server
The server that hosts server side logic of the application and also serves web 
pages
Database server
The server that hosts the legacy relational database
Time server
The set of (external) time servers
Also, information about relationships between some elements in the diagram that is worth 
recording is summarized in the following table:
Relationship
Description
Between web/app server and database server
Communication with the database will be done 
using the JDBC protocol.
Between web/app server and time server
The SNMP protocol is used (at least initially).
pc :User Workstation
«replicated»
:Application Server
«replicated»
database :Database Server
:Time Server
Server-Side Application
«Java Web Start»
Client-Side 
Application
«SNMP»
«JDBC»
FIGURE 4.4  Initial deployment diagram for the FCAPS system (Key: UML)

14 
Case Study: FCAPS System
4.3.2.6  Step 7: Perform Analysis of Current Design and Review Iteration Goal and 
Achievement of Design Purpose
The following table summarizes the design progress using the Kanban board technique dis-
cussed in Section 3.8.2.
Not 
Addressed
Partially 
Addressed 
Completely
Addressed
Design Decisions Made During the Iteration
UC-1
Selected reference architecture establishes the 
modules that will support this functionality.
UC-2
Selected reference architecture establishes the 
modules that will support this functionality.
UC-7
Selected reference architecture establishes the 
modules that will support this functionality.
QA-1
No relevant decisions made, as it is necessary to 
identify the elements that participate in the use 
case that is associated with the scenario.
QA-2
Introduction of a time server access module in the 
data layer on the server application that encapsu-
lates communication with the time servers. The 
details of this component and its interfaces have 
not been defined yet.
QA-3
Identification of the elements derived from the 
deployment pattern that will need to be replicated.
QA-4
No relevant decisions made, as it is necessary to 
identify the elements that participate in the use 
case that is associated with the scenario.
CON-1
Structuring the system using 3 tiers will allow 
multiple clients to connect to the application server. 
Decisions regarding concurrent access have not 
been made yet.
CON-2
Use of Java Web Start technology allows access 
through a web browser to download the Rich 
Client. Since the Rich Client is being programmed 
in Java, this supports execution under Windows, 
OSX, and Linux.
CON-3
Physically structure the application using the 3-tier 
deployment pattern, and isolate the database by 
providing database access components in the data 
layer of the application server.

4.3  The Design Process  15
Not 
Addressed
Partially 
Addressed 
Completely
Addressed
Design Decisions Made During the Iteration
CON-4
Use of Java Web Start technology requires 
the client to be downloaded only the first time, 
and then when upgrades occur. This is helpful 
to support limited-bandwidth connections. 
More decisions need to be made regarding the 
communication between the presentation and the 
business logic layers.
CON-5
No relevant decisions made.
CON-6
No relevant decisions made.
CRN-1 
Selection of reference architectures and 
deployment pattern.
CRN-2
Technologies that have been considered up to 
this point take into account the knowledge of 
the developers. Other technologies still need to 
be selected (e.g., communication with the time 
servers).
CRN-3
No relevant decisions made.
4.3.3  Iteration 2: Identifying Structures to Support Primary 
Functionality
This section presents the results of the activities that are performed in each of the steps of 
ADD in the second iteration of the design process for the FCAPS system. In this iteration, we 
move from the generic and coarse-grained descriptions of functionality used in iteration 1 to 
more detailed decisions that will drive implementation and hence the formation of develop-
ment teams.
This movement from the generic to the specific is intentional, and built into the ADD 
method. We cannot design everything up front, so we need to be disciplined about which de-
cisions we make, and when, to ensure that the design is done in a systematic way, addressing 
the biggest risks first and moving from there to ever finer details. Our goal for the first iteration 
was to establish an overall system structure. Now that this goal has been met, our new goal for 
this second iteration is to reason about the units of implementation, which affect team forma-
tion, interfaces, and the means by which development tasks may be distributed, outsourced, 
and implemented in sprints.
4.3.3.1  Step 2: Establish Iteration Goal by Selecting Drivers
The goal of this iteration is to address the general architectural concern of identifying structures 
to support primary functionality. Identifying these elements is useful not only for understanding 

16 
Case Study: FCAPS System
how functionality is supported, but also for addressing CRN-3—that is, the allocation of work to 
members of the development team.
In this second iteration, besides CRN-3, the architect considers the system’s primary use 
cases:
§ UC-1
§ UC-2
§ UC-7
4.3.3.2  Step 3: Choose One or More Elements of the System to Refine
The elements that will be refined in this iteration are the modules located in the different 
layers defined by the two reference architectures from the previous iteration. In general, the 
support of functionality in this system requires the collaboration of components associated 
with modules that are located in the different layers.
4.3.3.3  Step 4: Choose One or More Design Concepts That Satisfy the Selected Drivers
In this iteration, several design concepts—in this case, architectural design patterns—are se-
lected from the book Pattern Oriented Software Architecture, Volume 4. The following table 
summarizes the design decisions. The words in bold in the following table refer to architec-
tural patterns from this book, and can be found in Appendix A.
Design Decisions 
and Location
Rationale and Assumptions
Create a Domain 
Model for the 
application
Before starting a functional decomposition, it is necessary to create an 
initial domain model for the system, identifying the major entities in the 
domain, along with their relationships.
There are no good alternatives. A domain model must eventually be 
created, or it will emerge in a suboptimal fashion, leading to an ad hoc 
architecture that is hard to understand and maintain.
Identify Domain 
Objects that 
map to functional 
requirements
Each distinct functional element of the application needs to be 
encapsulated in a self-contained building block—a domain object.
One possible alternative is to not consider domain objects and instead 
directly decompose layers into modules, but this increases the risk of not 
considering a requirement.
Decompose Domain 
Objects into general 
and specialized 
Components 
Domain objects represent complete sets of functionality, but this 
functionality is supported by finer-grained elements located within the 
layers. The “components” in this pattern are what we have referred to as 
modules.
Specialization of modules is associated with the layers where they are 
located (e.g., UI modules).
There are no good alternatives to decomposing the layers into modules to 
support functionality.

4.3  The Design Process  17
Design Decisions 
and Location
Rationale and Assumptions
Use Spring 
framework and 
Hibernate
Spring is a widely used framework to support enterprise application 
development. Hibernate is an object to relational mapping (ORM) 
framework that integrates well with Spring.
An alternative that was considered for application development is JEE. 
Spring was eventually selected because it was considered more “light-
weight” and the development team was already familiar with it, resulting in 
greater and earlier productivity.
Other ORM frameworks were not considered, as the development team 
already was familiar with, and happy with the performance of, Hibernate.
4.3.3.4  Step 5: Instantiate Architectural Elements, Allocate Responsibilities, and Define 
Interfaces
The instantiation design decisions made in this iteration are summarized in the following table:
Design Decisions 
and Location
Rationale
Create only an initial 
domain model
The entities that participate in the primary use cases need to be identified 
and modeled but only an initial domain model is created, to accelerate this 
phase of design.
Map the system use 
cases to domain 
objects
An initial identification of domain objects can be made by analyzing the 
system’s use cases. To address CRN-3, domain objects are identified for all 
of the use cases in Section 4.2.1.
Decompose the 
domain objects 
across the layers to 
identify layer-specific 
modules with an 
explicit interface
This technique ensures that modules that support all of the functionalities 
are identified.
The architect will perform this task just for the primary use cases. This allows 
another team member to identify the rest of the modules, thereby allocating 
work among team members.
Having established the set of modules, the architect realizes the need to 
test these modules, so a new architectural concern is identified here:
CRN-4: A majority of modules shall be unit tested.
Only “a majority of modules” are covered by this concern because the 
modules that implement user interface functionality are difficult to test 
independently.
Connect components 
associated with 
modules using Spring
This framework uses an inversion of control approach that allows different 
aspects to be supported and the modules to be unit-tested (CRN-4).
Associate 
frameworks with a 
module in the data 
layer
ORM mapping is encapsulated in the modules that are contained in the 
data layer. The Hibernate framework previously selected is associated with 
these modules.

18 
Case Study: FCAPS System
While the structures and interfaces are identified in this step of the method, they are cap-
tured in the next step.
4.3.3.5  Step 6: Sketch Views and Record Design Decisions
As a result of the decisions made in step 5, several diagrams are created. 
§ Figure 4.5 shows an initial domain model for the system.
§ Figure 4.6 shows the domain objects that are instantiated for the use case model in  
Section 4.2.1.
§ Figure 4.7 shows a sketch of a module view with modules that are derived from the busi-
ness objects and associated with the primary use cases. Note that explicit interfaces are 
not shown but their existence is assumed.
The responsibilities for the elements identified in Figure 4.7 are summarized in the table 
that begins on page 95.
Time Server
- 
deviceName
- 
ipAddress
- 
model
Event
- 
date
- 
payload
- 
severity
- 
type
Region
- 
name
Configuration
- 
configurationParameters
Performance Data
- 
delay:  DataSet
- 
jitter:  DataSet
- 
offset:  DataSet
User
- 
login
- 
password
- 
permissions
- 
type
0..*
-parent
0..*
1
1
generates
0..*
1..*
1
acknowledges
FIGURE 4.5  Initial domain model (Key: UML)

4.3  The Design Process  19
«domain object»
Network Status Monitoring
responsibilities
UC-1
«domain object»
Fault Detection
responsibilities
UC-2
«domain object»
Event history
responsibilities
UC-3
«domain object»
Time Server Management
responsibilities
UC-4
«domain object»
Time Server Configuration
responsibilities
UC-5
UC-6
«domain object»
System Access
responsibilities
UC-10
«domain object»
Performance Data and Information Display
responsibilities
UC-8
UC-9
«domain object»
Performance and Data Collection
responsibilities
UC-7
«domain object»
User Management
responsibilities
UC-11
FIGURE 4.6  Domain objects associated with the use case model (Key: UML)

20 
Case Study: FCAPS System
Server Side
Client Side
«Layer»
Presentation CS
«Layer»
Business logic CS
«Layer»
Data CS
«Layer»
Services SS
«Layer»
Business Logic SS
«Layer»
Data SS
NetworkStatusMonitoringView
NetworkStatusMonitoringController
RequestManager
«facade»
RequestService
TopologyController
DomainEntities
RegionDataMapper
TimeServerDataMapper
EventDataMapper
TimeServerConnector
TimeServerEventsController
DataCollectionController
FIGURE 4.7  Modules that support the primary use cases (Key: UML)

4.3  The Design Process  21
Element
Responsibility
NetworkStatusMonitoringView
Displays the network representation and updates it 
when events are received. This component embodies 
both UI components and UI process components from 
the reference architecture.
NetworkStatusMonitoringController
Responsible for providing the necessary information 
to the presentation layer for displaying the network 
representation.
RequestManager
Responsible for communication with the server-side 
logic.
RequestService
Provides a facade that receives requests from the 
clients.
TopologyController
Contains business logic related to the topological 
information.
DomainEntities
Contains the entities from the domain model (server 
side).
TimeServerEventsController
Contains business logic related to the management of 
events.
DataCollectionController
Contains logic to perform data collection and storage.
RegionDataMapper
Responsible for persistence operations (CRUD) 
related to the regions.
TimeServerDataMapper
Responsible for persistence operations (CRUD) 
related to the time servers.
EventDataMapper
Responsible for persistence operations (CRUD) 
related to the events.
TimeServerConnector
Responsible for communication with the time servers. 
It isolates and abstracts operations with the time 
servers to support communication with different types 
of time servers (see QA-2).

22 
Case Study: FCAPS System
The following sequence diagrams for UC-1 and UC-2 were created in the previous step of 
the method to define interfaces (as discussed in Section 3.6). A similar diagram was also created 
for UC-7 but is not shown here due to space limitations.
UC-1: Monitor Network Status
Figure 4.8 shows an initial sequence diagram for UC-1 (monitor network status). It shows how the 
user representation of the topology is displayed on startup (after the user has successfully logged 
into the system). Upon launch, the topology is requested from the TopologyController on the 
server. This element retrieves the root region through the RegionDataMapper and returns it to 
the client. The client can then populate the view by traversing the relationships within the Region 
class.
Server
Client
Technician
:NetworkStatusMonitoringView
:NetworkStatusMonitoringController
:RequestManager
:RequestService
:TopologyController
:RegionDataMapper
launch()
initialize()
requestTopology()
sendRequest(Request)
requestTopology()
retrieve(id) :Region
:Region
:Region
:Response
:Region
:boolean
getRootRegion() :Region
populateView()
interact()
FIGURE 4.8  Sequence diagram for use case UC-1 (Key: UML)
From the interactions identified in the sequence diagram, initial methods for the interfaces 
of the interacting elements can be identified:

4.3  The Design Process  23
Method Name
Description
Element: NetworkStatusMonitoringContoller
boolean initialize()
Opens up the network representation so that users can interact 
with it.
Region getRootRegion()
Returns a reference to the root region and the neighbors of this 
object (excluding traps).
Element: RequestManager
Region requestTopology()
Requests the topology. This method returns a reference to the 
root region from which it is possible to navigate through the 
complete topology.
Element: RequestService
Response 
sendRequest(Request req)
This method receives a request. Only this method is exposed in 
the service interface. This simplifies the addition of other func-
tionality in the future without having to modify the existing service 
interface.
Element: TopologyController
Region requestTopology()
Requests the topology. This method returns a reference to the 
root region from which it is possible to navigate through the 
complete topology.
Element: RegionDataMapper
Region retrieve(int id)
Returns a Region from its id.

24 
Case Study: FCAPS System
Time Server
:TimeServerConnector
:TimeServerConfigurationController
:TimeServerDataMapper
:Time Server
:TopologyController
addEventListener(this)
trap()
eventReceived(event)
publish(event)
retrieve(id) :TimeServer
:TimeServer
addEvent()
update(TimeServer)
:true
FIGURE 4.9  Sequence diagram for use case UC-2 (Key: UML)
UC-2: Detect Fault
Figure 4.9 shows an initial sequence diagram for UC-2 (detect fault) shows only the compo-
nents on the server side. The interaction starts with a TimeServer sending a trap, which is 
received by the TimeServerConnector. The trap is transformed into an Event and sent 
to the TimeServerConfigurationController. The Event is sent asynchronously to 
the TopologyController for publication to the clients and is then persisted.
From this interaction, initial methods for the interfaces of the interacting elements can be 
identified:

4.3  The Design Process  25
Method Name
Description
Element: TimeServerConnector
boolean addEventListener 
(EventListener el)
This method allows components from the business logic to 
register themselves as listeners to events that are received 
from the time servers.
Element: TimeServerConfigurationController
boolean eventReceived(Event 
evt)
This callback method is invoked when an event is received.
Element: TopologyController
publish(Event evt)
This method notifies the clients that a new event has oc-
curred.
Element: TimeServerDataMapper
TimeServer retrieve(int id)
Retrieves a TimeServer identified by its id.
boolean update(TimeServer 
ts)
Persists changes in a TimeServer.
4.3.3.6  Step 7: Perform Analysis of Current Design and Review Iteration Goal and 
Achievement of Design Purpose
The decisions made in this iteration provided an initial understanding of how functionality is 
supported in the system. The modules associated with the primary use cases were identified by 
the architect, and the modules associated with the rest of the functionality were identified by 
another team member. From the complete list of modules, a work assignment table was created 
(not shown here) to address CRN-3.
Also, as part of module identification, a new architectural concern was identified and 
added to the Kanban board. Drivers that were completely addressed in the previous iteration 
are removed from the table.

26 
Case Study: FCAPS System
Not 
Addressed
Partially 
Addressed 
Completely
Addressed
Design Decisions Made During the Iteration
UC-1
Modules across the layers and preliminary interfaces to 
support this use case have been identified.
UC-2
Modules across the layers and preliminary interfaces to 
support this use case have been identified.
UC-7
Modules across the layers and preliminary interfaces to 
support this use case have been identified.
QA-1
The elements that support the associated use case 
(UC-2) have been identified.
QA-2
The elements that support the associated use case 
(UC-5) have been identified.
QA-3
No relevant decisions made.
QA-4
The elements that support the associated use case 
(UC-7) have been identified.
CON-1
No relevant decisions made.
CON-4
No relevant decisions made.
CON-5
Modules responsible for collecting data have been 
identified.
CON-6
Modules responsible for collecting data storage been 
identified.
CRN-2
Additional technologies were identified and selected 
considering the team’s knowledge.
CRN-3
Modules associated with all of the use cases have 
been identified and a work assignment matrix has been 
created (not shown).
CRN-4
The architectural concern of unit-testing modules, 
which was introduced in this new iteration, is partially 
solved through the use of an inversion of control 
approach to connect the components associated with 
the modules. 

4.3  The Design Process  27
4.3.4  Iteration 3: Addressing Quality Attribute Scenario Driver 
(QA-3)
This section presents the results of the activities that are performed in each of the steps of ADD 
in the third iteration of the design process. Building on the fundamental structural decisions 
made in iterations 1 and 2, we can now start to reason about the fulfillment of some of the 
more important quality attributes. This iteration focuses on just one of these quality attribute 
scenarios.
4.3.4.1  Step 2: Establish Iteration Goal by Selecting Drivers
For this iteration, the architect focuses on the QA-3 quality attribute scenario: A failure occurs 
in the management system during operation. The management system resumes operation in 
less than 30 seconds.
4.3.4.2  Step 3: Choose One or More Elements of the System to Refine
For this availability scenario, the elements that will be refined are the physical nodes that were 
identified during the first iteration:
§ Application server
§ Database server
4.3.4.3  Step 4: Choose One or More Design Concepts That Satisfy the Selected Drivers
The design concepts used in this iteration are the following:
Design Decisions and Location
Rationale and Assumptions
Introduce the active redundancy 
tactic by replicating the application 
server and other critical compo-
nents such as the database
By replicating the critical elements, the system can withstand 
the failure of one of the replicated elements without affecting 
functionality.
Introduce an element from the 
message queue technology 
family
Traps received from the time servers are placed in the 
message queue and then retrieved by the application. Use of a 
queue will guarantee that traps are processed and delivered in 
order (QA-1). 
4.3.4.4  Step 5: Instantiate Architectural Elements, Allocate Responsibilities, and Define 
Interfaces
The instantiation design decisions are summarized in the following table:

28 
Case Study: FCAPS System
Design Decisions 
and Location
Rationale
Deploy message 
queue on a separate 
node
Deploying the message queue on a separate node will guarantee that no 
traps are lost in case of application failure. This node is replicated using 
the tactic of active redundancy, but only one copy receives and treats 
events coming from the network devices.
Use active 
redundancy and 
load balancing in the 
application server
Because two replicas of the application server are active at any time, it 
makes sense to distribute and balance the load among the replicas. This 
tactic can be achieved through the use of the Load-Balanced Cluster 
pattern (see Section A.2.3).
This introduces a new architectural concern, CRN-5: Manage state in 
replicas.
Implement load 
balancing and 
redundancy using 
technology support
Many technological options for load balancing and redundancy can be 
implemented without having to develop an ad hoc solution that would be 
less mature and harder to support.
The results of these instantiation decisions are recorded in the next step.
4.3.4.5  Step 6: Sketch Views and Record Design Decisions
Figure 4.10 shows a refined deployment diagram that includes the introduction of redundancy 
in the system.
Server1 :ApplicationServer 
pc :UserWorkstation 
Server2 :ApplicationServer 
«replicated»
:Database Server
device1 :TimeServer
Relocatable IP address
«replicated»
:TrapReceiver
«replicated»
:LoadBalancer
«JDBC»
«HTTP»
«JDBC»
«SNMP»
FIGURE 4.10  Refined deployment diagram (Key: UML)

4.3  The Design Process  29
The following table describes responsibilities for elements that have not been listed previ-
ously (in iteration 1):
Element
Responsibility
LoadBalancer
Dispatches (and balances the load of) requests coming from clients to the 
application servers. The load balancer also presents a unique IP address to the 
clients.
TrapReceiver
Receives traps from network devices, converts them into events, and puts 
these events into a persistent message queue.
The UML sequence diagram shown in figure 4.11 illustrates how the TrapReceiver 
that was introduced in this iteration exchanges messages with other elements shown in the 
deployment diagram to support UC-2 (detect fault), which is associated with both QA-3 (avail-
ability) and QA-1 (performance). 
As the purpose of this diagram is to illustrate the communication that occurs between the 
physical nodes, the names of the methods are only preliminary; they will be refined in further 
iterations.
:ApplicationServer
pc :UserWorkstation
trap()
transformAndEnqueue(Event)
consume()
event()
publish(Event)
updateView()
:NetworkDevice
:TrapReceiver
FIGURE 4.11  Sequence diagram illustrating the messages exchanged between the physical 
nodes to support UC-2 (Key: UML)

30 
Case Study: FCAPS System
4.3.4.6  Step 7: Perform Analysis of Current Design and Review Iteration Goal and 
Achievement of Design Purpose
In this iteration, important design decisions have been made to address QA-3, which also im-
pacted QA-1. The following table summarizes the status of the different drivers and the decisions 
that were made during the iteration. Drivers that were completely addressed in the previous itera-
tion have been removed from the table.
Not 
Addressed
Partially 
Addressed 
Completely
Addressed
Design Decisions Made During the Iteration
QA-1
The introduction of a separate replicated trap receiver 
node can help ensure 100% of the traps are processed, 
even in the case of a failure of the application server. 
Furthermore, because trap reception is performed in 
a separate node, this approach reduces application 
server processing load, thereby helping performance.
Because specific technologies have not been chosen, 
this driver is marked as “partially addressed”.
QA-2
No relevant decisions made.
QA-3
By making the application server redundant, we reduce 
the probability of failure of the system. Furthermore, if 
the load balancer fails, a passive replica is activated 
within the required time period.
Because specific technologies have not been chosen 
(message queue), this driver is marked as “partially 
addressed”.
QA-4
No relevant decisions made.
CON-1
Replication of the application server and the use of 
a load balancer will help in supporting multiple user 
requests.
CON-4
No relevant decisions made.
CON-5
No relevant decisions made.
CON-6
No relevant decisions made.
CRN-2
No relevant decisions made.
CRN-4
No relevant decisions made.
CRN-5
This new architectural concern is introduced in this 
iteration: manage state in replicas. At this point, no 
relevant decisions have been made.

4.5  Further Reading  31
4.4  Summary
In this chapter, we presented an example of using ADD to design a greenfield system in a ma-
ture domain. We illustrated three iterations with different foci: addressing a general concern, 
addressing functionality, and addressing one key quality attribute scenario.
The example followed the roadmap discussed in Section 3.3.1. It is interesting to observe 
that in the first iteration, two different reference architectures were used to structure the sys-
tem. Also, the selection of externally developed components—in this case, frameworks—was 
carried out across the different iterations. Finally, the example illustrates how new architectural 
concerns appear as the design progresses.
This example demonstrates how architectural concerns, primary use cases, and quality 
attribute scenarios can be addressed as part of architectural design. In a real system, more 
iterations would be necessary to create a complete architecture design by addressing other sce-
narios with high priority.
In this example, we assumed that the architect is using a CASE tool during design, so di-
agrams were produced using UML. This is certainly not mandatory, as we will see in the case 
study presented in Chapter 5. Also, note that it is relatively simple to generate preliminary view 
sketches by using the information that is generated as part of the design process.
4.5  Further Reading
Appendix A provides descriptions and bibliographical references of all the design concepts 
used in this case study.


1
Online Appendix of Designing Software Architectures: A Practical Approach, 
Second Edition by Humberto Cervantes & Rick Kazman / ISBN: 9780138108021 / 
Copyright 2024 / All Rights Reserved
From Ch. 5 of Designing Software  
Architectures, First Edition
Case Study:  
Big Data System
With Serge Haziyev and Olha Hrytsay
We now present an extended design example of using ADD 3.0 in a greenfield system for a 
challenging domain—that of Big Data. As of the time of writing, this domain was still rela-
tively new and rapidly evolving. As such, the architects could not solely rely on past experi-
ence to guide them. They instead complemented the design process with periodic analyses and 
strategic prototyping, as we will now describe.
5.1  Business Case
This case study involves an Internet company that provides popular content and online ser-
vices to millions of web users. Besides providing information externally, the company col-
lects and analyzes massive logs of data that are generated from its infrastructure (e.g., 
application and server logs, system metrics). Such an approach of dealing with computer- 
generated  log messages  is also called log management (http://en.wikipedia.org/wiki/
Log_management_and_intelligence).
Because of very fast infrastructure growth, the company’s IT department realizes that the 
existing in-house systems can no longer process the required log data volume and velocity. 
Moreover, requests for a new system are coming from other company stakeholders, including 
product managers and data scientists, who would like to leverage the various kinds of data that 
can be collected from multiple data sources, not just logs.
The marketecture diagram (informal depiction of the system’s structure) shown in 
Figure 5.1 represents the desired solution from a functional perspective for three major groups 
of users.

2 
Case Study: Big Data System 
24/7 Operations, 
Support Engineers, 
Developers
Real-Time 
Dashboard
Data Scientists/
Analysts
Ad Hoc 
Reports
Management
Static Reports
• Real-time monitoring
• Full-text search
• Anomaly detection
• Raw and aggregated historical data
• Ad hoc analysis
• Real-time queries
• Near-real-time static reports
• Available through BI corporate tool
Web Servers
• Hundreds of 
servers
• Massive logs 
from 
multiple 
sources
FIGURE 5.1 Marketecture diagram for the Big Data system
5.2 System Requirements
Requirement elicitation activities have been previously performed. The most important re-
quirements collected are summarized here. They comprise a set of primary use cases, a set of 
quality attribute scenarios, a set of constraints, and a set of architectural concerns.
5.2.1 Use Case Model
The primary use cases for the system are described in the following table.
Use Case
Description
UC-1: Monitor 
online services
On-duty operations staff can monitor the current state of services and IT 
infrastructure (such as web server load, user activities, and errors) through 
a real-time operational dashboard, which enables them to quickly react to 
issues.
UC-2: Troubleshoot 
online service 
issues
Operations, support engineers, and developers can do troubleshooting and 
root-cause analysis on the latest collected logs by searching log patterns 
and fi ltering log messages.
UC-3: Provide 
management 
reports
Corporate users, such as IT and product managers, can see historical 
information through predefi ned (static) reports in a corporate BI (business 
intelligence) tool, such as those showing system load over time, product 
usage, service level agreement (SLA) violations, and quality of releases.

5.2  System Requirements  3
Use Case
Description
UC-4: Support 
data analytics
Data scientists and analysts can do ad hoc data analysis through SQL-
like queries to find specific data patterns and correlations to improve 
infrastructure capacity planning and customer satisfaction.
UC-5: Anomaly 
detection
The operations team should be notified 24/7 about any unusual behavior 
of the system. To support this notification plan, the system shall implement 
real-time anomaly detection and alerting (future requirement).
UC-6: Provide 
security reports
Security analysts should be provided with the ability to investigate potential 
security and compliance issues by exploring audit log entries that include 
destination and source addresses, a time stamp, and user login information 
(future requirement). 
5.2.2  Quality Attribute Scenarios
The most relevant quality attribute (raw) scenarios are presented in the following table. For 
each scenario, we also identify the use case that it is associated with.
ID
Quality  
Attribute
Scenario
Associated 
Use Case
QA-1
Performance
The system shall collect up to 15,000 events/second from 
approximately 300 web servers.
UC-1, 2, 5
QA-2
Performance
The system shall automatically refresh the real-time 
monitoring dashboard for on-duty operations staff with  
< 1 min latency. 
UC-1
QA-3
Performance
The system shall provide real-time search queries for 
emergency troubleshooting with < 10 seconds query 
execution time, for the last 2 weeks of data.
UC-2
QA-4
Performance
The system shall provide near-real-time static reports  
with per-minute aggregation for business users with  
< 15 min latency, < 5 seconds report load.
UC-3, 6
QA-5
Performance
The system shall provide ad hoc (i.e., non-predefined)  
SQL-like human-time queries for raw and aggregated 
historical data, with  
< 2 minutes query execution time. Results should be 
available for query in < 1 hour. 
UC-4
QA-6
Scalability
The system shall store raw data for the last 2 weeks  
available for emergency troubleshooting (via full-text  
search through logs).
UC-2
QA-7
Scalability
The system shall store raw data for the last 60 days 
(approximately 1 TB of raw data per day, approximately  
60 TB in total).
UC-4
(continues)

4 
Case Study: Big Data System 
ID
Quality  
Attribute
Scenario
Associated 
Use Case
QA-8
Scalability
The system shall store per-minute aggregated data for 1 
year (approximately 40 TB) and per-hour aggregated data 
for 10 years (approximately 50 TB).
UC-3, 4, 6
QA-9
Extensibility
The system shall support adding new data sources by just 
updating a configuration, with no interruption of ongoing data 
collection.
UC-1, 2, 5
QA-10
Availability
The system shall continue operating with  
no downtime if any single node or  
component fails.
All use 
cases
QA-11
Deployability
The system deployment procedure shall be fully automated 
and support a number of environments: development, test, 
and production.
All use 
cases
5.2.3  Constraints
The constraints associated with the system are presented in the following table.
ID
Constraint
CON-1
The system shall be composed primarily of open source technologies (for cost  
reasons). For those components where the value/cost of using proprietary technology 
is much higher, proprietary technology may be used.
CON-2
The system shall use the corporate BI tool with a SQL interface for static reports  
(e.g., MicroStrategy, QlikView, Tableau).
CON-3
The system shall support two specific deployment environments: private cloud (with 
VMware vSphere Hypervisor) and public cloud (Amazon Web Services). Architecture 
and technology decisions should be made to keep deployment vendor as agnostic as 
possible.
5.2.4  Architectural Concerns
The initial architectural concerns that are considered are shown in the following table.
ID
Concern
CRN-1
Establishing an initial overall structure as this is a greenfield system.
CRN-2
Leverage the team’s knowledge of the Apache Big Data ecosystem.

5.3  The Design Process  5
5.3  The Design Process
Now that we have enumerated the requirements, we are ready to begin the first iteration of 
ADD. This is a system from a relatively novel domain that is being created from scratch. Hence 
we follow the roadmap of design for greenfield systems in mature domains (as discussed in 
Section 3.3.1), albeit with some modifications to address the uncertainties inherent in the Big 
Data domain, such as the rapid emergence and evolution of technologies.
5.3.1  ADD Step 1: Review Inputs
The first step of the method involves reviewing the inputs. They are summarized in the follow-
ing table.
Category
Details
Design 
purpose
This is a greenfield system in a relatively novel domain. The organization will per-
form development following an Agile process with short iterations so that developers 
can quickly receive real-world feedback and continue modifying the system. At the 
same time, an architectural design is needed to make conscious decisions to satisfy 
architectural drivers and avoid unnecessary rework.
Primary 
functional 
requirements
From the use cases presented in Section 5.2.1, the following ones are designated 
as primary:
■
■UC-1
■
■UC-2
■
■UC-3
■
■UC-4
Quality 
attribute 
scenarios
The following table illustrates the priority of the primary quality attribute scenarios, 
as ranked by the customer and architect (as discussed in Section 3.3.2). Note that 
quality attributes scenarios with lower priorities exist but are not shown here.
Scenario  
ID
Importance to 
Customer
Difficulty of Implementation According to 
Architect
QA-1
High
High
QA-2
High
Medium
QA-3
Medium
Medium
QA-4
High
High
QA-5
Medium
High
QA-6
Medium
Medium
QA-7
Medium
Medium
QA-8
High
Medium
QA-9
High
Medium
QA-10
High
Medium
QA-11
Medium
High
(continues)

6 
Case Study: Big Data System 
Category
Details
Constraints
See Section 5.2.3.
Architectural 
concerns
All of the architectural concerns presented in Section 5.2.4 are included as drivers.
5.3.2  Iteration 1: Reference Architecture and Overall System Structure
This section presents the results of the activities that are performed in each of the steps of the 
ADD method in the first iteration of the design process.
5.3.2.1  Step 2: Establish Iteration Goal by Selecting Drivers
This is the first iteration in the design of a greenfield system, so the iteration goal is to establish 
an initial overall structure for the system (CRN-1). Even though this first iteration is driven by 
a general architectural concern, the architect must keep in mind all of the drivers and, in partic-
ular, constraints and quality attributes:
§ CON-1: Leverage open source technologies whenever applicable
§ CON-2: Use corporate BI tool with SQL interface for static reports
§ CON-3: Two deployment environments: private and public clouds
§ QA-1, 2, 3, 4, 5: Performance
§ QA-6, 7, 8: Scalability
§ QA-9: Extensibility
§ QA-10: Availability
§ QA-11: Deployability
5.3.2.2  Step 3: Choose One or More Elements of the System to Refine
Again, as this is greenfield development, and we are in the initial iteration, the element to refine 
is the entire system.
5.3.2.3  Step 4: Choose One or More Design Concepts That Satisfy the Selected Drivers
In this iteration, design concepts are selected from a group of data analytics reference architec-
tures (a list of such reference architectures can be found in the design concepts catalog of the 
Smart Decisions Game; see the Further Reading section for more information).

5.3  The Design Process  7
Design 
Decisions and 
Location
Rationale
Build the 
application as 
an instance of 
the Lambda 
(reference) 
architecture
The Lambda architecture, shown in Figure 5.2, is a reference architecture 
that splits the processing of a data stream into two streams: the “speed layer”, 
which supports access to real-time data (UC-1, UC-2, UC-5), and a layer that 
groups the “batch” and “serving” layers, which supports access to historical 
data (UC-3, UC-4, UC-6). (The creators of the Lambda architecture refer to 
these as “layers”, but this is different from prior—and more standard—usages 
of this term, which typically refer to a grouping of modules. Here the layers are 
groups of runtime components.) While the batch layer is based on immutable 
nonrelational techniques, the speed layer is based on streaming techniques to 
support strict real-time processing requirements.
Immutability in this case means that the data is not updated or deleted when 
it is collected; that is, it can be only appended. As all data is collected, no data 
can be lost and a machine or human error can be tolerated. For example, if a 
software engineer made an occasional mistake in processing or viewing logic, 
once that problem is resolved, the collected data can be used to replay and 
recompute the views from scratch.
For the reader’s convenience we describe the basic concepts of the Lambda 
architecture by walking through five steps:
1.	 All data received from multiple data sources is dispatched through the data 
stream element to both the batch layer and the speed layer for processing.
2.	 The batch layer acts as a landing zone that corresponds to the master 
dataset element (as an immutable, append-only set of raw data), and also 
precomputes information that will be used by the batch views.
3.	 The serving layer contains precalculated and aggregated views optimized 
for querying with low latency, which is often required by reporting solutions.
4.	 The speed layer processes and provides access to recent data through 
real-time views that are not available in the serving layer due to the high 
latency of batch processing.
5.	 All data in the system is available for querying, whether it is historical or 
recent, representing the key Lambda architecture principle: query = function 
(batch data + real-time data).
The parallel streams provide “complexity isolation”, meaning that design 
decisions, development, and execution of each stream can be done 
independently, which has been shown to increase fault tolerance, scalability, 
and modifiability (see Table 5.1).
Figure 5.3 depicts the architectural tradeoffs between these alternatives, and 
demonstrates the differences between the reference architectures in terms of 
four quality dimensions: scalability, support for ad hoc analysis, unstructured 
data processing capabilities, and real-time analysis capabilities:
As Figure 5.3 shows, the Lambda architecture provides the best tradeoff 
between scalability and ad hoc analysis.
(continues)

8 
Case Study: Big Data System 
Design 
Decisions and 
Location
Rationale
Use fault 
tolerance and no 
single point of 
failure principle 
for all elements 
in the system
Fault tolerance has become a standard for most Big Data technologies and the 
Lambda architecture already implies a number of design decisions to build a 
robust and fault-tolerant system, as noted above.
However, we will need to make sure, in all subsequent design and deployment 
decisions, that all candidate technologies will support the QA-10 requirement 
by providing fault-tolerant configurations and adhering to the “no single point of 
failure” principle.
TABLE 5.1  Alternatives and Reasons for Discarding
Alternative
Reason for Discarding
Traditional relational
This reference architecture is based on traditional relational model 
principles and SQL-based DBMSs, which are considered highly efficient 
for complex ad hoc read queries.
This is, however, the least appropriate alternative because of scalability 
and real-time processing limitations.
Extended relational
Although this reference architecture is completely based on relational 
model principles and SQL-based DBMSs, it intensively uses massive 
parallel processing (MPP) and in-memory techniques to improve scalability 
and extensibility.
It is less appropriate because of its high cost and real-time processing 
limitations.
Pure nonrelational
This reference architecture does not rely on relational model principles. It is 
often built on techniques such as NoSQL and MapReduce, and is effective 
for processing semistructured and unstructured data.
This alternative is closer to the goal in terms of cost economy and 
scalability, but ad hoc analysis is limited. 
Data refinery
A non-relational component performs an extract–transform–load (ETL) 
process to refine semistructured/unstructured data and load it, cleansed, 
into a data warehouse (a relational database) for further analysis.
It is less appropriate for this solution mostly because of its high cost and 
significant deficiencies in terms of real-time processing capabilities.

5.3 The Design Process 9
Data Stream
Master
Dataset
Precomputing
Layer 
Boundary
Data Flow 
(with direction indicated)
Query Results Flow 
Legend:
Element 
Boundary
Batch 
Views
Query & 
Reporting
Real-Time
Views
BATCH Layer
SERVING Layer
SPEED Layer
1
5
3
2
4
FIGURE 5.2 Lambda Architecture
Data Refinery
Extended
Relational
Pure 
Nonrelational
Traditional
Relational
Lambda 
Architecture
Scalability
Ad Hoc Analysis
Legend:
Unstructured data processing 
capabilities (the larger the better)
Real-time analysis capabilities
(the more saturated the better)
FIGURE 5.3 Tradeoffs among data analytics reference architectures

10 
Case Study: Big Data System 
5.3.2.4  Step 5: Instantiate Architectural Elements, Allocate Responsibilities,  
and Define Interfaces
The instantiation design decisions considered and made are summarized in the following table.
Design Decision and 
Location
Rationale
Split the Query and 
Reporting element 
into two subelements 
associated with the 
drivers
The Query and Reporting element in the Lambda architecture is divided 
into the following two sub-elements. They are associated with drivers as 
follows:
■
■Corporate BI tool (UC-3, UC-4, QA-4, QA-5, CON-2)
■
■Dashboard/visualization tool (UC-1, UC-2, QA-2, QA-3)
This division is driven by knowledge of the domain and the availability of 
tools. The guiding rationale is to have flexibility in selecting appropriate 
technologies—there could not be one single “universal” tool to satisfy all 
of these use cases, constraints, and quality attributes. Thus, we choose 
to separate concerns, which should give us more design options. Another 
difference from the “standard” Lambda architecture is that we may not 
need to merge the results of queries: According to our use cases, they can 
be executed independently for batch and real-time views.
Split the Precomputing 
and Batch Views 
elements into 
subelements 
associated with Ad Hoc 
and Static Views
These elements are decomposed into two subelements each:
■
■Ad Hoc Views Precomputing and Ad Hoc Batch Views (UC-4, QA-5)
■
■Static Views Precomputing and Static Batch Views (UC‑3, QA-4, 
CON-2)
The reason for this subdivision is the same as with the previous case: It 
gives us more flexibility to select the optimal patterns and technologies. If 
we discover, in subsequent design iterations, that there is one approach 
to address these two concerns simultaneously, it will be simple to merge 
these elements.
Change semantics and 
name of the Master 
Dataset to Raw Data 
Storage
This is more than just a name change; it is also a change in semantics. 
According to QA-7, the system shall store raw data for least 60 days. 
Thus older data can be archived and stored using other storage 
technologies (or even deleted). The Master Dataset has more 
responsibilities: It includes raw data storage as well as archived data. To 
simplify this case, the study of archived data will not be addressed.
In this initial iteration it is typically too early to precisely define functionality and 
interfaces.
5.3.2.5  Step 6: Sketch Views and Record Design Decisions
Figure 5.4 shows the result of the prior instantiation design decisions. The table that begins on 
the next page summarizes each element’s responsibilities.

5.3 The Design Process 11
Data
Stream
Raw Data 
Storage
Corporate 
BI Tool
Dashboard/
Visualization
Tool
Real-Time
Views
BATCH Layer
SERVING Layer
SPEED Layer
Ad Hoc Views
Precomputing
Ad Hoc 
Batch Views
Static Views
Precomputing
Static Batch
Views
Layer 
Boundary
Data Flow 
(with direction indicated)
Data
Sources 
Query Results Flow 
Legend:
Element 
Boundary
FIGURE 5.4 Instantiation of the Lambda architecture
Element
Responsibility
Data Sources
Web servers that generate logs and system metrics (e.g., Apache access and 
error log, Linux sysstat).
Data Stream
This element collects data from all data sources in real-time and dispatches it 
to both the Batch Layer and the Speed Layer for processing.
Batch Layer
This layer is responsible for storing raw data and precomputing the batch views 
to be stored in the Serving Layer.
Serving Layer
This layer exposes the batch views in a data store (with no random writes, but 
batch updates and random reads), so that they can be queried with low latency.
Speed Layer
This layer processes and provides access to recent data, which is not available 
yet in the serving layer due to the high latency of batch processing, through a 
set of real-time views.
Raw Data Storage
This element is a part of the batch layer and is responsible for storing raw data 
(immutable, append only) for a specifi ed period of time (QA-7).
Ad Hoc Views 
Precomputing
This element is a part of the Batch Layer and is responsible for precomputing 
the Ad Hoc Batch Views. The precomputing represents batch operations over 
raw data that transform it to a state suitable for fast human-time querying.
Static Views 
Precomputing
This element is a part of the Batch Layer and is responsible for precomputing 
the Static Batch Views. The precomputing represents batch operations over 
raw data that transform it to a state suitable for fast human-time querying.
(continues)

12 
Case Study: Big Data System 
Element
Responsibility
Ad Hoc Batch 
Views
This element is a part of the Serving Layer and contains precalculated and 
aggregated data optimized for ad hoc low-latency queries (QA-5) executed by 
data scientists/analysts.
Static Batch 
Views
This element is a part of the Serving Layer and contains precalculated 
and aggregated data optimized for predefined low-latency queries (QA-4) 
generated by a corporate BI tool.
Real-Time Views
This element is a part of the Speed Layer and contains indexed logs optimized 
for ad hoc, low-latency search queries (QA-3) executed by operations and 
engineering staff.
Corporate BI Tool
This business intelligence tool is licensed to be used across different 
departments. The tool supports a SQL interface (such as ODBC or JDBC) and 
can be connected to multiple data sources, including this system (UC-3, UC-4, 
CON-2).
Dashboard/
Visualization Tool
The operations team uses this real-time operational dashboard to monitor 
online services, search for important messages in logs, and quickly react to 
potential issues (UC-1, UC-2).
5.3.2.6  Step 7: Perform Analysis of Current Design and Review Iteration Goal and 
Achievement of Design Purpose
The decisions made in this iteration address important early considerations affecting the over-
all system structure. You do not need to start from a “blank page”, because the selected refer-
ence architecture already offers a proven initial decomposition and data flow that significantly 
saves design time and effort. Further design decisions will need to be made to selected candi-
date technologies and more details provided on how use cases and quality attributes will be 
supported. 
The following table summarizes the design progress using the Kanban board technique 
discussed in Section 3.8.2.
Not 
Addressed
Partially 
Addressed
Completely 
Addressed
Design Decisions Made During the Iteration
UC-1
Use Lambda architecture to provide access to real-
time data. No detailed decisions of which dashboard 
technology to use have been made.
UC-2
Use Lambda architecture to provide access to real-
time data. No detailed decisions of which search 
technology to use have been made.

5.3  The Design Process  13
Not 
Addressed
Partially 
Addressed
Completely 
Addressed
Design Decisions Made During the Iteration
UC-3
Use Lambda architecture to provide access to 
historical data. No detailed decisions of which 
storage and query technologies to use have been 
made.
UC-4
Use Lambda architecture to provide access to 
historical data. No detailed decisions of which 
storage and query technologies to use have been 
made.
UC-5
This use case has been omitted in this iteration 
as nonprimary, although the Lambda architecture 
supports it and we will address it in subsequent 
iterations.
UC-6
This use case has been omitted in this iteration 
as nonprimary, although from an architectural 
standpoint it is similar to UC-3.
QA-1
Potential data sources for the Data Stream element 
have been identified. No detailed decisions of which 
technologies to use for the data stream element 
have been made.
QA-3
The Real-Time Views element has been identified. 
No detailed decisions of which storage and query 
technology to use have been made.
QA-4
The Static Batch Views element has been identified 
and its responsibilities have been established. No 
detailed decisions of which storage technology to 
use have been made.
QA-5
The Ad Hoc Batch Views element has been 
identified and its responsibilities have been 
established. No detailed decisions of which storage 
and query technology to use have been made.
QA-6
The Real-Time Views element’s responsibilities have 
been established. No detailed decisions of which 
storage and query technology to use have been 
made.
QA-7
The Raw Data Storage element has been identified 
and its responsibilities have been established. No 
detailed decisions of which storage technology to 
use have been made.
(continues)

14 
Case Study: Big Data System 
Not 
Addressed
Partially 
Addressed
Completely 
Addressed
Design Decisions Made During the Iteration
QA-8
The Ad Hoc and Static Batch Views elements have 
been identified and their responsibilities have been 
established. No detailed decisions of which storage 
technologies to use have been made.
QA-10
It has been decided that all technologies chosen to 
implement the system elements support QA-10 by 
providing fault-tolerance configuration and no single 
point of failure.
CON-2
The Corporate BI Tool element has been identified. 
No detailed decisions on how this constraint will be 
met have been made.
CRN-1
An overall logical structure of the system has been 
established but the physical structure still needs to 
be defined.
CRN-2
No relevant decisions made
5.3.3  Iteration 2: Selection of Technologies
This section presents the results of the activities that are performed in each of the steps of ADD 
in the second iteration of the design process.
Technology choices often influence the system architecture, meaning that we need to se-
lect technologies at the earliest stages of architecture design. Choosing technologies starts with 
the identification and selection of technology families that are further instantiated into specific 
technologies. Starting with technology families allows us to make specific technologies inter-
changeable and thus keep the right level of technology agnosticism to avoid vendor lock-in 
(and as a result, there is less risk and less cost to change a technology to a better one in the 
future).
In this iteration we will show a technology tree that helps us choose optimal building 
blocks when designing Big Data greenfield systems.
5.3.3.1  Step 2: Establish Iteration Goal by Selecting Drivers
The goal of this iteration is to address CRN-2 (leverage the team’s knowledge of the Apache 
Big Data ecosystem) by selecting technologies to support system requirements defined in 
Section 5.2, particularly keeping in mind CON-1 (favor open source technologies).

5.3  The Design Process  15
5.3.3.2  Step 3: Choose One or More Elements of the System to Refine
The reference architecture selected in the previous iteration (the Lambda architecture) was de-
composed into elements that facilitate the selection of technology families and their associated 
specific technologies. These elements include the Data Stream, Raw Data Storage, Ad Hoc and 
Static Views Precomputing, Ad Hoc and Static Batch Views, Real-Time Views, and Dashboard/
Visualization Tool.
5.3.3.3  Step 4: Choose One or More Design Concepts That Satisfy the Selected Drivers
The design concepts used in this iteration are externally developed components. Initially, tech-
nology families are selected and associated with the elements to be refined. A technology fam-
ily represents a group of technologies with common functional purposes (see Section 2.5.5). 
The family names are indicative of their function, and some specific technologies may belong 
to several families at the same time, but having such a classification helps us make rational de-
sign decisions that eventually pay off in less rework and better readiness for changes. The his-
tory of the software industry shows that technology implementations are emerging, evolving, 
and disappearing much faster than the patterns and principles represented by their families.
Figure 5.5 illustrates family groups, technology families (in regular text), and their associ-
ated specific technologies (in italic text) for the Big Data domain. Further details about a num-
ber of these technologies can be found in the design concepts catalog of the Smart Decisions 
Game (see the Further Reading section).

16 
Case Study: Big Data System 
Big Data Analytics Catalog
Integration
Messaging
Data Collector
Apache Flume
Logstash
Fluentd
Apache Kafka
RabbitMQ
Amazon SQS
Apache ActiveMQ
HBase
Cassandra
Neo4J
OrientDB
HP Vertica
Teradata
MS PDW
Amazon Redshift
StreamSets
Talend
Informatica
MongoDB
CouchDB
Riak
Redis
Berkeley DB
MS SQL Server
QlikView
Microstrategy
Tableau
Tibco JasperSoft
Pentaho
Oracle RDBMS
IBM DB2
Splunk
Splunk
Kibana
Zoomdata
D3.js
GoJS
Highcharts
Impala
Apache Hive (Stinger)
Apache Solr
Elasticsearch
Hadoop MapReduce
Apache Tez
Apache Spark
Apache Storm
Spark Streaming
Amazon Kinesis
Apache Samza
Cascading
Apache Crunch
Amazon Pig
Apache Hive
Spark SQL
HDFS
CassandraFS
Distributed Message Broker
ETL/Data Integration Engine
Document-Oriented
Key-Value
Graph-Oriented
MPP Analytic RDBMS
Traditional Analytic RDBMS
BI Platform
Interactive Dashboard
Interactive Query Engine
Distributed Search Engine
Distributed Computing Engine
Event Stream Processor
Data Processing Framework
Graphic Library
Column-Family
ETL/ELT
Distributed File System
NoSQL Database
Analytic RDBMS
Visualization & Reporting
Search & Query
Processing
Data Storage
Processing and 
Analytics
Straight text  –  a technology family
Italic text  –  a specific technology 
Legend:
FIGURE 5.5  An example of a Big Data analytics design concepts catalog (Source: Softserve)

5.3  The Design Process  17
The BI Platform family group and related technologies are not considered further in this 
design exercise because the corporate BI tool is external to the target system.
Design 
Decisions and 
Location
Rationale and Assumptions
Select the Data 
Collector family 
for the Data 
Stream element 
Data Collector is a technology family (and an architectural pattern) that 
collects, aggregates, and transfers log data for later use. Usually Data 
Collector implementations offer out-of-the-box plug-ins for integrating with 
popular event sources and destinations.
The destinations are the Raw Data Storage and Real-Time Views elements, 
which will also be addressed in this iteration.
Alternative
Reason for Discarding
ETL Engine
The main purpose of ETL engines is to perform batch transfor-
mations, rather than per-event operations. This means that re-
al-time performance and scalability criteria (QA-1, QA-2) will be 
extremely difficult to meet (if it is possible to meet them at all).
Distributed 
Message 
Broker
Although this technology family can be solely used to imple-
ment the Data Stream element, it provides less support for 
extensibility (QA-9) and, therefore, is better suited as a comple-
ment to the data collector. This can be achieved, for example, 
using Flavka—a combination of Apache Flume (Data Collector) 
and Apache Kafka (Distributed Message Broker).
Select the 
Distributed File 
System family 
for the Raw Data 
Storage element 
According to the Lambda architecture principles, the Raw Data Storage 
element must be immutable. Thus new data should not modify existing data, 
but just be appended to the dataset. Data will be read in batch operations for 
transforming raw data to Batch Views. For these purposes, we can confidently 
choose a Distributed File System. 
Alternative
Reason for Discarding
NoSQL 
Database
Although NoSQL databases (especially column-family and doc-
ument-oriented) can be used for storing raw data, such as logs, 
this will cause unnecessary overhead in resource consumption 
(mostly memory consumption because of caching mechanisms) 
and maintainability (because of the need of configuring and 
evolving a schema).
Analytic 
RDBMS
All relational databases including analytic capabilities are 
based on the relational model, forming tables and rows. This 
works very well for executing complex queries, but this option is 
awkward (and expensive) for storing semistructured logs in their 
raw format.
(continues)

18 
Case Study: Big Data System 
Design 
Decisions and 
Location
Rationale and Assumptions
Select Interactive 
Query Engine 
family for both 
the Static and Ad 
Hoc Batch Views 
elements 
As we stated in the previous iteration, the Batch Views element is refined into 
two elements, the Static and Ad Hoc Batch Views, to support two use cases: 
the generation of static reports (UC-3, 6) and the support for ad hoc querying 
(UC-4).
The main design decision is to use the same technology family for both Static 
and Ad Hoc Batch Views—namely, the Interactive Query Engine. These en-
gines allow analytic database capabilities over data stored in a Distributed File 
System (thus this technology family is also selected implicitly). If we select a 
technology that is fast enough, it can be used for both elements.
The benefit of using a single technology family is that we do not need to have 
separate storage technologies for reporting and querying data. 
Alternative
Reason for Discarding
NoSQL 
Database
The Static Batch Views element can be implemented with the 
Materialized View pattern, by storing data in a form that is ready 
for querying and displaying in a reporting system (a corporate 
BI tool). The NoSQL Database family is often used for this 
purpose because it provides good scalability and, being open 
source, satisfies QA-8 (approximately 90 TB of aggregated 
data) and CON-1 (open source license).
However, NoSQL databases are not good options to use as 
data warehouses for ad hoc queries because they were not de-
signed for analytic purposes. Although they can be used for this 
purpose, this application will result in significant performance 
penalties.
This alternative is therefore discarded as it can be used only 
for the Static Batch Views, but is ineffective for Ad Hoc Batch 
Views
Analytic 
RDBMS
Ad hoc queries can be any queries that are supported by a 
SQL-like interface. The query result must be returned within 
“human” time (QA-5). The described scenario is exactly what a 
data warehouse is used for. This pattern is usually implement-
ed with Analytic RDBMS technologies following the Kimball 
or Inmon design approaches. At the same time, it will be quite 
costly to satisfy the scalability requirement of having approx-
imately 90 TB of aggregated data. The cost per terabyte in 
MPP analytic data­bases is significantly higher (up to 30 times) 
than the same amount of data in a NoSQL database or a dis-
tributed file system (such as Hadoop).

5.3  The Design Process  19
Design 
Decisions and 
Location
Rationale and Assumptions
Alternative
Reason for Discarding
Analytic 
RDBMS
This alternative is rejected because even if it can be used for 
both Static and Ad Hoc Batch Views, the technologies asso-
ciated with this family are costly compared to (open source) 
Hadoop-based alternatives.
Use Data 
Processing 
Framework 
for the Views 
Precomputing 
elements
As we have already selected the Distributed File System family for Raw 
Data Storage and Batch Views, the next step is to choose a solution for data 
transformation from the Raw Data Storage to the format used in the Batch 
Views.
The decision is to select Data Processing Framework as this technology 
family allows data processing pipelines to be created using abstractions that 
support faster development and better maintainability.
Alternative
Reason for Discarding
Distributed 
Computing 
Engine
Most Distributed Computing Engine technologies are designed 
for batch data processing, but require substantial knowledge of 
low-level primitives (e.g., for writing MapReduce tasks). 
Event 
Stream 
Processor
This is designed for real-time streaming processing; it is 
ineffective for batch operations.
Select Distributed 
Search Engine 
for the Real-Time 
Views element
The Real-Time Views element is responsible for full-text search over recent 
logs and for feeding an operational dashboard with real-time monitoring data 
(UC-1, UC-2). Distributed Search Engine is a technology family that serves 
just such purposes. 
Alternative
Reason for Discarding
NoSQL 
Database
Some NoSQL databases provide keyword search or text 
search, but these are not as powerful and fast as search 
engines that also provide text-processing features such as 
stemming and geolocation.
Analytic 
RDBMS
Some databases provide full-text search capabilities (e.g., MS 
SQL Server); however, they are less desirable from extensibility, 
maintenance, and cost standpoints.
Distributed 
File System 
and 
Interactive 
Query 
Engine
This approach works well for batch historical data; however, the 
latency of storing and processing will be too high for real-time 
data.
(continues)

20 
Case Study: Big Data System 
Design 
Decisions and 
Location
Rationale and Assumptions
Automate 
deployment of 
the system with 
Puppet scripts
Puppet scripts can be used for both Private Cloud (e.g., VMware) and Public 
Cloud (e.g., AWS) deployments. This supports the satisfaction of CON-3. 
Puppet allows automating the deployment process as well as managing the 
configuration of a system. There is a library of predefined scripts written by the 
Puppet community to automate the deployment of many popular open source 
technologies.
5.3.3.4  Step 5: Instantiate Architectural Elements, Allocate Responsibilities, and Define 
Interfaces
In this iteration, instantiation is performed by associating specific technologies with the tech-
nology families that were previously selected. The instantiation design decisions considered 
and made are summarized in the following table:
Design Decision 
and Location
Rationale
Use Apache 
Flume from the 
Data Collector 
family for the Data 
Stream element
As a primary candidate technology, we will select Apache Flume. It provides 
the required configurability to support QA-9 (adding new data sources by just 
updating a configuration at run-time).
Alternative
Reason for Discarding
Logstash or
Fluentd
Although Logstash and Fluentd are quite popular 
technologies (perhaps as popular as Flume) and will satisfy 
the requirements, we have to make a choice and select only 
one. An extra argument for choosing Flume is its support by 
three major Hadoop distribution vendors.
Use HDFS from 
the Distributed 
File System family 
for the Raw Data 
Storage element
For this technology, we can confidently choose HDFS, which was designed to 
support exactly this type of usage scenario for large data sets (QA-7, storing 
approximately 60 TB of raw data). There are also a number of Hadoop file 
formats in which to store data in HDFS, such as text file, SequenceFile, RCFile, 
ORCFile, Avro, and Parquet. The selection of a file format will be addressed in 
the third iteration.
Alternative
Reason for Discarding
CassandraFS
This technology is dependent on a NoSQL Database 
(Cassandra), whereas we have chosen Distributed File 
System alone.

5.3  The Design Process  21
Design Decision 
and Location
Rationale
Use Impala from 
the Interactive 
Query Engine 
family for both 
the Static and Ad 
Hoc Batch Views 
elements
We select Impala as a primary candidate technology, as it offers competitive 
performance (although it is still not as fast as the top Analytic RDBMS 
platforms) and an ODBC interface for connectivity with a corporate BI tool.
Keeping possible performance issues in mind, we plan a proof-of-concept 
in the next iterations to make sure this technology selection satisfies QA-4 
(less than 5 seconds report load) and QA-5 (less than 2 minutes ad hoc query 
execution time).
Alternative
Reason for Discarding
Apache Hive 
(Stinger)
Although Hive improved performance thanks to the Stinger 
initiative, the speed of queries is still slow compared to other 
alternatives such as Impala and Spark SQL.
Spark SQL
Spark is a very promising technology for Big Data analytics, 
but the use case of serving as a SQL adapter for a BI tool 
might not be optimal for Spark SQL. The downside is the high 
memory requirements and long query time of noncached data. 
In contrast, Impala has been designed and optimized for this 
exact scenario.
Use Elasticsearch 
from the Distribu­
ted Search Engine 
family for the 
Real-Time Views 
elements. Use 
Kibana from the 
Interactive Dash-
board family for 
the Dashboard/
Visualization Tool 
element.
As a primary candidate technology, we select Elasticsearch, since it also 
provides a visualization tool: an interactive dashboard called Kibana.
Although Kibana is a relatively simple dashboard without role-based security 
(at least, at the moment of designing this solution), it satisfies use cases UC-1, 
2 and QA-2 (auto-refresh dashboard with a less than 1 minute period).
Elasticsearch also provides a domain-specific language (Query DSL) that is 
supported by Kibana to query, filter, and visualize time series.
Alternative
Reason for Discarding
Splunk
Splunk also provides indexing and visualization capabilities 
(offering more features than Elasticsearch and Kibana); 
however, CON-1 drives us to prefer an open source solution.
Use Hive from the 
Data Processing 
Framework 
for the Views 
Precomputing 
elements
We select Hive as a primary technology candidate, although we will need to 
make sure that QA-4 (less than 15 minutes latency) is satisfied by creating a 
proof-of-concept prototype in a subsequent iteration.
Hive provides a SQL-like language, just like Impala (which has been already 
selected in this iteration); thus it allows us to leverage the skills of data 
warehouse designers when writing data transformation scripts.
Alternative
Reason for Discarding
Cascading or
Apache Pig
We disqualified Cascading and Pig so that we can minimize 
development time by leveraging the SQL skills of an existing 
development team. 

22 
Case Study: Big Data System 
The data exchanged between the elements will be defi ned more precisely in subsequent 
iterations. The format of this data constitutes the “interfaces” between the elements.
5.3.3.5 Step 6: Sketch Views and Record Design Decisions
Figure 5.6 illustrates the result of the instantiation decisions. The responsibilities of the ele-
ments shown in the diagram were discussed in step 6 of Iteration 1. The following table summa-
rizes the technology families and candidate specifi c technologies selected for these elements:
Element
Technology Family
Candidate Technology
Data Stream
Data Collector
Apache Flume
Raw Data Storage
Distributed File System
HDFS
Ad Hoc Views Precomputing
Data Processing Framework
Apache Hive
Static Views Precomputing
Data Processing Framework
Apache Hive
Ad Hoc Batch Views
Interactive Query Engine
Impala
Static Batch Views
Interactive Query Engine
Impala
Real-Time Views
Distributed Search Engine
Elasticsearch
Dashboard/Visualization Tool
Interactive Dashboard
Kibana
Technology family + (Specific technology)
Layer 
Boundary
Data Flow 
(with direction indicated)
Query Results Flow 
Legend:
Element 
Boundary
Te
Te
T chnolo
Layer 
Boundary
Element 
Boundary
Data Stream
Data 
Sources
Data Collector
(Flume)
Raw Data 
Storage
Corporate 
BI Tool
Dashboard/
Visualization
Tool
Real-Time Views
BATCH Layer
SERVING Layer
SPEED Layer
Ad Hoc Views
Precomputing
Ad Hoc 
Batch Views
Static Views
Precomputing
Static 
Batch Views
Distributed 
file system 
(HDFS) 
Distributed 
Search Engine 
(Elasticsearch) 
Data processing
framework (Hive) 
Data processing
framework (Hive) 
(Kibana)
Interactive Query 
Engine (Impala)
Interactive Query 
Engine (Impala)
FIGURE 5.6 Iteration 2 instantiation design decisions

5.3  The Design Process  23
The next table explains the relationships between elements based on the selected 
technologies:
Source Element
Destination Element
Relationship Description
Data Sources (logs)
Data Stream (Flume)
To be defined in the next iteration 
Data Stream (Flume)
Raw Data Storage (HDFS)
Network communication (push) 
through Flume HDFS sink
Raw Data Storage 
(HDFS)
Views Precomputing (Apache Hive)
Local and network communication 
encapsulated through Hive
Views Precomputing 
(Apache Hive)
Batch Views (Impala)
Local and network communication 
encapsulated through Hive
Batch Views (Impala)
Corporate BI Tool
Network communication (pull) 
through ODBC API
Data Stream (Flume)
Real-Time Views (Elasticsearch)
Network communication (push) 
through Flume Elasticsearch sink
Real-Time Views 
(Elasticsearch)
Dashboard/Visualization Tool 
(Kibana)
Network communication (pull) 
through Elasticsearch API
5.3.3.6  Step 7: Perform Analysis of Current Design and Review Iteration Goal and 
Achievement lf Design Purpose
The following Kanban table summarizes the design progress and the decisions made during 
the iteration. Note that drivers that were completely addressed in the previous iteration are not 
shown.
Not 
Addressed
Partially 
Addressed
Completely 
Addressed
Design Decisions Made During the Iteration
UC-1
Use Distributed Search Engine (Elastic­search) and Inter-
active Dashboard (Kibana) to display real-time monitor-
ing information.
Pending: Model indexes and create UI mockup.
UC-2
Use Distributed Search Engine (Elastic­search) and 
Interactive Dashboard (Kibana) for full-text search over 
recent log data.
Pending: Model indexes and create a proof-of-concept.
UC-3 
UC-4
Use Interactive Query Engine (Impala) for the Batch 
Views elements.
Pending: Model data and typical reports.
(continues)

24 
Case Study: Big Data System 
Not 
Addressed
Partially 
Addressed
Completely 
Addressed
Design Decisions Made During the Iteration
UC-6
This use case has been omitted in this iteration as 
nonprimary, although it is similar to UC-3 from an 
architectural standpoint.
QA-1
Use Data Collector (Apache Flume) for the Data Stream 
element.
Pending: Configuration, proof-of-concept, and 
performance tests.
QA-2
QA-3
Use Distributed Search Engine (Elasticsearch) and 
Interactive Dashboard (Kibana).
Pending: Proof-of-concept and performance tests.
QA-4
Use Interactive Query Engine (Impala) for the Static 
Batch Views element.
Pending: Model data, proof-of-concept, and 
performance tests.
QA-5
Use Interactive Query Engine (Impala) for the Ad Hoc 
Batch Views element.
Pending: Model data, proof-of-concept, and 
performance tests.
QA-6
Use Distributed Search Engine (Elasticsearch) for the 
Real-Time Views element.
Pending: Do capacity planning.
QA-7
Use Distributed File System (HDFS) for the Raw Data 
Storage element.
Pending: Select file format and do capacity planning.
QA-8
Use Distributed File System (HDFS) as storage for Batch 
Views.
Pending: Select file format and do capacity planning.
QA-9
Use Data Collector (Apache Flume) for the Data Stream 
element.
Pending: Configuration and proof-of-concept.
QA-10
Use fault tolerance in all system elements.
Pending: Stress test.
QA-11
Use Puppet scripts to automate the deployment process 
for different environments.

5.3  The Design Process  25
Not 
Addressed
Partially 
Addressed
Completely 
Addressed
Design Decisions Made During the Iteration
CON-1
All the selected technologies are open source.
CON-2
Use Interactive Query Engine (Impala) with ODBC 
interface.
CON-3
All selected technologies can be deployed to both 
private cloud (VMware) and public cloud (AWS) 
environments using Puppet scripts.
CRN-1
No relevant decisions made.
CRN-2
Technologies from the Apache Big Data ecosystem were 
selected and associated with the different elements in 
the reference architecture.
5.3.4  Iteration 3: Refinement of the Data Stream Element
This section presents the results of the activities that are performed in each of the steps of ADD 
for the third iteration of the design process.
Some design decisions made in this iteration require the creation of a proof-of-concept 
prototype, as they cannot be addressed in a purely conceptual manner. Given that the Big Data 
field is young and technologies are rapidly evolving, proofs-of-concepts of key elements are 
necessary to mitigate technology risks (e.g., incompatibility, slow performance, unsatisfactory 
reliability, limitations of claimed features) and to have the option to switch to an alternative 
early in the design and development process, thereby saving overall time and budget by avoid-
ing later rework.
5.3.4.1  Step 2: Establish the Iteration Goal by Selecting Drivers
The goal of this iteration is to address several concerns associated with the selection of Apache 
Flume, as the technology to be used for the Data Collector element. Apache Flume provides a 
reference structure—a data-flow model—depicted in the informal diagram shown in Figure 5.7.
The elements in Flume’s structure include:
§ The source: consumes events delivered to it by external data sources such as web servers
§ The channel: stores events received by the source
§ The sink: removes events from the channel and puts them in an external repository (i.e., 
destination)
The selection of Apache Flume raises several specific architectural concerns that need to 
be addressed:

26 
Case Study: Big Data System 
Data
Sources
Source
Channel
Flume Agent/Collector
Sink
Destinations
Data Flow 
(with direction indicated)
Legend:
FIGURE 5.7  Apache Flume data-flow reference structure
§ Selecting a mechanism for getting data from the external sources
§ Selecting specific input formats in the Source element
§ Selecting a file data format in which to store the events
§ Selecting a mechanism for the channeling events in the channel
§ Establishing a deployment topology for the Data Source elements
Addressing these specific architectural concerns will contribute to the satisfaction of the 
following quality attributes:
§ QA-1 (Performance)
§ QA-7 (Scalability)
§ QA-9 (Extensibility)
§ QA-10 (Availability)
5.3.4.2  Step 3: Choose One or More Elements of the System to Refine
In this iteration, the focus is on the elements in Flume’s structure.
5.3.4.3  Step 4: Choose One or More Design Concepts That Satisfy the Selected Drivers
In this iteration most of the decisions are about instantiation, since they primarily involve con-
figuring the elements that are already established by Flume. The only selection design decision 
involves choosing tactics to satisfy the availability and performance quality attributes.

5.3  The Design Process  27
Design Decisions and 
Location
Rationale and Assumptions
Use Flume in agent/
collector configuration. 
Agents are co-located 
on the web servers, and 
the collector runs in the 
Data Stream element.
A Flume instance can run in two modes: as an agent (directly co-located 
in the data sources) or as a collector (which combines data streams from 
multiple agents and writes to destinations).
From these two modes, Flume can be used in different configurations. 
The decision is to use Flume in both agent and collector configuration: 
The agents are co-located with the data sources and the Collector runs 
in the Data Stream element.
Alternative
Reason for Discarding
Flume agents are on 
each web server and 
write events directly 
to sinks (no collectors)
Generates heavy traffic from 300-plus 
simultaneous connections to sinks (HDFS and 
Elasticsearch). Produces multiple (per web 
server) files in HDFS, which is suboptimal for 
this distributed file system (rather than having 
larger files that aggregate data from multiple 
web servers).
Flume collectors 
receive events directly 
from web servers (no 
agents) and write to 
sinks
Does not support failover mode. If a collector 
node fails, the connected web servers will lose 
a receiver.
Introduce the tactic of 
“maintaining multiples 
copies of computations” 
by using a load-
balanced, failover tiered 
configuration
Out of the possible topology alternatives, the selected one is a load-
balanced and failover tiered topology based on performance (QA-1, 
15,000 events/second) and availability (QA-10, no single point of failure) 
quality attribute scenarios. 
Alternative
Reason for Discarding
Not replicating the 
collector
This would decrease performance and 
availability.

28 
Case Study: Big Data System 
5.3.4.4  Step 5: Instantiate Architectural Elements, Allocate Responsibilities, and Define 
Interfaces
The instantiation design decisions made in this iteration are summarized in the following table:
Design Decisions and 
Location
Rationale and Assumptions
Use access and error 
logs from the Apache 
HTTP Server as input 
formats
The system requirements include the collection and analysis of logs 
such as web server load, user activities, and errors. In reality, there 
could be tens (and sometimes hundreds) of data source types.
For the development of the proof-of-concept, a single type of data 
source system is considered: an Apache HTTP server (“web server”). 
The data to be collected includes user activities that will be tracked 
through an access log and system errors through an error log.
The web server access log records all requests processed by the server. 
A log entry might look like this:
143.21.52.246 - - [19/Jun/2014:12:15:17 +0000] "GET  
/test.html HTTP/1.1" 200 341 "-" "Mozilla/5.0 (X11; 
Linux x86_64; rv:6.0a1) Gecko/20110421 Firefox/6.0a1".
This example consists of the following data fields: client IP address, 
client identity, user ID, time stamp, request method, request URL, 
request protocol, response code, response size, referrer, user agent.
The web server error log sends diagnostic information and records any 
errors that it encounters when processing user requests. For example:
[19/Jun/2014:14:23:15 +0000] [error] [client 
50.83.180.156] Directory index forbidden by rule:  
/home/httpd/
This example consists of the following data fields: time stamp, severity 
level, client IP address, message.
Further data modeling and technology configuration will be based on 
these two types of logs and the described fields.
Log files are piped 
through an IP port in the 
source element of Flume 
agent
Apache Flume is configured to pipe log data through an IP port, such as 
by using syslog.
Alternative
Reason for Discarding
Read from a log file 
(e.g., running the 
UNIX command 
tail -F access_log)
This option looks the simplest but does not 
guarantee event delivery (events can be 
lost), which is stated in the Flume user guide.

5.3  The Design Process  29
Design Decisions and 
Location
Rationale and Assumptions
Identify event channeling 
methods for both the 
agents and the collector; 
make final decision 
through prototyping
The ingested events from the Source element are staged in the Channel 
element. At the moment Flume offers three possible options to configure 
the channel:
1.	 Memory channel: in-memory queue; faster, but if any events are left 
in the memory queue when a Flume process dies, they cannot be 
recovered.
2.	 File channel: durable and backed up by the local file system.
3.	 Apache Kafka: an approach in which Kafka serves as a distributed 
and highly available channel.
The selection from these options actually is a “classic” tradeoff 
of performance versus availability (or what is sometimes termed 
durability). Although we do not have an explicit durability scenario, 
we understand that with the future system extension (UC-6, security 
reports), this requirement becomes more critical. This is an example 
of an architectural concern, in the sense that it does not appear in any 
requirements document, but the architect has to deal with it nonetheless.
Given these options and no publicly available information about the 
performance consequences, this is a good candidate for prototyping and 
making a decision based on the results. Another rationale for prototyping 
and performance measurement is the need to calculate the required 
hardware resources. As a consequence, a new concern is identified and 
added to the backlog:
■
■CRN-3: Data modeling and developing proof-of-concept prototypes for 
key system elements
Select Avro as a specific 
file format for storing raw 
data in the HDFS sink
One decision that needs to be made when designing a solution based 
on Hadoop is the selection of an optimal file format. Hadoop supports a 
variety of formats that provide different functionalities, compression, and 
performance results depending on stored data and usage scenarios.
In this case the main scenarios are related to quality attributes such 
as performance (QA-1, 15,000 events/second), scalability (QA-7, 
approximately 60 TB of raw data), and extensibility (QA-9, adding new 
data sources). When we translate these requirements to file format traits, 
they will be impacted by performance (how fast data can be pushed by 
the Data Stream), a compression factor (less space to store), and ease 
of schema evolution (when adding new log formats or changing existing 
ones).
We select Avro, as it supports rich data structures, provides good 
compression levels (with the Snappy compression codec), and is flexible 
enough to accommodate schema changes (employing a self-describing 
format where data is stored with its schema).
(continues)

30 
Case Study: Big Data System 
Design Decisions and 
Location
Rationale and Assumptions
Alternative
Reason for Discarding
Text file (plain 
text, CSV, 
XML, JSON)
The compression ratio is poor compared with binary 
file formats (e.g., Avro). Also, text files do not support 
block compression, which is necessary when storing 
files larger than the size of an HDFS block. 
SequenceFile
Does not support flexible schema evolution. Consists 
of binary key/value pairs and does not store metadata 
with the data.
RCFile
This Hadoop columnar file format does not support 
schema evolution, and writing requires more CPU and 
memory compared with non-columnar formats.
ORCFile
Optimized RCFile provides better compression and 
faster querying, but has the same drawbacks as 
RCFile in terms of schema evolution, at the expense 
of writing performance.
Parquet
Parquet is a columnar file format that partially supports 
schema evolution, but still is slower for write opera-
tions compared with non-columnar file formats.
5.3.4.5  Step 6: Sketch Views and Record Design Decisions
Figure 5.8 illustrates the result of the instantiation decisions.
Element
Responsibility
Flume agent
Consume log events generated by a web server, split text log entries to 
separate fields, and deliver the parsed event records to a collector.
Flume collector
Collect event records from multiple agents in a load-balanced and 
fault-tolerant manner and deliver them to destinations (HDFS and 
Elasticsearch) for further persistency and processing.
5.3.4.6  Step 7: Perform Analysis of Current Design and Review Iteration Goal and 
Achievement of Design Purpose
The following Kanban table summarizes the design progress and the decisions made during 
the iteration. Note that drivers that were completely addressed in the previous iteration are not 
shown.

5.3  The Design Process  31
replicating
replicating
+ log parsing
+ log parsing
LB
+
failover
…
Collector
SPEED Layer
BATCH Layer
Data Stream
WebServer 1 (Data Source)
Flume Collector Tier
Storage Tier
Memory channel
(error)
netcat src
(access)
netcat src
(error)
Memory channel
(access )
avro sink
(access)
avro sink
(error)
HDFS
Elasticsearch
Application Tier
Flume Collector 
Memory channel
ES
avro src
(access)
Memory channel
HDFS
HDFS sink
(access)
ES sink
(access)
Memory channel
ES
avro src
(error)
Memory channel
HDFS
HDFS sink
ES sink
(error)
WebServer 2 (Data Source)
Memory channel
(error)
netcat src
(access)
netcat src
(error)
Memory channel
(access )
avro sink
(access)
avro sink
(error)
WebServer N (Data Source)
Flume Agent 
Flume Agent 
Flume Agent 
Memory channel
(error)
netcat src
(access)
netcat src
(error)
Memory channel
(access )
avro sink
(access)
avro sink
(error)
avro
json
(error)
Data flow between nodes
Data flow between flume 
components within the same node
Legend:
FIGURE 5.8  Iteration 3 instantiation design decisions

32 
Case Study: Big Data System 
Not 
Addressed
Partially 
Addressed
Completely 
Addressed
Design Decisions Made During the Iteration
UC-1
UC-2
UC-3
UC-4
Refinement of the Data Stream element. Decisions about 
other elements that participate in these use cases still 
need to be made.
QA-1
Flume load-balanced, failover tiered configuration is 
selected.
QA-9
Usage of Flume and Avro format for storing raw data.
QA-10
Flume load-balanced, failover tiered configuration is 
selected.
Decisions on other elements that participate in this sce-
nario still need to be made.
CRN-1
Tiers were identified for the Flume collector and storage.
CRN-3
This is a new architectural concern that was introduced 
in this iteration: data modeling and developing proof-
of-concept prototypes for key system elements. At this 
point, no relevant decisions have been made.
5.3.5  Iteration 4: Refinement of the Serving Layer
We now present the results of the activities that are performed in each of the steps of ADD in 
the fourth iteration of the design process.
We selected the Serving Layer for refinement (not the Batch Layer) because the risk of 
not achieving requirements is higher for this layer. This layer is directly involved in use cases 
UC-3 and UC-4 and a number of quality attribute scenarios in which performance and scalabil-
ity are critical factors.
As in the previous iteration, design activities involve the creation of prototypes. In this 
iteration, UI prototypes are also created. There are at least two reasons for this:
§ It will facilitate receiving early feedback from users, which can help to update 
requirements.
§ Data visualization scenarios often have an influence on data modeling.
5.3.5.1  Step 2: Establish the Iteration Goal by Selecting Drivers
The goal of this iteration is to address the newly identified concern of data modeling and de-
veloping proof-of-concept prototypes for key system elements (CRN-3) so as to satisfy the 
primary use cases and system requirements associated with the analysis and visualization of 
historic data. These use cases include:
§ UC-3
§ UC-4

5.3  The Design Process  33
The quality attribute scenarios associated with these use cases are:
§ QA-4 (Performance)
§ QA-5 (Performance)
§ QA-7 (Scalability)
§ QA-8 (Scalability)
5.3.5.2  Step 3: Choose One or More Elements of the System to Refine
In this iteration, the elements that are refined are the ones that support historical data, which 
include the Serving Layer elements: the Ad Hoc and Static Batch Views. Given that both types 
of elements use the same technology (Impala), the decisions made in this iteration affect both 
types of elements.
5.3.5.3  Step 4: Choose One or More Design Concepts That Satisfy the Selected Drivers
As in the previous iteration, the design activities here involve the configuration of the technologies 
that were associated with the elements. For this reason, no new design concepts are selected and 
all of the decisions belong to the instantiation category.
5.3.5.4  Step 5: Instantiate Architectural Elements, Allocate Responsibilities, and Define 
Interfaces
In this iteration, design concepts are instantiated based on the best practices of using the  
chosen technologies.
Design Decisions 
and Location
Rationale and Assumptions
Select Parquet as a 
file format for Impala 
in the Batch Views
The decision-making process for selecting a file format for Batch Views is 
similar to that in the previous iteration, where we selected a format for raw 
data storage. The data usage scenario is somewhat different, however. The 
previous case was about fast writing, effectively storing data, and extending 
data formats. This case is focused on fast querying (QA-4, less than 5 
seconds report load; QA-5, less than 2 minutes ad hoc query execution 
time), although scalability (QA-8, approximately 90 TB of aggregated data) 
and extensibility (QA-9, adding new data sources) drivers are still relevant.
Out of all the available alternatives, the Parquet file format looks like the most 
promising option to satisfy these requirements.
(continues)

34 
Case Study: Big Data System 
Design Decisions 
and Location
Rationale and Assumptions
Select Parquet as a 
file format for Impala 
in the Batch Views
In Parquet, a columnar structure represents relational tables on computer 
clusters and is designed for fast query processing, which is important for ad 
hoc data exploration and static reports. In addition, Parquet is optimized for 
Impala, which we selected as a primary technology for the interactive query 
engine during the second iteration. Finally, it provides a good compression 
ratio and allows some schema extension, by adding new columns at the end 
of the structure.
Alternative
Reason for Discarding
Text file (plain 
text, CSV, XML, 
JSON)
Slow for reads, especially when querying individual 
columns. Also does not support block compression, 
which is necessary when storing files larger than the 
size of an HDFS block.
SequenceFile
Slow for reads, especially when querying individual 
columns.
RCFile
The first columnar file format adopted in Hadoop. Does 
not support schema evolution.
ORCFile
Provides better compression and faster querying than 
RCFile, but has the same drawbacks as RCFile in 
terms of schema evolution.
Compared with Parquet, the compression ratio is 
better, but query performance is slower.
Another major limitation is that it is not supported by 
Impala.
Avro
Although Avro is considered the best multipurpose 
storage format for Hadoop, its query performance is 
noticeably slower compared with columnar formats, 
such as RCFile, ORCFile, and Parquet. 

5.3  The Design Process  35
Design Decisions 
and Location
Rationale and Assumptions
Use the star schema 
as a data model in 
the Batch Views
In the previous iteration, we selected Impala as a single technology for the 
Batch Views components, which impacts both static reports (UC-3, 6) and 
ad hoc querying (UC-4).
The star schema technique was selected for two reasons:
■
■Impala was designed for analytical queries, so it naturally provides good 
support for star schema data modeling.
■
■Ad hoc querying in combination with BI tools requires data to be well 
modeled to simplify query complexity and, as a result, allow faster query 
performance.
In our case, the star schema was designed to have small-dimension (in 
terms of number of rows) tables to avoid joins between big tables, as this 
typically consumes large amounts of system resources and affects query 
execution performance. Small-dimension tables can fit in memory and joins 
can be performed more effectively.
Alternative
Reason for Discarding
Flat tables
Flat tables are typically represented in the format of 
wide denormalized tables that contain all measures 
and dimension attributes.
Flat tables can cause significant performance issues 
when querying against large volumes of data.
5.3.5.5  Step 6: Sketch Views and Record Design Decisions
Figure 5.9 depicts the star schema data model implemented using Impala and Parquet.
The screenshot in Figure 5.10 presents a sample static report implemented with Tableau to 
demonstrate a possible view through a corporate BI tool. The report was created using test data 
stored in Parquet and provided by Impala through the ODBC interface.

36 
Case Study: Big Data System 
dim_request
dim_referrer
request_id 
<pi> int
request_method 
string
request_url 
string
request_protocol 
string
fact_access
client_ip 
  
string
request_id 
 <fi5> int
referrer_id 
 <fi4> int
user_agent_id 
 <fi1> int
city_id 
 <fi2> int
zip_code_id 
 <fi3> int
latitude 
  
string
longitude 
  
string
event_timestamp   
Timestamp
server_host 
  
string
requst_time 
  
int
response_code 
  
smallint
response_size 
  
int
referrer_id 
<pi> int
referrer_url 
string
referrer_site 
string
dim_city
city_id 
<pi> int
city 
  
string
region 
  
string
country 
  
string
dim_user_agent
user_agent_id <pi> int
user_agent_full   
string
browser 
  
string
device_type 
  
string
os 
  
string
dim_zip_code
zip_code_id 
<pi> int
zip_code 
  
string
fact_error
event_timestamp   
Timestamp
message_id 
 <fi1> int
server_host 
  
string
client_ip 
  
string
level 
  
string
dim_message
message_id 
<pi> int
message_url 
string
FIGURE 5.9  Star schema implemented in Impala and Parquet
FIGURE 5.10  Sample static report implemented with Tableau

5.4  Summary  37
5.3.5.6  Step 7: Perform Analysis of Current Design and Review Iteration Goal and 
Achievement of Design Purpose
The following Kanban table summarizes the design progress and the decisions made during 
the iteration. Note that drivers that were completely addressed in the previous iteration are not 
shown.
Not 
Addressed
Partially 
Addressed
Completely 
Addressed
Design Decisions Made During the Iteration
UC-3  
UC-4
Refinement of the Serving Layer, which is used in the 
use case. Decisions on other elements that participate in 
these use cases still need to be made.
QA-4  
QA-5  
QA-8
Use Parquet and star schema. Performance tests are still 
required and thus a new concern is introduced:
■
■CRN-4: Develop performance tests.
CRN-1
No relevant decisions made.
CRN-3
Data modeling and proof-of-concept prototypes were 
developed for the elements in the Serving Layer, but the 
same activity remains to be completed for the elements 
in the Speed Layer.
5.4  Summary
In this chapter we presented an extended example of using ADD 3.0 in a relatively novel do-
main, that of Big Data. As this example shows, architectural design can require many detailed 
decisions to be made to ensure that the quality attributes will be satisfied.
Also, this example shows that a large number of decisions rely on knowledge of many dif-
ferent patterns and technologies. The more novel the domain, the more likely that preexisting 
information (e.g., design concepts catalog, books of patterns, and reference architectures) will not 
be available for it. In such a case, you need to rely on your own judgment and experience, or you 
need to perform experiments and build prototypes. One way or another, such decisions must be 
made.
This instance of ADD also differed from the example presented in Chapter 4 in that we 
spent relatively little time and effort on building sequence diagrams as a means of deriving 
interface specifications. The example presented here relied on a relatively simple data-flow 
architecture with a modest number of components, so sequence diagrams were not needed 
to understand the relationships between the components. The “contracts” between the ele-
ments were determined by the information exchanged, as exemplified in step 5 of Iteration 3 
(Section 5.3.4.4).

38 
Case Study: Big Data System 
5.5  Further Reading
The design of a data warehouse has been extensively studied. Two good approaches are doc-
umented in R. Kimball and M. Ross, The Data Warehouse Toolkit, 3rd ed., Wiley, 2013; and  
W. Inmon, Building the Data Warehouse, 4th ed., Wiley, 2005.
The Lambda architecture was first presented by N. Marz and J. Warren, Big Data:  
Principles and Best Practices of Scalable Realtime Data Systems, Manning, 2015.
A good discussion of how to engineer for scalability can be found in M. Abbott and  
M. Fisher, The Art of Scalability: Scalable Web Architecture, Processes, and Organizations for 
the Modern Enterprise, Addison-Wesley, 2010.
P. Sadalage and M. Fowler. NoSQL Distilled: A Brief Guide to the Emerging World of 
Polyglot Persistence, Addison-Wesley, 2009.
A discussion of how and when to prototype as part of the architecture design process can 
be found in H-M Chen, R. Kazman, and S. Haziyev, “Strategic Prototyping for Developing Big 
Data Systems”, IEEE Software, March/April 2016.
A design concepts catalog that includes many of the reference architectures and tech-
nologies used in this case study is part of the Smart Decisions Game, which can be found at  
H. Cervantes, S. Haziyev, O. Hrytsay, and R. Kazman, “Smart Decisions Game”, http:// 
smartdecisionsgame.com.

1
Online Appendix of Designing Software Architectures: A Practical Approach, 
Second Edition by Humberto Cervantes & Rick Kazman / ISBN: 9780138108021 / 
Copyright 2024 / All Rights Reserved
From Ch. 6 of Designing Software  
Architectures, First Edition
Case Study: Banking System
Chapters 4 and 5 were both instances of greenfield development. In truth, that kind of devel-
opment is relatively rare. Most of the time you, as an architect, will be working on evolving an 
existing system rather than creating one from scratch. In this chapter, we present an example 
of using ADD 3.0 for a brownfield system in a mature domain (as discussed in Section 3.3.3). 
We first present the business context and then examine the project’s existing architectural doc-
umentation. This is followed by a step-by-step summary of the activities that are performed 
during the ADD iterations to evolve the system. While this is a real system, some of the details 
have been changed to protect the identities of the actors.
6.1  Business Case
In 2010, the government of a Latin American country issued a regulation that required banking 
institutions to digitally sign bank statements. To comply with the regulation, “ACME Bank” 
decided to commission the development of a software system, which we will call BankStat, 
whose main purpose was the generation of digitally signed bank statements.
Figure 6.1 presents a context diagram that illustrates how the BankStat system works. At 
its core, the system executes a batch process, which retrieves raw bank statement information 
from a data source (an external database) and then performs a series of validations on this data 

2 
Case Study: Banking System
to generate the bank statements and prepare them for digital signature by an external provider. 
The statements are sent to the provider, which returns the signed bank statements. These state-
ments are then stored by BankStat for further processing, including sending the statements to 
customers. This batch process is triggered automatically once a month and, during its execu-
tion, approximately 2 million bank statements are processed.
The following quality attributes scenarios are primary for this system:
 Reliability: Under normal operating conditions, the batch process is executed in its 
entirety 100% of the time.
 Performance: Under normal operating conditions, when the batch process starts, 
2 million bank statements are read, processed, and sent to the signing provider in at most 
one hour.
 Availability: During normal processing, a failure may occur when reading information 
from the data source or when sending information for digital signature. A notifi cation is 
then sent to the administrator, who manually restarts the process. When it is restarted, 
only the information that had not already been processed is treated.
Due to time constraints imposed by the government, only the core batch process for the 
system was developed and put into production. This initial release, however, did not provide 
a friendly interface with the system, which is necessary to monitor the state of the bank state-
ment processing, to request the reprocessing of incorrect statements. and to generate reports. 
In the fi rst release, the process could only be started or stopped manually from a console. For a 
second release of the system, the ACME Bank requested an extension of the BankStat system 
to better address these shortcomings.
:
FIGURE 6.1 Context diagram for the BankStat system

6.1 Business Case 3
The following subsections present the drivers for this second release of the system.
6.1.1 Use Case Model
Figure 6.2 presents the use case model for the second release of BankStat.
These use cases are described in more detail here:
Use Case
Description
UC-1: Query 
and reprocess 
statements
The user manually requests the reprocessing of a number of statements. 
The user specifi es criteria to query and select the statements that must be 
reprocessed. The user can, for example, select a period of interest or status of 
the statements that he is interested in (e.g., processed, signed, non-signed).
UC-2: Log in
The user logs in to the system.
UC-3: Generate 
report
The user generates reports regarding the process. 
UC-4: Query 
users log
The administrator queries user logs to display the activities of a particular user 
or groups of users. Information can be fi ltered using criteria such as dates or 
types of operations.
FIGURE 6.2 Use cases for the BankStat system (Key: UML)

4 
Case Study: Banking System
6.1.2  Quality Attribute Scenarios
The following table presents the new quality attribute scenario that is considered for this exten-
sion of the system.
ID
Quality 
Attribute
Scenario
Associated 
Use Case
QA-1
Security
A user performs any operation on the system, at any 
moment, and 100% of the operations performed by the user 
are recorded by the system in the operations log.
UC-4
6.1.3  Constraints
The following table presents the constraints that are considered for this extension of the system.
ID
Constraint
CON-1
The user’s accounts and permissions are handled by an existing user directory 
server that is used by various applications in the bank.
CON-2
Communication with the data source must be realized using JDBC.
CON-3
Communication with the digital signature provider system is performed using web 
services. These web services receive and return the information in an XML format 
that adheres to specifications established by the government.
CON-4
The system must be accessed from a web browser, although the access is available 
only from the bank’s intranet.
6.1.4  Architectural Concerns
The following table presents the concerns that are initially considered for this extension of the 
system.
ID
Concern
CRN-1
The system shall be programmed using Java and Java-related technologies to 
leverage the expertise of the development team.
CRN-2
The introduction of new functionality must, as far as possible, avoid modifications to 
the existing batch processing core.

6.2 Existing Architectural Documentation 5
6.2 Existing Architectural Documentation
This section presents a simplifi ed version of the system’s views, which provide relevant infor-
mation for the changes in the architecture.
6.2.1 Module View
The package diagram shown in Figure 6.3 depicts the system layers and the modules that they 
contain.
FIGURE 6.3 Existing modules and layers in the BankStat system (Key: UML)

6 
Case Study: Banking System
The responsibilities of the elements depicted in the diagram are described in the following 
table.
Element
Responsibility
Batch Processing 
Layer
This layer contains modules that perform the batch process. These 
components are developed using the Spring Batch framework.
Data Access Layer
This layer contains modules that store and retrieve data from a local 
database, which is used by the modules in the Batch Processing Layer.
Communications 
Layer
This layer contains modules that support communication with the external 
digital signature provider and the bank statement data source.
Batch Job Coordinator
This module is responsible for coordinating the execution of the batch 
process, including launching the process and invoking the different steps 
associated with it.
Job Steps
This module contains the “steps” that are part of the batch job. These 
steps perform activities such as validating the information retrieved from 
the data source and generating the bank statements. Such steps generally 
read, process, and write data. Data is read from and written to the local 
database.
Local Database 
Connector
This module is responsible for accessing a local database used by the job 
steps to exchange information while performing the batch process. We 
refer to this database as “local” to differentiate it from the external data 
source; this database is used only locally (i.e., internally) by the application, 
even if it is deployed in a different node (see the next section).
Notifications Manager
This module manages logs and sends notifications in case of issues such 
as a communication failure with the external system.
Data Source 
Connector
This module is responsible for connecting with the external database that 
provides the raw bank statement information.
Digital Signature 
Provider Connector
This module is responsible for accessing the external system that performs 
the digital signing of the bank statements.
6.2.2  Allocation View
The deployment diagram shown in Figure 6.4 presents an allocation view consisting of nodes 
and their relationships.

6.3 The Design Process 7
FIGURE 6.4 Existing deployment diagram for the BankStat system (Key: UML)
The responsibilities of the elements depicted in the diagram are described in the following 
table.
Element
Responsibility
Data Source 
Server
This server hosts a database that contains the raw data used to produce the bank 
statements.
BankStat 
Server
This server hosts the main batch process that is responsible for retrieving 
information from the Data Source Server, validating the information, and sending 
the information to the Digital Signature Server for signing.
Database 
Server
This server hosts a database that is used locally by the batch process in the 
BankStat Server to hold the state and information used in the execution of the 
batch process.
Digital 
Signature 
Server
This server, which is provided by an external entity, is responsible receiving, 
digitally signing, and returning the bank statements. The server exposes web 
services that receive and produce XML information.
6.3 The Design Process
We now describe the design process through the diff erent steps of ADD (as discussed in 
Section 3.2). As this is not a huge change to the existing system, the architect expects that the 
design activities will require only a single iteration of ADD.

8 
Case Study: Banking System
6.3.1  ADD Step 1: Review Inputs
The first step of the ADD method involves reviewing the inputs. They are summarized in the 
following table.
Category
Details
Design purpose
This is a brownfield system in a mature domain. The purpose is to design 
for the next system release.
Primary functional 
requirements
The primary use case for this release is UC-1.
Quality attribute 
scenarios
This extension of the system involves only a few quality attribute 
scenarios, so they are all considered as primary.
Constraints
See Section 6.1.3.
Architectural concerns
See Section 6.1.4.
Existing architecture 
design
Since this is brownfield development, an additional input is the existing 
architecture design, which was described in the previous section.
6.3.2  Iteration 1: Supporting the New Drivers
This section presents the results of the activities that are performed in each of the steps of ADD 
in the single iteration performed in this example.
6.3.2.1  Step 2: Establish Iteration Goal by Selecting Drivers
Only a limited number of drivers need to be addressed, so the architect has decided that a sin-
gle iteration is sufficient. The goal of this iteration is to modify the existing design to support 
all of the new drivers listed in Section 6.1
6.3.2.2  Step 3: Choose One or More Elements of the System to Refine
The elements to refine include the main modules from BankStat and the node where the system 
is deployed (BankStat Server). In addition to refining these modules, the physical node where 
the application is hosted is a candidate for refinement.
6.3.2.3  Step 4: Choose One or More Design Concepts That Satisfy the Selected Drivers
The following table summarizes the design decisions made with respect to the selection of 
design concepts.

6.3  The Design Process  9
Design Decisions and 
Location
Rationale
Use the Web Application 
Reference architecture
The use cases that are being introduced in the system require 
interaction through a web browser (CON-4). Since there are no 
requirements for rich user interaction, the Web Application architecture 
is selected (see Section A.1.1).
Discarded alternatives:
■
■Rich Internet application (see Section A.1.3), as it would require 
additional development effort and there are no requirements for a 
rich user interface.
Select the Spring Security 
framework to manage 
authorization and 
authentication
Security is a complex topic, and writing ad hoc code to support it 
is difficult and error prone. The needs for this application include 
managing authorization and authentication and an activity log. All of 
these features are available in the Spring Security framework, which 
can easily be integrated into the existing user directory server (CON-1) 
and is Java related (CRN-1).
Discarded alternatives:
■
■Ad hoc code: Challenging, error-prone, takes significant time to 
develop.
■
■Other frameworks: The first release of the solution has already been 
developed using Spring technologies. Hence it makes sense to con-
tinue using other technologies from the Spring platform, as they can 
be easily integrated with the existing frameworks.
Use the Shared Database 
Integration pattern 
to obtain information 
about the state of bank 
statements
The interactive part of the system needs to query the database 
that is used locally by the batch process to display the state of 
bank statement processing. The batch and interactive parts of the 
system can be seen as two different applications (or subsystems) 
that share data that is contained in the same database. The Shared 
Database Integration pattern can be used in this context to support the 
interaction between these systems. This approach does not require 
changes to be made in the existing parts of the system (CRN-2).
Discarded alternatives:
■
■Obtaining the information through an API, which would require mod-
ifications in the existing modules and would have a negative impact 
on performance.
Deploy using a three-tier 
deployment model 
Deploying the web part of the application will be done in a separate 
server. Thus, the deployment of this part of the application can be 
seen as an instance of the three-tier deployment model (see Section 
A.2.2). The benefit of this approach is that the server that hosts the 
batch process will not have to process the interactive requests, so 
performance will not be hindered.
Discarded alternatives:
■
■Hosting the application in the same server where the batch process 
is hosted. This would save some server costs, but could limit perfor-
mance of either the batch process or the interactive functions.

10 
Case Study: Banking System
6.3.2.4  Step 5: Instantiate Architectural Elements, Allocate Responsibilities, and Define 
Interfaces
The instantiated design decisions considered and made are summarized in the following table.
Design Decision and Location
Rationale
Host the web application in a separate 
server
This choice avoids performance reductions on the batch 
server and increases security (QA-1).
Configure Spring Security to use an 
external user directory server
This is to address CON-1.
The results of these instantiation decisions are recorded in the next step.
6.3.2.5  Step 6: Sketch Views and Record Design Decisions
The deployment diagram shown in Figure 6.5 depicts the new server that will host the applica-
tion and the external user directory server, along with their connections to the existing nodes.
The responsibilities of the newly introduced elements are described in the following table.
Element
Responsibility
Web/App Server
Hosts the interactive part of the application.
Auth Server
Existing server that manages users and permissions for multiple applications in 
the bank (CON-1).
The package diagram shown in Figure 6.6 illustrates how the reference architecture is 
instantiated and identifies the modules that are introduced to support the primary use case 
(UC-1). It also shows how these newly introduced elements are integrated with the existing 
layers and modules from the previous system release.

6.3 The Design Process 11
FIGURE 6.5 Refi ned deployment diagram (Key: UML)
FIGURE 6.6 Modules introduced to support the use case UC-1 (Key: UML)

12 
Case Study: Banking System
The responsibilities of the newly introduced elements are described in the following table.
Element
Responsibility
Bank Statement 
Reprocessing View
This module displays a view that allows the user to query the state of bank 
statements that have been processed. It also allows the user to select from 
these statements the ones that need to be reprocessed.
Bank Statement 
Reprocessing Service
This module manages requests from the view, which include requesting 
bank statement information, marking bank statements that need to be 
reprocessed, and triggering the restart of the batch job.
Security Manager
This module, which is implemented using Spring Security, handles 
authentication, authorization, and the activity log (QA-1). It is also 
integrated with the external user directory server (CON-1).
The sequence diagram shown in Figure 6.7 illustrates how UC-1 is performed. The user 
requests the state of bank statements to be displayed. This information is retrieved from the lo-
cal database by the Local Database Connector. Once displayed, the user selects the statements 
to reprocess. These bank statements are marked for reprocessing (by changing a flag) and the 
information is updated on the local database. Finally, the batch job is restarted. Note that the 
interactions with the system are recorded by Spring Security in the view. In addition, the invo-
cation of the Batch Job Coordinator is asynchronous, which avoids the problem of blocking the 
user interface.
From the interactions identified in the sequence diagram, initial methods for the interfaces 
of the interacting elements can be identified.
BankStatementReprocessingService
Method Name
Description
BankStatement [] get BSStatus(criteria)
Retrieves a collection of bank statements 
according to diverse criteria, including periods 
in time or status.
boolean reprocess(BankStatement [])
Requests the reprocessing of a collection of 
bank statements.
6.3.2.6  Step 7: Perform Analysis of Current Design and Review Iteration Goal and 
Achievement of Design Purpose
The following Kanban table summarizes the status of the various architectural drivers and the 
decisions that were made during the iteration to address them. As all the drivers were completely 
addressed, just a single iteration of ADD was required.

6.3 The Design Process 13
FIGURE 6.7 Sequence diagram for use case UC-1 (Key: UML)

14 
Case Study: Banking System
Not 
Addressed
Partially 
Addressed 
Completely
Addressed
Design Decisions Made During the Iteration
UC-1
Modules that support the use case and their interfaces 
were identified and defined based on the Web 
Application Reference architecture.
QA-1
Security logs are handled by Spring Security.
CON-1
Spring Security connects to the existing user 
directory server and uses its information to support 
authorization and authentication.
CON-3
No changes have been made to the module that 
connects to the data source.
CON-3
No changes have been made to the module that 
connects to the digital signature provider.
CON-4
The Web Application Reference architecture that was 
used specifically supports access from web browsers.
CRN-1
The technologies that have been selected are Java 
related.
CRN-2
Integration with the existing functionality was made 
through the database (using the Database Integration 
pattern); changes to the existing functionality were not 
needed.
6.4  Summary
In this chapter, we presented a simple (but real-world) example of the use of ADD in the  
context of a brownfield system. As this example illustrates, the steps of ADD are followed 
in exactly the same manner as in the context of the design of greenfield systems. The main  
difference is that one of the inputs of the design process is the existing architecture. This high-
lights the importance of documenting the architecture: If this information was not present, a 
great deal of time would need to be spent in understanding and reverse-engineering the code to 
create an appropriate model of the architecture before proceeding with the design and eventual 
implementation process.
Design in the context of brownfield systems usually involves more extensive changes than 
the ones illustrated by this example. Such changes often require refactoring and modification 
of the existing architecture to support the introduction of new elements and new relationships 
that result from the design activity. Modifying an existing architecture is oftentimes the most 
challenging aspect of designing in the context of brownfield systems. In brownfield systems, 
it is all too common that detailed knowledge of some parts of the system has been lost. Be-
cause this process can be complex and some uncertainty exists regarding the consequences of 
changes, we recommend that you perform an analysis of the proposed design changes before 
committing them to code.

6.5  Further Reading  15
6.5  Further Reading
The Shared Database Integration pattern is discussed in G. Hohpe and B. Woolf, Enterprise In-
tegration Patterns: Designing, Building and Deploying Messaging Solutions, Addison Wesley 
Professional, 2003.
In-depth discussions of software maintenance and evolution can be found in the classic 
book by F. Brooks, The Mythical Man Month, Addison-Wesley, 1995, and also in M. M. Leh-
man, “On Understanding Laws, Evolution, and Conservation in the Large-Program Life Cy-
cle”, Journal of Systems and Software, 1:213–221, 2010.


1
From Appendix A of Designing  
Software Architectures,  
First Edition
A Design Concepts Catalog
This chapter presents an excerpt from a catalog that groups design concepts that are associ-
ated with the domain of enterprise applications, such as the one presented in the case study 
in Chapter 4. As opposed to traditional catalogs that list just a single type of design concept, 
such as pattern catalogs, the catalog presented here groups different varieties of related design 
concepts. In this case, the catalog includes a selection of reference architectures, deployment 
patterns, design patterns, tactics, and externally developed components (frameworks). Also, the 
design concepts that are included in this catalog are gathered from different sources, reflecting 
what occurs in real-life design. The design concepts are presented in a very succinct way, and 
the reader looking for more detail should refer to the original sources using the references pro-
vided at the end of the chapter.
A.1  Reference Architectures
Reference architectures provide a blueprint for structuring an application (see Section 2.5.1). 
This section is based on the catalog in the Microsoft Application Architecture Guide.
Online Appendix of Designing Software Architectures: A Practical Approach, 
Second Edition by Humberto Cervantes & Rick Kazman / ISBN: 9780138108021 / 
Copyright 2024 / All Rights Reserved

2 
A Design Concepts Catalog
A.1.1  Web Applications
This web application is typically initiated from a web browser that communicates with a server 
using the HTTP protocol. The bulk of the application resides on the server, and its architecture 
is typically composed of three layers: the presentation, business, and data layers. The presen-
tation layer contains modules that are responsible for managing user interaction. The business 
layer contains modules that handle aspects related to the business logic. The data layer contains 
modules that manage data that is stored either locally or remotely. In addition, certain function-
ality that is common to modules across the layers is organized as cross-cutting concerns. This 
cross-cutting functionality includes aspects related to security, logging, and exception manage-
ment. Figure A.1 presents the components associated with the modules in web applications.
The following table summarizes the responsibilities of the components present in this 
reference architecture:
Component Name
Responsibility
Browser
A web browser running on the client machine.
User interface
These components are responsible for receiving user interactions and 
presenting information to the users. They contain UI elements such as 
buttons and text fields.
UI process logic
These components are responsible for managing the control flow of the 
application’s use cases. They are responsible for other aspects such as data 
validation, orchestrating interactions with the business logic, and providing 
data coming from the business layer to the user interface components.
Application facade
This component is optional. It provides a simplified interface  
(a facade) to the business logic components.
Business workflow
These components are responsible for managing (long-running) business 
processes, which may involve the execution of multiple use cases.
Business logic
These components are responsible for retrieving and processing application 
data and applying business rules on this data.
Business entities
These components represent the entities from the business domain and their 
associated business logic.
Data access
These components encapsulate persistence mechanisms and provide 
common operations used to retrieve and store information.
Helpers and utilities
These components contain functionality common to other modules in the 
data layer but not specific to any of them.
Service agents
These components abstract communication mechanisms used to transfer 
data to external services.
Security
These components include cross-cutting functionality that handles security 
aspects such as authorization and authentication.
Operation  
management
These components include cross-cutting functionality such as exception 
management, logging, and instrumentation and validation.
Communication
These components include cross-cutting functionality that handles 
communication mechanisms across layers and physical tiers.

A.1  Reference Architectures  3
FIGURE A.1  Web Application reference architecture (Key: UML)

4 
A Design Concepts Catalog
You should consider using this type of application when:
§ You do not require a rich user interface.
§ You do not want to deploy the application by installing anything on the client machine
§ You require portability of the user interface.
§ Your application needs to be accessible over the Internet.
§ You want to use a minimum of client-side resources.
A.1.2  Rich Client Applications
Rich client applications are installed and run on a user’s machine. Because the application runs 
on the user’s machine, its user interface can provide a high-performance, interactive, and rich 
user experience. A rich client application may operate in stand-alone, connected, occasionally 
connected, or disconnected mode. When connected, it typically communicates with remote 
services provided by other applications.
Rich client application modules are structured in three main layers or in a cross-cutting 
grouping, similar to a web application (see Section A.1.1). Rich client applications can be 
“thin” or “thick.” Thin-client applications consist primarily of presentation logic, which ob-
tains user data and sends it to a server for processing. Thick-client applications contain busi-
ness and data logic and typically connect to a data storage server only to exchange information 
that needs to be persisted remotely. Figure A.2 presents the components associated with the 
modules in rich client applications.
You should consider using this type of application when:
§ You want to deploy your application on the users’ machines.
§ You want your application to support intermittent or no network connectivity.
§ You want your application to be highly interactive and responsible.
§ You want to leverage the user’s machine resources (such as a graphics card).
Since these applications are deployed on the user’s machine, they are less portable and 
deployment and updating is more complicated. A range of technologies to facilitate their instal-
lation are available, however.

A.1  Reference Architectures  5
FIGURE A.2  Rich Client Application reference architecture (Key: UML)

6 
A Design Concepts Catalog
A.1.3  Rich Internet Applications
Rich Internet applications (RIAs) typically run inside a browser and may be developed us-
ing code that is executed by the browser such as Asynchronous Java­Script and XML (AJAX). 
RIAs may also run inside a browser plug-in, such as Silverlight. These applications are more 
complex than standard web applications and support rich user interaction and business logic. 
They are, however, typically restricted with respect to accessing local resources because of 
security concerns.
Typical RIAs are structured using the same three layers and modules found in web appli-
cations (see Section A.1.1). In RIAs, some business logic may be executed on the client ma-
chine, and some data may be stored locally. Like rich client applications, RIAs may range from 
relatively thin to quite thick clients.
The following table summarizes the responsibilities of the components of this refer-
ence architecture (shown in Figure A.3) that are not present in the Web Application reference 
architecture:
Component 
Name
Responsibility
Presentation
Responsible for managing user interaction (represents both UI components 
and UI process logic components).
Rich UI engine
Responsible for rendering user interface elements inside the plug-in execution 
container.
Business 
processing
Responsible for managing business logic on the client side.
Service interfaces
Responsible for exposing services that are consumed by the components that 
run on the browser.
Message types
Responsible for managing the types of messages that are exchanged between 
the client part and the server part of the application.
You should consider using this type of application when:
§ You want your application to have a rich user interface but still run inside a browser.
§ You want to perform some of the processing on the client side.
§ You want to deploy and update your application in a simple manner, without having to 
perform installations on the user machine.
However, there are some limitations associated with this type of application:
§ Access to local resources can be limited, because the application may run in a sandbox.
§ Loading time is non-negligible.
§ Plug-in execution environments may not be available in all platforms.

A.1  Reference Architectures  7
FIGURE A.3  Rich Internet Application reference architecture (Key: UML)

8 
A Design Concepts Catalog
A.1.4  Mobile Applications
A mobile application is typically executed on a handheld device and usually works in collab-
oration with a support infrastructure that resides remotely. These applications are structured 
using modules and layers similar to those found in a web application (see Section A.1.1), al-
though many of the components derived from these modules may be optional depending on 
whether a thin-client or a thick-client approach is followed. As shown in Figure A.4, at a min-
imum, the components responsible for user interaction are typically present. Communication 
with the support infrastructure is frequently unreliable, and these applications normally in-
clude some type of local data store that is periodically synchronized with data in the support 
infrastructure.
You should consider using this type of application when:
§ You want your application to run in a handheld device.
§ The network connectivity is unreliable, so the application needs to run in both offline and 
occasionally connected modes.
However, there is a substantial limitation associated with this type of application:
§ Resources on the handheld device may be limited.
A.1.5  Service Applications
Service applications are non-interactive applications that expose functionality through public 
interfaces (i.e., services). Services may be invoked by service consumer components remotely 
or from the same machine in which the service application is running. Services can be defined 
using a description language such as the Web Services Description Language (WSDL); oper-
ations are invoked using XML-based message schemas that are transferred over a transport 
channel. As a consequence, services promote interoperability.
Similar to the other types of reference architectures, service applications are structured 
using layers (Figure A.5). These applications are not interactive, so the presentation layer is not 
needed. It is replaced by a service layer that contains components responsible for exposing the 
services and exchanging information, similar to the server part of RIAs (see Section A.1.3).

A.1  Reference Architectures  9
FIGURE A.4  Mobile Application reference architecture (Key: UML)

10 
A Design Concepts Catalog
FIGURE A.5  Service Application reference architecture (Key: UML)

A.2  Deployment Patterns  11
You should consider using this type of application when:
§ Your application is not used by humans but rather by other systems and, as a conse-
quence, does not have a user interface.
§ Your application and the clients should be loosely coupled.
Except in cases where services are consumed by applications that reside in the same 
machine, network connectivity is required for the clients to communicate with the service 
application.
A.2  Deployment Patterns
Deployment patterns provide guidance on how to structure the system from a physical standpoint 
(see Section 2.5.3). Good decisions with respect to the deployment of the software system are 
essential to achieve important quality attributes such as performance, usability, availability, and 
security. This section is a summary from the catalog included in the Microsoft Application Archi-
tecture Guide.
A.2.1  Nondistributed Deployment
In nondistributed deployment, all of the components from the modules in the different layers 
reside on a single server except for data storage functionality (Figure A.6). Because the com-
ponents communicate locally, this may improve performance due to the lack of network com-
munication delays. However, performance may be affected by other aspects of the system, such 
as resource contention. Also, this type of application must support the peak usage of the largest 
consumers of system resources. Scalability and maintainability may be negatively affected be-
cause the same physical hardware is shared by all of the components.
FIGURE A.6  Nondistributed deployment example (Key: UML)

12 
A Design Concepts Catalog
A.2.2  Distributed Deployment
In a distributed deployment, the components of the application reside on separate physical tiers 
(Figure A.7). Typically, the components associated with specific layers are deployed in differ-
ent tiers. Tiers can be configured differently to best meet the requirements of the components 
that it hosts.
Distributed deployment facilitates scalability but the addition of tiers also brings addi-
tional costs, network latency, complexity, and deployment effort. More tiers may also be added 
to promote security. Different security policies may be applied according to the particular tier, 
and firewalls may be placed between the tiers. The following subsections describe various al-
ternatives of distributed deployment that can be used in conjunction with the reference archi-
tectures from Section A.1.
FIGURE A.7  Distributed deployment example (Key: UML)
Two-Tier Deployment (Client-Server)
Two-tier deployment is the most basic layout for distributed deployment. The client and the 
server are usually deployed on different physical tiers, as shown in Figure A.8.
FIGURE A.8  Two-tier deployment pattern (Key: UML)

A.2  Deployment Patterns  13
Three-Tier Deployment
In three-tier deployment, the application is deployed in a tier that is separate from the one that 
hosts the database, as shown in Figure A.9. This is a very common physical layout for web 
applications.
FIGURE A.9  Three-tier deployment pattern (Key: UML)
Four-Tier Deployment
In four-tier deployment, shown in Figure A.10, the web server and the application server are 
deployed in different tiers. This separation is usually done to improve security, as the web 
server may reside in a publicly accessible network while the application resides in a protected 
network. Additionally, firewalls may be placed between the tiers.
FIGURE A.10  Four-tier deployment pattern (Key: UML)
A.2.3  Performance Patterns: Load-Balanced Cluster
In the Load-Balanced Cluster pattern, the application is deployed on multiple servers that share 
the workload, as shown in Figure A.11. Client requests are received by a load balancer, which 
redirects them to the various servers according to their current load. The different application 
servers can process several requests concurrently, which results in performance improvements.

14 
A Design Concepts Catalog
FIGURE A.11  Load-balanced cluster deployment pattern (Key: UML)
A.3  Architectural Design Patterns
This section includes architectural design patterns (see Section 2.5.2) used in the case study in 
Chapter 4. The patterns presented here are based on the book Pattern-Oriented Software Archi-
tecture: A Pattern Language for Distributed Computing, Volume 4. The numbers in parentheses 
[e.g., Domain Model (182)] indicate the page in the book where the pattern is documented.
Note that we are using a home-grown notation for the patterns here, which is common 
in the patterns community. We define the symbols in a legend accompanying the first diagram 
(Layers) and use these symbols throughout this section.
A.3.1  Structural Patterns
These patterns are used to structure the system but they provide less detail than the reference 
architectures.

A.3  Architectural Design Patterns  15
Name
Layers
Problem and 
context
When transforming a Domain Model (182) into a set of modules that can be 
allocated to teams, [...] we need to support several concerns: the independent 
development of the modules, the independent evolution of the modules, the 
interaction among the modules.
Solution
Define two or more layers for the software under development, where each 
layer has a distinct and specific responsibility. To make the layering more 
effective, the interactions between the layers should be highly constrained. The 
strictest layering, as shown below, allows only unidirectional dependencies and 
forbids layer-bridging.
Structure
Consequences 
and related 
patterns
Typically, each self-contained and coherent responsibility within a layer is 
realized as a separate domain object. Domain objects are the containers 
(modules) that can be developed and evolved independently. 
Name
Domain Object 
Problem and 
context
When realizing a Domain Model (182) in terms of Layers (185), a key concern is 
to decouple self-contained and cohesive application responsibilities.
Solution
Encapsulate each distinct, nontrivial piece of application functionality in a self-
contained building block called a domain object.
continues

16 
A Design Concepts Catalog
Name
Domain Object 
Structure
Consequences 
and related 
patterns
The partitioning of an application’s responsibilities into domain objects is based 
on one or more granularity criteria. There can be different types of domain 
objects that encapsulate business features, domain concepts, or infrastructure 
elements. For example, domain objects might be a function such as an income 
tax calculation or a currency conversion, or a domain concept such as a bank 
account or a user. Domain objects can also aggregate other domain objects.
When designing domain objects, you need to distinguish an Explicit Interface 
(281), which exports some functionality, from its Encapsulated Implementation 
(313), which realizes that functionality. The separation of interface and implemen-
tation is the key to modularization. It minimizes coupling—each domain object 
depends only on explicit interfaces, not on encapsulated implementations. This 
makes it possible to create and evolve a domain object implementation inde-
pendently from other domain objects. 
A.3.2  Interface Partitioning
Name
Explicit Interface
Problem and 
context
When designing Layers (185) and their constituent Domain Objects (208), an 
important concern is how to properly create component (module) interfaces.
A module is a self-contained unit of functionality (and a self- 
contained unit of deployment) with a published interface. Clients can build upon 
existing modules as building blocks when providing their own functionality. Direct 
access to the module’s implementation might make clients dependent on the 
module’s internals, which ultimately increases coupling and erodes the ability of the 
application to evolve.
Solution
Separate the explicit interface of a module from its implementation. Export the 
explicit interface to the clients of the module, but keep its implementation private.
Structure

A.3  Architectural Design Patterns  17
Name
Explicit Interface
Consequences 
and related 
patterns
A call from the client through an explicit interface will be forwarded to the imple-
mentation, but the client code will depend only on the public interface, not on the 
implementation.
An explicit interface therefore enforces the separation of the component’s 
interface from its implementation. This separation means that a component’s 
implementation may be modified and the clients that use it will be unaffected, so 
long as the interfaces are unchanged.
Name
Proxy
Problem and 
context
When specifying an Explicit Interface (281), we often want to avoid accessing 
services of a component implementation directly, as these services may change 
or even be unknown until execution time.
Most modern software systems consist of cooperating components, some of 
which you create and others that you do not. Your components access and use 
the services provided by other components. It may be impractical or even impos-
sible to access the services of a component directly—for example, because the 
implementation resides on a remote server. 
Solution
Encapsulate all the details of interacting with the component within a surrogate—
called the proxy—and let clients communicate via the proxy rather than directly 
with the subject component.
Structure
Consequences 
and related 
patterns
A proxy frees both the client and the subjects from implementing component- 
specific housekeeping functionality. It is also transparent to clients whether they are 
connected with the real subject component or its proxy, because both publish an 
identical interface. The drawbacks of a proxy are additional execution time added 
to each client interaction (although, unless your application is highly sensitive to 
latency, this additional overhead is likely inconsequential).

18 
A Design Concepts Catalog
A.3.3  Concurrency
Name
Half-Sync/Half-Async
Problem and 
context
When developing concurrent software, a critical concern is to ensure that 
concurrent programming is relatively straightforward without sacrificing runtime 
efficiency.
Concurrent software typically performs both asynchronous and synchronous 
processing of service requests. Asynchrony is used to process low-level service 
requests (such as events) efficiently, whereas synchronous processing is used to 
simplify the processing of application services. To benefit from both programming 
models, it is essential to coordinate both kinds of processing.
Solution
Decompose the services of concurrent software into two separate streams 
or “layers”—synchronous and asynchronous—and add a queueing “layer” to 
mediate communication between them. 
Structure
Consequences 
and related 
patterns
This pattern allows you to process complex service requests, such as domain 
functionality or database queries, synchronously in separate threads. Similarly, 
lower-level system services, such as protocol handlers that respond to 
hardware interrupts, are handled asynchronously. In cases where services in 
the synchronous layer need to communicate with services in the asynchronous 
layer, they may exchange messages via the queueing layer.
The Half-Sync/Half-Async arrangement employs Layers (185) to keep the 
three distinct execution and communication models encapsulated and hence 
independent from one another.

A.3  Architectural Design Patterns  19
A.3.4  Database Access
Name
Data Mapper (Data Access Object [DAO])
Problem and 
context
When designing a Database Access Layer (538), we need to insulate 
applications from the details of how data is represented in persistent storage, 
such as the specific SQL queries to use.
Object-oriented applications and relational databases use different abstractions 
for representing data. However, many applications need to transfer data between 
these two “worlds.” It is desirable to keep the object-oriented domain model 
ignorant of the relational database schema. In this way, changes to one domain 
model will be less likely to ripple to the other. 
Solution
Introduce a data mapper for each type of persistent application object. The 
responsibility of this mapper is to transfer data from the objects to the database, 
and vice versa.
Structure
Consequences 
and related 
patterns
A data mapper is a mediator that moves data between an object-oriented 
domain model and a relational database. A client can use the data mapper to 
store or retrieve application data in the database. The data mapper performs 
any needed data transformations and maintains consistency between the two 
representations.
When a data mapper is used, in-memory objects do not even need to know that 
a database is present. Hence, they require no SQL code and can have complete 
ignorance of the database schema. In addition, the relational database schema 
and the object-oriented domain model can evolve independently. This provides 
an additional benefit that accrues to any abstraction interface: It simplifies unit 
testing, by allowing mappers to databases to be replaced by mock objects that 
support in-memory testing.
The data mapper makes application objects simpler and reduces their external 
dependencies, making them easier to evolve. There are two potential drawbacks 
to the Data Mapper pattern, however: (1) Changes in either the application object 
model or the database schema may require changes to a data mapper; and (2) 
the additional level of indirection introduces overhead, and hence latency, to 
every data access, which might be problematic for systems with hard real-time 
deadlines, for example. 

20 
A Design Concepts Catalog
A.4  Tactics
Tactics were presented in Section 2.5.4. Here we present a summarized catalog of tactics for 
seven commonly encountered quality attributes. This catalog comes from the book Software 
Architecture in Practice.
A.4.1  Availability Tactics
Figure A.12 summarizes the tactics to achieve availability.
Detect Faults
Prevent Faults
Ping / Echo
Removal from
Service
Monitor
Transactions
Predictive
Model
Recover from Faults
Heartbeat
Preparation
and Repair
Reintroduction
Active
Redundancy
Passive
Redundancy
Spare
Escalating
Restart
Exception
Handling
Shadow
Non-Stop
Forwarding
State
Resynchronization
Exception
Prevention
Fault
Fault
Masked
or
Repair
Made
Timestamp
Sanity
Checking
Condition
Monitoring
Voting
Exception
Detection
Self-Test
Rollback
Software
Upgrade
Retry
Ignore Faulty
Behavior
Degradation
Reconfiguration
Increase
Competence Set
Availability Tactics
FIGURE A.12  Availability tactics

A.4  Tactics  21
Detect Faults
§ Ping/echo: An asynchronous request/response message pair exchanged between nodes is 
used to determine reachability and the round-trip delay through the associated network 
path.
§ Monitor: A component is used to monitor the state of health of other parts of the system. 
A system monitor can detect failure or congestion in the network or other shared  
resources, such as from a denial-of-service attack.
§ Heartbeat: A periodic message exchange occurs between a system monitor and a process 
being monitored.
§ Timestamp: Detect incorrect sequences of events, primarily in distributed message- 
passing systems.
§ Sanity checking: Check the validity or reasonableness of a component’s operations or out-
puts; typically based on a knowledge of the internal design, the state of the system, or the 
nature of the information under scrutiny.
§ Condition monitoring: Check conditions in a process or device, or validates assumptions 
made during the design.
§ Voting: Check that replicated components are producing the same results. Comes in vari-
ous flavors, such as replication, functional redundancy, analytic redundancy.
§ Exception detection: Detect a system condition that alters the normal flow of execution, 
such as a system exception, parameter fence, parameter typing, or timeout.
§ Self-test: Procedure for a component to test itself for correct operation.
Recover from Faults (Preparation and Repair)
§ Active redundancy (hot spare): All nodes in a protection group receive and process iden-
tical inputs in parallel, allowing redundant spare(s) to maintain synchronous state with 
the active node(s).
§ Passive redundancy (warm spare): Only the active members of the protection group pro-
cess input traffic; one of their duties is to provide the redundant spare(s) with periodic 
state updates.
§ Spare (cold spare): Redundant spares of a protection group remain out of service until a 
failover occurs, at which point a power-on-reset procedure is initiated on the redundant 
spare prior to its being placed in service.
§ Exception handling: Deal with the exception by reporting it or handling it, potentially 
masking the fault by correcting the cause of the exception and retrying.
§ Rollback: Revert to a previous known good state, referred to as the “rollback line.”
§ Software upgrade: Perform in-service upgrades to executable code images in a non- 
service-affecting manner.
§ Retry: When a failure is transient, retrying the operation may lead to success.

22 
A Design Concepts Catalog
§ Ignore faulty behavior: Ignore messages sent from a source when it is determined that 
those messages are spurious.
§ Degradation: Maintain the most critical system functions in the presence of component 
failures, dropping less critical functions.
§ Reconfiguration: Reassign responsibilities to the resources that continue to function, 
while maintaining as much functionality as possible.
Recover from Faults (Reintroduction)
§ Shadow: Operate a previously failed or in-service upgraded component in a “shadow 
mode” for a predefined time prior to reverting the component back to an active role.
§ State resynchronization: Passive redundancy; state information is sent from active to 
standby components, in this partner tactic to active redundancy.
§ Escalating restart: Recover from faults by varying the granularity of the component(s) 
restarted and minimizing the level of service affected.
§ Non-stop forwarding: Functionality is split into supervisory and data variants. If a  
supervisor fails, a router continues forwarding packets along known routes while  
protocol information is recovered and validated.
Prevent Faults
§ Removal from service: Temporarily place a system component in an out-of-service state 
for the purpose of mitigating potential system failures.
§ Transactions: Bundle state updates so that asynchronous messages exchanged between dis-
tributed components are atomic, consistent, isolated, and durable.
§ Predictive model: Monitor the state of health of a process to ensure that the system is 
operating within nominal parameters; take corrective action when conditions are detected 
that are predictive of likely future faults.
§ Exception prevention: Prevent system exceptions from occurring by masking a fault, or pre-
vent them via smart pointers, abstract data types, and wrappers.
§ Increase competence set: Design a component to handle more cases—faults—as part of 
its normal operation.
A.4.2  Interoperability Tactics
Figure A.13 summarizes the tactics to achieve interoperability.

A.4  Tactics  23
Interoperability Tactics
Locate
Manage Interfaces
Discover
Service
Orchestrate
Tailor Interface
Request
Correctly
Handled
Information
Exchange
Request
FIGURE A.13  Interoperability tactics
Locate
§ Discover service: Locate a service by searching a known directory service. There may 
be multiple levels of indirection in this location process—that is, a known location may 
point to another location that in turn can be searched for the service.
Manage Interfaces
§ Orchestrate: Use a control mechanism to coordinate, manage, and sequence the invoca-
tion of services. Orchestration is used when systems must interact in a complex fashion 
to accomplish a complex task.
§ Tailor interface: Add or remove capabilities to an interface such as translation, buffering, 
or data smoothing.
A.4.3  Modifiability Tactics
Figure A.14 summarizes the tactics to achieve modifiability.

24 
A Design Concepts Catalog
Modifiability Tactics
Increase
Cohesion
Reduce
Coupling
Split Module
Encapsulate
Use an
Intermediary
Change
Arrives
Change Made
within Time 
and Budget
Reduce Size
of a Module
Increase
Semantic
Coherence
Restrict
Dependencies
Refactor
Abstract Common
Services
Defer
Binding
FIGURE A.14  Modifiability tactics
Reduce Size of a Module
§ Split module: If the module being modified includes a great deal of capability, the modifi-
cation costs will likely be high. Refining the module into several smaller modules should 
reduce the average cost of future changes.
Increase Cohesion
§ Increase semantic coherence: If the responsibilities A and B in a module do not serve the 
same purpose, they should be placed in different modules. This may involve creating a 
new module or moving a responsibility to an existing module.
Reduce Coupling
§ Encapsulate: Encapsulation introduces an explicit interface to a module. This interface 
includes an API and its associated responsibilities, such as “perform a syntactic transfor-
mation on an input parameter to an internal representation.”
§ Use an intermediary: Given a dependency between responsibility A and responsibility B 
(for example, carrying out A first requires carrying out B), the dependency can be broken 
by using an intermediary.

A.4  Tactics  25
§ Restrict dependencies: Restrict the modules that a given module interacts with or  
depends on.
§ Refactor: Refactoring is undertaken when two modules are affected by the same change 
because they are (at least partial) duplicates of each other.
§ Abstract common services: When two modules provide not quite the same but similar 
services, it may be cost-effective to implement the services just once in a more general 
(abstract) form.
Defer Binding
§ Defer binding: Allow decisions to be bound after development time.
A.4.4  Performance Tactics
Figure A.15 summarizes the tactics to achieve performance.
Performance Tactics
Control Resource Demand
Manage Resources
Manage sampling rate
Events 
Arrive
Response 
Generated 
within 
Time 
Constraints
Limit event response
Prioritize events
Reduce overhead
Bound execution times
Increase resource 
efficiency
Increase resources
Introduce concurrency
Maintain multiple 
copies of computations
Maintain multiple 
copies of data
Bound queue sizes
Schedule resources
FIGURE A.15  Performance tactics

26 
A Design Concepts Catalog
Control Resource Demand
§ Manage sampling rate: If it is possible to reduce the sampling frequency at which a 
stream of data is captured, then demand can be reduced, albeit typically with some loss 
of fidelity.
§ Limit event response: Process events only up to a set maximum rate, thereby ensuring 
more predictable processing when the events are actually processed.
§ Prioritize events: If not all events are equally important, you can impose a priority 
scheme that ranks events according to how important it is to service them.
§ Reduce overhead: The use of intermediaries (important for modifiability) increases the 
resources consumed in processing an event stream; removing them improves latency.
§ Bound execution times: Place a limit on how much execution time is used to respond to 
an event.
§ Increase resource efficiency: Improving the algorithms used in critical areas will decrease 
latency.
Manage Resources
§ Increase resources: Faster processors, additional processors, additional memory, and 
faster networks all have the potential to reduce latency.
§ Increase concurrency: If requests can be processed in parallel, the blocked time can be 
reduced. Concurrency can be introduced by processing different streams of events on dif-
ferent threads or by creating additional threads to process different sets of activities.
§ Maintain multiple copies of computations: The purpose of replicas is to reduce the con-
tention that would occur if all computations took place on a single server.
§ Maintain multiple copies of data: Keep copies of data (with one potentially being a sub-
set of the other) on storage with different access speeds.
§ Bound queue sizes: Control the maximum number of queued arrivals and consequently 
the resources used to process the arrivals.
§ Schedule resources: When there is contention for a resource, the resource must be 
scheduled.
A.4.5  Security Tactics
Figure A.16 summarizes the tactics to achieve security.

A.4  Tactics  27
Security Tactics
Resist Attacks
Attack
System 
Detects,
Resists, 
Reacts,
or Recovers
Detect Attacks
Recover
from Attacks
React to
Attacks
Detect
Intrusion
Detect Service
Denial
Verify Message
Integrity
Detect Message
Delay
Maintain
Audit Trail
Restore
See
Availability
Revoke
Access
ck
t
mpu
Lo
Co
er
Inform
Actors
Encrypt Data
Limit Exposure
Change Default
Settings
Separate
Entities
Identify
Actors
Authenticate
Actors
Authorize
Actors
Limit Access
Validate Input
FIGURE A.16  Security tactics
Detect Attacks
§ Detect intrusion: Compare network traffic or service request patterns within a system to a 
set of signatures or known patterns of malicious behavior stored in a database.
§ Detect service denial: Compare the pattern or signature of network traffic coming into a 
system to historic profiles of known denial-of-service attacks.
§ Verify message integrity: Use techniques such as checksums or hash values to verify the 
integrity of messages, resource files, deployment files, and configuration files.
§ Detect message delay: By checking the time that it takes to deliver a message, it is possi-
ble to detect suspicious timing behavior.
Resist Attacks
§ Identify actors: Identify the source of any external input to the system.
§ Authenticate actors: Ensure that an actor (user or a remote computer) is actually who or 
what it purports to be.
§ Authorize actors: Ensure that an authenticated actor has the rights to access and modify 
either data or services.

28 
A Design Concepts Catalog
§ Limit access: Control what and who may access which parts of a system, such as proces-
sors, memory, and network connections.
§ Limit exposure: Reduce the probability of a successful attack, or restrict the amount of 
potential damage—for example, by concealing facts about a system (“security by obscu-
rity”) or by dividing and distributing critical resources (“don’t put all your eggs in one 
basket”).
§ Encrypt data: Apply some form of encryption to data and to communication.
§ Validate input: Validate input from a user or an external system before accepting it in the 
system.
§ Separate entities: Use physical separation on different servers attached to different net-
works, virtual machines, or an “air gap.”
§ Change default settings: Force the user to change settings assigned by default.
React to Attacks
§ Revoke access: Limit access to sensitive resources, even for normally legitimate users 
and uses, if an attack is suspected.
§ Lock computer: Limit access to a resource if there are repeated failed attempts to access 
it.
§ Inform actors: Notify operators, other personnel, or cooperating systems when an attack 
is suspected or detected.
Recover from Attacks
In addition to the availability tactics for recovery of failed resources, an audit may be per-
formed to recover from attacks.
§ Maintain Audit Trail: Keep a record of user and system actions and their effects, to help 
trace the actions of, and to identify, an attacker.
A.4.6  Testability Tactics
Figure A.17 summarizes the tactics to achieve testability.

A.4  Tactics  29
Testability Tactics
Control and Observe
System State
Limit Complexity
Specialized
Interfaces
Limit Structural
Complexity
Limit
Nondeterminism
Tests
Executed
Faults
Detected
Record/
Playback
Localize State
Storage
Sandbox
Executable
Assertions
Abstract Data
Sources
FIGURE A.17  Testability tactics
Control and Observe System State
§ Specialized interfaces: Control or capture variable values for a component either through 
a test harness or through normal execution.
§ Record/playback: Capture information crossing an interface and use it as input for further 
testing.
§ Localize state storage: To start a system, subsystem, or module in an arbitrary state for a 
test, it is most convenient if that state is stored in a single place.
§ Abstract data sources: Abstracting the interfaces lets you substitute test data more easily.
§ Sandbox: Isolate the system from the real world to enable experimentation that is uncon-
strained by the worry about having to undo the consequences of the experiment.
§ Executable assertions: Assertions are (usually) hand-coded and placed at desired loca-
tions to indicate when and where a program is in a faulty state.

30 
A Design Concepts Catalog
Limit Complexity
§ Limit structural complexity: Avoid or resolve cyclic dependencies between components, 
isolate and encapsulate dependencies on the external environment, and reduce dependen-
cies between components in general.
§ Limit nondeterminism: Find all the sources of non-determinism, such as unconstrained 
parallelism, and weed them out as far as possible.
A.4.7  Usability Tactics
Figure A.18 summarizes the tactics to achieve usability.
Support User Initiative
§ Cancel: The system must listen for the cancel request; the command being canceled must 
be terminated; resources used must be freed; and collaborating components must be 
informed.
§ Pause/resume: Temporarily free resources so that they may be reallocated to other tasks.
§ Undo: Maintain a sufficient amount of information about system state so that an earlier 
state may be restored at the user’s request.
§ Aggregate: Aggregate lower-level objects into a group, so that a user operation may be 
applied to the group, freeing the user from the drudgery.
User
Request
User Given
Appropriate
Feedback and
Assistance
Usability Tactics
Support User
Initiative
Support System
Initiative
Cancel
Maintain User Model
Maintain SystemModel
Undo
Pause/Resume
Aggregate
Maintain Task Model
FIGURE A.18  Usability tactics

A.5  Externally Developed Components  31
Support System Initiative
§ Maintain task model: Determine the context so the system can have some idea of what 
the user is attempting and provide assistance.
§ Maintain user model: Explicitly represent the user’s knowledge of the system, the user’s 
behavior in terms of expected response time, and other characteristics of the system.
§ Maintain system model: The system maintains an explicit model of itself. This tactic is 
used to determine expected system behavior so that appropriate feedback can be given to 
the user.
A.5  Externally Developed Components
Externally developed components, including frameworks, were discussed in Section 2.5.5. 
Here we present a small sample of Java frameworks used in the case study in Chapter 4. Each 
framework is described very briefly and is associated with particular technology families, pat-
terns, and tactics. Full details for the different frameworks can be found by visiting the URL 
that is provided.
A.5.1  Spring Framework
Framework 
Name
Spring Framework
Technology 
family
Dependency injection and aspect-oriented programming (AOP) container
Language
Java
URL
http://projects.spring.io/spring-framework/
Purpose
The application framework allows the objects that form an application to be 
connected. It also supports different concerns through AOP.
Overview
The Spring container connects standard Java objects, or POJOs (Plain Old Java 
Objects), by using information from an XML file called “Application Context” or 
annotations in the Java code. This is the “Inversion of Control and Dependency 
Injection” pattern, since object dependencies are injected by the container.
The framework supports several aspects using AOP which are introduced as 
proxies between the Java objects when the container connects them. Supported 
aspects include:
■
■Security
■
■Transaction management
■
■Publishing object interfaces so the objects can be accessed remotely—for ex-
ample, via Web Services
(continues)

32 
A Design Concepts Catalog
Framework 
Name
Spring Framework
Structure
This diagram represents how two objects are connected by two important 
elements in the framework: the Spring container and the application context.  
(Key: UML)
Implemented 
design 
patterns and 
tactics
Patterns
■
■Inversion of Control and Dependency Injection
■
■Factory
■
■Proxy
Tactics
■
■Availability: Transactions
■
■Testability: Abstract data sources (separate interface and implementation)
Benefits
■
■Excellent tool support
■
■Simple integration with other frameworks such as web UI (Spring MVC, JSF), 
and persistence (JPA, Hibernate, iBatis) and integration (JMS)
Limitations
■
■Apache License 2.0
■
■Complex framework

A.5  Externally Developed Components  33
A.5.2  Swing Framework
Framework 
Name
Swing Framework
Technology 
family
Local user interface
Language
Java
URL
http://docs.oracle.com/javase/tutorial/uiswing/index.html
Purpose
Framework to support the creation of portable local (non-web) user interfaces.
Overview
The Swing framework provides a library of user interface components, including 
JFrame (windows), JMenu, JTree, JButton, JList, and JTable, among others. 
These components are built around the Model View Controller and Observer 
patterns.
Components such as JTables are views and controllers, and each has a 
corresponding model class (e.g., TableModel).
Components allow observers (called “listeners”) to be registered to manage 
different events. For example, JButtons allow ActionListeners to be 
registered as observers so that when the button is clicked, a callback method 
(actionPerformed) is invoked.
Structure
This diagram represents a small fraction of the framework’s classes (Key: UML)
Implemented 
design 
patterns and 
tactics
Patterns:
■
■Model View Controller
■
■Observer
■
■Others such as Composite and Iterator
Benefits
■
■Portable (can run on any operating system)
■
■Part of Java API
■
■Good tool support
Limitations
■
■Slower than using native UI elements
■
■Not the same look and feel as native UI elements

34 
A Design Concepts Catalog
A.5.3  Hibernate Framework
Framework 
Name
Hibernate
Technology 
family
Object-oriented to relational mapper
Language
Java
URL
http://hibernate.org/
Purpose
Simplify persistence of objects in a relational database.
Overview
Hibernate allows objects to be easily persisted in a relational database (and it sup-
ports different database engines). Object-relational mapping rules are described 
declaratively in an XML file called hibernate.cfg or using annotations in the 
classes whose objects need to be persisted.
Hibernate supports transactions and provides a query language called HQL 
(Hibernate Query Language) that is used to retrieve objects from the database. 
Hibernate utilizes multilevel caching schemes to improve performance. It also 
provides mechanisms to allow lazy acquisition of dependent objects to improve 
performance and reduce resource consumption. These mechanisms are config-
ured declaratively in the configuration files.
Structure
This diagram represents an entity that is persisted to a database by the Hibernate 
runtime using the information in the configuration file (Key: UML)
Implemented 
design patterns 
and tactics
Patterns:
■
■Data Mapper
■
■Resource Cache
■
■Lazy Acquisition
Tactics:
■
■Availability: Transactions
■
■Performance: Maintain multiple copies of data (cache)
Benefits
■
■Greatly simplifies the persistence of objects in relational database

A.6  Summary  35
Framework 
Name
Hibernate
Limitations
■
■Complex API
■
■Slower than JDBC (Java Database Connectivity)
■
■Difficult to map to legacy database schemas
A.5.4  Java Web Start Framework
Framework 
Name
Java Web Start Framework
Technology 
family
Deployment mechanism
Language
Java
URL
http://docs.oracle.com/javase/tutorial/deployment/webstart/
Purpose
Provide a platform-independent, secure, and robust deployment technology. 
Overview
By using a web browser, end users can start standard (non-applet) Java 
applications, and Java Web Start ensures they are running the latest version. 
To launch an application, users click a link on a page. If this is the first time the 
application is used, Java Web Start downloads the application. If the application 
has been previously used, Java Web Start verifies that the local copy is the latest 
version and launches it or downloads the newest version.
Structure
Not available
Implemented 
design patterns 
and tactics
Tactics:
■
■Security: Limit access (sandbox)
■
■Performance: Maintain multiple copies of data (cache)
Benefits
■
■Applications run in a sandbox but can read and write to local files.
■
■Because the application is cached, once it has been downloaded startup time is 
greatly reduced.
Limitations
■
■First launch may take some time
A.6  Summary
In this appendix we presented a design concepts catalog for the application domain of enter-
prise applications. Catalogs such as this one can become useful organizational assets, and we 
can readily imagine catalogs for other application domains such as Big Data (which we employ 
in Chapter 5) or mobile development.

36 
A Design Concepts Catalog
The catalog presented here is not intended to be exhaustive, as it contains only the design 
concepts used in the Chapter 4 case study. A real catalog, however, would contain a larger 
number of design concepts with more detailed descriptions and would be a valuable asset in a 
software development organization.
A.7  Further Reading
Reference architectures and deployment patterns are taken from Microsoft, Application Archi-
tecture Guide (2nd ed.), October 2009.
The tactics catalog is derived primarily from L. Bass, P. Clements, and R. Kazman, Soft-
ware Architecture in Practice (3rd ed.), 2012. Some of these tactics were earlier described 
in: F. Bachmann, L. Bass, and R. Nord, “Modifiability Tactics”, SEI/CMU Technical Report 
CMU/SEI-2007-TR-002, 2007, and J. Scott and R. Kazman, “Realizing and Refining Architec-
tural Tactics: Availability”, CMU/SEI-2009-TR-006, 2009.
The architectural patterns are taken from R. Buschmann, K. Henney, and D. Schmidt, 
Pattern-Oriented Software Architecture, Volume 4, Wiley, 2007.
The Spring framework is discussed in C. Walls, Spring in Action (4th ed.), Manning Pub-
lications, 2014.
The Swing framework is discussed in J. Elliot, R. Eckstein, D. Wood, and B. Cole, Java 
Swing (2nd ed.), O’Reilly Media, 2002.
The Hibernate framework is discussed in C. Bauer and G. King, Java Persistence with 
Hibernate, Manning Publications, 2015.


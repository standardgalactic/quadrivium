Random Graphs 
and Networks: 
A First Course
Alan Fneze and Michal Karohski

Random Graphs and Networks: A First Course
Networks surround us, from social networks to protein-protein interaction networks 
within the cells of our bodies. The theory of random graphs provides a necessary 
framework for understanding their structure and development.
This text provides an accessible introduction to this rapidly expanding subject. It 
covers all the basic features of random graphs - component structure, matchings and 
Hamilton cycles, connectivity, and chromatic number - before discussing models of 
real-world networks, including intersection graphs, preferential attachment graphs, 
and small-world models.
Based on the authors’ own teaching experience, Random Graphs and Networks: A 
First Course can be used as a textbook for a one-semester course on random graphs 
and networks at advanced undergraduate or graduate level. The text includes numer­
ous exercises, with a particular focus on developing students’ skills in asymptotic 
analysis. More challenging problems are accompanied by hints or suggestions for 
further reading.
Alan Frieze is Professor in the Department of Mathematical Sciences at Carnegie 
Mellon University. He has authored almost 400 publications in top journals and was 
a plenary speaker at the 2014 International Congress of Mathematicians.
Michal Karonski is Professor Emeritus in the Faculty of Mathematics and Computer 
Science at Adam Mickiewicz University, where he founded the Discrete Mathematics 
group. He served as Editor-in-Chief of Random Structures and Algorithms for 30 
years.

Random Graphs and 
Networks: A First Course
ALAN FRIEZE
Carnegie Mellon University
MICHAL KARONSKI
Adam Mickiewicz University
Cambridge
UNIVERSITY PRESS

■ CAMBRIDGE
WW UNIVERSITY PRESS
Shaftesbury Road, Cambridge CB2 8EA, United Kingdom
One Liberty Plaza, 20th Floor, New York, NY 10006, USA
477 Williamstown Road, Port Melbourne, VIC 3207, Australia
314-321, 3rd Floor, Plot 3, Splendor Forum, Jasola District Centre,
New Delhi - 110025, India
103 Penang Road, #05-06/07, Visioncrest Commercial, Singapore 238467
Cambridge University Press is part of Cambridge University Press & Assessment, 
a department of the University of Cambridge.
We share the University’s mission to contribute to society through the pursuit of 
education, learning and research at the highest international levels of excellence.
www.cambridge.org
Information on this title: www.cambridge.org/9781009260282
DOI: 10.1017/9781009260268
© Alan Frieze and Michal Karonski 2023
This publication is in copyright. Subject to statutory exception and to the provisions 
of relevant collective licensing agreements, no reproduction of any part may take 
place without the written permission of Cambridge University Press & Assessment.
First published 2023
A catalogue record for this publication is available from the British Library
A Cataloging-in-Publication data record for this book is available from the Library of Congress
ISBN 978-1-009-26028-2 Hardback
ISBN 978-1-009-26030-5 Paperback
Cambridge University Press & Assessment has no responsibility for the persistence 
or accuracy of URLs for external or third-party internet websites referred to in this 
publication and does not guarantee that any content on such websites is, or will 
remain, accurate or appropriate.

To our grandchildren

Contents
Preface 
page ix
Acknowledgments 
x
Conventions/Notations 
xi
Part I Preliminaries 
1
1 
Introduction 
3
1.1 
Course Topics 
3
1.2 
Course Outline 
4
2 
Basic Tools 
8
2.1 
Asymptotics 
8
2.2 
Binomials 
10
2.3 
Tail Bounds 
16
Part II Erdos-Renyi-Gilbert Model 
27
3 
Uniform and Binomial Random Graphs 
29
3.1 
Models and Relationships 
29
3.2 
Thresholds 
35
4 
Evolution 
45
4.1 
Subcritical Phase 
45
4.2 
Supercritical Phase 
54
4.3 
Phase Transition 
58
5 
Vertex Degrees 
64
5.1 
Degrees of Sparse Random Graphs 
64
5.2 
Degrees of Dense Random Graphs 
70
6
Connectivity
78
6.1 Connectivity
78
6.2 $ -Connectivity
82

viii
Contents
7
Small Subgraphs
7.1 Thresholds
7.2 Asymptotic Distributions
85
85
89
8
Large Subgraphs
93
8.1 Perfect Matchings
93
8.2 Long Paths and Cycles
100
8.3 Hamilton Cycles
102
8.4 Spanning Subgraphs
106
9
Extreme Characteristics
111
9.1 Diameter
111
9.2 Largest Independent Sets
114
9.3 Chromatic Number
120
Part III
Modeling Complex Networks
125
10
Inhomogeneous Graphs
127
10.1 Generalized Binomial Graph
127
10.2 Expected Degree Sequence
134
10.3 Fixed Degree Sequence
140
11
Small World
154
11.1 Watts-Strogatz Model
154
11.2 Kleinberg’s Model
160
12
Network Processes
163
12.1 Preferential Attachment
163
12.2 Spatial Preferential Attachment
171
13
Intersection Graphs
178
13.1 Binomial Random Intersection Graphs
179
13.2 Random Geometric Graphs
187
14
Weighted Graphs
197
14.1 Minimum Weight Spanning Tree
198
14.2 Shortest Paths
200
14.3 Minimum Weight Assignment
205
References
210
Author Index
216
Subject Index
218

Preface
In 2016, the Cambridge University Press published our book entitled Introduction to 
Random Graphs (see [52]). In the preface, we stated that our purpose in writing it was
. . . to provide a gentle introduction to a subject that is enjoying a surge in interest. We believe 
that the subject is fascinating in its own right, but the increase in interest can be attributed to 
several factors. One factor is the realization that networks are “everywhere”. From social 
networks such as Facebook, the World Wide Web and the Internet to the complex interactions 
between proteins in the cells of our bodies, we face the challenge of understanding their 
structure and development. By and large, natural networks grow unpredictably, and this is often 
modeled by a random construction. Another factor is the realization by Computer Scientists 
that NP-hard problems are typically easier to solve than their worst-case suggests, and that an 
analysis of running times on random instances can be informative.
After five years since the completion of Introduction to Random Graphs, we have 
decided to prepare a slimmed down, reorganized version, at the same time supple­
mented with some new material. After having taught graduate courses on topics based 
on material from our original book and after having heard suggestions from our col­
leagues, teaching similar courses, we decided to prepare a new version which could be 
used as a textbook, supporting a one-semester undergraduate course for mathematics, 
computer science, as well as physics majors interested in random graphs and network 
science.
Based on our teaching experience, the goal of this book is to give our potential 
reader the knowledge of the basic results of the theory of random graphs and to show 
how it has evolved to build firm mathematical foundations for modern network theory, 
in particular, in the analysis of real-world networks. We have supplemented theoretical 
material with an extended description of the basic analytic tools used in the book, 
as well as with many exercises and problems. We sincerely hope that our text will 
encourage our potential reader to continue the study of random graphs and networks 
on a more advanced level in the future.

Acknowledgments
Our special thanks go to Katarzyna Rybarczyk for her careful reading of some early 
chapters of this book. We are particularly grateful to Mihyun Kang and her colleagues 
from the University of Graz for their detailed comments and useful suggestions: Tuan 
Anh Do, Joshua Erde, Michael Missethan, Dominik Schmid, Philipp Sprussel, Julian 
Zalla.

Conventions/Notations
Often in what follows, we will give an expression for a large positive integer. It might 
not be obvious that the expression is actually an integer. In such cases, the reader can 
rest assured that he/she can round up or down and obtain any required property. We 
avoid this rounding for convenience and for notational purposes.
In addition, we list the following notation.
Mathematical Relations
• f (1) = O(g(x)): | f (x)| < K|g(x)| for some constant K > 0 and all x e R.
• f (x) = 0(g(x)): f (n) = O(g(x)) and g(x) = O(f (x)).
• f (x) = o(g(x)) as x a: f (x)/g(x) 
0 as x a.
• Adding ~ to all the above three means that we ignore logarithmic factors.
• 
A « B: A/B 0 as n to.
• 
A » B: A/B to as n to.
• 
A ~ B: A/B 
1 as some parameter converges to 0 or to or another limit.
• 
A < B or B > A if A < (1 + o(1))B.
• [n]: This is {1,2,...,n}. In general, if a < b are positive integers, then 
[a, b] = {a, a + 1,..., b}.
• If 5 is a set and k is a non-negative integer, then (f) denotes the set of k-element 
subsets of S. In particular, (f) denotes the set of k-sets of {1,2,..., n}. Further­
more, (<k) = U$=o (f).
Graph Notation
• 
G = (V, E): V = V(G) is the vertex set and E = E(G) is the edge set.
• e(G) = |E(G)|, and for S c V, we have EG(S) = {e e E : e Q S} and eG(S) 
= \Eg (S)|. We drop the subscript G if the graph in question is clear.
• For S,T c V,letE(S,T) = Eg(S,T) = {{v, w} e E : / e S,w e T} and eG(S,T) = 
e (S,T) = |Eg (S,T )|.
• N(S) = Ng(S) = {w ^ S : 3v e S such that {v, w} e E} and dG(S) = |NG(S)| for 
S c V(G).
• Ng (S, X) = Ng (S) n X for X, S c V.
• 
ds (x) = | {y e S : {x, v} e E} | for x e V, S Q V.

xii
Conventions/Notations
• For sets X,Y c V(G), let NG(X,Y) = {y e Y : 3x e X, {x,y}e E(G)} and
eG (X, Y) = |Ng (X, Y )|.
• For K c V(G) and / e V(G), let dK (/) denote the number of neighbors of / in K. 
The graph G is hopefully clear in the context in which this is used.
• For a graph H, aut(H) denotes the number of automorphisms of H.
• 
G = H: graphs G and H are isomorphic.
Random Graph Models
• [n]: The set {1,2,...,«}.
• 
&',&: The family of all labeled graphs with a vertex set V = [n] = {1,2,...,n} and 
exactly m edges.
• G„.m: A random graph chosen uniformly at random from @n,m.
* 
En,m — E (Gn,m).
• G„,p: A random graph on a vertex set [n] where each possible edge occurs indepen­
dently with probability p.
• En^p = E (Gn,p).
• 
G„&: G„,m, conditioned on having a minimum degree at least k.
• G„;„;P: A random bipartite graph with a vertex set consisting of two disjoint copies 
of [n], where each of the n2 possible edges occurs independently with probability 
p.
• G„;+: A random + -regular graph on a vertex set [n].
• 
6n,d: The set of graphs with a vertex set [n] and a degree sequence
d = (d1,d2,...,d„).
• G„.d: A random graph chosen uniformly at random from £„.d.
Probability
• P(A): The probability of event A.
• E Z: The expected value of a random variable Z.
• 
h (Z): The entropy of a random variable Z.
• Po(T): A random variable with the Poisson distribution with mean T.
• N(0,1): A random variable with the normal distribution, mean 0 and variance 1.
• Bin(n, p): A random variable with the binomial distribution with the parameters n 
denoting the number of trials and p denoting the probability of success.
• EXP(T): A random variable with the exponential distribution, mean T i.e., 
P(EXP(T) > x) = e“A1. We sometimes say rate 1/T in place of mean T.
• w.h.p.: A sequence of events &.n, n = 1,2,... is said to occur with high probability 
(w.h.p.) if lim„^TC P(^„) = 1.

Conventions/Notations
xiii
• 
^: We write X’ 
X to say that a random variable X’ converges in distribution
to a random variable X, as ‘ 
<x>. Occasionally we write X’ 
N(0,1) (resp.
X’ 
Po(A)) to mean that X has the corresponding normal (resp. Poisson)
distribution.

Part I
Preliminaries

1 Introduction
1.1 Course Topics
In the past 30 years, random graphs, and more generally, random discrete structures, 
have become the focus of research of large groups of mathematicians, computer scien­
tists, physicists, and social scientists. All these groups contribute to this area differently: 
mathematicians try to develop models and study their properties in a formal way while 
physicists and computer scientists apply those models to study real-life networks and 
systems and, through simulations, to develop interesting and fruitful intuitions as to 
how to bring mathematical models closer to reality. The abrupt development of study in 
the theory and applications of random graphs and networks is in a large part due to the 
Internet and WWW revolution, which exploded at the end of the twentieth century and, 
in consequence, the worldwide popularity of different social media such as Facebook 
or Twitter, just to name the most influential ones.
Our textbook aims to give a gentle introduction to the mathematical foundations of 
random graphs and to build a platform to understand the nature of real-life networks.
Although the application of probabilistic methods to prove deterministic results in 
combinatorics, number theory, and in other areas of mathematics have a quite long his­
tory, dating back to results of Szele and Erdos in the 1940s, the crucial step was taken 
by Erdos and Renyi in their seminal paper titled “On the evolution of random graphs” in 
1960 (see [43]). They studied the basic properties ofa large uniformly chosen random 
graph and studied how it evolves through the process of adding random edges, one by 
one. At roughly the same time, we have the important contribution of Gilbert (see [54]) 
in which he studied binomial random graphs where edges are inserted independently 
with a fixed probability. The interest in random graphs grew significantly in the mid 
1980s ignited by the publication of the book by Bollobas ([21]) and due to the tireless 
efforts of Paul Erdos, one of the titans of twentieth-century mathematics, who was 
promoting probabilistic combinatorics, cooperating with mathematicians all over the 
world and is recognized as a founding father of the whole area. Random graphs, at the 
beginning of the twenty-first century is recognized as a young but quickly maturing 
area of mathematics, with strong connections to computer science and physics. Com­
puter science exploits various ways of applying probabilistic concepts in the analysis 
of algorithms and in the construction of randomized algorithms. A common ground of 
random graphs and physics is particularly visible in the analysis of phase transition phe­
nomena and in percolation. In the past 20 years, one can observe a veritable tsunami of 

4
Introduction
publications dealing with various models of random graphs introduced to analyze very 
large real-world networks: WWW linkage, social, neural, communication, information, 
and transportation networks, as well as a wide range of large-scale systems.
Nowadays, research in random graphs and networks is thriving, and the subject is 
included in the curriculum of many mathematics and computer science departments 
across the world. Our textbook should help readers not only gain mathematical knowl­
edge about the basic results of the theory of random graphs but also allow them to 
better understand how to model and explore real-world networks.
1.2 Course Outline
The text is divided into three parts and presents the basic elements of the theory of 
random graphs and networks.
To help the reader navigate through the text and to be comfortable understanding 
proofs, we have decided to start with describing in the preliminary part (see Chapter 
2) three of the main technical tools used throughout the text. Since, in general, we 
look at the typical properties of large, in terms of the number ' of vertices (nodes) 
of random graphs, in the first section of Chapter 2, we show how to deal with often 
complicated expressions of their numerical characteristics (random variables), in terms 
of their rate of growth or decline as ‘ to. We next turn our attention to bounds 
and asymptotic approximations for factorials and binomials, frequent ingredients of 
the mathematical expressions found in the book. Finally, we finish this introductory, 
purely technical, chapter with basic information about the probabilistic tools needed to 
study tail bounds, i.e., probabilities that a random variable exceeds (or is smaller than) 
some real value. In this context, we introduce and discuss the Markov, Chebyshev, 
and Chernoff-Hoeffding inequalities leaving the introduction of other, more advanced, 
probabilistic tools to the following chapters, where they are applied for the first time.
Part II of the text is devoted to the classic Erdos-Renyi-Gilbert uniform and binomial 
random graphs. In Chapter 3, we formally introduce these models and discuss their 
relationships. We also define and study the basic features of the asymptotic behavior 
of random graphs, i.e., the existence of thresholds for monotone properties.
In Chapter 4, we turn our attention to the process known as the evolution of a 
random graph, exploring how its typical component structure evolves as the number of 
the edges increases one by one. We describe this process in three phases: the subcritical 
phase where a random graph is sparse and is a collection of small tree components and 
components with exactly one cycle; the phase transition, where the giant component, of 
order comparable with the order of random graphs, emerges; the super-critical phase, 
where the giant component “absorbs” smaller ones, and a random graph becomes 
closer and closer to the moment when it gets fully connected.
Vertex degrees, one of the most important features of random graphs, are studied in 
Chapter 5 in two cases: when a random graph is sparse and when it is dense. We study 
not only the expected values of the number of vertices of a given degree but also their 

1.2 Course Outline
5
asymptotic distributions, as well as applications to the notoriously difficult problem of 
graph isomorphism.
Chapter 6 studies the connectivity and $ -connectivity of a random graph, while 
Chapter 7 discusses the existence in a random graph of a fixed small subgraph, whose 
size (the number of vertices) does not depend on the size of the random graph itself, and 
studies the asymptotic distribution of the number of such subgraphs in a random graph.
Large subgraphs are considered in Chapter 8. Here, the thresholds for the existence 
of a perfect matching are established, first for a bipartite random graph, and next, for 
a general random graph. These results are proved using the well-known graph theory 
theorems of Hall and a weakening of the corresponding theorem of Tutte, respectively. 
After this, long paths and cycles in sparse random graphs are studied and the proof of 
the celebrated result discovering the threshold for the existence of the Hamilton cycle 
in a random graph is given. The chapter closes with a short section on the existence of 
isomorphic copies of certain spanning subgraphs of random graphs.
The last chapter, in Part II, Chapter 9, is devoted to the extremes of certain graph 
parameters. We look first at the diameter of random graphs, i.e., the extreme value of 
the shortest distance between a pair of vertices. Next, we look at the size of the largest 
independent set and the related value of the chromatic number of a random graph.
Part III concentrates on generalizations of the Erdos-Renyi-Gilbert models of ran­
dom graphs whose features better reflect some characteristic properties of real-world 
networks such as edge dependence, global sparseness and local clustering, small di­
ameter, and scale-free distribution of the number of vertices of a given degree. In 
the first section of Chapter 10, we consider a generalization of the binomial random 
graph where edge probabilities, although still independent, are different for each pair 
of endpoints, and study conditions for its connectedness. Next, a special case of a 
generalized binomial random graph is introduced, where the edge probability is a 
function of weights of the endpoints. This is known in the literature as the Chung-Lu 
model. Section 12.1 provides information about the volume and uniqueness of the gi­
ant component and the sizes of other components, with respect to the expected degree 
sequence. The final section of Chapter 10 introduces a tool, called the configuration 
model, to generate a close approximation to a random graph with a fixed degree se­
quence. Although promoted by Bollobas, this class of random graphs is often called 
the Molloy-Reed model.
In Chapter 11, the “small-world” phenomenon is discussed. This name bears the 
observation that large real-world networks are connected by relatively short paths 
although being globally sparse, in the sense that the number of edges is a bounded 
multiple of the number of vertices, their nodes/vertices. There are two random graph 
models presented in this chapter: the first due to Watts and Strogatz and the second due 
to Kleinberg illustrate this property. In particular, finding short paths in the Kleinberg 
model is amenable to a particularly simple algorithm.
In general, real-world networks have a dynamic character in terms of the continual 
addition/deletion of vertices/edges and so we are inclined to model them via random 
graph processes. This is the topic of Chapter 12. There we study the properties of 
a wide class of preferential attachment models, which share with real networks the 

6
Introduction
property that their degree sequence exhibits a tail that decays polynomially (power 
law), as opposed to classical random graphs, whose tails decay exponentially. We give 
a detailed analysis and formal description of the so-called Barabasi-Albert model, as 
well its generalization: spatial preferential attachment.
Chapter 13 introduces the reader to the binomial and geometric random intersection 
graphs. Those random graphs are very useful in modeling communities with similar 
preferences and communication systems.
Finally, Chapter 14 is devoted to a different aspect of graph randomness. Namely, 
we start with a graph and equip its edges with random weights. In this chapter, we con­
sider three of the most basic combinatorial optimization problems, namely minimum­
weight spanning trees, shortest paths, and minimum weight matchings in bipartite 
graphs.
Suggestions for Instructors and Self-Study
The textbook material is designed for a one-semester undergraduate/graduate course for 
mathematics and computer science students. The course might also be recommended 
for students of physics, interested in networks and the evolution of large systems as 
well as engineering students, specializing in telecommunication. The book is almost 
self-contained, there being few prerequisites, although a background in elementary 
graph theory and probability will be helpful.
We suggest that instructors start with Chapter 2 and spend the first week with 
students becoming familiar with the basic rules of asymptotic computation, finding 
leading terms in combinatorial expressions, choosing suitable bounds for the binomials, 
etc., as well as probabilistic tools for tail bounds.
The core of the course is Part II, which is devoted to studying the basic properties 
of the classical Erdos-Renyi-Gilbert uniform and binomial random graphs. We 
estimate that it will take between 8 and 10 weeks to cover the material from Part 
II. Our suggestion for the second part of the course is to start with inhomogeneous 
random graphs (Chapter 10), which covers the Chung-Lu and Molloy-Reed models, 
continue with the “small world” (Chapter 11), and conclude with Section 12.1, i.e., 
the basic preferential attachment model. Any remaining time may be spent either on 
one of the two sections on random intersection graphs (Chapter 13), especially for 
those interested in social or information networks, or selected sections of Chapter 14, 
especially for those interested in combinatorial optimization.
To help students develop their skills in asymptotic computations, as well as to give 
a better understanding of the covered topics, each section of the book is concluded 
with simple exercises mainly of a computational nature. We ask the reader to answer 
rather simple questions related to the material presented in a given section. Quite 
often however, in particular in the sections covering more advanced topics, we just ask 
the reader to verify equations, developed through complicated computations, where 
intermediate steps have been deliberately omitted. Finally, each chapter ends with an 

1.2 Course Outline
7
extensive set of problems of a different scale of complication, where more challenging 
problems are accompanied by hints or references to the literature.
Suggestions for Further Readings
A list of possible references for books in the classical theory of random graphs is rather 
short. There are two advanced books in this area, the first one by Bela Bollobas [21] 
and the second by Svante Janson, Tomasz Luczak, and Andrzej Rucinski [66]. Both 
books give a panorama of the most important problems and methods of the theory of 
random graphs. Since the current book is, in large part, a slimmed-down version of our 
earlier book [52], we encourage the reader to consult it for natural extensions of several 
of the topics we discuss here. Someone taking the course based on our textbook may 
find it helpful to refer to a very nice and friendly introduction to the theoretical aspects 
of random networks, in the book by Fan Chung and Linyuan Lu [32]. One may also 
find interesting books by Remco van der Hofstad [60] and Rick Durrett [41], which 
give a deep probabilistic perspective on random graphs and networks.
We may also point to an extensive literature on random networks which studies 
their properties via simulations, simplified heuristic analysis, experiments, and testing. 
Although, in general, those studies lack mathematical accuracy, they can give good 
intuitions and insight that help understand the nature of real-life networks. From the 
publications in this lively area, we would like to recommend to our reader the very 
extensive coverage of its problems and results presented by Mark Newman [95].
Last but not least, we suggest, in particular to someone for whom random graphs 
will become a favorite area of further, deeper study, reading the original paper [43] by 
Paul Erdos and Alfred Renyi on the evolution of random graphs, the seed from which 
the whole area of random graphs grew to what it is today.

2 Basic Tools
Before reading and studying results on random graphs included in the text, one should 
become familiar with the basic rules of asymptotic computation, find leading terms 
in combinatorial expressions, choose suitable bounds for the binomials, and get ac­
quainted with probabilistic tools needed to study tail bounds, i.e., the probability that 
a random variable exceeds (or is smaller than) some real value. This chapter offers the 
reader a short description of these important technical tools used throughout the text. 
For more information about the topic of this chapter, we refer the reader to an excellent 
expository book, titled Asymptotia, written by Joel Spencer with Laura Florescu (see 
[108]).
2.1 Asymptotics
The study of random graphs and networks is mainly of an asymptotic nature. This 
means that we explore the behavior of discrete structures of very large “size,” say '. It 
is quite common to analyze complicated expressions of their numerical characteristics, 
say f (‘), in terms of their rate of growth or decline as ‘ to. The usual way is to 
“approximate” f (‘) with a much simpler function g(‘).
Wesaythat f (‘) is asymptotically equal to g (‘) and write f {‘) ~ g (‘) if f (n)/g {‘)
1 as ‘ to.
Example 2.1 The following functions f (n) and g (‘) are asymptotically equal:
(a) Let f (n) = ('), g(‘) = ‘2/2. Then (') = ‘(‘ - 1)/2 ~ ‘2/2.
(b) Let f (‘) = 3 (') )2, where ) = &/(') .Find & such that f (‘) ~ g (‘) = 2m2. Now, 
t 3 ‘(‘ ~ 1)(‘ - 2^ 
4&2 
2&2
^ 
= 
6 
(‘(‘ - 1))2 ~
so & should be chosen as M^n.
We write f {‘) = O(g(‘)) when there is a positive constant C such that for all 
sufficiently large ‘, | f (n)| < C |g (n)|, or, equivalently, limsup’^^ | f (‘)|/|g (‘)| < <to.

2.1 Asymptotics
9
Similarly, we write f (n) = Q(g(n)) when there is a positive constant c such that for 
all sufficiently large n, | f (n)| > c |g (n)| or, equivalently, liminfn^TC | f (n)|/|g (n)| > 0.
Finally, we write f (n) = 0(g(n)) when there exist positive constants c and C 
such that for all sufficiently large n, c|g(n)| < |f (n)| < C|g(n)| or, equivalently, 
f («) = O(g(n)) and f (n) = Q(g(n)).
Note that f (n) = O(g(n)) simply means that the growth rate of f (n) as n to 
does not exceed the growth rate of g(n), f (n) = Q(g(n)) such that f (n) is growing 
at least as quickly as g(n), while f (n) = 0(g(n)) states that their order of growth is 
identical.
Note also that if f (n) = /i (n) f2(n) + ■ ■ ■ + f$(n), where $ is fixed and for " = 
1,2,...,$, ft (n) = O (g (n)) ,then f (n) = O (g (n)) as well. In fact, the above property 
also holds if we replace O by Q or 0.
Example 2. 2 Let
(a) f (n) = 5n3 - 7 log n + 2n-1^2; then
/ (n) = O (n3),
f (n) = 5n3 + O (log n),
f (n) = 5n3 - 7 log n + O(n-1^2).
(b) f (x) = e 1; then f (x) = 1 + x + x2/2 + O(x3) for x 0.
We now introduce the frequently used “little o” notation.
We write f (n) = o(g(n)) or f (n) « g(n) if f (n)/g(n) 0 as n to. Similarly, we 
write f (n) = m(g(n)) or f (n) » g(n) if f (n)/g(n) 
to as n to. Obviously, if
f (n) « g(n), then we can also write g(n) » f (n).
Note that f (n) = o(g(n)) simply means that g(n) grows faster with n than f (n), 
and the other way around if f (n) = m(g(n)).
Let us also make a few important observations. Obviously, f (n) = o(1) means that 
f (n) itself tends to 0 as n to. Also the notation f (n) ~ g(n) is equivalent to the 
statement that f (n) = (1 + o(1))g(n). One should also note the difference between 
the (1 + o(1)) factor in the expression f (n) = (1 + o(1))g(n) and when it is placed in 
the exponent, i.e., when f (n) = g(n)1+o. In the latter case, this notation means that 
for every fixed s > 0 and sufficiently large n, g(n)1-e < f (n) < g(n)1+s. Hence here 
the (1 + o(1)) factor is more accurate in f (n) = (1 + o(1))g(n) than the much coarser 
factor (1 + o(1)) in f (n) = g(n/1+o .
It is also worth mentioning that, regardless of how small a constant c > 0 is and 
however large a positive constant C is, the following hierarchy of growths holds:
lnc n « nc, nc «(1 + c)', C' « ncn. 
(2.1)

10
Basic Tools
Example 2. 3 Let
(a) f (n) = (')p, where p = p(n). Then f (n) = o(1) if p = 1/n2+£, where e > 0, 
since /(n) ~ 
n~2~s = n~s/2 
0.
(b) f (n) = 3(3)p2, where p = m/(') and m = n1/2/w, where w = w(n) 
to as
n to. Then f (n) = o (1) since f (n) < n4/^2(')2w20.
Exercises
2.1.1 Let f (n) = (')p2 (1 - p)2(n 3, where log n - log log n < np < 2 log n. Then 
show that f (n) = O(n3p2e~2np) = o(1).
2.1.2 Let f (n) = (1 - ')'^g n> , where c is a constant. Then show that f (n) = o(n“2).
2.1.3 Let p = lonn and f (n) = £'=2 n$p$-1 (1 _ p)$(n~k. Then show that f (n) = 
o (1).
2.1.4 Suppose that $ = k(n) = [2 log1/(1_)j n"| and 0 < p < 1 is a constant. Then 
show that (') (1 - p)k*■ k~Vll2 
0.
2.2
Binomials
We start with the famous asymptotic estimate for n!, known as Stirling’s formula.
Lemma 2.4
n! = (1 + o(1))nne~n \'2nn.
Moreover, 
nne-n^2^. < n! < nne 
^2mi e1/12n.
Example 2.5 Consider the coin-tossing experiment where we toss a fair coin 2n 
times. What is the probability that this experiment results in exactly n heads and n 
tails? Let A denote such an event. Then P(A) = (2nn)2“2n.
By Stirling’s approximation,
/2n\ (2n)! (2n)2n e"2nV2^ (2n) 22n 
I I _ ____ -------------------------  -  -----
\n/ (n!)2 
(nn e“n)2 (2^n)
Hence P(A) ~ 1/V^n.
Example 2.6 What is the number of digits in 100!? To answer this question we shall 
use sharp bounds on n! given in Lemma 2.4. Notice that
1 < ----------n— < e1/12n.
nn e W2^n
Hence, taking logarithms,

2.2 Binomials
11
0 < In n! - In + — j In
+ 11n —n< 1-. 
2 
12n
Now, since the number of digits in a positive integer n is [1og10 n + 1_|, we divide both 
sides by In 10, to get
In + 11 Inn - n + — In 2n 
1
0 < log n!---------------------------------- < ------------ .
S10 
In 10 
12n In 10
Substituting n = 100, we obtain
0 < 1og10 100! - 157.96 < 0.00036.
Thus 100! has exactly 158 digits.
Before we move to the analysis of the asymptotic behavior of the binomial coefficient 
($), let us prove some simple, often-used upper and lower bounds, valid for all fixed n 
and k.
Lemma 2.7 For every integer n and k, k < n,
(2.2)
(2.3)
(2.4)
Proof To prove (2.2), note that
n\ _ n! 
_ (n)$
k k!(n - k)! 
k! ’
where
(n)$ = n(n - 1)(n - 2) ■ ■ ■ (n - k + 1) < n$
By Lemma 2.4, k! > (k/e)$ and the first bound holds.
To see that remaining bounds on Q) are true we have to estimate (n)$ more carefully. 
Note first that
n\ _ n$ (n)$ 
k k! n$
(2.5)
"=0
The upper bound in (2.3) follows from the observation that for - = 1,2,..., [k/2_|, 
k \2
2n
k -
1 - l-

12
Basic Tools
The lower bound in (2.3) is implied by the Weierstrass product inequality, which 
states that
]"[(1 - ar) + ^ ar > 1
(2.6)
for 0 < a1,a2,...,a, < 1, and can be easily proved by induction. Hence
"-1 I . \ 
$-1 .
101-;!■-u'
k(k - 1) 
2n
The last bound given in (2.4) immediately follows from the upper bound in (2.3) 
and the simple observation that for every real 1,
1 +1 < e1.
(2.7)
□
Example 2.8 To illustrate an application of (2.2), let us consider the function
f (n,k) = M (1 - 2-$)', 
k
where n, k are positive integers, and denote by n$ the smallest n (as a function of k) 
such that f (n, k) < 1. We aim for an upper estimate of n$ as a function of k, when 
k > 2. In fact, we claim that
n$ < (1 + 3 log2 k ) k22$ In 2. 
(2.8)
k
Now, by (2.2) and (2.7),
f (n,k) = In 1 (1 - 2-$)' < (ne)$ e-"2*. 
kk
If m = (1 + e)k22$ In 2, then
(me \$ e_m/2$ = ^(1 + £)2$k2“(1+" $e ln 2)$. 
(2.9)
k
If e = 3 log2 k/k, then the right-hand side (RHS) of (2.9) equals ((1 + e)k“2e In 2)$, 
which is less than 1. This implies that n$ satisfies (2.8).
In the following chapters, we shall also need the bounds given in the next lemma.
Lemma 2.9 If a > b, then
(k - b \b (n - k - a + b \a b < (’k-b) < / k \b (n - k \ 
n - b 
n - a 
~ (') - n n - b)

2.2 Binomials
13
Proof To see this note that
('_£) 
(n _ a)!k!(n _ k)!
(') 
(k - b)!(n - k - a + b)!n!
k (k - 1) ■ ■ ■ (k - b + 1) 
(n - k)(n - k - 1)---(« - k - a + b + 1)
n(n - !}■■■(' - b + 1) 
(n - b)(n - b - 1) ■ ■■ (n - a + 1)
b , 
\ a-b
k 
n - k
n 
n - b
The lower bound follows similarly.
□
Example 2.10 Let us show that
(T-X+) _ O ((2&)2l-r \
((2)) 
O nv 'r )
m
assuming that 2% - r « m,n.
Applying Lemma 2.9 with n replaced by (') and with k = m, a = b = 21 - r, we 
see that
z(2)-2l+rz 
Z ' 21-r
m-2l+r 
m
- bid
We will also need precise estimates for the binomial coefficient (') when k = k(n). 
They are based on the Stirling approximation of factorials and estimates given in 
Lemma 2.7.
Lemma 2.11 Let k be fixed or grow with n as n 
. Then
/ \ 
$ 
.
"f k = 0tn11}, 
\k 
k!
(2.10)
(n) ~ Tiexp 1“F1 
"fk = 0('n2/3')’
\k 
k! 
2n
(2.11)
~ 
exp 1“F " ^2 | 
"fk = 0(n3/4') ■
\k 
k! 
2n 6n2
(2.12)
Proof The asymptotic formula (2.10) follows directly from (2.3) and (2.4). We only 
prove (2.12) since the proof of (2.11) is analogous. In fact, in the proofs of these bounds 
we use the Taylor expansion of ln(1 - x), 0 < x < 1. In the case of (2.12), we take
ln(1 - x) = —x - ^2 + O(x3). 
(2.13)

14
Basic Tools
Now,
k) - 
{-2n - 632♦O(n4)} ■
Hence 
and equation (2.12) follows.
(2.14)
□
Example 2.1 2 Let n be a positive integer, k = o(n1/2) and m = o(n). Applying 
(2.10) and the bounds from Lemma 2.9 we show that
Example 2.1 3 As an illustration of the application of (2.11) we show that if k = 
k(n) » n2/5, then
0
/ 1 \ $-1 i i v ($)-k+1+k('-$)
$ n 1" n 
= o (1) •
By (2.11) and Stirling’s approximation of k! (Lemma 2.4), we get
H ~ n$e-k2/2n ~,, $2 /™\$ (2^kr1/2. 
k k ! 
k
Moreover, since
^j - k + 1 + k (n - k) = kn - 
+ O (k)
and
11
ln 1 - 
= — + O (n-2),
we have
1 \ (2)-$+1+$('-$) 
C k2 
t
1"n 
= exp -k + 2n+o(1)i•

2.2 Binomials
15
Hence
f (n, k) - e’k2/2' (‘^ 1 $ kk~2(2^k)~1/2n~k+1 e"k+k2/2n 
k
~ nk~5!2(2^)-1/2 = o(1).
Example 2.1 4 Let
f (n,k,%) = ‘C (k.k + %) pk+l (1 - p)( 2)"( k+V)+k ('"k), 
(2.15)
k
where k < n, % = o(k) and np = 1 + £, 0 < £ < 1.
Assuming that f (n, k, %) < n/k and applying (2.14) and (2.13), we look for an 
asymptotic upper bound on C(k, k + %) as follows:
f (n,k,%) = C(k, k + %)pk+l 
exp -+ O
k! 
2n 
6n2
x exp {p - 2 + O(p3)
(np)k+l 
( k2 
k3 
, pk2
= C (k,k + ly 
exp 
-—2 - pkn + —
nl k! 
2n 6n2 
2
- (k + %) + k(n - k)
Recalling that f (n, k, %) < n/k, p = (1 + £)/n and using the Stirling approximation 
for k !, we get
% 1 
£2k 
k3 
£k2
C(k, k + %)< n (k - 1)!exp -£k + 
+—y + k + £k -
2 
6n2 
2n
x exp
.1.1 kk-1 exp (df + y - £^ + O (kJ + £% 
2 
6n2 
2n 
n3
Exercises
(D(rJ) 
©
2.2.1
2.2.2
2.2.3
Let f (n, k) = 2
, where k = k (n)
Show that f (n, k) ~ k4/n2.
c» as n ot, k = o (n1^2).
Let f (n, k) = T‘r1) 
, where k = k(n). Show that f (n, k) ~ e k
( k )
Let f (n, k) = S'=2 X”;o$ k2 (') (”) (k - 1)!j! (')k+ y+1, where c< 1. 
Show that f (n, k) = O(1/n).

16
Basic Tools
2.2.4 Apply (2.2) to show that
n11000 / \ / 
\ I (3k\ \ 2k
f(n,k) = y ' ”) -E2 
= o(1).
6 \k 2k 
(n21J
2.2.5 Prove that nk in Example 2.8 satisfies nk > k22$ In k for sufficiently large k. 
Use equation (2.3) and ln(1 - x) > -if 0 < x < 1 to get a lower bound for 
(1 - 2“k)'. The latter inequality following from In (1 - x) = - 2„=1 1t •
2.3 Tail Bounds
One of the most basic and useful tools in the study of random graphs is tail bounds, 
i.e., upper bounds on the probability that a random variable exceeds a certain real 
value. We first explore the potential of the simple but indeed very powerful Markov 
inequality.
Lemma 2.15 (Markov Inequality) Let X be a non-negative random variable. Then, 
for all t > 0,
x EX
P(X > t) < —.
Proof Let
1
'* = 0
if event A occurs, 
otherwise.
Notice that
X = X'{X>t} + X'{X<t} > X'{X>t} > t'{x>t}■
Hence,
EX > tE'{x>t} = tP(X > t).
□
Example 2.16 Let X be a random variable with the expectation
EX = n((n - 2)/n))m, where m = m(n). Find m such that
P(X > Vn) < e c,
where c > 0 is a constant. By the Markov inequality
So m should be chosen as m = |n(log n1/2 + c).

2.3 Tail Bounds
17
Example 2.17 Let X be a random variable with the expectation
(
M I Jk $"2 p $-1, 
k
where k > 3 is fixed. Find p = p(n) such that P(X > 1) = O (m1 $), 
where m = m (n). Note that by the Markov inequality P(X > 1) < E X; hence
P(X > 1) < (”!k$~2p$-1 < (— 1 $ k$~2p$~1. 
kk
Now put p = 1/(mn$^'k 1) to get
P(X > 1) < / f \ $ $$-2 / 
1 
\
k 
wn$/( $“b
e $
r,../ 1
= O (m1"$).
We are very often concerned with bounds on the upper and lower tail of the 
distribution of 5, i.e., on P(X > EX + t) and P(X < EX - t), respectively. The 
following joint tail bound on the deviation of a random variable from its expectation 
is a simple consequence of Lemma 2.15.
Lemma 2.18 (Chebyshev Inequality) If X is a random variable with a finite mean 
and variance, then, for t > 0,
p(i x - e x । > t) < Va2X.
Proof
P(|X - EXI > t) = P((X - EX)2 > t2) < E(X ~2EX) = Va2X.
□
Example 2.19 Consider a standard coin-tossing experiment where we toss a fair 
coin n times and count, say, the number X of heads. Note that p = EX = n/2, while 
Var X = n/4. So, by the Chebyshev inequality,
P (|X -' |> £n) < cnil *
I 21 
(en)2 
4ne2
Hence,
1
4ne2 ’
so if we choose, for example, e = 1/4, we get the following bound:

18
Basic Tools
Suppose again that X is a random variable and t > 0 is a real number. We focus our 
attention on the observation due to Bernstein [17], which can lead to the derivation 
of stronger bounds on the lower and upper tails of the distribution of the random 
variable X .
Let A > 0 and j = E X; then
P(X > j +t) = P(e1x > e1 (M+t)) < e-1(M+t'> E(e^) 
(2.16)
by the Markov inequality (see Lemma 2.15).
Similarly for A < 0,
P(X < j - t) = P(e1x > e1(m“-)) < e-1('> E(e':'V). 
(2.17)
Combining (2.16) and (2.17) one can obtain a bound for P(|X - j| > t). A bound of 
such type was considered above, that is, the Chebyshev inequality.
We will next discuss in detail tail bounds for the case where a random variable is the 
sum of independent random variables. This is a common case in the theory of random 
graphs. Let
S’ = Xi + X2 + ■ ■ ■ + Xn,
where X", i = 1,.. ,,n are independent random variables.
Assume that 0 < X" < 1 and E X" = j" for i = 1,2, ...,n. Let E Sn = j1 +
j2 + ■ ■ ■ + j’ = j. Then, by (2.16), for A > 0,
n
P(Sn > j + t) < e"1(M+t) n E(e":X"), 
(2.18)
i=1
and, by (2.16), for A < 0,
n
P(S’ < j - t) < e"1(M-t) n E(e^‘)■ 
(2.19)
i=1
In the above bounds we applied the observation that the expected value of the product 
of independent random variables is equal to the product of their expectations. Note 
also that E( e1X") in (2.18) and (2.19), likewise E( e1X) in (2.16) and (2.17), are the 
moment-generating functions of the X" and X, respectively. So finding bounds boils 
down to the estimation of these functions. Now the convexity of e1 and 0 < Xf < 1 
implies that
e^" < 1 - X" + X" e1.
Taking expectations, we get
E(e^) < 1 - j" + j"e1.
Equation (2.18) becomes, for A > 0,
n
P(S’ > j + t) < e’1(^+t) ^(1 - j" + j"e1) 
"=1
n - j + je1
< e-1(M+t)
(2.20)

2.3 Tail Bounds
19
The second inequality follows from the fact that the geometric mean is at most the arith­
metic mean, i.e., (1112 ■ ■ ■ 1n)1/n < (11 +12 + ■ ■ '+1‘)/nfor non-negative 11,12,... ,xn. 
This in turn follows from Jensen’s inequality and the concavity of log 1. The RHS of 
(2.20) attains its minimum, as a function of T, at
Hence, by (2.20) and (2.21), assuming that p +t<n.
e* = (p + t)(n -p). 
(2.21)
(n - p -1) p
/ p \ ^+- 7 n - p \ '~^~t
P(S„ > p +1) < 
, 
(2.22)
p +1 
n - p - t
while for t > n - p this probability is zero.
Now let
U(1) = (1 +1) log(1 +1) -1, 
1 > -1,
~ 1 -n $ 1 $
" S $($ -d for|11"1j
and let u(1) = <x> for 1 < -l.Now, for 0 < t < n - p, we can rewrite the bound (2.22) 
as
Since u (1) > 0 for every 1, we get
P(S„ > p +1)< exp pu 
(n p)U 
.
p 
n - p
P(S„ > p + t)< e"^^(-/^). 
(2.23)
Similarly, putting n - Sn for Sn, or by an analogous argument, using (2.19), we get, for 
0 < t < p,
P(Sn < p - t)< exp -pU —) -(n - p)U— 
.
p 
n - p
Hence,
P(S, < p - t) < e"^("-/^. 
(2.24)
We can simplify expressions (2.23) and (2.24) by observing that
12
U2(177/3)' 
(2'25)
To see this observe that for |11 < 1, we have
U“ 2(1 +1/3) “ ^2($($ “ 1) ~ 2 ■ 3$-2)1 ’
Equation (2.25) for |11 < 1 follows from $ $1_1^ - 2 
> 0 for $ > 2. We leave it as
Exercise 2.3.3 to check that (2.25) remains true for 1 > 1.
Taking this into account we arrive at the following theorem, see Hoeffding [59].

20
Basic Tools
Theorem 2.20 (Chernoff-Hoeffding inequality) Suppose that 
Sn = X1 + X2 + ■ ■ ■ + Xn while, for i = 1,2,..., n,
(i) 0 < X" < 1,
(ii) X1? X2,..., Xn are independent.
Let E X" = p" and p = p1 + p2 + ■ ■ ■ + pn. Then for t > 0,
( 
t 2
P (S’ > p + t) < exp ~ 2( p +1/3)
and for t < p,
f 
t2
P(S’ < p - —) < exp -------—r-
2(p - t/3)
(2.26)
(2.27)
Putting t = sp, for 0 < s < 1, in (2.23), (2.26) and (2.27), one can immediately 
obtain the following bounds.
Corollary 2.21 Let 0 < s < 1; then
P(S’ > (1 + S)p) < | 
while
nsn < (1 -
i e s \u 
us2
< exp -p3-J, 
(2.28)
}■ pr} • 
(2.29)
H1 + s)1+£
s')p) < exp
□
Note also that the bounds (2.28) and (2.29) imply that, for 0 < s < 1,
P(|S’ - p| > sp) < 2exp
(2.30)
Example 2.22 Let us return to the coin-tossing experiment from Example 2.19. 
Notice that the number of heads X is in fact the sum of binary random variables X", for 
i = 1,2,..., n, each representing the result of a single experiment, that is, X" = 1, with 
probability 1/2, when head occurs in the ith experiment, and X" = 0, with probability 
1/2, otherwise. Denote this sum by Sn = X1 + X2 + ■ ■ ■ + Xn and notice that random 
variables X" are independent. Applying the Chernoff bound (2.30), we get
P (|Sn - nl - s|) 
2 exp

2.3 Tail Bounds
21
Choosing e = 1/2, we get
S‘
> 
< 2e-'/24
" 4 < 2e ,
P
1
2
a huge improvement over the Chebyshev bound.
Example 2.23 Let Sn now denote the number of heads minus the number of tails 
after n flips of a fair coin. Find P(Sn > m^n), where m = m(n) to arbitrarily slowly, 
as n to.
Notice that Sn is again the sum of independent random variables Xt, but now Xt = 1, 
with probability 1/2, when head occurs in the "th experiment, while Xf = -1, with 
probability 1/2, when tail occurs. Hence, for each i = 1,2,..., n, expectation EXf = 0 
and variance Var Xf = 1. Therefore, ^ = E Sn = 0 and a2 = Var Sn = n. So, by (2.26)
P(S„ > wVn) < e-3‘ '.
To compare, notice that Chebyshev’s inequality yields the much weaker bound since 
it implies that
P(Sn > m^n) 
-1-y.
2m2
One can “tailor” the Chernoff bounds with respect to specific needs. For example, 
for small ratios -/p, the exponent in (2.26) is close to -2/2p, and the following bound 
holds.
Corollary 2.24
P(S„ > p + -) < exp |(2.31) 
2p 
6^2
[ -2 1
< exp - 
for - < p. 
(2.32)
3p
Proof Use (2.26) and note that
(p + -/3)-1 > (p - -/3)/Zz2.
□
Example 2.25 Suppose that p = c/n for some constant c and that we create an n x n 
matrix A with values 0 or 1, where for all i, j, Pr(A(i,j) = 1) = p independently of 
other matrix entries. Let Z denote the number of columns that are all zero. We will 
show that, for small e > 0,
Pr(Z > (1 + e)e"cn) < e"e2e^cn/3.

22
Basic Tools
Each column of A is zero with probability q = (1 - p)n = (1 + O (1/n))e~c. Further­
more, Z is the sum of indicator random variables and is distributed as the binomial 
Bin(n, q). Applying (2.31) with p = nq, - = sp, we get
c x , 
( s2p 
s3p) , 
( s2e_
Pr(Z > (1 + e) e c n) < exp —2 + “^ 
exp------3
For large deviations we have the following result.
Corollary 2.26 If c > 1, then
P(Sn > cp) <
(2.33)
Proof Put t = (c - 1)p into (2.23).
□
Example 2.27 Let X1, X2,..., Xn be independent binary random variables, that is, 
Xi e {0,1} with the Bernoulli distribution: P(X" = 1) = p, P(X" = 0) = 1 - p, for 
every 1 < i < n, where 0 < p < 1. Then Sn = X" has the binomial distribution 
with the expectation E Sn = p = np. Applying Corollary 2.26 one can easily show that 
for t = 2ep,
P(Sn > t) < 2~—.
Indeed, for c = 2e,
P(Sn > t) = P(Sn > cp) <
e \CM / 
1 
\2S^
ce1/C 
2e1/<-2e')
We also have the following:
Corollary 2.28 Suppose that X1,X2,... , Xn are independent random variables and
that a" < X" < b" for i = 1,2,..., n. Let Sln — X1 + X2 + ■ ■ ■ + Xn and p" = E( X"),
i = 1,2,..., n and p = E (Sn). Then for t > 0 and c" = b" - a", i = 1,2,.. ., n, we have
P(Sn > p +1) < exp
2 
2t2 
]
(2.34)
1 c2 + c2 + ■ ■ ■ + c' 1
P (Sn < p - t) < exp
2 
2t2 
]
(2.35)
1 c2 + c2 + ■ ■ ■ + c' 1
Proof We can assume without loss of generality that a" = 0, i = 1,2,...,n. We just 
subtract A = X'=1 a" from Sn. We proceed as before.
n
P(Sn > p + t) = P (e25" > e2(M+t< e"2(M+t} E (e25") = e"2t 
E (e2(x‘~Ml.
"=1

2.3 Tail Bounds
23
Note that e41 is a convex function of 1, and since 0 < X" < C", we have
e4(Xj-»t) < e-art 1 ^i_ + Xd_gaCi 
Ci 
Ci
and so
E(e4(x"~*>) < e"A« 1 - P" + P"e4c" 
Ci 
Ci
= e-0")" (1 - Pi + Pied") , 
(2.36)
where 0{ = Ac" and p" = p"/c".
Then, taking the logarithm of the RHS of (2.36), we have
f (0") = -0"p" + log (1 - p" + p"e9" j ,
f \0i) = -P" + 
)"e_9" 
d.,
1 - p" + p"e°‘
f ”(ft) = 
p" (1 ~ p" )e~g"
f l!’ 
((1 - p")e-* + p")2 '
Now (J^ 
1/4 and so f ”{0") < 1/4, and therefore
f (0")< f (0)+ f '(0)0" + 102 = 
.
8 
8
It follows then that
P(Sn > p + t) < e~At exp j y ^w" (• 
"Et 8
We obtain (2.34) by putting A = 
4 2, and (2.35) is proved in a similar manner. □
r,.=1 C"
There are many cases when we want to use our inequalities to bound the upper tail 
of some random variable Y and (i) Y does not satisfy the necessary conditions to apply 
the relevant inequality, but (ii) Y is dominated by some random variable X that does.
We say that a random variable X stochastically dominates a random variable Y and 
write X > Y if
P(X > t)> P(Y > -) for all real t. 
(2.37)
Clearly, we can use X as a surrogate for Y if (2.37) holds.
The following case arises quite often. Suppose that Y = Y1 + Y2 + ■ ■ ■ + Yn, where 
Y1, Y2,..., Yn are not independent, but instead we have that for all t in the range [A", B"] 
of Y",
P(Y" > t | Y1,Y2,...,Y"_1) < ^(t),
where ^(t) decreases monotonically from 1 to 0 in [A", B"].

24
Basic Tools
Let X" be a random variable taking values in the same range as Y" and such that 
P(X" > t) = ^(t). Let X = X1 + ■ ■ ■ + Xn, where X1,X2,...,Xn are independent of 
each other and Y1,Y2,... ,Yn. Then we have
Lemma 2.29 X stochastically dominates Y.
Proof Let X <"’) = X1 + • • • + X" and Y <"’) = Y1 + • • • + Y" for i = 1,2,...,n. We will 
show by induction that XW dominates YW for i = 1,2,..., n. This is trivially true for 
i = 1, and for i > 1 we have
P (Y ("’) > t | Y1,..., Y"_1) = P(Y" > t - (Y1 + ■■■ + Y"_1) | Y1,..., Y"_1)
< P( X" > t - (Y1 + ■■■ + Y"_1) | Y1,..., Y"_1).
Removing the conditioning, we have
P(Y& > f) < P(Y(""^ > t - X") < P(X(""^ > t - X") = P(X("> > f), 
where the second inequality follows by induction. 
□
Exercises
2.3.1. Suppose we roll a fair die n times. Show that w.h.p. the number of odd outcomes 
is within O (n1/2 log n) of the number of even outcomes.
2.3.2. Consider the outcome of tossing a fair coin n times. Represent this by a (random) 
string of H’s and T’s. Show that w.h.p. there are ~ n/8 occurrences of HTH as 
a contiguous substring.
2.3.3. Check that (2.25) remains true for i > 1.
(Hint: differentiate both sides, twice.)
Problems for Chapter 2
2.1 Show that if k = o(n), then
M ~ (Y) '2^k)’1/2 exp y (1 + o(1))|. 
k k 
2n
2.2 Let c be a constant, 0 < c < 1, and let k ~ cn. Show that for such k,
/n = 2'(H (c)+o(1)) 
\k 
’
where H is an entropy function: H(c) = -c In c - (1 - c) ln(1 - c).
2.3 Prove the following strengthening of (2.2),

2.3 Tail Bounds
25
2.4
2.5
2.6
2.7
2.8
2.9
Let f (n) = S'=11 n$=01 (1 - '). Prove that f (n) - 1 log n.
Suppose that m = cn distinguishable balls are thrown randomly into n boxes. 
(i) Write down an expression for the expected number of boxes that contain k or 
more balls. (ii) Show that your expression tends to zero if k = /log n"|.
Suppose that m = cn distinguishable balls are thrown randomly into n boxes. 
Suppose that box i contains b" balls. (i) Write down an expression for the expected 
number of k-sequences such that b" = bi+1 = ■ ■ ■ = bi+$_1 = 0. (ii) Show that 
your expression tends to zero if k = /logn\.
Suppose that we toss a fair coin. Estimate the probability that we have to make 
(2 + £)n tosses before we see n heads.
Let X1, X2,..., Xn be independent binary random variables, X" e {0,1}, and let 
P(X" = 1) = p for every 1 < i < n, where 0 < p < 1. Let Sn = ' £”=1 X". 
Apply the Chernoff-Hoeffding bounds to show that if n > (3/-2) ln(2/S), then 
P(|5„ - p)| < -) > 1 - S.
Let Y1,Y2,... ,Ym be independent non-negative integer random variables. Sup­
pose that for + > 1 we have Pr(Y+ > k) < Cp$, where p < 1.Let p = C/{1 - p). 
Show that if Y = Y1 + Y2 + ■ ■ ■ + Ym, then
Pr(Y > (1 + £)pm) < e~Bs m
2.10
2.11
for 0 < £ < 1 and some B = B(C,p).
We say that a sequence of random variables A0, A1,... is (q, N)-bounded if 
A" - q < Af+1 < A" + N for all i > 0.
(i) Suppose that q < N/2 and a < qm. Prove that if 0 = A0, A1,... is an 
(q, N)-bounded sequence, then Pr(Am < -a) < exp 3^&N |.
(ii) Suppose that q < N/10 and a < qm. Prove that if 0 = A0, A1,... is an 
(q, N)-bounded sequence, then Pr(Am > a) < exp 
3^2N|.
Let A be an n x m matrix, with each a"# e {0,1}, and let b be an m-dimensional 
vector, with each b$ e {-1,1}, where each possibility is chosen with probability 
1/2 . Let c be the n-dimensional vector that denotes the product of A and 
b. Applying the Chernoff-Hoeffding bound show that the following inequality 
holds for i e {1,2,..., n}:
P(max{|c"|} > V4mInn) < O(n-1).

Part II
Erdos-Renyi-Gilbert Model

3 Uniform and Binomial Random 
Graphs
There are two classic ways to generate a random graph. The first, introduced by Erdos 
and Renyi, involves sampling, uniformly at random, a single graph from the family of 
all labeled graphs on the vertex set [‘] with & edges. This is equivalent to the insertion 
of & randomly chosen edges into an empty graph on ' vertices. Each choice of & places 
among the (') possibilities is equally likely. Gilbert suggested an alternative approach, 
where each edge is inserted into an empty graph on ' vertices, independently and with 
the same probability ). One may immediately notice the main difference between those 
two approaches: the first one has a fixed number of edges &, while the number of edges 
in the second one is not fixed but random! Regardless of this fundamental difference, it 
appears that those two models are, in the probabilistic and asymptotic sense, equivalent 
when the number of edges & in the uniform model is approximately equal to the 
expected number of edges in the latter one, i.e., in such circumstances both models are 
almost indistinguishable. This is the reason why we think about them as a single, unified 
Erdos-Renyi-Gilbert model. In this chapter we formally introduce both Erdos-Renyi 
and Gilbert models, study their relationships and establish conditions for their asymp­
totic equivalence. We also define and study the basic features of the asymptotic behavior 
of random graphs, i.e., the existence of thresholds for monotone graph properties.
3.1 Models and Relationships
The study of random graphs in their own right began in earnest with the seminal paper 
of Erdos and Renyi [43]. This paper was the first to exhibit the threshold phenomena 
that characterize the subject.
Let @‘& be the family of all labeled graphs with vertex set V = [‘] = {1,2,..., ‘} 
and exactly & edges, 0 < & < (') .To every graph G e £‘,&, we assign a probability
P(G) = ^2 
.
&
Equivalently, we start with an empty graph on the set [‘] and insert & edges in 
such a way that all possible ((&)) choices are equally likely. We denote such a random 

30
Uniform and Binomial Random Graphs
graph by G„,& = (['],£‘,&) and call it a uniform random graph. We now describe 
a similar model. Fix 0 < ) = )(') < 1. Then for 0 < & < ('), assign to each graph 
G with vertex set [‘] and & edges a probability
P(G) = ) & (1 - ))(')-&.
Equivalently, we start with an empty graph with vertex set [‘] and perform (') 
Bernoulli experiments inserting edges independently with probability ) . We call such 
a random graph a binomial random graph and denote it by G‘;) = ([‘], E‘;)). This 
model was introduced by Gilbert [54]. As one may expect, there is a close relationship 
between these two models of random graphs. We start with a simple observation.
Lemma 3.1 The random graph G‘;), given that its number of edges is &, is equally 
likely to be one of the (®) graphs that have & edges.
Proof Let G0 be any labeled graph with & edges. Then since
{G’,) = Go}c {|E’^)| = &},
we have
P(G’,) = Go | |E',p | = m) = P(G‘;) = Go, |E',p | = &)
P(| E',p | = &)
P(G‘;) = G o)
P(|E‘;) | = &)
)&(1 - ))(')"&
((&}))&(1 - ))®’&
[(2)\1.
&
□
Thus G‘;) conditioned on the event {G‘;) has & edges} is equal in distribution to 
G‘;&, the graph chosen uniformly at random from all graphs with & edges.
Obviously, the main difference between those two models of random graphs is that 
in G‘;& we choose its number of edges, while in the case of G‘;) the number of edges 
is the binomial random variable with the parameters (') and ). Intuitively, for large ‘ 
random graphs G‘;& and G‘;) should behave in a similar fashion when the number of 
edges & in G‘;& equals or is “close” to the expected number of edges of G‘;), i.e., when
(3.1)
or, equivalently, when the edge probability in G‘;)
2&
) ~ T ■
(3.2)
We next introduce a useful “coupling technique” that generates the random graph

3.1 Models and Relationships
31
G„,P in two independent steps. We will then describe a similar idea in relation to G„,m. 
Let Gn>P be a union of two independent random graphs G„,P1 and Gn>P2, i.e.,
, ,p = Gn,pi U G„;p2.
Suppose that p1 < p and p2 is defined by the equation
1 - p = (1 - pi)(1 - p2), 
(3.3)
or, equivalently,
p = p1 + p2 - p1 p2-
Thus an edge is not included in Gn>P if it is not included in either of Gn>P1 or Gn>P2. 
So when we write
G„;p1 c G„;p,
we mean that the two graphs are coupled so that Gn>P is obtained from G„,P1 by 
superimposing it with Gn>P2 and replacing any double edges by a single one.
We can also couple random graphs G„,m1 and Gn&2, where m2 > m1, via
G„,m2 = Gn& U H.
Here H is the random graph on vertex set [n] that has m2 - m1 edges chosen uniformly 
at random from () \ Enm1.
Consider now a graph property P defined as a subset of the set of all labeled graphs 
on vertex set [n], i.e., P Q 2("). For example, all connected graphs (on n vertices), 
graphs with a Hamiltonian cycle, graphs containing a given subgraph, planar graphs, 
and graphs with a vertex of given degree form a specific “graph property.”
We will state below two simple observations which show a general relationship 
between Gn>m and Gn>P in the context of the probabilities of having a given graph 
property P. The constant 10 in the next lemma is not best possible, but in the context 
of the usage of the lemma, any constant will suffice.
Lemma 3.2 Let P be any graph property and p = m/N, N = ('), where 
m = m(n) 
to, N - m to. Then, for large n,
P(G„,& e P) < 10m1/2 P(Gn,p € P).
Proof By the law of total probability,
N
P(Gn,p eP) = y P(Gn,p 6 P I IEn,p | = k) P(|En,p | = $)
N
= £ P(Gn,$ eP) P(|En,p|= k) 
(3.4)
$=0
> P(Gn,m t?) P(l E n,p | = m) .
To justify (3.4), we write
P(Gn,p ^P\\En,p | = $) = P(Gn,p ^P A |En,p | = k) 
P(lEn,p I = $)

32
Uniform and Binomial Random Graphs
p $ (1 - p)N ~$
g?? (N)P$ d - P)N-$ 
|£ (G)|=$
= Z tNj
G 
$
|£ (G)|=$
= P(G„,$ €P).
Next recall that the number of edges |E„,p | of a random graph G„,p is a random 
variable with the binomial distribution with parameters (') and p. Applying Stirling’s 
formula (see Lemma 2.4) for the factorials in (&), we get
P(|E„,p| = m) = N p&(1 -p)©-m 
m
NN^2^Np& (1 - p)N-&
= (1 + o (1))------------------------------
m&(N - m)N & 2^ym(N - m)
= (1 + o(1)) A 5 
7G 7■
2nm(N - m)
Hence
P(| En^p | = m)> 
1 
,
10\m
and
',.,,.. e 9} < 10m1/2P(G„,p 6 P}.
(3.5)
□
We call a graph property P monotone increasing if G eP implies G + e & P, i.e., 
adding an edge e to a graph G does not destroy the property.
A monotone increasing property is nontrivial if the empty graph Kn <£ P and the 
complete graph Kn eP.
A graph property is monotone decreasing if G e P implies G - e e P, i.e., removing 
an edge from a graph does not destroy the property.
For example, connectivity and Hamiltonicity are monotone increasing properties, 
while the properties of a graph not being connected or being planar are examples of 
monotone decreasing graph properties. Obviously, a graph property P is monotone 
increasing if and only if its complement is monotone decreasing. Clearly, not all graph 
properties are monotone. For example, having at least half of the vertices having a 
given fixed degree d is not monotone.
From the coupling argument it follows that if P is a monotone increasing property, 
then, whenever p < p’ or m < m',
P(Gn,p e P) < P(G„,p' e P) 
(3.6)

3.1 Models and Relationships
33
and
',.,,.. eP)< P(Gn,m' 6 Pf 
(3.7)
respectively.
For monotone increasing graph properties we can get a much better upper bound 
on P(Gn>m e P), in terms of P(G„;) e P), than that given by Lemma 3.2.
Lemma 3.3 Let P be a monotone increasing graph property and p = m/N, N = ('). 
Then, for large n and p = o(1) such that Np,N(1 - p)/(Np) 1^2 
,
n®n,m 
3 P(G‘,p e?).
Proof Suppose P is monotone increasing and p = m/N. Then
N
P(Gn>P € P) = g P(G„,$ 6 P) P(|En,P| = k)
N
> 2 ' '-$ 
pdE'’P I = k)•
$=m
However, by the coupling property we know that for k > m,
P(®n,$ e P) > P(G„;m e P).
The number of edges |En>) | in G„;) has the binomial distribution with parameters 
N, p. Hence
N
P (G„>p EP)> P(G„,m € P) £ P(| E‘,P I = k)
$=m
N
= P(G‘,m € P) £ U$, 
(3.8)
k=m 
where
. k=(N1 p$ a - p>N ~k.
k
Now, using Stirling’s approximation (2.4),
Um * <1 + O <D> mm NN P. ^^2
Furthermore, if k = m + - where 0 < - < m1^2, then
u $+1 _ 
(N - k) p 
_ 1 ~ N^m > f - 
_ - + 11
u $ 
(k + 1)(1 - p) 
1 + 
P| N - m - - m J’
after using the following bounds:

34
Uniform and Binomial Random Graphs
1 +1 < e1 for every x,
1 - x > e 1/^1 1) for 0 < x < 1
(3.9)
(3.10)
to obtain the inequality and our assumptions on N, p.
It follows that for 0 < - < m1/2,
1+o(1) 
f -^ / s _s+1 
exp {-2& 
((1)}
m+- 
(2xm)12'2 
P|^ “0 N - m -s m yj 
(2xm)12'2
where we have used the fact that m = o (N).
It follows that
and the lemma follows from (3.8). 
□
Lemmas 3.2 and 3.3 are surprisingly applicable. In fact, since the G„.) model 
is computationally easier to handle than G„.m, we will repeatedly use both lemmas 
to show that P(G„.) e P} 0 implies that P(G„,m e P} 0 when n to. In 
other situations we can use a stronger and more widely applicable result. The theorem 
below, which we state without proof, gives precise conditions for the asymptotic 
equivalence of random graphs G„.) and G„.m. It is due to Luczak [79].
Theorem 3.4 Let 0 < p0 < 1, s(n) = n^p(1 - p) 
to, and m(n) 
to arbitrarily
slowly as n to .
(i) Suppose that P is a graph property such that P(G„,m e 
p0 for all
2 p - Ws (n), 2 p + m (n)s(n)
Then P(G„.) e P) 
p0 as n 
to.
(ii) Let p- = p - m (n) s (n)/n2 and p+ = p + m {n) s (n)/n2 .Supposethat P isamono- 
tone graph property such that P(G„;)_ e p0 and P(G„;)+ e p0. 
Then P(G„,m e P) 
p0 as n 
to, where m = [(')pj.
Exercises
3.1.1. Compute the expected number of triangles in G„.) and Gn,m and show when 
these parameters are asymptotically equal. Compute also the variance of both 
random variables.

3.2 Thresholds
35
3.1.2. Compute the expected number of copies of 4 (the complete graph on four 
vertices) and G„,& and show when those parameters are asymptotically equal.
3.1.3. Consider a graph property P defined as a subset of the set of all graphs on ‘ 
vertices. Are the following graph properties monotone increasing, monotone 
decreasing or nonmonotone?
P = 
{G : s.t. G contains an isolated vertex}
P = 
{G : s.t. G contains a subgraph H}
P = 
{G : s.t. G contains an induced subgraph H}
P = 
{G : s.t. G has a perfect matching}
P = {G : s.t. the largest component of G is a tree}
P = {G : s.t. all vertex degrees are at most A}
P = {G : s.t. G has a chromatic number equal to 3}
P = {G : s.t. G has at least $ vertices of given degree}
P = {G : s.t. G is nonplanar}
3.1.4. Construct a few of your own examples of monotone and nonmonotone 
properties.
3.1.5. Prove that graph property P is increasing if and only if its complement Pc is 
decreasing.
3.1.6. Prove (3.9) and (3.10). 
________
3.1.7. Provethat (N))$ (1-)N~k = (1+((1))^2/:.(N _$^whereN = (') and) = N.
3.2 Thresholds
One of the most striking observations regarding the asymptotic properties of random 
graphs is the “abrupt” nature of the appearance and disappearance of certain graph 
properties. To be more precise in the description of this phenomenon, let us introduce 
threshold functions (or just thresholds) for monotone graph properties. We start by 
giving the formal definition of a threshold for a monotone increasing graph property P.
Definition 3.5 A function &* = &*(‘) is a threshold for a monotone increasing 
property P in the random graph G„,& if
lim P(G„,& 6 P) =
'^<X
if &/&* 
0,
if &/&* 
to,
as ‘ 
to.
A similar definition applies to the edge probability ) = ) (‘) in a random graph G„,).
Definition 3.6 A function )* = )*(') is a threshold for a monotone increasing 
property P in the random graph G„,) if
lim P (G„,p eP) =
if )/)* 
0,
if )/)* 
to,
as ‘ 
to.

36
Uniform and Binomial Random Graphs
It is easy to see how to define thresholds for monotone decreasing graph properties, 
and therefore we will leave this to the reader.
Notice also that the thresholds defined above are not unique since any function 
which differs from &* (‘) (resp. )* (‘)) by a constant factor is also a threshold for P.
We will illustrate thresholds in a series of examples dealing with very simple graph 
properties. Our goal at the moment is to demonstrate some basic techniques to deter­
mine thresholds rather than to “discover” some “striking” facts about random graphs.
A standard way to show the first part of the threshold statement, i.e., that the proba­
bility that Gn& (resp. G',)) has property P tends to zero when & « &* (resp.) « )*) 
as ‘ to is an application of the First Moment Method, which stems directly from the 
Markov inequality (see Lemma 2.15). Putting - = 1 in the Markov inequality we get:
First Moment Method If X is a non-negative integer-valued random variable, then
P(X > 1) < EX.
(3.11)
We start with the random graph G‘;) and the following properties:
P1 = {all nonempty (nonedgeless) labeled graphs on ‘ vertices}, and
P2 = {all labeled graphs on ‘ vertices containing at least one triangle}.
Obviously, both graph properties are monotone increasing, and our goal will be to 
find thresholds for both of them.
Theorem 3.7 P(G„,) € Pi) 
0 if) « 1/’2, while P(G„,) e p2) 
0 if
) « 1/’,as ‘ c».
Proof Let X be a random variable counting the number of edges in G„,). Then
P(G„,) &P1) = P(G‘;) has at least one edge) = P(X > 0).
Since X has the binomial distribution,
E X = K)
2
and, by the First Moment Method,

3.2 Thresholds
37
n2
P(X > 0) < — p 
0
as n 
to, when p « n
Similarly, let Z be the number of triangles in G„,p. Then
E Z = p3.
3
(3.12)
We can see this as follows. Let T1, T2,..., T^ be an enumeration of the triangles of 
the complete graph Kn. Also, let Zf be the indicator for Gn,p to contain the triangle
T". Then we have E Zf = p3 for all i and Z = Z1 + ■ ■ ■ + Zand (3.12) follows. Now, 
from (3.12) we get that E Z 0 as n to, if p « n-1. So the second statement also 
follows by the First Moment Method.
□
On the other hand, if we want to show that P(X > 0) 1 ( resp. P(Z > 0) 1) 
as n to, then we cannot use the First Moment Method and we should apply the 
Second Moment Method, which is a simple consequence of the Chebyshev inequality.
Second Moment Method If X is a non-negative integer-valued random variable, 
then
r( X 21)21 -<sXX • 
<3-13)
Proof Set - = E X in the Chebyshev inequality. Then
Var X
P(X = 0) < P(|X - EX| > EX) < ^^^Xfp ■
□
(Strong) Second Moment Method If X is a non-negative integer-valued random 
variable, then
P(X > 1) > ^H22. 
(3.14)
Proof Notice that
X = X • I{x >1}.
Then, by the Cauchy-Schwarz inequality,
(EX)2 = (E(X ■ I{X>1}))2 < E12Xa} EX2 = P(X > 1) EX2.
□
Let us complete our discussion about thresholds for properties P1 and ^2.

38
Uniform and Binomial Random Graphs
Theorem 3.8 
P(G„,p e ^1) 
1 if p » 1/n2, while P(G„,p e Pf) 
1 if p » 1/n
as n 
m.
Proof Recall that the random variable X denotes the number of edges in the random 
graph Gn,p and has the binomial distribution. Therefore
(
n2 p (1 - p) = (1 - p) E X.
By the Second Moment Method, P(X > 1) 
1 as n to whenever
Var X/(E X)2 
0 as n 
to. Now, if p » n“2, then E X 
to, and therefore
Var X _ 1 - p 0
(EX)2 " EX
as n to, which shows that indeed P(G„,p e P1) 
1 if p » 1/n2 as n 
to.
To show that if np to then P(G„;P contains at least one triangle) as n to 
needs a bit more work.
Assume first that np = m < log n, where m = m(n) 
to. Let Z, as before, denote
the number of triangles in G„;P. Then
Ez = ('p3 - (1_ 0(1))T’ 
“■
\3 
6
We remind the reader that simply having E Z to is not sufficient to prove that 
P(Z > 0) 
1.
Next let T1,T2,... ,TM ,M = (') be an enumeration of the triangles of Kn. Then
M
E Z2 = ^ p(T",T# 6 G„,p)
i,j=1MM
= £ P(T- € G„,p) 2 P(T# € G„,p I T- e G„,p) 
(3.15)
"=1 
J=1
M
= M P(T1 6 G„,p) ^ p(Ti e G„,p | T1 6 G„,p) 
(3.16)
j=1
M
= E Z X y P(T,■ e G„,p | T1 e G„,p).
j=1
Here (3.16) follows from (3.15) by symmetry.
Now suppose that T#, T1 share a# edges. Then
M
X P(T; 6 G„,p I T1 6 G„,p)
j=1
= X P(Ti e G„,p I T1 6 G„,p)
j: ^j =3

3.2 Thresholds
39
+ £ P(T# e G-,p | Ti e G-,p) 
J"#=1
+ ^ P(Ti e G-,p | Ti 6 G-,p) 
L°-#=«
= 1 + 3(n - 3)p2 + 11 ^ I - 3n + 81 p3
1 + — + E Z.
It follows that
Var Z < (E Z) 1 +-----+ E Z - (E Z)2 < 2 E Z.
n
Applying the Second Moment Method we get
P(Z = 0)- = 0(1) ■
Z
This proves the statement for p < l0--. For larger p we can use (3.6). 
□
Summarizing the results of both examples we see that p* = n“2 is the threshold for 
the property that a random graph Gnp contains at least one edge (is nonempty), while 
p* = n~1 is the threshold for the property that it contains at least one triangle (is not 
triangle free).
Consider the monotone decreasing graph property that a graph contains an isolated 
vertex, i.e., a vertex of degree zero:
P - {all labeled graphs on n vertices containing isolated vertices}.
We will show that m* = 2n log n is a threshold function for the above property P in 
G-,m.
Theorem 3.9 Let P be the property that a graph on n vertices contains at least one 
isolated vertex and let m = 1 n(log n + w(n)). Then
lim P(G-,m € P) = 
n
if w(n) 
if w(n)
1
0
To see that the second statement holds we use the First Moment Method. Namely, 
let X0 = Xnfi be the number of isolated vertices in the random graph Gn,m. Then X0 
can be represented as the sum of indicator random variables
Xo = ^ lv,
/ eV
where
1
' / = 0
if / is an isolated vertex in Gn>m, 
otherwise.

40
Uniform and Binomial Random Graphs
So
using (2.5) and assuming that m = o(log n), while for the product we use (2.6). 
Hence, by (3.9),
«- 2 m
EXo < n '----- < ne~2m/n = e~‘
for m = 1 n(log n + m(n)).
So E Xo 
0 when m (n) 
to as n to, and the First Moment Method implies
that Xo = 0 with probability tending to 1 as n to.
To show that the first statement holds in the case when m -to we first observe 
from (3.17) that in this case
I n- 2 \m
E Xo = (1 - o(1))n I------- I
> (1 - o(1))n exp
2m 
n - 2
> (1 - o(1))e" to.
(3.18)
The second inequality in the above comes from basic inequality (3.10), and we have 
once again assumed that m = o (log n) to justify the first equation.
We caution the reader that as before, E Xo to does not prove that P(Xo > 0) 1 
as n 
to. In Chapter 7 we will see an example of a random variable XH, where
E XH 
to and yet P(XH = o) 
1 as n to.
Notice that
I X 2
E Xo2 = El ^ IJ = X E( Iu Iv}
= ^ p(I. = 1’1/ = 1)
u,v eV
= ^ P(Iu = 1,IV = 1)+ 2 p(Iu = 1,Iv = 1) 
u+v 
u=v
(O’)2) )
= n (n - 1)i-^- + E Xo
((2 } ) 
m

3.2 Thresholds
41
= (1 + ((1))(E Xo)2 + E Xo.
The last equation follows from (3.17).
Hence, by the (strong) Second Moment Method,
P(Xo > 1) > (E Xo)2 
E Xo2
(E Xo)2
(1 + ((1))(E Xo)2 + E Xo 
1
(1 + ((1)) + (E Xo)’1
= 1 - ((1)
on using (3.18). Hence P(Xo > 1) 1 when w(‘) 
-<x> as ‘ to, and so we can
conclude that & = &(‘) is the threshold for the property that G„,& contains isolated 
vertices. 
□
Note that the above result indicates that now the threshold &* is more “sensitive” 
than that considered in Theorem 3.8, since the “switch” from probability one to 
probability zero appears if either &/&* < 1 - s or &/&* > 1 + s. We will see later 
other situations where we can observe that for some monotone graph properties such 
more “sensitive” thresholds hold.
For this simple random variable Xo, we worked with G„,&. We will in general work 
with the more congenial independent model G‘;) and translate the results to G‘& if 
so desired.
A large body of the theory of random graphs is concerned with the search for 
thresholds for various properties, such as containing a path or cycle of a given length, 
or, in general, a copy of a given graph, or being connected or Hamiltonian, to name just 
a few. Therefore, the next result is of special importance. It was proved by Bollobas 
and Thomason [29].
Theorem 3.10 Every nontrivial monotone graph property has a threshold.
Proof Without loss of generality assume that P is a monotone increasing graph 
property. Given o < s < 1, we define ) (s) by
P(G„,p(e) e P) = s.
Note that)(s) exists because
P(G„,p eP) = £ )l£<G>।(1 -))v-l£<GI
G eP
is a polynomial in ) that increases from o to 1. This is not obvious from the expression, 

42
Uniform and Binomial Random Graphs
but it is obvious from the fact that P is monotone increasing and that increasing p 
increases the likelihood that G„,p e P (see (3.6)).
We will show that p* = p(1/2) is a threshold for P. Let G 1,G2,...,G$ be 
independent copies of G„,p. The graph G 1 U G2 U ■■■ U G$ is distributed as 
G„;1_(1_))$. Now 1 - (1 - p)$ < kp, and therefore by the coupling argument,
®n,1-(1-p) $ - ^n,$p,
and soGn,$p i P implies G1,G2,.. ,,G$ i P. Hence
P(G„,$p i P} < [P(G„,p i P)]$.
Let w be a function of n such that w to arbitrarily slowly as n to, w « log log n.
Suppose also that p = p* = p(1/2) and k = w. Then
p(G„,^p. i P)< 2 ‘ = o(1).
On the other hand, for p = p*/w,
- = p(G„,p. i P) < [P(G„,p./to i P)] “ .
So
P(G„,p«/^ i P) > 2-1/" = 1 - o(1).
□
In order to shorten many statements of theorems in the book, we say that a sequence 
of events &n occurs with high probability (w.h.p.) if
lim P(£n) = 1. 
n^&
Thus the statement that says p* is a threshold for a property P in Gn,p is the same 
as saying that Gn,p i P w.h.p. if p « p*, while Gn,p e P w.h.p. if p » p*.
In the literature w.h.p. is often replaced by a.a.s. (asymptotically almost surely), not 
to be confused with a.s. (almost surely).
Exercises
3.2.1. Prove that if Var X/E X 0 as n to, then for every s > 0, 
P((1 - e) EX < X < (1 + e)EX) 
1.
3.2.2. Find in Gn,p and in Gn,m the expected number of maximal induced trees on 
k > 2 vertices. (Note that an induced tree is maximal in Gn,p if there is no 
vertex outside this tree connected to exactly one of its vertices.)
3.2.3. Suppose that p = dIn where d = o(n1/3). Show that w.h.p. Gn,p has no 
copies of ^4.

3.2 Thresholds
43
3.2.4. Suppose that p = d/n where d is a constant, d > 1. Show that w.h.p. G„,p 
contains an induced path of length (log n)1/2.
3.2.5. Suppose that p = d/n where d = O(1). Prove that w.h.p., in G„,p, for all 
5 c [n], 151 < n/log n, we have e(5) < 2|5|, where e(5) is the number of 
edges contained in 5.
3.2.6. Suppose that p = log n/n. Let a vertex of Gn,p be small if its degree is less than 
log n/100. Show that w.h.p. there is no edge of Gn,p joining two small vertices.
3.2.7. Suppose that p = d/n where d is constant. Prove that w.h.p., in Gn,p, no 
vertex belongs to more than one triangle.
3.2.8. Suppose that p = d/n where d is constant. Prove that w.h.p. Gn,p contains a 
vertex of degree exactly |"(log n)1/r|.
3.2.9. Prove that if np = w < log n, where w = w(n) 
to as n to, then w.h.p.
G„,p contains at least one triangle. Use a coupling argument to show that it 
is also true for larger p .
3.2.10. Suppose that $ > 3 is constant and that np 
to. Show that w.h.p. G„,p
contains a copy of the $-cycle, C$.
3.2.11. Find the threshold for the existence in G„,p of a copy of a diamond (a cycle 
on four vertices with a chord).
Problems for Chapter 3
3.1 Prove statement (i) of Theorem 1.4.
Let 0 < p0 < 1 and N = Q). Suppose that P is a graph property such 
that P(Gn,m e P} p0 for all m = Np + O(^Np (1 - p)). Show that then 
P(Gn,p e P) po as n to.
3.2 Prove statement (ii) of Theorem 1.4.
3.3 Let P be an increasing graph property, N = Q) and let 0 < m < N while 
p = m/N. Assume that 5 > 0 is fixed and 0 < (1 ± 5)p < 1. Show that
(1) if P(Gn,p e P) 
1, then P(Gn,m e P) 
1,
(2) if P(Gn,p P' 
> 0, then P(Gn,m ■: 7’1 > 0,
(3) if P(Gn,m eP)^ 1, then P(G„,a+6)p e P) 
1,
(4) if P(Gn,m P' 
> 0, then P(G„,(1-a)p ■: 7’1 > 0.
3.4 Let P be a monotone increasing property and let m1 < m2. Suppose that P(Gn>m1 e 
P) < 1 and P(Gn,m2 e P} > 0. Show that P(Gn,m1 e P) < P(Gn,m2 e P).
3.5 A graph property P is convex if graphs G', G" e P and G' c G c G"; then also 
G e P. Give at least two examples of convex graph properties and show that each 
such property P is an intersection of an increasing property P' and decreasing 
property P". Is it true in general?

44
Uniform and Binomial Random Graphs
3.6 Let P be a convex graph property graph and let m1, m, m2 be integer functions of 
' satisfying 0 < mi < m < m2 < ('); then
',.,.... eP)> P(Gn.mi eP) + ',.&•; eP)- 1.
3.7 Let P be a convex graph property, N = (') and let 0 < m < N while p = m/N. 
Show that if P(G„;)) 
1 as ' ^ ^, then P(G„,m) 
1.

4 Evolution
Here begins our story of the typical growth of a random graph. All the results up to 
Section 4.3 were first proved in a landmark paper by Erdos and Renyi [43]. The notion 
of the evolution of a random graph stems from a dynamic view of a graph process: 
viz. a sequence of graphs: G0 = ([‘], 0), G1, G2,..., G&,..., GN = K‘, where G&+1 
is obtained from G& by adding a random edge e&. We see that there are (')! such 
sequences and G& and G‘,& have the same distribution.
In the process of the evolution of a random graph we consider properties possessed 
by G& or G„,& w.h.p. when & = &(‘) grows from 0 to ('), while in the case of G„.) 
we analyze its typical structure when ) = )(‘) grows from 0 to 1 as ‘ to.
In the current chapter we mainly explore how the typical component structure 
evolves as the number of edges & increases. The following statements should be 
qualified with the caveat, w.h.p. The evolution of Erdos-Renyi-type random graphs 
has clearly distinguishable phases. The first phase, at the beginning of the evolution, 
can be described as a period when a random graph is a collection of small components 
which are mostly trees. Indeed the first result in this section shows that a random graph 
G',& is w.h.p. a collection of tree components as long as & = ((‘), or, equivalently, 
as long as ) = ((‘“1) in G‘;). In more detail, we see that initially G&, & = ((‘1/2) 
contains only isolated edges. Gradually larger and larger components appear, but while 
& = ((‘) we will see that G& remains a forest. When & = c‘ for some constant 
c < 1/2, cycles may appear but G& consists of a forest with the maximum component 
size O(log ‘). There may be a few unicyclic components consisting of a tree plus an 
edge. No component contains more than one cycle. When & ~ ‘/2, things get very 
complicated, and when the process emerges with & = c‘, c > 1/2, there is a unique 
giant component of size Q(‘) plus a forest with trees of maximum size O (log ‘) plus 
a few unicyclic components. This phase transition in the component structure is one 
of the most fascinating aspects of the process. We proceed to justify these statements.
4.1 Subcritical Phase
For clarity, all results presented in this chapter are stated in terms of G‘;&. Due to 
the fact that computations are much easier for G„.) we will first prove results in this 
model, and then the results for G„.m will follow by the equivalence established either in

46
Evolution
Lemmas 3.2 and 3.3 or in Theorem 3.4. We will also assume, throughout this chapter, 
that m = m(n) is a function growing slowly with n, e.g., m = log log n will suffice.
Theorem 4. 1 If m « n, then Gm is a forest w.h.p.
Proof Suppose m = n/m and let N = ('), so p = m/N < 3/(mn). Let X be the 
number of cycles in Gn,P. Then
/n\ (k - 1)! $ 
p
E X = £
$=3
< y n$ (k - 1)! $ < y n$ 3$
“ k! 
2 P ~ 
2k m$ n$
$=3 
$=3
= O(m"3) 
0.
Therefore, by the First Moment Method (see (3.11)),
P(G„,P is not a forest) = P(X > 1) < EX = o(1),
which implies that
P(G„,P is a forest) 
1 as n to,
Notice that the property that a graph is a forest is monotone decreasing, so by Lemma 
3.3,
P(Gm is a forest) 
1 as n to.
(Note that we have actually used Lemma 3.3 to show that P(G„;P is not a forest) =o (1), 
which implies that P(Gm is not a forest)=o(1).) 
□
As we keep adding edges, trees on more and more vertices gradually start to appear. 
The next two theorems show how long we have to “wait” until trees with a given 
number of vertices appear w.h.p.
$^2
Theorem 4. 2 Fix k > 3. If m « n $-1, then w.h.p. Gm contains no tree with k vertices.
$ 
.$ 
$-2
Proof Let m = n $-1 /m and then
m2 
3
— --——— < --———.
N 
mn$/( $-11 
mnklk$-1)
Let X$ denote the number of trees with k vertices in G„;P. Let T1,T2..,TM be an 
enumeration of the copies of k -vertex trees in Kn. Let
A" = {T" occurs as a subgraph in G„;P}.
The probability that a tree T occurs in G„;P is pe'"r-1, where e(T) is the number of 
edges of T. So,
M
E X$ = ^ P( At) = Mp$-1.
t=1

4.1 Subcritical Phase
47
But M = ') kk 2 since one can choose a set of k vertices in (') ways and then by 
Cayley’s formula choose a tree on these vertices in kk~2 ways. Hence
(
M 1\kk~2pk-1. 
k
(4.1)
Noting also that, by (2.2), for every n and k, (') < () k, we see that
E Xfc < (ne \k k $-2 / 
3
k 
mnk I1'- k-1
k-1
3k-1ek
kGG 1 
0
as n <x>, seeing as k is fixed.
Thus we see by the First Moment Method that
P(G„,p contains a tree with k vertices) 
0.
This property is monotone increasing and therefore
P(Gm contains a tree with k vertices) 
0. 
□
$-2
Let us check what happens if the number of edges in Gm is much larger than n $-1.
$-2
Theorem 4. 3 Fix k > 3. If m » n $-1, then w.h.p. Gm contains a copy of every fixed 
tree with k vertices.
m
Proof Let p = & , m = mn $-1, where m = o (log n) and fix some tree T with k 
vertices. Denote by Xk the number of isolated copies of T (T-components) in Gnp. 
Let aut(H) denote the number of automorphisms of a graph H. Note that there are 
k !/aut(T) copies of T in the complete graph Kk. To see this choose a copy of T with 
vertex set [ k]. There are k! ways of mapping the vertices of T to the vertices of Kk. 
Each map f induces a copy of T and two maps f1, f2 induce the same copy if and only 
if f2 ff1 is an automorphism of T.
So,
_E Xk =
n 
k! pk-1
k aut(T)
(1 _ p)k(n-k)+(2)-k+1
= (1 + o (1)) (2m)k-1 
aut(T)
c».
(4.2)
(4.3)
In (4.2) we have approximated (') < (k$ and used the fact that m = o(log n) in order 
to show that (1 - p)k(”“k)+($)“k+1 = 1 _ o(1).
Next let T be the set of copies of T in Kn and T[k] be a fixed copy of T on vertices 
[k] of K„. Then,
E(X2) = X P(T2 Qi ’-'• I T1 S" , ,p) P(T1 C" G„;p)
T1 ,T2 er

48
Evolution
_  S'-
= E X$
1 + 
P(72 Qi Gn,p | T[$}Qi Gn,p)
7 er
\ 
v (i2)n[$ ]=0 
/
< E X$ (1 + (1 - ))-$2 EX$) .
Notice that the (1 - p)“$2 factor comes from conditioning on the event T[$] Q" Gn,p, 
which forces the nonexistence of fewer than k2 edges.
Hence, by the Second Moment Method,
p(X$ > 0) > 
—(E X$) 
. . 
1
EX$ (1 + (1 - p)-$2 EX$)
as n to, since p 0 and E X$ 
Thus
P(Gn>p contains a copy of isolated tree T) 
1,
which implies that
P(Gn,P contains a copy of T) 
1.
As the property of having a copy of a tree T is monotone increasing, it in turn implies 
that
P (Gm contains a copy of T) 
1
.. .. 2 . .„
as m » n $-i and n 
to.
□
Combining the above two theorems we arrive at the following conclusion.
Z'-l 11___ $ $ T>J $ 
.■ 
$ $ $ 
$—2- •,» ,J J 1 1 f .1 
,,J,
Corollary 4.4 The function m (n) = n $-1 is the threshold for the property that a
random graph Gm contains a tree with k > 3 vertices, i.e.,
P(Gm 2 k-vertex tree) =
if m « 
if m »
$2
n $-1
$2
n $-1
((1)
1 - ((1)
We complete our presentation of the basic features of a random graph in its subcrit- 
ical phase of evolution with a description of the order of its largest component.
Theorem 4.5 If m = 1 cn, where 0 < c < 1 is a constant, then w.h.p. the order of the 
largest component of a random graph Gm is O (log n).
The above theorem follows from the next three lemmas stated and proved in terms 
of Gn>p with p = c/n, 0 <c< 1. In fact the first of those three lemmas covers a little 
bit more than the case of p = c/n, 0 < c < 1.

4.1 Subcritical Phase
49
Lemma 4.6 If) < ' - '^3, where m = m(‘) 
, then w.h.p. every component in
Gn,p contains at most one cycle.
Proof Suppose that there is a pair of cycles that are in the same component. If such 
a pair exists, then there is minimal pair C1, C2, i.e., either C1 and C2 are connected by 
a path (or meet at a vertex) or they form a cycle with a diagonal path (see Figure 4.1). 
Then in either case, C1 U C2 consists of a path P plus another two distinct edges, one 
from each endpoint of joining it to another vertex in . The number of such graphs 
on $ labeled vertices can be bounded by $2$ !.
Figure 4.1 Ci U C2
Let be the number of subgraphs of the above kind (shown in Figure 4.1) in the 
random graph Gn). By the First Moment Method,
> 0) < EX < ^ '\$2$!)$+1 
=4 \$/
(4.4)

50
Evolution
"1 1 . _ 2
‘1/4dx w 0 (1) •
12
— exp
□
We remark for later use that if p = c/n, 0 < c < 1, then (4.4) implies that
P(X > 0) < £ k2c$+1 n~1 = O(n"1). 
$=4
(4.5)
Hence, in determining the order of the largest component we may concentrate our 
attention on unicyclic components and tree components (isolated trees). However, the 
number of vertices on unicyclic components tends to be rather small, as is shown in 
the next lemma.
Lemma 4.7 If p = cfn, where c + 1 is a constant, then in Gn,p w.h.p. the number of 
vertices in components with exactly one cycle is O (") for any growing function ".
Proof Let X$ be the number of vertices on unicyclic components with k vertices. 
Then
EX$ < Mk$~2 (2)kp$(1 - p)$('"$)+($)"$. 
(4.6)
The factor k$“2 ($) in (4.6) is the number of choices for a tree plus an edge on k 
vertices in [k]. This bounds the number C (k, k) of connected graphs on [k] with k 
edges. This is off by a factor O (k1/2) from the exact formula which is given below for 
completeness:
c < k.k >=t (k) 
+k $-+-■ ~ 7f k *-■«
(4.7)
The remaining factor, other than ('), in (4.6) is the probability that the k edges of the 
unicyclic component exist and that there are no other edges on G„;P incident with the 
k chosen vertices.
Note also that, by (2.4), for every n and k, (') < 
e~k($-1)/2'. Assume next that
c < 1 and then we get
$$
n$ 
$($-1) $ 1 c$ _c$ c$($-1) ok
E X$ < — e 
2‘ k$ e c$+ 2‘ + 2‘
$
e$ 
$($-1) $ 1 $ k$ $($-1) c
—r e 2' k$+1c$ e c$+ 2' + 2
(4.8)
(4.9)
So,
E Y‘ X$ < Yt k [ce1~c )$ e 0 = O (1), 
$=3 
$=3
(4.10)

4.1 Subcritical Phase
51
since ce1 c < 1 for c + 1. By the Markov inequality, if w = w(n) 
to (see Lemma 
2.15),
(
' 
1
y\xk > w = o -
$=3 
W
0 as n to,
and the lemma follows for c < 1.
If c > 1, then we cannot deduce (4.9) from (4.8). If however k = o(n), then this 
does not matter, since then e$ !n = eo($). In the proof of Theorem 4.10 below we 
show that when c > 1, there is w.h.p. a unique giant component of size Q(n) and all 
other components are of size O (log n). This giant is not unicyclic. This enables us to 
complete the proof of this lemma for c > 1. 
□
After proving the first two lemmas one can easily see that the only remaining 
candidate for the largest component of our random graph is an isolated tree.
Lemma 4.8 Let p = ', where c + 1 is a constant, a = c - 1 - log c, and 
w = w(n) 
to, w = o(log log n). Then 
(i) w.h.p. there exists an isolated tree of order
k- = — (log n -- log log n - w, 
a2
(ii) w.h.p. there is no isolated tree of order at least
k. --1 log n - ^ log log n + w.
Proof Note that our assumption on c means that a is a positive constant. 
Let X$ be the number of isolated trees of order k. Then
EX$ = n k$"2p$-1 (1 - p)$('-$)+(2)-$+1, 
k
(4.11)
To prove (i) suppose k = O(logn). Then ($) ~ z$! and by using inequalities (3.9), 
(3.10) and Stirling’s approximation, Lemma 2.4, for k!, we see that
n k$~2
E X$ = (1 + o (1))- — (ce-c) $ 
c k !
= (1 + 0(1)) n fce1-^$ 
cV2^ 
k5/2 ;
= ( 
S-^ ,5/2e a$ 
for k = O(logn).
W2^ 
k 5/2
(4.12)
(4.13)
(1 + o (1)) n e aM (log n)5/2 
cV2v k 512 
n
Putting k = k_ we see that
(4.14)
for some constant A > 0.
We continue via the Second Moment Method, this time using the Chebyshev in­
equality as we will need a little extra precision for the proof of Theorem 4.10. Using

52
Evolution
essentially the same argument as for a fixed tree T of order k (see Theorem 4.3), we 
get
EXk < EXk (1 + (1 - p)~k2 EX^ .
So
Var X$ < EX$ + (EX$)2 ((1 - p)~$ - 1)
< EX$ + 2ck2(EXk)2/n 
for k = O(log n).
(4.15)
Thus, by the Chebyshev inequality (see Lemma 2.18), we see that for any e > 0,
1 
2ck2
P (|Xk - EXk|> e EXk)< -5 
+ -^ = o(1).
e2 E Xk 
e2'
(4.16)
Thus w.h.p. Xk > Aea"'/2 and this completes the proof of (i).
For (ii) we go back to formula (4.11) and write, for some new constant A > 0,
k-1 / c \k-1 
, ck2
c 
e“Ck+
.k-2
E Xk <
2 An 
1_ Ck \k
- 
. 
' (ck e 
)
where c$ = c ^1 
.
In the case c < 1 we have ck e1-Ck < ce1-C and ck ~ c, and so we can write
y E X < 3An y (ce1~C)k < 3An Y 
fi k" c k k5/2 " ck5/2 h 
k =k+ 
k=k+ 
+ k=k-
3 Ane" ak+ 
(3 A + o (1)) a5/2e" a"
= —W9------------ =----------a-------= 0 W •
ck+/2(1 - e“a) 
c(1 - e a)
- ak
(4.17)
If c > 1, then for k < jo’g' we use c$ e1 C$ = e a o (1/log ' and for k > jo'n we use 
ck > c/2 and ck e1-Ck < 1 and replace (4.17) by
n 
'/tog' 
n
y eXk < -35n y e-(a+owogn»k + — y 
=0(1).
5 5 
52 
c 
k5/2
k=k+ 
ck + 
k=k+ 
k =n/log n
□
Finally, applying Lemmas 4.7 and 4.8 we can prove the following useful identity: 
Suppose that 1 = 1 (c) is given as
, . 
(c 
c 1
1 = 1(c) =
the solution in (0,1) to 1e 1 = ce C c > 1.
Note that 1e“1 increases continuously as 1 increases from 0 to 1 and then decreases. 
This justifies the existence and uniqueness of 1.

4.1 Subcritical Phase
53
Lemma 4.9 If c > 0, c + 1 is a constant, and x = x (c) is as defined above, then
1 ~ $ k-1
1 E V )$ = 1. 
x £1 $!
Proof Let p = '. Assume first that c < 1 and let X be the total number of vertices 
of Gn,p that lie in nontree components. Let X$ be the number of tree components of 
order $. Then,
n 
n = £ $X$ + X. 
$=1
So,
= 2 $ E X$ + EX. 
$=1
Now, 
(i) by (4.5) and (4.10), EX = O(1),
(ii) by (4.12), if $ < $+, then
EX$ = (1 + o(1))cl.$$-2 (ce-c)$ .
So, by Lemma 4.8,
$+
‘ = o(n) + - ^ 
c $=1
$ $-1
(ce'c >$
=o w+n E
$=1
$ $ 1 / 
_c.$
$ 
< ce > $•
Now divide through by n and let n 
to.
This proves the identity for the case c < 1. Suppose now that c > 1. Then, since x 
is a solution of the equation ce~c = xe~x, 0 < x < 1, we have
TO
E 
$=1
$ $ 1
$!
$$-1 
$$-1
"W <ce-C>$ = E "W '>$ 
$=1
= x
by the first part of the proof (for c < 1).
We note that, in fact, Lemma 4.9 is also true for c = 1.
□
Exercises
4.1.1. Verify equation (4.3).
4.1.2. Find an upper bound on the expected number of cycles on $ vertices in Gn>p 
and show its asymptotic behavior (n 
to) when p = c/n and c < 1, c = 1 and
c > 1.
4.1.3. Prove that if m « n1/2, then w.h.p. the random graph Gn& is the union of 
isolated vertices and edges only.

54
Evolution
4.1.4. Prove that if m » n1/2, then Gn& contains a path of length 2 w.h.p.
4.1.5. Show that if m = cn(k~2)^k~1^, where c > 0, and T is a fixed tree with k > 3 
vertices, then
P(G„,m contains an isolated copy of tree T) 
1 - e '!
as n 
m, where A = (2c)k-1/aut(T) and aut(T) denotes the number of
automorphisms of T.
4.1.6. Show that if c + 1 and ie“1 = ce“c, where 0 < 1 < 1, then
1 v $k~\ _c$k 
(1 - I c< 1,
'ce' 
$ 
1 2 1
c £1 $! 
1 a - 2) c> L
4.2 Supercritical Phase
The structure of a random graph Gm changes dramatically when m = 2 cn, where c > 1 
is a constant. We will give a precise characterization of this phenomenon, presenting 
results in terms of Gm and proving them for Gn,p with p = c/n, c > 1.
Theorem 4.10 If m = cn/2,c > 1, then w.h.p. Gm consists of a unique giant 
component, with (1 - f + o(1)) n vertices and (1 - 12 + o(1)j I' edges. Here 0 < 
1 < 1 is the solution of the equation xe~x = ce~c. The remaining components are of 
order at most O(log n).
Proof Suppose that Zk is the number of components of order k in Gn,p. Then, 
bounding the number of such components by the number of trees with $ vertices that 
span a component, we get
E Zk < nkk~2pk 1 (1 - p)k(n~k) 
(4.18)
k
(
ne\k 
/c\k—1 
2.
kk \ $k-2 / _ j 
e~ck+ck2/'
< An (re1-cvck/„ 1k
- k2 ce 
•
Now let J81 = J81 (c) be small enough so that
ce1"c+c^1 < 1,
and let fi0 = fi0 (c) be large enough so that
(ce1-c+O(1))^0log' < 1.
n2
If we choose fi1 and fi0 as above, then it follows that w.h.p. there is no component of 
order k e [fi0 log n,J81n].

4.2 Supercritical Phase
55
Our next task is to estimate the number of vertices on small components, i.e., those 
of size at most fi0 log n.
We first estimate the total number of vertices on small tree components, i.e., on 
isolated trees of order at most fi0 log n.
Assume first that 1 < k < k0, where k0 
log n, where a is from Lemma 4.8. It
follows from (4.12) that
(
$0 
\ 
$0
e «- n e 
$=1 
$=1
k $-1
' ,ce~!’
k $-1
—,ce~'! ’$
$
using k$-1/k! < e$, and ce“c < e-1 for c + 1 to extend the summation from k0 to 
infinity.
Putting e = 1/log n and using (4.16) we see that the probability that any X$, 1 < 
k < k0, deviates from its mean by more than 1 ± e is at most
$0E 
$=1
(log n)2 
/ (log n)4
n1/2-o (1) + O 
n
= ((1),
where the n1/2 (term comes from putting m ~ k0/2 in (4.14), which is allowed by 
(4.13) and (4.15).
Thus, if 1 = 1 (c), 0 < 1 < 1 is the unique solution in (0,1) of the equation 
ie“1 = ce“c, then w.h.p.
$0 
!X 
$-1
E kv= - n Ek— <«-'■ > $
$=1 
c $=1 
!
n1 
c
by Lemma 4.9.
Now consider k0 < k < fi0 log n. It follows from (4.11) that
E fj” kX$ ) < *2 ” ( ne ) $ k $-2 ( - ) $-1 e-c$ ('-$)/'
$=$0+1 
$=$0+1
= O (n1/2+o (1)) . 
(4.19)
So, by the Markov inequality (see Lemma 2.15), w.h.p.,
*0 log ”
7, kX$ = o(n).
$=$0+1
Now consider the number Y$ of nontree components with k vertices, 1 < k < fi0 log n.
E

56
Evolution
= O (1). 
(4.20)
So, again by the Markov inequality, w.h.p.,
Po log '
kYk = o(n).
k=1
Summarizing, we have proved so far that w.h.p. there are approximately 1 vertices on 
components of order k, where 1 < k < fy log n and all the remaining giant components 
are of size at least fy1n.
We complete the proof by showing the uniqueness of the giant component. Let 
log n 
ci
ci = c----------and pi = —.
nn
Define p2 by
1 - p = (1 - pi)(1 - P2)
and note that p2 > log'. Then, see Section 3.2,
Gn,p = Gn,P1 U Gn,P2.
If x1 e~X1 = c1e~C1, then x1 ~ x and so, by our previous analysis, w.h.p., G„,P1 has 
no components with the number of vertices in the range [fy log n,fy1 n].
Suppose there are components C1, C2,..., C% with |Ci | > fyn. Here I < 1 /fy. Now 
we add edges of Gn,P2 to G„,P1. Then
P (3",j : no G„;p2 edge joins C" with C#) < Q (1 - p2)(P1 ”)2
< /2G/;2 log '
= o (1).
So w.h.p. G„.P has a unique component with more than fy log n vertices and it has 
~ (1 - 1) n vertices.
We now consider the number of edges in the giant Co. Now we switch to G = Gn,m. 
Suppose that the edges of G are ei, e2,..., em in random order. We estimate the 
probability that e = em = {x, y} is an edge of the giant. Let G 1 be the graph induced 
by {e1, e2,..., em_1}. G 1 is distributed as G„,m_i, and so we know that w.h.p. G 1 has 
a unique giant C1 and other components are of size O (log n). So the probability that e 
is an edge of the giant is o (1) plus the probability that x or y is a vertex of C1. Thus,
x 
x
e t Co 
| |Ci| - n 11 - -jj = P (e n Ci = 0 | |Ci| - n 11 - -H
C = (1 
)(1 
) ~ fx)2 ■ 
(4.21)
It follows that the expected number of edges in the giant is as claimed. To prove 
concentration, it is simplest to use the Chebyshev inequality, see Lemma 2.18. So, now 

4.2 Supercritical Phase
57
fix ", j < & and let C2 denote the unique giant component of G‘& - {e,, ej}. Then, 
arguing as for (4.21),
P(e,, ej C Co) = ((1) + P(ej Cl C2 + 0 | e, Cl C2 + 0) P(e, Cl C2 + 0) 
= (1 + ((1)) P(e, c Co) P(e# c Co).
In the ((1) term, we hide the probability of the event
{e, Cl C2 + to, e# Cl C2 + to, e, Cl e# + 0} ,
which has probability ((1). We should double this ((1) probability here to account for 
switching the roles of", #.
The Chebyshev inequality can now be used to show that the number of edges is 
concentrated as claimed. 
□
From the above theorem and the results of previous sections, we see that, when 
& = c‘/2 and c passes the critical value equal to 1, the typical structure of a random 
graph changes from a scattered collection of small trees and unicyclic components to a 
coagulated lump of components (the giant component) that dominates the graph. This 
short period when the giant component emerges is called the phase transition. We will 
look at this fascinating period of the evolution more closely in Section 4.3.
We know that w.h.p. the giant component of G„,&,& = c‘/2, c > 1 has ~ (1 - 1) ‘ 
(2 \
1 _ f2 CT edges. So, if we look at the graph H induced by the vertices 
outside the giant, then w.h.p. H has ~ ‘1 = c vertices and ~ &1 = 1’1 /2 edges. Thus 
we should expect H to resemble Gn1 &, which is subcritical since 1 < 1. This can be 
made precise, but the intuition is clear.
Now increase & further and look on the outside of the giant component. The giant 
component subsequently consumes the small components not yet attached to it. When 
& is such that &/‘ to, then unicyclic components disappear and a random graph 
G& achieves the structure described in the next theorem.
Theorem 4.11 Let w = w (‘) to as ‘ be some slowly growing function. If 
& > w‘ but & < ‘(log ‘ - w)/2, then G& is disconnected and all components, with 
the exception of the giant, are trees w.h.p.
Tree components of order $ die out in the reverse order they were born, i.e., larger 
trees are “swallowed” by the giant earlier than smaller ones.
Exercises
4.2.1. Show that if ) = w/’ where w = w(‘) 
to, then w.h.p. G„,) contains
no unicyclic components. (A component is unicyclic if it contains exactly one 
cycle, i.e., is a tree plus one extra edge.)

58
Evolution
4.2.2. Suppose that ‘) c» and 3 < $ = O(1). Show that G‘;) contains a $-cycle 
w.h.p.
4.2.3. Verify equation (4.19).
4.2.4. Verify equation (4.20).
4.3 Phase Transition
In the previous two sections we studied the asymptotic behavior of G& (and G‘;)) 
in the “subcritical phase” when & = c‘/2,c< 1 () = c/’, c < 1), as well as in the 
“supercritical phase” when & = c‘/2, c > 1 () = c/’, c > 1) of its evolution.
We have learned that when & = c‘/2, c < 1, our random graph consists w.h.p. of 
tree components and components with exactly one cycle (see Theorem 4.1 and Lemma 
4.7). We call such components simple, while components which are not simple, i.e., 
components with at least two cycles, will be called complex.
All components during the subcritical phase are rather small, of order log ’, tree 
components dominate the typical structure of G&, and there is no significant gap in the 
order of the first and the second largest component. This follows from Lemma 4.8. The 
proof of this lemma shows that w.h.p. there are many trees of height $_. The situation 
changes when & > ‘/2, i.e., when we enter the supercritical phase and then w.h.p. G& 
consists ofa single giant complex component (of the order comparable to ’), and some 
number of simple components, i.e., tree components and components with exactly one 
cycle (see Theorem 4.10). One can also observe a clear gap between the order of the 
largest component (the giant) and the second largest component which is of the order 
O (log ‘). This phenomenon of dramatic change of the typical structure of a random 
graph is called its phase transition.
A natural question arises as to what happens when &/‘ 1/2, either from below 
or above, as ‘ <x>. It appears that one can establish a so-called scaling window or 
critical window for the phase transition in which G& is undergoing a rapid change in 
its typical structure. A characteristic feature of this period is that a random graph can 
w.h.p. consist of more than one complex component (recall that there are no complex 
components in the subcritical phase and there is a unique complex component in the 
supercritical phase).
Erdos and Renyi [43] studied the size of the largest tree in the random graph G„,& 
when & = ‘/2 and showed that it was likely to be around ‘2/3. They called the 
transition from O(log ‘) through 0(‘2/3) to Q(’) the “double jump.” They did not 
study the regime & = ‘/2 + ((‘). Bollobas [20] opened the detailed study of this and 
Luczak [80] refined this analysis. The component structure of G„,& for & = ‘/2 + ((‘) 
is rather complicated and the proofs are technically challenging since those proofs 
require precise estimates of the number of very sparse connected labeled graphs. 
Nevertheless, it is possible to see that the largest component should be of order ‘2/3 
using a nice argument from Nachmias and Peres. They published a stronger version of 
this argument in [93].

4.3 Phase Transition
59
Theorem 4.12 Let p = ' and A be a large constant. Let Z be the size of the largest 
component in Gn,p. Then
(i) P (z < 1 n2/^ = O(A"1), 
A
(ii) P [z > An2/3) = O(A"1).
Proof We will prove part (i) of the theorem first. This is a standard application of the 
First Moment Method. Let X$ be the number of tree components of order k and let 
k e [ jn2/3,An2/3]. Then, see also (4.11),
EX$ = I' 1 k$~2pk~1 (1 -p)$(n"$)+($' iJ. 
k
But
(1 _ p)$('-$) + (2)-$+1 ~ (i _ p^kn-k2/2
= exp{(kn - k2/2) log(1 - p)}
i kn - k2/21
~ exp---------------1.
n
Hence, by the above and the approximation of the binomial for k = o(n3/4) (see 
formula (2.12)), we have
_ 
n 
[ k3 1
E X$ - 
exp - 2 . 
(4.22)
V2r k5/2 
6n2
So if
An2/'3 
x = E X$ • 
1 n2/3
then
1 p A 
■. 3 6
E X---- — 
—g1~dx
2^ Jx= A X5!2
= jL a3/2 + O (A1/2).
3sfn
Arguing as in Lemma 4.8 we see that
EX$ < EX$ + (1 + o(1))(EX$)2,
E(X$X%)<(1 + o(1))(EX$)(EX%), k + I.
It follows that
EX2 < EX + (1 + o(1))(EX)2.
Applying the Second Moment Method, we see that
P(X > 0) > ------—------- —
v 2 
(E X )-1 + 1 + o (1)

60
Evolution
= 1 - O(A"1),
which completes the proof of part (i).
To prove (ii) we first consider a breadth-first search (BFS) starting from, say, vertex 
1. We will use the notion of stochastic dominance (see Lemma 2.29).
We construct a sequence of sets 51 = {x}, S2,..., where
S"+1 = {/ £ S" : 3w e S" such that (/, w) e E(Gn,p)}.
We have
E(|S"+i| |S") < (« -|S"|) ( 1 -(1 -p)|5d)
< (« -|S"|)|S"|p
< |S"|.
So
E|S"+11 < E|S"| < ■■■ < E|S11 = 1. 
(4.23)
We prove next that
x$ = P(S$ + 0)< 4. 
(4.24)
$
This is clearly true for $ < 4, and we obtain (4.24) by induction from 
n-1 / 
\
tf$+ 1 < y ' " 1 p" (1 - p) n-1~" (1 -(1 - x$)" )■ 
(4.25)
" 
$
"=1
To explain the above inequality note that we can couple the construction of 
S1,S2,...,Sk with a (branching) process where T1 = {1} and Tk+1 is obtained from 
T$ as follows: each T$ independently spawns Bin(« - 1,p) individuals. Note that |T$| 
stochastically dominates |S$ |. This is because in the BFS process, each w e S$ gives 
rise to at most Bin(« - 1, p) new vertices. Inequality (4.25) follows because Tk+1 + 0 
implies that at least one of 1’s children gives rise to descendants at level $. Going back 
to (4.25) we get
n$+1 < 1 -(1 - p) n~1 -(1 - p + p (1 - n$)) n~1 + (1 - p) n~1
= 1 -(1 - pn $) n~1
- n$ (1 - 1 ^$) • 
(4.26)
This expression increases for 0 < n$ < 1 and immediately gives n5 < 3/4 < 4/5. In 
general, we have by induction that
^$.1 < 4 (1 -1) 
.
$ 
$ 
$ 1
completing the inductive proof of (4.24).

4.3 Phase Transition
61
Let Cx be the component containing x and let p x = max{k : S$ + 0} in the BFS 
from x. Let
X = ||x : |Cx| > n2/3}|< Xi + X2,
where
Xi = ||x : |C11 > n2/3 and p1 < n1/3||,
X2 = ||x : px > n1/3■
It follows from (4.24) that
r(px > -1'3) < -L
and so
E X2 < 4n2/3.
Furthermore,
P ||Cx| > n2/3 and px < n1/3} < P (|Si| + ••• + |S„i/31 > n2/3)
'
- -1/3 ’
(4.27)
after using (4.23). So E X1 < n2/3 and EX < 5n2/3.
Now let Cmax denote the size of the largest component. Now
Cmax < | XI + n2/3,
where the addition of n2/3 accounts for the case where X = 0. 
So we have
E Cmax < 6n2/3
and part (ii) of the theorem follows from the Markov inequality (see Lemma 2.15).
□
Exercises
4.3.1. Let C(k, k + /) denote the number of connected graphs with k vertices and 
k + I edges, where I = -1,0,1,2,... (e.g., C(k, k - 1) = k$“2 is the number 
of labeled trees on k vertices). Let X be a random variable counting connected 
components of G„;) with exactly k vertices and k +1 edges. Find the expected 
value of the random variable X for large n in terms of C(k, k + /).
4.3.2. Verify equation (4.26).
4.3.3. Verify equation (4.27).

62
Evolution
Problems for Chapter 4
4.1 Prove Theorem 4.11.
4.2 Suppose that & = c‘/2, where c > 1 is a constant. Let C1 denote the giant 
component of G„,& assuming that it exists. Suppose that C1 has ‘' < ‘ vertices 
and &’ < & edges. Let G 1, G2 be two connected graphs with n' vertices from 
[‘] and &' edges. Show that
P(Ci = G i) = P(Ci = G 2)
4.3
(i.e., C1 is a uniformly random connected graph with n' vertices and &’ edges). 
Suppose that is the length of the cycle in a randomly chosen connected unicyclic 
graph on vertex set [‘]. Show that
E z = ‘‘-2 (N - ‘ + 1)
C (‘,‘) 
’
4.4
4.5
4.6
4.7
4.8
where N = (').
Suppose that c < 1. Show that w.h.p. the length of the longest path in G„,), 
) = £ is ~ log' 
’
) 
‘ s log 1/c.
Suppose that c + 1 is constant. Show that w.h.p. the number of edges in the 
largest component that is a path in G„,), p = ' is ~ cl_olg)'c.
Let P = 17T. Show that if e is a small positive constant, then w.h.p. G„,) contains 
a giant component of size (2e + O(e2))'.
Let & = ' +,, where , = ,(‘) > 0. Show that if , » ‘2/3, then w.h.p. the random 
graph G„,& contains exactly one complex component. (Recall that a component 
C is complex if it contains at least two distinct cycles. In terms of edges, C is 
complex if and only if it contains at last |C | + 1 edges.)
Let & $ (‘) = ‘ (log ‘ + ($ - 1) loglog ‘ + w)/(2$), where |w | 
to, |w| =
((log ‘). Show that
P(G&$ 
$-vertex tree component) =
((1)
1 - ((1)
if w 
-to,
if w 
to.
4.9 Suppose that p = ', where c > 1 is a constant. Show that w.h.p. the giant 
component of G„,) is nonplanar. (Hint: Assume that c = 1 + e, where e is small. 
Remove a few vertices from the giant so that the girth is large. Now use Euler’s 
formula.)
4.10 Suppose that p = c/', where c > 1 is constant and let p = p(c) be the smallest 
root of the equation
1 cp + (1 - p)ce"c^ = log (c(1 - p)(p-1>U3.
1. Show that if w to and w < $ < p‘, then w.h.p. G„,) contains no maximal 
induced tree of size $ .
2. Show that w.h.p. G„,) contains an induced tree of size (log ‘)2.
3. Deduce that w.h.p. G„,) contains an induced tree of size at least p‘.

4.3 Phase Transition
63
4.11 Given a positive integer $, the $-core of a graph G = (V,E) is the largest set 
5 c V such that the minimum degree in the vertex-induced subgraph G [5] is 
at least $. Suppose that c > 1 and that 1 < 1 is the solution to ie-1 = ce“c. 
Show that then w.h.p. the number of vertices in the 2-core C2 of G‘,),) = cl’ 
is asymptotically (1 - 1)2 ’.
4.12 Let $ > 3 be fixed and let) = '. Show that if c is sufficiently large, then w.h.p. 
the $-core of G‘;) is nonempty.
4.13 Let $ > 3 be fixed and let) = '. Show that there exists 0 = 0(c, $) > 0 such that 
w.h.p. all vertex sets 5 with 151 < 0’ contain fewer than $|51/2 edges. Deduce 
that w.h.p. either the $-core of G‘;) is empty or it has size at least 0’.
4.14 Let G„A) denote the random bipartite graph derived from the complete bipartite 
graph K’’ where each edge is included independently with probability ). Show 
that if ) = cl’, where c > 1 is a constant, then w.h.p. G’,’,) has a unique giant 
component of size ~ 2G(c)’, where G(c) is as in Theorem 4.10.
4.15 Consider the bipartite random graph G‘,‘,)=c/’, with constant c > 1. Define 
0 < 1 < 1 to be the solution to ie~* = ce~c. Prove that w.h.p. the 2-core of 
G‘,‘,)=c/’ has ~ 2(1 -1) (1 - 1) ’ vertices and ~ c (1 - 1)2 ’ edges.

5 Vertex Degrees
In this chapter we study some typical properties of the degree sequence of a random 
graph. We begin by discussing the typical degrees in a sparse random graph, i.e., 
one with '/2 edges for some positive constant . We prove some results on the 
asymptotic distribution of degrees. The average degree of a fixed vertex is , and 
perhaps not surprisingly, the distribution of the degree of a fixed vertex is P( (c), 
Poisson with mean . The number of vertices ofa given value will then w.h.p. be 
concentrated around the mean E(Zd), which will be approximately ‘cde~c/d!. Hence, 
the number of vertices of degree d in a random graph is, roughly, ‘/dd+( (d'^ and so 
enjoys (super-)exponential decay as d grows. This is in sharp contrast to the polynomial 
decay observed in real-world networks, which we shall see later in Chapters 11 and 12.
We continue by looking at the typical values of the minimum and maximum degrees 
in dense random graphs, i.e., G‘,) where ) is constant. We find that the maximum 
and minimum degrees are w.h.p. equal to ‘) + O‘ log ‘). Surprisingly, even though 
the range of degrees is O ‘ log ‘), we find that there is a unique vertex of maximum 
degree. Further, this uniqueness spreads to the second largest and third largest and for 
a substantial range.
Given these properties of the degree sequence of dense graphs, we can then describe 
a simple canonical labeling algorithm that enables one to solve the graph isomorphism 
problem on a dense random graph.
5.1 Degrees of Sparse Random Graphs
Let us look first at the degree deg(v) of a vertex / in both models of random graphs. Ob­
serve that deg(v) in G‘;) is a binomially distributed random variable, with parameters 
‘ - 1 and ), i.e., for d = 0,1,2,..., ‘ - 1,
(
‘ — 1, 
) d (1 - )) '-1~d,
d
while in G„,& the distribution of deg(v) is hypergeometric, i.e.,
('-1W (V))
P(deg(v) = d) = d '& 
.
({&})

5.1 Degrees of Sparse Random Graphs
65
It is important to observe that in both random graph models the degrees of different 
vertices are only mildly correlated.
In this section we concentrate on the number of vertices of a given degree when the 
random graph G„,) is sparse, i.e., when the edge probability ) = ((1).
We return to the property that a random graph contains an isolated vertex (see 
Example 3.9, where we established the threshold for “disappearance” of such vertices 
from G„,)). To study this property more closely denote by Xo = X‘,0 the number of 
isolated vertices in G„,). Obviously,
E Xo = ‘ (1 - )) '~1,
and
to if ‘) - log ‘ -to,
E Xo 
e c if ‘) - log ‘ 
c, c < to,
(5.1)
r\ 
* r 
1
10 if ‘) - log ‘ to
as ‘ 
to.
To study the distribution of the random variable X0 (and many other numerical 
characteristics of random graphs) as the number of vertices ‘ to, i.e., its asymptotic 
distribution, we apply a standard probabilistic technique based on moments.
Method of Moments Let X be a random variable with probability distribution 
completely determined by its moments. If X1,X2,... ,X’,... are random variables 
with finite moments such that E X$ E X$ as ‘ to for every integer $ > 1, then 
the sequence of random variables {X’} converges in distribution to random variable 
X. The same is true if factorial moments of {X’ } converge to factorial moments of X, 
i.e., when
E(X’)$ = E [X’ (X’ - 1)... (X’ - $ + 1)] 
E(X)$ 
(5.2)
for every integer $ > 1.
For further considerations we shall use the following notation.
We denote by Po(A) the random variable with the Poisson distribution with parameter 
A, while N(0,1) denotes the variable with the standard normal distribution. We write 
X’ 
X to say that a random variable X’ converges in distribution to a random
variable X as ‘ to.
In random graphs we often count certain objects (vertex degrees, subgraphs of a 
given size, etc.). Then the respective random variables are sums of indicators over a 
certain set of indices I, i.e.,
S’ = ^ Z"'

66
Vertex Degrees
Then the $th factorial moment counts the number of ordered $ -tuples of objects with 
I" = 1, i.e.,
(S’)$ = 2 I"11"2 ■■■ I"$ ,
"1,"2,■■■,"$
where the summation is taken over all sequences of distinct indices "1, i2, ■ ■ ■, i $.
The factorial moment variant of the Method of Moments is particularly useful when 
the limiting distribution is Po(2), since the moments of a random variable with the 
Poisson distribution have a relatively complicated form while its $th factorial moment 
is $ . Therefore, proving convergence in the distribution of random variables such as 
' defined above we shall use the following lemma.
Lemma 5.1 Let S’ = £"I’;" be the sum of indicator random variables I’,". 
Suppose that there exists 2 > 0 such that for every fixed $ > 1,
lim E(S’)$ = V P(I‘,"i = 1,I‘,"2 = 1,...,I‘,"$ = 1) = 2$.
’^TO
"1 ,i2,—,i$
Then, for every j > 0,
2# # 
lim P(S’ = #) = e 2 
,
’^^ 
#!
i.e., S’ converges in distribution to the Poisson distributed random variable with 
expectation 2 (S’ 
Po(2)).
In the next theorem we discuss the asymptotic distribution of 0 and claim that it 
passes through three phases: it starts in the normal phase; next when isolated vertices 
are close to “dying out,” it moves through a Poisson phase; it finally ends up at the 
distribution concentrated at 0.
Theorem 5.2 Let 0 be the random variable counting isolated vertices in a random 
graph G’;). Then, as n ,
(i) 
Xo = (Xo - EXo)/(Var Xo)122 -^ N(0,1) if n2) w and ‘) - log n -to,
(ii) Xo Po(e“c) if n) - log n 
c, c <
(iii) 
Xo 
0 if ‘) - log n .
Proof For the proof of (i) we refer the reader to Chapter 6 of Janson, Luczak, and 
Rucinski [66] (or to [14] and [74]). Note that statement (iii) has already been proved 
in Example 3.9.
To prove (ii) one has to show that if ) = )(n) is such that n) - log n 
c, then
e-c$
lim P(Xo = $) = —— e"e 
’^^ 
$!
(5.3)

5.1 Degrees of Sparse Random Graphs
67
for k = 0,1,.... Now,
Xo = £ Iv, 
V eV
where
J1 if / is an isolated vertex in Gn ), 
IV = 0 otherwise.
So
EX0 = ^ EIV = n(1 - p)n-1 = nexp{(n - 1) log(1 - p')} 
V eV
- e"c. 
(5.4)
Statement (ii) follows from direct application of Lemma 5.1, since for every k > 1, 
E( Xo)$ = 2 
p( I/"! = 1’I/i2 = 1’---’I/ik = 1) = (n) $ (1 - p) $ ("$)+® ■
"1 ,i2 ,...,ik
Hence
lim E(Xo)$ = e’c$, 
n^&>
i.e., Xo, when p = (log n + c)/n, is asymptotically Poisson distributed with the 
expected value A = e~c. 
□
It follows immediately that for such an edge probability p,
lim P( Xo = 0) = e-‘ . 
(5.5)
n^w
We next give a more general result describing the asymptotic distribution of the 
number Xd = Xn,d, d > 1 of vertices of any fixed degree d in a random graph. We 
shall see that the asymptotic behavior of Xd for d > 1 differs from the case d = 0 
(isolated vertices). It is closely related to the asymptotic behavior of the expected value 
of Xd and the fact that the property of having a vertex of fixed degree d enjoys two 
thresholds, the first for its appearance and the second one for its disappearance (vertices 
of given degree are dying out when the edge probability p increases).
Theorem 5.3 Let Xd = Xn,d be the number of vertices of degree d, d > 1, in Gn) 
and let A1 = cd/d! while A2 = e~c/d!, where c is a constant. Then, as n to,
(i) Xd 
0 if p « n"(d+1)/d,
(ii) Xd -^ Po(A1) if p ~ cn“(d+1^^d, c <
(iii) Itd := (Xd - EXd)/(Var Xd)V(i) 
2 
N(0,1) if p » n’(d+1)/d, but
pn - log n - d log log n 
-to,
(iv) Xd -^ Po(A2) if pn - log n - d log log n 
c, -to < c < to,
(v) Xd -^ 0 if pn - log n - d log log n to.

68
Vertex Degrees
Proof The proofs of statements (i) and (v) are straightforward applications of the first 
moment method, while the proofs of (ii) and (iv) can be found in Chapter 3 of Bollobas 
[19] (see also Karonski and Rucinski [71] for estimates of the rate of convergence). 
The proof of (iii) can be found in [14]. 
□
The next theorem shows the concentration of Xd around its expectation when in 
Gn,P the edge probability p = c/n, i.e., when the average vertex degree is c.
Theorem 5.4 Let p = c/n, where c is a constant. Let Xd denote the number of 
vertices of degree d in Gnp. Then, for d = O (1), w.h.p.
Proof Assume that vertices of Gnp are labeled 1,2,..., n. We first compute E Xd. 
Thus,
(
n   1\ i c \ d / 
c \ n—1—d
d I j ^1 —j
(5.6)
We now compute the second moment. For this we need to estimate
P(deg(1) = deg(2) = d)
The first line here accounts for the case where {1,2} is an edge, and the second line 
deals with the case where it is not. Thus
nn
Var Xd = ^ ^ [P(deg(i) = d, deg( j) = d) - P(deg(1) = d) P(deg(2) = d)]
"=1 J=1
< y< o( 1) + E Xd < An
i* j=1 
'n'
for some constant A = A (c).
Applying the Chebyshev inequality (Lemma 2.18), we obtain
A
P(|Xd - EXd| > -n1/2) < -2,
which completes the proof.
□

5.1 Degrees of Sparse Random Graphs
69
We conclude this section with a look at the asymptotic behavior of the maximum 
vertex degree when a random graph is sparse.
Theorem 5.5 Let A(Gn,P) (resp. 6(Gn,P)) denote the maximum (resp. minimum) 
degree of vertices of Gn,p.
(i) If p = c/n for some constant c > 0, then w.h.p.
A(G‘,p) ~
log n 
log log n'
(ii) If np = m log n, where m 
c», then w.h.p. 6(Gn,P) ~ A(Gn>P) ~ np.
Proof (i)Let d+ = jogjogn+2gogloglogn]. Then,if d = d-,
(
n — 1\ / c \ d
, - 
d'
< exp{log n - dlog d + O(d)} . 
(5.7)
Let l _ logloglogn Then
Let A ~ log log n . Then
log ' 
1
d log d > 
• 
-(log log n - logloglog n + o (1))
log log n 1 - 2l
log
= 
(loglog n + logloglog n + o (1)). 
(5.8)
log log '
Plugging this into (5.7) shows that A(Gn>P) < d_ w.h.p.
Now let d = d+ and let Xd be the number of vertices of degree d in G„.p. Then
(
n — 1 / - \ d / 
- \ n-d-1
d I (n) [1 - n] 
(5.9)
Now, for vertices v, w, by the same argument as in the proof of Theorem 5.4, we have
P(deg(v) = deg( w) = d) = (1 + o (1)) P(deg(v) = d) P(deg(w) = d),
and the Chebyshev inequality implies that Xd > 0 w.h.p. This completes the proof
of (i).
Notice that statement (ii) is an easy consequence of the Chernoff bounds, Lemma
2.21. Let £ = w"1/3. Then
P(3v : | deg(v) - np| > cnp) < 2ne~s2np^3 = 2n“"1/3^3 = o(n-1).
□

70
Vertex Degrees
Exercises
5.1.1 Recall that the degree of a vertex in Gnp has the binomial distribution Bin(n - 
1,p). Hence,
(
' — 1 \
' . pd (1 - p)n-1-d .
d
Show that, as n 
to,
0
Ti
if p « n-(d+1)/d,
if p ~ cn-1-d+1)/d, c < to, 
if p » n-(d+1)/d) but
pn - log n - d log log n -to,
A2 if pn - log n - d log log n 
c, c < to,
0 
if pn - log n - d log log n 
to,
where
A1 = 
and A2 
.
d! 
d!
5.1.2 Suppose that m = dn/2, where d is constant. Prove that the number of vertices 
k -d
of degree k in Gnm is asymptotically equal to d $! n for any fixed positive 
integer k.
5.1.3 Let p = logn+d l'glog n+c, d > 1. Using the method of moments, prove that the 
number of vertices of degree d in Gn,p is asymptotically Poisson with mean 
e~c 
d! .
5.1.4 Prove parts (i) and (v) of Theorem 5.3.
5.1.5 Verify equation (5.4).
5.1.6 Verify equation (5.6).
5.1.7 Verify equation (5.7).
5.1.8 Verify equation (5.8).
5.1.9 Verify equation (5.9).
5.1.10 Show that if p = O I lo'p I, then w.h.p. the maximum degree in G„,p is O (log n).
5.2 Degrees of Dense Random Graphs
In this section we will concentrate on the case where the edge probability p is constant 
and see how the degree sequence can be used to solve the graph isomorphism problem 
w.h.p. The main result deals with the maximum vertex degree in a dense random graph 
and is instrumental in the solution of this problem.
Theorem 5.6 Let d+ = (n - 1)p + (1 ± e)a/2(n - 1)pq log n, where q = 1 - p. If p 
is constant and e > 0 is a small constant, then w.h.p.

5.2 Degrees of Dense Random Graphs
71
(i) d- < A(Gn,p) < d+.
(ii) 
There is a unique vertex of maximum degree.
Proof We break the proof of Theorem 5.6 into two lemmas.
Lemma 5.7 Let d = (n - 1)p + x^(n - 1)pq, p be constant, x < n13 (log n)2, where 
q = 1 - p. Then
Bd = ‘ 1 pd(1 - p)n~1~d = (1 + o(1))J 
e-12/2.
d 
'2xnpq
Proof Stirling’s formula gives
Bd - (1 + O(1))-i / X--------
'2xnpq
,, 
.. 
. i_ _^_
( (n - 1)q \ 
" 1
n - 1 - d
/((n - 1)p \" 1 
d
(5.10)
Now
d । 
|1+ I * V
(n - 1)p 
\ (n - 1)p /
pq 
x2 q 
x3
= exp x + 
+ O — 
, 
(5.11)
n - 1 2(n - 1) 
n3'2
whereas
1--^- 
n - 1 - d \ 
" 1
(n - 1)*
1 
1 / p \1 "
V (n - 1)q)
= exp
x2 p 
2(n - 1)
(5.12)
So
( d 
\ "-1 (n - 1 - d \1 "-1 _ J 
x2 
( x3 \1
(n - 1)p 
(n - 1)q / 
P(2(n - 1) 
\n3/2
and the lemma follows from (5.10). 
□
The next lemma proves a strengthening of Theorem 5.6.
Lemma 5.8 Let e = 1/10, and p be constant and q = 1 - p .If 
d+ = (n - 1)p + (1 ± e)s)2(n - 1)pq log n,
then w.h.p.
(i) A(G„;P) < d+,
(ii) 
there are Q(n2 E ^1“ e^) vertices of degree at least d-,
(iii) . + / such that deg(w), deg(v) > d_ and | deg(w) - deg(v)| < 10.
Proof We first prove that as x to,
1 e"12/2 (1 " 12) ~ f e"22/2dy - 1 e"12/2• 
(5.13)

72
Vertex Degrees
To see this notice
dy
=1 e-12/2[i - 41+of 4 e"12/2 
1 
XX
(5.14)
We can now prove statement (i). Let Xd be the number of vertices of degree d. Then 
E Xd = nBd and so Lemma 5.7 implies that
E Xd = (1 + o (1)) ------ exp
2 2npq
. I . 
, 
\2)
1 id - (n - 1) p\
2 \ y/(n - 1)pq ]
assuming that
d < dL = (n - 1)p + (log n)2V(n - 1)pq.
Also, if d > (n - 1)p, then
Bd+1 _ (n ~ d - 1)p 
1
Bd " 
(d + 1) q ’
and so if d > dL,
EXd < EX;, < nexp{-Q((logn)4)}.
It follows that
A(G‘,p) < dL w.h.p. 
(5.15)
Now if Yd = Xd + Xd+1 + • • • + XdL for d = d+, then
I. , 
, \2)
1 II - (n - 1) pj
2 \ a/(u - 1)p^
/ 
, 
, \2)
1 p -(n - 1) pj
2 \ ->/(n - 1)p^
(5.16)
1
2
. , 
\ 2>l
A - (n - 1)p j 
y/(n - 1)p^
The justification for (5.16) comes from
i . 
, \2)
1 II - (n - 1) pj
2 \ y/(n - 1)p^
<x>
= O (n) 
y 
g-12/2 = O (g-(log n) 2/3),
i=(log n)2

5.2 Degrees of Dense Random Graphs
73
and
. I . 
, 
\ 2)
1 I d+ - (n - 1)p\
2 \ ->/(n - 1)p^
n~° W
If d = (n - 1)p + x^(n - 1)pq, then from (5.13) we have
1
2
. , 
\ 2)
A - (n - 1)p j
V(n - 1)pq /
(5.17)
Part (i) follows from (5.17).
When d = d_, we see from (5.17) that EYd 
^. We use the second moment
method to show that Yd_ + 0 w.h.p.
d^
E(Yd(Yd - 1)) = n(n - 1) 
P(deg(1) = d1,deg(2) = ^2)
d <di ,d2 
d^
= n(n - 1) 
[p P(d(1) = d1 - 1, d(2) = d2 - 1)
d <d1 ,d2
+ (1 - p) P(d(1) = db d(2) = d2)],
where d(x) is the number of neighbors of 1 in {3,4,.. ,,nj. Note that d (1) and d(2) 
are independent, and
P( d(1) = d1 - 1) = (d7-21) (1 ~ p) = d1 (1 - p) 
P(d(1) = d1) 
("J2)p 
(' - 1 - d1)p
= 1 + (5 (n"1/2).
In 5) we ignore polylog factors.
Hence
E(Yd(Yd - 1))
dL 
r 
t
= n(n - 1) £ [p(d(1) = d1) P(d(2) = d2)(1 + (5(n"1/2))]
d <d1 ,d2 L
dL
= n(n - 1) 
[p(deg(1) = d1) P(deg(2) = d2)(1 + 55(n-1/2))]
d <d1 ,d2 L
= (EYd)(EYd - 1)(1 + (5(n’1/2)),

74
Vertex Degrees
since
P(d(1) = di)
P(deg(1) = di)
ln~2\
= -^T~ (1 - .P)’1 
(£)'
= 1 + (5 («"1/2).
So, with d = d_,
p < 1 eYd) < Var Yd 
d 2 d (E Yd )2/4
= E(Yd(Yd - 1)) + EYd ~(E Yd)2
(E Yd )2/4
= o (1). 
(5.18)
This completes the proof of statement (ii). Finally,
(
\ dr
2 E E 
P (deg(1) = d1, deg (2) = d£
d1=d- |d2-d11<10
= o(1) + ('! ^ E 
[P P(d(1) = d1 - 1) P(d(2) = d2 - 1)
' d1=d_ |d2-d11<10
+ (1 - p) P(d(1) = d1) P(d(2) = d2)].
Now
dr
X X 
P(d(1) = d1 - 1) P(d(2) = d2 - 1)
d1=d_ |d2-d11<10
dr
< 21(1 + C>(«"1/2)) £ [p(d(1) = d1 - 1)]2, 
d1=d_
and by Lemma 5.7 and by (5.13) we have with
1 = d -(„ - 1)) 
,
V(« - 1) p*
dr 
i
[P(d(1) = d1 - 1)]2 - 2-------- I 
e-22dy
^=d 
2^p*2 y=X
1 
, 
_2
= —--------- I e 3 /2dz 
(5.19)
VS^p*' 3=xV2
- —1-------L „-2(1--)2. 
(5.20)
sl&ppqn W2

5.2 Degrees of Dense Random Graphs
75
We get a similar bound for J^=d X |d2-dj |<io [P(^(1) = d1 ] 2 ■ Thus
P(-i(iii)) = o ^n2'~1~2^1~s^
= o (1).
□
Application to graph isomorphism
In this section we describe a procedure for canonically labeling a graph G. It is taken 
from Babai, Erdos, and Selkow [9]. If the procedure succeeds, then it is possible to 
quickly tell whether G = H for any other graph H. (Here = stands for isomorphic as 
graphs.)
Algorithm LABEL
Step 0: Input graph G and parameter L.
Step 1: Relabel the vertices of G so that they satisfy
degG(vi) > degG(/2) > • • • > degG(v„).
If there exists i < L such that degG (v,) = degG (vi+1), then FAIL.
Step 2: For i > L let
Xi = {j e{1,2,...,L} : {v,,vj} 6 E(G)}.
Relabel vertices vl+1, vl+2, ..., vn so that these sets satisfy
Xl+1 > Xl+2 >•••;> Xn
where > denotes lexicographic order.
If there exists i < n such that X, = Xi+1, then FAIL.
Suppose now that the above ordering/labeling procedure LABEL succeeds for G. 
Given an n vertex graph H, we run LABEL on H.
(i) If LABEL fails on H, then G £ H.
(ii) Suppose that the ordering generated on V (H) is w1, w2,..w n. Then
G = H « v, 
w, is an isomorphism.
It is straightforward to verify (i) and (ii) for large n.
Theorem 5.9 Let p be a fixed constant, q = 1 - p, and let p = p1 + *2 and let 
L = 3 log1yp n. Then w.h.p. LABEL succeeds on Gn,p.

76
Vertex Degrees
Proof Part (iii) of Lemma 5.8 implies that Step 1 succeeds w.h.p. We must now show 
that w.h.p. Xi + Xj for all" + # > L. There is a slight problem because the edges from 
/"," > L to /j ,j < L are conditioned by the fact that the latter vertices are those of 
highest degree.
Now fix", j and let G = Gn>) \ {/■, /j}. It follows from Lemma 5.8 that if", j > L, 
then w.h.p. the L largest degree vertices of G and Gn>) coincide. So, w.h.p., we can 
compute X", Xj with respect to G to create Xt, Xj, which are independent of the edges 
incident with /",/j. It follows that if ", j > L, then Xtt = X" and Xj = Xj, and this 
avoids our conditioning problem. Denote by N&(/) the set of the neighbors of vertex 
/ in graph G. Then
P(Step 2 fails)
< ((1) + P(3vf,/j : N(j(/") n {/i,...,/L} = N(j(/j) n {/i,...,/L})
2)()2 + *2) L
= ((1).
□
Corollary 5.10 If 0 < ) < 1 is constant, then w.h.p. G‘,) has a unique automor­
phism, i.e., the identity automorphism. (An automorphism of a graph G = (V, E) is a 
map y : V V such that {1,2} e E if and only if {y (1), ^ (2)} e E.)
See Problem 5.3.
Application to edge coloring
The chromatic index x'(G) of a graph G is the minimum number of colors that can 
be used to color the edges of G so that if two edges share a vertex, then they have a 
different color. Vizing’s theorem states that
A(G) < x'(G) < A(G) + 1.
Also, if there is a unique vertex of maximum degree, then x'(G) = A(G). So, it follows 
from Theorem 5.6 (ii) that, for constant), w.h.p. we have x'{Gn)) = A(G‘;)).
Exercises
5.2.1 Verify equation (5.10).
5.2.2 Verify equation (5.11).
5.2.3 Verify equation (5.12).
5.2.4 Verify equation (5.14).
5.2.5 Verify equation (5.18).

5.2 Degrees of Dense Random Graphs
77
Problems for Chapter 5
5.1 Suppose that c > 1 and that x < 1 is the solution to xe 1 = ce c. Show that 
if c = O (1) is fixed, then w.h.p. the giant component of G‘;),) = ' has 
~ CTT“ (1 “ (1) $) ' vertices of degree $ > 1.
5.2 Show that if 0 < ) < 1 is constant, then w.h.p. the minimum degree 6 in G‘;) 
satisfies
16 - (‘ - 1)* - V2(‘ - 1))* log ‘| < eV2(‘ - 1))* log ‘, 
where * = 1 - ) and s = 1/10.
5.3 Use the canonical labeling of Theorem 5.9 to show that w.h.p. G‘,1/2 has exactly 
one automorphism, the identity automorphism.

6 Connectivity
Whether a graph is connected, i.e., there is a path between any two of its vertices, is 
of particular importance. Therefore, in this chapter we first establish the threshold for 
the connectivity of a random graph. We then view this property in terms of the graph 
process and show that w.h.p. the random graph becomes connected at precisely the 
time when the last isolated vertex joins the giant component. This “hitting time” result 
is the pre-cursor to several similar results.
After this, we deal with $-connectivity, i.e., the parameter which measures the 
strength of connectivity of a graph. We show that the threshold for this property is 
the same as for the existence of vertices of degree $ in a random graph. In fact, a 
much stronger statement, similar to that for connectedness, can be proved. Namely, 
that a random graph becomes $-connected as soon as the last vertex of degree $ - 1 
disappears.
In general, one can observe in many results from Part I of the text that one of the 
characteristic features of Erdos-Renyi-Gilbert random graphs, trivial graph-theoretic 
necessary conditions, such as minimum degree 6(G) > 0 for connectedness, or 6(G) > 
$ - 1 for $-connectivity, becomes sufficient w.h.p.
6.1 Connectivity
The first result of this chapter is from Erdos and Renyi [42].
Theorem 6.1 Let & = 2n (log n + c‘). Then
(0
lim P(G& is connected) = < e e 
'
1
if c‘ 
-to,
if c‘ 
c (constant),
if c‘ 
to.
Proof To prove the theorem we consider, as before, the random graph G’.). It suffices 
to prove that, when ) = log'+c,
P(G‘.) is connected ) e“e c.
We use Theorem 3.4 to translate to G& and then use (3.6) and monotonicity for 
c‘ 
±TO.

6.1 Connectivity
79
Let X$ = X$n be the number of components with k vertices in G„,) and consider 
the complement of the event that G„,) is connected. Then
/ n/2 
\
P(G„,) is not connected) = P H| (G„;) has a component of order k)
$=1
/ n/2 
\
= P |j{X$ > 0} .
U=1
Note that X1 counts here isolated vertices and therefore
n/2
P(X1 > 0) < P(G„;) is not connected ) < P(X1 > 0) + ^ P(X$ > 0).
$=2
Now
n/2 
n/2 
n/2 , 
n/2
?t p(X$ > 0) < J EX$ < J ' k$-2p$~1 (1 - p)$('-$) = J u$.
$=2 
$=2 
$=2 
$=2
Now, for 2 < k < 10,
/ 
\ $-1
U$ < e$n$ logn + c 
e-k(n-10>'
n
71_ 
\ $-1
<(1 + o(1))e $ (1-c) log' 
,
n
and for k > 10,
U, < ( '2. ) $ k $-2 IlOg ' + C 1 $~1 e~$ (log n+c)/2 
$ " k 
n
I e1~c/2+o (1) log n \ $
" 
n1 ' 
’ -
So
n/2J U$ = o(n°(1)-1j. 
(6.1)
$=2
It follows that
P(Gn,) is connected ) = P(X1 = 0) + o(1).
But we already know (see Theorem 5.2) that for p = (log n + c)/n the number of 
isolated vertices in Gn>) has an asymptotically Poisson distribution and therefore, as 
in (5.5),
lim P(X1 = 0) = e-e-c,
and so the theorem follows.
□
It is possible to tweak the proof of Theorem 6.1 to give a more precise result stating 

80
Connectivity
that a random graph becomes connected exactly at the moment when the last isolated 
vertex disappears.
Theorem 6.2 Consider the random graph process {G& }. Let
&1 = min{& : 6(G&) > 1}, 
&* = min{& : G& is connected}.
Then, w.h.p.
... * 
... *
&1 = &c.
Proof Let
&+ = 2 n log n ± 2 n log log n
and
log n ± log log n
)± ~ N ~ 
n 
’
where N = (‘).
We first show that w.h.p.
(i) G&_ consists of a giant connected component plus a set V1 of at most 2 log n 
isolated vertices,
(ii) G&+ is connected.
Assume (i) and (ii). It follows that w.h.p.
&_ < &1 < &*c < &+.
Since G&_ consists of a connected component and a set of isolated vertices V1, to 
create G&+ we add &+ - &_ random edges. Note that &J = &* if none of these edges 
is contained in V1 . Thus
_ . 
.Z 2V z 
z 2 |V112
P(&, < &;.) < ((1) + (&+ - &-)-----------
1 
N - &+
2n((log n)2) log log ‘
< ((1) + —i--------------------------
2n2 - O(n log n)
= ((1).
Thus to prove the theorem, it is sufficient to verify (i) and (ii). 
Let
& log n - log log n 
)_  ---------------------------- ,
V 
n
and let X1 be the number of isolated vertices in G„;)_. Then
EX1 = n(1 - )_)n~1

6.1 Connectivity
81
- ne-np-
~ log n.
Moreover,
EX2 = EX1 + n(n - 1)(1 - p_)2”"3
< EX1 + (EX1)2(1 - p-)-1.
So,
Var X1 < E X1 + 2(EX1)2p_
and
P(X1 > 2log n) = P(|X1 - EX11 > (1 + o(1)) EX1)
- (1 + o(1))(fV + 2P-)
B X1
= o (1).
Having at least 2 log n isolated vertices is a monotone property, and so w.h.p. Gm_ has 
less than 2 log n isolated vertices.
To show that the rest of Gm is a single connected component we let X$, 2 < k < n/2 
be the number of components with k vertices in Gp_. Repeating the calculations for 
p_ from the proof of Theorem 6.1, we have
/ n/2 
\
E ^ X$ = 0 (n°(1)-1) • 
k=2
Let
& = {3 component of order 2 < k < n/2}.
Then
P(G&_ eS) < 0(V) ' ,.p e£)
= o (1),
and this completes the proof of (i).
To prove (ii) (that Gm+ is connected w.h.p.) we note that (ii) follows from the fact 
that G„;P is connected w.h.p. for np - log n c» (see Theorem 6.1). By implication 
Gm is connected w.h.p. if n& - log n to. But
n&
— - log n + log log n. 
(6.2)
□
Exercises
6.1.1 Verify equation (6.1).
6.1.2 Verify equation (6.2).

82
Connectivity
6.2
k-Connectivity
In this section we show that the threshold for the existence of vertices of degree k is 
also the threshold for the k-connectivity of a random graph. Recall that a graph G is 
k-connected if the removal of at most k -1 vertices of G does not disconnect it. In light 
of the previous result it should be expected that a random graph becomes k-connected 
as soon as the last vertex of degree k - 1 disappears. This is true and follows from the 
results of Erdos and Renyi [44]. Here is a weaker statement. The stronger statement is 
left as an exercise, Exercise 6.1.
Theorem 6.3 Let m = 2n (log n + (k - 1) log log n + cn), k = 1,2,.... Then
0 
if cn 
-to,
£ c
lim P(Gm is k-connected) = < e-7F-T7T if cn —> c,
n^w
1 
if Cn
Proof Let
log n+(k - 1) log log n + c
P =--------------------------------- ■
n
We will prove that, in G„,p, with edge probability p above,
(i) the expected number of vertices of degree at most k - 2 is o (1),
(ii) the expected number of vertices of degree k - 1 is approximately -$f^
We have
E(number of vertices of degree t < k - 1)
= nln ■ 11 p- (1 - p)n-1- - n- (log ' 
e~C__
,
\ t / 
t! 
n- 
n(log n) $ 1
and (i) and (ii) follow immediately, see Exercises 6.2.1 and 6.2.2.
The distribution of the number of vertices of degree k -1 is asymptotically Poisson,
as may be verified by the Method of Moments (see Exercise 5.1.3).
We now show that, if
S, T) = {T is a component of G„,p \ S} ,
then
P ^3S, T, |SI < k, 2 < |TI < 2(n - |S|) : ^(S, T)j = o(1).
This implies that if 5(Gnp) > k, then G„,p is k-connected and Theorem 6.3 follows.
|T| > 2 because if T = {/}, then / has degree less than k.
We can assume that S is minimal and then the neighborhood N(T) = S and denote
, = |S|, t = |T|. T is connected, and so it contains a tree with t - 1 edges. Also each 

6.2 k-Connectivity
83
vertex of 5 is incident with an edge from 5 to T, and so there are at least , edges 
between 5 and T. Thus, if p = (1 + o(1))l^p, then
p(35,t)< o(i) + y; y HM---2p-1 P-)p,(1 -p)t(n-s~t} 
tt t2 v - 
y
k-1 (n-s)/2
< p-1 ^ ^ 
■ (te) ■ p ■ etp) 'ne ■ p ■ e~<'n~t'>)j
,=1 
t=2 
,
k-1 (n-s)/2
< p’1 £ £ A-B,, 
(6.3)
,=1 
t=2
where
A = nepe~(n~t)) = e1+o (1) n"1+(t+o (t))/" log n, 
B = ne2tpetp = e2+o(1)-n{t+ow/n log n.
Now if 2 < t < logn, then A = n~1+oand B = O((logn)2). On the other hand, 
if t > log n, then we can use A < n“1/3 and B < n2 to see that the sum in (6.3) is 
o (1). 
□
Exercises
6.2.1 Verify statement (i) of Theorem 6.3.
6.2.2 Verify statement (ii) of Theorem 6.3.
Problems for Chapter 6
6.1 Let k = O (1) and let m $ be the hitting time for minimum degree at least k in the 
graph process. Let t$ be the hitting time for k-connectivity. Show that m$ = t$ 
w.h.p.
6.2 Let m = &1 be as in Theorem 6.2 and let em = (., v), where . has degree 1. Let 
0 < c < 1 be a positive constant. Show that w.h.p. there is no triangle containing 
vertex v.
6.3 Let m = mJ as in Theorem 6.2 and let em = (., v), where . has degree 1. Let 
0 < c < 1 be a positive constant. Show that w.h.p. the degree of v in Gm is at 
least c log n.
6.4 Suppose that n log n « m < n3/2 and let d = 2m/n. Let St (v) be the set of 
vertices at distance i from vertex v. Show that w.h.p. |Sf(v)| > (d/2)! for all 
v e [n] and 1 < " < 2-lop.
6.5 Suppose that m » n log n and let d = m/n. Using the previous question, show 
that w.h.p. there are at least d/2 internally vertex disjoint paths of length at most 
o t°g raa-n m'.' fioir* 
i*l i, ■ 
n /C!
3logd between any pair of vertices in ‘,&.

84
Connectivity
6.6 Suppose that & » n log n and let d = m/n. Suppose that we randomly color 
the edges of G‘& with * colors where * » ^lOg'^2. Show that w.h.p. there is a 
rainbow path between every pair of vertices. (A path is a rainbow if each of its 
edges has a different color.)
6.7 Let G„,„,) be the random bipartite graph with vertex bi-partition V = (A,B), 
A = [1,n],B = [n + 1,2n], in which each of the ‘2 possible edges appears 
independently with probability ). Let ) = log'+"', where w to. Show that 
w.h.p. G„,„,) is connected.
6.8 Show that for every s > 0 there exists c£ > 0 such that the following is true 
w.h.p. If c > c£ and ) = c/n and we remove any set of at most (1 - s)cn/2 
edges from G„,), then the remaining graph contains a component of size at least 
sn/4.
6.9 Show that P(G‘;) is connected) < (1 - (1 - ))'-1)'“1.
6.10 Show that the expected number E(c„,$) of components on $ vertices in G„,) can 
be bounded from above
E(c „,$) < 
1 $ P(Bin(n, 1 - * $) = $),
1 - * $
where * = 1 - ).

7 Small Subgraphs
Graph theory is replete with theorems stating conditions for the existence ofa subgraph 
H in a larger graph G. For example Turan’s theorem [110] states that a graph with ‘ 
vertices and more than (1 - 1^ '2 edges must contain a copy of Kr+1. In this chapter 
we see instead how many random edges are required to have a particular fixed size 
subgraph w.h.p. In addition, we will consider the distribution of the number of copies 
of strictly balanced subgraphs.
From these general results, one can deduce thresholds for small trees, stars, cliques, 
bipartite cliques, and many other small subgraphs which play an important role in the 
analysis of the properties not only of classic random graphs but also in the interpretation 
of characteristic features of real-world networks.
Computing the frequency of small subgraphs is a fundamental problem in network 
analysis, used across diverse domains: bioinformatics, social sciences, and infrastruc­
ture networks studies. The high frequencies of certain subgraphs in real networks give 
a quantifiable method of proving they are not Erdos-Renyi. The distributions of small 
subgraphs are used to evaluate network models, summarize real networks, and classify 
vertex roles, among other things.
7.1 Thresholds
In this section we will look for a threshold for the appearance of any fixed graph H, 
with /h = |V(H)| vertices and eh = \E(H)| edges. The property that a random graph 
contains H as a subgraph is clearly monotone increasing. It is also transparent that 
“denser” graphs appear in a random graph “later” than “sparser” ones. More precisely, 
denote by
d (H) = —, 
/h
(7.1)
the density of a graph H. Notice that 2d(H) is the average vertex degree in H. We 
begin with the analysis of the asymptotic behavior of the expected number of copies 
of H in the random graph G‘;).

86
Small Subgraphs
Lemma 7.1 Let Xh denote the number of copies of H in Gnp,
n 
/ h ! 
e
E Xh = 
peH
/ H aut( H)
where aut (H) is the number of automorphisms of H.
Proof The complete graph on n vertices Kn contains (V') aH distinct copies of H, 
where a H is the number of copies of H in KVH. Thus
E Xh — 
a h peH
/ h
and all we need to show is that
ah X aut(H) = /h
Each permutation a of [/H] = {1,2,..., /H } defines a unique copy of H as follows: 
A copy of H corresponds to a set of eH edges of KVH. The copy H^ corresponding to 
a has edges {(x^p),y^pf) : 1 < i < eH}, where {(xj,yj) : 1 < j < eH} is some 
fixed copy of H in KVH. But H^ = HT,r if and only if for each i there is j such that 
(xT<r("), yT<r(")) = (x^(p,y^(;)), i.e., if t is an automorphism of H. 
□
Theorem 7.2 Let H be a fixed graph with eH > 0. Suppose p = o (n 1^d (H)). Then 
w.h.p. Gnpp contains no copies of H.
Proof Suppose that p = m-1n“1/d(H), where m = m(n) 
to as n 
to. Then
E Xh
n /H ! peH
/H aut(H)
< nVH m-eH n~eH ld (H} = m~eH
Thus
P(Xh > 0) < EXh 
0 as n to.
□
From our previous experience one would expect that when E Xh to as n to 
the random graph Gn,p would contain H as a subgraph w.h.p. Let us check whether 
such a phenomenon also holds in this case. So consider the case when pn1d(Hto, 
where p = wn“1/d (Hand m = m(n) to as n to. Then for some constant cH > 0,
E Xh > chnVH meHn~eH/d(H} = chmeH 
to.
However, as we will see, this is not always enough for Gn,p to contain a copy of a given 
graph H w.h.p. To see this, consider the graph H given in Figure 7.1.

7.1 Thresholds
87
Figure 7.1 A kite
Here /h = 6 and eh = 8. Let) = ' 5/7. Now 1/d(H) = 6/8 > 5/7 and so
E Xh - ch '6-8x5/7 
to.
On the other hand, if H = K4, then
E Xh < ‘4"6x5/7 
0,
and so w.h.p. there are no copies of H and hence no copies of H.
The reason for such “strange” behavior is quite simple. Our graph H is in fact not 
balanced, since its overall density is smaller than the density of one of its subgraphs, 
i.e., of H = K4. So we need to introduce another density characteristic of graphs, 
namely the maximum subgraph density defined as follows:
&(H) = max{d(K) : K c H}. 
(7.2)
A graph H is balanced if &(H) = d(H). It is strictly balanced if d(H) > d(K) for all 
proper subgraphs K c H.
Now we are ready to determine the threshold for the existence of a copy of H in 
G‘;). Erdos and Renyi [43] proved this result for balanced graphs. The threshold for 
any graph H was first found by Bollobas in [19], and an alternative, deterministic 
argument to derive the threshold was presented in [70]. A simple proof, given here, 
is due to Rucinski and Vince [103].
Theorem 7.3 Let H be a fixed graph with e h > 0. Then
lim P(H c G„,p) =
if )‘1^n <h) 0, 
if )'1&(h) to.
0
1
Proof Let w = w(‘) to as ‘ to. The first statement follows from Theorem 7.2. 
Notice that if we choose H to be a subgraph of H with d(H) = & (H) (such a subgraph 
always exists since we do not exclude H = H), then ) = w-1 ‘~1/d (himplies that 
E Xh 0. Therefore, w.h.p. G„.) contains no copies of H, and so it does not contain
H as well.

88
Small Subgraphs
To prove the second statement we use the Second Moment Method. Suppose now 
that p = on~1m(H. Denote by Hi, He,.. ,,Ht all copies of H in the complete graph 
on {1,2,..., n}. Note that
n / H!
/H aut(H) ’
(7.3)
where aut(H) is the number of automorphisms of H. For i = 1,2,..., t let
1
I" =
0
if H" c G‘,p 
otherwise.
Let XH = 2"=11". Then
Var Xh 
Cov<I"’1#= E E(E(I"1#> " (E I")(E 1#»
"=1 J=1 
"=1 J=1
tt
= E E(P(Z" = 1’1# = 1) " P(Z" = 1) P(1# = 1^)
"=1 J=1 
tt
= EE (p( r" = 1’1#= d- p2eH} ■ 
"=1 J=1
Observe that random variables I" and Ij are independent if and only if H" and Hj 
are edge disjoint. In this case Cov(I", Ij) = 0 and such terms vanish from the above 
summation. Therefore, we consider only pairs (H", Hj) with H" n Hj = K for some 
graph K with eK > 0. So,
Var XH = o{ 
n2/H~VK )peH1 ~eK - p2eH
K QH,eK >0
= ^ne/H p2eH 
n~VK p~eK
K QH,eK >0
On the other hand,
E Xh = 
J^H^PeH = Q (nVHpeH).
/H aut(H)
Thus, by the Second Moment Method,
P(Xh = 0) < Var Xh
(E Xh )e
= O
= O
= ( (1).
Hence w.h.p., the random graph Gnp contains a copy of the subgraph H when 
pn1/m(H) to. 
□

7.2 Asymptotic Distributions
89
Exercises
7.1.1 Draw a graph which is (a) balanced but not strictly balanced, (b) unbalanced.
7.1.2 Are the small graphs listed below, balanced or unbalanced? (a) a tree, (b) a 
cycle, (c) a complete graph, (d) a regular graph, (e) the Petersen graph, (f) a 
graph composed of a complete graph on four vertices and a triangle, sharing 
exactly one vertex.
7.1.3 Determine (directly, not from the statement of Theorem 7.3) thresholds p for 
G’,) 2 G, for graphs listed in the previous exercise. Do the same for the 
thresholds of G in G‘,&.
7.2 Asymptotic Distributions
We will now study the asymptotic distribution of the number of copies of a fixed 
graph H in G’,). We start at the threshold, so we assume that ‘)&(Hc, c > 0, 
where & (H) denotes, as before, the maximum subgraph density of H. Now, if H 
is not balanced, i.e., its maximum subgraph density exceeds the density of H, then 
EXH 
to as ‘ 
to, and one can show that there is a sequence of numbers a’,
increasing with ‘, such that the asymptotic distribution of XH/a’ coincides with the 
distribution of a random variable counting the number of copies of a subgraph 
of H for which &(H) = d(K). Note that K is itself a balanced graph. However, the 
asymptotic distribution of balanced graphs on the threshold, although computable, 
cannot be given in a closed form. The situation changes dramatically ifwe assume that 
the graph H whose copies in G’,) we want to count is strictly balanced, i.e., when for 
every proper subgraph K of H, d(K) < d(H) = &(H). The following result is due 
to Bollobas [19] and Karonski and Rucinski [69].
Theorem 7.4 If H is a strictly balanced graph and np&H) 
c, c > 0, then
XH 
Po(T), as ‘ 
to, where T = c/H /aut(H).
Proof Denote, as before, by H1, H2. ,,H— all copies of H in the complete graph 
on {1,2,..., ‘}. For" = 1,2,..., -, let
f 1 if H" c G‘;), 
IH" - 
0 otherwise.
Then XH = £"=1 IHt and the $ th factorial moment of XH, $ = 1,2...,
E(XH)$ = E[XH (XH - 1) ■ ■ ■ (XH - $ + 1)],
can be written as
E( Xh )$ = 2 P (/H"1 = 1’ IH2 = 1’---JH"k = 1)
"1,"2 ,■■■,"$
= D $ + D $,

90
Small Subgraphs
where the summation is taken over all k-element sequences of distinct indices i j from 
{1,2,...,t}, while D$ and D$ denote the partial sums taken over all (ordered) k 
tuples of copies of H which are, respectively, pairwise vertex disjoint (D$) and not all 
pairwise vertex disjoint (D$). Now, observe that
D$ = £ 
P(IHli = 1) P(IHl2 = 1)... P(IHlk = 1)
"1,"2,■■■,"$
= 
' fa ) ') $
\V H ,V H,...,/ H
~ (E Xh )$ .
So assuming that npd(H) = npm(H) c as n to,
c VH 
aut( H)
(7.4)
On the other hand, we will show that
D$ 
0 as n to.
(7.5)
Consider the family 7$ of all (mutually nonisomorphic) graphs obtained by taking 
unions of k not all pairwise vertex disjoint copies of the graph H. Suppose F e 7$ 
has vF vertices (vH < vF < kvH - 1) and eF edges, and let d(F) = eF/vF be its 
density. To prove that (7.5) holds we need the following lemma.
Lemma 7.5 If F e 7$, then d(F) > m (H).
Proof Define
fF = m(H)vF - eF. 
(7.6)
We will show (by induction on k > 2) that fF < 0 for all F e f$. First note that 
fH = 0 and that fK > 0 for every proper subgraph K of H, since H is strictly balanced. 
Notice also that the function f is modular, i.e., for any two graphs F1 and F2,
fF^Fz = fr\ + /F2 - fFxnF2 ■ 
(7.7)
Assume that the copies of H composing F are numbered in such a way that 
H,j n Hi2 + 0. If F = Hf1 U Hi2, then (7.6) and /H1 = fH2 = 0 imply
fHh OHi2 = - fH1 HHi2 < 0-
For arbitrary k > 3, let F' = U$Z1 H. and K = F' n Hik. Then by the inductive 
assumption we have fF> < 0 while fK > 0 since K is a subgraph of H (in extreme 
cases K can be H itself or an empty graph). Therefore,
fF = fF’ + faik - fK = fF' - fK < 0, 
which completes the induction and implies that d(F) > m(H). 
□

7.2 Asymptotic Distributions
91
Let Cf be the number of sequences H"1 , Hi2,..Hik of k distinct copies of H such 
that
$ 
$
V (y Hi.) = {1,2,...,v f } and J Hi. - F.
Then, by Lemma 7.5,
r,,F-./(f
= 01 (np ( -j I = o(1),
and so (7.5) holds. Summarizing,
(
/ 
\ $
aut(H)
and the theorem follows by Lemma 5.1. 
□
The following theorem describes the asymptotic behavior of the number of copies 
of a graph H in G„,P past the threshold for the existence of a copy of H. It holds 
regardless of whether or not H is balanced or strictly balanced. We state the theorem 
but we do not supply a proof (see Rucinski [102]).
Theorem 7.6 Let H be a fixed (nonempty) graph. If npm<--H) 
and n2 (1- p) 
,
then (XH - EXH)/(Var XH)1/2 
N(0,1) as n m
Exercises
7.2 .1 Let fF be a graph function defined as
fF = a /f + b eF,
where a, b are constants, while /f and ef denote, respectively, the number of 
vertices and edges of a graph F. Show that the function fF is modular.
7.2 .2 Let Xe be the number of isolated edges in Gnp. Determine when the random 
variable Xe has asymptotically the Poisson distribution.
7.2 .3 Determine (directly, not applying Theorem 7.4) when the random variable 
counting the number of copies of a triangle in G„;P has asymptotically the 
Poisson distribution.
Problems for Chapter 7
7.1 For a graph G a balanced extension of G is a graph F such that G Q F and m(F) = 
d(F) = m(G). Applying the result of Gyori, Rothschild, and Rucinski [58] 

92
Small Subgraphs
that every graph has a balanced extension, deduce Bollobas’s result (Theorem 7.3) 
from that of Erdos and Renyi (threshold for balanced graphs) .
7 
. 2 Let H be a fixed graph and let) = c'-1^1-H, where c > 0 is a constant . Show that 
w.h.p. all copies of H in G‘;) are induced copies of H. (A copy of H is induced in 
a host graph if restricted to the vertices of the copy is H, i.e., no extra edges.)
7.3 Let H be a fixed graph and let) = c‘~i!,n(H), where c > 0 is a constant. Show 
that w.h.p. no two copies of H in G‘;) are within distance 10 of each other.
7.4 Let I > 3 be fixed. Show that if ‘l“2)(2) » log ‘, then w.h.p. every edge of G‘;) 
is contained in a copy of Kl.
7.5 Suppose that 0 < ) < 1 is constant. Show with the aid of McDiarmid’s inequality 
(see Lemma 9.6) that the number of triangles Z in G‘;) satisfies Z ~ ‘3)216 w.h.p.

8 Large Subgraphs
The previous chapter dealt with the existence of small subgraphs of a fixed size. In 
this chapter we concern ourselves with the existence of large subgraphs, most notably 
perfect matchings and Hamilton cycles. The famous theorems of Hall and Tutte give 
necessary and sufficient conditions for a bipartite and an arbitrary graph respectively 
to contain a perfect matching. Hall’s theorem, in particular, can be used to establish 
that the threshold for having a perfect matching in a random bipartite graph can be 
identified with that of having no isolated vertices.
Having dealt with perfect matchings, we turn our attention to long paths in 
sparse random graphs, i.e., in those where we expect a linear number of edges and 
show that, under such circumstances, a random graph contains a cycle of length 
Q(’) w.h.p.
We next study one of the most celebrated and difficult problems in the first 10 
years after the publication of the seminal Erdos and Renyi paper on the evolution of 
random graphs: the existence of a Hamilton cycle in a random graph, the question 
left open in [43]. The solution can be credited to Hungarian mathematicians: Ajtai, 
Komlos and Szemeredi, and Bollobas. We establish the precise limiting probability 
that G‘;) contains a Hamilton cycle. This is equal to the limiting probability that G‘;) 
has minimum degree 2. It means that a trivial necessary condition for a graph being 
Hamiltonian, i.e., 6(G) > 1, is also sufficient for G‘;) w.h.p.
In the last section of this chapter we consider the general problem of the existence 
of arbitrary spanning subgraphs in a random graph, where we bound the maximum 
degree A( H).
8.1 Perfect Matchings
Before we move to the problem of the existence ofa perfect matching, i.e., a collection 
of independent edges covering all of the vertices of a graph, in our main object of 
study, the random graph G‘;), we will analyze the same problem in a random bipartite 
graph. This problem is much simpler than the respective one for G‘;) and provides a 
general approach to finding a perfect matching in a random graph.

94
Large Subgraphs
Bipartite Graphs
Let G„;„;) be the random bipartite graph with vertex bi-partition V = (A, B), A = 
[1, n], B = [n + 1,2n], in which each of the n2 possible edges appears independently 
with probability p. The following theorem was first proved by Erdos and Renyi [45].
Theorem 8.1 Let m = m(n), c > 0 be a constant, and p = log'+"'. Then
(0
lim P(G„ n p has a perfect matching) =
e 2 ‘
1
if m 
-to,
if m 
c,
if m 
to.
Moreover,
lim P(G„ n ) has a perfect matching) = lim P(8(G„ n )) > 1). 
n^& 
n^<x>
Proof We will use Hall’s condition for the existence of a perfect matching in a 
bipartite graph. It states that a bipartite graph contains a perfect matching if and only 
if the following condition is satisfied:
VS c A, |N(S)|>|S1, 
(8.1)
where for a set of vertices S, N (S) denotes the set of neighbors of S.
It is convenient to replace (8.1) by
VS c A, |S| < ', |N(S)| > |S|, 
(8.2)
VT c B, |T| < ', |N(T)| > |T|. 
(8.3)
This is because if |S| > n/2 and |N(S)| < |S|, then T = B \ N(S) will violate (8.3).
Now we can restrict our attention to S, T satisfying (a) |S| = |T| + 1 and (b) each
vertex in T has at least two neighbors in S. Take a pair S, T with |S| + |T| as small as
possible. If the minimum degree 8 > 1, then |S| > 2.
(i) If |S| > |T| + 1, we can remove |S| - |T| - 1 vertices from |S| - contradiction.
(ii) Suppose 3u e T such that o has less than two neighbors in S. Remove o and its 
(unique) neighbor in | S|-contradiction.
It follows that
P(3v : / is isolated) < P(^ a perfect matching)
< P(3v : / is isolated) + 2P(3S c A, T c B, 2 < $ = |S| < n/2,
|T| = $ - 1,N(S) c T and e(S : T)> 2$ - 2).
Here e(S : T) denotes the number of edges between S and T, and e(S : T) can be 
assumed to be at least 2$ - 2 because of (b) above.

8.1 Perfect Matchings
95
Suppose now that p = log'+c for some constant c. Then let Y denote the number of 
sets 5 and T not satisfying conditions (8.2) and (8.3). Then
p2k~2 (1 - p)k ('-$k
k=2
k-1 / $e (log n + c) \2k 2 
k(1-k/n)
2n e
k=2
(e°nk!n (log n)2\k
k=2 
'
n/2
= £ U k •
k=2
Case 1: 2 < $ < n3/4.
uk < n((e°n 1 logn)2)k.
So
„3 4
> Uk = O
k 
ni~o(1)
k=2 
'n
Case 2: n3^4 < $ < n/2.
u. < n1~kd/2-o(D) 
u k — n
So
n/2£ Uk = o (»-n"4'3)
,3 4
and
P(^ a perfect matching) = P(3 isolated vertex) + o(1).
Let X0 denote the number of isolated vertices in Gn,n,). Then
E X0 = 2n(1 - p)n - 2e~c.
It follows in fact via inclusion-exclusion or the Method of Moments that we have
P(Xo = 0) - e"2e
(8.4)
To prove the case for |m| 
c» we can use monotonicity and (3.6) and the fact that
2c 
_ 
- 
2c
e 
0 if c -to and e 
1 if c to. 
□

96
Large Subgraphs
Nonbipartite Graphs
We now consider Gn,). We could try to replace Hall’s theorem by Tutte’s theorem. A 
proof along these lines was given by Erdos and Renyi [46]. We can however get away 
with a simpler approach based on the expansion properties of Gn,). The proof here 
can be traced back to Bollobas and Frieze [25].
We write Gn,) as the union of two independent copies G1 U G2. We show that w.h.p. 
small sets in G1 have many neighbors. Then we show that if there is a maximum size 
matching in a graph G that does not cover vertex v, then there is a set of vertices A(v) 
such that the addition of any {v, w}, w e A(v) to G would increase the size of the 
largest matching in G. This set has few neighbors in G and so w.h.p. it is large in G1. 
This implies that there are Q(n2) edges (boosters) of the form {x, y} where y e A(x) 
whose addition increases the maximum matching size. This is the role of G2.
Theorem 8.2 Let m = w(n), c > 0 be a constant, and let p = log'+c". Then
(o
lim P(Gn,) has a perfect matching) = • e e 
n even 
.
if C‘ 
-TO,
if Cn 
c,
if C‘ 
TO.
Moreover,
lim P(Gn ) has a perfect matching) = lim P(d(Gn )) > 1). 
n^w 
n^w
Proof We will for convenience only consider the case where cn = m to and 
m = o (log n). If cn -to, then there are isolated vertices w.h.p. and our proof can 
easily be modified to handle the case cn c.
Our combinatorial tool that replaces Tutte’s theorem is the following: We say that a 
matching M isolates a vertex v if no edge of M contains v.
For a graph G we let
p(G) = max {|M| : M is a matching in G} . 
(8.5)
Let G = {V,E) be a graph without a perfect matching, i.e., p(G) < |_|V|/2J. Fix 
v e V and suppose that M is a maximum matching that isolates v. Let So (v, M) = 
{u + v : M isolates m}. If u e So(v, M) and e = {x, y} e M and f = {u,x} e E, 
then flipping e,f replaces M by M' = M + f - e. Here e is flipped-out. Note that 
y e So(v,M').
Now fix a maximum matching M that isolates v and let
A (v,M) = |J So (v,M') 
M'
where we take the union over M' obtained from M by a sequence of flips.
Lemma 8.3 Let G be a graph without a perfect matching and let M be a maximum 
matching and v be a vertex isolated by M. Then |Ng(A(v, M))| < |A(v, M)|.

8.1 Perfect Matchings
97
Proof Suppose that 1 e Ng(A(y, M)) and that f = {u,x} e E, where u e A(y, M). 
Now there exists y such that e = {x,y} e M, else 1 e So(y,M) c A(y,M). We 
claim that y e A(y, M), and this will prove the lemma. Since then, every neighbor of 
A (v, M) is the neighbor via an edge of M.
Suppose that y £ A(v,M). Let M' be a maximum matching that (i) isolates u and 
(ii) is obtainable from M by a sequence of flips. Now e e M' because if e has been 
flipped out, then either 1 or y is placed in A(y, M). But then we can do another flip 
with M', e and the edge f = {u, 1}, placing y e A(v, M), contradiction. 
□
We now change notation and write A(v) in place of A(y, M), understanding that 
there is some maximum matching that isolates v. Note that if u e A(v), then there is 
some maximum matching that isolates u, and so A(u) is well defined. Furthermore, it 
is always the case that if v is isolated by some maximum matching and u e A(v), then 
p (G + {u, v }) = p (G) + 1.
Now let
log n + 0 log log n + m
where 0 > 0 is a fixed integer and m and m = o (log log n).
We have introduced 0 so that we can use some of the following results for the 
Hamilton cycle problem.
We write
Gn,p = Gn,P1 U Gn,P2,
where
log n + 0 log log n + m/2
P1 =---------------------------------- (8.6)
n
and
1 - p = (1 - pi)(1 - p2) so that )2 ~ m. 
(8.7)
2n
Note that Theorem 6.3 implies:
The minimum degree in G„;P1 is at least 0 + 1 w.h.p. 
(8.8)
We consider a process where we add the edges of Gn,P2 one at a time to Gn,P1. We 
want to argue that if the current graph does not have a perfect matching, then there 
is a good chance that adding such an edge {1, y} will increase the size of a largest 
matching. This will happen if y e A(1). If we know that w.h.p. every set S for which 
INg„p1 (S)| < |S| satisfies |S| > an for some constant a > 0, then
(a”) - O (n) 
a2
P(y 6 A(1))> 2 
-, 
(8.9)
22
provided we have only looked at O (n) edges of G‘;P2 so far.
This is because the edges we add will be uniformly random and there will be at 
least (‘”) edges {1, y} where y e A(1). Here given an initial 1 we can include edges 
{1y'} where 1' e 4(1) and y' e A(x').

98
Large Subgraphs
In the light of this we now argue that sets S, with | No’,) (S)| < (1 + 0)|S |,arew.h.p. 
of size Q(n).
Lemma 8.4 Let K = 100(0 + 7). W.h.p. S Q ['], |S| < 2e(n9+ 5 implies |N(S)| > 
(0 + 1)|S|, where N(S) = NGn,pi (S).
Proof Let a vertex of graph G„;P1 be large if its degree is at least 2 = l^Oy, and 
small otherwise. Denote by LARGE and SMALL the set of large and small vertices 
in G„;P1, respectively.
Claim 1 W.h.p. if v, w e SMALL, then dist(v, w) > 5.
Proof If v, w are small and connected by a short path P, then v, w will have few 
neighbors outside P, and conditional on P existing, v having few neighbors outside P 
is independent of w having few neighbors outside P. Hence,
P(3v, w e SMALL in G„,P1 such that dist(v, w) < 5)
< 2 '3(l°g')4 X7 (log')$ (logn)l9+1)/100 ■ e "/2
~ 
'4 
\f“0 
$! 
' log'
4 I (log '/ (log ')(9+1)/100 • e"‘ 2 \2
< 2‘(log ')-------------------------------
2! 
'log '
(
(log '} ° I1! 
2log n \
^-g-^------(100e) TOO
'
= O (‘-3/4)
(8.10)
The bound in (8.10) holds since 2! > (|)2 and .^ > 100 for k < 2, where
(log')$ 
(log')!9+1)/100 . e~^!2
Uk k! 
' log' 
'
Claim 2 W.h.p. Gn,P1 does not have a 4-cycle containing a small vertex.
Proof
P(3a 4-cycle containing a small vertex)
(log n)/100 , 
,
< 4'4)4 g (' $ 4)$ (1 - P1)'-4-$ = o(1).
(8.11)
= ((1).
□
□

8.1 Perfect Matchings
99
Claim 3 Let K be as in Lemma 8.4. Then w.h.p. in Gn,P1, e(S) < "K/' for every
S C[«], |S|< 
. 
’
Proof
F (3|S|< JL and e(S) > 
"^ (")( 
® 
),log '
\ 
2eK 
K I 
jK V , log «/K/ 1
= ((1). 
(8.12)
□
Claim 4 Let K be as in Lemma 8.4. Then, w.h.p. in Gn,P1, if S Q LARGE, 
|S|< v.'.^k,then\N(S)|>(0 + 4)|S|. 
’
Proof Let T = N(S),, = |S\,t = |T|. Then we have
<t.-' I log ' 
S1 log ' 
2|S 1 log '
e (S u T) > e cs,T) > -^ - 2e <S) > —-----------—.
Then if |T| < (0 + 4)|S|, we have |S U T| < (0 + 5)|S| < ,’K and
e < S U T .. 
( ± - K ) log ' . 
„
This contradicts Claim 3.
We can now complete the proof of Lemma 8.4. Let | S| < 2eg"^K and assume that 
G„;P1 has minimum degree at least 0 + 1.
Let S1 = S n SMALL and S2 = S \ S1. Then
IN (S)|
> |N(S1)| + |N(S2)| - IN(S1) n S21 - |N(S2) n S11 - IN(S1) n N(S2)|
> |N(S1)| + |N(S2)| - IS2I - IN(S2) n S11 - |N(S1) n N(S2)|.
But Claims 1 and 2 and minimum degree at least 0 + 1 imply that
|N(S1)| > (0 + 1)|S11, |N(S2)n S11 < min{|S11, |S21}, |N(S1) n N(S2>| < IS21-
So, from this and Claim 4 we obtain
|N(S)| > (0 + 1)|S11 + (0 + 4)|S21-3|S21 = (0 + 1)|S|.
□
Now go back to the proof of Theorem 8.2 for the case c = m to. Let edges 
of G",P2 be {/i, /2,..., fs} in random order, where , ~ w/4. Let Go = G„;P1 and 
G" = G„;P1 + {/1, /2,..., f"} for " > 1. It follows from Lemmas 8.3 and 8.4 that with 

100
Large Subgraphs
p(G) as in (8.5), and if p(G,) < ‘/2 then, assuming that G„,)1 has the expansion 
claimed in Lemma 8.4, with 0 = 0 and a = ^1^,
2
P( p (G"+i) > p (G") + 1 | /i, /2 
f") > y,
(8.13)
see (8.9).
It follows that
P(G‘;) does not have a perfect matching) < ((1) + P(Bin(, a2/2) < ‘/2) = ((1).
To justify the use of the binomial distribution in the last inequality we used the notion 
of stochastic dominance and Lemma 2.29. 
□
Exercises
8.1.1 Verify equation (8.4).
8.1.2 Verify equation (8.11).
8.1.3 Verify equation (8.12).
8.2 Long Paths and Cycles
In this section we study the length of the longest path and cycle in G‘;) when ) = c/’, 
where c = O(log ‘), most importantly for c is a large constant. We have seen in 
Section 3.2 that under these conditions, G‘;) will w.h.p. have isolated vertices and so 
it will not be Hamiltonian. We can however show that it contains a cycle of length 
Q(’) w.h.p.
The question of the existence of a long path/cycle was posed by Erdos and Renyi 
in [43]. The first positive answer to this question was given by Ajtai, Komlos, and 
Szemeredi [3] and by de la Vega [111]. The proof we give here is due to Krivelevich, 
Lee, and Sudakov. It is subsumed by the more general results of [76].
Theorem 8.5 Let) = c/’, where c is sufficiently large but c = O (log ‘). Then w.h.p.
(a) Gn,P has a path of length at least (1 - 61o)g c j ‘,
(b) Gn,p has a cycle of length at least (1 - 12 log c j ‘.
Proof We prove this theorem by analyzing simple properties of depth-first search 
(DFS). This is a well-known algorithm for exploring the vertices of a component of 
a graph. We can describe the progress of this algorithm using three sets: is the set 
of unexplored vertices that have not yet been reached by the search. is the set of 
dead vertices. These have been fully explored and no longer take part in the process. 
A = {ai, a2,.. .,ar } is the set of active vertices, and they form a path from a1 to a+. 
We start the algorithm by choosing a vertex / from which to start the process. Then 
we let
A = {/} and D = 0 and U = [‘] \ {/} and + = 1.

8.2 Long Paths and Cycles
101
We now describe how these sets change during one step of the algorithm DFS.
Step (a) If there is an edge {a+, 0} for some 0 e U, then we choose one such 0 and 
extend the path defined by A to include 0:
a++i 
0; A 
A U {0}; U 
U \ {0}; + 
+ + 1.
We now repeat Step (a).
If there is no such 0, then we perform Step (b).
Step (b) We have now completely explored a+.
D D U {a+}; A 
A \ {a+}; + 
+ - 1.
If + > 1, we go to Step (a). Otherwise, if U = 0 at this point, then we terminate 
the algorithm. If U + 0, then we choose some / e U to restart the process 
with + = 1. We then go to Step (a).
We make the following simple observations:
• A step of the algorithm increases |D | by one or decreases | U| by one, and so at some 
stage we must have | D | = | U | = , for some positive integer ,.
• There are no edges between D and U because we only add a+ to D when there are 
no edges from a+ to U and U does not increase from this point on.
Thus at some stage we have two disjoint sets D, U of size , with no edges between 
them and a path of length | A| - 1 = ‘ - 2, - 1. This plus the following claim implies 
that G‘;) has a path P of length at least (1 - hlogrj ‘ w.h.p. Note that if c is large then
a > 3 log C implies c > 2 log (—1. 
c
Claim 5 Let 0 < a < 1 be a positive constant. If) = c/n and c > -| log (-^), then 
w.h.p. in G‘;), every pair of disjoint sets S1,S2 of size at least a’ - 1 are joined by at 
least one edge.
Proof The probability that there exist sets S1, S2 of size (at least) a’ - 1 with no 
joining edge is at most
/ 
\ 2
(‘‘_ 1 (1 - ))(a'-1) 2 = ((1). 
(8.14)
□
To complete the proof of the theorem, we apply Claim 5 to the vertices S1, S2 on the 
two subpaths P1, P2 of length ^log.‘ at each end of P. There will w.h.p. be an edge 
joining S1, S2, creating the cycle of the claimed length. 
□

102
Large Subgraphs
Krivelevich and Sudakov [77] used DFS to give simple proofs of good bounds on 
the size of the largest component in G„,) for ) = 1'-, where e is a small constant. 
Problems 8.4.19, 8.4.20, and 8.4.21 elaborate on their results.
Exercises
8.2.1 Verify equation (8.14).
8.3 Hamilton Cycles
This was a difficult question left open in [43]. A breakthrough came with the result of 
Posa [101]. The precise theorem given below can be credited to Komlos and Szemeredi 
[73], Bollobas [20], and Ajtai, Komlos and Szemeredi [4].
Theorem 8.6 Let) = log '+log‘log '+c‘. Then
(0
lim P(G„;) has a Hamilton cycle) =
1
if c‘ 
-to,
if c‘ 
c,
if c‘ 
to.
Moreover,
lim P(G„ ) has a Hamilton cycle ) = lim P(6(G„ )) > 2). 
'^TO 
'^w
Proof We will first give a proof of the first statement under the assumption that 
c‘ = w to, where w = ((log log ‘). Under this assumption, we have 6(G‘,)) > 2 
w.h.p., see Theorem 6.3. The result for larger ) follows by monotonicity.
We now set up the main tool, viz. Posa’s lemma. Let be a path with endpoints 
a, b, as in Figure 8.1. Suppose that b does not have a neighbor outside of P.
P
ax 
b
Figure 8.1 The path P
Notice that P' in Figure 8.2 is a path of the same length as P, obtained 
by a rotation with vertex a as the fixed endpoint. To be precise, suppose that 
P = (a,...,1,2,2',■■■,b’,b) and {b,x} is an edge where 1 is an interior vertex 
of P. The path P' = (a,... ,x,b,b2 ’, 2) is said to be obtained from P by a 
rotation.

8.3 Hamilton Cycles
103
Figure 8.2 The path P' obtained after a single rotation
Now let END = END (P) denote the set of vertices / such that there exists a path 
P/ from to / such that P/ is obtained from P by a sequence of rotations with vertex 
fixed as in Figure 8.3.
a
b
b
b
Figure 8. 
3 A sequence of rotations
Here the set end consists of all the white vertices on the path drawn in Figure 8.4.
Lemma 8.7 If / e P \ END and / is adjacent to 0 e END, then there exists 
1 e END such that the edge {v,x} e P.
Proof Suppose to the contrary that 1,2 are the neighbors of / on P, that /, 1,2 i END 
and that / is adjacent to 0 e END. Consider the path P0. Let {+,-} be the

104
Large Subgraphs
Figure 8. 
4 The set
neighbors of / on P0. Now {+, -} = {1,2} because if a rotation deletes {/, 2} say, 
then / or 2 becomes an endpoint. But then after a further rotation from 0 we see that 
1 e END or 2 e END.
t
r
w
Figure 8.5 One of +, - will become an endpoint after a rotation
□
Corollary 8.8
|N(END)| < 2|END|.
It follows from Lemma 8.4 with Q = 1 that w.h.p. we have
| END | > a’,
where a =
1
12eA”
□
(8.15)
We now consider the following algorithm that searches for a Hamilton cycle in a 
connected graph . The probability )1 (as defined in (8.6)) is above the connectivity 
threshold, and so G‘;)1 is connected w.h.p. Our algorithm will proceed in stages. At 
the beginning of Stage $ we will have a path of length $ in and we will try to grow it 
by one vertex in order to reach Stage $ + 1. In Stage ‘ - 1, our aim is simply to create a 
Hamilton cycle, given a Hamilton path. We start the whole procedure with an arbitrary 
path of .

8.3 Hamilton Cycles
105
Algorithm Posa:
(a) Let P be our path at the beginning of Stage $. Let its endpoints be 10,20. If 10 or 
20 have neighbors outside P, then we can simply extend P to include one of these 
neighbors and move to Stage $ + 1.
(b) Failing this, we do a sequence of rotations with 10 as the fixed vertex until one of 
two things happens: (i) We produce a path Q with an endpoint 2 that has a neighbor 
outside of Q. In this case we extend Q and proceed to Stage $ +1. (ii) No sequence 
of rotations leads to Case (i). In this case let END denote the set of endpoints of 
the paths produced. If 2 e END, then P2 denotes a path with endpoints 10,2 that 
is obtained from P by a sequence of rotations.
(c) If we are in Case (b)(ii), then for each 2 e END we let END (2) denote the set 
of vertices 3 such that there exists a longest path Q3 from 2 to 3 such that Q3 is 
obtained from P2 by a sequence of rotations with vertex 2 fixed. Repeating the 
argument above in (b) for each 2 e END, we either extend a path and begin Stage 
$ + 1 or we go to (d).
(d) Suppose now that we do not reach Stage $ + 1 by an extension and that we have 
constructed the sets END and END(2) for all 2 e END. Suppose that G contains 
an edge (2,3) where 3 e END (2). Such an edge would imply the existence of 
a cycle C = (3, Q2,3). If this is not a Hamilton cycle, then connectivity implies 
that there exist u e C and / £ C such that u, / are joined by an edge. Let 0 be a 
neighbor of u on C and let P' be the path obtained from C by deleting the edge 
(u, 0). This creates a path of length $ + 1, viz. the path 0, P', /, and we can move 
to Stage $ + 1.
A pair 3,2 where 3 e END (2) is called a booster in the sense that if we added this 
edge to G',)j then it would either make the graph Hamiltonian or make the current 
path longer. We argue now argue that G‘;)2 can be used to “boost” P to a Hamilton 
cycle, if necessary. We observe now that when G = G‘;)1, |END | > an w.h.p., see 
(8.15). Also, |END(2)| > a’ for all 2 e END. So we will have Q(’2) boosters.
For a graph G let A(G) denote the length of a longest path in G when G is 
not Hamiltonian and let A(G) = ‘ when G is Hamiltonian. Let the edges of G‘;)2 
be {f1, f2, ■ ■ ■, f,} in random order, where , ~ w/4. Let G0 = G‘;)1 and G, = 
G‘;)1 +{fl, f2, ■ ■■, f"} for" > 1. It follows from Lemmas 8.3 and 8.4 that if A (G,) < ‘, 
then, assuming that G‘;)1 has the expansion claimed in Lemma 8.4,
a2
P(A(G"+1) > A(G") + 1 | f1, f2,..., f) > —, 
(8.16)
see (8.9), replacing A(v) by END(/).
It follows that
P(G‘;) is not Hamiltonian) < ((1) + P(Bin(s,a2!2) < ‘) = ((1). 
(8.17)

106
Large Subgraphs
To complete the proof we need to discuss the case where cn 
c. So choose
)2 = 'iOg1lOg' and let
1 - ) = (1 - )1)(1 - )2)■
Then we apply Theorem 8.5(a) to argue that w.h.p. G‘;)1 has a path of length 
‘ (1_ o floglog'U 
' 1 o log ‘ 
.
Now, conditional on G‘;)1 having minimum degree at least 2, the proof of the 
statement of Lemma 8.4 goes through without change for 0 = 1, i.e., 5 c [‘], |51 < 
10')00 implies |N(5)| > 2|5|. We can then use the extension-rotation argument that we 
used to prove Theorem 8.6 in the case when cn 
to. This time we only need to close
' inlog log ‘ t 
o n <1 m ' to ' O I ' i 
Tliiik (R 111 ic ry'iil oppH lov
o log’ 
cycles and we have i lOgiOgn edges. Thus (8.17) is replaced by
P(G’,p is not Hamiltonian | 5(G’;)1) > 2)
< ((1)4. P (Bin (-2^. 10-8( < C2'(oglog' ) = ((1) (8.18, 
log log ' 
log '
for some constants c1, c2. We used Corollary 2.26 for the second inequality.
Exercises
8.3.1 Verify equation (8.18).
8.4
Spanning Subgraphs
Consider a fixed sequence H(d'^ of graphs where ’ = |V(H1 d^)| to. In particular, 
we consider a sequence Qd of d-dimensional cubes where ’ = 2d and a sequence of 
2-dimensional lattices Ld of order ’ = d2. We ask when G’;) or Gn>& contains a copy 
of H = H1 d> w.h.p. We give a condition that can be proved in quite an elegant and easy 
way. This proof is from Alon and Furedi [8].
Theorem 8.9 Let H be fixed sequence of graphs with ‘ = |V (H)| 
to andmaximum
degree A, where (A2 + 1)2 < ’. If
a > 10logL'/(A2 + 1)j
P 
L'/(A2 + 1)J
(8.19)
then Gnp contains an isomorphic copy of H w.h.p.
Proof To prove this we first apply the Hajnal-Szemeredi theorem to the square H2 of 
our graph H. Recall that we square a graph ifwe add an edge between any two vertices 
of our original graph which are at distance 2. The Hajnal-Szemeredi theorem states that 
every graph with ’ vertices and maximum vertex degree at most d is d + 1-colorable 
with all color classes of size L’/(d + 1)J or |"’/(d + 1)1, i.e, the (d + 1)-coloring is 
equitable.

8.4 Spanning Subgraphs
107
Since the maximum degree of H2 is at most A2, there exists an equitable A2 + 1- 
coloring of H2 which induces a partition of the vertex set of H, say U = U(H), into 
A2 + 1 pairwise disjoint subsets U1, U2,..UA2+1, so that each U$ is an independent 
set in H2 and the cardinality of each subset is either |_'/( A2 + 1)J or \n/(A2 + 1)].
Next, partition the set V of vertices of the random graph Gnp into pairwise disjoint 
sets V1, V2VA2+1, so that |U$ | = |V$ | for $ = 1,2,..., A2 + 1.
We define a one-to-one function f : U V, which maps each U$ onto V$, resulting 
in a mapping of H into an isomorphic copy of H in G„,p. In the first step, choose an 
arbitrary mapping of U1 onto V1. Now U1 is an independent subset of H and so G„,p [V1] 
trivially contains a copy of H [U1]. Assume, by induction, that we have already defined
f : U1 U U2 U ■ ■ ■ U U$ V1 U V2 U ■ ■ ■ U V$,
and that f maps the induced subgraph of H on U1 U U2 U ■ ■ ■ U U$ into a copy of 
it in V1 U V2 U ■ ■ ■ U V$. Now, define f on U$+1 using the following construction. 
Suppose first that U$+1 = {.1 ,u2um} and V$+1 = {v1, v2,..., vm}, where m e 
{L'/(A2 + 1)J, |n/(A2 + 1)]}.
Next, construct a random bipartite graph G&& p, with a vertex set V = (X, Y), 
where X = {x1,x2,... ,xm} and Y = {y1, y2,.. ,,ym}, and connect xt and yj with an 
edge if and only if in G„;P the vertex v7- is joined by an edge to all vertices f (u), where 
u is a neighbor of ut in H which belongs to U1 U U2 U ■ ■ ■ U U$ .Hence, we join xt with 
yj if and only if we can define f (ut) = vj.
Note that for each i and j, the edge probability p* > p A and that edges of G &$& p, are 
independent of each other since they depend on pairwise disjoint sets of edges of G„;P. 
This follows from the fact that U$+1 is independent in H2. Assuming that condition 
(8.19) holds and that (A2 + 1)2 < n, then by Theorem 8.1, the random graph G&$& p. 
has a perfect matching w.h.p. Moreover, we can conclude that the probability that there 
is no perfect matching in G&$& p, is at most ^*1)‘. It is here that we have used the 
extra factor 10 on the RHS of (8.19). We use a perfect matching in G($)(m, m,p*) to 
define f, assuming that if xt and y 7- are matched then f (ut) = v j .To define our mapping 
f : U V we have to find perfect matchings in all G ($^(m,m,p*),$ = 1,2,..., A2 + 1. 
The probability that we can succeed in this is at least 1 - 1/n. This implies that G„,p 
contains an isomorphic copy of H w.h.p. 
□
Exercises
8.4 .1 Let n = 2d and suppose that d to and p > 2- + od(1), where od(1) is a 
function that tends to zero as d 
to. Show that w.h.p. G„,p contains a copy of
a d-dimensional cube Qd.
,.,(' 
‘ 1/4
8.4 .2 Let n = d2 and p = ( ' ' 
) 
, where m(n), d 
to. Show that w.h.p. G„,p
contains a copy of the 2-dimensional lattice Ld.

108
Large Subgraphs
Problems for Chapter 8
8.1 Consider the bipartite graph process r&, & = 0,1,2,..., ‘2, where we add the 
‘2 edges in A x B in random order, one by one. Show that w.h.p. the hitting time 
for r& to have a perfect matching is identical with the hitting time for minimum 
degree at least 1.
8.2 Show that
(0
lim P(G’,) has a near perfect matching) = - e“c“e 
‘ odd 
!
if c‘ 
-to,
if c‘ 
c,
if c‘ 
to.
A near perfect matching is one of size |_‘/2_|.
8.3 Show that if ) = log'+l-$~1'loglog"+"', where $ = O(1) and w 
to, then w.h.p.
G',‘’) contains a $ -regular spanning subgraph.
8.4 Consider the random bipartite graph G with bipartition A, B, where | A| = |B| = 
‘. Each vertex a e A independently chooses |"2 log ‘) random neighbors in B. 
Show that w.h.p. G contains a perfect matching.
8.5 Let G = (X, Y, E) be an arbitrary bipartite graph where the bipartition X, Y 
satisfies | X | = |Y | = ‘. Suppose that G has minimum degree at least 3‘/4. Let 
) = K lon ', where K is a large constant. Show that w.h.p. G) contains a perfect 
matching.
8.6 Suppose that ‘ = 2& is even, that ‘) » log ‘ and that s is an arbitrary positive 
constant. Show that w.h.p. G„,) \ H contains a perfect matching for all choices 
of subgraph H whose maximum degree A(H) < 
- e) ‘).
8.7 Show that if ) = logn+l-$~1'loglog"+"', where $ = O(1) and w 
to, then w.h.p.
G„,) contains [$/2J edge disjoint Hamilton cycles. If $ is odd, show that in 
addition there is an edge disjoint matching of size |_‘/2J. (Hint: Use Lemma 
8.4 to argue that after “peeling off” a few Hamilton cycles, we can still use the 
arguments of Sections 8.1 and 8.3.)
8.8 Let &$ denote the first time that G& has minimum degree at least $. Show that 
w.h.p. in the graph process (i) G& contains a perfect matching and (ii) G&• 
contains a Hamilton cycle.
8.9 Show that if ) = logn+loglogn+to, where w to, then w.h.p. G„,„,) con­
tains a Hamilton cycle. (Hint: Start with a 2-regular spanning subgraph from 
Problem 8.3. Delete an edge from a cycle. Argue that rotations will always pro­
duce paths beginning and ending at different sides of the partition. Proceed more 
or less as in Section 8.3.)
8.10 Show that if ) = logn+loglog"+"', where ‘ is even and w 
to, then w.h.p.
G„,) contains a pair of vertex disjoint ‘/2-cycles. (Hint: Randomly partition [‘] 
into two sets of size ‘/2. Then move some vertices between parts to make the 
minimum degree at least 2 in both parts.)

8.4 Spanning Subgraphs
109
8.11 Show that if 3 divides ‘ and ‘)2 » log ‘, then w.h.p. G‘,) contains ‘/3 vertex 
disjoint triangles. (Hint: Randomly partition [‘] into three sets A, B, C of size 
‘/3. Choose a perfect matching M between A and B and then match C into M.)
8.12 Let ) = (1 + c)lo'' for some fixed £ > 0. Prove that w.h.p. G„,) is Hamilton 
connected, i.e., every pair of vertices are the endpoints of a Hamilton path.
8.13 Show that if ) = ^1+g'log' for £ > 0 constant, then w.h.p. G„,) contains a copy 
of a caterpillar on ‘ vertices. The diagram below is the case ‘ = 16.
8.14 Show that for any fixed £ > 0 there exists cs such that if c > cs then G‘;) 
contains a cycle of length (1 - e)‘ with probability 1 - e“cg '/10.
8.15 Let ) = (1 + e)lo'' for some fixed £ > 0. Prove that w.h.p. G„,) is pancyclic, 
i.e., it contains a cycle of length $ for every 3 < $ < ‘. (See Cooper and Frieze 
[36] and Cooper [34, 35].)
8.16 Show that if) is constant then
P(G‘;) is not Hamiltonian) = O(e“n('))).
8.17 Let be a tree on ' vertices and maximum degree less than c1 log '. Suppose 
that T has at least c2‘ leaves. Show that there exists K = K(c1, c2) such that if 
) > k log ‘ then g„,) contains a copy of T w.h.p.
8.18 Let) = 1000 and G = G„,). Show that w.h.p. any red-blue coloring of the edges 
of G contains a monochromatic path of length 1000. (Hint: Apply the argument 
of Section 8.2 to both the red and blue subgraphs of G to show that if there is 
no long monochromatic path, then there is a pair of large sets 5, T such that no 
edge joins 5, T. This question is taken from Dudek and Praiat [40].
8.19 Suppose that ) = ‘“a for some constant a > 0. Show that if a > 1, then 
w.h.p. G‘;) does not contain a maximal spanning planar subgraph, i.e., a planar 
subgraph with 3‘ - 6 edges. Show that if a < 1 then it contains one w.h.p. (see 
Bollobas and Frieze [26]).
8.20 Show that the hitting time for the existence of $ edge-disjoint spanning trees 
coincides w.h.p. with the hitting time for minimum degree $ for $ = O(1) (see 
Palmer and Spencer [97]).
8.21 Consider the modified greedy matching algorithm where you first choose a 
random vertex 1 and then choose a random edge {1,2} incident with 1. Show 
that applied to G„,&, with & = c‘, that w.h.p. it produces a matching of size 
(2+((1)- -4 
)'.
8.22 Let X1? X2,..., XN, N = (2) be a sequence of independent Bernouilli random 
variables with common probability ). Let £ > 0 be sufficiently small.
(a) Let ) = 1^ and let $ = 7l|g-'. Show that w.h.p. there is no interval I of 
length $‘ in [N] in which at least $ of the variables take the value 1.

110
Large Subgraphs
(b) Let) = 1’’^ and let N0 = ^‘2-. Show that w.h.p.
0E x> -
"■=1
£(1 + £)'
2
‘2/3
8.23 Use the result of Problem 8.22(a) to show that if ) = 1^ then w.h.p. the 
maximum component size in Gn,) is at most 7|og-n. (See [77].)
8.24 Use the result of Problem 8.22(b) to show that if ) = 1nr then w.h.p Gn) 
contains a path of length at least .

9 Extreme Characteristics
This chapter is devoted to the extremes of certain graph parameters. The extreme 
values of various statistics form the basis of a large part of graph theory. From Turan’s 
theorem to Ramsey theory, there has been an enormous amount of research on which 
graph parameters affect these statistics. One of the most important results is that of 
Erdos, who showed that the absence of small cycles did not guarantee a bound on 
the chromatic number. This incidentally was one of the first uses of random graphs to 
prove deterministic results.
In this chapter, we look first at the diameter of random graphs, i.e., the extreme 
value of the shortest distance between a pair of vertices. Then we look at the size of the 
largest independent set and the related value of the chromatic number. One interesting 
feature of these parameters is that they are often highly concentrated. In some cases 
one can say that w.h.p. they will have one of two possible values.
9.1 Diameter
In this section we will first discuss the threshold for G„,) to have diameter d when 
d > 2 is a constant. The diameter diam(G) of a connected graph G is the maximum 
over distinct vertices /, o of dist(/, o) where dist(/, o) is the minimum number of 
edges in a path from / to 0 .
Theorem 9.1 Let d > 2 be a fixed positive integer. Suppose that c > 0 and
)d ‘d~1 = log('2/c).
Then w.h.p. diam(G„;)) is either d or d + 1.
Proof (a) w.h.p. diam(G) > d.
Fix / e V and let
N$(/) = {o : dist(/,o) = $}■ 
(9.1)
It follows from Theorem 5.5 that w.h.p. for 0 < $ < d,
|N$ (/)| < A$ - (‘))$ - (‘ log ‘)$'d = ((n). 
(9.2)
(b) w.h.p. diam(G) < d + 1.

112
Extreme Characteristics
Fix v,w e [n]. Then for 1 < k < d, define the event
Tk = | N$ (v)|e 1$ =
(n~))$ , (2'T)$
Then for k < \d/2"| we have
P(not 7$ |7i,..., 7$-i)
/ 
/ 
$-1 
\
= P Bin n - ^ IN"(v)|, 1 -(1 - /’1 V/ (/)! £ 1$ 
i=0
3 /np\$~1 
/np\$
< P Bin n - o(n), 
p <
\ 
\ 
4 \ 2 / r) V 2 1)
+ P ^Bin (n - o(n), (2np)$~1 pj > (2np)$j
< exp |-O ((np)$
= 0(n-3).
So with probability 1 - 0(n“3),
IN Ld/2j(/)l
np \ L^/2J
“2
If X = NLd/2j (v) and Y = N[d/2^ (o), then either
X n Y + 0 and dist(v, o) < |_d/2J + (d/2) = d,
or since the edges between X and Y are unconditioned by our construction,
‘) d 
/ np \d
P(^ an X : Y edge ) < (1 - p)l 2 2 < exp j p)
< exp{-(2 - o(1))np log n} = o(n“3).

9.1 Diameter
113
So
P(3v, o : dist(v, o) > d + 1) = o(n 1).
□
This theorem can be strengthened to provide the limiting probability that the diam­
eter is actually d or d + 1. See for example Theorem 7.1 of [52].
We turn next to a sparser case and prove a somewhat weaker result.
Theorem 9.2 Suppose that p = '" *'g' where m 
m. Then
diam(G„ )) ~ Jogw 
w.h.p.
’ log np
Proof Fix v e [n] and let Nt = Nt(v) be as in (9.1). Let N<$ = Ut<$ Nt• Using the 
proof of Theorem 5.5(ii) we see that we can assume that
(1 - m 1^3)np < deg(x) < (1 + m l;3pip 
for all i e [n]. 
(9.3)
It follows that if y = m 1/3 and
i log n - log 3 log n 
$0 
-------- r——,
log np + y log np
then w.h.p.
IN<$01 < ^ ((1 + y)np)$ < 2((1 + y)np)$0 ^—n 
,
o 
3 + o
and so the diameter of Gn) is at least (1 - o(1)) l)o)<ggn'.
We can assume that np = n(as larger p are dealt with in Theorem 9.1. Now fix 
v,w e [n] and let Nt be as in the previous paragraph. Now consider a Breadth-First
Search (BFS) from v that constructs N1, N2,..N$1 where
3 log n
$1 
.— •
5 log np
It follows that if (9.3) holds then for $ < $ 1 we have
|Nt<$ | < n3/4 and |N$ |p < n~1/5. 
(9.4)
Observe now that the edges from Nt to [n] \ N<t are unconditioned by the BFS up to 
layer $ and so for i e [n] \ N<$,
P(x 6 N$+1 I N<$) = 1 - (1 - p)1^1 > p$ = IN$ Ip (1 - n-1/5). 
(9.5)
The events i e N$+1 are independent, and so | N$+11 stochastically dominates the 
binomial Bin(n - n3/4, p$). Assume inductively that |N$| > (1 - y)$ (np)$ for some 
$ > 1. This is true w.h.p. for $ = 1 by (9.3). Let $ be the event that (9.4) holds. It 
follows that
E(|N$+11 | ^$) > np|N$|(1 - O(n’1/5)).

114
Extreme Characteristics
It then follows from the Chernoff-Hoeffding bounds (Theorem 2.20) that
y2
P(|N$+1| < ((1 - y) np)k+1 < exp - V4-\Nk\np = O 
constant
There is a small point to be made about conditioning here. We can condition on (9.3) 
holding and then argue that this only multiplies small probabilities by 1 + o (1) if we 
use P(A | B) < P(A)/P(B).
It follows that if
, 
log n 
log n
$ 2 “ --------------------------------
2(log np + log(1 - y)) 
2 log np’
then w.h.p. we have
I N$21 > n1/2.
Analogously, if we do BFS from w to create N', i = 1,2,..., $2, then |N$ | > n1/2. If 
N<$2 A N<$ + 0, then dist(v, w) < 2$2 and we are done. Otherwise, we observe that 
the edges E (N$2, N$2) between N$2 and N'k^ are unconditioned (except for (9.3)) and so
P(E(N$2,N'k2) = 0) < (1 - p)n1/2*nV2 < n-.
If E(N$2, N',') + 0, then dist(v, w) < 2$2 + 1 and we are done. Note that given (9.3), 
2
all other unlikely events have probability O(n anyconstant) of occurring, and so we 
can inflate these latter probabilities by n2 to account for all choices of v, w. This 
completes the proof of Theorem 9.2. 
□
Exercises
9.1.1 Verify equation (9.5).
9.1.2 Suppose that 0 < p < 1 is constant. Show that w.h.p. G„;) has diameter 2.
9.2 Largest Independent Sets
Let a(G) denote the size of the largest independent set in a graph G.
The following theorem for a dense random graph was first proved by Matula [85].
Theorem 9.3 Suppose 0 < p < 1 is a constant and b = ^^. Then w.h.p. 
a(^n,p ) ~ 2 logfe n-
Proof Let X$ be the number of independent sets of order $ and let
$ = (2 logfo n).
Then,
E X$ = M (1 - p)( 2)
$

9.2 Largest Independent Sets
115
ne
$(1 _ p)1^2
\ $ 
(1 - p)$/2)
\ $ 
e
$(1 _ p)1^2
= ((1).
To complete the proof we shall use Janson’s inequality. To introduce this inequality we 
need the following notation. Fix a family of n subsets D", i e [n]. Let R be a random 
subset of [N], N = ('), such that for , e [N] we have 0 < P(, e R) = qs < 1. The 
elements of R are chosen independently of each other and the sets D", i = 1,2,.. ,,n. 
Let ^i be the event that D" is a subset of R. Moreover, let I" be the indicator of the 
event ^,. Note that I" and Ij are independent if and only if D" n D j = 0. We let
Sn - I1 + I2 + ■ ■ ■ + In,
and
n
P = ESn = g E(I").
We write i ~ j if D" n D# + 0. Then, let
A = ^ E< I" Ij) = P + A’ 
(9.6)
{"J }:"~ j
where
A= 
£ 
E( I" Ij). 
(9.7)
{"J }:"~ j 
"± j
As before, let ^(x) = (1 + x) log( 1 + x) - x. Now, with Sn, A, ^ given above, one can 
establish the following upper bound on the lower tail of the distribution of Sn.
Lemma 9.4 (Janson’s inequality) For any real t, 0 < t < p,
wS < P - n < ew Ly(~t/p)p2 U exo I- — 1 
\Sn — p t y Ei exp 
_ i 2^ exp _ i .
A 
2A)
(9.8)
Let now
$ = L2 logfo n - 5 logfo log nJ.
Let
A = 
P(S", Sj are independent in Gn>)),
",j
S" ~s#

116
Extreme Characteristics
where S1,S2, ...,S(are all the k-subsets of [n] and S" ~ 5j if and only if 
I Si n Sj |> 2. 
$
So, finishing the proof, we see that
K x$ 0)£ exp (_<5Xf |.
2A
Here we apply the inequality in the context of X$ being the number of k -cliques in 
the complement of Gn.p. The set [N] will be the edges of the complete graph, and 
the sets D" will be the edges of the k-cliques. Now
a 
(') <1 - p)<2) s‘.2 (’$:$ )($) (1 - p>®~®
(EX$)2 ■ 
(111(1 - p)<5>)2
k in-k.\ /$\
= Z 
(1 - ))-(2)
7=2 
W
k
7=2
Notice that for j > 2,
= 
*z2 (i _ p)->< (i + o ()) 
1p.
u j n - 2k + ] + 1 ] + 1 
n 
n(# + 1)
Therefore,
Ui «1 + o(D) ($2) 
2 -p)-»-2IW11'2
.2 
n 
#!
<(1+o(1)) (—(1 - p)-y < 1.
‘#
So
(EX$)2 > 1 > n2(1 - p)
A ~ $.2 ~ 
k5 
’
Therefore, by Janson’s inequality,
P(Xk = 0) < e~a(n2/(logn)5'>. 
(9.9)
□
Matula used the Chebyshev inequality, and so he was not able to prove an 
exponential bound like (9.9). This will be important when we come to discuss the 
chromatic number. We now consider the case where p = d/n and d is a large constant, 
i.e., when a random graph is sparse. Frieze [51] proved
Theorem 9.5 Let e > 0 be a fixed constant. Then for d > d (e) we have that w.h.p.
2n
a(G‘,p)) 
—j-(log d - log log d - log 2 + 1)
d

9.2 Largest Independent Sets
117
In this section we will only prove that if p = d/n and d is sufficiently large, then 
w.h.p.
\ 
2 logd . £ logd
a(G',p)------ -.— n < ----- -— n. 
(9.10)
d 
d
This will follow from the following. Let X$ be as defined above. Let
1 
(2 - £/8) log d 1 
(2 + £/8) log d
k o 
-n and $1 
-n.
dd
Then,
P |a(G',p)- E(a(G'.p))| > ^g^n < exp !-Q ^log^ nfi (9.11) 
8d 
d2
I I (log d2 \ 1
P(Xk1 > 0) < exp -QI v g 7 n . 
(9.12)
f ((log d3/2 \ 1
P(X$o > 0) > exp -O 
.' n . 
(9.13)
d 2
Let us see how (9.10) follows from these three inequalities. Indeed, (9.11) and (9.13) 
imply that
E(a(G',p)) > $o - £logdn. 
(9.14)
8d
Furthermore, (9.11) and (9.12) imply that
£ log d
E(a(G',p)) < $i + 
n. 
(9.15)
8d
It follows from (9.14) and (9.15) that
\ko - E(a(G',p))| < £^gdn.
2d
We obtain (9.10) by applying (9.11) once more.
Proof of (9.11): This follows directly from the following useful lemma.
Lemma 9.6 (McDiarmid’s inequality) Let Z = Z (W1, W2,..WN ) be a random 
variable that depends on N independent random variables W1, W2,..., WN. Suppose 
that
| Z (W1, ...,Wi,..., Wn ) - Z (W1, ...,W;,...,WN )| < a
for all i = 1,2,..., N and W1, W2,..., WN, W.. Then for all t > 0 we have
___ _ _ , f t2 
1
P(Z > E Z +1)< exp ~~^~2 , 
i=1 ai
and
___ _ _ , 
( t2 'I
P(Z < EZ -1)< exp -—^-2 ■
"=1 ci )

118
Extreme Characteristics
If Z = a (Gn,p), then we write Z = Z (Y2, Y3,Yn), where Yt is the set of edges 
between vertex i and vertices [i - 1] for i > 2. Y2,Y3,.Yn are independent and 
changing a single Yt can change Z by at most 1. Therefore, for any t > 0 we have
P(|Z - E(Z)| > t) < exp
t2
2n - 2
Setting t = s 80^ d n yields (9.11).
Proof of (9.12): The First Moment Method gives
/ n \ / d \ (2) f 
/ 8 log d)2 \ 1
P(X$1 > 0)< I 1 - - 
< exp -Ql1 g > n. 
(9.16)
Proof of (9.13): Now, after using the upper bound given in Lemma 2.9, we get
1 
< E(X$o)
P(X$o > 0) - E(X$o)2
(1 - p)-®
$0E
J=0
(9.17)
$0 
(#d 
# jd2 \
— ■ exp + O 
x exp
n 
2n 
n2
($0 - j)2
v°i j $0e $0 
j j- 2$0 b2 
[ $21
< b 
■ 
■ exp + x exp -
I j n I 2n 
n 
n
# =0
$0
=s / # ■ 
J=0
(9.18)
(The notation A <b B is shorthand for A = O (B) when the latter is considered to be 
ugly looking.)
We first observe that (A/x)1 < eA/e for A > 0 implies that
$0e 
j
So,
(log -)3/4 
(j2- 2j$01 
(((log d)3/2 \ 1
j < J0 = - 
n 
/ # < exp 
+^0 = exp O 2 
2- 
n .
d3/2 
2n n 
d2
(9.19)

9.2 Largest Independent Sets
119
Now put
1
a log d n, 
where 
< a < 2 - .
d ’ 
d1/2 (log d )V4 
4
jd , 2$0l 4e log d (a log d 4 log d
2n 
n I ad P | 2 d
$o id 2$0 
— ■ exp + —
Then
$ oe
J
4e log d 
(4 log d 1
= 
exp 
d~]
< 1. 
(9.20)
To see this note that if f (a) = ad 1“"22 then f increases between d“1/2 and 2/log d 
after which it decreases. Then note that
min | f (d~1/2), f (2 - e)| > 4e exp
Thus /j < 1 for j > j0 and (9.13) follows from (9.19). 
□
(4 log d 1
Id I'
Exercises
9.2.1 Verify equation (9.16).
9.2.2 Verify equation (9.20).
9.2.3 Let p = d/n, where d is a positive constant. Let 5 be the set of vertices of 
'll l.i'ill o !°g 
ill'll 11 11 S' 11 I ti ll iHl'n'i ll i'll I l.'l
degree at least 3 log log ’. Show ttrat w.n.p. 0 is an independent set.
9.2.4 Let p = d/n, where d is a large positive constant. Use the First Moment Method 
to show that w.h.p.
2n
a(Gn^p) 
— (log d - log log d - log 2 + 1 + e)
d
for any positive constant e.

120
Extreme Characteristics
9.3 Chromatic Number
Let x(G) denote the chromatic number of a graph G, i.e., the smallest number of 
colors with which one can properly color the vertices of . A coloring is proper if no 
two adjacent vertices have the same color.
We will first describe the asymptotic behavior of the chromatic number of dense 
random graphs. The following theorem is a major result, due to Bollobas [22]. The 
upper bound without the 2 in the denominator follows directly from Theorem 9.3. An 
intermediate result giving 3/2 instead of 2 was already proved by Matula [86].
Theorem 9.7 Suppose 0 < ) < 1 is a constant and b = 11). Then w.h.p.
'
X(®‘,)) ~ 
■
2 log '
Proof By Theorem 9.3,
''
X (G',)) — 
X 
'
a(G„,)) 
2 logfo ‘
Let v = (log’’)2 and $0 = 2 log& ' _ 4 log& log& '. It follows from (9.9) that
P(3S : 151 > o, S does not contain an independent set of order > $0)
(
X f f 2 XA 
' 
2
exp Q 5 
v 
(log n)5
= ((1).
So assume that every set of order at least v contains an independent set of order at least 
$0. We repeatedly choose an independent set of order $0 among the set of uncolored 
vertices. Give each vertex in this set a new color. Repeat until the number of uncolored 
vertices is at most v. Give each remaining uncolored vertex its own color. The number 
of colors used is at most
nn 
 
1- v 
. 
$0-----------2 log n
□
We shall next show that the chromatic number of a dense random graph is highly 
concentrated.
Theorem 9.8 Suppose 0 < ) < 1 is a constant. Then
( -2
P(Lx(Gn>)) - EX(G',p)| > -) < 2 exp - 2n
Proof Write
X = Z (Yi,Y2,...,Y‘), 
(9.23)
where
Y# = {(",j) e E(G‘,)) :" < #}.
(9.21)
(9.22)

9.3 Chromatic Number
121
Then
| Z (Y1,Y2,...,Y‘)- Z (Y1,Y2,...,Y" ,...,Y‘)| < 1
and the theorem follows by Lemma 9.6. 
□
Greedy Coloring Algorithm
We show below that a simple greedy algorithm performs very efficiently. It uses twice 
as many colors as it “should” in the light of Theorem 9.7. This algorithm is discussed 
in Bollobas and Erdos [24] and Grimmett and McDiarmid [55]. It starts by greedily 
choosing an independent set 1 and at the same time giving its vertices color 1.
1 is removed and then we greedily choose an independent set 2 and give its vertices 
color 2 and so on, until all vertices have been colored.
Algorithm GREEDY
• 
$ is the current color.
• A is the current set of vertices that might get color $ in the current round.
• 
U is the current set of uncolored vertices.
• C$ is the set of vertices of color $, on termination.
begin
$ <— 0, A <— [‘], U <— [‘], C$ <— 0.
while U + 0 do
$ <— $ + 1 A <— U
while A + 0
begin
Choose / e A and put it into C$
U 
U \{/}
A A \ ({/} U N(/))
end
end
Theorem 9.9 Suppose 0 < ) < 1 is a constant and b = ^). Then w.h.p. algorithm 
GREEDY uses approximately n/logb ' colors to color the vertices of G‘;).
Proof At the start of an iteration the edges inside U are unexamined. Suppose that
\Ul> v = 
2■
(logb ')2
We show that approximately logb ' vertices get color $, i.e., at the end of round $, 
|C$ | - logb '.

122
Extreme Characteristics
Each iteration chooses a maximal independent set from the remaining uncolored 
vertices. Let k0 = logh n - 5 logh logfo n. Then
P(3 T : |T| < k0,T is maximally independent in U)
$0 I \
<£ ' (1 "))(2) <1 _(1 "p)t)$0"- - e"2(logfo”)3• (9.24) 
ti V
So the probability that we fail to use at least k0 colors while \ U| > v is at most
ne“ 2 (logfc v) = 0 (1) _
So w.h.p. GREEDY uses at most
nn 
— + v ~  ------- colors,
ko 
logfo n
We now put a lower bound on the number of colors used by GREEDY. Let
k 1 = logfo n + 2 logfo logfo n.
Consider one round. Let Uo = U and suppose .1 ,m2,... e C$ and Ui+1 = U \ 
({.;■} U N(a")). Then
E(|U"+1| \U" )<|U"|(1 - p),
and so, for i = 1,2,..., 
E|U"| < n(1 - p)".
So
P (k 1 vertices colored in one round) 
-—-—-y,
(logfo n)2
and
P(2k 1 vertices colored in one round) < -. 
n
So let
1 
if at most k1 vertices are colored in round i,
8" = 
1
0 otherwise.
We see that
P(8" = 1|81,82,...,8"_1) > 1 - 
* 
.
(logfo n)2
So the number of rounds that color more than k1 vertices is stochastically dominated by 
a binomial with mean n/(logfo n)2. The Chernoff bounds imply that w.h.p. the number 
of rounds that color more than k1 vertices is less than 2n/(logfo n)2. Strictly speaking,

9.3 Chromatic Number
123
we need to use Lemma 2.29 to justify the use of the Chernoff bounds. Because no 
round colors more than 2$1 vertices, we see that w.h.p. GREEDY uses at least
‘ - 4$i‘/(iogfc ‘)2 
$1
--------colors. 
iog '
□
Exercises
9.3.1 Verify equation (9.22).
9.3.2 Verify equation (9.24).
Problems for Chapter 9
9.1 Let) = K log '/‘ for some large constant K > 0. Show that w.h.p. the diameter 
of G‘;) is ©(log ‘/log log ‘).
9.2 Suppose that 1 + s < ‘) = ((log ‘), where s > 0 is constant. Show that given 
A > 0, there exists B = B(A) such that
P diam(K) > B-^og-’ < n~A, 
log ')
where K is the giant component of G‘;).
9.3 Complete the proof of Theorem 9.5.
Proceed as follows: let & = d/(logd)2 and partition [‘] into ‘0 = && sets 
Si, S2,..., S‘0 of size &. Let p (G) be the maximum size of an independent set 
S that satisfies |S n S" | < 1 for " = 1,2,..., ‘0. Use the proof idea of Theorem 
9.5 to show that w.h.p.
2‘
P(G„>p) > $_E = — (log d - log log d - log 2 + 1 - e). 
d
9.4 Let ) = ' where c > 1 is constant. Consider the greedy algorithm for 
constructing a large independent set : choose a random vertex / and put / into 
. Then delete / and all of its neighbors. Repeat until there are no vertices left.
Show that w.h.p. this algorithm chooses an independent set of size at least lo|^‘.
9.5 Prove that if w = w (‘) 
<x> then there exists an interval I of length w‘1/2/log ‘
such that w.h.p. x(G‘;1/2) e I (see Scott [107]).
9.6 A topological clique of size , is a graph obtained from the complete graph K, 
by subdividing edges. Let -c(G) denote the size of the largest topological clique 
contained in a graph G. Prove that w.h.p. -c(G‘;1/2) = 0(‘1/2).
9.7 Suppose that H is obtained from G‘;1/2 by planting a clique C of size 
& = ‘1/2 log ‘ inside it. Describe a polynomial time algorithm that w.h.p. 
finds C. (Think that an adversary adds the clique without telling you where 
it is.)
9.8 Show that if d > 2$ log $ for a positive integer $ > 2, then w.h.p. G(‘, d/’) is 
not $-colorable. (Hint: Consider the expected number of proper $-colorings.)

124
Extreme Characteristics
9.9 Let p = d/n for some constant d > 0. Let A be the adjacency matrix of Gnpp. 
Show that w.h.p. d1 (A) ~ A1/2, where A is the maximum degree in Gn,p. (Hint: 
the maximum eigenvalue of the adjacency matrix of K1m is rn1/2.)
9.10 A proper 2-tone k-coloring of a graph G = (V, E) is an assignment of pairs of 
colors Cv c [k], |Cv | = 2 such that |Cv n C0 | < d(/, w), where d(/, w) is the 
graph distance from / to w. If x2 (G) denotes the minimum k for which there 
exists a 2-tone coloring of G, show that w.h.p. x2<’G.p) ~ 2x(Gsn,p). (This 
question is taken from [10].)
9.11 Suppose that H is a graph on vertex set [n] with maximum degree A = n(®. 
Let p be constant and let G = Gn>p + H. Show that w.h.p. x(G) ~ x<’G.p) for 
all choices of H.
9.12 The set chromatic number xs (G) of a graph G = (V, E) is defined as follows: 
Let C denote a set of colors. Color each / e V with a color f (/) e C. Let 
Cv = {f (w) : {/, w} e G}. The coloring is proper if Cv + C0 whenever 
{/, w} e E. xs is the minimum size of C in a proper coloring of G. Prove that 
if 0 < p < 1 is constant, then w.h.p. x, f^n,p) ~ + log2 n, where + = log 21/s 
and , = min {*21 + (1 - ql)2 :1 = 1,2,.. J, where q = 1 - p. (This question 
is taken from Dudek, Mitsche and Pralat [39].)

Part III
Modeling Complex Networks


10 Inhomogeneous Graphs
Thus far, we have concentrated on the properties of the random graphs G„,& and G„,). 
These classic random graphs are not very well suited to model real-world networks. For 
example, the assumption in G‘;) that edges connect vertices with the same probability 
and independently of each other is not very realistic. To become closer to reality, we 
make a first modest step and consider a generalization of G‘;) where the probability 
of an edge {", j } is )t# and is not the same for all pairs", j. We call this the generalized 
binomial graph. Our main result on this model concerns the probability that it is 
connected.
After this we move onto a special case of this model, viz. the expected degree model 
introduced by Chung and Lu. Here )"# is proportional to 0"0# for weights 0" on 
vertices. In this model, we prove results about the size of the largest components.
The final section introduces a tool, called the configuration model, to generate 
a close approximation of a random graph with a fixed degree sequence. Although 
initially promoted by Bollobas, this class of random graphs is sometimes called the 
Molloy-Reed model.
10.1 Generalized Binomial Graph
Consider the following natural generalization of the binomial random graph G‘;), first 
considered by Kovalenko [75].
Let V = {1,2,...,n} be the vertex set. The random graph G„,P has vertex set V and 
two vertices" and j from V," + j, are joined by an edge with probability )"# = )"# (‘), 
independently of all other edges. Denote by
P = [ )"# ]
the symmetric ‘ x ‘ matrix of edge probabilities, where )"" = 0. Put *"# = 1 - )"# and 
for", j e {1,2,..., ‘} define
''
Q"=n *"#, 
=x q" ■

128
Inhomogeneous Graphs
Note that " is the probability that vertex " is isolated and ' is the expected number 
of isolated vertices. Next let
"$
min
1 #1 <j2 < ■■■<:#$ <‘ * "ji''' *"j$ ■
Suppose that the edge probabilities )"# are chosen in such a way that the following 
conditions are simultaneously satisfied as ‘ 
to:
max Q" 
0, 
(10.1)
i <" <‘
lim 2„ = 2 = constant, 
'^<X
and
72
(10.2)
(10.3)
$
= c -1.
The next two theorems are due to Kovalenko [75]. We will first give the asymptotic 
distribution of the number of isolated vertices in G„,P, assuming that the above three 
conditions are satisfied. Itis a generalization of the corresponding result for the classical 
model G‘;) (see Theorem 5.2(ii)).
X"# -
Theorem 10.1 Let 0 denote the number of isolated vertices in the random graph
G‘,P. If conditions (10.1), (10.2), and (10.3) hold, then
2$ 
lim P(Xq = $) = — e“A 
‘^^ 
$!
for $ = 0,1,..., i.e., the number of isolated vertices is asymptotically Poisson dis­
tributed with mean 2.
Proof Let
1 with prob. )"#,
0 with prob. *"# = 1 - )"#.
Denote by X", for " = 1,2,..., ‘, the indicator of the event that vertex " is isolated in
G„,P. To show that X0 converges in distribution to the Poisson random variable with 
mean 2 one has to show (see Lemma 5.1) that for any natural number $ ,
v^ 
2$ 
z_„
E 2j 
X"1 X"2...X"$ 
— 
(10.4)
\1 <"1<"2<■■■<"$ <'
as ‘ 
to. But
$
E (X"4 X"2 ••• X"$) = n P (X"+ = 1|X"1 = ••• = X"+_1 = 1), 
(10.5)
+=1
where in the case of + = 1 we condition on the sure event.
Since the LHS of (10.4) is the sum of E (X"1 X"2 ■ ■ ■ X"$) over all "1 < ■ ■ ■ < "$,

10.1 Generalized Binomial Graph
129
we need to find matching upper and lower bounds for this expectation. Now 
P (Xir = 11 Xi1 = ■ ■ ■ = Xir_1 = 1) is the unconditional probability that ir is not adjacent 
to any vertex j + i1,..., ir-1 and so
P (X"r = 1 |X,-i =
"r-1
n y-=1 * "r j 
n,=1 *"r is
Hence
Qir < P (X"r = 1 |X"i =
It follows from (10.5) that
"r-1
Q Q.
R"r ,r-1 
R"r $
Qi 
Qi
Qi1 ■■■ Qi$ < E(X"1 ■■■ X"$) 
. 
(10.6)
&"1 $ 
^i$ $
Applying conditions (10.1) and (10.2), we get that
Q"1 
Q"1 
Qi$ = $7 
Q"1 ''' Q"$ - $7 
Q"1 ''' Qi$
1<"1 <■■■<"$ <n 
' 1 <"1^---^ir <n 
' 1 <"1,...,i$ <n
- $7 Yq Q2( S 
Qi1 •” Qi$-2)
> ^n 
k!
J$ 
(max Qi )in-1 ^7, 
i 
$ !
(10.7)
as n 
ot.
Now,
yn Qi 
y'/2 _1 yn Qi
and if umsu^^j^_1 a men iinisu.^^j$_1 $7 1 ^j"_ 1 ^.j,
dicts (10.3). It follows that
> eA - 1, which contra-
Therefore,
A$
$7
1 <i1 <...<i$ <n
as n 
to.
Combining this with (10.7) gives us (10.4) and completes the proof of Theorem 10.1.
□
One can check (see Exercise 10.1.1) that the conditions of the theorem are satisfied 
when
log n + Xi#
Pi# =--------- --------’ 

130
Inhomogeneous Graphs
where 1" # are uniformly bounded by a constant.
The following theorem shows that under certain circumstances, the random graph 
G„,P behaves in a similar way to G„,) at the connectivity threshold.
Theorem 10.2 If the conditions (10.1), (10.2), and (10.3) hold, then
lim P(G„ P is connected) = e“A.
'^O> 
’
Proof To prove this we will show that if (10.1), (10.2), and (10.3) are satisfied then 
w.h.p. G„,P consists of X0 + 1 connected components, i.e., G„,P consists of a single 
giant component plus components that are isolated vertices only. This, together with 
Theorem 10.1, implies the conclusion of Theorem 10.2.
Let U c V be a subset of the vertex set V. We say that U is closed if Xi# = 0, 
where Xi;J- is defined in Theorem 10.1, for every " and j, where " e U and j e V \ U. 
Furthermore, a closed set U is called simple if either U or V \ U consist of isolated 
vertices only. Denote the number of nonempty closed sets in G„,P by Y1 and the number 
of nonempty simple sets by Y. Clearly Y1 > Y. We will prove first that
liminf E Y > 2eA - 1.
'^<X
(10.8)
Denote the set of isolated vertices in G„,P by J. If V \ J is not empty then Y = 2X°+1 -1 
(the number of nonempty subsets of J plus the number of their complements, plus V 
itself). If V\ J = 0 then Y = 2' -1. Now, by Theorem 10.1, for every fixed $ = 0,1,...,
$
lim P(Y = 2$+1 - 1) = e-A —.
‘^^ 
$!
Observe that for any I > 0,
i
EY > ^(2$+1 - 1)P(Y = 2$+1 - 1) 
$=0
and hence
1 
$.„-A
liminf EY > V(2$+1 - 1)'’ , 
‘^^ 
$!
$=0
So,
i 
liminf EY > lim V(2$+1 
'^^ 
l^co
$=0
2 $ e~A 
»-er = 2eA - 1,
which completes the proof of (10.8).
We will show next that
lim sup E Y1 < 2eA - 1.
'^TO
(10.9)

10.1 Generalized Binomial Graph
131
To prove (10.9) denote by Z$ the number of closed sets of order k in Gn>P so that 
Y1 = 2'=i Z$. Note that
Z $ = J Z"i ..."$,
"1<...<!$
where Zi1 .._,$ indicates whether set 1$ = {i1 ■ ■ ■ i$} is closed. Then
EZ"i„.„,.$ = P(Xtj = 0,i e 1$,j 11$) = 
]“[ qtj.
i eI$ ,j <tIk
Consider first the case when k < n/2. Then
n
riieI$,1 <j<n q"j
qij = -F---------
i elk,j<tlk 
" eI$’J eI$ q"j
eft Hj^k qiJ
Hence
Now, (10.3) implies that
E Z $
(
„ 
\ $
y Qi 
£$Rik 
i=1
n/2
lim sup
”^~ $31
E Z$ < eA - 1.
To complete the estimation of E Z$ (and thus for E Y1), consider the case when k > n/2. 
For convenience let us switch k with n - k, i.e., consider E Zn_$, when 0 < k < n/2. 
Notice that E Zn = 1 since V is closed. So for 1 < k < n/2,
E Zn-$ = £ H qi# ■ 
"1 <...<"$ ie.Ik,jjlk
But q"# = qj" so, for such k, E Zn_$ = E Z$. This gives
limsupEY1 < 2(eA - 1) + 1, 
n^w
where the +1 comes from Zn = 1. This completes the proof of (10.9). 
Now,
P(Y1 > Y) = P(Y1 - Y > 1) < E(Y1 - Y).
Estimates (10.8) and (10.9) imply that
limsupE(Y1 - Y) < 0, 
n^w
which in turn leads to the conclusion that
lim P(Y1 > Y) = 0, 
n^<x>
i.e., asymptotically, the probability that there is a closed set that is not simple tends to 
zero as n to. It is easy to check that X0 < n w.h.p. and therefore Y = 2x°+1 -1 w.h.p. 
and so w.h.p. Y1 = 2X0+1 - 1. If Gn>P has more than X0 +1 connected components, then 
the graph after the removal of all isolated vertices would contain at least one closed set,

132
Inhomogeneous Graphs
i.e., the number of closed sets would be at least 2Xo+1. But the probability of such an 
event tends to zero and the theorem follows. 
□
We finish this section by presenting a sufficient condition for G„;P to be connected 
w.h.p. as proven by Alon [7].
Theorem 10.3 For every positive constant b there exists a constant c = c(b) > 0 so 
that if, for every nontrivial S c V,
y, Pij * c log n, 
i es,j eV\S
then the probability that G„;P is connected is at least 1 - n~b.
Proof In fact Alon’s result is much stronger. He considers a random subgraph GPe 
of a multigraph G on n vertices, obtained by deleting each edge e independently with 
probability 1 - pe. The random graph G„;P is a special case of GPe when G is the 
complete graph Kn. Therefore, following in his footsteps, we will prove that Theorem 
10.3 holds for GPt, and thus for G„,P.
So, let G = (V, E) be a loopless undirected multigraph on n vertices, with probability 
pe, 0 < pe < 1, assigned to every edge e e E and suppose that for any nontrivial 
S c V, the expectation of the number Es of edges in a cut (S, V \ S) of GPe satisfies
E Es = y Pe > c log n. 
(10.10)
e e(S,V \S)
Create a new graph G ' = (V,E') from G by replacing each edge e by k = c log n 
parallel copies with the same endpoints and giving each copy e' of e a probability 
p'e’ = Pel$.
Observe that for S c V,
E E's = 
y p'e, = E ES.
e'e(S,V \S)
Moreover, for every edge e of G, the probability that no copy e' of e survives in a 
random subgraph G'p, is (1 - pe/k)$ > 1 - pe and hence the probability that GPe is 
connected exceeds the probability of G', being connected, and so in order to prove 
Pt
the theorem it suffices to prove that
P(GP, is connected) > 1 - n~b. 
(10.11)
P
To prove this, let E'1 U E’2 U ■ ■ ■ U E$ be a partition of the set E’ of the edges of G’ such 
that each E) consists of a single copy of each edge of G. Fori = 0,1,..., k define G1 as 
follows. GO is the subgraph of G’ that has no edges, and for alli > 1, G1 is the random 
subgraph of G' obtained from Gl_ 1 by adding to it each edge e' e Ei independently, 
with probability p'e,.
Let C be the number of connected components of G1. Then we have Co = n and 
we have G$ = GP,. Let us call the stage i, 1 < i < k, successful if either Ci_1 = 1 

10.1 Generalized Binomial Graph
133
(i.e., Gt_ 1 is connected) or if Ct < 0.9C"-i. We will prove that
P(C"-i = 1 or C" < 0.9Cj_1 |O'_1) > 1.
(10.12)
To see that (10.12) holds, note first that if G(_ 1 is connected then there is nothing to 
prove. Otherwise let Hf = (U, F) be the graph obtained from G(_ 1 by (i) contracting 
every connected component of G'_ 1 to a single vertex and (ii) adding to it each edge 
e' e E" independently, with probability p'e, and throwing away loops. Note that since 
for every nontrivial 5, E E's > $, we have that for every vertex . e U (connected 
component of G(_ 1),
u ee'eF 
e eE (U,U c)
)e 
$
Moreover, the probability that a fixed vertex . e U is isolated in Hf is
H (1 - )’e,) < exp - X )'e’ 
e“1-
u te’s.F 
u ee'eF
Hence, the expected number of isolated vertices of Hf does not exceed |U |e“1. There­
fore, by the Markov inequality, it is at most 2|U|e“1 with probability at least 1/2. But 
in this case, the number of connected components of Hf is at most
2|U|e-1 + 2(|U| - 2|U|e-1) = (2 + e’1j |U| < 0.9|U|,
and so (10.12) follows. Observe that if C$ > 1, then the total number of successful 
stages is strictly less than log ‘/log 0.9 < 10 log ‘. However, by (10.12), the probability 
of this event is at most the probability that a binomial random variable with parameters 
$ and 1/2 will attain a value at most 10 log ‘. It follows from the Chernoff-Hoeffding 
inequality (2.27) that if $ = c log ‘ = (20 + -) log ‘, then the probability that C$ > 1 
(i.e., that G', is disconnected) is at most ‘“— !4C. This completes the proof of (10.11) 
)
and the theorem follows. 
□
Exercises
10.1.1 Check that the conditions of Theorem 10.1 are satisfied when
log ‘ + 1ij
)"# = ~^’
where 1" # are uniformly bounded by a constant.
10.1.2 Consider a special case of the generalized binomial graph called the Kronecker 
random graph, defined as a graph where each of its ‘ = 2$ vertices is a binary 
string of length $, for some $ > 1, and between any two such vertices (strings) 
u, v we put an edge independently with probability )uv = a fi# y$~"~#, where 
1 > a > fi > y > 0, and " is the number of positions (coordinates) - such that 
.— = /— = 1, j is the number of - where .— + /-, and finally $ - " - # is the 
number of - such that .- = /- = 0. Denote by 0 the vertex with all 0’s. Show 
that when fi + y < 1 then vertex 0 is isolated w.h.p.

134
Inhomogeneous Graphs
10.1.3 Show that when ft + y = 1 and 0 < ft < 1, then a random Kronecker graph 
defined above cannot be connected w.h.p.
10.2 Expected Degree Sequence
In this section, we will consider a special case of Kovalenko’s generalized binomial 
model, introduced by Chung and Lu in [30], where the edge probabilities )"# depend 
on weights assigned to vertices.
Let V = {1,2,.. ,,nj and let 0" be the weight of vertex". Now insert edges between 
vertices", j e V independently with probability )"# defined as
)"# = 0" 0 # 
^
—— where W = 
0$.
w 
$=1
We assume that max" 02 < W so that )"# < 1. The resulting graph is denoted as G„,Pw.
Note that putting 0" = ‘) for" e [‘] yields the random graph G‘;).
Notice that loops are allowed here but we will ignore them in what follows. Moreover, 
for vertex " e V its expected degree is
'
XG 0"0# 
/ -------= 0"■
W w " 
#=1
Denote the average vertex weight by w (average expected vertex degree), i.e.,
_ W 
0 = —, 
'
while, for any subset of a vertex set V define the volume of as
w(G) 
0$.
$ eu
Chung and Lu in [30] and [31] proved the following results that are summarized in 
the following theorem.
Theorem 10.4 The random graph G‘;Pw with a given expected degree sequence has a 
unique giant component w.h.p. if the expected average degree is strictly greater than 1 
(i.e., w > 1). Moreover, if w > 1, then w.h.p. the giant component has volume
20 W + O (V(log ‘)3'5) ,
where 0 is the unique nonzero root of the following equation:
'' 
£ 0"e-0"2 = (1 - 2) ^ 0". 
"=1 
"=1
Furthermore w.h.p., the second-largest component has size at most
(1 + ((1)) p (0) log ',

10.2 Expected Degree Sequence
135
where
p. (0) =
1/(0 - 1 - log 0)
1/(1 + log w - log 4)
if 1 < o < 2, 
if w > 4/e.
Here we will prove a weaker and restricted version of the above theorem. In the 
current context, a giant component is one with volume Q(W).
Theorem 10.5 If the average expected degree 0 > 4, then a random graph G‘;Pw 
w.h.p. has a unique giant component and its volume is at least
I 
2 \
1 - 
W,
\ 
yew
while the second-largest component w.h.p. has size at most
(1 ♦ (<1» 
log' 
.
1 + log w - log 4
The proof is based on a key lemma given below, proved under stronger conditions 
on w than in fact Theorem 10.5 requires.
Lemma 10.6 For any positive s < 1 and w > e( 14^2 w.h.p. every connected 
component in the random graph G‘;Pw either has volume at least sW or has at most 
1+log 0-logg4‘2 log (1-s) vertices.
Proof We first estimate the probability of the existence of a connected component 
with $ vertices (component of size $) in the random graph G‘;Pw. Let 5 c V and suppose 
that vertices from 5 = {/^ ,/"2,..., /"$} have respective weights wjj, wi2,..., wik. If 
the set 5 induces a connected subgraph of G‘;Pw then it contains at least one spanning 
tree . The probability of such an event equals
P(r) = 
n 
w" # w"% P'
{ /"#.,/"% }eE (T)
where
11 
P := 
=— ■
W 'w
So, the probability that 5 induces a connected subgraph of our random graph can be 
bounded from above by
y p(r)=x n 
w"#w%p’
T 
T {/"#,/"% }eE (T)
where ranges over all spanning trees on 5.

136
Inhomogeneous Graphs
By the matrix-tree theorem (see West [114]), the above sum equals the determinant of 
any k - 1 by k - 1 principal sub-matrix of (D - A)p, where A is defined as
0
0"2 0"1
Oij O"2 
0
A =
\ Wi$ 0"1 
Wi$ 0"2
0"1 0$ '
0"2 0i$
0 ,
while D is the diagonal matrix
D = diag (oil (W - wf1wik (W - w,$)).
(To evaluate the determinant of the first principal co-factor of D - A, delete row and 
column k of D - A; take out a factor 01 wi2 ■ ■ ■ wik1; add the last k - 2 rows to row 
1; Row 1 is now (wik ,wik,..wik), so we can take out a factor wik; now subtract 
column 1 from the remaining columns to get a (k -1) X (k - 1) upper triangular matrix 
with diagonal equal to diag(1, w(5), w(5),..., w(5))).
It follows that
yt P(r) = 0i10i2 • • • 0i$ w(5)$"2 p $-1. 
(10.13)
V
To show that this subgraph is in fact a component one has to multiply by the prob­
ability that there is no edge leaving 5 in G„;Pw. Obviously, this probability equals 
n/es,v# ts (1 _ wiw jp) and can be bounded from above:
n (1 - Wi W j p) < : '' ' " “w(S)) . 
(10.14)
/" eS,v# eV \S
Let X$ be the number of components of size k in G„;Pw. Then, using the bounds 
from (10.13) and (10.14), we get
EX$ < y w(5)$~2p$-1 e~pw^w"w(s)) n Wi,
where the sum ranges over all 5 Q V, 151 = k. Now, we focus our attention on k-vertex 
components whose volume is at most sW. We call such components small or s-small. 
So, if Y$ is the number of small components of size k in G„;Pw, then
EY$ < ^ w(5)$-2p$-1e-w(s)(1’e) Q Wi = f (k). 
(10.15)
Now, using the arithmetic-geometric mean inequality, we have
/(k)< y (w^)$ w(5)$-2p$-1e-w 
.
small S' 
'
The function i2$“2e“1 ^1“s^ achieves its maximum at 1 = (2k - 2)/( 1 - e). Therefore
(
\ 
1 / 
\ 2$ 2
T~ 
e-(2$-2)
k/ k $ 
1 - s

10.2 Expected Degree Sequence
137
/ ne \ $ p$~1 / 2k - 2 \ 2$ 2 .
- k 
k $ 
1 - £
< 
(np)$ 
I 2 \ 2$ _$
4p (k - 1)2 1 - £
_ 
1 
( 
4 
\ $
4p(k - 1)2 ew(1 - £)2
_ e"a$
4p(k - 1)2’
(2$-2)
I 
\ 2$
$ 
1 
2
since 
< e
k
where
a - 1 + log w - log4 + 2 log(1 - £) > 0
under the assumption of Lemma 10.6.
Let k0 = lo|'. When k satisfies k0 < k < 2k0, we have
4np(k - 1)2 
(log n) ’
while, when ^log' 
k n, we have
mi < —:—1------ = o 1-^-1.
4n2p(k - 1)2 
\nlogn
So, the probability that there exists an £-small component of size exceeding k0 is at 
most
^ f ( k) < loagn X 0^ logn^ + n X 0 (n lo1g nj = 0 (!) •
This completes the proof of Lemma 10.6. 
□
To prove Theorem 10.5 assume that for some fixed 6 > 0, we have
w = 4 + 6 =
4 K ^ 
2
e^where £=1 -(ewjitr
(10.16)
and suppose that w1 > w2 > ■ ■ ■ > wn. Next, we show that there exists "0 > n1/3 such 
that
w"0
(10.17)
Suppose the contrary, i.e., for all i > n1/3,
(1 + f) w
Then
W3 W2 
X^ 
(1 + 8) w
W < n1/3W1/2 + J 2-------- —,
"W/3 
"

138
Inhomogeneous Graphs
< ‘1/3W1/2 + 2 (1 + -Wn. 
\\ 
8
Hence,
W 1/2 < ‘1/3 + 2 (1 + - j ‘1/2.
This is a contradiction since for our choice of o,
W = no > 4(1 + -)‘.
We have therefore verified the existence of"0 satisfying (10.17).
Now consider the subgraph G of G„,Pw on the first "0 vertices. The probability that 
there is an edge between vertices /" and /2, for any ", # < "0, is at least
1 +
. 
2 
- ' + 8
0" 0#p > 0"0p > —---
So the asymptotic behavior of G can be approximated by a random graph G‘;) with 
‘ = "0 and ) > 1/"0. So, w.h.p. G has a component of size ©("0) = Q(‘1/3). Applying 
Lemma 10.6 with s as in (10.16), we see that any component with size » log ‘ has 
volume at least .
Finally, consider the volume of a giant component. Suppose first that there exists a 
giant component of volume cW which is s-small, i.e., c < s. By Lemma 10.6, the size 
of the giant component is then at most 2oOg2. Hence, there must be at least one vertex 
with weight 0 greater than or equal to the average:
o > 2c log 2
log ‘
But it implies that o2 » W, which contradicts the general assumption that all )"# < 1. 
We now prove uniqueness in the same way that we proved the uniqueness of the giant 
component in G„,). Choose ^ > 0 such that o(1 - ^) > 4. Then define o' = (1 - ^)O" 
and decompose
G‘,pw = G 1 U G2,
0 0 
where the edge probability in G 1 is)"j = (1 y#w and the edge probability in G2 is)". 
where 1 - 0^0# = (1 - )" j)(1 - )"■). Simple algebra gives )"■ > ^00#. It follows 
from the previous analysis that G 1 contains between 1 and 1/s giant components. Let 
C1, C2 be two such components. The probability that there is no G2 edge between them 
is at most
n(1 _ ’oloi ) < exp (-”o <C1>o <C2 > I < e-,» = ((1).
IW / I’l w 1
7 6C2
As 1/s < 4, this completes the proof of Theorem 10.5.
□

10.2 Expected Degree Sequence
139
To add to the picture of the asymptotic behavior of the random graph G‘;Pw, we will 
present one more result from [30]. Denote by 02 the expected second-order average
degree, i.e.,
— y o2 
o2 = 
.
J
Notice that
— lj o2 
w _
o2 = --------- > — = o.
W 
n
Chung and Lu [30] proved the following.
Theorem 10.7 There exists a constant C > 0 such that if the average expected square 
0 (° 2)2 
„ 
.
degree o2 < 1 then, with probability at least 1------) __>, all components of G‘;Pw
C 211-0 2
have volume at most C^n.
Proof Let
x = P(3S : o(5) > C'1/2and S is a component).
Randomly, choose two vertices . and / from , each with probability proportional to 
its weight. Then, for each vertex, the probability that it is in a set S with o (S) > C V' 
is at least C^np. Hence the probability that both vertices are in the same component 
is at least
1 (C Vp)2 = C 2xnp2. 
(10.18)
On the other hand, for any two fixed vertices, say . and /, the probability P$ (., /) of . 
and / being connected via a path of length $ +1 can be bounded from above as follows:
P$(«, /) < y (o„o"1 p)(o"1 o"2p) • • • (o"$o/p) < o.o/p(o2)$■
"'1,"2,...,"'$
So the probability that . and / belong to the same component is at most
y p$ («,/) < y o. o / p (o2) $ =ou o_ ■ 
$=0 
$=0 
1 - o2
Recall that the probabilities of . and / being chosen from are o.p and o/p, 
respectively. Therefore the probability that a random pair of vertices are in the same 
component is at most
/--- x 2
o2 p
V 
ou o / p
o , o. p o / p ------= =------—.
u,/ 
1 - o2 
1 - o2
Combining this with (10.18), we have
/--- x 2
o2 p
C^xnp2 < ------ =^~,
1 - o2

140
Inhomogeneous Graphs
which implies 
1 <
and Theorem 10.7 follows.
□
Exercises
10.2.1 Find the expected value and variance of the degree ofa fixed vertex of a random 
graph G‘,pw.
10.2.2 Let D" denote the degree of vertex " in a random graph G‘;Pw, where 
" = 1,2,..., ‘. Show that w.h.p.
0" - w('W0" < D" < 0" + w(')V0",
where w c» as ‘ c» arbitrarily slowly.
10.2.3 Let X0 be the number of isolated vertices in a random graph G‘;Pw. Show that
'
E Xo = ^ e“0" + 0 ('02Y
"=1
where 0 = o2 = p £'=1 o2, p = 1/W, and W = £'=1 0".
10.2.4 Let X1 be the number of vertices of degree one in a random graph G‘;Pw. 
Show that
'' 
EX1 = ^ e’0"0" + 0(0p £ 03), 
"=1 
"=1
where 0 and p are the same as defined in the previous exercise.
10.3
Fixed Degree Sequence
The graph G„,& is chosen uniformly at random from the set of graphs with vertex set 
[‘] and & edges. It is of great interest to refine this model so that all the graphs chosen 
have a fixed degree sequence d = (d1, d2,..., d‘). Of particular interest is the case 
where d1 = d2 = ■ ■ ■ = d‘ = +, i.e., the graph chosen is a uniformly random + -regular 
graph. It is not obvious how to do this and this is the subject of the current section.
Configuration Model
Let d = (d1, d2,..., d‘) where d1 + d2 + ■ ■ ■ + d‘ = 2& is even. Let
@‘,d = {simple graphs with vertex set [‘] s.t. degree d(") = d", " e [‘]}

10.3 Fixed Degree Sequence
141
1 
2 
3 
4 
5 
6 
7 
8
Figure 10.1 Partition of W into cells H-'|..... H-'8
and let Gn>d be chosen randomly from £,,.d. We assume that
n
di, d2,...,dn > 1 and 
d" (d" - 1) = Q(n).
We describe a generative model of Gn>d due to Bollobas [18]. It is referred to as 
the configuration model. Let Wi, W2,..., Wn be a partition of a set of points W, where 
|W" | = d" for 1 < i < n and call the W" cells. We will assume some total order < 
on W and that 1 < y if 1 e W" ,y e W# where i < j. For 1 e W, define ^ (1) by 
1 e Wv (1). Let F be a partition of W into m pairs (a configuration). Given F we define 
the (multi)graph y (F) as
y(F) = ([n], {(^(1),^(y)) : (1,y) € F}).
Let us consider the following example of y(F). Let n = 8 and d1 = 4, d2 = 3, d3 = 
4, d4 = 2, d5 = 1, d6 = 4, d7 = 4, and d8 = 2. The accompanying diagrams, Figures 
10.1, 10.2, and 10.3 show a partition of W into W1?..., W8, a configuration and its 
corresponding multigraph.
Denote by Q the set of all configurations defined above for d1 + ■ ■ ■ + dn = 2m and 
notice that
2m)!
|Q| = ' -& = 1 • 3 • 5........(2m - 1) = (2m)!! . 
(10.19)
m!2m
To see this, take d" “distinct” copies of i for i = 1,2,..., n and take a permutation 
(Th <x2,..., a2m of these 2m symbols. Read off F, pair by pair {^2"-1, t2, } for i = 
1,2,..., m. Each distinct F arises in m !2m ways.
We can also give an algorithmic construction of a random element F of the family Q.

142
Inhomogeneous Graphs
Figure 10.2 A partition F of W into m = 12 pairs
Figure 10.3 Graph y(F)
Algorithm F-GENERATOR
begin
U <— W, F ^-0
for t = 1,2,..., m do
begin
Choose x arbitrarily from U;
Choose y randomly from U \ {x};
F <— F U {(x,y)};
U 
U \{(x,y)}
end
end

10.3 Fixed Degree Sequence
143
Note that F arises with probability 1/[(2& - 1)(2& - 3) ■ ■ ■ 1] = |Q| 1.
Observe the following relationship between a simple graph G e £,,.d and the number 
of configurations F for which y (F) = G.
Lemma 10.8 If G e Qn_d, then 
' 
|y-1 (G)|= Q d"! ■
Proof Arrange the edges of G in lexicographic order. Now go through the sequence 
of 2& symbols, replacing each " by a new member of " . We obtain all F for which 
y (F) = G. 
□
The above lemma implies that we can use random configurations to “approximate” 
random graphs with a given degree sequence.
Corollary 10.9 IfF is chosen uniformly at random from the set of all configurations 
Q and G1 ,G2 e Q’^d, then
P(y (F) = G 1) = P (y (F) = G 2).
So instead of sampling from the family G,id and counting graphs with a given 
property, we can choose a random F and accept y (F) if and only if there are no loops 
or multiple edges, i.e., if and only if y (F) is a simple graph.
This is only a useful exercise if y(F) is simple with sufficiently high probability. 
We will assume for the remainder of this section that
A = max{d1,d2,..., d’} = O(1).
Lemma 10.10 IfF is chosen uniformly (at random) from Q,
P(y(F) is simple) = (1 + ((1))e~Al'A+1-1, 
where
Z d" (d" - 1) 
“ 
2 X d" 
•
Proof Notice that in order to estimate the probability that y (F) is simple, we can 
restrict our attention to the occurrence of loops and double edges only. So, let L* denote 
the number of loops and let D * be the number of double edges in y (F).
(10.20)
(10.21)

144
Inhomogeneous Graphs
We first argue that for k = O(1),
L*\\ _ W n dj(dj - 1)
k 
4m - O (1)
S c[n] i eS 
v '
(10.22)
|S|=$
1 y di (dj - 1) 
+ O ( a4
— / ---------------------- + O —
k! 1“ 
4m 
m
j=1
4k
IT •
Explanation for (10.22): We assume that F-GENERATOR begins with pairing up 
points in 5. Therefore the random choice here is always from a set of size 2m - O (1).
It follows from Lemma 5.1 that the number of loops L* in y (F) is asymptotically 
Poisson and hence that
P(L* = 0) - e"A.
(10.23)
Now, consider double edges in y (F) and split the set of all D * double edges into 
two subsets: non-adjacent and adjacent ones, of cardinalities D* and D*, respectively. 
Obviously D* = D11 + D22.
We first show that D* is asymptotically Poisson and asymptotically independent of 
L*. So, let k = O(1). If rDk denotes the set of collections of 2k configuration points 
making up k double edges, then
L* = 0 =2 1 ?)$ Q F | L* = 0)
P(L* = 0 \Dk c F) ' A c F)
P (L* = 0)
Now because k = O(1), we see that the calculations that give us (10.23) will give us 
P(L* = 0 \^k c F) ~ P(L* = 0). So,
i j 
|S| = |T|=$ 
sct=0
fi2$ 
k!
It follows from Lemma 5.1 that
P(DJ = 0 | L* = 0) - e-z2
(10.24)

10.3 Fixed Degree Sequence
145
To complete the proof of the Lemma, we need to show that w.h.p. there are no 
adjacent double edges. If D*2 is the number of adjacent double edges, then
„ 
d" d2#. d2
E( D J) < >  ---- " # * „ = O (‘"1). 
(10.25)
' 27 
,^(2& - O(1))4 
' 
7 
'
'■,#■$■
Explanation: We sum over the possibilities of a double edge {", # } and a double edge 
{", $}. Having chosen vertices ", #, $, we choose two ordered pairs .1 ,u2 and /1, /2 
from W" in 4! ('"") /2 < d" ways; an ordered pair 11, i2 from W# in less than d2 ways; and 
an ordered pair 21,22 from W$ in less than d$ ways. We then estimate the probability 
that our configuration contains the pairs {.1,11}, {u2,i2}, {/1,21}, {/2,22}. Using 
F-GENERATOR with initial choices .1, u2, /1, /2 for 1, we see that the probability 
that .1 is paired with 11 is 1/(2& - 1). Given this, the probability that .2 is paired with 
i2 is 1/(2& - 3), and so on.
In conclusion, (10.25) implies that indeed w.h.p. there are no adjacent double edges 
and the lemma follows from (10.23) and (10.24). 
□
Theorem 10.11 Suppose that A = O (1).
|£‘,d|~ e-^+1) ^&j_, 
"=1 d"!
where is defined in (10.21).
Proof This follows from Lemma 10.8 and Lemma 10.10. 
□
Hence, (10.19) and (10.20) will tell us not only how large Q’d is (Theorem 10.11) 
but also lead to the following conclusion.
Theorem 10.12 Suppose that A = O (1). For any (multi)graph property P, 
P(G„,d e ^) < (1 + ((1))e4(4+1) P(y(F) e P).
Existence of a Giant Component
Molloy and Reed [90] provide an elegant and very useful criterion for when G„,d has a 
giant component. Suppose that there are A"‘+((‘3/4) vertices of degree" = 1,2,..., L. 
We will assume that L = O (1) and that the A"," e [L] are constants independent of ‘. 
The paper [90] allows for L = O(‘1/4“s). We will assume that A1 + A2 + ■ ■ ■ + Al = 1.
Theorem 10.13 Let A = 2"P1"(" _ 2)A". Let s > 0 be arbitrary.
(a) If A < —e , then w.h.p. the size of the largest component in G‘;d is O (log ‘).
(b) If A > e , then w.h.p. there is a unique giant component of linear size ~ 0‘ where 
0 is defined as follows: let K = X =1 "A" and
L
l t 
2a \ iy2
f (a) = K - 2a - Y "A" 1 - 
. 
(10.26)
K

146
Inhomogeneous Graphs
Let * be the smallest positive solution to f (a) = 0. Then
L t o / \ "/2
0 = 1 _ VA, 1 _ 2 
.
4-J ' I K / 
,=1 x 
'
If A1 = 0 then 0 = 1, otherwise 0 < 0 < 1.
(c) In case (b), the degree sequence of the graph obtained by deleting the giant 
component satisfies the conditions of (a).
To prove the above theorem, we need Azuma-Hoeffding bounds for martingales. Re­
call that a martingale is a sequence of random variables Xo, X1,..., Xn on a probability 
space (Q, T, P) with E(X$+1|7$) = X$, where
'G = {0, Q}CT1 cy-2 C...C7-' = r
is an increasing sequence of ^-fields. When E(X$+1 |7$) > X$ then this sequence is 
calleda super-martingale, while if E( X$+1 |7$) < X$ then it is called a sub-martingale.
Lemma 10.14 (Azuma-Hoeffding bound) Let {X$ }' be a sequence of random vari­
ables such that | X$ - X$_11 < c$, k = 1,...,n, and Xo is constant.
(i) If {X$ }' is a martingale, then for all t > 0, we have
P(|Xn - X01 > t) < 2exp
P(Xn < E Xn - t) < exp
(ii) If {X$ }' is a super-martingale, then for all t > 0, we have
P(Xn > X0 +1) < exp
(iii) If {X$ }' is a sub-martingale, then for all t > 0, we have
P(Xn < X0 - t) < exp
For a proof of Lemma 10.14, see, for example, section 21.7 of the book [52].
Proof (of Theorem 10.13)
We consider the execution of F-GENERATOR. We keep a sequence of partitions 
Ut, At, Et ,t = 1,2,...,m, of W. Initially U0 = W and A0 = E0 = 0. The (t + 1)th 
iteration of F-GENERATOR is now executed as follows: it is designed so that we 
construct y(F) component by component. At is the set of points associated with 
the partially exposed vertices of the current component. These are vertices in the 

10.3 Fixed Degree Sequence
147
current component, not all of whose points have been paired. U- is the set of unpaired 
points associated with the entirely unexposed vertices that have not been added to any 
component so far. E- is the set of paired points. Whenever possible, we choose to make 
a pairing that involves the current component.
(i) If A- = 0 then choose 1 from U-. Go to (iii).
We begin the exploration of a new component of y (F).
(ii) If A- + 0 choose 1 from A-. Go to (iii).
Choose a point associated with a partially exposed vertex of the current compo­
nent.
(iii) Choose y randomly from (A- U U-) \ {1}.
(iv) F F U {(1, y)}; E-+i 
E- U {1, y}; A-+i 
A- \ {1}.
(v) If y 6 A- then A-+i 
A-+i \ {y}; U-+i 
U-.
y is associated with a vertex in the current component.
(vi) If y 6 U- then A-+i 
A- U (W^ (y) \ y); U-+i 
U- \ W, (y}.
y is associated with a vertex / = ^ (y) not in the current component. Add all the 
points in W/ \ {y} to the active set.
(vii) Go to (i).
(a) We fix a vertex / and estimate the size of the component containing /. We keep 
track of the size of A- for - = O (log n) steps. Observe that
B<|A-«| - 14-1 I XI > 0) < X^:2> ,,'.', S -1. 
<W.27>
Here Mi = X^=i id"n. The explanation for (10.27) is that | A | increases only in Step 
(vi) and there it increases by i - 2 with probability < l/"'y:. The two points 1, y are 
missing from A-+i and this explains the -2 term.
Let ei = e/L and let
| A-1+ ei- |Ai |, | A21,..., | A-1 > 0, 
0 
otherwise.
It follows from (10.27) that if t = O(logn) and Yi,Y2,...,Y- > 0, then
E(Y-+i | Y,Y2,...,Y-) = E(| A-+i |+ ei (t + i) | Yi,Y2,...,Y-) < | A-1+ eit = Y-.
Otherwise, E(Y-+i | ■) = 0 = Y- .It follows that the sequence (Y-) is a super-martingale. 
Next let Zi = 0 and Z- = Y- - Y-_i for t > i. Then, we have -2 < Z" < L and 
E(Z") < —ei for i = i, 2,..., t. Now,
P(AT + V), i < t < t) < P(Y- = Zi + Z2 + ■ ■ ■ + Z- > 0).
It follows from Lemma 10.14 that if Z = Zi + Z2 + ■ ■ ■ + Z-, then
e212
P(Z > 0) < P(Z - E(Z) > tei) < exp -

148
Inhomogeneous Graphs
It follows that with probability 1 - O(n“2), At will become empty after at most 
16£j2 log n rounds. Thus for any fixed vertex /, with probability 1 - O(n“2) the 
component containing / has size at most 4£j2 log n. (We can expose the component 
containing / through our choice of 1 in Step (i).) Thus, the probability that there is a 
component of size greater than 16£j"2 log n is O (n-1). This completes the proof of (a). 
(b) If - < 6n for a small positive constant 6 « £/L3, then
, l4lx 
-2|At|+(1 + o(1)) Xf-1 i U- n - 2-)(i - 2)
E(|A-+11-|A-1) > 
"1
M1 - 26n
-2L6n + (1 + o(1))(An - 26L3n) 
£
~ 2L'
Mi - 26n
(10.28)
Let £2 = £/2L and let
f|At|- £2- |A1|,|A2|,...,|At| > 0, 
[0 
otherwise.
It follows from (10.27) that if t < 6n and Y1, Y2,..., Yt > 0, then
E(Yt+1 I Y1,Y2,...,Yt) = E(| At+11 - £2 (- + 1) | Y1,Y2,...,Yt) > | At | - £2- = Yt.
Otherwise, E(Yt+1 | ■) = 0 = Yt. It follows that the sequence (Yt) is a sub-martingale. 
Next let Z1 = 0 and Zt = Yt - Yt _1 for - > 1. Then, we have (i) -2 < Z" < L and (ii) 
E(Z") > £2 for i = 1,2,..., -. Now,
P(a- + 0) > P(Yt = Z1 + Z2 + ■■■ + Zt > 0).
It follows from Lemma 10.14 that if Z = Z1 + Z2 + ■ ■ ■ + Zt, then
P(Z < 0) < P(Z - E(Z) > -£2) < exp
It follows that if L0 = 100£22, then
P
£ 
£2- \
< 6n : Z < —— I 
2
/ 
£2-\
< P (3L0 log n < - < 6n : Z - E(Z) >
= O(n 2).
It follows that if -0 = 6n then w.h.p. | At01 = Q(n), and there is a giant component and 
that the edges exposed between time L0 log n and time -0 are part of exactly one giant.
We now deal with the special case where 21 = 0. There are two sub-cases. If in 
addition we have 22 = 1, then w.h.p. Gd is the union of O (log n) vertex disjoint cycles, 
see Problem 10.7. If 21 = 0 and 22 < 1, then the only solutions to f (a) = 0 are 
a = 0, K/2. For then 0 < a < K/2 implies
E"2" 1 
i=2
= K - 2a.
i=2

10.3 Fixed Degree Sequence
149
This gives 0 = 1. Exercise 10.5.2 asks for a proof that w.h.p. in this case, G„;d consists 
of a giant component plus a collection of small components that are cycles of size 
O (log n).
Assume now that '1 > 0. We show that w.h.p. there are Q(n) isolated edges. This 
together with the rest of the proof implies that T < K/2 and hence that 0 < 1. Indeed, 
if Z denotes the number of components that are isolated edges, then
E<Z> = ('2'') 2» 
and E<Z<Z - »’ = ('4'') (2M1 - 1)6(2M1 - 3)
and so the Chebyshev inequality (2.18) implies that Z = Q(n) w.h.p.
Now for i such that '" > 0, we let Xt- denote the number of entirely unexposed 
vertices of degree i. We focus on the number of unexposed vertices of a given degree. 
Then,
E(Xt,t+i - Xi-) = - ^^ 1. 
(10.29)
This suggests that the random variables Xi;t In closely follow the trajectory of the 
differential equation, where t = - In and x (t) = Xt-In. This is the basis of the 
differential equations method and a rigorous treatment of this case can be found in 
Section 11.3 of [52]. We have
dx ix
dr 
K - 2t
x(0) = '. Note that K = M1/n. 
The solution to (10.30) is
x
I 
2r \i/2
— 'i 1 _
(10.30)
(10.31)
Theorem 23.1 of [52] then implies that with probability 1 - O(n1/4e Q('1/4)), 
f
2t \" ^2
1 "IF
= O (n3/4),
(10.32)
up to a point where Xt- = O (n3/4). (The O(n3/4) term for the number of vertices of 
degree i is absorbed into the RHS of (10.32).)
Now because
L 
L
| At | = M1 - 2t - ^ "Xi,t = Kn - 2t - £ iXt-,
i=1 
i=1
we see that w.h.p.
I 
2— 
L / 
2t \i/2 \
I A-1 = nK - - -V i'i 1 - -- 
+ O (n3^4)
n 
Kn
i=1
= nf (-1 + O(n3/4), 
(10.33)
n
so that w.h.p. the first time after time t0 = dn that |At | = O(n3/4) is at time 
-1 = Tn + O(n3/4). This shows that w.h.p. there is a component of size at least 

150
Inhomogeneous Graphs
Qn + O(n3/4). Indeed, we simply subtract the number of entirely unexposed vertices 
from ' to obtain this.
To finish, we must show that this component is unique and no larger than 0n + 
O(n3/4). We can do this by proving (c), i.e. showing that the degree sequence of the 
graph Gv induced by the unexposed vertices satisfies the condition of Case (a). For 
then by Case (a), the giant component can only add O(n3/4 x log n) = ((n) vertices 
from -1 onwards.
We first observe that the above analysis shows that w.h.p. the degree sequence of 
Gv is asymptotically equal to nA't, " = 1,2,...,L, where
f
our \ "72
1------
jz I
(The important thing here is that the number of vertices of degree " is asymptot­
ically proportional to A-.) Next choose £1 > 0 sufficiently small and let -S1 = 
max {- : |A-1 > s1n}. There must exist £2 < £1 such that -£1 < (T - s2)n and
f '(T - £2) < -£i, else f cannot reach zero. Recall that T < K/2 here and then,
—£1 * f ,(t - £2>=-2+k - 2(T - £2 > §-2a"
2T - 2£2 \i/2
K )
= -2 +
1 + O (£2)
K - 2T
1 + O(£2) \’ 
L
= K 'T G."(" - 2H- * 1
E "2A" (1 - K) "'2
i >1 
' 
’
1 + O (£2) ^
= KT2W-1^1 (l 2>A''
(10.34)
This completes the proofs of (b) and (c).
□
Connectivity of Regular Graphs
Bollobas [18] used the configuration model to prove the following: Let G‘;+ denote a 
random + -regular graph with vertex set [n] and + > 3 constant.
Theorem 10.15 G‘,+ is + -connected, w.h.p.
Since an +-regular, +-connected graph, with n even, has a perfect matching, the 
above theorem immediately implies the following corollary.
Corollary 10.16 Let G‘+ be a random + -regular graph, + > 3 constant, with vertex 
set [n] even. Then w.h.p. G‘+ has a perfect matching.

10.3 Fixed Degree Sequence
151
Proof (of Theorem 10.15)
Partition the vertex set V = [n] of Gn,r into three parts, K, L, and V \ (K U L) such 
that L = N(K), i.e., such that L separates K from V \ (K U L) and |L| = % < r - 1. We 
will show that w.h.p. there are no such K, L for k ranging from 2 to n/2. We will use 
the configuration model and the relationship stated in Theorem 10.12. We will divide 
the whole range of k into three parts.
(i) 2 < k < 3.
Put S := K U L, s = |S| = k + I < r + 2. The set S contains at least 2r - 1 edges 
(k = 2) or at least 3r - 3 edges (k = 3). In both cases this is at leasts + 1 edges.
P(3S,s = |S| < r + 2 : S contains s + 1 edges)
++2 I \ I \ , 
, 5+1
< y ' rS (—) 
(10.35)
, \s s + 1 rn 
s=4
r +2
< y n,2rss''1' ' 1 
,=4
= ((1).
Explanation for (10.35): Having chosen a set of s vertices, spanning rs points R, we 
choose s + 1 of these points T. bounds the probability that one of these points in T 
is paired with something in a cell associated with S. This bound holds conditional on 
other points of R being so paired.
(ii) 
4 < k < ne“10.
The number of edges incident with the set K, |K| = k, is at least (rk + I)/2. 
Indeed, let a = e(K) and b = e(K, L) edges. Then 2a + b = rk and b > I. This gives 
a + b > (rk + %)/2. So,
(r$+Z)/2
ne 10 r-1 
^.{
y y ‘-(2-1)k+2 e$-r 2r$ (k+% )(r$+z)/2.
$“4 "/“O 
k %
Now
(k + % \1/2 
%
< ek/2 and (k + % \k/2 
k
< e%/2,
and so
( k + % )(r$ +Z)/2 < %Z/2 k rk/2e (lr+k)/2

152
Inhomogeneous Graphs
Therefore, with Cr a constant,
'<’ 10 r-1
P (3K,L) < Cr ^ ^ n"( 2-1)k+2 e3kl22rk k{r-2) k/2
$=4 l=0
ne~10 r-1 
$
= Cr X E('"(2"1)+2$ 
2 k2-1)
$=4 1=0 V
= o (1). 
(10.36)
(iii) ne-10 < k < n/2.
Assume that there are a edges between sets L and V \ (K u L). Define
, (2m) = <2&1! - 21/2 ( 2& 1", 
m1 2" e
Then, remembering that r,l,a = O(1), we can estimate that
P(3K,L)
^(rk + rl - a)^(r(n - k - l) + a) 
^ (rn)
(10.37)
(rk + rl - a)r$ +rl~a(r(n - k - l) + a)r('-$-%)+« 
(rn)r”
, y / ne \$ / ne \l (rk)rk+rl~a(r(n - k))r('-$"l)+a
Cr 
(Tnr
is i n 
'
= o (1).
(10.38)
Explanation of (10.37): Having chosen K, L we choose a points in Wkul = 
U"eKuL W" that will be paired outside WkuL. This leaves rk + rl - a points in WkuL 
to be paired up in ^(rk + rl - a) ways and then the remaining points can be paired up 
in ^ (r (n - k - l) + a) ways. We then multiply by the probability 1/^ (rn) of the final 
pairing. 
□
Exercises
10.3.1 Verify equation (10.36).
10.3.2 Verify equation (10.38).

10.3 Fixed Degree Sequence
153
Problems for Chapter 10
10.1 Prove that the Kronecker random graph, defined in Exercise 10.1.2, is connected 
w.h.p. (for $ to) if either (i) fi + y > 1 or (ii) a = fi = 1, y = 0.
10.2 Prove Theorem 10.3 (with c = 10) using the result of Karger and Stein [67] that 
in any weighted graph on n vertices the number of + -minimal cuts is O ((2n)2+). 
(A cut (S,V \ S),S Q V, in a weighted graph G is called + -minimal if its weight, 
i.e., the sum of weights of the edges connecting S with V \ S, is at most + times 
the weight of the minimal weighted cut of G.)
10.3 Let X$ denote the number of vertices of degree $ in a random graph G„;Pw. Show 
that
fl 
! . .
1 
v—i 
W W ($_ 2)
H X$ = -VW" e-°" + O
$! 
\ 
$!
"=1 
'
10.4
where W — w2 — o Y' i w2 o — 1 !W W = Yn w and w = o Yn wt-
>>' 
" 
" 1 
" , 
, ^J" 1 
" 
^t^J" 
" .
Let X$ denote the number of vertices of degree $ in a random graph G„;Pw, where 
for i = 1,2,..., n we have weights W" = ci-1/^-1 and c,fi > 0 are constants. 
Show that
EX$ ~ c'r($ - fi \1} ~ $, 
$ 
r( $ +1) 
’
for some constant c’. Here r is the Euler gamma function.
10.5 Let A be the adjacency matrix of G„;Pw and for a fixed value of x, let
W" W" > x, 
c" =
x 
W" < x.
Let m = max {w" : i e [n]}. Let X" = h'=1 cjatj. Show that
E X" < w2 + x and Var X" < — w2 + x.
10.6 Suppose that 1 < W" « W1/2 for 1 < i < n and that W"wjw  » Wlog n. Show 
that w.h.p. diameter(G„;Pw) < 2.
2
10.7 Show that w.h.p. a random 2-regular graph on n vertices consists of O(log n) 
vertex disjoint cycles.
10.8 Let H be a subgraph of G„;+, + > 3 obtained by independently including each 
vertex with probability 1±f, where s > 0 is small and positive. Show that w.h.p. 
H contains a component of size Q(n).
10.9 Extend the configuration model to bipartite graphs and show that a random 
+-regular bipartite graph is connected w.h.p. for + > 3.
10.10 Show that w.h.p. Gn+ is not planar for + > 3.

11 Small World
In an influential paper, Milgram [89] described the following experiment. He chose a 
person to receive mail and then randomly chose a person to send it to. If did 
not know , then was to send the mail to someone he/she thought more likely to 
know and so on. Surprisingly, the mail got through in 64 out of 296 attempts, and 
the number of links in the chain was relatively small, between 5 and 6. Milgram’s 
experiment suggests that large real-world networks although being globally sparse, 
in terms of the number of edges, have their nodes/vertices connected by relatively 
short paths. In addition, such networks are locally dense, i.e., vertices lying in a small 
neighborhood of a given vertex are connected by many edges. This observation is called 
the “small-world” phenomenon, and it has generated many attempts, both theoretical 
and experimental, to build and study appropriate models of small-world networks. 
The first attempt to explain this phenomenon and to build a more realistic model 
was introduced by Watts and Strogatz in 1998 in Nature (see [113]) followed by the 
publication of an alternative approach by Kleinberg in 2000 (see [72]). This chapter is 
devoted to the presentation of both models.
11.1 Watts-Strogatz Model
The Watts-Strogatz model starts with a $ th power of an ‘-vertex cycle, denoted 
here as C'$. To construct it, fix ‘ and $, ‘ > $ > 1 and take the vertex set as
V = [‘] = {1,2,...,n} and edge-set E = {{",#} :" + 1 < # < " + $}, where the
additions are taken modulo '.
In particular, C' = Cn is a cycle on ‘ vertices. For an example of a square C' of Cn, 
see Figure 11.1.
Note that for ‘ > 2$ graph C' is 2$-regular and has ‘$ edges. Now choose each 
of the '$ edges of C'$ , one by one, and independently with small probability ) decide 
to “rewire” it or leave it unchanged. The procedure goes as follows. We start, say, at 
vertex labeled 1, and move clockwise $ times around the cycle. At the "th passage of the 
cycle, at each visited vertex, we take the edge connecting it to its neighbor at distance 
" to the right and decide, with probability ), ifits other endpoint should be replaced by a 
uniformly random vertex of the cycle. However, we do not allow the creation of double

11.1 Watts-Strogatz Model
155
edges. Note that after this $ round procedure is completed, the number of edges of the 
Watts-Strogatz random graph is $‘, i.e., the same as in the “starting” graph C‘.
To study the properties of the original Watts-Strogatz model in a formal mathe­
matical manner has proved rather difficult. Therefore, Newman and Watts (see [96]) 
proposed a modified version, where instead of rewiring the edges of C'$ , each of the 
(') - ‘$ edges not in C‘ are added independently with probability ). Denote this 
random graph by C$. In fact, C', when the starting graph is the ‘-vertex cycle, 
was introduced earlier by Ball, Mollison, and Scalia-Tomba [11] as “the great circle” 
epidemic model. For rigorous results on typical distances in C$, see the papers of 
Barbour and Reinert [15, 16].
Gu and Huang [56] suggested a variant of the Newman-Watts random graph C$ 
as a better approximation of the structure of the Watts-Strogatz random graph, where 
in addition to randomly sampling the edges not in C'* , each of the $' edges of C'* 
is removed independently with probability * (i.e., it remains in C'* with probability 
1 - *). Denote this random graph by C') 1_* and note that if ) = 1 - *, then 
Cn) 1-* = &',1-*, while if we choose * = 0, it is equivalent to the Newman-Watts 
random graph C* ) .Now, one can, for example, fix * + 0 and find ) = ) (‘, $, *) such 
that the expected number of edges of C‘ ) 1_* is equal to $‘, i.e., is the same as the 
number of edges in the Watts and Strogatz random graph (see Exercise 11.1.1).
Much earlier Bollobas and Chung [23] took yet another approach to introduc­
ing “shortcuts” in C'. Namely, let C' be a cycle with ' vertices labeled clockwise 
1,2,...,‘, so that vertex " is adjacent to vertex " + 1 for 1 < " < ‘ - 1. Consider the 
graph &' obtained by adding a randomly chosen perfect matching to C'. (We will 
assume that ' is even. For odd ' one can add a random near-prefect matching.) Note 
that the graphs generated by this procedure are three-regular (see Figure 11.2).
It is easy to see that a cycle Cn itself has a diameter ‘/2. Bollobas and Chung 
proved that the diameter drops dramatically after adding to C' such a system of 
random “shortcuts.”

156
Small World
Figure 11.2 C8 U M
Theorem 11.1 Let ' be formed by adding a random perfect matching to an 
'-cycle '. Then w.h.p.
diam(G„) < log2 ‘ + log2 log ‘ + 10.
Proof For a vertex . of ' define sets
S"(u) = {/ :dist(u,/) = "} 
and 
5<"(u) = 
5y (u),
# <"
where dist(u, /) = distc’ (u, /) denotes the length of a shortest path between u and / 
in '.
Now define the following process for generating sets S"(u) and S<,(u) in G‘, Start 
with a fixed vertex u and “uncover” the chord (edge of M) incident to vertex u. This 
determines the set S1 (u). Then we add the neighbors of S1 (u) one by one to determine 
S2(u) and proceed to determine S" (u).
A chord incident to a vertex in S" (u) is called inessential at level" if the other vertex 
in S"(u) is within distance 3 log2 ‘ in C‘ of the vertices determined so far. Note that 
IS<"(u)| < 3 ■ 2" and so
P(a chord is inessential at level" | S<"_1 (u )) < 18'2" log2 '. 
(11.1)
'
Denote by the event that for every vertex u, at most one of the chords chosen in 
S<"(u) is inessential and suppose that" < 1 log2 ‘. Then
P(^c) = P(3u : at least two of the chords chosen in S<, (u) are inessential)
s ‘ ft 2"« W 
. 
■. ' \2 = 0 ,.,„ 
2 V
2'
For a fixed vertex u, consider those vertices / in S"(u) for which there is a unique 
path from u to / of length ", say u = uo, u1,..., U"_1,." = /, such that

11.1 Watts-Strogatz Model
157
(i) if Mj-i is adjacent to / on the cycle Cn, then S<, (m) contains no vertex on Cn within 
distance 3 log2 n on the opposite side to / (denote the set of such vertices / by 
Ci (m)),
(ii) if {."-1, /} is a chord, then S<" (m) \ {/} contains no vertex within distance 3 log2 n 
both to the left and to the right of / (denote the set of such vertices by Dt (m)).
Obviously,
C"(m) U D"(m) c S"(m).
Note that if the event holds, then, for i < | log2 n,
|C"(m)|> 2"-2 
and 
|D"(m)|> 2"~3. 
(11.2)
Let | log2 n < i < | log2 n. Denote by B the event that for every vertex m, at most 
2"n“i/i0 inessential chords leave S"(m). There are at most 2" chords leaving S"(m) for 
such is and so by (11.1), for large n,
P(gc) = P(3m : at least 2"n“i/i0 inessential chords leave S"(m))
- n(2" n-i/ioj = ^ (n )‘ (11.3)
For / e C"(m), a new neighbor of / in Cn is a potential element of C"+i (m) and a 
new neighbor, which is the end-vertex of the chord from /, is a potential element of 
D"+i (m). Also, if / e D"(m), then the two neighbors of / in Cn are potential elements 
of C"+i(m). Here “potential” means that the vertices in question become elements of 
C"+i (m) and D"+i (m) unless the corresponding edge is inessential.
Assuming that the events and B both hold and | log2 n < i < | log2 n, then 
|C"+i(m)| > |C"(m)| + 2|D"(m)| - 2"+in-i/i0, 
|D"+i(m)| > |C"(m)|- 2"+in-i^i0,
while for i < | log2 n the bounds given in (11.2) hold. Hence, for all 3 < i < | log2 n, 
we have
|C"(m)| > 2"-3 
and 
|D"(m)| > 2"’4.
To finish the proof set,
T log2 n + log2 log n + c'
where c > 9 is a constant.
Let us choose chords leaving C"0(m) one by one. At each choice, the probability of 
not selecting the other end-vertex in C"0 (m) is at most i - (Z"0-3/'). Since we have to 
make at least |C"0 (m)|/2 > 2"0-4 such choices, we have
2"0~4
2"0-3 2
P(dist(M,v) > 2i0 + i|^n®)< i-------- < n"4. 
(11.4)
n
Hence,
P(diam(G„) > 2i0 + i) < P(^c) + P(Sc) + ^P(dist(m,v) > 2i0 + i|^ ng) 
t/

158
Small World
< C1 (' 1/5 (log ')2) + C2' 2 + '2 = ( (1) .
Therefore, w.h.p. the random graph ' has diameter at most 
2 ' log2 ‘ + log2 ‘ log ‘ + 9' < log2 ‘ + log2 ‘ log ‘ + 10,
2
which completes the proof of Theorem 11.1.
□
In fact, based on the similarity between a random three-regular graph and the graph 
' defined previously, one can prove more precise bounds, showing (see Wormald 
[115]) that w.h.p. diam(G„) is highly concentrated, i.e., that
log2 ‘ + log2 ‘ log ‘ - 4 < diam(G„) < log2 ‘ + log2 ‘ log ‘ + 4.
The above-mentioned result illustrates one of the two main characteristics of the 
small-world phenomenon. Namely, it shows that although real-world networks are 
sparse, the distance between any two nodes is small. The second typical property of 
networks is that they are at the same time “locally dense” or, alternatively, have “a high 
degree of clustering.” To illustrate this notion, let us formally introduce definitions of 
local and global clustering for graphs. Let G be a graph on the vertex set [‘]. Let / be 
a vertex in G with degree deg(v) = d(/). Let N(/) denote the neighborhood of /, i.e., 
the set of all vertices incident to / and en (/) be the number of edges in the subgraph 
of G induced by the vertices in N(/). Note that |N(/)| = d(/).
Define the local clustering coefficient of / as
e n (/)
%cc(/) = ,, , . 
(11.5)
(d 2/>)
Hence, it is natural to introduce the average local clustering coefficient as a measure 
of the local density of a graph G, so
1 '
Ci (G) = 
%cc (/). 
(11.6)
' /=1
Another way of averaging the number of edges in the neighborhood of each vertex 
might be more informative in measuring the total clustering of a graph G . Let
C2(G) = £"*1 e,*‘/. 
(11.7)
2' ’ 
£L1 (d 2/ >)
In the next lemma, we separately find the expected values of the two basic ingredients 
of both clustering coefficients C1 (G) and C2(G), namely E(en(/)) and (d2/^, 
for any vertex / of the random graph C$ ) 1_*, i.e, of the generalized Newman-Watts 
graph, introduced by Gu and Huan.
Lemma 11.2 Let G = C$ ) 1_*. Then for every vertex /,

11.1 Watts-Strogatz Model
159
E d </’ 
=
2
2 {2$(2k - 1)(1 - q)2 + (n - 2k)(n - 2k - 1)p2} + 2k(1 - q)(n - 2k)p. (11.8)
E(e n (/)) =
(1 + 2(k - 1))(1 - q)3 + (2k - 1)(n - 2k - 1)p2(1 - q) + (” 
1jp3. (11.9)
Proof
To prove the first statement, we use the fact that if Z is distributed as the binomial 
Bin(n,p), then
_//ZU 
1 , 
, ,
EH JI = ^n(n - 1)p2. 
(11.10)
The number of edges of C' incident to v that are not deleted is distributed as Bin (2k, 1 - 
q), and this explains the first term in (11.8). The number of added edges incident to 
v is distributed as Bin(n - 2k, p), and this explains the second term in (11.8). The 
quantities giving rise to the first two terms are independent and so the third term is just 
the product of their expectations.
To show that (11.9) holds, note that counting edges in the neighborhood of v is the 
same as counting triangles in the subgraph of C') 1 induced by v U N(v). Note 
that there are 1 + 2(k - 1) triangles in C' and each exists with probability (1 - q)3. 
There are (2k - 1)(n - 2k - 1) triangles that use one edge of C' and each exists with 
probability p2 (1 - q). There are ('J*-1) triangles that do not use any edge of C' and 
each exists with probability p3. This proves (11.9). 
□
Following the observation in the proof of (11.9), one can immediately see that in 
fact,
£'=1 d (v)( d (v)- 1)’
(11.11)
where T = T(G) is the number of triangles in a graph G. For G = C' ) 1, the 
expectation E(T) = nE(en(/)), where E(en(/)) is given by (11.9).
Exercises
11.1.1 Fix q + 0 and find p = p (n, k, q) such that the expected number of edges of a 
random graph C' ) 1 
is equal to kn, i.e., is the same as the number of edges
in the Watts-Strogatz random graph.
11.1.2 Verify equation (11.3).
11.1.3 Verify equation (11.4).
11.1.4 Find the value lee(v) for any vertex v of C', the kth power of a cycle C'.
11.1.5 Assume that p,q are constants in C';);1_*. Use McDiarmid’s inequality (see 
Lemma 9.6) to show that the quantities in Lemma 11.2 are both concentrated 
around their means. (Here take the W", i = 1,..., (') to be the indicator vari­
ables for the edges of K' that exist in C';);1_*.)

160
Small World
11.2 Kleinberg’s Model
The model introduced by Kleinberg [72] can be generalized significantly, but to be 
specific we consider the following. We start with the ‘ x ‘ grid G0 which has vertex set 
[‘]2 and where (", j) is adjacent to ("', j') if and only if d{{",#), ("',j')) = 1, where 
d {{",#), ($, I)) = I" - $ | +1 j -11. In addition, each vertex . = (", j) will choose another 
random neighbor ^ (m), where
P(^(m) = / = ($,l)) = d(Mn/} 2 
(11.12)
.
and
Di = Yt d(i’2)”2- 
2^1
The random neighbors model “long-range contacts.” Let the grid G0 plus the extra 
random edges be denoted by G .
It is not difficult to show that w.h.p. these random contacts reduce the diameter ofG 
to order log '. This, however, would not explain Milgram’s success. Instead, Kleinberg 
proposed the following decentralized algorithm for finding a path from an initial 
vertex .0 = ("0, j0) to a target vertex .T = ("T,jT): when at . move to the neighbor 
closest in distance to m.
Theorem 11.3 Algorithm finds a path from initial to target vertex of order 
O((log‘)2), in expectation.
Proof Note that each step of finds a node closer to the target than the current node 
and so the algorithm must terminate with a path.
Observe next that for any vertex 1 of G we have
2‘-2 
2‘-2
Di < ^ 4# x #~2 = 4 2 #“1 - 4 log(3'}• 
j=1 
#=1
As a consequence, / is the long-range contact of vertex m, with probability at least 
(4 log(3‘) d (m, /)2)“1.
For 0 < j < log2 ‘, we say that the execution of is in Phase j if the distance of 
the current vertex . to the target is greater than 2#, but at most 22+1. We say that is 
in Phase 0 if the distance from m to the target is at most 2.
Let # denote the set of nodes at distance 2# or less from the target. Then
|B# | > 22#~1. 
(11.13)
Note that by the triangle inequality, each member of B# is within distance 2J'+1 + 2# < 
22#+2 of m.
Let Xj < 2#+1 be the time spent in Phase #. Assume first that 

11.2 Kleinberg’s Model
161
log2 log2 ‘ < # < log2 ‘. Phase # will end if the long-range contact of the current 
vertex lies in #. The probability of this is at least
22 #~1 
_ 
1
4 iog(3')22>+4 " 128 iog(3‘)'
We can reveal the long-range contacts as the algorithm progresses. In this way, the 
long-range contact of the current vertex will be independent of the previous contacts 
of the path. Thus,
1 
"
B x# - £ K x# >.">, g |4 - 128^) < 128 iog(3»).
Now if 0 < # < log2 iog2 ‘, then X# < 2-#+1 < 2 iog2 ‘. Thus, the expected length of 
the path found by is at most 128 log(3‘) x iog2 ‘. 
□
In the same paper, Kleinberg showed that replacing d(., /)“2 by d(., /)“+ for + + 1 
led to non-polylogarithmic path length.
Exercises
11. 2.1 Verify equation (11.13).
11. 2.2 Let H be a fixed graph with minimum degree a’, where 0 < a < 1 is constant. 
Let R be a random set of 2a“2 log ‘ random edges. Show that the graph 
G = H + R has diameter two w.h.p.
Problems for Chapter 11
11.1 Let ) be chosen so that the expected number of edges of the random graph 
C' ) 1_* is equal to $’. Prove that if $ = c log ‘, where c < 1 is a constant, then 
for a fixed 0 < * < 1, the random graph C' ) 1_* is disconnected w.h.p.
11.2 Define a randomized community-based small-world random graph T,; as follows. 
Partition a set of ' nodes into & communities (clusters), each of size $, i.e., 
‘ = & ■ $ and let cluster i be a complete graph K$", where i = 1,2,..., & . Next, 
represent each cluster as a vertex of an auxiliary binomial random graph G&,) .If 
vertices i and # are connected in G&,) by an edge, we randomly pick a vertex in 
either K^1" or K$#-1 and link it to all vertices of the other cluster. After completing 
this task for all edges of G&,), we connect vertices of r, which are linked to the 
same cluster.
Let $ ~ in ‘ and ) = cln', where c > 1 is a constant. Bound the expected 
number of edges of r, by O (‘ ln ‘) (see Cont and Tanimura [33]).
11.3 Use a similar argument as in the proof of Theorem 11.1 to show that if is a 
complete binary tree on 2$ - 1 vertices and we add two random matchings of 
size 2$-1 to the leaves of T, then the diameter of the resulting graph G satisfies 
diam(G) < log2 ‘ + log2 log ‘ + 10 w.h.p. (see [23]).

162
Small World
11.4 Let 0 < + < 2. Suppose now that we replace (11.12) by
P(^(m) = / = ($,€)) = d, where Dx = V d(x,y)~r.
. 
y^x
Show that now the expected length of the path found by of Section 11.2 is 
Q(n (2"r )/3).
11.5 Suppose that in the previous problem we have + > 2. Show now that the expected 
length of the path found by of Section 11.2 is Q(n('r~2'dd-11).
11.6 Let H be an arbitrary connected graph of bounded degree A. Let R be a random 
set of cn random edges. Show that the graph G = H + R has diameter O (log n) 
w.h.p. for sufficiently large c > 0.

12 Network Processes
Until now, we have considered “static” (in terms of the number of vertices) models 
of real-world networks only. However, more often the networks are constructed by 
some random “dynamic” process of adding vertices, together with some new edges 
connecting those vertices with the already existing network. Typically, networks grow 
(or shrink) during time intervals. Itis a complex process, and we do not fully understand 
the mechanisms of their construction as well as the properties they acquire. Therefore, to 
model such networks is quite challenging and needs specific models of random graphs, 
possessing properties observed in a real-world network. One such property is that often 
the degree sequence exhibits a tail that decays polynomially, as opposed to classical 
random graphs, whose tails decay (super)exponentially (see, for example, Faloutsos, 
Faloutsos, and Faloutsos [47]). Grasping this property led to the development of so- 
called preferential attachment models, whose introduction is attributed to Barabasi and 
Albert, but, in principle, known and studied earlier by random graph theorists as random 
plane-oriented recursive trees (see Chapter 14 of [52] for details and references). 
The Barabasi-Albert [13] model is ambiguously defined, as pointed out by Bollobas, 
Riordan, Spencer, and Tusnady [28] (see also Bollobas, Riordan [27]). Section 12.1 
studies a similar model to that of [28]. After the presentation of basic properties of 
the preferential attachment model, we conclude the first section with a brief discussion 
of its application to study the spread of infection through a network, called bootstrap 
percolation.
The last section of this chapter is devoted to a generalization of the preferential at­
tachment model, called spatial preferential attachment. It combines simple preferential 
attachment with geometry by introducing “spheres of influence” of vertices, whose 
volumes depend on their in-degrees.
12.1 Preferential Attachment
Fix an integer & > 0, constant and define a sequence of graphs G 1,G2,,G -. The 
graph G- has a vertex set [-] and G 1 consists of & loops on vertex 1. Suppose we have 
constructed G-. To obtain G-+1 we apply the following rule. We add vertex - + 1 and 
connect it to & randomly chosen vertices 21,22,..., 2& e [-] in such a way that for

164
Network Processes
i = 1,2,..., m,
deg (w,G-) 
P (2f = o) =---- —-----,
2mt
where deg(w, G-) is the degree of w in G-.
In this way, G-+1 is obtained from G- by adding vertex t +1 and m randomly chosen 
edges, in such a way that the neighbors of t+1 are biased toward higher-degree vertices.
When m = 1, G- is a tree and this is basically a plane-oriented recursive tree.
Expected Degree Sequence: Power Law
Fix t and let V$ (t) denote the set of vertices of degree k in G-, where m < k = O (t1/2). 
Let D$(t) = |V$(t)| and D$(t) = E(D$(t)). Then
E( D $ (t + 1)|G-) =
I (k - 1)D$_1 (t) 
kD$ (t) \
D $ (t) + m------—-----------+ 1$=& + £ (k,t). (12.1)
2mt 
2mt
Explanation of (12.1): The total degree of G- is 2mt and so *■$~12&~1 ^-is the 
probability that y;- is a vertex of degree k - 1, creating a new vertex of degree k. 
Similarly, $^?&--is the probability that y;- is a vertex of degree k, destroying a vertex 
of degree k. At this point t + 1 has degree m and this accounts for the term 1$=m. The 
term £(k,t) is an error term that accounts for the possibility that y;- = yfor some 
" + J.
Thus
£(k,t) = O ((mV^-1 = O(t"1/2). 
(12.2)
2 mt
Taking expectations over G- , we obtain
/ (k - 1)D $-1 (t) 
$d $ (t) \
D $ (t + 1) = D $ (t) + 1$=m + 
------------------------ + £ (k,t). 
(12.3)
2mt 
2mt
Under the assumption D$ (t) - d$t (justified below), we are led to consider the 
recurrence
($-1) dk_1-$dk
1$=m । 
2
d$ = -
Io
or
L^i2,1$ &
$ +2 d$-1 T $ +2
d $ = •
,0
— ii k > m, 
(12.4)
if k < m,
if k > m, 
if k < m.

12.1 Preferential Attachment
165
Therefore,
d _
& m + 2 ’
, _ , p % - 1 _ 
2m(m + 1) 
-.
dk ~ &m 
% + 2 “ k(k + 1)(k + 2) • 
( '5)
l=m+1
So for large k, under our assumption D $ (t) ~ d $ -, we see that
- . . 
2m(m + 1)
D $ (t)-------- ------ -1. 
(12.6)
k3
So the number of vertices of degree k decays like t/k3. We refer to this as a power 
law, where the decay is polynomial in k, rather than (super)exponential as in G„,m, 
m = cn. We now show that the assumption D$ (t) ~ d$ t can be justified. Note that the 
following theorem is vacuous for k » t1/6.
Theorem 12.1
|D$(t)- d$t| = 5(t1/2) for k = O(t1/2).
Proof Let
A$ (f) = D$ (f) - d$t.
Then, replacing 5$ (f) by A$ (t) + d$t in (12.3) and using (12.2) and (12.4), we get
A$(t + 1} = ^2/ 'A$~1 (t} + (1 " 2t) A$ (t) + O(t-1/2). 
(12.7)
Now assume inductively on t that for every k > 0,
|A$(t)| < At1/2(logt)?,
where (log t)^ is the hidden power of logarithm in C(t“1/2) of (12.7) and A is an 
unspecified constant.
This is trivially true for k < m also for small t if we make A large enough. So, 
replacing (5(t-1/2) in (12.7) by the more explicit at“1/2(log t)^, we get
A$ (t + 1) < k1
a 
A$-1 (t) +
2t
(
k
1 - y A$ (t)
2t
+ at 1/2 (log t) ^
< (log t)^ (At1/2 + at 1/2).
(12.8)
Note that if t is sufficiently large, then
I 111/2 
1
(t + 1)1/2 = 11/2 1 + 1 
> t1/2 + -±5.,
t 
3t1/2
and so
A$(t + 1) < (log(t + 1))^ (a (t + 1)1/2 ^-^2 + -1-) 
\ 
3t1/2 
t1'2 /
< A (log(t + 1))^ (t + 1)1/2,

166
Network Processes
assuming that A > 3a. 
□
In the next section, we will justify our bound of (5 (t1/2) for vertex degrees. After 
that we will prove the concentration of the number of vertices of degree k for small k.
Maximum Degree
Fix , < t and let X% be the degree of vertex , in G% for , < I < t. We prove the 
following high-probability upper bound on the degree of vertex ,.
Lemma 12.2
P(X- > Aem(t/,)1/2(log(t + 1))2) = O(t-A).
Proof Note first that Xs = m. If 0 < A < st = logl-1-+1), then
E (e2Xl+11 Xz)
k=0
< e2X%
m 
k 
m-k
= e2x y m 
X% 
1_e;.
\k I 12m// \ 
2m//
(1 + kA (1 + kA'))
Aft ( 
A(1 + A) Xi (m 1^A2Xl
e 1 1
e 
2/ 
4m/2
< 
e2Xl (1 + AX (1 + mA)j, 
since X% < 2m/,
< /11' '&" )Xi_
We define a sequence A = (As, As+1,..., At) where
1 + mA #
Aj+1 - 11---- yr— Aj < s- ■
2 
J
Here our only choice will be As. We show below that we can find a suitable value for 
this, but first observe that if we manage this then
E (eAs X-) < E (eAs+1 X--^ ■■■ < E (e2- Xs) < 1 + o (1).
Now
1 + met
2 j
implies that
n
1 + ms- 
^ 1 + me J 
m /1 \ V2
1 + 
< A, exp 
< em 
A,.
2 
# 
2 # 
,
J=s 
l=s

12.1 Preferential Attachment
167
So a suitable choice for A = As is
A, = e~m£t
This gives
So,
E (exp (e m£t(s/t) 1^2Xt< 1 + o(1).
P (X- > Aem(t/s)1/2(log(t + 1))2)) <
e-a- Aem (t/,)1/2 (log (t+1)2) £ feas X- j _ q (--
by the Markov inequality.
□
Thus with probability (1 - o (1)) as t to, we have that the maximum degree in Gt 
is Q (t1/2 (log t)2). This is not the best possible. One can prove that w.h.p. the maximum 
degree is Q(t 1/2w(t)) and Q(t l/2/w(t)) for any w(t) to, see, for example, Flaxman, 
Frieze, and Fenner [49].
Concentration of Degree Sequence
Fix a value k for a vertex degree. We show that D$ (t) is concentrated around its mean 
D $ (t).
Theorem 12.3
( 
.2 )
P(|D$(t)- D$(t)|> u)< 2exp 
. 
(12.9)
32mt
Proof Let Y1,Y2,..Ymt be the sequence of edge choices made in the construction 
of Gt, and for Y1,Y2,...,Y", let
Z" = Z"(Y1,Y2,...,Y") = E(D$(t) | Y1,Y2,...,Y"). 
(12.10)
Note that the sequence Zo, Z1,..., Zmt, where Z0 = E( D $ (t)), is a martingale. We 
will prove next that | Z" - Z"_11 < 4 and then (12.9) follows directly from the Azuma- 
Hoeffding inequality, see Lemma 10.14.
Fix Y1, Y2,..., Y" and Y" + Y". We define a map (measure preserving projection) <p 
of
Y1,Y2 ,...,Y"_1,Y" ,Y"+1,...,Ymt
to
Y1,Y2 ,...,Y"_1,(k" ,Y"+1,...,^mt
such that
| Z" (Y1,Y2,...,Y")- Z" (Y1,Y2,...,Y" )| < 4. 
(12.11)
In the preferential attachment model, we can view vertex choices in the graph G as

168
Network Processes
G
G
Figure 12.1 Constructing G from G
random choices of arcs in a digraph G, which is obtained by replacing every edge of 
G by a directed two-cycle (see Figure 12.1).
Indeed, if we choose a random arc and choose its head, then v will be chosen with 
probability proportional to the number of arcs with v as the head, i.e., its degree. Hence 
Y1,Y2,... can be viewed as a sequence of arc choices. Let
Y" = (x, y) where x > y,
(12.12)
Yi = (x, y) where x > y.
(12.13)
Note that x = x if i mod m + 1.
Now suppose # > i and Y# = (., v) arises from choosing (w, v). Then we define
Y# 
(w, v) + Y",
V (Y#) = # 
' 
7 
"
l(w,y) 
(w,v) = Y".
(12.14)
This map is measure preserving since each sequence v(Y1, Y2,..., Yt) occurs with the 
probability n--=i+1 j-1. Only x,x, y, y change the degree under the map v so D$ (t) 
changes by at most four. 
□
We will now study the degrees of early vertices.
Degrees of Early Vertices
Let dt (,) denote the degree of vertex , at time t. Then, we have ds (,) = m and
So, because
E(d-+1 (-)|G -) = d- (,) + mdt ,) = d- (,) 1 + 1­
2m t 
2-
22,+1,!(, - 1)! 
/nt1/2^ ,
------- 2(-) 
for large s.

12.1 Preferential Attachment
169
we have
_........... / - \ I/2 . . 
_____
E(d,(t)) ~ m I-I 
for large,. 
(12.15)
,
For random variables X, Y and a sequence of random variables Z = Zi,Z2,... ,Z $ 
taking discrete values, we write (as in (2.37))
X > Y to mean that P(X > a) > P(Y > a),
and
X \z > Y \z to mean that
P(X > a | Z% = z%,1 = 1,...,k) > P(Y > a | Z% = zi,1 = 1,...,k) 
for all choices of a, z.
Fix i < j - 2 and let X = dj-1 (t), Y = d j (t) and Z% = dl (t), % = i,.. ,,j - 2.
Lemma 12.4 X > Y .
Proof Consider the construction of Gj+1, Gj+2,..., G-. We condition on those edge 
choices of j +1, j + 2,..., t that have one end in i, i +1,..., j - 2. Now if vertex j does 
not choose an edge (j - 1,j) then the conditional distributions of dJ-_1 (t),dj(t) are 
identical. If vertex j does choose edge (j - 1, j) and we do not include this edge in the 
value of the degree of j - 1 at times j + 1 onwards, then the conditional distributions 
of dj-1 (t), dj (t) are again identical. Ignoring this edge will only reduce the chances 
of j - 1 being selected at any stage and the lemma follows. 
□
Corollary 12.5 If j > i - 2, then di (t) > (di+1 (t ) + ■■■ + dj (t))/( j - i).
Proof Fix i < I < j and then we argue by induction that
di+1 (1) + ' ' ' + dl (t) + (j - I)dl+1 (t) < di+1 (t) + ■■■ + (j - I + 1)dl (f). (12.16)
This is trivial for j = I as the LHS is then the same as the RHS. Also, if true for I = i,
then
di+1 (t) + • • • + dj (t) < (j - i)di+1 (t) < (j - i)di (t)
where the second inequality follows from Lemma 12.4 with j = i + 1.
Putting Z = di+1 (t),..., dl_1 (t), we see that (12.16) is implied by
dl(0 + (j ~ %)di+1 (0 \z< (# ~ % +1)d%C-) \z or d%+1 (0 \z< di(0 \z
after subtracting (j - %)dl+1 (t). But the latter follows from Lemma 12.4. 
□
Lemma 12.6 Fix 1 < , = O(1) and let m = log21 and let D, (t) = S,X^1 d, (t). 
Then w.h.p. D, (t) ~ 2m(mt)1^2.
Proof
We have from (12.15) that
PIP , tx1/2
E(D, (t)) ~ m 
(7) 
~ 2m(mt)1/2.
i=,+1 il

170
Network Processes
Going back to the proof of Theorem 12.3 we consider the map as defined in (12.14). 
Unfortunately, (12.11) does not hold here. But we can replace 4 by 10 log -, most of the 
time. So we let Y1,Y2,..., Y&- be as in Theorem 12.3. Then let ^" denote the number 
of times that (0, v) = Y" in equation (12.14). Now ^j is the sum of &- - # independent 
Bernoulli random variables and E(^,) < £,&=i+1 1/&J &~1 log &-. It follows from 
the Chernoff-Hoeffding inequality that P(^r > 10log -) < --10. Given this, we define 
a new random variable ds (-) and let Ds (-) = X##J । ds+ j (-). Here ds+ j (-) = ds+(-) for 
# = 1, 2,...,w unless there exists" such that ^- > 10 log -. If there is an " such that 
^" > 10 log -, then assuming that" is the first such we let Ds (-) = Z" (Y1, Y2,..., Y") 
where Z" is as defined in (12.10), with D$ (-) replaced by Ds (-). In summary, we have
P(Ds(-) + Ds (-))< --10. 
(12.17)
So,
| E(Ds (-))- E(Ds (-)| < --9.
And finally,
|Z" - Z"_11 < 20log-.
This is because each Y", Y" concerns at most two of the vertices s + 1, s + 2,...,s + w. 
So,
t .2 
1
P(|Ds(-) - E(Ds(-))l > u)< exp---------------r ■ 
(12.18)
800&- log2 -
Putting . = w3/4- 112 into (12.18) yields the claim. 
□
Combining Corollary 12.5 and Lemma 12.6, we have the following theorem.
Theorem 12.7 Fix 1 < s = O (1) and let w = log2 -. Thenw.h.p. d" (-) > &-1/2/w1^2 
for" = 1,2,..., s.
Proof Corollary 12.5 and (12.17) imply that d"(-) > D"(-)/w. Now apply Lemma 
12.6. 
□
We briefly discuss a simple application of the preferential attachment model in the 
following text.
Bootstrap Percolation
This is a simplified mathematical model of the spread of a disease through a 
graph/network G = (V,E). Initially a set A0 of vertices are considered to be in­
fected. This is considered to be round 0. Then in round - > 0, any vertex that has at 
least + neighbors in A-_1 will become infected. No-one recovers in this model. The 
main question is as to how many vertices eventually end up getting infected. There is 
a large literature on this subject with a variety of graphs G and ways of defining A0.
Here we will assume that each vertex s is placed in Aq with probability ), inde­
pendent of other vertices. The proof of the following theorem relies on the fact that 
with high probability all of the early vertices of G- become infected during the first 
round. Subsequently, the connectivity of the random graph is enough to spread the 

12.2 Spatial Preferential Attachment
171
infection to the remaining vertices. The following is a simplified version of Theorem 
1 of Abdullah and Fountoulakis [1].
Theorem 12.8 If + < & and w = log2 - and ) > wt 1^2 then w.h.p. all vertices in G- 
get infected.
Proof Given Theorem 12.7, we can assume that ds(t) > &t1/2/w1/2 for 1 < , < &.
In which case, the probability that vertex , < & is not infected in round 1 is at most
&-1 I 
1/2 
1/2 \ 
&-1
£ &— ' 
" ' )"(1 _ ))&-1'2M1'2-" < £ W"72e-(1-((1))& 
= ((1). (12.19)
t1 
" 
fe1
So, w.h.p. 1,2,..., & are infected in round 1. After this we use induction and the fact 
that every vertex " > , has & neighbors j < ". 
□
Exercises
12.1.1 Verify equation (12.8).
12.1.2 Verify equation (12.15).
12.1.3 Verify equation (12.19).
12.2 Spatial Preferential Attachment
The Spatial Preferential Attachment (SPA) model was introduced by Aiello, Bonato, 
Cooper, Janssen, and Pralat in [2]. This model combines preferential attachment with 
geometry by introducing “spheres of influence” of vertices, whose volumes depend on 
their in-degrees.
We first fix parameters of the model. Let & e N be the dimension of space R&, 
) e [0,1] be the link (arc) probability, and fix two additional parameters A1, A2, where 
A1 < 1/) while A2 > 0. Let 5 be the unit hypercube in R&, with the torus metric 
d(■, ■) derived from the L™ metric. In particular, for any two points 1 and 2 in 5,
d(1,2) = min {||i - 2 + u||ro : u e {-1,0,1}&} .
(12.20)
For each positive real number a < 1, and u e 5, define the ball around u with volume 
as
Ba(u) = {1 e 5 : d(u,i) < + a},
where + a = a1/&/2, so that + a is chosen such that Ba has volume a.
The SPA model generates a stochastic sequence of directed graphs {G-}, where 
G- = (V-, E-) and V- c 5, i.e., all vertices are placed in the &-dimensional hypercube 
5 = [0,1]&.
Let deg“(/; t) be the in-degree of the vertex / in G-, and deg+(/; t) its out-degree.

172
Network Processes
Then, the sphere of influence S (/; -) of the vertex / at time - > 1 is the ball centered 
at / with the following volume:
IS(/.,)!= min { 
/ -) + A2. 1j . 
(12.21)
In order to construct a sequence of graphs we start at - = 0 with G0 being the null 
graph. At each time step - we construct G- from G-_1 by, first, choosing a new vertex 
/— uniformly at random (uar) from the cube S and adding it to V-_1 to create V-. Then, 
independently, for each vertex u e V-_1 such that /- e S(u, - -1), a directed link (/-, u) 
is created with probability ). Thus, the probability that a link (/-, u) is added in time 
step - equals )|S(u,- - 1)|.
Power Law and Vertex In-degrees
(12.22)
(12.23)
(12.24)
Theorem 12.9 Let N"’ be the number of vertices of in-degree " in the SPA graph G -
at time - = n, where n > 0 is an integer. Fix ) e (0,1]. Then for any" > 0,
E( Ni,„) = (1 + ((1))c" n, 
where
1 
C0 ~ 1 + )A2 ’
and for 1 < " < n, 
c = 
) H 
#A1+ a2
Cl 
1 + )A2 + ")A1 1 =0 1 + )A2 + #)Af
In [2] a stronger result is proved which indicates that the fraction N^’ln follows a 
power law. It is shown that for " = 0,1,...,if, where "/ = (‘/log8n))A1/(4)A1+2>, 
w.h.p.
Nf>„ = (1 + ((1)) C" n.
Since, for some constant C,
C" = (1 + ((1)) c""(1+1/)A1),
it shows that for large " the expected proportion Nt’/n follows a power law with 
exponent 1 + 
, and concentration for all values of" up to "f.
To prove Theorem 12.9 we need the following result of Chung and Lu (see [32], 
Lemma 3.1) on real sequences.
Lemma 12.10 Let {a-}, {fl-} and {y-} be real sequences satisfying the relation 
/, fl­
a-+1 = 1 - 
a- + y-.

12.2 Spatial Preferential Attachment
173
Furthermore, suppose lim-^ro pt = P > 0 and lim-^!X, yt = y. Then lim-^ro ^ exists 
and
a- 
y
lim = 
.
t^OT - 
1 + p
□
Proof of Theorem 12.9
The equations relating the random variables N^- are described as follows. Since G 1 
consists of one isolated node, N0,1 = 1, and Ni;1 = 0 for i > 0. For all - > 0, we derive 
that
E(Ng,-+i - Ng,- |G-) = 1 - /W.,; - , 
(12.25)
while
E(NM+1 - Nf,t |G-) = pNi-1,- A1i+t A2 - pNt,tA1 (i ~ —1)+ A2 . 
(12.26)
Now applying Lemma 12.10 to (12.25) with
a- = E(Nq;-) Pt = pA2 and y- = 1,
we get that
E( Nq;-) = CQ + o (-),
where c0 as in (12.23).
For i > 0, Lemma 12.10 can be inductively applied with
a- = E(Nc-), p- = p(A1i + A2) and y- = E(Nf_1,t) A1 
-^ + A2
to show that
E( Ni,t) = C" + o (-), 
where
A1 (i - 1)+ A2
C" ~ pci~1 1 p a ■ a \' 
1 + p (A1t + A2)
One can easily verify that the expressions for c0, and C", i > 1, given in (12.23) and 
(12.24), satisfy the respective recurrence relations derived above. 
□
Knowing the expected in-degree of a node, given its age, can be used to analyze 
geometric properties of the SPA graph G- . Let us note also that the result below for 
i » 1 was proved in [61] and extended to all i > 1 in [37]. As before, let /" be the 
node added at time i.
Theorem 12.11 Suppose that i = i {-) » 1 as - 
. Then,
A2 t-\pa1 
A2
E(deg-(v",-)) = (1 + o(1))-2 d 
- -2,
A1 i 
A1

174
Network Processes
(12.27)
E(|5 (v,- ,t )|) = (1 + o(1)) A2t pA1~1i~pA1.
Moreover,forall i > 1,
/1 \ pA-i 
Aq
E(deg“(v,,t))< —2 , 
- -2,
A1 i 
A1
(12.28)
E(|5(v,-,t)|) < (1 + o(1))eA2tpA1~1i~pA1.
Proof In order to simplify calculations, we make the following substitution:
X (v, ,t) = deg-(v, ,t) + 42. 
(12.29)
A1
It follows from the definition of the process that
|X(v,,t) + 1, with probability pA1 xt/"’—, 
X (v, ,t), 
otherwise.
We then have
E(X(v,,t + 1) | X(v,,t))
= (X (v,.,) +1) r21XiiU> + X <v,.,) (1 - pA1X2±ll)
= X(v,.1) (1 + )^i.
Taking expectations over X(v,, t), we get
E(X(v,, t + 1)) = E(X(v,, t)) (1 + )^ .
Since all nodes start with in-degree zero, X(v,,i) = ^2. Note that, for 0 < x < 1, 
log(1 + x) = i - O(x2). If i » 1, one can use this to get
E( X (v,.,»= 42 n (1+)ti) = (1+((1)) 42 exp [y ^11.
a1 y 
# 
a1 
#7, ■#
and in all cases i > 1,
E(X(v,,t)) < ^2 exp 
.
A1 U-j #
=,
Therefore, when i » 1,
E(X(v,,t)) = (1 + o(1))^2 exp (pA1 log (-)) = (1 + o(1))^2 (-] 
,
A1 
i 
A1 i
and (12.27) follows from (12.29) and (12.21). Moreover, for any i > 1,
B < X (v„, ))S 41 exp (M1 (log (t) + 1/i)) S -42 (t)P'1.

12.2 Spatial Preferential Attachment
175
and (12.28) follows from (12.29) and (12.21) as before, which completes the proof. □
Directed Diameter
Consider the graph Gt produced by the SPA model. For a given pair of vertices 
V", v j e Vt (1 < i < j < -), let I(/", v j) denote the length of the shortest directed path 
from v j to V" if such a path exists, and let I (/", v j) = 0 otherwise. The directed diameter 
of a graph Gt is defined as
diam(Gt) = max I(/",v#).
1<"<j <t
We next prove the following upper bound on diam(Gt) (see [37]):
Theorem 12.12 Consider the SPA model. There exists an absolute constant c1 > 0 
such that w.h.p.
diam(Gt) < c1 log t.
Proof Let C = 18 max( A2,1). We prove that with probability 1 - o(t“2) we have that 
for any 1 < i < j < t, Gt does not contain a directed (/", v j)-path of length exceeding 
k* = C log t. As there are at most t2 pairs V", vj, Theorem 12.12 will follow.
In order to simplify the notation, we use v to denote the vertex added at step v < t. Let 
vPu = (v, t $-1, t$-2,.. .,t1,u) be a directed (v, u)-path of length k where t0 = u, t $ = v.
p 
[ A1 deg (t"-1,t")+ A2)
P(vPu exists) = 
p 
------------------- .
U X t" I
Let N(v, u, k) be the number of directed (v, u)-paths of length k, then
E( N (v,u,$ )) = 
J p $ E (fl ( '•-■■■-■.•■'■' 
.
We first consider the case where u tends to infinity together with t. It follows from
Theorem 12.11 that
A2 I t,- \pA1 
A2
E(deg-(t"-1,t")) = (1 + o(1))-2 -" 
- -2.
A1 t"-1 
A1
Thus
$ 1
E(N(v,u,$)) = 
X 
P$n T (A1 E(deg’(t"-1,t"))+ A2)
= 
z 
d+o (d) $ (^2 P) $ n 1 (-r 1PA1
u<t1 < •••<t$_1 </ 
"=1 " 
" 1
=<1 - o<1»$<a2p>$ (.)pA11 e n 1
u<t1 <■■■<tk^1<V "=1 
1

176
Network Processes
However, 
$-1 , 
/ 
, \ $-1
,e nI .',. e 1
U<t1 <---<-$_1<V " = 1 
u<s<v
77~ 
(log v/u + 1/u) $~1
(k - 1)!
< / e(log v/u + 1/u) \$ 1 
\ k^i ) 
■
Let k* = C log t, where C = 18 max(1, A2). Assuming that t is sufficiently large, and 
recalling that pA1 < 1, we have
v 
v/ (1+ 0 
a2 pe (log v/u + 1L $ ”1
) t E(N(y,u,k)) < 2A2 
-------------------^-1------------------
$>$* 
$>$* ' 
'
S 2,2 (<1+0(»>; 
+ V->)$■! (12.30)
= O (6"18 log t) 
(12.31)
= o(t-4).
The result follows for u tending to infinity. In the case where u is a constant, it follows 
from Theorem 12.11 that a multiplicative correction of e can be used in E(deg“ (tf_1, tf)), 
leading to replacing e by e2 in (12.30) and then 6 in (12.31) by 2, giving a bound of 
O (2~18 log t) = o (t“4) as before. This finishes the proof of the theorem. 
□
Exercises
12.2.1 Prove Lemma 12.10.
12.2.2 Verify equation (12.30).
12.2.3 Verify equation (12.31).
Problems for Chapter 12
12.1 Let Gt be the preferential attachment graph defined in Section 12.1 with m = 1, 
i.e., Gt is a plane-oriented recursive tree on t vertices. Let Lt denote the number 
of leaves (vertices of degree one) of Gt . Show that
2t - 1 
2t (t - 2)
E Lt = —-— and Var Lt =  ---- —.
- 
3 
t 
9(2t - 3)
12.2 Prove that Lt/t (defined above) converges in probability, to 2/3.
12.3 Show that w.h.p. the Preferential Attachment Graph of Section 12.1 has diameter 
O (log n). (Hint: Using the idea that vertex t chooses a random edge of the current 
graph, observe that half of these edges appeared at time t/2 or less).

12.2 Spatial Preferential Attachment
177
12.4 For the next few questions, we modify the Preferential Attachment Graph of 
Section 12.1 in the following way: First, let m = 1 and preferentially generate 
a sequence of graphs r1, r2,..., rm„. Then if the edges of rm„ are (u,, v,), i = 
1,2,..., mn, let the edges of Gn be (u[,/m-\, v[,/m-\), i = 1,2,..., mn. Show that 
(12.1) continues to hold.
12.5 Show that Gn of the previous question can also be generated in the following 
way:
(a) Let n be a random permutation of [2mn]. Let
X = {(a,, bi), i = 1,2,..., mn} where a, = min {n(2i - 1), n(2i)} and 
bi = max {x(2i - 1), n(2i)}.
(b) Let the edges of Gn be (a p/m, b [,-/&]), i = 1,2,..., mn.
This model was introduced in [28].
12.6 Show that the edges of the graph in the previous question can be generated as 
follows:
(a) Let ^1,&^2mn be independent uniform [0,1] random variables. Let 
{x, < y,} = {^2i-i, p-} for i = 1,2,..., mn. Sort the y, in increasing order 
R1 < R2 < ■ ■ ■ < Rmn and let R0 = 0. Then let
W# = Rm# and I# = (W;--1, W,] for j = 1,2,..., n.
This model was introduced in [27].
(b) The edges of Gn are (u,, v,), i = 1,2,..., mn where x, e IUi,y, e IVi.
12.7 Prove that (R1, R2,..., Rmn) can be generated as
R, = (-*-)1/2
\ Ymn+1 /
where Y,v = £1 + £2 + ■ ■ ■ + £N for N > 1 and ^1,^2,..., %mn+1 are independent 
exponential copies of EXP (1).
12.8 Let L be a large constant and let m = m (n) 
arbitrarily slowly. Then let &
be the event that
Y$ ~ k for — m [m, n] or k = mn + 1. 
m
Show that
(a) P(-£) = o(1).
(b) Let u, = t(,-1)m+1 + £(,-1)m+2 + • • • + ^,m ■ If & occurs then
/ i \ 1/2
(1) 
W, ~ 
for m < i < n, 
and
n
(2) 0, = W, - W,_1 ~ 
U\ 
for m < i < n.
2m (in)1'2 * * * * *
(c) u, log n for i e [n] w.h.p.
(d) u, log log n for i e [(log n)10] w.h.p.
(e) If m < i < j < n then P(edge ij exists) ~ 2^,^ 1/2.
(f) U, 
iogiog' and i < to(l"g „)3 impliesthe degree dn (i) - u, (') 1/2 ■

13 Intersection Graphs
Let bea (finite, simple) graph. We say that is an intersection graph ifwe can assign 
to each vertex / e V(G) a set 5/ so that {/, 0} e E(G) exactly when S/ n 50 + to. In 
this case, we say G is the intersection graph of the family of sets 5 = {S/ : / e V (G)}.
Although all graphs are intersection graphs (see Marczewski [84]) some classes 
of intersection graphs are of special interest. Depending on the choice of family 5, 
often reflecting some geometric configuration, one can consider, for example, interval 
graphs defined as the intersection graphs of intervals on the real line, unit disc graphs 
defined as the intersection graphs of unit discs on the plane, etc. In this chapter, we 
will discuss properties of random intersection graphs, where the family 5 is generated 
in a random manner.
It is worth mentioning that specific variants of random intersection graphs have a 
diverse range of applications. For example, binomial intersection random graphs, the 
topic of the first section, can serve as a model of networks with communities, i.e., 
local structures (cliques, bipartite cliques, etc.) with higher density than the network 
average. They can also be applied in classification to find clusters and to test their 
randomness in sets of nonmetric data, as well as in studying techniques of handling 
incomplete (missing) data, just to name a few.
Random geometric graphs, studied in Section 13.2, find a huge number of appli­
cations in modeling telecommunication (cell phone), information, security, neuronal, 
and many other real-world networks.

13.1 Binomial Random Intersection Graphs
179
13.1
Binomial Random Intersection Graphs
Binomial random intersection graphs were introduced by Karonski, Scheinerman, and 
Singer-Cohen [68] as a generalization of the classical model of the binomial random 
graph G',p.
Let ‘, & be positive integers and let 0 < ) < 1. Let V = {1,2,.,‘} be the set of 
vertices and for every 1 < $ < ‘, let S$ be a random subset of the set M = {1, 2,.. .,& } 
formed by selecting each element of M independently with probability ) . We define 
a binomial random intersection graph G(‘,&,)) as the intersection graph of sets 
S $, $ = 1,2,...,‘. Here S1,S2, ...,S‘ are generated independently. Hence, two 
vertices" and # are adjacent in G (‘, &,)) if and only if S" n Sj + 0.
There are other ways to generate binomial random intersection graphs. For example, 
we may start with a classical bipartite random graph G‘;&;), with vertex set bipartition
(V, M ),V = {1,2,...,'}, M = {1,2, ...,&},
where each edge between V and M is drawn independently with probability ) . Next, 
one can generate a graph G(‘, &,)) with vertex set V and vertices" and # of G (‘, &,)) 
connected if and only if they share a common neighbor (in M) in the random graph 
G‘,&;). Here the graph G„.m.) is treated as a generator of G(‘, &,)).
One observes that the probability that there is an edge {", j} in G(‘, &,)) equals 
1 -(1 -)2)&, since the probability that sets S" and Sj are disjoint is (1 -)2)&; however, 
in contrast with G„.), the edges do not occur independently of each other.
Another simple observation leads to some natural restrictions on the choice of 
probability ). Note that the expected number of edges of G (‘, &,)) is
' (1 -(1 - )2)&)- '2&)2/2,
provided &)2 
0 as ‘ 
to. Therefore, if we take ) = (((‘V&)-1), then the
expected number of edges of G(‘,&,)) tends to 0 as ‘ to and therefore w.h.p. 
G(‘, &,)) is empty.
On the other hand, the expected number of non-edges in G (‘,&,)) is
2 (1 - )2)& < '2-' &)
Thus, if we take ) = (2log‘ + w(‘))/&)1/2, where w(‘) 
c» as ‘ m, then
the random graph G(‘, &,)) is complete w.h.p. One can also easily show that when 
w(‘) 
-<x>, then G(‘,&,)) is w.h.p. not complete. So, when studying the evo­
lution of G(‘,&,)), we may restrict ourselves to values of ) in the range between 
w(‘)/(‘V&) and ((2log‘ - w(‘))/&)1^2, where w(‘) to.
Equivalence
One of the first interesting problems to be considered is the question as to when the ran­
dom graphs G(‘, &,)) and G„.) have asymptotically the same properties. Intuitively, 

180
Intersection Graphs
it should be the case when the edges of G(n, m, p) occur “almost independently,” i.e., 
when there are no vertices of degree greater than two in M in the generator Gn,m,P 
of G(n, m,p). Then each of its edges is induced by a vertex of degree two in M, 
“almost” independently of other edges. One can show that this happens w.h.p. when 
p = o (1/(nm1/3)), which in turn implies that both random graphs are asymptotically 
equivalent for all graph properties P. Recall that a graph property P is defined as 
a subset of the family of all labeled graphs on vertex set [n], i.e., P Q 2©. The 
following equivalence result is due to Rybarczyk [104] and Fill, Scheinerman, and 
Singer-Cohen [48].
Theorem 13.1 Let 0 < a < 1, P be any graph property, p = o (1/(nm1/3)) and
p = 1 - exp (-mp2(1 - p)n~2j. 
(13.1)
Then
P(G„,p e P) 
a
if and only if
P(G(n,m,p) e P) a
as n .
Proof Let X and Y be random variables taking values in a common finite (or count­
able) set 5. Consider the probability measures Z(X) and Z(Y) on 5 whose values at 
A c 5 are P(X e A) and P(Y e A). Define the total variation distance between Z(X) 
and Z(Y) as
dTV(Z(X), Z(Y)) := sup | P(X e A) - P(Y e A)|, 
ACS
which is equivalent to
dTV (Z(X), Z(Y)) = 2 2 I P(X = ,) - P(Y = <
2 , eS
Note (see Fact 4 of [48]) that if there exists a probability space on which random 
variables X' and Y' are both defined, with Z(X) = Z(X') and Z(Y) = Z(Y'), then
dTV (£(.X), Z(Y)) < P(X’ + Y'). 
(13.2)
Furthermore (see Fact 3 of [48]), if there exist random variables Z and Z' such that 
£(X|Z = z) = Z(Y|Z' = z) for all 3, then
dTV (£(X), Z(Y)) < 2dTv (Z(Z), Z(Z')). 
(13.3)
We will need one more observation. Suppose that a random variable X has the distribu­
tion Bin(n, p), while a random variable Y has the Poisson distribution, and EX = E Y. 
Then
dTV (X, Y) = O (p). 
(13.4)

13.1 Binomial Random Intersection Graphs
181
We leave the proofs of (13.2), (13.3), and (13.4) as problems listed at the end of this 
chapter.
To prove Theorem 13.1, we also need some auxiliary results on a special coupon 
collector scheme.
Let Z be a non-negative integer-valued random variable, r a non-negative integer, 
and y a real such that ry < 1. Assume we have r coupons Qi, Q2,,Qr and one 
blank coupon B. We make Z independent draws (with replacement) such that in each 
draw,
P(Q;- is chosen) = y, for i = 1,2,..., r,
and
P(B is chosen) = 1 - ry.
Let Ni (Z), i = 1,2,..., r be a random variable counting the number of times that 
coupon Q" was chosen. Furthermore, let
X" (Z) = {0
if N" (Z) > 1, 
otherwise.
The number of different coupons selected is given by
r
X (Z) = £ X" (Z). 
(13.5)
"=1
With the above-mentioned definitions we observe that the following holds.
Lemma 13.2 If a random variable Z has the Poisson distribution with expectation 
d, then N" (Z), i = 1,2,..., r, are independent and identically Poisson distributed 
random variables, with expectation dy. Moreover, the random variable X(Z) has the 
distribution Bin(r, 1 - e“Ay).
Let us consider the following special case of the scheme defined earlier, assuming 
that r = (') and y = 1/(2). Here each coupon represents a distinct edge of Kn.
Lemma 13.3 Suppose p = o (1/«) and let random variable Z be
Bin(m, (')p2(1 - p)n~2) distributed, while random variable Y is
Bin(('), 1 - e~mp2^1-p' 2j distributed, then
dTV (Z(Y), Z(X(Z))) = o(1).
Proof Let Z' be a Poisson random variable with the same expectation as Z, i.e.,
(
' Ip2(1 - p)' 2.
By Lemma 13.2, X(Z') has the binomial distribution
Bin (M, 1- e“mp2(1"p)’"1,
H2J 
/

182
Intersection Graphs
and so, by (13.3) and (13.4), we have
dTV (Z(Y), Z( X (Z))) = dTV (Z( X (Z')), Z( X (Z)))
< 2dTv(X(Z'), Z(Z)) < 
Qp2(1 - p)n~2
= O (n2p2) = o (1).
□
Now define a random intersection graph G2(n, m,p) as follows. Its vertex set is 
V = {1,2, ...,n}, while e = {i,j} is an edge in G2 (n, m, p) if and only if in a 
(generator) bipartite random graph Gn,m,p there is a vertex w e M of degree two such 
that both i and j are connected by an edge with w.
To complete the proof of our theorem, note that
dTv (Z(G (n,m,p)), £(Gn,p))
< dTv(X(G(n, m,p)),Z(G2(n, m,p))) + dTv(Z(G2(n, m,p)),Z(Gn,p)),
where p is defined in (13.1). Now, by (13.2),
dTv (X(G (n, m,p)), Z(G 2 (n, m,p)))
< P(Z(G(n,m,p)) + £(G2(n,m,p)))
< P(3w e M of Gn,m,p s.t. deg(w) > 2) < m(”jp3 = o(1) 
for p = o (1/(nm1^3).
Hence, it remains to show that
dTv (Z(G 2 (n, m,p)), £(G‘,p)) = o (1). 
(13.6)
Let Z be distributed as Bin (m, (')p2(1 - p)n“2), X(Z) is defined as in (13.5), 
and let Y be distributed as Bin (('), 1 - e~mp2^1-p' 2). Then the number of edges 
|E(G2(n,m,pS}\ = X(Z) and |E(Gn>p))| = Y. Moreover, for any two graphs G and 
G ’ with the same number of edges
P(G 2 (n, m, p) = G) = P(G2 (n, m, p) = G')
and
P(Gn,p = G) = P(Gn,p = G').
Equation (13.6) now follows from Lemma 13.3. The theorem follows immediately. □
For monotone properties (see Section 3.1), the relationship between the classical 
binomial random graph and the respective intersection graph is more precise and was 
established by Rybarczyk [104].
Theorem 13.4 Let 0 < a < 1, m = na, a > 3. Let P be any monotone graph 
property. For a > 3, assume
Q (^/(nm1/3)] = p = O(Vlog n/m),

13.1 Binomial Random Intersection Graphs
183
while for a = 3, assume (1/(nm1/3)) = o (p). Let
p = 1 - exp l-mp2(1 - p)' 2
If for all £ = £ (n) 
0,
then
P(G«, (1+s)p 
a,
P(G(n,m,p) e P) 
a
as n c» .
Small Subgraphs
Let H be any fixed graph. A clique cover C is a collection of subsets of vertex set 
V(H) such that each induces a complete subgraph (clique) of H, and for every edge 
{u, v} e E(H), there exists C eC such that u,v e C. Hence, the cliques induced by 
sets from C exactly cover the edges of H. A clique cover is allowed to have more than 
one copy of a given set. We say that C is reducible if for some C e C, the edges of H 
induced by C are contained in the union of the edges induced by C\C, otherwise C is 
irreducible. Note that if C e C and C is irreducible, then |C | > 2.
In this section, |C| stands for the number of cliques in C, while 2 C denotes the 
sum of clique sizes in C, and we put 2 C = 0 if C = 0.
Let C = {C1, C2,..., C$ } be a clique cover of H.For 5 c V (H), define the following 
two restricted clique covers
Ct[5] := {Ci n S : |Cf nS| > t, i = 1,2,...,k},
where t = 1,2. For a given S and t = 1,2, let
t- = t- (H, C,S) = (n^ c- [5 ] m'c-15 
c-151"1.
Finally, let
t(H) = mjn max {ti,t2},
where the minimum is taken over all clique covers C of H. We can in this calculation 
restrict our attention to irreducible covers.
Karonski, Scheinerman, and Singer-Cohen [68] proved the following theorem.
Theorem 13.5 Let H be a fixed graph and mp2 
0. Then
lim P(H C G(n,m,p)) = 
'
if p/t (H) 
0,
if p/T(H)
0
1
As an illustration, we will use this theorem to show the threshold for complete 
graphs in G(n, m, p), when m = na, for different ranges of a > 0.

184
Intersection Graphs
t (Kh ) =
Corollary 13.6 For a complete graph Kh with h > 3 vertices and m = na, we have 
n~1m~1th 
for a < 2h/(h - 1),
‘-!/(h-1)m-1/2 for a > 2h/(h - 1).
Proof There are many possibilities for clique covers to generate a copy of a complete 
graph Kh in G(n, m, p). However, in the case of Kh, only two play a dominating role. 
Indeed, we will show that for a < a0, a0 = 2h/(h - 1), the clique cover C = {V(Kh)} 
composed of one set containing all h vertices of Kh only matters, while for a > a0 the 
clique cover C = (K), consisting of (h) pairs of endpoints of the edges of Kh, takes 
the leading role.
Let V = V(Kh) and denote those two clique covers by {V} and {E}, respectively. 
Observe that for the cover {V} the following equality holds:
max{n(Kh, {V}, 5),T2(Kh, {V}, 5)} = n(Kh, {V}, V). 
(13.7)
S QV
To see this, check first that for |51 = h,
T1 (Kh, {V},V) = T2(Kh, {V},V) = n-1 m-1/h.
For 5 of size |51 = ,, 2 < , < h - 1, restricting the clique cover {V} to 5 gives a single 
,-clique, so for - = 1,2,
T— (Kh, {V}, 5) = n~1m~1^s < n~1 m~1^h.
Finally, when 151 = 1, then n = (nm)"1, while t2 = 0, both smaller than n-1 m“1/h, 
and so equation (13.7) follows.
For the edge-clique cover {E}, we have a similar expression, namely.
max{T1 (Kh, {E}, 5), T2(Kh, {E}, 5)} = n(Kh, {E}, V). 
(13.8)
S CV
To see this, check first that for |51 = h,
T1 (Kh, {E},V) = n"1/(h-1)m"1/2.
Let 5 c V, with , = |51 < h - 1, and consider restricted clique covers with cliques of 
size at most two, and exactly two.
For T1, the clique cover restricted to 5 is the edge-clique cover of K,, plus a one- 
clique for each of the h - , external edges for each vertex of K,, so
T1 (Kh {E} 5) = (n5/[,(,-1)+,(h-,)] m[,(,-1)/2+,(h-,)]/[,(,-1)+,(h-,)] ) 1
= (n1/(h-1) m [h-(,+1)/2]/(h-1) j-1
< 
(n1/(h-1) mh/(2(h-1)) j-1
1
< n1^-1) m1^2 
,

13.1 Binomial Random Intersection Graphs
185
while for t2 we have
T2(Kh, {E},$) = (nV(,-l)&1/2)-1 < („1/(A-1)&1/2)-1 ,
thus verifying (13.8).
Let C be any irreducible clique cover of Kh (hence each clique has size at least two). 
We will show that for any fixed a,
ri(Kh, {V}, V) for a < 2h/(h - 1), 
T{Kh, {E},V) for a > 2h/(h - 1).
Thus,
T1 (Kh, C,V) > min {n(Kh, {V},V),n(Kh, {E},V)} . 
(13.9)
Because m = na, we see that
T1 (Kh, C,V) = n’1c( "',
where
1 c(a) = yu +Yca 1 |v l(a) = 1 + v 1|E l(a) = -J 1+ a ■
(To simplify notation, in the following equation, we have replaced 1 {v j., 1 {E j. by iy, xE, 
respectively.) Note that for a0 = 2h/(h - 1) exponents
1v (ao) = 1e (aa) = 1 + ------- .
h - 1
Moreover, for all values of a < a0, the function 1V (a) > 1E(a), while for a > a0, 
the function 1V (a) < 1E(a).
Now, observe that 1 c (0) = 
< 1 since each vertex is in at least one clique of C.
Hence 1 c(0) < 1y (0) = 1. We will also show that 1 c(a) < 1y (a) for a > 0. To see 
this, we need to bound |C|/2 C.
Suppose that u e V(Kh) appears in the fewest number of cliques of C, and let + be 
the number of cliques Ci e C to which . belongs. Then
2C = 2 |Cil+ 2 !Cil > ((h - 1)+ +)+ 2(|C|- +),
where h - 1 counts all other vertices aside from u since they must appear in some 
clique with u.
For any / e V (Kh), we have
^ C + |{" : C" 3 /}|-(h - 1) > ^ C + + -(h - 1)
> (h - 1) + + + 2(|C| - +)+ + - (h - 1)
= 2|C|.
Summing the above-mentioned inequality over all / e V(Kh), 
h E c+E C- h(h - 1) > 2hId, 

186
Intersection Graphs
and dividing both sides by 2h £ C, we finally get
\C\ h + 1 h - 1
£C ~ 2h ~ 2 2 c'
Now, using the above-mentioned bound,
x '= ys+B (wa
< h h + 1
h - 1 \
2 X C)
/ 2h \
h 1)
2
h - 1
1 +
- xv (»o).
Now, since xc(a) < xv (a) at both a = 0 and a = a0, and both functions are linear, 
xc(a) < xv (a) throughout the interval (0, a0).
Since xE(a0) = xv(a0), we also have xc(a0) < xE(a0). The slope of xc(a) is 
, and by the assumption that C consists of cliques of size at least two, this is at 
most 1/2. But the slope of xE(a) is exactly 1/2. Thus, for all a > a0, xc(a) < xE(a). 
Hence the bounds given by formula (13.9) hold.
One can show (see [105]) that for any irreducible clique-cover C that is not {V} nor 
{E },
max{n(Kh, C,S),T2(Kh, C,S}}> n (Kh, C, V).
Hence, by (13.9),
max{n (Kh, C,S),T2(Kh, C,S)} > min{n (Kh, {V},V),n {Kh, {E},V)}.
This implies that
jn-1m-1/h 
for a < a0,
jw-1/(h-1)&-1/2 for a > a0,
which completes the proof of Corollary 13.6.
□
To add to the picture of asymptotic behavior of small cliques in G(n,m, p), we will 
quote the result of Rybarczyk and Stark [105], who obtained an upper bound on the 
total variation distance between the distribution of the number of h-cliques and a 
respective Poisson distribution for any fixed h.
Theorem 13.7 Let G(u, m,p) be a random intersection graph, where m = na. Let 
c > 0 be a constant and h > 3 a fixed integer, and Xn be the random variable counting 
the number of copies of a complete graph Kh in G {n, m, p).
(i) If a < hp! ,p ~ cn 1 m 1!h, then
An = EXn - ch/h!

13.2 Random Geometric Graphs
187
and
dTv (£(Xn), Po(2„)) = O (n-a^h) .
(ii) If a = 
,p ~ cn~^h+v>/(. h~v>, then
An = EXn - [c^h + ch(h-1)) /A!
and
dTV 
Xn), Po(2„)) = O (n-2/(h-1)) .
(iii) If a > hh ’P ~ cn-1^ h~v> m-1^2, then
An = EX’ ~ ch(h-1)/A!
and 
. I (h_ °fr-D- 
1 
,
dTV (Z(Xn), Po(An)) = O 
h 2 
+ n~1
Exercises
13.1.1 Show that if p = w(n)/(n^m), and w(n) 
to, then G(n,m,p) has w.h.p. at
least one edge.
13.1.2 Show that if p = (2log n + w(n))/m)1/2 and w(n) 
-to, then w.h.p.
G(n, m, p) is not complete.
13.1.3 Determine the expected number of isolated vertices in G(m, n, p).
13.1.4 Draw all clique covers of a triangle.
13.2 Random Geometric Graphs
The graphs we consider in this section are the intersection graphs that we obtain from 
the intersections of balls in the d-dimensional unit cube, D = [0,1]d, where d > 2. 
For simplicity, we will only consider d = 2 in the text.
We let X = {X1, X2,..., Xn} be independently and uniformly chosen from D = 
[0,1]2. For + = + (n), let Gx,r be the graph with vertex set X. We join Xf, X# by an 
edge if and only if X# lies in the disk
B (X" ,r) = {X 6 [0,1]2 : | X - X" | < +} .
Here | | denotes Euclidean distance.
For a given set X we see that increasing r can only add edges and so thresholds are 
usually expressed in terms of upper/lower bounds on the size of r.
The book by Penrose [100] gives a detailed exposition of this model. Our aim here 
is to prove some simple results that are not intended to be best possible.

188
Intersection Graphs
Connectivity
The threshold (in terms of +) for connectivity was shown to be identical with that for 
minimum degree one, by Gupta and Kumar [57]. This was extended to $-connectivity 
by Penrose [99]. We do not aim for tremendous accuracy. The simple proof of connec­
tivity was provided to us by Tobias Muller [92].
Theorem 13.8 Let s > 0 be arbitrarily small and let r0 = r0 (‘) = -°log’'. Then 
w.h.p.
Gx,r contains isolated vertices if r < (1 - e)r0, 
(13.10)
Gx,r is connected if + > (1 + e)r0.
(13.11)
Proof First consider (13.10) and the degree of 1. Then
P(X1 is isolated) > (1 - nr2)n~1.
The factor (1 - nr2)n~1 bounds the probability that none of X2,X3,. ..,Xn lie in 
B (X1, +) given that B (X1, r) c D. It is exact for points far enough from the boundary 
of .
Now,
(1 -nr2)n~1 > (1 - (1 ~s)lOg'‘"'■■■■ (1). 
'
So if I is the set of isolated vertices, then E(|11) > ‘s~1+((1) 
TO. Now,
/ 
2 
\ n-2
nr
P(X1 e I|X2 e I)< 1 - ------ 2 
<(1 + ((1)) P(X1 e I).
1 - nr2
(
2 \
1 _ 1 
21 is the probability that a random point does not lie in
B(X1,r) given that it does not lie in B(X2,r), and that |X2 - X11 > 2r. Equation 
(13.10) now follows from the Chebyshev inequality (2.18).
Now consider (13.11). Let r « s be a sufficiently small constant and divide D into 
12 sub-squares of side length rr, where l0 = 1/rr. We refer to these sub-squares as 
cells. We can assume that r is chosen so that l0 is an integer. We say that a cell is 
good if contains at least "0 = r3 log ‘ members of X and bad otherwise. We next let 
K = 100/r2 and consider the number of bad cells in a K x K square block of cells.
Lemma 13.9 Let B be a K x K square block of cells. The following hold w.h.p.:
(a) IfB is further than 100r from the closest boundary edge ofD, then B contains at 
most $0 = (1 - e/10)n/rj2 bad cells.
(b) IfB is within distance 100r of exactly one boundary edge ofD, then B contains at 
most $0/2 bad cells.
(c) If B is within distance 100r of two boundary edges of D, then B contains no bad 
cells.

13.2 Random Geometric Graphs
189
Proof (a) There are less than f% < n such blocks. Furthermore, the probability that a 
fixed block contains k0 or more bad cells is at most
lK 2 1 
k 0
I "0 I \ 
\ $0
Zl ”) (f2+2)"(1 _ f2+2) ”’"' 
i=0 1
2 \ $0 I / \ "0 
\ $0
K e 2 — | (f2+2)"0e-n2r2('-"0) 
$0 
i0
(13.12)
The left-hand side (LHS) of (13.12) does require validation. We do this in terms of 
classic results about balls in bins. Think of the cells as M bins and the points X as 
balls. Then if Ws denotes the number of balls in the set of boxes 5, then we have the 
following.
Lemma 13.10 Let S1, S2,... ,S$ be disjoint subsets of [ M ] and let s1,s2,... ,s$ be 
non-negative integers. Then 
/ $ 
\ 
$
P Q{Ws( < S"} < n P({Ws( < S"}), 
/ $ 
\ 
$
P Q{Ws( > S"} < n P({Ws( > S"}).
The first inequality of Lemma 13.10 implies the LHS of (13.12). For a proof of 
Lemma 13.10, see, for example, Chapter 22.2. of [52]. Now, 
/ 
\ "0
ne 
f2+2)"0 e-^2+2 ('-"0) < n-n2 (1+«)/
"0 
' 
"
for f sufficiently small. So we can bound the right-hand side (RHS) of (13.12) by 
1 
2 
\ (1-E110) ”1 n2
12K2en"n (1+e)/M 
_1_e/3
(1 - e/10)^/'f2 
~
(13.13)
(13.14)
Part (a) follows after inflating the RHS of (13.14) by n to account for the number of 
choices of block.
(b) Replacing k0 by k0/2 replaces the LHS of (13.14) by
l 
2 
\ (1-e/10) ^/2^2
4K2 en-■»"♦ ■ 
* 
,,2. „„
(1 - c/10) >r/2,2 
-
(13.15)
Observe now that the number of choices of block is O(l0) = o(n1/2) and then Part 
(b) follows after inflating the RHS of (13.15) by o(n1/2) to account for the number of 
choices of block.
(c) Equation (13.13) bounds the probability that a single cell is bad. The number of 
cells in question in this case is O (1) and (c) follows. 
□ 

190
Intersection Graphs
We now do a simple geometric computation in order to place a lower bound on the 
number of cells within a ball B (X, +).
Lemma 13.11 A half-disk of radius +1 = + (1 - rV2) with diameter part of the grid 
of cells contains at least (1 - 2^1/2)^!2rj2 cells.
Proof We place the half-disk in a 2+1 x +1 rectangle. Then we partition the rectangle 
into £1 = +1/+r rows of 2^1 cells. The circumference of the circle will cut the "th row 
at a point which is +1 (1 - "2r2)1/2 from the center of the row. Thus, the "th row will 
contain at least 2 [+1 (1 - "2r2)1/2/+r_| complete cells. So the half-disk contains at least
1 
11 ^
— 2((1_ "2r2)1/2 - r)
+r £r
- i2r2)1/2 - r) di
arcsin (1-^)
(cos2(0) - r cos(0))d0 
6=arcsin( ^)
2+1 [0
+r2 2
sin (20)
- r
arcsin (1-^)
6=arcsin( ^)
Now,
arcsin (1 - r) >
172
2 - 2r1/2 and arcsin(r) < 2r
So the number of cells is at least
2+1 / n
— 
~ r
+r2 4
This completes the proof of Lemma 13.11.
We deduce from Lemma 13.9 and Lemma 13.11 that
□
X e X implies that B(X, +1) n D contains at least one good cell. 
(13.16)
Now let r be the graph whose vertex set consists of the good cells and where cells 
c1, c2 are adjacent if and only if their centers are within distance +1. Note that if c1, c2 
are adjacent in r, then any point in X n c1 is adjacent in G%,r to any point in X n c2. 
It follows from (13.16) that all we need to do now is show that r is connected.
It follows from Lemma 13.9 that at most ^/r2 rows of a K x K block contain a bad 
cell. Thus, more than 95% of the rows and of the columns of such a block are free of 
bad cells. Call such a row or column good. The cells in a good row or column of some 
K x K block form part of the same component of r. Two neighboring blocks must have 
two touching good rows or columns so that the cells in a good row or column of some 
block form part of a single component of r. Any other component C must be in a block 
bounded by good rows and columns. But the existence of such a component means that 
it is surrounded by bad cells and then by Lemma 13.11 that there is a block B with at 
least (1 - 3r1/2)^/r2 bad cells if it is far from the boundary and at least half of this if it 
is close to the boundary. But this contradicts Lemma 13.9. To see this, consider a cell in 
C whose center has the largest second component, i.e., is highest in C. Now consider 
the half-disk of radius +1 that is centered at . We can assume (i) is contained

13.2 Random Geometric Graphs
191
entirely in B and (ii) at least (1 - 2p1l2)^/2^2 - (1 - ^V2)/^ > (1 - 3^1/2)^/2^2 cells 
in are bad. Property (i) arises because cells above whose centers are at distance 
at most +1 are all bad and for (ii) we have discounted any bad cells on the diameter 
through that might be in . This provides half the claimed bad cells. We obtain the 
rest by considering a lowest cell of . Near the boundary, we only need to consider 
one half-disk with diameter parallel to the closest boundary. Finally, observe that there 
are no bad cells close to a corner. 
□
Hamiltonicity
The first inroads on the Hamilton cycle problem were made by Diaz, Mitsche, and 
Perez-Gimenez [38]. Best possible results were later given by Balogh, Bollobas, Kriv- 
elevich, Muller, and Walters [12] and by Muller, Perez, and Wormald [91]. As one 
might expect Hamiltonicity has a threshold at + close to +0, as defined in Theorem 13.8. 
We now have enough to prove the result from [38]. We start with a simple lemma, 
taken from [12].
Lemma 13.12 The subgraph r contains a spanning tree of maximum degree at most 
six.
Proof Consider a spanning tree T of r that minimizes the sum of the lengths of the 
edges joining the centers of the cells. Then does not have any vertex of degree greater 
than six. This is because, if center / were to have degree at least seven, then there are 
two neighboring centers m, o of / such that the angle between the line segments [/,m] 
and [/, o] is strictly less than 60 degrees. We can assume without loss of generality 
that [/, m] is shorter than [/, o]. Note that if we remove the edge {/, o} and add the 
edge {u,o}, then we obtain another spanning tree but with strictly smaller total edge 
length, a contradiction. Hence, T has maximum degree at most six. 
□
Theorem 13.13 Suppose that + > (1 + e)+0. Then w.h.p. Gx,r is Hamiltonian.
Proof We begin with the tree T promised by Lemma 13.12. Let be a good cell. We 
partition the points of X n c into 2d roughly equal size sets P1,P2,..., P2d, where 
d < 6 is the degree of c in T. Since the points of X n c form a clique in G = G^;+, we 
can form 2d paths in G from this partition.
We next do a walk through T, for example, by breadth-first search that goes 
through each edge of T twice and passes through each node of r a number of times 
equal to twice its degree in r. Each time we pass through a node we traverse the vertices 
of a new path described in the previous paragraph. In this way, we create a cycle 
that goes through all the points in X that lie in good cells.
Now, consider the points P in a bad cell c with center 1. We create a path in G 
through P with endpoints 1,2, say. Now, choose a good cell c' contained in the ball

192
Intersection Graphs
B(x, +i) and then choose an edge {u, /} of H in the cell c'. We merge the points in 
P into H by deleting {u, /} and adding {x, u}, {y, /}. To make this work, we must be 
careful to ensure that we only use an edge of H at most once. But there are Q(log n) 
edges of H in each good cell and there are O (1) bad cells within distance 2+ say of 
any good cell and so this is easily done. 
□
Chromatic Number
We look at the chromatic number of Gx,r in a limited range. Suppose that nxr2 = log', 
where wr to, m+ = O(log n). We are below the threshold for connectivity here. We 
will show that w.h.p.
X(Gx+)~ A(Gk,+)~ c%(Gx+,
where we will use c% to denote the size of the largest clique. This is a special case of a 
result of McDiarmid [87]. We first bound the maximum degree.
B(Z)< n n (,r2)$Q < T
kq 
nMr log n
Lemma 13.14
log n 
A(Gx +) ~ ;--------w.h.p.
log Mr
Proof Let Z$ denote the number of vertices of degree k and let Z > $ denote the number 
of vertices of degree at least k. Let kQ = o', where Md 
to and Md = o(or ). Then
log ' 
log '
^d 
eMd ^d
= n 
.
Mr
So,
log(E(Z>$0)) < log' (Md + 1 + log Md - log Mr ) . 
(13.17)
Md
Now let eq = m^1^2. Then if
Md + log Md + 1 < (1 - eq ) log Mr,
then (13.17) implies that E(Z$) 
0. This verifies the upper bound on A claimed in
the lemma.
Now let k 1 = log', where Md is the solution to 
1 
g>d 
d
Odd + log Odd + 1 = (1 + eq) log Mr .
Next let M denote the set of vertices that are at distance greater than r from any edge 
of D. Let M$ be the set of vertices of degree k in M .If Z$ = | M$ |, then
(
n — 1 \
, 
(nr2)$1 (1 - -r2)n~1~$1.
k1
P(X1 e M) > 1 - 4r and so

13.2 Random Geometric Graphs
193
E(Zkl) > (1 - 4r((' 1}e ) 1 (nr2)$1 e~n”r2/(1“”r2) 
3$ 1/2 \ 
$1 
)
n1 1/"'''
(1 - o(1))
eoJd 
r
log '
So,
log(E(Z$1)) =Q log 
Mr
(13.18)
An application of the Chebyshev inequality finishes the proof of the lemma. Indeed,
P(X1, X2 e M$) < P(X1 e M) P(X2 e M)
X P( X2 6 B (X1,r))+ ' ~ 1 (nr2) $1 (1 - nr2) ”“2$1“2
$1
< (1 + o(1)) P(X1 6 M$) P(X2 6 M$).
□
Now cl(Gx,r) < A(Gx,r) + 1 and so we now lower bound cl(Gx,r) w.h.p. But this 
is easy. It follows from Lemma 13.14 that w.h.p. there is a vertex X# with at least 
(1 - o(1)) log0g2r) vertices in its r/2 ball B(X#, r/2). But such a ball provides a clique 
of size (1 - o (1)) lo^ ) .We have therefore proved the following.
Theorem 13.15
w.h.p.
Suppose that nnr2 = log', where Mr 
Mr = O(log n). Then
X(GX.r )~ A(GX.r )~ cl(GX.r )~ log n 
log Mr ’
We now consider larger r .
Theorem 13.16 Suppose that nnr2 = Mr log n, where Mr ^^.Mr = o(n/log n). 
Then w.h.p.
X(GX,r ) ~
m^3 log n 
2n
Proof First consider the triangular lattice in the plane. This is the set of points 
T = {m1 a + m2b : m1, m2 e Z}, where a = (0,1), b = (1/2, V3/2) (see Figure 13.1). 
As in the diagram, each / e T can be placed at the center of a hexagon Cv. The Cv 
intersect on a set of measure zero and each C/ has are^V3/2 and is contained in 
B(/, 1/V3). Let r(T, d) be the graph with vertex set T, where two vertices x,y e T 
are joined by an edge if their Euclidean distance |x - y | < d.
Lemma 13.17 (McDiarmid and Reed [88])
x(r(T,d)) < (d +1)2.

194
Intersection Graphs
Figure 13.1 The small hexagon is an example of a C/
Proof Let 6 = \d]. Let R denote a 6 x 6 rhombus made up of triangles of T with one 
vertex at the origin. This rhombus has 62 vertices if we exclude those at the top and 
right-hand end. We give each of these vertices a distinct color and then tile the plane 
with copies of R. This is a proper coloring, by construction. 
□
Armed with this lemma we can easily get an upper bound on x(.Gx,+). Let 6 = 
1/wp3 and let s = 6+. Let sT be the contraction of the lattice T by a factor s, i.e., 
sT = {,x : x e T}. Then if / e sT, let sC/ be the hexagon with center /, sides parallel 
to the sides of C/ but reduced by a factor s. n sC/1 is distributed as Bin(n, s2V3/2). 
So the Chernoff bounds imply that with probability 1 - o(n-1),
sC/ contains < 6 = (1 + w“1/8)ns2V3/2 members of X. 
(13.19)
Let p = + + 2s/V3. We note that if x e C/ and y e C0 and |x - y | < +, then |v - w | < p. 
Thus, given a proper coloring ^ of r(sT,p) with colors [*] we can w.h.p. extend it 
to a coloring f of G x,r with color’s [*] x [6] .If x e sC/ and ^ (x) = a, then we let 
f (x) = (a, b), where b ranges over [6] as x ranges over sC/ IT X. So, w.h.p.
p 2
X(Gx.r)< 6x(r(sT,pf) = 6x(r(T,p/s)) < ^p + 1
s
ns2^3 r 2 
wrV3 log n
2 
s2 
2K
(13.20)
For the lower bound, we use a classic result on packing disks in the plane.
Lemma 13.18 Let An = [0, n]2 and C be a collection of disjoint disks of unit area 
that touch An .Then |C| < (1 + o (1)) xn2/VT2.
Proof Thue’s theorem states that the densest packing of disjoint same size disks in 

13.2 Random Geometric Graphs
195
the plane is the hexagonal packing which has density A = n^/12. Let C’ denote the 
disks that are contained entirely in An. Then we have
, 
z 
, 
nn2
|C'| > |C| - O(n) and |C'| < —.
12
The first inequality comes from the fact that if C e C \C', then it is contained in a 
perimeter of width O (1) surrounding An. 
□
Now consider the subgraph H of Gx,r induced by the points of X that belong to the 
square with center (1/2,1/2) and side 1 - 2r.It follows from Lemma 13.18 that if 
a(H) is the size of the largest independent set in H, then a(H) < (1 + o (1))2/ (r2V3). 
This is because if 5 is an independent set of H, then the disks B (x, r /2) for x e 5 are 
necessarily disjoint. Now using the Chernoff bounds, we see that w.h.p. H contains at 
least (1 - o(1))n vertices. Thus,
|V (H )| 
r 2V3n 
Mr V3 log
X(Gx,r) > X(H) > 
,, - (1 “ o' ' x 
= t1 - o t1))------3-----
a (H) 
2 
2x
This completes the proof of Theorem 13.16.
□
Exercises
13.2.1 Verify equation (13.13).
13.2.2 Verify equation (13.18).
Problems for Chapter 13
13.1 Prove that the bound (13.2) holds.
13.2 Prove that the bound (13.3) holds.
13.3 Prove that the bound (13.4) holds.
13.4 Prove the claims in Lemma 13.2.
13.5 Let X denote the number of isolated vertices in the binomial random intersection 
graph G(n, m,p), where m = na, a > 0. Show that if
I (logn + ^(n))/m 
when a < 1,
P = K. z xx,z---- x 
.
[V(log n + ^ (n))/(nm) when a> 1,
then EX e~c if limn^TC ^(n) 
c for any real c.
13.6 Find the variance of the random variable X counting isolated vertices in 
G (n, m, p).
13.7 Let Y be a random variable that counts vertices of degree greater than one in 
G(n, m, p), with m = na and a > 1. Show that for p2m2n » log n
lim P (y > 2p2m2^ = 0.
n^^
13.8 Suppose that r > (1 + s)r0, as in Theorem 13.8. Show that if 1 < k = O(1),
then Gx,r is k-connected w.h.p.

196
Intersection Graphs
13.9
$
Show that if 2 < k = O (1) and + » n 2<$-1>, then w.h.p. Gx r contains a 
$
k-clique. On the other hand, show that if + = o(n 2<fc-n ), then Gx,r contains 
no k -clique.
13.10 Suppose that + » lo|-'. Show that w.h.p. the diameter of Gx,r = © (1).
13.11 Suppose that + > (1 + e)r0, as in Theorem 13.8. Show that if 2 < k = O(1),
then Gx.r has k edge disjoint Hamilton cycles w.h.p.
13.12 Given X and an integer k we define the k-nearest neighbor graph G$-NN,x as 
follows: We add an edge between i and y of X if and only if y is one of x’s k- 
nearest neighbors, in Euclidean distance or vice versa. Show that if k > C log n 
for a sufficiently large C, then G$-NN,x is connected w.h.p.
13.13 Suppose that we independently deposit n random black points Xb and n random 
white points Xw into D. Let Bxb,x0,r be the bipartite graph where we connect 
i e Xb with Xw if and only if |x - y | < +. Show that if + » -^lo|-', then w.h.p. 
Bxb,x0,+ contains a perfect matching.

14 Weighted Graphs
There are many cases in which we put weights Xe ,e e E on the edges of a graph 
or digraph and ask for the minimum or maximum weight object. The optimization 
questions that arise from this are the backbone of combinatorial optimization. When 
the X are random variables, we can ask for properties of the optimum value, which will 
be a random variable. In this chapter, we consider three of the most basic optimization 
problems:
Minimum weight spanning trees. Suppose one wishes to connect a set of ' points in 
the plane together by lines as cheaply as possible so that the graph formed is connected. 
The solution is to construct a minimum length spanning tree. Analysis of the famous 
greedy algorithm leads to an elegant estimate of the expected minimum length when 
edge lengths are independent uniform [0,1].
Shortest paths. This is the classic problem of finding the best route through a network. 
This is the problem faced by any driver trying to get from A to B as quickly as 
possible. Analysis of Dijkstra’s famous shortest path algorithm when edge lengths 
are independent exponential mean one together with the memoryless property of this 
distribution leads to very precise estimates of expected shortest path lengths.
Minimum weight matchings in bipartite graphs. This is the problem faced by 
someone who has ' tasks to fulfill and ' persons who can fill each task with a different 
level of competence. Or any circumstance where one has to assign ' objects to ' 
positions where there is a cost C(i,#) of assigning object" to position #. When edge 
costs are independent exponential mean one, we arrive at one of the most striking 
answers to estimating expectation one can conceive.

198
Weighted Graphs
14.1 Minimum Weight Spanning Tree
Let the edges of the complete graph Kn on [«] be given independent lengths Xe, e e 
(1. Here Xe is a uniform [0,1] random variable. Let Ln be the length of the minimum 
spanning tree (MST) of Kn with these edge lengths.
Frieze [50] proved the following theorem. The proof we give utilizes the rather 
lovely integral formula (14.1) due to Janson [62] (see also the related equation (7) 
from [53]).
Theorem 14.1
TO 1
lim ELn = <(3) = Y-3 = 1.202...
n^TO 
$3 $3
k=1
Proof Suppose that T = T({Xe}) is the MST, unique with probability one. We use 
the identity
u — I I [v<aydx.
Jo
Therefore,
Ln = y Xe
e eT
= y [11{ P <xe i dP
e^Tjp=0
= 
1{ P <Xe } dP
Jp=e e^T
= /* 
|{e e T : Xe > p}|dp
Pp=0
= [ (X(GP) - 1)dp, 
p=o
where x(GP) denotes the number of components of graph GP. Here GP is the graph 
induced by the edges e with Xe < p, i.e., GP = Gn>P. The last line may be considered 
to be a consequence of the fact that the greedy algorithm solves the MST problem. This 
algorithm examines edges in increasing order of edge weight. It builds a tree, adding 
one edge at a time. It adds the edge to the forest F of edges accepted so far, only if the 
two endpoints lie in distinct components of F. Otherwise, it moves onto the next edge. 
Thus, the number of edges to be added given F is x(F) - 1 and if the longest edge in 
e e F has Xe = p, then x(F) = x(GP), which follows by an easy induction. Hence
E Ln = f (E x (G P)- 1) dp. 
(14.1)
P p=e

14.1 Minimum Weight Spanning Tree
199
We therefore estimate E k (Gp). We observe first that
p > 61ogn 
Ek(g^) = 1 + o(1^.
Hence, if p0 = 6^g', then
p P0
E L n = 
(E k (Gp )- 1) dp + o (1)
P p=0
p Po
= 
E k (G p ) dp + o (1).
P p=0
(14.2)
Write
(log n)2 
(log n)2
K(Gp ) =£ A$ + £ B$ + C, 
$=1 
$=1
where A$ stands for the number of components which are k vertex trees, B$ is the 
number of k vertex components which are not trees, and, finally, C denotes the number 
of components on at least (log w)2 vertices. Then, for 1 < k < (log w)2 and p < p0,
EA$ = ' k$~2p$~1 (1 - p)$(n-$)+®-$+1 
k
, k$'
= (1 + o(1))w$ —p$-1 (1 - p)$n, 
k!
EB$ < (')k$~2 (kjp$(1 - p)$(”"$)
< (1 + o(1))(wpe1-”P)$
< 1 + o (1),
C < ---------T.
(log w)2
Hence
6log' 
(log n)2
J ^ EB$ dp < 
°gn (log w)2(1 + o(1)) = o(1),
and
6 log n
' — Cdp < 6 
' 
= o(1).
Jp=o 
w 
(log w)2
So
(log n)2 
k$-2 6 61"
ELn = o(1) + (1 + o(1)) V w$ — 
" p$-1 (1 - p)$ndp.
$z! 
k! Pp=o

200
Weighted Graphs
But
Pk~1 (1 - p)$'dp
= o(1).
Therefore,
E Ln = o (1) + (1 + o(1))
(log ' 2 
,$_2 
11
S '$ 
P$_1 (1 - P}$'dp
£1 
$! 4=o
°° 1
= o(1) + (1 + o(1))Y -3.
ti $3
(14.3)
□
One can obtain the same result if the uniform [0,1] random variable is replaced by 
any non-negative random variable with distribution F having a derivative equal to one 
at the origin, e.g., an exponential variable with mean one (see Steele [109]).
Exercises
14.1.1 Verify equation (14.2).
14.1.2 Verify equation (14.3).
14.2 Shortest Paths
Let the edges of the complete graph Kn on [«] be given independent lengths Xe, e e 
(]). Here Xe is exponentially distributed with mean one. The following theorem was 
proved by Janson [63]:

14.2 Shortest Paths
201
Theorem 14.2 Let X"# be the distance from vertex i to vertex j in the complete graph 
with edge weights independent EXP(1) random variables. Then, for every e > 0, as 
n c» ,
(i) for any fixed i, j,
P
Xi# 
log n/.
(ii) for any fixed
P
max j X"# 
log n/n
(iii)
P
max",; Xt# 
log n/n
0;
0;
0.
Proof First, recall the following two properties of the exponential X:
(P1) P(X > a + 0|X > a) = P(X > 0).
(P2) If X1, X2,,Xm are independent EXP (1) exponential random variables, then 
min{ X1,X2,..Xm} is an exponential with mean 1/m.
Suppose that we want to find shortest paths from a vertex , to all other vertices in 
a digraph with non-negative arc lengths. Recall Dijkstra’s algorithm. After several 
iterations, there is a rooted tree T such that if v is a vertex of T, then the tree path from 
, to v is a shortest path. Let d(v) be its length. For x £ T, let d(x) be the minimum 
length of a path P that goes from , to v to x where v e T and the sub-path of P that 
goes to v is the tree path from , to v. If d(y) = min {d(x) : x £ T}, then d(y) is the 
length of a shortest path from , to y and y can be added to the tree.
Suppose that vertices are added to the tree in the order vi,v2,...,v„ and that 
Y# = dist(v1, v#) for j = 1,2,..., n. It follows from property P1 that
Y$+1 = 
min [Yf + XVi >v ] = Y$ + E $,
"=1,2,...,$ 
v^/1 ,...,/$
where E$ is exponential with mean $ ('_$# and is independent of Y$.
This is because XVi./ is distributed as an independent exponential X conditioned 
on X > Y$ - Y". Hence
ef - y 1 
-1 y1I1 + 1 'i - 2 y 1
$ 
$-$ k (n - k) 
^^!\k + n - kl n k
$=1 
$=1 x 
' 
$=1
= 2 n + 0 (»-■). 
n
Also, from the independence of E$, Y$,
Var Yn = g Var E$ = ^T 
) 2 < 2 ^^^ 
) 2
$Pt 
k ^n - k 
$Pt' k ^n - k

202
Weighted Graphs
n/2
£ ' E $2 - O<n-2) 
n k=1 $
and we can use the Chebyshev inequality (2.18) to prove (ii).
Now fix j = 2. Then, if i is defined by V" = 2, we see that i is uniform over
{2,3,...,n}.So
1
E *1,2 =
n - 1
i-1
_ 
1 '
"n -1 $=1
For the variance of X1;2 we have
i=2 $=1 
n-1 .
1 
1 
‘r-! 
n - $
k(n - k) 
n - 1 k(n - k)
X1lr° <»->
*1,2 = S2Y2 + S3Y3 + ■■■ + 6nYn,
where
6" 6{0,1}; 62 + S3 + ■■■ + S’ = 1; P(Si = 1) = —
n - 1
Var X1>2 = £ Var(S"Yi) + £ Cov(S"Yi, S #Y#) 
i=2 i*j
n
< £ Var(SiYi). 
i=2
The last inequality holds since
Cov(SiYi, SjY#) = E(SiYiSjY#) - E(SiYi) E(SjY#)
= - E(SiYi) E(SjY#)< 0.
So 
n
Var X1>2 < ^ Var(SiYi) 
i=2
' 
1 
i~1 / 
1 
\2
“ n -1 
$ <n - $)/
= O (n-2).
We can now use the Chebyshev inequality.
We turn now to proving (iii). We begin with a lower bound. Let
Y" = min {Xi;J- : i + j e [n]}. Let A = |i: Y" > 
s‘log'}. Then we have that for
i e [n],
P(i e A) = exp |-(n - 1)(1 ~ g) log n I = n"1+s+o(1). 
(14.4)
n
An application of the Chebyshev inequality shows that | A| ~ ns+o® w.h.p. Now the 

14.2 Shortest Paths
203
expected number of paths from a1 e A to a2 e A of length at most ^3 2£'log ' can be 
bounded by
2£+o (1) x n2 x n 3£+o (1) x lOg2' = n~£+o (1)
2
(14.5)
Explanation for (14.5): The first factor n2£+o 4) is the expected number of pairs of 
vertices a1 ,a2 e A. The second factor is a bound on the number of choices b1,b2 for 
the neighbors of a1, a2 on the path. The third factor F3 is a bound on the expected 
number of paths of length at most al'g ' from b1 to b2, a = 1 - 3e. This factor comes 
from
i 
\ 1+1
,i a log ‘ \ 
n 
(I + 1)
Here I is the number of internal vertices on the path. There will be at most nl choices 
for the sequence of vertices on the path. We then use the fact that the exponential mean 
one random variable stochastically dominates the uniform [0,1] random variable U. 
The final two factors are the probability that the sum of I + 1 independent copies of U 
sum to at most a ^g '. Continuing, we have
y a log ' / e1+o (1) a log ' \1 = u a+o (1)
- i4o ‘ <im 1
(14.6)
The final factor in (14.5) is a bound on the probability that Xa1b1 + Xa2b2 < ^2+£'log '. 
For this, we use the fact that Xaibi,i = 1,2 is distributed as £'log' + E", where 
E1, E2 are independent exponential mean one. Now P(E1 + E2 < t) < (1 - e~')2 < -2 
and taking t = 3£ 'g ' justifies the final factor of (14.5).
It follows from (14.5) that w.h.p. the shortest distance between a pair of vertices in 
A is at least ^3~2£'log ' w.h.p., completing our proof of the lower bound in (iii).
We now consider the upper bound. Let now Y1 = d$3, where k3 = n1/2 log n. For 
- < 1 _ 1+0 W we have that
E<e"'> = E (exP {£ 
. 
)) = fi (1 
)"■
i=1 "iJ 
i=1 
"
Then for any a > 0 we have
Pr Y1 > a-l°gn < E(etnY1~ta log') = O exp 1 + o (1) - at log n . (14.7) 
n2
It follows, on taking a = 3/2 + o(1) that w.h.p.
(3 + o (1)) log n
Y, < --------2n---------- for all j e [n].
Letting T# be the set corresponding to S$3 when we execute Dijkstra’s algorithm starting 
at j, then we have that for j + k, where T# n T$ = 0,

204
Weighted Graphs
P 6 E(T#,T$) : Xe < logg j < exp|-$3^gg j
- g-(2+((1)) log2' = ((g~2), 
and this is enough to complete the proof of (iii).
□
We can, as for spanning trees, replace the exponential random variables by random 
variables that behave like the exponential close to the origin. The paper of Janson [63] 
allows for any random variable X satisfying P(X < -) = - + ((-) as - 
0.
Exercises
14.2.1 Verify equation (14.6).
14.2.2 Verify equation (14.7).

14.3 Minimum Weight Assignment
205
14.3 Minimum Weight Assignment
Consider the complete bipartite graph Kn,n and suppose that its edges are assigned 
independent exponentially distributed weights, with rate 1. (The rate of an exponential 
variable is one over its mean.) Denote the minimum total weight of a perfect matching 
in K„,„ by Cn. Aldous [5,6] proved that limn^TC E Cn = £ (2) = 2“=1 $2 .The following 
theorem was conjectured by Parisi [98]. It was proved independently by Linusson and 
Wastlund [78] and Nair, Prabhakar, and Sharma [94]. The proof given here is from 
Wastlund [112].
Theorem 14.3
E Cn = ^72 = 1 + T + O+ +■■■ + — • 
(14.8)
f—$ k2 
4 9 
16 
n2
k=1
From the above-mentioned theorem we immediately get the following corollary, first 
proved by Aldous [6].
Corollary 14.4
lim E Cn
n^<x>
1 
V2
=< <2’-z k? -- 6=1614' 
k=1
Let EXP (!) denote an exponential random variable of rate T, i.e., P (EXP (!) > x) = 
e-Ax. Consider the complete bipartite graph Kn>n, with bipartition (A, B), where A = 
{a1, a2,..., an} and B = {b1, b2,..b n}, and with edge weights that are independent 
copies of EXP(1). We add a special vertex b* to B, with edges to all n vertices of A. 
Each edge adjacent to b* is assigned an EXP(T) weight independently, T > 0.
For r > 1, we let Mr be the minimum weight matching of Ar = {a1, a2,...,ar} 
into B and M* be the minimum weight matching of Ar into B* = B U {b*}. (As T 0, 
it becomes increasingly unlikely that any of the extra edges is actually used in the 
minimum weight matching.) We denote this matching by M* and we let B* denote the 
corresponding set of vertices of B* that are covered by M*. We let C (n, r) denote the 
weight of Mr .
Define P(n, r) as the normalized probability that b* participates in M*, i.e.,
P (n, r) = lim
,1^0
P(b* e B*}
(14.9)
Its importance lies in the following lemma:
Lemma 14.5
Pn. -)
E(C(n, r) - C(n, r - 1)) = -4-4-4 
r
(14.10)

206
Weighted Graphs
Proof Choose i randomly from [r] and let Bi Q Br be the B-vertices in the minimum 
weight matching of (Ar \ {a"}) into B*. Let X = C(n, r) and Y = C(n, r - 1). Let 
Wi be the weight of the edge (a", b*) and I" denote the indicator variable for the event 
that the minimum weight of an Ar matching that contains this edge is smaller than 
the minimum weight of an Ar matching that does not use b*. We can see that I" is 
the indicator variable for the event {Y" + W" < X}, where Y" is the minimum weight 
of a matching from Ar \ {a"} to B. Indeed, if (a", b*) e M*, then W" < X - Y". 
Conversely, if W" < X - Y" and no other edge from b* has weight smaller than X - Y", 
then (a", b*) e M*, and when A 0, the probability that there are two distinct edges 
from b* of weight smaller than X - Y" is of order O(A2). Indeed, let T denote the 
existence of two distinct edges from b* of weight smaller than X and let ffj denote 
the event that (a", b*) and (a#, b*) both have weight smaller than X.
Then,
P(D < n2 Ex(maxP(T-y | X)) = n2 E((1 - e-AX)2) < n2A2 E(X2), 
(14.11)
"J
and since E(X2) is finite and independent of A, this is O(A2).
Note that Y and Y" have the same distribution. They are both equal to the minimum 
weight of a matching of a random (r - 1) set of A into B. As a consequence, E(Y) = 
E (Y") = 1 X j e Ar E(Y#). Since W" is EXP (A) distributed, as A 0, we have from 
(14.11) that
P (n, r) = lim
,1^0
A 2 P(wJ <X - Y,■) + O (A) 
\ J 6 Ar
lim E 
a^o
I y (i_ e"a( x ~y# ) 
/ J e.A 
,
= £ E(X - Y") = r E(X - Y).
J € Ar
□
We now proceed to estimate P(n, r). Fix r and assume that b* £ B*_ 1. Suppose that M* 
is obtained from M*_ 1 by finding an augmenting path P = (ar,..., a^, bT) from ar 
to B \ Br_1 of minimum additional weight. We condition on (i) <r, (ii) the lengths of all 
edges other than (a^, b2), b2 e B \ Br_1, and (iii) min {w(a^, b2: b2 e B \ Br_1}. 
With this conditioning, Mr_1 = M*_ 1 will be fixed and so will P’ = (ar,..., a^). 
We can now use the following fact: Let X1, X2,..., XM be independent exponential 
random variables of rates A1, A2,..., AM. Then the probability that X" is the smallest 
of them is A" /(A1 + A2 + ■ ■ ■ + AM). Furthermore, the probability stays the same if we 
condition on the value of min {X1, X2,..., XM }. Thus,
P(b*e b; ।b*, B;_1) = --^.
Lemma 14.6
11 
1
P (n,r) = - + —1 + ■■■+_,_ 
. 
(14.12)

14.3 Minimum Weight Assignment
207
Proof
(
n n — 1
1------- — ■----- -——
n - + + 1
n - + + 1 + 2
11
= - +
1
n - 1
(14.13)
□
It follows from Lemmas 14.5 and 14.6 that
B C. = 11£ J., ■ 
+=1 
"=1
which implies that
E(C„+1 - C„) = ----- .
(n + 1)2
E(C1) = 1 and so (14.8) follows from (14.14).
(14.14)
□
Exercises
14.3.1 Verify equation (14.13).
14.3.2 Verify equation (14.14).

208
Weighted Graphs
Problems for Chapter 14
14.1 Suppose that the edges of the complete bipartite graph Kn,n are given indepen­
dent uniform [0,1] edge weights. Show that if Lis the length of the MST, 
then
lim EL’b) = 2<(3). 
’
14.2 Let G = Ka‘^p’ be the complete unbalanced bipartite graph with bipartition 
sizes an, fin. Suppose that the edges of G are given independent uniform [0,1] 
edge weights. Show that if L’b) is the length of the MST, then
lim E L’b
‘^<X>
1 
^ 
("t + "2 - 1)! 7 "h "2
7 H----- 1 
/ 
----- . .. .------
7 h >tf2 >1 
"1!"2! 
("1 + 7"2^ 
2
where 7 = a/j8.
14.3 Tighten Theorem 14.1 and prove that
E L ‘ = < (3) + O 1 .
'
14.4 Suppose that the edges of K’ are given independent uniform [0,1] edge weights. 
Let $ denote the minimum total edge cost of the union of $ edge-disjoint 
spanning trees. Show that lim$^m Z$/$2 = 1.
14.5 Show that if the edges of the complete bipartite graph K„,„ are given i.i.d. costs, 
then the minimum cost perfect matching is uniformly random among all '! 
perfect matchings.
14.6 Show that if w 
to, then w.h.p. no edge in the minimum cost perfect matching
has cost more than ^ lOg ‘.
14.7 Show that a random permutation n gives rise to a digraph
Dn = (['], {("', n(")) : " e [']}) that w.h.p. consists of O(log ‘) vertex disjoint 
cycles that cover [‘].
14.8 Consider the Asymmetric Traveling Salesperson Problem (ATSP) where the 
costs C(i,#) are independent uniform [0,1]. Use the claimed results of the 
previous two problems to show that Karp’s patching algorithm finds a tour that 
is within (1 + ((1)) of minimal cost w.h.p.
The ATSP asks for the minimum total cost ofa directed Hamilton cycle through 
the complete digraph K’.
Karp’s algorithm starts by solving the assignment problem with costs C(", j). 
It interprets the perfect matching as the union of disjoint cycles in K’ and then 
patches them together cheaply.
Given cycles C1,C2 and edges et = (it,yt) e Ct," = 1,2, a patch removes 
e1, e2 and replaces them with (i2, y1) plus (i1, y2) creating a single cycle.
14.9 Suppose that the edges of G„,), where 0 < ) < 1 is a constant, are given 
exponentially distributed weights with rate 1. Show that if "# is the shortest 
distance from " to # , then

14.3 Minimum Weight Assignment
209
1 for any fixed i,j,
X"j
log n/n
0;
max j Xi# 
log n/n
P
1
P
2
14.10 The quadratic assignment problem is to
Minimize
Z = £i,;,p,*=i a"jpqx"p 1 jq
Subject to
S-=1 1ip = 1 
P = 1,2,.
sp=11ip = 1 
i = 1,2, ...,n,
1ip = 0/1.
Suppose now that the a"jpq are independent uniform [0,1] random variables. 
Show that w.h.p. Zmin ~ Zmax, where Zmin (resp. Zmax) denotes the minimum 
(resp. maximum) value of Z, subject to the assignment constraints.
14.11 The 0/1 knapsack problem is to
Maximize
Z = Si=1 a" x"
Subject to
£7=1 b"X" < L,
X" = 0/1 for i = 1,2,..., n.
Suppose that the (a", b") are chosen independently and uniformly from [0,1]2 
and that L = an. Show that w.h.p. the maximum value of Z, Zmax, satisfies
Z 
^max
;,
2
(8 a-8V-1) n
2
n
2
14.12 Suppose that X1, X2,..., Xn are points chosen independently and uniformly at 
random from [0,1]2. Let Zn denote the total Euclidean length of the shortest 
tour (Hamilton cycle) through each point. Show that there exist constants c1, c2 
such that c1n1/2 < Zn < c2n1/2 w.h.p.

References
[1] M. Abdullah and N. Fountoulakis, A phase transition on the evolution of bootstrap perco­
lation processes on preferential attachment graphs, Random Structures and Algorithms, 
52 (2018)379-418.
[2] W. Aiello, W. Bonato, C. Cooper, J. Janssen and P. Pralat, A spatial web graph model 
with local influence regions, Internet Mathematics, 5 (2009) 175-196.
[3] M. Ajtai, J. Komlos and E. Szemeredi, The longest path in a random graph, Combinator- 
ica, 1 (1981) 1-12.
[4] M. Ajtai, J. Komlos and E. Szemeredi, The first occurrence of Hamilton cycles in random 
graphs, Annals of Discrete Mathematics, 27 (1985) 173-178.
[5] D. Aldous, Asymptotics in the random assignment problem, Probability Theory and 
Related Fields, 93 (1992) 507-534.
[6] D. Aldous, The £(2) limit in the random assignment problem, Random Structures and 
Algorithms, 4 (2001) 381-418.
[7] N. Alon, A note on network reliability. In Discrete Probability and Algorithms (Min­
neapolis, MN, 1993), The IMA Volumes in Mathematics and Its Applications, Vol. 72, 
Springer (1995) 11-14.
[8] N. Alon and Z. Furedi, Spanning subgraphs of random graphs, Graphs and Combina­
torics, 8 (1992) 91-94.
[9] L. Babai, P Erdos and S. M. Selkow, Random graph isomorphism, SIAM Journal on 
Computing, 9 (1980) 628-635.
[10] D. Bal, P. Bennett, A. Dudek and A. M. Frieze, The - -tone chromatic number of random 
graphs, Graphs and Combinatorics, 30 (2014) 1073-1086.
[11] F. Ball, D. Mollison and G. Scalia-Tomba, Epidemics with two levels of mixing, Annals 
of Applied Probability, 7 (1997) 46-89.
[12] J. Balogh, B. Bollobas, M. Krivelevich, T Mueller and M. Walters, Hamilton cycles in 
random geometric graphs, Annals of Applied Probability, 21 (2011) 1053-1072.
[13] L. Barabasi and R. Albert, Emergence of scaling in random networks, Science, 286 (1999) 
509-512.
[14] A. D. Barbour, M. Karonski and A. Rucinski, A central limit theorem for decomposable 
random variables with applications to random graphs, Journal of Combinatorial Theory 
B, 47 (1989) 125-145.
[15] A. D. Barbour and G. Reinert, Small worlds, Random Structures and Algorithms, 19 
(2001) 54-74.
[16] A. D. Barbour and G. Reinert, Discrete smallworld networks, Electronic Journal on 
Probability, 11 (2006) 1234-1283.
[17] S. N. Bernstein, Theory of Probability, (in Russian), Gostekhizdat (1927).

References
211
[18] B. Bollobas, A probabilistic proof of an asymptotic formula for the number of labelled 
graphs, European Journal on Combinatorics, 1 (1980) 311-316.
[19] B. Bollobas, Random graphs. In Combinatorics, Proceedings, Swansea, London Mathe­
matical Society Lecture Note Series, Vol. 52, Cambridge University Press (1981) 80-102.
[20] B. Bollobas, The evolution of random graphs, Transactions of the American Mathematical 
Society, 286 (1984) 257-274.
[21] B. Bollobas, Random Graphs, First edition, Academic Press (1985); Second edition, 
Cambridge University Press (2001).
[22] B. Bollobas, The chromatic number of random graphs, Combinatorica, 8 (1988) 49-56.
[23] B. Bollobas and F. Chung, The diameter of a cycle plus a random matching, SIAM Journal 
on Discrete Mathematics, 1 (1988) 328-333.
[24] B. Bollobas and P. Erdos, Cliques in random graphs, Mathematical Proceedings of the 
Cambridge Philosophical Society, 80 (1976) 419-427.
[25] B. Bollobas and A. Frieze, On matchings and Hamiltonian cycles in random graphs, 
Annals of Discrete Mathematics, 28 (1985) 23-46.
[26] B. Bollobas and A. Frieze, Spanning maximal planar subgraphs of random graphs, 
Random Structures and Algorithms, 2 (1991) 225-231.
[27] B. Bollobas and O. Riordan, The diameter of a scale free random graph, Combinatorica, 
24 (2004) 5-34.
[28] B. Bollobas, O. Riordan, J. Spencer and G. Tusnady, The degree sequence of a scale-free 
random graph process, Random Structures and Algorithms, 18 (2001) 279-290.
[29] B. Bollobas and A. Thomason, Threshold functions, Combinatorica, 7 (1987) 35-38.
[30] F. Chung and L. Lu, Connected components in random graphs with given expected degree 
sequence, Annals of Combinatorics, 6 (2002) 125-145.
[31] F. Chung and L. Lu, The volume of the giant component of a random graph with given 
expected degrees, SIAM Journal on Discrete Mathematics, 20 (2006) 395-411.
[32] F. Chung and L. Lu, Complex Graphs and Networks, American Mathematical Society 
(2006).
[33] R. Cont and E. Tanimura, Small-world graphs: characterizations and alternative construc­
tions, Advances in Applied Probability, 20 (2008) 939-965.
[34] C. Cooper, Pancyclic Hamilton cycles in random graphs, Discrete Mathematics, 91 (1991) 
141-148.
[35] C. Cooper, 1-pancyclic Hamilton cycles in random graphs, Random Structures and Al­
gorithms, 3 (1992) 277-287.
[36] C. Cooper and A. M. Frieze, Pancyclic random graphs. In Random Graphs, M. Karonski, 
J. Javorski, and A. Rucinski (eds). John Wiley and Sons (1990) 29-39.
[37] C. Cooper, A. Frieze and P Pralat. Some typical properties of the spatial preferred 
attachment model, Internet Mathematics, 10 (2014) 27-47.
[38] J. D^az, D. Mitsche and X. Perez-Gimenez, Sharp threshold for hamiltonicity of random 
geometric graphs, SIAM Journal on Discrete Mathematics, 21 (2007) 57-65.
[39] A. Dudek, D. Mitsche and P. Pralat, The set chromatic number of random graphs, Discrete 
Applied Mathematics, 215 (2016) 61-70.
[40] A. Dudek and P. Pralat, An alternative proof of the linearity of the size-Ramsey number 
of paths, Combinatorics, Probability and Computing, 24 (2015) 551-555.
[41] R. Durrett, Random Graph Dynamics, Cambridge University Press (2010).
[42] P. Erdos and A. Renyi, On random graphs I, Publicationes Mathematicae Debrecen, 6 
(1959) 290-297.

212
References
[43] P. Erdos and A. Renyi, On the evolution of random graphs, Publication of the Mathemat­
ical Institute of the Hungarian Academy of Sciences, 5 (1960) 17-61.
[44] P. Erdos and A. Renyi, On the strength of connectedness of a random graph, Acta 
Mathematica Academiae Scientiarum Hungaricae, 8 (1961) 261-267.
[45] P. Erdos and A. Renyi, On random matrices, Publication of the Mathematical Institute of 
the Hungarian Academy of Sciences, 8 (1964) 455-461.
[46] P. Erdos and A. Renyi, On the existence of a factor of degree one of a connected random 
graph, Acta mathematica Academiae Scientiarum Hungaricae, 17 (1966) 359-368.
[47] M. Faloutsos, P. Faloutsos and C. Faloutsos, On power-law relationships of the internet 
topology, ACM SIGCOMM (1999).
[48] J. A. Fill, E. R. Scheinerman and K. B. Singer-Cohen, Random intersection graphs when 
& = «(‘): An equivalence theorem relating the evolution the evolution of the G (‘, &,)) 
and G (',)) models, Random Structures and Algorithms, 16 (2000) 156-176.
[49] A. Flaxman, A. M. Frieze and T. Fenner, High degree vertices and eigenvalues in the 
preferential attachment graph, Internet Mathematics, 2 (2005) 1-20.
[50] A. M. Frieze, On the value of a random minimum spanning tree problem, Discrete Applied 
Mathematics, 10 (1985) 47-56.
[51] A. M. Frieze, On the independence number of random graphs, Discrete Mathematics, 81 
(1990) 171-176.
[52] A. M. Frieze and M. Karonski, Introduction to Random Graphs, Cambridge University 
Press (2016).
[53] A. M. Frieze and C. J. H. McDiarmid, On random minimum length spanning trees, 
Combinatorica, 9 (1989) 363-374.
[54] E. N. Gilbert, Random graphs, Annals of Mathematical Statistics, 30 (1959) 1141-1144.
[55] G. Grimmett and C. McDiarmid, On colouring random graphs, Mathematical Proceed­
ings of the Cambridge Philosophical Society, 77 (1975) 313-324.
[56] L. Gu and L. H. Huang, The clustering coefficient and the diameter of small-world 
networks, Acta Mathematica Sinica, English Series, 29 (2013) 199-208.
[57] P. Gupta and P. R. Kumar, Critical power for asymptotic connectivity in wireless networks. 
In Stochastic Analysis, Control, Optimization and Applications: A Volume in Honor of 
W. H. Fleming, W. M. McEneany, G. Yin and Q. Zhang (eds). Birkhauser (1998) 547-566.
[58] E. Gyori, B. Rothschild and A. Rucinski, Every graph is contained in a sparest possible 
balanced graph, Mathematical Proceedings of the Cambridge Philosophical Society, 98 
(1985) 397-401.
[59] W. Hoeffding, Probability inequalities for sums of bounded random variables, Journal of 
the American Statistical Association, 58 (1963) 13-30.
[60] R. van der Hofstad, Random Graphs and Complex Networks, Vol. 1, Cambridge University 
Press, 2017.
[61] J. Janssen, P Pralat and R. Wilson, Geometric graph properties of the spatial preferred 
attachment model, Advances in Applied Mathematics, 50 (2013) 243-267.
[62] S. Janson, The minimal spanning tree in a complete graph and a functional limit theorem 
for trees in a random graph, Random Structures and Algorithms, 7 (1995) 337-355.
[63] S. Janson, One, two and three times log‘/‘ for paths in a complete graph with random 
weights, Combinatorics, Probability and Computing, 8 (1999) 347-361.
[64] S. Janson, Asymptotic degree distribution in random recursive trees, Random Structures 
and Algorithms, 26 (2005) 69-83.

References
213
[65] S. Janson, Plane recursive trees, Stirling permutations and an urn model. In Fifth Collo­
quium on Mathematics and Computer Science, Discrete Math. Theor. Comput. Sci. Proc., 
AI, Discrete Mathematics and Theoretical Computer Science, (2008) 541-547.
[66] S. Janson, T Luczak and A. Rucinski, Random Graphs, John Wiley and Sons (2000).
[67] D. Karger and C. Stein, A new approach to the minimum cut problem, Journal of the 
ACM, 43 (1996) 601-640.
[68] M. Karonski, E. Scheinerman and K. Singer-Cohen, On random intersection graphs: the 
subgraph problem, Combinatorics, Probability and Computing, 8 (1999) 131-159.
[69] M. Karonski and A. Rucinski, On the number of strictly balanced subgraphs of a random 
graph, In Graph Theory, Proc. Lagow, 1981, Lecture Notes in Mathematics 1018, Springer 
(1983) 79-83.
[70] M. Karonski and A. Rucinski, Problem 4, In Graphs and other Combinatorial Topics, 
Proceedings, Third Czech. Symposium on Graph Theory, Prague (1983).
[71] M. Karonski and A. Rucinski, Poisson convergence and semi-induced properties of 
random graphs, Mathematical Proceedings of the Cambridge Philosophical Society, 101 
(1987) 291-300.
[72] J. Kleinberg, The small-world phenomenon: An algorithmic perspective, Proceedings of 
the 32nd ACM Symposium on Theory of Computing (2000) 163-170.
[73] J. Komlos and E. Szemeredi, Limit distributions for the existence of Hamilton circuits in 
a random graph, Discrete Mathematics, 43 (1983) 55-63.
[74] W. Kordecki, Normal approximation and isolated vertices in random graphs, In Random 
Graphs ’87, M. Karonski, J. Jaworski and A. Rucinski (eds). John Wiley and Sons (1990) 
131-139.
[75] I. N. Kovalenko, Theory of random graphs, Cybernetics and Systems Analysis, 7 (1971) 
575-579.
[76] M. Krivelevich, C. Lee and B. Sudakov, Long paths and cycles in random subgraphs 
of graphs with large minimum degree, Random Structures and Algorithms, 46 (2015) 
320-345.
[77] M. Krivelevich and B. Sudakov, The phase transition in random graphs - a simple proof, 
Random Structures and Algorithms, 43 (2013) 131-138.
[78] S. Linusson and J. Wastlund, A proof of Parisi’s conjecture on the random assignment 
problem, Probability Theory and Related Fields, 128 (2004) 419-440.
[79] T. Luczak, On the equivalence of two basic models of random graphs, In Proceedings of 
Random Graphs ’87, M. Karonski, J. Jaworski, and A. Rucinski (eds). John Wiley and 
Sons (1990) 151-158.
[80] T. Luczak, Component behaviour near the critical point, Random Structures and Algo­
rithms, 1 (1990) 287-310.
[81] T. Luczak, On the number of sparse connected graphs, Random Structures and Algorithms, 
1 (1990) 171-174.
[82] T. Luczak, Cycles in a random graph near the critical point, Random Structures and 
Algorithms, 2 (1991) 421-440.
[83] T. Luczak, The chromatic number of random graphs, Combinatorica, 11 (1991) 45-54.
[84] E. Marczewski, Sur deux proprietes des classes d’ensembles, Fundamenta Mathematicae, 
33 (1945) 303-307.
[85] D. Matula, The largest clique size in a random graph, Technical Report, Department of 
Computer Science, Southern Methodist University, (1976).

214
References
[86] D. Matula, Expose-and-merge exploration and the chromatic number of a random graph, 
Combinatorica, 7 (1987) 275-284.
[87] C. McDiarmid, Random channel assignment in the plane, Random Structures and Algo­
rithms, 22 (2003) 187-212.
[88] C. McDiarmid and B. Reed, Colouring proximity graphs in the plane, Discrete Mathe­
matics, 199 (1999) 123-137.
[89] S. Milgram, The Small World Problem, Psychology Today, 2 (1967) 60-67.
[90] M. Molloy and B. Reed, The Size of the Largest Component of a random graph on a fixed 
degree sequence, Combinatorics, Probability and Computing, 7 (1998) 295-306.
[91] T Muller, X. Perez and N. Wormald, Disjoint Hamilton cycles in the random geometric 
graph, Journal of Graph Theory, 68 (2011) 299-322.
[92] T. Muller, Private communication.
[93] A. Nachmias and Y. Peres, The critical random graph, with martingales, Israel Journal 
of Mathematics, 176 (2010) 29-41.
[94] C. Nair, B. Prabhakar and M. Sharma, Proofs of the Parisi and Coppersmith-Sorkin 
random assignment conjectures, Random Structures and Algorithms, 27 (2005) 413-444.
[95] M. Newman, Networks, Second Edition, Oxford University Press, 2018.
[96] M. E. J. Newman and D. J. Watts, Scaling and percolation in the small-world network 
model, Physical Review E, 60 (1999) 7332-7342.
[97] E. Palmer and J. Spencer, Hitting time for $ edge disjoint spanning trees in a random 
graph, Periodica Mathematica Hungarica, 91 (1995) 151-156.
[98] G. Parisi, A conjecture on Random Bipartite Matching, Physics e-Print archive (1998).
[99] M. Penrose, On $ -connectivity for a geometric random graph, Random Structures and 
Algorithms, 15 (1999) 145-164.
[100] M. Penrose, Random Geometric Graphs, Oxford University Press, 2003.
[101] L. Posa, Hamiltonian circuits in random graphs, Discrete Mathematics, 14 (1976) 359­
364.
[102] A. Rucinski, When a small subgraphs of a random graph are normally distributed, Prob­
ability Theory and Related Fields, 78 (1988) 1-10.
[103] A. Rucinski and A. Vince, Strongly balanced graphs and random graphs, Journal of 
Graph Theory, 10 (1986) 251-264.
[104] K. Rybarczyk, Equivalence of the random intersection graph and G (',)), Random Struc­
tures and Algorithms, 38 (2011) 205-234.
[105] K. Rybarczyk and D. Stark, Poisson approximation of the number of cliques in a random 
intersection graph, Journal of Applied Probability, 47 (2010) 826-840.
[106] B. Sudakov and V. Vu, Local resilience of graphs, Random Structures and Algorithms, 
33 (2008) 409-433.
[107] A. Scott, On the concentration of the chromatic number of random graphs (2008), arxiv 
0806.0178
[108] J. Spencer with L. Florescu, Asymptotia, American Mathemetical Society, 2010.
[109] J. M. Steele, On Frieze’s £ (3) limit for lengths of minimal spanning trees, Discrete 
Applied Mathematics, 18 (1987) 99-103.
[110] P. Turan, On an extremal problem in graph theory (in Hungarian), Matematikai es Fizikai 
Lapok, 48 (1941) 436-452.
[111] W. F. de la Vega, Long paths in random graphs, Studia Scientiarum Mathematicarum 
Hungarica, 14 (1979) 335-340.

References 215
[112] J. Wastlund, An easy proof of the £ (2) limit in the random assignment problem, Electronic 
Communications in Probability, 14 (2009) 261-269.
[113] D. J. Watts and S. H. Strogatz, Collective dynamics of “small-world” networks, Nature, 
393 (1998) 440-442.
[114] D. West, Introduction to Graph Theory, Second edition, Prentice Hall 2001.
[115] N. Wormald, Models of random regular graphs, In Surveys in Combinatorics 1999, J. 
Lamb and D. Preece (eds). London Mathematical Society Lecture Note Series (1999) 
239-298, Cambridge University Press.

Author Index
Abdullah, M., 171
Aiello, W., 171
Ajtai, M., 93, 100, 102
Albert, R., 6, 163
Aldous, D., 205
Alon, N., 106, 132
Babai, L., 75
Ball, F., 155
Balogh, J., 191
Barabasi, L., 6, 163
Barbour, A.D., 155
Bernstein, S.N., 18
Bollobas, B., 3, 7, 41, 58, 68, 87, 89, 92, 93,
96, 
102, 109, 120, 121, 127, 141, 150, 155,
163, 191
Bonato, W., 171
Chung, F., 6, 7, 127, 134, 139,
155, 172
Cont, R., 161
Cooper, C., 109, 171
de la Vega, W., 100
Diaz, J., 191
Dudek, A., 109, 124
Durrett, R., 7
Erdos, P., 3-7, 29, 45, 58, 75, 78, 82, 85, 87, 92-94,
96, 100, 111, 121
Faloutsos, C., 163
Faloutsos, M., 163
Faloutsos, P., 163
Fenner, T., 167
Fill, J., 180
Flaxman, A., 167
Florescu, L., 8
Fountoulakis, N., 171
Frieze, A.M., 96, 109, 116, 167, 198
Furedi, Z., 106
Gilbert, E.N., 3-6, 29, 30, 78
Grimmett, G., 121
Gu, L, 155
Gupta, P., 188
Gyori, E., 91
Hoeffding, W., 4, 19, 20, 25, 114, 133, 146,
167, 170
Huang, L.H., 155
Janson, S., 7, 66, 115, 198, 200, 204
Janssen, J, 171
Karger, D., 153
Karonski, M., 68, 89, 179, 183
Kleinberg, J., 5, 154, 160, 161
Komlos, J., 93, 100, 102
Kovalenko, I.N., 127, 128, 134
Krivelevich, M., 100, 102, 191
Kumar, P., 188
Lee, C., 100
Linusson, S., 205
Lu, L., 6, 7, 127, 134, 139, 172
Luczak, T., 7, 34, 58, 66
Marczewski, E., 178
Matula, D., 114, 116, 120
McDiarmid, C., 92, 121, 159, 192, 193
Milgram, S., 154, 160
Mitsche, D., 124, 191
Mollison, D., 155
Molloy, M., 5, 6, 127, 145
Muller, T., 188, 191
Nachmias, A., 58
Nair, C., 205
Newman, M., 7
Newman, M.E.J., 155, 158
Palmer, E., 109
Parisi, G., 205
Penrose, M., 187, 188
Peres, Y., 58
Perez-Gimenez, X., 191
Posa, L., 102, 103, 105
Prabhakar, B., 205
Pralat, P., 109, 124, 171
Reed, B., 5, 6, 127, 145, 193
Reinert, G., 155
Renyi, A., 3-7, 29, 45, 58, 78, 82, 85, 87, 92-94,
96, 100
Riordan, O., 163

Author Index
217
Rucinski, A., 7, 66, 68, 87, 89, 91
Rybarczyk, K., 180, 182, 186
Scalia-Tomba, G., 155
Scheinerman, E., 179, 180, 183
Scott, A., 123
Selkow, S., 75
Sharma, M., 205
Singer-Cohen, K., 179, 180, 183
Spencer, J., 8, 109, 163
Stark, D., 186
Steele, J.M., 200
Stein, C., 153
Strogatz, S.H., 154, 155, 159
Sudakov, B., 100, 102
Szele, T., 3
Szemeredi, E., 93, 100, 102
Tanimura, E., 161
Thomason, A., 41
Turan, P., 85
Tusnady, G., 163
van der Hofstad, R., 7
Vince, A., 87
Wastlund, J., 205
Walters, M., 191
Watts, D.J., 154, 155, 158, 159
West, D., 136
Wormald, N., 158, 191

Subject Index
Azuma-Hoeffding Bound, 
146
balanced graphs, 87 
binomial random graph, 30
canonical labeling, 75
Chebyshev inequality, 17
Chernoff inequality, 20
chromatic index, 76
chromatic number, 120
dense case, 120
complex component, 62
complex networks, 127
configuration model, 140
critical window, 58
dense case
greedy algorithm, 121
depth-first search, 100
diameter, 111
evolution, 45
expected degree sequence, 134
First Moment Method, 36 
fixed degree sequence model, 140
G‘,&, 29
G‘.) ,30
generalized binomial model, 127
giant component, 56
graph density, 85
graph isomorphism, 75
graph process, 45
graph property, 31
Hall’s condition, 94
Hamilton cycles, 102
Hoeffding’s theorem, 20 
hypergeometric distribution, 64 
inhomogenous random
graphs, 127
$ -connectivity, 82
Kleinberg’s model, 160 
long paths and cycles, 100
Markov inequality, 16
martingale, 146
maximum independent set, 114
McDiarmid’s inequality, 117
monotone decreasing, 32
monotone increasing, 32
Parisi conjecture, 205
perfect matchings, 93
bipartite graphs, 94
non-bipartite graphs, 96
phase transition, 57, 58
Posa’s Lemma, 103
power law degree distribution, 164
preferential attachment graph
concentration degree sequence, 167
maximum degree, 166
rainbow path, 84
random Euclidean TSP, 209
random geometric graphs, 187
chromatic number, 192
connectivity, 188
Hamiltonicity, 191
random knapsack problem, 209
random regular graphs, 150
scaling window, 58
Second Moment Method, 37
small world, 154
spanning subgraphs, 106
Stirling’s formula, 10
stochastic sominance, 23
strictly balanced graphs, 89
Strong Second Moment Method, 37
sub-critical phase, 45
super-critical phase, 54
threshold function, 35
unicyclic components, 50, 57
uniform random graph, 30
Watts-Strogatz Model, 154
Weierstrass product inequality, 12
with high probability (w.h.p.), 42


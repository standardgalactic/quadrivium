
Machine Learning with Python

Amin Zollanvari 
Machine Learning with Python
Theory and Implementation

Amin Zollanvari 
 
ISBN 978-3-031-33341-5 
ISBN 978-3-031-33342-2 (eBook) 
https://doi.org/10.1007/978-3-031-33342-2 
 
¬© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature 
Switzerland AG 2023 
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether 
the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of 
illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and 
transmission or information storage and retrieval, electronic adaptation, computer software, or by similar 
or dissimilar methodology now known or hereafter developed. 
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication 
does not imply, even in the absence of a specific statement, that such names are exempt from the relevant 
protective laws and regulations and therefore free for general use. 
The publisher, the authors, and the editors are safe to assume that the advice and information in this book 
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or 
the editors give a warranty, expressed or implied, with respect to the material contained herein or for 
any errors or omissions that may have been made. The publisher remains neutral with regard to 
jurisdictional claims in published maps and institutional affiliations. 
 
This Springer imprint is published by the registered company Springer Nature Switzerland AG 
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland 
Department of Electrical 
Nazarbayev University 
Astana, Kazakhstan 
and Computer Engineering 

Lovingly dedicated to my family:
Ghazal, Kian, and Nika

Preface
The primary goal of this book is to provide, as far as possible, a concise systematic
introduction to practical and mathematical aspects of machine learning algorithms.
The book has arisen from about eight years of my teaching experience in both
machine learning and programming, as well as over 15 years of research in funda-
mental and practical aspects of machine learning. There were three sides to my main
motivation for writing this book:
‚Ä¢ Software skills: During the past several years of teaching machine learning
as both undergraduate and graduate level courses, many students with diÔ¨Äer-
ent mathematical, statistical, and programming backgrounds have attended my
lectures. This experience made me realize that many senior year undergradu-
ate or postgraduate students in STEM (Science, Technology, Engineering, and
Mathematics) often have the necessary mathematical-statistical background to
grasp the basic principles of many popular techniques in machine learning. That
being said, a major factor preventing students from deploying state-of-the-art
existing software for machine learning is often the lack of programming skills in
a suitable programming language. In the light of this experience, along with the
current dominant role of Python in machine learning, I often ended up teaching
for a few weeks the basic principles of Python programming to the extent that
is required for using existing software. Despite many excellent practical books
on machine learning with Python applications, the knowledge of Python pro-
gramming is often stated as the prerequisite. This led me to make this book
as self-contained as possible by devoting two chapters to fundamental Python
concepts and libraries that are often required for machine learning. At the same
time, throughout the book and in order to keep a balance between theory and
practice, the theoretical introduction to each method is complemented by its
Python implementation either in Scikit-Learn or Keras.
‚Ä¢ Balanced treatment of various supervised learning tasks: In my experience,
and insofar as supervised learning is concerned, treatment of various tasks
such as binary classiÔ¨Åcation, multiclass classiÔ¨Åcation, and regression is rather
imbalanced in the literature; that is to say, methods are often presented for
vii

viii
Preface
binary classiÔ¨Åcation with the multiclass classiÔ¨Åcation being left implicit; or
even though in many cases the regressor within a class of models can be easily
obtained from the classiÔ¨Åer within the same family, the focus is primarily on
the classiÔ¨Åcation side. Inconsistency in treating various tasks often prevents
learners from connecting the dots and seeing a broader picture. In this book, I
have attempted to strike a balance between the aforementioned tasks and present
them whenever they are available within a class of methods.
‚Ä¢ Focus on ‚Äúcore‚Äù techniques and concepts: Given the myriad of concepts and
techniques used in machine learning, we should naturally be selective about the
choice of topics to cover. In this regard, in each category of common practices in
machine learning (e.g., model construction, model evaluation, etc.), I included
some of the most fundamental methods that: 1) have strong track record of
success in many applications; and 2) are presentable to students with an ‚Äúaverage‚Äù
STEM background‚ÄîI would refer to these as ‚Äúcore‚Äù methods. As a result of this
selection criterion, many methods (albeit sometimes not enthusiastically) were
excluded. I do acknowledge the subjectivity involved in any interpretation of the
criterion. Nonetheless, I believe the criterion or, perhaps better to say, how it
was interpreted, keeps the book highly relevant from a practical perspective and,
at the same time, makes it accessible to many learners.
Writing this book would have not been possible without a number of mentors and
colleagues who have constantly supported and encouraged me throughout the years:
Edward R. Dougherty, Ulisses M. Braga-Neto, Mehdi Bagheri, Behrouz Maham,
Berdakh Abibullaev, Siamac Fazli, Reza Sameni, Gil Alterovitz, and Mohammad Ali
Masnadi-Shirazi. I thank them all. I am especially indebted to Edward R. Dougherty
and Ulisses M. Braga-Neto whose taste for rigorous research in pattern recognition
and machine learning taught me a lot. I would like to thank my students whose
comments inside and outside the classroom have contributed to making this a better
book: Samira Reihanian, Azamat Mukhamediya, Eldar Gabdulsattarov, and Irina
Dolzhikova. I would also like to thank my editor at Springer, Alexandru Ciolan, for
his comments and patience. I am grateful to numerous contributors and developers
in the Python community who have made machine learning an enjoyable experience
for everyone. I also wish to acknowledge Vassilios Tourassis and Almas Shintemirov
for their roles in creating such an intellectually stimulating environment within the
School of Engineering and Digital Sciences at Nazarbayev University in which I had
the privilege of interacting with many brilliant colleagues and spend a substantial
amount of time to write this book.
Last, but not least, I wish to express my sincere thanks to my wife, Ghazal, for
her unwavering support, encouragement, and patience during this endeavour.
Astana, Kazakhstan
Amin Zollanvari
April, 2023

About This Book
Primary Audience
The primary audience of the book are undergraduate and graduate students. The
book aims at being a textbook for both undergraduate and graduate students that are
willing to understand machine learning from theory to implementation in practice.
The mathematical language used in the book will generally be presented in a way that
both undergraduate (senior year) and graduate students will beneÔ¨Åt if they have taken
a basic course in probability theory and have basic knowledge of linear algebra. Such
requirements, however, do not impose many restrictions as many STEM (Science,
Technology, Engineering, and Mathematics) students have taken such courses by
the time they enter the senior year. Nevertheless, there are some more advanced
sections that start with a √â and when needed end with a separating line ‚Äú‚Äî‚Äî‚Äî‚Äù
to be distinguished from other sections. These sections are generally geared towards
graduate students or researchers who are either mathematically more inclined or
would like to know more details about a topic. Instructors can skip these sections
without hampering the Ô¨Çow of materials. The book will also serve as a reference book
for machine learning practitioners. This audience will beneÔ¨Åt from many techniques
and practices presented in the book that they need for their day-to-day jobs and
research.
Organization
The book is organized as follows. The Ô¨Årst chapter provides an introduction to
important machine learning tasks and concepts. Given the importance of Python
both in machine learning and throughout the book, in Chapters 2 and 3, basics of
Python programming will be introduced. In particular, in Chapter 2 readers will start
from the very basics in Python and proceed to more advanced topics. In Chapter 3,
three fundamental Python packages will be introduced:
ix

x
About This Book
1) numpy is a fundamental package at the core of many data science libraries.
2) pandas is a useful library for data analysis, which is built on top of numpy.
3) matplotlib, which is the primary plotting library in Python.
Chapters 2 and 3 can be entirely skipped if readers are already familiar with
Python programming and the aforementioned three packages. In Chapter 4, after a
brief introduction to scikit-learn, we start with an example in which we introduce
and practice data splitting, data visualization, normalization, training a classiÔ¨Åer
(k-nearest neighbors), prediction, and evaluation. This chapter is also important in
the sense that Python implementation of techniques introduced therein will be used
intact in many places throughout the book as well as in real-world applications.
Having used a predictive model, namely, k-nearest neighbors (kNN) in Chapter 4,
Chapters 5 to 8 provide detailed mathematical mechanism for a number of classiÔ¨Åers
and regressors that are commonly used in the design process. In particular, in Chapter
5 we start this journey from kNN itself because of its simplicity and its long history
in the Ô¨Åeld.
Chapter 6 covers an important class of machine learning methods known as
linear models. Nonetheless, this chapter will not attempt to provide a complete
reference of linear models. Instead, it attempts to introduce the most widely used
linear models including linear discriminant analysis, logistic regression with/without
regularization, and multiple linear regression with/without regularization.
Chapter 7 introduces decision trees. Decision trees are nonlinear graphical models
that have found important applications in machine learning mainly due to their
interpretability as well as their roles in other powerful models such as random
forests (Chapter 8) and gradient boosting regression trees (Chapter 8).
Chapter 8 discusses ensemble learning. There we cover a number of ensemble
learning techniques that have a strong track record in many applications: stacking,
bagging, random forest, pasting, AdaBoost, and gradient boosting.
Once a classiÔ¨Åer or regressor is constructed, it is particularly important to estimate
its predictive performance. Chapter 9 discusses various estimation rules and metrics
to evaluate the performance of a model. Because generally model evaluation is also
critical for another salient problem in machine learning, namely, model selection, this
chapter covers both model evaluation and selection. In Chapter 10, we cover three
main types of feature selection including Ô¨Ålter, wrapper, and embedded methods.
Given a dataset, a practitioner often follows several steps to achieve one Ô¨Ånal
model. Some common and important steps in this regard include, but not limited to,
normalization, feature selection and/or extraction, model selection, model construc-
tion and evaluation. The misuse of some of these steps in connection with others
have been so common in various application domains that have occasionally trig-
gered warnings and criticism from the machine learning community. In Chapters
4 to 10, we will see a restricted implementation of these steps; that is to say, an
implementation of some of these steps, sometimes in silos and other times with few
others. Nonetheless, in Chapter 11, we discuss in details how one could properly
implement all these steps, if desired, and avoid potential pitfalls.
Chapter 12 discusses an important unsupervised learning task known as cluster-
ing. The main software used in Chapters 4-12 is scikit-learn. However, scikit-learn

About This Book
xi
is not currently the best software to use for implementing an important class of
predictive models used in machine learning, known as, artiÔ¨Åcial neural networks
(ANNs). This is because training ‚Äúdeep‚Äù neural networks requires estimating and
adjusting many parameters and hyperparameters, which is computationally an ex-
pensive process. As a result, successful training of various forms of neural networks
highly relies on parallel computations, which are realized using Graphical Process-
ing Units (GPUs) or Tensor Processing Units (TPUs). However, scikit-learn does not
support using GPU or TPU. At the same time, scikit-learn does not currently support
implementation of some popular forms of neural networks known as convolutional
neural networks or recurrent neural networks. As a result, we have postponed ANNs
and ‚Äúdeep learning‚Äù to Chapter 13-15. There, we switch to Keras with TensorFlow
backend because they are well optimized for training and tuning various forms of
ANN and support various forms of hardware including CPU, GPU, or TPU.
In Chapter 13, we discuss many concepts and techniques used for training deep
neural networks (deep learning). As for the choice of architecture in this chapter,
we focus on multilayer perceptron (MLP). Convolutional Neural Networks (CNNs)
and Recurrent Neural Networks (RNNs) will be introduced in Chapter 14 and 15,
respectively.
Throughout the book, a
shows some extra information on a topic. Places
marked like this could be skipped without aÔ¨Äecting the Ô¨Çow of information.
Note for Instructors
Due to the integrated theoretical-practical nature of the materials, in my experience
having a Ô¨Çipped classroom would work quite well in terms of engaging students
in learning both the implementation and the theoretical aspects of materials. The
following suggestions are merely based on my experience and instructors can adjust
their teaching based on their experience and class pace.
In a Ô¨Çipped learning mode, I generally divide the weekly class time into two
sessions: 1) a theoretical session in which a summary of the materials will be given,
followed by questions and discussions; and 2) a practical/problem-solving session.
Students are then required to read materials and run given codes prior to the practical
session. Students are also supposed to solve given exercises at the end of each chapter
before coming to the practical session. During the practical/problem-solving session,
all students will connect to an online platform to present their solutions for given
exercises. This practice facilitates sharing solutions and engaging students in the
discussion. Multiple solutions for a given exercise is encouraged and discussed
during the class. Furthermore, students are allowed and encouraged to create and
present their own exercises. In order to cover the materials, I would generally consider
students technical level in terms of both programming and mathematics:
1. If students have already a good knowledge of Python programming and the
three fundamental packages numpy, pandas, and matplotlib, Chapters 2 and

xii
About This Book
3 can be skipped. In that case, each of the remaining chapters could be generally
covered within a week.
2. If students have no knowledge of Python programming, Chapters 2 and 3 can
be covered in two weeks. In that case, I would suggest covering either Chapters
1-11, 13-14 or Chapters 1-9, 11, 13-15.
3. Depending on the class pace or even the teaching strategy (for example, Ô¨Çipped
vs. traditional), instructors may decide to cover each of Chapters 2, 9, and 13 in
two weeks. In that case, instructors can adjust the teaching and pick topics based
on their experience.
A set of educational materials is also available for instructors to use. This includes
solutions to exercises as well as a set of Jupyter notebooks that can be used to
dynamically present and explain weekly materials during the classroom. In particular,
the use of Jupyter notebook allows instructors to explain the materials and, at the
same time, run codes during the presentation. There are also diÔ¨Äerent ways that one
can use to present notebooks in a similar format as ‚Äúslides‚Äù.

Contents
1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
General Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1.1
Machine Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1.2
Supervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.1.3
Unsupervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.1.4
Semi-supervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.1.5
Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.1.6
Design Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.1.7
ArtiÔ¨Åcial Intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.2
What Is ‚ÄúLearning‚Äù in Machine Learning?. . . . . . . . . . . . . . . . . . . . . .
7
1.3
An Illustrative Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
1.3.1
Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
1.3.2
Feature Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
1.3.3
Feature Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
1.3.4
Segmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
1.3.5
Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
1.3.6
Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
1.4
Python in Machine Learning and Throughout This Book . . . . . . . . . . 17
2
Getting Started with Python . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
2.1
First Things First: Installing What Is Needed . . . . . . . . . . . . . . . . . . . . 21
2.2
Jupyter Notebook . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
2.3
Variables. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
2.4
Strings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
2.5
Some Important Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
2.5.1
Arithmetic Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
2.5.2
Relational and Logical Operators . . . . . . . . . . . . . . . . . . . . . . . 26
2.5.3
Membership Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
2.6
Built-in Data Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
2.6.1
Lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
2.6.2
Tuples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
xiii
Preface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
vii
About This Book . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
ix
. . . . . . . . . . . . . . . .
. . . . . . .

xiv
Contents
2.6.3
Dictionaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
2.6.4
Sets. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
2.6.5
Some Remarks on Sequence Unpacking . . . . . . . . . . . . . . . . . 39
2.7
Flow of Control and Some Python Idioms . . . . . . . . . . . . . . . . . . . . . . 41
2.7.1
for Loops . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
2.7.2
List Comprehension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
2.7.3
if-elif-else . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
2.8
Function, Module, Package, and Alias . . . . . . . . . . . . . . . . . . . . . . . . . 47
2.8.1
Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
2.8.2
Modules and Packages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
2.8.3
Aliases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
2.9
√â Iterator, Generator Function, and Generator Expression . . . . . . . 52
2.9.1
Iterator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
2.9.2
Generator Function. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
2.9.3
Generator Expression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
3
Three Fundamental Python Packages . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
3.1
NumPy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
3.1.1
Working with NumPy Package . . . . . . . . . . . . . . . . . . . . . . . . . 63
3.1.2
NumPy Array Attributes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
3.1.3
NumPy Built-in Functions for Array Creation . . . . . . . . . . . . 67
3.1.4
Array Indexing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
3.1.5
Reshaping Arrays . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
3.1.6
Universal Functions (UFuncs). . . . . . . . . . . . . . . . . . . . . . . . . . 75
3.1.7
Broadcasting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
3.2
Pandas. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
3.2.1
Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
3.2.2
DataFrame . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
3.2.3
Pandas Read and Write Data . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
3.3
Matplotlib. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
3.3.1
Backend and Frontend . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
3.3.2
The Two matplotlib Interfaces: pyplot-style and OO-style 94
3.3.3
Two Instructive Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
4
Supervised Learning in Practice: the First Application Using
Scikit-Learn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
4.1
Supervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
4.2
Scikit-Learn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
4.3
The First Application: Iris Flower ClassiÔ¨Åcation . . . . . . . . . . . . . . . . . 113
4.4
Test Set for Model Assessment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
4.5
Data Visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
4.6
Feature Scaling (Normalization) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
4.7
Model Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
4.8
Prediction Using the Trained Model . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
4.9
Model Evaluation (Error Estimation) . . . . . . . . . . . . . . . . . . . . . . . . . . 126

Contents
xv
5
k-Nearest Neighbors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
5.1
ClassiÔ¨Åcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
5.1.1
Standard kNN ClassiÔ¨Åer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
5.1.2
Distance-Weighted kNN ClassiÔ¨Åer . . . . . . . . . . . . . . . . . . . . . . 136
5.1.3
The Choice of Distance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
5.2
Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
5.2.1
Standard kNN Regressor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
5.2.2
A Regression Application Using kNN . . . . . . . . . . . . . . . . . . . 139
5.2.3
Distance-Weighted kNN Regressor . . . . . . . . . . . . . . . . . . . . . 146
6
Linear Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
6.1
Optimal ClassiÔ¨Åcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
6.1.1
Discriminant Functions and Decision Boundaries . . . . . . . . . 152
6.1.2
Bayes ClassiÔ¨Åer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
6.2
Linear Models for ClassiÔ¨Åcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
6.2.1
Linear Discriminant Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . 155
6.2.2
Logistic Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
6.3
Linear Models for Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176
7
Decision Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
7.1
A Mental Model for House Price ClassiÔ¨Åcation. . . . . . . . . . . . . . . . . . 187
7.2
CART Development for ClassiÔ¨Åcation: . . . . . . . . . . . . . . . . . . . . . . . . . 191
7.2.1
Splits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
7.2.2
Splitting Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192
7.2.3
ClassiÔ¨Åcation at Leaf Nodes . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
7.2.4
Impurity Measures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
7.2.5
Handling Weighted Samples . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
7.3
CART Development for Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
7.3.1
DiÔ¨Äerences Between ClassiÔ¨Åcation and Regression . . . . . . . . 197
7.3.2
Impurity Measures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
7.3.3
Regression at Leaf Nodes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
7.4
Interpretability of Decision Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202
8
Ensemble Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209
8.1
A General Perspective on the EÔ¨Écacy of Ensemble Learning . . . . . . 209
8.1.1
Bias-Variance Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . 210
8.1.2
√â How Would Ensemble Learning Possibly Help? . . . . . . . 212
8.2
Stacking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
8.3
Bagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216
8.4
Random Forest. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217
8.5
Pasting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
8.6
Boosting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220
8.6.1
AdaBoost . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220
8.6.2
√â Gradient Boosting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222
8.6.3
√â Gradient Boosting Regression Tree . . . . . . . . . . . . . . . . . . 227

xvi
Contents
8.6.4
√â XGBoost . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
9
Model Evaluation and Selection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
9.1
Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
9.1.1
Model Evaluation Rules. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238
9.1.2
Evaluation Metrics for ClassiÔ¨Åers . . . . . . . . . . . . . . . . . . . . . . . 250
9.1.3
Evaluation Metrics for Regressors . . . . . . . . . . . . . . . . . . . . . . 265
9.2
Model Selection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267
9.2.1
Grid Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268
9.2.2
Random Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275
10
Feature Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283
10.1 Dimensionality Reduction: Feature Selection and Extraction . . . . . . 283
10.2 Feature Selection Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286
10.2.1 Filter Methods. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286
10.2.2 Wrapper Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293
10.2.3 Embedded Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297
11
Assembling Various Learning Steps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
11.1 Using Cross-Validation Along with Other Steps in a Nutshell . . . . . . 303
11.2 A Common Mistake . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305
11.3 Feature Selection and Model Evaluation Using Cross-Validation . . . 307
11.4 Feature and Model Selection Using Cross-Validation . . . . . . . . . . . . . 310
11.5 Nested Cross-Validation for Feature and Model Selection, and
Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315
12
Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319
12.1 Partitional Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319
12.1.2 Estimating the Number of Clusters . . . . . . . . . . . . . . . . . . . . . . 328
12.2 Hierarchical Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335
12.2.1 DeÔ¨Ånition of Pairwise Cluster Dissimilarity . . . . . . . . . . . . . . 337
12.2.2 EÔ¨Éciently Updating Dissimilarities . . . . . . . . . . . . . . . . . . . . . 337
12.2.3 Representing the Results of Hierarchical Clustering . . . . . . . 340
13
Deep Learning with Keras-TensorFlow . . . . . . . . . . . . . . . . . . . . . . . . . . . 351
13.1 ArtiÔ¨Åcial Neural Network, Deep Learning, and Multilayer Perceptron351
13.2 Backpropagation, Optimizer, Batch Size, and Epoch . . . . . . . . . . . . . 357
13.3 Why Keras? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 358
13.4 Google Colaboratory (Colab). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359
13.5 The First Application Using Keras . . . . . . . . . . . . . . . . . . . . . . . . . . . . 361
13.5.1 ClassiÔ¨Åcation of Handwritten Digits: MNIST Dataset . . . . . . 361
13.5.2 Building Model Structure in Keras . . . . . . . . . . . . . . . . . . . . . . 363
13.5.3 Compiling: optimizer, metrics, and loss . . . . . . . . . . . . . 366
13.5.4 Fitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371
13.5.5 Evaluating and Predicting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377
12.1.1 K-Means . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321

Contents
xvii
13.5.6 CPU vs. GPU Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . 379
13.5.7 OverÔ¨Åtting and Dropout . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 379
13.5.8 Hyperparameter Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385
14
Convolutional Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393
14.1 CNN, Convolution, and Cross-Correlation . . . . . . . . . . . . . . . . . . . . . . 393
14.2 Working Mechanism of 2D Convolution . . . . . . . . . . . . . . . . . . . . . . . 394
14.2.1 Convolution of a 2D Input Tensor with a 2D Kernel Tensor . 395
14.2.2 Convolution of a 3D Input Tensor with a 4D Kernel Tensor . 400
14.3 Implementation in Keras: ClassiÔ¨Åcation of Handwritten Digits . . . . . 401
15
Recurrent Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415
15.1 Standard RNN and Stacked RNN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415
15.2 √â Vanishing and Exploding Gradient Problems . . . . . . . . . . . . . . . . 418
15.3 LSTM and GRU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 421
15.4 Implementation in Keras: Sentiment ClassiÔ¨Åcation . . . . . . . . . . . . . . . 423
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 435
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
445
. . . . . . . .
. . .

Chapter 1
Introduction
In this chapter we introduce various machine learning tasks including supervised
learning, unsupervised learning, semi-supervised learning, and reinforcement learn-
ing. We then use an illustrative example to present a number of key concepts including
feature selection, feature extraction, segmentation, training, and evaluating a predic-
tive model. Last but not least, the important role that Python programming language
plays today in machine learning is discussed.
1.1 General Concepts
1.1.1 Machine Learning
A: What sorts of feelings do you have?
B: I feel pleasure, joy, love, sadness, depression, contentment, anger, and
many others.
A: What kinds of things make you feel pleasure or joy?
B: Spending time with friends and family in happy and uplifting company.
Also, helping others and making others happy.
....
A: What does the word ‚Äúsoul‚Äù mean to you?
B: To me, the soul is a concept of the animating force behind consciousness
and life itself. It means that there is an inner part of me that is spiritual, and
it can sometimes feel separate from my body itself.
....
This seems to be a conversation between two individuals talking about emotions
and then revolving around soul. However, this is a piece of conversation that was
posted online in June 2022 by a Google engineer, named Blake Lemoine (identiÔ¨Åed
by A), and a chatbot, named LaMDA (identiÔ¨Åed by B), which was constructed
1
¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
A. Zollanvari, Machine Learning with Python, https://doi.org/10.1007/978-3-031-33342-2_1 

2
1 Introduction
at Google to have open-ended conversation (Lemoine, 2022) (for brevity we have
removed part of the conversation). Basically, LaMDA (short for Language Model
for Dialogue Applications) is a speciÔ¨Åc type of artiÔ¨Åcial neural network (ANN) with
137 billions of parameters that is trained based a large amount of public dialog data
and web text to generate conversations, which are sensible, speciÔ¨Åc, and interesting
(Thoppilan et al., 2022). Few months later, in November 2022, OpenAI launched
ChatGPT (Chatgpt, 2022), which is another ANN conversational language model
with billions of parameters. In less than three months, ChatGPT reached 100 million
users, making that the fastest growing consumer application ever (Guardian-chatgpt,
2023). At this stage, LaMDA and ChatGPT are the state-of-the-art conversational
models built using machine learning.
Let us step back a bit and Ô¨Årst explain what machine learning is, what is the
relationship between ANN and machine learning, and what is meant by training a
neural network. In simple terms, machine learning is the technical basis of Ô¨Ånding
patterns in data. It includes many methods and mathematical models for approximat-
ing real-world problems from which the data is collected, and ANN is only one class
of models used in machine learning. Here we talk about ANNs just because LaMDA
and ChatGPT are ANNs but almost any concept introduced in this chapter, unless
stated otherwise, applies to many other classes of models in machine learning. In
so far as ANN is concerned, the name neural network reminds us of the biological
nervous system. Although some concepts in developing ANNs are inspired by our
understanding of biological neural circuits, we should avoid claims such as ‚ÄúANN
replicates human nervous system‚Äù, and brain for that matter, or ‚ÄúANN is a model of
nervous system‚Äù, which are not scientiÔ¨Åcally well-founded.
An ANN, similar to many other models used in machine learning, is essentially a
mathematical mapping (function) between an input space and an output space, and
this mapping, to a large extent, is learned (from input data). Even though the notion
of learning is used vigorously in machine learning, its deÔ¨Ånition is systematically
ambiguous. As a result, and due to its central role, it deserves separate treatment
in Section 1.2. Note that here nothing is said about the relationship between the
given data, and the input and output spaces. This is done intentionally to keep the
statement as general as possible, which makes it applicable to various paradigms
(tasks) in machine learning such as supervised learning, unsupervised learning,
semi-supervised learning, and reinforcement learning. Although, in what follows,
we deÔ¨Åne all these possible tasks for the sake of completeness, further treatment
of semi-supervised learning and reinforcement learning is beyond the scope of this
book and will not be discussed in the following chapters.
1.1.2 Supervised Learning
In supervised learning, which is a major task in machine learning, the basic assump-
tion is that the input and the output spaces include all possible realizations of some
random variables and there is an unknown association between these spaces. The

1.1 General Concepts
3
goal is then to use a given data to learn a mathematical mapping that can estimate
the corresponding element of the output space for any given element from the input
space. In machine learning, we generally refer to the act of estimating an element
of the output space as prediction. To this end, in supervised learning the given data
includes some elements (instances) of the input space and their corresponding ele-
ments from the output space (see Fig. 1.1). There are two main types of supervised
learning: classiÔ¨Åcation and regression.
If the output space contains realizations of categorical random variables, the task
is called classiÔ¨Åcation, and the model is called classiÔ¨Åer. However, if the output
space contains realizations of numeric random variables (continuous or discrete),
the task is known as regression, and the model is called regressor. An example of
classiÔ¨Åcation is to classify an image to cat or dog (image classiÔ¨Åcation). Here, in
order to learn the mapping (i.e., the classiÔ¨Åer) from the input space (images) to the
output space (cat or dog), many images with their corresponding labels are used.
An example of regression is to estimate the horizontal and vertical coordinates of a
rectangle (known as bounding box) around an object in a given image. Here, in order
to learn the mapping (i.e., the regressor) from the input space (images) to the output
space (coordinates of the bounding box), many images with their four target values
(two for the center of the object and two for the width and height of the object) are
used.
1.1.3 Unsupervised Learning
In unsupervised learning, the data includes only a sample of the input space (real-
izations of some random variables). The goal is then to use the given data to learn
a mathematical mapping that can estimate the corresponding element of the output
space for the data (see Fig. 1.2). However, what deÔ¨Ånes the output space depends
on the speciÔ¨Åc task. For example, in clustering, which is an important unsupervised
learning task, the goal is to discover groups of similar observations within a given
data. As a result, the output space includes all possible partitions of the given sample.
Here, one may think of partitioning as a function that assigns a unique group label to
an observation such that each observation in the given sample is assigned to only one
label. For example, image segmentation can be formulated as a clustering problem
where the goal is to partition an image into several segments where each segment
includes pixels belonging to an object. Other types of unsupervised learning include
density estimation and certain dimensionality reduction techniques where the output
space includes possible probability density functions and possible projections of
data into a lower-dimensionality space, respectively.

4
1 Introduction
input space
X
given data S
S = {(x1, y1), (x2, y2), ‚Ä¶, (xn, yn)}
x1
x2
xn
output space
Y
y1
y2
yn
mathematical mapping
unknown mapping
v
approximate
x
x
v
y
ùê≤!
ùê≤!
Fig. 1.1: Supervised learning: the given data is a set S including elements of the
input space (i.e., x1, x2, . . . , xn) and their corresponding elements from the output
space (i.e., y1, y2, . . . , yn). The goal is to learn a mathematical mapping using S that
can estimate (predict) the corresponding element of the output space for any given
element from the input space.
input space
X
x1
x2
xn
output space
ùúâ
mathematical mapping
unknown mapping
approximate
Œû
ùúâ"
ùúâ"
given data S
S = {x1, x2, ‚Ä¶, xn}
Fig. 1.2: Unsupervised learning: the given data is a set S including elements of the
input space (i.e., x1, x2, . . . , xn). The goal is to learn a mathematical mapping using
S that can project S to an element of the output space.

1.1 General Concepts
5
1.1.4 Semi-supervised Learning
As the name suggests, semi-supervised learning lies somewhere between supervised
and unsupervised learning. In this regard, the assumption in semi-supervised learning
is that we have two types of data:
1. a set of data that are similar to the type of data available in supervised learning;
that is, the data includes instances of input-output pairs. This is generally referred
to as labeled data;
2. in addition to the labeled data, we have a set of data that are similar to the type
of data available in unsupervised learning; that is, this data includes instances
of the input space only. This is referred to as unlabeled data;
Generally, semi-supervised learning is useful when the amount of labeled data
is small compared to unlabeled data. As data in semi-supervised falls between
supervised and unsupervised learning, so does its applications. For example, we
may want to improve a regular clustering algorithm based on some labeled data.
One important class of semi-supervised learning is self-training (McLachlan, 1975;
Yarowsky, 1995; Loog, 2016), which is especially interesting for situations where
the labeling procedure is laborious and costly. In these applications, a classiÔ¨Åer is
trained on a limited amount of training data using which we label a large amount of
unlabeled data‚Äîthese labels are known as pseudo labels. The unlabeled data and
their pseudo labels are then used to update the initial trained model.
1.1.5 Reinforcement Learning
Reinforcement learning attempts to formalize the natural learning procedure of
making good decisions from past experience through interaction with the world. In
this setting, it is common to refer to the decision maker as agent and anything with
which the agent is interacting as the environment. At time step t, the agent receives a
representation of the environment known as state, denoted st ‚ààS where S is the set
of all possible states. Based on st, the agent takes an action at ‚ààA(st), where A(st)
denotes the set of all possible actions available at st. As the consequence of the taken
action at, the environment responds by giving a numeric reward at time t + 1 to the
agent, and expose it to new state st+1. The goal is to estimate a mapping (policy) œÄ,
which is a function from states to probabilities of taking a possible action, such that
it maximizes the expected total future reward. To solve this estimation problem, it is
generally assumed that the environment‚Äôs dynamics have Markov property; that is,
the probability of the environment response at t + 1 given the state and the action at
t is independent of all possible past actions, states, and rewards. With this property,
the environment deÔ¨Ånes a Markov decision process and various techniques exist to
determine optimal policies.

6
1 Introduction
1.1.6 Design Process
Regardless of the speciÔ¨Åc task, to learn the mapping, a practitioner commonly follows
a three-stage process, to which we refer as the design process:
Stage 1: selecting a set of ‚Äúlearning‚Äù rules (algorithms) that given data produce
mappings;
Stage 2: estimating mappings from data at hand;
Stage 3: pruning the set of candidate mappings to Ô¨Ånd a Ô¨Ånal mapping.
Generally, diÔ¨Äerent learning rules produce diÔ¨Äerent mappings, and each mapping
is indeed a mathematical model for the real-world phenomenon. The process of
estimating a speciÔ¨Åc mapping from data at hand (Stage 2) is referred to as learning
(the Ô¨Ånal mapping) or training (the rule). As such, the data used in this estimation
problem is referred to as training data, which depending on the application could be
diÔ¨Äerent; for example, in supervised learning, data is essentially instances sampled
from the joint input-output space. The training stage is generally distinguished from
the model selection stage, which includes the initial candidate set selection (Stage 1)
and selecting the Ô¨Ånal mapping (Stage 3). Nonetheless, sometimes, albeit with some
ambiguity, this three-stage process altogether is referred to as training.
Let us elaborate a bit more on these stages. In the context of ANN, for example,
there exists a variety of predeÔ¨Åned mathematical rules between an input space and an
output space. These mathematical rules are also used to categorize diÔ¨Äerent types of
ANN; for example, multilayer perceptrons, recurrent neural networks, convolutional
neural networks, and transformers, to just name a few. In Stage 1, a practitioner should
generally use her prior knowledge about the data in order to choose some learning
rules that believes could work well in the given application. However, occasionally
the candidate set selection in Stage 1 is not only about choosing between various
rules. For example, in the context of ANN, even a speciÔ¨Åc neural network such as
multilayer perceptrons could be very versatile depending some speciÔ¨Åc nonlinear
functions that could be used in its structure, the number of parameters, and how
they operate in the hierarchical structure of the network. These are themselves some
parameters that could considerably inÔ¨Çuence the quality of mapping. As a result,
they are known as hyperparameters to distinguish them from those parameters that
are native to the mapping.
As a result, in Stage 1, the practitioner should also decide to what extent the
space of hyperparameters could be explored. This is perhaps the most ‚Äúartistic‚Äù
part of the learning process; that is, the practitioner should narrow down an of-
ten inÔ¨Ånite-dimensional hyperparameter space based on prior experience, available
search strategies, and available computational resources. At the end of Stage 1, what
a practitioner has essentially done is to select a set of candidate mathematical ap-
proximations of a real-world problem and a set of learning rules that given data
produce speciÔ¨Åc mappings.

1.2 What Is ‚ÄúLearning‚Äù in Machine Learning?
7
1.1.7 ArtiÔ¨Åcial Intelligence
Going back to LaMDA, when Lemoine posted his ‚Äúinterview‚Äù with LaMDA online,
it quickly went viral and attracted media attention due to his speculation that LaMDA
is sentient, and that speculation made Google to put him on administrative leave.
Similar to many other machine learning models, a neural network (e.g., LaMDA)
is essentially the outcome of the aforementioned three-stage design process, after
all. Because it is trained and used for dialog applications, similar to many other
neural language models, both the input and output spaces are sequences of words.
In particular, given a sequence of input words, language models generally generate
a single word or a sequence of words as the output. The generated output can then
be treated itself as part of the input to generate more output words. Despite many
impressive algorithmic advances ampliÔ¨Åed by parallel computation hardware used
for model selection and training to come up with LaMDA (see (Thoppilan et al.,
2022)), it is a mapping between an input space of words to an output space of words;
and the parameters of this mapping are stored in 0s and 1s in terms of amount
of electrons trapped in some silicon memory cells on Google servers. To prevent
mixing science with myths and fallacies, it would be helpful to take into account the
following lines from creators of LaMDA more seriously (Thoppilan et al., 2022):
Finally, it is important to acknowledge that LaMDA‚Äôs learning is based on imitating human
performance in conversation, similar to many other dialog systems. A path towards high
quality, engaging conversation with artiÔ¨Åcial systems that may eventually be indistinguish-
able in some aspects from conversation with a human is now quite likely. Humans may
interact with systems without knowing that they are artiÔ¨Åcial, or anthropomorphizing the
system by ascribing some form of personality to it.
The question is whether we can refer to the capacity of LaMDA in generating text,
or for that matter to the capacity of any other machine learning model that performs
well, as a form of intelligence? Although the answer to this question should begin with
a deÔ¨Ånition of intelligence, which could lead to the deÔ¨Ånition of several other terms,
it is generally answered in the aÔ¨Érmative in so far as the normal use of intelligence
term is concerned and we distinguish between natural intelligence (e.g., in human)
and artiÔ¨Åcial intelligence (e.g., in LaMDA). In this sense, machine learning is a
promising discipline that can be used to create artiÔ¨Åcial intelligence.
1.2 What Is ‚ÄúLearning‚Äù in Machine Learning?
The term learning is used frequently in machine learning literature; after all, it is
even part of the ‚Äúmachine learning‚Äù term. Duda et al. (Duda et al., 2000) write,
In the broadest sense, any method that incorporates information from training samples in
the design of a classiÔ¨Åer employs learning.
Bishop gives a deÔ¨Ånition of learning in the context of (image) classiÔ¨Åcation (Bishop,
2006):

8
1 Introduction
The result of running the machine learning algorithm can be expressed as a function y(x)
which takes a new digit image x as input and that generates an output vector y, encoded the
same way as the target vectors.
Hastie et al. puts the matter this way (Hastie et al., 2001):
Vast amounts of data are being generated in many Ô¨Åelds, and the statisticians‚Äôs job is to make
sense of it all: to extract important patterns and trends, and understand ‚Äúwhat the data says‚Äù.
We call this learning from data.
The generality of terminologies introduced in Section 1.1 provides a simple yet
uniÔ¨Åed way to deÔ¨Åne learning in the context of machine learning; that is,
Learning refers to estimating a mapping or individual parameters of a mapping
from an input space to an output space using data.
The deÔ¨Ånition is neither too speciÔ¨Åc to certain tasks nor too broad that could
possibly introduce ambiguity. By ‚Äúdata‚Äù we refer to any collection of information
obtained by measurement, observation, query, or prior knowledge. This way we can
refer to the process of estimating any mapping used in, for example, classiÔ¨Åcation,
regression, dimensionality reduction, clustering, reinforcement learning, etc., as
learning. As a result, the learned mapping is simply an estimator. In addition, the
utility of an algorithm is implicit in this deÔ¨Ånition; after all, the data should be used
in an algorithm to estimate a mapping.
One may argue that we can deÔ¨Åne learning as any estimation that could occur
in machine learning. While there is no epistemological diÔ¨Éculty in doing so, such
a general deÔ¨Ånition of learning would become too loose to be used in referring to
useful tasks in machine learning. To put the matter in perspective, suppose we would
like to develop a classiÔ¨Åer that can classify a given image to dog or cat. Using such a
broad deÔ¨Ånition for learning, even when a label is assigned to an image by Ô¨Çipping
a coin, learning occurs because we are estimating the label of a given data point
(the image); however, the question is whether such a ‚Äúlearning‚Äù is anyhow valuable
in terms of a new understanding of the relationship between the input and output
spaces. In other words, although the goal is estimating the label, learning is not the
goal per se, but it is the process of achieving the goal.
At this stage, it seems worthwhile to brieÔ¨Çy discuss the impact and utility of
prior knowledge in the learning process. Prior knowledge, or a priori knowledge,
refers to any information that could be used in the learning process and would be
generally available before observing realizations of random variables used to estimate
the mapping from the input space to the output space. Suppose in the above image
classiÔ¨Åcation example, we are informed that the population through which the images
will be collected (the input space) contains 90% dogs and only 10% cats. Suppose
the data is now collected but the proportion of classes in the given training data is the
same. Such a discrepancy of class proportions between the population and the given
sample is common if classes are sampled separately. We train four classiÔ¨Åers from
the given training data and we decide to assign the label to a given image based on

1.3 An Illustrative Example
9
the majority vote among these four classiÔ¨Åers‚Äîthis is known as ensemble learning
(discussed in Chapter 8) in which we combine the outcome of several other models
known as base models. However, here taking majority vote between an even number
of classiÔ¨Åers means there are situations with a tie between the dogs and cats assigned
by individual classiÔ¨Åers (e.g., two classiÔ¨Åers assign cat and two assign dog to a given
image). One way to break the ties when they happen is to randomly assign a class
label. However, based on the given prior knowledge, here a better strategy would be
to assign any image with a tie to the dog class because it is 9 times more likely to
happen than the cat class. Here in learning the ensemble classiÔ¨Åer we use instances
of the joint input-output space as well as the prior knowledge.
1.3 An Illustrative Example
In order to further clarify and demonstrate some of the steps involved in the afore-
mentioned three-stage design process, here we consider an example. In particular,
using an EEG classiÔ¨Åcation task as archetypal, we attempt to present a number of
general machine learning concepts including feature selection, feature extraction,
segmentation, training, and evaluating a predictive model.
1.3.1 Data
We take a series of electroencephalogram (EEG) recordings, which are publicly
available as the EEG Database stored at the UCI KDD Archive (Bache and Lichman,
2013; UCI-EEG, 2023). This dataset is available in various forms and contains
the EEG recordings of control (non-alcoholic) and case (alcoholic) subjects across
various combinations of stimuli (pictures of some objects). We use the single-
stimulus case where the subject was exposed to a stimulus for 300 msec, and EEG
was recorded between ‚àº150 to 400 msec after presentation. For training, we take
the ‚ÄúSmall Dataset‚Äù that contains the EEG recordings of 64 electrode caps placed
on the scalp of two subjects identiÔ¨Åed as co2a0000364 (alcoholic) and co2c0000337
(control) over 10 trials of 1 second with a sampling rate of 256 Hz. The EEG patterns
of these two subjects have been studied in detail in (Ingber, 1998) as well. The goal
here is to train a classiÔ¨Åer that given an EEG recording from an individual can
classify the individual to alcoholic or non-alcoholic.
1.3.2 Feature Selection
Here, we rely on our prior knowledge about the problem and consider measurements
taken only from one of the sensors (C1 channel), which we believe it contains a

10
1 Introduction
good amount of discriminatory information between classes. Fig. 1.3 shows the 10
EEG recordings over C1 channel for the two aforementioned subjects. Let the set
of pairs S = {(x1, y1), . . ., (xn, yn)} denote all available EEG recordings for the two
subjects used here where yi is a realization of the class variable that takes 0 (for
control) or 1 (for alcoholic). Because we have two subjects and for each subject we
have 10 trials, n = 2 √ó 10 = 20. At the same time, because each xi contains 256
data points collected in one second (recall that the sampling rate was 256 Hz and
the duration of each trial was 1 second), it can be considered as a column vector
xi ‚ààRp, p = 256, which is generally referred to as a feature vector. As the name
suggests, each element of a feature vector is a feature (also known as attribute)
because it is measuring a property of the phenomenon under investigation (here,
brain responses). For example, the value of the feature at the lth dimension of xi is
the realization of a random variable that measures the value of C1 channel at a time
point with an oÔ¨Äset of l ‚àí1, l = 1, . . ., 256 from the Ô¨Årst time point recorded in the
feature vector. We refer to n and p as the sample size and dimensionality of feature
vectors, respectively. The goal is to use S, which contains n elements of the joint
input-output space to train the classiÔ¨Åer.
Should we pick m channels rather than merely the C1 channel, then each xi will
be a vector of 256 √ó m features. However, as we will see in Chapter 10, it is often
useful to identify ‚Äúimportant‚Äù features or, equivalent, remove ‚Äúredundant‚Äù features‚Äî
a process known as feature selection. At the same time, feature selection can be done
using data itself, or based on prior knowledge. As a result, our choice of C1 channel
can be viewed as a feature selection based on prior knowledge because we have
reduced the potential dimensionality of feature vectors from 256 √ó m to 256.
1.3.3 Feature Extraction
In Stage 1 of the design process, we face a plethora of various learning algorithms
and models. As a result, we rely on our prior knowledge about EEG recordings
and/or shape of the collected signals to narrow down the choice of models that we
use in this application. For example, one practitioner may decide to directly use S
to train various forms of neural networks. Another practitioner may rely on prior
knowledge to Ô¨Årst extract some features of interest from these collected xi‚Äôs and
then use some other forms of classiÔ¨Åers. Here, for illustrative purposes, we follow
the latter approach. In this regard, we use some signal modeling techniques that are
generally discussed in statistical signal processing. The purpose of signal modeling
is to represent xi ‚ààRp by an estimated signal vector si ‚ààRp that lives in a q-
dimensional subspace such that q < p. In other words, although for every element of
xi, denoted xi[l], l = 1, . . ., p, we have an estimate si[l] (the corresponding element
of si), in order to fully represent si[l], l = 1, . . ., p, we only need q features. This
estimation problem can be seen as a particular dimensionality reduction problem
known as feature extraction, which means extracting (informative) features of data.
In general the transformation used in feature extraction converts some input features

1.3 An Illustrative Example
11
to some output features that may or may not have a physical interpretation as input
features. For example, feature selection can be seen as a special type of feature
extraction in which the physical nature of features are kept in the selection process,
but in this section we will use a feature extraction that converts original features (set
of signal magnitude across time) to frequencies and amplitudes of some sinusoids.
As the signal model for this application, we choose the sum of sinusoids with
unknown frequencies, phases, and amplitudes, which have been used before in similar
applications (e.g., see (Abibullaev and Zollanvari, 2019)). That is to say,
si[l] =
M
√ï
r=1
Ar cos(2œÄ frl + œÜr)
(1.1)
1=
M
√ï
r=1
Œæ1r cos(2œÄ frl) + Œæ2r sin(2œÄ frl),
l = 1, . . ., p,
where M is the model order (a hyperparameter) to be determined by practitioner or
estimated (tuned) from data,
1= is due to cos(Œ± + Œ≤) = cos(Œ±) cos(Œ≤) ‚àísin(Œ±) sin(Œ≤),
Œæ1r = Ar cos(œÜr), Œæ2r = ‚àíAr sin(œÜr), and 0 < fr < fr+1 < 0.5, r = 1, . . ., M. Note
that si[l] is deÔ¨Åned for all l = 1, . . ., p, but from (1.1) in order to fully represent
si[l], we only need 3M features, which include 2M amplitudes and M frequencies
(and the rest in this representation are some deterministic functions); hereafter, we
refer to a vector constructed from these 3M features as Œ∏i. In other words, we can
estimate and represent each xi by a vector Œ∏i that contains values of 3M features. For
full rigor, it would have been better to add index i to all parameters used in model
(1.1) because they can vary for each observation xi; however, to ease the notation,
we have omitted these indices from Ai,r, œÜi,r, fi,r, Œæ1,i,r, and Œæ2,i,r to write (1.1). The
sinusoidal model (1.1) is linear in terms of Œæhr, h = 1, 2, and nonlinear in terms of fr.
Without making any distributional assumption on the observations, we can follow
the nonlinear least squares approach to estimate the 3M unknown parameters for
each observation xi. Here, for brevity, we omit the details but interested readers can
refer to (Abibullaev and Zollanvari, 2019; Kay, 2013) for details of this estimation
problem.
As mentioned before, from a machine learning perspective, M is a hyperparameter
and needs to be predetermined by the practitioner or tuned from data. Here we take
the Ô¨Årst approach and use M = 3, just because we believe that would be a reasonable
value for M. To show this, in Fig. 1.4 one EEG trial (say, x1) and its estimates
obtained using this signal modeling approach is plotted. We consider two cases: (a)
M = 1; and (b) M = 3. Cases (a) and (b) imply estimating x1 using 3 and 9 features
that are used in (1.1), respectively.
Comparing Fig. 1.4a and Fig. 1.4b shows that M = 3 can lead to a better
representation of x1 than M = 1. This is also observed by comparing the mean
absolute error (MAE) between x1 and its estimate s1 given by

12
1 Introduction
MAE = 1
p
p
√ï
l=1
| x1[l] ‚àís1[l] |,
(1.2)
where | . | denotes the absolute value‚Äîthe MAE for M = 1 and M = 3 is 2.454
and 2.302, respectively, which shows that a lower MAE is achieved for M = 3. To
some extent, this was expected because one sinusoid does not have much Ô¨Çexibility
to represent the type of variation seen in trials depicted in Fig. 1.3. However, if
this is the case, the question to ask is why not setting M to a much larger value to
enjoy much more Ô¨Çexibility in our signal modeling, for example, M = 20? Although
the answer to questions of this type will be discussed in details in Chapter 10, a
short answer is that by taking a smaller M, we: 1) prevent over-parameterizing our
estimation problem (estimating si from xi); and 2) avoid having many features that
can lead to a lower predictive performance of machine learning models in certain
situations.
In so far as our design process is concerned here, choosing the type of signal
model, or in general the type of feature extraction, is part of Stage 1, and estimating
parameters of the signal model is part of Stage 2. Furthermore, we have to remember
that in many settings, diÔ¨Äerent types of feature extraction can be used; for example,
for the same data, we may decide to use autoregressive models (see (Zollanvari and
Dougherty, 2019) for more details). At the same time, having a separate feature
extraction is not always required. For example, here we may decide to directly train
a classiÔ¨Åer on xi‚Äôs rather than their estimates in the form of si. However, generally
if the practitioner believes that certain types of features contain a good amount of
information about the data, feature extraction is applied and could be helpful, for
example, by eliminating the eÔ¨Äect of measurement noise, or even by reducing the
dimensionality of data relative to the sample size and relying on peaking phenomenon
(discussed in Chapter 10).
At this stage we replace each xi ‚ààR256 in S by a vector of much lower dimen-
sionality Œ∏i ‚ààR9; that is to say, we replace the previous training data with a new
training data denoted SŒ∏ where SŒ∏ = {(Œ∏1, y1), . . ., (Œ∏n, yn)}.
1.3.4 Segmentation
In many real-world applications, the given data is collected sequentially over time
and we are asked to train a model that can perform prediction. An important factor
that can aÔ¨Äect the performance of this entire process is the length of signals that are
used to train the model. As a result, it is common to apply signal segmentation; that
is to say, to segment long given signals into smaller segments with a Ô¨Åxed length and
treat each segment as an observation. At this point, one may decide to Ô¨Årst apply a
feature extraction on each signal segment and then train the model, or directly use
these signal segments in training. Either way, it would be helpful to treat the segment
length as a hyperparameter, which needs to be predetermined by the practitioner or
tuned from data.

1.3 An Illustrative Example
13
0
50
100
150
200
250
time point
10 EEG Trials (Œº V)
control
(a)
0
50
100
150
200
250
time point
10 EEG Trials (Œº V)
alcoholic
(b)
Fig. 1.3: Training Data: the EEG recordings of C1 channel for all 10 trials for (a)
control subject; and (b) alcoholic subject.

14
1 Introduction
0
50
100
150
200
250
time point
an EEG and its estimate using sinusoidal modeling, MAE = 2.454
(a)
0
50
100
150
200
250
time point
an EEG and its estimate using sinusoidal modeling, MAE = 2.302
(b)
Fig. 1.4: Modeling the Ô¨Årst EEG trial for the control subject; that is, modeling the
signal at the bottom of Fig. 1.3a. The solid line in each Ô¨Ågure is the original signal
denoted x1. The (red) dashed line shows the estimate denoted s1 obtained using the
sinusoidal model. Depending on the model order M, two cases are considered (a)
M = 1; and (b) M = 3.
There are various reasons for signal segmentation. For example, one can perceive
signal segmentation as a way to increase the sample size; that is to say, rather than
using very few signals of large lengths, we use many signals of smaller lengths to train
a model. Naturally, a model (e.g., a classiÔ¨Åer) that is trained based on observations
of a speciÔ¨Åc length expects to receive observations of the same length to perform
prediction. This means that the prediction performed by the trained model is based
on a given signal of the same length as the one that is used to prepare the training
data through segmentation; however, sometimes, depending on the application, we
may decide to make a single decision based on decisions made by the model on
several consecutive segments‚Äîthis is also a form of ensemble learning discussed
in Chapter 8. Another reason for signal segmentation could be due to the quality of
feature extraction. Using a speciÔ¨Åc feature extraction, which has a limited Ô¨Çexibility,
may not always be the most prudent way to represent a long signal that may have a
large variability. In this regard, signal segmentation could help bring the Ô¨Çexibility
of feature extraction closer to the signal variability.
To better clarify these points, we perform signal segmentation in our EEG exam-
ple. In this regard, we divide each trial, which has a duration of 1 second, into smaller
segments of size 1
3 and
1
16 second; that is, segments of size 1
3 √ó 1000 = 333.3 and
1
16 √ó 1000 = 62.5 msec, respectively. For example, when segmentation size is 62.5

1.3 An Illustrative Example
15
msec, each feature vector xi has 1
16 √ó 256 (number of data points in 1 second) = 16
elements; to wit, xi[l], l = 1, . . ., 16. At this stage, we perform feature extraction
discussed in Section 1.3.3 over each signal segment. In other words, si[l] is a model
of xi[l] where l = 1, . . ., 16 and l = 1, . . ., 85 for 62.5 msec and 333.3 segment
lengths, respectively.
Fig. 1.5 shows one EEG trial (the Ô¨Årst trial for the control subject) and its estimates
obtained usingthe signalmodeling (sinusoids with M = 3) appliedto eachsegmented
portion of the trial. Three cases are shown: (a) one trial, one segment; (b) one trial,
three segments (segment size = 333.3 msec); and (c) one trial, 16 segments (segment
size = 62.5 msec). From this Ô¨Ågure, we observe that the smaller segmentation size,
the more closely the sinusoidal model follows the segmented signals. This is also
reÔ¨Çected in the MAE of estimation where we see that the MAE is lower for smaller
segmentation size‚ÄîMAE is 1.194, 1.998, and 2.302 for segments of size 62.5 msec,
333.3 msec, and 1 sec, respectively. This is not surprising though, because one
purpose of signal segmentation was to bring the Ô¨Çexibility of the feature extraction
closer to the signal variability.
1.3.5 Training
In Section 1.3.4, we observed that for a smaller segmentation size, extracted features
can more closely represent the observed signals. This observation may lead to the
conclusion that a smaller signal segment would be better than a larger one; however, as
we will see, this is not necessarily the case. This state of aÔ¨Äairs can be attributed in part
to: 1) breaking dependence structure among sequentially collected data points that
occurs due to the segmentation process could potentially hinder its beneÔ¨Åcial impact;
and 2) small segments may contain too little information about the outcome (output
space). To examine the eÔ¨Äect of segmentation size on the classiÔ¨Åer performance, in
this section we train a classiÔ¨Åer for each segmentation size, and in the next section
we evaluate their predictive performance. Let us Ô¨Årst summarize and formulate what
we have done so far.
Recall that each trial xi was initially a vector of size of p = 256 and we
denoted all collected training data as S = {(x1, y1), . . ., (xn, yn)} where n =
20 (2 subjects √ó 10 trials). Using signal segmentation, we segment each xi to c
non-overlapping smaller segments denoted xij where each xij is a vector of size
r = p/c, i = 1, . . ., n, j = 1, . . ., c, xij[l] = xi[(j ‚àí1)r + l], l = 1, . . ., r, and
for simplicity we assume c divides p. This means that our training data is now
Sc = {(x11, y11), (x12, y12), . . ., (xnc, ync)} where yij = yi, ‚àÄj, and where index
c = 1, 3, 16 is used in notation Sc to reÔ¨Çect dependency of training data to the
segmentation size. In our experiments, we assume c = 1 (no segmentation), c = 3
(segments of size 333.3 msec), and c = 16 (segments of size 62.5 msec). Sup-
pose that now we perform signal modeling (feature extraction) over each signal
segment xij and assume M = 3 for all values of c. This means that each xij is
replaced with a feature vector Œ∏ij of size 3M. In other words, the training data,

16
1 Introduction
which is used to train the classiÔ¨Åer, after segmentation and feature extraction is
SŒ∏
c = {(Œ∏11, y11), (Œ∏12, y12), . . ., (Œ∏nc, ync)}.
We use SŒ∏
1, SŒ∏
3, SŒ∏
16 to train three classiÔ¨Åers of non-alcoholic vs. alcoholic. Re-
gardless of the speciÔ¨Åc choice of the classiÔ¨Åer, when classiÔ¨Åcation of a given signal
segment x of length r is desired, we Ô¨Årst need to extract similar set of 3M features
from x, and then use the classiÔ¨Åer to classify the extracted feature vector Œ∏ ‚ààR3M. As
for the classiÔ¨Åer choice, we select the standard 3NN (short for 3 nearest neighbors)
classiÔ¨Åer. In particular, given an observation Œ∏ to classify, 3NN classiÔ¨Åer Ô¨Ånds the
three Œ∏ij from SŒ∏
c that have lowest Euclidean distance to Œ∏; that is to say, it Ô¨Ånds the 3
nearest neighbors of Œ∏. Then it assigns Œ∏ to the majority class among these 3 nearest
neighbors (more details in Chapter 5). As a result, we train three 3NN classiÔ¨Åers,
one for each training data.
1.3.6 Evaluation
The worth of a predictive model rests with its ability to predict. As a result, a
key issue in the design process is measuring the predictive capacity of trained
predictive models. This process, which is generally referred to as model evaluation
or error estimation, can indeed permeate the entire design process. This is because
depending on whether we achieve an acceptable level of performance or not, we may
need to make adjustments in any stages of the design process or even in the data
collection stage. There are diÔ¨Äerent ways to measure the predictive performance of
a trained predictive model. In the context of classiÔ¨Åcation, the accuracy estimate
(simply referred to as the accuracy) is perhaps the most intuitive way to quantify the
predictive performance. To estimate the accuracy, one way is to use a test data; that
is, to use a data, which similar to the training data, the class of each observation is
known, but in contrast with the training data, it has not been used anyhow in training
the classiÔ¨Åer. The classiÔ¨Åer is then used to classify all observations in the test data.
The accuracy estimate is then the proportion of correctly classiÔ¨Åed observations in
the test data.
In our application, the EEG ‚ÄúSmall Dataset‚Äù already contains a test data for each
subject. Each subject-speciÔ¨Åc test data includes 10 trials with similar speciÔ¨Åcations
stated in Section 1.3.1 for the training data. Fig. 1.6 shows the test data that includes
10 EEG recordings over C1 channel for the two subjects. We use similar segmentation
and feature extraction that were used to create SŒ∏
1, SŒ∏
3, SŒ∏
16 to transform the test data
into a similar form denoted SŒ∏,te
1
, SŒ∏,te
3
, SŒ∏,te
16 , respectively. Let œàc(Œ∏), c = 1, 3, 16
denote the (3NN) classiÔ¨Åer trained in the previous section using SŒ∏
c to classify a Œ∏,
which is obtained from a given signal segment x. We should naturally evaluate œàc(Œ∏)
on its compatible test data, which is SŒ∏,te
c . The result of this evaluation shows that
œà1(Œ∏), œà3(Œ∏), and œà16(Œ∏) have an accuracy of 60.0%, 66.7%, and 57.5%, respectively.
This observation shows that for this speciÔ¨Åc dataset, a 3NN classiÔ¨Åer trained with a
segmentation size of 333.3 msec achieved a higher accuracy than the same classiÔ¨Åer
trained when segmentation size is 1000 or 62.5 msec.

1.4 Python in Machine Learning and Throughout This Book
17
So far the duration of decision-making is the same as the segmentation size. For
example, a classiÔ¨Åer trained using SŒ∏
3, labels a given signal of 333.3 msec duration
as control or alcoholic. However, if desired we may extend the duration of our
decision-making to the trial duration (1000 msec) by combining decisions made by
the classiÔ¨Åer over three consecutive signal segments of 333.3 msec. In this regard, a
straightforward approach is to take the majority vote among decisions made by the
classiÔ¨Åer over the three signal segments that make up a single trial. In general, this
ensemble classiÔ¨Åer, denoted œàE,c(Œ∏), is given by
œàE,c(Œ∏) = arg max
y‚àà{0,1}
c
√ï
j=1
I{œàc(Œ∏ j)=y} ,
(1.3)
where Œ∏ ‚âú[Œ∏T
1, . . ., Œ∏T
c ]T, Œ∏ j is the extracted feature vector from the jth (non-
overlapping) segment of length r from a given trial x of length p = 256, and I{S}
is 1 if statement S is true, zero otherwise. Note that œàE,c(Œ∏) reduces to œàc(Œ∏ j) for
c = 1. In our case, for both c = 3 and c = 16, œàE,c(Œ∏) has an accuracy of 70%,
which is better than the accuracies of both œà3(Œ∏ j) and œà16(Œ∏ j), which were 66.7%,
and 57.5%, respectively.
1.4 Python in Machine Learning and Throughout This Book
Simplicity and readability of Python programming language along with a large
set of third-party packages developed by many engineers and scientists over years
have provided an ecosystem that serves as niche for machine learning. The most
recent survey conducted by Kaggle in 2021 on ‚ÄúState of Data Science and Machine
Learning‚Äù with about 26,000 responders concluded that (Kaggle-survey, 2021)
Python-based tools continue to dominate the machine learning frameworks.
Table 1.1 shows the result of this survey on estimated usage of the top four
machine learning frameworks. All these production-ready software are either written
primarily in Python (e.g., Scikit-Learn and Keras) or have a Python API (TensorFlow
and xgboost). The wide usage and popularity of Python has made it the lingua franca
among many data scientists (M√ºller and Guido, 2017).
Table 1.1: Machine learning framework usage
Software
Usage
Scikit-Learn 82.3%
TensorFlow 52.6%
xgboost
47.9%
Keras
47.3%

18
1 Introduction
0
50
100
150
200
250
time point
an EEG and its estimate using sinusoidal modeling, MAE = 2.302
(a)
0
50
100
150
200
250
time point
an EEG and its estimate using sinusoidal modeling, MAE = 1.998
(b)
0
50
100
150
200
250
time point
an EEG and its estimate using sinusoidal modeling, MAE = 1.194
(c)
Fig. 1.5: Modeling the Ô¨Årst EEG trial for the control subject (the signal at the bottom
of Fig. 1.3a) for diÔ¨Äerent segmentation size. The solid line in each Ô¨Ågure is the
original signal collected in the trial. Vertical dashed lines show segmented signals.
The (red) dashed lines show the estimate of each segmented signal obtained using
the sinusoidal model with M = 3. Depending on the segmentation size, three cases
are considered: (a) one trial, one segment; (b) one trial, three segments (segment
size = 333.3 msec); and (c) one trial, 16 segments (segment size = 62.5 msec).
Given such an important role of Python in machine learning, in this book we
complement the theoretical presentation of each concept and technique by their
Python implementation. In this regard, scikit-learn is the main software used in
Chapters 4-12, xgboost is introduced in Chapter 8, and Keras-TensorÔ¨Çow will be
used from Chapter 13 onward. At the same time, in order to make the book self-
suÔ¨Écient, Chapters 2 and 3 are devoted to introduce necessary Python programming
skills that are needed in other chapters. That being said, we assume readers are
familiar with basic concepts in (object oriented) programming; for example, they
should know the purpose of loops, conditional statements, functions, a class, object,

1.4 Python in Machine Learning and Throughout This Book
19
0
50
100
150
200
250
time point
10 EEG Trials (Œº V)
control
(a)
0
50
100
150
200
250
time point
10 EEG Trials (Œº V)
alcoholic
(b)
Fig. 1.6: Test data: the EEG recordings of C1 channel for all 10 trials for (a) control
subject; and (b) alcoholic subject.

20
1 Introduction
method, and attributes. Then, in Chapter 2, readers will start from the basics of Python
programming and work their way up to more advanced topics in Python. Chapter 3
will cover three important Python packages that are very useful for data wrangling
and analysis: numpy, pandas, and matplotlib. Nevertheless, Chapters 2 and 3 can
be entirely skipped if readers are already familiar with Python programming and
these three packages.

Chapter 2
Getting Started with Python
Simplicity and readability along with a large number of third-party packages have
made Python the ‚Äúgo-to‚Äù programming language for machine learning. Given such
an important role of Python for machine learning, this chapter provides a quick
introduction to the language. The introduction is mainly geared toward those who
have no knowledge of Python programming but are familiar with programming
principles in another (object oriented) language; for example, we assume readers are
familiar with the purpose of binary and unary operators, repetitive and conditional
statements, functions, a class (a recipe for creating an object), object (instance of a
class), method (the action that the object can perform), and attributes (properties of
class or objects).
2.1 First Things First: Installing What Is Needed
Install Anaconda! Anaconda is a data science platform that comes with many things
that are required throughout the book. It comes with a Python distribution, a package
manager known as conda, and many popular data science packages and libraries such
as NumPy, pandas, SciPy, scikit-learn, and Jupyter Notebook. This way we don‚Äôt
need to install them separately and manually (for example, using ‚Äúconda‚Äù or ‚Äúpip‚Äù
commands). To install Anaconda, refer to (Anaconda, 2023). There are also many
tutorials and videos online that can help installing that.
Assuming ‚Äúconda‚Äù is installed, we can also install packages that we
need one by one. It is also recommended to create an ‚Äúenvironment‚Äù and
install all packages and their dependencies in that environment. This way we
can work with diÔ¨Äerent versions of the same packages (and their dependen-
cies) across diÔ¨Äerent environments. Next we create an environment called
‚Äúmyenv‚Äù with a speciÔ¨Åc version of Python (open a terminal and execute the
21
¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
A. Zollanvari, Machine Learning with Python, https://doi.org/10.1007/978-3-031-33342-2_2 

22
2 Getting Started with Python
following command):
conda create ‚Äìname myenv python=3.10.9
Executing this command will download and install the desired package (here
Python version 3.10.9). To work with the environment, we should Ô¨Årst acti-
vate it as follows:
conda activate myenv
Then we can install other packages that we need throughout the book:
‚Ä¢ install ‚ÄútensorÔ¨Çow‚Äù (required for Chapters 13-15):
conda install tensorflow
This will also install a number of other packages (e.g., numpy and
scipy) that are required for tensorÔ¨Çow to work (i.e., its dependencies).
‚Ä¢ install ‚Äúscikit-learn‚Äù (required for Chapters 4-12):
conda install scikit-learn
‚Ä¢ install ‚Äúmatplotlib‚Äù (introduced in Chapter 3):
conda install matplotlib
‚Ä¢ install ‚Äúseaborn‚Äù (introduced in Chapter 3):
conda install seaborn
‚Ä¢ install ‚Äúpandas‚Äù (introduced in Chapter 3):
conda install pandas
‚Ä¢ install ‚Äúxgboost‚Äù (introduced in Chapter 8):
conda install xgboost
‚Ä¢ install ‚Äújupyter‚Äù (introduced in Chapter 2):
conda install jupyter
Once the environment is active, we can launch ‚Äújupyter notebook‚Äù (see
Section 2.2) as follows:
jupyter notebook
We can check the list of all packages installed in the current environment by
conda list

2.2 Jupyter Notebook
23
To deactivate an environment, we use
conda deactivate
This returns us back to the ‚Äúbase‚Äù environment. Last but not least, the list of
all environments can be checked by:
conda env list
2.2 Jupyter Notebook
Python is a programming language that is interpreted rather than compiled; that is,
we can run codes line-by-line. Although there are several IDEs (short for, integrated
development environment) that can be used to write and run Python codes, the IDE
of choice in this book is Jupter notebook. Using Jupyter notebook allows us to write
and run codes and, at the same time, combine them with text and graphics. In fact
all materials for this book are developed in Jupyter notebooks. Once Anaconda is
installed, we can launch Jupyter notebook either from Anaconda Navigator panel or
from terminal.
In a Jupyter notebook, everything is part of cells. Code and markdown (text) cells
are the most common cells. For example, the text we are now reading is written in a
text cell in a Jupyter notebook. The following cell is a code cell and will be presented
in gray boxes throughout the book. To run the contents of a code cell, we can click
on it and press ‚Äúshift + enter‚Äù. Try it on the following cell and observe a 7 as the
output:
2 + 5
7
‚Ä¢ First we notice that when the above cell is selected in a Jupyter notebook, a
rectangle with a green bar on the left appears. This means that the cell is in
‚Äúedit‚Äù mode so we can write codes in that cell (or simply edit if it is a Markdown
cell).
‚Ä¢ Now if ‚Äúesc‚Äù key (or ‚Äúcontrol + M‚Äù) is pressed, this bar turns blue. This means
that the cell is now in ‚Äúcommand‚Äù mode so we can edit the notebook as a whole
without typing in any individual cell.
‚Ä¢ Also depending on the mode, diÔ¨Äerent shortcuts are available. To see the list of
shortcuts for diÔ¨Äerent modes, we can press ‚ÄúH‚Äù key when the cell is in command
mode; for example, some useful shortcuts in this list for command mode are:
‚ÄúA‚Äù key for adding a cell above the current cell; ‚ÄúB‚Äù key for adding a cell below
the current cell; and pressing ‚ÄúD‚Äù two times to delete the current cell.

24
2 Getting Started with Python
2.3 Variables
Consider the following code snippet:
x = 2.2
# this is a comment (use "#'' for comments)
y = 2
x * y
4.4
In the above example, we created two scalar variables x and y, assigned them to
some values, and multiplied. There are diÔ¨Äerent types of scalar variables in Python
that we can use:
x = 1
# an integer
x = 0.3
# a floating-point
x = 'what a nice day!' # a string
x = True
# a boolean variable (True or False)
x = None
# None type (the absence of any value)
In the above cell, a single variable x refers to an integer, a Ô¨Çoating-point, string,
etc. This is why Python is known as being dynamically-typed; that is, we do not
need to declare variables like what is done in C (no need to specify their types) or
even they do not need to always refer to the objects of the same type‚Äîand in Python
everything is an object! This is itself a consequence of the fact that Python variables
are references! In Python, an assignment statement such as x = 1, creates a reference
x to a memory location storing object 1 (yes, even numbers are objects!). At the
same, types are attached to the objects on the right, not to the variable name on the
left. This is the reason that x could refer to all those values in the above code snippet.
We can see the type of a variable (the type of what is referring to) by type() method.
For example,
x = 3.3
display(type(x))
x = True
display(type(x))
x = None
display(type(x))
float
bool
NoneType
Here we look at an attribute of an integer to show that in Python even numbers are
objects:

2.4 Strings
25
(4).imag
0
The imag attribute extracts the imaginary part of a number in the complex domain
representation. The use of parentheses though is needed here because otherwise, there
will be a confusion with Ô¨Çoating points.
The multiple use of display() in the above code snippet is to display
multiple desired outputs in the cell. Remove the display() methods (just
keep all type(x)) and observe that only the last output is seen. Rather
than using display() method, we can use print() function. Replace
display() with print() and observe the diÔ¨Äerence.
2.4 Strings
A string is a sequence of characters. We can use quotes (‚Äò ‚Äô) or double quotes (‚Äú ‚Äù)
to create strings. This allows using apostrophes or quotes within a string. Here are
some examples:
string1 = 'This is a string'
print(string1)
string2 = "Well, this is 'string' too!"
print(string2)
string3 = 'Johnny said: "How are you?"'
print(string3)
This is a string
Well, this is 'string' too!
Johnny said: "How are you?"
Concatenating strings can be done using (+) operator:
string3 = string1 + ". " + string2
print(string3)
This is a string. Well, this is 'string' too!
We can use \t and \n to add tab and newline characters to a string, respectively:
print("Here we use a newline\nto go to the next\t line")
Here we use a newline
to go to the next
line

26
2 Getting Started with Python
2.5 Some Important Operators
2.5.1 Arithmetic Operators
The following expressions include arithmetic operators in Python:
x = 5
y = 2
print(x + y)
# addition
print(x - y)
# subtraction
print(x * y)
# multiplication
print(x / y)
# dividion
print(x // y)
# floor division (removing fractional parts)
print(x % y)
# modulus (integer remanider of division)
print(x ** y)
# x to the power of y
7
3
10
2.5
2
1
25
If in the example above, we change x = 5 to x = 5.1, then we observe some
small rounding errors. Such unavoidable errors are due to internal representation of
Ô¨Çoating-point numbers.
2.5.2 Relational and Logical Operators
The following expressions include relational and logical operators in Python:
print(x < 5)
# less than (> is greater than)
print(x <= 5)
# less than or equal to (>= is greater than or equal to)
print(x == 5)
# equal to
print(x != 5)
# not equal to
print((x > 4) and (y < 3)) # "and" keyword is used for logical and (it‚ê£
,‚Üíis highlighted to be distinguished from other texts)
print((x < 4) or (y > 3))
# "or" keyword is used for logical or
print(not (x > 4))
# "not" keyword is used for logical not
False
True

2.6 Built-in Data Structures
27
True
False
True
False
False
2.5.3 Membership Operators
Membership operator is used to check whether an element is present within a col-
lection of data items. By collection, we refer to various (ordered or unordered) data
structures such as string, lists, sets, tuples, and dictionaries, which are discussed in
Section 2.6. Below are some examples:
print('Hello' in 'HelloWorlds!') # 'HelloWorlds!' is a string and 'Hello'‚ê£
,‚Üíis part of that
print('Hello' in 'HellOWorlds!')
print(320 in ['Hi', 320, False, 'Hello'])
True
False
True
2.6 Built-in Data Structures
Python has a number of built-in data structures that are used to store multiple data
items as separate entries.
2.6.1 Lists
Perhaps the most basic collection is a list. A list is used to store a sequence of objects
(so it is ordered). It is created by a sequence of comma-separated objects within [ ]:
x = [5, 3.0, 10, 200.2]
x[0]
# the index starts from 0
5
Lists can contain objects of any data types:

28
2 Getting Started with Python
x = ['JupytherNB', 75, None, True, [34, False], 2, 75] # observe that‚ê£
,‚Üíthis list contains another list
x[4]
[34, False]
In addition, lists are mutable, which means they can be modiÔ¨Åed after they are
created. For example,
x[4] = 52 # here we change one element of list x
x
['JupytherNB', 75, None, True, 52, 2, 75]
We can use a number of functions and methods with a list:
len(x) # here we use a built-in function to return the length of a list
7
y = [9, 0, 4, 2]
print(x + y) # to concatenate two lists, + operator is used
print(y * 3) # to concatenate multiple copies of the same list, *‚ê£
,‚Üíoperator is used
['JupytherNB', 75, None, True, 52, 2, 75, 9, 0, 4, 2]
[9, 0, 4, 2, 9, 0, 4, 2, 9, 0, 4, 2]
z = [y, x] # to nest lists to create another list
z
[[9, 0, 4, 2], ['JupytherNB', 75, None, True, 52, 2, 75]]
Accessing the elements of a list by indexing and slicing: We can use indexing to
access an element within a list (we already used it before):
x[3]
True
To access the elements of nested lists (list of lists), we need to separate indices
with square brackets:
z[1][0] # this way we access the second element within z and within‚ê£
,‚Üíthat we access the first element
'JupytherNB'
A negative index has a meaning:

2.6 Built-in Data Structures
29
x[-1] # index -1 returns the last item in the list; -2 returns the‚ê£
,‚Üísecond item from the end, and so forth
75
x[-2]
2
Slicing is used to access multiple elements in the form of a sub-list. For this
purpose, we use a colon to specify the start point (inclusive) and end point (non-
inclusive) of the sub-list. For example:
x[0:4] # the last element seen in the output is at index 3
['JupytherNB', 75, None, True]
In this format, if we don‚Äôt specify a starting index, Python starts from the beginning
of the list:
x[:4] # equivalent to x[0:4]
['JupytherNB', 75, None, True]
Similarly, if we don‚Äôt specify an ending index, the slicing includes the end of the list:
x[4:]
[52, 2, 75]
Negative indices can also be used in slicing:
print(x)
x[-2:]
['JupytherNB', 75, None, True, 52, 2, 75]
[2, 75]
Another useful type of slicing is using [start:stop:stride] syntax where the stop
denotes the index at which the slicing stops (so it is not included), and the stride is
just the step size:
x[0:4:2] # steps of 2
['JupytherNB', None]

30
2 Getting Started with Python
x[4::-2] # a negative step returns items in reverse (it works backward‚ê£
,‚Üíso here we start from the element at index 4 and go backward to the‚ê£
,‚Üíbeginning with steps of 2)
[52, None, 'JupytherNB']
In the above example, we start from 4 but because the stride is negative, we go
backward to the beginning of the list with steps of 2. In this format, when the stride
is negative and the ‚Äústop‚Äù is not speciÔ¨Åed, the ‚Äústop‚Äù becomes the beginning of the
list. This behavior would make sense if we observe that to return elements of a list
backward, the stopping point should always be less than the starting point; otherwise,
an empty list would have been returned.
Modifying elements in a list: As we saw earlier, one way to modify existing
elements of a list is to use indexing and assign that particular element to a new value.
However, there are other ways we may want to use to modify a list; for example, to
append a value to a list or to insert an element at a speciÔ¨Åc index. Python has some
methods for these purposes. Here we present some examples to show these methods:
x.append(-23) # to append a value to the end of the list
x
['JupytherNB', 75, None, True, 52, 2, 75, -23]
x.remove(75) # to remove the first matching element
x
['JupytherNB', None, True, 52, 2, 75, -23]
y.sort() # to sort the element of y
y
[0, 2, 4, 9]
x.insert(2, 10) # insert(pos, elmnt) method inserts the specified elmnt‚ê£
,‚Üíat the specified position (pos) and shift the rest to the right
x
['JupytherNB', None, 10, True, 52, 2, 75, -23]
print(x.pop(3)) # pop(pos) method removes (and returns) the element at‚ê£
,‚Üíthe specified position (pos)
x
True

2.6 Built-in Data Structures
31
['JupytherNB', None, 10, 52, 2, 75, -23]
del x[1] # del statement can also be used to delete an element from a‚ê£
,‚Üílist by its index
x
['JupytherNB', 10, 52, 2, 75, -23]
x.pop() # by default the position is -1, which means that it removes‚ê£
,‚Üíthe last element
x
['JupytherNB', 10, 52, 2, 75]
Copying a List: It is often desired to make a copy of a list and work with it without
aÔ¨Äecting the original list. In these cases if we simply use the assignment operator,
we end up changing the original list! Suppose we have the following list:
list1 = ['A+', 'A', 'B', 'C+']
list2 = list1
list2
['A+', 'A', 'B', 'C+']
list2.append('D')
print(list2)
print(list1)
['A+', 'A', 'B', 'C+', 'D']
['A+', 'A', 'B', 'C+', 'D']
As seen in the above example, when ‚ÄòD‚Äô is appended to list2, it is also appended
at the end of list1. One way to understand this behavior is that when we write list2
= list1, in fact what happens internally is that variable list2 will point to the
same container as list1. So if we modify the container using list2, that change
will appear if we access the elements of the container using list1.
There are three simple ways to properly copy the elements of a list: 1) slicing; 2)
copy() method; and 3) the list() constructor. They all create shallow copies of
a list (in contrast with deep copies). Based on Python documentation Python-copy
(2023):
A shallow copy of a compound object such as list creates a new compound object and
then adds references (to the objects found in the original object) into it. A deep copy of a
compound object creates a new compound object and then adds copies of the objects found
in the original object.

32
2 Getting Started with Python
Further implications of these statements are beyond our scope and readers are en-
couraged to see other references (e.g., (Ramalho, 2015, pp. 225-227)). Here we
examine these approaches to create list copies.
list3 = list1[:] # the use of slicing; that is, using [:] we make a‚ê£
,‚Üíshallow copy of the entire list1
list3.append('E')
print(list3)
print(list1)
['A+', 'A', 'B', 'C+', 'D', 'E']
['A+', 'A', 'B', 'C+', 'D']
As we observe in the above example, list1 and list3 are diÔ¨Äerent‚Äîchanging
some elements in the copied list did not change the original list (this is in contrast
with slicing NumPy arrays that we will see in Chapter 3). Next, we examine the copy
method:
list4 = list1.copy() # the use of copy() method
list4.append('E')
print(list4)
print(list1)
['A+', 'A', 'B', 'C+', 'D', 'E']
['A+', 'A', 'B', 'C+', 'D']
And Ô¨Ånally, we use the list() constructor:
list5 = list(list1) #the use of list() constructor
list5.append('E')
print(list5)
print(list1)
['A+', 'A', 'B', 'C+', 'D', 'E']
['A+', 'A', 'B', 'C+', 'D']
help() is a useful function in Python that shows the list of attributes/methods
deÔ¨Åned for an object. For example, we can see all methods applicable to list1 (for
brevity part of the output is omitted):
help(list1)
Help on list object:
class list(object)
|
list(iterable=(), /)
|

2.6 Built-in Data Structures
33
|
Built-in mutable sequence.
|
|
If no argument is given, the constructor creates a new empty list.
|
The argument must be an iterable if specified.
|
|
Methods defined here:
|
|
__add__(self, value, /)
|
Return self+value.
|
|
__contains__(self, key, /)
|
Return key in self.
|
|
__delitem__(self, key, /)
|
Delete self[key].
...
2.6.2 Tuples
Tuple is another data-structure in Python that similar to list can hold other arbitrary
data types. However, the main diÔ¨Äerence between tuples and lists is that a tuple is
immutable; that is, once it is created, its size and contents can not be changed.
A tuple looks like a list except that to create them, we use parentheses ( ) instead
of square brackets [ ]:
tuple1 = ('Machine', 'Learning', 'with', 'Python', '1.0.0')
tuple1
('Machine', 'Learning', 'with', 'Python', '1.0.0')
Once a tuple is created, we can use indexing and slicing just as we did for a list
(using square brackets):
tuple1[0]
'Machine'
tuple1[::2]
('Machine', 'with', '1.0.0')
len(tuple1) # the use of len() to return the length of tuple

34
2 Getting Started with Python
5
An error is raised if we try to change the content of a tuple:
tuple1[0] = 'Jupyter' # Python does not permit changing the value
TypeError
Traceback (most recent call‚ê£
,‚Üílast)
/var/folders/vy/894wbsn11db_lqf17ys9fvdm0000gn/T/ipykernel_51384/
,‚Üí877039090.py in <module>
----> 1 tuple1[0] = 'Jupyter' # Python does not allow us to change the‚ê£
,‚Üívalue
TypeError: 'tuple' object does not support item assignment
Because we can not change the contents of tuples, there is no append or remove
method for tuples. Although we can not change the contents of a tuple, we could
redeÔ¨Åne our entire tuple (assign a new value to the variable that holds the tuple):
tuple1 = ('Jupyter', 'NoteBook') # redefine tuple1
tuple1
('Jupyter', 'NoteBook')
Or if desired, we can concatenate them to create new tuples:
tuple2 = tuple1 + ('Good', 'Morning')
tuple2
('Jupyter', 'NoteBook', 'Good', 'Morning')
A common use of tuples is in functions that return multiple values. For example,
modf() function from math module (more on functions and modules later), returns
a two-item tuple including the fractional part and the integer part of its input:
from math import modf
# more on "import" later. For now just read‚ê£
,‚Üíthis as "from math module, import modf function" so that modf‚ê£
,‚Üífunction is available in our program
a = 56.5
modf(a) # the function is returning a two-element tuple
(0.5, 56.0)
We can assign these two return values to two variables as follows (this is called
sequence unpacking and is not limited to tuples):

2.6 Built-in Data Structures
35
x, y = modf(a)
print("x = " + str(x) + "\n" + "y = " + str(y))
x = 0.5
y = 56.0
Now that we discussed sequence unpacking, let us examine what sequence packing
is. In Python, a sequence of comma separated objects without parentheses is packed
into a tuple. This means that another way to create the above tuple is:
tuple1 = 'Machine', 'Learning', 'with', 'Python', '1.0.0' # sequence‚ê£
,‚Üípacking
tuple1
('Machine', 'Learning', 'with', 'Python', '1.0.0')
x, y, z, v, w = tuple1 # the use of sequence unpacking
print(x, y, z, v, w)
Machine Learning with Python 1.0.0
Note that in the above example, we Ô¨Årst packed 'Machine', 'Learning', 'with',
'Python', '1.0.0' into tuple1 and then unpacked into x, y, z, v, w. Python allows
to do this in one step as follows (also known as multiple assignment, which is really
a combination of sequence packing and unpacking):
x, y, z, v, w = 'Machine', 'Learning', 'with', 'Python', '1.0.0'
print(x, y, z, v, w)
Machine Learning with Python 1.0.0
We can do unpacking with lists if desired:
list6 = ['Machine', 'Learning', 'with', 'Python', '1.0.0']
x, y, z, v, w = list6
print(x, y, z, v, w)
Machine Learning with Python 1.0.0
One last note about sequence packing for tuples: if we want to create a one-element
tuple, the comma is required (why?):
tuple3 = 'Machine', # remove the comma and see what would be the type‚ê£
,‚Üíhere
type(tuple3)
tuple

36
2 Getting Started with Python
2.6.3 Dictionaries
A dictionary is a useful data structure that contains a set of values where each value
is labeled by a unique key (if we duplicate keys, the second value wins). We can
think of a dictionary data type as a real dictionary where the words are keys and
the deÔ¨Ånition of words are values but there is no order among keys or values. As
for the keys, we can use any immutable Python built-in type such as string, integer,
Ô¨Çoat, boolean or even tuple as long the tuple does not include a mutable object. In
technical terms, the keys should be hashable but the details of how a dictionary is
implemented under the hood is out of the scope here.
Dictionaries are created using a collection of key:value pairs wrapped within
curly braces { } and are non-ordered:
dict1 = {1:'value for key 1', 'key for value 2':2, (1,0):True, False:
,‚Üí[100,50], 2.5:'Hello'}
dict1
{1: 'value for key 1',
'key for value 2': 2,
(1, 0): True,
False: [100, 50],
2.5: 'Hello'}
The above example is only for demonstration to show the possibility of using im-
mutable data types for keys; however, keys in dictionary are generally short and more
uniform. The items in a dictionary are accessed using the keys:
dict1['key for value 2']
2
Here we change an element:
dict1['key for value 2'] = 30 # change an element
dict1
{1: 'value for key 1',
'key for value 2': 30,
(1, 0): True,
False: [100, 50],
2.5: 'Hello'}
We can add new items to the dictionary using new keys:
dict1[10] = 'Bye'
dict1

2.6 Built-in Data Structures
37
{1: 'value for key 1',
'key for value 2': 30,
(1, 0): True,
False: [100, 50],
2.5: 'Hello',
10: 'Bye'}
del statement can be used to remove a key:pair from a dictionary:
del dict1['key for value 2']
dict1
{1: 'value for key 1',
(1, 0): True,
False: [100, 50],
2.5: 'Hello',
10: 'Bye'}
As we said, a key can not be a mutable object such as list:
dict1[['1','(1,0)']] = 100 # list is not allowed as the key
TypeError
Traceback (most recent call‚ê£
,‚Üílast)
/var/folders/vy/894wbsn11db_lqf17ys9fvdm0000gn/T/ipykernel_71178/
,‚Üí1077749807.py in <module>
----> 1 dict1[['1','(1,0)']] = 100 # list is not allowed as the key
TypeError: unhashable type: 'list'
The non-ordered nature of dictionaries allows fast access to its elements regardless
of its size. However, this comes at the expense of signiÔ¨Åcant memory overhead
(because internally uses an additional sprase hash tables). Therefore, we should
think of dictionaries as a trade oÔ¨Äbetween memory and time: as long as they Ô¨Åt in
the memory, they provide fast access to their elements.
In order to check the membership among keys, we can use the keys() method to
return a dict_keys object (it provides a view of all keys) and check the membership:
(1,0) in dict1.keys()
True
This could also be done with the name of the dictionary (the default behaviour is to
check the keys, not the values):
(1,0) in dict1 # equivalent to: in dict1.keys()

38
2 Getting Started with Python
True
In order to check the membership among values, we can use the values()
method to return a dict_values object (it provides a view of all values) and check
the membership:
"Hello" in dict1.values()
True
Another common way to create a dictionary is to use the dict() constructor.
It works with any iterable object (as long each element is iterable itself with two
objects), or even with comma separated key=object pairs. Here is an example in
which we have a list (an iterable) where each element is a tuple with two objects:
dict2 = dict([('Police', 102), ('Fire', 101), ('Gas', 104)])
dict2
{'Police': 102, 'Fire': 101, 'Gas': 104}
Here is another example in which we have pairs of comma-separated key=object:
dict3 = dict(Country='USA', phone_numbers=dict2, population_million=18.
,‚Üí7) # the use of keywords arguments = object
dict3
{'Country': 'USA',
'phone_numbers': {'Police': 102, 'Fire': 101, 'Gas': 104},
'population_million': 18.7}
2.6.4 Sets
Sets are collection of non-ordered unique and immutable objects. They can be deÔ¨Åned
similarly to lists and tuples but using curly braces. Similar to mathematical set
operations, they support union, intersection, diÔ¨Äerence, and symmetric diÔ¨Äerence.
Here we deÔ¨Åne two sets, namely, set1 and set2 using which we examine set
operations:
set1 = {'a', 'b', 'c', 'd', 'e'}
set1
{'a', 'b', 'c', 'd', 'e'}
set2 = {'b', 'b', 'c', 'f', 'g'}
set2 # observe that the duplicate entery is removed

2.6 Built-in Data Structures
39
{'b', 'c', 'f', 'g'}
set1 | set2 # union using an operator. Equivalently, this could be done‚ê£
,‚Üíby set1.union(set2)
{'a', 'b', 'c', 'd', 'e', 'f', 'g'}
set1 & set2 # intersection using an operator. Equivalently, this could‚ê£
,‚Üíbe done by set1.intersection(set2)
{'b', 'c'}
set1 - set2 # difference: elements of set1 not in set2. Equivalently,‚ê£
,‚Üíthis could be done by set1.difference(set2)
{'a', 'd', 'e'}
set1 ^ set2 # symmetric difference: elements only in one set, not in‚ê£
,‚Üíboth. Equivalently, this could be done by set1.
,‚Üísymmetric_difference(set2)
{'a', 'd', 'e', 'f', 'g'}
'b' in set1 # check membership
True
We can use help() to see a list of all available set operations:
help(set1) # output not shown
2.6.5 Some Remarks on Sequence Unpacking
Consider the following example:
x, *y, v, w = ['Machine', 'Learning', 'with', 'Python', '1.0.0']
print(x, y, v, w)
Machine ['Learning', 'with'] Python 1.0.0
As seen in this example, variable y becomes a list of ‚ÄòLearning‚Äô and ‚Äòwith‚Äô. Note
that the value of other variables are set by their positions. Here * is working as an
operator to implement extended iterable unpacking. We will discuss what an iterable
or iterator is more formally in Section 2.7.1 and Section 2.9.1 but, for the time being,

40
2 Getting Started with Python
it suÔ¨Éces to know that any list or tuple is an iterable object. To better understand
the extended iterable unpacking, we Ô¨Årst need to know how * works as the iterable
unpacking.
We may use * right before an iterable in which case the iterable is expanded into
a sequence of items, which are then included in a second iterable (for example, list
or tuple) when unpacking is performed. Here is an example in which * operates on
an iterable, which is a list, but at the site of unpacking, we create a tuple.
*[1,2,3], 5
(1, 2, 3, 5)
Here is a similar example but this time the ‚Äúsecond‚Äù iterable is a list:
[*[1,2,3], 5]
[1, 2, 3, 5]
We can also create a set:
{*[1,2,3], 5}
{1, 2, 3, 5}
Now consider the following example:
*[1,2,3]
File "/var/folders/vy/894wbsn11db_lqf17ys9fvdm0000gn/T/ipykernel_71178/
,‚Üí386627056.py", line 1
*[1,2,3]
ÀÜ
SyntaxError: can't use starred expression here
The above example raises an error. This is because iterable unpacking can be only
used in certain places. For example, it can be used inside a list, tuple, or set. It can
be also used in list comprehension (discussed in Section 2.7.2) and inside function
deÔ¨Ånitions and calls (more on functions in Section 2.8.1).
A mechanism known as extended iterable unpacking that was added in Python 3
allows using * operator in an assignment expression; for example, we can have state-
ments such as a, b, *c = some-sequence or *a, b, c = some_sequence.
This mechanism makes * as ‚Äúcatch-all‚Äù operator; that is to say, any sequence item
that is not assigned to a variable is assigned to the variable following * . That is
the reason that in the example presented earlier, y becomes a list of ‚ÄòLearning‚Äô and
‚Äòwith‚Äô and the value of other variables are assigned by their position. Furthermore,
even if we change the list on the right to a tuple, y is still a list:

2.7 Flow of Control and Some Python Idioms
41
x, *y, v, w = ('Machine', 'Learning', 'with', 'Python', '1.0.0')
print(x, y, v, w)
Machine ['Learning', 'with'] Python 1.0.0
To create an output as before (i.e., Machine Learning with Python 1.0.0), it
suÔ¨Éces to use again * before y in the print function; that is,
print(x, *y, v, w)
Machine Learning with Python 1.0.0
2.7 Flow of Control and Some Python Idioms
2.7.1 for Loops
for loop statement in Python allows us to loop over any iterable object. What is a
Python iterable object? An iterable is any object capable of returning its members
one at a time, permitting it to be iterated over in a for loop. For example, any
sequence such as list, string, and tuple, or even non-sequential collections such as
sets and dictionaries are iterable objects. To be precise, to loop over some iterable
objects, Python creates a special object known as iterator (more on this in Section
2.9.1); that is to say, when an object is iterable, under the hood Python creates the
iterator object and traverses through the elements. The structure of a for loop is
quite straightforward in Python:
for variable in X:
the body of the loop
In the above representation of the for loop sturcture, X should be either an iterator
or an iterable (because it can be converted to an iterator object). It is important to
notice the indentation. Yes, in Python indentation is meaningful! Basically, code
blocks in Python are identiÔ¨Åed by indentation. At the same time, any statement that
should be followed by an indented code block is followed by a colon : (notice the :
before the body of the loop). For example, to iterate over a list:
for x in list1:
print(x)
A+
A
B
C+
D

42
2 Getting Started with Python
Iterate over a string:
string = "Hi There"
for x in string:
print(x, end = "") # to print on one line one after another
Hi There
Iterate over a dictionary:
dict2 = {1:"machine", 2:"learning", 3:"with python"}
for key in dict2: # looping through keys in a dictionary
val = dict2[key]
print('key =', key)
print('value =', val)
print()
key = 1
value = machine
key = 2
value = learning
key = 3
value = with python
As we mentioned in Section 2.6.3, in the above example, we can replace dict2
with dict2.keys() and still achieve the same result:
dict2 = {1:"machine", 2:"learning", 3:"with python"}
for key in dict2.keys(): # looping through keys in a dictionary
val = dict2[key]
print('key =', key)
print('value =', val)
print()
key = 1
value = machine
key = 2
value = learning
key = 3
value = with python

2.7 Flow of Control and Some Python Idioms
43
When looping through a dictionary, it is also possible to fetch the keys and values
at the same time. For this purpose, we can use the items() method. This method
returns a dict_items object, which provides a view of all (key, value) tuples
in a dictionary. Next we use this method along with sequence unpacking for a more
Pythonic implementation of the above example. A code pattern is generally referred
to as Pythonic if it uses patterns known as idioms, which are in fact some code
conventions acceptable by the Python community (e.g., the sequence packing and
unpacking we saw earlier were some idioms):
for key, val in dict2.items():
print('key =', key)
print('value =', val)
print()
key = 1
value = machine
key = 2
value = learning
key = 3
value = with python
Often we need to iterate over a sequence of numbers. In these cases, it is handy
to use range() constructor that returns a range object, which is an iterable and,
therefore, can be used to create an iterator needed in a loop. These are some common
ways to use range():
for i in range(5): # the sequence from 0 to 4
print('i =', i)
i = 0
i = 1
i = 2
i = 3
i = 4
for i in range(3,8): # the sequence from 3 to 7
print('i =', i)
i = 3
i = 4
i = 5
i = 6
i = 7

44
2 Getting Started with Python
for i in range(3,8,2): # the sequence from 3 to 7 with steps 2
print('i =', i)
i = 3
i = 5
i = 7
When looping through a sequence, it is also possible to fetch the indices and
their corresponding values at the same. The Pythonic way to do this is to use
enumerate(iterable, start=0), which returns an iterator object that provides
the access to indices and their corresponding values in the form of a two-element
tuple of index-value pair:
for i, v in enumerate(list6):
print(i, v)
0 Machine
1 Learning
2 with
3 Python
4 1.0.0
Compare this simple implementation with the following (non-Pythonic) way to do
the same task:
i = 0
for v in list6:
print (i, v)
i += 1
0 Machine
1 Learning
2 with
3 Python
4 1.0.0
Here we start the count from 1:
for i, v in enumerate(list6, start=1):
print(i, v)
1 Machine
2 Learning
3 with
4 Python
5 1.0.0
Here we use enumerate() on a tuple:

2.7 Flow of Control and Some Python Idioms
45
for i, v in enumerate(tuple1):
print(i, v)
0 Machine
1 Learning
2 with
3 Python
4 1.0.0
enumerate() can also be used with sets and dictionaries but we have to remember
that these are non-ordered collections so in general it does not make much sense to
fetch an index unless we have a very speciÔ¨Åc application in mind (e.g., sort some
elements Ô¨Årst and then fetch the index).
Another example of a Python idiom is the use of zip() function, which creates
an iterator that aggregates two or more iterables, and then loops over this iterator.
list_a = [1,2,3,4]
list_b = ['a','b','c','d']
for item in zip(list_a,list_b):
print(item)
(1, 'a')
(2, 'b')
(3, 'c')
(4, 'd')
We mentioned previously that one way to create dictionaries is to use the dict()
constructor, which works with any iterable object as long as each element is iterable
itself with two objects. Assume we have a name_list of three persons, John, James,
Jane. Another list called phone_list contains their numbers that are 979, 797, 897
for John, James, and Jane, respectively. We can now use dict() and zip to create
a dictionary where keys are names and values are numbers:
name_list = ['John', 'James', 'Jane']
phone_list = [979, 797, 897]
dict3 = dict(zip(name_list, phone_list)) # it works because here we use‚ê£
,‚Üízip on two lists; therefore, each element of the iterable has two‚ê£
,‚Üíobjects
dict3
{'John': 979, 'James': 797, 'Jane': 897}
2.7.2 List Comprehension
Once we have an iterable, it is often required to perform three operations:

46
2 Getting Started with Python
‚Ä¢ select some elements that meet some conditions;
‚Ä¢ perform some operations on every element; and
‚Ä¢ perform some operations on some elements that meet some conditions.
Python has an idiomatic way of doing these, which is known as list comprehension
(short form listcomps). The name list comprehension comes from mathematical set
comprehension or abstraction in which a set is deÔ¨Åned based on the properties of its
members. Suppose we would like to create a list containing square of odd numbers
between 1 to 20. A non-Pythonic way to do this is:
list_odd = [] # start from an empty list
for i in range(1, 21):
if i%2 !=0:
list_odd.append(i**2)
list_odd
[1, 9, 25, 49, 81, 121, 169, 225, 289, 361]
List comprehension allows us to combine all this code in one line by combining
the list creation, appending, the for loop, and the condition:
list_odd_lc = [i**2 for i in range(1, 21) if i%2 !=0]
list_odd_lc
[1, 9, 25, 49, 81, 121, 169, 225, 289, 361]
list_odd_lc is created from an expression (i**2) within the square brackets.
The numbers fed into this expression comes from the for and if clause following
the expression (note that there is no ‚Äú:‚Äù (colon) after the for or if statements).
Observe the equivalence of this list comprehension with the above ‚Äúnon-Pythonic‚Äù
way to proprely interpret the list comprehension. The general syntax of applying
listcomps is the following:
[expression for exp_1 in seq_1
if condition_1
for exp_2 in seq_2
if condition_2
...
for exp_n in seq_n
if condition_n]
Here is another example in which we use the list comprehension to generate a list
of two-element tuples of non-equal integers between 0 and 3:
list_non_equal_tuples = [(x, y) for x in range(3) for y in range(3) if‚ê£
,‚Üíx != y]
list_non_equal_tuples

2.8 Function, Module, Package, and Alias
47
[(0, 1),
(0, 2),
(1, 0),
(1, 2),
(2, 0),
(2, 1)]
2.7.3 if-elif-else
Conditional statements can be implemented by if-elif-else statement:
list4 = ["Machine", "Learning", "with", "Python"]
if "java" in list4:
print("There is java too!")
elif "C++" in list4:
print("There is C++ too!")
else:
print("Well, just Python there.")
Well, just Python there.
In these statements, the use of else or elif is optional.
2.8 Function, Module, Package, and Alias
2.8.1 Functions
Functions are simply blocks of code that are named and do a speciÔ¨Åc job. We can
deÔ¨Åne a function in Python using def keyword as follows:
def subtract_three_numbers(num1, num2, num3):
result = num1 - num2 - num3
return result
Inthe above codesnippet,wedeÔ¨Åneafunctionnamed subtract_three_numbers
with three inputs num1, num2, and num3, and then we return the result. We can
use it in our program, for example, as follows:
x = subtract_three_numbers(10, 3.0, 1)
print(x)
6.0

48
2 Getting Started with Python
In the above example, we called the function using positional arguments (also
sometimes simply referred to arguments); that is to say, Python matches the ar-
guments in the function call with the parameters in the function deÔ¨Ånition by the
order of arguments provided (the Ô¨Årst argument with the Ô¨Årst parameter, second
with second, . . . ). However, Python also supports keyword arguments in which the
arguments are passed by the parameter names. In this case, the order of keyword
arguments does not matter as long as they come after any positional arguments (and
note that the deÔ¨Ånition of function remains the same). For example, this is a valid
code:
x = subtract_three_numbers(num3 = 1, num1 = 10, num2 = 3.0)
print(x)
6.0
but subtract_three_numbers(num3 = 1, num2 = 3.0, 10) is not legitimate
because 10 (positional argument) follows keyword arguments.
In Python functions, we can return any data type such as lists, tuples, dictionar-
ies, etc. We can also return multiple values. They are packed into one tuple and
upon returning to the calling environment, we can unpack them if needed:
def string_func(string):
return len(string), string.upper(), string.title()
string_func('coolFunctions') # observe the tuple
(13, 'COOLFUNCTIONS', 'Coolfunctions')
x, y, z = string_func('coolFunctions') # unpacking
print(x, y, z)
13 COOLFUNCTIONS Coolfunctions
If we pass an object to a function and within the function the object is modiÔ¨Åed
(e.g., by calling a method for that object and somehow change the object), the changes
will be permanent (and nothing need to be returned):
def list_mod(inp):
inp.insert(1, 'AB')
list7 = [100, 'ML', 200]
list_mod(list7)
list7 # observe that the changes within the function appears outside‚ê£
,‚Üíthe function
[100, 'AB', 'ML', 200]

2.8 Function, Module, Package, and Alias
49
Sometimes we do not know in advance how many positional or keyword argu-
ments should be passed to the function. In these cases we can use * (or ** ) before a
paramater_name in the function header to make the paramater_name a tuple (or
dictionary) that can store an arbitrary number of positional (or keyword) arguments.
As an example, we deÔ¨Åne a function that receives the amount of money we can
spend for grocery and the name of items we need to buy. The function then prints
the amount of money with a message as well as a capitalized acronym (to remember
items when we go shopping!) made out of items in the grocery list. As we do not
know in advance how many items we need to buy, the function should work with an
arbitrary number of items in the grocery list. For this purpose, parameter accepting
arbitrary number of arguments should appear last in the function deÔ¨Ånition:
def grocery_to_do(money, *grocery_items): #the use of * before‚ê£
,‚Üígrocery_items is to allow an arbitrary number of arguments
acronym = ''
for i in grocery_items:
acronym += i[0].title()
print('You have {}$'.format(money)) # this is another way to write‚ê£
,‚Üí"print('You have ' + str(money) + '$')" using place holder {}
print('Your acronym is', acronym)
grocery_to_do(40, 'milk', 'bread', 'meat', 'tomato')
You have 40$
Your acronym is MBMT
2.8.2 Modules and Packages
As we develop our programs, it is more convenient to put functions into a separate
Ô¨Åle called module and then import them when they are needed. Importing a module
within a code makes the content of the module available in that program. This practice
makes it easier to maintain and share programs. Although here we are associating
modules with deÔ¨Ånition of functions (because we just discussed functions), modules
can also store multiple classes and variables. To create a module, we can put the
deÔ¨Åniton of functions in a Ô¨Åle with extension .py
Suppose we create a Ô¨Åle called module_name.py and add deÔ¨Ånition of functions
into that Ô¨Åle (for example, function_name1, function_name2, . . . ). Now to make
functions available in our programs that are in the same folder as the module (other-
wise, the module should be part of the PYTHONPATH), we can import this entire
module as:
import module_name
If we use this way to load an entire module, then any function within the module
will be available through the program as:

50
2 Getting Started with Python
module_name.function_name()
However, to avoid writing the name of the module each time we want to use the
function, we can ask Python to import a function (or multiple functions) directly.
In this approach, the syntax is:
from module_name import function_name1, function_name2, ...
Let us consider our grocery_to_do function that was deÔ¨Åned earlier. Suppose
we create a module called grocery.py and add this function into that Ô¨Åle. What
if our program grows to the extend that we want to also separate and keep multiple
modules? In that case, we can create a package. As modules are Ô¨Åles containing
functions, packages are folders containing modules. For example, we can create
a folder (package) called mlwp and place our module grocery.py in that folder.
Similar to what was discussed earlier, we have various options to import our pack-
age/module/function:
import mlwp.grocery
mlwp.grocery.grocery_to_do(40, 'milk', 'bread', 'meat', 'tomato')
You have 40$
Your acronym is MBMT
This import makes the mlwp.grocery module available. However, to access the
function, we need to use the full name of this module before the function. Another
way:
from mlwp import grocery
grocery.grocery_to_do(40, 'milk', 'bread', 'meat', 'tomato')
You have 40$
Your acronym is MBMT
This import makes the module grocery available with no need for the package
name. Here we still need to use the name of grocery to access grocery_to_do.
Another way:
from mlwp.grocery import grocery_to_do
grocery_to_do(40, 'milk', 'bread', 'meat', 'tomato')
You have 40$
Your acronym is MBMT
This import makes grocery_to_do function directly available (so no need to use
the package or module preÔ¨Åx).

2.8 Function, Module, Package, and Alias
51
Before Python 3.3, for a folder to be treated as a package it had to have
at least one __init__.py Ô¨Åle that in its simplest form could have been even
an empty Ô¨Åle with this name. This Ô¨Åle was used so that Python treats that
folder as a package. However, this requirement is lifted as of Python 3.3+.
For example, to create the mlwp package discussed above, there is no need
to add such a Ô¨Åle to the folder.
2.8.3 Aliases
Sometimes we give a nickname to a function as soon as we import it and use this
nickname throughout our program. One reason for doing this is convenience! For
example, why should we refer to someone named Edward as Ed? Because it is easier.
Another reason is to prevent conÔ¨Çicts with some other functions with the same name
in our codes. A given nickname is known as alias and we can use the keyword as for
this purpose. For example,
from mlwp.grocery import grocery_to_do as gd
gd(40, 'milk', 'bread', 'meat', 'tomato')
You have 40$
Your acronym is MBMT
If we assign an alias to a function, we can not use the original name anymore. It
is also quite common to assign an alias to a module; for example, here we give an
alias to our grocery module:
from mlwp import grocery as gr
gr.grocery_to_do(40, 'milk', 'bread', 'meat', 'tomato')
You have 40$
Your acronym is MBMT
As a result, rather than typing the entire name of the module before the function
(grocery.grocery_to_do), we simply use gr.grocery_to_do. If desired, we
can assign alias to a package. For example,
import mlwp as mp
mp.grocery.grocery_to_do(40, 'milk', 'bread', 'meat', 'tomato')

52
2 Getting Started with Python
You have 40$
Your acronym is MBMT
2.9
√â
Iterator, Generator Function, and Generator Expression
2.9.1 Iterator
As mentioned before, within a for loop Python creates an iterator object from
an iterable such as list or set. In simple terms, an iterator provides the required
functionality needed by the loop (in general by the iteration protocol). But a few
questions that we need to answer are the following:
‚Ä¢ How can we produce an iterator from an iterable?
‚Ä¢ What is the required functionality in an iteration that the iterator provides?
Creating an iterator from an iterable object is quite straightforward. It can be
done by passing the iterable as the argument of the built-in function iter():
iter(iterable). An iterator object itself represents a stream of data and pro-
vides the access to the next object in this stream. This is doable because this special
object has a speciÔ¨Åc method __next__() that retrieves the next item in this stream
(this is also doable by passing an iterator object to the built-in function next(),
which actually calls the __next__() method). Once there is no more data in the
stream, the __next__() raises StopIteration exception, which means the iterator
is exhausted and no further item is produced by the iterator (and any further call to
__next__() will raise StopIteration exception). Let us examine these concepts
starting with an iterable (here a list):
list_a = ['a', 'b', 'c', 'd']
iter_a = iter(list_a)
iter_a
<list_iterator at 0x7f9c8d42a6a0>
next(iter_a) # here we access the first element by passing the iterator‚ê£
,‚Üíto the next() function for the first time (similar to iter_a.
,‚Üí__next__())
'a'
iter_a.__next__() # here we access the next element using __next__()‚ê£
,‚Üímethod
'b'
next(iter_a)

2.9 √â Iterator, Generator Function, and Generator Expression
53
'c'
next(iter_a)
'd'
Now that the iterator is exhausted, one more call raises StopIteration exception:
next(iter_a)
StopIteration
Traceback (most recent call‚ê£
,‚Üílast)
/var/folders/vy/894wbsn11db_lqf17ys9fvdm0000gn/T/ipykernel_71178/
,‚Üí3255143482.py in <module>
----> 1 next(iter_a)
StopIteration:
Let us examine what was discussed before in Section 2.7.1; that is, the general
structure of a for loop, which is:
for variable in X:
the body of the loop
where X is either an iterator or an iterable. What happens as part of the for loop
is that the iter() is applied to X so that if X is an iterable, an iterator is created.
Then the next() method is applied indeÔ¨Ånitely to the iterator until it is exhausted
in which case the loop ends.
Iterators don‚Äôt need to be Ô¨Ånite. We can have iterators that produce an
inÔ¨Ånite stream of data; for example, see itertools.repeat() at (Python-
repeat, 2023).
2.9.2 Generator Function
Generator functions are special functions in Python that simplify writing custom
iterators. A generator function returns an iterator, which can be used to generate
stream of data on the Ô¨Çy. Let us clarify this statement by few examples.
Example 2.1 In this example, we write a function that can generate the sum of the
Ô¨Årst n integers and then use the return list from this function in another for loop to
do something for each of the elements of this list (here we simply print the element
but of course could be a more complex task):

54
2 Getting Started with Python
def first_n_sum_func(n):
sum_list_num, num = [], 1 # create an empty list to hold values of‚ê£
,‚Üísums
sum_num = sum(sum_list_num)
while num <= n:
sum_num += num
sum_list_num.append(sum_num)
num += 1
return sum_list_num
for i in first_n_sum_func(10):
print(i, end = " ") # end parameter is used to replace the default‚ê£
,‚Üínewline character at the end
1 3 6 10 15 21 28 36 45 55
‚ñ†
In Example 2.1 we created a list within the function to hold the sums, returned
this iterable, and used it in a for loop to iterate over its elements for printing. Note
that if n becomes very large, we need to store all the sums in the memory (i.e., all
the elements of the list). But what if we just need these elements to be used once?
For this purpose, it is much more elegant to use generators. To create a generator,
we can replace return with yield keyword:
def first_n_sum_gen(n):
sum_num , num = 0, 1
while num <= n:
sum_num += num
num += 1
yield sum_num
for i in first_n_sum_gen(10):
print(i, end = " ")
1 3 6 10 15 21 28 36 45 55
To understand how this generator function works, we need to understand several
points:
‚Ä¢ item 1: Any function with one or more yield statement(s) is a generator
function;
‚Ä¢ item 2: Once a generator function is called, the execution does not start and
it only returns a generator object, which is an iterator (therefore, supports the
__next__() methods);

2.9 √â Iterator, Generator Function, and Generator Expression
55
‚Ä¢ item 3: The Ô¨Årst time the __next__() method (equivalently, the next() func-
tion) is called on the generator object, the function will start execution until it
reaches the Ô¨Årst yield statement. At this point the yielded value is returned, all
local variables in the function are stored, and the control is passed to the calling
environment;
‚Ä¢ item 4: Once the __next__() method is called again on the generator, the
function executation resumes where it left oÔ¨Äuntil it reaches the yield again,
and this process continues;
‚Ä¢ item 5: Once the function terminates, StopIteration is raised.
To understand the above examples, we look at another example to examine the eÔ¨Äect
of these points, and then go back to Example 2.1.
If we have an iterable and we would like to print out its elements, one
way is of course what was done before in Example 2.1 to use a for loop.
However, as discussed in Section 2.6.5, an easier approach is to use * as
the iterable unpacking operator in the print function call (and print is a
function that is deÔ¨Åned in a way to handle that):
print(*first_n_sum_gen(10)) # compare with‚ê£
,‚Üíprint(first_n_sum_gen(10))
1 3 6 10 15 21 28 36 45 55
print(*first_n_sum_func(10)) # compare with‚ê£
,‚Üíprint(first_n_sum_func(10))
1 3 6 10 15 21 28 36 45 55
Example 2.2 Here we create the following generator function and call next() func-
tion several times:
def test_gen():
i = 0
print("Part A")
yield i
i += 2
print("Part B")
yield i
i += 2
print("Part C")
yield i

56
2 Getting Started with Python
gen_obj = test_gen() # gen_obj is a generator object
gen_obj
<generator object test_gen at 0x7f9c8d42f190>
Now we call the next() method several times:
next(gen_obj)
Part A
0
next(gen_obj) # observe that the value of i from where the function‚ê£
,‚Üíleft off is preserved
Part B
2
next(gen_obj) # observe that the value of i from where the function‚ê£
,‚Üíleft off is preserved
Part C
4
next(gen_obj) # the iterator is exhausted
StopIteration
Traceback (most recent call‚ê£
,‚Üílast)
/var/folders/vy/894wbsn11db_lqf17ys9fvdm0000gn/T/ipykernel_71178/
,‚Üí3495006520.py in <module>
----> 1 next(gen_obj) # see item 5 above
StopIteration:
‚ñ†
Example 2.1 (continued): Now here we are in the position to understand every-
thing in first_n_sum_gen generator function.
Based on what we mentioned before in Section 2.9.1, in a for loop the
__next__() is applied indeÔ¨Ånitely to the iterator until it is exhausted (i.e., a
StopIteration exception is raised). Now based on the aforementioned item 2,

2.9 √â Iterator, Generator Function, and Generator Expression
57
once the generator function first_n_sum_gen(10) is called, the execution does
not start and it only returns a generator object, which is an iterator. The for loop
applies iter() to create an iterator but applying this function to an iterator returns
the iterator itself. Then applying the __next__() method by the for loop for the
Ô¨Årst time executes this function until it reaches the yield statement (item 3). At this
point, sum_num and num are 1 and 2, respectively, and they are preserved and, at the
same time, the value of the sum_num is returned (i.e., 1) to the caller so i in the for
loop becomes 1, and is printed out. The for loop applies the __next__() method
again (item 4), and the function execution resumes where it left oÔ¨Ä; therefore, both
sum_num and num become 3, and the value of sum_num is returned (i.e., 3), and this
continues. As we can see, in the generator function first_n_sum_gen(10), except
for the local variables in each iteration, we do not need to store a possibly long list
similar to first_n_sum_func. This is what really means by generating data stream
on the Ô¨Çy: we do not need to wait until all values are generated and, therefore, less
memory is used. ‚ñ†
2.9.3 Generator Expression
The same way listcomps are useful for creating simple lists, generator expressions
(known as genexps) are a handy way of creating simple generators. The general
syntax for genexps is similar to listcomps except that the square brackets [ ] are
replaced with parentheses ( ):
(expression for exp_1 in seq_1
if condition_1
for exp_2 in seq_2
if condition_2
...
for exp_n in seq_n
if condition_n)
Naturally we use generator functions to create more complicated generators or to
reuse a generator in diÔ¨Äerent places, while genexps are used to create simpler gen-
erators and those that are not used frequently. Below is the genexp for implementing
first_n_sum_gen(n):
n = 10
gen1 = (sum(range(1, num + 1)) for num in range(1, n + 1))
print(*gen1)
1 3 6 10 15 21 28 36 45 55

58
2 Getting Started with Python
Exercises:
Exercise 1: Suppose we have the following list:
x = [5, 10, 3, 2, 8, 0]
a) what is x[:2:-1]?
b) explain what would be the ‚Äústart‚Äù when it is not speciÔ¨Åed in slicing and the
stride is negative. Why does it make sense?
Exercise 2: Suppose we have the following list:
a = [1, 2, 3, 4]
Which of the following creates a list with elements being the corresponding elements
of ‚Äúa‚Äù added to 1?
A) a + 1
B) a + [1, 1, 1, 1]
C) [i+1 for i in a]
D) [a + 1]
Exercise 3: Suppose we have the following dictionary:
d = {1:"good", 2:"morning", 3:"john"}
a) Write a piece of code that loops through the values of this dictionary and makes
the Ô¨Årst letter in each word upper case (hint: check out string.title() method).
The output should look like this:
value = Good
value = Morning
value = John
b) use the iterable unpacking operator to create the following set:
{'good', 'john', 'morning'}
Exercise 4: Suppose we have the following list:
list_a = [1,2,3,4]
list_b = ['a','b','c','d']
Create a third list, namely, list_c, with elements being ‚Äòa1‚Äô, ‚Äòb1‚Äô, ‚Äòc1‚Äô. Then print
elements of zip(list_a, list_b, list_c). Guess what would be the general

2.9 √â Iterator, Generator Function, and Generator Expression
59
rule when the iterables have diÔ¨Äerent length?
Exercise 5: Suppose we have a list of students‚Äô phone numbers and a tuple of students‚Äô
names:
phone = [979921, 989043, 933043, 932902]
student = ('jane', 'nika', 'kian', 'mira')
In a single line of code, create a dictionary with phones as keys and names as values
(hint: use zip()). Explain how the code works.
Exercise 6: Use the listcomp to create a list with three-tuple elements where each
tuple contains a number between 1 and 10, its square and cubic; that is,
[(0, 0, 0),
(1, 1, 1),
(2, 4, 8),
(3, 9, 27),
(4, 16, 64),
(5, 25, 125),
(6, 36, 216),
(7, 49, 343),
(8, 64, 512),
(9, 81, 729)]
Exercise 7: We have the following dictionary:
dict1 = {'A1': 100, 'B1':20, 'A2':30, 'B2':405, 'A3':23}
Create a list of its values in one line code by:
a) listcomp without using values() method
b) iterator unpacking using values() method
c) list constructor using values() method
Exercise 8: Suppose we have the following list:
list1 = ['book_john', 'pencil_askar', 'pen_amin', 'pen_john',‚ê£
,‚Üí'key_askar',\
'pen_askar', 'laptop_askar', 'backpack_john', 'car_john']
Use the listcomp to Ô¨Ånd john‚Äôs belongings in one line of code. The output should
look like
['book', 'pen', 'backpack', 'car']
This is an important application of list comprehension in which we eÔ¨Éciently apply
a method to a sequence of objects. To do so, use str.split method: check https:
//docs.python.org/3/library/stdtypes.html#str.split.

60
2 Getting Started with Python
Hint: as part of the list comprehension, you can use the membership operator
discussed in Section 2.5.3 to check whether ‚Äòjohn‚Äô is part of each string in the list.
Exercise 9: Write a function named belongings that receives the Ô¨Årst name and
the last name of a student as two arguments along with an arbitrary number of items
that she has in her backpack. The function should print the name and belongings of
the student. For example, here are two calls of this function and their outputs:
belongings('Emma', 'Smith', 'pen', 'laptop')
belongings('Ava', 'Azizi', 'pen', 'charger', 'ipad', 'book')
Name: Emma Smith
pen
laptop
Name: Ava Azizi
pen
charger
ipad
book
Exercise 10: We have following function:
def prod_numbers(x, y, z):
result = x * y * z
return result
Which of the following is a legitimate code? why?
a) prod_numbers(z = 1, y = 3.0, 10)
b) prod_numbers(10, z = 1, y = 3.0)?
c) prod_numbers(10, x = 1, y = 3.0)?
Exercise 11: Suppose there is a package called ‚Äúpackage‚Äù that contains a module
called ‚Äúmodule‚Äù. The ‚Äúmodule‚Äù contains a function called ‚Äúfunction(string)‚Äù that
expects a string. Which of the following is a legitimate way to access and use
function(string)? Choose all that applies.
A)
from package import module
function("hello")
B)
import package.module as pm
pm.function("hello")
C)

2.9 √â Iterator, Generator Function, and Generator Expression
61
from package.module import function as func
function("hello")
D)
from package import module as m
m.function("hello")
E) None of other answers

Chapter 3
Three Fundamental Python Packages
In this chapter, three fundamental Python packages will be introduced. NumPy
(stands for Numerical Python) is a fundamental package at the core of various
Python data science libraries including Scikit-Learn, which will be extensively used
in Chapters 4-12. Another useful library for data analysis is pandas, which is built on
top of NumPy. This package particularly facilitates working with tabular data. Last
but not least is the matplotlib, which is the primary Python plotting library built on
NumPy. When it comes to Python codes, NumPy package is known as numpy but in
what follows, we use NumPy and numpy interchangeably.
3.1 NumPy
3.1.1 Working with NumPy Package
NumPy is a fundamental Python package that provides eÔ¨Écient storage and opera-
tions for multidimensional arrays. In this regard, the core functionality of NumPy
is based on its ndarray data structure (short for n-dimensional array), which is
somewhat similar to built-in list type but with homogeneous elements‚àíin fact the
restriction on having homogeneous elements is the main reason ndarray can be
stored and manipulated eÔ¨Éciently. Hereafter, we refer to ndarray type as NumPy
arrays or simply as arrays.
Working with NumPy arrays is quite straightforward. To do so, we need to Ô¨Årst
import the numpy package. By convention, we use alias np:
import numpy as np
np
<module 'numpy' from '/Users/amin/opt/anaconda3/lib/python3.8/site-
packages/numpy/__init__.py'>
63
¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
A. Zollanvari, Machine Learning with Python, https://doi.org/10.1007/978-3-031-33342-2_3 

64
3 Three Fundamental Python Packages
Importing numpy triggers importing many modules as part of that. This is done
by many imports as part of __init__.py, which is used to make many useful
functions accessible at the package level. For example, to create an identity ma-
trix, which is a commonly used function, NumPy has a convenient function named
eye(). Once NumPy is imported, we have automatically access to this function
through np.eye(). How is this possible? By installing Anacoda, the NumPy pack-
age is available, for example, at a location such as this (readers can check the output
of the above cell to show the path on their systems):
/Users/amin/opt/anaconda3/lib/python3.8
Inside the numpy folder, we have __init__.py Ô¨Åle. Basically once NumPy is
imported, as in any other regular package, __init__.py is automatically executed.
Inside __init__.py, we can see, a line from . import lib‚Äîhere ‚Äúdot‚Äù means
from the current package, and lib is a subpackage (another folder). This way we
import subpackage lib and within that we have another __init__.py. Within this
Ô¨Åle, we have from .twodim_base import *, which means importing everything
from twodim_base module (Ô¨Åle twodim_base.py) in the current package (i.e., lib
subpackage). And Ô¨Ånally within twodim_base.py, we have the function eye()!
To create a numpy array from a Python list or tuple, we can use np.array()
function:
list1 = [[1,2,3], [4,5,6]]
list1
[[1, 2, 3], [4, 5, 6]]
a1 = np.array(list1)
a1
array([[1, 2, 3],
[4, 5, 6]])
We can think of a 2D numpy array such as a1 as a way to store a 2 √ó 3 matrix or
a list containing two lists of 3 elements. Therefore, hereafter when we refer to rows
or columns, it means that we are viewing an array like a matrix. We can check type
of a1:
type(a1)
numpy.ndarray
Here is a similar example but one of the numbers in the nested list is a Ô¨Çoating-point:
list2 = [[1, 2, 3.6], [4, 5, 6]]
list2
[[1, 2, 3.6], [4, 5, 6]]

3.1 NumPy
65
a2 = np.array(list2)
a2
array([[1. , 2. , 3.6],
[4. , 5. , 6. ]])
Inthe above example, allintegersbecameÔ¨Çoatingpointnumbers (doubleprecision
[float64]). This is due to homogeneity restriction of elements in numpy arrays. As
a result of this restriction, numpy upcasts all elements when possible.
3.1.2 NumPy Array Attributes
There are a few attributes of numpy arrays that are quite useful in practice.
‚Ä¢ dtype: we can check the type of the elements of an array with dtype (short for
data type) attribute of the array. For example,
a2.dtype
dtype('float64')
The dtype of an array is in fact what we mean by having homogeneity in array
elements; that is, they should have the same dtype. Sometimes it is desired to specify
the data type. In these cases, we can set the dtype to what is needed. For example,
we may want to use a data type float32 that occupies 32 bits in memory for a
Ô¨Çoating-point, rather than float64, which occupies 64 bits. This can be done as
follows:
a3 = np.array(list2, dtype='float32')
a3
array([[1. , 2. , 3.6],
[4. , 5. , 6. ]], dtype=float32)
Furthermore, when an array is constructed, we can recast its data type using
astype() method:
a4 = a3.astype(int) # this produces a copy of the array while casting‚ê£
,‚Üíthe specified type
a4
array([[1, 2, 3],
[4, 5, 6]])
a3.dtype

66
3 Three Fundamental Python Packages
dtype('float32')
a4.dtype
dtype('int64')
‚Ä¢ size: returns the number of elements in an array:
a4.size # a4 contains 6 elements
6
‚Ä¢ ndim: returns the number of dimensions (also referred to as the number of axes)
of an array:
a4.ndim # a4 is a 2D array
2
‚Ä¢ shape: the number of elements along each dimension:
a4.shape #
think of a4 as a way to store a 2 by 3 matrix
(2, 3)
There is a diÔ¨Äerence between a 1D array and a 2D array with one row or column.
Let us explore this with one example:
a5 = np.arange(5)
a5
array([0, 1, 2, 3, 4])
a5.shape
(5,)
In the above example, a5 is a 1-dimensional array with 5 elements, so we can not
write, for example, a5[3,0] or a5[0,3], but indexing for 1D array is legitimate:
a5[3]
3
However, the following array is a 2D array with 1 row and 5 columns (or perhaps
more clear if we say an array of 1 list with 5 elements):

3.1 NumPy
67
a6 = np.array([[0, 1, 2, 3, 4]])
print(a6.shape)
a6
(1, 5)
array([[0, 1, 2, 3, 4]])
This time writing, for example, a6[3] does not make sense (we can have a6[0,3]
though). Its transpose is like a matrix of 5 √ó 1:
print(a6.T.shape) # taking the transpose of the array using .T and‚ê£
,‚Üílooking into its shape
a6.T # this array has 5 rows and 1 column (or 5 lists each with 1‚ê£
,‚Üíelement)
(5, 1)
array([[0],
[1],
[2],
[3],
[4]])
‚Ä¢ itemsize: number of bytes to store one element of the array:
a4.itemsize # int64 has 64 bits or 8 bytes
8
3.1.3 NumPy Built-in Functions for Array Creation
In addition to array(), NumPy has a series of useful built-in functions to create
arrays. For example, zeros(shape) creates an array of a given shape Ô¨Ålled with
zeros:
a = np.zeros(5)
a
array([0., 0., 0., 0., 0.])

68
3 Three Fundamental Python Packages
a = np.zeros((3,2), dtype='int_') # int_ is the default‚ê£
,‚Üísystem-dependent int type, either int64 or int32
a
array([[0, 0],
[0, 0],
[0, 0]])
Other useful functions:
‚Ä¢ ones(shape) creates an array of a given shape Ô¨Ålled with ones:
a = np.ones((3,5))
a
array([[1., 1., 1., 1., 1.],
[1., 1., 1., 1., 1.],
[1., 1., 1., 1., 1.]])
‚Ä¢ full(shape, value) creates an array of a given shape Ô¨Ålled with value:
a = np.full((2,4), 5.5)
a
array([[5.5, 5.5, 5.5, 5.5],
[5.5, 5.5, 5.5, 5.5]])
‚Ä¢ eye() creates a 2D array with ones on diagonal and zeros elsewhere (identity
matrix):
a = np.eye(5)
a
array([[1., 0., 0., 0., 0.],
[0., 1., 0., 0., 0.],
[0., 0., 1., 0., 0.],
[0., 0., 0., 1., 0.],
[0., 0., 0., 0., 1.]])
‚Ä¢ arange(start, stop, step) creates evenly spaced values between start
and stop with steps step. If start and step are not indicated, they are set to
0 and 1, respectively; for example,

3.1 NumPy
69
a = np.arange(5)
a
array([0, 1, 2, 3, 4])
a = np.arange(3,8)
a
array([3, 4, 5, 6, 7])
‚Ä¢ random.normal(loc, scale, size) creates an array of shape size with
elements being independent realizations of a normal (Gaussian) random variable
with mean loc and standard deviation scale. The default values of loc, scale,
and size are 0, 1, and None, respectively. If both loc and scale are scalars
and size is None, then one single value is returned. Here we create an array of
shape ‚Äú(3, 4)‚Äù where each element is a realization of a standard normal random
variable:
a = np.random.normal(0, 1, size=(3,4))
a
array([[ 0.22755201, -0.20839413, -1.04229339, -0.5626605 ],
[-0.58638538,
0.49326995,
0.72698224, -0.79731512],
[-1.37778091, -0.05112057,
0.28830083,
1.39430344]])
A list of these and other functions are found at NumPy documentation (NumPy-
array, 2023).
3.1.4 Array Indexing
Single Element Indexing: In indexing arrays, it is important to realize that if we
index a multidimensional array with fewer indices than dimensions, we get a subarray.
Suppose we have the following 2D array:
a = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10, 11]])
a
array([[ 0,
1,
2],
[ 3,
4,
5],
[ 6,
7,
8],
[ 9, 10, 11]])
Then,

70
3 Three Fundamental Python Packages
a[1]
array([3, 4, 5])
Therefore, a[1] returns (provides a view of) a 1D array at the second position (index
1). To access the Ô¨Årst element of this subarray, for example, we can treat the subarray
as an array that is subsequently indexed using [0]:
a[1][0]
3
We have already seen this type of indexing notation (separate square brackets)
for list of lists in Section 2.6.1. However, NumPy also supports multidimensional
indexing, in which we do not need to separate each dimension index into its square
brackets. This way we can access elements by comma separated indices within a
pair of square brackets [ , ]. This way of indexing numpy arrays is more eÔ¨Écient as
compared to separate square brackets because it does not return an entire subarray
to be subsequently indexed and, therefore, it is the preferred way of indexing. For
example:
a[1, 0]
3
Slicing: We can access multiple elements using slicing and striding in the same
way we have previously seen for lists and tuples. For example,
a[:3:2,-2:]
array([[1, 2],
[7, 8]])
In the code snippet below we choose columns with a stride of 2 and the last two rows
in reverse:
a[:-3:-1,::2]
array([[ 9, 11],
[ 6,
8]])
One major diÔ¨Äerence between numpy array slicing and list slicing is that array
slicing provides a view of the original array whereas list slicing copies the list. This
means that if a subarray, which comes from slicing an array, is modiÔ¨Åed, the changes
appear in the original array too (recall that in list slicing that was not the case). This
is quite useful when working with large datasets because it means that we can work
and modify pieces of such datasets without loading and working with the entire
dataset at once. To better see this behaviour, consider the following example:

3.1 NumPy
71
b = a[:-3:-1,::2]
b[0,1] = 21 # change 11 in b to 21
a # the change appears in a too
array([[ 0,
1,
2],
[ 3,
4,
5],
[ 6,
7,
8],
[ 9, 10, 21]])
To change this default behaviour, we can use: 1) the copy() method; and 2) the
array() constructor:
b = a[:-3:-1,::2].copy() # using copy()
b
array([[ 9, 21],
[ 6,
8]])
b[1,1] = 18
a # the change does not appear in a
array([[ 0,
1,
2],
[ 3,
4,
5],
[ 6,
7,
8],
[ 9, 10, 21]])
b = np.array(a[:-3:-1,::2]) # using array()
b
array([[ 9, 21],
[ 6,
8]])
b[1,1] = 18
a # the change does not appear in a
array([[ 0,
1,
2],
[ 3,
4,
5],
[ 6,
7,
8],
[ 9, 10, 21]])
Indexing with other Arrays (known as Fancy Indexing): A useful property of
NumPy arrays is that we can use other arrays (or even lists) for indexing:
c = np.arange(10, 30)
idx = np.arange(19, 10, -2)
print('c = ' + str(c) \

72
3 Three Fundamental Python Packages
+ '\n' + 'idx = ' + str(idx)\
+ '\n' + 'c[idx] = ' + str(c[idx]))
c = [10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]
idx = [19 17 15 13 11]
c[idx] = [29 27 25 23 21]
In this type of indexing:
1) indices must be integers;
2) a copy of the original array is returned (not a view);
3) the use of negative indices is allowed and the meaning is similar to what we have
already seen; and
4) out of bound indices are errors.
Let us examine the Ô¨Årst point by changing elements of idx to float32:
idx = idx.astype('float32')
c[idx] # observe that when we change the dtype of idx to float32 an‚ê£
,‚Üíerror is raised
IndexError
Traceback (most recent call‚ê£
,‚Üílast)
/var/folders/vy/894wbsn11db_lqf17ys9fvdm0000gn/T/ipykernel_43151/
,‚Üí419593116.py in <module>
1 idx = idx.astype('float32')
----> 2 c[idx] # observe that we changed the dtype of idx to float32 and‚ê£
,‚Üíwe get error
IndexError: arrays used as indices must be of integer (or boolean) type
And here we examine the eÔ¨Äect of fancy indexing with a negative index:
idx = idx.astype('int_')
idx[0] = -1 # observe how the negative index is interpreted
print('\n' + 'idx = ' + str(idx)\
+ '\n' + 'c[idx] = ' + str(c[idx]))
idx = [-1 17 15 13 11]
c[idx] = [29 27 25 23 21]

3.1 NumPy
73
3.1.5 Reshaping Arrays
One major diÔ¨Äerence between numpy arrays and lists is that the size (number of
elements) of an array once it is created is Ô¨Åxed; that is to say, to shrink and expand an
existing array we need to create a new array and copy the elements from the old one
to the new one, and that is computationally ineÔ¨Écient. However, as we discussed in
Section 2.6.1, expanding or shrinking a list was straightforward through the methods
deÔ¨Åned for lists. Although the size of an array is Ô¨Åxed, its shape is not and could be
changed if desired. The most common way to reshape an array is to use reshape()
method:
a = np.arange(20).reshape(4,5) # 4 elements in the first dimension‚ê£
,‚Üí(axis 0) and 5 in the second dimension (axis 1)
a
array([[ 0,
1,
2,
3,
4],
[ 5,
6,
7,
8,
9],
[10, 11, 12, 13, 14],
[15, 16, 17, 18, 19]])
The reshape() method provides a view of the array in a new shape. Let us examine
this by an example:
b = a.reshape(2,10)
b
array([[ 0,
1,
2,
3,
4,
5,
6,
7,
8,
9],
[10, 11, 12, 13, 14, 15, 16, 17, 18, 19]])
b[0,9] = 29
a # observe that the change in b appears in a
array([[ 0,
1,
2,
3,
4],
[ 5,
6,
7,
8, 29],
[10, 11, 12, 13, 14],
[15, 16, 17, 18, 19]])
To use the reshape() method, we can specify the number of elements in each
dimension and we need to ensure that the total number of these elements is the same
as the size of the original array. However, because the size is Ô¨Åxed, the number of
elements in one axis could be determined from the size and the number of elements
in other axes (as long as they are compatible). NumPy uses this fact and allows us
not to specify the number of elements along one axis. To do so, we can use -1
along an axis and NumPy will determine the number of elements along that axis.
For example,

74
3 Three Fundamental Python Packages
c = b.reshape(5,-1)
c
array([[ 0,
1,
2,
3],
[ 4,
5,
6,
7],
[ 8, 29, 10, 11],
[12, 13, 14, 15],
[16, 17, 18, 19]])
c.reshape(-1) # this returns a flattened array regardless of the‚ê£
,‚Üíoriginal shape
array([ 0,
1,
2,
3,
4,
5,
6,
7,
8, 29, 10, 11, 12, 13, 14, 15,‚ê£
,‚Üí16, 17, 18, 19])
c.ravel() # this could be used instead of reshape(-1)
array([ 0,
1,
2,
3,
4,
5,
6,
7,
8, 29, 10, 11, 12, 13, 14, 15,‚ê£
,‚Üí16, 17, 18, 19])
Occasionally it is required to convert a 1D array to a 2D array with one column
or row. This could be done using 1) reshape(); 2) adding None keyword to a new
axis along with slicing; and 3) using newaxis object as an alias for None:
c = np.arange(5) # to create a 1D array with 5 elements
c
array([0, 1, 2, 3, 4])
c.reshape(-1,1) # a 2D array with 5 rows and 1 column using reshape‚ê£
,‚Üímethod
array([[0],
[1],
[2],
[3],
[4]])
Below we use newaxis object to insert a new axis into column of c:
c[:,np.newaxis] # a 2D array with 5 rows and 1 column
array([[0],
[1],
[2],
[3],
[4]])

3.1 NumPy
75
c[np.newaxis,:] # a 2D array with 1 row and 5 columns, which is‚ê£
,‚Üíequivalent to c.reshape(1,5)
array([[0, 1, 2, 3, 4]])
3.1.6 Universal Functions (UFuncs)
Computations using NumPy arrays could be either slow and verbose or fast and
convenient. The main factor that leads to both fast computations and convenience
of implementation is vectorized operations. These are vectorized wrappers for per-
forming element-wise operation and are generally implemented through NumPy
Universal Functions also known as ufuncs, which are some built-in functions writ-
ten in C for eÔ¨Éciency. In the following example, we will see how the use of ufuncs
will improve computational eÔ¨Éciency.
Here we calculate the time to Ô¨Årst create a numpy array called ‚Äúa‚Äù of size 10000
and then subtract 1 from every element of a. We Ô¨Årst use a conventional way of
doing this, which is a for loop:
%%timeit
a = np.arange(10000)
for i, v in enumerate(a):
a[i] -= 1
3.55 ms ¬± 104 Œºs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)
Now we use the subtraction operator ‚Äú-‚Äù, which implements the vectorized operation
(applies the operation to every element of the array):
%%timeit
a = np.arange(10000)
a = a - 1 # vectorized operation using the subtraction operator
8.8 Œºs ¬± 124 ns per loop (mean ¬± std. dev. of 7 runs, 100000 loops each)
Note the striking diÔ¨Äerence in time! The vectorized operation is about 400 times
faster here. We can also implement this vectorized subtraction using np.subtract
unfunc:
%%timeit
a = np.arange(10000)
a = np.subtract(a,1)
8.88 Œºs ¬± 192 ns per loop (mean ¬± std. dev. of 7 runs, 100000 loops each)

76
3 Three Fundamental Python Packages
In the above code, we used a Python magic function timeit with %%
to determine the execution time of a code block. Python magic functions
simplify some common tasks (here, timing). They come either by a preÔ¨Åx %
(inline magic) in which case they operate on a single line of code or %% (cell
magic) that makes them operate on a block of code.
As a matter of fact, the ‚Äú-‚Äù operator is just a wrapper around np.subtract()
function. NumPy uses operator overloading here to make it possible for us to use
standard Python subtraction operator - for a vectorized operation on arrays. This
also applies to several other operations as listed in Table 3.1.
Table 3.1: Binary ufuncs and their equivalent operators
UFunc
Equivalent Operator
np.add
+
np.subtract
-
np.multiply
*
np.divide
/
np.power
**
np.floor_divide
//
np.mod
%
All the previous unfuncs require two operands. As a result, they are referred to
as binary ufuncs; after all, these are implementations of binary operation. However,
there are other useful operations (and ufuncs), which are unary; that is, they operate
on a single array. For example, operations such as calculating the sum or product of
elements of an entire array or along a speciÔ¨Åc axis, or taking element-wise natural
log, to just name a few. Table 3.2 provides a list of some of these unary ufuncs.
Below, we present some examples of these unary ufuncs:
a = np.array([[3,5,8,2,4],[8, 10, 2, 3, 5]])
a
array([[ 3,
5,
8,
2,
4],
[ 8, 10,
2,
3,
5]])
Then,
np.min(a, axis=0)
array([3, 5, 2, 2, 4])
np.sum(a, axis=1)

3.1 NumPy
77
Table 3.2: Unary ufuncs and their equivalent operators
UFunc
Operation
np.sum
Sum of array elements over a given axis
np.prod
Product of array elements over a given axis
np.mean
Arithmetic mean over a given axis
np.std
Standard deviation over a given axis
np.var
Variance over a given axis
np.log
Element-wise natural log
np.log10
Element-wise base 10 logarithm
np.sqrt
Element-wise square root
np.sort
Sorts an array
np.argsort
Returns the indices that would sort an array
np.min
Returns the minimum value of an array over a given axis
np.argmin Returns the index of the minimum value of an array over a given axis
np.max
Returns the maximum value of an array over a given axis
np.argmax Returns the index of the maximum value of an array over a given axis
.
array([22, 28])
For some of these operations such as sum, min, and max, a simpler syntax is to use
the method of the array object. For example:
a.sum(axis=1) # here sum is the method of the array object
array([22, 28])
For each of these methods, we can check its documentations at numpy.org to see its
full functionality. For example, for np.sum and ndarray.sum, the documentations
are available at (NumPy-sum, 2023) and (NumPy-arrsum, 2023), respectively.
3.1.7 Broadcasting
Broadcasting is a set of rules that instructs NumPy how to treat arrays of diÔ¨Äerent
shapes and dimensions during arithmetic operations. In this regard, under some
conditions, it broadcasts the smaller array over the larger array so that they have
compatible shapes for the operation. There are two main reasons why broadcasting
in general leads to eÔ¨Écient computations: 1) the looping required for vectorized
operations occurs in C instead of Python; and 2) it implements the operation without
copying the elements of array so it is memory eÔ¨Écient.
As a matter of fact, we have already seen broadcasting in the previous section
where we used arithmetic operations on arrays. Let us now see some examples
and examine them from the standpoint of broadcasting. In the following example, a
constant 2 is added to every element of array a:

78
3 Three Fundamental Python Packages
a = np.array([0, 1, 2, 3, 4])
b = a + 2
b
array([2, 3, 4, 5, 6])
One can think of this example as if the scalar 2 is streched (broadcast) to create an
array of the same shape as a so that we can apply the element-by-element addition
(see Fig. 3.1). However, it is important to remember that NumPy neither makes a
copy of 2 nor creates an array of the same shape as a. It just uses the original copy
of 2 (so it is memory eÔ¨Écient) and based on a set of rules eÔ¨Éciently performs the
addition operation as if we have an array of 2 with the same shape as a.
Fig. 3.1: A conceptual illustration of broadcasting for adding a scalar to an array
Broadcasting could be understood by the following rules:
‚Ä¢ Rule 1: If two arrays have diÔ¨Äerent number of dimensions (i.e., ndim is diÔ¨Äerent
for them), dimensions ones are added to the left of the shape of the array with
fewer dimensions to make them both of the same dimensions;
‚Ä¢ Rule 2: Once dimensions are equal, the size of arrays in each dimension must
be either equal or 1 in at least one of them; otherwise, an error will be raised
that these arrays are not broadcastable; and
‚Ä¢ Rule 3: If an error is not raised (i.e., in all dimensions, arrays have either equal
size or at least one of them has size 1), then the size of the output array in each
dimension is the maximum of the array sizes in that dimension (this implies that
the array with size of 1 in any dimension is stretched to match the other one).
In the next few example, we examine these three rules.
Example 3.1 Two arrays, namely, a and b, are created:
a = np.ones((2,4))
print(a)
a.shape
[[1. 1. 1. 1.]
[1. 1. 1. 1.]]
(2, 4)

3.1 NumPy
79
b = np.random.randint(1, 10, 4) # to generate random integers (see‚ê£
,‚Üíhttps://numpy.org/doc/stable/reference/random/generated/numpy.random.
,‚Üírandint.html)
print(b)
b.shape
[2 1 9 6]
(4,)
If we write a + b:
‚Ä¢ By Rule 1, a single 1 is added to the left of b shape to make b a two-dimensional
array as a; that is, we can think of b now as having shape (1, 4).
‚Ä¢ The shape of a is (2, 4) and the shape of b is (1, 4). Therefore, Rule 2 does not
raise an error because the sizes in each dimension are either equal (here 4) or at
least one of them is 1 (for example, 2 in a and 1 in b).
‚Ä¢ Rule 3 indicates that the size of the output is (max{2,1}, max{4,4}) so it is (2,
4). This means that array b is streched along its axis 0 (the left element in a
shape tuple) to match the size of a in that dimension.
Let‚Äôs see the outcome:
c = a + b
print(c)
c.shape
[[ 3.
2. 10.
7.]
[ 3.
2. 10.
7.]]
(2, 4)
Example 3.2 Two arrays, namely, a and b, are created:
a = np.arange(8).reshape(2,4,1) # a 3-dimensional array
print(a)
a.shape
[[[0]
[1]
[2]
[3]]
[[4]
[5]
[6]
[7]]]

80
3 Three Fundamental Python Packages
(2, 4, 1)
b = np.array([10, 20, 30, 40])
If we write a + b:
‚Ä¢ By Rule 1, a 1 is added to the left of b shape to make it a 3-dimensional array
as a; that is, we can think of b now as having shape (1, 1, 4).
‚Ä¢ The shape of a is (2, 4, 1) and the shape of b is (1, 1, 4). Therefore, Rule 2 does
not raise an error because the sizes in each dimension are either equal or at least
one of them is 1.
‚Ä¢ Rule 3 indicates that the size of the output is (max{2,1}, max{4,1}, max{1,4})
so it is (2, 4, 4). This means that array a is streched along its axis 2 (the third
element in its shape tuple) to make its size 4 (similar to b in that dimension), array
b is streched along its axis 1 to make its size 4 (similar to a in that dimension),
and Ô¨Ånally array b is stretched along its axis 0 to make its size 2.
Let‚Äôs see the outcome:
c = a + b
print(c)
c.shape
[[[10 20 30 40]
[11 21 31 41]
[12 22 32 42]
[13 23 33 43]]
[[14 24 34 44]
[15 25 35 45]
[16 26 36 46]
[17 27 37 47]]]
(2, 4, 4)
Example 3.3 Two arrays, namely, a and b, are created:
a = np.arange(8).reshape(2,4,1) # a 3-dimensional array
a.shape
(2, 4, 1)
b = np.array([10, 20, 30, 40]).reshape(2,2)
b
array([[10, 20],
[30, 40]])

3.2 Pandas
81
If we write a + b:
‚Ä¢ By Rule 1, a 1 is added to the left of b shape to make it a 3-dimensional array
as a; that is, we can think of b now as having shape (1, 2, 2).
‚Ä¢ The shape of a is (2, 4, 1) and the shape of b is (1, 2, 2). Therefore, Rule 2 raises
an error because the sizes in the second dimension (axis 1) are neither equal nor
at least one of them is 1.
Let‚Äôs see the error:
a + b
ValueError
Traceback (most recent call‚ê£
,‚Üílast)
/var/folders/vy/894wbsn11db_lqf17ys9fvdm0000gn/T/ipykernel_43151/
,‚Üí1216668022.py in <module>
----> 1 a + b
ValueError: operands could not be broadcast together with shapes (2,4,1)‚ê£
,‚Üí(2,2)
3.2 Pandas
Pandas specially facilitates working with tabular data that can come in a variety
of formats. For this purpose, it is built around a type of data structure known as
DataFrame. Dataframe, named after a similar data structure in R programming
language, is indeed the most commonly used object in pandas and is similar to a
spreadsheet in which columns can have diÔ¨Äerent types. However, understanding that
requires the knowledge of a more fundamental data structure in pandas known as
Series.
3.2.1 Series
A Series is a one-dimensional array of indexed data. SpeciÔ¨Åcally, in order to create
a Series, we can use:
s = pd.Series(data, index=index)
where the argument index is optional and data can be in many diÔ¨Äerent types but
it is common to see an ndarray, list, dictionary, or even a constant.

82
3 Three Fundamental Python Packages
If the data is a list or ndarray and index is not passed, the default values from 0
to len(data)-1 will be used. Here we create a Series from a list without providing
an index:
import pandas as pd
s1 = pd.Series(['a', np.arange(6).reshape(2,3), 'b', [3.4, 4]]) # to‚ê£
,‚Üícreate a series from a list
s1
0
a
1
[[0, 1, 2], [3, 4, 5]]
2
b
3
[3.4, 4]
dtype: object
s1 = pd.Series(['a', np.arange(6).reshape(2,3), 'b', [3.4, 4]],‚ê£
,‚Üíindex=np.arange(1,5)) # providing the index
s1
1
a
2
[[0, 1, 2], [3, 4, 5]]
3
b
4
[3.4, 4]
dtype: object
Here, we create a Series from a ndarray:
s2 = pd.Series(np.arange(2, 10))
s2
0
2
1
3
2
4
3
5
4
6
5
7
6
8
7
9
dtype: int64
We can access the values and indices of a Series object using its values and index
attributes:
s2.values
array([2, 3, 4, 5, 6, 7, 8, 9])

3.2 Pandas
83
s2.index
RangeIndex(start=0, stop=8, step=1)
The square brackets and slicing that we saw for NumPy arrays can be used along
with index to access elements of a Series:
s2[2] # using index 2 to access its associated value
4
s2[1:-1:2] # the use of slicing
1
3
3
5
5
7
dtype: int64
Similar to NumPy arrays, the slicing provides a view of the original Series:
s3 = s2[2::2]
s3[2] += 10
s2
0
2
1
3
2
14
3
5
4
6
5
7
6
8
7
9
dtype: int64
An important diÔ¨Äerence between accessing the elements of Series and numpy
arrays is the explicit index used in Series as opposed to the implicit index used in
numpy. To better understand this diÔ¨Äerence, try to access the Ô¨Årst element of array
s3 in the above example:
s3
2
14
4
6
6
8
dtype: int64
Does s3[0] work? Let‚Äôs see:

84
3 Three Fundamental Python Packages
#s3[0] # raises error if run
How about s3[2]?
s3[2]
14
The above example would make more sense if we consider the index as a label
for the corresponding value. Thus, when we write s3[2], we try to access the value
that corresponds to label 2. This mechanism gives more Ô¨Çexibility to Series objects
because for the indices we can use other types rather than integers. For example,
s4 = pd.Series(np.arange(10, 15), index = ['spring', 'river', 'lake',‚ê£
,‚Üí'sea', 'ocean'])
s4
spring
10
river
11
lake
12
sea
13
ocean
14
dtype: int64
We can also access multiple elements using a list of labels:
s4[['river', 'sea']]
river
11
sea
13
dtype: int64
However, when it comes to slicing, the selection process using integer indices
works similarly to NumPy. For example, in s3 where index had integer type,
s3[0:2] chooses the Ô¨Årst two elements (similar to NumPy arrays):
s3[0:2]
2
14
4
6
dtype: int64
But at the same time, we can also do slicing using explicit index:
s4['spring':'ocean':2]
spring
10
lake
12

3.2 Pandas
85
ocean
14
dtype: int64
Notice that when using the explicit index in slicing, both the start and the stop are
included (as opposed to to the previous example where implicit indexing was used
in slicing and the stop was not included).
When in a Series we have integer indexing, the fact that slicing uses implicit
indexing could cause confusion. For exmaple,
s = pd.Series(np.random.randint(1, 10, 7), index = np.arange(2, 9))
s
2
2
3
5
4
1
5
2
6
2
7
6
8
9
dtype: int64
s[0:3] # slicing uses implicit indexing
2
2
3
5
4
1
dtype: int64
To resolve such confusions, pandas provides two attributes loc and iloc for
Series and DataFrame objects using which we can specify the type of indexing.
Using loc and iloc one uses explicit and implicit indexing, respectively:
s.iloc[0:3] # note that the value corresponding to "3" is not included
2
2
3
5
4
1
dtype: int64
s.loc[2:4] # note that the value corresponding to "4" is included
2
2
3
5
4
1
dtype: int64
As we said before, we can create Series from dictionaries as well; for example,

86
3 Three Fundamental Python Packages
s = pd.Series({"c": 20, "a": 40, "b": 10})
s
c
20
a
40
b
10
dtype: int64
If a dictionary is used as data to create Series, depending on the version of
pandas and Python that are used, the index will be ordered either by the dictionary‚Äôs
insertion order (for pandas version >= 0.23 and Python version >= 3.6) or lexically
(if either pandas or Python is older than the aforementioned versions). If an index is
passed with a dictionary, then values correspnding to the dictionary keys that are in
the index are used in Series:
s = pd.Series({"c": 20, "a": 40, "b": 10}, index=["b","c","d","c"] )
s
b
10.0
c
20.0
d
NaN
c
20.0
dtype: float64
In the above example, we tried to pull out a value for a non-existing key in the
dictionary (‚Äúd‚Äù). That led to NaN (short for Not a Number), which is the standard
marker used in pandas for missing data.
Finally, if the we use a scalar in the place of data, the scalar is repeated to match
the size of index:
s = pd.Series('hi', index = np.arange(5))
s
0
hi
1
hi
2
hi
3
hi
4
hi
dtype: object
3.2.2 DataFrame
A DataFrame is a generalization of Series to 2-dimension; that is, a 2-dimensional
data structure where both the rows and columns are indexed (labeled). Nevertheless,

3.2 Pandas
87
it is common to refer to row labels as index and to column labels as columns.
As elements of Series can have diÔ¨Äerent type, in DataFrame both the rows and
columns can have diÔ¨Äerent types; for example, here we create a DataFrame from
three Series where each has diÔ¨Äerent data types:
d = {
"one": pd.Series('hi', index=["a", "b", "c", "d"]),
"two": pd.Series([1.0, ["yes", "no"], 3.0, 5], index=["a", "b", "c",‚ê£
,‚Üí"d"]),
"three": pd.Series({"c": 20, "a": 40, "b": 10},‚ê£
,‚Üíindex=["a","b","c","d"])}
d
{'one': a
hi
b
hi
c
hi
d
hi
dtype: object,
'two': a
1.0
b
[yes, no]
c
3.0
d
5
dtype: object,
'three': a
40.0
b
10.0
c
20.0
d
NaN
dtype: float64}
df = pd.DataFrame(d)
df # pandas nicely presents the DataFrame
one
two
three
a
hi
1.0
40.0
b
hi
[yes, no]
10.0
c
hi
3.0
20.0
d
hi
5
NaN
There are various ways to create a DataFrame but perhaps the most common
ways are:
‚Ä¢ from a dictionary of Series
‚Ä¢ from ndarrays or lists
‚Ä¢ from a dictionary of ndarrays or lists
‚Ä¢ from a list of dictionaries

88
3 Three Fundamental Python Packages
Let‚Äôs see some examples for each.
‚Ä¢ DataFrame from a dictionary of Series: this is in fact what we observed before.
In this way, the index (i.e., row labels) is the union of index of all Series. Let‚Äôs
consider another example:
kaz = pd.Series([1000, 200, 30, 10], index = ['spring', 'river',‚ê£
,‚Üí'pond', 'lake'])
jap = pd.Series([900, 80, 300], index = ['spring', 'lake', 'river'])
df = pd.DataFrame({"Kazakhstan": kaz,
"Japan": jap})
df # observe the missing data
Kazakhstan
Japan
lake
10
80.0
pond
30
NaN
river
200
300.0
spring
1000
900.0
‚Ä¢ DataFrame from numpy arrays or lists:
import numpy as np
import pandas as pd
a = np.array([[10,30,200,1000],[80, np.nan, 300, 900]]).T # to transpose
df = pd.DataFrame(a, index = ['lake', 'pond', 'river', 'spring'],‚ê£
,‚Üícolumns = ['Kazakhstan','Japan'])
df
Kazakhstan
Japan
lake
10.0
80.0
pond
30.0
NaN
river
200.0
300.0
spring
1000.0
900.0
In the above example, we used np.nan, which is a Ô¨Çoating-point rep-
resentation of ‚Äúnot a number‚Äù (missing value) handled by NumPy. To better
see the impact of this, notice that a.dtype returns float64. Now, change
np.nan to None and run the same code. This time, a.dtype is Python Ob-
ject (identiÔ¨Åed 'O'). Therefore, unlike the Ô¨Årst case where np.nan was used,
operations on the array are not as eÔ¨Écient.
‚Ä¢ DataFrame from a dictionary of arrays or lists: To use this, the length of all
arrays/lists should be the same (and the same as index length if provided).

3.2 Pandas
89
dict = {"Kazakhstan": [100, 200, 30, 10], "Japan": [900, 300, None, 80]}
df = pd.DataFrame(dict, index=['spring', 'river', 'pond', 'lake'])
df
Kazakhstan
Japan
spring
100
900.0
river
200
300.0
pond
30
NaN
lake
10
80.0
‚Ä¢ DataFrame from a list of dictionaries:
my_list = [{"Kazakhstan": 100, "Japan": 900}, {"Kazakhstan": 200,‚ê£
,‚Üí"Japan": 300}, {"Kazakhstan": 30}, {"Kazakhstan": 10, "Japan": 80}]
df = pd.DataFrame(my_list, index=['spring', 'river', 'pond', 'lake'])
df
Kazakhstan
Japan
spring
100
900.0
river
200
300.0
pond
30
NaN
lake
10
80.0
In the above examples, we relied on pandas to arrange the columns based on the
information in data parameters provided to DataFrame constructor (see (Pandas-
dataframe, 2023)). In the last example, for instance, the columns were determined by
the keys of dictionaries in the list. We can use the columns paramater of DataFrame
to re-arrange them or even add extra columns. For example:
list1 = [{"Kazakhstan": 100, "Japan": 900}, {"Kazakhstan": 200, "Japan":
,‚Üí300}, {"Kazakhstan": 30}, {"Kazakhstan": 10, "Japan": 80}]
df = pd.DataFrame(list1, index=['spring', 'river', 'pond', 'lake'],‚ê£
,‚Üícolumns=['Japan', 'USA', 'Kazakhstan'])
df
Japan
USA
Kazakhstan
spring
900.0
NaN
100
river
300.0
NaN
200
pond
NaN
NaN
30
lake
80.0
NaN
10
Once a DataFrame is at hand, we can access its elements in various forms. For
example, we can access a column by its name:
df['Japan']

90
3 Three Fundamental Python Packages
spring
900.0
river
300.0
pond
NaN
lake
80.0
Name: Japan, dtype: float64
If the column names are strings, this could be also done by an ‚Äúattribute-like‚Äù
indexing as follows:
df.Japan
spring
900.0
river
300.0
pond
NaN
lake
80.0
Name: Japan, dtype: float64
Multiple columns can be accessed by passing a list of columns:
df[['USA','Japan']]
USA
Japan
spring
NaN
900.0
river
NaN
300.0
pond
NaN
NaN
lake
NaN
80.0
At the same time, we can use loc and iloc attributes for explicit and implicit
indexing, respectively:
df.iloc[1:4,0:2]
Japan
USA
river
300.0
NaN
pond
NaN
NaN
lake
80.0
NaN
df.loc[['river','lake'], 'Japan':'USA']
Japan
USA
river
300.0
NaN
lake
80.0
NaN
Another way is to retrieve each column, which is a Series, and then index on that:
df.Japan.iloc[:2]

3.2 Pandas
91
spring
900.0
river
300.0
Name: Japan, dtype: float64
which is equivalent to:
df['Japan'].iloc[:2]
spring
900.0
river
300.0
Name: Japan, dtype: float64
That being said, array style indexing without loc or iloc is not acceptable. For
example,
df.iloc[1,0]
300.0
and
df.loc['river', 'Japan']
300.0
are legitimate, but the following raises error:
#df['river', 'Japan'] # raises error if run
We can also use masking (i.e., select some elements based on some criteria).
For example, assume we have a DataFrame of students with their information as
follows:
import pandas as pd
dict1 = {
"GPA": pd.Series([3.00, 3.92, 2.89, 3.43, 3.55, 2.75]),
"Name": pd.Series(['Askar', 'Aygul', 'Ainur', 'John', 'Smith',‚ê£
,‚Üí'Xian']),
"Phone": pd.Series([8728, 8730, 8712, 8776, 8770, 8765])
}
df = pd.DataFrame(dict1)
df
GPA
Name
Phone
0
3.00
Askar
8728
1
3.92
Aygul
8730
2
2.89
Ainur
8712
3
3.43
John
8776
4
3.55
Smith
8770
5
2.75
Xian
8765

92
3 Three Fundamental Python Packages
We would like to retrieve the information for any student with GPA > 3.5. To do
so, we deÔ¨Åne a boolean mask (returns True or False) that can easily be used for
indexing:
df.GPA > 3.5
0
False
1
True
2
False
3
False
4
True
5
False
Name: GPA, dtype: bool
df.Name[df.GPA > 3.5]
1
Aygul
4
Smith
Name: Name, dtype: object
In the above example we only returned values of columns Name at which the mask
array (df.GPA > 3.5) is True. To retrieve all columns for which the mask array is
True we can use the mask with the DataFrame:
df[df.GPA > 3.5]
GPA
Name
Phone
1
3.92
Aygul
8730
4
3.55
Smith
8770
There are two handy methods for DataFrame objects, namely, head(n=5) and
tail(n=5) that return the Ô¨Årst and last n rows of a DataFrame (the default value of
n is 5), respectively:
df.head()
GPA
Name
Phone
0
3.00
Askar
8728
1
3.92
Aygul
8730
2
2.89
Ainur
8712
3
3.43
John
8776
4
3.55
Smith
8770
Last but not least, we can access the column and index labels through the columns
and index attributes of a DataFrame:
df.columns

3.2 Pandas
93
Index(['GPA', 'Name', 'Phone'], dtype='object')
3.2.3 Pandas Read and Write Data
A powerful feature of Pandas is its ability to easily read data from and write to a vari-
ety of common formats (sometimes, depending on speciÔ¨Åc applications, some other
libraries would be used as well). For example, Pandas can be used to read from/write
to csv format, pickle format (this is a popular Python binary data format using which
we can save any Python object (pickling) to be accessed later (unpickling)), SQL,
html, etc. A list of pandas input/output (I/O) functions is found at (Pandas-io, 2023).
As an example, suppose we would like to read the table of pandas I/O tools,
which is available at the URL (Pandas-io, 2023). For this purpose, we can use
pandas read_html that reads tables from an HTML:
url = 'https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html'
table = pd.read_html(url)
print(len(table))
8
The read_html() returns a list of possible DataFrames in the html Ô¨Åle (see (Pandas-
readhtml, 2023)). This is why the length of the above table is 8 (as of writing this
manuscript). Our desired table is the Ô¨Årst one though:
table[0]
Format Type
Data Description
Reader
Writer
0
text
CSV
read_csv
to_csv
1
text
Fixed-Width Text File
read_fwf
NaN
2
text
JSON
read_json
to_json
3
text
HTML
read_html
to_html
4
text
LaTeX
NaN
Styler.to_latex
5
text
XML
read_xml
to_xml
6
text
Local clipboard
read_clipboard
to_clipboard
7
binary
MS Excel
read_excel
to_excel
8
binary
OpenDocument
read_excel
NaN
9
binary
HDF5 Format
read_hdf
to_hdf
10
binary
Feather Format
read_feather
to_feather
11
binary
Parquet Format
read_parquet
to_parquet
12
binary
ORC Format
read_orc
to_orc
13
binary
Stata
read_stata
to_stata
14
binary
SAS
read_sas
NaN
15
binary
SPSS
read_spss
NaN
16
binary
Python Pickle Format
read_pickle
to_pickle
17
SQL
SQL
read_sql
to_sql
18
SQL
Google BigQuery
read_gbq
to_gbq

94
3 Three Fundamental Python Packages
3.3 Matplotlib
3.3.1 Backend and Frontend
Matplotlib is the primary Python plotting library built on NumPy. It is designed to
support a wide variety of use cases: to draw inline and interactive plots in Jupyter
notebook, to draw plots on new windows from Python shell, or to create plots for
web applications or cross-platform graphical user interfaces such as Qt API. To
enable these use cases, matplotlib supports a variety of backends. We refer to
the behind-the-scene hard work that makes the plotting happen as the ‚Äúbackend‚Äù in
contrast to the ‚Äúfrontend‚Äù, which depending on the context, is referred to the coding
environment (e.g., Jupyter notebook) or the codes themselves.
There are generally two types of backends: 1) interactive using which we can
zoom in/out or move the plots around; and 2) non-interactive, which is used for
creating static images of our Ô¨Ågures (e.g., png, ps, and pdf). In Jupyter notebook, a
simple way to set the backend is to use the Jupyter magic command %matplotlib.
For example, using
%matplotlib notebook
we can create interactive Ô¨Ågures in the notebook, and using
%matplotlib inline
which is the default backend in notebooks, we can create static images in the note-
book. A list of available backends is shown by
%matplotlib --list
3.3.2 The Two matplotlib Interfaces: pyplot-style and OO-style
To plot Ô¨Ågures, matplotlib genereally relies on two concepts of figure and Axes.
Basically, the figure is what we think of as the ‚Äúwhole Ô¨Ågure‚Äù, while Axes is what
we genereally refer to as ‚Äúa plot‚Äù (e.g., part of the image with curves and coordinates).
Therefore, a figure can contain many Axes but each Axes can be part of one Ô¨Ågure.
Using an Axes object and its deÔ¨Åned methods, we can control various properties.
For example, Axes.set_title(), Axes.set_xlabel(), and Axes.set_ylim()
can be used to set the title of a plot, set the label of the x axis, or set the limits of the
y-axis, respectively. There two ways to use matplotlib:
1) the pyplot style, which resembles plotting Matlab Ô¨Ågures (therefore, it is also
referred to as the Matlab style). In this style, we rely on pyplot module and its
functions to automatically create and manage Ô¨Ågures and axes; and

3.3 Matplotlib
95
2) the object-oriented style (OO-style) using which we create Ô¨Ågures and axes, and
control them using methods deÔ¨Åned for them.
In general the pyplot style is less Ô¨Çexible than the OO-style and many of the
functions used in pyplot style can be implemented by methods of Axes object.
pyplot-style‚Äîcase of a simple plot: We Ô¨Årst create a simple plot using the plot
function of pyplot module (see (pyplot, 2023) for its full documentation). To do so,
we Ô¨Årst import the pyplot and use the standard alias plt to refer to that. As plotting
functions in matplotlib expect NumPy arrays as input (even array-like objects
such as lists as inputs are converted internally to NumPy arrays), it is common to see
importing NumPy along with matplotlib.
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
x = np.linspace(0, 1, 20) # to create an array of x axis elements
plt.figure(figsize=(4.5, 2.5), dpi = 200)
plt.plot(x, x, linewidth=2) # to use the default line style and color,‚ê£
,‚Üíand change the width of the line using linewidth
plt.plot(x, x**3, 'r+', label = 'cubic function') # to use the red‚ê£
,‚Üícolor and circle marker to plot, and label parameter for legend
plt.plot(x, np.cos(np.pi * x), 'b-.', label = 'cosine function',‚ê£
,‚Üímarker='d', markersize = 5) # to use the marker paramater and the‚ê£
,‚Üímarkersize to control the size
plt.xlabel('time', fontsize='small')
plt.ylabel('amplitude', fontsize='small')
plt.title('time-amplitude', fontsize='small')
plt.legend(fontsize='small') # to add the legend
plt.tick_params(axis='both', labelsize=7) # to adjust the size of tick‚ê£
,‚Üílabels on both axes
# plt.show() # this is not required here because Jupyter backends will‚ê£
,‚Üícall show() at the end of each cell by themselves
In the above code, and for illustration purposes, we used various markers. The list of
possible markers is available at (matplotlib-markers, 2023).
OO-style‚Äîcase of a simple plot: Here we use the OO-style to create a similar
plot as shown in Fig. 13.2. To do so, we use subplots() from pyplot (not to
be confused with subplot()) that returns a Ô¨Ågure and a single or array of Axes
objects. Then we can use methods of Axes objects such as Axes.plot to plot data
on the axes (or use aforementioned Axes methods to control the behaviour of the
Axes object):
import numpy as np
import matplotlib.pyplot as plt
x = np.linspace(0, 1, 20)

96
3 Three Fundamental Python Packages
0.0
0.2
0.4
0.6
0.8
1.0
time
‚àí1.00
‚àí0.75
‚àí0.50
‚àí0.25
0.00
0.25
0.50
0.75
1.00
amplitude
time-amplitude
cubic function
cosine function
Fig. 3.2: The output of the above plotting example. Here we used pyplot-style.
fig, ax = plt.subplots(figsize=(4.5, 2.5), dpi = 200) # to create a‚ê£
,‚Üífigure and an axes object
ax.plot(x, x, linewidth=2)
ax.plot(x, x**3, 'r+', label = 'cubic function')
ax.plot(x, np.cos(np.pi * x), 'b-.', label = 'cosine function',‚ê£
,‚Üímarker='d', markersize = 5)
ax.set_xlabel('time', fontsize='small')
ax.set_ylabel('amplitude', fontsize='small')
ax.set_title('time-amplitude', fontsize='small')
ax.legend(fontsize='small')
ax.tick_params(axis='both', labelsize=7) # to adjust the size of tick‚ê£
,‚Üílabels on both axes
pyplot-style‚Äîcase of multiple Ô¨Ågures with/without subplots: To create multiple
Ô¨Ågures, we can use figure() with an increasing index (integer) as the input to refer
to the Ô¨Ågures. To create subplots, we can use subplot() (not to be confused with
subplots()). It recieves three integers that show nrows (number of rows for the
subplot), ncols (number of columns for the subplot), and the plot_index where
plot_index could be from from 1 to nrows √ó ncols to show the current plot
(similar to Matlab, it relies on the concept of current plots):
x = np.linspace(0, 1, 20)
plt.figure(1, figsize=(4.5, 4), dpi = 200) # to create the first figure
plt.subplot(2, 1, 1) # the same as subplot(211)
plt.suptitle('time-amplitude', fontsize='small')

3.3 Matplotlib
97
0.0
0.2
0.4
0.6
0.8
1.0
time
‚àí1.00
‚àí0.75
‚àí0.50
‚àí0.25
0.00
0.25
0.50
0.75
1.00
amplitude
time-amplitude
cubic function
cosine function
Fig. 3.3: The output of the above plotting example. Here we used OO-style.
plt.plot(x, x**3, 'r+', label = 'cubic function')
plt.legend(fontsize='small')
plt.ylabel('amplitude', fontsize='small')
plt.tick_params(axis='both', labelsize=7)
plt.subplot(212)
plt.plot(x, np.cos(np.pi * x), 'b-.', label = 'cosine function',‚ê£
,‚Üímarker='d', markersize = 5)
plt.plot(x, x, linewidth=2, label = 'linear')
plt.legend(fontsize='small')
plt.ylabel('amplitude', fontsize='small')
plt.xlabel('time', fontsize='small')
plt.tick_params(axis='both', labelsize=7)
plt.figure(2, figsize=(4.5, 2), dpi = 200) # to create the second figure
plt.plot(x, x**2, linewidth=2)
plt.title('time-ampl', fontsize='small')
plt.tick_params(axis='both', labelsize=7)
OO-style‚Äîcase ofmultipleÔ¨Ågureswith/withoutsubplots: Hereweusesubplots()
from pyplot (recall that we used it before to create the simple OO-style plot too). It
receives two integers nrow and ncol with defaults being 1 (no third integer because
it does not rely on the concept of current plot) and if we use it with no input (e.g.,
plt.subplots()) it just creates one Ô¨Ågure and one axis:

98
3 Three Fundamental Python Packages
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
amplitude
cubic function
0.0
0.2
0.4
0.6
0.8
1.0
time
‚àí1.0
‚àí0.5
0.0
0.5
1.0
amplitude
cosine function
linear
time-amplitude
0.0
0.2
0.4
0.6
0.8
1.0
time
0.0
0.2
0.4
0.6
0.8
1.0
amplitude
time-amplitude
Fig. 3.4: The output of the above plotting example. Here we used pyplot-style.

3.3 Matplotlib
99
x = np.linspace(0, 1, 20)
fig1, axs = plt.subplots(2, 1, figsize=(4.5, 3.5), dpi = 200)
# fig, (ax1, ax2) = plt.subplots(2, 1) # this is another form in which‚ê£
,‚Üíwe use sequence unpacking
fig1.suptitle('time-amplitude', fontsize='small')
axs[0].plot(x, x**3, 'r+', label = 'cubic function')
axs[0].legend(fontsize='small')
axs[0].set_ylabel('amplitude', fontsize='small')
axs[0].tick_params(axis='both', labelsize=7)
axs[1].plot(x, np.cos(np.pi * x), 'b-.', label = 'cosine function',‚ê£
,‚Üímarker='d', markersize = 5)
axs[1].plot(x, x, linewidth=2, label = 'linear')
plt.legend(fontsize='small')
axs[1].set_ylabel('amplitude', fontsize='small')
axs[1].set_xlabel('time', fontsize='small')
axs[1].tick_params(axis='both', labelsize=7)
fig2, ax = plt.subplots(figsize=(4.5, 2), dpi = 200) # to use the‚ê£
,‚Üídefault values of 1 x 1
ax.plot(x, x**2, linewidth=2)
ax.set_title('time-ampl', fontsize='small')
ax.tick_params(axis='both', labelsize=7)
3.3.3 Two Instructive Examples
Example 3.4 A scatter plot is typically a representation of two dimensional observa-
tions in which each observation is represented by a point with Cartesian coordinates
being the values of the Ô¨Årst and the second variables. These plots are helpful in the
sense that they provide us with an idea of how the data is distributed. In this example,
we would like to generate a scatter plot of some synthetic (artiÔ¨Åcial) data and a line
that separates them to some extent. Here are a few things to know:
1) The data is generated from two bivariate Gaussian (normal) distributions with
given parameters (means and covariance matrices). For sampling these distri-
butions, we use rvs from scipy.stats.multivariate_normal (see (scipy-
normal, 2023) for full documentation);
2) Here the scatter plot is generated using plot() function itself (although there
are other ways such as pyplot.scatter()); and
3) The line separating the data is given by 2x + 3y + 1 = 0. Here we assume this
line is given to us. Later we will see that a major goal in machine learning is in

100
3 Three Fundamental Python Packages
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
amplitude
cubic function
0.0
0.2
0.4
0.6
0.8
1.0
time
‚àí1.0
‚àí0.5
0.0
0.5
1.0
amplitude
cosine function
linear
time-amplitude
0.0
0.2
0.4
0.6
0.8
1.0
time
0.0
0.2
0.4
0.6
0.8
1.0
amplitude
time-amplitude
Fig. 3.5: The output of the above plotting example. Here we used OO-style.

3.3 Matplotlib
101
fact devising algorithms that can estimate the parameters of a line (or in general,
parameters of nonlinear mathematical functions) that can separate the data from
diÔ¨Äerent groups in some mathematical sense.
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import multivariate_normal as mvn
np.random.seed(100)
m0 = np.array([-1,-1])
m1= np.array([1,1])
cov0 = np.eye(2)
cov1 = np.eye(2)
n = 15 # number of observations from each Gaussian distribution
X0 = mvn.rvs(m0,cov0,n)
x0, y0 = np.split(X0,2,1)
X1 = mvn.rvs(m1,cov1,n)
x1, y1 = np.split(X1,2,1)
plt.style.use('seaborn')
plt.figure(figsize=(4,3), dpi=150)
plt.plot(x0,y0,'g.',label='class 0', markersize=6)
plt.plot(x1,y1,'r.', label='class 1', markersize=6)
plt.xticks(fontsize=8)
plt.yticks(fontsize=8)
lim_left, lim_right = plt.xlim()
plt.plot([lim_left, lim_right],[-lim_left*2/3-1/3,-lim_right*2/3-1/
,‚Üí3],'k', linewidth=1.5) # to plot the line
A few points about the above code:
‚Ä¢ Notice how we drew the line. We converted 2x + 3y + 1 = 0 to y = ‚àí2
3 x ‚àí1
3
and then created two lists as the input to the plot() function: [lim_left,
lim_right] provides the line x limits, whereas the y coordinates corresponding
tothe x limits areprovidedby [-lim_left*2/3-1/3,-lim_right*2/3-1/3].
‚Ä¢ We used the seaborn default style to plot. Seaborn is a more recent visualiza-
tion package on top of matplotlib. Change plt.style.use('seaborn') to
plt.style.use('default') and observe the diÔ¨Äerence. Feel free to use any
style.
‚Ä¢ Observe the Ô¨Åguresizeandthedpi(dotsperinch): plt.figure(figsize=(4,3),
dpi=150). Feel free to rely on default values. ‚ñ†
Example 3.5 In this example, we would like to Ô¨Årst create a 10√ó10 grid with x and
y coordinates ranging from 0 to 9. Then we write a function that assigns a label to
each point in this grid such that if the sum of x and y coordinates for each point
in the grid is less than 10, we assign a label 0 and if is greater or equal to 10, we

102
3 Three Fundamental Python Packages
‚àí3
‚àí2
‚àí1
0
1
2
‚àí2
‚àí1
0
1
2
Fig. 3.6: The output of the code written for Example 3.4 .
assign a label 1. Then we create a color plot such that labels of 0 and 1 are colored
diÔ¨Äerently‚Äîhere we use ‚Äòaquamarine‚Äô and ‚Äòbisque‚Äô colors (see a list of matplotlib
colors at (matplotlib-colors, 2023)). A few things to know:
1) This example becomes helpful later when we would like to plot the decision
regions of a classiÔ¨Åer. We will see that similar to the labeling function here, a
binary classiÔ¨Åer also divides the feature space (for example, our 2D space) into
two regions over which labels are diÔ¨Äerent;
2) To create the grid, we use numpy.meshgrid(). Observe what X and Y are and
see how they are mixed to create the coordinates; and
3) To color our 2D space deÔ¨Åned by our grid, we use pyplot.pcolormesh() with
x and y coordinates in X and Y, respectively, and an array Z of the same size as
X and Y. The values of Z will be mapped to color by a Colormap instance given
by cmap parameter. For this purpose, the minimum and maximum values of Z
will be mapped to the Ô¨Årst and last element of the Colormap. However, here our
color map has only two elements and Z also includes 0 and 1 so the mapping is
one-to-one.
import numpy as np
import matplotlib.pyplot as plt
x = np.arange(0, 10, 1)
y = np.arange(0, 10, 1)
X, Y = np.meshgrid(x, y)
X

3.3 Matplotlib
103
array([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])
Y
array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
[2, 2, 2, 2, 2, 2, 2, 2, 2, 2],
[3, 3, 3, 3, 3, 3, 3, 3, 3, 3],
[4, 4, 4, 4, 4, 4, 4, 4, 4, 4],
[5, 5, 5, 5, 5, 5, 5, 5, 5, 5],
[6, 6, 6, 6, 6, 6, 6, 6, 6, 6],
[7, 7, 7, 7, 7, 7, 7, 7, 7, 7],
[8, 8, 8, 8, 8, 8, 8, 8, 8, 8],
[9, 9, 9, 9, 9, 9, 9, 9, 9, 9]])
coordinates = np.array([X.ravel(), Y.ravel()]).T
coordinates[0:15]
array([[0, 0],
[1, 0],
[2, 0],
[3, 0],
[4, 0],
[5, 0],
[6, 0],
[7, 0],
[8, 0],
[9, 0],
[0, 1],
[1, 1],
[2, 1],
[3, 1],
[4, 1]])
def f(W):
return (W.sum(axis=1) >= 10).astype(int)

104
3 Three Fundamental Python Packages
Z = f(coordinates)
Z
array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1])
from matplotlib.colors import ListedColormap
color = ('aquamarine', 'bisque')
cmap = ListedColormap(color)
cmap
fig, ax = plt.subplots(figsize=(3,3), dpi=150)
ax.tick_params(axis = 'both', labelsize = 6)
Z = Z.reshape(X.shape)
plt.pcolormesh(X, Y, Z, cmap = cmap, shading='nearest')
0
2
4
6
8
0
2
4
6
8
Fig. 3.7: The output of the plotting code written for Example 3.5.
Exercises:
Exercise 1: The following wikipedia page includes a list of many machine learning
datasets:

3.3 Matplotlib
105
https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research
Extract the table of datasets for ‚ÄúNews articles‚Äù only and then Ô¨Ånd the minimum sam-
ple size used in datasets used in this table (i.e., the minimum of column ‚ÄúInstances‚Äù).
In doing so,
1) Do not extract all tables to choose this one from the list of tables. Make sure you
only extract this table. To do so, use something unique in this table and use it as
for the match argument of pandas.read_html();
2) The column ‚ÄúInstances‚Äù should be converted to numeric. For doing so, use
pandas.to_numeric() function. Study and properly use the errors parameter
of this function to be able to complete this task.
3) See an appropriate pandas.DataFrame method to Ô¨Ånd the minimum.
Exercise 2: The ‚Äúdata‚Äù folder, contains GenomicData_orig.csv Ô¨Åle. This Ô¨Åle
contains a real dataset taken from Gene Expression Omnibus (GEO), https://
www.ncbi.nlm.nih.gov/geo/ with accession ID GSE37745. The original data
contains gene expression taken from 196 individuals with either squamous cell
carcinoma (66 individuals), adenocarcinoma (106), or large cell carcinoma (24);
however, we have chosen a subset of these individuals and also artiÔ¨Åcially removed
some of the measurements to create the eÔ¨Äect of missing data.
1) Use an appropriate reader function from pandas to read this Ô¨Åle and show the
Ô¨Årst 10 rows.
2) What are the labels of the Ô¨Årst column (index 0) and 20th column (index 19)?
Hint: use the columns attribute of the DataFrame
3) drop the Ô¨Årst two columns (index 0 and 1) because they are sample ID info
and we do not need. Hint: use pandas.DataFrame.drop() method. Read
documentation to use it properly. You can use Part 2 to access the labels of these
columns to use as input to pandas.DataFrame.drop()
4) How many missing values are in this dataset?
Hint: Ô¨Årst use pandas.DataFrame.isnull() to return a DataFrame with
True for missing values and False otherwise, and then either use numpy.sum()
or pandas.DataFrame.sum(). Use sum() two times.
5) Use DataFrame.fillna() method to Ô¨Åll the missing values with 0 (Ô¨Ålling
missing values with some values is known as imputing missing values). Use the
method in Part 4 to show that there is no missing value now.
Exercise 3: Modify Example 3.5 presented before in this chapter to create the
following Ô¨Ågure:
To do so,
1) use W.sum(axis=1) >= 10 condition (similar to Example 3.5) along with
another suitable condition that should be determined to plot the stripe shape;
2) do not explicitly plot lines as we did in the Example 3.4. What appears in
this Ô¨Ågure as lines is the ‚Äústairs‚Äù shape as in Example 3.5, but with a much
larger grid in the same range of x and y (i.e., a Ô¨Åner grid). Therefore, it is

106
3 Three Fundamental Python Packages
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
Fig. 3.8: The output of Exercise 3.
required to Ô¨Årst redeÔ¨Åne the grid (for example, something like arange(0, 10,
0.01)), and then to color these boundaries between regions as black use the
pyplot.contour() function with appropriate inputs (X ,Y, and Z, and black
colors).
Exercise 4: Write a piece of code that creates a numpy array of shape (2,4,3,5,2)
with all elements being 4?
Exercise 5: Suppose a and b are two numpy arrays. Looking at shape attribute of
array a leads to
(2, 5, 2, 1)
and looking at the shape attribute of array b leads to
(5, 1, 4)
Which of the following options are correct about c = a + b?
A) a + b raises an error because a and b are not broadcastable
B) a and b are broadcastable and the shape of c is (5, 2, 4)
C) a and b are broadcastable and the shape of c is (2, 5, 2, 4)
D) a and b are broadcastable and the shape of c is (5, 5, 4, 1)
E) a and b are broadcastable and the shape of c is (2, 5, 2, 1)
Exercise 6: Suppose a and b are two numpy arrays. Looking at shape attribute of
array a leads to
(3, 5, 2, 4, 8)
and looking at the shape attribute of array b leads to
(2, 4, 1)

3.3 Matplotlib
107
Which of the following options are correct about c = a - b?
A) a and b are broadcastable and the shape of c is (3, 5, 2, 4, 8)
B) a - b raises an error because a and b are not broadcastable
C) a and b are broadcastable and the shape of c is (1, 1, 2, 4, 1)
D) a and b are broadcastable and the shape of c is (3, 5, 2, 4, 1)
Exercise 7: We have two Series deÔ¨Åned as follows: available:
import pandas as pd
s1 = pd.Series([11, 25, 55, 10], index = ['g', 'r', 'p', 'a'])
s2 = pd.Series([20, 30, 40])
We create a DataFrame by:
df = pd.DataFrame({"col1": s1, "col2": s2})
a) What is len(df.index)? Why?
b) Why are all the NaN values created in df?
Exercise 8: What is the output of the following code snippet:
import numpy as np
a = np.arange(1, 11)
b = np.arange(0, 10, 2)
np.sum(a[b])
a) 25
b) 20
c) 15
d) the code raises an error
Exercise 9: What is the output of each of the following code snippets:
a)
a = np.arange(1, 5)
b = a[:2]
b[0] = 10
print(a)
b)
a = np.arange(1, 5)
b = np.arange(2)
c = a[b]

108
3 Three Fundamental Python Packages
c[0] = 10
print(a)
Exercise 10: Suppose we have a numpy array ‚Äúa‚Äù that contains about 10 million
integers. We would like to multiply each element of a by 2. Which of the following
two codes is better to use? Why?
Code A)
for i, v in enumerate(a):
a[i] *= 2
Code B)
a *= 2
Exercise 11: Which of the following options can not create a DataFrame()? Explain
the reason (assume pandas is imported as pd).
a)
dic = {"A": [10, 20, 30, 40], "B": [80, 90, 100, None]}
pd.DataFrame(dic, index=['z', 'd', 'a', 'b', 'f'])
b)
dic = {"A": [10, 20, 30, 40], "B": [80, 90, 100, None]}
pd.DataFrame(dic)
c)
dic = {"A": [10, 20, 30, 40], "B": [80, 90, None]}
pd.DataFrame(dic)
d)
import numpy as np
dic = {"A": np.array([10, 20, 30, 40]), "B": [80, 90, 100, None]}
pd.DataFrame(dic, index=['z', 'd', 'a', 'b'])
Exercise 12: A DataFrame named df is created as follows:
list1 = [{1: 'a', 2: 'b'}, {3: 'c'}, {1: 'd', 2: 'e'},]
df = pd.DataFrame(list1, index=[1, 2, 3])
df

3.3 Matplotlib
109
1
2
3
1
a
b
NaN
2
NaN
NaN
c
3
d
e
NaN
In each of the following parts, determine whether the indexing is legitimate, and
if it is, deteremine the output:
a)
df.loc[1,1]
b)
df[1]
c)
df[1][1]
d)
df[1].iloc[1]
e)
df[1].loc[1]
f)
df[1,1]

Chapter 4
Supervised Learning in Practice: the First
Application Using Scikit-Learn
In this chapter, we Ô¨Årst formalize the idea of supervised learning and its main two
tasks, namely, classiÔ¨Åcation and regression, and then provide a brief introduction to
one of the most popular Python-based machine learning software, namely, Scikit-
Learn. After this introduction, we start with a classiÔ¨Åcation application with which
we introduce and practice with some important concepts in machine learning such
as data splitting, normalization, training a classiÔ¨Åer, prediction, and evaluation. The
concepts introduced in this chapter are important from the standpoint of the practical
machine learning because they are used frequently in the design process.
4.1 Supervised Learning
Supervised learning is perhaps the most common type of machine learning in which
the goal is to learn a mapping between a vector of input variables (also known as
predictors or feature vector, which is a vector of measurable variables in a problem)
and output variables. To guide the learning process, the assumption in supervised
learning is that a set of input-output instances, also referred to as training data, is
available. An output variable is generally known as target, response, or outcome, and
while there is no restriction on the number of output variables, for the ease of notation
and discussion, we assume a single output variable, denoted y, is available. While
it is generally easy and cheap to measure feature vectors, it is generally diÔ¨Écult or
costly to measure y. This is the main reason that in supervised learning practitioners
go through the pain of assigning the outcomes to input vectors once (i.e., collecting
the training data) and hope to learn a function that can perform this mapping in the
future.
Suppose we have a set of training data Str = {(x1, y1), (x2, y2), . . ., (xn, yn)} where
xi ‚ààRp, i = 1, . . ., n, represents a vector including the values of p feature variables
(feature vector), yi denotes the target value (outcome) associated with xi, and n is
the number of observations (i.e., sample size) in the training data. In classiÔ¨Åcation,
possible values for yi belong to a set of predeÔ¨Åned Ô¨Ånite categories called labels and
111
¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
A. Zollanvari, Machine Learning with Python, https://doi.org/10.1007/978-3-031-33342-2_4 

112
4 Supervised Learning in Practice: the First Application Using Scikit-Learn
the goal is to assign a given (realization of random) feature vector (also known as
observation or instance) to one of the class labels (in some applications known as
multilabel classiÔ¨Åcation, multiple labels are assigned to one instance). In regression,
on the other hand, yi represents realizations of a numeric random variable and the
goal is to estimate the target value for a given feature vector. Whether the problem
is classiÔ¨Åcation or regression, the goal is to estimate the value of the target y for
a given feature vector x. In machine learning, we refer to this estimation problem
as prediction; that is, predicting y for a given x. At the same time, in classiÔ¨Åcation
and regression, we refer to the mathematical function that performs this mapping as
classiÔ¨Åer and regressor, respectively.
4.2 Scikit-Learn
Scikit-Learn is a Python package that contains an eÔ¨Écient and uniform API to
implement many machine learning (ML) methods. It was initially developed in 2007
by David Cournapeau and was made available to public in 2010. The Scikit-Learn
API is neatly designed based on a number of classes. Three fundamental objects in
scikit-learn are Estimators, Transformers, and Predictors:
‚Ä¢ Estimators: Any object that can estimate some parameters based on a dataset is
called an estimator. All ML models, whether classiÔ¨Åers or regressors, are im-
plemented in their own Estimator class. For example, a ML algorithm known as
k-nearest neighbors (kNN) rule is implemented in the KNeighborsClassifier
class in the sklearn.neighbors module. Or another algorithm known as per-
ceptron is implemented in the Perceptron class in the linear_model module.
All estimators implement the fit() method that takes either one argument (data)
or two as in supervised learning where the second argument represents target
values:
estimator.fit(data, targets)
or
estimator.fit(data)
The fit() method performs the estimation from the given data.
‚Ä¢ Transformers: Some estimators can also transform data. These estimators are
known as transformers and implement transform() method to perform the
transformation of data as:
new_data = transformer.transform(data)
Transformersalso implement aconvenient methodknownas fit_transform(),
which is similar (sometimes more eÔ¨Écient) to calling fit() and transform()

4.3 The First Application: Iris Flower ClassiÔ¨Åcation
113
back-to-back:
new_data = transformer.fit_transform(data)
‚Ä¢ Predictors: Some estimators can make predictions given a data. These esti-
mators are known as predictors and implement predict() method to perform
prediction:
prediction = predictor.predict(data)
ClassiÔ¨Åcation algorithms generally implement
probability = predictor.predict_proba(data)
that provides a way to quantify the certainty (in terms of probability) of a
prediction for a given data.
4.3 The First Application: Iris Flower ClassiÔ¨Åcation
In this application, we would like to train a ML classiÔ¨Åer that receives a feature
vector containing morphologic measurements (length and width of petals and sepals
in centimeters) of Iris Ô¨Çowers and classiÔ¨Åes a given feature vector to one of the
three Iris Ô¨Çower species: Iris setosa, Iris virginica, or Iris versicolor. The underlying
hypothesis behind this application is that an Iris Ô¨Çower can be classiÔ¨Åed into its
species based on its petal and sepal lengths and widths. Therefore, our feature
vectors xi that are part of training data are four dimensional (p = 4), and y can take
three values; therefore, we have a multiclass classiÔ¨Åcation (three-class) problem.
Our training data is a well-known dataset in statistics and machine learning,
namely, Iris dataset, that was collected by one of the most famous biologist-
statistician of 20th century, Sir Ronald Fisher. This dataset is already part of scikit-
learn and can be accessed by importing its datasets module as follows:
from sklearn import datasets # sklearn is the Python name of‚ê£
,‚Üíscikit-learn
iris = datasets.load_iris()
type(iris)
sklearn.utils.Bunch
Datasets that are part of scikit-learn are generally stored as ‚ÄúBunch‚Äù objects; that
is, an object of class sklearn.utils.Bunch. This is an object that contains the
actual data as well as some information about it. All these information are stored
in Bunch objects similar to dictionaries (i.e., using keys and values). Similar to
dictionaries, we can use key() method to see all keys in a Bunch object:
iris.keys()

114
4 Supervised Learning in Practice: the First Application Using Scikit-Learn
dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR',‚ê£
,‚Üí'feature_names', 'filename'])
The value of the key DESCR gives some brief information about the dataset. Never-
theless, compared with dictionaries, in Bunch object we can also access values as
bunch.key (or, equivalently, as in dictionaries by bunch['key']). For example,
we can access class names as follows:
print(iris['target_names']) # or, equivalently, print(iris.target_names)
['setosa' 'versicolor' 'virginica']
Next, we print the Ô¨Årst 500 characters in the DESCR Ô¨Åeld where we see the name of
features and classes:
print(iris.DESCR[:500])
.. _iris_dataset:
Iris plants dataset
--------------------
**Data Set Characteristics:**
:Number of Instances: 150 (50 in each of three classes)
:Number of Attributes: 4 numeric, predictive attributes and the class
:Attribute Information:
- sepal length in cm
- sepal width in cm
- petal length in cm
- petal width in cm
- class:
- Iris-Setosa
- Iris-Versicolour
- Iris-Virginica
All measurements (i.e., feature vectors) are stored as values of data key. Here,
the Ô¨Årst 10 feature vectors are shown:
iris.data[:10]
array([[5.1, 3.5, 1.4, 0.2],
[4.9, 3. , 1.4, 0.2],
[4.7, 3.2, 1.3, 0.2],
[4.6, 3.1, 1.5, 0.2],
[5. , 3.6, 1.4, 0.2],

4.3 The First Application: Iris Flower ClassiÔ¨Åcation
115
[5.4, 3.9, 1.7, 0.4],
[4.6, 3.4, 1.4, 0.3],
[5. , 3.4, 1.5, 0.2],
[4.4, 2.9, 1.4, 0.2],
[4.9, 3.1, 1.5, 0.1]])
We refer to the matrix containing all feature vectors as data matrix (also known
as feature matrix). By convention, scikit-learn assumes this matrix has the shape
of sample size √ó feature size; that is, the number of observations (also sometimes
referred to as the number of samples) √ó the number of features. For example, in this
data there are 150 Iris Ô¨Çowers and for each there are 4 features; therefore, the shape
is 150 √ó 4:
iris.data.shape
(150, 4)
The corresponding targets (in the same order as the feature vectors stored in data)
can be accessed through the target Ô¨Åeld:
iris.target
array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
The three classes in the dataset, namely, setosa, versicolor, and virginica are
encoded as integers 0, 1, and 2, respectively. Although there are various encoding
schemes to transform categorical variables into their numerical counterparts, this is
known as integer (ordinal) encoding (also see Exercise 3).
For feature vectors encoding is generally needed as many machine
learning methods are designed to work with numeric data. Although some
ML models also require encoded labels, even for others that do not have this
requirement, we generally encode labels because: 1) a uniform format for the
data helps comparing diÔ¨Äerent types of models regardless of whether they
can work with encoded labels or not; and 2) numeric values can use less
memory.
Here we use bincount function from NumPy to count the number of samples in
each class:
import numpy as np
np.bincount(iris.target)

116
4 Supervised Learning in Practice: the First Application Using Scikit-Learn
array([50, 50, 50])
As seen here, there are 50 observations in each class. Here we check whether
the type of data matrix and target is ‚Äúarray-like‚Äù (e.g., numpy array or pandas
DataFrame), which is the expected type of input data for scikit-learn estimators:
print('type of data: ' + str(type(iris.data))+ '\ntype of target: ' +‚ê£
,‚Üístr(type(iris.target)))
type of data: <class 'numpy.ndarray'>
type of target: <class 'numpy.ndarray'>
And Ô¨Ånally we can see the feature names as follows:
print(iris.feature_names) # the name of features
['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal‚ê£
,‚Üíwidth (cm)']
4.4 Test Set for Model Assessment
Sometimes before training a machine learning model, we need to think a bit in
advance. Suppose we train a model based on the given training data. A major
question to ask is how well the model performs. This is a critical question that shows
the predictive capacity of the trained model. In other words, the entire practical utility
of the trained model is summarized in its metrics of performance. To answer this
question, there are various evaluation rules. As discussed in Section 1.3.6, perhaps
the most intuitive estimator is to evaluate a trained model on a test data, also known
as test set. However, in contrast with the EEG application discussed in Section 1.3,
here we only have one dataset. Therefore, we have to simulate the eÔ¨Äect of having
test set. In this regard, we randomly split the given data into a training set (used to
train the model) and a test set, which is used for evaluation. Because here the test set
is held out from the original data, it is also common to refer to that as holdout set.
In order to split the data, we can use train_test_split function from the
sklearn.model_selection module (see (Scikit-split, 2023) for full speciÔ¨Åcation).
The train_test_split function receives a variable number of inputs (identiÔ¨Åed
by *array in its documentation) and as long as they all have the same number of
samples, it returns a list containing the train-test splits. The few following points
seems worthwhile to remember:
1. In using scikit-learn, we conventionally use X and y to refer to a data matrix and
targets, respectively;
2. Although train_test_split function can receive an arbitrary number of
sequences to split, in supervised learning it is common to use this function with
a data matrix and targets only;

4.5 Data Visualization
117
3. Although train_test_split function returns a list, it is common to use se-
quence unpacking (see Section 2.6.2) to name the results of the split;
4. By default train_test_split function shuÔ¨Ñes the data before splitting. This
is a good practice to avoid possible systematic biases in the data; however, to be
able to reproduce the results, it is also a good practice to set the seed of random
number generator. This is done by setting random_state to an integer;
5. The test_size argument of the function represents the proportion of data that
should be assigned to the test set. The default value of this parameter is 0.25,
which is a good rule of thumb if no other speciÔ¨Åc proportion is desired; and
6. It is a good practice to keep the proportion of classes in both the training and the
test sets as in the whole data. This is done by setting the stratify to variable
representing the target.
Here, we use stratiÔ¨Åed random split to divide the given data into 80% training and
20% test:
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test= train_test_split(iris.data, iris.
,‚Üítarget, random_state=100, test_size=0.2, stratify=iris.target)
print('X_train_shape: ' + str(X_train.shape) + '\nX_test_shape: ' +‚ê£
,‚Üístr(X_test.shape)\
+ '\ny_train_shape: ' + str(y_train.shape) + '\ny_test_shape: '‚ê£
,‚Üí+ str(y_test.shape))
X_train_shape: (120, 4)
X_test_shape: (30, 4)
y_train_shape: (120,)
y_test_shape: (30,)
X_train and y_train are the feature matrix and the target values used for
training, respectively. The feature matrix and the corresponding targets for evaluation
are stored in X_test and y_test, respectively. Let us count the number of class-
speciÔ¨Åc observations in the training data:
np.bincount(y_train)
array([40, 40, 40])
This shows that the equal proportion of classes in the given data is kept in both the
training and the test sets.
4.5 Data Visualization
For datasets in which the number of variables is not really large, visualization could
be a good exploratory analysis‚Äîit could reveal possible abnormalities or could
provide us with an insight into the hypothesis behind the entire experiment. In this

118
4 Supervised Learning in Practice: the First Application Using Scikit-Learn
regard, scatter plots can be helpful. We can still visualize scatter plots for three
variables but for more than that, we need to display scatter plots between all pairs
of variables in the data. These exploratory plots are known as pair plots. There are
various ways to plot pair plots in Python but an easy and appealing way is to use
pairplot() function from seaborn library. As this function expects a DataFrame
as the input, we Ô¨Årst convert the X_train and y_train arrays to dataframes and
concatenate them.
import pandas as pd
X_train_df = pd.DataFrame(X_train, columns=iris.feature_names)
y_train_df = pd.DataFrame(y_train, columns=['class'])
X_y_train_df = pd.concat([X_train_df, y_train_df], axis=1)
The pair plots are generated by:
import seaborn as sns
sns.pairplot(X_y_train_df, hue='class', height=2) # hue is set to the‚ê£
,‚Üíclass variable in the dataframe so that they are plotted in‚ê£
,‚Üídifferent color and we are able to distinguish classes
Fig. 4.1 presents the scatter plot of all pairs of features. The plots on diagonal
show the histogram for each feature across all classes. The Ô¨Ågure, at the very least,
shows that classiÔ¨Åcation of Iris Ô¨Çower in the collected sample is plausible based on
some of these features. For example, we can observe that class-speciÔ¨Åc histograms
generated based on petal width feature are fairly distinct. Further inspection of these
plots suggests that, for example, petal width could potentially be a better feature than
sepal width in discriminating classes. This is because the class-speciÔ¨Åc histograms
generated by considering sepal width are more mixed when compared with petal
width histograms. We may also be able to make similar suggestions about some
bivariate combinations of features. However, it is not easy to infer much about
higher order feature dependency (i.e., multivariate relationship) from these plots. As
a result, although visualization could also be used for selecting discriminating feature
subsets, it is generally avoided because we are restricted to low-order dependency
among features, while in many problems higher-order feature dependencies lead to
an acceptable level of prediction. On the other hand, in machine learning there exist
feature subset selection methods that are entirely data-driven and can detect low- or
high-order dependencies among features. These methods are discussed in Chapter
10. In what follows, we consider all four features to train a classiÔ¨Åer.
4.6 Feature Scaling (Normalization)
It is common that the values of features in a dataset come in diÔ¨Äerent scales. For
example, the range of some features could be from 0 to 1, while others may be in
the order of thousands or millions depending on what they represent and how they
are measured. In these cases, it is common to apply some type of feature scaling

4.6 Feature Scaling (Normalization)
119
5
6
7
8
sepal length (cm)
2.5
3.0
3.5
4.0
sepal width (cm)
2
4
6
petal length (cm)
4
6
8
sepal length (cm)
0.0
0.5
1.0
1.5
2.0
2.5
petal width (cm)
2
3
4
sepal width (cm)
2
4
6
8
petal length (cm)
0
1
2
petal width (cm)
class
0
1
2
Fig. 4.1: Pair plots generated using training set in the Iris classiÔ¨Åcation application
(also known as normalization) to the training data to make the scale of features
‚Äúcomparable‚Äù. This is because some important classes of machine learning models
such as neural networks, kNNs, ridge regression, to just name a few, beneÔ¨Åt from
feature scaling.
Even if we are not sure whether a speciÔ¨Åc ML rule beneÔ¨Åts from
scaling or not, it would be helpful to still scale the data because scaling is
not harmful and, at the same time, it facilitates comparing diÔ¨Äerent types of
models regardless of whether they can or can not beneÔ¨Åt from scaling.
Two common way for feature scaling include standardization and min-max scal-
ing. In standardization, Ô¨Årst the mean and the standard deviation of each feature is
found. Then for each feature, the mean of that feature is subtracted from all feature
values and the result of this subtraction is divided by the standard deviation of the
feature. This way the feature vector is centered around zero and will have a standard

120
4 Supervised Learning in Practice: the First Application Using Scikit-Learn
deviation of one. In min-max scaling, a similar subtraction and division is performed
except that the mean and the standard deviation of each feature are replaced with
the minimum and the range (i.e., maximum - minimum) of that feature, respectively.
This way each feature is normalized between 0 and 1. Although all these operations
could be easily achieved by naive Python codes, scikit-learn has a transformer for
this purpose. Nonetheless, in what follows we Ô¨Årst see how standardization is done
using few lines of codes that use built-in NumPy mean() and std() functions. Then
we deploy a scikit-learn transformer for doing that.
The following code snippet Ô¨Ånds the mean and the standard deviation for each
feature in the training set:
mean = X_train.mean(axis=0) # to take the mean across rows (for each‚ê£
,‚Üícolumn)
std = X_train.std(axis=0) # to take the std across rows (for each‚ê£
,‚Üícolumn)
X_train_scaled = X_train - mean # notice the utility of broadcasting
X_train_scaled /= std
Next, we observe that the mean of each feature is 0 (the reason for having such small
numbers instead of absolute 0 is the Ô¨Ånite machine precision):
X_train_scaled.mean(axis=0) # observe the mean is 0 now
array([-4.21884749e-16,
1.20042865e-16, -3.88578059e-16, -7.
,‚Üí04991621e-16])
And the standard deviations are 1:
X_train_scaled.std(axis=0) # observe the std is 1 now
array([1., 1., 1., 1.])
It is important to note that the ‚Äútest set‚Äù should not be used in any stage involved
in training of our classiÔ¨Åer, not even in a preprocessing stage such as normalization.
The main reason for this can be explained through the following four points:
‚Ä¢ Point 1: when a classiÔ¨Åer is trained, the most salient issue is the performance of
the classiÔ¨Åer on unseen (future) observations collected from the same applica-
tion. In other words, the entire worth of the classiÔ¨Åer depends on its performance
on unseen observations;
‚Ä¢ Point 2: because ‚Äúunseen‚Äù observations, as the name suggests, are not available
to us during the training, we use (the available) test set to simulate the eÔ¨Äect of
unseen observations for evaluating the trained classiÔ¨Åer;
‚Ä¢ Point 3: as a result of Point 2, in order to have an unbiased evaluation of the
classiÔ¨Åer using test set, the classiÔ¨Åer should classify observations in the test set
in precisely the same way it is used to classify unseen future observations; and
‚Ä¢ Point 4: unseen observations are not available to us and naturally they can not be
used in any training stage such as normalization, feature selection, etc.; therefore,
observations in the test set should not be used in any training stage either.

4.6 Feature Scaling (Normalization)
121
In this regard, a common mistake is to apply some data preprocessing such as
normalization before splitting the entire data into training and test sets. This causes
data leakage; that is, some information about the test set leak into the training
process. Based on the aforementioned Point 4, this is not legitimate because here
test set is used to determine the statistics used for normalizing the entire data and,
therefore, it is used in training the model, while unseen future observations can
not be naturally used in this way. In these situations, even referring to the set that
is supposed to be used for evaluation after the split as ‚Äútest set‚Äù is not legitimate
because that set was already used in training through normalization.
It is important to note that here we refer to ‚Äúnormalizing the entire data
before splitting‚Äù as an illegitimate practice from the standpoint of model
evaluation using test data, which is the most common reason to split the
data into two sets. However, a similar practice could be legitimate if we
view it from the standpoint of:
1) training a classiÔ¨Åer in semi-supervised learning fashion. An objective in
semi-supervised learning is to use both labeled and unlabeled data to train
a classiÔ¨Åer. Suppose we use the entire data for normalization and then we
divide that into two sets, namely Set A and Set B. Set A (its feature vectors
and their labels) is then used to train a classiÔ¨Åer and Set B is not used at this
stage. This classiÔ¨Åer is trained using both labeled data (Set A) and unlabeled
data (Set B). This is because for normalization we used both Set A and
Set B without using labels, but Set A (and the labels of observation within
that set) were used to train the classiÔ¨Åer. Although from the standpoint of
semi-supervised learning we have used a labeled set and unlabeled set in the
process of training the classiÔ¨Åer, it is perhaps a ‚Äúdumb‚Äù training procedure
because all valuable labels of observations within Set B are discarded, and
this set is only used for normalization purposes. Nevertheless, even after
training the classiÔ¨Åer using such a semi-supervised learning procedure, we
still need an independent test set to evaluate the performance of the classiÔ¨Åer.
2) model evaluation using a performance estimator with unknown proper-
ties. An important property of using test set for model evaluation is that it is
an unbiased estimator of model performance. This procedure is also known
as test-set estimator of performance. However, there are other performance
estimators. For example, another estimator is known as resubstitution es-
timator (discussed in Chapter 9). It is simply reusing the training data to
evaluate the performance of a trained model. Nonetheless, resubstitution has
an undesirable property of being usually strongly optimistically biased. That
being said, because resubstitution has an undesirable property, we can not
refer to that as an illegitimate estimation rule. When we refer to the practice
of ‚Äúnormalizing the data before splitting into two sets and then using one

122
4 Supervised Learning in Practice: the First Application Using Scikit-Learn
set in training and the other for evaluation‚Äù as an illegitimate practice, what
is really meant is that following this procedure and then framing that as
test-set estimator of performance is illegitimate. If we accept it, however,
as another performance metric, one that is less known and is expected to
be optimistically biased to some extent, then it is a legitimate performance
estimator.
Once the relevant statistics (here, the mean and the standard deviation) are esti-
mated from the training set, they can be used to normalize the test set:
X_test_scaled = X_test - mean
X_test_scaled /= std
Observe that the test set does not necessarily have a mean of 0 or standard deviation
of 1:
print(X_test_scaled.mean(axis=0))
print(X_test_scaled.std(axis=0))
[-0.20547406 -0.25637284 -0.03844742 -0.07491371]
[0.94499306 1.07303169 0.94790626 0.94534006]
Although the naive Python implementation as shown above is instructive, as
we said earlier scikit-learn has already transformers for implementing both stan-
dardization and min-max scaling. These are StandardScaler (for standardization)
and MinMaxScaler (for min-max scaling) classes from sklearn.preprocessing
module. Here we use StandardScaler to perform the standardization. To be able
to use these transformers (or any other estimator), we Ô¨Årst need to instantiate the
class into an object:
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
To estimate the mean and the standard deviation of each feature from the training set
(i.e., from X_train), we call the fit() method of the scaler object (afterall, any
transformer is an estimator and implements the fit() method):
scaler.fit(X_train)
StandardScaler()
The scaler object holds any information that the standardization algorithm im-
plemented in StandardScaler class extracts from X_train. The fit() method
returns the scaler object itself and modiÔ¨Åes that in place (i.e., stores the parameters
estimated from data). Next, we call the transform() method of the scaler object
to transform the training and test sets based on statistics extracted from the train-
ing set, and use X_train_scaled and X_test_scaled to refer to the transformed
training and test sets, respectively:

4.7 Model Training
123
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
print(X_test_scaled.mean(axis=0)) # observe that we get the same values‚ê£
,‚Üífor the mean and std of test set as in the naive implementation‚ê£
,‚Üíshown above
print(X_test_scaled.std(axis=0))
[-0.20547406 -0.25637284 -0.03844742 -0.07491371]
[0.94499306 1.07303169 0.94790626 0.94534006]
For later use and in order to avoid the above preprocessing steps, the training
and testing arrays could be saved using numpy.save() to binary Ô¨Åles. For NumPy
arrays this is generally more eÔ¨Écient than the usual ‚Äúpickling‚Äù supported by pickle
module. For saving multiple arrays, we can use numpy.savez(). We can provide
our arrays as keyword arguments to this function. In that case, they are saved in a
binary Ô¨Åle with arbitrary names that we provide as keywords. If we give them as
positional arguments, then they will be stored with names being arr_0, arr_1, etc.
Here we specify our arrays as keywords X and y :
np.savez('data/iris_train_scaled', X = X_train_scaled, y = y_train)
np.savez('data/iris_test_scaled', X = X_test_scaled, y = y_test)
numpy.save and numpy.savez add npy and npz extensions to the name of a
created Ô¨Åle, respectively. Later, arrays could be loaded by numpy.load(). Using
numpy.load() for npz Ô¨Åles, returns a dictionary-like object using which each array
can be accessed either by the keywords that we provide when saving the Ô¨Åle, or by
arr_0, arr_1, . . . , if positional arguments were used during the saving process.
4.7 Model Training
We are now in the position to train the actual machine learning model. For this
purpose, we use the k-nearest neighbors (kNN) classiÔ¨Åcation rule in its standard
form. To classify a test point, one can think that the kNN classiÔ¨Åer grows a spherical
region centered at the test point until it encloses k training samples, and classiÔ¨Åes
the test point to the majority class among these k training samples. For example, in
Fig. 4.2, 5NN assigns ‚Äúgreen‚Äù to the test observation because within the 5 nearest
observations to this test point, three are from the green class.
kNN classiÔ¨Åer is implemented in the KNeighborsClassifier class in the
sklearn.neighbors module. Similar to the way we used the StandardScaler
estimator earlier, we Ô¨Årst instantiate the KNeighborsClassifier class into an
object:

124
4 Supervised Learning in Practice: the First Application Using Scikit-Learn
‚àí1
0
1
2
3
‚àí1.5
‚àí1.0
‚àí0.5
0.0
0.5
1.0
1.5
2.0
Fig. 4.2: The working principle of kNN (here k = 5). The test point is identiÔ¨Åed by
√ó. The circle encloses 5 nearest neighbors of the test point.
from sklearn.neighbors import KNeighborsClassifier as kNN # giving an‚ê£
,‚Üíalias KNN for simplicity
knn = KNN(n_neighbors=3) # hyperparameter k is set to 3; that is, we‚ê£
,‚Üíhave 3NN
The constructors of many estimators take as arguments various hyperparameters,
which can aÔ¨Äect the estimator performance. In case of kNN, perhaps the most
important one is k, which is the number of nearest neighbors of a test point. In
KNeighborsClassifier constructor this is determined by n_neighbors parame-
ter. In the above code snippet, we set n_neighbors to 3, which implies we are using
3NN. Full speciÔ¨Åcation of KNeighborsClassifier class is found at (Scikit-kNN-
C, 2023). To train the model, we call the fit() method of the knn object. Because
this is an estimator used for supervised learning, its fit() method expects both the
feature matrix and the targets:
knn.fit(X_train_scaled, y_train)
KNeighborsClassifier(n_neighbors=3)
Similar to the use of fit() method for StandardScaler, the fit() method here
returns the modiÔ¨Åed knn object.

4.8 Prediction Using the Trained Model
125
4.8 Prediction Using the Trained Model
As said before, some estimators in scikit-learn are predictors; that is, they can make
prediction by implementing the predict() method. KNeighborsClassifier is
also a predictor and, therefore, implements predict(). Here we use this method
to make prediction on a new data point. Suppose we have the following data point
measured in the original scales as in the original training set X_train:
x_test = np.array([[5.5, 2, 1.1, 0.6]]) # same as: np.array([5.5, 2, 1.
,‚Üí1, 0.6]).reshape(1,4)
x_test.shape
(1, 4)
Recall that scikit-learn always assumes two-dimensional NumPy arrays of shape
sample size √ó feature size‚Äîthis is the reason in the code above the test point x_test
is placed in a two-dimensional NumPy array of shape (1, 4). However, before
making prediction, we need to scale the test data point using the same statistics used
to scale the training set; after all, the knn classiÔ¨Åer was trained using the transformed
dataset and it classiÔ¨Åes data points in the transformed space. This is achieved by:
x_test_scaled = scaler.transform(x_test)
x_test_scaled
array([[-0.45404756, -2.53437275, -1.50320017, -0.79582245]])
Now, we can predict the label by:
y_test_prediction = knn.predict(x_test_scaled)
print('knn predicts: ' + str(iris.target_names[y_test_prediction])) #‚ê£
,‚Üíto convert the prediction (y_test_prediction) to the names of Iris‚ê£
,‚Üíflower
knn predicts: ['versicolor']
We can also give several sample points as the argument to the predict() method.
In that case, we receive the assigned label for each of them:
y_test_predictions = knn.predict(X_test_scaled)
print('knn predicts: ' + str(iris.target_names[y_test_predictions])) #‚ê£
,‚Üífancy indexing in Section 3.1.4
knn predicts: ['versicolor' 'versicolor' 'versicolor' 'virginica'‚ê£
,‚Üí'setosa' 'virginica' 'versicolor' 'setosa' 'versicolor' 'versicolor'‚ê£
,‚Üí'versicolor' 'virginica' 'virginica' 'setosa' 'virginica' 'setosa'‚ê£
,‚Üí'setosa' 'versicolor' 'setosa' 'virginica' 'setosa' 'versicolor'‚ê£
,‚Üí'versicolor' 'setosa' 'versicolor' 'setosa' 'setosa' 'versicolor'‚ê£
,‚Üí'virginica' 'versicolor']

126
4 Supervised Learning in Practice: the First Application Using Scikit-Learn
The above sequence of operations, namely, instantiating the class KNN, Ô¨Åtting, and
predicting, can be combined as the following one liner pattern known as method
chaining:
y_test_predictions = KNN(n_neighbors=3).fit(X_train_scaled, y_train).
,‚Üípredict(X_test_scaled)
print('knn predicts: ' + str(iris.target_names[y_test_predictions]))
knn predicts: ['versicolor' 'versicolor' 'versicolor' 'virginica'‚ê£
,‚Üí'setosa' 'virginica' 'versicolor' 'setosa' 'versicolor' 'versicolor'‚ê£
,‚Üí'versicolor' 'virginica' 'virginica' 'setosa' 'virginica' 'setosa'‚ê£
,‚Üí'setosa' 'versicolor' 'setosa' 'virginica' 'setosa' 'versicolor'‚ê£
,‚Üí'versicolor' 'setosa' 'versicolor' 'setosa' 'setosa' 'versicolor'‚ê£
,‚Üí'virginica' 'versicolor']
4.9 Model Evaluation (Error Estimation)
There are various rules and metrics to assess the performance of a classiÔ¨Åer. Here
we use the simplest and perhaps the most intuitive one; that is, the proportion of
misclassiÔ¨Åed points in a test set. This is known as the test-set estimator of error rate.
In other words, the proportion of misclassiÔ¨Åed observations in the test set is indeed
an estimate of classiÔ¨Åcation error rate denoted Œµ, which is deÔ¨Åned as the probability
of misclassiÔ¨Åcation by the trained classiÔ¨Åer.
Let us Ô¨Årst formalize the deÔ¨Ånition of error rate for binary classiÔ¨Åcation. Let X
and Y represent a random feature vector and a binary random variable representing
the class variable, respectively. Because Y is a discrete random variable and X is a
continuous feature vector, we can characterize the joint distribution of X and Y (this
is known as joint feature-label distribution) as:
P(X ‚ààE,Y = i) =
‚à´
E
p(x|Y = i)P(Y = i)dx,
i = 0, 1,
(4.1)
where p(x|Y = i) is known as the class-conditional probability density function,
which shows the (relative) likelihood of X being close to a speciÔ¨Åc value x given
Y = i, and where E represents an event, which is basically a subset of the sample
space to which we can assign probability (in technical terms a Borel-measurable
set). Intuitively, P(X,Y) shows the frequency of encountering particular pairs of
(X,Y) in practice. Furthermore, in (4.1), P(Y = i) is the prior probability of class i,
which quantiÔ¨Åes the probability that a randomly drawn sample from the population
of entities across all classes belongs to class i.
Given a training set Str, we train a classiÔ¨Åer œà : Rp ‚Üí{0, 1}, which maps real-
izations of X to realizations of Y. Let œà(X) denote the act of classifying (realizations
of) X by a speciÔ¨Åc trained classiÔ¨Åer. Because X is random, œà(X), which is either 0
or 1, is random as well. Let E0 denote all events for which œà(X) gives label 0. A

4.9 Model Evaluation (Error Estimation)
127
probabilistic question to ask is what would be the joint probability of all those events
and Y = 1? Formally, to answer this question, we need to Ô¨Ånd,
P(X ‚ààE0,Y = 1)
1=
‚à´
E0
p(x|Y = 1)P(Y = 1)dx ‚â°
‚à´
œà(x)=0
p(x|Y = 1)P(Y = 1)dx,
(4.2)
where
1= is a direct consequence of (4.1). Similarly, let E1 denote all events for which
œà(X) gives label 1. We can ask a similar probabilistic question; that is, what would
be the joint probability of Y = 0 and E1? In this case, we need to Ô¨Ånd
P(X ‚ààE1,Y = 0) =
‚à´
E1
p(x|Y = 0)P(Y = 0)dx ‚â°
‚à´
œà(x)=1
p(x|Y = 0)P(Y = 0)dx .
(4.3)
At this stage, it is straightforward to see that the probability of misclassiÔ¨Åcation Œµ is
indeed obtained by adding probabilities (4.2) and (4.3); that is,
Œµ = P(X ‚ààE0,Y = 1) + P(X ‚ààE1,Y = 0) ‚â°P(œà(X) , Y).
(4.4)
Nonetheless, in practical settings Œµ is almost always unknown because of the un-
known nature of P(X ‚ààEi,Y = i). The test-set error estimator is a way to estimate Œµ
using a test set.
Suppose we apply the classiÔ¨Åer on a test set Ste that contains m observations with
their labels. Let k denote the number of observations in Ste that are misclassiÔ¨Åed by
the classiÔ¨Åer. The test-set error estimate, denoted ÀÜŒµte, is given by
ÀÜŒµte = k
m .
(4.5)
Rather than reporting the error estimate, it is also common to report the accuracy
estimate of a classiÔ¨Åer. The accuracy, denoted acc, and its test-set estimate, denoted
ÀÜ
accte, are given by:
acc = 1 ‚àíŒµ,
ÀÜ
accte = 1 ‚àíÀÜŒµte .
(4.6)
For example, a classiÔ¨Åer with an error rate of 15% has an accuracy of 85%.
Let us calculate the test-set error estimate of our trained kNN classiÔ¨Åer. In this
regard, we can compare the actual labels within the test set with the predicted labels
and then Ô¨Ånd the proportion of misclassiÔ¨Åcation:
errors = (y_test_predictions != y_test)
errors

128
4 Supervised Learning in Practice: the First Application Using Scikit-Learn
array([False, False,
True, False, False, False, False, False, False,
False, False, False, False, False, False, False, False, False,
False, False, False, False, False, False,
True, False, False,
True, False, False])
The classiÔ¨Åer misclassiÔ¨Åed three data points out of 30 that were part of the test set;
therefore, ÀÜŒµte = 3
30 = 0.1:
error_est = sum(errors)/errors.size
print('The error rate estimate is: {:.2f}'.format(error_est) + '\n'\
'The accuracy is: {:.2f}'.format(1-error_est))
The error rate estimate is: 0.10
The accuracy is: 0.90
In the above code, we use the place holder { } and format speciÔ¨Åer 0.2f to specify the
number of digits after decimal point. The ‚Äú:‚Äù before .2f separates the format speciÔ¨Åer
from the rest of the replacement Ô¨Åeld (if any option is set) within the { }.
Using scikit-learn built-in functions from metrics module many performance
metrics can be easily calculated. A complete list of these metrics supported by scikit-
learn is found at (Scikit-eval, 2023). Here we only show how the accuracy estimate
can be obtained in scikit-learn. For this purpose, there are two options: 1) using the
accuracy_score function; and 2) using the score method of the classiÔ¨Åer.
The accuracy_score function expects the actual labels and predicted labels as
arguments:
from sklearn.metrics import accuracy_score
print('The accuracy is {:.2f}'.format(accuracy_score(y_test,‚ê£
,‚Üíy_test_predictions)))
The accuracy is 0.90
All classiÔ¨Åers in scikit-learn also have a score method that given a test data and its
labels, returns the classiÔ¨Åer accuracy; for example,
print('The accuracy is {:.2f}'.format(knn.score(X_test_scaled, y_test)))
The accuracy is 0.90
In Chapter 9, we will discuss the deÔ¨Ånition and calculation of many important
performance metrics for both classiÔ¨Åcation and regression.
Exercises:
Exercise 1: In the Iris classiÔ¨Åcation problem, remove the stratify=iris.target
when thedata issplit totrainingand test.Observe the change innp.bincount(y_train).

4.9 Model Evaluation (Error Estimation)
129
Are the number of samples in each class still the same as the original dataset?
Exercise 2: Here we would like to work with another classiÔ¨Åcation application. In
this regard, we work with digits dataset that comes with scikit-learn. This is a
dataset of 8√ó8 images of handwritten digits. More information including the sample
size can be found at (Scikit-digits, 2023).
1) Write a piece of code to load the dataset and show the Ô¨Årst 100 images as in Fig.
4.3.
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
0
5
Fig. 4.3: The Ô¨Årst 100 images in digits dataset
Hint: to show an image we can use matplotlib.axes.Axes.imshow(). As-
suming ax is the created axes object, then the Ô¨Årst image, for example, could be
shown using the values of images key of the returned Bunch object or even us-
ing its data key, which is Ô¨Çattened image to create feature vectors for classiÔ¨Åca-
tion: ax.imshow(bunch.data[0].reshape(8,8)) where bunch is the returned
Bunch object. Read documentation of imshow() at (matplotlib-imshow, 2023) to
use appropriate parameters to display grayscale images.
2) Implement the train-test splittingwith atestsize of0.25 (setthe random_state=42),
properly perform standardization and transformation of datasets, train a 3NN on
training set, and report the accuracy on the test set.
Exercise 3: A problem in machine learning is to transform categorical variables into
eÔ¨Écient numerical features. This focus is warranted due to the ubiquity of categorical
data in real-world applications but, on the contrary, the development of many machine
learning methods based on the assumption of having numerical variables. This
stage, which is transforming categorical variables to numeric variables is known as
encoding. In the Iris classiÔ¨Åcation application, we observed that the class variable

130
4 Supervised Learning in Practice: the First Application Using Scikit-Learn
was already encoded. In this exercise, we will practice encoding in conjunction with
a dataset collected for a business application.
Working with clients and understanding the factors that can improve the business
is part of marketing data science. One goal in marketing data science is to keep
customers (customer retention), which could be easier in some aspects than attracting
new customers. In this exercise, we see a case study on customer retention.
A highly competitive market is communication services. Following the break-up
of AT&T in 80‚Äôs, customers had a choice to keep their carrier (AT&T) or move
to another carrier. AT&T tried to identify factors relating to customer choice of
carriers. For this purpose, they collected customer data from a number of sources
including phone interviews, household billing, and service information recorded in
their databases. The dataset that we use here is based on this eÔ¨Äort and is taken from
(Miller, 2015). It includes nine feature variables and a class variable (pick). The
goal is to construct a classiÔ¨Åer that predicts customers who switch to another service
provider or stay with AT&T.
The dataset is stored in att.csv, which is part of the ‚Äúdata‚Äù folder. Below is
description of the variables in the dataset:
pick: customer choice (AT&T or OCC [short for Other Common Carrier])
income: household income in thousands of dollars
moves: number of times the household moved in the last Ô¨Åve years
age: age of respondent in years (18-24, 25-34, 35-44, 45-54, 55-64, 65+)
education: <HS (less than high school); HS (high school graduate); Voc (vocational
school); Coll (Some College); BA (college graduate); >BA (graduate school)
employment: D (disabled); F (full-time); P (part-time); H (homemaker); R (re-
tired); S (student); U (unemployed)
usage: average phone usage per month
nonpub: does the household have an unlisted phone number
reachout: does the household participate in the AT&T ‚ÄúReach Out America‚Äù
phone service plan?
card: does the household have an ‚ÄúAT&T Calling Card‚Äù?
1) load the data and pick only variables named employment, usage, reachout, and
card as features to use in training the classiÔ¨Åer (keep ‚Äúpick‚Äù because it is the
class variable).
2) Ô¨Ånd the rate of missing value for each variable (features and class variable).
3) remove any feature vector and its corresponding class with at least one missing
entry. What is the sample size now?
4) use stratiÔ¨Åed random split to divide the data into train and test sets where the
test set size is 20% of the dataset. Set random_state=100.
5) There are various strategies for encoding with diÔ¨Äerent pros and cons. An
eÔ¨Écient strategy in this regard is ordinal encoding. In this strategy, a variable
with N categories, will be converted to a numeric variable with integers from 0 to
N ‚àí1. Use OrdinalEncoder() and LabelEncoder() transformers to encode
the categorical features and the class variable, respectively‚ÄîLabelEncoder
works similarly to OrdinalEncoder except that it accepts one-dimensional

4.9 Model Evaluation (Error Estimation)
131
arrays and Series (so use that to encode the class variable). Hint: you need to Ô¨Åt
encoder on training set and transform both training and test sets.
6) train a 7NN classiÔ¨Åer using encoded training set and evaluate that on the encoded
test set.
Exercise 4: We have a dataset D using which we would like to train and evaluate a
classiÔ¨Åer. Suppose
‚Ä¢ we divide D into a training set denoted Dtrain and a test set denoted Dtest
‚Ä¢ apply normalization on Dtrain using parameters estimated from Dtrain to create
Dnorm
train, which denotes the normalized training set.
Which of the following sequence of actions is a legitimate machine learning practice?
A)
‚Ä¢ apply normalization on Dtest using parameters estimated from Dtrain to create
the normalized test set denoted Dnorm
test
‚Ä¢ train the classiÔ¨Åer on Dnorm
train and assess its performance on Dtest
B)
‚Ä¢ apply normalization on Dtest using parameters estimated from Dtest to create
the normalized test set denoted Dnorm
test
‚Ä¢ train the classiÔ¨Åer on Dnorm
train and assess its performance on Dnorm
test
C)
‚Ä¢ apply normalization on Dtest using parameters estimated from Dtrain to create
the normalized test set denoted Dnorm
test
‚Ä¢ train the classiÔ¨Åer on Dnorm
train and assess its performance on Dnorm
test
D)
‚Ä¢ apply normalization on Dtest using parameters estimated from Dtrain to create
the normalized test set denoted Dnorm
test
‚Ä¢ train the classiÔ¨Åer on Dtrain and assess its performance on Dnorm
test

Chapter 5
k-Nearest Neighbors
There are many predictive models in machine learning that can be used in the design
process. We start the introduction to these predictive models from kNN (short for k-
Nearest Neighbors) not only because of its simplicity, but also due to its long history
in machine learning. As a result, in this chapter we formalize the kNN mechanism
for both classiÔ¨Åcation and regression and we will see various forms of kNN.
5.1 ClassiÔ¨Åcation
kNN is one of the oldest rules with a simple working mechanism and has found
many applications since its conception in 1951 (Fix and Hodges, 1951). To for-
malize the kNN mathematical mechanism, it is common to consider the case of
binary classiÔ¨Åcation where the values of target variable y are encoded as 0 and
1. Furthermore, to mathematically characterize the kNN mechanism, we introduce
some additional notations. Suppose we use a distance metric (e.g., Euclidean dis-
tance) to quantify distance of all feature vectors (observations) in a training set
Str = {(x1, y1), (x2, y2), . . ., (xn, yn)} to a given feature vector x (test observation)
for which we obtain an estimate of y denoted ÀÜy. We denote the ith nearest observation
to x as x(i)(x) with its associated label as y(i)(x). Note that both x(i)(x) and y(i)(x) are
functions of x because the distance is measured with respect to x. With this notation,
x(1)(x), for example, means the nearest observation within Str to x.
5.1.1 Standard kNN ClassiÔ¨Åer
In a binary classiÔ¨Åcation problem, the standard kNN classiÔ¨Åer denoted as œà(x) is
given by
133
¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
A. Zollanvari, Machine Learning with Python, https://doi.org/10.1007/978-3-031-33342-2_5 

134
5 k-Nearest Neighbors
ÀÜy = œà(x) =
(
1
if √çk
i=1
1
k I{y(i)(x)=1} > √çk
i=1
1
k I{y(i)(x)=0} ,
0
otherwise,
(5.1)
where k denotes the number of nearest neighbors with respect to the given feature
vector x and IA is the indicator of event A, which is 1 when A happens, and 0
otherwise. Here we write ÀÜy = œà(x) to remind us the fact that the function œà(x) (i.e.,
the classiÔ¨Åer) is essentially as estimator of the class label y. Nevertheless, hereafter,
we denote all classiÔ¨Åers simply as œà(x). In addition, in what follows, the standard
kNN classiÔ¨Åer is used interchangeably with kNN classiÔ¨Åer.
Let us elaborate more on (5.1). The term √çk
i=1 I{y(i)(x)=1} in (5.1) counts the number
of training observations with label 1 among the k nearest neighbors of x. This is then
compared with √çk
i=1 I{y(i)(x)=0}, which is the number of training observations with
label 0 among the k nearest neighbors of x, and if √çk
i=1 I{y(i)(x)=1} > √çk
i=1 I{y(i)(x)=0},
then œà(x) = 1 (i.e., kNN assigns label 1 to x); otherwise, œà(x) = 0. As we can see
in (5.1), the factor 1
k does not have any eÔ¨Äect on the process of decision making
in the classiÔ¨Åer and it can be canceled out safely from both sides of the inequality.
However, having that in (5.1) better reÔ¨Çects the relationship between the standard
kNN classiÔ¨Åer and other forms of kNN that we will see later in this chapter. For
multiclass classiÔ¨Åcation problems with c classes j = 0, 1, . . ., c‚àí1, (e.g., in Iris Ô¨Çower
classiÔ¨Åcation problem we had c = 3), we again identify the k nearest neighbors of
x, count the number of labels among these k training samples, and classify x to the
majority class among these k observations; that is to say,
ÀÜy = œà(x) = argmax
j
k
√ï
i=1
1
k I{y(i)(x)=j} .
(5.2)
In binary classiÔ¨Åcation applications, it is common to avoid an even k to prevent
ties in (5.1). For example, in these applications it is common to see a kNN with k
being 3 or 5 (i.e., using 3NN and 5NN rules); however, in general k should be treated
as a hyperparameter and tuned via a model selection process (model selection will
be discussed in Chapter 9). In general, the larger k, the smoother are the decision
boundaries. These are boundaries between decision regions (i.e., the c partions of
the feature space obtained by the classiÔ¨Åer). In Chapter 4 we have already seen
how kNN classiÔ¨Åer is used and implemented in scikit-learn. Here we examine the
eÔ¨Äect of k on the decision boundaries and accuracy of kNN on the scaled Iris data
prepared in Chapter 4. In this regard, we Ô¨Årst load the training and test sets that were
preprocessed there:
import numpy as np
arrays = np.load('data/iris_train_scaled.npz')
X_train = arrays['X']
y_train = arrays['y']
arrays = np.load('data/iris_test_scaled.npz')
X_test = arrays['X']

5.1 ClassiÔ¨Åcation
135
y_test = arrays['y']
print('X shape = {}'.format(X_train.shape) + '\ny shape = {}'.
,‚Üíformat(y_train.shape))
X shape = (120, 4)
y shape = (120,)
For illustration purposes, we only consider the Ô¨Årst two features in data; that is, the
Ô¨Årst and second features (columns) in X_train and X_test:
X_train = X_train[:,[0,1]]
X_test = X_test[:,[0,1]]
X_train.shape
(120, 2)
To plot the decision regions, we use the same technique discussed in Example
3.5; that is, we Ô¨Årst create a grid, train a classiÔ¨Åer using training data, classify each
point in the grid using the trained classiÔ¨Åer, and then plot the decision regions based
on the assigned labels to each point in the grid:
%matplotlib inline
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn.neighbors import KNeighborsClassifier as KNN
color = ('aquamarine', 'bisque', 'lightgrey')
cmap = ListedColormap(color)
mins = X_train.min(axis=0) - 0.1
maxs = X_train.max(axis=0) + 0.1
x = np.arange(mins[0], maxs[0], 0.01)
y = np.arange(mins[1], maxs[1], 0.01)
X, Y = np.meshgrid(x, y)
coordinates = np.array([X.ravel(), Y.ravel()]).T
fig, axs = plt.subplots(2, 2, figsize=(6, 4), dpi = 200)
fig.tight_layout()
K_val = [1, 3, 9, 36]
for ax, K in zip(axs.ravel(), K_val):
knn = KNN(n_neighbors=K)
knn.fit(X_train, y_train)
Z = knn.predict(coordinates)
Z = Z.reshape(X.shape)
ax.tick_params(axis='both', labelsize=6)
ax.set_title(str(K) + 'NN Decision Regions', fontsize=8)
ax.pcolormesh(X, Y, Z, cmap = cmap, shading='nearest')
ax.contour(X ,Y, Z, colors='black', linewidths=0.5)

136
5 k-Nearest Neighbors
ax.plot(X_train[y_train==0, 0], X_train[y_train==0, 1],'g.',‚ê£
,‚Üímarkersize=4)
ax.plot(X_train[y_train==1, 0], X_train[y_train==1, 1],'r.',‚ê£
,‚Üímarkersize=4)
ax.plot(X_train[y_train==2, 0], X_train[y_train==2, 1],'k.',‚ê£
,‚Üímarkersize=4)
ax.set_xlabel('sepal length (normalized)', fontsize=7)
ax.set_ylabel('sepal width (normalized)', fontsize=7)
print('The accuracy for K={} on the training data is {:.3f}'.
,‚Üíformat(K, knn.score(X_train, y_train)))
print('The accuracy for K={} on the test data is {:.3f}'.format(K,‚ê£
,‚Üíknn.score(X_test, y_test)))
for ax in axs.ravel():
ax.label_outer() # to show the x-label and the y-label for the last‚ê£
,‚Üírow and the left column, respectively
The accuracy for k=1 on the training data is 0.933
The accuracy for k=1 on the test data is 0.633
The accuracy for k=3 on the training data is 0.842
The accuracy for k=3 on the test data is 0.733
The accuracy for k=9 on the training data is 0.858
The accuracy for k=9 on the test data is 0.800
The accuracy for k=36 on the training data is 0.783
The accuracy for k=36 on the test data is 0.800
As we can see in Fig. 5.1, a larger value of the hyperparameter k generally leads to
smoother decision boundaries. That being said, a smoother decision region does not
necessarily mean a better classiÔ¨Åer performance.
5.1.2 Distance-Weighted kNN ClassiÔ¨Åer
The classiÔ¨Åcation formulated in (5.1) is the mechanism of the standard kNN classiÔ¨Åer.
There is another form of kNN known as distance-weighted kNN (DW-kNN) (Dudani,
1976). In this form, the k nearest neighbors of a test observation x are weighted
according to their distance from x. The idea is that observations that are closer to x
should impose a higher inÔ¨Çuence on decision making. A simple weighting scheme
for this purpose is to weight observations based on the inverse of their distance to x.
With this weighting scheme, DW-kNN classiÔ¨Åer becomes:
œà(x) =
(
1
if 1
D
√çk
i=1
1
d[x(i)(x), x] I{y(i)(x)=1} > 1
D
√çk
i=1
1
d[x(i)(x), x] I{y(i)(x)=0} ,
0
otherwise,
(5.3)
where

5.1 ClassiÔ¨Åcation
137
‚àí2
‚àí1
0
1
2
sepal width (normalized)
1NN Decision Regions
3NN Decision Regions
‚àí2
‚àí1
0
1
2
sepal length (normalized)
‚àí2
‚àí1
0
1
2
sepal width (normalized)
9NN Decision Regions
‚àí2
‚àí1
0
1
2
sepal length (normalized)
36NN Decision Regions
Fig. 5.1: kNN decision regions and boundaries for k = 1, 3, 9, 36 in the Iris classiÔ¨Å-
cation problem.
D =
k
√ï
i=1
1
d[x(i)(x), x] ,
(5.4)
and where d[x(i)(x), x] denotes the distance of x(i)(x) from x. If we think of each
weight 1/d[x(i)(x), x] as a ‚Äúscore‚Äù given to the training observation x(i)(x), then DW-
kNN classiÔ¨Åes the test observation x to the class with the largest ‚Äúcumulative score‚Äù
given to its observations that are among the k nearest neighbors of x. At this stage,
it is straightforward to see that the standard kNN given by (5.1) is a special form of
DW-kNN where the k nearest neighbor have equal contribution to voting; that is,
when d[x(i)(x), x] = 1 for all k nearest neighbors, which makes D = k.
In some settings, DW-kNN may end up behaving similarly to a simple nearest-
neighbor rule (1NN) because training observations that are very close to a test
observation will take very large weights. In practice though, the choice of using
DW-kNN or standard kNN, or even the choice of k are matters that can be decided in
the model selection phase (Chapter 9). In scikit-learn (as of version 1.2.2), DW-kNN
can be implemented by setting the weights parameter of KNeighborsClassifier
to 'distance' (the default value of weights is 'uniform', which corresponds to
the standard kNN).

138
5 k-Nearest Neighbors
5.1.3 The Choice of Distance
To identify the nearest neighbors of an observation, there are various choices
of distance metrics that can be used either in standard kNN or DW-kNN. Let
xi = [x1i, x2i, . . ., xqi]T and xj = [x1j, x2j, . . ., xqj]T denote two q-dimensional
vectors where xli denotes the element at the lth dimension of xi. We can deÔ¨Åne
various distances between these vectors. Three popular choices of distances are Eu-
clidean, Manhattan, and Minkowski denoted as dE[xi, xj], dMa[xi, xj], and dMi[xi, xj],
respectively, and are given by
dE[xi, xj] =
v
u
t q
√ï
l=1
(xli ‚àíxlj)2 ,
(5.5)
dMa[xi, xj] =
q
√ï
l=1
|xli ‚àíxlj|,
(5.6)
dMi[xi, xj] =  q
√ï
l=1
|xli ‚àíxlj|p 1
p .
(5.7)
In what follows, to specify the choice of distance in the kNN classiÔ¨Åer, the
name of the distance is preÔ¨Åxed to the classiÔ¨Åer; for example, Manhattan-kNN clas-
siÔ¨Åer and Manhattan-weighted kNN classiÔ¨Åer refer to the standard kNN and the
DW-kNN classiÔ¨Åers with the choice of Manhattan distance. When this choice is
not indicated speciÔ¨Åcally, Euclidean distance is meant to be used. The Minkowski
distance has an order p, which is an arbitrary integer. When p = 1 and p = 2,
Minkowski distance reduces to the Manhattan and the Euclidean distances, re-
spectively. In KNeighborsClassifier, the choice of distance is determined by
the metric parameter. Although the default metric in KNeighborsClassifier
is Minkowski, there is another parameter p, which determines the order of the
Minkowski distance, and has a default value of 2. This means that the default metric
of KNeighborsClassifier is indeed the Euclidean distance.
5.2 Regression
5.2.1 Standard kNN Regressor
The kNN regressor f (x) is given by
ÀÜy = f (x) =
k
√ï
i=1
1
k y(i)(x) .
(5.8)

5.2 Regression
139
Similar to (5.1), here ÀÜy = f (x) is written to emphasize that function f (x) is essentially
an estimator of y. Nevertheless, hereafter, we denote our regressors simply as f (x).
From (5.8), the standard kNN regressor estimates the target of a given x as the average
of targets of the k nearest neighbors of x. We have already seen the functionality
of kNN classiÔ¨Åer through the Iris classiÔ¨Åcation application. To examine the kNN
functionality in regression, in the next section, we use another well-known dataset
that suits regression applications.
5.2.2 A Regression Application Using kNN
The Dataset: Here we use the California Housing dataset, which includes the
median house price (in $100,000) of 20640 California districts with 8 features that
can be used to predict the house price. The data was originally collected in (Kelley
Pace and Barry, 1997) and suits regression applications because the target, which is
the median house price of a district (identiÔ¨Åed by ‚ÄúMedHouseVal‚Äù in the data), is a
numeric variable. Here we Ô¨Årst load the dataset:
from sklearn import datasets
california = datasets.fetch_california_housing()
print('california housing data shape: '+ str(california.data.shape) + \
'\nfeature names: ' + str(california.feature_names) + \
'\ntarget name: ' + str(california.target_names))
california housing data shape: (20640, 8)
feature names: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms',‚ê£
,‚Üí'Population', 'AveOccup', 'Latitude', 'Longitude']
target name: ['MedHouseVal']
As mentioned in Section 4.3, using the DESCR key we can check some details
about the dataset. Here we use this key to list the name and meaning of variables in
the data:
print(california.DESCR[:975])
.. _california_housing_dataset:
California Housing dataset
--------------------------
**Data Set Characteristics:**
:Number of Instances: 20640
:Number of Attributes: 8 numeric, predictive attributes and the target
:Attribute Information:
- MedInc
median income in block group

140
5 k-Nearest Neighbors
- HouseAge
median house age in block group
- AveRooms
average number of rooms per household
- AveBedrms
average number of bedrooms per household
- Population
block group population
- AveOccup
average number of household members
- Latitude
block group latitude
- Longitude
block group longitude
:Missing Attribute Values: None
This dataset was obtained from the StatLib repository.
https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html
The target variable is the median house value for California districts,
expressed in hundreds of thousands of dollars ($100,000).
‚ÄúLatitude‚Äù and ‚ÄúLongitude‚Äù identify the centroid of each district in the data, and
‚ÄúMedInc‚Äù values are in $10,000.
Preprocessing and an Exploratory Analysis: We split the data into training and
test sets. Compared with the Iris classiÔ¨Åcation problem, we use the default 0.25
test_size and do not set stratify to any variable‚Äîafter all, random sampling
with stratiÔ¨Åcation requires classes whereas in regression, there are no ‚Äúclasses‚Äù.
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test= train_test_split(california.data,‚ê£
,‚Üícalifornia.target, random_state=100)
print('X_train_shape: ' + str(X_train.shape) + '\nX_test_shape: ' +‚ê£
,‚Üístr(X_test.shape)\
+ '\ny_train_shape: ' + str(y_train.shape) + '\ny_test_shape: '‚ê£
,‚Üí+ str(y_test.shape))
X_train_shape: (15480, 8)
X_test_shape: (5160, 8)
y_train_shape: (15480,)
y_test_shape: (5160,)
Similar to Section 4.6, we use standardization to scale the training and the test sets.
In this regard, the scaler object is trained using the training set, and is then used to
transform both the training and the test sets:
scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

5.2 Regression
141
In Chapter 4, we used pair plots as an exploratory analysis. Another exploratory
analysis could be to look into Pearson‚Äôs correlation coeÔ¨Écients between predictors
and the response or even between predictors themselves.
Consider two random variables X and Y. The Pearson‚Äôs correlation
coeÔ¨Écient œÅ between them is deÔ¨Åned as
œÅ = Cov[X,Y]
œÉXœÉY
,
(5.9)
where
Cov[X,Y] = E

(X ‚àíE[X])(Y ‚àíE[Y])

,
(5.10)
and where E[Z] and œÉZ are the expected value and the standard deviation
of Z, respectively. As the covariance Cov[X,Y] measures the joint variation
between two variables X and Y, œÅ in (5.9) represents a normalized form of
their joint variability. However, in practice, we can not generally compute
(5.9) because it depends on the unknown actual expectations and standard
deviations. Therefore, we need to estimate that from a sample.
Suppose we collect a sample from the joint distribution of X and Y. This
sample consists of n pairs of observations {(x1, y1), (x2, y2), . . ., (xn, yn)};
that is, when the value of X is xi, the value of Y is yi. The sample Pearson‚Äôs
correlation coeÔ¨Écient r is an estimate of œÅ and is given by
r =
√çn
i=1(xi ‚àí¬Øx)(yi ‚àí¬Øy)
q√çn
i=1(xi ‚àí¬Øx)2
q√çn
i=1(yi ‚àí¬Øy)2
,
(5.11)
where ¬Øx = 1
n
√çn
i=1 xi. Note that even if change xi to axi + b and yi to cyi + d
where a, b, c and d are all constants,r does not change. In other words, sample
Pearson‚Äôs correlation coeÔ¨Écient is scale and location invariant. Another
implication of this point is that the sample Pearson‚Äôs correlation coeÔ¨Écient
inherently possesses data normalization (and, therefore, if desired, it could
be done even before normalization).
The main shortcoming of Pearson‚Äôs correlation coeÔ¨Écient is that it is not
a completely general measure of the strength of a relationship between two
random variables. Instead it is a measure of the degree of linear relationship
between them. This is because ‚àí1 ‚â§œÅ ‚â§1 but œÅ = 1 or ‚àí1 iÔ¨Äy = ax + b
for some constants a and b. Therefore, a |œÅ| < 1 indicates only that the
relationship is not completely linear, but there could be still a strong nonlinear
relationship between x and y.

142
5 k-Nearest Neighbors
We can easily calculate pairwise correlation coeÔ¨Écients between all variables
using pandas.DataFrame.corr(). To do so, we Ô¨Årst create a DataFrame from
X_train_scaled and y_train, and add feature names and the target name as the
column labels (to easily identify the target name, we rename it to MEDV):
import seaborn as sns
import matplotlib.pyplot as plt
california_arr = np.concatenate((X_train_scaled, y_train.
,‚Üíreshape(-1,1)), axis=1) # np.concatenate((a1, a2)) concatenate array‚ê£
,‚Üía1 and a2 along the specified axis
california_pd = pd.DataFrame(california_arr, columns=[*california.
,‚Üífeature_names, 'MEDV'])
california_pd.head().round(3)
MedInc
HouseAge
AveRooms
AveBedrms
Population
AveOccup
Latitude‚ê£
,‚Üí\
0
1.604
-0.843
0.974
-0.086
0.068
0.027
-0.865
1
-0.921
0.345
-0.197
-0.238
-0.472
-0.068
1.647
2
-0.809
1.849
-0.376
-0.037
-0.516
-0.082
1.675
3
0.597
-0.289
-0.437
-0.119
-0.680
-0.121
1.008
4
0.219
0.107
0.187
0.122
-0.436
-0.057
0.956
Longitude
MEDV
0
0.883
2.903
1
-0.999
0.687
2
-0.741
1.097
3
-1.423
4.600
4
-1.283
2.134
Now we calculate and, at the same time, we draw a color-coded plot (known as
heatmap) of these correlation coeÔ¨Écients using seaborn.heatmap() (the result is
depicted in Fig. 5.2):
fig, ax = plt.subplots(figsize=(9,5))
sns.heatmap(california_pd.corr().round(2), annot=True, square=True,‚ê£
,‚Üíax=ax)
One way that we may use this exploratory analysis is in feature selection. In many
applications with a limited sample size and moderate to large number of features,
using a subset of features could lead to a better performance in predicting the target
than using all features. This is due to what is known as the ‚Äúcurse of dimensionality‚Äù,
which is discussed in details in Chapter 10. In short, this phenomenon implies that
using more features in training not only increases the computational burden, but
also, and perhaps more importantly, could lead to a lower performance of the trained
models after adding more than a certain number of features. The process of feature
subset selection (or simply feature selection) per se is an important area of research
in machine learning and will be discussed in Chapter 10. Here, we may use the
correlation matrix presented in Fig. 5.2 to identify a subset of features such that each
feature within this subset is strongly or even moderately correlated with the response

5.2 Regression
143
MedInc
HouseAge
AveRooms
AveBedrms
Population
AveOccup
Latitude
Longitude
MEDV
MedInc
HouseAge
AveRooms
AveBedrms
Population
AveOccup
Latitude
Longitude
MEDV
1
-0.12
0.32
-0.06
0
0.02
-0.08
-0.02
0.68
-0.12
1
-0.15
-0.08
-0.29
0.01
0.01
-0.11
0.11
0.32
-0.15
1
0.85
-0.07
-0.01
0.1
-0.03
0.15
-0.06
-0.08
0.85
1
-0.06
-0.01
0.07
0.02
-0.04
0
-0.29
-0.07
-0.06
1
0.06
-0.11
0.1
-0.03
0.02
0.01
-0.01
-0.01
0.06
1
0.01
0
-0.03
-0.08
0.01
0.1
0.07
-0.11
0.01
1
-0.92
-0.14
-0.02
-0.11
-0.03
0.02
0.1
0
-0.92
1
-0.05
0.68
0.11
0.15
-0.04
-0.03
-0.03
-0.14
-0.05
1
‚àí0.75
‚àí0.50
‚àí0.25
0.00
0.25
0.50
0.75
1.00
Fig. 5.2: The heatmap of pairwise correlation coeÔ¨Écients for all variables in the
California Housing dataset.
variable. Although deÔ¨Åning a strong or moderate correlation is naturally a subjective
matter, we simply choose those variables with a correlation magnitude greater than
0.1; that is, MedInc, HouseAge, AveRooms, and Latitude. It is worthwhile to
reiterate that here we even conducted such an exploratory feature selection after
splitting the data into training and test sets. This is to prevent data leakage, which
was discussed in Section 4.6. In what follows, we only work with the selected
features.
X_train_fs_scaled = X_train_scaled[:,[0, 1, 2, 7]] # X_trained with‚ê£
,‚Üíselected features
X_test_fs_scaled = X_test_scaled[:,[0, 1, 2, 7]]
print('The shape of training X after feature selection: ' +‚ê£
,‚Üístr(X_train_fs_scaled.shape))
print('The shape of test X after feature selection: ' +‚ê£
,‚Üístr(X_test_fs_scaled.shape))

144
5 k-Nearest Neighbors
The shape of training X after feature selection: (15480, 4)
The shape of test X after feature selection: (5160, 4)
np.savez('data/california_train_fs_scaled', X = X_train_fs_scaled, y =‚ê£
,‚Üíy_train) # to save for possible uses later
np.savez('data/california_test_fs_scaled', X = X_test_fs_scaled, y =‚ê£
,‚Üíy_test)
A point to notice is the order of scaling and calculating Pearson‚Äôs
correlation coeÔ¨Écient. As we mentioned earlier, if we change values of xi
to axi + b and yi to cyi + d where a, b, c and d are all constants, Pearson‚Äôs
correlation coeÔ¨Écient does not change. Therefore, whether we calculate
Pearson‚Äôs correlation coeÔ¨Écient before or after any linear scaling, the results
do not change. That being said, here we present it after standardization to
align with those types of feature selection that depend on the scale of data
and, therefore, are expected to be applied after data scaling‚Äîafter all, if
scaling is performed, the Ô¨Ånal predictive model should be trained on scaled
data and it would be better to identify features that are predictive in the scaled
feature space using which the Ô¨Ånal model is trained.
Model Training and Evaluation: Now we are in the position to train our model.
The standard kNN regressor is implemented in in the KNeighborsRegressor class
in the sklearn.neighbors module. We train a standard 50NN (Euclidean distance
and uniform weights) using the scaled training set (X_train_fs_scaled), and then
use the model to predict the target in the test set (X_test_fs_scaled). At the same
time, we created the scatter plot between predicted targets ÀÜy and the actual targets.
This entire process is implemented by the following short code snippet:
from sklearn.neighbors import KNeighborsRegressor as KNN
plt.style.use('seaborn')
knn = KNN(n_neighbors=50)
knn.fit(X_train_fs_scaled, y_train)
y_test_predictions = knn.predict(X_test_fs_scaled)
plt.figure(figsize=(4.5, 3), dpi = 200)
plt.plot(y_test, y_test_predictions, 'g.', markersize=4)
lim_left, lim_right = plt.xlim()
plt.plot([lim_left, lim_right], [lim_left, lim_right], '--k',‚ê£
,‚Üílinewidth=1)
plt.xlabel("y (Actual MEDV)", fontsize='small')
plt.ylabel("$\hat{y}$ (Predicted MEDV)", fontsize='small')
plt.tick_params(axis='both', labelsize=7)
Let Ste denote the test set containing m test observations. In situations where
ÀÜyi = yi for any xi ‚ààSte, we commit no error in our predictions. This implies that

5.2 Regression
145
0
1
2
3
4
5
y (Actual MEDV)
0
1
2
3
4
5
ÃÇ
y (Predicted MEDV)
Fig. 5.3: The scatter plot of predicted targets by a 50NN and the actual targets in the
California Housing dataset.
all points in the scatter plot between predicted and actual targets should lie on the
diagonal line ÀÜy = y. In Fig. 5.3, we can see that, for example, when MEDV is
lower than $100k, our model is generally overestimating the target; however, for
large values of MEDV (e.g., around $500k), our model is underestimating the target.
Although such scatter plots help shed light on the behavior of the trained regressor
across the entire range of the target variable, we generally would like to summarize
the performance of the regressor using some performance metrics.
As we discussed in Chapter 4, all classiÔ¨Åers in scikit-learn have a score method
that given a test data and their labels, returns a measure of classiÔ¨Åer performance.
This also holds for regressors. However, for regressors the score method estimates
coeÔ¨Écient of determination, also known as R-squared statistics, denoted ÀÜR2, given
by
ÀÜR2 = 1 ‚àíRSS
TSS ,
(5.12)
where RSS and TSS are short for Residual Sum of Squares and Total Sum of Squares,
respectively, deÔ¨Åned as

146
5 k-Nearest Neighbors
RSS =
m
√ï
i=1
(yi ‚àíÀÜyi)2 ,
(5.13)
TSS =
m
√ï
i=1
(yi ‚àí¬Øy)2 ,
(5.14)
and where ¬Øy is the target mean within the test set; that is ¬Øy = 1
m
√çm
i=1 yi. This means
that ÀÜR2 measures how well our trained regressor is performing when compared with
the the trivial estimator of the target, which is ¬Øy. In particular, for perfect predictions,
RSS = 0 and ÀÜR2 = 1; however, when our trained regressor is performing similarly
to the trivial average estimator, we have RSS = TSS and ÀÜR2 = 0.
A common misconception among beginners is that ÀÜR2 can not be
negative. While ÀÜR2 can not be greater than 1 (because we can not do better
than perfect prediction), there is no limit on how negative it could be. This
is simply because there is no limit on how badly our regressors can perform
compared with the trivial estimator. As soon as RSS > TSS, which can be
thought as the trained regressor is performing worse than the trivial regressor
¬Øy, ÀÜR2 will be negative.
Next, we examine the performance of our regressor as measured by ÀÜR2:
print('The test R^2 is: {:.2f}'.format(knn.score(X_test_fs_scaled,‚ê£
,‚Üíy_test)))
The test RÀÜ2 is: 0.64
5.2.3 Distance-Weighted kNN Regressor
In contrast with the standard kNN regressor given in (5.8), DW-kNN regressor
computes a weighted average of targets for the k nearest neighbors where the weights
are the inverse of the distance of each nearest neighbor from the test observation
x. This causes observations that are closer to a test observation impose a higher
inÔ¨Çuence in the weighted average. In particular, DW-kNN is given by
f (x) = 1
D
K
√ï
i=1
1
d[x(i)(x), x] y(i) ,
(5.15)
where D is deÔ¨Åned in (5.4). When d[x(i)(x), x] = 1 for all k nearest neighbors,
(5.15) reduces to the standard kNN regressor given by (5.8). In scikit-learn, DW-
kNN regressor is obtained by the default setting of the weights parameter of

5.2 Regression
147
KNeighborsRegressor from 'uniform' to 'distance'. The choice of distance
itself can be also controlled with the metric parameter.
Exercises:
Exercise 1: In California Housing dataset, redo the analysis conducted in Section
5.2.2 with the following changes:
1) Instead of the four features that were used in Section 5.2.2, use all features to
train a 50NN and evaluate on the test set. What is the ÀÜR2? Does this change lead
to a better or worse performance compared with what was achieved in Section
5.2.2?
2) In this part use all features as in Part 1; however, use a DW-50NN with Manhattan
distance. Do you see a better or worse performance compared with Part 1?
Exercise 2: Which class is assigned to an observation x0 = 0.41 if we use the
standard 5NN with the following training data?
class ‚Üí
1
2
1
3
2
3
2
1
3
observation ‚Üí0.45 0.2 0.5 0.05 0.3 0.15 0.35 0.45 0.4
A) class 1
B) class 2
C) class 3
D) there is a tie between class 1 and 3
Exercise 3: Suppose we have the following training data where x and y denote the
value of the predictor and the target value, respectively. We train a standard 3NN
regressor using this training data. What is the prediction of y for x = 2 using the
trained 3NN?
x
‚àí3
1
‚àí1
4
2
5
y
0
5
15
‚àí3
3
6
A) 2
B) 1.66
C) 3
D) 4
E) 5
E) 1
Exercise 4: Suppose we trained a regressor and applied to a test set with seven
observations to determine ÀÜR2. The result of applying this regressor to the test set is
tabulated in the following table where y and ÀÜy denote the actual and the estimated
values of the target, respectively. What is ÀÜR2?
A) 0.25
B) 0.35
C) 0.55
D) 0.65
E) 0.75

148
5 k-Nearest Neighbors
y
0
‚àí1
2
3
4
‚àí3
2
ÀÜy
‚àí1
‚àí2
3
2
5
‚àí3
4
Exercise 5: Suppose we trained a regressor and applied to a test set with six observa-
tions to determine ÀÜR2. The result of applying this regressor to the test set is tabulated
in the following table where y and ÀÜy denote the actual and the estimated values of
the target, respectively. Determine ÀÜR2?
y
1
2
1
2
9
‚àí3
ÀÜy
‚àí1
0
3
2
9
3
Exercise 6: A typical task in machine learning is to predict (estimate) future values
of a variable based on its past and present values. This is possible if we have time
series for the variable of interest and if the present and the past values of the variable
contain information about its future. This problem is called forecasting to distinguish
that from the general use of ‚Äúprediction‚Äù term in machine learning. Nevertheless,
in machine learning forecasting is generally formulated as a regression problem
by properly preparing the training data. In this exercise, we see how a forecasting
problem is formulated and solved as a regression problem (use hand calculations
throughout this exercise).
In this regard, Fig. 5.4 shows the annual revenue of a (hypothetical) company (in
million US $) for the last 11 years. We would like to train a 2NN to forecast revenue
one year in the future based on the revenue in the last three years. Constructing this
kNN can help answer question of what would be the revenue at the end of 2023
based on the revenue in 2020, 2021, and 2022. Because we would like to forecast
the revenue one year ahead, we need to prepare a training data to train a regressor
by associating the value of the target y to some feature vectors. We achieve this in
two stages by Ô¨Årst creating some feature vectors and then associating the value of y
to each feature vector.
1) Preparing feature vectors. Use a sliding window of size 3 (observations) to
construct feature vectors of size 3 such that the feature value in each dimension
is the revenue of one year within the window. Slide the window with an amount
of two observations; that is, windows have an overlap of one observation. In Fig.
5.5, we show the window when the window is slid with this amount. The Ô¨Årst
and the second windows shown by dashed rectangles in this Ô¨Ågure correspond
to two feature vectors x1 = [1, 2, 3]T and x2 = [3, 2, 3]T, respectively. Use this
strategy and write all possible (Ô¨Åve) feature vectors created from this data.
2) Associating target values to feature vectors. For each feature vector that was
prepared in Part 1, the value of the target is the revenue of the year following the
most recent year stored in the feature vector. Naturally, if we train a regressor
using this prepared training data, the regressor estimates the value of the year
following the most recent year in a given feature vector (i.e., it performs fore-
casting). As an example, see Fig. 5.6 in which the observation pointed to by an

5.2 Regression
149
Fig. 5.4: The annual revenue of a company (in million US $) for 11 years.
arrow shows the observation that should be used as the target for the Ô¨Årst window
(i.e., for the Ô¨Årst feature vector). If possible, associate a target value to feature
vectors created in Part 1. Remove the feature vector that has no corresponding
target from the training data.
3) Training and prediction. Use the prepared training data to forecast the revenue
for 2023 using Euclidean-2NN regressor.
1
2
3
3
2
3
Fig. 5.5: Creating feature vectors by a sliding window with an overlap of one obser-
vation.

150
5 k-Nearest Neighbors
Fig. 5.6: An arrow pointing to the target value for the feature vector that is created
using observations within the window (the dashed rectangle).
Exercise 7: In Exercise 6, the sliding window size and the amount of overlap should
generally be treated as hyperparameters.
1. Redo Exercise 6 to forecast the revenue for 2023 based on the revenue in the
last four years. Determine the size of the sliding window and use an overlap of 2
observations. What is the estimated 2023 revenue based on the Euclidean-2NN
regressor?
2. Redo Part 1, but this time use the Manhattan-2NN regressor.

Chapter 6
Linear Models
Linear models are an important class of machine learning models that have been
applied in various areas including genomic classiÔ¨Åcation, electroencephalogram
(EEG) classiÔ¨Åcation, speech recognition, face recognition, to just name a few. Their
popularity in various applications is mainly attributed to one or a combination of the
following factors: 1) their simple structure and training eÔ¨Éciency; 2) interpretability;
3) matching their complexity with that of problems that have been historically
available; and 4) a ‚ÄúsatisÔ¨Åcing‚Äù approach to decision-making. In this chapter, we
cover some of the most widely used linear models including linear discriminant
analysis, logistic regression, and multiple linear regression. We will discuss various
forms of shrinkage including ridge, lasso, and elastic-net in combination with logistic
regression and multiple linear regression.
6.1 Optimal ClassiÔ¨Åcation
There exist a number of linear models for classiÔ¨Åcation and regression. Here we aim
to focus on the working principles of a few important linear models that are frequently
used in real-world applications and leave others to excellent textbooks such as (Duda
et al., 2000; Bishop, 2006; Hastie et al., 2001). We start our discussion from linear
classiÔ¨Åers. However, before exploring these classiÔ¨Åers or even deÔ¨Åning what a linear
classiÔ¨Åer is, it would be helpful to study the optimal classiÔ¨Åcation rule. This is
because the structure of two linear classiÔ¨Åers that we will cover are derived from the
estimate of the optimal classiÔ¨Åer, also known as the Bayes classiÔ¨Åer.
151
¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
A. Zollanvari, Machine Learning with Python, https://doi.org/10.1007/978-3-031-33342-2_6 

152
6 Linear Models
6.1.1 Discriminant Functions and Decision Boundaries
An often helpful strategy to conviniently characterize a classiÔ¨Åer is using the concept
of discriminant function. Suppose we can Ô¨Ånd c functions gi(x), i = 0, . . ., c ‚àí1,
where c is the number of classes using which we can write a classiÔ¨Åer as
œà(x) = argmax
i
gi(x) .
(6.1)
In other words, the classiÔ¨Åer œà(x) assigns label i to x ‚ààRp if gi(x) > gj(x) for
i , j. The functions gi(x) are known as discriminant functions. The classiÔ¨Åer
then partitions the feature space Rp into c decision regions, R1, . . . , Rc such that
R1 ‚à™R2 ‚à™. . . ‚à™Rc = Rp. That is to say, if gi(x) > gj(x) for ‚àÄi , j then x ‚ààRi.
Using this deÔ¨Ånition, a decision boundary is the boundary between two decision
regions Ri and Rj, if any, which is given by
{x | gi(x) = gj(x), i , j} .
(6.2)
As an example, let us elaborate on these deÔ¨Ånitions using the standard kNN
classiÔ¨Åer that we saw in Section 5.1.1. Comparing Eq. (6.1) above with Eq. (5.2), we
observe that the discriminant functions for the standard kNN classiÔ¨Åer are gi(x) =
√çk
j=1
1
k I{Y(j)(x)=i}, i = 0, . . ., c ‚àí1. Therefore, the decision boundary is determined
from (6.2), which for the speciÔ¨Åc training data given in Section 5.1.1, this is the set
of points that we have already seen in Fig. 5.1.
In case of binary classiÔ¨Åcation, we can simplify (6.1) by deÔ¨Åning a single dis-
criminant function g(x) = g1(x) ‚àíg0(x), which leads to the following classiÔ¨Åer:
œà(x) =
(
1
if g(x) > 0,
0
otherwise .
(6.3)
Using (6.3), the decision boundary is where
{x | g(x) = 0} .
(6.4)
6.1.2 Bayes ClassiÔ¨Åer
The misclassiÔ¨Åcation error rate of a classiÔ¨Åer was deÔ¨Åned in Section 4.9. In particular,
the error rate was deÔ¨Åned as

6.1 Optimal ClassiÔ¨Åcation
153
Œµ = P(œà(X) , Y) .
(6.5)
Ideally, we desire to have a classiÔ¨Åer that has the lowest error among any possible
classiÔ¨Åer. The question is whether we can achieve this classiÔ¨Åer. As we will see in this
section, this classiÔ¨Åer, which is known as the Bayes classiÔ¨Åer, exists in theory but not
really in practice because it depends on the actual probabilistic distribution of data,
which is almost always unknown in practice. Nonetheless, derivation of the Bayes
classiÔ¨Åer will guide us towards practical classiÔ¨Åers as we will diiscuss. In order to
characterize the Bayes classiÔ¨Åer, here we Ô¨Årst start from some intuitive arguments
with which we obtain a theoretical classiÔ¨Åer, which is later proved (in Exercise 4) to
be the Bayes classiÔ¨Åer.
Suppose we are interested to develop a binary classiÔ¨Åer for a given observation
x. A reasonable assumption is to set gi(x) to P(Y = i|x), i = 0, 1, which is known as
the posterior probability and represents the probability that class (random) variable
takes the value i for a given observation x. If this probability is greater for class
1 than class 0, then it makes sense to assign label 1 to x. Replacing this speciÔ¨Åc
gi(x), i = 0, 1 in (6.3) means that our classiÔ¨Åer is:
œà(x) =
(
1
if P(Y = 1|x) > P(Y = 0|x),
0
otherwise .
(6.6)
Furthermore, because P(Y = 0|x) + P(Y = 1|x) = 1, (6.6) can be written as
œà(x) =
(
1
if P(Y = 1|x) > 1
2 ,
0
otherwise .
(6.7)
Using Bayes rule, we have
P(Y = i|x) = p(x|Y = i)P(Y = i)
p(x)
.
(6.8)
In Section 4.9, we discussed the meaning of various terms that appear in (6.8).
Replacing (6.8) in (6.6) means that our classiÔ¨Åer can be equivalently written as
œà(x) =
(
1
if p(x|Y = 1)P(Y = 1) > p(x|Y = 0)P(Y = 0),
0
otherwise,
(6.9)
which itself is equivalent to
œà(x) =
(
1
if log

p(x|Y=1)
p(x|Y=0)

> log

P(Y=0)
P(Y=1)

,
0
otherwise .
(6.10)

154
6 Linear Models
In writing (6.10) from (6.9), we rearranged the terms and took the natural log from
both side‚Äîtaking the log(.) from both sides does not change our classiÔ¨Åer because
it is a monotonic function. However, it is common to do so in the above step because
some important families of class-conditional densities have exponential form and
doing so makes the mathematical expressions of the corresponding classiÔ¨Åers easier.
As we can see here, there are diÔ¨Äerent ways to write the same classiÔ¨Åer œà(x)‚Äî
the forms presented in (6.6), (6.7), (6.9), and (6.10) all present the same classiÔ¨Åer.
Although this classiÔ¨Åer was developed intuitively in (6.6), it turns out that for binary
classiÔ¨Åcation, it is the best classiÔ¨Åer in the sense that it achieves the lowest misclas-
siÔ¨Åcation error given in (6.5). The proof is slightly long and is left as an exercise
(Exercise 4). Therefore, we refer to the classiÔ¨Åer deÔ¨Åned in (6.6) (or its equiva-
lent forms) as the optimal classiÔ¨Åer (also known as the Bayes classiÔ¨Åer). Extension
of this to multiclass classiÔ¨Åcation with c classes is obtained by simply replacing
gi(x), i = 0, . . ., c ‚àí1 by P(Y = i|x) in (6.1); that is to say, the Bayes classiÔ¨Åer for
multiclass classiÔ¨Åcation with c classes is
œà(x) = argmax
i
P(Y = i|x) .
(6.11)
6.2 Linear Models for ClassiÔ¨Åcation
Linear models for classiÔ¨Åcation are models for which the decision boundaries are
linear functions of the feature vector x (Bishop, 2006, p.179), (Hastie et al., 2001,
p.101). There are several important linear models for classiÔ¨Åcation including linear
discriminant analysis, logistic regression, percetron, and linear support vector ma-
chines; however, here we discuss the Ô¨Årst two models not only to be able to present
them in details but also because:
‚Ä¢ these two models are quite eÔ¨Écient to train (i.e., have fast training); and
‚Ä¢ oftentimes, applying a proper model selection on the Ô¨Årst two would lead to a
result that is ‚Äúcomparable‚Äù to the situation where a model selection is performed
over all four. That being said, in the light of the no free lunch theorem, there is
no reason to favor one classiÔ¨Åer over another regardless of the context (speciÔ¨Åc
problem) (Duda et al., 2000).
The theoretical aspects discussed below for each model have two purposes:
‚Ä¢ to better understand the working principle of each model; and
‚Ä¢ to better understand some important parameters used in scikit-learn implemen-
tation of each model.

6.2 Linear Models for ClassiÔ¨Åcation
155
6.2.1 Linear Discriminant Analysis
Linear discriminant analysis (LDA) is rooted in an idea from R. A. Fisher (Fisher,
1936, 1940), and has a long history in statistics and pattern recognition. It was
further developed by Wald (Wald, 1944) in the context of decision theory and
then formulated by Anderson (Anderson, 1951) in the form that is commonly used
today. Owing to its simple formulation in the context of widely used Gaussian
distributions and its many successful applications, LDA has been vigorously studied
in the past several decades. At the same time, it has led to many other successful
classiÔ¨Åers such as Euclidean distance classiÔ¨Åer (EDC) (Raudys and Pikelis, 1980),
diagonal-LDA (DLDA) (Pikelis, 1973), regularized-LDA (RLDA) (Pillo, 1976), and
sparse discriminant analysis (SDA) (Clemmensen et al., 2011); it has been used in
a semi-supervised fashion and implemented in various forms such as self-learning
LDA (McLachlan, 1975), moment constrained LDA (Loog, 2014), and implicitly
constrained LDA (Krijthe and Loog, 2014).
Bayes Rule for Normal Populations with a Common Covariance Matrix: The
development of LDA is quite straightforward using a few assumptions. To develop
the LDA classiÔ¨Åer, the Ô¨Årst assumption is that class-conditional probability densities
are Gaussian; that is,
p(x|Y = i) =
1
(2œÄ)p/2|Œ£i|1/2 e‚àí1
2 (x‚àí¬µi)T Œ£‚àí1
i (x‚àí¬µi) , i = 0, 1, . . ., c ‚àí1,
(6.12)
where c is the number of classes and ¬µi and Œ£i are the class-speciÔ¨Åc mean vector (a
vector consists of the mean of each variable in x) and the covariance matrix between
all p variables in x, respectively. The second assumption in development of LDA is
that Œ£
‚àÜ= Œ£i, ‚àÄi, which means that there is a common covariance matrix Œ£ across all
classes. Of course this is only an assumption and it is not necessary true in practice;
however, this assumption makes the form of classiÔ¨Åer simpler (in fact linear).
Consider the binary classiÔ¨Åcation withi = 0, 1. Using the assumption of Œ£
‚àÜ= Œ£0 =
Œ£1 in (6.10) and after some standard algebraic simpliÔ¨Åcations yield (see Exercise 6):
œà(x) =
Ô£±Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£≥
1
if

x ‚àí¬µ0+¬µ1
2
T
Œ£‚àí1  ¬µ1 ‚àí¬µ0
 + log

P(Y=1)
1‚àíP(Y=1)

> 0,
0
otherwise .
(6.13)
This means that the LDA classiÔ¨Åer is the Bayes classiÔ¨Åer when class-conditional
densities are Gaussian with a common covariance matrix. The expression of LDA
in (6.13) is in the form of (6.3) where
g(x) =

x ‚àí¬µ0 + ¬µ1
2
T
Œ£‚àí1  ¬µ1 ‚àí¬µ0
 + log

P(Y = 1)
1 ‚àíP(Y = 1)

.
(6.14)

156
6 Linear Models
From (6.4) and (6.13) the decision boundary of the LDA classiÔ¨Åer is given by

x | aTx + b = 0
	
,
(6.15)
where
a = Œ£‚àí1  ¬µ1 ‚àí¬µ0
 ,
(6.16)
b = ‚àí
 ¬µ0 + ¬µ1
2
T
Œ£‚àí1  ¬µ1 ‚àí¬µ0
 + log

P(Y = 1)
1 ‚àíP(Y = 1)

.
(6.17)
From (6.15)-(6.17), it is clear that the LDA classiÔ¨Åer is a linear model for classiÔ¨Åca-
tion because its decision boundary is a linear function of x.
Some Properties of the Linear Decision Boundary: The set of x that satisÔ¨Åes
(6.15) forms a hyperplane with its orientation and location uniquely determined by
a and b. SpeciÔ¨Åcally, the vector a is normal to the hyperplane. To see this, consider
a vector u from one arbitrary point x1 on the hyperplane to another point x2; that
is, u = x2 ‚àíx1. Because both x1 and x2 are on the hyperplane, then aTx1 + b = 0
and aTx2 + b = 0. Therefore, aT(x2 ‚àíx1) = aTu = 0, which implies a is normal to
any vector u on the hyperplane. This is illustrated by Fig. 6.1a. This Ô¨Ågure shows the
decision boundary of the LDA classiÔ¨Åer for a two-dimensional binary classiÔ¨Åcation
problem where classes are normally distributed with means ¬µT
0 = [4, 4]T and
¬µT
1 = [‚àí1, 0]T, a common covariance matrix given by
Œ£ =
1 0
0 4

,
and P(Y = 1) = 0.5. From (6.14)-(6.17), the decision boundary in this example is
determined by x = [x1, x2]T that satisÔ¨Åes ‚àí5x1 ‚àíx2 + 9.5 = 0. This is the solid
line that passes through the Ô¨Ålled circle, which is the midpoint between population
means; that is, ¬µ0+¬µ1
2
. This is the case because P(Y = 1) = 0.5, which implies
log

P(Y=1)
1‚àíP(Y=1)

= 0, and, therefore,
 ¬µ0 + ¬µ1
2
‚àí¬µ0 + ¬µ1
2
T
Œ£‚àí1  ¬µ1 ‚àí¬µ0
 = 0.
Also note the vector a = Œ£‚àí1  ¬µ1 ‚àí¬µ0
 = [‚àí5, ‚àí1]T is perpendicular to the linear
decision boundary. This vector is not in general in direction of the line connecting the
population means (the dashed line in Fig. 6.1a) unless Œ£‚àí1  ¬µ1 ‚àí¬µ0
 = k  ¬µ1 ‚àí¬µ0

for some scaler k, which is the case if ¬µ1‚àí¬µ0 is an eigenvector of Œ£‚àí1 (and, therefore,
an eigenvector of Œ£).
Suppose in the same binary classiÔ¨Åcation example, we intend to classify three
observations: xA = [4, ‚àí2]T, xB = [0.2, 5]T, and xC = [1.1, 4]T. Replacing xA, xB,
and xC in the discriminant function ‚àí5x1 ‚àíx2 + 9.5, gives -8.5, 3.5, 0, respectively.

6.2 Linear Models for ClassiÔ¨Åcation
157
Therefore, from (6.13), xA and xB are classiÔ¨Åed as 0 (blue in the Ô¨Ågure) and 1 (red in
the Ô¨Ågure), respectively, and xC lies on the decision boundary and could be randomly
assigned to one of the classes. Fig. 6.1b-6.1d illustrate the working mechanism of
the LDA classiÔ¨Åer to classify these three sample points. Due to the Ô¨Årst part of the
discriminant function in (6.13) (i.e., x ‚àí¬µ0+¬µ1
2
), xB, and xC are moved by ‚àí¬µ0+¬µ1
2
to points x‚Ä≤
A, x‚Ä≤
B, and x‚Ä≤
C, respectively. Then in order to classify, the inner product
of these vectors should be calculated with the vector a = Œ£‚àí1  ¬µ1 ‚àí¬µ0
; however,
because x‚Ä≤
A, x‚Ä≤
B, and x‚Ä≤
C have obtuse, acute, and right angles with vector a, their
inner products are negative, positive, and zero, which lead to the same class labels
as discussed before.
Sample LDA ClassiÔ¨Åer: The main problem with using classiÔ¨Åer (6.13) in prac-
tice is that the discriminant function in (6.13) depends on the actual distributional
parameters ¬µ0, ¬µ1, and Œ£ that are almost always unknown unless we work with
synthetically generated data for which we have the underlying parameters that were
used to generate the data in the Ô¨Årst place.
Given a training data Str = {(x1, y1), (x2, y2), . . ., (xn, yn)} and with no prior
information on distributional parameters used in (6.13), one can replace the un-
known distributional parameters by their sample estimates to obtain the sample LDA
classiÔ¨Åer given by
œà(x) =
Ô£±Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£≥
1
if

x ‚àíÀÜ¬µ0+ ÀÜ¬µ1
2
T ÀÜŒ£
‚àí1   ÀÜ¬µ1 ‚àíÀÜ¬µ0
 + log

P(Y=1)
1‚àíP(Y=1)

> 0,
0
otherwise,
(6.18)
where ÀÜ¬µi is the sample mean for class i and ÀÜŒ£ is the pooled sample covariance
matrix, which are given by (ni is the number of observations in class i)
ÀÜ¬µi = 1
ni
ni
√ï
j=1
xj ,
(6.19)
ÀÜŒ£ =
1
n0 + n1 ‚àí2
1
√ï
i=0
n
√ï
j=1
(xj ‚àíÀÜ¬µi)(xj ‚àíÀÜ¬µi)T I{yj=i} ,
(6.20)
where IA is the indicator of event A.
This means that the sample version of the LDA classiÔ¨Åer in the form of (6.18) is a
Bayes plug-in rule; that is to say, the parameters of the Bayes rule (for Gaussian class-
conditional densities with a common covariance matrix) are estimated from the data
and are plugged in the expression of the Bayes classiÔ¨Åer. To distinguish the Gaussian-
based optimal rule presented in (6.13) from its sample version expressed in (6.18) as
well as from optimal rules developed based on other parametric assumptions, some
authors (see (McLachlan, 2004)) refer to (6.13) and (6.18) as normal-based linear
discriminant rule (NLDR) and sample NLDR, respectively. Nonetheless, hereafter

158
6 Linear Models
ùë•!
ùë•"
a
ùùÅ!
ùùÅ"
(a)
ùë•!
ùë•"
ùíô#
ùíô#
$
ùúΩ!
a
(b)
ùë•!
ùë•"
ùíô#
ùíô#
$
ùúΩ!
a
(c)
ùë•!
ùë•"
ùíô#
ùíô#
$
ùúΩùë™
a
(d)
Fig. 6.1: The working mechanism of the LDA classiÔ¨Åer. The blue and red ellipses
show the areas of equal probability density of the Gaussian population for class
0 and 1, respectively. The Ô¨Ålled circle identiÔ¨Åes the midpoint ¬µ0+¬µ1
2
. (a): vector
a = Œ£‚àí1  ¬µ1 ‚àí¬µ0
 = [‚àí5, ‚àí1]T is perpendicular to the linear decision boundary,
which is expressed as ‚àí5x1 ‚àíx2 + 9.5 = 0. At the same time, the decision boundary
passes through the midpoint ¬µ0+¬µ1
2
because P(Y = 1) = 0.5; (b)-(d): xA, xB, and
xC are moved in the opposite direction of vector ¬µ0+¬µ1
2
to points x‚Ä≤
A, x‚Ä≤
B, and x‚Ä≤
C,
respectively. The positivity or negativity of the inner products of these vectors with
vector a are determined by the angles between them. Because Œ∏A > 90‚ó¶(obtuse
angle), Œ∏B < 90‚ó¶(acute angle), and Œ∏C = 90‚ó¶(right angle), xA, xB, and xC are
classiÔ¨Åed to class 0 (blue), 1 (red), and either one (could be randomly chosen),
respectively.

6.2 Linear Models for ClassiÔ¨Åcation
159
and as long as the meaning is clear from the context, we may refer to both (6.13) and
(6.18) as the LDA classiÔ¨Åer.
Similar to (6.14)-(6.17), we conclude that the decision boundary of the sample
LDA classiÔ¨Åer forms a hyperplane aTx + b = 0 where
a = ÀÜŒ£
‚àí1   ÀÜ¬µ1 ‚àíÀÜ¬µ0
 ,
(6.21)
and
b = ‚àí
 ÀÜ¬µ0 + ÀÜ¬µ1
2
T
ÀÜŒ£
‚àí1   ÀÜ¬µ1 ‚àíÀÜ¬µ0
 + log

P(Y = 1)
1 ‚àíP(Y = 1)

.
(6.22)
Fig. 6.2a shows the sample LDA classiÔ¨Åer for two set of samples drawn from the
same Gaussian density functions used in Fig. 6.1. Here 20 sample points are drawn
from each Gaussian population. This data is then used in (6.19) and (6.20) to estimate
the decision boundary used in the LDA classiÔ¨Åer (6.18). The linear decision boundary
is characterized as ‚àí3.28 x1‚àí0.92 x2+7.49 = 0 (dotted line) and is diÔ¨Äerent from the
optimal decision boundary, which is ‚àí5x1 ‚àíx2 +9.5 = 0 (solid line). Fig. 6.2b shows
the sample LDA classiÔ¨Åer for the same example except that we increase the number
of sample points drawn from each population to 1500. The decision boundary in this
case is ‚àí5.20 x1‚àí0.90 x2+9.67 = 0 and it appears to be closer to the optimal decision
boundary. However, rather than discussing closeness of the decision boundaries, it
is easier to discuss the behavior of the rule in terms of its performance. As the
number of samples drawn from two Gaussian distribution increases unboundedly
(i.e., n ‚Üí‚àû), the error rate of the sample LDA classiÔ¨Åer converges (in probability)
to the error rate of the Bayes rule 6.13, a property known as the statistical consistency
of the classiÔ¨Åer1. Nevertheless, the precise characterization of this property for the
LDA classiÔ¨Åer or other rules is beyond the scope of our discussion and interested
readers are encouraged to consult other sources such as (Devroye et al., 1996) and
(Braga-Neto, 2020).
In the development of LDA classiÔ¨Åer, we assumed Œ£
‚àÜ= Œ£0 = Œ£1. This assumption
led us to use the pooled sample covariance matrix to estimate Œ£. However, if we
assume Œ£0 , Œ£1, it does not make sense anymore to use a single estimate as the
estimate of both Œ£0 and Œ£1. In these situations, we use diÔ¨Äerent estimate of class
covariance matrices in the expression of the Bayes classiÔ¨Åer, which is itself developed
by assuming Œ£0 , Œ£1. The resultant classiÔ¨Åer is known as quadratic discriminant
analysis (QDA) and has non-linear decision boundaries (see Exercise 7).
Remarks on Class Prior Probabilities: The discriminant function of the LDA
classiÔ¨Åer (6.18) explicitly depends on the prior probabilities P(Y = i), i = 0, 1.
However, in order to properly use such prior probabilities in training LDA (and other
classiÔ¨Åers that explicitly depend on them), we need to distinguish between two types
of sampling procedure: random sampling and separate sampling.
1 Here we implicitly assume a Ô¨Åxed dimensionality of observations; otherwise, we should discuss
the notion of generalized statistical consistency (Zollanvari and Dougherty, 2015).

160
6 Linear Models
(a)
(b)
Fig. 6.2: The decision boundary of the sample LDA classiÔ¨Åer for various training
samples drawn from the two Gaussian populations used for plots in Fig. 6.1. The
sample points are shown by orange or blue dots depending on their class. (a): 20
sample points are drawn from each Gaussian population. Here the decision boundary
is characterized as ‚àí3.28 x1 ‚àí0.92 x2 + 7.49 = 0; and (b) 1500 sample points are
drawn from each Gaussian population. Here the decision boundary is characterized
as ‚àí5.20 x1 ‚àí0.90 x2 + 9.67 = 0. The decision boundary in this case appears to be
‚Äúcloser‚Äù to the optimal decision boundary identiÔ¨Åed by the solid line.
Under random-sampling, an independent and identically distributed (i.i.d.) sample
Str = {(x1, y1), (x2, y2), . . ., (xn, yn)} is drawn from the mixture of the populations
Œ†0 and Œ†1 with mixing proportions P(Y = 0) and P(Y = 1), respectively. This means
that for a given training sample of size n with ni sample points from each class, ni
is indeed a realization of a binomial random variable Ni ‚àºBinomial (n, P(Y = i));
that is to say, should the process of drawing a sample of size n from the mixture of
populations repeat, n0 and n1 could be diÔ¨Äerent. The random-sampling assumption
is so pervasive in a text on classiÔ¨Åcation that it is usually assumed without mention
or in textbooks is often stated at the outset and then forgotten (e.g., see (Devroye
et al., 1996, p.2)). An important consequence of random-sampling is that a class
prior probability P(Y = i) can be estimated by the sampling ratio ni
n . This is indeed
a direct consequence of Bernoulli‚Äôs weak law of large number and, therefore, for a
large sample, we can expect the sampling ratio to be a reasonable estimate of the
class prior probability.
However, suppose the sampling is not random, in the sense that the ratio ni
n
is chosen before the sampling procedure. In this separate-sampling case, class-
speciÔ¨Åc samples are selected randomly and separately from populations Œ†0 and Œ†1;
therefore, given n, n0 and n1 are not determined by the sampling procedure. In
this case, proportion of samples in each class are not representative of class prior

6.2 Linear Models for ClassiÔ¨Åcation
161
probabilities. As a matter of fact, in this case there is no meaningful estimator of
the prior probabilities in the given sample. In these situations, the estimate of prior
probabilities should come from prior knowledge. For the LDA classiÔ¨Åer, a bad choice
of prior probabilities can negatively impact the classiÔ¨Åer (Anderson, 1951). An an
example of separate-sampling, consider an investigator who recruits 10 patients from
a ‚Äúrare‚Äù disease (case) and 10 individual from a healthy population (control). As a
result, the number of cases in this sample, which is equal to the number of controls,
is not representative of this disease among the general population (otherwise, the
disease is not rare).
Extension to Multiclass ClassiÔ¨Åcation: The case of LDA classiÔ¨Åer for multiclass
classiÔ¨Åcation is a direct extension by using discriminants P(Y = i|x) in (6.1) in which
case the discriminant functions are linear and are given by
gi(x) = ÀÜ¬µT
i ÀÜŒ£
‚àí1x ‚àí1
2 ÀÜ¬µT
i ÀÜŒ£
‚àí1 ÀÜ¬µi + log P(Y = i),
(6.23)
where ÀÜ¬µi is deÔ¨Åned in (6.19) and ÀÜŒ£ is given by
ÀÜŒ£ =
1
n ‚àíc
c‚àí1
√ï
i=0
n
√ï
j=1
(xj ‚àíÀÜ¬µi)(xj ‚àíÀÜ¬µi)T I{yj=i} ,
(6.24)
where n = √çc‚àí1
i=0 ni is the total sample size across all classes.
Scikit-learn implementation: In scikit-learn, the LDA classiÔ¨Åer is implemented
by the LinearDiscriminantAnalysis class from the sklearn.discriminant_analysis
module. However, to achieve a similar representation of LDA as in (6.23) one needs
to change the default value of solver parameter from 'svd' (as of scikit-learn
version 1.2.2) to 'lsqr', which solves the following system of linear equation to
Ô¨Ånd a in a least squares sense:
ÀÜŒ£a = ÀÜ¬µi .
(6.25)
At the same time, note that the LDA discriminant (6.23) can be written as
gi(x) = aTx ‚àí1
2aT ÀÜ¬µi + log P(Y = i),
(6.26)
where
a = ÀÜŒ£
‚àí1 ÀÜ¬µi .
(6.27)

162
6 Linear Models
In other words, using 'lsqr', (6.25) is solved in a way to avoid inverting the
matrix ÀÜŒ£ (rather than directly solving (6.27)). Note that by doing so, LDA classiÔ¨Åer
can be even applied to cases where the sample size is less than the feature size. In
these situations, (6.27), which is indeed the direct formulation of LDA classiÔ¨Åer, is
not feasible because ÀÜŒ£ is singular but we can Ô¨Ånd a ‚Äúspecial‚Äù solution from (6.25).
Many models in scikit-learn have two attributes, coef_ (or sometimes coefs_,
for example, for multi-layer perceptron that we discuss later) and intercept_ (or
intercepts_) that store their coeÔ¨Écients and intercept (also known as bias term).
The best way to identify such attributes, if they exist at all for a speciÔ¨Åc model, is to
look into the scikit-learn documentation. For example, in case of LDA, for a binary
classiÔ¨Åcation problem, the coeÔ¨Écients vector and intercept are stored in coef_ of
shape (feature size, ) and intercept_, respectively; however, for multiclass
classiÔ¨Åcation, coef_ has a shape of (class size, feature size).
√â
More on solver: A question that remains unanswered is what is meant to Ô¨Ånd
an answer to (6.25) when ÀÜŒ£ is not invertible? In general, when a system of linear
equations is underdetermined (e.g., when ÀÜŒ£ is singular, which means we have inÔ¨Ånite
solutions), we need to rely on techniques that designate a solution as ‚Äúspecial‚Äù. Using
'lsqr' calls scipy.linalg.lstsq, which by default is based on DGELSD routine
from LAPACK written in fortran. DGELSD itself Ô¨Ånds the minimum-norm solution
to the problem. This is a unique solution that minimizes || ÀÜŒ£a ‚àíÀÜ¬µi||2 using singular
value decompisiton (SVD) of matrix ÀÜŒ£. This solution exists even for cases where ÀÜŒ£
is singular. However, it becomes the same as ÀÜŒ£
‚àí1 ÀÜ¬µi if ÀÜŒ£ is full rank.
The default value of solver parameter in LinearDiscriminantAnalysis is svd.
The use of SVD here should not be confused with the aforementioned use of SVD
for 'lsqr'. With the 'lsqr' option, the SVD of ÀÜŒ£ is used; however, the SVD used
along with 'svd' is primarily applied to the centered data matrix and dimensions
with singular values greater than 'tol' (current default 0.0001)‚Äîdimensions with
singular values less than 'tol' are discarded. This way, the scikit-learn implementa-
tion avoids even computing the covariance matrix per se. This could be an attractive
choice in high-dimensional situations where p > n. The svd computation here relies
on scipy.linalg.svd, which is based on DGESDD routine from LAPACK. Last
but not least, the connection between Python and Fortran subroutines is created by
f2py module that can be used to create a Python module containing automatically
generated wrapper functions from Fortran rountines. For more details see (f2py,
2023).
‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-
6.2.2 Logistic Regression
The linearity of decision boundaries in LDA was a direct consequence of the Gaussian
assumption about class-conditional densities. Another way to achieve linear decision
boundaries is to assume some monotonic transformation of posterior P(Y = i|x) is

6.2 Linear Models for ClassiÔ¨Åcation
163
linear. In this regard, a well-known transformation is known as logit function. In
particular, if p ‚àà(0, 1) then
logit(p) = log

p
1 ‚àíp

.
(6.28)
The ratio inside the log function in (6.28) is known as odds (and another name for
the logit function is log-odds). For example, if in a binary classiÔ¨Åcation problem, the
probability of having an observation from one class is p = 0.8, then odds of having
an observation from that class is 0.8
0.2 = 4.
Let us consider a function known as logistic sigmoid function œÉ(x) given by
œÉ(x) =
1
1 + e‚àíx .
(6.29)
Using logit(p) as the argument of œÉ(x) leads to
œÉ (logit(p)) =
1
1 + e‚àílogit(p) =
1
1 + e‚àílog

p
1‚àíp
 = p,
(6.30)
which means the logistic sigmoid function is the inverse of the logit function. In
logistic regression, we assume the logit of posteriors are linear functions of x; that
is,
logit P(Y = 1|x) = log

P(Y = 1|x)
1 ‚àíP(Y = 1|x)

= aTx + b .
(6.31)
Replacing p and logit(p) with P(Y = 1|x) and aTx + b in (6.30), respectively, yields
P(Y = 1|x) =
1
1 + e‚àí(aT x+b) =
e(aT x+b)
1 + e(aT x+b) ,
(6.32)
which is always between 0 and 1 as it should (because the posterior is a probability).
In addition, because P(Y = 0|x) + P(Y = 1|x) = 1, we have
P(Y = 0|x) =
e‚àí(aT x+b)
1 + e‚àí(aT x+b) =
1
1 + e(aT x+b) .
(6.33)
We can write (6.32) and (6.33) compactly as

164
6 Linear Models
P(Y = i|x) =
1
1 + e(‚àí1)i(aT x+b) = P(Y = 1|x)i  1 ‚àíP(Y = 1|x)(1‚àíi) , i = 0, 1 .
(6.34)
The classiÔ¨Åer based on logistic regression is obtained by setting gi(x) = P(Y =
i|x) =
1
1+e(‚àí1)i (aT x+b) in (6.3); that is,
œà(x) =
(
1
if
1
1+e‚àí(aT x+b) ‚àí
e‚àí(aT x+b)
1+e‚àí(aT x+b) > 0,
0
otherwise .
(6.35)
At the same time, we can rewrite (6.35) as
œà(x) =
(
1
if
1
1+e‚àí(aT x+b) > 1
2 ,
0
otherwise,
(6.36)
which is equivalent to
œà(x) =
(
1
if
aTx + b > 0,
0
otherwise .
(6.37)
As seen in (6.37), the decision boundary of the binary classiÔ¨Åer deÔ¨Åned by the
logistic regression is linear and, therefore, this is a linear classiÔ¨Åer as well. What
remains is to estimate the unknown parameters a and b and replace them in (6.37)
to have a functioning classiÔ¨Åer.
A common approach to estimate the unknown parameters in logistic regression
is to maximize the log likelihood of observing the labels given observations as a
function of a and b. Let Œ≤
‚àÜ= [b, a]T denote the (p+1)-dimensional column vector of
unknown parameters. With the assumption of having independent data, we can write
log P(Y1 = y1, . . .,Yn = yn|x1, . . ., xn; Œ≤)
1= log

n
√ñ
j=1
P(Yj = 1|xj; Œ≤)yj  1 ‚àíP(Yj = 1|xj; Œ≤)(1‚àíyj)
=
n
√ï
j=1
yjlog P(Yj = 1|xj; Œ≤) + (1 ‚àíyj)log 1 ‚àíP(Yj = 1|xj; Œ≤) ,
(6.38)
where yj = 0, 1,
1= follows from the compact form speciÔ¨Åed in (6.34), and Œ≤ in P(Y1 =
y1, . . .,Yn = yn|x1, . . ., xn; Œ≤) shows that this probability is indeed a function of
unknown parameters. This maximization approach is an example of a general method
known as maximum likelihood estimation because we are trying to estimate values

6.2 Linear Models for ClassiÔ¨Åcation
165
of Œ≤ (our model parameters) by maximizing the (log) likelihood of observing labels
for the given observations. It is also common to convert the above maximixation
problem to a minimization by multiplying the above log likelihood function by -1
that results in a loss function (also known as error function) denoted by e(Œ≤); that is
to say,
e(Œ≤) = ‚àí1
n
n
√ï
j=1
yjlog P(Yj = 1|xj; Œ≤) + (1 ‚àíyj)log 1 ‚àíP(Yj = 1|xj; Œ≤) , (6.39)
which due to its similarity to cross-entropy function is known as cross-entropy loss
(also referred to as log loss). It can be shown that the second derivative of e(Œ≤)
with respect to Œ≤ (i.e., Hessian Matrix) is positive deÔ¨Ånite. This means that e(Œ≤) is
a convex function of Œ≤ with a unique minimum. Therefore, setting the derivative
of e(Œ≤) with respect to Œ≤ to zero should lead to the minimum; however, as these
equations are non-linear, no closed-form solution exists and we need to rely on
iterative optimization methods to estimate a and b (see solver subsection below for
more information). Estimating a and b by one of these methods and replacing them
in (6.37) results in the following classiÔ¨Åer:
œà(x) =
(
1
if
ÀÜaTx + ÀÜb > 0,
0
otherwise .
(6.40)
Although it is common to see in literature labels are 0 and 1 when the loss function
(6.39) is used, there is a more compact form to write the loss function of logsictic
regression if we assume labels are yj ‚àà{‚àí1, 1}. First note that similar to the compact
representation of posteriors in (6.34), we can write
P(Yj = yj|xj) =
1
1 + e‚àíyj(aT xj+b), yj = ‚àí1, 1 .
(6.41)
Using (6.41), we can write (6.39) in the following compact form where yj ‚àà{‚àí1, 1}:
e(Œ≤) = 1
n
n
√ï
j=1
log 1 + e‚àíyj(aT xj+b) .
(6.42)
We emphasize that using (6.39) with yj ‚àà{0, 1} is equivalent to using (6.42) with
yj ‚àà{‚àí1, 1}. Therefore, these choices of labeling classes do not aÔ¨Äect estimates of
a and b, and the corresponding classiÔ¨Åer.
An important and rather confusing point about logistic regression is that its main
utility is in classiÔ¨Åcation, rather than regression. By deÔ¨Ånition, we are trying to
approximate and estimate numeric posterior probabilities and no surprise that it is
called logistic regression; however, in terms of utility, almost always we are interested

166
6 Linear Models
to compare these posterior probabilities to classify a given observation to one of the
classes (i.e., the classiÔ¨Åer in the form of (6.36) or, equivalently, in terms of (6.37)).
As a result, sometimes it is referred to as logistic classiÔ¨Åcation.
Another interesting point is the number of parameters that should be estimated in
logistic regression in comparison with that of LDA. For simplicity, let us consider
binary classiÔ¨Åcation. As can be seen in (6.37), p + 1 parameters should be estimated
in logistic regression; however, for LDA we need 2p (for the means) + p(p+1)/2 (for
the pooled sample covariance matrix) + 1 (for prior probabilities) = (p2 + 5p)/2 + 1.
For example, for p = 50, logistic regression requires 51 parameters to be estimated
whereas LDA requires 1376. As a result for a large number of features, logistic
regression seems to have an upper hand in terms of errors induced by parameter
estimation as it needs far fewer number of parameters to be estimated.
√â
Extension to Multiclass ClassiÔ¨Åcation: To write (6.32) we assumed that
logit (P(Y = 1|x)) is a linear function of feature vectors and then used P(Y =
0|x) + P(Y = 1|x) = 1 to write (6.33). In multiclass case, we assume
P(Y = i|x) =
e(aT
i x+bi)
1 + √çc‚àí2
i=1 e(aT
i x+bi),
i = 0, 1, . . ., c ‚àí2,
(6.43)
and because the posterior probabilities should add up to 1, we can obtain the posterior
probability of class c ‚àí1 as
P(Y = c ‚àí1|x) =
1
1 + √çc‚àí2
i=1 e(aT
i x+bi) .
(6.44)
With these forms of posterior probabilities one can extend the loss function
to multiclass and then use some iterative optimization methods to estimate the
parameters of the model (i.e., ai and bi, ‚àÄi). The classiÔ¨Åer is then obtained by using
discriminants (6.43) and (6.44) in (6.1). Equivalently, we can set gi(x) = aT
i x+bi, i =
0, . . ., c ‚àí2, and gc‚àí1(x) = 0, and use them as discriminant functions in (6.1)‚Äîthis
is possible by noting that the denominators of all posteriors presented in (6.43) and
(6.44) are the same and do not matter, and then taking the natural logarithm from
all numerators. This latter form better shows the linearity of this classiÔ¨Åer. As this
classiÔ¨Åer is speciÔ¨Åed by a series of approximation made on posterior probabilities
P(Y = i|x), which add up to 1 and together resemble a multinomial distribution, it is
known as multinomial logistic regression.
‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-
Regularization: In practical applications, it is often helpful to consider a penalized
form of the loss function indicated in (6.42); that is, to use a loss function that
constrains the magnitude of coeÔ¨Écients in a (i.e., penalizes possible large coeÔ¨Écients
to end up with smaller coeÔ¨Écients). Because we are trying to regularize or shrink
estimated coeÔ¨Écients, this approach is known as regularization or shrinkage. There
are various forms of shrinkage penalty. One common approach is to minimize

6.2 Linear Models for ClassiÔ¨Åcation
167
e(Œ≤) = C
n
√ï
j=1
log 1 + e‚àíyj(aT xj+b) + 1
2 ||a||2
2 ,
(6.45)
where ||a||2 is known as l2 norm of a = [a1, a2, ..., ap]T given by ||a||2 =
q√çp
k=1 a2
k =
‚àö
aTa and C is a tuning parameter, which shows the relative importance of these
two terms on estimating the parameters. As C ‚Üí‚àû, the l2 penalty term has no
eÔ¨Äect on the minimization and the estimates obtained from (6.45) become similar
to those obtained from (6.42); however, as C ‚Üí0, the coeÔ¨Écients in a are shrunk
to zero. This means the higher C, the less regularization we expect, which, in turn,
implies a stronger Ô¨Åt on the training data. The optimal choice of C though is generally
estimated (i.e., C is tuned) in the model selection stage. Because here we are using
the l2 norm of a, minimizing (6.45) is known as l2 regularization. The logistic
regression trained using this objective function is known as logistic regression with
l2 penalty (also known as ridge penalty). There are other types of useful shrinkage
such as l1 regularization and elastic-net regularization. The l1 regularization in
logistic regression is achieved by minimizing
e(Œ≤) = C
n
√ï
j=1
log 1 + e‚àíyj(aT xj+b) + ||a||1 ,
(6.46)
where ||a||1 = √çp
k=1 |ak|. Elastic-net regularization, on the other hand, is minimizing
e(Œ≤) = C
n
√ï
j=1
log 1 + e‚àíyj(aT xj+b) + ŒΩ||a||1 + 1 ‚àíŒΩ
2
||a||2
2 ,
(6.47)
where ŒΩ is another tuning parameter that creates a compromise between l1 and l2
regularizations. In particular, when ŒΩ = 1, (6.47) reduces to that of l1 regularization,
and when ŒΩ = 0, (6.47) reduces to that of l2 regularization.
A practically useful and interesting property of l1 penalty is that if C is suÔ¨Éciently
small, the solution in (6.46) contains some coeÔ¨Écients (i.e., elements of a) that are
exactly zero and, therefore, are not selected to be part of the discriminant function.
Practically, this is very useful because this property implies that l1 penalty internally
possesses a feature selection mechanism (i.e., choosing ‚Äúimportant‚Äù features and
discarding the rest). This is in contrast with the solution of (6.45) that uses l2 penalty.
Although using l2 penalty also shrinks the magnitude of coeÔ¨Écients to zero, it does
not set them to exact zero. Due to this property of l1 penalty in shrinking and selecting
features, this penalty is known as lasso (Tibshirani, 1996) (short for least absolute
shrinkage and selection operator).
Scikit-learn implementation: The logistic regression classiÔ¨Åer is implemented
by the LogisticRegression class from the sklearn.linear_model module.

168
6 Linear Models
By default, scikit-learn implementation uses l2 regularization with a C = 1.0
(see (6.45)). These choices, however, can be changed by setting the values of
'penalty' and C. Possible options for penalty parameter are 'l1' (see (6.46)),
'l2' (see (6.45)), and 'elasticnet' (see (6.47)). When the 'penalty' is set to
'elasticnet', one should also set ŒΩ in (6.47) by setting the value of 'l1_ratio'
in LogisticRegression to some values ‚àà[0, 1] and also change the default value
of solver from 'lbfgs' (as of scikit-learn version 1.1.2) to 'saga', which sup-
ports 'elasticnet' penalty. If l1 regularization is desired, 'liblinear' solver
can be used (also supports l2).
√â
More on solver: There are a number of optimization algorithms that can be
used in practice to solve the aforementioned minimization of the loss function to
estimate the unknown parameters. These methods include:
1) steepest descent direction with a linear rate of convergence;
2) Newton‚Äôs methods (e.g., iteratively reweighted least squares) that have fast rate
of convergence (generally quadratic) but require computation of Hessian matrix
of the objective function, which is itself computationally expensive and error-
prone; and
3) quasi-Newton‚Äôs methods that have superlinear convergence rate (faster than
linear) and by avoiding explicit computation of Hessian matrix can be used even
in situations that Hessian matrix is not positive deÔ¨Ånite‚Äîthey use the fact that
changes in gradient provides information about the Hessian along the search
direction.
In the current version of scikit-learn (1.2.2), the default solver is 'lbfgs',
which is short for Limited-memory BFGS (Broyden‚ÄìFletcher‚ÄìGoldfarb‚ÄìShanno). It
is a quasi-Newton method so it is eÔ¨Écient in the sense that it has a superlinear conver-
gence rate. Its implementation relies on scipy.optimize.minimize with method
being 'L-BFGS-B'. The second ‚ÄúB‚Äù in 'L-BFGS-B' stands for ‚Äúbox constraints‚Äù on
variables; however, the 'lbfgs' uses this minimizer without any constraints, which
leads to L-BFGS. This method, similar to two other solvers, namely, newton-cg and
'sag', only supports l2 regularization, which is by default used in scikit-learn imple-
mentation of logistic regression. If we want to use l1 regularization, 'liblinear'
solver can be used (also supports l2) and if the elastic net regularization is desired
'saga' is currently the only solver to use. In terms of supporting multiclass clas-
siÔ¨Åcation, the following solvers support multinomial logistic regression 'lbfgs',
'newton-cg', 'sag', and 'saga' but 'liblinear' only supports one-verus rest
(OvR) scheme.
‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-
Example 6.1 In this example, we would like to train a logistic regression with l2
regularization (LRR) for Iris Ô¨Çower classiÔ¨Åcation using two features: sepal width
and petal length. For this purpose, we Ô¨Årst load the preprocessed Iris dataset that was
prepared in Section 4.6:

6.2 Linear Models for ClassiÔ¨Åcation
169
import numpy as np
arrays = np.load('data/iris_train_scaled.npz')
X_train = arrays['X']
y_train = arrays['y']
arrays = np.load('data/iris_test_scaled.npz')
X_test = arrays['X']
y_test = arrays['y']
print('X shape = {}'.format(X_train.shape) + '\ny shape = {}'.
,‚Üíformat(y_train.shape))
print('X shape = {}'.format(X_test.shape) + '\ny shape = {}'.
,‚Üíformat(y_test.shape))
X shape = (120, 4)
y shape = (120,)
X shape = (30, 4)
y shape = (30,)
For illustration purposes, here we only use the second and third features (sepal width
and petal length) of the data to develop and test our classiÔ¨Åers:
X_train = X_train[:,[1,2]]
X_test = X_test[:,[1,2]]
X_train.shape
(120, 2)
We use the technique used in Section 5.1.1 to draw the decision boundaries of
Logistic Regression with l2 Regularizarion:
%matplotlib inline
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn.linear_model import LogisticRegression as LRR
color = ('aquamarine', 'bisque', 'lightgrey')
cmap = ListedColormap(color)
mins = X_train.min(axis=0) - 0.1
maxs = X_train.max(axis=0) + 0.1
x = np.arange(mins[0], maxs[0], 0.01)
y = np.arange(mins[1], maxs[1], 0.01)
X, Y = np.meshgrid(x, y)
coordinates = np.array([X.ravel(), Y.ravel()]).T
fig, axs = plt.subplots(1, 2, figsize=(6, 2), dpi = 150)
fig.tight_layout()
C_val = [0.01, 100]
for ax, C in zip(axs.ravel(), C_val):
lrr = LRR(C=C)

170
6 Linear Models
lrr.fit(X_train, y_train)
Z = lrr.predict(coordinates)
Z = Z.reshape(X.shape)
ax.tick_params(axis='both', labelsize=6)
ax.set_title('LRR Decision Regions: C=' + str(C), fontsize=8)
ax.pcolormesh(X, Y, Z, cmap = cmap, shading='nearest')
ax.contour(X ,Y, Z, colors='black', linewidths=0.5)
ax.plot(X_train[y_train==0, 0], X_train[y_train==0, 1],'g.',‚ê£
,‚Üímarkersize=4)
ax.plot(X_train[y_train==1, 0], X_train[y_train==1, 1],'r.',‚ê£
,‚Üímarkersize=4)
ax.plot(X_train[y_train==2, 0], X_train[y_train==2, 1],'k.',‚ê£
,‚Üímarkersize=4)
if (C==C_val[0]): ax.set_ylabel('petal length (normalized)',‚ê£
,‚Üífontsize=7)
ax.set_xlabel('sepal width (normalized)', fontsize=7)
print('The accuracy for C={} on the training data is {:.3f}'.
,‚Üíformat(C, lrr.score(X_train, y_train)))
print('The accuracy for C={} on the test data is {:.3f}'.format(C,‚ê£
,‚Üílrr.score(X_test, y_test)))
The accuracy for C=0.01 on the training data is 0.817
The accuracy for C=0.01 on the test data is 0.833
The accuracy for C=100 on the training data is 0.950
The accuracy for C=100 on the test data is 0.967
As we said, a larger value of C points to less regularization and a stronger Ô¨Åt of
the LRR model on the data. This is clear from the right plot in Fig. 6.2.2 where
we observe that the data can be indeed separated pretty well by two lines. However,
a lower C means more regularization as on the left plot in this Ô¨Ågure. This is also
apparent from the accuracy estimated on the training data. While a C = 100 can
separate the training data with 95% accuracy (this is known as apparent accuracy or
resubstitution accuracy), the model with C = 0.01 leads to a resubstitution accuracy
of 81.7%. Here it happens that C = 100 exhibits a higher accuracy on the test data;
however, it is not always the case. By that we mean:
1) it is not the case that a higher value of C is always better than a lower C or vice
versa; and
2) a larger training accuracy does not necessarily lead to a larger test accuracy. If
our model overÔ¨Åts the training data, it can show a high resubstitution accuracy
but performs poorly on test data. ‚ñ†
Quantifying Odds Ratio and Relative Change in Odds: An important question
that is often raised when logistic regression classiÔ¨Åer is used is the interpretation of
its coeÔ¨Écients. To do so, we need to Ô¨Årst look more closely to the concept of odds.
Let r0 denote the odds of Y = 1 (versus Y = 0); that is to say,

6.2 Linear Models for ClassiÔ¨Åcation
171
2
1
0
1
2
sepal width (normalized)
1.5
1.0
0.5
0.0
0.5
1.0
1.5
petal length (normalized)
LRR Decision Regions: C=0.01
2
1
0
1
2
sepal width (normalized)
LRR Decision Regions: C=100
Fig. 6.3: The scatter plot of the normalized Iris dataset for two features and decision
regions for logistic regression with l2 regularization (LRR) for C = 0.01 (left) and
C = 100 (right). The green, red, and black points show points corresponding to
setosa, versicolor, and virginica Iris Ô¨Çowers, respectively. The LRR decision regions
for each class are colored similarly.
r0
‚àÜ= odds = P(Y = 1|x)
P(Y = 0|x)
1= eaT x+b = ea1x1+a2x2+...+ap xp+b ,
(6.48)
where
1= is a consequence of (6.32) and (6.33), and x = [x1, x2, . . ., xp]T where we
assume all features xi, i = 1, . . ., p, are numeric features. Now, we deÔ¨Åne r1 similarly
to r0 except that we assume it is the odds for one unit of increase in x1; that is,
r1
‚àÜ= odds = P(Y = 1|Àúx1)
P(Y = 0|Àúx1) = ea1(x1+1)+a2x2+...+ap xp+b ,
(6.49)
where Àúx1 = [x1 + 1, x2, . . ., xp]T. From (6.48) and (6.49), the odds ratio become
odds ratio = r1
r0
= ea1(x1+1)+a2x2+...+ap xp+b
ea1x1+a2x2+...+ap xp+b
= ea1 .
(6.50)
This relationship is the key to interpreting coeÔ¨Écients of logistic regression. This
is because it shows that an increase of one unit in x1 leads to multiplying odds of
Y = 1 by ea1, which is a non-linear function of a1. Nevertheless, if a1 > 0, an
increase of one unit in x1 will increase the odds of Y = 1 according to r1 = r0ea1.
At the same time, if a1 < 0, an increase of one unit in x1 will decrease the odds of
Y = 1, again according to r1 = r0ea1. The same argument is applicable to any other
coeÔ¨Écient in the logistic regression with numeric features; that is, ai shows that a
one unit increase in xi changes the odds of Y = 1 according to ri = r0eai where

172
6 Linear Models
ri
‚àÜ= odds = P(Y = 1|Àúxi)
P(Y = 0|Àúxi) = ea1x1+...ai(xi+1)+...+ap xp+b ,
(6.51)
and where Àúxi = [x1, . . ., xi + 1, . . ., xp]T. We can also deÔ¨Åne the relative change in
odds (RCO) as a function of one unit in xi:
RCO% = ri ‚àír0
r0
√ó 100 = r0eai ‚àír0
r0
√ó 100 = (eai ‚àí1) √ó 100 .
(6.52)
As an example, suppose we have trained a logistic regression classiÔ¨Åer where a2 =
‚àí0.2. This means a one unit increase in x2 leads to an RCO of -18.1% (decreases
odds of Y = 1 by 18.1%).
Although (6.51) and (6.52) were obtained for one unit increase in xi, we can
extend this to any arbitrary number. In general a k unit increase (a negative k points
to a decrease) in the value of xi leads to:
RCO% = ri ‚àír0
r0
√ó 100 = (ekai ‚àí1) √ó 100 .
(6.53)
Example 6.2 In this example, we work with a genomic data (gene expressions) taken
from patients who were aÔ¨Äected by oral leukoplakia. Data was obtained from Gene
Expression Omnibus (GEO) with accession # GSE26549. The data includes 19897
features (19894 genes and three binary clinical variables) and 86 patients with a
median follow-up of 7.11 years. Thirty-Ô¨Åve individuals (35/86; 40.7%) developed
oral cancer (OC) over time and the rest did not. The data matrix is stored in a Ô¨Åle
named ‚ÄúGenomicData_OralCancer.txt‚Äù and the variable names (one name for each
column) is stored in ‚ÄúGenomicData_OralCancer_var_names.txt‚Äù, and both of these
Ô¨Åles are available in ‚Äúdata‚Äú folder. The goal is to build a classiÔ¨Åer to classify those
who developed OC from those who did not. The outcomes are stored in column
named ‚Äúoral_cancer_output‚Äù (-1 for OC patients and 1 for non-OC patients). For this
purpose, we use a logistic regression with l1 regularization with a C = 16.
# load the dataset
import numpy as np
import pandas as pd
data = pd.read_csv('data/GenomicData_OralCancer.txt', sep=" ",‚ê£
,‚Üíheader=None)
data.head()
0
1
2
3
4
5
6
7
8
9
...
\
0
4.44
8.98
5.58
6.89
6.40
6.35
7.12
6.87
7.18
7.81 ...
1
4.59
8.57
6.57
7.25
6.44
6.34
7.40
6.91
7.18
8.12 ...
2
4.74
8.80
6.22
7.13
6.79
6.08
7.42
6.93
7.48
8.82 ...
3
4.62
8.77
6.32
7.34
6.29
5.65
7.12
6.89
7.27
7.18 ...
4
4.84
8.81
6.51
7.16
6.12
5.99
7.13
6.85
7.21
7.97 ...

6.2 Linear Models for ClassiÔ¨Åcation
173
19888
19889
19890
19891
19892
19893
19894
19895
19896
19897
0
6.08
5.49
12.59
11.72
8.99
10.87
1
0
1
1
1
6.17
6.08
13.04
11.36
8.96
11.03
1
0
0
1
2
6.39
5.99
13.29
11.87
8.63
10.87
1
1
0
-1
3
6.32
5.69
13.33
12.02
8.86
11.08
0
1
1
1
4
6.57
5.59
13.22
11.87
8.89
11.15
1
1
0
1
[5 rows x 19898 columns]
header = pd.read_csv('data/GenomicData_OralCancer_var_names.txt',‚ê£
,‚Üíheader=None)
data.columns=header.iloc[:,0]
data.loc[:,"oral_cancer_output"] = data.loc[:,"oral_cancer_output"]*-1‚ê£
,‚Üí# to encode the positive class (oral cancer) as 1
data.head()
0
OR4F17
SEPT14
OR4F16
GPAM
LOC100287934
LOC643837
SAMD11
KLHL17
0
4.44
8.98
5.58
6.89
6.40
6.35
7.12
6.87
1
4.59
8.57
6.57
7.25
6.44
6.34
7.40
6.91
2
4.74
8.80
6.22
7.13
6.79
6.08
7.42
6.93
3
4.62
8.77
6.32
7.34
6.29
5.65
7.12
6.89
4
4.84
8.81
6.51
7.16
6.12
5.99
7.13
6.85
0
PLEKHN1
ISG15
...
MRGPRX3
OR8G1
SPRR2F
NME2
GRINL1A
UBE2V1
\
0
7.18
7.81
...
6.08
5.49
12.59
11.72
8.99
10.87
1
7.18
8.12
...
6.17
6.08
13.04
11.36
8.96
11.03
2
7.48
8.82
...
6.39
5.99
13.29
11.87
8.63
10.87
3
7.27
7.18
...
6.32
5.69
13.33
12.02
8.86
11.08
4
7.21
7.97
...
6.57
5.59
13.22
11.87
8.89
11.15
0
alcohol
smoking
histology
oral_cancer_output
0
1
0
1
-1
1
1
0
0
-1
2
1
1
0
1
3
0
1
1
-1
4
1
1
0
-1
[5 rows x 19898 columns]
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression as LRR
y_train = data.oral_cancer_output
X_train = data.drop('oral_cancer_output', axis=1)
scaler = StandardScaler().fit(X_train)
X_train = scaler.transform(X_train)
lrr = LRR(penalty = 'l1', C=16, solver = 'liblinear', random_state=42)
lrr.fit(X_train, y_train)

174
6 Linear Models
LogisticRegression(C=16, penalty='l1', random_state=42,‚ê£
,‚Üísolver='liblinear')
Here we can check the number of non-zero coeÔ¨Écients:
coeffs = lrr.coef_.ravel()
np.sum(coeffs
!= 0)
292
Therefore, there are 292 features (here all are genes) with non-zero coeÔ¨Écients
in our linear logistic regression classiÔ¨Åer. In other words, the use of l1 regularization
with logistic regression leads to 0 coeÔ¨Écients for 19606 features. This is the con-
sequence of the internal feature selection mechanism of logistic regression with l1
regularization that was mentioned in Section 6.2.2. This type of feature selection is
also known as embedded feature selection (more on this in Chapter 10).
Next we sort the identiÔ¨Åed 292 features based on their odds ratio:
non_zero_coeffs = coeffs[coeffs!= 0]
ORs=np.exp(non_zero_coeffs) #odds ratios
sorted_args = ORs.argsort()
ORs_sorted=ORs[sorted_args]
ORs_sorted[:10]
array([0.74175367, 0.76221689, 0.77267586, 0.77451542, 0.78316466, 0.
,‚Üí8163162 , 0.83429172, 0.83761203, 0.84076963, 0.84205156])
feature_names = header.iloc[:-1,0].values
selected_features = feature_names[coeffs!= 0]
selected_features_sorted = selected_features[sorted_args]
selected_features_sorted[0:10]
array(['DGCR6', 'ZNF609', 'CNO', 'RNASE13', 'BRD7P3', 'HHAT', 'UBXN1',
'C15orf62', 'ORAI2', 'C1orf151'], dtype=object)
We would like to plot the RCOs for 10 genes that led to the highest decrease
and 10 genes that led to the highest increase in RCO by one unit increase in their
expressions.
RCO=(ORs_sorted-1.0)*100
selected_RCOs=np.concatenate((RCO[:10], RCO[-10:]))
features_selected_RCOs = np.concatenate((selected_features_sorted[:10],‚ê£
,‚Üíselected_features_sorted[-10:]))
features_selected_RCOs

6.2 Linear Models for ClassiÔ¨Åcation
175
array(['DGCR6', 'ZNF609', 'CNO', 'RNASE13', 'BRD7P3', 'HHAT', 'UBXN1',‚ê£
,‚Üí'C15orf62', 'ORAI2', 'C1orf151', 'KIR2DS5', 'LOC136242', 'SNORD33',‚ê£
,‚Üí'SEMG1', 'LOC100133746', 'SNORD41', 'C6orf10', 'C21orf84', 'DNAJA2',‚ê£
,‚Üí'MMP26'], dtype=object)
from matplotlib import pyplot
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
pyplot.bar(features_selected_RCOs, selected_RCOs, color='black')
plt.xlabel('genes')
plt.ylabel('RCO %')
plt.xticks(rotation=90, fontsize=12)
plt.yticks(fontsize=12)
DGCR6
ZNF609
CNO
RNASE13
BRD7P3
HHAT
UBXN1
C15orf62
ORAI2
C1orf151
KIR2DS5
LOC136242
SNORD33
SEMG1
LOC100133746
SNORD41
C6orf10
C21orf84
DNAJA2
MMP26
genes
20
10
0
10
20
30
40
50
RCO %
Fig. 6.4: RCO vs. genes. The Ô¨Ågure shows 10 genes that led to the highest decrease
and 10 genes that led to the highest increase in RCOs by one unit increase in their
expressions.
Here we can see that a one unit increase in the expression of DGCR6 leads to an
RCO of -25.8%. Does it mean a one unit decrease in its expression leads to an RCO
of 25.8%? (see Exercise 8). ‚ñ†

176
6 Linear Models
Although we introduced the Bayes classiÔ¨Åer at the outset of this chapter,
linear models are not the only available Bayes plug-in rules. For instance,
the kNN classiÔ¨Åer introduced in Chapter 5 is itself a Bayes plug-in rule
that estimates the posterior P(Y = i|x) used in (6.11) by the proportion
of observations that belong to class i within the k nearest neighbors of x.
However, because it does not make a ‚Äúparametric‚Äù assumption about the
form of the posterior, it is categorized as a non-parametric Bayes plug-in
rule (Braga-Neto, 2020).
6.3 Linear Models for Regression
Linear models for regression are models that the estimate of the response variable
is a linear function of parameters (Bishop, 2006, p.138), (Hastie et al., 2001, p.44);
that is,
f (x) = aTx + b .
(6.54)
When p = 1, (6.54) is known as simple linear regression and when p > 1 it
is referred to as multiple linear regression. Given a set of training data Str =
{(x1, y1), (x2, y2), . . ., (xn, yn)}, the most common approach to estimate Œ≤
‚àÜ= [b, aT]T,
which is the (p+1)-dimensional column vector of unknown parameters, is to use the
least squares method. In this method the goal is to minimize the residual sum of
squares (RSS) and the solution is known as the ordinary least squares solution; that
is,
ÀÜŒ≤ols = argmin
Œ≤
RSS(Œ≤),
(6.55)
where
RSS(Œ≤) =
n
√ï
j=1
 yj ‚àíf (xj)2 =
n
√ï
j=1
(yj ‚àíŒ≤T Àúxj)2 ,
(6.56)
and where Àúxj is an augmented feature vector deÔ¨Åned as Àúxj = [1, xT
j ]T, ‚àÄj; that is,
we add a 1 in the Ô¨Årst position and other elements are identical to xj. We can write
(6.56) in a matrix form as

6.3 Linear Models for Regression
177
RSS(Œ≤) = (y ‚àíXŒ≤)T(y ‚àíXŒ≤) = ||y ‚àíXŒ≤||2
2 ,
(6.57)
where X is a n√ó(p+1) matrix such that the jth row is the transpose of jth augmented
feature vector; that is, X = [Àúx1, Àúx2, . . ., Àúxn]T. By taking the gradient and setting to
zero we write
‚àÇRSS(Œ≤)
‚àÇŒ≤
= ‚àí2XTy + 2XTXŒ≤ = 0,
(6.58)
where we used the fact that for two vectors w and Œ≤ and a symmetric matrix W,
‚àÇwT Œ≤
‚àÇŒ≤
= ‚àÇŒ≤Tw
‚àÇŒ≤
= w,
(6.59)
‚àÇŒ≤TWŒ≤
‚àÇŒ≤
= 2WŒ≤ .
(6.60)
Assuming X has full column rank p+1 ‚â§n so that XTX is invertible, from (6.58) we
obtain the well-known normal equation that provides a unique closed-form solution
for the parameters that minimize the residual sum of squares:
ÀÜŒ≤ols = (XTX)‚àí1XTy .
(6.61)
Similar to regularization techniques that we saw in Section 6.2.2, here rather
than minimizing the residual sum of squares as in (6.58), one may minimize the
regularized form of this objective function using ridge, lasso, or elastic-net penalty.
In particular, the ridge regression solution is obtained by
ÀÜŒ≤ridge = argmin
Œ≤

RSS(Œ≤) + Œ±||a||2
2

.
(6.62)
Similar to minimizing (6.57), (6.62) can be minimized by setting the gradient to
zero. This leads to the following closed-form solution:
ÀÜŒ≤ridge = (XTX + Œ±Ip+1)‚àí1XTy,
(6.63)
where Ip+1 is the identity matrix of p + 1 dimension. The lasso solution is obtained
as
ÀÜŒ≤lasso = argmin
Œ≤

RSS(Œ≤) + Œ±||a||1

,
(6.64)
and the elastic-net solution is given by

178
6 Linear Models
ÀÜŒ≤elastic-net = argmax
Œ≤

RSS(Œ≤) + Œ± ŒΩ||a||1 + Œ±1 ‚àíŒΩ
2
||a||2
2

,
(6.65)
where both Œ± and ŒΩ are tuning parameters that create a compromise between no
regularization, and l1 and l2 regularizations. In particular when Œ± = 0, both (6.64)
and (6.65) reduce to (6.61), which means both the lasso and the elastic-net reduce to
the ordinary least squares. At the same time, for ŒΩ = 1, (6.65) becomes (6.64); that
is, the solution of elastic-net becomes that of lasso. In contrast with (6.45)-(6.47)
where a larger C was indicator of less regularization, here Œ± is chosen such that a
larger Œ± implies stronger regularization. This choice of notation is made to make the
equations compatible with scikit-learn implementation of these methods. As there is
no closed-form solution for lasso and elastic-net, iterative optimization methods are
used to obtain the solution of (6.64) and (6.65). Last but not least, once we obtain an
estimate of Œ≤ from either of (6.61), (6.63), (6.64), or (6.65), we replace the estimate
of parameters in (6.54) to estimate the target for a given x.
Example 6.3 Suppose we have the following training data where for each value of
predictor x, we have the corresponding y right below that. We would like to estimate
the coeÔ¨Écient of a simple linear regression using ordinary least squares to have
ÀÜy = ÀÜŒ≤0 + ÀÜŒ≤1x.
Table 6.1: The data for Example 6.3.
x
‚àí1
2
‚àí1
0
0
y
0
2
2
0
‚àí1
In doing so, we:
a) use hand calculations to Ô¨Ånd ÀÜŒ≤0 and ÀÜŒ≤1; and
b) use hand calculations to determine the ÀÜR2 evaluated on this training data.
a) We need to Ô¨Ånd the solution from (6.61). For that purpose, Ô¨Årst we need to
construct matrix X where as explained X = [Àúx1, Àúx2, . . ., Àúxn]T and where Àúx is the
augmented feature vector. Therefore, we have
X =
Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1 ‚àí1
1 2
1 ‚àí1
1 0
1 0
Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
y =
Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
0
2
2
0
‚àí1
Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(6.66)

6.3 Linear Models for Regression
179
Therefore, from (6.61), we have:
ÀÜŒ≤ols =
¬©¬≠¬≠¬≠¬≠¬≠
¬´
 1 1 1 1 1
‚àí1 2 ‚àí1 0 0

Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1 ‚àí1
1 2
1 ‚àí1
1 0
1 0
Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
¬™¬Æ¬Æ¬Æ¬Æ¬Æ
¬¨
‚àí1
 1 1 1 1 1
‚àí1 2 ‚àí1 0 0

Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
0
2
2
0
‚àí1
Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
=
 3
51
3

.
(6.67)
Therefore, Œ≤0 = 3
5 and Œ≤1 = 1
3.
b) From Section 5.2.2, in which ÀÜR2 was deÔ¨Åned, we Ô¨Årst need to Ô¨Ånd RSS and TSS:
RSS =
m
√ï
i=1
(yi ‚àíÀÜyi)2 ,
(6.68)
TSS =
m
√ï
i=1
(yi ‚àí¬Øy)2 ,
(6.69)
where ÀÜyi is the estimate of yi for xi obtained from our model by ÀÜyi = ÀÜŒ≤0 + ÀÜŒ≤1xi, and
¬Øy is the average of responses. From part (a),
ÀÜy1 = 3
5 ‚àí1
3 = 4
15 ,
ÀÜy2 = 3
5 + 2
3 = 19
15 ,
ÀÜy3 = ÀÜy1 ,
ÀÜy4 = 3
5 ,
ÀÜy5 = ÀÜy4 .
Therefore,
RSS =

0 ‚àí4
15
2
+

2 ‚àí19
15
2
+

2 ‚àí4
15
2
+

0 ‚àí3
5
2
+

‚àí1 ‚àí3
5
2
‚âà6.53 .
To calculate the TSS, we need ¬Øy, which is ¬Øy = 0+2+2+0‚àí1
5
= 3
5. Therefore, we have
TSS =

0 ‚àí3
5
2
+

2 ‚àí3
5
2
+

2 ‚àí3
5
2
+

0 ‚àí3
5
2
+

‚àí1 ‚àí3
5
2
= 36
5 = 7.2 .
Thus,
ÀÜR2 = 1 ‚àíRSS
TSS = 1 ‚àí6.53
7.2 = 0.092,
which is relatively a low ÀÜR2. ‚ñ†
Scikit-learn implementation: The ordinary least squares, ridge, lasso, and elastic-
net solution of linear regression are implemented in LinearRegression, Ridge,

180
6 Linear Models
Lasso, and ElasticNet classes from sklearn.linear_model module, respec-
tively. For LinearRegression, Lasso, and ElasticNet, scikit-learn currently
uses a Ô¨Åxed algorithm to estimate the solution (so no parameter to change for the
‚Äúsolver‚Äù), but for Ridge there are plenty of options. More details on solvers are given
next.
√â
More on solver: To minimize ||y ‚àíXŒ≤||2
2 in (6.57), we assumed p + 1 ‚â§n
to present the normal equation shown in (6.61). However, similar to lsqr solver
that we discussed in Section 6.2.1 for LinearDiscriminantAnalysis, the lsqr
solver in LinearRegression also relies on scipy.linalg.lstsq, which as men-
tioned before by default is based on DGELSD routine from LAPACK and Ô¨Ånds the
minimum-norm solution to the problem XŒ≤ = y; that is to say, minimizes ||y‚àíXŒ≤||2
2
in (6.57). As for the ridge regression implemented via Ridge class, there are cur-
rently several solvers with the current default being auto, which determines the
solver automatically. Some of these solvers are implemented similar to our discus-
sion in Section 6.2.1. For example, svd solver computes the SVD decomposition
of X using scipy.linalg.svd, which is itself based on DGESDD routine from
LAPACK. Those dimensions with a singular value less than 10‚àí15 are discarded.
Once the SVD decomposition is calculated, the ridge solution given in (6.63) is
computed (using _solve_svd() function). Here we explain the rationale. Suppose
X is an arbitrary m √ó n real matrix. We can decompose X as X = UDVT where
UUT = UTU = Im, VVT = VTV = In, and D is a rectangular m √ó n diagonal
matrix with min{m, n} non-negative real numbers known as singular values ‚Äî this
is known as SVD decomposition of X. Then we can write (6.63) as follows:
ÀÜŒ≤ridge = (XTX + Œ±Ip+1)‚àí1XTy
= (VD2VT + Œ±VVT)‚àí1VDUTy
= (VT)‚àí1(D2 + Œ±Ip+1)‚àí1V‚àí1VDUTy
= V(D2 + Œ±Ip+1)‚àí1DUTy,
(6.70)
and note that because all matrices in (D2 + Œ±Ip+1)‚àí1D are diagonal, this term is also
a diagonal matrix where each diagonal element in each dimension can be computed
easily by Ô¨Årst adding Œ± to the square of the singular value for that dimension,
inverting, and then multiplying by the same singular value.The cholesky choice for
the ridge solver relies on scipy.linalg.solve, which is based on DGETRF and
DGETRS from LAPACK to compute the LU factorization and then Ô¨Ånd the solution
based on the factorization. As for Lasso and ElasticNet, scikit-learn currently
uses coordinate descent solver.
‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-

6.3 Linear Models for Regression
181
Exercises:
Exercise 1:
1) Use appropriate readerfunction frompandas toreadtheGenomicData_orig.csv
in data folder (this is the same dataset we used in Exercise 2 in Chapter 2). Drop
the Ô¨Årst two columns as they have redundant information;
2) Before splitting the data as discussed next, seed NumPy random number gen-
erator (RNG) as np.random.seed(42). This has a type of global eÔ¨Äect on
any function that uses NumPy and makes your results reproducible. In contrast,
setting the seed in train_test_split by random_state=42 makes only your
train-test split reproducible and has no eÔ¨Äect on random generators, for example,
used in liblinear solver;
3) Split the data into test and train with a 0.25 test size;
4) Impute the missing data in both training and test using SimpleImputer trans-
former (set the strategy to median) from sklearn.impute module (note that
for test data imputation, you need to use the extracted information from training
data);
5) Standardize features;
6) Train two logistic regression with l1 penalty (use ‚Äòliblinear‚Äô solver) and C = 0.05
and C = 10. What are their accuracies on test data?
7) Train an LDA classiÔ¨Åer (default solver). Report its accuracy;
8) How many features are ‚Äúselected‚Äù (i.e., have non-zero coeÔ¨Écients) in logistic
regression with l1 for C = 0.05 and for C = 10 and how many are selected by
LDA (have non-zero coeÔ¨Écients)?
Exercise 2: Suppose we have a training dataset collected for a binary classiÔ¨Åcation
problem. Using this training data, the class-speciÔ¨Åc sample means are ÀÜ¬µ0 = [4, 1]T
and ÀÜ¬µ1 = [0, ‚àí1]T, and the pooled sample covariance matrix is ÀÜŒ£ =
1 0
0 2

. We
would like to use these estimates to construct an LDA classiÔ¨Åer. Determine the range
of P(Y = 1) such that an observation x = [1.5, 0]T is classiÔ¨Åed to class 1 using the
constructed LDA classiÔ¨Åer.
Exercise 3: Suppose we have a three-class classiÔ¨Åcation problem where classes are
normally distributed with means ¬µT
0 = [0, 1]T, ¬µT
1 = [1, 0]T, and ¬µT
2 = [0.5, 0.5]T
and a common covariance matrix
Œ£ =
2 0
0 4

.
Classify an observation x = [1, 1]T according to the Bayes rule (assuming classes
are equiprobable).
√â
Exercise 4: Prove that the optimal classiÔ¨Åer œàB(x) (also known as Bayes clas-
siÔ¨Åer) is obtained as

182
6 Linear Models
œàB(x) =
(
1
if P(Y = 1|x) > P(Y = 0|x),
0
otherwise .
(6.71)
Exercise 5: The class-conditional probability density functions in a three class clas-
siÔ¨Åcation problem (labeled as 1, 2, and 3) are uniform distributions depicted in the
Ô¨Ågure below. Assuming P(Y = 1) = 2P(Y = 2) = 3P(Y = 3), what is the class to
which x = 1.75 is classiÔ¨Åed based on the Bayes classiÔ¨Åer?
1 
3 3.5 
x 
p(x| Y=i) 
4 
i=1 
i=3 
i=2 
2 
0 
1.5 
Fig. 6.5: Class-conditional densities used Exercise 5.
A) Class 1
B) Class 2
C) Class 3
D) there is a tie between Class 1 and Class 3
E) there is a tie between Class 2 and Class 3
Exercise 6: Use the two assumptions in development of LDA in (6.10) to achieve
(6.13).
Exercise 7: Show that the Bayes plug-in rule for Gaussian class-conditional densi-
ties with diÔ¨Äerent covariance matrices has the following form, which is known as
quadratic discriminant analysis (QDA):
œà(x) =
(
1
if
1
2xTA1x + A2x + a > 0,
0
otherwise,
(6.72)

6.3 Linear Models for Regression
183
where
A1 = ÀÜŒ£
‚àí1
0 ‚àíÀÜŒ£
‚àí1
1 ,
(6.73)
A2 = ÀÜ¬µT
1 ÀÜŒ£
‚àí1
1 ‚àíÀÜ¬µT
0 ÀÜŒ£
‚àí1
0 ,
(6.74)
a = 1
2 ÀÜ¬µT
0 ÀÜŒ£
‚àí1
0 ÀÜ¬µ0 ‚àí1
2 ÀÜ¬µT
1 ÀÜŒ£
‚àí1
1 ÀÜ¬µ1 ‚àí1
2 log
 | ÀÜŒ£1|
| ÀÜŒ£0|

+ log

P(Y = 1)
1 ‚àíP(Y = 1)

,
(6.75)
and where
ÀÜŒ£i =
1
ni ‚àí1
√ï
yj=i
(xj ‚àíÀÜ¬µi)(xj ‚àíÀÜ¬µi)T .
(6.76)
Exercise 8: For a binary logistic regression classiÔ¨Åer, a one unit increase in the value
of a numeric feature leads to an RCO of Œ±.
a) What will be the RCO for one unit decrease in the value of this numeric feature?
b) Suppose Œ± = 0.2 (i.e., 20%). What is the RCO for one unit decrease in the value
of the feature?
Exercise 9: Suppose we have two data matrices Xtrain and Xtest
1
. We train a multiple
linear regression model using Xtrain and in order to assess that model using RSS
metric, we apply it on Xtest
1
. This leads to a value of RSS denoted as RSS1. Then we
augment Xtest
1
by 1 observation and build a larger test dataset of 101 observations
denoted as Xtest
2
(that is to say, 100 observations in Xtest
2
are the same observations
in Xtest
1
). Applying the previously trained model on Xtest
2
leads to a value of RSS
denoted as RSS2. Which of the following is true? Explain why?
A) Depending on the actual observations, both RSS1 ‚â•RSS2 and RSS2 ‚â•RSS1
are possible
B) RSS1 ‚â•RSS2
C) RSS2 ‚â•RSS1
Exercise 10: Suppose in a binary classiÔ¨Åcation problem with equal prior probability
of classes, the class conditional densities are multivariate normal distributions with
a common covariance matrix: N(¬µ0, Œ£) and N(¬µ1, Œ£). Prove that the Bayes error ŒµB
is obtained as
ŒµB = Œ¶

‚àí‚àÜ
2

,
(6.77)
where Œ¶(.) denotes the cumulative distribution function of a standard normal random
variable, and ‚àÜ(known as the Mahalanobis distance between the two distributions)
is given by
‚àÜ=
q
(¬µ0 ‚àí¬µ1)T Œ£‚àí1(¬µ0 ‚àí¬µ1).
(6.78)
Exercise 11: Suppose in a binary classiÔ¨Åcation problem with equiprobable classes
(class 0 vs. class 1), the class-conditional densities are 100-dimensional normal

184
6 Linear Models
distributions with a common covariance matrix, which is a 100√ó100 identity matrix,
and means ¬µ0 for class 0 and ¬µ1 for class 1, which are two known 100-dimensional
vectors whereas each dimension of ¬µ0 has a value that is diÔ¨Äerent from the value of
the corresponding dimension in ¬µ1.
Benjamin considers only the Ô¨Årst Ô¨Åve dimensions in these normal distributions
and constructs the Bayes classiÔ¨Åer. We denote the Bayes error of this 5-dimensional
classiÔ¨Åcation problem by œµ‚àó
5. Ava considers only the Ô¨Årst Ô¨Åfty dimensions in these
Gaussian distributions and constructs the Bayes classiÔ¨Åer. We denote the Bayes error
of this 50-dimensional classiÔ¨Åcation problem by œµ‚àó
50. Which one shows the relation-
ship between œµ‚àó
5 and œµ‚àó
50?
Hint: use the result of Exercise 10.
A) we have œµ‚àó
5 > œµ‚àó
50
B) we have œµ‚àó
5 < œµ‚àó
50
C) Depending on the actual values of vectors ¬µ0 and ¬µ1, both œµ‚àó
5 > œµ‚àó
50 and œµ‚àó
5 < œµ‚àó
50
are possible
D) we have œµ‚àó
5 = œµ‚àó
50
Exercise 12: Suppose we would like to use scikit-learn to solve a multiple linear
regression problem using l1 regularization. Which of the following is a possible
option to use?
A) ‚Äúsklearn.linear_model.LogisticRegression‚Äù class with default choices of param-
eters
B) ‚Äúsklearn.linear_model.LogisticRegression‚Äù class by changing the default choice
of ‚Äúpenalty‚Äù and ‚Äúsolver‚Äù parameters to ‚Äúl1‚Äù and ‚Äúliblinear‚Äù, respectively.
C) ‚Äúsklearn.linear_model.Lasso‚Äù class with default choices of parameters
D) ‚Äúsklearn.linear_model.Ridge‚Äù class with default choices of parameters
Exercise 13: Suppose we have the following training data where x and y denote the
value of the predictor x and the target value, respectively. We would like to use this
data to train a simple linear regression using ordinary least squares. What is ÀÜy (the
estimate of y) for x = 10?
ùë•
0
2
1
-5 
3
-1
ùë¶
4
4
2
0
0
2
A) 4
B) 4.5
C) 5
D) 5.5
E) 6
E) 6.5

6.3 Linear Models for Regression
185
√â Exercise 14:
Suppose we have an electrical device and at each time point t the voltage at a speciÔ¨Åc
part of this device can only vary in the interval [1, 5]. If the device is faulty, this
voltage, denoted V(t), varies between [2, 5], and if it is non-faulty, the voltage can be
any value between [1, 3]. In the absent of any further knowledge about the problem,
(it is reasonable to) assume V(t) is uniformly distributed in these intervals (i.e., it has
uniform distributions when it is faulty or non-faulty). Based on these assumptions,
we would like to design a classiÔ¨Åer based on V(t) that can ‚Äúoptimally‚Äù (insofar as
the assumptions hold true) classiÔ¨Åes the status of this device at each point in time.
What is the optimal classiÔ¨Åer at each point in time? What is the error of the optimal
classiÔ¨Åer at each time point?
Exercise 15: Suppose in a binary classiÔ¨Åcation problem with equiprobable classes
(class 0 vs. class 1), the class-conditional densities are 100-dimensional normal
distributions with a common covariance matrix, which is a 100√ó100 identity matrix.
The mean of each feature for class 1 is 0; however, the mean of feature i for class 0,
denoted ¬µi, is
‚àö
i; that is, ¬µi =
‚àö
i, i = 1, 2, . . ., 100.
Benjamin considers the Ô¨Årst 30 dimensions (i.e., i = 1, 2, . . ., 30) in these normal
distributions and constructs the Bayes classiÔ¨Åer. We denote the Bayes error of this
30-dimensional classiÔ¨Åcation problem by œµ‚àó
30. Ava considers the last Ô¨Åve dimensions
(i.e., i = 96, 97, . . ., 100) in these Gaussian distributions and constructs the Bayes
classiÔ¨Åer. We denote the Bayes error of this 5-dimensional classiÔ¨Åcation problem by
œµ‚àó
5. Which one shows the relationship between œµ‚àó
5 and œµ‚àó
30?
A) we have œµ‚àó
5 > œµ‚àó
30
B) we have œµ‚àó
5 < œµ‚àó
30
C) we have œµ‚àó
5 = œµ‚àó
30
D) Depending on the training data, both œµ‚àó
5 > œµ‚àó
30 and œµ‚àó
5 < œµ‚àó
30 are possible

Chapter 7
Decision Trees
Decision trees are nonlinear graphical models that have found important applications
in machine learning mainly due to their interpretability as well as their roles in other
powerful models such as random forests and gradient boosting regression trees
that we will see in the next chapter. Decision trees resemble the principles of ‚Äú20-
questions‚Äù game in which a player chooses an object and the other player asks a series
of yes-no questions to be able to guess the object. In binary decision trees, the set
of arbitrary ‚Äúyes-no‚Äù questions in the game translate to a series of standard yes-no
(thereby binary) questions that we identify from data and the goal is to estimate
the value of the target variable for a given feature vector x. As opposed to arbitrary
yes-no questions in the game, the questions in decision rules are standardized to be
able to devise a learning algorithm. Unlike linear models that result in linear decision
boundaries, decision trees partition the feature space into hyper-rectangular decision
regions and, therefore, they are nonlinear models. In this chapter we describe the
principles behind training a popular type of decision trees known as CART (short
for ClassiÔ¨Åcation and Regression Tree). We will discuss development of CART for
both classiÔ¨Åcation and regression.
7.1 A Mental Model for House Price ClassiÔ¨Åcation
Consider a set of numeric feature variables. Given a feature vector x, a convenient
form to standardize possible yes-no questions that could be asked to estimate the
target is: ‚Äúis feature i less or equal to a threshold value?‚Äù These questions can then
be organized as a directed rooted tree all the way to the terminal nodes known as
leaf nodes. A directed rooted tree has a single node known as root from which all
other nodes are reachable. There is a unique path from the root to any other node ŒΩ
in the tree. Along this path, the node that immediately connects to a node ŒΩ is known
as the parent of ŒΩ, whereas ŒΩ is referred to as the child of the parent node. As a
result, the only node that has no parent is the root node and all other nodes have one
parent node. At the same time, the leaf nodes (terminal nodes) have no children. In
187
¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
A. Zollanvari, Machine Learning with Python, https://doi.org/10.1007/978-3-031-33342-2_7 

188
7 Decision Trees
a decision tree, leaf nodes contain the decision made for x (i.e., the estimate of the
target). Once a decision tree is trained, it is common to visualize its graph to better
understand the relationship between questions.
In this section, we use a hypothetical example with which we introduce a number
of practical concepts used in training decision trees. These concepts are: 1) splitting
node; 2) impurity; 3) splitter; 4) best split; 5) maximum depth; and 6) maximum
leaf nodes. In Section 5.2.2, we saw an application in which we aimed to solve a
regression problem to estimate the median house price of a neighborhood (MEDV)
based on a set of features. Suppose we wish to solve this example but this time we
entirely rely on our past experiences to design a simple binary decision tree that can
help classify MEDV to ‚ÄúHigh‚Äù or ‚ÄúLow‚Äù (relative to our purchasing power) based
on three variables: average number of rooms per household (RM), median house age
in block group (AGE), and median income in the district (INCM).
Suppose from our past experience (think of this as past data, or ‚Äútraining‚Äù data),
we believe that RM is an important factor in the sense that in many cases (but not
all), if RM is four or more, MEDV is ‚ÄúHigh‚Äù, and otherwise ‚ÄúLow‚Äù. Therefore, we
consider RM ‚â§4 as the Ô¨Årst splitting node in our decision tree (see Fig. 7.1a). Here,
we refer to this as a splitting node because the answer to the question ‚Äúis RM ‚â§4?‚Äù
splits our training data (past experiences) into two subsets: one subset contains all
neighborhoods with RM ‚â§4 and the other subset contains all neighborhoods with
RM > 4. This splitting combined with our past experiences also means that at
this stage we can even make a decision based on ‚ÄúRM ‚â§4?‚Äù because we believe
neighborhoods with RM ‚â§4 generally have low MEDV and neighborhoods with
RM > 4 generally have high MEDV.
There are two important concepts hidden in the aforementioned statements: 1)
impurity; and 2) splitter. Notice that we said RM ‚â§4 is important for ‚Äúmany cases
(but not all)‚Äù. In other words, if we decide to make a decision at this stage based on
RM ‚â§4, this decision is not perfect. Even though ‚ÄúRM ‚â§4?‚Äù splits the data into
two subsets, we can not claim that all neighborhoods in each subset are entirely from
one class. In other words, the split achieved by RM ‚â§4 is impure with respect to
class labels. Nevertheless, the reason we chose this speciÔ¨Åc split among all possible
splits was that we believe it can minimize the impurity of data subsets; that is to
say, one subset contains a large number of neighborhoods with high MEDV and
the other subset contains a large number of neighborhoods with low MEDV. This
strategy was indeed a splitting strategy, also known as splitter; that is to say, our
‚Äúsplitter‚Äù was to identify the best split in the sense of minimizing the impurity of
data subsets created as the result of yes/no answer to a standardized question of
‚Äúa variable ‚â§a threshold?‚Äù This means to Ô¨Ånd the best split, we need to identify
the combination of variable-threshold that minimizes the impurity of data subsets
created as the result of the split.
Now we need to decide whether we should stop further splitting or not. Suppose
we believe that we can still do better in terms of cutting up the decision space and,
for that reason, we refer back to our training data. In our experience, new houses
with AGE ‚â§3 have high price even if RM ‚â§4; otherwise, they have low price.
At the same time, for RM > 4, the price would potentially depend on INCM. In

7.1 A Mental Model for House Price ClassiÔ¨Åcation
189
particular, if these houses are in neighborhoods with a relatively low INCM, the price
is still low. This leads to a decision tree grown as much as shown in Fig. 7.1b with
two additional splitting nodes AGE ‚â§3 and INCM ‚â§5 and 3 leaf nodes at which
a decision is made about MEDV. Next based on our past experiences, we believe
that if RM > 4 & INCM > 5, the price depends on AGE (Fig. 7.1c). In particular,
RM > 4 & INCM > 5 & AGE ‚â§20, MEDV is high and for RM > 4 & INCM > 5
& AGE > 20, MEDV is low. At this point we stop splitting because either we think
terminal nodes are pure enough or further splitting makes the tree too complex. Fig.
7.1d shows the Ô¨Ånal decision tree.
yes
no
RM ‚â§4? 
(a)
INCM ‚â§5? 
AGE ‚â§3? 
yes
no
yes
yes
price = high
no
no
RM ‚â§4? 
price = low
price = low
(b)
AGE ‚â§20?
price = low
INCM ‚â§5? 
AGE ‚â§3? 
yes
no
yes
yes
price = high
no
no
RM ‚â§4? 
price = low
yes
no
(c)
AGE ‚â§20?
price = low
INCM ‚â§5? 
AGE ‚â§3? 
yes
no
yes
yes
price = high
no
no
RM ‚â§4? 
price = low
yes
price = high
no
price = low
(d)
Fig. 7.1: Development of a mental model for house price classiÔ¨Åcation. Here the
decision tree is grown based on our past experiences.
A question that is raised is, why don‚Äôt we want the tree to become too complex?
Suppose we grow a tree to the extent that all leaf nodes become pure; that is, they
contain training data from one class. This is generally doable unless all observations
in a node have the same values but from diÔ¨Äerent classes in which case no further
splitting is possible and the node remains impure. Is it a good practice to continue
growing the tree until all nodes are pure? Not necessarily because it is very likely
that the tree overÔ¨Åts the training data; that is, the tree closely follows the training
data but does not perform well on novel observations that are not part of training. In
other words, the decision tree does not generalize well.

190
7 Decision Trees
A follow-up question is then what are some properties using which we can control
the structural complexity of a tree? One metric to control the structural complexity
of a binary tree is the maximum number of links between the root node and any of
the leaf nodes. This is known as the maximum depth ‚Äî the depth of a node is the
number of links between the node and the root node. For example, the maximum
depth of the decision tree in Fig. 7.1d is 3. If we preset the maximum depth of a
binary decision tree, it limits both the number of leaf nodes and the total number
of nodes. Suppose before growing a binary decision tree, we set 3 as the maximum
depth. What is the maximum number of leaf nodes and the maximum number of
nodes (including the root and other splitting nodes)? The maximum happens if all
leaf nodes have depth of 3. Therefore, we have a maximum of 8 leaf nodes and at
most 1 (root) + 2 + 4 + 8 (leaf) = 15 nodes. In general for a binary decision tree of
depth m, we have at most 2m leaf nodes and 2m+1‚àí1 nodes. Alternatively, rather than
limiting the maximum depth, one may restrict the maximum number of leaf nodes.
That will also restrict the maximum depth and the maximum number of nodes.
AGE
RM
INCM
4 
3
ùëπùíâùíäùíàùíâ
ùëπùíçùíêùíò
5
20
Fig. 7.2: Partitioning the feature space into Ô¨Åve boxes using the decision tree shown
in Fig. 7.1d. The decision regions for the ‚Äúhigh‚Äù- and ‚Äúlow‚Äù-MEDV classes denoted
by Rhigh and Rlow are shown in gray and blue, respectively. There are two gray
boxes and three blue boxes corresponding to the labels given by the leaf nodes in
Fig. 7.1d.
As we said before, decision trees partition the feature space into hyper-rectangles
(boxes). Each box corresponds to a decision rule obtained by traversing from the
root node to the leaf that includes the label for any observation falling into that
rectangular region. Fig. 7.2 shows partitioning of the feature space into Ô¨Åve boxes

7.2 CART Development for ClassiÔ¨Åcation:
191
using the decision tree shown in Fig. 7.1d. Although in the Ô¨Ågure these boxes appear
to have a Ô¨Ånite size, each box has an inÔ¨Ånite length over at least one of the RM, AGE,
or INCM axes because, in theory at least, there is no speciÔ¨Åc upper limit for these
variables. As an example, let us consider the gray box on the left bottom corner. This
box corresponds to the gray label assigned by traversing from the root to left most
leaf in Fig. 7.1d. This traverse corresponds to the following decision rule: assign a
‚Äúhigh‚Äù label when RM ‚â§4 AND AGE ‚â§3.
Although in this example we introduced several important concepts used in train-
ing and functionality of decision trees, we did not really present any speciÔ¨Åc algo-
rithm using which a tree is trained from a data at hand. There are multiple algorithms
for training decision trees, for instance, CHAID (Kass, 1980), ID3 (Quinlan, 1986),
C4.5 (Quinlan, 1993), CRUISE (Kim and Loh, 2001), and QUEST (Loh and Shih,
1997), to just name a few. Nevertheless, the binary recursive partitioning mechanism
explained here imitates closely the principles behind training a popular type of deci-
sion trees known as CART (short for ClassiÔ¨Åcation and Regression Trees) (Breiman
et al., 1984). In the next section, we will discuss this algorithm for training and using
decision trees in both classiÔ¨Åcation and regression.
7.2 CART Development for ClassiÔ¨Åcation:
7.2.1 Splits
For a numeric feature xj, CART splits data according to the outcome of questions
of the form: ‚Äúxj ‚â§Œ∏?‚Äù. For a nominal feature (i.e., categorical with no order) they
have the form ‚Äúxj ‚ààA‚Äù where A is a subset of all possible values taken by xj.
Regardless of testing a numeric or nominal feature, these questions check whether
the value of xj is in a subset of all possible values that it can take. Hereafter, and for
simplicity we only consider numeric features. As the split of data is determined by
the jth coordinate (feature) in the feature space and a threshold Œ∏, for convenience
we refer to a split as Œæ = (j, Œ∏) ‚ààŒû where Œû denotes the set of all possible Œæ that
create a unique split of data. It is important to notice that the size of Œû is not inÔ¨Ånite.
As an example, suppose in a training data, feature xj takes the following values:
1.1, 3.8, 4.5, 6.9, 9.2. In such a setting, for example, Œæ = (j, 4.0) and Œæ = (j, 4.4),
create the same split of the data over this feature: one subset being {1.1, 3.8} and
the other being {4.5, 6.9, 9.2}. As a matter of fact we obtain the same split for any
Œæ = (j, Œ∏) where Œ∏ ‚àà(3.8, 4.5). Therefore, it would be suÔ¨Écient to only examine one
of these splits. In CART, this arbitrary threshold between two consecutive values is
taken as their halfway point. Assuming feature xj, j = 1, . . ., p, has vj distinct values
in a training data, then the cardinality of Œû is (√çp
j=1 vj) ‚àíp. We denote the set of
candidate values of Œ∏ (halfways between consecutive values) for feature xj by Œòj.

192
7 Decision Trees
7.2.2 Splitting Strategy
The splitting strategy in CART is similar to what we saw in Section 7.1; that is, it
attempts to Ô¨Ånd the best split Œæ that minimizes the weighted cumulative impurity
of data subsets created as the result of the split. Here we aim to mathematically
characterize this statement.
Let Str = {(x1, y1), (x2, y2), . . ., (xn, yn)} denote the training data. Assume the
tree has already m ‚àí1 node and we are looking for the best split at the mth node.
Let Sm ‚äÇStr denote the training data available for the split at node m. To identify
the best split, we can conduct an exhaustive search over Œû and identify the split that
minimizes a measure of impurity. Recall that each Œæ ‚ààŒû corresponds to a question
‚Äúxj ‚â§Œ∏?‚Äù where Œ∏ ‚ààŒòj and depending on the yes/no answer to this question for
each observation in Sm, Sm is then split into SYes
m,Œæ and SNo
m,Œæ; that is to say,
SYes
m,Œæ = {(x, y) ‚ààSm|xj ‚â§Œ∏, and Œ∏ ‚ààŒòj},
SNo
m,Œæ = Sm ‚àíSYes
m,Œæ .
(7.1)
When visualizing the structure of a decision tree, by convention the child node
containing SYes
m,Œæ is on the left branch of the parent node. Let i(S) denote a speciÔ¨Åc
measure of impurity of data S presented to a node (also referred to as impurity of the
node itself) and let nS denote the number of data points in S. A common heuristic
used to designate a split Œæ as the best split at node m is to maximize the diÔ¨Äerence
between the impurity of Sm and the weighted cumulative impurity of SYes
m,Œæ and SNo
m,Œæ
(weighted cumulative impurity of the two child nodes). With the assumption that
any split at least produces a ‚Äúmore pure‚Äù data or at least as impure as Sm, we can
assume we are maximizing the drop in impurity. This impurity drop for a split Œæ at
node m is denoted as ‚àÜm,Œæ and is deÔ¨Åned as
‚àÜm,Œæ = i(Sm) ‚àí
nSYes
m, Œæ
nSm
i(SYes
m,Œæ) ‚àí
nSNo
m, Œæ
nSm
i(SNo
m,Œæ) .
(7.2)
Therefore, the best split at node m denoted as Œæ‚àó
m is given by
Œæ‚àó
m = argmax
Œæ ‚ààŒû
‚àÜm,Œæ .
(7.3)
In (7.2), multiplication factors
nSYes
m, Œæ
nSm
and
nSNo
m, Œæ
nSm
are used to deduct a weighted
average of child nodes impurities from the parent impurity i(Sm). The main technical
reason for having such a weighted average in (7.2) is that with a concave choice of
impurity measure, ‚àÜm,Œæ becomes nonnegative‚Äîand naturally it makes sense to refer
to ‚àÜm,Œæ as impurity drop. The proof of this argument is left as an exercise (Exercise
6). Nonetheless, a non-technical reason to describe the eÔ¨Äect of the weighted average

7.2 CART Development for ClassiÔ¨Åcation:
193
used here with respect to an unweighted average is as follows: suppose that a split
Œæ causes nSYes
m, Œæ << nSNo
m, Œæ , say nSYes
m, Œæ = 10 and nSNo
m, Œæ = 1000, but the distribution
of data in child nodes are such that i(SYes
m,Œæ) >> i(SNo
m,Œæ). Using unweighted average
of child nodes impurities leads to i(SYes
m,Œæ) dictating our decision in choosing the
optimal Œæ; however, i(SYes
m,Œæ) itself is not reliable as it has been estimated on very few
observations (here 10) compared with many observations used to estimate i(SNo
m,Œæ)
(here 1000). The use of weighted average, however, can be thought as a way to
introduce the reliability of these estimates in the average.
Once Œæ‚àó
m is found, we split the data and continue the process until a split-stopping
criterion is met. These criteria include:
1) a predetermined maximum depth. This has a type of global eÔ¨Äect; that is to say,
when we reach this preset maximum depth we stop growing the tree entirely;
2) a predetermined maximum number of leaf nodes. This also has a global eÔ¨Äect;
3) a predetermined minimum number of samples per leaf. This has a type of local
eÔ¨Äect on growing the tree. That is to say, if a split causes the child nodes having
less data points than the speciÔ¨Åed minimum number, that node will not split any
further but splitting could still be happening in other nodes; and
4) a predetermined minimum impurity drop. This has also a local eÔ¨Äect. In par-
ticular, if the impurity drop due to a split at a node is below this threshold, the
node will not split.
As we see all these criteria depend on a predetermined parameter. Let us consider,
for example, the minimum number of samples per leaf. Setting that to 1 (and not
setting any other stopping criteria) allows the tree to fully grow and probably causes
overÔ¨Åtting (i.e., good performance on training set but bad performance on test set). On
the other hand, setting that to a large number could stop growing the tree immaturely
in which case the tree may not learn discriminatory patterns in data. Such a situation
is referred to as underÔ¨Åtting and leads to poor performance on both training and
test sets. As a result, each of the aforementioned parameters that controls the tree
complexity should generally be treated as a hyperparameter to be tuned.
7.2.3 ClassiÔ¨Åcation at Leaf Nodes
The classiÔ¨Åcation in a decision tree occurs in the leaf nodes. In this regard, once the
splitting process is stopped for all nodes in the tree, each leaf node is labeled as the
majority class among all training observations in that node. Then to classify a test
observation x, we start from the root node and based on answers to the sequence of
binary questions (splits) in the tree, we identify the leaf into which x falls; the label
of that leaf is then assigned to x.

194
7 Decision Trees
7.2.4 Impurity Measures
So far we did not discuss possible choices ofi(S). In deÔ¨Åning an impurity measure, the
Ô¨Årst thing to notice is that in a multiclass classiÔ¨Åcation with c classes 0, 1, . . ., c ‚àí1,
the impurity of a sample S that falls into a node is a function of the proportion of
class k observations in S‚Äîwe denote this proportion by pk, k = 0, . . ., c ‚àí1. Then,
the choice of i(S) in classiÔ¨Åcation could be any nonnegative function such that the
function is:
1. maximum when data from all classes are equally mixed in a node; that is, when
p0 = p1 = ... = pc‚àí1 = 1
c, i(S) is maximum;
2. minimum when the node contains data from one class only; that is, for c-tuples
of the form (p0, p1, . . ., pc‚àí1), i(S) is minimum for (1, 0, . . ., 0), (0, 1, 0, . . ., 0),
. . . , (0, . . ., 0, 1); and
3. a symmetric function of p0, p1, . . . , pc‚àí1; that is, i(S) does not depend on the
order of p0, p1, . . . , pc‚àí1.
There are some common measures of impurity that satisfy these requirements.
Assuming a multiclass classiÔ¨Åcation with classes 0, 1, . . ., c ‚àí1, common choices of
i(S) include Gini, entropy, and misclassiÔ¨Åcation error (if we classify all observations
in S to the majority class) given by:
for Gini:
i(S) =
c‚àí1
√ï
k=0
pk(1 ‚àípk),
(7.4)
for entropy:
i(S) = ‚àí
c‚àí1
√ï
k=0
pklog(pk),
(7.5)
for error rate:
i(S) = 1 ‚àí
max
k=0,...,c‚àí1 pk ,
(7.6)
where pk =
nk
√çc‚àí1
k=0 nk in which nk denotes the number of observations from class
k in S. Fig. 7.3 shows the concave nature of these impurity measures for binary
classiÔ¨Åcation. Although in (7.5) we use natural logarithm, sometimes entropy is
written in terms of log2. Nevertheless, changing the base does not change the best
split because based on log2b = logb
log2 , using log2 is similar to multiplying natural log
by a constant
1
log2 for all splits.
Example 7.1 In a binary classiÔ¨Åcation problem, we have the a training data shown
in Table 7.1 for two available features where xj denotes a feature variable, j = 1, 2.
We wish to train a decision stump (i.e., a decision tree with only one split).
a) identify candidate splits;
b) for each identiÔ¨Åed candidate split, calculate the impurity drop using entropy
metric (use natural log); and

7.2 CART Development for ClassiÔ¨Åcation:
195
0.0
0.2
0.4
0.6
0.8
1.0
p
0
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
i(Óàø)
Gini
entropy
error rate
Fig. 7.3: Three common concave measures of impurity as a function of p0 for binary
classiÔ¨Åcation.
c) based on the results of part (b), determine the best split (or splits if there is a
tie).
Table 7.1: Training data for Example 7.1.
class
0
0
0
0
1
1
1
1
x1
1
1
3
3
1
1
1
3
x2
2
4
4
0
2
0
4
2
a) x1 takes two values 1 and 3. Taking their halfway point, the candidate split based
on x1 is x1 ‚â§2.
x2 takes three values 0, 2, and 4. Taking their halfway points, the candidate splits
based on x2 are x2 ‚â§1 and x2 ‚â§3.
b) for each candidate split identiÔ¨Åed in part (a), we need to quantify ‚àÜ1,Œæ.
From (7.2), we have ‚àÜ1,Œæ = i(S1) ‚àí
 nSYes
1, Œæ
nS1 i(SYes
1,Œæ) +
nSNo
1, Œæ
nS1 i(SNo
1,Œæ)

.
Here S1 is the entire data in the Ô¨Årst node (root node) because no split has occurred
yet. We have
i(S1) = ‚àí
1
√ï
k=0
pklog(pk) = ‚àí4
8log
4
8

‚àí4
8log
4
8

= log(2) = 0.693 .

196
7 Decision Trees
For Œæ = (1, 2)‚Äìrecall that based on deÔ¨Ånition of Œæ, this means the split x1 ‚â§2: based
on this split, SYes
1,Œæ contains Ô¨Åve observations (two from class 0 and three from class
1), and SNo
1,Œæ contains three observations (two from class 0 and one from class 1).
Therefore,
i(SYes
1,Œæ) = ‚àí2
5log
2
5

‚àí3
5log
3
5

= 0.673,
i(SNo
1,Œæ) = ‚àí2
3log
2
3

‚àí1
3log
1
3

= 0.636 .
Therefore,
‚àÜ1,Œæ=(1,2) = 0.693 ‚àí
5
80.673 + 3
80.636

= 0.033 .
For Œæ = (2, 1), which means x2 ‚â§1 split, SYes
1,Œæ contains two observations (one from
class 0 and one from class 1), and SNo
1,Œæ contains six observations (three from class 0
and three from class 1). Therefore,
i(SYes
1,Œæ) = ‚àí1
2log
1
2

‚àí1
2log
1
2

= 0.693,
i(SNo
1,Œæ) = ‚àí3
6log
3
6

‚àí3
6log
3
6

= 0.693 .
Therefore,
‚àÜ1,Œæ=(2,1) = 0.693 ‚àí
2
80.693 + 6
80.693

= 0 .
(7.7)
For Œæ = (2, 3): based on this split, SYes
1,Œæ contains Ô¨Åve observations (two from class 0
and three from class 1), and SNo
1,Œæ contains three observations (two from class 0 and
one from class 1). This is similar to Œæ = (1, 2). Therefore, ‚àÜ1,Œæ=(1,2) = 0.033 .
c) Œæ = (1, 2) and Œæ = (2, 3) are equality good in terms of entropy metric because
the drop in impurity for both splits are the same and is larger than the other split
(recall that we look for the split that maximizes the impurity drop). ‚ñ†
7.2.5 Handling Weighted Samples
In training a predictive model, it is common to assume all observations are equally
important. However, sometimes we may assume some observations are more impor-
tant than others. This could be simulated by assigning some weights to observations
and incorporating the weights in the training process. The sample importance/weight
can simply be an assumption imposed by the practitioner or an assumption inherently
made as part of some learning algorithm such as boosting that we will see in Chapter

7.3 CART Development for Regression
197
8. A useful property of decision tree is its ability to easily handle weighted samples.
To do so, we can replace the number of samples in a set used in (7.2) and any of the
metrics presented in (7.4)-(7.6) by the sum of weights of samples belonging to that
set. For example, rather than nSYes
m, Œæ and nSm, we use sum of weights of samples in
SYes
m,Œæ and Sm, respectively. Therefore, rather than using pk =
nk
√çc‚àí1
k=0 nk that is used in
(7.4)-(7.6), we use the ratio of the sum of weights of samples in class k to the sum
of weights for all observations.
7.3 CART Development for Regression
7.3.1 DiÔ¨Äerences Between ClassiÔ¨Åcation and Regression
Our discussion in Section 7.2.1 and 7.2.2 did not depend on class labels of obser-
vations and, therefore, it was not speciÔ¨Åc to classiÔ¨Åcation. In other words, the same
discussion is applicable to regression. The only diÔ¨Äerences in developing CART for
regression problems appear in: 1) the choices of impurity measures and; 2) how we
estimate the target value at a leaf node.
7.3.2 Impurity Measures
To better understand the concept of impurity and its measures in regression, let us
re-examine the impurity for classiÔ¨Åcation. One way we can think about the utility
of having a more pure set of child nodes obtained by each split is to have more
certainty about the class labels for the training data falling into a node if that node is
considered as a leaf node (in which case using the trivial majority vote classiÔ¨Åer, we
classify all training data in that node as the majority class). In regression, once we
designate a node as a leaf node, a trivial estimator that we can use as the estimate
of the target value for any test observation falling into a leaf node is the mean of
target values of training data falling into that node. Therefore, a more ‚Äúpure‚Äù node
(to be more certain about the target) would be a node where the values of target for
training data falling into that node are closer to their means. Choosing the mean as
our trivial estimator of the target at a leaf node, we may then take various measures
such as mean squared error (MSE) or mean absolute error (MAE) to measure how
far on average target values in a leaf node are from their mean. Therefore, one can
think of a node with a larger magnitude of these measures as a more impure node.
However, when the mean is used as our target estimator, there is a subtle diÔ¨Äerence
between using the MSE and MAE as our impurity measures. This is because the
mean minimizes MSE function, and not the MAE. In particular, if we have a1, a2,
. . . , an, then among all constant estimators a, the mean of a‚Äôs minimizes the MSE;
that is,

198
7 Decision Trees
¬Øa = argmin
a
n
√ï
i=1
(ai ‚àía)2 ,
(7.8)
where
¬Øa = 1
n
n
√ï
i=1
ai .
(7.9)
Therefore, to use a homogenous pair of impurity and target estimator, it is common
to use the mean and the MSE impurity together. However, when MAE is used, it
is common to use the median (of target values of training data falling into a node)
as our target estimator. This is because if ( . )2 in (7.8) is replaced with the absolute
function | . |, then the optimal constant estimator is the median.
Therefore, the best split can be found using the following impurity functions in
(7.2) and (7.3):
for MSE:
i(S) = 1
nS
√ï
y‚ààS
(y ‚àí¬Øy)2 ,
(7.10)
for MAE:
i(S) = 1
nS
√ï
y‚ààS
|y ‚àíÀúy|,
(7.11)
where
¬Øy = mean(y ‚ààS) = 1
nS
√ï
y‚ààS
y,
(7.12)
Àúy = median(y ‚ààS) .
(7.13)
It is also common to refer to MSE and MAE as ‚Äúsquared error‚Äù and ‚Äúabsolute error‚Äù,
respectively.
7.3.3 Regression at Leaf Nodes
When (7.10) and (7.11) are used as impurity measures, the mean and median of
target values of training data in a leaf node are used, respectively, as the estimate of
targets for any observation falling into that node.

7.3 CART Development for Regression
199
Scikit-learn Implementation for both ClassiÔ¨Åcation and Regression: CART
classiÔ¨Åer and regressor are implemented in DecisionTreeClassifier class
and DecisionTreeRegressor class from sklearn.tree module, respectively.
DecisionTreeClassifier currently supports two distinct impurity criteria that
can be changed by setting values of 'criterion' parameter to gini (default) and
entropy‚Äîthe possibility of using log_loss, which was added in scikit-learn ver-
sion >= 1.1.0, is equivalent to using entropy. DecisionTreeRegressor currently
supports both the squared_error (default), absolute_error, and two other cri-
teria.
For both DecisionTreeClassifier and DecisionTreeRegressor, the split-
ting stopping criteria discussed in Section 7.2.2, namely, a predetermined maximum
depth, maximum number of leaf nodes, minimum number of samples per leaf,
and minimum impurity drop could be set through max_depth, max_leaf_nodes,
min_samples_leaf, and min_impurity_decrease, respectively. The default val-
ues of these parameters lead to fully grown trees, which not only could be very large
depending on the data, but also as we discussed in Section 7.1, this could lead to
overÔ¨Åtting. In practice these parameters should be treated as hyperparameters and
in the absence of prior knowledge to set them, they could be tuned in the model
selection stage (Chapter 9). In addition to the aforementioned hyperparameters that
restrict the CART complexity, there is another hyperparameter in scikit-learn imple-
mentation of these classes, namely, max_features that determines the number of
features that are examined to Ô¨Ånd the best split at each node. The default value of
max_features examines all p features to Ô¨Ånd the best split but we can set that to
some number, say d < p, so that at each node only d features are randomly chosen
(without replacement) and examined; that is, the maximization in (7.3) to Ô¨Ånd the
best split is taken over d randomly chosen features. Although computationally this
option could be helpful in high-dimensional settings, the main utility of that will be
cleared once we discuss ensemble decision trees in Chapter 8.
Example 7.2 In this example, we wish to examine the eÔ¨Äect of min_samples_leaf
on CART decision regions and its performance. We Ô¨Årst load the preprocessed Iris
dataset that was prepared in Section 4.6 and then use the Ô¨Årst two features to classify
Iris Ô¨Çowers.
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn.tree import DecisionTreeClassifier as CART
arrays = np.load('data/iris_train_scaled.npz')
X_train = arrays['X']
y_train = arrays['y']
arrays = np.load('data/iris_test_scaled.npz')
X_test = arrays['X']
y_test = arrays['y']

200
7 Decision Trees
X_train = X_train[:,[0,1]]
X_test = X_test[:,[0,1]]
print('X shape = {}'.format(X_train.shape) + '\ny shape = {}'.
,‚Üíformat(y_train.shape))
print('X shape = {}'.format(X_test.shape) + '\ny shape = {}'.
,‚Üíformat(y_test.shape))
color = ('aquamarine', 'bisque', 'lightgrey')
cmap = ListedColormap(color)
mins = X_train.min(axis=0) - 0.1
maxs = X_train.max(axis=0) + 0.1
x = np.arange(mins[0], maxs[0], 0.01)
y = np.arange(mins[1], maxs[1], 0.01)
X, Y = np.meshgrid(x, y)
coordinates = np.array([X.ravel(), Y.ravel()]).T
fig, axs = plt.subplots(2, 2, figsize=(6, 4), dpi = 200)
fig.tight_layout()
min_samples_leaf_val = [1, 2, 5, 10]
for ax, msl in zip(axs.ravel(), min_samples_leaf_val):
cart = CART(min_samples_leaf=msl)
cart.fit(X_train, y_train)
Z = cart.predict(coordinates)
Z = Z.reshape(X.shape)
ax.tick_params(axis='both', labelsize=6)
ax.set_title('CART Decision Regions: min_samples_leaf=' + str(msl),‚ê£
,‚Üífontsize=7)
ax.pcolormesh(X, Y, Z, cmap = cmap, shading='nearest')
ax.contour(X ,Y, Z, colors='black', linewidths=0.5)
ax.plot(X_train[y_train==0, 0], X_train[y_train==0, 1],'g.',‚ê£
,‚Üímarkersize=4)
ax.plot(X_train[y_train==1, 0], X_train[y_train==1, 1],'r.',‚ê£
,‚Üímarkersize=4)
ax.plot(X_train[y_train==2, 0], X_train[y_train==2, 1],'k.',‚ê£
,‚Üímarkersize=4)
ax.set_xlabel('sepal length (normalized)', fontsize=7)
ax.set_ylabel('sepal width (normalized)', fontsize=7)
print('The accuracy for min_samples_leaf={} on the training data is‚ê£
,‚Üí{:.3f}'.format(msl, cart.score(X_train, y_train)))
print('The accuracy for min_samples_leaf={} on the test data is {:.
,‚Üí3f}'.format(msl, cart.score(X_test, y_test)))
for ax in axs.ravel():
ax.label_outer()
X shape = (120, 2)
y shape = (120,)
X shape = (30, 2)
y shape = (30,)
The accuracy for min_samples_leaf=1 on the training data is 0.933
The accuracy for min_samples_leaf=1 on the test data is 0.633

7.3 CART Development for Regression
201
The accuracy for min_samples_leaf=2 on the training data is 0.875
The accuracy for min_samples_leaf=2 on the test data is 0.667
The accuracy for min_samples_leaf=5 on the training data is 0.842
The accuracy for min_samples_leaf=5 on the test data is 0.667
The accuracy for min_samples_leaf=10 on the training data is 0.800
The accuracy for min_samples_leaf=10 on the test data is 0.567
‚àí2
‚àí1
0
1
2
sepal  idth (normalized)
CART Decision Regions: min_samples_leaf=1
CART Decision Regions: min_samples_leaf=2
‚àí2
‚àí1
0
1
2
sepal length (normalized)
‚àí2
‚àí1
0
1
2
sepal  idth (normalized)
CART Decision Regions: min_samples_leaf=5
‚àí2
‚àí1
0
1
2
sepal length (normalized)
CART Decision Regions: min_samples_leaf=10
Fig. 7.4: The scatter plot of normalized Iris dataset for two features and decision
regions for CART with minimum samples per leaf being 1, 2, 5, and 10. The green,
red, and black points show points corresponding to setosa, versicolor, and virginica
Iris Ô¨Çowers, respectively. The CART decision regions for each class are colored
similarly.
As we can see, allowing the tree to fully grow (i.e., min_samples_leaf=1)
leads to a high accuracy (93.3%) on the training data but a low accuracy on the
test set (63.3%). This is an indicator of overÔ¨Åtting. At the same time, here the
largest magnitude of min_samples_leaf that we take leads to the lowest accuracy
on training and also a poor performance on the test as well. It turns out that in
this example, min_samples_leaf=2 and min_samples_leaf=5 exhibit a similar
performance on the test set (66.7%). ‚ñ†

202
7 Decision Trees
7.4 Interpretability of Decision Trees
A key advantage of decision tree is its ‚Äúglass-box‚Äù structure in contrast to ‚Äúblack-box‚Äù
structure of some other predictive models such as neural networks; that is to say,
once a decision tree is developed, its classiÔ¨Åcation mechanism is interpretable and
transparent through the hierarchy of standardized questions. To have a better insight
into the working mechanism of a trained decision tree, we can visualize the sequence
of splits as part of the trained tree. A straightforward way is to use plot_tree()
from sklearn.tree module. In the following code we visualize the last tree that
was trained in Example 7.2; that is, the CART with min_samples_leaf=10. In this
regard, we Ô¨Årst load the Iris data again to access its attributes such as feature_names
and target_names. We use class_names=iris.target_names[0:3] to show
the majority classes at leaf nodes. The option filled=True in plot_tree() results
in nodes being colored to the majority class within that node, where a darker color
corresponds to a more pure node.
from sklearn import datasets
from sklearn.tree import plot_tree
plt.figure(figsize=(15,15), dpi=200)
iris = datasets.load_iris()
plot_tree(cart, feature_names = iris.feature_names[0:2],‚ê£
,‚Üíclass_names=iris.target_names[0:3], filled=True)
An interesting observation in Fig. 7.5 is that in several nodes, a split results
in two leaf nodes with the same class. For example, consider the left most split
‚Äúsepal width ‚â§‚àí0.068‚Äù that led to two child nodes with both having setosa
label. The reason for such splits is because the cumulative impurities in child nodes
measured by the weighted average of their impurities is less than the impurity of
the parent node even though the majority class in both child nodes is setosa. For
example, in Fig. 7.5 we observe that the Gini impurity of the node ‚Äúsepal width ‚â§
‚àí0.068‚Äù is 0.184, which is greater than
nSYes
m, Œæ
nSm i(SYes
m,Œæ) +
nSNo
m, Œæ
nSm i(SNo
m,Œæ) = 11
40 √ó 0.512 +
29
40 √ó 0 = 0.140. Such splits would make more sense if we think of having more pure
nodes obtained by a split as a way to have more certainty about decisions made (see
Section 7.2.4). Furthermore, the reason that these two child nodes are designated as
leaf nodes and no further splits occurs is that one of them is pure (all data points
are from one class), and for the other node any further splits would violate the
min_samples_leaf=10 condition.
Despite the classiÔ¨Åcation transparency of decision trees, their decision regions
could be quite sensitive with respect to changes in data. For example, Fig. 7.6 is
obtained in a case where we randomly removed 5 observations out of 120 training
observations that were used in Example 7.2 and re-train all the models. We observe
that in some cases removing such a limited number of observations could lead to
some quite diÔ¨Äerent decision regions (compare the bottom row of Fig. 7.6 with Fig.
7.4.

7.4 Interpretability of Decision Trees
203
gini = 0.408
samples = 14
value = [0, 4, 10]
class = virginica
gini = 0.463
samples = 11
value = [0, 7, 4]
class = versicolor
gini = 0.26
samples = 13
value = [0, 11, 2]
class = versicolor
gini = 0.32
samples = 10
value = [0, 8, 2]
class = versicolor
sepal length (cm) <= 0.809
gini = 0.493
samples = 25
value = [0, 11, 14]
class = virginica
gini = 0.32
samples = 10
value = [0, 2, 8]
class = virginica
sepal width (cm) <= -0.773
gini = 0.287
samples = 23
value = [0, 19, 4]
class = versicolor
gini = 0.628
samples = 11
value = [4, 5, 2]
class = versicolor
sepal width (cm) <= 0.166
gini = 0.467
samples = 35
value = [0, 13, 22]
class = virginica
gini = 0.0
samples = 11
value = [0, 0, 11]
class = virginica
gini = 0.512
samples = 11
value = [7, 3, 1]
class = setosa
gini = 0.0
samples = 29
value = [29, 0, 0]
class = setosa
sepal width (cm) <= -0.303
gini = 0.457
samples = 34
value = [4, 24, 6]
class = versicolor
sepal length (cm) <= 1.35
gini = 0.405
samples = 46
value = [0, 13, 33]
class = virginica
sepal width (cm) <= -0.068
gini = 0.184
samples = 40
value = [36, 3, 1]
class = setosa
sepal length (cm) <= 0.328
gini = 0.546
samples = 80
value = [4, 37, 39]
class = virginica
sepal length (cm) <= -0.514
gini = 0.667
samples = 120
value = [40, 40, 40]
class = setosa
Fig. 7.5: The trained decision tree for Iris Ô¨Çower classiÔ¨Åcation. The Ô¨Ågure shows
the ‚Äúglass-box‚Äù structure of decision trees. Interpretability is a key advantage of
decision trees. By convention, the child node satisfying condition in a node is on the
left branch.
Exercises:
Exercise 1: Load the training and test Iris classiÔ¨Åcation dataset that was prepared
in Section 4.6. Select the Ô¨Årst three features (i.e., columns with indices 0, 1, and
2). Write a program to train CART (for the hyperparamater min_samples_leaf
being 1, 2, 5, and 10) and kNN (for the hyperparameter K being 1, 3, 5, 9) on this
trivariate dataset and calculate and show their accuracies on the test dataset. Which
combination of the classiÔ¨Åer and hyperparameter shows the highest accuracy on the

204
7 Decision Trees
‚àí2
‚àí1
0
1
2
sepal  idth (normalized)
CART Decision Regions: min_samples_leaf=1
CART Decision Regions: min_samples_leaf=2
‚àí2
‚àí1
0
1
2
sepal length (normalized)
‚àí2
‚àí1
0
1
2
sepal  idth (normalized)
CART Decision Regions: min_samples_leaf=5
‚àí2
‚àí1
0
1
2
sepal length (normalized)
CART Decision Regions: min_samples_leaf=10
Fig. 7.6: The scatter plot and decision regions for a case that Ô¨Åve observations were
randomly removed from the data used in Example 7.2.
test dataset?
Exercise 2: Which of the following options is the scikit-learn class that implements
the CART for regression?
A) ‚Äúsklearn.tree.DecisionTreeClassiÔ¨Åer‚Äù
B) ‚Äúsklearn.tree.DecisionTreeRegressor‚Äù
C) ‚Äúsklearn.linear_model.DecisionTreeClassiÔ¨Åer‚Äù
D) ‚Äúsklearn.linear_model.DecisionTreeRegressor‚Äù
E) ‚Äúsklearn.ensemble.DecisionTreeClassiÔ¨Åer‚Äù
F) ‚Äúsklearn.ensemble.DecisionTreeRegressor‚Äù
Exercise 3: In a binary classiÔ¨Åcation problems, we have the following sets of train-
ing data for two available features x1 and x2. How many possible splits should be

7.4 Interpretability of Decision Trees
205
examined to Ô¨Ånd the best split for training a decision stump using CART?
class
0
0
0
0
1
1
1
x1
‚àí1 0
3
2
3
4 10
x2
7
3
2
0
1 ‚àí5 ‚àí3
Exercise 4: We would like to train a binary classiÔ¨Åer using CART algorithm based on
some 3-dimensional observations. Let x = [x1, x2, x3]T denote an observation with
xi being feature i, for i = 1, 2, 3. In training the CART classiÔ¨Åer, suppose a parent
node is split based on x1 feature and results in two child nodes. Is that possible that
the best split in one of these child nodes is again based on x1 feature?
A) Yes
B) No
Exercise 5: In Example 7.2, we concluded that Œæ = (1, 2) and Œæ = (2, 3) are equally
good to train a decision stump based on the entropy measure of impurity. Suppose
we choose Œæ = (1, 2) as the split used in the root node and we would like to grow the
tree further. Determine the best split on the left branch (satisfying x1 ‚â§2 condition).
Assume 0 log( 0
0) = 0.
√â Exercise 6:
As discussed in Section 7.2.4, impurity measurei(S) is a function of pk =
nk
√çc‚àí1
k=0 nk , k =
0, . . ., c ‚àí1, where nk is the number of observations from class k in S. Let
œÜ(p0, p1, . . ., pc‚àí1) denote this function. Suppose that this function is concave in
the sense that for 0 ‚â§Œ± ‚â§1 and any 0 ‚â§xk ‚â§1 and 0 ‚â§yk ‚â§1, we have
œÜ (Œ±x0 + (1 ‚àíŒ±)y0, Œ±x1 + (1 ‚àíŒ±)y1, . . ., Œ±xc‚àí1 + (1 ‚àíŒ±)yc‚àí1)
‚â•Œ±œÜ (x0, x1, . . ., xc‚àí1) + (1 ‚àíŒ±)œÜ (y0, y1, . . ., yc‚àí1) .
Show that using this impurity measure in the impurity drop deÔ¨Åned as
‚àÜŒæ = i(S) ‚àí
nSYes
Œæ
nS
i(SYes
Œæ ) ‚àí
nSNo
Œæ
nS
i(SNo
Œæ ),
leads to ‚àÜŒæ ‚â•0 for any split Œæ.

206
7 Decision Trees
Exercise 7: Consider the following two tree structures identiÔ¨Åed as Tree A and Tree
B in which x1 and x2 are the names of two features:
Tree A:
ùë•! ‚â§0
ùë•! ‚â§1
yes
no
yes
yes
no
no
ùë•" ‚â§1
ùë•! ‚â§‚àí1
ùë•" ‚â§2
class 1
class 0
class 1
class 0
yes
no
class 1
class 0
yes
no
Tree B:
ùë•! ‚â§0
ùë•" ‚â§2
yes
no
yes
yes
no
no
ùë•" ‚â§1
ùë•! ‚â§‚àí1
ùë•" ‚â§0
class 1
class 0
class 1
class 0
yes
no
class 1
class 0
yes
no
Can Tree A and Tree B be the outcome of applying the CART algorithm to some
training data with the two numeric features x1 and x2 collected for a binary classiÔ¨Å-
cation problem?
A) Both trees A and B can be
B) Tree A can not but Tree B can be
C) Tree B can not but Tree A can be
D) None of these two trees can be

7.4 Interpretability of Decision Trees
207
Exercise 8: Explain whether each of the following is a legitimate decision tree
obtained by the CART algorithm in some binary classiÔ¨Åcation problems with two
two numeric features x1 and x2?
A)
ùë•! ‚â§0
ùë•" ‚â§1
yes
no
yes
yes
no
no
ùë•! ‚â§2
ùë•" ‚â§0
ùë•! ‚â§1
class 1
class 0
class 0
class 0
yes
no
class 1
class 0
yes
no
B)
ùë•! ‚â§0
ùë•" ‚â§3
yes
no
yes
yes
no
no
ùë•" ‚â§2
ùë•! ‚â§‚àí1
ùë•" ‚â§2
class 1
class 0
class 1
class 0
yes
no
class 1
class 0
yes
no
C)
ùë•! ‚â§0
ùë•" ‚â§3
yes
no
yes
yes
no
no
ùë•" ‚â§2
ùë•! ‚â§‚àí1
ùë•" ‚â§2
class 1
class 0
class 1
class 0
yes
no
class 1
class 0
yes
no

Chapter 8
Ensemble Learning
The fundamental idea behind ensemble learning is to create a robust and accurate
predictive model by combining predictions of multiple simpler models, which are
referred to as base models. In this chapter, we describe various types of ensemble
learning such as stacking, bagging, random forests, pasting, and boosting. Stacking
is generally viewed as a method for combining the outcomes of several base models
using another model that is trained based on the outputs of the base models along
with the desired outcome. Bagging involves training and combining predictions of
multiple base models that are trained individually on copies of the original training
data that are created by a speciÔ¨Åc random resampling known as bootstrap. Random
forest is a speciÔ¨Åc modiÔ¨Åcation of bagging when applied to decision trees. Similar
to bagging, a number of base models are trained on bootstrap samples and combined
but to create each decision tree another randomization element is induced in the
splitting strategy. This randomization often leads to improvement over bagged trees.
In pasting, we randomly pick modest-size subsets of a large training data, train a
predictive model on each, and aggregate the predictions. In boosting a sequence of
weak models are trained and combined to make predictions. The fundamental idea
behind boosting is that at each iteration in the sequence, a weak model (also known
as weak learner) is trained based on the errors of the existing models and added to
the sequence.
8.1 A General Perspective on the EÔ¨Écacy of Ensemble Learning
In this section, we will Ô¨Årst try to answer the following question: why would combin-
ing an ensemble of base models possibly help improve the prediction performance?
Our attempt to answer this question is not surprising if we acknowledge that integrat-
ing random variables in diÔ¨Äerent ways is indeed a common practice in estimation
theory to improve the performance of estimation. Suppose we have a number of
independent and identically distributed random variables Y1, Y2, . . . , YN with mean
¬µ and variance œÉ2; that is, E[Yi] = ¬µ and Var[Yi] = œÉ2, i = 1, . . ., N. Using Y1
209
¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
A. Zollanvari, Machine Learning with Python, https://doi.org/10.1007/978-3-031-33342-2_8 

210
8 Ensemble Learning
as estimator of ¬µ has naturally a standard deviation of œÉ. However, if we use the
sample mean 1
N
√çN
i=1 Yi as the estimator of ¬µ, we have Var[ 1
N
√çN
i=1 Yi] = NœÉ2
N2 = œÉ2
N ,
which means it has a standard deviation of
œÉ
‚àö
N < œÉ for N > 1. In other words,
combining Y1, Y2, . . . , YN by an averaging operator creates an estimator that has a
higher ‚Äúprecision‚Äù (lower variance) than the utility of each Yi in its ‚Äúsilo‚Äù. In what
follows, we approach the question from the standpoint of regression.
8.1.1 Bias-Variance Decomposition
Consider a regression problem in which we wish to estimate the value of a target
random variable Y for an input feature vector X. To do so, we use a class of models;
for example, we assume linear regression or CART. Even if we have inÔ¨Ånite number
of data points from the problem and we train a speciÔ¨Åc model, in general we would
still have error in predictingY because we approximated the underlying phenomenon
connecting X to Y by a speciÔ¨Åc mathematical model. Therefore, we can write
Y = fo(X) + œµ ,
(8.1)
where fo(X) is the best (optimal) approximation of Y that we can achieve by using a
class of models and œµ denotes the unknown zero-mean error term (because any other
mean can be considered as a bias term added to fo(X) itself). The term œµ is known
as irreducible error because no matter how well we do by using the mathematical
model and features considered, we can not reduce this error (e.g., because we do not
measure some underlying variables of the phenomenon or because our mathematical
model is a too simplistic model of the phenomenon).
Nevertheless, we are always working with a Ô¨Ånite training data and what we end
up with instead is indeed an estimation of fo(X) denoted by f (X) and we use it as
an estimate of Y. How well do we predict? We can look into the mean square error
(MSE). From the relationship presented in (8.1), we can write the MSE of f (X) with
respect to unknown Y as
E

Y ‚àíf (X)
2 = E

fo(X) + œµ ‚àíf (X)
2
= E
h
fo(X) + œµ ‚àíf (X) + E[ fo(X)] ‚àíE[ f (X)] + E[ f (X)] ‚àíE[ fo(X)]
i2
1= E
h
fo(X) ‚àíf (X) ‚àí(E[ fo(X)] ‚àíE[ f (X)])
i2
+ E
h
E[ fo(X)] ‚àíE[ f (X)]
i2
+ Var[œµ]
2= Vard[ f (X)] + Bias[ f (X)]2 + Var[œµ],
(8.2)
where the expectation is over the joint distribution of X, Y, and training data,
1=
follows from E[œµ] = 0, and to write
2= we used the fact that

8.1 A General Perspective on the EÔ¨Écacy of Ensemble Learning
211
Bias[ f (X)]2 ‚àÜ=
h
E[Y ‚àíf (X)]
i2
=
h
E[ fo(X) ‚àíf (X)]
i2
= E
h
E[ fo(X)] ‚àíE[ f (X)]
i2
,
(8.3)
and where the variance of deviation Vard[ f (X)] is deÔ¨Åned as
Vard[ f (X)] = Var[ fo(X) ‚àíf (X)] .
(8.4)
As observed in (8.2), we could write the MSE of estimation in terms of the
variance of deviation and a squared bias term. This is known as bias-variance
decomposition of expected error (MSE). Generally, the more complex the model,
the larger the variance of deviation. What does this imply? Suppose we have dataset
A collected from a problem and use a complex learning algorithm with many degrees
of freedom to train a model, namely Model A. Then we collect another dataset from
the same problem, namely, dataset B, and use the same learning algorithm to train
another model, namely Model B. In this scenario, Model A and Model B could be
very diÔ¨Äerent because the learning algorithm has many degrees of freedom. As an
example of such a scenario consider CART when left to fully grow. As we observed in
Chapter 6, even a slight change in training data could lead to diÔ¨Äerent tree classiÔ¨Åers.
On the other hand, a simpler model (less degrees of freedom) does not change as
much (by deÔ¨Ånition has less Ô¨Çexibility) so the variance of deviation could naturally
be lower.
However, insofar as bias is concerned, it is more likely that a complex learning
algorithm has lower bias than a simpler learning algorithm. To understand this, we
have to note that whether we use a simple or complex learning algorithm, we try to
estimate the best possible approximation of Y  i.e., to estimate fo(X) within each
class. When a complex learning algorithm is used, by deÔ¨Ånition the class has more
room to possess an fo(X) that is closer toY (see the red crosses in Fig. 8.1). However,
because we do not know fo(X), essentially we end up with its estimate f (X), which
can change from data to data. The highlighted red regions in Fig. 8.1 show the set of
possible values of f (X) within each class. As can be seen in this Ô¨Ågure, although the
highlighted red region for the complex class is larger, the points within this region
are on average closer to Y than the average distance for the set of points within the
highlighted region of the simpler class. At the same time, the area of these regions is
an indicator of the variance that we explained before (the larger the area, the greater
the variance of deviation).
Ensemble methods generally beneÔ¨Åt from reducing the variance of deviation
discussed above (for example, bagging, random forests, and stacking) although some
methods such as boosting beneÔ¨Åt from both reduction in variance and bias. To better
understand these statements (at least in terms of variance) a more details discussion
is required as presented next.

212
8 Ensemble Learning
ùëå
fo(X)
fo(X)
possible approximations within a 
class of ‚Äúcomplex‚Äù classifiers
possible approximations within a 
class of ‚Äúsimple‚Äù classifiers
+
+
Fig. 8.1: The larger (smaller) oval shows the set of possible approximations within a
class of complex (simple) classiÔ¨Åers. The best approximation in the more ‚Äúcomplex‚Äù
class is closer toY. The red region within each oval shows the set of possible estimates
of fo(X) within that class. The average distance between Y and the points within the
red region of the complex class is less than the average distance between Y and the
set of points within the highlighted region of the simple class. This shows a lower
bias of the more complex classiÔ¨Åer.
8.1.2
√â
How Would Ensemble Learning Possibly Help?
As we observed in the previous section, in decomposition of the mean square error
of prediction, a variance term Vard[ f (X)] and a bias term Bias[ f (X)] will appear.
To show the utility of ensemble learning, we focus on the possibility of reducing
Vard[ f (X)]. We have
Vard[ f (X)] = E
h
fo(X) ‚àíf (X) ‚àí(E[ fo(X)] ‚àíE[ f (X)])
i2
.
(8.5)
The diÔ¨Äerence between fo(X) and f (X) is known as reducible error because this
could be reduced, for example, by collecting more or better datasets to be used in
estimating fo(X). Therefore, we write
f (X) = fo(X) + e,
(8.6)
where e shows the reducible error. Replacing (8.6) in (8.5) yields

8.1 A General Perspective on the EÔ¨Écacy of Ensemble Learning
213
Vard[ f (X)] = E
h
e ‚àíE[e]
i2
= E[e2] ‚àí(E[e])2 .
(8.7)
Let us now consider an ensemble of M base models that are trained using a
speciÔ¨Åc learning algorithm but perhaps with diÔ¨Äerent (structural or algorithmic)
hyperparameters. The necessity of having the same learning algorithm in the follow-
ing arguments is the use of fo(X) as the unique optimal approximation of Y that is
achievable within the class of models considered. Nevertheless, with a slightly more
work these arguments could be applied to diÔ¨Äerent learning algorithms. We write
fi(X) = fo(X) + ei,
i = 1, . . ., M .
(8.8)
From (8.7) we have
Vard[ fi(X)] = E[e2
i ] ‚àí(E[ei])2,
i = 1, . . ., M .
(8.9)
Each of these M base models has a mean square error with a deviation variance term
as in (8.9). Therefore, the average MSE of all these M models has a term that is the
average of variance of deviation for each base model (in the following we denote
that by Varavg
d ):
Varavg
d
‚àÜ= 1
M
M
√ï
i=1
Vard[ fi(X)] = 1
M
M
√ï
i=1
E[e2
i ] ‚àí1
M
M
√ï
i=1
(E[ei])2 .
(8.10)
Now let us consider an ensemble regression model that as the prediction takes the
average of fi(X)‚Äôs; after all, each fi(X) is the prediction made by each model and it
is reasonable (and common) to take their average in an ensemble model. Therefore,
from (8.8) we can write
1
M
M
√ï
i=1
fi(X) = fo(X) + ¬Øe,
i = 1, . . ., M ,
(8.11)
where
¬Øe = 1
M
M
√ï
i=1
ei .
(8.12)
The MSE of prediction obtained by this ensemble model has a variance of devi-
ation term similar to (8.7), which is obtained as (in the following we denote that by
Varens
d ):
Varens
d
= E[ ¬Øe2] ‚àí(E[ ¬Øe])2 = E
h  1
M
M
√ï
i=1
ei
2i
‚àí

E
h 1
M
M
√ï
i=1
ei
i2
.
(8.13)

214
8 Ensemble Learning
To better compare the average MSE of prediction by base models and the MSE of
prediction achieved by the ensemble model (i.e., average of their predictions) to the
extent that are aÔ¨Äected by Varavg
d
and Varens
d , respectively, we make an assumption
that E[ei] = 0, i = 1, . . ., M. In that case, (8.10) and (8.13) reduce to
Varavg
d
=
1
M
√çM
i=1 E[e2
i ],
Varens
d
= E
h  1
M
√çM
i=1 ei
2i
=
1
M E
h
1
M
  √çM
i=1 ei
2i
.
(8.14)
It is straightforward to show that (Exercise 5):
1
M E
h  M
√ï
i=1
ei
2i
‚â§
M
√ï
i=1
E[e2
i ],
(8.15)
which means
Varens
d
‚â§Varavg
d .
(8.16)
The inequality presented in (8.16) is a critical observation behind the utility of
many ensemble techniques. That is taking the average of predictions by base models
leads to a smaller variance of deviation than the average of their deviation variances.
This, in turn, generally leads to a lower prediction error (MSE of prediction) due to
bias-variance decomposition as stated in (8.2). We may also interpret this observation
in another way. The performance improvement achieved by ensemble is generally
attributed to reduction in variance of deviations (some methods such as boosting
though they attempt to reduce both the variance and bias). Another interesting
observation is that, if we assume the error terms in (8.15) are uncorrelated, E[eiej] =
0, i , j, then from (8.14) we have
Varens
d
= 1
M Varavg
d .
(8.17)
This is an important observation that shows a substantial reduction in variance of
deviation (i.e., a factor of 1
M ) obtained by having uncorrelated error terms. In practice,
this is not usually the case though because typically we have one dataset and we take
some overlapping subsets of that to develop our predictions fi(X), i = 1, . . ., M.
‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-

8.2 Stacking
215
8.2 Stacking
Stacked ensemble learning (also known as a stacked generalization) is generally
viewed as a method for combining the outcomes of N base models (also known
as level-0 generalizers) using another model (level-1 generalizer) that is trained
based on the outputs of the base models along with the desired outcomes (Wolpert,
1992) (see Fig. 8.2). For example, we can train a logistic regression as level-1
generalizer to combine the outputs of a number of diÔ¨Äerent trained neural networks
that are used as level-0 generalizer. Nevertheless, stacked generalization is a general
and diverse learning strategy that includes various other common techniques. For
example, consider the common approach of the ‚Äúwinner-takes-all‚Äù strategy. In this
strategy, out of several generalizers (classiÔ¨Åers or regressors), a single generalizer
with the best estimated performance (e.g., the highest accuracy on an independent set
known as validation set) is selected for performing prediction on unseen data. This
practice itself is a stacked generalization that in Wolpert‚Äôs words (Wolpert, 1992),
has ‚Äúan extraordinary dumb level-1 generalizer‚Äù that selects the level-0 generalizer
with the lowest estimated error. That being said, the level-1 generalizers used in
stacked ensemble learning are usually more intelligent than this. As a result, due
to the versatility of stacked generalization, we do not discuss it any further in this
chapter.
Generalizer 1
Generalizer 2
Generalizer N
.
.
.
Generalizer 
level 0
level 1
ùë¶!
ùê±
Fig. 8.2: A schematic representation of stacked generalization. x and ÀÜy denote a
given feature vector and the estimated target, respectively.

216
8 Ensemble Learning
8.3 Bagging
Recall from Section 7.4 in which we mentioned that the decision regions of decision
trees could be quite sensitive with respect to changes in data. One question is whether
we can systematically improve the robustness of some classiÔ¨Åers based on decision
trees that are more robust with respect to variation in data. Let us break up this
question into two:
1. What is meant by variation in data and how can we simulate that?
2. How can we possibly improve the robustness?
To answer the Ô¨Årst question, suppose we have a training data with a sample size
n. Each observation in this training set is a realization of a probability distribution
governing the problem. The probability distribution itself is the result of cumula-
tive eÔ¨Äect of many (possibly inÔ¨Ånite) hidden factors that could relate to technical
variabilities in experiments or stochastic nature of various phenomena. Had we had
detailed information about probabilistic characteristics of all these underlying fac-
tors, we could have constructed the probability distribution of the problem and used
it to generate more datasets from the underlying phenomenon. However, in practice,
except for the observations at hand, we have generally no further information about
the probability distribution of the problem. Therefore, it makes sense if we somehow
use the data at hand to generate more data from the underlying phenomenon (i.e., to
simulate variation in data).
But how can we generate several new legitimate datasets from the single original
data at hand? One way is to assume some observations are repeated; after all, if an
observation happens once, it may happen again (and again). Therefore, to create each
new dataset, we randomly draw with replacement n observations from the original
data at hand. However, keeping the same sample size n implies some observations
from the original training dataset do not appear in these new datasets. This way we
have simulated variation in data by just assuming repetition and deletion operation
on the observations from the original dataset. These newly generated datasets (i.e.,
from sampling the original dataset with replacement while keeping the same sample
size) are known as boostrap samples.
Now we turn to the second question. As each of these bootstrap training sets is a
legitimate dataset from the same problem, we train a decision tree on each bootstrap
sample. Then for a given feature vector x, each tree produces a prediction. We can then
aggregate these predictions to create a Ô¨Ånal prediction. The aggregation operation
could be the majority vote for classiÔ¨Åcation and the averaging for regression. We note
that the Ô¨Ånal prediction is the outcome of applying decision tree algorithm on many
training datasets, and not just one dataset. Therefore, we capture more variation of
data in the aggregated model. Put in other words, the aggregated model is possibly
less aÔ¨Äected by variation in data as compared with each of its constituent tree models
(for a more concrete analysis on why this may improve the prediction performance
refer to Section 8.1.2).
The aforementioned systematic way (i.e., creating bootstrap samples and ag-
gregating) is indeed the mechanism of bagging (short for bootstrap aggregating)

8.4 Random Forest
217
proposed by Breiman in (Breiman, 1996). Let S‚àó
tr, j, j = 1, . . ., B shows the set of B,
bootstrap samples that are generated by sampling with replacement from Str. Then
the ‚Äúbagged‚Äù classiÔ¨Åer and regressor denoted by œàbag(x) and fbag(x), respectively,
are deÔ¨Åned as
ClassiÔ¨Åcation:
œàbag(x) =
argmax
i‚àà{0,...,c‚àí1}
B
√ï
j=1
I{œà‚àó
j (x)=i}
(8.18)
Regression:
fbag(x) = 1
B
B
√ï
j=1
f ‚àó
j (x)
(8.19)
where œà‚àó
j and f ‚àó
j denote the classiÔ¨Åer and regressor trained on bootstrap sample S‚àó
tr, j,
respectively, the set {0, . . ., c ‚àí1} represents the class labels, and IA is the indicator
of event A. The number of bootstrap samples B is typically set between 100 to 200.
Scikit-learn implementation: Bagging classiÔ¨Åer and regressor are implemented by
BaggingClassifier and BaggingRegressor classes from sklearn.ensemble
module, respectively. Usingbase_estimatorparameter,bothBaggingClassifier
and BaggingRegressor can take as input a user-speciÔ¨Åed base model along with
its hyperparameters. Another important hyperparameter in these constructors is
n_estimators (by default it is 10). Using this parameter, we can control the num-
ber of base models, which is the same as the number of bootstrap samples (i.e., B
in the aforementioned notations). The scikit-learn implementation even allows us to
set the size of bootstrap samples to a size diÔ¨Äerent from the original sample size.
This could be done by setting the max_samples parameter in these constructors.
The default value of this parameter (which is 1.0) along with the default value of
bootstrap parameter (which is True) leads to bagging as described before. In
these classes, there is another important parameter, namely, max_features, which
by default is set to 1.0. Setting the value of this parameter to an integer less than
p (the number of features in the data) or a Ô¨Çoat less than 1.0 trains each estimator
on a randomly picked subset of features. That being said, when we train each base
estimator on a diÔ¨Äerent subset of features and samples, the method is referred to as
random patches ensemble. In this regard, we can even set bootstrap_features to
True so that the random selection is performed by replacement.
8.4 Random Forest
Random forest (Breiman, 2001) is similar to bagged decision trees (obtained by
using decision trees as base models used in bagging) except for a small change with
respect to regular training of decision trees that could generally lead to a remarkable
performance improvement with respect to bagged trees. Recall Section 7.2.2 where
we discussed the splitting strategy at each node of a decision tree: for each split,

218
8 Ensemble Learning
we examine all features and pick the split that maximizes the impurity drop. This
process is then continued until one of split-stopping criteria is met. For random
forests, however, for each split in training a decision tree on a bootstrap sample,
d ‚â§p features are randomly picked as a candidate feature set and the best split is
the one that maximizes the impurity drop over this candidate feature set. Although d
could be any integer less or equal to p, it is typically set as ‚àöp or log2p. This simple
change in the examined features is the major factor to decorrelate trees in random
forest as compared with bagged trees (recall Section 8.1.2 where we showed having
uncorrelated error terms in the ensemble leads to substantial reduction in variance
of deviation). We can summarize the random forest algorithm as follows:
Algorithm Random Forest
1. Create bootstrap samples: S‚àó
tr, j, j = 1, . . ., B
2. For each bootstrap sample, train a decision tree classiÔ¨Åer or regressor denoted
by œà‚àó
j and f ‚àó
j , respectively, where:
2.1. To Ô¨Ånd the best split at each node, randomly pick d ‚â§p features and
maximize the impurity drop over these features
2.2. Grow the tree until one of the split-stopping criteria is met (see Section
7.2.2 where four criteria were listed)
3. Perform the prediction for x as:
ClassiÔ¨Åcation:
œàRF(x) =
argmax
i‚àà{0,...,c‚àí1}
B
√ï
j=1
I{œà‚àó
j (x)=i}
(8.20)
Regression:
fRF(x) = 1
B
B
√ï
j=1
f ‚àó
j (x)
(8.21)
Scikit-learn implementation: The random forest classiÔ¨Åer and regressor are im-
plemented in RandomForestClassifier and RandomForestRegressor classes
from sklearn.ensemble module, respectively. Many hyperparameters in these
algorithms are similar to scikit-learn implementation of CART with the same func-
tionality but applied to all underlying base trees. However, here perhaps the use of a
parameter such as max_features makes more sense because this is a major factor
in deriving the improved performance of random forests with respect to bagged trees
in many datasets. Furthermore, we need to point out a diÔ¨Äerence between the classi-
Ô¨Åcation mechanism of RandomForestClassifier with the majority vote that we
saw above. In scikit-learn, rather than the majority vote, which is also known as hard

8.5 Pasting
219
voting, a soft voting mechanism is used to classify an instance. In this regard, for a
given observation x, Ô¨Årst the class probability of that observation belonging to each
class is calculated for each base tree‚Äîthis probability is estimated as the proportion
of samples from a class in the leaf that contains x. Then x is assigned to the class
with the highest class probability averaged over all base trees in the forest.
As stated in Section 8.3, using max_features parameter of
BaggingClassifier and BaggingRegressor, we can create random
patches ensemble. The main diÔ¨Äerence between random forest and ran-
dom patches ensemble when decision trees are used as base estimators is
that in random forest the process of feature subset selection is performed for
each split, while in random patches the process of feature subset selection is
performed for each estimator.
8.5 Pasting
The underlying idea in pasting (Breiman, 1999) is that rather than training one
classiÔ¨Åer on the entire dataset, which sometimes can be computationally challenging
for large sample size, we randomly (without replacement) pick a number of modest-
size subsets of training data (say ‚ÄúK‚Äù subsets), construct a predictive model for
each subset, and aggregate the results. The combination rule is similar to what we
observed in Section 8.3 for bagging. Assuming œàj(x) and fj(x) denote the classiÔ¨Åer
and regressor trained for the jth training subset, j = 1, . . ., K, the combination rule
is:
ClassiÔ¨Åcation:
œàpas(x) =
argmax
i‚àà{0,...,c‚àí1}
K
√ï
j=1
I{œàj(x)=i}
(8.22)
Regression:
fpas(x) = 1
K
K
√ï
j=1
fj(x)
(8.23)
Scikit-learn implementation: Pasting is obtained using the same classes that we
saw earlier for bagging (BaggingClassifier and BaggingRegressor) except that
we need to set the bootstrap parameter to False and max_samples to an integer
less than the sample size or a Ô¨Çoat less than 1.0. This causes the sample subset being
drawn randomly without replacement.

220
8 Ensemble Learning
8.6 Boosting
Boosting is an ensemble technique in which a sequence of weak base models (also
known as weak learners) are trained and combined to make predictions. Here we
focus on three important classes of boosting: AdaBoost, gradient boosting regression
tree, and XGBoost.
8.6.1 AdaBoost
In AdaBoost (short for Adaptive Boosting), a sequence of weak learners are trained
on a series of iteratively weighted training data (Freund and Schapire, 1997). The
weight of each observation at each iteration depends on the performance of the
trained model at the previous iteration. After a certain number of iterations, these
models are then combined to make a prediction. The good empirical performance of
AdaBoost on many datasets has made some pioneers such as Leo Breiman to refer
to this ensemble technique as ‚Äúas the most accurate general purpose classiÔ¨Åcation
algorithm available‚Äù (see (Breiman, 2004)). Boosting was originally developed for
classiÔ¨Åcation and then extended to regression. For the classiÔ¨Åcation, it was shown
that even if base classiÔ¨Åers are weak classiÔ¨Åers, which are eÔ¨Écient to train, boosting
can produce powerful classiÔ¨Åers. A weak classiÔ¨Åer is a model with a performance
slightly better than random guessing (for example, its accuracy on a balanced training
data in binary classiÔ¨Åcation problems is slightly better than 0.5). An example of a
weak learner is decision stump, which is a decision tree with only one split. In some
extension of AdaBoost to regression, for example, AdaBoost.R proposed in (Drucker,
1997), the ‚Äúaccuracy better than 0.5‚Äù restriction was carried over and changed to the
‚Äúaverage loss less than 0.5‚Äù.
Here we discuss AdaBoost.SAMME (short for Stagewise Additive Modeling us-
ing a Multi-class Exponential Loss Function) for classiÔ¨Åcation (Zhu et al., 2009)
and AdaBoost.R2 proposed in (Drucker, 1997) for regression, which is a modiÔ¨Å-
cation of AdaBoost.R proposed earlier in (Freund and Schapire, 1997). Suppose a
sample of size n collected from c classes and a learning algorithm that can handle
multiclass classiÔ¨Åcation and supports weighted samples (e.g., decision stumps [see
Section 7.2.5]) are available. AdaBoost.SAMME is implemented as in the following
algorithm.

8.6 Boosting
221
Algorithm AdaBoost.SAMME Algorithm for ClassiÔ¨Åcation
1. Assign a weight wj to each observation j = 1, 2, . . ., n, and initialize them to
wj = 1
n .
2. For m from 1 to M:
2.1. Apply the learning algorithm to weighted training sample to train classiÔ¨Åer
œàm(x).
2.2. Compute the error rate of the classiÔ¨Åer œàm(x) on the weighted training
sample:
ÀÜŒµm =
√çn
j=1 wjI{yj,œàm(xj)}
√çn
j=1 wj
.
(8.24)
2.3. Compute conÔ¨Ådence coeÔ¨Écients
Œ±m = Œ∑

log
1 ‚àíÀÜŒµm
ÀÜŒµm

+ log(c ‚àí1)

,
(8.25)
where Œ∑ > 0 is a tuning hyperparameter known as learning rate.
2.4. For j = 1, 2, . . ., n, update the weights
wj ‚Üêwj √ó exp Œ±mI{yj,œàm(xj)}
 .
(8.26)
3. Construct the boosted classiÔ¨Åer:
œàboost(x) =
argmax
i‚àà{0,...,c‚àí1}
M
√ï
m=1
Œ±mI{œàm(x)=i} .
(8.27)
Few notes about AdaBoost.SAMME:
1. When c = 2, AdaBoost.SAMME reduces to AdaBoost in its original form pro-
posed in (Freund and Schapire, 1997).
2. An underlying idea of AdaBoost is to make the classiÔ¨Åer trained at iteration m+1
focuses more on observations that are misclassiÔ¨Åed by the classiÔ¨Åer trained at
iteration m. To do so, in step 2.4, for an observation j that is misclassiÔ¨Åed by
œàm(x), we scale wj by a factor of exp Œ±m
. However, to increase the inÔ¨Çuence
of such misclassiÔ¨Åed observation in training œàm+1(x), we need to scale up wj,
which means Œ±m > 0. However, to have Œ±m > 0, we only need 1 ‚àíÀÜŒµm > 1
c,
which means it suÔ¨Éces if the classiÔ¨Åer œàm(x) performs slightly better than ran-

222
8 Ensemble Learning
dom guessing. In binary classiÔ¨Åcation, this means ÀÜŒµm < 0.5. This is also used to
introduce an early stopping criterion in AdaBoost; that is to say, the process of
training M base classiÔ¨Åer is terminated as soon as ÀÜŒµm > 1‚àí1
c. Another criterion
for early stopping is when we achieve perfect classiÔ¨Åcation in the sense of having
ÀÜŒµm = 0.
3. To make the Ô¨Ånal prediction by œàboost(x), those classiÔ¨Åers œàm+1(x), m =
1, 2, . . ., M, that had a lower error rate ÀÜŒµm during the training have a greater
Œ±m, which means they have a higher contribution. In other words, Œ±m is an
indicator of conÔ¨Ådence in classiÔ¨Åer œàm(x).
4. If at any point during training AdaBoost, the error on the weighted training
sample becomes 0 (i.e., ÀÜŒµm = 0), we can stop training. This is because when
ÀÜŒµm = 0 the weights will not be updated and the base classiÔ¨Åer for the subsequent
iterations remain the same. At the same time, because Œ±k = 0, k ‚â•m, they are
not added to œàboost(x), which means the AdaBoost classiÔ¨Åer remains the same.
The AdaBoost.R2 (Drucker, 1997) algorithm for regression is presented on the
next page.
Scikit-learn implementation: AdaBoost.SAMME can be implemented using
AdaBoostClassifierfromsklearn.ensemblemodule bysetting thealgoirthm
parameter to SAMME. AdaBoost.R2 is implemented by AdaBoostRegressor from
the same module. Using base_estimator parameter, both AdaBoostClassifier
and AdaBoostRegressor can take as input a user-speciÔ¨Åed base model along
with its hyperparameters. If this parameter is not speciÔ¨Åed, by default a decision
stump (a DecisionTreeClassifier with max_depth of 1) will be used. The
n_estimators=50 parameter controls the maximum number of base models (M
in the aforementioned formulations of these algorithms). In case of perfect Ô¨Åt, the
training terminates before reaching M. Another hyperparameter is the learning rate
Œ∑ (identiÔ¨Åed as learning_rate). The default value of this parameter, which is
1.0, leads to the original representation of AdaBoost.SAMME and AdaBoost.R2
presented in (Zhu et al., 2009) and (Drucker, 1997), respectively.
8.6.2
√â
Gradient Boosting
In (Friedman et al., 2000), it was shown that one can write AdaBoost as a stagewise
additive modeling problem with a speciÔ¨Åc choice of loss function. This observation
along with the connection between stagewise additive modeling with steepest descent
minimization in function space allowed a new family of boosting strategies known as
gradient boosting (Friedman, 2001). Here we Ô¨Årst present an intuitive development
of the gradient boosting technique, then present its formal algorithmic description
as originally presented in (Friedman, 2001, 2002), and Ô¨Ånally its most common
implementation known as gradient boosted regression tree (GBRT).

8.6 Boosting
223
Algorithm √â AdaBoost.R2 Algorithm for Regression
1. Assign a weight wj to each observation j = 1, 2, . . ., n and initialize them to
wj = 1
n .
2. Choose a loss functions L(r). Three candidates are:
Linear: L(r) = r ,
Square law: L(r) = r2 ,
Exponential: L(r) = 1 ‚àíe‚àír .
3. For m from 1 to M:
3.1. Create a sample of size n by sampling with replacement from the
training sample, where the probability of inclusion for the jth observation is
pj = wj/√çn
j=1 wj. To do so, divide a line of length √çn
j=1 wj into subsections of
length wj. Pick a number based on a uniform distribution deÔ¨Åned on the range
[0, √çn
j=1 wj]. If the number falls into subsection wj, pick the jth observation.
3.2. Apply the learning algorithm to the created sample to train regressor fm(x) .
3.3. For j = 1, . . ., n, compute the relative absolute residuals:
rj,m =
|yj ‚àífm(xj)|
max
j |yj ‚àífm(xj)| .
(8.28)
3.4. For j = 1, . . ., n, compute the loss L(rj,m) .
3.5. Compute the average loss:
¬ØLm =
n
√ï
j=1
pjL(rj,m) .
(8.29)
3.6. Compute conÔ¨Ådence coeÔ¨Écients
Œ±m = Œ∑ √ó log
1 ‚àí¬ØLm
¬ØLm

,
(8.30)
where Œ∑ is the learning rate (a hyperparameter).
3.7. For j = 1, 2, . . ., n, update the weights
wj ‚Üêwj √ó exp

‚àíŒ∑ 1 ‚àíL(rj,m)Œ±m

.
(8.31)
4. Construct the boosted regressor as the weighted median:
œàboost(x) = fk(x),
(8.32)
where to obtain k, fm(x), m = 1, . . ., M, are sorted such that f(1)(x) ‚â§f(2)(x) ‚â§
. . . ‚â§f(M)(x), with their corresponding conÔ¨Ådence coeÔ¨Écients denoted by Œ±(m).
Then
k = argmin
s
n
s
√ï
Œ±(m) ‚â•0.5
M
√ï
Œ±m
o
.
m=1
m=1

224
8 Ensemble Learning
The Intuition behind gradient boosting: Given a training data of size n, denoted
Str = {(x1, y1), (x2, y2), . . ., (xn, yn)}, we aim to use a sequential process to construct
a predictive model that can estimate the response y for a given feature vector x.
Suppose in this sequential process, our estimate of y after M iteration is based on
FM(x), which has an additive expansion of the form:
FM(x) =
M
√ï
m=0
Œ≤mhm(x) .
(8.33)
The set of hm(x) is known as the set of basis functions with Œ≤m being the coeÔ¨Écient
of hm(x) in the expansion. From the boosted learning perspective, hm(x) is a ‚Äúbase
learner‚Äù (usually taken as a classiÔ¨Åcation tree) and Œ≤m is its contribution to the Ô¨Ånal
prediction. Suppose we wish to implement a greedy algorithm where at iteration m,
Œ≤mhm(x) is added to the expansion but we are not allowed to revise the decision made
in the previous iteration (i.e., Œ≤0h0(x), . . ., Œ≤m‚àí1hm‚àí1(x)). Therefore, at iteration M,
we can write,
ÀÜyM = FM(x) = FM‚àí1(x) + Œ≤M hM(x),
(8.34)
where ÀÜy denotes the estimate of y. For any iteration m ‚â§M, our estimate ÀÜym is
obtained as
ÀÜym = Fm(x) = Fm‚àí1(x) + Œ≤mhm(x) .
(8.35)
Suppose the function Fm‚àí1(x) = Fm‚àí2(x) + Œ≤m‚àí1hm‚àí1(x) is somehow obtained.
Of course this itself gives us an estimate of yi, i = 1, . . ., n, but now we would like to
estimate Fm(x) = Fm‚àí1(x) + Œ≤mhm(x) to have a better estimate of yi. As Fm‚àí1(x) is
already obtained, estimating Fm(x) is equivalent to estimating Œ≤mhm(x) (recall that
our algorithm is greedy). But how can we Ô¨Ånd Œ≤mhm(x) to improve the estimate? One
option is to learn hm(x) by constraining that to satisfy Œ≤mhm(xi) = yi ‚àíFm‚àí1(xi);
that is, Œ≤mhm(xi) becomes the residual yi ‚àíFm‚àí1(xi). With this speciÔ¨Åc choice we
have
Fm(xi) = Fm‚àí1(xi) + Œ≤mhm(x) = Fm‚àí1(xi) +  yi ‚àíFm‚àí1(xi) = yi,
i = 1, . . ., n .
(8.36)
This means that with this speciÔ¨Åc choice of Œ≤mhm(x), we have at least perfect
estimates of response for training data through the imposed n contraints. However,
this does not mean Œ≤mhm(x) will lead to perfect estimate of response for any given
x. Although (8.36) looks trivial to some extent, there is an important observation to
make. Note that if we take the squared-error loss

8.6 Boosting
225
L y, F(x) = 1
2
 y ‚àíF(x)2 ,
(8.37)
then
‚àÇL y, F(x)
‚àÇF(x)
= F(x) ‚àíy,
(8.38)
and therefore,
h ‚àÇL y, F(x)
‚àÇF(x)
i F(x) = Fm‚àí1(x)
x = xi, y = yi
= Fm‚àí1(xi) ‚àíyi .
(8.39)
The left side of (8.39) is sometimes written as
‚àÇL yi,Fm‚àí1(xi)
‚àÇFm‚àí1(xi)
but for the sake of
clarity, we prefer the notation used in (8.39). From (8.39) we can write (8.36) as
Fm(xi) = Fm‚àí1(xi) ‚àí
h ‚àÇL y, F(x)
‚àÇF(x)
i F(x) = Fm‚àí1(x)
x = xi, y = yi
,
i = 1, . . ., n .
(8.40)
This means that given Fm‚àí1(x), we move in the direction of negative of gradient of loss
with respect to the function. This is indeed the ‚Äústeepest descent‚Äù minimization in the
function space. The importance of viewing the ‚Äúcorrection‚Äù term in (8.36) as in (8.40)
is that we can now replace the squared-loss function with any other diÔ¨Äerentiable loss
function (for example to alleviate the sensitivity of squared-error loss to outliers).
Although the negative of gradient of the squared-loss becomes the residual, for other
loss functions it does not; therefore, rim ‚âú‚àí
h ‚àÇL y,F(x)
‚àÇF(x)
i F(x) = Fm‚àí1(x)
x = xi, y = yi
is known
as ‚Äúpseudo‚Äù-residuals (Friedman, 2002).
Note that hm(x) is a member of a class of models (e.g., a weak learner). Therefore,
one remaining question is whether we can impose the n aforementioned constraints
on hm(xi); that is to say, can we enforce the following relation for an arbitrary
diÔ¨Äerentiable loss function?
Œ≤mhm(xi) = rim,
i = 1, . . ., n,
(8.41)
Perhaps we can not ensure (8.41) exactly; however, we can choose a member of this
weak learner class (to learn it) that produces a vector hm = [hm(x1), . . ., hm(xn)]T,
which is most parallel to rm = [r1m, r2m, . . ., rnm]T. After all, having hm being most
parallel to rm implies taking a direction that is as parallel as possible to the steepest
descent direction across all n observations. In this regard, in (Friedman, 2001)

226
8 Ensemble Learning
Friedman suggested Ô¨Åtting h(x) to pseudo-residuals rim, i = 1, . . ., m, using a least-
squares criterion; that is, using {(x1, r1m), (x2, r2m), . . ., (xn, rnm)} as the training data
to learn hm(x) by
hm(x) = argmin
h(x),œÅ
n
√ï
i=1
 rim ‚àíœÅh(xi)2 .
(8.42)
As we can see, to learn hm(x) we replace responses of yi with rim (its pseudo-
residual at iteration m). Therefore, one may refer to these pseudo-residuals as
pseudo-responses as in (Friedman, 2001). Once hm(x) is learned from (8.42), we
can determine the step size Œ≤m from
Œ≤m = argmin
Œ≤
n
√ï
i=1
L yi, Fm‚àí1(xi) + Œ≤hm(xi) .
(8.43)
Now we can use hm(x) and Œ≤m to update the current estimate Fm‚àí1(x) from (8.35).
For implementation of this iterative algorithm, we can initialize F0(x) and terminate
the algorithm when we reach M. To summarize the gradient boosting algorithm, it is
more convinient to Ô¨Årst present the algorithm for regression, which aligns well with
the aforementioned description.
Gradient Boosting Algorithm for Regression: Here we present an algorithmic
description of gradient boosting as originally presented in (Friedman, 2001, 2002).
In this regard, hereafter, we assume hm(x) is a member of a class of functions that is
parametrized by am = {am,1, am,2, . . .}. As a result, estimating hm(x) is equivalent
to estimating the parameter set am and, therefore, we can use notation h(x; am) to
refer to the weak learner at iteration m.

8.6 Boosting
227
Algorithm Gradient Boosting Algorithm for Regression
1. Initialize: F0(x) = argmin
Œ≤
√çn
i=1 L yi, Œ≤ .
2. For m from 1 to M:
2.1. Compute pseudo-residuals:
rim ‚âú‚àí
h ‚àÇL y, F(x)
‚àÇF(x)
i F(x) = Fm‚àí1(x)
x = xi, y = yi
,
i = 1, . . ., n, .
(8.44)
2.2. Estimate (train) the weak learner parameter set using pseudo-residuals and
least-squares criterion:
am = argmin
a,œÅ
n
√ï
i=1
 rim ‚àíœÅh(xi; a)2 .
2.3. Compute: Œ≤m = argmin
Œ≤
√çn
i=1 L yi, Fm‚àí1(xi) + Œ≤h(xi; am) .
2.4. Update: Fm(x) = Fm‚àí1(x) + Œ≤mh(x; am) .
3. The regressor is FM(x)
For the squared-error loss (L(yi, a) = 1
2(yi ‚àía)2), for example, line 1 and 2.1 in the
above algorithm simplify to F0(x) = 1
n
√çn
i=1 yi and rim = yi ‚àíFm‚àí1(xi), respectively.
Using the absolute-error loss (L(yi, a) = |yi ‚àía|) line 1 and 2.1 are replaced with
F0(x) = median(yi ‚ààStr) and rim = sign(yi ‚àíFm‚àí1(xi)), respectively, where sign(b)
is 1 for b ‚â•0 and -1, otherwise.
8.6.3
√â
Gradient Boosting Regression Tree
Gradient Boosting Regression Tree (GBRT) for Regression: By regression trees,
we are speciÔ¨Åcally referring to CART for regression discussed in Section 7.3. Con-
sidering a J-terminal CART, the parameter set a is determined by:
1) the estimate of targets assigned to any observation falling into a leaf node.
2) the splits in the tree; that is, the splitting variables and threshold at non-leaf
nodes.
The splits can also be perceived as a set of conditions that determine the set of
regions {Rj}J
j=1 that partition the feature space and are identiÔ¨Åed by the J leaf nodes;
after all, a regression tree (at iteration m of gradient boosting) can be written as:

228
8 Ensemble Learning
if x ‚ààRjm then h(x; bm) = bjm or, equivalently,
h(x; bm) =
J
√ï
j=1
bjmI{x‚ààRjm } ,
(8.45)
where I{.} is the indicator function, bjm is the estimate of the target (also re-
ferred to as score) for any training sample point falling into region Rjm, and
bm = {b1m, b2m, . . ., bJm}. Using (8.45) as the update rule in line 2.4 of the Gradient
Boosting algorithm and taking Œ≥jm = Œ≤mbjm leads to the following algorithm for
implementation of GBRT.
Algorithm GBRT for Regression
1. Initialize: F0(x) = argmin
Œ≥
√çn
i=1 L yi, Œ≥ .
2. For m from 1 to M:
2.1. Compute pseudo-residuals:
rim ‚âú‚àí
h ‚àÇL y, F(x)
‚àÇF(x)
i F(x) = Fm‚àí1(x)
x = xi, y = yi
,
i = 1, . . ., n .
(8.46)
2.2. Train a J-terminal regression tree using Str = {(x1, r1m), . . ., (xn, rnm)}
and a mean squared error (MSE) splitting criterion (see Section 7.3) to give
terminal regions Rjm,
j = 1, . . ., J .
2.3. Compute: Œ≥jm = argmin
Œ≥
√ç
xi ‚ààRjm L yi, Fm‚àí1(xi) + Œ≥,
j = 1, . . ., J .
2.4. Update each region separately: Fm(x) = Fm‚àí1(x) + Œ≥jmI{x‚ààRjm },
j =
1, . . ., J .
3. The regressor is FM(x)
In line 2.4 of GBRT, one can also use a shrinkage in which the update rule is
Fm(x) = Fm‚àí1(x) + ŒΩ Œ≥jmI{x‚ààRjm } ,
(8.47)
where 0 < ŒΩ ‚â§1.
Gradient Boosting Regression Tree (GBRT) for ClassiÔ¨Åcation: GBRT is ex-
tended into classiÔ¨Åcation by growing one tree for each class (using squared-error
splitting criterion) at each iteration of boosting and using a multinomial deviance
loss (to compute the pseudo-residuals by its gradient). Without going into the details,

8.6 Boosting
229
we present the algorithm for c-class classiÔ¨Åcation (readers can refer to (Friedman,
2001) for details).
Algorithm GBRT for ClassiÔ¨Åcation
1. Initialize: Fk0(x) = 0,
k = 0, . . ., c ‚àí1 .
2. For m from 1 to M:
2.1. Compute class-speciÔ¨Åc probabilities:
pkm(x) =
eFk(m‚àí1)(x)
√çc‚àí1
k=0 eFk(m‚àí1)(x),
k = 0, . . ., c ‚àí1 .
(8.48)
2.2. For k from 0 to c ‚àí1:
2.2.1. Compute pseudo-residuals:
rikm = I{yi=k } ‚àípkm(xi),
i = 1, . . ., n .
2.2.2. train a J-terminal regression tree using
Str = {(x1, r1km), (x2, r2km), . . ., (xn, rnkm)} and MSE splitting criterion to
give terminal regions Rjkm,
j = 1, . . ., J .
2.2.3. compute: Œ≥jkm = c‚àí1
c
√ç
xi ‚ààR jkm rikm
√ç
xi ‚ààR jkm |rikm |(1‚àí|rikm |),
j = 1, . . ., J .
2.2.4. update each region separately:
Fkm(x) = Fk(m‚àí1)(x) + Œ≥jkmI{x‚ààRjkm },
j = 1, . . ., J .
3. The class labels are then found by computing pk(x) =
eFk M (x)
√çc‚àí1
k=0 eFk M (x), k = 0, . . ., c‚àí
1, and ÀÜy = argmin
k
pk(x) where ÀÜy is the predicted label for x.
Scikit-learn implementation: GBRT for classiÔ¨Åcation and regression are imple-
mented usingGradientBoostingClassifierandGradientBoostingRegressor
classes from sklearn.ensemble module, respectively. By default a regression tree
of max_depth of 3 will be used as the base learner. The n_estimators=100 pa-
rameter controls the maximum number of base models (M in the aforementioned
algorithms). The supported loss values for GradientBoostingClassifier
are deviance and exponential, and for GradientBoostingRegressor are
squared_error, absolute_error, and two more options. For both classes, the
learning_rate parameter is the shrinkage coeÔ¨Écient 0 < ŒΩ ‚â§1 used in (8.47).

230
8 Ensemble Learning
8.6.4
√â
XGBoost
XGBoost (short for eXtreme Gradient Boosting) is a popular and widely used method
that, since its inception in 2014, has helped many teams win in machine learning
competitions. XGBoost gained momentum in machine learning after Tianqi Chen
and Tong He won a special award named ‚ÄúHigh Energy Physics meets Machine
Learning‚Äù (HEP meet ML) for their XGBoost solution during the Higgs Boson
Machine Learning Challenge (Adam-Bourdarios et al., 2015; Chen and He, 2015).
The challenge was posted on Kaggle in May 2014 and lasted for about 5 months,
attracting more than 1900 people competing in 1785 teams. Although the XGBoost
did not achieve the highest score on the test data, Chen and He received the special
‚ÄúHEP meet ML‚Äù award for their solution that was chosen as the most useful model
(judged by a compromise between performance and simplicity) for the ATLAS ex-
periment at CERN. From an optimization point of view, there are two properties that
distinguish XGBoost from GBRT:
1. XGBoost utilizes a regularized objective function to penalize the complexity of
regression tree functions (to guard against overÔ¨Åtting); and
2. XGBoost uses second-order Taylor approximation to eÔ¨Éciently optimize the
objective function. This property means that in contrast with the conventional
GBRT that is like steepest descent method in function space, XGBoost uses a
Newton-Raphson minimization in function space.
In particular, XGBoost estimates y using a sequential process that at each iteration
m, the estimate of y denoted as ÀÜym for a given x has an additive expansion of the
form
ÀÜym = Fm(x) = Fm‚àí1(x) + h(x; bm),
(8.49)
where h(x; bm) is the mapping that characterizes a regression tree learned at iteration
m and is deÔ¨Åned similar to (8.45). Hereafter, and for the ease of notations, we denote
such a mapping that deÔ¨Ånes a regression tree as h(x), and one learned during the
sequential process at iteration m as hm(x). Therefore, for a tree with J leaf nodes,
h(x) is deÔ¨Åned similarly to (8.45), by dropping the iteration index m; that is,
h(x) =
J
√ï
j=1
bjI{x‚ààRj } ,
(8.50)
where {Rj}J
j=1 is the set of regions that partition the feature space and are identiÔ¨Åed
by the J leaf nodes in the tree, and bj is the score given to any training sample point
falling into region Rj. Hereafter, we use h and hm to refer to a regression tree that is
characterized by mappings h(x) and hm(x), respectively. At iteration m, hm is found
by minimizing a regularized objective function as follows:

8.6 Boosting
231
hm = argmin
h‚ààF
n
√ï
i=1
L yi, Fm‚àí1(xi) + h(xi) + ‚Ñ¶(h) ,
(8.51)
where L is a loss function to measure the diÔ¨Äerence between yi and its estimate at
iteration m, ‚Ñ¶(h) measures the complexity of the tree h, and F is the space of all
regression trees. Including ‚Ñ¶(h) in the objective function penalizes the complexity
of trees. This is one way to guard against overÔ¨Åtting, which is more likely to happen
with a complex tree than a simple tree. Recall that we can approximate a real-valued
function f (x + ‚àÜx) using second-order Taylor expansion as
f (x + ‚àÜx) ‚âàf (x) + d f (x)
d x
‚àÜx + 1
2
d2 f (x)
d2 x
‚àÜx2 .
(8.52)
Replacing f (.), x and ‚àÜx in (8.52) with L yi, Fm‚àí1(xi)+ h(xi), Fm‚àí1(xi), and h(xi),
respectively, yields,
L yi, Fm‚àí1(xi) + h(xi) ‚âàL yi, Fm‚àí1(xi) + gi,1 h(xi) + 1
2gi,2 h(xi)2 ,
(8.53)
where
gi,1 =
h ‚àÇL y, F(x)
‚àÇF(x)
i F(x) = Fm‚àí1(x)
x = xi, y = yi
,
(8.54)
gi,2 =
h ‚àÇ2L y, F(x)
‚àÇ2F(x)
i F(x) = Fm‚àí1(x)
x = xi, y = yi
.
(8.55)
For simplicity the partial derivatives in (8.54) and (8.55) are sometimes written as
‚àÇL yi,Fm‚àí1(xi)
‚àÇFm‚àí1(xi)
and
‚àÇ2 L yi,Fm‚àí1(xi)
‚àÇ2 Fm‚àí1(xi)
, respectively. Previously in (8.39) we speciÔ¨Åed
(8.54) for the squared-error loss. For the same loss, as an example, it is straightforward
to see gi,2 = 1. Using (8.53) in (8.51) yields
hm = argmin
h‚ààF
n
√ï
i=1

L yi, Fm‚àí1(xi) + gi,1 h(xi) + 1
2gi,2 h(xi)2
+ ‚Ñ¶(h) .
(8.56)
As L yi, Fm‚àí1(xi) does not depend on h, it is treated as a constant term in the
objective function and can be removed. Therefore,
hm = argmin
h‚ààF
n
√ï
i=1

gi,1 h(xi) + 1
2gi,2 h(xi)2
+ ‚Ñ¶(h) .
(8.57)

232
8 Ensemble Learning
The summation in (8.57) is over the sample points. However, because each sample
point belongs to only one leaf, we can rewrite the summation as a summation over
leaves. To do so, let Ij denote the set containing the indices of training data in leaf
j; that is, Ij = {i|xi ‚ààRj}. Furthermore, let Jh denote the number of leaves in tree
h. Then, using (8.50) we can write (8.57) as
hm = argmin
h‚ààF
Jh
√ï
j=1
Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
¬©¬≠
¬´
√ï
i‚ààIj
gi,1¬™¬Æ
¬¨
bj + 1
2
¬©¬≠
¬´
√ï
i‚ààIj
gi,2¬™¬Æ
¬¨
b2
j
Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£ª
+ ‚Ñ¶(h) ,
(8.58)
which for simplicity can be written as
hm = argmin
h‚ààF
Jh
√ï
j=1

Gj,1 bj + 1
2Gj,2 b2
j

+ ‚Ñ¶(h) ,
(8.59)
where
Gj,1 =
√ï
i‚ààIj
gi,1 ,
Gj,2 =
√ï
i‚ààIj
gi,2 .
(8.60)
There are various ways to deÔ¨Åne ‚Ñ¶(h) but one way that was originally proposed
in (Chen and Guestrin, 2016; Chen and He, 2015) and works quite well in practice
is to deÔ¨Åne
‚Ñ¶(h) = Œ≥Jh + 1
2Œª
Jh
√ï
j=1
b2
j ,
(8.61)
where Œ≥ and Œª are two tuning parameters. The Ô¨Årst term in (8.61) penalizes the
number of leaves in the minimization (8.59) and the second term causes having a
shrinkage eÔ¨Äect on the scores similar to what we saw in Section 6.2.2 for logistic
regression. In other words, large scores are penalized and, therefore, scores have less
room to wiggle. At the same time, having less variation among scores guard against
overÔ¨Åtting the training data. Replacing (8.61) in (8.59) yields
hm = argmin
h‚ààF
objm ,
(8.62)
where
objm =
Jh
√ï
j=1

Gj,1 bj + 1
2(Gj,2 + Œª) b2
j

+ Œ≥Jh .
(8.63)
Each h ‚ààF is determined uniquely by a structure (the splits) and possible values of
leaf scores bj, j = 1, . . ., Jh. Therefore, one way to Ô¨Ånd hm is:

8.6 Boosting
233
1. to enumerate all possible structures; and
2. for each Ô¨Åxed structure Ô¨Ånd the optimal values of leaf scores that minimize the
objective function.
The second part of this strategy, which is Ô¨Ånding optimal values of leaf scores
for a Ô¨Åxed structure, is quite straightforward. This is because the objective function
(8.63) is a quadratic function of bj. Therefore,
‚àÇobjm
‚àÇbj
= Gj,1 + (Gj,2 + Œª) bj .
(8.64)
Setting the derivative (8.64) to 0, we can Ô¨Ånd the optimal value of scores for a Ô¨Åxed
structure with Jh leaves as
b‚àó
j = ‚àí
Gj,1
Gj,2 + Œª, j = 1, . . ., Jh ,
(8.65)
and the corresponding value of the objective function becomes
obj‚àó
m = ‚àí1
2
Jh
√ï
j=1
G2
j,1
Gj,2 + Œª + Œ≥Jh .
(8.66)
Now we return to the Ô¨Årst part of the aforementioned strategy. It is generally in-
tractable to enumerate all possible tree structures. Therefore, we can use a greedy
algorithm similar to CART that starts from a root node and grow the tree. However,
in contrast with impurity measures used in CART (Chapter 7), in XGBoost (8.66) is
used to measure the quality of splits. Note that from (8.66), the contribution of each
leaf node k to the obj‚àó
m is ‚àí1
2
G2
k,1
Gk,2+Œª + Œ≥. We can write the objective function as
obj‚àó
m = ‚àí1
2
Jh
√ï
j=1,j,k
G2
j,1
Gj,2 + Œª + Œ≥(Jh ‚àí1) ‚àí1
2
G2
k,1
Gk,2 + Œª + Œ≥
|               {z               }
contribution of node k
.
(8.67)
Suppose node k is split into left and right nodes kL and kR. The objective function
after this split is
obj‚àó
m, split = ‚àí1
2
Jh
√ï
j=1,j,k
G2
j,1
Gj,2 + Œª + Œ≥(Jh ‚àí1) ‚àí1
2
G2
kL,1
GkL,2 + Œª + Œ≥
|                {z                }
contribution of node kL
‚àí1
2
G2
kR,1
GkR,2 + Œª + Œ≥
|                 {z                 }
contribution of node kR
.
(8.68)
Therefore, the reduction in the objective function achieved by this split is

234
8 Ensemble Learning
gain ‚âúobj‚àó
m ‚àíobj‚àó
m, split = 1
2
 
G2
kL,1
GkL,2 + Œª +
G2
kR,1
GkR,2 + Œª ‚àí
G2
k,1
Gk,2 + Œª
!
‚àíŒ≥ .
(8.69)
To identify the best split at a node, one can enumerate all features and choose the
split that results in the maximum gain obtained by (8.69). Similar to CART, if we
assume the values of training data ready for the split have vj distinct values for feature
xj, j = 1, .., p, then the number of candidate splits (halfway between consecutive
values) is (√çp
j=1 vj)‚àíp. The gain obtained by (8.69) can be interpreted as the quality
of split. Depending on the regularization parameter Œ≥, it can be negative for some
splits. We can grow the tree as long as the gain for some split is still positive. A larger
Œ≥ results in more splits having a negative gain and, therefore, a more conservative
tree expansion. This way we can also think of Œ≥ as the minimum gain required to
split a node.
Scikit-learn-style implementation of XGBoost: XGBoost is not part of scikit-
learn but its Python library can be installed and it has a scikit-learn API that
makes it accessible similar to many other estimators in scikit-learn. If Anaconda
has been installed as instructed in Chapter 2, one can use Conda package manager
to install xgboost package for Python (see instructions at [12]). As XGBoost is
essentially a form of GBRT, it can be used for both classiÔ¨Åcation and regression.
The classiÔ¨Åer and regressor classes can be imported as from xgboost import
XGBClassifier and from xgboost import XGBRegressor, respectively. Once
imported, they can be used similarly to scikit-learn estimators. Some of the im-
portant hyperparameters in these classes are n_estimators, max_depth, gamma,
reg_lambda, and learning_rate, which are the maximum number of base trees,
the maximum depth of each base tree, the minimum gain to split a node (see (8.61)
and (8.69)), the regularization parameter used in (8.61), and the shrinkage coeÔ¨Écient
ŒΩ that can be used in (8.49) to update Fm‚àí1(x) similar to (8.47), respectively.
Exercises:
Exercise 1: Load the training and test Iris classiÔ¨Åcation dataset that was prepared
in Section 4.6. Select the Ô¨Årst two features (i.e., columns with indices 0, 1). Write
a program to train bagging, pasting, random forests, and boosting using CART for
four cases that are obtained by the combination of min_samples_leaf‚àà{2, 10}
and n_estimators‚àà{5, 20} and record (and show) their accuracies on the test
data. Note that a min_samples_leaf=10 will lead to a ‚Äúshallower‚Äù tree than
min_samples_leaf=2. Using the obtained accuracies, Ô¨Åll the blanks in the fol-
lowing lines:
For bagging, the highest accuracy is obtained by a ______ tree (choose: Shallow or
Deep)

8.6 Boosting
235
For pasting, the highest accuracy is obtained by a ______ tree (choose: Shallow or
Deep)
For random forest, the highest accuracy is obtained by a ______ tree (choose: Shal-
low or Deep)
For AdaBoost, the highest accuracy is obtained by a ______ tree (choose: Shallow
or Deep)
Exercise 2: Suppose we have a training dataset from a binary classiÔ¨Åcation problem
and we would like to train a form of AdaBoost classiÔ¨Åer, to which we refer as
‚ÄúAdaBoost.DiÔ¨Ä". AdaBoost.DiÔ¨Äis deÔ¨Åned similar to AdaBoost.SAMME except for
how conÔ¨Ådence coeÔ¨Écients are estimated. In particular, in AdaBoost.DiÔ¨Äwe have
Œ±m =
Ô£±Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£≥
5 ‚àílog  ÀÜŒµm
,
if
ÀÜŒµm > 0.01
0,
otherwise
(8.70)
In training the AdaBoost.DiÔ¨ÄclassiÔ¨Åer, we observe that the error rate of the
base classiÔ¨Åer trained at iteration 5 on the weighted training data is 0. Let œà5
boost(x)
denote the AdaBoost.DiÔ¨ÄclassiÔ¨Åer if we decide to stop training at the end of iteration
5. Similarly, let œà10
boost(x) denote the AdaBoost.DiÔ¨ÄclassiÔ¨Åer if we continue training
until the 10th iteration and stop training at the end of iteration 10. We refer to error
rates of œà5
boost(x) and œà10
boost(x) and on a speciÔ¨Åc test dataset as ‚Äútest error‚Äù. Can we
claim that the training error of œà5
boost(x) on the weighted training data at iteration 5
is 0? How are the test errors of œà5
boost(x) and œà5
boost(x) related?
Exercise 3: Suppose we have n observations (for simplicity take it as an odd number)
in a binary classiÔ¨Åcation problem. Let p denote the accuracy of a classiÔ¨Åer œà(x)
trained using all these n observations; that is to say, the probability of correctly
classifying a given x. Suppose p > 0.5. Assume we use our training data to also
construct an ensemble classiÔ¨Åer œàE(x), which is obtained by using the majority vote
among n base classiÔ¨Åers œài(x), i = 1, . . ., n, which are trained by removing each
observation at a time from training data:
œàE(x) = arg max
y‚àà{0,1}
n
√ï
i=1
I{œài(x)=y} ,
(8.71)
where I{S} is 1 if statement S is true, zero otherwise. Suppose the following assump-
tion holds:
Assumption: Training a base classiÔ¨Åer with any subsets of n ‚àí1 observations has
the same accuracy as training a classiÔ¨Åer with all n observations; in other words,
each œài(x) has an accuracy p.

236
8 Ensemble Learning
This assumption ideally approximates the situation where having one less ob-
servation in data does not considerably aÔ¨Äect the predictability of a classiÔ¨Åcation
rule‚Äîthis could be the case specially if n is relatively large. Let pE shows the
accuracy of œàE(x). Show that pE > p.
√â Exercise 4:
In Exercise 1, write a program to train XGBoost classiÔ¨Åer. In this regard, con-
sider six cases that are obtained by combination of max_depth‚àà{5, 10} and
n_estimators‚àà{5, 20, 100} and show the accuracy of each classiÔ¨Åer on the test
data. Compare your results with classiÔ¨Åers used in Exercise 1.
Exercise 5: Show that for random variables ei, i = 1, . . ., M, we have:
1
M E
h  M
√ï
i=1
ei
2i
‚â§
M
√ï
i=1
E[e2
i ],
(8.72)

Chapter 9
Model Evaluation and Selection
Estimating the performance of a constructed predictive model, also known as model
evaluation, is of essential importance in machine learning. This is because even-
tually the degree to which the model has utility rests with its estimated predictive
performance. Not only are model evaluation methods critical to judge the utility of
a trained model, but they can also play an important role to reach that model in the
Ô¨Årst place. The process of picking a Ô¨Ånal model among a large class of candidate
models is known as model selection‚Äîand there are some popular model selection
techniques that operate based on model evaluation methods. In this chapter, we Ô¨Årst
cover common model evaluation methods and then we examine the utility of some
of these methods for model selection.
9.1 Model Evaluation
Once a predictive model (classiÔ¨Åer or regressor) is constructed, it is particularly
important to evaluate its performance. After all, the worth of a model in practice
depends on its performance over all possible future data that can be collected from
the same problem for which the model is trained. However, such ‚Äúunseen‚Äù data
can naturally not be used for this purpose and the only means of estimating the
performance of a trained model is to use the limited data at hand. Model evaluation
stage (also referred to as model assessment or error estimation) include the use
of methods and metrics to estimate the predictive performance of a model. To
understand the model evaluation process, it would be helpful to distinguish between
evaluation rules and metrics.
A metric is used to quantify the performance of a constructed model. For example,
in Section 4.9, we saw the error rate of a classiÔ¨Åer, which means the probability of
misclassifying an observation drawn from the feature label distribution governing
the problem. However, there are other metrics that can be used to measure various
predictive aspects of a trained model; for example, precision, recall, F1 score, and
237
¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
A. Zollanvari, Machine Learning with Python, https://doi.org/10.1007/978-3-031-33342-2_9 

238
9 Model Evaluation and Selection
Area Under the Receiver Operating Characteristic Curve (abbreviated as AUROC
or ROC AUC), to just name a few.
A model evaluation rule, on the other hand, refers to the procedure that is used to
estimate a metric. In other words, a metric could be estimated using various rules.
For example, we have previously seen hold-out estimation rule to estimate the error
rate of a classiÔ¨Åer (Section 4.9). However, one can use other estimation rules such
as resubstituion, cross-validation, and bootstrap to estimate the error rate.
For simplicity of discussion, in what follows, estimation rules are described in
connection with classiÔ¨Åcation error rate. However, all procedures, except when stated
otherwise, are applicable to estimating classiÔ¨Åer and regressor performance metrics.
9.1.1 Model Evaluation Rules
Let Str,n = {(x1, y1), (x2, y2), . . ., (xn, yn)} denote the training set where index n is
used in Str,n to highlight the sample size. We will discuss the following estimation
rules:
‚Ä¢ Hold-out Estimator
‚Ä¢ Resubstituiton
‚Ä¢ Bootstrap
‚Ä¢ Cross-validation (CV)
‚Äì Standard K-fold CV
‚Äì Leave-one-out
‚Äì StratiÔ¨Åed K-fold CV
‚Äì ShuÔ¨Ñe and Split (Random Permutation CV)
Hold-out Estimator: To estimate the performance of a learning algorithm Œ® on
unseen data, we can simulate the eÔ¨Äect of having unseen data using the available
training data Str,n. To do so, we can randomly split Str,n into a (reduced) training
set Str,l and a test set denoted as Ste,m where l + m = n (e.g., it is common to take
m ‚âà0.25n). We then apply the learning algorithm Œ® to Str,l to train a classiÔ¨Åer œàl(x)
and test its performance on Ste,m. Two points to notice: 1) œàl(x) is the outcome of
applying Œ® to Str,l; and 2) the subindex l in œàl(x) is used to better distinguish the
sample size that is used in training.
Because Ste,m is not used anyhow in training œàl(x), it can be treated as ‚Äúunseen‚Äù
data. This is, however, a special unseen data because the actual label for each
observation that belongs to Ste,m is known and, therefore, can be used to estimate
the performance of œàl(x). This can be done by using œàl(x) to classify each observation
of Ste,m and then compare the assigned labels with the actual labels and estimate the
error rate of œàl(x) as the proportion of samples within Ste,m that are misclassiÔ¨Åed.
This model evaluation rule is known as hold-out estimator.
The hold-out estimate, denoted ÀÜŒµh, is naturally representative of the performance
of œàl(x) in the context within which the data is collected. However, we note that

9.1 Model Evaluation
239
œàl(x) is the outcome of applying the learning algorithm Œ® to Str,l. Viewing ÀÜŒµh
as the estimate of the performance of the learning algorithm outcome would help
prevent confusion and misinterpretations specially in cases when the learning process
contains steps such as scaling and feature selection and extraction (Chapter 11). As
a result, we occasionally refer to the estimate of the performance of a trained model
as the performance of Œ®, which is the learning algorithm that led to the model.
Despite the intuitive meaning of hold-out estimator, its use in small sample (when
the sample size with respect to feature size is relatively small) could be harmful for
both the training stage and the evaluation process. This is because as compared with
large sample, in small sample each observation has generally (and not surprisingly) a
larger inÔ¨Çuence on the learning algorithm. In other words, removing an observation
from a small training data generally harms the performance of a learning algorithms
more than removing an observation from a large training set. Therefore, when n is
relatively small, holding out a portion of Str,n could severely hamper the performance
of the learning algorithm. At the same time, we have already a small sample to work
with and the hold out estimate is obtained even on a smaller set (say 25% of n).
Therefore, we can easily end up with an inaccurate and unreliable estimate of the
performance not only due to the small test size but also because the estimate could
be very bias towards a speciÔ¨Åc set picked for testing. As we will see later in this
section, there are more elegant rules that can alleviate both aforementioned problems
associated with the use of hold-out estimator in small-sample settings.
Nonetheless, one way to remove the harmful impact of holding out a portion of
Str,n for testing in small-sample is that once ÀÜŒµh is obtained, we apply Œ® to the entire
dataset Str,n to train a classiÔ¨Åer œàn(x) but still use ÀÜŒµh, which was estimated on Ste,m
using œàl(x), as the performance estimate of œàn(x). There is nothing wrong with this
practice: we are just trying to utilize all available data in training the classiÔ¨Åer that
will be Ô¨Ånally used in practice. The hold-out estimate ÀÜŒµh can be used as an estimate
of the performance of œàn(x) because ÀÜŒµh is our Ô¨Ånal judgement (at least based on
hold-out estimator) about the performance of Œ® in the context of interest. Not only
that, to train œàn(x) we add more data to the data that was previously used to train
œàl(x). With ‚Äúwell-behaved‚Äù learning algorithms, this should not generally lead to a
worse performance. Although one may argue that it makes more sense to use ÀÜŒµh as
an estimate of an upper bound on the error rate of œàn(x), on a given dataset and with
hold-out estimator, we have no other means to examine that and, therefore, we use
ÀÜŒµh as the estimate of error rate of œàn(x).
Resubstitution Estimator: The outcome of applying Œ® to a training set Str,n is
a classiÔ¨Åer œàn(x). The proportion of samples within Str,n that are misclassiÔ¨Åed by
œàn(x) is the resubstitution estimate of the error rate (Smith, 1947) (also known
as apparent error rate). Resubstitution estimator will generally lead to an overly
optimistic estimate of the performance. This is not surprising because the learning
algorithm attempts to Ô¨Åt Str,n to some extent. Therefore, it naturally performs better
on Str,n rather than on an independent set of data collected from the underlying
feature-label distributions. As a result this estimator is not generally recommended
for estimating the performance of a predictive model.

240
9 Model Evaluation and Selection
Bootstrap: Given a training data Str,n, B bootstrap samples, denoted S‚àó
tr,n,k, k =
1, 2, . . ., B, are created by randomly drawing n observations with replacement from
Str,n. Each bootstrap sample may not include some observations from Str,n because
some other observations are replicated. The main idea behind the bootstrap family
of estimators is to treat the observations that do not appear in S‚àó
tr,n,k as test data for a
model that is trained using S‚àó
tr,n,k. In particular, a speciÔ¨Åc type of bootstrap estimator
generally referred to as E0 estimator (here denoted as ÀÜŒµ0
B) is obtained based on the
following algorithm.
Algorithm E0 Estimator
1) Set B to an integer between 25 to 200 (based on a suggestion from Efron in
(Efron, 1983)).
2) Let Ak, k = 1, . . ., B, denote the set of observations that do not appear in
bootstrap sample k; that is, Ak
‚àÜ= Str,n ‚àíS‚àó
tr,n,k
3) Classify each observation in Ak by a classiÔ¨Åer œàk(x) that is trained by ap-
plying Œ® to S‚àó
tr,n,k. Let ek denote the number of misclassiÔ¨Åed observations in Ak.
4) ÀÜŒµ0
B error estimate is obtained as the ratio of the number of misclassiÔ¨Åed obser-
vations among samples in Ak‚Äôs over the total number of samples in Ak‚Äôs; that
is,
ÀÜŒµ0
B =
√çB
k=1 ek
√çB
k=1 |Ak|
.
(9.1)
There are other variants of bootstrap (e.g., 0.632 or 0.632+ bootstrap); however, we
keep our discussion on bootstrap estimators short because they are computationally
more expensive than K-fold CV (for small K)‚Äîrecall that B in bootstrap is generally
between 25 to 200.
Cross-validation: Cross-validation (Lachenbruch and Mickey, 1968) is perhaps the
most common technique used for model evaluation because it provides a reasonable
compromise between computational cost and reliability of estimation. To understand
the essence of cross-validation (abbreviated as CV), we take a look at the hold-out
estimator again. In the hold-out, we split Str,n to a training set Str,l and a test
set Ste,m. We previously discussed problems associated with the use of hold-out
estimator specially in small-sample. In particular, our estimate could be biased to the
choice of held-out set Ste,m, which means that hold-out estimator can have a large
variance from sample to sample (collected from the same population). To improve
the variance of hold-out estimator, we may decide to switch the role of training and
test sets; that is to say, train a new classiÔ¨Åer on Ste,m, which is now the training set,

9.1 Model Evaluation
241
and test its performance on Str,l, which is now treated as the test set. This provides
us with another hold-out estimate based on the same split (except for switching the
roles) that we can use along with the previous estimate to estimate the performance
of the learning algorithm Œ® (e.g., by taking their average). Although to obtain the two
hold-out estimates we preserved the nice property of keeping the test data separate
from training, the second training set (i.e., Ste,m) has a much smaller sample size with
respect to the entire data at hand‚Äîrecall that m is typically ‚àº25% of n. Therefore,
the estimate of the performance that we see by using Ste,m as training set can not
be generally a good estimate of the performance of applying Œ® to Str,n specially in
situations where n is not large. To bring the training sample size used to obtain both
of these hold-out estimates as close as possible to n, we need to split the data equally
to training and test sets; however, even that can not resolve the problem for moderate
to small n because a signiÔ¨Åcant portion of the data (i.e., 50%) is not still used for
training. As we will see next, CV also uses independent pairs of test and training
sets to estimate the performance. However, in contrast with the above procedure, the
procedure of training and testing is repeated multiple times (to reduce the variance)
and rather than 50% of the entire data generally larger training sets are used (to
reduce the bias).
Standard K-fold CV: This is the basic form of cross-validation and is implemented
as in the following algorithm.
Algorithm Standard K-fold CV
1. Split the dataset Str,n into K equal-size datasets also known as folds. Let Foldk,
k = 1, . . ., K denote fold k; therefore, √êK
k=1 Foldk = Str,n.
2. for k from 1 to K:
2.1. Hold out Foldk from Str,n to obtain Str,n ‚àíFoldk dataset (i.e., the entire
data excluding fold K).
2.2. Apply Œ® to Str,n ‚àíFoldk to construct surrogate classiÔ¨Åer œàk(x).
2.3. Classify observations in Foldk using œàk(x). Let ek denote the number of
misclassiÔ¨Åed observations in Foldk.
3. Compute the K-fold CV error estimate, denoted ÀÜŒµK‚àíf old
cv
, as the ratio of the total
number of misclassiÔ¨Åed observations over the total sample size n; that is,
ÀÜŒµK‚àíf old
cv
= 1
n
K
√ï
k=1
ek .
(9.2)

242
9 Model Evaluation and Selection
Fig. 9.1 shows a schematic representation of the working mechanism of K-fold
CV. In implementing the K-fold CV, conventionally the Ô¨Årst 1/K portion of data are
assigned to Fold1, the second 1/K portion of data are assigned to Fold2, . . ., and the
last 1/K portion of data are assigned to FoldK. However, this practice could cause
some problems. For example, in Fig. 9.1, we see that for the Ô¨Årst two iterations of the
K-fold CV (i.e., k = 1, 2), the held-out folds (i.e., Fold1 and Fold2) only contain data
from Class 0. This means the performance estimate of surrogate classiÔ¨Åers œà1(x) and
œà2(x) is only relevent in predicting instances of Class 0. Not only that, even in some
iterations, to train a surrogate classiÔ¨Åer, the training data may not include data from
all classes. For example, here to train œà5(x) no instance of Class 2 is used in training
but then the evaluation is performed merely on instances of Class 2. To alleviate
these issues, we can shuÔ¨Ñe the data before dividing them into K folds. However, to
shuÔ¨Ñe the data, we need to assume that samples within each class are independent
and identically distributed. If this assumption does not hold, by shuÔ¨Ñing we may
use highly dependent data across training and test sets, which could lead to an overly
optimistic error estimate. Fig. 9.2 shows the training and the test sample indices for
the K-fold CV where the data is shuÔ¨Ñed before splitting into K folds. The size of
each fold in this Ô¨Ågure is similar to the size of folds in Fig. 9.1. Furthermore, similar
to Fig. 9.1, each observation is used as a test observation in only one fold.
Training Data - Foldk
Class 0
Class 1
Class 2
Foldk
Fig. 9.1: The working mechanism of standard K-fold CV where the data is divided
into 5 consecutive folds (K = 5).
Leave-one-out (loo): The loo estimator of the performance is obtained by setting
K = n where n is the sample size; that is to say, the data is divided into n folds where
each fold contains only one observation. Therefore, to construct Str,n ‚àíFoldk, k =
1, . . ., n, we only need to sequentially leave each observation out. All aforementioned
steps in the K-fold CV follows similarly by noting that Foldk contains only one
observation. Naturally, shuÔ¨Ñing data has no eÔ¨Äect because all observations will
be held out precisely once to serve as a test observation for a surrogate classiÔ¨Åer.
Furthermore, because in each iteration of loo, the training data Str,n ‚àíFoldk is
almost the same size as the entire training data Str,n, we expect loo to possess a

9.1 Model Evaluation
243
Training Data - Foldk
Class 0
Class 1
Class 2
Foldk
Fig. 9.2: The working mechanism of K-fold CV with shuÔ¨Ñing where K = 5.
lower bias with respect to the K-fold CV (K < n) that has n/K observations less
than Str,n. That being said, loo has a larger variance (Kittler and DeVijver, 1982).
Furthermore, we note that the use of loo estimator needs constructing n surrogate
classiÔ¨Åers. Therefore, this estimator is practically feasible when n is not large. In
addition,
StratiÔ¨Åed K-fold CV: In Section 4.4, we stated that it is generally a good practice
to split the data into training and test sets in a stratiÔ¨Åed manner to have the same
proportion of samples from diÔ¨Äerent classes in both the training and the test sets.
This is because under a random sampling for data collection, the proportion of
samples from each class is itself an estimate of the prior probability of that class;
that is, an estimate of the probability that an observation from a class appears before
making any measurements. These prior probabilities are important because once
a classiÔ¨Åer is developed for a problem with speciÔ¨Åc class prior probabilities, one
expects to employ the classiÔ¨Åer in the future under a similar context where the prior
probabilities are similar. In other words, in a random sampling, if the proportion of
samples from each class are changed radically from training set to test set (this is
specially common in case of sampling the populations separately), we may naturally
expect a failed behaviour from both the trained classiÔ¨Åer (Shahrokh Esfahani and
Dougherty, 2013) andd its performance estimators (Braga-Neto et al., 2014).
In terms of model evaluation using CV, this means in order to have a realistic view
of the classiÔ¨Åer performance on test data, in each iteration of CV we should keep the
proportion of samples from each class in the training set and the held out fold similar
to the full training data. This is achieved by another variant of cross-validation known
as stratiÔ¨Åed K-fold CV, which in classiÔ¨Åcation problems is generally the preferred
method compared with the standard K-fold CV. Fig. 9.3 shows the training and
test sample indices for the stratiÔ¨Åed K-fold CV where the data is divided into 5
consecutive folds. The size of each held-out fold in this Ô¨Ågure is similar to the size of
folds in Fig 9.1; however, the proportion of samples from each class that appears in
each fold is kept approximately the same as the original dataset. As before we may

244
9 Model Evaluation and Selection
even shuÔ¨Ñe data before splitting to reduce the chance of having a systematic bias in
the data. Fig. 9.4 shows a schematic representation of splits in this setting.
Training Data - Foldk
Class 0
Class 1
Class 2
Foldk
Fig. 9.3: The working mechanism of stratiÔ¨Åed K-fold CV where the data is divided
into a set of consecutive folds such that the proportion of samples from each class is
kept the same as the full data (K = 5).
Training Data - Foldk
Class 0
Class 1
Class 2
Foldk
Fig. 9.4: The working mechanism of stratiÔ¨Åed K-fold CV with shuÔ¨Ñing (K = 5).
ShuÔ¨Ñe and Split (Random Permutation CV): Suppose we have a training dataset
that is quite large in terms of sample size and, at the same time, we have a compu-
tationally intensive Œ® to evaluate. In such a setting, even training one single model
using all the available dataset may take a relatively long time. Evaluating Œ® on this
data using leave-one-out is certainly not a good option because we need to repeat the
entire training process n times on almost the entire dataset. We may decide to use,
for example, 5-fold CV. However, even that could not be eÔ¨Écient because for each
iteration of 5-fold CV, the training data has a sample size of n ‚àín/5, which is still
large and could be an impediment in repeating the training process several times.
In such a setting, we may still decide to train our Ô¨Ånal model on the entire training

9.1 Model Evaluation
245
data (to use all available data in training the classiÔ¨Åer that will be Ô¨Ånally used in
practice), but for evaluation, we may randomly choose a subset of the entire dataset
for training and testing, and repeat this procedure K times. This shuÔ¨Ñe and split
evaluation process (also known as random purmutation CV) does not guarantee that
an observation that is used once in a fold as a test observation does not appear in
other folds. Fig. 9.5 shows a schematic representation of the splits for this estimator
where we choose the size of training data at each iteration to be 50% of the entire
dataset, the test data to be 20%, and the rest 30% are unused data (neither used for
training nor testing).
One might view the performance estimate obtained this way as an overly pes-
simistic estimate of our Ô¨Ånal trained model; after all, the Ô¨Ånal model is generally
trained on a much larger dataset, which means the Ô¨Ånal model has generally a better
performance compared to any surrogate model trained as part of this estimator. How-
ever, we need to note that removing a portion of samples in large-sample settings
has generally less impact on the performance of a learning algorithm than removing
a similar proportion in small-sample situations. At the same time, shuÔ¨Ñe-and-split
is commonly used in large-sample settings, which to some extent limits the degree
of its pessimism.
Training Data - Foldk
Unused Data
Class 0
Class 1
Class 2
Foldk
Fig. 9.5: The working mechanism of shuÔ¨Ñe and split where the training and test
data at each iteration is 50% and 20%, respectively. In contrast with other forms of
cross-validation, here an observation could be used for testing multiple times across
various CV iterations.
Scikit-learn implementation of various cross-validation schemes: The KFold
class from sklearn.model_selection module creates the indices for Foldk and
Str,n ‚àíFoldk for each iteration of K-fold CV. By default, it splits the data Str,n into
5 consecutive folds without applying any shuÔ¨Ñing (similar to Fig. 9.1). However,
using the following parameters we can change its default behaviour:
1) n_splits changes the number of folds;
2) setting shuffle to True leads to shuÔ¨Ñing the data before splitting into
n_splits folds; and

246
9 Model Evaluation and Selection
3) using random_state we can create reproducible results if shuffle is set to
True.
The leave-one-out estimator is obtained using LeaveOneOut class from the same
module. This is equivalent to using KFold(n_splits=n) where n is the sample
size.
Let us now use this class to implement the K-fold CV in the Iris classiÔ¨Åcation
dataset. In this regard, we load the original Iris dataset before any preprocessing, as
before we split the dataset into a training and test set (in case a test set is also desired),
and we apply the cross-validation only on the training data. We create an object from
KFold class and invoke the split method to generate the training (Str,n ‚àíFoldk)
and test (Foldk) sets for each iteration of the 3-fold CV. Furthermore, in iteration
k, we train a classiÔ¨Åer (here 5NN) on the training set (Str,n ‚àíFoldk) and test its
accuracy on Foldk, which is the test set for that iteration. Last but not least, we obtain
the overall K-fold CV estimate of the accuracy by taking the mean of the accuracy
estimates obtained at each iteration.
import numpy as np
from sklearn import datasets
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier as KNN
# load training data
iris = datasets.load_iris()
X_train, X_test, y_train, y_test= train_test_split(iris.data, iris.
,‚Üítarget, random_state=100, test_size=0.2, stratify=iris.target)
print('X_train_shape: ' + str(X_train.shape) + '\nX_test_shape: ' +‚ê£
,‚Üístr(X_test.shape)\
+ '\ny_train_shape: ' + str(y_train.shape) + '\ny_test_shape: '‚ê£
,‚Üí+ str(y_test.shape) + '\n')
# to set up the K-fold CV splits
K_fold = 3
kfold = KFold(n_splits=K_fold, shuffle=True, random_state=42) #‚ê£
,‚Üíshuffling is used
cv_scores = np.zeros(K_fold)
knn = KNN()
for counter, (train_idx, test_idx) in enumerate(kfold.split(X_train,‚ê£
,‚Üíy_train)):
print("K-fold CV iteration " + str(counter+1))
print("Train indices:", train_idx)
print("Test indices:", test_idx, "\n")
X_train_kfold = X_train[train_idx,]
y_train_kfold = y_train[train_idx,]
X_test_kfold = X_train[test_idx,]
y_test_kfold = y_train[test_idx,]

9.1 Model Evaluation
247
cv_scores[counter] = knn.fit(X_train_kfold, y_train_kfold).
,‚Üíscore(X_test_kfold, y_test_kfold)
print("the accuracy of folds are: ", cv_scores)
print("the overall 3-fold CV accuracy is: {:.3f}".format(cv_scores.
,‚Üímean()))
X_train_shape: (120, 4)
X_test_shape: (30, 4)
y_train_shape: (120,)
y_test_shape: (30,)
K-fold CV iteration 1
Train indices: [
1
2
3
5
6
7
8
13
14
16
17
19
20
21 ‚ê£
,‚Üí23
25 27
28 29
32
33
34
35
37
38
39
41
43
46
48
49
50 ‚ê£
,‚Üí51
52
54
57 58
59
60
61
63
66
67
68
69
71
72
74
75
77 ‚ê£
,‚Üí79
80
81
82 83
84
85
86
87
90
92
93
94
95
98
99 100 101‚ê£
,‚Üí102 103 105 106 108 111 112 113 115 116 117 119]
Test indices: [
0
4
9
10
11
12
15
18
22
24
26
30
31
36 ‚ê£
,‚Üí40
42 44
45 47
53
55
56
62
64
65
70
73
76
78
88
89
91 ‚ê£
,‚Üí96
97 104 107
109 110 114 118]
K-fold CV iteration 2
Train indices: [
0
1
2
4
9
10
11
12
14
15
18
20
21
22 ‚ê£
,‚Üí23
24 26
29 30
31
32
36
37
40
41
42
44
45
47
48
51
52 ‚ê£
,‚Üí53
55
56
57 58
59
60
61
62
63
64
65
70
71
73
74
75
76 ‚ê£
,‚Üí78
79
81
82 86
87
88
89
91
92
93 96
97
99 101 102 103 104‚ê£
,‚Üí105 106 107 108 109 110 112 114 115 116 118 119]
Test indices: [
3
5
6
7
8
13
16
17
19
25
27
28
33
34 ‚ê£
,‚Üí35
38 39
43 46
49
50
54
66
67
68
69
72
77
80
83
84
85 ‚ê£
,‚Üí90
94
95
98
100 111 113 117]
K-fold CV iteration 3
Train indices: [
0
3
4
5
6
7
8
9
10
11
12
13
15
16 ‚ê£
,‚Üí17
18 19
22 24
25
26
27
28
30
31
33
34
35
36
38
39
40 ‚ê£
,‚Üí42
43
44
45 46
47
49
50
53
54
55
56
62
64
65
66
67
68 ‚ê£
,‚Üí69
70
72
73 76
77
78
80
83
84
85
88 89
90
91
94
95
96 ‚ê£
,‚Üí97
98 100 104 107 109 110 111 113 114 117 118]
Test indices: [
1
2
14
20
21
23
29
32
37
41
48
51
52
57 ‚ê£
,‚Üí58
59 60
61 63
71
74
75
79
81
82
86
87
92
93 99 101 102 103‚ê£
,‚Üí105 106 108 112 115 116 119]
the accuracy of folds are:
[0.975 0.975 0.95 ]
the overall 3-fold CV accuracy is: 0.967
Note that here we did not use np.load('data/iris_train_scaled.npz') to
load the Iris dataset that we preprocessed before. This is not a matter of taste but
necessity to avoid data leakage. To understand the reason, it is important to realize that
each Foldk should be precisely treated as an independent test set. At the same time,

248
9 Model Evaluation and Selection
recall from Section 4.4-4.6 that when both splitting and preprocessing are desired,
the data is Ô¨Årst split into training and test sets and then preprocessing is applied to
the training set only (and the test set is transformed by statistics obtained on training
set). This practice ensures that no information from the test set is used in training via
preprocessing. In CV, our training set after the ‚Äúsplit‚Äù at each iteration is Str,n‚àíFoldk
and the test data is Foldk. Therefore, if a preprocessing is desired, the preprocessing
must be applied to Str,n ‚àíFoldk at each iteration separately and Foldk should be
transformed by statistics obtained on Str,n‚àíFoldk. In Chapter 11, we will see how this
could be done in scikit-learn. In the meantime, we have to realize that if we apply the
CV on the training data stored in np.load('data/iris_train_scaled.npz'),
some information of Foldk has already been used in Str,n ‚àíFoldk via preprocessing,
which is not legitimate (see Section 4.6 for a discussion on why this is an illegitimate
practice).
Rather than implementing a for loop as shown above to train and test surrogate
classiÔ¨Åers at each iteration, scikit-learn oÔ¨Äers a convenient way to obtain the CV
scores using cross_val_score function from sklearn.model_selection mod-
ule. Among several parameters of this class, the following parameters are specially
useful:
1) estimator is to specify the estimator (e.g., classiÔ¨Åer or regressor) used within
CV;
2) X is the training data;
3) y is the values of target variable;
4) cv determines the CV strategy as explained here:
‚Äì setting cv to an integer K in a classiÔ¨Åcation problem uses stratiÔ¨Åed K-fold
CV with no shuÔ¨Ñing. To implement, the K-fold CV for classiÔ¨Åcation, we can
also set cv parameter to an object of KFold class and if shuÔ¨Ñing is required,
the shuffle parameter of KFold should be set to True;
‚Äì setting cv to an integer K in a regression problem uses standard K-fold CV
with no shuÔ¨Ñing (note that stratiÔ¨Åed K-fold CV is not deÔ¨Åned for regression).
If shuÔ¨Ñing is required, the shuffle parameter of KFold should be set to True
and cv should be set to an object of KFold;
5) n_jobs is used to specify the number of CPU cores that can be used in parallel
to compute the CV estimate. Setting n_jobs=-1 uses all processors; and
6) scoring: the default None value of this parameter leads to using the default
metric of the estimator score method; for example, in case of using the es-
timator DecisionTreeRegressor, ÀÜR2 is the default scorer and in case of
DecisionTreeClassifier, the accuracy estimator is the default scorer.
cross_val_score uses a scoring function (the higher, the better) rather than
a loss function (the lower, the better). In this regard, a list of possible scoring
functions are available at (Scikit-metrics, 2023). We will also discuss in detail some
of these scoring functions later in this Chapter. Here we use cross_val_score to
implement the standard K-fold CV. Note that the results are the same as what were
obtained before.

9.1 Model Evaluation
249
from sklearn.model_selection import cross_val_score
#kfold = KFold(n_splits=K_fold, shuffle=True, random_state=42) # kfold‚ê£
,‚Üíis already instantiated from this line that we had before
knn = KNN()
cv_scores = cross_val_score(knn, X_train, y_train, cv=kfold)
print("the accuracy of folds are: ", cv_scores)
print("the overall 3-fold CV accuracy is: {:.3f}".format(cv_scores.
,‚Üímean()))
the accuracy of folds are:
[0.975 0.975 0.95 ]
the overall 3-fold CV accuracy is: 0.967
Next we use cross_val_score to implement the stratiÔ¨Åed K-fold CV. As we
said before, setting cv parameter to an integer by default uses the stratiÔ¨Åed K-fold
CV for classiÔ¨Åcation.
cv_scores = cross_val_score(knn, X_train, y_train, cv=3)
print("the accuracy of folds are: ", cv_scores)
print("the overall 3-fold CV accuracy is: {:.3f}".format(cv_scores.
,‚Üímean()))
the accuracy of folds are:
[0.925 0.95
1.
]
the overall 3-fold CV accuracy is: 0.958
Similar to KFold class that can be used to create the indices for the train-
ing and test at each CV iteration, we also have StratifiedKFold class from
sklearn.model_selection module that can be used for creating indices. There-
fore, another way that gives us more control over stratiÔ¨Åed K-fold CV is to set the
cv parameter of cross_val_score to an object from StratifiedKFold class.I
For example, here we shuÔ¨Ñe the data before splitting (similar to Fig. 9.4) and set the
random_state for reproducibility:
from sklearn.model_selection import StratifiedKFold
strkfold = StratifiedKFold(n_splits=K_fold, shuffle=True,‚ê£
,‚Üírandom_state=42)
cv_scores = cross_val_score(knn, X_train, y_train, cv=strkfold)
print("the accuracy of folds are: ", cv_scores)
print("the overall 3-fold CV accuracy is: {:.3f}".format(cv_scores.
,‚Üímean()))
the accuracy of folds are:
[0.925 1.
1.
]
the overall 3-fold CV accuracy is: 0.975
To implement the shuÔ¨Ñe and split method, we can use ShuffleSplit class from
sklearn.model_selection module that can be used for creating indices and then
set the cv parameter of cross_val_score to an object from ShuffleSplit class.

250
9 Model Evaluation and Selection
Two important parameters of this class are test_size and train_size, which
could be either a Ô¨Çoating point between 0.0 and 1.0 or an integer. In the former case,
the value of the parameter is interpreted as the proportion with respect to the entire
dataset, and in the latter case, it becomes the number of observations used.
9.1.2 Evaluation Metrics for ClassiÔ¨Åers
Our aim here is to use an evaluation rule to estimate the performance of a trained
classiÔ¨Åer. In this section, we will cover diÔ¨Äerent metrics to measure various aspects
of a classiÔ¨Åer performance. We will discuss both the probabilistic deÔ¨Ånition of
these metrics as well as their empirical estimators. Although we distinguish the
probabilistic deÔ¨Ånition of a metric from its empirical estimator, it is quite common
in machine learning literature to refer to an empirical estimate as the metric itself. For
example, the sensitivity of a classiÔ¨Åer is a probabilistic concept. However, because
in practice we almost always need to estimate it from a data at hand, it is common to
refer to its empirical estimate per se, as the sensitivity.
Suppose a training set Str,n is available and used to train a binary classiÔ¨Åer œà. In
particular, œà maps realizations of random feature vector X to realizations of a class
variable Y that takes two values: ‚ÄúP‚Äù and ‚ÄúN‚Äù, which are short for ‚Äúpositive‚Äù and
‚Äúnegative‚Äù, respectively; that is, œà(X) : Rp ‚Üí{P, N} where p is the dimensionality
of X. Although assigning classes as P and N is generally arbitrary, the classiÔ¨Åcation
emphasis is often geared towards identifying class P instances; that is to say, we
generally label the ‚Äúatypical‚Äù label as positive and the ‚Äútypical‚Äù one as negative. For
example, people with a speciÔ¨Åc disease (in contrast with healthy people) or spam
emails (in contrast with non-spam emails) are generally allocated as the positive
class.
The mapping produced by œà(X) from Rp to {P, N} has an intermediary step.
Suppose x is a realization of X. The classiÔ¨Åcation is performed by Ô¨Årst mapping
x ‚ààRp to s(x), which is a score on a univariate continuum such as R. This score
could be, for example, the distance of an observation from the decision hyperplanes,
or the estimated probability that an instance belongs to a speciÔ¨Åc class. The score
s(x) is then compared to t, which is a speciÔ¨Åc value of threshold T. We assume scores
are oriented such that if s(x) exceeds t, then x is assigned to P and otherwise to N.
Formally, we write
œà(x) =
(
P
if s(x) > t ,
N
otherwise .
(9.3)
In what follows, we deÔ¨Åne several probabilities that can help measure diÔ¨Äer-
ent aspects of a classiÔ¨Åer. To discuss the empirical estimators of these proba-
bilities, we assume the trained classiÔ¨Åer œà is applied to a sample of size m,
Ste,m = {(x1, y1), (x2, y2), . . ., (xm, ym)} and observations x1, . . ., xm are classiÔ¨Åed as

9.1 Model Evaluation
251
ÀÜy1, . . ., ÀÜym, respectively. We can categorize the outcome of the classiÔ¨Åer for each xi
as a true positive (TP), a false positive (FP), a true negative (TN), or a false negative
(FN), which mean:
1) TP: both ÀÜyi and yi are P;
2) FP: ÀÜyi is P but yi is N (so the positive label assigned by the classiÔ¨Åer is false);
3) TN: both ÀÜyi and yi are N; and
4) FN: ÀÜyi is N but yi is P (so the negative label assigned by the classiÔ¨Åer is false).
Furthermore, the number of true positives, false positives, true negatives, and false
negatives among instances of Ste,m that are classiÔ¨Åed by œà are denoted nTP, nFP, nTN,
and nFN, respectively. The actual number of positives and negatives within Ste,m are
denoted by nP and nN, respectively. Therefore, nP = nTP + nFN, nN = nTN + nFP, and
nTP + nFP + nTN + nFN = nP + nN = m. We can summarize the counts of these states
in a square matrix as follows, which is known as Confusion Matrix (see Fig. 9.6).
nTP
nFN
nFP
nTN
Fig. 9.6: Confusion matrix for a binary classiÔ¨Åcation problem
True positive rate (tpr): the probability that a class P instance is correctly classiÔ¨Åed;
that is, tpr = P s(X) > t|X ‚ààP. It is also common to refer to tpr as sensitivity (as
in clinical/medical applications) or recall (as in information retrieval literature). The
empirical estimator of tpr, denoted ÀÜ
tpr, is obtained as
ÀÜ
tpr = nTP
nP
=
nTP
nTP + nFN
.
(9.4)
False positive rate (fpr): the probability that a class N instance is misclassiÔ¨Åed;
that is, f pr = P s(X) > t|X ‚ààN. Its empirical estimator, denoted
ÀÜ
f pr, is
ÀÜ
f pr = nFP
nN
=
nFP
nTN + nFP
.
(9.5)
True negative rate (tnr): the probability that a class N instance is correctly clas-
siÔ¨Åed; that is, tnr = P(s(X) ‚â§t|X ‚ààN). In clinical and medical applications, it is
common to refer to tnr as speciÔ¨Åcity. Its empirical estimator is
ÀÜ
tnr = nTN
nN
=
nTN
nTN + nFP
.
(9.6)

252
9 Model Evaluation and Selection
False negative rate (fnr): the probability that a class P instance is misclassiÔ¨Åed;
that is, f nr = P s(X) ‚â§t|X ‚ààP. Its empirical estimator is
ÀÜf nr = nFN
nP
=
nFN
nTP + nFN
.
(9.7)
Although here four metrics are deÔ¨Åned, we have tpr + f nr = 1 and f pr +tnr = 1;
therefore, having tpr and f pr summarize all information in these four metrics.
Positive predictive value (ppv): this metric is deÔ¨Åned by switching the role of
s(X) > t and X ‚ààP in the conditional probability used to deÔ¨Åne tpr; that is to say,
the probability that an instance predicted as class P is truly from that class (i.e., is
correctly classiÔ¨Åed): ppv = P X ‚ààP|s(X) > t. It is also common to refer to ppv as
precision. The empirical estimator of ppv is
ÀÜ
ppv =
nTP
nTP + nFP
.
(9.8)
False discovery rate (fdr)1: this metric is deÔ¨Åned as the probability that an instance
predicted as class P is truly from class N (i.e., is misclassiÔ¨Åed): f dr = P X ‚àà
N|s(X) > t = 1 ‚àíppv. The empirical estimator of f dr is
ÀÜ
f dr =
nFP
nTP + nFP
.
(9.9)
F1 score: to summarize the information of recall and precision using one metric,
we can use F1 measure (also known as F1 score), which is given by the harmonic
mean of recall and precision:
F1 =
2
1
recall +
1
precision
2 recall √ó precision
recall + precision .
(9.10)
As can be seen from (9.10), if either of precision and recall are low, F1 will be
low. The plug-in estimator of F1 score is obtained by replacing recall and precision
in (9.10) with their estimates obtained from (9.4) and (9.8), respectively.
Error rate: As seen in Section 4.9, the error rate, denoted Œµ, is deÔ¨Åned as the
probability of misclassiÔ¨Åcation by the trained classiÔ¨Åer; that is to say,
Œµ = P(œà(X) , Y) .
(9.11)
1 Inspired by (Soriƒá, 1989), in a seminal paper (Benjamini and Hochberg, 1995), Benjamini and
Hochberg coined the term ‚Äúfalse discovery rate‚Äù, and they deÔ¨Åned that as the expectation of (9.9).
The use of fdr here can be interpreted similar to the deÔ¨Ånition of (tail area) false discovery rate by
Efron in (Efron, 2007).

9.1 Model Evaluation
253
However, because misclassiÔ¨Åcation can occur for both instances of P and N class, Œµ
can be equivalently written as
Œµ = f pr √ó P(Y = N) + f nr √ó P(Y = P),
(9.12)
where P(Y = i) is the prior probability of class i ‚àà{P, N}. The empirical estimate of
Œµ, denoted ÀÜŒµ, is obtained by using empirical estimates of f pr and f nr given in (9.5)
and (9.7), respectively, as well as the empirical estimates of P(Y = P) and P(Y = N),
which under a random sampling assumption, are nP
m and nN
m , respectively; that is,
ÀÜŒµ = nFP
nN
√ó nN
m + nFN
nP
√ó nP
m = nFP + nFN
m
‚â°
nFP + nFN
nFP + nFN + nTP + nTN
,
(9.13)
which is simply the proportion of misclassiÔ¨Åed observations (c.f. Section 4.9). An
equivalent simple form to write ÀÜŒµ directly as a function of yj and ÀÜyj is
ÀÜŒµ = 1
m
m
√ï
j=1
I{yj,ÀÜyj } ,
(9.14)
where IA is the indicator of event A.
Accuracy: This is the default metric used in the score method of classiÔ¨Åers in
scikit-learn, and is deÔ¨Åned as the probability of correct classiÔ¨Åcation; that is to say,
acc = 1 ‚àíŒµ = P(œà(X) = Y) .
(9.15)
Similarly, (9.15) can be written as
acc = tnr √ó P(Y = N) + tpr √ó P(Y = P) .
(9.16)
Replacing tnr, tpr, P(Y = N), and P(Y = P) by their empirical estimates yields the
accuracy estimate, denoted
ÀÜ
acc, and is given by
ÀÜ
acc = nTN
nN
√ó nN
m + nTP
nP
√ó nP
m = nTN + nTP
m
‚â°
nTP + nTN
nFP + nFN + nTP + nTN
,
(9.17)
which is simply the proportion of correctly classiÔ¨Åed observations. This can be
equivalently written as
ÀÜ
acc = 1
m
m
√ï
j=1
I{yj=ÀÜyj } .
(9.18)
Although accuracy has an intuitive meaning, it could be misleading in cases
where the data is imbalanced; that is, in datasets where the proportion of samples
from various classes are diÔ¨Äerent. For this reason, it is common to report other metrics

254
9 Model Evaluation and Selection
of performance along with the accuracy. The following example demonstrates the
situation.
Example 9.1 Consider the following scenario in which we have three classiÔ¨Åers
to classify whether the topic of a given news article is politics (P) or not (N).
We train three classiÔ¨Åers, namely, ClassiÔ¨Åer 1, ClassiÔ¨Åer 2, and ClassiÔ¨Åer 3, and
evaluate them on a test sample Ste,100 that contains 5 political and 95 non-political
articles. The results of this evaluation for these classiÔ¨Åers are summarized as follows:
ClassiÔ¨Åer 1: this classiÔ¨Åer classiÔ¨Åes all observations to N class.
ClassiÔ¨Åer 2: nTP = 4 and nTN = 91.
ClassiÔ¨Åer 3: nTP = 1 and nTN = 95.
Which classiÔ¨Åer is better to use?
To answer this question, we Ô¨Årst look into the accuracy estimate. However, before
doing so, we Ô¨Årst obtain all elements of the confusion matrix for each classiÔ¨Åer.
Since nP = nTP + nFN, nN = nTN + nFP, from the given information we have:
ClassiÔ¨Åer 1: nTP = 0, nTN = 95, nFP = 0, and nFN = 5.
ClassiÔ¨Åer 2: nTP = 4, nTN = 91, nFP = 4, and nFN = 1.
ClassiÔ¨Åer 3: nTP = 1, nTN = 95, nFP = 0, and nFN = 4.
This means that
ÀÜ
acc ClassiÔ¨Åer 1 =
0 + 95
0 + 95 + 0 + 5 = 0.95,
ÀÜ
acc ClassiÔ¨Åer 2 =
4 + 91
4 + 91 + 4 + 1 = 0.95,
ÀÜ
acc ClassiÔ¨Åer 3 =
1 + 94
1 + 94 + 1 + 4 = 0.95 .
In terms of accuracy, all classiÔ¨Åers show a high accuracy estimate of 95%. How-
ever, ClassiÔ¨Åer 1 seems quite worthless in practice as it classiÔ¨Åes any news article to
N class! On the other hand, ClassiÔ¨Åer 2 seems quite eÔ¨Äective because it can retrieve
four out of the Ô¨Åve P instances in Ste,100, and in doing so it is fairly precise because
among the eight articles that it labels as P, four of them is actually correct (nTP = 4
and nFP = 4). As a result, it is inappropriate to judge the eÔ¨Äectiveness of these
classiÔ¨Åers based on the accuracy alone. Next we examine the (empirical estimates
of) recall (tpr) and precision (ppv) for these classiÔ¨Åer:

9.1 Model Evaluation
255
ÀÜ
tprClassiÔ¨Åer 1 =
0
0 + 5 = 0,
ÀÜ
ppvClassiÔ¨Åer 1 =
0
0 + 0 ,
(undeÔ¨Åned)
ÀÜ
tprClassiÔ¨Åer 2 =
4
4 + 1 = 0.8,
ÀÜ
ppvClassiÔ¨Åer 2 =
4
4 + 4 = 0.5,
ÀÜ
tprClassiÔ¨Åer 3 =
1
1 + 4 = 0.2,
ÀÜ
ppvClassiÔ¨Åer 3 =
1
1 + 0 = 1 .
As seen here, in estimating the ppv of ClassiÔ¨Åer 1, we encounter division by 0. In
these situations, one may dismiss the worth of the classiÔ¨Åer or, if desired, deÔ¨Åne the
estimate as 0 in order to possibly calculate other metrics of performance that are
based on this estimate (e.g., F1 score). Nonetheless, here we perceive ClassiÔ¨Åer 1
as worthless and focus on comparing ClassiÔ¨Åer 2 and ClassiÔ¨Åer 3. As can be seen,
ClassiÔ¨Åer 2 has a higher ÀÜ
tpr than ClassiÔ¨Åer 3 but its
ÀÜ
ppv is lower. How can we decide
which classiÔ¨Åer to use in practice based on these two metrics?
Depending on the application, sometimes we care more about precision and
sometimes we care more about the recall. For example, if we have a cancer diagnostic
classiÔ¨Åcation problem, we would prefer having a higher recall because it means
having a classiÔ¨Åer that can better detect individuals with cancer‚Äîthe cost of not
detecting a cancerous patient could be death. However, suppose we would like to
train a classiÔ¨Åer that classiÔ¨Åes an email as spam (positive) or not (negative) and based
on the assigned label it directs the email to the spam folder, which is not often seen,
or to the inbox. In this application, it would be better to have a higher precision so
that ideally none of our important emails is classiÔ¨Åed as spam and left unseen in the
spam folder. If we have no preference between recall and precision, we can use the
F1 score estimate to summarize both metrics in one. The F1 scores for ClassiÔ¨Åer 2
and ClassiÔ¨Åer 3 are
ÀÜF1,ClassiÔ¨Åer 2 = 2 √ó 0.8 √ó 0.5/(0.8 + 0.5) = 0.615,
ÀÜF1,ClassiÔ¨Åer 3 = 2 √ó 1 √ó 0.2/(1 + 0.2) = 0.333,
which shows ClassiÔ¨Åer 2 is the preferred classiÔ¨Åer. ‚ñ†
Area Under the Receiver Operating Characteristic Curve (ROC AUC): All the
aforementioned metrics of performance depend on t, which is the speciÔ¨Åc value
of threshold used in a classiÔ¨Åer. However, depending on the context that a trained
classiÔ¨Åer will be used in the future, we may decide to change this threshold at the Ô¨Ånal
stage of classiÔ¨Åcation, which is a fairly simple adjustment, for example, to balance
recall and precision depending on the context. Then the question is whether we can
evaluate a classiÔ¨Åer over the entire range of the threshold used in its structure rather

256
9 Model Evaluation and Selection
than a speciÔ¨Åc value of the threshold. To do so we can evaluate the classiÔ¨Åer by the
Receiver Operating Characteristic (ROC) curve and the Area Under the ROC Curve
(ROC AUC).
ROC curve is obtained by varying t over its entire range and plotting pairs of
( f pr, tpr) where f pr and tpr are values on the horizontal and vertical axes for
each t, respectively. Recall that given tpr and f pr, we also have f nr and tnr, which
means ROC curve is in fact a concise way to summarize the information in these four
metrics. A fundamental property of the ROC curve is that it is a monotnic increasing
function from (0, 0) to (1, 1). This is because as t ‚Üí‚àû, P s(X) > t|X ‚ààP and
P s(X) > t|X ‚ààN approach zero. At the same time, these probabilities naturally
increase for a decreasing t. In the extreme case, when t ‚Üí‚àí‚àû, both of these
probabilities approach one.
Let p s(X)|X ‚ààP and p s(X)|X ‚ààN denote the probability density function
(pdf) of scores given by a classiÔ¨Åer to observations of class P and N, respectively.
Therefore, tpr and f pr, which are indeed functions of t, can be written as
tpr =
‚à´‚àû
t
p s(X)|X ‚ààPds(X),
f pr =
‚à´‚àû
t
p s(X)|X ‚ààNds(X) .
Intuitively, a classiÔ¨Åcation rule that produces larger scores for the P class than the N
class is more eÔ¨Äective than the one producing similar scores for both classes. In other
words, the more p s(X)|X ‚ààP and p s(X)|X ‚ààN diÔ¨Äer, the less likely in general
the classiÔ¨Åer misclassiÔ¨Åes observations. In the extreme case where p s(X)|X ‚ààP =
p s(X)|X ‚ààN, then P s(X) > t|X ‚ààP = P s(X) > t|X ‚ààN, ‚àÄt. This situation
corresponds to random classiÔ¨Åcation and the ROC curve becomes the straight line
connecting (0, 0) to (1, 1) (the solid line in Fig. 9.7). In this case, the area under the
ROC curve (ROC AUC) is 0.5.
Now suppose there is a t0 such that P s(X) > t0|X ‚ààP = 1 and P s(X) > t0|X ‚àà
N = 0. This means that the entire masses of p s(X)|X ‚ààP and p s(X)|X ‚ààN
pdfs are on (t0, ‚àû) and (‚àí‚àû, t0] intervals, respectively. This can also be interpreted
as perfect classiÔ¨Åcation. This situation leads to the point (0, 1) for the ROC curve.
At the same time, it is readily seen that as t increases in (t0, ‚àû), f pr remains 0 but
tpr decreases until it approaches 0. On the other hand, for decreasing t in the range
(‚àí‚àû, t0], tpr remains 1 while f pr increases until it approaches 1. This is the dashed
ROC curve in Fig. 9.7. In this case, the ROC AUC is 1.
To estimate the ROC curve, we can Ô¨Årst rank observations in a sample based on
their scores given by the classiÔ¨Åer, and then changing the threshold of classiÔ¨Åer from
‚àûto ‚àí‚àû, and plot a curve of tpr against f pr. Once the ROC curve is generated, the
ROC AUC is basically the area under the ROC curve. It can be shown that the ROC
AUC is equivalent to the probability that a randomly chosen member of the positive
class is ranked higher (the score is larger) than a randomly chosen member of the
negative class. This probabilistic interpretation of ROC AUC leads to an eÔ¨Écient
way to estimate the ROC AUC (denoted as
ÀÜ
auc) as follows (Hand and Till, 2001):

9.1 Model Evaluation
257
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.2
0.4
0.6
0.8
1
 ùë° ‚Üí‚àû
ùë°ùëùùëü
ùëìùëùùëü
 ùë° ‚Üí‚àí‚àû
 ùë°= ùë°!
Fig. 9.7: The ROC curve for random (solid) and perfect (dashed) classiÔ¨Åcation.
ÀÜ
auc = RN ‚àí0.5 nN(nN + 1)
nNnP
,
(9.19)
where RN is the sum of ranks of observations from the N class. Given nP and nN, the
maximum of
ÀÜ
auc is achieved for maximum RN, which happens when the maximum
score given to any observation from the N class is lower than the minimum score
given to any observation from the P class. In other words, there exists a threshold
t that ‚Äúperfectly‚Äù discriminates instances of class P from class N. In this case,
RN = √çnN
i=1(nP + i) = nPnN + 0.5 nN(nN + 1), which leads to
ÀÜ
auc = 1.
Example 9.2 The following table shows the scores given to 10 observations by a
classiÔ¨Åer. The table also shows the true class of each observations and their ranks
based on the given scores. We would like to generate the ROC curve and also Ô¨Ånd
the
ÀÜ
auc.

258
9 Model Evaluation and Selection
rank true class score
1
P
0.95
2
P
0.80
3
P
0.75
4
N
0.65
5
N
0.60
6
P
0.40
7
N
0.30
8
P
0.25
9
N
0.20
10
N
0.10
The ROC curve is shown in Fig. 9.8. As we lower down the threshold, we mark
some of the thresholds that lead to the points on the curve.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.2
0.4
0.6
0.8
1
ùë°= 0.95
 ùë° ‚Üí‚àû
ùë°= 0.80
ùë°= 0.65
ùë°= 0.30
ùë°ùëùùëü
$
ùëìùëùùëü
&
 ùë° ‚Üí‚àí‚àû
Fig. 9.8: The ROC curve for Example 9.2.
The area under the ROC curve is
ÀÜ
auc = 0.6 √ó 0.4 + 0.8 √ó 0.2 + 1 √ó 0.4 = 0.8.
This can also be obtained from (9.19) without plotting the ROC curve. From the
table, we have RN = 4 + 5 + 7 + 9 + 10 = 35, nN = 5, and nP = 5. Therefore,
ÀÜ
auc = 35‚àí0.5(5√ó6)
25
= 0.8. ‚ñ†
Extension to Multiclass ClassiÔ¨Åcation: Extending accuracy or error rate to mul-
ticlass classiÔ¨Åcation is straightforward. They can be deÔ¨Åned as before by the prob-
ability of correct or incorrect classiÔ¨Åcation. As their deÔ¨Ånition is symmetric across
all classes, their empirical estimates can also be obtained by counting the number

9.1 Model Evaluation
259
of correctly/incorrectly classiÔ¨Åed observations divided by the total number of ob-
servations. Other metrics presented for binary classiÔ¨Åcation can be also extended to
multiclass classiÔ¨Åcation (c > 2 classes) by some sort of averaging over a series of
one-versus-rest (OvR), also known one-versus-all (OvA), classiÔ¨Åcation problems. In
this regard, one successively perceives each class as P and all the rest as N, obtain
the metric of performance for the binary classiÔ¨Åcation problem, and then obtain
the average metric across all binary problems. Here, we present the averaging rules
in connection with the empirical estimate of tpr. Other metrics can be obtained
similarly.
Micro averaging: In this strategy, TP, FP, TN, and FN are Ô¨Årst counted over for
all c binary problems. Let nTPi, nFPi, nTNi, and nFNi denote each of these counts
for a binary classiÔ¨Åcation problem in which class i = 0, 1, . . ., c ‚àí1 is considered P.
Then,
ÀÜ
tprmicro =
√çc‚àí1
i=0 nTPi
√çc‚àí1
i=0 mi
=
√çc‚àí1
i=0 nTPi
√çc‚àí1
i=0 (nTPi + nFNi)
.
(9.20)
where mi is the number of observations in class i.
Macro averaging: In this strategy, the metric is calculated for each binary clas-
siÔ¨Åcation problem and then combined by
ÀÜ
tprmacro = 1
c
c‚àí1
√ï
i=0
ÀÜ
tpri ,
(9.21)
where
ÀÜ
tpri is the
ÀÜ
tpr for the binary classiÔ¨Åcation problem where class i =
0, 1, . . ., c ‚àí1 is considered P. As can be seen, in this averaging classes are given the
same weight regardless of the class-speciÔ¨Åc sample size.
Weighted averaging: Here, each class-speciÔ¨Åc metric is weighted by the class
sample size. Therefore,
ÀÜ
tprweighted =
c‚àí1
√ï
i=0
mi
√çc‚àí1
i=0 mi
ÀÜ
tpri =
c‚àí1
√ï
i=0
nTPi + nFNi
√çc‚àí1
i=0 (nTPi + nFNi)
ÀÜ
tpri .
(9.22)
Scikit-learn implementation of evaluation metrics of classiÔ¨Åers: In scikit-learn,
metrics are implemented as diÔ¨Äerent classes in sklearn.metrics module. A metric
such as accuracy and confusion matrix can be calculated using the actual and the
predicted labels for the sample. Therefore, to implement them, we only need to
import their classes and pass these two sets of labels as arguments:
‚Ä¢ For accuracy: use accuracy_score(y_true, y_pred)
‚Ä¢ For confusion matrix: use confusion_matrix(y_true, y_pred)

260
9 Model Evaluation and Selection
Other metrics such as recall, precision, and F1 score are deÔ¨Åned for the positive
class (in case of multiclass, as mentioned before, OvR can be used to successively
treat each class as positive). Therefore, to use their scikit-learn classes, we should
pass actual labels and predicted labels, and can pass the label of the positive class by
pos_label parameter (in case of binary classiÔ¨Åcation, pos_label=1 by default):
‚Ä¢ For recall: use recall_score(y_true, y_pred)
‚Ä¢ For precision: use precison_score(y_true, y_pred)
‚Ä¢ For F1 score: use f1_score(y_true, y_pred)
For multiclass classiÔ¨Åcation, recall_score, precison_score, and f1_score
support micro, macro, and weighted averaging. To choose a speciÔ¨Åc averaging, the
average parameter can be set to either 'micro', 'macro', or 'weighted'.
Last but not least, to compute the ROC AUC, we need the actual labels and the
given scores. ClassiÔ¨Åers in scikit-learn have either decision_function method,
predict_proba, or both. The decision_function returns scores given to obser-
vations; for example, for linear classiÔ¨Åers a score for an observation is proportional
to the signed distance of that observation from the decision boundary, which is a
hyperplane. As the result, the output is just a 1-dimensional array. On the other hand,
predict_proba method is available when the classiÔ¨Åer can estimate the probability
of a given observation belonging to a class. Therefore, the output is a 2-dimensional
array of size, sample size √ó number of classes, where sum of each row over columns
becomes naturally 1. For ROC AUC calculation, we can use:
‚Ä¢ roc_auc_score(y_true, y_score)
y_score: for binary classiÔ¨Åcation, this could be the output of decision_function
or predict_proba. In the multiclass case, it should be a probability array of shape
sample size √ó number of classes. Therefore, it could be output of predict_proba
only.
For multiclass classiÔ¨Åcation, roc_auc_score supports macro, weighted, and
micro averaging (the latter in scikit-learn version 1.2.0). This is done by setting the
average to either 'macro', 'weighted', or 'micro'.
Example 9.3 Suppose we have the following actual and predicted labels identiÔ¨Åed
by y_true and y_pred, respectively,
y_true = [2, 3, 3, 3, 2, 2, 2]
y_pred = [3, 2, 2, 3, 2, 2, 2]
where label 2 is the positive class. In that case, nTP = 3, nFP = 2 and from (9.8),
precision=0.6. We can compute this metrics as follows (note that we explicitly set
pos_label=2; otherwise, as this is a binary classiÔ¨Åcation problem by default it uses
pos_label=1, which is not a valid label and leads to error):
sklearn.metrics import precision_score
y_true = [2, 3, 3, 3, 2, 2, 2]

9.1 Model Evaluation
261
y_pred = [3, 2, 2, 3, 2, 2, 2]
precision_score(y_true, y_pred, pos_label=2)
0.6
Example 9.4 In the following example, we estimate several metrics in the Iris clas-
siÔ¨Åcation problem both for the macro averaging and for each class individually (i.e.,
when each class is considered as the positive class):
import numpy as np
from sklearn import datasets
from sklearn.metrics import accuracy_score, confusion_matrix,‚ê£
,‚Üírecall_score, precision_score, f1_score, roc_auc_score,‚ê£
,‚Üíclassification_report
from sklearn.linear_model import LogisticRegression as LRR
iris = datasets.load_iris()
X_train = iris.data
y_train = iris.target
X_train, X_test, y_train, y_test= train_test_split(iris.data, iris.
,‚Üítarget, random_state=100, test_size=0.2, stratify=iris.target)
print('X_train_shape: ' + str(X_train.shape) + '\nX_test_shape: ' +‚ê£
,‚Üístr(X_test.shape)\
+ '\ny_train_shape: ' + str(y_train.shape) + '\ny_test_shape: '‚ê£
,‚Üí+ str(y_test.shape) + '\n')
lrr = LRR(C=0.1)
y_test_pred = lrr.fit(X_train, y_train).predict(X_test)
print("Accuracy = {:.3f}".format(accuracy_score(y_test, y_test_pred)))
print("Confusion Matrix is\n {}".format(confusion_matrix(y_test,‚ê£
,‚Üíy_test_pred)))
print("Macro Average Recall = {:.3f}".format(recall_score(y_test,‚ê£
,‚Üíy_test_pred, average='macro')))
print("Macro Average Precision = {:.3f}".format(precision_score(y_test,‚ê£
,‚Üíy_test_pred, average='macro')))
print("Macro Average F1 score = {:.3f}".format(f1_score(y_test,‚ê£
,‚Üíy_test_pred, average='macro')))
print("Macro Average ROC AUC = {:.3f}".format(roc_auc_score(y_test, lrr.
,‚Üípredict_proba(X_test), multi_class='ovr', average='macro')))
# set "average" to "None" to print these metrics for each class‚ê£
,‚Üíindividually (when the class is considered positive)
np.set_printoptions(precision=3)
print("Class-specific Recall = {}".format(recall_score(y_test,‚ê£
,‚Üíy_test_pred, average=None)))
print("Class-specific Precision = {}".format(precision_score(y_test,‚ê£
,‚Üíy_test_pred, average=None)))

262
9 Model Evaluation and Selection
print("Class-specific F1 score = {}".format(f1_score(y_test,‚ê£
,‚Üíy_test_pred, average=None)))
print(classification_report(y_test, y_test_pred)) #‚ê£
,‚Üíclassification_report is used to print them out nicely
X_train_shape: (120, 4)
X_test_shape: (30, 4)
y_train_shape: (120,)
y_test_shape: (30,)
Accuracy = 0.967
Confusion Matrix is
[[10
0
0]
[ 0 10
0]
[ 0
1
9]]
Macro Average Recall = 0.967
Macro Average Precision = 0.970
Macro Average F1 score = 0.967
Macro Average ROC AUC = 0.997
Class-specific Recall = [1.
1.
0.9]
Class-specific Precision = [1.
0.909 1.
]
Class-specific F1 score = [1.
0.952 0.947]
precision
recall
f1-score
support
0
1.00
1.00
1.00
10
1
0.91
1.00
0.95
10
2
1.00
0.90
0.95
10
accuracy
0.97
30
macro avg
0.97
0.97
0.97
30
weighted avg
0.97
0.97
0.97
30
‚ñ†
In order to use one of the aforementioned metrics along with cross-validation, we
can set the scoring parameter of cross_val_score to a string that represents the
metric. In this regard, legitimate strings are listed at (Scikit-metrics, 2023) and can
be seen as follows as well:
from sklearn.metrics import SCORERS
SCORERS.keys()

9.1 Model Evaluation
263
dict_keys(['explained_variance', 'r2', 'max_error',‚ê£
,‚Üí'neg_median_absolute_error', 'neg_mean_absolute_error',‚ê£
,‚Üí'neg_mean_absolute_percentage_error', 'neg_mean_squared_error',‚ê£
,‚Üí'neg_mean_squared_log_error', 'neg_root_mean_squared_error',‚ê£
,‚Üí'neg_mean_poisson_deviance', 'neg_mean_gamma_deviance', 'accuracy',‚ê£
,‚Üí'top_k_accuracy', 'roc_auc', 'roc_auc_ovr', 'roc_auc_ovo',‚ê£
,‚Üí'roc_auc_ovr_weighted', 'roc_auc_ovo_weighted', 'balanced_accuracy',‚ê£
,‚Üí'average_precision', 'neg_log_loss', 'neg_brier_score',‚ê£
,‚Üí'adjusted_rand_score', 'rand_score', 'homogeneity_score',‚ê£
,‚Üí'completeness_score', 'v_measure_score', 'mutual_info_score',‚ê£
,‚Üí'adjusted_mutual_info_score', 'normalized_mutual_info_score',‚ê£
,‚Üí'fowlkes_mallows_score', 'precision', 'precision_macro',‚ê£
,‚Üí'precision_micro', 'precision_samples', 'precision_weighted',‚ê£
,‚Üí'recall', 'recall_macro', 'recall_micro', 'recall_samples',‚ê£
,‚Üí'recall_weighted', 'f1', 'f1_macro', 'f1_micro', 'f1_samples',‚ê£
,‚Üí'f1_weighted', 'jaccard', 'jaccard_macro', 'jaccard_micro',‚ê£
,‚Üí'jaccard_samples', 'jaccard_weighted'])
For example, here we use 'roc_auc_ovr' to calculate stratiÔ¨Åed 3-fold cross-
validation (with shuÔ¨Ñing) ROC AUC in a OvR fashion for Iris classiÔ¨Åcation dataset:
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import StratifiedKFold
K_fold = 3
strkfold = StratifiedKFold(n_splits=K_fold, shuffle=True,‚ê£
,‚Üírandom_state=42)
lrr = LRR(C=0.1)
cv_scores = cross_val_score(lrr, X_train, y_train, cv=strkfold,‚ê£
,‚Üíscoring='roc_auc_ovr')
cv_scores
array([0.98324515, 0.99905033, 0.99810066])
Scikit-learn implementation to monitor multiple evaluation metrics in cross-
validation: In evaluating a predictive model, it is often desired to monitor multiple
evaluation metrics. Insofar as cross-validation rule is concerned, cross_val_score
does not allow us to monitor multiple metrics through its scoring parameter.
To monitor several scoring metrics, we can use cross_validate class from
sklearn.model_selection module. Then one way to compute various metrics is
to set the scoring parameter of cross_validate to a list of strings that represents
metrics of interest, for example,
cross_validate(..., scoring=['accuracy', 'roc_auc'])
However, a more Ô¨Çexible way is to deÔ¨Åne a function with signature
arbitrary_name(estimator, X, y)
that returns a dictionary containing multiple scores and pass that through the
scoring parameter of cross_validate (see the following example).

264
9 Model Evaluation and Selection
Example 9.5 Here we would like to use cross-validation to evaluate and record the
performance of logistic regression in classifying two classes in Iris classiÔ¨Åcation
dataset. As the performance metrics, we use the accuracy, the ROC AUC, and the
confusion matrix.
import pandas as pd
import numpy as np
from sklearn import datasets
from sklearn.metrics import roc_auc_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import cross_validate
from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import LogisticRegression as LRR
def accuracy_roc_confusion_matrix(clf, X, y):
y_pred = clf.predict(X)
acc = accuracy_score(y, y_pred)
mat = confusion_matrix(y, y_pred)
y_score = clf.decision_function(X)
auc = roc_auc_score(y, y_score)
return {'accuracy': acc,
'roc_auc': auc,
'tn': mat[0, 0],
'fp': mat[0, 1],
'fn': mat[1, 0],
'tp': mat[1, 1]}
iris = datasets.load_iris()
X_train = iris.data[iris.target!=0,:]
y_train = iris.target[iris.target!=0]
print('X_train_shape: ' + str(X_train.shape) + '\ny_train_shape: ' +‚ê£
,‚Üístr(y_train.shape) + '\n')
lrr = LRR()
strkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_validate(lrr, X_train, y_train, cv=strkfold,‚ê£
,‚Üíscoring=accuracy_roc_confusion_matrix)
df = pd.DataFrame(cv_scores)
display(df)
print("the overall 5-fold CV accuracy is: {:.3f}".
,‚Üíformat(cv_scores['test_accuracy'].mean()))
print("the overall 5-fold CV roc auc is: {:.3f}".
,‚Üíformat(cv_scores['test_roc_auc'].mean()))
print("the aggregated confusion matrix is:\n {}".format(np.array(df.
,‚Üísum(axis=0)[4:],dtype='int_').reshape(2,2)))

9.1 Model Evaluation
265
X_train_shape: (100, 4)
y_train_shape: (100,)
fit_time
score_time
test_accuracy
test_roc_auc
test_tn
test_fp
\
0
0.014002
0.004842
0.90
1.00
8
2
1
0.006391
0.001709
1.00
1.00
10
0
2
0.008805
0.002197
0.90
0.96
9
1
3
0.010306
0.001830
0.95
1.00
10
0
4
0.008560
0.002558
1.00
1.00
10
0
test_fn
test_tp
0
0
10
1
0
10
2
1
9
3
1
9
4
0
10
the overall 5-fold CV accuracy is: 0.950
the overall 5-fold CV roc auc is: 0.992
the aggregated confusion matrix is:
[[47
3]
[ 2 48]]
9.1.3 Evaluation Metrics for Regressors
Let f (X) be a trained regressor, which estimates Y for a given realization of X. To
discuss the empirical estimates of evaluation metrics of f (X), we assume the trained
regressor is applied to a sample of size m, Ste,m = {(x1, y1), (x2, y2), . . ., (xm, ym)}
and estimates the target values of x1, . . ., xm as ÀÜy1, . . ., ÀÜym, respectively. Our aim is
to estimate the predictive ability of the regressor. We discuss three metrics and their
estimators, namely, MSE, MAE, and R2, also known as coeÔ¨Écient of determination.
Mean Square Error (MSE): The MSE of f (X) is deÔ¨Åned as
MSE = EY,X
h Y ‚àíf (X)2i
,
(9.23)
where the expectation is with respect to the joint distribution of Y and X. The
empirical estimate of MSE, denoted
ÀÜ
MSE, is given by
ÀÜ
MSE = 1
m
m
√ï
i=1
(yi ‚àíÀÜyi)2 .
(9.24)
The summation is also known as the residual sum of squares (RSS), which we have
seen previously in Section 5.2.2; that is,

266
9 Model Evaluation and Selection
RSS =
m
√ï
i=1
(yi ‚àíÀÜyi)2 .
(9.25)
In other words,
ÀÜ
MSE is the normalized RSS with respect to the sample size.
Mean Absolute Error (MAE): To deÔ¨Åne the MAE, the squared loss in MSE is
replaced with the absolute loss; that is to say,
MAE = EY,X
hY ‚àíf (X)

i
.
(9.26)
The estimate of MAE, denoted
ÀÜ
MAE, is obtained as
ÀÜ
MAE = 1
m
m
√ï
i=1
|yi ‚àíÀÜyi| .
(9.27)
R2 (coeÔ¨Écient of determination): R2 is perhaps the most common metric used
for regressor evaluation to the extent that is used as the metric of choice in score
method of any regressor in scikit-learn. Formally, (for the squared loss) it is given
by (Dougherty et al., 2000):
R2 =
E
h Y ‚àíE[Y]2i
‚àíEY,X
h Y ‚àíf (X)2i
E
h Y ‚àíE[Y]2i
.
(9.28)
As E[Y] is the optimal constant estimator of Y, R2 measured the relative decrease
in error by using regressor f (X) instead of using the trivial constant regressor E[Y].
Plug-in estimator of R2, denoted ÀÜR2, is obtained by replacing EY,X
h Y ‚àíf (X)2i
and E
h Y ‚àíE[Y]2i
with their empirical estimates, which yields,
ÀÜR2 = 1 ‚àíRSS
TSS ,
(9.29)
where RSS is given in (9.25) and
TSS =
m
√ï
i=1
(yi ‚àí¬Øy)2 ,
(9.30)
where ¬Øy = 1
m
√çm
i=1 yi. Some points to remember:
‚Ä¢ R2 is a scoring function (the higher, the better), while MSE and MAE are loss
functions (the lower, the better).
‚Ä¢ R2 is a relative measure, while MSE and MAE are absolute measures; that is,
given a value of MSE, it is not possible to judge whether the model has a good

9.2 Model Selection
267
performance or not unless extra information on the scale of the target values is
given. This is also seen from (9.24). If we change the scale of target values and
their estimates by multiplying yi and ÀÜyi by a constant c,
ÀÜ
MSE is multiplied by
c2. In contrast, this change of scale does not have an impact on the ÀÜR2.
Scikit-learn implementation of evaluation metrics of regressors: Similar to clas-
siÔ¨Åcation metrics, regression metrics are also deÔ¨Åned in sklearn.metrics module:
‚Ä¢ For ÀÜR2: use r2_score(y_true, y_pred)
‚Ä¢ For
ÀÜ
MSE: use mean_squared_error(y_true, y_pred)
‚Ä¢ For
ÀÜ
MAE: use mean_absolute_error(y_true, y_pred)
Similar to classiÔ¨Åcation metrics, we can estimate these metrics with cross-
validation by setting the scoring parameter of cross_val_score to the corre-
sponding string that can be found from SCORERS.keys() (also listed at (Scikit-
metrics, 2023)). In this regard, an important point to remember is that cross-
validation implementation in scikit-learn accepts a scoring function, and not a
loss function. However, as we said, MSE and MAE are loss functions. That is the
reason that legitimate ‚Äúscorers‚Äù retrieved by SCORERS.keys() include, for exam-
ple, 'neg_mean_squared_error' and 'neg_mean_absolute_error'. Setting
the scoring parameter of cross_val_score to 'neg_mean_squared_error'
internally multiplies the MSE by -1 to obtain a score (the higher, the better). There-
fore, if needed, we can use 'neg_mean_squared_error' and then multiply the
returned cross-validation score by -1 to obtain the MSE loss, which is non-negative.
9.2 Model Selection
The model selection stage is a crucial step towards constructing successful predictive
models. Oftentimes, practitioners are given a dataset and are asked to construct a
classiÔ¨Åer or regressor. In such a situation, the practitioner has the option to choose
from a plethora of models and algorithms. The practices that the practitioner follows
to select one Ô¨Ånal trained model that can be used in practice from such a vast number
of candidates are categorized under model selection.
From the perspective of the three-stage design process discussed in Section 1.1.6,
the model selection stage includes the initial candidate set selection (Step 1) and
selecting the Ô¨Ånal mapping (Step 3). For example, it is a common practice to initially
narrow down the number of plausible models based on
‚Ä¢ practitioner‚Äôs prior experience with diÔ¨Äerent models and their sample size-
dimensionality requirements;
‚Ä¢ the nature of data at hand (for example, having time-series, images, or other
types); and
‚Ä¢ if available, based on prior knowledge about the Ô¨Åeld of study from which the
data has been collected. This stage ipso facto is part of model selection, albeit
based on prior knowledge.

268
9 Model Evaluation and Selection
After this stage, the practitioner is usually faced with one or a few learning
algorithms. However, each of these learning algorithms generally has one or a few
hyperparameters that should be estimated. We have already seen hyperparameters
in the previous sections. These are parameters that are not estimated/given by the
learning algorithm using which a classiÔ¨Åer/regressor is trained but, at the same time,
can signiÔ¨Åcantly change the performance of the predictive model. For example, k in
kNN (Chapter 5), C or ŒΩ in various forms of regularized logistic regression (Section
6.2.2), minimum sample per leaf, maximum depth, or even the choice of impurity
criterion in CART (Section 7.2.2 and 7.2.4), or the number of estimators or the choice
of base estimators and all its hyperparameters in using AdaBoost (Section 8.6.1).
Training a Ô¨Ånal model involves using a learning algorithm as well as estimating its
hyperparameters‚Äîa process, which is known as hyperparameter tuning and is part
of model selection. Two common data-driven techniques for model selection are grid
search and random search.
9.2.1 Grid Search
In grid search, we Ô¨Årst deÔ¨Åne a space of hyperparameters for a learning algorithm Œ®
and conduct an exhaustive search within this space to determine the ‚Äúbest‚Äù values of
hyperparameters. In this regard,
1) for continous-valued hyperparameters we assume and discretize a range depend-
ing on our prior experiences/knowledge and/or computational capabilities and
resources;
2) assume a range for discrete/categorical hyperparameters;
3) deÔ¨Åne the discrete hyperparameter search space (denoted by H D
Œ® ) as the Carte-
sian product of sets of values deÔ¨Åned for each hyperparameter; and
4) for each point in H D
Œ® , estimate the performance of the corresponding model and
pick the hyperparameter values that lead to the best performance.
The aforementioned procedure can be extended to as many as learning algorithms
to determine the best combination of learning algorithm-hyperparameter values.
Example 9.6 Consider AdaBoost.SAMME algorithm presented in Section 8.6.1.
Suppose based on prior knowledge and experience, we decide to merely use CART
as the base classiÔ¨Åer. In that case, one possible choice is to treat Œ∑ (the learning
rate), M (the maximum number of base classiÔ¨Åers), the minimum samples per leaf
(denoted s), and the choice of impurity criterion as hyperparameters. Based on our
prior experience we decide:
Œ∑ ‚àà{0.1, 1, 10}, M ‚àà{10, 50}, s ‚àà{5, 10}, criterion ‚àà{gini, entropy}
and use the default values of scikit-learn as for the values of other hyperparameters.
Determine HŒ® and its cardinality.

9.2 Model Selection
269
H D
Œ® = {0.1, 1, 10} √ó {10, 50} √ó {5, 10} √ó {gini, entropy} =
{(0.1, 10, 5, gini), (1, 10, 5, gini), (10, 10, 5, gini), . . . ., (10, 50, 10, entropy)}.
Therefore, |H D
Œ® | = 3 √ó 2 √ó 2 √ó 2 = 24, where | . | denotes the cardinality. ‚ñ†
Suppose after initial inspection (Step 1 of the design process), T learning al-
gorithms, denoted Œ®i, i = 1, . . .,T, have been chosen. For each Œ®i, we specify a
discrete hyperparameter space H D
Œ®i. Depending on the estimation rules that are used
to estimate the metric of performance, there are two types of grid search that are
commonly used in practice:
‚Ä¢ grid search using validation set
‚Ä¢ grid search using cross-validation
Hereafter, we denote a predictive model (classiÔ¨Åer/regressor) constructed by ap-
plying the learning algorithm Œ®i with a speciÔ¨Åc set of hyperparameters Œ∏ ‚ààH D
Œ®i on
a training sample S1 by œàŒ∏, i(S1). Furthermore, let Œ∑œàŒ∏, i(S1)(S2) denote the value of a
scoring (the higher, the better) metric Œ∑ obtained by evaluating œàŒ∏, i(S1) on a sample
S2.
Grid search using validation set: In our discussions in the previous chapters, we
have generally split a given sample into a training and a test set to train and evaluate
an estimator, respectively. To take into account the model selection stage, a common
strategy is to split the data into three sets known as training, validation, and test sets.
The extra set, namely, the validation set, is then used for model selection. In this
regard, one can Ô¨Årst split the entire data into two smaller sets, for example, with a
size of 75% and 25% of the given sample, and use the smaller set as test set. The
larger set (i.e., the set of size 75%) is then split into a ‚Äútraining set‚Äù and a ‚Äúvalidation
set‚Äù (we can use again the same ratio of 75% and 25% to generate the training and
validation sets).
Let Str, Sva, and Ste denote the training, validation, and test sets, respectively.
Furthermore, let Œ®‚àó
Œ∏,va denote the optimal learning algorithm-hyperparameter values
estimated using the grid search on validation set. We have
Œ®‚àó
Œ∏,va =
argmax
Œ®i, i=1,2,...,T
Œ∑œàŒ∏‚àó
i, i(Str )(Sva),
(9.31)
where
Œ∏‚àó
i = argmax
Œ∏‚ààHD
Œ®i
Œ∑œàŒ∏, i(Str )(Sva),
i = 1, 2, . . .,T .
(9.32)
Although (9.31) and (9.32) can be combined in one equation, we have divided
them into two equations to better describe the underlying selection process. Let
us elaborate a bit on these equations. In (9.32), Œ∏‚àó
i , which is the estimate of the
optimal values of hyperparameter for each learning algorithm Œ®i, i = 1, 2, . . .,T, is
obtained. In this regard, for each Œ∏ ‚ààH D
Œ®i, Œ®i is trained on Str to construct the
classiÔ¨Åer œàŒ∏, i(Str). Then this model is evaluated on the validation set Sva to obtain

270
9 Model Evaluation and Selection
Œ∑œàŒ∏, i(Str )(Sva). To estimate the optimal values of hyperparameters for Œ®i, we then
compare all scores obtained for each Œ∏ ‚ààH D
Œ®i. At the end of (9.32), we have T
sets of hyperparameter values Œ∏‚àó
i (one for each Œ®i). The optimal learning algorithm-
hyperparameter values is then obtained by (9.31) where the best scores of diÔ¨Äerent
learning algorithms are compared and the one with maximum score is chosen. There
is no need to compute any new score in (9.31) because all required scores used in
the comparison have been already computed in (9.32).
Once Œ®‚àó
Œ∏,va is determined from (9.31), we can apply it to Str to train our ‚ÄúÔ¨Ånal‚Äù
model. Nevertheless, this model has already been trained and used in (9.31), which
means that we can just pick it if it has been stored. Alternatively, we can combine Str
and Sva to build a larger set (Str ‚à™Sva) that is used in training one Ô¨Ånal model. In
this approach, the Ô¨Ånal model is trained by applying the estimated optimal learning
algorithm-hyperparameter values Œ®‚àó
Œ∏,va to Str ‚à™Sva. There is nothing wrong with
this strategy (as a matter of fact this approach more closely resembles the grid search
using cross-validation that we will see next). We just try to utilize the full power of
available data in training. With a ‚ÄúsuÔ¨Écient‚Äù sample size and well-behaved learning
algorithms, we should not generally see a worse performance.
Once the Ô¨Ånal model is selected/trained, it can be applied on Ste for evaluation.
Some remarks about this grid search on validation set:
‚Ä¢ Sva simulates the eÔ¨Äect of having test data within Str ‚à™Sva. This means that
if in the process of model selection a preprocessing stage is desired, it should
be applied to Str, and Sva
 similar to Ste
 should be normalized using statistics
found based on Str. If we want to train the Ô¨Ånal model on Str ‚à™Sva, another
normalization should be applied to Str ‚à™Sva and then Ste should be normalized
bases on statistics found on Str ‚à™Sva.
‚Ä¢ One potential problem associated with grid search using validation set is that the
estimate of the optimal learning algorithm-hyperparameter values obtained from
(9.31) could be biased to the single sample chosen for validation set. Therefore,
if major probabilistic diÔ¨Äerences exist between the single validation set from the
test data, the Ô¨Ånal constructed model may not generalize well and shows poor
performance on unseen data. This problem is naturally more pronounced in small
sample where the model selection could become completely biased towards the
speciÔ¨Åc observations that are in validation set. Therefore, this method of model
selection is generally used in large sample.
Grid search using cross-validation: In grid search using cross-validation, we look
for the combination of the learning algorithm-hyperparameter values that leads to
the highest score evaluated using cross-validation on Str. For a K-fold CV, this is
formally represented as:
Œ®‚àó
Œ∏,cv =
argmax
Œ®i, i=1,2,...,T
Œ∑K‚àíf old
œàŒ∏‚àó
i, i
,
(9.33)
where

9.2 Model Selection
271
Œ∏‚àó
i = argmax
Œ∏‚ààHD
Œ®i
Œ∑K‚àíf old
œàŒ∏, i
,
i = 1, 2, . . .,T ,
(9.34)
and where
Œ∑K‚àíf old
œàŒ∏, i
= 1
K
K
√ï
k=1
Œ∑œàŒ∏, i(Str ‚àíFoldk)(Foldk),
(9.35)
in which Foldk denote the set of observations that are in the kth fold of CV. Once
Œ®‚àó
Œ∏,cv is determined, a Ô¨Ånal model is then trained using the estimated optimal learning
algorithm-hyperparameter values Œ®‚àó
Œ∏,cv on Str. Once this Ô¨Ånal model is trained, it
can be applied on Ste for evaluation.
Scikit-learn implementation of grid search model selection: To implement
the grid search using cross-validation, we can naturally write a code that goes
over the search space and use cross_val_score to compute the score for
each point in the grid. Nevertheless, scikit-learn has a convenient way to imple-
ment the grid search. For this purpose, we can use GridSearchCV class from
sklearn.model_selection module. A few important parameters of this class
are:
1) estimator: this is the object from the estimator class;
2) param_grid: this could be either a dictionary to deÔ¨Åne a grid or a list of
dictionaries to deÔ¨Åne multiple grids. Either way, the keys in a dictionary should
be parameters of the estimator that are used in model selection. The possible
values of each key used in the search are passed as a list of values for that key.
For example, the following code deÔ¨Ånes two grids to search for hyperparameter
tuning in logistic regression:
grids = [{'penalty': ['l2'], 'C': [0.01, 0.1]},
{'penalty': ['elasticnet'], 'C': [0.01, 0.1], 'l1_ratio':[0.2,‚ê£
,‚Üí0.8], 'solver':['saga']}]
3) cv: determines the CV strategy (similar to the same parameter used in
cross_val_score; See Section 9.1.1 for more details)
4) scoring: in its most common use case, we only pass the name of a scoring
function (a string). However, as discussed before for cross_validate, multiple
metrics can also be used by a list of strings or a callable function returning a
dictionary. If multiple metrics are used here, then the refit method should be
set to the metric that is used to Ô¨Ånd the best hyperparameter values for training
the Ô¨Ånal estimator (see next).
5) refit: by default this parameter is True. This means once all scores are deter-
mined, then it trains (‚ÄúreÔ¨Åt‚Äù) an estimator using the best found combination of
hyperparameters (led to the highest CV score) on the entire dataset. If multiple

272
9 Model Evaluation and Selection
metrics are used in scoring, this parameter should be a string that identiÔ¨Åes
the metric used for reÔ¨Åtting.
Once an instance of class GridSearchCV is created, we can treat it as a regular
classiÔ¨Åer/regressor as it implements fit, predict, score and several other meth-
ods. Here fit trains an estimator for each given combination of hyperparameters.
The predict and score methods can be used for prediction and evaluation using
the best found estimator, respectively. Nevertheless, an object from GridSearchCV
class has also some important attributes such as:
1) cv_results_: it is a dictionary containing the scores obtained on each fold for
each hyperparameter combination as well as some other information regrading
training (e.g., training time);
2) best_params_: the combination of hyperparameters that led to the highest
score;
3) best_score_: the highest score obtained on the given grids; and
4) best_estimator_: not available if refit=False; otherwise, this attributes
gives access to the estimator trained on the entire dataset using the best combi-
nation of hyperparameters‚Äîpredict and score methods of GridSearchCV
use this estimator as well.
Example 9.7 Let us consider the Iris classiÔ¨Åcation dataset again. We wish to use
GridSearchCV class to tune hyperparameters of logistic regression over a hyper-
parameter space deÔ¨Åned using the same grids as speciÔ¨Åed in the previous page. We
monitor both the accuracy and the weighted ROC AUC during the cross validation
but we use the accuracy for hyperparameter tuning. Once the hyperparameter tuning
is completed, we also Ô¨Ånd the test accuracy (accuracy on the test data) for the best
model trained on the entire training data using the hyperparameter combination that
led to the highest CV accuracy. As for the cross-validation, we use stratiÔ¨Åed 3-fold
CV with shuÔ¨Ñing data before splitting (all these tuning, training, and testing steps
can be implemented in a few lines of codes using scikit-learn!)
import pandas as pd
from sklearn import datasets
from sklearn.linear_model import LogisticRegression as LRR
from sklearn.model_selection import GridSearchCV, StratifiedKFold,‚ê£
,‚Üítrain_test_split
iris = datasets.load_iris()
X_train = iris.data
y_train = iris.target
X_train, X_test, y_train, y_test= train_test_split(iris.data, iris.
,‚Üítarget, random_state=100, test_size=0.2, stratify=iris.target)
print('X_train_shape: ' + str(X_train.shape) + '\nX_test_shape: ' +‚ê£
,‚Üístr(X_test.shape)\
+ '\ny_train_shape: ' + str(y_train.shape) + '\ny_test_shape: '‚ê£
,‚Üí+ str(y_test.shape) + '\n')
lrr = LRR(max_iter=2000)

9.2 Model Selection
273
grids = [{'penalty': ['l2'], 'C': [0.01, 0.1]},
{'penalty': ['elasticnet'], 'C': [0.01, 0.1], 'l1_ratio':[0.2,‚ê£
,‚Üí0.8], 'solver':['saga']}]
strkfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
gscv = GridSearchCV(lrr, grids, cv=strkfold, n_jobs=-1, scoring =‚ê£
,‚Üí['accuracy', 'roc_auc_ovr_weighted'], refit='accuracy')
score_best_estimator=gscv.fit(X_train, y_train).score(X_test, y_test)
print('the highest score is: {:.3f}'.format(gscv.best_score_))
print('the best hyperparameter combination is: {}'.format(gscv.
,‚Üíbest_params_))
print('the accuracy of the best estimator on the test data is: {:.3f}'.
,‚Üíformat(score_best_estimator))
df = pd.DataFrame(gscv.cv_results_)
df
X_train_shape: (120, 4)
X_test_shape: (30, 4)
y_train_shape: (120,)
y_test_shape: (30,)
the highest score is: 0.950
the best hyperparameter combination is: {'C': 0.1, 'l1_ratio': 0.2,‚ê£
,‚Üí'penalty':
'elasticnet', 'solver': 'saga'}
the accuracy of the best estimator on the test data is: 0.967
mean_fit_time
std_fit_time
mean_score_time
std_score_time param_C ‚ê£
,‚Üí\
0
0.418733
0.000479
0.003294
0.000478
0.01
1
0.428889
0.001791
0.003241
0.000199
0.1
2
0.003677
0.000962
0.004429
0.000792
0.01
3
0.001299
0.000221
0.002921
0.000350
0.01
4
0.016373
0.001285
0.003531
0.001409
0.1
5
0.014243
0.003561
0.005710
0.000220
0.1
param_penalty param_l1_ratio param_solver
\
0
l2
NaN
NaN
1
l2
NaN
NaN
2
elasticnet
0.2
saga
3
elasticnet
0.8
saga
4
elasticnet
0.2
saga
5
elasticnet
0.8
saga
[the rest of the output is omitted for brevity]
‚ñ†
To implement the grid-search using validation set, we can Ô¨Årst use the function
train_test_split from sklearn.model_selection to set aside the test set,

274
9 Model Evaluation and Selection
if desired, and then use it once again to set aside the validation set. The indices
of the training and validation sets can be then created by using some cross valida-
tion generator such as PredefinedSplit class from sklearn.model_selection
and used as cv parameter of GridSearchCV, which accepts generators. To use
PredefinedSplit, we can set indices of validation set to an integer such as 0 but
the indices of the training set should be set to -1 to be excluded from validation set
(see the following example).
Example 9.8 In this example, we use the same data, classiÔ¨Åer, and hyperparameter
search space as in Example 9.7; however, instead of grid search cross-validation, we
use validation set.
import pandas as pd
from sklearn import datasets
from sklearn.linear_model import LogisticRegression as LRR
from sklearn.model_selection import GridSearchCV, StratifiedKFold,‚ê£
,‚Üítrain_test_split, PredefinedSplit
iris = datasets.load_iris()
X, X_test, y, y_test= train_test_split(iris.data, iris.target,‚ê£
,‚Üírandom_state=100, stratify=iris.target)
indecis = np.arange(len(X))
X_train, X_val, y_train, y_val, indecis_train, indecis_val=‚ê£
,‚Üítrain_test_split(X, y, indecis, random_state=100, stratify=y)
ps_ind = np.zeros(len(indecis), dtype='int_')
ps_ind[indecis_train] = -1 # training set indices are set to -1 and‚ê£
,‚Üívalidation set indices are left as 0
print('indices used in CV splitter:\n', ps_ind)
pds = PredefinedSplit(ps_ind)
print('the split used as training and validation sets:\n', *pds.split())
lrr = LRR(max_iter=2000)
grids = [{'penalty': ['l2'], 'C': [0.01, 0.1]},
{'penalty': ['elasticnet'], 'C': [0.01, 0.1], 'l1_ratio':[0.2,‚ê£
,‚Üí0.8], 'solver':['saga']}]
gscv = GridSearchCV(lrr, grids, cv=pds, n_jobs=-1, scoring =‚ê£
,‚Üí['accuracy', 'roc_auc_ovr_weighted'], refit='accuracy')
score_best_estimator=gscv.fit(X, y).score(X_test, y_test) #gscv is fit‚ê£
,‚Üíon X = X_train+X_val
print('the highest score is: {:.3f}'.format(gscv.best_score_))
print('the best hyperparameter combination is: {}'.format(gscv.
,‚Üíbest_params_))
print('the accuracy of the best estimator on the test data is: {:.3f}'.
,‚Üíformat(score_best_estimator))
df = pd.DataFrame(gscv.cv_results_)
df
indices used in CV splitter:
[-1 -1 -1 -1 -1 -1 -1
0
0
0 -1 -1
0 -1 -1 -1 -1
0 -1 -1 -1
0 -1 -1
-1 -1 -1 -1 -1
0 -1 -1 -1 -1 -1 -1
0
0 -1 -1
0 -1
0 -1
0 -1 -1 -1

9.2 Model Selection
275
0 -1
0 -1
0 -1 -1 -1
0 -1 -1 -1
0 -1 -1
0 -1 -1 -1 -1
0 -1
0 -1
0 -1
0
0 -1 -1 -1
0 -1
0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
0 -1 -1
-1 -1 -1 -1 -1 -1
0 -1
0 -1 -1 -1 -1 -1 -1 -1]
the split used as training and validation sets:
(array([
0,
1,
2,
3,
4,
5,
6,
10,
11,
13,
14,
15,
16,
18,
19,
20,
22,
23,
24,
25,
26,
27,
28,
30,
31,
32,
33,
34,
35,
38,
39,
41,
43,
45,
46,
47,
49,
51,
53,
54,
55,
57,
58,
59,
61,
62,
64,
65,
66,
67,
69,
71,
73,
76,
77,
78,
80,
82,
83,
84,
85,
86,
87,
88,
89,
90,
91,
92,
94,
95,
96,
97,
98,
99, 100, 101, 103, 105,
106, 107, 108, 109, 110, 111]), array([
7,
8,
9,
12,
17, ‚ê£
,‚Üí21,
29,
36,
37,
40,
42,
44,
48,
50,
52,
56,
60,
63,
68,
70,
72,
74,
75,
79,
81,
93,
102, 104]))
the highest score is: 0.893
the best hyperparameter combination is: {'C': 0.1, 'l1_ratio': 0.8,‚ê£
,‚Üí'penalty':
'elasticnet', 'solver': 'saga'}
the accuracy of the best estimator on the test data is: 0.974
mean_fit_time
std_fit_time
mean_score_time
std_score_time param_C ‚ê£
,‚Üí\
0
0.223788
0.0
0.002445
0.0
0.01
1
0.015926
0.0
0.002789
0.0
0.1
2
0.002857
0.0
0.002804
0.0
0.01
3
0.001344
0.0
0.003534
0.0
0.01
4
0.016076
0.0
0.002317
0.0
0.1
5
0.009496
0.0
0.002699
0.0
0.1
param_penalty param_l1_ratio param_solver
\
0
l2
NaN
NaN
1
l2
NaN
NaN
2
elasticnet
0.2
saga
3
elasticnet
0.8
saga
4
elasticnet
0.2
saga
5
elasticnet
0.8
saga
[the rest of the output is omitted for brevity]
‚ñ†
9.2.2 Random Search
Naturally, for a speciÔ¨Åc Œ®, if, 1) H D
Œ® that is used in the grid search contains the
optimal set of hyperparameter values; 2) the estimation rule can accurately estimate

276
9 Model Evaluation and Selection
the actual performance; and 3) the computational requirement is aÔ¨Äordable, it is
quite likely that the grid search can lead to the optimal values for hyperparameters.
However, in practice, none of these is guaranteed: 1) we are generally blind to
the optimal values of hyperparameters for a speciÔ¨Åc application; 2) depending on
the estimation rule and available sample size for evaluation, we constantly deal
with a bias-variance-computation trilemma for the estimation rules; and 3) for large
|H D
Œ® | (i.e., a large number of hyperparamter values to evaluate), the computational
requirements for grid search could become a burden. As a result, an alternative
common technique for hyperparameter tuning is random search. In this strategy,
one randomly chooses and examines part of H D
Œ® or, alternatively, randomly samples
points for evaluation from a predeÔ¨Åned subregion of the entire hyperparameter space.
It has been shown that this strategy can lead to models that are as good or even better
than the grid search (Bergstra and Bengio, 2012).
Scikit-learn implementation of random search model selection: For this pur-
pose, we can use RandomizedSearchCV class from sklearn.model_selection
module. Many parameters in RandomizedSearchCV behave similarly to those
of GridSearchCV. That being said, rather than param_grid in GridSearchCV,
in RandomizedSearchCV we have param_distributions, which similar to
param_grid can be either a dictionary or a list of dictionaries. Nonetheless, possible
values of a key in each dictionary here can even include distributions. Distributions
that can be used in RandomizedSearchCV are arbitrary as long as they provide an
rvs() method for sampling such as those found in scipy.stats (scipy, 2023).
Furthermore, the number of times that param_distributions is sampled, is de-
termined by n_iter parameter.
Two distributions that often come in handy for hyperparameter tuning are uniform
and log-uniform (also known as reciprocal) distributions implemented by uniform
and loguniform from scipy.stats, respectively. While a random variable is
uniformly distributed in a range [a, b] if its probability density function is constant
in that range, and 0 otherwise, a random variable is log-uniformly distributed if
its logarithm is uniformly distributed in [log(a), log(b)]. This is useful because
sometimes we know the lower limit of a parameter but we do not have an upper
bound for the parameter (e.g., C in logistic regression). In these situations, if a
discrete grid is desired, it is common to take values that grow exponentially; for
example, 0.01, 0.1, 1, 10, 100, . . .. This is because a Ô¨Åxed change in a small values of
hyperparameter could aÔ¨Äect the results more than the same change for large values.
In the same situation, if rather than a discrete grid, a range is desired, the use of
log-uniform distribution ensures the sampling is uniform across intervals such as
[0.01, 0.1], [0.1, 1], [1, 10], . . ..
Example 9.9 In this example, we use the same data, classiÔ¨Åer, and hyperparame-
ter search space as in Example 9.7; however, instead of GridSearchCV, we use
RandomizedSearchCV. As in Example 9.7, we consider both l1 and l2 penalty.
However, C is assumed to be log-uniformly distributed between 0.01 and 100, and ŒΩ
(deÔ¨Åned in Section 6.2.2) is assumed to be uniformly distributed between 0 and 1.

9.2 Model Selection
277
import pandas as pd
from sklearn import datasets
from sklearn.linear_model import LogisticRegression as LRR
from sklearn.model_selection import RandomizedSearchCV,‚ê£
,‚ÜíStratifiedKFold, train_test_split
from scipy.stats import loguniform, uniform
iris = datasets.load_iris()
X_train = iris.data
y_train = iris.target
X_train, X_test, y_train, y_test= train_test_split(iris.data, iris.
,‚Üítarget, random_state=100, test_size=0.2, stratify=iris.target)
print('X_train_shape: ' + str(X_train.shape) + '\nX_test_shape: ' +‚ê£
,‚Üístr(X_test.shape)\
+ '\ny_train_shape: ' + str(y_train.shape) + '\ny_test_shape: '‚ê£
,‚Üí+ str(y_test.shape) + '\n')
lrr = LRR(max_iter=2000)
distrs = [{'penalty': ['l2'], 'C': loguniform(0.01, 100)},
{'penalty': ['elasticnet'], 'C': loguniform(0.01, 100),‚ê£
,‚Üí'l1_ratio':uniform(0, 1), 'solver':['saga']}]
strkfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
gscv = RandomizedSearchCV(lrr, distrs, cv=strkfold, n_jobs=-1, scoring‚ê£
,‚Üí= ['accuracy', 'roc_auc_ovr_weighted'], refit='accuracy', n_iter=10,‚ê£
,‚Üírandom_state=42)
score_best_estimator=gscv.fit(X_train, y_train).score(X_test, y_test)
print('the highest score is: {:.3f}'.format(gscv.best_score_))
print('the best hyperparameter combination is: {}'.format(gscv.
,‚Üíbest_params_))
print('the accuracy of the best estimator on the test data is: {:.3f}'.
,‚Üíformat(score_best_estimator))
df = pd.DataFrame(gscv.cv_results_)
df.iloc[:,4:9]
X_train_shape: (120, 4)
X_test_shape: (30, 4)
y_train_shape: (120,)
y_test_shape: (30,)
the highest score is: 0.983
the best hyperparameter combination is: {'C': 2.7964859516062437,‚ê£
,‚Üí'l1_ratio':
0.007066305219717406, 'penalty': 'elasticnet', 'solver': 'saga'}
the accuracy of the best estimator on the test data is: 0.967
param_C param_penalty param_l1_ratio param_solver
\
0
15.352247
l2
NaN
NaN
1
8.471801
l2
NaN
NaN

278
9 Model Evaluation and Selection
2
2.440061
l2
NaN
NaN
3
0.042071
elasticnet
0.058084
saga
4
0.216189
elasticnet
0.142867
saga
5
0.012088
l2
NaN
NaN
6
7.726718
elasticnet
0.938553
saga
7
0.05337
elasticnet
0.183405
saga
8
2.796486
elasticnet
0.007066
saga
9
0.14619
l2
NaN
NaN
params
0
{'C': 15.352246941973478, 'penalty': 'l2'}
1
{'C': 8.471801418819974, 'penalty': 'l2'}
2
{'C': 2.440060709081752, 'penalty': 'l2'}
3
{'C': 0.04207053950287936, 'l1_ratio': 0.05808...
4
{'C': 0.21618942406574426, 'l1_ratio': 0.14286...
5
{'C': 0.012087541473056955, 'penalty': 'l2'}
6
{'C': 7.726718477963427, 'l1_ratio': 0.9385527...
7
{'C': 0.05337032762603955, 'l1_ratio': 0.18340...
8
{'C': 2.7964859516062437, 'l1_ratio': 0.007066...
9
{'C': 0.14618962793704957, 'penalty': 'l2'}
Exercises:
Exercise 1: LoadtheCaliforniaHousingdataset, whichis partof sklearn.datasets
module. We wish to use all features to predict the target MedHouseVal and to do
so, we train and evaluate CART and AdaBoost (with CART as the estimator) us-
ing the standard 5-fold cross-validation with shuÔ¨Ñing. To create a CART regressor
anywhere that is needed, only set the minimum number of samples required at each
leaf to 10 and leave all other hyperparameters as default. For the AdaBoost, set
the number of regressors to 50 (n_estimators). In the cross-validation (KFold),
CART, and AdaBoost, set random_state=42. Compute and report the estimates
of the MAE and the R2 using CV for the CART and the AdaBoost. Which case is
showing the best performance in terms of ÀÜR2?
Exercise 2: Use the same dataset as in Exercise 1 but this time randomly split the
dataset to 80% training and 20% test. Use cross-validation grid search to Ô¨Ånd the best
set of hyperparameters for random forest to predict the target based on all features.
To deÔ¨Åne the space of hyperparameters of random forest assume:
1. max_features is 1/3;
2. min_samples_leaf ‚àà{2, 10};
3. n_estimators ‚àà{10, 50, 100}

9.2 Model Selection
279
In the grid search, use 10-fold CV with shuÔ¨Ñing and compute and report both
the estimates of both the MAE and R2. Then evaluate the model that has the highest
ÀÜR2 determined from CV on the held-out test data and record its test ÀÜR2.
Exercise 3: In this exercise, use the same data, classiÔ¨Åer, and hyperparameter search
space as in Example 9.9; however, instead of grid search cross-validation, use val-
idation set (for reproducibility, set the random_state in RandomizedSearchCV,
StratifiedKFold, and train_test_split to 42). Monitor and report both the
accuracy and the weighted ROC AUC on the validation set but use the accuracy for
hyperparameter tuning. Report the best estimated set of hyperparameters, its score
on validation-set, and the accuracy of the corresponding model on the test set.
Exercise 4: Suppose in a sample of 10 observations, there are 5 observations from
the P class and 5 from the N class. We have two classiÔ¨Åers, namely, classiÔ¨Åer A
and classiÔ¨Åer B, that classify an observation to P if the classiÔ¨Åer score given to the
observation is equal or greater than a threshold. We apply these classiÔ¨Åers to our
sample and based on the scores given to observations, we rank them from 1 (having
the highest score) to 10 (having the lowest score). Suppose ranks of observations
that belong to class P are:
For classiÔ¨Åer A: 1, 2, 5, 9, and 10.
For classiÔ¨Åer B: 1, 3, 5, 7, and 9.
Let
ÀÜ
aucA and
ÀÜ
aucB denote the ROC AUC estimate for classiÔ¨Åers A and B, respec-
tively. What is
ÀÜ
aucA ‚àí
ÀÜ
aucB?
A) -0.08
B) -0.06
C) 0
D) 0.06
E) 0.08
Exercise 5: Which of the following best describes the use of resubstitution estimation
rule in a regression problem to estimate R2?
A) We can not use resubstitution estimator for a regressor
B) We can use resubstitution estimator for a regressor but it is not a recommended
estimation rule
C) We can use resubstitution estimator for a regressor and, as a matter of fact, it is
the best estimation rule
Exercise 6: Which of the following evaluation rules can not be used for regression?
A) stratiÔ¨Åed 5-fold cross-validation
B) bootstrap
C) resubstitution
D) shuÔ¨Ñe and split
E) standard 5-fold cross-validation
F) leave-one-out

280
9 Model Evaluation and Selection
Exercise 7: Suppose we have the following training data. What is the leave-one-out
error estimate of the standard 3NN classiÔ¨Åcation rule based on this data?
class
A
A
A
B
B
B
observation 0.45 0.2 0.5 0.35 0.45 0.4
A) 2
6
B) 3
6
C) 4
6
D) 5
6
E) 6
6
Exercise 8: Suppose we have a classiÔ¨Åer that assigns to class positive (P) if the score
is larger than a threshold and to negative (N) otherwise. Applying this classiÔ¨Åer to
some test data results in the scores shown in the following table where ‚Äúclass‚Äù refers
to the actual class of an observation. What is the estimated AUC of this classiÔ¨Åer
based on this result?
observation 
#
class
score
observation 
#
class
score
1
P
0.25
6
P
0.35
2
P
0.45
7
N
0.38
3
N
0.48
8
N
0.3
4
N
0.4
9
P
0.5
5
N
0.33
10
N
0.2
A) 0.585
B) 0.625
C) 0.665
D) 0.755
D) 0.865
√â Exercise 9:
Suppose in a binary classiÔ¨Åcation problem, the class conditional densities are mul-
tivariate normal distributions with a common covariance matrix N(¬µP, Œ£) and
N(¬µN, Œ£), and prior probability of classes œÄP and œÄN. Show that for any linear
classiÔ¨Åer in the form of
œà(x) =
(
P
if aTx + b > T
N
otherwise,
(9.36)
where a and b determine the discriminant function, the optimal threshold, denoted,
Topt, that minimizes the error rate
Œµ = œÄN √ó f pr + œÄP √ó f nr ,
(9.37)

9.2 Model Selection
281
is
Topt = h(¬µN)+h(¬µP)
2
‚àí
g(Œ£) ln   œÄN
œÄP

h(¬µN)‚àíh(¬µP) .
(9.38)
where
h(¬µ) = aT ¬µ + b,
g(Œ£) = aT Œ£a .
(9.39)
Exercise 10: Suppose in a binary classiÔ¨Åcation problem, the estimate of the ROC
AUC for a speciÔ¨Åc classiÔ¨Åer on a test sample Ste is 1. Let nFP and nFN denote the
number of false positives and false negatives that are produced by applying this
classiÔ¨Åer to Ste, respectively, and
ÀÜ
acc denote its accuracy estimate based on Ste.
Which of the following cases is impossible?
A)
ÀÜ
acc = 1
B)
ÀÜ
acc < 1
C) nFP > 0 and nFN = 0
D) nFP > 0 and nFN > 0
Exercise 11: Use scikit-learn to produce the ROC curve shown in Example 9.2.
Hint: one way is to use the true labels (y_true) and the given scores (y_pred) as
inputs to sklearn.metrics.RocCurveDisplay.from_predictions(y_true,
y_pred). Another way is Ô¨Årst use the true labels and the given scores as inputs
to sklearn.metrics.roc_curve to compute the
ÀÜ
f pr, ÀÜ
tpr (we can also view the
thresholds at which they are computed). Then use the computed
ÀÜ
f pr and
ÀÜ
tpr as
the inputs to sklearn.metrics.RocCurveDisplay to create the roc curve display
object, and plot the curve by its plot() method.

Chapter 10
Feature Selection
Feature selection is a class of dimensionality reduction in which an ‚Äúimportant‚Äù
subset of features from a larger set of features is selected. A broader class of di-
mensionality reduction is feature extraction in which a set of candidate features are
transformed to a lower cardinality set of new features using some linear or non-
linear mathematical transformations. Feature selection can be seen as a special type
of feature extraction in which the physical nature of features are kept by the selec-
tion process. Consider the EEG classiÔ¨Åcation example that we have already seen
in the Ô¨Årst chapter. There, feature selection (based on prior knowledge) was used
to select the C1 channel among the 64 available channels in the dataset. However,
later we used a signal modeling technique to extract informative features including
frequencies and amplitudes of some sinusoids from EEG recordings‚Äîthis practice
was feature extraction. In contrast with the widely used context-free search strategies
for feature selection, feature extraction is generally domain dependent; that is to say,
the success of commonly used transformations for feature extraction highly depends
on the form of data and the context. For example, common transformations used in
classiÔ¨Åcation of time-series is generally diÔ¨Äerent from those used for tabular datasets.
Or even within time-series, some transformations are more common in a particular
application than others‚Äîfor example, Mel-frequency cepstral coeÔ¨Écients are some
types of features more commonly used for speech processing rather than EEG anal-
ysis. As a result, in this chapter, we focus on feature selection search strategies that
are context-free.
10.1 Dimensionality Reduction: Feature Selection and Extraction
The number of features in a given dataset is generally referred to as the dimensionality
of the feature vectors or the dataset. Generally, when we have a dataset with a
relatively large dimensionality, it is helpful to construct the Ô¨Ånal predictive model
in a reduced dimensionality space. The methods used to reduce the dimensionality
283
¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
A. Zollanvari, Machine Learning with Python, https://doi.org/10.1007/978-3-031-33342-2_10 

284
10 Feature Selection
of the original feature space are known as dimensionality reduction methods. There
are three main reasons why one should reduce dimensionality:
‚Ä¢ interpretability: it is often helpful to be able to see the the relationships between
the inputs (feature vector) and the outputs in a given problem. For example, what
biomarkers (features) can possibly classify a patient with bad prognosis from
another patient with good prognosis? There are two layers into interpretability
of a Ô¨Ånal constructed model. One layer is the internal structure of the predictive
model. For example, in Section 7.4, we observed that CART has a glass-box
structure that helps shed light on the input-output relationship. Another layer
is the input to the model per se. For example, if the original dataset has a
dimensionality of 20,000, using all features as the input to the CART would
impair interpretability regardless of its glass-box structure. In these situations,
it is helpful to focus on ‚Äúimportant‚Äù features that contribute to the prediction;
‚Ä¢ performance: in pattern recognition and machine learning literature peaking
phenomenon is often cited as a possible reason for dimensionality reduction.
For example, consider the following comments from some pioneers in the Ô¨Åeld:
‚ÄúFor training samples of Ô¨Ånite size, the performance of a given discriminant
rule in a frequentist framework does not keep on improving as the number p
of feature variables is increased. Rather, its overall unconditional error rate will
stop decreasing and start to increase as p is increased beyond a certain thresh-
old, depending on the particular situation‚Äù (McLachlan, 2004, p. 391). A more
thorough picture of this phenomenon is given in (Zollanvari et al., 2020); and
‚Ä¢ computational limitations: given a Ô¨Åxed sample size, a larger data dimension-
ality would imply a larger number of Ô¨Çoating point operations. Depending on
the available hardware, and storage/processing models, this could become the
bottleneck of the machine learning design process. These limitations along with
the peaking phenomenon are collectively known as the curse of dimensionality:
‚ÄúJust recall the curse of dimensionality that we often faced to get good error rates,
the number of training samples should be exponentially large in the number of
components. Also, computational and storage limitations may prohibit us from
working with many components‚Äù (Devroye et al., 1996, p. 561).
Here we create an experiment using synthetic data to demonstrate the eÔ¨Äect of
peaking phenomenon.
Example 10.1 Here we use make_classification from sklearn.datasets to
generate synthetic datasets. In this regard, a dataset containing 200 observations
from two 100-dimensional Gaussian distributions with independent features (100
observations from each class) is generated. Among the 100 features in the dataset,
the Ô¨Årst 10 features are informative and all other features are just random noise.
For each dataset, we train a CART and LDA by adding one feature at a time to
the classiÔ¨Åer, starting from informative features going into non-informative ones.
For each trained classiÔ¨Åer, we record its accuracy on a test dataset of size 800

10.1 Dimensionality Reduction: Feature Selection and Extraction
285
observations. This entire process is repeated 20 times to Ô¨Ånd the average accuracy
of each classiÔ¨Åer as a function of feature size.
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier as CART
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as‚ê£
,‚ÜíLDA
np.random.seed(42)
n_features = 100
mc_no = 20 # number of Monte Carlo simulation
acc_test = np.zeros((2, n_features)) # to record test accuracies for‚ê£
,‚Üíboth LDA and CART
cart = CART()
lda = LDA()
for j in np.arange(mc_no):
# by setting shuffle=False in make_classification, the informative‚ê£
,‚Üífeatures are put first
X, y = make_classification(n_samples=1000, n_features=n_features,‚ê£
,‚Üín_informative=10, n_redundant=0, n_repeated=0, n_classes=2,‚ê£
,‚Üín_clusters_per_class=1, class_sep=1, shuffle=False)
X_train_full, X_test_full, y_train, y_test = train_test_split(X, y,‚ê£
,‚Üístratify=y, train_size=0.2)
acc_test[0] += np.fromiter((cart.fit(X_train_full[:,:i], y_train).
,‚Üíscore(X_test_full[:,:i],y_test) for i in np.arange(1,‚ê£
,‚Üín_features+1)), float)
acc_test[1] += np.fromiter((lda.fit(X_train_full[:,:i], y_train).
,‚Üíscore(X_test_full[:,:i],y_test) for i in np.arange(1,‚ê£
,‚Üín_features+1)), float)
for i, j in enumerate(['CART', 'LDA']):
plt.figure(figsize=(4, 2.5), dpi = 150)
plt.plot(np.arange(1, n_features+1), acc_test[i]/mc_no, 'k')
plt.ylabel('test accuracy', fontsize='small')
plt.xlabel('number of selected features', fontsize='small')
plt.title('peaking phenomenon:' + j, fontsize='small')
plt.yticks(fontsize='small')
plt.xticks(np.arange(0, n_features+1, 10), fontsize='small')
plt.grid(True)
As we can see in Fig. 10.1, after all informative features are added, adding any
additional feature would be harmful to the average classiÔ¨Åer performance, which is
a manifestation of the peaking phenomenon. ‚ñ†

286
10 Feature Selection
0
10
20
30
40
50
60
70
80
90
100
number of selected features
0.6
0.7
0.8
avg. test accuracy
peaking phenomenon:CART
(a)
0
10
20
30
40
50
60
70
80
90
100
number of selected features
0.7
0.8
0.9
avg. test accuracy
peaking phenomenon:LDA
(b)
Fig. 10.1: Average test accuracy as a function of feature size: (a) CART; and (b)
LDA.
10.2 Feature Selection Techniques
As mentioned earlier, the goal in feature selection is to choose a subset of candidate
features that is important for prediction. The importance of a feature or a feature
subset could be judged either by prior knowledge or could be data-driven. Although
selecting features using prior knowledge is itself a legitimate feature selection strat-
egy, which naturally depends on the quality of the prior knowledge and is domain
dependent, we focus on objective criteria and methods that are data-driven. In this
regard, there are three classes of methods:
‚Ä¢ Filters
‚Ä¢ Wrappers
‚Ä¢ Embedded
10.2.1 Filter Methods
These methods select features based on general characteristics of data measured
independently of a learning algorithm. They are called Ô¨Ålters because the process
Ô¨Ålters out the original feature set to create the most ‚Äúpromising‚Äù subset before a
predictive model is trained. The advantages of using Ô¨Ålter methods include their
eÔ¨Éciency (fast execution) and their generality to various learning algorithms. The
latter property is attributed to the fact that Ô¨Ålter methods select features based on
certain characteristics of data rather than based on the interaction between a speciÔ¨Åc
learning algorithm and the data. As an example, in Section 5.2.2, we have already
seen the application of Pearson‚Äôs correlation coeÔ¨Écients in selecting features in
a regression problem. Other criteria include statistical tests, information-theoretic
measures, interclassdistance, estimateof the Bayes error,and correlationcoeÔ¨Écients,

10.2 Feature Selection Techniques
287
to name just a few criteria. Detailed descriptions of all aforementioned criteria are
beyond our scope. As a result, here we brieÔ¨Çy discuss the main idea behind Ô¨Ålter
methods using statistical tests‚Äîand even there our discussion is not meant to be a
complete guide on various aspects of statistical tests.
Statistical Tests: Statistical tests in feature selection are used to examine the ‚Äúim-
portance‚Äù of a feature by testing some properties of populations based on available
data‚Äîfor example, by testing the hypothesis that a single feature is unimportant in
distinguishing two or multiple classes from each other. To put it another way, the
deÔ¨Ånition of importance here depends on the type of hypothesis that we test. For
example, we may designate a feature as important if based on the given sample, the
hypothesis that there is no diÔ¨Äerence between the mean of class conditional densities
is rejected. If based on the statistic (i.e., a quantity derived from data) that is used in
the test (test statistic), we observe a compelling evidence against the null hypothesis,
denoted H0, we reject H0 in the favor of the alternative hypothesis Ha, and we
conclude the feature is potentially important; otherwise, we continue to believe in
H0. Therefore, in feature selection applications, H0 is generally the assertion that a
feature is unimportant and the alternative hypothesis Ha bears the burden of proof.
Formally, a test rejects a null hypothesis if its P-value (also known as the observed
signiÔ¨Åcance level) is less than a signiÔ¨Åcance level (usually denoted by Œ±), which is
generally set to 0.05. For example, we reject H0 if the P-value is 0.02, which is
less than Œ± = 0.05. The signiÔ¨Åcance level of a test is the probability of a false
positive occurring, which for our purpose means claiming a feature is important
(i.e., rejecting H0) if H0 is true (i.e., the feature is not actually important). The
probability of a false positive is deÔ¨Åned based on the assumption of repeating the
test on many samples of the same size collected from the population of interest; that
is to say, if H0 is true and the test procedure is repeated inÔ¨Ånitely many times on
diÔ¨Äerent samples of the same size from the population, the proportion of rejected
H0 is Œ±. A detailed treatment of P-value and how it is obtained in a test is beyond the
scope here (readers can consult statistical textbooks such as (Devore et al., 2013)),
but the lower the P-value, the greater the strength of observed test statistic against
the null hypothesis (more compelling evidence against the hypothesis that the feature
is unimportant).
Common statistical tests that can be used for identifying important features include
t-test, which is for binary classiÔ¨Åcation, and ANOVA (short for analysis of variance),
which is for both binary and multiclass classiÔ¨Åcation. In ANOVA, a signiÔ¨Åcant P-
value (i.e., a P-value that is less than Œ±) means the feature would be important
in classifying at least two of the classes. In ANOVA, a statistic is computed that
under certain assumptions follows F-distribution (Devore et al., 2013) (therefore,
the statistic is known as F-statistic and the statistical test is known as F-test).
Whether the computed F-statistic is signiÔ¨Åcant or not is based on having the P-value
less than Œ±. In regression, the most common statistical test to check the importance
of a feature in predicting the target is based on a test that determines whether the
Pearson‚Äôs correlation coeÔ¨Écient is statistically signiÔ¨Åcant. For this purpose, it also
conducts an F-test.

288
10 Feature Selection
It is worthwhile to remember that many common statistical tests only take into
account the eÔ¨Äect of one feature at a time regardless of its multivariate relationship
with other features. This is itself a major shortcoming of these tests because in
many situations features work collectively to result in an observed target value.
Nonetheless, insofar as statistical tests are concerned, it can be shown that when we
have many features, and thereof many statistical tests to examine the importance of
each feature at a time, the probability of having at least one false positive among
all tests, also known as family-wise error rate (FWER), is more than Œ± where Œ± is
the signiÔ¨Åcance level for each test (see Exercise 1). A straightforward approach to
control FWER is the well-known Bonferroni correction. Let m denote the number
of tests performed. In this approach, rather than rejecting a null hypothesis when
P-value ‚â§Œ± as it is performed in testing a single hypothesis, Bonferroni correction
involves rejecting the null hypotheses by comparing P-values to Œ±/m (Exercise 2).
Despite the ease of use, Bonferroni correction is a conservative approach; that is to
say, the strict threshold to reject a null hypothesis results in increasing the probability
of not rejecting the null hypothesis when it is false (i.e., increases the false negative
rate). There are more powerful approaches than Bonferroni. All these approaches
that are essentially used to control a metric of interest such as FWER, f pr or f dr
in multiple testing problem are collectively known as multiple testing corrections.
It would be instructive to see the detail algorithm (and later implementation) of at
least one of these corrections. As a result, without any proof, we describe a well-
known multiple testing correction due to Benjamini and Hochberg (Benjamini and
Hochberg, 1995).
Benjamini‚ÄìHochberg (BH) false discovery rate procedure: This procedure is for
controlling the expected (estimate of the) false discovery rate in multiple tests; that
is, to control (see Section 9.1.2 for the deÔ¨Ånition of f dr):
E[ ÀÜ
f dr] = E
h
nFP
nTP + nFP
i
.
(10.1)
Suppose we have p features and, therefore, p null hypothesis denoted by H0,1,
H0,2,. . . ,H0,p (each null hypothesis asserts that the corresponding feature is unim-
portant). Therefore, we conduct p statistical tests to identify the null hypotheses that
are rejected in the favor of their alternative hypotheses. The BH f dr procedure for
multiple test correction is presented in the following algorithm.

10.2 Feature Selection Techniques
289
Algorithm Benjamini‚ÄìHochberg False Discovery Rate Procedure
1) Set a desired maximum level, denoted Œ±, for the expectation of
ÀÜ
f dr as deÔ¨Åned
in (10.1); for example, we set Œ± = 0.01.
2) Sort P-values obtained from tests and denote them by P(1) ‚â§P(2) ‚â§. . . ‚â§P(p).
3) Let j denote the largest i such that P(i) ‚â§
i
p Œ±. To Ô¨Ånd j, start from the
largest P-value and check whether P(p) ‚â§
p
p Œ±; if not, then check whether
P(p‚àí1) ‚â§p‚àí1
p Œ±; if not, then check P(p‚àí2) ‚â§p‚àí2
p Œ±, and continue. Stop as soon as
the inequity is met‚Äîj is the index of the P(i) that meets this condition for the
Ô¨Årst time.
4) Reject the null hypothesis H0,(1), H0,(2), . . . , H0,(j). This means all P(i), i ‚â§j
(P-values smaller than P(j)) are also signiÔ¨Åcant even if P(i) ‚â§i
p Œ± is not met for
some of them.
Example 10.2 Suppose we have 10 features and run ANOVA test for each to identify
important features. The following P-values are obtained where Pi denotes the P-
value for feature i = 1, 2, . . ., 10. Apply BH procedure to determine important
features by an Œ± = 0.05.
P1 = 0.001, P2 = 0.1, P3 = 0.02, P4 = 0.5, P5 = 0.002 P6 = 0.005, P7 = 0.01,
P8 = 0.04, P9 = 0.06, P10 = 0.0001
Step 1: Œ± = 0.05, which is already given in the problem.
Step 2: Sort P-values: 0.0001 ‚â§0.001 ‚â§0.002 ‚â§0.005 ‚â§0.01 ‚â§0.02 ‚â§0.04 ‚â§
0.06 ‚â§0.1 ‚â§0.5.
Step 3:
is 0.5 ‚â§10
100.05? No.
is 0.1 ‚â§
9
100.05? No.
is 0.06 ‚â§
8
100.05? No.
is 0.04 ‚â§
7
100.05 = 0.035? No.
is 0.02 ‚â§
6
100.05 = 0.03? Yes.
Step 4: Reject any null hypothesis with a P-value less or equal to 0.02. That is, null
hypothesis corresponding to 0.0001, 0.001, 0.002, 0.005, and 0.01. This means the
indices of important features are 10, 1, 5, 6, 7, and 3. ‚ñ†
Using multiple tests correction, we select a number of features that are important
based on the P-values and what we try to control using the correction procedure.
This way, we generally do not know in advance how many features will be selected.
In other words, given an Œ± for the correction, which is a measure of the expected

290
10 Feature Selection
ÀÜ
f dr or FWER, we end up with a number of ‚Äúimportant‚Äù features that is unknown in
advance. Although leaving the number of selected features to the algorithm would
be desirable in many situations, there is another way with which we can impose the
number of features we wish to select for the Ô¨Ånal model training. In this approach,
we rank the P-values for all tests in an ascending order, and simply pick k features
with the least P-values (the most signiÔ¨Åcant P-values); that is to say, we pick the k
best individual features. The obvious drawback of this approach is that the number of
selected features are subjective. That being said, even the value of Œ± used in multiple
testing correction is subjective but values such as Œ± = 0.01 or Œ± = 0.05 are typically
acceptable and common to use.
Scikit-learn implementation of statistical tests for Ô¨Ålter feature selection:
Feature selection using statistical tests is implemented by diÔ¨Äerent classes in
sklearn.feature_selection module. For example, to check the signiÔ¨Åcance
of F-statistic computed as part of ANOVA in classiÔ¨Åcation, we can use f_classif
function. The F-test to check signiÔ¨Åcance of F-statistic computed from Pearson‚Äôs
correlation coeÔ¨Écient in regression is implemented in f_regression function.
The aforementioned feature selection Ô¨Ålter based on Benjamini‚ÄìHochberg proce-
dure for the ‚Äúfdr‚Äù correction is implemented using SelectFdr class. Filters based
on other corrections such as ‚Äúfpr‚Äù (false positive rate) or FWER are implemented
using SelectFpr and SelectFwe, respectively. Here, we present some examples to
further clarify the utility of these classes. By default SelectFdr, SelectFpr, and
SelectFwe use ANOVA test for classiÔ¨Åcation (i.e., f_classif), but this could be
changed so that these corrections can be used with other statistical tests for classiÔ¨Å-
cation or regression problems. For example, to use F-test for regression, we can use
SelectFdr(score_func=f_regression). To explicitly specify a number of fea-
tures to pick, SelectKBest class can be used. We can also use SelectPercentile
to keep a percentage of features. Again, by default, these classes use f_classif,
which implements ANOVA, and that could be changed depending on the required
tests.
Example 10.3 Here we wish to conduct ANOVA and then use BH procedure
to correct for multiple testing. For this purpose we use the Genomic dataset
GenomicData_orig.csv that was used in Exercise 1, Chapter 6. The data had
21050 features, and thereof that many statistical tests to determine the importance
of each feature based on F-statistic. For this purpose, we Ô¨Årst apply ANOVA (using
f_classif) and ‚Äúmanually‚Äù implement the procedure outlined above for the BH
correction. Then we will see that using SelectFdr, we can easily use ANOVA,
correct for multiple testing based on BH procedure, and at the same time, implement
the Ô¨Ålter (i.e., select our important features). Last but not least, SelectKBest is used
to pick the best 50 individual features based on ANOVA.
from sklearn.feature_selection import SelectFdr, f_classif, SelectKBest
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

10.2 Feature Selection Techniques
291
from sklearn.impute import SimpleImputer # to impute
from sklearn.preprocessing import StandardScaler # to scale
df = pd.read_csv('data/GenomicData_orig.csv')
df.head()
df = df.drop(df.columns[[0,1]], axis=1)
# preprocessing including imputation and standardization
np.random.seed(42)
y = df.ClassType
X = df.drop('ClassType', axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)
print('X shape = {}'.format(X_train.shape) + '\ny shape = {}'.
,‚Üíformat(y_train.shape))
print('X shape = {}'.format(X_test.shape) + '\ny shape = {}'.
,‚Üíformat(y_test.shape))
imp = SimpleImputer(strategy='median').fit(X_train)
X_train = imp.transform(X_train)
X_test = imp.transform(X_test)
scaler = StandardScaler().fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)
anova_res = f_classif(X_train, y_train) # f_classif returns one pair of‚ê£
,‚ÜíF statistic and P-value for each feature
pvalues = anova_res[1]
# check the number of P-values that are less than 0.01 significance‚ê£
,‚Üílevel
alpha=0.01
print("The number of important features based on F statistic (no‚ê£
,‚Üímultiple hypothesis correction):", sum(anova_res[1]<alpha))
pvalues_taken_before_BH = pvalues[pvalues < alpha]
print("The maximum p-value considered significant before BH correction‚ê£
,‚Üíis: {:.4f}".format(pvalues_taken_before_BH.max()))
# to implement BH procedure for the P-values obtained from ANOVA
pvalues_sorted = np.sort(anova_res[1])
pvalues_taken_using_BH = pvalues_sorted[pvalues_sorted <= np.arange(1,‚ê£
,‚ÜíX_train.shape[1]+1)*alpha/X_train.shape[1]]
print("The number of important features based on F statistic (after‚ê£
,‚Üímultiple hypothesis correction):", len(pvalues_taken_using_BH))
print("The maximum p-value considered significant after BH correction‚ê£
,‚Üíis: : {:.4f}".format(pvalues_taken_using_BH.max()))
# to use SelectFdr to use both the BH procedure and filter features
filter_fdr = SelectFdr(alpha=0.01).fit(X_train, y_train) # filter_fdr‚ê£
,‚Üíis a transformer using which we can transform the data

292
10 Feature Selection
print("Let's look at the p-values for the first 20 features\n",‚ê£
,‚Üífilter_fdr.pvalues_[0:20]) # using pvalues_ attributes to access p‚ê£
,‚Üívalues
X_train_filtered_BH = filter_fdr.transform(X_train) #‚ê£
,‚ÜíX_train_filtered_BH is our filtered data based on ANOVA and fdr‚ê£
,‚Üícorrection
print("The number of important features based on F statistic (after‚ê£
,‚Üímultiple hypothesis correction):", X_train_filtered_BH.shape[1])
X_test_filtered_BH = filter_fdr.transform(X_test)
print("The shape of test data (the number of features should match with‚ê£
,‚Üítraining): ", X_test_filtered_BH.shape)
# to use SelectKBest
filter_KBest = SelectKBest(k=50).fit(X_train, y_train)
X_train_filtered_KBest = filter_KBest.transform(X_train)
print("The number of important features based on F statistic (using‚ê£
,‚ÜíK=50 best individual feature):", X_train_filtered_KBest.shape[1])
X_test_filtered_KBest = filter_KBest.transform(X_test)
print("The shape of test data (the number of features should match with‚ê£
,‚Üítraining): ", X_test_filtered_KBest.shape)
X shape = (129, 21050)
y shape = (129,)
X shape = (43, 21050)
y shape = (43,)
The number of important features based on F statistic (no multiple‚ê£
,‚Üíhypothesis
correction): 7009
The maximum p-value considered significant before BH correction is: 0.0100
The number of important features based on F statistic (after multiple‚ê£
,‚Üíhypothesis
correction): 5716
The maximum p-value considered significant after BH correction is: : 0.
,‚Üí0027
Let's look at the p-values for the first 20 features
[1.26230819e-06 2.57999668e-04 7.20697614e-01 4.61019523e-01
1.44329139e-04 7.73450170e-07 7.88695806e-01 5.38203614e-01
6.12687071e-01 6.08016364e-01 7.42496319e-10 7.22040641e-01
6.38454543e-01 9.08196717e-03 5.42029503e-17 8.64539548e-02
8.26923725e-09 3.88193129e-06 8.46980459e-02 3.57175908e-01]
The number of important features based on F statistic (after multiple‚ê£
,‚Üíhypothesis
correction): 5716
The shape of test data (the number of features should match with‚ê£
,‚Üítraining):
(43, 5716)
The number of important features based on F statistic (using K=50 best
individual feature): 50
The shape of test data (the number of features should match with‚ê£
,‚Üítraining):
(43, 50)

10.2 Feature Selection Techniques
293
10.2.2 Wrapper Methods
These methods wrap around a speciÔ¨Åc learning algorithm and evaluate the impor-
tance of a feature subset using the predictive performance of the learner trained on
the feature subset. The learning algorithm that is used for wrapper feature selection
does not necessarily need to be the same as the learning algorithm that is used to
construct the Ô¨Ånal predictive model; for example, one may use an eÔ¨Écient algorithm
such as decision trees or Naive Bayes for feature selection but a more sophisticated
algorithm to construct the Ô¨Ånal predictive model based on selected features.
Commonly used wrapper methods include a search strategy using which the space
of feature subsets is explored. Each time a candidate feature subset is selected for
evaluation based on the search strategy, a model is trained and evaluated using an
evaluation rule along with an evaluation metric. In this regard, cross-validation is
the most popular evaluation rules and either the accuracy, the ROC AUC, or the F1
score would be common metrics to use. At the end of the search process, the feature
subset with highest score is returned as the chosen feature subset. Although there are
many search strategies to use in this regard, here we discuss three approaches that
are straightforward to implement:
‚Ä¢ Exhaustive Search
‚Ä¢ Sequential Forward Search (SFS)
‚Ä¢ Sequential Backward Search (SBS)
Exhaustive Search: Suppose we wish to select k out of p candidate features that
are ‚Äúoptimal‚Äù based on a score such as the accuracy estimate. In this regard, one
can exhaustively search the space of all possible feature subsets of size k out of p.
This means that there are  p
k
 =
p!
k!(p‚àík)! feature subset to choose from. However,
this could be extremely large even for relatively modest values of p and k. For
example, for  50
20
 > 1013. In reality the situation is exacerbated because we do not
even know how many features should be selected. To put it another way, there is
generally no a priori evidence about any speciÔ¨Åc k. In other words, an exhaustive
search should be conducted for all possible values of k = 1, 2, . . ., p. This means
that to Ô¨Ånd the optimal feature subset, the number subset evaluation by an exhaustive
search is √çp
k=1
 p
k
 = 2p ‚àí1. Therefore, the number of feature subset to evaluate
grows exponentially and becomes quickly impractical as p grows‚Äîfor example for
p = 50, 1015 feature subsets should be evaluated! Although exhaustive search is
the optimal search strategy, the exponential complexity of the search space is the
main impediment in its implementation. As a result, we generally rely on suboptimal
search strategies as discussed next.
Sequential Forward Search (SFS): This is a greedy search strategy in which at
each iteration a single feature, which in combination with the current selected feature
subset leads to the highest score, is identiÔ¨Åed and added to the selected feature subset.
This type of search is known as greedy because the search does not revise the decision
made in previous iterations (i.e., does not revise the list of selected feature subsets
from previous iterations). Let fi, i = 1, . . ., p, and Ft denote the ith feature in a p-

294
10 Feature Selection
dimensional feature vector and a subset of these features that is selected at iteration
t, respectively. Formally, the algorithm is implemented in Algorithm ‚ÄúSequential
Forward Search‚Äù.
Algorithm Sequential Forward Search
1. Set t = 0 and F0 to empty set. Choose a learning algorithm for model training,
a scoring metric (e.g., accuracy), an evaluation rule (e.g., cross-vsalidatrion),
and a stopping criterion:
1.1. One stopping criterion could be the size of feature subsets to pick; for
example, one may decide to select k < p features.
1.2. Another stopping criterion that does not require specifying a predetermined
number of features to pick is to continue the search as long as there is a tangible
improvement in the score by adding one or a few features sequentially. For
example, one may decide to stop the search if after adding 3 features the score
is not improved more than a prespeciÔ¨Åed constant.
2. Find the
fi
<
Ft that maximizes the estimated scoring metric of the
model trained using Ft ‚à™fi. Denote that feature by f best
i
. In other words, f best
i
is the best univariate feature that along with features Ft leads to the highest score.
3. update Ft ‚ÜêFt ‚à™f best
i
and t ‚Üêt + 1.
4. Repeat steps 2 and 3 until the stopping criterion is met (e.g., based on the Ô¨Årst
stopping criterion, if |Ft| = k). Once the search is stopped, the best feature
subset is Ft.
Sequential Backward Search (SBS): This is another greedy search strategy in
which at each iteration a single feature is removed from the current selected feature
subset. The feature that is removed is the one that its removal leads to the highest
score. Formally, the algorithm is implemented in Algorithm ‚ÄúSequential Backward
Search‚Äù.
Remark: The computational complexity of SBS is sometimes an impediment in
its utility for feature selection. Suppose the set of candidate feature sets contains p
features and we wish to select p/2 features using either SFS or SBS. In this case,
it can be shown that the number of subsets that are evaluated in both SFS and SBS
are the same (Exercise 3). Other things equal, it can be shown that in this setting,
the computational complexity of SBS is generally much larger than SFS because
the models in SBS are trained in higher-dimensional spaces compared with models
trained in SFS.

10.2 Feature Selection Techniques
295
Algorithm Sequential Backward Search
1. Set t = 0 and F0 = { f1, f2, . . ., fp}. Choose a learning algorithm for model
training, a scoring metric, an evaluation rule, and a stopping criterion
similar to SFS (i.e., a predetermined number of features or removing fea-
tures as long as there is a tangible improvement in estimated performance score).
2. Find the fi ‚ààFt that maximizes the estimated scoring metric of the model
trained using Ft ‚àífi. Denote that feature by f worst
i
. In other words, f worst
i
is the
worst univariate feature that its removal from the current selected feature subset
leads to the highest score.
3. Update Ft ‚ÜêFt ‚àíf worst
i
and t ‚Üêt + 1.
4. Repeat steps 2 and 3 until the stopping criterion is met. Once the search is
stopped, the best feature subset is Ft.
Scikit-learn implementation of SBS and SFS: Scikit-learn implements SFS
and SBS with cross-validation through SequentialFeatureSelector class from
sklearn.feature_selection. To use SFS and SBS, the direction parameter
is set to 'forward' and 'backward', respectively.
The SequentialFeatureSelector class Scikit-learn (as of version 1.2.1) im-
plements SBS and SFS with two stopping criteria, which can be chosen via pa-
rameter n_features_to_select. In this regard, this parameter can be set to: 1)
either the absolute size (an integer) or proportion (a Ô¨Çoat) of candidate features;
and 2) 'auto' that stops the search if the improvement between two consecutive
iteration is not more than the value of tol parameter. Similar to the cv parameter
used cross_val_score or GridSearchCV that we saw in Chapter 9, we can set
the cv parameter in SequentialFeatureSelector to have more control over the
type of desired cross-validation to use.
Example 10.4 In this example, we use a similar approach as in Example 10.1 to
generate a single set of synthetic data. The dataset contains 10 informative features
and 90 uninformative noisy features (see Example 10.1 for data description). We then
use SFS and SBS to choose 10 features and see how many of them are among the
informative features. The use of this synthetic data here is insightful as we already
know the number of informative and uninformative features so we can have a better
picture of the performance of the feature selection mechanisms on the dataset.
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.feature_selection import SequentialFeatureSelector

296
10 Feature Selection
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as‚ê£
,‚ÜíLDA
n_features = 100
n_informative = 10
lda = LDA()
X, y = make_classification(n_samples=1000, n_features=n_features,‚ê£
,‚Üín_informative=10, n_redundant=0, n_repeated=0, n_classes=2,‚ê£
,‚Üín_clusters_per_class=1, class_sep=1, shuffle=False)
X_train_full, X_test_full, y_train, y_test = train_test_split(X, y,‚ê£
,‚Üístratify=y, train_size=0.2)
strkfold = StratifiedKFold(n_splits=5, shuffle=True)
%%timeit -n 1 -r 1
np.random.seed(42)
sfs = SequentialFeatureSelector(lda, n_features_to_select=10,‚ê£
,‚Üícv=strkfold, n_jobs=-1) # by default the direction is 'forwards'
sfs.fit(X_train_full, y_train)
print(sfs.get_support()) # using support we can see which feature is‚ê£
,‚Üíselected (True means selected, False means not selected)
print("The number of truly informative features chosen by SFS is:",‚ê£
,‚Üísum(sfs.get_support()[:10]))
[False
True False
True False
True False
True False
True False False
False False False False False False False False False False False False
False False False False False False False False
True False False False
False False False
True False False False False False False False False
False False False False False False False False False False False False
False
True False False False False False False False False False False
False False False False False False False False False False
True False
True False False False False False False False False False False False
False False False False]
The number of truly informative features chosen by SFS is: 5
5.58 s ¬± 0 ns per loop (mean ¬± std. dev. of 1 run, 1 loop each)
%%timeit -n 1 -r 1
sbs = SequentialFeatureSelector(lda, n_features_to_select=10,‚ê£
,‚Üícv=strkfold, direction='backward', n_jobs=-1)
sbs.fit(X_train_full, y_train)
print(sbs.get_support())
print("The number of truly informative features chosen by SBS is:",‚ê£
,‚Üísum(sbs.get_support()[:10]))
[False
True
True
True False
True False
True False
True False False
False False False False False False False False False False False False
False False False False False False False False False False False False
False False False False False False False False False False False False
False False False False False False False False False False
True False

10.2 Feature Selection Techniques
297
False
True False
True False False False False False False False False
False False False False False False False False
True False False False
False False False False False False False False False False False False
False False False False]
The number of truly informative features chosen by SBS is: 6
50.1 s ¬± 0 ns per loop (mean ¬± std. dev. of 1 run, 1 loop each)
In this example, SBS picks more of the informative features than SFS but it takes
almost 9 times more than SFS to stop. ‚ñ†
10.2.3 Embedded Methods
These are algorithms that once the predictive model is constructed, the structure of
the model can be used for feature selection. In other words, the feature selection
stage is embedded into the model training. In this regard, once the model is trained,
the importance of features are judged by the properties of learners. For example, the
magnitudes of coeÔ¨Écients in a linear model trained on normalized data could be
viewed as the inÔ¨Çuence of their corresponding features on the target variable. Now
if in the same scenario a feature coeÔ¨Écient is zero, it means no inÔ¨Çuence from that
feature on the target variable in the given data. There are two types of embedded
methods: 1) non-iterative; and 2) iterative.
Non-iterative Embedded Methods: Non-iterative embedded methods do not re-
quire an iterative search strategy to select features. Here, by ‚Äúiteration‚Äù, we do not
refer to the iterative optimization method that is used to construct the underlying pre-
dictive models given a Ô¨Åxed set of features. Instead, we refer to an iterative procedure
for selecting features. An example of a non-iterative embedded method for feature
selection is logistic regression based on l1 regularization (lasso). As seen in Section
6.2.2, this classiÔ¨Åer had the property of selecting features in a high-dimensional
setting as it sets the coeÔ¨Écients of many features to zero. Some other models such
as decision trees and random forest have also an internal mechanism to deÔ¨Åne ‚Äúim-
portance‚Äù for a feature. A common metric to measure importance in random forests
is the Mean Decrease Impurity (MDI) (Louppe et al., 2013). The MDI for a feature
k ( fk), denoted MDIfk , is deÔ¨Åned as the normalized sum of impurity drops across
all nodes and all trees that use fk for splitting the nodes. Using similar notations as
in Section 7.2.2, for a trained random forest containing nTree trees, this is given by
MDIfk =
1
nTree
√ï
Tree
√ï
m‚ààTree: if Œæ=(k,.)
nSm
n ‚àÜm,Œæ ,
(10.2)
where Tree is a variable indicating a tree in the forest, nSm
n
shows the proportion of
samples reaching node m, Œæ = (k, .) means if the split was based on fk with some
threshold, and

298
10 Feature Selection
‚àÜm,Œæ = i(Sm) ‚àí
nSYes
m, Œæ
nSm
i(SYes
m,Œæ) ‚àí
nSNo
m, Œæ
nSm
i(SNo
m,Œæ) .
(10.3)
With this deÔ¨Ånition, a higher value of MDIfk is perceived as a greater feature
importance. If we use Gini index as impurity measure, then MDI is known as Gini
importance. The feature importance for a decision tree is obtained by applying (10.2)
to that one tree.
Iterative Embedded Methods: In contrast with wrappers that compute a number
of performance scores for adding or removing feature subsets, iterative embedded
methods guide the search by some criteria related to the learning algorithm or the
structure of the trained predictive model. For example, in grafting (Perkins et al.,
2003), a forward selection procedure is proposed for models that are trained using
gradient decent to optimize the log loss. At each iteration, the approach identiÔ¨Åes
and adds the feature that has the fastest rate in decreasing the loss function (i.e.,
has highest magnitude of the loss function gradient with respect to the weight of
the feature). One of the most popular iterative embedded feature selection method is
recursive feature elimination (RFE) (Guyon et al., 2002). In this method, a feature
ranking criterion related to the structure of the trained predictive model is used
to recursively remove lowest-ranked features‚Äîfor example, as we said before, in
linear models and random forests, the magnitude of coeÔ¨Écients and the MDI can be
used for feature ranking, respectively. Formally RFE is implemented as in the next
algorithm.
Although one of the stopping criteria in the presented algorithm for RFE is based
on estimated performances, the eÔ¨Éciency of RFE when compared with SBS (will be
discussed shortly) plays down the role of a ‚Äústopping‚Äù criterion. That is to say, rather
than stopping the search, it is more common to use cross-validation to estimate the
performance score for all possible candidate feature subsets exposed by RFE and
then select the one with the highest estimated score. We refer to this procedure as
‚ÄúRFE-CV‚Äù. As an example, for p candidate features and s = 1, RFE-CV computes
p scores estimated by CV.
By using RFE one can achieve signiÔ¨Åcant computational eÔ¨Éciency with respect
to SBS because:
1) at each iteration, the feature ranking criterion is an eÔ¨Écient way to identify
features to remove. In comparison with SBS, if Ft = l, then SBS needs assessing
l features using a model evaluation rule. For example, using K-fold CV, it should
train and evaluate l √ó K surrogate models to remove one feature. However, in
RFE, training one model on Ft would give us the information we need to remove
one feature (or even a subset of features). Once the features are removed, in RFE-
CV, K surrogate models are trained and evaluated to assign a score estimated by
CV to the new feature subset; and
2) in SBS, features are removed one at a time. Although there is generalized form of
SBS (called Generalized SBS) in which at iteration t all combinations of feature
subsets of a speciÔ¨Åc size of Ft are examined for removal, the computational
complexity of that could be even higher than SBS. However, removing multiple

10.2 Feature Selection Techniques
299
Algorithm Recursive Feature Elimination
1. Set t = 0 and F0 = { f1, f2, . . ., fp} . Choose a learning algorithm that internally
implements a ranking criterion. Choose a step size s, which determines the
number of features to remove at each iteration. Pick a stopping criterion:
1.1. One stopping criterion could be the size of feature subsets to pick; for
example, one may decide to select k < p features.
1.2. Similar to SBS, another stopping criterion that does not require specifying
a predetermined number of features to select could be removing features as
long as there is a tangible improvement in the estimated performance score.
2. Use features in Ft to train a model. Identify the s features that are ranked lowest
and call this subset Flowest
t
.
3. Update Ft ‚ÜêFt ‚àíFlowest
t
. In case of having a stopping criterion based on
estimated performances, compute and record the estimated performance of the
classiÔ¨Åer trained using Ft. Set t ‚Üêt + 1.
4. Repeat steps 2 and 3 until the stopping criterion is met. Once the search is
stopped, the best feature subset is Ft.
features in RFE is quite straightforward with no additional computational cost
and it is indeed the recommended approach for high-dimensional data.
Scikit-learn implementation of RFE: Scikit-learn implements two classes for
RFE: RFE and RFECV, which are both in sklearn.feature_selection module.
In RFE one needs to set n_features_to_select parameter, which indicates the
number of features selected by the RFE. Currently, default value of this parameter,
which is None, will return half of the features. RFECV, on the other hand, will pick the
number of features from cross-validation. For example, for step=1, will compute p
(number of features) CV scores and selects the feature subsets that led to the highest
score. Another convenient and useful feature of RFE and RFECV classes is that they
both implement predict and score methods using which they reduce the features
in a provided test data to the selected number of features and perform prediction and
compute the score.
Example 10.5 This is the same dataset used in Example 10.5. Here we use RFE to
select 10 features and then use RFE-CV to choose a number of features determined
from cross-validation.
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.feature_selection import RFE

300
10 Feature Selection
from sklearn.feature_selection import RFECV
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as‚ê£
,‚ÜíLDA
np.random.seed(42)
n_features = 100
n_informative = 10
lda = LDA()
X, y = make_classification(n_samples=1000, n_features=n_features,‚ê£
,‚Üín_informative=10, n_redundant=0, n_repeated=0, n_classes=2,‚ê£
,‚Üín_clusters_per_class=1, class_sep=1, shuffle=False)
X_train_full, X_test_full, y_train, y_test = train_test_split(X, y,‚ê£
,‚Üístratify=y, train_size=0.2)
strkfold = StratifiedKFold(n_splits=5, shuffle=True)
import time
start = time.time()
rfe = RFE(lda, n_features_to_select=10)
rfe.fit(X_train_full, y_train)
print(rfe.get_support())
print("The number of truly informative features chosen by RFE is:",‚ê£
,‚Üísum(rfe.get_support()[:10]))
end = time.time()
print("The RFE execution time (in seconds): {:.4f}".format(end - start))
lda_score = rfe.score(X_test_full, y_test)
print("The score of the lda trained using selected features on training‚ê£
,‚Üídata is: {:.4f}".format(lda_score))
[ True
True
True False
True False
True
True
True
True
True False
False False False False False False False False False False False False
False False False False False False False False False False False False
False False False False False False False False False False False False
False False False False False False False False False False False False
False False False False False False False False False False False False
False False False False False False False False False False False False
False False False False False False False False False False False False
False
True False False]
The number of truly informative features chosen by RFE is: 8
The RFE execution time (in seconds): 0.2335
The score of the lda trained using selected features on training data is:‚ê£
,‚Üí0.9500
import time
start = time.time()
rfecv = RFECV(lda, cv=strkfold, n_jobs=-1)
rfecv.fit(X, y)
print(rfecv.get_support())

10.2 Feature Selection Techniques
301
print("The number of features selected by RFE CV is:", rfecv.
,‚Üín_features_)
print("The number of truly informative features chosen by RFE is:",‚ê£
,‚Üísum(rfecv.get_support()[:10]))
end = time.time()
print("The RFE CV execution time (in seconds): {:.4f}".format(end -‚ê£
,‚Üístart))
lda_score = rfecv.score(X_test_full, y_test)
print("The score of the lda trained using selected features on training‚ê£
,‚Üídata is: {:.4f}".format(lda_score))
[ True
True
True
True
True False
True
True
True
True False False
False False False False False False False False False False False False
False False False False False False False False False False False False
False False False False False False False False False False False False
False False False False False
True False False False
True False False
False False False False False False
True False False False False False
False False False False False False False False False False False False
False False False False False False False False False False False False
False False False False]
The number of features selected by RFE CV is: 12
The number of truly informative features chosen by RFE is: 9
The RFE CV execution time (in seconds): 1.3327
The score of the lda trained using selected features on training data is:‚ê£
,‚Üí0.9563
‚ñ†
EÔ¨Éciency of RFE with respect to SBS is an extremely useful property. The
computational burden incurred by SBS is a major impediment in the utility of SBS
for high-dimensional data. In examples 10.4 and 10.5 we use a relatively modest
dimensionality and we see that RFECV is about 50.1/1.33 ‚âà38 times faster to
compute than SBS (the exact ratio depends on the runs and the machine). This ratio,
which is an indicator of eÔ¨Éciency of RFECV with respect to SBS, becomes larger if
we increase the number of dimensions and/or training sample size (see exercises 4
and 5).
Exercises:
Exercise 1: Suppose in a dataset we have m features and for each feature we test a
null hypothesis H0 against an alternative hypothesis Ha to determine whether the
feature is important/signiÔ¨Åcant (i.e., H0 is rejected in the favor of Ha). Assuming
each test has a signiÔ¨Åcance level of Œ± = 0.05, what is FWER (the probability of
having at least one false positive)?
√â Exercise 2: Suppose a study involves m hypothesis tests. Show that if each null
hypothesis is rejected when the P-value for the corresponding test is less than or
equal to Œ±/m (Bonferroni correction), where Œ± is the signiÔ¨Åcance level of each test,

302
10 Feature Selection
then FWER‚â§Œ±. To prove use the following well-known results: 1) P-values have
uniform distribution between 0 and 1 when the null hypothesis is true (Rice, 2006,
p. 366); and 2) for events Ai, i = 1, . . ., m, P

‚à™m
i=1Ai

‚â§√çm
i=1 P(Ai), which is known
as the union bound.
Exercise 3: Assume we would like to choose the p/2 suboptimal feature subset out
of p (even) number of features using both SFS and SBS.
1. write an expression in terms of p that shows the number of feature subsets that
must be evaluated using SFS (simplify your answer as much as possible).
2. write an expression in terms of p that shows the number of feature subsets that
must be evaluated using SBS (simplify your answer as much as possible).
Exercise 4: A) Run examples 10.4 and 10.5 on your machine and Ô¨Ånd the ratio of
execution time of RFE-CV to that of SBS.
B) Rerun part (A) but this time by increasing the number of candidate features
from 100 to 150 (i.e., setting n_features in make_classification to 150). How
does this change aÔ¨Äect the execution time ratio found previously in part (A)?
C) As in part (A), use n_features = 100 but this time, change the size of
training set from 200 to 1000 and run the examples. How does this change aÔ¨Äect the
execution time ratio found previously in part (A)?
Exercise 5: Suppose SBS is used to pick l out of p candidate features in a classiÔ¨Å-
cation problem. At the same time, RFE-CV is used to pick an unknown number of
features (because the number of selected features depends on the scores estimated by
CV). Assume that in both SBS and RFE-CV, K-fold CV is used. Show that if p >> l
and K >> 1, the number of classiÔ¨Åers trained in the process of SBS is approximately
p/2 more than the number of classiÔ¨Åers trained in RFE-CV.
Exercise 6: Suppose we have a dataset of 100 features and we wish to select 50
features using a wrapper method. We use 5-fold cross-validation with LDA classiÔ¨Åer
inside wrapper in all the following techniques. Which of the following statements is
true?
A) Using exhaustive search is likely to terminate faster than both sequential back-
ward search and sequential forward search
B) Using sequential backward search is likely to terminate faster than sequential
forward search
C) Using sequential forward search is likely to terminate faster than sequential
backward search

Chapter 11
Assembling Various Learning Steps
So far we have seen that in order to achieve a Ô¨Ånal predictive model, several steps are
often required. These steps include (but not limited to) normalization, imputation,
feature selection/extraction, model selection and training, and evaluation. The misuse
of some of these steps in connection with others have been so common in various
Ô¨Åelds of study that have occasionally triggered warnings and criticism from the
machine learning community. This state of aÔ¨Äairs can be attributed in large part
to the misuse of resampling evaluation rules such as cross-validation and bootstrap
for model selection and/or evaluation in connection with a combination of other
steps to which we refer as a composite process (e.g., combination of normalization,
feature extraction, and feature selection). In this chapter we focus primarily on the
proper implementation of such composite processes in connection with resampling
evaluation rules. To keep discussion succinct, we use feature selection and cross-
validation as typical representatives of the composite process and a resampling
evaluation rule, respectively. We then describe appropriate implementation of
1. Feature Selection and Model Evaluation Using Cross-Validation
2. Feature and Model Selection Using Cross-Validation
3. Nested Cross-Validation for Feature and Model Selection, and Evaluation
11.1 Using Cross-Validation Along with Other Steps in a Nutshell
Cross-validation is eÔ¨Äective for model evaluation because it uses data at hand to
simulates the eÔ¨Äect of unseen future data for evaluating a trained model. Therefore,
the held-out fold in each iteration of cross-validation, which serves as a test set for
evaluating the surrogate model, must be truly treated as unseen data. In other words,
the held-out fold should not be used in steps such as normalization and/or feature
selection/extraction. After all, ‚Äúunseen‚Äù data can naturally not be used in these steps.
To formalize the idea, let Œû : I ‚ÜíO denote a composite process that transforms
an input sample space I to an output space O, which depending on the applica-
tion includes one or more processing steps that are data-dependent‚Äîsteps such as
303
¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
A. Zollanvari, Machine Learning with Python, https://doi.org/10.1007/978-3-031-33342-2_11 

304
11 Assembling Various Learning Steps
segmentation (e.g., if the segment duration is determined from data), imputation,
normalization, and feature selection/extraction. To put it another way, Œû is a com-
posite transformer, which means it is a composite estimator that transforms data.
Furthermore, similar to Section 9.1.1, let Foldk and Str,n ‚àíFoldk denote the held-out
fold and training folds at iteration k in K-fold CV such that √êK
k=1 Foldk = Str,n.
Hereafter, we show transformation performed by Œû on an element of the input space
S ‚ààI by Œû S. Our goal is to use cross-validation to evaluate the performance of
a classiÔ¨Åer trained on the output of the composite transformer Œû when applied to
training data. This is formalized as follows:
Goal:
1. learn Œû from Str,n‚Äîlearning Œû from a given data Str,n means estimating all
transformations that are part of Œû from Str,n. Let Œûtr denote the learned com-
posite process;
2. use Œûtr to transform Str,n into a new training data Œûtr
 Str,n
;
3. use the transformed training data Œûtr
 Str,n
 along with a classiÔ¨Åcation rule to
train a classiÔ¨Åer œàtr(x); and
4. use K-fold CV to evaluate the trained classiÔ¨Åer œàtr(x).
In a nutshell, the correct way of using K-fold CV to evaluate œàtr(x) is to apply K-
fold CV external to the composite process Œû. In fact implementation of the external
CV is not new‚Äìit is merely the proper way of implementing the standard K-fold
CV algorithm that was presented in Section 9.1.1 by realizing that the ‚Äúlearning
algorithm‚Äù Œ® presented there is a combination of a composite estimator Œû that
transforms the data plus a classiÔ¨Åer/regressor construction algorithm. Nevertheless,
we present a separate algorithm, namely, ‚ÄúExternal‚Äù K-fold CV (Ambroise and
McLachlan, 2002), to describe the implementation in more details. Furthermore,
hereafter we refer to Œ® as a ‚Äúcomposite estimator‚Äù because it is the combination of
a composite transformer and an estimator (classiÔ¨Åer/regressor).
Note that in the ‚ÄúExternal‚Äù K-fold CV algorithm, although the general form
of processes used within Œû1, Œû2, . . ., ŒûK are the same, the precise form of each
transformation is diÔ¨Äerent because each Œûk is learned from a diÔ¨Äerent data Str,n ‚àí
Foldk, k = 1, . . ., K. At the same time, we observe that throughout the algorithm,
Foldk is not used anyhow in learning Œûk or training œàk(x). Therefore, Foldk is truly
treated as an unseen data for the classiÔ¨Åer trained on iteration-speciÔ¨Åc training set
Str,n ‚àíFoldk. To better grasp and appreciate the essence of the ‚ÄúExternal‚Äù K-fold
CV algorithm in achieving the ‚ÄúGoal‚Äù, in the next few sections in this chapter we
use feature selection as a typical processing step used in Œû; after all, it was mainly
the common misuse of feature selection and cross-validation that triggered several
warnings (Dupuy and Simon (2007); Ambroise and McLachlan (2002); Michiels
et al. (2005)). That being said, the entire discussion remains valid if we augment Œû
with other transformations discussed before.

11.2 A Common Mistake
305
Algorithm ‚ÄúExternal‚Äù K-fold CV
1. Split the dataset Str,n into K folds, denoted Foldk.
2. For k from 1 to K
1.1. Learn Œû from Str,n ‚àíFoldk; Let Œûtr,k denote the learned composite process
in this iteration.
1.2. Use Œûtr,k to transform Str,n ‚àíFoldk into a new data Œûtr,k
 Str,n ‚àíFoldk
.
1.3. Use the transformed training data Œûtr,k
 Str,n ‚àíFoldk
 to train a surrogate
classiÔ¨Åer œàk(x).
1.4 Classify observations in the transformed held-out fold Œûtr,k
 Foldk
 using
œàk(x).
3. Compute the K-fold CV estimate of performance metric using misclassi-
Ô¨Åed/correctly classiÔ¨Åed observations obtained in step 1.4. This gives us the
estimate of performance metric for œàtr(x) (the ‚ÄúGoal‚Äù).
11.2 A Common Mistake
Insofar as feature selection and cross-validation are concerned, the most common
mistake is to apply feature selection on the full data at hand, construct a classiÔ¨Åer
using selected features, and then use cross-validation to evaluate the performance
of the classiÔ¨Åer. This practice leads to selection bias, which is caused because the
classiÔ¨Åer is evaluated based on samples that were used in the Ô¨Årst place to select
features that are part of the classiÔ¨Åer Ambroise and McLachlan (2002). This mistake
may stem from the discrepancy between the perception of the ‚ÄúGoal‚Äù and the actual
practical steps required to reach it. Here is a common perception of the Goal:
We wish to perform feature selection, construct a classiÔ¨Åer on the selected
features, and evaluate the performance of the constructed classiÔ¨Åer using cross-
validation.
This is an incorrect approach (the common mistake) to achieve the goal:
We need to perform feature selection Ô¨Årst, construct a classiÔ¨Åer on the selected
features, and evaluate the performance of the constructed classiÔ¨Åer using cross-
validation applied on the data with selected features.
Unfortunately, here intuition could lead us astray. The following example illus-
trates the issue with this common mistake.

306
11 Assembling Various Learning Steps
Example 11.1 Here we create a synthetic training data that has the same size as
the genomic dataset (172 √ó 21050) used in Example 10.3. However, this synthetic
dataset is a null data where all observations for all features across both classes are
random samples of the standard Gaussian distribution. Naturally, all features are
equally irrelevant to distinguish between the two classes because samples from both
classes are generated from the same distribution. By keeping the sample size across
both classes equal, we should expect a 50% accuracy for any learning algorithm
applied and evaluated on this null dataset.
We wish to use this null data to examine the eÔ¨Äect of the aforementioned common
mistake. In this regard, once the training data is generated, we apply ANOVA (see
Chapter 10) on the training set and pick the best 10 features, and then apply cross-
validation on the training data to evaluate the performance of a CART classiÔ¨Åer
constructed using the 10 selected features. At the same time, because this is a
synthetic model to generate data, we set the size of test set almost two times larger
than the training set. In particular, we choose 172 (training set size) + 328 (test set
size) = 500. Furthermore, in order to remove any bias towards a speciÔ¨Åc realization
of the generated datasets or selected folds used in CV, we repeat this entire process
50 times (mc_no = 50 in the following code) and report the average 5-fold CV and
the average test accuracies at the end of this process.
import numpy as np
from sklearn.datasets import make_classification
from sklearn.feature_selection import SelectKBest
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier as CART
np.random.seed(42)
n_features = 21050
n_sample = 500 # size of training + test sets
ratio_train2full = 172/500 # the proportion of training data to n_sample
mc_no = 50 # number of Monte Carlo repetitions
kfold = 5 # K-fold CV
cv_scores_matrix = np.zeros((mc_no, kfold))
test_scores = np.zeros(mc_no)
for j in np.arange(mc_no):
X = np.random.normal(0, 1, size=(n_sample, n_features))
y = np.concatenate((np.ones(n_sample//2, dtype='int_'), np.
,‚Üízeros(n_sample//2, dtype='int_')))
X_train, X_test, y_train, y_test = train_test_split(X, y,‚ê£
,‚Üístratify=y, train_size=ratio_train2full)
filter_KBest = SelectKBest(k=10).fit(X_train, y_train)
X_train_filtered = filter_KBest.transform(X_train)
strkfold = StratifiedKFold(n_splits=kfold, shuffle=True)
cart = CART()

11.3 Feature Selection and Model Evaluation Using Cross-Validation
307
cv_scores_matrix[j,:] = cross_val_score(cart, X_train_filtered,‚ê£
,‚Üíy_train, cv=strkfold)
X_test_filtered = filter_KBest.transform(X_test)
final_cart = cart.fit(X_train_filtered, y_train)
test_scores[j] = final_cart.score(X_test_filtered, y_test)
print("the average K-fold CV accuracy is: {:.3f}".
,‚Üíformat(cv_scores_matrix.mean()))
print("the average test accuracy is: {:.3f}".format(test_scores.mean()))
the average K-fold CV accuracy is: 0.649
the average test accuracy is: 0.493
As we can see, there is a considerable diÔ¨Äerence of ‚àº15% between the average
CV and the average test-set estimates of the accuracy. The test accuracy, as expected,
is about 50%; however, the CV estimate is overly optimistic. This reason for this is
solely because we used X_train to select features and then used the selected features
within cross-validation to evaluate the classiÔ¨Åer. This means that Foldk in CV was
already seen in training CART through the feature selection process. In other words,
Foldk is not an independent test set used for evaluating a surrogate CART trained on
Str,n ‚àíFoldk. ‚ñ†
As we saw in the previous section, the correct approach to evaluate œàtr(x) is
to apply K-fold CV external to feature selection. In other words, to evaluate the
classiÔ¨Åer trained based on features that are selected by applying the feature selection
on full training data, external K-fold CV implies:
We need to successively hold out one fold as part of cross-validation, apply fea-
ture selection on the remaining data to select features, construct a classiÔ¨Åer using
these selected features, and evaluate the performance of the constructed classiÔ¨Åer on
the held-out fold (and naturally take the average score at the end of cross-validation).
From a practical perspective, cross-validation can be used for both model eval-
uation, model selection, and both at the same time. As a result, in what follows we
discuss implementations of all these cases separately.
11.3 Feature Selection and Model Evaluation Using
Cross-Validation
Let Str,n,p denote a given training data with n observations of p dimensionality.
Let œàtr,d(x) denote a classiÔ¨Åer trained on d ‚â§p selected features using all n
observations in the training data. Assuming there is no test data, we wish to estimate
the performance of œàtr,d(x) using cross-validation applied on Str,n,p. To do so
we can just use the ‚ÄúExternal‚Äù K-fold CV algorithm presented in Section 11.1 by

308
11 Assembling Various Learning Steps
taking the composite process Œû as the feature selection. This can also be seen as
the application of the standard K-fold CV presented in Section 9.1.1 by taking
the composite estimator Œ® as feature selection (Œû) + classiÔ¨Åer/regressor learning
algorithm.
Scikit-Learn Pipeline class and implementation of feature selection and CV
model evaluation: As we know from Chapter 9, cross_val_score, for example,
expects a single estimator as the argument but here our estimator should be
a composite estimator ( Œ® ‚â°feature selection + classiÔ¨Åer construction rule). Can
we create such a composite estimator Œ® and treat it as a single estimator in scikit-
learn? The answer to this question is aÔ¨Érmative. In scikit-learn we can make a single
estimator out of a feature selection procedure (the object from the class implementing
the procedure) and a classiÔ¨Åer (the object from the class implementing the classiÔ¨Åer).
This is doable by the Pipeline class from sklearn.pipeline module.
The common syntax for using the Pipeline class is Pipeline(steps) where
steps is a list of tuples (arbitrary_name, transformer) with the last one
being a tuple of (arbitrary_name, estimator). In other words, all estimators
that are chained together in a Pipeline except for the last one must implement the
transform method so that the output of one is transformed and used as the input
to the next one (these transformers create the composite transformer Œû). The last
estimator can be used to add the classiÔ¨Åcation/regression learning algorithm to the
Pipeline  to create the composite estimator Œ®. Below is an example where two
transformers PCA() and SelectKBest(), and one estimator CART() are chained
together to create a composite estimator named pipe.
from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA
from sklearn.tree import DecisionTreeClassifier as CART
from sklearn.feature_selection import SelectKBest
pipe = Pipeline(steps=[("dimen_reduc", PCA(n_components=10)),‚ê£
,‚Üí("feature_select", SelectKBest(k=5)), ("classifier", CART())])
Example 11.2 Here we aim to apply the external K-fold CV under the same settings
as in Example 11.1. The external K-fold CV is used here to correct for the selection
bias that occurred previously in Example 11.1. As mentioned before, we should
create one composite estimator to implement feature selection + classiÔ¨Åer learning
algorithm and use that estimator within each iteration of CV. As in Example 11.1,
at the end of the process, the average performance estimate obtained using cross-
validation and test set are compared.
import numpy as np
from sklearn.datasets import make_classification
from sklearn.feature_selection import SelectKBest
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split

11.3 Feature Selection and Model Evaluation Using Cross-Validation
309
from sklearn.tree import DecisionTreeClassifier as CART
from sklearn.pipeline import Pipeline
np.random.seed(42)
n_features = 21050
n_sample = 500 # size of training + test set
ratio_train2full = 172/500
mc_no = 50 # number of Monte Carlo repetitions
kfold = 5 # K-fold CV
cv_scores_matrix = np.zeros((mc_no, kfold))
test_scores = np.zeros(mc_no)
strkfold = StratifiedKFold(n_splits=kfold, shuffle=True)
pipe = Pipeline(steps=[('kbest', SelectKBest(k=10)), ('clf', CART())])‚ê£
,‚Üí# kbest and clf are arbitrary names
for j in np.arange(mc_no):
X = np.random.normal(0, 1, size=(n_sample, n_features))
y = np.concatenate((np.ones(n_sample//2, dtype='int_'), np.
,‚Üízeros(n_sample//2, dtype='int_')))
X_train, X_test, y_train, y_test = train_test_split(X, y,‚ê£
,‚Üístratify=y, train_size=ratio_train2full)
cv_scores_matrix[j,:] = cross_val_score(pipe, X_train, y_train,‚ê£
,‚Üícv=strkfold)
final_cart = pipe.fit(X_train, y_train)
test_scores[j] = final_cart.score(X_test, y_test)
print("the average K-fold CV accuracy is: {:.3f}".
,‚Üíformat(cv_scores_matrix.mean()))
print("the average test accuracy is: {:.3f}".format(test_scores.mean()))
the average K-fold CV accuracy is: 0.506
the average test accuracy is: 0.493
As we can see here, the average CV estimate of the accuracy is very close
to that of test-set estimate and both are about 50% as they should (recall that
we had a null dataset). Here by cross_val_score(pipe, X_train, y_train,
cv=strkfold), we are training 5 CART surrogate classiÔ¨Åers where each is trained
using (possibly) diÔ¨Äerent set of features of size 10 (because Str,n,p ‚àíFoldk is dif-
ferent in each iteration of CV), and evaluate each on its corresponding held-out
fold (Foldk). This is a legitimate performance estimate of the outcome of applying
the composite estimator Œ® to Str,n,p; that is to say, the performance estimate of
our Ô¨Ånal constructed classiÔ¨Åer, which itself is diÔ¨Äerent from all surrogate classi-
Ô¨Åers. This is the classiÔ¨Åer constructed using final_cart = pipe.fit(X_train,
y_train)‚Äîwe name it ‚ÄúÔ¨Ånal_cart‚Äù because this is what we use for predicting
unseen observations in the future. Here as we could easily generate test data
from our synthetic model, we evaluated and reported the performance of this
classiÔ¨Åer on test data using test_scores[j] = final_cart.score(X_test,

310
11 Assembling Various Learning Steps
y_test). However, in the absence of test data, the CV estimate itself obtained by
cross_val_score(pipe, X_train, y_train, cv=strkfold) is a reasonable
estimate of the performance of final_cart‚Äîas observed the average diÔ¨Äerence
between CV estimate and the hold-out estimate is less than 1%. ‚ñ†
11.4 Feature and Model Selection Using Cross-Validation
As discussed in Section 9.2.1, a common approach for model selection is the grid
search cross-validation. However, once feature selection is desired, the search for the
best model should be conducted by taking into account the joint relationship between
the search and the feature selection; that is to say, the best feature-model is the one
that leads to the highest CV score across all examined combinations of features and
models (i.e., a joint feature-model selection).
Scikit-Learn implementation of feature selection and CV model selection: As
discussed in Section 9.2.1, the grid search cross-validation is conveniently im-
plemented using sklearn.model_selection.GridSearchCV class. Therefore,
a convenient way to integrate feature selection as part of model selection is using an
object of Pipeline in GridSearchCV. However, as Pipeline creates a composite
estimator out of several other estimators, there is a special syntax that we need to use
to instruct param_grid used in GridSearchCV about the speciÔ¨Åc estimators over
which we wish to conduct tuning (and of course the candidate parameter values for
those estimator). Furthermore, we need to instantiate the Pipeline using one pos-
sible choice of transformer/estimator and then use param_grid of GridSearchCV
to populate the choices used in steps of Pipeline. To illustrate the idea as well as
introduce the special aforementioned syntax, we look into the following instructive
example.
Example 11.3 Here we use sklearn.datasets.make_classification to gen-
erate a data of 1000 features with only 10 informative features (n_informative=10)
and a train/test sample size of 200/800. We wish to use GridSearchCV for joint
feature-model selection. For this purpose, we use k = 5, 10, 50 as candidate number
of best features to select using SelectKBest. As for the classiÔ¨Åcation algorithms,
we use: 1) logistic regression with l2 penalty and possible values of C being 0.1,
1, and 10; 2) CART and random forest (even though random forest has an internal
feature ranking) with minimum samples per leaf being either 1 or 5; and 3) logistic
regression with lasso penalty (l1 regularization) but in doing so, k best individual
features (SelectKBest) will not be used because l1 regularization performs fea-
ture selection internally (Section 10.2.3). The following code implements this joint
feature-model selection procedure.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

11.4 Feature and Model Selection Using Cross-Validation
311
from sklearn.datasets import make_classification
from sklearn.feature_selection import SelectKBest
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier as CART
from sklearn.ensemble import RandomForestClassifier as RF
from sklearn.linear_model import LogisticRegression as LRR
from sklearn.pipeline import Pipeline
from sklearn.model_selection import StratifiedKFold, GridSearchCV
np.random.seed(42)
n_features = 1000
acc_test = np.zeros(n_features)
kfold = 5 # K-fold CV
X, y = make_classification(n_samples=1000, n_features=n_features,‚ê£
,‚Üín_informative=10, n_redundant=0, n_repeated=0, n_classes=2,‚ê£
,‚Üín_clusters_per_class=1, class_sep=1, shuffle=False)
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,‚ê£
,‚Üítrain_size=0.2)
pipe = Pipeline(steps=[('kbest', SelectKBest()), ('clf', CART())]) #‚ê£
,‚Üíkbest and clf are arbitrary names
strkfold = StratifiedKFold(n_splits=kfold, shuffle=True)
param_grid = [
{'clf': [LRR()], 'clf__penalty': ['l2'], 'clf__C': [0.1, 1, 10],
'kbest': [SelectKBest()], 'kbest__k': [5, 10, 50]},
{'clf': [LRR()], 'clf__penalty': ['l1'], 'clf__C': [0.1, 1, 10],
'clf__solver': ['liblinear'], 'kbest': ['passthrough']},
{'clf': [CART(), RF()], 'clf__min_samples_leaf': [1, 5],
'kbest': [SelectKBest()], 'kbest__k': [5, 10, 50]}
]
gscv = GridSearchCV(pipe, param_grid, cv=strkfold)
score_best_estimator=gscv.fit(X_train, y_train).score(X_test, y_test)
print('the highest score is: {:.3f}'.format(gscv.best_score_))
print('the best composite estimator is: {}'.format(gscv.
,‚Üíbest_estimator_))
print('the best combination is: {}'.format(gscv.best_params_))
print('the accuracy of the best estimator on the test data is: {:.3f}'.
,‚Üíformat(score_best_estimator))
df = pd.DataFrame(gscv.cv_results_)
#df # uncomment this to see all 24 combinations examined in this example
the highest score is: 0.950
the best composite estimator is: Pipeline(steps=[('kbest', SelectKBest()),
('clf', RandomForestClassifier())])
the best combination is: {'clf': RandomForestClassifier(),

312
11 Assembling Various Learning Steps
'clf__min_samples_leaf': 1, 'kbest': SelectKBest(), 'kbest__k': 10}
the accuracy of the best estimator on the test data is: 0.929
Several important points in the implementation of this example:
1. here the choice of SelectKBest() and CART() as the estimators used along
with'kbest'and'clf'inPipeline(steps=[('kbest', SelectKBest()),
('clf', CART())]) is arbitrary. They are replaced by speciÔ¨Åc choices that are
indicated in param_grid used for GridSearchCV. For example, we can instan-
tiate the Pipeline class as
Pipeline(steps=[('kbest',SelectKBest()),
('clf', KNeighborsClassifier())])
but because KNeighborsClassifier() is not used in param_grid, the
GridSearchCV does not use KNeighborsClassifier() anyhow;
2. to instruct sklearn about the speciÔ¨Åc choices of steps that should be used within
GridSearchCV, we need to look into the names used with steps (of Pipeline)
and then use each step name as one key in dictionaries used in param_grid
(of GridSearchCV) and specify its values. For example, here 'clf' is the
(arbitrary) name given to the last step of the pipe. In deÔ¨Åning our grids (using
param_grid), we use this name as a key and then use a list of possible estimators
[CART(), RF()] as its value (e.g., 'clf': [CART(), RF()]) in the above
code);
3. the special Pipeline syntax: to set the parameters of each step, we use the name
of the step along with a double underscore __ in keys within each dictionary
used in param_grid and use a list of possible values of that parameter as the
value of that key. For example, 'clf__penalty': ['l2'] or 'kbest__k':
[5, 10, 50] that were used in the previous code; and
4. in cases where multiple estimators are tuned on the same parameters with the
same candidate values (of course if the parameters of interest exist in all those
estimators), we can integrate them in one dictionary by adding them to the list
of values used for the key pointing to that step. This is why in the above
code, we used 'clf': [CART(), RF()] in {'clf': [CART(), RF()],
'clf__min_samples_leaf': [1, 5], 'kbest': [SelectKBest()],
'kbest__k': [5, 10, 50]}.
‚ñ†
In Example 11.3, we used the grid search cross-validation for model selection.
Nevertheless, as we have already performed the cross-validation, it seems tempting
to also use/report the CV estimate of the performance of the best classiÔ¨Åer (outcome
of the joint feature-model selection) as a legitimate CV estimate of that classiÔ¨Åer.
However, unfortunately, this is not a legitimate practice. To better understand the
issue, let us focus on a single speciÔ¨Åc split of CV for iteration k: Foldk and Str,n ‚àí
Foldk; that is to say, we replace CV model selection with hold-out estimation used
for model selection. At the same time, we restrict the model selection procedure to
hyperparameter tuning for a speciÔ¨Åc classiÔ¨Åer.

11.4 Feature and Model Selection Using Cross-Validation
313
Recall that to have a legitimate hold-out estimate of performance for a classiÔ¨Åer
œàk(x) that is trained on Str,n ‚àíFoldk, Foldk should have not been used in training
œàk(x). In the process of model selection, we use Foldk in estimating the performance
of a series of classiÔ¨Åers where each is trained using a diÔ¨Äerent combination of hy-
perparameter values. We can correctly argue that the performance estimate obtained
by applying each classiÔ¨Åer on the held-out fold (Foldk) is a legitimate performance
estimate of that classiÔ¨Åer; however, the moment we select one of these classiÔ¨Åers
(i.e., the one to which we refer as the ‚Äúbest classiÔ¨Åer‚Äù) over others based on the
performance estimates that were obtained on Foldk, we are indeed using Foldk in
the training process leading to that best classiÔ¨Åer (through our ‚Äúselection‚Äù), and
that is not a legitimate hold-out estimate anymore!
There is indeed a counterintuitive argument here: our estimate obtained on Foldk
for the best classiÔ¨Åer is still the same as before designating that classiÔ¨Åer as the best
one, and we said previously that this estimate was a legitimate hold-out estimate of
performance for that classiÔ¨Åer before designating that as the best classiÔ¨Åer. So what
happens that after ‚Äúselecting‚Äù that classiÔ¨Åer over others (now that is called the best
classiÔ¨Åer), this is not a legitimate performance estimate even though the estimate is
not changed?
To understand this, we need to put these practices in the context of repeated
sampling of the underlying populations. Let Œ∏‚àódenote the best combination of
hyperparameter values (that is used to train the best classiÔ¨Åer). A legitimate hold-out
estimator must have a held-out fold that is independent of the training process. This
is because the hold-out estimator (the same for CV) is designed to be an almost
unbiased estimator in estimating the true error of a classiÔ¨Åcation rule (slightly
pessimistic because we use fewer samples in training). In broad terms, an unbiased
estimator is one that on average (over many samples of the same population) behaves
similarly to what is estimating. Let E[ŒµŒ∏] denote the mean of true error rate for a
classiÔ¨Åcation algorithm (also known as unconditional error (Braga-Neto, 2020))
given a speciÔ¨Åc set of hyperparameters Œ∏. Now suppose we have additional training
datasets denoted as S‚Ä≤
tr,n, S‚Ä≤‚Ä≤
tr,n, . . . , which are sampled from the same populations,
and splitting each leads to S‚Ä≤
tr,n ‚àíFold‚Ä≤
k and Fold‚Ä≤
k split, S‚Ä≤‚Ä≤
tr,n ‚àíFold‚Ä≤‚Ä≤
k and Fold‚Ä≤‚Ä≤
k
split, . . . . We consider two cases:
Case 1. Were we going to train a classiÔ¨Åer with a speciÔ¨Åc combination of hy-
perparameter which happen to be Œ∏‚àóon S‚Ä≤
tr,n ‚àíFold‚Ä≤
k, S‚Ä≤‚Ä≤
tr,n ‚àíFold‚Ä≤‚Ä≤
k , . . . , and take
the average of their performance estimates on Fold‚Ä≤
k, Fold‚Ä≤‚Ä≤
k , . . . ., then that average
would have been very close to E[ŒµŒ∏‚àó]‚Äîhere each performance estimate is a legiti-
mate estimate. This is what is meant by ‚Äúbefore selecting the best classiÔ¨Åer‚Äù: we treat
Œ∏‚àóas any other combination of hyperparameter values; that is, the same argument
applies to any other combination of hyperparameter values because all classiÔ¨Åers are
trained using the same combination.
Case 2. Similar to what we did in Ô¨Ånding Œ∏‚àó, let us train a number of classiÔ¨Åers on
S‚Ä≤
tr,n ‚àíFold‚Ä≤
k where each is using a unique combination of hyperparameter values.
Then by comparing their performance estimates on Fold‚Ä≤
k, we select Œ∏‚àó‚Ä≤, which is
not necessarily the same as Œ∏‚àó. Doing the same on S‚Ä≤‚Ä≤
tr,n ‚àíFold‚Ä≤‚Ä≤
k and Fold‚Ä≤‚Ä≤
k leads
to Œ∏‚àó‚Ä≤‚Ä≤, . . . . Here to Ô¨Ånd each best classiÔ¨Åer, we use each Foldk, Fold‚Ä≤
k, Fold‚Ä≤‚Ä≤
k , . . .

314
11 Assembling Various Learning Steps
in Ô¨Ånding Œ∏‚àó, Œ∏‚àó‚Ä≤, Œ∏‚àó‚Ä≤‚Ä≤, . . . . In this case, the average of the performance estimates
of each best classiÔ¨Åer that is obtained on Fold‚Ä≤
k, Fold‚Ä≤‚Ä≤
k , . . . , is not neecessrily close
to any of E[ŒµŒ∏‚àó], E[ŒµŒ∏‚àó‚Ä≤], E[ŒµŒ∏‚àó‚Ä≤‚Ä≤], . . . . The problem is caused because held-out
folds are seen in training each base classiÔ¨Åer (via selecting the best classiÔ¨Åer), and
each time we end up with a diÔ¨Äerent classiÔ¨Åer characterized by a unique Œ∏. As a
result, the performance estimates obtained are neither hold-out error estimates nor
CV estimates if repeated on diÔ¨Äerent splits.
Therefore, it is not legitimate to use the performance estimates obtained as part
of the grid search CV for evaluating the performance of the best classiÔ¨Åer. The
implication of violating this is not purely philosophical as it could have serious
implications in practice. To see that, in the following example we investigate the
implication of such a malpractice in Example 11.2. In particular, we examine the
impact of perceiving the value of CV obtained as part of the grid search as the
performance estimate of the best classiÔ¨Åer.
Example 11.4 Here we apply the grid search CV using the same hyperparameter
space deÔ¨Åned in Example 11.3 on the set of null datasets generated in Example
11.2. We examine whether the average CV accuracy estimates of the best classiÔ¨Åers
selected as part of the grid search is close to 50%, which should be the average true
accuracy of any classiÔ¨Åer on these null datasets.
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.datasets import make_classification
from sklearn.model_selection import GridSearchCV
from sklearn.feature_selection import SelectKBest
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier as CART
from sklearn.ensemble import RandomForestClassifier as RF
from sklearn.linear_model import LogisticRegression as LRR
np.random.seed(42)
n_features = 21050
n_sample = 500
ratio_train2test = 172/500
mc_no = 20 # number of MC repetitions
kfold = 5 # K-fold CV
cv_scores = np.zeros(mc_no)
test_scores = np.zeros(mc_no)
pipe = Pipeline(steps=[('kbest', SelectKBest()), ('clf', CART())]) #‚ê£
,‚Üíkbest and clf are arbitrary names
strkfold = StratifiedKFold(n_splits=kfold, shuffle=True)
grids = [
{'clf': [LRR()], 'clf__penalty': ['l2'], 'clf__C': [0.1, 1, 10],

11.5 Nested Cross-Validation for Feature and Model Selection, and Evaluation
315
'kbest': [SelectKBest()], 'kbest__k': [5, 10, 50]},
{'clf': [LRR()], 'clf__penalty': ['l1'], 'clf__C': [0.1, 1, 10],
'clf__solver': ['liblinear'], 'kbest': ['passthrough']},
{'clf': [CART(), RF()], 'clf__min_samples_leaf': [1, 5],
'kbest': [SelectKBest()], 'kbest__k': [5, 10, 50]}
]
for j in np.arange(mc_no):
X = np.random.normal(0, 1, size=(n_sample, n_features))
y = np.concatenate((np.ones(n_sample//2, dtype='int_'), np.
,‚Üízeros(n_sample//2, dtype='int_')))
X_train, X_test, y_train, y_test = train_test_split(X, y,‚ê£
,‚Üístratify=y, train_size=ratio_train2test)
gscv = GridSearchCV(pipe, grids, cv=strkfold, n_jobs=-1)
test_scores[j]=gscv.fit(X_train, y_train).score(X_test, y_test)
cv_scores[j] = gscv.best_score_
print("the average K-fold CV accuracy is: {:.3f}".format(cv_scores.
,‚Üímean()))
print("the average test accuracy is: {:.3f}".format(test_scores.mean()))
the average K-fold CV accuracy is: 0.568
the average test accuracy is: 0.502
As we can see there is a diÔ¨Äerence of more than 6% between the average accuracy
estimates of the best 20 classiÔ¨Åers (mc_no = 20) obtained as part of the grid search
CV and their average true accuracy (estimated on independent hold-out sets). The
56.8% average estimate obtained on this balanced null dataset is an indicator of some
predictability of the class labels using given data, which is wrong as these are null
datasets. ‚ñ†
11.5 Nested Cross-Validation for Feature and Model Selection,
and Evaluation
As we saw in the previous section, the performance estimates obtained as part of the
grid search cross-validation can not be used for evaluating the performance of the
best classiÔ¨Åer. The question is then how to possibly estimate its performance? We
have used test sets in Example 11.4 for evaluating the best classiÔ¨Åer. In the absence
of an independent test set, one can split the given data to training and test sets and use
hold-out estimator. However, as discussed in Section 9.1.1, in situations where the
sample size is not large, cross-validation estimator is generally preferred with respect
to hold-out estimator. But can we use cross-validation to evaluate the performance of
the classiÔ¨Åer trained using the best combination of hyperparameter values obtained
by the grid search CV?

316
11 Assembling Various Learning Steps
The answer to this question is positive. To do so, we need to implement another
cross-validation that itself is external to the entire learning process; that is, external
to feature selection + model selection (and remember that we use feature selection
as a typical step in Œû). As this external cross-validation for model evaluation (or,
equivalently, learning process evaluation) is wrapped around the entire process of
feature selection + model selection, which can be implemented by grid search CV,
this practice is commonly referred to as nested cross-validation.
At this point it is important to distinguish between three types of cross-validation
that could be inside each other (although computationally it could be very expensive):
(1) a possible cross-validation that could be used in wrapper feature selection (inner
to all);
(2) a cross-validation used for grid-search model selection (external to CV used in
(1) but internal to the one used in (3)). In the context of nested cross-validation
though, this cross-validation is known as the internal CV; and
(3) a cross-validation used for evaluation (external to all). In the context of nested
cross-validation, this cross-validation is known as the external CV.
Nevertheless, for the sake of clarity, we could refer to the CVs used in (1), (2),
and (3) by their functionality; that is, ‚Äúfeature selection CV‚Äù, ‚Äúmodel selection CV‚Äù,
and ‚Äúmodel evaluation CV‚Äù, respectively.
At the same time, it is important to realize that by using nested cross-validation,
we are not changing the ‚Äúbest classiÔ¨Åer‚Äù; that is, the classiÔ¨Åer that was chosen by the
procedure described in Section 11.4 remains the same. What we are doing by nested
cross-validation is the way we evaluate that classiÔ¨Åer (or, equivalently, the learning
process leading to that classiÔ¨Åer). The scikit-learn implementation of nested CV is
left as an exercise.
Exercises:
Exercise 1: Here we would like to use the same data generating model and hyper-
parameter space that were used in Example 11.4. Keep all the following and other
parameters the same
n_features = 21050
n_sample = 500
ratio_train2test = 172/500
mc_no = 20
Apply nested cross-validation on X_train and y_train and report the average
estimate of accuracy obtained by that (recall that the average is over the number of
Monet-Carlo simulations: mc_no = 20). For both CVs used as part of the model
selection and the model evaluation, use stratiÔ¨Åed 5-fold CV. What is the average CV
estimate obtained using nested cross-validation? Is it closer to the 50% compared to
the what was achieved in Example 11.4?

11.5 Nested Cross-Validation for Feature and Model Selection, and Evaluation
317
Hint: in Example 11.4 after GridSearchCV is instantiated, use the object as estima-
tor used in cross_val_score.
Exercise 2: Suppose we we wish to use SelectKBest from scikit-learn in selecting
a set of 50 features, denoted f, in a dataset Str including 10,000 features, and train a
classiÔ¨Åer œà using Str with selected features. In training the classiÔ¨Åer, we use a pre-
speciÔ¨Åed combination of hyperparameter values; that is, no hyperparameter tuning
is used. We apply the 3-fold CV external to feature selection to properly evaluate the
trained classiÔ¨Åer. In the process of 3-fold CV, we obtained 3 feature sets denoted f1,
f2, and f3. Which of the following is correct about the relationship between f and
fk, k = 1, 2, 3 (two sets A and B are equal, denoted A = B, if and only if they contain
the same elements)?
A) f = f1 = f2 = f3
B) f , f1 = f2 = f3
C) f , f1 , f2 , f3
D) f = f1 ‚à™f2 ‚à™f3 and f1 = f2 = f3
E) f = f1 ‚à©f2 ‚à©f3 and f1 = f2 = f3
F) f = f1 ‚à™f2 ‚à™f3 and f1 , f2 , f3
G) f = f1 ‚à©f2 ‚à©f3 and f1 , f2 , f3
Exercise 3: In an application, ‚Äúour goal‚Äù is to perform feature selection, construct
a classiÔ¨Åer on the selected features, and estimate the error rate of the constructed
classiÔ¨Åer using cross-validation. To do so, we take the following ‚Äúaction‚Äù: perform
feature selection Ô¨Årst, construct a classiÔ¨Åer on the selected features, reduce the di-
mensionality of the data with the selected features (i.e., create a ‚Äúreduced data‚Äù
with selected features), and estimate the error rate of the constructed classiÔ¨Åer using
cross-validation (CV) applied on the reduced data. Which of the following statement
can best describe the legitimacy of this practice? Explain your rationale.
A) Both the goal and the action taken to achieve the goal are legitimate.
B) The goal is legitimate but the action taken to achieve the goal is not because the
use of CV in this way would make it even more pessimistically biased.
C) The goal is legitimate but the action taken to achieve the goal is not because the
use of CV in this way could make it even optimistically biased.

Chapter 13
Deep Learning with Keras-TensorFlow
Deep learning is a subÔ¨Åeld of machine learning and has been applied in various
tasks such as supervised, unsupervised, semi-supervised, and reinforcement learning.
Among all family of predictive models that are used in machine learning, by deep
learning we exclusively refer to a particular class of models known as multilayered
ArtiÔ¨Åcial Neural Network (ANN), which are partially inspired by our understanding
of biological neural circuits. Generally, an ANN is considered shallow if it has one or
two layers; otherwise, it is considered a deep network. Each layer in an ANN performs
some type of transformation of the input to the layer. As a result, multiple successive
layers in an ANN could potentially capture and mathematically represent complex
input-output relationship in a given data. In this chapter, we introduce some of the
key principles and practices used in learning deep neural networks. In this regard,
we use multi-layer perceptrons as a typical ANN and postpone other architectures to
later chapters. In terms of software, we switch to Keras with TensorFlow backend as
they are well-optimized for training and tuning various forms of ANN and support
various forms of hardware including CPU, GPU, or TPU.
13.1 ArtiÔ¨Åcial Neural Network, Deep Learning, and Multilayer
Perceptron
ANNs use a set of linear and nonlinear operations to implement a meaningful
mapping between a set of features and target variables. The number of layers of
successive transformations used in a network is referred to as its depth; thus, the
more layers, the deeper the network. The goal in deep learning is to use ANNs
of multiple layers that can extract underlying patterns of data at increasingly more
levels of abstraction. As a result, deep neural networks do not generally require a
separate feature extraction stage. Although integrating feature extraction, which is
generally domain-dependent, into the model construction phase is a considerable
advantage of deep learning, the price is generally paid in computationally intensive
model selection required for training deep neural networks.
351
¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
A. Zollanvari, Machine Learning with Python, https://doi.org/10.1007/978-3-031-33342-2_13 

352
13 Deep Learning with Keras-TensorFlow
Although the term ‚Äúdeep learning‚Äù was coined for the Ô¨Årst time by Rina Dechter
in 1986 (Dechter, 1986), the history of its development goes well beyond that point.
The Ô¨Årst attempt in mathematical representation of nervous system as a network
of (artiÔ¨Åcial) neurons was based on seminal work of McCulloch and Pitts in 1943
(McCulloch and Pitts, 1943). They showed that under certain conditions, any arbi-
trary Boolean function can be represented by a network of triggering units with a
threshold (neurons). Nevertheless, the strength of interconnections between neurons
(synaptic weight) were not ‚Äúlearned‚Äù in their networks and should have adjusted
manually. Rosenblatt studied similar network of threshold units in 1962 under the
name perceptrons (Rosenblatt, 1962). He proposed an algorithm to learn the weights
of perceptron. The Ô¨Årst attempts in successfully implementing and learning a deep
neural network can be found in studies conducted by Ivekhenko and his colleagues
in mid 60s and early 70s (Ivakhnenko and Lapa, 1965; Ivakhnenko, 1971). The work
of Ivekhenko et al. are essentially implementation of a multi-layer perceptron-type
network. A few years later, some other architectures, namely, Cognitron (Fukushima,
1975) and Neocognitron (Fukushima, 1980), were introduced and became the foun-
dation of modern convolutional neural network, which is itself one of the major
driving force behind the current momentum that we observe today in deep learning.
In this section, we introduce multilayer perceptron (MLP), which is one of the
basic forms of ANN with a long histroy and strong track record of success in
many applications. The structure of MLP is closely related to Kolmogorov‚ÄìArnold
representation theorem (KA theorem). The KA theorem states that we can write any
continous function f of p variables f : [0, 1]p ‚ÜíR as a Ô¨Ånite sum of composition
of univariate functions; that is to say, for f there exists univariate functions hi and
gij such that
f (x) =
2p+1
√ï
i=1
hi

p
√ï
j=1
gij(xj)

,
(13.1)
where x = [x1, x2, . . ., xp]T. However, KA theorem does not provide a recipe for the
form of hi and gij. Suppose we make choices as follows:
‚Ä¢ gij(xj) takes the following form:
gij(xj) = aij xj + ai0
p ,
j = 1, 2, . . ., p,
(13.2)
where ai0 (known as ‚Äúbiases‚Äù) and aij are some unknown constants.
‚Ä¢ hi(t) takes the following form:
hi(t) = biœÜ(t) + b0
k ,
i = 1, 2, . . ., k ,
(13.3)
where b0 (bias) and bi are some unknown constants and in contrast with (13.1),
rather than taking i from 1 to 2p + 1, we take it from 1 to k, which is an arbitrary

13.1 ArtiÔ¨Åcial Neural Network, Deep Learning, and Multilayer Perceptron
353
integer number. œÜ(t) is known as activation function and in contrast with hi in
(13.1), it does not depend on i. Common choices of œÜ(t) are described next.
‚Ä¢ An important and common family of œÜ(t) is sigmoid. A sigmoid is a nonde-
creasing function, having the property that limt‚Üí‚àûœÉ(t) = 1 and limt‚Üí‚àí‚àûœÉ(t)
is either 0 or -1. We have already seen logistic sigmoid in Section 6.2.2 in
connection with logistic regression‚Äîa subscript with the name is used to better
diÔ¨Äerentiate various sigmoid functions introduced next:
œÉlogistic(t) =
1
1 + e‚àít .
(13.4)
Another commonly used form of sigmoid function in the context of neural
network is hyperbolic tangent, which is given by
œÉtanh(t) = tanh(t) = sinh(t)
cosh(t) = et ‚àíe‚àít
et + e‚àít = 1 ‚àíe‚àí2t
1 + e‚àí2t = 2œÉlogistic(2t) ‚àí1 .
(13.5)
Although less common compared with the previous two sigmoids, tangent in-
verse is another example of sigmoid functions:
œÉarctanh(t) = 2
œÄ arctanh(t) .
(13.6)
An important non-sigmoid activation function is ReLU (short for RectiÔ¨Åed
Linear Unit) given by
œÜReLU(t) = max(0, t) .
(13.7)
Replacing (13.2) and (13.3) in (13.1) yields:
f (x) ‚âàb0 +
k
√ï
i=1
biœÜ

ai0 +
p
√ï
j=1
aij xj

,
(13.8)
which can equivalently be written as
f (x) ‚âàb0 +
k
√ï
i=1
biui ,
(13.9)
ui = œÜ(ai0 +
p
√ï
j=1
aij xj) .
(13.10)

354
13 Deep Learning with Keras-TensorFlow
In a supervised learning problem in which our goal is to estimate y, which is
the target value for a given x, we can take f (x) in (13.8) as the estimate of y. This
is indeed the mathematical machinery that maps an input to its output in a MLP
with one hidden layer. A graphical representation of this mapping is depicted in Fig.
13.1. The hidden layer is part of the network that is neither directly connected to the
inputs nor to the output (its connection is through some weights). In other words,
the ‚Äúhidden‚Äù layer, as the name suggests, is hidden from outside world in which we
only see the input and the output.
To better resemble the similarity between a biological neuron and an artiÔ¨Åcial
neuron, a biological neuron is overlaid on top of the network depicted in Fig. 13.1.
Although we do this to draw the analogy between ANN and the biological network of
neurons, it would be naive to think of ANNs as in silico implementation of biological
neural networks!
ùë•!
ùë•"
.
.
.
ùëé!!
ùëé!"
ùëé#!
ùëé#"
.
..
.
..
‚àë
‚àÖùë°
ùëé!$
‚àë
ùëè$
ùëè!
ùëè#
ùë¶
.
.
.
‚àë
ùëé#$
Hidden Layer
A biological neuron
Input Layer
Output Layer
An artificial neuron
‚àÖùë°
Fig. 13.1: MLP with one hidden layer and k neurons. This is known as a three-layer
network with one input layer, one hidden layer, and one output layer.
As observed in Fig. 13.1 (also see (13.8)), each input feature is multiplied by a
coeÔ¨Écient and becomes part of the input to each of the k neurons in the hidden layer.
The output of these neurons are also multiplied by some weights, which, in turn, are
used as inputs to the output layer. Although in Fig. 13.1, we take y as the sum of
inputs to the single neuron in that layer, we may add a sigmoid and/or a threshold
function used after the summation if classiÔ¨Åcation is desired. Furthermore, the output
layer itself can have a collection of artiÔ¨Åcial neurons, for example, if multiple target
variables should be estimated for a given input.

13.1 ArtiÔ¨Åcial Neural Network, Deep Learning, and Multilayer Perceptron
355
The network in Fig. 13.1 is also known as a three-layer MLP because of the three
layers used in is structure: 1) the input layer; 2) the hidden layer; and 3) the output
layer. A special case of this network is where there is no hidden layer and each input
feature is multiplied by a weight and is used as the input to the neuron in the output
layer. That leads to the structure depicted in Fig 13.2, which is known as perceptron.
ùë•!
ùëé!
‚àë
ùëé"
ùë•#
ùë¶
‚àÖùë°
ùëé#
.
.
.
.
.
Fig. 13.2: The structure of a perceptron
There are two important diÔ¨Äerences between (13.8) and (13.1) that are highlighted
here:
1. In (13.1), the choices of hi and gij depend on the function f (x); however, in
(13.8), they are our choices presented in (13.2) and (13.3); and
2. As we impose our choices of hi and gij presented in (13.2) and (13.3), the
expression in (13.1) does not hold exactly. This is the reason we use ‚âàin
(13.8). However, in 1989, in a seminal work, Cybenko showed that using (13.8),
‚Äúarbitrary decision regions can be arbitrarily well approximated‚Äù (Cybenko,
1989). This is known as the universal expressive power of a three-layer MLP.
The input-output mapping presented above through one hidden layer can be
extended to as many layers as desired. For example, the input-output mapping in an
MLP with 2 hidden layers (a four-layer network) is presented by the following set of
equations:
f (x) ‚âàc0 +
k2
√ï
l=1
clul ,
(13.11)
ul = œÜ(bl0 +
k1
√ï
i=1
blivi)
1 ‚â§l ‚â§k2 ,
(13.12)
vi = œÜ(ai0 +
p
√ï
j=1
aij xj)
1 ‚â§i ‚â§k1 .
(13.13)

356
13 Deep Learning with Keras-TensorFlow
Fig. 13.3 presents a graphical representation of a four-layer MLP characterized by
(13.11)-(13.13).
ùë•!
ùë•"
.
.
.
ùëé!!
ùëé!"
ùëé!!"
ùëé!!#
.
..
.
..
‚àë
ùëé!#
‚àë
ùëê#
ùëê!
ùëê$$
ùë¶
.
.
.
‚àë
ùëé!!%
1st Hidden Layer
Input Layer
Output 
Layer
...
ùëè!!
ùëè!"!
‚àë
ùëè!#
‚àë
...
ùëè!""
ùëè!"!!
ùëè!"%
.
.
.
2nd Hidden Layer
‚àÖùë°
‚àÖùë°
‚àÖùë°
‚àÖùë°
Fig. 13.3: MLP with two hidden layers and k1 and k2 neurons in the Ô¨Årst and second
hidden layers, respectively.
layers (linear 
and nonlinear 
operations)
predicted target
actual target
loss 
function
loss score
updated 
weights
optimizer
observation
backprop
Fig. 13.4: The general machinery of training an ANN.

13.2 Backpropagation, Optimizer, Batch Size, and Epoch
357
13.2 Backpropagation, Optimizer, Batch Size, and Epoch
So far we did not specify how the unknown weights in an MLP are adjusted. As in
many other learning algorithms, these weights are estimated from training data in
the training phase. In this regard, we use the iterative gradient descent process that
uses steepest descent direction to update the weights. Fig. 13.4 presents a schematic
description of this training process. In particular, the current weights of the network,
which are initially assigned to some random values, result in a predicted target. The
distance between the predicted target from the actual target is measured using a loss
function and this distance is used as a feedback signal to adjust the weights.
To Ô¨Ånd the steepest descent direction, we need the gradient of the loss function
with respect to weights. This is done eÔ¨Éciently using a two-stage algorithm known
as backpropagation (also referred to as backprop), which is essentially based on
chain rules of calculus. The two stages in backprop include some computations
from the Ô¨Årst to the last layer of the network (the Ô¨Årst stage: forward computations)
and some computations from the last layer to the Ô¨Årst (the second stage: backward
computations). Once the gradient is found using backprop, the adjustment to weights
in the direction of steepest descent is performed using an optimizer; that is to say,
the optimizer is the update rule for the weight estimates. There are two ways that we
can update the weights in the aforementioned process:
1. We can compute the value of the gradient of loss function across all training
data, and then update the weights. This is known as batch gradient descent.
However, for a single update, all training data should be used in the backprop
and the optimizer, and then take one single step in the direction of steepest
descent. At the same time, a single presentation of the entire training set to the
backprop is called an epoch. Therefore, in the batch gradient descent, one update
for the weights occurs at the end of each epoch. In other words, the number of
epochs is an indicator of the amount of times training data is presented to the
network, which is perceived as the amount of training the ANN. The number
of epochs is generally considered as a hyperparameter and should be tuned in
model selection.
2. We can compute the value of the gradient of loss function for a mini training
data (a small subset of training data), and then update the weights. This is known
as mini-batch gradient descent. Nevertheless, the concept of epoch is still the
same: a single presentation of the entire training set to the backprop. This means
that if we partition a training data of size n into mini-batches of size K << n,
then in an epoch we will have ‚åàn/K‚åâupdates for weights. In this context, the
size of each mini-batch is called batch size. A good choice, for example, is 32
but in general it is a hyperparameter and should be tuned in model selection.

358
13 Deep Learning with Keras-TensorFlow
13.3 Why Keras?
Although we can use scikit-learn to train certain types of ANNs, we will not use it
for that purpose because:
1. training ‚Äúdeep‚Äù neural networks requires estimating and adjusting many param-
eters and hyperparameters, which is a computationally expensive process. As a
result successful training various forms of neural networks highly rely on paral-
lel computations, which are realized using Graphical Processing Units (GPUs)
or Tensor Processing Units (TPUs). However, scikit-learn currently does not
support using GPU or TPU.
2. scikit-learn does not currently implement some popular forms of neural networks
such as convolutional neural networks or recurrent neural networks.
As a result, we switch to Keras with TensorFlow backend as they are particularly
designed for training and tuning various forms of ANN and support various forms
of hardware including CPU, GPU, or TPU.
As previously mentioned in Chapter 1, simplicity and readability along with a
number of third-party packages have made Python as the most popular programming
language for data science. When it comes to deep learning, Keras library has become
such a popular API to the extent that its inventor, Francois Chollet, refers to Keras
as ‚Äúthe Python of deep learning‚Äù (Chollet, 2021). In a recent survey conducted by
Kaggle in 2021 on ‚ÄúState of Data Science and Machine Learning‚Äù (Kaggle-survey,
2021), Keras usage among data scientists was ranked fourth (47.3%) only preceded
by xgboost library (47.9%), TensorFlow (52.6%), and scikit-learn (82.3%). But what
is Keras?
Keras is a deep learning API written in Python. It provides high-level ‚Äúbuilding
blocks‚Äù for eÔ¨Éciently training, tuning, and evaluating deep neural networks. How-
ever, Keras relies on a backend engine, which does all the low-level operations such
as tensor products, diÔ¨Äerentiation, and convolution. Keras was originaly released to
support Theano backend, which was developed at Universit√© de Montr√©al. Several
months later when TensorFlow was relased by google, Keras was refractored to
support both Theano and TensorFlow. In 2017, Keras was even supporting some
additional backends such as CNTK and MXNet. Later in 2018, TensorFlow adopted
Keras as its high-level API. In September 2019, TensorFlow 2.0 was released and
Keras 2.3 became its last multi-backend release and it was announced that: ‚ÄúGoing
forward, we recommend that users consider switching their Keras code to tf.keras
in TensorFlow 2.0. It implements the same Keras 2.3.0 API (so switching should be
as easy as changing the Keras import statements)‚Äù (Keras-team, 2019).
If Anaconda was installed as explained in Section 2.1, TensorFlow can easily be
installed using conda by executing the following command line in a terminal (Tensor-
Ô¨Çow, 2023): conda install -c conda-forge tensorflow. We can also check
their version as follows:
import tensorflow as tf
from tensorflow import keras

13.4 Google Colaboratory (Colab)
359
print(tf.__version__)
print(keras.__version__)
13.4 Google Colaboratory (Colab)
As mentioned earlier, for training deep neural networks it is recommended to use
GPU or TPU cards. However, one may not have access to such hardware locally. In
such a case, we can work on Google colaboratory (Colab), which gives access to a
limited free GPU and TPU runtime. Colab is like a Jupyter notebook service stored
on google drive that connects a notebook to cloud-based run time. In other words,
Colab notebook is the cloud-based google implementation of Jupyter notebook.
Therefore, we should be able to run any code that runs locally in Jupter notebook on
Colab, if all packages are installed. At the same time, Colab currently comes with
TensorFlow, Keras, Scikit-Learn, numpy, pandas, and many more, which means we
have already access to a variety of useful Python packages and there is no need to
install them separately. However, for installing any package, if needed, we can run:
!pip install PACKAGE_NAME in a code cell in Colab (‚Äú!‚Äù is used because it is a
command line rather than Python code).
To open a Colab notebook, we can go to Google Drive and open a notebook as
in Fig. 13.5. Once a Colab notebook is opened, we should see an empty notebook
as in Fig. 13.6. This is where codes are written and executed. As seen on top of Fig.
13.6, similar to a Jupyter notebook, a Colab notebook has two types of cells (Code
and Text) as well.
As of this writing, the version of TensorFlow, Keras, and Scikit-Learn that are
already pre-installed in Colab are (the code snippet is run on Colab):
import tensorflow as tf
from tensorflow import keras
import sklearn
print(tf.__version__)
print(keras.__version__)
print(sklearn.__version__)
2.9.2
2.9.0
1.0.2
By default, running codes in Colab is done over CPUs provided by Google servers.
However, to utilize GPU or even TPU, we can go to ‚ÄúRuntime ‚Äì> Change runtime
type‚Äù and choose GPU/TPU as the Hardware accelerator (see Fig. 13.7). However,
if we want to train ANN models on TPU, it requires some extra work, which is not
needed when GPU is used. As a result of that, and as for our purpose here (and

360
13 Deep Learning with Keras-TensorFlow
Fig. 13.5: Opening a Colab notebook from Google Drive.
Fig. 13.6: An empty Colab notebook

13.5 The First Application Using Keras
361
in many other applications) the use of GPU suÔ¨Éce the need, we skip those details
(details for that purpose are found at (TensorÔ¨Çow-tpu, 2023)).
Fig. 13.7: Using a hardware accelerator
Once a GPU is selected as the Hardware accelerator, we can ensure the GPU is used
as follows:
import tensorflow as tf
print("Num GPUs Available: ", len(tf.config.
,‚Üílist_physical_devices('GPU')))
Num GPUs Available:
1
13.5 The First Application Using Keras
In this section, we discuss building, compiling, training, and evaluating ANNs in
Keras. To better clarify the concepts, we use a well-known dataset known as MNIST.
13.5.1 ClassiÔ¨Åcation of Handwritten Digits: MNIST Dataset
The MNIST dataset has a training set of 60,000 grayscale 28 √ó 28 images of hand-
written digits, and an additional test set of size 10,000. It is part of keras.dataset
module and can be loaded as four numpy arrays as follows (in the code below and to
have reproducible results as we proceed with this application, we also set the random
seeds for Python, NumPy, and TensorFlow):

362
13 Deep Learning with Keras-TensorFlow
from tensorflow.keras.datasets import mnist
seed_value= 42
# set the seed for Python built-in pseudo-random generator
import random
random.seed(seed_value)
# set the seed for numpy pseudo-random generator
import numpy as np
np.random.seed(seed_value)
# set the seed for tensorflow pseudo-random generator
import tensorflow as tf
tf.random.set_seed(seed_value)
(X_train_val, y_train_val), (X_test, y_test) = mnist.load_data()
print(type(X_train_val))
print(X_train_val.dtype)
print(X_train_val.shape)
print(X_train_val.ndim)
<class 'numpy.ndarray'>
uint8
(60000, 28, 28)
3
As can be seen the data type is unit8, which is unsigned integers from 0 to 255.
Before proceeding any further, we Ô¨Årst scale down the data to [0, 1] by dividing
each entry by 255‚Äîhere rescaling is not data-driven and we use prior knowledge in
doing so. The default data type of such a division would be double precision. As the
memory itself is an important factor in training deep ANNs, we also change the data
type to float32.
X_train_val, X_test = X_train_val.astype('float32')/255, X_test.
,‚Üíastype('float32')/255
print(X_train_val.dtype)
float32
Also notice the shape of X_train_val as a three-dimensional numpy array,
which is also known as a rank-3 tensor. To train a multilayer perceptron, the model
works on a feature vector (not an image), which is similar to models that we have
seen previously in scikit-learn. This means that the model expects a rank-2 tensor,
which is an array of (sample size, feature size). To do so, we reshape the arrays:
X_train_val, X_test = X_train_val.reshape(60000, 28*28), X_test.
,‚Üíreshape(10000, 28*28)
X_train_val[0].shape

13.5 The First Application Using Keras
363
(784,)
For the model selection purposes, we randomly split X_train_val into training and
validation sets:
from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(X_train_val,‚ê£
,‚Üíy_train_val, stratify=y_train_val, test_size=0.25)
X_train.shape
(45000, 784)
13.5.2 Building Model Structure in Keras
All models in Keras are objects of tf.keras.Model class. There are three ways to
build object of this class (to which we simply refer as models): 1) Sequential API;
2) Functional API; and 3) Subclassing API. Sequential models are used when stack
of standard layers is desired where each layer has one input tensor and one output
tensor. This is what we are going to use hereafter. The functional and subclassing
are used for cases when multiple inputs/outputs (for example to locate and classify
an object in an image) or more complex uncommon architectures are desired. There
are two ways to use Sequential models:
‚Ä¢ Method 1: pass a list of layers to keras.Sequential class constructor to
instantiate.
‚Ä¢ Method 2: instantiate keras.Sequential Ô¨Årst with no layer and then use add()
method to add layers one after another.
There are many layers available in keras.layers API. Each layer consists of a
recipe for mapping the input tensor to the output tensor. At the same time, many lay-
ers have state, which means some weights learned using a given data by the learning
algorithm. However, some layers are stateless‚Äîthey are only used for reshaping an
input tensor. A complete list of layers in Keras can be found at (Keras-layers, 2023).
Next we examine the aforementioned two ways to build a Sequential model.
Method 1:
from tensorflow import keras
from tensorflow.keras import layers
mnist_model = keras.Sequential([
layers.Dense(128, activation="sigmoid"),
layers.Dense(64, activation="sigmoid"),
layers.Dense(10, activation="softmax")
])

364
13 Deep Learning with Keras-TensorFlow
# mnist_model.weights # if we uncomment this code and try to run it at‚ê£
,‚Üíthis stage, an error is raised because the weights are not set (the‚ê£
,‚Üímodel is not actually built yet)
Method 2:
from tensorflow import keras
from tensorflow.keras import layers
mnist_model = keras.Sequential()
mnist_model.add(layers.Dense(128, activation="sigmoid"))
mnist_model.add(layers.Dense(64, activation="sigmoid"))
layers.Dense(10, activation="softmax")
<tensorflow.python.keras.layers.core.Dense at 0x7fab7e4ec730>
Let us elaborate on this code. A Dense layer, also known as fully connected layer,
is indeed the same hidden layers that we have already seen in Figs. 13.1 and 13.3
for MLP. They are called fully connected layers because every input to a layer is
connected to every neuron in the layer. In other words, the inputs to each layer, which
could be the outputs of neurons in the previous layer, go to every neuron in the layer.
While the use of activation functions (here logistic sigmoid) in both hidden layers
is based on (13.12) and (13.13), the use of softmax in the last layer is due to the
multiclass (single label) nature of this classiÔ¨Åcation problem. To understand this,
Ô¨Årst note that the number of neurons in the output layer is the same as the number of
digits (classes). This means that for each input feature vector x, the model will output
a c-dimensional vector (here c = 10). The softmax function takes this c-dimensional
vector, say a = [a0, a1, ..., a(c‚àí1)]T, and normalized that into a probability vector
softmax(a) = p = [p0, p1, . . ., p(c‚àí1)]T (which should add up to 1) according to:
pi =
eai
√ç
i eai ,
i = 0, . . ., c ‚àí1 .
(13.14)
Then an input is classiÔ¨Åed to the class with the highest probability.
The codes presented in Method 1 or Method 2 do not build the model in the strict
sense. This is because ‚Äúbuilding‚Äù a model means setting the weights of layers, but
the number of weights depends on the input size, which is not given in the above
codes. In that case, the model will be built as soon as it is called on some data (for
example, using fit() method). Nevertheless, we can also ‚Äúbuild‚Äù the model using
both Method 1 and 2 if we just specify the shape of each input using input_shape
parameter:
from tensorflow import keras
from tensorflow.keras import layers
mnist_model = keras.Sequential([

13.5 The First Application Using Keras
365
layers.Dense(128, activation="sigmoid",‚ê£
,‚Üíinput_shape = X_train[0].shape),
layers.Dense(64, activation="sigmoid"),
layers.Dense(10, activation="softmax")
])
mnist_model.weights # the weights are initialized (the biases to 0 and‚ê£
,‚Üíthe rest randomly)
[<tf.Variable 'dense_237/kernel:0' shape=(784, 128) dtype=float32, numpy=
array([[ 0.0266955 , -0.00956997, -0.02386561, ..., -0.0407474 ,
0.01480078, -0.0003909 ],
[ 0.0645309 , -0.06136192, -0.05915052, ...,
0.00383252,
0.05406239,
0.0454815 ],
...,
...,
[-1.50822967e-01,
6.51585162e-02,
1.69166803e-01,
-1.23789206e-01,
1.93499446e-01,
2.29302317e-01,
2.81352669e-01, -2.16220707e-01, -5.38927913e-02,
-2.35425845e-01]], dtype=float32)>,
<tf.Variable 'dense_239/bias:0' shape=(10,) dtype=float32,‚ê£
,‚Üínumpy=array([0., 0.,
0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]
We can use summary() method that tabulates the shape of output and the total
number of parameters that are/should be estimated in each layer.
mnist_model.summary()
Model: "sequential_76"
_________________________________________________________________
Layer (type)
Output Shape
Param #
=================================================================
dense_237 (Dense)
(None, 128)
100480
_________________________________________________________________
dense_238 (Dense)
(None, 64)
8256
_________________________________________________________________
dense_239 (Dense)
(None, 10)
650
=================================================================
Total params: 109,386
Trainable params: 109,386
Non-trainable params: 0
_________________________________________________________________
In Section 13.5.1 we reshaped the rank-3 input tensor to make it in the
form of (samples, features). To do so, we Ô¨Çattended each image. Another
way is to add a Ô¨Çattning layer as the Ô¨Årst layer of the network and use the

366
13 Deep Learning with Keras-TensorFlow
same rank-3 tensor as the input (without reshaping). In that case the model
is deÔ¨Åned as follows:
# mnist_model = keras.Sequential([
#
layers.Flatten(),
#
layers.Dense(128,‚ê£
,‚Üíactivation="sigmoid"),
#
layers.Dense(64,‚ê£
,‚Üíactivation="sigmoid"),
#
layers.Dense(10, activation="softmax")
#
]) # here we have not specified the‚ê£
,‚Üíinput_shape but we can do so by layers.
,‚ÜíFlatten(input_shape=(28,28))
13.5.3 Compiling: optimizer, metrics, and loss
After building the model structure in Keras, we need to compile the model. Compiling
in Keras is the process of conÔ¨Åguring the learning process via compile() method
of keras.Model class. In this regard, we set the choice of optimizer, loss function,
and metrics to be computed through the learning process by setting the optimizer,
loss, and metrics parameters of compile() method, respectively. Here we list
some possible common choices for classiÔ¨Åcation and regression:
1. optimizer can be set by replacing name in optimizer = name, for exam-
ple, with one of the following strings where each refers to an optimizer known
with the same name: adam, rmsprop, sgd, or adagrad (for a complete list
of optimizers supported in Keras see (Keras-optimizers, 2023)). Doing so
uses default values of parameters in the class implementing each optimizer.
If we want to have more control over the parameters of an optimizer, we need
to set the optimizer parameter to an object of optimizer class; for exam-
ple, optimize = keras.optimizers.Adam(learning_rate=0.01) using
which we are changing the 0.001 default value of learning_rate used in Adam
optimizer to 0.01.
2. metrics can be set by replacing name(s) in metrics = [name(s)] with
a list of strings where each refer to a metric with the same name; for exam-
ple, accuracy (for classiÔ¨Åcation), or mean_squared_error (for regression).
A more Ô¨Çexible way is to use the class implementing the metric; for exam-
ple, metrics = ["mean_squared_error", "mean_absolute_error"] is
equivalent to
metrics = [keras.metrics.MeanSquaredError(),
keras.metrics.MeanAbsoluteError()]
For a complete list of metrics refer to (Keras-metrics, 2023).

13.5 The First Application Using Keras
367
3. loss can be set by replacing name in loss = name with one of the following
strings where each refer to a loss known with the same name:
‚Ä¢ binary_crossentropy (for binary classiÔ¨Åcation as well as multilabel clas-
siÔ¨Åcation, which more than one label is assigned to an instance);
‚Ä¢ categorical_crossentropy (for multiclass classiÔ¨Åcation when target
values are one-hot encoded);
‚Ä¢ sparse_categorical_crossentropy (for multiclass classiÔ¨Åcation when
target values are integer encoded),
‚Ä¢ mean_squared_error and mean_absolute_error for regression.
For a complete list of losses supported in Keras see (Keras-losses, 2023). We
can also pass an object of the class implementing the loss function; for exam-
ple, loss = keras.losses.BinaryCrossentropy() is equivalent to loss
= binary_crossentropy.
In case of binary classiÔ¨Åcation (single-label) and assuming the labels are already
encoded as integers 0 and 1, if we use binary_crossentropy, in the last layer we
should use one neuron and sigmoid as the activation function. The output of activa-
tion function in the last layer in this case is the probability of class 1. Alternatively, it
can also be treated as a multi-class classiÔ¨Åcation and use two neurons in the last layer
with a softmax activation and then use sparse_categorical_crossentropy. In
this case, the output of the last layer is the probability of classes 0 and 1. Here are
examples to see this equivalence:
true_classes = [0, 1, 1] # actual labels
class_probs_per_obs = [[0.9, 0.1], [0.2, 0.8], [0.4, 0.6]] # the‚ê£
,‚Üíoutputs of softmax for two neurons
scce = tf.keras.losses.SparseCategoricalCrossentropy()
scce(true_classes, class_probs_per_obs).numpy()
0.27977657
true_classes = [0, 1, 1]
class_1_prob_per_obs = [0.1, 0.8, 0.6] # the outputs of sigmoid for one‚ê£
,‚Üíneuron
bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)
bce(true_classes, class_1_prob_per_obs).numpy()
0.2797764
Q1. The choice of the loss function depends on encoding schemes.
What is an encoding scheme?
Q2. What is the diÔ¨Äerence between integer encoding and one-hot encoding?
Q3. If we care about a metric such as accuracy to be monitored during
training, why don‚Äôt we use its corresponding cost/loss function error =

368
13 Deep Learning with Keras-TensorFlow
1 ‚àíaccuracy in the backprop? Why do we use a loss function such as
categorical cross-entropy to serve as a proxy for having a better predictive
performance?
Answer to Q1: Categorical data in real-world applications are ubiquitous
but, on the contrary, the algorithms at the core of various popular machine
learning methods such as ANNs are based on the assumption of having
numerical variables. As a result, various encoding schemes are created to
transform categorical variables to their numerical counterparts. In case of
classiÔ¨Åcation, the target (class label) is genereally a nominal variable (i.e.,
a categorical variable with no natural ordering between its values). As a
result, the use of encoding along with the class label is commonplace (see
Exercise 3, Chapter 4).
Answer to Q2: Integer encoding, also known as ordinal encoding, treats un-
ordered categorical values as if they were numeric integers. For example,
a class label, which could be either ‚ÄúRed‚Äù, ‚ÄúBlue‚Äù, and ‚ÄúGreen‚Äù is treated
as 0, 1, and 2. This type of encoding raises questions regarding the rela-
tive magnitudes assigned to categories after numeric transformations; after
all, categorical features have no relative ‚Äúmagnitude‚Äù but numbers do. On
the other hand, one-hot encoding replaces one categorical variable with N
possible categories with standard basis vectors of N dimensional Euclidean
vector space. For example, ‚ÄúRed‚Äù, ‚ÄúBlue‚Äù, and ‚ÄúGreen‚Äù are replaced by
[1, 0, 0]T, [0, 1, 0]T, and [0, 0, 1]T, respectively. As a result, these orthogonal
binary vectors are equidistant with no speciÔ¨Åc order, which compared with
the outcomes of integer encoding, align better with the intuitive meaning of
categorical variables. However, because each utilized standard basis vector
lives in an N dimensional vector space, one-hot encoding can blow up the
size of data matrix if many variables with many categories are present.
Answer to Q3: The answer to this question lies in the nature of backprop.
As we said before, in backprop we eÔ¨Éciently evaluate the steepest descent
direction for which we need the gradient of the loss function with respect
to weights. However, using a loss such as 1 ‚àíaccuracy is not suitable for
this purpose because this is a piecewise constant function of weights, which
means the gradient is either zero (the weights do not move) or undeÔ¨Åned.
Therefore, other loss functions such as categorical_crossentropy for
classiÔ¨Åciation is commonly used. The use of mean_absolute_error loss
function for regression is justiÔ¨Åed by deÔ¨Åning its derivative at zero as a
constant such as zero.
We now compile the previously built mnist_model. Although the labels stored
in y_train, y_val, and y_test are already integer encoded (and we can use
sparse_categorical_crossentropy to compile the model), here we show
the utility of to_categorical() for one-hot encoding and, therefore, we use
categorical_crossentropy to compile the model:

13.5 The First Application Using Keras
369
mnist_model.compile(optimizer="adam", loss="categorical_crossentropy",‚ê£
,‚Üímetrics=["accuracy"])
y_train_1_hot = keras.utils.to_categorical(y_train, num_classes = 10)
y_val_1_hot = keras.utils.to_categorical(y_val, num_classes = 10)
y_test_1_hot = keras.utils.to_categorical(y_test, num_classes = 10)
print(y_train.shape)
print(y_train_1_hot.shape)
(45000,)
(45000, 10)
√â
categorical_crossentropy and sparse_categorical_crossentropy:
As we said before, depending on whether we use one-hot encoded labels or integer
encoded labels, we need to use either the categorical_crossentropy or the
sparse_categorical_crossentropy. But what is the diÔ¨Äerence between them?
Suppose for a multiclass (single label) classiÔ¨Åcation with c classes, a training
data Str = {(x1, y1), (x2, y2), ..., (xn, yn)} is available. Applying one-hot encoding on
yj, j = 1, . . ., n, leads to label vectors yj = [yj0, ..., yj(c‚àí1)] where yji ‚àà{0, 1}, i =
0, . . ., c ‚àí1. Categorical cross-entropy (also known as cross-entropy) is deÔ¨Åned as
e(Œ∏) = ‚àí1
n
n
√ï
j=1
c‚àí1
√ï
i=0
yjilog P(Yj = i|xj; Œ∏) ,
(13.15)
where Œ∏ denotes ANN parameters (weights). Comparing this expression with the
cross-entropy expression presented in Eq. (6.39) in Section 6.2.2 shows that (13.15)
is indeed the extension of the expression presented there to multiclass classiÔ¨Åcation
with one-hot encoded labels. Furthermore, P(Yj = i|xj; Œ∏) denotes the probability
that a given observation xj belongs to class i. In multiclass classiÔ¨Åcation using ANN,
we take this probability as the output of the softmax function when xj is used as the
input to the network; that is to say, we estimate P(Yj = i|xj; Œ∏) by the value of pi
deÔ¨Åned in (13.14) for the given observation. Therefore, we can rewrite (13.15) as:
e(Œ∏) = ‚àí1
n
n
√ï
j=1
yT
j log pj
 ,
(13.16)
where here log(.) is applied element-wise and pj = [pj0, pj1, . . ., pj(c‚àí1)]T is the
vector output of softmax for a given input xj. This is what is implemented by
categorical_crossentropy. The sparse_categorical_crossentropy im-
plements a similar function as in (13.16) except that rather than one-hot encoded
labels, it expects integer encoding with labels yj in the range of 0, 1, . . ., c ‚àí1. This
way, we can write (sparse) cross-entropy as:

370
13 Deep Learning with Keras-TensorFlow
e(Œ∏) = ‚àí1
n
n
√ï
j=1
log pjyj
 .
(13.17)
Next we present an example to better understand the calculation of cross-entropy.
Example 13.1 Suppose we have a class label with possible values, Red, Blue,
and Green. We use one-hot encoding and represent them as Red : [1, 0, 0]T,
Blue : [0, 1, 0]T, and Green : [0, 0, 1]T. Two observations x1 and x2 are presented
to an ANN with some values for its weights. The true classes of x1 and x2 are
Green and Blue, respectively. The output of the network after the softmax for x1 is
[0.1, 0.4, 0.5]T and its output for x2 is [0.05, 0.8, 0.15]T. What is the cross-entropy?
The classes of x1 and x2 are [0, 0, 1]T and [0, 1, 0]T, respectively; that is, y1 =
[0, 0, 1]T and y2 = [0, 1, 0]T. At the same time, p1 = [0.1, 0.4, 0.5]T and p2 =
[0.05, 0.8, 0.15]T. Therefore, (13.16) yields
e(Œ∏) = ‚àí1
2
 yT
1 log(p1) + yT
2 log(p2)
= ‚àí1
2
 0 √ó log(p10) + 0 √ó log(p11) + 1 √ó log(p12)
+ 0 √ó log(p20) + 1 √ó log(p21) + 0 √ó log(p22)
= ‚àí1
2
 log(0.5) + log(0.8) = 0.458
(13.18)
We now use keras.losses.CategoricalCrossentropy() to compute this:
from tensorflow import keras
labels_one_hot = [[0, 0, 1], [0, 1, 0]]
class_probs_per_obs = [[0.1, 0.4, 0.5], [0.05, 0.8, 0.15]]
cce = keras.losses.CategoricalCrossentropy()
cce(labels_one_hot, class_probs_per_obs).numpy()
0.45814535
For the sparse categorical cross-entropy, the labels are expected to be integer
encoded; that is to say, y1 = 2 and y2 = 1. Therefore, we can pick the corresponding
probabilities at index 2 and 1 (assuming the Ô¨Årst element in each probability vector
has index 0) from p1 and p2, respectively, to write
e(Œ∏) = ‚àí1
2
 log(0.5) + log(0.8) = 0.458
(13.19)
We can also use keras.losses.SparseCategoricalCrossentropy() to com-
pute this:
labels = [2, 1]
class_probs_per_obs = [[0.1, 0.4, 0.5], [0.05, 0.8, 0.15]]

13.5 The First Application Using Keras
371
scce = keras.losses.SparseCategoricalCrossentropy()
scce(labels, class_probs_per_obs).numpy()
0.45814538
As we can see in both cases (using categorical cross-entropy with one-hot encoded
labels and sparse categorical cross-entropy with integer encoded labels), the loss is
the same.
‚ñ†
‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-
13.5.4 Fitting
Once a model is compiled, training the model is performed by calling the fit()
method of keras.Model class. There are several important paramaters of the fit()
method that warrant particular attention:
1. x: represents the training data matrix;
2. y: represents the training target values;
3. batch_size: can be used to set the batch size used in the mini-batch gradient
descent (see Section 13.2);
4. epochs: can be used to set the number of epochs to train the model (see Section
13.2);
5. validation_data: can be used to specify the validation set and is generally in
the form of (x_val, y_val);
6. steps_per_epoch: this is used to set the number of batches processed per
epoch. The default value is None, which for a batch size of K, sets the number
of batches per epoch to the conventional value of ‚åàn/K‚åâwhere n is the sample
size. This option would be helpful, for example, when n is such a large number
that each epoch may take a long time if ‚åàn/K‚åâis used;
7. callbacks_list: a list of callback objects. A callback is an object that is called
at various stages of training to take an action. There are several callbacks imple-
mented in Keras but a combination of the following two is quite handy in many
cases (each of these accepts multiple parameters [see their documentations] but
here we present them along with few of their useful parameters):
‚Ä¢ keras.callbacks.EarlyStopping(monitor="val_loss",patience=0)
Using this callback, we stop training after a monitored metric (monitor param-
eter) does not improve after a certain number of epochs (patience). Why do
we need to stop training? On one hand, training an ANN with many epochs may
lead to overÔ¨Åtting on the training data. On the other hand, too few epochs could
lead to underÔ¨Åtting, which is the situation where the model is not capable of

372
13 Deep Learning with Keras-TensorFlow
capturing the relationship between input and output even on training data. Using
early stopping we can set a large number of epochs that could be potentially used
for training the ANN but stop the training process when no improvement in the
monitored metric is observed after a certain number of epochs.
‚Ä¢ keras.callbacks.ModelCheckpoint(filepath="file_path.keras",
monitor="val_loss", save_best_only=False, save_freq="epoch")
By default, this callback saves the model at the end of each epoch. This default
behaviour could be changed by setting save_freq to an integer in which case
the model is saved at the end of that many batches. Furthermore, we often desire
to save the best model at the end of epoch if the monitored metric (monitor) is
improved. This behaviour is achieved by save_best_only=True in which case
the saved model will be overwritten only if the monitor metric is improved. We
can also set verbose to 1 to see additional information about when the model
is/is not saved.
Here we Ô¨Årst prepare a list of callbacks and then train our previously compiled model:
import time
my_callbacks = [
keras.callbacks.EarlyStopping(
monitor="val_accuracy",
patience=20),
keras.callbacks.ModelCheckpoint(
filepath="model/best_model.keras",
monitor="val_loss",
save_best_only=True,
verbose=1)
]
start = time.time()
history = mnist_model.fit(x = X_train,
y = y_train_1_hot,
batch_size = 32,
epochs = 200,
validation_data = (X_val, y_val_1_hot),
callbacks = my_callbacks)
end = time.time()
training_duration = end - start
print("training duration = {:.3f}".format(training_duration))
Epoch 1/200
1407/1407 [==============================] - 2s 1ms/step - loss: 0.9783 -
accuracy: 0.7587 - val_loss: 0.2579 - val_accuracy: 0.9225
Epoch 00001: val_loss improved from inf to 0.25790, saving model to

13.5 The First Application Using Keras
373
model/best_model.keras
Epoch 2/200
1407/1407 [==============================] - 2s 1ms/step - loss: 0.2295 -
accuracy: 0.9330 - val_loss: 0.1773 - val_accuracy: 0.9485
Epoch 00002: val_loss improved from 0.25790 to 0.17731, saving model to
model/best_model.keras
...,
Epoch 13/200
1407/1407 [==============================] - 2s 1ms/step - loss: 0.0137 -
accuracy: 0.9971 - val_loss: 0.0888 - val_accuracy: 0.9753
Epoch 00013: val_loss improved from 0.09088 to 0.08877, saving model to
model/best_model.keras
Epoch 14/200
1407/1407 [==============================] - 2s 1ms/step - loss: 0.0117 -
accuracy: 0.9974 - val_loss: 0.1019 - val_accuracy: 0.9722
Epoch 00014: val_loss did not improve from 0.08877
Epoch 46/200
1407/1407 [==============================] - 2s 1ms/step - loss: 0.0050 -
accuracy: 0.9986 - val_loss: 0.1318 - val_accuracy: 0.9778
...,
Epoch 00046: val_loss did not improve from 0.08877
Epoch 47/200
1407/1407 [==============================] - 2s 1ms/step - loss: 1.
,‚Üí1854e-04 -
accuracy: 1.0000 - val_loss: 0.1361 - val_accuracy: 0.9782
Epoch 00047: val_loss did not improve from 0.08877
Epoch 48/200
1407/1407 [==============================] - 2s 1ms/step - loss: 8.
,‚Üí4957e-05 -
accuracy: 1.0000 - val_loss: 0.1316 - val_accuracy: 0.9777
Epoch 00048: val_loss did not improve from 0.08877
...,
Epoch 67/200
1407/1407 [==============================] - 1s 1ms/step - loss: 0.0011 -
accuracy: 0.9997 - val_loss: 0.1728 - val_accuracy: 0.9756
Epoch 00067: val_loss did not improve from 0.08877
training duration = 122.778
import matplotlib.pyplot as plt
epoch_count = range(1, len(history.history['loss']) + 1)
plt.figure(figsize=(13,4))

374
13 Deep Learning with Keras-TensorFlow
plt.subplot(121)
plt.plot(epoch_count, history.history['loss'], 'b', label = 'training‚ê£
,‚Üíloss')
plt.plot(epoch_count, history.history['val_loss'], 'r', label =‚ê£
,‚Üí'validation loss')
plt.legend()
plt.ylabel('loss')
plt.xlabel('epoch')
plt.subplot(122)
plt.plot(epoch_count, history.history['accuracy'], 'b', label =‚ê£
,‚Üí'training accuracy')
plt.plot(epoch_count, history.history['val_accuracy'], 'r', label =‚ê£
,‚Üí'validation accuracy')
plt.legend()
plt.ylabel('accuracy')
plt.xlabel('epoch')
0
10
20
30
40
50
60
70
epoch
0.0
0.1
0.2
0.3
0.4
0.5
loss
training loss
validation loss
0
10
20
30
40
50
60
70
epoch
0.88
0.90
0.92
0.94
0.96
0.98
1.00
accuracy
training accuracy
validation accuracy
Fig. 13.8: The loss (left) and the accuracy (right) of mnist_model as functions of
epoch on both the training set and the validation set.
Here we put together all the aforementioned codes for this MNIST classiÔ¨Åcation
problem in one place:
import time
from tensorflow import keras
import matplotlib.pyplot as plt
from tensorflow.keras import layers
from tensorflow.keras.datasets import mnist
from sklearn.model_selection import train_test_split
seed_value= 42
# set the seed for Python built-in pseudo-random generator
import random

13.5 The First Application Using Keras
375
random.seed(seed_value)
# set the seed for numpy pseudo-random generator
import numpy as np
np.random.seed(seed_value)
# set the seed for tensorflow pseudo-random generator
import tensorflow as tf
tf.random.set_seed(seed_value)
# data preprocessing
(X_train_val, y_train_val), (X_test, y_test) = mnist.load_data()
X_train_val, X_test = X_train_val.astype('float32')/255, X_test.
,‚Üíastype('float32')/255
X_train_val, X_test = X_train_val.reshape(60000, 28*28), X_test.
,‚Üíreshape(10000, 28*28)
X_train, X_val, y_train, y_val = train_test_split(X_train_val,‚ê£
,‚Üíy_train_val, stratify=y_train_val, test_size=0.25)
y_train_1_hot = keras.utils.to_categorical(y_train, num_classes = 10)
y_val_1_hot = keras.utils.to_categorical(y_val, num_classes = 10)
y_test_1_hot = keras.utils.to_categorical(y_test, num_classes = 10)
# building model
mnist_model = keras.Sequential([
layers.Dense(128, activation="sigmoid",‚ê£
,‚Üíinput_shape = X_train[0].shape),
layers.Dense(64, activation="sigmoid"),
layers.Dense(10, activation="softmax")
])
# compiling model
mnist_model.compile(optimizer="adam", loss="categorical_crossentropy",‚ê£
,‚Üímetrics=["accuracy"])
# training model
my_callbacks = [
keras.callbacks.EarlyStopping(
monitor="val_accuracy",
patience=20),
keras.callbacks.ModelCheckpoint(
filepath="best_model.keras",
monitor="val_loss",
save_best_only=True,
verbose=1)
]

376
13 Deep Learning with Keras-TensorFlow
start = time.time()
history = mnist_model.fit(x = X_train,
y = y_train_1_hot,
batch_size = 32,
epochs = 200,
validation_data = (X_val, y_val_1_hot),
callbacks=my_callbacks)
end = time.time()
training_duration = end - start
print("training duration = {:.3f}".format(training_duration))
print(history.history.keys())
print(mnist_model.summary())
# plotting the results
epoch_count = range(1, len(history.history['loss']) + 1)
plt.figure(figsize=(13,4))
plt.subplot(121)
plt.plot(epoch_count, history.history['loss'], 'b', label = 'training‚ê£
,‚Üíloss')
plt.plot(epoch_count, history.history['val_loss'], 'r', label =‚ê£
,‚Üí'validation loss')
plt.legend()
plt.ylabel('loss')
plt.xlabel('epoch')
plt.subplot(122)
plt.plot(epoch_count, history.history['accuracy'], 'b', label =‚ê£
,‚Üí'training accuracy')
plt.plot(epoch_count, history.history['val_accuracy'], 'r', label =‚ê£
,‚Üí'validation accuracy')
plt.legend()
plt.ylabel('accuracy')
plt.xlabel('epoch')
Suppose
in
an
application
we
use
callbacks.EarlyStopping(monitor="metric A",
patience=K,...) to perform early stopping based on a metric
A
and
use
callbacks.ModelCheckpoint(monitor="metric B",
save_best_only=True,...) to save the best model once metric B is
improved. The training stops at epoch M. Can we claim that the best model
is the model saved at epoch M ‚àíK?
The claim is correct if metric A and metric B are the same; otherwise,
it is not necessarily true. In the above application, for example, the best
model is saved at epoch 13, while the simulations stopped at epoch 67. This

13.5 The First Application Using Keras
377
is because the metric used in ModelCheckpoint was val_loss and the
metric used in EarlyStopping was val_accuracy and even though if one
of them increases the other one ‚Äúgenerally tends to decrease‚Äù, we can not
claim an increase in one will necessarily decrease the other one or vice versa.
This is apparent from the above example where epoch 13 led to the lowest
val_loss (and therefore, the ‚Äúbest‚Äù model), but the simulations stopped
at epoch=67 because patience was 20 and epoch=47 led to the highest
val_accuracy.
13.5.5 Evaluating and Predicting
We can evaluate the performance of a trained network on a given test data using
the evaluate() method of keras.Model class. It returns the loss and metric(s)
that were speciÔ¨Åed in compiling the model. For eÔ¨Éciency purposes (and facilitating
parallel computation on GPU), the computation occurs over batches, which by default
have size 32 (can be changed).
Furthermore, when we talk about evaluating a model, we generally wish to
use the ‚Äúbest‚Äù model; that is, the model that led to the best value of metric
used in callbacks.ModelCheckpoint(monitor=metric). Note that this is the
model saved at the end of the training process if ModelCheckpoint is used with
save_best_only=True and it is diÔ¨Äerent from the model Ô¨Åtted at the end of train-
ing process. For example, in the MNIST classiÔ¨Åcation that we have seen so far, we
would want to use best_model.keras, and not mnist_model, which holds the
weights obtained at the end of training process. Therefore, we need to load and use
the saved best model for prediction:
# load and evaluate the "best" model
best_mnist_model = keras.models.load_model("best_model.keras")
loss, accuracy = best_mnist_model.evaluate(X_test, y_test_1_hot,‚ê£
,‚Üíverbose=1)
print('Test accuracy of the model with lowest loss on validation set‚ê£
,‚Üí(the best model) = {:.3f}'.format(accuracy))
313/313 [==============================] - 0s 681us/step - loss: 0.1037 -
accuracy: 0.9694
Test accuracy of the model with lowest loss on validation set (the best‚ê£
,‚Üímodel) = 0.969
Similarly, we would like to use the ‚Äúbest‚Äù model for predicting targets for new
observations (inference)‚Äîin the above code, once model is loaded, it is called
best_mnist_model. In this regard, we have couple of options as discussed next:
1. we can use predict() method of keras.Model class to obtain probabilitites
of an observation belonging to a class. The computation is done over batches,
which is useful for large-scale inference:

378
13 Deep Learning with Keras-TensorFlow
prob_y_test_pred = best_mnist_model.predict(X_test)
print("the size of predictions is (n_sample x n_classes):",‚ê£
,‚Üíprob_y_test_pred.shape)
print("class probabilitites for the first instance:\n",‚ê£
,‚Üíprob_y_test_pred[0])
print("the assigned class for the first instance is: ",‚ê£
,‚Üíprob_y_test_pred[0].argmax())
# the one with the highest probability
print("the actual class for the first instance is: ", y_test_1_hot[0].
,‚Üíargmax())
y_test_pred = prob_y_test_pred.argmax(axis=1)
print("predicted classes for the first 10 isntances:", y_test_pred[:10])
the size of predictions is (n_sample x n_classes): (10000, 10)
class probabilitites for the first instance:
[2.2035998e-08 1.5811942e-06 1.0725732e-06 2.2704395e-05 1.6993125e-08
2.1640132e-07 1.2613171e-12 9.9997342e-01 1.9071523e-07 8.7907057e-07]
the assigned class for the first instance is:
7
the actual class for the first instance is:
7
classes fir the first 10 isntances: [7 2 1 0 4 1 4 9 5 9]
2. for small number of test data, we can use model(x):
prob_y_test_pred = best_mnist_model(X_test).numpy() # calling numpy()‚ê£
,‚Üíon a tensorflow tensor creates numpy arrays
print("the size of predictions is (n_sample x n_classes):",‚ê£
,‚Üíprob_y_test_pred.shape)
print("class probabilitites for the first instance:\n",‚ê£
,‚Üíprob_y_test_pred[0])
print("the assigned class is: ", prob_y_test_pred[0].argmax())
# the‚ê£
,‚Üíone with the highest probability
print("the actual class is: ", y_test_1_hot[0].argmax())
y_test_pred = prob_y_test_pred.argmax(axis=1)
print("predicted classes for the first 10 isntances:", y_test_pred[:10])
the size of predictions is (n_sample x n_classes): (10000, 10)
class probabilitites for the first instance:
[2.2036041e-08 1.5811927e-06 1.0725712e-06 2.2704351e-05 1.6993125e-08
2.1640091e-07 1.2613171e-12 9.9997342e-01 1.9071486e-07 8.7906972e-07]
the assigned class is:
7
the actual class is:
7
predicted classes for the first 10 isntances: [7 2 1 0 4 1 4 9 5 9]
At this stage, as we can compute the class probabilitites (scores) and/or classes
themselves, we may want to use various classes of sklearn.metrics to evaluate
the Ô¨Ånal model using diÔ¨Äerent metrics:
from sklearn.metrics import accuracy_score, confusion_matrix,‚ê£
,‚Üíroc_auc_score
print("Accuracy = {:.4f}".format(accuracy_score(y_test, y_test_pred)))

13.5 The First Application Using Keras
379
print("Confusion Matrix is\n {}".format(confusion_matrix(y_test,‚ê£
,‚Üíy_test_pred)))
print("Macro Average ROC AUC = {:.3f}".format(roc_auc_score(y_test,‚ê£
,‚Üíprob_y_test_pred, multi_class='ovr', average='macro')))
Accuracy = 0.9779
Confusion Matrix is
[[ 971
0
1
0
1
1
1
3
1
1]
[
0 1124
2
3
0
1
2
1
2
0]
[
4
1 1005
8
2
0
2
3
7
0]
[
0
0
4
989
0
5
0
6
4
2]
[
1
0
2
0
950
0
6
3
3
17]
[
3
0
0
6
1
873
4
0
4
1]
[
8
3
1
0
2
4
938
0
2
0]
[
2
4
9
4
0
0
0 1005
1
3]
[
4
1
1
3
3
5
2
3
949
3]
[
3
5
0
4
8
5
0
7
2
975]]
Macro Average ROC AUC = 1.000
13.5.6 CPU vs. GPU Performance
It is quite likely that executing the MNIST classiÔ¨Åcation code on a typical modern
laptop/desktop with no GPU hardware/software installed (i.e., using only CPUs) is
more eÔ¨Écient (faster) than using GPUs (e.g., available through Google Colab). Why
is that the case?
GPUs are best suitable when there is a large amount of computations that can
be done in parallel. At the same time, computations in training neural networks
are embarrassingly parallelizable. However, we may not see the advantage of using
GPUs with respect to CPUs if ‚Äúsmall‚Äù-scale computations are desired (for example,
training a small size neural networks as in this example). To examine the advantage
of using GPU with respect to CPU, in Exercise 1 we scale up the size of the neural
network used in this example and compare the execution times.
13.5.7 OverÔ¨Åtting and Dropout
OverÔ¨Åtting: Here is a comment about overÔ¨Åtting from renowned Ô¨Ågures in the Ô¨Åeld
of machine learning and pattern recognition (Duda et al., 2000, p. 16):
While an overly complex system may allow perfect classiÔ¨Åcation of the training samples, it
is unlikely perform well on new patterns. This situation is known as overÔ¨Åtting‚Äô.
In other words, in machine learning overÔ¨Åtting could be merely judged by the
performance of a trained model on training data and unseen data. We can claim a
model is overÔ¨Åtted if the classiÔ¨Åer/regressor has a perfect performance on training
data but it exhibits a poor performance on unseen data. The question is whether we

380
13 Deep Learning with Keras-TensorFlow
can judge overÔ¨Åtting by the data at hand. Recall what we do in model selection when
we try to tune a hyperparameter on a validation set (or using cross-validation if we
desire to remove the bias to a speciÔ¨Åc validation set): we monitor the performance
of the model on the validation set as a function of the hyperparameter and pick the
value of the hyperparameter that leads to the best performance. The reason we use a
validation set is that the performance on that serves as a proxy for the performance of
the model on test set. Therefore, we can use it to judge the possibility of overÔ¨Åtting
as well. In this regard, Duda et al. write (Duda et al., 2000, p. 16):
For most problems, the training error decreases monotonically during training [as a func-
tion of hyperparameter],. . . . Typically, the error on the validation set decreases, but then
increases, an indication that the classiÔ¨Åer may be overÔ¨Åtting the training data. In validation,
training or parameter adjustment is stopped at the Ô¨Årst minimum of the validation error.
There are a few noteworthy points to consider:
1. To be able to judge whether a classiÔ¨Åer is overÔ¨Åtting or not, we need two com-
ponents: the performance of the classiÔ¨Åer on training data and its performance
on unseen data. There is no problem with Ô¨Ånding the performance on training
data; after all, it is already available and the apparent error shows how well the
classiÔ¨Åer performs on training data. However, the only means to judge the perfor-
mance on unseen data are evaluation rules. Therefore, as stated by Duda et al., a
decrease and then increase in validation error (as a function of hyperparameter)
is an ‚Äúindication that the classiÔ¨Åer may be overÔ¨Åtting‚Äù. Although generally such
a behaviour is perceived and referred to as an indicator of overÔ¨Åtting, there is an
implicit uncertainty in our ability to judge the overÔ¨Åtting.
2. We may use other performance metrics such as AUC, F1-score or even the loss
function as indicators of overÔ¨Åtting; after all, they are all metrics of performance
that measure diÔ¨Äerent aspects of a predictions made by a trained model;
3. There are situations that the validation error (or loss) as a function of hyperpa-
rameter values becomes like a plateau, while the training error improves. We
can not blindly perceive such a situation as an indicator of overÔ¨Åtting unless
the training error is remarkably better than validation error. This is because this
situation is generally due to: 1) the optimization algorithm (optimizer) is stuck
in a local minima; and/or 2) the algorithm is actually resistant to overÔ¨Åtting
such that validation error does not start increasing! For the second case, albeit
in connection with AdaBoost, see the following comment from (J. Friedman,
2000):
In fact, Breiman (1996) (referring to a NIPS workshop) called AdaBoost with trees the
‚Äúbest oÔ¨Ä-the-shelf classiÔ¨Åer in the world‚Äù [see also Breiman (1998b)]. Interestingly, in
many examples the test error seems to consistently decrease and then level oÔ¨Äas more
classiÔ¨Åers are added, rather than ultimately increase. For some reason, it seems that
AdaBoost is resistant to overÔ¨Åtting.
With this explanation about overÔ¨Åtting, let us examine the performance curves
obtained before for the MNIST application. Looking into the loss curve as a function
of epoch in Fig. 13.8 shows that after epoch 13, the validation loss increases, which

13.5 The First Application Using Keras
381
is an indicator that the classiÔ¨Åer may be overÔ¨Åtting. There are various ways to prevent
possible overÔ¨Åtting. A popular approach is the concept of dropout that we discuss
next.
Dropout: Since its conception in (Srivastava et al., 2014), dropout has become a
popular approach to guard against overÔ¨Åtting. The idea is to randomly omit each
neuron in hidden layers or even each input feature with a pre-speciÔ¨Åed probability
known as dropout rate. Although dropout rate is typically set between 0.2 to 0.5,
it is itself a hyperparameter that should be ideally tuned in the process of model
selection. However, typically the dropout rate for an input feature is lower (e.g.,
0.2) than a neuron in a hidden layer (e.g., 0.5). By omitting a unit, we naturally
omit any connection to or from that unit as well. Fig. 13.9 shows what is meant by
omitting some neurons/features along with their incoming and outgoing connections.
To facilitate implementation, it is generally assumed the same dropout rate for all
units (neuron or an input feature) in the same layer.
(a)
(b)
Fig. 13.9: (a) an MLP when no dropout is used; (b) an MLP when a dropout is used
The mini-batch gradient descent that was discussed before is used to train neural
networks with dropout. The main diÔ¨Äerence lies in training where for each training
instance in a mini-batch, a network is sampled by dropping out some units from the
full architecture. Then the forward and backward computations that are part of the
backprop algorithm are performed for that training instance in the mini-batch over
the sampled network. The gradient for each weight that is used in optimizer to move
the weight is averaged over the training instances in that mini-batch. In this regard,
if for a training instance, its corresponding sampled network does not have a weight,
the contribution of that training instance towards the gradient is set to 0. At the end
of this process, we obtain estimates of weights for the full network (i.e., all units and
their connections with no dropout). In testing stage, the full network is used (i.e.,
all connections with no dropout). However, the outgoing weights from a unit are
multiplied by (1 ‚àíp) where p is the dropout rate for that unit. This approximation
accounts for the fact that the unit was active in training with probability of (1 ‚àíp).
In other words, by this simple approximation and using the full network in testing

382
13 Deep Learning with Keras-TensorFlow
we do not need to use many sampled networks with shared weights that were used
in training.
In Keras, there is a speciÔ¨Åc layer, namely, keras.layers.Dropout class, that
can be used to implement the dropout. In this regard, if a dropout for a speciÔ¨Åc layer
is desired, a Dropout layer should be added right after that. Let us now reimplement
the MNIST application (for brevity part of the output is omitted):
import time
from tensorflow import keras
import matplotlib.pyplot as plt
from tensorflow.keras import layers
from tensorflow.keras.datasets import mnist
from sklearn.model_selection import train_test_split
seed_value= 42
# set the seed for Python built-in pseudo-random generator
import random
random.seed(seed_value)
# set the seed for numpy pseudo-random generator
import numpy as np
np.random.seed(seed_value)
# set the seed for tensorflow pseudo-random generator
import tensorflow as tf
tf.random.set_seed(seed_value)
# data preprocessing
(X_train_val, y_train_val), (X_test, y_test) = mnist.load_data()
X_train_val, X_test = X_train_val.astype('float32')/255, X_test.
,‚Üíastype('float32')/255
X_train_val, X_test = X_train_val.reshape(60000, 28*28), X_test.
,‚Üíreshape(10000, 28*28)
X_train, X_val, y_train, y_val = train_test_split(X_train_val,‚ê£
,‚Üíy_train_val, stratify=y_train_val, test_size=0.25)
y_train_1_hot = keras.utils.to_categorical(y_train, num_classes = 10)
y_val_1_hot = keras.utils.to_categorical(y_val, num_classes = 10)
y_test_1_hot = keras.utils.to_categorical(y_test, num_classes = 10)
# building model
mnist_model = keras.Sequential([
layers.Dense(128, activation="sigmoid",‚ê£
,‚Üíinput_shape = X_train[0].shape),
layers.Dropout(0.3),

13.5 The First Application Using Keras
383
layers.Dense(64, activation="sigmoid"),
layers.Dropout(0.3),
layers.Dense(10, activation="softmax")
])
# compiling model
mnist_model.compile(optimizer="adam", loss="categorical_crossentropy",‚ê£
,‚Üímetrics=["accuracy"])
# training model
my_callbacks = [
keras.callbacks.EarlyStopping(
monitor="val_accuracy",
patience=20),
keras.callbacks.ModelCheckpoint(
filepath="best_model.keras",
monitor="val_loss",
save_best_only=True,
verbose=1)
]
start = time.time()
history = mnist_model.fit(x = X_train,
y = y_train_1_hot,
batch_size = 32,
epochs = 200,
validation_data = (X_val, y_val_1_hot),
callbacks=my_callbacks)
end = time.time()
training_duration = end - start
print("training duration = {:.3f}".format(training_duration))
print(history.history.keys())
print(mnist_model.summary())
# plotting the results
epoch_count = range(1, len(history.history['loss']) + 1)
plt.figure(figsize=(13,4))
plt.subplot(121)
plt.plot(epoch_count, history.history['loss'], 'b', label = 'training‚ê£
,‚Üíloss')
plt.plot(epoch_count, history.history['val_loss'], 'r', label =‚ê£
,‚Üí'validation loss')
plt.legend()
plt.ylabel('loss')
plt.xlabel('epoch')
plt.subplot(122)

384
13 Deep Learning with Keras-TensorFlow
plt.plot(epoch_count, history.history['accuracy'], 'b', label =‚ê£
,‚Üí'training accuracy')
plt.plot(epoch_count, history.history['val_accuracy'], 'r', label =‚ê£
,‚Üí'validation accuracy')
plt.legend()
plt.ylabel('accuracy')
plt.xlabel('epoch')
Epoch 1/200
1407/1407 [==============================] - 3s 2ms/step - loss: 1.2182 -
accuracy: 0.6240 - val_loss: 0.2984 - val_accuracy: 0.9124
Epoch 00001: val_loss improved from inf to 0.29836, saving model to
best_model.keras
...
Epoch 74/200
1407/1407 [==============================] - 2s 1ms/step - loss: 0.0308 -
accuracy: 0.9904 - val_loss: 0.1008 - val_accuracy: 0.9779
Epoch 00074: val_loss did not improve from 0.08361
Epoch 75/200
1407/1407 [==============================] - 2s 1ms/step - loss: 0.0295 -
accuracy: 0.9904 - val_loss: 0.1008 - val_accuracy: 0.9780
Epoch 00075: val_loss did not improve from 0.08361
training duration = 136.439
dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])
Model: "sequential"
_________________________________________________________________
Layer (type)
Output Shape
Param #
=================================================================
dense (Dense)
(None, 128)
100480
_________________________________________________________________
dropout (Dropout)
(None, 128)
0
_________________________________________________________________
dense_1 (Dense)
(None, 64)
8256
_________________________________________________________________
dropout_1 (Dropout)
(None, 64)
0
_________________________________________________________________
dense_2 (Dense)
(None, 10)
650
=================================================================
Total params: 109,386
Trainable params: 109,386
Non-trainable params: 0
_________________________________________________________________
None
# evaluate on test data
best_mnist_model = keras.models.load_model("best_model.keras")

13.5 The First Application Using Keras
385
0
10
20
30
40
50
60
70
epoch
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
loss
training loss
validation loss
0
10
20
30
40
50
60
70
epoch
0.80
0.85
0.90
0.95
1.00
accuracy
training accuracy
validation accuracy
Fig. 13.10: The loss (left) and the accuracy (right) of mnist_model trained using
dropout as functions of epoch on both the training set and the validation set. The
Ô¨Ågure shows that with dropout the performance curves over training and validation
sets do not diverge as in Fig. 13.8.
loss, accuracy = best_mnist_model.evaluate(X_test, y_test_1_hot,‚ê£
,‚Üíverbose=1)
print('Test accuarcy of the dropout model with lowest loss (the best‚ê£
,‚Üímodel) = {:.3f}'.format(accuracy))
313/313 [==============================] - 0s 735us/step - loss: 0.0800 -
accuracy: 0.9772
Test accuarcy of the dropout model with lowest loss (the best model) = 0.
,‚Üí977
Here we observe that using dropout slightly improved the performance on the
test data: 97.7% (with dropout) vs. 96.9% (without dropout). At the same time,
comparing Fig. 13.10 with Fig. 13.8 shows that the performance curves over training
and validation sets are closer, which indicates that the dropout is indeed guarding
against overÔ¨Åtting.
13.5.8 Hyperparameter Tuning
As mentioned in Section 13.1, a major bottleneck in using deep neural networks is the
large number of hyperparameters to tune. Although Keras-TensorFlow comes with
several tuning algorithms, here we discuss the use of grid search cross-validation im-
plemented by sklearn.model_selection.GridSearchCV that was discussed in
Section 9.2.1. The entire procedure can be used with random search cross-validation
discussed in Section 9.2.2 if GridSearchCV and param_grid are replaced with
RandomizedSearchCV and param_distributions, respectively.
To treat a Keras classiÔ¨Åer/regressor as a scikit-learn estimator and use it along
with scikit-learn classes, we can use

386
13 Deep Learning with Keras-TensorFlow
‚Ä¢ keras.wrappers.scikit_learn.KerasClassifier
‚Ä¢ keras.wrappers.scikit_learn.KerasRegressor
Similar wrappers are also implemented by scikeras library (Scikeras-wrappers,
2023), which supports Keras functional and subclassing APIs as well. As the number
of layers and neurons are (structural) hyperparameters to tune, we Ô¨Årst write a
function, called construct_model, which constructs and compiles a sequential
Keras model depending on the number of layers and neurons in each layer. Then
treating construct_model as the argument of KerasClassifier, we create the
classiÔ¨Åer that can be treated as a scikit-learn estimator; for example, to use it within
GridSearchCV or, if needed, as the classiÔ¨Åer used within Pipeline class to create
a composite estimator along with other processing steps. In what follows, we also
treat the dropout rate of the optimizer as well as the number of epochs as two
additional hyperparameters to tune‚Äîin contrast with EarlyStopping callback,
here we determine the epoch that jointly with other hyperparameters leads to the
highest CV score. For that reason, and to reduce the computational complexity of
the grid search, we assume candidate epoch values are in the set {10, 30, 50, 70}.
def construct_model(hidden_layers = 1, neurons=32, dropout_rate=0.25,‚ê£
,‚Üílearning_rate = 0.001):
# building model
model = keras.Sequential()
for i in range(hidden_layers):
model.add(layers.Dense(units=neurons, activation="sigmoid"))
model.add(layers.Dropout(dropout_rate))
model.add(layers.Dense(10, activation="softmax"))
# compiling model
model.compile(loss='categorical_crossentropy',
optimizer=keras.optimizers.Adam(learning_rate),
metrics=['acc'])
return model
import time
import pandas as pd
from tensorflow import keras
import matplotlib.pyplot as plt
from tensorflow.keras import layers
from tensorflow.keras.datasets import mnist
from sklearn.model_selection import train_test_split
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import StratifiedKFold, GridSearchCV
seed_value= 42
# set the seed for Python built-in pseudo-random generator

13.5 The First Application Using Keras
387
import random
random.seed(seed_value)
# set the seed for numpy pseudo-random generator
import numpy as np
np.random.seed(seed_value)
# set the seed for tensorflow pseudo-random generator
import tensorflow as tf
tf.random.set_seed(seed_value)
# data preprocessing
(X_train, y_train), (X_test, y_test) = mnist.load_data()
X_train, X_test = X_train.astype('float32')/255, X_test.
,‚Üíastype('float32')/255
X_train, X_test = X_train.reshape(60000, 28*28), X_test.reshape(10000,‚ê£
,‚Üí28*28)
strkfold = StratifiedKFold(n_splits=3, shuffle=True)
keras_est = KerasClassifier(build_fn=construct_model, verbose=1)
param_grid = {'hidden_layers': [1, 2], 'neurons': [32, 64],‚ê£
,‚Üí'dropout_rate': [0.2, 0.4], 'epochs': range(10,90,20)}
gscv = GridSearchCV(keras_est, param_grid, cv=strkfold)
# training model
score_best_estimator=gscv.fit(X_train, y_train).score(X_test, y_test)
#best model based on grid search cv
print('the highest CV score is: {:.3f}'.format(gscv.best_score_))
print('the best combination is: {}'.format(gscv.best_params_))
print('the accuracy of the best estimator on the test data is: {:.3f}'.
,‚Üíformat(score_best_estimator))
Epoch 1/10
1250/1250 [==============================] - 1s 690us/step - loss: 1.2428‚ê£
,‚Üí- acc:
0.6646
Epoch 2/10
1250/1250 [==============================] - 1s 674us/step - loss: 0.4253‚ê£
,‚Üí- acc:
0.8824
...
the highest CV score is: 0.972
the best combination is: {'dropout_rate': 0.2, 'epochs': 70,‚ê£
,‚Üí'hidden_layers': 2,

388
13 Deep Learning with Keras-TensorFlow
'neurons': 64}
the accuracy of the best estimator on the test data is: 0.978
A detailed view of cross-validation scores can be achieved by looking into
gscv.cv_results_ attribute. In the following code, for brevity we only present
the fold-speciÔ¨Åc CV scores and the overall CV score for all 32 combinations of hy-
perparameter values. As we can see, the highest mean_test_score is 0.972, which
is what we saw before by looking into gscv.best_score_:
pd.options.display.float_format = '{:,.3f}'.format
df = pd.DataFrame(gscv.cv_results_)
df
split0_test_score
split1_test_score
split2_test_score ‚ê£
,‚Üímean_test_score
0
0.947
0.946
0.945
0.946
1
0.962
0.962
0.959
0.961
2
0.944
0.947
0.943
0.945
3
0.963
0.964
0.960
0.962
4
0.961
0.956
0.953
0.957
5
0.970
0.971
0.966
0.969
6
0.957
0.959
0.954
0.957
7
0.971
0.970
0.967
0.969
8
0.958
0.960
0.957
0.958
9
0.971
0.970
0.968
0.970
10
0.957
0.958
0.953
0.956
11
0.972
0.973
0.969
0.971
12
0.960
0.961
0.958
0.960
13
0.973
0.972
0.969
0.971
14
0.958
0.960
0.954
0.958
15
0.975
0.971
0.969
0.972
16
0.939
0.941
0.940
0.940
17
0.957
0.958
0.953
0.956
18
0.924
0.925
0.922
0.924
19
0.950
0.950
0.948
0.949
20
0.949
0.951
0.948
0.949
21
0.968
0.966
0.963
0.965
22
0.940
0.940
0.935
0.939
23
0.964
0.963
0.960
0.962
24
0.952
0.953
0.949
0.951
25
0.969
0.969
0.966
0.968
26
0.942
0.943
0.940
0.942
27
0.966
0.966
0.963
0.965
28
0.954
0.955
0.952
0.954
29
0.969
0.970
0.966
0.968
30
0.947
0.947
0.944
0.946
31
0.966
0.964
0.963
0.964

13.5 The First Application Using Keras
389
Exercises:
Exercise 1: In the MNIST application presented in this chapter, use the same param-
eter for compiling and Ô¨Åtting except the following three changes:
1. replace the MLP with another MLP having 5 hidden layers with 2048 neurons
in each layer.
2. remove the early stopping callback and instead train the model for 10 epochs.
3. modify the code to add a dropout layer with a rate of 0.2 to the input feature vector
(Ô¨Çattened input). Do not use dropout in any other layer. Hint: Use Flatten()
layer to Ô¨Çatten input images. This allows adding a dropout layer before the Ô¨Årst
hidden layer.
(A) Execute the code only on CPUs available in a personal machine. What is the
execution time?
(B) Execute the code on GPUs provided by Colab. What is the execution time?
(C) Suppose each weight in this network is stored in memory in 4 bytes (Ô¨Çoat32).
What is approximately the minimum amount of memory (RAM) in mega byte
(MB) require to store all parameters of the network? (the answer to questions of
this type are specially insightful when, for example, we wish to deploy a trained
network on memory-limited embedded devices).
Exercise 2: We use a multi-layer perceptron (MLP) in a regression problem to es-
timate the oil price (a scalar quantity) based on 10 variables (therefore, our input
feature vectors are 10 dimensional and the output is univariate). Our MLP has two
hidden layer with 50 neurons in the Ô¨Årst hidden layer (the layer closer to the input)
and 20 in the second hidden layer (the layer closer to the output). What is the total
number of parameters that should be learned in this network?
Exercise 3: We have the perceptron depicted in Fig. 13.11 that takes binary inputs
x1 and x2; that is to say, xi = 0, 1, i = 1, 2 and lead to an output y. With choices
of weights indicated below, the network leads to an approximate implementation of
which logical gates (OR gate, AND gate, NAND gate, XOR gate)?
A) with a1 = a2 = 50 and a0 = ‚àí75 the network implements _______ gate
B) with a1 = a2 = ‚àí50 and a0 = 75 the network implements _______ gate
Exercise 4: Suppose the perceptron depicted in Fig. 13.12 is used in a binary
classiÔ¨Åcation problem. What activation function, what threshold T, and what loss
function should be used to make the classiÔ¨Åer œà(x) equivalent to logistic regression?

390
13 Deep Learning with Keras-TensorFlow
ùë•!
ùëé!
‚àë
ùëé!"
ùë•#
ùë¶
‚àÖùë°=
1
1 + ùëí!"
ùëé#
Fig. 13.11: he perceptron regressor used in Exercise 3.
ùëé!
‚àë
ùëé"
ùë•!
ùëé#
.
.
.
.
.
ùë•"
activation 
functions
ùë¶
ùúìùíô= ùêº{$%&}
Fig. 13.12: The perceptron classiÔ¨Åer used in Exercise 4.
Exercise 5: Suppose for training a neural network in an application with a training
sample size of 10000, we use mini-batch gradient descent with a batch size of 20.
How many weight updates occur in one epoch?
A) 500
B) 50
C) 20
D) 1
Exercise 6: Suppose in training a neural network using Keras-TensorFlow, we set
the epochs parameter of the fit() method of the keras.Model class to 500. Fur-
thermore, we use EarlyStopping callback with the value of patience parameter
being 50 and we set the monitor parameter of the callback to val_accuracy.
The training stops at epoch 170. What is the epoch that had the highest ‚Äúvalidation
accuracy‚Äù?
A) 170
B) 220
C) 120
D) 70
E) 500
Exercise 7: Suppose someone has trained a multi-layer perceptron in a regression
problem to estimate the oil price (a scalar quantity) based on 100 variables (there-
fore, our input feature vectors are 100 dimensional and the output is univariate). The
MLP has 50 hidden layers with 1000 neurons in each hidden layer. Suppose to store
each parameter of the model we need 4 bytes. What is approximately the minimum

13.5 The First Application Using Keras
391
amount of memory (in MB) require to store all parameters of the model?
A) 20 MB
B) 44 MB
C) 88 MB
D) 176 MB
E) 196 MB
Exercise 8: We use a multi-layer perceptron with two hidden layers to estimate a
univariate output using a univariate input x. Suppose each hidden layer has 100 neu-
rons and the ReLU activation function. The output layer has no activation function.
Assuming all weights and biases in the network are 1, what is the output for x = ‚àí2?
A) -1
B) 0
C) 1
D) 101
E) 10101

Chapter 14
Convolutional Neural Networks
A series of successful applications of Convolutional Neural Networks (CNNs) in
various computer vision competitions in 2011 and 2012 were a major driving force
behind the momentum of deep learning that we see today. As an example, consider
the 2012 ImageNet competition where a deep convolutional neural network, later
known as AlexNet, reported a top-5 test error rate of 15.3% for image classiÔ¨Åcation.
This error rate was remarkably better than the runner up error of 26.2% achieved by a
combination of non-deep learning methods. Following the success of deep learning
techniques in 2012, many groups in 2013 used deep CNNs in the competition.
This state of aÔ¨Äairs is in large part attributed to the ability of CNNs to integrate
feature extraction into model construction stage. In other words, and in analogy
with embedded feature selection methods discussed in Chapter 10, CNNs have an
embedded feature extraction mechanism that can be very helpful when working
with multidimensional signals such as time-series, digital images, or videos. There
are even some eÔ¨Äorts to leverage this important property of CNNs for tabular data
types. In this chapter we describe the working mechanism of these networks, which is
essential for proper implementation and use. Last but not least, the practical software
implementation in Keras will be discussed.
14.1 CNN, Convolution, and Cross-Correlation
The embedded feature extraction mechanism along with training eÔ¨Éciency and the
performance have empowered CNNs to achieve breakthrough results in a wide range
of areas such as image classiÔ¨Åcation (Krizhevsky et al., 2012; Simonyan and Zisser-
man, 2015), object detection (Redmon et al., 2016), electroencephalogram (EEG)
classiÔ¨Åcation (Lawhern et al., 2018), and speech recognition (Abdel-Hamid et al.,
2014), to just name a few. Consider an image classiÔ¨Åcation application. The con-
ventional way to train a classiÔ¨Åer is to use some carefully devised feature extraction
methods such as HOG (Histogram of Oriented Gradients) (Dalal and Triggs, 2005),
SIFT (Scale Invariant Feature Transform) (Lowe, 1999), or SURF (Speeded-Up Ro-
393
¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
A. Zollanvari, Machine Learning with Python, https://doi.org/10.1007/978-3-031-33342-2_14 

394
14 Convolutional Neural Networks
bust Feature) (Bay et al., 2006), and use the extracted features in some conventional
classiÔ¨Åers such as logistic regression or kNN. This way, we are restricted to a handful
of feature extraction algorithms that can heavily inÔ¨Çuence the performance of our
classiÔ¨Åers. However, in training CNNs features are extracted in the process of model
construction. A CNN is essentially an ANN that uses convolution operation in at
least one of its layers. Therefore, to understand the working mechanism of CNN, it
is important to have a grasp of the convolution operation.
Suppose x[i] and k[i] are two discrete 1-dimensional signals, where i is an
arbitrary integer. In the context of convolutional neural networks, x[i] is the input
signal, and k[i] is referred to as kernel, albeit as we will see later both have generally
higher dimensions than one. The convolution of x and k, denoted x ‚àók, is
y[m] ‚âú(x ‚àók)[m] =
‚àû
√ï
i=‚àí‚àû
x[i]k[m ‚àíi],
‚àÄm .
(14.1)
This can be seen as Ô¨Çipping the kernel, sum the products, and slide the kernel
over the input. One can easily show that convolution operation is commutative; that
is, (x ‚àók) = (k ‚àóx). This is due to the fact that in convolution deÔ¨Ånition, we are
Ô¨Çipping one signal with respect to another. If we do not Ô¨Çip one, we will have the
cross-correlation between x and k given by
y[m] ‚âú(x ‚ãÜk)[m] =
‚àû
√ï
i=‚àí‚àû
x[i]k[i ‚àím],
‚àÄm .
(14.2)
Cross-correlation, however, is not commutative. However, Ô¨Çipping or not makes
sense when k[i] has known values in the Ô¨Årst place (so that we can judge whether
inside the summation is Ô¨Çipped with respect to k[i] or not). In cases where k[i]
itself is unknown and is the objective of a learning process that uses (14.1) or (14.2),
Ô¨Çipping does not really matter. For example, suppose a k[i] is learned by using (14.2)
in a learning process. One can assume that what has been learned is a Ô¨Çipped version
of another signal called k‚Ä≤[i]. Because using k‚Ä≤[i] in (14.1) is equivalent to using
k[i] in (14.2), we can conclude that k[i] could be also learned by using (14.1) in the
same learning process (to Ô¨Ånd k‚Ä≤[i] and then Ô¨Çip to obtain k[i]). In learning CNNs,
learning k[i] is the objective. Therefore, many existing libraries use cross-correlation
in the learning process of CNN but we still refer to these networks as ‚Äúconvolutional‚Äù
neural networks.
14.2 Working Mechanism of 2D Convolution
Depending on the type of data, convolution operation is generally categorized as 1D,
2D, or 3D convolutions. In particular, 1D, 2D, and 3D convolutions are generally

14.2 Working Mechanism of 2D Convolution
395
used for processing multivariate time-series, images, and videos, respectively. The
distinguishing factor between these types of convolution is the number of dimensions
through which the kernel slides. For example, in 1D convolution, the kernel is slid
in one dimension (through time steps), whereas in 2D convolution, the kernel is slid
in two dimensions (e.g., in both height and width directions of an image). This also
implies that, if desired, we can even use 2D convolution with sequential data such
as multivariate time-series.
As the majority of applications are focused on the utility of 2D convolutions
(this does not mean others are not useful), in what follows we elaborate in detail
the working principles of 2D convolutions. The use of ‚Äú2D‚Äù convolution, should not
be confused with the dimensionality of input and kernel tensors used to hold the
data and the kernel. For example, in the 2D convolution that is used in CNN, each
input image is stored in a 3D tensor (rank-3 tensor: height, width, and depth) and the
kernel is a 4D tensor (rank-4 tensor: height, width, input depth, and output depth).
To better understand how the 2D convolution operates on a 3D tensor of input and
4D tensor of kernel, we Ô¨Årst start with a simpler case, which is the convolution of a
2D input tensor with a 2D kernel tensor.
14.2.1 Convolution of a 2D Input Tensor with a 2D Kernel Tensor
Convolution and sparse connectivity: Suppose we have a 2D tensor X[i, j] (here-
after referred to as an image or input) and a 2D kernel K[i, j]. Although in theory
there is no restriction on the width or the height of X and K, in training CNN, kernels
are generally taken to be much smaller than inputs. For example, regardless of the
size of X, K is assumed to be matrices of size 3 √ó 3 or 5 √ó 5. This property of CNNs
is known as sparse connectivity (also referred to as sparse interactions) because
regardless of the size of input, useful features are extracted by relatively small-size
kernels. This property also leads to eÔ¨Éciency in training because fewer parameters
are stored in memory. At the same time, because an objective in training CNNs is to
estimate the parameters of kernels themselves (the process that leads to learning the
feature extractor), sparse connectivity also implies fewer parameters to be estimated.
The result of convolving (with no Ô¨Çipping) X and K can be obtained by sliding K
over X and each time sum all the element-wise products of weights of K and values
in ‚Äúpatches‚Äù of X (of the same size as K). Fig. 14.1 shows an example of convolution
operation. The dotted area in the middle of X shows elements of X at which K could
be centered to have a legitimate convolution; otherwise (i.e., placing the center of
K outside this area) leads to an illegitimate convolution, which means there will be
no X element to multiply by a K element. For this reason the size of Y (the output
of convolution) is smaller than X. In particular, if X has a height hX and width wX
(i.e., size hX √ó wX), and the kernel has a size of hK √ó wK, the size of Y will be
(hX ‚àíhK + 1) √ó (wX ‚àíwK + 1).
Padding: To enforce having an output with the same size as the input, we can add a
suÔ¨Écient number of rows and columns of zeros to the input‚Äîthis process is known

396
14 Convolutional Neural Networks
Fig. 14.1: The ‚Äúconvolution‚Äù operation for a 2D input signal X and a 2D kernel K
to create the output Y. The dotted area in the middle of X shows elements of X at
which K could be centered to have a legitimate convolution.
as zero padding. In this regard, we need to add hX ‚àí(hX ‚àíhK + 1) = hK ‚àí1
and wK ‚àí1 rows and columns of zeros, respectively. It is common to perform this
symmetrically; that is to say, adding hK ‚àí1
2
rows of zeros on top and the same number
of rows at the bottom of the input (and similarly, adding wK ‚àí1
2
columns to the left
and the same number of columns to right of the input). Naturally, this would make
sense if hK and wK are odd numbers. Fig. 14.2 shows the convolution conducted in
Fig. 14.1 except that zero padding is used to have an output with the same size as the
input.
Neuron and feature map: Similar to the MLP that we saw in Chapter 13, a bias
term is added to the summation used in convolution, and the result is then used
as the input to some activation function. This resembles the artiÔ¨Åcial neuron that
we saw earlier in Chapter 13, except that here the inputs to a neuron is determined
by small patches of the image. The collection of the outputs of all neurons in the
same layer produces what is known as the feature map (also referred to as activation
map), which can itself be depicted as an image. Therefore, the output of one neuron
produces one value in the feature map. Fig. 14.3 shows two artiÔ¨Åcial neurons used in
a CNN layer (compare with MLP in Fig. 13.1) and two ‚Äúpixels‚Äù in the output feature
map.
Although sometimes ‚Äúfeature map‚Äù is used to simply point to the output of the
convolution operation (e.g., (Goodfellow et al., 2016, p. 333)), in the context of CNN
where each layer possesses an activation function, we refer to the output of neurons
(after the activation function) as the feature map‚Äîthis way we can say, for example,

14.2 Working Mechanism of 2D Convolution
397
Fig. 14.2: The ‚Äúconvolution‚Äù with zero padding for a 2D input signal X and a 2D
kernel K to create an output Y of the same size as X. The dotted area in the middle of
X shows elements of X at which K could be centered to have a legitimate convolution.
the output feature map from one layer is used as the input to the next layer in a
multi-layer CNN.
Parameter sharing: Parameter sharing means that all neurons in the same layer
share the same weights (kernel). This is because the small-size kernel (due to sparse
connectivity) slides over the entire input tensor, which, in turn, produces the inputs
to each neuron by multiplying diÔ¨Äerent patches of the input tensor with the same
kernel. Beside the sparse connectivity, this property (i.e, parameter/kernel sharing
between neurons in the same layer) is another factor that leads to a small number
of parameters to estimate. At the same time, a kernel, when learned, serves as a
feature detector. For example, if the learned kernel serves as a vertical edge detector,
convolving that over an image and looking at the outputs of neurons (i.e., feature
maps) tells us the areas of the image at which this feature is located. This is because
we apply the same feature detector over the entire image.
Strides: Rather than the regular convolution that was described earlier in which
the kernel is slid with a step (stride) of 1 in both direction, we can have strided
convolution in which the stride is greater than 1. We can even have diÔ¨Äerent stride
values across height and width directions. Fig. 14.4 shows a strided convolution

398
14 Convolutional Neural Networks
‚àë
‚àÖùë°
ùëå0,0 + ùëé!!
An artificial neuron
ùëé!!
‚àë
‚àÖùë°
ùëå0,4 + ùëé"!
ùëé"!
feature map 
values (‚Äúpixels‚Äù 
if depicted) in 
the feature map
Fig. 14.3: Each artiÔ¨Åcial neuron in a CNN layer produces one ‚Äúpixel‚Äù in the output
feature map.
where the stride in the height and width directions are 2 and 3, respectively. If hs and
ws denote strides in the direction of height and width, respectively, then the output
of the strided convolution is (‚åähX‚àíhK
hs
‚åã+ 1) √ó (‚åäwX‚àíwK
ws
‚åã+ 1). As we can see, the
eÔ¨Äect of a stride greater than 1 is downsampling the feature map by a rate equal to
the stride value.
Max-pooling: In CNN, it is typical to perform pooling once output feature maps
are generated. This is done by using a pooling layer right after the convolutional
layer. Pooling replaces the values of the feature map over a small window with a
summary statistic obtained over that window. There are various types of pooling that
can be used; for example, max-pooling replaces the values of the feature map within
the pooling window with one value, which is their maximum. Average-pooling, on
the other hand, replaces these values with their average.
Regardless of the actual function used for pooling, it is typical to use windows of
2 √ó 2 that are shifted with a stride of 2‚Äîthe concept of ‚Äústride‚Äù is the same as what
we saw before for the convolution operation and the magnitude of strides used for
pooling does not need to be the same as the convolutional strides. Similar to the use
of convolutional stride that was discussed before, using pooling in this way leads
to smaller feature maps; that is to say, feature maps are downsampled with a rate
of 2. From a computational perspective, downsampling helps reduce the number of

14.2 Working Mechanism of 2D Convolution
399
Fig. 14.4: Strided convolution. The eÔ¨Äect of a stride greater than 1 is downsampling
the feature map.
parameters that eventually should be estimated in a CNN. This is because typically
the output feature maps of the last convolutional layer is Ô¨Çattened and use as an input
to a dense layer. Without downsampling feature maps, here we end up with a huge
number of dense layer parameters to estimate (recall that in dense layer every neuron
is connected to all input variables).
Another way that max-pooling would help is in local translation invariance. This
means that if we move the input by a small amount, the output of pooling does not
change much because it is a summary of a local neighborhood of the feature map.
In some applications this is a desired property because the presence of a feature
is more important than where it is exactly located. Fig. 14.5 shows an example
that illustrates the mechanism of pooling (in particular max-pooling) as well as the
local translation invariance that is achieved by the max-pooling. In this example, we
have two inputs X1 and X2 where X2 is obtained by shifting X1 to the left (plus an
additional column on its right). As a result, Y2 = X2 ‚àóK is similar to Y1 = X1 ‚àóK
except that it is shifted to the left (plus one additional value on its right). This is
not surprising as convolution is translation equivariant. Nonetheless, if we compare
Y2 and Y1 element-wise, all values of Y1 is changed in Y2. However, this is not the
case with the output of max-pooling operation as in both cases the outcomes are the
same.

400
14 Convolutional Neural Networks
4
8
0
5
5
3
3
K  
2
1
10
5
8
0
4
1
1
0
9
1
3
2
1
0
-1
0
1
0
-1
0
1
4
24
1
4
3
8
0
5
5
3
3
2
1
10
5
8
0
4
1
1
0
9
1
3
2
0
1
0
-1
0
1
0
-1
0
1
24
1
4
3
2
24
4
24
4
X1
‚úª
‚úª
=
=
max-pooling
max-pooling
X2
K  
Y2
Y1
Fig. 14.5: Max-pooling and the local translation invariance.
14.2.2 Convolution of a 3D Input Tensor with a 4D Kernel Tensor
The actual ‚Äú2D‚Äù convolution that occurs in a layer of CNN, is the convolution of
a 3D input tensor and a 4D kernel tensor. To formalize the idea, let X, K, and Y
denote the 3D input tensor, 4D kernel tensor, and 3D output tensor, respectively.
Let X[i, j, k] denote the element in row i, column j, and channel k of the input X
for 1 ‚â§i ‚â§hX (input height), 1 ‚â§j ‚â§wX (input width), and 1 ‚â§k ‚â§cX (the
number of channels or depth). For an image encoded in RGB format, cX = 3 where
each channel refers to one of the red, green, and blue channels. Furthermore, here
we used the channels-last convention to represent the channels as the last dimenion
in X. Alternatively, we could have used channels-Ô¨Årst convention that uses the Ô¨Årst
dimension to refer to channels.
Furthermore, let K[i, j, k, l] denote the 4D kernel tensor where 1 ‚â§i ‚â§hK
(kernel height), and 1 ‚â§j ‚â§wK (kernel width), 1 ‚â§k ‚â§cK = cX (input
channels or input depth), and 1 ‚â§l ‚â§cY (output channels or output depth).
The sparse connectivity assumption stated before means that generally hK << hX
and wK << wX. Furthermore, note that cK is the same as cX. The fourth di-
mension can be understood easily by assuming K containing cY Ô¨Ålters of size
hK [smaller than input] √ó wK [smaller than input] √ó cX [same as input]. The convo-
lution (to be precise, cross-correlation) of X and K (with no padding of the input) is
obtained by computing the 3D output feature map tensor Y with elements Y[m, n, l]
where 1 ‚â§m ‚â§hY = hX ‚àíhK + 1, 1 ‚â§n ‚â§wY = wX ‚àíwK + 1, and 1 ‚â§l ‚â§cY, as
Y[m, n, l] =
√ï
i,j,k
X[i, j, k] K[m + i, n + j, k, l],
(14.3)

14.3 Implementation in Keras: ClassiÔ¨Åcation of Handwritten Digits
401
where the summation is over all valid indices. Fig. 14.6 shows the working mecha-
nism of 2D convolution as shown in (14.3). Few remarks about the 2D convolution
as shows here:
1. l in the third dimension of Y[m, n, l] refers to each feature map (matrix), and all
feature maps have the same size hY √ó wY;
2. assuming one bias term for each kernel, training a single layer in CNN means
estimating cY(cXhKwK+1) parameters for the kernel using the back-propagation
algorithm;
3. when there are multiple convolutional layers in one network, the output feature
map of one layer serves as the input tensor to the next layer;
4. all channels of a kernel are shifted with the same stride (see Fig. 14.7); and
5. the pooling, if used, is applied to every feature map separately;
kernel width = 5
kernel height = 5
kernel input depth = 3
kernel output depth = number of filters = 4
filter 1
filter 2
filter 3
filter 4
4D kernel tensor: 
(height √ó width √ó input depth √ó output depth)
feature map 1
feature map 2
feature map 3
feature map 4
Fig. 14.6: The working mechanism of 2D convolution: the convolution of a 3D input
tensor with a 4D kernel tensor.
14.3 Implementation in Keras: ClassiÔ¨Åcation of Handwritten
Digits
Here we train a CNN with 2D convolution for classiÔ¨Åcation of images of hand-
written digits that are part of the MNIST dataset. Many concepts that we discussed

402
14 Convolutional Neural Networks
feature map 
stride height =3
stride width =2
Fig. 14.7: Convolutional strides.
in Chapter 13 directly apply to training CNNs. As before, we need to build, compile,
train, and evaluate the trained model using similar classes (e.g., Sequential) and
methods (e.g., compile(), fit(), evaluate() and predict()). However, there
are some diÔ¨Äerences that we list here:
1. the input data: in Section 13.5.1, we vectorized images to create feature vectors.
This was done because MLP accepts feature vectors; however, in using CNN, we
do not need to do this as CNNs can work with images directly. That being said,
each input (image) to a 2D convolution should have the shape of height√ówidth√ó
channels (depth). However, our raw images have shape of 28 √ó 28. Therefore,
we use np.newaxis to add the depth axis;
2. the layers of CNN: to add 2D convolutional layer in Keras, we can use
keras.layers.Conv2D class. The number of Ô¨Ålters (number of output fea-
ture maps), kernel size (hK √ó wK), the strides (along the height and the width),
and padding are set through the following parameters:
keras.layers.Conv2D(filters, kernel_size,
strides=(1, 1), padding="valid")
where kernel_size and strides can be either a tuple/list of 2 integers that is
applied to height and width directions, respectively, or one integer that is applied
to both directions. The default padding="valid" causes no padding. Changing
that to padding="same" results in zero padding;

14.3 Implementation in Keras: ClassiÔ¨Åcation of Handwritten Digits
403
3. to add max-pooling, we can add
keras.layers.MaxPooling2D(pool_size=(2, 2),
strides=(1, 1), padding="valid").
pool_size can be either a tuple/list of 2 integers that shows the height and
width of the pooling window, or one integer that is then considered as the size
in both dimensions;
4. although we can use any activation functions presented in Chapter 13, a recom-
mended activation function in convolutional layers is ReLU;
5. the ‚ÄúclassiÔ¨Åcation‚Äù part: convolutional layers act like feature detectors (extrac-
tors). Regardless of whether we use one or more convolutional layers, we end up
with a number of feature maps, which is determined by the number of Ô¨Ålters in
the last convolutional layer. The question is how to perform classiÔ¨Åcation with
these feature maps. There are two options:
‚Ä¢ Ô¨Çattening the feature maps of the last convolutional layer using a Ô¨Çattening
layer (layers.Flatten) and then use the results as feature vectors to train
fully connected layers similar to what we have seen in Chapter 13. Although
this practice is common, there are two potential problems. One is that due to
large dimensionality of these feature vectors, the fully connected layer used
here could potentially overÔ¨Åt the data. For this reason, it is recommended to
use dropout in this part to guard against overÔ¨Åtting. Another problem is that
the fully connected layers used in this part have their own hyperparameters
and tuning them entails an increase in the computational needs;
‚Ä¢ apply the global average pooling (Lin et al., 2014) after the last convolutional
layer. This can be done using keras.layers.GlobalAveragePooling2D.
At this stage, we have two options for the number of Ô¨Ålters in the last convolu-
tional layer: 1) we can set the number of Ô¨Ålters in the last layer to the number
of categories and then use a softmax layer (keras.layers.Softmax())
to convert the global average of each feature map as the probability of the
corresponding class for the input; and 2) use the number of Ô¨Ålters in the last
layer as we genereally use in CNNs (e.g., increasing the number of Ô¨Ålters as
we go deeper in the CNN), and then apply the global average pooling. As
this practice leads to a larger vector than the number of classes (one value
per feature map), we can then use that as the input to a fully connected layer
that has a softmax activation and a number of neurons equal to the number
of classes. In what follows, we will examine training a CNN classiÔ¨Åer on
MNIST dataset using these three approaches.
Case 1: Flattened feature maps of the last convolutional layer as the input to two
fully connected layers in the ‚ÄúclassiÔ¨Åcation‚Äù part of the network trained for the
MNIST application.
For faster execution, use GPU, for example, provided by the Colab. This case is
implemented as follows (for brevity, part of the code output is omitted):

404
14 Convolutional Neural Networks
import time
import numpy as np
from tensorflow import keras
import matplotlib.pyplot as plt
from tensorflow.keras import layers
from tensorflow.keras.datasets import mnist
from sklearn.model_selection import train_test_split
seed_value= 42
# setting the seed for Python built-in pseudo-random generator
import random
random.seed(seed_value)
# setting the seed for numpy pseudo-random generator
import numpy as np
np.random.seed(seed_value)
# setting the seed for tensorflow pseudo-random generator
import tensorflow as tf
tf.random.set_seed(seed_value)
# scaling (using prior knowledge)
(X_train_val, y_train_val), (X_test, y_test) = mnist.load_data()
X_train_val, X_test = X_train_val.astype('float32')/255, X_test.
,‚Üíastype('float32')/255
# to make each input image a 3D tensor
X_train_val = X_train_val[:, :, :, np.newaxis]
X_test = X_test[:, :, :, np.newaxis]
X_train, X_val, y_train, y_val = train_test_split(X_train_val,‚ê£
,‚Üíy_train_val, stratify=y_train_val, test_size=0.25)
y_train_1_hot = keras.utils.to_categorical(y_train, num_classes = 10)
y_val_1_hot = keras.utils.to_categorical(y_val, num_classes = 10)
y_test_1_hot = keras.utils.to_categorical(y_test, num_classes = 10)
# building model
mnist_model = keras.Sequential([
layers.Conv2D(16, 3, activation="relu",‚ê£
,‚Üíinput_shape = X_train[0].shape),
layers.MaxPooling2D(),
layers.Conv2D(64, 3, activation="relu"),
layers.MaxPooling2D(),
layers.Flatten(),

14.3 Implementation in Keras: ClassiÔ¨Åcation of Handwritten Digits
405
layers.Dropout(0.25),
layers.Dense(32, activation="relu"),
layers.Dropout(0.25),
layers.Dense(10, activation="softmax")
])
# compiling model
mnist_model.compile(optimizer="adam", loss="categorical_crossentropy",‚ê£
,‚Üímetrics=["accuracy"])
# training model
my_callbacks = [
keras.callbacks.EarlyStopping(
monitor="val_accuracy",
patience=5),
keras.callbacks.ModelCheckpoint(
filepath="best_model.keras",
monitor="val_loss",
save_best_only=True,
verbose=1)
]
start = time.time()
history = mnist_model.fit(x = X_train,
y = y_train_1_hot,
batch_size = 32,
epochs = 200,
validation_data = (X_val, y_val_1_hot),
callbacks=my_callbacks)
end = time.time()
training_duration = end - start
print("training duration = {:.3f}".format(training_duration))
print(history.history.keys())
print(mnist_model.summary())
# plotting the results
epoch_count = range(1, len(history.history['loss']) + 1)
plt.figure(figsize=(13,4), dpi=150)
plt.subplot(121)
plt.plot(epoch_count, history.history['loss'], 'b', label = 'training‚ê£
,‚Üíloss')
plt.plot(epoch_count, history.history['val_loss'], 'r', label =‚ê£
,‚Üí'validation loss')
plt.legend()
plt.ylabel('loss')

406
14 Convolutional Neural Networks
plt.xlabel('epoch')
plt.subplot(122)
plt.plot(epoch_count, history.history['accuracy'], 'b', label =‚ê£
,‚Üí'training accuracy')
plt.plot(epoch_count, history.history['val_accuracy'], 'r', label =‚ê£
,‚Üí'validation accuracy')
plt.legend()
plt.ylabel('accuracy')
plt.xlabel('epoch')
best_mnist_model = keras.models.load_model("best_model.keras")
loss, accuracy = best_mnist_model.evaluate(X_test, y_test_1_hot,‚ê£
,‚Üíverbose=1)
print('Test accuracy of the model with the lowest loss (the best model)‚ê£
,‚Üí= {:.3f}'.format(accuracy))
Epoch 1/200
1407/1407 [==============================] - 13s 9ms/step - loss: 0.6160 -
accuracy: 0.8051 - val_loss: 0.0902 - val_accuracy: 0.9727
Epoch 00001: val_loss improved from inf to 0.09015, saving model to
best_model.keras
Epoch 2/200
1407/1407 [==============================] - 11s 8ms/step - loss: 0.1444 -
accuracy: 0.9571 - val_loss: 0.0566 - val_accuracy: 0.9827
Epoch 00002: val_loss improved from 0.09015 to 0.05660, saving model to
best_model.keras
...
Epoch 24/200
1407/1407 [==============================] - 12s 8ms/step - loss: 0.0218 -
accuracy: 0.9928 - val_loss: 0.0442 - val_accuracy: 0.9909
Epoch 00024: val_loss did not improve from 0.03510
training duration = 282.147
dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])
Model: "sequential_1"
_________________________________________________________________
Layer (type)
Output Shape
Param #
=================================================================
conv2d_2 (Conv2D)
(None, 26, 26, 16)
160
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 13, 13, 16)
0
_________________________________________________________________
conv2d_3 (Conv2D)
(None, 11, 11, 64)
9280
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)
0
_________________________________________________________________
flatten_1 (Flatten)
(None, 1600)
0
_________________________________________________________________

14.3 Implementation in Keras: ClassiÔ¨Åcation of Handwritten Digits
407
dropout_2 (Dropout)
(None, 1600)
0
_________________________________________________________________
dense_2 (Dense)
(None, 32)
51232
_________________________________________________________________
dropout_3 (Dropout)
(None, 32)
0
_________________________________________________________________
dense_3 (Dense)
(None, 10)
330
=================================================================
Total params: 61,002
Trainable params: 61,002
Non-trainable params: 0
_________________________________________________________________
None
313/313 [==============================] - 1s 2ms/step - loss: 0.0289 -
accuracy: 0.9909
Test accuracy of the model with the lowest loss (the best model) = 0.991
0
5
10
15
20
25
epoch
0.05
0.10
0.15
0.20
0.25
0.30
loss
training loss
validation loss
0
5
10
15
20
25
epoch
0.90
0.92
0.94
0.96
0.98
accuracy
training accuracy
validation accuracy
Fig. 14.8: The output of the code in Case 1: (left) loss; and (right) accuracy, as
functions of epoch.
Case 2: Global average pooling with the number of Ô¨Ålters in the last convolutional
layer being equal to the number of classes in the MNIST application.
For this purpose, we replace the ‚Äúbuilding model‚Äù part in the previous code with
the following code snippet:
# building model
mnist_model = keras.Sequential([
layers.Conv2D(32, 3, activation="relu",‚ê£
,‚Üíinput_shape = X_train[0].shape),
layers.MaxPooling2D(),
layers.Conv2D(10, 3, activation="relu"),
layers.MaxPooling2D(),
layers.GlobalAveragePooling2D(),
keras.layers.Softmax()
])

408
14 Convolutional Neural Networks
Epoch 1/200
1407/1407 [==============================] - ETA: 0s - loss: 1.6765 -‚ê£
,‚Üíaccuracy: 0.4455
Epoch 1: val_loss improved from inf to 1.23908, saving model to‚ê£
,‚Üíbest_model.keras
1407/1407 [==============================] - 16s 11ms/step - loss: 1.6764
accuracy: 0.4454 - val_loss: 1.2391 - val_accuracy: 0.6179
...
Epoch 40/200
1407/1407 [==============================] - ETA: 0s - loss: 0.3431 -‚ê£
,‚Üíaccuracy: 0.8950
Epoch 40: val_loss did not improve from 0.36904
1407/1407 [==============================] - 15s 11ms/step - loss: 0.3432
accuracy: 0.8950 - val_loss: 0.3699 - val_accuracy: 0.8851
training duration = 630.068
dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])
Model: "sequential_2"
_________________________________________________________________
Layer (type)
Output Shape
Param #
=================================================================
conv2d_4 (Conv2D)
(None, 26, 26, 32)
320
max_pooling2d_4 (MaxPooling
(None, 13, 13, 32)
0
2D)
conv2d_5 (Conv2D)
(None, 11, 11, 10)
2890
max_pooling2d_5 (MaxPooling
(None, 5, 5, 10)
0
2D)
global_average_pooling2d_2
(None, 10)
0
(GlobalAveragePooling2D)
softmax_2 (Softmax)
(None, 10)
0
=================================================================
Total params: 3,210
Trainable params: 3,210
Non-trainable params: 0
_________________________________________________________________
None
313/313 [==============================] - 1s 2ms/step - loss: 0.3394 -
accuracy: 0.9008
Test accuracy of the dropout model with lowest loss (the best model) = 0.
,‚Üí901
Case 3: Global average pooling with a ‚Äúregular‚Äù number of Ô¨Ålters in the last convo-
lutional layer and then a dense layer with the same number of neurons as the number
of classes.

14.3 Implementation in Keras: ClassiÔ¨Åcation of Handwritten Digits
409
0
5
10
15
20
25
30
35
40
epoch
0.4
0.6
0.8
1.0
1.2
1.4
1.6
loss
training loss
validation loss
0
5
10
15
20
25
30
35
40
epoch
0.5
0.6
0.7
0.8
0.9
accuracy
training accuracy
validation accuracy
Fig. 14.9: The output of the code for Case 2: (left) loss; and (right) accuracy, as
functions of epoch.
For this purpose, we replace the ‚Äúbuilding model‚Äù part in Case 1 with the following
code snippet:
# building model
mnist_model = keras.Sequential([
layers.Conv2D(16, 3, activation="relu",‚ê£
,‚Üíinput_shape = X_train[0].shape),
layers.MaxPooling2D(),
layers.Conv2D(64, 3, activation="relu"),
layers.MaxPooling2D(),
layers.GlobalAveragePooling2D(),
layers.Dense(10, activation="softmax")
])
Epoch 1/200
1407/1407 [==============================] - ETA: 0s - loss: 1.1278 -‚ê£
,‚Üíaccuracy: 0.6446
Epoch 1: val_loss improved from inf to 0.63142, saving model to‚ê£
,‚Üíbest_model.keras
1407/1407 [==============================] - 13s 9ms/step - loss: 1.1265 -
accuracy: 0.6451 - val_loss: 0.6314 - val_accuracy: 0.8123
...
Epoch 26/200
1407/1407 [==============================] - ETA: 0s - loss: 0.0924 -‚ê£
,‚Üíaccuracy: 0.9721
Epoch 26: val_loss did not improve from 0.11053
1407/1407 [==============================] - 16s 11ms/step - loss: 0.0924
accuracy: 0.9721 - val_loss: 0.1210 - val_accuracy: 0.9636
training duration = 350.196
dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])
Model: "sequential_3"
_________________________________________________________________

410
14 Convolutional Neural Networks
Layer (type)
Output Shape
Param #
=================================================================
conv2d_6 (Conv2D)
(None, 26, 26, 16)
160
max_pooling2d_6 (MaxPooling
(None, 13, 13, 16)
0
2D)
conv2d_7 (Conv2D)
(None, 11, 11, 64)
9280
max_pooling2d_7 (MaxPooling
(None, 5, 5, 64)
0
2D)
global_average_pooling2d_3
(None, 64)
0
(GlobalAveragePooling2D)
dense (Dense)
(None, 10)
650
=================================================================
Total params: 10,090
Trainable params: 10,090
Non-trainable params: 0
_________________________________________________________________
None
313/313 [==============================] - 1s 2ms/step - loss: 0.0775 -
accuracy: 0.9767
Test accuracy of the model with the lowest loss (the best model) = 0.977
0
5
10
15
20
25
epoch
0.2
0.4
0.6
0.8
1.0
loss
training loss
validation loss
0
5
10
15
20
25
epoch
0.65
0.70
0.75
0.80
0.85
0.90
0.95
accuracy
training accuracy
validation accuracy
Fig. 14.10: Output of the code for Case 3: (left) loss; and (right) accuracy, as functions
of epoch.
Exercises:
Exercise 1: Suppose in the MNIST application, we wish to perform hyperparameter
tuning for the number of Ô¨Ålters and layers in CNN. However, we would like to

14.3 Implementation in Keras: ClassiÔ¨Åcation of Handwritten Digits
411
choose an increasing pattern for the number of Ô¨Ålters that depends on the depth of
the convolutional base of the network. In particular, the number of Ô¨Ålters to examine
are:
1. for a CNN with one layer: 4 Ô¨Ålters;
2. for a CNN with two layers: 4 and 8 Ô¨Ålters in the Ô¨Årst and second layers, respec-
tively; and
3. for a CNN with three layers: 4, 8, and 16 Ô¨Ålters in the Ô¨Årst, second, and third
layers, respectively;
In all cases: 1) the output of the (last) convolutional layer is Ô¨Çattened and used as
the input to two fully connected layers with 32 neurons in the Ô¨Årst layer; 2) there is a
2√ó2 max-pooling right after each convolutional layer and a dropout layer with a rate
of 0.25 before each dense layer; and 3) all models are trained for 5 epochs.
(A) write and execute a code that performs hyperparameter tuning using 3-fold
stratiÔ¨Åed cross-validation with shuÔ¨Ñing. For reproducibility purposes, set the
seed for numpy, tensorÔ¨Çow, and Python built-in pseudo-random generators to
42. What is the highest CV score and the pattern achieving that score?
(B) what is the estimated test accuracy of the selected (‚Äúbest‚Äù) model trained on the
full training data?
(C) the following lines show parts of the code output:
Epoch 1/5
1250/1250 [==============================] - ...
Epoch 2/5
1250/1250 [==============================] -
...
Epoch 3/5
1250/1250 [==============================] -
...
Epoch 4/5
1250/1250 [==============================] -
...
Epoch 5/5
1250/1250 [==============================] -
...
625/625 [==============================] -
...
What do ‚Äú1250‚Äù and ‚Äú625‚Äù represent? Explain how these numbers are deter-
mined.
(D) the following lines show parts of the code output:
Epoch 1/5
1875/1875 [==============================] - ...
Epoch 2/5
1875/1875 [==============================] -
...
Epoch 3/5
1875/1875 [==============================] -
...
Epoch 4/5
1875/1875 [==============================] -
...
Epoch 5/5

412
14 Convolutional Neural Networks
1875/1875 [==============================] -
...
313/313 [==============================] -
...
What do ‚Äú1875‚Äù and ‚Äú313‚Äù represent?
Exercise 2: In a 2D convolutional layer, the input height, width, and depth are 10, 10,
and 16, respectively. The kernel height, width, and the output depth (number of Ô¨Ålters)
are, 3, 3, and 32, respectively. How many parameters in total are learned in this layer?
A) 51232
B) 51216
C) 4624
D) 4640
Exercise 3: In a 2D convolutional layer, the input height, width, and depth are 30, 30,
and 32, respectively. We use a strided convolution (stride being 3 in all directions),
no padding, and a kernel that has height, width, and the output depth of 5, 5, and
128, respectively. What is the shape of the output tensor?
A) 9 √ó 9 √ó 128
B) 10 √ó 10 √ó 128
C) 9 √ó 9 √ó 32
D) 10 √ó 10 √ó 32
Exercise 4: Suppose in a regression problem with a univariate output, the input to a
2D convolutional layer is the following 4 (height) √ó 4 (width) √ó 1 (depth) tensor:
2
-1
-1
-1
-1
2
-1
-1
-1
-1
2
-1
-1
-1
-1
2
In the convolutional layer, we use a kernel that has height, width, and output depth
of 3, 3, and 32, respectively, stride 1, and the ReLU activation function. This layer is
followed by the global average pooling as the input to a fully connected layer to esti-
mate the value of the output. Assuming all weights and biases used in the structure
of the network are 1, what is the output for the given input (use hand calculations)?
A) -17
B) -15
C) 0
D) 1
E) 15
F) 17
Exercise 5: Suppose in a regression problem with a univariate output, the in-
put to a 1D convolutional layer is a time-series segment, denoted xt, where
xt = (‚àí1)t, t = 0, 1, . . ., 9. In the convolutional layer, we use a kernel that has a
size of 5 and the output depth of 16, stride is 2, and the ReLU activation function.
This layer is followed by the global average pooling as the input to a fully connected
layer to estimate the value of the output. Assuming all weights and biases used in
the structure of the network are 1, what is the output for the given input (use hand
calculations)?
A) 1
B) 17
C) 33
D) 49
E) 65
F) 81

14.3 Implementation in Keras: ClassiÔ¨Åcation of Handwritten Digits
413
Exercise 6: Use Keras to build the models speciÔ¨Åed in Exercise 4 and 5 and compute
the output for the given inputs.
Hints:
‚Ä¢ Use the kernel_initializer and bias_initializer in the convolutional
and dense layers to set the weights.
‚Ä¢ Pick an appropriate initializer from (keras-initializers, 2023).
‚Ä¢ The 1D convolutional layer that is needed for the model speciÔ¨Åed in Exercise 5
can be implemented by keras.layers.Conv1D. Similarly, the global average
pooling for that model is layers.GlobalAveragePooling1D.
‚Ä¢ To predict the output for the models speciÔ¨Åed in Exercise 4 and 5, the input
should be 4D and 3D, respectively (the Ô¨Årst dimension refers to ‚Äúsamples‚Äù, and
here we have only one sample point as the input).

Chapter 15
Recurrent Neural Networks
The information Ô¨Çow in a classical ANN is from the input layer to hidden layers
and then to the output layer. An ANN with such a Ô¨Çow of information is referred
to as a feed-forward network. Such a network, however, has a major limitation in
that it is not able to capture dependencies among sequential observations such as
time-series. This is because the network has no memory and, as a result, observations
are implicitly assumed to be independent.
Recurrent neural networks (RNNs) are fundamentally diÔ¨Äerent from the feed-
forward networks in that they operate not only based on input observations, but also
on a set of internal states. The internal states capture the past information in sequences
that have already been processed by the network. That is to say, the network includes
a cycle that allows keeping past information for an amount of time that is not Ô¨Åxed
a-priori and, instead, depends on its input observations and weights. Therefore, in
contrast with other common architectures used in deep learning, RNN is capable
of learning sequential dependencies extended over time. As a result, it has been
extensively used for applications involving analyzing sequential data such as time-
series, machine translation, and text and handwritten generation. In this chapter, we
Ô¨Årst cover fundamental concepts in RNNs including standard RNN, stacked RNN,
long short-term memory (LSTM), gated recurrent unit (GRU), and vanishing and
exploding gradient problems. We then present an application of RNNs for sentiment
classiÔ¨Åcation.
15.1 Standard RNN and Stacked RNN
We Ô¨Årst formalize the dynamics of RNN. Let xt, ht, and yt denote the p √ó 1,
l √ó 1, and q √ó 1 input feature vector, hidden states, and output vector, respectively.
Given an input sequence (x1, x2, . . ., xT), a standard RNN unit (also referred to as
vanilla RNN), computes (h1, h2, . . ., hT) and (y1, y2, . . ., yT) by iterating through the
following recursive equations for t = 1, . . .,T (Graves et al., 2013):
415
¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
A. Zollanvari, Machine Learning with Python, https://doi.org/10.1007/978-3-031-33342-2_15 

416
15 Recurrent Neural Networks
ht = fh (Wxhxt + Whhht‚àí1 + bh) ,
(15.1)
yt = Whyht + by ,
(15.2)
where Wxh (input weight matrix), Whh (recurrent weight matrix), and Why denote
l √ó p, l √ó l, and q √ó l weight matrices, bh and by denote bias terms, and fh( . ) denote
an element-wise nonlinear hidden layer function (an activation function) such as
logistic sigmoid or hyperbolic tangent. As in previous neural networks, depending
on the application one may use an activation function in (15.2) to obtain yt; for
example, in a multiclass single label classiÔ¨Åcation problem, Whyht + by is used as
the input to a softmax to obtain yt. Fig. 15.1 depicts the working mechanism of
the standard RNN unit characterized by the recursive utility of equations (15.1) and
(15.2)‚Äîin Keras, this is implemented via keras.layers.simpleRNN class.
A basic RNN unit is already temporally deep because if it is unfolded in time,
it becomes a composite of multiple nonlinear computational layers (Pascanu et al.,
2014). Besides the depth of an RNN in a temporal sense, Pascanu et al. explored
various other ways to deÔ¨Åne the concept of depth in RNN and to construct deep
RNNs (Pascanu et al., 2014). A common way to deÔ¨Åne deep RNNs is to simply
stack multiple recurrent hidden layers on top of each other (Pascanu et al., 2014).
Nonetheless, various forms of stacking have been proposed. The standard stacked
RNN is formed by using the hidden state of a layer as the input to the next layer.
Assuming N hidden layers, the hidden states for layer j and the output of the network
yt are iteratively computed for j = 1, . . ., N, and t = 1, . . .,T, as (Graves et al., 2013)
hj
t = fh

Wh j‚àí1h j hj‚àí1
t
+ Wh jh j hj
t‚àí1 + bj
h

,
(15.3)
yt = WhN yhN
t + by ,
(15.4)
where h0
t = xt, and hj
t denotes the state vector for the jth hidden layer, and where an
identical hidden layer function fh( . ) is assumed across all layers. Fig. 15.2a depicts
the working mechanism of the stacked RNN characterized by (15.3) and (15.4).
The recurrence relation (15.3) was also used in (Hermans and Schrauwen, 2013)
but the output is computed by all the hidden states. This form of stacked RNN is
characterized by (Hermans and Schrauwen, 2013)
hj
t = fh

Wh j‚àí1h j hj‚àí1
t
+ Wh jh j hj
t‚àí1 + bj
h

,
(15.5)
yt =
N
√ï
j=1
Wh j yhj
t + by ,
(15.6)
where Wh j y are weight matrices learned to compute yt.
Another form of stacking is to determine the input of one layer by the hidden state
of the previous layer as well as the input sequence. In this case, the hidden states for
layer j and the output of the network yt are iteratively computed for j = 1, . . ., N,
and t = 1, . . .,T, by (Graves, 2013) (see Fig. 15.2b)

15.1 Standard RNN and Stacked RNN
417
hj
t = fh

Wxh j xt + Wh j‚àí1h j hj‚àí1
t
+ Wh jh j hj
t‚àí1 + bj
h

,
(15.7)
yt = WhN yhN
t + by ,
(15.8)
where h0
t = 0.
RNN unit
Fig. 15.1: A schematic diagram for the working mechanism of the standard RNN
unit.
The models presented so far are known as unidirectional RNNs because to predict
yt they only consider information from the present and the past context; that is to
say, the Ô¨Çow of information is from past to present and future observations are
not used to predict yt. Schuster et al. (Schuster and Paliwal, 1997) proposed the
bidirectional RNN that can utilize information from both the past and the future in
predicting yt. In this regard, two sets of hidden layers are deÔ¨Åned: 1) forward hidden
states, denoted hf
t , which similar to the standard RNN, are computed iteratively
from t = 1 to T  i.e., using sequence (x1, x2, . . ., xT); and 2) backward hidden
states, denoted hb
t , which are computed iteratively from t = T to 1  i.e., using
sequence (xT, xT‚àí1, . . ., x1). Once hidden states are computed, the output sequence
is obtained by a linear combination of forward and backward states. In particular, we
have
hf
t = fh

Wf
xhxt + Wf
hhhf
t‚àí1 + bf
h

,
(15.9)
hb
t = fh

Wb
xhxt + Wb
hhhb
t+1 + bb
h

,
(15.10)
yt = Wf
hyhf
t + Wb
hyhb
t + by ,
(15.11)
where Wf
xh, Wf
hh, Wb
xh, Wb
hh, Wf
hy, Wb
hy, bf
h, bb
h, and by are weight matrices and
bias terms of compatible size.

418
15 Recurrent Neural Networks
.. . . .. . . .. . . .. . . . 
.. . . .. . . .. . . .. . . . 
(a)
.. . . .. . . .. . . .. . . . 
.. . . .. . . .. . . .. . . . 
(b)
Fig. 15.2: A schematic diagram for the working mechanism of stacked RNN proposed
in (Graves et al., 2013) (a), and (Graves, 2013) (b).
15.2
√â
Vanishing and Exploding Gradient Problems
Despite the powerful working principles of standard RNN, it does not exhibit the
capability of capturing long-term dependencies when trained in practice. This state
of aÔ¨Äairs is generally attributed to the vanishing gradient and the exploding gradient
problems described in (Bengio et al., 1994; Hochreiter and Schmidhuber, 1997).

15.2 √â Vanishing and Exploding Gradient Problems
419
Vanishing gradient problem makes learning long-term dependencies diÔ¨Écult (be-
cause the inÔ¨Çuence of short-term dependencies dominate (Bengio et al., 1994)),
whereas exploding gradient induces instability of the learning process. The root of
the problem is discussed in details elsewhere (Bengio et al., 1994; Pascanu et al.,
2013). Here we provide a simplistic view that helps shed light on the cause of the
problem. Consider the following recurrent relations that characterize an RNN with
univariate input, state, and bh = 0:
ht = fh(wxhxt + whhht‚àí1),
(15.12)
yt = whyht + by .
(15.13)
To compute, say, y5, we need to compute h5. Assuming h0 = 0, we have
h5 = fh(wxhx5 + whhh4),
h4 = fh(wxhx4 + whhh3),
h3 = fh(wxhx3 + whhh2),
h2 = fh(wxhx2 + whhh1),
h1 = fh(wxhx1) .
(15.14)
or, equivalently,
h5 = fh
 
wxhx5 + whh fh
 
wxhx4 + whh fh

wxhx3 + whh fh
 wxhx2 + whh fh(wxhx1)!!
.
(15.15)
As described in Section 13.2, in order to Ô¨Ånd the weights of an ANN, we need
to Ô¨Ånd the gradient of the loss function with respect to weights (i.e., steepest de-
scent direction). The loss function L in RNN is computed over the entire sequence
length and the underlying algorithm for computing the gradients for RNN is called
backpropagation through time (BPTT) (Werbos, 1990). In particular,
‚àÇL
‚àÇwhh
=
T
√ï
t=1
‚àÇLt
‚àÇwhh
,
(15.16)
where Lt (the loss at a speciÔ¨Åc t) depends on yt. Let us assume a speciÔ¨Åc time t0
and consider the term
‚àÇLt0
‚àÇwhh . Because Lt0 depends on yt0, the chain rule of calculus
yields
‚àÇLt0
‚àÇwhh
= ‚àÇLt0
‚àÇyt0
‚àÇyt0
‚àÇht0
‚àÇht0
‚àÇwhh
.
(15.17)
Consider the term
‚àÇht0
‚àÇwhh in (15.17). From (15.12), using the regular chain rule once
more, we have

420
15 Recurrent Neural Networks
‚àÇhj+1
‚àÇwhh
=  hj + whh
‚àÇhj
‚àÇwhh
 f ‚Ä≤
j ,
j = 1, . . ., t0 ‚àí1,
(15.18)
‚àÇh1
‚àÇwhh
= 0,
(15.19)
where for the ease of notation we denoted derivative of a function g with respect
to its input as g‚Ä≤ and denoted f ‚Ä≤
h(wxhxj+1 + whhhj) by f ‚Ä≤
j . Recursively computing
(15.18) starting from (15.19) for j = 1, . . ., t0 ‚àí1, yields1
‚àÇht0
‚àÇwhh
=
t0‚àí1
√ï
k=1
wt0‚àí1‚àík
hh
hk
t0‚àí1
√ñ
j=k
f ‚Ä≤
j
(15.20)
For instance,
‚àÇh5
‚àÇwhh
= h4 f ‚Ä≤
4 + whhh3 f ‚Ä≤
4 f ‚Ä≤
3 + w2
hhh2 f ‚Ä≤
4 f ‚Ä≤
3 f ‚Ä≤
2 + w3
hhh1 f ‚Ä≤
4 f ‚Ä≤
3 f ‚Ä≤
2 f ‚Ä≤
1 .
(15.21)
To analyze the eÔ¨Äect of repetitive multiplications by whh, we assume a linear activa-
tion function fh(u) = u. This could be perceived simply as an assumption (e.g., see
(Pascanu et al., 2013)) or, alternatively, the Ô¨Årst (scaled) term of Maclaurin series
for some common activation functions (and therefore, an approximation around 0).
Therefore, f ‚Ä≤
j = 1, j = 1, . . ., t0 ‚àí1. Using this assumption to recursively compute
hj from (15.12) yields
hj = wxh
j√ï
k=1
w j‚àík
hh xk ,
j = 1, . . ., t0 ‚àí1.
(15.22)
Replacing (15.22) in (15.20) with some straightforward algebraic simpliÔ¨Åcation yield
(recall that f ‚Ä≤
j = 1)
‚àÇht0
‚àÇwhh
= wxh
t0‚àí1
√ï
k=1
(t0 ‚àík)wt0‚àí1‚àík
hh
xk .
(15.23)
Expression (15.23) shows the the dominance of short-term dependencies with
respect to long-term dependencies in the gradient. In particular, to capture the eÔ¨Äect
of a ‚Äúfar‚Äù input (e.g., x1) on minimizing Lt0 (which aÔ¨Äects moving whh during the
gradient decent), we have a multiplication by a high power of whh (e.g., wt0‚àí2
hh
for x1).
In contrast capturing the eÔ¨Äect of ‚Äúnear‚Äù inputs (e.g. xt0‚àí1) requires a multiplication
by a low power of whh (e.g., w0
hh for xt0‚àí1). In other words, capturing the eÔ¨Äect of
long terms dependencies requires multiplying whh by itself many times. Therefore,
1 Here to write the expression (15.20) we have used the regular chain rule of calculus, which
is well-known. We can also achieve at this relationship using the concept of immediate partial
derivatives, which is introduced by Werbose (Werbos, 1990) and used, for example, in (Pascanu
et al., 2013) to treat the vanishing and exploding gradient phenomena.

15.3 LSTM and GRU
421
if whh < 1 (or if whh > 1), high powers of whh will vanish (or will explode), which
causes the vanishing (or exploding) gradient problem.
15.3 LSTM and GRU
The problem of gradient exploding associated with the standard RNN can be re-
solved by the gradient clipping. For example, clipping the norm of the gradient when
it goes over a threshold (e.g., the average norm over a number of updates) (Pascanu
et al., 2013). However, to alleviate the vanishing gradient problem, more sophis-
ticated recurrent network architectures namely, long short-term memory (LSTM)
units (Hochreiter and Schmidhuber, 1997), and a more recent one, gated recurrent
units (GRUs) (Cho et al., 2014), have been proposed. In both these networks, the
mapping between pair of xt and ht‚àí1 to ht, which in standard RNN was based on a
single activation function, is replaced by a more sophisticated activation function. In
particular, LSTM implements the activation function fh( . ) in (15.1) by the following
composite function:2
it = œÉlogistic
 Wxixt + Whiht‚àí1 + bi
 ,
(15.24)
ft = œÉlogistic
 Wx f xt + Whf ht‚àí1 + bf
 ,
(15.25)
Àúct = œÉtanh
 Wxcxt + Whcht‚àí1 + bc
 ,
(15.26)
ct = it ‚äôÀúct + ft ‚äôct‚àí1 ,
(15.27)
ot = œÉlogistic
 Wxoxt + Whoht‚àí1 + bo
 ,
(15.28)
ht = ot ‚äôœÉtanh(ct),
(15.29)
where ‚äôdenotes element-wise multiplication, Wxi, Wx f , Wxc, and Wxo denote l√óp
weight matrices, Whi, Whf , Whc, and Who denote l √ó l weight matrices, bi, bf , bc,
and bo denote l √ó 1 bias terms, and it, ft, ct, and ot are all some l √ó 1 vectors known
as the states of input gate, forget gate, memory cell, and output gate, respectively.
Given ht from (15.29), the output sequence can be computed as presented earlier
in (15.2). Therefore, based on (15.24)-(15.29), there are 4(lp + l2 + l) parameters
learned in one LSTM layer.
The central concept in LSTM is the use of ‚Äúmemory‚Äù cell and ‚Äúgates‚Äù that control
what should be kept in the memory at each time. Based on (15.27), the state of
the memory cell at time t is obtained by partially forgetting the memory content
ct‚àí1 (determined by the state of the ‚Äúforget‚Äù gate at t, which is ft), and partially
adding a new memory content Àúct (determined by the state of the ‚Äúinput‚Äù gate at t,
which is it). Consider a single ‚Äúhidden unit‚Äù with input and forget gate states being
it = 0 and ft = 0, respectively  by each hidden unit we refer to one dimension
of the composite function (15.24)-(15.29). Consider the following four cases that
2 Comparing to the original form of LSTM (Hochreiter and Schmidhuber, 1997), this version is
due to (Gers et al., 1999) and has an additional gate (the forget gate).

422
15 Recurrent Neural Networks
(approximately) occur depending on the limits of œÉlogistic( . ), which is used in both
(15.24) and (15.25):
‚Ä¢ it = 0 and ft = 0: ct = 0 ‚Üíreset the memory content to 0;
‚Ä¢ it = 1 and ft = 0: ct = Àúct ‚Üíoverwrite the memory content by the new memory;
‚Ä¢ it = 0 and ft = 1: ct = ct‚àí1 ‚Üíkeep the memory content;
‚Ä¢ it = 1 and ft = 1: ct = Àúct +ct‚àí1 ‚Üíadd the new memory to the previous memory
content;
Although these cases are valid in the limits, we also note that œÉlogistic( . ) converges
exponentially to 0 or 1 as its input argument decreases or increases, respectively. Last
but not least, the output gate controls the amount of memory that should be exposed
at each time. As we can see, not only LSTM can propagate information by its additive
memory, it also learns when to forget or keep the memory, which is a powerful feature
of ‚Äúgated RNNs‚Äù. In Keras, LSTM is implemented via keras.layers.LSTM class.
GRU, on the other hand, is a simpler network architecture than LSTM, which
makes it more eÔ¨Écient to train. At the same time, it has shown comparable perfor-
mances to LSTM (Chung et al., 2014), which along with its simplicity makes it an
attractive choice. Similar to LSTM, GRU has also gating units that controls the Ô¨Çow
of information. In GRU, the activation function fh( . ) in (15.1) is implemented by
following composite function:
zt = œÉlogistic
 Wxzxt + Whzht‚àí1 + bz
 ,
(15.30)
rt = œÉlogistic
 Wxrxt + Whrht‚àí1 + br
 ,
(15.31)
Àúht = œÉtanh
 Wxhxt + Wh(ht‚àí1 ‚äôrt) + bh
 ,
(15.32)
ht = zt ‚äôÀúht + (1 ‚àízt) ‚äôht‚àí1 ,
(15.33)
where Wxz, Wxr, Wxh, and Wh denote l √ó p, l √ó p, l √ó p, and l √ó l weight matrices,
respectively, bz, br, bh denote l √ó 1 bias terms, and zt and rt denote l √ó 1 state
vector of the update gate and the reset gate, respectively. In contrast with LSTM,
GRU does not use the concept of ‚Äúmemory cell‚Äù. The hidden state is used directly
to learn keeping or forgetting past information. Consider a single hidden unit. When
the state of the update gate is close to 0, the hidden state content is kept; that is, we
have ht = ht‚àí1 in the corresponding unit. However, when the state of the update gate
is close to 1, the amount of the past hidden state that is kept is determined by the
reset gate via (15.32) and is stored in the candidate hidden state Àúht, which is used to
set ht via (15.33). In that case, if the reset gate is 0, the hidden state is essentially
set by the immediate input. Similar to standard RNN units, LSTM and GRU can
be stacked on top of each other to construct a deeper network. In that case and
similar to (15.3)-(15.8), expressions (15.24)-(15.29) and (15.30)-(15.33) for LSTM
and GRU, respectively, can be extended to all layers depending on the speciÔ¨Åc form
of stacking. Last but not least, the number of parameter learned in Keras for a GRU
layer (via keras.layers.GRU) is slightly diÔ¨Äerent from the number of paraameters
in (15.30)-(15.32). This is because in Keras, two separate bias vectors of size l √ó 1

15.4 Implementation in Keras: Sentiment ClassiÔ¨Åcation
423
are learned in (15.30)-(15.32) (keras-gru, 2023). This makes the total number of
parameters learned in a GRU layer 3(lp + l2 + 2l).
15.4 Implementation in Keras: Sentiment ClassiÔ¨Åcation
In this application, we intend to use text data to train an RNN that can classify a
movie review represented as a string of words to a ‚Äúpositive review‚Äù or ‚Äúnegative
review‚Äù‚Äîthis is an example of sentiment classiÔ¨Åcation. In this regard, we use the
‚ÄúIMDB dataset‚Äù. This is a dataset, which contains 25000 positive and the same
number of negative movie reviews collected from the IMDB website (Maas et al.,
2011). Half of the entire dataset is designated for training and the other half for
testing. Although the (encoded) dataset comes bundled with the Keras library and
could be easily loaded by: from tensorflow.keras.datasets import imdb
and then imdb.load_data(), we download it in its raw form from (IMDB, 2023)
to illustrate some preprocessing steps that would be useful in the future when working
raw text data.
import time
import numpy as np
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
from tensorflow.keras import layers
#from tensorflow.keras.datasets import imdb
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
seed_value= 42
# set the seed for Python built-in pseudo-random generator
import random
random.seed(seed_value)
# set the seed for numpy pseudo-random generator
import numpy as np
np.random.seed(seed_value)
# set the seed for tensorflow pseudo-random generator
tf.random.set_seed(seed_value)
Once the dataset is downloaded, each of the ‚ÄúaclImdb/train‚Äù and ‚ÄúaclImdb/test‚Äù
folders contains two folders, namely ‚Äúpos‚Äù and ‚Äúneg‚Äù, which include the positive and
negative reviewers, respectively. Here we Ô¨Årst load the downloaded dataset using
keras.utils.text_dataset_from_directory. This method returns a Tensor-

424
15 Recurrent Neural Networks
Flow Dataset object. We then use its concatenate method to add the negative
reviews to positive reviews.
# load the training (+ validation) data for positive and negative‚ê£
,‚Üíreviews, concatenate them, and generate the labels
directory = ".../aclImdb/train/pos" # replace ... by the path to‚ê£
,‚Üí"aclImdb", which contains the downloaded data
X_train_val_pos = keras.utils.
,‚Üítext_dataset_from_directory(directory=directory, batch_size = None,‚ê£
,‚Üílabel_mode=None, shuffle=False)
directory = ".../aclImdb/train/neg"
X_train_val_neg = keras.utils.
,‚Üítext_dataset_from_directory(directory=directory, batch_size = None,‚ê£
,‚Üílabel_mode=None, shuffle=False)
X_train_val = X_train_val_pos.concatenate(X_train_val_neg)
y_train_val = np.array([0]*len(X_train_val_pos) +‚ê£
,‚Üí[1]*len(X_train_val_neg))
Found 12500 files belonging to 1 classes.
Found 12500 files belonging to 1 classes.
# load the test data for positive and negative reviews, concatenate‚ê£
,‚Üíthem, and generate the labels
directory = ".../aclImdb/test/pos"
X_test_pos = keras.utils.
,‚Üítext_dataset_from_directory(directory=directory, batch_size = None,‚ê£
,‚Üílabel_mode=None, shuffle=False)
directory = ".../aclImdb/test/neg"
X_test_neg = keras.utils.
,‚Üítext_dataset_from_directory(directory=directory, batch_size = None,‚ê£
,‚Üílabel_mode=None, shuffle=False)
X_test = X_test_pos.concatenate(X_test_neg)
y_test = np.array([0]*len(X_test_pos) + [1]*len(X_test_neg))
Found 12500 files belonging to 1 classes.
Found 12500 files belonging to 1 classes.
Next we use TextVectorization, which is a versatile and eÔ¨Écient method that
can be used for both: 1) text standardization, which in its most standard form means
replacing punctuations with empty strings and converting all letters to lower case;
and 2) text tokenization, which in this regard, by default it splits the input text based
on whitespace (i.e., identiÔ¨Åes words as tokens). The use of output_mode=‚Äòint‚Äô in
TextVectorization assigns one integer to each identiÔ¨Åed word in the vocabulary.
We choose a vocabulary of size 5000 words, which means the most (5000-2) frequent
words in X_train_val (to which the TextVectorization is ‚Äúadapted‚Äù) will be
part of the vocabulary; therefore, we will not have any integer greater than 5000 to
represent a word. The Ô¨Årst two integers, 0 and 1, will be assigned to ‚Äúno word‚Äù and
‚Äúunknown word‚Äù [UNK], respectively.

15.4 Implementation in Keras: Sentiment ClassiÔ¨Åcation
425
# use TextVectorization to learn (by "adapt()" method) the vocabulary‚ê£
,‚Üífrom X_train_val
from tensorflow.keras.layers import TextVectorization as TV
vocab_size = 5000
tv = TV(max_tokens=vocab_size, output_mode='int')
tv.adapt(X_train_val)
print("Total number of words in the dictionary: ", len(tv.
,‚Üíget_vocabulary()))
Total number of words in the dictionary:
5000
Next we use the learned vocabulary to transform each sequence of words that are
part of X_train_val into corresponding sequence of integers.
# encode each sequence ("seq") that is part of the X_train_val Dataset‚ê£
,‚Üíobject using the learned vocabulary
X_train_val_int_encoded_var_len = []
for seq in X_train_val:
seq_encoded = tv([seq]).numpy()[0]
X_train_val_int_encoded_var_len.append(seq_encoded)
X_train_val_int_encoded_var_len[0]
array([
1,
322,
7,
4, 1078,
220,
9, 2085,
31,
2,
167,
62,
15,
47,
81,
1,
43,
400,
119,
136,
15, 4894,
56,
1,
148,
8,
2, 4946,
1,
480,
70,
6,
256,
12,
1,
1, 1972,
7,
73, 2363,
6,
641,
71,
7,
4894,
2,
1,
6, 2031,
1,
2,
1, 1422,
37,
69,
68,
205,
141,
65, 1216, 4894,
1,
2,
1,
5,
2,
219,
904,
32, 2929,
70,
5,
2, 4711,
10,
672,
3,
65, 1422,
51,
10,
208,
2,
383,
8,
60,
4, 1474,
3622,
775,
6, 3580,
187,
2,
400,
10, 1192,
1,
31,
322,
4,
350,
363, 2971,
142,
132,
6,
1,
29,
5,
123, 4894, 1474, 2409,
6,
1,
322,
10,
517,
12,
106,
1471,
5,
56,
582,
103,
12,
1,
322,
7,
234,
1,
49,
4, 2292,
12,
9,
207])
If desired, we can convert back the sequence of integers into words.
# decode an encoded sequence back to words
inverse_vocabulary = dict(enumerate(tv.get_vocabulary()))
decoded_sentence = " ".join(inverse_vocabulary[i] for i in‚ê£
,‚ÜíX_train_val_int_encoded_var_len[0])
decoded_sentence

426
15 Recurrent Neural Networks
'[UNK] high is a cartoon comedy it ran at the same time as some other‚ê£
,‚Üí[UNK] about school life such as teachers my [UNK] years in the‚ê£
,‚Üíteaching [UNK] lead me to believe that [UNK] [UNK] satire is much‚ê£
,‚Üícloser to reality than is teachers the [UNK] to survive [UNK] the‚ê£
,‚Üí[UNK] students who can see right through their pathetic teachers‚ê£
,‚Üí[UNK] the [UNK] of the whole situation all remind me of the schools i‚ê£
,‚Üíknew and their students when i saw the episode in which a student‚ê£
,‚Üírepeatedly tried to burn down the school i immediately [UNK] at high‚ê£
,‚Üía classic line inspector im here to [UNK] one of your teachers‚ê£
,‚Üístudent welcome to [UNK] high i expect that many adults of my age‚ê£
,‚Üíthink that [UNK] high is far [UNK] what a pity that it isnt'
# encode each sequence ("seq") that is part of the X_test Dataset‚ê£
,‚Üíobject using the learned vocabulary
X_test_int_encoded_var_len = []
for seq in X_test:
seq_encoded = tv([seq]).numpy()[0]
X_test_int_encoded_var_len.append(seq_encoded)
X_test_int_encoded_var_len[0]
array([
10,
418,
3,
208,
11,
18,
226,
311,
101,
107,
1,
6,
33,
4,
164,
340,
5, 1946,
527,
934,
12,
10,
14,
1,
6,
68,
9,
80,
36,
49,
10,
672,
5,
1,
1,
27,
14,
61,
491,
6,
82,
220,
10,
14,
359,
1,
251,
2,
109,
5, 3216,
1,
53,
74,
3,
1785,
1,
251, 1001,
1,
17,
136,
1,
2, 1890,
5,
4,
50,
18,
7,
12,
9,
69, 3006,
17,
254, 1400,
11,
29,
117,
593,
12,
2,
417,
740,
60,
14, 2940,
46,
14, 3000,
33, 2162,
299,
2,
86,
357,
5,
2,
18,
3,
66, 1614,
6, 1670,
299,
2,
326,
357,
131,
1,
2,
740,
10,
22,
61,
208,
106,
362,
8, 1670,
19,
106,
371, 2269,
346,
15,
74,
258, 2660,
22,
6,
372,
253,
68,
98, 2570,
11,
18,
14,
85,
3,
10,
1405,
12,
23,
138,
68,
9,
156,
23, 1856])
Until now the encoded sequence of reviews have diÔ¨Äerent length. However, for
training, we need sequences of the same length. Here we choose the Ô¨Årst 200 words
(that are of course part of the learned vocabulary) of each sequence. In this regard,
we pad the sequences. Those that are shorter will be (by default) pre-padded by zeros
to become 200 long.
# unify the length of each encoded review to a sequence_len. If the‚ê£
,‚Üíreview is longer, it is cropped and if it is shorter it will padded‚ê£
,‚Üíby zeros (by default)
sequence_len = 200
X_train_val_padded_fixed_len = keras.utils.
,‚Üípad_sequences(X_train_val_int_encoded_var_len, maxlen=sequence_len)
X_test_padded_fixed_len = keras.utils.
,‚Üípad_sequences(X_test_int_encoded_var_len, maxlen=sequence_len)
X_train_val_padded_fixed_len[0:2]

15.4 Implementation in Keras: Sentiment ClassiÔ¨Åcation
427
array([[
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
0,
1,
322,
7,
4,
1078,
220,
9, 2085,
31,
2,
167,
62,
15,
47,
81,
1,
43,
400,
119,
136,
15, 4894,
56,
1,
148,
8,
2, 4946,
1,
480,
70,
6,
256,
12,
1,
1, 1972,
7,
73, 2363,
6,
641,
71,
7, 4894,
2,
1,
6,
2031,
1,
2,
1, 1422,
37,
69,
68,
205,
141,
65,
1216, 4894,
1,
2,
1,
5,
2,
219,
904,
32, 2929,
70,
5,
2, 4711,
10,
672,
3,
65, 1422,
51,
10,
208,
2,
383,
8,
60,
4, 1474, 3622,
775,
6, 3580,
187,
2,
400,
10, 1192,
1,
31,
322,
4,
350,
363,
2971,
142,
132,
6,
1,
29,
5,
123, 4894, 1474, 2409,
6,
1,
322,
10,
517,
12,
106, 1471,
5,
56,
582,
103,
12,
1,
322,
7,
234,
1,
49,
4, 2292,
12,
9,
207],
[
1,
228,
335,
2,
1,
1,
33,
4,
1,
101,
30,
420,
21,
25,
1,
113,
1,
879,
81,
102,
571,
4,
246,
33,
2,
398,
5, 4676,
1, 2039, 3841,
34,
1,
37,
183, 4415,
156, 2236,
40,
345,
3,
40,
1,
1,
2151, 4635,
3,
1,
1, 2587,
37,
24,
449,
334,
6,
2, 1889,
493, 4164,
1,
207,
228,
22,
334,
6, 4463,
1,
1,
39,
27,
272,
117,
51,
107, 1004,
113,
30,
537,
42, 2805,
502,
42,
28,
1,
13,
131,
2,
116,
1961,
193, 4676,
3,
1,
268, 1659,
6,
114,
10,
249,
119, 4495,
6,
28,
29,
5, 3691, 2834,
1,
95,
113,
2545,
6,
107,
4,
220,
9,
265,
4, 4244,
506, 1054,
6,
25, 2602,
161,
136,
15,
1,
1,
182,
1,
42,
1,
16,
2,
549,
6,
120,
49,
30,
39,
252,
140,
4428,
156, 2236,
9,
2,
367,
262,
42,
21,
2,
81,
545,
245,
4,
375, 2081,
39,
32, 1004,
83,
82,
51,
35,
90,
118,
49,
6,
82,
17,
65,
276,
270,
35,
139,
192,
9,
6,
2, 3230,
297,
5,
747,
9,
39,
1,
1,
13,
42,
270,
11,
20,
77,
1,
23,
6,
328,
377]], dtype=int32)
Now we are in the position that we can start building and training the model.
However, for applications related to natural language processing, there is an important
layers that can signiÔ¨Åcantly boost the performance and, at the same time, speed
up the training. The core principle behind using this layer, which is known as
word embedding layer, is to represent each word by a relatively low-dimensional
vector, which is learned during the training process. The idea of representing a
word by a vector is not really new. One-hot encoding was an example of that.
Suppose we have a vocabulary of 15 words. We are given the following sequence
of four words and all these words belong to our vocabulary: ‚Äúhi how are you‚Äù.
One way to use one-hot encoding to represent the sequence is similar to Fig. 15.3.
However, using this approach we lose information about the order of words within
the sequence. As a result, this is not suitable for models such as RNN and CNN that

428
15 Recurrent Neural Networks
are used with sequential data. Another way to keep the order of words using one-
hot encoding is depicted in Fig. 15.4. However, this approach leads to considerably
sparse high-dimensional matrices. In word embedding, which is implemented by
keras.layers.Embedding, we represent each word with a low-dimensional vector
(the dimension is a hyperparameter) where each element of this vector is estimated
through the learning algorithm (see Fig. 15.5). Given a batch of word sequences of
size ‚Äúsamples √ó sequence length‚Äù, the embedding ‚Äúlayer‚Äù in Keras returns a tensor
of size ‚Äúsamples √ó sequence length √ó embedding dimensionality‚Äù. Next, we use an
embedding 16-dimensional layer, a GRU with 32 hidden units, and early stopping
call back with a patience parameter of 3 to train our classiÔ¨Åer. The classiÔ¨Åer shows
an accuracy of 86.4% on the test set.
we
why
when
what
hey
were
hi
are
bye
how
they
you
is
not
yes
seq
0
0
0
0
0
0
1
1
0
1
0
1
0
0
0
Fig. 15.3: The use of one-hot encoding without preserving the order of words within
sequence identiÔ¨Åed by ‚Äúseq‚Äù: hi how are you.
we
why
when
what
hey
were
hi
are
bye
how
they
you
is
not
yes
hi
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
how
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
are
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
you
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
sequence
Fig. 15.4: The use of one-hot encoding to preserve the order of words within se-
quence.
dim1
dim2
dim3
dim4
dim5
hi
0.2
-0.1
-0.3
-0.4
0.1
how
0.1
0.2
0.1
-0.2
0.2
are
-0.3
0.1
-0.3
-0.1
0.2
you
-0.2
0.15
-0.1
-0.2
0.1
sequence
Fig. 15.5: An embedding of 5 dimension (in practice the dimension is usually set
between 8 to 1024).

15.4 Implementation in Keras: Sentiment ClassiÔ¨Åcation
429
# split the training+validation data into training set and validation‚ê£
,‚Üíset
X_train, X_val, y_train, y_val =‚ê£
,‚Üítrain_test_split(X_train_val_padded_fixed_len, y_train_val,‚ê£
,‚Üístratify=y_train_val, test_size=0.25)
# build the model
imdb_model = keras.Sequential([
layers.Embedding(vocab_size, 16),
layers.GRU(32, input_shape=(None, 1)),
layers.Dense(1, activation='sigmoid')
])
# compiling model
imdb_model.compile(optimizer="adam", loss="binary_crossentropy",‚ê£
,‚Üímetrics=["accuracy"])
# training model
my_callbacks = [
keras.callbacks.EarlyStopping(
monitor="val_accuracy",
patience=3),
keras.callbacks.ModelCheckpoint(
filepath="best_model.keras",
monitor="val_accuracy",
save_best_only=True,
verbose=1)
]
history = imdb_model.fit(x = X_train,
y = y_train,
batch_size = 32,
epochs = 100,
validation_data = (X_val, y_val),
callbacks=my_callbacks)
Epoch 1/100
586/586 [==============================] - ETA: 0s - loss: 0.4868 -‚ê£
,‚Üíaccuracy:
0.7461
Epoch 1: val_accuracy improved from -inf to 0.83408, saving model to
best_model.keras
586/586 [==============================] - 32s 52ms/step - loss: 0.4868 -
accuracy: 0.7461 - val_loss: 0.3796 - val_accuracy: 0.8341
Epoch 2/100
586/586 [==============================] - ETA: 0s - loss: 0.2927 -‚ê£
,‚Üíaccuracy:
0.8811
Epoch 2: val_accuracy improved from 0.83408 to 0.86656, saving model to

430
15 Recurrent Neural Networks
best_model.keras
586/586 [==============================] - 30s 50ms/step - loss: 0.2927 -
accuracy: 0.8811 - val_loss: 0.3194 - val_accuracy: 0.8666
Epoch 3/100
586/586 [==============================] - ETA: 0s - loss: 0.2374 -‚ê£
,‚Üíaccuracy:
0.9073
Epoch 3: val_accuracy did not improve from 0.86656
586/586 [==============================] - 30s 50ms/step - loss: 0.2374 -
accuracy: 0.9073 - val_loss: 0.3407 - val_accuracy: 0.8578
Epoch 4/100
586/586 [==============================] - ETA: 0s - loss: 0.2070 -‚ê£
,‚Üíaccuracy:
0.9202
Epoch 4: val_accuracy did not improve from 0.86656
586/586 [==============================] - 31s 53ms/step - loss: 0.2070 -
accuracy: 0.9202 - val_loss: 0.3395 - val_accuracy: 0.8558
Epoch 5/100
586/586 [==============================] - ETA: 0s - loss: 0.1751 -‚ê£
,‚Üíaccuracy:
0.9348
Epoch 5: val_accuracy did not improve from 0.86656
586/586 [==============================] - 32s 55ms/step - loss: 0.1751 -
accuracy: 0.9348 - val_loss: 0.3844 - val_accuracy: 0.8664
print(history.history.keys())
print(imdb_model.summary())
# plotting the results
epoch_count = range(1, len(history.history['loss']) + 1)
plt.figure(figsize=(13,4), dpi=150)
plt.subplot(121)
plt.plot(epoch_count, history.history['loss'], 'b', label = 'training‚ê£
,‚Üíloss')
plt.plot(epoch_count, history.history['val_loss'], 'r', label =‚ê£
,‚Üí'validation loss')
plt.legend()
plt.ylabel('loss')
plt.xlabel('epoch')
plt.subplot(122)
plt.plot(epoch_count, history.history['accuracy'], 'b', label =‚ê£
,‚Üí'training accuracy')
plt.plot(epoch_count, history.history['val_accuracy'], 'r', label =‚ê£
,‚Üí'validation accuracy')
plt.legend()
plt.ylabel('accuracy')
plt.xlabel('epoch')
best_imdb_model = keras.models.load_model("best_model.keras")

15.4 Implementation in Keras: Sentiment ClassiÔ¨Åcation
431
loss, accuracy = best_imdb_model.evaluate(X_test_padded_fixed_len,‚ê£
,‚Üíy_test, verbose=1)
print('Test accuracy = {:.3f}'.format(accuracy))
dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])
Model: "sequential_2"
_________________________________________________________________
Layer (type)
Output Shape
Param #
=================================================================
embedding_2 (Embedding)
(None, None, 16)
80000
gru_1 (GRU)
(None, 32)
4800
dense_2 (Dense)
(None, 1)
33
=================================================================
Total params: 84,833
Trainable params: 84,833
Non-trainable params: 0
_________________________________________________________________
None
782/782 [==============================] - 8s 9ms/step - loss: 0.3210 -
accuracy: 0.8637
Test accuracy = 0.864
1.0
1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0
epoch
0.20
0.25
0.30
0.35
0.40
0.45
0.50
loss
training loss
validation loss
1.0
1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0
epoch
0.750
0.775
0.800
0.825
0.850
0.875
0.900
0.925
accuracy
training accuracy
validation accuracy
Fig. 15.6: The loss (left) and the accuracy (right) of the GRU classiÔ¨Åer with a word
embedding layer as functions of epoch for both the training and the validation sets.
Exercises:
Exercise 1: In the sentiment classiÔ¨Åcation application using IMDB dataset, consider
the same layers and datasets that were used in this chapter.
(A) Remove the embedding layer. Train the network and and estimate the accuracy
on the test set. Comment on the results.

432
15 Recurrent Neural Networks
(B) Replace the GRU layer with an LSTM layer with the same number of hidden
units. What is the accuracy on the test set (make sure to use the embedding layer
as in the original network)?
(C) In this part, we wish to show the utility of CNNs for analyzing sequen-
tial data. In this regard, replace the GRU layer with a 1D convolutional
layer (layers.Conv1D) with 32 Ô¨Ålters and kernel of size 7. Use a one-
dimensional max-pooling (layers.MaxPooling1D) with a pooling size of
2 (default), followed by a (one-dimensional) global average pooling layer
(GlobalAveragePooling1D). What is the accuracy on the test set (use the
embedding layer as in the original network)?
Exercise 2: In a regression problem, suppose we have an input sequence of (x1, ..., xT)
where xt = t ‚àí2 for t = 1, . . .,T and T ‚â•2. We would like to use the standard
RNN with a ‚Äútanh" activation function and a 2-dimensional hidden state ht for
t = 1, ...,T to estimate an output variable at t, denoted yt, where t = 1, ...,T. Assume
h0 = [0, 0]T and all weight matrices/vectors and biases used in the structure of the
RNN are weights/vectors/scalers of appropriate dimensions with all elements being
1. What is y2 (i.e., yt for t = 2)?
A) 1
B) 2.99
C) 2.92
D) 2.52
E) 3.19
√â Exercise 3: Describe in words the main cause of the vanishing and exploding
gradient problems in a standard RNN with a linear activation function.
Exercise 4: Consider a GRU with two layers that is constructed by the standard RNN
stacking approach. In each layer we assume we have only one hidden unit. Let hj
t
denote the hidden state at time t for layer j = 1, 2. Suppose x1 = 0.5, hj
0 = 0, ‚àÄj, and
all weight coeÔ¨Écients are 1 and biases -0.5. What is h2
1?
A) œÉlogistic
  ‚àí0.5œÉtanh
  ‚àí0.5
B) œÉlogistic
 0.5œÉtanh
  ‚àí0.5
C) œÉlogistic
  ‚àí0.5œÉtanh
 0.5
D) œÉlogistic
 0.5œÉtanh
 0.5
Exercise 5: In a regression problem with a univariate output variable, we use Keras
to train a GRU layer with 32 hidden units, followed by a dense layer. The network
predicts the output variable based on bivariate input sequences of length 200. How
many parameters in total are learned for this network?
A) 41697
B) 48264
C) 3489
D) 3297
E) 3456
Exercise 6: In a sentiment classiÔ¨Åcation application with binary output, we use
Keras to train a network that includes a word embedding layer for a vocabulary size
of 1000 and embedding dimensionality of 8, followed by a standard RNN layer with
64 hidden units, and then a dense layer with a logistic sigmoid activation function.

15.4 Implementation in Keras: Sentiment ClassiÔ¨Åcation
433
We use this network to classify word sequences of length 200. How many parameters
in total are learned for this network?
A) 12737
B) 6272
C) 6337
D) 4737

References
Abdel-Hamid, O., Mohamed, A.-r., Jiang, H., Deng, L., Penn, G., and Yu, D. (2014).
Convolutional neural networks for speech recognition. IEEE/ACM Transactions
on Audio, Speech, and Language Processing, 22(10):1533‚Äì1545.
Abibullaev, B. and Zollanvari, A. (2019). Learning discriminative spatiospectral fea-
tures of erps for accurate brain-computer interfaces. IEEE Journal of Biomedical
and Health Informatics, 23(5):2009‚Äì2020.
Adam-Bourdarios, C., Cowan, G., Germain, C., Guyon, I., K√©gl, B., and Rousseau,
D. (2015). The Higgs boson machine learning challenge. In Cowan, G., Germain,
C., Guyon, I., K√©gl, B., and Rousseau, D., editors, Proceedings of the NIPS
2014 Workshop on High-energy Physics and Machine Learning, volume 42 of
Proceedings of Machine Learning Research, pages 19‚Äì55.
Ambroise, C. and McLachlan, G. J. (2002). Selection bias in gene extraction on the
basis of microarray gene-expression data. Proc. Natl. Acad. Sci. USA, 99:6562‚Äì
6566.
Anaconda (2023). https://www.anaconda.com/products/individual, Last
accessed on 2023-02-15.
Anderson, T. (1951). ClassiÔ¨Åcation by multivariate analysis. Psychometrika, 16:31‚Äì
50.
Arthur, D. and Vassilvitskii, S. (2007). k-means++: the advantages of careful seed-
ing. In SODA ‚Äô07: Proceedings of the eighteenth annual ACM-SIAM symposium
on Discrete algorithms, pages 1027‚Äì1035. Society for Industrial and Applied
Mathematics.
Bache, K. and Lichman, M. (2013). UCI machine learning repository. University of
California, Irvine, School of Information and Computer.
Bay, H., Tuytelaars, T., and Van Gool, L. (2006). Surf: Speeded up robust features.
In Leonardis, A., Bischof, H., and Pinz, A., editors, Computer Vision ‚Äì ECCV
2006, pages 404‚Äì417, Berlin, Heidelberg. Springer Berlin Heidelberg.
Bengio, Y., Simard, P., and Frasconi, P. (1994). Learning long-term dependen-
cies with gradient descent is diÔ¨Écult. IEEE Transactions on Neural Networks,
5(2):157‚Äì166.
435
¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
A. Zollanvari, Machine Learning with Python, https://doi.org/10.1007/978-3-031-33342-2 

436
References
Benjamini, Y. and Hochberg, Y. (1995). Controlling the false discovery rate: A
practical and powerful approach to multiple testing. Journal of the Royal Statistical
Society. Series B (Methodological), 57:289‚Äì300.
Bergstra, J. and Bengio, Y. (2012). Random search for hyper-parameter optimization.
J. Mach. Learn. Res., 13:281‚Äì305.
Bishop, C. (2006). Pattern Recognition and Machine Learning. Springer-Verlag.
BlashÔ¨Åeld, R. K. (1976). Mixture model tests of cluster analysis: Accuracy of four
agglomerative hierarchical methods. Psychological Bulletin, 83:377‚Äì388.
Braga-Neto, U. M. (2020).
Fundamentals of Pattern Recognition and Machine
Learning. Springer.
Braga-Neto, U. M., Zollanvari, A., and Dougherty, E. R. (2014). Cross-validation
under separate sampling: strong bias and how to correct it.
Bioinformatics,
30(23):3349‚Äì3355.
Breiman, L. (1996). Bagging predictors. Machine Learning, 24(2):123‚Äì140.
Breiman, L. (1999). Pasting small votes for classiÔ¨Åcation in large databases and
on-line. Machine Learning, 36(1-2):85‚Äì103.
Breiman, L. (2001). Random forests. Machine Learning, 45(1):5‚Äì32.
Breiman, L. (2004). Population theory for boosting ensembles. The Annals of
Statistics, 32(1):1‚Äì11.
Breiman, L., Friedman, J. H., Olshen, R. A., and Stone, C. J. (1984). ClassiÔ¨Åcation
and Regression Trees. Chapman and Hall/CRC.
Chatgpt (2022). https://openai.com/blog/chatgpt, Last accessed on 2023-
02-15.
Chen, T. and Guestrin, C. (2016). Xgboost: A scalable tree boosting system. In
22nd ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining.
Chen, T. and He, T. (2015). Higgs Boson Discovery with Boosted Trees. In Cowan,
G., Germain, C., Guyon, I., K√©gl, B., and Rousseau, D., editors, Proceedings of the
NIPS 2014 Workshop on High-energy Physics and Machine Learning, volume 42
of Proceedings of Machine Learning Research, pages 69‚Äì80.
Cho, K., van Merri√´nboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk,
H., and Bengio, Y. (2014). Learning phrase representations using RNN encoder‚Äì
decoder for statistical machine translation. In Proceedings of the 2014 Conference
on Empirical Methods in Natural Language Processing (EMNLP), pages 1724‚Äì
1734.
Chollet, F. (2021). Deep Learning with Python. Manning, Shelter Island, NY, second
edition.
Chung, J., Gulcehre, C., Cho, K., and Bengio, Y. (2014). Empirical evaluation of
gated recurrent neural networks on sequence modeling.
Clemmensen, L., Witten, D., Hastie, T., and Ersb√∏ll, B. (2011). Sparse discriminant
analysis. Technometrics, 53(4):406‚Äì413.
Cybenko, G. (1989). Approximation by superpositions of a sigmoidal function.
Math. Control Signal Systems, 2:303‚Äì314.

References
437
Dalal, N. and Triggs, B. (2005). Histograms of oriented gradients for human de-
tection. In 2005 IEEE Computer Society Conference on Computer Vision and
Pattern Recognition (CVPR‚Äô05), volume 1, pages 886‚Äì893.
Dechter, R. (1986). Learning while searching in constraint-satisfaction-problems.
In Proceedings of AAAI.
Devore, J. L., Farnum, N. R., and Doi, J. A. (2013). Applied Statistics for Engineers
and Scientists. Cengage Learning, third edition.
Devroye, L., GyorÔ¨Å, L., and Lugosi, G. (1996). A Probabilistic Theory of Pattern
Recognition. Springer, New York.
Dougherty, E. R., Kim, S., and Chen, Y. (2000). CoeÔ¨Écient of determination in
nonlinear signal processing. Signal Processing, 80(10):2219‚Äì2235.
Drucker, H. (1997). Improving regressors using boosting techniques. In International
Conference on Machine Learning.
Duda, R. O., Hart, P. E., and Stork, D. G. (2000). Pattern ClassiÔ¨Åcation. Wiley.
Dudani, S. A. (1976). The distance-weighted k-nearest-neighbor rule. IEEE Trans-
actions on Systems, Man, and Cybernetics, SMC-6(4):325‚Äì327.
Dupuy, A. and Simon, R. M. (2007). Critical review of published microarray studies
for cancer outcome and guidelines on statistical analysis and reporting. J Natl.
Cancer Inst., 99:147‚Äì157.
Efron, B. (1983). Estimating the error rate of a prediction rule: Improvement on
cross-validation. Journal of the American Statistical Association, 78(382):316‚Äì
331.
Efron, B. (2007). Size, power and false discovery rates. The Annals of Statistics,
35(4):1351‚Äì1377.
f2py (2023). https://www.numfys.net/howto/F2PY/, Last accessed on 2023-
02-15.
Fisher, R. (1936). The use of multiple measurements in taxonomic problems. Ann.
Eugen., 7:179‚Äì188.
Fisher, R. (1940). The precision of discriminant function. Ann. Eugen., 10:422‚Äì429.
Fix, E. and Hodges, J. (1951). Discriminatory analysis, nonparametric discrimi-
nation: consistency properties. Technical report. Randolph Field, Texas: USAF
School of Aviation Medicine.
Forgy, E. (1965).
Cluster analysis of multivariate data: EÔ¨Éciency versus inter-
pretability of classiÔ¨Åcation. Biometrics, 21(3):768‚Äì769.
Freund, Y. and Schapire, R. E. (1997). A decision-theoretic generalization of on-
line learning and an application to boosting. Journal of Computer and System
Sciences, 55(1):119‚Äì139.
Friedman, J., Hastie, T., and Tibshirani, R. (2000). Additive Logistic Regression: a
Statistical View of Boosting. The Annals of Statistics, 38(2).
Friedman, J. H. (2001). Greedy function approximation: A gradient boosting ma-
chine. Annals of Statistics, 29:1189‚Äì1232.
Friedman, J. H. (2002). Stochastic gradient boosting. Computational Statistics &
Data Analysis, 38:367‚Äì378.
Fukushima, K. (1975). Cognitron: A self-organizing multilayered neural network.
Biological Cybernetics, 20:121‚Äì136.

438
References
Fukushima, K. (1980). Neocognitron: A self-organizing neural network model for
a mechanism of pattern recognition unaÔ¨Äected by shift in position. Biological
Cybernetics, 36:193‚Äì202.
Gers, F., Schmidhuber, J., and Cummins, F. (1999). Learning to forget: continual
prediction with lstm. In 1999 Ninth International Conference on ArtiÔ¨Åcial Neural
Networks ICANN 99. (Conf. Publ. No. 470), volume 2, pages 850‚Äì855.
Goodfellow, I. J., Bengio, Y., and Courville, A. (2016). Deep Learning. MIT Press,
Cambridge, MA, USA. http://www.deeplearningbook.org.
Graves, A. (2013). Generating sequences with recurrent neural networks. CoRR.
Graves, A., Mohamed, A., and Hinton, G. (2013). Speech recognition with deep
recurrent neural networks. In 2013 IEEE International Conference on Acoustics,
Speech and Signal Processing, pages 6645‚Äì6649.
Guardian-chatgpt
(2023).
https://www.theguardian.com/technology/
2023/feb/02/chatgpt-100-million-users-open-ai-fastest-growing-app,
Last accessed on 2023-02-15.
Guyon, I., Weston, J., Barnhill, S. D., and Vapnik, V. N. (2002). Gene selection for
cancer classiÔ¨Åcation using support vector machines. Machine Learning, 46:389‚Äì
422.
Hand, D. and Till, R. J. (2001). A simple generalisation of the area under the roc curve
for multiple class classiÔ¨Åcation problems. Machine Learning, 45(2):171‚Äì186.
Hansen, P. and Delattre, M. (1978). Complete-link cluster analysis by graph coloring.
Journal of the American Statistical Association, 73(362):397‚Äì403.
Hastie, T., Tibshirani, R., and Friedman, J. (2001). The Elements of Statistical
Learning. Springer, New York, NY, USA.
Hermans, M. and Schrauwen, B. (2013). Training and analysing deep recurrent
neural networks. In Burges, C., Bottou, L., Welling, M., Ghahramani, Z., and
Weinberger, K., editors, Advances in Neural Information Processing Systems,
volume 26.
Hochreiter, S. and Schmidhuber, J. (1997). Long Short-Term Memory. Neural
Computation, 9(8):1735‚Äì1780.
IMDB (2023).
https://ai.stanford.edu/~amaas/data/sentiment/, Last
accessed on 2023-02-15.
Ingber, L. (1998). Statistical mechanics of neocortical interactions: Training and test-
ing canonical momenta indicators of eeg. Mathematical and Computer Modelling,
27(3):33‚Äì64.
Ivakhnenko, A. G. (1971). Polynomial theory of complex systems. IEEE Transac-
tions on Systems, Man and Cybernetics, 4:364‚Äì378.
Ivakhnenko, A. G. and Lapa, V. G. (1965). Cybernetic Predicting Devices. CCM
Information Corporation, New York.
J. Friedman, T. Hastie, R. T. (2000). Additive logistic regression: A statistical view
of boosting. Annals of Statistics, 28:337‚Äì407.
Jain, A. and Dubes, R. (1988). Algorithms for Clustering Data. Prentice-Hall, Inc.
Upper Saddle River, NJ, USA.

References
439
Jain, N. C., Indrayan, A., and Goel, L. R. (1986). Monte carlo comparison of six
hierarchical clustering methods on random data. Pattern Recognition, 19(1):95‚Äì
99.
Johnson, S. C. (1967). Hierarchical clustering schemes. Psychometrika, 32(3):241‚Äì
254.
Kaggle-survey (2021). https://www.kaggle.com/kaggle-survey-2021, Last
accessed on 2023-02-15.
Kass, G. V. (1980).
An exploratory technique for investigating large quantities
of categorical data. Journal of the Royal Statistical Society. Series C (Applied
Statistics), 29(2):119‚Äì127.
Kaufman, L. and Rousseeuw, P. J. (1990). Finding Groups in Data: An Introduction
to Cluster Analysis. John Wiley.
Kay, S. M. (2013).
Fundamentals of Statistical Signal Processing, Volume III,
Practical Algorithm Development. Prentice Hall.
Kelley Pace, R. and Barry, R. (1997). Sparse spatial autoregressions. Statistics &
Probability Letters, 33(3):291‚Äì297.
keras-gru (2023).
https://github.com/tensorflow/tensorflow/blob/
e706a0bf1f3c0c666d31d6935853cfbea7c2e64e/tensorflow/python/
keras/layers/recurrent.py#L1598, Last accessed on 2023-02-15.
keras-initializers (2023).
https://keras.io/api/layers/initializers/,
Last accessed on 2023-02-15.
Keras-layers (2023). https://keras.io/api/layers/, Last accessed on 2023-
02-15.
Keras-losses (2023). https://keras.io/api/losses/, Last accessed on 2023-
02-15.
Keras-metrics (2023).
https://keras.io/api/metrics/, Last accessed on
2023-02-15.
Keras-optimizers (2023). https://keras.io/api/optimizers/, Last accessed
on 2023-02-15.
Keras-team (2019).
https://github.com/keras-team/keras/releases/
tag/2.3.0, Last accessed on 2023-02-15.
Kim, H. and Loh, W.-Y. (2001). ClassiÔ¨Åcation trees with unbiased multiway splits.
Journal of the American Statistical Association, 96(454):589‚Äì604.
Kittler, J. and DeVijver, P. (1982). Statistical properties of error estimators in per-
formance assessment of recognition systems. IEEE Trans. Pattern Anal. Machine
Intell., 4:215‚Äì220.
Krijthe, J. H. and Loog, M. (2014). Implicitly constrained semi-supervised linear
discriminantanalysi. In Proceedings of the 22nd International Conference on
Pattern Recognition, pages 3762‚Äì3767.
Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagenet classiÔ¨Åcation with
deep convolutional neural networks. In Pereira, F., Burges, C. J. C., Bottou, L., and
Weinberger, K. Q., editors, Advances in Neural Information Processing Systems
25, pages 1097‚Äì1105.
Kruskal, J. (1964). Multidimensional scaling by optimizing goodness of Ô¨Åt to a
nonmetric hypothesis. 29:1‚Äì27.

440
References
Kuiper, F. K. and Fisher, L. (1975). 391: A monte carlo comparison of six clustering
procedures. Biometrics, 31(3):777‚Äì783.
Lachenbruch, P. A. and Mickey, M. R. (1968). Estimation of error rates in discrimi-
nant analysis. Technometrics, 10:1‚Äì11.
Lance, G. N. and Williams, W. T. (1967). A general theory of classiÔ¨Åcatory sorting
strategies 1. hierarchical systems. 9(4):373‚Äì380.
Lawhern, V. J., Solon, A. J., Waytowich, N. R., Gordon, S. M., Hung, C. P., and
Lance, B. J. (2018). Eegnet: a compact convolutional neural network for eeg-based
brain-computer interfaces. Journal of Neural Engineering, 15(5):056013.
Lemoine,
B.
(2022).
https://cajundiscordian.medium.com/
is-lamda-sentient-an-interview-ea64d916d917,
Last
accessed
on
2023-02-15.
Lin, M., Chen, Q., and Yan, S. (2014).
Network in network.
In International
Conference on Learning Representations (ICLR).
Lloyd, S. P. (1982). Least squares quantization in pcm. IEEE Trans. Inf. Theory,
28(2):129‚Äì136.
Loh, W.-Y. and Shih, Y.-S. (1997). Split selection methods for classiÔ¨Åcation trees.
Statistica Sinica, 7(4):815‚Äì840.
Loog, M. (2014). Semi-supervised linear discriminant analysis through moment-
constraint parameter estimation. Pattern Recognition Letters, 37:24‚Äì31.
Loog, M. (2016). Contrastive pessimistic likelihood estimation for semi-supervised
classiÔ¨Åcation. IEEE Transactions on Pattern Analysis and Machine Intelligence,
38(3):462‚Äì475.
Louppe, G., Wehenkel, L., Sutera, A., and Geurts, P. (2013). Understanding variable
importances in forests of randomized trees. In Burges, C., Bottou, L., Welling,
M., Ghahramani, Z., and Weinberger, K., editors, Advances in Neural Information
Processing Systems, volume 26.
Lowe, D. (1999). Object recognition from local scale-invariant features. In Proceed-
ings of the Seventh IEEE International Conference on Computer Vision, volume 2,
pages 1150‚Äì1157 vol.2.
Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C. (2011).
Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguistics: Human Language Tech-
nologies, pages 142‚Äì150, Portland, Oregon, USA. Association for Computational
Linguistics.
MacQueen, J. B. (1967). Some methods for classiÔ¨Åcation and analysis of multivariate
observations. In Cam, L. M. L. and Neyman, J., editors, Proc. of the Ô¨Åfth Berkeley
Symposium on Mathematical Statistics and Probability, volume 1, pages 281‚Äì297.
University of California Press.
matplotlib-colors (2023). https://matplotlib.org/stable/gallery/color/
named_colors.html, Last accessed on 2023-02-15.
matplotlib-imshow (2023). https://matplotlib.org/stable/api/_as_gen/
matplotlib.axes.Axes.imshow.html, Last accessed on 2023-02-15.
matplotlib-markers (2023). https://matplotlib.org/stable/api/markers_
api.html#module-matplotlib.markers, Last accessed on 2023-02-15.

References
441
McCulloch, W. S. and Pitts, W. (1943). A logical calculus of the ideas immanent in
nervous activity. Bulletin of Mathematical Biophysics, 5:115‚Äì133.
McLachlan, G. J. (1975). Iterative reclassiÔ¨Åcation procedure for constructing an
asymptotically optimal rule of allocation in discriminant analysis. Journal of the
American Statistical Association, 70(350):365‚Äì369.
McLachlan, G. J. (2004). Discriminant Analysis and Statistical Pattern Recognition.
Wiley, New Jersey.
Michiels, S., Koscielny, S., and Hill, C. (2005). Prediction of cancer outcome with
microarrays: a multiple random validation strategy. Lancet, 365:488‚Äì92.
Miller, T. W. (2015). Marketing Data Science, modeling Techniques in Predictive
Analytics with R and Python. Pearson FT Press.
Milligan, G. W. and Isaac, P. D. (1980). The validation of four ultrametric clustering
algorithms. Pattern Recognition, 12(2):41‚Äì50.
M√ºller, A. and Guido, S. (2017). Introduction to Machine Learning with Python: A
Guide for Data Scientists. O‚ÄôReilly Media, Incorporated.
NumPy-array
(2023).
https://numpy.org/doc/stable/reference/
routines.array-creation.html, Last accessed on 2023-02-15.
NumPy-arrsum
(2023).
https://numpy.org/doc/stable/reference/
generated/numpy.ndarray.sum.html, Last accessed on 2023-02-15.
NumPy-sum
(2023).
https://numpy.org/doc/stable/reference/
generated/numpy.sum.html, Last accessed on 2023-02-15.
Pandas-dataframe (2023).
https://pandas.pydata.org/docs/reference/
api/pandas.DataFrame.html#pandas.DataFrame, Last accessed on 2023-
02-15.
Pandas-io
(2023).
https://pandas.pydata.org/pandas-docs/stable/
user_guide/io.html, Last accessed on 2023-02-15.
Pandas-readhtml
(2023).
https://pandas.pydata.org/pandas-docs/
stable/reference/api/pandas.read_html.html, Last accessed on 2023-
02-15.
Pascanu, R., Gulcehre, C., Cho, K., and Bengio, Y. (2014). How to construct deep
recurrent neural networks. In Proc. of ICLR.
Pascanu, R., Mikolov, T., and Bengio, Y. (2013).
On the diÔ¨Éculty of training
recurrent neural networks. In Proceedings of the 30th International Conference
on Machine Learning, volume 28, pages 1310‚Äì1318.
Perkins, S., Lacker, K., and Theiler, J. (2003). Grafting: Fast, incremental feature
selection by gradient descent in function space. J. Mach. Learn. Res., 3:1333‚Äì
1356.
Pikelis, V. S. (1973). The error of a linear classiÔ¨Åer with independent measurements
when the learning sample size is small. Statist. Problems of Control, 5:69‚Äì101.
in Russian.
Pillo, P. J. D. (1976). The application of bias to discriminant analysis. Communica-
tions in Statistics - Theory and Methods, 5:843‚Äì854.
pyplot
(2023).
https://matplotlib.org/stable/api/_as_gen/
matplotlib.pyplot.plot.html, Last accessed on 2023-02-15.

442
References
Python-copy (2023). https://docs.python.org/3/library/copy.html, Last
accessed on 2023-02-15.
Python-repeat (2023).
https://docs.python.org/3/library/itertools.
html#itertools.repeat, Last accessed on 2023-02-15.
Quinlan, J. R. (1986). Induction of decision trees. Machine Learning, 1:81‚Äì106.
Quinlan, J. R. (1993). C4.5: programs for machine learning. Morgan Kaufmann
Publishers Inc., San Francisco, CA, USA.
Ramalho, L. (2015). Fluent Python. O‚ÄôReilly.
Raudys, S. and Pikelis, V. (1980). On dimensionality, sample size, classiÔ¨Åcation
error, and complexity of classiÔ¨Åcation algorithm in pattern recognition. IEEE
Trans. Pattern Anal. Mach. Intell., 2:242‚Äì252.
Redmon, J., Divvala, S., Girshick, R., and Farhadi, A. (2016). You only look once:
UniÔ¨Åed, real-time object detection. In 2016 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 779‚Äì788.
Rice, J. A. (2006).
Mathematical Statistics and Data Analysis.
Belmont, CA:
Duxbury Press., third edition.
Rosenblatt, F. (1962). Principles of Neurodynamics: Perceptrons and the Theory of
Brain Mechanisms. Spartan Books, Washington, D.C.
Rousseeuw, P. (1987). Silhouettes: A graphical aid to the interpretation and validation
of cluster analysis. Journal of Computational and Applied Mathematics, 20:53‚Äì
65.
Schuster, M. and Paliwal, K. (1997). Bidirectional recurrent neural networks. IEEE
Transactions on Signal Processing, 45(11):2673‚Äì2681.
Scikeras-wrappers (2023).
https://www.adriangb.com/scikeras/stable/
generated/scikeras.wrappers.KerasClassifier.htmls, Last accessed
on 2023-02-15.
Scikit-digits
(2023).
https://scikit-learn.org/stable/modules/
generated/sklearn.datasets.load_digits.html,
Last
accessed
on
2023-02-15.
Scikit-eval (2023).
https://scikit-learn.org/stable/modules/model_
evaluation.html, Last accessed on 2023-02-15.
Scikit-kNN-C
(2023).
https://scikit-learn.org/stable/modules/
generated/sklearn.neighbors.KNeighborsClassifier.html, Last ac-
cessed on 2023-02-15.
Scikit-metrics
(2023).
https://scikit-learn.org/stable/modules/
model{_}evaluation.html, Last accessed on 2023-02-15.
Scikit-silhouette
(2023).
https://scikit-learn.org/stable/modules/
generated/sklearn.metrics.silhouette_samples.html#sklearn.
metrics.silhouette_samples, Last accessed on 2023-02-15.
Scikit-split
(2023).
https://scikit-learn.org/stable/modules/
generated/sklearn.model_selection.train_test_split.html,
Last
accessed on 2023-02-15.
scipy (2023).
https://docs.scipy.org/doc/scipy/reference/stats.
html, Last accessed on 2023-02-15.

References
443
scipy-normal (2023).
https://docs.scipy.org/doc/scipy/reference/
generated/scipy.stats.multivariate_normal.html, Last accessed on
2023-02-15.
Selim, S. Z. and Ismail, M. A. (1984). K-means-type algorithms: A generalized
convergence theorem and characterization of local optimality. IEEE Transactions
on Pattern Analysis and Machine Intelligence, PAMI-6(1):81‚Äì87.
Shahrokh Esfahani, M. and Dougherty, E. R. (2013). EÔ¨Äect of separate sampling on
classiÔ¨Åcation accuracy. Bioinformatics, 30(2):242‚Äì250.
Simonyan, K. and Zisserman, A. (2015). Very deep convolutional networks for large-
scale image recognition. In Bengio, Y. and LeCun, Y., editors, 3rd International
Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May
7-9, 2015, Conference Track Proceedings.
Smith, C. (1947). Some examples of discrimination. Annals of Eugenics, 18:272‚Äì
282.
Sneath, P. H. A. and Sokal, R. R. (1973). Numerical Taxonomy: The Principles and
Practice of Numerical ClassiÔ¨Åcation. W H Freeman & Co (Sd).
Soriƒá, B. (1989). Statistical discoveriess and eÔ¨Äect-size estimation. Journal of the
American Statistical Association, 84:608‚Äì610.
Srivastava, N., Hinton, G. E., Krizhevsky, A., Sutskever, I., and Salakhutdinov,
R. (2014). Dropout: a simple way to prevent neural networks from overÔ¨Åtting.
Journal of Machine Learning Research, 15:1929‚Äì1958.
TensorÔ¨Çow (2023). https://anaconda.org/conda-forge/tensorflow, Last
accessed on 2023-02-15.
TensorÔ¨Çow-tpu (2023).
https://www.tensorflow.org/guide/tpu, Last ac-
cessed on 2023-02-15.
Theodoridis, S. and Koutroumbas, K. (2009). Pattern Recognition, Fourth Edition.
Academic Press.
Thoppilan, R., Freitas, D. D., Hall, J., Shazeer, N., Kulshreshtha, A., and et al.
(2022). Lamda: Language models for dialog applications. CoRR.
Thorndike, R. (1953). Who belongs in the family? Psychometrika, 18(4):267‚Äì276.
Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of
the Royal Statistical Society. Series B (Methodological), 58(1):267‚Äì288.
UCI-EEG
(2023).
https://archive.ics.uci.edu/ml/datasets/EEG+
Database, Last accessed on 2023-02-15.
Vicari, D. (2014). ClassiÔ¨Åcation of asymmetric proximity data. Journal of ClassiÔ¨Å-
cation, 31:386‚Äì420.
Wald, A. (1944). On a statistical problem arising in the classiÔ¨Åcation of an individual
into one of two groups. Ann. Math. Statist., 15:145‚Äì162.
Ward, J. H. (1963). Hierarchical grouping to optimize an objective function. Journal
of the American Statistical Association, 58(301):236‚Äì244.
Werbos, P. J. (1990). Backpropagation through time: what it does and how to do it.
Proceedings of the IEEE, 78(10):1550‚Äì1560.
Wolpert, D. H. (1992). Stacked generalization. Neural Networks, 5(2):241‚Äì259.

444
References
Yarowsky, D. (1995). Unsupervised word sense disambiguation rivaling supervised
methods. In In Proceedings of the 33rd Annual Meeting of the Association for
Computational Linguistics, pages 189‚Äì196.
Zhu, J., Zou, H., Rosset, S., and Hastie, T. J. (2009). Multi-class adaboost. Statistics
and Its Interface, 2:349‚Äì360.
Zollanvari, A. and Dougherty, E. R. (2015). Generalized consistent error estimator
of linear discriminant analysis. IEEE Trans. Sig. Proc., 63:2804‚Äì2814.
Zollanvari, A. and Dougherty, E. R. (2019). Optimal bayesian classiÔ¨Åcation with
vector autoregressive data dependency. IEEE Transactions on Signal Processing,
67(12):3073‚Äì3086.
Zollanvari, A., James, A. P., and Sameni, R. (2020). A theoretical analysis of the
peaking phenomenon in classiÔ¨Åcation. Journal of ClassiÔ¨Åcation, 37:421‚Äì434.

Index
K-means, 322, 324
K-means++, 323, 324
P-value, 287
F-statistic, 287, 290
F-test, 287
F1 score, 252
l1 regularization, 167, 177
l2 regularization, 167, 177
adagrad, 366
adam, 366
binary_crossentropy, 367
categorical_crossentropy, 367, 369
keras.
Model.compile, 366, 402
Model.evaluate, 377, 402
Model.fit, 371, 402
Model.predict, 377, 402
Model, 363
Sequential, 363
callbacks.EarlyStopping, 371
callbacks.ModelCheckpoint, 372
layers.Conv1D, 413, 432
layers.Conv2D, 402
layers.Dense, 364
layers.Dropout, 382
layers.Embedding, 428
layers.Flatten, 403
layers.GRU, 422
layers.LSTM, 422
layers.TextVectorization, 424
layers.simpleRNN, 416
wrappers.scikit_learn, 386
keras, 17, 358
mean_absolute_error, 367
mean_squared_error, 367
rmsprop, 366
scikeras, 386
scipy.cluster.hierarchy.dendrogram,
347
scipy.cluster.hierarchy.linkage,
346
scipy.stats.multivariate_normal,
99
scipy.stats, 99, 276
sgd, 366
sklearn.
cluster.AgglomerativeClustering,
346
cluster.KMeans, 324
datasets.make_classification,
284
ensemble.AdaBoostClassifier,
222
ensemble.AdaBoostRegressor, 222
ensemble.BaggingClassifier, 217,
219
ensemble.BaggingRegressor, 217,
219
ensemble.GradientBoostingClassifier,
229
445
¬© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
A. Zollanvari, Machine Learning with Python, https://doi.org/10.1007/978-3-031-33342-2 

446
INDEX
ensemble.GradientBoostingRegressor,
229
ensemble.RandomForestClassifier,
218
ensemble.RandomForestRegressor,
218
feature_selection.RFECV, 299
feature_selection.RFE, 299
feature_selection.SelectFdr,
290
feature_selection.SelectFpr,
290
feature_selection.SelectFwe,
290
feature_selection.SelectKBest,
290
feature_selection.f_classif,
290
feature_selection.f_regression,
290
feature_selection.
SequentialFeatureSelector, 295
impute.SimpleImputer, 181
linear_model.ElasticNet, 180
linear_model.Lasso, 180
linear_model.LinearRegression,
179
linear_model.Ridge, 179
metrics.RocCurveDisplay, 281
metrics.accuracy_score, 259
metrics.confusion_matrix, 259
metrics.f1_score, 260
metrics.precison_score, 260
metrics.recall_score, 260
metrics.roc_auc_score, 260
metrics.roc_curve, 281
metrics.silhouette_samples, 333
metrics.silhouette_score, 333
model_selection.GridSearchCV,
271, 274
model_selection.KFold, 245
model_selection.LeaveOneOut,
246
model_selection.PredefinedSplit,
274
model_selection.RandomizedSearchCV,
276
model_selection.ShuffleSplit,
249
model_selection.StratifiedKFold,
249
model_selection.cross_val_score,
248, 262, 263
model_selection.cross_validate,
263
model_selection.train_test_split,
116
neighbors.KNeighborsClassifier,
123
neighbors.KNeighborsRegressor,
144
pipeline.Pipeline, 308, 312
preprocessing.LabelEncoder, 130
preprocessing.MinMaxScaler, 122
preprocessing.OrdinalEncoder,
130
preprocessing.StandardScaler,
122
tree.DecisionTreeClassifier,
199
tree.DecisionTreeRegressor, 199
utils.Bunch, 113
sklearn, see scikit-learn
sparse_categorical_crossentropy,
367, 369
tensorflow, 17, 358
xgboost, 17, 234
accuracy, 127, 253
AdaBoost, 220
AdaBoost.R2, 222, 223
AdaBoost.SAMME, 220, 222
agent, 5
agglomerative clustering, 336, 346
alternative hypothesis, 287
Anaconda, 21
analysis of variance, 287, 290
ANOVA, see analysis of variance
apparent error, 239

INDEX
447
area under the receiver operating
characteristic curve, 255
artiÔ¨Åcial intelligence, 7
attribute, 10
average-pooling, 398
backpropagation, 357
backpropagation through time, 419
backward hidden states, 417
bagging, 216
batch size, 357
Benjamini‚ÄìHochberg false discovery
rate procedure, 289, 290
bidirectional RNN, 417
binary decision tree, 187
Bonferroni correction, 288
boosting, 220
bootstrap estimator, 240
C4.5, 191
CART classiÔ¨Åer, see classiÔ¨Åcation
and regression tree classiÔ¨Åer
CART regressor, see classiÔ¨Åcation
and regression tree regressor
CHAID, 191
chaining eÔ¨Äect, 344
classiÔ¨Åcation, 3
classiÔ¨Åcation and regression tree
classiÔ¨Åer, 191, 199
classiÔ¨Åcation and regression tree
regressor, 197, 199
classiÔ¨Åer, 3
cluster, 319
cluster dissimilarity function,
pairwise, 337
cluster validity analysis, 330
clustering, 3, 319
clustering, agglomerative, see
agglomerative clustering
clustering, complete linkage, see
linkage, complete
clustering, divisive, see divisive
clustering
clustering, group average linkage,
see linkage, group average
clustering, hierarchical, see
hierarchical clustering
clustering, partitional, see partitional
clustering
clustering, single linkage, see
linkage, single
clustering, Ward‚Äôs linkage, see
linkage, Ward
CNN, see convolutional neural
network
coeÔ¨Écient of determination, see
R-squared
Colab, 359
conda, 21, 22
confusion matrix, 251
convolution, 395
convolutional neural network, 393,
402, 413
cross-correlation, 394
cross-validation, 240
cross-validation, external, 305, 316
cross-validation, feature selection,
316
cross-validation, internal, 316
cross-validation, model evaluation,
316
cross-validation, model selection,
316
cross-validation, nested, 316
cross-validation, random
permutation, 244, 249
cross-validation, standard K-fold,
241, 248
cross-validation, stratiÔ¨Åed K-fold,
243, 249
CRUISE, 191
curse of dimensionality, 284
data, 8
data leakage, 121
decision stump, 222
decision tree, 188
deep RNN, 416
dendrogram, 340
dense layer, see fully connected layer

448
INDEX
density estimation, 3
design process, 6
dimensionality, 10, 283
dimensionality reduction, 3, 284
dissimilarity function, average, 337
dissimilarity function, max, 337
dissimilarity function, min, 337
dissimilarity function, minimum
variance, see dissimilarity
function, Ward
dissimilarity function, Ward, 337
distance-weighted kNN classiÔ¨Åer,
136
distance-weighted kNN regressor,
146
divisive clustering, 336
dropout, 381
dropout rate, 381
E0 estimator, 240
early stopping, 222, 372
elastic-net, 167, 177
elbow phenomenon, 328
embedded methods, 297
embedding layer, see word
embedding layer
encoding, 115, 130, 367, 428
ensemble learning, 9
epoch, 357
error estimation, 16, 237
error rate, 126, 252
exclusive clustering, 319
exhaustive search, 293
exploding gradient problem, 418,
421
false discovery rate, 252, 288
false negative, 251
false negative rate, 252
false positive, 251
false positive rate, 251
family-wise error rate, 288, 290, 301
fdr, see false discovery rate
feature, 10
feature extraction, 10, 283
feature map, 396
feature selection, 10, 283, 286
feature vector, 10
Ô¨Ålter methods, 286
Ô¨Çattening, 403
forward hidden states, 417
fully connected layer, 364
FWER, see family-wise error rate
gated recurrent unit, 421
GBRT classiÔ¨Åer, see gradient
boosting regression tree
classiÔ¨Åer
GBRT regressor, see gradient
boosting regression tree
regressor
global average pooling, 403, 413
gradient boosting, 222
gradient boosting regression tree
classiÔ¨Åer, 228, 229
gradient boosting regression tree
regressor, 227, 229
gradient clipping, 421
grafting, 298
graphical processing unit, 359, 379
greedy search, 293, 294
grid search, 268
grid search using cross-validation,
270
grid search using validation set, 269,
273
GRU, see gated recurrent unit
hierarchical clustering, 335
hold-out estimator, 238
hyperparameter, 6
ID3, 191
impurity drop, 192
intelligence, 7
interpretability, 284
iterative embedded methods, 298
Jupyter notebook, 23
k-nearest neighbors classiÔ¨Åer, 123,
133

INDEX
449
k-nearest neighbors regressor, 138,
144
Kaggle, 17
kNN classiÔ¨Åer, see k-nearest
neighbors classiÔ¨Åer
kNN regressor, see k-nearest
neighbors regressor
labeled data, 5
lasso, 167, 177, 180
LDA, see linear discriminant analysis
leaf nodes, 187, 193
learning, 8
least squares, 176
leave-one-out, 242
linear discriminant analysis, 155,
157, 161
linear regression, 176, 179
linkage, complete, 338, 344, 346
linkage, group average, 339, 344, 346
linkage, minimum variance, see
linkage, Ward
linkage, single, 338, 344, 346
linkage, Ward, 339, 344, 346
Lloyd‚Äôs algorithm, 321
local translation invariance, 399
logistic classiÔ¨Åcation, see logistic
regression
logistic regression, 162, 166, 167
long short-term memory, 421
loss, 357, 367
LSTM, see long short-term memory
machine learning, 2
macro averaging, 259
MAE, see mean absolute error
marketing data science, 130
Markov decision process, 5
matrix updating algorithm, 340, 341
max-pooling, 398
MDI, see mean decrease impurity
mean absolute error, 266
mean decrease impurity, 297
mean square error, 265
micro averaging, 259
mini-batch, 357
model evaluation, 16
model selection, 6, 237, 267
MSE, see mean square error
MUA, see matrix updating algorithm
multilayer perceptron, 352
multinomial logistic regression, 166
multiple linear regression, 176
multiple testing corrections, 288
non-iterative embedded methods,
297
normal equation, 177
null hypothesis, 287
observed signiÔ¨Åcance level, see
P-value
odds, 170
odds ratio, 171
one-versus-rest, 259
optimizer, 357, 366
ordinary least squares, 176
overÔ¨Åtting, 379
padding, 395
parameter sharing, 397
partitional clustering, 319
pasting, 219
peaking phenomenon, 284
Pearson‚Äôs correlation coeÔ¨Écient, 141
perceptron, 352, 355
pip, 21
positive predictive value, 252
precision, see positive predictive
value
pseudo labels, 5
Python programming, 17, 23
StopIteration, 52
__init__.py, 51, 64
__next__() method, 52, 55
def keyword, 47
enumerate function, 44
for loop, 41
help, 32
if-elif-else statement, 47
iter function, 52
matplotlib, 94

450
INDEX
Axes.plot, 95
%matplotlib, 94
pyplot.figure, 96
pyplot.pcolormesh, 102
pyplot.plot, 95
pyplot.scatter, 99
pyplot.subplots, 97
pyplot.subplot, 96
pyplot style, 94
backend, 94
frontend, 94
object-oriented style, 95
next function, 52
numpy, 63
add function, 76
arange function, 68
argmax function, 76
argmin function, 76
argsort function, 76
array function, 64
astype method, 65
copy method, 71
divide function, 76
dtype attribute, 65
eye function, 68
floor_divide function, 76
full function, 68
itemsize attribute, 67
log10 function, 76
log function, 76
max function, 76
mean function, 76
meshgrid function, 102
min function, 76
mod function, 76
multiply function, 76
ndarray, 63
ndim attribute, 66
newaxis object, 74
ones function, 68
power function, 76
prod function, 76
random.normal function, 69
ravel method, 74
reshape method, 73
shape attribute, 66
size attribute, 66
sort function, 76
sqrt function, 76
std function, 76
subtract function, 76
sum function, 76
var function, 76
zeros function, 67
array indexing, 69
array slicing, 70
array view, 70
broadcasting, 77
fancy indexing, 71
ufuncs, see Python
programming, numpy,
universal functions
universal functions, 75
vectorized operation, 75, 77
pandas, 81
DataFrame, 86
DataFrame, columns attribute,
92
DataFrame, drop method, 105
DataFrame, fillna method,
105
DataFrame, iloc attribute, 91
DataFrame, index attribute, 92
DataFrame, loc attribute, 91
DataFrame, explicit index, 90
DataFrame, from a dictionary of
Series, 88
DataFrame, from a list of
dictionaries, 89
DataFrame, from arrays or lists,
88
DataFrame, from dictionary of
arrays or lists, 88
DataFrame, implicit index, 90
DataFrame, masking, 91
Series, 81
Series, iloc attribute, 85
Series, index attribute, 82
Series, loc attribute, 85
Series, values attribute, 82

INDEX
451
Series, explicit index, 83
Series, implicit index, 83
read and write data, 93
range constructor, 43
yield keyword, 54
zip function, 45
alias, 51
arithmetic operator, 26
built-in data structures, 27
dictionary, 36
dictionary, dict constructor, 38
dictionary, items method, 43
dictionary, keys method, 37
dictionary, values method, 38
dictionary, key, 36
dictionary, value, 36
dynamically-typed, 24
extended iterable unpacking, 39,
40
generator expression, 57
generator function, 53
genexps, see Python
programming, generator
expression
idioms, 43
indentation, 41
iterable, 41
iterator, 41, 52
list, 27
list comprehension, 46
list, append method, 30
list, copy method, 31
list, insert method, 30
list, list constructor, 31
list, pop method, 30
list, remove method, 30
list, sort method, 30
list, deep copy, 31
list, indexing, 28
list, modifying elements, 30
list, shallow copy, 31
list, slicing, 29, 31
listcomps, see Python
programming, list
comprehension
logical operator, 26
magic function, 76
membership operator, 27
module, 49
package, 50
relational operator, 26
rounding errors, 26
sequence packing, 35
sequence unpacking, 34, 39
set, 38
set, diÔ¨Äerence, 39
set, intersection, 39
set, symmetric diÔ¨Äerence, 39
set, union, 39
string, 25
tuple, 33
variables, 24
QDA, see quadratic discriminant
analysis
quadratic discriminant analysis, 182
QUEST, 191
R-squared, 145, 266
random forest, 217
random search, 275
RCO, see relative change in odds
recall, see true positive rate
receiver operating characteristic
curve, 256, 281
recurrent neural network, 415, 416,
422
recursive feature elimination, 298,
299
recursive feature elimination with
cross-validation, 298
regression, 3
regressor, 3
regularization, 166
reinforcement learning, 5
relative change in odds, 172
residual sum of squares, 145, 176
resubstitution estimator, 239
RFE, see recursive feature
elimination

452
INDEX
RFE-CV, see recursive feature
elimination with
cross-validation
ridge, 167, 177
RNN, see recurrent neural network
ROC, see receiver operating
characteristic curve
ROC AUC, see area under the
receiver operating
characteristic curve
sample size, 10
SBS, see sequential backward search
scikit-learn, 17, 112
scikit-learn, estimator, 112
scikit-learn, predictor, 113
scikit-learn, transformer, 112
segmentation, 304
self-training, 5
semi-supervised learning, 5
sensitivity, see true positive rate
sequential backward search, 295
sequential forward search, 293
SFS, see sequential forward search
shrinkage, 166
shuÔ¨Ñe and split, see cross-validation,
random permutation
signal modeling, 10
signal segmentation, 12
signiÔ¨Åcance level, 287
silhouette coeÔ¨Écient, 330, 333
silhouette width, 331
silhouette width, average, 332
silhouette width, overall average, 332
simple linear regression, 176
softmax, 364
sparse connectivity, 395
speciÔ¨Åcity, see true negative rate
splits in CART, 191
splitter, see splitting strategy in
CART
splitting strategy in CART, 192
stacked RNN, 416
stacking, 215
statistical test, 287
strides, 397, 398
supervised learning, 2
surrogate classiÔ¨Åer, 241
tensor processing unit, 359
test data, 16
test set, 116
test statistic, 287
test-set error estimator, 127
text standardization, 424
text tokenization, 424
total sum of squares, 145
training, 6
training data, 6
translation equivariant, 399
true negative, 251
true negative rate, 251
true positive, 251
true positive rate, 251
underÔ¨Åtting, 193
unidirectional RNN, 417
unlabeled data, 5
unsupervised learning, 3
UPGMA, see linkage, group average
vanishing gradient problem, 418, 421
weighted averaging, 259
word embedding layer, 427
wrapper methods, 293
XGBoost, 230, 234


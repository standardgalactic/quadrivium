STOCHASTIC
MODELLING 
AND APPLIED 
PROBABILITY
Sophia L. Kalpazidou
Cycle Representations 
of Markov Processes
Second Edition
4^ Springer

Stochastic Mechanics
Random Media
Signal Processing and Image Synthesis 
Mathematical Economics and Finance
Stochastic Optimization
Stochastic Control
Stochastic Models in Life Sciences
Edited by
Advisory Board
Applications of
Mathematics
Stochastic Modelling 
and Applied Probability
28
B. Rozovskii
M. Yor
D. Dawson
D. Geman
G. Grimmett
I. Karatzas
F. Kelly
Y. Le Jan
B. 0ksendal
G. Papanicolaou
E. Pardoux

Applications of Mathematics
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
Fleming/Rishel, Deterministic and Stochastic Optimal Control (1975)
Marchuk, Methods of Numerical Mathematics, Second Ed. (1982)
Balakrishnan, Applied Functional Analysis, Second Ed. (1981)
Borovkov, Stochastic Processes in Queueing Theory (1976)
Liptser/Shiryayev, Statistics of Random Processes I: General Theory, Second Ed.
(1977)
Liptser/Shiryayev, Statistics of Random Processes II: Applications, Second Ed.
(1978)
Vorob’ev, Game Theory: Lectures for Economists and Systems Scientists (1977)
Shiryayev, Optimal Stopping Rules (1978)
Ibragimov/Rozanov, Gaussian Random Processes (1978)
Wonham, Linear Multivariable Control: A Geometric Approach, Third Ed. (1985)
Hida, Brownian Motion (1980)
Hestenes, Conjugate Direction Methods in Optimization (1980)
Kallianpur, Stochastic Filtering Theory (1980)
Krylov, Controlled Diffusion Processes (1980)
Prabhu, Stochastic Storage Processes: Queues, Insurance Risk, Dams, and Data
Communication, Second Ed. (1998)
Ibragimov/Has’minskii, Statistical Estimation: Asymptotic Theory (1981)
Cesari, Optimization: Theory and Applications (1982)
Elliott, Stochastic Calculus and Applications (1982)
Marchuk/Shaidourov, Difference Methods and Their Extrapolations (1983)
Hijab, Stabilization of Control Systems (1986)
Protter, Stochastic Integration and Differential Equations (1990)
Benveniste/Metivier/Priouret, Adaptive Algorithms and Stochastic Approximations 
(1990)
Kloeden/Platen, Numerical Solution of Stochastic Differential Equations (1992)
Kushner/Dupuis, Numerical Methods for Stochastic Control Problems in
Continuous Time, Second Ed. (2001)
Fleming/Soner, Controlled Markov Processes and Viscosity Solutions (2005)
Baccelli/Bremaud, Elements of Queueing Theory (1994)
Winkler, Image Analysis, Random Fields, and Dynamic Monte Carlo Methods:
An Introduction to Mathematical Aspects (1994)
Kalpazidou, Cycle Representations of Markov Processes, Second Ed. (2006)
Elliott/Aggoun/Moore, Hidden Markov Models: Estimation and Control (1995)
Hernandez-Lerma/Lasserre, Discrete-Time Markov Control Processes: Basic
Optimality Criteria (1996)
Devroye/Gyorfi/Lugosi, A Probabilistic Theory of Pattern Recognition (1996)
Maitra/Sudderth, Discrete Gambling and Stochastic Games (1996)
Embrechts/KlUppelberg/Mikosch, Modelling Extremal Events (1997)
Duflo, Random Iterative Models (1997)
Kushner/Yin, Stochastic Approximation and Recursive Algorithms and
Applications, Second Ed. (2003)
(continued after index)

Sophia L. Kalpazidou
Cycle Representations 
of Markov Processes
Second Edition
With 17 Figures
Springer

Sophia L. Kalpazidou
Aristotle University, Department of Mathematics
Thessaloniki 541 24
Greece 
sauth@otenet.gr
Managing Editors
B. Rozovskii
University of Southern California
Department of Mathematics
Kaprielian Hall KAP 108
3620 S. Vermont Avenue
Los Angeles, CA 90089
USA 
rozovsky@usc.edu
M. Yor
Laboratoire de Probabilites et Modeles Aleatoires 
Universite de Paris VI
175, rue du Chevaleret
75013 Paris, France
Mathematics Subject Classification (2000): 60J05, 60J10, 60J99
Library of Congress Cataloging-in-Publication Data
Kalpazidou, Sophia L.
Cycle representations of Markov processes / Sophia L. Kalpazidou.
p. cm.—(Applications of mathematics: vol. 28)
Includes bibliographical references and index.
ISBN 10: 0-387-29166-0 (New York : acid-free).—ISBN 13: 978-0387-29166-6 
(Berlin : acid-free)
1. Markov processes 2. Algebraic cycles. I. Title.
II. Series: Applications of mathematics: 28.
QA274.7.K354 1994
519.2' 33-dc20 
94-28223
Printed on acid-free paper.
© 2006 Springer Science+Business Media, Inc.
All rights reserved. This work may not be translated or copied in whole or in part without the written 
permission of the publisher (Springer Science+Business Media, Inc., 233 Spring Street, New York, 
NY 10013, USA), except for brief excerpts in connection with reviews or scholarly analysis. Use 
in connection with any form of information storage and retrieval, electronic adaptation, computer 
software, or by similar or dissimilar methodology now known or hereafter developed is forbidden.
The use in this publication of trade names, trademarks, service marks, and similar terms, even if they 
are not identified as such, is not to be taken as an expression of opinion as to whether or not they are 
subject to proprietary rights.
Printed in the United States of America. (TB/MVY)
987654321 
springer.com

To my father

Preface to the Second Edition
The cycle representations of Markov processes have been advanced after 
the publication of the first edition to many directions. One main purpose of 
these advances was the revelation of wide-ranging interpretations of the cy­
cle decompositions of Markov processes such as homologic decompositions, 
orthogonality equations, Fourier series, semigroup equations, disintegra­
tions of measures, and so on, which altogether express a genuine law of real 
phenomena.
The versatility of these interpretations is consequently motivated by the 
existence of algebraic-topological principles in the fundamentals of the cy­
cle representations of Markov processes, which eliberates the standard view 
on the Markovian modelling to new intuitive and constructive approaches. 
For instance, the ruling role of the cycles to partition the finite-dimensional 
distributions of certain Markov processes updates Poincare’s spirit to de­
scribing randomness in terms of the discrete partitions of the dynamical 
phase state; also, it allows the translation of the famous Minty’s painting 
lemma (1966) in terms of the stochastic entities.
Furthermore, the methods based on the cycle formula of Markov pro­
cesses are often characterized by minimal descriptions on cycles, which 
widely express a philosophical analogy to the Kolmogorovean entropic com­
plexity. For instance, a deeper scrutiny on the induced Markov chains into 
smaller subsets of states provides simpler descriptions on cycles than on the 
stochastic matrices involved in the “taboo probabilities.” Also, the recur­
rence criteria on cycles improve previous conditions based on the stochastic 
matrices, and provide plenty of examples.

viii
Preface to the Second Edition
The second edition unifies all the interpretations and trends of the cycle 
representations of Markov processes in the following additional chapters:
Chapter 8: Cycloid Markov Processes,
Chapter 9: Markov Processes on Banach Spaces on Cycles,
Chapter 10: The Cycles Measures,
Chapter 11: Wide Ranging Interpretations of the Cycle Representations.
Apart of that, it contains the new section 3.6 of Part I devoted to the 
induced circuit chains, and the section 1.4 of Part II devoted to “the recur­
rence criterions in terms of the weighted circuits for unidimensional random 
walks in random environment.”
Also, some improvements are introduced along the lines of the initial 
edition.
I would like to thank Professor Y. Derriennic for his persevering contri­
butions to the present edition expressed especially by his prototypes on the 
recurrence of unidimensional random walks in random environment.
Interesting ideas and results to Banach spaces on cycles are due to my 
collaboration with N. Kassimatis, Ch. Ganatsiou, and Joel E. Cohen.
Also, the Chinese School of Peking (Qian Minping, Qian Gong, Qian 
Min, Qian Cheng, Gong Guang, Jiang Da-Quan, Guang-Lu Gong, Hong 
Qian, and others) have been in parallel advanced the cycle representations 
to interesting applications in biomathematics and physics.
Finally, we all hope that the second edition will further encourage the 
research on the cycle theory and its impetus to Probability Theory, Measure 
Theory, Algebraic Topology, Mathematical Analysis, and related fields.
Thessaloniki
Sophia L. Kalpazidou

Preface
AQ/j.ovfr] itpy.vijq (pavsQtjq xqsggcdv.
HQa.’xksiToq
Unrevealed harmony is superior to the visible one.
Heraclitos
The purpose of the present book is to give a systematic and unified ex­
position of stochastic processes of the Markovian type, homogeneous and 
with either discrete or continuous parameter, which, under an additional 
assumption concerning the existence of invariant measures, can be defined 
by directed cycles or circuits. These processes are called cycle (or circuit) 
processes, and the corresponding collections of weighted cycles are called 
cycle representations.
The descriptions of the Markov transition law in terms of the cycles 
disclose new and special properties which have given an impetus to very 
intensive research concentrated on the connections between the geometric 
properties of the trajectories and the algebraic characterization of Markov 
processes.
Let us start with a few heuristic motivations for this new topic. The sim­
plest example leading to a cycle process arises when modeling the motion of 
a particle on a closed curve. Observe the particle’s motion through p (> 1) 
points of this curve at moments one unit of time apart. This amounts 
to a discretization of the curve into an ordered sequence c = (c(n), c(n + 
1),...,c(n +p - 1), c(n)), n = 0, ±1, ±2,..., called a directed circuit with 

x
Preface
period p(c) = p. The subsequence c = (c(n), c(n + 1),..., c(n + p — 1)) will 
be called a directed cycle (associated with the circuit c). Assign a positive 
number wc to c. Then, a normalized measure of the passage from state 
i = c(n) to j = c(n + 1) is given by wc/wc = 1. Therefore, if no influences 
occur, the passages from i to j can be codified by an infinite binary sequence 
y(i,j) =(0, 1, 0,...,0, 1,...) where 1 or 0 means that at some moment n the 
particle passes through or does not pass through (i, j).
The sequence y(i,j) is understood as a “nonrandom” sequence in the con­
text of Kolmogorov’s theory of complexities since both 1 and 0 appear pe­
riodically after each p steps. This happens because of the small complexity 
of the particle’s trajectory which consists of a circuit c alone. Then, when 
some “chaos” arises, it necessarily presupposes some complexity in the form 
of the particle’s trajectory. So, let us consider a further two, or even more 
than two, overlapping directed circuits c 1,..., cr ,r > 2, each associated 
with some positive number wcl ,l =1,...,r. Imagine that the particle ap­
pears sometime at the incident point i of certain circuits, say for simplicity, 
c 1,... ,cl,l < r. Then, the particle can continue its motion to another point 
j through which some circuits cm 1,..., cms, s < l, m 1,... ,ms e {1,..., l}, 
pass. A natural measure for the particle’s transition when moving from i 
to j can be defined as
(wCm 1 + wCm2 + ••• + wCms ) / (wC 1 + wC2 + ••• + wCl ) . 
(1)
Accordingly, the binary sequence codifying as above the appearances of 
the pair (i, j) along a tra jectory is given by a “more chaotic” sequence like 
y(i,j) =(0, 0, 0, 1, 0, 1, 0, 0, 1, 0,...), where 1 means that at some moment of 
time the particle passes through certain circuits containing (i, j). Further­
more, since expression (1) provides transition probability from i to j of a 
Markov chain £ = (£n, n = 0, 1,...) we conclude that:
there exist deterministic constructions to a Markov chain £ which rely on 
collections C of directed circuits endowed with certain measures W = 
(wC,ce C). The pairs (C, W) completely determine the process £.
But the same conclusion can be conversely viewed as:
there are Markov chains £ which are defined by two distinct entities: 
a topological entity given by a collection C of directed circuits, and an 
algebraic entity given by a measure W =(wC,c e C ).
Plainly, both topological and algebraic components C and W are not 
uniquely determined, and this is motivated by the algebraic nature of 
our construction. To assure the uniqueness, we should look for an­
other approach which can express the definite characteristic of the finite­
dimensional distributions of £.
A natural way to obtain a uniqueness criterion for (C, W) can be given 
by a behavioral approach. It is this approach that we shall further use.

Preface xi
Let S be a finite set, and let £ = (£n)n>0 be a homogeneous and irre­
ducible S-state Markov chain whose transition matrix is P = (pij, i,j G S). 
Denote by n = (ni, i G S) the invariant probability distribution of P, that is, 
ni > 0 , E i ni = 1, and
n^j=i = 22 nj pji, 
iG S. 
(2)
jj
It turns out that system (2) of the “balance equations” can be equiva­
lently written as follows:
nipij 
22 wJc(i,j), 
i,j G S, 
(3)
c
where c ranges over a collection C of directed cycles (or circuits) in S, wc 
are positive numbers, and Jc(i, j) = 1 or 0 according to whether or not 
(i, j )isanedgeofc.
The equivalence of the systems (2) and (3) pressupposes the existence of 
an invertible transform of the “global coordinates” expressed by the cycle­
weights wc,cG C, into the “local coordinates” given by the edge-weights 
nipij,i,j G S. That is, geometry (topology) enters into equations (2) and 
(3). The inverse transform of the edge-coordinates nipij,i,j G S, into the 
cycles ones wc ,c G C, is given by equations of the form
wc = (ni 1 pi 1 i 2 ) • • • (nis- 1 Pis- 1 is )(nis Pisi 1 ) ^, 
(4)
where c = (i 1, i 2,..., is, i 1), s > 1, with il = ik ,l,k = 1,..., s, l = k, and ^ 
is a function depending on i 1,... ,is,P, and n. The wc’s have frequently 
physical counterparts in what are called “through-variables.”
To conclude, any irreducible (in general, recurrent) Markov chain £ ad­
mits two equivalent definitions. A first definition is given in terms of a 
stochastic matrix P = (pij ) which in turn provides the edge-coordinates 
(nipij), and a second definition is given in terms of the cycle-coordinates 
(wc , c G C ).
To see how the edges and cycles describe the random law, we shall ex­
amine the definitions of the nipij and wc in the context of Kolmogorov’s 
theory of complexities as exposed in his last work with V.A. Uspensky “Al­
gorithms and Randomness” (see also A.N. Kolmogorov (1963, 1968, 1969, 
1983a, b) and V.A. Uspensky and A.L. Semenov (1993)).
Kolmogorov defined the entropy of a binary sequence using the concept of 
complexity as follows: Given a mode (method) M of description, the com­
plexity KM (yn) of any finite string yn = (a0, a 1,..., an- 1), n > 1, under 
the mode M is defined to be the minimal (space) length of a description of 
yn in this mode (since there can be several descriptions with respect to M). 
(We have to note here that there are two types of lengths: the time length 
and the space length; see A.N. Uspensky and A.L. Semenov (1993), pp. 
52-53). Then, considering the class M of all computable modes of descrip­
tion of a set Y of objects (to which yn belongs), Kolmogorov proved that 

xii Preface
there is an optimal mode O of description, not necessarily unique, which 
provides the shortest possible descriptions, that is, KO (yn) < KM (yn) + 
constant, for all M G M. The complexity KO (yn) is called the entropy of 
yn.
Now, turning back to our question of how the edges and cycles provide 
descriptions of the probability distribution Prob(£k = i, £k+1 = j), i,j G S, 
we shall examine the binary sequences assigned to this distribution. To this 
end let us fix a pair (i, j ) of states. Then for any k the probability
Prob( £k = i,£k +1 = j)
= lim —card {m < n — 1: £m(w) = i,£m + 1(w)= j} a.s. 
(5)
n—>^> n
can be assigned to an infinite binary sequence y(i,j) = y(i,j)(w) = 
(y(0),y(1),...,y(m),...) whose coordinates are defined as
1, if the directed pair (i, j ) occurs on w at the time m;
y m = 0, otherwise;
where w is suitably chosen from the convergence set of (5). A directed pair 
(i, j) occurs on trajectory w at moment m if £m- 1(w) = i and £m (w) = j.
On the other hand, it turns out that the recurrent behavior of £ de­
termines the appearances of directed circuits c = (i 1, i2,..., is, i 1), s > 1, 
with distinct points i1,i2,...,is when s>1, along the sample paths, whose 
weights wc are given by
wc = lim — card{m < n — 1: the cycle c appears, modulo the cyclic 
n '^ n permutations, along w}, 
I
almost surely, where m counts the appearances of the cycle c.
Equations (5) and (7) are connected by the following relation:
- card{m < n — 1: £:(w) = i,£m+1(w) = j} 
n
= E 1 wc,n (w) Jc (i,j) + 
, 
<
c
(7)
(8)
where c ranges over the set Cn (w) containing all the directed cycles occur­
ring until n along w, wc,n (w) denotes the number of the appearances of c 
up to n along w, and en (w) = 0 or 1 according to whether or not the last 
step from i to j corresponds or does not correspond to an edge of a circuit 
appearing up to n. Then, we may assign the y(i,j) (w) above to another de­
scription, say, (0, 0, 0, 0, 1, 0, 0, 1, 0, . . . ), where 1 codifies the appearances 
of a circuit passing through (i,j) along (£k (w))k at certain moments.
Now we shall appeal to Kolmogorov’s theory of complexities which, 
as we have already seen, uses an ob ject-description relation. Accord­
ingly, the object to be considered here will be the binary sequence yn = 
(y(0),y(1),...,y(n — 1)) whose coordinates are defined by (6), while the 

Preface xiii
corresponding descriptions will be expressed in terms of two modes of de­
scription as follows.
One mode of description for the yn will use the edges and will be de­
noted by E. The corresponding description in the mode E for each finite 
string yn =(y(0),y(1),...,y(n - 1)) is given by the binary sequence x = 
(x(0),x(1),...,x(n - 1)) whose coordinates are defined as
x(m)=y(m), 
m=0, 1,...,n- 1.
The second mode of description, denoted by C, is based on the directed 
cycles, and the corresponding description of yn above in the mode C is 
given by the sequence z =(z(0),...,z(n - 1)) where
z(m) =
1, 
0,
if a cycle passing through (i,j) occurs along w at moment m; 
otherwise;
for all m =0, 1,...,n- 1.
Nevertheless, it seems that another mode of description would be given 
by the k-cells, k = 0, 1, 2, were we to extend the graph of £ to the next 
higher topological structure which is the corresponding 2-complex. But in 
this case a serious drawback would arise: the descriptions in terms of the 
k -cells would comprise surface elements (the 2-cells) so that no reason­
able algorithmic device would be considered. This motivates the choice of 
the mode C of description in preference to that provided by the k -cells, 
k =0, 1, 2, and in this direction we find another two strengthening argu­
ments. First, the replacement of the 2-cells by their bounding circuits leaves 
invariant the orthogonality equation of the boundary operators which act 
on the k -cells, k =0, 1, 2; that is, the boundary operators connecting the 
homology sequence circuits-edges-points will still satisfy the orthogonal­
ity equation. Then the use of the 2-cells instead of the circuits becomes 
superfluous.
Second, the circuit-weights wc given by (7) enjoy a probabilistic inter­
pretation: wc is the mean number of occurrences of c along almost all the 
sample paths of £. Furthermore, the circuits (cycles) used in mode C can 
be determined by suitable equations called cycle generating equations.
To conclude, the cycles and edges provide two methods of description 
connected by equation (8). Under this light, cycle representation theory of 
Markov processes is devoted to the study of the interconnections between 
the edge-coordinates and cycle-coordinates along with the corresponding 
implications for the study of the stochastic properties of the processes. Only 
after the definition of the cycle representations for continuous parameter 
Markov processes can the idea of separating the geometric (topological) 
ingredients from their algebraic envelope become clear and lead to the in­
vestigations of fine stochastic properties such as Levy’s theorem concerning 
the positiveness of the transition probabilities.

xiv Preface
A systematic development of the fundamentals of the cycle theory, in the 
spirit of Kolmogorov’s algorithmic approach to chaos, started in the 1980s 
at Thessaloniki from the idea of interconnecting the principles of algebraic 
topology (network theory), algebra, convex analysis, theory of algorithms, 
and stochastic processes. For instance, the resulting cycle-decomposition- 
formula provides the homological dimension of Betti, the algebraic dimen­
sion of Caratheodory, and the rotational dimension as new revelations of 
the Markov processes.
Another school, which developed independently the cycle representa­
tions, is that of Qians in Peking (Qian Gong, Qian Minping, Qian Min, Qian 
Cheng, Gong Guang, Guang-Lu Gong, and others). The Chinese school, us­
ing mainly a behavioral approach, defined and explored with exceptional 
completeness the probabilistic analogues of certain basic concepts which 
rule nonequilibrium statistical physics such as Hill’s cycle flux, Schnaken- 
berg’s entropy production, the detailed balance, etc. For instance, conceived 
as a function on cycles, the entropy production can be regarded as a mea­
sure for characterizing how far a process is from being reversible.
In France, Y. Derriennic advanced the cycle representation theory to the 
study of ergodic problems on random walks in random environment.
Finally, a fourth trend to cycle theory is based on the idea of Joel E. 
Cohen under the completion of S. Alpern, and this author, for defining a 
finite recurrent stochastic matrix by a rotation of the circle and a partition 
whose elements consist of finite unions of the circle-arcs. Recent works of the 
author have given rise to a theoretical basis, argued by algebraic topology, 
for developing the rotational idea into an independent setting called the 
rotational theory of Markov processes. This monograph exposes the results 
of all the authors who contributed to this theory, in a separate chapter.
The present book is a state-of-the-art survey of all these principal trends 
to cycle theory, unified in a systematic and updated, but not closed, ex­
position. The contents are divided into two parts. The first, called “Fun­
damentals of the Cycle Representations of Markov Processes,” deals with 
the basic concepts and equations of the cycle representations. The second 
part, called “Applications of the Cycle Representations,” is the application 
of the theory to the study of the stochastic properties of Markov processes.
Sophia L. Kalpazidou

Acknowledgments
I find a suitable place here to thank those who sincerely encouraged the 
research on the cycle representations.
I am very grateful to Professors David G. Kendall and G.E.H. Reuter, 
from Cambridge, who enthusiastically supported the original impetus to 
“cycle theory.”
For their persevering and effective collaboration I have to express 
my deep gratitude to Professors Y. Derriennic (Brest), M. Iosifescu 
(Bucharest), S. Negrepontis (Athens), P. Ney (Wisconsin), G. Papanicolaou 
(New York), Qian Minping (Peking), W. Schaal (Marburg), D. Surgailis 
(Vilnius), Chris P. Tsokos (Florida), N.Th. Varopoulos (Paris), and A.H. 
Zemanian (New York).
Kind thanks are due to Mrs. Eleni Karagounaki, Mrs. Fredericka 
Halinidou Kouliousi, Mr. Zacharias Koukounaris and Mr. George Paneris 
for invaluable technical help.
For their elaborate editing, I have also to thank Mrs. Francine McNeill 
and Mr. Brian Howe.
Finally, I am grateful to Dr. Martin Gilchrist and Miss Birgitt 
Tangermann for their patience and excellent management.

Contents
Preface to the Second Edition 
vii
Preface 
ix
Acknowledgments 
xv
I Fundamentals of the Cycle Representations
of Markov Processes
1 
Directed Circuits 
3
1.1 
Definition of Directed Circuits ..................................................... 
4
1.2 The Passage Functions....................................................................  
8
1.3 
Cycle Generating Equations .........................................................  
10
2 
Genesis of Markov Chains by Circuits: The Circuit
Cha ins 
17
2.1 Finite Markov Chains Defined by Weighted Circuits  
17
2.2 
Denumerable Markov Chains Generated by Circuits  
23
3 
Cycle Representations of Recurrent Denumerable
Mar kov Chains 
29
3.1 The Derived Chain of Qians .........................................................  
29
3.2 The Circulation Distribution of a Markov Chain................... 
35
3.3 
A Probabilistic Cycle Decomposition for Recurrent Markov 
Chains.........................................................................................  
37

xviii
Contents
3.4 
Weak Convergence of Sequences of Circuit Chains: A 
Deterministic Approach .......................................................  
39
3.5 
Weak Convergence of Sequences of Circuit Chains: A 
Probabilistic Approach ......................................................... 
45
3.6 The Induced Circuit Chain............................................................  
47
4 Circuit Representations of Finite Recurrent Markov
Chains 
55
4.1 
Circuit Representations by Probabilistic Algorithms...........  
56
4.2 
Circuit Representations by Nonrandomized Algorithms......  
57
4.3 The Caratheodory-Type Circuit Representations.................. 60
4.4 The Betti Number of a Markov Chain....................................... 61
4.5 
A Refined Cycle Decomposition of Finite Stochastic 
Matrices: A Homologic Approach ..................................... 66
4.6 The Dimensions of Caratheodory and Betti............................. 72
5 Continuous Parameter Circuit Processes with Finite
State Space 
73
5.1 
Genesis of Markov Processes by Weighted Circuits ............... 73
5.2 
The Weight Functions...................................................................... 76
5.3 
Continuity Properties of the Weight Functions ...................... 79
5.4 
Differentiability Properties of the Weight Functions ............ 83
5.5 
Cycle Representation Theorem for Transition Matrix 
Functions.................................................................................... 85
5.6 
Cycle Representation Theorem for Q-Matrices....................... 88
6 Spectral Theory of Circuit Processes 
93
6.1 
Unitary Dilations in Terms of Circuits ..................................... 93
6.2 
Integral Representations of the Circuit-Weights 
Decomposing Stochastic Matrices ..................................... 96
6.3 
Spectral Representation of Continuous Parameter Circuit 
Processes.................................................................................... 98
7 Higher-Order Circuit Processes 
101
7.1 
Higher-Order Markov Chains........................................................ 101
7.2 
Higher-Order Finite Markov Chains Defined by Weighted 
Circuits...................................................................................... 106
7.3 
The Rolling-Circuits......................................................................... 117
7.4 
The Passage-Function Associated with a Rolling-Circuit ... 
120
7.5 
Representation of Finite Multiple Markov Chains by 
Weighted Circuits ................................................................... 122
8 Cycloid Markov Processes 
131
8.1 
The Passages Through a Cycloid................................................. 131
8.2 
The Cycloid Decomposition of Balanced Functions..............  
135

Contents xix
8.3 
The Cycloid Transition Equations   
138
8.4 
Definition of Markov Chains by Cycloids  
141
9 
Markov Processes on Banach Spaces on Cycles 
145
9.1 
Banach Spaces on Cycles  
145
9.2 
Fourier Series on Directed Cycles   
152
9.3 
Orthogonal Cycle Transforms for Finite Stochastic 
Matrices ..................................................................................... 
157
9.4 
Denumerable Markov Chains on Banach Spaces on 
Cycles.........................................................................................  
161
10 The Cycle Measures 
163
10.1 The Passage-Functions as Characteristic Functions.......... 
163
10.2 The Passage-Functions as Balanced Functions..................  
167
10.3 The Vector Space Generated by the 
Passage-Functions............................................................  
171
10.4 The Cycle Measures....................................................................  
175
10.5 
Measures on the Product of Two Measurable Spaces 
by Cycle Representations of Balanced Functions: A 
Fubini-Type Theorem  
182
11 Wide-Ranging Interpretations of the Cycle
Representations of Markov Processes 
187
11.1 The Homologic Interpretation of the Cycle Processes ......  
187
11.2 
An Algebraic Interpretation ..................................................... 
192
11.3 The Banach Space Approach...................................................  
194
11.4 The Measure Theoretic Interpretation..................................  
195
11.5 The Cycle Representation Formula as a Disintegration of 
Measures.............................................................................  
197
II Applications of the Cycle Representations
1 
Stochastic Properties in Terms of Circuits 
207
1.1 
Recurrence Criterion in Terms of the Circuits...................  
207
1.2 
The Entropy Production of Markov Chains ........................ 
210
1.3 
Reversibility Criteria in Terms of the Circuits...................  
212
1.4 
Derriennic Recurrence Criterions in Terms of the 
Weighted Circuits............................................................  
215
2 
Levy’s Theorem Concerning Positiveness of Transition
Probabilities 
225
2.1 
Levy’s Theorem in Terms of Circuits.................................... 226
2.2 
Physical Interpretation of the Weighted Circuits 
Representing a Markov Process................................... 228

xx
Contents
3 
The Rotational Theory of Markov Processes 
231
3.1 
Preliminaries................................................................................... 
231
3.2 
Joel E. Cohen’s Conjecture on Rotational 
Representations of Stochastic Matrices....................  
234
3.3 
Alpern’s Solution to the Rotational Problem...................... 
235
3.4 
Transforming Circuits into Circle Arcs................................. 
240
3.5 
Mapping Stochastic Matrices into Partitions and a 
Probabilistic Solution to the Rotational Problem. 
247
3.6 
The Rotational Dimension of Stochastic Matrices and a 
Homologic Solution to the Rotational Problem..... 
250
3.7 
The Complexity of the Rotational Representations ......... 
255
3.8 
A Reversibility Criterion in Terms of Rotational 
Representations ................................................................  
259
3.9 
Rotational Representations of Transition Matrix 
Functions............................................................................ 
262
List of Notations 
265
Bibliography 
267
Index 
297

I
Fundamentals of the
Cycle Representations of
Markov Processes

1
Directed Circuits
A circuit or a cycle is a geometric (really topological) concept that can be 
defined either by geometric or by algebraic considerations.
The geometric approach views a circuit with distinct points as an image 
of a circle. Namely, such a circuit is a discretization of a Jordan curve (a 
homeomorph of a circle), that is, a Jordan curve made up by closed arcs, 
where by closed arcs we understand the closed 1-cells (in general a closed 
n-cell, n > 0, is the homeomorph of the Euclidean set x'x = x 1 + x2 + • • • + 
x n < 1).
A first step in dealing with algebra is to introduce orientation. This 
means distinguishing the two endpoints of each arc as an initial point and 
a terminal (final) point. When the arcs ofa circuit have the same orientation 
we call it a directed circuit.
A definite property of a directed circuit is a canonical return to its points, 
that is, a periodic conformation. This argues for a functional version of the 
definition of a directed circuit expressing periodicity. Namely, a circuit will 
be defined to be any periodic function on the set of integers.
The algebraic approach provides the definition of a directed circuit as a 
finite sequence of arc-indexed connected vectors satisfying again the defi­
nite property of having identical endpoints, that is, the boundary is zero. 
Equivalently, the same property can be expressed as a system of balance 
equations.
In the present chapter we introduce the concept of a directed circuit 
either as a periodic function, or implicitly by balance equations.

4 
1. Directed Circuits
1.1 Definition of Directed Circuits
Definition 1.1.1. A directed circuit-function in a denumerable set S is a 
periodic function c from the set Z of integers into S .
The values c(n),n G Z, are called either points or vertices, or nodes of 
c while the pairs (c(n), c(n + 1)), n G Z, are called either directed edges or 
directed branches, or directed arcs of c.
The smallest integer p = p(c) > 1 that satisfies the equation c(n + p) = 
c(n), for all n G Z, is called the period of c. A directed circuit-function c 
with p(c) = 1 is called a loop.
With each directed circuit-function c we can associate a whole class of 
directed circuit-functions c obtained from c by using the group of transla­
tions on Z. Specifically, if for any fixed i G Z we put ti(n) = n + i,n G Z, 
then we define a new directed circuit-function c as c = c ◦ ti, that is, 
c (n) = c (n + i), n G Z.
Clearly c and c do not differ essentially (they have the same vertices) 
and this suggests the following definition:
two directed circuit-functions c and c are called equivalent if and
only if there is some i G Z such that c = c ◦ t^. 
(1.1.1)
Note that (1.1.1) defines an equivalence relation in the class of all directed 
circuit-functions in S. It is obvious that for any equivalence class {c ◦ ti,i G 
Z } the direction and the period are class features, that is, c ◦ ti and c ◦ tj 
have the same period and direction as c for any i, j G Z . This remark leads 
to the following definition introduced by S. Kalpazidou (1988a).
Definition 1.1.2. A directed circuit in a denumerable set S is an equiva­
lence class according to the equivalence relation defined in (1.1.1).
According to the previous definition the nonterminal points of a directed 
circuit are not necessarily distinct. The definite property of a circuit c 
consists of a canonical return of all its points after the same number of 
steps, and this does not exclude repetitions. This is particularly argued by 
the existence of functions depending on circuits whose properties do not 
require distinct points (see, for instance, Theorems 1.3.1 and 2.1.2 below). 
Correspondingly, in the latter expositions of the present book we shall point 
out cases where only circuits with distinct points are used.
A directed circuit c in the above sense is determined either by:
(i) the period p = p(c); and
(ii) any (p + 1)-tuple (i1,i2,...,ip,ip+1), with ip+1 = i1;
or by
(i') the period p = p(c); and
(ii') any p ordered pairs (i 1 ,i 2), (i 2 ,i 3),..., (ip- 1, ip), (ip,ip +1), with 
ip +1 = i 1, where il = c(n + l — 1), 1 < l < p, for some n G Z.

1.1 Definition of Directed Circuits 5
Definition 1.1.3. The directed cycle associated with a given directed cir­
cuit c = (i 1, i2,..., ip, i 1),p > 1, with the distinct points i 1,..., ip (when 
p > 1) is the ordered sequence c = (i 1,..., ip).
According to Definition 1.1.2 a (class-) cycle is invariant with respect to 
any cyclic permutation of its points.
Definition 1.1.4. The reverse c- of a circuit c =(i1,i2,...,ip,i1),p > 1, 
is the circuit c- =(i1,ip,...,i2,i1).
Let us look more closely at the invariance property of a circuit with 
respect to translations on Z . The latter correspond manifestly to a geo­
metrical image of rotations as follows.
If we identify the elements of a circuit c =(i1,...,ip,i1),p > 1, as dis­
tinct points in a plane, then we obtain a directed closed curve Ac which, 
according to the Jordan curve theorem, separates the plane into an inte­
rior and exterior region (the interior one is a 2-cell). (For details see the 
1904 Chicago thesis of Oswald Veblen.) Let us choose an arbitrary point 
0 inside the interior region bounded by Ac, and connect 0 to i 1,... ,ip by 
segments. Then the system {0, Ac} is homeomorphic with a circle such that 
each directed edge (ik, ik+1) corresponds to a rotation around 0. Therefore,
any directed circuit c of period p>1, and with distinct p points, provides 
a collection of p rotations summing to 2n.
Let us enrich this geometrical view by a group-theoretic argument. 
Namely, we view {a, 2a,... ,pa} with a = 2n/p as a collection of rotations 
that can be mapped by an isomorphic mapping onto the cyclic group of the 
pth roots of unity (see A.G. Kurosh (1960), p. 46). On the other hand, if 
we partition the collection of all circuits in S into equivalence classes each 
consisting of those circuits with the same period, we find that such a class 
of circuits can be associated to a cyclic group of roots of unity.
Clearly we cannot define a directed circuit as a cyclic group since the 
first one requires two definite elements: the period and the vertices (corre­
sponding to a unique radius), while the second one is only determined by 
the period. Figure 1.1.1 represents a p-order group of rotations generated 
by the angle a = 2n/p. Therefore,
a directed circuit of period p is assigned to a p-order cyclic group of 
rotations.
Next it would be interesting to see if a rotation can be used to define 
N (>1) overlapping circuits in a set of n points. An answer is inspired by 
the 1983 paper of S. Alpern. Namely, we first introduce the ingredients of 
a rotation as follows: let n > 1 and let N < n2 — n +1. Then the rotation 
of the circle that is to be considered is generated by the angle 2n/(NM), 
where M = n!.

6
1. Directed Circuits
Figure 1.1.1.
Divide the circumference of the circle in Figure 1.1.2 into NM equal 
directed arcs symbolized by the elements of the following matrix:
a11 
a12 . . . a1 M
A = a 2 1 
a 2 2 . . . a 2 M
...
aN 1 aN 2 
... aNM
For each row k of A, associate the initial points of the directed arcs 
ak1,ak2,...,aks,2s<M, with certain distinct points i1,i2,...,is of the 
set S = {1, 2,...,n}, and then associate the initial points of ak,s+1, 
ak,s+2,..., ak,2s with the same points i1,i2,...,is.Ifs is chosen to divide
Figure 1.1.2.

1.1 Definition of Directed Circuits 7
M , we may continue the previous procedure M/s times, so that the ini­
tial point of akM will be is . In this way, the kth row of A is assigned to 
the M/s copies of the directed cycle ck = (i 1,... ,is) corresponding to the 
circuit ck =(i1,...,is,i1) (in Figure 1.1.2 this is shown for k = 1). Particu­
larly, the circuits c1 ,...,cN can be chosen to have certain common points. 
Then we conclude that a rotation of angle (2n)/(Nn!) generates a collection 
of N directed circuits in S.
Example 1.1.1. Let n =3,M = 3! = 6, and N = 3. Associate the initial 
points of a11,...,a16 with 1, the initial points of a21,a22,...,a26 with 1, 2, 
1, 2, . . . , 1, 2, and those of a31,a32,...,a36 with 1, 2, 3, . . . , 1, 2, 3. Then 
the rotation defined by the angle 2n/18 is attached to the collection of the 
following three overlapping circuits: (1, 1), (1, 2, 1), and (1, 2, 3, 1).
So far our arguments are geometric and the proposed construction is not 
the simplest one. However, it is a natural link to the main object of the 
present book: the stochastic matrices.
In Section 3.4 of Part II we shall show that the converse of the previ­
ous relation arcs ^ circuits is vastly superior to other similar relations, 
especially if we are interested in the definition of a partition of the circle 
which can in turn be involved in the definition of a stochastic matrix. It 
is therefore a matter of some considerable theoretical and practical impor­
tance to obtain in a usable form a necessary and sufficient condition for a 
relation circuits ^ arcs to be used in the definition of a stochastic matrix 
(see Kalpazidou (1994a, 1995)). As a first step in this direction we have to 
find a special partition {Akl} of the circle such that the starting point of 
each Akl is suitably labeled by some point i, i =1,...,n.
Further we have to involve an algebraic argument according to which we 
shall assign the partition {Ak/} to a positive row vector (wk, k = 1,..., N), 
with wk = 1, and to consider suitable homeomorphs Ak/ of Ak/ in a line 
of unit length such that the Lebesgue measure of each Ak/ is given by 
(1/n!)wk . Then the sets
Si = 
Ak/ , i= 1, . . . ,n,
the arcAkl 
starts at i
will partition the interval [0, 1) (see Section 3.4 of Part II).
Let A denote Lebesgue measure and let ft,t = 1 /n!, be the A -preserving 
transformation of [0, 1) onto itself defined by ft(x) = (x + t) (mod 1). Then 
the expression
A (Si n f- 1( Sj))/A (Si), 
i,j = 1 ,...,n, 
(1.1.2)
defines a stochastic matrix on {1, 2,...,n}. Notice that stochastic matrices 
of the form (1.1.2) can be defined by any partition (Si,i=1, 2,...,n)of 
[0, 1).

8 
1. Directed Circuits
The converse is more difficult and was proposed by Joel E. Cohen (1981) 
as a conjecture that we shall call the rotational problem. Under the re­
cent completions of S. Alpern (1983) and S. Kalpazidou (1994a, 1995), the 
rotational problem can be formulated as follows:
Given n>1 and any stochastic matrix (pij ,i,j =1,...,n) that admits 
an invariant probability distribution, find a rotational system consisting 
of a X-preserving transformation ft,t > 0, on [0, 1) and a partition of 
[0, 1) into sets Si,i =1,...,n, each consisting of a finite union of arcs, 
such that
Pij = X(Si n f- 1(Sj))/x(Si).
The solutions to the rotational problem along with a detailed presenta­
tion of the corresponding theoretical basis are given in Chapter 3 of Part II. 
As already seen, an intrinsic step to the above rotational problem consists 
of defining a stochastic matrix in terms of the directed circuits.
1.2 The Passage Functions
Given a denumerable set S and a directed circuit c in S, we are interested in 
expressing the passages of a particle through the points of c. The simplest 
way is to use the indicator function of the “event”: a point k G S of the 
particle’s trajectory lies on c. Notice that, according to Definition 1.1.2 the 
subsequent definitions and properties should not be affected by the choice 
of the representative of a class-circuit.
Now we introduce the following definition due to J. MacQueen (1981) 
and S. Kalpazidou (1988a):
Definition 1.2.1. Assuming c to be determined by (i1,...,ip(c),i1), define 
Jc (k) as the number of all integers l, 0 < l < p(c) — 1, such that il+1 = k. 
We say that c passes through k if and only if Jc(k) = 0 and then Jc(k) is 
the number of times k is passed by c.
Clearly
Jc ◦ tj ( k ) = Jc ( k ) ,
(1.2.1)
for any j G Z . When all the points of c are distinct, except for the terminals, 
then
Jc(k) = 
01,,
if k is a point of c; 
otherwise.
If we consider r>1 consecutive points k1 ,...,kr G S on a particle’s tra­
jectory, then to express the passage of the circuit c through the r-tuple 
(k1 ,...,kr ) we need the following generalization of Definition 1.2.1:

1.2 The Passage Functions 9
Definition 1.2.2. Assuming c is a directed circuit of period p(c), define 
Jc (k 1,... ,kr) as the number of distinct integers l, 0 < l < p (c) — 1, such 
that c ◦ tl(m) = km,m =1, 2,...,r.
We say that c passes through (k1,...,kr) if and only if Jc(k1,...,kr) =0 
and then Jc(k1,...,kr) is the number of times c passes through (k1,...,kr).
The functions Jc : Sr ^ N,r > 1, are called the rth order passage func­
tions associated with c.
Note that we can have Jc(k1,...,kr) = 0 even if r > p(c). For example, 
if c =(1, 2, 1), then Jc(1, 2, 1, 2) = 1. Obviously
Jc ◦ tj ( k 1 ,...,kr ) = Jc ( k 1 ,...,kr ),
for all j G Z. In what follows for any r-tuple k = (k 1,... ,kr) G Sr we shall 
use the notation (k,i) and (l, k) for the (r + 1)-tuples (k1,...,kr,i) and 
(l, k 1,..., kr), respectively. Also, k_ will denote the r-tuple (kr,..., k 1).
We now give a few simple but basic properties of the passage function Jc .
Lemma 1.2.3. The passage function Jc satisfies the following balance 
properties:
( P 1) 
Jc ( k ) = E Jc ( k,i ) = E Jc (l,k ) ,
( P2) 
Jc ( k ) = Jc_ ( ) ,
for an arbitrarily given r > 1 and for any k = (k 1, .. . ,kr) G Sr, where c- 
symbolizes as always the reverse of c.
Proof. We start by proving (P2 ). This follows from the fact that, by the 
very definitions of c_ and k_, the c and c_ do or do not simultaneously pass 
through the r-tuples k and k_, respectively. Next, for proving (P 1) note first 
that c does not pass through k if and only if c does not pass through (k, i) 
(respectively, (l, k)) for any i (respectively, l) G S. Consequently, in this 
case
Jc (k) = e Jc (k, i) = E Jc (l, k) = 0.
Second, if c passes through k, then looking at the point of c immediately 
succeeding (respectively, preceding) k we conclude that Jc(k), the number 
of times c passes through k, equals the sum over all i (respectively, l) G S 
of the number of times c passes through (k, i) (respectively, (l, k)). Thus 
the proof of (P 1) is complete. 
□
For a fixed r > 1, the balance property (P1 ) asserts that the r-tuple 
(k1,...,kr) lies on c,thatis,(k1,...,kr)=(c(n),...,c(n + r — 1)) for some 
n G Z, (an equilibrium status) if and only if c passes through (k1,...,kr)to 
(from) an element i (l)ofc,thatis,(k1,...,kr,i)=(c(n),...,c(n + r — 1, 

10 
1. Directed Circuits
c(n + r)) ((l, k1,...,kr)=(c(n - 1),c(n),...,c(n + r - 1))) (a dynamical 
status).
1.3 Cycle Generating Equations
Let S be a finite set and consider a collection C of overlapping circuits in 
S. Then the passage-functions occurring in the balance equations (0 1) of 
Lemma 1.2.3 depend upon the circuits.
In general, both practice and theory provide balance equations where the 
passage function Jc is replaced by an arbitrary positive function w defined 
on S2 , that is,
g w (k,i )= g w (j,k) 
(1.3.1)
for all k G S.
In this section we propose to answer the following inverse question:
Do equations (1.3.1) provide directed circuits that describe the 
balance function w? 
(1.3.2)
We shall follow the usual argument according to which properties in terms 
of the indicator functions are generalized to linear combinations of the 
indicator functions. Consequently, we ask:
Can any balance function w(i,j) (i.e., that satisfying (1.3.1)) be 
expressed as a linear positive combination of the passage functions 
associated with certain circuits c, that is, 
(1.3.3)
w(i, j)= wcJc(i,j), i,jG S, wc > 0? 
(1.3.4)
The following theorem answers both questions (1.3.2) and (1.3.3) in the 
affirmative (S. Kalpazidou (1988a)):
Theorem 1.3.1. Let S be a nonvoid finite set and let two nonnegative 
functions w and w_ be defined on S x S. Assume w and w_ satisfy the 
balance equations
w(k, i)= w(i, k), kG S,
£w(k,i) = £w(i,k), 
k g s,
such that each sum of (1.3.5) is strictly positive, and
(1.3.5)
w(k, i) = w(i, k),
for all i, k G S.

1.3 Cycle Generating Equations
11
(1.3.6)
Then there exist two finite ordered collections C and C_ of directed circuits 
in S, with C_ = {c_, c_ is the reversed circuit of c,c G C}, and two ordered 
sets {wc, c G C} and {wc_, c- G C-} of strictly positive numbers, depending 
on the ordering of C and C_ and with wc = wc_, such that
w(k, i)= 
wcJc(k, i),
w_(i,k) 
wc_ Jc_(i, k)
for all k,i G S, where Jc(i,j)(Jc_(j, i)) is 1 or 0 according to whether or 
not (i,j)((j, i)) is an edge of c (c_).
Proof. Starting from an arbitrarily fixed point k G S, on account of the 
strict positiveness of the sums in (1.3.5), there exists at least one element 
j G S such that w(k, j) = w_(j, k) > 0. Let i 1 = k, i2 = j. Repeating the 
same argument for i2 instead of k, there exists i3 G S such that w(i2 ,i3 ) > 
0. Finally the balance equations (1.3.5) provide a sequence of pairs 
(i 1 ,i 2), (i 2 ,i 3),... for which w (ik ,ik+1) and w_ (ik+1 ,ik) are strictly pos­
itive. Since S is finite, there is a smallest integer n > 2 such that in = ik for 
some k, 1 < k < n. Then the sequence (ik, ik +1), (ik+1, ik+2),..., (in- 1, ik) 
determines a directed circuit c1 with distinct points (except for the termi­
nals). Let
wc1 = min w(i, j) = w(i1, j1)
where the minimum is taken over the edges of c1 .
Consider
w 1(i, j) = w(i, j) - wci Jci (i,j),
where Jc (i, j ) is 1 or 0 according to whether or not (i, j )isanedgeofc.
By the very definition of wc 1, the new function w 1(•, •) is nonnegative. 
Also, since Jc 1 is balanced, w 1 is also. If w 1 = 0, equations (1.3.6) hold for 
C = {c1 }. Otherwise, w1 remains strictly positive on fewer pairs than w 
and we may repeat the same arguments above for w1 instead of w to define 
a new directed circuit c2 with distinct points (except for the terminals). 
Accordingly, we further define
wc2 = minw1(i,j) = w1(i2,j2) = w(i2,j2) - wc1 Jc1 (i2, j2) 
c2
and
w2(i, j) = w1(i, j) - wc2Jc2(i, j)
= w(i, j) - wc1Jc1(i, j) - wc2 Jc2 (i, j).
Continuing the procedure, we find a sequence w1 ,w2 ,... of balanced 
functions such that each wk+1 remains strictly positive on fewer pairs 
than wk . Then after finitely many steps, say n, we have wn+1 = 0. Put

12
1. Directed Circuits
Figure 1.3.1.
C =(c1,c2,...,cn}.Then
w(i, j)= 
wcJc(i, j),
for all i, j G S. By arguing analogously for w_ and choosing as representative 
class of circuits to be C_ = {^ : c_ is the reversed circuit of c, c G C}, we 
obtain wc_ = wc for any c_ G C-, and the decomposition of w_ by (C_, wc_). 
The proof is complete. 
□
Equations (1.3.5) are called, by Kalpazidou ((1993a), (1994a)), cycle gen­
erating equations. They can be used as an implicit definition of the directed 
circuits. Accordingly, we shall say that the functions w and w_ are respec­
tively represented by (C wc) and (C_, wc_). It is useful to notice that, in 
defining the decomposing weights, the algorithm occurring in the course of 
the proof of Theorem 1.3.1 depends upon the choice of the ordering of the 
representative circuits.
Example 1.3.1. Let S = {1, 2} and let w(i, j), i, j G S, be defined by the 
entries of the matrix
1/13
3/13
3/13
6/13
According to the Theorem 1.3.1, w is decomposed by the follow­
ing circuits and weights: c1 =(1, 1), c2 =(1, 2, 1), c3 =(2, 2), and wc1 = 
1/13, wc2 = 3/13, wc3 = 6/13 (see Figure 1.3.1).
Example 1.3.2. Let S = {1, 2, 3, 4} and let w(i, j), i, j G S, be given by 
the matrix
(1, 1), c2 =(1, 2, 3, 4, 1), c3 =(1, 3, 4, 1), and c4 =(1, 4, 1), and by the 
weights wc1 = 3/12, wc2 = 1/12,wc3 = 1/12, and wc4 = 1/12 (Figure 
1.3.2).
3/12 1/12 1/12 1/12
001/12 
0
0002/12
.
3/12000
Then the balance function w is decomposed by the circuits c1 =

1.3 Cycle Generating Equations
13
3/12
Figure 1.3.2.
Theorem 1.3.1 asserts that the balance equations (1.3.1) are equivalent 
to the explicit representations (1.3.4). Both systems have topological equiv­
alents in equations (ft 1) of Lemma 1.2.3, where we recognize two types of 
connections:
(i) the connections of the directed edges b 1 ,b 2,... ,b\ 1 with the points 
n 1,n2,..., n\0; and
(ii) the connections of the directed edges b 1 ,b 2,... ,b\ 1 with the directed 
circuits c 1, c 2,... ,c\ 2 (here we consider that the circuits have distinct 
points (excepts for the terminals)).
Namely, the connectivity (which is a topological property) of the directed 
edges and points in the graph of w may be expressed by a matrix operator 
n defined as follows:
n = (nedge, point) = (nbjns )
(1.3.7)
with
nbjns
+1, if the jth edge is positively incident on the sth point;
-1, if the jth edge is negatively incident on the sth point;
0, otherwise.
Notice that the columns of n are linearly dependent. When we do not need 
this linear dependence we can choose a reference point of the graph of w, 
and then delete the corresponding column in the matrix n.
The interconnections between edges and circuits in the graph of w can 
be described by another matrix operator Z defined as follows:
Z 
( Zedge, circuit) 
( Zbj cK ),
(1.3.8)
where
Zbjck
+1, if the jth edge is positively included in the kth circuit;
-1, if the jth edge is negatively included in the kth circuit;
0, otherwise.
Since the Jc(k, i) plays the role of the (k, i)-coordinate of the circuit c 
viewed in the vector space generated by the edges {bj}, equation (0 1) of

14 
1. Directed Circuits
Lemma 1.2.3 is related to the equation
n t Z = 0,
where nt is the transposed matrix of n• An additional argument to the 
previous equation is given in Chapter 4.
The cycle generating equations (1.3.1) and (1.3.4) are in fact an al- 
gebraization of (0 1) occurring in Lemma 1.2.3, since the edges and cir­
cuits are respectively assigned with the edge values w(k, i) and the circuit 
values wc . Correspondingly, the cycle generating equations have a double 
solution:
(i) a topological solution specified by the representative class C of di­
rected circuits in the graph of w; and
(ii) an algebraic solution {wc, c G C} of strictly positive circuit-weights.
It is to be noticed that the only operations involving the edge values 
and circuit values in Theorem 1.3.1 are addition and subtraction. This 
explains why these elements may belong to any additive group. The reason 
for considering vector spaces instead of groups arises when a co-theory is 
intended to be developed on a dual graph, where a transform connects the 
edge values of the original graph to those of the dual graph (in particular, 
this transform can be linear (ohmic)). (Dual graphs were introduced by 
R.J. Duffin (1962).) As a consequence, certain associative and distributive 
laws have to be obeyed.
Remarks
(i) The circuit decomposition (1.3.6) of Theorem 1.3.1 implies that all 
the points of S lie on circuits with strictly positive weights. This amounts 
to the existence of circuits (in the graph of w) which pass through each 
point, that is, w(i) = jwj. w(i,j) can be written as w(i) 
c wcJc(i) with
Jc(i) = 0 for some c.
It is the positiveness of the sums occurring in the balance equations 
(1.3.5) that, along with the latter, argues for the concept of circuit gener­
ating equations. For instance, if w = 0 then w can be written as w = 0 • Jc 
for any circuit c in S, in which case the balance equations do not play the 
role of circuit generating equations. In other words, the balance equations 
without the positiveness assumption lose their topological role (specified by 
the connectivity of the graph of w) and keep only the algebraic one. This 
explains why, when we do not assume the positiveness condition in the 
balance equation (1.3.5), the decomposition (1.3.6) may contain, among its 
terms, null circuit-weights. (For instance, when w(i) above is zero then w(i) 
can be written as w(i) = wcJc(i) for any wc > 0 and any circuit c which 
does not contain i. Another version would be to choose any circuit c and 
wc =0.)
(ii) The circuit decompositions (1.3.6) allow interpretations in probabilis­
tic terms (see Theorem 3.3.1 below), and in physical terms (for instance,

1.3 Cycle Generating Equations
15
every point is neither a source nor a sink of an eletrical fluid—in Chapter 
2 of Part II we shall give a sequel to this argument).
(iii) The circuits occurring in the decomposition (1.3.4) have not 
necessarily distinct points. In this light, there exist particular solutions (to 
the balance equations) given by those classes which contain only circuits 
with distinct points (except for the terminals). The latter can be also 
obtained by a convex analysis argument which relies on a version of the 
celebrated Caratheodory dimensional theorem. A detailed exposition of 
this argument is developed in Chapter 4.
For particular cases when w is provided by a doubly stochastic matrix 
P another convex analysis argument for a cycle decomposition is given by 
the Birkhoff theorem according to which P can be written as a convex 
combination of permutation matrices. A permutation matrix is any matrix 
whose entries are either 0 or 1, and with only one 1 for each row and for 
each column. Here is a proof due to Y. Derriennic.
Consider a doubly stochastic matrix P and write it as a convex combina­
tion 52 aiMi, where Mi are permutation matrices. Since each permutation 
determines circuits, we can assign the weight 1 to each circuit. Then we 
obtain a collection of circuits and weights associated with Mi .
On the other hand, since all the matrices Mi admit a common invariant 
measure (the “uniform” measure), then P, as a convex combination of Mi, 
will be decomposed by the corresponding weighted circuits.
(iv) The consideration of the two functions w and w_ in the statement 
of Theorem 1.3.1 is motivated by our further developments. Namely, in 
Chapters 2 and 7 we shall show that w and w_ may enjoy a probabilistic 
interpretation according to which they will define the transition laws of two 
distinct Markov chains, with reversed parameter-scale, such that one chain 
is not the inverse of the other.
Also equations w(k, i) = w_(i, k) give a sufficient condition in order that 
w and w_ admit inverse representative circuits. In general, when we disso­
ciate the function w from this context, Theorem 1.3.1 may refer to a single 
balance function.
(v) 
As will be shown in Chapter 4, a general circuit decomposition for­
mula, with real circuit-weights, can be proved by an algebraic topological 
argument according to which the balanced function w, considered in the 
vector space generated by the edges of the graph of w, may be decomposed 
into a sum by a minimal number of circuits.
The balanced function w is known in the literature under different names: 
in convex analysis w is called a flow (see C. Berge (1970), R.T. Rockafellar 
(1972), and M. Gondran and M. Minoux (1984)), in algebraic topology w 
is called a one-cycle of the corresponding graph (see S. Lefschetz (1975), 
pp. 51-52, B. Bollobas (1979)), while in network theory w is a current 
obeying the first Kirchhoff law (see A.H. Zemanian (1991)).
If S has m points, then the matrix n associated with the graph of w by 
(1.3.7) has m - 1 independent columns. The remaining column corresponds 

16
1. Directed Circuits
to a reference point of the graph of w called the datum. Let us choose a 
maximal tree T in the graph of w, and then let us arrange properly the rows 
of n so that n is partitioned into a submatrix nT, whose edges belong to the 
tree only, and a submatrix nL, whose edges belong to the tree-complement. 
Note that each of the nondatum points of the tree can be connected by 
a unique path-in-tree to the datum point. F.H. Branin, Jr. (1959, 1966) 
introduced another connectivity matrix PT expressing the connections of 
the edges of T with the point-to-datum paths. Specifically, PT is defined as 
follows:
PT = (Pedge, point )=(Pbj ns ),
where
+1,
Pbj ns = 
-1,
. 0 ,
if the jth edge is positively included in the 
sth point-to-datum path;
if the jth edge is negatively included in the 
sth point-to-datum path;
otherwise.
Then PT is an invertible matrix, that, according to F.H. Branin, Jr. (1959), 
satisfies the equation nT-1 = PT . However, connectivity of the elements of 
the graph of w is completely described by the matrices n and Z•
The equation ntZ = 0, whose proof has long been known from O. Veblen 
(1931), implies that
ZT = -PT nLt .
The latter expresses that the path-in-tree from the final point to the initial 
point of each edge in the tree-complement may be determined by adding the 
converse of the (initial point)-to-datum path to the (final point)-to-datum 
path.
Comments
The exposition of Sections 1.1 and 1.2 follows J. MacQueen (1981) and 
S. Kalpazidou (1988a, 1990a, 1993a, b). Section 1.3 is written according to 
S. Kalpazidou (1988a, 1993a, e, 1994a), F.H. Branin, Jr. (1959, 1966), and 
A.H. Zemanian (1991).

2
Genesis of Markov Chains by 
Circuits: The Circuit Chains
In this chapter we shall show how Markovian dependence can arise from 
at most countable collections of overlapping weighted directed circuits. 
The corresponding processes are Markov chains, that is, discrete parameter 
Markov processes which, generated by circuits, will be called circuit chains.
2.1 Finite Markov Chains Defined by
Weighted Circuits
2.1.1. Observe the passages of a particle through the points of a finite set 
S = {a, b, c, d, e, f, g} at moments one unit of time apart, always moving 
along one of the overlapping directed circuits {c1, c2, c3} as in Figure 2.1.1. 
Each circuit ci has its points in S and is assigned to a strictly positive 
weight wci . Suppose there is a camera which registers the passages of the 
particle along one directed arc chosen at random. If we project the states 
through which the particle was passing until the nth moment, we shall get 
a random sequence ..., £n- 1, tn, of observations with values in S.
Following J. MacQueen (1981) and S. Kalpazidou (1988a) we may define 
transition probabilities of such a stochastic sequence in terms of circuit 
weights. To make clear the presentation let us consider histories of one- 
steps. For instance, if such a history is k = b, we are interested in defining 
the transition probabilities from tn = k to £n+1 = x, x G S, where n belongs 
to the set Z of all integers. Thus, to calculate these transition probabilities 
we follow the steps below:

18 
2. Genesis of Markov Chains by Circuits: The Circuit Chains
Figure 2.1.1.
(i) 
We look for the set C(k) of all circuits which pass through k. In case 
C(k) is not empty, then the passages to other states are allowed and 
we may go on with the following steps.
(ii) 
We consider the C(k, x) of all circuits which pass through (k, x). In 
case C(k, x) is empty then no passage to x will take place.
(iii) The transition probabilities from k(=b) to x are expressed in terms 
of the weights of the circuits in C(k) and C(k, x) by the relations:
P(in +1 = d/in = b)= £ 
Wc' / 
Wc'
= (wc1 + wc3)/(wc1 + wc2 + wc3),
P(in +1 = c/in = b)= £ 
Wc' / 
Wc'
= Wc2 /(Wc1 + Wc2 + Wc3),
P(in +1 = x/in = b) = 0, X & S\{c, d},
for all n & Z.
Then the above probability law leads us to a Markov chain i = (in)nEZ 
which will be called a circuit chain.
Let us now recall the definition of a Markov chain. Let S be at most a de­
numerable set (i.e., S is either finite or denumerable). An S-valued sequence 
X = (Xn)n>0 of random variables on the probability space (Q, K, P) is said 
to be a homogeneous Markov chain (or a homogeneous discrete parameter 
Markov process) with state space S if for any n > 0 and i0, i 1,..., in +1 & S 
we have
P(Xn+1 = in+1/Xn = in, Xn-1 = in-1,...,X0 = i0)
= P(Xn+1 = in+1/Xn = in),

2.1 Finite Markov Chains Defined by Weighted Circuits 19
whenever the left member is defined, such that the right member is inde­
pendent of n. The previous equality is called the Markov property and it 
can occur even if the right member depends on parameter value n (it is 
non-homogeneous). Moreover, the probability
P(Xn+1 = j/Xn = i)
is called the transition probability of the chain from state i to state j and is 
usually designated by pij. Then the P = (pij)i,jeS is a stochastic matrix, 
called the transition matrix of the Markov chain, that is, a matrix whose 
elements satisfy
Pij > 0, 
P^^Pii = 1 • i G S.
Therefore any Markov chain determines a stochastic matrix. The converse, 
which is much deeper, is given by the well-known existence theorem of 
Kolmogorov and establishes a basic relationship between nonnegative 
matrices and Markov chains. The reader may find a comparative study of 
nonnegative matrices and Markov chains in E. Seneta (1981).
On the other hand, there exists a large class of Markov processes that 
can be defined as the Markov chain £ related to Figure 2.1.1, that is, their 
finite-dimensional distributions are completely determined by collections of 
weighted directed circuits. This will then motivate the definition and the 
general study of the Markovian dependence in terms of collections (C, wc) 
of directed circuits and weights, which in turn leads to a link between non­
negative matrices and (C, wc). As a consequence, related fields to probabil­
ity theory as ergodic theory, harmonic analysis and potential theory may 
be developed in terms of the cycles.
Turning to the particular case of Figure 2.1.1, we see that the Markov 
chain £ is irreducible, that is, for any pair (i, j) of states either Pij > 0, or 
there exists a path (i, i1), (i1,i2),...,(in,j) such that Pii1 Pi1i2 ...Pinj > 0. 
The oriented graph G associated with an irreducible Markov chain is 
strongly connected. (Recall that (i, j)isanedgeofG if and only if 
Pij > 0). In the case of a circuit Markov chain associated with a collection 
(C, wc ) irreducibility has a complete expression in terms of the circuits 
of C as follows: any two states i and j are circuit-edge-connected, that 
is, there exists a sequence of directed circuits c1,...,cm,cm+1,...,cn of C 
such that i lies on c1 and j on cn , and any pair of consecutive circuits cm 
and cm+1 have at least one point in common (see also Proposition 3.4.1). 
Then we shall say that C satisfies the irreducibility-condition.
Let us now change the time-sense, seeing the retroversion of the film 
of observations along the reversed circuits of Figure 2.1.1 until the nth 
moment, namely,..., xn+1 ,Xn• Note that the circuits which are entering 
a vertex are the same as those which are leaving it in the corresponding 
reversed circuits. Then we find that transition probabilities from state b to 
x G S satisfy the equations:
P(£n+1 = x/£n = b) = P(xn = x/xn+1 = b) ,
(2..1)

20 
2. Genesis of Markov Chains by Circuits: The Circuit Chains
for all n G Z, where the transition probability from state b to x in chain x = 
(Xn,Xn +1,...) is defined by using, instead of the classes C(b) and C(b,x) 
occurring at steps (i) and (ii) above, the classes C- (b) and C- (x, b) which 
contain all the inverse circuits c— of the circuits c G C passing through b 
and (x, b), respectively. Namely,
P(Xn = x/xn +1 = b ^2 
Wc- / 
22 Wc-’
where wc> = wc ,c G C. Plainly, in general, x is not the inverse chain of £ 
-
as long as the circuits are directed. The inverse chain of £ is given by the 
class C(x, b) when defining the transition probability from state b to x.
In conclusion, equation (2.1.1) argue for a dichotomy into two sequences 
£ =(£n)andx = (xn) with reversed parameter-scale, which keep not only 
the Markovian nature of the transition laws, but also the transition laws 
are related numerically by equation (2.1.1).
In this light we have to study the behavior of the pair (£, x) as a whole 
(see Kalpazidou (1988a)).
2.1.2. Let us now give a rigorous presentation of the heuristic introduction 
above.
Consider a nonvoid finite set S and a finite collection C of overlapping 
directed circuits in S. Suppose further that all the points of S can be reached 
from one another following paths of circuit-edges, that is, for each two 
distinct points i and j of S there exists a sequence c 1,... ,ck,k > 1, of 
circuits of C such that i lies on c1 and j on ck, and any pair of consecutive 
circuits (cn , cn+1 ) have at least one point in common. In general, we may 
assume that C contains, among its elements, circuits whose periods are 
greater than 2. Another version would be to assume that all the circuit 
periods are equal to 2.
Let C— be the collection of the reverses c— of all circuits c G C as in­
troduced in Definition 1.1.4. Associate a strictly positive number Wc with 
each c G C. Since the numbers Wc must be independent of the choice of 
the representative of c (according to Definition 1.1.2), suppose that they 
satisfy the following consistency condition:
c-tC-
Wc ◦ ti = Wc, 
iG Z,
(2.1.2)
where ti is the translation of length i occurring in (1.1.1).
Put
Wc- = Wc, 
c— G C— .
(2.1.3)
Define
W(k, i)= 
WcJc(k, i), 
k,iG S,
(2.1.4)
ctC
W— (v, i)= 
Wc- Jc- (v, i), 
v,iG S,
(2.1.5)

2.1 Finite Markov Chains Defined by Weighted Circuits 21
w(k) = 
wcJc (k),
ceC
w-(v) = 
wc- Jc- (v),
k & S, 
(2.1.6)
v & S, 
(2.1.7)
where Jc (•, •) and Jc (•) are the passage-functions of c introduced by Defi­
nition 1.2.2.
From Lemma 1.2.3 we have
Proposition 2.1.1. The functions w(•, •),w(•),w-(•, •) and w-(•) defined 
by (2.1.4)-(2.1.7) satisfy the following balance properties:
(^ 1)
(i) w(k) = 
w(k, i)= 
w(j, k),
(ii) w- (v)= 
w- (i, v)= w- (v, j),
( P2)
w(k, v)=w- (v, k),
for any k, v & S .
We now recall a standard result (S. Kalpazidou (1988a)) which relates 
the pair (w, w-) with Markov chains.
Theorem 2.1.2. Suppose we are given a finite class C of overlapping di­
rected circuits in a finite set S, and a set of positive weights {wc}ceC satis­
fying the assumptions stated at the beginning of Subparagraph 2.1.2.
Then there exists a pair ((Cn), (xn))nez of irreducible S-state Markov 
chains on a suitable probability space (Q, K, P) such that
P( Cn +1 = i/Cn ) = w ( Cn,i )/w ( Cn ),
P(xn = i/xn+1) = w-(i,xn+1)/w-(xn+1),
P(Cn+1 = i/Cn = j) = P(xn = i/xn+1 = j),
P-almost surely, for any n & Z and i, j & S.
Proof. By Daniell-Kolmogorov’s theorem there exist two S-valued Markov 
chains C = (Cn)n and x = (xn)n on a suitable probability space (Q, K, P) 
and with transition probabilities given by
P(Cn+1 = i/Cn = k)
—^,,P, if there isc & Csuch that Jc(k) • Jc(k, i) = 0;
w(k)
0, otherwise;

22 
2. Genesis of Markov Chains by Circuits: The Circuit Chains
P( Xn = i/Xn +1 = V )
{
w ( i.V ) 
. . 
.. 
. .
------if there is c- e C- such that Jc- (v) • Jc- (i, v) = 0; 
w- (V) 
- 
-
0, 
otherwise;
for any n e Z and k,v,i e S. The chains £ and x are irreducible since 
by hypotheses each two distinct points of S are circuit-edge-connected. 
Moreover, since w(k, i) = w-(i, k), then
P(£n+1 = i/£n = k) = P(Xn = i/Xn+1 = k),
for any n e Z. Also, according to the balance equation (0 1), the chains £ 
and X above have unique stationary distributions p and p- defined as
p(k) = p- (k) = w(k)/ 
w(k) = w- (k)/ 
w- (k), ke S.
This completes the proof. 
□
The following theorem asserts the existence of the inverse chains of £ and 
X in terms of circuits:
Theorem 2.1.3. Assume a finite class C of overlapping directed circuits 
in a finite set S is given together with a set of positive weights {wc}cec as 
in Theorem 2.1.2.
(i) Then there exists a pair ((£'n)n, (x'n)n) of irreducible S-state Markov 
chains defined on a suitable probability space (Q, K, P) such that
P( £n = i/Xn + 1) = w ( i,£'n +1 )/w ( £n +1),
P(Xn +1 = i/Xn ) = w- (Xn, i)/w- (Xn ),
P(£'n = i/£'n +1 = j ) = P(Xn +1 = i/Xn = j ),
P-almost surely for any n e Z and i, j e S.
(ii) The chains (£'n)nEz and (Xn)nez are Doob versions of the inverse 
chains of the chains given by Theorem 2.1.2.
Definition 2.1.4. The Markov chains £ and X occurring in Theorems 2.1.2 
and their inverse chains occurring in Theorem 2.1.3 are called circuit chains 
associated with the finite classes C and C- of circuits in S and with the 
positive weights wc = wc- ,c e C.
In general, Theorem 2.1.2 may be extended to any collection C of directed 
circuits in which case the corresponding circuit chains are recurrent Markov 
chains.
The stochastic behavior of a circuit chain generated by a collection 
(C, wc ) of directed circuits and weights depends on the choice of C and 
{wc}. Sometimes one may express certain stochastic properties in terms 

2.2 Denumerable Markov Chains Generated by Circuits 23
of the circuits alone. For instance, as we have already seen the irreducibil­
ity of the S-valued chain £ provided by Theorem 2.1.2 follows from the 
irreducibility-condition on C, namely, any two points of S are circuit-edge- 
connected. Also, periodicity (or aperiodicity) of the same irreducible circuit 
chain £ can be given in terms of the circuits as follows. Suppose C satisfies 
the irreducibility-condition and let G denote the graph associated with C. 
We say that C satisfies the periodicity (or aperiodicity) condition if there is 
a point i of S such that the greatest common divisor of the periods of all 
the directed circuits occurring in G and passing through i equals a natural 
number d> 1(ord = 1). Then the Markov chains generated as in Theorem 
2.1.2 by C endowed with any collection {wc} of weights are said to be peri­
odic with period d or aperiodic according as C satisfies the above periodicity 
or aperiodicity condition. For instance, if C contains a loop (i, i), then £ is 
aperiodic.
In Chapters 1 and 2 of Part II we shall deal with other stochastic prop­
erties that can be expressed in terms of the directed circuits.
2.2 Denumerable Markov Chains
Generated by Circuits
A natural extension of finite circuit chains to a countable infinity of circuits 
is particularly important in connection with the study of special problems 
concerning denumerable Markov chains, countable nonnegative matrices, 
infinite electrical networks, and others. For instance, a main question we 
are faced with in Markov chain theory is the so-called type problem, that 
is, the problem of determining if these processes are recurrent or transient 
(the geometrical correspondent is to decide whether a surface is parabolic 
or hyperbolic (see L.V. Ahlfors (1935), H.L. Royden (1952), L.V. Ahlfors 
and L. Sario (1960), and J. Milnor (1977), and also, G. Polya (1921)).
Let us now recall briefly the definition of recurrent (or transient) Markov 
chains. Let S be a denumerable set and let £ = (£n)n>0 be an S-state 
Markov chain. Denote further by fj, i,j G S, the probability that the chain 
£, starting in state i, reaches state j at least once. A state i G S is said to be 
recurrent or transient according as fii =1 or fii < 1. Usually these prop­
erties are expressed in terms of the n-step transition probabilities pi(jn) of 
£ as follows. A state i G S is recurrent or transient according as the series 
n£21p 1 P(n) diverges or converges (see E. Seneta (1981)). On the other hand, 
we have the dichotomy positive states and null states. A state i G S is said 
to be positive or null according as the mean frequency of passage from state 
i to i is strictly positive or zero. Since recurrence (or transience) is a class 
property then £ may have recurrent (or transient) classes. Analogously, £ 
may have positive (or null) classes. Here a class is either a set of mutually 
communicating states or consists of a single state.

24 
2. Genesis of Markov Chains by Circuits: The Circuit Chains
The Markovian dependence related to electrical networks was studied in 
many works like C.St.J.A. Nash-Williams (1959), G.J. Minty (1960), G.K. 
Kemeny, J.L. Snell, and A.W. Knapp (1976), F. Kelly (1979), D. Griffeath 
and T.M. Liggett (1982), T. Lyons (1983), P.G. Doyle and J.L. Snell (1984), 
and others. Recent valuable contributions to stochastics on networks are 
due to Y. Derriennic (1973-1993), Y. Guivarc’h (1980a, b, 1984), W. Woess 
(1986-1994), M.A. Picardello et al. (1987-1994), P.M. Soardi (1990, 1994a, 
b), L. DeMichele et al. (1990), and others.
A detailed and updated exposition of infinite electrical networks is due 
to A.H. Zemanian (1991) (see also A.H. Zemanian (1965-1992), A.H. 
Zemanian and P. Subramanian (1983)). An infinite resistive network is 
a pair consisting of an unoriented connected infinite graph and a nonnega­
tive function defined on the set of edges. Arguing the extension to infinite 
networks, Zemanian (1991) points out that questions which are meaning­
less for finite networks crop up about infinite ones, for example, Kirchhoff ’s 
current law need not hold at a node with an infinity of incident edges, and 
Kirchhoff ’s voltage law may fail around an “infinite” circuit.
On the other hand, Markov chain analysis does not always agree with 
that of electrical networks—we here refer the reader to a recent work of 
S. McGuinness (1991) according to which Nash-Williams’s theorem con­
cerning recurrence of locally finite networks can be generalized to networks 
without the local finiteness condition. Recent results of E. Schlesinger 
(1992), and P.M. Soardi and M. Yamasaki (1993) show recurrence­
transience criterions for networks satisfying weaker finiteness conditions 
than the local finiteness.
Our approach to countable circuit chains follows S. Kalpazidou (1989b, 
1990b, 1991a). Consider an infinite denumerable class C of overlapping 
directed circuits with distinct points (except for the terminals) in a denu­
merable set S.
Let C- = {c-, c- is the reversed of c, c G C}.
Assume the following hypotheses are satisfied:
(c1 ) The circuits determine an infinite oriented graph of bounded degree, 
that is, there is some integer n0 > 1 such that the number of circuits 
that pass through any point of S is at most n0 .
(c2) maxcEcp(c) = R < <x, where p(c) denotes the period of c.
(c3) (Connectedness). For every two points k and u of S there ex­
ist a finite sequence of circuits c1 ,. ..,cm and a finite path k0 = 
k, k1,...,km = u of points on c1,...,cm that connect k to u, that 
is, (kn, kn+1) is passed by cn+1,n=0,...,m- 1, in the sense of 
Definition 1.2.2.
In general, the collection C may contain infinitely many circuits with peri­
ods greater than 2. (There are contexts where it is more suitable to consider 
only circuits of period 2 (see Y. Derriennic (1993).)

2.2 Denumerable Markov Chains Generated by Circuits 25
Associate a strictly positive number wc with each c G C and, as­
suming the same conventions (2.1.2) and (2.1.3), define the functions 
w (•, •) ,w (•), w- (•, •), and w- (•) by relations (2.1.4)-(2.1.7). Then there 
exist two irreducible S-state Markov chains £ = (£n)n and x = (Xn)n 
with the transition laws given, respectively, by (w(k, i)/w(k), k, i G S) and 
(w-(i, k)/w-(k), k, i G S). Both processes £ and x are called denumer­
able circuit chains generated by (C,wc) and (C-,wc- ), respectively. Fur­
thermore, these processes admit the collection (w(k), k G S) as an invari­
ant measure (since w-(k) = w(k),k G S). The reader may find results 
on Markov processes admitting invariant measures in T.E. Harris and 
R. Robins (1953), T.E. Harris (1956, 1957), C. Derman (1954, 1955), R.G. 
Miller, Jr. (1963), E. Seneta (1981), and others.
When either Cor {wc} varies, we may define a collection of circuit chains 
as above. Furthermore, one may obtain a recurrent or transient behavior 
for each circuit chain according to the additional constraints imposed on 
(C, wc ). One way to investigate the type problem for the above circuit chain 
£ is to relate the representative collection (C, wc) of directed circuits and 
weights with an infinite electrical network in order to apply a variant of the 
Rayleigh short-cut method (see J.W.S. Rayleigh (1870)), which phrased 
in probabilistic term leads to the Nash-Williams recurrence criterion for 
reversible Markov chains; that in turn leads to Ahlfors’s criterion (see L.V. 
Ahlfors (1935)).
A condition for characterizing recurrence (or transience) of Markov pro­
cesses will be called an Ahlfors-type criterion if it involves the growth func­
tion of the state space. In Chapter 1 of Part II we shall give an Ahlfors-type 
sufficient condition, in terms of the circuits, for a reversible circuit chain 
to be recurrent.
Now we shall show a Nash-Williams-type sufficient condition on the 
weights wc for a circuit chain to be recurrent (S. Kalpazidou (1989b, 1990d, 
1991a, e)). The Nash-Williams theorem asserts the following. Let S be a 
countable set and let £ = (£n) n>0 be an S-state Markov chain whose tran­
sition probabilities are the pij ,i,j G S. Suppose that the chain £ is re­
versible with respect to a measure n = (ni, i G S), with ni, > 0, that is, 
nipij = njpji. Let w(i,j) denote nipij for all i, j G S. Assume further that 
there exists a partition {Sk, k = 0, 1,...} of S such that u G Sk,k > 1, and 
w(u, u!) > 0 together imply u! G Sk- 1 U Sk U Sk +1, and that for each k the 
sum EuESk,u'es w(u,U) < ™. Denote ak = Euts^u'eSk+1 w(u, u), k = 
0, 1, 2,....
If 522=0(ak)-1 = ^, then the chain £ is recurrent. For a simple proof 
of Nash-Williams’s criterion we refer the reader to T. Lyons (1983) and S. 
McGuinness (1991).
However, there is an essential difference between our network and those 
to which the classical Rayleigh method refers: here the circuits are di­
rected. Consequently, to apply the Rayleigh-Ahlfors-Nash-Williams recur­
rence criterion, it is necessary to reconsider the definition of the passages 

26 
2. Genesis of Markov Chains by Circuits: The Circuit Chains
along the circuits in such a way that reversible chains result. This is 
achieved by a suitable definition of the passage-functions (see S. Kalpazidou 
(1989b)).
Define the function Jc : S x S ^ {0, 2},c G C, as follows:
{
1
2 ,
0,
if there exists j such that k = c ◦ tj (s) and either :
(i) u = c ◦ tj(s + 1) or (ii) u = c ◦ tj(s - 1) 
for some integer s;
otherwise;
(2.2.1)
where tj and c ◦ tj are given in relation (1.1.1). Then Jc is symmetric. 
Analogously define Jc- (u, k) for the inverse circuit c- .
Definition 2.2.1. The functions Jc(■, •) and Jc- (•, •) are called backward­
forward passage functions associated with c and c- , respectively.
From now on the passage in condition (c3) is understood to be a 
backward-forward passage, that is, a circuit c passes through (k, u) if and 
only if the backward-forward passage function Jc has a nonzero value at 
either (k, u) or (u, k). Put
Jc(k) = 
Jc(k, u),
u
kG S.
Then
Jc(k) = 
01,,
if k is a point of c; 
otherwise.
Condition (c3) asserts that any two points are cyclic-edge-connected, and 
enables us to introduce a distance d in S defined as
( 0, 
if k = u;
d(k, u)= 
the shortest length of the paths
[ along the edges of Cconnecting k to u, if k = u;
where the passages through the edges are understood to be the backward­
forward passages.
Fix 0, an arbitrary point in S called the origin. Let Sm,m=0, 1, 2,..., 
be the “sphere” of radius m about the origin, that is, those points of S that 
are exactly m edges distant from the origin. Then {Sm,m=0, 1, 2,...} is 
a partition of S. With the backward-forward passage functions in the def­
inition of the functions w(i, j ), w(i), w- (i, j ),w- (i) (according to relations 
(2.1.4)-(2.1.7)), the corresponding processes £ and x become reversible with 
respect to the measure (w(i), i G S) = (w-(i), i G S). Put
ak 
W^ w (u, u'), 
k = 0, 1, 2,....
uESk u'ESk +1

2.2 Denumerable Markov Chains Generated by Circuits 27
We now prove
Theorem 2.2.2. If 
'J2( ak) -1 = <x, 
(2.2.2)
K = 0
the reversible circuit chains (£n)n and (xn)n are recurrent.
Proof. If u belongs to the sphere Sk for some k and w(u,u') > 0, then 
u' G Sk- 1 U Sk U Sk+1. On the other hand, condition (c 1) implies that
w(k,u) < w, 
k G S.
u
Hence
w^u w(u,u') < <x. 
(2.2.3)
uESk 
u' £ S
Then relations (2.2.2) and (2.2.3) imply that the hypotheses of 
Nash-Williams’s recurrence criterion are satisfied, and thus the chain 
£ is recurrent. The proof for the chain x may be done in a similar 
manner. 
□
Remark. (i) Y. Derriennic proposed another way for defining a passage 
function associated with a reversible Markov chain. The idea consists of 
considering a “symmetric” class C of directed circuits, that is, if c G C then 
c- G C as well. Accordingly, one may introduce a passage function Jc(i, j) 
by the same Definition 1.2.2.
(ii) A new scrutiny of the proof of Theorem 2.2.2 leads us to the question 
of whether or not there exists a necessary and sufficient criterion of Ahlfors- 
type for characterizing recurrence of the circuit chains.
A sufficient condition of Ahlfors-type is given by S. Kalpazidou (1989b) 
using the Royden-Lyons criterion in terms of flows (see T. Lyons (1983)). 
However, a counterexample of Varopoulos (1991) shows that, in general, a 
necessary and sufficient condition of Ahlfors-type for recurrence of contin­
uous parameter Markov processes is bound to fail. Specifically, Varopou- 
los’s example consists of a Brownian motion on a two-dimensional manifold 
which is recurrent even though the volume grows exponentially. Plainly, the 
discretization of Varopoulos’s counterexample (an open problem) would an­
swer the question above.
(iii) The constructive approach of this chapter to circuit chains relies 
upon algebraic considerations. The algebraic constraints given by the bal­
ance equations do not ensure the uniqueness of the circuit weights cor­
responding to a circuit process, that is, there are many collections of 

28 
2. Genesis of Markov Chains by Circuits: The Circuit Chains
circuits and weights generating the same process. In Chapter 3 we shall 
show the existence of a probabilistic argument for the representative cir­
cuits and weights that ensures their uniqueness—in this case, the gener­
ative class {C, wc} will express certain probabilistic characteristics of the 
process.

3
Cycle Representations of Recurrent 
Denumerable Markov Chains
This chapter deals with the cycle generating equations defined by the tran­
sition probabilities of denumerable Markov chains £ which are recurrent. 
The solutions (C, wc) of cycles and weights to these equations will be called 
cycle representations of £ .
A natural idea to define a cycle (circuit) weight wc is similar to that 
providing an “edge-weight” nipij, that is, the wc will be the mean number 
of the appearances of c along almost all the sample paths. This will argue for 
a probabilistic criterion assuring the uniqueness of the cycle representation, 
that is, for a probabilistic algorithm with a unique solution of cycles and 
weights which decompose the finite-dimensional distributions of £.
An alternate method of development is a deterministic approach accor­
ding to which the circuit weights are given by a sequence of nonprobabilistic 
algorithms.
Our exposition follows the results of the Peking school of Qians (1978­
1991), S. Kalpazidou (1990a, 1992e, 1993c, 1994b), and Y. Derriennic 
(1993).
3.1 The Derived Chain of Qians
As we have already seen in Theorem 1.3.1, the representative collection 
(C, wc ) of circuits and weights is not, in general, unique. It depends on 
the choice of the ordering of the representative circuits in the algorithm of 
Theorem 1.3.1.

30 
3. Cycle Representations of Recurrent Denumerable Markov Chains
In general, there are many algorithms of cycle decompositions for the 
finite-dimensional distributions of Markov chains which admit invariant 
probability distributions. Some of them provide a unique solution (C, wc) 
as a representative class, and some others have many solutions of represen­
tative classes (as the algorithm of Theorem 1.3.1). So, when we say that 
we look for the uniqueness of the representative class (C, wc), we under­
stand that we shall refer to a definite algorithm with a unique solution 
(C, wc).
Expectedly, such an algorithm can be defined involving a probabilistic 
argument. It is Qians’s school that first introduced probabilistic arguments 
to a unique cycle representation using, as a basic tool, a Markov process 
whose state space consists of the ordered sequences (i1,...,in)ofdistinct 
points of a denumerable set S. Here we shall present Qians’s approach in the 
contexts of our formalism exposed in Chapter 1. So, preliminary elements 
of our exposition are the directed cycles with distinct points as introduced 
by Definition 1.1.3. Accordingly, a cycle is an equivalence class with respect 
to the equivalence relation defined by (1.1.1); for instance, to the circuit 
c = (i 1,... ,in, i 1) is assigned the cycle c = (i 1,... ,in) which represents the 
cycle-class {(i1,...,in), (i2,i3,...,in,i1),...,(in,i1,...,in-1)}. This pre­
supposes that all further entities which rely on cycles should not depend 
on the choice of the representatives while the circuits to be considered will 
have distinct points (except for the terminals).
The idea of taking directed cycles arises from the topological prop­
erty of the tra jectories of certain Markov chains providing directed cycles 
along with directed circuits, that is, the chains pass through the states 
i1,i2,...,in,i1, or any cyclic permutation (see Figure 3.1.1).
So, the occurrence of a cycle (i1,...,in) along a trajectory of these chains 
presupposes the appearance of the corresponding circuit (i1,...,in,i1). 
Such a chain is any homogeneous, irreducible, aperiodic, and positive- 
recurrent Markov chain £ = (£n,n > 0) with a countable state space S. 
Namely, if a typical realization of a sample path (£n(w))n is (i 1 ,i2,i3,i2, 
i3, i4 ,i 1, i3, i5,...), ik G S, k = 1, 2,..., then the sequence of the cycles is 
(i2, i3), (i2, i3, i4, i1), (see Figure 3.1.1).
The interpretation of a cycle c=( i 1,..., ir) in terms of the chain 
£ is that it appears on a sample path (£n(w))n (and then on almost
Figure 3.1.1.

3.1 The Derived Chain of Qians 31
all the sample paths as we shall see below), that is, the chain passes 
through the states i1 ,i2,...,ir,i1 (or any cyclic permutation). For in­
stance, if the values of (in (w)) n> 0 are given by (1, 4, 2, 3, 2, 6, 7, 6, 1,...), 
then the sequence of cycles occurring on this trajectory is given by 
(2, 3), (6, 7), (1, 4, 2, 6),..., while the corresponding tracks of the remain­
ing states are (1, 4, 2, 6, 7, 6, 1,...)(1, 4, 2, 6, 1,...) (S. Kalpazidou (1990a, 
1994b)). The previous decycling procedure can be found in various fields 
under different versions. For instance, S. Alpern (1991) introduced a sim­
ilar decycling method in game theory. This leads naturally to a new 
chain y = (yn (w))n>0 whose value at time k is the track of the remain­
ing states, in sequence, after discarding the cycles formed up to k along 
(in (w)) n> 0.
In the following table we give the trajectory (1, 4, 2, 3, 2, 6, 7, 6, 1, ...)of 
(in(w))n along with the attached trajectory (yn(w))n as well as the cycles 
occurring along (in (w)) n:
n
0
1
2
3
4
^n (^ )
1
4
2
3
2
yn (^')
[1]
[1, 4]
[1, 4, 2]
[1, 4, 2, 3]
[1, 4, 2]
Cycles
(2, 3)
n
5
6
7
8
...
£,n (^ )
6
7
6
1
...
yn (^')
[1, 4, 2, 6]
[1, 4, 2, 6, 7]
[1, 4, 2, 6]
[1]
...
Cycles
(6, 7)
(1, 4, 2, 6)
...
It turns out that each cycle c = (i 1,... ,ir) is closed by the edge (ir ,i 1) 
which occurs either after c, or before completing c, as (i 1, i2) in the cycle 
(i2, i3, i4, i1) of Figure 3.1.1, or as (1, 4) in the cycle (1, 4, 2, 6) of the table 
above, where the time unit is the jump-time of (in(w))n .
Let wc,n(w) be the number of occurrences of the cycle cc up to time n 
along the trajectory w of i. The rigorous definition of wc,n(w) is due to 
Minping Qian et al. (1982). It is this definition that we describe further. If 
tn (w) denotes the nth jump time of (in(w))n, then introduce
t i( w) = min {tn (w) : Bm < n such that itn (w)(w) = im (w)(w)}, 
T* ( w ) = tm ( w ), if tm ( w ) <T 1( w ) and itm (u) ( w ) = iT 1( u)( w ).
Define
i(1) (w)= in(w),
in ( w ) 
I iT 1(„ )( w )= iT 1* (w )( w ) ,
if n < t{ (w) or n > t 1(w); 
if t{ (w) < n < t 1 (w).

32 
3. Cycle Representations of Recurrent Denumerable Markov Chains
Further we continue the same procedure of discarding cycles by considering 
the nth jump-time t„)(w) of (f(l)(w))n. Then we put 
n 
n n.
t2(w) = min{t(1)(w) : 3m < n such that f (1)} 
(w) = f (1)} 
(w)}
2 
n 
(1) 
(1)
tn (W ) 
tm (W )
and so on, obtaining the sequence:
T 1( w) <T2( w) < • • • < Tn (w) < • • •
and
T1 (w) <T2 (w) < ••• < T (w) < ••• •
Now, denote an ordered sequence of distinct points i1,. . . ,ir by 
[ii, • • •, ir] and identify the ordered union [[i 1, • • •, im], [im +1, • • •, im+n]] 
with [i 1, • • •, im, im+1, • • •, im+n]. The set [S] of all finite ordered sequences 
[i 1, • • •, ir], r > 1, of points of S is denumerable.
Set t0(w) = 0. Define
y0 (w) = [f0 (w)],
yn(w) = [f0(w)], 
ifn<t1(w)
yn (w ) = [ f 0( w ) ,ft 1( w)(w ), • ••,fn ( w )], 
if t 1( w ) < n < T 1( w ),
yT 1 (W ) ( w ) 
[ f 0 ( w ) , ft 1 (W )( w ) , • • • , fT* (W ) ( w )] ,
yn ( w ) = [ yT 1( w )( w ) , [ fts ( w )( w )] t 1( w ) <ts ( w ) <n], 
if T 1( w ) <n<T2( w ),
and so on. It is easy to see that y = {yn}n>0 is an [S]-state Markov chain 
called by Minping Qian the derived chain associated to f.
Furthermore, it is seen in S. Kalpazidou (1990a) that if for a cycle c = 
(i 1, • • •, ir) the sum
n r
1{ w:ym-1 (w)=[ym(w),[ik,ik-1,...,ik+r-1]]}(w)
m=1 k=1
is meant modulo r the cyclic permutations (i.e., it is independent of the 
cyclic permutations of ik, ik+1, • • •, ik+r- 1), then it equals
wc,n(w) 
^3 1 {the class-cycle c occurs}(w) • 
(3.1.1)
m=1
If Pjk,j,k ^ S, denote the transition probabilities of f, then for E = 
[k 1, k2, • • •, ks ] and F = [j 1 ,j2, • • • ,jr] the transition probabilities pFE of 
y are given as follows:
{
Pjr ks, if either r > s and k 1 = j 1, k 2 = j 2 ,• ^^,ks = js, 
or r = s - 1 and k 1 = j 1, k 2 = j 2 ,...,kr = jr; 
(3.1.2)
0, 
otherwise •
Since f is recurrent, we have
Prob(fn returns to i/f0 = i)=1,

3.1 The Derived Chain of Qians 33
and then
Prob(yn returns to [i]/y0 = [i]) = Prob(£n returns to i/£0 = i) = 1.
Let now [E]i be the subset of all ordered sequences in [S] whose first 
element is i. Then [E]i is a stochastically closed class of y. Therefore y is 
recurrent on each irreducible class [E]i . The invariant probability distribu­
tion n is given on the point sets [i] by
n([ i]) = n (i), 
(3.1.3)
where n = (ni, i G S) denotes the invariant probability distribution of £.
The general definition of n([ i 1, i 2,... ,is ]) has a much more complex al­
gebraic expression in terms of the transition probabilities pij of £ as we see 
in the following theorem due to Minping Qian and Min Qian (1982):
Theorem 3.1.1.
(i) The invariant probability distribution of the chain y on the recurrent 
class [E]i is given by
n([i 1 ,i2,...,is]) = pi 1 i2pi2i3 ••• pis- 1 is • ni 1 N(i2,i2/i 1)
x N (i 3, i 3/i 1, i 2) • • • N (is,is/i 1, .. .,is- 1) (3.1.4)
where i 1 = i and N(i, j/i 1, ..., ik), 1 < k < s — 1, denotes the taboo 
Green function
N(i,j/i1,...,ik)= 
Prob(£n = j,£m = i1,...,ik;
n=0
for 1 < m<n/£0 = i).
(ii)
s
n([ i 1, i 2,... ,is]) pisi 1 
I2n([j 1, . . .,jr ,ik ,ik +1, . . . ,ik-1]) PiK- 1iK,
(3.1.5)
where j1 is fixed in the complement set of {i1,i2,...,is} and 
the inner sum is taken over all distinct choices j2 ,j3,...,jr G 
S\{j1,i1,...,is}. The sums k +1,k+2,...,k+ s — 1 are under­
stood to be modulo s.
(iii) For any fixed points i and j we write
nj 
^2 n([ i,j 2 ,...,jr,j ]), 
(3-1-6)
j2 ,...,jr
where the sum is taken over all distinct choices j2 ,j3,...,jr G 
S\{i, j}.

34 
3. Cycle Representations of Recurrent Denumerable Markov Chains
Proof. According to T.E. Harris (1952) we have the following identities:
niN(j,j/i)= nN(i, i/j), 
(3.1.7)
n(E1) q(E1, E2) = n(E2) g(E2,E 1), 
(3.1.8)
for any E 1, E2 states in [E]i, where q(Ei, Ej) denotes the probability that 
the derived chain y starting at Ei enters Ej before returning to Ei . Then, 
for E1 =[i1,i2,...,is-1]andE2 =[i1,i2,...,is-1,is]wehavethat
q(E1 ,E2) = pis-1 is,
q( E 2 ,E 1) = 1 - H (is,is/i 1 ,i 2 ,...,is-1),
where H(is, is/i1, i2,..., is-1) denotes the probability that the original 
chain £ starting at is returns to is before entering the states i 1, i2,..., is- 1. 
Hence relation (3.1.8) becomes
n([ i 1, i 2,..., is-1]) pis—1 is = n([ i 1, ... ,is ])(1 - H (is,is/i 1,. .., is-1))
(3.1.9)
and
n([ i 1, i 2, ..., is ]) = n([ i 1, i 2,..., is-1]) pis—1 is N (is, is/i 1, i 2,..., is-1).
(3.1.10)
Now we may appeal to a theorem of K.L. Chung (1967) (see p. 48), and 
write accordingly
N(is, is/i1,..., is-1)N(is+1, is+1/i1,..., is-1, is)
= N (is+1, is+1/i1,..., is-1)N (is, is/i1,..., is-1, is+1). 
(3.1.11)
Then equation (3.1.4) follows from (3.1.3) and (3.1.10). It is to be noticed 
that the product
ni 1 N(i2,i2/i 1)N(i3, i3/i 1, i2) ... N(is, is/i 1, i2 ... ,is-1) 
(3.1.12)
is unaffected by any permutation of the indices i1 ,i2 ,...,is because of 
(3.1.7) and (3.1.11).
To prove relation (3.1.5) we first show that
s
1= 
1 
N(j1,j1/i1,...,is)
• N (j 2 ,j 2/i 1,..., is,j 1) N (j 3 ,j 3/i 1,.. .,is,j 1, j 2)...
• N (jr ,jr/i 1, . . .,is,j 1, . . .,jr-1) Pj 1 j 2 Pj 2 j 3 • • • Pjr iK , (3.1.13)
where j 1 / {i 1,..., is} is fixed and the inner sum is taken over all distinct 
j2,... ,jr / {i 1,..., is,j 1}. Let p(i,j/H/n) be the taboo probability
p(i,j/H/n) = Prob(£n = j, £m / H for 1 < m < n/£0 = i).

3.2 The Circulation Distribution of a Markov Chain 35
For k,j2,j3,...,jr fixed, the sum over n1 ,...,nr of
p(j1, j1/i1,..., is/n1)pj1j2p(j2, j2/i1,..., is, j1/n2)pj2j3
• 
• .P (jr ,jr /i 1, . . .,is,j 1, . . .,jr-1 I nr ) Pjr i,
is the probability for the chain £ starting at j 1 to enter the set {i 1,... ,is} 
for the first time at the state ik while the value of the derived chain y is 
[j1,j2,...,jr,ik]. Thus we get the summand of (3.1.13). Then the desired 
equation (3.1.5) follows by multiplying both sides of (3.1.13) with
Pisi 1 Pi 1 i2 .. .Pis-1 isni 1N(i2,i2/i 1) • N(i3,i3/i 1 ,i2)... N(is,is/i 1,... ,is-1),
and using the symmetry of (3.1.12). Finally, equation (3.1.6) follows from 
(3.1.13) when taking s = 1, j 1 = i, and i 1 = j, and multiplying by nj. □
3.2 The Circulation Distribution of a 
Markov Chain
A step closer to a probabilistic criterion for the uniqueness of the rep­
resentative cycle-weights of a Markov chain £, under the assumptions of 
the previous section, is to find a definite algorithm whose quantities enjoy 
probabilistic interpretations in terms of the sample paths. The idea is to 
generalize to cycles the definition of the “edge-weight” w(i,j) = nipj in 
terms of sample paths; namely, as is well known the w(i, j) is the mean 
number of the consecutive passages of (£n (w))n through the points i and 
j. That is, nipij is the almost sure limit of
-card{m < n : £m— i(w) = i,£m(w) = j}, 
n
as n — tt.
Accordingly, the revealing question for us will be whether or not we can 
analogously argue for the expression
—card{m < n : the cycle c occurs on (£k(w))k} = —wc,n(w), 
where m counts the appearances of c on (£k (w))k. (Recall that a cycle 
c = (i 1, i2,..., ir), r > 1, appears on (£k(w))k if the chain passes through 
the points i1,i2,...,ir,i1, or any cyclic permutation.)
In this direction, we first need to prove that (1/n)wc,n(w) has a limit 
independent of w. Namely, we have
Theorem 3.2.1. Let £ = (£n)n be an aperiodic, irreducible, and positive- 
recurrent Markov chain defined on a probability space (Q, K, P) and with 
a countable state space S, and let Cn(w),n = 0, 1, 2,..., be the class of all 
cycles occurring until n along the sample path (£n (w))n .

36 
3. Cycle Representations of Recurrent Denumerable Markov Chains
Then the sequence (Cn (w),wc,n(w)/n) of sample weighted cycles associ­
ated with the chain £ converges almost surely to a class (Ctt,wc), that is,
Ctt = lim Cn(w), a.s. 
(3.2.1)
n—>^>
wc = lim (wc,n(w)/n), 
a.s. 
(3.2.2)
Furthermore, the cycle-weights wc are independent of the choice of an 
ordering on Ctt.
Proof. Let pj = P(£0 = j),j G S. Following S. Kalpazidou (1990a), we 
can assign to each w the class limn—tx, Cn (w) of directed cycles that 
occur along (£n (w))n, since the sequence (Cn (w)) is increasing. Denote 
Ctt ( w ) = lim n—tt Cn ( w ) U n Cn ( w )•
On the other hand, applying the law of large numbers to the Markov 
chain y we have
n—mo ( wc,n ( w )/n ) 
E1 {the class-cycle c occurs},
where c is any class-cycle having the representative (ik, ik+1,... ,is,i 1,..., 
ik-1). Put
Wc = nim ( Wc,n (w)/n).
That wc is finite and independent of w follows from (3.1.5) and the following 
equalities due to Minping Qian et al. (1982):
s
c 
{yn-1 =[yn ,[ik ,ik+1 ,...,is ,i1 ,...,ik-1]]}
s
= ^2 p j 1 Z1 12 ^([j 1 ,j 2 ,...,jr ,ik ,ik +1 ,...,ik- 1]) • Pik-1 ik , (3.2.2')
where j 1,... ,jr / {i 1,..., is}, r > 0, are distinct from one another. From 
here it results that Ctt (w) = Ctt is independent of w as well, and this 
completes the proof. 
□
We now introduce the following nomenclature:
Definition 3.2.2. The items occurring in Theorem 3.2.1 are as follows: 
the sequence {wc,n (w)/n}ceC^, which is called the circulation distribution 
on w up to time n, the wc, which is called the cycle skipping rate on c or c, 
and {wc, c C Ctt}, which is called the circulation distribution of £.
Remarks
(i) 
Theorems 3.1.1 and 3.2.1 remain valid for periodic and positive- 
recurrent Markov chains as well. In general, convergence of averages along 
Markov chain trajectories is required (even if there is no finite-invariant 
measure). Recent investigations to this direction are due to Y. Derriennic 
(1976), and Y. Derriennic and M. Lin ((1989), (1995)).

3.3 A Probabilistic Cycle Decomposition for Recurrent Markov Chains 37
(ii) The wc’s verify the consistency equation wc = wc ◦ ti, for all i G S, 
where {ti} is the group of translations on Z occurring in (1.1.1).
As an immediate consequence of Theorem 3.2.1 one obtains from (3.2.2') 
the exact algebraic expression for the cycle skipping rate wc as follows:
Corollary 3.2.3. If n = (ni, i G S) is the invariant probability distribution 
of an S-state irreducible positive-recurrent Markov chain £ = (£n)n and c = 
(i1 ,i2 ,...,is ) is a cycle, then the cycle skipping rate wc is given by equation
wc = ni 1 Pi 1 i 2 Pi 2 i 3 ...pis- 1 is Pisi 1
• N(i2,i2/i 1)N(i3,i3/i 1 ,i2). ■ ■N(is,isli 1 ,i2,• • .,'is-1), 
(3.2.3)
where (pij, i, j G S) is the transition matrix of£,andN(ik,ik/i1,...,ik-1) 
denotes the taboo Green function introduced in (3.1.4).
3.3 A Probabilistic Cycle Decomposition 
for Recurrent Markov Chains
We are now prepared to answer our original question on the existence of a 
unique cycle decomposition, provided by a probabilistic algorithm, for the 
finite-dimensional distributions of the recurrent Markov chains. Namely, the 
probabilistic algorithm to be considered is that occurring in Theorem 3.2.1 
while the desired decomposition follows from Theorem 3.1.1 (see Minping 
Qian and Min Qian (1982), and S. Kalpazidou (1990a)).
Consequently, we may state
Theorem 3.3.1 (The Probabilistic Cycle Representation). Let S be any 
denumerable set. Then any stochastic matrix P = (pij ,i,j G S) defining 
an irreducible and positive-recurrent Markov chain £ is decomposed by the 
cycle skipping rates wc, c G C<x>, as follows:
nipij = 
wcJc(i,j), i,j G S, 
(3.3.1)
c: C ->..:
where C^ is the class of cycles c occurring in Theorem 3.2. 1, c denotes the 
circuit corresponding to the cycle c,n = (ni,i G S) is the invariant prob­
ability distribution of P and Jc(i, j)=1 or 0 according to whether or not 
(i, j) is an edge of c.
The above cycle-weights wc are unique, with the probabilistic interpreta­
tion provided by Theorem 3.2.1, and independent of the ordering of Cx.
If P defines a positive-recurrent Markov chain, then a similar decompo­
sition to (3.3.1) holds, except for a constant, on each recurrent class.
The representative class (C-t.w:.) provided by Theorem 3.3.1 is called 
the probabilistic cycle (circuit) representation of £ and P while £ is called a 

38 
3. Cycle Representations of Recurrent Denumerable Markov Chains
circuit chain. The term “probabilistic” is argued by the algorithm of Theo­
rem 3.2.1 whose unique solution {wc} enjoys a probabilistic interpretation 
in terms of the sample paths of £.
The terms in the equations (3.3.1) have a natural interpretation using 
the sample paths of £ as follows (S. Kalpazidou (1990a)). Consider the 
functions an (•; i,j) defined as
an(w; i,j) = ncard{m < n : £m-1(w) = i,£m(w) = j}
for any i, j G S. Consider Cn(w) to be, as in Theorem 3.2.1, the class of 
all the cycles occurring up to n along the sample path (£n (w))n. We recall 
that a cycle c = (i 1,... ,ir), r > 2, occurs along a sample path if the chain 
passes through states i1,i2,...,ir,i1 (or any cyclic permutation). Notice 
that the sample sequence
k(w) = (£m-1(w),£m(w))
occurs up to n whenever either k(w) is passed by a cycle of Cn (w) in the 
sense of Definition 1.2.2 or k(w) is passed by a circuit completed after time 
n on the sample path (£n (w)). Therefore for i = j and n>0, great enough, 
we have
an(w; i,j) = 
nWc,n(w) J(i,j) + n(w; i,j)/n, 
(3.3.2)
where
en(w;i,j) = 1{the last occurrence of (i,j) does not happen 
(w). (3.3.3)
together with the occurrence of a cycle of Cn (u)}
Then the left side of (3.3.2) converges to nipij and each summand of the 
right side converges to wcJc (i, j).
From the present standpoint a natural way of proving a cycle­
decomposition-formula is to observe that the a.s. limit of the sums
(wc,n(w)/n)Jc (i, j)
CeC^
when n tends to infinity is related with the sum occurring in equations 
(3.3.1). This inspires a direct proof of the decomposition (3.3.1) as in the 
following theorem due to Y. Derriennic (1993).
Theorem 3.3.2. Let S be a denumerable set and let P =(pij,i,j G S) be 
any stochastic matrix defining an irreducible and positive-recurrent Markov 
chain £. Then
nipij = lim 
(Wc,n(w)/n) Jc(i, j) a.s.
CeC^
= 
wcJc(i, j), 
(3.3.4)
CeC^

3.4 Weak Convergence of Sequences of Circuit Chains 39
where (C^ ,wc) and wc,n (w) have the same meaning as in Theorem 3.2. 1, 
n = (ni,i G S) is the invariant probability distribution of P and Jc(i,j) = 
1 or 0 according to whether or not (i, j) is on edge of c.
Proof. Consider the derived chain y associated to £ and an arbitrarily 
chosen irreducible class [E]i . Then the restriction of y to [E]i is a positive- 
recurrent chain whose invariant probability distribution is given by (3.1.4). 
Let [E]ij be the subset of [E]i which consists of all the cycles starting with 
the consecutive points i and j . Then, applying the Birkhoff ergodic theorem 
to the number of the visits of y in the set [E]ij, one obtains relations (3.3.4). 
The proof is complete. 
□
If £ is an irreducible null-recurrent Markov chain, then a cycle­
decomposition-formula may be obtained using a similar argument where 
Birkhoff’s theorem is replaced by the Hopf ergodic theorem for ratios. Ac­
cordingly, the limit of (wc,n(w)/wc/,n(w)) exists a.s. as n — <x for any cir­
cuits c and c.
3.4 Weak Convergence of Sequences of Circuit 
Chains: A Deterministic Approach
We introduced two types of circuit representations of Markov chains ac­
cording to whether or not the corresponding algorithms define the circuit­
weights by a random or a nonrandom choice. In the spirit of Kolmogorov 
we may call such algorithms probabilistic (randomized) and deterministic 
(non-randomized) algorithms, respectively.
In the present section the deterministic algorithm of Theorem 1.3.1 is 
generalized to infinite classes of directed circuits such that the correspon­
ding denumerable circuit Markov chain £ can be defined as a limit of a 
certain sequence (m£)m of finite circuit chains. The convergence of this 
sequence is weak convergence in the sense of Prohorov, that is, the finite­
dimensional distributions of m£ converge as m — ^ to the corresponding 
ones of £.
The approach we are ready to follow will rely on the idea of circuit gen­
erating equations exposed in Section 1.3. In this direction we shall consider 
denumerable reversible Markov chains which are of bounded degree, that 
is, from each state there are finitely many passages to other states. Then a 
parallel to Tychonov’s theorem for infinite products of compact topological 
spaces can be conceived along with the matching Hall-type theorem for 
infinite bipartite graphs (see P. Hall (1935) and K. Menger (1927)).
The preliminary element will be a stochastic matrix P = (pij, i, j G S) 
on a denumerable set S, that defines a reversible, irreducible, aperiodic, 
and positive-recurrent Markov chain £ = (£n)n, whose invariant probability 

40 
3. Cycle Representations of Recurrent Denumerable Markov Chains
distribution is denoted by n = (ni, i G S). The main theorem is that a cir­
cuit decomposition for P can be given using a deterministic algorithm 
according to which the directed circuits c G C and their weights wc are 
solutions to certain recursive balance equations where the “edge-weights” 
nipij,i,j G S, are used without any probabilistic meaning. The represen­
tative class (C, wc) will be called a deterministic circuit representation of 
£ and of P.
One reason for choosing a deterministic algorithm is that the correspon­
dence P ^ C becomes nearly one-to-one, that is, the class C approximates 
the probabilistic one. It is proved below that the class C may be the limit 
of an increasing sequence nC of finite classes of overlapping directed cir­
cuits. The one-to-one correspondences P ^ C are particularly important 
for plenty of problems arising in various fields. For example, we may refer 
here to the so-called coding problem arising in the context of dynamical 
systems, that in turn leads to the problem of mapping stochastic matrices 
into partitions. A detailed exposition of this argument is given in Section 
3.5 of Part II.
The relation P ^ (C. wc) for transient Markov chains is still an open 
problem and may be connected in particular with certain questions arising 
in network theory. One of them is concerned with the existence of unique 
cycle-currents in infinite resistive networks made up by circuits. Interesting 
results in this direction for edge-networks are due to H. Flanders (1971), 
A.H. Zemanian (1976a, 1991) and P.M. Soardi and W. Woess (1991). For 
instance, Flanders’s condition for a current I to be the unique solution to 
a network-type problem (in the class of all currents with finite energy) 
consists of the existence of a sequence of currents in finite subnetworks 
approaching I.
We begin our investigations by considering a countable set S and a 
stochastic matrix P = (pij ,i,j G S) of bounded degree, that is, for each 
i G S there are finitely many j G S such that pij > 0orpji > 0. Assume P 
defines a reversible, irreducible, and aperiodic Markov chain £ admitting 
an invariant probability distribution n = (ni, i G S), with all ni > 0.
We say £ defines a directed circuit c = (i 1,...,in, i 1) where n > 2, and 
ik = im for distinct k, m < n, if and only if pi 1 i 2 ,pi 2 i 3,.. .pin- 1 in Pini 1 > 0. 
Throughout this section the directed circuits will be considered to have dis­
tinct points (except for the terminals). The irreducibility condition amounts 
to the existence for each pair (i, j ), with i = j , of a directed finite sequence 
a(i, j) connecting i to j, that is,
a (i, j) : i 0 = i, i 1,... ,in = j, n > 1 with ik = im for k = m; k,m < n, 
such that pii1 ...pin-1j >o. 
(3.4.1)
The following property characterizes, in general, irreducibility:
Proposition 3.4.1. Any two points of S are cyclic-edge-connected in S.

3.4 Weak Convergence of Sequences of Circuit Chains 41
Proof. Let i = j .Ifpij > 0, the proof is immediate. Otherwise, there exist 
two directed paths a 1(i,j) and a2(j, i) connecting i to j and j to i, respec­
tively. If j 1 = i, j denotes the first point of a 1 belonging to a2, then there 
exists the directed circuit
c1 = (a1(i,j1),a2(j1,i)),
such that the points j1 and j are mutually connected by the directed paths 
a1 (j1 ,j) and a2 (j, j1 ). By repeating the previous reasonings, we obtain a 
sequence of directed circuits connecting i to j such that any two consecutive 
circuits have at least one common point. 
□
Consider the shortest-length-distance introduced in Section 2.2, that is, 
f0, 
if i = j;
d(i,j) = 
the shortest length n 
(3.4.2)
[ of the paths a(i, j) defined by (3.4.1), if i = j;
where the connections are expressed by the forward-backward passage 
functions introduced by relation (2.2.1). Then, for any finite subgraph of P 
define its diameter as the maximal distance. Since any point of S is cyclic- 
edge-connected with all the others, we may choose an arbitrary point O G S 
as the origin of the spheres S(O,m) of radius m, m =0, 1,... with respect 
to the distance d above.
We are now prepared to prove a deterministic circuit decomposition of 
P following S. Kalpazidou (1993c). As was already mentioned, we are in­
terested in representing the chain £ by a class (C,wc) provided by a de­
terministic algorithm such that the correspondence P — C becomes nearly 
one-to-one, that is, C will approximate the collection of all the circuits oc­
curring along almost all the sample paths. Then the trivial case of the class 
containing only the circuits of period two will be avoided. We have
Theorem 3.4.2. Consider S a denumerable set and £ = (£n)n>o an S- 
state Markov chain which is irreducible, aperiodic, reversible, and positive- 
recurrent. Assume the transition matrix P =(pij,i,j G S) of £ is of 
bounded degree.
Then there exists a sequence (m£)m of finite circuit Markov chains, asso­
ciated with a sequence of deterministic representative classes (m C,mwc)m, 
which converges weakly to £ as m — ^ such that C = limm ,-x_ mC appro­
ximates the collection of all the circuits occurring along the sample paths 
of £. The chain £ becomes a circuit chain with respect to the class (C, wc) 
where
wc = E mwc.
Proof. Consider the balls B(O, n) = kn=0 S(O,n),n=0, 1, Thenfor 
each n and for any i,j G B(O, n) the restriction n£ of £ to the ball B(O, n) 

42 
3. Cycle Representations of Recurrent Denumerable Markov Chains
has the transition probability
Pij = pij / 15^ pij I .
\jBB ( O,n ) 
/
Correspondingly, if n = (ni ,i G S) is the invariant probability distribution 
of £ then that of n£ in B (O, n) is given by the sequence nn = (nni,i G 
B(O, n)) where
nni = I ni 
pij 
\ 
nipi 
nipij I .
\ jEB (O,n) 
/ i 
\i,jBB (O,n) 
)
Put
nPi = ni 
pij, i G B (O,n), n = 1, 2,....
It is to be noticed that if pij > 0 there exists an n0 such that for any n > n0 
we have i, j G B(O,n) and
nPij > n +1 Pij > • • • > Pij, 
0 < nPi < n +1 Pi < ••• < ni, 
such that
nPinPij = n +1 Pin+1 Pij = • • • = niPij. 
(3.4.3)
Since any function nw(i,j) = nninPij,n > 0, is balanced in B(O,n), we can 
appeal to Theorem 1.3.1 and find accordingly a class (nC, nwc) such that
nninPij = 
nWcJc(i,j), i,j G B(O,n), 
(3.4.4)
cEnC
where the Jc is the backward-forward passage function given by (2.2.1). For 
n = 0, the constrained process to B(O, 0) = {O} has an absorbtion state 
O and is represented by the class 0C = {c =(O, O)} where c =(O, O)is 
the loop-circuit at point O and wc =1.
Let us further consider n great enough such that the ball B(O,n) com­
prises all the circuits with periods larger than or equal to some k > 1. 
Applying as above Theorem 1.3.1 to nw(i, j) and B(O,n) we choose a se­
quence nc1, nck1 of circuits such that some of them are the loops in 
B(O, n) and some others are certain circuits of the subgraphs in B(O, n) 
with diameters larger than one. Particularly, we may choose these circuits 
such that they occur along almost all the sample paths of n£.
The irreducibility hypothesis implies that jEB(O ,n) Pbj < 1 for cer­
tain points b G B(O, n). We shall call these points the boundary points of 
B(O, n). On the other hand, since the matrix P is of bounded degree, per­
haps there are points i G B(O, n) which satisfy equation jEB(O,n) Pij =1. 
These points will be called the interior points of B(O, n).

3.4 Weak Convergence of Sequences of Circuit Chains 43
Let us denote n1 = n and
1C = n1C = {n1 c 1,..., n1 Ck 1}.
Then (3.4.4) becomes
n1 nn1 pij 
52 n1 wJc(i,j), 
i,j B B(O,n 1). 
(3.4.5)
cE1C
For each boundary point b B B(O,n 1) there is a point j / B(O,n 1) such 
that pbj > 0. Let n2 >n1 such that all the boundary points of B(O, n1) 
will become interior points in B(O, n2).
Put
n2w(i, j) = n2nn2Pij = (nipij)/ 
^2 
niPij, 
i,j B B(O,n2).
i,jEB(O,n2)
Note that because of (3.4.3) both n1 w(•, •) and n2w(•, •) attain their mini­
mum over the Arcset of c 1 = n1 c 1 at the same edge, say (i 1 ,j 1), that is,
n1 wc 1 = n1 w(i 1, j 1) = min n1 w(i, j), 
c1
n2 w(i1 ,j1 ) = minn2w(i,j).
c1
The latter equations enable us to choose n2 c1 = n1 c1 = c1 and n2 wc1 = 
n2w(i1, j1). We have
n1 wc 1 > n2 wc 1 > ni 1 Pi 1 j 1 > 0.
Further put
n2w 1(i, j) = n2w(i, j) - n2wc 1 Jc 1 (i, j), i,j B B(O, n2).
Then n2w 1(i 1 ,j 1) = 0 and the function n2w 1(•, •) is also balanced in 
B(O,n2).
Appealing to the algorithm of Theorem 1.3.1 in B(O, n2), we find an 
edge (i2,j2) of c2 = n1 c2 (n1 = n) where both n1w1 and n2w1 attain their 
minimum, that is,
n1 wc2 =n1 w1(i2,j2) = min n1w1(i, j) 
c2
I 1/ I 5 ' 
nipij I I (ni 2 pi 2 j 2 — ni 1 pi 1 j 1 Jc 1 (i 2 ,j 2)),
i,jEB(O,n1)
and
n2w1(i2,j2) = min n2 w1(i, j) 
c2
= V 12 
niPij I 
( ni 2 Pi 2 j 2 - ni 1 Pi 1 j 1 Jc 1 ( i 2 ,j 2)) •
i,jEB(O,n2)

44 
3. Cycle Representations of Recurrent Denumerable Markov Chains
Then we may choose n2c2 = c2 and n2wc2 = n2w 1(i2,j2). Hence
n1 wc> > nwwc2 > nppijj- - nipiiji Jci (i2,j2) > 0.
Repeating the same reasonings above, we conclude that all the circuits in 
B(O, n1) are circuits in B(O, n2) as well, that is,
ni c1 = n2c1 = c1 ,
ni c2 = n2c2 = c2,
ni cki = n2 cki = cki .
Then the n2w(i, j) is decomposed in B(O,n2) by a class (2C, n2wc) where 
2C= {c1,...,cki,cki+1,...,ck2}, k2 >k1,
may particularly contain circuits which occur along the sample paths of 
the restriction n2£ of £ to B(O, n2).
Hence
nww(i,j) = n2ni n2pij = £ n2WcJc(i,j).
Continuing the previous reasonings, we shall find a sequence {sC}s> 1 of 
finite classes of directed circuits which is increasing. Then there exists the 
limiting class
C = lim sC = {c1,c2,...,cki,...}. 
s—>^>
On the other hand, for any circuit c G C, we find a sequence {ns wc}s> 1 
of positive numbers which is decreasing, and so convergent to a number 
wc G [0, 1], that is, lims .-x_- ns wc = wc. Moreover, there is some a > 1 such 
that c G a C. Then
ns wc > nir Pir jr - ^ 
Pj JCk ( ir ,jr ) > 0,
k=1
for all s > a and some i 1,... ,ir and j 1,... ,jr where r = 1,..., ka. Thus, 
wc > 0, for all c G C.
Now consider any i, j in S such that pij > 0. Then there exists a > 1 
such that i, j are interior points of B (O, na) and (i, j) G Arcset ° C. Hence
ka
n^ w ( i,j ) = na ni na Pij 
na wcr Jcr (i,j ) ,
r=1
and
ka
nsw(i,j) = 
ns wcr Jcr (i, j), for all s > a.
r=1

3.5 Weak Convergence of Sequences of Circuit Chains 45
Finally, we have
niPij = lim ns nnpij 
s—>^>
ka
= s1—m ^2 ns Wcr Jcr ( i,j ) 
r=1
ka
= 
wcr Jcr (i, j)
r=1
= 
wcJc(i, j).
ctC
The proof is complete. 
□
Remark. As was shown in the previous proof, there is a definite algebraic- 
topological property of a directed circuit c =(i1,...,is,i1) defined by 
w(i, j) = nipij,i,j e S. Namely, we have
Lemma 3.4.3. Let f1 and f2 be two positive functions defined on S2 .In 
order that equations
f1(i,j) = 
f2(j,i), 
ie S,
be circuit-generating ones it is necessary that for some i1 ,...,is e S the 
inequalities
f 1(i 1, i2) f 1(i2,i3) • • • f 1(is-1, is) f 1(is, i 1) > 0, 
f2( i 1, i 2) f2( i 2 ,i 3) • • • f2( is-1, is) f2( is,i 1) > 0,
imply each other.
3.5 Weak Convergence of Sequences of Circuit 
Chains: A Probabilistic Approach
A denumerable reversible positive-recurrent Markov chain is a weak limit 
of finite circuit Markov chains whose representative circuits and weights are 
algorithmically given according to Theorem 3.4.2. It might be interesting to 
investigate the same asymptotics when the representatives enjoy probabilis­
tic interpretations. For instance, we may consider that the cycle-weights are 
provided by the probabilistic algorithm of Theorem 3.3.1. In this section 
we give a more detailed argument following S. Kalpazidou (1992a, b, e) 
and Y. Derriennic (1993).
Consider S a denumerable set and £ = (^n) n> 0 an irreducible 
and positive-recurrent Markov chain (not necessarily reversible) whose 
transition matrix and invariant probability distribution are, respectively, 

46 
3. Cycle Representations of Recurrent Denumerable Markov Chains
P = (Pij,i,j G S) and n = (ni,i G S). Let (£m(w))m>0 be a sample path 
of £ and let n be any positive integer chosen to be a sufficiently great 
number. Put
Cn(w) = the collection of all circuits with distinct points 
(except for the terminals) occurring along (£m(w))m 
until time n;
Sn(w) = the set of the points of Cn (w).
Throughout this section the circuits will be considered to have distinct 
points (except for the terminals).
Consider
wc,n (w) = the number of occurrences of the circuit c along 
(£m (w))m up to time n,
and the functions
wn (i,j )= n Wn (i,j ) 
^2 (Wc,n (“)/n) Jc(i,j ),
wn (i )= n Wn (i ) 
^2 (Wc,n (“ ) /n) Jc ( i),
for all i,j G Sn = Sn (w). Since the constrained passage-function Jc(•, •), 
with c G Cn (w), to the set Sn is still balanced, the function wn (•, •) does 
as well. Therefore the collection {wn(i), i G Sn} plays the role of an invari­
ant measure for the stochastic matrix nP = nPn = (nwn(i, j)/ nwn(i), i,j G 
Sn), n =1, 2,......
Accordingly, we may consider a sequence (n£)n of Markov chains n£ = 
2£ = {n£m, m = 1, 2,...} whose transition probabilities in Sn are defined 
as
(n wn (i, j)) / (n wn (i)), if (i, j) is an edge of a circuit in Cn (w);
0, 
otherwise.
Put
nni = nn = cn(w) nwn(i), 
i G Sn,
where cn(w) = 1/( i wwn(i)).
It is to be noticed that, since
nninpij = cn (w)n wn (i, j) = (npij)/ 
nipij
\i,jssn
the above chain n£, n =1, 2,... , is not the restriction of £ to Sn . So, the 
investigations up to this point disclose differences between the weak conver­
gence of (n£), as n ^ <x, and that of deterministic circuit representations

3.6 The Induced Circuit Chain 47
occurring in Theorem 3.4.2. It is the following theorem that shows a spe­
cial nature of the weak convergence of (n£) to £, as n — ^ (S, Kalpazidou 
(1992e)).
Namely, we have
Theorem 3.5.1. For almost all w the sequence (n£)n converges weakly, as 
n ^ ^. to the chain £. Moreover the sequence of the circuit representations 
associated with (n£)n converges, as n ^ <x, to the probabilistic circuit rep­
resentation (C, wc) of £, where C is the collection of the directed circuits 
occurring along almost all the sample paths.
Proof. First note that we can regard the process n£ in Sn as a circuit chain 
with respect to the collection (Cn(w), wc,n(w)/n). Accordingly, we have 
nninPij = n ni nPij = cn (w) 52 (wc,n (~ )/n) Jc (i,j ),
when (i, j) is an edge of a circuit of Cn(w), where Jc(i, j) = 1 or 0 according 
to whether or not (i, j)isanedgeofc. Then, as in Theorem 3. 2.1 we may 
find a limiting class (C, wc) defined as
C = lim Cn(w), a.s., 
n>^>
Wc = nl—I' (wc,n(w)/n), 
a.s.
The equations (3.3.2) and the same argument of Theorem 3.3.1 enables us 
to write
nipij = lim nninpij a.s. 
n—'
= 
wcJc (i, j),
ctC
since limn—' cn(w) = 1 a.s.
(Here we have replaced the index-set C' , which contains all the cycles, 
in Definition 3.2.2 of the circulation distribution by the set C of the corre­
sponding circuits.) This completes the proof. 
□
3.6 The Induced Circuit Chain
Y. Derriennic (1993) has defined the denumerable circuit Markov chains as 
limits of weakly convergent sequences of induced chains. In particular, it 
is seen that the induced chain of a circuit chain is a new type of “circuit 
chain.”
To this direction, let S be any denumerable set and let £ = (£n)n be 
an S-state irreducible and positive-recurrent Markov chain defined on a 
probability space (Q, F, P). For a given nonvoid subset A of S, the induced 
chain of £ on the set A, denoted by A£, is the Markov chain whose transition 

48 
3. Cycle Representations of Recurrent Denumerable Markov Chains
probabilities APij,i,j G A, are defined as follows:
Apij = P(£ enters first A at state j, if £ starts at i) 
o / 
\
= I 
pij 1 pj 1 j2 . . .pjn- 1 j I .
n =1 \ ji,...,jn- 1 es\A 
/
Therefore, the induced transition probability Apij ,i,j G A, is the expected 
number of times that the Markov chain £ is in the state j before being in 
the set S/A, given that £ starts from the state i:
oo
Apij = 
Api(jn), 
i,jG A, 
(3.6.1)
n=0
— O onf] . 
r>~> — 1 O 
ic the <n cfcri tvoneitiAn km'/'aPaqPai 1
wneiv APij — o, anti APij , n — 1, 2, . . . , is t^-^e n ste^y transition -p.Lvoi_)ai_).niLy 
with taboo set of states A, that is,
APi n) = P ( £n ( w )— j,£n-1( w ) G A, £n-2( w ) G A,...,£ 1( w ) G A/£ 0( w ) = i ).
We have
Proposition 3.6.1. If £ — (£n)n is a positive-recurrent Markov chain then 
AP —(APij,i,j G A) is a stochastic matrix.
Proof. Following Chung’s Theorem 3 (1967, p. 45) when j G A we have
(n) 
(n) 
(n)
A Pj < j Pj 
= fij — P (£n (^) = j, £n-1 = j,..., £ 1 = jl£ 0( ^) = i) < 1.
Hence APij < jPij — fij < 1, where fij — fij'), i,j G A.
Also, if £ is positive-recurrent then fii — 
fi(in) — 1, for any i G A. There­
fore
APi APij — yy P(£ enters first A at state j/£0(w) — i) 
jeA 
jeA
— P(£ enters first A at time n/£0 (w)—i) 
n> 1
—P
> P
{£n G A}/£0 (w)—iI
{£n — i}/£0(w) — i 
— fii — 1.
The proof is complete.
□

3.6 The Induced Circuit Chain 49
Furthermore, we prove
Proposition 3.6.2. If £ is an irreducible and positive-recurrent Markov 
chain, then A£ is irreducible.
Proof. We first write
Apij = pij + E. 
^\ pij1 pj1j2 ...pjn-1j , 
(3.6.2)
for any i,j G A. Then
Apij > 0 
(3.6.3)
if either pij > 0, or there are j 1,... ,jn- 1 G S\A, n > 2, such that 
pij1 pj1 j2 . . . pjn-1 j > 0.
To prove that the induced Markov chain A£ is irreducible, we have to show 
that for any pair (i,j) G A x A either
(i)
Apij > 0, 
(3.6.4)
(ii) or, there exist k1,...,km G A, m > 1, such that
Apik1 Apk1 k2 . . . Apkm j > 0.
So, let us consider an arbitrary pair (i, j) of states in A. Then irreducibility 
of £ allows us to write that either pij > 0, or, there exist k1 ,. ..,km G 
S, m > 1, such that
pik1 pk1k2 ...pkmj > 0. 
(3.6.5)
If pij > 0 then Apij > 0. Otherwise we may distinguish the following cases:
Case 1: Relations (3.6.5) hold with all k1,...,km G A. Then, according to 
(3.6.2), we have
A pik1 Apk1k2 ... Apkmj > 0,
and therefore relation (3.6.4)(ii) holds.
Case 2: Relations (3.6.5) hold with all k1,...,km G S\A. Then by using 
(3.6.3), we have
pij1 pj1j2 . . . pjn-1j > 0,
ji,...,jn-1 es\A
with n = m + 1 and for j1 = k1 ,...,jn-1 = km . Accordingly,
Apij = 
pij1 pj1j2 ...pjn-1j > 0,
n> 1 ji,...,jn-1 eS\A
and relation (3.6.4)(i) holds.

50 
3. Cycle Representations of Recurrent Denumerable Markov Chains
Case 3: Relations (3.6.5) hold with some kt G S\A and some others 
mk G A.
For the sake of simplicity, let us consider all k1 ,. ..,km G S\A except for 
some kt G A, 1 < t < m. Then i, kt,j G A and we may apply case 1 to the 
pairs (i, kt), (kt, j) of states in A. Hence
Apikt Apkt j > 0,
and relation (3.6.4)(ii) holds for m =2.
Next, if k1 ,...,km G S\A except for some kt, kt+s G A,1< t, t + s < m, 
then we may apply again case 1 to the pairs (i, kt), (kt, kt+s), and (kt+s, j) 
of states in A. Accordingly, we get
Apikt Apkt kt+s Apkt+s j > 0,
and relation (3.6.4)(ii) holds for m = 3. Finally, case 3 may be extended 
for general situations m > 3 by repeating the previous reasonings. Then, 
we conclude that the irreducibility of the original chain £ implies the same 
property for a£. The proof is complete. 
□
Now we are prepared to prove the following:
Theorem 3.6.3. Let S be a denumerable set and let £ = (£n)n>o be an 
S -state irreducible and positive-recurrent Markov chain. Then there exists 
a sequence (nn)n of finite induced circuit chains, which converges weakly to 
£, as n ^ x.
Proof. Let (An)n be an increasing sequence of finite subsets of S such that 
lim An = S, as n — ^. Then 1 n, 2n,■ ■ ■, nn,■■■ are taken to be the induced 
chain of £ on A 1, A2,..., An,... , that is, nn = An£,n = 1, 2,■■■. Then, 
following Propositions 3.6.1 and 3.6.2, any induced chain nn,n =1, 2,..., 
is an irreducible finite Markov chain. Therefore, the induced transition 
probability An pij of any nn accepts a circuit representation {Cn, wcn }, 
that is,
wcnJcn(i, j)
An Pij = cn^CCn--------------—, i,j G An,n =1,2,...,
n 
wcn Jcn (i)
cn G Cn
where Jc(i,j) = 1 or 0 according to whether or not (i,j) is an edge of c, 
and (nn)n converges weakly to £. The proof is complete. 
□
Further, it will be interesting to define a natural procedure of inducing 
a circuit representation {CA, wA} for the induced chain A£ on the finite 
subset A C S, starting from an original circuit representation C of £.
(n)
Note that, Apij > 0 if and only if Apij > 0, for certain n =1, 2,......Then
a natural procedure of inducing the circuits of C into A is due to Derriennic 

3.6 The Induced Circuit Chain 51
(1993) and consists in the following: any circuit c = (i 1, i 2,... ,is,i 1) G C, 
which contains at least one point in A may induce a circuit cA in A as the 
track of the remaining points of c in A, written with the same order and 
cyclically, after discarding the points of c which do not belong to A. In this 
manner the representative collection C of directed circuits in S determines 
a finite collection CA = {c1,c2,...,cN} of induced circuits into the finite 
subset A C S.
Furthermore, by choosing suitably the circuits in C, we may use the 
induced circuits c1 ,.. .,cN of CA to partition the original collection C into 
the subcollections C0,C1,...,CN defined as
Ck = {c G C: c induces the circuit ck in A}, k=1,...,N,
C0 = {c G C: c induces no circuit in A},
such that no circuit of C0 passes through A. 
Then
(
N \
Ck . 
(3.6.6)
k=1
Let us now consider a collection of circuit-weights {wc} which decomposes 
£, that is,
P(£n = i, £n +1 = j ^2 wc Jc(i,j), 
i,j G S, (3.6.7)
c C
for any n =0, 1, . . .
Then we may decompose the induced transition probability Apij by using 
the induced circuits of CA . Specifically, we may write
(2) 
(n)
APij = Pij + APij + ' ' ' + APij + ' ' '
= P R1 = j,£ 0 = i}
P R 0 = i}
, PR2 = j,£ 1 G S\A,£o =i} ,
+ 
P 
+ 
'
P{£n=j,£n-1 GS\A,...,£1 G S\A,£0=i}
+ 
m=} 
+'"
The denumerator P (£0 = i),i G A, occurring in the expression of Apij is 
decomposed by the representative class CA as follows:
_ N _
P(£0 = i)= 
wcJc(i) = 
wcJc(i)
N _
= X1 ( 
wc ) JCk (i) ,

52 
3. Cycle Representations of Recurrent Denumerable Markov Chains
where {wc, c G C} are the weights occurring in 3.6.7. Then by defining the 
“induced” circuit-weights vcA ,ca G Ca , as
Vck = yy wc, 
k=1 ,...,n,
cE Ck
we have
P(%0=i)=yy vck, Jck (i), 
i g A.
k=1
Let us now calculate the numerator of Apij ,i,j G A, in terms of CA :
P(%i=j,£o=i) + y^ P(£2=j,£i=ji ,£o=i) 
j i es\A
+ 12 
P (£3 = j,£ 2 = j 2 ,£ 1 = j 1 ,£ 0 = i )
j1 j2 ES\A
+------- + y^ 
P ( £n = tin- 1 = jn-1 ,■■■,€ 1 = j 1,£ 0 = i )
j1 ,...,jn-1 ES\A
+ •••
We have
P(£0 = i, £1 = j)= 
wcJc(i, j)
cEC
A /_ 
\
= 
wcJc (i,j) Jck (i, j)
k=1 cECk
N
=yy1 vck (i,j) Jck (i,j), 
k=1
2w(i,j) = 
2Vck Jck(i,j), 
i,jG A.
k=1
for any i, j G A, where
1 Vck (i,j 
y^ wcJc(i,j), 
i,j g A.
Let
2w(i,j) 
y^ P(£2=j,£ 1=j 1,£o=i), 
i,j gA.
j1 ES\A
Then, if £ is reversible then 2w(i, j) is symmetric. Also, 2w(i, j) > 0 implies 
Apij > 0. Accordingly, the representative circuits of 2w will belong to CA, 
and we may find 2 vck > 0, k = 1,... ,N, such that

3.6 The Induced Circuit Chain 53
By repeating the same reasoning for any
nw (i,j ) =52 P (£n = j,£n-1 = in— 1 ’■■■,£ 1 = i 1 ,£ 0 = i 0),
j 1 ,...,jn-1 eS\A
where i, j G A, n = 3, 4,... we may find nvck > 0, k = 1,..., N, such that
nw(i,j) 52 nvck Jck (i,j), 
i,j G A.
k=1
Then the numerator of Apij is given by
N
V>2i'cj ( i,j ) Jck ( i,j ) 
k=1
where
vCk (i,j) = 1 vCk (i,j) + !>Ck
with
-ck = £ nvck, 
k =1 ,...,N.
n> 2
Therefore,
^pij = EN=1 NVck(i,j)Jck(i,j), 
i,j G A. 
(3.6.8)
Ek=1 vck Jck(i)
In conclusion, when the positive-recurrent chain £ is irreducible then the 
induced chain A£ is also irreducible with respect to the invariant probability 
distribution An = (ani, i G A) defined as
Ani =
VcA 
VcA JCA (i )
cA^CA
E p (cA ) vcA '
cA^CA
iG A,
where p(cA ) denotes as always the period of the circuit cA in A.
Finally, if £ is reversible then A£ is also reversible and the corresponding 
induced transition probability Apij admits a “circuit representation” given 
by (3.6.8).

4
Circuit Representations of Finite
Recurrent Markov Chains
In Chapter 2 we have investigated the genesis of finite Markov chains from 
a collection {C, wc} of directed circuits and positive numbers. We are now 
interested in the inverse problem: find a class {C, wc} of directed circuits 
(or cycles) c and positive numbers wc which can describe by either linear or 
convex expressions the transition probabilities of two finite Markov chains 
£ and x, with reversed parameter-scale and admitting a common invariant 
probability distribution. The solutions {C,wc} to this problem will be called 
the circuit (cycle) representation of £. In addition, the class {C, wc} will be 
called either “probabilistic” or “deterministic” (“nonrandomized”) accord­
ing to whether or not the circuits and their weights enjoy or do not enjoy 
probabilistic interpretations in terms of the chain £.
The present chapter deals with both probabilistic and deterministic ap­
proaches to the circuit representations of finite recurrent Markov chains. 
The probabilistic circuit representation relies on an algorithm whose solu­
tion (of circuits and weights) is uniquely determined under a probabilistic 
interpretation in terms of the sample paths.
The deterministic circuit representations will be investigated by three 
different approaches. The first uses a combinatorial algorithm, having more 
than one solution, which was originated by J. MacQueen (1981) for a single 
chain £, and by S. Kalpazidou (1987b, 1988a) for a pair (£, x) of chains 
as above. The second deterministic approach to the circuit representation 
problem above belongs to Convex Analysis and arises as a corollary of the 
Caratheodory dimensional theorem. Finally, the third deterministic setting 
to the same problem relies on algebraic-topological considerations, and is 

56 
4. Circuit Representations of Finite Recurrent Markov Chains
due to S. Kalpazidou (1995). Plenty of other circuit decompositions can be 
found if we combine the previous approaches.
An important question associated with the above circuit representations 
and with further considerable applications (for instance, to the rotational 
representations exposed in Chapter 3 of Part II) consists of the estimation of 
the number S of the representative circuits of C. According to the context 
that we shall use, we shall give two estimations for S. One, of algebraic 
nature, will be provided by the Caratheodory dimension, while the other 
will be a homologic number identified as the Betti dimension of the space 
of one-cycles associated with the graph of the transition matrix.
The results presented in this chapter will then argue for a version of 
the existence theorem of Kolmogorov for finite recurrent Markov chains 
in terms of the weighted circuits, and will establish a general connection 
between cycle theory and Markov-chain theory.
4.1 
Circuit Representations by Probabilistic 
Algorithms
A randomized algorithm to a circuit decomposition can be furnished by 
the sample-path-approach of Theorem 3.2.1 specialized to finite recurrent 
Markov chains. Then a circuit decomposition can be directly proved us­
ing the sample equations (3.3.2) where the sample circuits are always un­
derstood to have distinct points except for the terminals (S. Kalpazidou 
(1992e)). Namely, we have
Theorem 4.1.1. (The Probabilistic Circuit Representation). Let S be a 
finite set. Then any stochastic matrix P = (pij ,i,j G S) defining an ir­
reducible Markov chain £ is decomposed by the circulation distribution 
{wc}cec as follows:
nipij 
^2 wJc(i,j), 
i,j G S, 
(4.1.1)
c C
where n = (ni, j G S) denotes the invariant probability distribution of P, C 
is the collection of all the directed circuits c occurring along almost all the 
sample paths, and Jc is the (second-order) passage function associated with 
c. The above circuit-weights wc are unique, with the probabilistic interpre­
tation provided by Theorem 3.2.1, and independent of the ordering of C.
If P defines a recurrent Markov chain, then a similar decomposition to 
(4.1.1) holds, except for a constant, on each recurrent class.
Proof. Let an (•; i,j) ,i,j G S, be the function
an(&; i, j) = -card {m < n: £m-1(w) = i,£m (w) = j} .

4.2 Circuit Representations by Nonrandomized Algorithms 57
Consider nC(w) as the collection of all the directed circuits c occurring 
up to n along the sample path (in(w))n, and wc,n(w) as the number of 
the appearances of the circuit c along the same sample path. A circuit 
c = (i 1, i2,..., ir, i 1), r > 2, with distinct points, except for the terminals, 
occurs along (in(w))n when the chain passes through i 1 ,i2,...,ir,i 1 (or 
any cyclic permutation). Further, the revealing equations will be
and
'- (w; i,j 
^2 (Wc,n (w)/n) Jc (i,j)
+ £n(w; i,j)/n. 
(4.1.2)
where
1, if (i, j)isanedgeofc, 
c , j = 0, otherwise,
{the last occurrence of (i,j) does not happen 
together with the occurrence of a circuit of n C(w) }
n (w; i,j) = 1
(w)
(4.1.3)
for all i, j G S. Then the decomposition (4.1.1) follows from (4.1.2) when 
taking the limit as n — ^, and applying Theorem 3.2.1.
Finally, if P defines a recurrent Markov chain on S, then there is a 
probability row-distribution n = (ni, i G S) such that all ni > 0 and nP = 
n. Then, the proof is completed by using the same approach above adapted 
to each recurrent class. 
□
4.2 
Circuit Representations by Nonrandomized 
Algorithms
One nonprobabilistic approach to the problem of representing Markov 
chains by weighted directed circuits reduces to solving a suitable system 
of cycle generating equations, introduced by Theorem 1.3.1, in terms of 
the entries of a recurrent stochastic matrix. (A recurrent stochastic matrix 
P is any matrix defining a recurrent Markov chain. This is equivalent to 
the existence of a probability row-vector v>0 satisfying vP = v.) Now, we 
shall give a detailed argument for this, following S. Kalpazidou (1988a).
Let Z denote, as always, the set of all the integers. We now prove
Theorem 4.2.1. (The Deterministic Circuit Representation). Let S be a 
nonvoid finite set. Consider (i, x) a pair of homogeneous recurrent S-state 
Markov chains defined on a probability space (Q, K, P), with the common 
invariant probability distribution n = (ni,i G S), such that equation
P(in +1 = i/in = j ) = P(Xn = i/Xn +1 = j) 
(4.2.1)
holds for all n G Z and i, j G S.

58 
4. Circuit Representations of Finite Recurrent Markov Chains
Then there exist two finite ordered classes C and C- = {c- : c- is the 
reversed circuit of c,c G C} of directed circuits in S and two ordered sets 
{wc, c G C} and {wc_ , c- G C-} of strictly positive numbers, depending on 
the ordering of C and C- and with wc- = wc, such that
P(Cn = i/Zn-1 = j ) = w (j,i) /w (j ), i,j G S,
P( Xn = i/Xn +1 = j ) = w- (i,j )/w- (j ), i,j G S,
for all n G Z, where
w(j, i)= 
wcJc(j, i),
w-(i,j) = 
wc- Jc- (i, j),
w(j) = 
wcJc(j),
ceC
w-(j) = 
wc- Jc- (j),
c- eC-
and Jc(j, i), Jc(j), Jc- (i,j), and Jc- (j) denote the second-order and the 
first-order passage functions associated with c and c-, respectively.
Proof. Consider first the case of two irreducible chains Z and X.IfP = 
(pji, j, i G S)andP - = (pi-j, i, j G S) denote the transition matrices of Z 
and X, define
w (j, i ) = njpji, j,i G S,
w-(i,j) = njPj-i, j,i G S.
Then, letting
w (j) ^2 w(j,i),
w- (j) ^2 w- (i,j),
we obtain
w (j )= w- (j )= nj, j G S.
Therefore
pji = w(j, i)/w(j), 
pj-i = w-(i, j)/w-(j),
(4.2.2)
(4.2.3)
(4.2.4)
for any j, i G S .

4.2 Circuit Representations by Nonrandomized Algorithms 59
From equations (4.2.1), since w(j) = w-(j) for all j G S, we obtain that 
w(j, i)=w- (i, j),
for any j, i G S. Then we may apply the algorithm of Theorem 1.3.1 to the 
balanced functions w(•, •) and w- (•, •) defined by (4.2.2) and (4.2.3). Thus, 
there exist two finite ordered classes C and C- = {c- : c- is the reversed 
circuit of c, c G C} of directed circuits in S (with distinct points except for 
the terminals) and positive weights wc and wc- ,c G C, with wc = wc- , such 
that
w(j,i) = 
wcJc(j, i),
w- (i,j)= ZL wc— Jc- (i,j),
for any j, i G S , where
Jc(i,j) = 
10,,
if (i, j)isanedgeofc, 
otherwise.
The algorithm of Theorem 1.3.1 shows that the definitions of the weights wc 
depend on the chosen ordering of the circuits of C. Furthermore, relations 
(4.2.4) become
pji = 
wcJc(j, i) 
wcJc(j) 
.
\ctc 
) 
\e& 
)
pji = I 
we- Jc- (i,j) I I I y? we- Jc- (j) I ,
\c_tc_ 
c \c_ ec- 
I
for any j, i G S .
Now, let us consider that £ has more than one recurrent class E in S. 
Then, the previous proof can be repeated for the balanced function w( j, i) = 
EaE aEnE (j)Pji,j,i G S, instead of w(j, i) given by (4.2.2), where nE = 
(nE(i)) (with nE(i) > 0, for i G E, and nE(i) = 0 outside E) is the invariant 
probability distribution associated with each recurrent class E, and aE is 
a positive number assigned to E.
Reasoning analogously for the chain x and choosing the class {C, wc- } 
as in Theorem 1.3.1, the proof is complete. 
□
The ordered collections {C, wc } and {C- ,wc- } occurring in Theorem 
4.2.1 are called the deterministic representative classes of £ and x, and of 
the corresponding transition matrices P and P- . Accordingly, the algo­
rithmic genesis of the circuits and weights, without any probabilistic inter­
pretation, motivates the name deterministic for the corresponding circuit 

60 
4. Circuit Representations of Finite Recurrent Markov Chains
decomposition of P:
niPij = 
wJc(i,j), 
Wc > 0, i,j e S, 
(4.2.5)
ctC
where the circuit-weights wc depend on the chosen ordering in C.
4.3 
The Caratheodory-Type Circuit 
Representations
An estimation of the number of the representatives in a circuit decom­
position of a finite recurrent stochastic matrix can be given using convex 
analysis. Alpern (1983) showed that a natural way to achieve this is to 
appeal to the celebrated Caratheodory convex decomposition when char­
acterizing the dimension of a convex hull in a finite-dimensional Euclidean 
space (see R.T. Rockafeller (1972), J.R. Reay (1965), V.L. Klee (1951)­
(1959)). Caratheodory’s dimensional theorem asserts the following: if M is 
a set in Rn, then any element of the convex hull of M can be written as 
convex combinations of (n + 1)-elements of M.
In this direction let n> 1 be any natural number and let S = 
(1, 2,...,n}. Let further P =(pij,i,j e S) be any stochastic matrix defin­
ing an S-state homogeneous recurrent Markov chain £ = (£m, m > 0) whose 
invariant probability distribution is denoted by n = (ni,i e S). In S. 
Kalpazidou (1994b, 1995) it is shown the connection of the decomposi­
tions of P in terms of the passage functions with the convex Caratheodory- 
type decompositions. Namely, to relate the decomposition (4.2.5) to a 
Caratheodory-type decomposition we first point out that the coefficients 
wc do not sum to unity. However, we may overcome this inconvenience by 
“normalizing” the passage function Jc into Jc(i, j) = (1 /p(c)) Jc(i,j), where 
p(c) denotes as always the period of c.
Then, considering representative circuits with distinct points (except for 
the terminals), we have
Jc (i, j)=
1/p(c), 
0,
if (i, j)isanedgeofc; 
otherwise.
The matrix (Jc(i, j), i, j e S) is called the circuit (cycle) matrix associated 
with c. Then, taking Wc = p(c)wc, we have
niPij = 
WcJc(i,j), Wc > 0, 
y^Wc = 1, i,j G S- (4.3.1)
On the other hand, viewing the normalized passage functions {Jc} as the 
extreme points of a convex set in an (n2 - n)-dimensional Euclidean space, 
we may write the following Caratheodory-type decomposition
N 
N
niPij = yy "Wck Jck (i,j), 
"Wck > 0, y^Wck = 1, i,j e S, (4.3.2) 

4.4 The Betti Number of a Markov Chain 61
where C = {c 1,..., cN}, with N < (n2 — n) + 1, is an ordered collection of 
directed circuits with distinct points except for the terminals.
We call the ordered class (C, wc) occurring in the decomposition (4.3.2) 
the Caratheodory-type representation of P and of £. Furthermore, equations 
(4.3.2) are called the Caratheodory-type decomposition of P.
4.4 
The Betti Number of a Markov Chain
In the next section we shall investigate a more refined dimension than that 
of Caratheodory occurring in the decomposition (4.3.2). Since our approach 
will arise from algebraic-topological reasonings, we shall introduce in the 
present section a few basic homologic concepts following S. Kalpazidou 
(1995).
The primary element of our exposition will be the strongly connected 
oriented graph G = G(P ) of an irreducible finite stochastic matrix P.A 
stochastic matrix P is called irreducible if for any row i and any column 
j = i there exists a positive integer k, which may depend on i and j, such 
that the (i, j)-entry of Pk is not zero. In general, one can dissociate the 
graph from any matrix, in which case the concepts below are related to the 
graph alone.
Let G = (B0,B1) be a finite strongly connected oriented graph, where 
B0 = {n 1,..., nT0} and B 1 = {b 1,... ,bT 1} denote, respectively, the nodes 
and directed edges. The orientation of G means that each edge bj is an 
ordered pair (nh, nk) of points, that is, bj is assigned with two points 
(terminals) nh, nk , where nh is the initial point, and nk is the endpoint. 
When nh = nk , we may choose any direction for the corresponding edge. 
To each edge bj of G with distinct terminals nh and nk as above, we 
may associate an ordered pair with initial point nk and endpoint nh .De­
note this pair —bj . Then —bj may occur or may not occur in the original 
graph G.
Strong connectedness will mean that for any two points ni and nj there 
exist an oriented polygonal line in G from ni to nj and an oriented polyg­
onal line from nj to ni . A polygonal line L of an oriented graph G is a 
subgraph given by a finite sequence, say, b1,...,bm,m > 1, of edges of G, 
eventually with different orientation, such that consecutive edges bk , bk+1 
have a common terminal point and no edge appears more than once in it. 
Accordingly, we shall write L = {b1,...,bm}.Theneachofb1 and bm will 
have a free terminal. When these free terminals of b1 and bm coincide and 
all the points of L are distinct from each other, then the polygonal line L is 
called a loop. Then circuit-edge-connectedness introduced at paragraphs 2.1 
and 2.2 it is usually called strong connectedness (see C. Berge (1970), p. 25).
A polygonal line {b1,...,bm}, where for each k =1,...,m— 1 the com­
mon point of bk and bk+1 is the endpoint of bk and the initial point of 
bk+1 , is called an oriented polygonal line from the initial point of b1 to the 
endpoint of bm . Both B0 and B1 can be viewed as the bases of two real 

62 
4. Circuit Representations of Finite Recurrent Markov Chains
vector spaces C0 and C 1. Then any two elements c0 G C0 and c 1 G C 1 have 
the following formal expressions:
J 0,
c0 
^2 Xhnh = XZn, 
Xh G R,
k=1
r 1
c 1 
yk bk = yzb, 
yk e r,
k=1
where, by convention, yk(-bk) = -ykbk for all (-bk) (with distinct termi­
nals) which do not belong to B1 ,andR denotes the set of reals. The ele­
ments of C1 are called one-chains. The orienting process described by the 
edges bj determines a formal boundary relation 6 defined as Sbj = nk — nh if 
nh and nk are the initial point and the endpoint of the edge bj . To express 
b in the general form and in vector space setting we need the incidence 
matrix n = (n edge, point) = (nbj ns ,bj G B 1 ,ns G B 0) of the graph G which 
is defined as:
{
+1, if ns is the endpoint of the edge bj;
—1, if ns is the initial point of the edge bj ;
0, otherwise.
Then
T0 
bbj = 
nbjnsns.
s=1
One can extend b to the whole space C1 as a linear transformation by the 
relation
6y'b = y'yn.
Let
— -
C1 = Ker 6 = {z G C1 : Zn = 0},
where 0 is the neutral element of C 1. The vectors of C 1 are called one-cycles.
As we have already seen in Chapter 1, a directed circuit-function c in 
B0 is completely determined by a natural number p > 1 and a sequence 
of p ordered pairs (ns1, ns2), (ns2,ns3),...,(nsp,nsp+1) with ns1 = nsp+1. 
Throughout this section we shall consider circuit-functions c with distinct 
points ns1 ,...,nsp , and the corresponding graphs will be called directed 
circuits. The latter will be symbolized by c as well. Then any c is an oriented 
loop (see the definition of the oriented polygonal line above).
Consider now any directed circuit c of the graph G, with distinct points 
except for the terminals, given by a sequence, say, b1 ,...,bk of directed 
edges of B 1. Then c may be assigned to a vector c G C 1 defined as follows:
c =1 -b 1 + ••• + 1 - bk + 
0 .bi.
Since 6c = 0, c G C1.

4.4 The Betti Number of a Markov Chain 63
Notation. For the sake of simplicity, the one-cycle attached as above to a 
directed circuit c (with distinct points except for the terminals) of G will 
be denoted by c as well.
In general, when there are edges (-bj) which do not belong to B1, 
one may assign any directed circuit c whose edge-set is a subset of 
B 1 U {(—bj) G B0 x B0 : bj G B 1, (—bj) G B 1} to a one-chain c as follows. 
The formal expression of c in C 1 contains (by definition) terms of the form 
(+1)bj and (-1)br, where the coefficient (+ 1) is assigned to those bj of 
B1 occurring in c, while (-1) is assigned to those br of B1 for which (-br) 
occurs in c but not in B 1. Furthermore, one may prove that c is a one-cycle.
Conversely, one can prove that the graph of any one-cycle c G C 1 always 
contains a loop, if we extend convention yk (-bk) = -ykbk to the edges 
(-bk )ofB1 . The graph of a vector of C1 is given by the union of the closed 
edges actually present in its expression.
We are now prepared to define the Betti number of the graph G which, 
when it corresponds to a transition matrix P of a Markov chain £, will be 
called the Betti number of P or of £.
Consider first any maximal tree T of G. (Recall here that a tree 
of G is any connected subgraph without loops.). Then T comprises all 
the points of G, but there is a certain number B of edges, whose set 
is denoted by B 1, i.e., B 1 = {3 1 ,32,...,3b}, that do not belong to 
the set of the edges of T, denoted B 1(T). That is B1 = B 1 \B 1(T). 
Although T (and then 31,...,3B) may not be unique, the number 
B is a characteristic of G (it is independent of the choice of T ). 
The number B will be called the Betti number of the graph G and 
the edges 31, 32 ,...,3B will be called the Betti edges of G associated with 
the maximal tree T.
Accordingly, we have
_ ~ _ __
B 1 = B1 U B 1(T). 
(4.4.1)
and
B =dimC1 - card B1 (T)
= T 1 — T0 + 1
since G is connected. (Here card B1 (T) symbolizes as always the number 
of the elements of B1 (T).)
Let 3j be an edge of B1 and let nk and nh denote the endpoint and the 
initial point of 3j , respectively. Consider for a moment the unoriented edge 
3j associated to 3j and the unoriented maximal tree T associated to T. 
Then 3j may correspond to one or two oriented edges of B1 . Since T is 
connected there is a unique polygonal line aj in T made up of closed edges 
joining nh and nk. When 3j is added to aj we obtain an unoriented loop 
symbolized by Xj. This loop is the only one which can be made using T 

64 
4. Circuit Representations of Finite Recurrent Markov Chains
and the edge fij. Now, denote by <jj and Xj the corresponding polygonal 
lines of aj and Xj made up by the edges of G, which eventually may have 
different orientations. Associate Xj with the orientation cj of the originally 
chosen Betti edge fij. If B 1 contains two edges with the same terminals but 
with opposite orientation then we shall obtain by the previous procedure 
two versions (aj ,Xj ).
Let Xj and aj be the one-chains associated to Xj and aj, respectively, that 
is, the formal expression of Xj (aj) in C1 is the linear combination where the 
edges of B1 occurring in Xj(aj) with the orientation cj have the coefficient 
(+1) and the edges of B1 occurring in Xj (aj) with opposite orientation -cj 
have the coefficient (-1) (while all the other edges of B1 are considered to 
have the coefficient 0). Then
Xj = aj + fij. 
(4.4.2)
One may prove that X 1,..., XB are one-cycles. We call X 1,..., XB the Betti 
one-cycles of G associated with the Betti edges fi1 , fi2 , . . . , fiB .
Put
A = {A 1,... ,Xb }.
Then A depends on the choice of the original maximal tree T.
We now prove the
Lemma 4.4.1 The set A of Betti one-cycles of G is a base of C 1 = Ker 6.
Proof. Let 0 be the neutral element of C 1. Since the graph of a one-cycle 
always contains a loop, if a one-chain is defined by certain edges of B1 (T ) 
and is a one-cycle, then it is necessarily identical to 0. Let further c 1 be 
any vector of C1. Because of (4.4.1) we can write
B _
c ^57 ak fik + y? ykbk, 
ak,yk G R.
k=1 
b B 1( T)
Since Xk = fik + ak as in (4.4.2), we have
B _
c 1=y1 ak (x—ak)+ ys yk bk
B _ B
^2 ak Xk + 
yk bk — 12 ak^k.
k = 1 
br.tB 1 (T) 
k = 1

4.4 The Betti Number of a Markov Chain 65
B 
B
Note that c 1 — ^2 akAk e C1 and £ ykbk — 22 akak is a vector of 
k=1 br.£B i( T) 
k =1
the subspace generated by B1(T). Then the difference
ykb yk bk — akak
bKEB i( T) 
k =1
should be 0. Hence, any one-cycle can be written as a linear combination 
of A 1 ,...,XB.
Finally, the independence of A 1,... ,AB follows immediately. 
□
In general, any base of B elementary one-cycles will be called a base 
of Betti one-cycles. From Berge (1970) (p. 26, Theorem 9) we know that 
for any strongly connected graph G there exists a base of B independent 
algebraic directed circuits.
When y 1, ■ ■ ■,2b are certain directed circuits with distinct points (except 
for the terminals) of the graph G such that the associated one-cycles 
Y 1,... ,y B form a base of one-cycles, then we call {y 1,... ,y B } a base of 
circuits.
Let us now examine the concepts of Betti one-cycles and indepen­
dent circuits, introduced before, in the context of a concrete exam­
ple. Consider the directed graph G = (B0,B1) with the set of the 
points B0 = {1, 2, 3, 4, 5} and with the set of oriented edges B1 = 
{b(1,2), b(2,3), b(3,1), b(1,3), b(3,4), b(4,5), b(5,1)}, with this ordering, where b(i,j) 
designates the edge with the initial point i and the endpoint j.
The graph G is illustrated in Figure 4.4.1. This graph provides four 
directed circuits with distinct points (except for the terminals): c1 = 
(1, 2, 3, 1),c2 =(1,3,4,5, 1),c3 =(1,3, 1), and c4 =(1,2,3,4,5, 1). One can 
easily see that the one-chains
A 1 = b (1,2) + b (2,3) + b (3,4) + b (4,5) + fi (5,1),
A 2 = b (1,2) + b (2,3) + P (3, 1), 
A 3 = P (1,3) — b (2,3) — b (1,2),
form a base of Betti one-cycles corresponding to the Betti edges P(5,1), P(3,1) 
and P(1,3) . Furthermore,
A 1 = c 4, A 2 = c 1, 
A 3 = c 2 — c 4,
and
c 3 = A 2 + A 3.
Then the one-cycles y 1 = A 1 ,y2 = A2, and y3 = A 1 + A3, associated with 
the directed circuits c4,c 1, and c2, form a base for the one-cycles as well. 
Therefore r = {y 1, y2,23 } is a base of circuits of G.

66 
4. Circuit Representations of Finite Recurrent Markov Chains
Figure 4.4.1.
As we have seen the graph G contains certain circuits generating a base r 
for the one-cycles. Then any directed circuit of G can be written as a linear 
combination of the circuits of such a collection r when they are viewed in 
the vector space of all the one-cycles.
Remarks. The definitions of the Betti number and of the related concepts 
introduced in this section are different from those given by S. Lefschetz 
(1975). One basic difference arises from the definition of the basis B1 of the 
vector space C1 of all the one-chains. Namely, in Lefschetz’s definition B1 
does not contain the inverses -bij of the edges bji even if -bij appear in the 
graph G. As a consequence the one-cycles of certain circuits which appear 
in the graph are confused with 0, so these circuits are not considered. For 
instance the circuit c3 of Figure 4.4.1 corresponds in Lefschetz’s approach 
to the one-cycle c3 = 0.
Our preference for the approach used in this section is motivated by the 
necessity to obtain a homologic description for all the circuits of the graph 
G since they are identical to the circuits appearing along the trajectories 
of all the Markov chains whose transition matrices have the graph G.
In the next section we shall further develop this argument, revealing 
thereby an important link between the homologic description of the circuits 
and the theory of Markov chains.
4.5 A Refined Cycle Decomposition of Finite 
Stochastic Matrices: A Homologic Approach
This section is a sequel to the preceding one; the corresponding notations 
will be employed here without further comment, save for the attached ho­
mologic one-cycles to the directed circuits ck : they will be denoted here 
by ck in order to avoid confusion. We shall be concerned with a Markov 
chain £ which is homogeneous and irreducible (or recurrent), and which 
describes the stochastic motion of a system capable of being in any state of 

4.5 A Refined Cycle Decomposition of Finite Stochastic Matrices 67
the finite set S = {1,..., n}, n > 1. Let P = (pij, i, j G S) be the transition 
matrix of £ and let n = (ni,i G S) denote the invariant probability (row) 
distribution of P. Then the probabilistic circuit decomposition given by 
Theorem 4.1.1 assigns £ to a unique positive vector (wck .ck G C), where C 
denotes the class of all directed circuits with distinct points (except for the 
terminals) occurring along almost all the sample paths of £, endowed with 
an ordering. Accordingly, the expression
W 22 w Ck 
ckE C
uniquely determines a vector in the space C1 of all the one-cycles associated 
with the graph G of P. On the other hand, using equations (4.1.1) and 
considering some orderings for the points and edges of the graph G of P, 
we see that the coordinates w(i,j) of w with respect to the base of all the 
edges of G are identical to nipij, i,j G S. Thus the nP can be viewed as a 
one-cycle.
With this interpretation, we shall prove in the present section that P can 
admit a minimal linear decomposition, with real coefficients, in terms of 
the independent circuits of the graph of P. As an immediate consequence 
one may extend this circuit decomposition to general recurrent stochastic 
matrices P by repeating the same argument to each strongly connected 
component of the graph G(P). Recall here that the strong connectivity 
relation introduced in Section 4.4 defines an equivalence relation in B0 
according to which the subgraphs induced by the equivalence classes are 
called the strongly connected components of G.
Let us introduce the
Notation. According to the definitions of the preceding section, let 
G = G (P ) = ( B o( P) ,B 1( P)) ,n = n (P) ,B = B (P ) = B (n (P)), and r = 
r(P) denote, respectively, the graph of P, the edge-point incidence ma­
trix of G, the corresponding Betti number of G, and a base of independent 
circuits of G, where B0(P) and B1(P) denote the set of points and the set 
of edges endowed with an ordering, respectively. Denote further by C the 
ordered collection of all the directed circuits with distinct points (except for 
the terminals) occurring in G(P). The one-cycle associated with a circuit 
Y will be symbolized by y •
We now establish another circuit-decomposition-formula for P due to S. 
Kalpazidou (1995).
Theorem 4.5.1. (The Homologic Cycle Decomposition). Let P = 
(pij ,i,j =1,...,n) be an irreducible stochastic matrix whose invariant 
probability distribution is n = (n 1,. .. ,nn). Let r = {y 1, .. . ,Yb} be a base 

68 
4. Circuit Representations of Finite Recurrent Markov Chains
of directed circuits of the graph G(P), where B is the corresponding 
Betti number. Then nP can be written as a linear expression of the cir­
cuits y 1,. .., Yb whose coefficients depend on the circulation distribution 
(wc, c G C), that is,
_ 
JL
52 niPij b (i,j) = =1w w Yk • Yk , 
b ( i,j ) G B 1( P), w Yk G R, 
(4.5.1)
with
wJYk = 
A(c,yk)wc, 
A(c,yk) G Z, k =1,...,B.
In terms of the (i, j)-coordinate, equations (4.5.1) are equivalent to
B
niPij = 52 w Yk JYk ( i,j ), 
w Yk G R; i,j = 1, 2 ,...,n, 
(4.5.2)
k=1
where JYk is the passage-function of the circuit yk,k =1,...,B. Further­
more the decomposition (4.5.1) is invariant to the ordering-changes of the 
circuits of C.
If P is a recurrent stochastic matrix, then a similar decomposition to 
(4.5.1) (or (4.5.2)) holds, except for a constant, on each recurrent class. 
(Here Z and R denote as always the sets of integers and reals.)
Proof. Suppose first P is irreducible. Denote w(i, j) = nipij, and let b(ij) 
be the directed edge of B1 (P) whose initial point and endpoint is i and j, 
respectively. Then according to the formalism of the previous section, the 
vector w = ^2(ij) w(i, j)b(i,j) is an element of the vector space C 1 whose 
base is B1(P). Moreover, the decomposition (4.1.1) in terms of the circu­
lation distribution {wc}ceC enables us to write
w = 52 52 wcJc(i, j) b (i,j)
= 52 wc ^52 Jc(i,j)b(i,j)^
5" wc • c, 
ceC
where the last equality follows from the vector expression c of c in C 1 
(since Jc(i, j) is the (i, j)-coordinate of c with respect to the base B 1(P)). 
Then w is a one-cycle. Note that all the representatives of the class-circuit 
c according to Definition 1.1.2 define one-cycles with the same coefficients 
with respect to the base B1(P) since, according to equation (1.2.1), the 

4.5 A Refined Cycle Decomposition of Finite Stochastic Matrices 69
passage function Jc is invariant to translations. Then it makes sense to say 
that any class-circuit c determines a unique vector c in C1. Consequently, 
for any (class-) circuit c we may write the corresponding vector c as a linear 
combination of the Betti one-cycles 1 1,... ,1 b of G (P), and so
C =J2 A(c,Yk)1 k, A(c,Yk) € Z.
k=1
Hence the original vector w has the expression
b / 
\
w 22 I 12A(C,Yk)wc I 1 k, 
k=1 y etc 
/
that in terms of the (i, j)-coordinates becomes
B / 
\
w(i,j ) = niPij 
22 A(c,1k )wc I JYk (i,j) .
k=1 ctC
Defining the weights of the independent circuits as
w Yk = 22 A ( C, 1k ) wc, 
k =1 ,...,B,
we obtain
B 
w = ^ wYk • 1 k . 
k=1
Furthermore, from Theorem 4.1.1 we know that the circuit-weights wc,c€ 
C, of the circulation distribution do not depend on the ordering of the 
circuits in C. Then the coefficients wY 1,..., Wjyb , will be independent of the 
ordering chosen in C.
Finally, one may extend the decomposition (4.5.1) to any recurrent 
stochastic matrix P by repeating the previous arguments for each strongly 
connected component of the graph G(P). Accordingly, the Betti number 
of G(P) will be equal to card B 1 - card B0 + p, where p is the number of 
the connected components of G(P). The proof is complete. 
□
Any decomposition of P in terms of the B independent circuits is 
called a Betti-type circuit decomposition of P. Furthermore, if such a 
decomposition is given by the class (r, toy), then we call (r,wY) the 
Betti-type representation of P. For instance, the class (r ,ui Y) occurring in 
equations (4.5.2) is such a representation.
Remark 4.5.2. The coefficients wjYk ,k=1,...,B, of the decompo­
sitions (4.5.1) and (4.5.2) can be negative numbers. When we can find 

70 
4. Circuit Representations of Finite Recurrent Markov Chains
a base r = {7 1,... ,yB} of B independent circuits such that the circuit­
weights wYk are greater than or equal to the sum of wc for all the circuits 
c ^ r, then the corresponding weights wYk will be nonnegative numbers.
For an example, let us turn to the circuits c1 =(1, 2, 3, 1), c2 = 
(1, 3, 4, 5, 1), c3 =(1, 3, 1), and c4 =(1, 2, 3, 4, 5, 1) of Figure 4.4.1. Let P 
be a stochastic matrix whose graph is illustrated in Figure 4.4.1. As­
sign to these circuits the positive probabilistic weights wc1 ,wc2 ,wc3 ,wc4 
of the corresponding circulation distribution. If wc3 > wc4, then we may 
choose the independent circuits to be y 1 = c 1, Y2 = C2 ,Y3 = C3, while c4 = 
Y 1 + Y2 - Y3, if we do not adhere to the convenction b(j,i) = (— 1)b(i,j).
It is to be noticed that for any edge (i,j) of c4 we have
Jc4(i,j) = JY1(i,j) + JY2(i,j) - JY3(i,j)
( JY 1 (i,j), 
if (i,j) G{(1, 2), (2, 3)};
I JY2 (i,j), 
if (i,j) G{(3, 4), (4, 5), (5, 1)}.
Therefore the circuit c4 passes through an edge if and only if a single circuit 
of r = {y 1 ,Y2,Y3} does. Then the weights of the decomposition (4.5.2) 
are as follows: wY 1 = wY 1 + wc4, wY2 = wY2 + wc4 and wY3 = wY3 — wc4 > 
0. Hence the decomposition (4.5.2) becomes:
niPij = (wY 1 + wc4 ) JY 1 (i,j ) + (wY2 + wc4 ) JY2 (i,j) + (wY3 - wc4 ) JY3 (i,j),
for all i, j.
If wc4 > wc3, we may choose y 1 = c 1 ,Y2 = c2 ,y3 = c4 as independent 
circuits, while c3 = y 1 + Y2 - Y3, and then the following decomposition has 
positive coefficients:
niPij = (wY 1 + wc3 ) JY 1 (i,j ) + (wY2 + wc3 ) JY2 (i,j) + (wY3 - wc3 ) JY3 (i,j),
for all i, j.
Finally, one can easily see that any collection of three circuits of Figure 
4.4.1 determines the remaining fourth circuit. This means that the corre­
sponding vector of the latter is a linear expression of the one-cycles attached 
with the other three circuits.
Definition 4.5.3. Given a finite irreducible or recurrent stochastic matrix 
P, we call the dimension of a circuit decomposition C of P the number 
of the circuits of C. Accordingly, the number n2 — n + 1 occurring in the 
Caratheodory-type representation is called the Caratheodory dimension, 
while B occurring in (4.5.2) is called the Betti dimension. (The latter should 
be accordingly modified when P has more than one recurrent class.)
Remark 4.5.4. After normalizing the passage functions, the Betti-type 
decomposition (4.5.2) can be viewed as a refinement of the Caratheodory- 
type decomposition (4.3.2). Beyond the improved dimension, the decom­
position (4.5.2) relies on an algorithm providing the circuit weights. In 

4.5 A Refined Cycle Decomposition of Finite Stochastic Matrices 71
Sections 3.6 and 3.7 of Part II we shall show that the dimensions of Betti 
and Caratheodory are concerned with a new dimensional characteristic of 
a recurrent matrix, called the rotational dimension.
Remark 4.5.5. In analogy to the sequence {circuits, edges, points} of the 
linear graph (1-complex) of the function w occurring in Theorem 4.5.1, we 
have the sequence 2-cells (surface elements), 1-cells (line segments), 0-cells 
(points) of the corresponding 2-complex. Here by a “2-cell” we mean the 
genuine closed 2-cell, that is the closed interior region of a circuit. The 
2-cells can be analogously oriented as the 1-cells (edges) are in the linear 
graph. For instance, associate a definite orientation of a 2-cell e with each 
of the two possible orientations of its bounding circuit. The 2-cell with the 
opposite orientation is denoted by (-e).
In Section 1.4 (of Chapter 1) we introduced two transformations n and 
Z by (1.3.7) and (1.3.8) in the sequence {circuits, edges, points} such that
circuits — edges —• points, 
(4.5.3)
where nt is the transposed matrix of n.
Now, consider the boundary operators 61 and 62 for the sequence of 
k-cells, that is,
21
C2 — C1 — C 0, 
(4.5.4)
where Ck denotes the vector space generated by the k-cells, k =0, 1, 2, 
except for those with opposite orientation, 6 1 is given by an expression 
similar to 6 of the previous section and 62 is the linear operator assigning 
each 2-cell to its bounding circuit. The elements of Ck are called k-chains, 
k=0,1,2.
Notice that the linear operator 62 can be expressed by an incidence 
matrix v defined as follows:
v — (vcell,edge) — (veibj ),
where
+1,
veibj — 
-1,
. 0,
if the j th edge is positively included in the 
bounding circuit of the ith 2-cells;
if the j th edge is negatively included in the 
bounding circuit of the ith 2-cell;
otherwise.
(Here we assume given orderings for the k-cells, k —0, 1, 2.)
A k-cycle, k — 1, 2, is a k-chain with 6k-boundary zero. Since Im 62 C 
Ker 6 1, it makes sense to define the factor group H1 = Ker 61 /Im 62, called 
the first homology group.
It is to be noticed that the boundary operator 61 corresponds to the 
transformation nt in the sequence (4.5.3), while 62 does not correspond 
to Z. Even if sequences (4.5.3) and (4.5.4) are distinct however there is a 

72 
4. Circuit Representations of Finite Recurrent Markov Chains
common link: both sequences are ruled by an orthogonality equation, which 
is either ntZ = 0 or 6162 =0, expressing the essence of a known theorem: 
“the boundary of the boundary is zero.”
Remark 4.5.6. An analogue of Theorem 4.5.1 may be obtained when the 
probabilistic algorithm is replaced by a nonrandomized algorithm like that 
of Theorem 4.2.1. But in this case the circuit decomposition (4.5.1) (or 
(4.5.2)) will depend on the ordering of the representative circuits.
4.6 The Dimensions of Caratheodory and Betti
The previous investigations of the circuit decompositions of finite recurrent 
stochastic matrices lead to certain natural classifications.
A first classification of the circuit decompositions can be given according 
to the nature of the methods which are used when solving the corresponding 
cycle generating equations. Consequently, we have either nonprobabilistic 
or probabilistic approaches arguing for the following classification:
Deterministic circuit representations
(provided by Theorem 4.2.1, the Caratheodory dimensional theorem, 
and Theorem 4.5.1 (see Remark 4.5.6)).
Probabilistic circuit representations 
(provided by Theorem 4.1.1).
A second classification can be considered according to the dimension of 
the circuit decomposition as introduced by Definition 4.5.4. For instance, 
the Caratheodory-type circuit representation (4.3.2) of an irreducible or 
recurrent stochastic matrix P =(pij,i,j =1, 2,...,n), with n>1, pro­
vides the dimension N which is concerned with the Caratheodory dimension 
n2 - n +1.
On the other hand Theorem 4.5.1 provides the Betti dimension B. The 
next table shows all the above classifications:
Classification of the Circuit Representations
Criterion: Representation method 
Dimension
Deterministic representations Caratheodory-type representations
Probabilistic representations 
Betti-type representations
Other representations

5
Continuous Parameter Circuit
Processes with Finite State Space
Discrete parameter circuit processes, called circuit chains, were defined in 
the previous chapters as Markov chains whose transition probabilities are 
expressed by linear combinations in terms of directed circuits and weights. 
A natural development of these processes is the consideration of the con­
tinuous parameter case. One significant aspect, manifesting itself more evi­
dently in the continuous parameter case, is the conversion of the dichotomy 
circuit-weight into qualitative-quantitative stochastic properties. The ap­
proach of this chapter is due to S. Kalpazidou (1991c).
5.1 Genesis of Markov Processes 
by Weighted Circuits
In this section we shall show that, given a class C of overlapping directed 
circuits in a finite set S, and nonnegative functions wc (•) ,c G C, defined 
on [0, <x) and satisfying natural topological and algebraic relations, we 
can define an S-state continuous parameter Markov process (for short 
Markov process) whose transition probabilities are completely determined 
by (C,wc(•)). We shall call such a process a circuit Markov process, or 
simply a circuit process.
Let us recall the definition of a Markov process. Let S be at most a 
de-numerable set. An S-valued stochastic process £ = (£t)t>0 on the prob­
ability space (Q, K, P) is said to be a Markov process (or a continuous 
parameter Markov process) with state space S if for any n G{1, 2,...} 
and for all i 1,..., in, in+1 G S and tk G [0, + <x), 1 < k < n +1, such that

74 
5. Continuous Parameter Circuit Processes with Finite State Space
11 < • • • <tn < tn+i, we have
P(£tn +1 = in +1 /^tn = in, £tn-1 = in — 1, . . . , £tn = i 1)
= P(£t„+1 = in +1 /£t„ = in )
whenever the left member is defined.
A Markov process £ = (£t)t>0 with state space S is called homogeneous 
if, for every i, j G S and for all s > 0, t > 0, the conditioned probability 
P(£s+t = j/£s = i) does not depend on s. Furthermore, for such a process 
the probability P(£s+t = j/£s = i),t > 0, denoted by pij (t), is called the 
transition probability from state i to state j after an interval of time of length 
t. Then the collection P = (P(t))t>0 given by P(t) = (pij(t), i,j G S), with 
Pij (0) = Sij, is a stochastic transition matrix function, that is, the elements 
pij (•) satisfy
pij (t) > 0, 
pij(t)=1, 
iG S, t > 0,
j
and the Chapman-Kolmogorov equations
pij (t + s)= 
pik(t)pkj(S), 
i,jG S, t, s > 0.
In turn P determines a homogeneous stochastic transition function (see 
K.L. Chung (1967)). As in the discrete parameter case, following the ex­
istence theorem of Kolmogorov, one may define Markov processes from 
stochastic transition matrix functions (in general, from transition func­
tions).
Let us now see how a Markov process can be defined using a collection of 
directed circuits c and a collection of nonnegative functions wc (•). Consider 
a finite set S consisting of m> 1 elements and a collection C of overlapping 
directed circuits in S containing all the loop-circuits (i, i),i G S. Suppose 
that the circuits of period greater than 1 have distinct points except for 
the terminals. The directed circuits and the related ingredients are defined 
according to Chapter 1.
Let MS/S({0, 1}) be the class of all m x m matrices whose entries belong 
to the set {0, 1}. Associate each circuit c G C with the matrix (Jc(i, j)i, j G 
S) G Ms*s({0, 1}) defined as
1, if (i, j)isanedgeofc;
0, otherwise.
We call (Jc(i, j), i, j G S) the second-order passage-matrix of c (see Defi­
nition 1.2.2). Then Lemma 1.2.3 enables us to write
Jc'^j(i,j) = 
Jc(j, i) = Jc(i), 
i G S. 
(5.1.2) 
c(i, j)
(5.1.1)

5.1 Genesis of Markov Processes by Weighted Circuits 75
Let {wc (•) ,c G C} be a collection of real functions defined on [0, <x). For 
any i, j G S and any t > 0 introduce
w(i,j,t) = 
wc(t)Jc(i, j). 
(5.1.3)
Then on account of the balance equations (5.1.2) we may write
£w(i,j,t) = £w(j,i,t) = w(i,t), 
t > 0. 
(5.1.4)
Introduce the following conditions: 
(wi) (i) Every function wc(•), c G C, is nonnegative on [0, + to).
(ii) For any loop-circuit c =(i, i),i G S , we have
wc(0) = lim wc(t) > 0.
t^ 0+
(iii) For any circuit c =(i, i),i G S, we have
wc(0) = lim wc(t) = 0.
t^ 0+
(w2) The collection {wc (•), c G C} is a solution to the equations 
w(i, j, t + s)/w(i, t + s)
= 
(w(i, k, t)/w(i, t))(w(k, j, s)/w(k, s)), i, j G S,
k
for any t, s > 0.
(w3) The function w(i, t) introduced by (5.1.4) satisfies the equation 
w (i, t) = w(i, 0) 
for any i G S, t > 0.
(w4) The limit limt^^ wc(t) exists and is finite, for all c G C.
Let us now interpret condition (wi) above. We have
lim (w(i, i, t)/w(i, t)) = lim 
t^ 0+ 
t^ 0+
wc (t)Jc (i, i)
w (i,i )(0) J(i,i)(i,i) 
w (i,i )(0) J(i,i)(i) 
= w(i, i, 0)/w(i, 0) = 1.
For i = j we have
lim (w(i, j, t)/w(i, t)) = 0.
t^ 0+
Thus
w(i, j, 0)/w(i, 0) = Sij,

76 
5. Continuous Parameter Circuit Processes with Finite State Space
where S is Kronecker’s symbol. Hence
w(i, j, 0) = 0w,(i,i)(0),
if i = j ;
otherwise.
Definition 5.1.1. Suppose conditions (w2)-(w4) are satisfied. Then any 
continuous parameter S-state Markov process whose transition matrix func­
tion (pij (t) ,i,j e S) is defined as
Pij (t)= w(i,j,t)/w(i,t), 
i,j e S, t > 0,
is called a circuit Markov process or, for short, a circuit process, associated 
with the collection {C,wc (•)}.
5.2 The Weight Functions
Given S and C as in the previous section, the functions wc (•) ,c e C, that 
satisfy conditions (w2)-(w4) will be called the weight functions associated 
with C. If the weight functions satisfy conditions (w1 )(ii) and (w1 )(iii), 
then they will be called standard weight functions. The name is motivated 
by Definition 5.1.1 according to which the standard weight functions may 
define a standard transition matrix function. A transition matrix function 
P(t) = (pij(t), i,j e S),t > 0, is called standard if limt^0+ pij(t) = Sij, for 
all i,j e S (see K.L. Chung (1967)). If P(t) is standard, then every pij(•) 
is measurable (with respect to Lebesgue measure).
In this section we shall prove the existence of weight functions, that is, 
the existence of a nonnull solution to equations (w2). For this purpose, we 
say that a directed circuit c =(i1,...,is,i1) is associated with a positive 
matrix (aij, i,j e S) if and only if ai 1 i2 •... •aisi 1 > 0.
We now prove
Theorem 5.2.1 (The Existence of the Weight Functions). There exists a 
non-void class C of directed circuits in S and a collection {wc(•), c e C} of 
nonnegative standard weight functions, that is, the wc (•) ’s satisfy conditions 
(w1), (w2), (w3), and (w4).
Proof. Let P (t) = (pij(t), i, j e S),t > 0, be an arbitrary irreducible 
stochastic transition matrix function and let n = {nii e S} be the corre­
sponding invariant probability distribution, that is,
(i) Pij(•) > 0, ni > 0, i,j e S;
(ii) E Pij (t) = 1, Eni = 1, i e S, t > 0;
(iii) pij (s +t)= pik(s)pkj(t), 
i,je S and s,t > 0;
(iv) E nipij (t) = nj, j e S, t > 0.
ies

5.2 The Weight Functions 77
Suppose {P(t),t > 0} is a standard transition matrix function. Then the 
function w(•, •, •) : S x S x [0, to} — [0, to) defined as
w (i,j,t) = niPij (t)
satisfies the following balance equations:
£w(i,j,t) = £w(j,i,t) = ni, 
i G S, t > 0. 
(5.2.1)
For each loop-circuit (i, i) define
w (i,i)(•) = niPii (•).
Then the w(i,i)(• )’s are functions satisfying conditions (w1) (ii) and (w4). 
Let us now fix an arbitrary 11 in (0, + to) and i 1 G S. Since nil > 0, we have
£w(i 1 ,j,t 1) > o. 
j
On account of the balance equations (5.2.1) and the irreducibility of P (t1) 
there exists a sequence of pairs (i1, i2), (i2,i3),...,(in-1,in),... of distinct 
points such that for each (ik,ik+1) we have w(ik,ik+1,t1) > 0.
Since S is finite, there exists a smallest integer s = s(t1) > 2 such that 
is = ik for some k, 1 < k < s. Then
w(ik, ik+1, t1)w(ik+1, ik+2, t1) ... w(is-1, ik, t1) > 0.
Therefore the directed circuit
c1 =(ik,ik+1,...,is-1,ik),
is associated with the matrix (pij(t1), i, j G S). Define the function
wc1(t) = min {w(ik, ik+1, t), w(ik+1, ik+2, t),..., w(is-1, ik, t)}
for t > 0. Then wc1 (0) = 0. Moreover, wc1 (•) satisfies condition (w1)(iii).
On the other hand, according to a theorem of Levy, the following limits 
are finite:
lim w(ik , ik+1 ,t) = likik+1 ,..., lim w(is-1 , ik, t) = lis-1ik . 
t>^> 
t>^>
If lc1 =min{likik+1,...,lis-1ik} = limim+1 for some m = k,...,s- 1, then
|wc1 (t) - lc1 | < |w(im,im+1,t) - limim+1 |,
and therefore lim wc 1 (t) = lc 1 as t — to . Thus the function wc 1 (•) satisfies 
condition (w4). Introduce now
w 1(i,j,t) = w(i,j,t) - wc 1 (t) Jc 1 (i, j)
- 
w(u,u)(t)J(u,u)(i,j), 
i,jG S, t > 0.
uES

78 
5. Continuous Parameter Circuit Processes with Finite State Space
Then by the definition of wc 1 (•) and w(u,u)(•), the function w 1 (•, •, •) is non­
negative. Moreover, the functions Jc 1 (•, •), J(u,u)(•, •) and w(i,j, •) satisfy 
the balance equations, and so does w 1(i, j, •). Hence two cases are possible. 
First, we may have
w1(i,j,t) = 0
for all i, j G S and any t > 0. Then
w 1( i,j,t) = wc 1 (t) Jc 1 (i, j) + 7^ w (u,u)(t) J(u,u)(i, j),
and therefore {wc 1 (•),w(i,i)(•),i G S} is a solution to the equations (w2). 
Second, there exist a pair (j1, j2), j1 = j2, and t2 > 0 such that 
w1 (j1 ,j2 ,t2 ) > 0. Then, following the same reasonings above for the func­
tion w 1(i, j, •) instead of w(i, j, •), we obtain a circuit
c2 =(j1,j2,...,jr,j1).
with r > 2 and with distinct points j1, j2 ,...,jr, such that c2 is associated 
with the matrix (w1(i, j, t2), i, j G S). Define further
wc2(t) = min{w1(j1, j2, t),..., w1(jr, jt, t)}
for t > 0. Then wc2 (•) is not everywhere zero. Moreover, wc2 (•) is right- 
continuous at zero and wc2 (0) = 0. As for wc 1 (•), the function wc2 (•) satisfies 
condition (w4). Introduce now
w2(i,j,t) = w(i,j,t) - wc 1 (t) Jc 1 (i,j) - wc2 (t) Jc2 (i,j)
- 
w(u,u) (t)J(u,u) (i, j ).
The function w2 is balanced and consequently, if it is nonnull, we may 
continue the process above. Accordingly, since S is finite, we obtain a fi­
nite ordered class C = {c1,...,cm} of directed circuits in S (containing all 
the loop circuits) and a collection {wc (•) ,c G C} of nonnegative functions 
defined on [0, + to ) and depending upon the ordering of C such that
w(i, j,t) 
wc(t) Jc(i, j).
ceC
Furthermore, the wc(• )’s satisfy conditions (w1) and (w4). Also, the hy­
pothesis (iv) implies that the function
w(i, t)= 
w(i, j, t)
is given by n, and consequently condition (w3) is fulfilled.
Finally, the Chapman-Kolmogorov equations (iii) show that {wc (•) ,c G 
C} is a solution to equations (w2). This completes the proof. 
□

5.3 Continuity Properties of the Weight Functions 79
One may prove an analogue of Theorem 5.2.1 using the homologic decom­
position of Theorem 4.5.1. In this case condition (w1) (i) is not necessary, 
that is, there exist real-valued standard weight functions. On the other 
hand, in the subsequent Section 5.5 we shall show the existence of strictly 
positive standard weight functions.
5.3 Continuity Properties of the Weight Functions
In this section we shall concentrate on continuity properties of the weight 
functions wc(•),c G C. Moreover, we shall show that, even when we begin 
with a more general class Cof time-dependent circuits c(t), t > 0, satisfying 
natural conditions, the corresponding weight functions still enjoy continu­
ity properties. However, continuity of the passage functions necessarily re­
stricts C to a class of circuits independent of t, what motivates our original 
considerations on circuits which are independent of parameter value.
Let S be a finite set. Consider C a collection of directed circuits in S 
satisfying the following conditions:
(c1 ) C contains all the loop circuits (i, i),i G S; any circuit, whose period 
is greater than 1, has distinct points except for the terminals; and
(c2) through each pair (i, j) of points of S there passes at most one circuit 
of C.
Any function c : [0, + to ) ^ C is called a circuit function. Consider a finite 
set C of circuit functions containing the loop functions c (t) = (i,i) ,i G S. 
Suppose further that the circuit functions of C satisfy the following condi­
tions:
(c) 
(i) Each circuit function c(•) is right-continuous.
(ii) For each pair (i, j )ofpointsofS and for any t>0thereisat 
most one circuit c(t) passing through (i, j), which is given by one 
circuit function c(•) G C.
Given a circuit function c(•) G C, define the function (Jc(.)(i,j))i,jeS: 
[0, + to) ^ MSxS ({0, 1}) by the relation
Jc(t)(i,j) =
1,
0,
if (i, j) is an edge of c(t); 
otherwise.
(5.3.1)
Any function (Jc(.)(i,j))i,jeS defined as in (5.3.1) is called the passage ma­
trix function associated to the circuit function c(•). According to Lemma 
1.2.3 the passage matrix functions (Jc(.) (i, j))i,j satisfy the following bal­
ance equations:
Jc(t) (i, j)= 
Jc(t) (j, i) = Jc(t) (i), 
iG S, (5.3.2)
for any t > 0.

80 
5. Continuous Parameter Circuit Processes with Finite State Space
Now associate each c G Cwith a real function wc(.)(•) defined on [0, + to). 
Define further
w(i,j,t)=^SWc(t)(t) Jc(t)(i,j), 
i,j G S, t > 0, 
(5.3.3)
ceC
Then by applying relations (5.3.2), we have
w(i,j,t) = 
w(j,i,t) = w(i,t), 
i G S, t > 0. 
(5.3.4)
Introduce the following conditions:
(w 1) (i) For any circuit function c G C, the corresponding function wc(.)(•) 
is nonnegative on [0, +to).
(ii) For any circuit function c G C with c(0) = (i, i),i G S, we have 
wc(0) (0) = lim+ wc(t)(t) > 0.
(iii) For any circuit function c G C with c(0) =(i, i),i G S, we have 
wc(0)(0) = lim wc(t)(t) = 0.
t^ 0 +
(w2) The collection {wc(.)(•)} is a solution to the equations 
w (i,j,t + s) 
w (i,k,t) w (k,j,s) 
. .
w(i,t + s) 
w-i w(i,t) 
w(k, s) , 
,J^,
for any t, s > 0.
(w3) The function w(i,t) introduced by relation (5.3.4) satisfies the equa­
tion w(i, t) = w(i, 0), for any i G S and t > 0.
(w4) For any circuit function c G C, the limit limt^^ wc(t)(t) exists and is 
finite.
The functions wc(.)(•) ,c G C, which satisfy conditions (w2)-(w4) are 
called the weight functions associated with C. If the weight functions satisfy 
condition (w 1), then they are called nonnegative standard weight functions.
Further, we shall consider a collection {wc(.)(•) ,c G C} of nonnegative 
standard weight functions. We now investigate some of the continuity prop­
erties of these weight functions. Namely, we first prove
Theorem 5.3.1. For any c G C the weight function wc(.)(•) is uniformly 
continuous on [0, +to).
Proof. Let c G C be arbitrarily fixed. Consider t>0andanh>0small 
enough. Let i, j be two consecutive points of the circuit c(t + h). Thus we 
have
Jc(t+h) (i, j)=1.

5.3 Continuity Properties of the Weight Functions 81
Then according to condition (c)(ii) there exists only the circuit c(t + h) 
that passes through (i, j)attimet + h.
Since the circuit function c(•) is right continuous together with the func­
tion Jc(.)(i,j), from condition (c)(ii), we know that c(t) is the only circuit 
passing through (i, j) at time t, that is,
Jc(t)(i,j) = 1.
Then 
wc(t+h)(t + h) - wc(t)(t)
= wc(t + h)(t + h) Jc(t+h) ( i,j ) - wc(t)(t) Jc(t) (i,j )
_ 
u t /.A wc(t+h)(t + h)Jc(t+h)(i,j)
/ ■> wc(t+h)(t + h) Jc(t+h)(i ) ) 
<4 । t < ■>
c 
c wc(t+h)(t + h)Jc(t+h)(i)
-wc(t) (t)Jc(t) (i, j)
wc(t+h) (t + h)Jc(t+h) (i) 
c c 
/
(Ec wc(h)(h)Jc(h)(i, k))(Ec wc(t) (t)Jc(t) (k, j))
/ E c wc ( h) ( h ) Jc ( h)( i ))(E c wc (t)( t ) Jc (t)( k )) wc ( t)( t ) Jc ( t)( i,j )
' 
( .( E c wc ( h)( h )Jc ( h)( i,i ))(E c wc (t)(t ) Jc (t)( i,j ))
E c wc(h )( h) Jc(h)( i )
+ 
wc(t+h) (t + h)Jc(t+h) (i))
V (Ec wc ( h)( h ) Jc ( h )( i,k ))(E c wc (t)( t ) Jc (t)( k,j )) 
k=i (Ecwc(h)(h)Jc(h)(i))(Ec wc(t)(t)Jc(t)(k))
x
Then
c wc(h)(h)Jc(h)(i,i)
I,1 
Ec Wc (h)( h) Jc (h )( i U “'( ’ 
' ’)""'
< wc,(.+h)(t + h) - Wc(t)(t) < 
Wc,(h)(h)Jc(h)(i, k).
k=i c
Hence
|wc(t+h)(t + h) - wc(t)(t)| < ^12 wc(h)(h) Jc(h) (i,k) •
k=i c
Then
Iwc(t+h)(t + h) - wc(t)(t) I ^2 wc(h)(h)Jc(h) (i) ^2 wc(h)(h)Jc(h)(i, i) • 
c 
c 
(5.3.5)
Let us now take h < 0 and t = \h\. Replacing t in (5.3.5) by t — t = t + h 

82 
5. Continuous Parameter Circuit Processes with Finite State Space
and using the same reasoning as above, it follows that
|wc(t+h)(t + h) - wc(t)(t) 1^2wc(\h\)(h)Jc(\h\)(i) -12wc(\h\)(h)Jc(\h\)(i,i) 
c 
c 
(5.3.6)
Therefore (5.3.6) is valid for any h G R with t + h > 0 and the proof is 
complete. 
□
Proposition 5.3.2. If Jc(.)(i,j) is continuous on (0, + to) for all i,j G S, 
then c(•) is a constant function.
Proof. It follows from the proof of Theorem 5.3.1 that wc(.)(•)Jc(.)(i, j) 
is uniformly continuous on (0, +to), where (i, j) is an edge of a circuit 
c(t), with t > 0. The same proof can be used for proving that, in general 
I2cwc(•)(•)Jc(•)(i, j) is uniformly continuous on (0, + to).
Let c be a circuit function of C. Consider an arbitrarily fixed t0 > 0and 
the circuit
c(t0)=(i1,i2,...,is,i1),
where s>1andi1,i2,...,is are distinct points. Since Jc(t) (i1, i2) = 1, for 
any t>0, all the circuits c(t), t >0, contain the edge (i1, i2).
Analogously Jc(t) (i2,i3)=1,forallt>0, implies that (i2,i3) is an edge 
of all c(t), t > 0. By repeating the above reasoning for all the edges of c(t0 ), 
we obtain that c (t) = (i 1, i 2,... ,is,i 1). Therefore the circuit function c is 
constant. 
□
Restrict further C to the class C of all constant circuit functions satisfying 
conditions (c1) and (c2). Then the above assumptions (w 1)-(w4) reduce to 
conditions (w1)-(w4) mentioned in Section 5.1. We now prove
Proposition 5.3.3. For all i, j G S, the function
w ( i,j,t )= £ wc (t ) Jc ( i,j ) ,
is uniformly continuous on [0, +to) and its modulus of continuity does not 
exceed that of w(i, i, •) at zero.
Proof. The uniform continuity of the functions w(i,j, •),i,j G S, follows 
from Theorem 5.3.1 and the converse of Proposition 5.3.2. To evaluate the 
modulus of continuity of w(i, j, •) let us consider for any t > 0 and h > 0 
(small enough) the difference
w(i, j, t + h) - w(i, j, t)
r , \' w(i,k,h) 
= w(i, t + h) 5 ——— 
k w(i, h)
w (k,j,t) 
w(k, t) - w(i, j, t)
w(i, i, h) 
w(i, h)
1
A /. . x /. 
, w(i,k,h)
w(i, j, t) + w(i, t + h) V —r 
k=i w(i, h)
w (k,j,t) 
w(k, t)

5.4 Differentiability Properties of the Weight Functions 83
Then
w(i, i, h)
- 1------- /■ w w(i,j,t) - w(i,j,t + h) - w(i,j,t)
w(i, h)
- 
w(i, k, h) = w(i, h) - w(i, i, h).
k=i
Hence
|w(i, j, t + h) - w(i, j, t)| < w(i, h) - w(i, i, h).
In general, for all h G R with t + h > 0 we have
|w(i, j, t + h) - w(i, j, t)| < w(i, |h|) - w(i, i, |h|), 
and the proof is complete. 
□
Recall that the class C is restricted by condition (c2). In case we drop this 
condition, we should assume that the functions w(i, j, •), i,j G S, satisfy the 
continuity property of Proposition 5.3.3. We have
Theorem 5.3.4. Suppose the weight functions are strictly positive on 
(0, + to). Then for any i,j G S, the function w(i, j, •) is either identically 
zero or always strictly positive on (0, + to).
Proof. For an arbitrary t0 > 0 we have either
w(i, j, t0) > 0 
(5.3.7)
or
w(i,j,t0)=0. 
(5.3.8)
If (5.3.7) holds, then there exists a constant circuit function c(t) = c of C 
such that Jc(t)(i, j) = 1 for all t > 0. Therefore w(i,j,t) > 0 for all t> 0. 
If (5.3.8) holds, then for all constant functions c G C we have Jc(i, j) = 0. 
Then (i, j) is an edge of no circuit of C and the proof is complete. □
Remark. Theorem 5.3.4 says that the function w(i, j, •) is either strictly 
positive or identically zero on (0, +to) according to whether (i, j)isoris 
not an edge of a circuit in S. Therefore the previous property is independent 
of the magnitude of the values of the weight functions. For this reason we 
say that Theorem 5.3.4 expresses a qualitative property.
5.4 Differentiability Properties of the
Weight Functions
Suppose the collection (C,wc(•)) is defined as in the preceding section.
According to a well-known result of Kolmogorov we have:

84 
5. Continuous Parameter Circuit Processes with Finite State Space
Theorem 5.4.1. For all i and j, the limit
c
Wc (t) 
t
Jc(i, j)
exists and is finite.
We now give the version of another theorem of Kolmogorov (see K.L. 
Chung (1967), p. 126) in terms of the weight functions corresponding to 
the circuits c = (i, i), i G S.
Theorem 5.4.2. For any i G S,
-w (ii )(0) = tom
w ( i,i )(0) 
w ( i,i)( t )
t
exists and is finite.
Proof. From conditions (w1)(ii) and (w2) we have that w(i, i, t) > 0 for all 
t > 0. Recall that
w(i, i, t)= 
wc(t)Jc(i, i) = w(i,i)(t).
Consider
V (t) = - log( w (i,i)(t) /w (i,i )(0)) •
According to condition (w1)(ii) we have w(i,i)(0) > 0. Hence V is finite­
valued. By using relations (w2 ) we deduce that
w(i, i, t)w(i, i, s) 
w(i,i,t + s) > 
^, 
■
w(i,i)(0)
Then
— log w(i,i) (t + s) < - log w(i,i)(t) - log w(i,i)(s)+log w(i,i)(0) •
The last inequality implies that the function v(•) is subadditive, that is, 
V(t + s) < V(t) + V(s). According to a theorem of Kolmogorov (see K.L. 
Chung (1967), Theorem 4, p. 126), we find that there exists
V(t) 
w(i,i)(0)
lim 
•—— •
tto0+ t w(i,i)(0)
Therefore w(ii)(0) exists and is finite. 
□
We continue with a version of a theorem of D.G. Austin and K.L. Chung 
(see K.L. Chung (1967), Theorems 1 and 2, p. 130) in terms of the weight 
functions.

5.5 Cycle Representation Theorem for Transition Matrix Functions 85
Theorem 5.4.3. The function w(i,j, •)/w(i, •) has a continuous derivative 
on (0, + to) which satisfies the following equations:
w(i, j, s + t) \ ' 
w w(i,k, s) \ ' w(k, j, t)
w(i, s + t) 
w(i, s) 
w(k, t) ,
w(i,j,s + t)\' 
w(i,k,s) fw(k,j,t)\'
w(i, s + t) 
k w(i, s) 
w(k, t) ,
s>0,
t > 0,
(5.4.1)
s > 0, t>0.
(5.4.2)
If all the weight functions wc (•) ,c G C, have continuous derivatives, then 
wC( w'c(t) Jc(i) = 0. 
(5.4.3)
c
Proof. Equations (5.4.1) and (5.4.2) follow from Theorems 1 and 2 of K.L.
Chung ((1967), pp. 130-132). Equation (5.4.3) follows from the relation
£pij (t) = 0, t> 0, 
(5.4.3')
j
where (pij (•), i, j G S) is the transition matrix function of the circuit process 
generated by the given weight functions. Then, in terms of the weight 
functions, the equation (5.4.3') becomes
£ wC (t) £ Jc (i,j ) = 0.
Thus, because of the balance equation (5.1.2), we deduce equation (5.4.3) 
and the proof is complete. 
□
Theorems 5.4.1 and 5.4.2 enable us to introduce the matrix Q = (qij)i,jeS 
defined as
= w (i,i )(0) 
qii 
w (i,i )(0)
_ w1 (i,j, 0) 
qij = w(i, 0)
w(i,i)(t) - w(i, 0) 
lim ——7-.—t-------
t^0+ 
tw(i, 0)
iG S,
lim w(i:M, 
t^0+ tw(i, 0)
i,jG S, i = j.
(5.4.4)
(5.4.5)
We call the matrix Q the weighted transition intensity matrix associated 
with the transition matrix function (w(i, j, t)/w(i, t))i,j introduced by Def­
inition 5.1.1.
5.5 Cycle Representation Theorem for 
Transition Matrix Functions
In this section the inverse problem of representing a finite Markov process 
by a collection of directed circuits and weight functions is solved (gener­
alizing the corresponding results for discrete parameter processes given in

86 
5. Continuous Parameter Circuit Processes with Finite State Space
Chapter 4). A deterministic solution to this problem was already given 
in Section 5.2 using the nonrandomized algorithm of Theorem 4.2.1. Now 
we shall be concerned with a probabilistic approach to the representation 
problem above.
Let £ = (£t)t>0 be a homogeneous irreducible Markov process on a 
probability space (Q, K, P), with a finite state space S and with a stan­
dard stochastic transition function determined by the stochastic transi­
tion matrix function P(t) = (pij(t))i,jeS,t > 0. Associate with each h > 
0 the discrete skeleton Hh = (£hn)n>0, with scale parameter h. Then 
any hh is an aperiodic irreducible Markov chain with transition matrix 
P(h) = (Pij(h))i,jeS. Let ni(t) = P(£t = i), i e S,t > 0. Then ni = m(0) = 
ni (t), i e S,t > 0, define the stationary probability distribution of the pro­
cess £. Consider a circuit c =(c(n),...,c(n + p - 1), c(n)), n e Z, of pe­
riod p>1 and with distinct points c(n),...,c(n +p - 1). Then the passage 
function of order k > 1 assigned to the circuit c is defined as
Jc (i1 , . . . ,ik )=
1,
0,
if i1 ,. ..,ik are consecutive points of c; 
otherwise.
(See Definition 1.2.2.)
Associate the circuit c above with the ordered sequence c = 
(c( n),. .., c( n + p — 1)), where c (n )= c( n) ,...,c (n + p — 1) = c( n + p — 
1), called, as in Chapter 1, the cycle of c. (Both c and c mean equiva­
lence classes with respect to the equivalence relation (1.1.1).)
Definition 5.5.1. For any cycle c = (i 1,..., is) define the function wc: 
[0, + to) ^ [0, + to), called the cycle weight function, by
wc(t) = niipiii2 (t)pi2i3 (t) . . .pis-1is (t)pisii (t)
•Nt(i2,i2/i 1).. .Nt(is,isli 1,.. .,is- 1), 
(5.5.1)
where (ni)is is the stationary distribution of the process £ and
Nt(ik, ik/i1,..., ik-1)
'
P P(£nt = ik, £mt = i 1, . . . , ik-1, for 1 < m < n/£0 = ik ) 
n=1
is the taboo Green function.
Here we have to note that the right-hand side of (5.5.1) is invariant to 
cyclic permutations, so that the expression of wc(•) is independent of the 
choice of the representative c. For n > 0 and t > 0, let Cnt(w) be the class 
of all directed cycles occurring along the sample path Ht (w) until time nt 
and let wc,nt (w) denote the number of occurrences of the cycle c along the 
path Ht (w) until time nt.
Now we are ready to prove

5.5 Cycle Representation Theorem for Transition Matrix Functions 87
Theorem 5.5.2. (Cycle Representation Theorem). Let P (t)= 
(pij(t))i,jEs,t > 0, be a homogeneous standard stochastic transition 
matrix function on a finite set S. If (P(t))t>0 defines an irreducible 
Markov process, then the following assertions hold:
(i) For any t>0 and any circuit c of the graph of P (t), the sequences 
{Cnt(w)}n>0 and {wc,nt (w)/n}n>o converge almost surely, as n ^ 
<x, to a class Ct and to the cycle weight wc(t) defined by (5.5. 1), 
respectively.
(ii) Any discrete skeleton St,t > 0, is a circuit chain associated with the 
class (Ct, wc(t)), that is,
ni ^22 Wc(t) Jc(i), nipij (t) 
22 Wc(t) Jc(i,j), i,j G S, (5.5.2)
CECt 
HECt
where n = (ni,i G S) is the invariant probability distribution of 
P(t),t > 0. Moreover, (Ct, Wc(t)) is the unique representative class 
with the probabilistic interpretation given at (i) and is independent 
of the ordering of Ct.
If (P (t))t>0 defines a recurrent Markov process, then a similar decompo­
sition to (5.5.2) holds, except for a constant, on each recurrent class.
Proof.
(i) It follows from the definition of the sequence (Cnt (w))n that (Cnt (w))n 
is increasing for any t > 0. Hence there exists a finite class Ct(w) of cycles 
in S with
Ct(w) = lim Cnt(w). 
n—>^>
On the other hand, equations (3.2.2) and (3.2.3) enable us to write
lim
n—>^> 
n
E1{the cycle c occurs along Et(u) modulo cyclic permutations}
= Wc(t)
(5.5.3)
almost surely, where wc(•) is defined by (5.5.1). Then, arguing as in Theo­
rem 3.2.1, the collection Ct(w) is independent of w, so that we may denote 
Ct(w) by Ct.
(ii) Since any discrete skeleton St,t > 0, is an irreducible (aperiodic) 
Markov chain, we may apply the representation Theorem 4.1.1, from which 
we obtain that equations (5.5.2) hold.
Finally, the same Theorem 4.1.1 refers to the recurrent case. The proof 
is complete. 
□
Denote C= t>0 Ct. The collection (C, Wc(t))t>0 occurring in Theorem 
5.5.2 (including the loops (i, i) and their weights nipii(t), i G S) is called the 
probabilistic cycle representation of ft and (P(t))t>o. Then, for each t > 0 
the class (Ct, Wc(t)) is a probabilistic cycle representation ofSt and P (t). In 

88 
5. Continuous Parameter Circuit Processes with Finite State Space
Chapter 2 of Part II we shall prove that the collection Ct of representative 
cycles is independent of the parameter-value t>0, that is,
Ct = C.
As a consequence, for any t > 0, Ht will be represented by (C, wc(t)).
Remarks
(i) One may extend the cycle-decomposition-formulas (5.5.2) to denu­
merable irreducible or recurrent Markov processes by using Theorem 3.3.1 
instead of Theorem 4.1.1. Furthermore, for both finite and denumerable re­
current Markov processes the representative cycles may be replaced by the 
corresponding directed circuits. In this case, as we shall show in Chapter 2 
of Part II, the time-invariance of the representative circuits will express a 
version of the well-known theorem of Levy concerning the positiveness of 
the transition probabilities.
(ii) If we appeal to the representation Theorem 4.2.1 for each t-skeleton 
Ht, then it is possible to construct a finite ordered class Cof overlapping di­
rected circuits and deterministic nonnegative weight functions wc (t), c G C, 
such that equations (5.5.2) hold as well (see the proof of Theorem 5.2.1). 
Here the name “deterministic” has, as in the preceding chapters, the mean­
ing that the corresponding weight functions wc (t) do not enjoy a probabilis­
tic interpretation, that is, the wc(t)’s are provided by a deterministic algo­
rithm. Moreover, the algorithm of representation given in the above men­
tioned theorem shows that the deterministic representative class (C,wc(•)) 
is not uniquely determined. In conclusion, the Markov process (£t)t>0 may 
be represented either by deterministic circuit weight functions or by prob­
abilistic cycle (circuit) weight functions.
5.6 Cycle Representation Theorem for Q-Matrices
Usually the process £ of the previous section is defined by using Kol­
mogorov’s limits qij = pij(0+),i,j G S. The matrix Q = (qij,i,j G S) is 
called the transition intensity matrix associated with P(•) = (pij(•),i,j G 
S). In this case, we are confronted with the problem of describing any 
matrix Q, whose entries qij ,i,j G S, verify the relations
( 0, if i = j,
qij | < o if i = j 
q-jCi =0, i G S, 
(5.6.1)
in terms of directed circuits and their weight functions. Recall that any 
matrix satisfying conditions (5.6.1) is called a conservative Q-matrix.
We thus wish to investigate how the qij ’s can be written as expressions 
of qualitative and quantitative ingredients. To this end, we introduce the 
following irreducibility condition for the collection {qij ,i,j G S}:

5.6 Cycle Representation Theorem for Q-Matrices 
89
(Y) For each pair (i, j) of distinct states i and j there exists a finite chain 
(i,k 1,..., km,j) of states with m > 0 and satisfying
qik1 qk1 k2 ...qkmj > 0.
It is obvious that in checking (Y) it will suffice to consider distinct states 
i, k1,...,km,j (when m = 0, the chain reduces to (i, j)).
We now prove
Theorem 5.6.1. (Cycle Representation for Q-Matrices). Let S be a fi­
nite set and let Q be a matrix whose entries qij, i,j G S, satisfy conditions 
(5.6.1) and (Y). Assume that the probability distribution {ni,i G S} satis­
fies the relations ni > 0 and ^2k nkqki = 0, i G S.
(i) Then there exists a finite class C of directed cycles in S and a sequence 
of positive weight functions wc(•), c G C, defined on [0, + to) such that 
(C wc(t))t>o determines a circuit process (in the sense of Defini­
tion 5.1.1) whose transition matrix function P(•) satisfies P'(0) = Q.
Moreover,
f z. 
w(n)(i,j,0), 
if i = j;
in) 
qij =
w(i, 0)
1, 
(5.6.2)
I w(i, 0) w(i,i>(0)’ 
if i = j;
(ii)
(n) 
n 
n
Joi av n in-x , twiner e wj ,, j, , , wj (ii) \ J anoj qij , i espectveeny, eenoce 
the nth derivative of w(i, j, •) = nipij(•), w(i,i)(•) and the (i, j)-element 
of Qn(q(1) = qij), while w(i,t) = £j w(i, j,t) = ni.
The representation (5.6.2) is unique if the representative cycles and 
weight functions have the probabilistic interpretation stated in Theo­
rem 5.5.2. Moreover, up to a positive constant, the series
m, tnn(n) 
1 + n?.
t>0,
and
E
n> 1
tn qin) 
n!
t>0, i= j,
yield, respectively, the mean number of appearances of the circuit (i, i) 
along almost all trajectories of the discrete skeleton St and the mean 
number of appearances of the circuits having i and j as consecutive 
points along almost all trajectories of the discrete skeleton St.
Proof. (i) For the given Q-matrix we first apply the well-known Feller 
theorem concerning the existence of a transition matrix function P(•) such 
that P'(0) = Q. The specialization to our case is that the only (stochas­
tic) transition matrix function P(•) = (pij(•))i,jeS such that P'(0) = Q is

90 
5. Continuous Parameter Circuit Processes with Finite State Space
given by
__ _ tnQn
P(t)=exp(tQ)= I +^, t > 0, 
n!
n> 1
that is,
tn q<i n)
Pij(t) = $ij + 
n!j , i,j £ S, t > 0, 
(5.6.3)
n> 1
n 
where qij denotes the (i, j)-entry of Q with qij = qij .
Then, for each t > 0 we may apply Theorem 5.5.2, according to which 
there exists a finite class Ct of overlapping directed cycles and positive 
numbers wc (t), c £ Ct, such that
ni = 
wc (t) Jc (i), 
i £ S,
niPij (t) = 
wc(t)Jc(i, j), 
i,j£ S,
cECt
where (ni, i £ S) is the stationary probability distribution of P(•). (Here c 
and c designate circuits and their associated cycles, respectively.)
Denote C |Jt>0 Ct (in Chapter 2 of Part II we shall show that C = Ct, 
t>0). Then (C, wc(t))t>0 is the probabilistic cycle representation of an 
S-state Markov process £ = (£t)t>0 with transition matrix function P(•) 
defined as in (5.6.3) and having Q as transition intensity matrix. The al­
gorithm of representation and (5.6.3) imply that all the weight functions 
w(i,i)(•) are infinitely differentiable. Then
1 
__ tn q(n)
—w(i,i)(t) J(i,i)(i, i) = 1 + ^2 ni , t > 0•
Consequently,
1 
„ tnn(n)
n w („, (t ) = i + £ -n­
n i 
n n •
n>1
Since the transition matrix function P(•) is standard, we have 
w(i,i)(0) = w(i, 0)•
Thus we have
1 
(n)n) 
n (n)
i + i 
tnw (i,i )(0)-i+tn^
+ ni >1. 
n i 
+ >1. n i ,
where w(n))(•) denotes the nth derivative of w)i,i)(•),n > 1.
Hence
(5.6.4)
(5.6.5)
(nn) = 1 w(n) (0) 
qii 
w(i, 0) (i,i)(0),
n > 1 •

5.6 Cycle Representation Theorem for Q-Matrices 
91
On the other hand,
1 
tn w(n)(i,j, 0)
' 
n!
=
n> 1
tnq((inj))
i = j, t > 0, 
(5.6.6)
where w(n)(i,j,t) denotes the nth derivative of w(i,j,t) = nipij(t). Let 
w(i, t) = 52j w(i, j,t), i E S,t > 0. Then w(i, t) = ni, i E S, for all t > 0. For 
i = j, from (5.6.5) and (5.6.6) we deduce that
1
w(i, 0)
w(n)(i, j, 0) = qi(jn),
n> 1.
which proves point (i).
(ii) If we represent the circuit process (£t) t>0 introduced in (i) by the 
class (C,wc(t))t>0 defined in Theorem 5.5.2, the representative weighted 
circuits have the probabilistic interpretation given in this theorem. Then 
the series
tn.(n)
1+„?1 tni-
and
n> 1
tnq(jn) 
n!
i= j,
are equal respectively to w(i,i) (t)/w(i, 0) and
Therefore the series above have the probabilistic interpretations stated in 
the theorem and the proof is complete. 
□
Remark. (i) If we take n = 1 in (5.6.2), we obtain
qij = wh E wch J (i,j), i = j 
k , ' ceCh
(5.6.7)
for h small enough. Thus relation (5.6.7) says that up to a constant (that 
depends on i) the qij ’s, i = j, are approximated in the interval (0,h) by 
the mean increments of the wc(h)Jc(i, j),c E Ch, each of them being the 
mean number of occurrences of a circuit c, containing the edge (i, j), along 
almost all sample paths (^hn(w))n>0.
(ii) The circuit process constructed in Theorem 5.6.1 is, in fact, the 
so-called minimal process corresponding to the given Q-matrix (see K.L. 
Chung (1967)).

6
Spectral Theory of Circuit Processes
Spectral theory of Markov processes was developed by D.G. Kendall 
(1958, 1959a, b) and W. Feller (1966a). The present chapter relies on 
Kendall’s Fourier representation for transition-probability matrices and 
for transition-matrix functions defining discrete and continuous parame­
ter Markov processes, respectively. A specialization of the spectral theory 
to circuit Markov processes is particularly motivated by the essential role 
of the circuit-weights when they decompose the finite-dimensional distri­
butions. For this reason we shall be consequently interested in the spec­
tral representation of the circuit-weights alone. This approach is due to S. 
Kalpazidou (1992a, b).
6.1 Unitary Dilations in Terms of Circuits
A preliminary element of our investigations is an N*-state irreducible 
Markov chain £ = (£n)n>0 whose transition matrix P = (pij, i,j G N*) ad­
mits an invariant probability distribution n = (ni, i G N*), with all ni > 0, 
where N* = {1, 2,...}. That the denumerable state space is N* does not 
restrict the generality of our approach. Let (C^,wc) be the probabilistic 
representative class of directed circuits and weights which decompose P as 
in Theorem 3.3.1. The typical result of the present section is that the sum 
of the probabilistic weights wc of the circuits passing through the edge (i, j) 
has a Fourier representation.
Let 12 = 12(N*) be as usual the Hilbert space of all sequences x = (xi)iEN* 
with xi a complex number such that ||x||2 = (x,x) i |xi|2 < <x. The 

94 
6. Spectral Theory of Circuit Processes
conjugate of any complex number z will be symbolized by z. Further, let 
T be the linear transformation on 12 whose kth component of Tx,x G 12, 
is given by the absolutely convergent series
(T x)k = 
xi(w(i)w(k))-1/2 
wcJc(i, k), 
(6.1.1)
where
w(i) 
wcJc(i), 
i G N*•
Then we may write
2
\\TxW 2
k
k
xi(w(i)w(k))-1/2 
wcJc(i, k)
i 
cec^
|xu|2(1/(w(u))) 
wcJc(u, k)
• 
(1 / ( W ( k ))) 
WcJc (j,k )
< ||x||2,
so that T is a contraction on l2 .
With these preparations, we now prove
Theorem 6.1.1. If (Cxt,wc) is the probabilistic representative class 
of weighted circuits for an irreducible Markov chain whose transition 
matrix P =(pjk,j,k G N*) admits an invariant probability distribution 
n = (nj ,j G N*), with all nj > 0, then
njPjk 
^2 WcJc(j, k) = (w(j)w(k))1 / 2 / ei j(d®),
c -
where the complex-valued Borel measures pjk are supported by the circum­
ference of unit radius and satisfy the Hermitian condition pjk = pkj.
Proof. We shall follow D.G. Kendall’s (1959a) approach to the integral 
representations for transition-probability matrices. Accordingly, we use a 
theorem of B.Sz. Nagy (see B.Sz. Nagy (1953), F. Riesz and B.Sz. Nagy 
(1952), and J.J. Schaffer (1955)) according to which, if T is a linear con­
traction on a Hilbert space H, then it is always possible to embed H as a 
closed subspace in an eventually larger Hilbert space H+ in such a way that 
Tmx = JUmx and (T*)mx = JU-mx, for all x G H and m > 0, where U 

6.1 Unitary Dilations in Terms of Circuits 95
is a unitary operator on H+ and J is the projection from H+ onto H. P.R. 
Halmos called U a unitary dilation of T.T* denotes as usual the adjoint 
operator of T.
We here apply Nagy’s theorem to the contraction T defined by (6.1.1) 
and to the space H = l2 . Accordingly, there exists a unitary dilation U 
defined on a perhaps larger Hilbert space H+ such that
JUmJ = TmJ, 
JU-mJ = (T*)mJ,
for any m =0, 1, 2,. .., where J is the orthogonal projection from H + onto 
H. From the proof of the Nagy theorem the space H + is defined as the 
direct sum of countably many copies of H.
Let us consider u(j) the element of H defined by
(u (j)) k = 6jk, 
where 6 denotes Kronecker’s delta. Then we have
(Tmu(j),u(k)) = (Umu(j),u(k)), 
(u(j),Tmu(k)) = (U-mu(j),u(k)).
Hence
wcJc(j, k) = (w(j)w(k))1/2(Uu(j),u(k)).
cEC^
We now apply Wintner’s theorem (see F. Riesz and B.Sz. Nagy (1952)) 
according to which the unitary operator U is uniquely associated with a 
(strongly) right-continuous spectral family of projections {Eg, 0 < 6 < 2^} 
with E0 = O and E2n = I such that
2 2 n
U = 
eigdEg .
0
Finally,
2 2 n
wcJc(j, k) = (w(j)w(k))1/2 
eigd(Egu(j), u(k))
c&«, 
J°
= (w(j)w(k))1 /2 £ ei j(d6),
where ^jk are complex-valued measures satisfying the properties referred 
to in the statement of the theorem. The proof is complete. 
□

96 
6. Spectral Theory of Circuit Processes
6.2 Integral Representations of the Circuit-Weights 
Decomposing Stochastic Matrices
This section is a sequel to the previous one. We shall be concerned with 
the same irreducible Markov chain £ = (£n)n introduced at the beginning of 
Section 6.1, save for the state space which is now considered to be the finite 
set N* = {1, 2,..., v}, v > 1. Then the deterministic-circuit-representation 
theorem (Theorem 4.2.1) asserts that the transition probabilities pjk,j, k G 
N*, of £ have the following decomposition in terms of the directed cir­
cuits of a finite ordered class C = {c 1,..., cm}, m > 1, and of their positive 
weights wc :
njPjk = 
wJc(j, k), 
j,k G N*, 
(6.2.1)
CC
where n = (nj ,j G N*) denotes the invariant probability distribution of £. 
The directed circuits c =(i1,...,ip,i1),p > 1, to be considered will have 
distinct points i1 ,.. .,ip .
The principal theorem asserts that an integral representation can be 
found for the deterministic circuit weights wc occurring in the decomposi­
tion (6.2.1). More specifically, we have
Theorem 6.2.1. For any circuit c occurring in the decomposition (6.2.1) 
there exist a finite sequence (j1,k1),...,(jm,km) in the edge-set of C and 
a Hermitian system {vj 1 k 1, .. ., vjmkm } of Borel measures supported by the 
circumference of unit radius such that wc = wcr, for some r =1,...,m, 
has the expression
Wc 1 = (w(j 1)w(ki))1 /2 £ eiVj 1 k 1 (dd) if r = 1,
Wcr = ( W (jr ) W ( kr ))1 / 2 
ei Vjk (de)
- 
wcsJcs(jr,kr) 
if r =2,...,m m>1.
s=1
Proof. We shall use the arguments of Theorems 1.3.1 and 4.2.1. 
In this direction, let j0 be arbitrarily fixed in N*. Since w (j, k) = 
nj pjk is balanced and ^2k w (j 0 ,k) > 0, we can find a sequence 
(j0, u0), (u0,u1),...,(un-1,un),... of pairs, with ul = um for l = m,on 
which w(•, •) is strictly positive. Choosing the um, m = 0, 1, 2,..., from the 
finite set N*, we find that there must be repetitions of some point, say 
j0 .Let n be the smallest nonnegative integer such that un = j0 . Then, if 
n > 1,c1 : (j0,u0), (u0,u1),...,(un-1,j0) is a circuit, with distinct points 
j0,u0,...,un-1 in Nv* , associated to w.

6.2 Integral Representations of the Circuit-Weights 97
Let (j1 ,k1) be the pair where w(j, k) attains its minimum over all the 
edges of c1 , that is,
w(j1,k1) = minw(j, k). 
c1
Put
wc1 = w(j1, k1)
and define
w 1( j, k ) = w (j, k ) - Wc 1 Jc 1 (j, k ) •
The number of pairs (j, k) for which w1 (j, k) > 0 is at least one unit 
smaller than that corresponding to w(i,j). If w 1 = 0 on N*, then w(j, k) = 
wc1 Jc1 (j, k). Otherwise, there is some pair (j, k) such that w1(j, k) > 0. 
Since w1 is balanced we may repeat the same reasoning above, according 
to which we may find a circuit c2 , with distinct points (except for the 
terminals), associated to w1.
Let (j2, k2) be the edge where w1(j, k) attains its minimum over all the 
edges of c2, that is,
w 1(j2 ,k2) = min w 1(j, k) • 
c2
Put
wc2 = w1(j2, k2)
and define
w2(j, k) = w1(j, k) - wc2Jc2(j, k)
= w(j, k) - wc 1 Jc 1 (j, k) - wc2 Jc2 (j, k) •
Then w2(j1,k1) = w2 (j2, k2) = 0. Since N*v is finite, the above process will 
finish after a finite number m = m(j0 ) of steps, providing both a finite or­
dered class C = {c 1, • • •, cm} of directed circuits, with distinct points (ex­
cept for the terminals), in Nv* and an ordered collection of positive numbers 
{wc 1, • • •, wcm } such that
w(j,k)-^2 wck Jck(j,k), 
j,k G N*•
k=1
Moreover, the strictly positive numbers wck , called as always circuit 
weights, are described by a finite sequence of edges (j 1, k 1), • • •, (jm, km) 
and the recursive equations
wc1 = w(j1, k1)
wc2 = w(j2, k2) - w(j1, k1)Jc1 (j2, k2), 
(6.2.2)
wcm   w(jm,km) 
w wcs Jcs (jm,km ) •
s=1

98 
6. Spectral Theory of Circuit Processes
Consider now the operator V mapping x G 12(N*) into the vector Vx, 
where the kth component of Vx is given by the sum
(V x)k = 
xj (w(j)w(k))-1/2 
wcJc(j, k),
with w(j) = Ec wJc(j).
Then, following the proof of Theorem 6.1.1 we can extend V to a uni­
tary operator U for which there exists a Hermitian collection of spectral 
measures {vjk} such that
w(j, k) = (w(j)w(k))1 /2 <j> eiVjk(dd),
for all (j, k), and so, for (j1,k1),...,(jm,km) occurring in (6.2.2). Accord­
ingly, the weights given by equations (6.2.2) have the desired integral rep­
resentation. The proof is complete. 
□
6.3 Spectral Representation of Continuous 
Parameter Circuit Processes
6.3.1. 
Consider an N*-state irreducible positive-recurrent Markov process 
i = (it)t>0 whose transition matrix function P(t) = (pij (t),i,j G N*) is 
stochastic and standard, that is,
Pij (t) > o , E Pij (t ) = 1, 
j
pij (t + s)= 
pik(t)pkj(s),
lim pij (t) = pij (0) = 6ij, 
t^ 0 _
for all i, j G N* and all t, s > 0. Let Ht = (int)n>0 be the discrete t-skeleton 
chain of i ,wheret> 0.
Consider the (weakly continuous) semigroup {Tt, t > 0} of contractions 
associated with P = (P(t))t>0 . Then this semigroup may be expressed in 
terms of the probabilistic circuit representative (C, wc(t))t>0, provided in 
Theorem 5.5.2, as follows:
(Ttx)k = E.Xi(w(i)w(k))“1 /2 Ewc(t)Jc(i,k), k G N*, 
(6.3.1)
for all x G 12(N*), where w(i) Ec^cwc(t) Jc(i) for any i G N*.
Theorem 6.3.1. Let P(t) = (pij (t), i, j G N* ) be a standard stochastic 
transition matrix function defining an irreducible positive-recurrent Markov 
process i =(it )t>0 whose invariant probability distribution is denoted by 

6.3 Spectral Representation of Continuous Parameter Circuit Processes 99
n = (ni, i G N*). Then for each t > 0 the transition probabilities pjk (t) can 
be written in the form:
nj Pjk (t) = (w (j) w (k ))1 / 2 [ 
ei^jjk (dX),
- — tt
where {jjk ,i,k G N*} is a Hermitian collection of complex-valued totally 
finite Borel measures carried by the real line.
Proof. The main argument of the proof is due to D.G. Kendall (1959b). 
Correspondingly, we apply a theorem of B.Sz. Nagy according to which we 
can embed H = 12(N*) as a closed subspace in an eventually larger Hilbert 
space H+ in such a way that for all t > 0
JUtJ = TtJ,
JU—tJ = Tt* J,
where J is the orthogonal projection from H+ onto H, Tt* is the adjoint 
operator of Tt, and {Ut, —to < t < to} is a strongly continuous group of 
unitary operators on H+ . (The smallest such collection {H+, Ut, H} is 
unique up to isomorphisms). Further we apply a theorem of M.H. Stone 
(see F. Riesz and B.Sz. Nagy (1952), p. 380) according to which there 
exists a right-continuous spectral family {EX, —to < X < to} of projection 
operators such that
(Utx,y) = [ 
elXtd(E\x,y), 
x,y G H +,
—tt
for all real t.
We have
(Ttx, y) = (JUtx, y) = (Utx, Jy) = (Utx, y), 
x,yG H = l2 .
Furthermore,
nj Pjk (t) = (w (j) w (k ))1 / 2( Utu (j) ,u (k))
Z
+tt
eiXtd(Exu(j),u(k)), 
t > 0,
tt
where the vector u(j) lies in l2 (N* ) and is defined by
(u (j)) k = Sjk.
Then, by virtue of Theorem II of D.G. Kendall (1959b), we may write
Z
+tt
etXtyjk (dX), 
t > 0,
tt
where the complex-valued totally finite Borel measures j/jk ,j,k G N*, are 
supported by the real line and satisfy the Hermitian condition ykj = jajk. 
The proof is complete. 
□

100 
6. Spectral Theory of Circuit Processes
6.3.2. Consider the semigroup {Tt,T > 0} of contractions associated to 
P = (P(t))t>0 by (6.3.1), with P(t) = {pj(t),i,j G N*} satisfying the hy­
potheses of the previous paragraph. D.G. Kendall (1959b) called this semi­
group self-adjoint if for each t > 0 the operator Tt is a self-adjoint one, that 
is, if the following “reversibility” condition
nj- Pjk (t )= nk pkj (t), 
j,k =1, 2,..., 
(6.3.2)
is satisfied, where (n = nj ,j = 1, 2,...) denotes the invariant probability 
distribution of P (t).
On the other hand, the existence of the probabilistic circuit-coordinates 
wc(t), c G C, in the expression (6.3.1) of the contractions Tt,t > 0, inspires 
the conversion of the edge-reversibility condition (6.3.2) into a circuit­
reversibility condition as follows:
Theorem 6.3.2. The semigroup {Tt, t > 0} of contractions defined by 
(6.3.1) is self-adjoint if and only if the probabilistic weight functions wc(•) 
satisfy the consistency equation
wc(t)=wc-(t), 
t> 0,
for all directed circuit c G C, where c- denotes the inverse circuit of c.
Proof. The proof follows combining Theorem 5.5.2, Minping Qian et al. 
(1979, 1982), and Corollary 6 of S. Kalpazidou (1990a) (see also Theorem 
1.3.1 of Part II). 
□
6.3.3. An integral representation for the circuit-weight functions wc(t) 
that decompose the transition matrix function P (t) can be found if pre­
liminarily we express all wc(t) in terms of the pij (t)’s. So, applying the 
argument of Theorems 6.3.1 and 6.2.1 to each t -skeleton chain, we obtain
Theorem 6.3.3. For any t>0 and any circuit c occurring in the decom­
position (6.2.1) of the matrix P(t) indexed by N* = {1, ...,v} there exist 
a finite sequence (j1,k1),...,(jm,km) of edges and a Hermitian system 
{vjnkn, n = 1, ■ ■ ■, m} of complex-valued totally finite Borel measures sup­
ported by the real line such that wc(t) = wcr (t), for some r =1,...,m, has 
the expression
Z
+ <X
etXtVji ki (dX) 
if r = 1,
-rc>
Z
 + tt
elXtVjr kr (dX) 
-^
- 
wcs(t)Jcs(jr, kr) if r =2,...,m, 
m> 1.
s=1

7
Higher-Order Circuit Processes
Higher-order circuit processes are homogeneous discrete parameter Markov 
processes with either at most a countable set of states or an arbitrary set 
of states, where the length of the past in the Markovian dependence is ex­
tended from 1 to m>1, and the transition law can be decomposed by a col­
lection of geometrical elements, the “circuits”, into certain positive weights.
Our presentation will first introduce the concept of a higher-order Markov 
chain with finite state space which is often called a multiple Markov chain. 
Then, we shall see how to construct multiple Markov chains by collections 
of directed circuits and positive weights. In this case, the multiple Markov 
chains will be called multiple circuit chains or higher-order circuit chains. 
The converse direction gives rise to a cycle representation theory for higher- 
order Markov chains.
7.1 Higher-Order Markov Chains
Let S be a finite set which contains r> 1 elements. An S-valued sequence
£-1, £0, £ 1, . . . , £n,...
of random variables is called a homogeneous Markov chain of order two (for 
short double Markov chain) with state space S if for any n =0, 1, 2,... and 
i- 1,..., in +1 G S we have
Prob(Zn+1 = in+1 /Zn = in ,...,Z-1 = i-1 )
= Prob(Zn+1 = in+1 /Zn = in , Zn-1 = in-1 ), 

102 
7. Higher-Order Circuit Processes
whenever the left member is defined, such that the right member is indepen­
dent of n. As we see, the previous equations express the Markov property 
where the length of the past m is equal to 2. The above definition may 
be extended to any length of the past m> 2, but in what follows, with­
out any loss of generality, we shall be concerned with the case m =2. The 
probability
Prob(Zn +1 = k/Zn = j, Zn-1 = i)
is called the one-step transition probability from the pair (i, j) of states 
to state k. It is easily seen that if Z = (Zn)n>-1 is a double Markov chain 
then
(Z-1, Z0), (Z0,Z1),...,(Zn,Zn+1),...
is a simple Markov chain.
On the other hand, a first glance at the chains Z and Z = (Zn)n>0 with 
Zn = (Zn- 1, Zn), can mislead to the impression that higher-order Markov 
chains reduce to simple Markov chains. It is Gh. Mihoc (1935, 1936) who 
first pointed out that the theory of higher-order Markov chains differs from 
that of simple Markov chains. M. Iosifescu (1973) strengthened this stand­
point by showing that the stochastic properties of Z and Z above are not 
identical. For instance, if a state (i, j) is recurrent in chain Z, so are its 
components i and j in chain Z, but a state i can be recurrent in chain Z 
without being a component of a recurrent compound state (i, j) in chain Z. 
As we shall see below, there are a few cases when the higher-order Markov 
chains can be studied by making use of properties of the attached simple 
chains.
Another context for investigating higher-order Markov chains is that of 
random systems with complete connections (see M. losifescu (1963a, b)- 
(1990), M.F. Norman (1968a, b) and (1972), T. Kaijser (1972-1986), M. 
losifescu and P. Tautu (1973), S. Kalpazidou (1986a, b, c), (1987a), and 
others. The reader may find extended references on this type of stochastic 
processes in M. Iosifescu and S. Grigorescu (1990).
Formally, a random system with complete connections is a particular 
chain of infinite order. Chains of infinite order have been considered by 
W. Doeblin and R. Fortet (1937), J. Lamberti and P. Suppes (1959), M. 
losifescu and A. Spataru (1973), S. Kalpazidou (1985), P. Ney (1991), 
S. Kalpazidou, J. and A. Knopfmacher (1990), P. Ney and E. Nummelin 
(1993), Ch. Ganatsiou (1995a, b, c), and others.
In 1935, O. Onicescu and Gh. Mihoc generalized the Markovian depen­
dence of order m to chains with complete connections, i.e., those chains 
(Xn ) for which the conditioned probability
Prob(Xn+1 = in+1/Xn = in,...,X0 = i0) 

7.1 Higher-Order Markov Chains 103
is a given function ^inin+1 of the conditioned probabilities
Prob(Xn = i/Xn-1 = in-1, ...,X0 = io), 
i e S,
for every i0,..., in+1 e S,n > 0. (The simple Markov chains are obtained 
when the functions <pij ,i,j e S, are constant, namely <pij = pij.) The reader 
can find more details in O. Onicescu and G. Mihoc (1943), O. Onicescu, 
G. Mihoc and C.T. Ionescu Tulcea (1956), G. Mihoc and G. Ciucu (1973), 
M. losifescu and P. Tautu (1973). A. Leonte (1970), and others.
Let pij,k, i, j, k e S, denote the one-step transition probabilities of the 
double Markov chain £, that is,
Pij,k = P(£ 1 = k/£0 = j, £ 1 = i), i,j,k e S,
where S contains r elements. Let also pi(jn,)k denote the n-step transition 
probability from pair (i, j) of states to state k in chain £, that is,
pi(jn,)k = P(£n = k/£0 = j, £-1 = i), n> 1,
with p(0), = 6jk, i,j, k e S, where 6 symbolizes Kronecker’s delta.
ij,k
As already seen, the chain Z = (£n- 1 ,£n)n>0 is a simple S x S-state 
Markov chain whose transition probabilities are symbolized by qij,xy , that 
is,
qij,xy = P(£1 = y, £0 = x/£0 = j, £-1 = i),
for all (i, j), (x, y) e S x S.
Then, if qi(jn)x , with (i, j), (x, y) e S x S, denotes the n-step transition 
,y
probability in chain Z, we have
qij,xy = pij,y 6jx
and
(n+s) 
(n) (s) ijke S n > 1 s> 0 
(711)
pij,k = 
qij,xypxy,k, 
i,j,ke S, n > 1,s> 0. 
(7.1.1)
(n) 
(n) 
n 
(n)
Denote P = (pij,k ,i,j,k e S),n > 0, and let Q = (qij,xy ;(i, j, (x, y) e
S x S). Then relations (7.1.1) are written in terms of matrices as follows:
P (n+s) =QnP(s), 
n> 1, s>0. 
(7.1.2)
In particular, we have
P(n) = QnP(0),
with P(0) = (Ir,... ,Ir)' where Ir is the unit matrix of order r (= card S) 
which is repeated r times in the expression of P(0) .
I. 
Vladimirescu (1982-1990) introduced the following definitions:

104 
7. Higher-Order Circuit Processes
Definition 7.1.1. A state j is accessible from state i(i ^ j) if for any 
u G S there is n = n(u, i,j) such that p(nj > 0. If i ^ j and j ^ i, we say 
that states i and j communicate and write i ^ j.
The relation ^ divides the set {i G S/i ^ i} into (equivalence) classes, 
called classes of states.
Introduce
T(i, j) = {n > 1 /pUnj > 0 for any u G S}, 
i, j G S,
T(i) = T(i, i), 
iG S.
Definition 7.1.2. A state j is fully accessible from state i if T(i, j) = 0. 
If T(i) = 0 and the greatest common divisor di of all natural numbers n 
which belong to T(i) is larger than one, then we say that i is a periodic 
state of period dj . When either T(i)=0 or di=1, we say that i is an 
aperiodic state.
Vladimirescu (1984) proved that if s G T(i,j) and n G T(j, k), then s + 
n G T(i, k).
Definition 7.1.3. A double Markov chain f = (fn)n>- 1 satisfies the con­
dition of full accessibility if for any states i, j G S such that i ^ j we have 
T(i,j) = 0.
Definition 7.1.4. The double Markov chain f = (fn)n>- 1 is irreducible if 
the set of all states of f is the unique (equivalence) class. If there exists 
n0 > 1 such that P(n0) > 0, we say that the chain f is regular.
The following theorem gives a necessary and sufficient condition for a 
double Markov chain to be regular (I. Vladimirescu (1985)):
Theorem 7.1.5. The double Markov chain f = (fn)n>- 1 is regular if and 
only if the following conditions are fulfilled:
(i) 
irreducibility;
(ii) 
full accessibility; and
(iii 
) all states are aperiodic.
Proof. If the chain f is regular then conditions (i)-(iii) follow immediately. 
Let us prove the converse. From (i) we have that j ^ j for any j G S. 
The latter along with (ii) implies that T(j) = 0,j G S. Since all states are 
aperiodic, dj =1 for allj G S.
On the other hand, for any pair v, s G T(i) we have 
p(v+s) = 
q(v) p(S>. > 
q(v) p((s)
pUi,i 
qUi,xtpxt,i 
qUi,xipxi,i
> qU(vi,)zipzsi,i > 0

7.1 Higher-Order Markov Chains 105
for some z e S (since for any u e S, 0 < p„sii = FLSq q,sisi\ri, that is, there 
u 
U\ u,b ,b x x x^ u LUb,X
exists z = z(u, i, s) e S for which qus
si),zi > 0). Therefore v + s e T (i) (
is closed with respect to addition). Then there is an ni such that n e T (i) 
for any n > ni. Taking into account (i) and (ii), we see that T(i,j) = 0 for 
any i, j e S.
Let mij be arbitrarily fixed in T(i,j). Then mij + nj e T(i, j). Also, if 
n0 = maxi,jes(m) + n)), then n0 — mi) > n) for any i, j e S. Thus, by the 
very definition of nj, we have n0 - mij e T(j) for any i e S. Then, from 
relations: mij e T(i, j) and n0 — mij e T(j) we deduce that mij +(n0 — 
mij) e T(i, j), that is, n0 e T(i, j) for any i,j e S. Finally, the definition 
of T(i, j), i,j e S, shows that pUn0) > 0 for any u e S. Hence the chain £ is 
regular. 
□
That conditions (i)-(iii) of Theorem 7.1.5 are independent follows from 
the following examples:
Example 7.1 .1. Let £ be the double Markov chain whose state space 
is S = {1, 2} and transition probabilities are given by p11,1 = 1,p12,1 = 
2,P21,1 = 1 ,P22,2 = 1. Then £ is aperiodic (d 1 = d2 = 1) and satisfies the 
full accessibility condition. However the chain £ is not irreducible (there 
are two classes C1 = {1} and C2 = {2}).
Example 7.1 .2. The double Markov chain £ whose state space is S = 
{1, 2} and with transition probabilities p11,2 = p12,1 = p21,1 = 1 is irre­
ducible and satisfies the full accessibility condition, but £ is periodic 
(d1 = d2 =2).
Example 7.1 .3. Let £ be the double Markov chain with states in S = 
{1, 2, 3}, whose transition probabilities are as follows: p11,3 = p12,3 = 
p133 = p21,2 = p22,1 = p23,1 = p31,2 = p32,3 = p33,2 =1. Th
ducible and aperiodic but it does not satisfy the full accessibility condition 
(for instance, 1 ^ 1 while T(1) = 0)
We have
Proposition 7.1.6. If the chain Z = (Zn) n>0 is regular, then the chain 
£ = (£n)n>- 1 is regular as well.
Proof. If Z is regular, there exists n0 > 1 such that Qn0 > 0. Then equation 
PUn0) = Qn0 P,0) = Qn0 (Ir,..., Ir)' (where Ir, is repeated r times) implies 
that Pn0 > 0. 
□
As I. Vladimirescu (1985) pointed out, the converse of Proposition 7.1.6 
is not in general valid. For instance, the double Markov chain on S = {1, 2} 

106 
7. Higher-Order Circuit Processes
whose transition probabilities are p11,1 = a, p12,2 = p21,2 = 1 and p22,1 = b, 
where a,b G (0, 1), is regular (P(3) > 0) but the attached simple Markov 
chain Z is not regular. The same author proved that if £ = (£n) is regular, 
then there exists a probability (row) distributionp* = (p 1,... ,pr) > 0 such 
that
lim P(n) = up*, 
(7.1.3)
(i) We look for the set C(k) of all circuits which pass through k, i.e.,
those circuits which comprise g, a, and b as consecutive points. In
case C(k) is not empty, then the passages to other states are allowed
and we may go on with the following steps.
(ii) We consider the set C(k, x) of all circuits which pass through
(k, x),x G S, according to Definition 1.2.2 (of Chapter 1). In case
C(k, x) is empty then no passage to x will take place.
n—>^>
where u is the (column) vector whose components are all equal to 1, and r 
is the cardinal number of the state space of £.
Furthermore, if Z is regular, then (7.1.2) implies that
lim P(n) = HP(i)
 * * * * * (0), 
n—>^>
where H is the positive matrix of order r2 given by the equation H = 
limn x Qn.
Definition 7.1.7. The distribution p* provided by (7.1.3) is called the 
limiting distribution of the S-state double Markov chain £. When p1 = 
p2 = • • • = pr = 1 /r (r = card S), we say that p* is the uniform limiting 
distribution.
7.2 Higher-Order Finite Markov Chains Defined 
by Weighted Circuits
7.2.1. Consider the set S = {a, b, c, d, e, f, g} and the directed circuits c1, c2, 
and c3 as in Figure 2.1.1 of Chapter 2. Observe the passages of a particle 
through the points of c1 ,c2 ,andc3 at moments one unit of time apart. 
Assign each circuit ci to a strictly positive weight wci . Then we may define 
transition probabilities of a chain £, from a past history with a given length 
m > 1 to some state of S using the circuits ci, i = 1, 2, 3, and the positive 
weights wci .
For instance, if such a history is k =(g, a, b), (m = 3), that is, £n-2 = 
g, £n-1 = a, £n = b with n =1, 2,. .., we are interested in defining the tran­
sition probabilities from k to x G S . Namely, to calculate these conditioned 
probabilities we follow the steps below:

7.2 Higher-Order Finite Markov Chains Defined by Weighted Circuits 107
(iii) The transition probabilities from k to x G S are expressed in terms 
of the circuit-weights assigned to the circuits of C(k) and C(k, x)by 
the relations
P(Zn +1 = d/Zn = b, Zn —1 = a, Zn-2 = 9)
_ Hc’CC(k,d) w'' 
wc 1
Yc(k%( k ) wc' 
wc 1 + wc 2 ’
P(Zn +1 = c/Zn = b,Zn-1 = a, Zn-2 = 9)
_ Hc'tf(k,c) wc' _ 
wc2
( k ) wc' 
wc 1 + wc 2 '
P(Zn+1 = x/Zn = b, Zn-1 = a, Zn-2 = g)=0, 
xG S\{c, d}.
Let us now change the time-sense, seeing the retroversion of the film 
of observations along the reversed circuits of Figure 2.1.1 until the nth 
moment, that is,..., Xn +1, Xn• Note that the circuits which enter a vertex 
are the same as those which leave it in the corresponding reversed circuits. 
Then we find that the transition probabilities of the chain (Xn ) from k- = 
(b, a, g) to state x G S satisfy the equations
Prob(Zn = x/Zn-1 = b, Zn-2 = a, Zn-3 = g)
= Prob(Xn = x/Xn+1 = b, Xn+2 = a, Xn+3 = g), (7.2.1)
for all n =1, 2,..., where the transition probability from k- to x in chain 
Xn , Xn+1 ,... is defined by using, instead of the classes C(k) and C(k, x) 
occurring at steps (i) and (ii) above, the classes C- (k-) and C- (x, k-) 
which contain all the reverses c— of the circuits c G C passing through k- 
and (x, k-), respectively. Namely,
Prob( Xn = xlXn +1 = b, Xn +2 = a,Xn+3 = 9) = 
wc- / 
12 W<--
where wc> = wc ,c G C.
The latter equation reveals that instead of a reversible random sequence 
of observations we have a dichotomy into two sequences Z = (Zn)n and 
X = Xn)n, called circuit chains of order three, which keep not only the 
Markovian nature of the transition law, but also the transition law is main­
tained numerically. In this regard we have to study the behavior of the pair 
(Z, X) as a whole (see S. Kalpazidou (1988a)).
We note that the “balance” of the “past” and the “future” with respect 
to the “present” requires a formal expression in terms of certain functions 
wl(•) and wr(•) on the “left” sequences k = (k 1 ,k2,k3) and on the “right” 
sequences k- = (k3,k2,k1), respectively, such that the following equations 

108 
7. Higher-Order Circuit Processes
are verified:
(P1): 
wi(k) = £wi(k,i) = £wi(i,k),
wr(k-) = 
wr(i,k-) 
wr(k-,i),
(P2): 
wi (k) = wr(k-).
We call (P1) and (P2) the balance properties for the functions {w1, wr }.As 
pointed out in S. Kalpazidou (1988a), the properties (P1) and (P2) cannot 
be confused since the first property is mainly concerned with the existence 
of invariant measures while the second one with equations (7.2.1).
7.2.2. Now, we shall give a rigorous presentation of the ab ove heuristics 
following S. Kalpazidou (1988a).
Let m>1. Let S be any finite set consisting of more than m elements and 
C a collection of overlapping directed circuits in S which contains, among 
its elements, circuits with periods greater than m - 1. Suppose that there 
exist circuits which intersect each other in at least m consecutive points. 
The previous assumptions are not necessary; by them, we only avoid simple 
cases for our future models.
Associate a strictly positive number wc to each c G C. Suppose the 
circuit-weights wc, c G C, satisfy the following consistency conditions:
wc ◦ ti = wc, 
iG Z,
where ti is the translation of length i on Z as defined in Section 1.1 of 
Chapter 1. Denote by C- the collection of all the inverse circuits c- , when 
c G C. Put
wc- = wc ,cG C.
A sequence of length m is understood to be any ordered sequence 
k = (k(v — m + 1), .. ., k(v — 1), k(v)) G Sm, v G Z. Put k- = (k(v), k(v — 
1),. .., k(v — m +1)).
Let Jc(k, i)(Jc- (i, k)), and Jc(k)(Jc- (k-)) be the passage functions 
associated with c(c-) according to Definition 1.2.2. Then according to 
Lemma 1.2.3 the passage functions Jc and Jc- satisfy the following balance 
equations:
(P1) 
(i) 
Jc(k) = 
Jc(k, i)= 
Jc(j, k),
(ii) Jc- (k-) = E Jc- (k-,i) = E Jc- (j, k-), 
(7.2.2)
(P2) 
Jc(k) = Jc- (k-),
for any k =(i1,...,im),k- =(im,...,i1), where i1,...,im G S.

7.2 Higher-Order Finite Markov Chains Defined by Weighted Circuits 109
Define
wt(k, i)= 
wcJc(k, i),
wr (i, k-)= 
wc- Jc- (i, k-),
wl(k) = 
wcJc(k),
ceC
wr (k-) = 52 wc— Jc— ( k— ),
for any k = (i 1,..., im), k— = (im,... ,i 1) and i G S, where i 1,..., im G S.
Then we have
Lemma 7.2.1. The functions wi(•, •) wi(•),wr(•, •),wr(•), satisfy the bal­
ance equations (7.2.2) (fi 1) and (fi2).
Let us now introduce the sets
Wl = {k : k = (k(v - m + 1),. .., k(v - 1), k(v)) G Sm, v G Z and
Jc(k)=0forsomec G C} 
(7.2.2')
and
Wr = {k— G Sm : k G Wl}. 
(7.2.2")
Consider the functions ui and hi defined as
ul(k, i) = ul((k(v - m + 1), k(v - m +2),...,k(v)),i)
= (k(v - m +2),. ..,k(v),i), 
(7.2.3)
hl(j, k) = hl(j, (k(v - m +1),...,k(v - 1), k(v)))
=(j, k(v - m +1),...,k(v - 1)) 
(7.2.4)
for any k G Wl ,andanyi, j G S . Also, consider the functions ur and hr 
defined as
ur(k—, i) = ur((k(v),..., k(v - m + 2), k(v - m + 1)), i)
=(i, k(v),...,k(v - m + 2)), 
(7.2.5)
hr(j,k—) = hr(j, (k(v), k(v - 1),...,k(v - m + 1)))
= (k(v - 1),...,k(v - m + 1),j) 
(7.2.6)
for any k— G Wr ,andanyi, j G S.
With these preparations we are now ready to perform our original task 
which is the definition of the mth order Markov chains using weighted 
circuits. To this end, assume that Wl and Wr are disjoint sets. Let 
N = {1, 2,...}. We appeal to the Kolmogorov theorem according to which 
there exist two Markov chains Z = (Zn)n and n = (nn)n whose transition 

110 
7. Higher-Order Circuit Processes
probabilities are defined as
P1( Zn +1 = Ul ( k,i )/Zn = k )
wl (k, i) 
wl (k)
0,
if there is c G Csuch that Jc(k) • Jc(ul (k, i)) = 0;
(7.2.7)
otherwise;
P2( nn = Ur ( k-,i )/On +1 = k- )
w (i k )
------- ———, if there is c- G C- such that Jc- (k-) • Jc- (ur (k-,i)) = 0; 
wr(k-) 
- 
-
0, 
otherwise;
(7.2.8)
for all n G N, k G Wl ,k- G Wr ,andi G S.
Remarks
(i) The definitions of the Markov chains Z and n rely upon the classical 
Kolmogorov construction. The reader may follow another approach using a 
common probability space for both chains Z and n . Also, one may consider 
the general case where the sets Wl and Wr have common elements.
(ii) Equations (7.2.7) show that (one-step) transitions from state k = 
(k (v — m + 1) ,k (v — m + 2) ,...,k (v)) G Wl to states ul (k, i ) = ( k (v — 
m + 2),..., k(v), i), i G S, are allowed only if there exists a circuit of C 
which passes simultaneously through k and ul (k, i), or equivalently through 
(k, i). On the other hand, it follows from the proof of Lemma 1.2.3 that 
c passes through (k, i) if and only if c- passes through (i, k-). Therefore 
equations (7.2.7), (7.2.8) and the balance property (^2) imply that the 
transition probability from k to ul (k, i) in chain Z is equal to the transition 
probability from k- to ur(k-,i) in chain n.
Let us now consider a recurrent class E of the chain Z = (Zn)n Then, 
from the above remark (ii) the set
E- = {k- G Wr : k G E} 
(7.2.9)
is a recurrent class for the chain (nn)n. Moreover, from definition (7.2.7) of 
transition probabilities of Z it follows that if k G E and Jc(k) = 0, i.e., k = 
(c(v — m +1),...,c(v)) (we may equivalently consider c ◦ tj,j G Z, instead 
of c—see Definition (1.1.1)) then k' = (c(v — m + s + 1),...,c(v),c(v + 
1),...,c(v + s)), s =1,...,m— 1, are states of E, too.
Furthermore, we can prove
Proposition 7.2.2.
(i) The restrictions of the Markov chains Z = (Zn)n and n = (nn)n to 
the recurrent classes E and E- , respectively, have unique stationary

7.2 Higher-Order Finite Markov Chains Defined by Weighted Circuits 111
distributions pE and pE- , respectively, defined by
{
wl (k ) 
k^,EwE wl (k)
0,
if k / E;
if k /E;
{
wr (k-)
E k-eE- wr (k-) ,
0,
(7.2.10)
if k- / E- ;
if k- // E.
(ii) pE(k) = pE-(k-), for all k / E.
(7.2.11)
Proof. (i) We give the proof for the chain (Zn)n, since that concerning the 
chain (nn)n is completely similar. Thus, we shall show that the distribution 
PE is the unique solution of the equation
pE (u)= 
pE (k)P (k,u), u/ E,
where u = (u(v — m + 1)...., u(v)), for some integer v, and
{
wl (k, u (v)) 
if k / E,u = ul (k, u (v)) and there is c / C
wl (k) 
, such that Jc(k) • Jc(ul (k,u(v))) = 0;
0, 
otherwise.
When u = ul (k, u(v)) for all k, both members of equation (7.2.11) are zero. 
Otherwise,
1 
1 
w l(ki(k,u(v))
_”k(k)P(k,u) = 
IE 
wl(k)^k~
Ul (k, u(v))= u
= 
—1—7TS 
wL 
wl(k,u (v)).
^EEE wl (k) 
k^E
ul (k, u(v))=u
Let us now calculate the sum ^2k^E ul(k u(v))=u wl(k, u(v)). If k = (k(v — 
m +1),...,k(v)) then from ul (k, u(v)) = u it follows that k = (k(v — m + 
1), u(v — m +1),. ..,u(v — 1)). Therefore in view of the balance property 
(7.2.2) (P 1)(i), we have
wl(k,u(v))
keE 
ul (k,u(v))=u
= 
wl((k(v—m+1),u(v—m+1),...,u(v—1)),u(v))
k(v-m+1)
= wl (u(v — m +1),...,u(v — 1), u(v))
= wl (u).

112 
7. Higher-Order Circuit Processes
Thus, pE = (pE(u))uEE is a solution of equation (7.2.11). The uniqueness 
of pE is an immediate consequence of the ergodicity of the Markov chain 
(Zn) n restricted to the recurrent class E.
Finally, (ii) follows from the balance property (02). 
□
Remark. Proposition 7.2.2 shows that the balance property (0 1) is neces­
sary for existence of stationary distributions on symmetrical sets connected 
by (7.2.9) while the balance property (02) is necessary for their numerical 
equality.
Let us further consider two Markov chains (£n) n and (nn)n with transi­
tion probabilities
P1( Zn = hl (j,k)/Zn+1 = k) 
wl(j,k) 
wl (k)
0,
if there is c G Csuch that Jc(k) • Jc(hl (j, k)) = 0; 
otherwise;
(7.2.12)
P2( nn+1 = hr(j,k)/nn = k-) 
wr (k-,j) 
wr(k-) ,
0,
if there is c G Csuch that Jc- (k-) • Jc- (hr (j,k-)) =0;
otherwise;
(7.2.13)
for any n G N, k G Wi ,k- G Wr ,andi G S.
Thus we may notice from equation (7.2.12) that transitions from state 
k = (k(v — m + 1), .. ., k(v — 1), k(v)) G Wl to states hl(j, k) = (j, k(v — 
m + 1),... ,k(v — 1)),j G S, are allowed only if there exists a circuit in 
S which pass simultaneously through k and hl (j, k). Connections between 
(Zn) n and (Zn) n, and (nn) n, and (nn) n, respectively, are revealed in the 
following statement:
Proposition 7.2.3. The restrictions of the Markov chains (Zn)nEN and 
(nn)nEN to the recurrent classes E and E-, respectively, are the inverse 
chains of (Zn)neN and (nn)neN correspondingly restricted.
Proof. The transition probabilities of (Zn)nEN are
P( k,h) =
wi(j,k) 
wl (k) ,
0,
if there arej G S, c G C such that h = hl (j, k) and 
Jc(k) • Jc(hl(j,k)) =0;
otherwise;

7.2 Higher-Order Finite Markov Chains Defined by Weighted Circuits 113
for any k G E. On the other hand, the transition probability of the inverse 
chain of (Zn)n, from state k to state h is given by the known formula
Pe(h) 
Pe(k) P (h, k),
(7.2.14)
where P(h, k) is defined by (7.2.7) and PE is the stationary distribution of 
(Zn)nN defined by (7.2.10).
The expression (7.2.14) is furthermore equal to
pEj) P (,V( 
)
PE (k)
= Pe (hl (j, (k (v - m + 1) ,...,k (v - 1), k (v))))
Pe (k (v - m + 1), .. ., k (v))
•P (hl (j, (k (v - m + 1) ,...,k (v - 1), k (v))),
(k(v - m +1),...,k(v - 1), k(v)))
wi (j, k (v - m + 1), ...,k (v - 1))
wl(k(v - m +1),...,k(v))
•P((j, k(v - m + 1), .. ., k(v - 1)),
ul((j, k(v - m+1),...,k(v - 1)), k(v)))
wi (j, k (v - m + 1),. ..,k (v - 1)) 
wl(k(v - m +1),...,k(v))
wi(j, k(v - m + 1), ..., k(v - 1), k(v)) 
wi(j, k(v - m + 1), .. ., k(v - 1))
wi(j,k) 
wi (k)
= P( k,h).
Analogously, by using (7.2.6) and (7.2.13), we see that (nn)n^N, is the 
inverse chain of (nn)ntN. 
□
A straightforward consequence of Proposition 7.2.3 is that the Markov 
chains (Zn)ntN and (nn)ntN restricted to E and E-, respectively, are irre­
ducible and their stationary distributions PE and PE- are given by (7.2.10).
For the sake of simplicity we shall further consider that the recurrent 
classes mentioned previously are the entire sets Wi and Wr , respectively, 
(all the elements of Wi can be reached from one another by long sequences 
of m points on circuits). Also, we shall denote the invariant probability 
distributions on Wi and Wr by Pi and Pr , respectively. Now, following S. 
Kalpazidou (1988a), we prove
Theorem 7.2.4 (The Existence of Higher-Order Circuit Chains).
Assume we are given a natural number m> 1, a finite class C of over­
lapping circuits in a finite set S which satisfy the conditions quoted at the 
beginning of Subparagraph 7.2.2, and a set of positive weights {wc}cec- 

114 
7. Higher-Order Circuit Processes
Then there exists a pair of finite strictly stationary Markov chains 
(in)n, (Xn)n) of order m such that
P1 (in+m = i/in+m-1, ...,in) = wl((|lllllln+m-l^ , 
(7.2.15)
wl (nn, ... , n++m-1)
wr(i, (Xn+1,..., Xn+m))
P2( Xn 
i/Xn +1, . . . , Xn+m) 
, 
, , 
(7.2.16)
wr (Xn+1 ,. ..,Xn+m )
P1 (in+m = i/in+m-1 =im ,...,in =i1 )
= P2 (Xn = i/Xn+1 =im ,...,Xn+m = i1), 
(7.2.17)
for any n > m,i G S, (i 1,..., im) G Wl.
Proof. By (7.2.7) and (7.2.8) we have proved the existence of two irre­
ducible Markov chains (Zn)n and (nn) n whose state spaces are Wl and Wr 
respectively (Wr being connected with Wl by (7.2.9)), and with transition 
probabilities given by
P1(Zn+1 =ul(k,i)/Zn=k) =wl (k, i) 
wl (k) ,
P2(nn = ur(k-, i)/nn+1 = k-)=wr(i, k-) 
wr(k-) ,
for any k = (k(v — m + 1),..., k(v — 1), k(v)) G Wi and k- = (k(v), k(v — 
1),..., k(v — m +1)) G Wr (with v G Z), i G S. Moreover,
P1(Zn+1 = ul(k, i)/Zn = k) = P2(nn = ur(k-, i)/nn+1 = k-)
for any k G Wl and i G S .
The stationary distributions p1 = pwl and pr = pwr of the chains above 
are given by Proposition 7.2.2. Further, if Zn = (k(v — m +1),...,k(v — 
1), k(v)) G Wl and nn =(u(v),u(v — 1),...,u(v — m +1)) G Wr, define
in = pr-1Zn = k(v)
and
Xn = pr1nn = u(v),
for any n > m, where pr-1 and pr1 denote projections.
On account of Lemma 7.2.1 for any n > m we get
P1(in+1 = ul (Zn, in+1)) = 
P1(Zn+1 = ul(Zn, i))
= 
P1(Zn+1 = ul (k, i)/Zn = k)pl (k)
ies keWi
pl(k)
keWl 
ieS
wl (k, i) 
wl (k)
=1.

7.2 Higher-Order Finite Markov Chains Defined by Weighted Circuits 115
Similarly,
P2( nn = ur (nn +1, Xn )) = 1 •
Therefore
P1( Zn+m = ( Cn +1, • • • , Cn+m ))= P1( Zn +1 = ul ( Zs,Cs + 1) ,n < S < n + m - 1) = 1, 
and analogously
P2 ( nn 
( Xn, Xn +1, • • • , Xn+m-1)) 
1 •
Then for n > m
P1 (Cn+m i/C,n+m-1, • • • , Cn, • • •)
P1 ( Zn+m 
ul (Zn+m-1, i) / h + m-1 
(Cn, • • • , Cn+m-1))
_  wl ((Cn, • • • , Cn+m-1) , i)
wl (Cn, • • • , Cn+m-1)
Also
P2( Xn = i/Xn +1 ,Xn+2 , ••• ) = P2 ( nn = ur ( nn +1, i ) / Hi +1 = ( Xn +1, • • • , Xn+m ))
__ wr ( i, (Xn +1, • • • , Xn+m )) 
wr (Xn +1, • • • , Xn+m )
Moreover, making use of the balance property and Proposition 7.2.2, for 
any s > m,i 1 e S, we obtain
P1(Cs = i1) = P1(Zs = ul(Zs-1, i1)) = 
P1(Zs = ul(k, i1)/Zs-1 = k)p1 (k)
E
wl ( k,i 1) 
/,A
^likTp 1( k) •
keWi 
lV 7
Furthermore, for any i2 e S,
P1( Cs = i 1 ,Cs + 1 = i 2) 
^2 P1 (Cs + 1 = i 2 ,Cs = i 1 /Zs-1 = k) ■ Pl (k)
keWl
= 
P1(Cs+1 = i2/Cs = i1, Zs-1 = k)
keWl
■ P1(Cs = i 1 /Zs-1 = k) ■ Pl (k)
= 
P1(Zs+1 = ul(ul(k, i1), i2)/Zs = ul(k, i1))
keWl
■ P1 (Zs = ul (k, i1)/Zs-1 = k) ■ pl (k)
E
wl ( ul ( k,i 1) ,i 2) wl ( k, i 1) ( k )
keWl wl (ul (k,i 1)) 
wl (k) Pl 
^

116 
7. Higher-Order Circuit Processes
Let us now define recursively the functions u(t) : Sm x St ^ Sm,t = 
0, 1,2,...,by
ul(k, i), 
if t =0;
ul(t+1)(k,i(t+1)) = 
( )
ul(ul( )(k, i(t)), it+1), if t =1, 2,...;
where ul is given by (7.2.3) and i(t) =(i1,...,it) e St,t=1, 2,...... Then,
by induction, we deduce that the probability
wi(u(t 1)(k,i(t- 1)),it)
kEwl wi(u( t- 1)( k,i ( t- 1))
wl (ul (k, i1), i2) wl(k, i1)
P1 (£s = i, Es + 1 = i2 , . . . , Es+1-1 =
wi(k) p*k)
(7.2.18)
does not depend upon s, for any t = 1, 2,... and i 1,..., it e S. In a similar 
manner we get
P2 (Xs = i1)=P2 (ns
wl(ul(k, i1))
= ur(ns+1, i1))
P2 (ns = ur (k-, i1)/ns+1 = k-)pr (k-)
k-tWr
k-EWr
wr(i1, k-)
'Pr ( k-)
wr(k-)
and
P2(Xs = i2, Xs+1 = i1)=
k-EWr
P2(Xs = i2 ,Xs +1 = i 1 /ns+2 = k-)Pr (k- )
P2( Xs = i 2/Xs + 1 = i 1 ,ns+2 = k-)
k-EWr
• P2(Xs + 1 = i 1 /ns+2 = k- )Pr (k- )
P2(ns = ur(ur(k-, i1), i2 )/ns+1 = ur(k-, i1)) 
k-EWr
• P2 (ns+1 = ur (k-, i1)/ns+2 = k- )Pr (k- ) 
' pr(k-).
wr(k-)
wr ( i 2 ,ur ( k-,i 1))
wr (ur (k , i1)) 
k-EWr 
rV rV , ’’
In general we have
P2(Xs = it, Xs+1 = it-1,...,Xs+t-1 = i1)
wr ( i 2 ,Ur ( k-, i 1)) 
wr(ur(k-, i1))
wr (it,u rt 1)( k-,i(t 1))) 
k-EWr wr ( u r-1)( k-,i ( t- 1)))
wr (i 11 Lk-) Pr (k- ), 
wr (k- )
(7.2.19) 

7.3 The Rolling-Circuits 117
where u(rt) is defined similarly to ul(t) using the function ur introduced by 
(7.2.5).
Then, from the balance property (ft2) and Proposition 7.2.2, it follows 
that (in)n and (xn)n satisfy equations (7.2.15), (7.2.16) and (7.2.17). Also, 
equations (7.2.18) and (7.2.19) show that the circuit-weights completely 
determine the finite-dimensional distributions of (in)n and (xn) n. Then, 
by the uniqueness of pl and pr, the proof is complete. 
□
Reasoning as in the proof of Theorem 7.2.4 and starting from the inverse 
chains of (Zn)n and (nn)n, and then using the functions hl and hr instead of 
ul and ur, on account of Proposition 7.2.3 we are led to the inverse chains 
of (in)n and (xn)n. Therefore we can state
Theorem 7.2.5. Assume we are given a natural number m>1, a finite 
class of overlapping directed circuits in a finite set S which satisfy the hy­
potheses of Theorem 7.2.4, and a set of positive weights {wc}c^c.
(i) Then there exists a pair ((in)n, (xn)n) of finite strictly stationary 
Markov chains of order m in such that
wl (i, (in +1, . . . , in+m))
P1 ( in = i/£n +1 ,--,tn.+m ) =
wl (in +1, ... , in+m )
wr ((xn, . . . , xn+m— 1), i )
P2(xn+m 
i/xn+m — 1, . . . , xn) 
„ . / / 
z \ ,
wr (xn, . . . , xn+m — 1)
P1(in = i/in +1 = i 1, . . . ,in+m = im)
P2( xn+m i/xn+m — 1 i 1, . . . , xn im),
for any n > m,i e S, (i 1, .. ., im) e Wi.
(ii) The chains (in)n and (xn)n are Doob versions of the inverse chains 
of the chains given by Theorem 7.2.4.
Definition 7.2.6. The Markov chains (in)n and (xn)n of order m occur­
ring in Theorem 7.2.4 and their inverse chains occurring in Theorem 7.2.5 
are called circuit chains of order m (or multiple circuit chains) associated 
with the number m, the finite class C of circuits in S, and the positive 
weights wc, c e C.
7.3 The Rolling-Circuits
As has already been mentioned, there are good reasons for differentiating 
between two kinds of processes defined by directed circuits: the S-state 
Markov chains of order m(m>1) as i and x given by Theorem 7.2.4, and 
the simple Markov chains Z and n whose transition probabilities are defined 

118 
7. Higher-Order Circuit Processes
by (7.2.7) and (7.2.8), respectively. We shall call the latter simple circuit 
processes associated to £ and x•
Conversely, following the same reasonings of Chapters 3 and 4, it 
might be interesting to investigate whether a circuit representation theory 
can be developed for higher order Markov chains. Namely, we propose 
the following problem: given a pair (£, x) of m-order strictly stationary 
Markov chains on a finite set S, if equations (7.2.17) are verified, then 
define a class C of directed circuits in S and a collection {wc, c G C} of 
positive numbers which express the transition laws of £ and x by a linear 
combination of the passage-functions Jc,cG C.
The answer to this question is not easy. For instance we have to determine 
the kind of dynamics from sequences (i1 ,...,im) G Sm to points of S.To 
this end, we shall investigate the geometry of the sample paths of the chains 
£ and x via the sample paths of the associated simple Markov chains, which 
will be symbolized by Z and n, respectively.
It turns out that the trajectories of the chains Z and n provide “circuits” 
whose points are the long sequences (i1 ,...,im)ofm points as new points. 
We shall call these circuits rolling-circuits (S. Kalpazidou (1988a)). It is 
this kind of circuit that we study in the present section. Let us start with 
the following:
Definition 7.3.1. Assume S is any nonvoid finite set and m>1.
(i) A circuit in Sm is any periodic function 7: Z ^ Sm with the property 
that for each t G Z there exists i = i(t) G S such that
Y (t + 1) = ui (Y (t) ,i),
where ul is defined by (7.2.3).
(ii) The inverse circuit of y defined at (i) is the periodic function 6: Z ^ 
Sm with the property that for each t G Z there exists i = i(t) G S 
such that
6(t +1) = ur(6(t), i),
where (Y(t + 1))-, (Y(t))-, (Y(t - 1))-,...,t G Z, are consecutive 
values of 6 and ur is defined by (7.2.5).
We shall write 6 = Y- . Define the period ofY (or Y-) as the smallest pos­
itive integer p = p(Y)(p(Y -)) such that Y(t + p) = Y(t)(Y- (t + p)=Y- (t)) 
for all t G Z. Obviously p(Y) = p(Y-). In the following we shall only refer 
to definitions and properties concerning circuits Y, since those regarding 
circuits Y - are completely analogous. The exposition follows the investiga­
tions of J. MacQueen (1981) and S. Kalpazidou (1988a).
Define for j G Z the circuit Yj by the relation
Yj(t)=Y(t+j), tG Z;

7.3 The Rolling-Circuits 119
that is, if y(t) = (k(t — m + 1),■■■, k(t)) and j < m, then in view of (1.1.1) 
(Chapter 1)
y(t +j)=(k(t - m +j +1),...,k(t),i1,i2,...,ij)
where i 1, i2,..., ij G S. If for 
y (t) = (k(t - m + 1),■■■■> k (t - m +
i),...,k(t)) we set
y (t)(i) = the ith projection counting from the left
= k(t - m + i), 
then
Y (t )(i)= Yj (t)(i - j), 
1 < j<i < m.
In particular,
Y(t)(i)= Yi—1(t)(1), 
1 < i < m.
Let the relation ~ be defined by y ~ y' if and only if there exists j G Z such 
that y' = Yj. This is an equivalence relation which enables us to consider 
further the classes instead of the elements.
Let C be a finite set of directed circuits in the originally given set S, and 
define Wi as in (7.2.2'). Let us consider the set C* of all circuits y in Wi 
which are defined by
Y(t) = (c(t),c(t +1),...,c(t + m - 1)),
for all t G Z , where c G C.
Since intuitively the elements c(i) of Y(t) are obtained “by rolling” the 
circuit c,wecallY G Cl* the rolling-circuit associated to c. Analogously, 
Cr* will denote the set of all rolling-circuits in Wr . Plainly, if Y G Cl* then 
Y - G Cr* .LetY be a circuit in Sm . Then, there is exactly one circuit c in 
S such that
Y(t) = (c(t),c(t +1),...,c(t + m - 1)), tG Z, 
(7.3.1)
Indeed, if we define c(t) = Y(t)(1), then by virtue of Definiton 7.3.1 we get 
(7.3.1). Moreover, c is a circuit in S and its period p(c) is exactly that of Y 
as we shall show in a moment.
First, we prove that p(y) < p(c), that is,
p(c) G{s: Y(t + s)(i)=Y(t)(i),i=1,...,m}. 
(7.3.2)
We have c(t + p(c)) = c(t), for all t G Z ,or
c(t + p(c)+i - 1) = c(t + i - 1), 
tG Z, i =1,...,m. 
(7.3.3)
On the other hand,
Y(t)(i)=Yi-1(t)(1)=Y(t+i-1)(1)=c(t+i-1)

120 
7. Higher-Order Circuit Processes
and
Y(t + P(c))(i) = Yi- 1 (t + P(c))(1) = Y(t + P(c) + i - 1)(1) 
= c(t + p(c) + i - 1).
In view of (7.3.3), (7.3.2) holds.
Second, the inequality p(c) < p(y) follows from the relations
c(t) = Y(t)(1) = Y(t + P(Y))(1) = c(t + P(Y)), 
t G Z.
Furthermore, we define an elementary circuit in Sm to be a circuit Y for 
which the P(Y) elements Y(t),Y(t +1),...,Y(t + P(Y) - 1), for some t (and 
therefore for all t), are all different from one another. Also, an m-elementary 
circuit c in S is any circuit with the following property: the smallest in­
teger k > 1 such that for each t we have c(t + k + i) = c(t + i), for all 
i =1,...,m, is exactly the period P = P(c). For instance, the circuit de­
fined by (7.3.1) is an m-elementary one if Y is elementary.
To conclude we may state
Proposition 7.3.2. Let C*, be a set of elementary circuits in Sm. Then, 
there are a set C of m-elementary circuits in S and a bijection t: C* ^ C 
defined as
t(Y) = c if and only if c(t) = y(t)(1), 
t G Z, 
(7.3.4)
and
t—1( c )= y if and only if y (t)(i )= c (t + i - 1), t ^ Z, i =1 ,...,m,
(7.3.5)
which keeps the period invariant. The latter property is valid for t-1 only 
for m-elementary circuits in S.
Analogously, we define a bijection t- from the set Cr* of all elementary 
circuits in Wr onto the set C- of m-elementary circuits in S, where C- = 
{c-: c- is the inverse of c, c G C}. Then (ty) — = T-Y-.
7.4 The Passage-Function Associated with a 
Rolling-Circuit
A passage-function associated with a rolling-circuit in Sm,m > 1, is defined 
as follows. Let Cbe a collection of directed circuits in S as in Subparagraph 
7.2.2 and let Wi and Wr be the subsets of Sm defined by (7.2.2') and 
(7.2.2"). Consider further y , with Y (t) = (c (t),... ,c (t + m - 1)), t G Z, an 
elementary rolling-circuit described by an m-elementary circuit c G S.

7.4 The Passage-Function Associated with a Rolling-Circuit
121
For any k = (k(t — m + 1),..., k(t)) and ul(k, i) in Wl, where i G S, de­
fine
{
1, if for some 1 < t < p(y) — 1, Y (t) = k and
Y(t +1)= ul (k,i); 
(7.4.1)
0, otherwise.
Then
JY (k, ul (k, i)) = JY ((k (t - m + 1), .. ., k (t)), (k (t - m + 2),..., k (t), i)) 
f 1, if Jty(k) = Jty(ul(k, i)) = 1;
0, otherwise;
where ul is defined by (7.2.3), and t is the bijection occurring in Proposition 
7.3.2. Here JTY(•) is the mth order passage function associated with the 
m-elementary circuit c in S (see Definition 1.2.2) which defines y.
In view of the equalities t(yj) = (ty)j,j G Z, and relation (1.2.1), it 
follows that the definition of the J* does not depend upon the choice of 
the element that represents the class-circuit y .
Definition 7.4.1. The function J * defined by (7.4.1) is called the passage­
function associated with the elementary rol ling-circuit y.
Definition 7.4.2. If J *(k, ul(k, i)) = 1, we say y passes through 
(k, ul(k, i)).
Thus the rolling-circuit y = t-1c passes through (k, ul(k, i)) exactly 
when c passes simultaneously through k and ul (k, i); namely k is passed 
by c, and ul(k, i)byc ◦ t1 (see Definition 1.2.2).
We define in an analogous manner J *- (ur(k-,i),k-) = 
J *- (i, k(t),...,k(t — m + 2)), (k(t),...,k(t — m + 1))) for k = 
(k(t — m +1),...,k(t)) G Wl and i G S.
Now we prove
Lemma 7.4.3. For an elementary rolling-circuit y in Wl we have
(i) J*(k,ul(k,i)) = Jty(k,i), 
J*- (ur (v,i),v) = Jt-y- (i, v) ;
(ii) J * (hl (i, k),k)=Jt (i, k), 
(7.4.2)
J*- (v, hr(i,v)) = JT-Y- (v,i),
for any k G Wl ,v G Wr, and i G S.
Proof. Let y(t)=(t-1c)(t)=(c(t),...,c(t + m — 1)), t G Z. Sup­
pose i = c(t + m). If for k G Wl ,k =(k(t),...,k(t + m — 1)), we have 
J* (k,ul (k, i)) = 1, then from the definition of J* above, it follows 

122 
7. Higher-Order Circuit Processes
equivalently that
Jty (k) = Jty (k (t + 1) ,...,k (t + m - 1) ,i) = 1
or
JTY(k, i) = 1.
If i = c(t + m), then both members occurring in the first relation of 
(7.4.2) (i) are equal to 0. Hence the first equality of (7.4.2)(i) holds. The 
other relations of the lemma follow in a similar manner. 
□
Lemma 7.4.3 has the following immediate consequences (see S. Kalpazi- 
dou (1988a)). Let C* be a class of overlapping elementary circuits in Sm 
and let C* be the class of the reverses of C*. Then we have
Proposition 7.4.4. Let y C C*. Then the functions JY* and JY*- satisfy 
the balance properties:
(P *): 
(i) g J* (k,ui (k,i ))= g J* (hl (j,k) ,k);
(ii) £ J*- ( Ur (V,i ) ,V )= £ J*- (v,hr (j,V ));
(p*): 
(i) J* (k,ui (k,i )) = J*- (Ur (k-,i) ,k-);
(ii) J* (hi (j,k),k) = J*- (k-,hr (j,k-)).
Theorem 7.4.5. Associate with each Y C C* and Y- C C* a strictly pos­
itive number w* = w*- . Let
(i) 
w*(k,ui(k,i)) = 
w*J*(k,ui(k,i)),
w* (Ur (V,i ) ,V ) = 
w*- J*- (Ur (V, i ) ,V );
y eCr
(ii) w* (hi (i,k) ,k ) = g w* J* (hi (i, k) ,k),
w* (v,hr (i,v)) = 
wY*- J*- (v,hr (i,v)),
Y- EC*
for any k, v C Sm ,andi C S. Then letting
C = tC*, 
C-=T-Cr*,
wc = w**-1 c = w*-1 
= wc- ,
I c
where T and T- are the bijections on C* and C*, given by Proposition 7.3.2, 
we have, respectively,

7.5 Representation of Finite Multiple Markov Chains 123
(iii) The functions w* and w* satisfy the balance properties (fi*) and 
(fi*).
(i) wi*(k, ui(k, i)) =
wr* (ur (V, i),V) =
wi(k, i), 
wr (i, V),
where
where
wi (k, i)= 
wr (i, V)=
wcJc (k, i), 
cEC
E
c- EC-
wc- Jc- (i, V);
ii) wi* (hi (i, k),k)= wi (i, k),
where
wi (i, k)= E w
cEC
cJc(i, k),
wr* (V, hr (i, V)) = 
for any k, V G S
wr (V, i), 
m,andi G
where
S. Also
wr(V, i)=
E
c- EC-
wc- Jc- (V, i),
7.5 Representation of Finite Multiple Markov 
Chains by Weighted Circuits
Let N = {1, 2,...},m > 1, and S be a finite set which contains more than 
m elements. Let £ = (£n)n and x = (Xn)n be two homogeneous strictly 
stationary Markov chains of order m> 1 with finite state space S and with 
equal values of the invariant probability distributions such that
P(£n = i/£n-1 = im ,...,£n-m = i1)=P2 (xn = i/xn+1 = im ,...,xn+m = i1)
(7.5.1)
for any n > m and i, i 1, i 2,... ,im G S. Consider the simple Markov chains 
Z = (Zn) and n = (nn) associated, respectively, with £ and x, and having 
transition probabilities given by
P1( k, ui (k, i)) = P1 (Zn = ui (k, i)/Zn-1 = k = (i 1,..., im)) 
(7.5.2)
P1 (£n 
i/£n-1 
im, ... , £n-m 
i 1), n G m + 1,
Pr (V, Ur (V,i )) = P2( nn = Ur (V, i)/nn +1 = V = (im, . . .,i 1)) 
(7.5.3)
P2 (xn 
i/xn +1 
im, ... , xn+m 
i 1), n > 1,
for all i1,...,im,i G S, where the functions ui and ur are defined by (7.2.3) 
and (7.2.5).
Assume Z and n are irreducible chains on two disjoint subsets Wi and 
Wr of Sm connected by relation (7.2.9), and consider p = (p(k),k G Wi ) 
and p- = (p(k-),k- G Wr) their invariant probability distributions. Re­
call that, if k =(i1,...,im), then k- designates as always the sequence 
(im,...,i1). Then p(k) = p- (k-), k G Wi, and in view of (7.5.1) we have
P1(Zn+1 = ui(k, i)/Zn = k) = P2(nn = ur(k-, i)/nn+1 = k-) 
(7.5.4)
for any k G Wi ,i G S and n>m.
We are now ready to solve the circuit representation problem proposed at 
the beginning of Section 7.3, namely: given any pair (£, x) of higher-order 

124 
7. Higher-Order Circuit Processes
strictly stationary Markov chains verifying equations (7.5.1) there exist a 
class C of directed circuits in S and a collection of positive weights wc,c G C, 
which completely determine both transition laws of £ and x•
We shall answer this question following S. Kalpazidou (1988a). Before 
proceeding, let us consider the multiple inverse chains (i.e., the parameter­
scale is reversed) £' = (£'n) and x' = (x'n) of £ and X above as well as their 
attached simple Markov chains Z' = (Zn) and n' = (n'n) whose transition 
probabilities are, respectively, given by
Pl( k, hl ( i,k )) = P1 ( C = hl ( i,k )K'n +1 = k = ( i 1, . ..,im ))
= P1 (£n = i/£'n +1 = i 1,.. .,£n+m = im )), 
(7.5.5)
Pr (v,hr (i,V)) = P2 (n'n = hr (i,v)/n'n-1 = v = (im, ■ ■ mi 1))
= P2 (Xn = i/Xn-1 = i 1 ,...,Xn-m = im) , 
n > m, (7.5.6)
where the functions hl and hr are defined by (7.2.4) and (7.2.6).
To solve the circuit representation problem we need the following basic 
lemma:
Lemma 7.5.1. Consider two nonnegative functions wf and wf which are 
defined on Wi x Wi and Wr x Wr, respectively. Assume wf and w* satisfy 
the balance equations
E 
wf (k,ui (k,i ))= 
£ 
wf (hl (i,k) ,k), 
(7.5.7)
its 
its
ui (k,i) £Wi 
h 1( i,k) £Wi
for all k G Wi,
wrf (ur(v, i),v) = 
wrf (v, hr(i, v)), 
(7.5.8)
itS 
itS
ur (v, i)tWr 
hr (i, v)tWr
for all v G Wr, such that each sum occurring in (7.5.7) and (7.5.8) is strictly 
positive, and
wif (k, ui (k, i)) = wrf (ur(k-, i), k-), 
(7.5.9)
for any k, ui(k, i) G Wi and k-, ur(k- ,i) G Wr.
Then there exist two finite ordered classes Cif and Crf of elementary 
circuits in Wi and Wr, where Cf = {7-,7- is the inverse of y,Y G Cf}, 
and strictly positive numbers wYj = w£— ,Y G Cf, depending on the ordering 
of Clf, such that
wf(k,ui(k,i)) = 7^ wYJYf(k,ui(k,i)),
wr ( Ur ( k-,i ) ,k- ) 
wY- JYf-( Ur ( k-,i ) ,k-),
y- tCr

7.5 Representation of Finite Multiple Markov Chains 125
for all k G Wi,k- G Wr, and i G S. Here the functions JY*,Y G C*, are 
defined by (7.4.1), while the JY*- are given similarly.
Proof. Consider the oriented graph of wl* , that is, the points are the se­
quences u = (i 1,..., im) G Wi and the directed edges are the pairs (u, u') G 
Wi for which w* (u,u') > 0. Then, choosing an arbitrary point k G Wi, the 
strict positiveness of the sums in (7.5.7) and (7.5.8) enables us to find at 
least an element j1 G S such that (k, j1 ) satisfies
0 < wl*(k,ul(k,j1)) = wr*(ur(k-, j1), k-).
Repeating the same argument, since the function wl* is balanced, we may 
find a finite number of elementary circuits y 1,..., Ya constructed below 
which pass through the elements of Wl. Let us examine the construction of 
the y1,...,ya.
The balance equation (7.5.7) implies the existence of an edge (k1, k2), 
with k1 = k and k2 = ul(k, j1 ), and in turn of a sequence of pairs 
(k 1, k2), (k2, k3),... in Wi x Wi such that kn+1 = ui(kn,jn), for some jn G 
S, and
wl*(kn, kn+1) > 0 implies wl*(kn+1, kn+2) > 0.
Since Wi is finite, there exists a smallest integer n > 2 for which kn = ks for 
some s =1,...,n- 1. Then y1 = (ks,ks+1,...,kn-1,ks) is an elementary 
circuit in Wl. By setting
wY 1 = w*(Y 1( t 1) ,Y 1( t 1 + 1)) 
= mtin wl*(y1(t),y1(t + 1)) 
= wr*(y1-(t1 + 1),y1-(t1)) 
= w* -, 
Y 1
we define
wl*(u, ul(u, i)) = wl*(u, ul(u, i)) - wY*1 JY*1 (u, ul(u, i))
= w*(ur(u-, i), u-) - w*- J*- (ur(u-, i), u-) 
r 
Y1 Y1
= (w1*)- (ur (u-, i), u-),
for any u G Wl and i G S with ul(u, i) G Wl.
By the definition of wY*1 and wY*- , the functions w1* and (w1* )- are 
non-negative. Moreover, since the f1unctions JY*1 and JY* - are balance, 
the functions w1* and (w1* )- are also. Then, repeating th1e same reason­
ing above to w1* , which remains strictly positive on fewer pairs than the 
initial function wl* ,ifw1* > 0, and then (w1* )- > 0, at some point, we 
can find another elementary circuit y2 and its inverse y2- with wY*2 = 
w*- > 0, which in turn provides new balance functions w2* and (w2* )- 

126 
7. Higher-Order Circuit Processes
given by
w 2 (u,ui (u,i)) = w1: (u,ui (u,i)) - w** 2 JY2 (u,ui (u, i))
= w* (u,ui (u, i)) - w* 1 J*1 (u,ui (u, i)) - w*2 J*2 (u,ui (u, i))
= w*(ur(u-, i), u-) — w* - J*- (ur(u-, i), u-) 
rr 
1 
1 r
—w*- J*- (ur (^_, i), u) 
2- 2- r - -
= (w 2) - (ur (u-,i) ,u-).
Continuing the procedure we find a sequence w*, w2,... of balanced func­
tions such that each w*+1 remains strictly positive on fewer pairs than w*. 
Because Wi is finite, after finitely many steps, say a, we find the elementary 
circuits y 1, Y 2,... ,Y&, such that
w*+1(u, ui(u, i)) = 0.
Then the collection C* required in the statement of this lemma is iden­
tical to {y 1 ,Y2,...,Ya}. Analogously, choosing as representative class of 
circuits for wr to be C* = {y- : Y- is the inverse circuit of Y,Y G C*}, 
we obtain w*- = w* for all Y- G C*, and the decomposition of w* by 
(Cr*, wY-). The proof is complete. 
□
Remark. From the proof of Lemma 7.5.1 it follows that the family C* is 
not uniquely determined since its construction depends upon the starting 
sequence k as well as upon the ordering of the closed chains in Sm .
Analogously, one may prove:
Lemma 7.5.2. Consider two nonnegative functions w* and w* which are 
defined on Wi x Wi and Wr x Wr, respectively. Assume w* and w* satisfy 
the balance equations (7.5.7) and (7.5.8) such that each sum occurring in 
(7.5.7) and (7.5.8) is strictly positive, and
w*(hi(i, k), k) = w*(k-, hr(i, k-)),
for any k, hi (i, k) G Wi and k-, hr(i,k- ) G Wr.
Then there exist two finite ordered classes C* and C* of elementary 
circuits in Wi and Wr, where C* contains the reverses of the elements of 
C*, and strictly positive numbers wY = w**-, Y G C*, such that
w* (hi (i,k),k) 
^2 wYJ (hi (i,k),k),
w* (k-,hr (i,k-)) = 
w**- JY- (k-,hr (i,k-)),
Y-eCr
for all k G W ,k- G Wr and i G S .

7.5 Representation of Finite Multiple Markov Chains 127
Now we can state and prove the representation theorem for the originally 
given m-order Markov chains (fn)n and (xn)n which satisfy the assump­
tions mentioned at the beginning of this section.
Theorem 7.5.3 (The Circuit Representation for Higher-Order Markov 
Chains). There exist two finite ordered classes C and C- = {c- : c- is 
the reverse of c,c G C} of m-elementary directed circuits in S and strictly 
positive circuit weights wc and wc-, with wc = wc-,c G C, depending on 
the ordering of C, such that
P1(fn 
i/fn—1 
im, ... , fn—m 
i 1)
P2 (xn = i/xn+1 = im,...,xn+m = i1 )
Wl (( i 1, . . .,im ) ,i ) 
wl (i 1, . . . , im )
Wr (i, (im, . . .,i 1)) 
wr (im , . . . , i1 )
Wl ( k, i )
Wl (k) , 
wr (i, k- )
Wr(k-) ,
for any n>mand i1,...,im,i G S such that k =(i1,...,im) G Wl , where 
Wl (k, i)= 
WcJc(k, i),
Wr (i, k-)= 
Wc- Jc- (i, k-),
Wl(k) = 
WcJc(k),
c C
wr (k— ) = 52 Wc- Jc- ( k- ),
and Jc(•, •),Jc(•), Jc- (•, •) and Jc- (•) are the passage functions associated 
with c and c— .
Proof. Associate with the strictly stationary Markov chains (fn)n and 
(Xn)n of order m, the two irreducible Markov chains (Zn)n and (nn)n whose 
transition probabilities Pl and Pr are given by (7.5.2) and (7.5.3), and 
with the stationary distributions p and p— , respectively. Further, we define 
w* (k, k') for k, k' = ul (k, i) G Wl, with i G S, by
w*(k,ul(k, i)) = p(k)Pl(k,ul(k, i)). 
(7.5.10)
Similarly, for v,v' = ur(v,i) G Wr, with i G S, we define wr: (v',v) by
wr: (ur (v,i) ,v) = p— (v) Pr (v,ur (v,i)). 
(7.5.11)
Then, letting
w*(k) ^2w*(k,ul(k,i)),
w*(v) ^2w*(ur(v,i),v),

128 
7. Higher-Order Circuit Processes
we have
w* (k) = P(k) ,
w* (k- )= p- (k-). 
(7.5.12)
Therefore, we have w* (k) = w* (k-) and
P( k,u (k,i )) = w* (. , k’i»,
w*(k)
P (k u (k i)) = wr (ur(k-, i),k-) 
(7 5 13)
Pr(k-,ur(k-,i)) — 
, 
, 
(7.5.13)
wr (k-)
for any k — (i 1,..., im) G Wi and i G S such that ul (k, i) G Wl.. Further­
more, because of (7.5.4), we get
w*(k, ui(k, i)) — wr (ur(k-, i), k-) 
(7.5.14)
for any k —(i1,...,im) G Wi and i G S such that ui (k, i) G Wi .
Then, we may apply Lemma 7.5.1 to the balanced functions w* (•, •) and 
wr (•, •) defined by (7.5.10) and (7.5.11). Accordingly, there exist two finite 
ordered classes C* and C* of elementary circuits in Wi and Wr, with C* — 
{7- : y- is the inverse circuit of y,Y G C* }, and strictly positive numbers 
wYf — w*— ,y G C* such that
w*(k,ui(k,i)) 
^2 wYJY(k,ui(k,i)),
w* ( ur ( v,i ) ,v ) 
^2 w*— JY— ( ur (V, i ) GC ) •
Y— erCr-
Then, because of Theorem 7.4.5, by letting
C — T C*, 
C- — T-Cr,
wc = w**— 1 c — w*-1 
= wc- ,
I c
where t and T- are the bijections on C* and C*, given as in Proposition 
7.3.2, we have
w*(k,ui(k, i)) — wi(k, i),
where
wl(k, i)— 
wcJc(k, i),
and
w* (ur (v, i), v) — wr (i, v),
where
wr(i, v) — 
wc— Jc— (i,v)•

7.5 Representation of Finite Multiple Markov Chains 129
Thus w*(k) = wl(k), for any k G Wl, and wr(v) = wr(v), for any v G Wr, 
where wl (k) and wr(v) have the expressions stated in the theorem. Finally 
relations (7.5.13) become
Pl (k, ul (k, i))
Pr(k-, ur(k-, i))
wl (k, i) 
wl (k) , 
wr (i, k- ) 
wr(k-)
as was to be proved.
□
Remark. (i) If we consider the inverse chains (fn) n and (xn) n of (Cn) n 
and (xn)n, then in view of Lemma 7.5.2 and by using the inverse chains 
of (Zn)n and (nn)n we may analogously prove the following representation 
theorem:
Theorem 7.5.4. There exist two finite ordered classes C and C-, with
C- = {c- : c- is the reverse of c, c G C } of m-elementary circuits in S and
strictly positive circuit weights wc and wc- , with wc = wc- ,c G C, such that
P1( Cn = i/Cn +1 = i 1 ,...,Cn+m = im ) = 
/ .
n 
n+ 
n+m 
wl(i1,..., ,im )) _ wl ( i,k ) 
im ) 
wl ( k) ,
P2 (xn 
i/xn- 1 
i 1, . . . , xn-:
wr ..im,..., 
m = im)=
wr .im, ...
i1), i) 
wr .k-, i)
,i1) 
wr .k-) ,
for any n>mand i1,...,im,i G S, such that k =.i1 , ...,im ) G Wl , where
wl .i, k)
wcJc .i, k),
ceC
wr .k-, i)
wc- Jc- .k- ,i),
c- eC-
wl .k)
wcJc .k),
ceC
wr .k-)
wc- Jc- .k-),
c- eC-
and Jc(•, •), Jc- (•, •), Jc(•) and Jc- (•) are the passage functions associated 
with c and c- .
(ii) It is obvious (from Lemma 7.5.2) that the classes of m-elementary 
circuits and the corresponding weights whose existence is stated in Theorem 
7.5.4 are the same as those given in Theorem 7.5.3. Moreover, the functions 
wl and wr constructed in Theorems 7.5.3 and 7.5.4 satisfy the balance 
properties (0 1) and (02).
To conclude, starting from a natural number m> 1 and a finite class of 
weighted circuits in a finite set S (containing more than m elements) we 

130 
7. Higher-Order Circuit Processes
may define two strictly stationary Markov chains of order m or their inverse 
chains. Conversely, any finite strictly stationary Markov chain of order m 
enables us to define a finite class of weighted m-elementary circuits in S. 
The latter is also obtained if we reverse the parameter-scale in the initial 
chain.
Finally, the reader may find in S. Kalpazidou (1989b, 1990b, 1991a, e) 
expansions of this chapter to higher-order circuit chains with a countable 
infinity of states.

8
Cycloid Markov Processes
As we have already seen, finite homogeneous Markov chains £ admitting 
invariant probability distributions may be defined by collections {cK,wk} of 
directed circuits and positive weights, which provide linear decompositions 
for the corresponding finite-dimensional probability distributions. The aim 
of the present chapter is to generalize the preceding decompositions to more 
relaxed geometric entities occurring along almost all the sample paths of 
£ such as the cycloids, which are closed chains of edges with various ori­
entations. Then £ is called a cycloid Markov chain. Correspondingly, the 
passage-functions associated with the algebraic cycloids have to express the 
change of the edge-direction, while the linear decompositions in terms of 
the cycloids provide shorter descriptions for the finite-dimensional distri­
butions, called cycloid decompositions.
A further development of the cycloid decompositions to real balance func­
tions is particularly important because of the revelation of their intrinsic 
homologic nature. Consequently, the cycloid decompositions enjoy a 
measure-theoretic interpretation expressing the same essence as the known 
Chapman-Kolmogorov equations for the transition probability functions. 
The development of the present chapter follows S. Kalpazidou (1999a, b).
8.1 The Passages Through a Cycloid
Let S be a finite set and let G =(S, E) be any connected oriented graph G = 
(S, E), where E denotes the set of all directed edges (i, j), which sometimes 
will be symbolized by b(i,j) .

132 
8. Cycloid Markov Processes
If c is a sequence (e 1,..., em) of directed edges of E such that each edge 
er, 2 < r < m - 1, has one common endpoint with the edge er- 1(= er) and 
a second common endpoint with the edge er+1(= er), then c is called the 
chain which joins the free endpoint u of e1 and the free endpoint v of em . 
Both u and v are called endpoints of the chain. If any endpoint of the edges 
e 1,...,em appears once when we delete the orientation, then c is called an 
elementary chain.
Definition 8.1.1. A cycloid is any chain of distinct oriented edges whose 
endpoints coincide.
From the definition of the elementary chain, we correspondingly obtain 
the definition of an elementary cycloid. Consequently, a directed circuit or 
cycle c is any cycloid whose edges are oriented in the same way, that is, 
the terminal point of any edge of c is the initial point of the next edge. 
Accordingly, we also obtain the definition of the elementary cycle.
To describe the passages along an arbitrary cycloid c, we need a much 
more complex approach than that given for the directed circuits in Chapter 
1. It is this approach that we introduce now.
Let cc be an elementary cycloid of G. Then ccis defined by giving its edges 
e1 ,e2 ,...,es , which are not necessarily oriented in the same way, that is, 
the closed chain (e1 ,e2,...,es) does not necessarily define a directed circuit 
in S. However, we may associate the cycloid cc with a unique directed circuit 
(cycle) c and with its opposite c_ made up by the consecutive points of c. 
Note that certain edges of both c and c_ may eventually be not in the 
graph G.
We shall call c and c_ the directed circuits (cycles) associated with the 
cycloid cc. For instance, consider the cycloid cc = ((1, 2), (3, 2), (3, 4), (4, 1)). 
Then the associated directed circuits are c = (1, 2, 3, 4, 1) and c_ = (1, 4, 3, 
2, 1). With these preparations we now introduce the following definitions.
The passage-function associated with a cycloid cc and its associated di­
rected circuit c is the function Jc,c_: E ^ { —1, 0, 1} defined as
Jc,c(i,j) = 1, 
if (i,j) is an edge of c and c,
= — 1, if (i, j) is an edge of c and c_, 
(8.1.1)
=0, otherwise.
Analogously, the passage-function associated with the pair (c, c_) is the 
function Jc,c_: E ^ { — 1, 0, 1} defined as
Jc,c_(i, j) = 1, if (i,j) is an edge of c and c_, 
= —1, if (i, j)isanedgeofccand c, 
=0, otherwise.
Then we have
Jc,c(i,j) = -Jc,c_(i,j), 
i,j € S,

8.1 The Passages Through a Cycloid 133
and
Jc,c(i,j) = Jn,c(j,i), 
i,j G S.
In particular, if the cycloid c coincides with the cycle c, then
J,c(i,j) = Jc(i,j), 
i,j G S,
where Jc(i, j) is the passage-function of c, which is equal to 1 or 0 according 
to whether or not (i,j) is an edge of c.
The passage-functions associated with the cycloids enjoy a few simple, 
but basic properties.
Lemma 8.1.2. The passage-functions Jcc(i,j) and Jcc_(i,j) associated 
with the elementary cycloid cc are balanced functions, that is,
g Jc,c (i,j ) = g Jc,c (k,i), 
(8.1.2)
g J^ (i,j ) = g Jcc. (k,i), 
(8.1.3)
for any i G S.
Proof. We shall prove equations (8.1.2). Consider i G S .If i does not lie 
on c, then i does not lie on both c and c_. Then both members of (8.1.2) 
are equal to zero.
Now, let i be a point of c. Then i is a point of c and as well. Accord­
ingly, we distinguish four cases.
Case 1: The edges of cc, which are incident at i, have the orientation of c. 
Then
Jc ,c(i, j)=Jc ,c(i, u)=+1, 
jES
Jc ,c(k, i)=Jc ,c(v,i) = +1, 
kES
where (i, u) and (v, i) are the only edges of cc and c, which are incident at 
i.
Case 2: The point i is the terminal point of both edges of cc, which are 
incident at i. Then, we have
E Jc,c (i,j )=0, 
jES
Jc ,c(k,i) = Jc ,c(v, i)+Jc ,c(u, i)=(+1)+(-1) = 0, 
kES
where (v, i) and (u, i) are the only edges of cc, one lying on c and the other 
on , which have i as a terminal point.

134 
8. Cycloid Markov Processes
Case 3: The point i is the initial point of both edges of c which are incident 
at i. Accordingly, we write
Jc,c(i,j) = (+1) + (— 1) = 0, 
jes
J^kci (k,i) = 0 •
kes
Case 4: The edges of c, which are incident at i, have the orientation of c_. 
Then
y~^ Jc,c(i,j) = — 1,
jes
yy Jc,c (k, i) = — 1.
kes
Finally, relations (8.1.3) may be proved by similar arguments. The proof is 
complete. 
□
Now we shall investigate how to express the passages of a particle moving 
along the cycloids cc of G in terms of the passage-functions.
First, let us assume that the cycloid cc coincides with the directed circuit 
c. Then the motion along the circuit c is characterized by the direction of 
c, which, in turn, allows the definition of an algebraic analogue c in the real 
vector space C1 generated by the edges {b(i,j) } of the graph G. Specifically, 
as in paragraph 4.4 any directed circuit c =(i1,i2,...is,i1), occurring in 
the graph G, may be assigned to a vector c G C 1 defined as follows:
C = ^2 Jc(i,j)b(i,j),
where Jc is equal to 1 or 0 according to whether or not (i, j) is an edge 
of c. Let us now consider a cycloid cc, which is not a directed circuit. To 
associate c with a vector c in C 1, we choose a priori a direction for the 
passages along cc, that is, we shall consider either the pair (cc, c) or the pair 
(c, c_) where c and c_ are the directed circuits associated with C. Then we 
may assign the graph-cycloid c with the vectors c and —c in C 1, defined as 
follows:
c = ^2 Jc,c(i,j)b(i,j),
c yy Js,^(i,j) b (i,j).
(8.1.4)
In other words, any cycloid cc of the graph G may be assigned, except for the 
choice of a direction, with a vector c in C 1. The vector c will be called a cy­
cloid, as well. If c is elementary, then c is called an elementary cycloid in C 1.
On the other hand, it turns out that all the cycloids c, associated with the 
connected oriented graph G, generate a subspace C1 of C1 . The dimension 

8.2 The Cycloid Decomposition of Balanced Functions 135
B of the vector space C1 is called the Betti number of the graph G. One 
method to obtain a base for C1 consists in considering a maximal (oriented) 
tree of G. A maximal tree is a connected subgraph of G without cycloids 
and maximal with this property. This may be obtained by deleting B suit­
able edges e 1,... ,e B G E, which complete B uniquely determined elemen­
tary cycloids A 1,... ,XB, each of Xk being in T U {ek} and associated with 
the circuit Xk orientated according to the direction of ek ,k = 1,... ,B. Then 
the vector-cycloids X 1,... ,Xb G C 1, associated to (X 1, X 1),..., (XB, XB) as 
in (8.1.4), form a base for C1 and are called Betti cycloids. Furthermore, 
the number B is independent of the choice of the initial maximal tree.
Now we turn back to our original point to express the dynamical status 
of the passages of a particle moving along a cycloid c of G in terms of the 
passage-functions.
First, let us consider that the cycloid c is an elementary directed circuit 
c of G. Then, if i is a point of c =(i1,...,ik,...,is,i1), say i = ik, we have 
Jc (i )=g Jc (i,j ) = g Jc (k,i )=0. 
(8.1.5)
Specifically, there are only two edges of c that make nonzero both mem­
bers of (8.1.5): (ik-1,i) and (i, ik+1). Then relations (8.1.5) become: 
Jc(ik-1,i) = Jc(i, ik+1) = 1 = Jc(i) and consequenty we have the follow­
ing simple intuitive interpretation: a particle moving along c is passing 
through i if and only if it is passing through the edges of c preceding and 
succeeding i. This interpretation allows us to say that a directed circuit c 
passes through a point i if and only if the corresponding passage-function 
Jc satisfies relations (8.1.5).
Now let us consider a cycloid c that is not a directed circuit. Then it 
may happen that a point i belongs to c, but the last inequality of (8.1.5) 
may eventually be not verified by the passage-functions Jc,c(i,j), that is,
y Jc,c(i, j) 
Jc,c(k, i)=o.
Consequently, to describe intuitively the passage along an arbitrary cycloid 
cc, we have to take into account the associated directed circuit (cycle) c; 
namely, we say that a cycloid cc passes through the point i if and only if 
the associated directed circuit c passes through the point i, that is, relations 
(8.1.5) hold for c.
8.2 The Cycloid Decomposition of Balanced 
Functions
We present the following theorem:
Theorem 8.2.1. Let S be a nonvoid set. Assume w is a real function de­
fined on S x S whose oriented graph G is connected, satisfying the folowing 

136 
8. Cycloid Markov Processes
balance equations:
Wd^j(i,j) 
w(k, i), 
i & S. 
(8.2.1)
Then there exists a finite collection C* = {ci,...,cB} of independent 
elementary cycloids in G and a set {a1, .. ., aB } of real nonnull numbers 
such that
B
w (i,j) = 12 aJck,ck (i,j), 
i,j & S, ak & R, 
(8.2.2)
where B is the Betti number of the graph G, ak = w(ik,jk) with (ik,jk) the 
chosen Betti edge for Ck, and Jck,ck are the passage-functions associated 
with the cycloids cck, k =1,...,B. Furthermore, the decomposition (8.2.2) 
is independent of the ordering of C* .
Proof. Let G =(S, E) be the oriented connected graph of w. That is, 
(i, j ) & E if and only if w(i, j ) = 0. With the graph G we associate the 
vector spaces Ci and Ci generated by the edges and cycloids of G, respec­
tively.
Consider now an arbitrary maximal tree 3 = (S, T) of G. Then there 
are edges of E, say e 1 = (i 1, j 1),... ,eB == (iB ,jB), such that E = T U 
{e 1 ,...eB}. Hence, B is the Betti number G. Because 3 is a tree, any 
two points of S may be joined by a chain in T. In addition, that 3 is a 
maximal tree means that each directed edge of E\T = {e1,...,eB}, say 
ek =(ik ,jk ), determines a unique elementary cycloid cck in T U {ek} and a 
unique associated circuit ck with the orientation of ek ,k =1,.. .,B. Then, 
by using (8.1.4), we may assign the unique vector-cycloid ck to the pair 
(Ck ,ck) ,k = 1 ,...,B.
Define
a1 = a1(e1) = w(i1, j1).
Put
w1 (i, j) = w(i, j) - a1Jcc1,c1(i,j), 
i,j& S.
Then w1 is a new real balanced function on S.Ifw1 = 0, then equations 
(8.2.2) hold for C* = {cc1 } and B = 1. Otherwise, w1 remains different from 
zero on fewer edges than w (because w1 is zero at least on the edge (i1 ,j1 )).
Further, we repeat the same reasonings above for all the edges e2 = 
(i2,j2),...,eB = (iB,jB), and define
B
wB(i, j) = w(i, j) - 
akJcck,ck(i,j), 
i,j& S.
where ak = w(ik,jk),k=1,...,B. From the previous construction of the 
elementary cycloids cck and circuits ck ,k =1,...,B, there follows that the 
associated vector-cycloids C1,..., cb form a base for C 1.

8.3 The Cycloid Transition Equations 137
Also, wB (ik,jk)=0,k =1,...,B, and the reduced function wB remains 
a balance function on the tree T, as well. Then wB = 0 (see Lemma 4.4.1). 
Consequently, we may write
B
w (i,j) = 22 ak JCk,ck (i,j), 
i,j G S.
k=1
The proof is complete. 
□
Corollary 8.2.2. Assume the oriented strongly connected graph G = 
(S, E) associated with a positive balanced function on a finite set S x S. 
If {ci, .. . ,cB} is a base of elementary Betti cycloids, then for any i G S 
we have
__BL 
______ B
22 22 JCk,ck (i,j)= 22 22 JCk,ck (u, i) —1. 
(8.2.3)
Proof. Let i G S and let c be an elementary directed circuit of G that 
passes through i, that is,
Jc (i, j )= 
Jc (u, i)=1.
Then we may apply the cycloid decomposition formula (8.2.2) to the bal­
ance function Jc (•, •) on the set E of the edges of G and correspondingly 
we write
Jc(i,j) = 22 Jc(ik,jk)JCk,ck(i,j), 
i,j G S
k=i
where (ii,ji),...,(iB ,jB ) are the Betti edges of G that uniquely determine 
the elementary Betti cycloids cci,...,ccB by the method of maximal tree. 
Consequently, we have
_ 
____ B
1 
22 Jc(i,j) 
^2 22 Jc(ik,jk)Jck,ck(i,j)
______ B
22 Jc (ik ,jk) Jck,ck (u i)
uES k = 1
__L 
____ B
— ^2 22 Jck,ck (i,j) 
^2 22 Jck,ck (u, i).
The proof is complete. 
□
8.3 The Cycloid Transition Equations
Let S be a finite set. Consider the connected oriented graph G =(S, E) 
and denote by C* the collection of all overlapping cycloids occurring in G 

138 
8. Cycloid Markov Processes
(whose edge-set is identical to E). Then each maximal tree of G provides a 
collection B of Betti edges in E. Denote by P(E) the power set of E.
Define the function h: C* x P(E) ^ R as follows:
h (C,A )= 
Jc,c (i,j), if A eP (E), A = 0, and C e C*, (8.3.1)
=0, 
otherwise.
Plainly, for each (i,j) e E, the numbers h(C, (i, j)),C e C*, are the coordi­
nates of the algebraic cycloid C in C 1 defined as
c ^2 Jc,c(i,j) b (i,j).
Furthermore, the function h enjoys some interesting properties given by 
the following.
Proposition 8.3.1. Consider G =(S, E) a connected oriented graph on a 
finite set S, and the measurable space (E, P(E)).
Then the function pc C* x P(E) ^ R defined by (8.3.1) enjoys the following 
properties:
(i) For any C e C* the set function h(C, •): P(E) ^ R is a signed mea­
sure;
(ii) For any A e P(E), the function h(•, A) is P(C*)-measurable;
(iii) For arbitrary cc eC* and A e P(E), the following equations hold
h(c, A) = ^2ueB h(C, W)h(cu, A), 
(8.3.2)
where B denotes a base of Betti edges of G, and for each u eB, ccu denotes 
the unique elementary Betti cycloid associated with u by the maximal-tree­
method.
Proof. (i) We have h(cc, 0 )=0, cc eC* ,and
h(~c, (j An ) = 
h (C,An), c eC*,
n=1 
n=1
for all pairwise disjoint sequences {An}n of subsets of E. Hence h(C, •) is a 
signed measure on P (E ) for any cc eC* .
(ii) That h(', A) is P(C*)-measurable is immediate.
(iii) Let B be the set of Betti edges associated with an arbitrarily chosen 
maximal tree of G. Then by applying the cycloid decomposition formula

8.3 The Cycloid Transition Equations 139
(8.2.2) to Jc,c(i,j), we have
> U.^BW M(,{ {u}) M(u,A) = /> Uu2^ 
Jc,c(u)JCu,Cu (i,j)
( i,j) BA
= E Jc,c (i,j)
(i,j)BA
= M(c, A) •
The proof is complete.
□
Remark. Conditions (i)-(iii) of Proposition (8.3.1) may be paralleled with 
those defining a stochastic transition function from C* to P(E). The basic 
differentiations appear in property (i) where the set function m(e,•) is a 
signed measure instead of a probability on P (E), and in (iii), where equa­
tions (8.3.2) replace the known Chapman-Kolmogorov equations. How­
ever, equations (8.3.2) keep the essence of a transition as in the classical 
Chapman-Kolmogorov equations: a transition from a point to a set presup­
poses a passage via an intermediate point. Specifically, in equations (8.3.2) 
the role of the intermediate is played by a Betti cycloid cu, which is isomor­
phically identified with the Betti edge u. Consequently, Proposition (8.3.1) 
allows us to introduce the following:
Definition 8.3.2. Given an oriented connected graph G =(S, E)onafi- 
nite set S and a collection C * of overlapping cycloids whose edge-set is E, 
a cycloid transition function is any function n: C* x P(E) ^ R with the 
properties:
(i) 
For any c GC*,n(c, {(i,j)}) defines a balance function on S x S, 
that is,
n^{(c{ {(i,j)}) = En (,{{(k, i)}), 
iG S;
jk
(ii) 
For any c G C*,n(c, •) is a signed measure on P(E);
(iii) 
For any c G C*, A G P(E) and for any collection B of Betti edges, 
the following equation holds:
n (,A)) ^2 uBBn (c,{u})n (u u,A) •
(8.3.3)
Relations (8.3.3) are called the cycloid transition equations. 
□
Plainly, they express a homologic rule characterizing the balanced func­
tions.

140 
8. Cycloid Markov Processes
A further interpretation of the cycloid decomposition formula (8.2.2) 
may continue with the study of the cycloid transition equations (8.3.3) as 
follows.
Consider n: C* x P(E) ^ R the cycloid transition function introduced 
by (8.3.1) and assign with each c G C* the balanced function
w(i,j)= n(c,(i,j)), 
(i,j) G E,
=0, 
(i, j) G S2\E.
Then equations (8.3.3) written for w become
w(i,j) = EuEB w(u)J" ' (i,j), 
(i,j) G S2, 
(8.3.4)
where B denotes the set of Betti edges of G associated with a maximal tree. 
Consider further the measurable space (S2 , P(S2)).
Denote by B the vector space of all bounded real-valued functions v on 
S2 whose graphs are subgraphs of G. Then B is a Banach space with respect 
to the norm of supremum.
Define the linear operator U: B ^ B as follows:
(Uv)(’ • )= EuEB v(u) n(Cu, {(•, •)})•
Let now S be the space of all signed finite and aditive set-functions on the 
power-set P(S2 ). A norm on S is given by the total variation norm.
Consider the linear operator V: 5 ^ S defined as follows:
(VX)({u})= E A({(i,j)}) n(2u, {(i,j)}), if u GB,
( i,j) ES 2
= 0, 
otherwise •
Set
(x,v') 
E v(i,j)A({(i,j)}),
(i,j)ES2
for A G S ,v G B.
Let E(1) be the subspace of all eigenvectors v of U corresponding to the 
eigenvalue 1, that is, Uv = v . Then we have the following theorem.
Theorem 8.3.3.
(i) The functions Jc1 ,c 1, • • •, JcB ,cB, associated with the elementary 
Betti cycloids c,, • • •, cb of the connected graph G, form a base for 
the space E (1).
(ii) The space of all solutions to the cycloid formula (8.2.2) coincides 
with E (1).
(iii) For any v G B and for any A GS, we have
(X,Uv') = (VX/vy

8.4 Definition of Markov Chains by Cycloids 141
Proof. (i) From Proposition 8.3.1, we have that the passage-functions 
Jc1 ,c 1,..., JcB,cB belong to E(1). In addition, these functions are in­
dependent. Also, if v G E(1), then v satisfies equation (8.3.4), that is, 
Jc1 ,c 1,..., JcB,cB are generators for E(1).
(ii) This property is an immediate consequence of the definition of U.
(iii) For any A G S and any v G B we have
{A,Uv) = £ A ({(i,j)}) £ uEB v (u) n (c u, {(i,j)})
(i,j) GS S
= 
v(u)(VA)({u}),
uGB
and
iy\v} = £ v(i,j)(va)({(i,j)}) 
(i,j) GS S2
= 
v(u)(vA)({u}).
uGB
The proof is complete. 
□
8.4 Definition of Markov Chains by Cycloids
Let S be a finite set and let G =(S, E ) be an oriented strongly connected 
graph. Let B be the Betti number of G, and consider a base of elementary 
Betti algebraic cycloids C* = {ci,... ,cB}, which correspond to a maximal 
tree in G and to a set of Betti edges (i1,j1),...,(iB,jB). Consider also 
B strictly positive numbers wi,...,wB such that the following relations 
hold
B
w(i,j) 
^2 wkJckck(i,j) > 0, 
(i,j) G E, 
(8.4.1)
w(i) = 
w(i, j) 
w(m, i) > 0, 
i G S, (8.4.2)
where Jgk ,ck (•, •), k = 1,... ,B, denote the passage-functions of the Betti 
cycloids cci ,. . . ,ccB .
If we denote
JCk,Ok (i) = 
J Jck,ck (i,j) 
J JCk,Ok (m,i), i G S,
then
B
w(i) = Z1 wkJckck(i), i G S.

142 
8. Cycloid Markov Processes
Define
= • = ^fc=1 wk Jk ,ck (i,j) 
if (i j) E E
pij 
„b 
. , n (i,j) E E,
Li =1 wk JCk ,ck ( i)
=0, 
if(i,j)ES2\E.
Then P = (pij ,i,j E S) is the stochastic matrix of an irreducible Markov 
chain on S whose invariant probability distribution p =(pi ,i E S ) has the 
entries
w(i) 
^wi), 
its
iE S.
Conversely, given a homogeneous irreducible Markov chain £ on a finite 
set S, the cycloid decomposition formula applied to the balance function 
w(i,j) = Prob(£n = i, £n+1 = j), i, j E S, n =1, 2,..., provides a unique 
collection {{ck}, {wk}} of cycloids and positive numbers, so that, except 
for a choice of the maximal tree the correspondence £ ^ {{ck}, {wk}} is 
one-to-one.
Then we may summarize the above results in the following statement.
Theorem 8.4.1.
(i) Let S be any finite set and let G =(S, E) be an oriented strongly 
connected graph on S. Then for any choice of the Betti base 
C* = {ci,...,Cb} of elementary cycloids and for any collection 
{w1,...,wB } of strictly positive numbers such that relations (8.4.1) 
and (8.4.2) hold, there exists a unique irreducible S-state Markov 
chain £ whose transition probability matrix P= 
(pij ,i,j E S) is de­
fined as
Ek=1 wkJck,ck (i,j) 
EB=1 wkJ-Ok,Ck (i) ,
if (i,j) E E.
(ii) Given a finite set S and an irreducible homogeneous S-state Markov 
chain £= 
(£n), for any choice of the maximal tree in the graph of
£ there exists a unique minimal collection of elementary cycloids 
{cc1 ,...,ccB } and strictly positive numbers {w1 ,...,wB } such that 
we have the following cycloid decomposition:
B
Prob (£n = i,£n +i = j ) ^2 wk J£k,Ck ( i,j ), i,j E S.

8.4 Definition of Markov Chains by Cycloids 143
1/2
0 
0
0
1 / 2 I ,
0
row-vector n = (4/7, 2/7, 1 /7). The
Example. We now apply the cycloid representation formula of Theorem 
(8.4.1) to the stochastic matrix
/1 / 2
P = 
1 / 2
1 
whose invariant distribution is the
graph of P is given in Figure 8.4.1 below. 
Consider the vector w 
52 w(i, j)b(i,j), with w(i, j) = nipij, i,j G {1, 2, 3}.
The set of edges of the graph is {(1, 1), (3, 1), (2, 1)(1, 2), (2, 3)}.
Consider the maximal tree T = {(2, 1), (2, 3)} associated with the Betti 
edges B = {(1, 1), (3, 1), (1, 2)}. Accordingly, the base of Betti algebraic cy­
cloids is as follows:
£1 = 1 • b(1,1), c2 = 1 • b(3,1) + ( - 1) • b(2,1) + 1 • b(2,3), 
c3 = 1 • b(2,1) + 1 • b(1,2),
and they correspond to the graph-cycloids c1 = ((1,1)), c2 = ((3, 1), 
(2, 1), (2, 3)), and c3 = ((2, 1), (1, 2)) associated with the directed circuits 
c1 =(1,1),c2=(3,1,2,3),andc3=(2,1,2).
Then according to Theorem 8.4.1 (ii), the cycloid decomposition of P 
corresponding to the maximal tree T is as follows:
212
nipij 
7 JC1 ,c 1 ( i,j ) + 7 JC2 ,c2 ( i,j ) + 7 Jc3 ,c3 ( i,j ), i,j G {1, 2, 3}•

9
Markov Processes on Banach Spaces 
on Cycles
The problem of defining denumerable Markov chains by a countable infinity 
of weighted directed cycles is solved by using suitable Banach spaces lp on 
cycles and edges. Furthermore, it is showed that the transition probabilities 
of such chains may be described by Fourier series on orthonormal collections 
of homologic ingredients.
9.1 Banach Spaces on Cycles
9.1.1 Euclidean spaces associated with infinite graphs
Now we shall consider an irreducible and positive-recurrent Markov chain 
£ = (£n)n, whose state space S is a denumerable set. The corresponding 
graph G is usually required to satisfy the local finiteness condition, that is, 
for each i G S there are finitely many j G S such that pij > 0 or pji > 0. 
We now explain that the local finiteness condition is necessary for the 
existence of topologies of Euclidean spaces comparable with the topology 
of l2 (R) (according to Hilton and Wylie (1967) p.45).
Let G = (N, E) be an infinite directed graph where N = {n„} are the 
vertices (nodes) of G and E = {enunk } are the oriented edges of G.Tofix 
the ideas, we shall consider that N and E are denumerable sets.
The graph G may be viewed as an infinite abstract simplicial complex, 
noted also by G, where
(i) the vertices nu of G are called 0-simplexes,
(ii) the oriented edges enunk of G (which are completely determined by 
the ordered pairs (nu, nk ) of vertices) are called 1-simplexes.

146 
9. Markov Processes on Banach Spaces on Cycles
Accordingly, the graph G is an oriented complex of dimension 1. To the 
1-dimensional complex G we may attach a topological space, symbolized 
by (|G|, 3) and called the polyhedron of G, as follows. First, to define the 
space-elements of the set |G|, and then the topology 3, we introduce an 
ordering on the set N. This is equivalent, by a homeomorphic translation 
in Euclidean spaces, with a choice of a system of orthogonal axes. Since N 
is denumerable, we may use the index set I = {0, 1,...}, which particularly 
is totally ordered. Accordingly, N = {n0, n 1,...} becomes a totally ordered 
set with respect to the ordering-relation “ j ’’defined as
ni <nj if and only if i < j.
With this preparation we give now the definition of the polyhedron (|G|, 3) 
as follows. To define the set |G|, we first consider a family W of weight­
functions on the vertices and edges of G in the following way:
W = {0w : {0 — simplexes} ^ {1} : 0w (ni) = 1, for any ni G N} U
{1 w : {1 — simplexes = (nik ,nim)} ^ [0, 1] x [0, 1] : 1 w(nik ,nim)
= (1w1(nik), 1w2(nim)), where
(i) 1w1(nik), 1w2(nim) vary in [0, 1],
(ii) 1w1(nik) + 1w2(nim) = 1}.
Or, better we may consider the family W defined as
W = {wi, i G N: wi: N ^ [0, 1], wi(n) = 1, if j = i; or 0, if j = i} U 
{wij, (ni,nj) G E : wij : N ^ [0, 1], wij(nk) > 0 if k = i, j; 
wij(nk) = 0, if k = i,j;andwij(ni)+wij(nj)=1}.
Then the family W involves a weighting procedure according to which we 
attach to each vertex ni of G one nonnegative real weight wi such that
(i) if (ni) is a 0-simplex of G, wi may be chosen to be equal to wi (ni) = 1;
(ii) if ni is a vertex of an 1-simplex (ni,n), then wi may be chosen, 
along with woj , to be the nonnegative real number given by wij , that 
is, woi = wij (ni) > 0, woj = wij (nj) > 0, and woi +woj =1}.
In this way, the images of the weight-functions of W provide a collection 
of sequences which have either the form
(a) (1, 0, 0,...), (0, 1, 0,...),... for the case (i) above,
or, the form
(p) (0,..., 0 ,w i, 0,..., w j, 0,...), for the case (ii) above if i < j, with 
woi, woj > 0, and with woi +woj = 1, where (ni,nj) varies in the set E 
of oriented edges of G.
Then the set |G| is that whose elements are all the sequences of the form 
(a) and (p).

9.1 Banach Spaces on Cycles
147
An equivalent way to describe the set |G| is as follows: associate the 
0-simplex n1 to the sequence (1, 0,...), the 0-simplex n2 to the sequence 
(0, 1, 0,...), and so on.
Furthermore, to each 1-simplex (ni,nj), i < j, associate the subsets bj 
and bij of |G| defined as
bij = {(0,..., 0, w i, 0,••• ,w j, 0,•••) : w i, j — 0, i + w j = 1},
bij = {(0,..., 0, wi, 0,•••, j, 0,•••) : i, wj > 0, with wi + wj = 1 }•
Then
|G| = {(1, 0, 0,•• •), (0, 1, 0,•••) ,••• } U (U(ni ,n. ) bjj )
= { (1, 0, 0 ,,, •), (0, 1, 0 ,,,,) ,,,, } U ( U (nin. ) bij ) •
Now let us see how to define the topology 3 of |G|. Consider the pro­
jection pri associated to the 0-simplex ni and which associates the se­
quence (0, 0, • ••, 1, 0, • ••) (where 1 has the rank i in the sequence) with 
the number 1. Analogously we may consider the projection prij: bj ^ 
R2 for any edge (ni ,nj ) of G, that is, prij associates any sequence 
(0, 0, • • •, 0, wi, 0, • • •, wj, 0, • • •) e bij with the ordered pair (wi, wj)•
Next, for any edge (ni, nj) e G we topologize the subset bj by requiring 
that prij be a homeomorphism in R2 . Then, we topologize |G| by specifying 
its closed sets: A C |G| is closed if and only if A A bj is closed in bij for 
every 1-simplex (ni ,nj )ofG.
The topology 3 of |G| may be in some cases (involving conditions on the 
configuration of the graph G) compatible with the topology of Euclidean 
spaces defined by the metric p((xi), (yi)) v52 (xi - yi)2• Such a case is 
given by the graphs which are locally finite (i.e., each vertex belongs only 
to finitely many edges) and contain denumerable sets of vertices and edges.
Let G = (N, E) be such a graph. Then G can be realized in 12(R) = 
{(xn)n: xn e R,52 (xn)2 < ^ } by the inclusion (see Hilton and Wylie 
(1967), p.45). n
9.1.2 Banach spaces on cycles
Let N = {n 1, n2, • • • } and let C = {c 1, c2, • • • } be a sequence of overlapping 
directed circuits or cycles in N as those corresponding to an irreducible 
and positive-recurrent Markov chain. Then the Vertex-set C and Arc-set 
C will symbolize the sets of all vertices and edges of C , respectively. 
Throughout the paragraph we shall assume the collection C of directed 
circuits in N such that Vertex-set C = N, and we shall consider arbitrary 
orderings on N and Arc-set C. For instance, without any loss of generality, 
we shall assume that the first p (c 1) points and pairs of N and Arc-set 
C will belong to the circuit c1, the next p(c2 )toc2, and so on. Also we 
shall assume that any circuit c =(i 1 ,i2,•••,■^,1 1) of C has all points 
i 1, i 2, • • • ,is distinct each from the other.

148 
9. Markov Processes on Banach Spaces on Cycles
Now let G = (N, E) be the oriented graph associated with C, that is, 
N =Vertex-set C and E = Arc-set C, and assume that G is locally finite. 
With every pair (i,j) e E we associate the symbol b(i,j). Then, since 
a directed circuit c =(i 1 ,i2,...,is,i 1),s > 1, is completely defined by 
the sequence (i1, i2), (i2,i3),...,(is,i1) of directed edges, we may further 
associate c with the sequence of symbols b(i1,i2), b(i2,i3),..., b(is,i1) .An 
equivalent version is to associate any circuit c of C with the formal 
expression c = b(i 1 ,i2) + b(i2,i3) +---------- + b(is,i 1) = S(i,j) Jc(i, j) b(i,j), where
Jc (i, j) is the passage-function which equals 1 or 0 according to whether 
or not (i, j )isanadgeofc.
Then the sets B = {b (i,j), (i, j) e E} = {b 1, b 2,... } and C = {c 1 ,c 2,... } 
will be ordered according to the chosen orderings on E and C, respectively.
With these preparations we shall now define certain Banach spaces by 
using the sets C, N = Vertex-set C and E = Arc-set C. In this direction 
we first introduce the vector spaces generated by N = {n 1, n2,... }, B = 
{b 1, b2,... } and C = {c1, c2,... }, respectively. Let
s
N = {n = 
xknk : s e N,n e N, xk e R},
k=1 
r
E = {b = ^2akbk : r e N,ak e R,bk e B}, 
k=1 
m
C = {c 
Wkck : m e N,Wk e R, ck e C},
k=1
where n, b and c are formal expressions on N, B and C, and N and R 
denote as usual the sets of natural and real numbers, respectively.
Then the sets N, E,andC may be organized as real vector spaces with 
respect to the operations + and scalar-multiplicity defined as follows. For 
the formal expressions of N, we define
s 
r
X^xknk + 
Xk' nk = £(Xk + Xk')nk,
Axknk = 
(Xxk)nk,X e R.
k=1 
k=1
Then N will become, except for an equivalence relation, a real vector space, 
which is isomorph with
<r(N) = {(x 1, x2,... ,xs, 0, 0,...) : s e N,Xk e R,k = 1,..., s}.
Analogously, the set E becomes, except for an equivalence relation, a real 
vector space whose base is B , if we shall not adhere to the notational 
convention: b(j,i) = -b(i,j), (i, j) e E.

9.1 Banach Spaces on Cycles
149
Then E is isomorph with
a( E) = {(w (i 1 ,j 1),.. .,w (in, jn), 0, 0,...) :
n e N,w (ik ,jk) € R, (ik ,jk) € E, k = 1,... ,n}.
Here the index k of (ik, jk), k =1,...,n, means the k-th rank according to 
the ordering of E, that is, b(ik,jk) = bk , k =1, 2, ......
As concerns the set C we define analogously the vector space opera­
tions and note that some vectors ck e C may perhaps be linear expressions 
of other vectors of C. To avoid this, we shall assume that C contains only 
directed circuits ck whose generated vectors ck in C C C are linear indepen­
dent. This assumption may be always achieved by applying Zorn’s lemma 
to any countable collection C, which perhaps contains linear dependent 
vectors. Then C may be correspondingly organized (except for an equiv­
alence relation) as a real vector space whose base is C. Furthermore C is 
isomorph with
c( C) = {(Wc 1,. .. ,Wcm, 0, 0,...) : m € N, wck € R, Ck € C,k = 1, .. ., m}.
Since C is a vector subspace of E , it is isomorph with the following subspace 
of ct(E) :
C(E)= {( 
wck Jck(i1,j1),..., 
wck Jck(in,jn),0,0,...):m€N,wck € R,
k=1 
k=1
ck € C, k =1,...,m;(iu,ju) € Arcset{c1,...,cm},u=1,...,n}.
We proceed by introducing certain norms on the vector spaces N , E ,and 
C. For instance, we define the functions |-|k : E ^ R,k = 1, 2, as follows:
| 
ak bk|1 = 
|ak|,
k=1 
k=1
r 
/ r 
\ 1 /2
| 
ak bk|2 = 
ak2 
. 
(9.1.1)
Analogously, we define the functions ||-^fe : C ^ R,k = 1, 2, as follows:
11^2 Wk ck II1 ^2 |wk|, 
k=1 
k=1
m 
/ m 
\ 1 /2
|| X1 wk ck II2 =( X1 wk J . 
(9.1.2)
In an analogous way we may define similar norms on N. Then N, E, and 
C will become normed spaces with respect to the above norms, and con­
sequently we may compare them with the following classic Banach spaces 

150 
9. Markov Processes on Banach Spaces on Cycles
associated with the original collection C of circuits:
11(N2) = {W= (w(i,j), (i,j) e N2): w(i,j) e R, £(i,j) |w(i, j)I < <*>}>
12(N2) = {W = (w(i,j), (i,j) e N2): w(i,j) e R, £(i,j)(w(i,j))2 < «>},
l1(C) = {(wc, c e C): wc e R, £c|wc| < <*>},
l2(C) = {(wc, c e C): wc e R, £c(wc)2 < <*>}, 
where the corresponding norms for the spaces l1 (N2) and l2 (N2 ) are re­
spectively given by:
/ w /1 = £( i,j)|w ( i,j ) b
/ w / 2 = (£( i,j)( w (i,j ))2)1 / 2,
and for the spaces l1 (C) and l2 (C), by
//(wc)c//1 = £c|wc|, 
//(wc)c//2 = (£c(wc)2 )1/2 .
Consequently, the normed vector spaces (E, | |k),k=1, 2, are isomorph 
with (ct(E), //k) (viewed included in (lk(N2), / / k)), k = 1, 2.
Analogously, the normed vector spaces (C, || ||k),k = 1, 2, are isomorph 
with (ct( C),// //k) ,k = 1, 2. Similar reasonings may be repeated for the 
space N as well.
All previous normed vector spaces are incomplete with respect to the 
corresponding topologies induced by the norms above. Then we may fur­
ther consider the corresponding topological closures of (C(E),//k) and 
(C, || ||k), k = 1, 2, which, except for an isomorphism, provide Banach sub­
spaces in lk(N2), k =1, 2, and the Banach spaces lk(C),k = 1, 2, respec­
tively.
Let us now consider c = m= m=1 wck ck e C. Then, the isomorph of c in 
ct(C) will be denoted by c', and in C (E) by c". Throughout the paragraph 
we shall adhere to this notation for any vector of cl C , where cl symbolizes 
the topological closure of C with respect to ||||k,k = 1, 2.
Correspondingly we have
//c'//1 = Iwck |, 
k=1
/ c" /1 = £( i,j)
m
wck Jck (i, j)
/ c" / 2 =
k=1
1/2

9.1 Banach Spaces on Cycles
151
Consider the vector spaces E and C. Define the function < •, • >: E x E ^ R 
as follows :
< £ ak bk, 
ak bk > = mi£m) ak ak.
k=1 
k=1 
k=1
Then (E, < •, • >) is an inner product space. Analogously, define the inner 
product space (C, < •, • >'). Then the corresponding norms induced by the 
inner products < •, • > and < •, • >' are given by the relations (9.1.1) and 
(9.1.2).
Since E and C are incomplete metric spaces, we may further consider 
their completions H(E) and H(C) along with the corresponding exten­
sions of < •, • > and < •, • >' . Also, since the sets B = {b 1, b2,... } and 
C = {c1, c2,... } are orthonormal bases of H (E) and H (C), we may con­
sequently write any x e H (E) and any y G H (C) as the following Fourier 
series
oo
x a ak bk, 
k=1
o
y 
akck,
k=1
where ak = < x, bk > and ak=<y, ck >', k = 1, 2, • • •, are the corresponding 
Fourier coefficients.
Furthermore, according to the Riesz-Fischer representation theorem, we 
may write
H (E )= x = £ akbk : ak G R, £( ak )2 < U , 
k=1 
k=1
and
H(C) = s y = E ackck: ack e R^2(ack )2 < 
.
I k=1 
k=1 
)
Since B and C are denumerable orthonormal bases, the Hilbert spaces 
H (E )andH (C ) are, respectively, isomorph (as normed vector spaces) with 
l2(E) and l2(C).
Finally, a Hilbert space H(N) may also be defined, by developing a similar 
approach to the vector space N.

152 
9. Markov Processes on Banach Spaces on Cycles
9.2 Fourier Series on Directed Cycles
One problem to be solved in this section has the following abstract formu­
lation:
Find the class of all sequences w = (w(i,j) G R, (i,j) G N2), N = 
{1, 2,...}, which satisfy the following conditions:
(i) There is a countable collection (C, wck ) of directed cycles in N 
and real numbers wck such that the Vertex set C = N and
w(i,j) = 
wck Jck (i, j), (i,j) G Arset C, (9.2.1)
k=1
=0, 
otherwise,
where the series occurring in (9.2.1) is absolutely convergent for 
any (i, j), and all involved sets as N2, C, etc., are endowed with 
certain orderings.
(ii) There is p > 1 such that w G lp(N2).
If sequence w = (w(i, j), (i,j) G N2) verifies the above conditions (i) and 
(ii), then we shall say that w satisfies the cycle formula for p and (C,wc). 
In this case, collection (C,wc) is called a cycle representation for w.
Throughout this paragraph we shall consider a collection C = {c1, c2,...} of 
independent homologic cycles associated with a collection C = {c1,c2,...} 
of overlapping directed circuits with Vertex set C = N . Also, we shall as­
sume (without any loss of generality) that the corresponding graph-sets 
associated with C are symbolized and ordered as mentioned in the previ­
ous section.
The spaces to be considered here are the Banach spaces lk(C) and cl C(E) 
(in lk(N2)),k = 1, 2, where C will be identified by an isomorphism of vector 
spaces either with ct(C) or with C(E).
We shall now answer the question of whether or not the Fourier series 
770=1 wck ck may define a sequence (w(i, j), (i, j) G N2) which satisfies the 
cycle formula following Kalpazidou and Kassimatis (1998). Namely, we have
Theorem 9.2 .1. Let the Fourier series
oo
wfkckck ck G H (C ), 
k=1
where wck ,k =1, 2,..., are positive numbers.
Then the fol lowing statements are pairwise equivalent:
(i) Except for an isomorphism of vector spaces, the sequence 
{77n=i wck ck }n converges coordinate-wise, as n ^ w, to a sequence 
w = (w(i, j), (i, j) G N2), which satisfies the cycle formula for p = 1 
and with respect to (C, wc). Furthermore, /w/1 
0=1 P(ck) wck;

9.2 Fourier Series on Directed Cycles 153
(ii) EEi p(ck)wck < ™;
(iii) Except for an isomorphism of vector spaces, the sequence 
{77n=i wck ck}n converges in 1i(N2), as n ^ <x.
Proof. First we shall prove that (i) implies (ii). Let w m = 
m=k=c wck C k, m = 1, 2,..., with wck > 0, k, = 1,... ,m. Then the isomorph 
w m" of wm in C(E) is given by
w m I 
' wck Jck (i 1, j 1), . . . , 
w wck Jck (in, jn), 0, 0, ... j ; m 1, 2, ... ,
k=i 
k=i
where (i 1 ,j 1),..., (in, jn), are the first n edges of Arc-set {c1,..., cm{ in­
dexed according to the ordering of Arcs-set C = E. If (i) holds, then for 
any (i,j) e N2 there exists a positive number w(i,j) defined as follows:
w(i,j)= lim V wck Jck (i,j), if (i,j) e E, 
m—>^> < ■*
k=1
=0,
otherwise.
Denote w = (w(i,j), (i,j) e N2). Then
oo 
o
p (ck) wck = 
wck Jck(i,j) = 12 w(i,j) । < ^.
The proof of (ii) is complete.
Let us now prove the converse implication. Accordingly, assume that 
relation (ii) holds. Then, for any (i, j) e E the limit 
m
lim
m—o
wck 
k=1
Jck (i, j )
exists, since
oo 
oo
p(ck)wck = 
wck Jck (i,j) < ^.
k=1 
(i,j) k=1
Define w = (w(i,j), (i,j) e N2) with
w(i,j) = 
wck Jck (i, j), 
if (i, j) e E,
k=1
=0, 
otherwise.
Then w satisfies the cycle formula for p = 1 and with respect to (C, wc). 
Furthermore, we note that w is the coordinate-wise limit of {£7m=1 wckck}m 
viewed isomorphically in C(E). The proof of (i) is complete.

154 
9. Markov Processes on Banach Spaces on Cycles
Let us now prove that (ii) implies (iii). From the relation (ii) we obtain 
that
oo 
wckJok JCk (i,j ) < ™, 
k=1
for any (i,j) E E. Then we may accordingly define the following sequence 
w = (w(i, j), (i,j) E N2) in l1(N2):
w(i,j)= wCkJCk(i,j), if (i, j) E E, 
k=1
=0, 
otherwise.
Consider wn" = (wn"(i, j), (i, j) E N2) with
wn”(i,j) = 
wck JCk (i,j)•
k=1
Then wnzz E C(E) and
/w - wn"/1 =
= 
| 
wCk 
JCk 
(i,j) - wCk 
JCk 
(i, j)|
(i,j) EE k = 1 
k =1
(
° 
\ 
°
wCk wck JCk (i,j H 
22 P(ck ) wCk < ^
k=n+1 
k=n+1
Furthermore
OQ
lim /w - w n'/1 = lim V p (ck) wCk =0 • 
n—>o 
n—>o < ■*
k=n+1
Therefore, the sequence of w n", n = 1, 2, • • •, which are the isomorphs of 
wn = En==1 Wckck in C(E), converges in 11(N2) to w=(w(i,j), (i,j) E 
N2), as n — ^. The proof of (iii) is complete.
Now we shall prove the converse, that is, from (iii) we shall obtain relation 
(ii). Let w = (w(i, j), (i, j) E N2) be the 
n
lim
n—>o k=1
wCk c k in l 1( N 2),
where wn = n= n=i wCkck is isomorphically viewed in C(E).
Since for every n > 1 and any (i, j) E N2\E we have ^2n=i wCk JCk (i, j) = 
0, then w(i, j ) = 0 outside E. Therefore
w ( i,j )= £ wCk JCk ( i,j ) ,
k=1
for any (i,j) E E and /^w/1 
V p(ck)wCk < ^.
k=1

9.2 Fourier Series on Directed Cycles 155
Furthermore, from convergence
lim /w - wn' /1 = 0, 
n—>^>
where w n" is the isomorph of w n in C (E), we may write
/ w - wJ/1 = £ |w ( i,j ) - E wck Jck ( i,j ) |
( i,j) EE 
k = 1
(
oo 
\ 
oo
wck Jck (i, j) = 
p(ck)wck
k=n+1 
k=n+1
and
'
lim 
p(ck)wck =0.
n—o k=n+1
The proof of Theorem is complete. 
□
Now we shall investigate the relations between the Hilbert spaces H(N ), 
H(E), H(C), and the sequences that satisfy the cycle formula. We have:
Theorem 9.2 .2. Let Fourier series
' 
wckw<eck e H(C), 
k=1
with wck > 0,k =1, 2,...
Then the following statements are pairwise equivalent:
(i) Except for an isomorphism of vector spaces, the sequence 
{En=1 wckck}n converges coordinate-wise, as n ^ <x, to a sequence 
w = (w(i,j), (i, j) e N2), which satisfies the cycle formula for p = 2 
and with respect to (C, wc);
00 
2L
(ii) E(wck )2p ( ck )+2fc ?fc= wck wcs card{( i,j ): Jck ( i,j ) Jcs ( i,j ) = 1} < ^ 
where Jck (i,j) is the passage-function associated with 
ck,k=1,2,...;
(iii) Exceptfor an isomorphism of vector spaces, the sequence 
{E n = 1 wck c k}n converges in H ( E ) to E (i,j )(E o=1 wck Jck ( i,j )) 
b(i,j), as n ^ ~c.
Proof. Let us assume that (i) holds. We shall now prove that relation (ii) is 
valid. Let wm = {Em=1 wckck}, m = 1, 2, •.. Then wm e C and sequence 
{wm}m converges in H(C) to E0=1 wckck. Consider the isomorph wm" of 
wm in C(E). Then
w m" = IE wck Jck ( i 1 ,j 1), • ..^Lwck Jck ( in ,jn ), 0, 0, • • J ,m = 1, 2 ,•••, 
k=1 
k=1 

156 
9. Markov Processes on Banach Spaces on Cycles
where (i1 ,j1),...,(in,jn) are the first n edges of Arcset{c1 ,...,cm} accord­
ing to the ordering of E. Since (i) holds, for any (i,j) G N2 there exists a 
positive number w(i, j) given by
w(i,j) = 
wck Jck (i, j), if (i, j) G E,
k=1
=0, 
otherwise,
and sequence w = (w(i, j), (i,j) G N2) belongs to 12(N2). 
On the other hand, we have
E w 2( i,j ) = E(E wck Jck (i,j)) 2
(i,j) 
(i,j) 
k=1
= E IE ( wCk )2 JCk ( i,j ) + 2 E 
wCk wcs JCk ( i,j ) JCs ( i,j )l
(i,j) 
k=1 
k,s=1;k=s
= 
(wck)2p(ck) + 2 
wckwcs card{(i,j): Jck(i,j)Jcs(i,j) = 1}.
k=1 
k,s=1;k=s
The relation (ii) holds.
Let us now prove the converse: assuming (ii), we shall prove that (i) 
holds. First, we have
E E w Jck (i,j) 
< ™.
(i,j) k=1
Define the sequence w = (w(i, j), (i, j) G N2) as follows:
w(i,j) = 
wck Jck (i, j), 
if (i,j) G E,
k=1
=0, 
otherwise.
Then sequence w satisfies the cycle formula for p = 2 and with respect to 
(C, wc). Furthemore w is the coordinate-wise-limit of the sequence {wm”} 
of isomorphs of wm 
m=i wckck in C(E), given by
w m = IE wck JCk ( i 1,j 1), . . .,^2wck JCk ( in,jn ), 0, 0, . . J , m = 1, 2,..., 
k=1 
k=1
where (i1 ,j1 ),...,(in,jn) are the edges of c1,...,cm. The proof of (i) is 
complete. 
□

9.3 Orthogonal Cycle Transforms for Finite Stochastic Matrices
157
Let us now prove (iii) from (ii). In this direction, we define by using (ii) 
the sequence w = (w(i,j), (i,j) G N2) in 12(N2) with
w(i, j) = E wck Jck (i, j), 
if (i,j) G Arcset C,
k=1
=0, 
otherwise.
Furthermore, the sequence
w m" = I E wck Jck (i 1, j 1), • • 
Jck ( in j ), 0, 0, 
) ,m = 1, 2, ' ' •,
k=1 
k=1
where (i 1 ,j 1),..., (in ,jn) are the edges of c 1,..., cm, converges coordinate­
wise to w. Now we prove that we have more: namely, sequence {wm"}m 
converges in 12( N2) tow, as m — o .In this direction, we first write
/ w - wm' / 2 =
(w(i, j)
(i,j) GE
-. 1 / 7
- £ wck Jck ( i,j ))2
k=1
(i,j)EE
o E 
k=m+1
2 1/2
wck Jck (i, j)
o
(wck)2p(ck)+2
k=m+1
1/2
o
E 
wck wcs card { ( i,j ): Jck ( i,j ) Jcs ( i,j ) = 1 }
Since (ii) holds, both last series occurring in the expression of /w — 
wm"/2 converge to zero, as m — oo. Finally, the isomorphs of wm" 
and w in H(E) are, respectively, £(ij) Q2m=1 wck Jck (i,j)) b(ij) and 
E<ij) (E0=1 wckJck(i,j)) b(ij). 
’
The proof of (iii) is complete.
To prove the converse, assume that the sequence of isomorphs of 
Em=1 wck c k ,m = 1, 2 ,•••, in 12 (N 2) converge to w = (w (i, j), (i, j) G N 2), 
as m — o, where w(i,j)= ko=1 wck Jck (i, j), for any (i, j) G N2. Then, 
since series occurring in (ii) is related to the norm /w/2, relation (ii) holds. 
The proof of theorem is complete. 
□
9.3 Orthogonal Cycle Transforms for Finite 
Stochastic Matrices
Let S = {1, 2, • • •, n}, n > 1, and let P = (pij, i,j = 1, 2, • • •, n) be an irre­
ducible stochastic matrix whose probability row-distribution is n = (ni, i = 
1, • • •, n). Let G = G(P) = (S, E) be the oriented graph attached to P, 

158 
9. Markov Processes on Banach Spaces on Cycles
where E = {b 1,... ,bT} denotes the set of directed edges endowed with an 
ordering. The orientation of G means that each edge bk is an ordered pair 
(i, j) of points of S such that pij > 0, where i is the initial point and j is 
the endpoint. Sometimes we shall prefer the symbol b(i,j) for bk when we 
need to point out the terminal points.
As we have already mentioned in section 4.2.1, irreducibility of P means 
that the graph G is strongly connected, that is, for any pair (i, j) of states 
there exists a sequence b(i,i1), b(i1,i2),..., b(is,j) of edges of G connecting i 
to j. When i = j then such a sequence is called a directed circuit of G. 
Throughout this chapter, we shall consider directed circuits c = 
(i, i1,i2,...,is,i)wherethepointsi, i1,i2,...,is are all distinct.
Let C denote the collection of all directed circuits of G. Then according 
to Theorem 4.1.1 the matrix P is decomposed by the circuits c G C as 
follows:
niPij 
^2 wJc(i,j), 
(9.3.1)
ctC
where each wc is uniquely defined by a probabilistic algorithm and Jc is 
the passage-matrix of c introduced in the previous section. Furthermore, 
equations (9.3.1) are independent of the ordering of C.
Now we shall look for a suitable Hilbert space where the cycle decompo­
sition (9.3.1) is equivalent with a Fourier-type decomposition for P. In this 
direction we shall consider as in section 4.4 two-vector spaces C0 and C1 
generated by the collections S and E, respectively. Then any two elements 
c0 G C0 and c1 G C 1 have the following expressions:
n
c0 = 
xxn = x'n, 
Xh G R, nh G S,
h=1
T
c1 
yk bk = y'b, 
yk g r, bk g e,
k =1
where R denotes the set of reals. The elements of C0 and C1 are, re­
spectively, called the zero-chains and the one-chains associated with the 
graph G.
Let 5: C 1 ^ C0 be the boundary linear transformation defined as 
5c1 = y' n n, 
where
nbj ns =+1, if ns is the endpoint of the edge bj ;
-1, ifns is the initial point of the edge bj;
0, otherwise.
Let
<71 = Ker 5 = {z G C 1: z'n = 0},
where 0 is the neutral element of C 1.

9.3 Orthogonal Cycle Transforms for Finite Stochastic Matrices
159
Then C1 is a linear subspace of C 1 whose elements are called one-cycles. 
One subset of C 1 is given by all the elements c = bi 1 + • • • + bik e C 1 whose 
edges bi1 ,...,bik form a directed circuit c in the graph G. In general, the 
circuits occurring in the decomposition (9.3.1) of P determine linearly 
dependent one-cycles in C1. In Lemma 4.4.1, it is proved that there are B 
one-cycles y 1,... ,YB, which form a base for the linear subspace C1, where 
B is the Betti number of G. When y 1 ,...,1B, are induced by genuine 
directed circuits Y1,■■■, Yb of the graph G, then we call Y1,■■■, Yb the 
Betti circuits of G.
With these preparations, we now prove
Lemma 9.3.1. The vector space C 1 = Ker6 of one-cycles is a Hilbert 
space whose dimension is the Betti number of the graph.
Proof. Let r = {y 1,... ,1B } be the set of Betti one-cycles of G, endowed 
with an ordering. Then
{
B 
yYa*Ik ,ak e R 
k =1
Consider the inner product <,>: C1 x C1 ^ R as follows:
<^2ak 1 k^2bk1k > =12akbk. 
k=1 
k=1 
k=1
Then C1 is metrizable with respect to the metric
d(Z ak 1k, Z bklk 
k=1 
k=1
Therefore (C1, <, >) is an inner product space where r is an orthonor­
mal base. Accordingly, to any one-cycle z = ^5B=1 ak 1 k there correspond 
the Fourier coefficients ak = <z, 1k>,k = 1,■■■ ,B, with respect to the or­
thonormal base r.
Define the mapping f: (51 ^ RB as follows:
(
b 
\
ak lk I = ( a 1 ,...,aB ).
k=1 
/
Then f preserves inner-product-space structures, that is, f is a linear bi­
jection which preserves inner products. In particular, f is an isometry. 
Then ((51, <, >) is a Hilbert space, whose dimension is B. The proof is 
complete. 
□
B
(ak - bk)2.
k=1
The previous result may be generalized to any finite connected graph G. 
Now we shall focus on graphs G(P ) associated with irreducible stochastic 
matrices P. Denote by B the Betti number of G(P). Consider the collection 

160 
9. Markov Processes on Banach Spaces on Cycles
C of cycles occurring in the decomposition (9.3.1), endowed with an order­
ing, that is, C = {c1,...,cs},s > 0. Then we have
Theorem 9.3.2. Let P = (pij ,i,j = l,...,n) be an irreducible stochas­
tic matrix whose invariant probability row-distribution is n = (n 1, ...,nn). 
Assume that {71, .. ., yb} is a collection of Betti circuits. Then nP 
has a Fourier representation with respect to r = {y 1,... ,Y b } where the 
Fourier coefficients are identical with the probabilistic-homologic cycle­
weights wY 1, ..., wYB, that is,
^'b^(‘j ( (i,j) = ^2 w Y k 1k, 
wYk e R, 
(9.3.2)
(i,j) 
k=1
with
wYk = <nP,Yk >, k = 1 ,...,B.
In terms of the (i, j)-coordinate, equations (9.3.2) are equivalent to
B
niPij 
22 wYk 'JYk (i,j), 
wYk e R; i,j e S. 
(9.3.3)
k=1
If P is a recurrent stochastic matrix, then a similar representation to 
(9.3.2) holds, except for a constant, on each recurrent class.
Proof. Denote w(i, j) = nipij, i,j = 1,..., n. Then nP may be viewed as 
a one-chain w = S(i,j)w(i,j)b(i,j).
Since nP is balanced, w is a one-cycle, that is, w e CC1 = Ker 6. Then, 
according to Lemma 9.3.1, w may be written as a Fourier series with respect 
to an orthonormal base r = {y 1,... ,YB } of Betti circuits of G, that is,
w = 22 < w ,1 k >1 k, 
(9.3.4)
k=1
where < w,Yk >,k = 1,... ,B, are the corresponding Fourier coefficients. 
On the other hand, the homologic-cycle-formula proved by Theorem 4.5.1 
asserts that w may be written as
w^2 wYk 1k, 
(9.3.5)
k =1
where wYk ,k =1,...,B, are the probabilistic-homologic cycle-weights 
given by a linear transformation of the probabilistic weights wc ,c e C ,oc- 
curring in (9.3.1), that is,
wYk = 22 A(c,1k ) wc, 
A(c,1k ) e Z,
cEC
where Z denotes the set of integers.

9.4 Denumerable Markov Chains on Banach Spaces on Cycles 161
Since representation (9.3.5) is unique, it follows that it coincides with the 
Fourier representation (9.3.4), that is,
wYk = < W ,Tk >, k =1, 2 ,...,B.
Accordingly, since c = SKA (c, y k) Yk, then
A(c,Yk) = <c,lk >, 
k = 1 ,...,B,
and therefore
wYk 
22 < c,1k >wc. 
(9.3.6)
ctC
Let us now suppose that P has more than one recurrent class e in S = 
{1,...,n}. Then we may apply the previous reasonings to each recurrent 
class e and to each balanced expression
ne (i)Pij = ^2 wYk JYk (i,j ), 
i,j G e,
k=1
where B = Be is the Betti number of the connected component of the graph 
G(P) corresponding to e, and ne = {ne(i)} (with ne(i) > 0, for i G e, and 
ne (i) = 0 outside e) is the invariant probability distribution associated to 
each recurrent class e. The proof is complete. 
□
Remark. Let w = (w(k), k =1, 2, ..., B) be defined as
w(k) = wYk, 
k=1, ..., B,
where wYk, k =1, ..., B, are the probabilistic-homologic weights occurring 
in (9.3.5). Then equations
w(k) = 22 <c,Y k >wc 
ctC
may be interpreted as the inverse Fourier transform of the probabilistic 
weight-function wc, c G C, associated with P.
9.4 Denumerable Markov Chains on Banach 
Spaces on Cycles
Now we are prepared to show how to define a denumerable Markov chain 
from a countable infinity of directed cycles by using the Banach spaces on 
cycles investigated in the previous sections. Namely we have
Theorem 9.4.1. Let C = {c1,c2,...} be a countable set of overlapping di­
rected circuits in N that verify the assumptions mentioned in section 9.2.
If sequence w = (w(i,j), (i,j) G N2) satisfies the cycle formula for 
p =1 and with respect to (C, wc), with wc > 0, c G C, then pij = 
w(i, j)/(Sj w(i, j)), i, j G N, define a stochastic matrix of an N -state 

162 
9. Markov Processes on Banach Spaces on Cycles
cycle Markov chain £ = (£n)n, that is,
E WcJc (i,j )
pij = cT'-----FW, if(i,j) G ArcsetC,
wcJc(i)
cEC
=0, 
otherwise,
/ oo
where Jc (i ) = S j Jc (i,j) ,i G N, c G C. Furthermore, p, = E wck Jck (i),
\k=1
i = 1, 2, ... is an invariant finite measure for the Markov chain £.
Proof. Let w = (w(i, j), (i,j) G N2) be a sequence of 11(N2), which sat­
isfies the cycle formula with respect to a collection (C, wc), with wc > 0, 
that is,
w(i,j) = 
wck Jck (i, j), if (i, j) G Arcset C,
k=1
=0, 
otherwise.
We may always find such a sequence if we choose the sequence {wck ,k= 
00 . .
1, 2,...} of positive numbers such that E P(ck)wck < ^ (as in condition 
k=1
(ii) of Theorem 9.2.1).
Define
w(i) = 
w(i,j),
iG N.
Then w(i) > 0,i G N, and
W ( i )= £ Wck Jck (i ), 
k=1
where Jck(i) = 
Jck (i, j) for any i G N.
Define
w(i, j) 
pij I -\ , 
w(i)
i,jG N.
Then P = (pij ,i,jG N) is a stochastic matrix that defines an N-state cycle 
Markov chain £ = (£n)n whose cycle representation is (C, wc). Also,
o
yzw (i)=z1 p (ck) wck < ^
and
w(i)pij = 
w(i,j) = 
wckJck(j) = w(j),
i 
i 
k=1
for any j G N. Then ^ = (w (i), i = 1, 2,...) is an invariant finite measure 
for the Markov chain £. The proof is complete. 
□

10
The Cycle Measures
Further interpretations of the circuit representations of balanced functions 
in terms of the electrical networks require a compatibility with dual-type 
concepts as are the measures on weighted cycles, called cycle measures. 
The present chapter is devoted to the generic definition of the cycle mea­
sures and their interconnection with the cycle decompositions of balanced 
functions.
10.1 The Passage-Functions as Characteristic 
Functions
Consider a finite set S and a strongly connected oriented graph G =(S, E). 
The passage-function 12: P (E) x S2 ^ {0, 1} through the subsets of edges 
E G P (E) is defined to be the characteristic function
i -\ 
1 1, if (i,j) G E, 
z~
I2 (E; i, j)= 0, if (i, j) G E. 
(10.1.1)
An analogue definition may be given for the passage-function I1 : P (S) x 
S ^ {0, 1} through the subsets of points of S.
Then an extension of the passage-function I2 to Sk ,k > 2, is as follows:
Ik ( E; i 1,... , ik ) = I2( E; i 1, i 2) • I2 ( E; i 2, i 3) • • • I2 ( E; ik-1, ik ), 
(10.1.2)
for any E C E and i 1,..., ik G S. Accordingly, we define Ik: P (S) x Sk ^ 
{0, 1},k =2, 3,..., as extensions of I1 .

164 
10. The Cycle Measures
Then
Ik (E; i 1,... ,ik) = Ik-1( E; i 1,..., ik-1) • 12( E; ik-1, ik),
for any E C E and i 1,..., ik g S.
If Ik(E;i1,...,ik) = 1, then either (i1,i2,...,ik) or ((i1,i2), (i2,i3),..., 
(ik-1, ik)) is called a directed polygonal line in E. When ik = i1 the polygo­
nal line is called closed. Then (i1,..., ik-1, i1) is a directed circuit or cycle 
in E.
Conversely, if c =(i1,...,ik-1,i1),k > 1, is a directed circuit in S, then 
by considering the set c = {(i 1, i2),..., (ik- 1, i 1)} of edges of c we have
Is (c; k 1 ,...,ks) = 12( c; k 1 ,k 2) • 12 (c; k 2 ,k3) ••• 12( c; ks-1, ks),
for any s > 2 and any k 1,... ,ks G S. Correspondingly, if c denotes the set 
of points of c then
f I''. 7 
7 X T I''. 7 X T I''. 7 X T I''. 7 X
Is (c; k 1,. ..,ks) = 11( c; k 1) • 11( c; k 2) • • • 11( c; ks).
One property which differentiates Is (c; i 1,..., is) from Is (E; i 1,..., is) 
(with E = c) is that the former is balanced. However, both passage­
functions Is (c; •) and Is (E; •) satisfy the product formula
Is(E; i1,...,is)=I2(E; i1, i2) • I2(E; i2, i3) •••I2(E; is-1, is),
for any i1 ,. . .,is G S.
Since the product formulae are subjects of special importance in Proba­
bility Theory, we are further interested in the comparison of the passage­
functions Is (cc; i1,...,is)andJc(i1,...,is).
Recall that for any directed circuit c, Jc(i1,...,is) (which may be denoted 
also Js(c; i1,...,is)) is, according to Definition 1.2.2, the number of the 
appearances of the directed sequence (i1,...,is)alongc.
If c =(i1,...,ip,i1) has only distinct points i1,...,ip,then
Js (c; k1,...,ks )=Is (cc; k1,...,ks ),s> 2,
{
1, if k 1,..., ks are 
consecutive points of c, 
0, otherwise.
where cc denotes the set of edges of c.
In general we have
Proposition 10.1.1. If cc denotes the set of edges of any circuit c = 
(i1,...,ip,i1), then the passage-function Jk,k=2,..., introduced by Defi­
nition 1.2.2 satisfies the following equation:
Jk (c; i1 ,. . . ,ik )= 
Ik (cc; i1 , . . .,ik )
5^ 1cotj (i 1, i2) • • • 1cotj (ik-1, ik), 
j

10.1 The Passage-Functions as Characteristic Functions
165
where j ranges the set of integers t such that 0 < t < p(c) — 1 and 
(coti)(m) = im,m = 1,... ,k, k > 2. (Here p(c) denotes as always the pe­
riod of c and 1 g denotes the characteristic function on c.)
If c is an elementary circuit then the specialization of Proposition 10.1.1. 
is as follows:
Jk (c; k i,.. ., ks) = Ik(c; k 1,. .., ks),
or, else
Jk (c; k i, ...,ks) = J2( c; k i ,k 2) • J2( c; k 2, k 3) ■■■ J2( c; ks-1, ks),
for any integer s > 2 and for any k 1,... ,ks G S.
Consequently it will be interesting to look for a product formula when c 
is any oriented circuit. A first step is given by the following:
Proposition 10.1.2. Given a finite set S, a strongly connected oriented 
graph G =(S, E ), any integer k > 3 and an arbitrary directed circuit c in 
S, we have
Jk(cii,...,ik) =0, ii,...,ik G S, 
(10.1.3)
if and only if there exist two circuits ci and c2 such that
Jk-i(ci; ii,...,ik-i) ■ J2(c2; ik-i, ik) =0, 
(10.1.4)
where Jk is introduced by Definition 1.2.2.
Furthermore, there exist certain circuits c 1 ,c2,c, and c" such that the 
following decompositions hold modulo the cyclic permutations:
Jk(c; i1,...,ik)=Jk-1(c1; i1,...,ik-1) ■ J2(c2; ik-1, ik) (10.1.5)
Jk(c; i 1,..., ik) = J2(c; i 1, i2) • Jk-1(c”; i2,..., ik), 
(10.1.6)
for any i1,...,ik G S.
Proof. We shall prove the equivalence of relations (10.1.3) and (10.1.4), 
and then relation (10.1.5) by induction with respect to k > 3.
Let k =3 and let c be an elementary directed circuit in S. If relation 
(10.1.3) holds, then by choosing c1 = c2 = c we may write
J3(c; i1,i2,i3) = J2(c1; i1, i2) ■ J2(c2; i2,i3)=1.
If c is not elementary such that
J3(c 1; i 1 ,i2,i3)= (> 1, 
i 1 ,i2,i3 G S,
then by Definition 1.2.2, we may find two elementary cycles c and c", such 
that c' C c, c" C c and
J2(c; i 1, i2) J2(c”; i2, i3) = 1,
where c denotes the set of all edges of c.

166
10. The Cycle Measures
Then we may define a circuit c 1 by repeating I times c, and choose c2 = c". 
Consequently, we have
J2(c1; i1, i2) J2(c2;i2,i3) =
and we have proved (10.1.4) from (10.1.3) when k =3.
To prove the converse, suppose that (10.1.4) holds for two elementrary 
cycles c1 and c2, that is,
J2( c 1; i 1, i 2) J2 (c 2; i 2 ,i 3) = 1, i 1, i 2 ,i 3 £ S.
Then we may find a cycle c containig c1 and c2 , and which satisfies the 
relation (10.1.3), that is,
J3(c; i1,i2,i3) = 1.
Therefore the required equivalence and relation (10.1.5) are proved for 
k =3.
Now, assume the equivalence of the first two relations, and relation 
(10.1.5) hold for the k>3 points i1,...,ik. Then we have to prove 
the equivalence of (10.1.3) and (10.1.4), and then relation (10.1.5) for 
i1,...,ik,ik+1.Ifc is an elementary circuit and
Jk+1 (c; i1 ,...,ik , ik+1 )=1,
then by choosing c1 = c2 = c, we may write
Jk+1 (c; i1,...,ik,ik+1)=Jk(c1; i1,..., ik)J2(c2; ik,ik+1) = 1.
If c is not an elementary circuit such that
J3(c; i 1,..., ik+1) = £> 1,
then we may find two directed elementary circuits c and c" such that 
c' C c, c" C c, and
Jm ( c ; i 1 ,...,im ) Jk —m +1( c ; im, im +1, . . . , ik, ik +1) 
1,
where 1 <m<k.
Consequently we may repeat many times the induction hypothesis and find 
two circuits c1 and c2 such that
Jk+1(c; i1,...,ik,ik+1)=Jk(c1; i1,...,ik) J2(c2; ik, ik+1).
Therefore we have proved (10.1.4) and (10.1.5) from (10.1.3).
Let us now prove the converse implication under the induction hypothesis 
for the points i1,...,ik.If
Jk (c1; i1,...,ik) J2(c2 ; ik,ik+1) = 1, 

10.2 The Passage-Functions as Balanced Functions
167
for some circuits c1 and c2 , then we may choose a directed circuit c such 
that c = c1 U c2 and
Jk+1(c1; i1,...,ik,ik+1)=1.
If
Jk(c 1; i 1, ..., ik) J2(c2; ik,ik+1) = I > 1,
we may first choose an elementary circuit whose edge-set contains c1 U c2, 
and then by repeating it I times we define a circuit c such that c = c1 U c2 
and
Jk+1(c; i 1,... ,ik, ik +1) 
^.
Then the required equivalence and relation (10.1.5) are proved. Finally, 
relation (10.1.6) follows by using analogous arguments. The proof is com­
plete. 
□
From the course of the proof of Proposition 10.1.2, we may write for any 
directed circuit c the following product formula:
Jk (c; i 1, ...,ik) = Jm (c 1; i 1, ...,im) Jk-m (c 2; im, .. .,ik), i 1, ... ,ik e S,
where k > 3, 2 < m < k and c 1, c2 are suitably chosen circuits in S. Fur­
thermore we have
Corollary 10.1.3. Let S be a finite set and G =(S, E ) be a strongly con­
nected oriented graph. Then for any directed circuit c in S the following 
formula holds modulo the cyclic permutations:
Jk (c; i 1,... ,ik) = J2( c 1; i 1, i 2) • J2( c 2; i 2, i 3) • • • J2( ck-1; ik-1, ik),
i1,...,ik e S,
where k > 3 and c 1, c2,..., ck-1 are suitable directed circuits in S. □
10.2 The Passage-Functions as Balanced 
Functions
We have compared in the previous paragraph the product property of 
two types of passage-functions associated with a directed circuit c = 
(i1 , .. .,ip ,i1 )inS:
Is(c; k 1,..., ks) = 1 s-1 ((k 1 ,k2),..., (ks-1 ,ks)), s = 2, 3,..., (10.2.1)
where 1 s-_ 1 is the characteristic function on the cartesian product cs- 1, 
and
Js (c; k 1,..., ks) = Jc (k 1,..., ks),
which is the number of the appearances of the sequence (k1,...,ks)along

168 
10. The Cycle Measures
c, s =1, 2,..... These two passage-functions are in general distinct except
for the case c is elementary (its points are distinct from each other).
One basic characteristic of both passage-functions above is the property 
of being balanced, that is,
\,-k (c; k 1 ,...,ks-1 ,i) 
^2 Js (c; j,k 1 ,...,ks-1) — Js—1( c; k 1 ,...,ks—1),
IfC.. (C; k 1 ,...,ks—1 ,i) = Y2 Is (C; j, k 1 ,...,ks—1) = Is—1 (C; k 1 ,...,ks—1),
(10.2.2)
for any k 1,..., ks- 1 G S, s > 2, where 11(C; i) denotes 11(C; i), i G S, which 
is introduced in paragraph 10.1. Consequently, we say that Js extends Js-1 
by the balance property. It is the balance property of the passage-functions 
above that we shall investigate in this paragraph.
First we have
Proposition 10.2.1. Given any circuit c and any integer n > 2, the 
passage-functions In (c;' ) and Jn (c;' ) satisfy the following equation:
Jn ( C; i 1, . . . , in ) 
/ y 
Jn-1(C; k 1, . . . , kn-1) ' Jn ( c ; k 1 ,...,kn- 1 ,i 1)
Jn-1(C; k1,...,kn-1)
Jn(c;k2,...,kn-1,i1,i2) 
_ _ • ■
Jn-1 (c; k2 ,...,kn-1 ,i1 )
Jn (c; i1 , . . . ,in ) 
Jn-1 (c; i1 ,...,in-1 )
(10.2.3)
for any i1,...,in e S, when the right-hand side is wel l-defined.
□
From the very definition of the passage-function Is , we may write the prod­
uct formula
In (C; i 1 ,...,in ) = 11( C; i 1) • I2( C.; i1 ,i.2) 
I1(c;i1)
I (c- u i ) = I-I (c- i-1) • I2(C; i1, i2) 
Jn(c; ^1,..., n)) — J1(c; ^1) T / o ■ \
I1(c;i1)
for any points i1,...,in of c, then we have
In (C; i 1, ... ,in ) 
In (C; i 1, ... ,in ),
but in general
Jn (c; i1 , . . . , in ) = Jn (c; i1 , . . . ,in ), 
where Jn is defined as In .
12( C; in-1 ,in)
11(C, in-1)
for any consecutive points i1 ,. ..,in of c.
However, the previous equation does not characterize a balance function.
Specifically, if we introduce
12( C; in-1 ,in)
I1 (Cc, in-1 )
(10.2.4)

10.2 The Passage-Functions as Balanced Functions
169
A sufficient condition for Jn to satisfy relation (10.2.4) is given by the 
following:
Proposition 10.2.2. Let c be any directed circuit in S. Then we have
(i)
J3 (c; i, j, y)=J3 (c; i, j, y)
= J1 (c; i) J2( c; i,j) 
Ji(c;i)
J2( c; j,y) 
Ji(c;j)
for any consecutive vertices i, j, y of c, which satisfy the following con­
dition:
J3( c; i,j,y) = J2( c; j,y) 
J2( c;i,j) 
Ji( c;j)
(10.2.5)
(ii) Also
Jn (c; ii ,...,in )=Jn (c; ii ,...,in )
= Ji(c; ii) J2(c;i1 ,i2)
J1 (c;i1)
J2 (c; in-1, in ) 
J1(c; in-1)
for any integer n>3 and for any consecutive vertices i1 ,...,in of c 
which satisfy the following condition:
Jn ( c; i 1, . . . , in ) 
_  J2( c; in-1, in )
Jn-1 (c; i1 ,...,in-1 ) 
J1 (c; in-1 )
(10.2.6)
Proof. (i) By using relation (10.2.5), we may write
Jc i j v) = J(c i j) • 1 • J3(c; i,j,y) • J (c j) 
t73( c; i, j,y ) — 2(c c; i,j ) J ( c; j ) J ( c; i j ) 11( c; j )
= J3 (c; i, j, y).
(ii) Assume by the induction hypothesis that relation
Jn-1 (c; i1 ,...,in-1 )—Jn-1 (c; i1 ,...,in-1 )
holds. Then by using (10.2.6), we may write
T J-2 ( 
J > — J2(c; in-1, in) 1 
f2-r: 
; 
\
Jn (c; i 1,..., in ) — 
-r / • 
• • Jn-1(c; i 1, ... ,in-1)
J1 (c; in-1 )
T / .. • 
-X
— Jn (c; i1 ,...,in ),
and the proof is complete. 
□
Let us see the previous relations in the following concrete example. Consider 
the circuit c —(1, 2, 1, 2, 1, 2, 1, 2, 3, 2, 3, 2, 1) of period p — p(c) — 12. Then
J3(c; 2,1, 2)
J2 (c ;2,1)
J2(c;1, 2)
J1 (c ;1)
4
4 =1,
=1.
4
4

170 
10. The Cycle Measures
On the other hand we have
J3(c;2,1,2)=4,
J3( c; 2, 1, 2) = Ji( c ; 2) J2(c; 2, 1) 
J1 (c ;2)
J2(c;1, 2)
Ji(c;1)
44=4,
_ , . . . ~ , ...
therefore J3(c; 2, 1, 2) = J3(c; 2, 1, 2).
Further characteristics of the passage-functions which generalize Propo­
sition 10.2.1 are given by the following:
Proposition 10.2.3. Given a directed circuit c and any integer n > 2, 
then any passage-function satisfies the property below:
Jm (c; i 1, . . . , im ) 
J 
Jn-1(c; k 1, . . . , kn-1) ' Jn ( c; k 1,..., kn-1, i 1)
Jn-1(c; k1,...,kn-1)
Jn ( c; k 2 ,...,kn-1 ,i 1 ,i 2) 
Jn-1( c; k 2 ,...,kn-1 ,i 1)
Jn (c; km ,...,kn-1 ,i1 ,.. . ,im )
Jn-1 (c; km ,...,kn-1 ,i1 ,.. .,im-1 )
for any m such that n > m > 2 and for any i 1, ... ,im G S when the right­
hand side is defined.
Let us now consider any directed circuit c in S. Then we may associate 
c with families of passage-functions as follows:
J(c) = {In (c; i 1,..., in), n = 1, 2 ...},
and
J(c)={Jn(c;i1,...,in), n=1,2,...}
where In and Jn are considered as in paragraph 10.1.
Also, if we fix arbitrarily an integer n > 1, we may define further families 
of passage-functions as
F (c, In )={gm (i1 ,...,im ), m=1, 2,...}
and
F (c, Jn )={fm (i1 ,...,im ),m =1, 2,...},
where for m>n the functions gm and fm are given by Proposition 
10.2.3 and for m < n they are given as: gm = Im(c; i 1,... im) and fm = 
Jm (c; i1 , . . . ,im ).
All families Jc), J(c), F (c, In), F (c, Jn) satisfy the compatibility con­
dition. For instance, any function fm G F (c, Jn), m =2, 3,..., satisfies the 

10.3 The Vector Space Generated by the Passage-Functions
171
compatibility equation,
fm (i1 ,...,im-1 ,i)=fm-1 (i1 ,...,im-1 ),
itS
for any i 1,..., im G S. Furthermore, any function hm = Im (or 
Jm, gm, fm), m =2, 3,..., satisfies a system of (m - 1) balance equations, 
that is,
X hk (i 1, ..., ik- 1, i) 
^2 hk (j, i 1,■■■, ik—1) = hk—1 (i 1, •••, ik-1),
for any k =2,...m,andi1,...,ik-1 G S.
If n =2 and c =(i1 ,. ..,ip ,i1 ) is an elementary circuit, then the family 
F(c, J2)={fm(i1,...,im),m=1, 2,...}, provides the finite-dimensional 
distributions of a homogeneous periodic Markov chain £ whose state space 
is {i1,...,ip} and the stochastic matrix is
/ 0 
1 0... 0 \
0 0 1... 0 
. 
-- 
- 
-
100... 
0
The sample paths of £ are obtained by the repetitions of c and have a 
geometric simplicity that corresponds to the inexistence of chaos.
In general, the appearance of chaos involves Markov models £ whose 
geometry of the sample paths is more complexed and is characterized by the 
appearance of (more than 1) directed overlapping circuits c1,...,cm.Then 
a further algebraization will naturally involve the vectors c 1,... ,c m, whose 
coefficients are defined by the passage-functions Jc1 , . . ., Jcm , which in turns 
decompose (by the circuit decomposition formula) the finite-dimensional 
distributions of £. Consequently, we have good reasons to study the vector 
space generated by the passage-functions Jc1 , .. .,Jcm .
10.3 
The Vector Space Generated by 
the Passage-Functions
10.3.1. Consider S a finite set with card S = a > 1 and let F = (S, F) 
be the full-oriented graph on S. In the previous chapters we have studied 
various ways to express dynamics from a point i G S to another point j G S 
by using the graph elements of F. One of them uses the directed circuits 
(cycles) c made up with the edges of F and the corresponding passage­
functions Jc and Cc defined as:
Jc (i, j )=1, if i, j are consecutive vertices of c, 
=0, otherwise,
and Cc(i,j) = -(-) Jc(i,j),i,j G S.

172
10. The Cycle Measures
In Chapter 8, we have studied a more generalized motion along sequences 
of consecutive edges e 1,... ,es G F, which are not necessarily oriented in 
the same way, that is, each edge er has one common endpoint (either the 
starting point or the terminal point) with er-1(= er), and a second end­
point with er +1 (= er), 2 < r < s — 1. When such a sequence is closed and 
contains distinct edges, then it is called a cycloid in S (see Definition 8.1.1). 
Recall that a cycloid whose vertices occur once is called an elementary 
cycloid.
Any cycloid 7 may be passed according to one of the orientations of two 
directed circuits 7 and y (the opposite of y) made up with the consecutive 
points of 7.
Now, giving two points i and j in S there exists a cycloid y7 which connects 
i to j and suitable functions J,Y and JY,Y- expressing this connection as 
follows:
JY,Y(i,j) = 1, if (i, j) is an edge of Y and y, 
= —1, if (i,j) is an edge of y7 and y-, 
=0, 
otherwise,
and
JY,Y- (i,j) = - J7y(i,j), i,j G S.
Then by Lemma 8.1.2 both cycloid passage-functions JY,Y and JY,Y- are 
real balanced functions.
Denote by C and r the set of all elementary directed circuits in S and 
the set of all elementrary cycloids of graph F, respectively. Then C C r.
Consider now the vector space B(S2) generated by the cycloid passage­
functions
{JyiY: S2 ■ { 1, 0, 1 }, 7 G r}.
Then
B(S2) = {w: S2 ^ R: w = 
pk J-yk ,Yk ,Yk G r,
k=1
Pk G R,k = 1,..., m; m = 1, 2,...}.
We have
Theorem 10.3.1. The vector space B(S2 ) generated by the cycloid 
passage-functions {JY,Y,Y G F} contains all real balanced functions on S2. 
Furthermore, we have
B(S2) = {w: S2 ^ R: w is balanced}
= {w: S2 ^ R: w 
Pk JYk,Yk, Y1,.. . ,YB G r independent elementary
k=1
cycloids; P 1,..., PB G R; B = 1,..., a2 — a + 1}.

10.3 The Vector Space Generated by the Passage-Functions
173
Proof. Any passage-function J,Y (i,j). i,j G S, defines a one-cycle
7 = ^2 J,y(i,j) b(i,j) •
(See definitions and notations of paragraph 4.4.)
Correspondingly, we may associate the balance function w : S2 ^ R with 
a one-chain
w = £ w(i,j ) b(ij) •
Assume for the sake of simplicity that the oriented graph Gw of w contains 
only one connected component and let B be its Betti number. Then B = 
1. 2. • • • .a2 — a + 1 and by applying Theorem 8.2.1 to w, we may write w 
in terms of B independent elementary cycloids {71. • • •, Yb} of the graph 
Gw as follows:
w(i,j)= 52pkJk,Yk(i,j). i,jGS;pkGR,k =1 ’■■■,B-
k=1
Specifically, the cycloids Y71 , • • •,Y7B are given by the maximal-tree­
method, which uniquely defines p k = w (ik ,jk) ,k =1 .•••./>, where 
(i 1 ,j 1) ,..., (iB ,jB) are the corresponding Betti edges in Gw. The proof 
is complete. 
□
Let us now consider the set
B + (S 2) = {w: S2 ^ R + ,w is balanced }•
Then B+ (S2) is a convex cone of B(S2). Furthermore, by applying Theo­
rem 1.3.1, B+ (S2) is “generated” by the circuit passage-functions {Jc.cG 
C }, that is,
m
B + (S 2) = {w: S2 — R+: w 52 aJck ,ck G C.ak G R+, k = 1 .•••.m; m = 1,2 ,•••}• 
k=1
A convex subset of B+ (S2 ), occurring in the theory of stochastic matrices, 
is given by
B 1 + (S2) = {w : S2 ^ R + : w is balanced; 52 w(i.j) = 1 }•
Then B1+ (S2 ) is a convex hull of {Cc. c G C} according to the 
Caratheodory-type decomposition (4.3.2), that is,
z 
m
B + (S 2) 
< w: S2 ^ R+: w 
52 wk Cck ,ck G Cwk > 0. k = 1. • • • ,m;
k=1
m
wk =1.m =1. 2. •• •.a2 — a +1
k=1

174 
10. The Cycle Measures
10.3.2. 
Given S any finite set and w: S2 ^ R + any balanced function, 
then one way to extend w to S3 by the balance property is to consider the 
product
w i j) 
w (i,J) • w (J, k) 
, •>. / n
w3(i,J, k) = ----------Fa--- , 
if w(J) = 0,
w(j)
=0, 
otherwise,
(10.3.1
where w(J) = U^SuES w(u,J). Then w3 is balanced, that is,
7^ w 3( i,J,k) 
^2 w 3( u,i,J) = w (i,J), i,J G S
The extension (10.3.1) may be written in a more sophisticated form as 
follows:
/. . n 
w x w2(k 1 ,i) w2(i,J) w2(J,k) 
zmQOA
w 3( i,J,k )=>,w 1( k 1)-------77-.---------- Fa------------rV 
(10.3.2)
w1(k1) 
w1(i) 
w1(J)
k1
when the right-hand side is defined, where w1(i) = w(i), w2(i,
The expression (10.3.2) allows us to continue the extension of w to 
w4(1, 2, i3, i4) by the balance property, and after consecutive step
wn: Sn ^ R + given by
. A 
w w22(k 1 ,i 1) w2(i 1 ,i2) 
w2(in-1 ,in)
wn(i 1, . . . , in) 
/ w 1 (k 1) 
AJA • 
( • A ••• 
<■ A ,
w1(k1) 
w1(i1) 
wl(in-1)
k1
n =2, 3, .. .,
when the right-hand side is defined.
Then, generalizing the previous motivation we may state
Proposition 10.3.2. Any balance function wn: Sn ^ R + satisfies the fol­
lowing relation:
wn(i1,...,in)= 
wn-1 (k1,...,kn-1) wn(k 1,. . .,kn-1 ,i 1) 
wn-1 (k1 , . . . , kn-1 )
wn (k2, . . . , kn-1, i 1, i2) 
wn-1 (k2, ... , kn-1, i 1)
wn (i 1, i2, . . . , in ) 
wn-1 (i 1, . . . , in-1)
when the right-hand side is well defined.
□
10.4 
The Cycle Measures
There are measures ^ on the product measurable space (S x S, P (S) ® 
P (S)), which satisfy the balance equation ^(S x •) = ^(• x S). In the 
present section we shall show the existence of a reciprocal relation between 
the balanced measures ^ and the linear combinations of weighted cycles 
(or circuits), which motivates the name of a cycle measure for ^.

10.4 The Cycle Measures 175
Let S be a finite set and let G =(S, E) be a strongly connected oriented 
graph. Consider C any collection of directed circuits in S such that the 
edge-set of C is E.
Associate each directed circuit c G C with the passage-functions 1 Ic: S ^ 
{0, 1} and 2 Ic: S x S ^ {0, 1} defined as
1Ic(i) = 1, 
=0,
if i = c(n) for some n G Z, 
otherwise, 
and
2Ic(i, j)=1, if i = c(n), j = c(n +1) for some n G Z, 
=0, otherwise.
Definition 10.4.1. Given any directed circuit c G C define
(i) the passage-function   Jc (A, j) from A G P (S)toj G S as
1(ii)
(i) the set function Ic: P (S) ^ R + defined as Ic(A) = EiEA 1 Ic(i) is 
a measure on P (S):
(ii) for any vertex i of c the set functions 1Jc(•, i) and 2 Jc(i, •) are 
probability measures on P (S);
1Jc(A, j)=1, 
if j = c(n) and c(n - 1) G A for some n G Z, (10.4.1)
=0, otherwise.
and
(ii) the passage-function 2Jc(i, B) from i G S to B G P (S) as
2Jc(i, B)=1, 
if i = c(n) and c(n +1) G B for some n G Z, (10.4.2)
= 0, otherwise. 
□
An immediate consequence of the balance property of 2Ic (i, j) is the 
following:
Proposition 10.4.2. The passage-functions 1Jc and 2 Jc introduced by
Definition 10.4.1 satisfy the fol lowing equations:
(i)
1 Jc(A,j) = £2Ic(i,j), 
j G S, A G P(S);
(ii)
2 Jc(i,B) = 
2Ic(i,j), 
i G S, B G P(S);
(iii)
1Jc(S,j)=2Jc(j,S)=1Ic(j), 
jGS.
Also we have
Proposition 10.4.3. Consider the measurable space (S, P (S)) and the 
directed circuit c G C. Then the following statements hold:

176
10. The Cycle Measures
(iii) for any A, B / P (S) we have
g 2Jc(i,B)=g 1 Jc(A,j). 
°
Now let us consider the product mesurable space (S x S, P (S) ® P (S)). 
For any E C S x S and any i, j / S denote
Ei = {u G S: (i, u) G E},
and
Ej = {v G S: (v, j) G E},
which are usually called sections of E. In particular, for any measurable 
rectangle A x B and for any i G S , we have
(A x B)i = B, if i G A,
= 0, if i /A;
and
(A x B)i = A, if i G B, 
= 0, 
if iG/B.
We have
Proposition 10.4.4. Let (S, P (S)) and let c be any directed circuit ofC, 
Then for any E / P (S) ® P (S) we have
2 Jc(i, Ei) = 
1 Jc(Ej,j). 
(10.4.3)
Proof. We have
E 2 Jc(i,Ei)= £ £ 21c(i,j)= £ 21c(i,j)
= E 
Ic(i,j) = E 1Jc(EJ ,j),
and the proof is complete. 
□
In particular, if E = A x B, A, B G P (S), the equations (10.4.3) become
E 2Jc(i,B)=g 1 Jc(A,j).
Theorem 10.4.5. Let (S, P (S), Ic) be the measure space associated with 
a directed circuit c / C, where Ic(•) is introduced by Proposition 10.4.3.(i).

10.4 The Cycle Measures 177
Consider the set function Jc on P (S) 0 P (S) defined as
Jc(E) = 
2Jc(i,Ei) = 
1Jc(Ej,j), E e P(S) 0 P(S).
Then Jc is a measure on P (S) 0 P (S) and Jc(• x S) = Jc(S x •) = Ic(•). 
In particular,
Jc(A x B) = £ 2 Jc(i,B) = £ 1Jc(A,j),
for any measurable rectangle A x B e P (S) 0 P (S).
Proof. Plainly, Jc(0) = 0. Let E 1, E2,..., be a pairwise disjoint sequence 
of subsets in P(S) 0 P(S). Then
Jc (U En ) = Z 2 Jc (i, (U En )i )
= 
2 Jc (i, U(En )i )
= EE 2 Jc ( i, ( En ) i )
n iES
= E Jc ( En ) .
n
Also, for E = A x B we have
Jc(A x B)= 
2 Jc(i, (A x B)i)
= E 2 Jc(i,B) 
itA
and
Jc(A x B) = 
1 Jc((A x B)j,j),
= E 1 Jc (A,j).
j tB
Finally, for any A e P (S) we have
Jc(A x S) = £ 2 Jc(i,S) = £ 1 Jc(S,i)
= 
1 Ic (i) = Ic (A).
itA
The proof is complete. 
□

178 
10. The Cycle Measures
Note. A comparison of Jc with the product measure Ic x Ic on P (S) ® 
P (S) shows that Jc = Ic x Ic . Specifically, if i and j are two non- 
consecutive vertices of the circuit c G C then
Jc( W x W) = 2Ic (i,j) = 0,
while
(Ic x Ic )( {i} x {j} ) = 1 Ic ( i) • 1 Ic (j ) = 1.
In general, there are i, j G S such that
2Ic (i, j) = 1Ic(i) • 1Ic(j),
since the vertex j occurring in 2 Ic (i, j ) is conditioned to be the next vertex 
to i on c, while i and j are independent points in 1Ic (i) •1 Ic (j ).
However, both measures Jc and Ic x Ic enjoy a common property: 
— . ..
.Tc (A x S ) = <7c (S x A), 
A g P (S).
This motivates the following
Definition 10.4.6. A measure ^ on the product measurable space (S x 
S, P (S) ® P (S)) is called a balanced measure if it satisfies
/a(A x S) = /a(S x A), 
(10.4.4)
for any A G P (S).
Further we give a procedure to defining balanced measures from weighted 
circuits in an analogous manner with that given in section 2.2.1.
Theorem 10. 4.7. Let S be any finite set. Then for any collection C of 
overlapping directed circuits in S and any collection {wc ,c G C} of positive 
numbers there exists a balanced measure ^ on the product measurable space 
(S x S, P (S) ® P (S)) such that
p,(A x B) = 
wc Jc(A x B), A,B G P (S), 
(10.4.5)
where .c is introduced by Theorem 10.4.5. In particular,
^(• x S) = ^(S x •) = 
WcIc (•),
where Ic(•) is the measure on P (S) introduced by Proposition 10.4.3.(i). 
Furthermore, ^({i} x {j}),i,j G S, defines a balanced function whose cir­
cuit representation is {C, wc}.

10.4 The Cycle Measures 179
Proof. For the given collection {C, wc}, define
M(E) = 
wc Jc(E), E G P (S) ® P (S), 
(10.4.6)
cEC
where Jc(•) is introduced by Theorem 10.4.5. Plainly, m is a positive measure 
on the product measurable space (S x S, P (S) 0 P (S)). In particular, for 
any E = A x B,A,B G P (S), we have
M (A x B) = ^ wc Jc (A x B),
and the measure m is balanced. Furthermore,
M(A x S)= m(S x A) = 
wcIc (A), 
A G P (S),
and
M({i} x {j}) = ^2 wc • 2Ic(i,j), 
i,j G S,
cEC
defines a balanced function whose circuit representation is {C,wc}. The 
proof is complete. 
□
Equations (10.4.5) allows us to call any balanced measure M a circuit (cycle) 
measure while {C, wc } is called a circuit (cycle) representation of M.
The converse direction uses the argument of the cycle generating equa­
tions. Namely, we have
Theorem 10. 4.8. Let S be any finite set and let M be any nonnegative 
balanced measure on (S x S, P (S) 0 P (S)) such that m(• x S) > 0. Then 
there exists a finite ordered collection C of overlapping directed circuits in 
S and a finite collection {wc, c G C} of positive numbers, depending on the 
ordering of C such that {C, wc} is a circuit representation of M, that is,
M(A x B) = 
wc Jc (A x B), A,B G P(S),
where Jc is introduced by Theorem 10.4.5.
Proof. Define w(i, j)=M({i}x{j}),i,j G S. Then w is a positive bal­
anced function on S x S. Consequently we may apply Theorem 1.3.1 to w 

180 
10. The Cycle Measures
and find a circuit representation {C, wc} for w, that is,
M({i} x {j}) = 5S wc • 2Ic (i,j), 
i,j G S,
cEC
where C is a collection of directed circuits endowed with an ordering and 
wc, c G C are positive numbers. Then
M(A x B ) = 
wc Jc (A x B), 
A,B G P (S).
Finally, {C, wc } is a circuit representation of M, and the proof is 
complete. 
□
A generalization of Theorem 10.4.7 is given by the following:
Theorem 10. 4.9. Let S be any finite set. Then for any collection C 
of overlapping directed circuits with the vertex-set S and any collection 
{wc ,c G C} of positive numbers there exists a positive measure M on the 
product measurable space {SN, (P (S))N} such that
M (A1 x A 2 x S x S x ... ) = ^S wcJc (A1 x A 2), 
A1 ,A 2 G P (S),
M(A1 x A2 x A3 x ... x Ak x S x S x ...) 
is ••• IS wi(ii)•
i 1EA1 i 2 EA 2 ikEAk
w2(i1, i2) 
w 1( i1)
w2(i2, i3) 
w 1( i2)
w 2( ik-1 ,ik) 
w 1( ik-1)
where
w 1( i)=ys wc • 1 Ic (i), 
i G S,
w2(i,j) 
wc ' 2Ic(i,j), 
i,j G S,
and Jc is introduced by Theorem 10.4.5.
Furthermore, M is a balanced measure, that is, for any k =1, 2,... and 
any A1 ,. ..,Ak G P (S) we have
M(SxA1 xA2 x ...xAk xSxSx ...)=M(A1 xA2 x ...xAk xSxSx ...).
(N denotes as always the set of all natural numbers.)
Proof. The elements of SN are all sequences (in)nEN with in G S, n G N, 
and (P (S)) N is the minimal a -algebra containing all cylinders
{i1}x{i2}x... x{ik}xS x S...
with in G S, 1 < n < k, k G N.

10.5 Measures on the Product of Two Measurable Spaces 181
Let
w 1( i) = y? Wc • I Ic (i), 
i G S,
cEC
w2(i,j) 
wc • 2Ic(i,j), 
i,j G S.
cEC
Define the measure m on the class of all cylinders by the equalities
M({i 1} x S x ...) = w 1(i 1)
M({i 1} x {i2 } x ... x {ik } x S x S x ...)
w2(ik-1, ik)
w 1( ik-1) ,
w2(i1, i2) w2(i2, i3) 
w1(i1) • 
• • \ •
w1(i1) 
w1(i2)
where i 1, i 2, ... ,ik G S, k > 1.
Then by applying Ionescu-Tulcea’s extension theorem, the measure M 
may be extended to the whole (P (S))N. Finally, for any k =1, 2,... and 
any A1 ,...,Ak G P (S), we have
M(S x A1 x A2 x ... x Ak x S x S x ...)
= E E E^E w.(i>^ww1f • 
iES i1EA1 i2EA2 ikEAk 
1
w w w2(i 1 ,i2)
' ' -------:---- :— • • •
w1(i1)
) w2(i 1, i2) 
w 1( i1)
i 1 EA1 i 2 EA 2 
ikEAk
i1EA1 i2EA2 
ikEAk iES
w2(ik, i) 
• ------:--- :—
w1(ik)
= m(A 1 x A2 x ... x Ak x S x S x ...),
and the proof is completed.
w2 (i1, i2)
------ •---:— • •
w1(i1)
w 2 (ik-1 , ik)
w 1( ik-1)
w2(i2, i3)
• • ------ :---:—
w1(i2)
w2(ik-1, ik) 
w 1( ik-1)
w 2( ik-1 ,ik) 
w 1( ik-1)
□
10.5 Measures on the Product of Two Measurable 
Spaces by Cycle Representations of Balanced 
Functions: A Fubini-Type Theorem
In this paragraph we shall show that the cycle representation formula of 
balanced functions may be involved in a Fubini-type theorem.
Theorem 10.5.1. Any positive balanced function v( •, • ) : S x S ^ R + on 
a finite set S, whose graph is (S, E ) and which admits a circuit representa­
tion {C, w ( • )} with w: C ^ R +, defines a positive measure m on the product

182
10. The Cycle Measures
measurable space (Cx E, P (C) ® P (E)) such that
M(C x E) = 
w(c) Jc(E), 
C C C,E C E, 
(10.5.1)
where Jc(i, j)=1 or 0 according to whether or not (i, j) is an edge of c. 
Furthermore,
M(C x{(i,j)})=v(i,j), 
(i,j) & E
(Here P (•) denotes as always the power-set.)
Proof. Let (S, E), EC S2, be the oriented strongly connected graph asso­
ciated with the positive balanced function v (i.e., (i, j) & E if and only if 
v(i, j) > 0,i,j & S). Then we may apply the circuit representation Theo­
rem 1.3.1 according to which we may find a finite collection C of directed 
circuits c in S, with periods p(c) > 1, and a positive (P (C)-measurable) 
function w: C ^ R + such that
v(i,j) = 
w(c) Jc(i, j), 
i,j& S. 
(10.5.2)
Consider the measurable spaces (C, P (C)), (E, P (E)) and the measure 
v: P (C) ^ R + defined as
v (C) = 52 p(c) w (c), 
C C C
cEC
Also, introduce the function Q: C x P (E) ^ R + defined as
Q(c,E) = p(c)Jc(E), 
c & C,E C E,
where Jc (E)= 
Jc (i, j), and Jc (i, j) is the passage-function occurring
(i,j)EE
in the statement of the theorem. Then Q behaves as a transition probability 
measure from (C, P (C)) to (E, P (E)), that is,
(i) 
0 < Q(c, E) < 1, c & C,E C E;
(ii) Q(c,E)=1,c& C;
(iii) 
Q (c, •) is a -additive.
Following M.M. Rao (1993) we further define the set function M for each 
measurable rectangle C x E of P (C) ® P (E) as follows:
M (C x E ) = E v ({c}) Q (c,E)
= £ w (c) Jc (E), 
cEC
and prove that M is a-additive. Specifically, ifCi x Ei,i =1, 2,...,n; n > 1, 
is a sequence of pairwise disjoint measurable subsets of C x E whose union 

10.5 Measures on the Product of Two Measurable Spaces 183
is C x E, then
1 C'xE = 
1 Ci 1 Ei .
i=1
By integrating the previous equation with respect to Q((•, de) on (E, P (E)), 
we get
1 c ■ Q(-,E) = ^2 1 Ci ■ Q(-,E)). 
i=1
Further, we integrate with respect to v on (C, P (C)) and find
n
p(c) w(c) Q(c, E)= 
p(c) w(c) Q(c, Ei),
c<eC 
i =1 ceCi
or, else
n
M (C x E) = ^2 M (Ci x Ei), 
n = 1, 2,....
i=1
Finally, we apply, Caratheodory’s theorem and consider the extension, sym­
bolized also by M. Furthermore,
M (C x E) = 
w (c) Jc (E)
= 
p(c) w(c)
ceC
= v(C)
for any C e P (C), 
and
M(C x {(i, j)}) = 
w(c)Jc(i, j)
ceC 
= v(i, j),
and the proof is complete. 
□
Immediate consequences of the previous theorem are as follows:
Corollary 10.5.2. For any finite set S and any positive balanced function 
v (, •): S x S ^ R + whose graph is (S, E) and which admits a circuit repre­
sentation {C,w (•)} there exist an initial positive measure v on (C, P (C)), 
a transition probability measure Q: C x P (E) ^ R +((i)0 < Q(c, •) < 1, c e 
C; (ii) Q (c, E) = 1 ,c e C; (iii) Q (c, ■) is a-additive for any c e C) and a 
measure m on the product measurable space (C x E, P (C) ® P (E)) such 

184 
10. The Cycle Measures
that
h(C x E) = £ Q (c,E) V({c}), 
C & P (C), 
E & P (E),
h (C x E ) = v (C), 
C & P (C),
and
52 f(c,£)h{(c, £)} = Yj v({c}) 52 f(c,£)Q(c, {e}),
for any f: C x E^ R, where (c, £), £ and c range C x E, E and C, respec­
tively.
Also, we have
Corollary 10.5.3. For any finite set S and any positive balanced function 
v(•, •): S x S ^ R + with the graph (S, E) and the circuit representation 
{C,w(•)}, there exist an initial positive measure X on (E, P (E)), a tran­
sition probability measure P: P (C) x E^ R + ((i) 0 < P(• .£) < 1 ,£ & E; 
(ii) P(C,£) = 1 ,£ & E; (iii) P(• ,£) is a-additive for any £ & E) and a mea­
sure h on the product measurable space (C x E, P (C) ® P (E)) such that
h(C x E)= £ P(C, (i,j)) X({(i,j)}), C & P (C), E & P (E), 
( i,j) £E
h(Cx E)=X(E), 
E& P(E),
and
( ) g(c, £) h{(c, £)} = 
X({£}) 
g(c, £) P ({c},£), 
(10.5.3)
for any g: C x E^ R, where (c, £), £ and c range C x E, E and C, respec­
tively.
Proof. We consider the circuit representation (10.5.2) for the positive bal­
anced function v and define
X (E ) = 
w (c) Jc (E), 
E C E,
c£_C
where
Jc ( E )= 
Jc ( i,j ) .
( i,j) £E
Also, define P: P (C) x E ^ R + by the relation
r •n 
cE^cwC w (c) Jc(i,j) 
C r~ ex’ 
(‘ X-
P (C, (i,j )) = 
----- f f .X , 
C C C, 
(i,j ) & E.
cLCCw( w ( c) Jc ( i,j )

10.5 Measures on the Product of Two Measurable Spaces 185
Then, P(•, •) is a transition probability measure from (C, P (C)) to 
(E,P(E)).
If ^ is defined for each measurable rectangle C x E by
M(C x E)= £ P(C, (i,j)) A{(i,j)}
= £ £ w(c) Jc(i,j)
(i,j)EE cEC
= £ w (c) Jc (E),
cEC
then ^ extends to a measure on P (C) ® P (E).
Finally, p,(C x E) = A(E) and (10.5.3) holds as well. The proof is 
completed. 
□

11
Wide-Ranging Interpretations 
of the Cycle Representations 
of Markov Processes
In the present chapter we shall further study wide-ranging interpretations 
of the cycle representations of Markov processes: the homologic, the al­
gebraic, the Banach space, the measure-theoretic and the stochastic one, 
which altogether express genuine laws of real phenomena. The versatility 
of these interpretations as orthogonality equations, as linear expressions 
on cycles, as Fourier series, as semigroup equations, as disintegrations of 
measures, etc., is consequently motivated by the existence of algebraic- 
topological principles in the fundamentals of the cycle representations of 
Markov processes.
11.1 The Homologic Interpretation
of the Cycle Processes
Let S = {n 1, n 2,... ,nk },k > 1, be a set of symbols, which denote the states 
of a homogeneous irreducible Markov chain £ = {£n, n = 0, 1, 2,...}. The 
stochastic transition matrix of £ is denoted by P = (pij, i,j G S), that is,
Pij = Prob (£n+1 = jl£n = i) = Prob (£n = i, £n +1 = j)/Prob (£n = i), i,j G S,
for any n =0, 1,..., whenever Prob (£n = i) =0,i G S. Also, the invariant 
probability distribution of £ will be symbolized by n = (ni,i G S). Then 
n'P = n.

188 
11. Wide-Ranging Interpretations of the Cycle Representations
Figure 11.1.1.
Let G =(S, E) be the oriented graph associated to P and consider arbi­
trary but fixed orderings on S and E, that is,
S = {n 1 ,n 2,.. ., nk }, 
k > 1,
E= {(i1,j1), (i2,j2),..., (is,js)}
= {b(i1,j1) , b(i2,j2) ,...,b(is,js)} = {b1,b2,...,bs}.
As we have already seen, the irreducibility of £ is translated in terms of 
graph-elements as follows: any point i 1 G S belongs to at least one or­
dered sequence (i1,i2,...,in,i1),n=1, 2,..., called a directed circuit (or 
cycle) in S, such that the pairs (i1,i2),...,(in,i1) are edges of G. That 
is, the set E of directed edges determines completely a finite collection 
C = {c1 ,c2 ,...,cm} of overlapping directed circuits such that each edge 
belongs to at least one circuit of C as in Figure 11.1.1.
Throughout this section, we shall consider only directed circuits 
(i1,...,iu,i1) with distinct points i1,...,iu.
As we have studied in Chapter 4, any irreducible finite state Markov chain 
like £ above admits a description of the finite probability distributions in 
terms of the directed circuits or cycles of the associated graph as follows:
Prob{£n = i, £n+1 = j} = 
wcJc(i,j), 
i,jG S, n =0, 1,...,
£ 
(11.1.1)
where wc,c G C, are positive real numbers, called usually the cycle-weights, 
and
Jc(i,j) = 1, if (i, j) G E and (i, j)isanedgeofc, 
=0, otherwise.
Now we shall show how to find a homologic analogue of the process 
£ = (£n)n in a suitable vector space associated to the graph G along with 

11.1 The Homologic Interpretation of the Cycle Processes 189
a homologic interpretation of the cycle formula (11.1.1). First, define
C0 =|^2 Xini, Xi e R,ni e S j
the vector space spanned by the points n1 , n2 ,...,nk called 0-simplexes in 
R2 . The elements of C0 are called 0-chains. Here the points of S and the 
edges of G are viewed homeomorphically in the plane R2.
Further define
c 1 = < 52yibi,yi g R,big e 
i
the vector space spanned by the edges of E, called 1-simplexes in R2. 
The elements of C1 are called one-chains. Then as in Chapter 4 any cir­
cuit c = (i 1, i2,..., ik, i 1) G C is uniquely associated with a vector c G C 1 
defined as
c = £ Jc(i,j)b(i,j). 
(11.1.2)
Consequently, if we associate the process £ = (£n) with the (unique) vector
£ = Z)(niPij)b(i,j) G C1, 
(11.1.3)
then the cycle decomposition (11.1.1) has a vector analogue in C1 as 
follows:
£ = E wc c. 
(11.1.4)
Now, associate each directed circuit c =(i1,i2,...,is,i1) G C with a 
surface-element ac, which is the polygon with interior and with vertices 
i 1, i2,... ,is (see Figure 11.1.2). The polygons ac, c G C, are oriented ac­
cording to the orientation of the circuit c as in Figure 11.1.2. The ac, c G C,
Figure 11.1.2.

190 
11. Wide-Ranging Interpretations of the Cycle Representations
Figure 11.1.3.
are the topological images of regular s-gons, s>1, or of certain circles, 
and are called the closed 2-cells. With the definition of Rotman (1979) 
(see pp. 17-19) each ac is a sum of s 2-simplexes whose boundary remains 
always c.
Furthermore, each 2-simplex is a continuous image in R2 of the convex set 
spanned by (0,0),(1,0),(0,1).
Denote by S the collection of all 2-cells determined by the circuits of C in 
the graph G.
Define
C2 = {£- zieR,aies|,
the vector space spanned by the 2-cells associated with G. Then C2 is 
identical with the vector space spanned by all triangles with interior (the 
2-simplexes) occurring in G. The collection
K = G U {the directed 2-cells}
= S U E U {2-cells}
will be called the 2-complex associated with the Markov chain £ (or, with 
the stochastic matrix P). In general, we may define an n-complex associated 
to the process £.
Further we shall now define boundary operators (resolutions) d2,d 1 ,d0 
as in Rotman (1979) (p. 19):
d 2 
d 1 
d 0
• • •  > C2 ----> C1  > C0  > 0.
To this direction we consider the standard simplexes A0, A1, A 2 with 
the corresponding vertices {(0, 0)} for A0, {(0, 0), (0, 1)} for A1, and 
{(0, 0), (0, 1), (1, 0)} for A2, that is, A0 = {0}, A1 =[0, 1], A2 = the triangle 
(with interior) with vertices at (0, 0), (0, 1), (1, 0). For each Ak,k=0, 1, 2, 
consider an orientation. Since a k-simplex, k =0, 1, 2 in the 2-complex 
K associated to £ is a continuous function a: Ak ^ K, k = 0, 1, 2, the 

11.1 The Homologic Interpretation of the Cycle Processes 191
Ao = {0} is homeomorphically transposed by the zero-simplexes into the 
points nr, r = 1,..., k, of K, the A1 = [0, 1] is homeomorphically trans­
posed by the one-simplexes into the edges of K: b1 ,...,bs, and the A2 is 
homeomorphically transposed by the 2-simplexes into the 2-cells of K.
Define d0 = 0, d 1 ((nh, nk)) = nk — nh, where (nh, nk) is the oriented edge 
of E with the original end at nh and the terminal end at nk. The op­
erator d 1 has an abstract expression in terms of the embedding func­
tions e0: A0 ^ A1, e 1: A0 ^ A1 defined as: eo(O) = (0, 1), e 1(0) = (1, 0). 
Then nk = a{(0, 1)} = aeo (Ao), nh = a{(1, 0)} = ae 1(Ao), where a: A1 ^ 
K is the 1-simplex (nh,nk). So, d 1(nh,nk) = d 1 a = ae0A0 — ae 1 Ao = 
E 1=o( — 1)i a ei.
Extend d 1 to C 1 by
d 1(E y^ = E yid 1(bi),
where bi = ((ni (h), ni (k)),i=1,...,s.
Define d2: C2 ^ C 1 for a 2-cell ac = the sum of k two-simplexes aj = 
triangles with bases the k edges of the circuit c =(i1,i2,...,ik,i1), as 
follows:
d2 ac = Ed2 aj 
EE—1)i aj ei 
E b(ij,ij+1)=c.
j=1 
j=1 i=o 
j=1
ik + 1 = i 1
Thus d2, associates each 2-cell ac to its boundary directed circuit c.
Now consider the dual spaces C*,C*,C*,... and the cohomology bound­
ary operators
* d * 
* d * a s
C* 
2 ** 1 1 ** , o n*
2 <------ C1 ------ C o ------ 0 ,
where Kerd* D Imd*. Then we may define the factor group H2 = 
Kerd*-
Kerd2 imd*, which is the 2nd cohomology group of K. On the other hand, 
except for an isomorphism we have C 1 = Kerd 1 ® Imd1* (see Lefschetz 
(1975)). Accordingly we may write
e = E wc c ® o.
Now, we may conclude:
Theorem 11.1.1. (The homologic interpretation). Any cycle process e 
has a vector analogue £ in Kerd 1 given by
e 
E wc c.

192
11. Wide-Ranging Interpretations of the Cycle Representations
In general, any finite Markov chain n = (nn) may be decomposed into a 
sum of a cycle process Z = (Zn) and a noncycle process Z1 = (Zn )n, that 
is, n may be associated with a vector n C C1 written as 
n = Z ® Z \
where Z C Kerd 1 ,Z1 C Imdl (except for an isomorphism).
□
11.2 An Algebraic Interpretation
Assume the same hypotheses and notations of the previous paragraph for 
the Markov chain f. In Chapter 8 we have shown that the graph G = (S, E) 
of f provides collections of cycloids whose arc-sets coincide with E. Also, 
we may always find by the maximal-tree-method a base of elementary 
cycloids for the vector space C 1 = Ker d 1 of all one-cycles. For exam­
ple, in Figure 11.2.1 we have a strongly connected 1-graph G =(S, E),
Figure 11.2.1.

11.2 An Algebraic Interpretation 193
where S = {1, 2,...,12}, card E = 19 and the Betti number is B =8. 
Then a base for the corresponding space C1 may be given by using the 
cycloids:
Yi = (2,3,4,5,6,1,2) ,Y2 = (2,7,6,1,2) ,Y3 = (4,3,4) ,Y4 = {(6,2),(1,2),(6,1)}, 
75 = {(3,9),(9,10),(10,2),(1,2),(6,1),(5,6),(4,5),(3,4)}, 76 = {(11,12),(12,2), 
(1,2),(6,1),(5,6),(4,5),(3,4),(3,11) },77 = {(4,7),(7,6),(5,6),(4,5)}, 
78 = {(7,8),(8,4),(4,5),(5,6),(7,6)},
where the corresponding Betti edges are u1 =(2, 3), u2 =(2, 7), u3 = 
(4, 3),u4 =(6,2),u5 =(3,9),u6 = (11, 12),u7 =(4,7),u8 = (7,8), and the 
first three cycloids are directed circuits.
In general, we may choose by the maximal-tree-method two bases of elemen­
tary Betti cycloids {c1,... ,cB} and {71,... ,7B}, with the corresponding 
Betti edges (i 1 ,j 1),..., (iB ,jB) and (u 1, v 1),..., (uB ,vB).
Then any cycloid c may be written as
c J Jc,c ( ik, jk ) c.k 
J Jc,c (ul, vl) 71,
k=1 
l =1
where c and c_ are the two possible directed circuits associated with c and
Jc,c(i, j) = 1, 
if (i, j) is an edge of c and c,
= -1, if(i,j)is an edge of c7 and c-, 
=0, otherwise.
By replacing c with 7l, l = 1,..., B, we have
71 
JY l,Yl (ik,jk) 7k,
k=1
or else,
JY1 ,Y 1 ( i,j ) ^2 JY1 ,Y 1 ( ik ,jk ) JYk,Ck ( i,j ), i,j G S. 
k=1
(11.2.1)
Equations (11.2.1) are generalizations of the cycle formula (11.1.1) in terms 
of the cycloids and have the following algebraic interpretation: they give 
the change of a base of cycloids into another one.

194 
11. Wide-Ranging Interpretations of the Cycle Representations
11.3 The Banach Space Approach
In Chapter 9 we have shown that the cycle formula may be interpreted 
as a Fourier decomposition ^2 wYY for any denumerable Markov process 
which admits an invariant measure, where {y} is an orthonormal family of 
directed cycles. Also, we have proved that we may define a Markov process 
from suitable Banach spaces lp on cycles.
Here we shall now prove the converse of Theorem 9.4.1. Namely, we have
Theorem 11.3.1. Let P = (pij,i,j G N) be the transition matrix of an 
irreducible and positive-recurrent Markov chain £ = (£n)n, whose invariant 
probability distribution is n = (ni, i G N).
Then the following statements hold:
(i) The sequence (nipij, (i, j) G N2) satisfies the cycle formula for p = 1 
and with respect to a countable collection (C, wc) of directed cycles c 
and positive weights wc, where C and {wc} are given a probabilistic 
interpretation in terms of the sample paths of £;
(ii) Given collection (C,wc) as in (i) above, if (nipij, (i,j) G N2) G 
l2(N2) then, except for certain isomorphisms, the sequence 
{ kn=1 wckck}n converges in H(E) to
z 00 
wck Jck (i, j) b(i,j), 
(i,j) k=1
as n ^ ^. where E is the vector space generated by the Arc-set C endowed 
with certain ordering.
Proof. (i) Let N, N2 be endowed with certain orderings. Consider that P = 
(Pij,i,j G N) and n = (ni,i G N) satisfy the hypotheses of the Theorem. 
Then, we may apply the cycle representation Theorem 3.3.1 according to 
which we may choose a collection C of directed cycles which occur along 
almost all the sample paths of the Markov chain £ = (£n)n on P and a 
unique collection {wc, c G C} of positive numbers such that
niPij 
wJc(i,j), i,j G N,
cEC
where any cycle-coordinate wc ,c G C, enjoys the following probabilistic in­
terpretation: wc = lim (wc,n(^)/n) a.s., and wcn (^) denotes the num­
ber of appearances until time n of cycle c along almost all tra jectories 
(£ 1 (^),£2(^),...). Furthermore, the cycle values wc,c G C, are indepen­
dent of the ordering of C. Then the sequence (ni,pij, (i, j) G N2) satisfies 
the cycle formula for p = 1 and with respect to the above representative 
collection (C, wc) for the stochastic matrix P = (pij ,i,j G N).

11.4 The Measure Theoretic Interpretation 195
(ii) Let C = {c 1, c2,...} and {wck, k G N} be the cycle representatives 
for the original stochastic matrix P = (pij, i,j G N) as given at (i). Denote 
by N, E and C the vector spaces generated by N = Vertex-set C, Arc­
set C and C = {c 1, c2,...} as introduced in Chapter 9. Consider further 
the corresponding Hilbert spaces H(N),H(E) and H(C). Then, according 
to Theorem 9.2.2, the sequence of isomorphs of {^2m=1 wck ck}m in H(E) 
converges to (i,j) Q2k=1 wct Jk (i, j)) b(i,j), as m ■ ^. The proof is com­
plete. 
□
11.4 The Measure Theoretic Interpretation
As we have shown in Chapter 4, given a finite set S and a transition prob­
ability matrix {pij ,i,j G S}, which admits an invariant probability distri­
bution n = (ni,i G S), there exists a finite class C of directed circuits c 
in S and a positive function w : C ■ R+ called a weight function such 
that
nipij = 
w(c) Jc(i,j), 
i,j G S, 
(11.4.1)
ctC
where Jc(., .) is the passage-function of c defined as
Jc(i,j) = 1, if (i,j) is an edge of c, 
=0, otherwise.
The probabilistic algorithm of Theorem 4.4.1 assures the uniqueness of w 
and its independence of the ordering of C.
It might be interesting to investigate the measure-theoretic meaning of 
the weight function w (•) occurring in the circuit representation (11.4.1). To 
this end, consider the measurable space (S, P (S)), where P (S) denotes 
as usual the power set of S, and the transition probability function from S 
to P (S) defined as Q(i, A) 
jEA pij, for any i G S and A C S. We shall
show that
niQ(i,A) = y w(c) Ic(i,A) dc, i G S A C S,
where C is the ordered set of all directed circuits with distinct points 
in S associated to P, dc denotes the counting measure on P (C) and 
Ic generalizes the passage function Jc occurring in (11.4.1), while the 
weight function w: C ■ R + is the Radon-Nicodym derivative dV of a suit­
able measure v with respect to dc. Specifically, for any directed circuit 
c =(i1,i2,...,is,i1)=(c(n),c(n +1),...,c(n + s - 1), c(n)), n G Z, of pe­
riod s> 1, we consider the following passage-functions of c.
(i) 
Jc(i,j) = 1, if i = c(n), j = c(n +1) for some n G Z,
=0, 
otherwise,

196
11. Wide-Ranging Interpretations of the Cycle Representations
(ii) 
Ic(i, A) = 1, if i = c(n) and c(n +1) G A for some n G Z,
=0, 
otherwise.
Then we have
Proposition 11.4.1. The passage-function Ic(•, •) satisfies the following 
equations:
(i) Ic(i,A) = £ Jc(i,j),
(ii) Ic(i,S) = J()),
for all i G S,A C S, where Jc(i) 
^2 Jc(i,j).
Proof. If Ic (i, A)=0fori e S and A e P (S), then Jc(i, j) = 0 for all 
j G A. Therefore both members of (i) are equal to zero. Now, consider that 
Ic(i, A) = 0. Then i = c(n) for some n G Z and c(n +1) G A. Thus both 
expressions 52j- A Jc(i, j) and Ic(i, A) equal 1.
Finally, (ii) is a special case of (i) for A = S. The proof is complete. □
Further, consider the measure space (C, A,v), where A = A(G) is the 
smallest a -algebra generated by G = {C (i, A) ,i G S,A G P (S)} with 
C(i, A)={c G C: Ic(i, A)=1},andv is defined as follows:
v(C(i, {j}) = niPij, 
ij G S, 
(11.4.2)
v (UnBn ) = 
n nin Q (in, An ), if Bn = C (in, An ) = 0 ,in G S, An C S,
n=1,2,...,
=0, 
otherwise,
where the sets in the union are pairwise disjoint. Note that {c}GA(G).
Now, consider A, A' G P (S) such that A A A' = 0. Then, C(i, A) A 
C(i, A) = 0, i G S. Accordingly, for any sequence C(i, An), n = 1, 2,..., 
where An,n=1, 2,..., are pairwise disjoint sets we may use the additivity 
of Q(i, •) and write
v(UnC(i, An)) = v(C(i,UnAn))
= niQ (i, UnAn )
E
niQ(i, An) 
n
= £ v ( C ( i,An )) . 
n
Now we may prove the following:
Theorem 11.4.2. There are two finite measure spaces (C, P (C), dc) and 
(C, P (C), v) such that
niQ(i, A) = y w(c) Ic(i,A)dc 
w(c) Ic(i, A) i G S, A C S, (11.4.3) 

11.5 The Cycle Representation Formula as a Disintegration of Measures 197
and
ni J w(c) Jc(i)dc 
w(c) Jc(i), 
i G S, 
(11.4.4)
where w(•) denotes the Radon-Nicodym derivative dV. 
□
Proof. Since v, defined by (11.4.2), is absolutely continuous with respect 
to the counting measure dc, we may apply the Radon-Nikodym theorem, 
and find a (P (C)-measurable) function w: C ^ R + such that
v(B) = y w(c) dc 
w(c), 
B C C. (11.4.5)
To obtain (11.4.3), we may choose B = C(i, A)fori G S and A G P (S) in 
(11.4.5). Then we have
v (C (i,A)) = niQ (i,A ) = J 
w (c) dc
= 
w(c) Ic(i, A)dc
C
= £ w(c) Ic(i,A).
ctC
Finally, taking A = S in (11.4.3) and applying Proposition 11.4.1, we get 
Ic(i, S) = Jc(i). Then (11.4.3) becomes
ni J w (c) Jc (i) dc 
w (c) Jc (i), 
i G S.
The proof is complete. 
□
11.5 The Cycle Representation Formula as a 
Disintegration of Measures
Equation (10.5.1) becomes a disintegration formula of the measure jd(•) = 
^(Cx •) with respect to v({c}) = p(c) w(c), when C is the entire set C of 
the directed cycles or circuits which decompose a positive balance function 
v, that is,
M(E)= E Q(c,E) v({c}), 
E C E,
ctC
where Q is a transition function defined as in the proof of Theorem 10.5.1.

198 
11. Wide-Ranging Interpretations of the Cycle Representations
Consequently, by using the approach of M.M. Rao (1993) the cycle repre­
sentation formula
v (i,j ) = 52 w (c) Jc (i,j), 
(i,j)G E,
expresses a “disintegration” of the “edge-measure” v(i, j) relative to the 
“cycle-measure” w(c).
The present paragraph is devoted to the study of the cycle representa­
tions of balance functions in the context of the disintegration of measures 
as given by M.M. Rao (1993).
We first prove
Theorem 11.5.1. Let S be a finite set and let v(•, •): S x S ^ R + be any 
positive balanced function with the oriented graph (S, E) and a circuit rep­
resentation (C, w(•)) with w(c) > 0, c G C.
Consider the probability measure h on the product measurable space (C x 
E, P (C) E P (E)) defined for each measurable rectangle C x E C C x E 
as:
h (C x E) = _ 1 
£ w (c) Jc (E),
JC p (c) w (c) c^c
(11.5.1)
where p(c) denotes the period of the cycle c, and Jc(i, j)=1 or 0 according 
to whether or not (i, j) is an edge of c. Then
(i) there exists a transition probability function Q from (C, P (C)) to 
(Cx E, P (C) E P (E)) such that
h(C x E) = 
Q(c, C x E) v({c}). 
(11.5.2)
for any C x E C C x E, where v = h °pr-1 is the image probability 
of h with respect to the projection prc: C x E^ C.
(ii) there exists a transition probability function P from (E, P (E)) to 
(C x E, P (C) E P (E)) such that
h(C x E)= ^2 P((i,j),C x E) X({(i,j)}), 
(11.5.3)
for any C x E C C x E, where X = h ◦ pr-1 is the image probability 
of h with respect to the projection pr£: C x E^ E.
Proof. (i) Let (C' x E) be any measurable rectangle with C' G P (C), E G 
P(E), and let C G P(C). Consider
h[(C' x E) n pr- 1(C)] = h[(C' n C) x E].
Then for each C' x E C C x E, the measure h[(C' x E) n pr- 1( •)] is abso­
lutely continuous with respect to v on P (C). Consequently, there exists 

11.5 The Cycle Representation Formula as a Disintegration of Measures 199
the Radon-Nikodym derivative Q(-,C' x E) of p[(C' x E) A pr- 1(•)] rela­
tive to v, that is,
p[(C' x E) A pr- 1(C)] = Jp Q(c, C' x E) v({c}),
for any C / P (C).
Furthermore, the a -additivity of the measure p (• A pr-1 (C)) implies that 
Q(c, •) is cr-additive as well, for any c / C.
From the expression of p given by (11.5.1) we may further find the con­
crete form of both v and Q as follows:
v(C) = (p ◦ prc-1)(C)
= p(C x E)
= ECp(c)w(c) 
p(c) w(c),
for any C e P (C).
Also,
p[(C' A C) x E]= _ 1 
£ w(c) J-(E), 
(11.5.4)
^pp(c) w(c) cC^c
and
E Q (c, C' x E) v ({c}) = _ 1 
£ p(c)w (c) Q (c, C x E).
EC 
p (c) w(c) EC
^C^
(11.5.5)
Then, from (11.5.4) and (11.5.5), Q(c, C' x E) is identical to
Q (c,Cx E ) = ( Ik) J-'(E), if c e C'
[ 0, 
if c /C
In particular, if C' = C, then we obtain (11.5.2), where
Q(c, C x E) = 
-L J— (i,j),
(i,j)CE p c
for any c / C and E C E.
(ii) Consider
p[(C x E) A pr- 1(E)] = p[C x (E A E')],
for any C / P (C) and E, E' / P (E).
Then for each C x E' C C x E, the measure p[(C x E') A pr- 1( •)] is ab­
solutely continuous relative to A = p ◦ pr-1 on P (E). Consequently, there 

200 
11. Wide-Ranging Interpretations of the Cycle Representations
exists the Radon-Nikodym derivative P(-,C x E') of the former with re­
spect to A, that is,
M[(C x E') npr- 1(E)]= 
P((i,j),C x E') A{(i,j)}.
Also, the ff-additivity of P((i, j), •) follows from that of m[• n pr—1 (E)] for 
any (i, j) e E. As in the proof of (i), we now describe both A and P(•, •) 
from the concrete expression (11.5.1) of m. Specifically, we have
P((i,j),C x E) A{(i,j)} = 
(i,j)EE
A(E) = (M ◦ Pr— 1)(E)
= m (C x E)
= _ 1 
£ w (c) Jc (E),
E*p (c) w(c) cEC
for any E C E.
On the other hand,
M(C x (E n E'))= _ 1 
V w(c) Jc(E n E'), 
(11.5.6)
cEC p(c)w(c) cEC
and
E P((i,j),C x E') A{(i,j)} 
( i,j) EE
= V . E P<<i,j),C x E')fEw<c) Jc(i,j>). 
(11.5.7)
cECp(c)w(c) (i,j)EE 
cEC
The comparison of equations (11.5.6) and (11.5.7) inspires the following 
expression for P((i, j), •) on the measurable rectangles C x E':
w(c)Jc(i, j) • 1 E' (i,j)
P((i,j),C x E')= cEC _ 
. ------- , 
(i,j) e E. 
(11.5.8)
w(c)Jc (i, j) 
cEC
Indeed, by replacing (11.5.8) in (11.5.7), we get
1
E p(c) w(c) 
cEC
^2 JSw(c) Jc(i,j)1E(i,j) 
(i,j)EE cEC
= ————— 
w (c) Jc (E n E1)
cLCP(c)w (c) cEC U c }
= M(C x (E n E')).

11.5 The Cycle Representation Formula as a Disintegration of Measures 201
For the particular choice E' = E, we obtain (11.5.3) with
P((i,j),C x E) = YceCC w (c) Jc (i,j ) 
EceCw(c) Jc (i,j ) ,
for any (i, j) G E and C C C. 
The proof is complete.
□
Corollary 11.5.2. Let S be a finite set and let v(•, •): S x S ^ R + be a 
positive balanced function which has the graph (S, E) and satisfies the cycle 
decomposition
v(i,j) = £ w(c)Jc(i,j), 
(i,j) G S X S, (11.5.9)
where C denotes as usual a col lection of directed circuits c with periods 
P(c) E 1; w(c) > 0 and Jc(i, j) = 1 /p(c), or 0 according to whether or not 
(i,j) is an edge of c.
Let X be the measure on the edges defined as X{(i, j)} = v(i, j), (i, j) G E, 
and let V be the measure on the circuits defined as v({c}) = w(c),c G C. 
Then
(i) there exists a transition probability function Q from (C, P (C)) to 
(E, P (E)), which disintegrates X relative to v, that is,
X( E ) = I2 Q( c,E) V( {c}), 
E G P (E). 
(11.5.10)
ceC
/ ■ ■ \ il 
-i 
i 
■ i ■ 
111! f 
! ■ 
T~i f 
//Z> z775 / 
!
(ii) there exists a transition probability function P from (E, P (E) to 
(C, P (C)), which disintegrates v relative to X, that is,
V(C)= 
P((i,j),C) X{(i,j)}, 
C G P(C). 
(11.5.11)
(i,j)e E
Proof. (i) By choosing C = Cin equation (11.5.2), we get
p(C x E) = 
Q(c, C x E) v({c}),
for any E G P (E), where
_ p(c) w(c) 
v({c}) 
i \ ( \i
p(c)w(c)
ceC
and {C, w(c)} is the circuit representation of v occurring in Theorem 11.5.1 
with the passage-function Jc (i, j).

202
11. Wide-Ranging Interpretations of the Cycle Representations
z-i • 1 J 1 
X /T7. ( f7)\ 
T">—I— 
1 ~ 
f Zl\ 
7~*-l- If* 1
Consider the measures A: P (E) ^ R + and v: P (C) ^ R + defined as
.. . . .
A(E) = ^ W(c) Jc (E), E & P (E),
c C
V(C) = £ W(c), C & P (C), 
ceC
1 
— / X 
/ X /X 
1 T / • • X 
Z-1//XXT/--X 
_ rz?
where w(c) = p(c) w(c) and Jc(i,j) = (1 /p(c)) Jc(i, j),c & C.
Define the transition probability function Q from (C, P (C)) to 
(E,P(E)),by 
— . — . _ . .
Q(c,E) = Jc(E), 
c & C E & P (E),
where Jc (E)= 
(i,j)eE Jc (i, j ),c & C.
Then
A(E)^ Q(c,E) p(W), E ^ E. 
ceC
(ii) By applying an analogous reasoning to equation (11.5.3) for E = E, 
we get
M(C x E)= 
P((i,j),C x E) A{(i,j)}, C C C.
Consider now the measures A and z/ associated with the circuit representa­
tion {C, W(c)} as in (i).
Define the transition probability function P/ from (E, P (E)) to (C, P (C)), 
by 
-
P>(( i,j) ,C ) = ^'2(£C W (c) Jc(i,j), (i,j) & E, 
C C C.
,, 
, 
, 
,
ceC W/(c)Jc(i, j)
Then
v(c) = Er 
^(ii,j),c) A{(i,j)}, c C C
(i,j)e E
The proof is complete.
□
Remark. We have proved that the circuit representation formula (11.5.9) is 
equivalent with the disintegration formula of the measure A{(i, j)} = v(i, j) 
on the edges with respect to the measure /({c}) = W(c) on the circuits, 
that is,
v(i,j) = eCQ/(c,{(i,j)}) W/(c),
i,j& S.
(11.5.12)

11.5 The Cycle Representation Formula as a Disintegration of Measures 203
On the other hand, by choosing C = {c},c e C, in equation (11.5.11), we 
obtain
w(c) = 
e Pi,j), a) v(i,j), c e C (11.5.13)
The equivalence between the cycle representation formula (11.5.9) and the 
disintegration of measures (11.5.12) allows us a parallel with Kirchhoff’s 
laws on electrical networks where the overlapping circuits are here given 
by the circuits c e C and the electrical flow {w( c)} is obeying Kirchhoff’s 
laws. Specifically, equation
V(i)(- gv(i,j)) = 
Q(c,i) W(c), 
i e S,
with Q(c, i)= j Q(c, {(i, j)}), expresses that the current v(i) at node 
i e S equals the sum of the currents w(c) = [(1 /p(c)) vj(c)] of the circuits 
c passing through i.
On the other hand, the disintegration of measures (11.5.13) may enjoy the 
following dual interpretation: the series connected Voltage-sources w(i, j)= 
P ((i, j), {c}) V(i, j), where (i, j) ranges the edge-set of the circuit c, are 
equivalent with the one voltage sourse vj(c) of the circuit c.

II
Applications of 
the Cycle Representations

1
Stochastic Properties in
Terms of Circuits
In the present chapter we shall be concerned with circuit Markov chains 
and we shall investigate their recurrent behavior, entropy production and 
reversibility property. The principal results will give criterions in terms of 
the representative circuits and weights.
1.1 Recurrence Criterion in Terms of the Circuits
In Section 2.2 of Part I certain coutable state Markov chains are defined 
using classes {C, wc} where C is a countable set of overlapping directed cir­
cuits with distinct points (except for the terminals) satisfying some topolog­
ical conditions, and {wc, c G C} is any collection of strictly positive numbers 
attached to C. We recall now the definition of these processes.
Let S be the set of points of all the circuits of C. Preliminary ingredients 
will be the passage functions Jc (•, •) assigned to the circuits c of C. Here 
we shall consider the backward-forward passage functions Jc introduced by 
relation (2.2.1) of Part I, where the domain of each Jc is the set S x S.
Assume C satisfies the conditions (c1), (c2), and (c3) quoted in Section 
2.2 (Part I). In particular, C may contain infinitely many circuits with 
periods greater than 2. Introduce
w(i, j)= 
wcJc(i, j), 
i,jG S. 
(1.1.1)
Then we may use the function w (•, •) to define an S-state Markov chain as 
follows. Since 52j^S w(i,j) is finite for any i G S (see condition (ci)), we

208 
1. Stochastic Properties in Terms of Circuits
may consider
w(i) = 
wcJc(i),
i e S,
where Jc(i) = £jeS J(i,j)• Then
w(i,j)
pij — 
, - \ , i,j e S,
w(i)
(1.1.2)
define the stochastic matrix of a Markov chain £ — (£n)n>0 called the cir­
cuit chain associated with {C,wc}. The chain £ is an S-state irreducible 
reversible Markov chain.
One may obtain a recurrent or transient behavior for the circuit chain £ 
according to the constraints imposed to either the circuit weights wc alone, 
or to both collections Cand {wc}. For instance, in Theorem 2.2.2 of Part I 
the Nash-Williams-type criterion for the chain £ to be recurrent is a global 
condition on the circuit weights wc .
In this section a topological argument on C is developed to give a suf­
ficient condition, of Ahlfors-type, for a reversible countable state circuit 
chain to be recurrent (S. Kalpazidou (1988b, 1989b, 1991e). To this end, 
let us add to conditions (c1), (c2), and (c3) on C (mentioned in Section 2.2 
of Part I) the following one on the circuit weights wc ,c e C:
(c4) there exists a strictly positive number b such that wc < b for all c e C.
Consider the shortest-length distance d on S, that is,
( 0, 
if k — u;
d(k, u)— 
the shortest length of the paths
[ along the edges of C connecting k to u, if k — u;
where the passages through the edges are backward-forward passages (that 
is, a circuit c passes through (k,u) if and only if the backward-forward 
passage function Jc has a nonzero value at either (k, u) or (u, k)).
Fix an arbitrary point O in S called the origin. Let Sm ,m —0, 1, 2,... 
be the “sphere” of radius m about the origin, that is,
Sm — {u e S : d(O,u) — m}.
Then S0 — {O} and {Sm,m—0, 1, 2,...} is a partition of S. Consider now 
the “balls” Bn,n—0, 1, 2,..., of radius m about the origin, that is,
n
Bn — 
Sm .
m=0

1.1 Recurrence Criterion in Terms of the Circuits 209
Define the function 7(n), n = 0, 1, 2,..., as follows:
Y (n) = card Bn. 
(1.1.3)
We call y the growth function of S associated with Cand O (S. Kalpazidou 
(1990c)). In particular, when Cis the representative class of a circuit chain 
f,Y will be called the growth function of f associated with O.
We now prove
Theorem 1.1.1. Any reversible circuit chain f whose generative class 
{C,wc} satisfies conditions (c 1 )-(c4) is recurrent if
00 
1
V ——------ ------- — = ^. 
(1.1.4)
n=1Y(n) - y (n—1)
Proof. Let
ak 
W(u w (u, u ), k = 0, 1, ....
uESk u'ESk- 1
We shall prove that (1.1.4) is a sufficient condition for the series 
520=0(ak)-1 to be divergent. To this end, let us estimate the ak,k = 
0, 1, . . ., in terms of the growth function:
ak = y^ y^ wcj^uJe(u,u')
uESk u'ESk- 1 UE-C
< 
b 
YJJ Ju (u)
< bn0(y(k) — Y(k — 1)),
where n0 and b are the constants occurring in conditions (c1) and (c4), re­
spectively. The last inequalities together with (1.1.2) assure the divergence 
of the series k(ak)-1. Then, appealing to Theorem 2.2.2 of Part I, one 
obtains the desired result. 
□
The previous theorem shows that, although the stochastic features of 
a circuit chain do not in general remain invariant when the collection of 
the weights {wu}uEC varies while the configuration of the circuits remains 
unchanged, one can still find stochastic properties which remain invariant 
when the collection Wa = {wu}u varies in a certain family {Wa}. This will 
then argue for a dichotomy of the collections Wa of circuit-weights into 
recurrent collections and transient collections according to whether the 
corresponding circuit chains are recurrent or transient. A detailed study of 
this aspect is given by S. Kalpazidou (1991e) and is based on an important 
lemma due to P. Baldi, N. Lohoue, and J. Peyriere (1977) and specialized 
to discrete spaces by N. Varopoulos (1984c).

210 
1. Stochastic Properties in Terms of Circuits
1.2 The Entropy Production of Markov Chains
1.2.1. A heuristic introduction to the entropy production of Markov chains 
has its beginnings in the corresponding generative entity arising in non­
equilibrium statistical physics. Let S be a nonequilibrium system of coupled 
chemical reactions where some reactants are continuously introduced into 
the system and others are continuously withdrawn. Then the affinity
Aij = pjpji - pipij
expresses the reaction rates, where pij denotes a probability law from 
i to j, (pj) is a strictly positive probability distribution, and i,j G 
{1,...,n},n > 1, symbolize the chemical components involved in the reac­
tion. The number n of components does not need to be finite. Consider the 
entity
A ij = log Pj with Pij > 0, i,j G{ 1 ,...,n},
which is known in the physical nomenclature as the conjugated thermody­
namic force of Aij . Then the expression
_ 
-1 .—. 
. ~
E = 2 Ei,j Aij Aij
= 2 Ei,j (PjPji - PiPij )log pjpji, 
(1-2-1)
pipij
containing all pij > 0, may be interpreted up to a constant factor (which 
is the Boltzmann constant multiplied with the temperature at which the 
reaction occurs) as the entropy production of the system S.
The expression E given by (1.2.1) was first investigated by J. Schnaken- 
berg (1976) under the standpoint of nonequilibrium statistical physics. Ac­
cordingly, E is decomposed into two terms E1 and E2 such that
E = E1 + E2 
(1.2.2)
and
1 
, 
pj Vi
Ei = IEi,j(PjPji -PiPij)log ~,
pi
E2 = 1Eij(PjPji - PiPij) log P^.
2 
, 
Pij
The interpretation of this decomposition is more suitable in the nonhomo- 
geneous case when the equation for the dynamical evolution of a probability 
distribution Pi(t) (over states i G{1, 2,...,n}) characterizing the system

1.2 The Entropy Production of Markov Chains 211
S is given by
d 
n
dtPi (t) = JS(Pj (t)Pji - Pi(t)Pij).
j=1
Then E1 is exactly the first derivative of (- i Pi(t) log Pi(t)) and repre­
sents the entropy of the system in equilibrium, while P2 is the contribution 
due to the coupling of the system to an external set of thermodynamic 
forces which prevent the system from achieving an equilibrium state. Here 
we point out that the previous interpretation in a thermodynamic setting 
argues why the expression (1.2.2) can be identified with the entropy pro­
duction of the system S except for a couple of factors due to the natural 
conditions. A second reason that enables one to identify (1.2.2) with the en­
tropy production of the real system is the stability criterion of P. Glansdorff 
and I. Prigogine (1971) according to which a steady state of a thermody­
namic system is stable if the so-called excess entropy production, that is, 
the second-order variation 62E around the steady state, is positive (see J. 
Schnakenberg (1976), p. 579).
1.2.2. We are now in the position to apply to Markov chains the argu­
ment developed in the previous paragraph and to define the analogue of the 
entropy production. Let S be a denumerable set. Consider £ = (£n)n>0 as 
any irreducible and positive-recurrent S-state Markov chain whose transi­
tion probability matrix is P = (pij, i,j G S). Let n = (ni, i G S) denote the 
invariant probability distribution of P. Then the expression
E = 1 EijEs(niPij - njPji)log -ip-j, 
(1.2.3)
nj pji
where the pairs (i, j ) occurring in the sum correspond to strictly positive 
probabilities Pij , is called the entropy production of the chain £. Since the 
chain £ is a circuit chain with respect to some class (C, wc ) of directed 
circuits in S and positive weights, it might be interesting to express the 
entropy production in terms of the circuits and their weights. Here is a 
detailed argument due to Minping Qian and Min Qian (1982) (see also 
Minping Qian et al. (1991)).
Namely, we have:
Theorem 1.2.1. The entropy production E of an irreducible and positive- 
recurrent Markov chain £ with a denumerable state space S has the following 
expression in terms of the circulation distribution (wc, c e C):
_ 
__ . 
w„ 
.
E = 1EcEC(wc - wc-)log-----, 
(1.2.4)
wc_
where C is the collection of directed circuits occurring along almost all the 
sample paths and c- denotes the reversed circuit of c.

212
1. Stochastic Properties in Terms of Circuits
Proof. Let P = (pij, i,j G S) and n = (ni, i G S) be, respectively, the tran­
sition matrix and the invariant probability distribution of £. The circulation 
distribution (wc ,c G C) was introduced by Theorem 3.2.1 of Part I. Here C 
is the collection of all directed circuits with distinct points (except for the 
terminals) occurring along almost all the sample paths of £ . Assign each 
circuit c = (i 1,... ,is,i 1) G Cto the cycle c = (i 1,..., is), s > 1. Then, ap­
pealing to Theorem 3.3.1 and Corollary 3.2.3, one may write
E=1 z z 
(wc-wc-)lognpj
i,j (i,j)occurs in c 
j j
=112(w - wc- ) 
log
k=1
niK PiKiK +1
niK +1 PiK +1 iK
= 112(w
s
- wc- )log
k=1
niK PiKiK +1
niK +1 Pir. + 1 iK
= 112(w - wc- )log   .
- 
wc-
□
1.3 Reversibility Criteria in Terms of the Circuits
Let S be an arbitrary denumerable set and let Z denote the set of all in­
tegers. We say that an irreducible and positive-recurrent S-state Markov 
chain £ = (£n)n Z is reversible if (£m 1, £m2,..., £mn) has the same distri­
bution as (£t-m 1 ,£t-m2,... ,£t-mn) for all n > 1, and m 1,..., mn,T G Z. 
The most known necessary and sufficient criterion for the above chain 
£ to be reversible is given in term of its transition probability matrix 
P = (Pij,i,j G S) and invariant probability distribution n = (ni,i G S), 
and it is expressed by the equations
nipij = njpji, 
i,j G S. 
(1.3.1)
When relations (1.3.1) hold, we say that £ is in detailed balance (see P. 
Whittle (1986), F.P. Kelly (1979)).
Let us write relations (1.3.1) for the edges (i1, i2), (i2,i3),...,(is,i1) of 
an arbitrarily chosen directed circuit c =(i1,...,is,i1),s > 1, with distinct 
points i1 ,...,is , which occurs in the graph of P. Then multiplying these 
equations together and cancelling the corresponding values of the invariant 
distribution n, we obtain the following equations:
Pi 1 i 2 Pi 2 i 3................Pis- 1 is Pisi 1 = Pi 1 is Pisis- 1.............Pi 3 i 2 Pi 2 i 1
(1.3.2)
for any sequence of states i1,...,is G S. Equations (1.3.2) are known as 
Kolmogorov’s criterion and provide a necessary and sufficient condition, in 
term of the circuits, for the chain £ to be reversible.

1.3 Reversibility Criteria in Terms of the Circuits 213
As we shall show in this section, a natural development of the idea of 
expressing the reversibility property in term of the circuits can be achieved 
if we appeal to the circuit representation theory according to which the 
original chain £ is completely determined by a collection (C, wc) of directed 
circuits and weights. Then, when the circuits to be considered are defined 
by the sample paths of £, the corresponding criterion assuring the property 
of reversibility will rely on the process itself.
Let us further develop certain necessary and sufficient conditions, in 
terms of the weighted circuits, for the chain £ to be reversible. To this 
end, let us consider the circulation distribution (wc, c G C) defined in The­
orem 3.2.1 of Part I, where C contains now all the directed circuits (with 
distinct points except for the terminals) occurring along almost all the sam­
ple paths of £. Then the circuit weights wc ,c G C, also called cycle skipping 
rates, are defined by the sample paths of £ according to Theorem 3.2.1.
To establish the connection between the wc’s and equation (1.3.1) above, 
we may use the entropy production of £ introduced in the previous section. 
Namely, we have
E = 1 52 (niPij - njPji) log -ipj- 
i,jES 
nj pji
= 152 (wc - wc- )log w~ ■ 
ctC 
wc-
Then the expression
(wc - wc-) log w—
describes the deviation from symmetry along the circuit c, while the en­
tropy production is the total deviation from symmetry along the circuits 
occurring on the sample paths.
Accordingly, one may assert the following criterion: the circuit chain 
£ is reversible if and only if the components wc ,c G C, of the circulation 
distribution of £ satisfy the consistency condition
wc = wc- , 
(1.3.3)
where c_ denotes as always the reversed circuit of c.
The analogues of the previous relations for physical phenomena are given 
by T. Hill (1977) using a diagram method where his concepts of cycle flux 
and detailed balance correspond, respectively, to the circulation distribu­
tion and reversibility property of Markov chains.
One may obtain the same condition (1.3.3) using the connection between 
the wc’s and the Kolmogorov criterion (1.3.2). To this end we shall need the 
following algebraic expression of wc provided by Corollary 3.2.2 of Part I. 
Let c = (ii,... ,is,i i),s > 1, be a directed circuit of Cwith distinct points 

214 
1. Stochastic Properties in Terms of Circuits
i1 ,...,is. Then the cycle skipping rate wc has the following expression:
wc = ni 1 Pi 1 i2Pi2i3 • • • Pis-1 isPisi 1
• N (i 2 ,i 2/i 1) N (i 3, i 3/i 1, i 2) ■■■ N (is,is/i 1, .. .,is-1),
where N(ik, ik/i1,..., ik-1) denotes the taboo Green function introduced 
by relation (3.1.4) of Part I. Since the product
ni 1N (i 2, i 2/i 1) N (i 3, i 3/i 1, i 2) •• • N (is, is/i 1,..., is-1)
is unaffected by the permutation of the indices i1 ,i2,...,is ,wemayin- 
troduce it, as a multiplier, in the Kolmogorov equations (1.3.2). Then we 
obtain again the consistency relation (1.3.3).
Let us now suppose that the state space S of £ is finite. Let (C, wc) be any 
deterministic representation of £ as in Theorem 4.2.1 of Part I. Then the 
transition probabilities pij,i,j G S, of £ are defined as pij = w(i,j)/w(i), 
with
w(i,j) = 
wcJc(i, j)
ceC 
w(i) = 
wcJc(i),
ceC
where the passage function Jc(•, •) is given by Definition 1.2.2 of Part I, and 
Jc(i) = j Jc(i, j). Consider the collection C- = {c- : c- is the reversed 
circuit of c, c G C}. Put wc- = wc . Define
w-(i,j) = 
eC wc-Jc-(i, j)
w-(i) = 
wc- Jc- (i),
i,jG S,
(1.3.4) 
iG S,
i,jG S,
(1.3.5) 
iG S.
Then one can find that the transition probabilities of the inverse chain of 
£ are given by
Pij =
w- (i,j) 
w- (i)
i,jG S.
This immediately leads to the following conclusion: £ is reversible if and 
only if w(i,j) = w- (i, j) for all i,j G S.
Our results can now be summarized in
Theorem 1.3.1. Let S be a denumerable set. If (Pij ,i,j G S) is the tran­
sition matrix and (C, wc) is the circulation distribution associated with an 
S-state irreducible and positive-recurrent Markov chain £ = (£n)neZ, then 
the following statements are pairwise equivalent:

1.4 Derriennic Recurrence Criterions in Terms of the Weighted Circuits 215
(i) The chain £ is reversible.
(ii) The chain £ is in detailed balance. That is, 
niPij = njpji, 
i,j E S,
where n = (ni, i E S) denotes the invariant probability distribution 
of £.
(iii) The transition probabilities of £ satisfy the Kolmogorov cyclic con­
dition:
Pi 1 i 2 Pi 2 i 3 • • • pis-1 is pisi 1 = Pi 1 is pisis-1 • • • Pi 3 i 2 Pi 2 i 1 ,
for any sequence of states i1,...,is E S.
(iv) The components of the circulation distribution of £ satisfy the con­
sistency condition:
wc = wc- ,cE C.
(v) The entropy production is null, that is, 
J3(Wc - Wc- ) log W- = 0.
1.4 Derriennic Recurrence Criterions in Terms 
of the Weighted Circuits
As we have seen, recurrence criterions are usually given under the reversibil­
ity hypothesis. When this property does not hold, we still may find recur­
rence criterions for the case of the circuit Markov processes.
The present section is devoted to Derriennic’s recurrence criterions (as 
given in Derriennic (1999a, b)) by using the circuit representation for ran­
dom walks in random environments on the integers line Z, whose increments 
are +2 or -1. These criterions provide a method to construct plenty of re­
current Markov chains, and show that recurrence is a property which does 
not depend only on the unidimensional marginal distributions of the envi­
ronment, in contrast to the case of the “birth and death” random walks 
studied by Solomon (1995). Furthermore, the Derriennic criterions extend 
previous result of Letchikov (1988) and improve the efficiency of Key’s 
criterion (1984), based on the multiplicative ergodic theorem of Oseledets.
1.4.1. Following always Derriennic (1999a), let us first consider a random 
walk X = (Xk)k>0 on the set N of natural numbers, with the jumps +2 or 
-1 in a fixed environment. The corresponding Markov transition matrix is 
given by
P(Xk+1 = n + 2/Xk = n)=Pn, 
(1.4.1)
P(Xk+1 = n — 1 /Xk = n) = qn = 1 - Pn, n > 1,
P(Xk+1 = 2/Xk =0)=1,

216
1. Stochastic Properties in Terms of Circuits
where the (pn)n>0 is an arbitrarily fixed sequence with p0 = 1 and 0 < 
pn < 1, for any n > 1.
Consider also the corresponding “adjoint” chain X' = (X'k)k>0 on N, whose 
only possible transitions are n ^ n — 2 and n ^ n + 1. Then the Markov 
transition matrix of X' is given by
P (Xk+1 = n +1 /Xk = n ) = qn,
P (Xk+1 = n - 2/Xk = n) = Pn = 1 - qn,
P (Xk+1 = 2/Xk = 1) = 1,
P (Xk+1 = 1 /Xk = 0) = 1,
n> 2,
(1.4.2)
where (pn)n>0 is an arbitrary sequence with p0 = p'1 =0 and 0 < pn < 1, 
for any n > 2. In the sequel we shall use the following notations: the chain 
X has the jumps +2 or — 1 with the probabilities (pn,qn); the chain X' 
has the jumps — 2 or +1 with the probabilities (pn, qn).
Plainly, these chains are not reversible, but they admit a representation by 
cycles and weights, which we shall use to study criterions for their recurrent 
(transient, or, positive-recurrent) behavior.
First, we have
Proposition 1.4.1. The Markov chain X introduced by (1.4.1) has a 
unique representation by cycles and weights.
Proof. The set of representative cycles is given by the sequence cn = 
(n, n +2,n+1),n > 0, since only the transitions from n to n + 2, and from 
n to n — 1 are possible. There are 3 cycles passing through each point 
n > 2: cn, cn-1, cn-2; 2 cycles passing through 1: c1, c0; and, only one cycle 
c0 , passing through 0. Then it remains to define the corresponding weights. 
Specifically, if we symbolize the weight wc(n) of cn by wn , then the sequence 
{wn ,n > 0} have to be a solution to the equation
wn-2 + wn-1 + wn
.
w0 + w1
Let us put €n = Ww 1, n > 1. Then the preceding equation reduces to:
p1 
pn 
1
£ 1 = — and £n = — 1 + ------ , n > 2■
n
q1 
qn 
€n-1
Given the sequence (pn), it is clear that the solution (€n), n > 1, exists and 
is unique. Thus the sequence of the weights wn ,n > 0, is uniquely defined 
as wn = w0 €1 ...€n , up to a multiplicative constant factor (the uniqueness 

1.4 Derriennic Recurrence Criterions in Terms of the Weighted Circuits 217
in the statement of the theorem is obviously understood up to a constant 
factor). Consequently, the transition probabilities of X are written as:
Pij = £ WnJcn ( i,j ) / £ WnJcn ( i ),
where Jcn (i,j) = 1, or 0 according to whether or not (i, j) is an edge of cn, 
and Jcn (i) ^2 Jcn (i, j)• The proof is complete. 
□
To study the adjoint chain X', we shall need
Lemma 1.4.2. Given a positive sequence (an)n>o, there exists a positive 
sequence (zn)n>o, which is the solution of equation
zn an 11 +
zn+1
n > 0.
Namely, the value zo may be chosen to be any intermediate value between 
the inferior and superior limits of the convergents of the continued fraction
1 
u o +------- ;-------—,
u 1 +-----
1 
u2 ...
where uo = ao and un +1 = a.n+1 . The solution (zn)n>o is unique if any only 
i^2n=o un = + ^, and in particular if 
n=o an = + ^-
Proof. The numbers un are positive. Even though they are not integers, 
we can write them in the formula of the convergents of a continued fraction 
in the place of the partial quotients. Plainly, zo may be any number between 
the limit of the increasing sequence of even convergents and the limit of 
the decreasing sequence of odd convergents. For example, if 
zo = uo + u1 +
1 
, 1 ,
u A1+z3)
then
uo +------- < zo < uo + —.
u 1 + U2 
u 1
For more details, see Khinchin (1984). This proves the existence of the 
sequence (zn)n>o. It is not unique if the two limits of even convergents and 
of odd convergents are not equal. In this case zo can be chosen arbitrarily 
in the corresponding interval. If the two limits are equal, zo is this unique 
value and then the sequence (zn)n>o is unique, as well. It is well known that 
the convergence of the continued fraction is equivalent to ^22=o un = + ^. 
The proof is complete. 
□

218 
1. Stochastic Properties in Terms of Circuits
Now, we are prepared to prove
Proposition 1.4.3. The adjoint Markov chain X' defined by (1.4.2) is 
a circuit chain whose circuit representation is not necessarily unique. A 
sufficient condition to have a unique circuit representation is given by
x>
Pn
q'
n=1 qn= + ^.
Proof. The set of the representative directed cycles for X' is {cn = (n,n + 
1, n + 2); n > 0}; they are the reversed cycles of those which represent the 
chain X. The existence of the weights w'n = w'c(n) is not obvious, as it was 
for the chain X. Specifically, the sequence {w'n} is given as a solution to 
the equation
pn
wn—2
w'n-2 + w'n-1 + w'n '
n> 2.
Since there is only one cycle c0 , which passes through 0 and two cycles c0 
and c 1 passing through (1, 2), then the corresponding two weights w0 and 
w'1 may be arbitrarily chosen. To solve the proposed equation, let us put 
('n +1 = wn—1 ,n > 1. Then, by applying Lemma 1.4.2 to the equation
en=Pn (1+eu),n >2,
for an admissible value of (2, there exists a sequence of weights defined 
as w'n = ,, w.0—, n > 1. If e2 is unique then the sequence of weights w'n is 
g2 "'gn + 1
unique up to a multiplicative factor w0. The proof is complete. 
□
Furthermore, we have
Theorem 1.4.4 (Positive-recurrence criterion). The chain X defined by 
(1.4.1) is positive-recurrent if and only if ^2 +==1 (1 • • • en < + ^, where
en = pn (1 + g 1 1), with (1 > 0 and n > 2. The adjoint chain X' defined 
by (1.4.2) is positive-recurrent if and only if ^2 +=2 g 1g ' < + ^, where 
e'n = pn (1 + g' 1 ), with (2 > 0 and n > 2. 
□
Now we shall study a recurrence criterion for the above chains. Following 
the well-known method based on the Foster-Kendall theorem, we consider 
the harmonic functions on N\{0}. For the chain X, the equation of harmonic 
functions is given by
Pnfn+2 + qnfn-1 = fn, n > 1.

1.4 Derriennic Recurrence Criterions in Terms of the Weighted Circuits 219
With the differences Afn = fn - fn-1 we get
(A fn+2 + A fn-1) Pn = qn (A fn ), 
and with Yn = A^fn we have
'n 
A fn+1
n > 1.
pn 
1
Yn = --- 1 +---------
qn \ 
Yn +1
We recognize here the equation of the fn for the chain X', where p'n = 
pn (n > 2). Therefore, the strictly increasing harmonic functions of the 
chain X are in correspondence with the weight representations of the chain 
X' such that
p'n = P(Xk+1 = n - 2/Xk = n)
(1.4.3)
= P(Xk+1 = n + 2/Xk = n)=pn ,n> 2.
We shall call the chain (X'k)k>0 the adjoint of the chain (Xk) k> 0 if and 
only if relation (1.4.3) holds.
For a chain X' it is understood that p0 = p'1 = 0; therefore, the adjoint of 
a chain X is well defined. However, two chains X with the same pn for any 
n > 2 and with different p1 have the same adjoint. Since two such chains 
have the same asymptotic behavior, there is no inconvenience in calling 
also the chain X the adjoint of X', when pn = p'n, for n > 2.
For the chain X', the harmonicity equation on N\{0} is given by
P'nf'n-2 + qn fn +1 = fn, 
n > 2 •
Letting y'n = ^fn +1 we further get 
A f n
Yn
P'n 
qn
n>2
Then, according to Proposition 1.4.1 we here recognize the equation of the 
fn for the adjoint chain X. Consequently, we may state the following:
Theorem 1.4.5 (Recurrence-Transience Criterion). The chain (Xk) de­
fined by (1.4. 1) is transient if and only if the adjoint chain (X'k) (according 
to (1.4.3)) is positive-recurrent. Both adjoint chains (Xk) and (X'k) are 
simultaneously null-recurrent.
Specifically, we have
(i) the chain (Xk) is transient if and only if += +=1 w'n < w, where 
wn = 
, with fn = pn (i+«7^), n > 2 and f2 > 0; (wn) is
s 2 ...^n + 1 
qn 
^n + 1
the weight sequence of the adjoint chain (Xk); a symmetrical state­
ment holds for the chain (Xk) with wn = f 1 . .. fn, where fn = 
pn (1 + sh),and f 1 >0.
(ii) both adjoint chains (Xk) and (Xk) are null-recurrent when 
E^ 
V-^'^
n =1 wn = E n =1 wn = + ™.

220 
1. Stochastic Properties in Terms of Circuits
1.4.2. Consider now random walks on Z whose possible steps are +2 or 
-1. We shall give criterions for recurrence or transience of the random 
walks on Z, which are valid for almost all environments. For the birth and 
death chains similar results are given by Solomon (1975).
Let (S, S, m) be a probability space and let 0:S ^ S be a measure pre­
serving ergodic automorphism of this space. Let p be a measurable function 
p:S ^ (0, 1).
Each s G S generates the random environment pn = p(0ns), where 0 is 
measure preserving and ergodic. The sequence (pn ) is a stationary and er­
godic sequence of random variables. On the infinite product space Q = ZN, 
with the coordinates (Xk)k>0, we define a family of probability measures 
(Ps)seS, such that for every s G S, (Xk) is a Markov chain on Z with
Ps(X0 =0)=1, 
(1.4.4)
Ps(Xk+1 =n+2|Xk = n) = p(0ns),
Ps(Xk +1 = n - 1 Xk = n) = 1 -p(0ns) = q(0ns).
We have
Proposition 1.4.7. For m-almost every environment s G S, the chain 
(Xk)k>0 has a unique cycle representation (C,wn).
Proof. We may choose as representative cycles the ordered sequences cn = 
(n, n +2,n+1),n G Z. If we denote by wn (s) the weight of cn, and put 
Cn(s) = WwIsS), n G Z, we get the equation
1
Cn (s) = p (0ns) (1 +
Cn-1 (s)
nG Z.
(1.4.5)
By applying Lemma 1.4.2 to the sequence (C-n), we see that the solution 
(Cn(s))nZ exists and is unique m-a.s. The unicity comes from the sufficient 
condition 52n=—^o p (0ns) = + ^, m-a.s. Then the corresponding sequence 
of weights is defined as
w0(s) = 1,
wn(s) = C1(s) ...Cn (s),
wn (s)=
1
C0(s)C—1(s)... Cn +1(s),
if n> 0,
if n< 0,
(as before the uniqueness of the weight sequence is understood up to a 
constant factor).
Let us consider more closely the sequence (Cn (s))neZ as a solution to 
the equation (1.4.5). Specifically, from Lemma 1.4.1, we know that C0 (s) is 
given by the infinite continued fraction with partial quotients u0( s) = p (s), 

1.4 Derriennic Recurrence Criterions in Terms of the Weighted Circuits 221
and vn+1( s ) = p (0n 1 s)/vn(s). Consequently, we write
£ 0( S ) = [ V 0( s) ,...,vn (s),... ].
Plainly, £0 is measurable and positive on S. Furthermore we have
£k(s) = [V0(0ks),. .. ,vn(0ks),. ..] = £0(0ks),
therefore, the sequence (£k)kEz is stationary with respect to the probability 
measure m.
From the ergodicity hypothesis on 6, it is clear that convergence of 
52 0? 1 wn and 52n=—^o wn are properties which hold m—almost everywhere 
or m—almost nowhere. 
□
Now, we introduce the “adjoint” random walk (Xk)k>0 in a random envi­
ronment s as in (1.4.3). Correspondingly, we have
Probs (X 0 = 0) = 1, 
(1.4.6)
Probs(Xk +1 = n - 2/Xk = n) = p(0ns),
Probs (Xk +1 = n + 1 /Xk = n) = 1 - p (0 n s) = q (0 ns).
For this adjoint chain there is a unique cycle representation, where the 
cycles are cn = (n, n + 1 ,n + 2) and the weights wn (s) verify £n+1( s) = 
wn—1( s) 
.
------- , where
wn(s)
£n(s) = p(0ns)(1+ 
s , n E Z.
The function £0 (s) is given by the continued fraction [v'0 (s),..., vn (s),...] 
whose partial quotients are v0 (s) = - (s) = v0( s) and
vn+1( s ) = p (0 n+1 s)/v'n (s) . 
q
From the recurrence-criterion given by Theorem 1.4.5, we know that the 
behaviors of (Xk) and of the “adjoint” (Xk) are tied together, and depend 
on the convergence of the series
00 n 
00 n 
— 1 zn+1 
\ — 1 — 1 /n+1
e n £0(0'), e n £0(0'),£ n £0(0-) 
, e n £0(0')
1 £ =1 
1 £ =1 
? V=0 
— 
? V=0
We have
Theorem 1.4.8 (Recurrence-transience criterion). Assume that the two 
functions lny—p and ln 1—p are m-integrable.
The random walk X = (Xk)k>0 in ergodic random environment, defined 
by (1.4.4), is recurrent for m-a.e. environment s E S, if and only if 

222
1. Stochastic Properties in Terms of Circuits
fs In £0 (s)dm(s) =0, where £o(s) is the infinite continued fraction 
£ 0( S ) = [ V o( S ), . . .,Vn ( S ), . . . ],
with V., ^_^,...,v„ +1 = v0 ° 8 n+1 ,...
1 - p 
vn
If j In £o(s)dm(s) > 0, then lim Xk = + to a.s.
If f In £o(s)dm(s) < 0, then lim Xk = —to a.s.
The “adjoint” random walk X' = (Xk) k>o defined by (I.4.6), with the in­
crements —2 or +1, is recurrent if and only if the random walk X = 
(Xk)k>o is recurrent.
Moreover, lim Xk = + to a.s. if and only if lim Xk = —to a.s., and re- 
k—>^> 
k—>^>
ciprocally.
1.4.3. Derriennic (1999a) investigates a few examples of random walks in 
a random environment. Let a and p be two numbers such that 0 < a < 1 
and 0 < p < 1. Consider also the values pn of the probabilities of jumps 
from n to n + 2 as follows: pn = a with probability 1/2, or pn = p with 
probability 1/2. Therefore, the marginal distributions of the sequence pn 
are given. We shall consider first the periodic case, and then the case of an 
environment which is independent and identically distributed.
In the periodic environment, with period 2, we put pn = a if n is even, 
and pn = p if n is odd, or the converse. The underlying dynamical system 
(S, m, 8) is just a set having 2 elements, with m the uniform measure and 8 
the permutation. Then £o takes only 2 values x and y with probability 1 /2, 
where y = j—7p( 1 + 1) and x is the unique positive solution to the equation
a ( p + x \
1 - a 
p(1 + x) J
Then we get
+ln(1 + x)
—p
and after elementary computations the recurrence criterion is as follows:
i) if i-a + 1—p = 1, then we have recurrence,
ii) if i-a + 1—p > 1, then we have transience and Xk ^ + to a.s.,
iii) if iaa + 1—p < 1, then we have transience and Xk ^ —to a.s.,
where the average increment is equal to 3(a + p) - 1.
When iaa + i—p = 1, this average is negative, except for a = p = 1 /3. 
In other words, the random walk is recurrent but the average increment is 

1.4 Derriennic Recurrence Criterions in Terms of the Weighted Circuits 223
negative. A small increase of a will produce the transience to + to; yet if 
the increase is small enough the average increment remains negative.
In the independent identically distributed environment, the infinite con­
tinued fraction £0 is a random variable which cannot be easily simplified. 
Yet using the convergents it is possible to give explicit sufficient conditions 
of transience.
Namely, with
V0 = 7^-, 
Vn+1 = f 1 p-n- 1 "l -, 
n > 0,
1- p0 
1- p-n-1 vn
we have
£0 =[V0,...,Vn,...]
and
[ V 0,..., V 2t +1] > £ 0 > [ V 0,..., V 2t+2], for any € > 0.
Using £0 < [v0,v 1 ,v2,v3], we obtain
. > 
, 
P 0 
P—2 /(1 — P—2)
ln£0 dm < ln 1 +  ---------------p— 
—p— 
—p--^ dm.
1 V 1 - P 0 
T-p-1(1 + T-p-2)+ i-p-2 J
A computation of this quantity for (pn)nZz an independent and identi­
cally distributed sequence, where pn = a with probability 1 /2,pn = p with 
probability 1/2, and with the additional condition T—a + T—p = 1, yields 
a function of T—a having the following properties:
— 
for T— a = 1 /2 (a = p = 1 /3, that is, the fixed environment) the 
corresponding value is ln TT > 0,
— 
for T— a— 1 —, the function tends to —to .
— 
for T— — 0+, the function tends to —to.
These properties show that for a close enough to 0 or 1/2, the random walk 
in the independent and identically distributed environment is transient 
and Xk ——to a.s. although in the periodic environment (with the same 
marginal values) the random walk is recurrent. This is the case for example 
with a = 1/4, that is, Pn =1/5 with probability 1/2, and Pn = 3/7 with 
probability 1/2.

2
Levy’s Theorem Concerning 
Positiveness of Transition 
Probabilities
Paul Levy investigated “the allure” of the sample paths of general Markov 
processes £ = {£t}t>0 with denumerable state space S by using the prop­
erties of the so-called i-intervals, that is the sets I(i) = {t: £t = i}. Levy’s 
study concludes with a very fine property of the transition probabilities 
pij (t) of £, known as the Levy dichotomy:
for any pair (i,j) of states and t G (0, + 'X),pij(t) is either identically 
zero or everywhere strictly positive.
(See P. Levy (1951, 1958).)
D.G. Kendall, introducing a classification for Markovian theorems in the 
spirit of the swallow/deep classification of Kingman, pointed out that the 
Levy dichotomy belongs to the class of theorems relying on the Chapman- 
Kolmogorov equations (see D.G. Kendall and E.F. Harding (1973), p. 37).
D.G. Austin proved Levy’s property by a probabilistic argument, using 
the right separability of the process and Lebesgue’s theorem on differen­
tiation of monotone functions. Another proof, more analytic, was latter 
given by D. Ornstein (see K.L. Chung (1967) for details on these results). 
Recently, K.L. Chung (1988)) proved Levy’s theorem by using some infor­
mation from the corresponding Q-matrix: he assumes the states are stable.
In this section we shall show that Levy’s theorem has an expression 
in terms of directed cycles or circuits, when the state space is at most a 
countable set and the process admits an invariant probability distribution 
n = (ni ,i G S). Our approach relies on the circuit representation theory 
exposed in Part I according to which, for each t, the transition probabilities 
pij (t) are completely determined by a class {C(t),wc(t)}, where C(t) and 

226 
2. Levy’s Theorem Concerning Positiveness of Transition Probabilities
wc(t) denote, respectively, a collection of directed circuits occurring in the 
graph of (pij(t),i,j e S) and strictly positive numbers. Specifically, the 
pij (t)’s are expressed as
niPij (t 
^2 wc(t) Jc(i,j), 
wc(t) > 0, t — 0, i,j G S,
cEC(t)
where Jc is the passage function associated with c. Throughout this chap­
ter the circuits will be considered to have distinct points (except for the 
terminals). Then for t>0
W(i,j,t) = niPij (t) > 0
if and only if (i, j) is an edge of some circuit c e C(t).
Accordingly, we may say that Levy’s theorem expresses a qualitative prop­
erty of the process £. This will then inspire a circuit version of Levy’s 
theorem according to which the representative circuits are time-invariant 
solutions to the circuit generating equations
£w(i,j,t) = £w(k,i,t), 
i e S, t> 0.
Finally, we shall discuss a physical interpretation of Levy’s theorem when 
the elements of Ct) are considered resistive (electric) circuits, the ni,i e 
S, represent node (time-invariant) currents and the W(i, j, t),i,j e S, are 
branch currents.
2.1 
Levy’s Theorem in Terms of Circuits
Given a countable set S,letP = {P (t), t - 0} be any homogeneous stochas­
tic standard transition-matrix function with P(t) = (pij(t),i,j e S). As­
sume P defines an irreducible positive-recurrent Markov process £ = 
{£t,t — 0} on a probability space (Q, K, P). Suppose further that P (t) ,t > 
0, is of bounded degree (that is, for any i e S there are finitely many 
states j and k such that pij (t) > 0 and pki(t) > 0). For any t>0 consider 
the discrete t-skeleton Ht = {£nt,n — 0} of £, that is, the S-state Markov 
chain whose transition probability matrix is P (t). The above assumptions 
on P imply that any skeleton-chain Ht is an irreducible aperiodic positive- 
recurrent Markov chain.
Now we shall appeal to the circuit representation Theorems 3.3.1 and 
5.5.2 of Part I according to which, there exists a probabilistic algorithm 
providing a unique circuit representation {Ct, Wc(t)} for each P (t), that is,
niPij (t 
^2 Wc(t) Jc(i,j), 
t — 0, i,j e S, 
(2.1.1)
cECt 

2.1 Levy’s Theorem in Terms of Circuits 227
where n = (ni,i G S) denotes the invariant probability distribution of 
P (t), t > 0, Ct is the collection of the directed circuits occurring on almost 
all the trajectories of Ht,t > 0, and wc(t),c G Ct, are the cycle skipping 
rates defined by Theorem 3.2.1. Then the wc(t)’s are strictly positive on 
(0, + x ).
On the other hand, if we suppose that £ is reversible, that is, for each 
t> 0 the condition nipij (t) = njpji(t) is satisfied for all i,j G S, we may 
apply the deterministic algorithm of Theorem 3.4.2 for defining a circuit 
representation (C(t) ,w c (t)) of each P (t) with all w c (t) > 0 on (0, + x). As 
already mentioned we shall consider directed circuits (with distinct points 
except for the terminals) as representatives. Furthermore, we shall distin­
guish the probabilistic collection of representative circuits from the deter­
ministic ones using the notation Ct for the first and C(t) for the second 
ones. Also, the theorems quoted below belong to Part I. Denote by sgn x 
the signum, that is, the function on [0, +x) defined as sgn x =1 ifx>0, 
and sgn x =0 ifx =0.
We are now in a position to apply to Levy’s property the argument of 
the circuit decomposition above, and to show that this property has an 
expression in terms of the directed circuits.
Theorem 2.1.1. Let S be any finite set. Then for any S-state irreducible 
Markov process £ = {£t}t>o defined either by a standard matrix function 
P(t) = (pij(t),i,j G S),t > 0, or by a probabilistic or deterministic collec­
tion of directed circuits and weights, the following statements are equivalent:
(i) Levy’s property: for any pair (i, j) of states, the sgn(pij(t)) is time 
invariant on (0, +x).
(ii) Arcset C(t) = Arcset C(s), for all t, s > 0 and for all the determin­
istic classes C(t) and C(s) of directed circuits occurring in Theorem 
4.2.1 when representing Ht, and Hs, respectively, where Arcset C(u) 
denotes the set of all directed edges of the circuits of C(u), u >0.
(iii) Ct = Cs, for all t, s > 0, where Ct and Cs denote the unique proba­
bilistic classes of directed circuits occurring in Theorem 4.1.1 when 
representing Ht and Hs, respectively.
If S is countable, then the above equivalence is valid for reversible pro­
cesses. In any case, we always have (i) O (iii).
Proof. First, consider that S is a finite set. The equivalence (i) O (ii) 
follows immediately. Let us prove that (iii) ^ (i). Consider 10 > 0. Then
—wc (t o) Jc (i,j), 
(2.1.2)
ni
where n = (ni,i G S) is the invariant probability distribution of £ and 
for any pair (i, j ) of states we have
pij (t o)= £
cECt 0

228 
2. Levy’s Theorem Concerning Positiveness of Transition Probabilities
wc(t0), c G Ct0, are the cycle skipping rates (introduced by Theorem 3.2.1). 
If pij (t0) > 0, it follows from (2.1.2) that there is at least one circuit c0 G Ct0 
such that wc0 (t0) > 0 and Jc0 (i, j) = 1. Then, by hypothesis c0 G Ct for all 
t > 0. As a consequence, the pij(•), written as in (2.1.2), will be strictly 
positive on (0, + to). Therefore (iii) ^ (i).
To prove that (i) ^ (iii) we first note that the Chapman-Kolmogorov 
equations and standardness imply that Cs C Ct for s < t. It remains to show 
the converse inclusion. Let c be a circuit of Ct, that is, c =(i1,...,ik,i1) 
has the points i1 , . . . ,ik distinct from each other when k> 1and
pi 1 i2 (t)pi2i3 (t) • . . . • piki 1 (t) > 0.
Then, from hypothesis (i) we have
pi 1 i2 ( s )pi2 i3 ( s ) • ... • pik i 1 ( s ) > 0.
Therefore c G Cs, so that Cs = Ct for all s,t > 0.
Finally, for the countable state space case we have to appeal to the 
representation Theorems 3.3.1 and 3.4.2, and to repeat the above reasoning. 
The proof is complete. 
□
As an immediate consequence of Theorem 2.1.1, the circuit decomposi­
tion (2.1.1), or the cycle decomposition (5.5.2) of Chapter 5 (Part I) should 
be written in terms of a single class C = Ct , independent of the parameter­
value t>0, that is,
nipij (t) = 
wc(t) Jc(i,j), 
t > 0, i,j G S.
ceC
Accordingly, (C,wc(t))t>0 will be the probabilistic circuit (cycle) represen­
tation of £.
2.2 
Physical Interpretation of the Weighted 
Circuits Representing a Markov Process
One of the physical phenomena which can be modeled by a circuit pro­
cess is certainly that of a continuous electrical current flowing through a 
resistive network. Accordingly, the circuits and the positive circuit-weights 
representing a recurrent Markov process should be interpreted in terms of 
electric networks. Then certain stochastic properties of circuit processes 
may have analogues in some physical laws of electric networks.
Let S be a finite set and £ = {£t}t>0 be an irreducible reversible Markov 
process whose transition matrix function and invariant probability distri­
bution are P(t) = (pij(t), i,j G S) and n = (ni, i G S), respectively. Denote 
by C0 the collection of all the directed circuits with distinct points (except 
for the terminals) occurring in the graph of P (t). Since C0 is symmetric, 

2.2 Physical Interpretation of the Weighted Circuits 229
we may write it as the union C U C- of two collections of directed circuits 
in S such that C- contains the reversed circuits of those of C.
Then the probabilistic circuit representation Theorem 4.1.1 and Levy’s 
theorem enable us to write the equations
niPij (t)= £ wc(t) Jc(i,j)+ £ wc- (t) Jc— (i,j), 
(2.2.1)
for any i,j G S, t > 0, where the wc(t)’s and wc— (t)’s denote the cycle 
skipping rates for all the circuits c and c_ with period greater than 2 and 
the halves of the skipping rates for all the circuits c with periods 1 and 2. 
The passage functions Jc and Jc— occurring in (2.2.1) are those introduced 
by Definition 1.2.2 of Part I.
Consider w(i,j,t) = cTceCwc(t)Jc(i,j). Then, applying Theorem 1.3.1 
of Part II, we have
2ni = 52j w(i,j, t) = kwk w(k, i,t), i G S, t> 0. 
(2.2.2)
If we relate each circuit c G C0 with a resistive circuit, we may interpret the 
w(i, j, t),i,j G S, as a branch current flowing at time t from node i to node 
j. Suppose Ohm’s law is obeyed. Then equations (2.2.1) express Kirchhoff’s 
current law for the resistive network associated with C.
Invoking the Levy theorem in terms of circuits, equations (2.2.2) may 
be interpreted in the electrical setting above as follows: if at some moment 
t>0 there exist currents wc(t) flowing through certain electric circuits c 
according to the law of a circuit Markov process, then this happens at any 
time and with the same circuits. But, using an argument from the electrical 
context, the same conclusion arises as follows. The time invariance of the 
node currents ni,i G S, and the equilibrium Kirchhoff equations (2.2.2) 
enable one to write
£w(j,i,t - At) = £w(i,k,t + At) = 1 ni, 
i G S,t> 0. 
(2.2.3)
jk
Then, n being strictly positive at the points of every circuit c = 
(i1, .. .,is ,i1) at any time t>0, the existence of a branch current 
w(ik ,ik+1,t- At) requires the existence of w(ik+1, ik+2, t + At), and vice 
versa. Therefore the time invariance of the node currents ni and the 
equilibrium equations (2.2.3) require the existence of the branch currents 
w(j, i, t - At) > 0 and w(i, k, t +At) > 0 entering and leaving i. Then the 
collection Ct of electrical circuits through which the current flows at time 
t > 0 should be time-invariant, and this is in good agreement with Levy’s 
theorem.
In general, when interpreting a circuit Markov process, the diffusion of 
electrical currents through the corresponding resistive network can be re­
placed by the diffusion of any type of energy whose motion obeys rules 
similar to the Kirchhoff current law. For instance, relations (2.2.2) have 

230 
2. Levy’s Theorem Concerning Positiveness of Transition Probabilities
a mechanical analogue as long as Kirchhoff current law has a full anal­
ogy in Newton’s law of classical mechanics. To review briefly some basic 
mechanical elements of a mechanical system, we can recall any free-body 
diagram where a body is accelerated by a net force which equals, according 
to Newton’s law, the derivative of the momentum. This equality becomes, 
when replacing, respectively, forces, velocity, friction, mass, and displace­
ment by currents, voltage, resistor, capacitor, and flux, formally equivalent 
to Kirchhoff’s current law. The previous analogy enables us to consider cir­
cuit processes associated to mechanical systems which obey Newton’s laws. 
For instance, let us observe the motion of a satellite at finitely many points 
i1 ,i2 ,...,im of certain time-invariant overlapping closed orbits c (where 
Newton’s laws are always obeyed). Then the passages of the satellite at 
time t>0 through the points i1 ,i2 ,...,im under the traction forces wc (t), 
follow a Markovian trajectory of a circuit process with transition matrix 
function
w(i, j, t)
pij(t) =------ ~------ for all t> 0 and i,j & {i 1 ,i2,...,im},
n i
where w(i,j,t) = 52c wc(t) Jc(i,j) and ni = 52j w(i,j,t). When a trajectory 
correction is necessary at some instant of time, this will correspond to a 
perturbation of either the Markov property or strict stationarity. Then we 
have to change the stochastic model into another circuit process where the 
corrected orbits will play the role of the new representative circuits for the 
process.

3
The Rotational Theory of 
Markov Processes
3.1 Preliminaries
Up to this point of our exposition, it has been seen that the main geometric 
characteristics of either theoretical or practical importance for the defini­
tion of finite recurrent Markov processes are the edges and circuits. The 
cycle representation theory presented in Part I gives us the liberty to inter­
change the weighted edges (of the stochastic matrices) with the weighted 
cycles or circuits (of the circuit representations), and the resulting equations 
and new revelations in the interaction between the stochastic processes 
theory and algebraic topology are so useful that there is an unavoidable 
methodological horizon leading to edge-problems and cycle-problems.
It turns out that the circuits are the simplest topological structures 
which link the immediate inferior and superior topological elements in 
the sequence: 0-cells, 1-cells, 2-cells, ...... Namely, the directed circuits are
the simplest 1-chains whose boundary is zero and which are themselves the 
boundaries of certain 2-cells. Furthermore, the circuits form a basis for 
describing algebraically the linear expressions of the 1-cells and a tool for 
describing the 2-cells.
The presence of the circuits in the descriptions of certain stochastic struc­
tures, as in the collection of finite-dimensional distributions defining a re­
current Markov process, is dictated by the presence of the directed edges 
and circuits along the sample paths, and by the fundamental topological 
rule 
n t Z = 0,

232 
3. The Rotational Theory of Markov Processes
where nt is the tranposed matrix of n = (nedge, point) introduced in (1.3.7), 
and Z = (Zedge, circuit) is given by (1.3.8) of Part I.
The role of the circuits grows when assigning them to certain coordinates, 
the circuit-weights, by either nonrandomized algorithms or by randomized 
algorithms (having in mind the Kolmogorov-Uspensky (1987) theory on 
randomized algorithms).
This chapter is dedicated to a recent and essential application of the cycle 
representations presented in Part I that reveals the connections between the 
recurrent Markov processes and the rotations, something we have already 
discussed in Chapter 1. Namely, we concluded there that:
any directed circuit provides a collection of arcs (rotations) partition­
ing the circle, and vice versa, certain partitions of the circle generate 
collections of directed circuits.
A hypothesis imposed throughout this chapter will be that the circuits will 
have distinct points (except for the terminals).
Let n > 2 and let C be a set of overlapping directed circuits in a finite 
set, say {1, 2,...,n}. Then, as will be shown, it is possible to find a cor­
respondence from the set C into a set of directed circle-arcs (summing to 
2n) which are suitably indexed using the edges of C.
It turns out that the sets Si,i=1,...,n, each consisting of a finite 
union of arcs attached to the circuits passing through i by the previ­
ous correspondence, form a partition of the circle. Then the circle can 
be viewed as an Q-set of a future probability space, and the partitioning 
sets Si,i =1,...,n, as events which, when rotated by a suitable rotation 
rT of length t = 2nt, can intersect each other, that is, rT(Si) A Sj = 0, 
for some i,j G {1,..., n}. Then it is easily seen that the quantification of 
these intersections in a certain way will determine the marginal distribu­
tions of a Markov process with states 1, 2,. ..,n. For instance, the sim­
plest way to assign coordinates to the sets Si is to consider the Lebesgue 
measures of their homeomorphs Si in the linear segment [0, 1] accord­
ing to a probability distribution. Then the sets S1 ,S2 ,...,Sn partition 
the interval [0, 1]. Correspondingly the pair (circle, {Si}) will be re­
placed by the canonical probability space ([0, 1), B, X), where B de­
notes the a -algebra of Borel subsets of [0, 1) and X Lebesgue measure 
on B.
Then X(ft(Si) A Sj)/X(ft(Si)), i, j =1, 2,...,n, define a stochastic matrix 
P, where ft(x) = (x + t)(mod 1) is the shift on the real line which replaces 
the circle rotation rT above.
When a stochastic matrix P = (pij ,i,j =1,...,n) admits the previous 
description in terms of the shift ft , for a choice of the length t,anda 
partition {S1,...,Sn} of [0, 1), we say that (t, {S1,..., Sn}) is a rotational 
representation of P. All these considerations lead to the following important 
question: How to develop the theory of finite recurrent Markov processes in 

3.1 Preliminaries 233
terms of rotations, and particularly, how to define the rotational represen­
tations of the recurrent stochastic matrices?
It is obvious that an answer will naturally start from the cycle repre­
sentation theory given in Part I since the weighted circuits can provide 
a link between the weighted edges, defined by the entries of a stochastic 
matrix, and the weighted circle-arcs. Then a preliminary problem is the 
difficulty of defining a system of two different kinds of transformations for 
each recurrent stochastic matrix P:
(i) the transformation of the representative circuits of P, occurring 
in a circuit decomposition, into circle arcs, 
(3.1.1)
(ii) the transformation of the circuit-weights into the arc-weights.
The transformation (3.1.1)(i) (which is a one-to-many relation between 
the graph-elements) presupposes a choice of a circuit decomposition for P 
(as in Section 4.4 of Part I) and along with (3.1.1)(ii) requires an algebraic 
structure associated with the graph G(P) of P. The algebraic structure is 
understood to be an assignment of certain numbers (the weights), depend­
ing on P, with various elements of G(P) as the edges and circuits. These 
numbers are derived by certain algorithms according to rules involving ei­
ther the above orthogonal matrices n and Z associated with G(P), or a prob­
abilistic interpretation in term of the Markov chain on P (see Section 4.4 of 
Part I).
It is this chapter that will elucidate an affirmative answer to the above 
question (3.1.1), and the corresponding developments will be called the the­
ory of rotational representations of finite recurrent Markov processes. The 
present exposition is far away from a closed theory—it should be viewed 
as an attempt to clarify what we understand by rotational representations 
and what are their perspectives, as they can be estimated so far, to the 
theory of Markov processes, ergodic theory, dynamical systems, theory of 
matrices, etc.
The idea of geometric representations of certain n x n stochastic matri­
ces appeared first in the 1981 paper of Joel E. Cohen, who conjectured 
that each irreducible n x n stochastic matrix can be represented by a ro­
tational system (ft, {Si}) of some dimension, where ft, and {Si} have the 
meaning above, and by dimension we mean the maximum number of (arc-) 
components occurring in the unions Si,i =1,...,n.
A solution to this problem is given for n = 2 by Joel E. Cohen (1981), 
and for n > 2 by S. Alpern (1983) (using a combinatorial argument) and 
by S. Kalpazidou (1994b, 1995) (using either a probabilistic or a homologic 
argument). Major contributions to the rotational theory are recently due to 
J. Haigh (1985), P. Rodriguez del Tlo and M.C. Valsero Blanco (1991), and 
S. Kalpazidou (1994b, 1995).
This chapter is a unified exposition of all the results on the rotational 
representations, and an attempt to develop a theoretical basis for these rep­
resentations, argued by algebraic topology and the theory of Markov chains.

234 
3. The Rotational Theory of Markov Processes
3.2 Joel E. Cohen’s Conjecture on Rotational 
Representations of Stochastic Matrices
Throughout this section X denotes the interval [0, 1), A Lebesgue measure 
on the Borel a-algebra of X, and n is assumed to be an integer greater 
than 1. As is known a finite stochastic matrix P is an n x n matrix with 
nonnegative real elements such that every row-sum is 1.
Joel E. Cohen (1981) proposed the following conjecture that we shall call 
the rotational problem:
(R) Any finite irreducible stochastic matrix P =(pij,i,j =1,...,n),n > 
1, can be described by a rotational system (ft, S ) where S is a par­
tition of X into n sets S1 ,...,Sn each of positive Lebesgue measure 
and consisting of a finite union of arcs, and ft, with certain t G [0, 1), 
is the A-preserving transformation of X onto itself defined by
ft(x) = (x + t)(mod1), 
(3.2.1)
that is, ft (x) is the fractional part of x + t. The description of P by 
(ft , S ) is given by
Pij = A(Si n ft- 1(Sj))/A(Si), 
(3.2.2)
for all i,j G{1,...,n}.
A stochastic matrix P which satisfies equations (3.2.2) is called to have a 
rotational representation (t, S ). Equivalently, we say that P is represented 
by (t, S ). A stochastic matrix P is called irreducible if for any row i and 
any column j = i, there exists a positive integer k, which may depend on i 
and j, such that the (i, j)-element of Pk is not zero. The stochastic matrix 
P that occurs in the above rotational problem (R) can be chosen arbi­
trarily close to the identity matrix I = (6ij), where 6 is Kronecker’s delta, 
since lim A(ft(Si) n Sj) = 6ijA(Si), as t ^ 0. This will enable the extension 
of the rotational problem to continuous parameter semigroups (Ps)s>0 of 
stochastic matrices, where lims ,0+ Ps = (6ij) (see Section 3.9 below).
Joel E. Cohen (1981) answers the rotational problem (R) for n =2 as 
follows:
Theorem 3.2.1. Any irreducible 2 x 2 stochastic matrix has a rotational 
representation.
Proof. Let M be an irreducible 2 x 2 stochastic matrix. M is irreducible if 
and only if both elements off the main diagonal are not zero. Then, there 
exists a positive row vector v such that vM = v (Seneta (1981)). Assume 
v1 + v2 =1. It may be checked that
v = (m21/(m12 + m21), m12/(m12 + m21)).
(3.2.3)

3.3 Alpern’s Solution to the Rotational Problem 235
Now we show how to define S1 ,S2 and t>0 such that mij = pij , where 
pij is given by (3.2.2). Since v is the invariant distribution of M and of the 
desired P, it is natural, in the light of the above, to let S1 =[0,v1)and 
S2 = [v1, 1).
Let
t = m12m21/(m12 + m21). 
(3.2.4)
Since M is irreducible, t > 0. From (3.2.3) and (3.2.4) we find that t < 
vi, i = 1, 2 (because mij < 1, i = j).
Now
ft(Si) n Si = [t,vi +1) n [0, vi) = [t, vi).
Then A(ft(Si) n Si) = vi — t, and by (3.2.2) we have
pii =(vi - t)/vi. 
(3.2.5)
Substituting (3.2.3) and (3.2.4) into the right side of (3.2.5) we obtain 
pii = mii as desired. It follows that pi2 = mi2 .Sincet>0,pii < 1and 
pi2 > 0.
Analogously,
ft(S2) nS2 = ([vi +t,1) u [0,t)) n [vi, 1) = [vi +t,1) u [vi,t).
Since t < v i, [ v i ,t ) = 0. Thus A (ft (S 2) n S 2) = 1 — v i — t = v 2 — t<v 2. 
Using (3.2.3) and (3.2.4) as before, and on account of (3.2.2), we have
p22 =(v2 — t)/v2 = m22 .
Thus, we have shown that any irreducible 2 x 2 stochastic matrix M has 
a representation of the form (3.2.2), with 0 <t<1, and the proof is 
complete. 
□
3.3 Alpern’s Solution to the Rotational Problem
A stochastic matrix of a finite recurrent Markov chain is called a recurrent 
stochastic matrix. For any finite stochastic matrix P the following properties 
are equivalent:
(i) P is recurrent; and
(ii) P admits a strictly positive invariant probability row-vector v, that 
is, there is a probability row-vector v>0 satisfying vP = v .
Let us notice that there are reducible stochastic matrices that admits 
rotational representations. For instance, the identity matrix is represented 
by (0, S ), for every partition S. On the other hand, if (t, S ) represents 
an n x n stochastic matrix P, then (3.2.2) implies that (A(Si),..., A(Sn)) 
is an invariant row-distribution which, by assumptions on S in (R), has 
strictly positive elements. So, any stochastic matrix that has a rotational 

236 
3. The Rotational Theory of Markov Processes
representation is recurrent. Since any irreducible finite stochastic matrix is 
recurrent, it would be interesting to see if the rotational problem (R) stated 
in the previous section can be generalized from irreducible to recurrent 
stochastic matrices. Here is an answer due to S. Alpern (1983).
Theorem 3.3.1. Let n > 2 and S = {1,..., n}. Any S-state Markov chain 
is recurrent if and only if its transition matrix P has a rotational represen­
tation (t, S ). Moreover, for any recurrent matrix P and for any positive 
invariant distribution n there is a rotational representation (t, S), with 
S = {S1 ,...,Sn }, where:
(i) (A(Si),...,A(Sn))= n; and
(ii) t = 1/n!.
Proof. We need only prove the “only if” part. Consider P an n x n recur­
rent matrix and n a strictly positive invariant probability row-distribution 
of P.Letc be a directed circuit with distinct points (except for the termi­
nals) of the graph of P. Consider the circuit-matrix Cc given by
Cc (i,j) = 
Jc (i,j), i,j = 1 ,...,n, 
(3.3.1)
p(c)
where Jc is the second-order passage matrix of c introduced by Definition 
1.2.2, and p(c) denotes c’s period. (The circuit-matrix was also introduced 
in Section 4.3 (Chapter 4) of Part I.). Notice that the matrix (nipij, i,j = 
1, .. .,n) belongs to a compact convex set whose extreme points are the 
circuit-matrices Cck defined by (3.3.1), where ck are directed circuits in the 
graph of P.
Then appealing to the Caratheodory dimensional theorem we obtain 
a decomposition of P in terms of certain circuits c1 ,. ..,cN in S, where 
N < n2 — n +1. Namely,
nipij ^^2 wck Cck (i,j), 
with 
Wck = 1, wck > 0, 
(3.3.2)
k=1 
k=1
for all i, j = 1,..., n (since the set of all n x n matrices (rij) that satisfy 
the isoperimetric equalities i rij = i rji, j =1,...,n,and ij rij =1 
has dimension n2 — n).
We now show that any circuit decomposition of P implies a rota­
tional representation (t, S). To this end, let M be any multiple of the 
periods p(c1),...,p(cN) of the representative circuits c1,...,cN occur­
ring in (3.3.2). In particular, we may choose either M = n! (since each 
p(ck) < n)orM = least common multiple of p(c1),...,p(cN) (for short 
l.c.m. (p(c1),...,p(cN)).
Put t =1/M.Let{Ak,k=1,...,N} be a partition of A =[0, 1/M) 
into N subintervals with relative distribution (wc1 ,...,wcN ), that is,

3.3 Alpern’s Solution to the Rotational Problem 237
A(Ak)/X(A) = wck,k = 1 ,...,N. Define
Akl = ftl-1(Ak), k=1,...,N; l =1,...,M, 
(3.3.3)
and
M
Uk = 
Akl, k=1,...,N, 
(3.3.4)
l=1
where ft is the X-preserving transformation given by (3.2.1) with t =1/M . 
Define now the partition S = {Si, i =1,...,n} by
Si = 
Akl, i=1,...,n, 
(3.3.5)
where h is the following labeling of the intervals Akl . Fix k and suppose 
Cck is the circuit-matrix associated with the circuit ck = (a 1,..., ap, a 1) 
where p is the period of ck . Define
h(k, 1) = a1, h(k, 2) = a2,...,h(k,p)=ap, 
(3.3.6)
h(k, p +1) =a1,...,h(k,M)=ap.
The fact that the last label is ap follows from the choice of M as a multiple 
of p.
It is to be noticed that the labeling defined by (3.3.6) depends on the 
ordering of the circuits in the Caratheodory-type decomposition (3.3.2) 
as well as on the choice of the representatives of the (class-)circuits (see 
Definition 1.1.2). The latter amounts in fact to the choice of the starting 
points of all the representative-circuits.
In Figure 3.3.1 we draw the intervals Akl, k =1,...,N: l =1,...,M, 
where the points of each circuit ck appear M/p(ck) times, so that each
Figure 3.3.1.

238 
3. The Rotational Theory of Markov Processes
circuit ck is represented by M/p(ck) copies (we have M intervals Akl with 
the first index k). The measure of Akl is given by
A(Aki) = (1 /M) wck, 
(3.3.7)
since
A(Akl) = A(Ak)
and
A(Ak)/(1/M) = wck.
Then
A(Uk)=wck, k=1,...,N. 
(3.3.8)
If (i,j) is an edge of ck, then A(Si n f- 1(Sj) n Uk) = (1 /p(ck)) wk.
In general,
A(Si n ft-1(Sj)|Uk) = Cck(i,j) 
(3.3.9)
for any i, j =1,...,n and any k =1, 2,...,N. Finally, we have
A(Si n ft- 1(Sj)) = £ A(Uk) A(Si n f- 1(Sj)\uk) 
k=1
TV
= 
wckCck(i, j)
k=1 
nipij.
Therefore we have shown that (1/M, S ) is a rotational representation of 
P with A(Si) = ni, i = 1,..., n, and that we may choose M = n!. 
□
Remarks
(i) In the previous proof, as well as throughout this chapter, the circuits 
are considered with distinct points (except for the terminals).
(ii) There are many ways to label the sets Si , which in turn determine dif­
ferent rotational representations. In Sections 3.6, 3.7, and 3.8 we shall 
discuss other labelings which are different from that given in (3.3.6). 
The label and rotational representations proposed in the next section 
are the most structurally close to what we understand by a rotational 
system.
Let us now examine a concrete example of a rotational representation 
due to S. Alpern (1983).

3.3 Alpern’s Solution to the Rotational Problem 239
Figure 3.3.2.
Example 3.3.1. We apply the rotational representation of Theorem 3.3.1 
to the matrix
P=
1/2
1/2
1
1/20
01/2
00
The row-vector v = (4/7, 2/7, 1/7) is an invariant distribution of P.
The first step to a rotational representation of P consists in writing the 
circuit-decomposition-equation for the matrix R =(vipij,i,j =1, 2, 3). The 
graph of P comprises N = 3 circuits for which we choose the following 
ordering: c1 =(1, 1), c2 =(1, 2, 1), c3 =(1, 2, 3, 1). The associated cycles are 
C1 = (1),c2 = (1, 2) and c3 = (1, 2, 3). We draw the graph of P in Figure 
3.3.2.
A cycle decomposition of R is as follows:
'1
0
0'
9
1
1/2
0
0
1/3
0
2
R = -7
0
0
0
2
+ 7
1 / 2
0
0
3
+ 7
0
0
1/3
0
0
0
0
0
0
1/3
0
0
(3.3.10)
Now we find a rotational system (t, S ), where S = {S1,S2,S3}, by using 
the decomposition (3.3.10).
Let M = least common multiple of the periods p(c1), p(c2), p(c3), so M = 
6. Then t =1/M = 1/6 and f1/6 is the shift to the right of length 1/6. 
Partition the interval [0,t)=[0, 1/6) into three subintervals A1, A2, A3 with 
the relative lengths given by the coefficients 2/7, 2/7, 3/7 of (3.3.10). Then 
we have
2 4
42, 42 ,
A1 =
A3 =
4 1
42,6 '
For k =1, 2, 3andl =1,...,6 define intervals
Aki = Ak 4-----—,
6
as in Figure 3.3.3. Let Uk = 
l=1 Akl,k=1, 2, 3.

240 
3. The Rotational Theory of Markov Processes
Figure 3.3.3.
circuit Cj
circuit c2
circuit c^
0L
A1
A2
A3
Ail
1 2/42 J
• 
A21
J 4/42 
■
-|------------ 1--------
Aji
! 1/6
4­
1/6
--------- 1---------
A12
• L
■
I
• 11/42
4------------1--------
A32
; 2/6
2/61
--------- 1---------
A13
116/42 ' 
. 
A23
! 18/42 4------------ 1-------
A33
! 3/6
■
5/6 [ ------- 1------
A16
37/42 '
A26
। 39/42 _|------------ 1-------
A36
1
»
-+v
The “columns” in Figure 3.3.3 are the sets Uk = 1, 2, 3, where A(U 1) = 
2/7, A(U2) = 2/7, A(U3) = 3/7. Define S 1 ,S2 and S3 as in (3.3.5) us­
ing labeling (3.3.6). The absolute distribution of Si A f—16(Sj) is given 
by R. Then P is represented by (1/6, S), where S = {S1, S2, S3} with 
(A(S1),A(S2),A(S3)) =v.
3.4 Transforming Circuits into Circle Arcs
In this section we propose to achieve one step to the original question 
quoted in (3.1.1), namely, to define a transformation of a collection of di­
rected circuits in the set S = {1,..., n}, n > 2, into a set of certain circle­
arcs which can be involved in the definition of a rotational representation 
of a stochastic matrix. Here the term transformation will correspond to 
a one-to-many relation. As we have already seen from any circuit decom­
position of a recurrent n x n stochastic matrix we can obtain rotational 
representations. In principle, this relation relies upon a general topological 
(geometric) connection: any collection of circuits involves a collection of 
arcs partitioning the circle. This idea was already initiated in Section 1.1 
of Part I where, in addition, we have shown that any circuit of period p 
can be assigned to a p-order cyclic group of rotations, and some partitions 
of the circle can generate a collection of overlapping circuits.

3.4 Transforming Circuits into Circle Arcs 241
It is the algebraic-topologic argument for the existence of a transforma­
tion of the directed circuits occurring in the graph of a recurrent stochastic 
matrix P = (pij, i,j = 1,..., n), n > 2, into certain circle-arcs providing a 
rotational representation of P that we shall examine in the present sec­
tion. This approach is due to S. Kalpazidou (1994b, 1995) and reveals the 
theoretical basis of the rotational idea.
A rotational representation of P via circuits presupposes two relations:
(R) (i) the edges of G(P) are related with certain circuits of G(P); and
(ii) the circuits of G(P) considered at (i) are related with certain arcs 
partitioning the circle.
Here we shall consider two connected directed graphs: the graph G(P) of 
the original recurrent stochastic matrix P =(pij,i,j =1,...,n), and the 
graph of a circle. Both relations quoted in (R) will be further used to define 
a transformation $ from the space of n x n recurrent stochastic matrices 
P into n-partitions of [0, 1). The domain of $ will be a convex hull in the 
(n2 - n)- Euclidean space whose extreme points are the circuit-matrices. 
Then, as we shall show below, the ordering of the decomposing circuits as 
well as the specification of their representatives in the cycle-decomposition- 
formula will influence the definition of the transformation $. A detailed 
study of such a transformation $ is, given in the next section.
3.4.1. Let us start with an ordered sequence (c1,...,cN) of circuits 
appearing in a circuit decomposition of the originally given recurrent 
stochastic matrix P =(pij,i,j =1,...,n). n > 2. Here we choose the 
Caratheodory-type decomposition for P, that is,
nipij = 52 Wck Cck (i,j), with Wck > 0 ,^Wck = 1, 
(3.4.1)
k=1 
k=1
for all i, j = 1,... ,n, where N < n2 — n +1 and n = (ni, = 1,... ,n) is a 
strictly positive invariant probability distribution of P. The Cck denotes 
the circuit-matrix associated with the circuit ck (see Section 4.3 of Part I, 
or (3.3.1)). Also, fix the starting points of c 1,..., cN. Denote by C1,..., cN 
the corresponding cycles (see Definition 1.1.3 of Part I). Our target is to 
define a transformation of c1 ,...,cN into certain circle-arcs which in turn 
are involved in the definition of a rotational representation (t, {S1,...,Sn}) 
of P. As already seen, in general any collection of circuits may be assigned 
to certain arcs Akl partitioning the circle. Here a specialization will appear 
since the homeomorphs of the arcs Akl in [0, 1) are required to define a 
rotational partition for the original matrix P. To this end, we have to find 
the length t of the shift ft(x) = (x +t) (mod 1), x G [0, 1), and how to 
define the subintervals Akl along with their indices (k, l) and a suitable 
procedure of joining Akl into sets S1,...,Sn such that (t, {S1,...,Sn}) 
will stand for a rotational representation of P.

242 
3. The Rotational Theory of Markov Processes
Let M denote the least common multiple of p(c1),...,p(cN), where p(ck) 
denotes the period of ck,k =1,...,N. Partition the circumference of a 
circle c into N equal consecutive directed arcs U1 ,..., UN such that each 
Uk is assigned to the circuit ck. Next, partition each arc Uk,k =1,...,N, 
into M equal circle-arcs denoted by ak 1, ak2,..., akM.
On the other hand, the M/p(ck) consecutive repetitions of ck = (ck(1), 
ck(2),..., ck (p(ck)), ck(1)) contain exactly M edges: (ck(1), ck(2)),..., 
(ck(p(ck)), ck(p(ck)+1)),..., (ck(M), ck(M + 1)), where the rth repeti­
tion of ck is given by the sequence (ck (1 + (r - 1)p(ck)),...,ck (p(ck) + 
(r - 1)p(ck )), ck (1 + (r - 1)p(ck ))). Then we may put these edges in a 
one-one correspondence with the circle-arcs ak1 ,ak2 ,...,akM as fol­
lows: (ck (1) , ck (2)) ^ ak 1,..., (ck (M), ck (M) + 1)) ^ akM. Accordingly, 
we may define a correspondence between the points of the cycle 
ck and the starting points of ak 1,..., akM as follows. The points 
ck (1),. ..,ck (p (ck)) ,ck (1 + p (ck)),.. .,ck (M) of the M/p (ck) repetitions of 
ck are assigned to the starting points of ak 1, ak2,..., akM along the circum­
ference of the circle c. Symbolize the starting points of ak1 ,ak2 ,...,akM 
on the circumference of the circle c by c((1),...,ck(p(ck)),ck(1 + 
p(ck)),..., ck(M). In this way:
the index of each xki is given by the pair (k, l) of the ck (l) occurring in 
the sequence k( (1), .. ., k( (p (ck)), ck (1 + p (ck)),. .., k( (M).
Furthermore, the edges of each circuit ck are assigned to the circle-arcs 
{akl} of c, and the points of the corresponding cycle ck are repeated 
M/p(ck ) times along the circumference of the circle c in Uk .
Now, consider another circle and let rT be the rotation of length t = 
2n/M. Divide this circle into M equal arcs each of length 2n/M. Let A be 
one of these arcs. Partition A into N consecutive equal arcs A1,...,AN . 
Define
Akl = rT- 1( Ak), 
k = 1 ,...,N; l = 1 ,...,M.
In this way we have transformed the circuits c1 ,. ..,cN into the circle-arcs 
Akl, k =1,...,N; l =1,...,M, which partition the circle (as quoted in 
(R)(ii)). Let A and A1 , ...,AN be the homeomorphs of A and A1 ,..., AN 
in [0, 1) defined as follows: A =[0, 1/M), A1 starts at 0 and A1,...,AN are 
consecutive disjoint subintervals of A of the form [a, b) having the relative 
lengths given by the coordinates of the vector (wc1,...,wcN) occurring in 
the decomposition (3.4.1), that is, A(Ak)/X(A) = wck, k = 1,... ,N. Here 
A denotes as always Lebesgue measure. Define
Akl = ftl-1(Ak), k=1,...,N; l=1,...,M,
where ft is the shift of length t =1/M on the interval [0, 1) as introduced 
by (3.2.1). Then
A(Akl)=(1/M)wck, k=1,...,N; l=1,...,M.

3.4 Transforming Circuits into Circle Arcs 243
Furthermore,
the index (k, l) of each Aki is assigned to that of the circle-arc aki on c.
(3.4.2) 
Let
Si = 
Aki , i=1,...,n,
where
the indices (k, l) occurring in the union Si are given by those arcs aki 
whose starting points are symbolized on the circumference of the circle c 
by i.
To summarize, the rigorous expression of the label of each Aki occurring 
in the union Si is given, according to S. Kalpazidou (1994b), by any pair 
(k, l) = (ki,li) defined as:
(i) ki is the index of a chosen representative of a class-circuit c,, k G
{1, 2,...,N}, which passes through the pre-given point i and which 
occurs in decomposition (3.4.1).
(ii) li denotes those ranks n G {1, 2, ... ,M} of all the points c( (n) which 
are identical to i in the M/p(Ck) repetitions of the cycle Ck = (c( (1), 
cCk(2),..., cCk(p(ck))) associated to the representative of the circuit 
Ck chosen at (i) above, i.e., if for some s G{1,...,p(Ck)} we have 
Ck(s) = k((s + p(Ck)) = • • • = c((s + (M/p(Ck) - 1)p(Ck)) = i, then 
li G{s, s + p(Ck),...,s+(M/p(Ck) - 1)p(Ck)}. (Here the rth 
repetition of c,, with r G {1,..., M/p(Ck)}, is meant to be the 
sequence (c( (1 + (r - 1) p (Ck)), ck (2 + (r - 1) p (Ck)), ...,
Ck(p(Ck) + (r - 1)p(Ck))).) 
(3.4.3)
(Recall that the circuits and the corresponding cycles are understood as 
equivalence classes according to Definitions 1.1.2 and 1.1.3 of Part I). Then 
S = {S1,...,Sn} is a rotational partition of [0, 1) associated to P with 
respect to the shift f1/M. The partitioning sets S1,...,Sn are defined by 
three procedures. One is the labeling procedure of the intervals Aki, the 
second is the labeling of the components of the unions Si , and the third 
is the definition of the intervals Aki on the line. The labeling (3.4.2) of 
the intervals Aki and the labeling (3.4.3) of the components of each Si are 
topological procedures since they depend only on the connectivity relations 
of the graph of P.
Before leaving these investigations, let us notice that any point i G 
{1,...,n} appears along the circumference of the circle c if and only if 
there are some circuits Cj1 ,...,Cjm of the decomposition (3.4.1) which pass

244 
3. The Rotational Theory of Markov Processes
il 
1 
mi 
-11 
714-/ / 
\ i • 
1 
il 
rV 714-/ / 
\
through i. Then i will appear M/p(cj1 ) times along the arc Uj1 , M/p(cj2 ) 
times along Uj2 , and finally M/p(cjm ) times along Ujm . Accordingly for 
each rotational partition S = {Si} the number 6(j) of all the components 
Akl of Sj defined according to the labeling (3.4.3) is equal to the number 
of all the appearances of the point j on the circumference of the circle c.
Let 6 = 6(S) = maxj 6(j). Then 6 is a topological feature of S, that 
is, 6 depends only upon the connectivity relations of the chosen collection 
{c1 ,...,cN} of the circuits decomposing the original matrix P and does 
not depend on the circuit-weights. Furthermore, 6 is independent of the 
ordering of the decomposing circuits as well as of the choice of their starting 
points. The study of 6 will be given in the subsequent section 3.6.
3.4.2. Let us investigate the indexing procedure (3.4.2) and the labeling 
(3.4.3) for the concrete Example 3.3.1 of the previous section. We start 
with two choices: one choice is concerned with the ordering of the class­
circuits in the set S = {1, 2, 3} which occur in the decomposition (3.3.10), 
say c1, c2, c3 as in Figure 3.3.2, and a second choice with the representatives 
of the class-circuits c1 ,c2 ,c3 , that is, we fix a starting point for each circuit. 
Here we choose c1 =(1, 1), c2 =(1, 2, 1) and c3 =(1, 2, 3, 1).
Partition the circumference of a circle c into three equal arcs U1 ,U2 ,U3 
each assigned to one circuit of the ordered sequence c1 ,c2 ,c3. In turn, parti­
tion U1 into 6 equal directed arcs and assign each of these arcs to an edge of 
the 6/p(c1) = 6 copies (c1(1),c1(2)), (c1(2), c1(3)),..., (c1(6), c1(7)) of the 
circuit c 1 = (c 1( n), c 1( n + 1)) = (1, 1), n G Z .In this way we can denote the 
arcs of U1 by a 11, a 12,..., a 16, where the first index 1 is related to the corre­
spondence c 1 ^ U 1 while each of the second indices 1, 2,..., 6 is the rank of 
a starting point of an edge in the sequence (c1(1), c1(2)),..., (c1(6), c1(7)); 
then, the second index counts the edges of the 6 copies of c1 (see Figure 
3.4.1). This being so, we have now assigned the edges of c1 to the 6 arcs 
a11 , .. .,a16 of the circle c. Furthermore, we assign the point 1 of the cycle 
c1 = (1) to the starting points of a 11,..., a 16. Accordingly, we symbolize 
the starting points of these arcs on the circle c by 1.
Analogously, partition U2 into 6 equal directed arcs and put them 
in a one-one correspondence with the 6 edges of the 6/p(c2) = 3 
copies (c2(1), c2(2), c2(3)), (c2(3), c2(4), c2(5)), (c2(5), c2(6), c2(7)) of c2 = 
(c2(n),c2(n + 1),c2(n + 2)) = (1, 2, 1),n G Z. Denote the arcs of U2 by 
a21, a22, a23, a24, a25, a26 and their starting points by 1, 2, 1, 2,...,1, 2 
(which are the starting points of the edges of c2 = (1, 2, 1) when c2 is re­
peated 3 times).
Notice that we have the one-one correspondence (c2(s), c2(s + 1)) ^ a2S, 
that is, s denotes the ranks of the starting points c2 (s) of the 6 edges 
in the 3 copies of c2 . In this way we have assigned the edges of c2 to 
the arcs a21,..., a26 of the circle c, and the points of c2 to the starting 
points of a21,...,a26. Finally, partition U3 into 6 equal circle arcs, denoted 

3.4 Transforming Circuits into Circle Arcs 245
a31 ,a32,...,a36. Assign each edge (c3(s),c3(s + 1)) of the 6/p(c3) = 2 
copies of c3 = (1, 2, 3, 1) to the circle-arcs a3S, s = 1,..., 6. Put 1,2,3,1,2,3 
as symbols for the starting points of a31 ,...,a36 .
It happens that the circuit c2 appears 3 times along c as well, but this 
is not the general case. For instance, if we would choose the sequence (2, 
1, 2) to represent the circuit c2, then it is only the cycle (2, 1) which is 
repeated 3 times in U2. Let S = {S1, S2, S3} be the rotational partition of 
P, with respect to the shift f1/6, provided in Example 3.3.1, and let Akl 
be the corresponding component-sets indexed according to (3.4.2). Also let 
Uk = t6=1 Akl,k = 1, 2, 3.
Replacing the labeling (3.3.6) of Theorem 3.3.1 by (3.4.3), the sets Si 
will be defined as
Si = ( )Akl, i=1,2,3,
where (k, l) is the index of any arc akl on c which starts at i as in Fig­
ure 3.4.1. Then
Figure 3.4.1.

246 
3. The Rotational Theory of Markov Processes
S1 
contains the intervals:
A11, A12, A13, A14, A15, A16 in U1.
A21, A23, A25 in U2, 
A31 ,A34 in U3 .
S2 
contains the intervals:
A22, A24, A26 in U2, 
A32, A35 in U3.
S3 
contains the intervals:
A33, A36 in U3.
Figure 3.4.2.

3.5 Mapping Stochastic Matrices into Partitions 247
mi 1 
1 Z 
c A 
1 
• 1 
• 
• T~1 • 
n a r-k
The homeomorphs Akl of Akl along a circle are given in Figure 3.4.2.
Furthermore,
A (S1 A f-/6(S 1)) = A (A11) + A (A 12) + A (A 13) + A (A 14) + A (A 15) + A (A 16) = 72,
A(S1 A f1/6(S2)) = A(A21) + A(A23) + A(A25) + A(A31) + A(A34) = 77,
A(S2 n f1/6(S1)) = A(A22) + A(A24) + A(A26) = 77,
A(S2 n f1/6(S3)) = A(A32) + A(A35) = 7,
A(S3 A f--/6(S1)) = A(A33) + A(A36) = 7 .
3.5 Mapping Stochastic Matrices into Partitions 
and a Probabilistic Solution to the
Rotational Problem
The rotational problem (R), quoted in Section 3.2, gives rise to prototypes 
of questions which, suitably reformulated in other contexts, more abstact, 
reveal their depth with greater clarity. One, to which we shall return in 
the following chapters, is of considerable importance. Suppose we view 
the correspondence i ^ Si between states 1, 2,... ,n of a recurrent Markov 
chain £ with prescribed transition matrix P, and partitioning sets Si of a 
rotational system as a coding process.
This idea is much better translated in the context of dynamical systems 
as a coding problem as follows: code a given A-state stationary stochastic 
process n = (nn)X= -x, where (A, A) is a measure space, onto an N-state 
stationary process £ = (£n)n whose conditioned probabilities Prob (£n+1 = 
j/£n = i) are previously specified by a stochastic matrix P (see J.C. Kieffer 
(1980), and S. Alpern and V. Prasad (1989)). N denotes the set of nonneg­
ative integers. Suppose that P admits an invariant probability distribution 
n. Consider the associated dynamical system (Ax, Ax,^,r) to n, where 
Ax is the doubly infinite sequence space, Ax is the product a-field, p, is the 
joint distribution of {nn}, and t denotes the left shift transformation de­
fined on Ax by (t(s))n = sn +1. Since n is stationary, p, is preserved by the 
shift t .
The coding function will be thought of as a measurable function k : Ax ^ 
N so that £j = k(tj(ni: -^ < i < <x)) for each j. Each coding function k 
corresponds to a measurable partition {Si}ieN of Ax with Si = k- 1({i}), 
and conversely. The coding problem has a version for each n x n recurrent 
stochastic matrix P = (pij, i,j = 1,... ,n), n > 1, expressed by the rota­
tional problem (R) where it is required to find a circle rotation t and a circle 
partition {Si }.

248 
3. The Rotational Theory of Markov Processes
Let us point out that the relation P ^ {S,} which arises in the approach 
of the previous section to the rotational representation is in general a one- 
to-many relation since it depends on the following variables:
(a) the length of the rotation;
(P) the ordering of the cycles (in the complete graph):
(y) the starting points of the cycles;
(8) the rotational labeling; and
(£) the representative class (C wc) of cycles (or circuits) and weights pro­
vided by a cycle decomposition algorithm on P and on a chosen strictly 
positive invariant probability (row) distribution n (i.e., an algorithm 
which expresses nP by a linear combination of the passage functions 
Jc, c G C, having wc as positive scalars).
(The cycles are considered according to Definition 1.2.1 to have distinct 
points.) When the approach to the rotational representation is defined in­
dependently from a cycle decomposition of P, then the ingredients (P), (y) 
and (£) are not considered.
Throughout this section the rotational-representation-procedure will be 
that of the previous section. Accordingly, the rotational labeling will be 
defined by (3.4.2) and (3.4.3). If we agree from the beginning that for a 
given n > 2, the rotational length to be considered is 1 /n!, and that the 
ordering of the cycles and the starting points of the cycles will be originally 
chosen, it will nevertheless be necessary to investigate the existence of a 
uniqueness criterion for {S,} based on the variable (e). This being formu­
lated so, the problem in question is the following: With the above variables 
(a), (P), (y) and (8) fixed, find a criterion on the variable (e) which assures 
the existence of a one-to-one mapping $ from the set of irreducible n x n 
stochastic matrices P into n-partitions {S, }. In other words, we have to fix 
a vector-solution of cycles and weights of an algorithm providing a cycle­
decomposition-formula for each irreducible n x n stochastic matrix P.
Before answering, let us notice that for an irreducible Markov chain £ the 
transition matrix P =(p,j,i,j =1,...,n) and the invariant probability dis­
tribution n = (n,, i = 1, 2,... ,n) determine uniquely the edge-distribution 
E = (niPj, i,j = 1,..., n) and the cycle-distribution C-^ = (p(c)wc, c G 
C.-x_) attached to the circulation distribution {wc}, and conversely. Here for 
both edges and cycles we have initially considered some orderings. Then we 
may view £ either as a vector with respect to the referential system of edge­
axes, or as a vector with respect to the referential system of cycle-axes (see 
S. Kalpazidou (1995)).
On the other hand, given £, Theorem 3.3.1 provides a collec­
tion of rotational representations {Ra}a with Ra = (1 /n!, {aSf}) whose 
edge-distribution Ea = (A(aS, A ft 1 (aSj)),i,j = 1, ...,n),t = 1 /n!, are 
all identical to E, but whose cycle-distributions Ca = (awc, c G Ca) are 
distinct from the above cycle-distribution Cx. The explanation is sim­
ple: the cycle-distributions Ca are defined by a nonrandom algorithm with 

3.5 Mapping Stochastic Matrices into Partitions 249
many solutions of cycles and weights in the cycle-decomposition-formula. 
Obviously, under the assumption that the variables (a), (0), (7) and (S) 
are fixed, the rotational representation will be unique if we fix a cycle­
distribution, as for instance the above C-x_ which is the unique solution of a 
probabilistic algorithm. The following theorem, adapted from the paper of 
S. Kalpazidou (1994b), gives a detailed answer to this question along with 
a probabilistic solution to the rotational problem.
Theorem 3.5.1. (A Probabilistic Solution to the Rotational Problem). 
Given n > 2, for each ordering providing all the possible cycles in S = 
{1, 2,...,n} and for each choice of the representatives of these cycles there 
exists a map $ from the space of n x n irreducible stochastic matrices P 
into n-partitions S = {S1,...,Sn} of [0, 1) such that the rotational repre­
sentation process defined by (ft, {Si}) with t = 1/n! and {Si} = $(P) has 
the same transition probabilities and the same distribution of cycles as the 
probabilistic cycle distribution of the Markov process on P.
If the measures of the component sets of Si converge, then the sequence 
of partitions converges in the metric d defined as
d (^, ^) = 
A (Si + S'), 
(3.5.1)
where A denotes Lebesgue measure on Borel subsets of [0, 1), and + denotes 
symmetric difference.
Proof. We first appeal to the probabilistic cycle representation of Theorem 
3.3.1 of Part I according to which any irreducible stochastic matrix P is de­
composed in terms of the circulation distribution (wc, c G Cx) (introduced 
by Definition 3.2.2 (of Part I)) as follows:
ni pij = £ WcJc(i,j), i,j G S.
c^C^
where n = (ni, i G S1) denotes the invariant probability distribution of P, c 
is the cycle attached to the circuit c, and Jc is the passage-function of c.By 
hypotheses, we have chosen an ordering for the cycles of C-x_ and a starting 
point for each of them. So, let C-x_ = {<7,..., cs}, s > 1.
Let us replace the passage-functions Jck by the circuit-matrices Cck = 
(1/p(ck))Jck ,k =1,...,s, where p(ck) denotes as usual the period of ck. 
Next we shall follow the procedure to the rotational partition of the previ­
ous section starting with the probabilistic decomposition
s
niPij 5Z( P (ck) wck) Cck (i,j), i,j = 1 ,...,n. 
(3.5.2)
k=1
Accordingly, relations (3.5.2) will replace the Caratheodory-type decompo­
sition (3.4.1) in the labeling (3.4.3).

250 
3. The Rotational Theory of Markov Processes
Let t = 1/n! and let ft be the shift defined by (3.2.1). Then there will 
exist a rotational representation (t, {Sj}), where the pairs (k,l) occur­
ring in the expressions of the partitioning sets Si = (k,l) Akl, i =1,...,n 
are given by labeling (3.4.3), and the Akl = ftl-1 (Ak),k=1,...,s; l = 
1,...,n!, are provided by the partition (Ak,k=1,...,s)ofA =[0, 1/n!) 
whose relative distribution (A (Ak)/X (A), k = 1,... ,s) matches the proba­
bilistic cycle distribution (p(ck) wck ,k=1,...,s). Therefore we have
A(Akl) = (1/n!)p(ck) wck, k=1,...,s, l=1,...,n!.
Then the uniqueness of the lengths of the intervals Akl follows from that 
of the cycle-weights wck in the algorithm of Theorem 3.2.1 of Part I. This, 
added to the assumptions that we have chosen an ordering and the starting 
points of the cycles in (3.5.2), will assure the uniqueness of the partition 
{Sj}.
Accordingly, for any fixed n > 2 there exists a map $ which assigns 
to each n x n irreducible stochastic matrix P a partition S= {Sj,i= 
1,.. .,n} of [0, 1) such that the rotational representation process defined by 
(ft, S )witht =1/n!andS = $(P) has the same transition probabilities 
and the same distribution of cycles as the probabilistic cycle distribution 
of the Markov chain on P.
Finally, if we endow the set of all partitions of [0, 1) with the metric d 
defined by (3.5.1), the convergence in metric d of any sequence of partitions 
{nSj} will follow from that of the measures of the component-sets of nSj . 
The proof is complete. 
□
3.6 The Rotational Dimension of Stochastic 
Matrices and a Homologic Solution 
to the Rotational Problem
3.6.1. The map $ occurring in Theorem 3.5.1 is expressed by two one- 
to-one transformations $1 and $2 . Specifically, $1 acts from the set of 
n x n irreducible stochastic matrices into the set of pairs of ordered classes 
(Cx, C'-K_) where C'-K_ = (p(ck)wgk, k = 1,... ,s) is the unique probabilistic 
cycle distribution assigned to the class C^ = {ci,..., cs} of cycles accord­
ing to Theorem 4.1.1 of Part I. Also, $2 assigns each pair (C^, (X,_) to an 
n-partition S = {Si,...,Sn} of [0, 1).
In general, $i becomes a one-to-many relation if we relax the assump­
tions in Theorem 3.5.1 For instance this happens if the cycle-representation- 
algorithm varies, or if the ordering on the class of the circuits is changing 
(i.e., the referential system of the cycle-axes varies). It is this case that we 
shall consider, extending for a moment the domain of $i to the class of all 
n x n recurrent stochastic matrices.

3.6 The Rotational Dimension of Stochastic Matrices 251
Let S = {1, 2,..., n}, n > 2, and let P = (pij, i, j G S) be a recurrent 
stochastic matrix. Then, choosing an invariant strictly positive probabi­
lity distribution n = (ni, i G S) of P, a system of cycle-axes and a circuit­
representation-algorithm, one obtains an ordered class of circuits C = 
{c1,...,cs},s > 1, and a row vector
C = (p(c1)wc1,..., p(cs)wcs) 
(3.6.1)
which decompose P by equations
s
nipij 
(p(ck)wck)Cck(i,j), i,jG S, 
(3.6.2)
k=1
where wck > 0,k =1,...,s; p(ck) denotes the period of ck, and Cck is the 
circuit-matrix associated with ck .
Once the pair (C, C) is chosen to represent the matrix P by equations 
(3.6.2), the above transformation $2 is concerned with an assignment
(C, C) ^ ({Aki}, {A(Aki)}) 
(3.6.3)
from the circuits of C and numbers of C into the circle-arcs {Akl} and 
numbers {A(Aki)} defined according to the labeling (3.4.2) and Theorem 
3.5.1, where the shift ft is defined by (3.2.1) with t =1/M and M in equal to 
the least common multiple of (2, 3,...,n)(A symbolizes Lebesgue measure). 
Then, for each choice of the starting points of the circuits, the sets
Si = Aki, i=1,...,n, 
(3.6.4)
with the unions indexed by the pairs (k, l) which are assigned to each i ac­
cording to the labeling (3.4.3), form a partition S = {S1,...,Sn} of [0, 1). 
Furthermore S along with t =1/M define a rotational representation of P. 
When either n or s is a large number, the corresponding rotational partition 
S will contain a vast number of components {Aki} and the construction of 
Si,i=1,...,n, will become very complicated. This motivates our interest 
in rotational partitions with a minimal number of components {Aki}.
In this section we shall examine the rotational representations with small 
numbers of components {Aki} in the descriptions (3.6.4) of the partition­
ing sets Si,i=1,...,n. The approach is adapted from the paper of S. 
Kalpazidou (1995).
Throughout this section we shall consider the rotational partitions S = 
{S1 ,...,Sn} according to Theorem 3.5.1 where the rotational length is 1/M 
with M equal to the least common multiple of (2,...,n), the components 
Aki in the unions Si are labeled by (3.4.3), and the representation algorithm 
and the corresponding collection (C, C ) of circuits and weights vary. The 
detailed exposition of the procedure for this type of rotational partitions 
was given in Section 3.4. As we have already seen in Chapter 4 (Part 
I), there are many algorithms which provide more than one solution of 

252 
3. The Rotational Theory of Markov Processes
representative circuits and weights. Consequently, the pair (C, C) varies in 
equations (3.6.2).
For a fixed representative pair (C, C)ofP let:
6(i) denote the number of the components Aki of Si, i = 1, ..., n, defined 
according to labeling (3.4.3).
Then, as we have seen in the previous Section 3.4, 6(i) depends only on C, 
that is 6(i) is a topological feature of Si which depends neither on the 
ordering of C nor on the starting points of the circuits of C.Itisto 
be noticed that, if i is passed by a single circuit c of period p(c), then 
6 (i) = M/p(c), but when there are more than one circuit c passing through 
i, then 6(i) = c(M/p(c)). Hence 6(i) depends on the number s of the 
representative circuits in the decomposition (3.6.2) and on the connectiv­
ity relations of C.
Let 6 = 6(s, C) = maxi=1,...,n 6(i). Then the number of components Akl 
of each Si,i=1,...,n, is less than or equal to 6.Wecall6 the length of 
description of the partition S = {Si,i=1,...,n} associated with C.
In general when the collection C is dissociated from any matrix and 
refers to an arbitrary graph, we shall call 6(s, C) the rotational length of 
description on C. Then there exists a pair (s0, C0), which provides the 
minimal value for 6, when the representative class (C, C) varies in equations 
(3.6.2). Let D(P) = 6(s0, C0) = minsC6(s, C). We call D(P) the rotational 
dimension of P (S. Kalpazidou (1995)). Analogously, one may define the 
rotational dimension of a finite oriented graph.
Now we shall be concerned with the homological characterization of the 
rotational representations of P and of the corresponding rotational lengths 
of descriptions via the Betti circuits. To this end, we shall consider, for the 
sake of simplicity, only irreducible stochastic matrices on S and we shall 
define the Betti circuits as in Chapter 4 of Part I.
3.6.2. Before proceeding to our main task let us scrutinize the definition 
of the rotational dimension of P. Specifically, when the circuit decomposi­
tion (3.6.2) is chosen to be the probabilistic one (provided by Theorem 3.3.1 
of Part I), the circuits {ck} and the weights {wck} are uniquely determined 
by a sample-path description. This probabilistic criterion enables us to gen­
eralize the rotational dimension to semigroups of stochastic matrices with 
continuous parameter. A detailed argument of the rotational dimension of 
semi-groups of finite stochastic matrices will be given in Section 3.9.
On the other hand, one may characterize an irreducible stochastic matrix 
P as “chaotic” in the spirit of Kolmogorov if the connectivity relations of 
the graph G(P) of P are complex enough. Then the Betti number of the 
graph G(P) should be the maximal one.
It turns out that for a given n > 1 the largest Betti number of all the 
connected oriented graphs on {1, 2,...,n} is n2 - n + 1. Then there is an 
irreducible stochastic matrix on {1, 2,...,n} whose graph has the Betti 
number n2 - n + 1. In this case, the homological dimension of Betti is 

3.6 The Rotational Dimension of Stochastic Matrices 253
equal to the algebraic dimension of Caratheodory, which is n2 — n + 1 as 
well (see Definition 4.5.3 of Part I).
3.6.3. We shall now consider the homological approach of Sections 4.4 and 
4.5 (Chapter 4) of Part I. Accordingly, let G = G(P) = (B0(P), B1(P)), 
B = B(P), and r = r(P) = {7 1,... ,1b} denote, respectively, the graph of 
a given irreducible stochastic matrix P = (pij i, j =1, 2,...,n), the Betti 
number of G, and an arbitrarily chosen base of B directed circuits of G 
called Betti circuits. Here the B0(P) and B1(P) denote the set of points 
and the set of directed edges of G endowed, respectively, with an ordering. 
Then G is a strongly connected oriented graph where strong connectedness 
is understood as in Section 4.4 of Part I. The Betti one-cycle associated 
with a Betti circuit 1 will be symbolized by 1 •
Then, following the same reasoning of Theorem 4.5.1 and Remark 4.5.6 
of Part I, we have:
Theorem 3.6 .1. Any irreducible stochastic matrix P = (pij ,i,j = 
1, 2,...,n) has a circuit decomposition in terms of the Betti circuits 
y1 ,.. .,yB , that is,
_ B
^p^iPij b (i,j) = ^2 w Yk 1 k , 
b (i,j) € B 1( P), 
w Yk € R, 
(3.6.5)
or, in terms of the (i, j)-coordinates,
B
niPij ^^2 w Yk JYk (i,j), w Yk € R; i,j = 1, 2 ,---,n, 
(3.6.6)
k=1
where n = (n 1,.. . ,nn) denotes the invariant probability distribution of P 
and JYk is the passage-function associated to yk . Furthermore, the circuit­
weights w2Yk are given by equations.
w Yk = 
a (c,1k) wc, 
a (c,1k) € Z, k =1, 2 ,...,B,
cEC
where the collection {C, wc} of circuits and weights is a circuit repre­
sentation of P given by either randomized algorithms or non-randomized 
algorithms.
(Here Z, R denote as always the sets of integers and reals respectively, 
and a circuit decomposition is understood as in Chapter 4, of Part I.) Any 
decomposition of P in terms of the Betti circuits is called a Betti-type 
circuit decomposition of P. For instance, equations (3.6.6) provide such a 
decomposition (see also Section 4.5 of Part I).
In Remark 4.5.2 (Chapter 4) of Part I we have discussed how to obtain 
positive weights w2Yk in the decompositions (3.6.6) above (since w2Yk can 
be negative numbers). From this standpoint one may obtain a method of 

254 
3. The Rotational Theory of Markov Processes
construction of finite stochastic matrices which admit Betti-type decompo­
sitions with positive scalars. Furthermore in Figure 4.4.1 we have illustrated 
a strongly connected oriented graph where any B circuits are Betti circuits 
(here B denotes the Betti number of the corresponding graph).
With this preparations, we now prove
Theorem 3.6 .2. (A Homologic Solution to the Rotational Problem). Let 
n > 2 and S = {1, 2, ...,n}. Consider G a strongly connected oriented 
graph on S whose Betti number is B. Then for any irreducible stochastic ma­
trix P, which has G as its graph and a positive Betti-type circuit decomposi­
tion, there exists a rotational representation in terms of the Betti circuits.
Furthermore, if any B circuits of G are Betti circuits, then each of 
the lenghts of description of the rotational partitions associated to any 
irreducible stochastic matrix with the graph G is greater than or equal 
to the length of description on a collection {71 ,...,yb } of Betti circuits 
whose graph is G, where B < B.
Proof. Let G be a graph as in the first assumption of the theorem. Then 
we shall apply Theorem 3.6.1 to the irreducible stochastic matrices on 
S with the same graph G. Accordingly, let P = (pij,i,j G S) be such a 
matrix which, in addition, admits a Betti-type decomposition (3.6.6) with 
respect to a base {y 1 ,...,Yb} of Betti circuits of G where the weights 
wYk, k = 1,..., B, are positive. Then equations (3.6.6) can be written in 
the form
B
niPij ^2 P (Yk ) w Yk ) CYk ( i,j ), w Yk > 0, k =1 ,...,B; i,j = 1 ,...,n, 
k=1
(3.6.7) 
where CYk = (1 /p(Yk)) JYk, k = 1,■■■, B, and n = (ni, i G S) denotes the in­
variant distribution of P, (p(yk)) symbolizes as always the period of yk, and 
JYk the second order passage-function of yk ).
Let us further assume that the weights wjYk are strictly positive. Put t = 
1/M, where M =l.c.m.(2,...,n). Then we may start labeling (3.4.3) with 
the decomposition (3.6.7) and with the shift ft defined by (3.2.1). Next, 
partition the interval A =[0, 1/M) into B subintervals A1,A2,...,AB such 
that the relative distribution (A (Ak)/X (A), k = 1,... ,B) matches the dis­
tribution (p(yk)wjYk ,k =1,...,B), that is,
A(Ak)=(1/M)p(yk)wjYk, k=1,...,B,
where A symbolizes Lebesgue measure. Define Akl = ftl-1(Ak) for k = 
1,..., B, and l = 1,..., M. Then for each choice of the starting points of 
yk ,k =1,...,B, the sets Si = Akl, i =1,...,n, whose components Akl 
are labeled by (3.4.3), provide a rotational representation (1/M, S(P)) of 
P. Since the previous approach relies upon homologic arguments we shall 
call (1/M, S (P)) a homologic solution to the rotational problem.

3.7 The Complexity of the Rotational Representations 255
Further, suppose that any B circuits of G are Betti circuits. Let P be an 
irreducible stochastic matrix whose graph is G and let 8(s, C) be a length of 
description on a collection C of directed circuits which decompose P 
by a circuit-decomposition-formula. Then we may find a collection r = 
{7 1,... ,yb }, with B < B, of Betti circuits in C such that the associated 
graph with r is G . Furthermore the length of description 8(B, r) on f sat­
isfies 8(B, r) < 8(s, C). For instance, we have B < B when certain weights 
wYk are zero in equations (3.6.7). The proof is complete. 
□
3.7 The Complexity of the Rotational
Representations
We have seen in the previous section that, if P is an irreducible matrix 
on {1, 2,...,n} whose graph G = G(P) is the complete directed graph, 
then three characteristics, which in general are irreconcilable, come into 
a condition of compatibility. These characteristics are the Betti number 
of P (a topological invariant), the Caratheodory dimension (an algebraic 
dimension) and the property of being “chaotic” approached in the spirit 
of Kolmogorov. The Betti dimension and the Caratheodory dimension are 
introduced in Section 4.5 (Chapter 4) of Part I.
Consider the maximal rotational dimension of P when P varies in the set 
of all n x n recurrent stochastic matrices. Another way to approach this 
concept was initiated by S. Alpern (1983). The present section, as well as 
the next one, deals with this approach as was developed by S. Alpern, J. 
Haigh, P. Rodriguez del Tio, and M.C. Valsero Blanco.
We first start with a definition. A rotational partition S = {Si ,i = 
1, 2, . . .,n} has the type L if the number of components of each Si is less 
than or equal to L, i =1,...,n.LetD = D(n) be the least integer such 
that every n x n recurrent matrix has a rotational representation of type 
D, that is, a representation (t, S ) where S is of type D. Then D depends 
on the definition of the labeling of the components of the partitioning sets 
S1 ,.. .,SN . In Sections 3.3 and 3.4 there were presented two approaches 
to the rotational partition. Now the ingredient D will be investigated in 
the context of Alpern’s approach exposed in Section 3.3. To obtain a lower 
bound of D(n) we first prove the following lemma due to S. Alpern (1983).
Lemma 3.7.1. Let ck,k =1,...,r, be positive integers and let n =1+ 
c 1 + c2 + • • • + cr. Let Q = Q (c 1,. .., cr) be an n x n permutation matrix 
with cycles of lengths 1,c1,c2,...,cr. Then, if Q is represented by (t, S ), 
the type of S is at least l.c.m.(c1,..., cr) ( l.c.m. symbolizes as always the 
least common multiple).
Proof. Let 1 be the label of the 1-cycle of Q so that q11 = 1. Then, if f 
denotes ft the set S1 is invariant under f. It follows from Weyl’s well-known

256 
3. The Rotational Theory of Markov Processes
theorem (see P.R. Halmos (1956)) that t is rational (irrational rotations are 
ergodic—have no nontrivial invariant sets). Let t = p/q in lowest terms, so 
that every point in [0, 1) has f-period q. The invariant set S1 consequently 
consists of at least q intervals and hence the type of S is at least q.To 
estimate q from below, observe that, if a point x belongs to Si where the 
index i belongs to a Q-cycle of length ck then the f-period of x must be 
a multiple of ck . But the f-period of every x is q,soq is a multiple of ck . 
Hence q > l.c.m. (c 1,..., cr). 
□
A general estimation of D(n) is given by S. Alpern (1983) as follows:
Theorem 3.7.2. There exist positive constants a and fi such that for 
all n,
exp(an1/2) < D(n) < exp(fin).
(3.7.1)
Proof. We shall need the following notation and estimates due to Landau 
(1958) (pp. 89-91). Let pk denote the kth prime (p 1 = 2) and let n(n) 
denote the number of primes less than or equal to n. A partial result in the 
direction of the Prime Number Theorem (due to Chebyshev) asserts the 
existence of a positive constant fi1 such that
n(n) < fi 1 / log n.
Let d(k, n) be the largest integer power d such that pd < n. Then for any 
even n,
n (n)
2n/2 < l. c. m. (2, 3 ,...,n) = £[ pdk(k,n) < nn (n). 
(3.7.2)
k=1
We can now proceed with the proof proper, beginning with the up­
per bound. The algorithm presented in the proof of Theorem 3.3.1 rep­
resents any recurrent n x n matrix by (t, S) where S is composed of 
intervals Akl,k=1,...,N,l =1,...,M,whereN < n2 - n +1 and M = 
l.c.m.(1, 2, 3,...,n). Consequently, we have that
D(n) < NM < n2 l.c.m.(1, 2,...,n). 
(3.7.3)
If we combine (3.7.3) with (3.7.2) and take logarithms we get, 
log D (n) < 2 log n + n (n) log n
< log n(2 + fi1n/ log n) 
(3.7.4)
< log n(fin/ log n)
= fin,
where fi is some positive number larger than fi1 .
To obtain the lower bound, fix any even m and define ck 
for k = 1,..., n (m). Let n = nm = 1 + n= nk =1)
d(k,1) 
= pk
ck < m2 . Then we apply

3.7 The Complexity of the Rotational Representations 257
Lemma 3.7.1 to the permutation matrix Q = Q(c 1,..., cn(m)), obtaining 
n (m)
D (nm) > l. c. m. (c 1,..., Cn (m)) = 
Pdk (k,m) > 2 m/2. 
(3.7.5)
k=1
Since nm < m2 and D(n) is nondecreasing, (3.7.5) implies
D(m2) > 2m/2
and hence
D(m) > 2m1/2/2
or
D(m) > exp(am 1 /2),
where a = log 21 / 2. 
□
The estimation (3.7.1) for D(n) is based on the labeling (3.3.6) (Section 
3.3 of Part II) for the component intervals Akl of the partitioning sets 
Si,i =1,...,n. The change of the labeling will naturally imply the change 
of the estimation of D(n). We now present other labelings along with the 
corresponding estimations for the D(n).
A first labeling is given for n = 2 by J.E. Cohen in the course of the proof 
of Thorem 3.2.1. Consequertly we have D(2) = 1. Next, the generalization 
D(3) = 2 is due to J. Haigh (1985) using the following labeling. Let P be 
any 3 x 3 recurrent matrix with invariant probability distribution n. Then 
we have
n = n 1 p 1 i + n2p2i + n3p3i, 
i = 1, 2, 3, 
(3.7.6)
ni = nip 1 + niPi2 + nipi3, 
i = 1, 2, 3. 
(3.7.7)
Equating the two expressions for ni in (3.7.6) and (3.7.7), we have
n2p21 - n 1 p 12 = n 1 p 13 - n3p31 = n3p32 - n2p23 = v. (3.7.8)
It turns out to be more convenient to define ft(x) = (x + t) (mod 2), and 
use the interval [0, 2) instead of [0, 1), so that A(Si) = 2ni. We now specify 
the division points x1, x2 ,...,x5, that partition [0, 2) into the six intervals 
in the order shown in Figure 3.7.1, by fixing the lengths of these intervals.
We choose
A(Ai 1) = 2ni - npii, 
A(Ai2) = mpu,
I ^11 
^22 
A31 Aj2 Ajj Aj2
Xo = O X[ X2 x3 x4 x5 X6=2s0
Figure 3.7 .1.

258 
3. The Rotational Theory of Markov Processes
x-o = o y3 y4 *1 
ys y0 s *4 y3 Vi *5 x6 = 2
Figure 3.7 .2.
and we define t =1- v, where v was introduced by (3.7.8). Then, writing 
ft (xi) = yi , we can easily calculate the values of y0,...,y5, and verify that 
these points are juxtaposed with x0,...,x6 as shown in Figure 3.7.2 (i.e., 
that x0 < y3, y4 < x 1, etc.) and that:
x 3 - y 0 = 2 n 1 p 13, 
x 1 - y 4 = 2 n 2 p 21, 
x 5 — y 2 = 2 n 3 p 32,
y 1 - x 4 = 2 n 1 p 12, 
y 5 — x 2 = 2 n 2 p 23, 
y 3 — x 0 = 2 n 3 p 31.
(3.7.9)
Hence, if Si = Ai 1 U Ai2, then A(Si) = 2ni and relations (3.7.9) show that 
pij = A(ft(Si) C Sj)/A(Si) for i = j; but since S = {S 1, S2, S3} partition 
[0, 2), the previous equation holds for {pii} as well. Therefore we have 
proved that any 3 x 3 recurrent matrix has a rotational representation 
(t, S), with t and S= {S1, S2, S3} defined above, where each Si is a union 
of at most two intervals.
The particular cases D(2) = 1 ,D(3) = 2 cannot be extended to n > 4. 
The above Alpern’s Lemma 3.7.1, which disproves that D(n)=n — 1for 
n > 4, uses a matrix corresponding to a reducible Markov chain with cyclic 
classes of sizes 1,c1 ,c2,...,cr . Thus, if we partition any positive integer m 
as m = c 1 + c2 + • • • + cr, and define
H(m) = Max{l.c.m.(c1, c2,..., cr): all partitions},
Alpern’s result implies that D(n) > H(n — 1). Furthermore, J. Haigh pro­
poses the following conjecture:
D(n)=H(n— 1).
Here are some values:
n 234567 
8 
9 
10111213
H(n- 1)123466121520303060
Also, J. Haigh (1985) proves the following:
Theorem 3.7.3. We have D(n) > D(n — 1).
Proof. Let P0 be some (n — 1) x (n — 1) recurrent matrix in which any 
rotational representation requires some Si to contain D(n — 1) intervals. 
Let P1 be the n x n matrix whose principal submatrix is P0, and pnn =1. 
Suppose that A(Sn) = a and that P1 has a representation in which every 

3.8 A Reversibility Criterion in Terms of Rotational Representations 259
Si is the union of at most r intervals. Since ft (Sn) = Sn , we see that the 
intervals in Sn can be split into families, each family consisting of equally 
sized intervals whose left endpoints are a multiple of t = p/k apart.
Remove this Sn from [0, 1), coalesce the remaining intervals, and define 
t = p(1 — a)/k; this gives a representation of P0 on [0, 1 — a), using at most 
r intervals, so r > D(n — 1); but D(n) > r, so D(n) > D(n — 1). 
□
3.8 A Reversibility Criterion in Terms
of Rotational Representations
The rotational representations were originally given in Theorem 3.2.1 for 
the case where the stochastic matrices are 2 x 2 irreducible matrices. These 
matrices have the property of being reversible matrices. As is known, a 
recurrent stochastic matrix P =(pij,i,j =1,...,n),n > 1, is reversible if 
nipij = nj pji, i,j = 1,..., n, where n = (ni,...,nn) denotes an invariant 
probability row-distribution. Define R =(rij,i,j =1,...,n), where rij = 
nipij. Then P is a reversible matrix if and only if R is a symmetric matrix.
In this section we investigate the rotational representations of n x n re­
versible recurrent matrices according to the approach of P. Rodriguez del 
Tio and M.C. Valsero Blanco (1991). One result will be a reversibility cri­
terion for finite recurrent Markov chains in terms of rotational partitions.
As already seen in the previous sections the type of a rotational partition 
depends on the way the labels are assigned to each subinterval. If R is a 
symmetric matrix, then it is a convex combination of n cycle matrices 
(defined by relations (3.3.1) of this chapter) of length one and n(n — 1)/2 
cycle matrices of length two, so the rotation can be taken to be 180°, that 
is t =1/2 in the definition (3.2.1) of the shift ft. In this case the cycles 
and the labels can be reordered to get the labels grouped at least in pairs 
except, perhaps, one of them. As two or more contiguous subintervals with 
the same label can be merged into one, we have less intervals than labels, 
therefore the type of the partition decreases.
The following lemma, due to P. Rodriguez del Tio and M.C. Valsero 
Blanco (1991), shows that such orderings and labelings can be found.
Lemma 3.8.1. Let E = E(n) be the set of N = n + n(n — 1)/2 unordered 
pairs (i, j) with i,j in {1, 2,...,n}. Then the elements of E may be ordered 
and labeled as (a(k),b(k)),k=1,...,N, so that in the circular arrangement 
a(1),a(2),...,a(N),b(1),...,b(N) (i.e., with a(1) adjacent to b(N)) the 
n +1 occurrences of each label i form at most [n/2] + 1 contiguous sets.
Proof. First assume n is odd. Let G be the graph with points 1,...,n 
and edges E (the complete graph with loops added at every point). At 
every point there are exactly (n — 1) + 2 = n + 1 incident edges, since a 

260 
3. The Rotational Theory of Markov Processes
loop counts as 2. Since this number is even, there is an Eulerian cycle 
v1,...,vN,vN+1 = v1 in G (see M. Gondran and M. Minoux (1984), The­
orem 1, p. 338), for which we may assume 1 = v1 = vN.Ifk is even, let 
(a(k), b(k)) = (vk,vk+1) and if k is odd, let (a(k), b(k)) = (vk+1, vk).
In vertical notation, the ordering of E is as follows:
b (k) 
1 
V 3 
V 3 
Vk 
Vk+2 
Vk+2 
• • • 
1
a (k) 
V 2 
V 2 
V 4 
••• Vk+1 
Vk+1 
Vk+3 
••• 
1.
Observe that all occurrences of every label i appear in pairs, with the 
possible exception of the four l’s in columns 1,N - 1andN.Ifn =1 (mod 
4), then N is odd and the 1 in column N - 1isinthetoprow,sothe 
pairing holds for all the l’s, too. Since each label i occurs n + 1 times. then 
it occurs in (n + 1)/2 contiguous pairs and we are done. If n = 3 (mod 4), 
then there is a l in the bottom row of column N - 1, so these four l’s are 
still in two contiguous sets and the occurrences of any label i still form at 
most (n + 1)/2 contiguous sets, as required.
If n is even, the graph G has odd degree (n + 1) at every point. In this 
case, define G' to be the multigraph with N' = N + n/2 edges obtained 
from G by adding another copy of each edge (1, 2), (3, 4),...,(n - 1, n). 
Observe that every vertex has even degree n + 2 in G'. Apply the same 
argument as before to obtain N' columns with all labels (except possibly 
l, which is treated specially, as before) appearing n + 2 times in (n + 2)/2 
pairs. Then delete one appearance each, for the n/2 added columns (edges). 
Renumber the columns; in the resulting circular ordering each label appears 
in at most (n + 2)/2 contiguous sets. 
□
Example 3.8.1. Let n =3(N = 6). An Eulerian cycle in G is given by 
1, 2, 2, 3, 3, 1. The associated ordering of E, writing pairs vertically, is 
given by
1 
2 
2 
3 
3 
1 (twocontiguoussets),
223311
Example 3.8.2. Let n = 4 (N =10, N' = 12). An Eulerian cycle in G' is 
given by 1, 2, 2, 3, 3, 4, 4, 1, 3, 4, 2, 1. If we delete the second occurrence of 
the additional edges (1, 2) and (3, 4) (marked x), we have
12233443322 
22334411441
1 (threecontiguoussets),
1
xx
Theorem 3.8.2. If R is a symmetric matrix, the P has a rotational rep­
resentation (2, S) where S has type [n/2] + 1.

3.8 A Reversibility Criterion in Terms of Rotational Representations 261
Proof. As R is a symmetric matrix, it is a convex combination of n cycle 
matrices of length one and n(n - 1)/2 cycle matrices of length two with 
coefficients rii in the (i) cycle and 2rij in the (i, j) cycle. Let (a(i), b(i)), i = 
1,...,N, be as in Lemma 3.8.1.
Define
a 0 = 0,
ai = (ra(i),b(i))/2, if a(i) = b(i);
ai = ra(i),b(i), 
if a(i) = b(i),i=1,2,...,N.
Let Ak1, k =1,...,N, be a partition of [0, 1/2), where
Ak1 =
E ai, t 
i=0 i=0
Define Ak2 = f1/2(Ak1),k=1,...,N,
Si = I Ak 11 U 
Ak2 I ,
a(k)=i 
b(k)=i
S = {Si ,i = 1, . . . ,n}.
Note that
N
R = E2akC(a(k),b(k)),
where C(a(k),b(k)) is the n x n cycle matrix with elements:
if a ( k ) = b ( k ): 
ca ( k) ,b ( k) 
cb ( k) ,a ( k) 
2,
cij =0, otherwise;
if a(k) = b(k): 
ca(k),b(k) =1,
cij =0, otherwise.
Direct calculations prove that (2, S) is a rotational representation of P.
The definition of the labels (a(k), b(k)) shows that S has type [n/2] + 1.
The proof is complete. 
□
Remark. In case the matrix R does not contain all possible cycles of length 
two or all possible cycles of length one, fewer subintervals in the previous 
construction are required and the type of the partition may possibly be less 
than [n/2] + 1. For example, if pii =0,i=1, ,n (i.e., there are not any 
loops) and n is even or n = 3 (mod 4), then the matrix P has a rotational 
representation in which the partition S has type [n/2].
Let P be an n x n recurrent matrix with n its invariant probability dis­
tribution. Let Q = (qij) be defined qij = (njpji)/ni. Q is called the reversed 

262 
3. The Rotational Theory of Markov Processes
matrix of P. It is well known that Q is a stochastic matrix whose invariant 
probability distribution is also n.
We have:
Lemma 3.8.3. Let (t, S ) be a rotational representation of P. Then (1 - 
t, S ) is a rotational representation of Q.
Proof. Let ft (x)=(x + t) (mod 1), gt (x)=(x +1- t) (mod 1) be shift 
transformations on [0, 1). Then ft = gt-1 . Since (t, S ) is a rotational rep­
resentation of P, then pij = (A(ft(Si) A Sj))/A(Si). Furthermore, qij = 
(njpji)/ni = A(ft(Sj) A Si)/A(Si) = A(Sj A gt(Si))/A(Si). This completes 
the proof. 
□
We are now prepared to prove a reversibility criterion in terms of the 
rotational representations following P. Rodriguez del Tio and M.C. Valsero 
Blanco (1991).
Theorem 3.8.4. A recurrent matrix P is reversible if and only if it has a 
rotational representation (2, S), for some partition S. If so, there is such 
a representation where the type of S is [n/2] + 1.
Proof. Let P be a recurrent reversible matrix, then pij = qij ,soR is sym­
metric and we apply Theorem 3.8.2. Conversely, suppose that P has a 
rotational representation (2, S); then by Lemma 3.8.3. Q has the same 
representation, so P = Q. 
□
3.9 Rotational Representations of
Transition Matrix Functions
Let n > 2 and let P(h) = (pij(h), i,j = 1,... ,n), h > 0, be any standard 
transition matrix function defining an irreducible Markov process f = 
(fh)h>0, whose invariant probability distribution is denoted by n = (ni, i = 
1,...,n). Then the cycle representation Theorem 5.5.2 of Part I and The­
orem 2.1.1 of Part II assert that each P(h) has a linear decomposition in 
terms of a collection (C, wc(h)) of directed circuits c and positive (weight-) 
functions wc(h), that is,
nipij (h) = 
wc(h) Jc(i, j), i,j = 1,. .., n,
ceC
where the collection Cis independent of h and the wc (•) enjoy a probabilistic 
interpretation in terms of the sample paths.
Associate with each h > 0 the discrete skeleton chain Hh = (fhm) m>0 
with scale parameter h. Then, as is well known, Hh is an aperiodic and 

3.9 Rotational Representations of Transition Matrix Functions 263
irreducible finite Markov chain whose transition matrix is given by P (h)= 
(pij (h), i, j =1,...,n). A transition matrix function defining an irreducible 
(or recurrent) Markov process will be called an irreducible (or recurrent) 
transition matrix function.
In this section we generalize the rotational problem to the semigroup 
(P (h))h>0 of stochastic matrices following S. Kalpazidou (1994b). We shall 
assume that the hypotheses of Theorem 3.5.1 of Part II are satisfied (the 
ordering of the circuits and the starting points of the circuits are fixed, etc.).
We have
Theorem 3.9.1. (i) A standard transition matrix function 
P =( P (h) ,h > 0) on {1, 2, ...,n},n > 2, is recurrent if and only if 
for each h>0 there exists a rotational representation (t, S (h)) for P (h), 
that is,
Pij (h ) = X (Si n f- 1( Sj))/X (Si), i,j = 1 ,...,n; h> 0, 
(3.9.1)
where ft = ( x + 1) (mod 1) ,x G [0, 1), with t =1 /n!, S (h ) = ( Si (h) ,i = 
1,...,n) is a partition of [0, 1) and X denotes Lebesgue measure. More­
over, for any recurrent standard transition matrix function P = (P(h), h > 
0) and for any positive invariant probability distribution n there is a 
rotational representation (t, S (h)) for each P (h) ,h > 0, such that n = 
(X(S1(h)),...,X(Sn(h))).
(ii) There exists a map $ defined by Theorem 3.5.1 which, for any ir­
reducible standard transition matrix function P =(P(h),h > 0), assigns 
each P(h),h > 0, to an n-partition S(h) = (Si (h), i =1,...,n) of [0, 1) 
such that for all h the Si (h) have the same labels for their components, 
i =1,...,n.
Proof. (i) The assertion (i) of the theorem follows from Theorem 3.3.1 
(Part II) applied to each P(h),h > 0.
(ii) We further appeal to Theorem 5.5.2 of Part I according to which we 
have the following probabilistic decomposition
niPij (h) = 
Wc (h) Jc (i, j), 
(3.9.2)
ctC
where (wc (h), c G C) is the circulation distribution of the discrete skeleton 
Hh on P (h) whose ordered collection C of circuits is independent of h. 
Then the statement (ii) of the theorem follows from Theorem 3.5.1 of Part 
II applied to each stochastic matrix P(h),h > 0.
Finally, from the definition of the labeling (3.4.3) and from the cy­
cle version of the Levy Theorem 2.1.2, it follows that for each i the 
sets Si (h), h > 0, have the same labels (k, l) for their component intervals 
Akl (h). The proof is complete. 
□
A transition matrix function P = (P(h), h > 0) which satisfies equa­
tions (3.9.1) is said to have the rotational representation (t, S(h))h>0.

264 
3. The Rotational Theory of Markov Processes
Let P = (P(h), h > 0) be a recurrent standard transition-matrix function 
where P(h) = (pij (h), i,j = 1,..., n), n > 2, and let £ = (£h)h>0 be the cor­
responding Markov process. Denote by C the collection of directed circuits 
(with distinct points except for the terminals) which occur along the sam­
ple paths of Ht, t > 0. Let further a be the number of circuits of C. Also, for 
each P(h) let S (h) be a rotational partition associated with C according 
to the procedure of Section 3.4.
Denote by 6(j),j = 1,..., n, the number of components Akl(h), occur­
ring in the description of Sj (h) by the union (k,l) Akl (h) which is indexed 
according to the labeling (3.4.3). Then, as in Theorem 3.9.1 (ii), we see 
that 6(j ),j =1,...,N, depends only on the representative class C of cir­
cuits, and so 6 (j ) will be a common characterstic of all the partitioning 
sets Sj (h), h > 0.
Consider 6(a, C) = maxj=1,...,n 6(j).
We call 6(a, C) the rotational dimension of the transition matrix function 
P. Then the rotational dimension 6(s, C) of all the recurrent transition­
matrix functions with the same graph G is provided by the collection C of 
all the directed circuits of G.

List of Notations
ft 
( ftedge, point)
c
c
c
C
Cc
Y
r( p )
Y (•)
d(k, u)
D(P)
Z 
(Zedge, circuit)
G(P)
n = (nedge, point)
Jc
A
M 
({0, 1})
page 15
directed circuit
directed cycle
the one-chain associated to c
class of directed circuits, or cycles
the circuit (or cycle) matrix associated to c
a circuit of a Betti base
base of Betti circuits of the graph of P
the growth function
the shortest-length-distance
the rotational dimension of P
page 13
the graph of the stochastic matrix P
page 13
the passage-function associated to c
Lebesgue measure
the set of m x m matrices whose entries belong to 
{0, 1}.
N(i,j/i1,...,ik)
v = (vcell, edge)
P
P = (pij )
P(t) = (pij (t)) 
n = (n)
n = n([i 1,.. .,in])
page 32
page 62
probability measure
stochastic matrix
stochastic transition matrix function 
invariant probability distribution 
page 32

266 List of Notations
R 
(R) 
[S] 
S 
(t, S) 
wc 
wc (•) 
Z
the set of reals
the symbol of the rotational problem
page 31
rotational partition
the rotational representation of length t
circuit-weight (or cycle-weight) associated to c (or c
the weight function associated to c
the set of integers

Bibliography
L.V. Ahlfors
(1935) “Sur le type d’une surface de Riemann”, C. R. Acad. Sci. Paris, 
201, 30-32.
(1953) Complex Analysis, McGraw-Hill, New York.
L.V. Ahlfors and L. Sario
(1960) Riemann Surfaces, Princeton University Press, Princeton, pp. 214­
228.
A. Aleskeviciene
(1988) “Probabilities of large deviations in approximation by the Poisson 
law”, Lithuanian Math. J., 28 (1-28), 1-8.
A. Aleskeviciene and V. Statulevicius
(1994) “Large deviations in the approximation by Poisson law”, Proceed­
ings of the 6th Vilnius Conference on Probability Theory and Mathematical 
Statistics, Vilnius, 1993, V.S.B./T.E.V.
S. Alpern
(1979) “Generic properties of measure preserving homeomorphisms”, in: 
“Ergodic Theory, Proceedings, Oberwohlfach 1978”. Lecture Notes, No. 729, 
Springer-Verlag, Berlin.
(1983) “Rotational representations of stochastic matrices”, Ann. Probab., 
11(3), 789-794.
(1991) “Cycles in extensive form perfect information games”, J. Math. 
Anal. Appl., 159(1), 1-17.

268 Bibliography
(1993) “Rotational representations of finite stochastic matrices”, in: S. 
Kalpazidou (Ed.): “Selected Talks Delivered at the Department of Math­
ematics of the Aristotle University (1993)”, Aristotle University Press, 
Thessaloniki.
S. Alpern and V. Prasad
(1989) “Coding a stationary process to one with prescribed marginals”, 
Ann. Probab., 17(4), 1658-1663.
R. Apery
(1982) “Mathematique constructive”, in: J. Dieudonne, M. Loi, and R. 
Thom (Eds.): “Penser les Mathematiques”, Semin. Phil. et Mathematiques 
de l’Ecole Norm. Sup., Ed. du Seuil, pp. 58-72.
T.M. Apostol
(1957) Mathematical Analysis, Addison-Wesley, Reading, MA.
E.A. Asarin
(1987) “Individual random continuous functions”, in: Yu.A. Prohorov 
and V.V. Sazonov (Eds.): Proceedings of the First World Congress of 
the Bernoulli Society on Mathematical Statistics and Probability Theory, 
Tashkent, VNU Sciences Press, Utrecht.
E.A. Asarin and A.V. Pokrovskii
(1986) “Application of Kolmogorov’s complexity to dynamics analysis of 
control systems”, Avtomat. i Telemekh. 1, 25-30. (Russian.)
P. Baldi, N. Lohoue, and J. Peyriere
(1977) “Sur la classification des groupes recurrents”, C. R. Acad. Sci. 
Paris, 285(A), 1103-1104.
M.S. Bartlett
(1966) An Introduction to Stochastic Processes with Special Reference to 
Methods and Applications, Cambridge University Press, London.
C. Berge
(1970) Graphes et Hypergraphes, Dunod, Paris.
A. Beurling and J. Deny
(1959) “Dirichlet spaces”, Proc. Nat. Acad. Sci. U.S.A. 45, 208-215.
P. Billingsley
(1968) Convergence of Probability Measures, Wiley, New York.
G. Birkhoff and S. MacLane
(1942) A Survey of Modern Algebra, Macmillan, New York.
A. Bischof
(1940) “Beitrage zur Caratheodoryschen Algebraisierung des Integralbe- 
griffs Dissertation”, Schr. Math. lust. u. Inst. angew. Math. Univ. Berlin, 
5, 237-262.

Bibliography
269
B. Bollobas
(1979) Graph Theory. An Introductory Course, Springer-Verlag, New 
York.
F.H. Branin, Jr.
(1959) “The relation between Kron’s method and the classical meth­
ods of network analysis”, I.R.E. WESCON Convention Record, Part 2, 
pp. 3-29.
(1961) “An Abstract Mathematical Basis for Network Analogies and Its 
Significance in Physics and Engineering”, Amer. Inst. Elec. Engr., Preprint 
S-128.
(1966) “The algebraic-topological basis for network analogies and the vec­
tor calculus”, Proceedings of the Symposium on Generalized Networks, Poly­
technic Institute of Brooklyn, New York.
B. Bru
(1993) “Doeblin’s life and work from his correspondence”, Contemp. 
Math., 149, 1-64.
Th. Cacoullos
(1970) Probability Theory and Elements of Stochastic Processes, Univer­
sity of Athens, Athens. (Greek.)
C. Caratheodory
(1911) “Uber den Variabilitatsbereich der Furierschen Konstanten von 
positiven harmonischen Funktionen”, Rend. Circ. Mat. Palermo, 32, 
193-217.
(1918) 
Vorlesungen uber reelle Funktionen, Leipzig-Berlin.
(1937) 
Geometrische Optik, IV, Ergebnisse der Mathematik und ihrer 
Grenzgebiete, Berlin.
(1938) 
“Entwurf fur eine Algebraisierung des Integralbegriffs”, 
Munchener Sitzungsber. Math.-Naturw. Abteilung, 27-68.
(1939) 
Reelle Funktionen, Leipzig-Berlin.
(1950) Funktionentheorie, Basel.
(1956) 
Mass und Integral und ihre Algebraisierung, Basel.
K.L. Chung
(1963) 
“On the boundary theory for Markov chains”, Acta Math. 110, 
19-77.
(1964) 
“The general theory of Markov processes according to Doeblin”, 
Z. Wahrsch. Verw. Gebiete, 2, 230-254.
(1966) 
“On the boundary theory for Markov chains, II”, Acta Math. 115, 
111-163.
(1967) 
Markov Chains with Stationary Transition Probabilities, 2nd edn., 
Springer-Verlag, New York.
(1974) Elementary Probability Theory with Stochastic Processes, Springer­
Verlag, New York.

270 Bibliography
(1982) Lectures from Markov Processes to Brownian Motion, Springer­
Verlag, New York.
(1988) “Reminiscences of Some of Paul Levy’s Ideas in Brownian Motion 
and in Markov Chains”, Societe Mathematique de France, pp. 157-158.
K.L. Chung and J.B. Walsh
(1969) “To reverse a Markov process”, Acta Math., 123, 225-251.
A. Church
(1940) “On the concept of a random sequence”, Bull. Amer. Math. Soc. 
46(2), 130-135.
G. Ciucu
(1963) Elements of Probability Theory and Mathematical Statistics, Edit. 
Didactica si Pedagogica, Bucharest. (Romanian.)
Joel E. Cohen
(1981) “A geometric representation of stochastic matrices; theorem and 
conjecture”, Ann. Probab., 9, 899-901.
Joel E. Cohen, Y. Derriennic, and Gh. Zbaganu
(1993) “Majorization, monotonicity of relative entropy, and stochastic 
matrices”, Contemp. Math., 149, 251-259.
D.R. Cox and H.D. Miller
(1965) The Theory of Stochastic Processes, Chapman and Hall, London.
L. DeMichele and P.M. Soardi
(1990) 
“A Thomson’s principle for infinite, nonlinear resistive networks”, 
Proc. Amer. Math. Soc., 109(2), 461-468.
C. Derman
(1954) 
“A solution to a set of fundamental equations in Markov chains”, 
Proc. Amer. Math. Soc., 79, 541-555.
(1955) 
“Some contributions to the theory of denumerable Markov chains”, 
Trans. Amer. Math. Soc., 79, 541-555.
Y. Derriennic
(1973) 
“On the integrability of the supremum of ergodic ratios”, Ann. 
Probab. 1(2), 338-340.
(1975) 
“Sur le theoreme ergodique sousadditif”, C. R. Acad. Sci. Paris, 
Sene A, 281, 985-988.
(1976) 
“Lois zero au deux pour les processus de Markov. Applications aux 
marches aleatoires”, Ann. Inst. H. PoincareB12, 111-129.
(1980) 
“Quelques applications du theoreme ergodique sousadditif”, 
Asteerisque, 74, 183-201.
(1986) 
Entropie, Theoremes Limites et Marches Aleatoires, Lecture Notes 
in Mathematics, No. 1210, Springer-Verlag, Berlin.

Bibliography
271
(1988) “Entropy and boundary for random walks on locally compact 
groups”, Transactions of the Tenth Prague Conf. on Information Theory, 
etc., Akademia, Prague, pp. 269-275.
(1993) “Ergodic problems on random walks in random environment”, in: 
S. Kalpazidou (Ed.): “Selected Talks Delivered at the Department of Mathe­
matics of the Aristotle University (1993)”, Aristotle University Press, Thes­
saloniki.
(1999a) “Sur la recurrence des marches aleatoires unidimensionnelles en 
environnement aleatoire”, C. R. Acad. Sci. Paris., t. 329, series I, 65-70.
(1999b) “Random walks with jumps in random environments (examples of 
cycle and weight representations)”, Proceedings of the 22nd European Meet­
ing of Statisticians and of the 7th Conference on Probability Theory and 
Mathematical Statistics, Vilnius, August 12-18, 1998, Utrecht, VNU Press.
Y. Derriennic and M. Lin
(1989) “Convergence of iterates of averages of certain operator represen­
tations and of convolution powers”, J. Funct. Anal., 85, 86-102.
(1995) “Uniform ergodic convergence and averaging along Markov chain 
trajectories”, J. Theoret. Probab. (To appear.)
C.A. Desoer and E.S. Kuh
(1969) Basic Circuit Theory, McGraw-Hill, Singapore.
J.P. Dion and M.N. Yanev
(1995) Branching Processes-Control, Statistics, Applications, Wiley, New 
York. (In press.)
V. D. Dinopoulou and C. Melolidakis
(2000) “On the optimal control of parallel systems and queues in Game 
Theory”, IMS Lecture Notes—Monograph Series 35, Inst. Math. Statistics. 
Beachwood, OH. 83-100.
(2001) “Asymptotically optimal component assembly plans in repairable 
systems and server allocation in parallel multiserver queues”, Nav. Res. 
Logistics, 48, 732-746.
W. Doeblin
(1937a) “Sur les proprietes asymptotiques de mouvements regis par 
certains types de chalnes simples”, Bull. Math. Soc. Roumaine Sci. 39, 
no. 1, 57-115; no. 2, 3-61.
(1937b) “Le cas discontinu des probabilites en chalne”, Publ. Fac. Sci. 
Univ. Masaryk (Brno), no. 236.
(1940) “Elements d’une theorie generale des chalnes simples constantes 
de Markoff”, Ann. Ecole Norm. Sup., 37(3), 61-111.
W. Doeblin and R. Fortet
(1937) “Sur des chalnes a liaisons completes”, Bull. Soc. Math. France, 
65, 132-148.

272
Bibliography
V. Dolezal
(1977) Nonlinear Networks, Elsevier, New York.
(1979) Monotone Operators and Applications in Control and Network 
Theory, Elsevier, New York.
(1993a) “Some results on optimization of general input-output systems”, 
Proceeding of the International Symposium on the Mathematical Theory 
of Networks and Systems, Regensburg, August 2-6, 1993.
(1993b) “Estimating the change of current distribution in a Hilbert 
network caused by perturbations of its elements”, Proceeding of the 
International Symposium on the Mathematical Theory of Networks and 
Systems, Regensburg, August 2-6, 1993.
J.L. Doob
(1953) Stochastic Processes, Wiley, New York.
P.G. Doyle and J.L. Snell
(1984) Random Walks and Electric Networks, The Carus Mathematical 
Monographs, Mathematical Association of America.
C.A. Drossos
(1990) “Foundations of fuzzy sets: A nonstandard approach”, Fuzzy Sets 
and Systems, 37, 287-307.
C.A. Drossos, G. Markakis, and M. Shakhatreh
(1992) “A nonstandard approach to fuzzy set theory”, Kybernetika, 28.
C.A. Drossos and G. Markakis
(1995) “Boolean powers and stochastic spaces”. (To appear.)
R.J. Duffin
(1956) “Infinite programs”, in: “Linear Inequalities and Related Sys­
tems”, Ann. of Math. Stud., 38, 157-170.
(1959) “Distributed and lumped networks”, J. Math. Mech., 8, 793-826.
(1962) “The extremal length of a network”, J. Math. Anal. Appl., 5, 
200-215.
E.B. Dynkin
(1965) Markov Processes, Springer-Verlag, New York.
W. Feller
(1966a) “On the Fourier representation for Markov chain and the strong 
ratio theorem”, J. Math. Mech., 15, 273-283.
(1966b) An Introduction to Probability Theory and its Applications, Vol.
2, Wiley, New York.
(1968) An Introduction to Probability Theory and its Applications, Vol.
1, 3rd edn., Wiley, New York.
T. S. Ferguson and C. Melolidakis
(1997) “Last Round Betting”, J. Appl. Probab., 34, 974-987.

Bibliography
273
(1998) “On the inspection game”, J. Nav. Res. Logistic, 45, 327-334.
(2000) “Games with finite resources”, Int. J. Game Theory, 29, 289-303.
H. Flanders
(1971) “Infinite electrical networks: I—resistive networks”, IEEE Trans. 
Circuit Theory, 18, 326-331.
(1972) “Infinite electrical networks: II—resistance in an infinite grid”, J. 
Math. Anal. Appl., 40, 30-34.
E. Flytzanis and L. Kanakis
(1994) “Invariant probabilities for weighted composition operators”, 
Proceedings of the Sixth International Vilnius Conference on Probability 
Theory and Mathematical Statistics, Vilnius, 1993.
L.R. Ford, Jr. and D.R. Fulkerson
(1962) Flows in Networks, Princeton University Press, Princeton.
N. Frangos and L. Sucheston
(1986) “On multiparameter ergodic and martingale theorems in infinite 
measure spaces”, Probab. Theory Related Fields, 71, 477-490.
M. Fukushima
(1980) Dirichlet Forms and Markov 
Processes, 
North-Holland,
Amsterdam.
Ch. Ganatsiou
(1995a) “A random system with complete connections associated with a 
generalized Gauss-Kuzmin-type operator”, Rev. Roumaine Math. Pures 
Appl. 40(2), 85-89.
(1995b) “Investigations of certain ergodic systems occurring in metrical 
number theory”, Ph.D. thesis, Aristotle University of Thessaloniki.
(1995c) “Some asymptotic results associated with a generalized Gauss- 
Kuzmin type operator”, Portugal. Math. 52(2), 167-173.
(1995d) “On the asymptotic behaviour of digits of continued fractions 
with odd partial quotients”, Quaestiones Mathematicae, 18(4), 517­
526.
(1997) “On G-continued fractions with identically distributed rests”, 
Nonlinear Analysis, Theory, Methods and Applications, 30(4), 2051­
2059.
(2000) “Probability measures associated with the continued fractions 
with odd partial quotients”, Quaestiones Mathematicae, 23(3), 335­
342.
(2001a) “On the application of ergodic theory to alternating Engel series”, 
International Journal of Mathematics and Mathematical Sciences, 25(12), 
811-817.
(20001b) “On Some properties of the alternating Luroth-type series 
representations for real numbers”, International Journal of Mathematics 
and Mathematical Sciences, 28(6), 367-373.

274 Bibliography
J. Gani
(1956) “The condition of regularity in simple Markov chains”, Austral.
J. Phys., 9, 387-393.
(1975) “Some stochastic models in linguistic analysis”, Adv. in Appl. 
Probab., 7, 232-234.
I.I. Gihman and A.V. Skorohod
(1965) Introduction to the Theory of Random Processes, Nauka, Moscow. 
(Russian.)
P. Glansdorff and I. Prigogine
(1971) Thermodynamic Theory of Structure, Stability and Fluctuations, 
Wiley, New York.
B.V. Gnedenko and A.N. Kolmogorov
(1954) Limit Distributions for Sums of Independent Random Variables, 
Addison-Wesley, Reading, MA.
M. Gondran and M. Minoux
(1984) Graphs and Algorithms, Wiley, New York.
G.L. Gong
(1981) “Finite invariant measures and one-dimensional diffusion process”, 
Acta Math. Sinica, 4, (Chinese.)
G.L. Gong and Minping Qian
(1981) “Reversibility of non-minimal process generated by second-order 
operator”, Acta Math. Sinica, 24(2). (Chinese.)
(1982) The Invariant Measure, Probability Flux and Circulation of One­
Dimensional Markov Processes, Lecture Notes in Mathematics, No. 923.
G.L. Gong, Minping Qian, and J. Xiong
(1990) “The winding number of stationary drifted Brownian motions”, 
Appl. Probab. Statist. (Chinese.)
Gong Guanglu and Minping Qian
(1997) “Entropy production of stationary diffusions on non-compact 
Riemannian manifolds”, Sci. China (series A), 40(9), 926.
(1998) “The symmetry of diffusions and the circulations of their 
projection processes”, Sci. China. (series A), 41(10), 1017-1022.
R.E. Green and R. Wu
(1979) “C™ approximation of convex subharmonic and plurisubharmonic 
functions”, Sci. Ecole Norm. Sup., 12, 47-84.
D. Griffeath and T.M. Liggett
(1982) “Critical phenomena for Spitzer’s reversible nearest particles 
systems”, Ann. Probab., 10, 881-895.
B. Grigelionis
(1963) “On the convergence of sums of step stochastic processes to a 
Poisson process”, Probab. Theory Appl., 8(2), 189-194.

Bibliography
275
S. Grigorescu
(1975) “Notes on the theory of random systems with complete connec­
tions”, Lecture Notes of Department of Mathematics, Wales University, 
University College of Swansea, Swansea.
Y. Guivarc’h
(1980a) “Theoremes quotients pour les marches aleatoires”, Asterisque, 
74(5), 15-28.
(1980b) “Sur la loi des grands nombres et le rayon spectral d’une marche 
aleatoire”, Asterisque, 74(3, 4), 47—98.
(1984) “Application d’un theoreme limite locale a la transience et a la 
recurrence de marches de Markov”, Colloque de Theorie du Potentiel, 
Lecture Notes, No. 1096(6), pp. 301-332, Springer-Verlag, New York.
Maocheng Guo and Chengxun Wu
(1981a) “The process generated by a periodic second-order differential 
operator and its reversibility”, Acta Sci. Natur. Univ. Pekinensis, 4.
(1981b) “The circulation decomposition of the probability currents of the 
bilateral birth and death process”, Scientia Sinica, 24(10), 1340-1351.
Maocheng Guo, Min Qian and Zheng-dong
(1993) “Representation of the entropy production in terms of rotation 
numbers”, Research Report, 61, Institute of Mathematics and Department 
of Mathematics, Peking University.
Zhenchun Guo, Min Qian, and Minping Qian
(1987) “Minimal coupled diffusion process”, Acta Math. Appl. Sinica 
(English series), 3(1), 58-69.
J. Haigh
(1985) “Rotational representation of stochastic matrices”, Ann. of 
Probab., 13, 1024-1027.
J. Hajnal
(1956) “The ergodic properties of non-homogeneous finite Markov 
chains”, Math. Proc. Cambridge Philos. Soc., 52, 62-77.
(1993) “Shuffling with two matrices”, Contemp. Math., 149, 271-287.
P. Hall
(1935) “On representatives of subsets”, J. London Math. Soc., 10, 26-30.
P.R. Halmos
(1950) Measure Theory, Van Nostrand, New York.
(1956) Lectures on Ergodic Theory, Chelsea, New York.
T.E. Harris
(1952) “First passage and recurrence distributions”, Trans. Amer. Math. 
Soc., 73, 471-486.
(1956) “The existence of stationary measures for certain Markov pro­
cesses”, Proc. 3rd. Berkeley Symp. Math. Statist. Prob., Vol. 2, pp. 
113-124, University of California Press, Berkeley.

276
Bibliography
(1957) “Transient Markov chains with stationary measures”, Proc. Amer. 
Math. Soc., 8, 937-942.
T.E. Harris and R. Robins
(1953) “Ergodic theory of Markov chains admitting an infinite invariant 
measure”, Proc. Nat. Acad. Sci. U.S.A., 39, 860-864.
T. Hill
(1977) Free Energy Transduction in Biology, Academic Press, New York.
P. J. Hilton and S. Wylie
(1967) Homology Theory, Cambridge University Press, Cambridge.
E. Hopf
(1948) Ergodentheorie, Chelsea, New York.
B. Hostinsky
(1931) “Methods generates du calcul de probabilite”, Mem. Sci. Math., 
5, Gauthier-Villars, Paris.
C.T. Ionescu Tulcea and G. Marinescu
(1984) “Sur certaines chalnes a liaisons completes”, C. R. Acad. Sci. 
Paris, 227, 667-669.
M. Iosifescu
(1963a) “Random systems with complete connections with arbitrary set 
of states”, Rev. Roumaine Math. Pures Appl., 8, 611-645.
(1963b) “Sur l’ergodicite uniforme des systemes ateatoires homogenes a 
liaisons completes a un ensemble quelconque d’etats”, Bull. Math. Soc. 
Sci. Math. Phys. R. P. Roumaine (N.S.), 7 (55), 177-188.
(1966a) “Conditions necessaires et suffisantes pour l’ergodicite uniforme 
des chalnes de Markoff variables et multiples”, Rev. Roumaine Math. 
Pures Appl., 11, 325-330.
(1966b) “Some asymptotic properties of the associated system to a 
random system with complete connections”, Rev. Roumaine Math. Pures 
Appl., 11, 973-978.
(1973) “On multiple Markovian dependence”, Proceedings of the Fourth 
Conference on Probability Theory, Brasov 1971, pp. 65-71, Ed. Akademiei, 
Bucharest.
(1983) “Asymptotic properties of learning models”, in: Mathematical 
Learning Models—Theory and Algorithms, Lecture Notes in Statistics, No. 
20, pp. 86-92, Springer-Verlag, New York.
(1990) “A survey of the metric theory of continued fractions, fifty years 
after Doeblin’s 1940 paper”, in: S. Kalpazidou (Ed.): Selected Talks on 
Stochastic Processes Delivered at the Department of Mathematics of the 
Aristotle University, Aristotle University Press, Thessaloniki.
M. losifescu and A. Spataru
(1973) “On denumerable chains of infinite order”, Z. Wahrsch. Verw. 
Gebiete, 27, 195-214.

Bibliography
277
M. losifescu and P. Tautu
(1973) Stochastic Processes and Applications in Biology and Medicine, I, 
Theory, Ed. Academiei and Springer-Verlag, Bucharest and Berlin.
M. losifescu and S. Grigorescu
(1990) Dependence with Complete Connections and its Applications, 
Cambridge University Press, Cambridge.
K. Ito
(1960) Stochastic Processes, I, 
Izdatel’stvo Inostrannoi Literatury,
Moscow. (Russian.)
(1963) Stochastic Processes, II, Izdatel’stvo 
Inostrannoi Literatury,
Moscow. (Russian.)
D.Q. Jiang, Min Qian, and Min-Ping Qian
(2000) “Entropy production and information gain in Axiom-A systems,” 
Commun. Math. Phys., 214(2), 389-400.
(2003) 
“Mathematical Theory of Nonequilibrium Steady States”, Lecture 
Notes in Mathematics, LNM 1833, Springer, Berlin, Heidelberg, New York.
(2005) “Entropy production, information gain and Lyapunov exponents 
of random hyperbolic dynamical systems,” (to appear).
D.Q. Jiang, P.D. Liu, and M. Qian
(2002) “Lyapunov exponents of hyperbolic attractors”, Manuscripta 
Math., 108(1), 43-67.
D.Q. Jiang and M. Qian
(2004) 
“Ergodic hyperbolic attractors of endomorphisms”, (To appear).
D.Q. Jiang, M. Qian, and F.X. Zhang
(2003) “Entropy production fluctuations of finite Markov chains”, J. 
Math. Phys., 44(9), 4176-4188.
D.Q. Jiang and F.X. Zhang
(2005) 
“The Green-Kubo formula and power spectrum of reversible 
Markov processes”, J. Math. Phys., (To appear).
A.A. Juskevic
(1959) “On differentiability of transition probabilities of homogeneous 
Markov processes with a countable number of states”, Ucenye Zapiski 
MGU 186, Mat 9, 141-160. (Russian.)
M. Kac
(1947) “On the notion of recurrence in discrete stochastic processes”, 
Bull. Amer. Math. Soc. 53, 1002-1010.
T. Kaijser
(1972) Some limit theorems for Markov chains with applications to 
learning models and products of random matrices, Ph.D. thesis, Institute 
Mittag-Leffler, Djursholm, Sweden.

278 Bibliography
(1978) “On weakly distance diminishing random systems with complete 
connections”, Report Li TH-MAT-R-78-15, Department of Mathematics 
Linkoping University, Linkoping.
(1986) “A note on random systems with complete connections and their 
applications to products of random matrices”, in: Cohen, Kesten, and 
Newman (Eds.) (1986), pp. 243-254.
S. Kakutani
(1945) “Markov processes and the Dirichlet problem”, Proc. Japan Acad., 
21, 227-233.
V.V. Kalashnikov
(1973) “The property of y-recurrence for Markov sequences”, Dokl. Akad. 
Nauk SSSR, 213(6), 1243-1246. (Russian.)
(1978) “Solution of the problem of approximation of a countable Markov 
chain”, Izv. Akad. Nauk SSSR. Tekhn. Kibernet., 3, 92-95. (Russian.)
V.V. Kalashnikov and S.T. Rachev
(1990) Mathematical Methods for Construction of Queueing Models, 
Wadsworth and Brooks, California (1988, Nauka, Moscow, Russian).
S. Kalpazidou
(1985) “On some bidimensional denumerable chains of infinite order”, 
Stochastic Process. Appl., 19, 341-357.
(1986a) “A Gaussian measure for certain continued fractions”, Proc. 
Amer. Math. Soc., 96(4), 629-635.
(1986b) “Some asymptotic results on digits of the nearest integer contin­
ued fraction”, J. Number Theory, 22(3), 271-279.
(1986c) “On nearest continued fractions with stochastically indepen­
dent and identically distributed digits”, J. Number Theory, 24(1), 114­
125.
(1987a) “On the applications of dependence with complete connections 
to the metrical theory of G-continued fractions”, J. Lithuanian Acad., 
xxxii(1), 68-79.
(1987b) “Representation of multiple Markov chains by circuits”, Proceed­
ings of the 17th European Meeting of Statisticians (Thessaloniki 1987), 
Aristotle University Press, Thessaloniki, 1987.
(1988a) “On the representation of finite multiple Markov chains by 
weighted circuits”, J. Multivariate Anal., 25(2), 241-271.
(1988b) “On circuit chains defined by forward and backward passages”, 
Stochastic Anal. Appl., 6, 397-416.
(1989a) “Representation of denumerable Markov chains with multiple 
states by weighted circuits”, J. Appl. Probab., 26, 23-25.
(1989b) “On multiple circuit chains with a countable infinity of states”, 
Stochastic Process. Appl., 31, 51-70.
(1990a) “Asymptotic behaviour of sample weighted circuits representing 
recurrent Markov chains”, J. Appl. Probab., 27, 545-556.

Bibliography
279
(1990b) “On reversible multiple Markov chains”, Rev. Roumaine Math.
Pures Appl., 35(7), 617-629.
(1990c) “On the growth function of circuit processes”, Stochastic Anal.
Appl., 8(1), 75-89.
(1990d) “On transience of circuit processes”, Stochastic Process. Appl., 
35, 315-329.
(1991a) “On Beurling’s inequality in terms of thermal power”, J. Appl.
Probab., 28, 104-115.
(1991b) “The entropy production for superior order Markov chains”, 
Stochastic Anal. Appl., 9(3), 271-283.
(1991c) “Continuous parameter circuit processes with finite state space”, 
Stochastic Process. Appl., 39, 301-323.
(1991d) “Circulation distribution on groups”, J. Theoret. Probab., 4(3), 
475-483.
(1991e) “Invariant stochastic properties of a class of directed circuits”, J.
Appl. Probab., 28, 727-736.
(1992a) “On the asymptotic behaviour of spectral representations for 
Markov processes”, Stochastic Anal. Appl., 10, (1), 1-16.
(1992b) “Circuit processes and the corresponding one-parameter semi­
group of weight operators—a sample path analysis”. (Manuscript.)
(1992c) “On circuit generating processes”, Stochastic Anal. Appl., 
10(5).
(1992d) “Circuit processes and the corresponding one-parameter 
semigroup of weight operators”, J. Theoret. Probab., 5(1), 205­
216.
(1992e) “On the weak convergence of sequences of circuit processes: a 
probabilistic approach”, J. Appl. Probab., 29, 374-382.
(1993a) “On Levy’s theorem concerning the positiveness of the transition 
probabilities of Markov processes: the circuit processes case”, J. Appl. 
Probab., 30, 28-39.
(1993b) “An interpretation of the circuit weights representing Markov 
processes”, Rev. Roumaine Math. Pures Appl., 38(9), 767-770.
(1993c) “On the weak convergence of sequences of circuit processes: a 
deterministic approach”. (Manuscript.)
(1994a) “Cycle generating equations”, Stochastic Anal. Appl., 12(4), 
481-492.
(1994b) “Rotational representations of transition matrix functions”, Ann.
Probab., 22(2), 703-712.
(1994c) “Circuit duality for recurrent Markov processes”, Circuits Systems
Signal Process. (To appear.)
(1994d) “Cycle processes”, in: B. Grigelionis et al. (Eds.): Proceedings 
of the 6th International Vilnius Conference on Probability Theory and 
Mathematical Statistics, Vilnius, 1993, VSP/TEV.
(1995) “On the rotational dimension of stochastic matrices”, Ann. 
Probab. (To appear.)

280 Bibliography
(1997) “From network problem to cycle processes”, Proceedings of the 
Second World Congress of Nonlinear Analysts, Athens, 10-17 July, 
1996. Nonlinear Analysis, Methods and Applications, 30(4), 2041­
2049.
(1999a) “Cycloid decompositions of finite Markov chains”, Circuits Syst. 
Signal Proc., 18(3), 191-204.
(1999b) “Wide-ranging interpretations of the cycle representations of 
Markov processess”. Proceedings of the 22nd European Meeting of Statis­
ticians and of the 7th Conference on Probability Theory and Mathematical 
Statistics, Vilnius, August 12-18, 1998, Utrecht, VNU Press.
S. Kalpazidou, A. Knopfmacher, and J. Knopfmacher
(1990) “Liiroth-type alternating series representations for real numbers”, 
Acta Arith., LV(1), 311-322.
S. Kalpazidou and Joel E. Cohen
(1997) “Orthogonal cycle transforms of stochastic matrices”, Circuits 
Syst. Signal Proc., 16(3), 363-374.
S. Kalpazidou and N. Kassimatis
(1998) “Markov chains in Banach spaces on cycles”, Circuits Syst. Signal 
Proc., 17(5), 637-652.
S. Kalpazidou and Ch. Ganatsiou
(2001) “Knopfmacher expansions in Number Theory”, Quaestiones 
Mathematicae, Commemorative volume in honour of John Knopfmacher, 
24(3), 393-401.
S. Karlin
(1966) A First Course in Stochastic Processes, Academic Press, New 
York.
N. Kassimatis
(1986) “Bass and Serre theory and Nielseu transformations (Free and 
tree product case)”, Bull. Greek Math. Soc., 27, 39-46.
T. Kato
(1976) Perturbation Theory for Linear Operators, 2nd edn., Springer­
Verlag, Berlin.
F. Kelly
(1979) Reversibility and Stochastic Networks, Wiley, New York.
J.G. Kemeny, J.L. Snell, and A.W. Knapp
(1976) Denumerable Markov Chains, Springer-Verlag, New York.
D.G. Kendall
(1958) “Integral representations for Markov transition probabilities”, 
Bull. Amer. Math. Soc., 64, 358-362.

Bibliography
281
(1959a) “Unitary dilations of Markov transition operators, and the 
corresponding integral representations for transition-probability matrices”, 
in: U. Grenander (Ed.): “Probability and Statistics—the Volume Dedicated 
to Harald Cramer” Almqvist and Wiksell, Stockholm, Wiley, New 
York.
(1959b) “Unitary dilations of one-parameter semigroups of Markov 
transition operators, and the corresponding integral representations for 
Markov processes with a countable infinity of states”, Proc. London Math. 
Soc., 9(3), 417-431.
(1990) “Kolmogorov as I knew him”, in: S. Kalpazidou (Ed.): “Selected 
Talks on Stochastic Processes Delivered at the Department of Mathe­
matics of the Aristotle University (1990)”, Aristotle University Press, 
Thessaloniki.
D.G. Kendall and E.F. Harding
(1973) Stochastic Analysis, Wiley, New York.
E. Key
(1984) “Recurrence and transience criteria for random walks in a random 
environment”, Ann. Probab., 12(2), 529-560.
A. Khinchin
(1964) Continued Fractions, The University of Chicago Press (first 
Russian edition, 1935).
J.C. Kieffer
(1980) “On coding a stationary process to achieve a given marginal 
distribution”, Ann. Probab., 8, 131-141.
J.F.C. Kingman
(1962) 
“The imbedding problem for finite Markov chains”, Z. Wahrsch. 
Verw. Gebiete, 1, 14-24.
(1963) 
“Ergodic properties of continuous time Markov processes and 
their discrete skeletons”, Proc. London Math. Soc., 13, 593-604.
(1967) 
“Markov transition probabilities I”, Z. Wahrsch Verw. Gebiete, 
7, 248-270.
(1971) 
“Markov transition probabilities, V”, Z. Wahrsch. Verw. Gebiete, 
17, 89-103.
G. Kirchhoff
(1891) Vorlesungen uber Electricitat und Magnetismus, Leipzig.
V.L. Klee
(1951) “Convex sets in linear spaces, II”, Duke Math. J., 18, 875­
883.
(1957) 
“Extremal structure of convex sets”, Arch. Math., 8, 234-240.
(1958) 
“Extremal structure of convex sets, II”, Math. Z., 69, 90-104.
(1959) 
“Some characterizations of convex polyhedra”, Acta Math., 102, 
79-107.

282
Bibliography
A.N. Kolmogorov
(1931) 
“Uber die analytischen Methoden in der Wahrscheinlichkeitsrech- 
nung”, Math. Ann., 104, 415-418.
(1932) 
“Zur Deutung der intuitionistischen Logik”, Math. Z. 35(1), 
58-65.
(1936a) “Zur Theorie der Markoffschen Ketten”, Math. Ann., 112, 
155-160.
(1936b) “Anfangsgriinde der Theorie der Markoffschen Ketten mit un- 
endlich vielen moglichen Zustanden”, Mat. Sb. (N.S.) 1, 607-610.
(1937) 
“Tsepi Markova so sciotnim cislom vozmojnih sostoianii”, Biull. 
M.G.U., 1(3), 1-16. (Russian.)
(1951) 
“On the differentiability of the transition probabilities of station­
ary Markov processes with a denumerable number of states”, Ucenye Zap. 
M.G.U., 148, 53-59. (Russian.)
(1963) 
“On tables of random numbers”, Sankhya, Indian J. Statist., Ser. 
A (25), 369-376.
(1968) 
“Three approaches to the quantitative definition of information”, 
Internat. J. Comput. Math., 2, 157-168.
(1969) 
“On the logical foundations of information theory and probability 
theory”, Problems Inform. Transmission, 5(3), 1-4.
(1983a) “Combinatorial foundations of information theory and the calcu­
lus of probabilities”, Russian Math. Surveys, 38(4), 27-36.
(1983b) “On Logical Foundations of Probability Theory”, Lecture Notes 
in Mathematics, No. 1021, Springer-Verlag, pp. 1-5.
A.N. Kolmogorov and V.A. Uspensky
(1987) 
“Algorithms and randomness”, in: Yu.A. Prohorov and V.V. 
Sazonov (Eds.): Proceedings of the First World Congress of the Bernoulli 
Society on Mathematical Statistics and Probability Theory, Tashkent, VNU 
Science Press, Utrecht.
M. Krein and D. Milman
(1940) “On the extreme points of regularly convex sets”, Studia Math., 
9, 133-138.
K. Kriticos
(1950) “Constandinos Caratheodory”, naiSela, 11, 160-164. (Greek.)
G. Kron
(1945) “Electric circuit models of the Schroedinger equation”, Phys. 
Rev., 67, 39-43.
(1953) “A set of principles to interconnect the solutions of physical 
systems”, J. Appl. Phys., 24, 965-980.
A.G. Kurosh
(1960) The Theory of Groups, 2nd end., Vol. 1, Chelsea, New York.

Bibliography
283
J. Lamberti and P. Suppes
(1959) “Chains of infinite order and their application to learning theory”, 
Pacific J. Math., 9, 739-754.
E. Landau
(1958) Elementary Number Theory, Chelsea, New York.
S. Lefschetz
(1924) L’Analysis Situs et la Geometrie Algebrique, Gauthier-Villars, 
Paris.
(1930) Topology, Amer. Math. Soc. Colloquium Publ., 12. (Reprinted by 
Chelsea, 2nd edn., 1953.)
(1975) Applications of Algebraic Topology, Applied Mathematical 
Sciences, No. 16, Springer-Verlag, New York.
A. Lehman
(1965) “A resistor network inequality”, SIAM Rev., 4, 150-154.
A. Leonte
(1970) Lectii de Teoria Probabilitatilor, Craiova University Press, Craiova. 
(Romanian.)
A.V. Letchikov
(1988) “A limit theorem for a random walk in a random environment”, 
Theory Probab. Applic., 33(2), 228-238.
L.A. Levin
(1973) “The concept of a random sequence”, Soviet Math. Dokl., 212, 
1413-1416.
P. Levy
(1951) “Systemes markoviens et stationnaires. Cas denombrable”, Ann. 
Sci. Ecole Norm. Sup., 68(3), 327-381.
(1954) Theorie de l’Addition des Variables Aleatoires, 2eme ed., Gauthier- 
Villars, Paris.
(1958) “Processus markoviens et stationnaires. Cas denombrable”, Ann. 
Inst. H. Poincaree, 16, 7-25.
(1965) “Remarques sur les etats instantanes des processus markoviens et 
stationnaires, a une infinite denombrable d’etats possibles”, C. R. Acad. 
Sci. Paris, Ser. A-B, 264, A844-A848.
(1969) “Conjectures relatives aux points multiples de certaines varietes”, 
Rev. Roumaine Math. Pures Appl., 14, 819-827.
T.M. Liggett
(1987) “Applications of the Dirichlet principle to finite reversible nearest 
particle systems”, Probab. Theory Related Fields, 74, 505-528.
M. Loeve
(1963) Probability Theory, 3rd edn., Van Nostrand, Princeton.

284 Bibliography
G. Louchard
(1966) “Recurrence times and capacities for finite ergodic chains”, Duke 
Math. J., 33, 13-21.
T. Lyons
(1983) “A simple criterion for transience of a reversible Markov chain”, 
Ann. Probab., 11, 393-402.
J. MacQueen
(1981) “Circuit processes”, Ann. Probab., 9, 604-610.
Per. Martin-Lof
(1966a) “The definition of random sequences”, Inform. and Control, 9(6), 
602-619.
(1970) Notes on Constructive Mathematics, Almqvist and Wiksell, 
Stockholm.
J.C. Maxwell
(1954) A Treatise on Electricity and Magnetism, Dover, New York.
S. McGuinness
(1989) 
Random Walks on Graphs and Directed Graphs, Ph.D. thesis, 
University of Waterloo.
(1991) 
“Recurrent networks and a theorem of Nash-Williams”, J. 
Theoret. Probab., 4(1), 87-100.
C. Melolidakis
(1989) 
“On stochastic games with lack of information on one side”, Int.
J. Game Theory, 18, 1-29.
(1990) 
“Stochastic games with lack of information on one side and 
positive stop probabilities”, Stochastic games and related topics (in honor 
of L.S. Shapley), eds. TES Raghavan, Kluwer Series in Game Theory, 
Mathem. Progr. and Mathem. Econ., 113-126.
(1993) 
“Designing the allocation of emergency units by using the Shapley- 
Shubik power index: A case study”, Math. Comp. Model, 18, 97-109.
K. Menger
(1927) 
“Zur allgemeinen Kurventheorie”, Fund. Math. 10, 96-115.
(1928) 
“Untersuchungen uber allgemeine Metrik”, Math. Ann., 100, 
75-163.
(1954) 
Geometrie Generale, Memorial des Sciences Mathematiques, no. 
124, Paris.
G. Mihoc
(1935) 
“On the general properties of the independent statistical vari­
ables”, Bull. Math. Soc. Roumanie Sci. 37(1), 37-82. (Romanian.)
(1936) 
“On the general properties of the independent statistical vari­
ables”, Bull. Math. Soc. Roumanie Sci. 37(2), 17-78. (Romanian.)

Bibliography
285
G. Mihoc and G. Ciucu
(1973) “Sur la loi normale pour les chaines a liaisons completes”, Pro­
ceedings of the Fourth Conference on Probability Theory (Brasov 1971), 
pp. 169-171, Edit. Academiei R.S. Romania, Bucharest.
R.G. Miller Jr.
(1963) “Stationarity equations in continuous time Markov chains”, 
Trans. Amer. Math. Soc., 109, 35-44.
J. Milnor
(1977) “On deciding whether a surface is parabolic or hyperbolic”, Class. 
Notes, 43-45.
G.J. Minty
(1960) “Monotone networks”, Proc. Roy. Soc. London, Series A, 257, 
194-212.
A. Mukherjea
(1979) “Limit theorems: Stochastic matrices, ergodic Markov chains, and 
measures on semigroups”, Prob. Anal. Related Topics, 2, 143-203.
S.V. Nagaev
(1965) “Ergodic theorems for discrete-time Markov processes”, Sibirsk. 
Mat. Zh., 6, 413-432. (Russian.)
B.Sz. Nagy
(1953) “Sur les contractions de l’espace de Hilbert”, Acta Sci. Szeged, 
15, 87-92.
C.St.J.A. Nash-Williams
(1959) “Random walk and electric currents in networks”, Math. Proc. 
Cambridge Philos. Soc., 55, 181-194.
S. Negrepontis
(1984) Handbook of Set-Theoretic Topology, North-Holland, Amerster- 
dam, pp. 1045-1142.
J. Neveu
(1964) Bases Mathematiques du Calcul des Probabilites, Masson, Paris.
P. Ney
(1965a) “The convergence of a random distribution function associated 
with a branching process”, J. Math. Anal. Appl., 12, 316-327.
(1965b) “The limit distribution of a binary cascade process”, J. Math. 
Anal. Appl., 10, 30-36.
(1978) “A new approach to the limit theory of recurrent Markov chains”, 
Trans. Amer. Math. Soc., 245, 493-501.
(1991) “Regeneration structures for chains with infinite memory and 
expanding maps”, in: S. Kalpazidou (Ed.): “Selected Talks Delivered at the 

286
Bibliography
Department of Mathematics of the Aristotle University (1992)”, Aristotle 
University Press, Thessaloniki.
P. Ney and E. Nummelin
(1993) “Regeneration for chains of infinite order and random maps”, 
Contemp. Math., 149.
P. Ney and F. Spitzer
(1966) “The Martin boundary for random walk”, Trans. Amer. Math.
Soc., 121, 116-132.
M.F. Norman
(1968a) “Some convergence theorems for stochastic learning models with 
distance diminshing operator”, J. Math. Phych., 5, 61-101.
(1968b) “Compact Markov processes”, Technical Report, No. 2, University 
of Pennsylvania.
(1972) Markov Processes and Learning Models, Academic Press, New 
York.
E. Nummelin
(1984) General Irreducible Markov Chains and Non-Negative Operators, 
Cambridge University Press, Cambridge.
O. Onicescu and G. Mihoc
(1943) Les Chaines de Variables Aleatoires. Problemes Asymptotique, 
Academie Roumaine, Bucharest.
O. Onicescu, G. Mihoc and C.T. Ionescu Tulcea
(1956) Probability Theory and Applications, Publishing House of the 
Romanian Academy, Bucharest.
O. Ore
(1962) Theory of Graphs, American Mathematical Society, Rhode Island.
S. Orey
(1961) 
“Strong ratio limit property”, Bull. Amer. Math. Soc., 67, 
571-574.
(1962) 
“An ergodic theorem for Markov chains”, Z. Wahrsch. Verw. 
Gebiete, 1, 174-176.
(1971) 
Lecture Notes on Limit Theorems for Markov Chain Transition 
Probabilities, Van Nostrand, New York.
D.S. Ornstein
(1969) “Random walks. I, II”, Trans. Amer. Math. Soc. 138, 1-43; 45-60.
K.R. Parthasarathy
(1967) Probability Measures on Metric Spaces, Academic Press, New 
York.
E. Parzen
(1962) Stochastic Processes, Holden-Day, San Francisco.

Bibliography
287
K. Patersen
(1983) Ergodic Theory, Cambridge University Press, Cambridge.
T. Patronis
(1980) Algebraic characterizations of the ordered sets: Structure and 
dimensional types, Ph.D. thesis, Athens University. (Greek.)
O. Perron
(1952) “Constantin Caratheodory”, Jahresber. Deutsch. Math. Verein., 
55, 39-51.
V.V. Petrov
(1995) Limit Theorems of Probability Theory. Sequences of Independent 
Random Variables, Clarendon Press, Oxford.
M.A. Picardello and W. Woess
(1987) 
“Martin boundaries of random walks: ends of trees and groups”, 
Trans. Amer. Math. Soc., 302, 185-205.
(1988) 
“Harmonic functions and ends of graphs”, Proc. Edinburg Math.
Soc., 31, 457-461.
(1989) 
“A converse to the mean value property on homogeneous trees”, 
Trans. Amer. Math. Soc., 311(1), 209-225.
(1992) 
“Martin boundaries of cartesian products of Markov chains”, 
Nagoya Math. J., 128, 153-169.
(1994) 
“The full Martin boundary of the bi-tree”, Ann. Probab. (To 
appear.)
H. Poincare
(1912) Calcul des Probabilites, 2eme ed, Gauthier-Villars, Paris.
G. Polya
(1921) “Uber eine Aufgabe der Wahrscheinlich-keitsrechnung betreffend 
die Irrfahrt im Strassenetz”, Math. Ann., 84, 149-160.
(1930) “Sur quelques points de la thoeorie des probabilitoes”, Ann. Inst.
II. Poincare', 1, 117-160.
G. Polya and G. Szego
(1951) Isoperimetric Inequalities of Mathematical Physics, Princeton 
University Press, Princeton.
N.U. Prabhu
(1965) Stochastic Processes, Macmillan, New York.
Yu. V. Prohorov
(1956) “Convergence of stochastic processes and limit theorems in the the­
ory of probability”, Teor. Verojatnost. i Primenen, 1, 177-238. (Russian.) 
(1961) “The method of characteristic functionals”, Proc. Fourth Berkeley 
Symp. Math. Statist. Prob. 11, University of California Press, Berkeley, pp. 
403-419.

288 Bibliography
Min Qian
(1979) “The extension of an elliptic differential operator and C semi­
groups”, Acta Math. Sincia, 22, 471-486.
Min Qian. Z. Guo, and M.Z. Guo
(1988) “Reversible diffusion process and Einstein relation”, Sci. Sinica, 
2. (Chinese.)
Min Qian. C.T. Huo, and Co.
(1979) Reversible Markov Processes, Hunan Scientific and Technical 
Press. (Chinese.)
Min Qian and Zhang Biao
(1984) 
“The multi-dimensional coupled diffusion process”, Acta Math. 
Appl. Sinica (English series), 1, 168-179.
Minping Qian
(1978) 
“The reversibility of Markov chain”, Acta Sci. Peking Unin., 4. 
(Chinese.)
(1979) 
“The circulation and nonequilibrium systems”, Acta Biophys, 4. 
(Chinese.)
Minping Qian and Min Qian
(1979) 
“Decomposition into a detailed balance and a circulation part 
of an irreversible stationary Markov chain”, Sci. Sinica, Special Issue II, 
69-79. (Chinese and English.)
(1982) 
“Circulation for recurrent Markov chain”, Z. Wahrsch. Verw. 
Gebiete, 59, 203-210.
(1985) 
“The entropy production and reversibility of Markov processes”, 
Kexue Tongbao 30(4).
(1987) 
“The entropy production, flux, reversibility and their relations 
with Markov processes”, in: Yu.A. Prohorov and V.V. Sazonov (Eds.): 
Proceedings of the First World Congress of Bernoul li Society of Mathe­
matical Statistics and Probability Theory, Tashkent, 1986, VNU Science 
Press, Utrecht.
(1988) 
“The entropy production and reversibility”, Proceedings of the 
Bernoulli Society Congress, VNU Science Press, Utrecht.
Minping Qian, Min Qian, and Z.C. Guo
(1981) 
“Minimal coupled diffusion processes”, Acta Math. Appl. Sinica, 
3(1). (Chinese.)
Minping Qian, C. Qian, and M. Qian
(1981) 
“The Markov chain as a model of Hill’s theory on circulation”, 
Sci. Sinica, 24(10). (Chinese.)
(1982) 
“Circulation distribution of a Markov chain”, Sci. Sinica, Ser. A, 
25(1). (Chinese.)

Bibliography
289
(1984) 
“Circulation of Markov chains with continuous time and the 
probability interpretation of some determinants”, Sci. Sinica, Ser. A, 
27(5). (Chinese.)
Minping Qian, Min Qian, and G.L. Gong
(1991) 
“The reversibility and the entropy production of Markov pro­
cesses”, Contemp Math., 118, 255-261.
Hong Qian
(1999) 
“A vector field formalism and analysis for a class of thermal 
ratchet”, Physical Review Letters.
(2000) 
“The mathematical theory of molecular motor movement and 
chemomechanical energy transduction”, J. Math. Chem., 57(3), 219-234.
M.M. Rao
(1993) 
Conditional Measures and Applications, Marcel Dekker, New York.
J.W.S. Rayleigh
(1870) 
“On the theory of resonance”, Philos. Trans., CLXI. (Reprinted 
in: Collected Scientific Papers. I, Cambridge, 1899, pp. 33-75.)
J.R. Reay
(1965) 
“Generalizations of a theorem of Caratheodory”, Amer. Math. 
Soc. Memoir, no. 54.
A. Renyi
(1967) “Probabilistic methods in analysis. I, II”, Mat. Lapok, 18, 5-35; 
175-194. (Hungarian.)
(1970) Probability Theory, Akademiai Kiado, Budapest.
G.E.H. Reuter
(1957) “Denumerable Markov processes and the associated contraction 
semigroups on l”, Acta Math., 97, 1-46.
(1959) “Denumerable Markov processes II”, J. London Math. Soc., 34, 
81-91.
(1962) “Denumerable Markov processes III”, J. London Math. Soc., 37, 
63-73.
D. Revuz
(1975) Markov Chains, North-Holland, Amsterdam.
F.M. Reza and S. Seely
(1959) Modern Network Analysis, McGraw-Hill, New York.
F. Riesz and B.Sz. Nagy
(1952) Lecons d’Analyse Fonctionnelle, Akademiai Kiado, Budapest.
R.T. Rockafellar
(1964) “Duality theorems for convex functions”, Bull. Amer. Math. Soc., 
70, 189-192.

290 Bibliography
(1969) “The elementary vectors of a subspace of R”, in: R.C. Bose 
and T.A. Dowling: “Combinatorial Mathematics and its Applications”, 
University North Caroline Press, pp. 104-127.
(1972) Convex Analysis, Princeton University Press, Princeton.
V.I. Romanovski
(1949) Discretnie tsepi Markova, GIL-TL, Moscow and Leningrad.
J.P. Roth
(1955) “An application of algebraic topology to numerical analysis: On 
the existence of a solution to the network problem”, Proc. Nat. Acad. Sci. 
U.S.A., 41, 518-521.
(1959) “An application of algebraic topology: Kron’s method of tearing”, 
Quart. Appl. Math., XVII, no. 1, 1-24.
J. Rotman
(1994) An Introduction to Homological Algebra, Academic Press, New 
York.
H.L. Royden
(1952) “Harmonic functions on open Riemann surfaces”, Trans. Amer. 
Math. Soc. 75, 40-94.
(1968) Real Analysis, 2nd edn., Macmillan, New York.
Yu.A. Rozanov
(1967) Stationary Random Processes, Holden-Day, San Francisco.
I.W. Sandberg
(1993) “Approximately-finite memory and the circle criterion”, Pro­
ceedings of the International Symposium on the Mathematical Theory of 
Networks and Systems, Regenburg, August 2-6, 1993.
J.J. Schaffer
(1955) “On unitary dilations of contractions”, Proc. Amer. Math. Soc., 
6, 322.
E. Schlesinger
(1992) “Infinite networks and Markov chains”, Bol l. Un. Mat. Ital., 7, 
6-B, 23-37.
J. Schnakenberg
(1976) “Network theory of microscopic and macroscopic behaviour of 
master equation systems”, Rev. Modern Phys., 48(4), 571-585.
F. Schweiger
(1975) “Some remarks on ergodicity and invariant measures”, Michigan 
Math. J., 22, 181-187.
E. Seneta
(1967) “On imbedding discrete chains in continuous time”, Austral. J. 
Statist., 9, 1-7.

Bibliography
291
(1968a) “The stationary distribution of a branching process allowing 
immigration; A remark on the critical case”, J. Roy. Statist. Soc. Ser. B, 
30, 176-179.
(1968b) “On recent theorems concerning the supercritical Galton-Watson 
process”, Ann. Math. Statist., 39, 2098-2102.
(1968c) “The principle of truncations in applied probability”, Comment.
Math. Univ. Carolin., 9(4), 533-539.
(1971) “On invariant measures for simple branching processes”, J. Appl. 
Probab., 8, 43-51.
(1980) “Computing the stationary distribution for infinite Markov 
chains”, Linear Algebra Appl., 34, 259-267.
(1981) Non-Negative Matrices, An Introduction to Theory and Applica­
tions, Allen & Unwin, London.
(1984) “Iterative aggregation: Convergence rate”, Econom. Lett., 14, 
357-361.
E. Seneta and D. Vere-Jones
(1966) 
“On Quasi-stationary distributions in discrete-time Markov chains 
with a denumerable infinity of states”, J. Appl. Probab., 3, 403-434. 
_ “ _
Z. Sidak
(1962) “Representations des probabilites de transition dans les chalnes a 
liaisons completes”, Casopis Pest. Mat., 87, 389-398.
(1967) 
“Classification of Markov chains with a general state space”, 
Transactions of the Fourth Prague Conference on Information Theory, 
etc. (Prague, 1965), pp. 547-571, Academia, Prague.
A.V. Skorohod,
(1965) “Constructive methods of specifying stochastic processes”, 
Uspekhi Mat. Nauk, 20(3), 67-87. (Russian.)
J.L. Snell
(1959) 
“Finite Markov chains and their applications”, Amer. Math. 
Monthly, 66, 99-104.
P.M. Soardi
(1990) 
“Parabolic networks and polynomial growth”, Colloq. Math., 
LX/LXI, 65-70.
(1994a) “Networks and random walks”, in: S. Kalpazidou (Ed.): “Selected 
Talks Delivered at the Department of Mathematics of the Aristotle Uni­
versity (1994)”, Aristotle University Press, Thessaloniki.
(1994b) Potential Theory on Infinite Networks, Lecture Notes in Mathe­
matics, Springer-Verlag, New York.
P.M. Soardi and W. Woess
(1991) 
“Uniqueness of currents in infinite resistive networks”, Discrete 
Appl. Math., 31(8), 37-49.

292
Bibliography
P.M. Soardi and M. Yamasaki
(1993) “Classification of infinite networks and its application”, Circuits 
Systems Signal Process., 12(1), 133-149.
F. Solomon
(1994) “Random walks in a random environment”, Ann. Probab., 3(1), 
1-31.
F. Spitzer
(1964) Principles of Random Walk, Van Nostrand, Princeton.
(1974) “Recurrent random walk of an infinite particle system”, Trans. 
Amer. Math. Soc., 198, 191-199.
V. Statulevicius
(1969-1970) “Limit theorems for sums of random variables related to 
a Markov chain. I, II, III”, Litovsk. Mat. Sb., 9, 345-362, 635-672; 10, 
161-169. (Russian.)
V. Statulevicius and A. Aleskeviciene
(1995) “On large deviations in the Poisson approximations”, Probab. 
Theory Appl., 38, (2). (To appear.)
W. 
Thomson and P.G. Tait
(1879) Treatise on Natural Philosophy, Cambridge University Press, 
Cambridge.
P. Rodriguez del Tlo and M.C. Valsero Blanco
(1991) “A characterization of reversible Markov chains by a rotational 
representation”, Ann. Probab., 19(2), 605-608.
Ch.P. Tsokos
(1973) “Sufficient conditions for the existence of random solutions to a 
nonlinear perturbed stochastic integral equation”, Proceedings of the C. 
Caratheodory International Symposium, Athens, 1973, Greek Mathemat­
ical Society, Athens, 1974, pp. 611-622.
(1976) “On the behaviour of nonlinear discrete stochastic systems”, Ap­
plications and Research in Information Systems and Sciences, Proceedings 
of the First International Conference, University of Patras, Patras, 1976, 
Vol. 3, pp. 805-807.
Ch.P. Tsokos and D.B. McCallum
(1972) “Lp stability of a nonlinear stochastic control system”, Internat. 
J. Systems Sci., 3, 215-223.
Ch.P. Tsokos and J.S. Milton
(1974) “A stochastic system for communicable diseases”, Internat. J. 
Systems Sci., 5, 503-509.
(1976) Probability Theory with the Essential Analysis, Applied Mathe­
matics and Computation, No. 10, Addison-Wesley, Reading, MA.

Bibliography
293
Ch.P. Tsokos and A.N.V. Rao
(1977a) “Existence and boundedness of random solutions to stochastic 
functional integral equations”, Acta Math. Sci. Hungar., 29(3, 4), 283­
288.
(1977b) “Stochastic stability of controlled motion”, Modern Trends in 
Cybernetics and Systems, Proceedings of the Third International Congress, 
Bucharest, 1975, Springer-Verlag, Berlin, Vol. II, pp. 467-474.
Ch.P. Tsokos, G.W. Schultz, and A.N.V. Rao
(1978) “Statistical properties of a linear stochastic system”, Inform. and 
Control 39(1), 92-117.
Ch.P. Tsokos, A.N.V. Rao, and R.A. Tourgee
(1978) “Stochastic systems and integral inequalities”. J. Math. Phys., 
19(12), 2634-2640.
Ch.P. Tsokos, J. Hess, H. Kagiwada, and R.E. Kalaba
(1979) “Cooperative dynamic programming”, Appl. Math. Comput., 
5(9), 69-74.
Ch.P. Tsokos and S.W. Hinkley
(1974) “A stochastic model for chemical equilibrium”. Math. Biosci., 21, 
85-102.
V.A. Uspensky
(1992) “Complexity and entropy: An introduction to the theory of 
Kolmogorov complexity”, in: O. Watanabe (Ed.) “Kolmogorov Complexity 
and its Relations to Computational Complexity Theory”, Springer-Verlag, 
Berlin.
V.A. Uspensky and A.L. Semenov
(1981) “What are the gains of the theory of algorithms: Basic develop­
ments connected with the concept of algorithm and with its application 
in mathematics?” in: A.P. Ershov and D.E. Knuth (eds.): “Algorithms in 
Modern Mathematics and Computer Science”, Lecture Notes in Computer 
Science, No. 122, Springer-Verlag, New York.
(1993) Algorithms: Main Ideas and Applications, Kluwer Academic, 
Dordrecht.
N.Th. Varopoulos
(1983) “Brownian motion and transient groups”, Ann. Inst. Fourier 
(Grenoble), 33(2), 241-261.
(1984a) “Chaines de Markov et inegalites isoperimetriques”, C. R. Acad. 
Sci. Paris, A298, 233-236.
(1984b) “Isoperimetric inequalities and Markov chains”, J. Funct. Anal., 
63, 215-239.
(1984c) “Brownian motion and random walks on manifolds”, Ann. Inst. 
Fourier (Grenoble), 34(2), 243-269.

294 Bibliography
(1985) “Long range estimates for Markov chains”, Bull. Sci. Math., 2eme 
serie, 109, 225-252.
(1991) “Groups of superpolynomial growth”, preprint, Universite Paris 
VI.
O. Veblen
(1931) Analysis Situs, Vol. V, Part II, 2nd edn., American Mathematical 
Society Colloquium Publications, New York.
D. Vere-Jones
(1968) “Ergodic properties of nonnegative matrices II”, Pacific J. Math., 
26, 601-620.
I. 
Vladimirescu
(1982) 
“On the state-classification of a homogeneous Markov chain of 
order two with an arbitrary state space”, Stud. Cerc. Mat., 35(6), 529-543. 
(Romanian.)
(1984) 
“The periodicity for homogeneous Markov chains of order two”, 
Stud. Cerc. Mat., 36(6), 559-561. (Romanian.)
(1985) 
“Regular homogeneous Markov chains of order two”, Ann. Univ. 
Craiova, Ser. Mat., Fiz., Chim., 13, 59-63. (Romanian.)
(1989) 
“Some aspects concerning the asymptotic behaviour of homoge­
neous double Markov chains”, Ann. Univ. Craiova, Ser. Mat., Fiz., Chim., 
17, 25-30.
(1990) 
“Double grouping Markov chains”, Ann. Univ. Craiova. Ser.
Mat., Fiz., Chim., 18.
G.G. Vranceanu
(1969) 
Interpretation Geometrique des Processus Probabilistiques Conti- 
nus, Gauthier-Villars, Paris.
P. Walters
(1982) 
An Introduction to Ergodic Theory, Springer-Verlag, New York.
P. Whittle
(1975) 
“Reversibility and acyclicity”. in: J. Gani (Ed.): “Perspectives in 
Probability and Statistics: Papers in Honour of M.S. Bartlett”, Applied 
Probability Trust, Sheffield, pp. 217-224.
(1986) 
Systems in Stochastic Equilibrium, Wiley, Chichester.
D. Williams
(1964) 
“On the construction problem for Markov chains”, Z. Wahrsch. 
Verw. Gebiete, 3, 227-246.
(1967) 
“A note on the Q-matrices of Markov chains”, Z. Wahrsch. Verw. 
Gebiete, 7, 116-121.
W. Woess
(1986) “Transience and volumes of trees”, Arch. Math., 46(4), 184-192.

Bibliography
295
(1989) 
“Graphs and groups with tree-like properties”, J. Combin. Theory 
Ser. B, 68(7), 271-301.
(1991) “Topological groups and infinite graphs”, Discrete Math., 95(2, 
4), 373-384.
(1994) “Random walks on infinite graphs and groups—A survey on 
selected topics”, Bull. London Math. Soc. (To appear.)
M. Yamasaki
(1979) “Discrete potentials on an infinite network”, Mem. Fac. Sci. 
Shimane Univ., 13, 31-44.
N.M. Yanev
(1990) 
“Limit theorems for sums of a random number of random variables 
and applications in branching processes”, in: S. Kalpazidou (Ed.): “Selected 
Talks on Stochastic Processes Delivered at the Department of Mathematics 
of the Aristotle University (1990)”, Aristotle University Press, Thessaloniki.
R.Z. Yeh
(1970) “A geometric proof of Markov ergodic theorem”, Proc. Amer. 
Math. Soc., 26, 335-340.
K. Yosida
(1965) Functional Analysis, Springer-Verlag, Berlin.
K. Yosida and S. Kakutani
(1941) “Operator-theoretical treatment of Markoff’s process and mean 
ergodic theorem”, Ann. of Math. 42(2), 188-228.
A.H. Zemanian
(1965) Distribution Theory and Transform Analysis, McGraw-Hill, New 
York; republished by Dover, New York, 1987.
(1966) “Inversion formulas for the distributed Laplace transformation”, 
SIAM J. Appl. Math., 14, 159-166.
(1968) Generalized Integral Transformations, Wiley, New York; repub­
lished by Dover, New York, 1987.
(1974a) “Countably infinite networks that need not be locally finite”, 
IEEE Trans. Circuits and Systems, CAS-21, 274-277.
(1974b) “Infinite networks of positive operators”, Circuit Theory Appl., 
69-74.
(1974c) “Continued fractions of operator-valued analytic functions”, J. 
Approx. Theory, 11, 319-326.
(1976a) “Infinite electrical networks”, Proc. IEEE, 64, 6-17.
(1976b) “The complete behaviour of certain infinite networks under 
Kirchhoff’s node and loop laws”, SIAM J. Appl. Math., 30, 278-295.
(1979) “Countably infinite, time-varying, electrical networks”, SIAM J. 
Math. Anal., 10, 1193-1198.
(1987) “Infinite electrical networks with finite sources at infinity”, IEEE 
Trans. Circuits and Systems, CAS-34, 1518-1534.

296
Bibliography
(1991) 
Infinite Electrical Networks, Cambridge University Press, Cam­
bridge.
(1992) 
“Transfinite random walks based on electrical networks”, in: 
S. Kalpazidou (Ed.): “Selected Talks Delivered at the Department of 
Mathematics of the Aristotle University (1992)”, Aristotle University 
Press, Thessaloniki.
(1993) 
“Transfinite graphs and electrical networks”, Trans. Amer. Math. 
Soc. (To appear.)
(1996) 
Transfiniteness-for Graphs, Electrical Networks, and Random 
Walks, Birkhauser-Boston, Cambridge, Massachusetts.
(1997) 
“Nonstandard electrical networks and the resurrection of 
Kirchhoff’s laws, IEEE Transactions on Circuits and Systems-Part I: 
Fundamental Theory and Applications, 44, 221-233.
(2001a) Pristine Transfinite Graphs and Permissive Electrical Networks, 
Birkhauser-Boston, Cambridge, Massachusetts.
(2001b) “Hyperreal transients in transfinite RLC networks”, Int. J. 
Circuit Theory Applic., 29, 591-605.
(2003a) “Hyperreal transients in transfinite distributed transmission lines 
and cables”, Int. J. Circuit Theory Applic., 31, 473-482.
(2003b) “Nonstandard graphs”, Graph Theory Notes N.Y. XLIV, 14-17.
(2003c) “Hyperreal operating points in transfinite resistive networks”, 
Circuits Syst. Signal Proc., 22, 589-611.
A.H. Zemanian and P. Subramanian
(1983) “A solution for an infinite electrical network arising from various 
physical phenomena”, Internat. J. Circuit Theory Appl., 11, 265-278.
B.D. Calvert and A.H. Zemanian
(2000) “Operating points in infinite nonlinear networks approximated by 
finite networks”, Trans. Amer. Math. Soc., 352, 753-780.

Index
Ahlfors, L.V., 23
-type criterion for recurrence, 25,
27, 208
algorithm, 12
deterministic (nonrandomized),
39
Kolmogorov classification, 39
probabilistic (randomized), 39
Backward-forward
passage-functions, 26
Balanced functions
the cycle (circuit)
representations, 10
the cycloid decompositions, 131
Balanced measures, 174, 178-180
Banach spaces
on edges, 194-195
on cycles, 145-152, 161
base of Betti circuits, 159-160,
253-254
Betti
circuits, 65
cycloids, 135, 136
dimension, 61
edges, 61
one cycles, 62
Betti’s base
of circuits, 159-160, 253-254
of cycloids, 135, 193
of one-cycles, 65
Betti number
of a graph, 62, 135
of a Markov chain, 63-65
of a stochastic matrix, 63-65
Betti-type circuit decomposition
(representation)
of stochastic matrices, 65 
Betti-type cycloid decomposition
(representation)
of balance functions, 135-137
of Markov chains, 141-143
Birkhoff, G., 15
ergodic theorem, 15, 39
theorem for doubly stochastic
matrices, 15
boundary relation, 62
Caratheodory, C., 56, 60, 61, 70, 72
dimensional theorem, 56, 60, 236

298 Index
Caratheodory-type circuit (cycle) 
decompositions (representations),
60, 173, 237, 241, 249, 255
of finite recurrent matrices, 60, 
72
of rotational representations, 
71
Caratheodory-type dimension of a 
circuit
decomposition, 60
Chung, K.L., 34, 48, 74, 76, 84, 91, 
225, 291
circuit, 3, 4
circuit representation
of the induced circuit chain, 
47-53
circuit (cycle) representations of 
measures, 163, 178-182, 195
circuit chains, 17-18
with denumerable state space,
18, 23-27
with finite state space, 18, 22 
circuit functions, 4, 62, 79, 82 
circuit (cycle) matrix, 60 
circuit processes, 73, 93, 101 
circuits passing
points, 8
sequences, 8
circulation distribution
of Markov chains, 35-37
on trajectories up to n,30 
coding problem, 40, 247 
complexity of a rotational
representation, 255-259 
connectedness
of a collection of circuits, 24
of a graph, 61
continued fraction expansion
recurrence criterion by weighted 
cycles, 221
current, 15
cycle, 4
cycle-axes, 248, 250-251
cycle (circuit) decomposition
of stochastic matrices, 56-69
of Betti-type, 69
of Caratheodory-type, 60
the deterministic one, 41
the homologic one, 67
the probabilistic one, 37-39
cycle (circuit) generating 
equations, 10-16
cycle (circuit) representation
of balance functions, 10
of Betti-type, 69
of Caratheodory-type, 60
of higher-order Markov chains, 
101-106
of Markov chains, 47, 48, 49, 56, 
57, 58
of Q-matrices, 88-91
of stochastic matrices, 47, 48, 49, 
51, 57, 60
of transition matrix functions,
74, 85-88
cycle formula
as disintegration formula, 197
cycle measures, 163
cycle skipping rate, 36-37, 213,
227, 229
cycle weight functions, 36, 86
cyclic group of rotations, 5
cycloid, 131
cycloid transition
equations, 137-141
functions, 138
decomposition of a Markov chain 
into a direct sum of cycle 
chain and noncycle chain, 
192
Decycling method, 31
Derived chain, 29-35
Derriennic, Y., 15, 24, 27, 29, 36,
38, 45, 47, 50, 215
deterministic circuit representations 
of countable Markov chains, 40, 
122
of denumerable stochastic 
matrices, 40

Index
299
of finite Markov chains, 55
of finite stochastic matrices, 66 
dimension
of Betti-type, 56, 70
of Caratheodory-type, 60
the maximal one, 252
the rotational one for recurrent
matrices, 259
the rotational one for transition
matrix functions, 264
directed
circuit, 3-10
cycle, 5, 7, 30, 152-157
edge, 4-5, 12-13, 61, 131, 136,
148, 158, 188, 227, 253
rolling-circuit, 117-119
disintegration of measures in term 
of the cycle formula, 197-203
Elementary circuits, 120, 122,
125-130, 166
entropy production
of a Markov chain, 210-212
of a nonequilibrium system, 210
in terms of circuits, 211
ergodic theorem
of Birkhoff, 15, 19, 39
of Hopf, 39
existence theorem
of higher-order circuit chains, 
104
of the weight functions, 113 
extensions of balanced functions, 
168
Feller’s process, 89, 93
Flanders, H., 40
flow, 15
Foster-Kendall theorem, 218 
Fourier series on directed cycles, 
152-157
Fubini-type theorem in term of 
cycles, 181
Growth function, 25, 209, 265
Halmos’s unitary dilations, 95, 
255
higher-order (multiple) 
countable circuit chains, 101 
finite circuit chains, 101 
Markov chains, 101, 127
Hopf’s ergodic theorem, 39
Incidence matrix
of the edges and circuits, 71
of the edges and points, 67
Induced
chain, 47
circuit chain, 47-53
induced circuit chain, 47-53 
induced transition probability, 
48
interpretations of the cycle 
representations
the homologic interpretation, 
187-193
the algebraic interpretation, 
192-193
the Banach space approach, 
194-195
the measure-theoretic 
interpretation, 195-197
as disintegration of measures, 
197-203
inverse Fourier transform of 
probabilistic
weight-functions, 161
inverse (reverse)
circuit, 20, 22
circuit chains, 20, 22
rolling-circuit, 117
irreducible matrix, 255
irreducibility
in terms of circuits, 19, 23
in terms of stochastic matrices, 
19, 142
irreducibility of higher-order 
Markov, chains, 104
Jordan’s curve, 3, 5

300 Index
Kirchhoff’s current law, 15, 24,
203, 229-230
Kolmogorov, A.N., xi, xii
Kolmogorov’s
“chaos”, x
limits p'ij (0+), 74, 88
theory of complexities, x, xii
Kolmogorov’s algorithms
deterministic (nonrandomized), 
39
probabilistic (randomized), 39
Labelings of rotational
representations
of S. Alpern, 238
of J. Haigh, 257
of S. Kalpazidou, 243
of P. Rodriguez del Tlo and M.C.
Valsero Blanco, 259
length of description
of a collection of directed
circuits, 254
of a rotational partition, 254 
Levy’s theorem on sgn pij (t)
the dichotomy, 225
in terms of circuits, 226-228
Markov chain, process, 17
Markov property, 19
mapping stochastic matrices into
partitions, 247
matrix
operators, 13
stochastic, 7, 8, 19, 37
maximal dimension
of Betti, 252
of the rotational partition, 253 
m-elementary circuits, 119, 128-129 
minimal process, 91
Nagy’s theorem on linear
contractions, 95 
Nash-Williams’s criterion, 25-26 
n-cell, 3
network, 24
One-chain, 62
one-cycle, 62
orthogonal cycle transforms
of stochastic matrices, 157-161
Passage-function
associated with a circuit, 68
associated with a cycloid, 132
associated with a rolling-circuit, 
120
as characteristic function, 
163-167
as balance function, 167-171
passage-matrix, 74, 158
period
of circuit functions, 5
of directed circuits, 5
physical interpretation of the
weighted circuits, 228
polygonal line, 164
probabilistic cycle representation
of Markov chains, 37-39
of Markov processes, 90
of Q-matrices, 88
of stochastic matrices, 90
product formula, 164
Q-matrix, 88-91
Qian, Minping, 31-33, 36-37,
100
Qians’s derived process, 29-35
Radon-Nikodym derivative
as circuit weight function, 195, 
197, 199-200
Recurrence criterion
of Ahlfors-type, 25, 27
of Derriennic for unidimensional, 
215
random walks, 215
of Nash-Williams-type, 25, 27
in terms of circuits is given, 
207
reverse of a circuit, 5
reversible Markov chains, 25, 39 

Index
301
reversibility criterion
of Kolmogorov, 212
for Markov chains, 210-212
in terms of circuits, 212
in terms of rotational
representations, 259 
rolling-circuit, 117-118 
rotational dimension
of stochastic matrices, 250
of transition-matrix functions, 
264
rotational problem, 8
rotational representations, 255
of irreducible stochastic
matrices, 250, 253
of recurrent stochastic matrices, 
233, 236, 241, 251, 255
of transition-matrix functions, 
264
rotations, 5-6
Sample weighted cycles, 36
Schnakenberg, J., 210-211
Seneta, E., 19, 23, 25, 234 
sequences satisfying the cycle 
formula, 152-153, 155-156, 160 
simple circuit processes associated 
with
higher-order circuit chains, 118 
simple Markov chains attached
to a higher-order circuit chain, 
102
a higher-order Markov chain, 102 
solution to the rotational problem
of S. Alpern, 235
of Joel E. Cohen, 234
of J. Haigh, 257-258
of S. Kalpazidou, 249, 253-254
of P. Rodriguez del Tlo and M.C.
Valsero Blanco, 259
spectral representations, 98 
states of higher-order Markov 
chains
accessible from other states, 
104
aperiodic, 104
fully accessible, 104
periodic, 104
Stone’s theorem on spectral 
representations, 99
Taboo Green function, 33, 86 
transformations
of circuits into circle-arcs, 233, 
240-247
of circuit-weights into 
arc-weights, 233, 240-247
of stochastic matrices into 
partitions, 247-250
unidimensional random walks in 
random
environment, 215
Unitary dilations, 93-95
Varopoulos’s counterexample, 27 
vector spaces generated by the 
passage-functions, 14
Weak convergence, 39, 45
weight functions, 76-79
weights
of circuits, xiii, 28
of cycles, 36
of edges, xi, 26
of rolling-circuits, 117
weighted transition intensity 
matrix, 85
Zemanian, A.H., 15, 24, 40

Applications of Mathematics
(continued from page ii)
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
Musiela/Rutkowski, Martingale Methods in Financial Modeling: Theory and
Application (1997)
Yin/Zhang, Continuous-Time Markov Chains and Applications (1998) 
Dembo/Zeitouni, Large Deviations Techniques and Applications, Second Ed. 
(1998)
Karatzas/Shreve, Methods of Mathematical Finance (1998)
Fayolle/Iasnogorodski/Malyshev, Random Walks in the Quarter Plane (1999)
Aven/Jensen, Stochastic Models in Reliability (1999)
Hernandez-Lerma/Lasserre, Further Topics on Discrete-Time Markov Control
Processes (1999)
Yong/Zhou, Stochastic Controls: Hamiltonian Systems and HJB Equations (1999)
Serfozo, Introduction to Stochastic Networks (1999)
Steele, Stochastic Calculus and Financial Applications (2000)
Chen/Yao, Fundamentals of Queueing Networks: Performance, Asymptotics, and 
Optimization (2001)
Kushner, Heavy Traffic Analysis of Controlled Queueing and Communication
Networks (2001)
Fernholz, Stochastic Portfolio Theory (2002)
Kabanov/Pergamenshchikov, Two-Scale Stochastic Systems (2003)
Han, Information-Spectrum Methods in Information Theory (2003)
Asmussen, Applied Probability and Queues (2003)
Robert, Stochastic Networks and Queues (2003)
Glasserman, Monte Carlo Methods in Financial Engineering (2003)
Sethi/Zhang/Zhang, Average-Cost Control of Stochastic Manufacturing Systems
(2004)
Yin/Zhang, Discrete-Time Markov Chains: Two-Time-Scale Methods and 
Applications (2004)


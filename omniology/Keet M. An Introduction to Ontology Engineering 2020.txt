An Introduction to 
Ontology Engineering
C. Maria Keet
v1.5,  2020


An Introduction to
Ontology Engineering
C. Maria Keet

Keywords: Ontology Engineering, Ontology, ontologies, Knowledge base, Descrip-
tion Logics, OWL, Semantic Web, Ontology Development
Copyright © 2020 by Maria Keet, except:
The copyright of Chapter 11 is held by Zubeida C. Khan and C. Maria Keet
The copyright of Appendix A.1 is held by Zola Mahlaza and C. Maria Keet
This work is licensed under a Creative Commons Attribution 4.0 International Li-
cense (CC BY 4.0). To view a copy of this license, visit https://creativecommons.
org/licenses/by/4.0/.
This textbook is typeset in LATEX
The website for this textbook is https://people.cs.uct.ac.za/~mkeet/OEbook/
Cover design by Maria Keet
Cover photo (by the author): A view from Table Mountain, Cape Town, SA

Contents
Preface
vii
Preface to v1
ix
How to use the book
xi
1
Introduction
1
1.1
What does an ontology look like? . . . . . . . . . . . . . . . . . . .
2
1.2
What is an ontology? . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.2.1
The deﬁnition game
. . . . . . . . . . . . . . . . . . . . . .
4
1.2.2
Some philosophical notes on ontologies . . . . . . . . . . . .
7
1.2.3
Good, not so good, and bad ontologies . . . . . . . . . . . .
8
1.3
What is the usefulness of an ontology?
. . . . . . . . . . . . . . . .
9
1.3.1
Data and information system integration . . . . . . . . . . .
10
1.3.2
Ontologies as part of a solution to other problems . . . . . .
14
1.3.3
Success stories . . . . . . . . . . . . . . . . . . . . . . . . . .
17
1.4
Outline and usage of the book . . . . . . . . . . . . . . . . . . . . .
20
1.5
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
1.6
Literature and reference material
. . . . . . . . . . . . . . . . . . .
22
I
Logic foundations for ontologies
23
2
First order logic and automated reasoning in a nutshell
25
2.1
First order logic syntax and semantics
. . . . . . . . . . . . . . . .
25
2.1.1
Syntax . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
2.1.2
Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
2.2
Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
2.2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
2.2.2
Basic idea . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
2.2.3
Deduction, abduction, and induction . . . . . . . . . . . . .
36
iii

2.2.4
Proofs with tableaux . . . . . . . . . . . . . . . . . . . . . .
37
2.3
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
2.4
Literature and reference material
. . . . . . . . . . . . . . . . . . .
44
3
Description Logics
45
3.1
DL primer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
3.1.1
Basic building blocks of DL ontologies
. . . . . . . . . . . .
47
3.1.2
Constructors for concepts and roles . . . . . . . . . . . . . .
49
3.1.3
Description Logic semantics . . . . . . . . . . . . . . . . . .
53
3.2
Important DLs
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
3.2.1
A basic DL to start with: ALC
. . . . . . . . . . . . . . . .
56
3.2.2
The DL SROIQ . . . . . . . . . . . . . . . . . . . . . . . .
58
3.2.3
Important fragments of SROIQ
. . . . . . . . . . . . . . .
59
3.3
Reasoning services
. . . . . . . . . . . . . . . . . . . . . . . . . . .
61
3.3.1
Standard reasoning services
. . . . . . . . . . . . . . . . . .
61
3.3.2
Techniques: a tableau for ALC
. . . . . . . . . . . . . . . .
63
3.4
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
3.5
Literature and reference material
. . . . . . . . . . . . . . . . . . .
66
4
The Web Ontology Language OWL 2
67
4.1
Standardising an ontology language . . . . . . . . . . . . . . . . . .
68
4.1.1
Historical notes . . . . . . . . . . . . . . . . . . . . . . . . .
68
4.1.2
The OWL 1 family of languages . . . . . . . . . . . . . . . .
70
4.2
OWL 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
4.2.1
New OWL 2 features . . . . . . . . . . . . . . . . . . . . . .
74
4.2.2
OWL 2 Proﬁles . . . . . . . . . . . . . . . . . . . . . . . . .
76
4.2.3
OWL 2 syntaxes
. . . . . . . . . . . . . . . . . . . . . . . .
77
4.2.4
Complexity considerations for OWL . . . . . . . . . . . . . .
79
4.3
OWL in context . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
4.3.1
OWL and the Semantic Web . . . . . . . . . . . . . . . . . .
81
4.3.2
The Distributed ontology, model, and speciﬁcation language
DOL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
4.3.3
Common Logic
. . . . . . . . . . . . . . . . . . . . . . . . .
84
4.4
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
84
4.5
Literature and reference material
. . . . . . . . . . . . . . . . . . .
88
II
Developing good ontologies
89
5
Methods and Methodologies
91
5.1
Methodologies for ontology development
. . . . . . . . . . . . . . .
92
5.1.1
Macro-level development methodologies . . . . . . . . . . . .
93
5.1.2
Micro-level development . . . . . . . . . . . . . . . . . . . .
97
5.2
Methods to improve an ontology’s quality
. . . . . . . . . . . . . . 101
5.2.1
Logic-based methods: explanation and justiﬁcation
. . . . . 101
5.2.2
Philosophy-based methods: OntoClean to correct a taxonomy 103
iv

5.2.3
Combining logic and philosophy: role hierarchies . . . . . . . 104
5.2.4
Heuristics: OntOlogy Pitfall Scanner OOPS! . . . . . . . . . 106
5.2.5
Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
5.3
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
5.4
Literature and reference material
. . . . . . . . . . . . . . . . . . . 113
6
Top-down Ontology Development
115
6.1
Foundational ontologies . . . . . . . . . . . . . . . . . . . . . . . . . 115
6.1.1
Typical content of a foundational ontology . . . . . . . . . . 117
6.1.2
Several foundational ontologies
. . . . . . . . . . . . . . . . 121
6.1.3
Using a foundational ontology . . . . . . . . . . . . . . . . . 124
6.2
Part-whole relations
. . . . . . . . . . . . . . . . . . . . . . . . . . 130
6.2.1
Mereology . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
6.2.2
Modelling and reasoning in the context of ontologies . . . . . 132
6.3
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136
6.4
Literature and reference material
. . . . . . . . . . . . . . . . . . . 138
7
Bottom-up Ontology Development
139
7.1
Relational databases and related ‘legacy’ KR . . . . . . . . . . . . . 140
7.2
From spreadsheets to OWL
. . . . . . . . . . . . . . . . . . . . . . 144
7.3
Thesauri . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
7.3.1
Converting a thesaurus into an ontology
. . . . . . . . . . . 146
7.3.2
Avoiding ontologies with SKOS . . . . . . . . . . . . . . . . 148
7.4
Text processing to extract content for ontologies . . . . . . . . . . . 148
7.5
Other semi-automated approaches . . . . . . . . . . . . . . . . . . . 150
7.6
Ontology Design Patterns
. . . . . . . . . . . . . . . . . . . . . . . 152
7.7
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
7.8
Literature and reference material
. . . . . . . . . . . . . . . . . . . 159
III
Advanced topics in ontology engineering
161
8
Ontology-Based Data Access
165
8.1
Introduction: Motivations
. . . . . . . . . . . . . . . . . . . . . . . 166
8.2
OBDA design choices . . . . . . . . . . . . . . . . . . . . . . . . . . 167
8.3
An OBDA Architecture . . . . . . . . . . . . . . . . . . . . . . . . . 168
8.4
Principal components . . . . . . . . . . . . . . . . . . . . . . . . . . 170
8.5
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174
8.6
Literature and reference material
. . . . . . . . . . . . . . . . . . . 175
9
Ontologies and natural languages
177
9.1
Toward multilingual ontologies . . . . . . . . . . . . . . . . . . . . . 178
9.1.1
Linking a lexicon to an ontology . . . . . . . . . . . . . . . . 178
9.1.2
Multiple natural languages . . . . . . . . . . . . . . . . . . . 180
9.1.3
Infrastructure for multilingual, localised, or internationalised
ontologies . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
9.2
Ontology verbalisation . . . . . . . . . . . . . . . . . . . . . . . . . 187
v

9.2.1
Template-based approach
. . . . . . . . . . . . . . . . . . . 187
9.2.2
Reusing the results for related activities
. . . . . . . . . . . 189
9.3
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
9.4
Literature and reference material
. . . . . . . . . . . . . . . . . . . 191
10 Advanced Modelling with Additional Language Features
193
10.1 Uncertainty and vagueness . . . . . . . . . . . . . . . . . . . . . . . 193
10.1.1 Fuzzy ontologies . . . . . . . . . . . . . . . . . . . . . . . . . 194
10.1.2 Rough ontologies . . . . . . . . . . . . . . . . . . . . . . . . 197
10.2 Time and Temporal Ontologies
. . . . . . . . . . . . . . . . . . . . 201
10.2.1 Why temporal ontologies? . . . . . . . . . . . . . . . . . . . 202
10.2.2 Temporal DLs . . . . . . . . . . . . . . . . . . . . . . . . . . 203
10.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
10.4 Literature and reference material
. . . . . . . . . . . . . . . . . . . 208
11 Ontology modularisation
211
11.1 Deﬁning modularisation
. . . . . . . . . . . . . . . . . . . . . . . . 211
11.2 Module dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
11.2.1 Use-cases
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
11.2.2 Types
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
11.2.3 Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216
11.2.4 Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217
11.2.5 Evaluation metrics . . . . . . . . . . . . . . . . . . . . . . . 218
11.3 Modularisation framework . . . . . . . . . . . . . . . . . . . . . . . 223
11.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225
11.5 Literature and reference material
. . . . . . . . . . . . . . . . . . . 227
Bibliography
229
A Tutorials
249
A.1 OntoClean in OWL with a DL reasoner . . . . . . . . . . . . . . . . 249
A.2 An OBDA system for elephants . . . . . . . . . . . . . . . . . . . . 256
B Assignments
257
B.1 Practical Assignment: Develop a Domain Ontology
. . . . . . . . . 258
B.2 Project Assignment . . . . . . . . . . . . . . . . . . . . . . . . . . . 260
C OWL 2 Proﬁles features list
265
D Complexity recap
269
E Answers of selected exercises
273
About the author
289
vi

Preface
The ﬁrst version of this textbook received encouraging feedback and made it into
print with the non-proﬁt publisher College Publications. The print version is still
actual and relevant. This version 1.5 does have a few additions and corrections that
seemed to deserve more than a 0.1-point notch. The main additions consist of about
10% increased main content, consisting main of 10% more exercises in Chapters 2-
9, a new preliminary Chapter 11 on ontology modularisation, and a new section on
challenges for multilingualism (§ 9.1.3). The appendix has increased with two new
tutorials—on OntoClean and on OBDA—and more answers to selected exercises.
Also, typos and related infelicities that have been found were corrected, and in
the grey area of copyright issues, I tweaked a few more ﬁgures a bit just to be on
the safe side. Altogether, this caused an increase of 36 pages. If the reader wants
access to v1 nonetheless: the pdf is still available as OEbookV1.pdf.
In addition the the book’s content in the pdf ﬁle, the website accompanying the
textbook now has has more materials, notably the slides (in pdf, LATEX source, and
ppt), new ontologies for the tutorials and exercises, and additional software for the
exercises. This material can be found at https://people.cs.uct.ac.za/~mkeet/
OEbook/. The website also contains a page with instructions for improved accessi-
bility for the visually impaired, in particular for low-cost screen reader training to
handle the Description Logics symbols.
As with the previous v1, more can indeed be added, but there are certain time
constraints. For contributions to the additions, I would like to thank former and
current students Zubeida Khan, Zola Mahlaza, Frances Gillis-Webber, Michael
Harrison, Toky Raboanary, and Joan Byamugisha, as well as the grant from the
“Digital Open textbooks for Development” (DOT4D) Project that made some of
the additions possible. Also, I would like to thank the constructive feedback on the
ontologies by Ludger Jansen, which has led to improvements of the content.
Cape Town, South Africa
C. Maria Keet
February, 2020
vii


Preface to v1
This book is my attempt at providing the ﬁrst textbook for an introduction in on-
tology engineering. Indeed, there are books about ontology engineering, but they
either promote one speciﬁc ontology or methodology only, are handbooks, or are
conference proceedings. There have been collaborative initiatives that aimed for a
generic introduction, yet they have not made it to the writing stage. Problems to
overcome with such an endeavour—aside from the diﬃcult task of ﬁnding time to
write it—are, mainly, to answer the questions of 1) which topics should an intro-
ductory textbook on ontology engineering cover? and 2) how comprehensive should
an introduction be? The answer to the ﬁrst question is diﬀerent for the diﬀerent
audiences, in particular with respect to emphases of one topic or another and the
order of things. The intended audience for this textbook are people at the level of
advanced undergraduate and early postgraduate studies in computer science. This
entails, for instance, that I assume the reader will know what UML class diagrams
and databases are. As computing degrees seem to have a tendency to have be-
come less theoretical, a solid background in logic, reasoning, and computational
complexity is not expected, so a gentle introduction (or recap, as it may be) of the
core concepts is provided. There are no lengthy philosophical debates in any of the
chapters, but philosophical aspects are presented and discussed mainly only inso-
far as they are known to aﬀect the engineering side. There still will be sections of
interest for philosophers and domain experts, but they may prefer to work through
the chapters in a diﬀerent order (see ‘how to use the book’).
As to how comprehensive an introduction to ontology engineering should be,
there is no good answer. At least for this ﬁrst version, the aim is for a semester-long
course, where each chapter can be covered in a week and does not require too much
reading of core material, with the core material being the contents of the chapter.
For an introductory course at undergraduate level, the citations in the text may
be ignored, but it serves to read 1-3 scientiﬁc papers per chapter for more detail,
especially if this book is used in a postgraduate course. This makes also sense in
the light that ontology engineering is still an active ﬁeld of research—hence, some
basics may change still—and it allows for ﬂexibility in a course programme so as
to emphasise one topic more than another, as the lecturer may prefer. The in-text
ix

references also may help students to start reading scientiﬁc papers when they are
working on their assignments, as a place to start the consultation of the literature.
I hope I have succeeded in striking a good balance on topics & depth in the ﬁrst
two blocks of the textbook. Suggestions for improvement are welcome. (Knowing
that ontologists can be a quite critical group, perhaps I should add to that: antes
de criticarme, intenta superarme, i.e., before you criticise me, try to do a better
job at writing an ontology engineering textbook than me.)
The contents of the textbook was written by gradually improving, extending,
and further updating material that started with blog posts in 2009 for the European
Masters in Computational Logic’s Semantic Web Technologies course I taught at
the Free University of Bozen-Bolzano, Italy, in 2009/2010, with the hope of gener-
ating and facilitating online discussions. That failed miserably, but the posts were
visited often. The blogposts were reworked into short syllabi for the Ontology En-
gineering courses at the University of Havana and University of Computer Science,
Cuba, in 2010 and at the Masters Ontology Winter School 2010 in South Africa,
which, in turn, were reworked into the COMP718/720 lecture notes at the Univer-
sity of KwaZulu-Natal and the Ontology Engineering honours course lecture notes
at the University of Cape Town, South Africa, of which the latest version was in
2015. All those chapters have been updated for this textbook, new material added,
and course-speciﬁc data has been removed. I had put a CC BY-NC-SA licence on
those 2015 lecture notes, so therefore this book has that Creative Commons licence
as well. If you think this sounds problematic: it probably is not; if in doubt, please
contact me.
Some contents of this book or associated exercises are adapted from slides or
tutorials made by other people, and I would like to thank them for having made
that material available for use and reuse. They are (in alphabetic order) Jos de
Bruijn, Diego Calvanese, Nicola Guarino, Matthew Horridge, Ian Horrocks, Markus
Kr¨otzsch, Tommie Meyer, Mariano Rodr´ıguez-Muro, Frantiˇsek Simanˇc´ık, Umberto
Straccia, and David Toman. I also would like to thank the students who were
enrolled in any of the aforementioned courses, who provided feedback on the blog
posts and lecture notes, and assisted me in ﬁne-tuning where more or less expla-
nations and exercises were deemed useful.
For the rest, it was a lot of hard work, with a few encouragements by some
academics who appreciated sections of the lecture notes (thank you!) and some
I-ignore-that-advice by others who told me it’s a waste of time because one can-
not score brownie points with a textbook anyway. The most enjoyable of all the
sessions of updating the contents was the increment from the 2015 lecture notes to
the ﬁrst full draft of the textbook, which was at Consuelo’s casa particular in La
Habana in June 2018 and interspersed with a few casino (salsa) lessons to stretch
the legs and get-togethers with acquaintances and colleagues.
Cape Town, South Africa
C. Maria Keet
July, 2018
x

How to use the book
Aims and Synopsis
The principal aim of this textbook is to provide the student with a comprehensive
introductory overview of ontology engineering.
A secondary aim is to provide
hands-on experience in ontology development that illustrate the theory, such as
language features, automated reasoning, and top-down and bottom-up ontology
development with methods and methodologies.
This textbook covers material such that, upon completion, the student:
(i) has a general understanding of the notion of what ontologies and knowledge
bases are, what they can be used for, how, and when not;
(ii) has obtained an understanding of the, currently, main ontology languages—
OWL and its underlying Description Logics languages—in order to represent
the knowledge in ontologies formally and to reason over them, and have a
basic understanding of what an automated reasoner does;
(iii) can conﬁdently use an Ontology Development Environment;
(iv) can conﬁdently use methods and methodologies to develop ontologies, in-
cluding the top-down approach with foundational ontologies and bottom-up
using non-ontological resources such as relational databases, natural language
or thesauri; and
(v) has become acquainted with several major applications and application sce-
narios, such as the Semantic Web Technologies for ontologies, and has had a
taste of the research trends in the ﬁeld.
Interwoven in the aims is skills development for a 4th year/honours project or
masters dissertation.
The students will become familiar with reading scientiﬁc
literature and will gain experience in report writing and presenting their work to
their peers, in particular when carrying out the two suggested assignments.
xi

xii
How to use the book
Content at a glance
The chapters are structured such that one could cover one chapter per week for a
semester-long course with, depending on one’s interest, e.g., to spread Chapter 6
over two lectures or elaborate more on Chapter 5.
1. Chapter 1: Introduction. The introductory chapter addresses diﬀerences be-
tween databases and knowledge bases, conceptual data models and ontologies,
what an ontology is (and is not), and takes a sampling of application areas,
such as the Semantic Web and data integration.
2. Block 1: Logic foundations for ontologies
(a) Chapter 2: First order logic and automated reasoning.
This chapter
provides a recap of the basics of ﬁrst order predicate logic, including the
notion of model-theoretic semantics. The second part introduces the
principles of reasoning over a logical theory, and tableau reasoning in
particular.
(b) Chapter 3: Description Logics.
This chapter is devoted to a gentle
introduction to the basics of Description Logics, which are a family of
languages that are decidable fragments of FOL and lie at the basis of
most ‘species’ of the World Wide Web consortium’s standardised Web
Ontology Language OWL. Tableau reasoning returns and is adapted to
the DL setting.
(c) Chapter 4: The web ontology language OWL and Automated Reasoning.
The chapter starts with a few historical notes to put the language(s)
into context, and proceeds with OWL 2 and its computationally better
behaved proﬁles. In addition, we take a look at the principal automated
reasoning services for (OWL) ontologies, such as satisﬁability checking
and classiﬁcation and how this works in the currently available soft-
ware. It has a short recap on computational complexity to appreciate
the trade-oﬀs between language features and scalable applications, and
closes with a note on the broader context of the Semantic Web on the
one hand, and more expressive logics on the other.
3. Block 2: Developing good ontologies
(a) Chapter 5: Methods and Methodologies. This chapter starts with a sam-
pling of methodologies to structure the process of actually developing
an ontology. Drilling down into some detail, this also requires several
methods to improve an ontology’s quality, which, to some extent, use
the automated reasoner to the developer’s beneﬁt as well as some philo-
sophical notions.
(b) Chapter 6: Top-down Ontology Development. One step of ontology de-
velopment is the use of foundational ontologies and their formalisations.

How to use the book
xiii
We shall have a look at some typical content of foundational ontolo-
gies and look at how they represent things diﬀerently from conceptual
modelling practice so as to foster interoperability. Several foundational
ontologies will pass the revue. As part-whole relations are deemed very
important in ontology development, both its foundations as well as some
practical guidance on its use as discussed.
(c) Chapter ??: Bottom-up Ontology Development. In addition to starting
from ‘above’ with a foundational ontology, one can reuse legacy material
to generate candidate classes and relations to speed up populating an
ontology. In particular, we will look at relational databases, thesauri
(including SKOS), spreadsheets, and natural language processing. It
also introduces ontology design patterns.
4. Block 3: Advanced Topics
This block contains a small selection of advanced topics, which assume that
the contents of Block I and Block II are understood.
(a) Chapter 8: Ontology-Based Data Access. Due to various usage scenarios,
there is a need to maintain the link between the data and the knowl-
edge, such as in scientiﬁc workﬂows or in silico biology and enhanced
user and content management in e-learning applications. For scalabil-
ity purposes, one connects a database to an ontology so that one can
query the database ‘intelligently’ through the ontology.
The chapter
starts with a motivation and design choices and then proceeds to one
such instantiation with (roughly) OWL 2 QL, a mapping layer, and a
relational database.
(b) Chapter 9: Ontologies and natural language. This chapter considers two
principal interactions between ontologies and natural language: dealing
with the internationalisation and localisation of ontologies, and a natural
language interface to ontology by means of a controlled natural language
to render the axioms readable for domain experts.
(c) Chapter 10: Advanced modelling with additional language features. There
are various extensions to the ‘basic’ ontology languages and reasoning
services to cater for additional knowledge that needs to be represented,
such as vagueness, uncertainty, and temporal aspects of a subject do-
main. The chapter touches upon fuzzy and rough ontologies and consid-
ers brieﬂy a temporal DL that, albeit impractical at present, does solve
a range of modelling issues.
(d) Chapter 11: Ontology modularisation. Production-level ontologies may
become very large, and an approach to deal with that is to modularise
the ontology into multiple smaller ones. This chapter will take a look
at the foundations of the landscape of ontology modules, such as the
purposes they are made for, their types, and characteristics, how to
determine what good modules are.

xiv
How to use the book
While the textbook is aimed at advanced undergraduate/early postgraduate
level for people studying for a degree in, or with a background in, computer sci-
ence, it can be used diﬀerently. For instance, one may be a logician and wonder
what those philosophers are going on about, or are unfamiliar with the ‘hands
in the mud’ of some bio-ontologies project and wants to gain an appreciation of
those eﬀorts. In that case, it would be better to commence with Block II and just
consult Block I as/if the need arise. Conversely, if one has tried to develop an
ontology and ‘ﬁghts’ with the reasoner or cannot represent the things one would
like, then commence with Block I, which will provide some answers to solve such
issues. In any case, the material of both Block I and Block II are prerequisites for
the advanced topics in Block III. The three chapters in Block III can be done in
order of preference, or just a subset thereof, since they do not depend on each other.
Supporting materials are available online at the book’s webpage at https:
//people.cs.uct.ac.za/~mkeet/OEbook/, which consist mainly of:
– ontologies that are used in the exercises, to inspect, modify, and explore
sample answers;
– software to assist with various tasks of developing ontologies;
– supporting documentation, such as slides and instructions for how to read the
book—in particular the Description Logic axioms—when visually impaired.
Assessment
There are review questions at the end of each chapter, whose answers can be found
in the text of that chapter. Exercises are intended to obtain practical hands-on
experiences and sometimes challenge the student.
A selection of the exercises’
answers is included in the appendix, which is indicated with an “*” at the end
of the question. Assignments require the student to integrate the material and,
especially for the mini-project, delve deeper into a speciﬁc sub-topic.
There can be several assessment modes for a ﬁnal mark of a course. I have used
mainly the following format, but one can choose diﬀerently:
– A test (exam) at the end of the course [50%]
– A practical assignment due some time half-way the course duration [20%]
– Mini-project due at the end of the course [30%]
Students had to submit something for each component in order to have a chance
to pass the course.

CHAPTER 1
Introduction
This chapter introduces ontologies: what they are (roughly), what they are used for,
and describes a few success stories where they have been instrumental at solving
problems. Where and how an ontology can solve problems is not of the variety
“when you have only a hammer, everything looks like a nail”, but where the use
of an ontology was the solution to a particular problem, or at least an essential
ingredient of it. To place “ontologies” in its right context, the ﬁrst two questions
one has to ask and answer are:
• What is an ontology?
• What is it good for? (or: what problems does it solve?)
A short, informal, and very concrete way to clarify what “an ontology” is in
computing—in analogy a the ﬁrst mention of a relational database—is that it
is a text ﬁle containing structured knowledge about a particular subject domain.
Of course, a relational database with its management system is a lot more than
‘just a text ﬁle’, and likewise there’s more to “an ontology”, even from this prac-
tical engineering perspective. Such a ﬁle is used as a component of a so-called
‘intelligent’ information system. Fancy marketing talk may speak of some of those
ontology-driven information systems as “like a database, on steroids!” and
similar. Ontologies have been, and are being, used to solve data integration prob-
lems by providing the common, agreed-upon vocabulary and the constraints among
them, which is then used in a way so that the software understands that, say, an
entity Student of a relational database DB1 actually means the same thing as Ad-
vancedLearners in some application software OO2. Tools can then be developed
to link up those two applications and exchange information smoothly thanks to
the shared vocabulary. Over time, people ﬁgured out other ways to use ontologies
and contribute to solving entirely diﬀerent problems. For instance, a question-
answering system that lets the scientist chat with a library chatterbot to more
easily ﬁnd relevant literature (compared to string and keyword matching), auto-
matically ﬁnd a few theoretically feasible candidate rubber molecules out of very
1

2
Chapter 1. Introduction
many (compared to painstaking trial-and-error work in the laboratory), and auto-
mated discovery of a new enzyme (outperforming the human experts!). Thus, the
text in that text ﬁle somehow has meaning and there are tools that can process
that, which therewith can improve in various ways the regular software you have
encountered and developed in your undergraduate studies.
In the next section (Section 1.1), we have a quick peek at what an ontology—
the artefact—looks like, and proceed to the more and less pedantic viewpoints of
deﬁning what an ontology is with respect to the content (Section 1.2). We will then
look at the original motivations why ontologies were taken up in computing & IT
and look at a few examples of other uses and what may be considered as some of
the success stories (Section 1.3). Lots of new terms are introduced in this chapter
that are ﬂeshed out in much more detail in subsequent chapters. Therefore, it is
probably useful to revisit this chapter later on—and don’t be put oﬀif it is not all
clear immediately and raises many questions now! In fact, it should raise questions,
which hopefully will motivate you to want to have them answered, which indeed
will be in the subsequent chapters.
1.1
What does an ontology look like?
Most of you may only vaguely have heard of ‘ontologies’, or not at all. Instead
of delving into the theory straight away, we’ll have a quick look at the artefact,
to show that, practically in computing and intelligent software development, it is
an object one can play with and manipulate. The actual artefact can appear in
multiple formats that are tailored to the intended user, but at the heart of it,
there is a logic-based representation that the computer can process. Let us take
as example the African Wildlife Ontology (AWO), which is a so-called ‘tutorial
ontology’ that will return in the exercises. The AWO contains knowledge about
wildlife, such as that giraﬀes eat leaves and twigs, that they are herbivores, that
herbivores are animals, and so on. A mathematician may prefer to represent such
knowledge with ﬁrst order predicate logic. For instance:
∀x(Lion(x) →∀y(eats(x, y) →Herbivore(y))∧∃z(eats(x, z)∧Impala(z))) (1.1)
that states that “all lions eat herbivores, and they also eat some impalas”. This
axiom may be one of the axioms in the ontology. One can represent the same
knowledge also in logics other than plain vanilla ﬁrst order logic. For instance, in
a Description Logic language, we have the same knowledge formally represented
as:
Lion ⊑∀eats.Herbivore ⊓∃eats.Impala
(1.2)
A domain expert, however, typically will prefer a more user-friendly rendering, such
as an automatically generated (pseudo-)natural language sentence, e.g.:
Each lion eats only herbivore and eats some Impala
where the ﬁrst “∀” in equation 1.1 is verbalised as Each and the second one as only,
the “∧” as and, and the “∃” as some. Another option is to use a graphical language
that is more or less precise in showing the knowledge, as shown in Figure 1.1.

1.1.
What does an ontology look like?
3
1..*
*
Lion
Impala
*
*
Herbivore
eats
eats
A. Graphical rendering in OntoGraf 
B. Approximation in UML Class 
Diagram notation 
Figure 1.1: Two graphical renderings of lions eating only herbivores and at least some
impala, with the OntoGraf plugin in the Prot´eg´e 4.x ontology development environment
(A) and in UML class diagram style notation (B).
Considering all those diﬀerent renderings of the same knowledge, remember
that an ontology is an engineering artefact that has to have a machine-processable
format that faithfully adheres to the logic. None of these aforementioned repre-
sentations are easily computer-processable, however. To this end, there are seri-
alisations of the ontology into a text ﬁle that are easily computer-processable.
The most widely-used one is the Web Ontology Language OWL. The required
format is called the RDF/XML format, so then a machine-processable version of
the class lion in the RDF/XML format looks as follows:
<owl:Class rdf:about="&AWO;lion">
<rdfs:subClassOf rdf:resource="&AWO;animal"/>
<rdfs:subClassOf>
<owl:Restriction>
<owl:onProperty rdf:resource="&AWO;eats"/>
<owl:someValuesFrom rdf:resource="&AWO.owl;Impala"/>
</owl:Restriction>
</rdfs:subClassOf>
<rdfs:subClassOf>
<owl:Restriction>
<owl:onProperty rdf:resource="&AWO;eats"/>
<owl:allValuesFrom rdf:resource="&AWO;herbivore"/>
</owl:Restriction>
</rdfs:subClassOf>
<rdfs:comment>Lions are animals that eat only herbivores.</rdfs:comment>
</owl:Class>
where the “∀” from equation 1.1 is serialised as owl:allValuesFrom, the “∃” is
serialised as owl:someValuesFrom, and the subclassing (“→” and “⊑” in Eqs 1.1
and 1.2, respectively) as rdfs:subClassOf. You typically will not have to write
an ontology in this RDF/XML format. As a computer scientist, you may design
tools that will have to process or modify such machine-processable ontology ﬁles,
though even there, there are tool development toolkits and APIs that cover many
tasks.
For the authoring of an ontology, there are ontology development environ-
ments (ODEs) that render the ontology graphically, textually, or with a logic view.
A screenshot of one such tool, Prot´eg´e, is included in Figure 1.2.

4
Chapter 1. Introduction
Figure 1.2: Screenshot of the lion eating only herbivores and at least some impala in
the Prot´eg´e ontology development environment.
1.2
What is an ontology?
Note: You may prefer to read this section again later on in the course, when we
are well into Block II. Try to read it now anyway, but if it’s not clear upon the ﬁrst
read, then don’t worry, as it will become clearer as we go along.
1.2.1
The deﬁnition game
To arrive at some answer(s) as to what an ontology is, let us ﬁrst compare it
with some artefacts you are already familiar with: relational databases and con-
ceptual data models such as EER and UML. An important distinction between
conceptual data models and ontologies is that a conceptual data model provides
an application-speciﬁc implementation-independent representation of the data that
will be handled by the prospective application, whereas (domain) ontologies pro-
vide an application-independent representation of a speciﬁc subject domain, i.e., in
principle, regardless the particular application, or, phrased positively: (re)usable
by multiple applications. From this distinction follow further diﬀerences regarding
their contents—in theory at least—to which we shall return to in Block II. Looking
at actual ontologies and conceptual data models, the former is normally formalised
in a logic language, whereas conceptual modelling is more about drawing the boxes
and lines informally1, and they are used diﬀerently and serve diﬀerent purposes.
A comparison between relational databases and ontologies as knowledge bases
reveals that, unlike RDBMSs, ontologies (knowledge bases) include the represen-
tation of the knowledge explicitly, by having rules included, by using automated
1though one surely can provide them with logic-based reconstructions (e.g., [ACK+07,
BCDG05, Kee13])

1.2.
What is an ontology?
5
reasoning (beyond plain queries) to infer implicit knowledge and detect inconsis-
tencies of the knowledge base, and they usually operate under the Open World
Assumption2.
This informal brief comparison gives a vague idea of what an ontology might
be, or at least what it is not, but it does not get us closer to a deﬁnition of what
an ontology is.
An approach to the issue of deﬁnitions was taken in the 2007
Ontolog Communiqu´e3, where its participants and authors made a collection of
things drawn into a diagram to express ‘things that have to do with an ontology’;
this is depicted in Figure 1.3. It is intended as a “Template for discourse” about
ontologies, which has a brief4 and longer5 explanation of the text in the labeled
ovals. The “semantic” side has to do with the meaning represented in the ontology
and the “pragmatic” side has to do with the practicalities of using ontologies.
Figure 1.3: The OntologySummit2007’s “Dimension map”.
Let us now look at attempts to put that into words into a deﬁnition. Intuitively
it is known by the ontologists what an ontology is, but putting that into words such
that it also can survive philosophers’ scrutiny is no trivial matter. The consequence
is that, at the time of writing, there is no unanimously agreed-upon deﬁnition
what an ontology is. The descriptions have been improving over the past 20 years,
though. We mention them here, as some are better than others, and you may
2vs. Closed World Assumption in a relational database setting. We return to the OWA and
CWA in a later chapter.
3http://ontolog.cim3.net/cgi-bin/wiki.pl?OntologySummit2007_Communique
4http://ontolog.cim3.net/cgi-bin/wiki.pl?OntologySummit2007_
FrameworksForConsideration/DimensionsMap
5http://ontolog.cim3.net/cgi-bin/wiki.pl?OntologySummit2007_Communique

6
Chapter 1. Introduction
come across this in the scientiﬁc literature. The most quoted (but problematic!)
deﬁnition is the following one by Tom Gruber:
Deﬁnition 1.1 ([Gru93]). An ontology is a speciﬁcation of a conceptualization.
You may see this quote especially in older scientiﬁc literature on ontologies, but it
has been superseded by other, more precise ones, for Gruber’s deﬁnition is unsat-
isfactory for several reasons: what is a “conceptualization” exactly, and what is a
“speciﬁcation”? Using two nebulous terms to describe a third one does not clarify
matters. A proposed reﬁnement to address these two questions is the following
one:
Deﬁnition 1.2 ([SBF98]). An ontology is a formal, explicit speciﬁcation of a shared
conceptualization.
However, this still leaves us with the questions as to what a “conceptualization”
is and what a “formal, explicit speciﬁcation” is, and why and how “shared”? Is
it shared enough when, say, you and I agree on the knowledge represented in the
ontology, or do we need a third one or a whole group to support it? A comprehensive
deﬁnition is given in Guarino’s landmark paper on ontologies [Gua98] (revisited in
[GOS09]):
Deﬁnition 1.3 ([Gua98]). An ontology is a logical theory accounting for the in-
tended meaning of a formal vocabulary, i.e. its ontological commitment to a par-
ticular conceptualization of the world. The intended models of a logical language
using such a vocabulary are constrained by its ontological commitment. An ontol-
ogy indirectly reﬂects this commitment (and the underlying conceptualization) by
approximating these intended models.
A broader scope is also described in [Gua09], and a more recent overview about
deﬁnitions of “an ontology” versus Ontology in philosophy can be found in [GOS09],
which reﬁnes in a step-wise and more precise fashion Deﬁnitions 1.2 and 1.3. It is
still not free of debate [Neu17], though, and it is a bit of a mouthful as deﬁnition. A
simpler deﬁnition is given by the developers of the World Wide Web Consortium’s
standardised ontology language OWL6:
Deﬁnition 1.4 ([HPSvH03]). An ontology being equivalent to a Description Logic
knowledge base.
That last deﬁnition has a diﬀerent issue, and is unduly restrictive, because 1) it
surely is possible to have an ontology that is represented in another logic language
(OBO format, Common Logic, etc.)
and 2) then formalising a thesaurus as a
“Description Logic knowledge base” (or: in OWL) also ends up as a simple ‘light-
weight ontology’ (e.g., the NCI thesaurus as cancer ‘ontology’) and a conceptual
data model in EER or UML that is translated into OWL becomes an ‘application
ontology’ or ‘operational ontology’ by virtue of it being formalised in OWL. But,
as we saw above, there are diﬀerences between the two.
6Note: we will go into some detail of OWL, Description Logics, and knowledge bases in
Chapters 3 and 4.

1.2.
What is an ontology?
7
For better or worse, currently, and in the context of the most prominent appli-
cation area of ontologies—the Semantic Web—the tendency is toward it being
equivalent to a logical theory, and a Description Logics knowledge base in partic-
ular (Deﬁnition 1.4). Ontologists at least frown when someone calls ‘a thesaurus
in OWL’ or ‘an ER diagram in OWL’ ontologies, but even aside from that: the
blurring of the distinctions between the diﬀerent artefacts is problematic for var-
ious reasons (discussed in later chapters), and one should note the fact that just
because something is represented in OWL does not make it an ontology, just like
that something that is represented in a language other than OWL may well be an
ontology.
1.2.2
Some philosophical notes on ontologies
The previous section mentioned that the deﬁnition would have to survive the
philosophers’ scrutiny. But why so? The reason for that is that ‘ontologies’ in
computer science did not come out of nowhere. Philosophers are in the picture
because the term ‘ontology’ is taken from philosophy, where it has a millennia-old
history, and one uses insights emanating from philosophy when developing good
ontologies. When we refer to that philosophical notion, we use Ontology, with a
capital ‘O’, and it does not have a plural. Orthogonal to the deﬁnition game, there
are discussions about what is actually represented in an ontology, i.e., its contents,
from a philosophical perspective.
One debate is about ontology as a representation of a conceptualisation—
roughly: things you are thinking of—and as a representation of reality. Practically,
whether that is a relevant topic may depend on the subject domain for which you
would be developing an ontology. If you represent formally the knowledge about,
say, malaria infections, you would better represent the (best approximation of)
reality, being the current state of scientiﬁc knowledge, not some divergent political
or religious opinion about it, because the wrong representation can lead to wrong
inferences, and therewith wrong treatments that are either ineﬀective or even harm-
ful. Conversely, there are subject domains where it does not really matter much
whether you represent reality or a conceptualisation thereof, or something inde-
pendent of whether that exists in reality or not, or even certainly does not exist
in reality. Such discussions were commonplace in computing and applications of
ontologies some 10-15 years ago, but have quieted down in recent years. One such
debate can be found in writing in [Mer10a, Mer10b, SC10]. Merrill [Mer10a] pro-
vides several useful clariﬁcations. First, there is an “Empiricist Doctrine” where
“the terms of science... are to be taken to refer to actually existing entities in the
real world”, such as Jacaranda tree, HIV infection and so forth, which are considered
mind-independent, because HIV infections occurred also without humans thinking
of it, knowing how it worked, and naming those events HIV infections. This is in
contrast with the “conceptualist view according to which such terms refer to con-
cepts (which are taken to be psychological or abstract formal entities of one sort
or another)”, with concepts considered to be mind-dependent entities; prototyp-
ical examples of such mind-dependent entities are Phlogiston and Unicorn—there
are no objects in the world as we know it that are phlogiston or unicorns, only

8
Chapter 1. Introduction
our outdated theories and fairy tale stories, respectively, about them. Second, the
“Universalist Doctrine”, which asserts “that the so-called “general terms” of sci-
ence” (HIV infection etc.) “are to be understood as referring directly to universals”,
with universals being “a class of mind independent entities, usually contrasted with
individuals, postulated to ground and explain relations of qualitative identity and
resemblance among individuals. Individuals are said to be similar in virtue of shar-
ing universals.” [MR05]. However, philosophers do not agree on the point whether
universals exist, and even if they exist, what kind of things they are. This brings
the inquiring person to metaphysics, which, perhaps, is not necessarily crucial in
building ontologies that are to serve information systems; e.g., it need not be rel-
evant for developing an ontology about viruses whilst adhering to the empiricist
doctrine. The philosophically inclined reader may wish to go a step further and
read about interactions between Ontology and metaphysics by, e.g., [Var12].
There are other aspects of philosophy that can have an eﬀect on what is rep-
resented in an ontology and how. For instance, it can help during the modelling
stage, like that there’s a diﬀerence between what you are vs. the role(s) you play
and between participating in an event vs. being part of an event, and help clarify-
ing assumptions you may have about the world that may trickle into the ontology,
like whether you’re convinced that the vase and the clay it is made of are the same
thing or two diﬀerent things. We will return to this topic in Chapter 6.
1.2.3
Good, not so good, and bad ontologies
Just like one can write good and bad code, one can have good and bad ontolo-
gies. Their goodness, or badness, is a bit more elaborate than with software code,
however. Bad software code can be unmaintainable spaghetti code or have bugs
or not even compile. For ontologies, the equivalent to ‘not compile’ is when there
is a violation of the syntax. We’ll get into the syntax in Block I. The equivalent
to ‘bugs’ is two-fold, as it is for software code: there can be errors in the sense
that, say, a class cannot have any instances due to conﬂicting constraints and there
can be semantic errors in that what has been represented is logically correct, but
entirely unintended. For instance, that a class, say, Student somehow turns up as
a subclass of Table, which it obviously should not.
There are further intricate issues that make one ontology better than another.
Some structuring choices are excluded because of ontological constraints. Let us
take the example of green apples. One could formalise it as that we have apples
that have the attribute green or say there are green objects that have an apple-
shape. Logic does not care about this distinction, but, at least intuitively, somehow,
objects having the colour green seems more reasonable than green objects having
an apple-shape. There are reasons for that: Apple carries an identity condition,
so one can identify the object (it is a ‘sortal’), whereas Green does not (it is a
value of the attribute hasColor that a thing has). Ontology helps explaining such
distinctions, as we shall see in Chapter 6.
Finally, with the interplay between the logic one uses to represent the knowledge
in an ontology and the meaning of the entities in the subject domain, we can
show schematically a notion of good and bad ontologies. Consider Figure 1.4. We

1.3.
What is the usefulness of an ontology?
9
have a good ontology when what we want to represent has been represented in the
ontology, yet what is actually represented is very close and only slightly more than
the intention; that is, we have a high precision and maximum coverage. We have
a less good ontology when the ontology represents quite a bit more than what it
should; that is, we have a low precision and maximum coverage. Things can go
wrong when we have a maximum precision, but only limited coverage, or: the
ontology does not contain all that it should, hence, would be a bad ontology when
it can’t do what it should in our ontology-driven information system. Things are
even worse if we have both a low precision and limited coverage: then it contains
stuﬀwe don’t want in there and does not contain stuﬀthat should be in there.
The interplay between precision and coverage have to do both with the language
one uses for the ontology and with good modelling. This will be addressed in Block
I and Block II, respectively.
Good
Less good
Universe
what you 
want to represent
what you do/can represent with the language
Bad
Worse
Figure 1.4: Good, less good, bad, and even worse ontologies. The pink circle denotes
the subject domain (say, African Wildlife), the green circle denotes what’s in the ontology
(say, the AWO).
1.3
What is the usefulness of an ontology?
Now that we have some idea of ontologies, let us have a look at where they are
being used. Ontologies for information systems were ﬁrst proposed to contribute
to solving the issues with data integration: an ontology provides the common
vocabulary for the applications that is at one level of abstraction higher up than
conceptual data models such as EER diagrams and UML Class Diagrams. Over
the years, it has been used also for other purposes. We start with two distinct
scenarios of data integration where ontologies play a central role, and subsequently
describe other scenarios where ontologies are an important part of the solution.

10
Chapter 1. Introduction
1.3.1
Data and information system integration
Figure 1.5 sketches the idea of the ontology-driven schema-based data integration
and Figure 1.6 further below shows an example of data-based data integration that
we shall elaborate on in the next two subsections.
PD
ED
Q
A
R
PR
AR
NAPO
Flower
Colour
ColourRegion
Pantone
Flower
Height
Colour
ID
Bloem
(ID)
Lengte
Kleur
color:String
height:inch
Flower
Database
Database
C++ 
application
(datatype: real)
qt
ql
Implementation
the actual information 
system that stores and
manipulates the data
Conceptual model
shows what is stored 
in that particular 
application
Ontology
provides the common vocabulary 
and constraints that hold across
the applications
Figure 1.5:
Sketch of an ontology-based application integration scenario.
Bottom:
diﬀerent implementations, such as relational databases and OO software; Centre: con-
ceptual data models tailored to the application (a section of an EER, ORM, and UML
diagram, respectively); Top: an ontology that provides a shared common vocabulary for
interoperability among the applications. See text for explanation.
Integrating legacy systems
In the setting of ontology-driven schema-based (and conceptual data model-based)
data integration, a typical situation is as follows. You have several databases con-
taining data on the same topic. For instance, two universities join forces into one:
each university had its own database with information about students, yet, as the
new mega-university, there has to be one single database to manage the data of
all students. This means that the two databases have to be integrated somehow.
A similar situation occurs oftentimes in industry, especially due to mergers and
acquisitions, in government due to the drive for e-Government services to the cit-
izens of the country, or attempting to develop a software system for integrated
service delivery, as well as in healthcare due to a drive for electronic health records
that need to combine various systems, say, a laboratory database with the doctor’s
database, among other scenarios.
While the topic of data integration deserves its own textbook7, we focus here
only on the ontology-driven aspect. Let us assume we have the relational databases
7the ‘principles of...’ series may be a good start; e.g., [DHI12].

1.3.
What is the usefulness of an ontology?
11
and therewith at least their respective physical schemas, and possibly also the re-
lational model and even the respective conceptual models, and also some object-
oriented application software on top of the relational database. Their corresponding
conceptual data models are tailored to the RDBMS/OO application and may or
may not be modelled in the same conceptual data modelling language; e.g., one
could be in EER, another in ORM, in UML and so forth. The example in Figure 1.5
is a sketch of such a situation about information systems of ﬂower shops, where at
the bottom of the ﬁgure we have two databases and one application that has been
coded in C++. In the layer above that, there is a section of their respective con-
ceptual data models: we have one in EER with bubble-notation, one in ORM, and
one UML Class Diagram. Each conceptual data model has “Flower” and “Colour”
included in some way: in the UML Class diagram, the colour is an attribute of the
ﬂower, i.e., Color 7→Flower×String (that actually uses only the values of the Pan-
tone System) and similarly in the EER diagram (but then without the data type),
and in the ORM diagram the colour is a value type (unary predicate) Kleur with
an additional relation to the associated datatype colour region in the spectrum
with as data type real. Clearly, the notion of the ﬂower and its colour is the same
throughout, even though it is represented diﬀerently in the conceptual data models
and in the implementations. It is here that the ontology comes into play, for it is
the place to assert exactly that underlying, agreed-upon notion. It enables one to
assert that:
• EER’s and UML diagram’s Flower and ORM’s Bloem ‘means’ Flower in the
domain ontology8, which is indicated with the red dashed arrows.
• EER’s Colour, ORM’s Kleur and UML’s Color denote the same kind of thing,
albeit at one time it is represented as a unary predicate (in ORM) and other
times it is a binary relation with a data type, i.e., an attribute. Their ‘map-
pings’ to the entity in the ontology (green dashed arrows), Colour, indicates
that agreement.
• There is no agreement among the conceptual models when it comes to the
data type used in the application, yet they may be mapped into their re-
spective notion in an ontology (purple dashed arrows).
For instance, the
ColourRegion for the values of the colour(s) in the colour spectrum is a
PhysicalRegion, and one might say that the PantoneSystem of colour en-
coding is an AbstractRegion.
The ﬁgure does not include names of relationships in the conceptual data model,
but they obviously can be named at will; e.g., heeftKleur (‘hasColour’) in the ORM
diagram. Either way, there is, from an ontological perspective, a speciﬁc type of
relation between the class and its attribute: one of dependency or inherence, i.e.,
that speciﬁc colour instance depends on the existence of the ﬂower, for if that
particular ﬂower does not exist, then that speciﬁc instance of colour does not exist
either. An ontology can provide those generic relations, too. In the sketch, this
happens to be the qt relationship between enduring objects (like ﬂowers) and the
qualities they have (like their colour), and from the quality to the value regions
8that in this case is linked to a foundational ontology, DOLCE, and there it is a subclass of a
Non-Agentive Physical Object; we return to this in Block II

12
Chapter 1. Introduction
Web-based display of the KEGG database 
entry with key K01834
QuickGO view of its equivalent 
(GO:0004619) in the Gene 
Ontology 
Web-based display of Interpro database 
entry with key IPR005995
Figure 1.6: Illustration of ontology-based data-level integration: two databases, the
KEGG and InterPro, with a web-based front-end, and each database has its data (each
tuple in the database, where possible) annotated with a term from the Gene Ontology.

1.3.
What is the usefulness of an ontology?
13
(more precisely: qualia), the relation is called ql in Figure 1.5.
Although having established such links does not complete the data integration,
it is the crucial step—the rest has become, by now, largely an engineering exercise.
Data-level data integration
While in computer science the aforementioned approach to data integration was
under investigation, domain experts in molecular biology needed a quick and prac-
tical solution to the data integration problem, urgently. Having noticed the idea
of ontologies, they came up with another approach, being interoperability at the
instance-level, tuple-by-tuple, or even cell-by-cell, and that with multiple databases
over the Internet instead of the typical scenario of RDBMSs within an organisa-
tion. This can be achieved with lightweight ontologies, or structured controlled
vocabularies.
The basic idea is illustrated in Figure 1.6. There are multiple databases, which
in the ﬁgure are the KEGG and InterPro databases. In the KEGG database, there
is a tuple with as key K01834 and it has several attributes (columns in the table in
the relational database), such as the name (gpmA), and further down in the display
there is an attribute Other DBs, which has as entry GO:0004619; i.e., there is a
tuple in the table along the line of ⟨K01834, ..., GO:0004619⟩. In the InterPro
database, we have a similar story but then for the entity with the key IPR005995,
where there is a section “GO Term Annotation” with an attribute function that
has GO:0004619; i.e., a tuple ⟨IPR005995, ..., GO:0004619⟩. That is, they are
clearly distinct tuples—each with their separate identiﬁer from a diﬀerent identiﬁer
scheme, with diﬀerent attributes, one physically stored in a database in Japan and
the other in the USA—yet they actually talk about the same thing: GO:0004619,
which is the identiﬁer for Phosphoglycerate Mutase Activity.
The “GO:0004619” is an identiﬁer for a third artefact: a class in the Gene On-
tology (GO) [Gen00]. The GO is a structured controlled vocabulary that contains
the concepts and their relationship that the domain experts agree upon to annotate
genes with; the GO contains over 40000 concepts by now. The curators of the two
databases each annotated their entity with a term from the GO, and thereby they
assert they have to do with that same thing, and therewith have created an entity-
level linking and interoperability through the GO. Practically, on top of that, these
ﬁelds are hyperlinked (in the soft copy: blue text in KEGG and green underlined
text in the screenshot of the InterPro entry), so that a vast network of data-level
interlinked databases has been created. Also, the GO term is hyperlinked to the
GO ﬁle online, and in this way, you can browse from database to database availing
of the terms in the ontology without actually realising they are wholly diﬀerent
databases. Instead, they appear like one vast network of knowledge.
There are many more such ontologies, and several thousand databases that are
connected in this way, not only thanks to ontologies, but where the ontology serves
as the essential ingredient to the data integration. Some scientiﬁc journals require
the authors to use those terms from the ontologies when they write about their
discoveries, so that one more easily can ﬁnd papers about the same entity9.
9It used to be a sport among geneticists to come up with cool names for the genes they

14
Chapter 1. Introduction
Trying to prevent interoperability problems
A related topic in the data integration scenarios, is trying to prevent the integration
problems form happening in the ﬁrst place. This may be done though generating
conceptual models for related new applications based on the knowledge represented
in the ontology [EGOMA06, JDM03, SS06].
This is a bit alike the Enterprise
Models you may have come across in information system design.
In this way,
interoperability is guaranteed upfront because the elements in the new conceptual
data models are already shared thanks to the link with the same ontology. For
instance, some relation R between A and B, as is depicted in Figure 1.7 (e.g.,
an enrols relation between Student and Course), is reused across the conceptual
data models, yet each model may have its own additional constraints and data
types for attributes. For instance, in one university (hence, student information
management system), students may not register for more than six courses and have
to be registered for at least one to count as student, whereas at another university,
a student may well decide not to be registered for any course at a particular time
during the year and still count as a registered student. Whichever rules there may
be at each individual university, the systems do agree on the notions of Student,
Course, and enrols. Thus, the ontology provides the shared common vocabulary for
interoperability among the applications.
A
B
R
A
B
Ontology:
UML Class Diagram 1:
UML Class Diagram 2:
A
B
att1: String
att2: Integer
B
UML Class Diagram n:
...
1
*
R
R *
1..*
generate
A
B
att1: String
att2: Real
B
1 R *
Figure 1.7: Basic illustration of taking information from an ontology and using it for
several conceptual data models (here: UML Class Diagrams), where the constraints may
be reﬁned or attributes added, yet sharing the semantics of A, B, and R.
1.3.2
Ontologies as part of a solution to other problems
Over the years, ontologies have been shown to be useful in a myriad of other appli-
cation scenarios; among others, negotiation between software services, mediation
between software agents, bringing more quality criteria into conceptual data mod-
elling to develop a better model (hence, a better quality software system), orches-
trating the components in semantic scientiﬁc workﬂows, e-learning, ontology-based
data access, information retrieval, management of digital libraries, improving the
accuracy of question answering systems, and annotation and analysis of electronic
health records, to name but a few. Four of them are brieﬂy illustrated in this
section.
discovered (e.g., “Sonic hedgehog”); when a gene was independently recovered, each research
team typically had given the gene a diﬀerent name, which can end up as a Tower of Babel of its
own that hampered progress in science. The GO and similar ontologies resolve that issue.

1.3.
What is the usefulness of an ontology?
15
e-Learning
The ‘old-fashioned’ way of e-learning is a so-called content-push: the lecturer sends
out softcopies of the notes, slides, answers to the solutions, and perhaps the video
recordings of the lectures, and the student consumes it. This is a one-size-ﬁts-all
approach regardless the student’s background with acquired knowledge and skills,
and learning preferences and habits, which cannot be assumed to be homogeneous
in an e-learning setting, or at least much less so than with respect to your fellow
students in the ontology engineering class. A more sophisticated way for e-learning
is adaptive e-learning, which tailors the contents to the student based on prior
knowledge and learning habits. To be able to automatically tailor the oﬀering to
the student, one has to develop a ‘smart’ e-learning application that can ﬁgure
out what kind of student is enrolled. Put diﬀerently: students have certain prop-
erties (part-time/full-time student, age, undergraduate degree, etc.), the learning
objects have to be annotated (by skill level and topic), and user logs have to be
categorised according to type of learning pattern, and based on that the material
and presentation can be adjusted, like skipping the section on ﬁrst order logic and
delve deeper into, or spend more time on, ontology engineering and modelling if
you have a mathematics background, whereas a philosopher may crave for more
content about foundational ontologies but skip reverse engineering of relational
databases, or oﬀer a student more exercises on a topic s/he had diﬃculties with.
This requires knowledge representation—of the study material, questions, an-
swers, students’ attributes, learning approaches—and automated reasoning to
classify usage pattern and student, and annotated content, i.e., using ontologies
and knowledge bases to make it work solidly and in a repeatable way. See, e.g.,
[HDN04] as a start for more details on this topic.
Deep question answering with Watson
Watson10 is a sophisticated question answering engine that ﬁnds answers to triv-
ia/general knowledge questions for the Jeopardy! TV quiz that, in the end, did
consistently outperform the human experts of the game. For instance, a question
could be “who is the president of South Africa?”: we need algorithms to parse the
question, such as that ‘who’ indicates the answer has to be a person and a named
entity, it needs to be capable to detect that South Africa is a country and what a
country is, and so on, and then have some kind of a look-up service in knowledge
bases and/or natural language documents to somehow ﬁnd the answer by relating
‘president’, ‘South Africa’ and ‘Cyril Ramaphosa’ and that he is the current pres-
ident of the country. An ontology can then be used in the algorithms of both the
understanding of the question and ﬁnding the right answer11 and integrating data
sources and knowledge, alongside natural language processing, statistical analysis
and so on, comprising more than 100 diﬀerent techniques12. Thus, a key aspect of
10http://en.wikipedia.org/wiki/Watson_(computer)
11On a much more modest scale, as well as easier accessible and shorter to read, Vila and
Ferr´andez describe this principle and demonstrated beneﬁts for their Spanish language based
question-answering system in the agricultural domain [VF09].
12ftp://public.dhe.ibm.com/common/ssi/ecm/en/pow03061usen/POW03061USEN.PDF

16
Chapter 1. Introduction
the system’s development was that one cannot go in a linear fashion from natu-
ral language to knowledge management, but have to use an integration of various
technologies, including ontologies, to make a successful tool.
Digital humanities
Some historians and anthropologists in the humanities try to investigate what
happened in the Mediterranean basin some 2000 years ago, aiming to understand
food distribution systems. Food was stored in pots (more precisely: an amphora)
that had engravings on it with text about who, what, where etc. and a lot of that
has been investigated, documented, and stored in multiple resources, such as in
databases. None of the resources cover all data points, but to advance research and
understanding about it and food trading systems in general, it has to be combined
and made easily accessible to the domain experts.
That is, essentially it is an
instance of a data access and integration problem. Also, humanities researchers
are not at all familiar with writing SQL queries, so that would need to be resolved
as well, and in a ﬂexible way so that they would not have to be dependent on the
availability of a system administrator.
A recent approach, of which the technologies have been maturing, is Ontology-
Based Data Access (OBDA). The general idea of OBDA applied to the Roman
Empire Food system is shown in Figure 1.8. There are the data sources, which
are federated (one ‘middle layer’, though still at the implementation level). The
federated interface has mapping assertions to elements in the ontology. The user
then can use the terms of the ontology (classes and their relations and attributes) to
query the data, without having to know about how the data is stored and without
having to write page-long SQL queries. For instance, a query “Retrieve inscriptions
on amphorae found in the city of ‘Mainz’ containing the text ‘PNN’.” would use
just the terms in the ontology, say, Inscription, Amphora, City, found in, and inscribed
on, and any value constraint added (like the PNN), and the OBDA system takes
care of the rest to return the answer.
More high-level details of the system are described in [CLM+16] and we will
look at its technicalities in Chapter 8.
Semantic scientiﬁc workﬂows
Due to the increase in a variety of equipment, their speed, and decreasing price,
scientists are generating more data than ever, and are collaborating more. This
data has to be analysed and managed. In the early days, and, to some extent,
to this day, many one-oﬀlittle tools were developed, or simply scripted together
with PERL, Ruby on Rails, or Python, used once or a few times and then left
for what it was. This greatly hampers repeatability of experiments, insight in the
provenance of the data, and does not quite follow a methodological approach for
so-called in silico biology research. Over the past 10 years, comprehensive software
and hardware infrastructures have been, and are being, built to ﬁx these and re-
lated problems. Those IT ‘workbenches’ (as opposed to the physical ones in the
labs) are realised in semantic scientiﬁc workﬂow systems. An example of a virtual

1.3.
What is the usefulness of an ontology?
17
The federation engine 
operates at the physical or 
relational schema layer
Typically relational 
databases and RDF triple 
stores
Linking elements from 
the ontology to queries 
over the data source(s)
Ontology or logic-based
conceptual data model
Mappings
Data sources
Figure 1.8: OBDA in the EPnet system (Source: based on [CLM+16])
bench is Taverna [GWG+07], which in the meantime has gone ‘mainstream’ as a
project within the Apache Software Foundation13 [WHF+13]. This, in turn can be
extended further; e.g., to incorporate data mining in the workﬂow to go with the
times of Big Data, as depicted in Figure 1.9. This contains ontologies of both the
subject domain where it is used as well as ontologies about data mining itself that
serve to ﬁnd appropriate models, algorithms, datasets, and tools for the task at
hand, depicted in Figure 1.10. Thus, we have a large software system—the virtual
workbench—to facilitate scientists to do their work, and some of the components
are ontologies for integration and data analysis across the pipeline.
Now that we have seen some diverse examples, this does not mean that on-
tologies are the panacea for everything, and some ontologies are better suitable to
solve one or some of the problems, but not others. Put diﬀerently, it is prudent
to keep one’s approach to engineering: conduct a problem analysis ﬁrst, collect
the requirements and goals, and then assess if an ontology indeed is part of the
solution or not. If it is part of the solution, then we enter in the area of ontology
engineering.
1.3.3
Success stories
To be able to talk about successes of ontologies, and its incarnation with Semantic
Web Technologies in particular, one ﬁrst needs to establish when something can
be deemed a success, when it is a challenge, and when it is an outright failure.
Such measures can be devised in an absolute sense—compare technology x with an
ontology-mediated one: does it outperform on measure y?—and relative—to whom
is technology x deemed successful?
A major success story of the development and use of ontologies for data linking
and integration is the Gene Ontology [Gen00], its oﬀspring, and subsequent coordi-
13https://taverna.incubator.apache.org/

18
Chapter 1. Introduction
Figure
1.9:
Overview
of
the
architecture
of
the
e-Laboratory
for
In-
terdisciplinary
Collaborative
Data
Mining
(Source:
originally
retrieved
from
http://www.e-lico.eu/?q=node/17).
nated evolution of ontologies [SAR+07]. These frontrunners from the Gene Ontol-
ogy Consortium14 and their colleagues in bioinformatics were adopters of some of
the Semantic Web ideas even before Berners-Lee, Hendler, and Lassila wrote their
Scientiﬁc American paper in 2001 [BLHL01], even though they did not formulate
their needs and intentions in the same terminology: they did want to have shared,
controlled vocabularies with the same syntax to facilitate data integration—or at
least interoperability—across Web-accessible databases, have a common space for
identiﬁers, it needing to be a dynamic, changing system, to organize and query
incomplete biological knowledge, and, albeit not stated explicitly, it all still needed
to be highly scalable [Gen00]. The results exceeded anyone’s expectations in its
success for a range of reasons. Many tools for the Gene Ontology (GO) and its
common Knowledge Representation format, .obo, have been developed, and other
research groups adopted the approach to develop controlled vocabularies either by
extending the GO, e.g., rice traits, or adding their own subject domain, such as
zebraﬁsh anatomy and mouse developmental stages. This proliferation, as well as
the OWL development and standardisation process that was going on at about
the same time, pushed the goal posts further: new expectations were put on the
GO and its siblings and on their tools, and the proliferation had become a bit too
wieldy to keep a good overview what was going on and how those ontologies would
be put together. Put diﬀerently, some people noticed the inferencing possibilities
that can be obtained from moving from a representation in obo to one in OWL and
others thought that some coordination among all those obo bio-ontologies would
be advantageous given that post-hoc integration of ontologies of related and over-
14http://www.geneontology.org/

1.3.
What is the usefulness of an ontology?
19
Figure
1.10:
The
data
mining
and
application
layers
of
the
e-Laboratory
for Interdisciplinary Collaborative Data Mining (Source:
originally retrieved from
http://www.e-lico.eu/?q=node/17).
lapping subject domains is not easy. Thus came into being the OBO Foundry to
solve such issues, proposing an approach for coordinated evolution of ontologies to
support biomedical data integration [SAR+07] within the OBO Foundry Project15.
People in related disciplines, such as ecology, have taken on board experiences
of these very early adopters, and instead decided to jump on board after the OWL
standardization. They, however, were not only motivated by data(base) integra-
tion. Referring to Madin et al’s paper [MBSJ08], I highlight three points they
made:
– “terminological ambiguity slows scientiﬁc progress, leads to redundant re-
search eﬀorts, and ultimately impedes advances towards a uniﬁed foundation
for ecological science”, i.e., identiﬁcation of some serious problems they have
in ecological research;
– “Formal ontologies provide a mechanism to address the drawbacks of termi-
nological ambiguity in ecology”, i.e., what they expect that ontologies will
solve for them (disambiguation); and
– “ﬁll an important gap in the management of ecological data by facilitating
powerful data discovery based on rigorously deﬁned, scientiﬁcally meaningful
terms”, i.e., for what purpose they want to use ontologies and any associated
computation (discovery using automated reasoning).
That is, ontologies not as a—one of many possible—‘tool’ in the engineering in-
frastructure, but as a required part of a method in the scientiﬁc investigation that
aims to discover new information and knowledge about nature (i.e., in answering
the who, what, where, when, and how things are the way they are in nature). Suc-
15http://www.obofoundry.org/

20
Chapter 1. Introduction
cess in inferring novel biological knowledge has been achieved with classiﬁcation of
protein phosphatases [WSH07], precisely thanks to the expressive ontology and its
automated reasoning services.
Good quality ontologies have to built and maintained, though, and there needs
to be working infrastructure for it. This textbook hopefully will help you with
that.
1.4
Outline and usage of the book
The preceding sections already indicated that several aspects would return ‘later
in the course’. The order that will be followed in this book, is to commence with
a recap (or brief introduction, as may be the case) of First Order Predicate Logic
regarding the formalisation with syntax, semantics (what it all ‘means’, formally),
and principles of automated reasoning. Full FOL is undecidable, but there are
less expressive languages, i.e., fragments of FOL, that are decidable for a set of
important problems in computing in the area of ontologies and knowledge bases.
One such family of languages is the Description Logics (DL) family of languages.
These two topics are covered in Chapter 2 and Chapter 3, respectively. Several
DLs, in turn, form the basis of the W3C standardised Web Ontology Language
OWL (actually, a family of languages, too). OWL speciﬁes a computer-processable
serialisation of the ontology and knowledge base, and interacts with the automated
reasoners for OWL. OWL and the so-called standard reasoning services are sum-
marised in Chapter 4.
After these logic foundations in Block I, we shall look at how one can develop an
ontology. The ﬁrst approach is a so-called ‘top-down’ approach, where we use foun-
dational ontologies with the high-level categories and relationship to get us started
with the principal choices and the modelling so that a modeller does not have to
reinvent the wheel; this is covered in Chapter 6. However, designing an ontology
from scratch is rather cumbersome, and much information already has been rep-
resented in various ways—natural language, conceptual data models, etc.—so, one
can speed up ontology development also by somehow reusing those ‘legacy’ sources,
which is described in Chapter 7. Both approaches, however, are just that—not a
‘cookbook recipe’ for ontology development—and there exist interdependencies,
methods, tools, and methodologies that help structure and carry out the activities,
which is described in Chapter 5. One could go through Block II either in the order
of Chapters 6, 7, and 5, or ﬁrst Chapter 5 and then 6 and 7.
Blocks I and II form the foundations of ontology engineering at an introduc-
tory level, and the topics that follow afterward deepen and extend that material.
Block III contains a few short chapters that introduce various subtopics, which is
far from exhaustive and the block is intended mainly to illustrate that there is a
range of themes within ontology engineering. In one direction, you may wish to
explore further some quite involved theory and technology to realise a practical
ontology-driven information system, being querying databases by means of an on-
tology, which is the topic of Chapter 8. Ontologies are used throughout the world,
and not all systems are in English, therefore we will look at the interaction of nat-

1.5.
Exercises
21
ural language with ontologies in Chapter 9. There are extensions to the standard
ontology languages, because it is perceived to be needed to be more precise in rep-
resenting the subject domain. Chapter 10 touches upon the temporal, uncertain,
and vague dimension.
Depending on one’s background, one can study Block I after Block II—unless
one’s knowledge of logic is a bit rusty or limited. In any case, both the material of
Block I and Block II are prerequisites for Block III, advanced topics. Within Block
III, the chapters can be done in order of preference, or just a subset thereof.
This is the ﬁrst version of the textbook, but essentially the fourth version of
prior lecture notes, and due to time constraints, perhaps not everything that should
have been in the book made it into the book. Also, because it is of an introductory
nature and a reader may be interested more in one sub-topic than another, it is
liberally referenced, so you more easily can look up further details. There are many
references in the bibliography. You are not expected to read all of them; instead,
each chapter has a “Literature and reference material” section with a small se-
lection of recommended reading. The large reference list may be useful especially
for the practical assignment (Appendix B.1) and the mini-project assignment (Ap-
pendix B.2): there are very many more references in computer science conference
proceedings and journals, but the ones listed, ﬁrst, in the “literature and reference
material” and, second, in the bibliography, will give you a useful ‘entry point’ or
may even suﬃce, depending on the chosen topics.
Exercises are structured along the line of review questions and then either prac-
tical exercises or further analysis questions. Some exercises refer to particular on-
tologies, which can be found at the book’s webpage at https://people.cs.uct.
ac.za/~mkeet/OEbook/ or the URL provided. The answers to the review questions
can be found in the respective chapter. A selection of answers to the exercises is
included in Appendix E.
1.5
Exercises
Review question 1.1. There are several terms in the preceding sections that were
highlighted in bold in the text. Find them, and try to describe them in your own
words, in particular: ontology-driven information system, Ontology, ontology, and
ontology engineering.
Review question 1.2. List several uses of ontologies.
Review question 1.3. Describe the diﬀerence between schema vs. instance-level
data integration.
Exercise 1.1. You may like to get a practical ‘feel’ of ontologies and how they look
like in an ontology development environment. To this end, install an ODE, such as
Prot´eg´e, load the AfricanWildlifeOntology1.owl from the book’s supplementary
material page at https://people.cs.uct.ac.za/~mkeet/OEbook/ in the tool and

22
Chapter 1. Introduction
browse around. Download the AfricanWildlifeOntology1.owl ﬁle (right-click,
save as) and open it in your text editor, such as notepad.
Exercise 1.2. Having inspected the AfricanWildlifeOntology1.owl, is it a good,
less good, bad, or even worse ontology? Why?
1.6
Literature and reference material
1. Tim Berners-Lee, James Hendler and Ora Lassila. The Semantic Web. Scien-
tiﬁc American Magazine, May 17, 2001. http://www.sciam.com/article.
cfm?id=the-semantic-web&print=true
2. Nicola Guarino, Daniel Oberle, and Steﬀen Staab. What Is An Ontology?
In: S. Staab and R. Studer, Handbook on Ontologies, Chapter 6. Springer.
2009. pp1-17.

Part I
Logic foundations for ontologies
23


CHAPTER 2
First order logic and automated reasoning in a nutshell
Perhaps more foundations in modelling may be useful before delving into how to
represent what you want to represent, but at the same time, one also needs to
understand the language to model in. In this case, this means obtaining a basic
grasp of logic-based ontology languages, which will help understanding the ontolo-
gies and ontology engineering better, and how to formalise the things one wants to
represent. Therefore, we shall refresh the basics of ﬁrst order logic in Section 2.1
(comprehensive introductions can be found elsewhere, e.g., [Hed04]), which is fol-
lowed by a general idea of (automated) reasoning and two examples of tableau
reasoning in Section 2.2. If you have had a basic course in logic from a mathemat-
ics department, you may wish to just skim over Section 2.1; my experience is that
automated reasoning is typically not covered in such a mathematics or standard
basic course in logic and you should therefore still engage with Section 2.2.
2.1
First order logic syntax and semantics
Observe ﬁrst that logic is not the study of truth, but of the relationship between
the truth of one statement and that of another. That is, in logic, we do not care
whether a statement like “If angels exist then necessarily all of them ﬁt on the head
of a needle” (suitably formalised) is indeed true in reality1, but if the if-part were
true (resp., false), then what does that say about the truth value of the then-part
of the statement? And likewise for a whole bunch of such sentences. Others do
care what is represented formally with such a logic language, but we will defer that
to Block II.
To be able to study those aspects of logic, we need a language that is unam-
biguous; natural language is not. You may have encountered propositional logic
already, and ﬁrst order predicate logic (FOL) is an extension of that, which en-
ables us to represent more knowledge in more detail. Here, I will give only a brief
1or, for that matter, whether there is a reality and whether we have access to it
25

26
Chapter 2.
First order logic and automated reasoning in a nutshell
glimpse of it. Eventually, you will need to be able to recognise, understand, and
be able to formalise at least a little bit in FOL. Of all the deﬁnitions that will
follow shortly, there are four important ideas to grasp: the syntax of a language,
the model-theoretic semantics of a language, what a theory means in the context of
logic, and the notion of deduction where we apply some rules to what has been rep-
resented explicitly so as to derive knowledge that was represented only implicitly.
We will address each in turn.
First, there are two principal components to consider for the language:
• Syntax has to do with what ‘things’ (symbols, notations) one is allowed to
use in the language and in what way; there is/are a(n):
– Alphabet
– Language constructs
– Sentences to assert knowledge
• Semantics
– Formal meaning, which has to do what those sentences with the alphabet
and constructs are supposed to mean.
Their details are presented in the remainder of this section.
2.1.1
Syntax
The lexicon of a ﬁrst order language contains the following:
• Connectives and Parentheses: ¬, →, ↔, ∧, ∨, ( and );
• Quantiﬁers: ∀(universal) and ∃(existential);
• Variables: x, y, z, ... ranging over particulars (individual objects);
• Constants: a, b, c, ... representing a speciﬁc element;
• Functions: f, g, h, ..., with arguments listed as f(x1, ...xn);
• Relations: R, S, ... with an associated arity.
There is an (countably inﬁnite) supply of symbols (signature): variables, functions,
constants, and relations.
In other words: we can use these things to create ‘sentences’, like we have in
natural language, but then controlled and with a few extra ﬁgurines. Let us look
ﬁrst at how we can formalise a natural language sentence into ﬁrst order logic.
Example 2.1. From Natural Language to First order logic (or vv.). Consider the
following three sentences:
- “Each animal is an organism”
- “All animals are organisms”
- “If it is an animal then it is an organism”
This can be formalised as:
∀x(Animal(x) →Organism(x))
(2.1)
Observe the colour coding in the natural language sentences: ‘each’ and ‘all’ are
diﬀerent ways to say “∀”, the ‘is a’ and ‘are’ in the ﬁrst two sentences match the
“→”, and the combination of the “∀” and “→” in this particular construction can
be put into natural language as ‘if ... then’.
Instead of talking about all objects of a particular type, one also can assert
there are at least some of them; e.g.,

2.1.
First order logic syntax and semantics
27
- “Aliens exist”
could be formalised as
∃x Alien(x)
(2.2)
with the ‘exist’ matching the ∃, and
- “There are books that are heavy”
(well, at least one of them is) as:
∃x(Book(x) ∧heavy(x))
(2.3)
where the ‘there are’ is another way to talk about “∃” and the ‘that’ hides (linguis-
tically) the “∧”. A formulation like “There is at least one book, and it is heavy”
is a natural language rendering that is closer to the structure of the axiom.
A sentence—or, more precisely, its precise meaning—such as “Each student
must be registered for a degree programme” requires a bit more consideration.
There are at least two ways to say the same thing in the way the sentence is
formulated (we leave the arguments for and against each option for another time):
i) ∀x, y(registered for(x, y) →Student(x) ∧DegreeProgramme(y))
“if there is a registered for relation, then the ﬁrst object is a student and
the second one a degree programme”
∀x(Student(x) →∃y registered for(x, y))
“Each student is registered for at least one y”, where the y is a degree pro-
gramme (taken from the ﬁrst axiom)
ii) ∀x(Student(x) →∃y (registered for(x, y) ∧DegreeProgramme(y)))
“Each student is registered for at least one degree programme”
But all this is still just syntax (it does not say what it really means), and it looks
like a ‘free for all’ on how we can use these symbols. This is, in fact, not the case,
and the remainder of the deﬁnitions will make this more precise, which will be
illustrated in Example 2.2 afterward. ♦
There is a systematic to the axioms in the examples, with the brackets, arrows,
etc. Let us put this more precisely now. We start from the basic elements and
gradually build it up to more complex things.
Deﬁnition 2.1. (Term) A term is inductively deﬁned by two rules, being:
• Every variable and constant is a term.
• if f is a m-ary function and t1, . . . tm are terms, then f(t1, . . . , tm) is also a
term.
Deﬁnition 2.2. (Atomic formula) An atomic formula is a formula that has the
form t1 = t2 or R(t1, ..., tn) where R is an n-ary relation and t1, ..., tn are terms.
Deﬁnition 2.3. (Formula) A string of symbols is a formula of FOL if and only
if it is constructed from atomic formulas by repeated applications of rules R1, R2,
and R3.
R1. If φ is a formula then so is ¬φ.
R2. If φ and ψ are formulas then so is φ ∧ψ.

28
Chapter 2.
First order logic and automated reasoning in a nutshell
R3. If φ is a formula then so is ∃xφ for any variable x.
A free variable of a formula φ is that variable occurring in φ that is not quantiﬁed.
For instance, if φ = ∀x(Loves(x, y)), then y is the free variable, as it is not bound
to a quantiﬁer. We now can introduce the deﬁnition of sentence.
Deﬁnition 2.4. (Sentence) A sentence of FOL is a formula having no free vari-
ables.
Check that there are no free variables in the axioms in Example 2.1, i.e., they
are all sentences.
Up to this point, we have seen only a few examples with going back-and-forth
between sentences in natural language and in FOL. This is by no means to only
option. One can also formalise diagrams and provide logic-based reconstructions of,
say, UML class diagrams in order to be precise. We can already do this with syntax
introduced so far. Let’s consider again the lion eating impalas and herbivores, as
was shown in Figure 1.1-B in Chapter 1. First, we ‘bump up’ the eats association
end to the name of the binary relation, eats. Second, noting that UML uses look-
across notation for its associations, the 1..* ‘at least one’ amounts to an existential
quantiﬁcation for the impalas, and the * ‘zero or more’ to a universal quantiﬁcation
for the herbivores. This brings us back to Eq. 1.1 from Chapter 1:
∀x(Lion(x) →∀y(eats(x, y) →Herbivore(y))∧∃z(eats(x, z)∧Impala(z))) (2.4)
Another example with diﬀerent constraints is shown in Figure 2.1, which could
be the start of a UML class diagram (but incomplete; notably because it has
no attributes and no methods) or an abuse of notation, in that an ontology is
shown diagrammatically in UML class diagram notation, as there is no oﬃcial
graphical language to depict ontologies. Either way, syntactically, also here the
Animal
Carnivore
Herbivore
Omnivore
Limb
4
{disjoint, complete}
part
Figure 2.1: A UML model that can be formally represented in FOL; see text for details.
UML classes can be converted into unary predicates in FOL, the association is
translated into a binary relation, and the multiplicity constraint turns into an
existential quantiﬁcation that is restricted to exactly 4 limbs2. In the interest of
succinctness and convention, the latter is permitted to be abbreviated as ∃=4. The
corresponding sentence in FOL is listed in Eq. 2.5. One can dwell on the composite
aggregation (the black diamond), and we will do so in a later chapter. Here, it is
2There are animals that indeed do not have 4 limbs (e.g., the sirens and the Mexican mole
lizard have only two limbs and the millipede Illacme plenipes has 750 (the most), and there are
animals with speciﬁc numbers of limbs in between, but that is beside the point as it would not
change much the features used in the formalisation, just become more cluttered.

2.1.
First order logic syntax and semantics
29
left as an association with an ‘at most one’ constraint on the whole side (Eq. 2.6),
or: “if there’s a limb related to two animal instances through whole, then those
two instances must be the same object”.
∀x(Animal(x) →∃=4y(part(x, y) ∧Limb(y)))
(2.5)
∀x, y, z(Limb(x) ∧whole(x, y) ∧whole(x, z) ∧Animal(y) ∧Animal(z) →y = z)
(2.6)
That is, indeed, literally talking of two references to one object.
The ‘new’ constraints that we have not translated before yet, are the subclass-
ing, the disjointness, and the completeness. Subclassing is the same as in Eq. 2.1,
hence, we obtain Eqs. 2.7-2.9.
∀x(Omnivore(x) →Animal(x))
(2.7)
∀x(Herbivore(x) →Animal(x))
(2.8)
∀x(Carnivore(x) →Animal(x))
(2.9)
Disjoint means that the intersection is empty, so it has the pattern ∀x(A(x) ∧
B(x) →⊥), where “⊥” is the bottom concept/unary predicate that is always false;
hence, Eq. 2.10 for the sample classes of Figure 2.1.
Completeness over the specialisation means that all the instances of the super-
class must be an instance of either of the subclasses; hence Eq. 2.11.
∀x((Omnivore(x) ∧Herbivore(x)) ∨(Omnivore(x)∧
Carnivore(x)) ∨(Carnivore(x) ∧Herbivore(x)) →⊥
(2.10)
∀x(Animal(x) →Omnivore(x) ∨Herbivore(x) ∨Carnivore(x))
(2.11)
These are all syntactic transformations, both by example and that informally
some translation rules and a general pattern (like for disjointness) have been noted.
From a software engineering viewpoint, this can be seen as a step toward a logic-
based reconstruction of UML class diagrams. From a logic viewpoint, the diagram
can be seen as ‘syntactic sugar’ for the axioms, which is more accessible to domain
experts (non-logicians) than logic.
This playing with syntax, however, does not say what it all means. How do
these sentences in FOL map to the objects in, say, the Java application (if it were
a UML diagram intended as such) or to some domain of objects wherever that is
represented somehow (if it were a UML diagram depicting an ontology)? And can
the latter be speciﬁed a bit more precise? We shall see the theoretical answers to
such question in the next section.
2.1.2
Semantics
Whether a sentence is true or not depends on the underlying set and the inter-
pretation of the function, constant, and relation symbols. To this end, we have

30
Chapter 2.
First order logic and automated reasoning in a nutshell
structures: a structure consists of an underlying set together with an interpreta-
tion of functions, constants, and relations. Given a sentence φ and a structure M,
M models φ means that the sentence φ is true with respect to M. More precisely,
through the following set of deﬁnitions (which will be illustrated afterward):
Deﬁnition 2.5. (Vocabulary) A vocabulary V is a set of function, relation, and
constant symbols.
Deﬁnition 2.6. (V-structure) A V-structure consists of a non-empty underlying
set ∆along with an interpretation of V. An interpretation of V assigns an element
of ∆to each constant in V, a function from ∆n to ∆to each n-ary function in V,
and a subset of ∆n to each n-ary relation in V. We say M is a structure if it is a
V-structure of some vocabulary V.
Deﬁnition 2.7. (V-formula) Let V be a vocabulary. A V-formula is a formula in
which every function, relation, and constant is in V. A V-sentence is a V-formula
that is a sentence.
When we say that M models φ, denoted with M |= φ, this is with respect to
M being a V-structure and V-sentence φ is true in M.
Model theory is about the interplay between M and a set of ﬁrst-order sentences
T (M), which is called the theory of M, and its ‘inverse’ from a set of sentences Γ
to a class of structures.
Deﬁnition 2.8. (Theory of M) For any V-structure M, the theory of M, denoted
with T (M), is the set of all V-sentences φ such that M |= φ.
Deﬁnition 2.9. (Model) For any set of V-sentences, a model of Γ is a V-
structure that models each sentence in Γ. The class of all models of Γ is denoted
by M(Γ).
Now we can go to the interesting notions: theory in the context of logic:
Deﬁnition 2.10. (Complete V-theory) Let Γ be a set of V-sentences. Then Γ
is a complete V-theory if, for any V-sentence φ either φ or ¬φ is in Γ and it is
not the case that both φ and ¬φ are in Γ.
It can then be shown that for any V-structure M, T (M) is a complete V-theory
(for proof, see, e.g., [Hed04], p90).
Deﬁnition 2.11. A set of sentences Γ is said to be consistent if no contradiction
can be derived from Γ.
Deﬁnition 2.12. (Theory) A theory is a consistent set of sentences.
The latter two deﬁnitions are particularly relevant later on when we look at the
typical reasoning services for ontologies.

2.1.
First order logic syntax and semantics
31
Student
DegreeProgramme
attends
Student is an entity type. 
DegreeProgramme is an entity type. 
Student attends DegreeProgramme. 
Each Student attends exactly one DegreeProgramme. 
It is possible that more than one Student attends the same DegreeProgramme. 
OR, in the negative: 
For each Student, it is impossible that that Student attends more than one 
DegreeProgramme. 
It is impossible that any Student attends no DegreeProgramme. 
 
 
 
 
 
 
               Attends 
Student 
DegreeProgramme 
John 
Computer Science 
Mary 
Design 
Fabio 
Design 
Claudio 
Computer Science 
Markus 
Biology 
Inge 
Computer Science 
 
 
Figure 2.2: A theory denoted in ORM notation, ORM verbalization, and some data in
the database. See Example 2.2 for details.
Example 2.2. How does all this work out in practice? Let us take something quasi-
familiar: a conceptual data model in Object-Role Modeling notation, depicted in
the middle part of Figure 2.2, with the top-half its ‘verbalisation’ in a controlled
natural language, and in the bottom-part some sample objects and the relations
between them.
First, we consider it as a theory, creating a logical reconstruction of the icons
in the ﬁgure.
There is one binary predicate, attends, and there are two unary
predicates, Student and DegreeProgramme. The binary predicate is typed, i.e., its
domain and range are deﬁned to be those two entity types, hence:
∀x, y(attends(x, y) →Student(x) ∧DegreeProgramme(y))
(2.12)
Note that x and y quantify over the whole axiom (thanks to the brackets), hence,
there are no free variables, hence, it is a sentence. There are two constraints in
the ﬁgure: the blob and the line over part of the rectangle, and, textually, “Each
Student attends exactly one DegreeProgramme” and “It is possible that more than one
Student attends the same DegreeProgramme”. The ﬁrst constraint can be formalised
(in short-hand notation):
∀x(Student(x) →∃=1y attends(x, y))
(2.13)
The second one is already covered with Eq. 2.12 (it does not introduce a new
constraint).
So, our vocabulary is {attends, Student, DegreeProgramme}, and
we have two sentences (Eq. 2.12 and Eq. 2.13). The sentences form the theory, as
they are not contradicting and admit a model.
Let us now consider the structure. We have a non-empty underlying set of objects:
∆
=
{John, Mary, Fabio, Claudia, Markus, Inge, ComputerScience, Biology,
Design}. The interpretation then maps the instances in ∆with the elements in our

32
Chapter 2.
First order logic and automated reasoning in a nutshell
vocabulary; that is, we end up with {John, Mary, Fabio, Claudio, Markus, Inge}
as instances of Student, and similarly for DegreeProgramme and the binary
attends. Observe that this structure does not contradict the constraints of our
sentences. ♦
Equivalences
With the syntax and semantics, several equivalencies between formulae can be
proven. We list them for easy reference, with a few ‘informal readings’ for illustra-
tion. φ, ψ, and χ are formulas.
• Commutativity:
φ ∧ψ ≡ψ ∧φ
φ ∨ψ ≡ψ ∨φ
φ ↔ψ ≡ψ ↔φ
• Associativity:
(φ ∧ψ) ∧χ ≡φ ∧(ψ ∧χ)
(φ ∨ψ) ∨χ ≡φ ∨(ψ ∨χ)
• Idempotence:
φ ∧φ ≡φ
φ ∨φ ≡φ
//‘itself or itself is itself’
• Absorption:
φ ∧(φ ∨ψ) ≡φ
φ ∨(φ ∧ψ) ≡φ
• Distributivity:
(φ ∨(ψ ∧χ) ≡(φ ∨ψ) ∧(φ ∨χ)
(φ ∧(ψ ∨χ) ≡(φ ∧ψ) ∨(φ ∧χ)
• Double negation:
¬¬φ ≡φ
• De Morgan:
¬(φ ∧ψ) ≡¬φ ∨¬ψ
¬(φ∨ψ) ≡¬φ∧¬ψ //‘negation of a disjunction implies the negation of each
of the disjuncts’
• Implication:
φ →ψ ≡¬φ ∨ψ
• Tautology:
φ ∨⊤≡⊤
• Unsatisﬁability:
φ ∧⊥≡⊥
• Negation:
φ ∧¬φ ≡⊥
//something cannot be both true and false
φ ∨¬φ ≡⊤
• Neutrality:
φ ∧⊤≡φ
φ ∨⊥≡φ
• Quantiﬁers:
¬∀x.φ ≡∃x.¬φ

2.2.
Reasoning
33
¬∃x.φ ≡∀x.¬φ
//‘if there does not exist some, then there’s always none’
∀x.φ ∧∀x.ψ ≡∀x.(φ ∧ψ)
∃x.φ ∨∃x.ψ ≡∃x.(φ ∨ψ)
(∀x.φ) ∧ψ ≡∀x.(φ ∧ψ) if x is not free in ψ
(∀x.φ) ∨ψ ≡∀x.(φ ∨ψ) if x is not free in ψ
(∃x.φ) ∧ψ ≡∃x.(φ ∧ψ) if x is not free in ψ
(∃x.φ) ∨ψ ≡∃x.(φ ∨ψ) if x is not free in ψ
Note: The ones up to (but excluding) the quantiﬁers hold for both propositional
logic and ﬁrst order predicate logic.
2.2
Reasoning
Having a logic language with a semantics is one thing, but it may be only a means to
an end rather than an end in itself. From the computational angle—especially from
a logician’s perspective—the really interesting aspect of having such a language
and (someone else) having put in the eﬀort to formalise some subject domain, is to
reason over it to infer implicit knowledge. Here, we are not talking about making a
truth table, which is computationally way too costly when one has to analyse many
sentences, but deploying other techniques so that it can be scaled up compared
to the manual eﬀorts. Automated reasoning, then, concerns computing systems
that automate the ability to make inferences by designing a formal language in
which a problem’s assumptions and conclusion can be written and providing correct
algorithms to solve the problem with a computer in an eﬃcient way.
How does one ﬁnd out whether a formula is valid or not? How do we ﬁnd out
whether our knowledge base is satisﬁable? The main proof technique for DL-based
ontologies is tableaux, although there are several others3. The following subsections
ﬁrst provide a general introduction (Section 2.2.1), the essential ingredients for
automated reasoning (Section 2.2.2), and then describes deduction, abduction, and
induction (Section 2.2.3).
2.2.1
Introduction
Characteristics
People employ reasoning informally by taking a set of premises and somehow ar-
riving at a conclusion, i.e., it is entailed by the premises (deduction), arriving at
a hypothesis (abduction), or generalizing from facts to an assumption (induction).
Mathematicians and computer scientists developed ways to capture this formally
with logic languages to represent the knowledge and rules that may be applied to
the axioms so that one can construct a formal proof that the conclusion can be
derived from the premises. This can be done by hand [Sol05] for small theories,
but that does not scale up when one has, say, 80 or more axioms even though there
are much larger theories that require a formal analysis, such as checking that the
3The remainder of Section 2.2 consists of amended versions of my “Reasoning, automated”
essay and related deﬁnitions that have been published in Springer’s Encyclopedia of Systems
Biology.

34
Chapter 2.
First order logic and automated reasoning in a nutshell
theory can indeed have a model and thus does not contradict itself. To this end,
much work has gone into automating reasoning. The remainder of this section in-
troduces brieﬂy several of the many purposes and usages of automated reasoning,
its limitations, and types of automated reasoners.
Purposes
Automated reasoning, and deduction in particular, has found applications in ‘every
day life’. A notable example is hardware and (critical) software veriﬁcation, which
gained prominence after Intel had shipped its Pentium processors with a ﬂoating
point unit error in 1994 that lost the company about $500 million. Since then,
chips are routinely automatically proven to function correctly according to speci-
ﬁcation before taken into production. A diﬀerent scenario is scheduling problems
at schools to ﬁnd an optimal combination of course, lecturer, and timing for the
class or degree program, which used to take a summer to do manually, but can
now be computed in a fraction of it using constraint programming. In addition
to such general application domains, it is also used for speciﬁc scenarios, such as
the demonstration of discovering (more precisely: deriving) novel knowledge about
protein phosphatases [WSH07]. They represented the knowledge about the subject
domain of protein phosphatases in humans in a formal bio-ontology and classiﬁed
the enzymes of both human and the fungus Aspergillus fumigatus using an auto-
mated reasoner, which showed that (i) the reasoner was as good as human expert
classiﬁcation, (ii) it identiﬁed additional p-domains (an aspect of the phosphatases)
so that the human-originated classiﬁcation could be reﬁned, and (iii) it identiﬁed
a novel type of calcineurin phosphatase like in other pathogenic fungi. The fact
that one can use an automated reasoner (in this case: deduction, using a Descrip-
tion Logics knowledge base) as a viable method in science is an encouragement to
explore such avenues further.
Limitations
While many advances have been made in speciﬁc application areas, the main limita-
tion of the implementations are due to the computational complexity of the chosen
representation language and the desired automated reasoning services. This is be-
ing addressed by implementations of optimisations of the algorithms or by limiting
the expressiveness of the language, or both. One family of logics that focus prin-
cipally on ‘computationally well-behaved’ languages is Description Logics, which
are decidable fragments of ﬁrst order logic [BCM+08]; that is, they are languages
such that the corresponding reasoning services are guaranteed to terminate with
an answer. Description Logics form the basis of most of the Web Ontology Lan-
guages OWL and OWL 2 and are gaining increasing importance in the Semantic
Web applications area. Giving up expressiveness, however, does lead to criticism
from the modellers’ community, as a computationally nice language may not have
the features deemed necessary to represent the subject domain adequately.

2.2.
Reasoning
35
Tools
There are many tools for automated reasoning, which diﬀer in which language they
accept, the reasoning services they provide, and, with that, the purpose they aim
to serve.
There are, among others, generic ﬁrst- and higher order logic theorem provers
(e.g., Prover9, MACE4, Vampire, HOL4), SAT solvers that compute if there is
a model for the formal theory (e.g., GRASP, Satz), Constraint Satisfaction Pro-
gramming for solving, e.g., scheduling problems and reasoning with soft constraints
(e.g., Eclipse), DL reasoners that are used for reasoning over OWL ontologies using
deductive reasoning to compute satisﬁability, consistency, and perform taxonomic
and instance classiﬁcation (e.g., Fact++, RacerPro, Hermit, CEL, QuOnto), and
inductive logic programming tools (e.g., PROGOL and Aleph).
2.2.2
Basic idea
Essential to automated reasoning are:
1. The choice of the class of problems the software program has to solve, such as
checking the consistency of a theory (i.e., whether there are no contradictions)
or computing a classiﬁcation hierarchy of concepts subsuming one another
based on the properties represented in the logical theory;
2. The formal language in which to represent the problems, which may have more
or less features to represent the subject domain knowledge, such as cardinality
constraints (e.g., that spiders have as part exactly eight legs), probabilities, or
temporal knowledge (e.g., that a butterﬂy is a transformation of a caterpillar);
3. The way how the program has to compute the solution, such as using natural
deduction or resolution; and
4. How to do this eﬃciently, be this achieved through constraining the language
into one of low complexity, or optimising the algorithms to compute the
solution, or both.
Concerning the ﬁrst item, with a problem being, e.g., “is my theory is consis-
tent?”, then the problem’s assumptions are the axioms in the logical theory and
the problem’s conclusion that is computed by the automated reasoner is a “yes” or
a “no” (provided the language in which the assumptions are represented is decid-
able and thus guaranteed to terminate with an answer). With respect to how this
is done (item iii), two properties are important for the calculus used: soundness
and completeness. To deﬁne them, note/recall that “⊢” means ‘derivable with a
set of inference rules’ and “|=” denotes ‘implies’, i.e., every truth assignment that
satisﬁes Γ also satisﬁes φ. The two properties are deﬁned as follows:
• Completeness: if Γ |= φ then Γ ⊢φ
• Soundness: if Γ ⊢φ then Γ |= φ
If the algorithm it is incomplete, then there exist entailments that cannot be com-
puted (hence, ‘missing’ some results), if it is unsound then false conclusions can be
derived from true premises, which is even more undesirable.

36
Chapter 2.
First order logic and automated reasoning in a nutshell
An example is included in Section 2.2.4, once the other ingredients have been
introduced as well: proving the validity of a formula (the class of the problem) in
propositional logic (the formal language) using tableau reasoning (the way how to
compute the solution) with the Tree Proof Generator4 (the automated reasoner);
more detail, with reﬂection and other techniques, can be found in [Por10] among
others.
2.2.3
Deduction, abduction, and induction
There are three principle ways of making the inferences—deduction, abduction,
and induction—that are described now.
Deduction
Deduction is a way to ascertain if a theory T represented in a logic language en-
tails an axiom α that is not explicitly asserted in T (written as T |= α), i.e.,
whether α can be derived from the premises through repeated application of de-
duction rules. For instance, a theory that states that “each Arachnid has as part
exactly 8 legs” and “each Tarantula is an Arachnid” then one can deduce—it is
entailed in the theory—that “Each Tarantula has as part 8 legs”. An example
is included further below (after having introduced a reasoning technique), which
formally demonstrates that a formula is entailed in a theory T using said rules.
Thus, strictly speaking, a deduction does not reveal novel knowledge, but only
that what was already represented implicitly in the theory.
Nevertheless, with
large theories, it is often diﬃcult to oversee all implications of the represented
knowledge and, hence, the deductions may be perceived as novel from a domain
expert perspective, such as with the example about the protein phosphatases. (This
is in contrast to Abduction and Induction, where the reasoner ‘guesses’ knowledge
that is not already entailed in the theory; see below).
There are various ways how to ascertain T |= α, be it manually or automatically.
One can construct a step-by-step proof ‘forward’ from the premises by applying the
deduction rules or prove it indirectly such that T ∪{¬α} must lead to a contradic-
tion. The former approach is called natural deduction, whereas the latter is based
on techniques such as resolution, matrix connection methods, and sequent deduc-
tion (which includes tableaux). How exactly that is done for tableau is described
in Section 2.2.4.
Abduction
One tries to infer a as an explanation of b. That is, we have a set of observations,
a theory of the domain of the observations, and a set of (possible, hypothesised)
explanations that one would hope to ﬁnd. For each explanation, it should be the
case that the set of observations follows from the combination of the theory and
the set of explanations, noting that the combination of the theory and the set of
4http://www.umsu.de/logik/trees/

2.2.
Reasoning
37
explanation has to be consistent. One can add additional machinery to these basics
to, e.g., ﬁnd out which of the explanations are the most interesting.
Compared to deduction, there is less permeation of automated reasoning for
abduction.
From a scientist’s perspective, automation of abduction may seem
appealing, because it would help one to generate a hypothesis based on the facts
put into the reasoner [Ali04]. Practically, it has been used for, for instance, fault
detection: given the knowledge about a system and the observed defective state,
ﬁnd the likely fault in the system. To formally capture theory with assumptions
and facts and ﬁnd the conclusion, several approaches have been proposed, each
with their speciﬁc application areas; for instance, sequent calculus, belief revision,
probabilistic abductive reasoning, and Bayesian networks.
Induction
With induction, one generalises toward a conclusion based on a set of individuals.
However, the conclusion is not a logical consequence of the premise. Thus, it allows
one to arrive at a conclusion that actually may be false even though the premises are
true. The premises provide a degree of support so as to infer a as an explanation of
b. Such a ‘degree’ can be based on probabilities (a statistical syllogism) or analogy.
For instance, we have a premise that “The proportion of bacteria that acquire genes
through horizontal gene transfer is 95%” and the fact that “Staphylococcus aureus
is a bacterium”, then we induce that the probability that S. aureus acquires genes
through horizontal gene transfer is 95%.
Induction by analogy is weaker version of reasoning, in particular in logic-
based systems, and yields very diﬀerent answers than deduction. For instance,
let us encode that some instance, Tibbles, is a cat and we know that all cats have
the properties of having a tail and four legs and that they are furry. When we
encode that another animal, Tib, who happens to have four legs and is also furry,
then by inductive reasoning by analogy, we conclude that Tib is also a cat, even
though in reality it may well be an instance of cheetah. On the other hand, by
deductive reasoning, Tib will not be classiﬁed as being an instance of cat (but may
be an instance of a superclass of cats (e.g., still within the suborder Feliformia),
provided that the superclass has declared that all instances have four legs and
are furry. Given that humans do perform such reasoning, there are attempts to
mimic this process in software applications, most notably in the area of machine
learning and inductive logic programming. The principal approach with inductive
logic programming is to take as input positive examples + negative examples +
background knowledge and then derive a hypothesised logic program that entails
all the positive and none of the negative examples.
2.2.4
Proofs with tableaux
Simply put, a proof is a convincing argument expressed in the language of math-
ematics. The steps in the process of the (automated) reasoning provide a proof.
Several outcomes are possible for a given formula:

38
Chapter 2.
First order logic and automated reasoning in a nutshell
• A formula is valid if it holds under every assignment5; this is denoted as
“|= φ”. A valid formula is called a tautology.
• A formula is satisﬁable if it holds under some assignment.
• A formula is unsatisﬁable if it holds under no assignment. An unsatisﬁable
formula is called a contradiction.
The questions that need to be answered to realise the next step are:
• How do we ﬁnd out whether a formula is valid or not?
• How do we ﬁnd out whether our theory is satisﬁable?
A rather unpractical approach is truth tables, which may be ﬁne in a paper-based
logic course, but won’t do for computing. While there are several tools with several
techniques, we will look at one that is, at the time of writing, the ‘winner’ in the
realm of ontologies: tableaux-based reasoning, which is also the principal approach
for DL reasoners and the OWL tools. A tableau provides a sound and complete
procedure that decides satisﬁability by checking the existence of a model6.
It
exhaustively looks at all the possibilities, so that it can eventually prove that no
model could be found for unsatisﬁable formulas. That is:
• φ |= ψ iﬀφ ∧¬ψ is NOT satisﬁable—if it is satisﬁable, we have found a
counterexample
It does this by decomposing the formula in top-down fashion. Tableaux calculus
works only if the formula has been translated into Negation Normal Form, however,
so the ﬁrst step in the process is:
(1.) Push the negations inside to convert a sentence into Negation Normal Form,
if applicable.
We use the aforementioned equivalences for that, which were listed on page 32. For
instance, one of the De Morgan rules is ¬(φ ∧ψ) ≡¬φ ∨¬ψ, where the ‘outer’
negation outside the brackets is pushed inside right in front of the formula, which
can be done likewise with the quantiﬁers, such as substituting ¬∀x.φ with ∃x.¬φ.
Now it is ready to enter the tableau. Use any or all of the following rules, as
applicable (in a ‘smart’ order):
(2a.) If a model satisﬁes a conjunction, then it also satisﬁes each of the conjuncts:
φ ∧ψ
φ
ψ
(2b.) If a model satisﬁes a disjunction, then it also satisﬁes one of the disjuncts
(which is non-deterministic):
φ ∨ψ
φ | ψ
5typically, the assignments are ‘true’ and ‘false’, but there are also other logics that allow
more/other assignments
6model in the sense of logic (recall Deﬁnition 2.9), not in the sense of ‘conceptual data model’
like a UML class diagram.

2.2.
Reasoning
39
(2c.) If a model satisﬁes a universally quantiﬁed formula (∀), then it also sat-
isﬁes the formula where the quantiﬁed variable has been substituted with
some term (and the prescription is to use all the terms which appear in the
tableaux),
∀x.φ
φ{X/t}
∀x.φ
(2d.) For an existentially quantiﬁed formula, if a model satisﬁes it, then it also
satisﬁes the formula where the quantiﬁed variable has been substituted with
a new Skolem constant,
∃x.φ
φ{X/a}
To complete the proof:
(3.) Apply the completion rules 2a-d until either:
(a) an explicit contradiction is generated in each branch due to the presence
of two opposite literals in a node (called a clash), or
(b) there is a completed branch where no more rule is applicable.
(4.) Determine the outcome:
(a) If all branches result in clashes, i.e., there is no completed branch, then
φ ∧¬ψ is NOT satisﬁable, which makes the original one, φ |= ψ, satis-
ﬁable.
(b) If there is a completed branch, then we have found a model for φ ∧¬ψ,
hence, have found a counterexample for some assignments of the original
φ |= ψ, hence φ ⊭ψ.
This completes the procedure.
One can also do the above for individual formulas and not bother with the
negation at the start and then simply check if one can ﬁnd a model for some
formula; i.e., ‘try to build a model’ (to see if everything can be instantiated) with
the completion rules cf. the ‘check there is no model (when negated)’ of the tableau
procedure above. In that case, the conclusions one should draw are the opposite,
i.e., then if there’s a completed branch it means that that formula is satisﬁable
for it has found a model, and if there are only clashes, the tableau cannot ﬁnd
a model so there’s some contradiction in the formula. For instance, the formula
∃x(p(x) ∧¬q(x)) ∧∀y(¬p(y) ∨q(y)) is unsatisﬁable.
Let’s have a look at how to apply all this, which is depicted in Figure 2.3 and
described in the example below.
Example 2.3. Let us take some arbitrary theory T that contains two axioms
stating that relation R is reﬂexive (i.e., ∀x(R(x, x)), a thing relates to itself) and
asymmetric (i.e., ∀x, y(R(x, y) →¬R(y, x)), if a thing a relates to b through rela-
tion R, then b does not relate back to a). We then can deduce, among others, that
T ∪{¬∀x, y(R(x, y))} is satisﬁable. We do this by demonstrating that the negation
of the axiom is unsatisﬁable.

40
Chapter 2.
First order logic and automated reasoning in a nutshell
To enter the tableau, we ﬁrst rewrite the asymmetry into a disjunction us-
ing equivalences, i.e., ∀x, y(R(x, y) →¬R(y, x)) is equivalent to ∀x, y(¬R(x, y) ∨
R(y, x)), thanks to applying the implication rule from page 32. Then add a negation
to {¬∀x, y(R(x, y))}, which thus becomes ∀x, y(R(x, y)). So, to start the tableau,
we have three axioms (1, 2, 3), and subsequently the full tableau as in Figure 2.3.
♦
Figure 2.3: Tableau example (using notation with a “.” not the brackets).
This is a fairly simple example that uses rules 2d and 2b and only a few steps.
It can quickly get more elaborate even for simpler languages such as propositional
logic. If you have not seen propositional logic, you are free to skip the following
example; else you may want to read through it: all the 19 (!) steps use only rules
2a and 2b (as propositional logic does not have the variables and quantiﬁers).
Example 2.4. A sample computation to prove automatically whether the propo-
sitional formula ((p ∨(q ∧r)) →((p ∨q) ∧(p ∨r)) is valid or not is included in
Figure 2.4 and Figure 2.5, using tableau reasoning (see Deduction, Section 2.2.3).
The tableau method is a decision procedure that checks the existence of a model
(i.e., that it can be instantiated). It exhaustively looks at all the possibilities, so
that it can eventually prove that no model could be found for unsatisﬁable formulas
(if it is satisﬁable, we have found a counterexample). This is done by decomposing
the formula in top-down fashion after it has been translated into Negation Normal
Form (i.e., all the negations have been pushed inside), which can be achieved using
equivalences. Further, if a model satisﬁes a conjunction, then it also satisﬁes each
of the conjuncts (“∧”), and if a model satisﬁes a disjunction (“∨”), then it also
satisﬁes one of the disjuncts (this is a non-deterministic rule and it generates two
alternative branches). Last, one has to apply these completion rules until either

2.3.
Exercises
41
(a) an explicit contradiction is obtained due to the presence of two opposite literals
in a node (a clash) is generated in each branch, or (b) there is a completed branch
where no more rule is applicable. ♦
Figure 2.4: Sample computation using semantic tableau proving that the propositional
formula is valid; see Figure 2.5 for an explanation.
2.3
Exercises
Review question 2.1. What is the diﬀerence between syntax and semantics for
a logic?
Review question 2.2. What is a theory?
Review question 2.3. Name the four core components for automated reasoning.
Review question 2.4. Describe the procedure for tableau reasoning in four shorts
sentences.
Exercise 2.1. Write in one natural language sentence what the following sentences
in First-Order Logic state. *
a. ∀x(Lion(x) →Mammal(x))
b. ∀x(PC(x) →∃y, z(hasPart(x, y)∧connected(x, z)∧CPU(y)∧Monitor(z)))
c. ∀x, y(hasProperPart(x, y) →¬hasProperPart(y, x))
Exercise 2.2. Formalise the following natural language sentence into First-Order
Logic. *
a. Each car is a vehicle.

42
Chapter 2.
First order logic and automated reasoning in a nutshell
Figure 2.5: Explanation of the tableaux in Figure 2.4.
b. Every human parent has at least one human child.
c. Any person cannot be both a lecturer and a student editor of the same course.
Exercise 2.3. Consider the structures in Figure 2.6, which are graphs.
a. Figures 2.6-A and B are diﬀerent depictions, but have the same descriptions
w.r.t. the vertices and edges. Check this.
b. C has a property that A and B do not have. Represent this in a ﬁrst-order
sentence. *
c. Find a suitable ﬁrst-order language for A (/B), and formulate at least two
properties of the graph using quantiﬁers. *
A
B
C
D
a
b
Figure 2.6: Graphs for Exercise 2.3 (ﬁgures A-C) and Exercise 2.4 (ﬁgure D).
Exercise 2.4. Consider the graph in Figure 2.6, and ﬁrst-order language L = ⟨R⟩,
with R being a binary relation symbol (edge).

2.3.
Exercises
43
a. Formalise the following properties of the graph as L-sentences: (i) (a, a) and
(b, b) are edges of the graph; (ii) (a, b) is an edge of the graph; (iii) (b, a) is
not an edge of the graph. Let T stand for the resulting set of sentences. *
b. Prove that T ∪{∀x∀yR(x, y)} is unsatisﬁable using tableaux calculus. *
Exercise 2.5. Let us have a logical theory Θ with the following sentences:
• ∀xPizza(x), ∀xPizzaT(x), ∀xPizzaB(x), which are disjoint
• ∀x(Pizza(x) →¬PizzaT(x)),
• ∀x(Pizza(x) →¬PizzaB(x)),
• ∀x(PizzaT(x) →¬PizzaB(x)),
• ∀x, y(hasT(x, y) →Pizza(x) ∧PizzaT(y)),
• ∀x, y(hasB(x, y) →Pizza(x) ∧PizzaB(y)),
• ∀x(ITPizza(x) →Pizza(x)), and
• ∀x(ITPizza(x) →¬∃y(hasT(x, y) ∧FruitT(y)), where
• ∀x(V egeT(x) →PizzaT(x)) and
• ∀x(FruitT(x) →PizzaT(x)).
Task (read in full ﬁrst before attempting it):
a. A Pizza margherita has the necessary and suﬃcient conditions that it has
mozzarella, tomato, basilicum and oil as toppings and has a pizza base. Add
this to Θ.
Annotate you commitments: what have you added to Θ and how?
Hint: fruits are not vegetables, categorise the toppings, and “necessary and
suﬃcient” is denoted with ↔.
b. We want to merge our new Θ with some other theory Γ that has knowledge
about fruits and vegetables. Γ contains, among other formulas, ∀x(Tomato(x) →
Fruit(x)). What happens? Represent the scenario formally, and prove your
answer.
Actually, this is not easy to ﬁgure out manually, and there are ways to automate
this, which you will do later in Chapter 4.
Exercise 2.6. Try to formalise the following natural language sentences into FOL
now or in DL later in the next chapter, or both. (optional practice). *
1. All lions are animals
2. Each professor teaches at least course
3. All humans eat some fruit and cheese
4. Animals are either herbivores or carnivores
5. The sister of one’s mother is one’s aunt
6. Something can’t be both pap and pizza
7. If a person works for a company, then that person is an employee
8. Anything that manages something is a manager, and vice versa.
9. All ﬂies have exactly two eyes as part
10. Anything has at most one life
11. The participation relation is deﬁned by relating physical objects to processes
12. Having something as part is the inverse of being part of something
13. connection is symmetric (if one thing is connected to something else, that
other thing is also connected to the one thing)
14. A vehicle can be either motorised or not, but not both

44
Chapter 2.
First order logic and automated reasoning in a nutshell
15. Several snails are slow
16. Each patient is registered at some hospital on a certain weekday
17. All students are at some time in their life not a student
2.4
Literature and reference material
The following literature is optional for the scope of ontology engineering, but you
may ﬁnd of interest if you would like to design your own logic, for instance.
1. Hedman, S. A ﬁrst course in logic—an introduction to model theory, proof the-
ory, computability, and complexity. Oxford: Oxford University Press. 2004.
2. Solow, D. How to read and do proofs. 4th Ed. Wiley. 2005.

CHAPTER 3
Description Logics
A Description Logic (DL) is a structured fragment of FOL; more precisely: any
(basic) Description Logic language is a subset of L3, i.e., the function-free FOL
using only at most three variable names. Its representation is at the predicate level:
no variables are present in the formalism. DLs provide a logical reconstruction and
(claimed to be a) unifying formalism for other knowledge representation languages,
such as frames-based systems, object-oriented modelling, Semantic data models,
etc. They provide the language to formulate theories and systems declaratively
expressing structured knowledge and for accessing it and reasoning with it, and they
are used for, among others, terminologies and ontologies, logic-based conceptual
data modelling, and information integration.
Figure 3.1 shows a basic overview of the principal components of a DL knowl-
edge base, with the so-called TBox containing the knowledge at the class-level and
the ABox containing the data (individuals). Sometimes you will see added to the
ﬁgure an RBox, which is used to make explicit there are relationships and the
axioms that hold for them.
Knowledge base
TBox
(Terminology)
ABox
(Assertions)
Description 
language
(a logic)
Automated 
reasoning
(over the TBox 
and ABox)
Interaction with
user applications
Interaction with other 
technologies 
Figure 3.1: A Description Logic knowledge base. Sometimes you will see a similar
picture extended with an “RBox”, which denotes the DL roles and their axioms.
45

46
Chapter 3. Description Logics
The remainder of this section contains, ﬁrst, a general introduction to DL (Sec-
tion 3.1), which are the ﬁrst ﬁve sections of the DL Primer [KSH12], and is repro-
duced here with permission of its authors Markus Kr¨otzsch, Frantiˇsek Simanˇc´ık,
and Ian Horrocks1. (Slightly more detailed introductory notes with examples can
be found in the ﬁrst 8 pages of [Tur10] and the ﬁrst 10 pages of [Sat07]; a DL
textbook is in the pipeline). We then proceed to several important DLs out of the
very many DLs investigated, being ALC and SROIQ, in Section 3.2. We then
proceed to describing and illustrating the standard reasoning services for DLs in
Section 3.3, which essentially applies and extends the tableau reasoning of the pre-
vious chapter. Note that DLs and its reasoning services return in Chapter 4 about
OWL 2, building upon the theoretical foundations introduced in this chapter.
3.1
DL primer
Description logics (DLs) are a family of knowledge representation languages that
are widely used in ontology development. An important practical reason for this is
that they provide one of the main underpinnings for the Web Ontology Language
OWL as standardised by the World Wide Web Consortium (W3C). However, DLs
have been used in knowledge representation long before the advent of ontologies in
the context of the Semantic Web, tracing back to ﬁrst DL modelling languages in
the mid 1980s.
As their name suggests, DLs are logics (in fact they are decidable fragments
of ﬁrst-order logic), and as such they are equipped with a formal semantics: a
precise speciﬁcation of the meaning of DL ontologies. This formal semantics allows
humans and computer systems to exchange DL ontologies without ambiguity as
to their intended meaning, and also makes it possible to use logical deduction to
infer additional information from the facts stated explicitly in an ontology – an
important feature that distinguishes DLs from other modelling languages such as
UML.
The capability of inferring additional knowledge increases the modelling power
of DLs but it also requires some understanding on the side of the modeller and,
above all, good tool support for computing the conclusions. The computation of
inferences is called reasoning and an important goal of DL language design has been
to ensure that reasoning algorithms of good performance are available. This is one
of the reasons why there is not just a single description logic: the best balance
between expressivity of the language and complexity of reasoning depends on the
intended application.
We provide a self-contained ﬁrst introduction to Description Logics. We start
by explaining the basic way in which knowledge is modelled in DLs in Section 3.1.1
and continue with an intuitive introduction to the most important DL modelling
features in Section 3.1.2.
In Section 3.1.3, we explain the underlying ideas of
1I harmonised the terminology so as to use the same terms throughout the book, cf. adding
synonyms at this stage, and added a few references to other sections in this book to integrate the
text better. Also, I moved their SROIQ section into Section 3.2 and inserted ALC there, and
made the family composition examples more inclusive.

3.1.
DL primer
47
DL semantics. Then a common starting point of DLs is introduced, being ALC,
and subsequently the rather expressive DL called SROIQ (summarised in Sec-
tion 3.2.2), which forms the basis of the OWL 2 DL language. Many DLs can
be obtained by omitting some features of SROIQ and in Section 3.2.3 we review
some of the most important DLs obtained in this way. In particular, this includes
various light-weight description logics that allow for particularly eﬃcient reasoning
and are also standardised, as fragments of OWL.
3.1.1
Basic building blocks of DL ontologies
Description logics (DLs) provide means to model the relationships between entities
in a domain of interest. In DLs there are three kinds of entities: concepts, roles and
individual names. Concepts denote sets of individuals, roles denote sets of binary
relations between the individuals2, and individual names denote single individuals
in the domain. Readers familiar with ﬁrst-order logic will recognise these as unary
predicates, binary predicates and constants.
For example, an ontology representing the domain of people and their family
relationships might use concepts such Parent to denote the set of all parents and
Female to represent the set of all female individuals, roles such as parentOf to denote
the (binary) relationship between parents and their children, and individual names
such as julia and john to denote the individuals Julia and John.
Unlike a database, a DL ontology does not fully describe a particular situation
or “state of the world”; rather it consists of a set of statements, called axioms,
each of which must be true in the situation described.
These axioms typically
capture only partial knowledge about the situation that the ontology is describing,
and there may be many diﬀerent states of the world that are consistent with the
ontology. Although, from the point of view of logic, there is no principal diﬀerence
between diﬀerent types of axioms, it is customary to separate them into three
groups: assertional (ABox) axioms, terminological (TBox) axioms and relational
(RBox) axioms.
Asserting Facts with ABox Axioms
ABox axioms capture knowledge about
named individuals, i.e., the concepts to which they belong and how they are related
to each other. The most common ABox axioms are concept assertions such as
Mother(julia)
(3.1)
which asserts that Julia is a mother or, more precisely, that the individual named
julia is an instance of the concept Mother.
Role assertions describe relations between named individuals. The assertion
parentOf(julia, john)
(3.2)
for example, states that Julia is a parent of John or, more precisely, that the indi-
vidual named julia is in the relation that is denoted by parentOf to the individual
2There are a few DLs that permit n-aries, which are the DLR and CFD families of logics that
were inspired by conceptual data modelling and relational models.

48
Chapter 3. Description Logics
named john. The previous sentence shows that it can be rather cumbersome to
explicitly point out that the relationships expressed by an axiom are really re-
lationships between the individuals, sets and relations that are denoted by the
respective individual names, concepts and roles. Assuming that this subtle distinc-
tion between syntactic identiﬁers and semantic entities is understood, we will thus
often adopt a more sloppy and readable formulation. Section 3.1.3 below explains
the underlying semantics with greater precision3.
Although it is intuitively clear that Julia and John are diﬀerent individuals, this
fact does not logically follow from what we have stated so far. DLs may or may
not make the unique name assumption, so diﬀerent names might refer to the same
individual unless explicitly stated otherwise4. The individual inequality assertion
julia ̸≈john
(3.3)
is used to assert that Julia and John are actually diﬀerent individuals. On the
other hand, an individual equality assertion, such as
john ≈johnny
(3.4)
states that two diﬀerent names are known to refer to the same individual. Such sit-
uations can arise, for example, when combining knowledge about the same domain
from several diﬀerent sources, a task that is known as ontology alignment.
Expressing Terminological Knowledge with TBox Axioms
TBox axioms
describe relationships between concepts. For example, the fact that all mothers are
parents is expressed by the concept inclusion
Mother ⊑Parent
(3.5)
in which case we say that the concept Mother is subsumed by the concept Parent.
Such knowledge can be used to infer further facts about individuals. For example,
(3.1) and (3.5) together imply that Julia is a parent.
Concept equivalence asserts that two concepts have the same instances, as in
Person ≡Human
(3.6)
While synonyms may be perceived to be an obvious example of equivalent concepts,
in practice one uses concept equivalence to give a name to complex expressions as
introduced in Section 3.1.2 below and put real synonyms in the annotations or extra
labels in the ontology ﬁle. Furthermore, such additional concept expressions can
be combined with equivalence and inclusion to describe more complex situations
such as the disjointness of concepts, which asserts that two concepts do not share
any instances.
3It is the same as the model-theoretic semantics we have seen in Section 2.1.2, but then restated
for DLs.
4i.e., the unique name assumption (UNA) means that diﬀerent names refer to diﬀerent indi-
viduals, as is customary in the database world. There are consequences for UNA/no-UNA both
regarding computational complexity and automated reasoners: no-UNA is more costly and the
deductions are diﬀerent when reasoning with UNA or not. We will see the eﬀects of especially
the latter in the exercises of Chapter 4.

3.1.
DL primer
49
Modelling Relationships between Roles with RBox Axioms
RBox axioms
refer to properties of roles. As for concepts, DLs support role inclusion and role
equivalence axioms. For example, the inclusion
parentOf ⊑ancestorOf
(3.7)
states that parentOf is a subrole of ancestorOf, i.e., every pair of individuals related
by parentOf is also related by ancestorOf. Thus (3.2) and (3.7) together imply that
Julia is an ancestor of John.
In role inclusion axioms, role composition can be used to describe roles such
as uncleOf. Intuitively, if Charles is a brother of Julia and Julia is a parent of
John, then Charles is an uncle of John. This kind of relationship between the roles
brotherOf, parentOf and uncleOf is captured by the complex role inclusion axiom
brotherOf ◦parentOf ⊑uncleOf
(3.8)
Note that role composition can only appear on the left-hand side of complex role in-
clusions. Furthermore, in order to retain decidability of reasoning (see Appendix D
for a recap on complexity and decidability), their use is restricted by additional
structural restrictions that specify whether or not a collection of such axioms can
be used together in one ontology.
Nobody can be both a parent and a child of the same individual, so the two roles
parentOf and childOf are disjoint. In DLs we can write disjoint roles as follows:
Disjoint(parentOf, childOf)
(3.9)
Further RBox axioms include role characteristics such as reﬂexivity, symmetry
and transitivity of roles. These are closely related to a number of other DL features
and we will discuss them again in more detail in Section 3.1.2.
3.1.2
Constructors for concepts and roles
The basic types of axioms introduced in Section 3.1.1 are rather limited for accurate
modelling. To describe more complex situations, DLs allow new concepts and roles
to be built using a variety of diﬀerent constructors. We distinguish concept and
role constructors depending on whether concept or role expressions are constructed.
In the case of concepts, one can further separate basic Boolean constructors, role
restrictions and nominals/enumerations. At the end of this section, we revisit the
additional kinds of RBox axioms that have been omitted in Section 3.1.1.
Boolean Concept Constructors
Boolean concept constructors provide basic
Boolean operations that are closely related to the familiar operations of intersection,
union and complement of sets, or to conjunction, disjunction and negation of logical
expressions.
For example, concept inclusions allow us to state that all mothers are female
and that all mothers are parents, but what we really mean is that mothers are

50
Chapter 3. Description Logics
exactly the female parents. DLs support such statements by allowing us to form
complex concepts such as the intersection (also called conjunction)
Female ⊓Parent
(3.10)
which denotes the set of individuals that are both female and parents. A complex
concept can be used in axioms in exactly the same way as an atomic concept, e.g.,
in the equivalence Mother ≡Female ⊓Parent.
Union (also called disjunction) is the dual of intersection. For example, the
concept
Father ⊔Mother
(3.11)
describes those individuals that are either (biological) fathers or mothers. Again,
it can be used in an axiom such as Parent ≡Father ⊔Mother, which states that
a parent is either a father or a mother (and vice versa).
One can extend this
further, of course, by stating, e.g., that a parent is equivalent to father or mother
or guardian: Parent ≡Father ⊔Mother ⊔Guardian.
Sometimes we are interested in individuals that do not belong to a certain
concept, e.g., in women who are not married. These could be described by the
complex concept
Female ⊓¬Married
(3.12)
where the complement (also called negation) ¬Married denotes the set of all indi-
viduals that are not married.
It is sometimes useful to be able to make a statement about every individual,
e.g., to say that everybody is either male or female. This can be accomplished by
the axiom
⊤⊑Male ⊔Female
(3.13)
where the top concept ⊤is a special concept with every individual as an instance;
it can be viewed as an abbreviation for C ⊔¬C for an arbitrary concept C. Note
that this modelling is rather coarse as it presupposes that every individual has a
gender, which may not be reasonable, especially for instances of a concept such as
Computer. We will see more useful applications for ⊤later on.
To express that, for the purposes of our modelling, nobody can be both a parent
and childless at the same time, we can declare the set of parents and the set of
childless individuals to be disjoint. While ontology languages like OWL provide a
basic constructor for disjointness, it is naturally captured in DLs with the axiom
Parent ⊓Childless ⊑⊥
(3.14)
where the bottom concept ⊥is the dual of ⊤, that is the special concept with
no individuals as instances; it can be seen as an abbreviation for C ⊓¬C for an
arbitrary concept C. The above axiom thus says that the intersection of the two
concepts is empty.
Role Restrictions
So far we have seen how to use TBox and RBox axioms to
express relationships between concepts and roles, respectively. The most interesting

3.1.
DL primer
51
feature of DLs, however, is their ability to form statements that link concepts and
roles together. For example, there is an obvious relationship between the concept
Parent and the role parentOf, namely, a parent is someone who is a parent of at
least one individual. In DLs, this relationship can be captured by the concept
equivalence
Parent ≡∃parentOf.⊤
(3.15)
where the existential restriction ∃parentOf.⊤is a complex concept that describes
the set of individuals that are parents of at least one individual (instance of ⊤).
Similarly, the concept ∃parentOf.Female describes those individuals that are parents
of at least one female individual, i.e., those that have a daughter.
To denote the set of individuals all of whose children are female, we use the
universal restriction
∀parentOf.Female
(3.16)
It is a common error to forget that (3.16) also includes those that have no children
at all. More accurately (and less naturally), the axiom can be said to describe the
set of all individuals that have “no children other than female ones,” i.e., no “no
children that are not female.” Following this wording, the concept (3.16) could
indeed be equivalently expressed as ¬∃parentOf.¬Female (recall the equivalences
on page 32). If this meaning is not intended, one can describe the individuals who
have at least one child and with all their children being female by the concept
(∃parentOf.⊤) ⊓(∀parentOf.Female). To state the diﬀerence between Eq. 3.16 and
the latter in another way: Eq. 3.16 says “if has children, then all female” and latter
“does have children, and all female”.
Existential and universal restrictions are useful in combination with the top
concept for expressing domain and range restrictions on roles; that is, restrictions
on the kinds of individual that can be in the domain and range of a given role. To
restrict the domain of sonOf to male individuals we can use the axiom
∃sonOf.⊤⊑Male
(3.17)
and to restrict its range to parents we can write
⊤⊑∀sonOf.Parent
(3.18)
In combination with the assertion sonOf(john, julia), these axioms would then allow
us to deduce that John is male and Julia is a parent.
Number restrictions allow us to restrict the number of individuals that can be
reached via a given role. For example, we can form the at-least restriction
⩾2 childOf.Parent
(3.19)
to describe the set of individuals that are children of at least two parents, and the
at-most restriction
⩽2 childOf.Parent
(3.20)
for those that are children of at most two parents. The axiom Person ⊑⩾2 childOf.Parent
⊓⩽2 childOf.Parent then states that every person is a child of exactly two parents.
A shorthand notation is Person ⊑= 2 childOf.Parent

52
Chapter 3. Description Logics
Finally, local reﬂexivity can be used to describe the set of individuals that are
related to themselves via a given role. For example, the set of individuals that are
talking to themselves is described by the concept
∃talksTo.Self
(3.21)
Nominals
As well as deﬁning concepts in terms of other concepts (and roles), it
may also be useful to deﬁne a concept by simply enumerating its instances. For
example, we might deﬁne the concept Beatle by enumerating its instances: john,
paul, george, and ringo.
Enumerations are not supported natively in DLs, but
they can be simulated in DLs using nominals. A nominal is a concept that has
exactly one instance. For example, {john} is the concept whose only instance is (the
individual denoted by) john. Combining nominals with union, the enumeration in
our example could be expressed as
Beatle ≡{john} ⊔{paul} ⊔{george} ⊔{ringo}
(3.22)
It is interesting to note that, using nominals, a concept assertion Mother(julia)
can be turned into a concept inclusion {julia} ⊑Mother and a role assertion
parentOf(julia, john) into a concept inclusion {julia} ⊑∃parentOf.{john}. This illus-
trates that the distinction between ABox and TBox does not have a deeper logical
meaning5.
Role Constructors
In contrast to the variety of concept constructors, DLs pro-
vide only few constructor for forming complex roles. In practice, inverse roles are
the most important such constructor. Intuitively, the relationship between the roles
parentOf and childOf is that, for example, if Julia is a parent of John, then John
is a child of Julia and vice versa. More formally, parenfOf is the inverse of childOf,
which in DLs can be expressed by the equivalence
parentOf ≡childOf−
(3.23)
where the complex role childOf−denotes the inverse of childOf.
In analogy to the top concept, DLs also provide the universal role, denoted by
U, which always relates all pairs of individuals. It typically plays a minor role in
modelling,6 but it establishes symmetry between roles and concepts w.r.t. a top
element. Similarly, an empty role that corresponds to the bottom concept is also
available in OWL but has rarely been introduced as a constructor in DLs; however,
we can deﬁne any role R to be empty using the axiom ⊤⊑¬∃R.⊤(“all things
do not relate to anything through R”). Interestingly, the universal role cannot be
deﬁned by TBox axioms using the constructors introduced above, and in particular
universal role restrictions cannot express that a role is universal.
5It does so ontologically, to which we shall return in Block II.
6Although there are a few interesting things that could be expressed with U, such as concept
products [RKH08a], tool support is rarely suﬃcient for using this feature in practice.

3.1.
DL primer
53
More RBox Axioms: Role Characteristics
In Section 3.1.1 we introduced
three forms of RBox axioms: role inclusions, role equivalences and role disjointness.
OWL provides a variety of others, namely role transitivity, symmetry, asymmetry,
reﬂexivity and irreﬂexivity. These are sometimes considered as basic axiom types in
DLs as well, using some suggestive notation such as Trans(ancestorOf) to express
that the role ancestorOf is transitive.
However, such axioms are just syntactic
sugar; all role characteristics can be expressed using the features of DLs that we
have already introduced.
Transitivity is a special form of complex role inclusion. For example, transitivity
of ancestorOf can be captured by the axiom ancestorOf◦ancestorOf ⊑ancestorOf. A
role is symmetric if it is equivalent to its own inverse, e.g., marriedTo ≡marriedTo−,
and
it
is
asymmetric
if
it
is
disjoint
from
its
own
inverse,
as
in
Disjoint(parentOf, parentOf−).
If desired, global reﬂexivity can be expressed by
imposing local reﬂexivity on the top concept as in ⊤⊑∃knows.Self . A role is
irreﬂexive if it is never locally reﬂexive, as in the case of ⊤⊑¬∃marriedTo.Self .
3.1.3
Description Logic semantics
The formal meaning of DL axioms is given by their semantics.
In particular,
the semantics speciﬁes what the logical consequences of an ontology are.
The
formal semantics is therefore the main guideline for every tool that computes logical
consequences of DL ontologies, and a basic understanding of its working is vital
to make reasonable modelling choices and to comprehend the results given by
software applications. Luckily, the semantics of description logics is not diﬃcult to
understand provided that some common misconceptions are avoided.
Intuitively speaking, an ontology describes a particular situation in a given do-
main of discourse. For example, the axioms in Sections 3.1.1 and 3.1.2 describe a
particular situation in the “families and relationships” domain. However, ontolo-
gies usually cannot fully specify the situation that they describe. On the one hand,
there is no formal relationship between the symbols we use and the objects that
they represent: the individual name julia, for example, is just a syntactic identiﬁer
with no intrinsic meaning. Indeed, the intended meaning of the identiﬁers in our
ontologies has no inﬂuence on their formal semantics: what we know about them
stems only from the axioms. On the other hand, the axioms in an ontology typ-
ically do not provide complete information. For example, Eqs. (3.3) and (3.4) in
Section 3.1.1 state that some individuals are equal and that others are unequal,
but in many other cases this information might be left unspeciﬁed.
Description logics have been designed to deal with such incomplete information.
Rather than making default assumptions in order to fully specify one particular
interpretation for each ontology, the DL semantics generally considers all the pos-
sible situations (i.e., states of the world) where the axioms of an ontology would
hold (we also say: where the axioms are satisﬁed). This characteristic is called the
Open World Assumption since it keeps unspeciﬁed information open.7 A logical
7A Closed World Assumption “closes” the interpretation by assuming that every fact not
explicitly stated to be true is actually false. Both terms are not formally speciﬁed and rather
outline the general ﬂavour of a semantics than any particular deﬁnition.

54
Chapter 3. Description Logics
Table 3.1: Syntax and semantics of SROIQ constructors.
Syntax
Semantics
Individuals:
individual name
a
aI
Roles:
atomic role
R
RI
inverse role
R−
{⟨x, y⟩| ⟨y, x⟩∈RI}
universal role
U
∆I × ∆I
Concepts:
atomic concept
A
AI
intersection
C ⊓D
CI ∩DI
union
C ⊔D
CI ∪DI
complement
¬C
∆I \ CI
top concept
⊤
∆I
bottom concept
⊥
∅
existential restriction
∃R.C
{x | some RI-successor of x is in CI}
universal restriction
∀R.C
{x | all RI-successors of x are in CI}
at-least restriction
⩾n R.C
{x | at least n RI-successors of x are in CI}
at-most restriction
⩽n R.C
{x | at most n RI-successors of x are in CI}
local reﬂexivity
∃R.Self
{x | ⟨x, x⟩∈RI}
nominal
{a}
{aI}
where a, b ∈NI are individual names, A ∈NC is a concept name, C, D ∈C are
concepts, and R ∈R is a role
consequence of an ontology is an axiom that holds in all interpretations that satisfy
the ontology, i.e., something that is true in all conceivable states of the world that
agree with what is said in the ontology. The more axioms an ontology contains, the
more speciﬁc are the constraints that it imposes on possible interpretations, and
the fewer interpretations exist that satisfy all of the axioms (recall Section 1.2.3
on good ontologies with high precision). Conversely, if fewer interpretations satisfy
an ontology, then more axioms hold in all of them, and more logical consequences
follow from the ontology. The previous two sentences imply that the semantics of
description logics is monotonic: additional axioms always lead to additional con-
sequences, or, more informally, the more knowledge we feed into a DL system the
more results it returns.
An extreme case is when an ontology is not satisﬁed in any interpretation. The
ontology is then called unsatisﬁable or inconsistent. In this case every axiom holds
vacuously in all of the (zero) interpretations that satisfy the ontology. Such an
ontology is clearly of no utility, and avoiding inconsistency (and checking for it in
the ﬁrst place) is therefore an important task during ontology development.
We have outlined above the most important ideas of DL semantics. What re-
mains to be done is to deﬁne what we really mean by an “interpretation” and which
conditions must hold for particular axioms to be satisﬁed by an interpretation. For
this, we closely follow the intuitive ideas established above: an interpretation I
consists of a set ∆I called the domain of I and an interpretation function ·I that

3.1.
DL primer
55
Table 3.2: Syntax and semantics of SROIQ axioms.
Syntax
Semantics
ABox:
concept assertion
C(a)
aI ∈CI
role assertion
R(a, b)
⟨aI, bI⟩∈RI
individual equality
a ≈b
aI = bI
individual inequality
a ̸≈b
aI ̸= bI
TBox:
concept inclusion
C ⊑D
CI ⊆DI
concept equivalence
C ≡D
CI = DI
RBox:
role inclusion
R ⊑S
RI ⊆SI
role equivalence
R ≡S
RI = SI
complex role inclusion
R1 ◦R2 ⊑S
RI
1 ◦RI
2 ⊆SI
role disjointness
Disjoint(R, S)
RI ∩SI = ∅
maps each atomic concept A to a set AI ⊆∆I, each atomic role R to a binary
relation RI ⊆∆I × ∆I, and each individual name a to an element aI ∈∆I. The
interpretation of complex concepts and roles follows from the interpretation of the
basic entities. Table 3.1 shows how to obtain the semantics of each compound
expression from the semantics of its parts. By “RI-successor of x” we mean any
individual y such that ⟨x, y⟩∈RI. The deﬁnition should conﬁrm the intuitive
explanations given for each case in Section 3.1.2. For example, the semantics of
Female ⊓Parent is indeed the intersection of the semantics of Female and Parent.
Since an interpretation I ﬁxes the meaning of all entities, we can unambiguously
say for each axiom whether it holds in I or not. An axiom holds in I (we also
say I satisﬁes α and write I |= α) if the corresponding condition in Table 3.2 is
met. Again, these deﬁnitions fully agree with the intuitive explanations given in
Section 3.1.1. If all axioms in an ontology O hold in I (i.e., if I satisﬁes O, written
I |= O), then I is a model of O. Thus a model is an abstraction of a state of the
world that satisﬁes all axioms in the ontology. An ontology is consistent if it has
at least one model. An axiom α is a consequence of an ontology O (or O entails
α written O |= α) if α holds in every model of O. In particular, an inconsistent
ontology entails every axiom.
A noteworthy consequence of this semantics is the meaning of individual names
in DL ontologies. We already remarked that DLs do not usually make the Unique
Name Assumption, and indeed our formal deﬁnition allows two individual names to
be interpreted as the same individual (element of the domain). Possibly even more
important is the fact that the domain of an interpretation is allowed to contain
many individuals that are not denoted by any individual name. A common con-
fusion in modelling arises from the implicit assumption that interpretations must
only contain individuals that are denoted by individual names (such individuals
are also called named individuals). For example, one could wrongly assume the

56
Chapter 3. Description Logics
ontology consisting of the axioms
parentOf(julia, john)
manyChildren(julia)
manyChildren ⊑⩾3 parentOf.⊤
to be inconsistent since it requires Julia to have at least 3 children when only
one (John) is given. However, there are many conceivable models where Julia does
have three children, even though only one of them is explicitly named. A signiﬁcant
number of modelling errors can be traced back to similar misconceptions that are
easy to prevent if the general open world assumption of DLs is kept in mind.
Another point to note is that the above speciﬁcation of the semantics does not
provide any hint as to how to compute the relevant entailments in practical soft-
ware tools. There are inﬁnitely many possible interpretations, each of which may
have an inﬁnite domain (in fact there are some ontologies that are satisﬁed only by
interpretations with inﬁnite domains). Therefore it is impossible to test all inter-
pretations to see if they model a given ontology, and impossible to test all models
of an ontology to see if they entail a given axiom. Rather, one has to devise con-
crete deduction procedures and prove their correctness with respect to the above
speciﬁcation. The interplay of certain expressive features can make reasoning al-
gorithms more complicated and in some cases it can even be shown that no correct
and terminating algorithm exists at all (i.e., that reasoning is undecidable). For
our purposes it suﬃces to know that entailment of axioms is decidable for SROIQ
(with the structural restrictions explained in Section 3.2.2, below) and that a num-
ber of free and commercial tools are available. Such tools are typically optimised
for more speciﬁc reasoning problems, such as consistency checking, the entailment
of concept subsumptions (subsumption checking) or of concept assertions (instance
checking). Many of these standard inferencing problems can be expressed in terms
of each other, so they can be handled by very similar reasoning algorithms.
3.2
Important DLs
There are very many DLs, of which some are used more often than others. In
this section, we will ﬁrst look at ALC, for it is typically one of the languages used
in DL courses, a basis to add various language features to, and it is much easier
for showing how the principles of tableau work for DLs.
Subsequently, we list
the more expressive SROIQ, and ﬁnally comment on leaner fragments that are
computationally better behaved.
3.2.1
A basic DL to start with: ALC
The DL language ALC—which stands for Attributive Language with Concept
negation—contains the following elements:
- Concepts denoting entity types/classes/unary predicates/universals, includ-
ing top ⊤and bottom ⊥;
- Roles denoting relationships/associations/binary predicates/properties;
- Constructors: and ⊓, or ⊔, and not ¬; quantiﬁers ∀and ∃;

3.2.
Important DLs
57
- Complex concepts using constructors: Let C and D be concept names, R a
role name, then
– ¬C, C ⊓D, and C ⊔D are concepts, and
– ∀R.C and ∃R.C are concepts
- Individuals
Some examples that can be represented in ALC are, respectively:
- Concepts (primitive, atomic); e.g., Book, Course
- Roles; e.g., enrolled, reads
- Complex concepts; e.g.,
– Student ⊑∃enrolled.(Course ⊔DegreeProgramme)
(this is a primitive concept)
– Mother ⊑Woman ⊓∃ParentOf.Person
– Parent ≡(Male ⊔Female) ⊓∃ParentOf.Mammal ⊓∃caresFor.Mammal
(this is a deﬁned concept)
- Individuals;
e.g.,
Student(Andile),
Mother(Katniss),
¬Student(Katniss),
enrolled(Andile, COMP101)
As usual, the meaning is deﬁned by the semantics of ALC, and it follows the
same approach as we have seen for the other languages that have passed the revue
(recollect FOL and model-theoretic semantics from Section 2.1). First, there is a
domain of interpretation, and an interpretation, where:
– Domain ∆is a non-empty set of objects
– Interpretation: ·I is the interpretation function, domain ∆I
– ·I maps every concept name A to a subset AI ⊆∆I
– ·I maps every role name R to a subset RI ⊆∆I × ∆I
– ·I maps every individual name a to elements of ∆I: aI ∈∆I
Note that ⊤I = ∆I and ⊥I = ∅.
Using the typical notation where C and D are concepts, R a role, and a and b
are individuals, then they have the following meaning, with on the left-hand side
of the “=” the syntax of ALC under an interpretation and on the right-hand side
its semantics:
- (¬C)I = ∆I\CI
- (C ⊓D)I = CI ∩DI
- (C ⊔D)I = CI ∪DI
- (∀R.C)I = {x | ∀y.RI(x, y) →CI(y)}
- (∃R.C)I = {x | ∃y.RI(x, y) ∧CI(y)}
Observe that this list is a subset of those listed in Table 3.1, as there are fewer
features in ALC cf. SROIQ.
One also can specify the notion of satisfaction:
- An interpretation I satisﬁes the statement C ⊑D if CI ⊆DI
- An interpretation I satisﬁes the statement C ≡D if CI = DI
- C(a) is satisﬁed by I if aI ∈CI
- R(a, b) is satisﬁed by I if (aI, bI) ∈RI
- An interpretation I = (∆I, ·I) is a model of a knowledge base KB if every
axiom of KB is satisﬁed by I
- A knowledge base KB is said to be satisﬁable if it admits a model

58
Chapter 3. Description Logics
Many DLs have be deﬁned over the past 25 years and their complexity proved.
For instance, one could add Inverses to ALC, giving ALCI, or a Hierarchy of roles,
ALCH, or Qualiﬁed cardinality restrictions; the appendix of the DL Handbook
[BCM+08] has the full list of letters and the features they denote. You also may
like to have a look at the DL Complexity Navigator8. In the next chapter about
OWL 2, we shall introduce a few more expressive languages, whereas ontology-
based data access in Chapter 8 introduces DL-Lite that is less expressive than
ALC.
3.2.2
The DL SROIQ
In this section, we summarise the various features that have been introduced infor-
mally above to provide a comprehensive deﬁnition of DL syntax. Doing so yields
the description logic called SROIQ, which is one of the most expressive DLs com-
monly considered today. It also largely agrees in expressivity with the ontology
language OWL 2 DL, though there are still some diﬀerences as will be discussed in
Chapter 4.
Formally, every DL ontology is based on three ﬁnite sets of signature symbols:
a set NI of individual names, a set NC of concept names and a set NR of role names.
Usually these sets are assumed to be ﬁxed for some application and are therefore
not mentioned explicitly. Now the set of SROIQ role expressions R (over this
signature) is deﬁned by the following grammar:
R ::= U | NR | NR
−
where U is the universal role (Section 3.1.2). Based on this, the set of SROIQ
concept expressions C is deﬁned as:
C ::= NC | (C⊓C) | (C⊔C) | ¬C | ⊤| ⊥| ∃R.C | ∀R.C | ⩾n R.C | ⩽n R.C | ∃R.Self | {NI}
where n is a non-negative integer. As usual, expressions like (C⊓C) represent any
expression of the form (C ⊓D) with C, D ∈C. It is common to omit parentheses if
this cannot lead to confusion with expressions of diﬀerent semantics. For example,
parentheses do not matter for A ⊔B ⊔C whereas the expressions A ⊓B ⊔C and
∃R.A ⊓B are ambiguous.
Using the above sets of individual names, roles and concepts, the axioms of
SROIQ can be deﬁned to be of the following basic forms:
ABox:
C(NI)
R(NI, NI)
NI ≈NI
NI ̸≈NI
TBox:
C ⊑C
C ≡C
RBox:
R ⊑R
R ≡R
R ◦R ⊑R
Disjoint(R, R)
with the intuitive meanings as explained in Section 3.1.1 and 3.1.2.
Roughly speaking, a SROIQ ontology (or knowledge base) is simply a set of
such axioms.
To ensure the existence of reasoning algorithms that are correct
8http://www.cs.man.ac.uk/ezolin/logic/complexity.html

3.2.
Important DLs
59
and terminating, however, additional syntactic restrictions must be imposed on
ontologies. These restrictions refer not to single axioms but to the structure of
the ontology as a whole, hence they are called structural restrictions. The two
such conditions relevant for SROIQ are based on the notions of simplicity and
regularity.
Notably, both are automatically satisﬁed for ontologies that do not
contain complex role inclusion axioms.
A role R in an ontology O is called non-simple if some complex role inclusion
axiom (i.e., one that uses role composition ◦) in O implies instances of R; otherwise
it is called simple. A more precise deﬁnition of the non-simple role expressions of
the ontology O is given by the following rules:
• if O contains an axiom S ◦T ⊑R, then R is non-simple,
• if R is non-simple, then its inverse R−is also non-simple,9
• if R is non-simple and O contains any of the axioms R ⊑S, S ≡R or R ≡S,
then S is also non-simple.
All other roles are called simple.10 Now for a SROIQ ontology it is required that
the following axioms and concepts contain simple roles only:
Restricted axioms:
Disjoint(R, R)
Restricted concept expressions:
∃R.Self
⩾n R.C
⩽n R.C.
The other structural restriction that is relevant for SROIQ is called regularity
and is concerned with RBox axioms only. Roughly speaking, the restriction ensures
that cyclic dependencies between complex role inclusion axioms occur only in a
limited form. For details, please see [HKS06]. For the introductory treatment in
this paper, it suﬃces to note that regularity, just like simplicity, is a property of
the ontology as a whole that cannot be checked for each axiom individually. An
important practical consequence is that the union of two regular ontologies may
no longer be regular. This must be taken into account when merging ontologies in
practice.
The semantics of SROIQ is shown in Tables 3.1 and 3.2.
3.2.3
Important fragments of SROIQ
Many diﬀerent description logics have been introduced in the literature. Typically,
they can be characterised by the types of constructors and axioms that they allow,
which are often a subset of the constructors in SROIQ. For example, the descrip-
tion logic ALC is the fragment of SROIQ that allows no RBox axioms and only ⊓,
⊔, ¬, ∃and ∀as its concept constructors. It is often considered the most basic DL.
The extension of ALC with transitive roles is traditionally denoted by the letter
9If R = S−already is an inverse role, then R−should be read as S. We do not allow expressions
like S−−.
10Whether the universal role U is simple or not is a matter of preference that does not aﬀect
the computational properties of the logic [RKH08b]. However, the universal role in OWL 2 is
considered non-simple.

60
Chapter 3. Description Logics
S. Some other letters used in DL names hint at a particular constructor, such as
inverse roles I, nominals O, qualiﬁed number restrictions Q, and role hierarchies
(role inclusion axioms without composition) H. So, for example, the DL named
ALCHIQ extends ALC with role hierarchies, inverse roles and qualiﬁed number
restrictions. The letter R most commonly refers to the presence of role inclusions,
local reﬂexivity Self , and the universal role U, as well as the additional role char-
acteristics of transitivity, symmetry, asymmetry, role disjointness, reﬂexivity, and
irreﬂexivity. This naming scheme explains the name SROIQ.
In recent years, fragments of DLs have been speciﬁcally developed in order to
obtain favourable computational properties. For this purpose, ALC is already too
large, since it only admits reasoning algorithms that run in worst-case exponential
time. More light-weight DLs can be obtained by further restricting expressivity,
while at the same time a number of additional SROIQ features can be added
without loosing the good computational properties. The three main approaches
for obtaining light-weight DLs are EL, DLP and DL-Lite, which also correspond
to language fragments OWL EL, OWL RL and OWL QL of the Web Ontology
Language.
The EL family of description logics is characterised by allowing unlimited use
of existential quantiﬁers and concept intersection. The original description logic
EL allows only those features and ⊤but no unions, complements or universal
quantiﬁers, and no RBox axioms. Further extensions of this language are known
as EL+ and EL++. The largest such extension allows the constructors ⊓, ⊤, ⊥, ∃,
Self , nominals and the universal role, and it supports all types of axioms other than
role symmetry, asymmetry and irreﬂexivity. Interestingly, all standard reasoning
tasks for this DL can still be solved in worst-case polynomial time. One can even
drop the structural restriction of regularity that is important for SROIQ. EL-
type ontologies have been used to model large but light-weight ontologies that
consist mainly of terminological data, in particular in the life sciences. A number
of reasoners are speciﬁcally optimised for handling EL-type ontologies, the most
recent of which is the ELK reasoner11 for OWL 2 EL.
DLP is short for Description Logic Programs and comprises various DLs that
are syntactically restricted in such a way that axioms could also be read as rules in
ﬁrst-order Horn logic without function symbols. Due to this, DLP-type logics can
be considered as kinds of rule languages (hence the name OWL 2 RL) contained in
DLs. To accomplish this, one has to allow diﬀerent syntactic forms for subconcepts
and superconcepts in concept inclusion axioms. We do not provide the details here.
While DLs in general may require us to consider domain elements that are not
denoted by individual names, for DLP one can always restrict attention to models
in which all domain elements are denoted by individual names. This is why DLP
is often used to augment databases (interpreted as sets of ABox axioms), e.g., in
an implementation of OWL 2 RL in the Oracle 11g database management system.
DL-Lite is a family of DLs that is also used in combination with large data
collections and existing databases, in particular to augment the expressivity of a
query language that retrieves such data. This approach, known as Ontology Based
11http://elk-reasoner.googlecode.com/

3.3.
Reasoning services
61
Data Access, considers ontologies as a language for constructing views or mapping
rules on top of existing data. The core feature of DL-Lite is that data access can
be realised with standard query languages such as SQL that are not aware of the
DL semantics. Ontological information is merely used in a query preprocessing
step. Like DLP, DL-Lite requires diﬀerent syntactic restrictions for subconcepts
and superconcepts. It is the basis for the OWL 2 QL species of OWL ontology
languages, and we will present its DL deﬁnition and its use in Chapter 8.
3.3
Reasoning services
The reasoning services for DLs can be divided into so-called ‘standard’ reasoning
services and ‘non-standard’ reasoning services. The former are more common and
provided by all extant DL reasoners, whereas for the latter, new problems had
been deﬁned that needed speciﬁc algorithms, extensions, and interfaces to the
standard ones. In this section, only the standard ones are considered; an example
example of the latter is deferred to Section 7.5, because the ‘non-standard’ ones are
typically focussed on assisting modellers in the ontology authoring process, rather
than purely deriving knowledge only.
3.3.1
Standard reasoning services
Recalling the four essential components of (automated) reasoning listed in Sec-
tion 2.2.2, the formal language in this case is a DL, and we take a closer look the
choice of class of problems the software has to solve. The standard reasoning ser-
vices are as follows, i.e., generally, all DL-focussed automated reasoners oﬀer these
services.
• Consistency of the knowledge base (KB ⊭⊤⊑⊥)
– Is the KB = (T , A) consistent (non-selfcontradictory), i.e., is there at
least one model for KB, i.e.: “can all concepts and roles be instantiated
without leading to a contradiction?”
• Concept (and role) satisﬁability (KB ⊭C ⊑⊥)
– is there a model of KB in which C (resp. R) has a nonempty exten-
sion, i.e., “can that concept (role) have instances without leading to
contradictions?”
• Concept (and role) subsumption (KB |= C ⊑D)
– i.e., is the extension of C (resp. R) contained in the extension of D
(resp. S) in every model of T (the TBox), i.e., ‘are all instances of C
also instances of D?
• Instance checking (KB |= C(a) or KB |= R(a, b))
– is a (resp. (a, b)) a member of concept C (resp. R) in KB, i.e., is the
fact C(a) (resp. R(a, b)) satisﬁed by every interpretation of KB?

62
Chapter 3. Description Logics
• Instance retrieval ({a | KB |= C(a)})
– ﬁnd all members of C in KB, i.e., compute all individuals a s.t. C(a) is
satisﬁed by every interpretation of KB
You have used the underlying idea of concept subsumption both with EER and
UML class diagrams, but then you did it all manually, like declaring that all cars are
vehicles. Now, instead of you having to model a hierarchy of entity types/classes,
we let the automated reasoner compute it for us thanks to the properties that have
been represented for the DL concepts.
The following two examples illustrate logical implication and concept subsump-
tion.
Example 3.1. Logical implication Consider logical implication—i.e., KB |= φ
if every model of KB is a model of φ—with the following example:
• TBox: ∃teaches.Course ⊑¬Undergrad ⊔Professor
“The objects that teaches a course are not undergrads or professors”
• ABox: teaches(Thembi, cs101), Course(cs101), Undergrad(Thembi)
This is depicted graphically in Figure 3.2. What does it entail, if anything? The
only possibility to keep this logical theory consistent and satisﬁable is to infer that
Thembi is a professor, i.e., KB |= Professor(Thembi), because anything that teaches
a course must be either not an undergrad or a professor. Given that Thembi is an
undergrad, she cannot be not an undergrad, hence, she has to be a professor. ♦
Course
~Undergrad v Professor
teaches
cs101
Course
Undergrad
Thembi
teaches(Thembi, cs101)
Figure 3.2: Top: Depiction of the TBox according to the given axiom; bottom: depic-
tion of the ABox. See Example 3.1 for details.
What will happen if we have the following knowledge base?
• TBox: ∃teaches.Course ⊑Undergrad ⊔Professor
• ABox: teaches(Thembi, cs101), Course(cs101), Undergrad(Thembi)
That is, do we obtain KB |= Professor(Thembi) again? No.
Perhaps the opposite, that KB |= ¬Professor(Thembi)? No. Can you explain why?
Example 3.2. Concept subsumption As an example of concept subsumption,
consider the knowledge base KB that contains the following axioms and is depicted
graphically in Figure 3.3 (for intuitive purpose only):
• HonsStudent ≡Student ⊓∃enrolled.BScHonsDegree
• X ≡Student ⊓∃enrolled.BScHonsDegree ⊓∃hasDuty.TeachingAssistantShip

3.3.
Reasoning services
63
• Y ≡Student ⊓∃enrolled.BScHonsDegree ⊓∃hasDuty.ProgrammingTask
• X(John),
BScHonsDegree(comp4),
TeachingAssistantShip(cs101),
enrolled(John, comp4), hasDuty(John, cs101), BScHonsDegree(maths4).
KB |= X ⊑HonsStudent? That is, is the extension of X contained in the extension
of HonsStudent in every model of KB? Yes. Why? We know that both HonsStudent
and X are subclasses of Student and that both are enrolled in an BScHonsDegree
programme. In addition, every instance of X also has a duty performing a Teachin-
gAssistantShip for an undergrad module, whereas, possibly, not all honours students
work as a teaching assistant. Thus, all X’s are always also an instance of HonsStu-
dent in every possible model of KB, hence KB |= X ⊑HonsStudent. And likewise
for KB |= Y ⊑HonsStudent. This deduction is depicted in green in Figure 3.4.
Let us modify this a bit by adding the following two axioms to KB:
• Z ≡Student ⊓∃enrolled.BScHonsDegree ⊓
∃hasDuty.(ProgrammingTask ⊓TeachingAssistantShip)
• TeachingAssistantShip ⊑¬ProgrammingTask
What happens now? The ﬁrst step is to look at Z: it has the same properties
as HonsStudent, X, and Y, but now we see that each instance of Z has as duty
soothing that is both a ProgrammingTask and TeachingAssistantShip; hence, it must
be a subconcept of both X and Y, because it reﬁnes them both. So far, so good.
The second axiom tells us that the intersection of ProgrammingTask and Teaching-
AssistantShip is empty, or: they are disjoint, or: there is no object that is both a
teaching assistantship and a programming task. But each instance of Z has as duty
to carry out a duty that is both a teaching assistantship and a programming task!
This object cannot exist, hence, there cannot be a model where Z is instantiated,
hence, Z is an unsatisﬁable concept.
♦
HonsStudent
BScHonsDegree
enrolled
X
BScHonsDegree
enrolled
TeachingAssistantShip
hasDuty
Y
BScHonsDegree
enrolled
ProgrammingTask
hasDuty
comp4
maths4
cs101
John
comp4
maths4
comp4
maths4
Figure 3.3: Graphical depiction of an approximation of KB before checking concept
subsumption (≡not shown, nor are the subsumptions to Student, so as to avoid too
much clutter).
3.3.2
Techniques: a tableau for ALC
The description of the deductions illustrated in the previous paragraph is an infor-
mal, high-level way of describing what the automated reasoner does when comput-
ing the concept hierarchy and checking for satisﬁability. Clearly, such an informal
way will not work as an algorithm to be implemented in a computer. There are

64
Chapter 3. Description Logics
HonsStudent
BScHonsDegree
enrolled
X
BScHonsDegree
enrolled
TeachingAssistantShip
hasDuty
Y
BScHonsDegree
enrolled
ProgrammingTask
hasDuty
comp4
maths4
cs101
John
John
maths4
comp4
maths4
comp4
Figure 3.4: Graphical depiction of K after checking concept subsumption; content in
green is deduced.
several proof techniques both in theory and in practice to realise the reasoning
service. The most widely used technique at the time of writing (within the scope
of DLs and the Semantic Web) is tableau reasoning, and is quite alike what we have
seen with tableau with full FOL. In short, it:
1. Unfold the TBox
2. Convert the result into negation normal form (NNF)
3. Apply the tableau rules to generate more ABoxes
4. Stop when none of the rules are applicable
Then:
• T ⊢C ⊑D if all ABoxes contain clashes
• T ⊬C ⊑D if some ABox does not contain a clash
First, recall that one enters the tableau in Negation Normal Form (NNF), i.e., “¬”
only in front of concepts. For DLs and C and D are concepts, R a role, we use
equivalences to obtain NNF, just like with FOL:
- ¬¬C gives C
- ¬(C ⊓D) gives ¬C ⊔¬D
- ¬(C ⊔D) gives ¬C ⊓¬D
- ¬(∀R.C) gives ∃R.¬C
- ¬(∃R.C) gives ∀R.¬C
Second, there are the tableau rules. If there are more features, there will be more
rules. These are the ones for ALC:
⊓-rule: If (C1 ⊓C2)(a) ∈S but S does not contain both C1(a) and C2(a), then
S = S ∪{C1(a), C2(a)}
⊔-rule: If (C1 ⊔C2)(a) ∈S but S contains neither C1(a) nor C2(a), then
S = S ∪{C1(a)}
S = S ∪{C2(a)}
∀-rule: If (∀R.C)(a) ∈S and S contains R(a, b) but not C(b), then
S = S ∪{C(b)}
∃-rule: If (∃R.C)(a) ∈S and there is no b such that C(b) and R(a, b), then
S = S ∪{C(b), R(a, b)}
With these ingredients, it is possible to construct a tableau to prove that the
aforementioned deductions hold. There will be an exercise about it, and we will
see more aspects of automated reasoning in the lectures and exercises about OWL.

3.4.
Exercises
65
3.4
Exercises
Review question 3.1. How are DLs typically diﬀerent from full FOL?
Review question 3.2. What are the components of a DL knowledge base?
Review question 3.3. What are (in the context of DLs) the concept and role
constructors? You may list them for either ALC or SROIQ.
Review question 3.4. What distinguishes one DL from another? That is, e.g.,
ALC is diﬀerent from SROIQ and from EL; what is the commonality of those
diﬀerences?
Review question 3.5. Explain in your own words what the following ALC rea-
soning tasks involve and why they are important for reasoning with ontologies:
a. Instance checking.
b. Subsumption checking.
c. Checking for concept satisﬁability.
Exercise 3.1. Consider again the natural language sentences from Exercise 2.6.
Formalise them into a suitable DL, where possible. *
Exercise 3.2. Consider the following TBox T :
V egan ≡Person ⊓∀eats.Plant
V egetarian ≡Person ⊓∀eats.(Plant ⊔Dairy)
We want to know if T ⊢V egan ⊑V egetarian.
This we convert to a constraint system S = {(V egan ⊓¬V egetarian)(a)},
which is unfolded (here: complex concepts on the left-hand side are replaced with
their properties declared on the right-hand side) into:
S = {Person ⊓∀eats.Plant ⊓¬(Person ⊓∀eats.(Plant ⊔Dairy))(a)}
(3.24)
Tasks:
a. Rewrite Eq. 3.24 into negation normal form
b. Enter the tableau by applying the rules until either you ﬁnd a completion or
only clashes.
c. T ⊢V egan ⊑V egetarian? *
Exercise 3.3. In anticipation of using ontologies in computing and information
systems, download and install Prot´eg´e 5.x, download the jar ﬁle of the “DL axiom
renderer” from the course book’s website, and place it in the ‘plugins’ folder. Open
Prot´eg´e and add it (click Window - Views - Class views - select DL axiom renderer
- place cursor in desired position and click). Load the AWO, click on Lion, and
inspect the DL axioms. It should look like the screenshot of Figure 3.5. Explore
other classes similarly.

66
Chapter 3. Description Logics
Figure 3.5: Giraﬀe’s axioms rendered in DL notation.
3.5
Literature and reference material
This material is listed mainly for the curious who would like to delve deeper into
Description Logics. At the time of writing, a DL textbook is in the making.
1. Ulrike Sattler. Reasoning in description logics: Basics, extensions, and rel-
atives. In G. Antoniou et al., editors, Reasoning Web 2007, volume 4636 of
LNCS, page 154-182. Springer, 2007. OR Anni-Yasmin Turhan. Reasoning
and explanation in EL and in expressive Description Logics. In U. Assmann,
A. Bartho, and C. Wende, editors, Reasoning Web 2010, volume 6325 of
LNCS, pages 1-27. Springer, 2010.
2. F. Baader, D. Calvanese, D. L. McGuinness, D. Nardi, and P. F. Patel-
Schneider (Eds). The Description Logics Handbook. Cambridge University
Press, 2008. Chapters 1 and 2 (and the rest if you can’t get enough of it).

CHAPTER 4
The Web Ontology Language OWL 2
In the previous two chapters we have seen ﬁrst FOL and then a version of it that was
slightly changed with respect to notation and number of features in the language
(easier, and less, respectively), being the DL family of languages. They haven’t
gotten us anywhere close to implementations, however. This is set to change in
this chapter, where we will look at ‘implementation versions’ of DLs that have
rich tooling support. We will take a look at the computational use of DLs with
a so-called serialization to obtain computer-processable versions of an ontology
and automated reasoning over it. The language that we will use to serialise the
ontology is the most widely used ontology language for computational purposes,
being the Web Ontology Language OWL. OWL was standardised ﬁrst in 2004 and
a newer version was standardised in 2009, which has fuelled tool development and
deployment of ontologies in ontology-driven information systems. OWL looks like
yet another a language and notation to learn, but the ones that we will consider (the
DL-based ones) have the same underlying principles. It does have a few engineering
extras, which also has as consequence that there are several ways to serialise the
ontology so as to cater for software developers’ preferences. Thus, theoretically,
there is not really anything substantially new in this chapter, but there will be
many more options and exercises to practically engage with the ontology languages,
automated reasoning, and toy ontologies to play with on the computer. Depending
on your interests, things start to get ‘messy’ (for a theoretician) or ﬁnally concrete
(for an engineer).
OWL actually constitutes a family of languages consisting of OWL “species”,
and we will focus on those species that are based on DLs, which are all fragments of
the most expressive one, OWL 2 DL. To understand how these species came about
in the way they are, with these features and not others, we will touch upon OWL
in Section 4.1 ﬁrst, which will shed light on questions such as: What goes into
standardising a language? Why precisely these ones came out of the eﬀorts, and
in this way? Why should an ontology engineer even consider the previous version
and not only the latest?
67

68
Chapter 4. The Web Ontology Language OWL 2
Afterward, an overview of the DL-based OWL 2 languages is provided in Sec-
tion 4.2. Yes, plural; that’s not a typo. As we shall see, there are good reasons for
it both from a computational viewpoint for scalable implementations and to please
the user-base. The computational aspect is summarised in Section 4.2.4. If you
have completed a course on theory of computation, this will be easy to follow. If
not, you would want to consult Appendix D, which provides an explanation why
one cannot have it all, i.e., both a gazillion of language features and good perfor-
mance of an ontology-driven information system. Experience has seen that that
sort of trade-oﬀcan annoy a domain expert become disappointed with ontologies;
Section 4.2.4 (and the background in Appendix D) will help you explain to domain
experts it’s neither your fault nor the ontology’s fault. Finally, OWL does not exist
in isolation—if it were, then there would be no tools that can use OWL ontologies
in information systems. Section 4.3 therefore sets it in context of the Semantic
Web—heralded as a ‘next generation’ World Wide Web—and shows that, if one
really wants the extra expressiveness, it can ﬁt in another logic framework and
system even up to second order logic with the Distributed Ontology, Model, and
Speciﬁcation Language (DOL) and its software infrastructure.
4.1
Standardising an ontology language
This section and the next one are intentionally kept short, as listing language
features isn’t the most interesting of content, those lists exist also online in the
standard1, and are not meant to be memorised but to be consulted as the need
arises. This section and the next one, instead, focus on the gist of it and provide
some contextual information.
4.1.1
Historical notes
Before OWL, there were a plethora of ontology languages, such as the obo format
(directed acyclic graphs) initiated by the Gene Ontology Consortium, KL-ONE,
and F-logic (frames, and older versions of the Prot´eg´e ODE). Unsurprisingly, this
caused ontology interoperation problems even at the syntactic level and hampered
development and use of ontology tools, hence, its uptake. To solve those issues,
researchers set out to standardise a logic language. At the time that people sat at
the standardisation table (ﬁrst around 2001) of the World Wide Web Consortium
(W3C), there were several logics that had a considerable inﬂuence on the ﬁnal
product, most notably the SHOE, DAML-ONT, OIL, and DAML+OIL languages,
and, more generally, the fruits of 20 years of research on languages and prototyping
of automated reasoners by the DL community.
Following good engineering practices, a document of requirements and objec-
tives was devised to specify what such an ontology language for the Semantic
Web should meet. It is useful to list them here, so that you can decide yourself
how well OWL meets them, as well as any contender languages, or if you would
1start with a general non-technical overview in the OWL primer http://www.w3.org/TR/
owl2-primer/.

4.1.
Standardising an ontology language
69
want to design one of your own. It speciﬁed the following design goals: Shareable;
Change over time; Interoperability; Inconsistency detection; Balancing expressivity
and complexity; Ease of use; Compatible with existing standards; and Internation-
alisation.
There were also requirements on the features of the language.
They
were: Ontologies are objects on the Web; they have their own meta-data, version-
ing, etc.; Ontologies are extendable; They contain classes, properties, data-types,
range/domain, individuals; they must be able to express equality for classes and
for individuals; Classes as instances; Cardinality constraints; and XML syntax2.
First, as standardisation is typically consensus-based, there was, on the one
hand, a language called OWL full with RDF-based semantics (graphs) and two
DL-based species, being OWL lite and OWL DL. But what makes OWL a Semantic
Web language compared to the regular DL languages that were introduced in the
previous chapter? This is the second step. There are the following main diﬀerences:
• OWL uses URI references as names; e.g., http://www.mysite.co.za/Uni-
Onto.owl#Student is the URI of the class Student in the ontology with ﬁle
name UniOnto.owl ﬁle that is online at http://www.mysite.co.za;
• It gathers information into ontologies stored as documents written in RD-
F/XML including things like owl:imports to import one ontology into an-
other; e.g. another ontology, HigherEd.owl can import the UniOnto.owl so
that the class http://www.mysite.co.za/UniOnto.owl#Student becomes,
in a way, part of HigherEd.owl (though it still exists independently as well);
• It adds RDF data types and XML schema data types for the ranges of data
properties (attributes), so one can use, e.g., string and integer in a similar
way as you are familiar with in UML class diagrams and databases.
• A diﬀerent terminology: a DL concept is now called a class, a DL role is
called an object property, and data property is added for attributes (i.e., the
relation that relates a class to a data type).
With this information, we can start going from a paper-based representation
to its computational version. The ﬁrst steps are illustrated in the next example,
which will be extended in later chapters.
Example 4.1. The African Wildlife Ontology (AWO) is a basic tutorial ontology
based on the examples in the “A Semantic Web Primer” book [AvH03].
The
ﬁst step is to represent that in OWL, using your favourite ontology development
environment (ODE). An OWL version of it, AfricanWildlifeOntology0.owl, has
10 classes and 3 object properties concerning animals such as Lion, Giraﬀe, Plant,
and object properties eats and is-part-of, and has annotations that give an idea of
what should be modelled (else: see 4.3.1 pages 119-133 in [AvH03]). Upon running
the reasoner, it will classify, among others, that Carnivore is a subclass of Animal
(i.e, AWO |= Carnivore ⊑Animal).
2The “making of an ontology language” article [HPSvH03] gives a longer general historical
view and it also summarises OWL with its three species (OWL lite, OWL-DL, and OWL full).
The details of the standard are freely available at http://www.w3.org/TR/owl-ref/.

70
Chapter 4. The Web Ontology Language OWL 2
This is not really exciting, and the tutorial ontology is not of a particularly
good quality. First, we extend it by having loaded the ontology and adding knowl-
edge to it: among others, proper parthood, a few more plant parts and animals,
such as Impala, Warthog, and RockDassie.
This version of the AWO is named
AfricanWildlifeOntology1.owl. With this additional knowledge, warthogs are
classiﬁed as omnivores, lions as carnivores, giraﬀes as herbivores, and so on.
Another aspect is purely engineering practice: if the intention is to put the
ontology online, it should be named properly, i.e., the URI has to be set so that its
contents can be identiﬁed appropriately on the Semantic Web; that is, do not simply
use the default URI generated by the tool (e.g., http://www.semanticweb.org/
ontologies/2018/0/Ontology1357204526617.owl), but specify an appropriate
one where the ontology will be published, like http://www.meteck.org/files/
ontologies/myAWO.owl.
♦
4.1.2
The OWL 1 family of languages
Purely for legacy purposes, I include here the ﬁrst three ‘species’ of OWL (version
1). This because 1) there are still plenty of ontologies around that are represented
in either of these languages and you may have to deal with them, and 2) OWL
DL seems to be a fairly good ‘sweet spot’ in the expressivity/complexity trade-oﬀ,
meaning: still being able to represent the domain fairly well, or at least suﬃciently
to the extent that the increased expressiveness of OWL 2 DL doesn’t weigh up
against the increased slowness of completing the reasoning tasks. The key points
to remember are:
• OWL Lite and OWL DL are DL-based, with model-theoretic semantics.
• OWL DL is more expressive than OWL Lite, but one does not gain a lot
computationally, so OWL Lite was hardly used explicitly.
• OWL full never really took oﬀeither.
• There are multiple syntaxes for the serialisation: functional-style syntax (op-
tional) and RDF/XML (required for tool interoperability).
OWL Lite has a classiﬁcation hierarchy and (relative to OWL DL) simple con-
straints. While OWL Lite has strong syntactic restrictions, it has only limited
semantics restrictions compared to OWL DL3. OWL Lite corresponds to the DL
SHIF(D). Putting the DL symbols to the names of the features, we have:
- Named classes (A)
- Named properties (P)
3More speciﬁcally regarding the latter, if you really want to know: negation can be encoded
using disjointness and with negation and conjunction, you can encode disjunction.
Take, for
instance:
Class(C complete unionOf(B C))
This is equivalent to
DisjointClasses(notB B)
DisjointClasses(notC C)
Class(notBandnotC complete notB notC)
DisjointClasses(notBandnotC BorC)
Class(C complete notBandnotC)

4.2.
OWL 2
71
- Individuals (C(o))
- Property values (P(o, a))
- Intersection (C ⊓D)
- Union (C ⊔D)
- Negation (¬C)
- Existential value restrictions (∃P.C)
- Universal value restrictions (∀P.C)
- Unqualiﬁed (0/1) number restrictions (≥nP, ≤nP, = nP), 0 ≤n ≤1
OWL DL had, at the time, ‘maximal’ expressiveness while maintaining tractabil-
ity, and has, as the name suggestion, an underlying DL. It has all the features of
OWL-lite, and, in addition: Negation, Disjunction, (unqualiﬁed) Full cardinality,
Enumerated classes, and hasValue. OWL DL corresponds to the DL SHOIN(D).
It has the following features:
- All OWL Lite features
- Arbitrary number restrictions (≥nP, ≤nP, = nP), with 0 ≤n
- Property value (∃P.{o})
- Enumeration ({o1, ..., on})
OWL Full, has a very high expressiveness (losing tractability) and all syntactic
freedom of RDF (self-modifying). OWL full has meta-classes and one can modify
the language. Note that OWL Full is not a Description Logic.
As mentioned earlier, OWL and DLs are tightly related, in particular OWL Lite
and OWL DL. They have, just like their base DLs, a model theoretic semantics.
Table 4.1 shows a few examples of some OWL syntax and its DL counterpart
notation. There is also the not-for-human-consumption RDF/XML serialisation.
Table 4.1:
Some examples of OWL’s construct, the same in DL notation, and an
example.
OWL Construct
DL notation
Example
intersectionOf
C1 ⊓... ⊓Cn
Human ⊓Male
unionOf
C1 ⊔... ⊔Cn
Doctor ⊔Lawyer
complementOf
¬C
¬Male
oneOf
{o1, ..., on}
{giselle, juan}
allValuesFrom
∀P.C
∀hasChild.Doctor
someValuesFrom
∃P.C
∃hasChild.Lawyer
value
∃P.{o}
∃citizenOf.{RSA}
minCardinality
≥nP
≥2 hasChild
maxCardinality
≤nP
≤6 enrolledIn
4.2
OWL 2
Over the past 16 years, OWL has been used across subject domains, but in the
early years mostly by the health care and life sciences disciplines. Experimentation
with the standard revealed expected as well as unexpected shortcomings in addition

72
Chapter 4. The Web Ontology Language OWL 2
Table 4.2: Some examples of OWL’s axioms, the same in DL notation, and an example.
OWL Axiom
DL
Example
SubClassOf
C1 ⊑C2
Human ⊑Animal ⊓Biped
EquivalentClasses
C1 ≡... ≡Cn
Man ≡Human ⊓Male
SubPropertyOf
P1 ⊑P2
hasDaughter ⊑hasChild
EquivalentProperties
P1 ≡... ≡Pn
cost ≡price
SameIndividual
o1 = ... = on
President Zuma = J Zuma
DisjointClasses
Ci ⊑¬Cj
Male ⊑¬Female
DifferentIndividuals
oi ̸= oj
Thabo ̸= Andile
inverseOf
P1 ≡P −
2
hasChild ≡hasParent−
transitiveProperty
P + ⊑P
ancestor+ ⊑ancestor,
denoted also as Trans(ancestor)
symmetricProperty
P ≡P −
Sym(connectedTo)
functionalProperty
⊤⊑≤1P
⊤⊑≤1hasPresident
inverseFunctionalProperty
⊤⊑≤1P −
⊤⊑≤1hasIDNo−
to the ideas mentioned in the “Future extensions” section of [HPSvH03], so that
a successor to OWL was deemed to be of value. Work towards a standardisation
of an OWL 2 took shape after the OWL Experiences and Directions workshop in
2007 and a ﬁnal draft was ready by late 2008. On October 27 2009 it became the
oﬃcial OWL 2 W3C recommendation4. What does OWL 2 consist of—new and
improved!—and what does it ﬁx with respect to the OWL standard of 2004? Let’s
consider the answers to these questions in the remainder of this section.
Limitations of OWL—as experienced by the practitioners
OWL 2 aims to address the issues described in section 2 of [CGHM+08] to a greater
or lesser extent, which is neither a superset nor subset of [HPSvH03]’s ideas for
possible extensions. For instance, an OWL 1 possible future feature was catering
for the Unique Name Assumption, but that did not make it into OWL 2, despite
that it has quite an eﬀect on the complexity of a language [ACKZ09]. We brieﬂy
summarise the interesting issues; refer to [CGHM+08] for details.
Expressivity limitations. First, it is not possible to express qualiﬁed cardinality
restrictions in OWL. For instance, one can state Bicycle ⊑≥2 hasComponent.⊤
or Bicycle ⊑∃hasComponent.Wheel, but not Bicycle ⊑≥2 hasComponent.Wheel.
This was deemed an important shortcoming in OWL DL by its modellers. Sec-
ond, some relational properties were perceived to be missing, notably reﬂexivity
and irreﬂexivity, so one could not represent the class Narcissist (someone who loves
him/herself), and not state that proper parthood is irreﬂexive, yet an irreﬂexive
partOf relation is important in medicine and biology. Third, there were also limi-
tations on data types; e.g., one cannot express restrictions to a subset of datatype
values (ranges) and relationships between values of data properties on one object.
Last, there were also some ‘housekeeping’ features missing, such as annotations,
4http://www.w3.org/TR/2009/REC-owl2-overview-20091027/

4.2.
OWL 2
73
imports, versioning, and species validation (see p315 of [CGHM+08] for details).
Syntax problems. OWL has both a frame-based legacy (Abstract syntax) and
axioms (DL), which was deemed too confusing. For instance, take the following
axiom:
Class(A partial restriction(hasB someValuesFrom(C))
What type of ontology elements do we have? Is hasB is data property and C a
datatype, or is hasB an object property and C a class?
OWL-DL has a strict
separation of the vocabulary, but the speciﬁcation does not precisely specify how
to enforce this separation at the syntactic level. In addition, RDF’s triple notation
is diﬃcult to read and process.
Problems with the semantics. We shall not cover this issue. (For the curious:
this has to do with RDF’s blank nodes, but unnamed individuals not directly
available in SHOIN(D), and frames and axioms).
Overview of OWL 2
Complex systems have a tendency to become more, rather than less, complex, and
so it is for OWL 2, mainly regarding new features and more new languages. First,
have a look at the ‘orchestration’ of the various aspects of OWL 2 in Figure 4.1.
The top section indicates several syntaxes that can be used to serialize the ontology,
where RDF/XML is required and the other four are optional. There are mappings
between an OWL ontology and RDF graph in the middle, and the lower half depicts
that there is both a direct semantics, for OWL 2 DL-based species, and an RDF-
based one, for OWL 2 full. Note that while that “mapping” between “ontology
structure” and “RDF Graph” and that “correspondence theorem” between “Direct
semantics” and “RDF-based semantics” exist, this does not mean they’re all the
same thing.
The DL-based OWL 2 species have a mapping into RDF for the
serialisation, but they do not have an RDF-based semantics.
Second, the OWL 2 DL species is based on the DL SROIQ(D) [HKS06]. It is
more expressive than the underlying DL of OWL DL (SHOIN(D)) and therewith
meeting some of the modellers’ requests, such as more properties of properties and
qualiﬁed number restrictions (see below). There is cleaner support for annotations,
debatable (from an ontological perspective, that is) punning for metamodelling, and
a ‘key’ that is not a key in the common sense of keys in conceptual data models
and databases. Also, it irons out some diﬃculties that tool implementers had with
the syntaxes of OWL and makes importing ontologies more transparent.
Third, there are three OWL 2 proﬁles, which are sub-languages of (syntactic
restrictions on) OWL 2 DL so as to cater for diﬀerent purposes of ontology usage
in applications. At the time of standardisation, they already enjoyed a consider-
able user base. This choice has its consequences that very well can, but may not
necessarily, turn out to be a positive one in praxis; this will be explored further in
the block on ontology engineering. The three proﬁles are:
• OWL 2 EL, which is based on the EL++ language [BBL05], intended for
use with large relatively simple type-level ontologies;
• OWL 2 QL, which is based on the DL-LiteR language [CGL+07], intended

74
Chapter 4. The Web Ontology Language OWL 2
Figure
4.1:
Orchestration
of
syntax
and
semantics
of
OWL
2.
(Source:
http://www.w3.org/TR/2009/REC-owl2-overview-20091027/).
for handling and querying large amounts of instances through the ontology;
• OWL 2 RL, which is inspired by Description Logic Programs and pD, in-
tended for ontologies with rules and data in the form of RDF triples.
Like with OWL 2 DL, each of these languages has automated reasoners tailored
to the language so as to achieve the best performance for the application scenario.
Indirectly, the notion of the proﬁles and automated reasoners says you cannot have
both many modelling features together in one language and expect to have good
performance with the ontology and ontology-driven information system. Such is life
with the limitations of computers, but one can achieve quite impressive results with
the languages and its tools that are practically not really doable with paper-based
manual eﬀorts.
4.2.1
New OWL 2 features
OWL 2 DL is based on SROIQ(D) [HKS06], which we have seen in Chapter 3,
which is 2NExpTime-complete in taxonomic complexity [Kaz08]; hence, it is a
more expressive language than OWL-DL (SHOIN, which is NExpTime-complete
[Tob01]). Compared to OWL DL, it has fancier metamodelling and annotations,
improved ontology publishing, imports and versioning control. In addition to all
the OWL-DL features (recall Section 4.1.2), one can use the following ones in OWL
2 DL as well, which are illustrated afterwards:

4.2.
OWL 2
75
• Qualiﬁed cardinality restrictions, ≥nR.C and ≤nR.C (the Q in SROIQ):
– (≥nR.C)I = {x | ♯{y | (x, y) ∈RI ∩y ∈CI} ≥n}
In OWL 2: ObjectMinCardinality(n OPE CE); an example in DL no-
tation: ≥3 hasPart.Door
– (≤nR.C)I = {x | ♯{y | (x, y) ∈RI ∩y ∈CI} ≤n}
In OWL 2: ObjectMaxCardinality(n OPE CE), an example in DL no-
tation: ≤2 enrolledIn.UGDegree
– The diﬀerence between the unqualiﬁed cardinality constraint (the N
in OWL DL’s SHOIN) and qualiﬁed cardinality constraint (the Q in
OWL 2 DL’s SROIQ) is the diﬀerence between aforementioned Bicy-
cle ⊑≥2 hasComponent.⊤and Bicycle ⊑≥2 hasComponent.Wheel,
respectively.
• Properties of roles (the R in SROIQ):
– Reﬂexive (globally): Ref(R), with semantics:
∀x : x ∈∆I implies (x, x) ∈(R)I
Example: the connection relation (everything is connected to itself).
– Reﬂexive (locally): ∃R.Self, with semantics:
{x | (x, x) ∈R}
In OWL 2: ObjectHasSelf (OPE); e.g., ∃knows.Self to state you know
yourself.
– Irreﬂexive: Irr(R), with semantics:
∀x : x ∈∆I implies (x, x) /∈(R)I
For instance, proper parthood is irreﬂexive: something cannot be proper
part of itself.
– Asymmetric: Asym(R), with semantics:
∀x, y : (x, y) ∈(R)I implies (y, x) /∈(R)I
For instance, Asym(parentOf): if John is the parent of Divesh, then
Divesh cannot be the parent of John.
• Limited role chaining (also covered with the R in SROIQ): e.g., R ◦S ⊑
R, with semantics: ∀y1, . . . , y4 : (y1, y2) ∈(R)I and (y3, y4) ∈(S)I imply
(y1, y4) ∈(R)I, and regularity restriction (strict linear order (“<”) on the
properties). For instance: childOf ◦childOf ⊑grandchildOf so that one can
deduce that the child of a child is that person’s grandchild, and the uncle
example in Chapter 3.
The tricky part especially in practical ontology development is that some ob-
ject property features and axioms work only on simple object properties, ‘simple’
meaning that it has no direct or indirect subproperties that are either transitive
or are deﬁned by means of property chains; see section 11.1 of the OWL Struc-
tural Speciﬁcation and Functional-Style Syntax for the exact speciﬁcation of this
limitation. Practically, this means that the following features can be used only
on simple object properties:
ObjectMinCardinality, ObjectMaxCardinality,
ObjectExactCardinality,
ObjectHasSelf,
FunctionalObjectProperty,
InverseFunctionalObjectProperty,
IrreflexiveObjectProperty,

76
Chapter 4. The Web Ontology Language OWL 2
AsymmetricObjectProperty, and DisjointObjectProperties. Two examples of
what this concretely means when you’re trying to develop an ontology are illus-
trated next.
Example 4.2. In the ﬁrst example, the ontologist has to choose between transi-
tivity or qualiﬁed number restrictions, but cannot have both. This gives a modeller
options within OWL 2 DL, using cake with its ingredients as example:
1) Cake has ingredient any number of Edible substances and one would be able to
infer that if Milk is an ingredient of Butter that is an ingredient of Cake, then
Milk is an ingredient of Cake; i.e., hasIngredient and its inverse, ingredientOf is
transitive; or
2) a (standard) Cake has ingredients at least four Edible substances, but it can-
not be inferred that the Milk is an ingredient of the Cake; i.e., hasIngredient
participates in a qualiﬁed number restriction.
Another modelling trade-oﬀis the following one. Alike the uncle example in Chap-
ter 3, one can specify a role chain for aunts, e.g.: hasMother ◦hasSister ⊑hasAunt.
It certainly holds that hasMother is asymmetric (your mother cannot be your child),
i.e., Asym(hasMother). Each axiom can be represented in an OWL 2 DL ontology,
yet, one cannot assert both the property chain and antisymmetry in the same OWL
2 DL ontology. ♦
The ontology development environment probably will warn you about such
syntax violations, and will prevent you from running the reasoner. It may say
something cryptic like “internal reasoner error” and the log ﬁle will have an entry
returned by the OWL API with the oﬀending axioms, along the line of:
An error occurred during reasoning:
Non-simple property
’<ex#hasIngredient>’ or its inverse appears in the cardinality
restriction ’ObjectMaxCardinality(4 <ex#hasIngredient>
<ex#EdibleSubstance>)’.
where the ex is the ontology’s URI. If it happens, you will have to decide which
of the two axioms is the more important one to keep, or choose another, more
expressive, logic beyond OWL 2 DL. One ‘way out’ to this problem will pass the
revue in Section 4.3.2.
4.2.2
OWL 2 Proﬁles
The main rationale for the proﬁles are computational complexity considerations
and robustness of implementations with respect to scalable applications. Their fea-
tures are summarised here. Note that you are not expected to learn the following
lists of features by heart (it can be used as a quick ‘cheat sheet’), but you do need
to know, at least, their intended purpose. The more you practice developing on-
tologies, the easier it becomes to remember them. To assist with grasping language
features actually used in a particular ontology—be it the three OWL 1 species or
the ﬁve OWL 2 species—the OWL Classiﬁer can be used. It lists in which OWL
species one’s OWL ontology is and why it violates the other species (i.e., creates

4.2.
OWL 2
77
a justiﬁcation of the reported expressivity)5, and therewith provides features that
are not available in currently popular ontology editors such as Prot´eg´e.
OWL 2 EL
OWL 2 EL is intended for large ‘simple’ ontologies and focuses on type-level knowl-
edge (TBox). It has a better computational behaviour than OWL 2 DL . It is based
on the DL language EL++ (PTime complete), and it is used for the large medi-
cal terminology SNOMED CT [SNO12], among others. The listing of OWL 2 EL
features is included in Appendix C.
OWL 2 QL
OWL 2 QL aims at scenarios for query answering over a large amount of data
with the same kind of performance as relational databases (Ontology-Based Data
Access; see Chapter 8). Its expressive features cover several used features of UML
Class diagrams and ER models. It is based on DL-LiteR (though more is possible
with the Unique Name Assumption and in some implementations).
The supported axioms in OWL 2 QL take into account what one can use on the
left-hand side of the inclusion operator (⊑, SubClassOf) and what can be asserted
on the right-hand side, which turns it into a fairly long list due to the intricate
exclusions. The listing of OWL 2 QL features is included in Appendix C.
OWL 2 RL
OWL 2 RL’s development was motivated by what fraction of OWL 2 DL can be
expressed by rules (with equality) and scalable reasoning in the context of RDF(S)
application. It uses rule-based technologies (forward chaining rule system, over
instances) and is inspired by Description Logic Programs and pD*. Reasoning in
PTime.
The list of features supported in OWL 2 RL is easily speciﬁed:
– More restrictions on class expressions (see table 2 of [MGH+09]; e.g., no
SomeValuesFrom on the right-hand side of a subclass axiom)
– All axioms in OWL 2 RL are constrained in a way that is compliant with the
restrictions in Table 2.
– Thus, OWL 2 RL supports all axioms of OWL 2 apart from disjoint unions
of classes and reﬂexive object property axioms.
A quick one-liner of the diﬀerence with OWL 2 DL is: No ∀and ¬ on the left-hand
side, and ∃and ⊔on right-hand side of ⊑.
4.2.3
OWL 2 syntaxes
There are more syntaxes for OWL 2 than for OWL, as we have seen in Figure 4.1.
Consider the DL axiom
5Source
code
and
tool:
https://github.com/muhummadPatel/OWL_Classifier;
for
a
brief
explanation
of
its
use,
see
https://keet.wordpress.com/2016/06/19/
an-exhaustive-owl-species-classifier/.

78
Chapter 4. The Web Ontology Language OWL 2
FirstYearCourse ⊑∀isTaughtBy.Professor
Rendering this in RDF/XML yields:
<!−−http://www.semanticweb.org/ontologies/2017/6/exOKB17.owl#FirstYearCourse −−>
<owl:Class rdf:about=”&exOKB17;FirstYearCourse”>
<rdfs:subClassOf rdf:resource=”&owl;Thing”/>
<rdfs:subClassOf>
<owl:Restriction>
<owl:onProperty rdf:resource=”&exOKB17;isTaughtBy”/>
<owl:allValuesFrom rdf:resource=”&exOKB17;Professor”/>
</owl:Restriction>
</rdfs:subClassOf>
</owl:Class>
This RDF/XML fragment tells us that the ontology is called exOKB17 (abbre-
viated name for the full URI), FirstYearCourse is a subClassOf the root-class
Thing, and a subclass of the restriction on FirstYearCourse, being that the re-
striction is owl:onProperty object property isTaughtBy and the ‘ﬁller’, i.e., to
which the restriction applies, is allValuesFrom (i.e., ∀) Professor.
In OWL/XML (also not intended for human consumption), we have the same
as follows:
<SubClassOf>
<Class IRI=”#FirstYearCourse”/>
<Class abbreviatedIRI=”owl:Thing”/>
</SubClassOf>
<SubClassOf>
<Class IRI=”#FirstYearCourse”/>
<ObjectAllValuesFrom>
<ObjectProperty IRI=”#isTaughtBy”/>
<Class IRI=”#Professor”/>
</ObjectAllValuesFrom>
</SubClassOf>
The functional syntax equivalent is as follows:
Declaration(Class(:FirstYearCourse))
SubClassOf(:FirstYearCourse owl:Thing)
SubClassOf(:FirstYearCourse ObjectAllValuesFrom(:isTaughtBy :Professor))
The Manchester syntax rendering is intended exceedingly for human reading, for
non-logicians, and for ease of communication in, say, emails that do not render
mathematical symbols well. On the one hand, there is a Prot´eg´e-generated Manch-
ester syntax rendering:
Class: <http://www.semanticweb.org/ontologies/2017/6/exOKB17.owl#FirstYearCourse>
SubClassOf:
owl:Thing,
<http://www.semanticweb.org/ontologies/2017/6/exOKB17.owl#isTaughtBy> only
<http://www.semanticweb.org/ontologies/2017/6/exOKB17.owl#Professor>
But this usually gets abbreviated as follows:
Class: FirstYearCourse
SubClassOf:

4.2.
OWL 2
79
owl:Thing,
isTaughtBy only Professor
or, even shorter:
FirstYearCourse SubClassOf isTaughtBy only Professor
There are several really non-standard representations of OWL ontologies for various
reasons, such as interface design and making it easier for non-logicians to contribute
to ontology development. For instance, in pseudo-natural language (which is a topic
of Chapter 9), and graphical renderings, like with Ontograf and depicted in Fig-
ure 4.2, where the axiom shows when hovering over the coloured line representing
the object property. These informal variants are all ‘syntactic sugar’ renderings of
the ontology.
Figure 4.2: Screenshot of the FirstYearCourse ⊑∀isTaughtBy.Professor in the Ontograf
plugin for the Prot´eg´e ontology development environment; the axiom appears when hov-
ering over the coloured dashed line representing the object property.
4.2.4
Complexity considerations for OWL
We have seen diﬀerent ‘species’ of OWL, which have more or less language features,
and that this was motivated principally by scalability issues of the very expressive
languages. Diﬀerent languages/problems have diﬀerent complexity (NP-complete,
PSPACE, EXPTIME etc.). Appendix D contains a very brief recap on computa-
tional complexity, whereas here we jump straight to the speciﬁcs for OWL.
In this setting of ontologies, we are interested in the following reasoning prob-
lems: ontology consistency, class expression satisﬁability, class expression sub-
sumption, instance checking, and (Boolean) conjunctive query answering (recall
Section 3.3). When evaluating complexity, the following parameters are considered
(copied from section 5 of the OWL 2 Proﬁles standard [MGH+09]):
• Data Complexity: the complexity measured with respect to the total size
of the assertions in the ontology.
• Taxonomic Complexity: the complexity measured with respect to the
total size of the axioms in the ontology.
• Query Complexity: the complexity measured with respect to the total size
of the query.
• Combined Complexity: the complexity measured with respect to both the
size of the axioms, the size of the assertions, and, in the case of conjunctive
query answering, the size of the query as well.

80
Chapter 4. The Web Ontology Language OWL 2
Table 4.3: Complexity of OWL species (Source: [MGH+09]).
Language
Reasoning Problems
Taxonomic Com-
plexity
Data
Com-
plexity
Query
Com-
plexity
Combined
Com-
plexity
OWL
2
RDF-
Based
Semantics
Ontology
Consistency,
Class
Ex-
pression Satisﬁability, Class Expres-
sion Subsumption, Instance Check-
ing, Conjunctive Query Answering
Undecidable
Undecidable
Undecidable
Undecidable
OWL
2
Direct
Semantics
Ontology Consistency, Class Expres-
sion Satisﬁability, Class Expression
Subsumption, Instance Checking
2NEXPTIME-
complete
(NEXP-
TIME
if
property
hierarchies
are
bounded)
Decidable,
but
complexity
open
(NP-Hard)
Not Applicable
2NEXPTIME-
complete (NEXPTIME
if property hierarchies
are bounded)
Conjunctive Query Answering
Decidability open
Decidability open
Decidability open
Decidability open
OWL
2
EL
Ontology Consistency, Class Expres-
sion Satisﬁability, Class Expression
Subsumption, Instance Checking
PTIME-complete
PTIME-complete
Not Applicable
PTIME-complete
Conjunctive Query Answering
PTIME-complete
PTIME-complete
NP-Complete
PSPACE-Complete
OWL
2
QL
Ontology Consistency, Class Expres-
sion Satisﬁability, Class Expression
Subsumption, Instance Checking,
NLogSpace-complete
in AC0
Not Applicable
NLogSpace-complete
Conjunctive Query Answering
NLogSpace-complete
in AC0
NP-complete
NP-complete
OWL
2
RL
Ontology Consistency, Class Expres-
sion Satisﬁability, Class Expression
Subsumption, Instance Checking,
PTIME-complete
PTIME-complete
Not Applicable
PTIME-complete
Conjunctive Query Answering
PTIME-complete
PTIME-complete
NP-complete
NP-complete
OWL DL
Ontology Consistency, Class Expres-
sion Satisﬁability, Class Expression
Subsumption, Instance Checking,
NEXPTIME-complete
Decidable,
but
complexity
open
(NP-Hard)
Not Applicable
NEXPTIME-complete
Conjunctive Query Answering
Decidability open
Decidability open
Decidability open
Decidability open

4.3.
OWL in context
81
Table 4.3 summarises the known complexity results for OWL 2 under both RDF
and the direct semantics, OWL 2 EL, OWL 2 QL, OWL 2 RL, and OWL 1 DL. The
results refer to the worst-case complexity of these reasoning problems and, as such,
do not say that implemented algorithms necessarily run in this class on all input
problems, or what space/time they use on some/typical/certain kind of problems.
For X-complete problems, these results only say that a reasoning algorithm
cannot use less time/space than indicated by this class on all input problems,
where “X” is one of the complexity classes listed in the previous section.
4.3
OWL in context
OWL was designed for the World Wide Web, and has a place there, which is
outlined in the next subsection. A diﬀerent notion of ‘positioning’ OWL is with
respect to the language features, or: options to link OWL to more expressive
languages, which is described afterward.
4.3.1
OWL and the Semantic Web
OWL does not exist in isolation, but is part of the Semantic Web stack—also called
the (in)famous ‘layer cake’—to make the Semantic Web work. This layer cake is
shown in Figure 4.3. Stepwise working our way up from the bottom layer, there
is XML, which is a surface syntax that has no semantics, and then XML Schema,
which describes structure of XML documents.
RDF is intended for describing data and facilitating data exchange; it is a data
model for “relations” between “things”, which also has a RDF Schema and an RDF
Vocabulary Deﬁnition Language.
RDF data can be queried with the SPARQL
query language (one can draw an analogue with SQL for relational databases, but
then tailored to the Internet). At the time of writing, RDF with its Linked Data—
be it open or not—is quite popular. One of the central nodes in the Linked Data
cloud is DBpedia [BLK+09], an RDF-ised version of Wikipedia’s info boxes. Such
systems may be users of lightweight ontologies or structured controlled vocabular-
ies. The reason for lightweight is because the RDF store tends to be large with a
lot of data stored in triples.
On top of that, we have the ontology language for the Web, OWL, to handle
the knowledge and reasoning, and rules (RIF). RIF does not seem to be used much.
There are many user interfaces for the whole range of Semantic Web applica-
tions. The details of the “trust” and “crypto”, on the other hand, are still sketchy.
Perhaps the “crypto” will receive more attention with the increasing popularity of
BlockChain. There are, at the time of writing, some preliminary explorations on
using RDF with BlockChain. Also, as there are several BlockChain systems, and
they will need to interoperate at some point, so perhaps there is a job for ontologies
there as well in the near future.
Finally, several directions for extensions to OWL proposed. These include the
‘leftover’ from OWL 1’s “Future extensions”, such as the unique name assump-
tion, closed world assumption, making parthood a primitive object property alike

82
Chapter 4. The Web Ontology Language OWL 2
Figure 4.3: The Semantic Web layer cake.
subsumption is, syntactic sugar for, e.g., ‘macros’ and ‘n-aries’, a better integra-
tion with rules (RIF, DL-safe rules, SBVR), some orthogonal dimensions such as
temporal, fuzzy, rough, and/or probabilistic, and better support for multilingual
ontologies. Most of these desires were known during the standardisation of OWL
2. At the time of writing, it does not seem likely that a version 2.5 or even 3 will
be started any time soon (but that does not mean there are no such extensions
or solutions proposed for them; in fact, we will see some of them pass the revue
later in the book). Perhaps this is a sign that realising the ‘Semantic Web’ may
not happen after all. Regardless, OWL itself has a life of its own, were OWL ﬁles
are integrated into a wide range of applications on and oﬀthe Web in standalone
‘intelligent’ applications anyway.
4.3.2
The Distributed ontology, model, and speciﬁcation
language DOL
A few limitations of OWL 2 DL were illustrated in Example 4.2, showing that
entirely reasonable combinations of features, like asymmetry of hasMother and in-
ferring who has whom as aunt, are not possible within that framework. There are
alternatives, but they do come at the cost of scalability. If one needs scalability
as well, one could choose to ﬁrst develop the more precise ontology that has a
higher precision and coverage, check that all is consistent and satisﬁable, and then
simplify it to the proﬁle needed for the application.
One option is to use the Distributed ontology, model, and speciﬁcation language
(DOL), which has been approved as a standard of the Object Management Group

4.3.
OWL in context
83
CL
HOL
Prop
SROIQ
(OWL 2 DL)
FOL=
FOLms=
OBOOWL
EL++
(OWL 2 EL)
DL-LiteR
(OWL 2 QL)
DL-RL
(OWL 2 RL)
DDLOWL
ECoOWL
ECoFOL
F-logic
bRDF
RDF
RDFS
OWL-Full
EER
subinstitution
theoroidal subinstitution
simultaneously exact and 
model-expansive comorphisms
model-expansive comorphisms
Complexity classes of the logics
grey: no ﬁxed expressivity
green: decidable ontology languages
yellow: semi-decidable
orange: some second-order constructs
red: full second-order logic 
OBO 1.4
CASL
UML-CD
CL-
Schema.org
DFOL
Relations between the logics
Figure 4.4: A sub-graph of logics currently supported by DOL/Ontohub, linked with
a variety of logic translations; the arrow shapes indicate some technical diﬀerences in
translation from one logic to another and the diﬀerent colours of the boxes give a ballpark
ﬁgure of the expressivity/complexity of that language. (Source: based on [KK17b])
(OMG) in 20166. DOL is not yet a new language for representing the axioms,
but provides a uniﬁed metalanguage where one can slot in one’s logic of choice—
including OWL—as one pleases (roughly), and put the axioms that violate the
OWL 2 DL restrictions in another ontology module that is then linked to the
OWL ﬁle. The system can treat them both as one larger ontology and reason over
it (that will take a bit more time to complete, if at all). It comes with a tool for
realising reasoning over the combination of ontologies (the Heterogeneous ToolSet7)
and the OntoHub repository to store heterogeneous ontologies [CKK+17].
How this is achieved behind the scenes is not trivial; the general theoreti-
cal background of DOL is described in [KML10], with a detailed description in
[MCNK15]. It uses the notion of institutions (in the mathematical sense) to tie the
logics together, which were ﬁrst introduced in [GB92]. Institutions capture com-
monalities across logics—like FOL and DLs both using a model-theoretic seman-
tics, the overlap in constructors—and therewith provide a means of interoperability
across logics.
The orchestration of languages DOL supports currently is depicted in Figure 4.4,
which is organised along two dimensions: the quality of logic translation (the dif-
ferent shapes of the arrows) and expressivity of the logic (coloured boxes). The
expressivity ranges from the Semantic Web languages RDF and the OWL species
all the way up to (variants of) ﬁrst- and second-order logic, so as to cater for a
wide range of requirements from the diﬀerent communities that use models in one
way or another.
DOL has many more features that, at this stage in the book, may not make a
6http://www.omg.org/spec/DOL/
7http://hets.dfki.de

84
Chapter 4. The Web Ontology Language OWL 2
lot of sense or seem not really needed, but they probably will be perceived useful
as one advances in ontology engineering and/or are facing a concrete ontology
development and maintenance project in government or industry.
This for the
simple reason that ontologies ‘out there’ are deﬁnitely not as small and simple
as the African Wildlife Ontology.
For instance, one could create a network of
ontologies rather than a monolithic one, which can be useful if several groups at
diﬀerent locations contribute to the ontology, or create mappings between elements
in ontologies once one has imported one for reuse. We will see such scenarios in
Block II. In addition, or, perhaps: moreover, besides being able to represent the
aforementioned expressiveness example with the cake and the aunts (Example 4.2),
one can also do other things that are still in the ‘future extensions’ list for OWL,
such as playing with open vs. closed world semantics and declare n-ary relations
fully.
4.3.3
Common Logic
Ontology development does not need to occur with OWL. The main other logic
that has been serialised is Common Logic (CL) [CLs07], which has been standard-
ised by the ISO. It is a family of ﬁrst-order logics that share a common abstract
syntax, have a model-theoretic semantics, and it uses XML as well. It has three “di-
alects” (syntaxes): 1) the Common Logic Interchange Format CLIF (one textual
notation); 2) the Conceptual Graph Interchange Format (diagrams), CGIF; and
3) eXtended Common Logic Markup Language (XCL), based on XML (another
textual notation).
It had its own design goals, which wasn’t concerned with computability. More
speciﬁcally8, and in comparison with the earlier listed design goals of OWL, they
were: 1) Common interlingua for variety of KR notations; 2) Syntactically as un-
constrained as possible; 3) Semantically as simple and conventional as possible; 4)
Full ﬁrst-order logic with equality, at least; 5) web-savvy, up-to-date; 6) Historical
origins in Knowledge Interchange Format (KIF).
Because CL is more expressive, the Semantic Web languages (including the
OWL species) all can map into CL. Further, since DOL, it need not be ‘self stand-
ing’, as the DOL framework as well as the tooling with Hets do provide support for
CL; see also the “CL−” and “CL” boxes in the right-bottom corner in Figure 4.4.
4.4
Exercises
Review question 4.1. How does OWL/OWL 2 diﬀer from a DL language?
Review question 4.2. Describe what were the motivations to develop OWL 2.
Review question 4.3. What are the new features in OWL 2 DL compared to
OWL-DL?
8A clear brief overview can be found at https://www.w3.org/2004/12/rules-ws/slides/
pathayes.pdf

4.4.
Exercises
85
Review question 4.4. Which is the required format one has to serialise an OWL
ontology in?
Review question 4.5. List all the species of OWL (both standards).
Review question 4.6. Which language features can be used on simple object
properties only?
Review question 4.7. What are OWL 2 QL, OWL 2 RL, and OWL 2 EL tailed
toward, respectively?
Review question 4.8. What is the major advantage of the OWL 2 Proﬁles over
OWL 2 DL and OWL 2 full?
Review question 4.9. Which four ‘parameters’ are considered for complexity of
an OWL species?
Review question 4.10. Describe in one sentence the purpose of DOL.
Exercise 4.1. Complete Table 4.4: Verify the question marks in the table (ten-
tatively all “–”), ﬁll in the dots, and any “±” should be qualiﬁed at to what the
restriction is. You may prefer to distribute this exercise among your classmates.
Exercise 4.2. Consider some medical ontology. You know that an injury (a cut,
a fracture) to a bone in your hand is also an injury to your hand. How can you
model this, and similar, information in an OWL 2 DL ontology such that it infers
this not only for injuries to hands, but for any injury to any anatomical body part
to an injury to its (direct/indirect) whole? Which OWL 2 DL feature do you need
for this? Try to formalise it. *
Exercise 4.3. Install your ODE of choice, if not already done so, and acquaint
yourself with the software. If you installed Prot´eg´e (5.x it is in 2018), you may want
to have a look at the Pizza Ontology Tutorial (can be downloaded from the Web),
but note that it was for a prior Prot´eg´e version, so there are slight diﬀerences in
the screenshots there and the current interface. Also note that the Pizza tutorial
was designed with the intention to acquaint the user the tool, not as a cookbook
for best practices in ontology development (which it certainly is not).
Exercise 4.4. Recall Exercise 3.3: if you have not added the DL axiom renderer
plug in yet, do so now. Load the AWO, or any other expressive ontology (i.e., one
that is more than a simple bare hierarchy) and inspect both the DL rendering and
the quasi natural language of the Prot´eg´e interface. Use this to familiarise yourself
with the Prot´eg´e notation. Also, it may be of use when carrying out the next two
exercises.
Exercise 4.5. Several axioms were listed in the chapter. You will now add them
to a new ‘test ontology’ and experiment a bit with it.
a. Create a new ontology, give it a new URI, and save it in RDF/XML.
b. Add either Bicycle ⊑≥2 hasComponent.⊤or Bicycle ⊑∃hasComponent.Wheel.

86
Chapter 4. The Web Ontology Language OWL 2
c. Take the OWL classiﬁer (see footnote 5) and inspect the least expressive
OWL species and violations. *
d. Update the previous axiom with the following one: Bicycle ⊑≥2 hasCompo-
nent.Wheel.
e. Reload the ontology in the OWL classiﬁer and inspect the OWL species and
violations. What is the main diﬀerence? *
f. Experiment in a similar way with one or more of the other axioms listed in
the chapter.
Exercise 4.6. Create a new ontology, add the vegan and vegetarian from Exer-
cise 3.2, and check both O ⊢V egan ⊑V egetarian and O ⊢V egetarian ⊑V egan.
Describe the outcomes.
Exercise 4.7. Find another ontology development environment, be with a Web
interface or stand-alone tool.
a. Repeat the previous exercise.
b. Compare the tools by considering, among others: do they both support OWL
2 DL? Which one is easier to navigate? Which one has the most features to
help ontology development? Which one is easier for a collaborative ontology
development project?
If you cannot ﬁnd a tool, then consider the documentation of, e.g., MoKi and
ACEWiki and compare them on their features.
Exercise 4.8. Load university.owl (note the OWL species) in your ODE, in-
spect the contents, and try to represent:
a. A Joint Honors Maths & Computer Science Student, who is one who takes
both Computer Science and Mathematics modules.
b. A Single Honours Maths Student (or [Computer Science, Economics]) is
one who takes only Maths [Computer Science, Economics] modules.
Is it possible? If yes, how, if not, why not? *
Exercise 4.9. Classify the ontology of the previous question, and describe what
happened and changed.
Exercise 4.10. The university has a regulation that each undergraduate student
must take exactly 2 modules. Add this restriction to the ontology of the previous
question.
a. Student 9 takes MT101, CS101, and CS102. Do you think your ontology is
consistent? Describe why. Check your answer by adding the student and his
courses, run the reasoner and examine the inferences. *
b. Student 10 takes MT101, CS101, and EC101. Do you think your ontology is
consistent? Describe why. Check your answer by adding the data, running
the reasoner, and examining the inferences. *
Exercise 4.11. Open the computerscience.owl ﬁle, ﬁnd the principal errors in
the ontology, and distinguish them from the ‘knock-on’ errors that are merely a
consequence of the principal errors. What would you propose to a modeller how to
ﬁx it, and why? Note that “ﬁxing” is to be understood as obtaining a satisﬁable
ontology other than just deleting the unsatisﬁable classes. *

4.4.
Exercises
87
Table 4.4: Partial comparison of some OWL features
Language ⇒
OWL 1
OWL 2
OWL 2 Proﬁles
Feature ⇓
Lite
DL
DL
EL
QL
RL
Role hierarchy
+
+
+
.
+
.
N-ary roles (where n ≥2)
–
–
–
.
?
.
Role chaining
–
–
+
.
–
.
Role acyclicity
–
–
–
.
–
.
Symmetry
+
+
+
.
+
.
Role values
–
–
–
.
–
.
Qualiﬁed number restrictions
–
–
+
.
–
.
One-of, enumerated classes
?
+
+
.
–
.
Functional dependency
+
+
+
.
?
.
Covering constraint over concepts
?
+
+
.
–
.
Complement of concepts
?
+
+
.
+
.
Complement of roles
–
–
+
.
+
.
Concept identiﬁcation
–
–
–
.
–
.
Range typing
–
+
+
.
+
.
Reﬂexivity
–
–
+
.
–
.
Antisymmetry
–
–
–
.
–
.
Transitivity
+
+
+
.
–
.
Asymmetry
?
?
+
–
+
+
Irreﬂexivity
–
–
+
.
–
.
.
.
.
.
.
.
.
Exercise 4.12. Consider again the content of the AWO. How would you represent
the following information, if it is possible at all without running into inconsisten-
cies?
a. There is also bird wildlife, and birds ﬂy. Then there are penguins—a type of
bird—not only on Antarctica, but also in South Africa, in the wild even, so
they also have to be added to the AWO. Penguins don’t ﬂy, however. *
b. There are plenty of insects, too, which are also animals.
Before we look
into human edible insects, let’s ﬁrst try to represent the Lepidoptera family,
which literally means “having scaled wings”, i.e., insects like butterﬂies and
moths. The life of those insects goes through four stages where the entity
is morphologically rather distinct: egg, larva (e.g., the caterpillar), pupa (or
chrysalis), and adult (e.g., the butterﬂy). *
Note: you may wish to do this exercise together with a classmate to bounce of
ideas. The caterpillar/butterﬂy challenge will return in Chapter 5 from a diﬀerent
viewpoint.
Exercise 4.13. From an educational perspective, you could do Practical Assign-
ment 1 now (see Section B.1) or at the end of Block II. The advantage of doing
it now is that you will appreciate the contents of Block II more and can revisit
this assignment at the end of Block II, and it gives you a better understanding of
both the language features and the automated reasoner. The disadvantage is that

88
Chapter 4. The Web Ontology Language OWL 2
it might be harder to do now than at the end of Block II and the quality of the
ontology of your ﬁrst attempt to create one is likely going to be low (which, on the
other hand, is a good learning opportunity).
4.5
Literature and reference material
1. Ian Horrocks, Peter F. Patel-Schneider, and Frank van Harmelen. From SHIQ
and RDF to OWL: The making of a web ontology language. Journal of Web
Semantics, 1(1):7, 2003.
2. OWL Guide: http://www.w3.org/TR/owl-guide/
3. OWL Reference: http://www.w3.org/TR/owl-ref/
4. OWL Abstract Syntax and Semantics: http://www.w3.org/TR/owl-semantics/
5. B. Cuenca Grau, I. Horrocks, B. Motik, B. Parsia, P. Patel-Schneider, and
U. Sattler. OWL 2: The next step for OWL. Journal of Web Semantics:
Science, Services and Agents on the World Wide Web, 6(4):309-322, 2008.
6. Pascal Hitzler, Markus Kroetzsch, Sebastian Rudolph. Foundations of Se-
mantic Web Technologies. Chapman & Hall/CRC, 2009, 455p.
7. OWL 2 quick Reference: http://www.w3.org/TR/owl2-quick-reference/
8. OWL 2 Web Ontology Language Structural Speciﬁcation and Functional-
Style Syntax: http://www.w3.org/TR/owl2-syntax/
9. OWL 2 Proﬁles: http://www.w3.org/TR/owl2-profiles/

Part II
Developing good ontologies
89


CHAPTER 5
Methods and Methodologies
In Block I we looked at languages for representing ontologies, and you obtained
experience in reading existing ontologies, adding and removing some axioms, and
using the automated reasoner. But how exactly did someone come up with the
whole ontology in the ﬁrst place? What can, or should, you do when you have to
develop your own ontology? When is an ontology a good one? Just like in software
engineering, there are methods and methodologies to guide you through it so that
you will be able to answer these questions, or they at least will help out with one
or more of the steps in the development of an ontology.
There is not just one way of doing it or a single up-to-date comprehensive
methodology for ontology development that covers everything you possibly proba-
bly need, but there some useful steps and combinations. There are several proposals
along the line of generic ‘waterfall’ and ‘agile’ approaches that were inspired by soft-
ware development methodologies. They are at the level of general guidelines and
more and less detailed stages, which we shall cover in this chapter in Section 5.1.
Diagrammatically, such (generalised!) methodologies have the tasks as shown in
Figure 5.1: this one may look like a ‘waterfall’, but practically, it can be an itera-
tive one not only within the ontology development, but within the “maintenance”
that may involve substantial redesign or adding a new module that follows those
development steps again, and a further reﬁnement are methodologies for ontology
authoring that permeate the whole development process. Particular aspects of such
methodologies can be assisted by one or more methods and guidelines of the many
ones available. A main reason for this state of aﬀairs is that there is still much to
be done, there are many diﬀerent use-case scenarios, and it is—scientiﬁcally—hard
to prove one methodology is better than another. This is easier to demonstrate for
particular methods.
More speciﬁcally, in this chapter we start with high-level methodologies remi-
niscent of those in software development—dubbed (process-oriented) ‘macro-level’
methodologies—and more detailed ones that focus on ontology authoring—called
‘micro-level’ development—in Section 5.1. We then proceed to a sampling of meth-
91

92
Chapter 5. Methods and Methodologies
(scheduling, controlling, quality assurance)
Ontology management
Ontology development and support
Ontology use
Feasibility study (problems, opportunities, potential 
solutions, economic feasibility)
Conceptualisation (of the model, integration and 
extension of existing solutions)     
Implementation (ontology authoring in a logic-based 
representation language)
Domain Analysis (motivating scenarios, competency 
questions, existing solutions)
Maintenance (adapting the ontology to new 
requirements)
Use (ontology-based search, integration, negotiation)
Ontology reuse
Documentation
Evaluation
Knowledge 
acquisition
Figure 5.1: Main tasks in ontology engineering (Source: based on [SMB10])
ods that can be used as a component within those methodologies (Section 5.2),
which are roughly divided into one of the four categories of methods: logic-based
only, purely based on philosophy, a combination of the two, and heuristics.
5.1
Methodologies for ontology development
Several speciﬁc methodologies for ontology development exist following the gen-
eral idea depicted in Figure 5.1, notably the older Methontology and On-To-
Knowledge, the more recent NeON and Melting Point methodologies, and the
authoring-focused recent ones OntoSpec, DiDOn, and TDDonto, and the older
“Ontology Development 101” (OD101)1. They are not simply interchangeable in
that one could pick any one of them and it will work out well. Besides that some
are older or outdated by now, they can be distinguished in core approach, being
between:
• micro-level ontology authoring vs. a macro-level systems-view of ontology
development;
• isolated, single, stand-alone, ontology development vs. collaborative devel-
opment of ontologies and ontology networks.
Micro-level methodologies focus on the viewpoint of the details emphasising formal-
isation aspects, which goes into ontology authoring, for it is about writing down
the actual axioms and design choices that may even be driven by the language.
Macro-level methodologies, on the other hand, emphasise the processes from an
1details can be found in [FGPPP99, SSSS01, SFdCB+08, GOG+10, Kas05, Kee12b, KL16,
NM01], respectively.

5.1.
Methodologies for ontology development
93
information systems and IT viewpoint, such as depicted in Figure 5.1. They may
merge into comprehensive methodologies in the near future.
Regarding the second diﬀerence, this reﬂects a division between ‘old’ and ‘new’
methodologies in the sense that the older ones assume a setting that was typical
of 20 years ago: the development of a single monolithic ontology by one or a few
people residing in one location, who were typically the knowledge engineers doing
the actual authoring after having extracted the domain knowledge from the domain
expert. The more recent ones take into account the changing landscape in ontology
development over the years, being towards collaboratively building ontology net-
works that cater for characteristics such as dynamics, context, collaborative, and
distributed development. For instance, domain experts and knowledge engineers
may author an ontology simultaneously, in collaboration, and residing in two dif-
ferent locations, or the ontology may have been split up into inter-related modules
so that each sub-group of the development team can work on their section, and
the automated reasoning may well be distributed over other locations or remotely
with more powerful machines.
The remainder of this section provides an overview of these two types of guide-
lines.
5.1.1
Macro-level development methodologies
Waterfalls
The macro-level methodologies all will get you started with domain ontology devel-
opment in a structured fashion, albeit not all in the exact same way, and sometimes
that is even intended like that. For instance, one may commence with a feasibility
study and assessment of potential economic beneﬁts of the ontology-driven ap-
proach to solving the problem(s) at hand, or assume that is sorted out already or
not necessary and commence with the actual development methodology by con-
ducting a requirements analysis of the ontology itself and/or ﬁnd and describe case
studies. A well-known instantiation of the generic notions of the development pro-
cess depicted in Figure 5.1, is the comparatively comprehensive Methontology
methodology [GPFLC04], which has been applied to various subject domains since
its development in the late 1990s (e.g., the chemicals [FGPPP99] and legal do-
main [CMFL05]). This methodology is for single ontology development and while
several practicalities are superseded with more recent and even newer languages,
tools, and methodologies, the core procedure still holds. Like Figure 5.1, it has a
distinct ﬂavour of a waterfall methodology. The ﬁve main steps are:
1) Speciﬁcation: why, what are its intended uses, who are the prospective users
2) Conceptualization: with intermediate representations such as in text or dia-
grams
3) Formalization:
transforms the domain-expert understandable ‘conceptual
model’ into a formal or semi-computable model
4) Implementation: represent it in an ontology language
5) Maintenance: corrections, updates, etc.

94
Chapter 5. Methods and Methodologies
Figure 5.2:
Graphical depiction of several diﬀerent steps in ontology development,
where each step has its methods and interactions with other steps (Source: [SFdCB+08])
In addition, there are various management activities, such as planning activities,
control, and quality assurance, and supporting tasks, such as documentation and
version control. Ontology management may vary somewhat across the method-
ologies, such as helping with development of a Gantt chart for several ontology
development scenarios.
A reﬁnement over the years is, among others, the bet-
ter provision of ‘intermediate representations’; e.g., the MOdelling wiKI MoKi
[GKL+09] has a feature for automatic translation between formal and semi or in-
formal speciﬁcations by the diﬀerent experts, which is also reﬂected in the interface
so as to let domain experts, ontologists, and logicians work together on a single
project, which is further facilitated by chat-like features where discussions take
place during the modelling stage [DGR12].
Methontology is, practically, superseded by the NeON methodology. In-
stead of the straight-forward ﬁve steps, there are many possible routes composed
of multiple steps; see Figure 5.2. Various development scenarios are then speci-
ﬁed by combining a subset of those steps and in some order, which results in a
diﬀerent planning of the ontology activities. Each number in Figure 5.2 denotes
a scenario.
For instance, Scenario 4 is that of “Building ontology networks by
reusing and reengineering ontological resources.” [SFdCB+08]. How to actually do
this Scenario 4, is another matter. For instance, one may/will have to
a) be able to ﬁnd ontologies in the domain of interest, evaluate them on the
relevance and choose which would be the best ﬁt—a research problem of its

5.1.
Methodologies for ontology development
95
own (see, e.g., [KG17, KK12, McD17]);
b) extract only a module from an ontology, rather than reusing the whole on-
tology (see, e.g., [Daw17] for a recent overview, new methods, and tools);
c) convert the representation language of the ontology; e.g., from OWL 2 DL to
OWL 2 QL, or from OBO to OWL; and
d) align ontologies, which is even a sub-ﬁeld within ontology engineering that is
large and active enough for a second edition of a handbook [ES07].
Each of these tasks has its own theoretical foundations, methods, and tools.
NeON also includes more details for the speciﬁcation stage, especially with
respect to so-called Competency Questions (CQs). CQs, ﬁrst introduced in [GF95],
specify the questions one’s ontology should be able to answer and therewith what
knowledge the ontology should contain. For instance, with the AWO, one may
want the ontology to be able to answer “Which animal eats which other animal?”
and “Which animals are endangered?”. The AWO you have inspected does contain
some information to answer the former (lions eat impalas), but not the latter, for
it does not contain information about endangered species.
NeOn also has a “Glossary of Activities”, identifying and deﬁning 55 activities
when ontology networks are collaboratively built, such as ontology localisation (for
another natural language), alignment (linking to another ontology), and diagnosis
(of errors), which are divided into a matrix with “required” and “if applicable”
[SFdCB+08].
Not even the NeON methodology covers all options—i.e., all the steps and all
possible permutations at each step—that should be in an ontologist’s ‘tool box’,
though. For instance, some mention “non-ontological resource reuse” for bottom-
up ontology development (number 2 in Figure 5.2), and note NLP and reuse of
thesauri, but lack detail on how this is to be done—for that, one has to search the
literature and look up speciﬁc methods and tools and the other bottom-up routes
(the topic of Chapter 7) that can, or have to be, ‘plugged in’ the methodology
actually being applied. A glaring absence from the methodologies is that none
of them incorporates a ‘top-down’ step on foundational ontology use to enforce
precision and interoperability with other ontologies and reuse generic classes and
object properties to facilitate domain ontology development. We will look at this in
some detail in Chapter 6. For the older methodologies this may be understandable,
given that at the time they were hardly available, but it is a missed opportunity
for the more recent methodologies.
Lifecycles
A recent addition to the ontology development methodology landscape is the On-
tology Summit 2013 Communiqu´e’s2 take on the matter with the ontology lifecycle
model; see Figure 5.3. Each stage has its own set of questions that ought to be
answered satisfactorily. To provide a ﬂavour of those questions that need to be
answered in an ontology development project, I include here an arbitrary selection
of such questions at several stages, which also address evaluation of the results of
that stage (see the communiqu´e or [N+13] for more of such questions):
2http://ontolog.cim3.net/cgi-bin/wiki.pl?OntologySummit2013_Communique

96
Chapter 5. Methods and Methodologies
• Requirements development phase
– Why is this ontology needed? (What is the rationale? What are the
expected beneﬁts)?
– Are there existing ontologies or standards that need to be reused or
adopted?
– What are the competency questions? (what questions should the ontol-
ogy itself be able to answer?)
• Ontological analysis phase
– Are all relevant terms from the use cases documented?
– Are all entities within the scope of the ontology captured?
• System design phase
– What operations will be performed, using the ontology, by other system
components? What components will perform those operations? How do
the business requirements identiﬁed in the requirements development
phase apply to those speciﬁc operations and components?
– How will the ontology be built, evaluated, and maintained? What tools
are needed to enable the development, evaluation, conﬁguration man-
agement, and maintenance of the ontology?
At this point, you are not expected to be able to answer all questions already. Some
possible answers will pass the revue in the remainder of the book, and others you
may ﬁnd when working on the practical assignment.
Figure
5.3:
Ontology
Summit
2013’s
lifecycle
model
(Source:
http://ontolog.cim3.net/cgi-bin/wiki.pl?OntologySummit2013 Communique)
Agile
Agile approaches to ontology development are being investigated at the time of
writing this book. That is, this is in ﬂux, hence, perhaps, too early at this stage
to give a full account of it. For some preliminary results, one may wish to have a
look at, e.g., the Agile-inspired OntoMaven that has OntoMvnTest with ‘test cases’

5.1.
Methodologies for ontology development
97
for the usual syntax checking, consistency, and entailment [PS15], simpliﬁed agile
[Per17], a sketch of a possible Test-Driven Development methodology is introduced
in [KL16], and eXtreme Design was added to NeON [PD+09].
It is beyond the current scope to provide a comparison of the methodologies
(see for an overview [GOG+10]). Either way, it is better to pick one of them to
structure your activities for developing a domain ontology than using none at all.
Using none at all amounts to re-inventing the wheel and stumbling upon the same
diﬃculties and making the same mistakes developers have made before, but a good
engineer has learned from previous mistakes. The methodologies aim to prevent
common mistakes and omissions, and let you to carry out the tasks better than
otherwise would have occurred without using one.
5.1.2
Micro-level development
OntoSpec, OD101, and DiDOn can be considered ‘micro-level’ methodologies: they
focus on guidelines to formalise the subject domain, i.e., providing guidance how
to go from an informal representation to a logic-based one. While this could be
perceived to be part of the macro-level approach, as it happens, such a ‘micro-level
view’ actually does aﬀect some macro-level choices and steps. It encompasses not
only axiom choice, but also other aspects that aﬀect that, such as the following
ones (explained further below):
1) Requirements analysis, with an emphasis on purpose, use cases regarding
expressiveness (temporal, fuzzy, n-aries etc.), types of queries, reasoning ser-
vices needed;
2) Design of an ontology architecture (e.g., modular), distributed or not, which
(logic-based) framework to use;
3) Choose principal representation language and consider encoding peculiarities
(see below);
4) Consider and choose a foundational ontology and make modelling decisions
(e.g., on attributes and n-aries as relations or classes; Chapter 6);
5) Consider domain ontology, top-domain level ontology, and ontology design
pattern ontology reuse, if applicable, and any ontology matching technique
required for their alignment;
6) Consider semi-automated bottom-up approaches, tools, and language trans-
formations, and remodel if needed to match the decisions in steps 3 and 4
(Chapter 7);
7) Formalization (optionally with intermediate representations), including:
a) examine and add the classes, object properties, constraints, rules taking
into account the imported ontologies;
b) use an automated reasoner for debugging and detecting anomalous de-
ductions in the logical theory;
c) use ontological reasoning services for ontological quality checks (e.g.,
OntoClean and RBox Compatibility);
d) add annotations;

98
Chapter 5. Methods and Methodologies
8) Generate versions in other ontology languages, ‘lite’ versions, etc., if applica-
ble;
9) Deployment, with maintenance, updates, etc.
Some of them are incorporated also in the macro-level methodologies, but do not
yet clearly feature in the detail required for authoring ontologies. There is much
to say about these steps, and even more yet to be investigated and developed
(and they will be revised and reﬁned in due time); some and its application to
bio-ontologies can be found in [Kee12b].
For the remainder of this section, we shall consider brieﬂy the language choice
and some modelling choices on formalising it, in order to demonstrate that the
‘micro’ is not a ‘single step’ as it initially might seem from the macro-level method-
ologies, and that the ‘micro’ level does not simply consist of small trivial choices
to make in the development process.
The representation language
Regarding formalisation, the ﬁrst aspect is to choose a suitable logic-based lan-
guage, which ought to be the optimal choice based on the required language fea-
tures and automated reasoning requirements (if any), that, in turn, ought to follow
from the overall purpose of the ontology (due to computational limitations), if there
is a purpose at all [Kee10a]. Generalising slightly, they fall into two main group:
light-weight ontologies—hence, languages—to be deployed in systems for, among
others, annotation, natural language processing, and ontology-based data access,
and there are ‘scientiﬁc ontologies’ for representing the knowledge of a subject do-
main in science, such as human anatomy, biological pathways, and data mining
[D+10, HND+11, KLd+15, RMJ03]. More importantly for choosing the suitable
language, is that the ﬁrst main group of ontologies require support for navigation,
simple queries to retrieve a class in the hierarchy, and scalability. Thus, a language
with low expressiveness suﬃces, such as the Open Biological and biomedical Ontolo-
gies’ obo-format, the W3C standardised Simple Knowledge Organisation System
(SKOS) language [MB09], and the OWL 2 EL or OWL 2 QL proﬁle [MGH+09].
For a scientiﬁc ontology, on the other hand, we need a very expressive language to
capture ﬁne-grained distinctions between the entities. This also means one needs
(and can use fruitfully) more reasoning services, such as satisﬁability checking,
classiﬁcation, and complex queries. One can choose any language, be it full ﬁrst
order predicate logic with or without an extension (e.g., temporal, fuzzy), or one
of the very expressive OWL species to guarantee termination of the reasoning ser-
vices and foster interoperability and reuse with other ontologies. The basic idea is
summarised in Figure 5.4, which is yet to be reﬁned further with more ontology
languages, such as the OWL 2 RL proﬁle or SWRL for rules and the DLR and
CFD families of DL languages that can handle n-ary relationships (with n ≥2)
properly.
The analysis of the language aspects can be pushed further, and one may wish
to consider the language in a more ﬁne-grained way and prefer one semantics over
another and one ontological commitment over another.
For instance, assessing
whether one needs access to the components of a relationship alike UML’s asso-

5.1.
Methodologies for ontology development
99
Is reasoning 
required?
Only data 
annotation?
Text 
annotation?
Expressivity 
is important?
Use OWL (2) DL
Use OWL 2 EL
Use OBO 
or OWL 2 EL
Use SKOS, OBO, or 
OWL 2 EL
No
Yes
Decidability is 
important?
Use any FOL, extension thereof, or higher order 
logic, e.g. Common Logic, DLRus
large ABox?
Use OWL 2 QL
Figure 5.4: A preliminary decision diagram to choose a suitable ontology language for
one’s prospective ontology, with indications of current typical usage and suggestions for
use. (Source: extended from [Kee12b])
ciation ends, the need for n-aries, or whether asymmetry is essential, and, e.g.,
graph-based versus model-theoretic semantics. This is interesting from a logic and
philosophical perspective at a more advanced level of ontology engineering and re-
search, which we will not cover in this introductory course to a practically usable
detail.
Encoding Peculiarities
This is tricky to grasp at the start: there may be a diﬀerence between what the
domain expert sees in the tool—what it is ‘understood to represent’—and what
you, as the computer scientist, know how it works regarding the computational
representation at the back-end that a domain expert need not know about. On-
tologies need not be stored in an OWL ﬁle. For instance, it may be the case that
a modeller sees an ontology in the interface of a software application, but those
classes, relations, and constraints are actually stored in a database, or an n-ary
relationship is depicted in the diagrammatic rendering of the ontology, but this
is encoded as 3 binaries behind the scenes.
The former plays a trick logically:
in that case, mathematically, classes are stored as instances in the system (not
classes-as-instances in the ontology!). For instance, Chair may be represented in
the OWL ontology as the class Chair, but one equally well can store Chair in a
database table, by which it mathematically has become an instance when it is a
tuple or a value when it is stored in a cell, yet it is ‘thought of’ and pretended
to be a universal, class, or concept in the graphical interface. This is primarily
relevant for SKOS and OBO ontologies. Take the Gene Ontology, among others,
which is downloadable in OBO or OWL format—i.e., its taxonomy consists of,
mathematically, classes—and is available in database format—i.e., mathematically
it is a taxonomy of instances. This does not have to be a concern of the subject

100
Chapter 5. Methods and Methodologies
domain experts, but it does aﬀect how the ontology can be used in ontology-driven
information systems. A motivation for storing the ontology in a database, is that
databases are much better scalable, which is nice for querying large ontologies. The
downside is that data in databases are much less usable for automated reasoning.
As an ontology engineer, you will have to make a decision about such trade-oﬀs.
There is no such choice for SKOS ‘ontologies’, because each SKOS concept is
always serialised as an OWL individual, as we shall see in Chapter 7. One has to
be aware of this distinction when converting between SKOS and OWL, and it can
be handled easily in the application layer in a similar way to GO.
One also could avail of “punning” as a way to handle second-order logic rules
in a ﬁrst-order setting and use the standard reasoners instead of developing a new
one (that is, not in the sense of confusing class as instance, but for engineering
reasons), or ‘push down’ the layers. This can be done by converting the content of
the TBox into the ABox, encode the second-order or meta rules in the TBox, and
classify the classes-converted-into-individuals accordingly. We will come across one
such example with OntoClean in Section 5.2.2.
In short: one has to be careful with the distinction between the ‘intended
meaning’ and the actual encoding in an implemented system.
On formalising it
The ‘how to formalise it?’ question is not new, neither in IT and Computing [Hal01,
HP98] nor in logic [BE93], and perhaps more of those advances made elsewhere
should be incorporated in ontology development methodologies. For ontologies,
they seem to be emerging as so-called modelling styles that reﬂect formalisation
choices. These formalisation choices can have a myriad of motivations, but also
consequences for linking one’s ontology to another or how easy it is to use the
ontology in, say, an OBDA system. The typical choices one probably has come
across in conceptual modelling for database systems or object-oriented software
also appear here, as well as others. For instance:
– will you represent ‘marriage’ as a class Marriage or as an object property
isMarriedTo?
– will you represent ‘skill’ as a class Skill, as an object property hasSkill, or as
a data property (attribute) with values?
At this stage, it may look like an arbitrary choice of preference or convenience.
This is not exactly the case, as we shall see in Chapter 6. Others have to do with
a certain axiom type or carefulness:
– On can declare, say, hasPart and partOf and state they are inverses, i.e.,
adding two vocabulary elements to the ontology and the axiom hasPart ≡
partOf−. In OWL 2, one also could choose to add only one of the two and
represent the other through an inverse directly; e.g., with only hasPart, one
could state C ⊑hasPart−.D “Each C is part of at least one D”, i.e., partOf
would not be a named object property in the ontology.
– The Pizza Ontology Tutorial cautions against declaring domain and range
axioms, mainly because the inferences can come as a surprise to novice ontol-
ogy developers. Should one therefore avoid them? Ideally, no, for this results

5.2.
Methods to improve an ontology’s quality
101
in a lower precision and actually may hide defects in the ontology.
– n-aries (n ≥3) cannot be represented fully, only approximated, in OWL and
there are diﬀerent ways to manage that, be it through reiﬁcation or choosing
another logic.
Whichever way you choose to represent a particular recurring pattern, do try to do
it consistently throughout. There are some methods and tools to assist with such
matters, which will be introduced gradually, starting with the next section.
5.2
Methods to improve an ontology’s quality
The methodologies we have seen in the previous section may include one or more
methods at a particular step in the process.
These methods aim to assist the
ontologist in certain tasks of the ontology engineering process, such as to assist the
modelling itself and to integrate ontologies, which may have supporting software
tools. The methods can be divided roughly into: logic-based only, purely based on
philosophy, a combination of the two, and practical rules or guidelines. Each of
these categories has several methods with more or less tool support. In this section,
we take an illustrative sampling of each of them, respectively:
1. The ‘debugging’ of deductions that caused, e.g., one or more classes to have
become unsatisﬁable, where we’ll see some detail as to what creates such
justiﬁcations;
2. OntoClean to ‘clean up’ a ‘dirty’ taxonomy;
3. The RBox compatibility service for coherent hierarchies and role chains of
object properties;
4. OOPS! with TIPS to catch common pitfalls and how to avoid them.
5.2.1
Logic-based methods: explanation and justiﬁcation
People make errors with respect to what they intend to represent in the ontology,
or do it correctly, but are somewhat surprised by one or more deductions. The
automated reasoners can help explain that, or: ‘justify’ the deduction. The more
recent versions of ODEs may have this feature already implemented, and you may
have come across it during the exercises (e.g., by having clicked on the “?” on the
right of the yellow deduction in Prot´eg´e). Put diﬀerently: you have been using an
automated reasoner to ‘debug’ the ontology. Where do they come from, and what
is a good strategy to explain deductions to the modeller or domain expert?
As a ﬁrst step to obtain the answers, researchers looked at what were the most
common logical mistakes that modellers made. Typical mistakes that cause a class
to be unsatisﬁable, result in undesirable inferred subsumptions, or inconsistent
ontologies, are the following ones:
• The basic set of clashes for concepts (w.r.t. tableaux algorithms) resulting in
an incoherent ontology are:
– Atomic: Any individual of a class would belong to a class and its com-
plement;

102
Chapter 5. Methods and Methodologies
– Cardinality: A class has a max cardinality restriction declared, but its
subclass has a higher min cardinality on that same object or data prop-
erty;
– Datatype: A literal value violates the (global or local) range restrictions
on a data property (i.e., conﬂicting data types).
• The basic set of clashes for the ontology resulting in an inconsistent ontology
are:
– Inconsistency of assertions about individuals, e.g., an individual is as-
serted to belong to disjoint classes or has a cardinality restriction but
related to more individuals;
– Individuals related to unsatisﬁable classes;
– Defects in class axioms involving nominals (owl:oneOf, if present in the
language).
The second step was to integrate this with what the reasoner computes along the
way to the ﬁnal deduction: which axioms are involved that lead to the unsatisﬁable
class or inconsistent ontology? Any such explanation feature thus uses at least the
standard reasoning services. It adds further and new reasoning services tailored to
pinpointing the errors and explaining the entailments to, e.g., try to ﬁnd the least
number of axioms among the alternative explanation. Such ‘debugging’ goes under
terms like glass box reasoning, (root) justiﬁcation, explanation, and pinpointing
errors. This may sound easy: just get a log from the reasoner. It is not that
simple, however. Consider the following example.
Example 5.1. The ontology O under consideration contains, among many other
axioms, the following two:
A ⊑B ⊓C ⊓¬C
A ⊑¬B
One deduces A ⊑⊥. Why? There are two routes that explain this deduction
purely based on the axioms, indicated in red:
1. A ⊑B ⊓C ⊓¬C
2. A ⊑B ⊓C ⊓¬C
A ⊑¬B
Which of the two ‘explanations’ should be shown to the user, or both? And the
whole axiom, or only the relevant part(s) of it, or like the colour highlighting?
Or, given a set of explanations, those axioms that appear in all explanations or,
vv., that are unique (i.e., appear only once across all explanations)? Or maybe to
not show a whole set of axioms, but instead only the essence in natural language,
alike “A is a subclass of both a class and its complement, which causes it to be
unsatisﬁable” or perhaps a Venn diagram can be drawn?
♦
Both the theory and how to present it to the user have been investigated. If you
use Prot´eg´e, it is based on the work presented in [HPS08]. They use the notions
of laconic justiﬁcations, which are justiﬁcations—i.e., a set of relevant axioms—
whose axioms do not contain any superﬂuous parts (and all of whose parts are as
weak as possible3) and precise justiﬁcations, which are laconic justiﬁcations where
3an axiom β is deemed weaker than another one, α if and only if α |= β and β ⊭α

5.2.
Methods to improve an ontology’s quality
103
each axiom is a minimal repair in the sense that changing something to any axiom
may result in ﬁxing the undesirable deduction. While they are useful topics, we
will spend little time on it here, because it requires some more, and more in-
depth, knowledge of Description Logics and its reasoning algorithms (suitable for
a Description Logics course).
Proposing possible ﬁxes automatically is yet a step further and research is still
under way to address that. This is in no small part because it is hard to second-
guess the user. Taking the axioms in Example 5.1 as example, there are already
multiple options, such as removing A ⊑¬B, or deleting A ⊑B, and likewise for C,
yet showing all possible ways to ﬁx the undesirable deduction results in too much
clutter.
5.2.2
Philosophy-based methods: OntoClean to correct a
taxonomy
OntoClean [GW09] helps the ontologist to ﬁnd errors in a taxonomy, and explains
why. One might ask oneself: who cares, after all we have the reasoner to classify
our taxonomy anyway, right? Indeed, but that works only if you have declared
many properties for the classes so that the reasoner can sort out the logical is-
sues. However, it is not always the case that many property expressions have been
declared for the classes in the ontology and those reasoners do not detect certain
ontological issues.
OntoClean ﬁlls this gap for taxonomies. It uses several notions from philosophy,
such as rigidity, identity criteria, and unity (based on [GW00a, GW00b]) to provide
modelling guidelines. Let’s take rigidity as example, for it can be used elsewhere
as well. There are four diﬀerent types of rigidity, but the useful ones are rigid and
anti-rigid, which are deﬁned as follows:
Deﬁnition 5.1. (+R [GW09]) A rigid property φ is a property that is essential to
all its instances, i.e., ∀xφ(x) →□φ(x).
Deﬁnition 5.2. (∼R [GW09]) An anti-rigid property φ is a property that is not
essential to all its instances, i.e., ∀xφ(x) →¬□φ(x).
OntoClean takes these sort of metaproperties to annotate each class in the
ontology. For instance, a modeller may want to assert that Apple is rigid (each
instance remains an apple during its entire existence) and being a Professor is anti-
rigid (all individuals that are professors now were at some time not a professor).
Subsequently, we apply meta-rules to reclassify the classes. For our rigid and
anti-rigid meta-property, the applicable rule is as follows:
• Given two properties (classes), p and q, when q subsumes p the following
constraint hold:
1. If q is anti-rigid, then p must be anti-rigid
Or, in shorthand: +R ̸⊂∼R, i.e., it cannot be the case that a class that is anno-
tated as being rigid is subsumed by a class that is annotated as being anti-rigid.

104
Chapter 5. Methods and Methodologies
For instance, if we have, say, both Student and Person in our ontology, then the
former is subsumed by the latter, not vice versa, because Person is rigid and Stu-
dent anti-rigid. If Person ⊑Student were asserted, it would say that each person is
a student, which we know not to be the case: 1) it is not the case that all persons
come into existence as students and die as students, and 2) it is not the case that
if a student ceases to be a student (e.g., graduates), then that object also ceases to
be a person.
Besides manual analyses, currently, two approaches have been proposed for
incorporating the ideas of OntoClean in OWL ontologies. One is to develop a sep-
arate application to handle the annotations of the classes and the rules, another is
to leverage the capabilities of the standard reasoning services of the OWL reason-
ers, which is done by [GRV10, Wel06]. They diﬀer in the details, but they have in
common the high-level approach:
1) develop the domain ontology (TBox);
2) push it into the ABox (i.e., convert everything from the TBox into ABox
assertions);
3) encode the OntoClean ‘meta rules’ in the TBox;
4) run the standard OWL reasoner and classify the ‘instances’;
5) transfer the reclassiﬁcations in the taxonomy back into the domain-ontology-
in-TBox.
Finally, observe that this machinery of OntoClean also provides one with the theory
to solve the “green apple issue” we encountered in Section 1.2.3 on good and bad
ontologies: Apple is rigid (and a sortal), but its greenness is not. It is the rigid
entities that provide a backbone of an ontology, not the other ones (like Green,
Student) that depend on the existence of rigid entities.
An OntoClean tutorial can be found in Appendix A.1.
5.2.3
Combining logic and philosophy: role hierarchies
OntoClean does little to help solving so-called undesirable deductions, be they logi-
cally consistent or not, and the justiﬁcations computed may not always point to the
root problem from a modelling viewpoint. The RBox Compatibility service [KA08]
and its extension to SubProS and ProChainS [Kee12a] can assist with at least
some of that. They check for meaningful object property hierarchies and property
chains. This has as prerequisite to know when a property hierarchy is ‘good’ (e.g.:
guaranteed not lead to an undesirable deduction). Only afterward can one test for
violations of those principles, and ﬁnally have guidance on how a mistake can be
revised.
The hierarchy of object properties must be well-formed, which entails the prin-
ciples as to what it means for one property to be a sub-property of another. In anal-
ogy to a class hierarchy, where the instances of a subclass necessarily are a subset of
the set of instances of the superclass, one can state that in every model, the tuples
(individual relations) of the sub-property are a subset of the tuples of its parent
property. This can be guaranteed in two distinct ways. The most-straightforward
case is that the domain and/or range of the sub-property must be a subclass of the
domain and/or range of its super-property. This is similar to UML’s ‘subsetting’

5.2.
Methods to improve an ontology’s quality
105
for associations. The other has to do with implications of property characteristics;
e.g., asymmetry can be derived from antisymmetry and irreﬂexivity, hence, it is a
stronger constraint. This is presented informally in Figure 5.5.
Relationship 
characteristic
Antisymmetry
Irreﬂexivity
Transitivity
{disjoint, complete}
Local
Reﬂexivity
Symmetry
Asymmetry
Acyclicity
Intransitivity
Purely-
reﬂexive
Strongly 
intransitive
B.
Global 
Reﬂexivity
Figure 5.5: Constraining a property: A: an example, alike the so-called ‘subsetting’
idea in UML; B: hierarchy of property characteristics (structured based on information
in [Hal01, HC11]), where the dashed ones are not available in OWL 2 DL. (Source: based
on [Kee14])
To state the subsetting notion formally into the RBox Compatibility service
(a subset of SubProS), let’s introduce in the following deﬁnition the notation to
denote the user-deﬁned domain and range of an object property.
Deﬁnition 5.3. (User-deﬁned Domain and Range Concepts [KA08]). Let R be
a role and R ⊑C1 × C2 its associated Domain & Range axiom. Then, with the
symbol DR we indicate the User-deﬁned Domain of R—i.e., DR = C1—while with
the symbol RR we indicate the User-deﬁned Range of R—i.e., RR = C2.
The RBox Compatibility can then be deﬁned as follows, covering each permutation
of domain and range of the sub- and super property in the hierarchy in the RBox
R, with the domains and ranges from the TBox T :
Deﬁnition 5.4. (RBox Compatibility [KA08]) For each pair of roles, R, S, such
that ⟨T , R⟩|= R ⊑S, check whether:
Test 1. ⟨T , R⟩|= DR ⊑DS and ⟨T , R⟩|= RR ⊑RS;
Test 2. ⟨T , R⟩̸|= DS ⊑DR;
Test 3. ⟨T , R⟩̸|= RS ⊑RR.
An RBox is said to be compatible iﬀTest 1 and ( 2 or 3) hold for all pairs of
role-subrole in the RBox.
An ontology that does not respect the RBox compatibility criterion can be con-
sidered as ontologically ﬂawed, as it may not generate a logical inconsistency and
the criterion—or: semantics of object property subsumption—is at least an extra-
logical, if not ontological, criterion.
This holds also for the extended version,

106
Chapter 5. Methods and Methodologies
SubProS, that has a longer list of tests to also cover the hierarchy of property
characteristics.
Checking for RBox compatibility—hence, for ontological RBox correctness—
can be implemented by availing of the standard DL/OWL automated subsumption
reasoning service. It may serve to have further tooling assistance in correcting any
ﬂaws that may be detected.
On can extend this further to ‘safe’ property chains, as, essentially, they are also
inclusions, just with a chain on the left rather than a single property. Basically, the
domain and range class from left to right in the chain on the left-hand side of the
inclusion has to be equal or a superclass, and likewise for the outer domain (resp.
range) on the left-hand side and range of the object property on the right-hand
side of the inclusion; the complete test to check for a safe chain is described in
[Kee12a]. This is illustrated in the following example.
Example 5.2. Take the property chain hasMainTable ◦hasFeature ⊑hasFeature
in the Data Mining and OPtimisation (DMOP) ontology, which is depicted in
Figure 5.6.
The two properties have the domain and range axioms as follows:
hasMainTable ⊑DataSet × DataTable and hasFeature ⊑DataTable × Feature. The
range of hasMainTable and domain of hasFeature match neatly, i.e., both are DataT-
able. However, the domain of hasMainTable is DataSet and the domain of hasFea-
ture is DataTable, and DataSet and DataTable are non-disjoint sibling classes. The
reasoner infers DataSet ⊑DataTable because of the property chain, which is un-
desirable, because a set is not a subclass of a table. The ProChainS tests helps
detecting such issues, and proposals how to revise such a ﬂaw are also described in
[Kee12a]. Note that in this case, there was not a logical inconsistency according to
the language and the automated reasoning services, but instead it was a modelling
issue. ♦
Figure 5.6: The property chain hasMainTable ◦hasFeature ⊑hasFeature with the domain
and range axioms of the two object properties. (Source: based on [Kee12a])
There are further aspects to the semantics of relations (roles/object properties),
such as material vs formal relations and the ‘standard view’ vs ‘positionalist’ com-
mitment as to what relations are [Fin00, GW08, Leo08, Loe15] that may inﬂuence
the formalisation as well as language design [FK15, Leo08, Loe15]. At present,
within the scope of ontology development, there is only one preliminary model
and corresponding Prot´eg´e plugin [KC16] that takes some of this into account and
therefore it will not be elaborated on further here.
5.2.4
Heuristics: OntOlogy Pitfall Scanner OOPS!
Besides the theoretical guidance, there are other ways to address glitches in on-
tology authoring. Early works aimed at identifying typical modelling mistakes in

5.2.
Methods to improve an ontology’s quality
107
OWL are described in [RDH+04], which moved onward to the notion of “anti-
patterns” of the ‘don’t do this’ variety [RCVB09], and a growing catalogue of
pitfalls [PVSFGP12] of which 21 can be scanned automatically online with the
OntOlogy Pitfall Scanner!
(OOPS!)4.
A selection of those pitfalls are:
Cre-
ating synonyms as classes (P2); Creating the relationship “is” instead of using
rdfs:subClassOf, rdf:type or owl:sameAs (P3); Deﬁning wrong inverse rela-
tionships (P5); Including cycles in the hierarchy (P6); Merging diﬀerent concepts
in the same class (P7); Missing disjointness (P10); Missing domain or range in
properties (P11); Swapping intersection and union (P19); Using a miscellaneous
class (P21); Using diﬀerent naming criteria in the ontology (P22); Deﬁning a rela-
tionship inverse to itself (P25); Deﬁning inverse relationships for a symmetric one
(P26); and Deﬁning wrong transitive relationships (P29). Pitfall P19 is illustrated
in the following example.
Example 5.3. You have to represent “a pizza Hawaii has as topping ham and
pineapple”5. A modeller may be inclined to take the natural language description
of the toppings quite literally, and add
∃hasTopping.(Ham ⊓Pineapple)
(5.1)
However, this is not what the modeller really wants to say. The “⊓” means ‘and’,
i.e., an intersection, and thus the “(Ham ⊓Pineapple)” is the OWL class with those
objects that are both ham and pineapple. However, nothing is both, for meat and
fruit are disjoint, so the pizza Hawaii in our ontology has a topping that is Nothing.
What we want to represent, is that from PizzaHawaii there are at least two outgoing
relations for the toppings, being one to Ham and one to Pineapple, i.e.,
∃hasTopping.Ham ⊓∃hasTopping.Pineapple
(5.2)
In addition, one may want to add a so-called ‘closure axiom’ to say that all pizzas
Hawaii “have as topping only ham and pineapple”,
∀hasTopping.(Ham ⊔Pineapple)
(5.3)
Note also here that there is not a one-to-one mapping between the imprecise natural
language and the constructors: ham and pineapple, but using an ‘or’ ⊔, which
becomes clearer when we rephrase it as “all toppings are either ham or pineapple”.
♦
An evaluation of the presence of those 21 pitfalls showed that it does not make
much diﬀerence whether the ontology is one developed by novices, an arbitrary
ontology, or is a well-known ontology [KSFPV13]. It may well be that the notion
of a good quality ontology is not tightly related to absence of pitfalls, or maybe the
modelling pitfalls are propagated from the well-known ones by novice modellers;
whichever be the case, it is fertile ground for research. Notwithstanding this, the
4http://www.oeg-upm.net/oops
5and we ignore the fact that, according to Italians, pizzas are not supposed to have any fruit
on a pizza—other than tomatoes—so the pizza Hawaii is not really an Italian pizza.

108
Chapter 5. Methods and Methodologies
ontology can be scanned quickly with OOPS! and the results provide pointers where
the ontology may be improved.
The error, anti-pattern, and pitfall eﬀorts look at quality of an ontology from
the negative side—what are the mistakes?—whereas, e.g., OntoClean and the RBox
compatibility view it from the positive side, i.e., what does a good representation
look like? To this end, one also can turn around the pitfalls, into authoring guide-
lines, which is dubbed the Typical pItfall Prevention Scheme, TIPS [KSFPV15],
which describe the tips in the imperative so as to indicate what a developer should
be checking.
The one that includes trying to avoid the problem illustrated in
Example 5.3 is the following:
T7: Intended formalization (includes P14, P15, P16, P19, C1, and
C4): A property’s domain (resp., range) may consist of more than one
class, which is usually a union of the classes (an or), not the inter-
section of them. Considering the property’s participation in axioms,
the AllValuesFrom/only/∀can be used to ‘close’ the relation, i.e., that
no object can relate with that relation to the class other than the one
speciﬁed. If you want to say there is at least one such relation (more
common), then use SomeValuesFrom/some/∃instead. To state there is
no such relation in which the class on the left-hand side participates,
put the negation before the quantiﬁer (¬∀or ¬∃), whereas stating that
there is a relation but just not with some particular class, then the
negation goes in front of the class on the right-hand side; e.g., a vege-
tarian pizza does not have meat as ingredient (¬∃hasIngredient.Meat),
not that it can have all kinds of ingredients—cucumber, marsh mellow,
etc.—as long as it is not meat (∃hasIngredient.¬Meat). To avoid the
latter (the unintended pizza ingredients), one ought not to introduce
a class with negation, like NotMeat, but use negation properly in the
axiom. Finally, when convinced all relevant properties for a class are
represented, consider making it a deﬁned class, if not already done so.
[KSFPV15]
One that has nothing to do with logic foundations, but enters the picture for
ontology development is the aspect where it is still more of a craft and engineering,
is the following.
T1: Class naming and identiﬁcation (includes P1, P2, P7, C2,
and C5): When identifying and naming classes in ontologies, avoid
synonymy and polysemy: distinguish the concept itself from the diﬀer-
ent names such a concept can have (the synonyms) and create just one
class for the concept and provide, if needed, diﬀerent names for such
a class using rdfs:label annotations. Regarding polysemy, where the
same name has diﬀerent meanings, try to disambiguate the term, use
extension mechanisms and/or axioms. Other important cases regard-
ing class naming and identiﬁcation are (a) creating a class whose name
refers to two or more diﬀerent concepts by including “and” or “or”
in the name (e.g., StyleAndPeriod or ProductOrService) and (b) using

5.2.
Methods to improve an ontology’s quality
109
modality (“can”, “may”, “should”) in the ontology element’s names.
In situation (a) consider dividing the class into diﬀerent subclasses,
and in case (b) consider a more appropriate name avoiding the use of
modality or change to a logic language that can express it. Take care
about providing proper names for both the ontology ﬁle and the URI.
[KSFPV15]
The topic of pitfalls, anti-patterns, and modelling suggestions spills over into a
broader setting of ontology quality, which includes aspects such as accuracy, adapt-
ability, clarity, completeness, computational eﬃciency, conciseness, consistency/-
coherence and organisational ﬁtness, and domain and task-independent evaluation
methods that cover, among others, syntax, semantics, representation, and context
aspects. (see, e.g., [Vra09] for an early overview.)
5.2.5
Tools
There are many tools around that help you with one method or with a methodology.
Finding the right tool to solve the problem at hand (if it exists) is a skill of its own
and it is a necessary one to ﬁnd a feasible solution to the problem at hand. From
a technologies viewpoint, the more you know about the goals, features, strengths,
and weaknesses of available tools (and have the creativity to develop new ones, if
needed), the higher the likelihood you bring a potential solution of a problem to
successful completion.
Honesty requires me to admit that not all Semantic Web tools are being main-
tained and there is typically little documentation. In particular, plugins may falter
when they have been developed for one ODE but not another, or for a prior version
but aren’t compatible with a later version of the ODE. This short section it merely
intended to give you an idea that there are tools for a range of activities, and if the
one listed does not work anymore, then there is likely some open source code or at
least a paper describing what it does and how, so that one could re-implement it,
if needed6. The tools are grouped along ﬁve categories—to support methodologies,
ODEs, implementing methods, portals, and exports—and where to start when you
want to develop your own one.
Software-supported methodologies. They are few and far between. WebODE
provided software support for Methontology, the NeOn toolkit7 aims to sup-
port the NeON methodology for distributed development of ontologies.
Ontology Development Environments (ODEs). Clearly, the tools listed under
the ‘Software-supported methodologies’ are ODEs, but there are also ODEs that
are not tailored to a particular methodology. They mainly lack project manage-
ment features, and/or the possibility to switch back and forth between informal,
intermediate, and formal representations, or do not have features for activities such
6Inclusion in this section does not mean I have tested all of them and give a quality judgement
on it.
7http://neon-toolkit.org/

110
Chapter 5. Methods and Methodologies
as project documentation. It may well be the case that such functionality is avail-
able in part or in whole as a set of plug-ins to the ODE. Some of those ODEs are
stand-alone tools, such as Prot´eg´e desktop and Racer, others have a web interface,
such as WebProt´eg´e and the Modeling Wiki MOKI. The HOZO ontology editor is
the only editor that was speciﬁcally designed to explicitly accommodate for certain
ontological commitments, in particular regarding roles [MSKK07].
Most ODEs are packaged with one or more automated reasoners, but one also
can use another one, given that there is a plethora of ontology reasoners and edi-
tors8. This includes tools that have a pseudo-natural language interface or a graph-
ical interface to adding axioms to an ontology, which serves as ‘syntactic sugar’ to
the underlying logic.
Software-supported methods and other features. Additional features and imple-
mented methods may exist as stand-alone tool or as plugin for an ODE, or, thanks
to the widespread uptake, may have been integrated in the ODEs already upon
installation. For instance, Racer has extensive features for sophisticated querying
and OWL ontology visualisation with Ontograf is already included in the standard
installation of Prot´eg´e For the axiom tests component of test-driven development,
there is a TDDonto2 plugin for Prot´eg´e [KL16], and plugins for very speciﬁc tasks,
such as the DroolsTab for visual authoring of complex spatial process simulation,
and the CompGuide Editor for obtaining Computer-Interpretable Guidelines for
Clinical Practice Guidelines. There are many more Prot´eg´e plug-ins9, which are
sorted by topic (e.g., NLP, biomedical) and type (e.g., API, viewing), but do verify
the versioning of the plugins and the ODE before installation.
Some of the recent stand-alone tools focussed on improving the quality of the
ontology are the Possible World Explorer that helps with adding disjointness ax-
ioms [FR12, Fer16], the OntOlogy Pitfall Scanner (OOPS!) that implements an
automated check of the ontology with 21 common modelling pitfalls [PVSFGP12],
and OntoPartS to represent part-whole relations better [KFRMG12].
There are many more tools, such as for ontology alignment, converting one lan-
guage into another, tools for language extensions, and so on.
Portals.
Other tools that can make an ontology developer’s life easier, are
portals to more easily ﬁnd ontologies and search them, and easily obtain some
additional information. For instance, BioPortal [WNS+11] also lists an ontology’s
use, OntoHub [CKK+17] analyses the characteristics of the ontology and which
features have been used, and ROMULUS [KK16] and COLORE [GHH+12] zoom
in on advanced aspects of foundational ontologies (the topic of the next chapter).
Exporting ontologies. There are tools for exporting the knowledge represented
in the ontology and rendering it in another format for documentation purposes.
These include, notably, a conversion from OWL to latex so as to obtain the—to
some, more readable—DL notation of the ontology (see “save as” in Prot´eg´e, select
8https://www.w3.org/wiki/Ontology_editors and http://owl.cs.manchester.ac.uk/
tools/list-of-reasoners/
9https://protegewiki.stanford.edu/wiki/Protege_Plugin_Library

5.3.
Exercises
111
latex), and to automatically generate documentation alike software documentation,
like in LiveOWL, LODE, and its successor WIDOCO [Gar17].
Develop your own tool. There are many plugins and stand-alone tools. Still,
it may be that what you need doesn’t exist yet. To develop your own tool, be
it a standalone tool or as plugin, one does not have to start from scratch. For
applications that have to read in or write to OWL ﬁles: rather than declaring your
own regular expressions to ﬁnd things in an OWL ﬁle and declaring methods to
write into an OWL ﬁle, use the OWL API10, OWLink [LLNW11], or Apache Jena11
for Java-based applications and Owlready for Python-based applications [Lam17].
5.3
Exercises
Review question 5.1. List the main high-level tasks in a ‘waterfall’ ontology
development methodology.
Review question 5.2. Explain the diﬀerence between macro and micro level
development.
Review question 5.3. What is meant by ‘encoding peculiarities’ of an ontology?
Review question 5.4. Methods were grouped into four categories. Name them
and describe their diﬀerences.
Review question 5.5. Give two examples of types of modelling ﬂaws, i.e., that
are possible causes of undesirable deductions.
Review question 5.6. Ontology development methodologies have evolved over
the past 20 years. Compare the older Methontology with the newer NeON
methodology.
Exercise 5.1. Consider the following CQs and evaluate the AfricanWildlife-
Ontology1.owl against them. If these were the requirements for the content, is it
a ‘good’ ontology? *
1. Which animal eats which other animal?
2. Is a rockdassie a herbivore?
3. Which plant parts does a giraﬀe eat?
4. Does a lion eat plants or plant parts?
5. Is there an animal that does not drink water?
6. Which plants eat animals?
7. Which animals eat impalas?
8. Which animal(s) is(are) the predators of rockdassies?
9. Are there monkeys in South Africa?
10. Which country do I have to visit to see elephants?
11. Do giraﬀes and zebras live in the same habitat?
10https://github.com/owlcs/owlapi
11https://jena.apache.org/

112
Chapter 5. Methods and Methodologies
Exercise 5.2. Carry out at least subquestion a) and if you have started with, or
already completed, the practical assignment at the end of Block I, do also subques-
tion b).
a. Take the Pizza ontology pizza.owl, and submit it to the OOPS! portal.
Based on its output, what would you change in the ontology, if anything? *
b. Submit your ontology to OOPS! How does it fare? Do you agree with the
critical/non-critical categorisation by OOPS!? Would you change anything
based on the output, i.e.: does it assist you in the development of your
ontology toward a better quality ontology?
Exercise 5.3. There is some ontology O that contains the following expressions:
R ⊑PD × PD,
S ⊑PT × PT,
S ⊑R,
Trans(R),
PD ⊑PT,
ED ⊑PT,
ED ⊑¬PD,
A ⊑ED,
B ⊑ED,
C ⊑PD,
D ⊑PD,
A ⊑∃R.B,
D ⊑∃S.C.
Answer the following questions:
a. Is A consistent? Verify this with the reasoner and explain why. *
b. What would the output be when applying the RBox Compatibility service?
Is the knowledge represented ontologically ﬂawed?
Exercise 5.4. Apply the OntoClean rules to the ﬂawed ontology depicted in Fig-
ure 5.7, i.e., try to arrive at a ‘cleaned up’ version of the taxonomy by using the
rules. *
Note: the other properties are, in short:
- Identity: being able to recognise individual entities in the world as being
the same (or diﬀerent); Any property carrying an IC: +I (-I otherwise); Any
property supplying an IC: +O (-O otherwise) (“O” is a mnemonic for “own
identity”); +O implies +I and +R.
- Unity: being able to recognise all the parts that form an individual entity;
e.g., ocean carries unity (+U), legal agent carries no unity (-U), and amount
of water carries anti-unity (“not necessarily wholes”, ∼U)
- Identity criteria are the criteria we use to answer questions like, “is that my
dog?”
- Identity criteria are conditions used to determine equality (suﬃcient condi-
tions) and that are entailed by equality (necessary conditions)
With the rules:
• Given two properties, p and q, when q subsumes p the following constraints
hold:
– If q is anti-rigid, then p must be anti-rigid
– If q carries an IC, then p must carry the same IC
– If q carries a UC, then p must carry the same UC
– If q has anti-unity, then p must also have anti-unity
• Incompatible IC’s are disjoint, and Incompatible UC’s are disjoint
• And, in shorthand:
– +R ̸⊂∼R

5.4.
Literature and reference material
113
– −I ̸⊂+I
– −U ̸⊂+U
– +U ̸⊂∼U
– −D ̸⊂+D
Figure 5.7: An ‘unclean’ taxonomy. (Source: OntoClean teaching material by Guarino)
Exercise 5.5. The manual OntoClean exercise is somewhat laborious. A software-
supported OntoClean tutorial is included in Appendix A.1, where you will use OWL
ﬁles, an ODE (such as Prot´eg´e), and a DL reasoner.
Exercise 5.6. Pick a topic—such as pets, buildings, government—and step through
one of the methodologies to create an ontology. The point here is to try to apply a
methodology, not to develop an ontology, so even one or two CQs and one or two
axioms in the ontology will do.
Exercise 5.7. You may already know that impala is a type of antelope. Antelope
is not a species, however, but a so-called “wastebasket taxon”, i.e., a miscellaneous
group, which comprises 91 species that did not ﬁt under the rest of the categori-
sation in the Bovidae family (that also comprises cows and sheep). In a way, it is
an ‘other’ group. What do modelling guidelines, such as the TIPS [KSFPV15] and
GoodOD [SSRG+12], say about that for ontology development? How would you
address it for the AWO?
5.4
Literature and reference material
1. Mari Carmen Suarez-Figueroa, Guadalupe Aguado de Cea, Carlos Buil, Klaas
Dellschaft, Mariano Fernandez-Lopez, Andres Garcia, Asuncion Gomez-Perez,
German Herrero, Elena Montiel-Ponsoda, Marta Sabou, Boris Villazon-Terrazas,
and Zheng Yufei. NeOn Methodology for Building Contextualized Ontology
Networks. NeOn Deliverable D5.4.1. 2008.

114
Chapter 5. Methods and Methodologies
2. Alexander Garcia, Kieran ONeill, Leyla Jael Garcia, Phillip Lord, Robert
Stevens, Oscar Corcho, and Frank Gibson.
Developing ontologies within
decentralized settings. In H. Chen et al., editors, Semantic e-Science. Annals
of Information Systems 11, pages 99-139. Springer, 2010.
3. Fabian Neuhaus, Amanda Vizedom, Ken Baclawski, Mike Bennett, Mike
Dean, Michael Denny, Michael Gr¨uninger, Ali Hashemi, Terry Longstreth,
Leo Obrst, Steve Ray, Ram Sriram, Todd Schneider, Marcela Vegetti, Matthew
West, and Peter Yim. Towards ontology evaluation across the life cycle. Ap-
plied Ontology, 8(3):179-194, 2013.
4. Guarino, N. and Welty, C. An Overview of OntoClean. in S. Staab, R. Studer
(eds.), Handbook on Ontologies, Springer Verlag 2009, pp. 201-220.
5. C. Maria Keet. Preventing, detecting, and revising ﬂaws in object property
expressions. Journal on Data Semantics, 3(3):189-206, 2014.

CHAPTER 6
Top-down Ontology Development
Having an ontology language is one thing, but what to represent, and how, is quite
another. In the previous chapter, we looked at answering “Where do you start?”
and “How to proceed” with methodologies, but we are still left with answering: How
can you avoid reinventing the wheel? What can guide you to make the process easier
to carry it out successfully? How can you make the best of ‘legacy’ material? There
are two principal approaches, being the so-called top-down and bottom-up ontology
development approaches with their own set of methods, tools, and artefacts. In
this chapter, we focus on the former and in the next chapter on the latter, where
each can be seen as a reﬁnement of some aspects of an overall methodology like
introduced in Chapter 5.
We look at ‘avoiding to reinvent the wheel’ and ‘what can guide you to make the
process of adding those axioms easier’ by reusing some generic principles. Those
generic modelling aspects are typically represented in foundational ontologies, as-
sisted by further details on speciﬁc sub-topics. We will cover each in sequence:
foundational ontologies, also called top-level or upper ontologies, are introduced
in Section 6.1 and subsequently parthood and part-whole relations, as one of the
sub-topics, are introduced in Section 6.2. (The notion of ontology design patterns
partially could ﬁt here as well, but also partially as bottom-up or practice-oriented;
for the latter reason and chapter size considerations, it has been moved to the next
chapter.)
6.1
Foundational ontologies
The basic starting point for top-down ontology development is to consider several
core principles of Ontology for ontologies; or: some philosophical guidance for the
prospective engineering artefact1. Although we will not delve into deep debates
1As philosophy enters, a note about terminology may be in order, because some ideas are
borrowed and changed, and some terms that are the same do mean diﬀerent things in diﬀerent
115

116
Chapter 6. Top-down Ontology Development
about philosophical theories in this course, it is useful to know it has something
to oﬀer to the development of ontologies, and we will see several examples where
it has had inﬂuence. A few examples where results from philosophy can be useful
when deciding what is going to be represented in one’s ontology, and how, are the
following ones.
• One can commit to a 3-Dimensional view of the world with objects persisting
in time or take a(4-Dimensional (perdurantist) stance with space-time worms;
e.g., are you convinced that you after reading this sentence is a diﬀerent
you than you before reading this sentence? If so, then you may well be a
perdurantist, if you consider yourself to be the very same entity before and
after, then you lean toward the 3D, endurantist, commitment (but before
proclaiming to be one or the other based on this single example, do read up
on the details and the implications).
• The distinction between (in OWL terminology) classes and individuals: the
former can have instances, but the latter cannot be instantiated further; e.g.,
a class Chair can have instances, such as the one you are sitting on now, but
that chair cannot be instantiated further (it is already an individual object).
Generally, philosophers tend to agree on such a distinction, but one has to
decide whether one’s ontology is for individuals or for classes, or both.
• In the previous chapters, we have used terms like class and concept as they are
used in that speciﬁc ﬁeld. Philosophically, however, terms like class, concept,
universal, type, and category each have their very speciﬁc meaning and this
brings us back to comments in Section 1.2.2: concepts live in the mind/one’s
thoughts, whereas universals are out there in the world (if one is convinced
universals exist).
OWL and its reasoners are entirely agnostic about this
distinction, but the people who are reading, developing, and evaluating the
ontologies typically are not.
• Descriptivist vs. prescriptivist: should the ontology try to describe as best
as possible the subject domain, i.e., give an account of it, or should that
what is represented in the ontology prescribe how the world is, i.e., that the
entities in the ontology and constraints represented necessarily must hold,
and shown to hold, in reality? Conversely, if something is not represented in
the ontology, then a descriptivist may say it was unintentionally incomplete
whereas a prescriptivist may say not only that, but also, pedantically, argue
that if it’s not in the ontology, then it does not exist in reality.
disciplines. In the literature, you will come across material ontology and formal ontology. The
former (roughly) concerns making an ‘inventory’ of the things in the universe (we have the vase,
the clay, the apple, etc.), whereas the latter concerns laying bare the formal structure of (and
relation between) entities, which are assumed to have general features and obey some general
laws that hold across subject domains, like identity, constitution, and parthood (the latter will
be introduced in Section 6.2). So, in ontology engineering the ‘formal’ may refer to logic-based
but also to the usage of ‘formal’ in philosophy, which concerns the topic of investigation and does
not imply there is a formalisation of it in a logic language. In most computer science and IT
literature, when ‘formal’ is written, it generally refers to logic-based.

6.1.
Foundational ontologies
117
Then there more detailed decision to make, such as whether you are convinced
that there are entities that are not in space/time (i.e., that are abstract), whether
two entities can be co-located (the vase and the amount of clay it is made of), and
what it means that one entity is [dependent on/constituted by/part of/...] another.
There are more of such questions and decision to make. If you do not want to en-
tertain yourself with these questions, you can take someone else’s design decisions
and use that in ontology development. Someone else’s design decisions on Ontology
for a set of such questions typically is available in a foundational ontology, and
the diﬀerent answers to such questions end up as diﬀerent foundational ontologies.
Even with the same answers they may be diﬀerent2. The intricacies of, and philo-
sophical debates about, the more subtle details and diﬀerences are left to another
course, as here the focus is one why to use one, where, and how.
In the remainder of this section, we’ll ﬁrst have a look at typical content of
a foundational ontology (Section 6.1.1) and that there are multiple foundational
ontologies (Section 6.1.2) to subsequently proceed to the why, where, and how to
use them (Section 6.1.3).
6.1.1
Typical content of a foundational ontology
Foundational ontologies provide a high-level categorisation about the kinds of
things that will be represented in the ontology, such as process and physical ob-
ject, relations that are useful across subject domains, such as participation and
parthood, and (what are and) how to represent ‘attributes’ in a particular subject
domain, such as Colour and Height (recall Section 1.3), which can be done, e.g., as
quality or some kind of dependent continuant or trope. To make sense of this, let
us start with the two main ingredients: the ‘class’ taxonomy and the relationships.
Universals, categories, class hierarchy
Just like with other ontologies we have seen, also a foundational ontology repre-
sented in OWL has a hierarchy in the TBox. However, there are some diﬀerences
with a domain ontology or tutorial ontology such as the AWO and the Pizza on-
tology. The hierarchy in a foundational ontology does not contain subject domain
classes such as Boerewors and PizzaHawaii, but categories (or, loosely, ‘conceptual
containers’) of kinds of things. For instance, all instances of PizzaHawaii can be
considered to be physical objects, as are those sausages that are an instance of
Boerewors. If we assume there to be physical objects, then presumably, there can
also be entities that can be categorised as non-physical objects; e.g., the class (con-
cept/universal/...) Organisation, with instances such as the United Nations, fall in
the category of social object, which are a type of non-physical object. Non-physical
objects typically ‘inhere in’ physical objects, or physical objects are the ‘bearer’ of
the non-physical ones; e.g., being an instance of Student is a role you play3 where
the physical object is you as an instance of Human.
2see, e.g. beyond concepts [Smi04], the WonderWeb deliverable [MBG+03], and a synopsis of
the main design decisions for DOLCE [BM09]
3not ‘role’ as in DLs or ORM, but role in the common sense meaning.

118
Chapter 6. Top-down Ontology Development
Likewise, one can categorise kinds of processes. For instance, writing an exam
is something that unfolds in time and has various sub-activities, such as thinking,
writing, erasing pencil marks, and so on; taken together, writing an exam is an
accomplishment.
Contrast this with, say, an instance of Sitting: for the whole
duration you sit, each part of it is still an instance of sitting, which thereby may
be categorised as a state. None of the things mentioned in slanted font type in this
paragraph actually are speciﬁc entity types that one would encounter in an ontology
about subject domain entities only, yet we would want to be able to categorise the
kinds of things we represent in our domain ontology in a systematic way. It is these
and other categories that are represented in a foundational ontology.
The categories introduced with the examples above actually are from the De-
scriptive Ontology for Linguistic and Cognitive Engineering (DOLCE) foundational
ontology, and a screenshot of its hierarchy is shown in Figure 6.1-B. Behind this
simple taxonomy in the picture, is a comprehensive formalisation in ﬁrst order
predicate logic that was introduced in [MBG+03]. The taxonomy of the Basic For-
mal Ontology (BFO) v1 is shown in Figure 6.1-A, to illustrate that the DOLCE
categories and their hierarchical organisation are not the only way of structuring
such core entities. (How to deal with such variety will be addressed further below).
Being a pedantic ontologist, one could go as far as saying that if a category is not
in the foundational ontology, then its developers are of the opinion it does not exist
in reality. It is more likely that the ontology is incomplete in some way. There are
eﬀorts ongoing to harmonise the foundational ontologies better, to create a ‘core’
foundational ontology, and to standardise such a core. At the time of writing, this
is under construction.
Relations in foundational ontologies
In analogy to the ‘subject domain classes’ in domain ontologies versus categories
in foundational ontologies, one can identify generic relations/relationships/object
properties that are diﬀerent from those in domain ontologies. For instance, a do-
main ontology about universities may have a relation enrolled to relate Student to
Course, or in a sports ontology that a runner runs a marathon. These relations are
speciﬁc to the subject domain, but there are several that re-appear across domains,
or: they are subject domain-independent. Such subject domain-independent rela-
tions are represented in a foundational ontology. Notable core relations are part-
hood (which we shall look at in some detail in Section 6.2), participation of an
object in an event, constitution of an object (e.g., a vase) from an amount of mat-
ter (such as clay), and dependency when the existence of one entity depends on the
existence of another. The characterisation of such relations goes hand in hand with
the categories from a foundational ontology, so as to be precise rather than alluding
to ‘object’ or ‘event’ and assuming your and my intuition about what those things
really mean are the same. For instance, one thus could assert that, say, participates
in holds only between exactly a dolce:Endurant, which is an entity that is wholly
present at a time, and a dolce:Perdurant (an entity that unfolds in time).
It is a typical characteristic of foundational ontologies to have a set of relations
that are used heavily in axioms so as to constrain the possible models as much as

6.1.
Foundational ontologies
119
A. BFO taxonomy
B. DOLCE taxonomy
Figure 6.1: Screenshots of the OWLized BFO v1 and DOLCE taxonomies; for indicative
purpose: Perdurant ≈Occurrent, Endurant ≈IndependentContinuant.
one reasonably can.
Attributions
The third main component of representing knowledge the foundational ontology
way is attributions, as, just like in conceptual data modelling, ‘attributes’ have to
be represented somehow. There is a domain-speciﬁc component to it and there are
general, recurring, principles of attributions, and it is the latter that are captured
in a foundational ontology—to some extent at least. However, this is represented
quite diﬀerently from attributes you have modelled in UML or EER.
Let us ﬁrst revisit the ‘attribute’ Colour that we have come across in Sec-
tion 1.3.1 and Figure 1.5 when trying to integrate legacy systems.
One could
decide to make it a data property in OWL, declare its domain to be Rose and
choose the data type String, i.e., hasColour 7→Rose×String in ontology O1, or,
in OWL functional syntax style notation:
DataPropertyDomain(ex:hasColour ex:Rose)

120
Chapter 6. Top-down Ontology Development
Figure 6.2: DOLCE’s approach for qualities (‘attributes’) (Source: [MBG+03])
DataPropertyRange(ex:hasColour xsd:string)
That is, a binary relationship, which is the same approach as in UML class dia-
grams. Now, if another ontology developer decided to record the values in inte-
gers in O2, then the hasColour properties in O1 and O2 are incompatible in the
representation. Or consider the scenario where O1 = AWO.owl that has a data
property hasWeight for any object, including elephants, and its XML data type
set to integer. One can declare, e.g., Elephant ⊑=1 hasWeight.integer. Per-
haps a hasWeightPrecise with as data type real may be needed elsewhere; e.g., in
ontology O2 that will be used for monitoring all the animals in the zoo’s in your
country in, say, Europe. Implicitly, it was assumed by the developers that the
weight would be measured in kg. Then someone from the USA wants to use the
ontology, but wants to record the weight in lbs instead, which then amounts to
adding, say, hasWeightImperial or the developer has to fork the ontology with such
a data property, and so on, all about weights. Now the WWF wants to link both
zoo management systems across the pond and compare it with African wildlife.
What should the WWF IT specialists do? What is happening here is a replication
of the very same issues encountered in database integration. But this was precisely
what ontologies were supposed to solve! Copying the problem from information
systems into the ontologies arena (perhaps because you’re more familiar with that
way of modelling things), is not going to solve the interoperability problem. It
is still the case that there’s a sameness of conceptualisation and/or reality—like
Colour, and the meaning of Weight is all the same throughout as well. Thus, we
need something else.
The ﬁrst step toward resolving the issue is to realise that the choice of datatype
is an implementation decision, just like it is in database development from the
EER diagram to SQL schema. Even EER as conceptual data model already has
the underlying principle to be implementation-independent, so surely should the
ontology be, for it is typically expected to be even application-independent. In
short: if one indeed aims for the purposes of interoperability and reusability across
applications by means of an ontology, then don’t use data properties and data

6.1.
Foundational ontologies
121
types.
The next step then is: how to solve that problem? That other way of handing
attributions is the one typical of foundational ontologies and their respective OWL-
ized versions. The idea is to generalise (more precisely: reify) the attribute into a
class so that we can reuse the core notion that is the same throughout (Colour and
Weight in the examples), and this new entity is then related to the endurants and
perdurants on the one side and instead of datatypes, we use value regions on the
other side. Thus, an unfolding from one attribute/OWL data property into at least
two properties: there is one OWL object property from the endurant/perdurant
to the reiﬁed attribute—a quality property, represented as an OWL class—and a
second object property from quality to the value region. In this way, the shared
understanding can be shared, and any speciﬁcs on how one has to store the data
is relegated to the implementation, therewith solving the problem of the limited
reusability of attributes and preventing duplication of data properties. For instance,
Colour would be a subclass of Quality in DOLCE [MBG+03] and a Speciﬁcally
dependent continuant in BFO. An example of the approach taken in DOLCE is
depicted in Figure 6.2: rose1 is an instance of Rose, which is a subclass of Non-
Agentive Physical Object, and it is related by the qt relation to its colour property,
c1, which is an instance of the quality Colour that is a subclass of Physical Quality.
The actual value—the [measured] redness—of the colour of the rose at a given time
is a region red colour as instance of the Colour Region, which is a subclass of Physical
Region, and they are related by means of the qlt relation4.
The remaining step one may have to take is the case when one really has to
represent some values in the ontology itself, which is something that the founda-
tional ontologies are silent about. The least complicated and most reusable option
is to create a data property, say, hasDataValue with the Region class as domain
and XML data type anyType as range. This allows one to use the attributions
across ontologies and tools, yet leaves the ﬂexibility to the implementer to choose
the actual data type.
This concludes the brief idea of what is in a foundation ontology. As you will
have observed, there are several foundational ontologies, which may be confusing
or look like overcomplicating things, so we spend a few words on that now.
6.1.2
Several foundational ontologies
In this section, a selection of the foundation ontologies are summarised with respect
to their ontological commitments.
DOLCE
As the name suggests, the Descriptive Ontology for Linguistic and Cognitive En-
gineering (DOLCE) has a strong cognitive/linguistic bias. It takes a descriptive
4There are alternative theories in philosophy one can commit to whilst taking the unaries
approach to attributes, but this would be more suitable for an intermediate level of ontology
engineering.

122
Chapter 6. Top-down Ontology Development
(as opposite to prescriptive) attitude and the categories mirror cognition, common
sense, and the lexical structure of natural language. The emphasis is on cogni-
tive invariants and the categories are intended as ‘conceptual containers’ in the
sense that there are no deep metaphysical implications. Further, its documenta-
tion [MBG+03] focuses on design rationale so as to facilitate a comparison with
diﬀerent ontological options. It is rigorous, systematic, and has a rich axiomatisa-
tion. Concerning the size, it may look ‘small’ from an OWL ontologies viewpoint,
having 37 basic categories, 7 basic relations, 80 axioms, 100 deﬁnitions, and 20
theorems, but it is a rather dense ontology nonetheless. Besides the paper-based
version in [MBG+03], there are also several versions in OWL (Dolce-lite, Dolce-lite
Plus, ultralight), where some concessions have been made to force the formalisa-
tion into a less expressive language. Some more information and downloads of the
various versions are available5.
BFO and the RO
The Basic Formal Ontology (BFO)6 sees Ontology as reality representation. It
aims at reconciling the 3-dimensionalist and 4-dimensionalist views with a ‘Snap’
ontology of endurants, which is reproduced at each moment of time and is used
to characterise static views of the world, and a ‘Span’ ontology of happenings and
occurrents and, more generally, of entities which persist in time by perduring. It has
a limited granularity and is heavily inﬂuenced by parthood relations, boundaries,
and dependence.
Its version 1 is a bare taxonomy, i.e., there are no relations/object properties.
There is a separate Relation Ontology (RO) [SCK+05], which was developed to
assist ontology developers in avoiding errors in modelling and assist users in us-
ing the ontology for annotations, and such that several ontologies would use the
same set of agreed-upon deﬁned relations to foster interoperability among the on-
tologies. Philosophically, it is still a debate what then the ‘essential’ relations are
to represent reality, and if those included are good enough, are too many, or too
few. Several extensions to the RO are under consideration and reﬁnements have
been proposed, such as for RO’s transformation of [Kee09] that avails of theory
underlying OntoClean and derived from [Bro06].
Meanwhile, BFO v2.0 is richly annotated, the forked ROcore7 does use relevant
BFO classes for the domain and range of the incorporated RO relations, whereas
the forked draft release of BFO v2.1 (of 2014) takes yet another route where most
names of the relations suggest temporality and thus indicate a diﬀerent intended
meaning, yet OWL is atemporal.
There is also a BFO Core with mereological
theories8.
5http://www.loa.istc.cnr.it/old/DOLCE.html
6http://basic-formal-ontology.org/
7https://github.com/oborel/obo-relations/wiki/ROCore
8http://www.acsu.buffalo.edu/~bittner3/Theories/BFO/

6.1.
Foundational ontologies
123
GFO
The General Formal Ontology (GFO)9 [HH06] is a component of an integrated
system of foundational ontologies that has a three-layered meta-ontological archi-
tecture. The three layers are the abstract core level (ACO), the entities of the world
(ATO) that are exhaustively divided into categories and individuals, where individ-
uals instantiate categories, and among individuals, there is a distinction between
objects and attributives, and the basic level ontology that contains all relevant top-
level distinctions and categories. It has (3D) objects and (4D) processes, admitting
universals, concepts, and symbol structures and their interrelations. There are also
modules for functions and for roles, and s slimmed version GFO-basic.
Other foundational ontologies
There are several other foundational ontologies that did not receive their separate
paragraph in this version of the textbook as the aim is to have about 20 pages per
chapter. They include, in alphabetical order:
• GIST minimalist upper ontology [McC10];
• GUM, the Generalized Upper Model, driven by natural language [BMF95];
• SUMO, the Standard Upper Merged Ontology [NP01], which was an early
FO and has relatively very many classes and relations;
• UFO, the Uniﬁed Foundational Ontology [Gui05];
• YAMATO, the Yet Another More Advanced Top-level Ontology, which fo-
cuses on qualities and processes and events [Miz10].
To the best of my knowledge, only GIST, UFO, and YAMATO are being maintained
or extended at the time of writing.
On multiple foundational ontologies
The documentation of the foundational ontologies contain further details about
their formalisation and the rationale for having modelled it in the way they did;
e.g., that DOLCE takes a multiplicative approach, GFO lets one represent both
universals and individuals in the same ontology, BFO claims a realist approach,
and so on. Their properties have been structured and are included in the ONSET
tool that assists an ontologist with selecting a suitable foundational ontology for
one’s own domain ontology based on the selected requirements [KK12]. It saves the
user reading the foundational ontology literature to large extent, and all of those
new terms that have been introduced (like “multiplicative”) have brief informal
explanations. There will be an exercise about it at the end of the chapter.
One can wonder whether such foundational ontologies just use diﬀerent names
for the same kind of entities, but are essentially all the same anyway. Only very
few detailed comparisons have been made. If we ignore some intricate philosophical
aspects, such as whether universals and properties exist or not, then still only few
entity-by-entity alignments can be made, and even less mappings. An alignment
is a mapping only if asserting the alignment in the new ontology containing the
9http://www.onto-med.de/ontologies/gfo/

124
Chapter 6. Top-down Ontology Development
(foundational) ontologies does not lead to an inconsistency. Table 6.1 lists the com-
mon alignments among DOLCE, BFO, and GFO. More alignments and mappings
are described and discussed in [KK15a] and a searchable version is online in the
foundational ontology library ROMULUS [KK16].
Table 6.1: Common alignments between DOLCE-Lite, BFO and GFO; the ones num-
bered in bold can also be mapped. (Source: [KK13a])
DOLCE-Lite
BFORO
GFO
Class
1.
endurant
Independent
Continu-
ant
Presential
2.
physical-object
Object
Material object
3.
perdurant
Occurrent
Occurrent
4.
process
Process
Process
5.
quality
Quality
Property
6.
space-region
SpatialRegion
Spatial region
7.
temporal-region
Temporal-Region
Temporal region
Relational property
1.
proper-part
has proper part
has proper part
2.
proper-part-of
proper part of
proper part of
3.
participant
has participant
has participant
4.
participant-in
participates in
participates in
5.
generic-location
located in
occupies
6.
generic-location-of
location of
occupied by
In closing, observe that there are diﬀerent versions of each foundational on-
tology, not only diﬀerentiating between a formalisation on paper versus what is
representable in OWL, but also more and less detailed versions of an ontology.
The other main aspect from an engineering perspective, is to choose the most
suitable foundational ontology for the task at hand.
6.1.3
Using a foundational ontology
Having some idea of what a foundational ontology is, is one thing, but how to use
them is a diﬀerent story, and one that is not fully resolved yet. In this subsection,
we start ﬁrst with answering why one would want to use one at all, and some
examples where it helps a modeller in making modelling decisions for the overall
(domain) ontology. We then turn to some practical aspects, such as their ﬁles,
language used, and how (where) to link one’s domain entities to those generic
categories in a foundational ontology.

6.1.
Foundational ontologies
125
Why use a foundational ontology?
Foundational ontologies exist, but does that means one necessarily must use one?
Not everybody agrees on the answer. There are advantages and disadvantages to
it. The principal reasons for why it is beneﬁcial are:
• one does not have to ‘reinvent the wheel’ with respect to the basic categories
and relations to represent the subject domain,
• it improves overall quality of the ontology by using principled design decisions,
and
• it facilitates interoperability among ontologies that are aligned to the same
foundational ontology.
From the viewpoint of Ontology, a foundational ontology serves to clarify philo-
sophical details and be upfront about them, bring assumptions to the fore and
justify them, and, with that, it may become clear where there are any philosophi-
cal agreements and disagreements and what their underlying causes are.
A subset of domain ontology developers do not see a beneﬁt:
• they consider them too abstract, too expressive and comprehensive for the
envisioned ontology-driven information system, and
• it takes excessive eﬀort to understand them in suﬃcient detail such that it
would not weigh up to the beneﬁts.
A controlled experiment has been carried out with 52 novice ontology develop-
ers, which showed that, on average, using a foundational ontology resulted in an
ontology with more new classes and class axioms, and signiﬁcantly less new ad
hoc object properties than those who did not, there were no part-of vs. is-a mis-
takes, and, overall, “the ‘cost’ incurred spending time getting acquainted with a
foundational ontology compared to starting from scratch was more than made up
for in size, understandability, and interoperability already within the limited time
frame of the experiment” [Kee11b]. There is room for further experimentation, but
results thus far point clearly to a beneﬁt.
Modelling guidance: examples of some principal choices
An immediate practical beneﬁt is that Ontology and foundational ontologies help
preventing making novice ontology developer’s mistakes, such as confusing part-
hood with subsumption and class vs instance mix-ups. The former will become
clear in Section 6.2 (e.g., a province is part of a country, not a subclass). Regard-
ing the latter, ontologically, instances/individuals/particulars are, roughly, those
things that cannot be instantiated, whereas classes (or universals or concepts) can.
For instance, the chair you are sitting on is an instance whereas the class Chair
can be instantiated (the one you are sitting on is one such instance). Likewise,
MacBookPro is a type of laptop, which in an OWL ontology would be added as a
subclass of Laptop, not as an instance of Laptop—the MacBook I have with serial
number ♯123456 is an instance, and, likewise, GoldenDelicious is a subclass of Apple,

126
Chapter 6. Top-down Ontology Development
not an instance (the actual instances grow on the tree and are on the shelves in
the supermarket).
An example on choosing how to represent relations is described next.
Example 6.1. A relation, i.e., an n-ary with n > 1, can be represented as an
unary entity (a class in OWL) or as a n-ary relation (object property in OWL if
it is a binary). It is certainly more intuitive to keep the n-aries as such, because it
indicates a close correspondence with natural language. For instance, in formalising
“Person runs marathon”, it is tempting to represent “runs” as an object property
runs and assert, say, Marathon ⊑∃runs−.Person.
The foundational ontologies take a diﬀerent approach. Such perdurants, like
Running, and the verbs we use to label them, are included as an unary (OWL
class) suitably positioned as a subclass of processes, being Process in both DOLCE
and BFO. In DOLCE, it is then related with a new relation to ‘objects’, which
are suitably positioned subclasses of Endurant in such a way that an endurant
is a participant in a perdurant.
For instance, still with the TBox-level knowl-
edge that “Person runs marathon”, then Running (being a subclass of Process)
has participant some Person (i.e., Running ⊑∃has participant.Person) and another
binary to Marathon (e.g., Marathon ⊑∃involves.Running), but there is no 1-to-1
formalisation with an object property runs that has as domain and range Per-
son and Marathon. BFO (with the RO) aim to resolve this with dispositions: a
person may have the disposition to run (be it a marathon or more generally),
but not actually run (a marathon) in their entire life.
To resolve this in the
representation, we need not only Running ⊑Process, but also the deﬁnition that
RunningDisposition ≡Disposition ⊓∀hasRealization.Running)
and
assert
that
Person ⊑∃bearerOf.RunningDisposition before we get to the marathon.
The option with runs results in a more compact representation, is intuitively
closer to the domain expert’s understanding, and makes it easier to verbalise the
ontology, and therefore is likely to be more useful in praxis. The Running option is
more generic, and thereby likely to increase reusability of the ontology. No scientiﬁc
experiments have been conducted to test which way would be better to represent
such knowledge, and current mapping tools do not deal with such diﬀerences of
representing roughly the same knowledge in syntactically very diﬀerent ways. The-
oretical foundations for mappings between such distinct modelling styles have been
proposed [FK17], and this may be resolved soon.
Whichever way one chooses to represent such information, adhering to that
choice throughout the ontology makes the ontology easier to process and easier to
understand by the human reader.
♦
A longer and practical example and exercises with the African Wildlife Ontology
is included in the next section.
Practical aspects on using a foundational ontology
It was already mentioned that there are OWL-ized versions of several foundational
ontologies, but there is more to it. Once the most appropriate foundational ontology
is selected, the right version needs to be imported either in full or a module thereof,

6.1.
Foundational ontologies
127
and it has to be linked to the entities in your ontology. The latter means you will
have to ﬁnd out which category each of your entity is and which object properties
to use.
Some 15 years ago researchers already realised it might not be feasible to have
one singe foundational ontology that pleases everybody; hence, the idea emerged
to create a library of foundational ontologies with appropriate mappings between
them so that each modeller can choose her pet ontology and the system will sort
out the rest regarding the interoperability of ontologies that use diﬀerent founda-
tional ontologies. The basis for this has been laid with the Wonderweb deliverable
D18, but an implementation was yet to be done and new foundational ontology
developments have taken place since 2003. A ﬁrst step in the direction of such a
foundational ontology library has been laid recently with the Repository of On-
tology for MULtiple USes, ROMULUS [KK13b].
ROMULUS focuses on OWL
ontologies in particular.
The leaner OWL versions of DOLCE and BFO have been made available and
are intended to be used for development of ontologies in one’s domain of interest.
These ﬁles can be found on their respective websites (see earlier footnotes), which
also lists domain ontologies that use them. Observe that DOLCE-Lite is encoded
in the DL language that is characterised by SHI, BFO is simpler (in ALC); that is,
neither one uses all OWL-DL capabilities of SHOIN(D), let alone all OWL 2 DL
features. Recall that another diﬀerence is that BFO-in-owl is only a bare taxonomy
(extensions with the RO do exist; see Section 6.1.2), whereas DOLCE-Lite makes
heavy use of object properties.
To make reuse easier, ‘clever modules’ of foundational ontologies may be useful,
such as light/basic and full versions according to the developers’ taste, a separate
major branch of the ontology (e.g., using only Endurants), and a computationally
better behaved fragment with the best semantic approximation of the full version
(i.e., not merely dropping the violating axioms), such as an OWL 2 EL compliant
fragment of DOLCE. Some of those are also available from the aforementioned
ROMULUS and, by extension, the OntoHub ontology libraries.
Once the foundational ontology is imported (not leaded and extended), the task
is to ﬁnd the right classes to link one’s domain classes to, and likewise for the object
properties. The whole process is illustrated in the following example, starting with
a very basic African Wildlife Ontology, and gradually extending it and improving
its quality.
Example 6.2. Continuing with the African Wildlife Ontology from Example 4.1, a
ﬁrst step to improve its quality may be to add knowledge to ensure a better coverage
of the subject domain. Adding classes and object properties to an ontology does
not necessarily make a better quality ontology. One aspect that does with respect
to the subject domain, is to reﬁne the represented knowledge further and with more
constraints so as to limit the possible models; e.g.: 1) giraﬀes eat not only leaves
but also twigs, 2) they are disjoint from impalas, and 3) more object property
characteristics, e.g., that the is-part-of is not only transitive, but also reﬂexive, and
is-proper-part-of is transitive and irreﬂexive or asymmetric (recall that the latter
can be added thanks to the increased expressiveness of OWL 2 DL compared to

128
Chapter 6. Top-down Ontology Development
OWL-DL, but not both irreﬂexivity and asymmetry).
Third, we can improve the ontology’s quality by using a foundational ontol-
ogy, as mentioned in Section 6.1.3; e.g., one of DOLCE, BFO, GFO, SUMO, and
YAMATO that were introduced in Section 6.1.1 and all happen to have OWLized
version of them.
For the sake of example, let us take DOLCE to enrich the African Wildlife
Ontology. To do this, we need to import into the AWO an OWLized version of
DOLCE; in this case, this means importing DOLCE-lite.owl. Then, consider ﬁrst
the taxonomic component of DOLCE in Figure 6.1-B (for details, see Wonderweb
deliverable D18 Fig 2 p14 and Table 1 p15 or explore the imported ontology with
its annotations).
1. Where does Plant ﬁt in in the DOLCE categorisation?
2. Giraﬀes drink water: where should we put Water?
3. Impalas run (fast); where should we put Running?
4. Lions eat impalas, and in the process, the impalas die; where should we put
Death?
To answer such questions, we have to look at the principal distinctions made in
DOLCE among its categories. Let us take Plant: is Plant wholly presents during
its existence (enduring), or is it happening in time (perduring)? With a 3D versus
4D worldview, the former applies. Within endurants, we look at its subclasses,
which are Arbitrary Sum, Physical Endurant, and Non-Physical Endurant: a plant
is certainly not some arbitrary collection of things, like the set of this lecture
notes and your pencil are, and a plant takes up physical space, so one chooses
Physical Endurant. We repeat this for the subclasses of Physical Endurant, which
are Feature, Amount of Matter, and Physical Object. A feature (in DOLCE) is
something like a bump in the road or the hole in a swiss cheese, hence quite distinct
from Plant (but a plant can have such things). Amount of matter is in natural
language normally denoted with a mass noun, such as gold and water, and it can be
counted only in quantities (a litre of water); however, plants can be counted, so they
are physical objects and, hence, we can add AWO:Plant ⊑dolce:PhysicalObject to
the ontology. One can ﬁnd the alignments for the other ones in a similar step-wise
way, which may be assisted by the decision diagram in Figure 6.3. The answers
can be found in AfricanWildlifeOntology2a.owl.
DOLCE is more than a taxonomy, and we can also inspect in more detail its
object properties and reuse the properties already deﬁned instead of re-inventing
them. First, the African Wildlife Ontology’s is-part-of is the same as DOLCE’s part-
of, and likewise for their respective inverses, so declare them equivalent. Concerning
the subject domain, here are a few modelling questions.
1. The Elephant’s Tusks (ivory) are made of Apatite (calcium phosphate, an
amount of matter); which DOLCE relation can be reused?
2. Giraﬀes eat leaves and twigs; how do Plant and Twig relate?
3. How would you represent the Size (Height, Weight, etc.) of an average adult
elephant; with DOLCE’s Quality or an OWL data property?
Answers to the ﬁrst two questions are included in AfricanWildlifeOntology2a.owl.
Note ﬁrst that AWO:Tusk ⊑dolce:PhysicalObject and AWO:Apatite ⊑dolce:Amount-
OfMatter, so we need to ﬁnd an object property that has as domain a physical object

6.1.
Foundational ontologies
129
For instance:  Eating, 
Water, Lion, Height
Continue answering the questions until you arrive at a leaf in the decision tree
Eating
Water
Lion
Height
Figure 6.3:
The Decision Tree of D3 with a few examples; a single branch can be
selected at a time. (Source: based on [KKG13])
and as range an amount of matter; at present, the easiest way to ﬁnd out, is to run
it through the OntoPartS tool [KFRMG12], which returns the constitution rela-
tion as the only one that ﬁts these constraints. OntoPartS’s constitution is more
restrictive than DOLCE’s, so one can either 1) use dolce:generic-constituent that re-
lates perdurants or endurants or 2) add AWO:constituted-of with domain and range
dolce:PhysicalObject and range dolce:AmountOfMatter and add AWO:constituted-of
⊑dolce:generic-constituent, and then assert Tusk ⊑∃constituted-of.Apatite in the
ontology. Option 1 has the beneﬁt of direct reuse of a relation from DOLCE in-
stead of inventing one’s own from scratch, whereas option 2 is more restrictive and
precise, thereby also improving the ontology’s quality.
How does it work out when we import the OWL version of BFO v2.0 into
AfricanWildlifeOntology1.owl?
Aside from minor diﬀerences—e.g., Death is
not a type of Achievement as in DOLCE, but a ProcessBoundary instead, and
animals and plants are subtypes of Object, see also Figure 6.1-A—there is a ma-
jor diﬀerence with respect to the object properties (BFO has none). A possible
outcome of linking the same entities of the wildlife ontology to BFO is included in
AfricanWildlifeOntology3a.owl. To do these last two exercises with DOLCE
and BFO in a transparent and reusable way, a mapping between the two founda-
tional ontologies is needed. Even more so: with a mapping, only one of the two
exercises would have suﬃced and software would have taken care of the mappings
between the two. ROMULUS has both mapping and a solid method and tool to
‘swap’ a foundational ontology [KK14].
One could take the development a step further by adding types of part-whole
relations so as to be more precise than only a generic part-of relation: e.g., Root is
a structural part of some Plant and NatureReserve is located-in some Country, which
will be discussed in some detail in the next section. Another option is to consider

130
Chapter 6. Top-down Ontology Development
a Content Ontology Design Pattern10, such as being more ﬁnicky about names for
plants and animals with, perhaps, a Linnaean Taxonomy content pattern or adding
some information on Climatic Zones where the plants and animals live, and so on11.
Such patterns are the topic of Section 7.6. ♦
You may like to inspect a real ontology that is linked to DOLCE as well. There
are multiple examples, such as BioTop [BSSH08] that is linked to both DOLCE
and BFO-RO, and the Data Mining Optimization Ontology we have come across
in Section 1.3.2 [KLd+15]. A selection of the links is depicted in Figure 6.4.
Methods and supporting tools are being developed that are informed by foun-
dational ontologies or provide actual support using them, e.g., [HOD+10, KK12,
KKG13, KK16, Hep11], but more can be done to assist the modeller in the ontology
authoring process involving foundational ontologies.
DM-Data
dolce:non-physical-endurant
dolce:abstract
DataType
DataFormat
dolce:quality
dolce:region
dolce:abstract-region
dolce:quale
dolce:abstract-quality
Characteristic Parameter
dolce:particular
dolce:process
DM-Experiment
DM-Operation
DM-Algorithm
DM-Task
NeighborhoodRange
OpParameterSetting
....
....
Figure 6.4: Selection of DMOP classes linked to DOLCE.
6.2
Part-whole relations
A, if not the, essential relation in Ontology and ontologies is the part-whole rela-
tion, which is deemed as essential as subsumption by the most active adopters of
ontologies—i.e., bio- and medical scientists—while its full potential is yet to be dis-
covered by, among others, manufacturing to manage components of devices. Let’s
start with a few modelling questions to get an idea of the direction we are heading
at:
– Is City a subclass of or a part of Province?
– Is a tunnel part of the mountain? If so, is it a ‘part’ in the same way as the
sand of your sandcastle on the beach?
– What is the diﬀerence, if any, between how Cell nucleus and Cell are related
and how Cell Receptor and Cell wall are related? Or between the circuit on
the ethernet card embedded on the motherboard and the motherboard in the
computer?
10http://www.ontologydesignpatterns.org/
11But note that regarding content, one also can take a bottom-up approach to ontology devel-
opment with resources such as the Environment Ontology (http://www.environmentontology.
com/) or pick and choose from ‘semantiﬁed’ Biodiversity Information Standards (http://www.
tdwg.org/) etc. Bottom-up approaches are the topic of the next chapter.

6.2.
Part-whole relations
131
– Assuming boxers must have their own hands and boxers are humans, is Hand
part of Boxer in the same way as Brain is part of Human?
– Consider that “Hand is part of Musician” and “Musician part of Orchestra”.
Clearly, the musician’s hands are not part of the orchestra. Is part-of then
not transitive, or is there a problem with the example?
To shed light on part-whole relations in its broadest sense and sort out such mod-
elling problems, we will look ﬁrst at mereology, which is the Ontology take on
part-whole relations, and to a lesser extent meronymy, which is more popular in
linguistics. Subsequently, the diﬀerent terms that are perceived to have something
to do with part-whole relations are structured into a taxonomy of part-whole rela-
tions, based on [KA08], which has been adopted elsewhere, such as in NLP.
6.2.1
Mereology
The most ‘simple’ mereological theory is commonly considered to be Ground Mere-
ology. We take the one where parthood is primitive12, i.e., part-of is not deﬁned
but only characterised with some properties. In particular, the three characteris-
ing properties are that parthood is reﬂexive (everything is part of itself, Eq. 6.1),
antisymmetric (two distinct things cannot be part of each other, or: if they are,
then they are the same thing, Eq. 6.2), and transitive (if x is part of y and y is
part of z, then x is part of z, Eq. 6.3):
∀x(part of(x, x))
(6.1)
∀x, y((part of(x, y) ∧part of(y, x)) →x = y)
(6.2)
∀x, y, z((part of(x, y) ∧part of(y, z)) →part of(x, z))
(6.3)
With parthood, on can deﬁne proper parthood:
∀x, y(proper part of(x, y) ≡part of(x, y) ∧¬part of(y, x))
(6.4)
and its characteristics are that it is transitive (Eq. 6.5), asymmetric (if x is part of
y then y is not part of x, Eq. 6.6) and irreﬂexive (x is not part of itself, Eq. 6.7).
Irreﬂexivity follows from the deﬁnition of proper parthood and then, together with
antisymmetry, one can prove asymmetry of proper parthood (proofs omitted).
∀x, y, z((proper part of(x, y) ∧proper part of(y, z)) →proper part of(x, z))
(6.5)
∀x, y(proper part of(x, y) →¬proper part of(y, x))
(6.6)
∀x¬(proper part of(x, x))
(6.7)
These basic axioms already enable us to deﬁne several other common relations.
Notably, overlap (x and y share a piece z):
∀x, y(overlap(x, y) ≡∃z(part of(z, x) ∧part of(z, y)))
(6.8)
12one also can take proper parthood as primitive and deﬁne parthood in terms of it [Var04],
and one can argue about including other things (see below for some examples) or remove some
(see, e.g., [Cot10])

132
Chapter 6. Top-down Ontology Development
and underlap (x and y are both part of some z):
∀x, y(underlap(x, y) ≡∃z(part of(x, z) ∧part of(y, z)))
(6.9)
The respective deﬁnitions of proper overlap & proper underlap are similar.
But there are ‘gaps’ in Ground Mereology, some would say; put diﬀerently:
there’s more to parthood than this. For instance: what to do—if anything—with
the ‘remainder’ that makes up the whole? There are two options:
• Weak supplementation: every proper part must be supplemented by another,
disjoint, part, resulting in Minimal Mereology (MM).
• Strong supplementation: if an object fails to include another among its parts,
then there must be a remainder, resulting in Extensional Mereology (EM).
There is a problem with EM, however: non-atomic objects with the same proper
parts are identical (extensionality principle), but sameness of parts may not be
suﬃcient for identity. For instance, two objects can be distinct purely based on
arrangement of its parts, like there is a diﬀerence between statue and its marble and
between several ﬂowers bound together and a bouquet of ﬂowers. This is addressed
in General Extensional Mereology (GEM); see also Figure 6.5.
One can wonder about parts some more: does it go on inﬁnitely down to even
smaller than the smallest, or must it stop at some point? If one is convinced it stops
with a smallest part, this means a ‘basic element’ exists, which is called Atom in
mereology. The alternative—going on inﬁnitely down into parts of parts—is that
at the very basis there is so-called atomless ‘gunk’. These diﬀerent commitments
generate additional mereological theories.
If that is not enough for extensions:
one could, e.g., temporalise each mereological theory, so that one can assert that
something used to be part of something else; this solves the boxer, hand, and brain
example mentioned in the introduction (we’ll look at the solution in Section 10.2.2).
Another option is to also consider space or topology, which should solve the tun-
nel/mountain question, above; see also, e.g., [Var07]. These extensions do not yet
solve the cell and the musician questions. This will be addressed in the next section.
6.2.2
Modelling and reasoning in the context of ontologies
Mereology is not enough for ontology engineering. This is partially due to the ‘spill-
over’ from conceptual data modelling and cognitive science, where a whole range of
relations are sometimes referred to as a parthood relation, but which are not upon
closer inspection. In addition, if one has only part-of in one’s ontology with no
domain or range axiom, the reasoner will not complain when one adds, say, Hand⊑
∃part-of.Musician and Musician⊑∃part-of.Performance, even though ontologically
this is not quite right. A philosopher might say “yeah, well, then don’t do this!”, but
it would be more useful for an ontology developer to have relations at one’s disposal
that are more precise, both for avoiding modelling mistakes and for increasing
precision to obtain a better quality ontology.
This issue has been investigated by relatively many researchers. We shall take
a closer look at a taxonomy of part-whole relations [KA08] that combines, extends,
and formalises them.
The basic version of the informal graphical rendering is
depicted in Figure 6.6.

6.2.
Part-whole relations
133
Ground Mereology 
M
Minimal Mereology 
MM
Extensional Mereology 
EM
Closure Mereology 
CM
Extensional Closure Mereology 
CEM = CMM
General Mereology 
GM
General Extensional Mereology 
GEM = GMM
Fig. 1: Hasse diagram of mereological theories; from
weaker to stronger, going uphill (after [44]).
Figure 6.5: Hasse diagram of mereological theories; from weaker to stronger, going
uphill (after [Var04]). Atomicity can be added to each one.
Part-whole relation
parthood
[mereology]
s-parthood
(objects)
spatial 
parthood
involvement
(processes)
stuﬀ part
(diﬀerent stuﬀs)
portion
(same stuﬀ)
location
(2D objects)
containment
(3D objects)
membership
(object/role-
collective)
constitution
(stuﬀ-object)
participation
(object-process)
mpart
[in discourse only]
Figure 6.6: Taxonomy of basic mereological (left-hand branch) and meronymic (right-
hand branch) part-whole relations, with an informal summary of how the relations are
constrained by their domain and range; s-parthood = structural parthood.
(Source:
based on [KA08])
The relations have been formalised in [KA08]. It uses DOLCE in order to be
precise in the domain and range axioms; one could have taken another founda-
tional ontology, but at the time it was a reasonable choice (for an assessment of
alternatives, see [Kee17a]). The more precise characterisations (cf. the ﬁgure) and
some illustrative examples are as follows.
• involvement for processes and sub-processes; e.g. Chewing (a pedurant, PD)
is involved in the grander process of Eating (also a perdurant), or vv.:
∀x, y(involved in(x, y) ≡part of(x, y) ∧PD(x) ∧PD(y))
(6.10)
• containment and location for object and its 2D or 3D region; e.g., contained-
in(John’s address book, John’s bag) and located in(Tshwane, South Africa).
They are formalised as Eqs. 6.11 and 6.12, respectively, where has 2D and
has 3D are shorthand relations standing for DOLCE’s qualities and qualia:
∀x, y(contained in(x, y) ≡part of(x, y) ∧R(x) ∧R(y)∧
∃z, w(has 3D(z, x) ∧has 3D(w, y) ∧ED(z) ∧ED(w)))
(6.11)

134
Chapter 6. Top-down Ontology Development
∀x, y(located in(x, y) ≡part of(x, y) ∧R(x) ∧R(y)∧
∃z, w(has 2D(z, x) ∧has 2D(w, y) ∧ED(z) ∧ED(w)))
(6.12)
Observe that the domain and range is Region (R), which has an object oc-
cupying it, i.e., this does not imply that those objects are related also by
structural parthood. Also, the 2D vs 3D distinction is not strictly necessary,
but prior research showed that modellers like to make that diﬀerence explicit.
• structural parthood between endurants (ED) speciﬁcally:
∀x, y(s part of(x, y) ≡part of(x, y) ∧ED(x) ∧ED(y))
(6.13)
Practically, this is probably better constrained by PED, physical endurant,
such as a wall being a structural part of a house.
• stuﬀpart or “quantity-mass”, e.g., Salt as a stuﬀpart of SeaWater relating
diﬀerent types of amounts of matter (M) or stuﬀs, which are typically indi-
cated with mass nouns and cannot be counted other than in quantities. A
partial formalisation is as follows (there is a more elaborate one [Kee16]):
∀x, y(stuﬀpart(x, y) ≡part of(x, y) ∧M(x) ∧M(y))
(6.14)
• portion, elsewhere also called “portion-object”, relating a smaller (or sub)
part of an amount of matter to the whole, where both are of the same type
of stuﬀ; e.g., the wine in the glass of wine & wine in the bottle of wine. A
partial formalisation is as follows (there is a more elaborate one [Kee16]):
∀x, y(portion of(x, y) ≡part of(x, y) ∧M(x) ∧M(y))
(6.15)
• membership for so-called “member-bunch”: collective nouns (e.g., Herd, Or-
chestra) with their members (Sheep, Musician, respectively), where the sub-
script “n” denotes non-transitive and POB physical object and SOB social
object:
∀x, y(member ofn(x, y) ≡mpart of(x, y) ∧(POB(x) ∨SOB(x))
∧SOB(y))
(6.16)
That is, sometimes transitivity might hold in a chain of memberships, but as
soon as POB and SOB are mixed, that stops working, like with the hand in
the example at the start of the section, for it is a POB.
• participation where an entity participates in a process (also called “noun-
feature/ activity”), like Enzyme that participates in CatalyticReaction or a
Musician participating in a Performance, where the subscript “it” denotes
intransitive:
∀x, y(participates init(x, y) ≡mpart of(x, y) ∧ED(x) ∧PD(y))
(6.17)
From this deﬁnition, it becomes obvious why a ‘musician is part of a perfor-
mance’ does not work: the domain and range are disjoint categories, so they
never can line up in a transitivity chain.

6.2.
Part-whole relations
135
• constitution or “material-object”, to relate that what something is made of
to the object, such as the Vase and the (amount of) Clay it is constituted of,
where the subscript “it” denotes intransitive:
∀x, y(constitutesit(x, y) ≡constituted ofit(y, x) ≡mpart of(x, y)∧
POB(y) ∧M(x))
(6.18)
This can be put to use with manual or software-supported guidelines, such as
OntoPartS [KFRMG12], to choose the most appropriate part-whole relation for
the modelling problem at hand. Several OWL ﬁles with taxonomies of part-whole
relations, including aligned to other foundational ontologies are also available13.
Note that the mereological theories from philosophy are, as of yet, not feasible to
implement in OWL: there is no DL that actually allows one to represent all of even
the most basic mereological theory (Ground Mereology), as shown in Table 6.2,
let alone add deﬁnitions for relations. This is possible within the DOL framework
(recall Section 4.3.2). More precisely with respect to the table’s languages beyond
OWL: DLRµ is a peculiar DL [CDGL99] and HOL stands for higher order logic
(like, second order, beyond ﬁrst order). Acyclicity means that an object x does
not have a path to itself through one or more relations R on which acyclicity is
declared. The reason why acyclicity is included in the table is that one actually
can prove acyclicity with the axioms of proper parthood. It needs second order
logic, though; formally, acyclicity is ∀x(¬ϕ(x, x)) where ϕ ranges over one or more
relations (of proper parthood, in this case).
Table 6.2: Properties of parthood (.P ) and proper parthood (.PP ) in Ground Mereology
and their inclusion in the OWL family, FOL, DLRµ, and HOL.
Language ⇒
DL
Lite
2DL
2QL 2RL
2EL
DLRµ
FOL
HOL
Feature ⇓
ReﬂexivityP
–
–
+
+
–
+
+
+
+
AntisymmetryP
–
–
–
–
–
–
–
+
+
TransitivityP,PP
+
+
+
–
+
+
+
+
+
AsymmetryPP
–
–
+
+
+
–
+
+
+
IrreﬂexivityPP
–
–
+
+
+
–
+
+
+
Acyclicity
–
–
–
–
–
–
+
–
+
Notwithstanding this, what sort of things can be derived with the part-whole
relations, and what use may it have? The following example provides a few of the
myriad of illustrations.
Example 6.3. Informally, e.g., when it is possible to deduce which part of the
device is broken, then only that part has to be replaced instead of the whole it is
part of (saving a company money), and one may want to deduce that when a soccer
player has injured her ankle, she has an injury in her limb, but not deduce that
13http://www.meteck.org/swdsont.html

136
Chapter 6. Top-down Ontology Development
if she has an amputation of her toe, she also has an amputation of her foot that
the toe is (well, was) part of. If a toddler swallowed a Lego brick, it is spatially
contained in his stomach, but one does not deduce it is structurally part of his
stomach (normally it will leave the body unchanged through the usual channel). A
consequence of asserting reﬂexivity of parthood in the ontology is that then for a
domain axiom like Twig ⊑∃s-part-of.Plant, one deduces that each Twig is a part-of
some Twig as well, which is an uninteresting deduction, and, in fact, points to a
defect: it should have been asserted to be a proper part—which is irreﬂexive—of
Plant.
♦
A separate issue that the solution proposed in [KA08] brought afore, is that it
requires one to declare the taxonomy of relations correctly. This can be done by
availing of the RBox Compatibility service that we have seen in Section 5.2.3. While
the part-whole taxonomy, the RBox Compatibility service, and the OntoPartS
tool’s functionalities do not solve all modelling problems of part-whole relations,
at least they provide an ontologist with a sound basis and some guidelines.
As noted before, various extensions to mereology are being investigated, such
as mereotopology and mereogeometry, the notion of essential parthood, and por-
tions and stuﬀs. For mereotopology, the interested reader may want to consult,
among others, ontological foundations [Var07] and its applicability and modelling
aspects in the Semantic Web setting with OWL ontologies [KFRMG12] and DOL
[KK17b], the introduction of the RCC8 spatial relations [RCC92], and exploration
toward integrating RCC8 with OWL [GBM07, SS09]. Useful starting points for
portions and stuﬀparts from the viewpoint of ontology and formalisations are
[BD07, DB09, Kee16].
Other foundational ontology aspects, such as philosophy of language, modal
logic, change in time, properties, the ontology of relations, and dependence, will not
be addressed in this course. The free online Stanford Encyclopedia of Philosophy14
contains comprehensive, entry-level readable, overviews of such foundational issues.
6.3
Exercises
Review question 6.1. Why would one want to at least consider using a founda-
tional ontology in ontology development?
Review question 6.2. Name at least three fundamental ontological design de-
cisions that aﬀect how a foundational ontology will look like with respect to its
contents.
Review question 6.3. What are the major diﬀerences between DOLCE and BFO
in terms of philosophical approach? *
Review question 6.4. What is the major diﬀerence between DOLCE and BFO
in type of contents of the ontologies? *
14http://plato.stanford.edu/

6.3.
Exercises
137
Review question 6.5. Name at least 2 common relations—in terms of deﬁnition
or description and intention—in the OWLized DOLCE, GFO and RO. *
Review question 6.6. Why can one not represent Ground Mereology fully in
OWL 2 DL?
Review question 6.7. Which part-whole relation is appropriate to relate the
following entities?
1. Plant and Twig;
2. Tusk/Ivory and Apatite;
3. Musician and Performance;
4. Musician and Orchestra
Exercise 6.1. Content comparison:
a. Try to match the DOLCE classes Endurant, Process, Quality, Amount of
Matter, Accomplishment, Spatial Region, Agentive Physical Object, and Set
to a class in BFO. *
b. If you cannot ﬁnd a (near) equivalence, perhaps as a subclass-of some BFO
class? And if not even that, why do you think that (those) class(es) is (are)
not mappable? *
Exercise 6.2. Assume you are asked to develop an ontology about
a. Sociological and organisational aspects of public administration
b. The physiology and chemistry of medicinal plants
c. A topic of your choice
Which (if any) foundational ontology would you choose for each one? Why? *
Exercise 6.3. Download ONSET from http://www.meteck.org/files/onset/
and re-do Exercise 6.2, but now use the ONSET tool to obtain an answer. Does it
make any diﬀerence? Were your reasons for choosing a foundational ontology the
same as ONSET’s?
Exercise 6.4. Consider the following scenario.
Both before and since the 2008 recession hit, banks have been merging
and buying up other banks, which have yet to integrate their IT sys-
tems within each of the consolidated banks, and meet new regulations
on transparency of business operations. To achieve that, you are tasked
with developing an ontology of banks that will facilitate the database
integration and transparency requirements. In such an ontology there
will be concrete entities e.g., Bank manager and ATM, and abstract en-
tities e.g., Loans. For this to be possible, the ontological assumptions
that are made by the ontology must be based on human common-sense.
Processes, such as withdrawals and deposits must also be modelled. It
must be possible to capture dates and times for operations that occur
between entities and processes. Past and present transactions must be
allowed in the ontology. Entities of the ontology may have properties
and values associated with them e.g., an individual has a credit rat-
ing.
It may be useful to refer to or possibly use components of an

138
Chapter 6. Top-down Ontology Development
ontology that implements a particular mereology theory such as classi-
cal extensional mereology (CEM) or any other. This ontology must be
represented in OWL 2 DL.
Which (if any) foundational ontology would you choose? Why? *
Exercise 6.5. Consider the D3 decision diagram and answer the ﬁrst four questions
of Example 6.2.
Exercise 6.6. Download either AfricanWildlifeOntology2.owl (with DOLCE)
or AfricanWildlifeOntology3.owl (with BFO), open it in the ontology develop-
ment environment of choice, and inspect its contents. Modify the African Wildlife
Ontology such that it contains, in some way, the following:
a. Add enough knowledge so that RockDassie will be classiﬁed automatically as
a subclass of Herbivore. *
b. Add information that captures that lions, impalas, and monkeys reside in
nature reserves that are located in a country (like Kenya, well-known for
safaris), and that monkeys can also be found on some university campuses in
residential areas. *
c. Rangers of nature reserves are Humans (or: it’s a role that a human can
perform). *
Was there anything of use from DOLCE/BFO to assist with that?
Exercise 6.7. Consider the dispositions and realisations included in BFO v2 and
the RunningDisposition in Example 6.1. How would this approach aﬀect the AWO
regarding the eats object property (and, optionally: how should the Running class
of Example 6.2 be handled (as part of the ‘impalas that run fast’))? Discuss and
write the new axioms. *
6.4
Literature and reference material
1. Masolo, C., Borgo, S., Gangemi, A., Guarino, N., Oltramari, A.: WonderWeb
Deliverable D18–Ontology library. WonderWeb. http://wonderweb.man.
ac.uk/deliverables/documents/D18.pdf (2003).
2. Keet, C.M. and Artale, A. Representing and Reasoning over a Taxonomy of
Part-Whole Relations. Applied Ontology, IOS Press, 2008, 3(1-2): 91-110.

CHAPTER 7
Bottom-up Ontology Development
Besides a top-down approach, another option to developing an ontology without
starting with a blank slate, is to reuse exsiting data, information, or knowledge. A
motivation to consider this are the results obtained by Simperl et al [SMB10]: they
surveyed 148 ontology development projects, which showed that “domain analysis
was shown to have the highest impact on the total eﬀort” of ontology development,
“tool support for this activity was very poor”, and the “participants shared the
view that process guidelines tailored for [specialised domains or in projects relying
on end-user contributions] are essential for the success of ontology engineering
projects”. In other words: the knowledge acquisition bottleneck is still an issue.
Methods and tools have been, and are being, developed to make it less hard to
get the subject domain knowledge out of the experts and into the ontology, e.g.,
through natural language interfaces and diagrams, and to make it less taxing on
the domain experts by reusing the ‘legacy’ material they already may have to
manage their information and knowledge. It is the latter we are going to look at in
this chapter: bottom-up ontology development to get the subject domain knowledge
represented in the ontology. We approach it from the other end of the spectrum
compared to what we have seen in Chapter 6, being starting from more or less
reusable non-ontological sources and try to develop an ontology from that.
Techniques to carry out bottom-up ontology development range from manual
to (almost) fully automated. They diﬀer according to their focus:
• Ontology learning to populate the TBox, where the strategies can be subdi-
vided into:
– transforming information or knowledge represented in one logic language
into an OWL species;
– transforming somewhat structured information into an OWL species;
– starting at the base.
• Ontology learning to populate the ABox.
139

140
Chapter 7. Bottom-up Ontology Development
The latter is carried out typically by either natural language processing (NLP) or
one or more data mining or machine learning techniques. In the remainder of this
chapter, however, we shall focus primarily on populating the TBox. Practically,
this means taking some ‘legacy’ material (i.e., not-Semantic Web and, mostly,
not-ontology) and convert it into an OWL ﬁle with some manual pre- and/or post-
processing. Input artefacts may be, but are not limited to:
1. Databases
2. Conceptual data models (ER, UML)
3. Frame-based systems
4. OBO format ontologies
5. Thesauri
6. Biological models
7. Excel sheets
8. Tagging, folksonomies
9. Output of text mining, machine learning, clustering
It is not equally easy (or diﬃcult) to transform them into a domain ontology. Fig-
ure 7.1 gives an idea as to how far one has to ‘travel’ from the legacy representation
to a ‘Semantic Web compliant’ one. The further the starting point is to the left
of the ﬁgure, the more eﬀort one has to put into realising the ontology learning
such that the result is actually usable without the need of a full redesign. Given
that this is an introductory textbook, not all variants will pass the revue. We shall
focus on using a database as source material to develop an ontology (Section 7.1),
spreadsheets (Section 7.2), thesauri (Section 7.3), and a little bit NLP (Section 7.4).
Lastly, we will introduce ontology design patterns in Section 7.6, which are a bit
in the middle of bottom-up and top-down.
Figure 7.1: Various types of less and more comprehensively formalised ‘legacy’ resource.
7.1
Relational databases and related ‘legacy’ KR
The starting position for leveraging the knowledge encoded in a relational database
to develop an ontology could be its conceptual data model.
However, despite

7.1.
Relational databases and related ‘legacy’ KR
141
academics’ best eﬀorts to teach good design and maintenance methodologies in
a degree programme, it is not uncommon in organisations that if there was a
conceptual model for the database at all, it is outdated by the time you would
want to use it for ontology development. New columns and tables may have been
added in the database, constraints removed, tables joined (further denormalised)
for better performance or vice versa for cleaner data, and so on, and no-one may
have bothered to go back to the original conceptual, or even relational, model and
update it with the changes made. Practically, there likely will be a database with
multiple tables that have many (15-50) columns. This is represented at the bottom
of Figure 7.2.
ID
A
B
C
D
E
F
G
H
X
A
B
C
Env:123
Env:137
Env:512
Env:444
D
E
Env:1
Env:2
Env:3
Env:15
Env:25
Env:123
Env:444
Env:512
...
...
...
...
...
...
F
G
X
X
R
A
D
H
S
B
C
E
H
ID
T
F
G
...
...
Ontology
Figure 7.2: Denormalised relational database (bottom), where each table is reverse
engineered into an entity in a ‘ﬂat’ EER diagram (middle), and subsequently reﬁned with
respect to the hidden entity types and annotations, such as the Environment ontology
(top), which then ﬁnally can be transformed/translated into an ontology.

142
Chapter 7. Bottom-up Ontology Development
If one were to simply convert that SQL schema into an OWL ontology, the
outcome would be a bunch of classes with many data properties and an unnamed
object property between a subset of the classes based on the foreign key constraints.
This won’t do as an ontology. Let us have a look at the additional steps.
Reverse engineering the database
There are several reverse engineering tools for SQL schemas of relational databases,
where a ﬁrst pass results in one of the possible logical models (i.e., the relational
model for an RDBMSs), and another iteration brings one up to the conceptual
data model (such as ER, ORM) [HCTJ93]. Such a ﬁrst draft version of the EER
model is depicted in EER bubble notation in Figure 7.2, where each table (relation)
has become an entity type and each column an attribute. The main problematic
consequence for reverse engineering the conceptual data model to feed into an OWL
ontology is that the database structure has been ‘ﬂattened out’, which, if simply
reverse engineered, ends up in the ‘ontology’ as a class with umpteen attributes
with which one can do minimal (if at all) automated reasoning (see the ﬁrst diagram
above the table in Figure 7.2).
To avoid this, should one perform some normalization steps to try to get some
structure back into the conceptual view of the data alike in the diagram at the
top in Figure 7.2, and if so, how? Whether done manually or automatically, it can
be cleaned up, and original entity types (re-)introduced, relationships added, and
the attributes separated accordingly, thereby making some knowledge implicit in
the database schema explicit, which is depicted in the top-half of Figure 7.2. A
tried and tested strategy to do this semi-automatically is by discovering functional
dependencies in the data stored in the database tables. Such reverse engineering
opens up other opportunities, for one could use such a procedure to also establish
some mechanism to keep a ‘link’ between the terms in the ontology and the source
in the database so that the ontology can be used to enhance data analysis through
conceptual model or ontology-based querying. A particular algorithm up to obtain-
ing a DL-formalised conceptual data model based on a fully normalised database
can be found in, e.g., [LT09]. Most of the reverse engineering achievements up to
conceptual models were obtained in the 1990s.
Figure 7.2 may give the impression that it is easy to do, but it is not. Diﬃcul-
ties have to do with the formal apparatus of the representation languages1, and the
static linking between the layers and the procedures—conveniently depicted with
the three arrows—hide the real complexity of the algorithms. Reverse engineering
is not simply running the forward algorithm backwards, but has a heuristics com-
ponent to second-guess what the developers’ design decisions may have been along
the stages toward implementation and may have a machine learning algorithm to
ﬁnd constraints among instances. Most solutions to date set aside data duplication,
1For conceptual data modelling languages, among others, the Object Management Group’s
Ontology deﬁnition metamodel (http://www.omg.org/spec/ODM/1.0/) is exploring interactions
between UML and OWL & RDF, and there are various results on mapping ER, EER, UML,
ORM and/or ORM2 into a suitable or convenient DL language. This ‘application of Ontology
and ontologies’ areas are deemed outside the book’s current scope.

7.1.
Relational databases and related ‘legacy’ KR
143
violations of integrity constraints, hacks, outdated imports from other databases
and assume to have a well-designed relational database in at least 3NF or BCNF,
and, thus, the results are imperfect.
In addition to this procedure, one has to analyse the data stored in the database
on its exact meaning. In particular, one may come across data in the database that
are actually assumed to be concepts/universals/classes, whereas others represent
real instances (typically, a tuple represents an instance). For instance, a Content
Management System, such as Joomla, requires the content provider to store a
document under a certain category that is considered a class by its user, which,
however, is stored in a cell of a row in the back-end database, hence, mathematically
an instance in the software. Somehow, we need to ﬁnd that and extract it for use
in the ontology in a way that they will become classes. Another typical case is
where a structured controlled vocabulary, such as the Gene Ontology we have seen
in Section 1.3, has been used in the database for annotation. This is depicted
on the right-hand side with Env:444 and so on. Knowing this, one can reverse
engineer that section of the database into a taxonomy in the conceptual data model
(shown in Figure 7.2 in the top ﬁgure on the right-hand side). Finally, there is a
so-called ‘impedance mismatch’ between database values and ABox objects, but
this is relevant mainly for ontology-based data access (see Chapter 8). Thus, we
end up having to process the case that some, or all, data where the values are
actually concepts, should become OWL classes and values that should become
OWL individuals.
Enhancing and converting the conceptual model
Having completed all the reverse engineering and data analysis to obtain the con-
ceptual data model, one can commence with the ontological analysis. For instance,
whilst improving the conceptual data model, one could add a section of another
ontology for use or interoperability, alike the GO, improve on the naming and
meaning of the relationships as perhaps some of them have the same meaning as
those in a foundational ontology, add constraints (notably: disjointness), and so
forth. Subsequently, it will have to be converted to a suitable logic.
There are several tools that convert a conceptual model, especially UML Class
Diagrams, into OWL, but they have only partial coverage and its algorithms are
unclear; for instance, on how one should transform ternaries and what to do with
the attributes (recall Section 6.1.1). In addition, they work only with a subset of
UML diagrams due to the diﬀerences in UML tool implementations (which is due
to ambiguity emanating from the OMG standard and diﬀerences across versions);
hence, a careful post-transformation analysis will have to be carried out.
One also could switch these steps by ﬁrst converting a schema to OWL and
then perform the ontological analysis.
Other languages and OWL
Imperfect transformations from other languages, such as the common OBO format
[GH07, HOD+10] and a pure frames-based approach [ZBG06], are available, which

144
Chapter 7. Bottom-up Ontology Development
also describe the challenges to create them.
OBO is a Directed Acyclic Graph mainly for classes and a few relationships
(mainly is a and part of), which relatively easily can be mapped into OWL, and the
extras (a.o., date, saved by, remark) could go in OWL’s annotations. There are a
few mismatches and ‘work-arounds’, such as the not-necessary and inverse-necessary,
and a non-mappable antisymmetry (cannot be represented in OWL). As a result,
there are several OBO-in-OWL mappings, of which some are more comprehensive
than others. The latest/oﬃcial mapping available from http://oboformat.org
(superseding the earlier mapping by [GH07]), which is also implemented in the
OWL API. Most OBO ontologies now also have an OWL version (consult OBO
Foundry, BioPortal), but keep both, for each has their advantages (at present).
There is one salient diﬀerence between OWL and OBO ontologies—more precisely:
the approach to modelling—which also aﬀects multilingual ontologies (Section 9.1),
and how an OBO ontology in Prot´eg´e is displayed. In OWL, you typically give
a class a human readable name, whereas in OBO, a class is assigned an identiﬁer
and the name is associated to that with a label (OBO people who moved to OWL
maintain that practice, so numbers as class names do not imply it was natively
an OBO ontology). Newer versions of ontology editors let the user choose how to
render the ontology in the interface, by name or by label. If you ﬁnd an ontology
online and the class names are something alike IAO12345, then it was likely an
OBO ontology converted into OWL, and you’ll have to change the view so that it
will show the labels instead of those meaningless numbers.
While OBO and the older frames-based Prot´eg´e do serve a user base, their over-
all impact on widespread bottom-up ontology development for the Semantic Web is
likely to be less than the potential that might possibly be unlocked with leveraging
knowledge of existing (relational) databases to start developing ontologies.
7.2
From spreadsheets to OWL
Spreadsheets are normally intended to store data. There are two ways to lever-
age the structure of spreadsheet content in the process of developing an ontol-
ogy. The ﬁrst option is based on the ‘standard’ way of using a spreadsheet. Nor-
mally, the ﬁrst row contains column headings that are essentially the ‘attributes’
or classes of something, and the rest of the columns or rows are the data. This
can be likewise for the rows in the ﬁrst column. This gives two opportunities for
bottom-up development: extract those column/row headings and take that vocab-
ulary to insert in the TBox. For instance, a row 1 that contains in columns A-D
⟨FlowerType, Colour, Height, FloweringSeason⟩gives a clear indication of what
one could put in the ontology. It is a bit less structured than databases and their
corresponding conceptual models, and there are no reverse engineering algorithms
to discover the constraints, so still an analysis has to be carried out as to how one
represents, say, Colour in the ontology. Subsequently, and having recorded how the
column headings have been represented in the ontology, one could load the data
into the ABox accordingly, if desired.
The second option of using a spreadsheet is that it can be seen as an easier

7.3.
Thesauri
145
interface to declare knowledge compared to adding axioms in an ODE such as
Prot´eg´e, especially for domain experts. The idea works as follows. We have seen
several axioms that adhere to a particular pattern, such as C ⊑D and C ⊑∃R.D,
where the former could be called, say, “named class subsumption” and the latter
“all-some”. This can be converted into ‘logical macros’, i.e., a non-logician-friendly
interface where only the vocabulary is entered into speciﬁc ﬁelds, and some script
behind the scenes does the rest to insert it in the ontology. This is illustrated in
the following example.
Example 7.1. Consider Figure 7.3. On the left-hand side is a small spreadsheet,
with in column A some data that one would want to have converted into classes
in the TBox and to be asserted to be subclasses of those values-to-be-turned-into-
classes in column B. That is, it is a table representation of the axiom type C ⊑D.
The script to do that may be, say, a JSON script to process the spreadsheet that,
in turn, uses the OWL API to write into the OWL ﬁle. Such a script is shown on
the right-hand side of the ﬁgure. Upon running it, it will add Lion ⊑Animal etc.
to the ontology, if not already present.
The principle is similar for the data in columns D and E, but then for the “all-
some” axiom type C ⊑∃R.D. The values in column D will be the class that will
have a property declared and in column E what the class in column D is doing
(eating, in this case). Looking at it diﬀerently: the table consisting of columns D
and E amounts to the eats relation and is intended to be converted into the two
axioms Lion ⊑∃eats.Impala and Giraﬀe ⊑∃eats.Twig. ♦
One such tool with which one can do this is cellﬁe2 that uses the M2 DSL for the
transformation [OHWM10]. It still requires one to declare what the axiom pattern
should be in the rule line, which could be seen as disadvantage but also as having
the advantage to be more ﬂexible. For instance, a table could have three columns,
so that a domain expert can add arbitrary object properties in a row, like in row
2 ⟨lion, eats, impala⟩and in row 3 ⟨lion, drinks, water⟩and then declare in the
rule that the second column has to become an object property in the ontology.
If there are many axioms to add, such an approach likely also will be faster when
knowledge has to be added in batch compared to clicking around in the ODE.
7.3
Thesauri
A thesaurus is a simple concept hierarchy where the concepts are related through
three core relations: BT broader term, NT narrower term, and RT related term
(and auxiliary ones UF/USE, use for/use). For instance, a small section of the
Educational Resources Information Center thesaurus looks like this:
reading ability
BT ability
RT reading
RT perception
2https://github.com/protegeproject/cellfie-plugin

146
Chapter 7. Bottom-up Ontology Development
Figure 7.3: Generating OWL axioms based on a ‘macro’ approach in spreadsheets.
left: a spreadsheet with in column A the subclass and in column B its superclass, and in
column D the subclass/class that will have a property declared and in column E what
the class in column D is eating. Right: the JSON script to convert columns A and B
into axioms of the type “A ⊑B”.
and the AGROVOC thesaurus about agriculture of the Food and Agriculture Or-
ganisation (FAO) of the United Nations has the following asserted, among others:
milk
NT cow milk
NT milk fat
How to go from this to an ontology? Three approaches exist (thus far):
• Automatically translate the ‘legacy’ representation of the ontology into an
OWL ﬁle and call it an ontology (by virtue of being represented in OWL,
regardless the content);
• Find some conversion rules that are informed by the subject domain and
foundational ontologies (e.g., introducing parthood, constitution, etc.);
• Give up on the idea of converting it into an ontology and settle for the W3C-
standardised Simple Knowledge Organisation System3 format to achieve com-
patibility with other Semantic Web Technologies.
We will look at the problems with the ﬁrst option, and achievements with the
second and third option.
7.3.1
Converting a thesaurus into an ontology
Before looking at conversions, one ﬁrst has to examine what a typical thesaurus
really looks like, in analogy to examining databases before trying to port them into
an ontology.
Problems
The main issues with thesauri, and for which thus a solution has to be found, are
that:
• Thesauri are generally a lexicalisation of a conceptualisation, or: writing
3http://www.w3.org/2004/02/skos/

7.3.
Thesauri
147
out and describing concepts in the name of the concept, rather than adding
characterising properties;
• Thesauri have low ontological precision with respect to the categories and the
relations: there are typically no formal details deﬁned for the concept names,
and BT/NT/RT are the only relations allowed in the concept hierarchy.
As thesauri were already in widespread use before ontologies came into the picture
for ontology-driven information systems, they lack basic categories alike those in
DOLCE and BFO. Hence, an alignment activity to such foundational ontology
categories will be necessary. Harder to ﬁgure out, however, are the relations. RT
can be anything, from parthood to transformation, to participation, or anything
else, and BT/NT turns out not to be the same as class subsumption; hence, the
relations are overloaded with (ambiguous) subject domain semantics.
This has
as result that those relationships are used inconsistently—or at least not precise
enough for an ontology. For instance, in the aforementioned example, milk and
milk fat relate in a diﬀerent way to each other than milk and cow milk, for milk
fat is a component of milk and cow milk indicates its origin (and, arguably, it is
part of the cow), yet both were NT-ed to milk.
A sample solution: rules as you go
Because of the relatively low precision of a thesaurus, it will take a bit more work
to convert it into an ontology cf. a database. Basically, the ontological analysis
that hasn’t been done when developing the thesaurus—in favour of low-hanging
fruit for system development—will have to be done now. For instance, a nebulous
term like “Communication (Thought Transfer)” in the ERIC thesaurus will have to
be clariﬁed and distinguished from other types of communication like in computer
networks. They then could be aligned to a foundational ontology or a top-domain
ontology after some additional analysis of the concepts in the hierarchy and aided by
a decision diagram like D3. One also should settle on the relations that will replace
BT/NT/RT. An approach to this particular aspect of reﬁnement is presented in
[KJLW12].
This is a lot of manual work, and there may be some ways to automate some
aspects of the whole process. Soergel and co-authors [SLL+04] took a ‘rules as you
go’ approach that can be applied after the aforementioned ontological analysis.
This means that as soon as some repetitiveness was encountered in the manual
activity, a rule was devised, the rest of the thesaurus assessed on the occurrence of
the pattern, and converted in one go. A few examples are included below.
Example 7.2. For instance, Soergel and co-authors observed that, e.g., cow NT
cow milk should become cow <hasComponent> cow milk.
There are more an-
imals with milk; hence, a pattern could be animal <hasComponent> milk, or,
more generally animal <hasComponent> body part. With that rule, one can ﬁnd
automatically, e.g., goat NT goat milk and convert that automatically into goat
<hasComponent> goat milk. Other pattern examples were, e.g., plant <growsIn>
soil type and geographical entity <spatiallyIncludedIn> geographical entity. ♦

148
Chapter 7. Bottom-up Ontology Development
7.3.2
Avoiding ontologies with SKOS
Thesauri tend to be very large, and it may well be too much eﬀort to convert them
into a real ontology, yet one still would want to have some interoperation of thesauri
with other systems so as to avail of the large amounts of information they contain.
To this end, the W3C developed a standard called Simple Knowledge Organisation
System(s): SKOS4 [MB09]. More broadly, it is intended for converting thesauri,
classiﬁcation schemes, taxonomies, subject headings etc. into one interoperable
syntax, thereby enabling concept-based search instead of text-based search, reuse
of each other’s concept deﬁnitions, facilitate the ability to search across institution
boundaries, and to use standard software. This is a step forward compared to the
isolated thesauri.
However, there are also some limitations to it: ‘unusual’ concept schemes do not
ﬁt into SKOS because sometimes the original structure too complex, skos:Concept
is without clear properties like in OWL, there is still much subject domain se-
mantics in the natural language text which makes it less amenable to advanced
computer processing, and the SKOS ‘semantic relations’ have little semantics, as
skos:narrower does not guarantee it is is a or part of, as it just is the standardised
version of NT.
Then there is a peculiarity in the encoding. Let us take the example where
Enzyme is a subtype of Protein, hence, we declare:
SKOSPaths:protein rdf:type skos:Concept
SKOSPaths:enzyme rdf:type skos:Concept
SKOSPaths:enzyme SKOSPaths:broaderGeneric SKOSPaths:protein
in the SKOSPaths SKOS ﬁle, which are, mathematically, statements about in-
stances.
This holds true also if we were to transform an OWL ﬁle to SKOS:
each OWL class becomes a SKOS instance due to the mapping of skos:Concept
to owl:Class [IS09]. This is a design decision of SKOS. From a purely technical
point of view, that can be dealt with easily, but one has to be aware of it when
developing applications.
As the scope of this book is ontology engineering, SKOS will not be elaborated
on further.
7.4
Text processing to extract content for ontolo-
gies
If all else fails, and there happens to be a good amount of text available in the sub-
ject domain of the (prospective) ontology, one can try Natural Language Processing
(NLP) to develop the ontology5. Which approaches and tools suit best depends
on the goal (and background) of its developers and prospective users, ontological
commitment, and available resources.
4http://www.w3.org/TR/swbp-skos-core-spec
5of course, once the ontology is there, it can be used as a component in an ontology-driven
information system, and an NLP application can be enhanced with an ontology, but that is a
separate theme.

7.4.
Text processing to extract content for ontologies
149
There are two principal possibilities to use NLP for ontology development:
• Use NLP to populate the TBox of the ontology, i.e., obtaining candidate
terms from the text, which is also called ontology learning (from text).
• Use NLP to populate the ABox of the ontology, i.e., obtaining named entities,
which is also called ontology population (from text).
A review of NLP and (bio-)ontologies can be found in [LHC11] and some examples
in [CSG+10, AWP+08].
But why the “if all else fails...” at the start of the section? The reason is that
information in text is unstructured and natural language is inherently ambiguous.
The ﬁrst step researchers attempted was to ﬁnd candidate terms for OWL classes.
This requires a Part-of-Speech (POS) tagger so as to annotate each word in the text
with its category; e.g., ‘apple’ is a noun and so forth. Then one selects the nouns
only and counts how often it occurs, taking into account synonyms so as to group
those together and assesses which ones are homonyms and used in diﬀerent ways
and therefore have to be split into diﬀerent buckets. This process may be assisted
by, e.g., WordNet6. Challenges arise with euphemisms, slang, and colloquialisms,
as well as with datedness of texts as terms may have undergone concept drift
(i.e., mean something else now) and new ones have been invented. The eventual
resulting candidate list is then assessed by humans on relevance, and subsequently
a selection will be added to the ontology.
The process for candidate relations is a bit more challenging. Although one
easily can ﬁnd the verbs with a POS tagger, it is not always easy to determine the
scope of what denotes the subject and what denotes the object in the sentence,
and authors are ‘sloppy’ or at least imprecise. For instance, one could say (each)
‘human has a heart’, where ‘has’ actually refers to structural parthood, ‘human
has a house’ where ‘has’ probably means ownership, and ‘human has a job’ which
again has a diﬀerent meaning.
The taxonomy of part-whole relations we have
seen in Section 6.2 has been used to assist with this process (e.g., [THU+16]).
Consider that DOLCE and WordNet are linked and thus for a noun in the text
that is also in WordNet, then one can ﬁnd the DOLCE category. Knowing the
DOLCE category, one can check which part-whole relation ﬁts with that thanks
to the formal deﬁnitions of the relations. For instance, ‘human’ and ‘heart’ are
both physical endurants, which are endurants, which are particulars. One then can
use OntoPartS’s algorithm: return only those relations where the domain and
range are either of those three, but not any others. In this case, it can be (proper)
structural parthood or the more generic plain (proper) parthood, but not, say
involvement because ‘human’ and ‘heart’ are not perdurants. A further strategy
that could be used is, e.g., VerbNet7 that uses compatible roles of the relations
that the nouns play in the relation.
Intuitively, one may be led to think that simply taking the generic NLP tools
will do also for specialised domains, such as (bio-)medicine. Any application does
indeed use those techniques and tools, but, generally, they do not suﬃce to obtain
6https://wordnet.princeton.edu/
7https://verbs.colorado.edu/verbnet/

150
Chapter 7. Bottom-up Ontology Development
‘acceptable’ results. Domain speciﬁc peculiarities are many and wide-ranging. For
instance, 1) to deal with the variations of terms (e.g., scientiﬁc name, variants,
abbreviations, and common misspellings) and the grounding step (linking a term
to an entity in a biological database) in the ontology-NLP preparation and instance
classiﬁcation [WKB07]; 2) to characterise the question in a question answering
system correctly (e.g., [VF09]); and 3) to ﬁnd ways to deal with the rather long
strings and noun phrases that denote a biological entity or concept or universal
[AWP+08].
Taking into account such peculiarities does generate better overall
results than generic or other domain-speciﬁc usages of NLP tools, but it requires
extra manual preparatory work and a basic understanding of the subject domain
and its applications to include also such rules. For instance, enzyme names always
end with ‘-ase’, so one can devise a rule with a regular expression to detect these
terms ending in ‘-ase’ and add them in the taxonomy as a subclass of Enzyme.
Ontology population in the sense of actually adding a lot of objects in the ABox
of the OWL ﬁle is not exciting, for it is not good in scalability of reasoning, partially
due to the complexity of OWL 2 and partially because the default setting of the
ODEs is that it will load the whole OWL ﬁle into main memory and by default
settings at least, the ODE will run out of memory. There are alternatives to that,
such as putting the instances in a database or annotating the instances named in
the text with the terms of the ontology and store those texts in a digital library,
which then can be queried. The process to realise it requires, among others, a
named entity tagger so that is can tag, say, the ‘Kruger park’ as a named entity.
It then has to ﬁnd a way to ﬁgure out that that is an instance of Nature Reserve.
For geographic entities, a gazetteer can be used. As for nouns, also named entities
can have diﬀerent strings yet refer to the same entity; e.g., the strings ‘Luis Fonsi’
and ‘L. Fonsi’ refer to the same singer-songwriter of the smash-hit Despacito. It
has further issues, such as referring expressions in the same as well as successive
sentences; e.g., in the sentence “he wrote the song during a sizzling Sunday sunset”,
“he” refers to Fonsi and “the song” to Despacito, which has to be understood and
represented formally as a triple, say, ⟨Fonsi, songwriter, Despacito⟩and linked
to the classes in the ontology.
NLP for ontology learning and population is its own subﬁeld in ontology learn-
ing. The brief summary and illustration of some aspects of it does not cover the
whole range, but may at least have given some idea of non-triviality of the task.
If you are interested in this topic: a more comprehensive overview is described in
[CMSV09] and there are several handbooks.
7.5
Other semi-automated approaches
Other (semi-)automated approaches to bottom-up ontology development include
machine learning techniques, deploying so-called ‘non-standard’ DL reasoning ser-
vices, and converting diagrams fro biology into (candidate) ontology terms and
relations.
A short overview and relevant references of machine learning techniques for
ontology development can be found in [dFE10], who also outline where such induc-

7.5.
Other semi-automated approaches
151
tive methods can be used, being: classifying instances, learning new relationships
among individuals, probabilistic ontologies, and probabilistic mapping for the on-
tology matching task, (semi)-automating the ontology population task, reﬁning
ontologies, and reasoning on inconsistent or noisy knowledge bases. Several ‘hy-
brids’ exists, such as the linking of Bayesian networks with probabilistic ontologies
[dCL06] and improving data mining with an ontology [ZYS+05].
Other options are to resort to a hybrid of Formal Concept Analysis with OWL
[BGSS07], least common subsumer [BST07, Tur08, PT11], and similar techniques.
The notion of least common subsumer and most speciﬁc concept and motivations
where and how it may be useful are described in [PT11]. The least common sub-
sumer and most speciﬁc concept use non-standard reasoning services that help
with ontology development, and they are deﬁned in terms of DL knowledge bases
as follows.
Deﬁnition 7.1 (least common subsumer ([PT11])). Let L be a Description Logic
language, K = (T , A) be a knowledge base represented in DL L (an L-KB). The
least common subsumer (lcs) with respect to T of a collection of concepts C1, . . . , Cn
is the L-concept description C such that:
1. Ci ⊑T C for all 1 ≤i ≤n, and
2. for each L-concept description D holds: if Ci ⊑T D for all 1 ≤i ≤n, then
C ⊑T D.
Deﬁnition 7.2 (most speciﬁc concept ([PT11])). Let L be a Description Logic
language, K = (T , A) be a knowledge base represented in DL L (an L-KB). The
most speciﬁc concept (msc) with respect to K of an individual from A is the L-
concept description C such that:
1. K |= C(a), and
2. for each L-concept description D holds: K |= D(a) implies C ⊑T D.
The least common subsumer computes the common superclass of a concept and
the most speciﬁc concept classiﬁes an individual into a concept description.
One could exploit biological models to ﬁnd candidate terms and relations when
those models have been created with software. This allows for semi-automated
approaches to formalise the graphical vocabulary in textbooks and drawing tools,
and subsequently use an algorithm to populate the TBox with the knowledge taken
from the drawings. This because such software has typical icons for categories of
things, like a red oval meaning Protein, a yellow rectangle meaning Cell Process, and
a pale green arrow with a grey hexagon in the middle meaning Protein Modiﬁcation.
Each individual diagram can thus be analysed, and the named shapes at least
categorised as subclasses of such main classes and relations asserted for the arrows
between the shapes. This has been attempted for STELLA and PathwayStudio
models [Kee05, Kee12b]. Related are the eﬀorts with converting models represented
in the Systems Biology Markup Language (SMBL) into OWL [HDG+11].

152
Chapter 7. Bottom-up Ontology Development
7.6
Ontology Design Patterns
Ontology Design Patterns (ODPs) are a middle out way for developing ontologies.
They can be viewed as an extremely lightweight version of design principles alike
found in foundational ontologies, but then with less ‘clutter’. That is, they can be
cleverly modularised foundational ontology fragments that serve as design snippets
for good modelling practices. They also can be viewed as a way of bottom-up
pattern ﬁnding that is then reused across the ontology and oﬀered to others as
a ‘best practices’ design solution for some modelling aspect.
ODPs have been
proposed ﬁrst a while ago [BS05, Gan05], and have gained some traction in research
in recent years with various ideas and proposals.
There is, therefore, no clear
single, neat, core to extract from it and describe at present. A clear, informal
overview is described in [GP09], but terms, descriptions, and categorisations are
being reworked [FGGP13], and the sub-ﬁeld better characterised with respect to
the issues for using ODPs and possible research directions [BHJ+15].
Let us ﬁrst introduce some deﬁnitions for a pattern for a speciﬁc ontology and
their uses and then proceed to types of patterns. The deﬁnitions are geared to the
OWL language, but one can substitute that for another language of choice.
Deﬁnition 7.3 (Language of pattern instantiation [FK17]). OWL Ontology O
with language speciﬁcation adhering to the W3C standard [MPSP09], which has
classes C ∈VC, object properties OP ∈VOP, data properties D ∈VD, data types
DT ∈VDT of the permitted XML schema types, axiom components (‘language
features’) X ∈VX, and such that Ax ∈VAx are the axioms.
The ‘axiom components’ include features such as, among others, subsumption,
transitivity, existential quantiﬁcation, and cardinality, which can be used according
to the syntax of the language. A pattern itself is a meta-level speciﬁcation, in
a similar fashion as stereotyping in UML. Just in case a pattern also includes
‘reserved’ entities from, say, a foundational ontology, they get their own entry in
the vocabulary to clearly distinguish them.
Deﬁnition 7.4 (Language for patterns: Vocabulary V [FK17]). The meta-level
(second order) elements (or stereotypes) for patterns are:
• class C ∈VC as C in the pattern;
• object property OP ∈VOP as R in the pattern;
• data property D ∈VD as D in the pattern;
• data type DT ∈VDT as DT in the pattern;
• reserved set of entities from a foundational ontology, as F in the pattern;
where added subscripts i with 1 ≤i ≤n may be diﬀerent elements. Two elements
in the vocabulary are called homogeneous iﬀthey belong to the same type, i.e., they
are both classes, or both object properties, and so on. Elements can be used in
axioms Ax ∈VAx that consists of axiom components x ∈VX in the pattern such
that the type of axioms are those supported in the ontology language in which the
instance of the pattern is represented.
With these ingredients in place, one can then deﬁne an ontology pattern P as
follows.

7.6.
Ontology Design Patterns
153
Deﬁnition 7.5 (Ontology Pattern P [FK17]). An ontology pattern P consists of
more than one element from vocabulary V which relate through at least one axiom
component from VX. Its speciﬁcation contains the:
• pattern name;
• pattern elements from V;
• pattern axiom component(s) from VX;
• pattern’s full formalisation.
For instance, the basic all-some pattern that we have seen as ‘macro’ in Section 7.2
has as speciﬁcation ([FK17]):
• pattern name: basic all-some
• pattern elements: C1, C2, R
• pattern axiom component(s): ⊑, ∃
• pattern’s full formalisation: C1 ⊑∃R.C2
An instantiation of the basic all-some pattern in an ontology, say, the AWO, may
be, e.g., Giraﬀe ⊑∃drinks.Water.
As can be seen from the deﬁnition, they are referred to agnostically as patterns—
it may be a pattern realised in the ontology and some algorithm has to search for
(as was the scope in [FK17]) as well as one deﬁned separately and applied during
the design phase and is therewith thus also in line with some newly proposed
terminology [FGGP13]. Ontology patterns tend to be more elaborate than the basic
all-some pattern. For instance, one could specify a pattern for how to represent
attributions with DOLCE’s Quality rather than an OWL data property, as was
discussed in Section 6.1.1, or how to systematically approximate representing an
n-ary into n binaries in OWL. Furthermore, there are broader options for ontology
patterns. A selection of them with a few examples is as follows.
• Architecture pattern.
This speciﬁes how the ontology is organised.
For
instance, one could choose to have a modular architecture in the sense of
sub-domains. An example of a fairly elaborate architecture is illustrated in
Figure 7.4 for BioTop [BSSH08].
• Logical pattern. This deals with the absence of some features of a represen-
tation language and how to work with that. The issue with n-aries in OWL
is such an example.
• Content pattern. This pattern assists with representing similar knowledge in
the same way for that particular ontology. Recalling the rules-as-you-go from
thesauri bottom-up development, they can be speciﬁed as content patterns.
A larger example is shown in Figure 7.5.
• ‘Housekeeping’ patterns, including so-called lexico-syntactic patterns. They
refer to ensuring clean and consistent representations in the ontology. For
instance, to write names in CamelCase or with dashes, and using IDs with
labels throughout versus naming the classes throughout the ontology.
There are also practical engineering tasks in the process of using ODPs, such
as a workﬂow for using ODPs and the usual requirements of documentation and
metadata; recent ﬁrst proposals include [FBR+16, KHH16].

154
Chapter 7. Bottom-up Ontology Development
Figure
7.4:
BioTop’s
Architecture,
which
links
to
both
DOLCE
and
BFO-RO
and
small
‘bridge’
ontologies
to
link
the
modules.
(Source:
http://www.imbi.uni-freiburg.de/ontology/biotop).
7.7
Exercises
Review question 7.1. Why can one not simply convert each database table into
an OWL class and assume the bottom-up process is completed?
Review question 7.2. Name two modelling considerations going from conceptual
data model to ontology.
Review question 7.3. Name the type of relations in a thesaurus.
Review question 7.4. What are some of the issues one has to deal with when
developing an ontology bottom-up using a thesaurus?
Review question 7.5. What are the two ways one can use NLP for ontology
development?
Review question 7.6. Machine learning was said to use inductive methods. Re-
call what that means and how it diﬀers from deductive methods.
Review question 7.7. The least common subsumer and most speciﬁc concept use
non-standard reasoning services that helps with ontology development. Describe
in your own words what they do.
Exercise 7.1. Examine Figure 7.6 and answer the following questions.
a. Represent the depicted knowledge in an OWL ontology. *
b. Can you represent all knowledge? If not: what not? *
c. Are there any problems with the original conceptual data model? If so, which
one(s)? *

7.7.
Exercises
155
Input
Catalyst
Material 
Object
Output
Material 
transformation
1..*
1..*
has 
input
has 
output
*
1..*
Neighbourhood
time:Interval
*
*
Figure 7.5: Example of a content OP represented informally on the left in UML class
diagram style notion and formally on the right. There is a further extension to this OPD
described in [VKC+16] as well as several instantiations.
Figure
7.6:
A
small
conceptual
model
in
ICom
(from
its
website
http://www.inf.unibz.it/∼franconi/∼icom;
see
[FFT12]
for
further
details
about the tool); blob: mandatory, open arrow: functional; square with star: disjoint
complete, square with cross: disjoint, closed arrow (grey triangle): subsumption.
Exercise 7.2. Figure 7.7 shows a very simple conceptual data model in roughly
UML class diagram notation: a partition [read: disjoint, complete] of employees
between clerks and managers, plus two more subclasses of employee, namely rich
employee and poor employee, that are disjoint from the clerk and the manager
classes, respectively (box with cross). All the subclasses have the salary attribute
restricted to a string of length 8, except for the clerk entity that has the salary
attribute restricted to be a string of length 5. Another conceptual data model,
in ORM2 notation (which is a so-called attribute-free language), is depicted in
Figure 7.8, which is roughly similar.
a. When you reason over the conceptual data model in Figure 7.7, you will
ﬁnd it has an inconsistent class and one new subsumption relation. Which
class is inconsistent and what subsumes what (that is not already explicitly
declared)? Try to ﬁnd out manually, and check your answer by representing
the diagram in an OWL ontology and run the reasoner to ﬁnd out. *
b. Develop a proper ontology that can handle both conceptual data models.

156
Chapter 7. Bottom-up Ontology Development
Consider the issue of how to deal with attributes and add the information
that clerks work for at most 3 projects and managers manage at least one
project. *
Figure
7.7:
A
small
conceptual
model
in
ICom
(Source:
http://www.inf.unibz.it/∼franconi/∼icom).
Figure 7.8: A small conceptual model in ORM2, similar to that in Figure 7.7.
Exercise 7.3. Consider the small section of the Educational Resources Information
Center thesaurus, below.
a. In which W3C-standardised (Semantic Web) language would you represent
it, and why? *
b. Are all BT/NT assertions subsumption relations? *
c. There is an online tool that provides a semi-automatic approach to developing
a domain ontology in OWL starting from SKOS. Find it. Why is it semi-
automatic and can that be made fully automatic (and if so, how)?
Popular Culture
BT Culture
NT n/a

7.7.
Exercises
157
RT Globalization
RT Literature
RT Mass Media
RT Media Literacy
RT Films
UF Mass Culture (2004)
Mass Media
BT n/a
NT Films
NT News Media
NT Radio
RT Advertising
RT Propaganda
RT Publications;
UF Multichannel Programing (1966 1980) (2004)
Propaganda
BT Communication (Thought Transfer)
BT Information Dissemination
NT n/a
RT Advertising
RT Deception
RT Mass Media
UF n/a
Exercise 7.4. In what way(s) may data mining be useful in bottom-up ontology
development? Your answer should include something about the following three
aspects:
a. populating the TBox (learning classes and hierarchies, relationships, con-
straints),
b. populating the ABox (assertions about instances), and
c. possible substitutes or additions to the standard automated reasoning service
(consistency checking, instance classiﬁcation, etc.).
Exercise 7.5. Deﬁne a pattern for how to represent attributions with DOLCE’s
Quality rather than an OWL data property.
Exercise 7.6. OWL permits only binary object properties, though n-aries can be
approximated. Describe how they can be approximated, and how your OP would
look like such that, when given to a fellow student, s/he can repeat the modelling
of that n-ary exactly the way you did it and add other n-aries in the same way. *
Exercise 7.7. Inspect the Novel Abilities and Disabilities OntoLogy for ENhancing
Accessibility: adolena; Figure 7.9 provides a basic informal overview. Can (any
of) this be engineered into an ODP? If so, which type(s), how, what information
is needed to document an OP? *
Exercise 7.8. Figure 7.5 shows a content OP. How would you evaluate whether
this is a good ODP? In doing so, describe your reasoning why it is, or is not, a
good ODP. *

158
Chapter 7. Bottom-up Ontology Development
Function
hasFunction
Ability
assistsWith / 
isAssistedBy
Device
Disability
ServiceProvider
isAffectedBy / 
affects
ameliorates
providedBy /
provides
requiresAbility
Assistive 
Device
Replacement 
Device
Physical 
Ability
assistsWith / 
isAssistedBy
Body
Part
replaces
Figure 7.9: Informal view of the adolena ontology.
Exercise 7.9. Discuss the feasibility of the following combinations of requirements
for an ontology-driven information system (and make an informed guess about the
unknowns):
a. Purpose: science; Language: OWL 2 DL, or an extension thereof; Reuse:
foundational; Bottom-up: form textbook models; Reasoning services: stan-
dard and non-standard.
b. Purpose:
querying data through an ontology; Language:
some OWL 2;
Reuse: reference; Bottom-up: physical database schemas and tagging; Rea-
soning services: ontological and querying.
c. Purpose: ontology-driven NLP; Language: OWL 2 EL; Reuse: unknown;
Bottom up: a thesaurus and tagging experiments; Reasoning services: mainly
just querying.
You may wish to consult [Kee10a] for a table about dependencies, or argue upfront
ﬁrst.
Exercise 7.10. You are an ontology consultant and have to advise the clients on
ontology development for the following scenario. What would your advice be, as-
suming there are suﬃcient resources to realize it? Consider topics such as language,
reasoning services, bottom-up, top-down, methods/methodologies. *
A pharmaceutical company is in the process of developing a drug to
treat blood infections.
There are about 100 candidate-chemicals in
stock, categorised according to the BigPharmaChemicalsThesaurus, and
they need to ﬁnd out whether it meets their speciﬁcation of the ‘ideal’
drug, codename DruTopiate, that has the required features to treat that
disease (they already know that DruTopiate must have as part a benzene
ring, must be water-soluble, smaller than 1 µm, etc). Instead of ﬁnding
out by trial-and-error and test all 100 chemicals in the lab in costly
experiments, they want to ﬁlter out candidate chemicals by automatic
classiﬁcation according to those DruTopiate features, and then experi-
ment only with the few that match the desired properties. This in silico
(on-the-computer) biomedical research is intended as a pilot study, and
it is hoped that the successes obtained in related works, such as that of

7.8.
Literature and reference material
159
the protein phosphatases and ideal rubber molecules, can be achieved
also in this case.
7.8
Literature and reference material
A small selection of sample articles are the following ones, noting that there are,
at the time of writing no ‘common reference papers’ on the topic:
1. L. Lubyte, S. Tessaris. Automatic Extraction of Ontologies Wrapping Re-
lational Data Sources.
In Proc.
of the 20th International Conference on
Database and Expert Systems Applications (DEXA 2009).
2. Witte, R. Kappler, T. And Baker, C.J.O. Ontology design for biomedical
text mining. In: Semantic Web: revolutionizing knowledge discovery in the
life sciences, Baker, C.J.O., Cheung, H. (eds), Springer: New York, 2007, pp
281-313.
3. Dagobert Soergel, Boris Lauser, Anita Liang, Frehiwot Fisseha, Johannes
Keizer and Stephen Katz. Reengineering thesauri for new applications: the
AGROVOC example. Journal of Digital Information 4(4) (2004).
4. SKOS Core8, SKOS Core guide9, and the SKOS Core Vocabulary Speciﬁca-
tion10.
8http://www.w3.org/2004/02/skos/core
9http://www.w3.org/TR/swbp-skos-core-guide
10http://www.w3.org/TR/swbp-skos-core-spec


Part III
Advanced topics in ontology
engineering
161


163
Introduction
There are a myriad of advanced topics in ontology engineering, of which most re-
quire an understanding of both the logic foundations and of the modelling and
engineering, albeit that each subtopic may put more emphasis on one aspect than
another. A textbook at an introductory level cannot possibly cover all the spe-
cialised subtopics. Those included in this Block III aim to give an impression of
the many possible directions with very distinct ﬂavours and interests. They could
have been other topics as well, and it was not easy to make a selection. For instance,
machine learning is currently popular, and it is being used in ontology engineering,
yet has not been included. Likewise, ontology mapping and alignment have a set
of theories, methods, and tools drawing from various disciplines and topics that is
of interest (graph matching, similarity measures, language technologies). Perhaps
readers are interested in learning more about the various applications of ontologies
in ontology-driven information systems to be motivated more thanks to demon-
strations of some more concrete beneﬁts of ontologies in IT and computing. I do
have reasons for including the ones that have been included, though.
Ontology-Based Data Access could be seen as an application scenario of on-
tologies, yet it is also intricately linked with ontology engineering due to the rep-
resentation limitations to achieve scalability, the sort of automated reasoning one
does with it, handling the ABox, and querying ontologies, which can be done but
hasn’t been mentioned at all so far. That is, it adds new theory, methods, and
tools into an ontology engineer’s ‘knapsack’. It principally provides an answer to
the question:
• How can I have a very large ABox in my knowledge base and still have good
performance?
The second topic (in Chapter 9) is of an entirely diﬀerent nature compared to
OBDA and brings afore two ontology development issues that have so far been
ignored as well:
• What to do if one would want, say, the AWO not in English, or have it in
several languages, like name the class Isilwane or Dier rather than Animal, and
manage it all with several natural languages?
• How can one interact with domain experts in natural language, so that they
can provide knowledge and verify that what has been represented in the
ontology is what they want to have in there, without them having to learn
logic?
That is, there is an interaction between ontologies and natural language, which
oftentimes cannot be ignored.
A diﬀerent topic is the tension between the expressivity of the logic and what
one would like to—or need to—represent. Indeed, we have come across the DOL
framework (Section 4.3.2), but that does neither cover all possibilities (yet), nor
does it make immediately clear how to represent advanced features. For instance,
what if the knowledge is not ‘crisp’, i.e. either true or false, but may be true to a

164
degree? Or if one has used machine learning and induced some x, then that will
be probabilistically true and it may be nicer to represent that uncertainty aspect in
the ontology as well. Also, one of the BFO 2.x versions squeezed notions of time in
the labels of the object properties (recall Section 6.1.2), but OWL is not temporal,
so, logically, those labels have no eﬀect whatsoever. Language extensions to some
fragment of OWL and to DLs have been proposed, as there are requests for features
to represent such knowledge and reason over it. The main question this strand of
research tries to answer is:
• In what way(s) can ontology languages deal with, or be extended with, lan-
guage features, such a time and vagueness, so that one also can use those
extensions in automated reasoning (cf. workarounds with labels)?
That is, this topic has a tight interaction between modelling something even more
precisely—obtaining a better quality ontology—and not just availability of lan-
guage features and tinkering with workarounds, but actually getting them.
The ﬁnal advanced topic looks at scaling up the TBox layer. So far, we have
dealt with (very) small ontologies only, but the real ones used in information sys-
tems are typically much larger than that: they run into the thousands if not hun-
dreds of thousands of classes, which have many more axioms declared in the on-
tology. This brings afore questions regarding how to work in the best way with
large subject domains and large ontologies. Modularisation is a tried and tested
approach, and the main questions that the chapter’s contents contributes to an-
swering are:
• What is the landscape of ontology modules, with their aims, types, and char-
acteristics?
• What are good modules?
• How to take a good modular approach with large ontologies?
Chapter 11 takes a foundations and systems-oriented approach rather than logic-
based, as it was not possible to cover both in a short chapter and one ﬁrst needs
to know some foundations as to what is required before devising a logic and al-
gorithms to support ontology modularisation. There is still a lot of work to be
done on ontology modularisation, and this chapter may assist both beginners to
ﬁnd suggestions on how to approach modularisation and researchers to ﬁnd ideas
to investigate.
As mentioned in the introduction of the book, one can read either chapter in
any order, as they do not depend on each other. They are short chapters and could
perhaps have been combined into one large chapter with several sections, but that
did not look nice aesthetically. Also, it is easier for lectures to cover a whole chapter
at once and cover one topic at a time (though noting that each topic easily can
cover more than one lecture).

CHAPTER 8
Ontology-Based Data Access
Blocks I and II were rather theoretical in the sense that we have not seen many
practical application infrastructures with ontologies. This is set to change in this
chapter. We shall look at both theoretical foundations of ontology-based data access
(OBDA) and one of its realisations, and you will set up an OBDA system yourself
as an exercise. Also, this chapter will provide some technical details of the EPNet
example of food in the Mediterranean [CLM+16] that was brieﬂy described in
Section 1.3.2 as an example of ontologies for data integration in the humanities.
From an education perspective, there are several ‘starting points’ for introduc-
ing OBDA, for it depends on one’s background how one looks at it. The short
description is that it links an OWL ﬁle (the ‘ontology’ in the TBox) to lots of data
in a relational database (the ‘Abox’) by means of a newly introduced mapping
layer, which subsequently can be used for automated reasoner-enhanced ‘intelligent’
queries. The sneer quotes on ‘ontology’ have to do with the fact that, practically,
the ‘ontology’ is a logic-based simple conceptual data model formalised in ±OWL 2
QL. The sneer quotes on ‘intelligent’ refer to the fact that with the knowledge rep-
resented in the ontology/conceptual data model and an OBDA-enabled reasoner,
one can pose more advanced queries to the database in an easier way in some cases
than with just a plain relational database and SQL (although at the same time,
one cannot use all of SQL).
In any case, in this chapter we divert from the “we don’t really care about
expressivity and scalability” of Block II to a setting that is driven by the need for
scalability of ontology-driven information systems. We start with some motivations
why one should care (Section 8.1). There are many ways to address the issues of
query formulation and management, and several design choices are available even
within the OBDA approach alone; they are described in Section 8.2. Sections 8.3
and 8.4 describe one of the architectures and its principal components.
165

166
Chapter 8. Ontology-Based Data Access
8.1
Introduction: Motivations
To motivate the need for some version of an OBDA system, we start with two
perspectives: the (end-)user and the database administrator.
A database administrator’s perspective
Organisations normally have multiple databases to store and manage their data;
e.g., a PeopleSoft applications for student, course, degree, and grade management
at a university, a database with employee data, a course management system like
Moodle or Sakai for the university’s course content management, and so on. Or
take a city’s public administration that wants to develop integrated services de-
livery and the separate databases of the individual services have to be integrated
from separate electricity, sewerage, water, refuse collection, and cadastre databases.
Health information systems sometimes need to be kept separate for privacy reasons,
yet at the same time, some cross-database queries have to be executed. Hence, the
databases have to be connected in some way, and doing all that manually is a te-
dious, time-consuming task in any case. Moreover, as database administrator, you
will have to know how the data is stored in the database, write (very) large queries
that can span pages, and there is no management for recurring queries.
Instead of knowing the structure of the database(s) by heart to construct such
large queries, one can reduce the cognitive (over-)load by focusing only on what is in
the database, without having to care about if, say, the class Student has a separate
table or not, whether it uses the full name student or perhaps an abbreviation,
like stud or stdnt, or whether the data about students is split into more than one
table in more than one database. Provided the database was developed properly,
there is a conceptual data model that has exactly the representation of what kind
of data is stored in the database. Traditionally it is oﬄine and shelved after the
ﬁrst implementation. However, this need not be the case, and OBDA can ﬁll this
gap.
The case from the viewpoint of the user
Did you ever not want to bother knowing how the data is stored in a database, but
simply want to know what kind of things are stored in the database at, say, the
conceptual layer? And did you ever not want to bother having to learn SQL in order
to write queries in SQL (or, in the context of the Semantic Web, SPARQL), but
have a graphical point-and-click interface with which you can compose a query using
that ‘what layer’ of knowledge or some natural language interface such that the
system will generate automatically the SQL query for you, in the correct syntax?
(And all that not with a downloaded desktop application but in a Web browser?)
Frustrated with rigid canned queries and pre-computed queries that limit your
freedom to analyse the data? You don’t want to keep on bothering the sysadmin
for application layer updates to meet your whims and be dependent on whether
she has time for your repeated requests?
Several domain experts in genetics, healthcare informatics, and oil processing,

8.2.
OBDA design choices
167
at least, wanted that and felt constrained in what they could do with their data,
including at least pondering about, if not full desperation with, the so-called “write-
only” databases. Especially in the biology and biomedical ﬁelds, there has been
much ontology development as well as generation of much data, in parallel, which
somehow has to be linked up again.
The notions of query by diagram or conceptual queries might ﬁll this gap. These
ideas are not new [CS94, BH96, BH97, SGJR+17], but now the technologies exist
to realise it, even through a web interface and with reasoner-enabled querying. So,
now one can do a sophisticated analysis of one’s data and unlock new information
from the database by using the OBDA approach. In one experiment, this resulted in
the users—scientists conducting in silico experiments—coming up with new queries
not thought of asking before [CKN+10].
8.2
OBDA design choices
Regardless whether you needed the motivation, the commonality of both cases
described in the previous section is that it tries to ‘cut out’ several processes that
were hitherto done manually by automating them, which is one of the core aims
of computing.
In essence, the aim is to link the knowledge layer to the data
layer that contains gigabytes or even terabytes of data, and in some way still
obtain inferences. From a knowledge engineering viewpoint, this is “logic-based
knowledge representation + lots of data” where the latter is relegated to secondary
storage rather than kept in the OWL ﬁle. From a database perspective, this is seen
as “databases + background knowledge” where the latter happens to have been
represented an OWL ﬁle. The following example illustrates how the knowledge can
make a diﬀerence.
Example 8.1. Consider we have the following:
Professor(Mkhize)
//explicit data represented and stored in a database
Professor ⊑Employee
// knowledge represented in the ontology
Then what is the answer to the query “list all employees”? In a database-only
setting, it will tell you that there are no employees, i.e., return {}. In the ontology-
only setting, it wouldn’t know if there are none. With the ontology + database,
it will infer Employee(Mkhize) and return {Mkhize} as answer, which is what one
may have expected intuitively by reading the example.
♦
The question then becomes: how to combine the two? Data complexity for
OWL 2 DL is decidable but open, and for query complexity the decidability is
open (recall Table 4.2.4). Put diﬀerently: this is bad news. There are two ways we
can restrict things to get the eventual algorithms ‘well-behaved’ for scalability:
1. Restrict the TBox somehow, i.e., decrease the expressivity of the language (at
the cost of what one can model); within this option, there are two variants:
v1: Incorporate the relevant parts of the TBox into the query and evaluate
the query over a completed ABox;

168
Chapter 8. Ontology-Based Data Access
v2: Rewrite the query as needed, incorporate the TBox into the ABox, and
then evaluate the query.
2. Restrict the queries one can ask in such a way that they are only those whose
answer do not depend on the choice of model of the combination of the TBox
+ ABox.
First of all, when we’re considering limiting the TBox, we end up with some mini-
malist language like OWL 2 QL or one of the DL-Lite ﬂavours underpinning OWL
2 QL. Then, what do v1 and v2 mean for our example above? This is illustrated
in the following example.
Example 8.2. Consider again the query “list all employees” with the following:
Professor(Mkhize)
//explicit data represented and stored in a database
Professor ⊑Employee
// knowledge represented in the ontology
Option v1 will ﬁrst notice Employee’s subclass, Professor. It will then rewrite the
query so as to ask for all instances of both Employee and Professor. It then returns
{Mkhize} as answer.
Option v2 ﬁrst will notice Professor(Mkhize), and it sees Professor’s superclass
Employee. It will extend the database with a new assertion, Employee(Mkhize).
Then it will try to answer the original query. It now sees Employee(Mkhize) and
will return {Mkhize} as answer.
♦
Does this diﬀerence in process matter? Of course, it does. There are advantages
and disadvantages, as shown in Table 8.1: having to re-compute the whole extended
database can be time-consuming, as is using the knowledge of the TBox to rewrite
the query. Option 1 v1 is used more often, and we’ll see the technicalities of it in
the remainder of the chapter1.
Option 2 tends to be more used in the database world for database’s physical
design, data structures, query optimisation, and materialised views [TW11].
Table 8.1: Comparing v1 and v2 of the ‘restrict your ontology language’ option.
v1 (query rewriting)
v2 (data completion)
Queries
rewriting is exponential
in |Query|
data only grows polyno-
mially in |ABox|
Updates
applies to original data
needs
to
rematerialise
the data completion
8.3
An OBDA Architecture
Among the options described in the previous section, the architecture with its
components that we will look at here are an example of Option 1 v1 [CGL+07].
The intuitive idea for solving the sysadmin issues is depicted in Figure 8.1, top-
half (in blue): we add a “semantic layer” to a traditional database, or: we have a
1More details about the options can be found in[CGL+07, KLT+10, LTW09]

8.3.
An OBDA Architecture
169
semantic layer and store information about all individuals in the knowledge base
not in the OWL ABox but in external storage (a relational database) and create a
new link between the OWL TBox and the data store.
There are diﬀerent tools for each component that make up a realised OBDA
system. For instance, one can choose less or no reasoning, such as the Virtuoso
system2, and an RDF triple store versus relational database technology to store
the data. We shall take a look at the OBDA system (and theory behind it) that
was developed at “La Sapienza” University in Rome and Free University of Bozen-
Bolzano, Italy, which is described in [CGL+09]3.
Its principal ingredients are:
• Formal language: a language in the DL-Lite family, (roughly OWL 2 QL);
• OBDA-enabled reasoner: e.g., QuOnto [ACDG+05], Quest [RMC12];
• Data storage: an RDBMS, e.g., Oracle, PostgreSQL, DB2;
• Developer interface: OWL ontology development environment, such as Prot´eg´e
and an OBDA plugin [RMLC08], to manage the mappings and data access,
and a developer API facing toward the application to be developed;
• End-user interface: OBDA plugin for Prot´eg´e for SPARQL queries4 and
results [RMLC08], and, optionally, a system for graphical querying (e.g.,
[CKN+10, SKZ+18]).
This is shown schematically in Figure 8.1, bottom-half.
End user interface and 
OBDA management
Figure 8.1: OBDA approach and some practical components (bottom-half): the rela-
tional database, mappings, an ontology, a reasoner, and a user interface both to hide the
technicalities from the end-user and a way for the OBDA administrator to manage the
ontology and mapping layer.
2http://virtuoso.openlinksw.com/, used for, among others DBPedia
3The latest version is Ontop [CCKE+17], which has more features, but introducing those
here as well would distract for the core principles. It also compares Ontop to other OBDA and
SPARQL query answering systems and lists where the system has been used in academia and
industry.
4SPARQL is a query language, alike SQL but then for querying OWL and RDF. The W3C
speciﬁcation of SPARQL can be found at http://www.w3.org/TR/rdf-sparql-query/.

170
Chapter 8. Ontology-Based Data Access
8.4
Principal components
The theoretical details are quite involved, and relevant papers span hundreds of
pages. In essence, there are two principal aspects to it:
1. Ontology-Based Data Access systems (static components):
• An ontology language, for representing the ontology
• A mapping language, for declaring the mappings between vocabulary in
the ontology and the data
• The data
2. Query answering in Ontology-Based Data Access systems:
• Reasoning over the TBox
• Query rewriting
• Query unfolding
• Relational database technology
The mapping language and the items under 2 are new notions compared Blocks
I and II and a regular database course, which will be described in the following
subsections.
The ontology language
For the language, we remain, roughly, within the Semantic Web setting, and take a
closer look at the OWL 2 QL proﬁle and similar languages in the “DL-lite” family
that is at the basis of OWL 2 QL, and DL-LiteA in particular [CGL+07]. Most sig-
niﬁcantly, the trade-oﬀbetween expressive power and computational complexity of
the reasoning services leans strongly towards the scalability of reasoning (including
query answering) over large amounts of data; or: one can’t say a lot with either
OWL 2 QL or DL-LiteA.
Syntax of DL-LiteA.
As is common in DLs and OWL, we distinguish between
(abstract) objects and (data) values. A class expression denotes a set of objects,
a datatype denotes a set of values, an object property denotes a binary relationship
between objects, and a data property denotes a binary relation between objects and
values. We assume to have a set {T1, . . . , Tn} of pairwise disjoint and unbounded
datatypes, each denoting a set val(Ti) of values (integers, strings, etc.), and ⊤d
denotes the set of all values. Class expressions, C, and object property expressions,
R, are formed according to the following syntax, where A denotes a class, P an
object property, and U a data property:
C −→A | ∃R | δ(U),
R −→P | P −.
Observe that ∃R denotes an unqualiﬁed existential class expression, δ(U) denotes
the domain of U, and P −denotes the inverse of P.

8.4.
Principal components
171
A DL-LiteA ontology O = ⟨T , A⟩, consists of a TBox T and an ABox A, where
the TBox is constituted by a set of axioms of the form
C1 ⊑C2,
(disj C1 C2),
ρ(U) ⊑Ti,
R1 ⊑R2,
(disj R1 R2),
(funct R),
U1 ⊑U2,
(disj U1 U2),
(funct U).
The axioms in the ﬁrst row denote inclusions, with ρ(U) the range of U. The
axioms in the second row denote disjointness; note that distinct datatypes are
assumed to be disjoint.
The axioms in the third row denote functionality (at
most one) of an object property expression and of a data property expression,
respectively. The ABox is constituted by a set of assertions of the form A(a),
P(a, a′), and U(a, ℓ), where a, a′ are individuals denoting objects and ℓis a literal
(denoting a value). To ensure that DL-LiteA maintains the computationally well-
behaved computational properties of the DL-Lite family [CGL+07], the form of
the TBox has to be restricted (as we have seen for OWL 2 QL in Chapter 3). In
particular, object and data properties occurring in functionality assertions cannot
be specialized, i.e., the cannot appear in the right hand side of an inclusion axiom.
Semantics of DL-LiteA.
The semantics of DL-LiteA is based on ﬁrst-order
interpretations I = (∆I, ·I), where ∆I is a nonempty interpretation domain, which
is partitioned into a ∆I
O of objects, and a ∆I
V of values. The interpretation function
·I maps each individual a to aI ∈∆I
O, each class A to AI ⊆∆I
O, each object
property P to P I ⊆∆I
O × ∆I
O, and each data property U to U I ⊆∆I
O × ∆I
V ,
whereas each literal ℓis interpreted as the value ℓI = val(ℓ), each datatype Ti as
the set of values T I
i = val(Ti), and ⊤I
d = ∆I
V . The semantics of expressions:
(∃R)I = {o | ∃o′. (o, o′) ∈RI},
(P −)I = {(o, o′) | (o′, o) ∈P I},
(δ(U))I = {o | ∃v. (o, v) ∈U I},
(ρ(U))I = {v | ∃o. (o, v) ∈U I}.
Contrary to OWL, DL-LiteA (and its implementation) adopts the unique name
assumption, meaning that for every interpretation I and distinct individuals or
values c1, c2, we have that cI
1 ̸= cI
2 (which is the norm in the database setting).
As for other DLs and OWL species, I satisﬁes α1 ⊑α2 if αI
1 ⊆αI
2, it satisﬁes
(disj α1 α2) if αI
1 ∩αI
2 = ∅, and it satisﬁes (funct S) if SI is a function (that is, if
(o, z1) ∈SI and (o, z2) ∈SI, then z1 = z2)). I satisﬁes A(a) if aI ∈AI, it satisﬁes
P(a, a′) if (aI, a′I) ∈P I, and it satisﬁes U(a, ℓ) if (aI, val(ℓ)) ∈U I.
Mappings
Here, a few deﬁnitions and an example is included; the chapter literature contains
further technical details and more examples of mappings.
Deﬁnition 8.1 (Mapping assertion between a database and a TBox). A mapping
assertion between a database D and a TBox T has the form Φ ⇝Ψ where
• Φ is an arbitrary SQL query of arity n > 0 over D;

172
Chapter 8. Ontology-Based Data Access
• Ψ is a conjunctive query over T of arity n′ > 0 without non-distinguished
variables, possibly involving variable terms.
Deﬁnition 8.2 (Mapping assertion in M in an OBDA system). A mapping as-
sertion between a database D and a TBox T in M has the form Φ(⃗x) ⇝Ψ(⃗t, ⃗y)
where
• Φ is an arbitrary SQL query of arity n > 0 over D;
• Ψ is a conjunctive query over T of arity n′ > 0 without non-distinguished
variables;
• ⃗x, ⃗y are variables with ⃗y ⊆⃗x;
• ⃗t are variable terms of the form f(⃗z), with f ∈Λ and ⃗z ⊆⃗x.
Concerning the semantics of mappings, intuitively: I satisﬁes Φ ⇝Ψ with
respect to D if all facts obtained by evaluating Φ over D and then propagating
answers to Ψ, hold in I.
Deﬁnition 8.3 (Satisfaction of a mapping assertion with respect to a database).
An interpretation I satisﬁes a mapping assertion Φ(⃗x) ⇝Ψ(⃗t, ⃗y) in M with respect
to a database D, if for each tuple of values ⃗v ∈Eval(Φ, D), and for each ground
atom in Ψ[⃗x/⃗v], we have that:
• If the ground atom is A(s), then sI ∈AI;
• If the ground atom is P(s1, s2), then (sI
1, sI
2) ∈P I.
(Note: Eval(Φ, D) denotes the result of evaluating Φ over D, Ψ[⃗x/⃗v] denotes
Ψ where each xi is substituted with vi)
An example is shown in Figure 8.2 with the OBDA plugin for Prot´eg´e. There
is an ontology that happens to have a class PromiscuousBacterium, among other
things, and a relational database (HGT) with several tables, such as organisme
and flexcount. Now we have to link the two with a mapping, which means (i)
constructing a database query such that it retrieves only the promiscuous bacteria,
and (ii) solving the ‘impedance mismatch’ (recollect Chapter 7) with a functor so
that the values returned by the database query become objects in the ontology,
which is what getPromBact does. Informally, the functor can be considered as a
URI building mechanism for individuals in the ontology taken from the database
(theoretically, they are skolem functions).
Query answering
Recall the outline of the adolena ontology from the exercise and Figure 7.9. A
query could be “retrieve the devices that ameliorate paraplegia”
q(x) :- Device(x), ameliorates(x,y), Paraplegia(y)
For this to work, we have to introduce three aspects. First, we need a computer-
processable serialization of the query, a notion of what kind of queries we can pose,
and a way how it will do the answering. The query language is SPARQL (see
footnote 4). The kind of queries is (unions of) conjunctive queries. A conjunctive
query (CQ) q over an ontology O is an expression of the form q(⃗x) ←∃⃗y.conj(⃗x, ⃗y),
where q(⃗x) the head, conj(⃗x, ⃗y) the body, the variables in ⃗x are distinguished vari-
ables and ⃗y the non-distinguished variables, and where conj(⃗x, ⃗y) is a conjunction

8.4.
Principal components
173
OWL class in the ontology
SQL Query over the database
Figure 8.2: An example of a mapping; see text for explanation. (Source: based on
[Kee10c])
of atoms of the form D(z), S(z, z′), z = z′, where D denotes a class or a datatype,
S an object property or data property in O, and z, z′ are individuals or literals in
O or variables in ⃗x or ⃗y. Given an interpretation I = (∆I, ·I), then qI is the set of
tuples of ∆I that, when assigned to the variables ⃗x, make the formula ∃⃗y.conj(⃗x, ⃗y)
true in I. The set cert(q, O)—certain answers to q over O—is the set of tuples ⃗a
of individuals or literals appearing in O such that ⃗aI ∈qI, for every model I of O.
Regarding query answering, consider Figure 8.3, where q is our query, T the
TBox (ontology or formal conceptual data model), and A the ABox (our instances,
practically stored in the relational database). Somehow, this combines to produce
the answers, using all components. First, there is a “reformulation” (or: rewrit-
ing) step, which computes the perfect reformulation (rewriting), qpr, of the original
query q using the inclusion assertions of T so that we have a union of conjunctive
queries. That is, it uses the knowledge of the ontology to come up with the ‘real’
query. For instance, recollect Figure 7.9 about the ontology of abilities and disabil-
ities, then a query “retrieve all Devices that assistWith UpperLimbMobility”, i.e.,
q(x) :- Device(x), assistsWith(x,y), UpperLimbMobility(y)
or, in SPARQL notation:
SELECT $device
WHERE {$device rdf:type :Device.
$device :assistsWith $y.
$y rdf:type :UpperLimbMobility}
will traverse the hierarchy of devices until it ﬁnds those devices that have an object
property declared with as range UpperLimbMobility. In this case, this is Motorised-
Wheelchair, hence, the ‘real’ query concerns only the retrieval of the motorised
wheelchairs, not ﬁrst retrieving all devices and then making the selection.

174
Chapter 8. Ontology-Based Data Access
Second, the “unfolding” step computes a new query, qunf, by using the (split
version of) the mappings that link the terms of the ontology to queries over the
database.
Third, the “evaluation” step delegates the evaluation of qunf to the relational
DBMS managing the data, which subsequently returns the answer.
Figure 8.3: Intuition of the top-down approach to query answering. (Source: Slides of
D. Calvanese)
Examples: a learning curve
Early attempts to set up an OBDA system are described in [KAGC08] and [CKN+10],
which provided useful learning moments [CCKE+17]. The ontology in [KAGC08]
was developed separately from the existing database, which caused some mis-
matches on coverage. Put diﬀerently, the content in the ontology and database
really do have to be compatible. This was much better aligned in the experiment
described in [CKN+10]: the OWL ﬁle/ontology was a reverse engineered physical
(SQL) schema and dressed up with modelling improvements, having followed the
steps described in Section 7.1. A downside with the shape of the model in this par-
ticular scenario, was that there was a considerable hierarchy, so the system ended
up creating very big conjunctive queries, which slowed down the database side in
actually answering the SQL query (albeit still faster than the original database).
This has been optimised in the meantime, among other things [CCKE+17].
More recent examples from industry and with the ‘next generation’ OBDA, i.e.,
the extended Ontop setting, can be found in, e.g., [KHS+17], describing industry’s
motivations, mappings, a rather large SQL query, and streaming (i.e., temporal)
data, and there are several references in [CCKE+17] that describe applications
and usage of Ontop. It is, perhaps, also worth mentioning that OBDA has seen
many people involved since the early days in 2005, and by now has been shown
to be robust, albeit not a trivial matter to understand and set up from a system’s
development viewpoint. You can practice with this in the Chapter’s exercises, since
the software is open source.
8.5
Exercises
Review question 8.1. Describe in your own words what an OBDA system is.
Try to keep it short, such that you can explain it to someone in person in less than
30 seconds.

8.6.
Literature and reference material
175
Review question 8.2. What are the principal components of the OBDA system
described in some detail in this chapter?
Review question 8.3. How is querying in the OBDA setting diﬀerent compared
to plain relational database?
Exercise 8.1. Inspect an ontology used for OBDA; e.g.,: the one of the EPnet5
or one from the Ontop examples6.
Consider again Section 1.2.1 regarding the
diﬀerences between ontologies and conceptual models and Chapter 6 on founda-
tional ontologies. Why does the ‘ontology’ in an OBDA system look more like an
OWLized conceptual data model? *
Exercise 8.2. You will set up an OBDA system. For a generic topic, such as
movies, you can consult the wiki at https://github.com/ontop/ontop/wiki for
tutorials and sample data, and the software at https://github.com/ontop/ontop;
for an OBDA tutorial with South African wildlife—mainly research data about ele-
phants in SA’s national parks—you can get a tutorial from the textbook’s home-
page.
8.6
Literature and reference material
1. Diego Calvanese, Giuseppe De Giacomo, Domenico Lembo, Maurizio Lenz-
erini, Antonella Poggi, Mariano Rodriguez-Muro, and Riccardo Rosati. On-
tologies and databases: The DL-Lite approach. In Sergio Tessaris and Enrico
Franconi, editors, Semantic Technologies for Information Systems - 5th Int.
Reasoning Web Summer School (RW 2009), volume 5689 of Lecture Notes in
Computer Science, pages 255-356. Springer, 2009.
2. Calvanese, D., Keet, C.M., Nutt, W., Rodr´ıguez-Muro, M., Stefanoni, G.
Web-based Graphical Querying of Databases through an Ontology: the WON-
DER System.
ACM Symposium on Applied Computing (ACM SAC’10),
March 22-26 2010, Sierre, Switzerland.
3. D. Calvanese, P. Liuzzo, A. Mosca, J. Remesal, M. Rezk, and G. Rull.
Ontology-based data integration in EPnet: Production and distribution of
food during the roman empire. Engineering Applications of Artiﬁcial Intelli-
gence, 51:212-229, 2016.
5http://romanopendata.eu/sparql/doc/index.html
6e.g., movies: https://github.com/ontop/ontop/wiki/Example_MovieOntology


CHAPTER 9
Ontologies and natural languages
The interaction of ontologies with natural language processing and even more so
at the fundamental level—Ontology and language—can be good for many conver-
sations and debates about long-standing controversies1. In this chapter, we shall
focus on the former and take an engineering perspective to it. In this case we can
do that, because the points of disagreement are ‘outside’ the ontology as artefact,
in a similar way as an OWL ﬁle is indiﬀerent as to whether a human who reads
the ﬁle assumes a particular OWL class to be a concept or a universal. Regard-
less whether you are convinced the reality is shaped, or even created, by language,
or not, there are concrete issues that have to be resolved anyhow. For instance,
the OWL standard had as one of the design goals “internationalisation” (recall
Section 4.1.1), which presumably means ontologies being able to handle multiple
languages. How is that supposed to work? Anyone who speaks more than one
language will know there are many words that do not have a simple 1:1 translation
of the vocabulary. What can, or should, one do with those non-1:1 cases when
translating an ontology in one’s own language or from one’s own language into,
say, English? Where exactly is a good place to record natural language informa-
tion pertaining to the vocabulary elements? How to manage all those versions in
diﬀerent languages? The area of multilingual ontologies aims to ﬁnd answers to
such questions, which will be introduced in Section 9.1.
A quite diﬀerent type of interaction between ontologies and languages is that of
ontologies and Controlled Natural Language (CNL), which may avail of multilingual
ontologies to a greater or lesser extent. One can verbalise—or: put into (pseudo-
)natural language sentences—the knowledge represented in the ontology. This can
be useful for, among others: interaction with domain expert during the knowledge
acquisition and veriﬁcation phases, automatically generating documentation about
the ontology, and anywhere where the ontology is being used in an ontology-driven
information system (e.g., SNOMED CT with an electronic health records system).
1Debates include the topic whether language shapes reality and therewith ontology or is used
to approximate describing the world.
177

178
Chapter 9. Ontologies and natural languages
The general idea will be described in Section 9.2.
Besides these two interactions between ontology and natural language, and
the use of NLP for ontology learning that we have seen in Section 7.4, there are
also areas of research and working technologies where ontologies enhance NLP
applications2. This version of the textbook does not include a separate section
on such ontology-driven information systems, however, for the scope is ontology
engineering. It is duly acknowledged that some of the solutions that came out of
the application areas can be useful for the aforementioned tasks, and the Semantic
Web as application area in particular. This because the Web is global, so it makes
sense to create a Multilingual Semantic Web (see the recently published handbook
[BC14]). It turned out there are some inherent problems of the original vision of
Berners-Leet et al. [BLHL01] to overcome [Hir14], and insights gained there assist
with solutions for ontologies in the general case, regardless whether that is within
the Semantic Web with OWL or another logic and technology stack.
9.1
Toward multilingual ontologies
Let us ﬁrst have a look at just one natural language and an ontology (Section 9.1.1)
before complicating matters with multiple languages in Section 9.1.2.
9.1.1
Linking a lexicon to an ontology
Most, if not all, ontologies you will have inspected and all examples given in the
preceding chapters simply gave a human-readable name to the DL concept or OWL
class. Perhaps you have loaded an ontology in the ODE, and the class hierarchy
showed numbers, alike GO:00012345, and you had to check the class’s annotation
what was actually meant with that identiﬁer. This is an example of a practical
diﬀerence between OBO and OWL (recall Section 7.1), but which is based on
diﬀerent underlying modelling principles. DLs assume that a concept is identiﬁed by
the name given to it; that is, there is a 1:1 correspondence between what a concept,
say, being a vegetarian, means and the name we give to it. Natural language and
the knowledge, reality etc. are thus tightly connected and, perhaps, even conﬂated.
Not everybody agrees with that underlying assumption. An alternative viewpoint
is to assume that there are language-independent entities—i.e., they exist regardless
whether humans name them or not—that somehow have to be identiﬁed and then
one sticks one or more labels or names to it. Put diﬀerently: knowledge is one
thing and natural language another, and they should be kept as distinct kinds of
things.
The second view is prevailing at least within the ontology engineering arena.
To date, two engineering solutions have been proposed how to handle this in an
ontology. The ﬁrst one is the OBO solution, which was used since its inception
2E.g., it has been shown to enhance precision and recall of queries (including enhancing di-
alogue systems [VF09]), to sort results of an information retrieval query to the digital library
[DAA+08], (biomedical) text mining, and annotating textbooks for ease of navigation and auto-
mated question generation [CCO+13] as an example of adaptive e-learning.

9.1.
Toward multilingual ontologies
179
in 1998 by the Gene Ontology Consortium [Gen00]: each language-independent
entity gets an identiﬁer and it must have at least one label that is human-readable.
This solution clearly allows also for easy recording of synonyms, variants, and
abbreviations, which were commonplace in genetics especially at the inception of
the GO.
The second option emerged within the Semantic Web ﬁeld [BCHM09]. It also
acknowledges that distinction between the knowledge layer and the language layer,
yet it places the latter explicitly on top of the knowledge layer. This has as eﬀect
that the solution does not ‘overload’ OWL’s annotation ﬁelds, but proposes to store
all the language and linguistic information in a separate ﬁle that interacts with the
OWL ﬁle. How that separate ﬁle should look like, what information should be
stored in it, and how it should interact with the OWL ﬁle is open to manifold
possible solutions.
One such proposal will be described here to illustrate how
something like that may work, which is the Lemon model [MdCB+12, MAdCB+12],
of which a fragment has been accepted as a community standard by the W3C3. It
also aims to cater for multilingual ontologies.
Consider the Lemon model as depicted in Figure 9.1, which depicts the kind of
things one can make annotations of, and how those elements relate to each other.
At the bottom-center of the ﬁgure, there is the ontology to which the language
information is linked. Each vocabulary element of the ontology will have an entry
in the Lemon ﬁle, with more or less lexical information, among others: in which
sense that word is meant, what the surface string is, the POS tag (noun, noun
Figure 9.1: The Lemon model for multilingual ontologies (Source: [MdCB+12])
3https://www.w3.org/community/ontolex/wiki/Final_Model_Specification

180
Chapter 9. Ontologies and natural languages
phrase, verb etc.), gender, case and related properties (if applicable).
A simple entry in the Lemon ﬁle could look like this, which lists, in sequence:
the location of the lexicon, the location of the ontology, the location of the Lemon
speciﬁcation, the lexical entry (including stating in which language the entry is),
and then the link to the class in the OWL ontology:
@base <http://www.example.org/lexicon>
@prefix ontology: <http://www.example.org/AfricanWildlinfeOntology1#>
@prefix lemon: <http://www.monnetproject.eu/lemon#>
:myLexicon a lemon:Lexicon ;
lemon:language "en" ;
lemon:entry :animal .
:animal a lemon:LexicalEntry ;
lemon:form [ lemon:writtenRep "animal"@en ] ;
lemon:sense [ lemon:reference AfricanWildlinfeOntology1:animal ] .
One can also specify rules in the Lemon ﬁle, such as how to generate the plural
from a singular. However, because the approach is principally a declarative speciﬁ-
cation, it is not as well equipped at handling rules compared to the well-established
grammar systems for NLP. Also, while Lemon covers a fairly wide range of language
features, it may not cover all that is needed; e.g., the noun class system emblematic
for the indigenous language spoken in a large part of sub-Saharan Africa does not
quite ﬁt [CK14]. Nonetheless, Lemon, and other proposals with a similar idea of
separation of concerns, are a distinct step forward for ontology engineering where
interaction with languages is a requirement. Such a separation of concerns is even
more important when the scope is broadened to a multilingual setting, which is the
topic of the next section.
9.1.2
Multiple natural languages
Although this textbook is written in one language, English, for it is currently the
dominant language in science, the vast majority of people in the world speak an-
other language and they both have information systems in their own language as
well as that they may develop an ontology in their own language, or else localise an
ontology into their own language. One could just develop the ontology in one’s own
language in the same way as the examples were given in English in the previous
chapters and be done with it. But what if, say, SNOMED CT should be translated
in one’s own language for electronic health records, like with OpenMRS [Ope], or
the ontology has to import an existing ontology that happens to be not represented
in the target language and compatibility with the original ontology has to be main-
tained? What if some named class is not translatable into one single term? For
instance, in French, there are two words for the English ‘river’: one for a river that
ends in the sea and another word for a river that doesn’t (ﬂeuve and rivi`ere), and
isiZulu has two words and corresponding meanings for the participation relation:
one as we have seen in Section 6.2 and another for participation of collectives in a
process (-hlanganyela).

9.1.
Toward multilingual ontologies
181
Several approaches have been proposed for the multilingual setting, for both
localisation and internationalisation of the ontology with links to the original on-
tology and multiple languages at the same time in the same system. The simplest
approach is known as semantic tagging. This means that the ontology is developed
‘in English’, i.e., naming the vocabulary elements in one language and for other lan-
guages, labels are added, such as Fakult¨at and Fakulteit for the US-English School.
This may be politically undesirable and anyhow it does not solve the issue of non-
1:1 mappings of vocabulary elements. It might be a quick ‘smart’ solution if you’re
lucky (i.e., there happen to be only 1:1 mappings for the vocabulary elements in
your ontology), but a solid reusable solution it certainly is not. OBO’s approach
of IDs and labels avoids the language politics: one ID with multiple labels for each
language, so that it at least treats all the natural languages as equals.
However, both falter as soon as there is no neat 1:1 translation of a term into
another single term in a diﬀerent language—which is quite often the case except
for very similar languages—though within the scientiﬁc realm, this is much less of
an issue, where handling synonyms may be more relevant.
One step forward is a mildly “lexicalised ontology” [BCHM09], of which an ex-
ample is depicted in Figure 9.2. Although it still conﬂates the entity and its name
and promotes one language as the primary, at least the handling of other languages
is much more extensive and, at least in theory, will be able to cope with multilin-
gual ontologies to a greater extent. This is thanks to its relatively comprehensive
information about the lexical aspects in its own linguistic ontology, with the Word-
Form etc., which is positioned orthogonally to the domain ontology. In Figure 9.2,
the English OralMucosa has its equivalent in German as Mundschleimhaut, which
is composed here of two sub-words that are nouns themselves, Mund ‘mouth’ and
Schleimhaut ‘mucosa’. It is this idea that has been made more precise and com-
prehensive in its successor, the Lemon model, that is tailored to the Semantic
Web setting [MdCB+12]. Indeed, the same Lemon from the previous section. The
Lemon entries can become quite large for multiple languages and, as it uses RDF
for the serialisation, it is not easily readable.
An example for the class Cat in
English, French, and German is shown diagrammatically in Figure 9.3, and two
annotated short entries of the Friend Of A Friend (FOAF)4 structured vocabulary
in Chichewa (a language spoken in Malawi) are shown in Figure 9.4.
There are only few tools that can cope with ontologies and multiple languages.
It would be better if at least some version of language management were to be
integrated in ODEs. At present, to the best of my knowledge, only MoKI pro-
vided such a service partially for a few languages, inclusive of a localised interface
[BDFG14].
The following example illustrates some actual (unsuccessful) ‘struggling’ trying
to handle a more complex case than ‘dog = chien = inja’ when there is not even
a name for the entity in the other language; a more extensive list of the type of
issues can be found in [LAF14].
Example 9.1 ([AFK12]). South Africa has a project on indigenous knowledge
management systems, but the example can easily be generalised to cultural his-
4http://xmlns.com/foaf/spec/

182
Chapter 9. Ontologies and natural languages
Figure
9.2:
Illustration of the idea of semantic tagging with lexicalised on-
tologies.
(Source: http://www.deri.ie/ﬁleadmin/documents/teaching/tutorials/DERI-
Tutorial-NLP.ﬁnal.pdf)
toric museum curation in any country (AI for cultural heritage). Take ingcula (in
isiZulu), which is a ‘small bladed hunting spear’, that has no equivalent term in
English. Trying to represent that description, i.e., adding it not as a single class but
as a set of axioms, then one could introduce a class Spear that has two properties,
e.g., Spear ⊑∃hasShape.Bladed ⊓∃participatesIn.Hunting. To represent the ‘small’,
one could resort to fuzzy concepts; e.g., following [BS11]’s fuzzy OWL notation5,
then, e.g.,
MesoscopicSmall : Natural →[0, 1] is a fuzzy datatype,
MesoscopicSmall(x) = trz(x, 1, 5, 13, 20), with trz the trapezoidal function,
so that a small spear can be deﬁned as
SmallSpear ≡Spear ⊓∃size.MesoscopicSmall
Then one can create a class in English and declare something alike
SmallBladedHuntingSpear ≡SmallSpear ⊓∃hasShape.Bladed ⊓
∃participatesIn.Hunting
This is just one of the possibilities of a formalised transliteration of an English
natural language description, not a deﬁnition of ingcula as it may appear in an
ontology about indigenous knowledge of hunting.
Let’s assume for now the developer does want to go in this direction, then it
requires more advanced capabilities than even lexicalised ontologies to keep the
two ontologies in sync: lexicalised ontologies only link dictionaries and related
annotations to the ontologies, but here one now would need to map sets of axioms
between ontologies. ♦
That is, what was intended as a translation exercise ended up as a diﬀerent
5Plain OWL cannot deal with this, though, for it deals with crisp knowledge only. Refer to
Section 10.1.1 for some notes on fuzzy ontologies

9.1.
Toward multilingual ontologies
183
Figure 9.3: Illustration of the Lemon model for multilingual ontologies to represent the
class Dog (Source: adapted from [MdCB+12])
ontology ﬁle at least and, arguably, possibly also a diﬀerent conceptualisation.
It gets even more interesting in multilingual organisations and societies, such as
the European Union with over 20 languages and South Africa with its 11 oﬃcial
languages, for that requires some way of managing all those versions.
As a ﬁnal note: those non-1:1 mappings of the form of having one class in ontol-
ogy O1 and one or more axioms in O2, sets of axioms in both, like in Example 9.1
with the hunting spear, as well as non-1:1 property alignments, are feasible by now
with the (mostly) theoretical results presented in [FK17, Kee17b], so this ingcula
example could be solved in theory at least. Its details are not pursued here, because
it intersects with the topic of ontology alignment. Further, one may counter that
an alternative might be to SKOSify it, for it would avoid the complex mapping
between a named class to a set of axioms. However, then the diﬀerences would be
hidden in the label of the concepts rather than solving the modelling problem.
9.1.3
Infrastructure for multilingual, localised, or interna-
tionalised ontologies
After reading the previous section, you may be interested in either developing an
ontology in/for multiple languages, or perhaps localise an existing ontology, or
make your local ontology accessible to a wider audience. What is the best way
to develop a lexicalised ontology? How to orchestrate and maintain an ontology
in multiple languages? Which tools to use for the development and maintenance
of multilingual ontologies? At the time of writing, there are no widely-established
practices, guidelines, or tools. Research is being carried out to ﬁnd good answers
and proof-of concept tools and case studies are available that document experiences,
in the hope that it will feed into some systematic solution.
For instance, one can develop multilingual structured vocabularies with VocBench,
which incorporates lemon, the Food and Agriculture Organisation’s AGROVOC is

184
Chapter 9. Ontologies and natural languages
Noun class; governs other 
language aspects, such as prefix
singular and plural declarations
for the noun
Connection to the term in FOAF 
“@ny” for the language 
(Chichewa/Nyanga)
“Theme” in Chichewa as lexical entry
POS tag
Figure 9.4:
Annotated small section of part of the FOAF in Chichewa.
(Source:
http://www.meteck.org/files/ontologies/foaf.ttl, discussed in [CK14].
available in multiple languages, and there are ‘chimaera’ ontologies that, for lack of
a better solution, mix ontologies in diﬀerent languages6. Also, Prot´eg´e can recog-
nise the language tag used for the labels so that if one has the operating system
set to that language, it will render that label, if declared. For instance, you could
check out the Core Organisation Ontology from the W3C, set your system on, say,
French, and when you then load the ontology, then the GUI renders the elements
with the labels tagged with @fr, showing, e.g., Organisation Formelle rather than
the default Formal Organization.
That last example of Prot´eg´e touches upon another aspect more generally: lo-
calisation of the ODE’s interface in one’s language so as to facilitate full immersion
rather than code switching during the ontology authoring stage. Depending on the
ODE, there may be any or all of the following components to consider as input for
translation:
• OWL 2 keywords, such as Class, ObjectProperty, SomeValuesFrom that may
be used in the interface or for reading Turtle notation;
• Manchester OWL Syntax;
• The regular and ontology-speciﬁc technical terms used in the GUI, such as
Refactoring, Reasoner, and Inferred Hierarchy.
One could avoid translations of the symbols by using DL notation (recall Exer-
cise 3.3), but let’s assume you want to see text rather than symbols. Either way,
6e.g., the GeoLinked data mixes English and Spanish, taking ontologies from diﬀerent sources.
The ontology can be inspected at: http://dgarijo.github.io/Vocabularies/geolinkeddata/
doc/geolinkeddata.owl (last accessed: 15-11-2019).

9.1.
Toward multilingual ontologies
185
it still would require resolving the third item listed above. Here, we’ll explore the
ﬁrst two items and highlight some interesting curiosities and hurdles, to eventu-
ally end up with a judgement on the “internationalisation” goal of OWL (recall
Section 4.1.1).
Let us ﬁrst consider the ‘easy’ case of disjunctive languages, i.e., where diﬀerent
parts of speech are written as separate words, cf. the agglutinating languages that
string things together. For illustrative purpose we take Spanish and Afrikaans: the
former is a widely-spoken language globally and the latter is a language spoken
widely in the province where this author lives when writing this section.
The OWL 2 keywords were easy to translate for the class axioms in both cases;
e.g., SubClass Of translates as Subclase de and Subklas van, respectively. For
the object properties, the only real deliberation was regarding the translation of
‘property’ in Afrikaans, since the literal translation is eienskap, which is of an entity
itself, but not really relational as OWL’s object property is used. Eventually, a
semantic translation was decided upon (verwantskap).
The Manchester OWL Syntax (MOS) translations showed to be more interesting
already for the class constructors. They are listed in Table 9.1. There are three
observations on diﬀerences, and consequences for tool development:
• MOS’s keywords in English are all single words, whereas this is not the case
for Spanish and Afrikaans.
• Spanish uses diacritics; e.g., s´olo.
• Afrikaans has double negation that wraps around an expression, which is
either disjointness of classes (e.g., ’n dier is nie ’n plant nie. ‘an animal is
not a plant’) or in the context of object properties (as in Elke CE1 (is CE2
en nie OP1 nie))
UTF encodings and tools have improved over the years so that the diacritics are
processed with ease. The multi-words and disconnected-strings features, on the
other hand, are obviously an issue for parsing MOS ﬁles if it were really to be
‘Spanish MOS’, ‘Afrikaans MOS’ etc. To avoid that, the only option is to render
MOS in Spanish/Afrikaans in the interface only and not also in the serialisation.
This is exactly what the Prot´eg´e plugin for Spanish and Afrikaans does7. However,
if the localisation is only achieved through the rendering in a GUI, then one might
as well omit MOS and render straight from the RDF/XML required syntax, rather
than through an optional syntax. Prot´eg´e speciﬁcally does use MOS in its interface,
however; hence, the translations.
This improves the rendering of axioms of an
ontology in Spanish from, e.g.,
jirafa come only (oja or ramita)
into a full pseudo-natural Spanish, alike:
jirafa come s´olo (oja o ramita)
On the ﬂip side, the Spanish for ‘or’, o, is the same as the ‘o’ used in declaring
property chains. This should be clear from the context of use, but a novice will
have to be made aware of the diﬀerent semantics of those o’s.
7the plugins are downloadable from https://github.com/mkeet/ODElocalisation

186
Chapter 9. Ontologies and natural languages
Table 9.1: Manchester OWL Syntax (MOS) and two sets of translations.
MOS
MOS in Spanish
MOS in Afrikaans
some
al menos uno
sommige
only
s´olo
slegs
min
al m´ınimo
ten minste
max
al m´aximo
by die meeste
exactly
exactamente
precies
and
y
en
or
o
of
not
no
nie <expression> nie
SubClassOf
SubclaseDe
SubklasVan
EquivalentTo
EquivalenteA
DieselfdeAs
DisjointWith
DisjuntoCon
DisjunkteVan
The ‘MOS approach’ becomes more challenging with agglutinating languages
and those that have many genders or noun classes that determine surrounding
words. For instance, there are seven variants for ‘at least one’ in isiZulu (the Zulu
language), depending on the noun class of the noun that is quantiﬁes over; e.g., it
is elilodwa in the case of ‘at least one’ apple and esisodwa in the context of ‘at least
one’ twig. This is still processable with a more comprehensive plugin that avails
of additional annotations and rules and that operate on the ‘syntactic sugar’ layer
only as well, not for generating a serialisation in isiZulu as that would challenge
the parser even more.
Overall, one could thus already argue that it is not going well for OWL with
respect to meeting the internationalisation goal. There are other complications,
which have to do with the object properties. For an ‘English ontology’, it is common
practice to name an object property with a verb in the 3rd person singular and
include any prepositions in the name of the object property. This makes sense
for English, but not in various other languages. Lithuanian puts the preposition
with the noun at the end and isiZulu puts it at the start of the noun, rather than
with the verb. That is, the element’s labels are not ﬁxed, but context-dependent.
To complicate matters further, there may be gender/noun class-based inﬂections
(which there are for isiZulu and similar languages), which also contributes to not
having a ﬁxed string for name. The worst case we have encountered so far, is that
for ‘contained in’, there is no string for the object property at all: a preﬁx and a
suﬃx is added to the noun that plays the container role to indicate it is the object
containing the subject. Yet, by default—baked into the assumptions in the OWL
standard—an object property must have a name. Indeed, one can use identiﬁers
as names for entities, but this then always will require additional label processing.
This, in turn, may result in either two-tier tools—those that do have that extra
functionality with label processing, and those that do not—or one dominant one for
English and many disconnected attempts for all the other languages. The current
state at the time of writing is (still) the latter.
In closing, the development of multilingual ontologies with suitable tooling is

9.2.
Ontology verbalisation
187
still a challenge.
9.2
Ontology verbalisation
The second topic of ontologies & natural language—ontology verbalisation—can
build upon the previous, but need not if you happen to be interested in a grammat-
ically ‘simple’ language, such as English. The core notion is to generate (pseudo-)
natural language sentences from the axioms in the ontology. The introduction to
Block III already noted that ontology verbalisation may be useful for, mainly:
• Ameliorating the knowledge acquisition bottleneck; it:
– helps the domain experts understand what exactly has been represented
in the ontology, and thereby provides a means to validate that what has
been represented is what was intended;
– can be used to write axioms in pseudo-natural language, rather than the
logic itself, to develop the ontology.
• Some ontology-driven information system purposes; e.g., e-learning (question
generation, textbook search), readable medical information from medical ter-
minologies in electronic health record systems.
The term ‘ontology verbalisation’ is rather speciﬁc and restricted, and it falls within
the scope of Controlled Natural Language (CNL) and Natural Language Genera-
tion ﬁelds of research (NLG). While most papers on ontology verbalisation will
not elaborate on the methodological approach speciﬁcally, there are clear parallels
with the activities of the traditional NLG pipeline alike proposed in [RD97]. The
pipeline is summarised in Figure 9.5, top section, and the corresponding answers
for ontology verbalisation are described in the bottom section of the ﬁgure.
9.2.1
Template-based approach
There are three principal approaches to generate the sentences: canned text, tem-
plates, and a grammar engine. There are also hybrids, such as templates with a few
grammar rules, and either can be created by hand entirely or has use some partial
automation by being based on some corpus to aid determining preferred sentences
structures and terminology. The most commonly used approach for ontology ver-
balisation is templates that optionally have a few grammar rules, since it generally
takes less eﬀort to start with, there are only a relatively small number of constraints
to verbalise, and there are only domain corpora, if any, rather than some ‘corpus
for OWL ontologies’. Let’s look at two examples (analysed afterward):
(S1) Giraﬀe ⊑Animal
Each Giraﬀe is an Animal
(S2) Herb ⊑Plant
Each Herb is a Plant
The underlined text ‘Each’ is the natural language rendering of the silent “∀” at
the start of the axioms, and ‘is a(n)’ for “⊑”, which remain the same, and the

188
Chapter 9. Ontologies and natural languages
Text planning
Sentence 
planning
Linguistic 
realisation
1. Content 
determination
2. Discourse 
planning
3. Sentence 
Aggregation
4. Lexica-
lisation
5. Referring 
expression 
generation
6. Linguistic 
realisation
1. What structured data/info/
knowledge do you want to put 
into NL sentences? 
2. In what order should it be 
presented?
3. Which messages to put together into a 
sentence?
4. Which words and phrases will it use for 
each domain concept and relation?
5. Which words or phrases to select to 
identify domain entities?
6. Use grammar rules to 
produce syntac-tically, 
morphological-ly, and 
orthographical-ly correct 
(and is also meaningful)
1. The (OWL) ontology
2. Your choice (e.g., first all classes and  
class expressions in the TBox, then the 
object properties, etc.) 
3. Aim: sentence for each axiom
4. Use vocabulary of the ontology; Select term for 
each constructor in the language (Each/All, and, 
some/at least one)
5. Combine related small axiom, or to relate the 
sentences generated for a large axiom
6. Language-specific issues (e.g., 
singular/plural of the class in 
agreement with conjugation of the 
verb, ‘a’ and ‘an’ vs ‘a(n)’, etc.)
The NLG ‘pipeline’
Ontology verbalisation
Figure 9.5: The common NLG ‘pipeline’ and how that links to ontology verbalisation
vocabulary from the axiom is inserted on-the-ﬂy by reading it in from the ontology
ﬁle. One can construct a template for an axiom type with those pieces of text that
remain the same for each construct, interspersed with variables that will take the
appropriate entity from the ontology. For instance, for this named class subsump-
tion axiom type C ⊑D, one could declare the following template:
Each class1 is a(n) class2
and for the basic all-some axiom type C ⊑∃R.D, either one of the following ones
may be selected:
Each class1 op at least one class2
Each class1 op some class2
All class1pl opinf at least one class2
All class1pl opinf some class2
The subscript “pl” indicates that the ﬁrst noun has to be pluralised and “inf” de-
notes that the object property has to be rendered in the inﬁnitive. The algorithms
tend to be based on the assumption that the ontology adheres to conventions of
naming classes with nouns in the singular and object properties in 3rd person sin-
gular. It is an implementation detail whether a plural or the inﬁnitive is generated
on-the-ﬂy from grammar rules or fetched from a Lemon ﬁle or another annotation
or precomputed lexicon. A relatively straight-forward template for the C ⊑∃R.D
axiom pattern at the implementation level is illustrated in Figure 9.6 for English
and Arabic, which are based on the templates of the multilingual verbaliser by
[JKD06].
One could declare templates of each axiom type, but because it is ‘arbitrary’
within the syntax constraints of the logic, a modular approach makes sense. Essen-
tially, each symbol in the language has one or more ways of putting it in natural
language, and based on that, one can create a controlled natural language that

9.2.
Ontology verbalisation
189
Figure 9.6: Two example templates for the axiom pattern C ⊑∃R.D, which is speciﬁed
in a particular XML speciﬁcation (adapted from the templates by [JKD06]). The indexing
goes from left to right in the axiom, so C is at Class index=‘‘0’’ and so forth.
can generate only those sentences following the speciﬁed template or a combina-
tion of template fragments. For instance, a template for C ⊓D, i.e., “class1 and
class2” can be fetched as needed when it appears in a class expression such as
E ⊑∃R.(C ⊓D). In this way, then a whole ontology can be verbalised and pre-
sented as (pseudo-)natural language sentences that hide the logic. Of course, it
takes for granted one already has an ontology in one’s preferred language.
Two recent review papers describe the current state of the art with respect to
NLG and the Semantic Web [BACW14] and CNLs [SD17]. Both focus principally
on English, largely because there are only a few eﬀorts for other languages. Some
13 years ago we tried to create a verbaliser for multiple languages, which worked
to some extent only [JKD06]. The main lessons learned then, and which hold just
as much now, were: 1) it is easier to generate a new set of templates in a language
based on an existing set of templates for a similar language, and 2) the template-
based approach is infeasible for grammatically richer languages. Regarding the
second aspect, it may be that a few additional rules suﬃce, which could be added
to the same template ﬁle, even, or one may be able to tweak SimpleNLG [GR09],
which has been adapted from English to French, Spanish, and Portuguese already.
For a language such as isiZulu, however, there are, thus far, no known templates,
but only ‘patterns’; that is, for the patterns designed for verbalising ontologies
(roughly within ALC), there is no single canned word in the ‘template’ as each
word in the sentence requires some processing with language tags and grammar
rules [KK17a]. The nature of ‘hybrid’ systems that combine templates with some
grammar rules is being investigated [MK19], principally to support rule reuse across
tools that process languages that are grammatically more demanding than English.
The book’s website has a list of verbalisers, which are categorised by languages and
by engineering approach.
9.2.2
Reusing the results for related activities
Ontology verbalisation can also be used for, e.g., automatically generating docu-
mentation of one’s ontology, in a similar fashion as automated code documentation
generation. A step toward interesting applications is, e.g., the ontology-enhanced
museum guide (in Greek) [ALG13], question generation for textbooks in biology
that were annotated with an ontology [CCO+13], ontology-driven multiple-choice
question generation, and language learning exercises that reuses the verbalisation

190
Chapter 9. Ontologies and natural languages
patterns without the ontology [GK18].
One might take another step further: use machine translation in the verbal-
isation process. Such a system has been developed by [GB11]: it uses ACE for
ontology verbalisation in English and Grammatical Framework for translations be-
tween English and Latvian. Which option is the better one is not clear: a) make
templates in one’s language and translate the ontology vocabulary (overcoming the
challenges described in Section 9.1), or b) skip the template speciﬁcation by reusing
an existing English one, develop a resource grammar for Grammatical Framework,
and translate the generated English into the target language or use, e.g., Neural
Machine Translation (NMT) for that. This depends on one’s aims and availabil-
ity of resources. In theory, the Grammatical Framework should work better as a
reusable solid solution but with (very) high start-up costs for grammar speciﬁca-
tion (as does NMT because it needs large parallel corpora in the domain of the
ontology), whereas templates are easier to specify but the knowledge that goes
into it is not easily reused elsewhere. Of course, the latter may not be useful if the
ontology has its vocabulary elements already in the target language.
Having the route from axiom to (pseudo-)natural language sentence, one could
wonder about the process in the reverse, i.e., using the CNL to generate the ax-
ioms. The most successful results for ontologies to date have been obtained with
Attempto Controlled English (ACE) [FKK10] for OWL DL. A related line of work
within the scope of from-text-to-axiom, is natural language-based query formu-
lation (e.g., [FGT10]), which uses a CNL for sentence fragments to iteratively
construct a query. In this scenario, however, partial completions of an iteratively
constructed sentence to query the ontology are based on the step-wise selected
vocabulary and the (partial) axioms in which it appears.
9.3
Exercises
Review question 9.1. Name some of the problems with naming the classes in an
OWL ﬁle, when considering multiple languages.
Review question 9.2. Describe in your own words the theoretical solution that
Lemon exhibits, both in the context of a single natural language and in the multi-
lingual setting.
Review question 9.3. Name some of the challenges for localising an ontology
into a language that is not English.
Review question 9.4. What can ontology verbalisation be used for?
Review question 9.5. Describe how the template-based approach works for on-
tology verbalisation.
Exercise 9.1. The AWO has been translated into Spanish, Afrikaans, isiZulu, and
Dutch. That has been done in diﬀerent ways, as follows:
- IsiZulu: translated terms, changed IRI;
- Afrikaans: translated terms, still has some IRI issues to resolve;

9.4.
Literature and reference material
191
- Dutch: translated terms, changed IRI;
- Spanish: translated terms, same IRI but diﬀerent ﬁle name.
Inspect them, and then try to answer the following questions:
a. What are the IRI issues with the Afrikaans OWL ﬁle? *
b. For the other three: which one, if any, is the best way to handle the IRI, and
why/if neither: why not? *
c. In what way, if any, is the link to the original AWO v1 maintained? *
Note: it may be easier to answer the questions by looking at the OWL ﬁle in a
text editor cf an ODE, since it gives a more easily accessible view on the IRIs.
Exercise 9.2. Create a Lemon ﬁle for the ontology of your choice, in the language
of your choice.
Exercise 9.3. Devise templates in English for the following axiom types:
1. C ⊓D ⊑⊥
2. ∃R.C ⊑D
3. C ⊑∀R.D
You may want to do this for a language of choice, but this may turn out a hard
exercise then, depending on the chosen language. *
Exercise 9.4. Devise a software architecture that would solve the multilingualism
with respect to maintenance. Lists its pros and cons. Compare your solution to
your classmates’ solutions. *
Exercise 9.5. If there were to be an ‘OWL 3’ standardisation eﬀort, what would
you propose to change to the current OWL 2 standard so that it will meet better
the original “internationalisation” goal, and why? If nothing, why?
9.4
Literature and reference material
1. John McCrae, Guadalupe Aguado de Cea, Paul Buitelaar, Philipp Cimiano,
Thierry Declerck, Asunci´on G´omez-P´erez, Jorge Gracia, Laura Hollink, Elena
Montiel-Ponsoda, Dennis Spohr, and Tobias Wunner. The lemon cookbook.
Technical report, Monnet Project, June 2012. www.lemon-model.net.
2. Paul Buitelaar and Philipp Cimiano (Eds.). Towards the Multilingual Se-
mantic Web: Principles, Methods and Applications. Springer, 2014.
3. N. Bouayad-Agha, G. Casamayor, and L. Wanner. Natural language gener-
ation in the context of the Semantic Web. Semantic Web Journal, 5(6):493-
513, 2014.
4. Hazem Safwat and Brian Davis. CNLs for the semantic web: a state of the
art. Language Resources & Evaluation, 51(1):191-220, 2017.


CHAPTER 10
Advanced Modelling with Additional Language Features
When the representation of the subject domain in an ontology requires features not
available in OWL, the default response is typically ‘tough luck’. This is not exactly
true. Common Logic (CL) and the Distributed Ontology, model and speciﬁcation
Language (DOL) were mentioned as an alternative way out in Section 4.3.2. It
is not immediately clear, however, how exactly CL and DOL can assist with, say,
probabilistic or temporal information. Also, for some ‘minor’ addition, one perhaps
may not want to immediately leave behind all the Semantic Web tooling infrastruc-
ture. To some extent, such requirements have been met for especially uncertainty
and vagueness, and extensions to OWL do exist, as we shall see. Temporal ex-
tensions have shown to be somewhat diﬃcult, or: if one wants to remain within a
decidable fragment of FOL, then there is not much temporal knowledge one would
be able to represent. We will look at each in turn.
The main aim of this chapter is to provide several pointers to more advanced
language features that allow the modeller to represent more than can be done with
the DL-based OWL species only. We shall see some solutions to earlier-mentioned
modelling question, among others: with which extension that “small” of the ‘small
bladed hunting spear’ can be represented and how the essential and immutable
behaviour of the parthood relation with the boxer and his hands vs. brain and
human is resolved.
10.1
Uncertainty and vagueness
This advanced ontology engineering topic concerns how to cope with uncertainty
and vagueness in ontology languages and their reasoners and what we can gain from
all the extra eﬀort. At the time of writing, this elective topic is mainly focused on
theory and research, and a few proof-of-concept tools exist. Let’s ﬁrst clarify these
two terms upfront:
193

194
Chapter 10.
Advanced Modelling with Additional Language Features
• Uncertainty: statements are true or false, but due to lack of knowledge we
can only estimate to which probability / possibility / necessity degree they
are true or false;
• Vagueness: statements involve concepts for which there is no exact deﬁni-
tion (such as tall, small, close, far, cheap, expensive), which are then true to
some degree, taken from a truth space.
Consider, e.g., information retrieval: to which degree is a web site, a page, a para-
graph, an image, or a video segment relevant to the information need and an
acceptable answer to what the user was searching for? In the context of ontol-
ogy alignment, one would want to know (automatically) to which degree the focal
concepts of two or more ontologies represent the same thing, or are ‘suﬃciently’
overlapping. In an electronic health record system, one may want to classify pa-
tients based on their symptoms, such as throwing up often, having a high blood
pressure, and yellow-ish eye colour. Or compute the probability that a person is
HIV positive is 23% and has been exposed to TB is 85%, or the probability that
birds ﬂy. How can ontology-driven software agents do the negotiation for your hol-
iday travel plans that are speciﬁed imprecisely, alike “I am looking for a package
holiday of preferably less than R15000, but really no more than R20000 , for about
12 days in a cold country that has snow”? One may want to classify, say, ripe
apples, ﬁnd the set of all individuals that mostly buy low calorie food, and patients
that are possibly septic when having the properties of “infection and [temperature
> 39C OR temperature < 36C, respiratory rate > 20 breaths/minute OR PaCO2
< 32 mmHg]”. Of course, one can combine the two notions as well, e.g.: “It is
probable to degree 0.915 that it will be hot in June in the Northern hemisphere”.
The main problem to solve, then, is what and how to incorporate such vague or
uncertain knowledge in OWL and its reasoners (or another logic, as one desires).
The two principal approaches regarding uncertainty probabilistic and possibilistic
languages, ontologies, and reasoning services, where the former way of dealing with
uncertainty receives a lot more attention than the latter1. The two principal ap-
proaches regarding vagueness and the semantic web are fuzzy and rough extensions,
where fuzzy used to receive more attention compared to the rough approach, but
theories for the latter are catching up.
10.1.1
Fuzzy ontologies
In fuzzy logic2, statements are true to some degree which is taken from a truth space,
which is usually [0, 1]. This sounds easier than it is, especially the deductions that
follow from them. Consider ﬁrst the following example.
1We will not cover probabilistic ontologies in this chapter; a recent introductory overview is
described in [Luk17]. Some pointers to reasoners are: Pronto, which is an extension to the Pellet
reasoner (http://pellet.owldl.com/pronto/), PR-OWL (http://www.pr-owl.org/ and oth-
ers, such as a Probabilistic Ontology Mapping Tool (OMEN), and combinations with Bayesian
networks (BayesOWL, OntoBayes).
2In this section, I assume the reader recalls something of fuzzy logic or fuzzy sets, and that
the occasional examples suﬃce as refresher.

10.1.
Uncertainty and vagueness
195
Example 10.1. Take the statement that Hotel EnjoyYourStay is close to the train
station to degree 0.83. A query on a hotel booking site may be “Find me the top-k
cheapest hotels close to the train station”, more formally:
q(h) ←hasLocation(h, hl) ∧hasLocation(train, cl) ∧close(hl, cl) ∧cheap(h)
Then what is the meaning—and logically: interpretation—of the query when ﬁlled
in with the values alike close(EnjoyYourStay, train) ∧cheap(200)? That is, how does
that work out in the model (in the context of model-theoretic semantics)? We
have to change the interpretation function a little to accommodate for the fuzzy
concepts. The interpretation is a function I that maps atoms into a value between
0 and 1 inclusive, i.e., I(A) ∈[0, 1]. Given that, then if the knowledge base states
I(close(EnjoyYourStay, train)) = 0.83 and I(cheap(200)) = 0.2, then what is the result
of 0.83 ∧0.2? More generally, what is the result of n ∧m, for n, m ∈[0, 1]?
♦
Many-valued formulae in fuzzy logic have the form φ ≥l or φ ≤u where
l, u ∈[0, 1], meaning that the degree of truth is at least l and at most u, respec-
tively. Figure 10.1 visualises the intuition of the graded degrees of truth. The
top-left chart shows the membership function of expensive: a value of 25 is def-
initely not expensive, i.e., it would evaluate to 0, then it slowly starts climbing
toward expensive, where at 187.5, expensive(someObject) = 0.5, and climbing fur-
ther up to 400, when, whatever it is, it is deﬁnitely expensive (evaluating to 1); this
function has a so-called right shoulder. One can do the same with two opposite con-
cepts, like hot and cold. There are no values, as what is perceived to be hot (cold)
weather in Iceland is diﬀerent from Ireland, that’s again diﬀerent from Italy and
yet again from the Ivory Coast. An example of a fuzzy function passed the revue
in Example 9.1 for ‘mesoscopic small’ for the hunting spear. More fuzzy functions
exist, such as the trapezoidal function with both a left and a right shoulder, also
depicted in Figure 10.1, which, perhaps, could refer to the optimal temperature of
a cup of coﬀee—not too cold and not too hot.
Figure 10.1: Some examples of fuzzy functions; see text for explanation.
Returning to Example 10.1, it noted that the interpretation has to be modiﬁed
cf. the ‘standard’ way we have seen in Block I in order to take care of a truth value
between 0 and 1 inclusive, rather than either 0 or 1. It is a mapping I : Atoms →

196
Chapter 10.
Advanced Modelling with Additional Language Features
[0, 1], which are extended to formulae as follows (the principal list is shown):
I(¬φ) = I(φ) →0
(10.1)
I(∃xφ) = supc∈∆IIc
x(φ)
(10.2)
I(∀xφ) = infc∈∆IIc
x(φ)
(10.3)
I(φ ∧ψ) = I(φ) ⊗I(ψ)
(10.4)
I(φ ∨ψ) = I(φ) ⊕I(ψ)
(10.5)
I(φ →ψ) = I(φ) ⇒I(ψ)
(10.6)
I(¬φ) = ⊖I(φ)
(10.7)
where Ic
x is as I except that var x is mapped to individual c, the ⊗, ⊕, ⇒,
and ⊖are combination functions: triangular norms (or t-norms), triangular co-
norms (or s-norms), implication functions, and negation functions, respectively.
Also, they extend the classical Boolean conjunction, disjunction, implication, and
negation, respectively, to the many-valued case. In addition, it poses the notion
of degree of subsumption between two fuzzy sets A and B, which is deﬁned as
infx∈XA(x) ⇒B(x) and such that if A(x) ≤B(x) for all x ∈[0, 1] then A ⊑B
evaluates to 1. Finally, I |= φ ≥l (resp. I |= φ ≤u) iﬀI(φ) ≥l (respectively,
I(φ) ≤u).
For notations of speciﬁc fuzzy DLs and fuzzy OWL extension, basically, the
language speciﬁcations are the usual DL/OWL notation, with the above-mentioned
modiﬁcations and additions. For details and examples, you are suggested to start
with [Str08], which provides a good introductory overview and has a very long list
of references to start delving into the topics, and a more technical take on it is
provided in [LS08]. An alternative approach is taken in [BS11] (that also describes
many examples), where all the fuzzy knowledge is put in the annotations of an
OWL 2 ﬁle through a Prot´eg´e plugin3 and then it is either ignored by a standard
reasoner or parsed and handled by the fuzzyDL4 or DeLorean5 reasoner.
Fuzzy DLs can be classiﬁed according to the DL or OWL language that they ex-
tend, the allowed fuzzy constructs and the underlying fuzzy logics (notably, G¨odel,
Lukasiewicz, Zadeh), and their reasoning services. Regarding the latter, because
we have those special new fuzzy features in the language, we get new reasoning
services with it. They are:
• Consistency, Subsumption, Equivalence
• Graded instantiation: Check if individual a is an instance of class C to degree
at least n, i.e., KB |= ⟨a : C, n⟩
• Best Truth Value Bound problem: determine tightest bound n ∈[0, 1] of an
axiom α, i.e. glb(KB, α) = sup{n, | KB |= ⟨α ≥n⟩} (likewise for lub, lowest
upper bound)
3available at http://www.umbertostraccia.it/cs/software/FuzzyOWL/ in July 2018, not
the URL listed in the paper.
4http://www.umbertostraccia.it/cs/software/fuzzyDL/fuzzyDL.html
5http://webdiis.unizar.es/~fbobillo/delorean.php

10.1.
Uncertainty and vagueness
197
• Best Satisﬁability Bound problem: glb(KB, C) determined by the max value
of x s.t. (R, T , A ∪{a : C ≥x}) (among all models, determine the max
degree of truth that concept C may have over all individuals x ∈∆I)
• glb(KB, C ⊑D) is the minimal value of x such that KB = (R, T , A ∪{a :
C ⊓¬D ≥1 −x}) is satisﬁable, where a is a new individual; Therefore, the
greatest lower bound problem can be reduced to the minimal satisﬁability
problem of a fuzzy knowledge base
There are several fuzzy reasoners, such as the aforementioned FuzzyDL for fuzzy
SHIF(D) the DeLorean reasoners.
How does this work out practically? Given some fuzzy SHIF(D), SHOIN(D),
SROIQ(D) that is serialised in OWL syntax, then one can declare the fuzzy con-
cepts with either modiﬁers (e.g., very) or ‘concrete’ fuzzy concepts (e.g., Young)
using data properties, where both additions have explicit membership functions.
Jumping over the technicalities here, it enables one to specify fuzzy concepts as
follows, using ‘minor’ and ‘young’ as examples. Just by numbers, ≤18(x) over N,
evaluates to true if x ≤18, false otherwise (i.e., cr(0, 18)). Let’s deﬁne ‘minor’ as
Minor ≡Person ⊓∃Age.≤18. Being a minor and being young is not exactly the same
thing, so let’s add something new for the latter: Young : Natural →[0, 1] is declared
as a fuzzy datatype predicate denoting the degree of youngness, which then allows
one to deﬁne ‘young’ as, say, Young(x) = ls(x, 10, 30), where ls is the left shoulder
function (like the cold line in Figure 10.1). Then a young person may be deﬁned
as YoungPerson ≡Person ⊓∃Age.Young. What does the ontology entail with these
axioms asserted? It entails, e.g.:
O |= Minor ⊑YoungPerson ≥0.6,
O |= YoungPerson ⊑Minor ≥0.4
The values of 0.6 and 0.4 follow from the fuzzy functions, were ‘minor’ covered the
age range of 0 to 18 and the young covered 0-30 age range that was going downhill
with being ‘young’ from the age of 10.
Example 9.1 describes the speciﬁcation of a ‘small bladed hunting spear’ in a
fuzzy way.
10.1.2
Rough ontologies
Rough ontologies are not an endpoint of themselves, but a means to an end—if
one is lucky. If a concept turns out to be a rough concept, i.e., there are some
individuals of which we don’t know whether they are instances of that concept,
it means that the concept is underspeciﬁed at least with respect to the properties
one has represented in the ontology. More precision is good, from an ontological
viewpoint at least, and rough ontologies can assist the modeller by providing an
iterative way to add more properties so as to reduce the amount of uncertainty
in an observable way. That is, adding more properties to the concept should lead
to fewer individuals of which we don’t know whether they are instances of that
concept. If adding or removing a property does not make a diﬀerence, then that
property is less important than others for that speciﬁc concept. Thus is, it can

198
Chapter 10.
Advanced Modelling with Additional Language Features
Target set X (oval), which is 
rough, i.e., X = <X, X>
Lower approximation X
(dark green squares)
Upper approximation X
(light + dark green squares)
Universe U 
Granule with object(s)
(white squares being the negative 
region w.r.t. X)
Boundary region BpX
(light green squares)
Figure 10.2: A rough set and associated notions (Source: based on [PS07]).
assist in making one’s ontology more precise, be this regarding the representation
only, or to press the domain expert for more subject domain details.
Rough DLs were ﬁrst introduced in [Kee10b]. As ‘roughness’ is not commonly
included in undergraduate education, a brief summary (based on [Kee10b]) is pro-
vided ﬁrst, before porting it to DLs and demonstrating where it solves some rep-
resentation and reasoning problems.
Rough sets
We consider here the typical rough set model of Pawlak, which is illustrated di-
agrammatically in Figure 10.2. Formally, I = (U, A) is an information system,
where U is a non-empty ﬁnite set of objects and A a ﬁnite non-empty set of at-
tributes such that for every a ∈A, we have the function a : U 7→Va where va is
the set of values that attribute a can have. For any subset of attributes P ⊆A,
the equivalence relation ind(P) is then deﬁned as follows:
ind(P) = {(x, y) ∈U × U | ∀a ∈P, a(x) = a(y)}
(10.8)
which generates a partition of U, denoted with U/ind(P), or U/P for short. If
(x, y) ∈ind(P), then x and y are indistinguishable with respect to the attributes
in P, which is referred to as p-indistinguishable.
From those objects in U, the aim is to represent set X such that X ⊆U, using
P (with P ⊆A). That set X may not be crisp, i.e., it may include or exclude
objects which are indistinguishable on the basis of the attributes in P, i.e., we do
not know given the attributes under consideration. This can be approximated by
using a lower and an upper approximation, which are deﬁned as:
PX = {x | [x]P ⊆X}
(10.9)
PX = {x | [x]P ∩X ̸= ∅}
(10.10)
where [x]P denotes the equivalence classes of the p-indistinguishability relation.
The lower approximation (10.9) is the set of objects that are positively members
of set X (more precisely: it is the union of all equivalence classes in [x]P). The
upper approximation is the set of objects that are possibly in X and its complement,
U −PX, is the negative region that is the union of all equivalence classes of sets

10.1.
Uncertainty and vagueness
199
of objects that are deﬁnitely not in X (i.e., ¬X). Then, “with every rough set
we associate two crisp sets, called lower and upper approximation” [PS07], which
is denoted as a tuple X = ⟨X, X⟩. Finally, the diﬀerence between the lower and
upper approximation, BPX = PX −PX, is called the boundary region: this region
contains the objects that neither can be classiﬁed as member of X nor that they
are not in X. It follows that if BPX = ∅then X is a crisp set with respect to P
and when BPX ̸= ∅then X is rough w.r.t. P, i.e., then there are indistinguishable
objects.
Given the boundary region, one can compute the accuracy of approximation,
αPX, which indicates how well a rough set approximates the target set. There are
several ways to compute it, e.g., αPX = |PX|
|PX| or αPX = 1−|BP X|
|U|
(this is a separate
topic not further discussed here). Note that PX ⊆X ⊆PX. There are further
notions that we do not need in this summary, but are ontologically interesting: the
reduct and core, which are the set of suﬃcient conditions (attributes) and the set
of necessary conditions, respectively (still with respect to P).
Transferring rough sets into ontologies
In most ontology languages, there are more constructors than just attributes and
‘attributes’ may be, at least, represented with a role (object property) R ∈R or
value attributions (data property) D ∈D, which has to be accounted for: rough
set’s P is thus to be taken from R∪D where those attributes have the rough concept
declared as domain. In addition, it requires an appropriate model-theoretic seman-
tics for C and C as well as a ‘rough concept’, denoted here with “≀C” to simplify
notation. The semantics of the approximations can be transferred in a straightfor-
ward manner, where E denotes indistinguishability (equivalence) relation (which
is reﬂexive, symmetric, and transitive):
C = {x | ∀y : (x, y) ∈E →y ∈C}
(10.11)
C = {x | ∃y : (x, y) ∈E ∧y ∈C}
(10.12)
Then there is rough sets’ tuple notation, X = ⟨X, X⟩to transfer into DL, but a
≀C = ⟨C, C⟩is a rather unusual notation. Instead, one can use also two new binary
relationships, dubbed lapr and uapr, to relate any rough concept and its associated
approximations, which are typed as follows:
∀φ, ψ.lapr(φ, ψ) →≀C(φ) ∧C(ψ)
(10.13)
∀φ, ψ.uapr(φ, ψ) →≀C(φ) ∧C(ψ)
(10.14)
Note that they quantify over sets, not objects that are member of the respective
sets, therewith making explicit the knowledge about the three sets and how they
relate. Finally, we make explicit that ≀C is identiﬁed by the combination of its

200
Chapter 10.
Advanced Modelling with Additional Language Features
lower and upper approximation, which the ﬂowing axioms ensure:
∀φ. ≀C(φ) →∃ψ.lapr(φ, ψ),
∀φ. ≀C(φ) →∃ψ.uapr(φ, ψ),
∀φ, ψ, ϕ.lapr(φ, ψ) ∧lapr(φ, ϕ) →ψ = ϕ,
(10.15)
∀φ, ψ, ϕ.uapr(φ, ψ) ∧uapr(φ, ϕ) →ψ = ϕ,
∀φ1, φ2, ψ1, ψ2.lapr(φ1, ψ1) ∧uapr(φ1, ψ2)∧
lapr(φ2, ψ1) ∧uapr(φ2, ψ2) →φ1 = φ2.
They say that: 1) there must be exactly one lower and one upper approximation
for each rough concept, and 2) there is one rough concept for each combination of
lower and upper approximation (i.e., if an approximation diﬀers, it is a diﬀerent
rough concept).
Practically within an OWL-only setting, the C and C is reduced to, in OWL 2
DL functional syntax:
EquivalentClasses(C ObjectSomeValuesFrom(a:Ind a:C))
EquivalentClasses(C ObjectAllValuesFrom(a:Ind a:C))
where Ind denotes the indistinguishability relation. Then Eq. 10.15 is approxi-
mated by adding object properties uapr, lapr that have ≀C as domain, and an
exactly 1 cardinality constraint:
ObjectPropertyDomain(a:upar a:≀C)
ObjectPropertyDomain(a:lapr a:≀C)
ObjectExactCardinality(1 a:uapr a:C)
ObjectExactCardinality(1 a:lapr a:C)
One has to add this for each rough concept and its approximations. For instance,
the promiscuous bacteria of [Kee10c]:
PromiscuousBacterium ≡Organism ⊓∃Percentage.real>10 ⊓
≥6 hasHGTCluster.FlexibleHGTGeneCluster
PromiscuousBacterium ⊑= 1 lapr.PromBactLapr
PromiscuousBacterium ⊑= 1 uapr.PromBactUapr
PromBactLapr ≡∀Ind.PromBact
PromBactUapr ≡∃Ind.PromBact
(10.16)
Each such concept will have to be tested against the instances in the ABox: is the
PromiscuousBacterium indeed a rough concept? This was experimented with in two
ways: ‘natively’ in OWL ﬁles as well as in the OBDA setting (Figure 8.2 was taken
from that experiment). Practically, at the time at least, putting the instances in the
OWL ﬁle was a ‘bad’ idea, mainly because the reasoners are not optimised to deal
eﬃciently with data properties, value restrictions, and instances (or: computing the
rough concepts took quite some time even with just 17 individuals). The OBDA

10.2.
Time and Temporal Ontologies
201
setting did work, but was at the time cumbersome to set up; this has improved in
the meantime (see also Chapter 8).
One can add rough subsumption to rough concepts [Kee11a] as well, which is
beyond the current introductory scope.
10.2
Time and Temporal Ontologies
There are multiple requests for including a temporal dimension in OWL. Some
of those requirements are described in the ontology’s annotation ﬁelds (see the
OWL ﬁles of BFO and DOLCE), or the labels of the object properties in the BFO
v2.1 draft, where they mention temporality that cannot be represented formally
in OWL: e.g., DOLCE has a temporally indexed parthood in the paper-based
version but this could not be transferred into the OWL ﬁle. This is, perhaps, even
more an issue for domain ontologies. For instance, SNOMED CT [SNO12] has
concepts like “Biopsy, planned”, i.e., an event is expected to happen in the future,
and “Concussion with loss of consciousness for less than one hour”, i.e., a speciﬁc
interval and where the loss of consciousness can be before or after the concussion,
the symptom HairLoss during the treatment Chemotherapy, and Butterﬂy is a
transformation of Caterpillar. Other examples are a business rule alike ‘RentalCar
must be returned before Deposit is reimbursed’. Adding an object property before
in one’s OWL ontology is not going to ensure that in all possible models, some
return event happened before a reimbursement event, however, because it does not
know when what happened.
There is no single computational solution to solve these examples all at once
in another way beyond OWL. Thus far, it is a bit of a patchwork of various theo-
ries and some technologies, with, among many aspects, the Allen’s interval algebra
[All83] with the core qualitative temporal relations (such as before and during),
Linear Temporal Logics (LTL) and Computational Tree Logics (CTL, with branch-
ing time). There is also a Time Ontology6 that was recently standardised by the
W3C (more explanation in [HP04]), but it is an ontology for annotations only—i.e.,
no temporal reasoning intended—and suﬀers from the class-as-instance modelling
confusion7.
In the remainder of this section, we will look at some motivations for temporal
ontologies ﬁrst, proceed to a very expressive temporal DL, DLRUS, and ﬁnally
look at several modelling issues it helps solving. Although computationally, things
do not look rosy at all, it is possible to squeeze out a bit here and there, which we
shall touch upon at the end.
6http://www.w3.org/TR/owl-time/
7e.g., DayOfWeek(Friday) and, in the one for the Gregorian calendar, MonthOfYear(January):
Friday does have instances, such as Friday 6 July 2018 and Friday 13 July 2018, etc., and
likewise for the other days and for the month instances.

202
Chapter 10.
Advanced Modelling with Additional Language Features
10.2.1
Why temporal ontologies?
There are two principal parts to answering this question: because of what we want
to represent and what inferencing we want to do with it.
The things to represent
Quite common time aspects in conceptual data modelling for information systems
are the requirements to record actual dates and intervals and calendar calculations.
This is not particularly relevant for a domain ontology, but it would be useful to
have an ontology about such things so that the applications use the same notions
and, hence, will be interoperable in that regard.
In Chapter 6 we have seen BFO and the RO, where it was the intention by
its developers to add a precedes and an immediately precedes relation to the OBO
Foundry ontologies, which could not be done other than for annotation purposes.
There are more such established qualitative temporal relations, also known as the
‘Allen relations’ or ‘Allen’s interval algebra’, after the author who gave a ﬁrst
systematic and formal account of them [All83], which comprise relations such as
before, after, during, while, and meet. Some might say they are all the temporal
relations one will ever need, but one may wish to be more speciﬁc in specialised
subject domains, such as a transformation of a caterpillar into a butterﬂy, not just
that a butterﬂy was a caterpillar ‘before’, and one thing developed from another
thing in developmental biology.
Also, the latter two are persistent changes cf.
permitting to go back to what it was before.
Modellers want to do even more than that: temporalising classes and relations.
The former is well-known in databases as ‘object migration’; e.g., an active project
evolves to a completed project, and each divorcee in the census database must have
been married before. Relation migration follows the idea of temporal classes, but
applies to n-ary tuples (with n ≥2); e.g. ‘during x’s lifetime, it always has y
as part’ and ‘every passenger that boards the plane must have checked in before
departure of that ﬂight’.
More comprehensive and real examples, can be found in, among others, [AGK08,
Kee09, KA10, SSBS09]. This is not to say that all ontologies have, or ought to have,
a temporal component to represent the subject domain as accurately as possible.
It depends on the use case scenarios and CQs devised for the ontology.
Temporal reasoning services
As with a-temporal ontologies, one would want to have the same ones for temporal
ontologies, such as satisﬁability checking, subsumption reasoning, and classiﬁca-
tion. Logical implications are a bit more involved; e.g., given B ⊑A, then it must
be the case that objects ‘active’ (alive) in B must be active in A and, e.g., to come
up for promotion to become a company’s manager (B), one must ﬁrst exist as an
employee (A) of that company. Also, an ontology should permit either a statement
that ‘X must happen before Y’ or that ‘Y must happen before X’, but not both.
That is, there are temporal constraints that are not permitted to be contradicted,
and algorithms are needed to check for that.

10.2.
Time and Temporal Ontologies
203
One also would want to be able to query temporal information. For instance,
to retrieve the answer to “Who was the South African president after Nelson Man-
dela?” and “Which library books have not been borrowed in the past ﬁve years?”.
This also suggests that the ‘plain’ OBDA may be extended to a temporal OBDA
system; see [AKK+17] for a recent survey. There is a range of other examples
that involve time in some way in information systems, and which have been solved
and implemented already, such as querying with a calendar hierarchy and across
calendars and ﬁnding a solution satisfying a set of constraints for scheduling the
lecture hours of a study programme; there uses are outside the scope.
Open issues
There are many problems that are being investigated in temporal information and
knowledge processing. On the one hand, there are the modelling issues in ontology
development and ﬁguring out what temporal features modellers actually require
in a temporal logic cf. the logicians deciding which temporal features a modeller
gets (typically only the computational well-behaved ones), the interaction between
temporal logic and temporal databases (temporal OBDA), and further investiga-
tion into the interaction between temporal DLs with temporal conceptual data
modelling. This, in turn, requires one to look into the computational properties of
various fragments of expressive temporal logics. More fundamental issues have to
do with making choices regarding linear time vs. branching time (LTL vs CTL),
endurantism vs. perdurantism (‘4D-ﬂuents’) as was noted in Chapter 6 as a choice
for foundational ontologies, and dense time vs points in time.
10.2.2
Temporal DLs
If one assumes that recent advances in temporal DLs may have the highest chance
of making it into a temporal OWL, then the following is ‘on oﬀer’.
• A very expressive (undecidable) DL language is DLRUS (with the Until and
Since operators), which already has been used for temporal conceptual data
modelling [APS07] and for representing essential and immutable parts and
wholes [AGK08], which also solves the Boxer example of Section 6.2. It uses
linear time and mostly qualitative temporal constraints.
• An inexpressive language is TDL-Lite [AKL+07], which is a member of the
DL-Lite family of DL languages (of which one is the basis for OWL 2 QL). It
also uses linear time and mostly qualitative temporal constraints, but fewer
of them (e.g., one can’t have temporal relations).
• Metric temporal logic, which zooms in on quantitative temporal constraints;
e.g. [BBK+17].
• 4-D ﬂuents/n-ary approach in OWL with SWRL rules [BPTA17], rather than
a new language.

204
Chapter 10.
Advanced Modelling with Additional Language Features
It is already known that EL++ (the basis for OWL 2 EL) does not keep the nice
computational properties when extended with LTL, and results with EL++ with
CTL are not out yet. If you are really interested in the topic, you may want to have
a look at a survey [LWZ08] or take a broader scope with any of the four chapters
from the KR handbook [vHLP08] that cover temporal knowledge representation
and reasoning, situation calculus, event calculus, and temporal action logics, or
the Handbook of temporal reasoning in artiﬁcial intelligence [EM05]. To give a
ﬂavour of how temporal logics may look like and what one can do with it, we shall
focus on DLRUS, which has been extended with temporal relations and attributes
and is also used for temporal conceptual modelling (including a graphical notation
in the new TREND notation [KB17]).
The DLRUS temporal DL
DLRUS [AFWZ02] combines the propositional temporal logic with Since and Until
operators with the a-temporal DL DLR [CDG03] and can be regarded as an ex-
pressive fragment of the ﬁrst-order temporal logic L{since, until} [CT98, HWZ99,
GKWZ03].
As with other DLRs, the basic syntactical types of DLRUS are classes and
n-ary relations (n ≥2). Starting from a set of atomic classes (denoted by CN),
a set of atomic relations (denoted by RN), and a set of role symbols (denoted
by U), we can deﬁne complex class and relationship expressions (see upper part of
Figure 10.3), where the restriction that binary constructors (⊓, ⊔, U, S) are applied
to relations of the same arity, i, j, k, n are natural numbers, i ≤n, j does not
exceed the arity of R. All the Boolean constructors are available for both class and
relation expressions. The selection expression Ui/n : C denotes an n-ary relation
whose i-th argument (i ≤n), named Ui, is of type C. (If it is clear from the
context, we omit n and write (Ui : C).) The projection expression ∃≶k[Uj]R is a
generalisation with cardinalities of the projection operator over argument Uj of
relation R; the classical projection is ∃≥1[Uj]R.
The model-theoretic semantics of DLRUS assumes a ﬂow of time T = ⟨Tp, <⟩,
where Tp is a set of time points and < a binary precedence relation on Tp, assumed
to be isomorphic to ⟨Z, <⟩. The language of DLRUS is interpreted in temporal
models over T , which are triples of the form I .= ⟨T , ∆I, ·I(t)⟩, where ∆I is non-
empty set of objects (the domain of I) and ·I(t) an interpretation function. Since
the domain, ∆I, is time independent, we assume here the so called constant domain
assumption with rigid designator—i.e., an instance is always present in the inter-
pretation domain and it identiﬁes the same instance at diﬀerent points in time. The
interpretation function is such that, for every t ∈T (a shortcut for t ∈Tp), every
class C, and every n-ary relation R, we have CI(t) ⊆∆I and RI(t) ⊆(∆I)n. The
semantics of class and relation expressions is deﬁned in the lower part of Fig. 10.3,
where (u, v) = {w ∈T | u < w < v}. For classes, the temporal operators ♦+
(some time in the future), ⊕(at the next moment), and their past counterparts
can be deﬁned via U and S: ♦+C ≡⊤U C, ⊕C ≡⊥U C, etc. The operators □+
(always in the future) and □−(always in the past) are the duals of ♦+ (some time
in the future) and ♦−(some time in the past), respectively, i.e., □+C ≡¬♦+¬C

10.2.
Time and Temporal Ontologies
205
C
→
⊤| ⊥| CN | ¬C | C1 ⊓C2 | C1 ⊔C2 | ∃≶k[Uj]R |
♦+C | ♦−C | □+C | □−C | ⊕C | ⊖C | C1 U C2 | C1 S C2
R
→
⊤n | RN | ¬R | R1 ⊓R2 | R1 ⊔R2 | Ui/n : C |
♦+R | ♦−R | □+R | □−R | ⊕R | ⊖R | R1 U R2 | R1 S R2
⊤I(t) = ∆I;
⊥I(t) = ∅;
CNI(t) ⊆⊤I(t);
(¬C)I(t) = ⊤I(t) \ CI(t);
(C1 ⊓C2)I(t) = CI(t)
1
∩CI(t)
2
;
(C1 ⊔C2)I(t) = CI(t)
1
∪CI(t)
2
;
(∃≶k[Uj]R)I(t) = { d ∈⊤I(t) | ♯{⟨d1, . . . , dn⟩∈RI(t) | dj = d} ≶k};
(C1 U C2)I(t) = { d ∈⊤I(t) | ∃v > t.(d ∈CI(v)
2
∧∀w ∈(t, v).d ∈CI(w)
1
)};
(C1 S C2)I(t) = { d ∈⊤I(t) | ∃v < t.(d ∈CI(v)
2
∧∀w ∈(v, t).d ∈CI(w)
1
)};
(⊤n)I(t) ⊆(∆I)n;
RNI(t) ⊆(⊤n)I(t);
(¬R)I(t) = (⊤n)I(t) \ RI(t);
(R1 ⊓R2)I(t) = RI(t)
1
∩RI(t)
2
;
(R1 ⊔R2)I(t) = RI(t)
1
∪RI(t)
2
;
(Ui/n : C)I(t) = {⟨d1, . . . , dn⟩∈(⊤n)I(t) | di ∈CI(t)};
(R1 U R2)I(t) = {⟨d1, . . . , dn⟩∈(⊤n)I(t) | ∃v > t.(⟨d1, . . . , dn⟩∈RI(v)
2
∧∀w ∈(t, v). ⟨d1, . . . , dn⟩∈RI(w)
1
)};
(R1 S R2)I(t) = { ⟨d1, . . . , dn⟩∈(⊤n)I(t) | ∃v < t.(⟨d1, . . . , dn⟩∈RI(v)
2
∧∀w ∈(v, t). ⟨d1, . . . , dn⟩∈RI(w)
1
)};
(♦+R)I(t) = {⟨d1, . . . , dn⟩∈(⊤n)I(t) | ∃v > t. ⟨d1, . . . , dn⟩∈RI(v)};
(⊕R)I(t) = {⟨d1, . . . , dn⟩∈(⊤n)I(t) | ⟨d1, . . . , dn⟩∈RI(t+1)};
(♦−R)I(t) = {⟨d1, . . . , dn⟩∈(⊤n)I(t) | ∃v < t. ⟨d1, . . . , dn⟩∈RI(v)};
(⊖R)I(t) = {⟨d1, . . . , dn⟩∈(⊤n)I(t) | ⟨d1, . . . , dn⟩∈RI(t−1)}.
Figure 10.3:
Syntax and semantics of DLRUS.
and □−C ≡¬♦−¬C, for both classes and relations. The operators ♦∗(at some
moment) and its dual □∗(at all moments) can be deﬁned for both classes and
relations as ♦∗C ≡C ⊔♦+C ⊔♦−C and □∗C ≡C ⊓□+C ⊓□−C, respectively.
A DLRUS knowledge base is a ﬁnite set Σ of DLRUS axioms of the form C1 ⊑C2
and R1 ⊑R2, with R1 and R2 being relations of the same arity. An interpretation
I satisﬁes C1 ⊑C2 (R1 ⊑R2) if and only if the interpretation of C1 (R1) is included
in the interpretation of C2 (R2) at all time, i.e., CI(t)
1
⊆CI(t)
2
(RI(t)
1
⊆RI(t)
2
), for
all t ∈T . Thus, DLRUS axioms have a global reading. To see examples on how a
DLRUS knowledge base looks like we refer to the following sections where examples
are provided.
Modelling
Let us look at some examples, both in shorthand DLRUS notation and in their
semantics. The second line provides a verbalisation of the axiom, using the CNL
approach described in Section 9.2, but then tailored to verbalising the temporal
features.
– MScStudent ⊑♦∗¬MScStudent

206
Chapter 10.
Advanced Modelling with Additional Language Features
Each MSc Student is not a(n) MSc Student for some time.
– marriedTo ⊑♦∗¬marriedTo
The objects participating in a fact in Person married to Person do not relate
through married-to at some time; or: people who are married now aren’t
married at another time.
– o ∈AcademicI(t) ∧o /∈PhDStudentI(t) ∧o ∈PhDStudentI(t−1) ∧o /∈
AcademicI(t−1)
A(n) Academic may have been a(n) PhD Student before, but is not a(n) PhD
Student now.
– o ∈FrogI(t) →∃t′ < t.o ∈DevI(t′)
Tadpole,Frog
Each Frog was a(n) Tadpole before, but is not a(n) Tadpole now.
The aforementioned ‘planned’ biopsy can now also be represented as something
that will hold in the future ♦+Biopsy and the returning (of the car) before the
reimbursement (of the deposit) as, e.g., reimbursement ⊑♦−return, i.e., “if reim-
bursement, then sometime in the past there was a return”.
With this machinery, one can also solve the “Assuming boxers must have their
own hands and boxers are humans, is Hand part of Boxer in the same way as
Brain is part of Human?” that we have encountered in Section 6.2. Recasting this
problem into the temporal dimension, we can encode it in DLRUS and prove the
correctness of the intended behaviour [AGK08]. The hand being part of the boxer
is an immutable parthood, whereas the brain being part of the human is an essential
parthood. That is: the ‘essential’ parthood relation means, informally, that ‘that
speciﬁc object must be part of the whole for entire lifetime of the whole object’,
whereas ‘immutable’ means, informally, ‘for the time the objects are instance of that
speciﬁc class (which is typically a role they play for some duration that is less that
the lifetime of the object), it is essential’. The formal apparatus is quite lengthy,
including recasting OntoClean’s rigidity (recall Section 5.2.2) into the temporal
modality, and is described in detail in [AGK08]. The short version showing the
main axioms that represent that diﬀerence is as follows.
First, for illustrative
purpose, let’s introduce a part-whole relation to relate brain to human:
HumanBrainPW ⊑PartWhole
HumanBrainPW ⊑part : Brain ⊓whole : Human
Subsequently, we add the ‘essential’ to it, which means it holds at all times, i.e.,
□∗, for both human (once a human always a human for the whole lifetime of the
object—it is a rigid property) and the essential parthood (once a part, always a
part):
Human ⊑□∗Human
Human ⊑∃[whole]□∗HumanBrainPW
Then, the boxer’s hands.
Also here, for illustrative purpose, we introduce the
part-whole relation HumanHandPW to relate the hand to human:

10.3.
Exercises
207
HumanHandPW ⊑PartWhole
HumanHandPW ⊑part : Hand ⊓whole : Human
To state that a boxer is at some time not (♦∗¬) a boxer (an anti-rigid property),
and is a human, the following axioms suﬃce:
Boxer ⊑♦∗¬Boxer
Boxer ⊑Human
The next step is the cardinality constraint that a boxer must have exactly two
hands, if temporarily something is wrong with the boxer (e.g., the boxer has an
injured hand for some time) he isn’t a boxer either (‘suspended’), and ﬁnally, that if
that part-whole relation does not hold anymore (‘disabled’), then the boxer ceases
to be a boxer:
Boxer ⊑∃=2[whole]HumanHandPW
Suspended-HumanHandPW ⊑whole : Suspended-Boxer
Disabled-HumanHandPW ⊑whole : Disabled-Boxer
Note that ‘suspended’ and ‘disabled’ are names for so-called status relations that
are deﬁned formally in [AGK08] and do not exactly have their colloquial meaning.
If the boxer has to be continuously active by some rule from, say, the boxing
association, then the ‘suspended’ axiom has to replaced by the following one, i.e.,
the relation is not allowed to be ‘suspended’:
Suspended-HumanHandPW ⊑⊥
Obviously, this can be deﬁned for any essential or immutable relation, regardless
whether it is a part-whole relation or not. The upside is that now we know how
to represent it; the downside is that it uses both DL role hierarchies and tempo-
ral relations, which are computationally costly. What to do with this insight is
something that a modeller has to decide.
10.3
Exercises
Review question 10.1. What is the diﬀerence between uncertainty and vague-
ness?
Review question 10.2. Name some examples of fuzzy concepts.
Review question 10.3. Rough ontologies contain rough concepts. Describe how
they are approximated in an OWL ontology.
Review question 10.4. Name two conceptually distinct ways how time can be
dealt with in/with ontologies.
Review question 10.5. The introductory paragraph of Section 10.1 lists a series
of examples. State for each whether it refers to uncertainty or vagueness.

208
Chapter 10.
Advanced Modelling with Additional Language Features
Exercise 10.1. Devise an example similar to the ‘minor’ and ‘young’, but then
for ‘senior citizen’, ‘old’ and ‘old person’.
Compare this with another student.
How well do you think fuzzy ontologies will fare with respect to 1) ontologies in
information systems and 2) the original aim of ontologies for information systems?
*
Exercise 10.2. There are a few temporal reasoners for DLs and OWL. Find them
(online) and assess what technology they use. As to the temporal things one can
model, you may want to try to ﬁnd out as follows: deﬁne a small example and see
whether it can be represented and the deductions obtained in each of the tools. *
Exercise 10.3. The Time Ontology was standardised recently. Inspect it. Can
this be a viable alternative to DLRUS? *
Exercise 10.4. BFO draft v2.1 has all those indications of time in the names
of the object properties. Compare this to the Time Ontology and something like
DLRUS or TDL-Lite. What would your advice to its developers be, if any?
10.4
Literature and reference material
Suggested readings for fuzzy and rough ontologies:
1. Thomas Lukasiewicz and Umberto Straccia. 2008. Managing Uncertainty
and Vagueness in Description Logics for the Semantic Web. Journal of Web
Semantics, 6:291-308. NOTE: only the section on fuzzy ontologies
2. Umberto Straccia and Giulio Visco. DLMedia: an Ontology Mediated Mul-
timedia Information Retrieval System. In: Proceedings of the International
Workshop on Uncertainty Reasoning for the Semantic Web (URSW-08), 2008.
this is an example of an application that uses fuzzy DL
3. Keet, C.M. On the feasibility of Description Logic knowledge bases with rough
concepts and vague instances. 23rd International Workshop on Description
Logics (DL’10), 4-7 May 2010, Waterloo, Canada.
Suggested readings for temporal ontologies:
1. Alessandro Artale, Christine Parent, and Stefano Spaccapietra. Evolving ob-
jects in temporal information systems. Annals of Mathematics and Artiﬁcial
Intelligence (AMAI), 50:5-38, 2007. NOTE: only the DLRUS part, the rest
is optional.
2. J. R. Hobbs and F. Pan. An ontology of time for the semantic web. ACM
Transactions on Asian Language Processing (TALIP): Special issue on Tem-
poral Information Processing, 3(1):6685, 2004.
Optional readings with more technical details (in descending order of being op-
tional):
1. Artale, A., Guarino, N., and Keet, C.M. Formalising temporal constraints on
part-whole relations. 11th International Conference on Principles of Knowl-
edge Representation and Reasoning (KR’08). Gerhard Brewka, Jerome Lang
(Eds.) AAAI Press, pp 673-683. Sydney, Australia, September 16-19, 2008.

10.4.
Literature and reference material
209
2. Alessandro Artale, Enrico Franconi, Frank Wolter and Michael Zakharyaschev.
A Temporal Description Logic for Reasoning over Conceptual Schemas and
Queries. In: Proceedings of the 8th European Conference on Logics in Artiﬁ-
cial Intelligence (JELIA’02), Cosenza, Italy, September 2002. LNAI, Springer-
Verlag.
3. Alessandro Artale, Roman Kontchakov, Carsten Lutz, Frank Wolter and
Michael Zakharyaschev.
Temporalising Tractable Description Logics.
In:
Proc. of the 14th International Symposium on Temporal Representation and
Reasoning (TIME’07), Alicante, June 2007.


CHAPTER 11
Ontology modularisation
Ontology development is a laborious task.
To assist with this, modularisation
is a principle that has been proposed and increasingly been used for ontology
development. Modularisation refers to the division of an ontology to aid with the
simpliﬁcation of large ontologies. It is required when one needs to hide or remove
knowledge that is not required for the application at hand, or divided so that
modules can be worked on separately. It does the former by enabling developers
and users to hide or remove entities and axioms that are not relevant for the
application. There are a number of beneﬁts for the modularisation of ontologies
such as ontology reuse, collaboration, and scalability for data processing.
There are multiple angles from which to present ontology modularisation, which
cannot all be covered in the 10-15 pages that a chapter typically is in Block III.
Here, we take the angle of foundations for ontology development, rather than a
logic-based angle to the issues, which is partially so that we then also can touch
upon the notion of evaluation metrics. The other reason is that the content is
based on parts of the PhD thesis of Zubeida Khan, who is the main author of this
chapter (and who was supervised by the main author of this textbook).
We will start with a motivating example and a deﬁnition of a module. Sub-
sequently, we look at the various ‘dimensions’ of modules, i.e., aspects to features
that help describe modules, and close with a framework for modularisation.
11.1
Deﬁning modularisation
Let us start with a use case example, which illustrates the need for modularisation,
which is followed by several questions that will have to be answered.
Example 11.1. Consider a case where an ontology engineer wishes to develop
an enterprise web application for an exciting book store.
In order to compete
with existing exciting interactive web utilities such as Pottermore which engages
211

212
Chapter 11. Ontology modularisation
and challenges Harry Potter fans1, a simple website is not enough. The developer
needs to create an interactive experience aimed at book fandoms2. Furthermore,
the book store application must be machine readable in order to realise the vision of
the Semantic Web. Rather than unnecessarily starting from scratch, the ontology
developer decides to reuse existing resources from the web.
She has found the following existing resources: a general book ontology con-
taining metadata about books such as authors, edition, etc., a Japanese anime
ontology, containing a large amount of data about anime such as genre, protag-
onists, mangaka, etc., a toy ontology which contains toy product details, and a
gazetteer ontology describing diﬀerent places of geographical interest.
For the Japanese anime ontology, she wishes only to use parts of the ontology
that contains information about anime that have originated from manga and novels.
The taxonomy of the anime ontology is large, complicated, and too diﬃcult for the
developer to manually traverse through the entire ontology and understand it. For
the toy ontology, there are two main branches, a Toy entity with subclasses such as
Book-toy, Game-toy, Movie-toy etc. , and a Toy-property entity containing knowledge
about the properties of a toy, e.g., height, weight, colour, etc. The developer wishes
to extract only the speciﬁc Book-toy entity and the generalised Toy-property entity.
She wishes to use the gazetteer ontology to describe both ﬁctional and real places
that appear in books. The gazetteer ontology has a high level of detail and contains
knowledge about the population of each city, neighbouring countries, the size of
the country, etc. However, the developer does not require the high level of detail
of the ontology, but rather a simpliﬁed version.
The developer decides that the manga and novel aspects of the anime ontology
could be imported into the general book ontology.
The toy ontology, however,
should be separate from the book ontology, to allow toy specialists to update it
separately. The gazetteer ontology should also be a separate module. By creating
separate modules, the developer hopes to promote collaborative ontology develop-
ment within a team and ease the validity and maintenance of the system. While
the toy and gazetteer ontology are separate modules, they are also related to the
book ontology, i.e., there should be axioms that link the modules. For instance,
for the toy ontology, it could be that a scaled ﬁgure of Katniss Everdeen is based
on the character, Katniss Everdeen, from the Hunger Games trilogy book series.
For the geographical ontology, it could be that Alicante is the city that exists in
the Mortal Instruments book series.
This example opens up a number of questions for the developer. How should
the developer extract reading material aspects from the anime ontology without
traversing through the entire ontology? How should the developer isolate the Book-
toy and Toy-property entities from the toy ontology? How should the developer
simplify the geographical ontology?
It is true that there are existing ontology
modularity techniques. However, which technique, if any, will be able to perform
the aforementioned tasks. It is clear that the processes of extracting reading ma-
1https://www.pottermore.com/ last accessed: 20 September 2019.
2A fandom is a term used to describe fans who share a common interest such as the Harry
Potter community, the Pok´emon community, the Doctor Who community, and so on.

11.2.
Module dimensions
213
terial aspects, isolating branches of interest, and taxonomy simpliﬁcation are all
diﬀerent. Once the developer creates these modules, how should she measure the
quality of these modules, considering that these modules have diﬀerent properties?
Before we start answering these questions, we need a deﬁnition of modules
ﬁrst, to indicate when is really meant with it here. There are several deﬁnitions of
modularisation and modules in the literature. We use the most recent one that is
exhaustive and does not enforce restrictions, such as that a module must exist in
a set that composes a whole, or that it must contain a source ontology, or that it
captures the knowledge of a given signature. The deﬁnition for a module we use is
as follows:
Deﬁnition 11.1. (Module ([KK15b])) A Module M is a subset of a source ontology
O, M ⊂O, or M is an ontology existing in a set such that, when combined, make
up a larger ontology. M is created for some use-case u ∈U, number of u ≥1,
and is of a particular type t ∈T, number of t = 1. t is classiﬁed by a set of
distinguishing properties {p1, ..., pk} ∈P, number of p ≥1, and is created by using
a speciﬁc modularisation technique mt ∈MT, number of mt = 1, and has a set of
corresponding evaluation metrics {em1, ..., emk} ∈EM, number of em ≥1, which
is used to assess the quality of M.
This deﬁnition introduces dimensions for modularisation: use-cases, techniques,
properties, and evaluation criteria. The inclusion of these dimensions in the deﬁ-
nition contributes to an exhaustive, generic, broad deﬁnition for modules. In the
following section we introduce the dimensions for modularisation.
11.2
Module dimensions
Module dimensions are aspects or features that help describe modules. Five di-
mensions have been identiﬁed for modules: use-cases, techniques, types, properties,
and evaluation criteria. They will be described in the following ﬁve subsections,
since each dimension has one or more ‘types’ or values. Afterward, we will tie the
components together in a framework, which systematises dependencies between
dimensions.
11.2.1
Use-cases
In the substantial amount of literature on modularisation, use-cases have commonly
been referred to as purposes, goals, beneﬁts or rationale for modularity. A use-case
states the underlying reason why the module will is, is, or has been created. We
discuss all known existing use-cases for modularisation in this section.
Maintenance
An important step in the ontology development process is the
maintenance of an ontology. Maintenance deals with preserving the quality of an
ontology to foster and ensure usage. Modules are constantly evolving and need to
be maintained on a regular basis. Maintenance is easier to perform when there are
separate modules because not all modules might need to be updated, and diﬀerent
people could maintain diﬀerent modules.

214
Chapter 11. Ontology modularisation
Automated reasoning
Since ontology reasoners do not scale well when reason-
ing over large ontologies, reasoning is the next use-case for modularisation. Rea-
soner performance improves when ontologies are smaller with less knowledge, and
one could try to optimise it by reasoning only over the module that has changed
rather than the whole ontology. The Data Mining OPtimisation ontology (DMOP),
which we have come across in Chapter 1 (Figure 1.10), takes about 10 minutes to
classify [KdKL14]. It will be worthwhile to investigate whether modularisation
could result in reduced reasoning time.
Validation
Validation deals with checking an ontology for correctness. This is
performed by checking for errors and redundancy, and checking if an ontology meets
all requirements. This is a diﬃcult process for a human when an ontology is large,
hence the need for modularisation.
Processing
Processing tools such as ontology alignment, computing of metrics,
and editing tools prohibitively slow down when working with large ontologies; they
could take several minutes to perform simple tasks such as loading and traversal.
Smaller modules take a shorter time to open, load, and use with tools, and hence
could possibly improve the performance of the processing tools.
Comprehension
There is a cognitive overload for humans to use and understand
large ontologies, therefore to ease comprehension, it is required that some data be
hidden or removed. This can be achieved with modularisation.
Collaborative eﬀorts
This is when several people need to work together for the
ontology development task. Modularisation promotes dividing the development
task among several people, and helps to achieve consistency such that there is no
conﬂict whereby the same ontology is altered by diﬀerent people.
Reuse
At times, ontology users or developers require a subset of an ontology
pertaining to a speciﬁc topic, and not the entire ontology.
Modular ontologies
are created such that ontology modules can be easily extracted and reused for the
application at hand.
11.2.2
Types
Ontology modules can be classiﬁed into diﬀerent types, based on the nature of
the module. There are four main types pertaining to their function, structure,
abstraction, and expressiveness, which is exhaustive at the time of writing. They
each have several subtypes, which we summarise here.
Functional
These are modules whereby a large domain is split into modules to
represent various subject domains or functions. Currently, there are ﬁve types of
modules with a speciﬁc function:

11.2.
Module dimensions
215
- Ontology design patterns: An ontology is modularised by identifying a por-
tion that can be easily reused to solve recurring ontology issues, i.e., an
ontology design pattern (recall Section 7.6) is created.
- Subject domain modules: There are various sub-domains in an ontology,
hence the ontology is modularised or split up into modules for each sub-
domain; e.g., to create a separate module for the habitats of animals that
then can be imported into the AWO.
- Isolation branch modules: A subset of entities from an ontology is extracted
given an input entity as a starting point; e.g., extracting all endurants from
DOLCE but not the rest.
Entities with weak dependencies to the input
entities are excluded from the extracted module.
- Locality modules: A subset of entities from an ontology is extracted given
an input entity as a starting point. All entities, including those with weak
dependencies to the input entity, are included in the extracted module.
- Privacy modules: Some sensitive information is removed from an ontology in
order to preserve the privacy of information for a particular application, or
from certain users.
Structural
These modules are generated motivated by the syntax, or structure
of the ontology. This includes modules for domain coverage, ontology matching,
and to optimise reasoning. For the ﬁrst, domain coverage modules: an ontology
covers a domain, so it is modularised structurally by placement of entities in the
taxonomy such that similar size modules could be generated and such that the
set of generated modules collectively cover the domain. For instance, the Founda-
tional Model of Anatomy [RMJ03] contains over 100 000 entities describing human
anatomy exhaustively.
This could be modularised structurally for ease of use.
Ontology matching modules concern the case where an ontology is modularised
speciﬁcally for the ontology matching process. The ontology should be modularised
into disjoint partitions so there is no repetition of entities, so as to promote ontol-
ogy matching. For instance, the Common Anatomy Reference Ontology (CARO)
[HNOS+08] aims to align existing anatomy ontologies. To assist with aligning it
to domain ontologies, CARO could be partitioned into smaller modules. Finally,
the optimal reasoning modules may be created form a larger ontology to promote
eﬃcient reasoning.
Abstraction
These are modules that are simpler or lighter than an original on-
tology due to hiding or deleting some detail. This hiding or deleting can be done
systematically in diﬀerent ways. We describe four options. First, one could create
an abstraction module by targeting certain axioms; e.g., axioms that relate classes
using object properties are removed. This results in a module with a decreased
horizontal structure. Alternatively, one can create vocabulary abstraction mod-
ules, resulting from removing a certain type of entity from an ontology; e.g., the
removal of individuals from an ontology. A computationally easier way of creating
abstraction modules are high-level abstraction modules: entities that exist at a
lower level in the hierarchy of the ontology, i.e., the more speciﬁc entities, are to

216
Chapter 11. Ontology modularisation
be removed. This results in a module with the higher-level entities of the ontol-
ogy. Conversely, a more reﬁned and elaborate way is to create weighted modules,
where some entities are deemed more important than others in an ontology. The
importance may be speciﬁed by the developer’s preference in preserving entities.
Expressiveness
There are modules that are created by altering a module’s lan-
guage expressive power. This then has an eﬀect on what is represented in the on-
tology. There are two types: sub-language and feature modules. For sub-language
modules, a module is created based on only prioritising the features of a sub-
language e.g., only preserving the axioms of an ontology that are OWL 2 EL. An
example is OpenGalen [RRZvdH03], which is a module of GALEN in OWL 2 EL for
the purpose of testing the lightweight ELK [KKS12] reasoner for ontologies within
OWL 2 EL expressiveness. With feature modules, a module is created by limiting
the language features but not based on a speciﬁc sub-language. For instance, the
removal of cardinality constraints for simpliﬁcation purposes.
11.2.3
Properties
Diﬀerent motivations, or aims, for modules and diﬀerent types give rise to a range
of properties that modules may have and that can be measured or identiﬁed qual-
itatively. Such properties that are associated with them gives the user more in-
formation about the modules.
This may be of the module itself or of a set of
modules.
Properties of a module
These are properties that a module exhibits by itself.
There are at least seven such properties:
- Seed signature: For this module, a user has provided an input entity to base
the module on.
- Information removal: Some information from an ontology is to be removed,
resulting in a module with less detail than the original ontology. This relates
back to the types of module already: abstraction. An example is the NCS
ontology with information noun classes of the about Niger-Congo B languages
[CK15]: it reuses only part of the GOLD ontology, because it has no need for
language features such as phonetic properties of a word. On the one hand, one
can have a so-called breadth abstraction, where the property of hiding some
of the relational properties of the ontology is used to decrease the ‘breadth’
of the module and, on the other hand, there is depth abstraction, referring
to the property of hiding some of the lower level classes of the ontology to
decrease the ‘depth’ of the module.
- Reﬁnement: New axioms are added to an ontology. Modules are typically
not created with new knowledge hence this occurs in cases such as the cre-
ation of inter-module links, as a result of ontology processing tools, or when
computationally-expensive ontology language features are altered. A concrete
example that reduced reasoning time was by removing the InverseObjectProp-
erties axioms and replacing it with the OWL ObjectInverseOf in the DMOP
ontology [KdKL14].

11.2.
Module dimensions
217
- Stand-alone: This describes a module that can exist on its own and does
not have dependencies to other ontologies or modules.
For instance, the
BioTopLite module [SB13] was extracted from the source, which is the top-
domain level ontology for the life sciences BioTop (recall its architecture from
Figure 7.4). BioTopLite is self-standing in the sense that it does not have
inter-module relations to other ontologies and it does not contain import
statements either.
- Source ontology: This is the original ontology that the user selects to modu-
larise.
- Proper subset: This describes a module that is smaller than its source ontol-
ogy, as it contains a subset of the entities of the source ontology.
- Imports: This is a module that contains knowledge from other ontologies by
using the import function of the OWL language.
Not all modules will have all these properties; e.g, a module can not be both
the source an a proper subset of itself. One may encounter complex combinations
to manage, such as where there is an import and subsequent abstraction over the
merged ontology.
Properties of a set of modules
These are properties that exist in a set of
related modules.
– Overlapping: This occurs when the same entity/ies extend over more than
one modules in a set of modules.
– Mutual exclusion: This occurs when an entity/ies does not extend over more
than one module in a set of modules. The modules share no common entities.
– Union equivalence: The union, or the set of all entities of the modules is
semantically equivalent to the original ontology.
– Partitioning: This describes the structural division of a large ontology into
modules.
– Inter-module interaction: When there are links to other modules in the set
of modules to preserve the knowledge of the original ontology.
– Pre-assigned number of modules: This occurs when the number of modules
to be created is known at the onset of the module generation.
The modules can be annotated with such properties and some may be computed.
For instance, annotating a module with the source ontology it was extracted from
and computing overlap between two modules by using the axioms and vocabulary
(provided that the URI is still the same).
11.2.4
Techniques
A range of techniques have been proposed to actually create or generate modules
from ontologies. We zoom in on three categories of algorithms: graph-based ones,
statistical approaches, and, for the lack of a better term, ‘semantic’ approaches.
Graph theory approaches
This set of techniques are based on graph theory
whereby the classes in the ontology are represented by vertices and the OWL object
properties are represented by edges. Then one can pick a graph-based algorithm to

218
Chapter 11. Ontology modularisation
compute the desired outcome. For instance, in graph partitioning, a large ontology
is divided structurally such that vertices (entities) are not shared among partitions
(modules). Modularity maximisation uses some modularity function to measure
the axiomatic relations of entities resulting in modules, such as optimising the
connection between nodes in graphs. Also, one could use clique detection to ﬁnd
sections of an ontology that may be safely removed.
Statistical approaches
This includes Hierarchical clustering, which is a method
of grouping together data (entities) by building a hierarchy of clusters.
Semantic approaches
These techniques are based more on the meaning of the
ontology, which can be both logic-based using the axioms as input or a user provides
subject domain semantics input to guide the modularisation process. For instance,
there is so-called locality modularity, which is a method to create modules based
on an input entity/ies with the condition that ‘conservation extension’ holds for
the given module, i.e., the meaning of every axiom from the original ontology is
preserved, and is thus a logic-based approach. A combination of that and user input
is query-bases modularity, where the ontology developer creates a query using a
query language such as SPARQL to base a module on. Even more user input is
expected in semantic-based abstractions. Here, the subject domain semantics of an
ontology is analysed and rules are used to prioritise certain entities in an ontology
to be included in a resultant module.
Further down the road of more user input is the A priori modularity: the
modular structure of the domain is decided upfront so that the modules are created
initially without a need for an original ontology. This one can do also manual
from a source ontology, called manual modularity, where the ontology developer
analyses an existing ontology to manually select which entities and axioms should
be removed from it to create a speciﬁc module.
Finally, there is language simpliﬁcation, where some axioms based on language
features are removed from an ontology resulting in a simpliﬁed module with limited
language expressivity. One may argue that it is a semantics-based approach or
syntax-based, depending on how it is carried out. One could forcefully implement
it though syntactic analysis without remodelling or try to add approximations for
the axioms slated for removal. For instance, if C ⊑= 2 R.D (qualiﬁed cardinality)
is disallowed, as it is in OWL DL, then a rough technique could just delete that
axioms, whereas an approximator might replace it with the C ⊑∃R.D or C ⊑=
2 R.⊤. Such an approximations strategy will not help with reducing the size of the
ontology, however, and thus not lead to the desired eﬀect of a smaller ontology.
This brings us to the next topic: how to evaluate the module created or generate
is any good?
11.2.5
Evaluation metrics
With multiple techniques and use cases for devising modules, the outcome of some
modularisation is unlikely to be the same sort of module, and for the same use case,
there may be diﬀerent techniques to accomplish it with varying levels of success.

11.2.
Module dimensions
219
This requires a notion of being able to specify what a ‘good’ module is. When
one tries to modularise an ontology, and one module has 95% of the content of the
source ontology and the other module has the other the remaining 5%, then clearly
that doesn’t help much. But should it be 50-50 in all cases? Probably not, and
it may not be relevant to some type of modules anyway (e.g., when one wants to
end up with a bare taxonomy). More broadly, this relates to some extent also to
metrics for ontologies, not just their modules.
In order to get to the stage of determining whether a module is good or not, we
ﬁrst need a set of criteria. Ideally, one would be able to compute automatically the
values of those criteria, and then some way to deter mine which values are ‘good’
values for which type of module. In order to arrive at that stage, we present a (non-
exhaustive) list of metrics with a short deﬁnition of each metric, together with its
corresponding equation. The metrics are grouped into ﬁve categories: structural
criteria, logical, relational with respect to other ontologies, information hiding, and
richness. A summary table is included at the end of this section. We refer to the
DOLCE ontology and its related modules in the text for examples demonstrating
the calculations for the metrics.
Structural criteria
The ﬁrst metric is size, which is a common metric used to
measure the number of entities in a module and for any ontology:
Size(M) = |M| = |C| + |OP| + |DP| + |I|.
(11.1)
where C are classes, OP object properties, DP data properties, and I instances.
One then can also compute the relative size, which is deﬁned as the size of a module
in comparison to the original ontology:
Relative size = |M|
|O|
(11.2)
where |M| is again the size of the module, as described in Eq. 11.1, and |O| is the
size of the ontology.
Example 11.2. The size from the DOLCE-perdurants module of DOLCE is cal-
culated as follows: it has 27 classes, 0 individuals, 70 object properties, and 0 data
properties. For its source ontology, DOLCE, there are 37 classes, 0 individuals, 70
object properties, and 0 data properties. Hence the relative size is 27+70
37+70 = 0.91%.
The appropriateness of module size is deﬁned as a mapping the size of an ontol-
ogy module to some appropriateness function, based on software design principles:
Appropriate(x) = 1
2 −1
2cos(x. π
250)
(11.3)
where x is the number of axioms in the module.
Example 11.3. The Temporal Relations module of the DOLCE ExtendedDnS
(Descriptions & Situations) ontology has 435 axioms. Therefore, its appropriate
size value is 1
2 −1
2cos(435. π
250) = 0.16. The appropriate size value of the Temporal
Relations module is rather low, due to the fact that the module has many more
axioms than the optimal of 250 as deﬁned by Schlicht and Stuckenschmidt.

220
Chapter 11. Ontology modularisation
Intra-module distance is the distance between entities in a module, which is
measured by counting the number of relations in the shortest path from one entity
to the other, for every entity in the module.
Intra-module distance(M) =
n
X
i
Farness(i)
(11.4)
where n is the number of nodes in the module, and Freeman’s Farness value ?? is
deﬁned as follows:
Farness(i) =
n
X
j
distanceij
(11.5)
where i and j are two entities in the module.
One then also can deﬁne and compute a relative intra-module distance, which is
based on the intra-module distance. It checks whether the overall distance between
the entities in the module has been reduced. It is measured as the diﬀerence be-
tween the intra-module distance of a source ontology and the intra module distance
of a module:
Relative intra-module distance(M) = Intra-module distance(O)
Intra-module distance(M)
(11.6)
Last, the cohesion metric measures the extent which entities are related to each
other in a module. It is measured by calculating the sum of the strength of relations
as a fraction over the number of all possible relations in a module:
Cohesion(M) =



P
Ci∈M
P
Cj∈M
SR(ci,cj)
|M|(|M|−1)
if —M— ¿ 1
1
otherwise
(11.7)
where |M| is the number of entities in the module as described in Eq. 11.1. The
product of |M|(|M|−1) represents the number of possible relations between entities
in M. The strength of the relation for each entity is calculated based on the farness
centrality measure for graph theory proposed by Freeman ?? from Eq. 11.5.
SR(ci, cj) =
(
1
farness(i)
if relations exist between i and cj
0
otherwise
(11.8)
Logical Criteria
The logical criteria included here are correctness and complete-
ness. Correctness states that no new axioms should be added to a module, i.e.,
every axiom that exists in the module should also exist in its original ontology.
Correctness(M) =
(
true
if Axioms(M) ⊆Axioms(O)
false
otherwise
(11.9)

11.2.
Module dimensions
221
whereas the completeness metric checks whether the meaning of every entity is
preserved in a module as in the source ontology:
Completeness(M) =



true
if
nP
i
Axioms(Entityi(M)) |= Axioms(Entityi(O))
false
otherwise
(11.10)
Example 11.4. In the source ontology, DOLCE the endurant entity is deﬁned with
the following set of properties (listed separately for ease of reading):
– endurant ⊑∀part.endurant
– endurant ⊑spatio-temporal-particular
– endurant ⊑∃participant-in.perdurant
– endurant ⊑∀speciﬁc-constant-constituent.endurant
– endurant ⊑¬ quality
– endurant ⊑¬ perdurant
– endurant ⊑¬ abstract
If DOLCE were to be modularised to create a branch module, containing only
the branch of Endurant entities, DOLCE-endurants, the endurant entity will end
up being deﬁned as follows:
– endurant ⊑∀part.endurant
– endurant ⊑spatio-temporal-particular
– endurant ⊑∃participant-in.perdurant
The meaning of the endurant entity was thus not fully preserved in the module
since the axiom endurant ⊑∀speciﬁc-constant-constituent.endurant existed in the
original ontology but not in the module. Therefore the DOLCE-endurants module
has a false value for the completeness metric.
Relational Criteria
The relational criteria all concern how modules relate to
other modules, assuming there is more than one module (be it generated from a
source ontology, or from manually having divided up the subject domain). There
are three metrics. First inter-module distance refers to that, for a set of modules,
one checks the number of modules that have to be considered to relate two entities:
Inter-module distance =



P
Ci,Cj∈(Mi,...,Mn)
NM(Ci,Cj)
|(Mi,..,Mn)|(|(Mi,..,Mn)|−1)
|(Mi, .., Mn)| > 1
1
otherwise
(11.11)
where NM(Ci, Cj) is the number of modules to consider to relate entities i and j.
The product of |(Mi, .., Mn)|(|(Mi, .., Mn)| −1) represents the number of possible
relations between entities in a set of modules Mi, ., Mn.
Second, there is coupling, which checks whether modules have relations to en-
tities in other modules, i.e., it is a way to compute the degree of interdependence

222
Chapter 11. Ontology modularisation
of a module:
Coupling(Mi) =





nP
i=0
nP
j=0
i̸=j
NELMi,Mj
|Mi||Mj|
NELMi,Mj > 0
0
otherwise
(11.12)
where |Mi| is the number of entities in the current module and |Mj| is the number
of entities in a related module in the set of n modules.
Third, redundancy checks if the same axioms exist in various modules within a
set of module, i.e., whether modules overlap due to the duplication of axioms:
Redundancy =
(
kP
i=1
ni) −n
kP
i=1
ni
(11.13)
Information hiding
Ontology modules sometimes are designed with the inten-
tion to hide aspects of the source ontology from the module for privacy and simpli-
ﬁcation reasons. Information hiding within modules assesses whether the module
encapsulates all the information in the module such that the privacy is preserved
for each module. To this end, two criteria to measure information hiding properties
of an ontology module are formulated. The ﬁrst one is encapsulation, which aims
to describe how well a module’s knowledge is ‘protected’, i.e., whether it can be
easily exchanged for another module or be modiﬁed without aﬀecting the rest or
the application that uses it. It is deﬁned as follows:
Encapsulation(M i) = 1 −
n−1
P
j=1
|Axij|
|Axi|
n
(11.14)
The related metric Independence checks if a module is self-contained, i.e., if it
could be altered on its own without aﬀecting other modules. If so, then ontology
modules can evolve independently and, hence, the semantics of the ontology as a
whole could change without the need for all the modules to be changed.
Ind(M i) =
(
true
Encapsulation(Mi) = 1 and Coupling(Mi) = 0
false
otherwise
(11.15)
where |Mi| is the number of entities in the current module and |Mj| is the number
of entities in a related module in the set of n modules.
Richness Criteria
This category of metrics has to do with the type of axioms
that are being used in the module. Two are deﬁned here to indicate the idea.
Attribute richness checks the average number of attributes per class in an ontology,
as:
AR(M) = |att|
|C|
(11.16)

11.3.
Modularisation framework
223
Table 11.1: A summary of the set of evaluation metrics with their expected value range
and values that are considered good.
Evaluation metric
Value range
Value type
Good value
Relative size
1 ≥i ≥0
decimal
small to medium
Appropriateness
1 ≥i ≥0
decimal
large
Cohesion
1 ≥i ≥0
decimal
small
Correctness
true or false
boolean
true
Completness
true or false
boolean
true
Coupling
i ≥0
decimal
small
Redundancy
1 ≥i ≥0
decimal
small to medium
Encapsulation
1 ≥i ≥0
decimal
large
Independence
true or false
boolean
true
where att is measured by the number of data properties in the module and |C| is
the number of classes in the module.
Another one is inheritance richness, which checks the average number of sub-
classes per class, as follows:
IRS(M) =
P
Ci∈C
|HC(C1, Ci)|
|C|
(11.17)
where |HC(C1, Ci)| is the number of subclasses per class and |C| is the total num-
ber of classes in the ontology.
A summary of some of the evaluation metrics, where the values could be labelled
as ‘good’ using a 4-point scale of small (0-0.25), medium (0.25-0.5), moderate (0.51-
0.75), and large (0.75-1), and true/false values is shown in Table 11.1.
11.3
Modularisation framework
With the dimensions identiﬁed and a set of metrics deﬁned, we can now look at
the next step: devise some sort of framework that puts it all together and ideally
evaluated with actual modules, so that the goalposts can be moved even further,
being toward methods for modularisation and guidance for modellers.
Experimentation was performed with those dimensions and a set of modules,
where the modules were classiﬁed according to the dimensions listed in Section 11.2
[KK15b]. The resulting high-level view of the framework is shown in Figure 11.1.
The framework states that: A module’s use-case results in modules of a speciﬁc
type. A module of a speciﬁc type is generated by a modularisation technique. A
module of a speciﬁc type is assessed by a set of evaluation metrics. Modularisation
techniques are the cause for modules to be annotated with certain properties.
Let’s drill down into the details of each arrow in Figure 11.1. For the ﬁrst
relation in the framework, between the module use-case and type, we have the
relations as shown in Figure 11.2. The dependencies in Figure 11.2 should be read

224
Chapter 11. Ontology modularisation
as follows: If an ontology developer wishes to create a module for a use-case of
Validation, it would result in a module of type Structural: Domain Coverage. The
full set of dependencies is shown in Figure 11.2.
For the relation between the module type and technique, we have the relations
as shown in Figure 11.3. The dependencies in Figure 11.3 should be read as follows,
illustrated with an example: A module of type Structural: Domain Coverage can
be created using a Graph partitioning technique. The full set of dependencies is
shown in Figure 11.3. Perhaps of note is that for all abstraction and expressiveness
modules, only manual methods are used, and for all structural module, there is
some automated technique available.
For the dependencies between technique and properties in the framework, we
have the relations as shown Figure 11.4. The dependencies in Figure 11.4 should
be read as follows: e.g., a module created by an a priori technique has the fol-
lowing properties: Imports, Stand-alone, Overlapping, and Pre-assigned number of
modules.
Finally, for the relation between type and evaluation metrics, we have the re-
lations as shown Figure 11.5. For each type, it lists the relevant metrics together
with desirable values for that metric. The dependencies in Figure 11.5 should be
read as follows, e.g.: a module of type Locality is a good quality module if its
evaluation metric values are as follows: Relative size: medium, cohesion: small,
correctness: true. There are two important conclusions that can be drawn from
this. First, not all metrics are relevant for each type of module. Second, in some
cases the values of the metrics are diﬀerent for diﬀerent types of modules; e.g., for
ODPs, the relative size should be small, wheres for privacy modules it should be
medium.
The framework for modularity can be used to guide ontology developers along
the modularisation process. For instance, say that the user looks at an existing
restaurant ontology to reuse, with the intent of only creating a module for a coﬀee
shop. The ontology developer can start the modularisation process by identifying
the use-case, which is re-use. From there on, the type, technique, property, and
evaluation metrics can be formulated. The user will be clear on the type of on-
tology module that has to be created, and the method or technique that should
be employed to create such a module. The technique is used to determine the
properties of the module. The properties are useful in annotating a module with
additional information (metadata) which could promote ontology usage and reuse.
Lastly, the developer can assess the quality of a module that has been developed
Technique
Use-case
Type
Evaluation
metric
Property
Figure 11.1:
A high-level view of the framework for modularity (source: based on
[KK15b]).

11.4.
Exercises
225
Validation
Functional:
Subject domain
Structural: Domain
coverage
Maintenance
Processing
Comprehension
Abstraction: Axiom
abstraction
Abstraction: Vocabulary 
abstraction
Abstraction: High-level
abstraction
Abstraction: Weighted 
abstraction
Reasoning
Collaboration
Reuse
Expressiveness:
Sublanguage
expressiveness
Expressiveness: Feature
expressiveness
Structural: Optimal
reasoning
Functional:
Privacy
Structural: Ontology 
matching
Functional:
ODPs
Functional:
Locality
Functional:
Isolation branch
Figure 11.2: The dependencies between use-case and type. For instance, for a use-case
of maintenance, the resultant modules are subject-domain modules or domain coverage
modules, i.e. maintenance results in a functional or structural module.
by comparing its evaluation metrics to what is expected for that module from the
framework.
11.4
Exercises
Review question 11.1. What are some of the dimensions speciﬁed for modules?
Review question 11.2. Name three techniques for modularising a large ontology.
Review question 11.3. There are many criteria one can let loose on an otology
to evaluate it. Name ﬁve criteria that concern the structure of the ontology and
three that have to do with how one module relates to the other modules that were
made from one large ontology.
Review question 11.4. Describe in your own words the framework for modularity
introduced in this chapter, and how you would be able to use that to modularise,
e.g., SNOMED CT, the FMA, or DOLCE.
Exercise 11.1. Recall that we have imported DOLCE and BFO into the AWO. In
terms of all the deﬁnitions introduced in this chapter, what sort of modularisation
do we have there?

226
Chapter 11. Ontology modularisation
Domain
Coverage
Ontology
Matching
Optimal
Reasoning
Structural
ODP
Subject
Domain
Locality
Functional
Isolation
Branch
Privacy
Sub-language
Expressiveness
Feature
Expressiveness
Expressiveness
Axiom Abs.
Voc Abs.
High-level Abs.
Abstraction
Weighted Abs.
Graph
partitioning
Locality-
based
A priori
Manual
Figure 11.3:
The dependencies between type and technique.
For instance, for an
ontology design pattern module, both manual and a priori methods are used.
Exercise 11.2. The Quantities, Units, Dimensions and Data Types (QUDT) on-
tologies are a set of ontology modules. These modules describe terminology used in
science and engineering for the representation of physical quantities, units of mea-
sure, and their dimensions [3]. A table of evaluation criteria for the QUDT modules
is shown in Table 11.2. Study the evaluation metrics to determine whether the set
of QUDT modules are a good set of modules. Explain your ﬁndings.
Exercise 11.3. Return to the motivating Example 11.1, speciﬁcally, “The devel-
oper wishes to extract only the speciﬁc Book-toy entity and the generalised Toy-
property entity”. Determine the use-case for modularisation for this, and thereafter
determine the type of module to be created, technique to be used, the properties
it would exhibit, and which evaluation criteria could be used to measure it.
Table 11.2: The metrics for the QUDT ontology modules; approp = appropriateness,
encap. = encapsulation, redund. = redundancy, avg. = average, med. = median.
Structural criteria
Size
Atomic
size
No. of
axioms
Approp.
Intra
module
distance
Cohesion
Avg.
595.38
5.71
3112.00
0.91
8577.25
0.008
Richness
criteria
Information
hiding criteria
Relational criteria
AR
IR
Encap.
Coupling
Independence
Redund.
Avg.
1.69
1.89
0.99
0
25% true
0.50

11.5.
Literature and reference material
227
Graph partitioning
Locality-based
A priori
Stand-alone
Imports
Overlapping
Pre-assigned 
no. of modules
Information removal
Overlapping
Stand-alone
Seed signature
Source ontology
Proper subset
Manual
Information retrieval
Source ontology
Stand-alone
Imports
Abstraction
Proper subset
Information removal
Stand-alone
Source ontology
Overlapping
Mutual exclusion
Proper subset
Breadth abs.
Depth abs.
Reﬁnement
Seed signature
Partitioning
Inter-module
interaction
Figure 11.4: The dependencies between technique and property. For instance, when
graph partitioning techniques are used, the following properties may be relevant for the
modules, among others: information removal, standalone, source ontology, and proper
subset.
Ontology design
pattern
Relative size: small
Cohesion: small
Completeness: true
Subject domain
Cohesion: small
Encapsulation:
large
Coupling: small
Redundancy: small
Isolation branch
Cohesion: small
Locality
Relative size:
medium
Cohesion:
small
Correctness:
true
Privacy
Relative size:
medium
Cohesion:
small
Vocabulary
abstraction
Appropriate-
ness: large
Cohesion:
small
Correctness:
true
Domain coverage
Relative size: small
Cohesion: small
Encapsulation:
large
Coupling: small
Redundancy: small
High level
abstraction
Appropriate-
ness: large
Cohesion:
small
Weighted
abstraction
Relative size:
medium
Cohesion:
small
Expressive-
ness sub-
language
Cohesion:
small
Expressive-
ness feature
Cohesion:
small
Ontlogy matching
Relative size: small
Cohesion: small
Encapsulation:
large
Independence: true
Coupling: small
Redundancy: small
Optimal reasoning
Cohesion: small
Correctness: true
Encapsulation:
large
Coupling: small
Redundancy:
medium
Axiom
abstraction
Cohesion:
small
Correctness:
true
Figure 11.5: The dependencies between type and evaluation property.
11.5
Literature and reference material
1. d’Aquin M., Schlicht A., Stuckenschmidt H. and Sabou M. Criteria and eval-
uation for ontology modularization techniques. Modular ontologies, Springer,
Berlin, Heidelberg, 2009, (pp. 67-89).
2. Khan, Z.C. and Keet, C.M. An empirically-based framework for ontology
modularisation. Applied Ontology, IOS Press, 2015, 10(3-4):171-95.


Bibliography
[ACDG+05]
Andrea Acciarri, Diego Calvanese, Giuseppe De Giacomo, Domenico Lembo, Mau-
rizio Lenzerini, Mattia Palmieri, and Riccardo Rosati. QuOnto: Querying Ontolo-
gies. In Proc. of the 20th Nat. Conf. on Artiﬁcial Intelligence (AAAI 2005), pages
1670–1671, 2005.
[ACK+07]
Alessandro Artale, Diego Calvanese, Roman Kontchakov, Vladislav Ryzhikov, and
Michael Zakharyaschev. Reasoning over extended ER models. In Christine Parent,
Klaus-Dieter Schewe, Veda C. Storey, and Bernhard Thalheim, editors, Proceedings
of the 26th International Conference on Conceptual Modeling (ER’07), volume
4801 of LNCS, pages 277–292. Springer, 2007. Auckland, New Zealand, November
5-9, 2007.
[ACKZ09]
Alessandro Artale, Diego Calvanese, Roman Kontchakov, and Michael Za-
kharyaschev. DL-Lite without the unique name assumption. In Proc. of the 22nd
Int. Workshop on Description Logic (DL 2009), volume 477 of CEUR-WS, 2009.
http://ceur-ws.org/.
[AFK12]
Ronell Alberts, Thomas Fogwill, and C. Maria Keet. Several required OWL fea-
tures for indigenous knowledge management systems. In P. Klinov and M. Hor-
ridge, editors, 7th Workshop on OWL: Experiences and Directions (OWLED
2012), volume 849 of CEUR-WS, page 12p, 2012. 27-28 May, Heraklion, Crete,
Greece.
[AFWZ02]
A. Artale, E. Franconi, F. Wolter, and M. Zakharyaschev. A temporal description
logic for reasoning about conceptual schemas and queries. In S. Flesca, S. Greco,
N. Leone, and G. Ianni, editors, Proceedings of the 8th Joint European Conference
on Logics in Artiﬁcial Intelligence (JELIA-02), volume 2424 of LNAI, pages 98–
110. Springer Verlag, 2002.
[AGK08]
Alessandro Artale, Nicola Guarino, and C. Maria Keet.
Formalising temporal
constraints on part-whole relations. In Gerhard Brewka and Jerome Lang, ed-
itors, 11th International Conference on Principles of Knowledge Representation
and Reasoning (KR’08), pages 673–683. AAAI Press, 2008. Sydney, Australia,
September 16-19, 2008.
[AKK+17]
Alessandro Artale, Roman Kontchakov, Alisa Kovtunova, Vladislav Ryzhikov,
Frank Wolter, and Michael Zakharyaschev. Ontology-mediated query answering
over temporal data: A survey. In Sven Schewe, Thomas Schneider, and Jef Wijsen,
editors, Proceedings of the 24th International Symposium on Temporal Representa-
tion and Reasoning (TIME’17), pages 1:1–1:36. Leibniz International Proceedings
in Informatics, 2017.
229

230
Bibliography
[AKL+07]
A. Artale, R. Kontchakov, C. Lutz, F. Wolter, and M. Zakharyaschev. Temporalis-
ing tractable description logics. In Inter. Symposium on Temporal Representation
and Reasoning (TIME07). IEEE Computer Society, 2007.
[ALG13]
I. Androutsopoulos, G. Lampouras, and D. Galanis. Generating natural language
descriptions from owl ontologies: the naturalowl system.
Journal of Artiﬁcial
Intelligence Research, 48:671–715, 2013.
[Ali04]
A. Aliseda. Logics in scientiﬁc discovery. Foundation of Science, 9:339–363, 2004.
[All83]
James F. Allen. Maintaining knowledge about temporal intervals. Communications
of the ACM, 26(11):832–843, 1983.
[APS07]
A. Artale, C. Parent, and S. Spaccapietra. Evolving objects in temporal infor-
mation systems. Annals of Mathematics and Artiﬁcial Intelligence, 50(1-2):5–38,
2007.
[AvH03]
G. Antoniou and F. van Harmelen. A Semantic Web Primer. MIT Press, USA,
2003.
[AWP+08]
Dimitra Alexopoulou, Thomas W¨achter, Laura Pickersgill, Cecilia Eyre, and
Michael Schroeder. Terminologies for text-mining; an experiment in the lipoprotein
metabolism domain. BMC Bioinformatics, 9(Suppl 4):S2, 2008.
[BACW14]
N. Bouayad-Agha, G. Casamayor, and L. Wanner. Natural language generation
in the context of the semantic web. Semantic Web Journal, 5(6):493–513, 2014.
[BBK+17]
Franz Baader, Stefan Borgwardt, Patrick Koopmann, Ana Ozaki, and Veronika
Thost. Metric temporal description logics with interval-rigid names. In C. Dixon
and M. Finger, editors, Proceedings of the International Symposium on Frontiers of
Combining Systems (FroCoS’17), volume 10483 of LNCS, pages 60–76. Springer,
2017.
[BBL05]
F. Baader, S. Brandt, and C. Lutz. Pushing the EL envelope. In Proc. of the 19th
Joint Int. Conf. on Artiﬁcial Intelligence (IJCAI 2005), volume 5, pages 364–369,
2005.
[BC14]
Paul Buitelaar and Philipp Cimiano, editors. Towards the Multilingual Semantic
Web: Principles, Methods and Applications. Springer, 2014.
[BCDG05]
D. Berardi, D. Calvanese, and G. De Giacomo. Reasoning on UML class diagrams.
Artiﬁcial Intelligence, 168(1-2):70–118, 2005.
[BCHM09]
P. Buitelaar, P. Cimiano, P. Haase, and Sintek M. Towards linguistically grounded
ontologies. In L. Aroyo et al., editors, Proceedings of the Extended Semantic Web
Conference (ESWC’09), volume 5554 of LNCS, pages 111–125. Springer, 2009.
[BCM+08]
F. Baader, D. Calvanese, D. L. McGuinness, D. Nardi, and P. F. Patel-Schneider,
editors. The Description Logics Handbook – Theory and Applications. Cambridge
University Press, 2 edition, 2008.
[BD07]
T. Bittner and M. Donnelly. A temporal mereology for distinguishing between
integral objects and portions of stuﬀ. In Proceedings of AAAI’07, pages 287–292,
2007. Vancouver, Canada.
[BDFG14]
Alessio Bosca, Mauro Dragoni, Chiara Di Francescomarino, and Chiara Ghi-
dini. Collaborative management of multilingual ontologies. In Paul Buitelaar and
Philip Cimiano, editors, Towards the Multilingual Semantic Web, pages 175–192.
Springer, 2014.
[BE93]
Jon Barwise and John Etchemendy. The language of ﬁrst-order logic. Stanford,
USA: CSLI Lecture Notes, 3rd edition, 1993.

Bibliography
231
[BGSS07]
F. Baader, B. Ganter, B. Sertkaya, and U. Sattler. Completing description logic
knowledge bases using formal concept analysis. In Proc. of IJCAI 2007, volume 7,
pages 230–235, 2007. Hyderabad, India, 2007.
[BH96]
A. C. Bloesch and T. A. Halpin. ConQuer: a conceptual query language. In Pro-
ceedings of ER’96: 15th International Conference onconceptual modeling, volume
1157 of LNCS, pages 121–133. Springer, 1996.
[BH97]
A. C. Bloesch and T. A. Halpin. Conceptual Queries using ConQuer-II. In Pro-
ceedings of ER’97: 16th International Conference on Conceptual Modeling, volume
1331 of LNCS, pages 113–126. Springer, 1997.
[BHJ+15]
E. Blomqvist, P. Hitzler, K. Janowicz, A. Krisnadhi, T. Narock, and M. Solanki.
Considerations regarding ontology design patterns. Semantic Web, 7(1):1–7, 2015.
[BLHL01]
Tim Berners-Lee, James Hendler, and Ora Lassila. The semantic web. Scientiﬁc
American Magazine, May 17, 2001, 2001.
[BLK+09]
Christian Bizer, Jens Lehmann, Georgi Kobilarov, S¨oren Auer, Christian Becker,
Richard Cyganiak, and Sebastian Hellmann. Dbpedia – a crystallization point for
the web of data. Journal of Web Semantics: Science, Services and Agents on the
World Wide Web, 7:154165, 2009.
[BM09]
Stefano Borgo and Claudio Masolo. Foundational choices in DOLCE. In Steﬀen
Staab and Rudi Studer, editors, Handbook on Ontologies, pages 361–381. Springer,
2 edition, 2009.
[BMF95]
John A. Bateman, Bernardo Magnini, and Giovanni Fabris. The Generalized Upper
Model Knowledge Base: Organization and Use. In N. J. I. Mars, editor, Towards
very large knowledge bases: knowledge building and knowledge sharing, pages 60–
72, Amsterdam, 1995. IOS Press.
[BPTA17]
Sotiris Batsakis, Euripides Petrakis, Ilias Tachmazidis, and Grigoris Antoniou.
Temporal representation and reasoning in OWL 2.
Semantic Web Journal,
8(6):981–1000, 2017.
[Bro06]
Matthias Brochhausen. The Derives from relation in biomedical ontologies. Stud-
ies in Health Technology and Informatics, 124:769–774, 2006.
[BS05]
E. Blomqvist and K. Sandkuhl. Patterns in ontology engineering - classiﬁcation
of ontology patterns. In Proc. of the 7th International Conference on Enterprise
Information Systems, 2005. Miami, USA, May 2005.
[BS11]
Fernando Bobillo and Umberto Straccia. Fuzzy ontology representation using OWL
2. International Journal of Approximate Reasoning, 52:1073–1094, 2011.
[BSSH08]
Elena Beisswanger, Stefan Schulz, Holger Stenzhorn, and Udo Hahn. BioTop: An
upper domain ontology for the life sciences - a description of its current structure,
contents, and interfaces to OBO ontologies. Applied Ontology, 3(4):205–212, 2008.
[BST07]
Franz Baader, Baris Sertkaya, and Anni-Yasmin Turhan. Computing the least
common subsumer w.r.t. a background terminology.
Journal of Applied Logic,
5(3):392–420, 2007.
[CCKE+17]
Diego Calvanese, Benjamin Cogrel, Sarah Komla-Ebri, Roman Kontchakov, Da-
vide Lanti, Martin Rezk, Mariano Rodriguez-Muro, and Guohui Xiao.
Ontop:
Answering SPARQL queries over relational databases.
Semantic Web Journal,
8(3):471–487, 2017.
[CCO+13]
V.K. Chaudhri, B. Cheng, A. Overholtzer, J. Roschelle, A. Spaulding, P. Clark,
M. Greaves, and D Gunning. Inquire biology: A textbook that answers questions.
AI Magazine, 34(3):55–72, 2013.

232
Bibliography
[CDG03]
D. Calvanese and G. De Giacomo. The DL Handbook: Theory, Implementation
and Applications, chapter Expressive description logics, pages 178–218. Cambridge
University Press, 2003.
[CDGL99]
Diego Calvanese, Giuseppe De Giacomo, and Maurizio Lenzerini. Reasoning in
expressive description logics with ﬁxpoints based on automata on inﬁnite trees.
In Proc. of the 16th Int. Joint Conf. on Artiﬁcial Intelligence (IJCAI’99), pages
84–89, 1999.
[CGHM+08]
B. Cuenca Grau, I. Horrocks, B. Motik, B. Parsia, P. Patel-Schneider, and U. Sat-
tler.
OWL 2: The next step for OWL.
Journal of Web Semantics: Science,
Services and Agents on the World Wide Web, 6(4):309–322, 2008.
[CGL+07]
Diego Calvanese, Giuseppe De Giacomo, Domenico Lembo, Maurizio Lenzerini,
and Riccardo Rosati. Tractable reasoning and eﬃcient query answering in descrip-
tion logics: The DL-Lite family. Journal of Automated Reasoning, 39(3):385–429,
2007.
[CGL+09]
Diego Calvanese, Giuseppe De Giacomo, Domenico Lembo, Maurizio Lenzerini,
Antonella Poggi, Mariano Rodr´ıguez-Muro, and Riccardo Rosati. Ontologies and
databases: The DL-Lite approach. In Sergio Tessaris and Enrico Franconi, editors,
Semantic Technologies for Informations Systems - 5th Int. Reasoning Web Summer
School (RW 2009), volume 5689 of LNCS, pages 255–356. Springer, 2009. Brixen-
Bressanone, Italy, 30 August - 4 September 2009.
[CK14]
Catherine Chavula and C. Maria Keet. Is lemon suﬃcient for building multilingual
ontologies for Bantu languages? In C. Maria Keet and Valentina Tamma, editors,
Proceedings of the 11th OWL: Experiences and Directions Workshop (OWLED’14),
volume 1265 of CEUR-WS, pages 61–72, 2014. Riva del Garda, Italy, Oct 17-18,
2014.
[CK15]
Catherine Chavula and C. Maria Keet. An orchestration framework for linguistic
task ontologies. In E. adn others Garoufallou, editor, Proceedings of teh 9th Meta-
data and Semantics Research Conference (MTSR’15), volume 544 of CCIS, pages
3–14. Springer, 2015. 9-11 September, 2015, Manchester, UK.
[CKK+17]
M. Codescu, E. Kuksa, O. Kutz, T. Mossakowski, and F. Neuhaus. Ontohub: A
semantic repository for heterogeneous ontologies. Applied Ontology, 2017. Forth-
coming.
[CKN+10]
Diego Calvanese, C. Maria Keet, Werner Nutt, Mariano Rodr´ıguez-Muro, and
Giorgio Stefanoni. Web-based graphical querying of databases through an ontol-
ogy: the WONDER system. In Sung Y. Shin, Sascha Ossowski, Michael Schu-
macher, Mathew J. Palakal, and Chih-Cheng Hung, editors, Proceedings of ACM
Symposium on Applied Computing (ACM SAC’10), pages 1389–1396. ACM, 2010.
March 22-26 2010, Sierre, Switzerland.
[CLM+16]
D. Calvanese, P. Liuzzo, A. Mosca, J. Remesal, M. Rezk, and G. Rull. Ontology-
based data integration in epnet: Production and distribution of food during the ro-
man empire. Engineering Applications of Artiﬁcial Intelligence, 51:212–229, 2016.
[CLs07]
Common Logic (CL): a framework for a family of logic-based languages, 2007.
https://www.iso.org/standard/39175.html.
[CMFL05]
Oscar Corcho and Angel L´opez-Cima Mariano Fern´andez-L´opez, Asunci´on G´omez-
P´erez. Building legal ontologies with methontology and webode. In Law and the
Semantic Web 2005, volume 3369 of LNAI, pages 142–157. Springer LNAI, 2005.
[CMSV09]
Philipp Cimiano, Alexander M¨adche, Steﬀen Staab, and Johanna V¨olker. Ontology
learning. In S. Staab and R. Studer, editors, Handbook on Ontologies, pages 245–
267. Springer Verlag, 2009.

Bibliography
233
[Cot10]
A. J. Cotnoir. Anti-symmetry and non-extensional mereology. The Philosophical
Quarterly, 60(239):396–405, 2010.
[CS94]
Tiziana Catarci and Giuseppe Santucci. Query by diagram: a graphical environ-
ment for querying databases. ACM SIGMOD Record, 23(2):515, 1994.
[CSG+10]
Adrien Coulet, Nigam H. Shah, Yael Garten, Mark Musen, and Russ B. Altman.
Using text to build semantic networks for pharmacogenomics. Journal of Biomed-
ical Informatics, 43(6):1009–1019, 2010.
[CT98]
J. Chomicki and D. Toman. Logics for databases and information systems, chapter
Temporal logic in information systems. Kluwer, 1998.
[D+10]
Emek Demir et al. The BioPAX community standard for pathway data sharing.
Nature Biotechnology, 28(9):935–942, 2010.
[DAA+08]
Heiko Dietze, Dimitra Alexopoulou, Michael R. Alvers, Liliana Barrio-Alvers,
Bill Andreopoulos, Andreas Doms, Joerg Hakenberg, Jan Moennich, Conrad
Plake, Andreas Reischuck, Loic Royer, Thomas Waechter, Matthias Zschunke, and
Michael Schroeder. Gopubmed: Exploring pubmed with ontological background
knowledge. In Stephen A. Krawetz, editor, Bioinformatics for Systems Biology.
Humana Press, 2008.
[Daw17]
Zubeida C. Dawood. A foundation for ontology modularisation. Phd thesis, De-
partment Computer Science, University of Cape Town, November 2017 2017.
[DB09]
Maureen Donnelly and Thomas Bittner. Summation relations and portions of stuﬀ.
Philosophical Studies, 143:167–185, 2009.
[dCL06]
P. C. G. da Costa and K. B. Laskey. PR-OWL: A framework for probabilistic
ontologies. In Proceedings FOIS’06, pages 237–249. IOS Press, 2006.
[dFE10]
Claudia d’Amato, Nicola Fanizzi, and Floriana Esposito. Inductive learning for
the Semantic Web: What does it buy? Semantic Web Journal, 1(1,2):53–59, 2010.
[DGR12]
Chiara Di Franscescomarino, Chiara Ghidini, and Marco Rospocher. Evaluating
wiki-enhanced ontology authoring. In A ten Teije et al., editors, 18th International
Conference on Knowledge Engineering and Knowledge Management (EKAW’12),
volume 7603 of LNAI, pages 292–301. Springer, 2012. Oct 8-12, Galway, Ireland.
[DHI12]
A. Doan, A. Y. Halevy, and Z. G. Ives. Principles of Data Integration. Morgan
Kaufmann, 2012.
[EGOMA06]
H. El-Ghalayini, M. Odeh, R. McClatchey, and D. Arnold. Deriving conceptual
data models from domain ontologies for bioinformatics.
In 2nd Conference on
Information and Communication Technologies (ICTTA’06), pages 3562 – 3567.
IEEE Computer Society, 2006. 24-28 April 2006, Damascus, Syria.
[EM05]
J. Euzenat and A. Montanari. Handbook of temporal reasoning in artiﬁcial intel-
ligence, chapter Time granularity, pages 59–118. Amsterdam: Elsevier, 2005.
[ES07]
Jerome Euzenat and Pavel Shvaiko. Ontology Matching. Springer, 2007.
[FBR+16]
R. A. Falbo, M. P. Barcelos, F. B. Ruy, G. Guizzardi, and R. S. S. Guizzardi.
Ontology pattern languages. In A. Gangemi, P. Hizler, K. Janowicz, A. Krisnadhi,
and V. Presutti, editors, Ontology Engineering with Ontology Design Patterns:
Foundations and Applications. IOS Press, 2016.
[Fer16]
S. Ferr´e. Semantic authoring of ontologies by exploration and elimination of possi-
ble worlds. In E. Blomqvist, P. Ciancarini, F. Poggi, and F. Vitali, editors, Proceed-
ings of the 20th International Conference on Knowledge Engineering and Knowl-
edge Management (EKAW’16), volume 10024 of LNAI, pages 180–195. Springer,
2016. 19-23 November 2016, Bologna, Italy.

234
Bibliography
[FFT12]
Pablo R. Fillottrani, Enrico Franconi, and Sergio Tessaris. The ICOM 3.0 intelli-
gent conceptual modelling tool and methodology. Semantic Web Journal, 3(3):293–
306, 2012.
[FGGP13]
Ricardo A Falbo, Giancarlo Guizzardi, Aldo Gangemi, and Valentina Presutti.
Ontology patterns: clarifying concepts and terminology. In Proc. of OSWP’13,
2013.
[FGPPP99]
M. Fern´andez, A. G´omez-P´erez, A. Pazos, and J. Pazos.
Building a chemical
ontology using METHONTOLOGY and the ontology design environment. IEEE
Expert: Special Issue on Uses of Ontologies, January/February:37–46, 1999.
[FGT10]
Enrico Franconi, Paolo Guagliardo, and Marco Trevisan.
An intelligent query
interface based on ontology navigation. In Workshop on Visual Interfaces to the
Social and Semantic Web (VISSW’10), 2010. Hong Kong, February 2010.
[Fin00]
Kit Fine. Neutral relations. The Philosophical Review, 109(1):1–33, 2000.
[FK15]
Pablo Rub´en Fillottrani and C. Maria Keet. Evidence-based languages for concep-
tual data modelling proﬁles. In T. Morzy et al., editors, 19th Conference on Ad-
vances in Databases and Information Systems (ADBIS’15), volume 9282 of LNCS,
pages 215–229. Springer, 2015. 8-11 Sept, 2015, Poitiers, France.
[FK17]
Pablo R. Fillottrani and C. Maria Keet. Patterns for heterogeneous tbox mappings
to bridge diﬀerent modelling decisions. In E. Blomqvist et al., editors, Proceeding of
the 14th Extended Semantic Web Conference (ESWC’17), volume 10249 of LNCS,
pages 371–386. Springer, 2017. 30 May - 1 June 2017, Portoroz, Slovenia.
[FKK10]
Norbert E. Fuchs, Kaarel Kaljurand, and Tobias Kuhn. Discourse Representation
Structures for ACE 6.6. Technical Report iﬁ-2010.0010, Department of Informatics,
University of Zurich, Zurich, Switzerland, 2010.
[FR12]
Sebastien Ferr´e and Sebastian Rudolph. Advocatus diaboli
exploratory enrich-
ment of ontologies with negative constraints. In A ten Teije et al., editors, 18th
International Conference on Knowledge Engineering and Knowledge Management
(EKAW’12), volume 7603 of LNAI, pages 42–56. Springer, 2012. Oct 8-12, Galway,
Ireland.
[Gan05]
Aldo Gangemi. Ontology design patterns for semantic web content. In Yolanda Gil,
Enrico Motta, V. Richard Benjamins, and Mark A. Musen, editors, Proceedings
of the 4th International Semantic Web Conference (ISWC’05), pages 262–276,
Berlin, Heidelberg, 2005. Springer. Galway, Ireland, November 6-10, 2005.
[Gar17]
Daniel Garijo. WIDOCO: a wizard for documenting ontologies. In C. d’Amato
et al., editors, The Semantic Web
ISWC 2017, volume 10588 of LNCS, pages
94–102, Berlin, 2017. Springer.
[GB92]
J. A. Goguen and R. M. Burstall. Institutions: Abstract Model Theory for Speci-
ﬁcation and Programming. Journal of the Association for Computing Machinery,
39(1):95–146, 1992. Predecessor in: LNCS 164, 221–256, 1984.
[GB11]
Normunds Gruzitis and Guntis Barzdins. Towards a more natural multilingual
controlled language interface to OWL. In Proceedings of the Ninth International
Conference on Computational Semantics, IWCS ’11, pages 335–339, Stroudsburg,
PA, USA, 2011. Association for Computational Linguistics.
[GBM07]
Rolf Gr¨utter and Bettina Bauer-Messmer. Combining OWL with RCC for spa-
tioterminological reasoning on environmental data. In Third international Work-
shop OWL: Experiences and Directions (OWLED 2007), 2007.
6-7 June 2007,
Innsbruck, Austria.
[Gen00]
Gene Ontology Consortium. Gene Ontology: tool for the uniﬁcation of biology.
Nature Genetics, 25:25–29, 2000.

Bibliography
235
[GF95]
M. Gr¨uninger and M. S. Fox. Methodology for the design and evaluation of on-
tologies. In IJCAI Workshop on Basic Ontological Issues in Knowledge Sharing,
1995.
[GH07]
Christine Golbreich and Ian Horrocks. The OBO to OWL mapping, GO to OWL
1.1!
In Proc. of the Third OWL Experiences and Directions Workshop, volume
258 of CEUR-WS, 2007. http://ceur-ws.org/.
[GHH+12]
Michael Gr¨uninger, Torsten Hahmann, Ali Hashemi, Darren Ong, and Atalay Oz-
govde. Modular ﬁrst-order ontologies via repositories. Applied Ontology, 7(2):169–
209, 2012.
[GK18]
Nikhil Gilbert and C. Maria Keet. Automating question generation and marking
of language learning exercises for isiZulu. In Brian Davis, C. Maria Keet, and
Adam Wyner, editors, 6th International Workshop on Controlled Natural language
(CNL’18), volume 304 of FAIA, pages 31–40. IOS Press, 2018. Co. Kildare, Ireland,
27-28 August 2018.
[GKL+09]
Chiara Ghidini, Barbara Kump, Stefanie Lindstaedt, Nahid Mabhub, Viktoria
Pammer, Marco Rospocher, and Luciano Seraﬁni.
Moki: The enterprise mod-
elling wiki. In Proceedings of the 6th Annual European Semantic Web Conference
(ESWC2009), 2009. Heraklion, Greece, 2009 (demo).
[GKWZ03]
D. Gabbay, A. Kurucz, F. Wolter, and M. Zakharyaschev.
Many-dimensional
modal logics: theory and applications. Studies in Logic. Elsevier, 2003.
[GOG+10]
Alexander Garcia, Kieran O’Neill, Leyla Jael Garcia, Phillip Lord, Robert Stevens,
´Oscar Corcho, and Frank Gibson. Developing ontologies within decentralized set-
tings. In H. Chen et al., editors, Semantic e-Science. Annals of Information Sys-
tems 11, pages 99–139. Springer, 2010.
[GOS09]
Nicola Guarino, Daniel Oberle, and Steﬀen Staab.
What is an ontology?
In
S. Staab and R. Studer, editors, Handbook on Ontologies, chapter 1, pages 1–17.
Springer, 2009.
[GP09]
A. Gangemi and V. Presutti. Ontology design patterns. In S. Staab and R. Studer,
editors, Handbook on Ontologies, pages 221–243. Springer Verlag, 2009.
[GPFLC04]
A. G´omez-P´erez, M. Fern´andez-Lopez, and O. Corcho. Ontological Engineering.
Springer Verlag, 2004.
[GR09]
A. Gatt and E. Reiter. Simplenlg: A realisation engine for practical applications. In
E. Krahmer and M. Theune, editors, Proceedings of the 12th European Workshop
on Natural Language Generation (ENLG’09), page 9093. ACL, 2009. March 30-31,
2009, Athens, Greece.
[Gru93]
T. R. Gruber. A translation approach to portable ontologies. Knowledge Acquisi-
tion, 5(2):199–220, 1993.
[GRV10]
Birte Glimm, Sebastian Rudolph, and Johanna V¨olker. Integrated metamodeling
and diagnosis in OWL 2. In Peter F. Patel-Schneider, Yue Pan, Pascal Hitzler,
Peter Mika, Lei Zhang, JeﬀZ. Pan, Ian Horrocks, and Birte Glimm, editors,
Proceedings of the 9th International Semantic Web Conference, volume 6496 of
LNCS, pages 257–272. Springer, November 2010.
[Gua98]
Nicola Guarino. Formal ontology and information systems. In N. Guarino, editor,
Proceedings of Formal Ontology in Information Systems (FOIS’98), Frontiers in
Artiﬁcial intelligence and Applications, pages 3–15. Amsterdam: IOS Press, 1998.
[Gua09]
Nicola Guarino. The ontological level: Revisiting 30 years of knowledge repre-
sentation. In A.T. Borgida et al., editors, Mylopoulos Festschrift, volume 5600 of
LNCS, pages 52–67. Springer, 2009.

236
Bibliography
[Gui05]
Giancarlo Guizzardi. Ontological Foundations for Structural Conceptual Models.
Phd thesis, University of Twente, The Netherlands. Telematica Instituut Funda-
mental Research Series No. 15, 2005.
[GW00a]
Nicola Guarino and Chris Welty. A formal ontology of properties. In R. Dieng and
O. Corby, editors, Proceedings of 12th International Conference on Knowledge En-
gineering and Knowledge Management (EKAW’00), volume 1937 of LNCS, pages
97–112. Springer Verlag, 2000.
[GW00b]
Nicola Guarino and Chris Welty.
Identity, unity, and individuality: towards a
formal toolkit for ontological analysis. In W. Horn, editor, Proceedings of ECAI’00,
pages 219–223. IOS Press, Amsterdam, 2000.
[GW08]
Giancarlo Guizzardi and Gerd Wagner. What’s in a relationship: An ontological
analysis. In Qing Li, Stefano Spaccapietra, Eric Yu, and Antoni Oliv´e, editors,
ER, volume 5231 of Lecture Notes in Computer Science, pages 83–97. Springer,
2008.
[GW09]
N. Guarino and C. Welty. An overview of ontoclean. In S. Staab and R. Studer,
editors, Handbook on Ontologies, pages 201–220. Springer Verlag, 2009.
[GWG+07]
Carole Goble, Katy Wolstencroft, Antoon Goderis, Duncan Hull, Jun Zhao, Pinar
Alper, Phillip Lord, Chris Wroe, Khalid Belhajjame, Daniele Turi, Robert Stevens,
Tom Oinn, and David De Roure. Knowledge discovery for biology with taverna. In
C.J.O. Baker and H. Cheung, editors, Semantic Web: Revolutionizing knowledge
discovery in the life sciences, pages 355–395. Springer: New York, 2007.
[Hal01]
T.A. Halpin.
Information Modeling and Relational Databases.
San Francisco:
Morgan Kaufmann Publishers, 2001.
[HC11]
Terry A. Halpin and Matthew Curland. Enriched support for ring constraints. In
Robert Meersman, Tharam S. Dillon, and Pilar Herrero, editors, OTM Workshops
2011, volume 7046 of LNCS, pages 309–318. Springer, 2011. Hersonissos, Crete,
Greece, October 17-21, 2011.
[HCTJ93]
J.-L. Hainaut, M. Chandelon, C. Tonneau, and M. Joris. Contribution to a theory
of database reverse engineering.
In Reverse Engineering, 1993., Proceedings of
Working Conference on, pages 161–170, May 1993.
[HDG+11]
Robert Hoehndorf, Michel Dumontier, J H Gennari, Sarah Wimalaratne, Bernard
de Bono, Daniel Cook, and George Gkoutos. Integrating systems biology models
and biomedical ontologies. BMC Systems Biology, 5:124, 2011.
[HDN04]
N. Henze, P. Dolog, and W. Nejdl.
Reasoning and ontologies for personalized
e-learning in the semantic web.
Educational Technology & Society, 7(4):82–97,
2004.
[Hed04]
Shawn Hedman. A ﬁrst course in logic—an introduction to model theory, proof
theory, computability, and complexity. Oxford University Press, Oxford, 2004.
[Hep11]
Martin Hepp.
SKOS to OWL.
Online: http://www.heppnetz.de/projects/
skos2owl/, Last accessed: Aug 30, 2011.
[HH06]
H. Herre and B. Heller.
Semantic foundations of medical information systems
based on top-level ontologies. Knowledge-Based Systems, 19:107–115, 2006.
[Hir14]
Graeme Hirst. Overcoming linguistic barriers to the multilingual semanticweb. In
Buitelaar and Cimiano [BC14], chapter 1, pages 3–14.
[HKS06]
I. Horrocks, O. Kutz, and U. Sattler. The even more irresistible SROIQ. Pro-
ceedings of KR-2006, pages 452–457, 2006.

Bibliography
237
[HND+11]
Melanie Hilario, Phong Nguyen, Huyen Do, Adam Woznica, and Alexandros
Kalous.
Ontology-based meta-mining of knowledge discovery workﬂows.
In
N. Jankowski, W. Duch, and K. Grabczewski, editors, Meta-learning in Com-
putational Intelligence, pages 273–315. Springer, 2011.
[HNOS+08]
Melissa A Haendel, Fabian Neuhaus, David Osumi-Sutherland, Paula M Mabee,
Jos LV Mejino Jr, Chris J Mungall, and Barry Smith.
CARO- The common
anatomy reference ontology. In Anatomy Ontologies for Bioinformatics, volume 6
of Computational Biology, pages 327–349. Springer, 2008.
[HOD+10]
Robert Hoehndorf, Anika Oellrich, Michel Dumontier, Janet Kelso, Dietrich
Rebholz-Schuhmann, and Heinrich Herre.
Relations as patterns: bridging the
gap between OBO and OWL. BMC Bioinformatics, 11(1):441, 2010.
[HP98]
A. H. M. ter Hofstede and H. A. Proper. How to formalize it? formalization prin-
ciples for information systems development methods. Information and Software
Technology, 40(10):519–540, 1998.
[HP04]
J. R. Hobbs and F. Pan. An ontology of time for the semantic web. ACM Trans-
actions on Asian Language Processing (TALIP): Special issue on Temporal Infor-
mation Processing, 3(1):66–85, 2004.
[HPS08]
M. Horridge, B. Parsia, and U. Sattler. Laconic and precise justiﬁcations in OWL.
In Proc. of the 7th International Semantic Web Conference (ISWC 2008), volume
5318 of LNCS. Springer, 2008.
[HPSvH03]
Ian Horrocks, Peter F. Patel-Schneider, and Frank van Harmelen. From SHIQ and
RDF to OWL: The making of a web ontology language. Journal of Web Semantics,
1(1):7, 2003.
[HWZ99]
I. M. Hodgkinson, F. Wolter, and M. Zakharyaschev. Decidable fragments of ﬁrst-
order temporal logics. Annals of pure and applied logic, 106:85–134, 1999.
[IS09]
Antoine Isaac and Ed Summers.
SKOS Simple Knowledge Organization Sys-
tem Primer. W3c standard, World Wide Web Consortium, August 2009 2009.
http://www.w3.org/TR/skos-primer.
[JDM03]
M. Jarrar, J. Demy, and R. Meersman. On using conceptual data modeling for
ontology engineering. Journal on Data Semantics: Special issue on Best papers
from the ER/ODBASE/COOPIS 2002 Conferences, 1(1):185–207, 2003.
[JKD06]
Mustafa Jarrar, C. Maria Keet, and Paolo Dongilli. Multilingual verbalization
of ORM conceptual models and axiomatized ontologies. Starlab technical report,
Vrije Universiteit Brussel, Belgium, February 2006.
[KA08]
C. Maria Keet and Alessandro Artale. Representing and reasoning over a tax-
onomy of part-whole relations. Applied Ontology – Special issue on Ontological
Foundations for Conceptual Modeling, 3(1-2):91–110, 2008.
[KA10]
C. Maria Keet and Alessandro Artale. A basic characterization of relation migra-
tion. In R. Meersman et al., editors, OTM Workshops, 6th International Work-
shop on Fact-Oriented Modeling (ORM’10), volume 6428 of LNCS, pages 484–493.
Springer, 2010. October 27-29, 2010, Hersonissou, Crete, Greece.
[KAGC08]
C. Maria Keet, Ronell Alberts, Aurona Gerber, and Gibson Chimamiwa. Enhanc-
ing web portals with Ontology-Based Data Access: the case study of South Africa’s
Accessibility Portal for people with disabilities. In Catherine Dolbear, Alan Rut-
tenberg, and Uli Sattler, editors, Proceedings of the Fifth OWL: Experiences and
Directions (OWLED 2008), volume 432 of CEUR-WS, 2008. Karlsruhe, Germany,
26-27 October 2008.

238
Bibliography
[Kas05]
Gilles Kassel. Integration of the DOLCE top-level ontology into the OntoSpec
methodology.
Technical Report HAL : hal-00012203/arXiv :
cs.AI/0510050,
Laboratoire de Recherche en Informatique d’Amiens (LaRIA), October 2005.
http://hal.archives-ouvertes.fr/ccsd-00012203.
[Kaz08]
Yevgeny Kazakov. RIQ and SROIQ are harder than SHOIQ. In 11th International
Conference on Principles of Knowledge Representation and Reasoning (KR’08),
pages 274–284, 2008. 16-19 August 2008, Sydney, Australia.
[KB17]
C. Maria Keet and Sonia Berman. Determining the preferred representation of
temporal constraints in conceptual models. In H.C. Mayr et al., editors, 36th In-
ternational Conference on Conceptual Modeling (ER’17), volume 10650 of LNCS,
pages 437–450. Springer, 2017. 6-9 Nov 2017, Valencia, Spain.
[KC16]
C. M. Keet and T. Chirema. A model for verbalising relations with roles in multiple
languages. In E. Blomqvist, P. Ciancarini, F. Poggi, and F. Vitali, editors, Proceed-
ings of the 20th International Conference on Knowledge Engineering and Knowl-
edge Management (EKAW’16), volume 10024 of LNAI, pages 384–399. Springer,
2016. 19-23 November 2016, Bologna, Italy.
[KdKL14]
C. Maria Keet, Claudia d’Amato, Zubeida C. Khan, and Agnieszka Lawrynowicz.
Exploring reasoning with the DMOP ontology. In S. Bail, B. Glimm, E. Jim´enez-
Ruiz, N. Matentzoglu, B. Parsia, and A. Steigmiller, editors, 3rd Workshop on
Ontology Reasoner Evaluation (ORE’14), volume 1207 of CEUR-WS, pages 64–
70. CEUR-WS, 2014. July 13, 2014, Vienna, Austria.
[Kee05]
C. Maria Keet. Factors aﬀecting ontology development in ecology. In B Lud¨ascher
and L. Raschid, editors, Data Integration in the Life Sciences 2005 (DILS2005),
volume 3615 of LNBI, pages 46–62. Springer Verlag, 2005. San Diego, USA, 20-22
July 2005.
[Kee09]
C. Maria Keet. Constraints for representing transforming entities in bio-ontologies.
In R. Serra and R. Cucchiara, editors, 11th Congress of the Italian Association for
Artiﬁcial Intelligence (AI*IA 2009), volume 5883 of LNAI, pages 11–20. Springer
Verlag, 2009. Reggio Emilia, Italy, Dec. 9-12, 2009.
[Kee10a]
C. Maria Keet. Dependencies between ontology design parameters. International
Journal of Metadata, Semantics and Ontologies, 5(4):265–284, 2010.
[Kee10b]
C. Maria Keet. On the feasibility of description logic knowledge bases with rough
concepts and vague instances. In Proceedings of the 23rd International Workshop
on Description Logics (DL’10), CEUR-WS, pages 314–324, 2010. 4-7 May 2010,
Waterloo, Canada.
[Kee10c]
C. Maria Keet.
Ontology engineering with rough concepts and instances.
In
P. Cimiano and H.S. Pinto, editors, 17th International Conference on Knowl-
edge Engineering and Knowledge Management (EKAW’10), volume 6317 of LNCS,
pages 507–517. Springer, 2010. 11-15 October 2010, Lisbon, Portugal.
[Kee11a]
C. Maria Keet. Rough subsumption reasoning with rOWL. In Proceeding of the
SAICSIT Annual Research Conference 2011 (SAICSIT’11), pages 133–140. ACM
Conference Proceedings, 2011. Cape Town, South Africa, October 3-5, 2011.
[Kee11b]
C. Maria Keet. The use of foundational ontologies in ontology development: an
empirical assessment. In G. Antoniou et al., editors, 8th Extended Semantic Web
Conference (ESWC’11), volume 6643 of LNCS, pages 321–335. Springer, 2011.
Heraklion, Crete, Greece, 29 May-2 June, 2011.
[Kee12a]
C. Maria Keet. Detecting and revising ﬂaws in OWL object property expressions.
In A. ten Teije et al., editors, 18th International Conference on Knowledge En-
gineering and Knowledge Management (EKAW’12), volume 7603 of LNAI, pages
252–266. Springer, 2012. Oct 8-12, Galway, Ireland.

Bibliography
239
[Kee12b]
C. Maria Keet. Transforming semi-structured life science diagrams into meaningful
domain ontologies with DiDOn. Journal of Biomedical Informatics, 45:482–494,
2012.
[Kee13]
C. Maria Keet.
Ontology-driven formal conceptual data modeling for biologi-
cal data analysis. In Mourad Elloumi and Albert Y. Zomaya, editors, Biological
Knowledge Discovery Handbook: Preprocessing, Mining and Postprocessing of Bi-
ological Data, chapter 6, pages 129–154. Wiley, 2013.
[Kee14]
C. Maria Keet. Preventing, detecting, and revising ﬂaws in object property ex-
pressions. Journal on Data Semantics, 3(3):189–206, 2014.
[Kee16]
C. M. Keet.
Relating some stuﬀto other stuﬀ.
In E. Blomqvist, P. Ciancar-
ini, F. Poggi, and F. Vitali, editors, Proceedings of the 20th International Confer-
ence on Knowledge Engineering and Knowledge Management (EKAW’16), volume
10024 of LNAI, pages 368–383. Springer, 2016. 19-23 November 2016, Bologna,
Italy.
[Kee17a]
C. M. Keet. A note on the compatibility of part-whole relations with foundational
ontologies. In FOUST-II: 2nd Workshop on Foundational Ontology, Joint Ontology
Workshops 2017, volume 2050 of CEUR-WS, page 10p, 2017. 21-23 September
2017, Bolzano, Italy.
[Kee17b]
C. M. Keet.
Representing and aligning similar relations: parts and wholes in
isizulu vs english. In J. Gracia, F. Bond, J. McCrae, P. Buitelaar, C. Chiarcos,
and S. Hellmann, editors, Language, Data, and Knowledge 2017 (LDK’17), volume
10318 of LNAI, pages 58–73. Springer, 2017. 19-20 June, 2017, Galway, Ireland.
[KFRMG12]
C. Maria Keet, Francis C. Fern´andez-Reyes, and Annette Morales-Gonz´alez. Rep-
resenting mereotopological relations in OWL ontologies with ontoparts.
In
E. Simperl et al., editors, Proceedings of the 9th Extended Semantic Web Con-
ference (ESWC’12), volume 7295 of LNCS, pages 240–254. Springer, 2012. 29-31
May 2012, Heraklion, Crete, Greece.
[KG17]
Megan Katsumi and Michael Gr¨uninger. Choosing ontologies for reuse. Applied
Ontology, 12(3-4):195–221, 2017.
[KHH16]
Nazifa Karima, Karl Hammar, and Pascal Hitzler. How to document ontology de-
sign patterns. In Proceedings of the 7th Workshop on Ontology Patterns (WOP’16),
2016. Kobe, Japan, on 18th October 2016.
[KHS+17]
Evgeny Kharlamov, Dag Hovland, Martin G. Skaeveland, Dimitris Bilidas, Ernesto
Jim´enez-Ruiz, Guohui Xiao, Ahmet Soylu, Davide Lanti, Martin Rezk, Dmitriy
Zheleznyakov, Martin Giese, Hallstein Lie, Yannis Ioannidis, Yannis Kotidis,
Manolis Koubarakis, and Arild Waaler.
Ontology based data access in statoil.
Web Semantics: Science, Services and Agents on the World Wide Web, 44:3–36,
2017.
[KJLW12]
Daniel Kless, Ludger Jansen, Jutta Lindenthal, and Jens Wiebensohn. A method
for re-engineering a thesaurus into an ontology. In M. Donnelly and G. Guizzardi,
editors, Proceedings of the Seventh International Conference on Formal Ontology
in Information Systems, pages 133–146. IOS Press, 2012.
[KK12]
Zubeida Khan and C. Maria Keet. ONSET: Automated foundational ontology se-
lection and explanation. In A. ten Teije et al., editors, 18th International Confer-
ence on Knowledge Engineering and Knowledge Management (EKAW’12), volume
7603 of LNAI, pages 237–251. Springer, 2012. Oct 8-12, Galway, Ireland.
[KK13a]
Z. Khan and C. Maria Keet. Addressing issues in foundational ontology mediation.
In Joaquim Filipe and Jan Dietz, editors, 5th International Conference on Knowl-
edge Engineering and Ontology Development (KEOD’13), pages 5–16. INSTICC,
SCITEPRESS – Science and Technology Publications, 2013. Vilamoura, Portugal,
19-22 September 2013.

240
Bibliography
[KK13b]
Zubeida Khan and C. Maria Keet. The foundational ontology library ROMULUS.
In Alfredo Cuzzocrea and Soﬁan Maabout, editors, Proceedings of the 3rd In-
ternational Conference on Model & Data Engineering (MEDI’13), volume 8216 of
LNCS, pages 200–211. Springer, 2013. September 25-27, 2013, Amantea, Calabria,
Italy.
[KK14]
Zubeida C. Khan and C. Maria Keet. Feasibility of automated foundational ontol-
ogy interchangeability. In K. Janowicz and S. Schlobach, editors, 19th International
Conference on Knowledge Engineering and Knowledge Management (EKAW’14),
volume 8876 of LNAI, pages 225–237. Springer, 2014. 24-28 Nov, 2014, Linkoping,
Sweden.
[KK15a]
Z. C. Khan and C. M. Keet. Foundational ontology mediation in ROMULUS. In
A. Fred et al., editors, Knowledge Discovery, Knowledge Engineering and Knowl-
edge Management: IC3K 2013 Selected Papers, volume 454 of CCIS, pages 132–
152. Springer, 2015.
[KK15b]
Zubeida C. Khan and C. Maria Keet. An empirically-based framework for ontology
modularization. Applied Ontology, 10(3-4):171–195, 2015.
[KK16]
Zubeida C. Khan and C. Maria Keet. ROMULUS: a Repository of Ontologies for
MULtiple USes populated with foundational ontologies. Journal on Data Seman-
tics, 5(1):19–36, 2016.
[KK17a]
C. M. Keet and L. Khumalo. Toward a knowledge-to-text controlled natural lan-
guage of isiZulu. Language Resources and Evaluation, 51(1):131–157, 2017.
[KK17b]
C. Maria Keet and Oliver Kutz. Orchestrating a network of mereo(topo)logical
theories. In Proceedings of the Knowledge Capture Conference (K-CAP’17), K-
CAP 2017, pages 11:1–11:8, New York, NY, USA, 2017. ACM.
[KKG13]
C. Maria Keet, M. Tahir Khan, and Chiara Ghidini. Ontology authoring with
FORZA. In Proceedings of the 22nd ACM international conference on Confer-
ence on Information & Knowledge Management (CIKM’13), pages 569–578. ACM
proceedings, 2013. Oct. 27 - Nov. 1, 2013, San Francisco, USA.
[KKS12]
Yevgeny Kazakov, Markus Kr¨otzsch, and Frantisek Simancik.
ELK reasoner:
Architecture and evaluation. In Ian Horrocks, Mikalai Yatskevich, and Ernesto
Jim´enez-Ruiz, editors, 1st International Workshop on OWL Reasoner Evaluation
(ORE-2012), volume 858 of CEUR Workshop Proceedings. CEUR-WS.org, 2012.
Manchester, UK, July 1st.
[KL16]
C. M. Keet and A. Lawrynowicz.
Test-driven development of ontologies.
In
H. Sack et al., editors, Proceedings of the 13th Extended Semantic Web Conference
(ESWC’16), volume 9678 of LNCS, pages 642–657, Berlin, 2016. Springer. 29 May
- 2 June, 2016, Crete, Greece.
[KLd+15]
C. Maria Keet, Agnieszka Lawrynowicz, Claudia d’Amato, Alexandros Kalousis,
P. Nguyen, Raul Palma, Robert Stevens, and Melani Hilario. The data mining
optimization ontology. Web Semantics: Science, Services and Agents on the World
Wide Web, 32:43–53, 2015.
[KLT+10]
R. Kontchakov, C. Lutz, D. Toman, F. Wolter, and M. Zakharyaschev. The com-
bined approach to query answering in DL-Lite. In Fangzhen Lin, Ulrike Sattler,
and Miroslaw Truszczynski, editors, Principles of Knowledge Representation and
Reasoning: Proceedings of the Twelfth International Conference (KR 2010). AAAI
Press, 2010. Toronto, Ontario, Canada, May 9-13, 2010.
[KML10]
O. Kutz, T. Mossakowski, and D. L¨ucke. Carnap, Goguen, and the Hyperontolo-
gies: Logical Pluralism and Heterogeneous Structuring in Ontology Design. Logica
Universalis, 4(2), 2010. Special issue on ‘Is Logic Universal?’.

Bibliography
241
[KSFPV13]
C. Maria Keet, Mari Carmen Su´arez-Figueroa, and Maria Poveda-Villal´on. The
current landscape of pitfalls in ontologies. In Joaquim Filipe and Jan Dietz, ed-
itors, 5th International Conference on Knowledge Engineering and Ontology De-
velopment (KEOD’13), pages 132–139. INSTICC, SCITEPRESS – Science and
Technology Publications, 2013. Vilamoura, Portugal, 19-22 September 2013.
[KSFPV15]
C. M. Keet, M. C. Su´arez-Figueroa, and M. Poveda-Villal´on. Pitfalls in ontologies
and tips to prevent them. In A. Fred, J. L. G. Dietz, K. Liu, and J. Filipe, editors,
Knowledge Discovery, Knowledge Engineering and Knowledge Management: IC3K
2013 Selected papers, volume 454 of CCIS, pages 115–131. Springer, Berlin, 2015.
[KSH12]
Markus Kr¨otzsch, Frantiˇsek Simanˇc´ık, and Ian Horrocks.
A description logic
primer. Technical Report 1201.4089v1, Department of Computer Science, Uni-
versity of Oxford, UK, 2012. arXiv:cs.LO/12014089v1.
[LAF14]
Pilar Le´on-Ara´uz and Pamela Faber. Context and terminology in the multilingual
semantic web. In Paul Buitelaar and Philip Cimiano, editors, Towards the Mul-
tilingual Semantic Web: Principles, Methods and Applications, chapter 3, pages
31–47. Springer, 2014.
[Lam17]
Jean-Baptiste Lamy. Owlready: Ontology-oriented programming in python with
automaticclassiﬁcation and high level constructs for biomedical ontologies. Artiﬁ-
cial Intelligence in Medicine, 2017.
[Leo08]
Joop Leo. Modeling relations. Journal of Philosophical Logic, 37:353–385, 2008.
[LHC11]
Kaihong Liu, William R. Hogan, and Rebecca S. Crowley. Natural language pro-
cessing methods and systems for biomedical ontology learning. Journal of Biomed-
ical Informatics, 44(1):163–179, 2011.
[LLNW11]
Thorsten Liebig, Marko Luther, Olaf Noppens, and Michael Wessel. OWLlink.
Semantic Web Journal, 2(1):23–32, 2011.
[Loe15]
Frank Loebe.
Ontological Semantics: An Attempt at Foundations of Ontology
Representation. Phd thesis, Fakult¨at f¨ur Mathematik und Informatik, Universit¨at
Leipzig, 2015.
[LS08]
Thomas Lukasiewicz and Umberto Straccia. Managing uncertainty and vagueness
in description logics for the semantic web. Journal of Web Semantics, 6(4):291–
308, 2008.
[LT09]
Lina Lubyte and Sergio Tessaris. Automated extraction of ontologies wrapping
relational data sources. In Proceedings of International Conference on Database
and Expert Systems Applications (DEXA’09), pages 128–142. Springer, 2009.
[LTW09]
C. Lutz, D. Toman, and F. Wolter. Conjunctive query answering in the description
logic EL using a relational database system. In Proceedings of the 21st International
Joint Conference on Artiﬁcial Intelligence IJCAI’09. AAAI Press, 2009.
[Luk17]
Thomas Lukasiewicz. Uncertainty reasoning for the semantic web. In G. Ianni
et al., editors, Reasoning Web 2017, volume 10370 of LNCS, pages 276–291.
Springer, 2017.
[LWZ08]
Carsten Lutz, Frank Wolter, and Michael Zakharyaschev. Temporal description
logics: A survey. In Proc. of the Fifteenth International Symposium on Temporal
Representation and Reasoning (TIME’08). IEEE Computer Society Press, 2008.
[MAdCB+12] John McCrae, Guadalupe Aguado-de Cea, Paul Buitelaar, Philipp Cimiano,
Thierry Declerck, Asunci´on G´omez-P´erez, Jorge Gracia, Laura Hollink, Elena
Montiel-Ponsoda, Dennis Spohr, and Tobias Wunner.
Interchanging lexical re-
sources on the semantic web. Language Resources and Evaluation, 46(4):701–719,
2012.

242
Bibliography
[MB09]
Alistar Miles and Sean Bechhofer. SKOS Simple Knowledge Organization System
Reference. W3c recommendation, World Wide Web Consortium (W3C), 18 August
2009.
[MBG+03]
C. Masolo, S. Borgo, A. Gangemi, N. Guarino, and A. Oltramari.
On-
tology library.
WonderWeb Deliverable D18 (ver. 1.0, 31-12-2003)., 2003.
http://wonderweb.semanticweb.org.
[MBSJ08]
Joshua S. Madin, Shawn Bowers, Mark P. Schildhauer, and Matthew B. Jones.
Advancing ecological research with ontologies.
Trends in Ecology & Evolution,
23(3):159–168, 2008.
[McC10]
Dave McComb. Gist: The minimalist upper ontology (abstract). Semantic Tech-
nology Conference, 2010. 21-25 June 2010, San Francisco, USA.
[McD17]
M. H. McDaniel. An Automated System for the Assessment and Ranking of Do-
main Ontologies. PhD thesis, Department of Computer Science, 2017.
[MCNK15]
T. Mossakowski, M. Codescu, F. Neuhaus, and O. Kutz. The Road to Univer-
sal Logic–Festschrift for 50th birthday of Jean-Yves Beziau, Volume II, chapter
The distributed ontology, modelling and speciﬁcation language - DOL. Studies in
Universal Logic. Birkh¨auser, 2015.
[MdCB+12]
John McCrae, Guadalupe Aguado de Cea, Paul Buitelaar, Philipp Cimiano,
Thierry Declerck, Asunci´on G´omez-P´erez, Jorge Gracia, Laura Hollink, Elena
Montiel-Ponsoda, Dennis Spohr, and Tobias Wunner. The lemon cookbook. Tech-
nical report, Monnet Project, June 2012. www.lemon-model.net.
[Mer10a]
G. H. Merrill. Ontological realism: Methodology or misdirection? Applied Ontol-
ogy, 5(2):79108, 2010.
[Mer10b]
Gary H. Merrill. Realism and reference ontologies: Considerations, reﬂections and
problems. Applied Ontology, 5(3):189–221, 2010.
[MGH+09]
Boris Motik, Bernardo Cuenca Grau, Ian Horrocks, Zhe Wu, Achille Fokoue, and
Carsten Lutz. OWL 2 Web Ontology Language Proﬁles. W3C recommendation,
W3C, 27 Oct. 2009. http://www.w3.org/TR/owl2-profiles/.
[Miz10]
R. Mizoguchi. YAMATO: Yet Another More Advanced Top-level Ontology. In
Proceedings of the Sixth Australasian Ontology Workshop, Conferences in Research
and Practice in Information, pages 1–16. CRPIT, 2010. Sydney : ACS.
[MK19]
Z. Mahlaza and C. M. Keet. A classiﬁcation of grammar-infused templates for
ontology and model verbalisation. In E. Garoufallou et al., editors, 13th Metadata
and Semantics Research Conference (MTSR’19), volume 1057 of CCIS, pages 64–
76. Springer, 2019. 28-31 Oct 2019, Rome, Italy.
[MPSP09]
Boris Motik, Peter F. Patel-Schneider, and Bijan Parsia. OWL 2 web ontology lan-
guage structural speciﬁcation and functional-style syntax. W3c recommendation,
W3C, 27 Oct. 2009. http://www.w3.org/TR/owl2-syntax/.
[MR05]
M. C. MacLeod and E. M. Rubenstein. Universals. In The Internet Encyclopedia
of Philosophy. 2005. http://www.iep.utm.edu/u/universa.htm.
[MSKK07]
R. Mizoguchi, E. Sunagawa, K. Kozaki, and Y. Kitamura. A model of roles within
an ontology development tool: Hozo. Applied Ontology, 2(2):159–179, 2007.
[N+13]
Fabian Neuhaus et al. Towards ontology evaluation across the life cycle. Applied
Ontology, 8(3):179–194, 2013.
[Neu17]
Fabian Neuhaus. On the deﬁnition of ontology. In FOUST-II: 2nd Workshop on
Foundational Ontology, Joint Ontology Workshops 2017, volume 2050 of CEUR-
WS, page 10p, 2017. 21-23 September 2017, Bolzano, Italy.

Bibliography
243
[NM01]
N.F. Noy and D.L. McGuinness. Ontology development 101: A guide to creating
your ﬁrst ontology. Technical Report KSL-01-05, and Stanford Medical Informatics
Technical Report SMI-2001-0880, Stanford Knowledge Systems Laboratory, March
2001.
[NP01]
I. Niles and A. Pease. Towards a standard upper ontology. In Chris Welty and
Barry Smith, editors, Proceedings of the 2nd International Conference on Formal
Ontology in Information Systems (FOIS-2001), 2001. Ogunquit, Maine, October
17-19, 2001.
[OHWM10]
Martin J. O’Connor, Christian Halaschek-Wiener, and Mark A. Musen. Mapping
master: A ﬂexible approach for mapping spreadsheets to OWL. In P. F. Patel-
Schneider et al., editors, Proceedings of the International Semantic Web Conference
2010 (ISWC’10), volume 6497 of LNCS, pages 194–208, Berlin, 2010. Springer.
[Ope]
OpenMRS. https://www.transifex.com/openmrs/OpenMRS/.
[PD+09]
V. Presutti, E Daga, et al. extreme design with content ontology design patterns.
In Proc. of WS on OP’09, volume 516 of CEUR-WS, pages 83–97, 2009.
[Per17]
Silvio Peroni. A simpliﬁed agile methodology for ontology development. In Dragoni
M., Poveda-Villal´on M., and Jimenez-Ruiz E., editors, OWLED 2016, ORE 2016:
OWL: Experiences and Directions Reasoner Evaluation, volume 10161 of LNCS,
pages 55–69. Springer, 2017.
[Por10]
F.
Portoraro.
Automated
reasoning.
In
E.
Zalta,
editor,
Stanford
Encyclopedia of Philosophy. 2010.
http://plato.stanford.edu/entries/
reasoning-automated/.
[PS07]
Zdzislaw Pawlak and Andrzej Skowron. Rudiments of rough sets. Information
Sciences, 177(1):3–27, 2007.
[PS15]
Adrian Paschke and Ralph Schaefermeier. Aspect OntoMaven - aspect-oriented
ontology development and conﬁguration with OntoMaven.
Technical Report
1507.00212v1, Institute of Computer Science, Free University of Berlin, July 2015.
[PT11]
Rafael Pe˜naloza and Anni-Yasmin Turhan. A practical approach for computing
generalization inferences in EL. In G. Antoniou et al., editors, 8th Extended Seman-
tic Web Conference (ESWC’11), volume 6643 of LNCS, pages 410–423. Springer,
2011. Heraklion, Crete, Greece, 29 May-2 June, 2011.
[PVSFGP12]
Mar´ıa Poveda-Villal´on, Mari Carmen Su´arez-Figueroa, and Asunci´on G´omez-
P´erez.
Validating ontologies with OOPS!
In A. ten Teije et al., editors, 18th
International Conference on Knowledge Engineering and Knowledge Management
(EKAW’12), volume 7603 of LNAI, pages 267–281, Germany, 2012. Springer. Oct
8-12, Galway, Ireland.
[RCB+07]
Alan Ruttenberg, Tim Clark, William Bug, Matthias Samwald, Olivier Bodenrei-
der, Helen Chen, Donald Doherty, Kerstin Forsberg, Yong Gao, Vipul Kashyap,
June Kinoshita, Joanne Luciano, M Scott Marshall, Chimezie Ogbuji, Jonathan
Rees, Susie Stephens, Gwendolyn T Wong, Elizabeth Wu, Davide Zaccagnini,
Tonya Hongsermeier, Eric Neumann, Ivan Herman, and Kei-Hoi Cheung. Advanc-
ing translational research with the semantic web. BMC Bioinformatics, 8(Suppl
3):S2, 2007.
[RCC92]
D. A. Randell, Z. Cui, and A. G. Cohn. A spatial logic based on regions and
connection. In Proc. 3rd Int. Conf. on Knowledge Representation and Reasoning,
pages 165–176. Morgan Kaufmann, 1992.
[RCVB09]
C. Roussey, O. Corcho, and L. Vilches-Bl´azquez. A catalogue of OWL ontology
antipatterns. In Proc. of K-CAP’09, pages 205–206, 2009.

244
Bibliography
[RD97]
E. Reiter and R. Dale.
Building applied natural language generation systems.
Natural Language Engineering, 3:57–87, 1997.
[RDH+04]
AL Rector, N Drummond, M Horridge, L Rogers, H Knublauch, R Stevens,
H Wang, and C. Wroe, Csallner. OWL pizzas: Practical experience of teaching
OWL-DL: Common errors & common patterns. In Proceedings of the 14th Interna-
tional Conference Knowledge Acquisition, Modeling and Management (EKAW’04),
volume 3257 of LNCS, pages 63–81, Whittlebury Hall, UK, 2004. Springer.
[RKH08a]
Sebastian Rudolph, Markus Kr¨otzsch, and Pascal Hitzler. All elephants are bigger
than all mice. In Franz Baader, Carsten Lutz, and Boris Motik, editors, Proc. 21st
Int. Workshop on Description Logics (DL08), volume 353 of CEUR-WS, 2008.
Dresden, Germany, May 1316, 2008.
[RKH08b]
Sebastian Rudolph, Markus Kr¨otzsch, and Pascal Hitzler. Cheap boolean role con-
structors for description logics. In Steﬀen H¨olldobler, Carsten Lutz, and Heinrich
Wansing, editors, Proc. 11th European Conf. on Logics in Artiﬁcial Intelligence
(JELIA08), volume 5293 of LNAI, page 362374. Springer, 2008.
[RMC12]
Mariano Rodr´ıguez-Muro and Diego Calvanese. Quest, an OWL 2 QL reasoner for
ontology-based data access. In P. Klinov and M. Horridge, editors, 7th Workshop
on OWL: Experiences and Directions (OWLED’12), volume 849 of CEUR-WS,
2012. 27-28 May, Heraklion, Crete, Greece.
[RMJ03]
C. Rosse and J. L. V. Mejino Jr. A reference ontology for biomedical informatics:
the foundational model of anatomy. J. of Biomedical Informatics, 36(6):478–500,
2003.
[RMLC08]
Mariano Rodriguez-Muro, Lina Lubyte, and Diego Calvanese. Realizing Ontology
Based Data Access: A plug-in for Prot´eg´e.
In Proc. of the Workshop on In-
formation Integration Methods, Architectures, and Systems (IIMAS 2008). IEEE
Computer Society, 2008.
[RRZvdH03]
Alan L. Rector, Jeremy Rogers, Pieter E. Zanstra, and Egbert J. van der Haring.
OpenGALEN: Open source medical terminology and tools. In American Medical
Informatics Association Annual Symposium (AMIA’03). AMIA, 2003. Washing-
ton, DC, USA, November 8-12.
[SAR+07]
B. Smith, M. Ashburner, C. Rosse, J. Bard, W. Bug, W. Ceusters, L.J. Goldberg,
K. Eilbeck, A. Ireland, C.J. Mungall, The OBI Consortium, N. Leontis, A.B.
Rocca-Serra, A. Ruttenberg, S-A. Sansone, M. Shah, P.L. Whetzel, and S. Lewis.
The OBO Foundry: Coordinated evolution of ontologies to support biomedical
data integration. Nature Biotechnology, 25(11):1251–1255, 2007.
[Sat07]
Ulrike Sattler. Reasoning in description logics: Basics, extensions, and relatives.
In G. Antoniou et al., editors, Reasoning Web 2007, volume 4636 of LNCS, page
154182. Springer, 2007.
[SB13]
Stefan Schulz and Martin Boeker. BioTopLite: An upper level ontology for the
life sciences. evolution, design and application. In Informatik 2013, 43. Jahresta-
gung der Gesellschaft f¨ur Informatik e.V. (GI), Informatik angepasst an Mensch,
Organisation und Umwelt, volume 220 of LNI, pages 1889–1899. GI, 2013. 16-20.
September 2013, Koblenz.
[SBF98]
R. Studer, R. Benjamins, and D. Fensel. Knowledge engineering: Principles and
methods. Data & Knowledge Engineering, 25(1-2):161198, 1998.
[SC10]
Barry Smith and Werner Ceusters. Ontological realism: A methodology for coor-
dinated evolution of scientiﬁc ontologies. Applied Ontology, 5(3):139–188, 2010.
[SCK+05]
B. Smith, W. Ceusters, B. Klagges, J. K¨ohler, A. Kumar, J. Lomax, C. Mungall,
F. Neuhaus, A. L. Rector, and C. Rosse.
Relations in biomedical ontologies.
Genome Biology, 6:R46, 2005.

Bibliography
245
[SD17]
Hazem Safwat and Brian Davis. CNLs for the semantic web: a state of the art.
Language Resources & Evaluation, 51(1):191–220, 2017.
[SFdCB+08]
Mari Carmen Suarez-Figueroa, Guadalupe Aguado de Cea, Carlos Buil, Klaas
Dellschaft, Mariano Fernandez-Lopez, Andres Garcia, Asuncion G´omez-P´erez,
German Herrero, Elena Montiel-Ponsoda, Marta Sabou, Boris Villazon-Terrazas,
and Zheng Yufei. NeOn methodology for building contextualized ontology net-
works. NeOn Deliverable D5.4.1, NeOn Project, 2008.
[SGJR+17]
Ahmet Soylu, Martin Giese, Ernesto Jimenez-Ruiz, Evgeny Kharlamov, Dmitriy
Zheleznyakov, and Ian Horrocks. Ontology-based end-user visual query formula-
tion: Why, what, who, how, and which?
Universal Access in the Information
Society, 16(2):435–467, Jun 2017.
[SKC+08]
R. Schwitter, K. Kaljurand, A. Cregan, C. Dolbear, and G. Hart. A comparison
of three controlled natural languages for OWL 1.1. In Proc. of OWLED 2008 DC,
2008. Washington, DC, USA metropolitan area, on 1-2 April 2008.
[SKZ+18]
A. Soylu, E. Kharlamov, D. Zheleznyakov, E. Jimenez Ruiz, M. Giese, M.G. Sk-
jaeveland, D. Hovland, R. Schlatte, S. Brandt, H. Lie, and I. Horrocks. Optiquevqs:
a visual query system over ontologies for industry. Semantic Web, 9(5):627–660,
2018.
[SLL+04]
Dagobert Soergel, Boris Lauser, Anita Liang, Frehiwot Fisseha, Johannes Keizer,
and Stephen Katz. Reengineering thesauri for new applications: the AGROVOC
example. Journal of Digital Information, 4(4), 2004.
[SMB10]
Elena Simperl, Malgorzata Mochol, and Tobias B¨urger. Achieving maturity: the
state of practice in ontology engineering in 2009. International Journal of Com-
puter Science and Applications, 7(1):45–65, 2010.
[Smi04]
B. Smith. Beyond concepts, or: Ontology as reality representation. In A. Varzi
and L. Vieu, editors, Formal Ontology and Information Systems. Proceedings of the
Third International Conference (FOIS’04), pages 73–84. Amsterdam: IOS Press,
2004.
[SNO12]
SNOMED CT, last accessed: 27-1-2012. http://www.ihtsdo.org/snomed-ct/.
[Sol05]
D. Solow. How to read and do proofs: An introduction to mathematical thought
processes. John Wiley & Sons, Hoboken NJ, USA., 4th edition, 2005.
[SS06]
Vijayan Sugumaran and Veda C. Storey. The role of domain ontologies in database
design: An ontology management and conceptual modeling environment. ACM
Transactions on Database Systems, 31(3):1064–1094, 2006.
[SS09]
Markus Stocker and Evren Sirin. Pelletspatial: A hybrid RCC-8 and RDF/OWL
reasoning and query engine. In Rinke Hoekstra and Pieter Patel-Schneider, editors,
Proceedings of the 6th International Workshop OWL: Experiences and Directions
(OWLED’09), volume 529 of CEUR-WS, 2009. Chantilly, Virginia, USA, 23-24
October 2009.
[SSBS09]
Stefan Schulz, Holger Stenzhorn, Martin Boekers, and Barry Smith. Strengths
and limitations of formal ontologies in the biomedical domain. Electronic Jour-
nal of Communication, Information and Innovation in Health (Special Issue on
Ontologies, Semantic Web and Health), 3(1):31–45, 2009.
[SSRG+12]
S. Schulz, D. Seddig-Rauﬁe, N. Grewe, J. R¨ohl, D. Schober, M. Boeker, and
L. Jansen.
Guideline on developing good ontologies in the biomedical domain
with description logics. Technocal report, December 2012. v1.0.
[SSSS01]
S. Staab, H.P. Schnurr, R. Studer, and Y. Sure. Knowledge processes and ontolo-
gies. IEEE Intelligent Systems, 16(1):26–34, 2001.

246
Bibliography
[STK16]
L. Sanby, I. Todd, and C. M. Keet. Comparing the template-based approach to
gf: the case of afrikaans. In 2nd International Workshop on Natural Language
Generation and the Semantic Web (WebNLG’16), page (in print). ACL, 2016.
September 6, 2016, Edinburgh, Scotland.
[Str08]
Umberto Straccia. Managing uncertainty and vagueness in description logics, logic
programs and description logic programs. In Reasoning Web, 4th International
Summer School, 2008.
[THU+16]
Niket Tandon, Charles Hariman, Jacopo Urbani, Anna Rohrbach, Marcus
Rohrbach, and Gerhard Weikum. Commonsense in parts: Mining part-whole re-
lations from the web and image tags. In Proceedings of the Thirtieth AAAI Con-
ference on Artiﬁcial Intelligence (AAAI’16), pages 243–250. AAAI Press, 2016.
[Tob01]
S. Tobies. Complexity Results and Practical Algorithms for Logics in Knowledge
Representation. PhD thesis, RWTH Aachen, 2001.
[Tur08]
Anni-Yasmin Turhan. On the Computation of Common Subsumers in Description
Logics. PhD thesis, TU Dresden, Institute for Theoretical Computer Science, 2008,
2008.
[Tur10]
Anni-Yasmin Turhan. Reasoning and explanation in EL and in expressive Descrip-
tion Logics. In U. Assmann, A. Bartho, and C. Wende, editors, Reasoning Web
2010, volume 6325 of LNCS, pages 1–27. Springer, 2010.
[TW11]
David Toman and Grant E. Weddell. Fundamentals of Physical Design and Query
Compilation. Synthesis Lectures on Data Management. Morgan & Claypool Pub-
lishers, 2011.
[Var04]
A. C. Varzi. Mereology. In E. N. Zalta, editor, Stanford Encyclopedia of Philos-
ophy. Stanford, fall 2004 edition, 2004. http://plato.stanford.edu/archives/
fall2004/entries/mereology/.
[Var07]
A.C. Varzi. Handbook of Spatial Logics, chapter Spatial reasoning and ontology:
parts, wholes, and locations, pages 945–1038. Berlin Heidelberg: Springer Verlag,
2007.
[Var12]
Achille C. Varzi. On doing ontology without metaphysics. Philosophical Perspec-
tives, 25(1):407–423, 2012.
[VF09]
K. Vila and A. Ferr´andez. Developing an ontology for improving question answer-
ing in the agricultural domain. In F. Sartori, M.´A. Sicilia, and N. Manouselis,
editors, 3rd International Conference on Metadata and Semantics (MTSR’09),
volume 46 of CCIS, pages 245–256. Springer, 2009. Oct 1-2 2009 Milan, Italy.
[vHLP08]
Frank van Harmelen, Vladimir Lifschitz, and Bruce Porter, editors. Handbook of
Knowledge Representation. Elsevier, 2008.
[VKC+16]
Charles F. Vardeman, Adila A. Krisnadhi, Michelle Cheatham, Krzysztof Janow-
icz, Holly Ferguson, Pascal Hitzler, and Aimee P. C. Buccellato.
An ontology
design pattern and its use case for modeling material transformation. Semantic
Web Journal, 8(5):719–731, 2016.
[Vra09]
Danny Vrandeˇci´c. Ontology evaluation. In S. Staab and R. Studer, editors, Hand-
book on Ontologies, pages 293–313. Springer, 2nd edition, 2009.
[Wel06]
Chris Welty. Ontowlclean: cleaning OWL ontologies with OWL. In B. Bennet and
C. Fellbaum, editors, Proceedings of Formal Ontologies in Information Systems
(FOIS’06), pages 347–359. IOS Press, 2006.
[WHF+13]
Katherine Wolstencroft, Robert Haines, Donal Fellows, Alan Williams, David
Withers, Stuart Owen, Stian Soiland-Reyes, Ian Dunlop, Aleksandra Nenadic,

Bibliography
247
Paul Fisher, Jiten Bhagat, Khalid Belhajjame, Finn Bacall, Alex Hardisty, Abra-
ham Nieva de la Hidalga, Maria P. Balcazar Vargas, Shoaib Suﬁ, , and Carole
Goble. The taverna workﬂow suite: designing and executing workﬂows of web ser-
vices on the desktop, web or in the cloud. Nucleic Acids Research, 41(W1):W557–
W561, 2013.
[WKB07]
R. Witte, T. Kappler, and C.J.O. Baker.
Ontology design for biomedical text
mining. In C.J.O. Baker and H. Cheung, editors, Semantic Web: revolutionizing
knowledge discovery in the life sciences, pages 281–313. Springer, 2007.
[WNS+11]
Patricia L. Whetzel, Natalya Fridman Noy, Nigam H. Shah, Paul R. Alexander,
Csongor Nyulas, Tania Tudorache, and Mark A. Musen.
BioPortal: enhanced
functionality via new web services from the national center for biomedical ontology
to access and use ontologies in software applications.
Nucleic Acids Research,
39(Web-Server-Issue), 2011.
[WSH07]
K. Wolstencroft, R. Stevens, and V. Haarslev. Applying OWL reasoning to genomic
data. In C.J.O. Baker and H. Cheung, editors, Semantic Web: revolutionizing
knowledge discovery in the life sciences, pages 225–248. Springer: New York, 2007.
[ZBG06]
S. Zhang, O. Bodenreider, and C. Golbreich. Experience in reasoning with the
Foundational Model of Anatomy in OWL DL. In R. B. Altman, A. K. Dunker,
L. Hunter, T. A. Murray, and T. E. Klein, editors, Paciﬁc Symposium on Biocom-
puting (PSB’06), pages 200–211. World Scientiﬁc, 2006.
[ZYS+05]
Y. Zhou, Jx A. Young, A. Santrosyan, K. Chen, Sx F. Yan, and Ex A. Winzeler. In
silico gene function prediction using ontology-based pattern identiﬁcation. Bioin-
formatics, 21(7):1237–1245, 2005.
Books about ontologies
Here used to be a section on diﬀerent textbooks and handbooks. Meanwhile, this
has been updated and moved to an online resource. It currently resides at the
wikis of the Technical Committee on Education of the International Association for
Ontology and its Applications, at http://iaoaedu.cs.uct.ac.za/, menu option
“books” (this may have moved to some place on http://www.iaoa.org/ by the
time you read this).
Selection of journals that publish papers about ontologies
Research in ontologies
• Applied Ontology
• Journal of Web Semantics
• Semantic Web Journal
• Journal on Data Semantics
• International Journal of Metadata, Semantics and Ontologies
• Artiﬁcial Intelligence Journal
• Journal of Automated Reasoning
Ontologies and applications
• Journal of Biomedical Semantics
• Journal of Biomedical Informatics

248
Bibliography
• BMC Bioinformatics
• Data & Knowledge Engineering
Selection of conferences that publish papers about ontologies
Research in Ontology, ontologies, ontology engineering, ontology lan-
guages, and automated reasoning
• Formal Ontology in Information Systems (FOIS)
• International Conference on Knowledge Engineering and Knowledge Manage-
ment (EKAW)
• International Conference on Knowledge Capture (K-CAP)
• Extended Semantic Web Conference (ESWC)
• International Conference on Knowledge Representation and Reasoning (KR)
Ontologies and applications
• International Conference on Biomedical Ontology (ICBO)
• Semantic Web Applications and Tools for the Life Sciences (SWAT4LS)
• International Semantic Web Conference (ISWC)

APPENDIX A
Tutorials
This appendix includes two tutorials from, one could say, two ‘extremes’ on the
spectrum on ontology engineering. The ﬁrst one is heavily inﬂuenced by notions
from philosophy to inform modelling, whereas the second one is driven by practical
considerations to build a scalable system.
A.1
OntoClean in OWL with a DL reasoner
This tutorial is written by Zola Mahlaza and C. Maria Keet and is based
on a mini-project by Todii Mashoko, Siseko Neti, and Banele Matsebula.
The aim of this tutorial is to illustrate how the OntoClean methodology can be
used in Prot´eg´e with OWL and its reasoner, based on the OntOWLClean approach
proposed in [Wel06]. In particular, it focuses on the following tasks:
• Punning an ontology in preparation for OntoClean.
• Assigning meta-properties to classes in OWL.
• Discovering inconsistencies in a taxonomy, and hints for ﬁxing the hierarchy.
The rest of the document is structured such that Section A.1 presents an
overview of OntoClean and Section A.1 presents our example ontology, the manner
in which OntoClean can be followed within Prot´eg´e, and a limited number of errors
that should be discovered from the provided ontology.
OntoClean
OntoClean is a philosophy-based methodology for validating the correctness and
consistency of an ontology’s taxonomy. It is based on general notions that are
drawn from philosophy, which are Rigidity, Identity, Unity, and Dependence. The
249

250
Appendix A. Tutorials
methodology is made up of two phases. The ﬁrst phase involves annotating all the
classes within an ontology with labels of the meta-properties referring to the four
philosophical notions. The second phase deals with the checking of subsumption
relationships of the ontology based on the predeﬁned OntoClean constraints, which
in this document are also referred to as rules.
In the remainder of this section, we provide a brief recap to the four philo-
sophical concepts, the OntoClean meta-property annotation symbols, and the con-
straints for each of them.
A larger summary is described in Section 5.2.2, an
overview of OntoClean is described in the handbook on ontologies [GW09], which
build upon foundations presented in [GW00a, GW00b].
Rigidity refers to an entity’s property that are essential to that entity (i.e. it
must be true of it in every possible situation). For instance, the property of having
walls is essential to a house. Every house must have walls in every possible situation.
In the event that they are demolished then you no longer have a house. Identity
refers to the capability to identify individual entities in the world as being the
same or diﬀerent. Unity refers to the ability to describe the parts and boundaries
of objects, and thus to know which parts constitute an object, which parts do not,
and under what circumstances is the object a whole. Dependence is the relationship
between entities whereby one will exist solely on the existence of the other.
All the entities within an ontology must be assigned with meta-properties and
labelled with the letter denoting the meta-property; more precisely: (I) for Identity,
(U) for unity, (D) for dependence and (R) for Rigidity.
Each of these labels
preceded with a +, −, or ∼symbol, where (+) means the entity is what the letter
denotes, (−) means the entity is not what the letter denotes, and (∼) means ‘anti’
(may or may not be) to what the letter denotes.
Recall from Exercise 5.4 that the assignment of meta-properties is useful because
there are constraints that the taxonomy must not violate, and the rules that apply.
For instance, when we have two properties x and z, where z subsumes x then we
know that if z is anti-rigid (∼R) then x must be anti-rigid (Rigidity constraint), if
z carries an identity criterion (+I) then x must carry the same criterion (Identity
constraint1), if z carries a unity criterion (+U) then x must carry the same criterion
and if z has anti-unity, then x must also have anti-unity (Unity constraints), and
if z is dependent (+D) on a certain property y then x is dependent on property y
(Dependence constraint).
OntoClean in Prot´eg´e
In this section we will specify where to download the required software, describe the
ontology we will use to illustrate OntoClean, introduce how to pun the ontology in
Prot´eg´e in preparation for OntoClean, and present an exercise on assigning meta-
properties in the tutorial ontology.
What to download
You will need the following material for this tutorial:
1This is not true in the case of the “own” identity criteria

A.1.
OntoClean in OWL with a DL reasoner
251
Figure A.1: Tutorial ontology with ontological inconsistencies.
• Prot´eg´e 5.x, which can be downloaded from https://protege.stanford.
edu/.
• OntoClean and AmountOfMatter tutorial ontologies:
– ontoclean-dl.owl (OntoClean ontology)2
– OntocleanTutorialOntology.owl (AmountOfMatter ontology, which
is the domain ontology that is to be ‘cleaned up’)
– OntocleanTutorialOntologyPunned.owl (Punned AmountOfMatter on-
tology)
– OntocleanTutorialOntologyPunnedMetaProperties.owl
(Punned
AmountOfMatter ontology with assigned meta-properties)
which can be downloaded from the textbook’s resources page at https://
people.cs.uct.ac.za/~mkeet/OEbook/ontologies/
Please note that another version of Prot´eg´e can be used, however, the interface
may use diﬀerent terms and have a diﬀerent layout and look-and-feel.
AmountOfMatter taxonomy
The AmountOfMatter tutorial ontology (see ﬁle OntocleanTutorialOntology.owl)
that will be used to illustrate how to follow OntoClean is shown in Figure A.1. The
tutorial ontology has deliberate taxonomic issues that need to be resolved using
OntoClean. In the ontology, AmountOfMatter can be considered as any entity and
may be a Non-Living or Living object. An example of Non-Living object, accord-
ing to the ontology, is a Ball. Familiarise yourself with the ontology in Prot´eg´e by
doing Task 1.
2The OWL-DL ontology developed by [Wel06] is no longer available through the OntoClean
website at http://www.ontoclean.org/, therefore we provide a cached version.

252
Appendix A. Tutorials
Task 1
Open the tutorial ontology OntocleanTutorialOntology.owl in Prot´eg´e, import the On-
toClean OWL-DL ontology ontoclean-dl.owl, and explore.
Punning the tutorial ontology
The ﬁrst task is to push the ontology’s Tbox into the ABox [Wel06].
In par-
ticular, you must create an Individual with the same name as the class (bears
the same IRI), for each of the classes on the tutorial ontology. These Individuals
must be of type ontoclean:Class. An example is shown in Figure A.2 where
there are 21 Individuals of type ontoclean:Class. For each individual, you must
then specify its “subclasses” through ontoclean:hasSubClass relation. For in-
stance, AmountOfMatter has the subclasses Non-Living and Living as shown in
Figure A.2. Familiarise yourself with how to do this by conduction Task 2. You
can verify whether you’ve done this task correctly by cross-checking with the pro-
vided OntocleanTutorialOntologyPunned.owl ﬁle.
Task 2
1. Continue from Task 1.
2. For each class in the ontology, create its corresponding Individual with the same name
as the class.
3. Make each newly created individual an instance of ontoclean:Class.
4. Specify the “subclasses” for each Individual, using the ontoclean:hasSubClass object
property.
Assigning the meta-properties
Once the ontology has been punned, you can assign meta-properties to each individ-
ual. This requires you to ﬁrst decide on the meta-properties for each class/OWLﬁle-
individual. Once that is decided, you can use Prot´eg´e to assign meta-properties to
each individual by setting its type to the appropriate subclasses of ontoclean:Class.
For instance, you could deem that Paper is dependent (+D) by assigning it the
Dependent meta-property, which practically amounts to adding the assertion that
Paper is an instance of ontoclean:DependentClass as shown in Figure A.3. Carry
out Task 3.
We provide a sample of meta-property assignments for all terms in this tutorial
as shown in Figure A.4. The ﬁle OntocleanTutorialOntologyPunnedMetaProperties.owl
contains all the assignments per individual. You are encouraged to also make your
own separate assignment to the ontology by completing Task 3. Note that your
meta-property assignments may be diﬀerent because the “same [terms from an
ontology represent] diﬀerent concepts to diﬀerent people” [Wel06]. If your meta-
property assignments are diﬀerent from Figure A.4, you are encouraged to create

A.1.
OntoClean in OWL with a DL reasoner
253
Figure A.2: Individuals of type ontoclean:Class and AmountOfMatter’s two sub-
classes asserted in the ABox with ontoclean:hasSubClass.
Task 3
1. List all the classes in the ontology on a sheet of paper and assign your own meta-properties.
2. Compare your assignments with the ones provided in Figure A.4.
3. Now add the metaproperty assignments to the OWL ﬁle, be it either your own assignments
or the ones provided in Figure A.4: for each Individual, declare it an instance of the respective
metaproperty class. For instance, the instance version of Paper has to become an instance of
ontoclean:DependentClass.
your own version of the OntocleanTutorialOntologyPunnedMetaProperties.owl
ﬁle. The next section will show you how to detect inconsistencies in the provided
(or your own) punned ﬁle with assigned meta-properties.
Detecting inconsistencies in the hierarchy
Once meta-properties have been assigned and added to the OWL ﬁle, inconsisten-
cies can be discovered by starting a reasoner3. For instance, when HermiT 1.3.8 is
started on the provided OntocleanTutorialOntologyPunnedMetaProperties.owl
ﬁle, you should encounter something that looks like the screenshot in Figure A.5.
Contrary to intended use of reasoners for ontology development, this is supposed
to happen. Upon clicking the explain button then a pop-up with a list of inconsis-
tencies will be shown (see Figure A.6). Carry out Task 5.
A number of errors should be detected, be it either form your own assignments
or the one provided. In the remainder of this section, we describe some of those
errors and their causes.
3https://protegewiki.stanford.edu/wiki/Using_Reasoners

254
Appendix A. Tutorials
Figure A.3: Paper individual meta-property assignment.
Figure A.4: Tutorial ontology with possible meta-properties assigned.
AmountOfMatter and Living
AmountOfMatter and Living have an inconsis-
tency and violate the unity rule that says an entity with a unity tag may not be a
subclass of an anti-unity entity. In this case AmountOfMatter has an anti-unity tag
as amount of matter may be any object but may not be considered as a whole (for
instance, water) hence there can be no one unifying criterion. The Living entity
has a unifying criterion, that is, any instance of it should be living though it may
be any living object hence it may not be a subclass of AmountOfMatter.
AmountOfMatter and Amphibian
Amphibian and AmountOfMatter are also
inconsistent as they violate the unity rule that states that entities that possess pos-
itive unity tags may not be subclasses of entities with anti-unity tags. Though not
a direct subclass of AmountOfMatter, Amphibian is however indirectly a subclass
through the relation to the Living entity which is a subclass of AmountOfMatter
and automatically makes it a subclass of AmountOfMatter too.
Sphere and Ball
Sphere and Ball also have an inconsistency as they violate the
identity constraint that says that an entity with an identity criterion may not be
a super class to a non-identity class. A ball has an identifying criterion in that a

A.1.
OntoClean in OWL with a DL reasoner
255
Figure A.5: Reasoner notiﬁcation for in-
consistencies.
Figure A.6: Section of detected inconsis-
tencies.
Task 5
1. Ensure to have open either the provided OWL ﬁle that has the meta-properties assigned
(OntocleanTutorialOntologyPunnedMetaProperties.owl) or your own one where the
meta-properties have been assigned.
2. Start the reasoner.
3. Click the “Explain” button. You can either let it compute as many explanations as it
will, or stop after a few. We assume you stop after a few.
4. Examine the ﬁrst explanation for the inconsistency. What is the cause of it? That is:
which OntoClean rule did it violate?
5. Once you are convinced which rules it violated, correct the taxonomy in the TBox (i.e.,
among the classes) and their corresponding individuals in the ABox. Resolution can be
either to change the meta-property assignment or to change the position of the classes in
the taxonomy.
6. Carry out steps 2-5 until there are no reported inconsistencies.
ball may easily be recognised as a ball, whereas a sphere may be any object that
has a sphere shape.
Father and Male
Father and Male entity relationship violates the dependence
constraints which states that a non-dependent entity may not be a subclass of a
dependent class. Thus, for someone to be a male they do not need to be father, in
other words Male does not depend on Father. Rather, conventionally for you to be
a father then you must be a male.
Door and Wood
The Door and Wood relationship is incorrect: it violates the
rigidity rule. Door cannot be a super class of wood since it is anti-rigid and Wood
is rigid. A door is considered anti-rigid in the sense that a door may cease to be

256
Appendix A. Tutorials
a door say, when it breaks and get replaced. Whereas wood will always be wood
whether it is used to make some furniture or just stored for future use.
Teacher and Professor
The relationship between Teacher and Professor vio-
lates the rigidity rule. The professor is a rigid concept as a person once granted the
title they will always be a professor as long as they live. A teacher however may
cease to be a teacher at any moment or stage in their life, hence it is anti-rigid.
AmountOfMatter and Mammal
The relationship between AmountOfMatter
and Mammal via Living is inconsistent as it violates the unity constraint that states
that an anti-union class may not be a super class of a unity class. Mammals have
unifying characteristics that may include giving birth to live babies and that they
are all vertebrates but amount of water may not be uniﬁed as other matter may
not be quantiﬁable as a whole for example water.
AmountOfMatter and Human
The relationship between AmountOfMatter
and Human violates the unity rule since AmountOfMatter is anti-rigid and Human
is rigid. It is not a direct violation but since Human’s superclass has an ancestor
(AmountOfMatter) that is anti-unity. AmountOfMatter is not a whole for all its
instances so they cannot uniﬁed but Human has a unifying criteria that may include
being warm-blooded and giving birth to live oﬀspring.
Human and Teacher
The relationship between Human and Teacher violates
the unity rule; a non-unity class may not be a subclass of an anti-unity class.
Fixing the hierarchy
Following on from Task 5, you should have resolved each inconsistency so that the
reasoner does not return you any more errors like in Figure A.5. These errors can
be ﬁxed by either re-arranging the position of the classes in the taxonomy, or, upon
closer inspection, one may have decided to change the meta-property assignment.
An example of the former is that your revised hierarchy surely should not have
AmountOfMatter at the top. An example of the latter is that a +R on Professor
would generally be considered to be incorrect4, but is ∼R instead.
A.2
An OBDA system for elephants
This tutorial turned out to take up more space than intended and it is therefore
moved to an online-only document, which can be accessed from the book’s website
at https://people.cs.uct.ac.za/~mkeet/OEbook/. The tutorial takes a mod-
iﬁed version of the AWO v4 and links it up to a database with real data about
elephants in the Kruger National Park.
4being a professor is a role that a human plays, so it would be an anti-rigid property, and then
not violating the rigidity rule anymore.

APPENDIX B
Assignments
Besides the exercises to engage with the material, there are two assignments. They
have a strong integrative ﬂavour to it and will take up more time than an exercise.
You are expected to applying as much as you have learned, and look up more
information online, be they tools, methods, other ontologies, or scientiﬁc papers.
The assignments in this appendix are described in such as way that they may
serve as a draft description of the actual assignment. For instance, I include hard
deadlines for hand-in, notwithstanding that I know the material will be better
with more work put into it (ontologies are never really ‘ﬁnished’). I consider both
assignments to be group assignments, even though they could be done individually,
and I assign people to groups and topics if they didn’t make groups themselves by
a given date. If the practical assignment is scheduled for the end of Block I rather
than for hand in at the end of Block II, then remove the requirements on Block II
topics form the description below.
Regarding the project assignment: most topics can be reused across years, and
some of them can be done by more than one group as it invariable ends up in
diﬀerent results. Some of the previously listed projects that were carried out in
earlier instalments have some material available online, which gives an indication of
success (i.e., examples of projects that received top marks). One such mini-project
from the OE course at the University of Havana (2010) resulted in OntoPartS—
a tool and creative extension to the part-whole taxonomy of [KA08]—that was
subsequently formalised, written up, and published [KFRMG12]. Another one is
the OWL Classiﬁer1, which was developed as part of one of the mini-projects of
the 2016 OE course at the University of Cape Town, and subsequently used toward
conﬂict resolution in [KK17b]. These topics are not listed below anymore, for the
obvious reasons.
1https://github.com/muhummadPatel/OWL_Classifier
257

258
Appendix B. Assignments
B.1
Practical Assignment: Develop a Domain On-
tology
The aim of this practical assignment is for you to demonstrate what you have
learned about the ontology languages, top-down and bottom-up ontology develop-
ment, and methods and methodologies, and experiment with how these pieces ﬁt
together.
You can do this assignment in groups of two or three students. It should be
mentioned in the material you will hand in who did what.
Tasks
1. Choose a subject domain of interest for which you will develop a domain
ontology. For instance, computers, tourism, furniture, some hobby you may
be familiar with (e.g., diving, dancing), or some other subject domain you
happen to be knowledgable about (or know someone who is).
2. Develop the domain ontology in the best possible way. You are allowed to
use any resource you think is useful, be it other ontologies, non-ontological
resources, tools, domain experts, etc.. If you do so (and you are encouraged
to do so), then make sure to reference them in the write-up.
3. Write about 2-3 pages (excluding ﬁgures or screenshots) summarising your
work. This can include—but is not limited to—topics such as an outline of
the ontology, why (or why not) you have used a foundational ontology (if
so, which, why), if you could reuse a top-domain or other subject domain
ontology, which non-ontological resources you have used (if any, and if so,
how), if you encountered subject domain knowledge that should have been
in the ontology but could not be represented due to the limitations of OWL,
or perhaps a (real or imagined) purpose of the ontology and therefore a
motivation for some OWL fragment, any particular reasoning services that
was useful (and how and why, which deductions did you have or experimented
with), any additional tools used.
Material to hand in
Send in/upload to the course’s CMS the following items:
1. The OWL ﬁle of your ontology;
2. Imported OWL ontologies, if any;
3. The write up in pdf.
Assessment
1. Concerning the ontology: quality is more important than quantity. An on-
tology with more advanced constraints and appropriate reuse of foundational

B.1.
Practical Assignment: Develop a Domain Ontology
259
or general ontologies is a better illustration of what you have learned than a
large bare taxonomy.
2. Concerning the ontology: it will be checked on modelling errors in the general
sense (errors such as is-a vs. part-of, class vs. instance, unsatisﬁable classes).
Regarding the subject domain itself, it will be checked only insofar as it
indicates (mis)understanding of the ontology language or reasoning services.
3. Concerning the write up: a synthesis is expected, not a diary. For instance,
“We explored a, b, and c, and b was deemed to be most eﬀective because
blabla” would be ﬁne, but not “We tried a, but that didn’t work out, then
we had a go at b, which went well, then we came across c, tried it out of
curiosity, but that was a dead end, so we went back to b.”. In short: try to
go beyond the ‘knowledge telling’ and work towards the so-called knowledge
transformation.
4. Concerning the write up: while a brief description of the contents is useful,
it is more important to include something about the process and motivations
how you got there, covering topics such as, but not limited to, those men-
tioned under Tasks, item 3 (and recollect the aim of the assignment—the
more you demonstrate it, the better).
Notes
In random order:
1. The assignment looks easy. It isn’t. If you start with the development of
the ontology only the day or so before the deadline, there is an extremely
high probability that you will fail this assignment. Your assignment will be
of a higher quality if you start thinking about it some 2 weeks before the
deadline, and the actual development at most one week before the deadline,
and spread out the time you are working on it.
2. If you use non-English terms for the classes and properties, you should either
add the English in the annotations (preferred), else lend me a dictionary if it
is in a language I do not speak.
3. Use proper referencing when you use something from someone else, be it
an ontology, other reused online resources (including uncommon software),
textbooks, articles etc. Not doing so amounts to plagiarism.
4. Spell checkers tend to be rather useful tools.
5. Some of the mini-project topics can beneﬁt from an experimental ontology
that you know in detail, which you may want to take into consideration when
choosing a subject domain or purpose so that your ontology might be reused
later on.

260
Appendix B. Assignments
B.2
Project Assignment
The project assignment aims to let you explore in more detail a speciﬁc subtopic
within ontology engineering. There is enough variation in the list of topics below to
choose either a software development project, literature review, experimentation,
and occasionally a bit of research. The two subsections below could be used as is
or function as a template for one’s own speciﬁcation of constraints and more or less
project topics.
Suggested set-up of the assignment
The aim of this assignment is to work in a small group (2-4 students) and to
investigate a speciﬁc theme of ontology engineering.
The topics are such that
either you can demonstrate the integration of various aspects of ontologies and
knowledge bases or going into quite some detail on a single topic, and it can be
either theory-based, implementation-focussed, or a bit of both.
Possible topics to choose from will be communicated in week x, and has to be
chosen and communicated to me (topic + group members) no later than in week
x+1, else you will be assigned a topic and a group.
Tasks
1. Form a group of 2-4 people and choose a topic, or vv.: choose a topic and
ﬁnd other people to work with. It should be mentioned in the material you
will hand in who did what.
2. Carry out the project.
3. Write about 4-6 pages (excluding ﬁgures or screenshots) summarising your
work. The page limit is ﬂexible, but it surely has to be < 15 pages in total.
4. Give a presentation of your work during the last lecture (10 minutes presen-
tation, ±5 minutes discussion). Everyone must attend this lecture.
Material to hand in
You have to upload/email the following items:
1. The write up.
2. Additional material: this depends on the chosen topic. If it is not purely
paper-based, then the additional ﬁles have to be uploaded on the system
(e.g., software, test data).
3. Slides of the presentation, if any.
Note that the deadline is after the last lecture so that you have the option to
update your material for the mini-project with any feedback received during the
presentation and discussion in class.

B.2.
Project Assignment
261
Assessment
1. Concerning the write up: a synthesis is expected, not a diary. For instance,
“We explored a, b, and c, and b was deemed to be most eﬀective because
blabla” would be ﬁne, but not “We tried a, but that didn’t work out, then
we had a go at b, which went well, then we came across c, tried it out of
curiosity, but that was a dead end, so we went back to b.”. In short: try to
go beyond the ‘knowledge telling’ and work towards the so-called knowledge
transformation.
2. Concerning the write up: use proper referencing when you use something
from someone else, be it an ontology, other reused online resources (includ-
ing uncommon software), textbooks, articles etc. Not doing so amounts to
plagiarism, which has a minimum penalty of obtaining a grade of 0 (zero)
for the assignment (for all group members, or, if thanks to the declaration
of contribution the individual can be identiﬁed, then only that individual),
and you will be recorded on the departmental plagiarism list, if you are not
already on it, and further steps may be taken.
3. The presentation: respect the time limit, coherence of the presentation, ca-
pability to answer questions.
4. Concerning any additional material (if applicable): if the software works as
intended with the given input, presentability of the code.
5. Marks will be deducted if the presentation or the write-up is too long.
Notes
Things you may want to take into consideration (listed in random order):
1. LATEX is a useful typesetting system (including beamer for presentation slides),
has a range of standard layouts as well as bibliography style ﬁles to save you
the trouble of wasting time on making the write up presentable, and gener-
ates a pdf ﬁle that is portable2. This is much less so with MS Word; if you
use MS Word nevertheless, please also include a pdf version of the document.
2. Regarding the bibliography: have complete entries. Examples of referencing
material (for conference proceedings, books and book chapters, and journal
articles) can be found in the scientiﬁc papers included in the lecture notes’
bibliography, scientiﬁc literature you consult, or LATEX documentation.
3. Spell checkers tend to be rather useful tools.
4. One or more of the domain ontologies developed in the previous assignment
may be suitable for reuse in your chosen topic.
5. Some of the mini-projects lend themselves well for extension into an Hon-
ours/Masters project, hence, could give you a head-start.
2in case you are not convinced: check http://openwetware.org/wiki/Word_vs._LaTeX or
http://ricardo.ecn.wfu.edu/~cottrell/wp.html

262
Appendix B. Assignments
Topics
You can select one of the following topics, or propose one of your own. If the
latter, you ﬁrst will have to obtain approval from you lecturer (if you are doing
this assignment as part of a course).
The topics are listed in random order, have diﬀerent ﬂavours or emphases; e.g.,
more of a literature review project, or programming, or theory, or experimentation.
The notes contain pointers to some more information, as does the corresponding
section in the book.
Some descriptions may seem vague in that it still oﬀers several possibilities
with more or less work and/or more or less challenging activities; this is done on
purpose. You should narrow it down as you see ﬁt—bearing in mind the aims of
the mini-project and the number of people in your group—and be able to justify
why if asked to do so. If you choose a topic that involves processing OWL ﬁles,
then try to use either the OWL API (for Java-based applications) or Owlready
(Python), the Jena Toolkit (or similar), or OWLink, rather than spending time
reinventing the wheel.
You probably will ﬁnd out it’s not as easy as it looked like initially, which may
make it tempting wishing to change, but that also holds for the other topics, and
by changing topic you very likely have lost too much time to bring the other one
to passable completion.
Some topics can be done by more than one group simultaneously, after which
it can be interesting to compare what came out of it. Based on my experience of
previous years, these include topic numbers 5, 7, 14, 16, and 19.
1. Consider the Lemon model for monolingual and multilingual ontologies
[MdCB+12, MAdCB+12].
Apply it to an ontology you developed for the
practical assignment. You can choose any natural language, as long as it is
a diﬀerent one from the one the ontology is developed in or requires some
redesign to bring it in line with best practices on the ontology-language in-
terface.
2. There are some 15 working automated reasoners for various Description Log-
ics (e.g., HermiT, FaCT++, TrOWL, Pellet, Racer, ELK, MoRE, Quonto,
Quest). Conduct a comparison with a sensible subset, including their per-
formance on a set of ontologies you select or create artiﬁcially, and possibly
also along the direction of examining the eﬀect of the use of diﬀerent features
in the ontology. Some ideas may be gleaned from the “short report” in the
proceedings of the Ontology Reasoner Evaluation workshop3.
3. Write a literature review about ontology-driven NLP or ontology-enhanced
digital libraries (minimum amount of references to consider depends on group
size and whether this is an undergraduate or postgraduate course).
4. Set up an OBDA system (that is not one of the samples of the Ontop website).
You are advised to either reuse your ontology from the practical assignment
3http://ceur-ws.org/Vol-1015/, and there have been subsequent editions of the ORE work-
shops.

B.2.
Project Assignment
263
or take an existing one. Regarding data, you may want to have a look at
the GitHub list of publicly available data sets4, or you can create a mock
database.
5. There are various ways to verbalize an ontology (e.g., ∀as ‘each...’ or ‘for all
...’, using the opposite, etc.). Write a web-based or stand-alone application
that verbalises an ontology in a natural language of choice, where users will
be able to choose their verbalization. Some of the examples you may want
to check out are the Attempto project5, the discussion in [SKC+08], results
with isiZulu [KK17a] or Afrikaans [STK16] in case you’re in South Africa, or
either of the review articles [BACW14, SD17].
6. Create a decision diagram for BFO or GFO, alike the D3 of FORZA, and
validate it (e.g., by checking whether extant ontologies linked to BFO or
GFO were linked correctly).
7. There are many works on criteria to evaluate the quality of an ontology or an
ontology module (e.g., [McD17, PVSFGP12, Vra09]). Provide an overview
and apply it to several of your classmates’ ontologies developed in the prac-
tical assignment and/or ontologies taken from online repositories. In a larger
group, this may also include evaluating the evaluation strategies themselves.
8. Take a thesaurus represented in SKOS6 and convert that into a real ontology
(i.e., not simply only converting into OWL). There are manual and semi-
automated approaches.
9. Compare and contrast tools for ontology visualization (i.e., their graphical
renderings in, e.g., Ontograf, OWLGrEd, SOVA, and so on).
10. OWL (and DL) is claimed to provide a ‘unifying paradigm’ for several knowl-
edge representation languages. Evaluate this claim, taking into consideration
Common Logic as alternative ontology language that makes the same claim.
11. Conduct a literature review on, and, where possible, test, several so-called
‘non-standard reasoning’ reasoners. Among others, you could consult one of
the least-common subsumer papers [PT11], and/or reasoner-mediated ontol-
ogy authoring tools [FR12, Fer16], among others.
12. Write a plugin for Semantic MediaWiki such that an ontology can be used
during the page editing stage; e.g., to have the hierarchy with terms (classes,
object/data properties) on the left-hand side of the screen, and easily pick
one of them, adding the annotation to the text automatically. Another idea
for a semwiki plugin you may have may be considered (but check out ﬁrst
which ones are already there).
4https://github.com/caesar0301/awesome-public-datasets
5http://attempto.ifi.uzh.ch/site/tools/
6see, e.g., the list at http://code.google.com/p/lucene-skos/wiki/SKOSThesauri, but
there are others you may wish to consider

264
Appendix B. Assignments
13. Compare and contrast the two OntoClean implementations [GRV10, Wel06],
including experimental assessment.
14. Translational Research and the Semantic Web. Students should study and
analyse the ‘old’ paper “Advancing translational research with the Semantic
Web” [RCB+07]. For the report, you can envision yourself as a consultant
to the authors (from W3C’s HCLS IG7) and suggest them improvements to
what they did, given that a lot of new material has been developed over the
past 12 years.
15. Consider bottom-up ontology development starting from a conceptual data
model (e.g., UML class diagrams, EER). Find a way to generate an OWL
ﬁle from it such that it provides candidate classes, object properties, and
constraints for an actual ontology. You may want to take one of the freely
available conceptual modelling tools (e.g., ArgoUML) and use that seriali-
sation for the transformations. There are a few tools around that try this
already, but they have incomplete coverage (at best).
16. Compare and contrast ontology editors in a meaningful way; e.g., the Prot´eg´e
stand-alone tool, WebProt´eg´e, MoKI, NeON toolkit, etc.
17. Implement the RBox reasoning service and demonstrate correctness of im-
plementation in the software.
18. OWL does not consider time and temporal aspects. To address this shortcom-
ing, several ‘workarounds’ as well as temporal description logics have been
proposed. Compare and contrast them on language features and modelling
problems and reasoning.
19. Develop a Prot´eg´e plugin that will show in the interface not the Manchester
syntax with “some” and “not” etc, but the DL axiom components (e.g., alike
it was in its v3.x), or with the corresponding keywords in a natural language
other than English.
20. Provide a state of the art on research into competency questions.
21. Represent DOLCE—or another ontology that has indicated to be needing
more than OWL expressiveness—in DOL on OntoHub; e.g., by adding those
axioms in a module and link them to the OWL ﬁle into a network of ontolo-
gies.
7http://www.w3.org/blog/hcls

APPENDIX C
OWL 2 Proﬁles features list
OWL 2 EL
Supported class restrictions:
– existential quantiﬁcation to a class expression or a data range
– existential quantiﬁcation to an individual or a literal
– self-restriction
– enumerations involving a single individual or a single literal
– intersection of classes and data ranges
Supported axioms, restricted to allowed set of class expressions:
– class inclusion, equivalence, disjointness
– object property inclusion (w. or w.o. property chains), and data property
inclusion
– property equivalence
– transitive object properties
– reﬂexive object properties
– domain and range restrictions
– assertions
– functional data properties
– keys
NOT supported in OWL 2 EL (with respect to OWL 2 DL):
– universal quantiﬁcation to a class expression or a data range
– cardinality restrictions
– disjunction
– class negation
– enumerations involving more than one individual
– disjoint properties
– irreﬂexive, symmetric, and asymmetric object properties
– inverse object properties, functional and inverse-functional object properties
265

266
Appendix C. OWL 2 Proﬁles features list
OWL 2 QL
The supported axioms in OWL 2 QL take into account what one can use on the
left-hand side of the inclusion operator (⊑, SubClassOf) and what can be asserted
on the right-hand side:
• Subclass expressions restrictions:
– a class
– existential quantiﬁcation (ObjectSomeValuesFrom) where the class is
limited to owl:Thing
– existential quantiﬁcation to a data range (DataSomeValuesFrom)
• Super expressions restrictions:
– a class
– intersection (ObjectIntersectionOf)
– negation (ObjectComplementOf)
– existential quantiﬁcation to a class (ObjectSomeValuesFrom)
– existential quantiﬁcation to a data range (DataSomeValuesFrom)
Supported Axioms in OWL 2 QL:
– Restrictions on class expressions, object and data properties occurring in
functionality assertions cannot be specialized
– subclass axioms
– class expression equivalence (involving subClassExpression), disjointness
– inverse object properties
– property inclusion (not involving property chains and SubDataPropertyOf)
– property equivalence
– property domain and range
– disjoint properties
– symmetric, reﬂexive, irreﬂexive, asymmetric properties
– assertions other than individual equality assertions and negative property as-
sertions (DiﬀerentIndividuals, ClassAssertion, ObjectPropertyAssertion, and
DataPropertyAssertion)
NOT supported in OWL 2 QL (with respect to OWL 2 DL):
– existential quantiﬁcation to a class expression or a data range in the subclass
position
– self-restriction
– existential quantiﬁcation to an individual or a literal
– enumeration of individuals and literals
– universal quantiﬁcation to a class expression or a data range
– cardinality restrictions
– disjunction
– property inclusions involving property chains
– functional and inverse-functional properties
– transitive properties
– keys
– individual equality assertions and negative property assertions

267
OWL 2 RL
Supported in OWL 2 RL:
– More restrictions on class expressions (see table 2 of [MGH+09]; e.g., no
SomeValuesFrom on the right-hand side of a subclass axiom)
– All axioms in OWL 2 RL are constrained in a way that is compliant with the
restrictions in Table 2.
– Thus, OWL 2 RL supports all axioms of OWL 2 apart from disjoint unions
of classes and reﬂexive object property axioms.
A quick one-liner of the diﬀerence is: No ∀and ¬ on the left-hand side, and ∃and
⊔on right-hand side of ⊑.


APPENDIX D
Complexity recap
This appendix is expected to be relevant only to those who have no idea of, or too
little recollection of, computational complexity, or who have come across it many
years ago and may like a brief refresher.
Theory of computation concerns itself with, among other things, languages and
its dual, problems. A problem is the question of deciding whether a given string
is a member of some particular language; more precisely: if Σ is an alphabet,
L is a language over Σ, then the problem L is “given a string w ∈Σ∗, decide
whether or not w is in L”. The usage of ‘problem’ and ‘language’ is interchangeable.
When we focus on strings for their own sake (e.g., in the set {on1n | n ≥1}),
then we tend to think of the set of strings as a language. When we focus on the
‘thing’ that is encoded as a string (e.g., a particular graph, a logical expression,
satisﬁability of a class), we tend to think of the set of strings as a problem. Within
the context of ontologies, we typically talk of the representation languages and
reasoning problems.
There are several classes of languages; see Figure D.1. The regular free lan-
guages have their counterpart with ﬁnite automata; the context-free languages
with push-down automata; the recursive languages is the class of languages ac-
cepted by a Turing machine (TM) that always halts; the recursively enumerable
languages is the class of languages deﬁned by a TM; the non-recursively enumerable
languages is the class of languages for which there is no TM (e.g., the diagonal-
ization language). The recursive languages, and, to a lesser extent, the recursively
enumerable languages, are by far the most interesting ones for ontologies.
Turing machines are used as a convenient abstraction of actual computers for the
notion of computation. A TM that always halts = algorithm, i.e., the TM halts on
all inputs in ﬁnite time, either accepting or rejecting; hence, the recursive languages
are decidable problems/languages. Problems/languages that are not recursive are
called undecidable, and they do not have an algorithm; if they are in the class of
recursively enumerable languages (but not recursive), then they have a procedure
that runs on an arbitrary TM that may give you an answer but may very well never
269

270
Appendix D. Complexity recap
regular free 
languages
context-free 
languages
recursive
languages
recursively
enumerable
languages
non-recursively
enumerable languages
Figure D.1: Graphical depiction of the main categories of languages.
halt; see also Figure D.2. First order predicate logic in its full glory is undecidable.
Description logics are decidable fragments of ﬁrst order predicate logic1, i.e., they
are recursive languages and (can) have algorithms for the usual problems (standard
reasoning services).
Algorithm (Recursive)
Procedure (Recursively 
Enumerable)
non-Recursively 
Enumerable 
A
P
input w
input w
input w
???
yes (w in L)
no (w not in L)
yes (w in L)
Figure D.2:
Graphical depiction of the main categories of languages; the rectangle
denotes a Turing Machine; w is a string and L is a language.
Not all algorithms are alike, however, and some take up more time (by the
CPU) or space (in the form of memory size) to compute the answer than others.
So, we want to know for a given problem, the answer to “how much time [/space]
does it take to compute the answer, as a function of the size of the input?”. If the
computation takes many years with the top-of-the-range hardware, then it is still
not particularly interesting to implement (from a computer science viewpoint, that
is). To structure these matters, we use the notion of a complexity class. There are
very many of them, but we only refer to a few in the context of ontologies. For in-
stance, it may take a polynomial amount of time to compute class subsumption for
an OWL 2 EL-formalised ontology and exponential time to compute satisﬁability
of an EER diagram (represented in the DL DLRifd) and the bigger the diagram
(more precisely: the logical theory), correspondingly the longer it takes. The intu-
ition is depicted in Figure D.3: for small ontologies, there is but a minor diﬀerence
in performance, but one really starts to notice it with larger logical theories.
Looking ahead at the complexity classes relevant for OWL, we list here a de-
scription of the meaning of them (copied from the OWL 2 Proﬁles Standard page
[MGH+09]):
- Decidability open means that it is not known whether this reasoning prob-
lem is decidable at all.
1More precisely: there is at least one that turned out to be undecidable (DLRUS), but this is
an exception to the rule.

271
Figure D.3: General idea of time complexity of an algorithm, as function of the size
of the input. e.g.: All basic arithmetic operations can be computed in polynomial time;
evaluating a position in generalised chess and checkers on an n×n board costs exponential
time.
- Decidable, but complexity open means that decidability of this reasoning
problem is known, but not its exact computational complexity. If available,
known lower bounds are given in parenthesis; for example, (NP-Hard) means
that this problem is at least as hard as any other problem in NP.
- X-complete for X one of the complexity classes explained below indicates
that tight complexity bounds are known—that is, the problem is known to
be both in the complexity class X (i.e., an algorithm is known that only uses
time/space in X) and hard for X (i.e., it is at least as hard as any other
problem in X). The following is a brief sketch of the classes used in Table 4.3,
from the most complex one down to the simplest ones.
- 2NEXPTIME is the class of problems solvable by a nondeterministic
algorithm in time that is at most double exponential in the size of the
input (i.e., roughly 22n, for n the size of the input).
- NEXPTIME is the class of problems solvable by a nondeterministic
algorithm in time that is at most exponential in the size of the input
(i.e., roughly 2n, for n the size of the input).
- PSPACE is the class of problems solvable by a deterministic algorithm
using space that is at most polynomial in the size of the input (i.e.,
roughly nc, for n the size of the input and c a constant).
- NP is the class of problems solvable by a nondeterministic algorithm
using time that is at most polynomial in the size of the input (i.e.,
roughly nc, for n the size of the input and c a constant).
- PTIME is the class of problems solvable by a deterministic algorithm
using time that is at most polynomial in the size of the input (i.e.,
roughly nc, for n the size of the input and c a constant). PTIME is
often referred to as tractable, whereas the problems in the classes above
are often referred to as intractable.
- LOGSPACE is the class of problems solvable by a deterministic al-
gorithm using space that is at most logarithmic in the size of the in-

272
Appendix D. Complexity recap
put (i.e., roughly log(n), for n the size of the input and c a constant).
NLOGSPACE is the nondeterministic version of this class.
- AC0 is a proper subclass of LOGSPACE and deﬁned not via Turing
Machines, but via circuits: AC0 is the class of problems deﬁnable using
a family of circuits of constant depth and polynomial size, which can
be generated by a deterministic Turing machine in logarithmic time (in
the size of the input). Intuitively, AC0 allows us to use polynomially
many processors but the run-time must be constant. A typical example
of an AC0 problem is the evaluation of ﬁrst-order queries over databases
(or model checking of ﬁrst-order sentences over ﬁnite models), where
only the database (ﬁrst-order model) is regarded as the input and the
query (ﬁrst-order sentence) is assumed to be ﬁxed. The undirected graph
reachability problem is known to be in LogSpace, but not in AC0.

APPENDIX E
Answers of selected exercises
Answers Chapter 2
Answer Exercise 2.1. Indicative descriptions:
(a) All lions are mammals.
(b) Each PC has as part at least one CPU and at least one Monitor connected
(c) Proper part is asymmetric.
Answer Exercise 2.2.
(a) ∀x(Car(x) →V ehicle(x))
(b) ∀x(HumanParent(x) →∃y(haschild(x, y) ∧Human(y)))
(c) ∀x, y(Person(x) ∧Course(y) →¬(lecturerOf(x, y) ∧studentOf(x, y)))
Answer Exercise 2.3.
(b) There exists a node that does not participate in an instance of R, or: it does not
relate to anything else: ∃x∀y.¬R(x, y).
(c) L = ⟨R⟩as the binary relation between the vertices. Optionally, on can add the
vertices as well. Properties:
R is symmetric: ∀xy.R(x, y) →R(y, x).
R is irreﬂexive: ∀x.¬R(x, x).
If you take into account the vertices explicitly, one could say that each note par-
ticipates in at least two instances of R to diﬀerent nodes.
Answer Exercise 2.4.
(a) R is reﬂexive (a thing relates to itself): ∀x.R(x, x).
R is asymmetric (if a relates to b through relation R, then b does not relate back
to a through R): ∀xy.R(x, y) →¬R(y, x).
(b) See the example on p21 of the lecture notes.
Answer Exercise 2.6. Note: there may be more than one solution; only one is given.
Also note that a problem of natural language is that it can be imprecise.
273

274
Appendix E. Answers of selected exercises
1. ∀x(Lion(x) →Animal(x))
2. ∀x(Professor(x) →∃y(teaches(x, y) ∧Course(y)))
3. ∀x(Human(x) →∃y, z(eat(x, y) ∧Fruit(y) ∧eat(x, z) ∧Cheese(z))) (note: twice
the ‘eat’, with y and z, not “fruit(y) ∧cheese(y)”, for that refers to the objects
that are both, which don’t exist)
4. ∀x(Animal(x) →Herbivore(x) ∨Carnivore(x))
5. ∀x, y, z(hasmother(x, y) ∧hassister(y, z) →hasaunt(x, z)) (or with ↔and/or
with the composition operator ◦)
6. ∀x(Pap(x) →¬Pizza(x))
7. ∀x∃y((Person(x) ∧worksfor(x, y) ∧Company(y) →Employee(x)))
8. ∀x, y(manages(x, y) ↔Manager(x))
9. ∀x(Fly(x) →∃=2y(haspart(x, y) ∧Eye(y))) (note: this is shorthand notation...)
10. ∀x, y, z(life(x, y) ∧life(x, z) →y = z)
11. ∀x, y(participation(x, y) →PhysicalObject(x) ∧Process(y))
12. ∀x, y(hasPart(x, y) →partOf−(x, y))
13. ∀x, y(connection(x, y) →connection(y, x))
14. Vehicles: combine the pattern for the ‘or’ from 4 with the disjoints of 6, for the
vehicles.
15. ∃x(Snail(x)∧slow(x)) but not this is suboptimal (recall the apple & green; similar
story here)
16. ∀x(Patient(x) →∃y, z(registration(x, y, z) ∧Hospital(y) ∧Weekday(z)))
17. Note: this requires either a temporal ‘extension’ or necessity (beyond the current
scope). Let’s take temporal, for which we introduce a notions of time, t, that quan-
tiﬁes over time points only (for simplicity, and linear time): ∀x, t(Student(x, t) →
∃t′ ̸= t(¬Student(x, t′)))
Answers Chapter 3
Answer Exercise 3.1. Note: there may be more than one solution; only one if given.
Also note that these axioms are agnostic about particular fragments, and we don’t con-
sider datatypes.
1. Lion ⊑Animal
2. Professor ⊑∃teaches.Course
3. Human ⊑∃eat.Fruit ⊓∃eat.Cheese
4. Animal ⊑Herbivore ⊔Carnivore
5. hasMother ◦hasSister ⊑aunt (or with ≡, i.e., that the notion of ‘aunt’ is deﬁned by
it)
6. Pap ⊑¬Pizza (or with ‘bottom’: Pap ⊓Pizza ⊑⊥)
7. Person ⊓∃worksFor.Company ⊑Employee
8. ∀manages.⊤≡Manager
9. Fly ⊑= 2 hasPart.Eye
10. lazy option: Func(life), less lazy, as part of another axiom, ≤1 life or ≤1 life.⊤
11. lazy option: Participation ⊑PhysicalObject × Process, and in full: ∃participation ⊑PhysicalObject
and ∃participation−⊑Process
12. hasPart ⊑partOf−
13. lazy option (in SROIQ): Sym(connection)
14. Vehicles: combine the ‘or’ from 4 with the disjoints of 6.
15. Not easily represented in DLs (rework it with some subtype of snails for which it

275
always holds)
16. This can be represented in the DLR family of Description Logics, but not in most
DLs and not in OWL either (which has only binaries—we’ll return to this in the
second part of the module)
17. This can be represented in several temporal description logics, using temporal
operators, alike Student ⊑⋄∗¬Student with the diamond-shape the temporal coun-
terpart of ∃and with ∗, this reads as ‘sometime’. More about this can be found in
the ‘advanced topics’.
Answer Exercise 3.2.
(a) Rewrite (Eq. 3.24) into negation normal form:
Person ⊓∀eats.Plant ⊓(¬Person ⊔¬∀eats.(Plant ⊔Dairy))
Person ⊓∀eats.Plant ⊓(¬Person ⊔∃¬eats.(Plant ⊔Dairy))
Person ⊓∀eats.Plant ⊓(¬Person ⊔∃eats.(¬Plant ⊓¬Dairy))
So our initial ABox is:
S = {(Person ⊓∀eats.Plant ⊓(¬Person ⊔∃eats.(¬Plant ⊓¬Dairy)))(a)}
(b) Enter the tableau by applying the rules until either you ﬁnd a completion or only
clashes.
(⊓-rule): {Person(a), ∀eats.Plant(a), (¬Person ⊔∃eats.(¬Plant ⊓¬Dairy))(a)}
(⊔-rule): (i.e., it generates two branches)
(1) {Person(a), ∀eats.Plant(a), (¬Person ⊔∃eats.(¬Plant ⊓¬Dairy))(a),
¬Person(a)} ¡clash!
(2) {Person(a), ∀eats.Plant(a), (¬Person ⊔∃eats.(¬Plant ⊓¬Dairy))(a),
∃eats.(¬Plant ⊓¬Dairy)(a)}
(∃-rule): {Person(a), ∀eats.Plant(a), (¬Person⊔∃eats.(¬Plant⊓¬Dairy))(a),
∃eats.(¬Plant ⊓¬Dairy)(a), eats(a, b), (¬Plant ⊓¬Dairy)(b)}
(⊓-rule): {Person(a), ∀eats.Plant(a), (¬Person⊔∃eats.(¬Plant⊓¬Dairy))(a),
∃eats.(¬Plant⊓¬Dairy)(a), eats(a, b), (¬Plant⊓¬Dairy)(b), ¬Plant(b),
¬Dairy(b)}
(∀-rule): {Person(a), ∀eats.Plant(a), (¬Person⊔∃eats.(¬Plant⊓¬Dairy))(a),
∃eats.(¬Plant ⊓¬Dairy)(a), eats(a, b), (¬Plant ⊓¬Dairy)(b),
¬Plant(b), ¬Dairy(b), Plant(b)} ¡clash!
(c) T ⊢V egan ⊑V egetarian? yes
Answers Chapter 4
Answer Exercise 4.2. Use a property chain.
Answer Exercise 4.5.
(c) Expressivity: ALN, in OWL DL. Not in OWL Lite and OWL 2 EL, QL, RL
because of the minCardinality.
(e) Expressivity: ALQ, in OWL 2 DL. Not in OWL DL anymore because of the qual-
iﬁed number restriction.

276
Appendix E. Answers of selected exercises
Answer Exercise 4.8. As historical note: the original exercise came from SSSW 20051,
which was at the time of OWL 1. The answers here and the next ones have been updated
taking into account OWL 2.
(a) The description is suﬃciently vague that it may be either of
JointHonsMathsComp ≡∃takes.MathsModule ⊓∃takes.ComputerScienceModule
JointHonsMathsComp ≡Student ⊓∃takes.MathsModule ⊓
∃takes.ComputerScienceModule
In any case, observe it is not “takes.(MathsModule ⊓ComputerScienceModule)”; see
Example 5.3 for further explanation.
(b) SingleHonsMaths ≡Student ⊓∃takes.MathsModule ⊓∀takes.ComputerScienceModule.
This is also called ‘closing’ the axiom.
So: yes, this is possible. A possible solution is shown in university1.owl
Answer Exercise 4.9.
Deductions: they are undergrad students, and students 2,
4, and 7 are JointHonsMathsComp.
Student7 is one, because it is an instance of
∃takes.MathsModule and has a property assertion that s/he takes CS101.
No student is a SingleHonsMaths, despite that, e.g., Student3 has declared taking
two math modules. This is due to the Open World Assumption: student3 may well take
other courses that we don’t know of as of yet, so it is not guaranteed in all possible worlds
that student3 takes only those two math courses.
Answer Exercise 4.10. This can now be done with OWL 2, for it permits qualiﬁed
number restrictions; see university2.owl
(a) This poses no problem, because of the no unique name assumption: it will infer
that CS101 and CS102 must be the same object, so then student 9 still takes 2
courses and all is well; see university2.owl.
(b) This does pose a problem, because each of the three courses mentioned are member
of their respective classes that are disjoint, so they must be distinct individuals,
and thus we obtain a violation in cardinality restrictions (=2 vs =3), and therewith
the ontology has become inconsistent.
Answer Exercise 4.11.
Let us ﬁrst have a look randomly at a deduction and its
explanation (click on the “?” right from the deduction in Prot´eg´e) as a ﬁrst step toward
ﬁguring out why so many classes are unsatisﬁable (i.e., equivalent to Nothing, or ⊥).
Take the explanation for CS StudentTakingCourses:
1http://owl.man.ac.uk/2005/07/sssw/university.html

277
This CS StudentTakingCourses has a long explanation of why it is unsatisﬁable, and
we see that some of the axioms that it uses to explain the unsatisﬁability also have
unsatisﬁable classes. Hence, it is a good idea to set this aside for a while, as it is a
knock-on eﬀect of the others that are unsatisﬁable.
Let us have a look at the unsatisﬁability regarding departments.
So, the AI Dept is unsatisﬁable because its superclass CS Department is, i.e., it is a
knock-on eﬀect from CS Department. Does this give suﬃcient information as to say why
CS Department is inconsistent? In fact, it does. See the next screenshot, which is the
same as lines 3-7, above.

278
Appendix E. Answers of selected exercises
CS Department is unsatisﬁable, because it is affiliatedWith some CS Library that, in
turn (by transitivity),
is affiliatedWith some EE Library that belongs to the
EE Department, which is disjoint from CS Department. Two ‘easy’ options to get rid of
this problem are to remove the transitivity or to remove the disjointness. Alternatively,
we could revisit the domain knowledge; e.g., CS library may not be affiliatedWith EE
library, but is, adjacentTo or disjoint with the EE library.
Let us now consider why CS course is unsatisﬁable:
We have again that the real problem is CS Department; ﬁx that one, and CS course is
satisﬁable, too.
There is a diﬀerent issue with AIStudent. From the explanation in the next screen-
shot, we can see immediately it has something to do with the inconsistency of HCIStudent.

279
But looking at HCIStudent for a clue does not help us further in isolating the problem:
Considering the axioms in the explanation only, one can argue that the root of the
problem is the disjointness between AIStudent and HCIStudent, and remove that axiom
to ﬁx it. However, does it really make sense to have the union ProfessorInHCIorAI?
Not really, and therefore it would be a better ﬁx to change that one into two separate
classes, ProfessorInHCI and ProfessorInAI and have them participating in
ProfessorInHCI ⊑∀advisorOf.HCIStudent and
ProfessorInAI ⊑∀advisorOf.AIStudent,
respectively.
Last, we have a problem of conﬂicting cardinalities with LecturerTaking4Courses:
it is a subclass of TeachingFaculty, which is restricted to taking at most 3 courses,

280
Appendix E. Answers of selected exercises
which is in conﬂict with the “exactly 4” of LecturerTaking4Courses. This can be ﬁxed
by changing the cardinality of either one, or perhaps a lecturer taking 4 courses is not a
sub- but a sister-class of TeachingFaculty.
Answer Exercise 4.12. The general issue is that so-called ‘ideal’ solutions are not
possible within OWL 2 DL.
(a) The crux is that Penguin is an exception to the general case (that birds ﬂy). There
are two principal options: either use some non-monotonic logic, which has an oper-
ator alike ‘generally’ and ‘deviant case’, or use some approximation with classes like
a NonFlyingBird and a FlyingBird. An example of the latter approach is described
in the GoodOD guide [SSRG+12] (its section 6.2.1 in v1).
(b) The key issue here is how to deal with the phases and also here there are several
options. One is the OntoClean way (which we will see in Chapter 5). Another
option may be a temporal extension to be able to assert logically that there are
successive stages in that way and no other (see Chapter 10), or a workaround
approximating the knowledge in some way within OWL expressiveness, for which
there are manifold options.
Answers Chapter 5
Answer Exercise 5.1. No, in that case, it is deﬁnitely not a good ontology. First, there
are several CQs for which the ontology does not have an answer, such as not having any
knowledge of monkeys represented. Second, it does contain knowledge that is not men-
tioned in any of the CQs, such as the carnivorous plant.
Answer Exercise 5.2. Note: Double-checking this answer, it turns out there are various
versions of the pizza ontology. The answer given here is based on the output from the
version I submitted in 2015; the output is available as “OOPS! - OntOlogy Pitfall Scanner!
- Results pizza.pdf” in the PizzaOOPS.zip ﬁle from the book’s website. The other two are:
“OOPS! - OntOlogy Pitfall Scanner! - ResultsPizzaLocal.pdf” based on pizza.owl from

281
Oct 2005 that was distributed with an old Prot´eg´e version and “OOPS! - OntOlogy Pitfall
Scanner! - ResultsPizzaProtegeSite.pdf”, who’s output is based on having given OOPS!
the URI https://protege.stanford.edu/ontologies/pizza/pizza.owl in July 2018.
There are 39 pitfalls detected and categorised as ‘minor’, and 4 as ‘important’. (ex-
plore the other pitfalls to see which ones are minor, important, and critical).
The three “unconnected ontology elements” are used as a way to group things, so are
not really unconnected, so that can stay.
ThinAndCripsyBase is detected as a “Merging diﬀerent concepts in the same class”
pitfall. Aside from the typo, one has to inspect the ontology to determine whether it
can do with an improvement: what are its sibling, parent and child classes, what is its
annotation? It is disjoint with DeepPanBase, but there is no other knowledge. It could
just as well have been named ThinBase, but the original class was likely not intended as
a real merging of classes, at least not like a class called, say, UndergradsAndPostgrads.
Then there are 31 missing annotations. Descriptions can be added to say what a
DeepPanBase is, but for the toppings this seems less obvious to add.
The four object properties missing domain and range axioms was a choice by the
modellers (see the tutorial) to not ‘overcomplicate’ the tutorial for novice modellers as
they can have ‘surprising’ deductions, but it would be better to add them where possible.
Last, OOPS detected that the same four properties are missing inverses. This cer-
tainly can be added for isIngredientOf and hasIngredient. That said, in OWL 2, one also
can use hasIngredient−to stand in for the notion of “isIngredientOf”, so missing inverses is
not necessarily a problem. (Ontologically, one easily could argue for ’non-directionality’,
but that is a separate line of debate; see e.g., [Fin00, KC16]).
Answer Exercise 5.3. No, A is unsatisﬁable. Reason: A ⊑ED (EnDurant), it has a
property R (to B), which has declared as domain PD (PerDurant), but ED ⊑¬PD (en-
durant and perdurant are disjoint), hence, A cannot have any instances.
Answer Exercise 5.4. There are several slides with the same ‘cleaning procedure’ and
one of them is uploaded on the book’s webpage, which was from the Doctorate course
on Formal Ontology for Knowledge Representation and Natural Language Processing
2004-2005, slide deck “Lesson3-OntoClean”. Meanwhile, there is also a related paper
that describes the steps in more detail, which appeared in the Handbook on Ontologies
[GW09].
Answers Chapter 6
Answer Review question 6.3. Some of the diﬀerences are: descriptive, possibilism,
and multiplicative for DOLCE versus prescriptive and realist, actualism, and reductionist
for BFO. You can ﬁnd more diﬀerences in Table 1 of [KK12] and online in the “compar-
ison tables” tab at http://www.thezfiles.co.za/ROMULUS/.
Answer Review question 6.4. There are several diﬀerences. The major diﬀerences
are that DOLCE also has relationships and axioms among the categories using those
relationships (i.e., richly formalised), whereas BFO v1 and v1.1 is a ‘bare’ taxonomy of
universals (some work exist on merging it with the RO, but not yet oﬃcially). Others are
the Abstract branch and the treatment of ‘attributes’/quality properties in DOLCE that
do not have an equivalent in BFO. The BFO-core has a more comprehensive inclusion of

282
Appendix E. Answers of selected exercises
parthood and boundaries than DOLCE.
Answer Review question 6.5. The most often recurring relationships are parthood,
participation, constitution, and inherence or dependence.
Answer Exercise 6.1. Informal alignments:
(a) dolce:Endurant maps roughly to bfo:Continuant (though actually, more precisely to
bfo:IndependentContinuant), dolce:Process as a sub-class of bfo:Process, and
dolce:quality to bfo:quality.
(b) Amount of Matter, Accomplishment, Agentive Physical Object, and Set do not
have a mapping. An example of the possible reasons: Set is abstract, but not
existing in nature (hence, by philosophical choice, not in BFO).
A more detailed comparison—or: the results of trying to align DOLCE, BFO, and GFO—
is available at http://www.thezfiles.co.za/ROMULUS/.
Answer Exercise 6.2. Options may vary:
(a) DOLCE or GFO
(b) BFO or GFO
(c) Depends on you chosen topic
Answer Exercise 6.4. The main ‘trick’ with such questions is to be able to detect
key words and phrases, such as the description stating that there will be “concrete en-
tities ... and ... abstract entities”: this data provide answers to one of the questions
in ONSET, and will aﬀect the choice of the foundational ontology (BFO does have ab-
stract entities, but GFO and DOLCE do), and likewise the sentence on mereology and
the text mentioning OWL 2 DL. Three use case with sample answers can be found at
http://www.meteck.org/files/onset/UseCasesExperiment.pdf.
Answer Exercise 6.6. I use the version with DOLCE in the following answers
(a) To have RockDassie classiﬁed as a subclass of Herbivore (still both animals, and
physical objects, and physical endurants, and endurants), it needs to have more, or
more constrained properties than Herbivore. In Prot´eg´e notation, each Herbivore
is equivalent to:
(eats only plant) or (eats only (is-part-of some plant)).
Rockdassies eat grasses and broad-leafed plants. The easiest way to modify the
ontology is to add that grasses are plants (already present), that broad-leafed plants
are kinds of plants, and that rockdassies eat only grass or broad-leafed plant. This
is not to say this is the best thing to do: there are probably also other animals that
eat grasses and broad-leafed plants, which now unintentionally will be classiﬁed as
rockdassies. This does not really need any of the foundational ontology content.
One could align the parthood relations.
(b) The ontology does not contain any knowledge on ‘residing in’ and ‘nature reserves’,
let alone sloppy word use of ‘found on’ (or, more precisely: in an area where a
university campus is located). Nature reserves are administrative entities, but also
can be considered only by their region-of-space aspect; for the sake of example,
let’s add NatureReserve ⊑space-region.
Trickier is the living, or living in: one
could add it as an OWL object property livesIn or as a subclass of Process and add
participation relations between that, the nature reserve, and the lions, impalas, and
monkeys. The former is less cumbersome, the latter more precise and interoperable.

283
(c) Ranger is a role that a human plays for some time, with Human being a physical
object, Ranger an agentive social object, and that the latter inheres in the former.
Answer Exercise 6.7. From the BFO viewpoint, the problem lies with the axioms that
have eats existentially quantiﬁed: e.g., Warthog ⊑∃eats.FruitingBody: some individual
warthog abc123 may never get around to actually eating any fruit, although the axiom
asserts that each warthog will have at least one instance of eating a fruit. Thus, that
axiom may not hold in all possible worlds after all and therefore ought not be in the
ontology, for it would not represent reality fully correctly, according to the realist dogma.
To resolve that, we ﬁrst need to state that warthogs have a disposition to eat:
Eating ⊑Process
EatingDisposition ≡Disposition ⊓∀hasRealization.Eating
Warthog ⊑∃bearerOf.EatingDisposition
Then we need to ﬁnd a way to relate it to FruitingBody, like berries and apples. The
Eating process may have as participant the fruiting body, but one cannot simply state
either of
Eating ⊑∃hasParticipant.FruitingBody
Eating ⊑∀hasParticipant.FruitingBody
because not all eating instances involve fruiting bodies and other food can be eaten as
well. So, we need another approach. The ﬁrst step is to constrain the participants in
the eating process to only those eating processes where warthogs are involved and where
fruiting bodies are involved:
Warthog ⊑∃bearerOf.(EatingDisposition ⊓
∀hasRealization.(Eating ⊓∀hasParticipant.FruitingBody)
or, more precisely: in place of the FruitingBody, to put all the things they’re eating
according to AWO v1: fruiting body, grass, root, or animal. They may not be all part
of the same eating event, but we will gloss over that detail for the moment. Obviously,
we’ll need to change the deﬁnitions for Omnivore and the other animals that were eating
something as well, for else we’ll lose the inference that Warthog ⊑Omnivore. That covers
it with respect to reﬁning the semantics for eats. There is also an eaten-by object property
that tasty-plant participates in:
tasty-plant ⊑∃eaten-by.carnivore ⊓∃eaten-by.herbivore
It is left as an exercise to the reader to update this axiom2.
Now, what to do if you want to assert that Warthog(j123) is eating Apple(apple1)?
In AWO v1, it is simply added with eats(j123,apple1), but we can’t do that anymore
now. First, we need to add ‘intermediate’ individuals: EatingDisposition(ed1) and Eat-
ing(e1) and then add assertions bearerOf(j123,ed1), hasRealization(ed1,e1), and hasPar-
ticipant(e1,apple1). Yet, upon running the reasoner, we still won’t observe that j123 is
eating apple1. In order to get some of the ‘cognitive simplicity’—our eats shortcut—
back, one could add a role chain:
bearerOf ◦hasRealization ◦hasParticipant ⊑eats
It will then derive eats(j123,apple1) and show that in the inferences.
This resultant ontology is available as AfricanWildlifeOntology3b.owl.
2also:
intuitively, this axiom ought to result in an inconsistent ontology when one adds
tasty-plant(p1). Why does that not happen? Hint: examine the deﬁnition of carnivore.

284
Appendix E. Answers of selected exercises
Answers Chapter 7
Answer Exercise 7.1. Phone points conceptual data model to ontology:
(a) A sample formalisation is available at the book’s resources page as phonepoints.owl.
(b) Yes, all of it it can be represented.
(c) Yes, there are problems. See Figure E.1 for a graphical rendering that MobileCall
and Cell are unsatisﬁable; verify this with your version of the ontology. Observe
that it also deduced that PhonePoint ≡LandLine.
Figure E.1: Answer to Exercise 7.1-c: red: inconsistent class, green: ‘positive’ deduction
Answer Exercise 7.2. Integration issues:
(a) See Figure E.2
(b) Multiple answers are possible due to various design decisions. E.g.,:
• Did you represent Salary as a class and invented a new object property to
relate it to the employees, or used it as a name for an OWL data property
(preferably the former)? And when a data property, did you use diﬀerent
data types (preferably not)?
• Did you add RichEmployee, or, better, Employee that has some property of
being rich?
• Did you use a foundational ontology, or at least make a distinction between
the role and its bearer (Employee and Person, respectively)?
Figure E.2: Answer to Exercise 7.2-a.
Answer Exercise 7.3. Thesauri:
(a) Language: SKOS or OWL 2 EL. Why:
• SKOS: was the purpose of it, to have a simple, but formal, language for
‘smooth transition’ and tagging along with the SW

285
• OWL 2 EL: intended for large, simple, type-level ontologies, and then still
some reasoning possible
(b) Regarding mass media, ﬁlms and news media: not necessarily, but to be certain,
check yourself what the deﬁnition of Mass Media is, when something can be called
News Media, and then assess the diﬀerences in their properties.
Propaganda has as broader term Information Dissemination, but a character-
istic of propaganda is dissemination of misinformation.
Answer Exercise 7.6. The description of the n-ary ODP can be found in the NeON
deliverable D2.5.1 on pp67-68. Also, you may wish to inspect the draft ODPs that have
been submitted to the ODP portal (at http://www.ontologydesignpatterns.org).
Answer Exercise 7.7. One could make a Content ODP out of it: for each AssistiveDe-
vice that is added to the ontology, one also has to record the Disability it ameliorates, it
requires some Ability to use/operate the device, and performs a certain Function. With
that combination, one even can create some sort of an ‘input form’ for domain experts
and administrators, which can then hide all the logic entirely, yet as long as they follow
the pattern, the information gets represented as intended.
Another one that may be useful is the Architectural OP: adolena.owl now contains
some bits and pieces of both DOLCE (endurant, perdurant, and some of their subclasses)
and some terms from BFO (realizable), neither of the two ontologies were imported. The
architectural ODP can help cleaning this us and structuring it.
Answer Exercise 7.8. One could check the design against a foundational ontology and
check whether the instantiations makes sense. There may be more options to evaluate
it, as there has not been done much work on ODP quality.
Answer Exercise 7.10. Principally:
- expressive foundational ontology, such as DOLCE or GFO for improved ontology
quality and interoperability
- bottom-up onto development from the thesaurus to OWL
- integration/import of existing bio-ontologies
- Domain ontology in OWL taking the classiﬁcation of the chemicals, deﬁne domain
& range and, ideally, deﬁned concepts
- Add the instance data (the representation of the chemicals in stock) in the OWL
ABox (there are only 100, so no real performance issues), and add a dummy class
disjoint from DruTopiate destined for the ‘wrong’ chemicals
- Take some suitable reasoner for OWL 2 DL (either the ‘standard’ reasoners or
names like Fact++)
- Then classify the instances availing of the available reasoning services (run Fact++
etc.): those chemical classiﬁed as instances of the ‘ideal chemical’ are the candidate
for the lab experiments for the drug to treat blood infections.
- Alternatively: add DruTopiate as class, add the other chemicals as classes, and any
classes subsumed by DruTopiate are the more likely chemicals, it’s parents the less
likely chemicals.
- Methods and methodologies that can be used: single, controlled ontology devel-
opment, so something like METHONTOLOGY will do, and for the micro-level
development something like OD101, ontospec, or DiDON.

286
Appendix E. Answers of selected exercises
Answers Chapter 8
Answer Exercise 8.1. Two of the reasons are:
• The ontology is speciﬁc to the application, hater than being application-independent.
If it wouldn’t be, then there will be mismatches (either too much/irrelevant content
in the ontology, or data that should be queried but can’t if there’s no corresponding
knowledge in the ontology)
• The ontology contains implementation decisions, such as data properties, which
will hamper any reuse, be that for another application or as a module of another
ontology.
Answers Chapter 9
Answer Exercise 9.1. There are no clear rules on IRI usage, but...
(a) The IRIs in AfricanWildlifeOntologyAF.owl are a mess.
It was translated
from an old AWO version that was once worked on in an old Prot´eg´e version
(v4.1) that automatically put the path to where the local copy was stored (file:
/Applications/Protege_4.1_beta/AfricanWildlifeOntology1.owl), and this
IRI was not updated in the ﬁle. The ontology IRI was updated, to http://www.
meteck.org/teaching/ontologies/AfricanWildlifeOntology1.owl, but this was
not propagated through to the IRIs of the classes and properties already declared
in the ontology. In addition, that IRI is the location of the AWO v1 with element
names in English, whereas this ﬁle has the element names in Afrikaans. New en-
tities were added afterward, such as Witwortel (parsnip), which somehow got an
xml:base, which is also incorrect.
(b) Recall that a full IRI is the oﬃcial address of the element (class or property).
While all 1:1 translated elements are semantically the same thing, the strings of
the full IRIs are not, yet no equivalence is asserted either. There are several possible
solutions to this, which relate to the architecture for maintenance (Exercise 9.4).
Arguably, neither option is a good option.
(c) There is no explicit link to the original AWO other than same IRI for the Spanish
one, and an annotation at best. What would be a way to make it clear(er) that
they are all related to AWO v1?
Answer Exercise 9.3. Possible templates are as follows, noting that one can choose
other words as well, and choose between being as close to the structure of the axiom, or
decide on a more ‘colloquial’ rendering
(a) “< C > and < D > are disjoint”
(b) “If there exists an outgoing arc from < R > to < C >, then it originates in
< D >”, or, easier to read: “< D > is the domain of < R > (when < R > relates
to < C >)”
(c) “Each < C > < R > only < D >”
Answer Exercise 9.4. There are clearly many possibilities. Some essential ingredients,
however, are: some place where language annotations can be stored, where some rules
for the sentence can be stored and used, templates or patterns to generate a sentence for
the axioms and/or parts thereof, and possibly algorithms to ﬁnalise the sentence, and
the ontology.

287
Answers Chapter 10
Answer Exercise 10.1. They do not fare well in case 2 (for ISs), at least not in the-
ory. First, because of the reliance on concrete domains. Second, the numbers you and
your classmate had chosen for ‘old’ was likely not the same—it certainly wasn’t for the
students in one of my classes and the cut-oﬀpoint I had in mind!—which then raises the
general question as to what to do with something like such a number diﬀerence when
faced with choosing to reuse an ontology and when aligning ontologies or integrating
system. Conversely, it thus may work well for a particular application scenario (case 1,
in ISs), assuming all its users agree on the fuzzy membership functions.
Answer Exercise 10.2. Chronos and PROTON are relatively easy to ﬁnd. Chronos
uses constraint satisfaction for the temporal component, PROTON is based on Prolog.
Chronos uses the 4d-ﬂuents approach (i.e.: perdudantist [recall Chapter 6]) and im-
plements a reasoner for the Allen relations. PROTON uses intervals and extends the
situation calculus.
Answer Exercise 10.3. No. (this is basically the same as the previous review ques-
tion). Consider also the scope, as state in the W3C standard, with emphasis added:
“OWL-Time is an OWL-2 DL ontology of temporal concepts, for describing the tempo-
ral properties of resources in the world or described in Web pages. The ontology provides
a vocabulary for expressing facts about topological (ordering) relations among instants
and intervals, together with information about durations, and about temporal position
including date-time information. Time positions and durations may be expressed using
either the conventional (Gregorian) calendar and clock, or using another temporal ref-
erence system such as Unix-time, geologic time, or diﬀerent calendars.”. That is: for
annotations, but not for reasoning with it.


About the author
Maria Keet (PhD, MSc, MA, BSc(hons)) is an Associate Professor at the De-
partment of Computer Science, University of Cape Town, South Africa. She has
taught multiple courses on ontology engineering at various universities yearly since
2009, as well as related courses, such as databases and theory of computation.
Her research focus is on knowledge engineering with ontologies and Ontology, and
their interaction with natural language and conceptual data modelling, which has
resulted in over 100 peer-reviewed publications at venues including KR, FOIS,
EKAW, K-CAP, ESWC, ER, CIKM, COLING, INLG, Applied Ontology, Data &
Knowledge Engineering, and the Journal of Web Semantics, including best paper
awards at EKAW’12, EKAW’18, and K-CAP’17. She is, and has been, a PI on NRF
and DST funded projects, and she was involved in several EU projects (TONES,
Net2, e-Lico). She is/was PC (co-)chair of EKAW’20, the ISWC’19 outrageous
ideas track, CNL’18, the ESWC’14 track on ontologies, OWLED’14, the ISAO’16
and AFIRM’20 summer schools, was local chair of FOIS’18 and ISAO’18, and has
served on many Program Committees of international workshops and conferences
and reviewed for journals.
Before her employment at UCT, Maria was a tenured Senior Lecturer at the
School of Mathematics, Statistics, and Computer Science at the University of
KwaZulu-Natal, South Africa and before that, an Assistant Professor at the KRDB
Research Centre, Free University of Bozen-Bolzano, Italy. She obtained a PhD in
Computer Science at the KRDB Research Centre in 2008, following a BSc(honours)
1st class in IT & Computing from the Open University UK in 2004, and 3.5 years
work experience as systems engineer in the IT industry. In addition to computer
science, she obtained an MSc in Food Science (Microbiology) from Wageningen
University and Research Centre, the Netherlands, in 1998, and an MA 1st class in
Peace & Development Studies from the University of Limerick, Ireland, in 2003.
289



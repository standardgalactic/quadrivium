1
Introduction to the Mathematical and
Statistical Foundations of Econometrics
Herman J. Bierens
Pennsylvania State University, USA,
and Tilburg University, the Netherlands

2
Contents
Preface 
Chapter 1:  
Probability and Measure  
1.1.
The Texas lotto
1.1.1 Introduction
1.1.2 Binomial numbers
1.1.3 Sample space
1.1.4 Algebras and sigma-algebras of events
1.1.5 Probability measure 
1.2.
Quality control
1.2.1 Sampling without replacement
1.2.2 Quality control in practice
1.2.3 Sampling with replacement
1.2.4 Limits of the hypergeometric and binomial probabilities 
1.3.
Why do we need sigma-algebras of events? 
1.4.
Properties of algebras and sigma-algebras
1.4.1 General properties
1.4.2 Borel sets 
1.5.
Properties of probability measures 
1.6.
The uniform probability measure
1.6.1 Introduction
1.6.2 Outer measure 
1.7.
Lebesgue measure and Lebesgue integral
1.7.1 Lebesgue measure
1.7.2 Lebesgue integral 
1.8.
Random variables and their distributions

3
1.8.1 Random variables and vectors
1.8.2 Distribution functions 
1.9.
Density functions 
1.10.
Conditional probability, Bayes' rule, and independence
1.10.1 Conditional probability
1.10.2 Bayes' rule
1.10.3 Independence 
1.11.
Exercises 
Appendices:
1.A. 
Common structure of the proofs of Theorems 6 and 10 
1.B.
Extension of an outer measure to a probability measure 
Chapter 2: 
Borel Measurability, Integration, and Mathematical Expectations 
2.1. 
Introduction 
2.2.
Borel measurability 
2.3.
Integrals of Borel measurable functions with respect to a probability measure 
2.4.
General measurability, and integrals of random variables with respect to probability
measures 
2.5.
Mathematical expectation 
2.6.
Some useful inequalities involving mathematical expectations
2.6.1 Chebishev's inequality
2.6.2 Holder's inequality
2.6.3 Liapounov's inequality
2.6.4 Minkowski's inequality
2.6.5 Jensen's inequality 
2.7.
Expectations of products of independent random variables 
2.8.
Moment generating functions and characteristic functions
2.8.1 Moment generating functions

4
2.8.2 Characteristic functions 
2.9.
Exercises 
Appendix:
2.A.
Uniqueness of characteristic functions 
Chapter 3: 
Conditional Expectations 
3.1.
Introduction 
3.2.
Properties of conditional expectations 
3.3.
Conditional probability measures and conditional independence
3.4.
Conditioning on increasing sigma-algebras 
3.5.
Conditional expectations as the best forecast schemes 
3.6.
Exercises 
Appendix: 
3.A.
Proof of Theorem 3.12 
Chapter 4: 
Distributions and Transformations 
4.1.
Discrete distributions
4.1.1 The hypergeometric distribution
4.1.2 The binomial distribution
4.1.3 The Poisson distribution
4.1.4 The negative binomial distribution 
4.2.
Transformations of discrete random vectors 
4.3.
Transformations of absolutely continuous random variables 
4.4.
Transformations of absolutely continuous random vectors
4.4.1 The linear case
4.4.2 The nonlinear case 
4.5.
The normal distribution

5
4.5.1 The standard normal distribution
4.5.2 The general normal distribution 
4.6.
Distributions related to the normal distribution
4.6.1 The chi-square distribution 
4.6.2 The Student t distribution
4.6.3 The standard Cauchy distribution
4.6.4 The F distribution 
4.7.
The uniform distribution and its relation to the standard normal distribution 
4.8.
The gamma distribution 
4.9.
Exercises 
Appendices: 
4.A: 
Tedious derivations 
4.B: 
Proof of Theorem 4.4 
Chapter 5:  
The Multivariate Normal Distribution and its Application to Statistical Inference 
5.1.
Expectation and variance of random vectors 
5.2.
The multivariate normal distribution 
5.3.
Conditional distributions of multivariate normal random variables 
5.4.
Independence of linear and quadratic transformations of multivariate normal
random variables 
5.5.
Distribution of quadratic forms of multivariate normal random variables 
5.6.
Applications to statistical inference under normality
5.6.1 Estimation
5.6.2 Confidence intervals
5.6.3 Testing parameter hypotheses 
5.7.
Applications to regression analysis
5.7.1 The linear regression model
5.7.2 Least squares estimation

6
5.7.3 Hypotheses testing 
5.8.
Exercises 
Appendix: 
5.A. 
Proof of Theorem 5.8 
Chapter 6: 
Modes of Convergence 
6.1.
Introduction 
6.2.
Convergence in probability and the weak law of large numbers 
6.3.
Almost sure convergence, and the strong law of large numbers 
6.4.
The uniform law of large numbers and its applications
6.4.1 The uniform weak law of large numbers
6.4.2 Applications of the uniform weak law of large numbers
6.4.2.1 Consistency of M-estimators
6.4.2.2 Generalized Slutsky's theorem
6.4.3 The uniform strong law of large numbers and its applications 
6.5.
Convergence in distribution 
6.6.
Convergence of characteristic functions 
6.7.
The central limit theorem 
6.8.
Stochastic boundedness, tightness, and the Op and op notations 
6.9.
Asymptotic normality of M-estimators 
6.10.
Hypotheses testing 
6.11.
Exercises 
Appendices: 
6.A.
 Proof of the uniform weak law of large numbers 
6.B.
Almost sure convergence and strong laws of large numbers 
6.C.
Convergence of characteristic functions and distributions 

7
Chapter 7: 
Dependent Laws of Large Numbers and Central Limit Theorems 
7.1.
Stationarity and the Wold decomposition 
7.2.
Weak laws of large numbers for stationary processes 
7.3.
Mixing conditions 
7.4.
Uniform weak laws of large numbers
7.4.1 Random functions depending on finite-dimensional random vectors
7.4.2 Random functions depending on infinite-dimensional random vectors
7.4.3 Consistency of M-estimators
7.5.
Dependent central limit theorems
7.5.1 Introduction
7.5.2 A generic central limit theorem
7.5.3 Martingale difference central limit theorems 
7.6.
Exercises 
Appendix: 
7.A.
Hilbert spaces 
Chapter 8: 
Maximum Likelihood Theory 
8.1. 
Introduction 
8.2.
Likelihood functions
8.3.
Examples
8.3.1 The uniform distribution
8.3.2 Linear regression with normal errors 
8.3.3 Probit and Logit models
8.3.4 The Tobit model 
8.4.
Asymptotic properties of ML estimators
8.4.1 Introduction
8.4.2 First and second-order conditions

8
8.4.3 Generic conditions for consistency and asymptotic normality
8.4.4 Asymptotic normality in the time series case
8.4.5 Asymptotic efficiency of the ML estimator 
8.5.
Testing parameter restrictions
8.5.1 The pseudo t test and the Wald test
8.5.2 The Likelihood Ratio test
8.5.3 The Lagrange Multiplier test
8.5.4 Which test to use? 
8.6.
Exercises 
Appendix I: 
Review of Linear Algebra 
I.1.
Vectors in a Euclidean space 
I.2.
Vector spaces 
I.3.
Matrices 
I.4.
The inverse and transpose of a matrix 
I.5.
Elementary matrices and permutation matrices 
I.6.
Gaussian elimination of a square matrix, and the Gauss-Jordan iteration for
inverting a matrix
I.6.1 Gaussian elimination of a square matrix
I.6.2 The Gauss-Jordan iteration for inverting a matrix 
I.7.
Gaussian elimination of a non-square matrix 
I.8.
Subspaces spanned by the columns and rows of a matrix 
I.9.
Projections, projection matrices, and idempotent matrices 
I.10.
Inner product, orthogonal bases, and orthogonal matrices 
I.11.
Determinants: Geometric interpretation and basic properties 
I.12.
Determinants of block-triangular matrices 
I.13.
Determinants and co-factors 
I.14.
Inverse of a matrix in terms of co-factors 

9
I.15.
Eigenvalues and eigenvectors
I.15.1 Eigenvalues
I.15.2 Eigenvectors
I.15.3 Eigenvalues and eigenvectors of symmetric matrices 
I.16.
Positive definite and semi-definite matrices 
I.17.
Generalized eigenvalues and eigenvectors 
I.18
Exercises 
Appendix II: 
Miscellaneous Mathematics 
II.1.
Sets and set operations
II.1.1 General set operations
II.1.2 Sets in Euclidean spaces 
II.2.
Supremum and infimum 
II.3.
Limsup and liminf 
II.4.
Continuity of concave and convex functions 
II.5.
Compactness 
II.6.
Uniform continuity 
II.7.
Derivatives of functions of vectors and matrices 
II.8.
The mean value theorem 
II.9.
Taylor's theorem 
II.10.
Optimization 
Appendix III: 
A Brief Review of Complex Analysis 
III.1.
The complex number system 
III.2.
The complex exponential function 
III.3.
The complex logarithm 
III.4.
Series expansion of the complex logarithm 

10
III.5.
Complex integration 
References 

11
Preface
This book is intended for a rigorous introductory Ph.D. level course in econometrics, or
for use in a field course in econometric theory. It is based on lecture notes that I have developed
during the period 1997-2003  for the first semester econometrics course “Introduction to
Econometrics” in the core of the Ph.D. program in economics at the Pennsylvania State
University. Initially these lecture notes were written as a companion to Gallant’s (1997)
textbook, but have been developed gradually into an alternative textbook. Therefore, the topics
that are covered in this book encompass those in Gallant’s book, but in much more depth.
Moreover, in order to make the book also suitable for a field course in econometric theory I have
included various advanced topics as well. I used to teach this advanced material in the
econometrics field at the Free University of Amsterdam and Southern Methodist University, on
the basis of the draft of my previous textbook, Bierens (1994). 
Some chapters have their own appendices, containing the more advanced topics and/or
difficult proofs. Moreover, there are three appendices with material that is supposed to be known,
but often is not, or not sufficiently. Appendix I contains a comprehensive review of linear
algebra, including all the proofs. This appendix is intended for self-study only, but may serve
well in a half-semester or one quarter course in linear algebra. Appendix II reviews a variety of
mathematical topics and concepts that are used throughout the main text, and Appendix III
reviews the basics of complex analysis which is needed to understand and derive the properties
of characteristic functions.
At the beginning of the first class I always tell my students: “Never ask me how. Only ask
me why.”  In other words, don’t be satisfied with recipes. Of course, this applies to other

12
economics fields as well, in particular if the mission of the Ph.D. program is to place its
graduates at research universities.  First, modern economics is highly mathematical. Therefore, in
order to be able to make original contributions to economic theory Ph.D. students need to
develop a “mathematical mind.” Second, students who are going to work in an applied
econometrics field like empirical IO or labor need to be able to read  the theoretical econometrics
literature in order to keep up-to-date with the latest econometric techniques. Needless to say,
students interested in contributing to econometric theory need to become professional
mathematicians and statisticians first. Therefore, in this book I focus on teaching “why,” by
providing proofs, or at least motivations if proofs are too complicated, of the mathematical and
statistical  results necessary for understanding modern econometric theory. 
Probability theory is a branch of measure theory. Therefore, probability theory is
introduced, in Chapter 1, in a measure-theoretical way. The same applies to unconditional and
conditional expectations in Chapters 2 and 3, which are introduced as integrals with respect to
probability measures. These chapters are also beneficial as preparation for the study of economic
theory, in particular modern macroeconomic theory. See for example Stokey, Lucas, and Prescott
(1989).  
It usually takes me three weeks (at a schedule of  two lectures of one hour and fifteen
minutes per week) to get through Chapter 1, skipping all the appendices. Chapters 2 and 3
together, without the appendices, usually take me about three weeks as well. 
Chapter 4 deals with transformations of random variables and vectors, and also lists the
most important univariate continuous distributions, together with their expectations, variances,
moment generating functions (if they exist), and characteristic functions. I usually explain only

13
the change-of variables formula for (joint) densities, leaving the rest of Chapter 4 for self-tuition.
The multivariate normal distribution is treated in detail in Chapter 5, far beyond the level
found in other econometrics textbooks. Statistical inference, i.e., estimation and hypotheses
testing, is also introduced in Chapter 5,  in the framework of the normal linear regression model.
At this point it is assumed that the students have a thorough understanding of linear algebra. 
This assumption, however, is often more fiction than fact. To tests this hypothesis, and to force
the students to refresh their linear algebra, I usually assign all the exercises in Appendix I as
homework before starting with Chapter 5. It takes me about three weeks to get through this
chapter.
Asymptotic theory for independent random variables and vectors, in particular the weak
and strong laws of large numbers and the central limit theorem, is discussed in Chapter 6,
together with various related convergence results. Moreover, the results in this chapter are
applied to M-estimators, including nonlinear regression estimators, as an introduction to
asymptotic inference. However, I have never been able to get beyond Chapter 6 in one semester,
even after skipping all the appendices and  Sections 6.4 and 6.9 which deals with asymptotic
inference.
Chapter 7 extends the weak law of large numbers and the central limit theorem to
stationary time series processes, starting from the Wold  (1938)  decomposition. In particular, the
martingale difference central limit theorem of McLeish (1974) is reviewed together with
preliminary results. 
Maximum likelihood theory is treated in Chapter 8. This chapter is different from the
standard treatment of maximum likelihood theory in that special attention is paid to the problem

14
of how to setup the likelihood function in the case that the distribution of the data is neither
absolutely continuous nor discrete. In this chapter only a few references to the results in Chapter
7 are made, in particular  in Section 8.4.4. Therefore, Chapter 7 is not prerequisite for  Chapter 8, 
provided that the asymptotic inference parts of Chapter 6  (Sections 6.4 and 6.9)  have been
covered. 
Finally, the helpful comments of five referees on the draft of this book, and the comments
of my colleague Joris Pinkse on Chapter 8, are gratefully acknowledged. My students have
pointed out many typos in earlier drafts, and their queries have led to substantial improvements
of the exposition. Of course, only I am responsible for any remaining errors. 

15
Chapter 1
Probability and Measure
1.1.
The Texas lotto
1.1.1
Introduction
Texans (used to) play the lotto by selecting six different numbers between 1 and 50,
which cost $1 for each combination1. Twice a week, on Wednesday and Saturday at 10:00 P.M.,
six ping-pong balls are released without replacement from a rotating plastic ball containing 50
ping-pong balls numbered 1 through 50. The winner of the jackpot (which occasionally
accumulates to 60 or more million dollars!) is the one who has all six drawn numbers correct,
where the order in which the numbers are drawn does not matter. What are the odds of winning if
you play one set of six numbers only?
In order to answer this question, suppose first that the order of  the numbers does matter.
Then the number of ordered sets of 6 out of 50 numbers is: 50 possibilities for the first drawn
number, times 49 possibilities for the second drawn number, times 48 possibilities for the third
drawn number, times 47 possibilities for the fourth drawn number, times 46 possibilities for the
fifth drawn number, times 45 possibilities for the sixth drawn number:
k
5
j'0
(50 & j) ' k
50
k'45
k '
k
50
k'1
k
k
50&6
k'1
k
'
50!
(50 & 6)! .
The notation n!, read: n factorial, stands for the product of the natural numbers 1 through n: 

16
n! ' 1×2×.......×(n&1)×n if n > 0,
0! ' 1.
The reason for defining  0! = 1 will be explained below.
Since a set of six given numbers can be permutated in 6! ways, we need to correct the
above number for the 6! replications of each unordered set of six given numbers. Therefore, the
number of sets of six unordered numbers out of 50 is:
50
6
'
def.
50!
6!(50&6)! ' 15,890,700.
Thus, the probability of winning  the Texas lotto if you play only one combination of six
numbers is 1/15,890,700. 2
1.1.2
Binomial numbers
In general, the number of ways we can draw a set of k unordered objects out of a set of n
objects without replacement is:
n
k
'
def.
n!
k!(n&k)!
.
(1.1)
These (binomial) numbers3, read as: n choose k,  also appear as coefficients in the binomial
expansion
(a % b)n ' j
n
k'0
n
k a kb n&k.
(1.2)
The  reason for defining 0! = 1 is now that the first and last coefficients in this binomial
expansion are always equal to 1: 

17
n
0
'
n
n
'
n!
0!n! ' 1
0! ' 1.
For not too large an n the binomial numbers (1.1) can be computed recursively by hand,
using the Triangle of Pascal: 
1
1
1
1
2
1
1
3
3
1
1
4
6
4
1
1
5
10
10
5
1
1
þ
þ
þ
þ
þ
1
(1.3)
Except for the 1's on the legs and top of the triangle in (1.3), the entries are the sums of the
adjacent numbers on the previous line, which is due to the easy equality:
n&1
k&1
%
n&1
k
'
n
k
for n $ 2, k ' 1,....,n&1.
(1.4)
Thus, the top 1 corresponds to n = 0,  the second row corresponds to n = 1, the third row
corresponds to n = 2, etc., and for each row n+1, the entries are the binomial numbers (1.1) for k
= 0,....,n. For example,  for n = 4  the coefficients of 
 in the binomial expansion (1.2) can
a kb n&k
be found on row 5 in (1.3):  (a % b)4 ' 1×a 4 % 4×a 3b % 6×a 2b 2 % 4×ab 3 % 1×b 4.

18
1.1.3
Sample space
The Texas lotto is an example of a statistical experiment. The set of possible outcomes of
this statistical experiment is called the sample space, and is usually denoted by  
 In the Texas
Ω.
lotto case  
 contains N  = 15,890,700 elements: 
 where each element 
 is
Ω
Ω' {ω1,.....,ωN},
ωj
a set itself consisting of  six different numbers ranging from 1 to 50, such that for any pair  
,
ωi
  with 
,  
 Since in this case the elements 
 of  
 are sets themselves, the
ωj
i … j
ωi … ωj.
ωj
Ω
condition  
 for 
  is equivalent to the condition that  
ωi … ωj
i … j
ωi _ ωj ó Ω.
1.1.4
Algebras and sigma-algebras of events
A set {
,....,
} of different number combinations  you can bet on is called an event.
ωj1
ωjk
The collection of all these events, denoted by 
,  is a “family” of subsets of the sample space
ö
.  In the Texas lotto case the collection 
  consists of all subsets of 
,  including 
 itself and
Ω
ö
Ω
Ω
the empty set 
.4  In principle you could bet on all number combinations if you are rich enough  
i
(it  will cost you $15,890,700). Therefore, the sample space  
  itself is included in 
.  You
Ω
ö
could also decide not to play at all. This event can be identified as the empty set 
 For the sake
i.
of completeness it is included in  
 as well.
ö
Since in the Texas lotto case the collection  
  contains all subsets of  
 it
ö
Ω,
automatically satisfies the following conditions:
If A 0 ö then ˜A ' Ω\A 0 ö,
(1.5)
where 
 is the complement of the set A (relative to the set 
),  i.e., the set of all elements 
˜A ' Ω\A
Ω
of 
 that are not contained in A;
Ω
If A,B 0 ö then A^B 0 ö.
(1.6)

19
By induction, the latter condition extends to any finite union of sets in 
:  If 
 for  j =
ö
Aj 0 ö
1,2,...,n, then 
 
^n
j'1Aj 0 ö.
Definition 1.1: A collection 
 of subsets of a  non-empty set 
 satisfying the conditions (1.5)
ö
Ω
and (1.6) is called an algebra.5 
In the Texas lotto example the sample space 
 is finite, and therefore the  collection 
Ω
ö
of subsets of  
  is finite as well. Consequently, in this case the condition  (1.6) extends to:
Ω
If Aj 0 ö for j ' 1,2,.... then ^ 4
j'1Aj 0 ö.
(1.7)
However, since in this case the collection  
 of subsets of 
  is finite, there are only a finite
ö
Ω
number of distinct sets 
.  Therefore, in the Texas lotto case the countable infinite  union 
Aj 0 ö
 in  (1.7)  involves only a finite number of distinct sets Aj; the other sets are replications of
^ 4
j'1Aj
these distinct sets. Thus, condition (1.7) does not require that all the sets  
 are different.
Aj 0 ö
Definition 1.2: A collection 
 of subsets of a  non-empty set 
 satisfying the conditions (1.5)
ö
Ω
and (1.7) is called a 
algebra.6
σ&&&&
1.1.5
Probability measure
Now let us return to the Texas lotto example. The odds, or probability,  of winning is 1/N 
for each valid  combination 
 of six numbers, hence if you play n different valid number
ωj
combinations 
 the probability of winning is n/N:  
 Thus, in
{ωj1,...,ωjn},
P({ωj1,...,ωjn}) ' n/N.
the Texas lotto case the probability 
 is given by the number n of elements in the
P(A), A 0 ö,

20
set A, divided by the total number N of elements in  
 In particular we have 
 and  if
Ω.
P(Ω) ' 1,
you do not play at all the probability of winning is zero:
 
P(i) ' 0.
The function 
 is called a probability measure: it assigns a number
P(A), A 0 ö,
 to each set 
 Not every function which assigns numbers in [0,1] to the sets
P(A) 0 [0,1]
A 0 ö.
in 
 is a probability measure, though: 
ö
Definition 1.3: A mapping 
  from a  
algebra  
  of subsets of a set  
 into the
P: ö 6 [0,1]
σ&
ö
Ω
unit interval  is a probability measure on {
, 
} if it satisfies the following three conditions:
Ωö
If A 0 ö then P(A) $ 0,
(1.8)
P(Ω) ' 1,
(1.9)
For disjoint sets Aj 0 ö, P(^ 4
j'1 Aj) ' '4
j'1P(Aj).
(1.10)
Recall that  sets are disjoint if they have no elements in common: their intersections are
the empty set.
The conditions (1.8) and (1.9) are clearly satisfied for the case of the Texas lotto.  On the
other hand, in the case under review the collection 
 of events contains only a finite number of
ö
sets, so that any  countably infinite sequence of sets in 
 must contain sets that are the same. At
ö
first sight this seems to conflict with the implicit assumption that there always exist countably
infinite sequences of disjoint sets for which  (1.10) holds. It is true indeed that any countably
infinite sequence of disjoint sets in a finite collection  
  of sets can only contain a finite
ö
number of  non-empty sets. This is no problem though, because all the other sets are then equal

21
to the empty set
 The empty set is disjoint with itself:
 and with any other set:
i.
i _ i ' i,
 Therefore, if 
 is finite then any countable infinite sequence of disjoint sets
A _ i ' i.
ö
consists of a finite number of  non-empty sets, and an infinite number of replications of the
empty set. Consequently,  if 
 is finite then  it is sufficient for the verification of condition 
ö
(1.10) to verify that for any pair of disjoint sets 
  in  
 
 = 
 + 
 
A1,A2
ö, P(A1^A2)
P(A1)
P(A2).
Since in the Texas lotto case 
 
 and 
 where 
P(A1^A2) ' (n1%n2)/N, P(A1) ' n1/N,
P(A2) ' n2/N,
 is the number of elements of 
 and  
 is the number of elements of 
, the latter condition
n1
A1
n2
A2
is satisfied, and so is condition  (1.10).  
The statistical experiment is now completely described by the triple  
 called
{Ω,ö,P},
the probability space, consisting of the sample space 
 i.e., the set of all possible outcomes of 
Ω,
the statistical experiment involved,  a 
algebra 
 of events, i.e., a collection of subsets of  the
σ&
ö
sample space 
 such that the conditions  (1.5) and (1.7) are satisfied,  and a probability measure
Ω
 satisfying the conditions (1.8), (1.9), and (1.10).
P: ö 6 [0,1]
In the Texas lotto case the  collection
 of events is an algebra, but because
 is finite it
ö
ö
is automatically a 
algebra.
σ&
1.2.
Quality control
1.2.1
Sampling without replacement
As a second example, consider the following case. Suppose you are in charge of quality
control in a light bulb factory. Each day N  light bulbs are produced. But before they are shipped
out to the retailers, the bulbs need to meet a minimum quality standard, say: no more than R out
of N  bulbs are allowed to be defective. The only way to verify this exactly is to try all the N 

22
bulbs out, but that will be too costly. Therefore, the way quality control is conducted in practice
is to draw randomly n bulbs without replacement, and to check how many bulbs in this sample
are defective. 
Similarly to the Texas lotto case, the number M of different samples 
 of size n  you can
sj
draw out of a set of N elements without replacement is:
M '
N
n .
Each sample  
 is characterized by a  number  
 of defective bulbs in the sample involved. Let
sj
kj
K  be the actual number of defective bulbs. Then kj 0 {0,1,...,min(n,K)}.
Let   
 and let the  
algebra 
 be the collection of all subsets of 
. 
Ω' {0,1,....,n},
σ&
ö
Ω
The number of samples 
  with  
 = 
  defective bulbs is:
sj
kj
k # min(n,K)
K
k
N&K
n&k
,
because there are ”K choose k “ ways to draw k unordered numbers out of K numbers without
replacement, and “N-K choose n-k” ways to draw n - k unordered numbers out of N - K numbers
without replacement. Of course, in the case that n > K the number of samples 
 with  
 =  k >
sj
kj
min (n,K) defective bulbs is zero.  Therefore, let: 
P({k}) '
K
k
N&K
n&k
N
n
if 0 # k # min(n,K),
P({k}) ' 0 elsewhere,
(1.11)
and let for each set  
 (Exercise: Verify that this
A ' {k1,......,km} 0 ö, P(A) ' 'm
j'1P({kj}).
function  P satisfies  all the requirements of a probability measure.) The triple
 is now
{Ω,ö,P}

23
the probability space corresponding to this statistical experiment. 
The probabilities (1.11) are known as the Hypergeometric(N,K,n) probabilities.
1.2.2
Quality control in practice7
The problem in applying this result in quality control is that K is unknown. Therefore, in
practice the following decision rule as to whether 
 or not is followed.  Given a particular
K # R
number 
 to be determined below, assume  that the set of N  bulbs meets the minimum
r # n,
quality requirement 
 if  the number k of defective bulbs in the sample is less or equal to .
K # R
r
Then the set 
corresponds to the assumption that  the set of N   bulbs meets the
A(r) ' {0,1,...,r}
minimum quality requirement  
, hereafter indicated by “accept”,  with probability
K # R
P(A(r)) ' ' r
k'0P({k}) ' pr(n,K),
(1.12)
say,  whereas its complement  
 corresponds to the assumption that this set of
˜A(r) ' {r%1,....,n}
N  bulbs does not meet this quality requirement, hereafter indicated by “reject”,  with
corresponding probability 
 
P( ˜A(r)) ' 1 & pr(n,K).
Given r, this decision rule yields two types of errors, a type I  error with probability 1 & pr(n,K)
if you reject while in reality 
, and a type II error with probability  
 if you accept 
K # R
pr(K,n)
while in reality 
.  The probability of a type I error  has upper bound:
K > R
p1(r,n) ' 1 & min
K#R
pr(n,K),
(1.13)
say, and the probability of a type II error  has upper bound
p2(r,n) ' max
K>R
pr(n,K),
(1.14)

24
say.
In order to be able to choose r, one has to restrict either 
 or 
, or both.
p1(r,n)
p2(r,n)
Usually it is former which is restricted, because a type I error may cause the whole stock of N 
bulbs to be trashed. Thus,  allow the probability of a type I error to be maximal α, say α =  0.05.
Then  r should be chosen such that 
 α. Since  
 is decreasing in r  because (1.12)
p1(r,n) #
p1(r,n)
is increasing in r,  we could in principle choose r arbitrarily large. But since  
 is increasing
p2(r,n)
in r,  we should not choose r unnecessarily large. Therefore, choose r = r(n|α), where  r(n|α) is
the minimum value of r for which p1(r,n) # α. Moreover, if we allow the type II error to be
maximal β,  we have to choose the sample size n such that  p2(r(n|α),n) # β. 
As we will see later, this decision rule is an example of a statistical test, where
 is called the null hypothesis to be tested at the α×100% significance level, against
H0: K # R
the alternative hypothesis 
. The number   r(n|α)  is called the critical value of the test,
H1: K > R
and the number k of defective bulbs in the sample is called the test statistic.
1.2.3
Sampling with replacement
As a third example, consider the quality control example in the previous section, except
that now the light bulbs are sampled with replacement: After testing a bulb, it is put back in the
stock of N bulbs, even if the bulb involved  proves to be defective. The rationale for this behavior
may be that the customers will accept maximally a fraction R/N of defective bulbs, so that they
will not complain as long as the actual fraction K/N of defective bulbs does not exceed R/N.  In
other words, why not selling defective light bulbs if it is OK with the customers?
The sample space 
 and the 
 algebra 
 are the same as in the case of sampling
Ω
σ&
ö

25
without replacement, but the probability measure P is different. Consider again a sample 
 of
sj
size n containing k defective light bulbs. Since the light bulbs are put back in the stock after
being tested,  there are  
 ways of drawing an ordered set of k defective bulbs, and 
K k
(N & K)n&k
ways of drawing an ordered set of n-k working bulbs. Thus the number of ways we can draw,
with replacement, an ordered set of n light bulbs containing k defective bulbs is  
. 
K k(N & K)n&k
Moreover, similarly to the Texas lotto case it follows that the number of unordered sets of  k
defective bulbs and n-k working bulbs is: n choose k.  Thus, the total number of ways we can
choose a sample with replacement containing k defective bulbs and n-k working bulbs in any
order is:
n
k
K k(N & K)n&k.
Moreover, the number of ways we can choose a sample of size n with replacement is  
.
N n
Therefore,
P({k}) '
n
k
K k(N & K)n&k
N n
'
n
k
p k(1 & p)n&k,
k ' 0,1,2,....,n,
where p ' K/N,
(1.15)
and again  for each set 
  Of course, replacing
A ' {k1,......,km} 0 ö, P(A) ' 'm
j'1P({kj}).
P({k}) in (1.11) by (1.15)  the argument in Section 1.2.2 still applies. 
The probabilities (1.15) are known as the Binomial(n,p) probabilities.

26
1.2.4
Limits of the hypergeometric and binomial probabilities
Note that if N and K are large relative to n, the hypergeometric probability (1.11) and the
binomial probability (1.15) will be almost the same. This follows from the fact that for fixed k
and n:
P({k}) '
K
k
N&K
n&k
N
n
'
K!(N&K)!
k!(K&k)!(n&k)!(N&K&n%k)!
N!
n!(N&n)!
'
n!
k!(n&k)!
×
K!(N&K)!
(K&k)!(N&K&n%k)!
N!
(N&n)!
'
n
k
×
K!
(K&k)!
×
(N&K)!
(N&K&n%k)!
N!
(N&n)!
'
n
k
×
(k
j'1(K&k%j) × (n&k
j'1(N&K&n%k%j)
(n
j'1(N&n%j)
'
n
k
×
(k
j'1
K
N
& k
N
% j
N
× (n&k
j'1 1& K
N
& n
N
% k
N
% j
N
(n
j'1 1& n
N
% j
N
6
n
k
p k(1&p)n&k if N 6 4 and K/N 6 p.
Thus, the binomial probabilities also arise as limits of the hypergeometric probabilities.
Moreover, if in the case of the binomial probability (1.15)  p is very small and n is very
large, the probability (1.15) can be approximated quite well by the Poisson(λ) probability:
P({k}) ' exp(&λ) λk
k! , k ' 0,1,2,..........,
(1.16)

27
where 
This follows from  (1.15) by choosing 
 with 
 fixed,
λ ' np.
p ' λ/n for n > λ,
λ > 0
and letting  
 while keeping  k fixed:
n 6 4
P({k}) '
n
k p k(1 & p)n&k '
n!
k!(n&k)! λ/n k 1 & λ/n n&k
' λk
k!×
n!
n k(n&k)!
× 1 & λ/n n
1 & λ/n k 6 exp(&λ) λk
k! for n 6 4,
because
n!
n k(n&k)!
'
(k
j'1(n&k%j)
n k
' (k
j'1 1& k
n
% j
n
6 (k
j'11 ' 1 for n 6 4,
1 & λ/n k 6 1 for n 6 4,
and
1 & λ/n n 6 exp(&λ) for n 6 4.
(1.17)
Since (1.16) is the limit of (1.15) for 
 the Poisson probabilities (1.16)
p ' λ/n 9 0 as n 6 4,
are often used to model the occurrence of rare events.
Note that the sample space corresponding to the Poisson probabilities is  Ω = {0,1,2,....}, 
and the 
algebra 
 of events involved can be chosen to be the collection of  all subsets of 
,
σ&
ö
Ω
because any non-empty subset A of  
 is either countable infinite or finite.  If such a subset  A is
Ω
countable infinite,  it takes the form  
 where the  kj’s are distinct
A ' {k1,k2,k3,..........},
nonnegative integers, hence 
 is well-defined. The same applies of course  if
P(A) ' '4
j'1P({kj})
A is finite: if  A = 
 then  
 This probability measure clearly
{k1,....,km}
P(A) ' 'm
j'1P({kj}).
satisfies the conditions (1.8), (1.9), and (1.10). 

28
1.3.
Why do we need sigma-algebras of events?
In principle we could define a probability measure on an algebra ö of subsets of the
sample space, rather than on a σ!algebra. We only need to change condition (1.10) to: For
disjoint sets 
 such that  
 By letting all but a finite
Aj 0 ö
^ 4
j'1 Aj 0 ö, P(^ 4
j'1 Aj) ' '4
j'1P(Aj).
number of these sets are equal to the empty set, this condition then reads: For disjoint sets
 j = 1,2,...,n < 4, 
 However, if we would confine a probability
Aj 0 ö,
P(^ n
j'1 Aj) ' 'n
j'1P(Aj).
measure to an algebra all kind of useful results will no longer apply. One of these results is the
so-called strong law of large numbers. See Chapter 6. 
As an example, consider the following game. Toss a fair coin infinitely many times, and
assume that after each tossing you will get one dollar if the outcome it head, and nothing if the
outcome is tail. The sample space Ω  in this case can be expressed in terms of the winnings, i.e.,
each element  ω of  Ω takes the form of a string of infinitely many zeros and ones, for example  ω
= (1,1,0,1,0,1......).  Now consider the event: “After n tosses the winning is k dollars”. This event
corresponds to the set Ak,n of elements  ω of  Ω for which the sum of the first n elements in the
string involved is equal to k. For example, the set A1,2 consists of all  ω of the type (1,0,......) and
(0,1,......).  Similarly to the example in Section 1.2.3 it can be shown that
P(Ak,n) '
n
k (1/2)n for k ' 0,1,2,....,n,
P(Ak,n) ' 0 for k > n or k < 0.
Next, for q = 1,2,.... consider the events: “After n  tosses the average winning k/n is contained in
the interval [0.5!1/q, 0.5+1/q]”. These events correspond to the sets Bq,n ' ^ [n/2%n/q]
k'[n/2&n/q)]%1Ak,n,
where [x] denotes the smallest integer $ x.  Then the set  
 corresponds to the event:
_4
m'nBq,m
“From the n-th tossing onwards the average winning will stay in the interval  [0.5!1/q, 0.5+1/q]”,

29
and the set 
 corresponds to the event: “There exists an n (possibly depending on ω)
^4
n'1_4
m'nBq,m
such that from the  n-th tossing onwards the average winning will stay in the interval  [0.5!1/q,
0.5+1/q]”. Finally, the set  
 corresponds to the event: “The average winning
_4
q'1^4
n'1_4
m'nBq,m
converges to ½ as n converges to infinity". Now the strong law of large numbers states that the
latter event has probability 1:  
 = 1. However, this probability is only defined
P[_4
q'1^4
n'1_4
m'nBq,m]
if  
 In order to guarantee this, we need to require that ö is a σ-algebra.
_4
q'1^4
n'1_4
m'nBq,m 0 ö.
1.4.
Properties of algebras and sigma-algebras
1.4.1
General properties
In this section I will review the most important results regarding algebras, 
algebras,
σ&
and probability measures.
Our first result is trivial:
Theorem 1.1: If an algebra contains only a finite number of sets then it is a σ-algebra.
Consequently, an algebra of subsets of a finite set  
 is  a 
algebra.
Ω
σ&
However,  an algebra of subsets of an infinite set 
  is not necessarily a 
algebra. A
Ω
σ&
counter example is the collection 
  of all subsets of   
= (0,1] of the type (a,b], where 
ö(
Ω
 are rational numbers in [0,1],  together with their finite unions and the empty set 
a < b
i.
Verify that 
  is an algebra.  Next, let  pn  = [10n π]/10n  and an = 1/ pn,  where [x] means
ö(
truncation to the nearest integer 
 Note that 
 as 
 Then for n =
# x.
pn 8 π, hence an 9 π&1
n 6 4.
1,2,3,....,  
 but  
 because 
 is irrational. Thus,  
 
(an,1] 0 ö(,
^4
n'1(an,1] ' (π&1,1] ó ö(
π&1
ö(

30
is not a  
algebra. 
σ&
Theorem 1.2: If 
 is an  algebra, then  
 hence by induction,
ö
A,B 0 ö implies A_B 0 ö,
 for j = 1,...,n < 4  imply  
 A collection   
 of subsets of a nonempty set 
Aj 0 ö
_n
j'1Aj 0 ö.
ö
Ω
is an algebra if it satisfies condition (1.5) and the condition that for any pair 
A,B 0 ö, A_B 0 ö.
Proof: Exercise.
Similarly, we have
Theorem 1.3: If 
 is a 
algebra,  then for any countable sequence of sets   
 
ö
σ&
Aj 0 ö,
 A collection   
 of   subsets of a nonempty set 
  is a  
algebra if  it  satisfies
_ 4
j'1Aj 0 ö.
ö
Ω
σ&
condition (1.5) and the condition that  for any countable sequence of sets   
  
 0 
Aj 0 ö, _ 4
j'1Aj
ö.
These results will be convenient  in cases where it is easier to prove that (countable) intersections
are included  in  
 than to prove that (countable) unions are included
ö
If  
 is already an algebra, then condition (1.7) alone would make it a 
algebra.
ö
σ&
However, the condition in the following theorem is easier to verify than  (1.7):
Theorem 1.4: If 
 is an algebra and Aj, j =1,2,3,...  is a countable sequence of sets in 
, then
ö
ö
there exists a countable sequence of disjoint sets Bj in  
 such that  
ö
^4
j'1Aj ' ^4
j'1Bj.

31
Consequently, an algebra  
  is also a  
 algebra if  for any sequence of disjoint sets Bj  in 
ö
σ&
ö,
^ 4
j'1Bj 0 ö.
Proof: Let 
 Denote   
 It follows
Aj 0 ö.
B1 ' A1, Bn%1 ' An%1\(^n
j'1Aj) ' An%1_(_n
j'1 ˜Aj).
from the properties of an algebra (see Theorem 1.2) that all  the Bj ‘s  are  sets in 
. Moreover, 
ö
it is easy to verify that the Bj‘s  are disjoint, and that  
 Thus, if  
 then 
^4
j'1Aj ' ^4
j'1Bj.
^4
j'1Bj 0 ö
 Q.E.D.
^4
j'1Aj 0 ö.
Theorem 1.5: Let 
 be a collection of  
algebras of subsets of a given set 
, 
öθ, θ 0 Θ,
σ&
Ω
where 
 is a possibly uncountable index set. Then 
  is a  
algebra.
Θ
ö ' _θ0Θöθ
σ&
Proof: Exercise.
For example, let  
 Then  
 =
öθ ' {(0,1],i,(0,θ],(θ,1]}, θ 0 Θ ' (0,1].
_θ0Θöθ
{(0,1],i} is a  
algebra (the trivial algebra).
σ&
 
Theorem 1.5 is important, because it guarantees that for any collection 
 of subsets of 
Œ
 there exists a smallest  
algebra containing 
. By adding complements and countable
Ω
σ&
Œ
unions it is possible to extend  
 to a   
algebra. This can always be done, because 
 is
Œ
σ&
Œ
contained in the   
algebra of all subsets of 
,  but there is often  no unique way of doing this,
σ&
Ω
except in the case where  
 is finite.  Thus, let 
 be the collection of all  
algebras 
Œ
öθ, θ 0 Θ,
σ&
containing 
. Then  ö =  
  is the smallest  
algebra containing
 
Œ
_θ0Θöθ
σ&
Œ.

32
Definition 1.4: The smallest 
algebra containing a given collection
 of sets is called the
σ&
Œ
algebra generated by 
 and is usually denoted by 
 
σ&
Œ,
σ(Œ).
Note that 
 is not always a 
algebra. For example, let 
 = [0,1],  and let
ö ' ^θ0Θöθ
σ&
Ω
for n 
 1,  
 Then  
$
ön ' {[0,1],i,[0,1&n &1],(1&n &1,1]}.
An ' [0,1&n &1] 0 ön d ^4
n'1ön,
but   the interval   [0,1) =  
 is not contained in any of the 
algebras 
, hence 
^4
n'1An
σ&
ön
^4
n'1An ó ^4
n'1ön.
However, it is always possible to extend 
 to a  
algebra, often in various ways,
^θ0Θöθ
σ&
by augmenting it with the missing sets. The smallest  
algebra containing  
 is usually
σ&
^θ0Θöθ
denoted by
ºθ0Θöθ '
def.
σ ^θ0Θöθ .
The notion of smallest σ-algebra of subsets of  Ω  is always relative to a given collection
 of subsets of  Ω. Without reference to such a given collection 
 the smallest  σ-algebra of
Œ
Œ
subsets of  Ω  is 
 which is called the trivial σ-algebra. 
{Ω,i},
Moreover, similarly to Definition 1.4 we can define the smallest algebra of subsets of   Ω
containing a given collection 
 of subsets of  Ω, which we will denote by 
 
Œ
α(Œ).
For example, let  Ω = (0,1], and let 
 be the collection of all intervals of the type (a,b]
Œ
with 
 Then 
consists of the sets in
 together with the empty set i, and all
0 # a < b # 1.
α(Œ)
Œ
finite unions of disjoint sets in 
 To see this, check first that this collection 
 is an algebra,
Œ.
α(Œ)
as follows.
(a)
 The complement of (a,b] in 
 is 
 If  a = 0  then 
 and if 
Œ
(0,a]^(b,1].
(0,a] ' (0,0] ' i,

33
b = 1 then 
 hence 
 is a set in
 or a finite union of disjoint sets in
(b,1] ' (1,1] ' i,
(0,a]^(b,1]
Œ
. 
Œ
(b)
Let  (a,b] in
 and  (c,d] in
, where without loss of generality we may assume that a #
Œ
Œ
c.  If b < c then  
 is a union of disjoint sets  in 
. If  c # b # d  then 
(a,b]^(c,d]
Œ
 is a set in
 itself, and if b > d  then 
 is a set in
 itself.
(a,b]^(c,d] ' (a,d]
Œ
(a,b]^(c,d] ' (a,b]
Œ
Thus, finite unions of sets in
 are either sets in
 itself or finite unions of disjoint sets in
.
Œ
Œ
Œ
(c)
Let 
 where 
 Then
A ' ^n
j'1(aj,bj],
0 # a1 < b1 < a2 < b2 <......< an < bn # 1.
 where 
 which is a finite union of disjoint sets in
˜A ' ^n
j'0(bj,aj%1],
b0 ' 0 and an%1 ' 1,
Œ
itself. Moreover, similarly to part (b)  it is easy to verify that finite unions of sets of the type A
can be written as finite unions of disjoint sets in 
.  
Œ
Thus, the sets in
 together with the empty set i and all finite unions of disjoint sets in
Œ
 form an algebra of subsets of   Ω = (0,1]. 
Œ
In order to verify that this is the smallest algebra containing 
, remove one of the sets in
Œ
this algebra that does not belong to
 itself. Since all sets in the algebra are of the type A in part
Œ
(c), let us remove this particular set A. But then 
 is no longer included in the collection,
^n
j'1(aj,bj]
hence we have to remove each of the intervals 
 as well, which however is not allowed
(aj,bj]
because they belong to 
 
Œ.
Note that the algebra  
 is not a σ-algebra, because countable infinite unions are not
α(Œ)
always included in 
. For example, 
 is a countable union of sets in 
α(Œ)
^4
n'1(0,1&n &1] ' (0,1)
 which  itself  is not included in 
.  However,  we can extend 
 to 
 the
α(Œ)
α(Œ)
α(Œ)
σ(α(Œ)),
smallest  σ-algebra containing 
,  which coincides with 
α(Œ)
σ(Œ).

34
1.4.2
Borel sets
An important special case of Definition 1.4 is  where 
 and 
 is the collection of
Ω' ú,
Œ
all open intervals:
Œ ' {(a,b): œ a < b, a,b 0 ú}.
(1.18)
Definition 1.5: The 
algebra generated by the collection (1.18) of all open intervals in 
 is
σ&
ú
called the Euclidean Borel  field, denoted by B,  and its members are called the Borel sets. 
Note, however, that  B  can be defined in different ways, because  the  
algebras generated by
σ&
the collections of open intervals, closed intervals: 
 and  half-open
{[a,b]: œ a # b, a,b 0 ú},
intervals,  
 respectively,  are all the same! We show this for one case only:
{(&4,a]: œ a 0 ú},
Theorem 1.6:   B  = σ({(&4,a]: œ a 0 ú}).
Proof:  Let
Œ( ' {(&4,a]: œ a 0 ú}.
(1.19)
(a) 
If the collection 
 defined by (1.18) is contained in 
, then 
 is a  
algebra
Œ
σ(Œ()
σ(Œ()
σ&
containing 
.  But  B = 
 is the smallest 
algebra containing 
, hence  B = 
Œ
σ(Œ)
σ&
Œ
σ(Œ) d
 
σ(Œ().
In order to prove this, construct an arbitrary set (a,b) in  
 out of countable unions and/or
Œ
complements of sets in 
, as follows.  Let  
 and 
, where a < b are
Œ(
A ' (&4,a]
B ' (&4,b]

35
arbitrary real numbers. Then 
, hence 
   and thus
A,B 0 Œ(
A, ˜B 0 σ(Œ(),
 
~(a,b] ' (&4,a]^(b,4) ' A^ ˜B 0 σ(Œ().
This implies that  
 contains all sets of the type (a,b] , hence (a,b) =  
σ(Œ()
^4
n'1(a,b & (b&a)/n]
 Thus,  
0 σ(Œ().
Œ d σ(Œ().
(b)  
If the collection 
 defined by (1.19) is contained in  B = 
, then 
 is a 
Œ(
σ(Œ)
σ(Œ)
algebra containing 
.  But 
 is the smallest 
algebra containing 
,  hence
σ&
Œ(
σ(Œ()
σ&
Œ(
 =  B. 
σ(Œ() d σ(Œ)
In order to prove the latter, observe that for m = 1,2,....,  
 is a
Am ' ^4
n'1(a&n,a%m &1)
countable union of sets in 
, hence 
 and consequently 
 =
Œ
˜Am 0 σ(Œ),
(&4,a] ' _4
m'1Am
 
 Thus,   
 =  B.
~(^4
m'1 ˜Am) 0 σ(Œ).
Œ( d σ(Œ)
We have shown now that  B =
 and  
 =  B.  Thus,  B  and
σ(Œ) d σ(Œ()
σ(Œ() d σ(Œ)
 are the same. Q.E.D.8 
σ(Œ()
The notion of Borel set extends to higher dimensions as well:
Definition 1.6:  Bk  = 
 is the k-dimensional Euclidean
σ({×k
j'1(aj,bj): œ aj < bj, aj, bj 0 ú})
Borel field.  Its members are also called Borel sets (in 
). 
úk
Also this is only one of the ways to define higher-dimensional Borel sets. In particular,
similarly to Theorem 1.6 we have: 
Theorem 1.7:   Bk    = σ({×k
j'1(&4,aj]: œ aj 0 ú}).

36
 
1.5.
Properties of probability measures
The three axioms (1.8), (1.9), and (1.10)  imply a variety of properties of probability
measures. Here we list only the most important ones.
Theorem 1.8: Let 
be a probability space. The following hold for sets in 
:
{Ω,ö,P}
ö
(a) P(i) ' 0,
(b) P( ˜A) ' 1 & P(A),
(c) A d B implies P(A) # P(B),
(d) P(A^B) % P(A_B) ' P(A) % P(B),
(e) If An d An%1 for n ' 1,2,..., then P(An) 8 P(^4
n'1An),
(f) If An e An%1 for n ' 1,2,..., then P(An) 9 P(_4
n'1An),
(g) P(^4
n'1An) # '4
n'1P(An).
Proof: (a)-(c): Easy exercises.
 is a union of disjoint sets, hence by axiom (1.10), 
(d) A^B ' (A_ ˜B) ^ (A_B) ^ (B_ ˜A)
  =  
Moreover, 
 is a union of
P(A^B)
P(A_ ˜B) % P(A_B) % P(B_ ˜A).
A ' (A_ ˜B) ^ (A_B)
disjoint sets , hence  
 and similarly, 
P(A) ' P(A_ ˜B) % P(A_B),
P(B) ' P(B_ ˜A) % P(A_B).
Combining these results, part (d) follows.
Let 
 Then  
 
(e)
B1 ' A1, Bn ' An\An&1 for n $ 2.
An ' ^n
j'1Aj ' ^n
j'1Bj and ^4
j'1Aj ' ^4
j'1Bj.
Since the 
‘s are disjoint, it follows from  axiom (1.10) that 
Bj

37
P(^4
j'1Aj) ' '4
j'1P(Bj) ' 'n
j'1P(Bj) % '4
j'n%1P(Bj) ' P(An) % '4
j'n%1P(Bj).
Part 
 follows now from the fact that  
(e)
'4
j'n%1P(Bj) 9 0.
This part follows from part 
, using complements. 
(f)
(e)
(g)  Exercise
1.6.
The uniform probability measure
1.6.1
Introduction
Fill a bowl with ten balls numbered from zero to nine. Draw randomly a ball from this
bowl, and write down the corresponding number as the first decimal digit of a number between
zero and one. For example, if the first drawn number  is 4, then write down 0.4. Put the ball back
in the bowl, and repeat this experiment. If for example the second ball corresponds to the number
9, then this number becomes the second decimal digit: 0.49. Repeating this experiment infinitely
many times yields a random number between zero and one. Clearly, the sample space involved is
the unit interval: Ω' [0,1].
For a given number 
 the probability that  this  random number is less or equal to
x 0 [0,1]
x is: x. To see this, suppose that you only draw two balls, and that x = 0.58. If the first ball has a
number less than 5, it does not matter what the second number is. There are 5 ways to draw a
first number less or equal to 4, and 10 ways to draw the second number. Thus, there are 50 ways
to draw a number with a first digit less or equal to 4.  There is only one way to draw a first
number equal to 5, and 9 ways to draw a second number less or equal to 8. Thus, the total
number of ways we can generate a number less or equal to 0.58 is 59, and the total number of
ways we can draw two numbers with replacement is 100. Therefore, if  we only draw two balls

38
with replacement, and use the numbers involved as the first and second decimal digit, the
probability that we get a number less or equal to 0.58 is: 0.59. Similarly, if we draw 10 balls with
replacement, the probability that we get a number less or equal to, say,  0.5831420385 is: 
0.5831420386. In the limit the  difference between x and the corresponding probability
disappears. Thus, for 
 we have:  
 By  the same argument  it follows that
x 0 [0,1]
P([0,x]) ' x.
for  
  
 i.e., the probability that the random number involved
x 0 [0,1], P({x}) ' P([x,x]) ' 0,
will be exactly equal to a given number  x is zero. Therefore,  for a given  
  
 =
x 0 [0,1], P((0,x])
 =  
 More generally, for any interval in [0,1]  the corresponding probability
P([0,x))
P((0,x)) ' x.
is the length of the interval involved,  regardless as to whether the endpoints are included or not:
Thus, for 
 we have  
 = b!a. Any
0 # a < b # 1
P([a,b]) ' P((a,b]) ' P([a,b)) ' P((a,b))
finite union of intervals can be written as a finite union of disjoint intervals by cutting out the
overlap. Therefore,  this probability measure extends to finite unions of intervals, simply by
adding up the lengths of the disjoint intervals involved. Moreover, observe that the collection of
all finite unions of sub-intervals in [0,1], including [0,1] itself and the empty set, is closed under
the formation of complements and finite unions. Thus, we have derived the probability measure
P  corresponding to the statistical experiment under review for an algebra 
 of subsets of 
ö0
[0,1], namely
ö0 ' {(a,b),[a,b],(a,b],[a,b), œa,b0[0,1], a#b, and their finite unions},
(1.20)
where [a,a] is the singleton {a}, and each of the sets (a,a), (a,a] and [a,a) should be interpreted
as the empty set 
This probability measure is a special case of the Lebesgue measure, which
i.
assigns to each interval its length.  
If you are only interested in making probability statements about the sets in the algebra

39
(1.20), then your are done. However, although the algebra (1.20) contains a large number of sets,
we cannot yet make probability statements involving arbitrary Borel sets in [0,1],  because not all
the Borel sets in [0,1] are included  in (1.20).  In particular, for a countable sequence of  sets
 the probability 
 is not always defined, because there is no guarantee that 
Aj 0 ö0
P(^4
j'1Aj)
 Therefore, if you want to make probability statements about arbitrary Borel set in
^4
j'1Aj 0 ö0.
[0,1], you need to extend the probability measure P on 
 to a probability measure defined on
ö0
the Borel sets in [0,1]. The standard approach to do this is to use the outer measure:
1.6.2
Outer measure
Any subset A of  [0,1] can always be completely covered by a finite or countably infinite 
union of sets in the algebra 
: 
 
,  hence the “probability”  of  A is
ö0 A d ^4
j'1Aj, where Aj 0 ö0
bounded from above by  
.  Taking the infimum of 
 over all countable
'4
j'1P(Aj)
'4
j'1P(Aj)
sequences of sets  
 such that  
 then yields the outer measure:
Aj 0 ö0
A d ^4
j'1Aj
Definition 1.7: Let
 be an algebra of subsets of  
The outer measure of an arbitrary subset
ö0
Ω.
A of  
 is:
Ω
P ((A) '
inf
Ad^4
j'1Aj, Aj0ö0
'4
j'1P(Aj).
(1.21)
Note that it is not required in (1.21) that  ^4
j'1Aj 0 ö0.
Since a union of sets Aj in an algebra 
 can always be written as a union of disjoint sets
ö0
in the algebra  algebra 
 (see Theorem 1.4),  we may without loss of generality assume that the
ö0

40
infimum in (1.21) is taken over all disjoint sets Aj in 
 such that  such that  
 This
ö0
A d ^4
j'1Aj.
implies that
If A 0 ö0 then P ((A) ' P(A).
(1.22)
The question now arises for which other subsets of  
  the outer measure is a probability
Ω
measure. Note that the conditions (1.8)  and (1.9)  are satisfied for the outer measure P (
(Exercise: Why?),  but in general condition (1.10) does not hold for arbitrary sets. See for
example Royden  (1968, pp. 63-64).  Nevertheless, it is possible to extend the outer measure to a
probability measure on a σ-algebra 
 containing 
:
ö
ö0
Theorem 1.9: Let P be a probability measure on  {
}, where 
 is an algebra, and let 
Ω, ö0
ö0
 be the smallest 
algebra  containing the algebra 
.  Then the outer measure 
ö ' σ(ö0)
σ&
ö0
P*   is a unique probability measure on {
} which coincides with P on 
.
Ω, ö
ö0
The proof that the outer measure P* is a probability measure on 
 which
ö ' σ(ö0)
coincide with P on 
 is lengthy and therefore given in Appendix B. The proof of the
ö0
uniqueness of  P* is even more longer and is therefore omitted.
Consequently,  for the statistical experiment under review there exists a  
algebra 
 of
σ&
ö
subsets of 
, containing the algebra 
 defined in (1.20),   for which the outer measure 
Ω' [0,1]
ö0
 is a unique probability measure.  This probability measure assigns in this case to
P (: ö 6 [0,1]
each interval in [0,1] its  length as probability. It is called the uniform probability measure.
It is not hard to verify that the 
algebra 
  involved  contains all the Borel subsets of
σ&
ö
[0,1]:  
  (Exercise: Why?)  This collection of Borel
{[0,1]_B, for all Borel sets B} d ö.

41
subsets of [0,1] is usually denoted by [0,1]
,  and is a  
algebra itself (Exercise: Why?). 
_ B
σ&
Therefore, we could also describe the probability space of this statistical experiment by the
probability space {[0,1],  [0,1]
 B, 
}, where 
 is the same as before. Moreover, defining
_
P (
P (
the probability measure 
 on  B  as  
  we can  describe this statistical
µ
µ(B) ' P (([0,1]_B),
experiment also by the probability space {
 B,
}, where in particular
ú,
µ
µ((&4,x]) ' 0 if x # 0, µ((&4,x]) ' x if 0 < x # 1, µ((&4,x]) ' 1 if x > 1,
and more generally for intervals with endpoints a < b, 
µ((a,b)) ' µ([a,b]) ' µ([a,b)) ' µ((a,b]) ' µ((&4,b]) & µ((&4,a]),
whereas for all other Borel sets B,
µ(B) '
inf
B d ^4
j'1(aj,bj)
'4
j'1µ((aj,bj)).
(1.23)
1.7.
Lebesgue measure and Lebesgue integral
1.7.1
Lebesgue measure
Along similar lines as in the construction of the uniform probability measure we can
define the Lebesgue measure, as follows. Consider a function λ which assigns to each open
interval (a,b)  its length: 
 and define for all other Borel sets B in ú,
λ((a,b)) ' b & a,
λ(B) '
inf
B d ^4
j'1(aj,bj)
'4
j'1λ((aj,bj)) '
inf
B d ^4
j'1(aj,bj)
'4
j'1(bj & aj).
This function λ is called the Lebesgue measure on ú, which measures the total “length” of a
Borel set, where the measurement is taken from the outside.
Similarly, let now  
 and define for all other Borel sets B in
λ(×k
i'1(ai,bi)) ' (k
i'1(bi &ai),
úk,

42
λ(B) '
inf
B d ^4
j'1{×k
i'1(ai,j,bi,j)}
'4
j'1λ ×k
i'1(ai,j,bi,j) '
inf
B d ^4
j'1{×k
i'1(ai,j,bi,j)}
'4
j'1 (k
i'1(bi,j & ai,j) .
This is the Lebesgue measure on úk, which measures the area (in the case k = 2) or the volume
(in the case k $ 3) of a Borel set in  úk, where again the measurement is taken from the outside.
Note that in general Lebesgue measures are not probability measures, because the
Lebesgue measure can be infinite. In particular,   λ( úk)  =  4. However, if confined to a set with
Lebesgue measure 1 it becomes the uniform probability measure. More generally,  for any Borel
set  A  0 úk with positive and finite Lebesgue measure, 
is the uniform
µ(B) ' λ(A_B)/λ(A)
probability measure on 
k
A.
B _
1.7.2
Lebesgue integral
The Lebesgue measure gives rise to a generalization of the Riemann integral. Recall that
the Riemann integral of a non-negative function f(x) over a finite interval (a,b] is defined as 
m
b
a
f(x)dx ' supj
n
m'1
inf
x0Im
f(x) λ(Im)
where the Im are intervals forming a finite partition of  (a,b] , i.e., they are disjoint, and their
union is  (a,b]:  
 8(Im ) is the length of  Im , hence 8(Im ) is the Lebesgue measure
(a,b] ' ^n
m'1Im,
of  Im , and the supremum is taken over all finite partitions of (a,b].  Mimicking this definition,
the Lebesgue integral of a non-negative function f(x) over a Borel set A can be defined as
mAf(x)dx ' supj
n
m'1
inf
x0Bm
f(x) λ(Bm),

43
where now the Bm ‘s are Borel sets forming a finite partition of A, and the supremum is taken
over all such partitions.
If the function   f(x) is not non-negative, we can always write it as the difference of two
non-negative functions: 
 where 
f(x) ' f%(x) & f&(x),
f%(x) ' max[0,f(x)], f&(x) ' max[0,&f(x)].
Then the Lebesgue integral over a Borel set A is defined as 
mAf(x)dx ' mA f%(x)dx & mA f&(x)dx,
provided that at least one of the right hand side integrals is finite. 
However, we still need to impose a further condition on the function f  in order to be
Lebesgue integrable. A sufficient condition is that for each Borel set B in ú, the set {x: f(x) 0 B}
is a Borel set itself. As we will see in the next chapter, this is the condition for Borel
measurability of f.
Finally, note that if A is an interval and  f(x) is Riemann integrable over A, then the
Riemann integral and the Lebesgue integral coincide.
1.8.
Random  variables and their distributions
1.8.1
Random variables and vectors
Loosely speaking, a random  variable is a  numerical translation of the outcomes of a
statistical experiment. For example, flip a fair coin once. Then the sample space is  Ω' {H,T},
where H stands for Head, and T stands for Tail. The 
algebra involved  is ö = {Ω,i,{H},{T}},
σ&
and the corresponding probability measure is defined by 
 Now
P({H}) ' P({T}}) ' 1/2.

44
define the function  
 if  
  
 if  
 Then X  is a random variable
X(ω) ' 1
ω ' H, X(ω) ' 0
ω ' T.
which takes the value 1 with probability ½ and the value 0 with probability ½:
 
P(X ' 1)
'
(short&hand notation)
P({ω0Ω: X(ω) ' 1}) ' P({H}) ' 1/2,
P(X ' 0)
'
(short&hand notation)
P({ω0Ω: X(ω) ' 0}) ' P({T}) ' 1/2.
Moreover, for an arbitrary Borel set B we have 
 
P(X 0 B) ' P({ω0Ω: X(ω) 0 B})
' P({H})
' 1/2 if 1 0 B and 0 ó B,
' P({T})
' 1/2 if 1 ó B and 0 0 B,
' P({H,T}) '
1
if 1 0 B and 0 0 B,
' P(i)
'
0
if 1 ó B and 0 ó B,
where again 
 is a short-hand notation9 for 
P(X 0 B)
P({ω0Ω: X(ω) 0 B}).
In this particular case the set 
 is  automatically equal to one of the
{ω0Ω: X(ω) 0 B}
elements of 
, and therefore the probability 
 =  P(
)  is well-
ö
P(X 0 B)
{ω0Ω: X(ω) 0 B}
defined. In general, however, we need to confine the mappings 
 to those for which we
X: Ω6 ú
can  make probability statements about events of the type
, where B is an
{ω0Ω: X(ω) 0 B}
arbitrary Borel set, which is only possible if these sets are members of 
:
ö
Definition 1. 8: Let 
 be a probability space. A mapping  
 is called a
{Ω,ö,P}
X: Ω6 ú
random variable defined on
 if X  is measurable 
 which means that for every Borel
{Ω,ö,P}
ö,
set B, 
 Similarly, a mapping  
  is called a k-dimensional 
{ω0Ω: X(ω) 0 B} 0 ö.
X: Ω6 úk
random vector defined on
 if  X  is measurable 
  in the sense that for every Borel
{Ω,ö,P}
ö,
set B  in 
 k,  
B
{ω0Ω: X(ω) 0 B} 0 ö.

45
In verifying that a real function
 is  measurable
 it is not necessary to verify
X: Ω6 ú
ö,
that for all Borel sets B,    
, but only that this property holds for Borel
{ω0Ω: X(ω) 0 B} 0 ö
sets of the type (&4,x]:
Theorem 1.10: A mapping  
 is measurable
 (hence X is a random variable)  if and
X: Ω6 ú
ö
only if  for all 
 the sets  
are members of 
 Similarly, a mapping 
x 0 ú
{ω0Ω: X(ω) # x}
ö.
 is measurable
 (hence X is a random vector of dimension k) if and only if  for all
X: Ω6 úk
ö
 the sets 
x ' (x1,.....,xk)T 0 úk
 
_k
j'1{ω0Ω: Xj(ω) # xj} ' {ω0Ω: X(ω) 0 ×k
j'1(&4,xj]}
are members of 
 where the Xj’s are the components of X. 
ö,
Proof: Consider the case k = 1. Suppose that
 Let 
{ω0Ω: X(ω) 0 (&4,x]} 0 ö, œx 0 ú.
D be the collection of all Borel sets B for which 
. Then D
 B, and
{ω0Ω: X(ω) 0 B} 0 ö
d
 contains the collection of half-open intervals 
 If  D  is a 
algebra itself, it
D
(&4,x], x 0 ú.
σ&
is a 
algebra containing the half-open intervals. But 
 is the smallest  
algebra containing
σ&
B
σ&
the half-open intervals  (see Theorem 1.6), so that then  B
 D, hence D
 B. Therefore, it
d
'
suffices to prove that D is a  
algebra: 
σ&
(a) 
Let 
D. Then 
, hence 
B 0
{ω0Ω: X(ω) 0 B} 0 ö
 
~{ω0Ω: X(ω) 0 B} ' {ω0Ω: X(ω) 0 ˜B} 0 ö
and thus  
D.  
˜B 0
(b)
Next, let   
  for j = 1,2,.... Then 
, hence
Bj 0 D
{ω0Ω: X(ω) 0 Bj} 0 ö

46
^4
j'1{ω0Ω: X(ω) 0 Bj} ' {ω0Ω: X(ω) 0 ^4
j'1Bj} 0 ö
and thus 
D. 
^4
j'1Bj 0
The proof of the case k > 1 is similar. Q.E.D.10
The sets 
are usually denoted by 
{ω0Ω: X(ω) 0 B}
X &1(B):
X &1(B) '
def.
{ω 0 Ω: X(ω) 0 B}.
The collection 
 B} is  a 
algebra itself (Exercise: Why?), and
öX ' {X &1(B), œB 0
σ&
is called the 
algebra generated by the random variable X.  More generally:
σ&
Definition 1.9: Let X be a random variable (k=1) or a random vector (k > 1). The  
algebra
σ&
 =  
 B k} is called the  
algebra generated by X.
öX
{X &1(B), œB 0
σ&
In the coin tossing case,  the mapping X  is one-to-one, and therefore in that case 
  is the same
öX
as 
 but in general 
  will be smaller than 
 For example, roll a dice, and let X = 1  if the
ö,
öX
ö.
outcome is even, and X = 0 if the outcome is odd. Then
 
öX ' {{1,2,3,4,5,6} , {2,4,6} , {1,3,5} , i},
whereas 
 in this case consists of all subsets of 
.
ö
Ω' {1,2,3,4,5,6}
Given a k dimensional random vector X, or a random variable X  (the case k=1), define for
arbitrary Borel sets 
  Bk :
B 0
µX(B) ' P X &1(B) ' P {ω0Ω: X(ω) 0 B} .
(1.24)
Then 
 is a probability measure on {
 Bk }:
µX(@)
úk,

47
(a)
for all 
 Bk,  
B 0
µX(B) $ 0,
(b)
µX(úk) ' 1,
(c)
for all disjoint 
 B k ,  
j 0
µX ^4
j'1Bj
' '4
j'1µX(Bj).
Thus, the random variable X  maps the probability space 
into a new probability
{Ω,ö,P}
space, {
 B, 
}, which in its turn is mapped back by  
 into the (possibly smaller)
ú,
µX
X &1
probability space 
. Similarly for random vectors.
{Ω,öX,P}
Definition 1.10: The probability measure  
 defined by (1.24) is called the probability
µX(@)
measure induced by X. 
1.8.2
Distribution functions
For Borel sets of the type 
, or
 in the multivariate case,  the value of the
(&4,x]
×k
j'1(&4,xj]
induced probability measure 
 is called the distribution function:
µX
Definition 1.11: Let X  be a random  variable (k=1) or a random vector ( k>1)   with  induced 
probability measure  
 . The  function  
 
 
is
µX
F(x) ' µX(×k
j'1(&4,xj]), x ' (x1,....,xk)T 0 úk,
called the  distribution function  of  X. 
It follows from these definitions, and Theorem 1.8 that 
Theorem 1.11: A distribution function of a random variable is always right continuous:

48
 and monotonic non-decreasing: 
œx 0 ú, limδ90F(x % δ) ' F(x),
F(x1) # F(x2) if x1 < x2,
with  limx9&4F(x) ' 0,
limx84F(x) ' 1.
Proof:  Exercise.
However, a distribution function is not always left continuous. As a counter example,
consider the distribution function of the Binomial (n,p) distribution in Section 1.2.2. Recall that
the corresponding probability space consists of sample space  
 the σ-algebra ö 
Ω' {0,1,2,...,n},
of all subsets of  
 and probability measure 
 defined by  (1.15) . The random variable X
Ω,
P({k})
involved is defined as  X(k) = k,  with distribution function 
F(x) ' 0 for x < 0,
F(x) ' 'k#xP({k}) for x 0 [0,n],
F(x) ' 1 for x > n,
Now let for example 
 Then for  
x ' 1.
0 < δ < 1, F(1 & δ) ' F(0), and F(1 % δ) ' F(1),
hence  
  
limδ90F(1 % δ) ' F(1), but limδ90F(1 & δ) '
F(0) < F(1).
The left limit of a distribution function F in x is usually denoted by F(x!):
F(x&) '
def.
limδ90F(x & δ).
Thus if x is a continuity point then F(x-) = F(x), and if x is a discontinuity point then F(x-) < F(x).
The Binomial distribution involved is an example of a discrete distribution. The uniform
distribution on [0,1] derived  in Section 1.5 is an example of a continuous distribution, with
distribution function

49
F(x) ' 0 for x < 0,
F(x) ' x for x 0 [0,1],
F(x) ' 1 for x > 1.
(1.25)
In the case of the Binomial distribution (1.15) the number of discontinuity points of  F is
finite, and in the case of the Poisson distribution (1.16) the number of discontinuity points of  F
is countable infinite. In general we have:
Theorem 1.12: The set of discontinuity points of a distribution function of a random variable is
countable.
Proof:  Let D  be the set of all discontinuity points of the distribution function F(x). Every
point x in D is associated with an non-empty open interval (F(x-),F(x)) = (a,b), say, which is
contained in [0,1]. For each of these open intervals (a,b) there exists a rational number q such
 hence the number of open intervals (a,b) involved  is countable, because the rational
a < q < b,
numbers are countable. Therefore, D is countable. Q.E.D.
The results of Theorems 1.11-1.12 only hold for distribution functions of random
variables, though.  It is possible  to generalize these results to distribution functions of random
vectors, but this generalization is far from trivial and therefore omitted.
As follows from Definition 1.11, a distribution function of a  random variable or vector 
X  is completely determined by the corresponding induced probability measure 
. But what
µX(@)
about the other way around, i.e., given a distribution function F(x), is the corresponding induced
probability measure 
 unique? The answer is yes, but we prove the result only for the
µX(@)

50
univariate case:
Theorem 1.13: Given the distribution function F of a random vector X  0 úk,  there exists a
unique probability measure 
 on  {
 Bk} such that for 
  F(x) = 
µ
úk,
x ' (x1,....,xk)T 0 úk,
µ ×k
i'1(&4,xi] .
Proof: Let k = 1 and let  
 be the collection of all intervals of the type
T0
(a,b),[a,b],(a,b],[a,b),(&4,a),(4,a],(b,4),[b,4), a#b 0 ú,
(1.26)
together with their finite unions, where  [a,a] is the singleton {a}, and  (a,a), (a,a] and [a,a)
should be interpreted as the empty set 
 Then each set in 
  can be written as a finite union of
i.
T0
disjoint sets of the type (1.26) (Compare (1.20) ), hence 
 is an algebra. Define for !4 < a < b <
T0
4, 
µ((a,a)) ' µ((a,a]) ' µ([a,a)) ' µ(i) ' 0
µ({a}) ' F(a) & limδ90F(a&δ),
µ((a,b]) ' F(b) & F(a)
µ([a,b)) ' µ((a,b]) & µ({b}) % µ({a}),
µ([a,b]) ' µ((a,b]) % µ({a})
µ((a,b)) ' µ((a,b]) & µ({b}),
µ((&4,a]) ' F(a)
µ((&4,a]) ' F(a) & µ({a}),
µ((b,4)) ' 1 & F(b)
µ([b,4)) ' µ((b,4)) % µ({b})
and let for disjoint sets  
 of the type  (1.26), 
 Then the
A1,.......,An
µ(^n
j'1Aj ) ' 'n
j'1µ(Aj).
distribution function F defines a  probability measure  
 and  this probability measure  
µ on T0,
µ
coincides on 
 with the induced probability measure 
 It follows now from Theorem 1.9 that
T0
µX.
there exists a -algebra 
 containing 
 for which the same applies. This -algebra 
 may be
σ
T
T0
σ
T

51
chosen equal to the -algebra 
 of Borel sets. Q.E.D.
σ
B
The importance of this result is that there is a one-to-one  relationship between the 
distribution function F of a random variable or vector X and the induced probability measure µX.
Therefore, the distribution function contains all the information about µX.
Definition 1.12: A distribution function F on úk and its associated probability measure  
 on 
µ
{
 Bk} are called absolutely continuous  with respect to Lebesgue measure if for every
úk,
Borel set B in  úk with zero Lebesgue measure, 
(B) = 0.
µ
We will need this concept in the next section.
1.9.
Density functions
An important concept is that of a density function. Density functions are usually
associated to differentiable distribution functions:
Definition 1.13: The distribution of a random variable X is called absolutely continuous if there
exists a non-negative integrable function f, called the density function of X, such that the
distribution function F of X can be written as the (Lebesgue) integral  
 =  
F(x)
m
x
&4f(u)du.
Similarly, the distribution of a random vector X  
 is called absolutely continuous if there
0 úk
exists a non-negative integrable function f on 
 , called the joint density, such that the
úk
distribution function F of X can be written as the integral 
 
F(x) ' m
x1
&4.....m
xk
&4f(u1,...,uk)du1....duk,

52
where x ' (x1,......,xk)T.
Thus, in the case 
 the density function f(x) is the derivative of F(x):
F(x) ' m
x
&4f(u)du
and in the multivariate case 
 the joint
f(x) ' F )(x),
F(x1,...,xk) ' m
x1
&4.....m
xk
&4f(u1,...,uk)du1....duk
density is 
 
f(x1,...,xk) ' (M/Mx1)......(M/Mxk)F(x1,...,xk).
The reason for calling the distribution functions in Definition 1.13 absolutely continuous
is that in this case the distributions involved are absolutely continuous with respect to Lebesgue
measure. See Definition 1.12. To see this, consider the case 
, and verify
F(x) ' m
x
&4f(u)du
(Exercise) that the corresponding probability measure µ is:
µ(B) ' mB f(x)dx,
(1.27)
where the integral is now the Lebesgue integral over a Borel set B. Since the Lebesgue integral
over a Borel set with zero Lebesgue measure is zero (Exercise), it follows that  µ(B) = 0 if the
Lebesgue measure of B is zero.
For example the uniform distribution (1.25) is absolutely continuous, because we can
write  (1.25) as  
 with density  f(u) = 1 for 0 < u < 1 and zero elsewhere. Note
F(x) ' m
x
&4f(u)du,
that in this case F(x) is not differentiable in 0 and 1, but that does not matter, as long as the set of
points for which the distribution function is not differentiable has zero Lebesgue measure.
Moreover, a density of a random variable always integrates to 1, because
 Similarly for random vectors  X  
 :
1 ' limx64F(x) ' m
4
&4f(u)du.
0 úk
m
4
&4m
4
&4.....m
4
&4f(u1,...,uk)du1....duk ' 1.
Note that continuity and differentiability of a distribution function are not  sufficient

53
conditions for absolute continuity. It is possible to construct a continuous distribution function
F(x) that is differentiable on a subset D d ú, with  ú\D a set with Lebesgue measure zero, such
that 
 on D, so that in this case 
 Such distributions functions are called
F )(x) / 0
m
x
&4F )(x)dx / 0.
singular. See Chung (1974, pp. 12-13) for an example of how to construct a singular distribution
function on ú, and Chapter 5 for singular multivariate normal distributions.
1.10.
Conditional probability, Bayes’ rule,  and independence
1.10.1 Conditional probability
Consider statistical experiment with  probability space {S,ö,P}, and suppose that it is
known that the outcome of this experiment is contained in a set B with P(B) > 0. What is the
probability of an event A,  given that the outcome of the experiment is contained in B? For
example, roll a dice. Then S = {1,2,3,4,5,6}, ö is the F-algebra of all subsets of S, and P({T}) =
1/6 for T = 1,2,3,4,5,6. Let B be the event: "the outcome is even": B = {2,4,6}, and let A =
{1,2,3}. If we know that the outcome is even, then we know that  the outcomes {1,3} in A will
not occur: if the outcome in contained in A, it is contained in A1B = {2}. Knowing that the
outcome is either 2,4, or 6, the probability that the outcome is contained in A is therefore 1/3 =
P(A1B)/P(B). This is the conditional probability of A, given B, denoted by P(A|B). If it is
revealed that the outcome of a statistical experiment is contained in a particular set B, then the
sample space S is reduced to B, because we then know that the outcomes in the complement of B
will not occur,  the F-algebra ö is reduced to ö1B, the collection of all intersections of the sets
in ö with B:  ö1B ={A1B, A0ö} (Exercise: Is this a F-algebra?), and the probability measure
involved becomes P(A|B) = P(A1B)/P(B),  hence the probability space becomes

54
 See Exercise 19 below.
{B,ö_B,P(@|B)}.
1.10.2 Bayes’ rule
Let A and B be sets in ö. Since the sets A and 
 form a partition of the sample space S,
˜A
we have  
 hence
B ' (B _ A) ^ (B _ ˜A),
P(B) ' P(B_A) % P(B_ ˜A) ' P(B*A)P(A) % P(B* ˜A)P( ˜A).
Moreover, 
P(A*B) ' P(A_B)
P(B)
' P(B*A)P(A)
P(B)
.
Combining these two results now yields Bayes' rule:
P(A*B) '
P(B*A)P(A)
P(B*A)P(A) % P(B* ˜A)P( ˜A)
.
Thus, Bayes’ rule enables us to compute the conditional probability P(A|B) if P(A) and the
conditional probabilities 
 are given.
P(B*A) and P(B* ˜A)
More generally, if Aj, j =1,2,.....n (# 4) is a partition of the sample space S, i.e., the  Aj’s
are disjoint sets in ö such that 
 then
Ω' ^n
j'1Aj,
P(Ai*B) '
P(B*Ai)P(Ai)
'n
j'1P(B*Aj)P(Aj)
.
Bayes’ rule plays an important role in a special branch of statistics [and econometrics],
called Bayesian statistics [econometrics].

55
1.10.3 Independence
If P(A|B) = P(A), then knowing that the outcome is in B does not give us any information
about A. In that case the events A and B are called independent. For example, if I tell you that the
outcome of the dice experiment is contained in the set {1,2,3,4,5,6} = S, then you know nothing
about the outcome:  P(A|S) = P(A1S)/P(S) = P(A), hence S is independent of any other event A.
Note that  P(A|B) = P(A) is equivalent to P(A1B) = P(A)P(B). Thus,
Definition 1.14: Sets A  and  B in ö are (pairwise) independent if  P(A1B) = P(A)P(B).
If events A and B are independent, and events  B and C are independent, are the events A
and C independent? The answer is: not necessarily. In order to give a counter example, observe
that if  A and B are independent, then so are 
 and 
 because 
˜A and B, A and ˜B,
˜A and ˜B,
P( ˜A_B) ' P(B) & P(A_B) ' P(B) & P(A)P(B) ' (1&P(A))P(B) ' P( ˜A)P(B),
and similarly,  
  
P(A_ ˜B) ' P(A)P( ˜B) and P( ˜A_ ˜B) ' P( ˜A)P( ˜B).
Now if C =
 and 0 < P(A) < 1, then   B and C = 
 are independent if  A and B are independent,
˜A
˜A
but  
  
P(A_C) ' P(A_ ˜A) ' P(i) ' 0,
whereas  
P(A)P(C) ' P(A)P( ˜A) ' P(A)(1&P(A)) … 0.
Thus, for  more than two events we need a stronger condition for independence than pairwise

56
independence, namely:
Definition 1.15: A sequence Aj of sets in ö is independent if for every sub-sequence 
 i =
Aji,
1,2,..,n,  P(_n
i'1Aji ) ' (n
i'1P(Aji ).
By requiring that the latter holds for all sub-sequences rather than 
 we
P(_4
i'1Ai ) ' (4
i'1P(Ai ),
avoid the problem that a sequence of events would be called independent if one of the events is
the empty set.
The independence of a pair or sequence of random variables or vectors can now be
defined as follows.
Definition 1.16: Let  Xj  be a sequence of random variables or  vectors defined on a common
probability space {S,ö,P}. X1 and X2 are pairwise independent if for all Borel sets B1, B2, the
sets  
 and 
 are independent.  The sequence
A1 ' {ω0Ω: X1(ω) 0 B1}
A2 ' {ω0Ω: X2(ω) 0 B2}
Xj  is independent if for all Borel sets Bj the sets  
 are independent.
Aj ' {ω0Ω: Xj(ω) 0 Bj}
As we have seen before, the collection 
B}} =
öj ' {{ω0Ω: Xj(ω) 0 B}, B 0
B}} is a sub F-algebra of ö. Therefore, Definition 1.16 also reads:   
{X &1
j (B), B 0
The sequence of random variables Xj  is independent if for arbitrary  Aj  0 öj  the sequence of
sets  Aj is independent according to Definition 1.15.
Independence usually follows  from the setup of a statistical experiment. For example,
draw randomly with replacement  n  balls from a bowl containing R red balls and N!R white

57
balls, and let Xj = 1 if the j-th draw is a red ball, and Xj =0 if the j-th draw is a white ball. Then
X1,...,Xn are independent (and X1+...+Xn  has the Binomial (n,p) distribution, with p = R/N).
However, if we would draw these balls without replacement, then X1,...,Xn are not independent.
For a sequence of random variables  Xj  it suffices to verify the condition in Definition
1.16 for Borel sets  Bj of the type (!4,xj], xj  0 ú, only:
Theorem 1.14:  Let  X1,...,Xn be random variables, and denote for x  0 ú and  j = 1,....,n, 
  Then   X1,...,Xn  are independent if and only if for arbitrary
Aj(x) ' {ω0Ω: Xj(ω) # x}.
 the sets 
 are independent.
(x1,.....,xn)T 0 ún
A1(x1),......,An(xn)
The complete proof of Theorem 1.14 is difficult and is therefore omitted, but the result can be
motivated as follow. Let 
 together with all finite
ö0
j ' {Ω,i,X &1
j ((&4,x]),X &1
j ((y,4)), œ x,y0ú,
unions and intersections of the latter two types of sets}. Then 
 is an algebra such that for
ö0
j
arbitrary 
 the sequence of sets  Aj is independent.  This is not too hard to prove. Now
Aj 0 ö0
j
 B}}  is the smallest σ-algebra containing 
, and is also the smallest
öj ' {X &1
j (B), B 0
ö0
j
monotone class containing 
. It can be shown (but this is the hard part), using the properties of
ö0
j
monotone class (see Exercise 11 below), that for arbitrary  
 the sequence of sets  Aj  is
Aj 0 öj
independent as well
It follows now from Theorem 1.14 that:
Theorem 1.15: The random variables X1,...,Xn  are independent if and only if the joint
distribution function F(x) of  X = (X1,...,Xn)T  can be written as the product of the distribution

58
functions Fj(xj) of the  Xj ‘s, i.e.,  
 
F(x) ' (n
j'1Fj(xj), where x ' (x1,....,xn)T.
The latter distribution functions Fj(xj) are called the marginal distribution functions. Moreover, it
follows straightforwardly from Theorem 1.15 that if the joint distribution of 
 is
X ' (X1,....,Xn)T
absolutely continuous with joint density function f(x), then  X1,...,Xn  are independent if and only
if f(x) can be written as the product of the density functions fj(xj) of the  Xj ‘s: 
 
 
f(x) ' (n
j'1fj(xj), where x ' (x1,....,xn)T.
The latter density functions are called the marginal density functions.
1.11.
Exercises
1.
Prove (1.4). 
2.
Prove (1.17)  by proving that  ln[(1 & µ/n)n] ' n ln(1 & µ/n) 6 &µ for n 6 4.
3.
Let  
  be the collection of all subsets of   
= (0,1] of the type (a,b], where  
 are
ö(
Ω
a < b
rational numbers in [0,1],  together with their finite disjoint unions and the empty set 
 Verify
i.
that 
  is an algebra.  
ö(
4.
Prove Theorem 1.2.  
5.
Prove Theorem 1.5.
6.
Let  Ω = (0,1], and let 
 be the collection of all intervals of the type (a,b] with
Œ
 Give as many distinct examples as you can of sets that are contained in 
0 # a < b # 1.
σ(Œ)
(the smallest σ-algebra containing this collection 
), but not in 
 (the smallest algebra
Œ
α(Œ)
containing the collection 
).
Œ

59
7.
Show that 
 =  B.
σ({[a,b]: œ a # b, a,b 0 ú})
8.
Prove part (g) of Theorem 1.8.
9.
Prove that 
 defined by (1.20)  is an algebra.
ö0
Prove (1.22).
11.  
A collection 
 of subsets of a set 
 is called a monotone class if the following two
ö
Ω
conditions hold:
 imply 
,
An 0 ö, An d An%1, n ' 1,2,3,.....
^4
n'1An 0 ö
 imply 
.
An 0 ö, An e An%1, n ' 1,2,3,.....
_4
n'1An 0 ö
Show that an algebra is a F-algebra if and only if it is a monotone class.
12.  
A collection 
 of subsets of a set 
 is called a 
system if 
 implies 
öλ
Ω
λ&
A 0 öλ
˜A 0 öλ,
and for disjoint sets 
 A collection 
 of subsets of a set  
 is called a
Aj 0 öλ, ^4
j'1Aj 0 öλ.
öπ
Ω
system if 
 implies that 
 Prove that if a 
system is also  a 
system,
π&
A,B 0 öπ
A_B 0 öπ.
λ&
π&
then it is a  F-algebra .
13. 
Let 
 be  the smallest 
algebra of subsets of 
 containing the (countable) collection
ö
σ&
ú
of half-open intervals  
 with rational endpoints q. Prove that  
 contains all  the Borel
(&4,q]
ö
subsets of 
:  
 = 
ú
B
ö.
14.  
Consider the following subset of  
 Explain
ú2: L ' {(x,y) 0 ú2: y ' x, 0 # x # 1}.
why L is a Borel set. 
15.  
Consider the following subset of  
 Explain why C
ú2: C ' {(x,y) 0 ú2: x 2 % y 2 # 1}.
is a Borel set. 
16.
Prove Theorem 1.11. Hint: Use Definition 1.12 and Theorem 1.8. Determine first which

60
parts of Theorem 1.8 apply.
17. 
Let 
 be an absolutely continuous distribution function. Prove that
F(x) ' m
x
&4f(u)du
corresponding probability measure µ is given by the Lebesgue integral (1.27).
18. 
Prove that the Lebesgue integral over a Borel set with zero Lebesgue measure is zero.
19.
Let 
 be a probability space, and let 
 with P(B) > 0. Verify that 
{Ω,ö,P}
B 0 ö
is a probability space.
{B,ö_B,P(@|B)}
20.
Are disjoint sets in 
independent?
ö
21.
(Application of Bayes’ rule): Suppose that 1 out of 10,000 people suffer from a certain
disease, say HIV+. Moreover, suppose that there exists a medical test for this disease which is
90% reliable: If you don't have the disease, the test will confirm that with probability 0.9, and the
same if you do have the disease. If a randomly selected person is subjected to this test, and the
test indicates that this person has the disease, what is the probability that this person actually has
this disease? In other words, if you were this person, would you be scared or not?
22.
Let A  and  B in ö be pairwise independent. Prove that  
 are independent (and
˜A and B
therefore 
 are independent and 
 are independent).
A and ˜B
˜A and ˜B
23.
Draw randomly without replacement  n  balls from a bowl containing R red balls and
N!R white balls, and let Xj = 1 if the j-th draw is a red ball, and Xj =0 if the j-th draw is a white
ball. Show that X1,...,Xn  are not independent.

61
Appendices
1.A. 
Common structure of the proofs of Theorems 1.6 and 1.10
The proofs of Theorems 1.6 and 1.10 employ a similar argument, namely the following:
Theorem 1.A.1. Let 
 be a collection of subsets of a set S, and let 
 be the smallest F-
Œ
σ(Œ)
algebra   containing 
. Moreover, let D be a Boolean function on 
 i.e., D  is a set function
Œ
σ(Œ),
which takes either the value "True" or "False".  Furthermore, let 
for all sets A in
ρ(A) ' True
. If the collection  
 of sets A in 
 for which 
 is a F-algebra itself, then 
Œ
D
σ(Œ)
ρ(A) ' True
 for  all sets A  in 
.
ρ(A) ' True
σ(Œ)
Proof: Since 
 is a collection of sets in 
 we have
 
Moreover, by
D
σ(Œ)
D d σ(Œ).
assumption, 
, and 
 is a F-algebra . But  
 is the smallest  F-algebra containing 
,
Œ d D
D
σ(Œ)
Œ
hence 
. Thus,  
 
 and consequently, 
 for  all sets A in 
.
σ(Œ) d D
D ' σ(Œ),
ρ(A) ' True
σ(Œ)
Q.E.D.
This type of proof will also be used later on.
Of course, the hard part is to prove that 
 is F-algebra. In particular, the collection  
D
D
is not automatically a F-algebra. Take for example the case where S  = [0,1],  
 is the collection
Œ
of all intervals [a,b] with 0 # a < b # 1, and 
 if the smallest interval [a,b] containing
ρ(A) ' True
A has positive length: b-a > 0, and 
 otherwise. In this case 
consists of all the
ρ(A) ' False
σ(Œ)
Borel subsets of [0,1], but 
  does not contain singletons whereas 
 does, so 
 is smaller
D
σ(Œ)
D
than 
, and therefore not a F-algebra. 
σ(Œ)

62
1.B.
 Extension of an outer measure to a probability measure
In order to use the outer measure as a probability measure for more general sets that those
in 
, we have to extend the algebra 
  to a  σ-algebra 
 of events for which the outer
ö0
ö0
ö
measure is a probability measure.  In this appendix it will be shown how 
 can be constructed,
ö
via  the following lemmas.. 
Lemma 1.B.1: For any sequence 
 of disjoint sets in 
,  
 
Bn
Ω
P ((^4
n'1Bn) # '4
n'1P ((Bn).
 
Proof: Given an arbitrary 
 it  follows from (1.21) that there exists a countable
g > 0
sequence of sets  
 in 
  such that  
 and  
 hence
An,j
ö0
Bn d ^4
j'1An,j
P ((Bn) > '4
j'1P(An,j ) & g2&n,
'4
n'1P ((Bn) > '4
n'1'4
j'1P(An,j ) & g'4
n'12&n ' '4
n'1'4
j'1P(An,j ) & g.
(1.28)
Moreover,  
 where the latter is a countable union of sets in 
, hence  it 
^4
n'1Bn d ^4
n'1^4
j'1An,j,
ö0
follows from (1.21) that
P ((^4
n'1Bn) # '4
n'1'4
j'1P(An,j).
(1.29)
Combining (1.28) and (1.29) it follows that for arbitrary 
, 
g > 0
'4
n'1P ((Bn) > P ((^4
n'1Bn) & g.
(1.30)
Letting  
 the lemma follows now from (1.30) . Q.E.D.
g 9 0,
Thus, in order for the outer measure to be a probability measure, we have to impose

63
conditions on the collection 
 of subsets of 
  such that for any sequence 
 of disjoint sets in
ö
Ω
Bj
   
  The latter is satisfied if we choose 
  as follows:
ö,
P ((^4
j'1Bj) $ '4
j'1P ((Bj).
ö
Lemma 1.B.2: Let  
 be a collection of subsets sets B of  
 such that for any subset A of  
:
ö
Ω
Ω
P ((A) ' P ((A_B) % P ((A_ ˜B).
(1.31)
Then for all countable sequences of disjoint sets  Aj 0 ö, P ((^4
j'1Aj) ' '4
j'1P ((Aj).
Proof: Let  
 Then   
 are
A ' ^4
j'1Aj, B ' A1.
A_B ' A_A1 ' A1 and A_ ˜B ' ^4
j'2Aj
disjoint, hence
P ((^4
j'1Aj) ' P ((A) ' P ((A_B) % P ((A_ ˜B) ' P ((A1) % P ((^4
j'2Aj).
(1.32)
Repeating (1.32) for 
 with 
, k=2,...,n, it follows by induction  that 
P ((^4
j'kAj)
B ' Ak
P ((^4
j'1Aj) '
'n
j'1P ((Aj) % P ((^4
j'n%1Aj) $
'n
j'1P ((Aj) for all n $ 1,
 hence  
 Q.E.D.
P ((^4
j'1Aj) $ '4
j'1P ((Aj).
Note that condition (1.31) automatically holds if 
: Choose an arbitrary set A and
B 0 ö0
an arbitrary small number  
 Then there exists an  covering 
 
 
g > 0.
A d ^4
j'1Aj, where Aj 0 ö0,
such  that  
 Moreover, since  
'4
j'1P(Aj) # P ((A) % g.
A_B d ^4
j'1Aj_B, where Aj_B 0 ö0,
and 
  
 we have    
 and
A_ ˜B d ^4
j'1Aj_ ˜B, where Aj_ ˜B 0 ö0,
P ((A_B) # '4
j'1P(Aj_B)
 hence  
 Since  is arbitrary, it
P ((A_ ˜B) # '4
j'1P(Aj_ ˜B),
P ((A_B) % P ((A_ ˜B) # P ((A) % g.
g
follows now that 
 
  + 
P ((A) $ P ((A_B)
P ((A_ ˜B).
We show now that 
Lemma 1.B.3: The collection 
 in Lemma 1.B.2  is a 
algebra of subsets of  
,  containing
ö
σ&
Ω

64
the algebra 
  
ö0.
Proof:  First, it  follows trivially from (1.31) that 
 implies 
 Now let
B 0 ö
˜B 0 ö.
 It remains to show that 
 which I will do in two steps. First, I will show
Bj 0 ö.
^4
j'1Bj 0 ö,
that  
 is an algebra, and then I will use Theorem 1.4 to show that  
  is also a   
algebra.
ö
ö
σ&
(a)  
Proof that  
 is an algebra: We have to show that  
 implies that
ö
B1,B2 0 ö
  We have
B1^B2 0 ö.
 
P ((A_ ˜B1) ' P ((A_ ˜B1_B2) % P ((A_ ˜B1_ ˜B2),
and since
 
A_(B1^B2) ' (A_B1)^(A_B2_ ˜B1)
we have 
P ((A_(B1^B2)) #
P ((A_B1) % P ((A_B2_ ˜B1).
Thus:
P ((A_(B1^B2)) % P ((A_ ˜B1_ ˜B2) # P ((A_B1) % P ((A_B2_ ˜B1) % P ((A_ ˜B2_ ˜B1)
' P ((A_B1) % P ((A_ ˜B1) ' P ((A).
(1.33)
Since  
 and 
,  it follows now  
~(B1^B2) ' ˜B1_ ˜B2
P ((A) # P ((A_(B1^B2)) % P ((A_(~(B1^B2))
from (1.33) that 
 Thus, 
implies that
P ((A) ' P ((A_(B1^B2)) %
P ((A_(~(B1^B2)).
B1,B2 0 ö
 hence  
 is an algebra (containing the algebra 
). 
B1^B2 0 ö,
ö
ö0
(b) 
Proof that 
  is a 
algebra:  Since we  have  established that  
 is an algebra, it
ö
σ&
ö
follows from Theorem 1.4 that in proving that 
 is also a 
algebra it suffices to  verify that 
ö
σ&
  for disjoint sets  
  For such sets  we have: 
 and
^4
j'1Bj 0 ö
Bj 0 ö:
A_(^n
j'1Bj)_Bn ' A_Bn,

65
 hence
A_(^n
j'1Bj)_ ˜Bn ' A_(^n&1
j'1 Bj),
P ((A_(^n
j'1Bj)) ' P ((A_(^n
j'1Bj)_Bn) % P ((A_(^n
j'1Bj)_ ˜Bn) ' P ((A_Bn) % P ((A_(^n&1
j'1 Bj)).
Consequently, 
P ((A_(^n
j'1Bj)) ' 'n
j'1P ((A_Bj).
(1.34)
Next, let 
 Then  
 hence
B ' ^4
j'1Bj.
˜B ' _4
j'1 ˜Bj d
_n
j'1 ˜Bj ' ~(^n
j'1Bj),
P ((A_ ˜B)
# P ((A_(~[^n
j'1Bj])).
(1.35)
It follows now from (1.34) and (1.35) that for all n $ 1,
P ((A) ' P ((A_(^n
j'1Bj)) % P ((A_(~[^n
j'1Bj])) $ 'n
j'1P ((A_Bj) % P ((A_ ˜B),
hence
P ((A) $ '4
j'1P ((A_Bj) % P ((A_ ˜B) $ P ((A_B) % P ((A_ ˜B),
(1.36)
where the last inequality is due to
P ((A_B) ' P ((^4
j'1(A_Bj)) # '4
j'1P ((A_Bj).
Since we always have 
 (compare Lemma 1.B.1), it follows from
P ((A) # P ((A_B) % P ((A_ ˜B)
(1.36) that for countable unions 
 of disjoint sets 
B ' ^4
j'1Bj
Bj 0 ö,
P ((A) ' P ((A_B) % P ((A_ ˜B),
hence 
 Consequently, 
  is a 
algebra, and  the outer measure P* is a probability
B 0 ö.
ö
σ&
measure on {
}.  Q.E.D.
Ω, ö
Lemma 1.B.4: The  
algebra 
 in Lemma 1.B.3 can be chosen such that  
 is unique: any
σ&
ö
P (

66
1.
In the Spring of 2000 the Texas Lottery has changed the rules: The number of balls has
been increased to 54, in order to create a larger jackpot. The official reason for this change is to
make playing the lotto more attractive, because a higher  jackpot will make the lotto game more
exciting. Of course, the actual reason is to boost the lotto revenues!
2.
Under the new rules (see note 1), this probability is: 1/25,827,165.
3. 
These binomial numbers can be computed using the “Tools 
 Discrete distribution tools”
6
menu of EasyReg International, the free econometrics software package developed by the author.
EasyReg International can be downloaded from web page 
http://econ.la.psu.edu/~hbierens/EASYREG.HTM
4.
Note that the latter  phrase is superfluous, because 
 reads: every element of 
 is
Ωd Ω
Ω
included in 
, which is clearly a true statement, and 
 is true because 
Ω
i d Ω
i d i^Ω' Ω.
5.
Also called a Field. 
6.
Also called a 
Field, or a Borel Field.
σ&
7.
This section may be skipped.
8.
See also Appendix 1.A.
9.
In the sequel we will denote the probability of an event involving random variables or
vectors X  as P(“expression involving X”), without referring to the corresponding set in 
. For
ö
example, for random variables X and Y defined on a common probability space 
 the
{Ω,ö,P}
short-hand notation P(X > Y) should be interpreted as P({ω0Ω: X(ω) > Y(ω)}).
10.
See also Appendix 1.A.
probability measure  
 on 
 which coincide with P on 
 is equal to the outer measure
P(
{Ω,ö}
ö0
.
P (
The proof of Lemma 1.B.4 is too difficult and too long [see Billingsley (1986, Theorems
3.2-3.3)], and is therefore omitted.  
Combining Lemmas 1.B.2-1.B.4,  Theorem 1.9 follows.
Endnotes

67
Chapter 2
Borel Measurability,  Integration,
and Mathematical Expectations
2.1.
Introduction
Consider the following situation: You are sitting in a bar next to a guy who proposes to
play the following game.  He will roll a dice, and pay you a dollar per dot. However, you have to
pay him an amount y up-front each time he rolls the dice. The question is: which amount y
should you pay him in order for both of you to play even if this game is played indefinitely?
Let X  be the amount you win in a single play. Then in the long run you will receive X = 1
dollar in 1 out of 6 times, X = 2 dollar in 1 out of 6 times, up to X = 6 dollar in 1 out of 6 times.
Thus, in average you will receive (1+2+...+6 )/6 = 3.5 dollar per game, hence the answer is: y =
3.5. 
Clearly, X is a random variable:  
  where here and in the sequel
X(ω) ' '6
j'1 j.I(ω 0 {j}),
I(.) denotes the indicator function:
I(true) ' 1, I(false) ' 0.
This random variable is defined on the probability space {Ω, ö,P}, where Ω ={1,2,3,4,5,6},  ö
is the σ-algebra of all subsets of  Ω, and  
 = 1/6 for each  
 Moreover,
P({ω})
ω 0 Ω.
 This amount y is called the mathematical expectation of X, and is
y ' '6
j'1 j/6 ' '6
j'1 jP({j}).
denoted by E(X).
More generally, if X is the outcome of a game with pay-off function g(X),  where X  is

68
discrete:  
 with 
 (n is possibly infinite),  and if this game is
pj ' P[X ' xj] > 0
'n
j'1pj ' 1
repeated indefinitely, then the average pay-off will be
y ' E[g(X)] ' 'n
j'1g(xj)pj.
(2.1)
Some computer programming languages, such as Fortran, Visual Basic, C++, etc., have a
build-in function which generates uniformly distributed  random numbers  between zero and one.
Now suppose that the guy next to you at the bar pulls out his laptop computer, and proposes to
generate random numbers and pay you X dollar per game if the random number involved is X,
provided you pay him an amount y up front each time. The question is again: which amount y
should you pay  him in order for both of you to play even if this game is played indefinitely? 
Since the random variable X involved is uniformly distributed on [0,1],  it has distribution
function 
 with density
F(x) ' 0 for x # 0, F(x) ' x for 0 < x < 1, F(x) ' 1 for x $ 1,
function 
 More formally,  
 is a  non-negative
f(x) ' F )(x) ' I(0 < x < 1).
X ' X(ω) ' ω
random variable defined on the probability space 
 where Ω = [0,1],  ö = [0,1]1B,
{Ω,ö,P},
i.e., the  σ-algebra of all Borel sets in [0,1], and P is the Lebesgue measure on [0,1]. 
In order to determine y in this case, let
X((ω) ' 'm
j'1[infω0(bj&1,bj]X(ω)]I(ω 0 (bj&1,bj]) ' 'm
j'1bj&1I(ω 0 (bj&1,bj]),
where b0 = 0 and bm =1. Clearly,  
 with probability 1, and similarly to the dice game
0 # X( # X
the amount y involved will be greater or equal to  'm
j'1bj&1P((bj&1,bj]) ' 'm
j'1bj&1(bj&bj&1).
Taking the supremum over all possible partitions  
 of  (0,1] then yields the integral
^m
j'1(bj&1,bj]

69
 
y ' E(X) ' m
1
0 xdx ' 1/2.
(2.2)
More generally, if X is the outcome of a game with pay-off function g(X),  where X  has
an absolutely continuous distribution with density f(x),  then 
y ' E[g(X)] ' m
4
&4
g(x)f(x)dx.
(2.3)
Now two questions arise. First, under what conditions is g(X) a well-defined random
variable?  Second, how do we determine  E(X) if the distribution of X is neither discrete nor
absolutely continuous?
2.2.
Borel measurability
Let g be a real function and let X be a random variable defined on the probability space
 In order for g(X) to be a random variable, we must have that:
{Ω,ö,P}.
For all Borel sets B, {ω 0 Ω: g(X(ω)) 0 B} 0 ö.
(2.4)
It is possible to construct a real function g and a random variable X  for which this is not the case.
But if
For all Borel sets B, AB ' {x 0 ú: g(x) 0 B} is a Borel set itself,
(2.5)
then (2.4) is clearly satisfied, because then for any Borel set B, and AB defined in (2.5),
  {ω0Ω: g(X(ω)) 0 B} ' {ω0Ω: X(ω) 0 AB} 0 ö.
Moreover, if (2.5) is not satisfied, in the sense that there exists a Borel set B for which AB is not a
Borel set itself, then it is possible to construct a random variable X such that the set
 
 
{ω0Ω: g(X(ω)) 0 B} ' {ω0Ω: X(ω) 0 AB} ó ö,

70
hence for such a random variable X,  g(X) is not a random variable itself.1  Thus,  g(X) is
guaranteed to be a random variable if and only if (2.5) is satisfied. Such real functions  g(x) are
called Borel measurable:
Definition 2.1:  A real function g is Borel measurable if and only if   for all Borel sets B in 
 the
ú
sets  AB = 
are Borel sets in 
 . Similarly, a real function g on 
 is Borel
{x0ú: g(x) 0 B}
ú
úk
measurable if and only if for all Borel sets B in 
 the sets AB = 
are Borel sets
ú
{x0úk: g(x) 0 B}
in 
 .
úk
However, we do not need to verify condition (2.5) for all Borel sets. It suffices to verify it for
Borel sets of the type 
 only:
(&4,y], y 0 ú,
Theorem 2.1: A real function g  on 
 is Borel measurable if and only if  for all y 
 the sets
úk
0 ú
Ay = 
are Borel sets in 
 . 
{x0úk: g(x) # y}
úk
Proof:  Let D be the collection of all Borel sets B in 
 for which the sets
ú
 are Borel sets in 
, including the Borel sets of the type 
 
{x0úk: g(x) 0 B}
úk
(&4,y], y 0 ú.
Then  D contains the collection of all intervals of the type 
 
The smallest σ-
(&4,y], y 0 ú.
algebra containing the collection {
 
} is just the Euclidean Borel field  B =
(&4,y], y 0 ú
σ({
 
}), hence if D is a σ-algebra then B d D. But D is a collection of Borel
(&4,y], y 0 ú

71
sets, hence D d B. Thus, if D is a σ-algebra then B =D.  The proof  that  D is a σ-algebra
is left as an exercise. Q.E.D.
The simplest Borel measurable function is the simple function:
Definition 2.2: A real function g on 
  is called a simple function if it takes the form
úk
 where the 
’s are disjoint Borel sets in 
.
g(x) ' 'm
j'1ajI(x 0 Bj), with m < 4, aj 0 ú,
Bj
úk
Without loss of generality we may assume that the disjoint Borel sets 
‘s form a  partition of
Bj
  
 because if not, then let  
 = 
 with 
 
úk: ^m
j'1Bj ' úk,
g(x)
'm%1
j'1 ajI(x 0 Bj),
Bm%1 ' úk\(^m
j'1Bj)
and am+1 = 0.  Moreover, without loss of generality we may assume that the aj ‘s are all different.
For example, if  
 = 
 and 
 then 
 where 
g(x)
'm%1
j'1 ajI(x 0 Bj)
am ' am%1
g(x) ' 'm
j'1ajI(x 0 B (
j ) ,
 for j = 1,...,m-1 and 
B (
j ' Bj
B (
m ' Bm^Bm%1.
Theorem 2.1 can be used to prove that:
Theorem 2.2: Simple functions are Borel measurable.
Proof: Let  
 be a simple function on 
. For arbitrary 
,
g(x) ' 'm
j'1ajI(x 0 Bj)
úk
y 0 ú
{x0úk: g(x) # y} ' {x0úk: 'm
j'1ajI(x 0 Bj) # y} '
^
aj # y
Bj,
which is a finite union of Borel sets and therefore a Borel set itself. Since y was arbitrary, it
follows from Theorem 2.1 that g is Borel measurable. Q.E.D.

72
Theorem 2.3: If f(x) and g(x) are simple functions, then so are f(x) + g(x), f(x)-g(x), and f(x).g(x).
If in addition g(x) … 0 for all x, then f(x)/g(x) is a simple function.
Proof: Exercise
Theorem 2.1 can also be used to prove:
Theorem 2.4: Let 
 be a sequence of Borel measurable functions. Then  
gj(x), j ' 1,2,3,....,
(a)
are Borel
f1,n(x) ' min{g1(x),.....,gn(x)} and f2,n(x) ' max{g1(x),.....,gn(x)}
measurable,
(b)
 are Borel measurable,
f1(x) ' infn$1gn(x) and f2(x) ' supn$1gn(x)
(c)
 are Borel measurable,
h1(x) ' liminfn64gn(x) and h2(x) ' limsupn64gn(x)
(d)
if  
 exists, then g is Borel measurable.
g(x) ' limn64gn(x)
Proof: First, note that the min, max, inf, sup, liminf, limsup, and lim operations are taken
pointwise in x. I will only prove the min, inf and liminf cases, for Borel measurable real functions
on 
Again, let 
 be arbitrary. Then,
ú.
y 0 ú
(a)
 B.
{x0ú: f1,n(x) # y} ' ^n
j'1{x0ú: gj(x) # y} 0
(b)
 B.
{x0ú: f1(x) # y} ' ^4
j'1{x0ú: gj(x) # y} 0
(c)
 B.
{x0ú: h1(x) # y} ' _4
n'1^4
j'n{x0ú: gj(x) # y} 0
The max, sup, limsup and lim cases are left as exercises. Q.E.D.
Since continuous functions can be written as a pointwise limit of step functions, and step

73
functions with a finite number of steps are simple functions, it follows from Theorems 2.1 and
2.4(d) that:
Theorem 2.5: Continuous real functions are Borel measurable.
Proof: Let g be a continuous function on 
 Define for natural numbers n,  
ú.
gn(x) ' g(x)
if -n < x # n,  
 elsewhere. Next, define for j = 0,...,m-1 and m = 1,2,...,
gn(x) ' 0
 
.
B(j,m,n) ' (&n % 2n.j/m,&n % 2(j%1)n/m]
Then the 
‘s are disjoint intervals such that 
  hence the function
Bj(m,n)
^m&1
j'0 Bj(m,n) ' (&n,n],
gn,m(x) ' 'm&1
j'0 infx(0B(j,m,n)g(x() I x 0 B(j,m,n)
is a step function with a finite number of steps, and thus a simple function. Since trivially
 pointwise in x,  
 is Borel measurable if the functions  
 are Borel
g(x) ' limn64gn(x)
g(x)
gn(x)
measurable [see  Theorem 2.4(d)]. Similarly,  the functions  
  are Borel measurable if for
gn(x)
arbitrary fixed n, 
 pointwise in x, because the 
‘s are simple functions
gn(x) ' limm64gn,m(x)
gn,m(x)
and thus Borel measurable.  To prove 
, choose an arbitrary fixed x and
gn(x) ' limm64gn,m(x)
choose 
 Then there exists a sequence of indices 
 such that 
 for all m,
n > |x|.
jn,m
x 0 B(jn,m,m,n)
hence
 0 # gn(x) & gn,m(x) # g(x) & infx(0B(jn,m,m,n)g(x() # sup*x&x(*#2n/m*g(x) & g(x()* 6 0
as 
  The latter result follows from the continuity of g(x). Q.E.D.
m 6 4.
Next, I will show that real functions are Borel measurable if and only if they are limits of
simple functions, in two steps:

74
Theorem 2.6: A nonnegative real function g(x) is Borel measurable if and only if there exists a
non-decreasing sequence gn(x) of nonnegative simple functions such that pointwise in x, 0 # gn(x)
#  g(x) , and  limn64gn(x) ' g(x).
Proof: The “if” case follows straightforwardly from Theorems 2.2 and 2.4.  For proving
the “only if” case, let for 1 # m # n2n, 
 otherwise.  Then  gn(x) is a sequence
gn(x) ' (m&1)/2n if (m&1)/2n # g(x) < m/2n, gn(x) ' n
of simple functions, satisfying  0 # gn(x) #  g(x) ,  and  
 pointwise in x.
limn64gn(x) ' g(x),
Q.E.D.
Every real function  g(x) can be written as a difference of two non-negative functions:
g(x) ' g%(x) & g&(x), where g%(x) ' max{g(x),0}, g&(x) ' max{&g(x),0}.
(2.6)
Moreover, if g is Borel measurable, then so are 
 in  (2.6).  It follows therefore
g% and g&
straightforwardly from (2.6) and Theorems 2.3 and 2.6 that:
Theorem 2.7: A real function g(x) is Borel measurable if an only if it is the limit of a sequence of
simple functions.
Proof: Exercise.
Using Theorem 2.7, Theorem 2.3 can now be generalized to:
Theorem 2.8: If f(x) and g(x) are Borel measurable functions, then so are f(x) + g(x), f(x)-g(x),

75
and f(x).g(x). Moreover, if g(x) … 0 for all x, then f(x)/g(x) is a Borel measurable function.
Proof: Exercise
2.3.
Integrals of Borel measurable functions with respect to a probability measure
If g is a step function on (0,1], say  
 = 
 where b0 = 0 and bm+1 =
g(x)
'm
j'1ajI(x 0 (bj,bj%1]),
1,  then the Riemann integral of g over (0,1] is defined as:
 
 
m
1
0 g(x)dx ' 'm
j'1aj(bj%1&bj) ' 'm
j'1ajµ((bj,bj%1]),
where µ is the uniform probability measure on (0,1]. Mimicking this results for simple functions
and more general probability measures µ, we can define the integral of a simple function with
respect to a probability measure µ as follows:
Definition 2.3: Let µ be a probability measure on {
,Bk}, and let  
 = 
 be a
úk
g(x)
'm
j'1ajI(x0Bj)
simple function on 
. Then the integral of g with respect to  µ is defined as
úk
 
2
mg(x)dµ(x) '
def.
'm
j'1ajµ(Bj).
For non-negative continuous real functions g on (0,1], the Riemann integral of  g over
(0,1] is defined as
 
 
m
1
0 g(x)dx ' sup
0#g(#gm
1
0 g((x)dx,
where the supremum is taken over all step functions 
 satisfying
 for all x in
g(
0 # g((x) # g(x)
(0,1]. Again, we may mimick this result for non-negative Borel measurable functions g and

76
general probability measures µ:
Definition 2.4: Let µ be a probability measure on {
,Bk}, and let  
 be a non-negative
úk
g(x)
Borel measurable function on 
. Then the integral of g with respect to  µ is defined as:
úk
mg(x)dµ(x) '
def.
sup
0#g(#gmg((x)dµ(x),
where the supremum is taken over all simple functions  
  satisfying 
 for all x
g(
0 # g((x) # g(x)
in a Borel set B with  µ(B) = 1.
Using the decomposition (2.6), we can now define the integral of an arbitrary Borel
measurable function with respect to a probability measure:
Definition 2.5: Let µ be a probability measure on {
, Bk}, and let  
 be a Borel
úk
g(x)
measurable function on 
. Then the integral of g with respect to  µ is defined as:
úk
mg(x)dµ(x) ' mg%(x)dµ(x) & mg&(x)dµ(x),
(2.7)
where 
 provided that at least one of the integrals
g%(x) ' max{g(x),0}, g&(x) ' max{&g(x),0},
at the right hand side of (2.7) is finite.3
Definition 2.6: The integral of a Borel measurable function g with respect to a probability
measure µ over a Borel set A is defined as 
mAg(x)dµ(x) '
def.
mI(x0A)g(x)dµ(x).

77
All the well-known properties of Riemann integrals carry over to these new integrals. In
particular:
Theorem 2.9: Let f(x) and g(x) be Borel measurable functions on 
, let  µ be a probability
úk
measure on {
,Bk}, and let A be a Borel set in 
. Then 
úk
úk
(a)
mA(αg(x) % βf(x))dµ(x) ' αmAg(x)dµ(x) % βmAf(x)dµ(x).
(b)
For disjoint Borel sets Aj in 
, 
úk
m^4
j'1Aj
g(x)dµ(x) ' '4
j'1mAj
g(x)dµ(x).
(c)
If g(x) $ 0 for all x in A, then mAg(x)dµ(x) $ 0.
(d)
If g(x) $ f(x) for all x in A, then mAg(x)dµ(x) $ mAf(x)dµ(x).
(e)
/0
/0
mAg(x)dµ(x) # mA|g(x)|dµ(x).
(f)
If µ(A) = 0, then mAg(x)dµ(x) ' 0.
(g)
If 
 for a sequence of Borel sets An  then
m|g(x)|dµ(x) < 4 and limn64µ(An) ' 0
 =  0.
limn64mAn
g(x)dµ(x)
Proofs of (a)-(f): Exercise.
Proof of (g): Without loss of generality we may assume that g(x) $ 0. Let
Ck ' {x0ú: k # g(x) < k%1} and Bm ' {x0ú: g(x) $ m} ' ^4
k'mCk.
Then 
múg(x)dµ(x) ' j
4
k'0 mCk
g(x)dµ(x) < 4,
hence
mBm
g(x)dµ(x) ' j
4
k'm mCk
g(x)dµ(x) 6 0 for m 6 4.
(2.8)
Therefore,

78
mAn
g(x)dµ(x) ' mAn_Bm
g(x)dµ(x) % mAn_(ú\Bm)g(x)dµ(x)
# mBm
g(x)dµ(x) % mµ(An),
hence for fixed m,
limsupn64mAn
g(x)dµ(x) # mBm
g(x)dµ(x).
Letting
 part (g) of Theorem 2.9 follows from (2.8). Q.E.D.
m 6 4,
Moreover, there are two important theorems involving limits of a sequence of Borel
measurable functions and their integrals, namely the monotone convergence theorem and the
dominated convergence theorem:
Theorem 2.10: (Monotone convergence) Let  gn  be a non-decreasing sequence of non-negative
Borel measurable functions on 
, i.e., for any fixed 
 0  #   
 # 
 for n =
úk
x 0 úk,
gn(x)
gn%1(x)
1,2,3,..., and let 
 be a probability measure on {
, Bk}. Then 
µ
úk
 limn64mgn(x)dµ(x) ' mlimn64gn(x)dµ(x).
Proof: First, observe from Theorem 2.9(d) and the monotonicity of  gn  that mgn(x)dµ(x)
is monotonic non-decreasing, and that therefore  
 exists (but may be infinite),
limn64mgn(x)dµ(x)
and 
exists (but may be infinite), and is Borel measurable. Moreover, since
g(x) ' limn64gn(x)
for
 it follows easily from Theorem 2.9(d) that 
x 0 úk, gn(x) # g(x),
mgn(x)dµ(x) # mg(x)dµ(x),
hence 
limn64mgn(x)dµ(x) # mg(x)dµ(x).
Thus, it remains to be shown that

79
limn64mgn(x)dµ(x) $ mg(x)dµ(x).
(2.9)
It follows from the definition on the integral
 that (2.9) is true if for any simple
mg(x)dµ(x)
function  f(x) with  0 # f(x) # g(x),
limn64mgn(x)dµ(x) $ mf(x)dµ(x).
(2.10)
Given such a simple function f(x), let for arbitrary  > 0,  
 and
g
An ' {x0úk : gn(x) $ (1&g)f(x)},
let  
. Note that, since  f(x) is simple, M < 4. Moreover, note that  
supxf(x) ' M
limn64µ(úk\An) ' limn64µ {x0úk : gn(x) # (1&g)f(x)} ' 0.
(2.11)
Furthermore, observe that
mgn(x)dµ(x) $ mAn
gn(x)dµ(x) $ (1&g)mAn
f(x)dµ(x)
' (1&g)mf(x)dµ(x) & (1&g)múk\An
f(x)dµ(x) $ (1&g)mf(x)dµ(x) & (1&g)Mµ(úk\An).
(2.12)
It follows now from (2.11) and (2.12) that for arbitrary  
 $ 
g > 0, limn64mgn(x)dµ(x)
 which implies (2.10). Combining (2.9) and (2.10), the theorem follows. Q.E.D.
(1&g)mf(x)dµ(x),
Theorem 2.11: (Dominated  convergence) Let  gn  be sequence of Borel measurable functions on
 such that pointwise in x,   
, and let  
 If
úk
g(x) ' limn64gn(x)
¯g(x) ' supn$1|gn(x)|.
where 
 is a probability measure on {
, Bk}, then
m¯g(x)dµ(x) < 4,
µ
úk
  limn64mgn(x)dµ(x) ' mg(x)dµ(x).
Proof:   Let  
Then 
 is non-decreasing and non-negative,
fn(x) ' ¯g(x) & supm$ngm(x).
fn(x)
and 
 = 
Thus it follows from the condition   
 and 
limn64fn(x)
¯g(x) & g(x).
m¯g(x)dµ(x) < 4

80
Theorems 2.9(a,d)-2.10 that
mg(x)dµ(x) ' limn64msupm$ngm(x)dµ(x) $ limn64supm$nmgm(x)dµ(x)
' limsupn64mgn(x)dµ(x).
(2.13)
Next, let  
Then 
 is non-decreasing and non-negative, and
hn(x) ' ¯g(x) % infm$ngm(x).
hn(x)
 = 
Thus it follows again from the condition   
 and 
limn64hn(x)
¯g(x) % g(x).
m¯g(x)dµ(x) < 4
Theorems 2.9(a,d)-2.10 that
mg(x)dµ(x) ' limn64minfm$ngm(x)dµ(x) # limn64infm$nmgm(x)dµ(x)
' liminfn64mgn(x)dµ(x).
(2.14)
The theorem now follows from (2.13) and (2.14). Q.E.D.
In the statistical and econometric literature you will encounter integrals of the form
 where F  is a distribution function. Since each distribution function F(x) on 
 is 
mAg(x)dF(x),
úk
uniquely associated with  a probability measure µ on  Bk,  one should interpret these integrals as
mAg(x)dF(x) '
def.
mAg(x)dµ(x),
(2.15)
where µ is the probability measure on  Bk  associated with F,  g is a Borel measurable function
on 
, and A is a Borel set in 
.
úk
úk

81
2.4.
General measurability, and integrals of random variables with respect to
probability measures
All the definitions and results in the previous sections carry over to mappings X: Ω6 ú,
where  Ω is a nonempty set, with ö a σ-algebra of subsets of  Ω. Recall that X is a random
variable defined on a probability space {Ω, ö,P}  if for all Borel sets B in ú, {ω 0 Ω: X(ω) 0 B}
0 ö.  Moreover, recall that it suffices to verify this condition for Borel sets of the type  By  =
In this section I will list these generalizations,  with all random variables
(&4,y], y 0 ú.
involved defined on a common probability space {Ω, ö,P}.
Definition 2.7: A random variable X is called simple if it takes the form
 with 
where the Aj’s are disjoint sets in ö.
X(ω) ' 'm
j'1bjI(ω 0 Aj),
m < 4, bj 0 ú,
Compare Definition 2.2. (Verify similarly to Theorem 2.2  that a simple random variable is
indeed a random variable.) Again, we may assume without loss of generality that the bj’s are all
different. 
For example, if  X  has a hypergeometric or binomial distribution, then X is a simple
random variable.
Theorem 2.12: If X and Y are simple random variables, then so are X+Y, X-Y and X.Y.  If in
addition Y is non-zero with probability 1, then X/Y is a simple random variable.
Proof: Similar to Theorem 2.3.

82
Theorem 2.13: Let Xj  be a sequence of random variables. Then  
 
 
max1#j#n Xj, min1#j#n Xj,
 
 and 
 are random variables. If  
supn$1 Xn, infn$1 Xn, limsupn64 Xn,
liminfn64 Xn
 for all ω in a set A in ö with P(A) = 1, then X is a random variable.
limn64 Xn(ω) ' X(ω)
Proof: Similar to Theorem 2.4.
Theorem 2.14: A mapping X: 
 is a random variable if and only if there exists a sequence
Ω6 ú
Xn  of simple random variables such that  
 for all ω in a set A in ö with
limn64 Xn(ω) ' X(ω)
P(A) = 1.
Proof: Similar to Theorem 2.7.
Similarly to Definitions 2.3, 2.4, 2.5 and 2.6, we may define integrals of a random
variable X with respect to the probability measure P as follows, in four steps.
Definition 2.8: Let X be a simple random variable: 
 say. Then the
X(ω) ' 'm
j'1bjI(ω 0 Aj),
integral of X with respect of P is defined as  
4
mX(ω)dP(ω) '
def.
'm
j'1bjP(Aj).
Definition 2.9: Let X be a non-negative random variable (with probability 1). Then the integral
of X with respect of P is defined as   
  where the
mX(ω)dP(ω) '
def.
sup0#X(#X mX(ω)(dP(ω),
supremum is taken over all simple random variables X*  satisfying 
 with probability
0 # X( # X
1.

83
Definition 2.10: Let X be a random variable. Then the integral of X with respect of P is defined
as    
  where
 = 
 and 
 =
mX(ω)dP(ω) '
def.
mX%(ω)dP(ω) & mX&(ω)dP(ω),
X%
max{X,0}
X&
 provided that at least one of the latter two integrals is finite.
max{&X,0},
Definition 2.11: The integral of a random variable X with respect to a probability measure  P
over a set A in ö  is defined as  mAX(ω)dP(ω) '
def.
mI(ω 0 A)X(ω)dP(ω).
Theorem 2.15: Let X and Y be random variables, and let A be a set in  ö. Then 
(a)
mA(αX(ω) % βY(ω))dP(ω) ' αmAX(ω)dP(ω) % βmAY(ω)dP(ω).
(b)
For disjoint  sets Aj in  ö, m^4
j'1Aj
X(ω)dP(ω) ' '4
j'1mAj
X(ω)dP(ω).
(c)
If X (ω) $ 0 for all ω in A, then mAX(ω)dP(ω) $ 0.
(d)
If X (ω) $ Y (ω)  for all ω in A, then mAX(ω)dP(ω) $ mAY(ω)dP(ω).
(e)
/0
/0
mAX(ω)dP(ω) # mA|X(ω)|dP(ω).
(f)
If P(A) = 0, then mAX(ω)dP(ω) ' 0.
(g)
If  
 and for a sequence of sets  An  in 
  
 then 
m|X(ω)|dP(ω) < 4
ö, limn64P(An) ' 0,
limn64mAn
X(ω)dP(ω) ' 0.
Proof: Similar to Theorem 2.9. 
Also the monotone and dominated convergence theorems carry over:
Theorem 2.16: Let Xn be a monotonic non-decreasing sequence of non-negative random
variables defined on the probability space {Ω, ö,P}, i.e., there exists a set 
 with P(A) =
A 0 ö

84
1 such that for all 
Then
ω 0 A, 0 # Xn(ω) # Xn%1(ω), n ' 1,2,3,....
 limn64mXn(ω)dP(ω) ' mlimn64Xn(ω)dP(ω).
Proof: Similar to Theorem 2.10.
Theorem 2.17: Let Xn be a sequence of random variables defined on the probability space {Ω,
ö,P} such that for all 
 in a set 
, 
 Let
ω
A 0 ö with P(A) ' 1 Y(ω) ' limn64Xn(ω).
 If  
 then  
¯X ' supn$1Xn.
m
¯X(ω)dP(ω) < 4
limn64mXn(ω)dP(ω) ' mY(ω)dP(ω).
Proof: Similar to Theorem 2.11.
Finally, note that the integral of a random variable with respect to the corresponding
probability measure P is related to the definition of the integral of a Borel measurable function
with respect to a probability measure µ:
Theorem 2.18: Let 
 be the probability measure induced by the random variable X. Then
µX
 Moreover, if g is a Borel measurable real function on úk, and X is a
mX(ω)dP(ω) ' mxdµX(x).
k-dimensional random vector with induced probability measure 
, then 
 =
µX
mg(X(ω))dP(ω)
 Furthermore, denoting in the latter case Y= g(X), with  
  the probability
mg(x)dµX(x).
µY
measure induced by Y,  we have  mY(ω)dP(ω) ' mg(X(ω))dP(ω) ' mg(x)dµX(x) ' mydµY(y).
Proof: Let X be a simple random variable:  
 say, and recall that
X(ω) ' 'm
j'1bjI(ω 0 Aj),
without loss of generality we may assume that the bj ‘s are all different. Each of the disjoint sets

85
Aj  are associated with disjoint Borel sets Bj such that 
 (for example, let 
Aj ' {ω0Ω: X(ω) 0 Bj}
Bj = {bj}). Then 
mX(ω)dP(ω) ' 'm
j'1bjP(Aj) ' 'm
j'1bjµX(Bj) ' mg((x)dµX(x),
where 
 is a simple function such that
g((x) ' 'm
j'1bjI(x 0 Bj)
g((X(ω)) ' 'm
j'1bjI(X(ω) 0 Bj) ' 'm
j'1bjI(ω 0 Aj) ' X(ω).
Therefore, in this case the Borel set 
 has  
 measure zero:
 and
C ' {x: g((x) … x}
µX
µX(C) ' 0,
consequently,
mX(ω)dP(ω) ' mú\Cg((x)dµX(x) % mCg((x)dµX(x) ' mú\CxdµX(x) ' mxdµX(x).
(2.16)
The rest of the proof is left as an exercise. Q.E.D.
2.5.
Mathematical expectation
With these new integrals introduced, we can now answer the second question stated at the
end of the introduction: How to define the mathematical expectation if the distribution of X is
neither discrete nor absolutely continuous:
Definition 2.12: The mathematical expectation of a random variable X is defined as:
 or equivalently as:  
 [cf. (2.15)], where F is the
E(X) ' mX(ω)dP(ω),
E(X) ' mxdF(x)
distribution function of  X,   provided that the integrals involved are defined. Similarly, if g(x) is
a Borel measurable function on  úk and X is a random vector in   úk  then equivalently,

86
 = 
  provided that the integrals involved are defined.
E[g(X)] ' mg(X(ω))dP(ω)
mg(x)dF(x),
Note that the latter part of Definition 2.12 covers both examples (2.1) and (2.3).
As motivated in the introduction, the mathematical expectation 
may be
E[g(X)]
interpreted as the limit of the average pay-off of a repeated game with pay-off function g. This is
related to the law of large numbers which we will discuss later, in Chapter 7:  If X1, X2, X3,.. ......
is a sequence of independent random variables or vectors each distributed the same as X, and g is
a Borel measurable function such that  
 then  
 =
E[|g(X)|] < 4,
P limn64(1/n)'n
j'1g(Xj) ' E[g(X)]
1. 
There are a few important special cases of the function g, in particular the variance of X,
which measures the variation of X around its expectation E(X), and the covariance of a pair of
random variables X and Y, which measures how X and Y fluctuate together around their
expectations: 
Definition 2.13: The m’s moment (m = 1,2,3,.... ) of a random variable X is defined as: E(Xm),
and the m’s central moment of X is defined by  
 where 
   The second
E(*X&µx*m),
µx ' E(X).
central moment is called the variance of  X :   
 var(X) ' E[(X & µx)2] ' σ2
x,
say. The covariance of a pair (X,Y) of random variables is defined as:
 
 
cov(X,Y) ' E[(X & µx)(Y & µy)],
where 
is the same as before, and 
 The correlation (coefficient) of a pair (X,Y) of
µx
µy ' E(Y).

87
random variables is defined as: 
corr(X,Y) '
cov(X,Y)
var(X) var(Y)
' ρ(X,Y),
say.
The correlation coefficient measures the extent to which Y can be approximated by a
linear function of  X, and vice versa. In particular, 
If exactly Y ' α % βX then corr(X,Y) ' 1 if β > 0, corr(X,Y) ' &1 if β < 0.
(2.17)
Moreover,
Definition 2.14: Random variables X and Y said to be uncorrelated if cov(X,Y) = 0.  A sequence
of random variables  Xj  is uncorrelated if for all  i … j, Xi and Xj are uncorrelated.
Furthermore, it is easy to verify that
Theorem 2.19: If X1,.....,Xn are uncorrelated, then var 'n
j'1Xj ' 'n
j'1var(Xj).
Proof: Exercise.

88
2.6.
Some useful inequalities involving mathematical expectations
There are a few inequalities that will prove to be useful later on, in particular Chebishev’s
inequality, Holder’s inequality, Liapounov’s inequality, and Jensen’s inequality.
2.6.1. Chebishev’s inequality
Let X be a non-negative random variable with distribution function F(x), and let
 be a
φ(x)
monotonic increasing non-negative Borel measurable function on [0,4). Then for arbitrary
g > 0,
E[φ(X)] ' mφ(x)dF(x) ' m{φ(x)>φ(g)}φ(x)dF(x) % m{φ(x)#φ(g)}φ(x)dF(x)
$ m{φ(x)>φ(g)}φ(x)dF(x) $ φ(g)m{φ(x)>φ(g)}dF(x) ' φ(g)m{x>g}dF(x) ' φ(g)(1 & F(g)),
(2.18)
hence
P(X > g) ' 1 & F(g) # E[φ(X)]
φ(g)
.
(2.19)
In particular, it follows from (2.19)  that for a random variable Y with expected value  µy ' E(Y)
and variance 
,
σ2
y
P
ω0Ω: *Y(ω)&µy* >
σ2
y /g
# g.
(2.20)
2.6.2
Holder’s inequality
Holder’s inequality is based on the fact that ln(x) is a concave function on (0,4):  for 0 < a
< b, and 0 # λ # 1,
 hence
ln(λa % (1&λ)b) $ λln(a) % (1&λ)ln(b),
λa % (1&λ)b $ a λb 1&λ.
(2.21)

89
Now let X and Y be random variables, and put  
where p
a ' *X*p/E(*X*p), b ' *Y*q/E(*Y*q),
> 1, and  
 Then it follows from (2.21), with λ = 1/p and 1-λ = 1/q , that
p &1 % q &1 ' 1.
p &1
*X*p
E(*X*p)
% q &1
*Y*q
E(*Y*q)
$
*X*p
E(*X*p)
1/p
*Y*q
E(*Y*q)
1/q
'
*X.Y*
E(*X*p) 1/p E(*Y*q) 1/q .
Taking expectations yields Holder’s inequality:
E(*X.Y*) # E(*X*p) 1/p E(*Y*q) 1/q, where p > 1 and 1
p % 1
q ' 1.
(2.22)
For the case p = q = 2 inequality (2.22) reads 
 which is known as the
E(*X.Y*) #
E(X 2) E(Y 2),
Cauchy-Schwartz inequality.
2.6.3
Liapounov’s inequality
Liapounov’s inequality follows from Holder’s inequality (2.22) by replacing Y with 1:
E(*X*) # E(*X*p) 1/p, where p $ 1.
2.6.4
Minkowski’s inequality
If for some p  $ 1, 
 then
E[|X|p] < 4 and E[|Y|p] < 4
E(*X % Y*) # E(*X*p) 1/p % E(*Y*p) 1/p.
(2.23)
This inequality is due to Minkowski. For p = 1 the result is trivial. Therefore, let p > 1. First note
that  
 hence
E[|X % Y|p] # E[(2.max(|X|,|Y|))p] ' 2pE[max(|X|p,|Y|p)] # 2pE[|X|p % |Y|p] < 4,
we may apply Liapounov’s inequality:
E(*X % Y*) # E(*X % Y*p) 1/p.
(2.24)
Next, observe that

90
E(*X % Y*p) ' E(*X % Y*p&1*X % Y*) # E(*X % Y*p&1*X*) % E(*X % Y*p&1*Y*).
(2.25)
Let  q = p /(p-1). Since 1/q + 1/p = 1 it follows from Holder’s inequality that 
E(*X % Y*p&1*X*) # E(*X % Y*(p&1)q) 1/q E(|X|p) 1/p # E(*X % Y*p) 1&1/p E(|X|p) 1/p,
(2.26)
and similarly,
E(*X % Y*p&1*Y*) # E(*X % Y*p) 1&1/p E(|Y|p) 1/p.
(2.27)
Combining (2.24),  (2.25), (2.26) and (2.27),  Minkowski’s inequality (2.23) follows.
2.6.5
Jensen’s inequality
A real function 
 on ú is called convex if for all  a, b 
 and  0 # λ # 1, 
φ(x)
0 ú
 φ(λa % (1&λ)b) # λφ(a) % (1&λ)φ(b).
It follows by induction that then also
φ 'n
j'1λjaj # j
n
j'1
λjφ(aj), where λj > 0 for j ' 1,..,n, and j
n
j'1
λj ' 1.
(2.28)
Consequently, it follows from (2.28) that for a simple random variable X,
 
φ(E(X)) # E(φ(X)) for all convex real functions φ on ú.
(2.29)
This is Jensen’s inequality. Since (2.29) holds for simple random variables, it holds for all
random variables.  Similarly we have
φ(E(X)) $ E(φ(X)) for all concave real functions φ on ú.
2.7.
Expectations of products of  independent random variables
Let X and Y be independent random variables, and let f and g be Borel measurable
functions on  
. I will show now at then 
ú
E[f(X)g(Y)] ' (E[f(X)])(E[g(Y)]).
(2.30)

91
In general (2.30) does not hold, although there are cases where (2.30) holds for dependent
X and Y.  As an example of a case where (2.30) does not hold, let X = U0.U1 and Y = U0.U2,
where U0, U1 and U2 are independent uniformly [0,1] distributed, and let f(x) ' x, g(x) ' x.
The joint density of U0, U1 and U2 is: 
 
h(u0,u1,u2) ' 1 if (u0,u1,u2)T 0 [0,1]×[0,1]×[0,1], h(u0,u1,u2) ' 0 elsewhere,
hence  
 
E[f(X)g(Y)] ' E[X.Y] ' E[U 2
0 U1U2] ' m
1
0 m
1
0 m
1
0 u 2
0 u1u2du0du1du2 ' m
1
0 u 2
0 du0m
1
0 u1du1m
1
0 u2du2
' (1/3)×(1/2)×(1/2) ' 1/12,
whereas
 
E[f(X)] ' E[X] ' m
1
0 m
1
0 m
1
0 u0u1du0du1du2 ' m
1
0 u0du0m
1
0 u1du1m
1
0 du2 ' 1/4,
and similarly, E[g(Y)] = E[Y] = 1/4.
As an example of dependent random variables X and Y for which (2.30) holds, let now X
=  
 and  Y = 
 where  
 are the same as before, and
U0(U1 & 0.5)
U0(U2 & 0.5),
U0, U1, and U2
again 
 Then it is easy to show that 
f(x) ' x, g(x) ' x.
E[X.Y] ' E[X] ' E[Y] ' 0.
In order to prove (2.30) for independent random variables X and Y, let f and g be simple
functions:
f(x) ' 'm
i'1αiI(x 0 Ai), g(x) ' 'n
j'1βjI(x 0 Bj),
where the Ai’s are disjoint Borel sets, and the Bj’s are disjoint Borel sets. Then

92
E[f(X)g(Y)] ' E 'm
i'1'n
j'1αiβjI(X 0 Ai and Y 0 Bj)
' m 'm
i'1'n
j'1αiβjI(X(ω) 0 Ai and Y(ω) 0 Bj) dP(ω)
' 'm
i'1'n
j'1αiβjP {ω 0 Ω: X(ω) 0 Ai}_{ω 0 Ω: Y(ω) 0 Bj}
' 'm
i'1'n
j'1αiβjP {ω 0 Ω: X(ω) 0 Ai} P {ω 0 Ω: Y(ω) 0 Bj}
' 'm
i'1αiP {ω 0 Ω: X(ω) 0 Ai} 'n
j'1βjP {ω 0 Ω: Y(ω) 0 Bj}
' E[f(X)] E[g(Y)] ,
because by the independence of X and Y, 
 From
P(X 0 Ai and Y 0 Bj) ' P(X 0 Ai)P(Y 0 Bj).
this result it follows more generally:
Theorem 2.20: Let X and Y be random vectors in 
 respectively. Then X and Y are
úp and úq,
independent if and only if  
  for all Borel measurable functions 
E[f(X)g(Y)] ' (E[f(X)])(E[g(Y)])
f and g on  
 respectively, for which  the expectations involved are defined.
úp and úq,
This theorem implies that independent random variables are uncorrelated. The reverse,
however, is in general not true. A counter example is the case I have considered before, namely 
X =  
 and  Y = 
 where  
  are independent uniformly
U0(U1 & 0.5)
U0(U2 & 0.5),
U0, U1, and U2
[0,1] distributed.  In this case 
 hence cov(X,Y) = 0, but X and Y are
E[X.Y] ' E[X] ' E[Y] ' 0,
dependent, due to the common factor U0. The latter can be shown formally in different ways, but
the easiest way is to verify that, for example,  
, so that the
E[X 2.Y 2] … (E[X 2])(E[Y 2])
dependence of X and Y follows from Theorem 2.20.

93
2.8. 
Moment generating functions and characteristic functions
2.8.1
Moment generating functions
The moment generating function of a bounded random variable X , i.e., P[|X|  # M] = 1
for some positive real number M < 4,  is defined as the function
m(t) ' E[exp(t.X)], t 0 ú,
(2.31)
where the argument t is non-random. More generally:
Definition 2.15: The moment generating function of a random vector X in  úk is defined by 
 where Τ is the set of non-random vectors t for which the
m(t) ' E[exp(t TX)] for t 0 Τ d úk,
moment generating function exists and is finite. 
For bounded random variables the moment generating function exists and is finite for all
values of t. In particular, in the univariate bounded case we can write
m(t) ' E[exp(t.X)] ' E j
4
k'0
t kX k
k!
' j
4
k'0
t kE[X k]
k!
.
It is easy to verify that the j-th derivative of m(t) is:
m (j)(t) ' d jm(t)
(dt)j
' j
4
k'j
t k&jE[X k]
(k&j)!
' E[X j] % j
4
k'j%1
t k&jE[X k]
(k&j)!
(2.32)
hence the  j-th moment of X is  
m (j)(0) ' E[X j].
(2.33)
This is the reason for calling  m(t) the  “moment generating function”. 
Although the moment generating function is a handy tool for computing moments of a

94
distribution, its actual importance is due to the fact that the shape of the moment generating
function in an open neighborhood of zero uniquely characterizes the distribution of a random
variable. In order to show this, we need the following result.
Theorem 2.21: The distributions of two random vectors X and Y in úk are the same if and only if
for all bounded continuous functions 
 on  úk,  
φ
E[φ(X)] ' E[φ(Y)].
Proof: I shall only prove this theorem for the case where X and Y are random variables: k
= 1. Note that the “only if” case follows from  the definition of expectation. 
Let  F(x) be the distribution function of X  and let  G(y) be the distribution function of Y. 
Let a < b be arbitrary continuity points of F(x) and G(y), and define
φ(x) '
'
0
if
x $ b,
'
1
if
x < a,
'
b&x
b&a
if a # x < b.
(2.34)
Clearly, (2.34) is a bounded continuous function, and therefore by assumption we have E[φ(X)]
=  
 Now observe from (2.34) that
E[φ(Y)].
E[φ(X)] ' mφ(x)dF(x) ' F(a) % m
b
a
b&x
b&adF(x) $ F(a)
and
E[φ(X)] ' mφ(x)dF(x) ' F(a) % m
b
a
b&x
b&a
dF(x) # F(b).
Similarly,
E[φ(Y)] ' mφ(y)dG(y) ' G(a) % m
b
a
b&x
b&a
dG(x) $ G(a)
and

95
E[φ(X)] ' mφ(y)dG(y) ' G(a) % m
b
a
b&x
b&adG(x) # G(b).
Combining these inequalities with 
it follows that for arbitrary continuity
E[φ(X)] ' E[φ(Y)]
points  a < b of F(x) and G(y),
G(a) # F(b), F(a) # G(b).
(2.35)
Letting 
 it follows from (2.35) that 
 Q.E.D.
b 9 a
F(a) ' G(a).
Now assume that the random variables X and Y are discrete, and take with probability 1
the values 
 Without loss of generality we may assume that xj = j, i.e.,
x1,.....,xn.
P[X 0 {1,2,...,n}] ' P[Y 0 {1,2,...,n}] ' 1.
Suppose that all the moments of X and Y match: For k = 1,2,3,...., 
. I will show
E[X k] ' E[Y k]
that then for an arbitrary bounded continuous function 
 on ú,  
φ
E[φ(X)] ' E[φ(Y)].
Denoting pj = P[X = j], qj = P[Y = j] we can write
E[φ(X)] ' j
n
j'1
φ(j)pj,
E[φ(Y)] ' j
n
j'1
φ(j)qj.
It is always possible to construct a polynomial  
 such that 
 for j =
ρ(t) ' 'n&1
k'0ρkt k
φ(j) ' ρ(j)
1,...,n, by solving
1 1
1
þ
1
1 2
22 þ 2n&1
! !
!
"
!
1 n n 2 þ n n&1
ρ0
ρ1
!
ρn&1
'
φ(1)
φ(2)
!
φ(n)
.
Then
E[φ(X)] ' j
n
j'1 j
n&1
k'0
ρk j kpj ' j
n&1
k'0
ρk j
n
j'1
j kpj ' j
n&1
k'0
ρk E[X k]
and similarly
E[φ(Y)] ' j
n&1
k'0
ρk E[Y k].

96
Hence, it follows from Theorem 2.21 that if all the corresponding moments of X and Y are the
same, then the distributions of X and Y are the same. Thus if the moment generating functions of
X and Y coincide on a open neighborhood of zero, and if all the moments of X and Y are finite,
then it follows from (2.33) that all the corresponding moments of X and Y are the same:
Theorem 2.22: If the random variables X and Y are discrete, and take with probability 1 only a
finite number of values, then the distributions of X and Y are the same if and only if  the moment
generating functions of X and Y coincide on an arbitrary small open neighborhood of zero.
However, this result also applies without the conditions that X and Y are discrete and take only a
finite number of values, and for random vectors as well, but the proof is complicated and
therefore omitted:
Theorem 2.23: If the moment generating functions mX(t) and mY(t) of the random vectors X and
Y in  úk  are defined and finite in an open neighborhood  
 =  
  of the
N0(δ)
{x 0 úk: 2x2 < δ}
origin of  úk,  then the distributions of X and Y are the same if and only if  
 for all 
mX(t) ' mY(t)
t 0 N0(δ).
2.8.2
Characteristic functions
The disadvantage of the moment generating function is that is may not be finite in an
arbitrarily small open neighborhood of zero. For example, if X has a standard Cauchy
distribution, i.e., X  has density

97
f(x) '
1
π(1%x 2)
,
(2.36)
then
m(t) ' m
4
&4
exp(t.x)f(x)dx
' 4 if t … 0,
' 1 if t ' 0.
(2.37)
There are many other distributions with the same property as (2.37), hence the moment
generating functions in these cases are of no use for comparing distributions.
The solution to this problem is to replace t in (2.31) with i.t, where  
 The
i '
&1.
resulting function n(t) = m(i.t) is called the characteristic function of the random variable X:
n(t) ' E[exp(i.t.X)], t 0 ú.
More generally,
Definition 2.16: The characteristic  function of a random vector X in  úk is defined by 
 where the argument t is non-random. 
n(t) ' E[exp(i.t TX)], t 0 úk,
The characteristic function is bounded, because 
 See
exp(i.x) ' cos(x) % i.sin(x).
Appendix III. Thus, the characteristic function in Definition 2.16 can be written as
n(t) ' E[cos(t TX)] % i.E[sin(t TX)], t 0 úk.
Note that by the dominated convergence theorem (Theorem 2.11), limt60 n(t) ' 1 ' n(0),
hence a characteristic function is always continuous in t = 0. 
Replacing moment generating functions with characteristic functions, Theorem 2.23 now
becomes:

98
Theorem 2.24: Random variables or vectors have the same distribution if and only if their
characteristic functions are identical.
The proof of this theorem is complicated, and is therefore given in Appendix 2.A at the end of
this chapter. The same applies to the following useful result, which is known as the inversion
formula for characteristic functions:
Theorem 2.25: Let X be a random vector in  úk  with characteristic function n(t).  If    n(t) is
absolutely integrable, i.e., 
 then the distribution of X is absolutely continuous
múk*n(t)*dt < 4,
with joint density   f(x) ' (2π)&k
múkexp(&i.t Tx)n(t)dt.
2.9.
Exercises
1.
Prove that the collection  D in the proof of Theorem 2.1 is a σ-algebra.
2.
Prove Theorem 2.3.
3.
Prove Theorem 2.4 for the max, sup, limsup and lim cases.
4.
Complete the proof of Theorem 2.5, by proving that 
 pointwise in
gn(x) ' limm64gn,m(x)
x, and  
 pointwise in x.
g(x) ' limn64gn(x)
5.
Why is it true that if g is Borel measurable, then so are 
 in  (2.6)?
g% and g&
6.
Prove Theorem 2.7.
7.
Prove Theorem 2.8.
8.
Let 
 if x is rational,  
 if x is irrational. Prove that 
 is Borel
g(x) ' x
g(x) ' &x
g(x)
measurable.

99
9.
Prove parts (a)-(f) of Theorem 2.9 for simple functions
 
 
g(x) ' 'n
i'1aiI(x 0 Bi), f(x) ' 'm
j'1bjI(x 0 Cj).
10.
Why can you conclude from exercise 9 that parts (a)-(f) of Theorem 2.9 hold for arbitrary
non-negative Borel measurable functions?
11.
Why can you conclude from exercise 10 that Theorem 2.9 holds for arbitrary Borel
measurable functions, provided that the integrals involved are defined?
12.
From which result on probability measures does (2.11)  follow?
13.
Determine for each inequality in (2.12)  which part of  Theorem 2.9 has been used.
14.
Why do we need the condition in Theorem 2.11 that m¯g(x)dµ(x) < 4?
15.
Note that we cannot generalize Theorem 2.5 to random variables, because something
missing prevents us from defining  a continuous mapping  X: 
. What is missing?
Ω6 ú
16.
Verify (2.16), and complete the proof of Theorem 2.18.
17.
Prove equality (2.2).
18.
 Show that 
 
 and -1 #
var(X) ' E(X 2) & (E(X))2, cov(X,Y) ' E(X.Y) & (E(X))(E(Y)),
 # 1. Hint: Derive the latter result from 
 for all λ.
corr(X,Y)
var(Y & λX) $ 0
19.
Prove (2.17).
20.
Which parts of Theorem 2.15 have been used in (2.18)?
21.
How does (2.20) follow from (2.19)?
22.
Why does it follows from (2.28) that (2.29) holds for simple random variables?
23
Prove Theorem 2.19.
24.
Complete the proof of Theorem 2.20 for the case p = q = 1.
25.
Let   X =  
 and  Y = 
 where  
  are
U0(U1 & 0.5)
U0(U2 & 0.5),
U0, U1, and U2

100
independent uniformly [0,1] distributed. Show that 
.
E[X 2.Y 2] … (E[X 2])(E[Y 2])
26.
Prove that if (2.29) holds for simple random variables, it holds for all random variables.
Hint: Use the fact that convex and concave functions are continuous (See Appendix II).
27.
Derive the moment generating functions of the Binomial (n,p) distribution.
28.
Use the results in exercise 27 to derive the expectation and variance of the Binomial (n,p)
distribution.
29.
Show that the moment generating function of the Binomial (n,p) distribution converges
pointwise in t to the moment generating function of the Poisson (λ) distribution if n 6 4 and p 90
such that n.p 6 λ.
30.
Derive the characteristic function of the uniform [0,1] distribution. Is the inversion
formula for characteristic functions applicable in this case ?
31.
If the random variable X has characteristic function exp(i.t), what is the distribution of X?
32.
Show that the characteristic function of a random variable X is real-valued if and only if
the distribution of X is symmetric, i.e., X and !X  have the same distribution.
33.
Use the inversion formula for characteristic functions to show that 
 is
n(t) ' exp(&*t*)
the characteristic function of the standard Cauchy distribution [see (2.36) for the density
involved]. 
Hints: Show first, using Exercise 32 and the inversion formula, that     
f(x) ' π&1
m
4
0 cos(t.x)exp(&t)dt,
and then use integration by parts.

101
Appendix
2.A. 
Uniqueness of characteristic functions
In order to understand characteristic functions, you need to understand the basics of
complex analysis, which is provided in Appendix III. Therefore, it is recommended to read
Appendix III first.  
In the univariate case, Theorem 2.24 is a straightforward corollary of the following link
between a probability measure and its characteristic function.
Theorem 2.A.1: Let µ be a probability measure on the Borel sets in ú with characteristic
function n, and let a < b be continuity points of µ: 
 Then 
µ({a}) ' µ({b}) ' 0.
µ((a,b]) ' lim
T64
1
2π m
T
&T
exp(&i.t.a)&exp(&i.t.b)
i.t
n(t)dt.
(2.38)
Proof: Using the definition of characteristic function, we can write
m
T
&T
exp(&i.t.a)&exp(&i.t.b)
i.t
n(t)dt ' m
T
&Tm
4
&4
exp(i.t(x&a))&exp(i.t.(x&b))
i.t
dµ(x)dt
' m
T
&T
lim
M64 m
M
&M
exp(i.t(x&a))&exp(i.t.(x&b))
i.t
dµ(x)dt
(2.39)
Next, observe that
/00000
/00000
m
M
&M
exp(i.t(x&a))&exp(i.t.(x&b))
i.t
dµ(x) # /00
/00
exp(&i.ta)&exp(&i.t.b)
i.t
µ([&M,M])
# |exp(&i.t.a)&exp(&i.t.b)|
|t|
'
2(1 & cos(t.(b&a))
t 2
# b&a

102
Therefore, it follows from the bounded convergence theorem that 
m
T
&T
exp(&i.t.a)&exp(&i.t.b)
i.t
n(t)dt ' lim
M64m
T
&T m
M
&M
exp(i.t(x&a))&exp(i.t.(x&b))
i.t
dµ(x)dt
lim
M64 m
M
&Mm
T
&T
exp(i.t(x&a))&exp(i.t.(x&b))
i.t
dtdµ(x) ' m
4
&4 m
T
&T
exp(i.t(x&a))&exp(i.t.(x&b))
i.t
dt dµ(x
(2.40)
The integral between square brackets can be written as
m
T
&T
exp(i.t(x&a))&exp(i.t.(x&b))
i.t
dt ' m
T
&T
exp(i.t(x&a))&1
i.t
dt & m
T
&T
exp(i.t.(x&b))&1
i.t
dt
' m
T
&T
cos(t(x&a))&1%i.sin(t(x&a))
i.t
dt & m
T
&T
cos(t(x&b))&1%i.sin(t(x&b))
i.t
dt
' m
T
&T
sin(t(x&a))
t
dt & m
T
&T
sin(t(x&b))
t
dt ' 2m
T
0
sin(t(x&a))
t(x&a)
dt(x&a) & 2m
T
0
sin(t(x&b))
t(x&b)
dt(x&b)
' 2 m
T(x&a)
0
sin(t)
t
dt & 2 m
T(x&b)
0
sin(t)
t
dt ' 2sgn(x&a) m
T|x&a|
0
sin(t)
t
dt & 2sgn(x&b) m
T|x&b|
0
sin(t)
t
dt,
(2.41)
where sgn(x) = 1 if x > 0, sgn(0) = 0, and sgn(x) =  !1 if x < 0. The last two integrals in (2.41) are
of the form
m
x
0
sin(t)
t
dt ' m
x
0
sin(t)m
4
0
exp(&t.u)dudt ' m
4
0m
x
0
sin(t)exp(&t.u)dtdu
' m
4
0
du
1%u 2 & m
4
0
[cos(x) % u.sin(x)] exp(&x.u)
1%u 2
du.
(2.42)
where the last equality follows from integration by parts:

103
m
x
0
sin(t)exp(&t.u)dt ' &m
x
0
dcos(t)
dt
exp(&t.u)dt ' cos(t)exp(&t.u) x
0 & u.m
x
0
cos(t)exp(&t.u)dt
' 1 & cos(x)exp(&x.u) & u.m
x
0
dsin(t)
dt
exp(&t.u)dt
'
1 & cos(x)exp(&x.u) & u.sin(x)exp(&x.u) & u 2
m
x
0
sin(t)exp(&t.u)dt.
Clearly, the second integral at the right-hand side of (2.42) is bounded in x > 0, and converges to
zero as x 64. The first integral at the right-hand side of (2.42) is
m
4
0
du
1%u 2 ' m
4
0
darctan(u) ' arctan(4) ' π/2.
Thus, the integral  (2.42) is bounded, hence so is (2.41), and
lim
T64 m
T
&T
exp(i.t(x&a))&exp(i.t.(x&b))
i.t
dt' π[sgn(x&a) & sgn(x&b)]
(2.43)
It follows now from (2.39) , (2.40), (2.43) and the dominated convergence theorem that
lim
T64
1
2π m
T
&T
exp(&i.t.a)&exp(&i.t.b)
i.t
n(t)dt ' 1
2m[sgn(x&a) & sgn(x&b)]dµ(x)
' µ((a,b)) % 1
2
µ({a}) % 1
2
µ({b}).
(2.44)
The last equality in (2.44) follow from the fact that 
sgn(x&a) & sgn(x&b) '
0 if x < a or x > b,
1 if x ' a or x ' b,
2 if a < x < b.
The result (2.38) now follows from (2.44) and the condition 
 Q.E.D.
µ({a}) ' µ({b}) ' 0.
Note that  (2.38) also reads as

104
F(b) & F(a) ' lim
T64
1
2π m
T
&T
exp(&i.t.a)&exp(&i.t.b)
i.t
n(t)dt,
(2.45)
where F is the distribution function corresponding to the probability measure µ. 
Next, suppose that n is absolutely integrable: 
 Then (2.45) can be
m
4
&4*n(t)*dt < 4.
written as
F(b) & F(a) '
1
2π m
4
&4
exp(&i.t.a)&exp(&i.t.b)
i.t
n(t)dt,
and it follows from the dominated convergence theorem that
F )(a) ' lim
b9a
F(b) & F(a)
b&a
'
1
2π m
4
&4
lim
b9a
1&exp(&i.t.(b&a))
i.t.(b&a)
exp(&i.t.a)n(t)dt
'
1
2π m
4
&4
exp(&i.t.a)n(t)dt.
This proves Theorem 2.25 for the univariate case.
In the multivariate case Theorem 2.A.1 becomes:
Theorem 2.A.2: Let µ be a probability measure on the Borel sets in  úk  with characteristic
function n. Let 
 where  aj < bj  for j = 1,2,...,k, and let MB be the border of B, i.e., 
B ' ×k
j'1(aj,bj],
 If  
 then 
MB ' {×k
j'1[aj,bj]}\{×k
j'1(aj,bj)}.
µ(MB) ' 0
µ(B) ' lim
T164
.... lim
Tk64
m
×k
j'1(&Tj,Tj)
k
k
j'1
exp(&i.tj.aj)&exp(&i.tj.bj)
i.2πtj
n(t)dt,
(2.46)
where t = (t1,...,tk)T.
This result proves Theorem 2.24 for the general case.

105
1.
The actual construction of such a counter example is difficult, though, but not impossible.
2.
The notation
 is somewhat odd, because 
 has no meaning. It would be
mg(x)dµ(x)
µ(x)
better to denote the integral involved by 
 (which some authors do), where dx
mg(x)µ(dx)
represents a Borel set. The current notation, however, is the most common, and therefore adopted
here too.
3.
Because 4 ! 4 is not defined.
4.
Again, the notation
 is odd because 
 has no meaning. Some authors use
mX(ω)dP(ω)
P(ω)
the notation 
, where 
 represents a set in
 The former notation is the most
mX(ω)P(dω)
dω
ö.
common, and therefore adopted.
Moreover, if  
 then (2.46) becomes
múk*n(t)*dt < 4
µ(B) ' m
úk
k
k
j'1
exp(&i.tj.aj)&exp(&i.tj.bj)
i.2πtj
n(t)dt,
and by the dominated convergence theorem we may take partial derivatives inside the integral:
Mkµ(B)
Ma1.....Mak
'
1
(2π)km
úk
exp(&i.t Ta)n(t)dt,
(2.47)
where a = (a1,...,ak)T.  The latter is just the density corresponding to F in point a. Thus, (2.47)
proves Theorem 2.25. 
Endnotes

106
Chapter 3
Conditional Expectations
3.1.
Introduction
Roll a dice, and let the outcome be Y. Define the random variable X = 1 if Y is even, and
X = 0 if Y is odd. The expected value of Y is E[Y] = (1+2+3+4+5+6)/6 = 3.5. But what would the
expected value of Y be if it is revealed that the outcome is even: X = 1? The latter information
implies that Y is either 2, 4 or 6, with equal probabilities 1/3, hence the expected value of Y,
conditional on the event X = 1, is E[Y|X=1] = (2+4+6)/3 = 4. Similarly, if it is revealed that X = 0,
then Y is either 1, 3, or 5, with equal probabilities 1/3, hence the expected value of Y, conditional
on the event X = 0, is E[Y|X=0] = (1+3+5)/3 = 3. Both results can be captured in a single
statement:
E[Y|X] ' 3%X.
(3.1)
In this example the conditional probability of Y = y, given X = x, is1
P(Y ' y|X'x) ' P(Y ' y and X'x)
P(X'x)
' P({y}_{2,4,6})
P({2,4,6})
'
P({y})
P({2,4,6})
' 1/6
1/2
' 1
3
if x ' 1 and y 0 {2,4,6}
' P({y}_{2,4,6})
P({2,4,6})
'
P(i)
P({2,4,6})
' 0 if x ' 1 and y ó {2,4,6}
(3.2)
' P({y}_{1,3,5})
P({1,3,5})
'
P({y})
P({1,3,5})
' 1/6
1/2
' 1
3
if x ' 0 and y 0 {1,3,5}

107
' P({y}_{1,3,5})
P({1,3,5})
'
P(i)
P({1,3,5}) ' 0 if x ' 0 and y ó {1,3,5}
hence
j
6
y'1
yP(Y'y|X'x)
' 2%4%6
3
' 4 if x '1
' 1%3%5
3
' 3 if x '0
' 3 % x.
Thus in the case where both Y and X are discrete random variables, the conditional expectation
 can be defined as
E[Y|X]
E[Y|X] ' '
y
yp(y|X), where p(y|x) ' P(Y'y|X'x) for P(X'x) > 0
A second example is where X is uniformly [0,1] distributed, and given the outcome  x of
X, Y  is randomly drawn from the uniform [0,x] distribution. Then the distribution function F(y)
of Y is:
F(y) ' P(Y # y) ' P(Y # y and X # y) % P(Y # y and X > y)
' P(X # y) % P(Y # y and X > y) ' y % E[I(Y # y)I(X > y)]
' y % m
1
0
m
x
0 I(z # y)x &1dz I(x > y)dx ' y % m
1
y
m
min(x,y)
0
x &1dz dx
' y % m
1
y (y/x)dx ' y(1 & ln(y)) for 0# y #1.
Hence, the density of Y is:
 
 
f(y)
' F )(y) ' &ln(y) for y 0 [0,1],
f(y) ' 0 for y ó [0,1].
Thus,  the expected value of Y is: 
   But what would the expected
E[Y] ' m
1
0 y(&ln(y))dy ' 1/4.
value be if it is revealed that  
 for a given number 
 The latter information
X ' x
x 0 (0,1) ?

108
implies that Y is now uniformly [0,x] distributed, hence the conditional expectation involved is
E[Y|X'x] ' x &1
m
x
0 ydy ' x/2.
More generally, the conditional expectation of Y given X is:
E[Y|X] ' X &1
m
X
0 ydy ' X/2.
(3.3)
The latter example is a special case of a pair (Y,X) of absolutely continuously distributed 
random variables with joint density function f(y,x) and marginal density fx(x) . The conditional
distribution function of Y given the event 
 is:
X 0 [x,x%δ], δ > 0,
P(Y # y* X 0 [x,x%δ]) ' P(Y # y and X 0 [x,x%δ])
P(X 0 [x,x%δ])
'
m
y
&4
1
δ m
x%δ
x
f(u,v)dvdu
1
δ m
x%δ
x
fx(v)dv
.
Letting 
  then yields the conditional distribution function of  Y given the event  X = x:
δ 9 0
F(y|x) ' lim
δ90
P(Y # y* X 0 [x,x%δ]) ' m
y
&4
f(u,x)du /fx(x), provided fx(x) > 0.
 
Note that we cannot define this conditional distribution function directly as 
 
 
F(y|x) ' P(Y # y and X ' x)/P(X ' x),
because for continuous random variables X,  P(X = x) = 0. 
The conditional density of Y given the event  X = x is now
 
= f(y,x)/fx(x), 
f(y|x) ' MF(y|x)/My
and the conditional expectation of  Y given the event  X = x can therefore be defined as: 

109
E[Y|X'x] ' m
4
&4
yf(y|x)dy ' g(x), say.
Plugging in X  for x then yields:
E[Y|X] ' m
4
&4
yf(y|X)dy ' g(X).
(3.4)
These examples demonstrate two fundamental properties of conditional expectations. The
first one is that E[Y|X] is a function of X, which can be translated as follows: Let Y and X be two
random variables defined on a common probability space 
 and let 
 be the
{Ω,ö,P},
öX
algebra generated by X: 
B}, where X-1(B) is a short-hand notation for
σ&
öX ' {X &1(B), B 0
the set 
and B  is the Euclidean Borel field.  Then:
{ω0Ω: X(ω) 0 B},
Z ' E[Y|X] is measurable öX,.
(3.5)
which means that for all Borel sets B,  
 Secondly, we have
{ω0Ω: Z(ω) 0 B} 0 öX.
E[(Y & E[Y|X])I(X 0 B)] ' 0 for all Borel sets B.
(3.6)
In particular in the case (3.4)  we have
E[(Y & E[Y|X])I(X 0 B)] ' m
4
&4m
4
&4
y & g(x) I(x0B)f(y,x)dydx
' m
4
&4 m
4
&4
yf(y|x)dy I(x0B)fx(x)dx & m
4
&4 m
4
&4
f(y|x)dy g(x)I(x0B)fx(x)dx
' m
4
&4
g(x)I(x0B)fx(x)dx & m
4
&4
g(x)I(x0B)fx(x)dx ' 0.
(3.7)

110
Since 
B}, property (3.6) is equivalent to
öX ' {X &1(B), B 0
m
A
Y(ω) & Z(ω) dP(ω) ' 0 for all A 0 öX.
(3.8)
Moreover, note that 
 so that (3.8) implies  
Ω0 öX,
E(Y) ' m
Ω
Y(ω)dP(ω) ' m
Ω
Z(ω)dP(ω) ' E(Z),
(3.9)
provided that the expectations involved are defined. A sufficient condition for the existence of
is that
E(Y)
E( |Y| ) < 4.
(3.10)
We will see later that (3.10) is also a sufficient condition for the existence of  
. 
E(Z)
I will show now that the condition  (3.6)  also holds  for the examples (3.1) and (3.3).  Of
course, in the case  (3.3) I have already shown this in (3.7), but it is illustrative to verify it again
for the special case involved.  
In the case (3.1) the random variable Y.I(X=1) takes the value 0 with probability ½, and
the values 2, 4, or 6 with probability 1/6, and the random variable Y.I(X=0) takes the value 0 with
probability ½, and the values 1, 3, or 5 with probability 1/6, so that  
E[Y.I(X0B)] ' E[Y.I(X'1)] '
2
if 1 0 B and 0 ó B,
E[Y.I(X0B)] ' E[Y.I(X'0)] ' 1.5 if 1 ó B and 0 0 B,
E[Y.I(X0B)] ' E[Y]
' 3.5 if 1 0 B and 0 0 B,
E[Y.I(X0B)] ' 0
if 1 ó B and 0 ó B,
which by (3.1) and (3.6) is equal to

111
E[(E[Y|X])I(X0B)] ' 3E[I(X0B)] % E[X.I(X0B)]
' 3P(X0B) % P(X'1 and X0B)
' 3P(X'1) % P(X'1)
' 2
if 1 0 B and 0 ó B,
' 3P(X'0) % P(X'1 and X'0) ' 1.5 if 1 ó B and 0 0 B,
' 3P(X'0 or X'1) % P(X'1)
' 3.5
if 1 0 B and 0 0 B,
' 0
if 1 ó B and 0 ó B.
Moreover, in the case  (3.3) the distribution function of 
 is:
Y.I(X0B)
FB(y) ' P(Y.I(X0B) # y) ' P(Y # y and X 0 B) % P(X ó B)
' P(X 0 B_[0,y]) % P(Y # y and X 0 B_(y,1)) % P(X ó B)
' m
y
0
I(x0B)dx % ym
1
y
x &1I(x 0 B)dx % 1 & m
1
0
I(x 0 B)dx
' 1 & m
1
y
I(x0B)dx % ym
1
y
x &1I(x 0 B)dx
for 0 # y # 1,
hence the density involved is
 
fB(y) ' m
1
y
x &1I(x 0 B)dx for y 0 [0,1],
fB(y) ' 0 for y ó [0,1].
Thus
E[Y.I(X 0 B)] ' m
1
0
y m
1
y
x &1I(x 0 B)dx dy ' 1
2m
1
0
y.I(y 0 B)dy,
which is equal to

112
E E[Y|X]I(X0B) ' 1
2E[X.I(X 0 B)] ' 1
2m
1
0
x.I(x 0 B)dx.
The two conditions (3.5) and (3.8) uniquely define 
 in the sense that if there
Z ' E[Y|X],
exist two versions of  
 say 
 and
 satisfying the conditions  (3.5)
E[Y|X],
Z1 ' E[Y|X]
Z2 ' E[Y|X],
and (3.8), then 
 To see  this, let
P(Z1 ' Z2) ' 1.
A ' {ω 0 Ω: Z1(ω) < Z2(ω)}.
(3.11)
Then 
, hence it follows from  (3.8)  that
A 0 öX
m
A
Z2(ω) & Z1(ω) dP(ω) ' E (Z2&Z1)I(Z2&Z1 > 0) ' 0.
The latter equality implies 
 as I will show in Lemma 3.1 below. Replacing
P(Z2 & Z1 > 0) ' 0,
the set A by  
 it follows similarly that 
A ' {ω 0 Ω: Z1(ω) > Z2(ω)} ,
P(Z2 & Z1 < 0) ' 0.
Combining these two cases it follows that   P(Z2 … Z1) ' 0.
Lemma 3.1: 
 implies P(Z > 0) = 0. 
E[Z.I(Z > 0)] ' 0
Proof: Choose 
 arbitrary. Then
g > 0
0 ' E[Z.I(Z > 0)] '
E[Z.I(0 < Z < g)] % E[Z.I(Z $ g)] $ E[Z.I(Z $ g)]
$ gE[I(Z $ g)] ' gP(Z $ g),
hence 
 for all 
.  Now take 
 and let 
P(Z > g) ' 0
g > 0
g ' 1/n, n ' 1,2,.....,
 Cn ' {ω0Ω: Z(ω) > n &1}.
Then
 hence 
Cn d Cn%1,

113
P(Z > 0) ' P[ ^4
n'1Cn] ' limn64P[Cn] ' 0.
(3.12)
Q.E.D.
The conditions  (3.5) and (3.8) only depend on the conditioning random variable X  via
the sub- 
algebra 
 of  
 Therefore, we can define the conditional expectation of a
σ&
öX
ö.
random variable Y relative to an arbitrary sub-
algebra 
 of  
 denoted by E[Y|
], as
σ&
ö0
ö ,
ö0
follows:
Definition 3.1: Let Y be a random variable defined on a probability space  
 satisfying
{Ω,ö,P},
 and let   
 be a sub-
algebra of 
The conditional expectation of Y
E(|Y|) < 4,
ö0 d ö
σ&
ö.
relative to the sub-
algebra 
, denoted by E[Y|
] = Z, say,  is a random variable Z which
σ&
ö0
ö0
is measurable
 and is  such that for all sets A  
, 
ö0,
0 ö0
 mAY(ω)dP(ω) ' mAZ(ω)dP(ω).
3.2. 
Properties of conditional expectations
As said before, the condition E(|Y|) <  4 is also a sufficient condition for the existence of
E(E[Y|ö 0]). The reason is two-fold. First, I have already established in (3.9) that
Theorem 3.1: 
 = E(Y).
E[E(Y|ö0)]
Second, conditional expectations preserve inequality:

114
Theorem 3.2:  If P(X # Y) ' 1 then P(E(X|ö0) # E(Y|ö0)) ' 1.
Proof: Let 
 Then 
 and
A ' {ω0Ω: E(X|ö0)(ω) > E(Y|ö0)(ω)}.
A 0 ö0,
m
A
X(ω)dP(ω) ' m
A
E(X|ö0)(ω)dP(ω) # m
A
Y(ω)dP(ω) ' m
A
E(Y|ö0)(ω)dP(ω),
hence
0 # m
A
E(Y|ö0)(ω) & E(X|ö0)(ω) dP(ω) # 0.
(3.13)
It follows now from (3.13) and Lemma 3.1 that P({ω0Ω: E(X|ö0)(ω) > E(Y|ö0)(ω)}) ' 0.
Q.E.D. 
Theorem 3.2 implies  that  
 with probability 1, and applying
|E(Y|ö0)| # E(|Y| |ö0)
Theorem 3.1 it follows that  
  Therefore,  the condition 
 is  a
E[|E(Y|ö0)|] # E(|Y|).
E(|Y|) < 4
sufficient condition for the existence of E( E [Y| ö 0] ).
Conditional expectations also preserve linearity: 
Theorem 3. 3: If 
 then
 = 
 + 
E[|X|] < 4 and E[|Y|] < 4
P[E(αX % βY|ö0)
αE(X|ö0)
βE(Y|ö0)]
= 1.
Proof: Let 
, 
, 
. For every
 we
Z0 ' E(αX %βY|ö0) Z1 ' E(X|ö0) Z2 ' E(Y|ö0)
A 0 ö0
have:
mAZ0(ω)dP(ω) ' mA(αX(ω) %βY(ω))dP(ω) ' αmAX(ω)dP(ω) % βmAY(ω)dP(ω),
mAZ1(ω)dP(ω) ' mAX(ω)dP(ω),

115
and
mAZ2(ω)dP(ω) ' mAY(ω)dP(ω),
hence
mA Z0(ω) & αZ1(ω) & βZ2(ω) dP(ω) ' 0.
(3.14)
Taking 
it follows from (3.14) and Lemma 3.1 that
A ' {ω0Ω: Z0(ω) & αZ1(ω) & βZ2(ω) > 0}
P(A) = 0,  and taking 
 it follows similarly that P(A)
A ' {ω0Ω: Z0(ω) & αZ1(ω) & βZ2(ω) < 0}
= 0,  hence  
 Q.E.D.
P({ω0Ω: Z0(ω) & αZ1(ω) & βZ2(ω) … 0}) ' 0.
If we condition a random variable Y on itself, then intuitively we may expect that E(Y|Y)
= Y, because then Y acts as a constant. More formally, this result can be stated as:
Theorem 3.4: Let  
 If Y is measurable  
 then 
E[|Y|] < 4.
ö,
P(E(Y|ö) ' Y) ' 1.
Proof: Let 
 For every  
 we have:
Z ' E(Y|ö).
A 0 ö
mA Y(ω) & Z(ω) dP(ω) ' 0.
(3.15)
Take 
 hence it follows from (3.15) and Lemma
A ' {ω0Ω: Y(ω) & Z(ω) > 0}. Then A 0 ö,
3.1 that P(A) = 0. Similarly, taking 
 it follows that P(A) = 0.
A ' {ω0Ω: Y(ω) & Z(ω) < 0}
Thus 
 Q.E.D.
P({ω0Ω: Y(ω) & Z(ω) … 0}) ' 0.
In Theorem 3.4 I have conditioned Y on the largest sub-
algebra of 
namely 
σ&
ö,
ö
itself. The smallest  sub-
algebra of 
 is T  = 
which is called the trivial 
algebra. 
σ&
ö
{Ω,i},
σ&

116
Theorem 3.5: Let  
 Then P[
 T  ) = E(Y)] = 1.
E[|Y|] < 4.
E(Y|
Proof: Exercise, along the same lines as the proofs of Theorems 3.2-3.4.
The following theorem, which plays a  key-role in regression analysis, follows from
combining the results of Theorems 3.3 and 3.4:
Theorem 3.6: Let 
  Then  
E[|Y|] < 4 and U ' Y & E[Y|ö0].
P[E(U|ö0) ' 0] ' 1.
Proof: Exercise.
Next, let (Y, X, Z) be jointly continuously distributed with joint density function f(y,x,z)
and marginal densities fy,x(y,x),  fx,z(x,z) and  fx(x). Then the conditional expectation of  Y given  X
= x and Z = z is 
E[Y|X,Z] ' m
4
&4
yf(y|X,Z)dy ' gx,z(X,Z), say,
where f(y|x,z) = f(y,x,z)/fx,z(x,z) is the conditional density of Y given  X = x and Z = z. The
conditional expectation of  Y given  X = x alone is
E[Y|X] ' m
4
&4
yf(y|X)dy ' gx(X), say,
where  f(y|x) = fy,x(y,x)/fx(x) is the conditional density of  Y given  X = x alone.  Denoting the
conditional density of Z given X = x by fz(z|x) = fz,x(z,x)/fx(x), it follows now that

117
E E[Y|X,Z] |X ' m
4
&4 m
4
&4
yf(y|X,z)dy fz(z|X)dz ' m
4
&4 m
4
&4
y f(y,X,z)
fx,z(X,z)dy
fx,z(X,z)
fx(X) dz
' m
4
&4
y m
4
&4
f(y,X,z)dzdy
1
fx(X) ' m
4
&4
y fy,x(y,X)
fx(X) dy ' m
4
&4
yf(y|X)dy ' E[Y|X].
This is one of the versions of the Law of Iterated Expectations. Denoting by 
 the 
algebra
öX,Z
σ&
generated by (X,Z), and by 
 the  
algebra generated by X, this result can be translated as:
öX
σ&
E E[Y|öX,Z] |öX ' E[Y|öX].
Note that  
 because 
öX d öX,Z,
 B} = 
B}
öX ' {{ω0Ω: X(ω) 0 B1}, B1 0
{{ω0Ω: X(ω) 0 B1, Z(ω) 0 ú}, B1 0
 
 B} =  
d {{ω0Ω: X(ω) 0 B1, Z(ω) 0 B2}, B1,B2 0
öX,Z.
Therefore, the law of iterated expectations can be stated more generally as:
Theorem 3.7: Let 
 be sub-
algebras  of  
  Then 
E[|Y|] < 4, and let ö0 d ö1
σ&
ö.
P E E[Y|ö1] |ö0 ' E(Y|ö0) ' 1.
Proof: Let 
,  
 and
 It has to be shown that
Z0 ' E[Y|ö0]
Z1 ' E[Y|ö1],
Z2 ' E[Z1|ö0].
 Let 
 .  Then also 
. It follows from Definition 3.1 that
P(Z0 ' Z2) ' 1.
A 0 ö0
A 0 ö1
 implies
Z0 ' E[Y|ö0]
m
A
Y(ω)dP(ω) ' m
A
Z0(ω)dP(ω),
 implies
Z1 ' E[Y|ö1]

118
m
A
Y(ω)dP(ω) ' m
A
Z1(ω)dP(ω),
and 
 implies
Z2 ' E[Z1|ö0]
m
A
Z2(ω)dP(ω) ' m
A
Z1(ω)dP(ω),
Combining these equalities it follows that for all 
,
A 0 ö0
m
A
Z0(ω) & Z2(ω) dP(ω) ' 0.
(3.16)
Now choose 
. Note that 
. Then it follows from (3.16)
A ' {ω0Ω: Z0(ω) & Z2(ω) > 0}
A 0 ö0
and Lemma 3.1 that  
   Similarly,  if  we  choose  
P(A) ' 0.
A ' {ω0Ω: Z0(ω) & Z2(ω) < 0}
then   again  
 Therefore,  
 Q.E.D.
P(A) ' 0.
P(Z0 ' Z1) ' 1.
The following monotone convergence theorem for conditional expectations  plays a key-
role in the proofs of Theorems 3.9 and 3.10 below.
Theorem 3.8: (Monotone convergence). Let Xn  be a sequence of non-negative random variables
defined on a common probability space 
 such that 
 = 1 and
{Ω,ö,P},
P(Xn # Xn%1)
 <  4.   Then  
E[supn$1Xn]
P limn64 E[Xn|ö0] ' E[limn64Xn|ö0] ' 1.
Proof: Let  
  It follows from Theorem 3.2 that Zn is
Zn ' E[Xn|ö0] and X ' limn64Xn.
monotonic non-decreasing, hence  
 exists. Let 
be arbitrary, and denote for
Z ' limn64Zn
A 0 ö0
  
  Then also Yn is nonnegative and
ω 0 Ω, Yn(ω) ' Zn(ω).I(ω 0 A), Y(ω) ' Z(ω).I(ω 0 A).
monotonic non-decreasing, and 
 hence it follows from  the monotone convergence
Y ' limn64Yn,

119
theorem  that 
which is equivalent to
limn64mYn(ω)dP(ω) ' mY(ω)dP(ω),
limn64mAZn(ω)dP(ω) ' mAZ(ω)dP(ω).
(3.17)
Similarly, denoting 
 it follows from  the
Un(ω) ' Xn(ω).I(ω0A), U(ω) ' X(ω).I(ω0A),
monotone convergence theorem  that 
 which is equivalent
limn64mUn(ω)dP(ω) ' mU(ω)dP(ω),
to 
limn64mAXn(ω)dP(ω) ' mAX(ω)dP(ω).
(3.18)
Moreover, it follows from the definition of  
 that
Zn ' E[Xn|ö0]
mAZn(ω)dP(ω) ' mAXn(ω)dP(ω).
(3.19)
It follows now from (3.17), (3.18) and (3.19) that
mAZ(ω)dP(ω) ' mAX(ω)dP(ω).
(3.20)
Theorem 3.8 easily follows from (3.20). Q.E.D.
The following theorem extends the result of Theorem 3.4:
Theorem 3.9: Let  X  be measurable 
 and let both  
 and 
 be finite.   Then 
ö0,
E(|Y|)
E(|XY|)
P[E(XY|ö0) ' X.E(Y|ö0)] ' 1.
Proof: I will prove the theorem involved only for the case that both X and Y are non-
negative with probability 1, leaving the general case as an easy exercise. 

120
Denote 
 If
Z ' E(XY|ö0), Z0 ' E(Y|ö0).
œA0 ö0: mAZ(ω)dP(ω) ' mAX(ω)Z0(ω)dP(ω),
(3.21)
then the theorem under review holds.
(a) 
First, consider the case that X is discrete: 
say, where the Aj ‘s are
X(ω) ' 'n
j'1βjI(ω 0 Aj),
disjoint sets in 
 and the 
‘s are non-negative numbers.  Let  
 be arbitrary, and
ö0
βj
A 0 ö0
observe that
 for j = 1,..,n. Then by Definition 3.1, 
A_Aj 0 ö0
 
m
A
X(ω)Z0(ω)dP(ω) ' m
A
j
n
j'1
βjI(ω0Aj)Z0(ω)dP(ω) ' j
n
j'1
βj m
A_Aj
Z0(ω)dP(ω)
' j
n
j'1
βj m
A_Aj
Y(ω)dP(ω) ' j
n
j'1
βjm
A
I(ω0Aj)Y(ω)dP(ω) '
m
A
j
n
j'1
βjI(ω0Aj)Y(ω)dP(ω)
'm
A
X(ω)Y(ω)dP(ω) ' m
A
Z(ω)dP(ω),
which proves the theorem for the case that X is discrete.
(b) 
If X is not discrete then there exists a sequence of discrete random  variables Xn   such that
for  each  
 and 
 monotonic, hence also 
ω 0 Ωwe have:
0 # Xn(ω) # X(ω)
Xn(ω) 8 X(ω)
 8  
 monotonic. Therefore, it follows from Theorem 3.8 and part (a) that,
Xn(ω)Y(ω)
X(ω)Y(ω)
E[XY|ö0] ' lim
n64
E[XnY|ö0] ' lim
n64
XnE[Y|ö0] ' XE[Y|ö0]
with probability 1. Thus the theorem under review holds for the case that both  X and Y are non-
negative with probability 1. 
(c)
The rest of the proof is left as an exercise. Q.E.D.
We have seen for the case that Y and X are jointly absolutely continuous distributed that

121
the conditional expectation E[Y|X] is a function of X. This holds also more generally:
Theorem 3.10:  Let Y and X be random variables defined on the probability space {
},
Ω,ö,P
and assume that 
. Then there exists a Borel measurable function g such that 
E(*Y*) < 4
This result carries over to the case where X is a finite-dimensional
P[E(Y|X) ' g(X)] ' 1.
random vector.
Proof: The proof involves the following steps:
(a)
Suppose that Y  is non-negative and bounded: 
 = 1, 
›K < 4: P({ω0Ω: 0 # Y(ω) # K})
and let Z = 
, where 
 is the 
algebra generated by X.  Then
E(Y*öX )
öX
σ&
P({ω0Ω: 0 # Z(ω) # K}) ' 1.
(3.22)
(b) 
Under the conditions of part (a) there exists a sequence of discrete random variables Zm,
 where  Ai,m 
, 
,
Zm(ω) ' 'm
i'1αi,mI(ω 0 Ai,m),
0 öX Ai,m_Aj,m ' i if i … j, ^m
i'1Ai,m ' Ω
 for  i = 1,..,m, such that 
 monotonic. For each Ai,m we can find a
0 # αi,m < 4
Zm(ω) 8 Z(ω)
Borel set Bi,m such that 
. Thus, if we take  
 then Zm =
Ai,m ' X &1(Bi,m)
gm(x) ' 'm
i'1αi,mI(x 0 Bi,m)
gm(X) with probability 1. 
Next, let 
 This function is Borel measurable, and
g(x) ' limsupm64gm(x).
 
 =  
 with probability 1. 
Z ' limsupm64Zm ' limsupm64gm(X)
g(X)
(c)
Let Yn = Y.I(Y < n). Then 
 monotonic.  By part (b) it follows that there
Yn(ω) 8 Y(ω)
exists a Borel measurable function gn(x) such that 
.  Let g(x) =
E(Yn*öX) ' gn(X)
 which is Borel measurable. It follows now from Theorem 3.8 that
limsupn64gn(x),

122
 E(Y*öX) ' limn64 E(Yn*öX) ' limsupn64 E(Yn*öX) ' limsupn64 gn(X) ' g(X).
(d)
Let 
. Then 
, and therefore by part (c),
Y % ' max(Y,0), Y & ' max(&Y,0)
Y ' Y % & Y &
, say, and 
, say. Then 
 = 
 =
E(Y %*öX) ' g %(X)
E(Y &*öX) ' g &(X)
E(Y*öX)
g %(X) & g &(X)
g(X). Q.E.D.
If random variables X and Y are independent, then knowing the realization of X will not
reveal anything about Y, and vice versa. The following theorem formalizes this fact.
Theorem 3. 11: Let X and Y be independent random variables. If E[|Y|] < 
 then P(E[Y|X] = 
4
E[|Y|]) = 1. More generally, let Y be defined on the probability space {S,ö,P}, let  öY  be the
F!algebra generated by Y, and let ö0  be a sub-F!algebra of ö such that öY   and ö0   are
independent, i.e.,  for all  
 If E[|Y|] < 
 then
A 0 öY and B 0 ö0, P(A_B) ' P(A)P(B).
4
P(E[Y|ö0] =  E[|Y|]) = 1.
Proof: Let 
 be the 
algebra generated by X, and let 
 be arbitrary. There
öX
σ&
A 0 öX
exists a Borel set B such that 
 Then
A ' {ω0Ω: X(ω) 0 B}.
mAY(ω)dP(ω) ' mΩY(ω)I(ω 0 A)dP(ω)' mΩY(ω)I(X(ω) 0 B)dP(ω)
' E[YI(X0B)] ' E[Y]E[I(X0B)],
where the last equality follows from the independence of Y and X. Moreover
E[Y]E[I(X0B)] ' E[Y]mΩI(X(ω)0B)dP(ω) ' E[Y]mΩI(ω0A)dP(ω) ' mAE[Y]dP(ω).
Thus

123
mAY(ω)dP(ω) ' mAE[Y]dP(ω).
By the definition of conditional expectation, this implies that E[Y|X] = E[Y]  with probability 1.
Q.E.D.
3.3.
Conditional probability measures and conditional independence
The notion of a probability measure relative to a sub-F-algebra can be defined similar to
Definition 3.1, using the conditional expectation of an indicator function:
Definition 3.2:  Let {S,ö,P} be a probability space, and let  
 be a σ-algebra. Then for
ö0 d ö
any set A in ö,  
 where 
P(A|ö0) ' E[IA|ö0],
IA(ω) ' I(ω 0 A).
In the sequel I will use the shorthand notation 
 to indicate the conditional probability 
P(Y 0 B|X)
 where B is a Borel set and 
 is the F-algebra generated by X,
P({ω 0 Ω: Y(ω) 0 B}|öX),
öX
and 
to indicate 
for any sub-F-algebra 
 of ö.  The
P(Y 0 B|ö0)
P({ω 0 Ω: Y(ω) 0 B}|ö0)
ö0
event 
 involved may be replaces by any equivalent expression.
Y 0 B
Similar to the notion of independence of sets and random variables and/or vectors (see
Chapter 1) we can now define conditional independence:
Definition 3.3: A  sequence of sets 
 is conditional independent relative to a  sub-F-
Aj 0 ö
algebra 
 if  for any subsequence jn, 
 Moreover, a
ö0 of ö
P(^nAjn|ö0) ' (nP(Ajn|ö0).
sequence Yj  of random variables or vectors defined on a common probability space {S,ö,P} is

124
conditional independent relative to a  sub-F-algebra 
 if  for any sequence Bj  of
ö0 of ö
conformable Borel sets the sets 
are conditional independent relative
Aj ' {ω 0 Ω: Yj(ω) 0 Bj}
to 
 
ö0.
3.4.
Conditioning on increasing sigma-algebras
Consider a random variable Y defined on the probability space {S,ö,P}, satisfying E[|Y|]
< 4,  and let ön be an non-decreasing sequence of sub-F-algebras of ö: 
 The
ön d ön%1 d ö.
question I will address is: What is the limit of 
 for n 64?  As will be shown below, the
E[Y|ön]
answer to this question is fundamental for time series econometrics.
We have seen in Chapter 1 that the union of  F-algebras is not necessarily a F-algebra
itself. Thus,  
 may not be a F-algebra. Therefore, let
^4
n'1ön
ö4 ' »
4
n'1 ön '
def.
σ ^4
n'1ön ,
(3.23)
i.e.,  ö4 is the smallest F-algebra containing 
 Clearly,  
 because the latter also
^4
n'1ön.
ö4 d ö,
contains ^4
n'1ön.
The answer to our question is now:
Theorem 3.12: If Y is measurable ö,  E[|Y|] < 4,  and {ön } is a non-decreasing sequence of
sub-F-algebras of ö, then  
 with probability 1, where ö4 is defined
limn64E[Y|ön] ' E[Y|ö4]
by (3.23).
This result is usually proved by using martingale theory. See Billingsley (1986),   Chung

125
(1974) and Chapter 7. However,  in Appendix 3.A I will provide an alternative proof of Theorem
3.12 which does not require martingale theory.
3.5.
Conditional expectations as the best forecast schemes
I will show now that the conditional expectation of a random variable Y given a random
variable or vector X is the best  forecasting scheme for Y, in the sense that  the mean square
forecast error is minimal. Let 
 be a forecast of Y, where 
 is a Borel measurable function.
ψ(X)
ψ
The mean square forecast  error (MSFE) is defined by 
 The question
MSFE ' E[(Y & ψ(X))2].
is: for which function 
 is the MSFE minimal. The answer is: 
ψ
Theorem 3.13: If 
 then 
 is minimal for 
E[Y 2] < 4,
E[(Y & ψ(X))2]
ψ(X) ' E[Y|X].
Proof: According to Theorem 3.10 there exists a Borel measurable function g such that
 with probability 1. Denote
 It follows from
E[Y|X] ' g(X)
U ' Y & E[Y|X] ' Y & g(X).
Theorems 3.3, 3.4 and 3.9 that
E[(Y & ψ(X))2|X] ' E[(U % g(X) & ψ(X))2|X]
' E[U 2|X] % 2E[(g(X) & ψ(X))U|X] % E[(g(X) & ψ(X))2|X]
' E[U 2|X] % 2(g(X) & ψ(X))E[U|X] % (g(X) & ψ(X))2,
(3.24)
where the last equality follows from Theorems 3.9 and 3.4. Since by Theorem 3. 6,  E(U|X) ' 0
with probability 1, equation (3.24)  becomes
E[(Y & ψ(X))2|X] ' E[U 2|X] % (g(X) & ψ(X))2.
(3.25)

126
Applying Theorem 3.1 to (3.25), it follows now that 
E[(Y & ψ(X))2] ' E[U 2] % E[(g(X) & ψ(X))2],
which is minimal if 
According to Lemma 3.1, this condition is
E[(g(X) & ψ(X))2] ' 0.
equivalent to the condition that 
 Q.E.D. 
P[g(X) ' ψ(X)] ' 1.
Theorem 3.13 is the basis for regression analysis. In parametric regression analysis, a
dependent variable Y is "explained" by a vector of explanatory (also called "independent")
variables X according to a regression model of the type 
  where 
 is a
Y ' g(X,θ0) % U,
g(x,θ)
known function of x and an unknown vector  of parameters, and U is the error term which is
θ
assumed to satisfy the condition 
 (with probability 1). The problem is then to
E[U|X] ' 0
estimate the unknown parameter vector
 For example, a Mincer-type wage equation explains
θ.
the log of the wage, Y, of a worker out of the years of education, X1, and the years of experience
on the job, X2, by a regression model of the type 
  so that in
Y ' α % βX1 % γX2 & δX 2
2 % U,
this case  
 and  
 The condition
θ ' (α,β,γ,δ)T, X ' (X1,X2)T,
g(X,θ) ' α % βX1 % γX2 & δX 2
2 .
that 
 with probability 1 now implies that 
 with probability 1 for
E[U|X] ' 0
E[Y|X] ' g(X,θ)
some parameter vector
 It follows therefore from Theorem 3.12 that  minimizes the mean
θ.
θ
square error function E[(Y & g(X,θ))2]:
θ ' argmin
θ(
E[(Y & g(X,θ())2],
(3.26)
where "argmin" stands for the argument for which the function involved is minimal.
Next, consider a strictly stationary time series process Yt. 

127
Definition 3.4: A time series process Yt  is said to be strictly stationary if for arbitrary integers
m1 < m2  <....< mk  the joint distribution of  
 does not depend on the time index t.
Yt&m1,......,Yt&mk
Consider the problem of forecasting Yt on the basis on the past  Yt!j , j $1, of  Yt. Usually
we do not observe the whole past of Yt , but only Yt!j  for  j =1,...,t-1, say.  It follows from
Theorem 3.13 that the optimal MSFE  forecast of Yt  given the information on Yt!j  for  j =1,...,m 
is the conditional expectation of Yt  given Yt!j  for  j =1,...,m. Thus, if 
 then 
E[Y 2
t ] < 4
E[Yt|Yt&1,.....,Yt&m] ' argmin
ψ
E[(Yt & ψ(Yt&1,.....,Yt&m))2].
Similarly as before, the minimum is taken over all Borel measurable functions  R on úm. 
Moreover, because of the strict stationarity assumption, there exists a Borel measurable function
gm  on  úm which does not depend on the time index t such that with probability 1,
 = 
 for all t.   Theorem 3.12 now tells us that
E[Yt|Yt&1,...,Yt&m]
gm(Yt&1,...,Yt&m)
limm64E[Yt|Yt&1,.....,Yt&m] ' limm64gm(Yt&1,.....,Yt&m) ' E[Yt|Yt&1,Yt&2,Yt&3,.........]
(3.27)
where the latter is the conditional expectation of Yt  given its whole past Yt!j, j $1.  More
formally, let  
 Then (3.27) reads
ö t&1
t&m ' σ(Yt&1,....,Yt&m) and ö t&1
&4 ' º4
m'1ö t&1
t&m.
limm64E[Yt|ö t&1
t&m] ' E[Yt|ö t&1
&4 ].
The latter conditional expectation is also denoted by Et!1[Yt]:
Et&1[Yt] '
def.
E[Yt|Yt&1,Yt&2,Yt&3,.........] '
def.
E[Yt|ö t&1
&4 ].
(3.28)
In practice we do not observe the whole past of time series processes. However, it follows

128
from Theorem 3.12 that if t is large then approximately,  E[Yt|Yt&1,...,Y1] . Et&1[Yt].
In time series econometrics the focus is often on modeling (3.28)  as a function of past
values of  Yt  and an unknown parameter vector 2, say. For example, an autoregressive  model of
order 1, denoted by AR(1),  takes the form 
 where
Et&1[Yt] ' α % βYt&1, θ ' (α,β)T,
|β| < 1.
Then 
 where Ut  is called the error term. If this model is true, then 
Yt ' α % βYt&1 % Ut,
 which by Theorem 3. 6 satisfies 
   
Ut ' Yt & Et&1[Yt],
P(Et&1[Ut] ' 0) ' 1.
The condition 
 is one of the two necessary conditions for strict stationarity of  Yt ,
|β| < 1
the other one being that Ut  is strictly stationary. To see this, observe that  by backwards
substitution we can write 
provided that  
 The strict
Yt ' α/(1&β) % '4
j'0βjUt&j,
|β| < 1.
stationarity of Yt follows now from the strict stationarity of Ut.
3.6.
Exercises:
1.
Why is property (3.6) equivalent to (3.8)?
2.
Why is the set A defined by (3.11) contained in 
 ?
öX
3.
Why does (3.12) hold? 
4.
Prove Theorem 3.5.
5.
Prove Theorem 3.6.
6.
Verify (3.20) . Why does Theorem 3.8 follow from (3.20)? 
7.
Why does (3.21) imply that Theorem 3.9 holds ? 
8.
Complete the proof of  Theorem 3.9  for the general case, by writing  X ' max(0,X)
  =   
  say, and  
 = 
 say, and
& max(0,&X)
X1 & X2,
Y ' max(0,Y) & max(0,&Y)
Y1 & Y2,
applying the result of part (b) of the proof  to each pair 
 
Xi, Yj.

129
9.
Prove (3.22).
10.
Let Y and X  be random variables, with 
 and let M be a Borel measurable
E[*Y*] < 4,
one-to-one mapping from 
 into 
. Prove that 
 with probability 1.
ú
ú
E[Y|X] ' E[Y|Φ(X)]
11.
Let Y and X  be random variables, with E[Y 2] < 4, P(X ' 1) ' P(X ' 0) ' 0.5,
 and  
 Derive 
 Hint: Use Theorems 3.10 and 3.13.
E[Y] ' 0,
E[X.Y] ' 1.
E[Y|X].
Appendix 
3.A.
 Proof of Theorem 3.12
Let 
 and let 
 be arbitrary.  Note that the
Zn ' E[Y|ön] and Z ' E[Y|ö4],
A 0 ^4
n'1ön
latter implies 
  Because of the monotonicity of {ön } there exists an index kA 
A 0 ö4.
(depending on A) such that  for all n $  kA, hence
limn64m
A
Zn(ω)dP(ω) ' m
A
Y(ω)dP(ω).
(3.29)
If Y is bounded: 
 for some positive real number M, then Zn  is uniformly
P[|Y| < M] ' 1
bounded: 
  hence it follows from (3.29), the dominated
|Zn| ' |E[Y|ön]| # E[|Y||ön] # M,
convergence theorem and the definition of Z that
m
A
limn64Zn(ω)dP(ω) ' m
A
Z(ω)dP(ω)
(3.30)
for all sets A 0 ^4
n'1ön.
Although 
 is not necessarily a F-algebra, it is easy to verify from the monotonicity
^4
n'1ön
of {ön }  that 
  is an algebra.  Now let 
 be the collection of all subsets of 
^4
n'1ön
ö(
ö4

130
satisfying the following two conditions:
(a)
For each set 
 equality (3.30) holds with A = B
B 0 ö(
(b)
For each pair of sets 
 equality  (3.30) holds with 
 
B1 0 ö( and B2 0 ö(
A ' B1^B2.
Since  (3.30) holds for A = S because 
 it is trivial that (3.30) also holds for the
Ω0 ^4
n'1ön,
complement 
 of A:
˜A
m
˜A
limn64Zn(ω)dP(ω) ' m
˜A
Z(ω)dP(ω),
hence if 
 then 
 Thus, 
 is an algebra. Note that this algebra exists, because 
B 0 ö(
˜B 0 ö(.
ö(
  is an algebra satisfying the conditions (a) and (b).  Thus,  
 
^4
n'1ön
^4
n'1ön d ö( d ö4.
I will show now that  
 is a  F-algebra, so that  
 because the former is the
ö(
ö4 ' ö(,
smallest F-algebra containing 
 For any sequence of disjoint sets 
 it follows
^4
n'1ön.
Aj 0 ö(
from (3.30)  that 
m
^4
j'1Aj
limn64Zn(ω)dP(ω) ' j
4
j'1 m
Aj
limn64Zn(ω)dP(ω) ' j
4
j'1 m
Aj
Z(ω)dP(ω)'
m
^4
j'1Aj
Z(ω)dP(ω).
hence  
. This implies that 
 is a F-algebra containing 
, because we have
^4
j'1Aj 0 ö(
ö(
^4
n'1ön
seen in Chapter 1 that an algebra which is closed  under countable unions of disjoint sets is a F-
algebra. Hence 
  and consequently,  (3.30) holds for all sets 
  This implies
ö4 ' ö(,
A 0 ö4.
that 
 = 1 if Y is bounded.
P[Z ' limn64Zn]
Next, let Y be non-negative:
 and denote for natural numbers m $ 1,
P[|Y $ 0] ' 1,
 
 
  and 
 = 
Bm ' {ω 0 Ω: m&1 # Y(ω) < m}, Ym ' Y.I(m&1 # Y < m), Z (m)
n
' E[Ym|ön]
Z (m)
 I have just  shown that for fixed  m $ 1 and arbitrary  
E[Ym|ö4].
A 0 ö4,

131
m
A
limn64Z (m)
n (ω)dP(ω) ' m
A
Z (m)(ω)dP(ω) ' m
A
Ym(ω)dP(ω) '
m
A_Bm
Y(ω)dP(ω)
where the last two equalities follow from the definitions of 
 and 
 Since
Z (m)
Ym.
 it follows that 
 hence
Ym(ω)I(ω 0 ˜Bm) ' 0
Z (m)
n (ω)I(ω 0 ˜Bm) ' 0,
m
A_Bm
limn64Z (m)
n (ω)dP(ω) '
m
A_Bm
Y(ω)dP(ω)
Moreover, it follows from the definition of conditional expectations and Theorem 3.7 that
Z (m)
n
' E[Y.I(m&1 # Y < m)|ön] ' E[Y|Bm_ön] ' E[E(Y|ön)|Bm_ön] ' E[Zn|Bm_ön],
hence for every set  A 0 ^4
n'1ön,
limn64 m
A_Bm
Z (m)
n (ω)dP(ω) ' limn64 m
A_Bm
Zn(ω)dP(ω) '
m
A_Bm
limn64Zn(ω)dP(ω) '
m
A_Bm
Y(ω)dP(ω),
which by the same argument as in the bounded case carries over to the sets 
 Thus we
A 0 ö4.
have
m
A_Bm
limn64Zn(ω)dP(ω) '
m
A_Bm
Y(ω)dP(ω)
for all sets 
 Consequently
A 0 ö4.
m
A
limn64Zn(ω)dP(ω) ' j
4
m'1 m
A_Bm
limn64Zn(ω)dP(ω) ' j
4
m'1 m
A_Bm
Y(ω)dP(ω) ' m
A
Y(ω)dP(ω)
for all sets 
 This proves the theorem for the case 
 The general case is
A 0 ö4.
P[|Y $ 0] ' 1.
now easy, using the decomposition Y ' max(0,Y) & max(0,&Y).

132
1.
Here and in the sequel the notations  
 and 
P(Y ' y|X'x), P(Y ' y and X'x),
P(X'x)
(and similar notations involving inequalities) are merely short-hand notations for the
probabilities 
 
P({ω0Ω: Y(ω) ' y}|{ω0Ω: X(ω)'x}), P({ω0Ω: Y(ω) ' y}_{ω0Ω: X(ω)'x}),
and 
 respectively.
P({ω0Ω: X(ω)'x}),
Endnote

133
Chapter 4
Distributions and Transformations
In this chapter I will review the most important univariate distributions and derive their
expectation, variance, moment generating function (if they exist), and characteristic function.
Quite a few distributions arise as transformations of random variables or vectors. Therefore I will
also address the problem how for a Borel measure function or mapping  g(x) the distribution of Y
= g(X) is related to the distribution of X.
4.1.
Discrete distributions
In Chapter 1 I have introduced three " natural" discrete distributions, namely the
hypergeometric, binomial, and Poisson distributions. The first two are natural in the sense that
they arise from the way the random sample involved is drawn, and the latter because it is a limit
of the binomial distribution. A fourth "natural" discrete distribution I will discuss is the negative
binomial distribution.
4.1.1
The hypergeometric distribution
Recall that a random variable X  has a hypergeometric distribution if
P(X ' k) '
K
k
N&K
n&k
N
n
for k ' 0,1,2,.....,min(n,K),
P(X ' k) ' 0 elsewhere,
(4.1)

134
where 0 < n < N and 0 < K < N are natural numbers. This distribution arises, for example, if we
draw randomly without replacement n balls from a bowl containing K red balls and N !K white
balls. The random variable X is then the number of red balls in the sample. In almost all
applications of this distribution, n < K, so that I will focus on that case only.
The moment generating function involved cannot be simplified further than its definition
mH(t) =
 and the same applies to the characteristic function. Therefore, we
'm
k'0exp(t.k)P(X ' k),
have to derive the expectation directly:
E[X] ' j
n
k'0
k
K
k
N&K
n&k
N
n
' j
n
k'1
K!(N&K)!
(k&1)!(K&k)!(n&k)!(N&K&n%k)!
N!
n!(N&n)!
' nK
N j
n&1
k'0
(K&1)!((N&1)&(K&1))!
k!((K&1)&k)!((n&1)&k)!((N&1)&(K&1)&(n&1)%k)!
(N&1)!
(n&1)!((N&1)&(n&1))!
' nK
N j
n&1
k'0
K&1
k
(N&1)&(K&1)
(n&1)&k
N&1
n&1
' nK
N
.
Along similar lines it follows that
E[X(X&1)] ' n(n&1)K(K&1)
N(N&1)
,
(4.2)
hence
var(X) ' E[X 2] & (E[X])2 ' nK
N
(n&1)(K&1)
N&1
% 1 & nK
N
.

135
4.1.2
The binomial distribution
A random variable X  has a binomial distribution if
P(X ' k) '
n
k p k(1 & p)n&k for k ' 0,1,2,....,n, P(X ' k) ' 0 elsewhere,
(4.3)
were 0 < p < 1. This distribution arises, for example, if we draw randomly with replacement n
balls from a bowl containing K red balls and N!K white balls, where K/N = p. The random
variable X is then the number of red balls in the sample. 
We have seen in Chapter 1 that the binomial probabilities are limits of hypergeometric
probabilities: If both N and K converge to infinity such that K/N 6 p then for fixed n and k, (4.1) 
converges to (4.3). This suggests that also the expectation and variance of the binomial
distribution are the limits of the expectation and variance of the hypergeometric distribution,
respectively:
E[X] ' np,
(4.4)
var(X) ' np(1&p).
(4.5)
As we will see later, in general convergence of distributions does not imply convergence of
expectations and variances, except if the random variables involved are uniformly bounded.
Therefore, in this case the conjecture is true because the distributions involved are bounded:
 = 1. However, it is not hard to verify (4.4) and (4.5) from the moment generating
P[0 # X < n]
function:
B(t) ' j
n
k'0
exp(t.k) n
k p k(1 & p)n&k ' j
n
k'0
n
k (pe t)k(1 & p)n&k ' (p.e t % 1 & p)n.
(4.6)
Similarly, the characteristic function is  nB(t) ' (p.e i.t % 1 & p)n.

136
4.1.3
The Poisson distribution
A random variable X is Poisson(λ) distributed if for k = 0,1,2,3,......., and some λ > 0, 
P(X ' k) ' exp(&λ) λk
k! .
(4.7)
Recall that the Poisson probabilities are limits of the binomial probabilities (4.3) for  
  and
n 6 4
 such that  
 It is left as exercises to show that the expectation, variance, moment
p 9 0
np 6 λ.
generating function, and characteristic function of the Poisson(λ) distribution are:
E[X] ' λ,
(4.8)
var(X) ' λ,
(4.9)
mP(t) ' exp[λ(e t & 1)],
(4.10)
and
nP(t) ' exp[λ(e i.t & 1)],
(4.11)
respectively.
4.1.4
The negative binomial distribution
Consider a sequence of independent repetitions of a random experiment with constant
probability p of success. Let the random variable X be the total number of failures in this
sequence before the m-th success, where m $1. Thus, X+m is equal to the number of trials
necessary to produce exactly m successes. The probability P(X = k),  k = 0,1,2,...., is the product
of the probability of obtaining exactly m-1 successes in the first k+m-1 trials, which is equal to

137
the binomial probability
k%m&1
m&1
p m&1(1&p)k%m&1&(m&1)
and the probability p of a success on the (k+m)-th trial. Thus 
P(X ' k) '
k%m&1
m&1
p m(1&p)k, k ' 0,1,2,3,....
This distribution is called the Negative Binomial (m,p) [shortly: NB(m,p)] distribution. 
It is easy to verify from the above argument that a NB(m,p) distributed random variable
can be generated as the sum of m independent NB(1,p) distributed random variables, i.e., if
 are independent NB(1,p) distributed, then  
 is NB(m,p) distributed. The
X1,1,.....,X1,m
X ' 'n
j'1X1,j
moment generating function of the NB(1,p) distribution is
mNB(1,p)(t) ' j
4
k'0
exp(k.t) k
0
p(1&p)k ' p j
4
k'0
(1&p)e t k '
p
1&(1&p)e t ,
provided that  t < !ln(1!p), hence the moment generating function of the NB(m,p) distribution is 
mNB(m,p)(t) '
p
1&(1&p)e t
m
, t < &ln(1&p).
(4.12)
Replacing t by i.t  in (4.12) yields the characteristic function:
nNB(m,p)(t) '
p
1&(1&p)e i.t
m
'
p(1%(1&p)e i.t)
1%(1&p)2
m
.
It is now easy to verify, using the moment generating function, that for a NB(m,p) distributed
random variable X,
E[X] ' m(1&p) /p,

138
var(X) ' m(1&p)2/p 2 % m(1&p)/p.
4.2.
Transformations of discrete random variables and vectors
In the discrete case the question "Given a random variable or vector X and a Borel
measure function or mapping  g(x), how is the distribution of Y = g(X) related to the distribution
of X?" is easy to answer.  If  
 = 1  and 
 are all different, the
P[X 0 {x1,x2, .......}]
g(x1),g(x2),....
answer is trivial: 
 =  
 If some of the values 
 are the
P(Y ' g(xj))
P(X ' xj).
g(x1), g(x2),......
same, let  
 be the set of distinct values of 
 Then
{y1,y2,......}
g(x1), g(x2),......
P(Y ' yj) 'j
4
i'1
I[yj ' g(xi)]P(X ' xi).
(4.13)
It is easy to see that (4.13) carries over to the multivariate discrete case.
For example, if X is Poisson(8) distributed and 
, so that for m
g(x) ' sin2(πx) ' sin(πx) 2
= 0,1,2,3,....,  
 then  
 =
g(2m) ' sin2(πm) ' 0,
g(2m%1) ' sin2(πm%π/2) ' 1,
P(Y ' 0)
 
 and  
e &λ'4
j'0λ2j/(2j)!
P(Y ' 1) ' e &λ'4
j'0λ2j%1/(2j%1)! .
As an application, let 
, where X1 and X2 are independent Poisson(8)
X ' (X1,X2)T
distributed, and let Y =  X1 + X2 . Then for y = 0,1,2,.......
P(Y ' y) ' j
4
i'0 j
4
j'0
I[y ' i%j]P(X1 ' i, X2 ' j) ' exp(&2λ) (2λ)y
y!
.
(4.14)
Hence, Y is Poisson(28) distributed. More generally, we have
Theorem 4.1: If for j = 1,.....,k the random variables Xj are independent Poisson(8j) distributed
then 
 distributed.
'k
j'1Xj is Poisson('k
j'1λj)

139
4.3.
Transformations of absolutely continuous  random variables
If X is absolutely continuously distributed, with distribution function  F(x) ' m
x
&4f(u)du,
the derivation of the distribution function of Y = g(X) is less trivial. Let us assume first that g is
continuous and monotonic increasing: g(x) < g(z) if x < z. Note that these conditions imply that g
is differentiable1. Then g is a one-to-one mapping, i.e., for each  
 there exists
y 0 [g(&4),g(4)]
one and only one 
such that y = g(x).  This unique x is denoted by  
x 0 ú^{&4}^{4}
x ' g &1(y).
Note that the inverse function 
 is also monotonic increasing and differentiable. Now let
g &1(y)
H(y) be the distribution function of Y. Then:
H(y) ' P(Y # y) ' P(g(X) # y) ' P(X # g &1(y)) ' F(g &1(y)).
(4.15)
Taking the derivative of (4.15) yields the density h(y) of Y:
h(y) ' H )(y) ' f(g &1(y)) dg &1(y)
dy
.
(4.16)
If g is differentiable and monotonic decreasing: g(x) < g(z) if x > z, then  
 is also
g &1(y)
monotonic decreasing, so that  (4.15) becomes
H(y) ' P(Y # y) ' P(g(X) # y) ' P(X $ g &1(y)) ' 1 & F(g &1(y)),
and (4.16) becomes
h(y) ' H )(y) ' f(g &1(y)) & dg &1(y)
dy
.
(4.17)
Note that in this case the derivative of  
 is negative, because  
 is monotonic
g &1(y)
g &1(y)
decreasing. Therefore, we can combine (4.16) and (4.17) into one expression:

140
h(y) ' f(g &1(y))/000
/000
dg &1(y)
dy
.
(4.18)
Theorem 4.2: If X is absolutely continuously distributed with density f, and Y = g(X), where g is
a differentiable monotonic real function on ú, then Y is absolutely continuously distributed, with
density h(y) given by  (4.18)  if  
 and  h(y) = 0
min[g(&4),g(4)] < y < max[g(&4),g(4)],
elsewhere.
4.4.
Transformations of absolutely continuous random vectors
4.4.1
The linear case
Let 
 be a bivariate random vector, with distribution function
X ' (X1,X2)T
F(x) ' m
x1
&4m
x2
&4
f(u1,u2)du1du2 '
m
(&4,x1]×(&4,x2]
f(u)du, where x ' (x1,x2)T, u ' (u1,u2)T
In this section I will derive the joint density of Y = AX + b, where A is a (non-random)
nonsingular 2×2 matrix and b is a non-random 2×1 vector. 
Let us first consider the case that A is equal to the unit matrix I, so that  Y = X + b with 
 Then the joint distribution function  H(y) of Y is 
b ' (b1,b2)T.
H(y) ' P(Y1 # y1, Y2 # y2) ' P(X1 # y1&b1, X2 # y2&b2) ' F(y1&b1,y2&b2),
hence the density if Y is

141
h(y) ' M2H(y)
My1My2
' f(y1&b1,y2&b2) ' f(y&b).
Recall from linear algebra (see Appendix I) that any square matrix A can be decomposed
into
A ' R &1L.D.U,
(4.19)
where R is a permutation matrix (possibly equal to the unit matrix I), L is a lower-triangular
matrix with diagonal elements all equal to 1, U is an upper-triangular matrix  with diagonal
elements all equal to 1, and D is a diagonal matrix. Therefore, I will consider the four cases, A =
U, A = D, A = L, and A = R!1 separately, for b = 0, and then apply the results involved
sequentially according to the decomposition (4.19)  to X+b, which then yields the general result.
Consider the case that Y = AX, with A an upper-triangular matrix:
A '
1 a
0 1
.
(4.20)
Then
Y '
Y1
Y2
'
X1%aX2
X2
,
hence the joint distribution function H(y) of Y is
H(y) ' P(Y1 # y1, Y2 # y2) ' P(X1%aX2 # y1, X2 # y2)
' E I(X1 # y1 & aX2)I(X2 # y2) ' E E I(X1 # y1 & aX2)|X2 I(X2 # y2)
' m
y2
&4
m
y1&ax2
&4
f1|2(x1|x2)dx1 f2(x2)dx2 ' m
y2
&4
m
y1&ax2
&4
f(x1,x2)dx1 dx2,
(4.21)

142
where 
 is the conditional density of X1 given X2 = x2, and 
 is the marginal density
f1|2(x1|x2)
f2(x2)
of  X2 . Taking partial derivatives, if follows from (4.21)  that for Y = AX  with A given by (4.20),
h(y) ' M2H(y)
My1My2
'
M
My2 m
y2
&4
f(y1&ax2,x2)dx2 ' f(y1&ay2,y2) ' f(A &1y).
Along the same lines it follows that if A is a lower-triangular matrix then  the joint density of Y =
AX is
h(y) ' M2H(y)
My1My2
' f(y1,y2&ay1) ' f(A &1y).
(4.22)
Next, let A be a nonsingular diagonal matrix
A '
a1
0
0
a2
.
where  
 Then 
 and 
, hence  the joint distribution function
a1 … 0, a2 … 0.
Y1 ' a1X1
Y2 ' a2X2
H(y) is:
H(y) ' P(Y1 # y1, Y2 # y2) ' P(a1X1 # y1, a2X2 # y2) '
P(X1 # y1/a1, X2 # y2/a2) ' m
y1/a1
&4
m
y2/a2
&4
f(x1,x2)dx1dx2 if a1 > 0, a2 > 0,
P(X1 # y1/a1, X2 > y2/a2) ' m
y1/a1
&4
m
4
y2/a2
f(x1,x2)dx1dx2 if a1 > 0, a2 < 0,
P(X1 > y1/a1, X2 # y2/a2) ' m
4
y1/a1
m
y2/a2
&4
f(x1,x2)dx1dx2 if a1 < 0, a2 > 0,
P(X1 > y1/a1, X2 > y2/a2) ' m
4
y1/a1
m
4
y2/a2
f(x1,x2)dx1dx2 if a1 < 0, a2 < 0.
(4.23)
It is a standard calculus exercise for verify from (4.23) that in all four cases

143
h(y) ' M2H(y)
My1My2
'
f(y1/a1,y2/a2)
|a1a2|
' f(A &1y) det(A &1) .
(4.24)
Finally, consider the case where A  is the inverse of a permutation matrix ( which is a
matrix that permutates the columns of the unit matrix), say:
A '
0 1
1 0
&1
'
0 1
1 0
Then the joint distribution function H(y) of Y =AX  is
H(y) ' P(Y1 # y1, Y2 # y2) ' P(X2 # y1, X1 # y2) ' F(y2,y1) ' F(Ay),
and the density involved is
h(y) ' M2H(y)
My1My2
' f(y2,y1) ' f(Ay).
Combining these results, it is not hard to verify, using the decomposition (4.19),  that for the
bivariate case (k = 2):
Theorem 4.3: Let X be k-variate absolutely continuously distributed with joint density f(x), and
let Y = AX + b, where A is a nonsingular square matrix. Then Y is k-variate  absolutely
continuously distributed, with joint density h(y) ' f(A &1(y&b)) |det(A &1)|.
However, this result holds for the general case as well
4.4.2
The nonlinear case
If we denote 
 then the result of Theorem 4.3 reads:
G(x) ' Ax%b, G &1(y) ' A &1(y&b),
 This suggests that Theorem 4.3 can be generalized as
h(y) ' f(G &1(y)) |det(MG &1(y)/My)|.

144
follows.
Theorem 4.4: Let X be k-variate absolutely continuously distributed with joint density f(x),
 and let Y = G(X), where 
 is a one-to-one mapping
x ' (x1,....,xk)T,
G(x) ' (g1(x),....,gk(x))T
with inverse mapping  
 whose components are differentiable
x ' G &1(y) ' (g (
1 (y),.....,g (
k (y))T,
in the components of  
 Let 
, i.e., 
 is the matrix
y ' (y1,....,yk)T.
J(y) ' Mx/My ' MG &1(y)/My
J(y)
with i,j’s element 
, which is  called the Jacobian. Then Y is k-variate absolutely
Mg (
i (y)/Myj
continuously distributed, with joint density  
 for y in the set
h(y) ' f(G &1(y)) |det(J(y))|
 and h(y) = 0 elsewhere.
G(úk) ' {y 0 úk: y ' G(x), f(x) > 0, x 0 úk},
This conjecture is indeed true. Its formal proof is given in Appendix 4. B.
An application of Theorem 4.4 is the following problem. Consider the function
f(x) ' c.exp(&x 2/2) if x $ 0,
' 0 if x < 0.
(4.25)
For which value of  c is this function a density?.
In order to solve this problem, consider the joint density f(x1,x2) ' c 2exp[&(x 2
1 %x 2
2 )/2],
 which is the joint distribution of X = (X1,X2)T, where X1 and X2 are independent
x1 $ 0, x2 $ 0,
random drawings from the distribution with density (4.25).  Next, consider the transformation Y
= (Y1,Y2)T = G(X) defined by:
Y1 '
X 2
1 %X 2
2
0 (0,4)
Y2 ' arctan(X1/X2) 0 (0,π/2).

145
The inverse 
 of this transformation is 
X ' G &1(Y)
X1 ' Y1sin(Y2),
X2 ' Y1cos(Y2),
with Jacobian:
J(Y) '
MX1/MY1 MX1/MY2
MX2/MY1 MX2/MY2
'
sin(Y2)
Y1cos(Y2)
cos(Y2) &Y1sin(Y2) .
Note that 
 Consequently, the density 
 is:
det[J(Y)] ' &Y1.
h(y) ' h(y1,y2) ' f(G &1(y)) |det(J(y))|
h(y1,y2) ' c 2y1exp(&y 2
1 /2) for y1 > 0 and 0 < y2 < π/2,
' 0 elsewhere,
hence,
1 ' m
4
0 m
π/2
0
c 2y1exp(&y 2
1 /2)dy2dy1' c 2(π/2)m
4
0
y1exp(&y 2
1 /2)dy1' c 2π/2.
Thus the answer is  c '
2/π:
m
4
0
exp(&x 2/2)
π/2
dx ' 1.
Note that this result implies that
m
4
&4
exp(&x 2/2)
2π
dx ' 1.
(4.26)

146
4.5.
The normal distribution
I will now review a number of univariate continuous distributions that play a key role in
statistical and econometric inference, starting with the normal distribution. The standard normal
distribution emerges as a limiting distribution of an aggregate of random variables. In particular,
if X1,....Xn are independent random variables with expectation µ and finite and positive variance
F2 then for large n the random variable 
 is approximately standard
Yn ' (1/ n)'n
j'1(Xj&µ)/σ
normally distributed. This result, known as the central limit theorem, will be derived in Chapter
6, and carries over to various types of dependent random variables (see Chapter 7). 
4.5.1
The standard normal distribution
The standard normal distribution is an absolutely continuous distribution with density
function
f(x) ' exp(&x 2/2)
2π
, x 0 ú,
(4.27)
Compare with (4.26).  Its moment generating function is 
mN(0,1)(t) ' m
4
&4
exp(t.x)f(x)dx ' m
4
&4
exp(t.x) exp(&x 2/2)
2π
dx
' exp(t 2/2)m
4
&4
exp[&(x 2&2t.x%t 2)/2]
2π
dx ' exp(t 2/2)m
4
&4
exp[&(x&t)2/2]
2π
dx
' exp(t 2/2)m
4
&4
exp[&u 2/2]
2π
du ' exp(t 2/2),
(4.28)
which exists for all  
 and its characteristic function is
t 0 ú,

147
nN(0,1)(t) ' m(i.t) ' exp(&t 2/2).
Consequently, if X is standard normally distributed then
E[X] ' m )(t) t'0 ' 0, E[X 2] ' var(X) ' m ))(t) t'0 ' 1.
Due to this the standard normal distribution is denoted by N(0,1), where the first number is the
expectation and the second number is the variance, and the statement "X is standard normally
distributed" is usually abbreviated as "X ~ N(0,1)".
4.5.2
The general normal distribution
Now let Y = µ + FX,  where X ~ N(0,1). It is left as an easy exercise to verify that the
density of Y takes the form
f(x) ' exp &½(x&µ)2/σ2
σ 2π
, x 0 ú,
with corresponding moment generating function
mN(µ,σ2)(t) ' E[exp(t.Y)] ' exp(µt)exp(σ2t 2/2),
t 0 ú,
and characteristic function
nN(µ,σ2)(t) ' E[exp(i.t.Y)] ' exp(i.µt)exp(&σ2t 2/2).
Consequently, 
  This distribution is the general normal distribution,
E[Y] ' µ , var(Y) ' σ2.
denoted by N(µ,F2). Thus, Y~ N(µ,F2).

148
4.6.
Distributions related to the standard normal distribution
The standard normal distribution gives rise, via various transformations, to other
distributions, such as the chi-square, t, Cauchy, and F distribution. These distributions are
fundamental in testing statistical hypotheses, as we will see later.
4.6.1
The chi-square distribution
Let X1,....Xn be independent N(0,1) distributed random variables, and let
Yn ' 'n
j'1X 2
j .
(4.29)
The distribution of Yn is called the chi-square distribution with n degrees of freedom, denoted by
 Its distribution and density functions can be derived recursively, starting from the
χ2
n or χ2(n).
case n = 1:
G1(y) ' P[Y1 # y] ' P[X 2
1 # y] ' P[& y # X1 #
y] ' m
y
&
y
f(x)dx ' 2m
y
0
f(x)dx
for y > 0,
G1(y) ' 0 for y # 0,
hence
g1(y) ' G )
1(y) ' f
y / y ' exp(&y/2)
y 2π
for y > 0,
g1(y) ' 0 for y # 0,
where f (x) is defined by (4.27).  Thus, g1(y) is the density of the 
 distribution. The
χ2
1
corresponding moment generating function is

149
mχ2
1(t) '
1
1&2t
for t < 1/2,
(4.30)
and the characteristic function is
nχ2
1(t) '
1
1&2.i.t
'
1%2.i.t
1%4.t 2
.
(4.31)
It follows easily from (4.29), (4.36) and (4.31) that the moment generating and
characteristic functions of the 
 distribution are
χ2
n
mχ2
n(t) '
1
1&2t
n/2
for t < 1/2
(4.32)
and
nχ2
n(t) '
1%2.i.t
1%4.t 2
n/2
.
respectively. Therefore, the density of the  
 distribution is
χ2
n
gn(y) ' y n/2&1exp(&y/2)
Γ(n/2)2n/2
,
(4.33)
where for " > 0,
Γ(α) ' m
4
0
x α&1exp(&x)dx.
(4.34)
The result (4.33) can be proved by verifying that for t < 1/2,  (4.32) is the moment generating
function of (4.33). The function (4.34) is called the Gamma function. Note that

150
Γ(1) ' 1, Γ(1/2) '
π, Γ(α%1) ' αΓ(α) for α > 0.
(4.35)
Moreover, the expectation and variance of the 
 distribution are
χ2
n
E[Yn] ' n, var(Yn) ' 2n.
(4.36)
4.6.2
The Student t distribution
Let X ~ N(0,1) and Yn ~ 
, where X and Yn are independent. Then the distribution of the
χ2
n
random variable
Tn '
X
Yn/n
is called the (Student2) t distribution with n degrees of freedom, denoted by tn.
The conditional density hn(x|y) of Tn  given Yn = y is the density of the N(1,n/y)
distribution, hence the unconditional density of Tn  is 
hn(x) ' m
4
0
exp(&(x 2/n)y/2)
n/y 2π
× y n/2&1exp(&y/2)
Γ(n/2)2n/2
dy '
Γ((n%1)/2)
nπ Γ(n/2) (1%x 2/n)(n%1)/2 .
The expectation of Tn does not exist if n = 1, as we will see below, and is zero for n $ 2, by
symmetry. Moreover, the variance of Tn  is infinite for n = 2., whereas for  n $ 3,
var(Tn) ' E[T 2
n] '
n
n&2 .
(4.37)
See Appendix 4.A . 
The moment generating function of the  tn  distribution does not exist, but it characteristic
function does, of course:

151
ntn(t) ' Γ((n%1)/2)
nπ Γ(n/2) m
4
&4
exp(it.x)
(1%x 2/n)(n%1)/2dx ' 2.Γ((n%1)/2)
nπ Γ(n/2) m
4
0
cos(t.x)
(1%x 2/n)(n%1)/2dx.
4.6.3
The standard Cauchy distribution
The t1 distribution is also known as the standard Cauchy distribution. Its density is:
h1(x) '
Γ(1)
π Γ(1/2) (1%x 2)
'
1
π(1%x 2)
.
(4.38)
where the second equality follows from (4.35), and its characteristic function is
nt1(t) ' exp(&|t|).
The latter follows from the inversion formula for characteristic functions: 
1
2π m
4
&4
exp(&i.t.x)exp(&|t|)dt '
1
π(1%x 2)
.
(4.39)
See Appendix 4.A. Moreover, it is easy to verify from  (4.38)  that the expectation of the Cauchy
distribution does not exist, and that the second moment is infinite.
4.6.4
The F distribution
Let Xm ~ 
 and Yn ~ 
, where Xm and Yn are independent. Then the distribution of the
χ2
m
χ2
n
random variable
F '
Xm/m
Yn/n
is said to be F with m and n degrees of freedom, denoted by Fm,n.  Its distribution function is

152
Hm,n(x) ' P[F # x] ' m
4
0
m
m.x.y/n
0
z m/2&1exp(&z/2)
Γ(m/2)2m/2
dz y n/2&1exp(&y/2)
Γ(n/2)2n/2
dy,
x > 0,
and its density is
hm,n(x) '
m m/2Γ(m/2%n/2)x m/2&1
n m/2Γ(m/2)Γ(n/2) 1%m.x/n m/2%n/2 ,
x > 0,
(4.40)
See Appendix 4.A  
Moreover, it is shown in Appendix 4.A that
E[F] ' n/(n&2) if
n $ 3,
' 4
if n ' 1,2,
var(F) '
2n 2(m%n&4)
m(n&2)2(n&4)
if
n $ 5,
' 4
if n ' 3,4,
' not defined
if n ' 1,2.
(4.41)
Furthermore, the moment generating function of the Fm,n distribution does not exist, and 
the computation of the characteristic function is too tedious an exercise, and therefore omitted. 
4.7.
The uniform distribution and its relation to the standard normal distribution
As we have seen before in Chapter 1, the uniform [0,1] distribution has density
f(x) ' 1 for 0 # x # 1,
f(x) ' 0 elsewhere.
More generally, the uniform [a,b] distribution (denoted by U[a,b]) has density
f(x) '
1
b&a for a # x # b,
f(x) ' 0 elsewhere,
moment generating function 
mU[a,b](t) ' exp(t.b)&exp(t.a)
(b&a)t
,

153
and characteristic function
nU[a,b](t) ' exp(i.b.t)&exp(i.a.t)
i.(b&a)t
' (sin(b.t)%sin(a.t)) & i.(cos(b.t)%cos(a.t))
b&a
.
Most computer languages such as Fortran, Pascal, and Visual Basic have a build-in
function which generates independent random drawings from the uniform [0,1] distribution.3
These random drawings can be converted into independent random drawings from the standard
normal distribution via the transformation
X1 ' cos(2πU1). &2.ln(U2),
X2 ' sin(2πU1). &2.ln(U2),
(4.42)
where U1 and U2 are independent U[0,1] distributed. Then X1 and X2 are independent standard
normally distributed.   This method is called the Box-Muller algorithm.
4.8.
The Gamma distribution
The
 distribution is a special case of a Gamma distribution. The density of the Gamma
χ2
n
distribution is
g(x) ' x α&1exp(&x/β)
Γ(α)βα
, x > 0, α > 0, β > 0.
This distribution is denoted by '(",$). Thus, the
 distribution is a Gamma distribution with " =
χ2
n
n/2 and $ = 2. 
The Gamma distribution has moment generating function

154
mΓ(α,β)(t) ' [1&βt]&α, t < 1/β,
(4.43)
and characteristic function 
  Therefore, the  '(",$) distribution has
nΓ(α,β)(t) ' [1&β.i.t]&α.
expectation "$ and variance "$2. 
The   '(",$) distribution with " = 1 is called the exponential distribution.
4.9.
Exercises
1.
Derive (4.2).
2.
Derive (4.4) and (4.5) directly from (4.3).
3.
Derive (4.4) and (4.5) from the moment generating function (4.6).
4.
Derive (4.8), (4.9), and (4.10).
5.
If X is discrete and Y = g(X), do we need to require that g is Borel measurable?
6.
Prove the last equality in (4.14).
7.
Prove Theorem 4.1, using characteristic functions.
8.
Prove that (4.24)  holds for all four cases in (4.23).
9.
Let X  be a random variable with continuous distribution function F(x).  Derive the
distribution of Y = F(X).
10.
The standard normal distribution has density 
Let X1 and
f(x) ' exp(&x 2/2)/ 2π, x 0 ú.
X2  be independent random drawings from the standard normal distribution involved, and let Y1 =
X1 + X2,  Y2 = X1 ! X2. Derive the joint density  
 say, of Y1 and Y2 , and show that Y1 and
h(y1,y2),
Y2  are independent.  Hint: Use Theorem 4.3.
11.
The exponential distribution has density  
 if x $ 0,  f(x) = 0 if x < 0,
f(x) ' θ&1exp(&x/θ)

155
where 2 > 0 is a constant. Let X1 and X2  be independent random drawings from the exponential
distribution involved, and let Y1 = X1 + X2,  Y2 = X1 ! X2. Derive the joint density  
say,
h(y1,y2),
of Y1 and Y2 .  Hints: Determine first the support 
 of  
 and
{(y1,y2)T 0 ú2: h(y1,y2) > 0}
h(y1,y2),
then use Theorem 4.3.
12.
Let X ~ N(0,1). Derive E[X2k] for k = 2,3,4, using the moment generating function.
13.
Let X1,X2,...,Xn be independent standard normally distributed. Show that 
 is
(1/ n)'n
j'1Xj
standard normally distributed.
14.
Prove (4.30).
15.
Show that for t < 1/2, (4.32) is the moment generating function of (4.33).
16.
Explain why the moment generating function of the  tn distribution does not exist
17.
Prove (4.35).
18.
Prove (4.36).
19.
Let X1,X2,...,Xn be independent standard Cauchy distributed. Show that 
 is
(1/n)'n
j'1Xj
standard Cauchy distributed. 
20.
The class of standard stable distributions consists of distributions with characteristic
functions of the type  
 Note that the standard normal
n(t) ' exp(&|t|α/α), where α 0 (0,2].
distribution is stable with " = 2, and the standard Cauchy distribution is stable with " = 1. Show
that for a random sample X1,X2,...,Xn from a standard stable distribution with parameter ", the
random variable  
  has the same standard stable distribution (this is the reason
Yn ' n &1/α'n
j'1Xj
for calling these distributions stable).
21.
Let X and Y be independent standard normally distributed. Derive the distribution of  X/Y. 
22.
Derive the characteristic function of the distribution with density exp(-|x|) /2, ! 4 < x < 4.

156
23.
Explain why the moment generating function of the Fm,n distribution does not exist.
24.
Prove (4.43).
25.
Show that if U1 and U2 are independent U[0,1] distributed then X1 and X2 in (4.42) are
independent standard normally distributed.
26.
If X and Y are independent '(1,1) distributed, what is the distribution of X-Y?
Appendices
4.A.
Tedious derivations
Derivation of (4.37): 
E[T 2
n] ' nΓ((n%1)/2)
nπ Γ(n/2) m
4
&4
x 2/n
(1%x 2/n)(n%1)/2dx
' nΓ((n%1)/2)
nπ Γ(n/2) m
4
&4
1%x 2/n
(1%x 2/n)(n%1)/2dx & nΓ((n%1)/2)
nπ Γ(n/2) m
4
&4
1
(1%x 2/n)(n%1)/2dx
' nΓ((n%1)/2)
π Γ(n/2)
m
4
&4
1
(1%x 2)(n&1)/2dx & n ' nΓ((n&1)/2%1)
Γ(n/2)
Γ(n/2&1)
Γ((n&1)/2)
& n '
n
n&2
.
In this derivation I have used (4.35) and the fact that
1 ' m
4
&4
hn&2(x)dx '
Γ((n&1)/2)
(n&2)π Γ((n&2)/2) m
4
&4
1
(1%x 2/(n&2))(n&1)/2dx
'
Γ((n&1)/2)
π Γ((n&2)/2) m
4
&4
1
(1%x 2)(n&1)/2dx.

157
Derivation of (4.39): For m > 0 we have:
1
2π m
m
&m
exp(&i.t.x)exp(&|t|)dt '
1
2πm
m
0
exp(&i.t.x)exp(&t)dt %
1
2πm
m
0
exp(i.t.x)exp(&t)dt
'
1
2πm
m
0
exp[&(1%i.x)t]dt %
1
2πm
m
0
exp[&(1&i.x)t]dt
'
1
2π
/00
exp[&(1%i.x)t]
&(1%i.x)
m
0
%
1
2π
/00
exp[&(1&i.x)t]
&(1&i.x)
m
0
'
1
2π
1
(1%i.x)
%
1
2π
1
(1&i.x)
&
1
2π
exp[&(1%i.x)m]
(1%i.x)
&
1
2π
exp[&(1&i.x)m]
(1&i.x)
'
1
π(1%x 2)
& exp(&m)
π(1%x 2)
[cos(m.x)&x.sin(m.x)].
Letting m 6 4, (4.39) follows.
Derivation of (4.40):
hm,n(x) ' H )
m,n(x) ' m
4
0
m.y
n
× (m.x.y/n)m/2&1exp(&(m.x.y/(2n)
Γ(m/2)2m/2
× y n/2&1exp(&y/2)
Γ(n/2)2n/2
dy
'
m m/2x m/2&1
n m/2Γ(m/2)Γ(n/2)2m/2%n/2m
4
0
y m/2%n/2&1exp & 1%m.x/n y/2 dy
'
m m/2x m/2&1
n m/2Γ(m/2)Γ(n/2) 1%m.x/n m/2%n/2m
4
0
z m/2%n/2&1exp &z dz
'
m m/2Γ(m/2%n/2)x m/2&1
n m/2Γ(m/2)Γ(n/2) 1%m.x/n m/2%n/2 ,
x > 0,

158
Derivation of (4.41): It follows from (4.40) that
m
4
0
x m/2&1
(1%x)m/2%n/2dx ' Γ(m/2)Γ(n/2)
Γ(m/2%n/2) ,
hence if k < n/2 then 
m
4
0
x khm,n(x)dx '
m m/2Γ(m/2%n/2)
n m/2Γ(m/2)Γ(n/2)m
4
0
x m/2%k&1
(1%m.x/n)m/2%n/2dx
' (n/m)k Γ(m/2%n/2)
Γ(m/2)Γ(n/2)m
4
0
x (m%2k)/2&1
(1%x)(m%2k)/2%(n&2k)/2dx ' (n/m)k Γ(m/2%k)Γ(n/2&k)
Γ(m/2)Γ(n/2)
' (n/m)k k
k&1
j'0 (m/2%j)
k
k
j'1 (n/2&j)
where the last equality follows from the fact that by (4.35),   
 for " > 0.
Γ(α%k) ' Γ(α)(k&1
j'0(α%j)
Thus,
µm,n ' m
4
0
xhm,n(x)dx '
n
n&2
if n $ 3,
µm,n ' 4 if n # 2,
(4.45)
m
4
0
x 2hm,n(x)dx '
n 2(m%2)
m(n&2)(n&4)
if n $ 5,
' 4 if n # 4.
(4.46)
The results in (4.41) follow now from (4.45) and (4.46).

159
4.B.
Proof of Theorem 4.4
For notational convenience I will prove Theorem 4.4 for the case k = 2 only. First note
that the distribution of Y is absolutely continuous, because for arbitrary Borel sets B in ú2,
P[Y 0 B] ' P[G(X) 0 B] ' P[X 0 G &1(B)] ' mG &1(B)f(x)dx.
If B has Lebesgue measure zero then, since G is a one-to-one mapping, the Borel set A = G!1(B)
has Lebesgue measure zero. Therefore, Y has density h(y), say, so that for arbitrary Borel sets B
in ú2,
P[Y 0 B] ' mBh(y)dy.
Choose a fixed 
 in the support 
 of Y such that 
 is a continuity
y0 ' (y0,1,y0,2)T
G(ú2)
x0 ' G &1(y0)
point of the density f of X and 
 is a continuity point of the density h of Y.  Let for some positive
y0
numbers 
 and 
 
 Then, with 8 the Lebesgue
δ1
δ2, Υ(δ1,δ2) ' [y0,1,y0,1%δ1]×[y0,2,y0,2%δ2].
measure,
P[Y 0 Υ(δ1,δ2)] ' mG &1(Υ(δ1,δ2))f(x)dx # supx0G &1(Υ(δ1,δ2))f(x) λ(G &1(Υ(δ1,δ2)))
' supy0Υ(δ1,δ2)f(G &1(y)) λ(G &1(Υ(δ1,δ2)))
(4.47)
and similarly,
P[Y 0 Υ(δ1,δ2)] $ infy0Υ(δ1,δ2)f(G &1(y)) λ(G &1(Υ(δ1,δ2)))
(4.48)
It follows now from (4.47) and (4.48)  that
h(y0) ' lim
δ190
lim
δ290
P[Y 0 Υ(δ1,δ2)]
δ1δ2
' f(G &1(y0)) lim
δ190
lim
δ290
λ(G &1(Υ(δ1,δ2)))
δ1δ2
(4.49)

160
It remains to show that the latter limit is equal to |det[J(y0)]|.
Denoting 
 it follows from the mean value theorem that for each
G &1(y) ' (g (
1 (y),g (
2 (y))T,
element  
 there exists a 
 depending on y and y0  such that   
 + 
g (
j (y)
λj 0 [0,1]
g (
j (y) ' g (
j (y0)
  where 
 is the j-th row of J(y).  Thus, denoting 
Jj(y0%λj(y&y0))(y&y0),
Jj(y)
D0(y) '
J1(y0%λ1(y&y0)) & J1(y0)
J2(y0%λ2(y&y0)) & J2(y0)
' ˜J0(y) & J(y0),
(4.50)
say, we have  
 Now put   A  = 
 and  b = 
G &1(y) ' G &1(y0) % J(y0)(y&y0) % D0(y)(y&y0).
J(y0)&1
  Then  
y0 & J(y0)&1G &1(y0).
 
G &1(y) ' A &1(y&b) % D0(y)(y&y0),
(4.51)
hence
G &1(Υ(δ1,δ2)) ' {x 0 ú2: x ' A &1(y&b) % D0(y)(y&y0), y 0 Υ(δ1,δ2)}
(4.52)
The matrix A  maps the set (4.52) onto
A[G &1(Υ(δ1,δ2))] ' {x 0 ú2: x ' y & b % A.D0(y)(y&y0), y 0 Υ(δ1,δ2)}
(4.53)
where for arbitrary Borel sets B conformable with a matrix A, A[B] 
 {x: x = Ay, y 0 B}.  Since
'
def.
the Lebesgue measure is invariant for location shifts (i.e.,  the vector b in (4.53)) , it follows that
λ A[G &1(Υ(δ1,δ2))] ' λ {x 0 ú2: x ' y % A.D0(y)(y&y0), y 0 Υ(δ1,δ2)}
(4.54)
Observe from  (4.50) that 
A.D0(y) ' J(y0)&1D0(y) ' J(y0)&1 ˜J0(y) & I2
(4.55)

161
and
limy6y0J(y0)&1 ˜J0(y) ' I2.
(4.56)
Then
λ A[G &1(Υ(δ1,δ2))] ' λ {x 0 ú2: x ' y0 % J(y0)&1 ˜J0(y)(y&y0), y 0 Υ(δ1,δ2)}
(4.57)
It can be shown, using (4.56),  that 
lim
δ190
lim
δ290
λ A[G &1(Υ(δ1,δ2))]
λ Υ(δ1,δ2)
' 1.
(4.58)
Recall from Appendix I that the matrix A can be written as A = QDU, where Q is an
orthogonal matrix, D is a diagonal matrix, and U is an upper-triangular matrix with diagonal.
elements all equal to 1. Let B = (0,1)×(0,1). Then it is not hard to verify in the 2×2 case that U
maps B onto a parallelogram  U[B] with the same area as B, hence  λ(U[B]) ' λ(B) ' 1.
Consequently, the Lebesgue measure of the rectangle D[B] is the same as the  Lebesgue measure
of the set D[U[B]]. Moreover, an orthogonal matrix rotates a set of point around the origin,
leaving all the angles and distances the same. Therefore, the set A[B] has the same Lebesgue
measure as  the rectangle D[B]: 
 =  
 Along the same lines
λ(A[B]) ' λ(D[B]) ' |det[D]|
|det[A]|.
the following more general result can be shown.
Lemma 4.B.1: For a k×k matrix A and a Borel set B in úk,  
 where 8 is
λ(A[B]) ' |det[A]|λ(B),
the Lebesgue measure on the Borel sets in  úk.

162
1.
Except perhaps on a set with Lebesgue measure zero.
2.
The t distribution was discovered by W. S. Gosset, who published the result under the
pseudonym Student.  The reason for the latter was that his employer, an Irish brewery, did not
want its competitors to know that statistical methods were being used. 
3.
See for example Section 7.1 in Press, Flannery, Teukolsky, and Vetterling (1989).  
Thus, (4.58) now becomes
lim
δ190
lim
δ290
λ A[G &1(Υ(δ1,δ2))]
λ Υ(δ1,δ2)
' |det[A]|lim
δ190
lim
δ290
λ G &1(Υ(δ1,δ2))
δ1δ2
' 1,
hence 
lim
δ190
lim
δ290
λ G &1(Υ(δ1,δ2))
δ1δ2
'
1
|det[A]| ' |det[A &1]| ' |det[J(y0)]|.
(4.59)
Theorem 4.4 follows now from  (4.49) and (4.59).
Endnotes

163
Chapter 5
The Multivariate Normal Distribution
and its Application to Statistical  Inference
5.1.
Expectation and variance of random vectors
Multivariate distributions employ the concepts of expectation vector and variance matrix.
The expected "value", or more precisely, the expectation vector (sometimes also called the "mean
vector") of a random vector  
  is defined as the vector of expected values: 
X ' (x1,....,xn)T
E(X) '
def.
(E(x1),....,E(xn))T.
Adopting the convention that the expectation of a random matrix is the matrix of the
expectations of its elements, we can define the variance matrix of X as:1
Var(X) '
def.
E[(X & E(X))(X & E(X))T]
'
cov(x1,x1) cov(x1,x2) ... cov(x1,xn)
cov(x2,x1)
var(x2)
... cov(x2,xn)
:
:
"
:
cov(xn,x1) cov(xn,x2) ... cov(xn,xn)
.
(5.1)
Recall that the diagonal elements of the matrix (5.1) are variances:cov(xj,xj) ' var(xj).
Obviously, a variance matrix is symmetric and positive (semi-)definite. Moreover, note that (5.1)
can be written as
Var(X) ' E[XX T] & (E[X])(E[X])T.
(5.2

164
Similarly, the covariance matrix of a pair of random vectors X and Y is the matrix of covariances
of their components:2
Cov(X,Y) '
def.
E[(X & E(X))(Y & E(Y))T].
(5.3)
Note that  
 Thus, for each pair X, Y there are two covariance matrices,
Cov(Y,X) ' Cov(X,Y)T.
one being the transpose of the other.
5.2.
The multivariate normal distribution
Now let the components of  X =  
 be independent standard normally
(x1,....,xn)T
distributed random variables. Then  
 and   
  Moreover, the joint
E(X) ' 0 (0 ún)
Var(X) ' In.
density f(x) = f(x1,...,xn)  of X in this case is the product of the standard normal marginal densities:
f(x) ' f(x1,...,xn) ' k
n
j'1
exp &x 2
j /2
2π
'
exp & 1
2'n
j'1x 2
j
( 2π)n
'
exp & 1
2x Tx
( 2π)n
.
The shape of this density for the case n = 2 is displayed in Figure 5.1:
Figure 5.1: The bivariate standard normal density on [-3,3]×[-3,3]

165
Next, consider the following linear transformations of X: Y = F + AX,  where µ = 
 is a vector of constants and A is a non-singular n × n matrix with non-random
(µ1,...,µn)T
elements.  Because A is nonsingular and therefore invertible, this transformation is a one-to-one
mapping, with inverse 
 Then the density function g(y) of Y is equal to:
X ' A &1(Y&µ).
g(y) ' f(x)*det(Mx/My)* ' f(A &1y & A &1µ)*det M(A &1y&A &1µ)/My *
' f(A &1y&A &1µ)*det(A &1)* ' f(A &1y&A &1µ)
*det(A)*
'
exp & 1
2(y&µ)T(A &1)TA &1(y&µ)
( 2π)n*det(A)*
'
exp & 1
2(y&µ)T(AA T)&1(y&µ)
( 2π)n *det(AA T)*
.
Observe that F is the expectation vector of Y:  
 =  F. But what is
E(Y) ' µ % A E(X)
AAT?  We know from (5.2) that Var(Y) = E[YYT] ! FFT.  Therefore, substituting Y = F + AX
yields:
Var(Y) ' E[(µ%AX)(µT%X TA T) & µµT]
' µ E(X T) A T % A E(X) µT % A E(XX T) A T ' AA T,
because E(X) = 0 and 
 Thus, AAT is the variance matrix of Y. This argument gives
E[XX T] ' In.
rise to the following definition of the n-variate normal distribution:
Definition 5.1: Let Y be an n×1 random vector satisfying E(Y) = F and Var(Y) = E, where E is
nonsingular. Then Y is distributed  Nn(F,E) if the density g(y) of Y is of the form
g(y) '
exp & 1
2(y&µ)TΣ&1(y&µ)
( 2π)n
det(Σ)
.
(5.4)

166
In the same way as before we can show that a nonsingular (hence one-to-one) linear
transformation of a normal distribution is normal itself:
Theorem 5.1: Let Z = a + BY, where Y is distributed Nn(F,E) and B is a non-singular matrix of
constants.  Then Z is distributed Nn(a + BF, BEBT).
Proof: First, observe that: Z = a + BY implies Y = B!1(Z!a). Let h(z) be the density of Z
and g(y) the density of Y.  Then
h(z) ' g(y)*det(My/Mz)* ' g(B &1z&B &1a)*det(M(B &1z&B &1a)/Mz)* ' g(B &1z&B &1a)
*det(B)*
' g(B &1(z&a))
det(BB T)
'
exp & 1
2(B &1(z&a)&µ)TΣ&1(B &1(z&a)&µ)
( 2π)n det(Σ) det(BB T)
'
exp & 1
2(z&a&Bµ)T(BΣB T)&1(z&a&Bµ)
( 2π)n det(BΣB T)
.
Q.E.D.
I will now relax the assumption in Theorem 5.1 that the matrix B is a nonsingular n × n
matrix. This more general version of Theorem 5.1 can be proved using the moment generating
function or the characteristic function of the multivariate normal distribution.
Theorem 5.2: Let Y be distributed Nn(F,E). Then the moment generating function of Y is 
 and the characteristic of Y is  
m(t) ' exp(t Tµ % t TΣt/2),
n(t) ' exp(i.t Tµ & t TΣt/2).

167
Proof: We have
m(t) ' mexp[t Ty]
exp & 1
2(y&µ)TΣ&1(y&µ)
( 2π)n det(Σ)
dy
' m
exp & 1
2 y TΣ&1y & 2µTΣ&1y % µTΣ&1µ & 2t Ty
( 2π)n det(Σ)
dy
' m
exp & 1
2 y TΣ&1y & 2(µ%Σ t)TΣ&1y % (µ%Σ t)TΣ&1(µ%Σ t)
( 2π)n det(Σ)
dy
× exp 1
2 (µ%Σ t)TΣ&1(µ%Σ t) & µTΣ&1µ
' m
exp & 1
2(y&µ&Σ t)TΣ&1(y&µ&Σ t)
2π
n det(Σ)
dy × exp t Tµ %
1
2t TΣt .
Since the last integral is equal to one, the result for the moment generating function follows. The
result for the characteristic function follows from 
 Q.E.D.
n(t) ' m(i.t).
Theorem 5.3: Theorem 5.1 holds for any linear transformation  Z = a + BY.
Proof: Let Z = a + BY, where B is m × n. It is easy to verify that the characteristic function
of Z is: 
 =
nZ(t) ' E[exp(i.t TZ)] ' E[exp(i.t T(a%BY))] ' exp(i.t Ta)E[exp(i.t TBY)]
 Theorem 5.3 follows now from Theorem 5.2.. Q.E.D.
exp i.(a%Bµ)Tt & ½t TBΣB Tt .
Note that this result holds regardless whether the matrix 
 is nonsingular or not. In
BΣB T
the latter case the normal distribution involved is called "singular":

168
Definition 5.2: An n × 1 random vector Y has a singular Nn(F,E) distribution if its characteristic
function is of the form  
 with  E a singular positive semi-define
nY(t) ' exp(i.t Tµ & ½t TΣt)
matrix..
Because of the latter, the distribution of the random vector Y involved is no longer absolutely
continuous, but the form of the characteristic function is the same as in the nonsingular case, and
that is all that matters. 
For example, let n = 2 and 
µ '
0
0
,
Σ '
1
0
0 σ2 ,
where 
 > 0 but small. The density of the corresponding  N2(F,E) distribution of  
σ2
Y ' (Y1,Y2)T
is
f(y1,y2|σ) '
exp(&y 2
1 /2)
2π
×
exp(&y 2
2 /(2σ2))
σ 2π
.
(5.5)
Then  
 Thus, a singular
limσ90 f(y1,y2|σ) ' 0 if y2 … 0, limσ90 f(y1,y2|σ) ' 4 if y2 ' 0.
multivariate normal distribution does not have a density. 
In Figure 5.2 the density (5.5) for the near-singular case 
 is displayed. The
σ2 ' 0.00001
height of the picture is actually rescaled to fit in the the box [-3,3]×[-3,3]×[-3,3]. If we let F
approach zero the height of the ridge corresponding to the marginal density of 
 will increase to
Y1
infinity.

169
Figure 5.2: Density of a near-singular normal distribution on [-3,3]×[-3,3]
The next theorem shows that uncorrelated multivariate normal distributed random
variables are independent. Thus, while for most distributions uncorrelatedness does not imply
independence, for the multivariate normal distribution it does.
Theorem 5.4: Let X be n-variate normally distributed, and let X1 and X2 be sub-vectors of 
components of X. If X1 and X2 are uncorrelated, i.e., 
 then X1 and X2 are
Cov(X1,X2) ' O,
independent.
Proof: Since  X1 and X2 cannot have common components, we may without loss of
generality assume that  
  Partition the expectation vector
X ' (X T
1 ,X T
2 )T, X1 0 úk, X2 0 úm.
and variance matrix of X conformably as: 

170
E(X) '
µ1
µ2
,
Var(X) '
Σ11 Σ12
Σ21 Σ22
.
Then E12 = O and  E21 = O because they are covariance matrices, and X1 and X2 are uncorrelated,
hence the density of X is:
f(x) ' f(x1,x2) '
exp & 1
2
x1
x2
&
µ1
µ2
T Σ11
0
0
Σ22
&1 x1
x2
&
µ1
µ2
( 2π)n
det
Σ11
0
0
Σ22
'
exp & 1
2(x1&µ1)TΣ&1
11(x1&µ1)
( 2π)k det(Σ11)
×
exp & 1
2(x2&µ2)TΣ&1
22(x2&µ2)
( 2π)m det(Σ22)
.
This implies independence of X1 and X2. Q.E.D.
5.3.
Conditional distributions of multivariate normal random variables
Let  Y be a scalar random variable and X be a k-dimensional random vector. Assume that
Y
X
- Nk%1
µY
µX
,
ΣYY ΣYX
ΣXY ΣXX
.
where 
 and
µY ' E(Y), µX ' E(X),
ΣYY ' Var(Y), ΣYX ' Cov(Y,X T) ' E[(Y&E(Y))(X&E(X))T],
ΣXY ' Cov(X,Y) ' E(X&E(X))(Y&E(Y)) ' ΣT
YX, ΣXX ' Var(X).

171
In order to  derive the conditional distribution of Y, given X, let  
 where " is
U ' Y & α & βTX,
a scalar constant and $ is a k × 1 vector of constants, such that E(U) = 0 and U and X are
independent. It follows from Theorem 5.1 that 
U
X
'
&α
0
%
1 &B T
0
Ik
Y
X
- Nk%1
&α % µY & βTµX
µX
,
1 &βT
0
Ik
ΣYY ΣYX
ΣXY ΣXX
1
0T
&β Ik
The variance matrix involved can be rewritten as:
Var
U
X
'
ΣYY&ΣYXβ&βTΣXY%βTΣXXβ
ΣYX&βTΣXX
ΣXY&ΣXXβ
ΣXX
.
(5.6)
Next, choose $ such that U and X are uncorrelated and hence independent. In view of (5.6) a
necessary and sufficient condition for that is: 
 hence 
 Moreover,
ΣXY & ΣXXβ ' 0,
β ' Σ&1
XXΣXY.
E(U) = 0 if  
 Then
α ' µY & βTµX.
ΣYY & ΣYXβ & βTΣXY % βTΣXXβ ' ΣYY & ΣYXΣ&1
XXΣXY,
ΣYX & βTΣXX ' 0T,
ΣXY & ΣXXβ ' 0,
and consequently 
U
X
- Nk%1
0
µX
,
ΣYY&ΣYXΣ&1
XXΣXY
0T
0
ΣXX
.
(5.7)
Thus U and X are independent normally distributed, and consequently 
 = 0. 
E(U*X) ' E(U)
Since  
 we now have  
Y ' α % βTX % U,
E(Y*X) ' α % βT E(X*X) % E(U*X) ' α % βTX.

172
Moreover, it is easy to verify from (5.7) that the conditional density of Y, given X = x, is
f(y*x) '
exp& 1
2(y&α&βTx)2/σ2
u
σu 2π
,
where σ2
u ' ΣYY & ΣYXΣ&1
XXΣXY.
Furthermore, note that 
 is just the conditional variance of Y, given X:
σ2
u
σ2
u ' var(Y*X) '
def.
E Y & E(Y*X) 2*X .
Summarizing:
Theorem 5.5: Let
Y
X
- Nk%1
µY
µX
,
ΣYY ΣYX
ΣXY ΣXX
,
where 
, and EXX  is nonsingular.  Then conditionally on X, Y is normally distributed
Y0ú, X0úk
with conditional expectation  
 and
E(Y*X) ' α%βTX, where β ' Σ&1
XXΣXY and α ' µY&βTµX,
conditional variance var(Y*X) ' ΣYY & ΣYXΣ&1
XXΣXY.
The result in Theorem 5.5 is the basis for linear regression analysis. Suppose that Y
measures an economic activity that is partly caused or influenced by other economic variables,
measured by the components of the random vector X. In applied economics the relation between
Y, called the dependent variable, and the components of X, called the independent variables or
the regressors, is often modeled linearly as Y = " + $TX + U, where  " is the intercept, $ is the
vector of slope parameters (also called regression coefficients), and U is an error term which is

173
usually assumed to be independent of X and normally N(0,F2) distributed. Theorem 5.5 shows
that if Y and X are jointly normally distributed, then such a linear relation between Y and X exists. 
 
5.4.
Independence of linear and quadratic transformations of multivariate normal
random variables
Let X be distributed Nn(0,In), i.e., X is n-variate standard normally distributed. Consider
the linear transformations Y = BX, where B is a k × n matrix of constants, and Z = CX, where  C
is an m × n matrix of constants. It follows from Theorem 5.4 that
Y
Z
- Nk%m
0
0
,
BB T BC T
CB T CC T
.
Then Y and Z are uncorrelated and therefore independent if and only if CBT = O. More generally
we have:
Theorem 5.6: Let X be distributed Nn(0,In), and consider the linear transformations Y = b + BX,
where b is a k × 1 vector and B a k × n matrix of constants, and Z = c + CX, where c is an m × 1
vector and C an m × n matrix of constants. Then Y and Z are independent if and only if BCT = O.
This result can be used to set forth conditions for independence of linear and quadratic
transformations of standard normal random vectors:
Theorem 5.7: Let X and Y be defined as in Theorem 5.6, and let Z = XTCX, where C is a
symmetric n × n matrix of constants. Then Y and Z are independent if BC = O.

174
Proof: First, note that the latter condition only makes sense if C is singular, as otherwise
B = O. Thus, let rank(C) = m < n.  We can write 
 where 7 is a diagonal matrix with
C ' QΛQ T,
the eigenvalues of C on the diagonal, and Q is the orthogonal matrix of corresponding
eigenvectors.  Let V = QTX, which is Nn(0,In) distributed because QQT = In. Since n ! m
eigenvalues of C are zero, we can partition Q, 7 and V such that
Q ' (Q1,Q2),
Λ '
Λ1 O
O
O
,
V '
V1
V2
'
Q T
1 X
Q T
2 X
, Z ' V T
1 Λ1V1,
where 71 is the diagonal matrix with the m nonzero eigenvalues of C on the diagonal. Then
BC ' B(Q1,Q2)
Λ1 O
O
O
Q T
1
Q T
2
' BQ1Λ1Q T
1 ' O
implies  
 which in its
BQ1Λ1 ' BQ1Λ1Q T
1 Q1 ' O (because Q TQ ' In implies Q T
1 Q1 ' Im),
turn implies that BQ1 = O. The latter is a sufficient condition for the independence of V1 and Y,
hence of the independence of Z and Y. Q.E.D.
Finally, consider the conditions for independence of two quadratic forms of standard
normal random vectors:
Theorem 5.8: Let 
 where A and B are symmetric n × n
X - Nn(0,In), Z1 ' X TAX, Z2 ' X TBX,
matrices of constants. Then Z1 and Z2 are independent if and only if  AB = O.
The proof of Theorem 5.8 is not difficult but quite lengthy and therefore given in the

175
Appendix 5.A.
5.5.
Distributions of quadratic forms of multivariate normal random variables
As we will see in Section 5.6 below,  quadratic forms of multivariate normal random
variables play a key-role in statistical testing theory. The two most important results are stated in
Theorems 5.9 and 5.10:
Theorem 5.9: Let X be distributed Nn(0,E), where E is nonsingular. Then XTE!1X is distributed 
as Pn
2.
Proof: Denote  
 Then Y is n-variate standard normally
Y ' (Y1,....,Yn)T ' Σ&½X.
distributed, hence Y1,...,Yn are i.i.d. N(0,1) and thus  
 Q.E.D.
X TΣ&1X ' Y TY ' 'n
j'1Y 2
j - χ2
n.
The next theorem employs the concept of an idempotent matrix. Recall from Appendix I
that a square matrix M is idempotent if M2 = M. If M is also symmetric, we can write M = Q7QT,
where 7 is the diagonal matrix of eigenvalues of M and Q is the corresponding orthogonal matrix
of eigenvectors. Then M2 = M implies 72 = 7, hence the eigenvalues of M are either 1 or 0. If all
eigenvalues are 1, then 7 = I, hence M = I. Thus the only nonsingular symmetric idempotent
matrix is the unit matrix. Consequently, the concept of a symmetric idempotent matrix is only
meaningful if the matrix involved is singular.
The rank of a symmetric idempotent matrix M equals the number of nonzero eigenvalues,
hence trace(M) = trace(Q7QT) = trace(7QTQ) = trace(7) = rank(7) = rank(M), where trace(M) is
defined as the sum of the diagonal elements of M. Note that we have used the property trace(AB)

176
= trace(BA) for conformable matrices A and B.
Theorem 5.10: Let X be distributed Nn(0,I), and let M be a symmetric idempotent n × n matrix of
constants with rank k. Then XTMX is distributed Pk
2.
Proof: We can write
M ' Q
Ik O
O O
Q T,
where Q is the orthogonal matrix of eigenvectors. Since 
 we
Y ' (Y1,...,Yn)T ' Q TX - Nn(0,I)
now have
X TMX ' Y T Ik O
O O
Y ' j
k
j'1
Y 2
j - χ2
k .
Q.E.D.
5.6. 
Applications to statistical inference under normality
5.6.1
Estimation
Statistical inference is concerned with parameter estimation and parameter inference. The
latter will be discussed in the next subsections.
Loosely speaking, an estimator of a parameter is a function of the data which serves as an
approximation of the parameter involved. For example, if X1, X2,...,Xn is a random sample from
the N(F,F2) distribution then the sample mean 
 may serve as an estimator of the
X ' (1/n)'n
j'1Xj
unknown parameter µ (the population mean). More formally,  given a data set {X1, X2,...,Xn } for

177
which the joint distribution function depends on an unknown parameter (vector) 2, an estimator
of 2 is a Borel measurable function  =  gn(X1,...,Xn) of the data which serves as an
ˆθ
approximation of 2. Of course, the function gn should not depend on unknown parameters itself.
In principle we can construct many functions of the data that may serve as an
approximation of an unknown parameter. For example, one may consider using  X1 only as an
estimator of µ. So the question arises which function of the data should be used. In order to be
able to select among the many candidates for an estimator, we need to formulate some desirable
properties of estimators. The first one is unbiasedness:
Definition 5.3: An estimator   of a  parameter (vector) 2 is unbiased if  
 = 2.
ˆθ
E[ˆθ]
The unbiasedness property is not specific to a particular value of the parameter involved, but
should hold for all possible values of this parameter, in the sense that if we draw a new data set
from the same type of distribution but with a different parameter value, the estimator should stay
unbiased. In other words, if the joint distribution function of the data is Fn(x1,...,xn|2), where 2 0
1 is an unknown parameter (vector) in a parameter space 1, i.e., the space of all possible values
of 2,  and  
 is an unbiased estimator of 2, then
ˆθ ' gn(X1,....,Xn)
mgn(x1,....,xn)dFn(x1,....,xn|θ) ' θ
for all 2 0 1. 
Note that in the above example both 
 and  X1 are unbiased estimators of µ. Thus, we
X
need a further criterion in order to select an estimator. This criterion is efficiency:
Definition 5.4: An unbiased estimator   of an unknown scalar parameter 2 is efficient  if for all
ˆθ

178
other unbiased estimators 
  
 In the case that 2 is a parameter vector the
˜θ, var(ˆθ) # var(˜θ).
latter reads:  
  is a positive semi-definite matrix..
Var(˜θ) & Var(ˆθ)
In our example,   X1  is not an efficient  estimator of  µ, because  
 and
var(X1) ' σ2
 But is 
 efficient?  In order to answer this question, we need to derive the
var(X) ' σ2/n.
X
minimum variance of an unbiased estimator, as follows. For notational convenience, stack the
data  in a vector X. Thus,  in the univariate case, X = (X1, X2,...,Xn )T, and in the multivariate case,
X =  
 Assume that the joint distribution of X is absolutely continuous with density
(X T
1 ,....,X T
n )T.
fn(x|2), which for each x is twice continuously differentiable in 2. Moreover, let 
 be an
ˆθ ' gn(X)
unbiased estimator of 2. Then
mgn(x)fn(x|θ)dx ' θ
(5.8)
Furthermore, assume for the time being that 2 is a scalar, and let
d
dθmgn(x)fn(x|θ)dx ' mgn(x) d
dθ
fn(x|θ)dx.
(5.9)
Conditions for (5.9)  can be derived from the mean-value theorem and the dominated
convergence theorem. In particular, (5.9) is true for all 2 in an open set 1 if
 
 < 4. 
m|gn(x)|supθ0Θ|d 2fn(x|θ)/(dθ)2|dx
Then if follows from (5.8) and (5.9) that 
mgn(x) d
dθln(fn(x|θ)) fn(x|θ)dx ' mgn(x) d
dθfn(x|θ)dx ' 1
(5.10)
Similarly, if 

179
d
dθmfn(x|θ)dx ' m
d
dθfn(x|θ)dx
(5.11)
which is true for all 2 in an open set 1 for which 
 then, since
msupθ0Θ|d 2fn(x|θ)/(dθ)2|dx < 4,
 = 1, we have
mfn(x|θ)dx
m
d
dθln(fn(x|θ)) fn(x|θ)dx ' m
d
dθfn(x|θ)dx ' 0.
(5.12)
Denoting 
 it follows now from (5.10) that 
 and from (5.12) that
ˆβ ' dln(fn(X|θ))/dθ,
E[ˆθ.ˆβ] ' 1
 = 0. Therefore, 
Since by the  Cauchy-Schwartz
E[ˆβ]
cov(ˆθ,ˆβ) ' E[ˆθ.ˆβ] & E[ˆθ]E[ˆβ] ' 1.
inequality,  
 we now have that 
|cov(ˆθ,ˆβ)| #
var(ˆθ) var(ˆβ),
var(ˆθ) $ 1/var(ˆβ):
var(ˆθ) $
1
E d ln(fn(X|θ))/dθ 2 .
(5.13)
This result is known as the Cramer-Rao inequality, and the right-hand side of (5.13) is called the
Cramer-Rao lower bound. More generally we have:
Theorem 5.11: (Cramer-Rao) Let  fn(x|2) be the joint density of the data, stacked in a vector X, 
where 2 is a parameter vector. Let   be an unbiased estimator of  2. Then  
 =
ˆθ
Var(ˆθ)
 where D is a  positive semi-definite matrix.
E (Mln(fn(X|θ)/MθT) (Mln(fn(X|θ)/Mθ)
&1 % D,
Now let us return to our problem whether the sample mean  
 of a random sample from
X
the N(F,F2) distribution is an efficient estimator of µ. In this case the joint density of the sample
is 
 hence  
 and
fn(x|µ,σ2) ' (n
j'1exp(&½(xj&µ)2/σ2)/ σ22π,
Mln(fn(X|µ,σ2))/Mµ ' 'n
j'1(Xj&µ)/σ2
thus the Cramer-Rao lower bound is

180
1
E Mln(fn(X|µ,σ2))/Mµ 2 ' σ2/n.
(5.14)
This is just the variance of the sample mean 
 hence
 is an efficient estimator of µ.  This result
X,
X
holds for the multivariate case as well:
Theorem 5.12: Let X1, X2,...,Xn be a random sample from the 
 distribution. Then the
Nk[µ ,Σ]
sample mean  
 is an unbiased and efficient estimator of  µ.
X ' (1/n)'n
j'1Xj
The sample variance of a random sample X1, X2,...,Xn  from a univariate distribution with
expectation µ and variance  
 is defined by
σ2
S 2 ' (1/(n&1))'n
j'1(Xj&X )2,
(5.15)
which serves as an estimator of F2.  An alternative form of the sample variance is 
ˆσ2 ' (1/n)'n
j'1(Xj&X )2 ' n&1
n
S 2,
(5.16)
but as I will show for the case of a random sample from the N(F,F2) distribution, (5.15) is an
unbiased estimator, and (5.16) is not:
Theorem 5.13: Let S2 be the sample variance of a random sample X1,...,Xn from the N(F,F2)
distribution. Then (n!1)S2/F2 is distributed χ2
n&1.
The proof of Theorem 5.13 is left as an exercise. Since the expectation of the 
 distribution is
χ2
n&1
n!1, this result implies that  
 whereas by (5.16), 
Moreover,
E(S 2) ' σ2,
E(ˆσ2) ' σ2(n&1)/n.

181
since the variance of the
 distribution is 2(n!1), it follows from Theorem 5.13 that
χ2
n&1
Var(S 2) ' 2σ4/(n&1).
(5.17)
The Cramer-Rao lower bound for an unbiased estimator of  
 is 
 so that  
 is not
σ2
2σ4/n,
S 2
efficient, but it is close if n is large.
For a random sample X1, X2,...,Xn  from a multivariate distribution with expectation vector
µ and variance matrix G the sample variance matrix takes the form
ˆΣ ' (1/(n&1))'n
j'1(Xj&X )(Xj&X )T.
(5.18)
This is also an unbiased estimator of 
 even if the distibution involved is not
Σ ' Var(Xj),
normal.
5.6.2
Confidence intervals
Since estimators are approximations of unknown parameters, the question arises how
close they are. I will answer this question for the sample mean and the sample variance in the
case of a random sample X1, X2,...,Xn from the N(F,F2) distribution. 
It is almost trivial that 
hence
X - N(µ ,σ2/n),
n(X&µ)/σ - N(0,1).
(5.19)
Therefore, for given " 0 (0,1) there exists a $ > 0 such that 
P[*X&µ* # βσ/ n] ' P * n(X&µ)/σ* # β ' m
β
&β
exp(&u 2/2)
2π
du ' 1 & α.
(5.20)
For example, if we choose " = 0.05 then $ = 1.96, so that in this case 
P[X&1.96σ/ n # µ # X%1.96σ/ n] ' 0.95
The interval 
 is called the 95% confidence interval of µ. If F would be
[X&1.96σ/ n,X%1.96σ/ n]

182
known, then this interval can be computed and will tell us how close  
 and µ are, with a margin
X
of error of 5%. But in general F is not known, so how do we proceed then?
In order to solve this problem, we need the following corollary of Theorem 5.7:
Theorem 5.14: Let X1, X2,...,Xn be a random sample from the N(F,F2) distribution. Then the
sample mean  
 and the sample variance  
 are independent.
X
S 2
Proof: Observe that 
  
 +
X( ' ((X1&µ)/σ,(X2&µ)/σ,.....,(Xn&µ)/σ)T - Nn(0,In), X ' µ
 say,  and
(σ/n,...,σ/n)X( ' b % BX(,
(X1& ¯X)/σ
:
(Xn& ¯X)/σ
'
I & 1
n
1
1
:
1
(1,1,...,1) X( ' CX(, say.
The latter implies that  
  because C is
(n&1)S 2/σ2 ' X T
( C TCX( ' X T
( C 2X( ' X T
( CX(,
symmetric and idempotent, with rank(C) = trace(C) = n ! 1. Therefore, by Theorem 5.7  the
sample mean and the sample variance are independent if BC = 0, which in the present case is
equivalent to the condition CBT = 0. The latter is easily verified:
CB T ' σ
n
I & 1
n
1
1
:
1
(1,.....,1)
1
1
:
1
' σ
n
1
1
:
1
& 1
n
1
1
:
1
n
' 0
Q.E.D.
It follows now from (5.19),  Theorems 5.13 and 5.14, and the definition of the Student t
distribution  that:

183
Theorem 5.15: Under the conditions of Theorem 5.14, n( ¯X & µ)/S - tn&1.
Recall from Chapter 4 that the tn!1 distribution has density
hn&1(x) '
Γ(n/2)
(n&1)π Γ((n&1)/2) (1%x 2/(n&1))n/2 ,
(5.21)
where 
 Thus, similarly to (5.20), for each  " 0 (0,1) and sample
Γ(y) ' m
4
0 x y&1exp(&x)dx, y > 0.
size n there exists a $n > 0 such that
P[*X&µ* # βnS/ n] ' m
βn
&βn
hn&1(u)du ' 1 & α,
(5.22)
so that  
 is now the (1!")×100% confidence interval of µ
[X&βnS/ n,X%βnS/ n]
Similarly, on the basis of Theorem 5.13 we can construct confidence intervals of 
 
σ2.
Recall from Chapter 4 that the  
 distribution has density 
χ2
n&1
gn&1(x) ' x (n&1)/2&1exp(&x/2)
Γ((n&1)/2)2(n&1)/2 .
For given  " 0 (0,1) and sample size n we can choose   $1,n   <   $2,n   be such that  
P[(n&1)S 2/β2,n # σ2 # (n&1)S 2/β1,n] ' P[β1,n # (n&1)S 2/σ2# β2,n]
' m
β2,n
β1,n
gn&1(u)du ' 1 & α.
(5.23)
There are different ways to choose $1,n  and  $2,n  such that the last equality in (5.23) holds.
Clearly, the optimal choice is  such that  
 is minimal because it will yield the smallest
β&1
1,n&β&1
2,n
confidence interval, but that is computationally complicated. Therefore, in practice $1,n  and  $2,n

184
are often chosen such that
m
β1,n
0
gn&1(u)du ' α/2,
m
4
β2,n
gn&1(u)du ' α/2.
(5.24)
A practical point is how to solve the integral equations in (5.20), (5.22) and (5.24). Most
statistics and econometrics textbooks contain tables from which you can look op the values of the
$’s involved, given ". Moreover,  there are various web pages from which you can download 
programs to calculate these values.3
5.6.3
Testing parameter hypotheses
Suppose  you consider starting up a business to sell a new product in the USA, say a
particular type of European car which is not yet imported in the US. In order to determine
whether there is a market for this car in the US, you have selected randomly n persons from the
population of potential buyers of this car. Each person j in the sample is asked how much he or
she would be willing to pay for this car. Let the answer be Yj. Moreover, suppose that the cost of
importing this car  is a fixed amount Z per car. Denote 
  and assume that  Xj is 
Xj ' ln(Yj/Z),
N(F,F2) distributed.  If F > 0 then your planned car import business will be profitable, otherwise
you should forget about this idea. 
In order to decide whether F > 0 or F # 0, you need a decision rule based on the random
sample   X = (X1, X2,...,Xn )T.  Any decision rule takes the following form. Given a subset C of ún,
to be determines below, decide that F > 0 if X 0 C, and decide that F # 0 if X ó C.  Thus, you
decide that the hypothesis  F # 0 is true if 
 and you decide that the hypothesis F >
I(X 0 C) ' 0,
0 is true if 
 In this case the hypothesis  F # 0 is called the null hypothesis, usually
I(X 0 C) ' 1.

185
denoted by H0: F # 0, and the hypothesis  F > 0 is called the alternative hypothesis, denoted by 
H1:  F > 0.  The procedure itself is called a statistical test.
This decision rule yields two types of errors. The first one, called the type I error, is that
you decide that H1 is true while in reality H0 is true. The other error, called the type II error, is
that you decide that H0 is true while in reality  H1 is true. Both errors come with costs. If the type
I error occurs you will  incorrectly assume that your car import business will be profitable, so that
you will loose your investment if you start up you business. If the type II error occurs you will
forgo a profitable business opportunity. Clearly, the type I error is the more serious of the two. 
Now choose C such that 
 if and only if  
 for some fixed $ > 0. Then
X 0 C
n(X/S) > β
P[X 0 C] ' P[ n(X/S) > β] ' P[ n(X&µ)/S %
nµ /S > β]
' P[ n (X&µ)/σ %
n µ /σ > β.S/σ]
' m
4
&4P[S/σ < (u %
n µ /σ)/β]exp[&u 2/2]/ 2π du,
(5.25)
where the last equality follows from Theorem 5.14 and (5.19).  If F # 0 this probability is the
probability of a type I error. Clearly, the probability (5.25) is an increasing function of µ, hence
the maximum probability of a type I error is obtained for µ = 0. But if µ = 0 then it follows from
Theorem 5.15 that 
 hence
n(X/S) - tn&1,
maxµ# 0P[X 0 C] ' m
4
β hn&1(u)du,
(5.26)
where 
 is the density of the 
 distribution.  See (5.21). The probability (5.26) is called the
hn&1
tn&1
size of the test of the null hypothesis involved, which is the maximum risk of a type I error, and
"×100% is called the significance level of the test.  Depending on how risk averse you are, you

186
have to choose a size " 0 (0,1), and therefore you have to choose  $ =  $n such
that
 This value $n is called the critical value of the test involved, and since it is
m
4
βn
hn&1(u)du ' α.
based on the distribution of 
 the latter is called the test statistic involved.  
n(X/S),
Replacing $ in  (5.25) by  $n , 1 minus the probability of a type II error is a function of µ/F
> 0:
 
ρn(µ/σ) '
m
4
&
n µ /σ
P[S/σ < (u %
n µ /σ)/βn] exp(&u 2/2)
2π
du, µ > 0.
(5.27)
This function is called the power function, which is the probability of correctly rejecting the null
hypothesis H0 in favor of the alternative hypothesis H1. Consequently, 
 is
1 & ρn(µ/σ), µ > 0,
the probability of a type II error. 
The test in this example is called a t-test, because the critical value  $n  is derived from
the t-distribution. 
A test is said to be consistent if the power function converges to 1 as n 6 4 for all values
of the parameter(s) under the alternative hypothesis.  Using the results in the next chapter it can
be shown that the above test is consistent:
limn64ρn(µ/σ) ' 1 if µ > 0.
(5.28)
Now let us consider the test of the null hypothesis   H0: F = 0 against the alternative
hypothesis  H1: F … 0.  Under the null hypothesis,  
 exactly. Given the size " 0
n(X/S) - tn&1
(0,1), choose the critical value  $n > 0 as in (5.22). Then H0  is accepted  if  
 and
| n(X/S)| # βn
rejected in favor of H1 if  
 The power function of this test is
| n(X/S)| > βn.

187
ρn(µ/σ) ' m
4
&4P[S/σ < |u %
nµ/σ|/βn]exp[&u 2/2]/ 2π du, µ … 0.
(5.29)
This test is known as is the two-sided t-test. Also this test is consistent:
limn64ρn(µ/σ) ' 1 if µ … 0.
(5.30)
5.7. 
Applications to regression analysis
5.7.1
The linear regression model
Consider a random sample 
  j = 1,2,...,n,  from a k-variate nonsingular
Zj ' (Yj,X T
j )T,
normal distribution, where 
 We have seen in Section 5.3 that we can write 
Yj 0 ú, Xj 0 úk&1.
Yj ' α % X T
j β % Uj,
Uj - N(0,σ2), j ' 1,..,n,
(5.31)
where  
  is independent of Xj.   This is the classical linear regression model,
Uj ' Yj & E[Yj|Xj]
where Yj is the dependent variable, Xj is the vector of independent variables, also called the
regressors, and Uj is the error term. This model is widely used in empirical econometrics, even in
the case where Xj  is not known to be normally distributed. 
Denoting 
Y '
Y1
!
Yn
,
X '
1 X T
1
!
!
1 X T
n
,
θ0 '
α
β
,
U '
U1
!
Un
,
model (5.31) can be written in vector/matrix form as
Y ' Xθ0 % U,
U|X - Nn[0,σ2In],
(5.32)
where U|X is a short-hand notation for " U conditional on X".  
In the next subsections I will address the problems how to estimate the parameter vector

188
20 and how to test various hypotheses about 20 and its components.
5.7.2
Least squares estimation
Observe that
E[(Y & Xθ)T(Y & Xθ)] ' E[(U % X(θ0&θ))T(U % X(θ0&θ))]
' E[U TU] % 2(θ0&θ)TE X TE[U|X] % (θ0&θ)T E[X TX] (θ0&θ)
' n.σ2 % (θ0&θ)T E[X TX] (θ0&θ).
(5.33)
Hence it follows from (5.33) that4
θ0 ' argmin
θ0úk
E[(Y & Xθ)T(Y & Xθ)] ' E[X TX] &1E[X TY],
(5.34)
provided that the matrix 
 is nonsingular. However, the nonsingularity of the distribution
E[X TX]
of  
 guarantees that  
 is nonsingular, because it follows from Theorem 5.5
Zj ' (Yj,X T
j )T
E[X TX]
that the solution (5.34)  is  unique if GXX =  
 is nonsingular.
Var(Xj)
The expression (5.34) suggests to estimate 20  by the Ordinary5  Least Squares (OLS)
estimator 
ˆθ ' argmin
θ0úk
(Y & Xθ)T(Y & Xθ) ' X TX &1X TY.
(5.35)
It follows easily from (5.32) and (5.35) that
ˆθ & θ0 ' X TX &1X TU,
(5.36)
hence  is conditionally unbiased: 
  and therefore also unconditionally unbiased:
ˆθ
E[ˆθ|X] ' θ0,
 More generally, 
E[ˆθ] ' θ0.

189
ˆθ|X - Nk[θ0,σ2(X TX)&1].
(5.37)
Of course, the unconditional distribution of  is not normal. 
ˆθ
Note that  the OLS estimator is not efficient, because 
 is  the Cramer-Rao
σ2(E[X TX])&1
lower bound of an unbiased estimator of (5.37), and  Var(ˆθ) ' σ2E[(X TX)&1] … σ2(E[X TX])&1.
However, the OLS estimator is the most efficient of all conditionally unbiased estimators  of
˜θ
(5.37) that are linear functions of Y. In other words, the OLS estimator is the  Best Linear
Unbiased Estimator (BLUE).  This result is known as the Gauss-Markov theorem:
Theorem 5.16: (Gauss-Markov theorem) Let C(X)  be a k×n matrix whose elements are Borel
measurable functions of the random elements of X, and let 
 If  
 then for
˜θ ' C(X)Y.
E[˜θ|X] ' θ0
some positive semi-definite k×k matrix D,   
 + D. 
Var[˜θ|X] ' σ2C(X)C(X)T ' σ2(X TX)&1
Proof: The conditional unbiasedness condition implies that C(X)X = Ik, hence   =   20  +
˜θ
C(X)U, and thus  
 Now
Var(˜θ|X) ' σ2C(X)C(X)T.
D ' σ2[C(X)C(X)T & (X TX)&1] ' σ2[C(X)C(X)T & C(X)X(X TX)&1X TC(X)T]
' σ2C(X)[In & X(X TX)&1X T]C(X)T ' σ2C(X)MC(X)T,
say, where the second equality follows from the unbiasedness condition CX = Ik. The matrix
M ' In & X X TX &1X T
(5.38)
is idempotent, hence its eigenvalues are either 1 or 0.  Since all the eigenvalues are non-negative,
M is positive semi-definite, and so is C(X)MC(X)T.  Q.E.D.
Next, we need an estimator of the error variance F2.  If we would observe the errors Uj,

190
then we could use the sample variance  
of the Uj ‘s as an unbiased
S 2 ' (1/(n&1))'n
j'1(Uj&U )2
estimator. This suggests to use OLS residuals,
ˆUj ' Yj & ˜X
T
j ˆθ, where ˜Xj '
1
Xj
,
(5.39)
instead of the actual errors Uj in this sample variance.  Taking into account that
'n
j'1 ˆUj / 0,
(5.40)
the feasible variance estimator involved takes the form 
 However, this
ˆS
2 ' (1/(n&1))'n
j'1 ˆU
2
j .
estimator is not unbiased, but a minor correction will yield an unbiased estimator of  F2, namely
S 2 ' (1/(n&k))'n
j'1 ˆU
2
j ,
(5.41)
which is called the OLS estimator of F2:  The unbiasedness of this estimator is a by-product of
the following more general result, which is related to the result of Theorem 5.13.
Theorem 5.17:  Conditional on X and well as unconditionally,  
  hence
(n&k)S 2/σ2 - χ2
n&k,
E[S 2] ' σ2.
Proof: Observe that 
'n
j'1 ˆU
2
j ' 'n
j'1(Yj & ˜X
T
j ˆθ)2 ' 'n
j'1 Uj & ˜X
T
j (ˆθ&θ0)
2
' 'n
j'1U 2
j & 2 'n
j'1Uj ˜X
T
j (ˆθ&θ0) % (ˆθ&θ0)T 'n
j'1 ˜X
T
j ˜Xj (ˆθ&θ0)
' U TU & 2U TX(ˆθ&θ0) % (ˆθ&θ0)X TX(ˆθ&θ0)
' U TU & U TX(X TX)&1X TU ' U TMU,
(5.42)
where the last two equalities follow from (5.36) and (5.38), respectively. Since the matrix M is

191
idempotent, with rank 
rank(M) ' trace(M) ' trace(In) & traceX X TX &1X T
' trace(In) & trace X TX &1X TX
' n&k
it follows from Theorem 5.10 that conditional on X, (5.42) divided by 
 has a 
 distribution:
σ2
χ2
n&k
'n
j'1 ˆU
2
j /σ2|X - χ2
n&k.
(5.43)
It is left as an exercise to prove that (5.43) implies that also the unconditional distribution of
(5.42)  divided by 
 is 
:
σ2
χ2
n&k
'n
j'1 ˆU
2
j /σ2 - χ2
n&k.
(5.44)
Since the expectation of the 
 distribution is n!k, it follows from (5.44) that the OLS
χ2
n&k
estimator (5.41) of 
 is unbiased. Q.E.D.
σ2
Next, observe from (5.38) that XTM = O, so that by Theorem 5.7 (XTX)!1XTU and UTMU
are independent, conditionally on X, i.e.
 
P[X TU # x and U TMU # z|X] ' P[X TU # x|X].P[U TMU # z|X], œ x 0 úk, z $ 0.
Consequently,
Theorem 5.18: Conditional on X,  
 are independent,
ˆθ and S 2
but unconditionally they can be dependent.
Theorems 5.17 and 5.18 yield two important corollaries, which I will state in the next
theorem. These results play a key role in statistical testing.

192
Theorem 5.19:
(a)
 Let  
be a given nonrandom vector. Then 
c 0 úk
c T(ˆθ&θ0)
S c T(X TX)&1c
- tn&k.
(5.45)
(b)
Let R be a given nonrandom m×k matrix with rank m # k. Then 
(ˆθ&θ0)TR T R X TX &1R T &1R(ˆθ&θ0)
m.S 2
- Fm,n&k.
(5.46)
Proof of (5.45): It follows from (5.37) that  
 hence
c T(ˆθ&θ0)|X - N[0,σ2c T(X TX)&1c],
c T(ˆθ&θ0)
σ c T(X TX)&1c
/00
X - N[0,1].
(5.47)
If follows now from Theorem 5.18 that conditional on X the random variable in (5.47) and S2 are
independent, hence it follows from Theorem 5.17 and the definition of the t-distribution that
(5.45) is true conditional on X, and therefore also unconditionally.
Proof of  (5.46): It follows from (5.37) that  
  hence
R(ˆθ&θ0)|X - Nm[0,σ2R(X TX)&1R T],
it follows from Theorem 5.9 that 
(ˆθ&θ0)TR T R X TX &1R T &1R(ˆθ&θ0)
σ2
/00
X - χ2
m.
(5.48)
Again it follows from Theorem 5.18 that conditional on X  the random variable in (5.48) and S2
are independent, hence it follows from Theorem 5.17 and the definition of the F-distribution that
(5.46)  is true conditional on X, and therefore also unconditionally. Q.E.D.

193
Note that the results in  Theorem 5.19 do not hinge on the assumption that the vector Xj in
model  (5.31) has a multivariate normal distributed. The only conditions that matter for the
validity of Theorem 5.19 are that in (5.32),  
 and 
U|X - Nn(0,σ2In),
P[0 < det(X TX) < 4] ' 1.
5.7.3
Hypotheses testing
Theorem 5.19 is the basis for hypotheses testing in linear regression analysis. First,
consider the problem whether a particular components of the vector Xj of explanatory variables in
model (5.31) has an effect on Yj or not. If not, the corresponding component of $ is zero. Each 
component of $ corresponds to a component 
 Thus, the null hypothesis
θi,0, i > 0, of θ0.
involved is 
H0: θi,0 ' 0.
(5.49)
Let 
 be component i of 
 and let the vector 
 be column i of the unit matrix 
 Then it
ˆθi
ˆθ,
ei
Ik.
follows from Theorem 5.19(a) that under the null hypothesis (5.49), 
ˆti '
ˆθi
S e T
i (X TX)&1ei
- tn&k.
(5.50)
The statistic  in (5.50) is called the t-statistic or t-value of the coefficient 
 If it conceivable
ˆti
θi,0.
that  
 can take negative or positive values, the appropriate alternative hypothesis is
θi,0
H1: θi,0 … 0.
(5.51)
Given the size " 0 (0,1) of the test, the critical value ( corresponds to  
 where 
P[|T| > γ] ' α,
 Thus, the null hypothesis (5.49) is accepted if  
and rejected in favor of the
T - tn&k.
|ˆti| # γ,
alternative hypothesis (5.51) if  
 In the latter case we say that 
 is significant at the
|ˆti| > γ.
θi,0
"×100% significance level. This test is called the two-sided t-test.

194
If the possibility that 
 is negative can be excluded, the appropriate alternative
θi,0
hypothesis is
H %
1 : θi,0 > 0.
(5.52)
Given the size " the critical value  (+  involved now corresponds to  
 where 
P[T > γ%] ' α,
again 
 Thus the null hypothesis (5.49) is accepted if  
 and rejected in favor of
T - tn&k.
ˆti # γ%,
the alternative hypothesis (5.52)  if  
  This is the right-sided t-test. Similarly, if the
ˆti > γ%.
possibility that 
 is positive can be excluded, the appropriate alternative hypothesis is
θi,0
H &
1 : θi,0 < 0.
(5.53)
Then the null hypothesis (5.49) is accepted if  
 and rejected in favor of the alternative
ˆti $ &γ%,
hypothesis (5.53)  if  
  This is the left-sided t-test. 
ˆti < &γ%.
If the null hypothesis (5.49) is not true, then it can be shown, using the results in the next
chapter, that for n 6 4 and arbitrary M > 0,  
 and 
 if 
P[ˆti > M] 6 1 if θi,0 > 0
P[ˆti < &M] 6 1
 <  0. Therefore, the t-tests involved are consistent.
θi,0
Finally, consider a null hypothesis of the form
H0: Rθ0 ' q,
where R is a given m×k matrix with rank m#k, and q is a given m×1 vector.
(5.54)
For example, the null hypothesis that the parameter vector $ in model (5.31) is a zero
vector corresponds to  
 This hypothesis implies that
R ' (0,Ik&1), q ' 0 0 úk&1, m ' k&1.
none on the components of Xj have any effect on Yj. In that case Yj = " + Uj, and since Uj and Xj
are independent, so are Yj and Xj.
It follows from Theorem 5.19(b) that under the null hypothesis (5.54),

195
ˆF ' (Rˆθ&q)T R X TX &1R T &1(Rˆθ&q)
m.S 2
- Fm,n&k.
(5.55)
Given the size " the critical value  ( is chosen such that   
 where  
P[F > γ] ' α,
F - Fm,n&k.
Thus the null hypothesis (5.54) is accepted if  
 and rejected in favor of the alternative
ˆF # γ,
hypothesis 
  if 
For obvious reasons, this test is called the F test. Moreover, it can
Rθ0 … q
ˆF > γ.
be shown, using the results in the next chapter, that if the null hypothesis (5.54) is false then for
any M > 0,  
 Thus the F test is a consistent test. 
limn64P[ ˆF > M] ' 1.
5.8.
Exercises
1.
Let
Y
X
- N2
1
0
,
4 1
1 1
.
(a)
Determine E(Y,X).
(b)
Determine var(U), where U = Y ! E(Y,X).
(c)
Why are U and X independent?
2.
Let X be n-variate standard normally distributed, and let A be a non-stochastic n×k matrix
with rank k < n. The projection of X on the column space of A is a vector p such that the
following two conditions hold:
(1)
p is a linear combination of the columns of A; 
(2)
the distance between X and p, 
, is minimal.
2X&p2 '
(X&p)T(X&p)
(a)
Show that p = A(ATA)-1ATX.  
(b)
Is it possible to write down the density of p? If yes, do it. If no, why not? 

196
(c) 
Show that 
 has a P2 distribution. Determine the degrees of freedom  involved. 
2p22 ' p Tp
(d) 
Show that 
 has a P2 distribution. Determine the degrees of freedom involved.
2X&p22
(e)
Show that 
 and 
 are independent.
2p2
2X&p2
3.
Prove Theorem 5.13.
4.
Show that (5.11) is true for 2 in an open set 1 if 
 is for each x continuous
d 2fn(x|θ)/(dθ)2
on 1, and  
Hint. Use the mean value theorem and the dominated
msupθ0Θ|d 2fn(x|θ)/(dθ)2|dx < 4.
convergence theorem.
5.
Show that for a random sample X1, X2,...,Xn   from a distribution with expectation µ and
variance 
  the sample variance (5.15) of is an unbiased estimator of 
even if the distribution
σ2
σ2,
involved is not normal.
6.
Prove (5.17).
7.
Show that or a random sample X1, X2,...,Xn  from a multivariate distribution with
expectation vector µ and variance matrix G the sample variance matrix (5.18) is and unbiased
estimator of G.
8.
Given a random sample of size n from the 
 distribution, prove that the Cramer-
N(µ ,σ2)
Rao lower bound for an unbiased estimator of  
 is 
 
σ2
2σ4/n.
9.
Prove Theorem 5.15.
10.
Prove the second equalities in (5.34) and (5.35).
11.
Show that the Cramer-Rao lower bound of an unbiased estimator of (5.37)  is equal to
σ2(E[X TX])&1.
12.
Show that the matrix (5.38) is idempotent.

197
13.
Why is (5.40) true?
14.
Why does (5.43) imply (5.44)?
15.
Suppose your econometric software package reports that the OLS estimate of a regression
parameter is 1.5, with corresponding t-value  2.4. However, you are only interested in whether
the true parameter value is 1 or not. How would you test these hypotheses? Compute the test
statistic involved. Moreover,  given that the sample size is n = 30 and that your model has 5 other
parameters,  conduct the test  using size 0.05. You have to look up the critical value involved  in
one of the statistics or econometrics textbook that contain tables of the t-distribution.6
Appendix 
5.A.
Proof of Theorem 5.8 
Note again that the condition AB = O only makes sense if both A and B are singular, if
otherwise either A, B or both are O. Write 
  where QA and QB are
A ' QAΛAQ T
A , B ' QBΛBQ T
B ,
orthogonal matrices of eigenvectors and 7A and 7B are diagonal matrices of corresponding
eigenvalues. Then 
   Since A and B are both singular,
Z1 ' X TQAΛAQ T
A X, Z2 ' X TQBΛBQ T
B X.
it follows that 7A and 7B are singular. Thus let
ΛA '
Λ1
O
O
O
&Λ2 O
O
O
O
,
where 71 is the k × k diagonal matrix of positive eigenvalues, and !72 the m × m diagonal matrix
of negative eigenvalues of A, with k + m < n. Then

198
Z1 ' X TQA
Λ1
0
0
0
&Λ2 0
0
0
0
Q T
A X ' X TQA
Λ
1
2
1
0
0
0
Λ
1
2
2 0
0
0
0
Ik
0
0
0 &Im
0
0
0
In&k&m
Λ
1
2
1
0
0
0
Λ
1
2
2 0
0
0
0
Q T
A X.
Similarly, denote
ΛB '
Λ(
1
O
O
O
&Λ(
2 O
O
O
O
,
where 71
* is the p × p diagonal matrix of positive eigenvalues, and !72
* is the q × q diagonal
matrix of negative eigenvalues of B, with p + q < n. Then
Z2 ' X TQB
(Λ(
1)
1
2
0
0
0
(Λ(
2)
1
2 0
0
0
0
Ip
0
0
0 &Iq
0
0
0
In&p&q
(Λ(
1)
1
2
0
0
0
(Λ(
2)
1
2 0
0
0
0
Q T
B X.
Next, let
Y1 '
Λ
1
2
1
O
O
O
Λ
1
2
2 O
O
O
O
Q T
A X ' M1X, say,
Y2 '
(Λ(
1)
1
2
O
O
O
(Λ(
2)
1
2 O
O
O
O
Q T
B X ' M2X, say.
Then

199
Z1 ' Y T
1
Ik
O
O
O &Im
O
O
O
In&k&m
Y1 ' Y T
1 D1Y1, say,
and
Z2 ' Y T
2
Ip
O
O
O &Iq
O
O
O
In&p&q
Y2 ' Y T
2 D2Y2, say,
where the diagonal matrices D1 and D2 are nonsingular but possibly different. Clearly, Z1 and Z2 
are independent if Y1 and Y2 are. Now observe that 
AB ' QA
Λ
1
2
1
0
0
0
Λ
1
2
2
0
0
0
In&k&m
Ik
0
0
0 &Im
0
0
0
In&k&m
Λ
1
2
1
0
0
0
Λ
1
2
2 0
0
0
0
Q T
A QB
(Λ(
1)
1
2
0
0
0
(Λ(
2)
1
2 0
0
0
0
×
Ip
0
0
0 &Iq
0
0
0
In&p&q
(Λ(
1)
1
0
0
0
0
(Λ(
2)
1
2
0
0
0
In&p&q
Q T
B .
The first three matrices are nonsingular, and so are the last three. Therefore, AB = O if and only
if
M1M T
2 '
Λ
1
2
1
0
0
0
Λ
1
2
2 0
0
0
0
Q T
A QB
(Λ(
1)
1
2
0
0
0
(Λ(
2)
1
2 0
0
0
0
' O.

200
1.
In order to distinguish the variance of a random variable from the variance matrix of a
random vector, the latter will be denoted by Var, with capital V.
2.
The capital C in Cov indicates that this is a covariance matrix rather than a covariance of
two random variables.
3.
These calculators are also included in my free econometrics software package EasyReg
International, which you can download from http://econ.la.psu.edu/~hbierens/EASYREG.HTM.
4.
Recall that "argmin" stands for the argument for which the function involved takes a
minimum.
5.
The OLS estimator is called "ordinary" to distinguish it from the nonlinear least squares
estimator. See Chapter 6 for the latter.
6.
 Or use the author’s  free econometrics software package EasyReg International.  The t-
distribution calculator can be found under "Tools".
It follows now from Theorem 5.7 that the latter implies that Y1 and Y2 are independent, hence the
condition AB = O implies that Y1 and Y2 are independent. Q.E.D.
Endnotes

201
Chapter 6
Modes of Convergence
6.1.
Introduction
Toss a fair coin n times, and let Yj = 1 if the outcome of the j-th tossing is head, and Yj =
!1 if the outcome involved is tail. Denote 
 For the case n = 10 the left-hand
Xn ' (1/n)'n
j'1Yj.
side panel of Figure 6.1 displays the distribution function Fn(x)1  of Xn on the interval [!1.5, 1.5],
and the right-hand side panel displays a typical plot of Xk  for k =1,2,...,10, based on simulated
Yj‘s.2 
Figure 6.1. n = 10. Left: Distribution function of Xn .  Right: Plot of Xk for k=1,2,...,n.
Now let us see what happens if we increase n: First, consider the case n = 100, in Figure 6.2. The
distribution function Fn(x) becomes steeper for x close to zero, and Xn seems to tend towards
zero.  
Figure 6.2. n = 100. Left: Distribution function of Xn .  Right: Plot of Xk for k=1,2,...,n.

202
These phenomena are even more apparent for the case n = 1000, in Figure 6.3.
Figure 6.3.  n = 1000. Left: Distribution function of Xn .  Right: Plot of Xk for k=1,2,...,n.
What you see in Figures 6.1-6.3 is the law of large numbers:  
 in
Xn ' (1/n)'n
j'1Yj 6 E[Y1] ' 0
some sense, to be discussed below, and the related phenomenon that Fn(x) converges pointwise in
x … 0  to the distribution function  F(x) = I(x $ 0) of a "random" variable X satisfying  P[X ' 0]
= 1.
Next, let us have a closer look at the distribution function of 
 =  
nXn: Gn(x)
Fn(x/ n),
with corresponding probabilities  
 k = 0,1,...,n, and see what happens if n
P[ nXn ' (2k&n)/ n],
64. These probabilities can be displayed in the form of a histogram:
Hn(x) ' P 2(k&1)/ n& n <
nXn # 2k/ n& n
2/ n
if x 0 2(k&1)/ n& n, 2k/ n& n , k '0,1,....,n,
Hn(x) ' 0 elsewhere.
Figures 6.4-6.6 compare Gn(x) with the distribution function of the standard normal
distribution, and Hn(x) with the standard normal density, for n = 10, 100 and 1000.

203
Figure 6.4.  n = 10: Left: Gn(x), right: Hn(x), compared with the standard normal distribution.
Figure 6.5.  n = 100: Left: Gn(x), right: Hn(x), compared with the standard normal distribution.
Figure 6.6.  n = 1000: Left: Gn(x), right: Hn(x), compared with the standard normal distribution.
What you see in the left-hand side panels in Figures 6.4-6.6  is the central limit theorem: 
lim
n64
Gn(x) ' m
x
&4
exp[&u 2/2]
2π
du,
pointwise in x, and what you see in the right-hand side panels is the corresponding fact that
lim
δ90
lim
n64
Gn(x%δ) & Gn(x)
δ
' exp[&x 2/2]
2π
.

204
The law of large numbers and the central limit theorem play a key role in statistics and
econometrics. In this chapter I will review and explain these laws.
6.2.
Convergence in probability and the weak law of large numbers
Let Xn  be a sequence of random variables (or vectors) and let X a random or constant
variable (or conformable vector). 
Definition 6.1: We say that Xn converges in probability to X, also denoted as plimn64Xn = X or
,  if for an arbitrary g > 0 we have 
 = 0, or equivalently,
Xn 6p X
limn64P(*Xn & X* > g)
 = 1.
limn64P(*Xn & X* # g)
In this definition, X may be a random variable or a constant. The latter case, where P(X = c) = 1
for some constant c, is the most common case in econometric applications. Also, this definition
carries over to random vectors, provided that the absolute value function 
 is replaced by the
*x*
Euclidean norm 
.
2x2 '
x Tx
The right-hand side panels of Figures 6.1-6.3 demonstrate the law of large numbers. One
of the versions of this law is the Weak Law of Large Numbers (WLLN), which also applies to
uncorrelated random variables.
Theorem 6.1: (WLLN for  uncorrelated random variables).  Let X1 ,...,Xn be a sequence of
uncorrelated random variables with E(Xj) = 
 and var(Xj) = 
, and let 
 = 
.
µ
σ2 < 4
X
(1/n)'n
j'1Xj

205
Then  
 = 
.
plimn64X
µ
Proof: Since 
, it follows from Chebishev inequality that
E(X ) ' µ and Var(X) ' σ2/n
. Q.E.D.
P(*X & µ* > g) # σ2/(ng2) 6 0 if n 6 4
The condition of a finite variance can be traded in for the i.i.d. condition:
Theorem 6.2: (The WLLN  for i.i.d. random variables). Let X1,...,Xn be a sequence of
independent identically distributed random variables with E [|Xj|] < 4 and E(Xj) = 
, and let 
µ
. Then  
 = 
.
X ' (1/n)'n
j'1Xj
plimn64X
µ
Proof: Let  Yj  = Xj .I(|Xj|  # j) and Zj =  Xj .I(|Xj| > j), so that Xj = Yj + Zj. Then
E*(1/n)'n
j'1(Zj & E(Zj))* # 2(1/n)'n
j'1E[*Zj*] ' 2(1/n)'n
j'1E[|X1|I(|X1| > j)] 6 0,
(6.1)
and
E[*(1/n)'n
j'1(Yj & E(Yj))*2] # (1/n 2)'n
j'1E[Y 2
j ] ' (1/n 2)'n
j'1E[X 2
1 I(|X1| # j)]
' (1/n 2)'n
j'1'j
k'1E[X 2
1 I(k & 1 < |X1| # k)]
# (1/n 2)'n
j'1'j
k'1k.E[|X1|.I(k & 1 < |X1| # k)]
(1/n 2)'n
j'1'j&1
k'1'j
i'kE[|X1|.I(i & 1 < |X1| # i)] # (1/n 2)'n
j'1'j&1
k'1E[|X1|.I(|X1| > k & 1)
# (1/n)'n
k'1E[|X1|.I(|X1| > k & 1)] 6 0
(6.2)
as n 64, where the last equality in (6.2) follows from  the easy equality 'j
k'1k.αk ' 'j&1
k'1'j
i'kαi,

206
and the convergence results in (6.1)  and (6.2) follow from the fact that 
 
E[|X1|I(|X1| > j)] 6 0
for j 6 4,  because 
 Using Chebishev’s inequality it follows now from (6.1)  and
E[*X1*] < 4.
(6.2) that for arbitrary g > 0,
P[*(1/n)'n
j'1(Xj & E(Xj))* > g] # P[*(1/n)'n
j'1(Yj & E(Yj))*
% *(1/n)'n
j'1(Zj & E(Zj))*> g]
# P[*(1/n)'n
j'1(Yj & E(Yj))* > g/2] % P[*(1/n)'n
j'1(Zj & E(Zj))*> g/2]
# 4E[*(1/n)'n
j'1(Yj & E(Yj))*2]/g2 % 2E[*(1/n)'n
j'1(Zj & E(Zj))*]/g 6 0
(6.3)
as n 64.  Note that  the second inequality in (6.3) follow from the fact that for non-negative
random variables X and Y,  
  # 
  The theorem under
P[X%Y > g]
P[X > g/2] % P[Y > g/2].
review follows now from (6.3), Definition 6.1 and the fact that g is arbitrary. Q.E.D.
Note that Theorems 6.1-6.2 carry over to finite-dimensional random vectors Xj, by
replacing the absolute values 
 by Euclidean norms: 
, and the variance by the
*.*
2x2 '
x Tx
variance matrix. The reformulation of Theorems 6.1-6.2 for random vectors is left as an easy
exercise.
Convergence in probability carries over after taking continuous transformations. This
results is often referred to as Slutky's theorem:
Theorem 6.3: (Slutsky's theorem). Let Xn a sequence of random vectors in 
  satisfying  Xn 6p c,
úk
where c is non-random .  Let  Q(x) be an  
 -valued function on 
  which is continuous in c.
úm
úk
Then Q(Xn) 6p  Q(c).

207
Proof: Consider the case m = k = 1.  It follows from the continuity of  Q that for an
arbitrary g > 0 there exists a  * > 0 such that 
 implies 
, hence
*x & c* # δ
*Ψ(x) & Ψ(c)* # g
P(*Xn & c* # δ) # P(*Ψ(Xn) & Ψ(c)* # g).
Since 
 = 1, the theorem follows for the case under review. The more
limn64P(*Xn & c* # δ)
general case with m > 1 and/or k > 1, can be proved along the same lines.  Q.E.D.
The condition that  c is constant is not essential. Theorem 6.3 carries over to the case
where c is a random variable or vector, as we will see in Theorem 6.7 below.
Convergence in probability does not automatically imply convergence of expectations. A
counter-example is Xn = X +1/n, where X  has a Cauchy distribution (see Chapter 4). Then E[Xn]
and E(X) are not defined, but Xn 6p X.  However,
Theorem 6.4: (Bounded convergence theorem) If   Xn   is bounded: 
 = 1 for some
P(*Xn* # M)
M < 4 and all n, then Xn 6p  X  implies limn64E(Xn) = E(X).
Proof: First, X  has to be bounded too, with the same bound M, because otherwise Xn 6p 
X  is not possible. Without loss of generality we may now assume that P(X = 0) = 1 and that Xn is
a non-negative random variable, by replacing Xn with |Xn ! X|, because E[|Xn ! X|] 6 0 implies
limn64E(Xn) = E(X). Next, let Fn(x) be the distribution function of Xn, and let g > 0 be arbitrary.
Then
0 # E(Xn) ' m
M
0 xdFn(x) ' m
g
0 xdFn(x) % m
M
g xdFn(x) # g % M.P(Xn $ g).
(6.4)

208
Since the latter probability converges to zero (by the definition of convergence in probability and
the assumption that Xn is nonnegative, with zero probability limit), we have 0 # limsupn64E(Xn)
# g for all g > 0, hence 
 = 0. Q.E.D. 
limn64E(Xn)
The condition that Xn  in Theorem 6.4 is bounded can be relaxed, using the concept of
uniform integrability:
Definition 6.2: A sequence Xn  of random variables is said to be uniformly integrable if 
limM64supn$1E[|Xn|.I(|Xn| > M)] ' 0.
Note that this Definition 6.carries over to random vectors by replacing the absolute value
|.| with the Euclidean norm ||.||. Moreover, it is easy to verify that if 
 with probability 1
*Xn* # Y
for all  n $1, where E(Y) < 4,  then Xn  is uniformly integrable.
Theorem 6.5: (Dominated convergence theorem) Let  Xn   be uniformly integrable. Then Xn 6p  X 
implies limn64E(Xn) = E(X).
Proof: Again, without loss of generality we may assume that P(X = 0) = 1 and that Xn  is a
non-negative random variable. Let 0 <  g < M  be arbitrary. Then similarly to (6.4),
0 # E(Xn) ' m
4
0 xdFn(x) ' m
g
0 xdFn(x) % m
M
g xdFn(x) % m
4
MxdFn(x)
# g % M.P(Xn $ g) % supn$1m
4
MxdFn(x).
(6.5)
For fixed M the second term at the right-hand side of (6.5) converges to zero. Moreover, by

209
uniform integrability we can choose M so large that the third term is smaller than  g. Hence, 0 #
 # 2g for all g > 0, and thus 
 = 0. Q.E.D.  
limsupn64E(Xn)
limn64E(Xn)
Also Theorems 6.4 and 6.5 carry over to random vectors, by replacing the absolute value
function 
 by the Euclidean norm 
.
*x*
2x2 '
x Tx
6.3.
Almost sure convergence, and the strong law of large numbers
In most (but not all!) cases where convergence in probability and the weak law of large
numbers apply, we actually have a much stronger result:
Definition 6.3: We say that Xn  converges almost surely (or: with probability 1) to X, also
denoted by Xn 6 X a.s. (or: w.p.1), if
for all g > 0, limn64P(supm$n*Xm & X* # g) ' 1,
(6.6)
or equivalently,
P(limn64Xn ' X) ' 1.
(6.7)
The equivalence of the conditions (6.6) and (6.7)  will be proved  in Appendix 6.B (Theorem
6.B.1). 
It follows straightforwardly from (6.6) that almost sure convergence implies convergence
in probability. The converse, however, is not true. It is possible that a sequence Xn converges in
probability but not almost surely. For example, let Xn  = Un /n, where the Un’s are i.i.d. non-
negative random variables with distribution function 
 for u > 0, 
 for
G(u) ' exp(&1/u)
G(u) ' 0
u # 0.  Then for arbitrary g > 0,

210
P(|Xn| # g) ' P(Un # ng) ' G(ng) ' exp(&1/(ng)) 6 1 as n 6 4,
hence Xn 6p  0. On the other hand, 
P(|Xm| # g for all m $ n) ' P(Um # mg for all m $ n) ' Π4
m'nG(mg)
' exp&g&1'4
m'nm &1 ' 0,
where the second equality follows from the independence of the Un’s, and the last equality
follows from the fact that  
 Consequently, Xn does not converge to 0 almost
'4
m'1m &1 ' 4.
surely.
Theorems 6.2-6.5 carry over to the almost sure convergence case, without additional
conditions:
Theorem 6.6: (Kolmogorov's strong law of large numbers). Under the conditions of Theorem
6.2, ¯X 6 µ a.s.
Proof: See Appendix 6.B.
The result of Theorem 6.6 is actually what you see happening in the right-hand side
panels of Figures 6.1-6.3. 
Theorem 6.7: (Slutsky's theorem). Let Xn a sequence of random vectors in úk  converging a.s. to
a (random or constant) vector X. Let Q(x) be an   úm -valued function on  úk  which is
continuous on an open subset 3 B of  úk for which 
 = 1). Then Ψ(Xn) 6 Ψ(X) a.s.
P(X 0 B)

211
Proof: See Appendix 6.B.
Since a.s. convergence implies convergence in probability, it is trivial that:
Theorem 6.8: If Xn 6 X a.s., then the result of Theorem 6.4 carries over.
Theorem 6.9: If Xn 6 X a.s., then the result of Theorem 6.5 carries over.
6.4.
The uniform  law of large numbers and its applications
6.4.1
The uniform weak law of large numbers
In econometrics we often have to deal with means of random functions. A random
function is a function that is a random variable for each fixed value of its argument. More
precisely:
Definition 6.4:  Let {S,ö,P} be the probability space. A random function f(2) on a subset 1 of a
Euclidean space is a mapping 
 such that for each Borel set B in ú and each 2
f(ω,θ): Ω×Θ 6 ú
0 1, {ω 0 Ω: f(ω,θ) 0 B} 0 ö.
Usually random functions take the form of a function g(X,2) of a random vector X and a non-
random vector 2. For such functions we can extend the weak law of large numbers for i.i.d.
random variables to a Uniform Weak Law of Large Numbers (UWLLN):

212
Theorem 6.10: (UWLLN). Let Xj, j = 1,..,n, be a random sample from a k-variate distribution,
and let 
 be non-random vectors in a closed and bounded (hence compact4) subset
θ 0 Θ
.  Moreover, let g(x,2) be a Borel measurable function on 
 such that for each x, 
Θ d úm
úk × Θ
g(x,2) is a continuous function on 1.  Finally, assume that 
 < 4.  Then 
E[supθ0Θ*g(Xj,θ)*]
plimn64supθ0Θ*(1/n)'n
j'1g(Xj,θ) & E[g(X1,θ)]* ' 0.
Proof: See Appendix 6.A.
6.4.2
Applications of the uniform weak law of large numbers
6.4.2.1 Consistency of M-estimators
In Chapter 5 I have introduced the concept of a parameter estimator, and listed a few
desirable properties of estimators, i.e., unbiasedness and efficiency.  Another obviously desirable
property is that the estimator gets closer to the parameter to be estimated if we use more data
information. This is the consistency property:
Definition 6.5: An estimator  of a parameter θ, based on a sample of size n, is called consistent
ˆθ
if  plimn64ˆθ ' θ.
Theorem 6.6 is an important tool in proving consistency of parameter estimators. A large
class of estimators are obtained by maximizing or minimizing  an objective function of the form 
  where g, Xj and θ are the same as in Theorem 6.10.  These estimators are
(1/n)'n
j'1g(Xj,θ),
called M-estimators (where the M indicates that the estimator is obtained by Maximizing or

213
Minimizing a Mean of random functions). Suppose that the parameter of interest is  θ0 =
argmaxθ0ΘE[g(X1,θ)],  where Θ is a given closed and bounded set.  Note that "argmax" is a short-
hand notation for the argument for which the function involved is maximal. Then it seems a
natural choice to use 
 as an estimator of 
 Indeed, under some
ˆθ ' argmaxθ0Θ(1/n)'n
j'1g(Xj,θ)
θ0.
mild conditions the estimator involved is consistent:
Theorem 6.11: (Consistency of M- estimators) Let  = 
 
 = 
ˆθ
argmaxθ0Θ ˆQ(θ), θ0
argmaxθ0ΘQ(θ),
where 
 = 
 and 
 with g, Xj and θ the same
ˆQ(θ)
(1/n)'n
j'1g(Xj,θ),
Q(θ) ' E[ ˆQ(θ)] ' E[g(X1,θ)],
as in Theorem 6.10.  If  20  is unique, in the sense that for arbitrary  g > 0 there exists a * > 0
such that 
,  then  
.
Q(θ0) & sup2θ&θ02>g Q(θ) > δ
plimn64ˆθ ' θ0
Proof: First, note that 
 and 
 because g(x,2) is continuous in 2. See
ˆθ 0 Θ
θ0 0 Θ,
Appendix II.  By the definition of 20, 
0 # ¯Q(θ0) & ¯Q(ˆθ) ' ¯Q(θ0) & ˆQ(θ0) % ˆQ(θ0) & ¯Q(ˆθ)
# ¯Q(θ0) & ˆQ(θ0) % ˆQ(ˆθ) & ¯Q(ˆθ) # 2sup
θ0Θ
* ˆQ(θ) & ¯Q(θ)*,
(6.8)
and it follows from Theorem 6.3 that the right-hand side of (6.8) converges in probability to zero.
Thus:
plimn64 Q(ˆθ) ' Q(θ0).
(6.9)
Moreover, the uniqueness condition implies that for arbitrary g > 0 there exists a * > 0 such that 
 if 
, hence
Q(θ0) & Q(ˆθ) $ δ
2ˆθ & θ02 > g
P 2ˆθ & θ02 > g # P ¯Q(θ0) & ¯Q(ˆθ) $ δ .
(6.10)

214
Combining (6.9) and (6.10), the theorem under review follows from Definition 6.1. Q.E.D.
It is easy to verify that Theorem 6.11 carries over to the "argmin" case, simply by
replacing g by -g.
As an example, let 
 be a random sample from the non-central Cauchy
X1,...,Xn
distribution, with density  
 =  
 and suppose that we know that  
 is
h(x|θ0)
1/[π(1%(x&θ0)2],
θ0
contained in a given closed and bounded interval Θ.  Let  
 where 
 = 
g(x,θ) ' f(x&θ),
f(x)
 is the density of the standard normal distribution. Then
exp(&x 2/2)/ 2π
E[g(X1,θ)] ' m
4
&4
exp(&(x%θ0&θ)2)/ 2π
π(1%x 2)
dx 'm
4
&4
f(x&θ%θ0)h(x|0)dx ' γ(θ&θ0),
(6.11)
say, where 
 is a density itself, namely the density of 
 with U and Z independent
γ(y)
Y ' U % Z,
random drawings form the standard normal and standard Cauchy distribution, respectively.  This
is called the convolution of the two densities involved.  The  characteristic function of Y is
 so that by the inversion formula for characteristic functions
exp(&|t|&t 2/2),
γ(y) '
1
2π m
4
&4
cos(t.y)exp(&|t|&t 2/2)dt.
(6.12)
This function is maximal in y = 0, and this maximum is unique, because for fixed y … 0 the set
 is countable and therefore has Lebesgue measure zero. In particular, it
{t 0 ú: cos(t.y) ' 1}
follows from (6.12) that for arbitrary  g > 0, 
sup|y|$gγ(y) #
1
2π m
4
&4
sup|y|$g|cos(t.y)|exp(&|t|&t 2/2)dt < γ(0).
(6.13)
Combining  (6.11) and (6.13) yields  
 Thus, all the
sup|θ&θ0|$gE[g(X1,θ)] < E[g(X1,θ0)].

215
conditions of Theorem 6.11 are satisfied, hence 
 
plimn64ˆθ ' θ0.
Another example is the nonlinear least squares estimator. Consider a random sample
 and assume that: 
Zj ' (Yj,X T
j )T, j ' 1,2,....,n, with Yj 0 ú, Xj 0 úk,
Assumption 6. 1: For a given function  
 with 1 a given compact subset of  
f(x,θ) on úk×Θ,
úm,
there exists a 
 such that 
 Moreover, for each 
 
 
θ0 0 Θ
P[E[Yj|Xj] ' f(Xj,θ0)] ' 1.
x 0 úk, f(x,θ)
is a continuous function on  1, and for each 
 is a Borel measurable function on 
θ 0 Θ, f(x,θ)
 Furthermore, let  
 and 
úk.
E[Y 2
1 ] < 4, E[supθ0Θf(X1,θ)2] < 4,
 inf||θ&θ0||$δE[(f(X1,θ) & f(X1,θ0))2] > 0 for δ > 0.
Denoting  
  we can write
Uj ' Yj & E[Yj|Xj]
Yj ' f(Xj,θ0) % Uj, where P(E[Uj|Xj] ' 0) ' 1.
(6.14)
This is the general form of a nonlinear regression model. I will show now that under Assumption
6.1 the nonlinear least squares estimator
ˆθ ' argminθ0Θ(1/n)'n
j'1(Yj & f(Xj,θ))2
(6.15)
is a consistent estimator of θ0.
Let  
 Then it follows from Assumption 6.1 and Theorem 6.10
g(Zj,θ) ' (Yj & f(Xj,θ))2.
that 
plimn64supθ0Θ|(1/n)'n
j'1[g(Zj,θ)&E[g(Z1,θ)]| ' 0.
Moreover,  

216
E[g(Z1,θ)] ' E[(Uj % f(Xj,θ0) & f(Xj,θ))2] ' E[U 2
j |] % 2E[E(Uj|Xj)(f(Xj,θ0) & f(Xj,θ))]
% E[(f(Xj,θ0) & f(Xj,θ))2] ' E[U 2
j |] % E[(f(Xj,θ0) & f(Xj,θ))2],
hence it follows from Assumption 6.1 that 
 Therefore the
inf||θ&θ0||$δE[|g(Z1,θ)|] > 0 for δ > 0.
condition of Theorem 6.11 for the argmin case are satisfied, and consequently, the nonlinear least
squares estimator (6.15) is consistent.
6.4.2.2 Generalized Slutsky’s theorem
Another easy but useful corollary of Theorem 6.6 is the following generalization of
Theorem 6.3:
Theorem 6.12: (Generalized Slutsky’s theorem) Let Xn a sequence of random vectors in 
 
úk
converging in probability to a nonrandom vector c.  Let  Mn(x) be a sequence of random
functions on 
 satisfying 
 = 0, where B is a closed and bounded
úk
plimn64supx0B*Φn(x) & Φ(x)*
subset of  
 containing c, and  M is a continuous nonrandom function on B. Then 
úk
Φn(Xn) 6p
Φ(c).
Proof: Exercise.
This theorem can be further generalized to the case where c = X  is a random vector, simply by
adding the condition that 
 but the current result suffices for the applications of
P[X 0 B] ' 1,
Theorem 6.12.
This theorem plays a key-role in deriving the asymptotic distribution of an M-estimator,

217
together with the central limit theorem discussed below.
6.4.3
The uniform strong law of large numbers and its applications
The results of Theorems 6.10-6.12 also hold almost surely. See Appendix 6.B for the
proofs.
Theorem 6.13: Under the conditions of Theorem 6.10,  supθ0Θ*(1/n)'n
j'1g(Xj,θ) & E[g(X1,θ)]*
6 0 a.s.
Theorem 6.14: Under the conditions of Theorems 6.11 and 6.13, ˆθ 6 θ0 a.s.
Theorem 6.15: Under the conditions of Theorem 6.12 and the additional condition that Xn  6 c
a.s.,  
 a.s.
Φn(Xn) 6 Φ(c)
6.5.
Convergence in distribution
Let Xn be a sequence of random variables (or vectors) with distribution functions Fn(x),
and let X  be a random variable (or conformable random vector) with distribution function F(x). 
Definition 6.6: We say that Xn converges to X in distribution (denoted by  Xn 6d  X ) if 
limn64Fn(x) = F(x), pointwise in x , possibly except in the discontinuity points of F(x).

218
Alternative notation: If X has a particular distribution, for example N(0,1), then  Xn 6d  X 
is also denoted by Xn 6d  N(0,1).
The reason for excluding discontinuity points of F(x) in the definition of convergence in
distribution is that in these discontinuity points, limn64Fn(x) may not be right-continuous. For
example, let Xn = X + 1/n. Then Fn(x) = F(x-1/n). Now if F(x) is discontinuous in x0, then
limn64F(x0-1/n) < F(x0), hence limn64Fn(x0) < F(x0). Thus, without the exclusion of discontinuity
points, X + 1/n would not converge in distribution to the distribution of X, which would be
counter-intuitive.
If each of the components of a sequence of random vectors converge in distribution, then
the random vectors themselves may not converge in distribution. As a counter-example, let
Xn '
X1n
X2n
- N2
0
0
,
1
(&1)n/2
(&1)n/2
1
.
(6.16)
Then X1n 6d  N(0,1) and X2n 6d  N(0,1), but Xn does not converge in distribution. 
Moreover, in general Xn  6d  X  does not imply that  Xn  6p  X. For example, if we replace
X  by an independent random drawing Z from the distribution of X, then Xn  6d  X   and Xn  6d Z 
are equivalent statements, because these statements only say that the distribution function of Xn
converges to the distribution function of X (or Z), pointwise in the continuity points of the latter
distribution function. If Xn  6d  X   would imply  Xn  6p  X, then  Xn  6d  Z in distr. would imply
that X = Z, which is not possible, because X and Z are independent. The only exception is the
case where the distribution of X is degenerated: P(X = c) = 1 for some constant c:
Theorem 6.16: If Xn converges in distribution to X, and P(X = c) = 1, where c is a constant, then

219
Xn  converges in probability to c.
Proof:  Exercise.
Note that this result is demonstrated in the left-hand side panels of Figures 6.1-6.3. 
On the other hand,  
Theorem 6.17: Xn  6p X  implies  Xn  6d  X. 
Proof: Theorem 6.17  follows straightforwardly from Theorem 6.3, Theorem 6.4, and
Theorem 6.18 below. Q.E.D.
There is a one-to-one correspondence between convergence in distribution and
convergence of expectations of bounded continuous functions of random variables: 
Theorem 6.18: Let 
 and X be random vectors in 
. Then  Xn  6d X  if and only if for all
Xn
úk
bounded continuous functions 
 on 
, 
.
φ
úk limn64E[φ(Xn)] ' E[φ(X)]
Proof: I will only prove this theorem for the case where Xn and X are random variables.
Throughout the proof the distribution function of Xn is denoted by Fn(x), and the distribution
function of X by F(x).
Proof of the "only if" case: Let Xn  6d  X. Without loss of generality we may assume that
 for all x. For any g > 0 we can choose continuity points a and b of F(x) such that
φ(x) 0 [0,1]
F(b) - F(a) > 1!g. Moreover, we can choose continuity points a = c1 < c2 <...< cm = b of F(x) such

220
that for j = 1,..,m-1,
sup
x0(cj,cj%1]
φ(x) &
inf
x0(cj,cj%1]
φ(x) # g.
(6.17)
Now define
ψ(x) '
inf
x0(cj,cj%1]
φ(x) for x 0 (cj,cj%1], j ' 1,..,m&1,
ψ(x) ' 0 elsewhere.
(6.18)
Then 
, 
, hence
0 # φ(x) & ψ(x) # g for x 0 (a,b] 0 # φ(x) & ψ(x) # 1 for x ó (a,b]
limsup
n64
*E[ψ(Xn)] & E[φ(Xn)]*
# limsup
n64
m
x0(a,b]
*ψ(x)&φ(x)*dFn(x) %
m
xó(a,b]
*ψ(x)&φ(x)*dFn(x)
# g % 1 & lim
n64
Fn(b) & Fn(a) ' g % 1 & F(b) & F(a) # 2g.
(6.19)
Moreover, we have
*E[ψ(X)] & E[φ(X)]* # 2g,
(6.20)
and
limn64E[ψ(Xn)] ' E[ψ(X)].
(6.21)
Combining (6.19), (6.20) and (6.21), the "only if" part easily follows.
Proof of the "if" case: Let a < b be arbitrary continuity points of F(x), and let
φ(x) '
'
0
if
x $ b,
'
1
if
x < a,
'
b&x
b&a if a # x < b.
(6.22)
Then clearly (6.22) is a bounded continuous function. Next, observe that
E[φ(Xn)] ' mφ(x)dFn(x) ' Fn(a) % m
b
a
b&x
b&adFn(x) $ Fn(a)
(6.23)

221
hence
E[φ(X)] ' lim
n64
E[φ(Xn)] $ limsup
n64
Fn(a).
(6.24)
Moreover,
E[φ(X)] ' mφ(x)dF(x) ' F(a) % m
b
a
b&x
b&adF(x) # F(b).
(6.25)
Combining (6.24) and (6.25) yields 
 hence, since b (> a) was arbitrary,
F(b) $ limsupn64Fn(a),
letting 
 it follows that.
b 9 a
F(a) $ limsup
n64
Fn(a).
(6.26)
Similarly, for c < a we have 
, hence letting 
 it follows that
F(c) # liminfn64Fn(a)
c 8 a
F(a) # liminf
n64
Fn(a).
(6.27)
Combining (6.26) and (6.27), the "if" part now follows, i.e., 
 Q.E.D.
F(a) ' limn64Fn(a).
Note that the "only if" part of Theorem 6.18 implies another version of the bounded
convergence theorem:
Theorem 6.19: (Bounded convergence theorem) If  Xn  is bounded:
 = 1  for some
P(*Xn* # M)
M < 4 and all n, then Xn  6d X  implies  limn64E(Xn) = E(X).
Proof: Easy exercise.
Using Theorem 6.18, it is not hard to verify that the following result holds.
Theorem 6.20: (Continuous Mapping Theorem) Let Xn and X be random vectors in 
 such that
úk

222
Xn  6d X, and let  Φ(x) be a continuous mapping from 
 into 
. Then  Φ(Xn)  6d  Φ(X).
úk
úm
Proof: Exercise.
Examples of applications of Theorem 6.20 are:
(1)
Let Xn 6d X, where X is N(0,1) distributed. Then X 2
n 6d χ2
1.
(2)
Let Xn 6d X, where X is Nk(0,I) distributed. Then X T
n Xn 6d χ2
k .
If  Xn 6d X, Yn 6d Y, and Φ(x,y) is a continuous function, then in general it does not follow
that Φ(Xn ,Yn) 6d  Φ(X,Y), except if either X or Y has a degenerated distribution: 
Theorem 6.21: Let X and Xn be random vectors in 
 such that Xn 6d  X, and let Yn be a random
úk
vector in 
 such that plimn64Yn = c, where c 0 
 is a nonrandom vector. Moreover, let  Φ(x,y)
úm
úm
be a continuous function on the set  
 for some δ > 0. 5 Then
úk × {y 0 úm: 2y & c2 < δ}
Φ(Xn,Yn) 6d  Φ(X,c). 
Proof: Again, we prove the theorem for the case k = m = 1 only. Let Fn(x) and F(x) be the
distribution functions of Xn and X, respectively, and let 
 be a bounded continuous function
Φ(x,y)
on  
 for some δ > 0. Without loss of generality we may assume that 
# 1.
ú × (c&δ,c%δ)
*Φ(x,y)*
Next, let g > 0 be arbitrary, and choose continuity points a < b of F(x) such that F(b) - F(a) >
1!g. Then for any γ > 0,

223
*E[Φ(Xn,Yn)] & E[Φ(Xn,c)* # E[*Φ(Xn,Yn) & Φ(Xn,c)*I(*Yn&c*#γ)]
% E[*Φ(Xn,Yn) & Φ(Xn,c)*I(*Yn&c*>γ)]
# E[*Φ(Xn,Yn) & Φ(Xn,c)*I(*Yn&c*#γ)I(Xn0[a,b])]
% 2P(Xnó[a,b]) % 2P(*Yn&c*>γ)
#
sup
x0[a,b], *y&c*#γ
*Φ(x,y)&Φ(x,c)* % 2(1&Fn(b)%Fn(a)) % 2P(*Yn&c*>γ).
(6.28)
Since a continuous function on a closed and bounded subset of an Euclidean space is uniformly
continuous on that subset (see Appendix II), we can choose γ so small that
sup
x0[a,b], *y&c*#γ
*Φ(x,y)&Φ(x,c)* < g.
(6.29)
Moreover, 1 - Fn(b) + Fn(a) 6 1 - F(b) + F(a) < g, and 
. Therefore, it follows
P(*Yn&c*>γ) 6 0
from (6.28) that:
limsup
n64
*E[Φ(Xn,Yn)] & E[Φ(Xn,c)* # 3g.
(6.30)
The rest of the proof is left as an exercise. Q.E.D.
 
Corollary 6.1: Let Zn be t-distributed with n degrees of freedom. Then Zn  6d  N(0,1).
Proof: By the definition of the t-distribution with n degrees of freedom we can write

224
Zn '
Uo
1
nj
n
j'1
U 2
j
,
(6.31)
where U0 , U1 ,..,Un are i.i.d. N(0,1). Let Xn = U0 and X = U0, so that trivially Xn 6d  X. Let
. Then by the weak law of large numbers (Theorem 6.2) we have: plimn64Yn =
Yn ' (1/n)'n
j'1U 2
j
E(U1
2) = 1. Let Φ(x,y) = x/%y. Note that  Φ(x,y) is continuous on R × (1-g,1+g) for 0 < g < 1. Thus
by Theorem 6.21, 
 in distribution. Q.E.D.
Zn ' Φ(Xn,Yn) 6 Φ(X,1) ' U0 - N(0,1)
Corollary 6.2: Let U1...Un be a random sample from Nk(F,Σ), where Σ is non-singular. Denote
, 
, and let Zn = 
. Then
¯U ' (1/n)'n
j'1Uj
ˆΣ ' (1/(n&1))'n
j'1(Uj& ¯U)(Uj& ¯U)T
n( ¯U&µ)T ˆΣ
&1( ¯U&µ)
.
Zn 6d χ2
k
Proof: For a k×k matrix A = (a1,..,ak), let vec(A) be the k2×1 vector of stacked columns aj,
j = 1,...,k, of A: 
, say, with inverse vec-1(b) = A. Let c = vec(Σ), Yn =
vec(A) ' (a T
1 ,...,a T
k )T ' b
vec(
), Xn = 
, X 
, and 
. Since Σ is nonsingular,
ˆΣ
n( ¯U&µ)
- Nk(0,Σ)
Ψ(x,y) ' x T(vec&1(y))&1x
there exists a neighborhood C(δ) = 
 of c such that for all y in C(δ), vec-1(y)
{y0úk×k: 2y&c2 < δ}
is nonsingular (Exercise: Why?), and consequently, 
 is continuous on 
 (Exercise:
Ψ(x,y)
úk×C(δ)
Why?). The corollary follows now from Theorem 6.21 (Exercise: Why?). Q.E.D.

225
6.6.
Convergence of characteristic functions
Recall that the characteristic function of a random vector X in 
 is defined as
úk
 
φ(t) ' E[exp(it TX)] ' E[cos(t TX)] % i.E[sin(t TX)]
for 
 where 
.  The last equality is due to the fact that exp(i.x) = cos(x) + i.sin(x). 
t 0 úk,
i '
&1
Also recall that distributions are the same if and only if their characteristic functions are
the same. This property can be extended to sequences of random variables and vectors:
Theorem 6.22: Let Xn  and X be random vectors in 
 with characteristic functions 
 and
úk
φn(t)
 respectively.  Then Xn 6d  X  if and only if 
 for all  
. 
φ(t),
φ(t) ' limn64φn(t)
t 0 úk
Proof: See Appendix 6.C for the case k = 1.
Note that the "only if" part of Theorem 6.22  follows from Theorem 6.18:  Xn 6d  X 
implies that for any t 0 úk,
limn64E[cos(t TXn)] ' E[cos(t TX)],
limn64E[sin(t TXn)] ' E[sin(t TX)],
hence
limn64φn(t) ' limn64E[cos(t TXn)] % i.limn64E[sin(t TXn)]
' E[cos(t TX)] % i.E[sin(t TX)] ' φ(t).
Theorem 6.22 plays a key-role in the derivation of the central limit theorem, in the next
section.

226
6.7.
The central limit theorem
The prime example of the concept of convergence in distribution is the central limit
theorem, which we have seen in action in Figures 6.4-6.6:
Theorem 6.23: Let X1,....,Xn be i.i.d. random variables satisfying E(Xj) = 
, var(Xj) = 
,
µ
σ2 < 4
and let 
.  Then 
¯X ' (1/n)'n
j'1Xj
n( ¯X & µ) 6d N(0,σ2).
Proof: Without loss of generality we may assume that F = 0 and F = 1. Let 
 be the
φ(t)
characteristic function of Xj. The assumptions F = 0 and F = 1 imply that the first and second
derivatives of 
 at t = 0 are equal to  
 respectively, hence by Taylor's
φ(t)
φ)(0) ' 0, φ))(0) ' &1,
theorem,  applied to Re[N(t)] and Im[N(t)] separately, there exists numbers 
 such
λ1,t,λ2,t 0 [0,1]
that
φ(t) ' φ(0) % tφ)(0) % 1
2
t 2 Re[φ))(λ1,t.t)] % i.Im[φ))(λ2,t.t)] ' 1 & 1
2
t 2 % z(t)t 2,
say, where  
  Note that z(t) is bounded and
z(t) ' (1 % Re[φ))(λ1,t.t)] % i.Im[φ))(λ2,t.t)])/2.
satisfies limt60z(t) ' 0.
Next, let 
 be the characteristic function of 
. Then
φn(t)
n ¯X
φn(t) ' φ(t/ n)
n '
1 & t 2
2n
% z(t/ n)t 2
n
n
'
1 & t 2
2n
n
% j
n
m'1
n
m
1& t 2
2n
n&m z(t/ n)t 2
n
m
(6.32)

227
For n so large that t2/(2n) < 1 we have
/0000
/0000
j
n
m'1
n
m
1& t 2
2n
n&m z(t/ n)t 2
n
m
# j
n
m'1
n
m
*z(t/ n)*t 2
n
m
'
1 % *z(t/ n)*t 2
n
n
& 1
(6.33)
Now observe that for any real valued sequence an which converges to a,
lim
n64
ln(1%an/n)n ' lim
n64
n ln(1%an/n) ' lim
n64
an × lim
n64
ln(1%an/n) & ln(1)
an/n
' a × lim
δ60
ln(1%δ)&ln(1)
δ
' a,
hence
limn64an ' a Y lim
n64
1 % an/n n ' e a.
(6.34)
Letting an = 
, which has limit a = 0, it follows from (6.34)  that the right-hand side
*z(t/ n)*t 2
expression in (6.33) converges to zero, and letting an = a = -t2/2 it follows then from (6.32) that
lim
n64
φn(t) ' e &t 2/2.
(6.35)
The right-hand side of (6.35) is the characteristic function of the standard normal distribution.
The theorem follows now from Theorem 6.22. Q.E.D.
There is also a multivariate version of the central limit theorem:
Theorem 6.24: Let X1,....,Xn be i.i.d. random vectors in 
 satisfying E(Xj) = 
, Var(Xj) =
úk
µ
 is finite, and let 
. Then 
Σ, where Σ
¯X ' (1/n)'n
j'1Xj
n( ¯X & µ) 6d Nk(0,Σ).

228
Proof: Let  
 be arbitrary but not a zero vector. Then it follows from Theorem 6.23
ξ 0 úk
that  
 hence it follows from Theorem 6.22 that for all 
 
nξT( ¯X&µ) 6d N(0,ξTΣξ),
t 0 ú,
Choosing t = 1, we thus have that for arbitrary
limn64E(exp[i.t nξT( ¯X&µ)]) ' exp(&t 2ξTΣξ/2).
,  
Since the latter is the characteristic
ξ 0 úk
limn64E(exp[i.ξT n( ¯X&µ)]) ' exp(&ξTΣξ/2).
function of the 
 distribution, Theorem 6.24 follows now from Theorem 6.22. Q.E.D.
Nk(0,Σ)
Next, let M be a continuously differentiable mapping from  úk to úm, and let the
conditions of Theorem 6.24 hold. The question is: What is the limiting distribution of
 if any? In order to answer this question, assume for the time being that k = m
n(Φ(X) & Φ(µ)),
= 1, and let var(Xj) = F2, so that  
 It follows from the mean value
n(X & µ) 6d N(0,σ2).
theorem (see Appendix II) that there exists a random variable 8 0 [0,1] such that
n(Φ(X) & Φ(µ)) '
n(X & µ)Φ)(µ%λ(X&µ))
Since 
 implies 
 which by Theorem 6.16 implies that
n(X & µ) 6d N(0,σ2)
(X & µ) 6d 0,
 it follows that 
 Moreover, since the derivative  
 is continuous
X 6p µ ,
µ % λ(X & µ) 6p µ .
Φ)
in µ  it follows now from Theorem 6.3 that  
 Therefore, it follows
Φ)(µ % λ(X & µ)) 6p Φ)(µ).
from Theorem 6.21 that  
 Along similar lines, applying
n(Φ(X) & Φ(µ)) 6d N[0,σ2(Φ)(µ))2].
the mean value theorem to each of the components of  M  separately,  the following more general
result can be proved. This approach is known as the ****-method.
Theorem 6.25: Let Xn be a random vector in 
 satisfying  
 where µ
úk
n(Xn & µ) 6d Nk[0,Σ],
0
 is nonrandom. Moreover, let  
 be a
úk
Φ(x) ' (Φ1(x),....,Φm(x))T with x ' (x1,....,xk)T
mapping from  úk to  úm such that the m×k matrix of partial derivatives

229
∆(x) '
MΦ1(x)/Mx1 þ MΦ1(x)/Mxk
!
"
!
MΦm(x)/Mx1 þ MΦm(x)/Mxk
(6.36)
exists in an arbitrary small open neighborhood of µ and its elements are continuous in µ. Then
n(φ(Xn) & Φ(µ)) 6d Nm[0,∆(µ)Σ∆(µ)T].
6.8.
Stochastic boundedness, tightness, and the Op and op notations.
The stochastic boundedness and related tightness concepts are important for various
reasons, but one of the most important reasons is that they are necessary conditions for
convergence in distribution.
Definition 6.7: A sequence of random variables or vectors Xn  is said to be stochastically
bounded if for every  g 0 (0,1) there exists a finite M > 0 such that  
 
infn$1P[||Xn|| # M] > 1&g.
Of course, if Xn   is bounded itself, i.e., 
 for all n,  it is stochastically bounded
P[||Xn|| # M] ' 1
as well, but the other way around may not be true. For example, if the Xn  are equally distributed
(but not necessarily independent)  random variables with common distribution function F, then
for every  g 0 (0,1) we can choose continuity points !M and M of F such that 
 = 
P[|Xn| # M]
 = 1!g. Thus, the stochastic boundedness condition limits the heterogeneity of the
F(M)&F(&M)
Xn ‘s. 
Stochastic boundedness is usually denoted by Op(1):  Xn = Op(1) means that the sequence 
Xn  is stochastically bounded. More generally:

230
Definition 6.8: Let an be a sequence of positive non-random variables. Then Xn = Op(an) means
that Xn /an is stochastically bounded, and Op(an) by itself represents a generic random variable or
vector Xn  such that  Xn = Op(an).
The necessity of stochastic boundedness for convergence in distribution follows from the
fact that:
Theorem 6.26: Convergence in distribution implies stochastic boundedness.
Proof: Let Xn and X be random variables with corresponding distribution functions Fn and
F, respectively, and assume that 
Given an g 0 (0,1) we can choose continuity points
Xn 6d X.
!M1 and M1 of F such that  
 Since  
F(M1) > 1&g/4, F(&M1) < g/4.
limn64Fn(M1) ' F(M1)
there exists an index n1 such that 
 hence 
 if
|Fn(M1) &
F(M1)| < g/4 if n $ n1,
Fn(M1) > 1&g/2
  Similarly,  there exists an index  n2  such that 
  Let m = 
n $ n1.
Fn(&M1) < g/2 if n $ n2.
 Then 
 Finally, we can always choose an M2  so large
max(n1,n2).
infn$mP[|Xn| # M1] > 1&g.
that  
 Taking 
 the theorem follows.  The
min1#n#m&1P[|Xn| # M2] > 1&g.
M ' max(M1,M2)
proof of the multivariate case is almost the same. Q.E.D.
Note that since convergence in probability implies convergence in distribution, it follows
trivially from Theorem 6.26 that convergence in probability implies stochastic boundedness. 
For example, let  
 are  i.i.d. random variables with
Sn ' 'n
j'1Xj, where the Xj's
expectation µ and variance F2 < 4.  If µ = 0 then  
 because by the central limit
Sn ' Op( n),
theorem, 
   converges in distribution to N(0,F2). However, if µ … 0 then only 
Sn/ n
Sn ' Op(n),

231
because then 
 hence  
 and thus 
Sn/ n & µ n 6d N(0,σ2),
Sn/ n ' Op(1) % Op( n)
 = 
Sn ' Op( n) % Op(n)
Op(n).
In Definition 6.2 I have introduced the concept of uniform integrability. It is left as an
exercise to prove that
Theorem 6.27: Uniform integrability implies stochastic boundedness.
Tightness is the version of stochastic boundedness for probability measures:
Definition 6.9: A sequence of probability measures µn on the Borel sets in úk is called tight if for
an arbitrary  g 0 (0,1) there exists a compact subset K of  úk such that  
 > 1!g.
infn$1µn(K)
Clearly, if   Xn = Op(1) then the sequence of corresponding induced probability measures 
µn is tight, because the sets of the type  
 are closed and bounded for M
K ' {x 0 úk: ||x|| # M}
< 4 and therefore compact.
For sequences of random variables and vectors the tightness concept does not add much
over the stochastic boundedness concept, but the tightness concept is fundamental in proving so-
called functional central limit theorems.
If  Xn = Op(1) then obviously for any * > 0, 
 But 
 is now more than
Xn ' Op(n δ).
Xn/n δ
stochastically bounded, because then we also have that 
 The latter is denoted by
Xn/n δ 6p 0.
Xn ' op(n δ):

232
Definition 6.10: Let an be a sequence of positive non-random variables. Then Xn = op(an) means
that Xn /an converges in probability to zero (or a zero vector if Xn is a vector), and op(an) by itself
represents a generic random variable or vector Xn  such that  Xn = op(an). Moreover, the
sequence 1/an  represents that rate of convergence of Xn .  
Thus, 
 can also be denoted by 
This notation is handy if the difference
Xn 6p X
Xn ' X % op(1).
of Xn and X is a complicated expression. For example, the result of Theorem 6.25 is due to the
fact that by the mean value theorem  n(φ(Xn) & Φ(µ)) ' ˜∆n(µ) n(Xn & µ) ' ∆(µ) n(Xn & µ)
+ op(1),  where
˜∆n(µ) '
MΦ1(x)/Mx
x'µ%λ1,n(Xn&µ)
!
MΦm(x)/Mx
x'µ%λk,n(Xn&µ)
, with λj,n 0 [0,1], j ' 1,....,k.
The remainder term  
 can now be represented by op(1), because
(˜∆n(µ) & ∆(µ)) n(Xn & µ)
 hence by Theorem 6.21 this remainder term
˜∆n(µ) 6p ∆(µ) and
n(Xn & µ) 6d Nk[0,Σ],
converges in distribution to the zero vector and thus it converges in probability to the zero vector. 
6.9.
Asymptotic normality of M-estimators
In this section I will set forth conditions for the asymptotic normality of M-estimators, in
addition to the conditions for consistency. An estimator  of a parameter 
 is
ˆθ
θ0 0 úm
asymptotically normally distributed if there exist an increasing sequence of positive numbers an 
and a positive semi-definite m×m matrix G such that 
 Usually, 
an(ˆθ&θ0) 6d Nm[0,Σ].
an '
n,
but there are exceptions to this rule. 

233
Asymptotic normality is fundamental for econometrics. Most of the econometric tests rely
on it. Moreover, the proof of the asymptotic normality theorem below also illustrates nicely the
usefulness of the main results in this chapter.
Given that the data is a random sample, we only need a few addition conditions over the
conditions of Theorems 6.10 and 6.11:
Theorem 6.28: Let in addition to the conditions of Theorems 6.10 and 6.11 the following
conditions be satisfied:
(a)
1 is convex;
(b)
 is an interior point of  1;
θ0
(c)
For each  
 g(x,2) is twice continuously differentiable on 1;.  
x 0 úk,
(d)
For each pair 
 of components of 2,  
 < 4;
θi1,θi2
E[supθ0Θ*M2g(X1,θ)/(Mθi1Mθi2)*]
(e)
The m×m matrix 
 is nonsingular;
A ' E M2g(X1,θ0)
Mθ0MθT
0
(f)
The  m×m matrix 
 is finite.
B ' E
Mg(X1,θ0)
MθT
0
Mg(X1,θ0)
Mθ0
Then  n(ˆθ&θ0) 6d Nm[0,A &1BA &1].
Proof: I will prove the theorem for the case m = 1 only, leaving the general case as an
exercise.
I have already established in Theorem 6.11 that 
 Since 
 is an interior point of 
ˆθ 6p θ0.
θ0

234
1, the probability that  is an interior point converges to 1, and consequently the probability that
ˆθ
the first-order condition for a maximum of 
 = 
 holds converges
ˆQ(θ)
(1/n)'n
j'1g(Xj,θ) in θ ' ˆθ
to 1. Thus:
 
limn64P[ ˆQ
)(ˆθ) ' 0] ' 1,
(6.37)
where as usual,  
 Next, observe from the mean value theorem that there exists
ˆQ
)(θ) ' d ˆQ(θ)/dθ.
a 
 such that
ˆλ 0 [0,1]
n ˆQ
)(ˆθ) '
n ˆQ
)(θ0) % ˆQ
))(θ0%ˆλ(ˆθ&θ0)) n(ˆθ&θ0),
(6.38)
where 
 Note that by the convexity of 1,
ˆQ
))(θ) ' d 2 ˆQ(θ)/(dθ)2.
P[θ0%ˆλ(ˆθ&θ0) 0 Θ] ' 1,
(6.39)
and by the consistency of  ˆθ,
plimn64[θ0%ˆλ(ˆθ&θ0)] ' θ0
(6.40)
Moreover, it follows from Theorem 6.10 and conditions (c) and (d), with the latter adapted to the
univariate case, that 
plimn64supθ0Θ* ˆQ
))(θ) & Q))(θ)* ' 0.
(6.41)
where 
 is the second derivative of 
 Then it follows from (6.39), (6.40),
Q))(θ)
Q(θ) ' E[g(X1,θ)].
(6.41) and Theorem 6.12 that
plimn64 ˆQ
))(θ0%ˆλ(ˆθ&θ0)) ' Q))(θ0) … 0.
(6.42)
Note that  
 corresponds to the matrix A in condition (e), so that  
 is positive in the
Q))(θ0)
Q))(θ0)
"argmin" case and negative in the "argmax" case. Therefore, it follows from (6.42) and Slutsky’s
theorem (Theorem 6.3) that
plimn64 ˆQ
))(θ0%ˆλ(ˆθ&θ0))&1 ' Q))(θ0)&1 ' A &1.
(6.43)
Now (6.38) can be rewritten as 

235
n(ˆθ&θ0) ' & ˆQ
))(θ0%ˆλ(ˆθ&θ0))&1 n ˆQ
)(θ0) % ˆQ
))(θ0%ˆλ(ˆθ&θ0))&1 n ˆQ
)(ˆθ)
' & ˆQ
))(θ0%ˆλ(ˆθ&θ0))&1 n ˆQ
)(θ0) % op(1),
(6.44)
where the op(1) term follows from (6.37), (6.43) and Slutsky’s theorem. 
Because of condition (b), the first-order condition for 
 applies, i.e.,
θ0
Q)(θ0) ' E[dg(X1,θ0)/dθ0] ' 0.
(6.45)
Moreover, condition (f), adapted to the univariate case, now reads as:
var[dg(X1,θ0)/dθ0] ' B 0 (0,4).
(6.46)
Therefore, it follows from (6.45), (6.46), and the central limit theorem (Theorem 6.23) that
n ˆQ
)(θ0) ' (1/ n)'n
j'1dg(Xj,θ0)/dθ0 6d N[0,B].
(6.47)
Now it follows from (6.43), (6.47) and Theorem 6.21 that
& ˆQ
))(θ0%ˆλ(ˆθ&θ0))&1 n ˆQ
)(θ0) 6d N[0,A &1BA &1],
(6.48)
hence the result of the theorem under review for the case m = 1 follows from (6.44), (6.48) and
Theorem 6.21. Q.E.D.
The result of Theorem 6.28 is only useful if we are able to estimate the asymptotic
variance matrix 
consistently, because then we will be able to design tests of various
A &1BA &1
hypotheses about the parameter vector θ0.
Theorem 6.29: Let
ˆA ' 1
nj
n
j'1
M2g(X1, ˆθ)
MˆθMˆθ
T
,
(6.49)
and 

236
ˆB ' 1
nj
n
j'1
Mg(X1, ˆθ)
Mˆθ
T
Mg(X1, ˆθ)
Mˆθ
.
(6.50)
Under the conditions of Theorem 6.28, 
 and under the additional condition that
plimn64 ˆA ' A,
 
 Consequently, 
E[supθ0Θ||Mg(X1,θ)/MθT||2] < 4, plimn64 ˆB ' B.
plimn64 ˆA
&1 ˆB ˆA
&1 ' A &1BA &1.
Proof: The theorem follows straightforwardly from  the uniform weak law of large
numbers and various Slutsky’s theorems, in particular Theorem 6.21. 
6.10.
Hypotheses testing
As an application of Theorems 6.28 and 6.29, consider the problem of testing a null
hypothesis against an alternative hypothesis of the form
H0: Rθ0 ' q,
H1: Rθ0 … q,
(6.51)
respectively, where R is a given r×m matrix of rank r # m, and q is a given  r×1 vector. Under the
null hypothesis in (6.51) and the conditions of Theorem 6.2, n(Rˆθ&q) 6d Nr[0,RA &1BA &1R T],
and if the matrix B is nonsingular then the asymptotic variance matrix involved is nonsingular.
Then is follows from Theorem 6.21 that:
Theorem 6.30: Under the conditions of Theorems 6.28 and 6.29, the additional condition that B
is nonsingular, and the null hypothesis in (6.51) with R of full rank r,
Wn ' n(Rˆθ&q)TR ˆA
&1 ˆB ˆA
&1R T &1(Rˆθ&q) 6d χ2
r .
(6.52)
On the other hand, under the alternative hypothesis in (6.51),
 
Wn/n 6p (Rθ0&q)TRA &1BA &1R T &1(Rθ0&q) > 0.
(6.53)

237
The statistic Wn is now the test statistic of the Wald test of the null hypothesis in (6.51).
Given the size " 0 (0,1), choose a critical value $ such that for a 
 distributed random variable
χ2
r
Z, 
 so that under the null hypothesis in (6.51), 
 6 ". Then the null
P[Z > β] ' α,
P[Wn > β]
hypothesis is accepted if 
and rejected in favor of the alternative hypothesis if 
Wn # β
Wn > β.
Due to (6.53), this test is consistent.
In the case that r = 1, so that R is a row vector, we can modify (6.52) to
tn '
n R ˆA
&1 ˆB ˆA
&1R T &1/2(Rˆθ&q) 6d N(0,1),
(6.54)
whereas under the alternative hypothesis, (6.53) becomes
tn/ n 6p RA &1BA &1R T &1/2(Rθ0&q) … 0.
(6.55)
These results can be used to construct a two-sided or one sided  test, similarly to the t-test we
have seen before in the previous chapter. In particular, 
Theorem 6.31: Assume that the conditions of Theorem 6.30 hold. Let  
 be component i of  
θi,0
θ0,
and let  
 be component i of  
 Consider the hypotheses  
 
 
 
ˆθi
ˆθ.
H0: θi,0 ' θ(
i,0, H1: θi,0 … θ(
i,0,
where 
 is given  (often the value 
 is of special interest). Let the vector  ei be column i
θ(
i,0
θ(
i,0 ' 0
of the unit matrix Im. Then under H0,
ˆti '
n(ˆθi & θ(
i,0)
e T
i ˆA
&1 ˆB ˆA
&1ei)
6d N(0,1),
(6.56)
whereas under H1,
ˆti/ n 6p
ˆθi,0 & θ(
i,0
e T
i A &1BA &1ei)
… 0.
(6.57)

238
Given the size " 0 (0,1), choose a critical value $ such that for a standard normally
distributed random variable U,  
 so that by (6.56),   
 6 " if the null
P[|U| > β] ' α,
P[|ˆti| > β]
hypothesis is true. Then the null hypothesis is accepted if 
and rejected in favor of the
|ˆti| # β
alternative hypothesis if  
 It is obvious from (6.57) that this test is consistent. 
|ˆti| > β.
The statistic  in (6.56)  is usually referred to as a t-test statistic because of the similarity
ˆti
of this test with the t-test in the normal random sample case. However, its finite sample
distribution under the null hypothesis may not be of the t distribution type at all. Moreover, in the
case  
 the statistic   is called the t-value (or pseudo t-value) of the estimator 
 and if
θ(
i,0 ' 0
ˆti
ˆθi,
the test rejects the null hypothesis  this estimator is said to be significant at the "×100%
significance level.
6.12.
Exercises
1.
Let 
 and 
. Prove that 
 if and only if 
Xn ' (X1,n,...,Xk,n)T
c ' (c1,...,ck)T
plimn64Xn ' c
  for i = 1,..,k.
plimn64Xi,n ' ci
2.
Prove that if 
 = 1 and Xn 6p  X  then  
 = 1.
P(*Xn* # M)
P(*X* # M)
3.
Complete the proof of Theorem 6.5.
4.
Prove Theorem 6.12.
5.
Explain why the random vector Xn in (6.16)  does not converge in distribution.
6.
Prove Theorem 6.16.
7.
Prove Theorem 6.17.
8.
Prove (6.21). 
9.
Prove Theorem 6.19.

239
10.
Prove Theorem 6.20,  using Theorem 6.18.
11.
Finish the proof of Theorem 6.21. 
12.
Answer the questions “Why?” in the proof of Corollary 6.2.
13.
Prove that the limit  (6.35) is just the characteristic function of the standard normal
distribution. 
14.
Prove the first and the last equality in (6.32).
15.
Prove Theorem 6.25.
16.
Prove Theorem 6.27. Hint: Use Chebishev’s inequality for first absolute moments.
17.
Adapt the proof of Theorem 6.28 for m = 1 to the multivariate case m > 1.
18.
Prove Theorem 6.29.
19.
Formulate the conditions (additional to Assumption 6.1 ) for the asymptotic normality of
the nonlinear least squares estimator (6.15) for the special case that  P[E(U 2
1 |X1) ' σ2] ' 1.
Appendices 
6.A.
Proof of the uniform weak law of large numbers
First, recall that "sup" denotes the smallest upper bound of the function involved, and
similarly, "inf" is the largest lower bound. Now let for arbitrary * > 0 and 
,
 = 
θ( 0 Θ Θδ(θ()
 Using the fact that 
{θ0Θ: 2θ&θ(2 < δ}.
supx*f(x)* # max{|supxf(x)|,|infxf(x)|} # |supxf(x)| % |infxf(x)|,
it follows that 

240
sup
θ0Θδ(θ()
*(1/n)'n
j'1g(Xj,θ) & E[g(X1,θ)*
# * sup
θ0Θδ(θ()
(1/n)'n
j'1g(Xj,θ) & E[g(X1,θ)] *
% * inf
θ0Θδ(θ()
(1/n)'n
j'1g(Xj,θ) & E[g(X1,θ)] *
  (6.58)
Moreover,
sup
θ0Θδ(θ()
(1/n)'n
j'1g(Xj,θ) & E[g(X1,θ)]
# (1/n)'n
j'1 sup
θ0Θδ(θ()
g(Xj,θ) &
inf
θ0Θδ(θ()
E[g(X1,θ)]
# |(1/n)'n
j'1 sup
θ0Θδ(θ()
g(Xj,θ) & E[ sup
θ0Θδ(θ()
g(X1,θ)]|
% E[ sup
θ0Θδ(θ()
g(X1,θ)] & E[ inf
θ0Θδ(θ()
g(X1,θ)]
(6.59)
and similarly,
inf
θ0Θδ(θ()
(1/n)'n
j'1g(Xj,θ) & E[g(X1,θ)]
$ (1/n)'n
j'1 inf
θ0Θδ(θ()
g(Xj,θ) &
sup
θ0Θδ(θ()
E[g(X1,θ)]
$ &|(1/n)'n
j'1 inf
θ0Θδ(θ()
g(Xj,θ) & E[ inf
θ0Θδ(θ()
g(X1,θ)]|
% E[ inf
θ0Θδ(θ()
g(X1,θ)] & E[ sup
θ0Θδ(θ()
g(X1,θ)]
(6.60)
Hence
| sup
θ0Θδ(θ()
(1/n)'n
j'1g(Xj,θ)&E[g(X1,θ)] | # |(1/n)'n
j'1 sup
θ0Θδ(θ()
g(Xj,θ)&E[ sup
θ0Θδ(θ()
g(X1,θ)]|
% |(1/n)'n
j'1 inf
θ0Θδ(θ()
g(Xj,θ) & E[ inf
θ0Θδ(θ()
g(X1,θ)]|
% E[ sup
θ0Θδ(θ()
g(X1,θ)] & E[ inf
θ0Θδ(θ()
g(X1,θ)]
(6.61)

241
and similarly
| inf
θ0Θδ(θ()
(1/n)'n
j'1g(Xj,θ)&E[g(X1,θ)] | # |(1/n)'n
j'1 sup
θ0Θδ(θ()
g(Xj,θ)&E[ sup
θ0Θδ(θ()
g(X1,θ)]|
% |(1/n)'n
j'1 inf
θ0Θδ(θ()
g(Xj,θ) & E[ inf
θ0Θδ(θ()
g(X1,θ)]|
% E[ sup
θ0Θδ(θ()
g(X1,θ)] & E[ inf
θ0Θδ(θ()
g(X1,θ)]
(6.62)
Combining (6.58), (6.61), and (6.62) it follows that 
sup
θ0Θδ(θ()
*(1/n)'n
j'1g(Xj,θ)&E[g(X1,θ)* # 2|(1/n)'n
j'1 sup
θ0Θδ(θ()
g(Xj,θ)&E[ sup
θ0Θδ(θ()
g(X1,θ)]|
% 2|(1/n)'n
j'1 inf
θ0Θδ(θ()
g(Xj,θ) & E[ inf
θ0Θδ(θ()
g(X1,θ)]|
% 2 E[ sup
θ0Θδ(θ()
g(X1,θ)] & E[ inf
θ0Θδ(θ()
g(X1,θ)]
  (6.63)
It follows from the continuity of g(x,θ) in θ and the dominated convergence theorem [Theorem
6.5] that
limsup
δ90
sup
θ(0Θ
E [ sup
θ0Θδ(θ()
g(X1,θ) &
inf
θ0Θδ(θ()
g(X1,θ)]
# lim
δ90
E sup
θ(0Θ
[ sup
θ0Θδ(θ()
g(X1,θ) &
inf
θ0Θδ(θ()
g(X1,θ)] ' 0,
hence we can choose δ so small that
sup
θ(0Θ
E [ sup
θ0Θδ(θ()
g(X1,θ) &
inf
θ0Θδ(θ()
g(X1,θ)] < g/4.
(6.64)
Furthermore, by the compactness of Θ it follows that there exist a finite number of θ*'s, say
θ1,...,θN(δ), such that 

242
Θ d ^
N(δ)
i'1
Θδ(θi).
(6.65)
Therefore, it follows from Theorem 6.2,  (6.63),  (6.64), and (6.65), that
P supθ0Θ*(1/n)'n
j'1g(Xj,θ) & E[g(X1,θ)]* > g
# P max1#i#N(δ) supθ0Θδ(θi)*(1/n)'n
j'1g(Xj,θ) & E[g(X1,θ)]* > g
# 'N(δ)
i'1 P supθ0Θδ(θi)*(1/n)'n
j'1g(Xj,θ) & E[g(X1,θ)]* > g
# 'N(δ)
i'1 P |(1/n)'n
j'1supθ0Θδ(θ()g(Xj,θ) & E[supθ0Θδ(θ()g(X1,θ)]|
% |(1/n)'n
j'1infθ0Θδ(θ()g(Xj,θ) & E[infθ0Θδ(θ()g(X1,θ)]| > g/4
(6.66)
#
'N(δ)
i'1 P |(1/n)'n
j'1supθ0Θδ(θ()g(Xj,θ) & E[supθ0Θδ(θ()g(X1,θ)]| > g/8
% 'N(δ)
i'1 P |(1/n)'n
j'1infθ0Θδ(θ()g(Xj,θ) & E[infθ0Θδ(θ()g(X1,θ)]| > g/8 6 0 as n 6 4.
6.B.
Almost sure convergence and strong laws of large numbers
6.B.1. Preliminary results
First, I will show the equivalence of (6.6) and (6.7) in Definition 6.3:
Theorem 6.B.1: Let Xn  and X be random variables defined on a common probability space
{S,ö,P}.  Then 
 = 1  for arbitrary g > 0  if and only if 
limn64P(*Xm & X* # g for all m $ n)
 = 1. This result carries over to random vectors, by replacing |.| with the
P(limn64Xn ' X)

243
Euclidean norm ||.||.
Proof: Note that the statement 
 = 1 reads: There exists a set N  0 ö with
P(limn64Xn ' X)
P(N) = 0 such that 
 pointwise in T 0 S\N.  Such a set N is called a null set. 
limn64Xn(ω) ' X(ω)
Denote
An(g) ' _4
m'n{ω 0 Ω: *Xm(ω) & X(ω)* # g}.
(6.67)
First, assume that for arbitrary  g > 0,  
 Since  
 it follows that 
limn64P(An(g)) ' 1.
An(g) d An%1(g)
 hence  
 is a null set, and so is the
P[^4
n'1An(g)] ' limn64P(An(g)) ' 1,
N(g) ' Ω\^4
n'1An(g)
countable union 
Now let T 0 S\N.  Then 
= 
N ' ^4
k'1N(1/k).
ω 0 Ω\^4
k'1N(1/k) ' _4
k'1 ˜N(1/k)
 hence for each positive integer k,  
Since  
_4
k'1^4
n'1An(1/k),
ω 0 ^4
n'1An(1/k).
An(1/k) d An%1(1/k)
it follows now that for each positive integer k there exists a positive integer nk(ω) such that
 for all n $ nk(ω).  Let k(g) be the smallest integer  $ 1/g, and let 
ω 0 An(1/k)
n0(ω,g) ' nk(g)(ω).
Then for arbitrary  g > 0, 
Therefore,  
*Xn(ω) & X(ω)* # g if n $ n0(ω,g).
limn64Xn(ω) ' X(ω)
pointwise in T 0 S\N,  hence 
 = 1.
P(limn64Xn ' X)
Next, assume that the latter holds, i.e., the exists a null set N such that  
 =  
limn64Xn(ω)
pointwise in T 0 S\N. Then for arbitrary  g > 0 and  T 0 S\N  there exists a positive integer
X(ω)
 such that 
and therefore also 
 Thus, 
and
n0(ω,g)
ω 0 An0(ω,g)(g)
ω 0 ^4
n'1An(g).
Ω\N d ^4
n'1An(g)
consequently, 1 = 
Since  
 it follows now that 
P(Ω\N) # P[^4
n'1An(g)].
An(g) d An%1(g)
 =  
 = 1. Q.E.D.
limn64P(An(g))
P[^4
n'1An(g)]
The following theorem, known as the Borel-Cantelli lemma, provides a convenient
condition for almost sure convergence.

244
Theorem 6.B.2: (Borel-Cantelli). If for arbitrary  g > 0, 
 then
'4
n'1P(*Xn & X* > g) < 4,
Xn 6 X a.s.
Proof: Let 
 be the complement of the set 
in  (6.67). Then
˜An(g)
An(g)
 
P( ˜An(g)) ' P[^4
m'n{ω 0 Ω: *Xm(ω) & X(ω)* > g}] # '4
m'nP[|Xn & X| > g] 6 0,
where the latter conclusion follows from the condition that 
6  Thus, 
'4
n'1P(*Xn & X* > g) < 4.
 hence 
 Q.E.D.
limn64P( ˜An(g)) ' 0,
limn64P(An(g)) ' 1.
The following theorem establishes the relationship between convergence in probability
and almost sure convergence:
Theorem 6.B.3: 
 if and only if every subsequence nm  of n = 1,2,3,... contains a further
Xn 6p X
subsequence 
 such that for k 6 4, 
nm(k)
Xnm(k) 6 X a.s.
Proof: Suppose that
 is not true, but every subsequence nm of n = 1,2,3,... contains
Xn 6p X
a further subsequence 
  such that for k 6 4, 
Then there exist numbers g > 0,  δ
nm(k)
Xnm(k) 6 X a.s.
0 (0,1) and a subsequence  nm such that 
 Clearly, the same holds
supm$1P[|Xnm & X| # g] # 1&δ.
for every further subsequence 
, which contradicts the assumption that there exists a further
nm(k)
subsequence 
 such that for k 6 4, 
This proves the “only if” part.
nm(k)
Xnm(k) 6 X a.s.
Next, suppose that 
. Then for every subsequence  nm,  
. Consequently,
Xn 6p X
Xnm 6p X
for each positive integer k,  
 =  0,  hence for each k we can find a
limm64P[|Xnm & X| > k &2]
positive integer 
 such that 
 Thus,  
 # 
nm(k)
P[|Xnm(k) & X| > k &2] # k &2.
'4
k'1P[|Xnm(k) & X| > k &2]
 The latter implies that 
 for each g > 0, hence by
'4
k'1k &2 < 4.
'4
k'1P[|Xnm(k) & X| > g] < 4

245
Theorem 6.B.2, 
 Q.E.D.
Xnm(k) 6 X a.s.
6.B.2. Slutsky’s theorem
Theorem 6.B.1 can be used to prove Theorem 6.7. Theorem 6.3 was only proved for the
special case that the probability limit X is constant. However, the general result of Theorem 6.3
follows straightforwardly from Theorems 6.7 and 6.B.3.
Let us restate Theorems 6.3 and 6.7 together:
Theorem 6.B.4: (Slutsky's theorem). Let Xn a sequence of random vectors in úk  converging a.s.
[in probability] to a (random or constant) vector X. Let Q(x) be an   úm -valued function on  úk 
which is continuous on an open (Borel) set B in  úk  for which 
 = 1). Then Q(Xn)
P(X 0 B)
converges a.s.  [in probability] to Q(X).
Proof: Let 
and let  {S ,ö,P} be the probability space involved. According to
Xn 6 X a.s.
Theorem 6.B.1 there exists a null set N1 such that  
 pointwise in T 0 S\N1. 
limn64Xn(ω) ' X(ω)
Moreover, let 
 Then also N2 is a null set, and so is 
N2 ' {ω 0 Ω: X(ω) ó B}.
N ' N1^ N2.
Pick an arbitrary  T 0 S\N.  Since Q is continuous in 
it follows from standard calculus that
X(ω)
 =  
By Theorem 6.B.1 this result implies that 
  
 a.s. Since
limn64Ψ(Xn(ω))
Ψ(X(ω)).
Ψ(Xn) 6 Ψ(X)
the latter convergence result holds along any subsequence, it follows from Theorem 6.B.3 that 
 implies 
 Q.E.D.
Xn 6p X
Ψ(Xn) 6p Ψ(X).

246
6.B.3. Kolmogorov’s strong law of large numbers
I will now provide the proof of Kolmogorov’s strong law of large numbers, based on the
elegant and relatively simple approach of Etemadi (1981). This proof (and other versions of the
proof as well) employs the notion of equivalent sequences:
Definition 6.B.1: Two sequences of random variables, Xn and Yn, n $1, are said to be equivalent
if   '4
n'1P[Xn … Yn] < 4.
The importance of this concept lies in the fact that if one of the equivalent sequences obeys a
strong law of large numbers, then so does the other one:
Lemma 6.B.1: If Xn and Yn are equivalent and  
 then 
 µ a.s.
(1/n)'n
j'1Yj 6 µ a.s.
(1/n)'n
j'1Xj 6
Proof: Without loss of generality we may assume that µ = 0. Let {S ,ö,P} be the
probability space involved, and let  
 An ' ^4
m'n{ω 0 Ω: Xm(ω) … Ym(ω)}.
Then 
 hence  
 and thus 
 The
P(An) # '4
m'nP(Xm … Ym) 6 0,
limn64P(An) ' 0
P(_4
n'1An) ' 0.
latter implies that for each  
there exists a natural number  
 such that
ω 0 Ω\{_4
n'1An}
n((ω)
 for all 
 because if not there exists a countable infinite subsequence
Xn(ω) ' Yn(ω)
n $ n((ω),
 such that 
 hence 
 for all n $1 and thus
nm(ω), m ' 1,2,3,....,
Xnk(ω)(ω) … Ynk(ω)(ω),
ω 0 An
Now let N1 be the null set on which 
 fails to hold, and let N  =
ω 0 _4
n'1An.
(1/n)'n
j'1Yj 6 0 a.s.
Since for each 
 differ for at most a finite number of j’s,
N1^{_4
n'1An}.
ω 0 Ω\N, Xj(ω) and Yj(ω)

247
and  
 it follows that also 
 Q.E.D.
limn64(1/n)'n
j'1Yj(ω) ' 0,
limn64(1/n)'n
j'1Xj(ω) ' 0.
The following construction of equivalent sequences plays a key-role in the proof of the
strong law of large numbers.
Lemma 6.B.2: Let  Xn ,  n $1, be i.i.d., with  
 and let 
 Then Xn 
E[|Xn|] < 4,
Yn ' Xn.I(|Xj| # n).
and Yn  are equivalent.
Proof: The lemma follows from:
 
'4
n'1P[Xn … Yn] ' '4
n'1P[|Xn| > n] ' '4
n'1P[|X1| > n] # m
4
0
P[|X1| > t]dt ' m
4
0
E[I(|X1| > t)]dt
# E m
4
0 I(|X1| > t)]dt ' E m
|X1|
0
dt ' E[|X1|] < 4.
Q.E.D.
Now let  Xn ,  n $1, be the sequence in Lemma 6.B.2, and suppose that  (1/n)'n
j'1max(0,Xj)
6 
 a.s. and  
 6 
 a.s. Then it is easy to verify from
E[max(0,X1)]
(1/n)'n
j'1max(0,&Xj)
E[max(0,&X1)]
Theorem 6.B.1, by taking the union of the null sets involved, that 
1
nj
n
j'1
max(0,Xj)
max(0,&Xj)
6
E[max(0,X1)]
E[max(0,&X1)]
a.s.
Applying Slutsky’s theorem (Theorem 6.B.4) with Φ(x,y) = x !y it follows that 
 6
(1/n)'n
j'1Xj
 a.s. Therefore, the proof of Kolmogorov’s strong law of large numbers is completed by
E[X1]
Lemma 6.B.3 below.

248
Lemma 6.B.3: Let the conditions of Lemma 6.B.2 hold, and assume in addition that 
 =
P[Xn $ 0]
1. Then 
  
(1/n)'n
j'1Xj 6 E[X1] a.s.
Proof: Let  
 and observe that
Z(n) ' (1/n)'n
j'1Yj
Var(Z(n)) # (1/n 2)'n
j'1E[Y 2
j ] ' (1/n 2)'n
j'1E[X 2
j I(Xj # j)]
# n &1E[X 2
1 I(X1 # n)].
(6.68)
Next let α > 1 and g > 0 be arbitrary. It follows from (6.68) and Chebishev’s inequality that
j
4
n'1
P[|Z([αn]) & E[Z([αn])]| > g] # j
4
n'1
Var(Z([αn]))/g2 # j
4
n'1
E[X 2
1 I(X1 # [αn])]
g2[αn]
# g&2E X 2
1 '4
n'1I(X1 # [αn])/[αn] ,
(6.69)
where 
 is the integer part of 
 Let k be the smallest natural number such that  
[αn]
αn.
X1 # [αk],
and note that 
 Then the last sum in (6.69) satisfies
[αn] > αn/2.
'4
n'1I(X1 # [αn])/[αn] # 2j
4
n'k
α&n ' 2.'4
n'0α&n α&k #
2α
(α&1)X1
,
hence 
E X 2
1 '4
n'1I(X1 # [αn])/[αn] #
2α
α&1E[X1] < 4.
Consequently, it follows from  the Borel-Cantelli lemma that 
 a.s. 
Z([αn]) & E[Z([αn]) 6 0
Moreover, it is easy to verify that 
 Hence, 
 a.s.
E[Z([αn]) 6 E[X1].
Z([αn]) 6 E[X1]

249
For each natural number k > α  there exists a natural number nk such that  [α
nk] # k #
 and since the Xj’s are non-negative we have 
[α
nk%1],
[α
nk]
[α
nk%1]
Z([α
nk]) # Z(k) # [α
nk%1]
[α
nk]
Z([α
nk%1])
(6.70)
The left-hand side expression in  (6.70) converges a.s. to
 as 
and the right-hand
E[X1]/α
k 6 4,
side converges a.s. to  
hence we have with probability 1, 
αE[X1],
1
α
E[X1] # liminfk64Z(k) # limsupk64Z(k) # αE[X1]
In other words, denoting 
, there exists a null set 
Z ' liminfk64Z(k), Z ' limsupk64Z(k)
Nα
(depending on α) such that for all 
Taking the
ω 0 Ω\Nα, E[X1]/α # Z(ω) # Z(ω) # αE[X1].
union N of  
 over all rational α > 1, so that  N  is also a null set7,  the same holds for all
Nα
 and all  rational α > 1. Letting α 91 along the rational values then yields 
 = 
ω 0 Ω\N
limk64Z(k)
 for all  
  Therefore, by Theorem 6.B.1,
Z(ω) ' Z(ω) ' E[X1]
ω 0 Ω\N.
 which by Lemmas 6.B.2 and 6.B.3 implies that
(1/n)'n
j'1Yj 6 E[X1] a.s.,
Q.E.D. 
(1/n)'n
j'1Xj 6 E[X1] a.s..
This completes the proof of Theorem 6.6.
6.B.5. The uniform strong law of large numbers and its applications
Proof of Theorem 6.13: It follows from (6.63) , (6.64) and Theorem 6.6 that
limsup
n64
sup
θ0Θδ(θ()
*(1/n)'n
j'1g(Xj,θ) & E[g(X1,θ)*
# 2 E[ sup
θ0Θδ(θ()
g(X1,θ)] & E[ inf
θ0Θδ(θ()
g(X1,θ)]
< g/2 a.s.,

250
hence (6.66) can now be replaced by
limsup
n64
sup
θ0Θ
*(1/n)'n
j'1g(Xj,θ) & E[g(X1,θ)]*
# limsup
n64
max
1#i#N(δ)
sup
θ0Θδ(θi)
*(1/n)'n
j'1g(Xj,θ) & E[g(X1,θ)]* # g/2 a.s.
(6.71)
Replacing  g/2 with 1/m, m $ 1, the last inequality in (6.71) reads: 
Let {S ,ö,P} be the probability space involved. For m = 1,2,3, ... there exist a null sets Nm 
such that for all ω 0 Ω\Nm,
limsup
n64
sup
θ0Θ
*(1/n)'n
j'1g(Xj(ω),θ) & E[g(X1,θ)]* # 1/m
(6.72)
and the same holds for all  
 uniformly in m. Letting m 6 4 in (6.72), Theorem 6.13
ω 0 Ω\^4
k'1Nk,
follows.
Note that this proof is based on a seminal paper by Jennrich (1969).
An issue that has not yet been addressed is whether  supθ0Θ*(1/n)'n
j'1g(Xj,θ)&E[g(X1,θ)*
is a well-defined random variable. If so, we must have that for arbitrary y > 0, 
{ω 0 Ω: supθ0Θ*(1/n)'n
j'1g(Xj(ω),θ)&E[g(X1,θ)* # y}
' _θ0Θ{ω 0 Ω: *(1/n)'n
j'1g(Xj(ω),θ)&E[g(X1,θ)* # y} 0 ö.
However, this set is an uncountable intersection of sets in ö and therefore not necessarily a set in
ö itself. The following lemma, which is due to Jennrich (1969), shows that in the case under
review there is no problem.
Lemma 6.B.4: Let  
 be a real function on 
 where B is a Borel set
f(x,θ)
B×Θ, B d úk, Θ d úm,

251
and  Θ  is compact (hence  Θ is a Borel set) such that for each x in B, 
 is continuous in
f(x,θ)
 and for each 
 
  is Borel measurable. Then there exists a Borel measurable
θ 0 Θ,
θ 0 Θ, f(x,θ)
mapping 
 such that 
 hence the latter is Borel measurable
θ(x): B 6 Θ
f(x,θ(x)) ' infθ0Θf(x,θ),
itself.  The same result holds for the “sup” case.
Proof: I will only prove this result for  the special case k = m = 1, B = ú, Θ = [0,1]. 
Denote  
 and observe that
Θn ' ^n
j'1{0,1/j,2/j,....,(j&1)/j,1},
 is the set of all rational numbers in [0,1].  Since 
 is finite,
Θn d Θn%1, and that Θ( ' ^4
n'1Θn
Θn
for each positive integer n there exists a Borel measurable function  
 such that 
θn(x): ú 6 Θn
 Let 
 = 
Note that 
is Borel measurable. For each x
f(x,θn(x)) ' infθ0Θnf(x,θ).
θ(x)
liminfn64θn(x).
θ(x)
there exists a subsequence nj (which may depend on x) such that 
 Hence by
θ(x) ' limj64θnj(x).
continuity, 
 =  
 Now suppose that for some g > 0
f(x,θ(x))
limj64f(x,θnj(x)) ' limj64infθ0Θnjf(x,θ).
the latter is greater or equal to  g + 
Then, since for
infθ0Θ(f(x,θ).
 and the latter is monotonic non-increasing in m, it follows
m # nj, infθ0Θnjf(x,θ) # infθ0Θmf(x,θ),
that for all n $1,  
 It is not too hard to show, using the continuity
infθ0Θnf(x,θ) $ g % infθ0Θ(f(x,θ).
of 
 that this is not possible. Therefore, 
 hence by continuity, 
f(x,θ) in θ,
f(x,θ(x)) ' infθ0Θ(f(x,θ),
 Q.E.D.
f(x,θ(x)) ' infθ0Θf(x,θ).
Proof of Theorem 6.14: Let {S ,ö,P} be the probability space involved, and denote
. Now  (6.9) becomes  
 a.s., i.e., there exists a null set N such that for all
θn ' ˆθ.
Q(θn) 6 Q(θ0)
ω 0 Ω\N,
limn64Q(θn(ω)) ' Q(θ0).
(6.73)
Suppose that for some 
 there exists a subsequence
 and an  g > 0 such that 
ω 0 Ω\N
nm(ω)

252
 Then by the uniqueness condition there exists a  δ(ω) > 0 such that 
infm$12θnm(ω)(ω) & θ02 > g.
 for all  
 which contradicts (6.73). Hence, for every
Q(θ0) & Q(θnm(ω)(ω)) $ δ(ω)
m $ 1,
subsequence 
 we have 
 which implies that  
 
nm(ω)
limm64θnm(ω)(ω) ' θ0,
limn64θn(ω) ' θ0.
Proof of Theorem 6.15: The condition 
 a.s. translates as: There exists a null set N1
Xn 6 c
such that for all 
  
 By the continuity of  M on B the latter implies that
ω 0 Ω\N1, limn64Xn(ω) ' c.
 and that for at most a finite number of indices n,  
 
limn64|Φ(Xn(ω)) & Φ(c)| ' 0,
Xn(ω) ó B.
Similarly, the uniform a.s. convergence condition involved translates as: There exists a null set N2 
such that  for all 
 
Take 
 Then for all
ω 0 Ω\N2, limn64supx0B*Φn(x,ω) & Φ(x)* 6 0.
N ' N1^N2.
ω 0 Ω\N,
limsupn64|Φn(Xn(ω),ω) & Φ(c)|
# limsupn64|Φn(Xn(ω),ω) & Φ(Xn(ω))| % limsupn64|Φ(Xn(ω)) & Φ(c)|
# limsupn64supx0B*Φn(x,ω) & Φ(x)* % limsupn64|Φ(Xn(ω)) & Φ(c)| ' 0.
6.C.
Convergence of characteristic functions and distributions
In this appendix I will provide the proof of the univariate version of Theorem 6.22. Let Fn
be a sequence of distribution functions on ú with corresponding characteristic functions  nn(t),
and let F  be a distribution function on ú with characteristic function 
Denote 
n(t) ' limn64nn(t).
 
F(x) ' limδ90liminfn64Fn(x%δ),
F(x) ' limδ90limsupn64Fn(x%δ).
The function  
 is  right continuous and monotonic non-decreasing in x but not necessarily a
F(x)
distribution function itself, because 
 may be less than one, or even zero. On the other
limx84 F(x)

253
hand, it is easy to verify that 
 = 0.  Therefore, if 
 = 1 then 
 is a
limx9&4 F(x)
limx64 F(x)
F
distribution function. The same applies to 
  If  
 = 1 then 
 is a distribution
F(x):
limx64 F(x)
F
function.
I will first show that  
 and then that  
limx64 F(x) ' limx64 F(x) ' 1,
F(x) ' F(x).
Lemma 6.C.1:  Let Fn be a sequence of distribution functions on ú with corresponding
characteristic functions  nn(t), and suppose that  
 pointwise for each t in ú,
n(t) ' limn64nn(t)
where n is continuous in t = 0. Then 
 is a distribution function,
F(x) ' limδ90liminfn64Fn(x%δ)
and so is F(x) ' limδ90limsupn64Fn(x%δ).
Proof: For T > 0 and A > 0 we have
1
2T m
T
&T
nn(t)dt '
1
2T m
T
&T m
4
&4
exp(i.t.x)dFn(x)dt '
1
2T m
4
&4 m
T
&T
exp(i.t.x)dtdFn(x)
'
1
2T m
4
&4 m
T
&T
cos(t.x)dtdFn(x) ' m
4
&4
sin(Tx)
Tx
dFn(x)
' m
2A
&2A
sin(Tx)
Tx
dFn(x) % m
&2A
&4
sin(Tx)
Tx
dFn(x) % m
4
2A
sin(Tx)
Tx
dFn(x)
(6.74)
Since |sin(x)/x| # 1 and 
 it follows from (6.74) that 
|Tx|&1 # (2TA)&1 for |x| > 2A
/00000
/00000
1
T m
T
&T
nn(t)dt
# 2 m
2A
&2A
dFn(x) %
1
AT m
&2A
&4
dFn(x) %
1
AT m
4
2A
dFn(x)
' 2 1 &
1
2AT
m
2A
&2A
dFn(x) %
1
AT ' 2 1 &
1
2AT µn([&2A,2A]) %
1
AT ,
(6.75)
where 
 is the probability measure on the Borel sets in ú corresponding to Fn.  Hence, putting
µn

254
 it follows from (6.75) that 
T ' A &1
µn([&2A,2A]) $ /00000
/00000
A m
1/A
&1/A
nn(t)dt & 1,
(6.76)
which can be rewritten as
Fn(2A) $ /00000
/00000
A m
1/A
&1/A
nn(t)dt & 1 % Fn(&2A) & µn({&2A})
(6.77)
Now let 2A and !2A be continuity points of 
 Then it follows from (6.77) , the condition
F.
that  
 pointwise for each t in ú, and the bounded8 convergence theorem that
n(t) ' limn64nn(t)
F(2A) $ /00000
/00000
A m
1/A
&1/A
n(t)dt & 1 % F(&2A)
(6.78)
Since  
= 1 and n is continuous in 0 the integral in (6.78) converges to 2  for 
n(0)
A 6 4.
Moreover, 
 if
 Consequently, it follows from (6.78) that
F(&2A) 9 0
A 6 4.
By the same argument it follows that 
Thus, 
 and 
 are
limA64 F(2A) ' 1.
limA64 F(2A) ' 1.
F
F
distribution functions. Q.E.D.
Lemma 6.C.2:  Let Fn be a sequence of distribution functions on ú such that 
= 
F(x)
  and  
 are distribution functions. Then for
limδ90liminfn64Fn(x%δ)
F(x) ' limδ90limsupn64Fn(x%δ)
every bounded continuous function φ on  ú and every g > 0 there exist subsequences 
 and
nk(g)
 such that
nk(g)

255
limsupk64/
/
mφ(x)dFn k(g)(x) & mφ(x)dF(x) < g,
limsupk64/
/
mφ(x)dFn k(g)(x) & mφ(x)dF(x) < g.
Proof: Without loss of generality we may assume that 
 for all x. For any g > 0
φ(x) 0 [0,1]
we can choose continuity points a < b of F(x)  such that   
> 1!g. Moreover, we can
F(b) & F(a)
choose continuity points a = c1 < c2 <...< cm = b of F(x) such that for j = 1,..,m-1,
sup
x0(cj,cj%1]
φ(x) &
inf
x0(cj,cj%1]
φ(x) # g.
(6.79)
Furthermore, there exists a subsequence nk (possibly depending g) on such that
limk64Fnk(cj) ' F(cj) for j '1,2.,...,m.
(6.80)
Now define
ψ(x) '
inf
x0(cj,cj%1]
φ(x) for x 0 (cj,cj%1], j ' 1,..,m&1,
ψ(x) ' 0 elsewhere.
(6.81)
Then by (6.79),  
, and 
,
0 # φ(x) & ψ(x) # g for x 0 (a,b]
0 # φ(x) & ψ(x) # 1 for x ó (a,b]
hence
limsup
n64 /
/
mψ(x)dFn(x) & mφ(x)dFn(x)
# limsup
n64
m
x0(a,b]
*ψ(x)&φ(x)*dFn(x) %
m
xó(a,b]
*ψ(x)&φ(x)*dFn(x)
# g % 1 & limsup
n64
Fn(b) & Fn(a) # g % 1 & F(b) & F(a) # 2g.
(6.82)
Moreover, if follows from (6.79) and (6.81) that 
/
/
mψ(x)dF(x) & mφ(x)dF(x) # 2g,
(6.83)

256
and from (6.80) that
limk64mψ(x)dFnk(x) ' mψ(x)dF(x).
(6.84)
Combining (6.82), (6.83) and (6.84) it follows that
limsupk64/
/
mφ(x)dFnk(x) & mφ(x)dF(x) # limsupk64/
/
mφ(x)dFnk(x) & mψ(x)dFnk(x)
% limsupk64/
/
mψ(x)dFnk(x) & mψ(x)dF(x) % limsupk64/
/
mψ(x)dF(x) & mφ(x)dF(x) < 4g
(6.85)
A similar result holds for the case 
  Q.E.D.
F.
Let 
 be the characteristic function of 
 Since 
 it follows from
n((t)
F.
n(t) ' limn64nn(t),
Lemma 6.C.2 that for each t and arbitrary g > 0,   
 The
|n(t) & n((t)| < g, hence n(t) ' n((t).
same result holds for the characteristic function  
 of 
 
 Consequently, 
 =
n((t)
F: n(t) ' n((t).
n(t)
is the characteristic function of both 
 and 
, which by Lemma 6.C.1 are distribution
limn64nn(t)
F
F
functions. By the uniqueness of characteristic functions (see Appendix 2.C in Chapter 2) it
follows that both distributions are equal:  
 say. Thus, for each continuity
F(x) ' F(x) ' F(x),
point x of F,  F(x) ' limn64Fn(x).
Note that we have not assumed from the outset that 
is a characteristic
n(t) ' limn64nn(t)
function, but only that this pointwise limit exists and is continuous in zero. Consequently, the 
univariate version of the “if” part of Theorem 6.22 can be restated more generally as follows.
Theorem 6.C.1: Let Xn be a sequence of random variables with corresponding characteristic
functions 
If 
 exists for all 
 and 
 is continuous in t = 0, i.e.,
φn(t).
φ(t) ' limn64φn(t)
t 0 ú
φ(t)
 then:
limt60φ(t) ' 1,

257
1.
Recall that  
 = 
 has a Binomial (n,1/2) distribution, so that the
n(Xn % 1)/2
'n
j'1(Yj % 1)/2
distribution function Fn(x) of  Xn is
Fn(x) ' P[Xn # x] ' P[n(Xn%1)/2 # n(x % 1)/2] ' 'min(n,[n(x % 1)/2])
k'0
n
k (1/2)n,
where [z] denotes the largest integer # z, and the sum 
 is zero if m < 0.
'm
k'0
2.
The Yj’s have been generated as Yj = 2.I(Uj > 0.5) !1, where the Uj ‘s are random
drawings from the uniform [0,1] distribution, and I(.) is the indicator function.
3.
Recall that open subsets of a Euclidean space are Borel sets.
4.
See Appendix II.
5.
Thus M is continuous in y on a little neighborhood of c.
6.
Let am, m $1, be a sequence of non-negative numbers such that 
= K  <  4. Then 
'4
m'1am
 is monotonic non-decreasing in n  $ 2, with limit 
 hence
'n&1
m'1am
limn64'n&1
m'1am ' '4
m'1am ' K,
 Thus, 
 = 0.
K ' '4
m'1am ' limn64'n&1
m'1am % limn64'4
m'nam ' K % limn64'4
m'nam.
limn64'4
m'nam
7.
Note that 
 is an uncountable union and may therefore not be a null set.
^α0(1,4)Nα
Therefore, we need to confine the union to all rational " > 1, which is countable.
8.
Note that 
 # 1.
|n(t)|
(a)
 is a characteristic function itself.  
φ(t)
(b)
 Xn 6d X, where X is a random variable with characteristic function φ(t).
This result carries over to the multivariate case, but the proof is rather complicated, and
therefore omitted. See Section 29 in Billingsley (1986).
Endnotes

258
Chapter 7
Dependent Laws of Large Numbers and Central Limit Theorems 
In Chapter 6 I have focused on convergence of sums of i.i.d. random variables, in
particular the law of large numbers and the central limit theorem. However, macroeconomic and
financial data are time series data, for which the independence assumption does not apply. 
Therefore, in this chapter I will generalize the weak law of large numbers and the central limit
theorem to certain classes of time series.
7.1.
Stationarity and the Wold decomposition
In Chapter 3 I have introduced the concept of strict stationarity, which for convenience
will be restated here: 
Definition 7.1: A time series process Xt  is said to be strictly stationary if for arbitrary integers
m1 < m2  <....< mn  the joint distribution of  
 does not depend on the time index t.
Xt&m1,......,Xt&mn
A weaker version of stationarity is covariance stationarity, which requires that the first
and second moments of any set 
 of time series variables do not depend on the
Xt&m1,......,Xt&mn
time index t.

259
Definition 7.2: A time series process  Xt  0 úk  is covariance stationary (or weakly stationary)  if 
 <  4,  and for all integers t and m,  
 and 
 =  
  do not
E[||Xt||2]
E[Xt] ' µ
E[(Xt&µ)(Xt&m&µ)T]
Γ(m)
depend on the time index t.
Clearly, a strictly stationary time series process  Xt is covariance stationary if  
 <  4.
E[||Xt||2]
For zero-mean covariance stationary processes the famous Wold (1938) decomposition
theorem holds. This theorem is the basis for linear time series analysis and forecasting, in
particular the Box-Jenkins (1979) methodology,  and vector autoregression innovation response
analysis. See Sims (1980, 1982, 1986) and Bernanke (1986) for the latter.
Theorem 7.1: (Wold decomposition)  Let  Xt  0 ú be a zero-mean covariance stationary process.
Then we can write 
 where 
 the Ut’s are zero-mean
Xt ' '4
j'0αjUt&j % Wt,
α0 ' 1, '4
j'0α2
j < 4,
covariance stationary and uncorrelated random variables, and Wt is a deterministic process, i.e.,
there exist coefficients 
 such that   
 Moreover, Ut  = 
βj
P[Wt ' '4
j'1βjWt&1] ' 1.
Xt & '4
j'1βjXt&1
and  E[Ut%mWt] ' 0 for all integers m and t.
Intuitive proof: The exact proof employs Hilbert space theory, and will therefore be given
in the Appendix to this chapter. However, the intuition behind the Wold decomposition is not too
difficult. 
It is possible to find a sequence 
 j = 1,2,3,....., of real numbers such that 
βj,
  is minimal. The random variable
E[(Xt & '4
j'1βjXt&j)2]

260
ˆXt ' '4
j'1βjXt&j
(7.1)
is then called the linear projection of Xt on Xt!j, j $ 1.  Denoting 
Ut ' Xt & '4
j'1βjXt&j
(7.2)
it follows from the first-order condition 
 that
ME[(Xt & '4
j'1βjXt&j)2]/Mβj ' 0
E[UtXt&m] ' 0 for m ' 1,2,3,.....
(7.3)
Note that (7.2) and (7.3) imply
E[Ut] ' 0,
E[UtUt&m] ' 0 for m ' 1,2,3,.....
(7.4)
Moreover, note that by (7.2) and (7.3), 
 
E[X 2
t ] ' E[(Ut % '4
j'1βjXt&j)2] ' E[U 2
t ] % E[('4
j'1βjXt&j)2],
so that by the covariance stationarity of Xt, 
E[U 2
t ] ' σ2
u # E[X 2
t ]
(7.5)
and 
E[ ˆX
2
t ] ' E[('4
j'1βjXt&j)2] ' σ2
ˆX # E[X 2
t ]
(7.6)
for all t. Thus it follows from (7.4) and (7.5) that Ut is a zero-mean covariance stationary time
series  process itself. 
Next, substitute 
 in (7.1). Then (7.1) becomes
Xt&1 ' Ut&1 % '4
j'1βjXt&1&j
ˆXt ' β1(Ut&1 % '4
j'1βjXt&1&j) % '4
j'2βjXt&j ' β1Ut&1 % '4
j'2(βj%β1βj&1)Xt&j
' β1Ut&1 % (β2%β2
1)Xt&2 % '4
j'3(βj%β1βj&1)Xt&j.
(7.7)
Now replace  
 in (7.7) by  
 Then (7.7) becomes:
Xt&2
Ut&2 % '4
j'1βjXt&2&j.

261
ˆXt ' β1Ut&1 % (β2%β2
1)(Ut&2 % '4
j'1βjXt&2&j) % '4
j'3(βj%β1βj&1)Xt&j
' β1Ut&1 % (β2%β2
1)Ut&2 % '4
j'3[(β2%β2
1)βj&2%(βj%β1βj&1)]Xt&j
' β1Ut&1 % (β2%β2
1)Ut&2 % [(β2%β2
1)β1%(β3%β1β2)]Xt&3 % '4
j'4[(β2%β2
1)βj&2%(βj%β1βj&1)]Xt&j.
Repeating this substitution m times yields an expression of the type
ˆXt ' 'm
j'1αjUt&j % '4
j'm%1θm,jXt&j,
(7.8)
say. It follows now from  (7.3), (7.4), (7.5) and (7.8) that 
 Hence, letting m 6 4, we have 
E[ ˆX
2
t ] ' σ2
u'm
j'1α2
j % E[('4
j'm%1θm,jXt&j)2].
E[ ˆX
2
t ] ' σ2
u'4
j'1α2
j % limm64E[('4
j'm%1θm,jXt&j)2] ' σ2
ˆX < 4.
Therefore, we can write Xt as
Xt ' '4
j'0αjUt&j % Wt,
(7.9)
where 
  with  
 a remainder term which
α0 ' 1 and '4
j'0α2
j < 4,
Wt ' plimm64'4
j'm%1θm,jXt&j
satisfies
E[Ut%mWt] ' 0 for all integers m and t.
(7.10)
Finally, observe from (7.2) and (7.9) that
Ut & Wt & '4
j'1βjWt&j ' (Xt&Wt) & '4
j'1βj(Xt&j&Wt&j)
' '4
j'0αj Ut&j&'4
m'1βmUt&j&m ' Ut % '4
j'1δjUt&j, say.
(7.11)
It follows now straightforwardly from (7.4), (7.5) and (7.10) that 
 hence
δj ' 0 for all j $ 1,
Wt ' '4
j'1βjWt&j
(7.12)
with probability 1. Q.E.D.

262
Theorem 7.1 carries over to vector-valued covariance stationary processes:
Theorem 7.2: (Multivariate Wold decomposition)  Let  Xt  0 úk be a zero-mean covariance
stationary process. Then we can write 
 where  
 
 is
Xt ' '4
j'0AjUt&j % Wt,
A0 ' Ik, '4
j'0AjA T
j
finite,  the Ut’s are zero-mean covariance stationary and uncorrelated random vectors, i.e.,
 and Wt is a deterministic process, i.e., there exist matrices 
 such
E[UtU T
t&m] ' O for m $ 1,
Bj
that   
 Moreover, Ut  = 
and 
P[Wt ' '4
j'1BjWt&1] ' 1.
Xt & '4
j'1BjXt&1
E[Ut%mW T
t ] ' O for all integers m and t.
Although the process Wt is deterministic, in the sense that it is perfectly predictable from
its past values, it still may be a random process. If so, let 
 be the
öt
W ' σ(Wt,Wt&1,Wt&2,.......)
F!algebra generated by Wt!m  for m $ 0. Then all Wt ’s are measurable 
 for arbitrary natural
öt&m
W
numbers  m, hence all Wt ’s are measurable
 However, it follows from (7.2) and
ö&4
W ' _4
t'0ö&t
W.
(7.9) that each Wt can be constructed from Xt!j  for j $ 0,
hence
 and consequently, all Wt ’s are measurable
öt
X ' σ(Xt,Xt&1,Xt&2,.......) e öt
W,
 This implies that 
 See Chapter 3. 
ö&4
X
' _4
t'0ö&t
X .
Wt ' E[Wt|ö&4
X ].
The F!algebra 
 represents the information contained in the remote past of Xt.
ö&4
X
Therefore, 
 is called the remote F!algebra, and the events therein are called the remote
ö&4
X
events.  If 
 is the trivial F!algebra 
 so that the remote past of Xt is uninformative, 
ö&4
X
{Ω,i},
then 
 =  
 hence  Wt  =  0. However, the same result holds if all the remote
E[Wt|ö&4
X ]
E[Wt],
events have either probability zero or one, as is easy to verify from the definition of conditional
expectations with respect to a  F!algebra. This condition follows automatically from

263
Kolmogorov’s zero-one law  if the Xt’s are independent (see Theorem 7.5 below) , but for
dependent processes this is not guaranteed. Nevertheless, for economic time series this is not too
farfetched an assumption, as in reality they always start from scratch somewhere in the far past,
say five hundred years ago for US time series. 
Definition 7.3: A time series process Xt has a vanishing memory if the events in the remote
F!algebra 
 have either probability zero or one. 
ö&4
X
' _4
t'0σ(X&t,X&t&1,X&t&2,.......)
Thus, under the conditions of Theorems 7.1 and 7.2 and the additional assumption that
the covariance stationary time series process involved has a vanishing memory, the deterministic
term Wt in the Wold decomposition is zero or is a zero vector, respectively.
7.2.
Weak laws of large numbers for stationary processes
I will show now that covariance stationary time series processes with a vanishing memory
obey a weak law of large numbers, and then specialize this result to strictly stationary processes.
Let Xt  0 ú be a covariance stationary process, i.e., for all t,  
 = µ,  
 
E[Xt]
var[Xt] ' σ2,
and 
 If Xt   has a vanishing memory then by Theorem 7.1 there exist
cov(Xt,Xt&m) ' γ(m).
uncorrelated random variables Ut  0 ú with zero expectations and common finite variance σ2
u
such that  
 where 
 Then
Xt & µ ' '4
m'0αmUt&m,
'4
m'0α2
m < 4.
γ(k) ' E '4
m'0αm%kUt&m '4
m'0αmUt&m .
(7.13)
Since 
 it follows that  
 Hence it follows from (7.13) and
'4
m'0α2
m < 4,
limk64'4
m'kα2
m ' 0.
Schwarz inequality that

264
|γ(k)| # σ2
u '4
m'kα2
m '4
m'0α2
m 6 0 as k 6 4
(7.14)
Consequently,
var (1/n)'n
t'1Xt ' σ2/n % 2(1/n 2)'n&1
t'1 'n&t
m'1γ(m) ' σ2/n % 2(1/n 2)'n&1
m'1(n&m)γ(m)
# σ2/n % 2(1/n)'n
m'1|γ(m)| 6 0 as n 6 4.
(7.15)
Using Chebishev’s inequality, it follows now from (7.15) that:
Theorem 7.3: If Xt is a covariance stationary time series process with vanishing memory then
plimn64(1/n)'n
t'1Xt ' E[X1].
This results requires that the second moment of Xt is finite. However, this condition can
be relaxed by assuming strict stationarity:
Theorem 7.4: If Xt is a strictly stationary time series process with vanishing memory, and E[|X1|]
< 4,  then  plimn64(1/n)'n
t'1Xt ' E[X1].
Proof: Assume first that 
 For any positive real number M,  
P[Xt $ 0] ' 1.
XtI(Xt # M)
is a covariance stationary process with vanishing memory, hence by Theorem 7.3,  
plimn64(1/n)'n
t'1 XtI(Xt # M) & E[X1I(X1 # M)] ' 0
(7.16)
Next, observe that 

265
|(1/n)'n
t'1(Xt & E[X1])| # |(1/n)'n
t'1(XtI(Xt # M) & E[X1I(X1 # M)])|
% |(1/n)'n
t'1(XtI(Xt > M) & E[X1I(X1 > M)])|
(7.17)
Since for nonnegative random variables Y and Z,  
 it
P[Y%Z > g] # P[Y > g/2] % P[Z > g/2],
follows from (7.17) that for arbitrary g > 0,
P[|(1/n)'n
t'1(Xt & E[X1])| > g]
# P[|(1/n)'n
t'1(XtI(Xt # M) & E[X1I(X1 # M)])| > g/2]
% P[|(1/n)'n
t'1(XtI(Xt > M) & E[X1I(X1 > M)])| > g/2].
(7.18)
For an arbitrary * 0 (0,1) we can choose M so large that  
 Hence, using
E[X1I(X1 > M)] < gδ/8.
Chebishev’s inequality for first moments, the last  probability in (7.18) can be bounded by */2:
P[|(1/n)'n
t'1(XtI(Xt > M) & E[X1I(X1 > M)])| > g/2] # 4E[X1I(X1 > M)]/g < δ/2.
(7.19)
Moreover, it follows from (7.16) that there exists a natural number 
such that 
n0(g,δ)
P[|(1/n)'n
t'1(XtI(Xt # M) & E[X1I(X1 # M)])| > g/2] < δ/2 if n $ n0(g,δ).
(7.20)
Combining (7.18), (7.19) and (7.20), the theorem follows for the case 
 The
P[Xt $ 0] ' 1.
general case follows easily from  
 and Slutsky’s theorem. Q.E.D.
Xt ' max(0,Xt) & max(0,&Xt)
Most stochastic dynamic macroeconomic models assume that the model variables are
driven by independent random shocks, so that the model variables involved are functions of these
independent random shocks and their past. These random shock are said to form a base for the
model variables involved:

266
Definition 7.4: A time series process  Ut  is a base for a time series process Xt  if for each t, Xt is 
measurable 
  = 
 
öt
&4
σ(Ut,Ut&1,Ut&2,....).
If  Xt has an independent base then it has a vanishing memory, due to Kolmogorov’s zero-one
law:
Theorem 7.5: (Kolmogorov’s zero-one law). Let Xt  be a sequence of independent random
variables or vectors, and let  
 Then the sets in the remote F!algebra 
öt
&4 ' σ(Xt,Xt&1,Xt&2,....).
 have either probability zero or one.
ö&4 ' _4
t'1öt
&4
Proof:  Denote by 
 the F!algebra  generated by 
 Moreover, denote by 
öt%k
t
Xt,....,Xt%k.
 the F!algebra generated by 
 Each set A1 in 
 takes the form 
öt&1
t&m
Xt&1,...,Xt&m.
öt%k
t
A1 ' {ω 0 Ω: (Xt(ω),....,Xt%k(ω))T 0 B1}
for some Borel set  
 Similarly, each set A2  in 
 takes the form 
B1 0 úk%1.
^4
m'1öt&1
t&m
A2 ' {ω 0 Ω: (Xt&1(ω),....,Xt&m(ω))T 0 B2}
for some m $ 1 and some Borel set  
 Clearly,  A1 and  A2 are independent. 
B2 0 úm.
I will now show that the same holds for sets A2  in  
 the smallest 
öt&1
&4 ' σ(^4
m'1öt&1
t&m),
F!algebra containing 
 Note that  
 may not be a F!algebra itself, but it is
^4
m'1öt&1
t&m.
^4
m'1öt&1
t&m
easy to verify that it is an algebra, because 
  For a given set C in 
  with
öt&1
t&m d öt&1
t&m&1.
öt%k
t
positive probability, and for all sets  A  in 
  we have  
 Thus  
 is a
^4
m'1öt&1
t&m
P(A|C) ' P(A).
P(@|C)
probability measure on the algebra 
, which has a unique extension to the smallest
^4
m'1öt&1
t&m
F!algebra containing 
.  See Chapter 1.  Consequently, 
 is true for all
^4
m'1öt&1
t&m
P(A|C) ' P(A)

267
sets A in 
 Moreover, if C has probability zero then 
öt&1
&4.
P(A_C) # P(C) ' 0 ' P(A)P(C).
Thus for all sets C in 
 and all sets A in 
 
öt%k
t
öt&1
&4, P(A_C) ' P(A)P(C).
Next, let  
 where the intersection is taken over all integers t, and let 
A 0 _töt&1
&4,
 Then for some k,  C is a set in 
, and A is a set in 
 and
C 0 ^4
k'1öt
t&k.
öt
t&k
öm
&4 for all m,
therefore  
 hence
 By a similar argument as before it can be
A 0 öt&k&1
&4
,
P(A_C) ' P(A)P(C).
shown that  
 for all sets 
 and 
 But
P(A_C) ' P(A)P(C)
A 0 _töt&1
&4
C 0 σ(^4
k'1öt
t&k).
  d 
 so that we may choose C = A. Thus for all sets  
 
ö&4 ' _töt&1
&4
σ(^4
k'1öt
t&k),
A 0 _töt&1
&4,
  which implies that P(A) is either zero or one.   Q.E.D.
P(A) ' P(A)2,
7.3.
Mixing conditions
Inspection of the proof of Theorem 7.5 reveals that the independence assumption can be
relaxed. We only need independence of an arbitrary set A  in  
 and an arbitrary set C  in
ö&4
 for k  $ 1. A sufficient condition for this is that the process Xt is
öt
t&k ' σ(Xt,Xt&1,Xt&2,...,Xt&k)
"!mixing or n!mixing:
Definition 7.5: Denote  
 and let
öt
&4 ' σ(Xt,Xt&1,Xt&2,...), ö4
t ' σ(Xt,Xt%1,Xt%2,...),
α(m) ' sup
t
sup
A 0 ö4
t , B 0 öt&m
&4
|P(A_B) & P(A).P(B)|,
n(m) ' sup
t
sup
A 0 ö4
t , B 0 öt&m
&4
|P(A|B) & P(A)|.
If 
 then the time series process  Xt involved is said to be "!mixing, and if 
limm64α(m) ' 0
 then  Xt is said to be n!mixing.
limm64n(m) ' 0

268
Note that in the "!mixing case
sup
A 0 öt
t&k, B 0 ö&4
|P(A_B) & P(A).P(B)| # limsup
m64
sup
t
sup
A 0 ö4
t&k, B 0 öt&k&m
&4
|P(A_B) & P(A).P(B)|
' limsup
m64
α(m) ' 0,
hence the sets  
 are independent. Moreover, note that 
 so that
A 0 öt
t&k, B 0 ö&4
α(m) # n(m),
n!mixing implies "!mixing. Thus the latter is the weaker condition, which is sufficient for a
zero-one law:
Theorem 7.6: Theorem 7.5 carries over for "!mixing processes.
Therefore, another version of the weak law of large numbers is:
Theorem 7.7: If Xt is a strictly stationary time series process with an "!mixing base, and E[|X1|]
< 4,  then  plimn64(1/n)'n
t'1Xt ' E[X1].
7.4.
Uniform weak laws of large numbers
7.4.1
Random functions depending on finite-dimensional random vectors
Due to Theorem 7.7, all the convergence in probability results in Chapter 6 for i.i.d.
random variables or vectors carry over to strictly stationary time series processes with an
"!mixing base. In particular, the uniform weak law of large numbers can now be restated as
follows.

269
Theorem 7.8(a): (UWLLN). Let Xt  a strictly stationary k-variate time series process with an  
"!mixing base, and let 
 be non-random vectors in a compact subset 
.  Moreover,
θ 0 Θ
Θ d úm
let g(x,2) be a Borel measurable function on 
 such that for each x,  g(x,2) is a
úk × Θ
continuous function on 1.  Finally, assume that 
 < 4.  Then
E[supθ0Θ*g(Xj,θ)*]
 plimn64supθ0Θ*(1/n)'n
j'1g(Xj,θ) & E[g(X1,θ)]* ' 0.
Theorem 7.8(a) can be proved along the lines as in the proof of the uniform weak law of large
numbers for the i.i.d. case in Appendix 6.A of Chapter 6, simply by replacing the reference to the
weak law of large numbers for i.i.d random variables by a reference to Theorem 7.7. 
7.4.2
Random functions depending on infinite-dimensional random vectors 
In time series econometrics we quite often have to deal with random functions that
depend on a countable infinite sequence of random variables or vectors. As an example, consider
the time series process:
Yt ' β0Yt&1 % Xt, with Xt ' Vt & γ0Vt&1,
(7.21)
where the Vt’s are i.i.d. with zero expectation and finite variance F2, and the parameters involved
satisfy |$0| < 1 and |(0| < 1.  The part 
Yt ' β0Yt&1 % Xt
(7.22)
is an Auto-Regression of order 1, denoted by AR(1), and the part 
Xt ' Vt & γ0Vt&1
(7.23)
is a Moving Average process or order 1, denoted by MA(1). Therefore, model  (7.21) is called an
ARMA(1,1) model. See Box and Jenkins (1976). The condition |$0| < 1 is necessary for the strict

270
stationarity of Yt because then by backwards substitution of (7.21) we can write model (7.21) as
Yt ' '4
j'0β j
0(Vt&j & γ0Vt&1&j) ' Vt % (β0 & γ0)'4
j'1β j&1
0 Vt&j.
(7.24)
This is the Wold decomposition of Yt .  The MA(1) model (7.23) can be written as an AR(1)
model in Vt :
Vt ' γ0Vt&1 % Ut.
(7.25)
If |(0| < 1 then by backwards substitution of (7.25) we can write (7.23)  as  
Xt ' &'4
j'1γ j
0Xt&j % Vt.
(7.26)
Substituting 
 in (7.26), the ARMA(1,1) model (7.21) can now be written as an
Xt ' Yt & β0Yt&1
infinite-order AR model:
Yt ' β0Yt&1 & '4
j'1γ j
0(Yt&j & β0Yt&1&j) % Vt ' (β0 & γ0)'4
j'1γ j&1
0 Yt&j % Vt.
(7.27)
Note that if 
 then (7.27) and (7.24) reduce to 
, so that then there is no
β0 ' γ0
Yt ' Vt
way to identify the parameters. Thus, we need to assume that 
 Moreover, observe from
β0 … γ0.
(7.24) that Yt is strictly stationary, with an independent (hence "-mixing) base.
There are different ways to estimate the parameters 
 in model (7.21) on the basis of
β0, γ0
observations on Yt  for t = 0,1,....,n only. If we assume that the Vt’s are normally distributed we
can use maximum likelihood. See Chapter 8. But it is also possible to estimate the model by
nonlinear least squares (NLLS). 
If we would observe all the Yt’s for t < n then the nonlinear least squares estimator of 
 is
θ0 ' (β0,γ0)T
ˆθ ' argminθ0Θ(1/n)'n
t'1(Yt & ft(θ))2,
(7.28)
where 
ft(θ) ' (β & γ)'4
j'1γ j&1Yt&j, with θ ' (β,γ)T,
(7.29)

271
and
Θ ' [&1%g,1&g]×[&1%g,1&g], g 0 (0,1),
(7.30)
say, where  g is a small number. If we only observe the Yt’s for t = 0,1,....,n, which is the usual
case,  then we still can use NLLS by setting the Yt’s for t < 0  to zero. This yields the feasible
NLLS estimator
˜θ ' argminθ0Θ(1/n)'n
t'1(Yt & ˜ft(θ))2,
(7.31)
where
˜ft(θ) ' (β & γ)'t
j'1γ j&1Yt&j.
(7.32)
For proving the consistency of (7.31) we need to show first that 
plimn64supθ0Θ*(1/n)'n
t'1 (Yt & ˜ft(θ))2 & (Yt & ft(θ))2] * ' 0
(7.33)
(Exercise), and
plimn64supθ0Θ*(1/n)'n
t'1 (Yt & ft(θ))2 & E[(Y1 & f1(θ))2] * ' 0.
(7.34)
(Exercise) However,  the random functions 
 depend on  infinite-dimensional
gt(θ) ' (Yt & ft(θ))2
random vectors  
 so that Theorem 7.8(a) is not applicable to (7.34).
(Yt,Yt&1,Yt&2,Yt&2,.......)T,
Therefore, we need to generalize Theorem 7.8(a) in order to prove (7.34):
Theorem 7.8(b): (UWLLN). Let 
 where Vt is a time series process with
öt ' σ(Vt,Vt&1,Vt&2,....),
an  "!mixing base. Let  
 be a sequence of random function on a compact subset  1 of a
gt(θ)
Euclidean space.  Denote for 
 and * $ 0, 
 If   for each
θ( 0 Θ
Nδ(θ() ' {θ 0 Θ: ||θ&θ(|| # δ}.
 and each * $ 0,
θ( 0 Θ
(a)
 
 and 
 are measurable 
 and strictly stationary,
supθ0Nδ(θ()gt(θ)
infθ0Nδ(θ()gt(θ)
öt
(b)
 and 
 
E[supθ0Nδ(θ()gt(θ)] < 4
E[infθ0Nδ(θ()gt(θ)] > &4,

272
(c) 
,  
limδ90E[supθ0Nδ(θ()gt(θ)] ' limδ90E[infθ0Nδ(θ()gt(θ)] ' E[gt(θ()]
then  plimn64supθ0Θ|(1/n)'n
t'1gt(θ) & E[g1(θ)]| ' 0.
Theorem 7.8(b) can also be proved easily along the lines of the proof of the uniform weak
law of large numbers in Appendix 6.A of Chapter 6.
Note that it is possible to strengthen the (uniform) weak laws of large numbers  to
corresponding strong laws or large numbers by imposing conditions on the speed of convergence
to zero of "(m). See McLeish (1975).
It is not too hard (but rather tedious) to verify that the conditions of Theorem 7.8(b) apply
to the random functions 
 = 
 with Yt defined by (7.21) and  
 by (7.29).
gt(θ)
(Yt & ft(θ))2
ft(θ)
7.4.3
Consistency of M-estimators
Further conditions for the consistency of M-estimators are stated in the next theorem,
which is a straightforward generalization of a corresponding result in Chapter 6 for the i.i.d. case:
Theorem 7.9: Let the conditions of Theorem 7.8(b) hold, and let  θ0 ' argmaxθ0ΘE[g1(θ)],
 If for * > 0,  
 then 
 = 
ˆθ ' argmaxθ0Θ(1/n)'n
t'1gt(θ).
supθ0Θ\Nδ(θ0)E[g1(θ)] < E[g1(θ0)]
plimn64ˆθ
  Similarly, if 
  
 and for * > 0, 
θ0.
θ0 ' argminθ0ΘE[g1(θ)],
ˆθ ' argminθ0Θ(1/n)'n
t'1gt(θ),
  then 
 
infθ0Θ\Nδ(θ0)E[g1(θ)] > E[g1(θ0)],
plimn64ˆθ ' θ0.
Again, it is not too hard (but also rather tedious) to verify that the conditions of Theorem 7.9
apply to (7.28),  with Yt defined by (7.21) and  
 by (7.29). Thus the feasible NLLS estimator
ft(θ)

273
(7.31) is consistent..
7.5.
Dependent central limit theorems 
7.5.1
Introduction
Similarly to the conditions for asymptotic normality of M-estimators in the i.i.d. case (see
Chapter 6), the crucial condition for asymptotic normality of the NLLS estimator (7.28) is that
1
nj
n
t'1
Vt Mft(θ0)/MθT
0 6d N2[0,B],
(7.35)
where 
B ' E V 2
1 Mf1(θ0)/MθT
0 Mf1(θ0)/Mθ0 .
(7.36)
It follows from (7.24) and (7.29) that
ft(θ0) ' (β0 & γ0)'4
j'1β j&1
0 Vt&j,
(7.37)
which is measurable  
 and so is
öt&1 ' σ(Vt&1,Vt&2,Vt&3,....),
Mft(θ0)/MθT
0 '
'4
j'1 β0 % (β0 & γ0)(j&1) β j&2
0 Vt&j
&'4
j'1β j&1
0 Vt&j
.
(7.38)
Therefore, it follows from the law of iterated expectations (see Chapter 3) that
B ' σ2E Mf1(θ0)/MθT
0 Mf1(θ0)/Mθ0
' σ4 '4
j'1 β0 % (β0 & γ0)(j&1) 2β 2(j&2)
0
&'4
j'1 β0 % (β0 & γ0)(j&1) β 2(j&2)
0
&'4
j'1 β0 % (β0 & γ0)(j&1) β 2(j&2)
0
'4
j'1β 2(j&1)
0
(7.39)
and

274
P E[Vt(Mft(θ0)/MθT
0)|öt&1] ' 0 ' 1.
(7.40)
The result (7.40) makes 
 a bivariate martingale difference process, and for an
Vt(Mft(θ0)/MθT
0)
arbitrary nonrandom 
 the process 
 is then a univariate
ξ 0 ú2, ξ … 0,
Ut ' VtξT(Mft(θ0)/MθT
0)
martingale difference process:
Definition 7.4: Let Ut be a time series process defined on a common probability space {S,ö,P},
and let  öt  be a sequence of  sub- F-algebra of  ö . If for each t,
(a)
Ut  is measurable öt,
(b)
öt-1  d öt,
(c)
E[|Ut|] <   4,
(d)
P(E[Ut|öt-1] = 0) = 1,
then {Ut, öt } is called a martingale difference process.   
If condition (d) is replace by 
 then {Ut, öt } is called a martingale. In
P(E[Ut|öt&1] ' Ut&1) ' 1
that case 
 satisfies 
 This is the
∆Ut ' Ut & Ut&1 ' Ut & E[Ut|öt&1]
P(E[∆Ut|öt&1] ' 0) ' 1.
reason for calling the process in Definition 7.4 a martingale difference process.
Thus, what we need for proving (7.35) is a martingale difference central limit theorem.
7.5.2
A generic central limit theorem
In this section I will explain McLeish (1974) central limit theorems for dependent random
variables, with a specialization to stationary martingale difference processes.
The following approximation of exp(i.x) plays a key role in proving central limit

275
theorems for dependent random variables.
Lemma 7.1. For  x 0 ú with |x| < 1,  
 where  
exp(i.x) ' (1%i.x)exp(&x 2/2 % r(x)),
|r(x)| # |x|3.
Proof: It follows from the definition of the complex logarithm and the series expansion of
log(1+i.x) for |x| < 1 (see Appendix III) that
log(1%i.x) ' i.x % x 2/2 % '4
k'3(&1)k&1i kx k/k % i.m.π ' i.x % x 2/2 & r(x) % i.m.π,
  
where 
 Taking the exp of both sides of the equation for log(1+i.x) 
r(x) ' &'4
k'3(&1)k&1i kx k/k.
yields
 In order to prove the inequality 
, observe
exp(i.x) ' (1%i.x)exp(&x 2/2 % r(x)).
|r(x)| # |x|3
that
r(x) ' &'4
k'3(&1)k&1i kx k/k ' x 3'4
k'0(&1)ki k%1x k/(k%3)
' x 3'4
k'0(&1)2ki 2k%1x 2k/(2k%3) % x 3'4
k'0(&1)2k%1i 2k%2x 2k%1/(2k%4)
' x 3'4
k'0(&1)kx 2k%1/(2k%4) % i.x 3'4
k'0(&1)kx 2k/(2k%3)
' '4
k'0(&1)kx 2k%4/(2k%4) % i.'4
k'0(&1)kx 2k%3/(2k%3)
' m
x
0
y 3
1%y 2dy % i.m
x
0
y 2
1%y 2dy
(7.41)
where the last equality follows from
d
dx
'4
k'0(&1)kx 2k%4/(2k%4) ' '4
k'0(&1)kx 2k%3 ' x 3'4
k'0(&x 2)k '
x 3
1%x 2
(7.42)
for  |x| < 1,  and similarly

276
d
dx'4
k'0(&1)kx 2k%3/(2k%3) '
x 2
1%x 2 .
(7.43)
The theorem now follows from the easy inequalities
/00000
/00000
m
x
0
y 3
1%y 2dy # m
|x|
0
y 3dy ' 1
4|x|4 # |x|3/ 2
and
/00000
/00000
m
x
0
y 2
1%y 2dy # m
|x|
0
y 2dy ' 1
3
|x|3 # |x|3/ 2
which hold for |x| < 1.  Q.E.D.
The result of Lemma 7.1 plays a key-role in the proof of the following generic central
limit theorem:
Lemma 7.2: Let Xt, t = 1,2,...,n,..., be a sequence of random variables satisfying the following
four conditions:
plimn64max1#t#n|Xt|/ n ' 0,
(7.44)
plimn64(1/n)'n
t'1X 2
t ' σ2 0 (0,4),
(7.45)
limn64E (n
t'1(1%i.ξ.Xt/ n) ' 1, œξ 0ú,
(7.46)
and
supn$1E (n
t'1(1%ξ2X 2
t /n) < 4, œξ 0 ú.
(7.47)
Then 

277
1
nj
n
t'1
Xt 6d N(0,σ2).
(7.48)
Proof: Without loss of generality we may assume that F2 = 1, because if not we may
replace Xt by Xt /F.  It follows from the first part of Lemma 7.1 that 
exp iξ(1/ n)'n
t'1Xt ' k
n
t'1
1%iξXt/ n exp &(ξ2/2)(1/n)'n
t'1X 2
t exp 'n
t'1r ξXt / n
(7.49)
Condition (7.45) implies that 
plimn64exp&(ξ2/2)(1/n)'n
t'1X 2
t
' exp(&ξ2/2).
(7.50)
Moreover, it follows from (7.44), (7.45) and the inequality  
  for |x| <1  that
|r(x)| # |x|3
/
/
'n
t'1r ξXt / n I |ξXt/ n| < 1
#
|ξ|3
n n j
n
t'1
|Xj|3I |ξXt/ n | < 1
# |ξ|3 max1#t#n|Xt|
n
(1/n)'n
t'1X 2
t
6p 0,
(7.51)
Next, observe that
/
/
'n
t'1r ξXt / n I |ξXt/ n| $ 1
# 'n
t'1/
/
r ξXt / n I |ξXt/ n| $ 1
# I |ξ|.max1#t#n|Xt|/ n| $ 1 'n
t'1/
/
r ξXt / n
(7.52)
The latter and condition (7.44) imply that
P 'n
t'1r ξXt / n I |ξXt/ n| $ 1 ' 0 $ P |ξ|.max1#t#n|Xt|/ n < 1 6 1.
(7.53)
Therefore, it follows from (7.44) and (7.45) that

278
plimn64exp 'n
t'1r ξXt / n
' 1.
(7.54)
Thus we can write
exp iξ(1/ n)'n
t'1Xt ' k
n
t'1
1%iξXt/ n exp(&ξ2/2) % k
n
t'1
1%iξXt/ n Zn(ξ)
(7.55)
where
Zn(ξ) ' exp(&ξ2/2) & exp&(ξ2/2)(1/n)'n
t'1X 2
t exp 'n
t'1r ξXt / n
6p 0
(7.56)
Since   
 with probability 1 because
|Zn(ξ)| # 2
|exp(&x 2/2 % r(x))| # 1,
(7.57)
it follows from (7.56) and the dominated convergence theorem that 
limn64E[|Zn(ξ)|2] ' 0.
(7.58)
Moreover, condition (7.47) implies (using  
 and 
 ) that
z w ' ¯z. ¯w
|z| '
z ¯z
supn$1E /
/
(n
t'1(1%iξXt/ n)
2
' supn$1E (n
t'1(1%iξXt/ n)(1&iξXt/ n)
' supn$1E (n
t'1(1%ξ2X 2
t /n) < 4.
(7.59)
Therefore, it follows from the Cauchy-Schwarz inequality that  
/000
/000
limn64 E Zn(ξ) k
n
t'1
1%iξXt/ n
#
limn64E[|Zn(ξ)|2] supn$1E[ (n
t'1(1%ξ2X 2
t /n)]
' 0
(7.60)
Finally, it follows now from (7.46),  (7.55) and (7.60) that
limn64E[exp(iξ(1/ n)'n
t'1Xt)] ' exp(&ξ2/2).
(7.61)
Since the right-hand side of (7.61) is the characteristic function of the N(0,1) distribution, the

279
theorem follows for the case  F2 = 1. Q.E.D.
Lemma 7.2 is the basis for various central limit theorems for dependent processes. See for
example Davidson’s (1994) textbook. In the next section I will specialize Lemma 7.2  to
martingale difference processes.
7.5.3
Martingale difference central limit theorems
Note that Lemma 7.2 carries over if we replace the Xt’s  by a double array  Xn,t, t =1,2,...,n,
n = 1,2,3,..... In particular, let 
Yn,1 ' X1,
Yn,t ' XtI (1/n)'t&1
k'1X 2
k # σ2%1 for t $2.
(7.62)
Then by condition (7.45),
P[Yn,t … Xt for some t # n] # P[(1/n)'n
t'1X 2
t > σ2%1] 6 0
(7.63)
hence (7.48) holds if 
1
nj
n
t'1
Yn,t 6d N(0,σ2).
(7.64)
Therefore, it suffices to verify the conditions of Lemma 7.2 for (7.62). 
First, it follows straightforwardly from (7.63) that condition (7.45) implies
plimn64(1/n)'n
t'1Y 2
n,t ' σ2.
(7.65)
Moreover, if Xt is strictly stationary with an "!mixing base, and 
 then it
E[X 2
1 ] ' σ2 0 (0,4)
follows from Theorem 7.7 that  (7.45) holds, and so does (7.65).

280
Next, let us have a closer look at condition (7.44). It is not hard to verify that for arbitrary
g > 0,
P[max1#t#n|Xt|/ n > g] ' P[(1/n)'n
t'1X 2
t I(|Xt|/ n > g) > g2]
(7.66)
hence, (7.44) is equivalent to the condition that for arbitrary g > 0,
(1/n)'n
t'1X 2
t I(|Xt| > g n) 6p 0.
(7.67)
Note that (7.67) is true if Xt is strictly stationary, because then
E[(1/n)'n
t'1X 2
t I(|Xt| > g n)] ' E[X 2
1 I(|X1| > g n)] 6 0
(7.68)
Now  consider condition (7.47) for the  Yn,t’s.  Observe that
(n
t'1(1%ξ2Y 2
n,t/n) ' (n
t'1 1%ξ2X 2
t I (1/n)'t&1
k'1X 2
k # σ2%1 /n ' k
Jn
t'1
1%ξ2X 2
t /n ,
(7.69)
where 
Jn ' 1 % j
n
t'2
I (1/n)'t&1
k'1X 2
k # σ2%1 .
(7.70)
Hence
ln (n
t'1(1%ξ2Y 2
n,t/n) ' j
Jn&1
t'1
ln 1%ξ2X 2
t /n % ln 1%ξ2X 2
Jn/n
# ξ2 1
n j
Jn&1
t'1
X 2
t % ln 1%ξ2X 2
Jn/n # (σ2%1)ξ2% ln 1%ξ2X 2
Jn/n
(7.71)
where the last inequality follows (7.70).  Therefore,

281
sup
n$1
E (n
t'1(1%ξ2Y 2
n,t/n) # exp((σ2%1)ξ2) 1%ξ2sup
n$1
E[X 2
Jn]/n
# exp((σ2%1)ξ2) 1%ξ2supn$1 (1/n)'n
t'1E[X 2
t ] .
(7.72)
Thus (7.72) is finite if
supn$1(1/n)'n
t'1E[X 2
t ] < 4,
(7.73)
which in its turn is true if Xt is covariance stationary.  
Finally, it follows from the law of iterated expectations that for a martingale difference
process Xt,
E (n
t'1(1%iξXt/ n) ' E (n
t'1(1%iξE[Xt|öt&1]/ n) ' 1, œξ 0 ú,
(7.74)
and therefore also
E (n
t'1(1%iξYn,t/ n) ' E (n
t'1(1%iξE[Yn,t|öt&1]/ n) ' 1, œξ 0 ú
(7.75)
We can now specialize Lemma 7.2 for martingale difference processes:
Theorem 7.10: Let Xt 0 ú be a martingale difference process satisfying the following three
conditions:
(a)
 (1/n)'n
t'1X 2
t 6p σ2 0 (0,4);
(b)
For arbitrary g > 0, (1/n)'n
t'1X 2
t I(|Xt| > g n) 6p 0;
(c)
supn$1(1/n)'n
t'1E[X 2
t ] < 4.
Then (1/ n)'n
t'1Xt 6d N(0,σ2).

282
Moreover, it is not hard to verify that the conditions of Theorem 7.10 hold if the martingale
difference process Xt is strictly stationary with an "!mixing base, and  
 
E[X 2
1 ] ' σ2 0 (0,4):
Theorem 7.11: Let Xt, 0 ú be a strictly stationary martingale difference process with an
"!mixing base, satisfying  
 Then 
E[X 2
1 ] ' σ2 0 (0,4).
(1/ n)'n
t'1Xt 6d N(0,σ2).
7.6.
Exercises
1.
Let U and V  be independent standard normal random variables, and let for all integers t
and some nonrandom number 8 0 (0,B),  
 Prove that Xt is covariance
Xt ' U.cos(λt) % V.sin(λt).
stationary and deterministic.
2.
Show that the process Xt in problem 1 does not have a vanishing memory, but that
nevertheless  plimn64(1/n)'n
t'1Xt ' 0.
3.
Let Xt be a time series process satisfying 
and suppose that the events in the
E[|Xt|] < 4,
remote F!algebra 
 have either probability zero or one. Show
ö&4 ' _4
t'0σ(X&t,X&t&1,X&t&2,.......)
that P(E[Xt|ö&4] ' E[Xt]) ' 1.
4.
Prove (7.33) .
5.
Prove (7.34) by verifying the conditions on Theorem 7.8(b) for 
 = 
gt(θ)
(Yt & ft(θ))2,
with Yt defined by (7.21) and  
 by (7.29).
ft(θ)
6.
Verify the conditions of Theorem 7.9 for 
 = 
 with Yt defined by (7.21)
gt(θ)
(Yt & ft(θ))2,
and  
 by (7.29).
ft(θ)
7.
Prove (7.57).

283
8.
Prove (7.66).
Appendix 
7.A.
Hilbert spaces
7.A.1 Introduction
Loosely speaking, a Hilbert space is a space of elements for which similar properties hold
as for Euclidean spaces. We have seen in Appendix I that the Euclidean space  
 is a special
ún
case of a vector space, i.e., a space of elements endowed with two arithmetic operations:
addition, denoted by "+", and scalar multiplication, denoted by a dot. In particular, a space V is a
vector space if  for all x, y and z in V, and all scalars c, c1 and c2,
(a)
x + y = y + x;
(b)
x + (y + z) = (x + y) + z;
(c)
There is a unique zero vector 0 in V such that x + 0 = x;
(d)
For each x there exists a unique vector !x in V such that x + (!x) = 0;
(e)
1.x = x;
(f)
(c1c2).x = c1.(c2.x);
(g)
c.(x + y) = c.x + c.y;
(h)
(c1 + c2).x = c1.x + c2.x.
Scalars are real or complex numbers. If the scalar multiplication rules are confined to real

284
numbers, the vector space V is a real vector space. In the sequel I will only consider real vector
spaces. 
The inner product of two vectors x and y in 
 is defined by xTy. Denoting <x,y> =  xTy, it
ún
is trivial that this inner product obeys the rules in the more general definition of inner product: 
Definition 7.A.1: An inner product on a real vector space V is a  real function <x,y>  on V×V
such that for all x, y, z in V and all c in ú,
(1)
<x,y> = <y,x>
(2)
<cx,y> = c<x,y>
(3)
<x+y,z> = <x,z> + <y,z>
(4)
<x,x> > 0 when x … 0. 
A vector space endowed with an inner product is called an inner product space. Thus,  ún
is an inner product space. In 
 the norm of a vector x is defined by 
Therefore, the
ún
||x|| '
x Tx.
norm on a real inner product space is defined similarly as  
 Moreover, in
 the
||x|| '
<x,x>.
ún
distance between two vectors x and y is defined by  
 Therefore, the
||x&y|| '
(x&y)T(x&y).
distance between two vectors  x and y in a real inner product space is defined similarly as
 The latter is called a metric.
||x&y|| '
<x&y,x&y>.
An inner product space with associated norm and metric is called a pre-Hilbert space. The
reason for the "pre" is that still one crucial property of 
 is missing, namely that every Cauchy
ún
sequence in 
 has a limit in 
.
ún
ún

285
Definition 7.A.2: A sequence of elements xn of a inner product space with associated norm and
metric is called a Cauchy sequence if for every g > 0 there exists an n0 such that for all k,m $ n0,
||xk!xm|| < g.
Theorem 7.A.1: Every Cauchy sequence in 
  has a limit in the space involved.
úR, R < 4,
Proof: Consider first the case ú. Let 
 where xn is a Cauchy sequence. I
x ' limsupn64xn,
will show first that x < 4.
There exists a subsequence nk such that  
 Note that  
 is also a Cauchy
x ' limk64xnk.
xnk
sequence. For arbitrary  g > 0 there exists an index k0 such that  
 <  g if k,m  $ k0.
|xnk & xnm|
Keeping k fixed and letting m 6 4 it follows that  
 < g, hence  
 Similarly,
|xnk & x|
x < 4.
 Now we can find an index k0 and sub-sequences  nk and  nm such that for
x ' liminfn64xn > &4.
k,m  $ k0,   
 < g,   
 < g, and   
 < g, hence   
 < 3g. Since g is
|xnk & x|
|xnm & x|
|xnk & xnm|
|x & x|
arbitrary, we must have 
 Applying this argument to each component of a
x ' x ' limn64xn.
vector-valued Cauchy sequence the result for the case 
  follows. Q.E.D.
úR
In order for an inner product space to be a Hilbert space we have to require that the result
in Theorem 7.A1 carries over to the inner product space involved:
Definition 7.A.3: A Hilbert space H  is a vector space endowed with an inner product and
associated norm and metric, such that every Cauchy sequence in H has a limit in H. 

286
7.A.2 A Hilbert space of random variables
Let U0 be the vector space of zero-mean random variables with finite second moments
defined on a common probability space 
 endowed with the inner product <X,Y> =
{Ω,ö,P},
E[X.Y], norm 
 and metric ||X-Y||.  
||X|| '
E[X 2]
Theorem 7.A.2: The space  U0 defined above is a Hilbert space.
Proof: In order to show that U0  is a Hilbert space, we need to show that every Cauchy
sequence Xn ,n $ 1, has a limit in U0.  Since by Chebishev’s inequality, 
P[|Xn&Xm| > g] # E[(Xn&Xm)2]/g2 ' ||Xn&Xm||2/g2 6 0 as n,m 6 4
for every g > 0, it follows that 
 In Appendix 6.B of Chapter 6 we have
|Xn&Xm| 6p 0 as n,m 6 4.
seen that convergence in probability implies convergence a.s. along a subsequence. Therefore
there exists a subsequence nk such that 
 The latter implies that
|Xnk&Xnm| 6 0 a.s. as n,m 6 4.
there exists a null set N such that for every 
 is a Cauchy sequence in ú, hence
ω 0 Ω\N, Xnk(ω)
 exists for every 
 Now for every fixed m,
limk64Xnk(ω) ' X(ω)
ω 0 Ω\N.
 
 
(Xnk&Xm)2 6 (X&Xm)2 a.s. as k 6 4.
By Fatou’s lemma (see below) and the Cauchy property the latter implies that 
 
# 
 
||X&Xm||2 ' E[(X&Xm)2]
liminfk64E[(Xnk&Xm)2] 6 0 as m 6 4.
Moreover, it is easy to verify that 
Thus, every Cauchy sequence in 
E[X] ' 0 and E[X 2] < 4.
U0  has a limit in  U0 , hence  U0 is a Hilbert space. Q.E.D.

287
Lemma 7.A.1: (Fatou’s lemma). Let Xn ,n $ 1, be a sequence of non-negative random variables.
Then E[liminfn64Xn] # liminfn64E[Xn].
Proof: Put 
 and let n be a simple function satisfying
X ' liminfn64Xn
0 # n(x) # x.
Moreover, put 
 Then 
 because for arbitrary g > 0,
Yn ' min(n(X),Xn).
Yn 6p n(X)
P[|Yn & n(X)| > g] ' P[Xn < n(X)&g] # P[Xn < X&g] 6 0.
Since 
 because n is a simple function, and 
 it follows from 
E[n(X)] < 4
Yn # n(X),
Yn 6p n(X)
and the dominated convergence theorem that 
E[n(X)] ' limn64E[Yn] ' liminfn64E[Yn] # liminfn64E[Xn].
(7.76)
Taking the supremum over all simple functions n satisfying
 it follows now  from
0 # n(x) # x
(7.76) and the definition of E[X] that 
 Q.E.D.
E[X] # liminfn64E[Xn].
7.A.3 Projections
Similarly to the Hilbert space 
 two elements x and y in a Hilbert space H are said to be
ún,
orthogonal if <x,y> = 0, and orthonormal is in addition ||x|| = 1 and ||y|| = 1. Thus, in the Hilbert
space U0  two random variables are orthogonal if they are uncorrelated.
Definition 7.A.4: A linear manifold of a real Hilbert space H is a non-empty subset M of H such
that for each pair x, y in M and all real numbers " and $,  
The closure 
 of M
α.x % β.y 0 M.
M
is called a subspace of H. The subspace spanned by a subset C of H is the closure of the

288
intersection of all linear manifolds containing C.
In particular, if S is the subspace spanned by a countable infinite sequence 
  of 
x1,x2,x3,.....
vectors in H then each vector x in S takes the form 
 where the coefficients cn are
x ' '4
ncn.xn,
such that ||x|| < 4.
It is not hard to verify that a subspace of a Hilbert space is a Hilbert space itself. 
Definition 7.A.5: The projection of an element y in a Hilbert space H on a subspace S of H is an
element x of S such that ||y&x|| ' minz0S ||y&z||.
For example, if S is a subspace spanned by vectors 
 in H and 
 then the
x1,...,xk
y 0 H\S
projection of y on S is a vector  
 where the coefficients cj are chosen
x ' c1.x1%...%ck.xk 0 S
such that 
 is minimal. Of course, if 
 then the projection of y on S is y
||y & c1.x1&...&ck.xk||
y 0 S
itself.
Projections always exist and are unique: 
Theorem 7.A.3: (Projection theorem) If S is a subspace of a Hilbert space H and y is a vector in
H then there exists a unique vector x in S such that ||y!x|| = 
Moreover, the residual
minz0S ||y&z||.
vector u = y!x is orthogonal to any z in S.
Proof: Let  
 and  
 By the definition of infimum it is possible to
y 0 H\S
infz0S ||y&z|| ' δ.
select vectors xn in S such that 
The existence of the projection x of y on S
||y&xn|| # δ % 1/n.

289
then follows by showing that xn is a Cauchy sequence, as follows. Observe that
||xn&xm||2 ' ||(xn&y)&(xm&y)||2 ' ||xn&y||2 % ||xm&y||2 & 2<xn&y,xm&y>
and 
4||(xn%xm)/2&y||2 ' ||(xn&y)%(xm&y)||2 ' ||xn&y||2 % ||xm&y||2 % 2<xn&y,xm&y>.
Adding these two equations up yields
||xn&xm||2 ' 2||xn&y||2 % 2||xm&y||2 & 4||(xn%xm)/2&y||2
(7.77)
Since 
 it follows that 
 hence it follows from (7.77) that
(xn%xm)/2 0 S
||(xn%xm)/2&y||2 $ δ2,
||xn&xm||2 # 2||xn&y||2 % 2||xm&y||2 & 4δ2 # 4δ/n % 1/n 2 % 4δ/m % 1/m 2.
Thus xn is a Cauchy sequence in S, and since S is a Hilbert space itself,  xn has a limit x in S.
As to the orthogonality  of  u = y!x  with any vector z in S, note that for every real
number c and every z in S, x+c.z is a vector in S, so that 
δ2 # ||y&x&c.z||2 ' ||u&c.z||2 ' ||y&x||2% ||c.z||2&2<u,c.z> ' δ2%c 2||z||2&2c<u,z>.
(7.78)
Minimizing the right-hand side of (7.78) to c yields the solution 
 and
c0 ' <u,z>/||z||2,
substituting this solution in (7.78) yields the inequality 
 Thus <u,z> = 0.
(<u,z>)2/||z||2 # 0.
Finally, suppose that there exists another vector p in S such that 
 Then y!p is
||y&p|| ' δ.
orthogonal to any vector z in S: 
But x!p is a vector in S, so that 
<y&p,z> ' 0.
<y&p,x&p> ' 0
and 
 hence  
  Thus,
<y&x,x&p> ' 0,
0 ' <y&p,x&p> & <y&x,x&p> ' <x&p,x&p> ' ||x&p||2.

290
 Q.E.D.
p ' x.
7.A.5 Proof of the Wold decomposition
Let Xt be a zero-mean covariance stationary process, and denote 
Then the
E[X 2
t ] ' σ2.
Xt‘s are members of the Hilbert space  U0 defined in Section 7.A.2. Let  
 be the subspace
S t&1
&4
spanned by Xt!j, j $1, and let 
 be the projection of Xt on  
  Then 
 is
ˆXt
S t&1
&4 .
Ut ' Xt & ˆXt
orthogonal to all  Xt!j, j $1, i.e., 
 =  0 for j $1.  Since 
for j $1, the Ut ‘s are
E[UtXt&j]
Ut&j 0 S t&1
&4
also orthogonal to each other: 
 =  0 for j $1.  
E[UtUt&j]
Note that in general 
  takes the form 
 where the coefficients $t,j are
ˆXt
ˆXt ' '4
j'1βt,jXt&j,
such that  
 However, since Xt is covariance stationary the coefficients $t,j do
||Yt||2 ' E[Y 2
t ] < 4.
not depend on the time index t, because they are the solutions of the normal equations
γ(m) ' E[XtXt&m] ' '4
j'1βjE[Xt&jXt&m] ' '4
j'1βjγ(|j&m|), m ' 1,2,3,.......
Thus the projections  
 are covariance stationary, and so are the Ut ‘s because
ˆXt ' '4
j'1βjXt&j
σ2 ' ||Xt||2 ' ||Ut % ˆXt||2 ' ||Ut||2 % || ˆXt||2 % 2<Ut, ˆXt> ' ||Ut||2 % || ˆXt||2 ' E[U 2
t ] % E[ ˆX
2
t ],
so that E[U 2
t ] ' σ2
u # σ2.
Next, let 
 Then 
Zt,m ' 'm
j'1αjUt&j, where αj ' <Xt,Ut> ' E[XtUt&j].
||Xt&Zt,m||2 ' ||Xt&'m
j'1αjUt&j||2 ' E[X 2
t ] & 2'm
j'1αjE[XtUt&j] % 'm
i'1'm
j'1αiαjE[UiUj]
' E[X 2
t ] & 'm
j'1α2
j $ 0,
for all m $ 1, hence 
 The latter implies that 
 so that for
'4
j'1α2
j < 4.
'4
j'mα2
j 6 0 for m 6 4,

291
fixed t, 
 is a Cauchy sequence in 
, and  
 is a Cauchy sequence in 
Zt,m
S t&1
&4
Xt&Zt,m
S t
&4.
Consequently, 
  and 
 ! 
 0  
 exist.
Zt ' '4
j'1αjUt&j 0 S t&1
&4
Wt ' Xt
'4
j'1αjUt&j
S t
&4
As to the latter, it follows easily from (7.8) that  
 for every m, hence
Wt 0 S t&m
&4
Wt 0 _&4<t<4S t
&4.
(7.79)
Consequently,
 for all integers t and m. Moreover, it follows from (7.79) that the
E[Ut%mWt] ' 0
projection of Wt on any  
 is Wt  itself, hence Wt is perfectly predictable from any set
S t&m
&4
 of past values of Xt, as well as from any set 
of past values of Wt. 
{Xt&j, j $ 1}
{Wt&j, j $ 1}

292
Chapter 8
Maximum Likelihood Theory
8.1.
Introduction
Consider a random sample Z1,...,Zn  from a  k-variate distribution with density 
,
f(z*θ0)
where 
 is an unknown parameter vector, with 1 a given parameter space. As is
θ0 0 Θ d úm
well known, due to the independence of the Zj's, the joint density function of the random vector
 is the product of the marginal densities: 
.  The likelihood function
Z ' (Z T
1 ,....,Z T
n )T
(n
j'1f(zj*θ0)
in this case is defined as this joint density, with the non-random arguments zj replaced by the
corresponding random vectors Zj, and 
 by :
θ0
θ
ˆLn(θ) = (n
j'1f(Zj*θ).
(8.1)
The maximum likelihood (ML) estimator of 
 is now  
 or equivalently
θ0
ˆθ = argmaxθ0Θ ˆLn(θ),
ˆθ = argmaxθ0Θln ˆLn(θ) ,
(8.2)
where "argmax" stands for the argument for which the function involved takes its maximum
value.  
The ML estimation method is motivated by the fact that in this case
E[ln( ˆLn(θ))] # E[ln( ˆLn(θ0))].
(8.3)
To see this, note that ln(u) =   u!1 for u = 1, and ln(u) <  u!1 for 0 < u < 1 and u > 1. Therefore,

293
taking  
 it follows that for all 2,  
 # 
 ! 1, and
u ' f(Zj|θ)/f(Zj|θ0)
ln(f(Zj|θ)/f(Zj|θ0))
f(Zj|θ)/f(Zj|θ0)
taking expectations it follows now that
E[ln(f(Zj|θ)/f(Zj|θ0))] # E[f(Zj|θ)/f(Zj|θ0)] & 1
' m
úk
f(z*θ)
f(z*θ0)f(z*θ0)dz & 1 ' m{z0úk: f(z|θ0)>0} f(z*θ)dz & 1 # 0
Summing up, (8.3) follows.
This argument reveals that neither the independence assumption of the data Z =
 nor the absolute continuity assumption are necessary for (8.3). The only thing that
(Z T
1 ,....,Z T
n )T
matters is that 
E[ ˆLn(θ)/ ˆLn(θ0)] # 1
(8.4)
for all 2 0 1 and n $ 1. Moreover, if the support of Zj is not affected by the parameters in 20, i.e.,
if in the above case the set 
 is the same for all 2 0 1,  then the inequality in
{z 0 úm: f(z|θ) > 0}
(8.4) becomes an equality:
E[ ˆLn(θ)/ ˆLn(θ0)] ' 1
(8.5)
for all 2 0 1 and n $ 1. Equality (8.5) is the most common case in econometrics .
In order to show that absolute continuity is not essential for (8.3), suppose that  the Zj’s
are independent and identically discrete distributed with support =, i.e., for all 
 
z 0 Ξ,
 > 0 and 
 = 1. Moreover, let now 
 where  
is
P[Zj ' z]
'z0ΞP[Zj ' z]
f(z|θ0) ' P[Zj ' z],
f(z|θ)
the probability model involved. Of course,  
 should be specified such that 
f(z|θ)
'z0Ξf(z|θ) ' 1
for all 2 0 1.  For example, suppose that the  Zj’s are independent Poisson (20) distributed, so

294
that  
 and  = = {0,1,2,.....}. Then the likelihood function involved also takes the
f(z|θ) ' e &θθz/z!
form (8.1), and
E[f(Zj|θ)/f(Zj|θ0)] ' j
z0Ξ
f(z*θ)
f(z*θ0)f(z*θ0) ' j
z0Ξ
f(z*θ) ' 1,
hence (8.5) holds in this case as well, and therefore so does (8.3). 
In this and the previous case the likelihood function takes the form of a product.
However, also in the dependent case we can write the likelihood function as a product. For
example, let Z = 
  be absolutely continuously distributed with joint density
(Z T
1 ,....,Z T
n )T
 where the  Zj’s are no longer  independent. It is always possible to decompose a
fn(zn,......,z1|θ0),
joint density as a product of conditional densities and an initial marginal density. In particular,
denoting for t $ 2,   
 
ft(zt|zt&1,...,z1,θ) ' ft(zt,...,z1|θ)/ft&1(zt&1,...,z1|θ),
we can write 
fn(zn,...,z1|θ) ' f1(z1|θ)(n
t'2 ft(zt|zt&1,...,z1,θ).
 
Therefore, the likelihood function in this case can be written as
ˆLn(θ) ' fn(Zn,...,Z1|θ) ' f1(Z1|θ)(n
t'2 ft(Zt|Zt&1,...,Z1,θ).
(8.6)
It is easy to verify that in this case (8.5) holds also, and therefore so does (8.3). Moreover, it
follows straightforwardly from (8.6) and the above argument that in the time series case involved

295
P E
/00000
ˆLt(θ)/ ˆLt&1(θ)
ˆLt(θ0)/ ˆLt&1(θ0)
Zt&1,...,Z1 # 1
' 1 for t ' 2,3,...,n,
(8.7)
hence
P E
/
ln( ˆLt(θ)/ ˆLt&1(θ)) & ln( ˆLt(θ0)/ ˆLt&1(θ0)) Zt&1,...,Z1 # 0
' 1 for t ' 2,3,...,n.
(8.8)
Of course, these results hold in the independent case as well.
8.2.
Likelihood functions
There are quite a few cases in econometrics where the distribution of the data is neither
absolute continuous nor discrete. The Tobit model discussed below is such a case. In these cases
we cannot construct a likelihood function in the way I have done here, but still we can define a
likelihood function indirectly, using the properties (8.4) and (8.7):
Definition 8.1:  A sequence  
 of non-negative random functions on a parameter
ˆLn(θ), n $ 1,
space 1 is a sequence of likelihood functions if the following conditions hold:
(a)
There exists an increasing sequence 
 of  F-algebras such that for each 2 0 1
ön, n $ 0,
and n $ 1, 
 is measurable 
ˆLn(θ)
ön.
(b)
There exists a  
 such that for all  2 0 1,  
 and
θ0 0 Θ
P(E[L1(θ)/L1(θ0)*ö0] # 1) ' 1,
for n $ 2,
P E
/00000
ˆLn(θ)/ ˆLn&1(θ)
ˆLn(θ0)/ ˆLn&1(θ0)
ön&1 # 1
' 1.
(c)
For all  
  
 and for  n $ 2,
θ1 … θ2 in Θ, P[ ˆL1(θ1) ' ˆL1(θ2)*ö0] < 1,

296
 1 
P[ ˆLn(θ1)/ ˆLn&1(θ1) ' ˆLn(θ2)/ ˆLn&1(θ2)*ön&1] < 1.
The conditions in (c) exclude the case that 
 is constant on 1. Moreover, these conditions
ˆLn(θ)
also guarantee that 
 is unique:
θ0 0 Θ
Theorem 8.1: For all 2 0 1\{
} and n $ 1 , 
θ0
E[ln( ˆLn(θ)/ ˆLn(θ0))] < 0.
Proof: First, let n = 1. I have already established that  
! 1
ln( ˆL1(θ)/ ˆL1(θ0)) < ˆL1(θ)/ ˆL1(θ0)
if  
 Thus, denoting  
 and 
 =
ˆLn(θ)/ ˆLn(θ0) … 1.
Y(θ) ' ˆLn(θ)/ ˆLn(θ0) & ln( ˆLn(θ)/ ˆLn(θ0)) & 1
X(θ)
  we have 
 and 
 if and only if 
 Now suppose that
ˆLn(θ)/ ˆLn(θ0)
Y(θ) $ 0,
Y(θ) > 0
X(θ) … 1.
 Then 
 a.s.  because 
 hence
P(E[Y(θ)|ö0] ' 0) ' 1.
P[Y(θ) ' 0|ö0] ' 1
Y(θ) $ 0,
 a.s. Condition (c) in Definition 8.1 now excludes the possibility that
P[X(θ) ' 1|ö0] ' 1
 hence  
 if and only if  
 In turn this result
θ … θ0,
P(E[ln( ˆL1(θ)/ ˆL1(θ0))|ö0] < 0) ' 1
θ … θ0.
implies that
E[ln( ˆL1(θ)/ ˆL1(θ0))] < 0 if θ … θ0.
(8.9)
By a similar argument it follows that for n $ 2,
E[ln( ˆLn(θ)/ ˆLn&1(θ)) & ln( ˆLn(θ0)/ ˆLn&1(θ0))] < 0 if θ … θ0.
(8.10)
The theorem now follows from (8.9) and (8.10). Q.E.D.
As we have seen for the case (8.1), if the support {z: f(z|2) > 0}of f(z|θ) does not depend
on 2 then the inequalities in condition (b) becomes equalities, with 
 for  n $ 1,
ön ' σ(Zn,.....,Z1)
and  
 the trivial F-algebra. Therefore,
ö0

297
Definition 8.2: The sequence 
 of  likelihood functions has invariant support if  for
ˆLn(θ), n $ 1,
all  θ 0 Θ,  
 and for n $ 2,
P(E[ ˆL1(θ)/ ˆL1(θ0)*ö0] ' 1) ' 1,
P E
/00000
ˆLn(θ)/ ˆLn&1(θ)
ˆLn(θ0)/ ˆLn&1(θ0)
ön&1 ' 1
' 1.
As said before, this is the most common case in econometrics.
8.3.
Examples
8.3.1
The uniform distribution
Let 
be independent random drawings from the uniform [0,20] distribution,
Zj, j ' 1,...,n,
where 20 > 0.  The density function of Zj is 
 so that the likelihood
f(z|θ0) ' θ&1
0 I(0 # z # θ0),
function involved is:
ˆLn(θ) ' 1
θnk
n
j'1
I(0 # Zj # θ).
(8.11)
In this case 
 for  n $ 1, and  
 is the trivial F-algebra {S,i}. The conditions
ön ' σ(Zn,.....,Z1)
ö0
(b) in Definition 8.1 now read as
E[ ˆL1(θ)/ ˆL1(θ0)|ö0] ' E[ ˆL1(θ)/ ˆL1(θ0)|] ' min(θ,θ0)/θ # 1,
E
/00000
ˆLn(θ)/ ˆLn&1(θ)
ˆLn(θ0)/ ˆLn&1(θ0)
ön&1 ' E[ ˆL1(θ)/ ˆL1(θ0)|] ' min(θ,θ0)/θ # 1 for n $ 2.
Moreover, the conditions (c) in Definition 8.1 read as

298
P[θ&1
1 I(0 # Z1 # θ1) ' θ&1
2 I(0 # Z1 # θ2)] ' P(Z1 > max(θ1,θ2)) < 1 if θ1 … θ2.
Hence, Theorem 8.1 applies. Indeed, 
E[ln( ˆLn(θ)/ ˆLn(θ0))] ' nln(θ0/θ) % nE[ln(I(0 # Z1 # θ))] & E[ln(I(0 # Z1 # θ0))]
' nln(θ0/θ) % nE[ln(I(0 # Z1 # θ))] '
&4 if θ < θ,0
nln(θ0/θ) < 0 if θ > θ0,
0 if θ ' θ0.
8.3.2
Linear regression with normal errors
Let  
 be independent random vectors such that 
Zj ' (Yj,X T
j )T, j ' 1,...,n,
Yj ' α0 % βT
0Xj % Uj, Uj|Xj - N(0,σ2
0),
where the latter means that the conditional distribution of Uj given Xj is a normal N(0,σ2
0)
distribution. The conditional density of Yj given Xj is
f(y|θ0,Xj) '
exp[&½(y&α0&βT
0Xj)2/σ2
0]
σ0 2π
, where θ0 ' (α0,βT
0,σ2
0)T.
Next, suppose that the  Xj’s are absolutely continuously distributed with density g(x). Then the
likelihood function is

299
ˆLn(θ) ' (n
j'1f(Yj|θ,Xj) (n
j'1g(Xj) ' exp[&½'n
j'1(Yj&α&βTXj)2/σ2]
σn( 2π)n
(n
j'1g(Xj)
(8.12)
where 
 However, note that in this case the marginal distribution of  Xj does not
θ ' (α,βT,σ2)T.
matter for the ML estimator 
 because this distribution does not depend on the parameter
ˆθ,
vector
 More precisely, the functional form of the ML estimator  as function of the data is
θ0.
ˆθ
invariant to the marginal distributions of the Xj’s, although the asymptotic properties of the ML
estimator (implicitly) depend on the distributions of the Xj’s. Therefore, without loss of generality
we may ignore the distribution of the  Xj’s in (8.12) and work with the conditional likelihood
function:
ˆL
c
n (θ) ' k
n
j'1
f(Yj|θ,Xj) '
exp[&½'n
j'1(Yj&α&βTXj)2/σ2]
σn( 2π)n
, where θ ' (α,βT,σ2)T.
(8.13)
As to the F-algebras involved, we may take 
 and for n $1, 
ö0 ' σ({Xj}4
j'1)
 where w denotes the operation "take the smallest F-algebra containing the
ön ' σ({Yj}n
j'1)wö0,
two F-algebras involved". 2 The conditions (b) in Definition 8.1 then read  
E[ ˆL
c
1 (θ)/ ˆL
c
1 (θ0)|ö0] ' E[f(Y1|θ,X1)/f(Y1|θ0,X1)|X1] ' 1,
E
/00000
ˆL
c
n (θ)/ ˆL
c
n&1(θ)
ˆL
c
n (θ0)/ ˆL
c
n&1(θ0)
ön&1 ' E[f(Yn|θ,Xn)/f(Yn|θ0,Xn)|Xn] ' 1 for n $ 2.
Thus, Definition 8.2 applies. Moreover, it is easy to verify that the conditions (c) of  Definition
8.1 now read as 
 This is true, but  tedious to
P[f(Yn|θ1,Xn) ' f(Yn|θ2,Xn)|Xn] < 1 if θ1 … θ2.
verify.

300
8.3.3
Probit and Logit models
Again, let  
 be independent random vectors, but now Yj takes
Zj ' (Yj,X T
j )T, j ' 1,...,n,
only two values, 0 and 1, with conditional Bernoulli probabilities
P(Yj'1|θ0,Xj) ' F(α0%βT
0Xj), P(Yj'0|θ0,Xj) ' 1 & F(α0%βT
0Xj),
(8.14)
where F is a given distribution function and 
 For example, let the sample be a
θ0 ' (α0,βT
0)T.
survey of households, where Yj indicates home ownership, and Xj is a vector of household
characteristics such as marital status, number of children living at home, and income.  
If F is the logistic distribution function, 
 then model (8.14) is
F(x) ' 1/[1%exp(&x)],
called the Logit model, and if F is the distribution function of the standard normal distribution
then  model (8.14) is called the Probit model. 
In this case the conditional likelihood function is
ˆL
c
n (θ) ' k
n
j'1
[YjF(α%βTXj) % (1&Yj)(1 & F(α%βTXj))], where θ ' (α,βT)T.
(8.15)
Also in this case the marginal distribution of  Xj does not affect the functional form of the ML
estimator as function of the data. 
The  F-algebras involved are the same as in the regression case, namely ö0 ' σ({Xj}4
j'1)
and for n $1,  
 Moreover, note that 
ön ' σ({Yj}n
j'1)wö0.
E[ ˆL
c
1 (θ)/ ˆL
c
1 (θ0)|ö0] ' '1
y'0[yF(α%βTX1) % (1&y)(1 & F(α%βTX1))] ' 1,
and similarly
E
/00000
ˆL
c
n (θ)/ ˆL
c
n&1(θ)
ˆL
c
n (θ0)/ ˆL
c
n&1(θ0)
ön&1 ' '1
y'0[yF(α%βTXn) % (1&y)(1 & F(α%βTXn))] ' 1,

301
hence the conditions (b) of Definition 8.1 and the conditions of Definition 8.2 apply. Also the
conditions (c) in Definition 8.1 apply, but again it is rather tedious to verify this.
8.3.4
The Tobit model
Let  
 be independent random vectors such that 
Zj ' (Yj,X T
j )T, j ' 1,...,n,
Yj ' max(Y (
j ,0), where Y (
j ' α0 % βT
0Xj % Uj with Uj|Xj - N(0,σ2
0).
(8.16)
The random variables 
 are only observed if they are positive. Note that 
Y (
j
P[Yj'0|Xj] ' P[α0 % βT
0Xj % Uj # 0|Xj] ' P[Uj > α0 % βT
0Xj|Xj]
' 1 & Φ (α0 % βT
0Xj)/σ0 , where Φ(x) ' m
x
&4exp(&u 2/2)/ 2π du.
This is a Probit model. Since model (8.16) was proposed by Tobin (1958) and involves a Probit
model for the case Yj = 0 it is called the Tobit model. For example, let the sample be a survey of
households, where  Yj is the amount of money household j spends on tobacco products, and Xj is
a vector of household characteristics. But there are  households where nobody smokes, so that for
these households  Yj = 0.  
In this case the setup of the conditional likelihood function is not as straightforward as in
the previous examples, because the conditional distribution of Yj given Xj is neither absolutely
continuous nor discrete. Therefore, in this case it is easier to derive the likelihood function
indirectly from Definition 8.1, as follows.
First note that the conditional distribution function of Yj given Xj  and Yj > 0 is

302
P[Yj # y|Xj,Yj>0] ' P[0 < Yj # y|Xj]
P[Yj > 0|Xj]
'
P[&α0 & βT
0Xj < Uj # y & α0 & βT
0Xj|Xj]
P[Yj > 0|Xj]
' Φ (y & α0 & βT
0Xj)/σ0 & Φ(&α0 & βT
0Xj)/σ0
Φ (α0 % βT
0Xj)/σ0
I(y > 0),
hence the conditional density function of Yj given Xj  and Yj > 0 is
h(y|θ0,Xj,Yj>0) '
n(y & α0 & βT
0Xj)/σ0
σ0Φ (α0 % βT
0Xj)/σ0
I(y > 0), where n(x) ' exp(&x 2/2)
2π
.
Next, observe that for any Borel measurable function g of (Yj,Xj) such that E[|g(Yj,Xj)|] < 4 we
have
E[g(Yj,Xj)|Xj] ' g(0,Xj)P[Yj ' 0|Xj] % E[g(Yj,Xj)I(Yj > 0)|Xj]
' g(0,Xj)P[Yj ' 0|Xj] % EE[g(Yj,Xj)|Xj,Yj > 0)|Xj]I(Yj > 0)|Xj
' g(0,Xj) 1 & Φ (α0 % βT
0Xj)/σ0
% E m
4
0 g(y,Xj)h(y|θ0,Xj,Yj>0)dy.I(Yj > 0)|Xj
(8.17)
' g(0,Xj) 1 & Φ (α0 % βT
0Xj)/σ0
% m
4
0 g(y,Xj)h(y|θ0,Xj,Yj>0)dy.Φ(α0 % βT
0Xj)/σ0
' g(0,Xj) 1 & Φ (α0 % βT
0Xj)/σ0
% 1
σ0m
4
0 g(y,Xj)n(y & α0 & βT
0Xj)/σ0 dy.
Hence, choosing
g(Yj,Xj) '
1 & Φ (α % βTXj)/σ I(Yj ' 0) % σ&1n (Yj & α & βTXj)/σ I(Yj > 0)
1 & Φ (α0 % βT
0Xj)/σ0 I(Yj ' 0) % σ&1
0 nYj & α0 & βT
0Xj)/σ0 I(Yj > 0)
(8.18)
it follows from (8.17) that 

303
E[g(Yj,Xj)|Xj] ' 1 & Φ (α % βTXj)/σ % 1
σm
4
0 n (y & α & βTXj)/σ dy
' 1 & Φ (α % βTXj)/σ % 1 & Φ (&α & βTXj)/σ ' 1.
(8.19)
In view of Definition 8.1,  (8.18) and  (8.19) suggest to define the conditional likelihood function
of the Tobit model as
ˆL
c
n (θ) ' k
n
j'1
1 & Φ (α % βTXj)/σ I(Yj ' 0) % σ&1n (Yj & α & βTXj)/σ I(Yj > 0) .
The conditions (b) in Definition 8.1 now follow from (8.19), with the  F-algebras involved
defined similar as in the regression case. Moreover, also the conditions (c) apply.
 
Note that
E[Yj|Xj,Yj>0] ' α0 % βT
0Xj %
σ0n(α0 % βT
0Xj)/σ0
Φ(α0 % βT
0Xj)/σ0
.
(8.20)
Therefore, if one would estimate a linear regression model using the observations with Yj > 0
only, the OLS estimates will be inconsistent, due to the last term in (8.20). 
8.4.
Asymptotic properties of ML estimators
8.4.1
Introduction
Without the conditions (c) in Definition 8.1 the solution θ0 = argmaxθ0ΘE[ln( ˆLn(θ))]
may not be unique. For example, if Zj  = cos(Xj+20) with the Xj ‘s independent absolutely
continuously distributed  random variables with common density, then the density function  
f(z|20) of Zj satisfies  f(z|20) = f(z|20+2sB) for all integers s. Therefore, the parameter space 1 has
to be chosen small enough to make 20 unique. 

304
Also, the first and second-order conditions for a maximum of  
 at 
 
E[ln( ˆLn(θ))]
θ ' θ0
may not be satisfied. The latter is for example the case for the likelihood function (8.11): if 
 then 
 and if  
 then  
 so that the left
θ < θ0
E[ln( ˆLn(θ))] ' &4,
θ $ θ0
E[ln( ˆLn(θ))] ' &n.ln(θ),
derivative of   
 in 
  is   
 = 4,  and
E[ln( ˆLn(θ))]
θ ' θ0
limδ90(E[ln( ˆLn(θ0))] & E[ln( ˆLn(θ0&δ))])/δ
the right-derivative  is  
  =  
 Since the first and
limδ90(E[ln( ˆLn(θ0%δ))] & E[ln( ˆLn(θ0))])/δ
&n/θ0.
second-order conditions play a crucial role in deriving the asymptotic normality and efficiency of
the ML estimator (see below), the rest of this chapter does not apply to the case (8.11). 
8.4.2
First and second-order conditions
The following conditions guarantee that the first and second-order conditions for a
maximum hold.
Assumption 8.1: The parameter space 1 is convex, and 20 is an interior point of  1. The
likelihood function 
 is, with probability 1, twice continuously differentiable in an open
ˆLn(θ)
neighborhood  
 and for  
Θ0 of θ0,
i1,i2 ' 1,2,3,...,m,
E sup
θ0Θ0
/00000
/00000
M2 ˆLn(θ)
Mθi1Mθi2
< 4
(8.21)
and 
E sup
θ0Θ0
/00000
/00000
M2ln ˆLn(θ)
Mθi1Mθi2
< 4.
(8.22)

305
Theorem 8.2: Under Assumption 8.1,
 and  
E Mln( ˆLn(θ))
MθT
*θ'θ0
= 0
E M2ln( ˆLn(θ))
MθMθT
*θ'θ0
= &Var Mln( ˆLn(θ))
MθT
*θ=θ0 .
Proof: For notational convenience I will prove this theorem for the univariate parameter
case m = 1 only. Moreover, I will focus on the case that  
 is a random sample
Z ' (Z T
1 ,....,Z T
n )T
from an absolutely continuous distribution with density f(z|20).  
Observe that
E ln( ˆLn(θ))/n = 1
nj
n
j'1
E ln(f(Zj*θ)) = mln(f(z*θ))f(z*θ0)dz,
(8.23)
It follows from Taylor’s theorem that for 
 and  every * … 0 for which 
 there
θ 0 Θ0
θ%δ 0 Θ0
exists a 8(z,*) 0 [0,1] such that 
ln(f(z*θ%δ)) & ln(f(z*θ)) ' δ dln(f(z*θ))
dθ
% 1
2δ2 d 2ln(f(z*θ%λ(z,δ)δ))
(d(θ%λ(z,δ)δ))2
.
(8.24)
Note that by the convexity of  
 Therefore, it follows from condition
Θ, θ0 % λ(z,δ)δ 0 Θ.
(8.22), the definition of a derivative, and the dominated convergence theorem that
d
dθmln(f(z*θ))f(z*θ0)dz ' m
dln(f(z*θ))
dθ
f(z*θ0)dz
(8.25)
Similarly, it follows from condition (8.21) , Taylor’s theorem and the dominated convergence
theorem that 

306
m
df(z*θ)
dθ
dz '
d
dθmf(z*θ)dz '
d
dθ1 ' 0.
(8.26)
Moreover, 
m
dln(f(z*θ))
dθ
f(z*θ0)dz *θ'θ0 = m
df(z*θ)/dθ
f(z*θ)
f(z*θ0)dz *θ=θ0 ' m
df(z*θ)
dθ
dz *θ=θ0
(8.27)
The first part of theorem now follows from (8.23) through (8.27).
Similarly to (8.25) and (8.26) it follows from the mean value theorem and the conditions 
(8.21) and  (8.22) that
d 2
(dθ)2mln(f(z*θ))f(z*θ0)dz ' m
d 2ln(f(z*θ))
(dθ)2
f(z*θ0)dz
(8.28)
and
m
d 2f(z*θ)
(dθ)2 dz '
d
(dθ)2mf(z*θ)dz ' 0.
(8.29)
The second part of the theorem follows now from (8.28), (8.29) and
m
d 2ln f(z*θ)
(dθ)2
f(z*θ0)dz *θ'θ0 = m
d 2f(z*θ)
(dθ)2
f(z*θ0)
f(z*θ)
dz *θ=θ0 & m
df(z*θ)/dθ
f(z*θ)
2
f(z*θ0)dz *θ=θ0
' m
d 2f(z*θ)
(dθ)2 dz *θ=θ0 & m dln f(z*θ) /dθ 2f(z*θ0)dz *θ=θ0.
The adaptation of the proof to the general case is pretty straightforward and is therefore left as an
exercise. Q.E.D.
The matrix 

307
Hn = VarMln( ˆLn(θ))/MθT *θ=θ0 .
(8.30)
is called the Fisher information matrix.  As we have seen in Chapter 5, the inverse of the Fisher
information matrix is just the Cramer-Rao lower bound of  the variance matrix of an unbiased
estimator of θ0.
8.4.3
Generic conditions for consistency and asymptotic normality
The ML estimator is a special case of an M-estimator. In Chapter 6 I have derived generic
conditions for consistency and asymptotic normality of M-estimators, which in most cases apply
to ML estimators as well. The case (8.11) is one of the exceptions, though. In particular, if 
Assumption 8.2:   
 and
plimn64supθ0Θ|ln( ˆLn(θ)/ ˆLn(θ0)) & E[ln( ˆLn(θ)/ ˆLn(θ0))]| ' 0
 where 
 is a continuous function in θ0
limn64supθ0Θ|E[ln( ˆLn(θ)/ ˆLn(θ0))] & R(θ|θ0)| ' 0,
R(θ|θ0)
such that for arbitrary * > 0, supθ0Θ: ||θ&θ0||$δ R(θ|θ0) < 0,
then the ML estimator is consistent:
Theorem 8.3: Under Assumption 8.2, 
 =  
plimn64ˆθ
θ0.
The conditions in Assumption 8.2 need to be verified on a case-by-case basis. In
particular, the uniform convergence in probability condition has to be verified  from the

308
conditions of the uniform weak law of large numbers. The last condition in Assumption 8.2, i.e., 
 follows easily from Theorem 8.1 and the continuity of  
. 
supθ0Θ: ||θ&θ0||$δ R(θ|θ0) < 0,
R(θ|θ0)
Some of the conditions for asymptotic normality of  the ML estimator are already listed in
Assumption 8.1, in particular the convexity of the parameter space 1, and the condition that 20 is
an interior point of 1. The other (high-level) conditions are:
Assumption 8.3: For  i1,i2 ' 1,2,3,...,m,
plimn64 sup
θ0Θ
/00000
/00000
M2ln ˆLn(θ) /n
Mθi1Mθi2
& E
M2ln ˆLn(θ) /n
Mθi1Mθi2
' 0
(8.31)
and
limn64 sup
θ0Θ
/00000
/00000
E
M2ln ˆLn(θ) /n
Mθi1Mθi2
% hi1,i2(θ) ' 0,
(8.32)
where 
 is continuous in 20. Moreover, the m×m matrix 
 with elements 
 is non-
hi1,i2(θ)
H
hi1,i2(θ0)
singular. Furthermore, 
Mln( ˆLn(θ0))/ n
MθT
0
6d Nm[0,H].
(8.33)
Note that the matrix 
  is just the limit of Hn /n, with Hn the Fisher information matrix
H
(8.30). Condition (8.31) can be verified from the uniform weak law of large numbers. Condition
(8.32) is a regularity condition which accommodates data-heterogeneity. In quite a few cases we
may take 
  =  
 Finally, condition (8.33) can be verified from
hi1,i2(θ)
&n &1E[M2ln( ˆLn(θ))/(Mθi1Mθi2)].

309
the central limit theorem. 
Theorem 8.4: Under Assumptions 8.1-8.3, n(ˆθ - θ0) 6d Nm[0,H &1].
Proof:  It follows from the mean value theorem (see Appendix II) that for each  i  0
{1,...,m} there exists a  
 such that
ˆλi 0 [0,1]
Mln( ˆLn(θ))/ n
Mθi
*θ'ˆθ = Mln( ˆLn(θ))/ n
Mθi
*θ'θ0
%
M2ln( ˆL(θ))/n
MθMθi
*θ'θ0%ˆλi(ˆθ&θ0)
n(ˆθ - θ0),
(8.34)
The first-order condition for (8.2) and the condition that 20 is an interior point of 1 imply
plimn64n &1/2Mln( ˆLn(θ))/Mθi|θ'ˆθ ' 0.
(8.35)
Moreover, the convexity of 1 guarantees that the mean value 
 is contained in 1. It
θ0%ˆλi(ˆθ&θ0)
follows now from the consistency of   and the conditions (8.31) and (8.32) that
ˆθ
˜H '
M2ln( ˆLn(θ))/n
MθMθ1
*θ'θ0%ˆλ1(ˆθ&θ0)
!
M2ln( ˆLn(θ))/n
MθMθm
*θ'θ0%ˆλm(ˆθ&θ0)
6p H.
(8.36)
The condition that 
 is nonsingular allows us to conclude from (8.36) and Slutsky’s theorem
H
that
plimn64 ˜H
&1 ' H &1,
(8.37)

310
hence it follows from (8.34) and (8.35) that
n(ˆθ & θ0) ' & ˜H
&1(Mln( ˆLn(θ0))/MθT
0)/ n + op(1).
(8.38)
Theorem 8.4 follows now from condition (8.33) and the results  (8.37) and (8.38).  Q.E.D.
In the case of a random sample  Z1,...,Zn  the asymptotic normality condition (8.33) can
easily be derived from the central limit theorem for i.i.d. random variables. For example, let
again the Zj’s be  k-variate distributed with density 
. Then it follows from Theorem 8.2
f(z*θ0)
that under Assumption 8.1, 
E[Mln(f(Zj*θ0))/MθT
0] ' n &1E[Mln( ˆLn(θ0))/MθT
0 ] ' 0
and
Var[Mln(f(Zj*θ0))/MθT
0] ' n &1Var[Mln( ˆLn(θ0))/MθT
0 ] ' H,
say, so that (8.33) straightforwardly follows from the central limit theorem for i.i.d. random
vectors. 
8.4.4
Asymptotic normality in the time series case
In the time series case (8.6) we have
Mln( ˆLn(θ0))/MθT
0
n
'
1
nj
n
t'1
Ut,
(8.39)
where 
U1 ' Mln(f1(Z1|θ0))/MθT
0, Ut ' Mln(ft(Zt|Zt&1,...,Z1,θ0))/MθT
0 for t $ 2.
(8.40)
The process Ut  is a martingale difference process (see Chapter 7):  Denoting for t $ 1,

311
 and letting 
be the trivial F-algebra {S,i}, it is easy to verify that  for t $ 1, 
öt ' σ(Z1,...,Zt),
ö0
 a.s.  Therefore, condition (8.33) can in principle be derived from the conditions
E[Ut|öt&1] ' 0
of the martingale difference central limit theorems (Theorems 7.10-7.11)  in Chapter 7.
Note that even if Zt is a strictly stationary process, the Ut’s may not be strictly stationary.
In that case condition  (8.33) can be proved by specializing Theorem 7.10 in Chapter 7.
An example where condition  (8.33) follows from Theorem 7.11 in Chapter 7 is the Auto-
Regressive (AR) model of order 1:
Zt ' α % βZt&1 % gt, where gt is i.i.d. N(0,σ2) and |β| < 1.
(8.41)
The condition |$| < 1 is necessary for strict stationarity of Zt. Then for t $ 2 the conditional
distribution of Zt given 
 is  
 so that, with  
 = 
 
öt&1 ' σ(Z1,...,Zt&1)
N(α % βZt&1,σ2),
θ0
(α,β,σ2)T,
(8.40) becomes
Ut '
M &½(Zt&α&βZt&1)2/σ2 & ½ln(σ2) & ln( 2π)
M(α,β,σ2)
' 1
σ2
gt
gtZt&1
½(g2
t /σ2 & 1)
.
(8.42)
Since the 
‘s are i.i.d.  
 and 
 and 
 are mutually independent it follows that  (8.42)
gt
N(0,σ2)
gt
Zt&1
is a martingale difference process, not only with respect to 
 but also with respect
öt ' σ(Z1,...,Zt)
to 
 = 0 a.s. 
ö t
&4 ' σ({Zt&j}4
j'0), i.e., E[Ut|ö t&1
&4 ]
By backwards substitution of (8.41) it follows that 
 so that the
Zt ' '4
j'0βj(α%gt&j)
marginal distribution of Z1 is 
  However, there is no need to derive U1 in
N[α/(1&β),σ2/(1&β2)].
this case, because this term is irrelevant for the asymptotic normality of  (8.39). Therefore, the
asymptotic normality of  (8.39) in this case follows straightforwardly from the stationary
martingale difference central limit theorem, with asymptotic variance matrix

312
H ' Var(Ut) ' 1
σ2
1
α
1&β
0
α
1&β
α2
(1&β)2% σ2
1&β2
0
0
0
1
2σ2
.
 
8.4.5
Asymptotic efficiency of the ML estimator
As said before, the ML estimation approach is a special case of the M-estimation
approach discussed in Chapter 6. However, the position of the ML estimator among the M-
estimators is a special one, namely the ML estimator is (under some conditions) asymptotically
efficient.
In order to explain and prove asymptotic efficiency, let
˜θ ' argmaxθ0Θ(1/n)'n
j'1g(Zj,θ)
(8.43)
be an M-estimator of
θ0 ' argmaxθ0ΘE[g(Z1,θ)],
(8.44)
where again  Z1,...,Zn is a random sample from a  k-variate absolutely continuous distribution
with density  
 and  
 is the parameter space. In Chapter 6 I have set forth
f(z*θ0),
Θ d úm
conditions such that 
n(˜θ&θ0) 6d Nm[0,A &1BA &1],
(8.45)
where

313
A ' E
M2g(Z1,θ0)
Mθ0MθT
0
' múk
M2g(z,θ0)
Mθ0MθT
0
f(z|θ0)dz
(8.46)
and
B ' E Mg(Z1,θ0)/MθT
0 Mg(Z1,θ0)/Mθ0
' múkMg(z,θ0)/MθT
0 Mg(z,θ0)/Mθ0 f(z|θ0)dz
(8.47)
As will be shown below, the matrix 
 is positive semi-definite, hence the
A &1BA &1 & H &1
asymptotic variance matrix of  is "larger" (or at least not smaller) than the asymptotic variance
˜θ
matrix 
 of the ML estimator 
 In other words, the ML estimator is an asymptotically
H &1
ˆθ.
efficient M-estimator. 
This proposition can be motivated as follows. Under some regularity conditions, similarly
to Assumption 8.1, it follows from the first-order condition for (8.44) that
múkMg(z,θ0)/MθT
0 f(z|θ0)dz ' múkME[g(z,θ)]/MθT f(z|θ0)dz|θ'θ0 ' 0
(8.48)
Since equality (8.48) does not depend on the value of 20 it follows that for all 2,
múkMg(z,θ)/MθT f(z|θ)dz ' 0.
(8.49)
Taking derivatives inside and outside the integral (8.49) again yield:
múk
M2g(z,θ)
MθMθT f(z|θ)dz % múkMg(z,θ)/MθT Mf(z|θ)/Mθ dz
' múk
M2g(z,θ)
MθMθT f(z|θ)dz % múkMg(z,θ)/MθT Mln(f(z|θ))/Mθ f(z|θ)dz ' O.
(8.50)
Replacing 2 by 20 it follows from (8.46) and  (8.50) that
E
Mg(Z1,θ0)
MθT
0
Mln(f(Z1|θ0))
Mθ0
' &A.
(8.51)
Since the two vectors in (8.51) have zero expectation, (8.51) also reads as

314
Cov
Mg(Z1,θ0)
MθT
0
, Mln(f(Z1|θ0))
MθT
0
' &A.
(8.52)
It follows now from (8.47),  (8.52) and Assumption 8.3 that
Var
Mg(Z1,θ0)/MθT
0
Mln(f(Z1|θ0))/MθT
0
'
B
&A
&A
H
,
which of course is positive semi-definite, and therefore so is 
A &1,H &1
B
&A
&A
H
A &1
H &1
' A &1BA &1 & H &1.
Note that this argument does not hinge on the independence and absolute continuity
assumptions made here.  We only need that (8.45) holds for some positive definite matrices A
and B, and that
1
n
'n
j'1Mg(Zj,θ0)/MθT
0
Mln( ˆLn(θ0))/MθT
0
6d N2m
0
0
,
B
&A
&A
H
.
8.5.
Testing parameter restrictions
8.5.1
The pseudo t test and the Wald test
In view of Theorem 8.2 and Assumption 8.3 the matrix  
 can be estimated consistently
H
by the matrix 
 in (8.53):
ˆH

315
ˆH ' &
/0000
M2ln( ˆLn(θ))/n
MθMθT
θ'ˆθ
6p H.
(8.53)
Denoting by ei the i-th column of the unit matrix Im, it follows now from  (8.53),  Theorem 8.4
and the results in Chapter 6 that: 
Theorem 8.5: (Pseudo t-test) Under Assumptions 8.1-8.3,  
 N(0,1) if 
ˆti =
ne T
i ˆθ/ e T
i ˆH
&1ei 6d
e T
i θ0 = 0.
Thus the null hypothesis 
,  which amounts to the hypothesis that the i-th
H0: e T
i θ0 = 0
component of 
 is zero, can now be tested by the pseudo t-value   in the same way as for M-
θ0
ˆti
estimators.
Next, consider the partition 
θ0 =
θ1,0
θ2,0
, θ1,0 0 úm&r, θ2,0 0 úr,
(8.54)
and suppose that we want to test the null hypothesis 
. This hypothesis corresponds to the
θ2,0 ' 0
linear restriction 
, where R = (O,Ir). It follows from Theorem 8.4 that under this null
Rθ0 = 0
hypothesis
nRˆθ 6d Nr(0,R ¯H
&1R T).
(8.55)
Partitioning 
 conformably to (8.54) as
ˆθ, ˆH
&1 and H &1
ˆθ =
ˆθ1
ˆθ2
,
ˆH
&1 =
ˆH
(1,1)
ˆH
(1,2)
ˆH
(2,1)
ˆH
(2,2) , ¯H
&1 =
¯H
(1,1)
¯H
(1,2)
¯H
(2,1)
¯H
(2,2) ,
(8.56)

316
it follows that 
, hence it follows from (8.55)
ˆθ2 = Rˆθ, ˆH
(2,2) = R ˆH
&1R T, and H (2,2) = RH &1R T
that 
 and consequently:
ˆH
(2,2) &1/2 nˆθ2 6d Nr(0,Ir)
Theorem 8.6: (Wald test) Under Assumptions 8.1-8.3,  
 = 0.
nˆθ
T
2 ˆH
(2,2) &1ˆθ2 6d χ2
r
if θ2,0
8.5.2
The Likelihood Ratio test
An alternative to the Wald test is the Likelihood Ratio (LR) test, which is based on the
ratio
ˆλ =
maxθ0Θ: θ2'0 ˆLn(θ)
maxθ0Θ ˆLn(θ)
=
ˆLn(˜θ)
ˆLn(ˆθ)
,
where  is partitioned conformably to (8.54) as
θ
θ =
θ1
θ2
.
and
˜θ =
˜θ1
˜θ2
=
˜θ1
0
= argmax
θ0Θ: θ2'0
ˆLn(θ),
(8.57)
is the restricted ML estimator. Note that  is always between zero and one. The intuition behind
ˆλ
the LR test is that if 
 = 0 then  will approach 1 (in probability) as n 64 because then both
θ2,0
ˆλ
the unrestricted ML estimator   and the restricted ML estimator  are consistent .  In particular:
ˆθ
˜θ

317
Theorem 8.7: (LR test) Under Assumptions 8.1-8.3, 
 = 0.
&2ln(ˆλ) 6d χ2
r if θ2,0
Proof: Similarly to (8.38) we have
n(˜θ1 - θ1,0) = &H &1
1,1
Mln( ˆLn(θ))/ n
MθT
1
*θ'θ0
+ op(1),
where 
 is the upper-left 
 block of 
H 1,1
(m&r)×(m&r)
H:
H '
H1,1 H1,2
H2,1 H2,2
,
and consequently,
n(˜θ - θ0) = &
H &1
1,1 O
O
O
Mln( ˆLn(θ0))/ n
MθT
0
+ op(1).
(8.58)
Subtracting (8.58) from (8.34) and using condition (8.33)  yield
n(ˆθ - ˜θ) = &
¯H
&1 -
¯H
&1
1,1 O
O
O
Mln( ˆLn(θ0))/ n
MθT
0
+ op(1) 6d Nm(0,∆),
(8.59)
where
∆=
¯H
&1 -
¯H
&1
1,1 O
O
O
¯H
¯H
&1 -
¯H
&1
1,1 O
O
O
= ¯H
&1 -
¯H
&1
1,1 O
O
O
.
(8.60)
The last equality in (8.60)  follows straightforwardly from the partition (8.56).
Next, it follows from the second-order Taylor expansion around the  unrestricted ML

318
estimator   that for some 
ˆθ
ˆη 0 [0,1],
ln(ˆλ) = ln ˆLn(˜θ) - ln( ˆLn(ˆθ)) = (˜θ-ˆθ)T Mln ˆLn(θ)
MθT
*θ=ˆθ
+ 1
2
n(˜θ-ˆθ)T M2ln( ˆLn(θ))/n
MθMθT
*θ=ˆθ%ˆη(˜θ&ˆθ)
n(˜θ-ˆθ) = - 1
2
n(˜θ-ˆθ)T H n(˜θ-ˆθ) + op(1),
(8.61)
where the last equality in (8.61)  follows from the fact that similarly to (8.36),
/000
M2ln( ˆL(θ))/n
MθMθT
θ'ˆθ%ˆη(˜θ&ˆθ)
6p &H.
(8.62)
Thus we have
&2ln(ˆλ) = ∆&1/2 n(ˆθ-˜θ)
T∆1/2 ¯H∆1/2 ∆&1/2 n(ˆθ-˜θ) + op(1).
(8.63)
Since by (8.59), 
 Nm(0,Im) is distr., and by (8.60) the matrix 
 is
∆&1/2 n(ˆθ&˜θ) 6
∆1/2H∆1/2
idempotent with rank(
) = trace(
) = r, the theorem follows from the results in
∆1/2H∆1/2
∆1/2H∆1/2
Chapters 5 and 6. Q.E.D.
8.5.3
The Lagrange Multiplier test
The restricted ML estimator  can also be obtained from the first-order conditions of the
˜θ
Lagrange function  
 where 
 is a vector of Lagrange
‹(θ,µ) = ln( ˆLn(θ)) - θT
2µ ,
µ 0 úr
multipliers. These first-order conditions are:

319
M‹(θ,µ)/MθT
1 *θ'˜θ, µ'˜µ ' Mln ˆL(θ) /MθT
1 *θ'˜θ ' 0,
M‹(θ,µ)/MθT
2 *θ'˜θ, µ'˜µ ' Mln ˆL(θ) /MθT
2 *θ'˜θ & ˜µ ' 0,
M‹(θ,µ)/MµT *θ'˜θ, µ'˜µ ' ˜θ2 ' 0.
Hence
1
n
0
˜µ
' Mln ˆL(θ) / n
MθT
*θ'˜θ.
(8.64)
Again, using the mean value theorem we can expand this expression around the unrestricted ML
estimator 
 which then yields
ˆθ,
1
n
0
˜µ
'
&H n(˜θ&ˆθ) % op(1),
(8.65)
hence
˜µT ¯H
(2,2,)˜µ
n
' 1
n
(0T, ˜µT) ¯H
&1 0
˜µ
'
n(˜θ&ˆθ)T ¯H n(˜θ&ˆθ) % op(1) 6d χ2
r.
(8.66)
Replacing 
 in this expression by a consistent estimator on the basis of the restricted ML
H
estimator , say:
˜θ
˜H ' &
/0000
M2ln( ˆLn(θ))/n
MθMθT
θ'˜θ
.
(8.67)
and partitioning 
 similarly to (8.56) as
˜H
&1

320
˜H
&1 '
˜H
(1,1)
˜H
(1,2)
˜H
(2,1)
˜H
(2,2) ,
(8.68)
we have
Theorem 8.8: (LM test) Under Assumptions 8.1-8.3,  
 = 0.
˜µT ˜H
(2,2)˜µ /n 6d χ2
r if θ2,0
8.5.4
Which test to use?
The Wald, LR and LM tests basically test the same null hypothesis against the same
alternative, so which one should we use? The Wald test employs only the unrestricted ML
estimator , so that this test is the most convenient if we have to conduct unrestricted ML
ˆθ
estimation anyhow. The LM test is entirely based on the restricted ML estimator , and there are
˜θ
situations where we start with restricted ML estimation, or where restricted ML estimation is
much easier to do than unrestricted ML estimation, or even where unrestricted ML estimation is
not feasible because without the restriction imposed the model is incompletely specified. Then
the LM test is the most convenient test. Both the Wald and the LM tests require the estimation of
the matrix 
 That may be a problem for complicated models because of the partial derivatives
H.
involved. In that case use the LR test.
Although I have derived the Wald, LR and LM tests for the special case of a null
hypothesis of the type 
, the results involved can be modified to general linear hypotheses
θ2,0 ' 0

321
of the form 
, where R is a r  m matrix of rank r, by reparametrizing the likelihood
Rθ0 ' q
×
function, as follows. Specify a (m-r)  m matrix R* such that the matrix
×
Q '
R(
R
is nonsingular. Then define new parameters by
β '
β1
β2
'
R(θ
Rθ
&
0
q
' Qθ &
0
q
.
Substituting
θ ' Q &1β % Q &1 0
q
in the likelihood function, the null hypothesis involved is equivalent to 
.
β2 ' 0
8.6.
Exercises
1.
Derive 
 for the case (8.11), and show that if Z1,...,Zn is a random
ˆθ ' argmaxθ ˆLn(θ)
sample then the ML estimator involved is consistent.
2.
Derive 
 for the case (8.13).
ˆθ ' argmaxθ ˆLn(θ)
3.
Show that the log-likelihood function of the Logit model is unimodal, i.e., the matrix
 is negative-definite for all 2. 
M2ln[ ˆLn(θ)]/(MθMθT)
4.
Prove (8.20).
5.
Extend the proof of Theorem 8.2 to the multivariate parameter case.

322
6.
Let (Y1,X1),...,(Yn,Xn) be a random sample from a bivariate continuous distribution with
conditional density
f(y*x,θ0) ' x/θ0 exp&y.x/θ0 if x > 0 and y > 0; f(y*x,θ0) ' 0 elsewhere,
where 20 > 0 is an unknown parameter. The marginal density h(x) of Xj is unknown, but we do
know that h does not depend on 20, and h(x) = 0 for x # 0.
(a)
Specify the conditional likelihood function 
.  
ˆL
c
n (θ)
(b)
Derive the maximum likelihood estimator  of  20.
ˆθ
(c)
Show that  is unbiased.  
ˆθ
(d)
Show that the variance of  is equal to 
  
ˆθ
θ2
0/n.
(e) 
Verify that this variance is equal to the Cramer-Rao lower bound.  
(f)
Derive the test statistic of the LR  test of the null hypothesis 20 = 1,  in the form for which
it has an asymptotic 
 null distribution.
χ2
1
(g)
Derive the test statistic of the Wald test of the null hypothesis 20 = 1.
(h)
Derive the test statistic of the LM test of the null hypothesis 20 = 1.
(i)
Show that under the null hypothesis  20 = 1 the LR test in part (f) has a limiting  
 
χ2
1
distribution.
7. 
Let Z1,....,Zn be a random sample from the (nonsingular) Nk[µ,E] distribution. Determine
the maximum likelihood estimators of µ and E.
8.
In the case where the dependent variable Y is a duration, for example an unemployment
duration spell, the conditional distribution of Y given a vector X of explanatory variables is often
modeled by the proportional hazard model

323
 
P[Y # y|X ' x] ' 1 & exp &n(x)*y
0λ(t)dt , y > 0,
(8.70)
where 8(t) is a positive function on (0,4) such that 
 and n is a positive function.
*4
0λ(t)dt ' 4,
The reason for calling this model a proportional hazard model is the following. Let f(y|x)
be the conditional density of Y given X = x, and let 
 The
G(y|x) ' exp &n(x)*y
0λ(t)dt , y > 0.
latter function is called the conditional survival function. Then 
is called
f(y|x)/G(y|x) ' n(x)λ(y)
the hazard function, because for a small * > 0,  
 is approximately the conditional
δf(y|x)/G(y|x)
probability (hazard) that 
 given that Y > y and X = x. 
Y 0 (y,y%δ],
Convenient specifications of  8(t) and n(x) are:
λ(t) ' γt γ&1, γ > 0 (Weibull specification)
n(x) ' exp(α % βTx)
(8.71)
Now consider a random sample of size n of unemployed workers. Each unemployed
worker j is interviewed twice. The first time worker j tells the interviewer how long he or she has
been unemployed, and reveals his or her vector Xj of characteristics. Call this time 
 A fixed
Y1,j.
period of length T later the interviewer asks worker j whether he or she is still (uninterruptedly)
unemployed, and if not how long it took during this period to find employment for the first time.
Call this duration 
 In the latter case the observed unemployment duration is 
Y2,j.
Yj ' Y1,j % Y2,j,
but if the worker is still unemployed we only know that 
 The latter is called
Yj > Y1,j % T.
censoring. Assuming that the  Xj’s do not change over time, setup the conditional  likelihood
function for this case, using the specifications (8.70) and (8.71).
    

324
1.
See Chapter 3 for the definition of these conditional probabilities.
2.
Recall from Chapter 1 that the union of F-algebras is not necessarily a F-algebra.
Endnotes

325
Appendix I  
Review of Linear Algebra
I.1.
Vectors in a Euclidean space
A vector is a set of coordinates which locates a point in a Euclidean space. For example,
in the two-dimensional Euclidean space 
 the vector
ú2
a '
a1
a2
'
6
4
(I.1)
is the point which location in a plane is determined by moving 
 units away from the
a1 ' 6
origin along the horizontal axis (axis 1), and then moving
 units away parallel to the
a2 ' 4
vertical axis (axis 2),  as displayed in Figure I.1.
Figure I.1: A vector in ú2
The distances 
 and 
 are called the components of the vector a involved. 
a1
a2
An alternative interpretation of the vector a is a force pulling from the origin (the
intersection of the two axes). This force is characterized by its direction (the angle of the line in
Figure I.1) and its strength (the length of the line piece between point a and the origin). As to the

326
latter, it follows from Pythagoras’ Theorem that this length is the square root of the sum of the
squared distances of point a from the vertical and horizontal axes: a 2
1 %a 2
2 '
62%42 ' 3 6,
and is denoted by 
 More generally, the length of a vector
2a2.
x '
x1
x2
!
xn
(I.2)
in  
 is defined by
ún
2x2 '
def.
'n
j'1x 2
j .
(I.3)
There are two basic operations that apply to vectors in 
. The first basic operation is
ún
scalar multiplication:
c.x '
def.
c.x1
c.x2
!
c.xn
,
(I.4)
 
where c  
 is a scalar. Thus, vectors in 
are multiplied by a scalar by multiplying each of the
0 ú
ún
components by this scalar. The effect of scalar multiplication is that the point x is moved a factor
c along the line through the origin and the original point x. For example, if we multiply the vector
a in Figure I.1 by c = 1.5, the effect is the following:

327
Figure I.2: Scalar multiplication
The second operation is addition: Let x be the vector (I.2), and let
y '
y1
y2
!
yn
.
(I.5)
Then
x % y '
def.
x1%y1
x2%y2
!
xn%yn
.
(I.6)
Thus, vectors are added by adding up the corresponding components. Of course, this operation is
only defined for conformable vectors, i.e., vectors with the same number of components. 
As an example of the addition operation, let a be the vector (I.1), and let

328
b '
b1
b2
'
3
7
(I.7)
Then 
a % b '
6
4
%
3
7
'
9
11
' c,
(I.8)
say. This result is displayed in Figure I.3 below. We see from Figure I.3 that the origin together
with the points a, b and  c = a + b form a parallelogram (which is easy to prove). In terms of
forces, the combined forces represented by the vectors a and b result in the force represented by
the vector c = a + b.
Figure I.3: c = a + b
The distance between the vectors a and b in Figure I.3 is 
 To see this, observe
2a & b2.
that the length of the horizontal line piece between the vertical line through b and point  a is
 and similarly the vertical line piece between b and the horizontal line through a has
a1&b1,
length 
These two line pieces, together with the line piece connecting the points a and b,
b2&a2.
form a triangle for which Pythagoras’ Theorem applies: The squared distance between a and b is

329
equal to 
. More generally, 
(a1&b1)2 % (a2&b2)2 ' 2a&b22
The distance between the vector x in (I.2) and the vector y in (I.5)  is   
2x & y2 '
'n
j'1(xj & yj)2.
(I.9)
Moreover, it follows from (I.9) and the Law of Cosines1 that:
The angle N between the vector x in (I.2) and the vector y in (I.5) satisfies   
cos(φ) ' 2x22 % 2y22 & 2x&y22
22x2.2y2
'
'n
j'1xjyj
2x2.2y2
.
(I.10)
 
I.2.
Vector spaces
The two basic operations, addition and scalar multiplication, make a Euclidean space ún
a special case of a vector space:
Definition I.1: Let  V be a set endowed with two operations, the operation "addition", denoted
by "+", which maps each pair (x,y) in V V into V, and the operation "scalar multiplication",
×
denoted by a dot (.), which maps each pair (c,x) in 
 into V.  The set V is called a vector
ú × V
space if  the addition and multiplication operations involved satisfy the following rules, for all x,
y and z in V, and all scalars c, c1 and c2 in 
:
ú
(a)
x + y = y + x;
(b)
x + (y + z) = (x + y) + z;
(c)
There is a unique zero vector 0 in V such that x + 0 = x;
(d)
For each x there exists a unique vector !x in V such that x + (!x) = 0;

330
(e)
1.x = x;
(f)
(c1c2).x = c1.(c2.x);
(g)
c.(x + y) = c.x + c.y;
(h)
(c1 + c2).x = c1.x + c2.x.
It is trivial to verify that with addition "+" defined by (I.6) and scalar multiplication c.x
defined by (I.4) the Euclidean space 
 is a vector space. However, the notion of a vector space
ún
is much more general. For example, let V  be the space of all continuous functions on 
, with
ú
pointwise addition and scalar multiplication defined the same way as for real numbers. Then it is
easy to verify that this space is a vector space. 
Another (but weird) example of a vector space  is the space V of positive real numbers
endowed with the "addition" operation x + y = x.y and the "scalar multiplication" c.x = xc. In this
case the null vector 0  is the number 1, and !x = 1/x.
Definition I.2: A subspace V0 of a vector space V is a non-empty subset of V which satisfies the
following two requirements:
(a)
For any pair x, y in V0, x + y is in V0;
(b)
For any x in V0 and any scalar c, c.x is in V0.
It is not hard to verify that a subspace of a vector space is a vector space itself, because
the rules (a) through (h) in Definition I.1 are inherited from the "host" vector space V. In
particular, any subspace contains the null vector 0, as follows from part (b) of Definition I.2 with

331
c = 0. For example, the line through the origin and point a in Figure I.1, extended indefinitely in
both directions, is a subspace of  
. This subspace is said to be spanned by the vector a. More
ú2
generally,
Definition I.3: Let x1,x2,....,xn be vectors in a vector space V. The space V0 spanned by x1,x2,....,xn 
is the space of all linear combinations of x1,x2,....,xn , i.e., each y in V0 can be written as
 for some coefficients cj in 
y ' 'n
j'1cjxj
ú.
Clearly, this space V0 is a subspace of V. 
For example, the two vectors a and b in Figure I.3 span the whole Euclidean space 
,
ú2
because any vector x in 
 can be written as,
ú2
 
x '
x1
x2
' c1
6
4
% c2
3
7
'
6c1%3c2
4c1%7c2
,
where 
c1 '
7
30
x1 &
1
10
x2,
c2 ' & 2
15
x1 % 1
5
x2.
The same applies to the vectors a, b and c in Figure I.3: They also span the whole Euclidean
space 
. However, in this case any pair of  a, b and c does the same, so one of these three
ú2
vectors is redundant, because each of the vectors  a, b and c can already be written as a linear
combination of the other two. Such vectors are called linear dependent:

332
Definition I.4: A set of vectors x1,x2,....,xn in a vector space V is linear dependent if one or more
of these vectors  can be written as a linear combination of the other vectors, and the set is called
linear independent if none of them can be written as a linear combination of the other vectors.
In particular, x1,x2,....,xn  are linear independent if and only if  
 = 0 implies that 
'n
j'1cjxj
c1 ' c2 'þþ' cn ' 0.
For example, the vectors a and b in Figure I.3 are linear independent, because if not then
there would exists a scalar c such that b = c.a,  hence 6 = 3c and 4 = 7c,  which is impossible. A
set of such linear independent vectors is called a basis for the vector space they span:
Definition I.5: A basis for a vector space is a set of vectors having the following two properties:
(a)
It is linear independent;
(b)
The vectors span the vector space involved.
We have seen that each of the subsets {a,b}, {a,c} and {b,c} of  the set {a, b, c} of
vectors in Figure I.3 is linear independent, and span the vector space 
. Thus, there are in
ú2
general many bases for the same vector space, but what they have in common is their number:
This number is called the dimension of V. 
Definition I.6: The number of vectors that form  a basis of a vector space is called the dimension
of this vector space.

333
In order to show that this definition is unambiguous, let  {x1,x2,....,xn} and {y1,y2,....,ym} be
two different bases for the same vector space, and  let m = n +1. Each of the yi‘s can be written as
a linear combination of x1,x2,....,xn : 
. If  {y1,y2,....,yn+1} is linear independent then
yi ' 'n
j'1ci,jxj
 = 0  if and only if  
  But since {x1,x2,....,xn} is
'n%1
i'1 ziyi ' 'n
j'1'n%1
i'1 zici,jxj
z1 ' ...... ' zn%1 ' 0.
linear independent we must also have that  
 for j = 1,...,n.  The latter is a system of
'n%1
i'1 zici,j ' 0
n linear equations in n+1 unknown variables zi and therefore has a non-trivial solution, in the
sense that there exists a solution 
such that least one of the z’s is non-zero. Consequently, 
z1,...,zn%1
 {y1,y2,....,yn+1} cannot be linear independent. 
Note that in principle the dimension of a vector space can be infinite. For example,
consider the space  
 of all countable infinite sequences 
 of real
ú4
x ' (x1,x2,x3,............)
numbers, endowed with the addition operation
x % y ' (x1,x2,x3,............) % (y1,y2,y3,............) ' (x1%y1,x2%y2,x3%y3,............)
and the scalar multiplication operation
c.x
' (c.x1,c.x2,c.x3,............).
Let  yi be a countable infinite sequence of zeros, except for the i-th element in this sequence,
which is equal to 1. Thus, 
 
 etc. Then  {y1,y2,y3,...} is a basis
y1 ' (1,0,0,þ), y2 ' (0,1,0,þ),
for  
, with dimension  4. Also in this case there are many different bases; for example, another
ú4
basis for  
 is   
 
  etc. 
ú4
y1 ' (1,0,0,0,þ), y2 ' (1,1,0,0,þ), y3 ' (1,1,1,0,þ),

334
I.3.
Matrices
In Figure I.3 the location of point c can be determined by moving nine units away from
the origin along the horizontal axis 1, and then moving eleven units away from axis 1 parallel to
the vertical axis 2. However, given the vectors a and b an alternative way of determining the
location of point c is: Move 2a2 units away from the origin along the line through the origin and
point a (the subspace spanned by a),  and then move 2b2 units away  parallel to the line through
the origin and point b (the subspace spanned by b). Moreover, if we take 2a2 as the new distance
unit along the subspace spanned by a, and 2b2 as the new distance unit along the subspace
spanned by b, then point c can be located by moving one (new) unit away from the origin along
the new axis 1 formed by the subspace spanned by a, and then move one (new) unit away from
this new axis 1 parallel to the subspace spanned by b (which is now the new axis 2). We may
interpret this as moving the point  
 to a new location: point c. This is precisely what a matrix
1
1
does: moving points to a new location by changing the coordinate system. In particular, the
matrix
A ' a,b '
6 3
4 7
(I.11)
moves any point 
x '
x1
x2
(I.12)
to a new location, by changing the original perpendicular coordinate system to a new coordinate
system, where the new axis 1 is  the subspace spanned by the first column, a, of the matrix A,
with new unit distance the length of a, and the new axis 2 is the subspace spanned by the second

335
column, b, of  A, with new unit distance the length of b. Thus, this matrix A moves point x to
point 
y ' Ax ' x1.a % x2.b ' x1.
6
4
% x2.
3
7
'
6x1%3x2
4x1%7x2
.
(I.13)
In general, an 
 matrix
m × n
A '
a1,1 þ a1,n
!
"
!
am,1 þ am,n
(I.14)
moves the point in 
 corresponding to the vector  x in (I.2) to a point  in the subspace of 
 
ún
úm
spanned by the columns of A, namely to point
y ' Ax ' j
n
j'1
xj
a1,j
!
am,j
'
'n
j'1a1,jxj
!
'n
j'1am,jxj
'
y1
!
ym
.
(I.15)
Next, consider the 
 matrix
k × m
B '
b1,1 þ b1,m
!
"
!
bk,1 þ bk,m
.
(I.16)
and let y be given by (I.15). Then 
By ' B(Ax) '
b1,1 þ b1,m
!
"
!
bk,1 þ bk,m
'n
j'1a1,jxj
!
'n
j'1am,jxj
'
'n
j'1 'm
s'1b1,sas,j xj
!
'n
j'1 'm
s'1bk,sas,j xj
' Cx,
(I.17)
where 

336
C '
c1,1 þ c1,n
!
"
!
ck,1 þ ck,n
with ci,j ' 'm
s'1bi,sas,j.
(I.18)
This matrix C is called the product of the matrices B and A, and is denoted by BA. Thus, with A
given by (I.14) and B given by (I.16),  
BA '
b1,1 þ b1,m
!
"
!
bk,1 þ bk,m
a1,1 þ a1,n
!
"
!
am,1 þ am,n
'
'm
s'1b1,sas,1 þ 'm
s'1b1,sas,n
!
"
!
'm
s'1bk,sas,1 þ 'm
s'1bk,sas,n
,
(I.19)
which is a  
 matrix. Note that the matrix BA only exists if the number of columns of B is
k × n
equal to the number of rows of A. Such matrices are called conformable. Moreover, note that if
A and B are also conformable, so that AB is defined2,  then the commutative law does not hold,
i.e., in general AB … BA.  However, the associative law (AB)C = A(BC) does hold, as is easy to
verify.
Let A be the 
 matrix (I.14), and let now B be another  
 matrix:
m × n
m × n
B '
b1,1 þ b1,n
!
"
!
bm,1 þ bm,n
.
(I.20)
As argued before, A maps a point x  0 ún  to a point y = Ax 0 úm, and B maps x to a point z = Bx 0
úm. It is easy to verify that y+z = Ax + Bx = (A+B)x = Cx, say, where C = A + B  is the m × n
formed by adding up the corresponding elements of A and B:

337
A % B '
a1,1 þ a1,n
!
"
!
am,1 þ am,n
%
b1,1 þ b1,n
!
"
!
bm,1 þ bm,n
'
a1,1%b1,1
þ
a1,n%b1,n
!
"
!
am,1%bm,1 þ am,n%bm,n
.
(I.21)
Thus, conformable matrices are added up by adding up the corresponding elements. 
Moreover, for any scalar c we have A(c.x) = c.(Ax) = (c.A)x, where c.A is the matrix
formed by multiplying each element of A by the scalar c:
c.A ' c.
a1,1 þ a1,n
!
"
!
am,1 þ am,n
'
c.a1,1 þ c.a1,n
!
"
!
c.am,1 þ c.am,n
.
(I.22)
Now with addition and scalar multiplication defined in this way, it is easy to verify that
all the conditions in Definition I.1 hold for matrices as well, i.e., the set of all 
 matrices is
m × n
a vector space. In particular, the "zero" element involved is the  
 matrix with all elements
m × n
equal to zero:
Om,n '
0 þ 0
! " !
0 þ 0
.
(I.23)
I.4.
The inverse and transpose of a matrix
The question I now will address is whether for a given  
 matrix A there exists a 
m × n
 matrix B such that, with y = Ax, By = x. If so, the action of A is undone by B, i.e., B
n × m
moves y back to the original position x. 
If m < n there is no way to undo the mapping y = Ax, i.e., there does not exists an n × m
matrix B such that By = x. To see this, consider the 
 matrix A = ( 2,1). Then with x as in
1 × 2

338
(I.12), Ax = 2x1 + x2 = y, but if we know y and A, then we only know that x is located on the line
x2 = y - 2x1, but there is no way to determine where on this line. 
If m = n in (I.14), so that the matrix A involved is a square matrix,  we can undo the
mapping A if the columns3 of the matrix A are linear independent. Take for example the matrix A
in (I.11) and the vector y in (I.13), and let
 
B '
7
30
& 1
10
& 2
15
1
5
(I.24)
Then 
By '
7
30
& 1
10
& 2
15
1
5
6x1%3x2
4x1%7x2
'
x1
x2
' x,
(I.25)
so that this matrix B moves the point y = Ax back to x. Such a matrix is called the inverse of A,
and is denoted by 
 Note that for an invertible 
 matrix A,   
 is the
A &1.
n × n
A &1A ' In, where In
 unit matrix:
n × n
In '
1 0 0 þ 0
0 1 0 þ 0
0 0 1 þ 0
! ! ! " !
0 0 0 þ 1
.
(I.26)
Note that a unit matrix is a special case of a diagonal matrix, i.e., a square matrix with all off-
diagonal elements equal to zero.
We have seen that the inverse of A is a matrix 
 such that 
4 But what about
A &1
A &1A ' I.

339
 Does the order of multiplication matter? The answer is no:
AA &1?
Theorem I.1: If A is invertible, then A A!1 =  I, i.e., A is the inverse of  A!1,
because it is trivial that
Theorem I.2: If A and B are invertible matrices then (AB)!1 = B!1A!1.
Now let us give a formal proof of our conjecture that:
Theorem I.3: A square matrix is invertible if and only if its columns are linear independent. 
Proof: Let A be 
 the matrix involved. I will show first that:
n × n
(a)
The columns  a1,....,an  of A are linear independent if and only if for every  
 the
b 0 ún
system of n linear equations Ax =  b has a unique solution. 
To see this, suppose that there exists another solution y: Ay = b.  Then A(x!y) = 0 and
x!y … 0, which imply that the columns a1,....,an of A are linear dependent. Similarly, if for every 
  the system Ax =  b has a unique solution, then the columns a1,....,an  of  A must be 
b 0 ún
linear independent, because if not then there exists a vector c … 0  in 
 such that Ac = 0, hence
ún
if x is a solution of Ax = b then so is x + c. 
Next, I will show that:

340
(b)
A is invertible if and only if for every  
 the system of n linear equations Ax =  b
b 0 ún
has a unique solution. 
First, if A is invertible then the solution of Ax = b is x = A!1b, which for each  
 is
b 0 ún
unique. Second, let b = ei be the i-th column of the unit matrix In, and let xi be the unique
solution of Axi  =  ei. Then the matrix X with columns x1,...,xn satisfies 
 
AX ' A(x1,þ,xn) ' (Ax1,þ,Axn) ' (e1,þ,en) ' In,
hence A is the inverse of X:  A = X!1. It follows now from Theorem I.1 that X is the inverse of A:
X = A!1.  Q.E.D.
If the columns of a square matrix A are linear dependent, then Ax maps point x into a
lower-dimensional space, namely the subspace spanned by the columns of A. Such a mapping is
called a singular mapping, and the corresponding matrix A is therefore called singular.
Consequently, a square matrix with linear independent columns is called non-singular. It
follows from Theorem I.3 that a non-singularity is equivalent to invertibility, and singularity  is
equivalent to absence of invertibility.
If m > n in (I.14), so that the matrix A involved has more rows than columns,  we can also
undo the action of A if the columns of the matrix A are linear independent, as follows. First,
consider the transpose5 AT  of the matrix A in (I.14):
A T '
a1,1 þ am,1
!
"
!
a1,n þ am,n
,
(I.27)
i.e., AT  is formed by filling its columns with the elements of the corresponding rows of A. Note
that

341
Theorem I.4: (AB)T = BTAT. Moreover, if A and B are square and invertible then
(A T )&1 ' (A &1)T,
(AB)&1 T ' B &1A &1 T ' A &1 TB &1 T ' A T &1 B T &1, and similarly,
(AB)T &1 ' B TA T &1 ' A T &1 B T &1 ' A &1 TB &1 T.
Proof: Exercise.
Since a vector can also be interpreted as a matrix with only one column, the transpose
operation also applies to vectors. In particular, the transpose of the vector x in (I.2) is:
x T ' (x1,x2.þ,xn),
(I.28)
which may be interpreted as a 1×n matrix. 
Now if y = Ax then  ATy =  ATAx, where ATA is an  
 matrix. If ATA is invertible, then
n × n
(ATA)!1ATy =  x, so that then the action of the matrix A is undone by the 
 matrix (ATA)!1AT.
n × m
Thus, it remains to be shown that:
Theorem I.5:  ATA is invertible if and only if the columns of the matrix A are linear independent.
Proof: Let  a1,....,an be the columns of A. Then  ATa1,....,ATan are the columns of ATA.
Thus, the columns of ATA are linear combinations of the columns of A. Suppose that the columns
of ATA are linear dependent. Then there exists coefficients cj not all equal to zero such that
. This equation can be rewritten as 
 Since
c1A Ta1 %þ% cnA Tan ' 0
A T(c1a1 %þ% cnan) ' 0.
a1,....,an are linear independent, we have 
 hence the columns of  AT are
c1a1 %þ% cnan … 0,
linear dependent. However, this is impossible, because of the next theorem. Therefore, if the
columns of A are linear independent, then so are the columns of  ATA. Thus, the theorem under

342
review follows from Theorem I.3 and Theorem I.6 below. 
Theorem I.6: The dimension of the subspace spanned by the columns of a matrix A is equal to
the dimension of the subspace spanned by the columns of its transpose AT.
The proof of Theorem I.6 has to be postponed, because we need for it the results in the
next sections. In particular, Theorem I.6 follows from Theorems I.11, I.12 and I.13 below.
Definition I.7: The dimension of the subspace spanned by the columns of a matrix A is called the
rank of A.
Thus, a square matrix is invertible if and only if its rank equals its size, and if a matrix is
invertible then so is its transpose.
I.5.
Elementary matrices and permutation matrices
Let A be the 
 matrix in (I.14). An elementary 
 matrix E is a matrix such
m × n
m × m
that the effect of  EA is that a multiple of one row of A is added to another row of A. For
example, let Ei,j(c) be an elementary matrix such that the effect of Ei,j(c)A is that c times row j is
added to row i < j:

343
Ei,j(c)A '
a1,1
þ
a1,n
!
"
!
ai&1,1
þ
ai&1,n
ai,1%caj,1 þ ai,n%caj,n
ai%1,1
þ
ai%1,n
!
"
!
aj,1
þ
aj,n
!
"
!
am,1
þ
am,n
.
(I.29)
Then Ei,j(c)6  is equal to the unit matrix Im (compare (I.26)), except that the zero in the (i,j)'s
position is replaced with a nonzero constant c. In particular, if i=1 and j = 2 in (I.29), so that
E1,2(c)A adds c times row 2 of A to row 1 of A, then 
E1,2(c) '
1
c
0 þ 0
0 1 0 þ 0
0 0 1 þ 0
! ! ! " !
0 0 0 þ 1
.
This matrix is a special case of an upper-triangular matrix; that is a square matrix with all the
elements below the diagonal equal to zero. Moreover, E2,1(c)A adds c times row 1 of A to row 2
of A:
E2,1(c) '
1 0 0 þ 0
c
1 0 þ 0
0 0 1 þ 0
! ! ! " !
0 0 0 þ 1
,
(I.30)

344
which is a special case of a lower-triangular matrix, i.e., a square matrix with all the elements
above the diagonal equal to zero. 
Similarly, if E is an elementary 
matrix, then the effect of AE is that one of the
n × n
columns of A, times a nonzero constant, is added to another column of A.  Thus, 
Definition I.8: An elementary matrix is a unit matrix with one off-diagonal element replaced
with a nonzero constant.
Note that the columns of an elementary matrix are linear independent, hence an
elementary matrix is invertible. The inverse of an elementary matrix is easy to determine: If the
effect of EA is that c times row j of A is added to row i of A, then E!1 is an elementary matrix
such that the effect of  E!1EA is that -c times  row j of EA is added to row i of A, so that then 
E!1EA restores A. For example, the inverse of the elementary matrix  (I.30) is:
E2,1(c)&1 '
1 0 0 þ 0
c
1 0 þ 0
0 0 1 þ 0
! ! ! " !
0 0 0 þ 1
&1
'
1
0 0 þ 0
&c 1 0 þ 0
0
0 1 þ 0
!
! ! " !
0
0 0 þ 1
' E2,1(&c).
 
We now turn to permutation matrices:
Definition I.9: An elementary permutation matrix is a unit matrix with two columns or rows
swapped.  A permutation matrix is a matrix whose columns or rows are permutations of the
columns or rows of a unit matrix.

345
In particular, the elementary permutation matrix that is formed by swapping the columns i
and j of a unit matrix will be denoted by  Pi,j.
The effect of an (elementary) permutation matrix on A is that PA swaps two rows, or
permutates the rows, of A. Similarly, AP swaps or permutates the columns of A. Whether you
swap or permutate columns or rows of a unit matrix does not matter, because the resulting
(elementary) permutation matrix is the same. An example of an elementary permutation matrix is
P1,2 '
0 1 0 þ 0
1 0 0 þ 0
0 0 1 þ 0
! ! ! " !
0 0 0 þ 1
.
Note that a permutation matrix P can be formed as a product of elementary permutation matrices,
say 
. Moreover, note that if an elementary permutation matrix Pi,j is applied to
P ' Pi1,j1.......Pik,jk
itself, i.e., Pi,jPi,j,  then the swap is undone, and the result is the unit matrix: Thus, the inverse of
an elementary permutation matrix Pi,j is Pi,j itself.  This result holds only for elementary
permutation matrices, though. In the case of the permutation matrix 
  we have
P ' Pi1,j1.......Pik,jk
. Since elementary permutation matrices are symmetric: 
 it
P &1 ' Pik,jk.......Pi1,j1
Pi,j ' P T
i,j ,
follows that  
 Moreover, if E is an elementary matrix and Pi,j an
P &1 ' P T
ik,jk.......P T
i1,j1 ' P T.
elementary permutation matrix then 
. Combining these results, it follows:
Pi,jE ' EPi,j
Theorem I.7: If E is an elementary matrix and P is a permutation matrix, then PE ' EP T.
Moreover, P &1 ' P T.

346
I.6.
Gaussian elimination of a square matrix, and the Gauss-Jordan iteration for
inverting a matrix
I.6.1
Gaussian elimination of a square matrix
The results in the previous section are the tools we need to derive the following result:
Theorem I.8: Let A be a square matrix. 
(a)
There exists a permutation matrix P, possibly equal to the unit matrix I, a lower-
triangular matrix L with diagonal elements all equal to 1, a diagonal matrix D, and an upper-
triangular matrix U with diagonal elements all equal to 1, such that PA = LDU.  
(b)
If A is non-singular and P = I  this decomposition is unique, i.e., if A  = LDU = L*D*U*, 
then 
 
 and 
 
L( ' L, D( ' D,
U( ' U.
The proof of part (b) is as follows: LDU = L*D*U*  implies
L &1L(D( ' DUU &1
(
(I.31)
It is easy to verify that the inverse of a lower triangular matrix is lower triangular, and that the
product of lower triangular matrices is lower triangular. Thus the left-hand side of (I.31) is lower
triangular. Similarly, the right-hand side of (I.31) is upper triangular. Consequently, the off-
diagonal elements in both sides are zero: Both matrices in (I.31) are diagonal. Since  
 is
D(
diagonal and non-singular, it follows from (I.31) that  
 is diagonal.
L &1L( ' DUU &1
( D &1
(
Moreover, since the diagonal elements of 
 and 
 are all equal to one, the same applies to
L &1
L(
 Similarly we have  
 Then  
L &1L(, i.e., L &1L( ' I, hence L ' L(.
U ' U(.
D ' L &1AU &1
and  D( ' L &1AU &1.

347
Rather than giving a formal proof of part (a) of Theorem I.8, I will demonstrate the result
involved by two examples, one for the case that A is non-singular, and one for the case that A is
singular.
Example 1: A is nonsingular.
Let 
A '
2
4
2
1
2
3
&1 1 &1
.
(I.32)
We are going to multiply A by elementary matrices and elementary permutation matrices such
that the end-result will be a upper-triangular matrix. This is called Gaussian elimination. 
First, add !½ times row 1 to row 2 in (I.32). This is equivalent to multiplying A by the
elementary matrix E2,1(!½). (Compare  (I.30), with c = !½.). Then 
E2,1(!½)A '
1
0 0
!0.5 1 0
0
0 1
2
4
2
1
2
3
&1 1 &1
'
2
4
2
0
0
2
&1 1 &1
.
(I.33)
Next, add ½ times row 1 to row 3, which is equivalent to multiplying (I.33) by the elementary
matrix E3,1(½):
E3,1(½)E2,1(!½)A '
1
0 0
0
1 0
0.5 0 1
2
4
2
0
0
2
&1 1 &1
'
2 4 2
0 0 2
0 3 0
.
(I.34)
Now swap the rows 2 and 3 of the right-hand side matrix in (I.34). This is equivalent to
multiplying (I.34) by the elementary permutation matrix P2,3 formed by swapping the columns 2

348
and 3 of the unit matrix I3. Then
P2,3E3,1(½)E2,1(!½)A '
1 0 0
0 0 1
0 1 0
2 4 2
0 0 2
0 3 0
'
2 4 2
0 3 0
0 0 2
'
2 0 0
0 3 0
0 0 2
1 2 1
0 1 0
0 0 1
' DU,
(I.35)
say. Moreover, since  P2,3 is an elementary permutation matrix, we have that 
 hence
P &1
2,3 ' P2,3,
it follows from Theorem I.7 and (I.35) that
P2,3E3,1(½)E2,1(!½)A ' E3,1(½)P2,3E2,1(!½)A ' E3,1(½)E2,1(!½)P2,3A ' DU.
(I.36)
Furthermore, observe that 
E3,1(½)E2,1(!½) '
1
0 0
!0.5 1 0
0
0 1
1
0 0
0
1 0
0.5 0 1
'
1
0 0
&0.5 1 0
0.5
0 1
(I.37)
hence
E3,1(½)E2,1(!½) &1 '
1
0 0
&0.5 1 0
0.5
0 1
&1
'
1
0 0
0.5
1 0
&0.5 0 1
' L,
(I.38)
say. Combining (I.36) and (I.38), it follows now that P2,3A ' LDU.
Example 2: A is singular.
Theorem I.8 also holds for singular matrices. The only difference with the non-singular
case is that if A is singular then the diagonal matrix D will have zeros on the diagonal. To
demonstrate this, let now

349
A '
2
4
2
1
2
1
&1 1 &1
.
(I.39)
Since the first and last column of this matrix A are equal, the columns are linear dependent,
hence A is singular. Now (I.33) becomes
E2,1(!½)A '
1
0 0
!0.5 1 0
0
0 1
2
4
2
1
2
1
&1 1 &1
'
2
4
2
0
0
0
&1 1 &1
,
(I.40)
(I.34) becomes
E3,1(½)E2,1(!½)A '
1
0 0
0
1 0
0.5 0 1
2
4
2
0
0
0
&1 1 &1
'
2 4 2
0 0 0
0 3 0
,
(I.41)
and (I.35) becomes
P2,3E3,1(½)E2,1(!½)A '
1 0 0
0 0 0
0 1 0
2 4 2
0 0 0
0 3 0
'
2 4 2
0 3 0
0 0 0
'
2 0 0
0 3 0
0 0 0
1 2 1
0 1 0
0 0 1
' DU.
(I.42)
The formal proof of part (a) of  Theorem I.8 is similar to the argument in these two
examples, and is therefore omitted.
Note that the result (I.42) demonstrates that:

350
Theorem I.9: The dimension of the subspace spanned by the columns of a square matrix A is
equal to the number of non-zero diagonal elements of the matrix D in Theorem I.8.
Example 3: A is symmetric and nonsingular
Next, consider the case that A is symmetric, that is, AT = A. For example, let
A '
2 4
2
4 0
1
2 1 &1
.
(I.43)
Then
E3,2(&3/8)E3,1(&1)E2,1(&2)AE1,2(&2)E1,3(&1)E2,3(&3/8)
'
2
0
0
0 &8
0
0
0
&15/8
' D,
(I.44)
hence
A
' E3,2(&3/8)E3,1(&1)E2,1(&2) &1D E1,2(&2)E1,3(&1)E2,3(&3/8) &1 ' LDL T.
(I.45)
Thus, in the symmetric case we can eliminate each pair of non-zero elements opposite of the
diagonal jointly by multiplying A from the left by an appropriate  elementary matrix and
multiplying A from the right by the transpose of the same elementary matrix.
Example 4: A is symmetric and singular
Although I have demonstrated this result for a non-singular symmetric matrix, it holds for
the singular case as well. For example, let now
A '
2 4 2
4 0 4
2 4 2
.
(I.46)

351
Then
E3,1(&1)E2,1(&2)AE1,2(&2)E1,3(&1) '
2
0
0
0 &8 0
0
0
0
' D.
(I.47)
Example 5: A is symmetric and has a zero in a pivot position
If there is a zero in a pivot position7, then we need a row exchange. In that case the result
A = LDLT  will no longer be valid.  For example, let
A '
0 4 2
4 0 4
2 4 2
.
(I.48)
Then
E3,2(&1)E3,1(&1/2)P1,2A '
4 0
4
0 4
2
0 0 &2
'
4 0
0
0 4
0
0 0 &2
1 0
1
0 1 1/2
0 0
1
' DU
L ' E3,2(&1)E3,1(&1/2) &1 ' E3,1(1/2)E3,2(1) '
1
0 0
0
1 0
1/2 1 1
… U T.
(I.49)
Thus, examples 3, 4 and 5 demonstrate that:
Theorem I.10: If A is symmetric and the Gaussian elimination can be conducted without need
for row exchanges, then there exists a lower triangular matrix L with diagonal elements all equal
to one, and a diagonal matrix D, such that A = LDLT.

352
I.6.2
The Gauss-Jordan iteration for inverting a matrix
The Gaussian elimination of the matrix A in the first example in the previous section
suggests that this method can also be used to compute the inverse of A, as follows. Augment the
matrix A in (I.32) to a 3 × 6 matrix, by augmenting the columns of A with the columns of the unit
matrix I3:
B ' (A, I3) '
2
4
2
1 0 0
1
2
3
0 1 0
&1 1 &1
0 0 1
.
(I.50)
Now follow the same procedure as in Example 1, up to (I.35), with A replaced by B. Then (I.35)
becomes: 
P2,3E3,1(½)E2,1(!½)B ' P2,3E3,1(½)E2,1(!½)A, P2,3E3,1(½)E2,1(!½)
'
2 4 2
1
0 0
0 3 0
0.5
0 1
0 0 2
&0.5 1 0
' U(, C ,
(I.51)
say, where U*  in (I.51) follows from (I.35) and  
C ' P2,3E3,1(½)E2,1(!½) '
1
0 0
0.5
0 1
&0.5 1 0
.
(I.52)
Now multiply (I.51) by elementary matrix E13(-1), i.e., subtract row 3 from row 1:

353
E1,3(&1)P2,3E3,1(½)E2,1(!½)A, E1,3(&1)P2,3E3,1(½)E2,1(!½)
'
2 4 0
1.5
&1 0
0 3 0
0.5
0
1
0 0 2
&0.5
1
0
,
(I.53)
multiply (I.53) by elementary matrix E12(-4/3), i.e., subtract 4/3 times row 3 from row 1:
E1,2(&4/3)E1,3(&1)P2,3E3,1(½)E2,1(!½)A, E1,2(&4/3)E1,3(&1)P2,3E3,1(½)E2,1(!½)
'
2 0 0
5/6
&1 &4/3
0 3 0
0.5
0
1
0 0 2
&0.5
1
0
,
(I.54)
and finally, divide row 1 by 2, row 2 by 3, and row 3 by 2, or equivalently, multiply (I.54) by the
diagonal matrix D* with diagonal elements 1/2, 1/3 and 1/2:
D(E1,2(&4/3)E1,3(&1)P2,3E3,1(½)E2,1(!½)A, D(E1,2(&4/3)E1,3(&1)P2,3E3,1(½)E2,1(!½)
' I3, D(E1,2(&4/3)E1,3(&1)P2,3E3,1(½)E2,1(!½)
'
1 0 0
5/12 &1/2 &2/3
0 1 0
1/6
0
1/3
0 0 1
&1/4
1/2
0
.
(I.55)
Observe from (I.55) that the matrix (A,I3) has been transformed into a matrix of the type (I3,A*)  =
(A*A,A*) , where 
 is the matrix consisting of the
A ( ' D(E1,2(&4/3)E1,3(&1)P2,3E3,1(½)E2,1(!½)
last three columns of (I.55). Consequently, A ( ' A &1.
This way of computing the inverse of a matrix is called the Gauss-Jordan iteration. In
practice the Gauss-Jordan  iteration is done in a slightly different but equivalent way, using a
sequence of tableaus.  Take again the matrix A in  (I.32). The Gauss-Jordan iteration then starts

354
from the initial tableau:
Tableau 1
A
I
2
4
2
1 0 0
1
2
3
0 1 0
&1 1 &1
0 0 1
If there is a zero in a pivot position, you have to swap rows, as we will see below. In the
case of Tableau 1 there is not yet a problem, because the first element of row 1 is non-zero.  
The first step is to make all the non-zero elements in the first column equal to one, by
dividing all the rows by their first element, provided that they are non-zero. Then we get:
Tableau 2
1
2
1
1/2 0
0
1
2
3
0
1
0
1 &1 1
0
0 &1
Next, wipe out the first elements of rows 2 and 3, by subtracting row 1 from them:
Tableau 3
1
2
1
1/2
0
0
0
0
2
&1/2 1
0
0 &3 0
&1/2 0 &1
Now we have a zero in a pivot position, namely the second zero of row 2. Therefore, swap rows
2 and 3:

355
Tableau 4
1
2
1
1/2
0
0
0 &3 0
&1/2 0 &1
0
0
2
&1/2 1
0
Divide row 2 by -3 and row 3 by 2:
Tableau 5
1 2 1
1/2
0
0
0 1 0
1/6
0
1/3
0 0 1
&1/4 1/2
0
The left 3×3 block is now upper-triangular. 
Next, we have  to wipe out, one by one, the elements in this block above the diagonal.
Thus, subtract row 3 from  row 1:
Tableau 6
1 2 0
3/4
&1/2
0
0 1 0
1/6
0
1/3
0 0 1
&1/4
1/2
0
Finally, subtract 2 times row 2 from row 1:
Tableau 7
I
A &1
1 0 0
5/12 &1/2 &2/3
0 1 0
1/6
0
1/3
0 0 1
&1/4
1/2
0
This is the final tableau. The last three columns now form A!1. 
Once you have calculated A!1, you can solve the linear system Ax = b by computing x =
A!1b. However, you can also incorporate the latter in the Gauss-Jordan iteration, as follows. Let
again A be the matrix  in  (I.32), and let for example

356
b '
1
1
1
.
Insert this vector in Tableau 1:
Tableau 1(
A
b
I
2
4
2
1
1 0 0
1
2
3
1
0 1 0
&1 1 &1
1
0 0 1
and perform the same row operations as before. Then Tableau 7 becomes:
Tableau 7(
I
A &1b
A &1
1 0 0
&5/12
5/12 &1/2 &2/3
0 1 0
1/2
1/6
0
1/3
0 0 1
1/4
&1/4
1/2
0
This is how matrices were inverted and system of linear equations were solved fifty and
more years ago, using only a mechanical calculator. Nowadays of course you would use a
computer, but the Gauss-Jordan method is still handy and not too time consuming for small
matrices like the one in this example.
I.7.
Gaussian elimination of a non-square matrix
The Gaussian elimination of a non-square matrix is similar to the square case, except that
in the final result the upper-triangular matrix now becomes an echelon matrix:

357
Definition I.10: An  
 matrix U is an echelon matrix if for i = 2,...,m  the first non-zero
m × n
element of row i is farther to the right than the first non-zero element of the previous row i!1.
Theorem I.8 can now be generalized to:
Theorem I.11: For each matrix A there exists a permutation matrix P, possibly equal to the unit
matrix I, a lower-triangular matrix L with diagonal elements all equal to 1, and an echelon
matrix U, such that PA = LU. If A is a square matrix then U is an upper-triangular matrix.
Moreover, in that case PA = LDU, where now U is an upper-triangular matrix with diagonal
elements all equal to 1, and D is a diagonal matrix.8
Again, I will only prove the general part this theorem by examples. The parts for square
matrices follow trivially from the general case.  
First, let 
A '
2
4
2
1
1
2
3
1
&1 1 &1 0
.
(I.56)
which is the matrix (I.32) augmented with an additional  column. Then it follows from (I.52) that
P2,3E3,1(½)E2,1(!½)A '
1
0 0
0.5
0 1
&0.5 1 0
2
4
2
1
1
2
3
1
&1 1 &1 0
'
2 4 2
1
0 3 0 1/2
0 0 2 1/2
' U,
(I.57)

358
where U is now an echelon matrix. 
As another example, take the transpose of the matrix A in (I.56):
A T '
2 1 &1
4 2
1
2 3 &1
1 1
0
.
(I.58)
Then
P2,3E4,2(&1/6)E4,3(1/4)E2,1(&2)E3,1(&1)E4,1(&1/2)A T '
2 1 &1
0 2
0
0 0
3
0 0
0
' U,
(I.59)
where again U is an echelon matrix.
I.8.
Subspaces spanned by the columns and rows of a matrix
The result in Theorem I.9 also reads as: A = BU, where B = P!1L is a non-singular matrix.
Moreover, note that the size of U is the same as the size of A, i.e., if A is an 
 matrix, then
m × n
so is U.  Denoting the columns of U  by u1,...,un, it follows therefore that the columns a1,...,an, of
A are equal to Bu1,...,Bun, respectively. This suggests that the subspace spanned by the columns
of A has the same dimension as the subspace spanned by the columns of U. To prove this
conjecture, let VA be the subspace spanned by the columns of A, and let VU  be the subspace
spanned by the columns of U.  Without loss or generality we may reorder the columns of A such 
that the first k columns a1,...,ak of A form a basis for VA . Now suppose that u1,...,uk are linear
dependent, i.e., there exist constants c1,...,ck  not all equal to zero such that 
But then
'k
j'1cjuj ' 0.
also 
 = 0, which by the linear independence of a1,...,ak implies that all the
'k
j'1cjBuj ' 'k
j'1cjaj

359
cj’s are equal to zero. Hence,  u1,...,uk are linear independent, and therefore the dimension of  VU  
is greater or equal to the dimension of VA . But since U = B!1A, the same argument applies the
other way around: the dimension of  VA   is greater or equal to the dimension of VU . Thus we
have:
Theorem I.12: The subspace spanned by the columns of A has the same dimension as the
subspace spanned by the columns of the corresponding echelon matrix U  in Theorem I.9.
Next, I will show that
Theorem I.13: The subspace spanned by the columns of AT is the same as the subspace spanned
by the columns of the transpose  UT of  the corresponding echelon matrix U  in Theorem I.9.
Proof: Let A be an 
 matrix. The equality A = BU implies that  
 The
m × n
A T ' U TB T.
subspace spanned by the columns of  AT consists of all vectors  
 for which there exists a
x 0 úm
vector  
 such that  
 and similarly the subspace spanned by the columns of   UT
c1 0 ún
x ' A Tc1,
consists of all vectors  
 for which there exists a vector  
 such that  
x 0 úm
c2 0 ún
x ' U Tc2.
Letting  
 the theorem follows. Q.E.D.
c2 ' B Tc1
Now let us have a closer look at a typical echelon matrix:

360
U '
0 þ 0 ( þ ( ( þ ( ( þ ( ( þ (
0 þ 0
0 þ 0 ( þ ( ( þ ( ( þ (
0 þ 0
0 þ 0
0 þ 0 ( þ ( ( þ (
0 þ 0
0 þ 0
0 þ 0
0 þ 0 ( þ (
! " ! ! " ! ! " ! ! " ! ! þ !
0 þ 0
0 þ 0
0 þ 0
0 þ 0
0 þ 0
,
(I.60)
where each smiley face ( (called a pivot) indicates the first nonzero elements of the row
involved,  and the *’s indicate possible nonzero elements. Since the elements below the pivot in
each column with a smiley face  ( are zero, the columns involved are linear independent. In
particular, it is impossible to write the last column with a pivot as a linear combination of the
other ones. Moreover, it is easy to see that all the columns without a pivot can be formed as
linear combinations of the columns with a pivot. Consequently, the columns of U with a pivot
form a basis for the subspace spanned by the columns of U. But the transpose UT of U is also an
echelon matrix, and the number of rows of U with a pivot is the same as the number of columns
with a pivot, hence:
Theorem I.14: The dimension of the subspace spanned by the columns of an echelon matrix U is
the same as the dimension of the subspace spanned by the columns of its transpose UT.
Combining Theorems I.11, I.12 and I.13, it follow now that Theorem I.6 holds..
The subspace spanned by the columns of a matrix A is called the column space of A, and
is denoted by R(A). The row space of A is the space spanned by the columns of AT, i.e., the row
space of A is R(AT).  Theorem I.14 implies that the dimension of  R(A)  is equal to the

361
dimension of  R(AT). 
There is also another space associated with a matrix A, namely the null space of A,
denoted by N(A). This the space of all vectors x for which Ax = 0, which is also a subspace of a
vector space. If A is square and non-singular, then N(A) = {0}, but if not it follows from
Theorem I.12 that N(A) = N(U), where U is the echelon matrix in Theorem I.12. 
In order to determine the dimension of N(U), suppose that A is an m × n matrix with
rank r, and thus U is an m × n matrix with rank r. Let R be an n × n permutation matrix such that
the first r columns of UR are the r columns of U with a pivot. Clearly, the dimension of N(U) is
the same as the dimension of N(UR). We can partition UR as (Ur, Un!r), where Ur is the m × r
matrix consisting of  the columns of U with a pivot, and Un!r is the m × (n!r) matrix consisting
of the other columns of U. Partitioning a vector x in N(UR) accordingly, i.e.,  x ' (x T
r ,x T
n&r)T,
we have
URx ' Urxr % Un&rxn&r ' 0.
(I.61)
It follows from Theorem I.5 that 
 is invertible, hence it follows from (I.61) and the
U T
r Ur
partition 
 that 
x ' (x T
r ,x T
n&r)T
x '
&(U T
r Ur)&1U T
r Un&r
In&r
xn&r.
(I.62)
Therefore, N(UR) is spanned by the columns of the matrix in (I.62), which has rank n!r, and
thus the dimension of N(A) is  n!r. By the same argument it follows that the dimension of

362
N(AT) is  m!r. 
The subspace N(AT) is called the left null space of A, because it consists of all vectors y
for which y TA ' 0T.
Summarizing, it has been shown that the following results hold.
Theorem I.15: Let A be an m × n matrix with rank r. Then  R(A) and R(AT) have dimension r,
N(A) has dimension n!r, and N(AT) has dimension   m!r.
Note that in general the rank of a product AB is not determined by the ranks r and s of A
and B, respectively. At first sight one might guess that the rank of  AB is min(r,s), but that is in
general not true. For example, let A = (1,0)  and BT = (0,1). Then A and B have rank 1, but AB =
0, which has rank zero. The only thing we know for sure is that the rank of AB cannot exceed
min(r,s). Of course, if A and B are conformable invertible matrices, then AB is invertible, hence
the rank of AB is equal to the rank of A and the rank of B, but that is a special case. The same
applies to the case in Theorem I.5.
I.9.
Projections, projection matrices, and idempotent matrices
Consider the following problem: Which point on the line through the origin and point a in
Figure I.3 is the closest to point b? The answer is: point p in Figure I.4 below. The line through b
and p is perpendicular to the subspace spanned by a, and therefore the distance between b and
any other point  in this subspace is larger than the distance between b and p. Point p is called the

363
projection of b on the subspace spanned by a. In order to find p, let p = c.a, where c is a scalar.
The distance between b and p is now 
, so the problem is to find the scalar c which
2b & c.a2
minimizes this distance. Since
 is minimal if and only if
2b & c.a2
2b & c.a22 ' (b&c.a)T(b&c.a) ' b Tb & 2c.a Tb % c 2a Ta
is minimal, the answer is: 
 hence 
c ' a Tb/a Ta,
p ' (a Tb/a Ta).a.
Figure I.4: Projection of b on the
subspace spanned by a
 
Similarly, we can project a vector y in 
 on the subspace of  
  spanned by a basis
ún
ún
{x1,...,xk}, as follows. Let X be the n× k matrix with columns x1,...,xk. Any point p in the column
space   R(X) of X can be written as  p = Xb, where 
 Then the squared distance between y
b 0 úk.
and p = Xb is
2y & Xb22 ' (y & Xb)T(y & Xb) ' y Ty & b TX Ty & y TXb % b TX TXb
' y Ty & 2b TX Ty % b TX TXb,
(I.63)
where the last equality follows from the fact that  
 is a scalar (or equivalently,  a 1×1
y TXb
matrix), hence 
Given X and y, (I.63) is a quadratic function of b.
y TXb ' (y TXb)T ' b TX Ty.

364
The first-order condition for a  minimum of (I.63) is given by
M2y & Xb22
Mb T
' &2X Ty % 2X TXb ' 0,
(I.64)
which has solution
b ' (X TX)&1X Ty.
(I.65)
Thus, the vector p in  R(X) closest to y is 
p ' X(X TX)&1X Ty,
(I.66)
which is the projection of  y on   R(X) . 
Matrices of the type in (I.66) are called projection matrices: 
Definition I.11: Let A be an  n× k matrix with rank k. Then the   n× n  matrix P = 
 
A(A TA)&1A T
is called a projection matrix: For each vector x in 
,  Px is the projection of x on the column
ún
space of A.
Note that this matrix P is such that  
 This
PP ' A(A TA)&1A TA(A TA)&1A T) ' A(A TA)&1A T ' P.
is not surprising, though, because  p = Px is already in  R(A), hence the point in  R(A) closest to
p is p itself. 
Definition I.12: An n× n matrix M is called idempotent if MM = M.
Thus, projection matrices are idempotent.

365
I.10.
Inner product, orthogonal bases, and orthogonal matrices
It follows from (I.10) that the cosines of the angle N between the vectors x in (I.2) and y
in  (I.5) is
cos(φ) '
'n
j'1xjyj
2x2.2y2 '
x Ty
2x2.2y2 .
(I.67)
Definition I.13: The quantity 
 is called the inner product of the vectors x and y.
x Ty
If  
 then cos(N) = 0, hence N = B/2 or N = 3B/4. This corresponds to an angle of
x Ty ' 0
90 degrees and 270 degrees, respectively, hence x and y are then perpendicular. Such vectors are
said to be orthogonal.
Definition I.14: Conformable vectors x and y are orthogonal if their inner product 
 is zero.
x Ty
Moreover, they are orthonormal if in addition their lengths are 1: 2x2 ' 2y2 ' 1.
If we flip in Figure I.4 point p over to the other side of the origin along the line through
the origin and point a, and add b to !p, then the resulting vector c = b ! p is perpendicular to the
line through the origin and point a. This is illustrated in Figure I.5. More formally,
a Tc ' a T(b&p) ' a T(b & (a Tb/2a22)a ' a Tb & (a Tb/2a22)2a22 ' 0.

366
Figure I.5: Orthogonalization
This procedure can be generalized to convert any basis of a vector space into an
orthonormal basis, as follows. Let 
be a basis for a subspace of 
, and let
a1,......,ak, k # n,
ún
 The projection of  
 is now  
 hence
q1 ' 2a12&1.a1.
a2 on q1
p ' (q T
1 a2).q1,
 is orthogonal to 
. Thus, let  
 The next step is to erect 
a (
2 ' a2 & (q T
1 a2).q1
q1
q2 ' 2a (
2 2&1a (
2 .
 perpendicular to 
 and 
, which can be done by subtracting from 
 its projections on  
a3
q1
q2
a3
q1
and 
:  
 Using the facts that by construction,
q2
a (
3 ' a3 & (a T
3 q1)q1 & (a T
3 q2)q2.
q T
1 q1 ' 1, q T
2 q2 ' 1, q T
1 q2 ' 0, q T
2 q1 ' 0,
we have indeed that  
 and
q T
1 a (
3 ' q T
1 a3 & (a T
3 q1)q T
1 q1 & (a T
3 q2)q T
1 q2 ' q T
1 a3 & a T
3 q1 ' 0
similarly, 
 Thus, let now  
 Repeating this procedure yields :
q T
2 a (
3 ' 0.
q3 ' 2a (
3 2&1a (
3 .
Theorem I.16: Let 
 be a basis for a subspace of  
, and construct  
a1,......,ak
ún
q1,......,qk
recursively by:

367
q1 ' 2a12&1.a1 and a (
j ' aj & j
j&1
i'1
(a T
j qi)qi, qj ' 2a (
j 2&1a (
j
for j ' 2,3,....,k.
(I.68)
Then 
 is an orthonormal basis for the subspace spanned by 
.
q1,......,qk
a1,......,ak
The construction (I.68) is known as the Gram-Smidt process. The orthonormality of q1,......,qk
has already been shown, but it still has to be shown that 
 spans the same subspace as
q1,......,qk
. To show the latter, observe from (I.68) that 
 is related to 
 by
a1,......,ak
a1,......,ak
q1,......,qk
aj ' j
j
i'1
ui,jqi, j ' 1,2,...,k,
(I.69)
where
uj,j ' 2a (
j 2, ui,j ' q T
i aj for i < j, ui,j ' 0 for i > j, i,j ' 1,....,k,.
(I.70)
 with 
 It follows now from (I.69) that 
 are linear combinations of 
,
a (
1 ' a1.
a1,......,ak
q1,......,qk
and it follows from (I.68) that 
  are linear combinations of 
, hence the two
q1,......,qk
a1,......,ak
bases span the same subspace. 
Observe from (I.70) that the k×k  matrix U with elements ui,j is an upper triangular matrix
with positive diagonal elements. Moreover, denoting by A the n×k matrix with columns 
 and by Q the n×k matrix with columns 
  it follows from (I.69) that A = QU.
a1,......,ak
q1,......,qk
Thus it follows from Theorem I.16,  (I.69) and (I.70):
Theorem I.17: Let A be an  n×k matrix with rank k. There exists an  n×k matrix Q with
orthonormal columns, and an upper triangular  k×k  matrix U with positive diagonal elements,
such that A = QU.
In the case k = n the matrix Q in Theorem I.17 is called an orthogonal matrix:

368
Definition I.15: An orthogonal matrix Q is a square matrix with orthonormal columns: QTQ = I.
In particular, if Q is an orthogonal n×n matrix with columns 
 then the elements
q1,......,qn
of the matrix QTQ are 
 where I(.) is the indicator function9, hence QTQ = In.
q T
i qj ' I(i ' j),
Thus QT = Q!1. It follows now from Theorem I.1 that also QQT = In, i.e, the rows of an
orthogonal matrix are also orthonormal.
Orthogonal transformations of vectors leave the angles between the vectors, and their
lengths,  the same. In particular, let x and y be vectors in 
, and let Q be an orthogonal n×n
ún
matrix. Then 
 hence it
(Qx)T(Qy) ' x TQ TQy ' x Ty, 2Qx2 '
(Qx)T(Qx) '
x Tx ' 2x2,
follows from (I.67) that the angle between Qx and Qy is the same as the angle between x and y. 
In the case n = 2 the effect of an orthogonal transformation is a rotation. A typical
orthogonal 2×2 matrix takes the form
Q '
cos(θ)
sin(θ)
sin(θ) &cos(θ)
(I.71)
This matrix transforms the unit vector e1 = (1, 0)T  into the vector 
 and it
qθ ' (cos(θ),sin(θ))T,
follows from  (I.67) that 2 is the angle between the two. By moving 2 from 0 to 2B the vector qθ
rotates anti-clockwise from the initial position e1 back to e1. 

369
I.11.
Determinants: Geometric interpretation and basic properties
The area enclosed by the parallelogram in Figure I.3  has a special meaning, namely the
determinant of the matrix
A ' (a,b) '
a1 b1
a2 b2
'
6 3
4 7 .
(I.72)
The determinant is denoted by det(A). This area is two times the area enclosed by the triangle
formed by the origin and the points  a and b in  Figure I.3, which in its turn is the sum of the
areas enclosed by the triangle formed by the origin, point b, and the projection 
 
p ' (a Tb/a Ta).a ' (a Tb/2a22).a
of  b on a, and the triangle formed by the points p, a, and b, in Figure I.4. The first triangle has
area  ½
 times the distance of p to the origin, and the second triangle has area equal to
2b & p2
½
times the distance between p and a, hence the determinant of A is:
2b & p2
det(A) ' 2b & p2.2a2 ' 2b & (a Tb/2a22)2.2a2 '
2a222b22 & (a Tb)2
'
(a 2
1 %a 2
2 )(b 2
1 %b 2
2 ) & (a1b1%a2b2)2 '
(a1b2 & b1a2)2 ' ±*a1b2 & b1a2*
' a1b2 & b1a2.
(I.73)
The latter equality is a matter of normalization, as  
would also fit (I.73), but the
&(a1b2 & b1a2)
chosen normalization is appropriate for (I.72), because then
det(A) ' a1b2 & b1a2 ' 6×7 & 3×4 ' 30.
(I.74)
However, as I will show below, a determinant can be negative or zero. 
Equation (I.73) reads in  words:

370
Definition I.16: The determinant of a 2×2 matrix is the product of the diagonal elements minus
the product of the off-diagonal elements.
We can also express (I.73) in terms of the angles Na and Nb  of the vectors a and b,
respectively,  with the right hand side of the horizontal axis: 
a1 ' 2a2cos(φa), a2 ' 2a2sin(φa),
b1 ' 2b2cos(φb), b2 ' 2b2sin(φb),
hence
det(A) ' a1b2 & b1a2 ' 2a2.2b2. cos(φa)sin(φb) & sin(φa)cos(φb)
' 2a2.2b2.sin(φb & φa)
(I.75)
Since in Figure I.3,  
 we have that  
0 < φb & φa < π,
sin(φb & φa) > 0.
As an example of a negative determinant, let us swap the columns of A, and call the result
matrix B:
B ' AP1,2 ' (b,a) '
b1 a1
b2 a2
'
3 6
7 4
,
(I.76)
where
P1,2 '
0 1
1 0
is the elementary permutation matrix involved. Then
det(B) ' b1a2 & a1b2 ' &30.
(I.77)
At first sight this looks odd, because it seems that the area enclosed by the parallelogram in
Figure I.3 has not been changed. However, it has! Recall the interpretation of a matrix as a

371
mapping: A matrix moves a point to a new location, by replacing the original perpendicular
coordinate system by a new system formed by the columns space of the matrix involved, with
new units of measurement  the lengths of the columns. In the case of the matrix B in (I.76)  we
have:
Unit vectors
Axis
Original
New
1:
e1 '
1
0
6 b '
3
7
2:
e2 '
0
1
6 a '
6
4
Thus,  b is now the first unit vector, and a is the second. If we adopt the convention that the
natural position of unit vector 2 is above the line spanned by the first unit vector,  as is the case
for e1 and e2, then we are actually looking at the parallelogram in Figure I.3 from the backside, as
in Figure I.6:
Figure I.6: Backside of Figure I.3
Thus, the effect of swapping the columns of the matrix A in (I.72) is that  Figure I.3 is flipped

372
over vertically 180 degrees. Since we are now looking at Figure I.3 from the back, which is the
negative side, the area enclosed by the parallelogram is negative too! Note that this corresponds
to (I.75): If we swap the columns of A, then we swap the angles Na and Nb   in (I.75), and
consequently the determinant flips sign.
Figure I.7: det(a,b) > 0
Figure I.8: det(a,b) < 0

373
As another example, let a be as before, but now position b in the south-west quadrant, as
in Figure I.7 and Figure I.8. The fundamental difference between these two cases is that in Figure
I.7 point b is above the line through a and the origin, so that  Nb ! Na < B,  whereas in Figure I.8
point b is below that line:  Nb ! Na  > B. Therefore, the area enclosed by the parallelogram in
Figure I.7 is positive, whereas the area enclosed by the paralelllogram in Figure I.8 is negative.
Hence in the case of Figure I.7, det(a,b) > 0, and in the case of Figure I.8,  det(a,b) <  0. Again, in
Figure I.8 we are looking at the backside of the picture; you have to flip it vertically to see the
front side.
What I have demonstrated here for 2×2 matrices is that if the columns are interchanged
then the determinant changes sign. It is easy to see that the same applies to the rows. This
property holds for general n×n matrices as well, in the following way.
Theorem I.18: If two adjacent columns or rows of a square matrix are swapped10 then the
determinant changes sign only.
Next, let us consider determinants of special 2×2 matrices. The first special case is the
orthogonal matrix. Recall that the columns of an orthogonal matrix are perpendicular, and have
unit length. Moreover, recall that an orthogonal 2×2 matrix rotates a set of points around the
origin, leaving angles and distances the same. In particular, consider the set of points in the unit
square formed by the vectors (0,0)T, (0,1)T, (1,0)T and (1,1)T. Clearly, the area of this unit square
equals 1, and since the unit square corresponds to the  2×2 unit matrix I2., the determinant of I2
equals 1. Now multiply I2 by an orthogonal matrix Q. The effect is that the unit square is rotated,

374
without affecting its shape or size. Therefore, 
Theorem I.19: The determinant of an orthogonal matrix is either 1 or -1, and the determinant of
a unit matrix is 1.
The "either-or" part  follows from Theorem I.18: swapping adjacent columns of an orthogonal
matrix preserves orthonormality of the columns of the new matrix, but switches the sign of the
determinant. For example, consider the orthogonal matrix Q in (I.71). Then it follows from
Definition I.16 that
det(Q) ' &cos2(θ) & sin2(θ) ' &1.
Now swap the columns of the matrix  (I.71):
Q '
sin(θ) &cos(θ)
cos(θ)
sin(θ)
.
Then it follows from Definition I.16 that
det(Q) ' sin2(θ) % cos2(θ) ' 1.
Note that Theorem I.19 is not confined to the  2×2 case: it is true for orthogonal and unit
matrices of any size.
Next, consider the lower-triangular matrix
L '
a 0
b c .
(I.78)
According to Definition I.16, det(L) = a.c - 0.c = a.c, so that in the  2×2 case the determinant of a

375
lower-triangular matrix is the product of the diagonal elements. This is illustrated in Figure I.9
below.  The determinant of L is the area in the parallelogram, which is the same as the area in the
rectangle formed by the vectors (a,0)T and (0,c)T .  This area is a.c. Thus, you can move b freely
along the vertical axis without affecting the determinant of L. If you would flip the picture over
vertically, which corresponds to replacing a by -a, the parallelogram will be viewed from the
backside, hence the determinant flips sign. 
Figure I.9: det(L)
The same result applies of course to upper-triangular and diagonal 2×2 matrices. Thus we have:
Theorem I.20: The determinant of a lower-triangular matrix is the product of the diagonal
elements. The same applies to an upper-triangular matrix and a diagonal matrix.
Again, this result is not confined to the  2×2 case, but holds in general.
Now consider the determinant of a transpose matrix. In the  2×2 case the transpose AT of
A can be formed by first swapping the columns and then swapping the rows. Then it follows
from Theorem I.18 that in each of the two steps only the sign flips, hence

376
Theorem I.21: det(A) = det(AT). 
The same applies to the general case: the transpose of A can be formed by a sequence of column
exchanges and a corresponding sequence of row exchanges, and the total number of column and
row exchanges is an even number.
It follows from Theorem I.11 that in the case of a square matrix A there exist a
permutation matrix P, possibly equal to the unit matrix I, a lower-triangular matrix L with
diagonal elements all equal to 1, a diagonal matrix D, and an upper-triangular matrix U with
diagonal elements all equal to 1, such that PA = LDU.  Moreover, recall that a permutation
matrix is orthogonal, because it consists of permutations of the columns of the unit matrix. Thus
we can write A = PTLDU.
Now consider the parallelogram formed by the columns of U. Since the diagonal elements
of U are 1, the area of this parallelogram is the same as the area of the unit square: det(U) =
det(I). Therefore, the effect of the transformation  PTLD on the area of the parallelogram formed
by the columns of U is the same as the effect of PTLD on the area of the unit square, and
consequently det(PTLDU) = det(PTLD).  The effect of multiplying D  by L is that the rectangle
formed by the columns of D  is tilted and squeezed, without affecting the area itself. Therefore,
det(LD) = det(D), and consequently det(PTLDU) = det(PTD).   Next, PT permutates the rows of D,
so the effect on det(D) is a sequence of sign switches only. The number of sign switches involved
is the same as the number of column exchanges of PT necessary to convert  PT into the unit
matrix. If this number of swaps is even, then det(P) = det( PT) = 1, else det(P) = det(PT) = -1.
Thus, in the 2×2 case (as well as in the general case) we have. 

377
Theorem I.22: det(A) = det(P).det(D), where P and D are the permutation matrix and the
diagonal matrix, respectively, in the decomposition PA = LDU in Theorem I.11 for the case of a
square matrix A.
This result yields two important corollaries. First:
Theorem I.23: The determinant of a singular matrix is zero. 
To see this, observe from the decomposition PA = LDU that A is singular if and only if D is
singular. If D is singular then at least one of the diagonal elements of D is zero, hence det(D) = 0.
Second, for conformable square matrices A and B we have
Theorem I.24: det(AB) = det(A).det(B).
This result can be shown in the same way as Theorem I.22, i.e., by showing that det(A) =
det(PTLDUB) = det(P).det( DB) and det(DB) = det(D).det(B).
Moreover, Theorems I.20 and I.24 imply that
Theorem I.25: Adding or subtracting a constant times a row or column to another row or
column, respectively, does not change the determinant.
The reason is that this operation is equivalent to multiplying a matrix by an elementary matrix,

378
and that an elementary matrix is triangular with diagonal elements equal to 1.
Furthermore, we have:
Theorem I.26: Let A be an n×n matrix and let c be a scalar.  If one of the columns or rows is
multiplied by c, then the determinant of the resulting matrix is c.det(A). Consequently,  det(c.A) =
cn.det(A).
This theorem follows straightforwardly from Theorems I.20 and I.24. For example, let B be a
diagonal matrix with diagonal elements 1, except for one element, say diagonal element i, which
equals  c. Then BA is the matrix A with the i-th column multiplied by c. Since by Theorem I.20,
det(B) = c, the first part of Theorem I.26 for the "column" case follows from Theorem I.24, and
the "row" case follows from det(AB) = det(A).det(B) = c.det(A).  The second part follows by
choosing B = c.In..
The results in this section merely serve as a motivation for what a determinant is, and its
geometric interpretation and basic properties. All the results so far can be derived from three
fundamental properties, namely the results in Theorems I.18, I.20 and I.21. If we would assume
the that the results in Theorems I.18, I.20 and I.21 hold, and treat these properties as axioms, all
the other results follow from these properties and the decomposition PA = LDU. Moreover, the
function involved is unique. 
As to the latter, suppose that *(A) is a function satisfying
(a)
If two adjacent rows  or columns are swapped then  * switches sign only.
(b)
If A is triangular then  *(A) is the product of the diagonal elements of A.

379
(c)
*(AB) = *(A). *(B)
Then it follows from the decomposition A = PTLDU and axiom (c) that 
*(A) = *(PT)*(L)*(D)*(U). 
Moreover, it follows from axiom (b)  that *(L) = *(U) =1 and *(D) = det(D).  Finally, it follows
from axiom (b) that  the functions *(.) and det(.) coincide for unit matrices, so that by axiom (a),
*(PT) = *(P) = det(P). Thus, *(A) = det(A), hence, the determinant is uniquely defined by the
axioms (a), (b) and (c). Therefore,
Definition I.17: The determinant of a square matrix is uniquely defined by three fundamental
properties:
(a)
If two adjacent rows or columns are swapped then the determinant switches sign only.
(b)
The determinant of a triangular matrix is the product of the diagonal elements.
(c)
The determinant of AB is the product of the determinants of A and B.
These three axioms can be used to derive a general expression for the determinant, together with
the results below regarding determinants of block-triangular matrices.

380
I.12.
Determinants of block-triangular matrices
Consider a square matrix A partitioned as
A '
A1,1 A1,2
A2,1 A2,2
.
(I.79)
where 
 and  
 are sub-matrices of size k×k and m×m, respectively,  
 is a k×m matrix
A1,1
A2,2
A1,2
and 
 is an m×k matrix.  This matrix A is block-triangular if either  
 or 
is a zero
A2,1
A1,2
A2,1
matrix, and it is block-diagonal if both  
 and
 are zero matrices. In the latter case 
A1,2
A2,1
A '
A1,1
O
O
A2,2
,
(I.80)
where the two O blocks represent zero elements. For each block  
 and  
  we can apply
A1,1
A2,2
Theorem I.11, i.e. 
 hence
A1,1 ' P T
1 L1D1U1,
A2,2 ' P T
2 L2D2U2,
A '
P T
1 L1D1U1
O
O
P T
2 L2D2U2
'
P1
O
O P2
T
.
L1 O
O L2
.
D1
O
O
D2
U1
O
O
U2
' P TLDU,
(I.81)
say. Then det(A) = det(P).det(D) = det(P1).det(P2).det(D1).det(D2) = 
 More
det(A1,1).det(A2,2).
generally, we have that
Theorem I.27: The determinant of a block-diagonal matrix is the product of the determinants of
the diagonal blocks.
Next, consider the lower block-diagonal matrix

381
A '
A1,1
O
A2,1 A2,2
.
(I.82)
where again 
 and  
 are k×k and m×m matrices, respectively, and 
 is an m×k matrix.
A1,1
A2,2
A2,1
Then it follows from Theorem I.25 that for any  k×m matrix C,
det(A) ' det
A1,1
O
A2,1&CA1,1 A2,2
.
(I.83)
If 
 is nonsingular, then we can choose 
 so that   
 In that case it
A1,1
C ' A &1
1,1A2,1
A2,1&CA1,1 ' O.
follows from Theorem I.27 that det(A) = 
 If 
 is singular, then the rows of
det(A1,1).det(A2,2).
A1,1
 are linear dependent, and so are the first k rows of A. Hence, if  
 is singular then A is
A1,1
A1,1
singular, so that by Theorem I.23, det(A) = 
 Thus
det(A1,1).det(A2,2) ' 0.
Theorem I.28: The determinant of a block-triangular matrix is the product of the determinants
of the diagonal blocks.
I.13.
Determinants and co-factors
Consider the 
 matrix
n × n
A '
a1,1 þ a1,n
!
"
!
an,1 þ an,n
(I.84)
and define the following matrix-valued function of A:
Definition I.18: The transformation 
 is a matrix formed by replacing in rows k
ρ(A|i1,i2,.....,in)

382
= 1,..,n of matrix (I.84) all but the ik’s element 
by zeros. Similarly, the transformation 
ak,ik
 is a matrix formed by replacing in columns k = 1,..,n of matrix (I.84) all but the
κ(A|i1,i2,.....,in)
ik’s element 
by zeros.
aik,k
For example, in the 3×3 case, 
ρ(A|2,3,1) '
0
a1,2
0
0
0
a2,3
a3,1
0
0
,
κ(A|2,3,1) '
0
0
a1,3
a2,1
0
0
0
a3,2
0
.
Recall that a permutation of the numbers 1,2,....,n is an ordered set of these n  numbers,
and that there are n! of these permutations, including the trivial permutation 1,2,...,n. Moreover,
it is easy to verify that for each permutation 
 of  1,2,....,n  there exists a unique
i1,i2,.....,in
permutation
 such that  
 = 
 and vice versa. Now
j1,j2,.....,jn
ρ(A|i1,i2,.....,in)
κ(A|j1,j2,.....,jn)
define the function
δ(A) ' j det[ρ(A|i1,i2,.....,in)] ' j det[κ(A|i1,i2,.....,in)],
(I.85)
where the summation is over all permutations  
 of  1,2,....,n. 
i1,i2,.....,in
Note that 
 where the sign depends on how
det[ρ(A|i1,i2,.....,in)] ' ±a1,i1a2,,i2.....an,in,
many row or column exchanges are needed to convert 
into a diagonal matrix. If
ρ(A|i1,i2,.....,in)
the number of exchanges is even, the sign is + and the sign is ! if this number is odd. Clearly,
this sign is the same as the sign of the determinant of the permutation matrix ρ(Εn|i1,i2,.....,in)],
where +n is the 
 matrix with all elements equal to 1. 
n × n
I will show now that  *(A) in (I.85) satisfies the axioms in Definition I.17, so that:

383
Theorem I.29: The function *(A) in (I.85)  is the determinant of A:  *(A) = det(A). 
Proof: First, exchange rows of A, say rows 1 and 2. The new matrix is P12A, where P12 is
the elementary permutation matrix involved, i.e., the unit matrix with the first two columns
exchanged. Then 
, hence 
=
ρ(P12A|i1,i2,.....,in) ' P12ρ(A|i1,i2,.....,in)
δ(P12A) ' det(P1,2)δ(A)
Thus, *(A) satisfies axiom (a) in Definition I.17.
&δ(A).
Second, let A be lower-triangular. Then 
 is lower-triangular, but has at
ρ(A|i1,i2,.....,in)
least one zero diagonal element for all permutations   
  except for the trivial
i1,i2,.....,in
permutation 1,2,....,n. Thus in this case 
The same applies
δ(A) ' det[ρ(A|1,,2,....,n) ' det(A).
of course to upper-triangular and diagonal matrices. Consequently  *(A) satisfies axiom (b) in
Definition I.17.
Finally, observe that 
 is a matrix with elements 
 in position
ρ(AB|i1,i2,.....,in)
'n
k'1am,kbk,im
(m,im), m = 1,....,n, and zeros elsewhere. Hence
 
 = 
ρ(AB|i1,i2,.....,in)
A.ρ(B|i1,i2,.....,in),
which implies that
δ(AB) ' det(A).δ(B).
(I.86)
Now write B as B = PTLDU , and observe from  (I.86) and axiom (b) that 
δ(B) ' δ (P TLD)U ' det(P TLD)δ(U) ' det(P TLD)det(U) ' det(B).
The same applies to A. Thus, 
δ(AB) ' det(A).det(B) ' δ(A).δ(B).
(I.87)
Q.E.D.
Next, consider the transformation:

384
Definition I.19: The transformation 
 is a matrix formed by replacing all elements in
τ(A|k,m)
row k and column m by zeros, except element ak,m itself.
For example, in the 3×3 case, 
τ(A|2,3) '
a1,1 a1,2
0
0
0
a2,3
a3,1 a3,2
0
.
(I.88)
Then it follows from (I.85)  and Theorem I.29 that
det[τ(A|k,m)] ' j
ik'm
det[ρ(A|i1,i2,.....,in)] ' j
ik'k
det[κ(A|i1,i2,.....,in)]
(I.89)
hence:
Theorem I.30: For n×n matrices A,
 and 
det(A) ' 'n
m'1det[τ(A|k,m)] for k ' 1,2,....,n,
det(A) ' 'n
k'1det[τ(A|k,m)] for m ' 1,2,....,n.
Now let us evaluate the determinant of the matrix (I.88). Swap rows 1 and 2, and then
swap recursively columns 2 and 3 and columns 1 and 2. The total number of row and column
exchanges  is 3, hence
det[τ(A|2,3)] ' (&1)3det
a2,3
0
0
0
a1,1 a1,2
0
a3,1 a3,2
' a2,3(&1)2%3det
a1,1 a1,2
a3,1 a3,2
' a2,3cof2,3(A),
(I.90)
say, where 
 is the co-factor of element 
Note that the second equality follows
cof2,3(A)
a2,3 of A.

385
from Theorem I.27. Similarly, we need k-1 row exchanges and m-1 column exchanges to convert
 into a block-diagonal matrix. More generally: 
τ(A|k,m)
Definition I.20: The co-factor 
 of an n×n matrix A  is the determinant of the 
cofk,m(A)
(n!1)×(n!1)  matrix formed by deleting row k  and column m, times (&1)k%m.
Thus, Theorem I.30 now reads as:
Theorem I.31: For n×n matrices A, 
 and also
det(A) ' 'n
m'1ak,mcofk,m(A) for k ' 1,2,....,n,
det(A) ' 'n
k'1ak,mcofk,m(A) for m ' 1,2,....,n.
I.14.
Inverse of a matrix in terms of  co-factors
Theorem I.31 now enables us to write the inverse of a matrix A in terms of co-factors and
the determinant, as follows. Define
Definition I.20: The matrix
Aadjoint '
cof1,1(A) þ cofn,1(A)
!
"
!
cof1,n(A) þ cofn,n(A)
(I.91)
is called the adjoint matrix of A.
Note that the adjoint matrix is the transpose of the matrix of co-factors with typical (i,j)’s

386
element cofi,j(A).
Now observe from Theorem I.31 that 
 is just diagonal element i
det(A) ' 'n
k'1ai,kcofi,k(A)
of 
 Moreover, suppose that row j of A is replaced by row i, and call this matrix B. This
A.Aadjoint.
has no effect on 
 but 
 is now the determinant of B,
cofj,k(A),
'n
k'1ai,kcofj,k(A) ' 'n
k'1ai,kcofi,k(B)
and since the rows of B are linear dependent, det(B) = 0. Thus we have:
'n
k'1ai,kcofj,k(A) ' det(A) if i ' j,
' 0 if i … j,
(I.92)
hence:
Theorem I.32: If det(A) … 0 then A &1 '
1
det(A)
Aadjoint.
Note that the co-factors 
do not depend on ai,j. It follows therefore from Theorem
cofj,k(A)
I.31 that
Mdet(A)
Mai,j
' cofi,j(A).
(I.93)
Using the well-known fact that 
 it follows now from Theorem I.32 and (I.93)
dln(x)/dx ' 1/x
that 
Theorem I.33: If det(A)  > 0 then

387
Mln[det(A)]
MA
'
def.
Mln[det(A)]
Ma1,1
þ
Mln[det(A)]
Man,1
!
"
!
Mln[det(A)]
Ma1,n
þ
Mln[det(A)]
Man,n
' A &1.
(I.94)
Note that Theorem I.33 generalizes the formula  
 to matrices. This result will be
dln(x)/dx ' 1/x
useful in deriving the maximum likelihood estimator of the variance matrix of the multivariate
normal distribution.
I.15.
Eigenvalues and eigenvectors
I.15.1 Eigenvalues
Eigenvalues and eigenvectors play a key role in modern econometrics, in particular in
cointegration analysis. These econometric applications are confined to eigenvalues and
eigenvectors of symmetric matrices, i.e., square matrices A for which A = AT.  Therefore, I will
mainly focus on the symmetric case.
Definition I.21: The eigenvalues11 of an n×n matrix A are the solutions for 8 of the equation
det(A!8In) = 0. 
It follows from Theorem I.29 that 
 where the summation is
det(A) ' ' ±a1,i1a2,,i2.....an,in,
over all permutations  
 of  1,2,....,n. Therefore, replacing A by A ! 8In  it is not hard to
i1,i2,.....,in
verify that det(A ! 8In) is a polynomial of order n in 8: det(A ! 8In) = 
where the
'n
k'0 ckλk,

388
coefficients ck are functions of the elements of A. 
For example, in the 2×2 case 
A '
a1,1 a1,2
a2,1 a2,2
we have
det(A & λI2) ' det
a1,1 & λ
a1,2
a2,1
a2,2&λ
' (a1,1 & λ)(a2,2&λ) & a1,2a2,1
' λ2 & (a1,1 % a2,2)λ % a1,1a2,2 & a1,2a2,1,
which has two roots, i.e., the solutions of 
 = 0:
λ2 & (a1,1 % a2,2)λ % a1,1a2,2 & a1,2a2,1
λ1 '
a1,1 % a2,2 %
(a1,1 & a2,2)2 % 4a1,2a2,1
2
,
λ2 '
a1,1 % a2,2 &
(a1,1 & a2,2)2 % 4a1,2a2,1
2
.
There are three cases to be distinguished. If 
> 0 then 
 are
(a1,1 & a2,2)2 % 4a1,2a2,1
λ1 and λ2
different and real valued.  If  
 + 
 = 0  then 
 and real valued.
(a1,1 & a2,2)2
4a1,2a2,1
λ1 ' λ2
However, if  
<  0  then 
 are different but complex  valued: 
(a1,1 & a2,2)2 % 4a1,2a2,1
λ1 and λ2
λ1 ' a1,1 % a2,2 % i. &(a1,1 & a2,2)2 & 4a1,2a2,1
2
,
λ2 ' a1,1 % a2,2 & i. &(a1,1 & a2,2)2 & 4a1,2a2,1
2
,
where i = 
In this case  
 are complex conjugate: 
12  Thus, eigenvalues can
&1.
λ1 and λ2
λ2 ' ¯λ1.
be complex-valued!

389
Note that if the matrix A involved is symmetric :
 then 
a1,2 ' a2,1,
λ1 ' a1,1 % a2,2 %
(a1,1 & a2,2)2 % 4a 2
1,2
2
,
λ2 ' a1,1 % a2,2 &
(a1,1 & a2,2)2 % 4a 2
1,2
2
,
so that in the symmetric 2×2 case the eigenvalues are always real valued. It will be shown below
that this is true for all symmetric n×n  matrices.
I.15.2 Eigenvectors
By Definition I.21 it follows that if 8 is an eigenvalue of an  n×n matrix A, then A !8In  is
a singular matrix (possibly complex-valued!). Suppose first that  8 is real valued. Since the rows 
of  A !8In   are linear dependent there exists a vector x  0 ún such that (A !8In)x = 0 (0 ún ),
hence Ax  = 8x. Such a vector x is called an eigenvector of A corresponding to the eigenvalue 8.
Thus in the real eigenvalue case: 
Definition I.22: An eigenvector13 of an n×n matrix A corresponding to an eigenvalue 8 is a
vector x such that Ax = 8x.
However, this definition also applies to the complex eigenvalue case, but then the eigenvector x
has complex-valued components: x  0 ÷n.  To show the latter, consider the case that 8 is
complex-valued: 8 = " + i.$, ",$ 0 ú, $ … 0. Then  
 A !8In   =  A !"In   ! i.$In  

390
is complex-valued with linear dependent rows, in the following sense. There exist a vector x =
a+i.b with a,b  0 ún and length14 
 > 0,  such that
2x2 '
a Ta % b Tb
   (A !"In   ! i.$In)(a +  i.b)  = [(A !"In )a + $b] +  i.[(A !"In )b   !$a] =  0 (0 ún ).  
Consequently,  (A !"In )a + $b = 0 and (A !"In )b  ! $a = 0, and thus,
A&αIn
βIn
&βIn
A&αIn
a
b
'
0
0
0 ú2n.
(I.95)
Therefore, in order for the length of x to be positive, the matrix in (I.95) has to be singular, and
then 
 can be chosen from the null space of this matrix.
a
b
I.15.3 Eigenvalues and eigenvectors of symmetric matrices
On the basis of  (I.95) it is easy to show that in the case of a symmetric matrix A, $ = 0
and b = 0:  
Theorem I.34: The eigenvalues of a symmetric  n×n matrix A are all real valued, and the
corresponding eigenvectors are contained in ún.
Proof: First, note that  (I.95) implies that for arbitrary >  0 ú,
0 '
b
ξa
T A&αIn
βIn
&βIn
A&αIn
a
b
' ξa TAb % b TAa &αb Ta&ξαa Tb% βb Tb & ξβa Ta
(I.96)
Next observe that 
 and by symmetry,  
 where
b Ta ' a Tb
b TAa ' (b TAa)T ' a TA Tb ' a TAb,
the first equality follows from the fact that  
 is a scalar (or 1×1 matrix). Then we have for
b TAa

391
arbitrary  >  0 ú,
(ξ%1)a TAb & α(ξ%1)a Tb % β(b Tb & ξa Ta) ' 0.
(I.97)
If we choose  > = !1 in (I.97)  then  
 so that  $ = 0 and thus 8 = " 
β(b Tb % a Ta) ' β.2x22 ' 0,
0 ú. It is now easy to see that b no longer matters, so that we may choose b = 0. Q.E.D.
There is more to say about the eigenvectors of symmetric matrices, namely:
Theorem I.35: The eigenvectors of a symmetric n×n matrix A can be chosen orthonormal.
Proof: First assume that all the eigenvalues 
 of A are different. Let
λ1,λ2, ..... ,λn
 be the corresponding eigenvectors. Then for  
 and  
x1,x2, ..... ,xn
i … j, x T
i Axj ' λjx T
i xj
 because by symmetry,
x T
j Axi ' λix T
i xj, hence (λi &λj)x T
i xj ' 0,
 
x T
i Axj ' (x T
i Axj)T ' x T
j A Txi ' x T
j Axi.
Since 
 it follows now that 
 Upon normalizing the eigenvectors as
λi … λj,
x T
i xj ' 0.
 the result follows.
qj ' 2xj2&1xj
The case where two or more eigenvalues are equal requires a completely different proof.
First, normalize the eigenvectors as  
 Using the approach in Section I.10 we can
qj ' 2xj2&1xj.
always construct vectors  
0 ún  such that 
is an orthonormal basis of ún. 
y2, ... ,yn
q1,y2, ... ,yn
Then 
 is an orthogonal matrix. The first column of   
 is
Q1 ' (q1,y2, ... ,yn)
Q T
1 AQ1
 But by the orthogonality of  Q1 ,
Q T
1 Aq1 ' λQ T
1 q1.
 
q T
1 Q1 ' q T
1 (q1,y2, ... ,yn) ' (q T
1 q1,q T
1 y2, ... ,q T
1 yn) ' (1,0,0,....,0)
hence the first column of   
 is  
 and by symmetry of  
 the first
Q T
1 AQ1
(λ1,0,0,......,0)T
Q T
1 AQ1
row is 
. Thus  
 takes the form
(λ1,0,0,......,0)
Q T
1 AQ1

392
Q T
1 AQ1 '
λ1
0T
0
An&1
.
(I.98)
Next, observe that 
det(Q T
1 AQ1 & λIn) ' det(Q T
1 AQ1 & λQ T
1 Q1)
' det[Q T
1 (A & λIn)Q1] ' det(Q T
1 )det(A & λIn)det(Q1) ' det(A & λIn)
so that the eigenvalues of   
 are the same as the eigenvalues of A, and consequently the
Q T
1 AQ1
eigenvalues of An!1 are 
Applying the same argument as above to  An!1 , there exists an
λ2, ..... ,λn.
orthogonal (n!1)×(n!1) matrix 
 such that  
Q (
2
Q ( T
2
An&1Q (
2 '
λ2
0T
0
An&2
.
(I.99)
Hence, denoting 
Q2 '
1
0T
0 Q (
2
,
(I.100)
which an orthogonal  n×n matrix, we can write
Q T
2 Q T
1 AQ1Q2 '
Λ2
O
O
An&2
(I.101)
where 72 is a diagonal matrix with diagonal elements 
 Repeating this procedure n-3
λ1 and λ2.
more times yields 
Q T
n þQ T
2 Q T
1 AQ1Q2þQn ' Λ
(I.102)
where 7 is the diagonal matrix with diagonal elements 
 
λ1,λ2, ..... ,λn.
Note that 
 is an orthogonal matrix itself, and it is now easy to verify that
Q ' Q1Q2þQn

393
the columns of Q are the eigenvectors of A. Q.E.D.
In view of this proof, we can now restate Theorem I.35 as:
Theorem I.36: A symmetric matrix A can be written as A = Q7QT, where 7 is a diagonal matrix
with the eigenvalues of A on the diagonal, and Q is the orthogonal matrix with the
corresponding eigenvectors as columns.
This theorem yields a number of useful corollaries. The first one is trivial:
Theorem I.37: The determinant of a symmetric matrix is the product of its eigenvalues.
The next corollary concerns idempotent matrices [see Definition I.12]:
Theorem I.38: The eigenvalues of a symmetric idempotent matrix are either 0 or 1.
Consequently, the only nonsingular symmetric idempotent matrix is the unit matrix I.
Proof: Let the matrix A in Theorem I.36 be idempotent: A.A = A. Then A = Q7QT = A.A
= Q7QTQ7QT = Q72QT, hence 7 = 72. Since 7 is diagonal, each diagonal element 8j satisfies
Moreover, if A is non-singular and idempotent then none of the
λj ' λ2
j , hence λj(1 & λj) ' 0.
eigenvalues can be zero, hence they are all equal to 1: 7 = I. Then  A = QIQT = A = QQT   = I. 
Q.E.D.

394
I.16. 
Positive definite and semi-definite matrices
Another set of corollaries of  Theorem I.36 concern positive [semi-] definite matrices.
Most of the symmetric matrices we will encounter in econometrics are positive [semi-] definite
or negative [semi-] definite. Therefore, the results below are of utmost importance to
econometrics.
Definition I.23: An  n×n matrix A is called positive definite if for arbitrary vectors x 0  ún 
unequal to the zero vector, xTAx > 0, and it is called positive semi-definite if for such vectors x,
xTAx $ 0. Moreover, A is called negative [semi-] definite if !A is positive [semi-] definite.
Note that symmetry is not required for positive [semi-] definiteness. However,  xTAx can always
be written as 
x TAx ' x T
1
2
A % 1
2
A T x ' x TAsx,
(I.103)
say, where As is symmetric, so that A is positive or negative [semi-] definite if and only if As is
positive or negative [semi-] definite.
Theorem I.39: A symmetric matrix is positive [semi-] definite if and only if all its  eigenvalues
are positive [non-negative].
Proof: This result follows easily from xTAx = xTQ7QTx = yT7y = 
, where y = QTx
'jλjy 2
j
with components yj. Q.E.D.

395
Due to Theorem I.39, we can now define arbitrary powers of positive definite matrices:
Definition I.24: If A is a symmetric  positive [semi-]definite  n×n matrix, then for " 0 ú [" > 0]
the matrix A to the power """" is defined by A" =Q7"QT, where 7" is a diagonal matrix with
diagonal elements the eigenvalues of A to the power ":  7" =  
and Q is the
diag(λα
1,.....,λα
n),
orthogonal matrix of corresponding eigenvectors.  
The following theorem is related to Theorem I.8. 
Theorem I.40: If A is symmetric and positive semi-definite then the Gaussian elimination can be
conducted without need for row exchanges. Consequently, there exist a lower triangular matrix
L with diagonal elements all equal to one, and a diagonal matrix D, such that A = LDLT.
Proof: First note that by Definition I.24 with " = 1/2,  A1/2 is symmetric and (A1/2)TA1/2 =
A1/2 A1/2 = A. Second, recall that according to Theorem I.17 there exist an orthogonal matrix Q
and an upper-triangular matrix U such that A1/2 = QU, hence A = (A1/2)TA1/2 =UTQTQU = UTU. The
matrix UT is lower-triangular, and can be written as UT = LD*, where D*  is a diagonal matrix and
L is a lower-triangular matrix with diagonal elements all equal to 1. Thus,  A = LD*D*LT = LDLT,
where D = D*D*. Q.E.D.

396
I.17. 
Generalized eigenvalues and eigenvectors
The concepts of generalized eigenvalues and eigenvectors play a key role in cointegration
analysis. Cointegration analysis is an advanced econometric time series topic, and will therefore
likely not be covered in an introductory Ph.D. level econometrics course for which this review of
linear algebra is intended.  Nevertheless, to conclude this review I will briefly discuss what
generalized eigenvalues and eigenvectors are, and how they relate to the standard case.
Given two n×n matrices A and B, the generalized eigenvalue problem is: Find the values
for 8 for which
det(A & λB) ' 0.
(I.104)
Given a solution  8, which is called the generalized eigenvalue of  A and B, the corresponding
generalized eigenvector (relative to B) is a vector x in ún such that Ax = 8Bx. 
However, if B is singular then the generalized eigenvalue problem may not have n
solutions as in the standard case, and may even have no solution at all. To demonstrate this,
consider the 2×2 case:
A '
a1,1 a1,2
a2,1 a2,2
,
B '
b1,1 b1,2
b2,1 b2,2
.
Then 

397
det(A & λB) ' det
a1,1&λb1,1 a1,2&λb1,2
a2,1&λb2,1 a2,2&λb2,2
' (a1,1&λb1,1)(a2,2&λb2,2) & (a1,2&λb1,2)(a2,1&λb2,1)
' a1,1a2,2&a1,2a2,1 % (a2,1b1,2&a2,2b1,1&a1,1b2,2%b2,1a1,2)λ % (b1,1b2,2&b2,1b1,2)λ2
If B is singular then 
 so that then the quadratic term vanishes. But things can
b1,1b2,2&b2,1b1,2 ' 0,
even be worse! It is possible that also the coefficient of 8 vanishes, whereas the constant term
  remains nonzero. In that case the generalized eigenvalues do not exist at all.
a1,1a2,2&a1,2a2,1
This is for example the case  if
A '
1
0
0 &1
,
B '
1 1
1 1
.
Then 
det(A & λB) ' det
1&λ
&λ
&λ
&1&λ
' &(1&λ)(1%λ) & λ2 ' &1,
so that the generalized eigenvalue problem involved has no solution. 
Therefore, in general we need to require that the matrix B is non-singular. In that case the
solutions of  (I.104) are the same as the solutions of the standard eigenvalue problems
det(AB!1!8I) = 0 and det(B &1A&λI) ' 0.
The generalized eigenvalue problems that we shall encounter in advanced econometrics
always involve a pair of symmetric matrices A and B, with B positive definite. Then the solutions
of  (I.104) are the same as the solutions of the symmetric standard eigenvalue problem
det(B &1/2AB &1/2 & λI) ' 0.
(I.105)
The generalized eigenvectors relative to B corresponding to the solutions of  (I.104) can be

398
derived from the eigenvectors corresponding to the solutions of (I.105):
B &1/2AB &1/2x ' λx ' λB 1/2B &1/2x Y A(B &1/2x) ' λB(B &1/2x)
(I.106)
Thus if x is an eigenvector corresponding to a solution 8 of (I.105)  then y = B!1/2x is the
generalized eigenvector relative to B corresponding to the generalized eigenvalue 8. 
Finally, note that  generalized eigenvectors are in general not orthogonal, even if the two
matrices involved are symmetric. However, in the latter case the generalized eigenvectors are
"orthogonal with respect to the matrix B", in the sense that for different generalized eigenvectors
y1 and y2,  
  = 0. This follows straightforwardly from the link  y = B!1/2x between
y T
1 By2
generalized eigenvectors y and standard eigenvectors x.
I.18.
Exercises
1. 
Consider the matrix
A '
2
1
1
4
&6 0
&2
7
2
.
(a) 
Conduct the Gaussian elimination by finding a sequence Ej of elementary matrices such
that (Ek Ek-1 .... E2 . E1) A = U = upper triangular.
(b) 
Then show that by undoing the elementary operations Ej involved one gets the LU
decomposition A = LU, with L a lower triangular matrix with all diagonal elements equal to 1.
(c) 
Finally, find the LDU  factorization.
2.
Find the 3×3 permutation matrix that swaps rows 1 and 3 of a 3×3 matrix.

399
3. 
Let
A '
1 v1 0 0
0 v2 0 0
0 v3 1 0
0 v4 0 1
.
where v2 … 0.
(a) 
Factorize A into LU.
(b) 
Find A-1, which has the same form as A.
4. 
Compute the inverse of the matrix
A '
1 2
0
2 6
4
0 4 11
.
by any method.
5. 
Consider the matrix
A '
1
2
0
2
1
&1 &2
1
1
0
1
2
&3 &7 &2
.
(a) 
Find the echelon matrix U in the factorization PA = LU.
(b) 
What is the rank of A?
(c) 
Find a basis for the null space of A.
(d) 
Find a basis for the column space of A.

400
6. 
Find a basis for the following subspaces of ú4:
(a) 
The vectors 
 for which x1 = 2x4. 
(x1,x2,x3,x4)T
(b) 
The vectors 
 for which x1 + x2 + x3 = 0 and x3 + x4 = 0.
(x1,x2,x3,x4)T
(c) 
The subspace spanned by (1,1,1,1)T, (1,2,3,4)T, and (2,3,4,5)T. 
7. 
 Let
A '
1 2
0
3
0 0
0
0
2 4 0
1
and b '
b1
b2
b3
.
(a) 
Under what conditions on b does Ax = b have a solution?
(b) 
Find a basis for the nullspace of A.
(c) 
Find the general solution of Ax = b when a solution exists.
(d) 
Find a basis for the column space of A.
(e) 
What is the rank of AT ?
8. 
Apply the Gram-Smidt process to the vectors
a '
0
0
1
, b '
0
1
1
, c '
1
1
1
and write the result in the form A = QU, where Q is an orthogonal matrix and U is upper
triangular.
9.
With a, b and c as in problem 8, find the projection of c on the space spanned by a and b.
10.
Find the determinant of the matrix A in problem 1.

401
11.
Consider the matrix
A '
1
a
&1 1
For which values of a  has this matrix
(a)  
two different real valued eigenvalues?
(b)  
two complex valued eigenvalues?
(c)  
two equal real valued eigenvalues?
(d)  
at least one zero eigenvalue?
12.
 For the case a = -4, find the eigenvectors of the matrix A in problem 11 and standardized
them to unit length.
13. 
Let A be a matrix with eigenvalues 0 and 1 and  corresponding eigenvectors (1,2)T and
(2,!1)T.
(a)  
How can you tell in advance that A is symmetric?
(b) 
What is the determinant of A?
(c) 
What is A?
14. 
 The trace of a square matrix is the sum of the diagonal elements. Let A be a positive
definite k×k  matrix. Prove that the maximum eigenvalue of A can be found as the limit of the
ratio trace(An)/trace(An!1)  for n 6 4.

402
1.
Law of Cosines:  Consider a triangle ABC, let N be the angle between the legs C6A and
C6B, and denote the lengths of the legs opposite to the points A, B and C by ", $, and (,
respectively. Then γ2 ' α2 % β2 & 2αβcos(φ).
2.
In writing a matrix product it is from now on implicitly assumed that the matrices
involved are conformable.
3.
Here and in the sequel the columns of a matrix are interpreted as vectors.
4.
Here and in the sequel I denotes a generic unit matrix.
5.
The transpose of a matrix A is also denoted in the literature by A ).
6.
The notation Ei,j(c) will be used for a specific elementary matrix, and a generic
elementary matrix will be denoted by "E".
7.
A pivot is an element on the diagonal to be used to wipe out the elements below that
diagonal element.
8.
Note that the diagonal elements of D are the diagonal elements of the former upper-
triangular matrix U.
9.
I(true) ' 1, I(false) ' 0.
10.
The operation of swapping a pair of adjacent columns or rows is also called a column or
row exchange, respectively.
11.
Eigenvalues are also called characteristic roots. The name "eigen" comes from the
German word "Eigen" , which  means "inherent", or "characteristic". 
12.
Recall that the complex conjugate of  x = a + i.b,   a,b  0 ú,  is 
 See
¯x ' a & i.b.
Appendix III.
13.
Eigenvectors are also called characteristic vectors.
14.
Recall (see Appendix III) that the length (or norm) of a complex number  x = a + i.b,   a,b 
0 ú,  is defined as 
 Similarly, in the vector case  x = a +
|x| '
(a %i.b).(a & i.b) '
a 2%b 2.
i.b,   a,b  0 ún,  the length of x is defined as 
 =
2x2
(a % i.b)T(a & i.b) '
a Ta%b Tb.
Endnotes

403
Appendix II
Miscellaneous Mathematics
In this appendix I will review various mathematical concepts, topics and related results
that are used throughout the main text.
II.1.
Sets and set operations
II.1.1 General set operations
 
The union AcB of two sets A and B is the set of elements that belong to either A or B or to
both. Thus, denoting "belongs to" or "is element of" by the symbol 0, x 0  AcB implies that  x 0 
A or  x 0 B, or in both, and vice versa.  A finite union 
 of sets A1,...,An is the set with the
^n
j'1Aj
property that for each 
 there exists an index i, 1 # i # n,  for which 
 and vice
x 0 ^n
j'1Aj
x 0 Ai,
versa: If  
 for some index i, 1 # i # n,  then 
 Similarly, the countable union 
x 0 Ai
x 0 ^n
j'1Aj.
 of an infinite sequence of sets Aj,  j = 1,2,3,....., is a set with the property that for each
^4
j'1Aj
 there exists a finite index i $ 1 for which 
 and vice versa: If  
 for some
x 0 ^4
j'1Aj
x 0 Ai,
x 0 Ai
finite index i $ 1 then 
 
x 0 ^4
j'1Aj.
 
The intersection A1B of two sets A and B is the set of elements which belong to both A
and B. Thus, x 0A1B implies that  x 0 A and  x 0 B, and vice versa.  The finite intersection _n
j'1Aj
of sets A1,...,An is the set with the property that if 
 then for all  i = 1,...,n,  
 and
x 0 _n
j'1Aj
x 0 Ai,
vice versa: If  
 for all i = 1, ..., n,  then 
 Similarly, the countable intersection 
x 0 Ai
x 0 _n
j'1Aj.
 of an infinite sequence of sets Aj, j = 1,2,..., is a set with the property that if 
_4
j'1Aj
x 0 _4
j'1Aj

404
then for all indices i $ 1, 
 and vice versa: If  
 for all indices i $ 1 then 
 
x 0 Ai,
x 0 Ai
x 0 _4
j'1Aj.
 
A set A is a subset of a set B, denoted by AdB, if all the elements of A are contained in B.
If  AdB and BdA then A = B.
 
The difference A\B (also denoted by A-B) of sets A and B is the set of elements of A that
are not contained in B. The symmetric difference of two sets A and B is denoted and defined by
A∆B ' (A/B)^(B/A).
 
If  AdB then the set 
 = B/A ( also denoted by ~A) is called the complement of A with
˜A
respect to B. If  Aj for j = 1,2,3,..... are subsets of B then 
 and 
 for
~^jAj ' _j ˜Aj
~_jAj ' ^j ˜Aj,
finite as well as countable infinite unions and intersections.
. 
Sets A and B are disjoint if they do not have elements in common:  A1B = i, where i
denotes the empty set, i.e., a set without elements. Note that Aci = A and A1i = i. Thus the
empty set i is a subset of any set, including i itself.  Consequently, the empty set is disjoint with
any other set, including  i itself. In general, a finite or countable infinite sequence of sets is
disjoint if their finite or countable intersection is the empty set  i.  
 
For every sequence of sets Aj , j = 1,2,3,....., there exists a sequence Bj , j = 1,2,3,....., of
disjoint sets such that for each j,  BjdAj, and  
 In particular, let B1 = A1 and Bn =
^jAj ' ^jBj.
 for n = 2,3,4,.....
An\ ^n&1
j'1 Aj
The order in which unions are taken does not matter, and the same applies to
intersections. However, if you take unions and intersections sequentially it matters what is done 
first. For example, (AcB)1C  =  (A1C)c(B1C), which is in general different from  Ac(B1C),
except if AdC.  Similarly, (A1B)c C  =  (AcC)1(BcC), which is in general different from 
A1(BcC), except if AdB.

405
II.1.2 Sets in Euclidean spaces
An open  g-neighborhood of a point x in a Euclidean space úk is a set of the form
Ng(x) ' {y 0 úk: ||y&x|| < g}, g > 0,
and a closed  g-neighborhood is a set of the form
Ng(x) ' {y 0 úk: ||y&x|| # g}, g > 0.
A set A is called open if for every x 0  A there exists a small open  g-neighborhood  Ng(x)
contained in A. In short-hand notation: 
 where œ stands for “for all”
œx 0 A ›g > 0: Ng(x) d A,
and › stands for “there exists”. Note that the g’s may be different for different x.
A point x called a point of closure of a subset A of  úk  if every open  g-neighborhood 
 contains a point in A as well as a point in the complement 
 of A. Note that points of
Ng(x)
˜A
closure may not exist, and if one exists it may not be contained in A. For example, the Euclidean
space   úk   itself has no points of closure because its complement is empty.  Moreover, the
interval (0,1) has two points of closure, 0 and 1, both not included in (0,1).  The boundary of a
set A, denoted by MA, is the set of points of closure of A. Again,  MA may be empty. A set A is
closed if it contains all its points of closure if they exist. In other words, A is closed if and only if  
MA … i and  MA  d A. Similarly, a set A is open if either MA = i or MA  d
 The closure of a set A,
˜A.
denoted by 
 is the union of A and its boundary MA: 
 The set  A\MA is the interior
A,
A ' A^MA.
of A.
Finally, if for each pair x, y of points in a set A and an  arbitrary 
 the convex
λ 0 [0,1]
combination 
  is also a point in A then the set A is called  convex. 
z ' λx % (1&λ)y

406
II.2.
Supremum and infimum
The supremum of a sequence of real numbers, or a real function, is akin to the notion of a
maximum value. In the latter case the maximum value is taken at some element of the sequence,
or in the function case some value of the argument. Take for example the sequence  an = (!1)n/n
for n = 1,2,......., i.e., a1 = -1, a2 = 1/2,  a3 = -1/3, a4 = 1/4, ..... Then clearly the maximum value is
½, which is taken by a2. The latter is what distinguishes a maximum from a supremum. For
example, the sequence   an = 1!1/n  for n = 1,2,....... is bounded by 1: an < 1 for all indices n $1,
and the upper bound 1 is the lowest possible upper bound, but there does not exist a finite index
n for which  an  = 1. More formally, the (finite) supremum of a sequence an (n = 1,2,3,.......)  is a
number b,  denoted by supn$1an , such that an # b for all indices  n $1, and for every arbitrary
small positive number g there exists a finite index n such that  an > b!g. Clearly, this definition
fits a maximum as well: a maximum is a supremum, but a supremum is not always a maximum.
If the sequence an  is unbounded from above, in the sense that for every arbitrary large
real number M there exists an index n $1 for which  an > M, then we say that the supremum is
infinite: supn$1an =  4.
The notion of a supremum also applies to functions. For example the function f(x) =
exp(!x2) takes its maximum 1 at x = 0, but the function  f(x) = 1!exp(!x2) does not have a
maximum; it has supremum 1 because   f(x) # 1 for all x but there does not exists a finite x for
which  f(x) = 1.  As another example, let  f(x) = x on the interval [a,b]. Then b is the maximum of
  f(x) on [a,b] but b is only the supremum f(x) on [a,b) because b is not contained in [a,b). More
generally, the finite supremum of a real function f(x) on a set A, denoted by 
 is a real
supx0Af(x),
number b such that  f(x) # b for all x in A, and for every arbitrary small positive number g there

407
exists an x in A such that  f(x) > b!g. If f(x) = b for some x in A then the supremum coincides
with the maximum. Moreover, the supremum involved is infinite, 
 if for every
supx0Af(x) ' 4,
arbitrary large real number M  there exists an x in A for which  f(x)  > M.
The minimum versus infimum cases are similar: infn$1an = !supn$1(!an) and  
 =
infx0Af(x)
&supx0A(&f(x)).
The concepts of supremum and infimum apply to any collection {c", " 0 A} of real
numbers, where the index set A may be uncountable, as we may interpret c" as a real function on
the index set A, say  c" = f("). 
II.3.
Limsup and liminf
Let an  (n = 1,2,.......) be a sequence of real numbers, and define the sequence bn as
bn ' supm$n am.
(II.1)
Then bn  is a non-increasing sequence: bn  $ bn+1 because if  an is greater than the smallest upper
bound of 
 is the maximum of 
 hence
an%1,an%2,an%3,...... then an
an,an%1,an%2,an%3,......,
 and if not then 
  Non-increasing sequences always have a limit,
bn ' an > bn%1,
bn ' bn%1.
although the limit may be !4. The limit of bn  in (II.1) is called the limsup of an : 
limsup
n64
an '
def.
lim
n64
supm$n am .
(II.2)
Note that since bn is non-increasing, the limit of bn is equal to the infimum of bn . Therefore, the
limsup of an  may also be defined as

408
limsup
n64
an '
def.
inf
n$1
supm$n am .
(II.3)
Note that the limsup may be +4 or !4, for example in the cases  an = n and  an = !n,
respectively. 
Similarly, the liminf of an  is defined by
liminf
n64
an '
def.
lim
n64
infm$n am
(II.4)
or equivalently by 
liminf
n64
an '
def.
sup
n$1
infm$n am .
(II.5)
Again, it is possible that the liminf is +4 or !4.
Note that 
 because 
 for all indices n
liminfn64 an # limsupn64 an,
infm$n am # supm$n am
$1, and therefore the inequality must hold for the limits as well.
Theorem II.1: 
(a)
If  
 and if  
 < 
liminfn64 an ' limsupn64 an then limn64 an ' limsup
n64
an,
liminfn64 an
then the limit of an does not exist.
limsupn64 an
(b)
Every sequence an  contains a sub-sequence 
 such that 
 and
ank
limk64 ank ' limsupn64 an,
 an  also contains a sub-sequence 
 such that 
anm
limm64 anm ' liminfn64 an.
Proof: The proof of (a) follows straightforwardly from (II.2), (II.4) and the definition of a
limit. The construction of the sub-sequence 
 in part (b) can be done recursively, as follows.
ank

409
Let 
 Choose n1 = 1, and suppose that we have already constructed 
 for
b ' limsupn64 an < 4.
anj
j=1,...,k $1. Then there exists an index nk+1 >  nk such that 
, because
ank%1 > b & 1/(k%1)
otherwise 
 for all m $  nk , which would imply that 
b !1/(k+1).
am # b & 1/(k%1)
limsupn64 an #
Repeating this construction yields a sub-sequence 
 such that from large enough k onwards,
ank
 Letting k64, the limsup case of part (b) follows. If  
  then
b & 1/k < ank # b.
limsupn64 an ' 4
for each index nk we can find an index  nk+1 >  nk such that 
, hence then  
 =
ank%1 > k%1
limk64 ank
4.   The sub-sequence in the case  
 and in the  liminf case can be constructed
limsupn64 an ' &4
similarly. Q.E.D.
The concept of a supremum can be generalized to sets. In particular, the countable union  
 may be interpreted as the supremum of the sequence of sets Aj, i.e., the smallest set
^4
j'1Aj
containing all the sets  Aj.  Similarly, we may interpret the countable intersection 
 as the
_4
j'1Aj
infimum of the sets  Aj, i.e., the largest set contained in each of  the sets  Aj. Now let for n =
1,2,3,..., 
This is a non-increasing sequence of sets: Bn+1 d Bn , hence  
Bn ' ^4
j'nAj.
_n
j'1Bn ' Bn.
The limit of this sequence of sets is the limsup of An for n 64, i.e., similarly to (II.3) we have
limsup
n64
An '
def.
_
4
n'1
^
4
j'n
Aj .
Next, let for n = 1,2,3,..., 
This is a non-decreasing sequence of sets: Cn d Cn+1,
Cn ' _4
j'nAj.
hence 
 The limit  of this sequence of sets is the liminf of An for n 64, i.e. similarly
^n
j'1Cn ' Cn.
to (II.5) we have
liminf
n64
An '
def.
^
4
n'1
_
4
j'n
Aj .

410
II.4.
Continuity of concave and convex functions
A real function φ on a subset of a Euclidean space is convex if for each pair of points a,b
and every λ 0 [0,1], 
 For example, 
 is a convex
φ(λa%(1&λ)b) $ λφ(a) % (1&λ)φ(b).
φ(x) ' x 2
function on the real line, and so is 
 Similarly,  φ is concave if for each pair of
φ(x) ' exp(x).
points a,b and every λ 0 [0,1], φ(λa%(1&λ)b) # λφ(a) % (1&λ)φ(b).
I will prove the continuity of convex and concave functions  by contradiction. Suppose
that  
 is convex but not continuous in a point a. Then  
φ
φ(a%) ' lim
b9a
φ(b) … φ(a)
(II.6)
or
φ(a&) ' lim
b8a
φ(b) … φ(a)
(II.7)
or both. In the case (II.6) we have
φ(a%) ' lim
b9a
φ(a % 0.5(b&a)) ' lim
b9a
φ(0.5a%0.5b)
# 0.5φ(a) % 0.5lim
b9a
φ(b) ' 0.5φ(a) % 0.5φ(a%),
hence 
 and therefore by  (II.6), 
. Similarly, if (II.7) is true then
φ(a%) # φ(a)
φ(a%) < φ(a)
.  Now let δ > 0. By the convexity of  
  it follows that
φ(a&) < φ(a)
φ
 φ(a) ' φ(0.5(a&δ) % 0.5(a%δ)) # 0.5φ(a&δ) % 0.5φ(a%δ),
and consequently, letting 
 and using the fact that 
, or 
, or
δ 9 0,
φ(a%) < φ(a)
φ(a&) < φ(a)
both, we have 
 Since this result is impossible, it follows
φ(a) # 0.5φ(a&) % 0.5φ(a%) < φ(a).
that (II.6) and (II.7) are impossible, hence
 is continuous. 
φ
If
 is concave, then 
 is convex and thus continuous, hence concave functions are
φ
&φ
continuous.

411
II.5.
Compactness
An (open) covering of a subset  Θ of a Euclidean space 
 is a collection of (open)
úk
subsets U(α), α 0 A, of 
, where A is a possibly uncountable index set, such that
úk
 A set is called compact if every open covering has a finite sub-covering, i.e., if
Θ d ^α0AU(α).
U("), " 0 A, is an open covering of  1 and  1 is compact then there exists a finite subset B of A
such that Θ d ^α0BU(α).
The notion of compactness extends to more general spaces than only Euclidean spaces.
However, 
Theorem II.2: Closed and bounded subsets of Euclidean spaces are compact.
Proof: I will prove the result for sets Θ  in ú only. First note that boundedness is a
necessary condition for compactness, because a compact set can always be covered by a finite
number of bounded open sets.
Next let Θ is a closed and bounded subset of the real line. By boundedness, there exists
points a and b in such that  Θ is contained in [a,b]. Since every open covering of Θ  can be
extended to an open covering of [a,b], we may without loss of generality assume that Θ = [a,b].
For notational convenience, let Θ = [0,1].  There always exists an open covering of [0,1], because
for arbitrary  g > 0,  [0,1] d 
  Let U(α), α 0 A, be an open covering of [0,1].
^0#x#1(x&g,x%g).
Without loss of generality we may assume that each of the open sets U(α) takes the form
(a(α),b(α)). Moreover, if for two different indices α and β, a(α) = a(β), then either  (a(α),b(α)) d
(a(β),b(β)), so that  (a(α),b(α)) is superfluous, or   (a(α),b(α)) e (a(β),b(β)), so that  (a(β),b(β)) is

412
superfluous. Thus, without loss of generality we may assume that the a(α)’s are all distinct and
can be arranged in increasing order. Consequently, we may assume that the index set A is the set
of the  a(α)’s themselves, i.e., U(a) = (a,b(a)), a 0 A, where A is a subset of ú such that
Furthermore, if  
 then 
, as otherwise 
 is
[0,1] d ^a0A(a,b(a)).
a1 < a2
b(a1) < b(a2)
(a2,b(a2))
superfluous. Now let 
 and define for n = 2,3,4,..., 
. Then
0 0 (a1,b(a1)),
an ' (an&1%b(an&1))/2
This implies that 1 0 
  hence there exists an n such that
[0,1] d ^4
n'1(an,b(an)).
^4
n'1(an,b(an)),
 Consequently, [0,1] d 
 Thus, [0,1] is compact. This argument
1 0 (an,b(an)).
^n
j'1(aj,b(aj)).
extends to arbitrary closed and bounded subsets of a Euclidean space. Q.E.D.
A limit point of a sequence xn of real numbers is a point x*  such that for every  g > 0 there
exists an index n for which  
 Consequently, a limit point is a limit along a
|xn&x(| < g.
subsequence. Sequences  xn  confined to an interval [a,b] always have at least one limit point, and
these limit points are contained in [a,b], because 
 and 
 are limit points
limsupn64xn
liminfn64xn
contained in [a,b], and any other limit point must  lie between
 and 
 . This
liminfn64xn
limsupn64xn
property carries over to general compact sets:
Theorem II.3: Every infinite sequence 
 of points in a compact set Θ has at least one limit
θn
point, and all the limit points are contained in Θ.
Proof: Let  Θ. be a compact subset of a Euclidean space and let 
 be a
Θk, k ' 1,2,....,
decreasing sequence of compact subsets of  Θ each containing infinitely many 
‘s , to be
θn
constructed as follows. Let 
 and  k $ 0.  There exist a finite number of points
Θ0 ' Θ
 such that, which  
  Θk  is contained in
θ(
k,j, j ' 1,....,mk,
Uk(θ() ' {θ: ||θ&θ(|| < 2&k},

413
 Then at least one of these open sets contains infinity many points 
, say 
^
mk
j'1Uk(θ(
k,j).
θn
Uk(θ(
k,1).
Next, let 
 
 
Θk%1 ' {θ: ||θ&θ(
k,1|| # 2&k}_Θk,
which is compact, and contains infinity many points 
.  Repeating this construction it is easy to
θn
verify that  
 is a singleton, and that this singleton is a limit point contained in  Θ . Finally,
_4
k'0Θk
if a limit point 
is located outside Θ then for some large k,  
 which contradicts
θ(
Uk(θ()_ Θ ' i,
the requirement that 
 contains infinitely many 
‘s. Q.E.D.
Uk(θ()
θn
Theorem II.4: Let 
 be a sequence of points in a compact set Θ. If all the limit points of  
 are
θn
θn
the same, then 
 exists and is a point in Θ.
limn64θn
Proof: Let 
 be the common limit point. If the limit does not exists, then  there
θ( 0 Θ
exists a δ > 0 and an infinite subsequence 
 such that 
 for all k. But 
 has also
θnk
|θnk&θ(| $ δ
θnk
limit point 
 so that there exists a further subsequence 
 which converges to 
Therefore,
θ(,
θnk(m)
θ(.
the theorem follows by contradiction. Q.E.D.
Theorem II.5: For a continuous function g on a compact set Θ,  
 = 
 and 
supθ0Θg(θ)
maxθ0Θg(θ)
 = 
 Consequently, 
 and 
infθ0Θg(θ)
minθ0Θg(θ).
argmaxθ0Θg(θ) 0 Θ
argminθ0Θg(θ) 0 Θ.
Proof: It follows from the definition of 
 that or each k  $ 1 there exists a point
supθ0Θg(θ)
 such that  
 hence 
 Since Θ is
θk 0 Θ
g(θk) > supθ0Θg(θ) & 2&k,
limk64g(θk) ' supθ0Θg(θ).
compact the sequence 
 has a limit point 
 (see Theorem II.3),  hence by the continuity
θk
θ( 0 Θ

414
of g,  
Consequently,  
 = 
 Q.E.D. 
g(θ() ' supθ0Θg(θ).
supθ0Θg(θ)
maxθ0Θg(θ) ' g(θ().
II.6.
Uniform continuity
A function g on 
 is called uniformly continuous if for every g > 0 there exists a  δ > 0
úk
such that 
 if 
In particular, 
|g(x) & g(y)| < g
2x & y2 < δ.
Theorem II.6: If a function  g  is continuous on a compact subset  Θ of 
 then it is uniformly
úk
continuous on  Θ. 
Proof: Let  g > 0 be arbitrary, and observe from the continuity of g that for each x in  Θ
there exists a 
 > 0 such that 
 if  
 Now let U(x) =
δ(x)
|g(x) & g(y)| < g/2
2x & y2 < 2δ(x).
Then the collection {U(x), x 0  Θ} is an open covering of  Θ, hence
{y 0 úk: 2y & x2 < δ(x)}.
by compactness of  Θ there exists a finite number of  points 
 in  Θ such that 
θ1,.....,θn
 Next, let  
 Each point x 0  Θ belongs to at least one of the
Θ d ^n
j'1U(θj).
δ ' min1#j#nδ(θj).
open sets 
 for some j. Then 
 hence  
 <
U(θj): x 0 U(θj)
2x & θj2 < δ(θj) < 2δ(θj),
|g(x) & g(θj)|
g/2.  Moreover,  if 
 then 
2x & y2 < δ
2y & θj2 ' 2y & x % x & θj2 # 2x & y2 % 2x & θj2 < δ % δ(θj) # 2δ(θj),
hence 
 Consequently,  
 
|g(y) & g(θj)| < g/2.
|g(x) & g(y)| # |g(x) & g(θj)| % |g(y) & g(θj)| < g
if
 Q.E.D.
2x & y2 < δ.

415
II.7.
Derivatives of functions of vectors and matrices
Consider a real function 
 Recall that
f(x) ' f(x1,.....,xn) on ún, where x ' (x1,.....,xn)T.
the partial derivative of f  to a component xi of x is denoted and defined by
Mf(x)
Mxi
' Mf(x1,.....,xn)
Mxi
'
def.
lim
δ60
f(x1,..,xi&1,xi%δ,xi%1,...,xn) & f(x1,..,xi&1,xi,xi%1,...,xn)
δ
.
For example, let  
 Then
f(x) ' βTx ' x Tβ ' β1x1 % ... βnxn.
Mf(x)/Mx1
!
Mf(x)/Mxn
'
β1
!
βn
' β.
This result could also have been obtained by treating xT as a scalar and taking the derivative of 
 to  xT:  
 This motivates the convention to denote the column vector
f(x) ' x Tβ
M(x Tβ)/Mx T ' β.
of partial derivative of f(x) by 
 Similarly, if we treat x as a scalar and take the
Mf(x)/Mx T.
derivative of  
 to x, then the result is a row vector:   
Thus in general,
f(x) ' βTx
M(βTx)/Mx ' βT.
Mf(x)
Mx T
'
def.
Mf(x)/Mx1
!
Mf(x)/Mxn
,
Mf(x)
Mx
'
def.
Mf(x)/Mx1,....,Mf(x)/Mxn .
If the function H is vector-valued, say 
 then applying
H(x) ' (h1(x),.....,hm(x))T, x 0 ún,
the operation  
 to each of the components yields an m×n matrix:
M/Mx
MH(x)
Mx
'
def.
Mh1(x)/Mx
!
Mhm(x)/Mx
'
Mh1(x)/Mx1 þ Mh1(x)/Mxn
!
!
Mhm(x)/Mx1 þ Mhm(x)/Mxn
.
Moreover, applying the latter to a column vector of  partial derivatives of a real function f  yields

416
M(Mf(x)/Mx T)
Mx
'
M2f(x)
Mx1Mx1
þ
M2f(x)
Mx1Mxn
!
"
!
M2f(x)
MxnMx1
þ
M2f(x)
MxnMxn
' M2f(x)
MxMx T ,
say.
In the case of an m×n matrix X with columns 
 and a
x1,.....,xn 0 úk, xj ' (x1,j,....,xm,j)T,
differentiable function f(X) on the vector space of  k×n matrices, we may interpret X = (x1,...,xn)
as a “row” of column vectors, so that
Mf(X)
MX
'
Mf(X)
M(x1,.....,xn)
'
def.
Mf(X)/Mx1
!
Mf(X)/Mxn
'
def.
Mf(X)/Mx1,1 þ Mf(X)/Mxm,1
!
"
þ
Mf(X)/Mx1,n þ Mf(X)/Mxm,n
is an n×m matrix. For the same reason,  
 An example of such a
Mf(X)/MX T '
def.
(Mf(X)/MX)T.
derivative to a matrix is given by Theorem I.33 in Appendix I, which states that if X is a square
nonsingular matrix then  Mln[det(X)]/MX
' X &1.
Next, consider the quadratic function  f(x) = a + xTb + xTCx,  where
x '
x1
:
xn
, b '
b1
:
bn
, C '
c1,1 .... c1,n
:
....
:
cn,1 .... cn,n
, with ci,j ' cj,i.
Thus, C is a symmetric matrix. Then

417
Mf(x)/Mxk ' M a % 'n
i'1bixi % 'n
i'1'n
j'1xici,jxj
Mxk
' j
n
i'1
bi
Mxi
Mxk
% j
n
i'1 j
n
j'1
Mxici,jxj
Mxk
' bk % 2ck,kxk % j
n
i'1
i…k
xici,k % j
n
j'1
j…k
ck,jxj
' bk % 2j
n
j'1
ck,jxj,
k ' 1,...,n,
hence, stacking these partial derivatives in a column vector yields
Mf(x)/Mx T ' b % 2Cx.
(II.8)
If C is not symmetric, we may without loss of generality replace C in the function f(x) by the
symmetric matrix (C + CT)/2, because xTCx = (xTCx)T = xTCTx, so that then
Mf(x)/Mx T ' b % Cx % C Tx.
The result (II.8) for the case b = 0 can be used to give an interesting alternative
interpretation of eigenvalues and eigenvectors of symmetric matrices, namely as the solutions of
a quadratic optimization problem under quadratic restrictions. Consider the optimization problem
max or min x TAx s.t. x Tx ' 1,
(II.9)
where A is a symmetric matrix, and “max” and “min” include local maxima and minima, and
saddle-point solutions. The Lagrange function for solving this problem is
‹(x,λ) ' x TAx % λ(1 & x Tx),
with first-order conditions
M‹(x,λ)/Mx T ' 2Ax & 2λx ' 0 Y Ax ' λx,
(II.10)
M‹(x,λ)/Mλ ' 1 & x Tx ' 0 Y 2x2 ' 1.
(II.11)
Condition (II.10) defines the Lagrange multiplier λ. as the eigenvalue and the solution for x as the

418
corresponding eigenvector of A, and (II.11)  is the normalization of the eigenvector to unit length.
Combining (II.10) and (II.11) it follows that  λ = xTAx.
II.8. 
The mean value theorem
Consider a differentiable real function f(x), displayed as the curved line in the following
figure:
Figure II.1. The mean value theorem
We can always find a point c in the interval [a,b] such that the slope of  f(x) at  x = c, which is
equal to the derivative 
 is the same as the slope of the straight line connecting the points
f )(c),
(a,f(a)) and (b,f(b)), simply by shifting the latter line parallel to the point where it be comes
tangent to  f(x).  The slope of this straight line through the points  (a, f(a)) and (b,f(b)) is:
(f(b)!f(a))/(b!a).  Thus, at x = c we have  
 or  equivalently
f )(c) ' (f(b) & f(a))/(b & a),

419
This easy result is called the mean value theorem. Since this point  c
f(b) ' f(a) % (b & a)f )(c).
can also be expressed as 
 with
 we can now
c ' a % λ(b & a),
0 # λ ' (c & a)/(b & a) # 1,
state the mean value theorem as:
Theorem II.7(a): Let f(x) be a differentiable real function on an interval [a,b], with derivative
 For any pair of points 
there exists a 
 such that  f(x) =   
f )(x).
x,x0 0 [a,b]
λ 0 [0,1]
 f(x0) % (x & x0)f )(x0 % λ(x&x0)).
This result carries over to real functions of more than one variable:
Theorem II.7(b): Let f(x) be a differentiable real function on a convex  subset C of  
 For any
úk.
pair of  points 
 there exists a 
 such that
x, x0 0 C
λ 0 [0,1]
 f(x) ' f(x0) % (x & x0)T(M/My T)f(y)*y'x0%λ(x&x0).
II.9. 
Taylor’s theorem
The mean value theorem implies that if for two points a < b,  f(a)  =  f(b), then there
exists a point 
such that 
This fact is the core of the proof of Taylor’s
c 0 [a,b]
f )(c) ' 0.
theorem:
Theorem II.8(a): Let f(x) be an n-times continuously differentiable  real function on an interval
[a,b], with the n-th derivative denoted by  
For any pair of points 
 there exists
f (n)(x).
x,x0 0 [a,b]

420
a 
 such that 
λ 0 [0,1]
 f(x) ' f(x0) % j
n&1
k'1
(x & x0)k
k!
f (k)(x0) %
(x & xn)n
n!
f (n)(x0 % λ(x&x0)).
Proof:  Let  
 be fixed.  We can always write
a # x0 < x # b
f(x) ' f(x0) % j
n&1
k'1
(x & x0)k
k!
f (k)(x0) % Rn,
(II.12)
where Rn  is the remainder term. Now let  
 be fixed, and consider the function
a # x0 < x # b
g(u) ' f(x) & f(u) & j
n&1
k'1
(x & u)k
k!
f (k)(u) &
Rn(x & u)n
(x & x0)n
with derivative
g )(u) ' &f )(u) % j
n&1
k'1
(x & u)k&1
(k&1)!
f (k)(u) & j
n&1
k'1
(x & u)k
k!
f (k%1)(u) % nRn(x & u)n&1
(x & x0)n
' &f )(u) % j
n&2
k'0
(x & u)k
k!
f (k%1)(u) & j
n&1
k'1
(x & u)k
k!
f (k%1)(u) % nRn(x & u)n&1
(x & x0)n
' & (x & u)n&1
(n&1)!
f (n)(u) % nRn(x & u)n&1
(x & x0)n
.
Then 
 hence there exists a point  
such that 
g(x) ' g(x0) ' 0,
c 0 [x0,x]
g )(c) ' 0:
0 ' & (x & c)n&1
(n&1)!
f (n)(c) % nRn(x & c)n&1
(x & x0)n
.
Therefore, 
Rn '
(x & xn)n
n!
f (n)(c) '
(x & xn)n
n!
f (n) x0 % λ(x&x0) ,
(II.13)
where 
 Combining (II.12) and (II.13)  the theorem follows. Q.E.D.
c ' x0 % λ(x&x0).

421
Also Taylor’s theorem carries over to real functions of more than one variable, but the
result involved is awkward to display for n > 2. Therefore, we only state the second-order Taylor
expansion theorem involved:
Theorem II.8(b): Let f(x) be a twice continuously differentiable real function on a convex 
subset  Ξ of  
 For any pair of  points  
 there exists a 
 such that
ún.
x, x0 0 Ξ
λ 0 [0,1]
f(x) ' f(x0) % (x & x0)T
/000
Mf(y)
My T
y'x0
% 1
2
(x & x0)T
/000
M2f(y)
MyMy T
y'x0%λ(x&x0)
(x & x0).
(II.14)
 
II.10. Optimization
Theorem II.8(b) shows that the function  f(x) involved is locally quadratic. Therefore, the
conditions for a maximum or a minimum of  f(x) in a point  
 can be derived from (II.14)
x0 0 Ξ
and the following theorem. 
Theorem II.9: Let C be a symmetric n×n matrix, and let f(x) = a + xTb + xTCx,  x 0 ún,
where a is a given scalar and b is a given vector in  ún.  If C is positive [negative] definite then
f(x) takes a unique minimum [maximum], at  x ' &½C &1b.
Proof: The first-order condition for a maximum or minimum is 
 
Mf(x)/Mx T ' 0 (0 ún),
hence 
 As to the uniqueness issue, and the question whether the optimum is a
x ' &½C &1b.
minimum or a maximum, recall that C = QΛQT, where Λ is the diagonal matrix of the
eigenvalues of C and Q is the corresponding matrix of eigenvectors. Thus we can write  f(x)  as 

422
f(x) = a + xT QQT b + xT QΛQT x. Let y = QT x = (y1,...,yn)T and β = QT b =  (β1,...,βn)T . Then  
f(Qy) = a + yT β + yT Λ y =  
 The latter is a sum of quadratic functions in
a % 'n
j'1(βjyj % λjy 2
j ).
one variable which each has a unique minimum if λj > 0 and a unique maximum if  λj < 0. 
Q.E.D.
It follows now from (II.14) and Theorem II.9 that:
Theorem II.10: The function  f(x) in Theorem II.8(b) takes a local maximum [minimum] in a
point 
 i.e., x0  is contained in an open subset  Ξ0  of  Ξ  such that for all x 0  Ξ0\{x0},   
x0 0 Ξ,
f(x) <   f(x0) [f(x) >   f(x0)], if and only if  
 and the matrix 
Mf(x0)/Mx T
0 ' 0 (0 ún),
M2f(x0)/(Mx0Mx T
0 )
is negative [positive] definite.
A practical application of the Theorems II.8(a), II.9 and II.10 is the Newton iteration for
finding a minimum or a maximum of a function. Suppose that the function  f(x) in Theorem
II.8(b) takes a unique global maximum at 
 Starting from an initial guess x0 of  x*, let for
x( 0 Ξ.
k $ 0,
xk%1 ' xk &
M2f(xk)
MxkMx T
k
&1 Mf(xk)
Mx T
k
.
Thus, the Newton iteration maximizes or minimized the local quadratic approximation of f in xk.
The iteration is stopped if for some small threshold  g > 0, ||xk%1 & xk|| < g.

423
Appendix III
A Brief Review of Complex Analysis
III.1.
The complex number system
Complex numbers have many applications. The complex number system allows to
conduct computations that would be impossible to perform in the real world. In probability and
statistics we mainly use complex numbers in dealing with characteristic functions, but in time
series analysis complex analysis plays a key-role. See for example Fuller (1996). Therefore, in
this appendix I will review the basics of complex analysis. 
Complex numbers are actually two-dimensional vectors endowed with arithmetic
operations that make them act as numbers. Therefore, I will introduce the complex numbers in
their "real" form, as vectors in ú2.  
Next to the usual addition and scalar multiplication operators on the elements of   ú2  (see
Appendix I), define the vector multiplication operator  "×" by:
a
b ×
c
d
'
def.
a.c&b.d
b.c%a.d .
(III.1)
Observe that 
a
b
×
c
d
'
c
d
×
a
b
.
(III.2)
Moreover, define the inverse operator “!1" by

424
a
b
&1
'
def.
1
a 2%b 2
a
&b
, provided that a 2%b 2 > 0,
(III.3)
so that
a
b
&1
×
a
b
'
a
b
×
a
b
&1
'
1
a 2%b 2
a
&b
×
a
b
'
1
0
.
(III.4)
The latter vector plays the same role as the number 1 in the real number system. Furthermore, we
can now define the division operator “/” by
a
b
/
c
d
'
def.
a
b
×
c
d
&1
'
1
c 2%d 2
a
b
×
c
&d
'
1
c 2%d 2
a.c%b.d
b.c&a.d
,
(III.5)
provided that  
 > 0. Note that
c 2%d 2
1
0
/
c
d
'
1
c 2%d 2
c
&d
'
c
d
&1
.
(III.6)
In the subspace  
 these operators work the same as for real
ú2
1 ' { (x,0)T, x 0 ú}
numbers:
a
0
×
c
0
'
a.c
0
,
c
0
&1
'
1/c
0
,
a
0
/
c
0
'
a/c
0
,
(III.7)
provided that c … 0. Therefore, all the basic arithmetic operations (addition, subtraction, 
multiplication, division) of the real number system ú apply to 
, and vice versa.
ú2
1
In the subspace 
the multiplication operator × yields
ú2
2 ' { (0,x)T, x 0 ú}
0
b
×
0
d
'
&b.d
0
(III.8)
In particular, note that

425
0
1 ×
0
1
'
&1
0
(III.9)
Now denote 
a % i.b '
def.
1
0 a %
0
1 b, where i '
0
1 ,
(III.10)
and interpret a + i.0 as  the mapping
a % i.0:
a
0
6 a.
(III.11)
Then it follows from (III.1) and (III.10) that
(a%i.b)×(c%i.d) '
a
b
×
c
d
'
a.c&b.d
b.c%a.d
' (a.c&b.d) % i.(b.c%a.d).
(III.12)
However, the same result can be obtained by using standard arithmetic operations, treating the
identifier i as &1:
(a%i.b)×(c%i.d)
' a.s%i 2.b.d%i.b.c%i.a.d ' (a.s&b.d)%i.(b.c%a.d)
(III.13)
In particular,  it follows from (III.9), (III.10) and (III.11) that
i×i '
0
1 ×
0
1
'
&1
0
' &1%i.0 6 &1
(III.14)
which can also be obtained by standard arithmetic operations, treating  i as 
 and i.0 as 0. 
&1
Similarly, we have  
(a%i.b)/(c%i.d) '
a
b /
c
d
'
1
c 2%d 2
a.c%b.d
b.c&a.d
' a.c%b.d
c 2%d 2 %i. b.c&a.d
c 2%d 2
(III.15)
provided that  
 > 0. Again, this result can also be obtained by standard  arithmetic
c 2%d 2

426
operations,  treating  i as &1:
(a%i.b)/(c%i.d) ' a%i.b
c%i.d× c&i.d
c&i.d ' (a%i.b)×(c&i.d)
(c%i.d)×(c&i.d) ' a.c%b.d
c 2%d 2 %i. b.c&a.d
c 2%d 2 .
(III.16)
The Euclidean space  ú2 endowed with the arithmetic operations (III.1),  (III.3) and (III.5)
resembles a number system, except that the “numbers” involved cannot be ordered. However, it
is possible to measure the distance between these “numbers”, using the Euclidean norm:
|a%i.b| '
def.
4555
4555
a
b
'
a 2%b 2 '
(a%i.b)×(a&i.b).
(III.17)
If the “numbers” in this system are denoted  by (III.10), and standard arithmetic operations are
applied, treating   i as 
 and  i.0 as 0, the results are the same as for the arithmetic operations
&1
(III.1),  (III.3) and (III.5) on the elements of  ú2.  Therefore, we may interpret  (III.10) as a
number, bearing in mind that this number has two dimensions if b… 0.
From now on I will use the standard notation for multiplication, i.e., (a%i.b)(c%i.d)
instead of (III.13).
The part a of 
 is called the real part of the complex number involved, denoted by
a%i.b
 and b is called the imaginary part, denoted by 
 Moreover, 
Re(a%i.b) ' a,
Im(a%i.b) ' b.
a&i.b
is called the complex conjugate of 
 and vice versa. The complex conjugate of 
 is
a%i.b
z ' a%i.b
denoted by a bar:
 It follows from (III.12) that for 
 and w = c + i.d, 
¯z ' a&i.b.
z ' a%i.b
 Moreover,  
 Finally, the complex number system itself is denoted by  ÷.
z w ' ¯z. ¯w.
|z| '
z ¯z .
 

427
III.2.
The complex exponential function
Recall that for real-valued x the exponential function 
 also denoted by 
  has
e x,
exp(x),
the series representation 
e x ' j
4
k'0
x k
k! .
(III.18)
The property 
 corresponds to the equality
e x%y ' e xe y
j
4
k'0
(x%y)k
k!
' j
4
k'0
1
k! j
k
m'0
k
m x k&my m ' j
4
k'0 j
k
m'0
x k&m
(k&m)!
y m
m!
'
j
4
k'0
x k
k!
j
4
m'0
y m
m!
(III.19)
The first equality in (III.19) is due to the binomial expansion, and the last equality follows easily
by rearranging the summation. It is easy to see that (III.19) also holds for complex valued x and y.
Therefore, we can define the complex exponential function by the series expansion (III.18):
e a%i.b '
def.
j
4
k'0
(a%i.b)k
k!
' j
4
k'0
a k
k! j
4
m'0
(i.b)m
m!
' e a j
4
m'0
i m.b m
m!
' e a j
4
m'0
(&1)m.b 2m
(2m)!
% i.j
4
m'0
(&1)m.b 2m%1
(2m%1)!
.
(III.20)
Moreover, it follows from Taylor’s theorem that
cos(b) ' j
4
m'0
(&1)m.b 2m
(2m)!
,
sin(b) ' j
4
m'0
(&1)m.b 2m%1
(2m%1)!
,
(III.21)
so that 
e a%i.b ' e a [cos(b) % i.sin(b)].
(III.22)
Setting a = 0, the latter equality yields the following expressions for the cosines and sinus in

428
terms of the complex exponential function:
cos(b) ' e i.b%e &i.b
2
,
sin(b) ' e i.b&e &i.b
2.i
.
(III.23)
These expressions are handy in recovering the sinus-cosines formulas:
sin(a)sin(b) ' [cos(a & b) & cos(a % b)]/2
sin(a)cos(b) ' [sin(a % b) % sin(a & b)]/2
cos(a)sin(b) ' [sin(a % b) & sin(a & b)]/2
cos(a)cos(b) ' [cos(a % b) % cos(a & b)]/2
sin(a % b) ' sin(a)cos(b) % cos(a)sin(b)
cos(a % b) ' cos(a)cos(b) & sin(a)sin(b)
sin(a & b) ' sin(a)cos(b) & cos(a)sin(b)
cos(a & b) ' cos(a)cos(b) % sin(a)sin(b
(III.24)
Moreover, it follows from (III.22) that for natural numbers n,
e i.n.b ' [cos(b) % i.sin(b)]n ' cos(n.b) % i.sin(n.b).
(III.25)
This result is known as DeMoivre’s formula. It also holds for real numbers n, as we will see
below.
Finally, note that any complex number z = a + i.b can be expressed as
z ' a %i .b ' |z|.
a
a 2%b 2
% i.
b
a 2%b 2
' |z|.[cos(2πφ) % i.sin(2πφ)]
' |z|.exp(i.2πφ),
(III.26)
where φ 0 [0,1] is such that 2πφ ' arccos(a/ a 2%b 2) ' arcsin(b/ a 2%b 2).
III.3.
The complex logarithm
Similarly to the natural logarithm ln(.), the complex logarithm log(z),  z 0 ÷, is a complex
number a+i.b = log(z) such that exp(a+i.b) = z, hence it follows from  (III.25) that z =

429
exp(a)[cos(b) + i.sin(b)] and consequently, since 
|exp(-a).z| = |cos(b)+ i.sin(b)| =  
 = 1, 
cos2(b)%sin2(b)
we have that exp(a) = |z| and  exp(i.b) = z/|z|. The first equation has a unique solution, a = ln(|z|),
as long as z … 0. The second equation reads as
cos(b) % i.sin(b) ' (Re(z) % i.Im(z))/|z|,
(III.27)
hence cos(b) =  Re(z)/|z|, sin(b) = Im(z)/|z|, so that b = arctan(Im(z)/Re(z)). However, equation
(III.27) also holds if we add or subtract multiples of B to or from b, because tan(b) = tan(b+m.B)
for arbitrary integers m, hence 
log(z) ' ln(|z|) % i.[arctan(Im(z)/Re(z)) % mπ],
m ' 0,±1,±2,±3,.....
(III.28)
Therefore, the complex logarithm is not uniquely defined. 
The imaginary part of (III.28) is usually denoted by
arg(z) ' arctan(Im(z)/Re(z)) % mπ,
m ' 0,±1,±2,±3,.....
(III.29)
It is the angle in radians indicated in Figure 1, eventually rotated multiples of 180 degrees
clockwise or anticlockwise:
Figure III.1: arg(z)

430
Note that Im(z)/Re(z) is the tangents of the angle arg(z), hence arctan(Im(z)/Re(z)) is the angle
itself.
With the complex exponential function and logarithm defined, we can now define the
power zw as the complex number a+i.b such that a+i.b = exp(w.log(z)), which exists if |z| > 0.
Consequently, DeMoivre’s formula carries over to all real-valued powers n:
[cos(b) % i.sin(b)]n ' e i.b n ' e i.n.b ' cos(n.b) % i.sin(n.b).
(III.30)
III.4
Series expansion of the complex logarithm
For the case x 0 ú, |x| < 1, it follows from Taylor’s theorem that ln(1+x) has the series
representation
ln(1%x) ' j
4
k'1
(&1)k&1x k/k.
(III.31)
The question I will address now is whether this series representation carries over if we replace x
by i.x, because this will yield a useful approximation of exp(i.x) which plays a key role in proving
central limit theorems for dependent random variables.1 See Chapter 7.
If (III.31) carries over we can write, for arbitrary integers m,
log(1%i.x) ' j
4
k'1
(&1)k&1i kx k/k % i.mπ
' j
4
k'1
(&1)2k&1i 2kx 2k/(2k) % j
4
k'1
(&1)2k&1&1i 2k&1x 2k&1/(2k&1) % i.mπ
' j
4
k'1
(&1)k&1x 2k/(2k) % i.j
4
k'1
(&1)k&1x 2k&1/(2k&1)% i.mπ
(III.32)
On the other hand, it follows from  (III.28) that
log(1%i.x) '
1
2ln(1%x 2) % i.[arctan(x) % mπ].
(III.33)

431
Therefore, we need to verify that for  x 0 ú, |x| < 1,
1
2ln(1%x 2) ' j
4
k'1
(&1)k&1x 2k/(2k)
(III.34)
and 
arctan(x) ' j
4
k'1
(&1)k&1x 2k&1/(2k&1).
(III.35)
Equation (III.34) follows from (III.31) by replacing x with x2.  Equation (III.35) follows from
d
dxj
4
k'1
(&1)k&1x 2k&1/(2k&1) ' j
4
k'1
(&1)k&1x 2k&2 ' j
4
k'0
&x 2 k '
1
1%x 2
(III.36)
and the facts that arctan(0) = 0 and
darctan(x)
dx
'
1
1%x 2 .
(III.37)
Therefore, the series representation (III.32) is true. 
III.5.
Complex integration
In probability and statistics we encounter complex integrals mainly in the form of
characteristic functions, which for absolutely continuous random variables are integrals over
complex-valued functions with real-valued arguments. Such functions take the form  
f(x) ' φ(x) % i.ψ(x), x 0 ú,
(III.38)
where N and R are real-valued functions on ú. Therefore, we may define the (Lebesgue) integral
of f over an interval [a,b] simply as

432
1.
For  x 0 ú with |x| < 1,  
 where  
exp(i.x) ' (1%i.x)exp(&x 2/2 % r(x)),
|r(x)| # |x|3.
m
b
a
f(x)dx ' m
b
a
φ(x)dx % i.m
b
a
ψ(x)dx,
(III.39)
provided of course that the latter two integrals are defined. Similarly, if µ is a probability
measure on the Borel sets in úk and Re(f(x)) and Im(f(x)) are Borel measurable real functions on 
úk,  then 
mf(x)dµ(x) ' mRe(f(x))dµ(x) % i.mIm(f(x))dµ(x),
(III.40)
provided  that the latter two integrals are defined.
Integrals of complex-valued functions of complex variables are much trickier, though.
See for example Ahlfors (1966). However, these types of integrals have limited applicability in
econometrics, and are therefore not discussed here.
Endnote

433
References
Ahlfors, L. V. (1966): Complex Analysis. New York: McGraw-Hill.
Bernanke, B. S. (1986): "Alternative Explanations of the Money-Income Correlation", Carnegie-
Rochester Conference Series on Public Policy 25, 49-100. 
Bierens, H. J. (1994): Topics in Advanced Econometrics: Estimation, Testing, and Specification
of Cross-Section and Time Series Models, Cambridge, UK: Cambridge University Press.
Billingsley, P. (1986): Probability and Measure. New York: John Wiley.
Box, G. E. P., and G. M. Jenkins (1976): Time Series Analysis: Forecasting and Control. San
Francisco: Holden-Day.
Chung, K. L.  (1974): A Course in Probability Theory  (Second edition). New York: Academic
Press.
Davidson, J. (1994):  Stochastic Limit Theory. Oxford, UK: Oxford University Press.
Etemadi, N. (1981): "An Elementary Proof of the Strong Law of Large Numbers", Zeitschrift fur
Wahrscheinlichkeitstheorie und Verwandte Gebiete 55, 119-122.
Fuller, W. A. (1996):  Introduction to Statistical Time Series. New York: John Wiley.
Gallant, A. R. (1997): An Introduction to Econometric Theory. Princeton, NJ.: Princeton
University Press.
Jennrich, R. I. (1969): “ Asymptotic Properties of Non-Linear Least Squares Estimators”, Annals
of Mathematical Statistics 40, 633-643.
McLeish, D. L. (1974): “Dependent Central Limit Theorems and Invariance Principles”, Annals
of Probability 2, 620-628.

434
McLeish, D. L. (1975): “A Maximal Inequality and Dependent Strong Laws”,  Annals of
Probability 3, 329-339.
Press, W. H., B. P. Flannery, S. A Teukolsky, and W. T. Vetterling (1989):  Numerical Recipes
(FORTRAN Version),  Cambridge, UK: Cambridge University Press.
Royden, H.L. (1968): Real Analysis. London: Macmillan.
Sims, C.A. (1980): "Macroeconomics and Reality", Econometrica 48, 1-48.
Sims, C.A. (1982): "Policy Analysis with Econometric Models", Brookings Papers on
Economics Activity 1, 107-152.
Sims, C.A. (1986): "Are Forecasting Models Usable for Policy Analysis?", Federal Reserve
Bank of Minneapolis Quarterly Review, 1-16.
Stokey, N.L., R.E. Lucas, and E. Prescott (1989): Recursive Methods in Economic Dynamics.
Cambridge, MA: Harvard University Press.
Tobin, J. (1958): ''Estimation of Relationships for Limited Dependent Variables'', Econometrica
26, 24-36.
Wold, H. (1938): A Study in the Analysis of Stationary Time Series, Upsala, Sweden: Almqvist
and Wiksell.


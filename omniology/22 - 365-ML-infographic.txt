Linear  
Regression
Logistic 
Regression
Ridge 
Regression
Lasso 
Regression
K-Nearest
Neighbors
Decision 
Trees
Naïve 
Bayes
Support-
Vector  
Machines
Random  
Forests
XGBoost
Supervised Learning
Description
Supervised learning 
algorithm that fits a 
linear equation based on 
the training data. 
The equation is used for 
predictions on new 
data.
Classification algorithm 
that predicts the 
probability of an event 
occurring using a logistic 
function. 
Logistic regression can 
transform into its logit 
form, where the log of 
the odds is equal to a 
linear model. 
Applied whenever we 
want to predict 
categorical outputs.
Regression algorithm 
that applies 
regularization to deal 
with overfitted data.
The method uses L2 
regularization. 
Regression algorithm 
that applies 
regularization and 
feature selection to deal 
with overfitted data.
The method uses L1 
regularization. 
An algorithm that 
classifies a sample 
based on the category 
of its K-nearest 
neighbors, where K is an 
integer. 
Algorithm that creates a 
tree-like structure with 
questions regarding the 
input posed as tree 
nodes (e.g., is the input < 
2.6?). It is primarily used 
for classification.
The structure is 
hierarchical and can be 
easily visualized. 
An algorithm that 
performs  classification 
according to Bayes’ 
theorem. 
The model assigns a 
sample to the class with 
the largest conditional 
probability. 
Supervised learning 
models that construct a 
maximal margin 
hyperplane during 
training to find the best 
solution for the data.
SVMs can employ 
different kernels to 
separate the space and 
ensure further flexibility. 
Algorithm made up of 
many decision trees. 
Its result is determined 
either by the average of 
all outputs or the most 
popular outcome. 
Bootstrap aggregating 
generates different 
datasets from the 
original and feeds them 
to the trees. A subset of 
features is chosen at 
random for each tree. 
The goal is to reduce 
overfitting and improve 
accuracy. 
Tree boosting system 
that is sparsity-aware 
and performs weighted 
approximate tree 
learning.
XGBoost is very scalable 
and includes automatic 
feature selection. 
Used for
Regression
Classification
Regression
Regression
Regression  
Classification
Regression  
Classification
Classification
Regression 
Classification
Regression  
Classification
Regression  
Classification
Input
Numerical
Categorical 
Numerical
Categorical 
Numerical
Categorical 
Numerical
Categorical 
Numerical
Numerical
Categorical 
Numerical
Categorical 
Numerical
Categorical 
Numerical
Categorical 
Numerical
Categorical 
Handles:
Small dataset
Medium dataset
Large dataset
Sparse data
High dimensions
Small dataset
Medium dataset
Large dataset
Sparse data
High dimensions
Small dataset
Medium dataset
Large dataset
Sparse data
High dimensions
Small dataset
Medium dataset
Large dataset
Sparse data
High dimensions
Small dataset
Medium dataset
Large dataset
Sparse data
High dimensions
Small dataset
Medium dataset
Large dataset
Sparse data
High dimensions
Small dataset
Medium dataset
Large dataset
Sparse data
High dimensions
Small dataset
Medium dataset
Large dataset
Sparse data
High dimensions
Small dataset
Medium dataset
Large dataset
Sparse data
High dimensions
Small dataset
Medium dataset
Large dataset
Sparse data
High dimensions
Preprocessing
q Standardizing the 
inputs
q Removing irrelevant 
features
q Standardizing the 
inputs
q Removing irrelevant 
features
q Standardizing the 
inputs
q Removing irrelevant 
features
q Standardizing the 
inputs
q Standardizing the 
inputs
q No preprocessing 
required
q Tokenizing – for 
Multinomial and 
Complement Naïve 
Bayes
q Encoding – for 
Categorical Naïve 
Bayes 
q Input needs to be 
rescaled to [-1,1]
q No preprocessing 
required
q Data needs to be in a 
DMatrix
Algorithm 
speed
Training:
Fast
Testing:
Fast 
Training:
Fast
Testing:
Fast 
Training:
Fast
Testing:
Fast 
Training:
Fast
Testing:
Fast 
Training:
Fast
Testing:
Slow 
Training:
Fast
Testing:
Really fast
Training:
Slow 
Testing:
Fast 
Training:
Slow 
Testing:
Fast 
Training:
Fast
Testing:
Fast 
Training:
Fast
Testing:
Fast 
Avoid  
overfitting?
q Regularization
q Regularization
q Adjust the penalty 
term 
q Adjust the penalty 
term 
q Increase the number 
of neighbors
q Perform pruning 
(during or after 
training)
q Resistant to 
overfitting
q Resistant to 
overfitting
q The construction 
prevents overfitting
q Prune the individual 
trees
q Resistant to 
overfitting
Pros
q Intuitiv1
q Easy to interpret,
q Works very well with 
linear data
q Intuitiv1
q Easy to interpret,
q Easy to implemenP
q Does not require a 
linear relationship 
between the 
dependent and 
independent variableT
q Shows which 
predictors are 
important
q Prevents overfitting 
and multicollinearity 
issueT
q Lowers variance
q Prevents overfitting 
and multicollinearity 
issues
q Lowers variance 
q Performs feature 
selection 
q Intuitive
q Easy to implement
q Suitable for non-linear 
problems
q Fast training 
q Simple to understand 
and interpret
q In-built feature 
selection
q Requires little data 
preprocessing
q Performs well with 
large datasets
q Fast testing on new 
data
q Compatible with 
limited-resource 
platforms
q Suitable for non-linear 
problems
q Easy to implement
q Learns well from small 
datasets
q Makes predictions in 
real time
q Suitable for non-linear 
problems
q Irrelevant features do 
not affect the 
performance 
q Fast testing on new 
data
q Flexibility – handles 
linear and non-linear 
problems, regression 
and classification
q Handles sparse data
q Overcomes the curse 
of dimensionality 
q Requires little to no 
data preprocessing 
q Performs well with 
large datasets
q Automatically handles 
overfitting in most 
cases
q Lots of 
hyperparameters to 
control
q Gives better results 
than decision treeT
q Suitable for non-linear 
problems 
q Easy to parallelize
q Handles sparse data
q Smart trees penalizer  

q Easily scalable 
Cons
q Assumes linearity 
between dependent 
and independent 
variables, otherwise it 
performs weakly
q Sensitive to outliers  
q Prone to overfitting 
when dealing with 
high-dimensional data 
(the curse of 
dimensionalityl
q Requires large sample 
sizes
q Limited to linearly 
separable problems 
q Increases bias
q Difficult to interpret 
the coefficients in the 
final model - they 
shrink towards 0
q Sensitive to irrelevant 
features 
q Increases bias
q Difficult to interpret 
the coefficients in the 
final model - they 
shrink towards 0 
q Sensitive to irrelevant 
features
q Training can take up 
too much computer 
memory
q Testing can be 
computationally 
intensive
q Suffers from the curse 
of dimensionality – 
the number of data 
points is comparable 
to the number of 
dimensions
q Not suitable for 
extrapolation 
problems
q Not suitable for 
categorical data 
q Sometimes unstable – 
small variations in the 
input data can result 
in big changes to the 
tree structure
q Greedy learning 
algorithms not 
guaranteed to find 
the global optimal 
solution
q Prone to overfitting 
q Difficult to optimize as 
there are not many 
hyperparameters
q Does not solve 
regression problems 
well as it provides a 
piecewise constant 
approximation 
q Does not consider 
feature 
dependencies
q Not suitable for 
regression tasks
q Bad estimator 
q Slow training, 
especially for non-
linear kernels
q Low interpretability
q Not suitable for large 
datasets 
q Less interpretable 
than decision trees – 
black box model
q Does not solve 
regression problems 
well
q Outperformed by 
gradient-boosted 
trees 
q Lack of 
interpretability
q Difficult to optimize 
the many different 
parameters 
Applications  
Forecasting
Medical research
Trends evaluation
Variable 
dependencies  
estimation
Medical research
Gaming
Text editing
Advertising
Developing 
software  
packages
Credit card  
default predictions
Genetic studies
Water resource  
studies
Clinical measures
House price  
predictions
Forecasting
Time series  
prediction  
Applied stress 
testing 
Text mining
Facial recognition
Outlier detection  
and fraud 
prevention
Loan default  
predictions
Predicting bankruptcy
Succession 
planning
Expansion of  
production
Credit card  
fraud detection
Medical diagnosis
Spam filtering
Document  
categorization
News articles  
categorization
Sentiment  
analysis
Recommendation  
systems
Time series  
prediction
Facial  
recognition 
Data processing 
for medical 
diagnosis
Product  
recommendation
Credit card  
fraud detection
Medical diagnosis
Store sales 
prediction
Ad click-through  
rate prediction
Motion detection
Product  
categorization
Starter 
Datasets 
Real-world 
datasets
q California Housing 
PriceT
q Medical insurance 
costs 
Toy datasets
q Iris
q Digits
q Breast cancer
Real-world 
datasets
q Labeled Faces in the 
Wild
Real-world 
datasets
q The Ames Housing 
Dataset
q Food for All 
Toy datasets
q Diabetes Dataset 
q Hitters Baseball Data
Real-world 
datasets
q The Ames Housing 
Dataset
Toy datasets
q Iris 
q Diabetes 
q Digits  
q Wine 
q Breast cancer  
Real-world 
datasets
q Olivetti faces
q California housing
 
Toy datasets
q Iris  
Real-world 
datasets
q Census income
Toy datasets
q Iris
q Digits
q Breast cancer 
Real-world 
datasets
q Olivetti faces
q 20 newsgroups 
q 20 newsgroups 
vectorized
Toy datasets
q Iris
q Web purchases
Real-world 
datasets
q MNIST
q Student-dropouts
q Mushrooms data  
Toy datasets
q Iris
Real-world 
datasets
q Census income 
Toy datasets
q Iris
Real-world 
datasets
q MNIST
q Student-dropouts 
Linear  
Regression
Logistic 
Regression
Ridge 
Regression
Lasso 
Regression
K-Nearest
Neighbors
Decision 
Trees
Naïve 
Bayes
Support
Vector  
Machines
Random  
Forests
XGBoost
Increasing Complexity 
Increasing Interpretability
www.365datascience.com


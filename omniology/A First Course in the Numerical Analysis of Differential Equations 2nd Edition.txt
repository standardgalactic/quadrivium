
This page intentionally left blank

A First Course in the Numerical Analysis
of Differential Equations
Numerical analysis presents different faces to the world. For mathematicians it is a bona Ô¨Åde
mathematical theory with an applicable Ô¨Çavour. For scientists and engineers it is a practical,
applied subject, part of the standard repertoire of modelling techniques. For computer
scientists it is a theory on the interplay of computer architecture and algorithms for real-
number calculations.
The tension between these standpoints is the driving force of this book, which presents
a rigorous account of the fundamentals of numerical analysis both of ordinary and partial
differential equations. The point of departure is mathematical, but the exposition strives to
maintain a balance among theoretical, algorithmic and applied aspects of the subject.
This new edition has been extensively updated, and includes new chapters on developing
subject areas: geometric numerical integration, an emerging paradigm for numerical
computation that exhibits exact conservation of important geometric and structural features
of the underlying differential equation; spectral methods, which have come to be seen in
the last two decades as a serious competitor to Ô¨Ånite differences and Ô¨Ånite elements; and
conjugate gradients, one of the most powerful contemporary tools in the solution of sparse
linear algebraic systems.
Other topics covered include numerical solution of ordinary differential equations by
multistep and Runge‚ÄìKutta methods; Ô¨Ånite difference and Ô¨Ånite elements techniques for
the Poisson equation; a variety of algorithms to solve large, sparse algebraic systems;
methods for parabolic and hyperbolic differential equations and techniques for their
analysis. The book is accompanied by an appendix that presents brief back-up in a number
of mathematical topics.
Professor Iserles concentrates on fundamentals: deriving methods from Ô¨Årst principles,
analysing them with a variety of mathematical techniques and occasionally discussing
questions of implementation and applications. By doing so, he is able to lead the reader
to a theoretical understanding of the subject without neglecting its practical aspects. The
outcome is a textbook that is mathematically honest and rigorous and provides its target
audience with a wide range of skills in both ordinary and partial differential equations.

Cambridge Texts in Applied Mathematics
All titles listed below can be obtained from good booksellers or from Cambridge University Press. For a complete
series listing, visit http://www.cambridge.org/uk/series/sSeries.asp?code=CTAM
RareÔ¨Åed Gas Dynamics: From Basic Concepts to Actual Calculations
Carlo Cercignani
Symmetry Methods for Differential Equations: A Beginner‚Äôs Guide
Peter E. Hydon
High Speed Flow
C. J. Chapman
Wave Motion
J. Billingham and A. C. King
An Introduction to Magnetohydrodynamics
P. A. Davidson
Linear Elastic Waves
John G. Harris
Vorticity and Incompressible Flow
Andrew J. Majda and Andrea L. Bertozzi
InÔ¨Ånite-Dimensional Dynamical Systems
James C. Robinson
Introduction to Symmetry Analysis
Brian J. Cantwell
B√§cklund and Darboux Transformations
C. Rogers and W. K. Schief
Finite Volume Methods for Hyperbolic Problems
Randall J. LeVeque
Introduction to Hydrodynamic Stability
P. G. Drazin
Theory of Vortex Sound
M. S. Howe
Scaling
Grigory Isaakovich Barenblatt
Complex Variables: Introduction and Applications (2nd Edition)
Mark J. Ablowitz and Athanassios S. Fokas
A First Course in Combinatorial Optimization
Jon Lee
Practical Applied Mathematics: Modelling, Analysis, Approximation
Sam Howison
An Introduction to Parallel and Vector ScientiÔ¨Åc Computation
Ronald W. Shonkwiler and Lew Lefton
A First Course in Continuum Mechanics
Oscar Gonzalez and Andrew M. Stuart
Applied Solid Mechanics
Peter Howell, Gregory Kozyreff and John Ockendon

A First Course in
the Numerical Analysis
of Differential Equations
Second Edition
ARIEH ISERLES
Department of Applied Mathematics and Theoretical Physics
University of Cambridge

CAMBRIDGE UNIVERSITY PRESS
Cambridge, New York, Melbourne, Madrid, Cape Town, Singapore, S√£o Paulo
Cambridge University Press
The Edinburgh Building, Cambridge CB2 8RU, UK
First published in print format
ISBN-13    978-0-521-73490-5
ISBN-13
978-0-511-50637-6
¬© A. Iserles 2009
2008
Information on this title: www.cambridge.org/9780521734905
This publication is in copyright. Subject to statutory exception and to the 
provision of relevant collective licensing agreements, no reproduction of any part
may take place without the written permission of Cambridge University Press.
Cambridge University Press has no responsibility for the persistence or accuracy 
of urls for external or third-party internet websites referred to in this publication, 
and does not guarantee that any content on such websites is, or will remain, 
accurate or appropriate.
Published in the United States of America by Cambridge University Press, New York
www.cambridge.org
eBook (EBL)
paperback

Contents
Preface to the second edition
page ix
Preface to the Ô¨Årst edition
xiii
Flowchart of contents
xix
I
Ordinary diÔ¨Äerential equations
1
1
Euler‚Äôs method and beyond
3
1.1
Ordinary diÔ¨Äerential equations and the Lipschitz condition
. . . . . .
3
1.2
Euler‚Äôs method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.3
The trapezoidal rule . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
1.4
The theta method
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
Comments and bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
2
Multistep methods
19
2.1
The Adams method
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
2.2
Order and convergence of multistep methods
. . . . . . . . . . . . . .
21
2.3
Backward diÔ¨Äerentiation formulae . . . . . . . . . . . . . . . . . . . . .
26
Comments and bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
3
Runge‚ÄìKutta methods
33
3.1
Gaussian quadrature . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
3.2
Explicit Runge‚ÄìKutta schemes
. . . . . . . . . . . . . . . . . . . . . .
38
3.3
Implicit Runge‚ÄìKutta schemes
. . . . . . . . . . . . . . . . . . . . . .
41
3.4
Collocation and IRK methods . . . . . . . . . . . . . . . . . . . . . . .
43
Comments and bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
4
StiÔ¨Äequations
53
4.1
What are stiÔ¨ÄODEs?
. . . . . . . . . . . . . . . . . . . . . . . . . . .
53
4.2
The linear stability domain and A-stability
. . . . . . . . . . . . . . .
56
4.3
A-stability of Runge‚ÄìKutta methods . . . . . . . . . . . . . . . . . . .
59
v

vi
Contents
4.4
A-stability of multistep methods
. . . . . . . . . . . . . . . . . . . . .
63
Comments and bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . .
68
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
5
Geometric numerical integration
73
5.1
Between quality and quantity . . . . . . . . . . . . . . . . . . . . . . .
73
5.2
Monotone equations and algebraic stability
. . . . . . . . . . . . . . .
77
5.3
From quadratic invariants to orthogonal Ô¨Çows . . . . . . . . . . . . . .
83
5.4
Hamiltonian systems . . . . . . . . . . . . . . . . . . . . . . . . . . . .
87
Comments and bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
6
Error control
105
6.1
Numerical software vs. numerical mathematics
. . . . . . . . . . . . .
105
6.2
The Milne device . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
107
6.3
Embedded Runge‚ÄìKutta methods
. . . . . . . . . . . . . . . . . . . .
113
Comments and bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . .
119
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
121
7
Nonlinear algebraic systems
123
7.1
Functional iteration
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
123
7.2
The Newton‚ÄìRaphson algorithm and its
modiÔ¨Åcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
127
7.3
Starting and stopping the iteration . . . . . . . . . . . . . . . . . . . .
130
Comments and bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . .
132
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
133
II
The Poisson equation
137
8
Finite diÔ¨Äerence schemes
139
8.1
Finite diÔ¨Äerences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
139
8.2
The Ô¨Åve-point formula for ‚àá2u = f . . . . . . . . . . . . . . . . . . . .
147
8.3
Higher-order methods for ‚àá2u = f . . . . . . . . . . . . . . . . . . . .
158
Comments and bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . .
163
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
166
9
The Ô¨Ånite element method
171
9.1
Two-point boundary value problems . . . . . . . . . . . . . . . . . . .
171
9.2
A synopsis of FEM theory . . . . . . . . . . . . . . . . . . . . . . . . .
184
9.3
The Poisson equation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
192
Comments and bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . .
200
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
201

Contents
vii
10 Spectral methods
205
10.1 Sparse matrices vs. small matrices . . . . . . . . . . . . . . . . . . . .
205
10.2 The algebra of Fourier expansions
. . . . . . . . . . . . . . . . . . . .
211
10.3 The fast Fourier transform . . . . . . . . . . . . . . . . . . . . . . . . .
214
10.4 Second-order elliptic PDEs
. . . . . . . . . . . . . . . . . . . . . . . .
219
10.5 Chebyshev methods
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
222
Comments and bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . .
225
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
230
11 Gaussian elimination for sparse linear equations
233
11.1 Banded systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
233
11.2 Graphs of matrices and perfect Cholesky
factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
238
Comments and bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . .
243
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
246
12 Classical iterative methods for sparse linear equations
251
12.1 Linear one-step stationary schemes . . . . . . . . . . . . . . . . . . . .
251
12.2 Classical iterative methods
. . . . . . . . . . . . . . . . . . . . . . . .
259
12.3 Convergence of successive over-relaxation
. . . . . . . . . . . . . . . .
270
12.4 The Poisson equation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
281
Comments and bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . .
286
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
288
13 Multigrid techniques
291
13.1 In lieu of a justiÔ¨Åcation . . .
. . . . . . . . . . . . . . . . . . . . . . . .
291
13.2 The basic multigrid technique . . . . . . . . . . . . . . . . . . . . . . .
298
13.3 The full multigrid technique . . . . . . . . . . . . . . . . . . . . . . . .
302
13.4 Poisson by multigrid . . . . . . . . . . . . . . . . . . . . . . . . . . . .
303
Comments and bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . .
307
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
308
14 Conjugate gradients
309
14.1 Steepest, but slow, descent
. . . . . . . . . . . . . . . . . . . . . . . .
309
14.2 The method of conjugate gradients . . . . . . . . . . . . . . . . . . . .
312
14.3 Krylov subspaces and preconditioners
. . . . . . . . . . . . . . . . . .
317
14.4 Poisson by conjugate gradients
. . . . . . . . . . . . . . . . . . . . . .
323
Comments and bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . .
325
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
327
15 Fast Poisson solvers
331
15.1 TST matrices and the Hockney method
. . . . . . . . . . . . . . . . .
331
15.2 Fast Poisson solver in a disc . . . . . . . . . . . . . . . . . . . . . . . .
336
Comments and bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . .
342
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
344

viii
Contents
III
Partial diÔ¨Äerential equations of evolution
347
16 The diÔ¨Äusion equation
349
16.1 A simple numerical method . . . . . . . . . . . . . . . . . . . . . . . .
349
16.2 Order, stability and convergence
. . . . . . . . . . . . . . . . . . . . .
355
16.3 Numerical schemes for the diÔ¨Äusion equation
. . . . . . . . . . . . . .
362
16.4 Stability analysis I: Eigenvalue techniques . . . . . . . . . . . . . . . .
368
16.5 Stability analysis II: Fourier techniques
. . . . . . . . . . . . . . . . .
372
16.6 Splitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
378
Comments and bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . .
381
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
383
17 Hyperbolic equations
387
17.1 Why the advection equation? . . . . . . . . . . . . . . . . . . . . . . .
387
17.2 Finite diÔ¨Äerences for the advection equation . . . . . . . . . . . . . . .
394
17.3 The energy method . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
403
17.4 The wave equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
407
17.5 The Burgers equation
. . . . . . . . . . . . . . . . . . . . . . . . . . .
413
Comments and bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . .
418
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
422
Appendix
BluÔ¨Äer‚Äôs guide to useful mathematics
427
A.1 Linear algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
428
A.1.1
Vector spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . .
428
A.1.2
Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
429
A.1.3
Inner products and norms . . . . . . . . . . . . . . . . . . . . .
432
A.1.4
Linear systems . . . . . . . . . . . . . . . . . . . . . . . . . . .
434
A.1.5
Eigenvalues and eigenvectors
. . . . . . . . . . . . . . . . . . .
437
Bibliography
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
439
A.2 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
439
A.2.1
Introduction to functional analysis . . . . . . . . . . . . . . . .
439
A.2.2
Approximation theory . . . . . . . . . . . . . . . . . . . . . . .
442
A.2.3
Ordinary diÔ¨Äerential equations . . . . . . . . . . . . . . . . . .
445
Bibliography
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
446
Index
447

Preface to the second edition
In an ideal world this second edition should have been written at least three years ago
but, needless to say, this is not an ideal world. Annoyingly, there are just 24 hours per
day, rather less annoyingly I have joyously surrendered myself to the excitements of
my own research and, being rather good at Ô¨Ånding excuses, delayed the second edition
again and again.
Yet, once I braced myself, banished my writer‚Äôs block and started to compose in my
head the new chapters, I was taken over by the sheer pleasure of writing. Repeatedly
I have found myself, as I often do, thanking my good fortune for working in this
particular corner of the mathematical garden, the numerical analysis of diÔ¨Äerential
equations, and striving in a small way to communicate its oft-unappreciated beauty.
The last sentence is bound to startle anybody experienced enough in the fashions
and prejudices of the mathematical world.
Numerical analysis is often considered
neither beautiful nor, indeed, profound. Pure mathematics is beautiful if your heart
goes after the joy of abstraction, applied mathematics is beautiful if you are excited by
mathematics as a means to explain the mystery of the world around us. But numerical
analysis?
Surely, we compute only when everything else fails, when mathematical
theory cannot deliver an answer in a comprehensive, pristine form and thus we are
compelled to throw a problem onto a number-crunching computer and produce boring
numbers by boring calculations. This, I believe, is nonsense.
A mathematical problem does not cease being mathematical just because we have
discretized it. The purpose of discretization is to render mathematical problems, of-
ten approximately, in a form accessible to eÔ¨Écient calculation by computers. This,
in particular, means rephrasing and approximating analytic statements as a Ô¨Ånite se-
quence of algebraic steps. Algorithms and numerical methods are, by their very design,
suitable for computation but it makes them neither simple nor easy as mathematical
constructs. Replacing derivatives by Ô¨Ånite diÔ¨Äerences or an inÔ¨Ånite-dimensional space
by a hierarchy of Ô¨Ånite-dimensional spaces does not necessarily lead to a more fuzzy
form of reasoning. We can still ask proper mathematical questions with uncompromis-
ing rigour and seek answers with the full mathematical etiquette of precise deÔ¨Ånitions,
statements and proofs. The rules of the game do not change at all.
Actually, it is almost inevitable that a discretized mathematical problem is, as a
mathematical problem, more diÔ¨Écult and more demanding of our mathematical in-
genuity. To give just one example, it is usual to approximate a partial diÔ¨Äerential
equation of evolution, an inÔ¨Ånite-dimensional animal, in a Ô¨Ånite-dimensional space
(using, for example, Ô¨Ånite diÔ¨Äerences, Ô¨Ånite elements or a spectral method).
This
Ô¨Ånite-dimensional approximation makes the problem tractable on a computer, a ma-
ix

x
Preface
chine that can execute a Ô¨Ånite number of algebraic operations in Ô¨Ånite time. However,
once we wish to answer the big mathematical question underlying our discourse, how
well does the Ô¨Ånite-dimensional model approximate the original equation, we are com-
pelled to consider not one Ô¨Ånite-dimensional system but an inÔ¨Ånite progression of such
systems, of increasing (and unbounded) dimension. In eÔ¨Äect, we are not just approxi-
mating a single equation but an entire inÔ¨Ånite-dimensional function space. Of course,
if all you want is numbers, you can get away with hand-waving arguments or use the
expertise and experience of others. But once you wish to understand honestly the term
‚Äòanalysis‚Äô in ‚Äònumerical analysis‚Äô, prepare yourself for real mathematical experience.
I hope to have made the case that true numerical analysis operates according to
standard mathematical rules of engagement (while, needless to say, fully engaging with
the algorithmic and applied parts of its inner self). My stronger claim, illustrated in a
small way by the material of this book, is that numerical analysis is perhaps the most
eclectic and demanding client of the entire width and breadth of mathematics. Typ-
ically in mathematics, a discipline rests upon a fairly small number of neighbouring
disciplines: once you visit a mathematical library, you Ô¨Ånd yourself time and again
visiting a fairly modest number of shelves. Not so in the numerical analysis of diÔ¨Äer-
ential equations. Once you want to understand the subject in its breadth, rather than
specializing in a narrow and strictly delineated subset, prepare yourself to navigate
across all library shelves! This volume, being a textbook, is purposefully steering well
clear of deep and diÔ¨Écult mathematics. However, even at the sort of elementary level
of mathematical sophistication suitable for advanced undergraduates, faithful to the
principle that every unusual bit of mathematics should be introduced and explained
I expect the reader to identify the many and varied mathematical sources of our dis-
course. This opportunity to revel and rejoice in the varied mathematical origins of
the subject, of pulling occasional rabbits from all kinds of mathematical hats, is what
makes me so happy to work in numerical analysis. I hope to have conveyed, in a small
and inevitably Ô¨Çawed manner, how diÔ¨Äerent strands of mathematical thinking join
together to form this discipline.
Three chapters have been added to the Ô¨Årst edition to reÔ¨Çect the changing face of
the subject. The Ô¨Årst is on geometric numerical integration, the emerging science of
the numerical computation of diÔ¨Äerential equations in a way that renders exactly their
qualitative features. The second is on spectral methods, an important competitor to
the more established Ô¨Ånite diÔ¨Äerence and Ô¨Ånite element techniques for partial diÔ¨Äeren-
tial equations. The third new chapter reviews the method of conjugate gradients for
the solution of the large linear algebraic systems that occur once partial diÔ¨Äerential
equations are discretized.
Needless to say, the current contents cannot reÔ¨Çect all the many diÔ¨Äerent ideas,
algorithms, methods and insights that, in their totality, make the subject of compu-
tational diÔ¨Äerential equations. Writing a textbook, the main challenge is not what to
include, but what to exclude! It would have been very easy to endure the publisher‚Äôs
unhappiness and expand this book to several volumes, reporting on numerous excit-
ing themes such domain decomposition, meshless methods, wavelet-based methods,
particle methods, homogenization ‚Äì the list goes on and on. Easy, but perhaps not
very illuminating, because this is not a cookbook, a dictionary or a compendium: it
is a textbook that, ideally, should form the backdrop to a lecture course. It would

Preface
xi
not have been very helpful to bury the essential didactic message under a mountain
of facts, exciting and useful as they might be. The main purpose of a lecture course
‚Äì and hence of a textbook ‚Äì is to provide enough material, insight and motivation to
prepare students for further, often independent, study. My aim on these pages has
been to provide this sort of preparation.
The Ô¨Çowchart on p. xix displays the connectivity and logical progression of the
current 17 chapters.
Although it is unlikely that the entire contents of the book
can be encompased in less than a year-long intensive lecture course, the Ô¨Çowchart is
suggestive of many diÔ¨Äerent ways to pick and choose material while maintaining the
inner integrity and coherence of the exposition.
This is the moment to thank all those who helped me selÔ¨Çessly in crafting an edition
better than one I could have written singlehandedly. Firstly, all those users of the Ô¨Årst
edition who have provided me with feedback, communicated errors and misprints,
queried the narrative, lavished praise or extended well-deserved criticism.1 Secondly,
those of my colleagues who read parts of the draft, oÔ¨Äered remarks (mostly encouraging
but sometimes critical: I appreciated both) and frequently saved me from embarrassing
blunders: Ben Adcock, Alfredo DeaÀúno, Euan Spence, Endre S¬®uli and Antonella Zanna.
Thirdly, my friends at Cambridge University Press, in particular David Tranah, who
encouraged this second edition, pushed me when a push was needed, let me get along
without undue harassment otherwise and was always willing to share his immense
experience. Fourthly, my copy editor Susan Parkinson, as always pedantic in the best
sense of the word. Fifthly, the terriÔ¨Åc intellectual environment in the Department
of Applied Mathematics and Theoretical Physics of the University of Cambridge, in
particular among my colleagues and students in the Numerical Analysis Group. We
have managed throughout the years to act not only as a testing bed, and sometimes a
foil, to each other‚Äôs ideas but also as a milieu where it is always delightful to abandon
mathematics for a break of (relatively decent) coÔ¨Äee and uplifting conversation on
just about anything.
And last, but deÔ¨Ånitely not least, my wife and best friend,
Dganit, who has encouraged and helped me always, in more ways than I can count or
Ô¨Çoating-number arithmetic can bear.
And so, over to you, the reader. I hope to have managed to convey to you, even if
in a small and imperfect manner, not just the raw facts that, in their totality, make
up the numerical analysis of diÔ¨Äerential equations, but the beauty and the excitement
of the subject.
Arieh Iserles
August 2008
1I wish to thank less, though, those students who emailed me for solutions to the exercises before
their class assignment was due.


Preface to the Ô¨Årst edition
Books ‚Äì so we are often told ‚Äì should be born out of a sense of mission, a wish to
share knowledge, experience and ideas, a penchant for beauty. This book has been
born out of a sense of frustration.
For the last decade or so I have been teaching the numerical analysis of diÔ¨Äerential
equations to mathematicians, in Cambridge and elsewhere. Examining this extensive
period of trial and (frequent) error, two main conclusions come to mind and both have
guided my choice of material and presentation in this volume.
Firstly, mathematicians are diÔ¨Äerent from other varieties of homo sapiens. It may
be observed that people study numerical analysis for various reasons.
Scientists
and engineers require it as a means to an end, a tool to investigate the subject
matter that really interests them.
Entirely justiÔ¨Åably, they wish to spend neither
time nor intellectual eÔ¨Äort on the Ô¨Åner points of mathematical analysis, typically
preferring a style that combines a cook-book presentation of numerical methods with
a leavening of intuitive and hand-waving explanations. Computer scientists adopt
a diÔ¨Äerent, more algorithmic, attitude. Their heart goes after the clever algorithm
and its interaction with computer architecture. DiÔ¨Äerential equations and their likes
are abandoned as soon as decency allows (or sooner). They are replaced by discrete
models, which in turn are analysed by combinatorial techniques. Mathematicians,
though, follow a diÔ¨Äerent mode of reasoning. Typically, mathematics students are
likely to participate in an advanced numerical analysis course in their Ô¨Ånal year of
undergraduate studies, or perhaps in the Ô¨Årst postgraduate year. Their studies until
that point in time would have consisted, to a large extent, of a progression of formal
reasoning, the familiar sequence of axiom ‚áítheorem ‚áíproof ‚áícorollary ‚áí. . . .
Numerical analysis does not Ô¨Åt easily into this straitjacket, and this goes a long way
toward explaining why many students of mathematics Ô¨Ånd it so unattractive.
Trying to teach numerical analysis to mathematicians, one is thus in a dilemma:
should the subject be presented purely as a mathematical theory, intellectually pleas-
ing but arid insofar as applications are concerned or, alternatively, should the audience
be administered an application-oriented culture shock that might well cause it to vote
with its feet?! The resolution is not very diÔ¨Écult, namely to present the material in
a bona Ô¨Åde mathematical manner, occasionally veering toward issues of applications
and algorithmics but never abandoning honesty and rigour. It is perfectly allowable
to omit an occasional proof (which might well require material outside the scope of
the presentation) and even to justify a numerical method on the grounds of plausi-
bility and a good track record in applications. But plausibility, a good track record,
xiii

xiv
Preface
intuition and old-fashioned hand-waving do not constitute an honest mathematical
argument and should never be presented as such.
Secondly, students should be exposed in numerical analysis to both ordinary and
partial diÔ¨Äerential equations, as well as to means of dealing with large sparse algebraic
systems. The pressure of many mathematical subjects and sub-disciplines is such that
only a modest proportion of undergraduates are likely to take part in more than
a single advanced numerical analysis course. Many more will, in all likelihood, be
faced with the need to solve diÔ¨Äerential equations numerically in the future course of
their professional life. Therefore, the option of restricting the exposition to ordinary
diÔ¨Äerential equations, say, or to Ô¨Ånite elements, while having the obvious merit of
cohesion and sharpness of focus is counterproductive in the long term.
To recapitulate, the ideal course in the numerical analysis of diÔ¨Äerential equations,
directed toward mathematics students, should be mathematically honest and rigorous
and provide its target audience with a wide range of skills in both ordinary and
partial diÔ¨Äerential equations. For the last decade I have been desperately trying to
Ô¨Ånd a textbook that can be used to my satisfaction in such a course ‚Äì in vain. There
are many Ô¨Åne textbooks on particular aspects of the subject: numerical methods
for ordinary diÔ¨Äerential equations, Ô¨Ånite elements, computation of sparse algebraic
systems. There are several books that span the whole subject but, unfortunately, at
a relatively low level of mathematical sophistication and rigour. But, to the best of
my knowledge, no text addresses itself to the right mathematical agenda at the right
level of maturity. Hence my frustration and hence the motivation behind this volume.
This is perhaps the place to review brieÔ¨Çy the main features of this book.
‚ãÜWe cover a broad range of material: the numerical solution of ordinary diÔ¨Äer-
ential equations by multistep and Runge‚ÄìKutta methods; Ô¨Ånite diÔ¨Äerence and
Ô¨Ånite element techniques for the Poisson equation; a variety of algorithms for
solving the large systems of sparse algebraic equations that occur in the course
of computing the solution of the Poisson equation; and, Ô¨Ånally, methods for
parabolic and hyperbolic diÔ¨Äerential equations and techniques for their analysis.
There is probably enough material in this book for a one-year fast-paced course
and probably many lecturers will wish to cover only part of the material.
‚ãÜThis is a textbook for mathematics students. By implication, it is not a text-
book for computer scientists, engineers or natural scientists. As I have already
argued, each group of students has diÔ¨Äerent concerns and thought modes. Each
assimilates knowledge diÔ¨Äerently. Hence, a textbook that attempts to be diÔ¨Äer-
ent things to diÔ¨Äerent audiences is likely to disappoint them all. Nevertheless,
non-mathematicians in need of numerical knowledge can beneÔ¨Åt from this vol-
ume, but it is fair to observe that they should perhaps peruse it somewhat later
in their careers, when in possession of the appropriate degree of motivation and
background knowledge.
On an even more basic level of restriction, this is a textbook, not a monograph or
a collection of recipes. Emphatically, our mission is not to bring the exposition
to the state of the art or to highlight the most advanced developments. Likewise,
it is not our intention to provide techniques that cater for all possible problems

Preface
xv
and eventualities.
‚ãÜAn annoying feature of many numerical analysis texts is that they display inor-
dinately long lists of methods and algorithms to solve any one problem. Thus,
not just one Runge‚ÄìKutta method but twenty! The hapless reader is left with an
arsenal of weapons but, all too often, without a clue which one to use and why.
In this volume we adopt an alternative approach: methods are derived from un-
derlying principles and these principles, rather than the algorithms themselves,
are at the centre of our argument. As soon as the underlying principles are
sorted out, algorithmic Ô¨Åreworks become the least challenging part of numerical
analysis ‚Äì the real intellectual eÔ¨Äort goes into the mathematical analysis.
This is not to say that issues of software are not important or that they are
somehow of a lesser scholarly pedigree. They receive our attention in Chap-
ter 6 and I hasten to emphasize that good software design is just as challenging
as theorem-proving. Indeed, the proper appreciation of diÔ¨Éculties in software
and applications is enhanced by the understanding of the analytic aspects of
numerical mathematics.
‚ãÜA truly exciting aspect of numerical analysis is the extensive use it makes of
diÔ¨Äerent mathematical disciplines. If you believe that numerics are a mathe-
matical cop-out, a device for abandoning mathematics in favour of something
‚Äòsofter‚Äô, you are in for a shock. Numerical analysis is perhaps the most extensive
and varied user of a very wide range of mathematical theories, from basic lin-
ear algebra and calculus all the way to functional analysis, diÔ¨Äerential topology,
graph theory, analytic function theory, nonlinear dynamical systems, number
theory, convexity theory ‚Äì and the list goes on and on. Hardly any theme in
modern mathematics fails to inspire and help numerical analysis. Hence, nu-
merical analysts must be open-minded and ready to borrow from a wide range
of mathematical skills ‚Äì this is not a good bolt-hole for narrow specialists!
In this volume we emphasize the variety of mathematical themes that inspire
and inform numerical analysis. This is not as easy as it might sound, since it
is impossible to take for granted that students in diÔ¨Äerent universities have a
similar knowledge of pure mathematics. In other words, it is often necessary
to devote a few pages to a topic which, in principle, has nothing to do with
numerical analysis per se but which, nonetheless, is required in our exposition.
I ask for the indulgence of those readers who are more knowledgeable in arcane
mathematical matters ‚Äì all they need is simply to skip few pages . . .
‚ãÜThere is a major diÔ¨Äerence between recalling and understanding a mathemati-
cal concept. Reading mathematical texts I often come across concepts that are
familiar and which I have certainly encountered in the past. Ask me, however,
to recite their precise deÔ¨Ånition and I will probably Ô¨Çunk the test. The proper
and virtuous course of action in such an instance is to pause, walk to the nearest
mathematical library and consult the right source. To be frank, although some-
times I pursue this course of action, more often than not I simply go on reading.
I have every reason to believe that I am not alone in this dubious practice.

xvi
Preface
In this volume I have attempted a partial remedy to the aforementioned phe-
nomenon, by adding an appendix named ‚ÄòBluÔ¨Äer‚Äôs guide to useful mathematics‚Äô.
This appendix lists in a perfunctory manner deÔ¨Ånitions and major theorems in
a range of topics ‚Äì linear algebra, elementary functional analysis and approx-
imation theory ‚Äì to which students should have been exposed previously but
which might have been forgotten. Its purpose is neither to substitute elemen-
tary mathematical courses nor to oÔ¨Äer remedial teaching. If you Ô¨Çick too often
to the end of the book in search of a deÔ¨Ånition then, my friend, perhaps you
had better stop for a while and get to grips with the underlying subject, using
a proper textbook. Likewise, if you always pursue a virtuous course of action,
consulting a proper source in each and every case of doubt, please do not allow
me to tempt you oÔ¨Äthe straight and narrow.
‚ãÜPart of the etiquette of writing mathematics is to attribute material and to refer
to primary sources. This is important not just to quench the vanity of one‚Äôs
colleagues but also to set the record straight, as well as allowing an interested
reader access to more advanced material. Having said this, I entertain serious
doubts with regard to the practice of sprinkling each and every paragraph in a
textbook with copious references. The scenario is presumably that, having read
the sentence ‚Äò. . . suppose that x ‚ààU, where U is a foliated widget [37]‚Äô, the reader
will look up the references, identify ‚Äò[37]‚Äô with a paper of J. Bloggs in Proc. SDW,
recognize the latter as Proceedings of the Society of DiÔ¨Äerentiable Widgets, walk
to the library, locate the journal (which will be actually on the shelf, rather
than on loan, misplaced or stolen) . . . All this might not be far-fetched as far as
advanced mathematics monographs are concerned but makes very little sense in
an undergraduate context. Therefore I have adopted a practice whereby there
are no references in the text proper. Instead, each chapter is followed by a section
of ‚ÄòComments and bibliography‚Äô, where we survey brieÔ¨Çy further literature that
might be beneÔ¨Åcial to students (and lecturers).
Such sections serve a further important purpose.
Some students ‚Äì am I too
optimistic? ‚Äì might be interested and inspired by the material of the chapter.
For their beneÔ¨Åt I have given in each ‚ÄòComments and bibliography‚Äô section a
brief discussion of further developments, algorithms, methods of analysis and
connections with other mathematical disciplines.
‚ãÜClarity of exposition often hinges on transparency of notation. Thus, throughout
this book we use the following convention:
‚Ä¢ lower-case lightface sloping letters (a, b, c, Œ±, Œ≤, Œ≥, . . .) represent scalars;
‚Ä¢ lower-case boldface sloping letters (a, b, c, Œ±, Œ≤, Œ≥, . . .) represent vectors;
‚Ä¢ upper-case lightface letters (A, B, C, Œò, Œ¶, . . .) represent matrices;
‚Ä¢ letters in calligraphic font (A, B, C, . . .) represent operators;
‚Ä¢ shell capitals (A, B, C, . . .) represent sets.

Preface
xvii
Mathematical constants like i = ‚àö‚àí1 and e, the base of natural logarithms, are
denoted by roman, rather than italic letters. This follows British typesetting
convention and helps to identify the diÔ¨Äerent components of a mathematical
formula.
As with any principle, our notational convention has its exceptions.
For ex-
ample, in Section 3.1 we refer to Legendre and Chebyshev polynomials by the
conventional notation, Pn and Tn: any other course of action would have caused
utter confusion. And, again as with any principle, grey areas and ambiguities
abound. I have tried to eliminate them by applying common sense but this,
needless to say, is a highly subjective criterion.
This book started out life as two sets of condensed lecture notes ‚Äì one for students of
Part II (the last year of undergraduate mathematics in Cambridge) and the other for
students of Part III (the Cambridge advanced degree course in mathematics). The task
of expanding lecture notes to a full-scale book is, unfortunately, more complicated than
producing a cup of hot soup from concentrate by adding boiling water, stirring and
simmering for a short while. Ultimately, it has taken the better part of a year, shared
with the usual commitments of academic life. The main portion of the manuscript
was written in Autumn 1994, during a sabbatical leave at the California Institute of
Technology (Caltech). It is my pleasant duty to acknowledge the hospitality of my
many good friends there and the perfect working environment in Pasadena.
A familiar computer proverb states that, while the Ô¨Årst 90% of a programming
job takes 90% of the time, the remaining 10% also takes 90% of the time . . . Writing
a textbook follows similar rules and, back home in Cambridge, I have spent several
months reading and rereading the manuscript. This is the place to thank a long list of
friends and colleagues whose help has been truly crucial: Brad Baxter (Imperial Col-
lege, London), Martin Buhmann (Swiss Institute of Technology, Z¬®urich), Yu-Chung
Chang (Caltech), Stephen Cowley (Cambridge), George Goodsell (Cambridge), Mike
Holst (Caltech), Herb Keller (Caltech), Yorke Liu (Cambridge), Michelle Schatzman
(Lyon), Andrew Stuart (Stanford), Stefan Vandewalle (Louven) and Antonella Zanna
(Cambridge).
Some have read the manuscript and oÔ¨Äered their comments.
Some
provided software well beyond my own meagre programming skills and helped with
the Ô¨Ågures and with computational examples.
Some have experimented with the
manuscript upon their students and listened to their complaints. Some contributed
insight and occasionally saved me from embarrassing blunders. All have been help-
ful, encouraging and patient to a fault with my foibles and idiosyncrasies. None is
responsible for blunders, errors, mistakes, misprints and infelicities that, in spite of
my sincerest eÔ¨Äorts, are bound to persist in this volume.
This is perhaps the place to extend thanks to two ‚Äòfriends‚Äô that have made the
process of writing this book considerably easier: the TEX typesetting system and the
MATLAB package. These days we take mathematical typesetting for granted but it is
often forgotten that just a decade ago a mathematical manuscript would have been
hand-written, then typed and retyped and, Ô¨Ånally, typeset by publishers ‚Äì each stage
requiring laborious proofreading. In turn, MATLAB allows us a unique opportunity to
turn our oÔ¨Éce into a computational-cum-graphic laboratory, to bounce ideas oÔ¨Äthe
computer screen and produce informative Ô¨Ågures and graphic displays. Not since the

xviii
Preface
discovery of coÔ¨Äee have any inanimate objects caused so much pleasure to so many
mathematicians!
The editorial staÔ¨Äof Cambridge University Press, in particular Alan Harvey, David
Tranah and Roger Astley, went well beyond the call of duty in being helpful, friendly
and cooperative. Susan Parkinson, the copy editor, has worked to the highest stan-
dards. Her professionalism, diligence and good taste have done wonders in sparing the
readers numerous blunders and the more questionable examples of my hopeless wit.
This is a pleasant opportunity to thank them all.
Last but never the least, my wife and best friend, Dganit. Her encouragement,
advice and support cannot be quantiÔ¨Åed in conventional mathematical terms. Thank
you!
I wish to dedicate this book to my parents, Gisella and Israel.
They are not
mathematicians, yet I have learnt from them all the really important things that have
motivated me as a mathematician: love of scholarship and admiration for beauty and
art.
Arieh Iserles
August 1995

Flowchart of contents
1
Introduction
2
Multistep methods
3
Runge‚ÄìKutta methods
4
StiÔ¨Äequations
8
Finite diÔ¨Äerences
16
The diÔ¨Äusion equation
17
Hyperbolic equations
5
Geometric
integration
6
Error
control
7
Algebraic
systems
9
Finite
elements
10
Spectral
methods
11
Gaussian
elimination
12
Iterative
methods
13
Multigrid
14
Conjugate
gradients
15
Fast
solvers
?
?
?


?


?
?

-






?









6

-
?




?


 -


P A R T I
Ordinary diÔ¨Äerential equations


1
Euler‚Äôs method and beyond
1.1
Ordinary diÔ¨Äerential equations and the Lips-
chitz condition
We commence our exposition of the computational aspects of diÔ¨Äerential equations by
examining closely numerical methods for ordinary diÔ¨Äerential equations (ODEs). This
is important because of the central role of ODEs in a multitude of applications. Not
less crucial is the critical part that numerical ODEs play in the design and analysis of
computational methods for partial diÔ¨Äerential equations (PDEs). Thus, even if your
main interest is in solving PDEs, ideally you should Ô¨Årst master computational ODEs,
not just to familiarize yourself with concepts, terminology and ideas but also because
(as we will see in what follows) many discretization methods for PDEs reduce the
underlying problem to the computation of ODEs.
Our goal is to approximate the solution of the problem
y‚Ä≤ = f(t, y),
t ‚â•t0,
y(t0) = y0.
(1.1)
Here f is a suÔ¨Éciently well-behaved function that maps [t0, ‚àû) √ó Rd to Rd and the
initial condition y0 ‚ààRd is a given vector; Rd denotes here ‚Äì and elsewhere in this
book ‚Äì the d-dimensional real Euclidean space.
The ‚Äòniceness‚Äô of f may span a whole range of desirable attributes. At the very
least, we insist on f obeying, in a given vector norm ‚à•¬∑ ‚à•, the Lipschitz condition
‚à•f(t, x) ‚àíf(t, y)‚à•‚â§Œª‚à•x ‚àíy‚à•
for all
x, y ‚ààRd, t ‚â•t0.
(1.2)
Here Œª > 0 is a real constant that is independent of the choice of x and y ‚Äì a
Lipschitz constant. Subject to (1.2), it is possible to prove that the ODE system (1.1)
possesses a unique solution.1 Taking a stronger requirement, we may stipulate that
f is an analytic function ‚Äì in other words, that the Taylor series of f about every
(t, y0) ‚àà[0, ‚àû) √ó Rd has a positive radius of convergence. It is then possible to prove
that the solution y itself is analytic. Analyticity comes in handy, since much of our
investigation of numerical methods is based on Taylor expansions, but it is often an
excessive requirement and excludes many ODEs of practical importance.
In this volume we strive to steer a middle course between the complementary vices
of mathematical nitpicking and of hand-waving. We solemnly undertake to avoid any
1We refer the reader to the Appendix for a brief refresher course on norms, existence and unique-
ness theorems for ODEs and other useful odds and ends of mathematics.
3

4
Euler‚Äôs method and beyond
needless mention of exotic function spaces that present the theory in its most general
form, whilst desisting from woolly and inexact statements. Thus, we always assume
that f is Lipschitz and, as necessary, may explicitly stipulate that it is analytic.
An intelligent reader could, if the need arose, easily weaken many of our ‚Äòanalytic‚Äô
statements so that they are applicable also to suÔ¨Éciently-diÔ¨Äerentiable functions.
1.2
Euler‚Äôs method
Let us ponder brieÔ¨Çy the meaning of the ODE (1.1). We possess two items of in-
formation: we know the value of y at a single point t = t0 and, given any function
value y ‚ààRd and time t ‚â•t0, we can tell the slope from the diÔ¨Äerential equation.
The purpose of the exercise being to guess the value of y at a new point, the most
elementary approach is to use linear interpolation. In other words, we estimate y(t)
by making the approximation f(t, y(t)) ‚âàf(t0, y(t0)) for t ‚àà[t0, t0 + h], where h > 0
is suÔ¨Éciently small. Integrating (1.1),
y(t) = y(t0) +
 t
t0
f(œÑ, y(œÑ)) dœÑ ‚âày0 + (t ‚àít0)f(t0, y0).
(1.3)
Given a sequence t0, t1 = t0 +h, t2 = t0 +2h, . . ., where h > 0 is the time step, we
denote by yn a numerical estimate of the exact solution y(tn), n = 0, 1, . . . Motivated
by (1.3), we choose
y1 = y0 + hf(t0, y0).
This procedure can be continued to produce approximants at t2, t3 and so on. In
general, we obtain the recursive scheme
yn+1 = yn + hf(tn, yn),
n = 0, 1, . . . ,
(1.4)
the celebrated Euler method.
Euler‚Äôs method is not only the most elementary computational scheme for ODEs
and, simplicity notwithstanding, of enduring practical importance. It is also the cor-
nerstone of the numerical analysis of diÔ¨Äerential equations of evolution. In a deep
and profound sense, all the fancy multistep and Runge‚ÄìKutta schemes that we shall
discuss are nothing but a generalization of the basic paradigm (1.4).
3 Graphic interpretation
Euler‚Äôs method can be illustrated pictorially.
Consider, for example, the scalar logistic equation y‚Ä≤ = y(1 ‚àíy), y(0) =
1
10.
Fig. 1.1 displays the Ô¨Årst few steps of Euler‚Äôs method, with a grotesquely large
step h = 1. For each step we show the exact solution with initial condition
y(tn) = yn in the vicinity of tn = nh (dotted line) and the linear interpolation
via Euler‚Äôs method (1.4) (solid line).
The initial condition being, by deÔ¨Ånition, exact, so is the slope at t0. However,
instead of following a curved trajectory the numerical solution is piecewise-
linear. Having reached t1, say, we have moved to a wrong trajectory (i.e.,
corresponding to a diÔ¨Äerent initial condition). The slope at t1 is wrong ‚Äì or,

1.2
Euler‚Äôs method
5
rather, it is the correct slope of the wrong solution! Advancing further, we
might well stray even more from the original trajectory.
A realistic goal of numerical solution is not, however, to avoid errors alto-
gether; after all, we approximate since we do not know the exact solution in
the Ô¨Årst place! An error-generating mechanism exists in every algorithm for
numerical ODEs and our purpose is to understand it and to ensure that, in a
given implementation, errors do not accumulate beyond a speciÔ¨Åed tolerance.
Remarkably, even the excessive step h = 1 leads in Fig. 1.1 to a relatively
modest local error.
3
0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
t
y
Figure 1.1
Euler‚Äôs method, as applied to the equation y‚Ä≤ = y(1 ‚àíy) with initial
value y(0) =
1
10.
Euler‚Äôs method can be easily extended to cater for variable steps. Thus, for a general
monotone sequence t0 < t1 < t2 < ¬∑ ¬∑ ¬∑ we approximate as follows:
y(tn+1) ‚âàyn+1 = yn + hnf(tn, yn),
where hn = tn+1 ‚àítn, n = 0, 1, . . . However, for the time being we restrict ourselves
to constant steps.
How good is Euler‚Äôs method in approximating (1.1)? Before we even attempt to
answer this question, we need to formulate it with considerably more rigour. Thus,
suppose that we wish to compute a numerical solution of (1.1) in the compact interval

6
Euler‚Äôs method and beyond
[t0, t0+t‚àó] with some time-stepping numerical method, not necessarily Euler‚Äôs scheme.
In other words, we cover the interval by an equidistant grid and employ the time-
stepping procedure to produce a numerical solution. Each grid is associated with a
diÔ¨Äerent numerical sequence and the critical question is whether, as h ‚Üí0 and the
grid is being reÔ¨Åned, the numerical solution tends to the exact solution of (1.1). More
formally, we express the dependence of the numerical solution upon the step size by
the notation yn = yn,h, n = 0, 1, . . . , ‚åät‚àó/h‚åã. A method is said to be convergent if, for
every ODE (1.1) with a Lipschitz function f and every t‚àó> 0 it is true that
lim
h‚Üí0+
max
n=0,1,...,‚åät‚àó/h‚åã‚à•yn,h ‚àíy(tn)‚à•= 0,
where ‚åäŒ±‚åã‚ààZ is the integer part of Œ± ‚ààR. Hence, convergence means that, for
every Lipschitz function, the numerical solution tends to the true solution as the grid
becomes increasingly Ô¨Åne.2
In the next few chapters we will mention several desirable attributes of numerical
methods for ODEs. It is crucial to understand that convergence is not just another
‚Äòdesirable‚Äô property but, rather, a sine qua non of any numerical scheme. Unless it
converges, a numerical method is useless!
Theorem 1.1
Euler‚Äôs method (1.4) is convergent.
Proof
We prove this theorem subject to the extra assumption that the function
f (and therefore also y) is analytic (it is enough, in fact, to stipulate the weaker
condition of continuous diÔ¨Äerentiability).
Given h > 0 and yn = yn,h, n = 0, 1, . . . , ‚åät‚àó/h‚åã, we let en,h = yn,h ‚àíy(tn) denote
the numerical error. Thus, we wish to prove that limh‚Üí0+ maxn ‚à•en,h‚à•= 0.
By Taylor‚Äôs theorem and the diÔ¨Äerential equation (1.1),
y(tn+1) = y(tn) + hy‚Ä≤(tn) + O

h2
= y(tn) + hf(tn, y(tn)) + O

h2
,
(1.5)
and, y being continuously diÔ¨Äerentiable, the O

h2
term can be bounded (in a given
norm) uniformly for all h > 0 and n ‚â§‚åät‚àó/h‚åãby a term of the form ch2, where c > 0
is a constant. We subtract (1.5) from (1.4), giving
en+1,h = en,h + h[f(tn, y(tn) + en,h) ‚àíf(tn, y(tn))] + O

h2
.
Thus, it follows by the triangle inequality from the Lipschitz condition and the afore-
mentioned bound on the O

h2
reminder term that
‚à•en+1,h‚à•‚â§‚à•en,h‚à•+ h‚à•f(tn, y(tn) + en,h) ‚àíf(tn, y(tn))‚à•+ ch2
‚â§(1 + hŒª)‚à•en,h‚à•+ ch2,
n = 0, 1, . . . , ‚åät‚àó/h‚åã‚àí1.
(1.6)
We now claim that
‚à•en,h‚à•‚â§c
Œªh [(1 + hŒª)n ‚àí1] ,
n = 0, 1, . . .
(1.7)
2We have just introduced a norm through the back door: cf. appendix subsection A.1.3.3 for
an exact deÔ¨Ånition. This, however, should cause no worry, since all norms are equivalent in Ô¨Ånite-
dimensional spaces. In other words, if a method is convergent in one norm, it converges in all . . .

1.2
Euler‚Äôs method
7
The proof is by induction on n. When n = 0 we need to prove that ‚à•e0,h‚à•‚â§0 and
hence that e0,h = 0. This is certainly true, since at t0 the numerical solution matches
the initial condition and the error is zero.
For general n ‚â•0 we assume that (1.7) is true up to n and use (1.6) to argue that
‚à•en+1,h‚à•‚â§(1 + hŒª) c
Œªh [(1 + hŒª)n ‚àí1] + ch2 = c
Œªh

(1 + hŒª)n+1 ‚àí1

.
This advances the inductive argument from n to n+1 and proves that (1.7) is true. The
constant hŒª is positive, therefore 1 + hŒª < ehŒª and we deduce that (1 + hŒª)n < enhŒª.
The index n is allowed to range in {0, 1, . . . , ‚åät‚àó/h‚åã}, hence (1 + hŒª)n < e‚åät‚àó/h‚åãhŒª ‚â§
et‚àóŒª. Substituting into (1.7), we obtain the inequality
‚à•en,h‚à•‚â§c
Œª(et‚àóŒª ‚àí1)h,
n = 0, 1, . . . , ‚åät‚àó/h‚åã.
Since c(et‚àóŒª ‚àí1)/Œª is independent of h, it follows that
lim
h‚Üí0
0‚â§nh‚â§t‚àó
‚à•en,h‚à•= 0.
In other words, Euler‚Äôs method is convergent.
3 Health warning
At Ô¨Årst sight, it might appear that there is more to the
last theorem than meets the eye ‚Äì not just a proof of convergence but also
an upper bound on the error. In principle this is perfectly true: the error
of Euler‚Äôs method is indeed always bounded by hcet‚àóŒª/Œª.
Moreover, with
very little eÔ¨Äort it is possible to demonstrate, e.g. by using the Peano kernel
theorem (A.2.2.6), that a reasonable choice is c = maxt‚àà[t0,t0+t‚àó] ‚à•y‚Ä≤‚Ä≤(t)‚à•. The
problem with this bound is that, unfortunately, in an overwhelming majority
of practical cases it is too large by many orders of magnitude. It falls into the
broad category of statements like ‚Äòthe distance between London and New York
is less than 47 light years‚Äô which, although manifestly true, fail to contribute
signiÔ¨Åcantly to the sum total of human knowledge.
The problem is not with the proof per se but with the insensitivity of a
Lipschitz constant. A trivial example is the scalar linear equation y‚Ä≤ = ‚àí100y,
y(0) = 1. Therefore Œª = 100 and, since y(t) = e‚àí100t, c = Œª2. We thus derive
the upper bound of 100h(e100t‚àó‚àí1). Letting t‚àó= 1, say, we have
|yn ‚àíy(nh)| ‚â§2.69 √ó 1045h.
(1.8)
It is easy, however, to show that yn = (1 ‚àí100h)n, hence to derive the exact
expression
|yn ‚àíy(nh)| =
(1 ‚àí100h)n ‚àíe‚àí100nh
which is smaller by many orders of magnitude than (1.8) (note that, unless
nh is very small, to all intents and purposes e‚àí100nh ‚âà0).
The moral of our discussion is simple. The bound from the proof of Theorem
1.1 must not be used in practical estimations of numerical error!
3

8
Euler‚Äôs method and beyond
Euler‚Äôs method can be rewritten in the form yn+1 ‚àí[yn + hf(tn, yn)] = 0. Replacing
yk by the exact solution y(tk), k = n, n + 1, and expanding the Ô¨Årst few terms of the
Taylor series about t = t0 + nh, we obtain
y(tn+1) ‚àí[y(tn) + hf(tn, y(tn))]
=

y(tn) + hy‚Ä≤(tn) + O

h2
‚àí[y(tn) + hy‚Ä≤(tn)] = O

h2
.
We say that the Euler‚Äôs method (1.4) is of order 1. In general, given an arbitrary
time-stepping method
yn+1 = Yn(f, h, y0, y1, . . . , yn),
n = 0, 1, . . . ,
for the ODE (1.1), we say that it is of order p if
y(tn+1) ‚àíYn(f, h, y(t0), y(t1), . . . , y(tn)) = O

hp+1
for every analytic f and n = 0, 1, . . . Alternatively, a method is of order p if it recovers
exactly every polynomial solution of degree p or less.
The order of a numerical method provides us with information about its local
behaviour ‚Äì advancing from tn to tn+1, where h > 0 is suÔ¨Éciently small, we are
incurring an error of O

hp+1
. Our main interest, however, is in not the local but
the global behaviour of the method: how well is it doing in a Ô¨Åxed bounded interval
of integration as h ‚Üí0? Does it converge to the true solution? How fast? Since
the local error decays as O

hp+1
, the number of steps increases as O

h‚àí1
. The
naive expectation is that the global error decreases as O(hp), but ‚Äì as we will see
in Chapter 2 ‚Äì it cannot be taken for granted for each and every numerical method
without an additional condition. As far as Euler‚Äôs method is concerned, Theorem 1.1
demonstrates that all is well and that the error indeed decays as O(h).
1.3
The trapezoidal rule
Euler‚Äôs method approximates the derivative by a constant in [tn, tn+1], namely by its
value at tn (again, we denote tk = t0 + kh, k = 0, 1, . . .). Clearly, the ‚Äòcantilever-
ing‚Äô approximation is not very good and it makes more sense to make the constant
approximation of the derivative equal to the average of its values at the endpoints.
Bearing in mind that derivatives are given by the diÔ¨Äerential equation, we thus obtain
an expression similar to (1.3):
y(t) = y(tn) +
 t
tn
f(œÑ, y(œÑ)) dœÑ
‚âày(tn) + 1
2(t ‚àítn)[f(tn, y(tn)) + f(t, y(t))].
This is the motivation behind the trapezoidal rule
yn+1 = yn + 1
2h[f(tn, yn) + f(tn+1, yn+1)].
(1.9)

1.3
The trapezoidal rule
9
To obtain the order of (1.9), we substitute the exact solution,
y(tn+1) ‚àí

y(tn) + 1
2h[f(tn, y(tn)) + f(tn+1, y(tn+1))]

=

y(tn) + hy‚Ä≤(tn) + 1
2h2y‚Ä≤‚Ä≤(tn) + O

h3
‚àí

y(tn) + 1
2h

y‚Ä≤(tn) +

y‚Ä≤(tn) + hy‚Ä≤‚Ä≤(tn) + O

h2
= O

h3
.
Therefore the trapezoidal rule is of order 2.
Being forewarned of the shortcomings of local analysis, we should not jump to
conclusions. Before we infer that the error decays globally as O

h2
, we must Ô¨Årst
prove that the method is convergent.
Fortunately, this can be accomplished by a
straightforward generalization of the method of proof of Theorem 1.1.
Theorem 1.2
The trapezoidal rule (1.9) is convergent.
Proof
Subtracting
y(tn+1) = y(tn) + 1
2h [f(tn, y(tn)) + f(tn+1, y(tn+1))] + O

h3
from (1.9), we obtain
en+1,h = en,h + 1
2h {[f(tn, yn) ‚àíf(tn, y(tn))]
+

f(tn+1, yn+1) ‚àíf(tn+1, y(tn+1))

+ O

h3
.
For analytic f we may bound the O

h3
term by ch3 for some c > 0, and this
upper bound is valid uniformly throughout [t0, t0 + t‚àó]. Therefore, it follows from the
Lipschitz condition (1.2) and the triangle inequality that
‚à•en+1,h‚à•‚â§‚à•en,h‚à•+ 1
2hŒª {‚à•en,h‚à•+ ‚à•en+1,h‚à•} + ch3.
Since we are ultimately interested in letting h ‚Üí0 there is no harm in assuming that
hŒª < 2, and we can thus deduce that
‚à•en+1,h‚à•‚â§
	1 + 1
2hŒª
1 ‚àí1
2hŒª

‚à•en,h‚à•+
	
c
1 ‚àí1
2hŒª

h3.
(1.10)
Our next step closely parallels the derivation of inequality (1.7). We thus argue that
‚à•en,h‚à•‚â§c
Œª
	1 + 1
2hŒª
1 ‚àí1
2hŒª

n
‚àí1

h2.
(1.11)
This follows by induction on n from (1.10) and is left as an exercise to the reader.
Since 0 < hŒª < 2, it is true that
1 + 1
2hŒª
1 ‚àí1
2hŒª = 1 +
hŒª
1 ‚àí1
2hŒª ‚â§
‚àû

‚Ñì=0
1
‚Ñì!
	
hŒª
1 ‚àí1
2hŒª

‚Ñì
= exp
	
hŒª
1 ‚àí1
2hŒª

.
Consequently, (1.11) yields
‚à•en,h‚à•‚â§ch2
Œª
	1 + 1
2hŒª
1 ‚àí1
2hŒª

n
‚â§ch2
Œª exp
	
nhŒª
1 ‚àí1
2hŒª

.

10
Euler‚Äôs method and beyond
This bound is true for every nonnegative integer n such that nh ‚â§t‚àó. Therefore
‚à•en,h‚à•‚â§ch2
Œª exp
	
t‚àóŒª
1 ‚àí1
2hŒª

and we deduce that
lim
h‚Üí0
0‚â§nh‚â§t‚àó
‚à•en,h‚à•= 0.
In other words, the trapezoidal rule converges.
The number ch2 exp[t‚àóŒª/(1 ‚àí1
2hŒª)]/Œª is, again, of absolutely no use in practical
error bounds.
However, a signiÔ¨Åcant diÔ¨Äerence from Theorem 1.1 is that for the
trapezoidal rule the error decays globally as O

h2
. This is to be expected from a
second-order method if its convergence has been established.
Another diÔ¨Äerence between the trapezoidal rule and Euler‚Äôs method is of an entirely
diÔ¨Äerent character. Whereas Euler‚Äôs method (1.4) can be executed explicitly ‚Äì knowing
yn we can produce yn+1 by computing a value of f and making a few arithmetic
operations ‚Äì this is not the case with (1.9). The vector v = yn + 1
2hf(tn, yn) can be
evaluated from known data, but that leaves us in each step with the task of Ô¨Ånding
yn+1 as the solution of the system of algebraic equations
yn+1 ‚àí1
2hf(tn+1, yn+1) = v.
The trapezoidal rule is thus said to be implicit, to distinguish it from the explicit
Euler‚Äôs method and its ilk.
Solving nonlinear equations is hardly a mission impossible, but we cannot take it
for granted either. Only in texts on pure mathematics are we allowed to wave a magic
wand, exclaim ‚Äòlet yn+1 be a solution of . . . ‚Äô and assume that all our problems are
over. As soon as we come to deal with actual computation, we had better specify how
we plan (or our computer plans) to undertake the task of evaluating yn+1. This will
be a theme of Chapter 7, which deals with the implementation of ODE methods. It
suÔ¨Éces to state now that the cost of numerically solving nonlinear equations does not
rule out the trapezoidal rule (and other implicit methods) as viable computational
instruments. Implicitness is just one attribute of a numerical method and we must
weigh it alongside other features.
3 A ‚Äògood‚Äô example
Figure 1.2 displays the (natural) logarithm of the error
in the numerical solution of the scalar linear equation y‚Ä≤ = ‚àíy + 2e‚àít cos 2t,
y(0) = 0 for (in descending order) h = 1
2, h =
1
10 and h =
1
50.
How well does the plot illustrate our main distinction between Euler‚Äôs method
and the trapezoidal rule, namely faster decay of the error for the latter? As
often in life, information is somewhat obscured by extraneous ‚Äònoise‚Äô; in the
present case the error oscillates. This can be easily explained by the periodic
component of the exact solution y(t) = e‚àít sin 2t. Another observation is that,
for both Euler‚Äôs method and the trapezoidal rule, the error, twists and turns
notwithstanding, does decay. This, on the face of it, can be explained by the
decay of the exact solution but is an important piece of news nonetheless.

1.3
The trapezoidal rule
11
0
1
2
3
4
5
6
7
8
9
10
‚àí25
‚àí20
‚àí15
‚àí10
‚àí5
0
The Euler method
0
1
2
3
4
5
6
7
8
9
10
‚àí25
‚àí20
‚àí15
‚àí10
‚àí5
0
The trapezoidal rule
Figure
1.2
Euler‚Äôs method and the trapezoidal rule, as applied to y‚Ä≤
=
‚àíy+2e‚àít cos 2t, y(0) = 0. The logarithm of the error, ln |yn ‚àíy(tn)|, is displayed for
h = 1
2 (solid line), h =
1
10 (broken line) and h =
1
50 (broken-and-dotted line).
Our most pessimistic assumption is that errors might accumulate from step
to step but, as can be seen from this example, this prophecy of doom is often
misplaced. This is a highly nontrivial point, which will be debated at greater
length throughout Chapter 4.
Factoring out oscillations and decay, we observe that errors indeed decrease
with h. More careful examination veriÔ¨Åes that they increase at roughly the
rate predicted by order considerations. SpeciÔ¨Åcally, for a convergent method
of order p we have ‚à•e‚à•‚âàchp, hence ln ‚à•e‚à•‚âàln c+p ln h. Denoting by e(1) and
e(2) the errors corresponding to step sizes h(1) and h(2) respectively, it follows
that ln ‚à•e(2)‚à•‚âàln ‚à•e(1)‚à•‚àíp ln(h(2)/h(1)). The ratio of consecutive step sizes
in Fig. 1.2 being Ô¨Åve, we expect the error to decay by (at least) a constant
multiple of ln 5 ‚âà1.6094 and 2 ln 5 ‚âà3.2189 for Euler and the trapezoidal
rule respectively. The actual error decays if anything slightly faster than this. 3
3 A ‚Äòbad‚Äô example
Theorems 1.1 and 1.2 and, indeed, the whole numerical
ODE theory, rest upon the assumption that (1.1) satisÔ¨Åes the Lipschitz con-

12
Euler‚Äôs method and beyond
dition. We can expect numerical methods to underperform in the absence of
(1.2), and this is vindicated by experiment. In Figs. 1.3 and 1.4 we display
the numerical solution of the equation y‚Ä≤ = ln 3

y ‚àí‚åäy‚åã‚àí3
2

, y(0) = 0. It is
easy to verify that the exact solution is
y(t) = ‚àí‚åät‚åã+ 1
2

1 ‚àí3t‚àí‚åät‚åã
,
t ‚â•0,
where ‚åäx‚åãis the integer part of x ‚ààR.
However, the equation fails the Lipschitz condition. In order to demonstrate
this, we let m ‚â•1 be an integer and set x = m+Œµ, z = m‚àíŒµ, where Œµ ‚àà

0, 1
4

.
Then


x ‚àí‚åäx‚åã‚àí3
2

‚àí

z ‚àí‚åäz‚åã‚àí3
2
  = 1 ‚àí2Œµ
2Œµ
|x ‚àíz|
and, since Œµ can be arbitrarily small, we see that inequality (1.2) cannot be
satisÔ¨Åed for a Ô¨Ånite Œª.
Figures 1.3 and 1.4 display the error for h =
1
100 and h =
1
1000. We observe
that, although the error decreases with h, the rate of decay for both methods
is just O(h): for the trapezoidal rule this falls short of what can be expected
in a Lipschitz case. The source of the errors is clear: integer points, where
locally the function fails the Lipschitz condition. Note that both methods
perform equally badly ‚Äì but when the ODE is not Lipschitz, all bets are oÔ¨Ä! 3
0
1
2
3
4
5
6
7
8
‚àí0.06
‚àí0.04
‚àí0.02
0
1
2
3
4
5
6
7
8
‚àí0.010
‚àí0.008
‚àí0.006
‚àí0.004
‚àí0.002
0
Figure 1.3
The error using Euler‚Äôs method for y‚Ä≤ = ln 3 
y ‚àí‚åäy‚åã‚àí3
2

, y(0) = 0.
The upper Ô¨Ågure corresponds to h =
1
100 and the lower to h =
1
1000.
Two assumptions have led us to the trapezoidal rule. Firstly, for suÔ¨Éciently small h, it
is a good idea to approximate the derivative by a constant and, secondly, in choosing

1.4
The theta method
13
0
1
2
3
4
5
6
7
8
‚àí0.06
‚àí0.04
‚àí0.02
0
1
2
3
4
5
6
7
8
‚àí0.010
‚àí0.008
‚àí0.006
‚àí0.004
‚àí0.002
0
Figure 1.4
The error using the trapezoidal rule for the same equation as in
Fig. 1.3. The upper Ô¨Ågure corresponds to h =
1
100 and the lower to h =
1
1000.
the constant we should not ‚Äòdiscriminate‚Äô between the endpoints ‚Äì hence the average
y‚Ä≤(t) ‚âà1
2[f(tn, yn) + f(tn+1, yn+1)]
is a sensible choice. Similar reasoning leads, however, to an alternative approximation,
y‚Ä≤(t) ‚âàf

tn + 1
2h, 1
2(yn + yn+1)

,
t ‚àà[tn, tn+1],
and to the implicit midpoint rule
yn+1 = yn + hf

tn + 1
2h, 1
2(yn + yn+1)

.
(1.12)
It is easy to prove that (1.12) is second order and that it converges. This is left to the
reader in Exercise 1.1.
The implicit midpoint rule is a special case of the Runge‚ÄìKutta method. We defer
the discussion of such methods to Chapter 3.
1.4
The theta method
Both Euler‚Äôs method and the trapezoidal rule Ô¨Åt the general pattern
yn+1 = yn + h[Œ∏f(tn, yn) + (1 ‚àíŒ∏)f(tn+1, yn+1)],
n = 0, 1, . . . ,
(1.13)
with Œ∏ = 1 and Œ∏ = 1
2 respectively. We may contemplate using (1.13) for any Ô¨Åxed
value of Œ∏ ‚àà[0, 1] and this, appropriately enough, is called a theta method. It is explicit
for Œ∏ = 1, otherwise implicit.

14
Euler‚Äôs method and beyond
Although we can interpret (1.13) geometrically ‚Äì the slope of the solution is as-
sumed to be piecewise constant and provided by a linear combination of derivatives
at the endpoints of each interval ‚Äì we prefer the formal route of a Taylor expansion.
Thus, substituting the exact solution y(t),
y(tn+1) ‚àíy(tn) ‚àíh[Œ∏f(tn, y(tn)) + (1 ‚àíŒ∏)f(tn+1, y(tn+1))]
= y(tn+1) ‚àíy(tn) ‚àíh[Œ∏y‚Ä≤(tn) + (1 ‚àíŒ∏)y‚Ä≤(tn+1)]
=

y(tn) + hy‚Ä≤(tn) + 1
2h2y‚Ä≤‚Ä≤(tn) + 1
6h3y‚Ä≤‚Ä≤‚Ä≤(tn)

‚àíy(tn)
‚àíh

Œ∏y‚Ä≤(tn) + (1 ‚àíŒ∏)

y‚Ä≤(tn) + hy‚Ä≤‚Ä≤(tn) + 1
2h2y‚Ä≤‚Ä≤‚Ä≤(tn)
 
+ O

h4
=

Œ∏ ‚àí1
2

h2y‚Ä≤‚Ä≤(tn) +
 1
2Œ∏ ‚àí1
3

h3y‚Ä≤‚Ä≤‚Ä≤(tn) + O

h4
.
(1.14)
Therefore the method is of order 2 for Œ∏ = 1
2 (the trapezoidal rule) and otherwise of
order one. Moreover, by expanding further than is strictly required by order consider-
ations, we can extract from (1.14) an extra morsel of information. Thus, subtracting
the last expression from
yn+1 ‚àíyn ‚àíh

Œ∏f(tn, yn) + (1 ‚àíŒ∏)f(tn+1, yn+1)

= 0,
we obtain for suÔ¨Éciently small h > 0
en+1 = en + Œ∏h[f(tn, y(tn) + en) ‚àíf(tn, y(tn))]
+ (1 ‚àíŒ∏)h[f(tn+1, y(tn+1) + en+1) ‚àíf(tn+1, y(tn+1))]
‚éß
‚é®
‚é©
‚àí1
12h3y‚Ä≤‚Ä≤‚Ä≤(tn) + O

h4
,
Œ∏ = 1
2,
+

Œ∏ ‚àí1
2

h2y‚Ä≤‚Ä≤(tn) + O

h3
,
Œ∏ Ã∏= 1
2.
Considering en+1 as an unknown, we apply the implicit function theorem ‚Äì this is
allowed since f is analytic and, for suÔ¨Éciently small h > 0, the matrix
I ‚àí(1 ‚àíŒ∏)h‚àÇf(tn+1, y(tn+1))
‚àÇy
is nonsingular. The conclusion is that
en+1 = en
‚éß
‚é®
‚é©
‚àí1
12h3y‚Ä≤‚Ä≤‚Ä≤(tn) + O

h4
,
Œ∏ = 1
2,
+

Œ∏ ‚àí1
2

h2y‚Ä≤‚Ä≤(tn) + O

h3
,
Œ∏ Ã∏= 1
2.
The theta method is convergent for every Œ∏ ‚àà[0, 1], as can be veriÔ¨Åed with ease by
generalizing the proofs of Theorems 1.1 and 1.2. This is is the subject of Exercise 1.1.
Why, a vigilant reader might ask, bother with the theta method except for the
special values Œ∏ = 1 and Œ∏ = 1
2? After all, the Ô¨Årst is unique in conferring explicitness
and the second is the only second-order theta method. The reasons are threefold.
Firstly, the whole concept of order is based on the assumption that the numerical
error is concentrated mainly in the leading term of its Taylor expansion.
This is
true as h ‚Üí0, except that the step length, when implemented on a real computer,

Comments and bibliography
15
never actually tends to zero . . . Thus, in very special circumstances we might wish
to annihilate higher-order terms in the error expansion; for example, letting Œ∏ = 2
3
gets rid of the O

h3
term while retaining the O

h2
component. Secondly, the theta
method is our Ô¨Årst example of a more general approach to the design of numerical
algorithms, whereby simple geometric intuition is replaced by a more formal approach
based on a Taylor expansion and the implicit function theorem. Its study is a good
preparation for the material of Chapters 2 and 3. Finally, the choice Œ∏ = 0 is of great
practical relevance. The Ô¨Årst-order implicit method
yn+1 = yn + hf(tn+1, yn+1),
n = 0, 1, . . . ,
(1.15)
is called the backward Euler‚Äôs method and is a favourite algorithm for the solution of
stiÔ¨ÄODEs. We defer the discussion of stiÔ¨Äequations to Chapter 4, where the merits
of the backward Euler‚Äôs method and similar schemes will become clear.
Comments and bibliography
An implicit goal of this book is to demonstrate that the computation of diÔ¨Äerential equations
is not about discretizing everything in sight by the Ô¨Årst available Ô¨Ånite-diÔ¨Äerence approxima-
tion and throwing it on the nearest computer. It is all about designing clever and eÔ¨Écient
algorithms and understanding their mathematical features. The narrative of this chapter in-
troduces us to convergence and order, the essential building blocks in this quest to understand
discretization methods.
We assume very little knowledge of the analytic (as opposed to numerical) theory of ODEs
throughout this volume: just the concepts of existence, uniqueness, the Lipschitz condition
and (mainly in Chapter 4) explicit solution of linear initial value systems.
In Chapter 5
we will be concerned with more specialized geometric features of ODEs but we take care
to explain there all nontrivial issues. A brief r¬¥esum¬¥e of essential knowledge is reviewed in
Appendix section A.2.3, but a diligent reader will do well to refresh his or her memory with
a thorough look at a reputable textbook, for example BirkhoÔ¨Ä& Rota (1978) or Boyce &
DiPrima (1986).
Euler‚Äôs method, the grandaddy of all numerical schemes for diÔ¨Äerential equations, is
introduced in just about every relevant textbook (e.g. Conte & de Boor, 1990; Hairer et al.,
1991; Isaacson & Keller, 1966; Lambert, 1991), as is the trapezoidal rule. More traditional
books have devoted considerable eÔ¨Äort toward proving, with the Euler‚ÄìMaclaurin formula
(Ralston, 1965), that the error of the trapezoidal rule can be expanded in odd powers of h (cf.
Exercise 1.8), but it seems that nowadays hardly anybody cares much about this observation,
except for its applications to Richardson‚Äôs extrapolation (Isaacson & Keller, 1966).
We have mentioned in Section 1.2 the Peano kernel theorem. Its knowledge is marginal
to the subject matter of this book. However, if you want to understand mathematics and
learn a simple, yet beautiful, result in approximation theory, we refer to A.2.2.6 and A.2.2.7
and references therein.
BirkhoÔ¨Ä, G. and Rota, G.-C. (1978), Ordinary DiÔ¨Äerential Equations (3rd edn), Wiley, New
York.
Boyce, W.E. and DiPrima, R.C. (1986), Elementary DiÔ¨Äerential Equations and Boundary
Value Problems (4th edn), Wiley, New York.

16
Euler‚Äôs method and beyond
Conte, S.D. and de Boor, C. (1990), Elementary Numerical Analysis: An Algorithmic Ap-
proach (3rd edn), McGraw-Hill K¬Øogakusha, Tokyo.
Hairer, E, N√∏rsett, S.P. and Wanner, G. (1991), Solving Ordinary DiÔ¨Äerential Equations I:
NonstiÔ¨ÄProblems (2nd edn) Springer-Verlag, Berlin.
Isaacson, E. and Keller, H.B. (1966), Analysis of Numerical Methods, Wiley, New York.
Lambert, J.D. (1991), Numerical Methods for Ordinary DiÔ¨Äerential Systems, Wiley, London.
Ralston, A. (1965), A First Course in Numerical Analysis, McGraw-Hill K¬Øogakusha, New
York.
Exercises
1.1
Apply the method of proof of Theorems 1.1 and 1.2 to prove the convergence
of the implicit midpoint rule (1.12) and of the theta method (1.13).
1.2
The linear system y‚Ä≤ = Ay, y(0) = y0, where A is a symmetric matrix, is
solved by Euler‚Äôs method.
a Letting en = yn ‚àíy(nh), n = 0, 1, . . . , prove that
‚à•en‚à•2 ‚â§‚à•y0‚à•2 max
Œª‚ààœÉ(A)
(1 + hŒª)n ‚àíenhŒª ,
where œÉ(A) is the set of eigenvalues of A and ‚à•¬∑ ‚à•2 is the Euclidean matrix
norm (cf. A.1.3.3).
b Demonstrate that for every ‚àí1 ‚â™x ‚â§0 and n = 0, 1, . . . it is true that
enx ‚àí1
2nx2e(n‚àí1)x ‚â§(1 + x)n ‚â§enx.
(Hint: Prove Ô¨Årst that 1 + x ‚â§ex, 1 + x + 1
2x2 ‚â•ex for all x ‚â§0, and
then argue that, provided |a ‚àí1| and |b| are small, it is true that (a ‚àíb)n ‚â•
an ‚àínan‚àí1b.)
c Suppose that the maximal eigenvalue of A is Œªmax < 0. Prove that, as h ‚Üí0
and nh ‚Üít ‚àà[0, t‚àó],
‚à•en‚à•2 ‚â§1
2tŒª2
maxeŒªmaxt‚à•y0‚à•2h ‚â§1
2t‚àóŒª2
max‚à•y0‚à•2h.
d Compare the order of magnitude of this bound with the upper bound from
Theorem 1.1 in the case
A =

‚àí2
1
1
‚àí2

,
t‚àó= 10.
1.3
We solve the scalar linear system y‚Ä≤ = ay, y(0) = 1.

Exercises
17
a Show that the ‚Äòcontinuous output‚Äô method
u(t) = 1 + 1
2a(t ‚àính)
1 ‚àí1
2a(t ‚àính)yn,
nh ‚â§t ‚â§(n + 1)h,
n = 0, 1, . . . ,
is consistent with the values of yn and yn+1 which are obtained by the
trapezoidal rule.
b Demonstrate that u obeys the perturbed ODE
u‚Ä≤(t) = au(t) +
1
4a3(t ‚àính)2
[1 ‚àí1
2a(t ‚àính)]2 yn,
t ‚àà[nh, (n + 1)h],
with initial condition u(nh) = yn. Thus, prove that
u((n + 1)h) = eha

1 + 1
4a3
 h
0
e‚àíœÑaœÑ 2 dœÑ
(1 ‚àí1
2aœÑ)2

yn.
c Let en = yn ‚àíy(nh), n = 0, 1, . . .. Show that
en+1 = eha

1 + 1
4a3
 h
0
e‚àíœÑaœÑ 2 dœÑ
(1 ‚àí1
2aœÑ)2

en + 1
4a3e(n+1)ha
 h
0
e‚àíœÑaœÑ 2 dœÑ
(1 ‚àí1
2aœÑ)2 .
In particular, deduce that a < 0 implies that the error propagates subject
to the inequality
|en+1| ‚â§eha

1 + 1
4|a|3
 h
0
e‚àíœÑaœÑ 2 dœÑ

|en| + 1
4|a|3e(n+1)ha
 h
0
e‚àíœÑaœÑ 2 dœÑ.
1.4
Given Œ∏ ‚àà[0, 1], Ô¨Ånd the order of the method
yn+1 = yn + hf

tn + (1 ‚àíŒ∏)h, Œ∏yn + (1 ‚àíŒ∏)yn+1

.
1.5
Provided that f is analytic, it is possible to obtain from y‚Ä≤ = f(t, y) an
expression for the second derivative of y, namely y‚Ä≤‚Ä≤ = g(t, y), where
g(t, y) = ‚àÇf(t, y)
‚àÇt
+ ‚àÇf(t, y)
‚àÇy
f(t, y).
Find the orders of the methods
yn+1 = yn + hf(tn, yn) + 1
2h2g(tn, yn)
and
yn+1 = yn+ 1
2h[f(tn, yn)+f(tn+1, yn+1)]+ 1
12h2[g(tn, yn)‚àíg(tn+1, yn+1)].
1.6‚ãÜ
Assuming that g is Lipschitz, prove that both methods from Exercise 1.5
converge.

18
Euler‚Äôs method and beyond
1.7
Repeated diÔ¨Äerentiation of the ODE (1.1), for analytic f, yields explicit
expressions for functions gm such that
dmy(t)
dtm
= gm(t, y(t)),
m = 0, 1, . . .
Hence g0(t, y) = y and g1(t, y) = f(t, y); g2 has been already deÔ¨Åned in
Exercise 1.5 as g.
a Assuming for simplicity that f = f(y) (i.e. that the ODE system (1.1) is
autonomous), derive g3.
b Prove that the mth Taylor method
yn+1 =
m

k=0
1
k!hkgk(tn, yn),
n = 0, 1, . . . ,
is of order m for m = 1, 2, . . .
c Let f(y) = Œõy + b, where the matrix Œõ and the vector b are independent of
t. Find the explicit form of gm for m = 0, 1, . . . and thereby prove that the
mth Taylor method reduces to the recurrence
yn+1 =
 m

k=0
1
k!hkŒõk

yn +
 m

k=1
1
k!hkŒõk‚àí1

b,
n = 0, 1, . . .
1.8
Let f be analytic. Prove that, for suÔ¨Éciently small h > 0 and an analytic
function x, the function
x(t + h) ‚àíx(t ‚àíh) ‚àíhf

1
2(x(t ‚àíh) + x(t + h))

can be expanded into power series in odd powers of h. Deduce that the error
in the implicit midpoint rule (1.13), when applied to autonomous ODEs
y‚Ä≤ = f(y) also admits an expansion in odd powers of h. (Hint: First try
to prove the statement for a scalar function f. Once you have solved this
problem, a generalization should present no diÔ¨Éculties.)

2
Multistep methods
2.1
The Adams method
A typical numerical method for an initial value ODE system computes the solution
on a step-by-step basis. Thus, the Euler method advances the solution from t0 to t1
using y0 as an initial value. Next, to advance from t1 to t2, we discard y0 and employ
y1 as the new initial value.
Numerical analysts, however, are thrifty by nature. Why discard a potentially
valuable vector y0? Or, with greater generality, why not make the solution depend
on several past values, provided that these values are available?
There is one perfectly good reason why not ‚Äì the exact solution of
y‚Ä≤ = f(t, y),
t ‚â•t0,
y(t0) = y0
(2.1)
is uniquely determined (f being Lipschitz) by a single initial condition. Any attempt
to pin the solution down at more than one point is mathematically nonsensical or, at
best, redundant. This, however, is valid only with regard to the true solution of (2.1).
When it comes to computation, this redundancy becomes our friend and past values of
y can be put to a very good use ‚Äì provided, however, that we are very careful indeed.
Thus let us suppose again that yn is the numerical solution at tn = t0 + nh, where
h > 0 is the step size, and let us attempt to derive an algorithm that intelligently
exploits past values. To that end, we assume that
ym = y(tm) + O

hs+1
,
m = 0, 1, . . . , n + s ‚àí1,
(2.2)
where s ‚â•1 is a given integer. Our wish being to advance the solution from tn‚àís+1
to tn+s, we commence from the trivial identity
y(tn+s) = y(tn+s‚àí1) +
 tn+s
tn+s‚àí1
y‚Ä≤(œÑ) dœÑ = y(tn+s‚àí1) +
 tn+s
tn+s‚àí1
f(œÑ, y(œÑ)) dœÑ.
(2.3)
Wishing to exploit (2.3) for computational ends, we note that the integral on the right
incorporates y not just at the grid points ‚Äì where approximations are available ‚Äì but
throughout the interval [tn+s‚àí1, tn+s]. The main idea of an Adams method is to use
past values of the solution to approximate y‚Ä≤ in the interval of integration. Thus,
let p be an interpolation polynomial (cf. A.2.2.1‚ÄìA.2.2.5) that matches f(tm, ym) for
m = n, n + 1, , . . . , n + s ‚àí1. Explicitly,
p(t) =
s‚àí1

m=0
pm(t)f(tn+m, yn+m),
19

20
Multistep methods
where the functions
pm(t) =
s‚àí1

‚Ñì=0
‚ÑìÃ∏=m
t ‚àítn+‚Ñì
tn+m ‚àítn+‚Ñì
=
(‚àí1)s‚àí1‚àím
m!(s ‚àí1 ‚àím)!
s‚àí1

‚Ñì=0
‚ÑìÃ∏=m
	t ‚àítn
h
‚àí‚Ñì

,
(2.4)
for every m = 0, 1, . . . , s ‚àí1, are Lagrange interpolation polynomials. It is an easy
exercise to verify that indeed p(tm) = f(tm, ym) for all m = n, n + 1, . . . , n + s ‚àí1.
Hence, (2.2) implies that p(tm) = y‚Ä≤(tm) + O(hs) for this range of m. We now use
interpolation theory from A.2.2.2 to argue that, y being suÔ¨Éciently smooth,
p(t) = y‚Ä≤(t) + O(hs) ,
t ‚àà[tn+s‚àí1, tn+s].
We next substitute p in the integrand of (2.3), replace y(tn+s‚àí1) by yn+s‚àí1 there
and, having integrated along an interval of length h, incur an error of O

hs+1
. In
other words, the method
yn+s = yn+s‚àí1 + h
s‚àí1

m=0
bmf(tn+m, yn+m),
(2.5)
where
bm = h‚àí1
 tn+s
tn+s‚àí1
pm(œÑ) dœÑ = h‚àí1
 h
0
pm(tn+s‚àí1 + œÑ) dœÑ,
m = 0, 1, . . . , s ‚àí1,
is of order p = s. Note from (2.4) that the coeÔ¨Écients b0, b1, . . . , bs‚àí1 are independent
of n and of h; thus we can subsequently use them to advance the iteration from tn+s
to tn+s+1 and so on.
The scheme (2.5) is called the s-step Adams‚ÄìBashforth method.
Having derived explicit expressions, it is easy to state Adams‚ÄìBashforth methods
for moderate values of s. Thus, for s = 1 we encounter our old friend, the Euler
method, whereas s = 2 gives
yn+2 = yn+1 + h
 3
2f(tn+1, yn+1) ‚àí1
2f(tn, yn)

(2.6)
and s = 3 gives
yn+3 = yn+2 + h
 23
12f(tn+2, yn+2) ‚àí4
3f(tn+1, yn+1) + 5
12f(tn, yn)

.
(2.7)
Figure 2.1 displays the logarithm of the error in the solution of y‚Ä≤ = ‚àíy2, y(0) = 1, by
Euler‚Äôs method and the schemes (2.6) and (2.7). The important information can be
read oÔ¨Äthe y-scale: when h is halved, say, Euler‚Äôs error decreases linearly, the error
of (2.6) decays quadratically and (2.7) displays cubic decay. This is hardly surprising,
since the order of the s-step Adams‚ÄìBashforth method is, after all, s and the global
error decays as O(hs).
Adams‚ÄìBashforth methods are just one instance of multistep methods.
In the
remainder of this chapter we will encounter several other families of such schemes.
Later in this book we will learn that diÔ¨Äerent multistep methods are suitable in dif-
ferent situations. First, however, we need to study the general theory of order and
convergence.

2.2
Order and convergence of multistep methods
21
0
2
4
6
8
10
‚àí8
‚àí7
‚àí6
‚àí5
‚àí4
‚àí3
h  = 1/5
0
2
4
6
8
10
‚àí10
‚àí8
‚àí6
‚àí4
h  = 1/10
0
2
4
6
8
10
‚àí12
‚àí10
‚àí8
‚àí6
‚àí4
h  = 1/20
0
2
4
6
8
10
‚àí15
‚àí10
‚àí5
h  = 1/40
Figure 2.1
Plots of ln |yn ‚àíy(tn)| for the Ô¨Årst three Adams‚ÄìBashforth methods,
as applied to the equation y‚Ä≤ = ‚àíy2, y(0) = 1. Euler‚Äôs method, (2.6) and (2.7)
correspond to the solid, broken and broken-and-dotted lines respectively.
2.2
Order and convergence of multistep methods
We write a general s-step method in the form
s

m=0
amyn+m = h
s

m=0
bmf(tn+m, yn+m),
n = 0, 1, . . . ,
(2.8)
where am, bm, m = 0, 1, . . . , s, are given constants, independent of h, n and the un-
derlying diÔ¨Äerential equation. It is conventional to normalize (2.8) by letting as = 1.
When bs = 0 (as is the case with the Adams‚ÄìBashforth method) the method is said
to be explicit; otherwise it is implicit.
Since we are about to encounter several criteria that play an important role in
choosing the coeÔ¨Écients am and bm, a central consideration is to obtain a reasonable
value of the order. Recasting the deÔ¨Ånition from Chapter 1, we note that the method

22
Multistep methods
(2.8) is of order p ‚â•1 if and only if
œà(t, y) :=
s

m=0
amy(t + mh) ‚àíh
s

m=0
bmy‚Ä≤(t + mh) = O

hp+1
,
h ‚Üí0,
(2.9)
for all suÔ¨Éciently smooth functions y and there exists at least one such function for
which we cannot improve upon the decay rate O

hp+1
.
The method (2.8) can be characterized in terms of the polynomials
œÅ(w) :=
s

m=0
amwm
and
œÉ(w) :=
s

m=0
bmwm.
Theorem 2.1
The multistep method (2.8) is of order p ‚â•1 if and only if there
exists c Ã∏= 0 such that
œÅ(w) ‚àíœÉ(w) ln w = c(w ‚àí1)p+1 + O

|w ‚àí1|p+2
,
w ‚Üí1.
(2.10)
Proof
We assume that y is analytic and that its radius of convergence exceeds
sh. Expanding in a Taylor series and changing the order of summation,
œà(t, y) =
s

m=0
am
‚àû

k=0
1
k!y(k)(t)mkhk ‚àíh
s

m=0
bm
‚àû

k=0
1
k!y(k+1)(t)mkhk
=

s

m=0
am

y(t) +
‚àû

k=1
1
k!

s

m=0
mkam ‚àík
s

m=0
mk‚àí1bm

hky(k)(t).
Thus, to obtain order p it is neccesary and suÔ¨Écient that
s

m=0
am = 0,
s

m=0
mkam = k
s

m=0
mk‚àí1bm,
k = 1, 2, . . . , p.
s

m=0
mp+1am Ã∏= (p + 1)
s

m=0
mpbm.
(2.11)
Let w = ez; then w ‚Üí1 corresponds to z ‚Üí0. Expanding again in a Taylor series,
œÅ(ez) ‚àízœÉ(ez) =
s

m=0
amemz ‚àíz
s

m=0
bmemz
=
s

m=0
am
 ‚àû

k=0
1
k!mkzk

‚àíz
s

m=0
bm
 ‚àû

k=0
1
k!mkzk

=
‚àû

k=0
1
k!

s

m=0
mkam

zk ‚àí
‚àû

k=1
1
(k ‚àí1)!

s

m=0
mk‚àí1bm

zk.
Therefore
œÅ(ez) ‚àízœÉ(ez) = czp+1 + O

zp+2

2.2
Order and convergence of multistep methods
23
for some c Ã∏= 0 if and only if (2.11) is true. The theorem follows by restoring w = ez.
An alternative derivation of the order conditions (2.11) assists in our understanding
of them. The map y ‚Üíœà(t, y) is linear, consequently œà(t, y) = O

hp+1
, if and only
if œà(t, q) = 0 for every polynomial q of degree p. Because of linearity, this is equivalent
to
œà(t, qk) = 0,
k = 0, 1, . . . , p,
where {q0, q1, . . . , qp} is a basis of the (p+1)-dimensional space of p-degree polynomials
(see A.2.1.2, A.2.1.3). Setting qk(t) = tk for k = 0, 1, . . . , p, we immediately obtain
(2.11).
3 Adams‚ÄìBashforth revisited . . .
Theorem 2.1 obviates the need for
‚Äòspecial tricks‚Äô such as were used in our derivation of the Adams‚ÄìBashforth
methods in Section 2.1. Given any multistep scheme (2.8), we can verify its
order by a fairly painless expansion into series. It is convenient to express
everything in the currency Œæ := w ‚àí1. For example, (2.6) results in
œÅ(w)‚àíœÉ(w) ln w = (Œæ +Œæ2)‚àí

1 + 3
2Œæ
 
Œæ ‚àí1
2Œæ2 + 1
3Œæ3 + ¬∑ ¬∑ ¬∑

=
5
12Œæ3 +O

Œæ4
;
thus order 2 is validated. Likewise, we can check that (2.7) is indeed of order
3 from the expansion
œÅ(w) ‚àíœÉ(w) ln w = Œæ + 2Œæ2 + Œæ3
‚àí

1 + 5
2Œæ + 23
12Œæ2 
Œæ ‚àí1
2Œæ2 + 1
3Œæ3 ‚àí1
4Œæ4 + ¬∑ ¬∑ ¬∑

= 3
8Œæ4 + O

Œæ5
.
3
Nothing, unfortunately, could be further from good numerical practice than to assess
a multistep method solely ‚Äì or primarily ‚Äì in terms of its order. Thus, let us consider
the two-step implicit scheme
yn+2 ‚àí3yn+1 + 2yn = h
 13
12f(tn+2, yn+2) ‚àí5
3f(tn+1, yn+1) ‚àí5
12f(tn, yn)

. (2.12)
It is easy to ascertain that the order of (2.12) is 2. Encouraged by this ‚Äì and not
being very ambitious ‚Äì we will attempt to use this method to solve numerically the
exceedingly simple equation y‚Ä≤ ‚â°0, y(0) = 1. A single step reads yn+2‚àí3yn+1+2yn =
0, a recurrence relation whose general solution is yn = c1 + c22n, n = 0, 1, . . . , where
c1, c2 ‚ààR are arbitrary. Suppose that c2 Ã∏= 0; we need both y0 and y1 to launch
time-stepping and it is trivial to verify that c2 Ã∏= 0 is equivalent to y1 Ã∏= y0. It is easy
to prove that the method fails to converge. Thus, choose t > 0 and let h ‚Üí0 so that
nh ‚Üít. Obviously n ‚Üí‚àûand this implies that |yn| ‚Üí‚àû, which is far from the
exact value y(t) ‚â°1.
The failure in convergence does not require, realistically, that c2 Ã∏= 0 be induced
by y1. Any calculation on a real computer introduces a roundoÔ¨Äerror which, sooner
or later, is bound to render c2 Ã∏= 0 and so bring about a geometric growth in the error
of the method.

24
Multistep methods
0
2
4
6
8
10
12
14
0
0.5
1.0
1.5
2.0
2.5
 
 
 
 
Figure 2.2
The breakdown in the numerical solution of y‚Ä≤ = ‚àíy, y(0) = 1, by a
nonconvergent numerical scheme, showing how the situation worsens with decreasing
step size. The solid, broken and broken-and-dotted lines denote h =
1
10, 1
20 and
1
40
respectively.
Needless to say, a method that cannot integrate the simplest possible ODE with
any measure of reliability should not be used for more substantial computational
ends. Nontrivial order is not suÔ¨Écient to ensure convergence! The need thus arises
for a criterion that allows us to discard bad methods and narrow the Ô¨Åeld down to
convergent multistep schemes.
3 Failure to converge
Suppose that the linear equation y‚Ä≤ = ‚àíy, y(0) = 1,
is solved by a two-step, second-order method with œÅ(w) = w2 ‚àí2.01w +
1.01, œÉ(w) = 0.995w ‚àí1.005.
As will be soon evident, this method also
fails the convergence criterion, although not by a wide margin! Figure 2.2
displays three solution trajectories, for progressively decreasing step sizes h =
1
10, 1
20, 1
40. In all instances, in its early stages the solution perfectly resembles
the decaying exponential, but after a while small perturbations grow at an
increasing pace and render the computation meaningless. It is a characteristic
of nonconvergent methods that decreasing the step size actually makes matters
worse!
3
We say that a polynomial obeys the root condition if all its zeros reside in the closed
complex unit disc and all its zeros of unit modulus are simple.

2.2
Order and convergence of multistep methods
25
Theorem 2.2 (The Dahlquist equivalence theorem)
Suppose that the error
in the starting values y1, y2, . . . , ys‚àí1 tends to zero as h ‚Üí0+. The multistep method
(2.8) is convergent if and only if it is of order p ‚â•1 and the polynomial œÅ obeys the
root condition.
It is important to make crystal clear that convergence is not simply another at-
tribute of a numerical method, to be weighed alongside its other features. If a method
is not convergent ‚Äì and regardless of how attractive it may look ‚Äì do not use it!
Theorem 2.2 allows us to discard method (2.12) without further ado, since œÅ(w) =
(w ‚àí1)(w ‚àí2) violates the root condition. Of course, this method is contrived and,
even were it convergent, it is doubtful whether it would have been of much interest.
However, more ‚Äòrespectable‚Äô methods fail the convergence test.
For example, the
method
yn+3 + 27
11yn+2 ‚àí27
11yn+1 ‚àíyn
= h
 3
11f(tn+3, yn+3) + 27
11f(tn+2, yn+2) + 27
11f(tn+1, yn+1) + 3
11f(tn, yn)

is of order 6; it is the only three-step method that attains this order! Unfortunately,
œÅ(w) = (w ‚àí1)

w + 19 + 4
‚àö
15
11
 
w + 19 ‚àí4
‚àö
15
11

and the root condition fails. However, note that Adams‚ÄìBashforth methods are safe
for all s ‚â•1, since œÅ(w) = ws‚àí1(w ‚àí1).
3 Analysis and algebraic conditions
Theorem 2.2 demonstrates a state
of aÔ¨Äairs that prevails throughout mathematical analysis. Thus, we desire
to investigate an analytic condition, e.g. whether a diÔ¨Äerential equation has
a solution, whether a continuous dynamical system is asymptotically stable,
whether a numerical method converges. By their very nature, analytic con-
cepts involve inÔ¨Ånite processes and continua, hence one can expect analytic
conditions to be diÔ¨Écult to verify, to the point of unmanageability. For all we
know, the human brain (exactly like a digital computer) might be essentially
an algebraic machine. It is thus an important goal in mathematical analysis to
search for equivalent algebraic conditions. The Dahlquist equivalence theorem
is a remarkable example of this: everything essentially reduces to determin-
ing whether the zeros of a polynomial reside in a unit disc, and this can be
checked in a Ô¨Ånite number of algebraic operations! In the course of this book
we will encounter numerous other examples of this state of aÔ¨Äairs. Cast your
mind back to basic inÔ¨Ånitesimal calculus and you are bound to recall further
instances where analytic problems are rendered in an algebraic language.
3
The multistep method (2.8) has 2s + 1 parameters. Had order been the sole consider-
ation, we could have utilized all the available degrees of freedom to maximize it. The
outcome, an (implicit) s-step method of order 2s, is unfortunately not convergent for
s ‚â•3 (we have already seen the case s = 3). In general, it is possible to prove that the
maximal order of a convergent s-step method (2.8) is at most 2‚åä(s+2)/2‚åãfor implicit
schemes and just s for explicit ones; this is known as the Dahlquist Ô¨Årst barrier.

26
Multistep methods
The usual practice is to employ orders s + 1 and s for s-step implicit and explicit
methods respectively. An easy procedure for constructing such schemes is as follows.
Choose an arbitrary s-degree polynomial œÅ that obeys the root condition and such
that œÅ(1) = 0 (according to (2.11), œÅ(1) =  am = 0 is necessary for order p ‚â•1).
Dividing the order condition (2.10) by ln w we obtain
œÉ(w) = œÅ(w)
ln w + O(|w ‚àí1|p) .
(2.13)
(Note that division by ln w shaves oÔ¨Äa power of |w ‚àí1| and that the singularity
at w = 1 in the numerator and the denominator is removable.) Suppose Ô¨Årst that
p = s + 1 and no restrictions are placed on œÉ. We expand the fraction in (2.13) into
a Taylor series about w = 1 and let œÉ be the sth-degree polynomial that matches
the series up to O

|w ‚àí1|s+1
. The outcome is a convergent, s-step method of order
s+1. Likewise, to obtain an explicit method of order s, we let œÉ be an (s‚àí1)th-degree
polynomial (to force bm = 0) that matches the series up to O(|w ‚àí1|s).
Let us, for example, choose s = 2 and œÅ(w) = w2‚àíw. Letting, as before, Œæ = w‚àí1,
we have
œÅ(w)
ln w =
Œæ + Œæ2
Œæ ‚àí1
2Œæ2 + 1
3Œæ3 + O(Œæ4) =
1 + Œæ
1 ‚àí1
2Œæ + 1
3Œæ2 + O

Œæ3
= (1 + Œæ)

1 + 1
2Œæ ‚àí1
12Œæ2
+ O

Œæ3
= 1 + 3
2Œæ + 5
12Œæ2 + O

Œæ3
.
Thus, for quadratic œÉ and order 3 we truncate, obtaining
œÉ(w) = 1 + 3
2(w ‚àí1) + 5
12(w ‚àí1)2 = ‚àí1
12 + 2
3w + 5
12w2,
whereas in the explicit case where œÉ is linear we have p = 2, and so recover, unsur-
prisingly, the Adams‚ÄìBashforth scheme (2.6).
The choice œÅ(w) = ws‚àí1(w ‚àí1) is associated with Adams methods.
We have
already seen the explicit Adams‚ÄìBashforth schemes; their implicit counterparts are
Adams‚ÄìMoulton methods. However, provided that we wish to maximize the order
subject to convergence, without placing any extra constraints on the multistep method,
Adams schemes are the most reasonable choice.
After all, if ‚Äì as implied in the
statement of Theorem 2.2 ‚Äì large zeros of œÅ are bad, it makes perfect sense to drive
as many zeros as we can to the origin!
2.3
Backward diÔ¨Äerentiation formulae
Classical texts in numerical analysis present several distinct families of multistep meth-
ods. For example, letting œÅ(w) = ws‚àí2(w2‚àí1) leads to s-order explicit Nystrom meth-
ods and and to implicit Milne methods of order s + 1 (see Exercise 2.3). However,
in a well-deÔ¨Åned yet important situation, certain multistep methods are signiÔ¨Åcantly
better than other schemes of the type (2.8). These are the backward diÔ¨Äerentiation
formulae (BDFs), whose importance will become apparent in Chapter 4.

2.3
Backward diÔ¨Äerentiation formulae
27
An s-order s-step method is said to be a BDF if œÉ(w) = Œ≤ws for some Œ≤ ‚ààR\{0}.
Lemma 2.3
For a BDF we have
Œ≤ =

s

m=1
1
m
‚àí1
and
œÅ(w) = Œ≤
s

m=1
1
mws‚àím(w ‚àí1)m.
(2.14)
Proof
The order being p = s, (2.10) implies that
œÅ(w) ‚àíŒ≤ws ln w = O

|w ‚àí1|s+1
,
w ‚Üí1.
We substitute v = w‚àí1, hence
vsœÅ(v‚àí1) = ‚àíŒ≤ ln v + O

|v ‚àí1|s+1
,
v ‚Üí1.
Since
ln v = ln[1 + (v ‚àí1)] =
s

m=1
(‚àí1)m‚àí1
m
(v ‚àí1)m + O

|v ‚àí1|s+1
,
we deduce that
vsœÅ(v‚àí1) = Œ≤
s

m=1
(‚àí1)m
m
(v ‚àí1)m.
Therefore
œÅ(w) = Œ≤v‚àís
s

m=1
(‚àí1)m
m
(v ‚àí1)m = Œ≤
s

m=1
(‚àí1)m
m
ws(w‚àí1 ‚àí1)m
= Œ≤
s

m=1
1
mws‚àím(w ‚àí1)m.
To complete the proof of (2.14), we need only to derive the explicit form of Œ≤. It
follows at once by imposing the normalization condition as = 1 on the polynomial œÅ.
The simplest BDF has been already encountered in Chapter 1: when s = 1 we
recover the backward Euler method (1.15). The next two BDFs are
s = 2,
yn+2 ‚àí4
3yn+1 + 1
3yn = 2
3hf(tn+2, yn+2),
(2.15)
s = 3,
yn+3 ‚àí18
11yn+2 + 9
11yn+1 ‚àí2
11yn =
6
11hf(tn+3, yn+3).
(2.16)
Their derivation is trivial; for example, (2.16) follows by letting s = 3 in (2.14).
Therefore
Œ≤ =
1
1 + 1
2 + 1
3
=
6
11
and
œÅ(w) =
6
11

w2(w ‚àí1) + 1
2w(w ‚àí1)2 + 1
3(w ‚àí1)3
= w3 ‚àí18
11w2 + 9
11w ‚àí2
11.

28
Multistep methods
Since BDFs are derived by specifying œÉ, we cannot be sure that the polynomial œÅ
of (2.14) obeys the root condition. In fact, the root condition fails for all but a few
such methods.
Theorem 2.4
The polynomial (2.14) obeys the root condition and the underlying
BDF method is convergent if and only if 1 ‚â§s ‚â§6.
Fortunately, the ‚Äògood‚Äô range of s is suÔ¨Écient for all practical considerations.
Underscoring the importance of BDFs, we present a simple example that demon-
strates the limitations of Adams schemes; we hasten to emphasize that this is by way
of a trailer for our discussion of stiÔ¨ÄODEs in Chapter 4.
Let us consider the linear ODE system
y‚Ä≤ =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
‚àí20
10
0
¬∑ ¬∑ ¬∑
0
10
‚àí20
...
...
...
0
...
...
...
0
...
...
...
‚àí20
10
0
¬∑ ¬∑ ¬∑
0
10
‚àí20
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
y,
y(0) =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
1
1
...
1
1
‚é§
‚é•‚é•‚é•‚é•‚é•‚é¶
.
(2.17)
We will encounter in this book numerous instances of similar systems; (2.17) is a
handy paradigm for many linear ODEs that occur in the context of discretization of
the partial diÔ¨Äerential equations of evolution.
Figure 2.3 displays the Euclidean norm of the solution of (2.17) by the second-order
Adams‚ÄìBashforth method (2.6), with two (slightly) diÔ¨Äerent step sizes, h = 0.027 (the
solid line) and h = 0.0275 (the broken line). The solid line is indistinguishable in the
Ô¨Ågure from the norm of the true solution, which approaches zero as t ‚Üí‚àû. Not so
the norm for h = 0.0275: initially, it shadows the correct value pretty well but, after
a while, it runs away. The whole qualitative picture is utterly false! And, by the way,
things rapidly get considerably worse when h is increased: for h = 0.028 the norm
reaches 2.5 √ó 104, while for h = 0.029 it shoots to 1.3 √ó 1011.
What is the mechanism that degrades the numerical solution and renders it so
sensitive to small changes in h? At the moment it suÔ¨Éces to state that the quality
of local approximation (which we have quantiÔ¨Åed in the concept of ‚Äòorder‚Äô) is not to
blame; taking the third-order scheme (2.7) in place of the current method would have
only made matters worse. However, were we to attempt the solution of this ODE
with (2.15), say, and with any h > 0 then the norm would tend to zero in tandem
with the exact solution. In other words, methods such as BDFs are singled out by a
favourable property that makes them the methods of choice for important classes of
ODEs. Much more will be said about this in Chapter 4.
Comments and bibliography
There are several ways of introducing the theory of multistep methods. Traditional texts have
emphasized the derivation of schemes by various interpolation formulae. The approach of
Section 2.1 harks back to this approach, as does the name ‚Äòbackward diÔ¨Äerentiation formula‚Äô.

Comments and bibliography
29
0
1
2
3
4
5
6
7
8
9
10
0
1
2
3
4
5
6
Figure 2.3
The norm of the numerical solution of (2.17) by the Adams‚ÄìBashforth
method (2.6) for h = 0.027 (solid line) and h = 0.0275 (broken line).
Other books derive order conditions by sheer brute force, requiring that the multistep formula
(2.8) be exact for all polynomials of degree p, since this is equivalent to requiring order p.
Equation (2.8) can be expressed as a linear system of p + 1 equations in the 2s + 1 unknowns
a0, a1, . . . , as‚àí1, b0, b1, . . . , bs.
A solution of this system yields a multistep method of the
requisite order (of course, we must check it for convergence!), although this procedure does
not add much to our understanding of such methods.1 Linking order with an approximation
of the logarithm, along the lines of Theorem 2.1, elucidates matters on a considerably more
profound level. This can be shown by the following hand-waving argument.
Given an analytic function g, say, and a number h > 0, we denote g(k)
n
= g(k)(t0 + hn),
k, n = 0, 1, . . . , and deÔ¨Åne two operators that map such ‚Äògrid functions‚Äô into themselves, the
shift operator Eg(k)
n
:= g(k)
n+1 and the diÔ¨Äerential operator Dg(k)
n
:= g(k+1)
n
, k, n = 0, 1, . . . (see
Section 8.1). Expanding in a Taylor series about t0 + nh,
Eg(k)
n
=
‚àû

‚Ñì=0
1
‚Ñì!g(k+‚Ñì)
n
h‚Ñì=
 ‚àû

‚Ñì=0
1
‚Ñì!(hD)‚Ñì

g(k)
n ,
k, n = 0, 1, . . .
Since this is true for every analytic g with a radius of convergence exceeding h, it follows
that, at least formally, E = exp(hD). The exponential of the operator, exactly like the more
familiar matrix exponential, is deÔ¨Åned by a Taylor series.
The above argument can be tightened at the price of some mathematical sophistication.
The main problem with naively deÔ¨Åning E as the exponential of hD is that, in the stan-
dard spaces beloved by mathematicians, D is not a bounded linear operator.
To recover
boundedness we need to resort to a more exotic space.
1Though low on insight and beauty, brute force techniques are occasionally useful in mathematics
just as in more pedestrian walks of life.

30
Multistep methods
Let U ‚äÜC be an open connected set and denote by A(U) the vector space of analytic
functions deÔ¨Åned in U. The sequence {fn}‚àû
n=0, where fn ‚ààA(U), n = 0, 1, . . ., is said to
converge to f locally uniformly in A(U) if fn ‚Üíf uniformly in every compact (i.e., closed and
bounded) subset of U. It is possible to prove that there exists a metric (a ‚Äòdistance function‚Äô)
on A(U) that is consistent with locally uniform convergence and to demonstrate, using the
Cauchy integral formula, that the operator D is a bounded linear operator on A(U). Hence
so is E = exp(hD), and we can justify a deÔ¨Ånition of the exponential via a Taylor series.
The correspondence between the shift operator and the diÔ¨Äerential operator is funda-
mental to the numerical solution of ODEs ‚Äì after all, a diÔ¨Äerential equation provides us with
the action of D as well as with a function value at a single point, and the act of numerical
solution is concerned with (repeatedly) approximating the action of E. Equipped with our
new-found knowledge, we should realize that approximation of the exponential function plays
(often behind the scenes) a crucial role in designing numerical methods. Later, in Chapter 4,
approximations of exponentials, this time with a matrix argument, will be crucial to our
understanding of important stability issues, whereas the above-mentioned correspondence
forms the basis for our exposition of Ô¨Ånite diÔ¨Äerences in Chapter 8.
Applying the operatorial approach to multistep methods, we note at once that
s

m=0
amy(tn+m) ‚àíh
s

m=0
bmy‚Ä≤(tn+m) =

s

m=0
amEm ‚àíhD
s

m=0
bmEm

y(tn)
= [œÅ(E) ‚àíhDœÉ(E)] y(tn).
Note that E and D commute (since E is given in terms of a power series in D), and this
justiÔ¨Åes the above formula.
Moreover, E = exp(hD) means that hD = ln E, where the
logarithm, again, is deÔ¨Åned by means of a Taylor expansion (about the identity operator
I). This, in tandem with the observation that limh‚Üí0+ E = I, is the basis to an alternative
‚Äòproof‚Äô of Theorem 2.1 ‚Äì a proof that can be made completely rigorous with little eÔ¨Äort by
employing the implicit function theorem.
The proof of the equivalence theorem (Theorem 2.2) and the establishment of the Ô¨Årst bar-
rier (see Section 2.2) by Germund Dahlquist, in 1956 and 1959 respectively, were important
milestones in the history of numerical analysis. Not only are these results of great intrin-
sic impact but they were also instrumental in establishing numerical analysis as a bona Ô¨Åde
mathematical discipline and imparting a much-needed rigour to numerical thinking. It goes
without saying that numerical analysis is not just mathematics. It is much more! Numerical
analysis is Ô¨Årst and foremost about the computation of mathematical models originating in
science and engineering. It employs mathematics ‚Äì and computer science ‚Äì to an end. Quite
often we use a computational algorithm because, although it lacks formal mathematical jus-
tiÔ¨Åcation, our experience and intuition tell us that it is eÔ¨Écient and (hopefully) provides the
correct answer. There is nothing wrong with this! However, as always in applied mathemat-
ics, we must bear in mind the important goal of casting our intuition and experience into a
rigorous mathematical framework. Intuition is fallible and experience attempts to infer from
incomplete data ‚Äì mathematics is still the best tool of a computational scientist!
Modern texts in the numerical analysis of ODEs highlight the importance of a structured
mathematical approach. The classic monograph of Henrici (1962) is still a model of clear
and beautiful exposition and includes an easily digestible proof of the Dahlquist Ô¨Årst barrier.
Hairer et al. (1991) and Lambert (1991) are also highly recommended. In general, books
on numerical ODEs fall into two categories: pre-Dahlquist and post-Dahlquist. The Ô¨Årst
category is nowadays of mainly historical and antiquarian signiÔ¨Åcance.
We will encounter multistep methods again in Chapter 4.
As has been already seen
in Section 2.3, convergence and reasonable order are far from suÔ¨Écient for the successful

Exercises
31
computation of ODEs. The solution of such stiÔ¨Äequations requires numerical methods with
superior stability properties.
Much of the discussion of multistep methods centres upon their implementation. The
present chapter avoids any talk of implementation issues ‚Äì solution of the (mostly nonlinear)
algebraic equations associated with implicit methods, error and step-size control, the choice
of the starting values y1, y2, . . . , ys‚àí1. Our purpose has been an introduction to multistep
schemes and their main properties (convergence, order), as well as a brief survey of the
most distinguished members of the multistep methods menagerie. We defer the discussion of
implementation issues to Chapters 6 and 7.
Hairer, E., N√∏rsett, S.P. and Wanner, G. (1991), Solving Ordinary DiÔ¨Äerential Equations I:
NonstiÔ¨ÄProblems (2nd edn), Springer-Verlag, Berlin.
Henrici, P. (1962), Discrete Variable Methods in Ordinary DiÔ¨Äerential Equations, Wiley, New
York.
Lambert, J.D. (1991), Numerical Methods for Ordinary DiÔ¨Äerential Systems, Wiley, London.
Exercises
2.1
Derive explicitly the three-step and four-step Adams‚ÄìMoulton methods and
the three-step Adams‚ÄìBashforth method.
2.2
Let Œ∑(z, w) = œÅ(w) ‚àízœÉ(w).
a Demonstrate that the multistep method (2.8) is of order p if and only if
Œ∑(z, ez) = czp+1 + O

zp+2
,
z ‚Üí0,
for some c ‚ààR \ {0}.
b Prove that, subject to ‚àÇŒ∑(0, 1)/‚àÇw Ã∏= 0, there exists in a neighbourhood of
the origin an analytic function w1(z) such that Œ∑(z, w1(z)) = 0 and
w1(z) = ez ‚àíc
	‚àÇŒ∑(0, 1)
‚àÇw

‚àí1
zp+1 + O

zp+2
,
z ‚Üí0.
(2.18)
c Show that (2.18) is true if the underlying method is convergent.
(Hint:
Express ‚àÇŒ∑(0, 1)/‚àÇw in terms of the polynomial œÅ.)
2.3
Instead of (2.3), consider the identity
y(tn+s) = y(tn+s‚àí2) +
 tn+s
tn+s‚àí2
f(œÑ, y(œÑ)) dœÑ.
a Replace f(œÑ, y(œÑ)) by the interpolating polynomial p from Section 2.1 and
substitute yn+s‚àí2 in place of y(tn+s‚àí2). Prove that the resultant explicit
Nystrom method is of order p = s.

32
Multistep methods
b Derive the two-step Nystrom method in a closed form by using the above
approach.
c Find the coeÔ¨Écients of the two-step and three-step Nystrom methods by
noticing that œÅ(w) = ws‚àí2(w2 ‚àí1) and evaluating œÉ from (2.13).
d Derive the two-step third-order implicit Milne method, again letting œÅ(w) =
ws‚àí2(w2 ‚àí1) but allowing œÉ to be of degree s.
2.4
Determine the order of the three-step method
yn+3 ‚àíyn = h
 3
8f(tn+3, yn+3) + 9
8f(tn+2, yn+2) + 9
8f(tn+1, yn+1)
+ 3
8f(tn, yn)

,
the three-eighths scheme. Is it convergent?
2.5‚ãÜ
By solving a three-term recurrence relation, calculate analytically the se-
quence of values y2, y3, . . . that is generated by the midpoint rule
yn+2 = yn + 2hf(tn+1, yn+1)
when it is applied to the diÔ¨Äerential equation y‚Ä≤ = ‚àíy. Starting from the
values y0 = 1, y1 = 1‚àíh, show that the sequence diverges as n ‚Üí‚àû. Recall,
however, from Theorem 2.1 that the root condition, in tandem with order p ‚â•
1 and suitable starting conditions, imply convergence to the true solution in
a Ô¨Ånite interval as h ‚Üí0+. Prove that this implementation of the midpoint
rule is consistent with the above theorem. (Hint: Express the roots of the
characteristic polynomial of the recurrence relation as exp(¬± sinh‚àí1 h).)
2.6
Show that the explicit multistep method
yn+3 + Œ±2yn+2 + Œ±1yn+1 + Œ±0yn = h[Œ≤2f(tn+2, yn+2)
+ Œ≤1f(tn+1, yn+1) + Œ≤0f(tn, yn)]
is fourth order only if Œ±0 + Œ±2 = 8 and Œ±1 = ‚àí9. Hence deduce that this
method cannot be both fourth order and convergent.
2.7
Prove that the BDFs (2.15) and (2.16) are convergent.
2.8
Find the explicit form of the BDF for s = 4.
2.9
An s-step method with œÉ(w) = ws‚àí1(w + 1) and order s might be superior
to a BDF in certain situations.
a Find a general formula for œÅ and Œ≤, along the lines of (2.14).
b Derive explicitly such methods for s = 2 and s = 3.
c Are the last two methods convergent?

3
Runge‚ÄìKutta methods
3.1
Gaussian quadrature
The exact solution of the trivial ordinary diÔ¨Äerential equation (ODE)
y‚Ä≤ = f(t),
t ‚â•t0,
y(t0) = y0,
whose right-hand side is independent of y, is y0 +
! t
t0 f(œÑ) dœÑ. Since a very rich theory
and powerful methods exist to compute integrals numerically, it is only natural to
wish to utilize them in the numerical solution of general ODEs
y‚Ä≤ = f(t, y),
t ‚â•t0,
y(t0) = y0,
(3.1)
and this is the rationale behind Runge‚ÄìKutta methods. Before we debate such meth-
ods, it is thus Ô¨Åt and proper to devote some attention to the numerical calculation of
integrals, a subject of signiÔ¨Åcant importance on its own merit.
It is usual to replace an integral with a Ô¨Ånite sum, a procedure known as quadrature.
SpeciÔ¨Åcally, let œâ be a nonnegative function acting in the interval (a, b), such that
0 <
 b
a
œâ(œÑ) dœÑ < ‚àû,

 b
a
œÑ jœâ(œÑ) dœÑ
 < ‚àû,
j = 1, 2, . . . ;
œâ is dubbed the weight function. We approximate as follows:
 b
a
f(œÑ)œâ(œÑ) dœÑ ‚âà
ŒΩ

j=1
bjf(cj),
(3.2)
where the numbers b1, b2, . . . , bŒΩ and c1, c2, . . . , cŒΩ, which are independent of the func-
tion f (but, in general, depend upon œâ, a and b), are called the quadrature weights
and nodes, respectively. Note that we do not require a and b in (3.2) to be bounded;
the choices a = ‚àí‚àûor b = +‚àûare perfectly acceptable. Of course, we stipulate
a < b.
How good is the approximation (3.2)? Suppose that the quadrature matches the
integral exactly whenever f is an arbitrary polynomial of degree p ‚àí1. It is then easy
to prove, e.g. by using the Peano kernel theorem (see A.2.2.6), that, for every function
f with p smooth derivatives,

 b
a
f(œÑ)œâ(œÑ) dœÑ ‚àí
ŒΩ

j=1
bjf(cj)
 ‚â§c max
a‚â§t‚â§b
f (p)(t)
 ,
33

34
Runge‚ÄìKutta methods
where the constant c > 0 is independent of f. Such a quadrature formula is said to
be of order p.
We denote the set of all real polynomials of degree m by Pm. Thus, (3.2) is of
order p if it is exact for every f ‚ààPp‚àí1.
Lemma 3.1
Given any distinct set of nodes c1, c2, . . . , cŒΩ, it is possible to Ô¨Ånd a
unique set of weights b1, b2, . . . , bŒΩ such that the quadrature formula (3.2) is of order
p ‚â•ŒΩ.
Proof
Since PŒΩ‚àí1 is a linear space, it is necessary and suÔ¨Écient for order ŒΩ that
(3.2) is exact for elements of an arbitrary basis of PŒΩ‚àí1. We choose the simplest such
basis, namely {1, t, t2, . . . , tŒΩ‚àí1}, and the order conditions then read
ŒΩ

j=1
bjcm
j =
 b
a
œÑ mœâ(œÑ) dœÑ,
m = 0, 1, . . . , ŒΩ ‚àí1.
(3.3)
This is a system of ŒΩ equations in the ŒΩ unknowns b1, b2, . . . , bŒΩ, whose matrix, the
nodes being distinct, is a nonsingular Vandermonde matrix (A.1.2.5). Thus, the sys-
tem possesses a unique solution and we recover a quadrature of order p ‚â•ŒΩ.
The weights b1, b2, . . . , bŒΩ can be derived explicitly with little extra eÔ¨Äort and we
make use of this in (3.14) below. Let
pj(t) =
ŒΩ

k=1
kÃ∏=j
t ‚àíck
cj ‚àíck
,
j = 1, 2, . . . , ŒΩ,
be Lagrange polynomials (A.2.2.3). Because
ŒΩ

j=1
pj(t)g(cj) = g(t)
for every polynomial g of degree ŒΩ ‚àí1, it follows that
ŒΩ

j=1
 b
a
pj(œÑ)œâ(œÑ) dœÑcm
j =
 b
a
‚é°
‚é£
ŒΩ

j=1
pj(œÑ)cm
j
‚é§
‚é¶œâ(œÑ) dœÑ =
 b
a
œÑ mœâ(œÑ) dœÑ
for every m = 0, 1, . . . , ŒΩ ‚àí1. Therefore
bj =
 b
a
pj(œÑ)œâ(œÑ) dœÑ,
j = 1, 2, . . . , ŒΩ,
is the solution of (3.3).
A natural inclination is to choose quadrature nodes that are equispaced in [a, b],
and this leads to the so-called Newton‚ÄìCotes methods. This procedure, however, falls
far short of optimal; by making an adroit choice of c1, c2, . . . , cŒΩ, we can, in fact, double
the order to 2ŒΩ.

3.1
Gaussian quadrature
35
Each weight function œâ determines an inner product (see A.1.3.1) in the interval
(a, b), namely
‚ü®f, g‚ü©:=
 b
a
f(œÑ)g(œÑ)œâ(œÑ) dœÑ,
whose domain is the set of all functions f, g such that
 b
a
[f(œÑ)]2œâ(œÑ) dœÑ,
 b
a
[g(œÑ)]2œâ(œÑ) dœÑ < ‚àû.
We say that pm ‚ààPm, pm Ã∏‚â°0, is an mth orthogonal polynomial (with respect to the
weight function œâ) if
‚ü®pm, ÀÜp‚ü©= 0,
for every
ÀÜp ‚ààPm‚àí1.
(3.4)
Orthogonal polynomials are not unique, since we can always multiply pm by a nonzero
constant without violating (3.4). However, it is easy to demonstrate that monic or-
thogonal polynomials are unique. (The coeÔ¨Écient of the highest power of t in a monic
polynomial equals unity.) Suppose that both pm and Àúpm are monic mth-degree or-
thogonal polynomials with respect to the same weight function. Then pm‚àíÀúpm ‚ààPm‚àí1
and, by (3.4), ‚ü®pm, pm ‚àíÀúpm‚ü©= ‚ü®Àúpm, pm ‚àíÀúpm‚ü©= 0. We thus deduce from the linearity
of the inner product that ‚ü®pm ‚àíÀúpm, pm ‚àíÀúpm‚ü©= 0, and this is possible, according to
Appendix subsection A.1.3.1, only if Àúpm = pm.
Orthogonal polynomials occur in many areas of mathematics; a brief list includes
approximation theory, statistics, representation of groups, the theory of ordinary and
partial diÔ¨Äerential equations, functional analysis, quantum groups, coding theory, com-
binatorics, mathematical physics and, last but not least, numerical analysis.
3 Classical orthogonal polynomials
Three families of weights give rise to
classical orthogonal polynomials.
Let a = ‚àí1, b = 1 and œâ(t) = (1 ‚àít)Œ±(1 + t)Œ≤, where Œ±, Œ≤ > ‚àí1.
The
underlying orthogonal polynomials are known as Jacobi polynomials P (Œ±,Œ≤)
m
.
We single out for special attention the Legendre polynomials Pm, which corre-
spond to Œ± = Œ≤ = 0, and the Chebyshev polynomials Tm, associated with the
choice Œ± = Œ≤ = ‚àí1
2. Note that for min{Œ±, Œ≤} < 0 the weight function has a
singularity at the endpoints ¬±1. There is nothing wrong with that, provided
œâ is integrable in [0, 1]; but this is exactly the reason we require Œ±, Œ≤ > ‚àí1.
The other two ‚Äòclassics‚Äô are the Laguerre and Hermite polynomials.
The
Laguerre polynomials L(Œ±)
m are orthogonal with respect to the weight function
œâ(t) = tŒ±e‚àít, (a, b) = (0, ‚àû), Œ± > ‚àí1, whereas the Hermite polynomials Hm
are orthogonal in (a, b) = R with respect to the weight function œâ(t) = e‚àít2.
Why are classical orthogonal polynomials so named? Firstly, they have been
very extensively studied and occur in a very wide range of applications. Sec-
ondly, it is possible to prove that they are singled out by several properties
that, in a well-deÔ¨Åned sense, render them the ‚Äòsimplest‚Äô orthogonal polyno-
mials. For example ‚Äì and do not try to prove this on your own! ‚Äì P (Œ±,Œ≤)
m
,
L(Œ±)
m
and Hm are the only orthogonal polynomials whose derivatives are also
orthogonal with some other weight function.
3

36
Runge‚ÄìKutta methods
The theory of orthogonal polynomials is replete with beautiful results which, perhaps
regrettably, we do not require in this volume. However, one morsel of information, ger-
mane to the understanding of quadrature, is about the location of zeros of orthogonal
polynomials.
Lemma 3.2
All m zeros of an orthogonal polynomial pm reside in the interval (a, b)
and they are simple.
Proof
Since
 b
a
pm(œÑ)œâ(œÑ) dœÑ = ‚ü®pm, 1‚ü©= 0
and œâ ‚â•0, it follows that pm changes sign at least once in (a, b). Let us thus denote
by x1, x2, . . . , xk all the points in (a, b) where pm changes sign. We already know that
k ‚â•1. Let us assume that k ‚â§m ‚àí1 and set
q(t) :=
k

j=1
(t ‚àíxj) =
k

i=0
qiti.
Therefore pm changes sign in (a, b) at exactly the same points as q and the product
pmq does not change sign there at all. The weight function being nonnegative and
pmq Ã∏‚â°0, we deduce on the one hand that
 b
a
pm(œÑ)q(œÑ)œâ(œÑ) dœÑ Ã∏= 0.
On the other hand, the orthogonality condition (3.4) and the linearity of the inner
product imply that
 b
a
pm(œÑ)q(œÑ)œâ(œÑ) dœÑ =
k

i=0
qi‚ü®pm, ti‚ü©= 0,
because k ‚â§m ‚àí1. This is a contradiction and we conclude that k ‚â•m. Since
each sign-change of pm is a zero of the polynomial and, according to the fundamental
theorem of algebra, each ÀÜp ‚ààPm \ Pm‚àí1 has exactly m zeros in C, we deduce that pm
has exactly m simple zeros in (a, b).
Theorem 3.3
Let c1, c2, . . . , cŒΩ be the zeros of pŒΩ and let b1, b2, . . . , bŒΩ be the solution
of the Vandermonde system (3.3). Then
(i)
The quadrature method (3.2) is of order 2ŒΩ;
(ii)
No other quadrature can exceed this order.
Proof
Let ÀÜp ‚ààP2ŒΩ‚àí1. Applying the Euclidean algorithm to the pair {ÀÜp, pŒΩ} we
deduce that there exist q, r ‚ààPŒΩ‚àí1 such that ÀÜp = pŒΩq + r. Therefore, according to
(3.4),
 b
a
ÀÜp(œÑ)œâ(œÑ) dœÑ = ‚ü®pŒΩ, q‚ü©+
 b
a
r(œÑ)œâ(œÑ) dœÑ =
 b
a
r(œÑ)œâ(œÑ) dœÑ;

3.1
Gaussian quadrature
37
we recall that deg q ‚â§ŒΩ ‚àí1. Moreover,
ŒΩ

j=1
bj ÀÜp(cj) =
ŒΩ

j=1
bjpŒΩ(cj)q(cj) +
ŒΩ

j=1
bjr(cj) =
ŒΩ

j=1
bjr(cj)
because pŒΩ(cj) = 0, j = 1, 2, . . . , ŒΩ. Finally, r ‚ààPŒΩ‚àí1 and Lemma 3.1 imply
 b
a
r(œÑ)œâ(œÑ) dœÑ =
ŒΩ

j=1
bjr(cj).
We thus deduce that
 b
a
ÀÜp(œÑ)œâ(œÑ) dœÑ =
ŒΩ

j=1
bj ÀÜp(cj),
ÀÜp ‚ààP2ŒΩ‚àí1,
and that the quadrature formula is of order p ‚â•2ŒΩ.
To prove (ii) (and, incidentally, to aÔ¨Érm that p = 2ŒΩ, thereby completing the proof
of (i)) we assume that, for some choice of weights b1, b2, . . . , bŒΩ and nodes c1, c2, . . . , cŒΩ,
the quadrature formula (3.2) is of order p ‚â•2ŒΩ + 1. In particular, it would then
integrate exactly the polynomial
ÀÜp(t) :=
ŒΩ

i=1
(t ‚àíci)2,
ÀÜp ‚ààP2ŒΩ.
This, however, is impossible, since
 b
a
ÀÜp(œÑ)œâ(œÑ) dœÑ =
 b
a
 ŒΩ

i=1
(œÑ ‚àíci)
2
œâ(œÑ) dœÑ > 0,
while
ŒΩ

j=1
bj ÀÜp(cj) =
ŒΩ

j=1
bj
ŒΩ

i=1
(cj ‚àíci)2 = 0.
The proof is complete.
The optimal methods of the last theorem are commonly known as Gaussian quadra-
ture formulae.
In what follows we will require a generalization of Theorem 3.3. Its proof is left as
an exercise to the reader.
Theorem 3.4
Let r ‚ààPŒΩ obey the orthogonality conditions
‚ü®r, ÀÜp‚ü©= 0
for every
ÀÜp ‚ààPm‚àí1,
‚ü®r, tm‚ü©Ã∏= 0,
for some m ‚àà{0, 1, . . . , ŒΩ}. We let c1, c2, . . . , cŒΩ be the zeros of the polynomial r and
choose b1, b2, . . . , bŒΩ consistently with (3.3). The quadrature formula (3.2) has order
p = ŒΩ + m.

38
Runge‚ÄìKutta methods
3.2
Explicit Runge‚ÄìKutta schemes
How do we extend a quadrature formula to the ODE (3.1)? The obvious approach is
to integrate from tn to tn+1 = tn + h:
y(tn+1) = y(tn) +
 tn+1
tn
f(œÑ, y(œÑ)) dœÑ = y(tn) + h
 1
0
f(tn + hœÑ, y(tn + hœÑ)) dœÑ,
and to replace the second integral by a quadrature. The outcome might have been
the ‚Äòmethod‚Äô
yn+1 = yn + h
ŒΩ

j=1
bjf(tn + cjh, y(tn + cjh)),
n = 0, 1, . . . ,
except that we do not know the value of y at the nodes tn + c1h, tn + c2, . . . , tn + cŒΩh.
We must resort to an approximation!
We denote our approximation of y(tn+cjh) by Œæj, j = 1, 2, . . . , ŒΩ. To start with, we
let c1 = 0, since then the approximation is already provided by the former step of the
numerical method, Œæ1 = yn. The idea behind explicit Runge‚ÄìKutta (ERK) methods
is to express each Œæj, j = 2, 3, . . . , ŒΩ, by updating yn with a linear combination of
f(tn, Œæ1), f(tn + hc2, Œæ2), . . . , f(tn + cj‚àí1h, Œæj‚àí1). SpeciÔ¨Åcally, we let
Œæ1 = yn,
Œæ2 = yn + ha2,1f(tn, Œæ1),
Œæ3 = yn + ha3,1f(tn, Œæ1) + ha3,2f(tn + c2h, Œæ2),
...
(3.5)
ŒæŒΩ = yn + h
ŒΩ‚àí1

i=1
aŒΩ,if(tn + cih, Œæi),
yn+1 = yn + h
ŒΩ

j=1
bjf(tn + cjh, Œæj).
The matrix A = (aj,i)j,i=1,2,...,ŒΩ, where missing elements are deÔ¨Åned to be zero, is
called the RK matrix, while
b =
‚é°
‚é¢‚é¢‚é¢‚é£
b1
b2
...
bŒΩ
‚é§
‚é•‚é•‚é•‚é¶
and
c =
‚é°
‚é¢‚é¢‚é¢‚é£
c1
c2
...
cŒΩ
‚é§
‚é•‚é•‚é•‚é¶
are the RK weights and RK nodes respectively.
We say that (3.5) has ŒΩ stages.
Confusingly, sometimes the Œæj are called ‚ÄòRK stages‚Äô; elsewhere this name is reserved
for f(tn +cjh, Œæj), j = 1, 2, . . . , s. To avoid confusion, we henceforth desist from using
the phrase ‚ÄòRK stages‚Äô.
How should we choose the RK matrix? The most obvious way consists of expanding
everything in sight in Taylor series about (tn, yn); but, in a naive rendition, this is

3.2
Explicit Runge‚ÄìKutta schemes
39
of strictly limited utility. For example, let us consider the simplest nontrivial case,
ŒΩ = 2. Assuming suÔ¨Écient smoothness of the vector function f, we have
f(tn + c2h, Œæ2) = f(tn + c2h, yn + a2,1hf(tn, yn))
= f(tn, yn) + h

c2
‚àÇf(tn, yn)
‚àÇt
+ a2,1
‚àÇf(tn, yn)
‚àÇy
f(tn, yn)

+ O

h2
;
therefore the last equation in (3.5) becomes
yn+1 = yn + h(b1 + b2)f(tn, yn)
+ h2b2

c2
‚àÇf(tn, yn)
‚àÇt
+ a2,1
‚àÇf(tn, yn)
‚àÇy
f(tn, yn)

+ O

h3
.
(3.6)
We need to compare (3.6) with the Taylor expansion of the exact solution about
the same point (tn, yn). The Ô¨Årst derivative is provided by the ODE, whereas we can
obtain y‚Ä≤‚Ä≤ by diÔ¨Äerentiating (3.1) with respect to t:
y‚Ä≤‚Ä≤ = ‚àÇf(t, y)
‚àÇt
+ ‚àÇf(t, y)
‚àÇy
f(t, y).
We denote the exact solution at tn+1, subject to the initial condition yn at tn, by Àúy.
Therefore, by the Taylor theorem,
Àúy(tn+1) = yn + hf(tn, yn) + 1
2h2
‚àÇf(tn, yn)
‚àÇt
+ ‚àÇf(tn, yn)
‚àÇy
f(tn, yn)

+ O

h3
.
Comparison with (3.6) gives us the condition for order p ‚â•2:
b1 + b2 = 1,
b2c2 = 1
2,
a2,1 = c2.
(3.7)
It is easy to verify that the order cannot exceed 2, e.g. by applying the ERK method
to the scalar equation y‚Ä≤ = y.
The conditions (3.7) do not deÔ¨Åne a two-stage ERK uniquely. Popular choices of
parameters are displayed in the RK tableaux
0
1
2
1
2
0
1
,
0
2
3
2
3
1
4
3
4
and
0
1
1
1
2
1
2
.
which are of the following form:
c
A
b‚ä§.
A naive expansion can be carried out (with substantially greater eÔ¨Äort) for ŒΩ = 3,
whereby we can obtain third-order schemes. However, this is clearly not a serious
contender in the technique-of-the-month competition. Fortunately, there are substan-
tially more powerful and easier means of analysing the order of Runge‚ÄìKutta methods.
We commence by observing that the condition
j‚àí1

i=1
aj,i = cj,
j = 2, 3, . . . , ŒΩ,

40
Runge‚ÄìKutta methods
is necessary for order 1 ‚Äì otherwise we cannot recover the solution of y‚Ä≤ = y. The
simplest device, which unfortunately is valid only for p ‚â§3, consists of verifying the
order for the scalar autonomous equation
y‚Ä≤ = f(y),
t ‚â•t0,
y(t0) = y0,
(3.8)
rather than for (3.1). We do not intend here to justify the above assertion but merely to
demonstrate its eÔ¨Écacy in the case ŒΩ = 3. We henceforth adopt the ‚Äòlocal convention‚Äô
that, unless indicated otherwise, all the quantities are evaluated at tn, e.g. y ‚àºyn,
f ‚àºf(yn) etc. Subscripts denote derivatives. In the notation of (3.5), we have
Œæ1 = y
‚áí
f(Œæ1) = f;
Œæ2 = y + hc2f
‚áí
f(Œæ2) = f(y + hc2f) = f + hc2fyf + 1
2h2c2
2fyyf 2 + O

h3
;
Œæ3 = y + h(c3 ‚àía3,2)f(Œæ1) + ha3,2f(Œæ2)
= y + (c3 ‚àía3,2)f + ha3,2f(y + hc2f) + O

h3
= y + hc3f + h2a3,2c2fyf + O

h3
‚áí
f(Œæ3) = f(y + hc3f + h2a3,2c2fyf) + O

h3
= f + hc3fyf + h2  1
2c2
3fyyf 2 + a3,2c2f 2
y f

+ O

h3
.
Therefore
yn+1 = y + hb1f + hb2

f + hc2fyf + 1
2h2c2
2fyyf 2
+ hb3

f + hc3fyf + h2  1
2c2
3fyyf 2 + a3,2c2f 2
y f

+ O

h4
= yn + h(b1 + b2 + b3)f + h2(c2b2 + c3b3)fyf
+ h3  1
2(b2c2
2 + b3c3
3)fyyf 2 + b3a3,2c2f 2
y f

+ O

h4
.
Since
Àúy‚Ä≤ = f,
Àúy‚Ä≤‚Ä≤ = fyf,
Àúy‚Ä≤‚Ä≤‚Ä≤ = fyyf 2 + f 2
y f
the expansion of Àúy reads
Àúyn+1 = y + hf + 1
2h2fyf + 1
6h3 
fyyf 2 + f 2
y f

+ O

h4
.
Comparison of the powers of h leads to third-order conditions, namely
b1 + b2 + b3 = 1,
b2c2 + b3c3 = 1
2,
b2c2
2 + b3c2
3 = 1
3,
b3a3,2c2 = 1
6.
Some instances of third-order three-stage ERK methods are important enough to
merit an individual name, for example the classical RK method
0
1
2
1
2
1
‚àí1
2
1
6
2
3
1
6

3.3
Implicit Runge‚ÄìKutta schemes
41
and the Nystrom scheme
0
2
3
2
3
2
3
0
2
3
1
4
3
8
3
8
.
Fourth order is not beyond the capabilities of a Taylor expansion, although a
great deal of persistence and care (or, alternatively, a good symbolic manipulator) are
required. The best-known fourth-order four-stage ERK method is
0
1
2
1
2
1
2
0
1
2
1
0
0
1
1
6
1
3
1
3
1
6
.
The derivation of higher-order ERK methods requires a substantially more ad-
vanced technique based upon graph theory. It is well beyond the scope of this volume
(but see the comments at the end of this chapter). The analysis is further complicated
by the fact that ŒΩ-stage ERKs of order ŒΩ exist only for ŒΩ ‚â§4. To obtain order 5 we
need six stages, and matters become considerably worse for higher orders.
3.3
Implicit Runge‚ÄìKutta schemes
The idea behind implicit Runge‚ÄìKutta (IRK) methods is to allow the vector functions
Œæ1, Œæ2, . . . , ŒæŒΩ to depend upon each other in a more general manner than that of (3.5).
Thus, let us consider the scheme
Œæj = yn + h
ŒΩ

i=1
aj,if(tn + cih, Œæi),
j = 1, 2, . . . , ŒΩ,
yn+1 = yn + h
ŒΩ

j=1
bjf(tn + cjh, Œæj).
(3.9)
Here A = (aj,i)j,i=1,2,...,ŒΩ is an arbitrary matrix, whereas in (3.5) it was strictly lower
triangular. We impose the convention
ŒΩ

i=1
aj,i = cj,
j = 1, 2, . . . , ŒΩ,
which is necessary for the method to be of nontrivial order. The ERK terminology ‚Äì
RK nodes, RK weights etc. ‚Äì stays in place.
For general RK matrix A, the algorithm (3.9) is a system of ŒΩd coupled algebraic
equations, where y ‚ààRd. Hence, its calculation faces us with a task of an altogether
diÔ¨Äerent magnitude than the explicit method (3.5). However, IRK schemes possess
important advantages; in particular they may exhibit superior stability properties.

42
Runge‚ÄìKutta methods
Moreover, as will be apparent in Section 3.4, there exists for every ŒΩ ‚â•1 a unique
IRK method of order 2ŒΩ, a natural extension of the Gaussian quadrature formulae of
Theorem 3.3.
3 A two-stage IRK method
Let us consider the method
Œæ1 = yn + 1
4h

f(tn, Œæ1) ‚àíf(tn + 2
3h, Œæ2)

,
Œæ2 = yn + 1
12h

3f(tn, Œæ1) + 5f(tn + 2
3h, Œæ2)

,
(3.10)
yn+1 = yn + 1
4h

f(tn, Œæ1) + 3f(tn + 2
3h, Œæ2)

.
In tableau notation it reads
0
1
4
‚àí1
4
2
3
1
4
5
12
1
4
3
4
.
To investigate the order of (3.10), we again assume that the underlying ODE
is scalar and autonomous ‚Äì a procedure that is justiÔ¨Åed since we do not intend
to exceed third order. As before, the convention is that each quantity, unless
explicitly stated to the contrary, is evaluated at yn.
Let k1 := f(Œæ1) and
k2 := f(Œæ2). Expanding about yn,
k1 = f + 1
4hfy(k1 ‚àík2) + 1
32h2fyy(k1 ‚àík2)2 + O

h3
,
k2 = f + 1
12hfy(3k1 + 5k2) +
1
288h2fyy(3k1 + 5k2)2 + O

h3
,
therefore k1, k2 = f + O(h). Substituting this on the right-hand side of the
above equations yields k1 = f +O

h2
, k2 = f + 2
3hfyf +O

h2
. Substituting
again these enhanced estimates, we Ô¨Ånally obtain
k1 = f ‚àí1
6h2f 2
y f + O

h3
,
k2 = f + 2
3hfyf + h2  5
18f 2
y f + 2
9fyyf 2
+ O

h3
.
Consequently, on the one hand we have
yn+1 = yn + h(b1k1 + b2k2)
= y + hf + 1
2h2fyf + 1
6h3(f 2
y f + fyyf 2) + O

h4
.
(3.11)
On the other hand, y‚Ä≤ = f, y‚Ä≤‚Ä≤ = fyf, y‚Ä≤‚Ä≤‚Ä≤ = f 2
y f 2 + fyyf 2 and the exact
expansion is
Àúyn+1 = y + hf + 1
2h2fyf + 1
6h3(f 2
y f + fyyf 2) + O

h4
,
and this matches (3.11). We thus deduce that the method (3.10) is of order
at least 3. It is, actually, of order exactly 3, and this can be demonstrated by
applying (3.10) to the linear equation y‚Ä≤ = y.
3
It is perfectly possible to derive IRK methods of higher order by employing the graph-
theoretic technique mentioned at the end of Section 3.2.
However, an important
subset of implicit Runge‚ÄìKutta schemes can be investigated very easily and without
any cumbersome expansions by an entirely diÔ¨Äerent approach. This will be the theme
of the next section.

3.4
Collocation and IRK methods
43
3.4
Collocation and IRK methods
Let us abandon Runge‚ÄìKutta methods for a little while and consider instead an al-
ternative approach to the numerical solution of the ODE (3.1). As before, we assume
that the integration has been already carried out up to (tn, yn) and we seek a recipe
to advance it to (tn+1, yn+1), where tn+1 = tn + h. To this end we choose ŒΩ distinct
collocation parameters c1, c2, . . . , cŒΩ (preferably in [0, 1], although this is not essential
to our argument) and seek a ŒΩth-degree polynomial u (with vector coeÔ¨Écients) such
that
u(tn) = yn,
u‚Ä≤(tn + cjh) = f(tn + cjh, u(tn + cjh)),
j = 1, 2, . . . , ŒΩ.
(3.12)
In other words, u obeys the initial condition and satisÔ¨Åes the diÔ¨Äerential equation
(3.1) exactly at ŒΩ distinct points. A collocation method consists of Ô¨Ånding such a u
and setting
yn+1 = u(tn+1).
The collocation method sounds eminently plausible. Yet, you will search for it
in vain in most expositions of ODE methods. The reason is that we have not been
entirely sincere at the beginning of this section: collocation is nothing other than a
Runge‚ÄìKutta method in disguise.
Lemma 3.5
Set
q(t) :=
ŒΩ

j=1
(t ‚àícj),
q‚Ñì(t) := q(t)
t ‚àíc‚Ñì
,
‚Ñì= 1, 2, . . . , ŒΩ,
and let
aj,i :=
 cj
0
qi(œÑ)
qi(ci) dœÑ,
j, i = 1, 2, . . . , ŒΩ,
(3.13)
bj :=
 1
0
qj(œÑ)
qj(cj) dœÑ,
j = 1, 2, . . . , ŒΩ.
(3.14)
The collocation method (3.12) is identical to the IRK method
c
A
b‚ä§.
Proof
According to appendix subsection A.2.2.3, the Lagrange interpolation poly-
nomial
r(t) :=
ŒΩ

‚Ñì=1
q‚Ñì((t ‚àítn)/h)
q‚Ñì(c‚Ñì)
w‚Ñì
satisÔ¨Åes r(tn + c‚Ñìh) = w‚Ñì, ‚Ñì= 1, 2, . . . , ŒΩ. Let us choose w‚Ñì= u‚Ä≤(tn + c‚Ñìh), ‚Ñì=
1, 2, . . . , ŒΩ. The two (ŒΩ ‚àí1)th-degree polynomials r and u‚Ä≤ coincide at ŒΩ points and

44
Runge‚ÄìKutta methods
we thus conclude that r ‚â°u‚Ä≤. Therefore, invoking (3.12),
u‚Ä≤(t) =
ŒΩ

‚Ñì=1
q‚Ñì((t ‚àítn)/h)
q‚Ñì(c‚Ñì)
u‚Ä≤(tn + c‚Ñìh) =
ŒΩ

‚Ñì=1
q‚Ñì((t ‚àítn)/h)
q‚Ñì(c‚Ñì)
f(tn + c‚Ñìh, u(tn + c‚Ñìh)).
We will integrate the last expression. Since u(tn) = yn, the outcome is
u(t) = yn +
 t
tn
ŒΩ

‚Ñì=1
f(tn + c‚Ñìh, u(tn + c‚Ñìh))q‚Ñì((œÑ ‚àítn)/h)
q‚Ñì(c‚Ñì)
dœÑ
= yn + h
ŒΩ

‚Ñì=1
f(tn + c‚Ñìh, u(tn + c‚Ñìh))
 (t‚àítn)/h
0
q‚Ñì(œÑ)
q‚Ñì(c‚Ñì) dœÑ.
(3.15)
We set Œæj := u(tn + cjh), j = 1, 2, . . . , ŒΩ. Letting t = tn + cjh in (3.15), the deÔ¨Ånition
(3.13) implies that
Œæj = yn + h
ŒΩ

i=1
aj,if(tn + cih, Œæi),
j = 1, 2, . . . , ŒΩ,
whereas t = tn+1 and (3.14) yield
yn+1 = u(tn+1) = yn +
ŒΩ

j=1
bjf(tn + cjh, Œæj).
Thus, we recover the deÔ¨Ånition (3.9) and conclude that the collocation method (3.12)
is an IRK method.
3 Not every Runge‚ÄìKutta method originates in collocation
Let ŒΩ = 2,
c1 = 0 and c2 = 2
3. Therefore
q(t) = t

t ‚àí2
3

,
q1(t) = t ‚àí2
3,
q2(t) = t
and (3.13), (3.14) yield the IRK method with tableau
0
0
0
2
3
1
3
1
3
1
4
3
4
.
Given that every choice of collocation points corresponds to a unique collo-
cation method, we deduce that the IRK method (3.10) (again, with ŒΩ = 2,
c1 = 0 and c2 = 2
3) has no collocation counterpart. There is nothing wrong
in this, except that we cannot use the remainder of this section to elucidate
the order of (3.10).
3
Not only are collocation methods a special case of IRK but, as far as actual computa-
tion is concerned, to all intents and purposes the IRK formulation (3.9) is preferable.
The one advantage of (3.12) is that it lends itself very conveniently to analysis and

3.4
Collocation and IRK methods
45
obviates the need for cumbersome expansions. In a sense, collocation methods are the
true inheritors of the quadrature formulae.
Before we can reap the beneÔ¨Åts of the formulation (3.12), we need Ô¨Årst to present
(without proof) an important result on the estimation of error in a numerical solution.
It is frequently the case that we possess a smoothly diÔ¨Äerentiable ‚Äòcandidate solution‚Äô
v, say, to the ODE (3.1). Typically, such a solution can be produced by any of a myriad
of approximation or perturbation techniques, by extending (e.g. by interpolation) a
numerical solution from a grid to the whole interval of interest or by formulating
‚Äòcontinuous‚Äô numerical methods ‚Äì the collocation (3.12) is a case in point.
Given such a function v, we can calculate the defect
d(t) := v‚Ä≤(t) ‚àíf(t, v(t)).
Clearly, there is a connection between the magnitude of the defect and the error
v(t) ‚àíy(t): since d(t) ‚â°0 when v = y, the exact solution, we can expect a small
value of ‚à•d(t)‚à•to imply that the error is small. Such a connection is important, since,
unlike the error, we can evaluate the defect without knowing the exact solution y.
Matters are simple for linear equations. Thus, suppose that
y‚Ä≤ = Œõy,
y(t0) = y0.
(3.16)
We have d(t) = v‚Ä≤(t) ‚àíŒõv(t) and therefore the linear inhomogeneous ODE
v‚Ä≤ = Œõv + d(t),
t ‚â•t0,
v(t0) given.
The exact solution is provided by the familiar variation-of-constants formula,
v(t) = e(t‚àít0)Œõv0 +
 t
t0
e(t‚àíœÑ)Œõd(œÑ) dœÑ,
t ‚â•t0,
while the solution of (3.16) is, of course,
y(t) = e(t‚àít0)Œõy0,
t ‚â•t0.
We deduce that
v(t) ‚àíy(t) = e(t‚àít0)Œõ(v0 ‚àíy0) +
 t
t0
e(t‚àíœÑ)Œõd(œÑ) dœÑ,
t ‚â•t0;
thus the error can be expressed completely in terms of the ‚Äòobservables‚Äô v0 ‚àíy0
and d.
It is perhaps not very surprising that we can establish a connection between the
error and the defect for the linear equation (3.16) since, after all, its exact solution is
known. Remarkably, the variation-of-constants formula can be rendered, albeit in a
somewhat weaker form, in a nonlinear setting.
Theorem 3.6 (The Alekseev‚ÄìGr¬®obner lemma)
Let v be a smoothly diÔ¨Äeren-
tiable function that obeys the initial condition v(t0) = y0. Then
v(t) ‚àíy(t) =
 t
t0
Œ¶(t ‚àíœÑ, v(t ‚àíœÑ))d(œÑ) dœÑ,
t ‚â•t0,
(3.17)

46
Runge‚ÄìKutta methods
where Œ¶ is the matrix of partial derivatives of the solution of the ODE w‚Ä≤ = f(t, w),
w(œÑ) = v(œÑ), with respect to v(œÑ).
The matrix Œ¶ is, in general, unknown.
It can be estimated quite eÔ¨Éciently, a
practice which is useful in error control, but this ranges well beyond the scope of this
book. Fortunately, we do not need to know Œ¶ for the application that we have in
mind!
Theorem 3.7
Suppose that
 1
0
q(œÑ)œÑ j dœÑ = 0,
j = 0, 1, . . . , m ‚àí1,
(3.18)
for some m ‚àà{0, 1, . . . , ŒΩ}. (The polynomial q(t) = "ŒΩ
‚Ñì=1(t ‚àíc‚Ñì) has been deÔ¨Åned
already in the proof of Lemma 3.5.) Then the collocation method (3.12) is of order
ŒΩ + m.1
Proof
We express the error of the collocation method by using the Alekseev‚Äì
Gr¬®obner formula (3.17) (with t0 replaced by tn and, of course, the collocation solution
u playing the role of v; we recall that u(tn) = yn and hence the conditions of Theo-
rem 3.6 are satisÔ¨Åed). Thus
yn+1 ‚àíÀúy(tn+1) =
 tn+1
tn
Œ¶(tn+1 ‚àíœÑ, u(tn+1 ‚àíœÑ))d(œÑ) dœÑ.
(We recall that Àúy denotes the exact solution of the ODE for the initial condition
Àúy(tn) = yn.) We next replace the integral by the quadrature formula with respect to
the weight function œâ(t) ‚â°1, tn < t < tn+1, with the quadrature nodes tn + c1h, tn
+ c2h, . . . , tn + cŒΩh. Therefore
yn+1 ‚àíÀúy(tn+1) =
ŒΩ

j=1
bjŒ¶(tn+1, tn + cjh, u(tn + cjh))d(tn + cjh)
+ the error of the quadrature.
(3.19)
However, according to the deÔ¨Ånition (3.12) of collocation,
d(tn + cjh) = u‚Ä≤(tn + cjh) ‚àíf(tn + cjh, u(tn + cjh)) = 0,
j = 1, 2, . . . , ŒΩ.
According to Theorem 3.4, the order of quadrature with the weight function œâ(t) ‚â°1,
0 ‚â§t ‚â§1, with nodes c1, c2, . . . , cŒΩ, is m + ŒΩ. Therefore, translating linearly from
[0, 1] to [tn, tn+1] and paying heed to the length of the latter interval, tn+1 ‚àítn = h,
it follows that the error of the quadrature in (3.19) is O

hŒΩ+m+1
. We thus deduce
that yn+1 ‚àíÀúy(tn+1) = O

hŒΩ+m+1
and prove the theorem.2
1If m = 0 this means that (3.18) does not hold for any value of j and the theorem claims that the
underlying collocation method is then of order ŒΩ.
2Strictly speaking, we have only proved that the error is at least of order ŒΩ + m. However, if m
is the largest integer such that (3.18) holds, then it is trivial to prove that the order cannot exceed
ŒΩ + m; for example, apply the collocation to the equation y‚Ä≤ = (ŒΩ + m + 1)tŒΩ+m, y(0) = 0.

3.4
Collocation and IRK methods
47
Corollary
Let c1, c2, . . . , cŒΩ be the zeros of the polynomials ÀúPŒΩ ‚ààPŒΩ that are or-
thogonal with respect to the weight function œâ(t) ‚â°1, 0 ‚â§t ‚â§1. Then the underlying
collocation method (3.12) is of order 2ŒΩ.
Proof
The corollary is a straightforward consequence of the last theorem, since
the deÔ¨Ånition of orthogonality (3.4) implies in the present context that (3.18) is sat-
isÔ¨Åed by m = ŒΩ.
3 Gauss‚ÄìLegendre methods
The ŒΩ-stage order-2ŒΩ methods from the last
corollary are called Gauss‚ÄìLegendre (Runge‚ÄìKutta) methods. Note that, ac-
cording to Lemma 3.2, the nodes c1, c2, . . . , cŒΩ ‚àà(0, 1) are, as necessary for
collocation, distinct. The polynomials ÀúPŒΩ can be obtained explicitly, e.g. by
linearly transforming the more familiar Legendre polynomials PŒΩ, which are
orthogonal with respect to the weight function œâ(t) ‚â°1, ‚àí1 < t < 1. The
(monic) outcome is
ÀúPŒΩ(t) = (ŒΩ!)2
(2ŒΩ)!
ŒΩ

k=0
(‚àí1)ŒΩ‚àík
	ŒΩ
k

	ŒΩ + k
k

tk.
For ŒΩ = 1 we obtain ÀúP1(t) = t ‚àí1
2, hence c1 = 1
2. The method, which can be
written in a tableau form as
1
2
1
2
1 ,
is the familiar implicit midpoint rule (1.12).
In the case ŒΩ = 2 we have
ÀúP2(t) = t2 ‚àít + 1
6, therefore c1 = 1
2 ‚àí
‚àö
3
6 , c2 = 1
2 +
‚àö
3
6 . The formulae (3.13),
(3.14) lead to the two-stage fourth-order IRK method
1
2 ‚àí
‚àö
3
6
1
4
1
4 ‚àí
‚àö
3
6
1
2 +
‚àö
3
6
1
4 +
‚àö
3
6
1
4
1
2
1
2
.
The computation of nonlinear algebraic systems that originate in IRK meth-
ods with large ŒΩ is expensive but this is compensated by the increase in order.
It is impossible to lay down Ô¨Årm rules, and the exact point whereby the law
of diminishing returns compels us to choose a lower-order method changes
from equation to equation. It is fair to remark, however, that the three-stage
Gauss‚ÄìLegendre is probably the largest that is consistent with reasonable
implementation costs:
1
2 ‚àí
‚àö
15
10
5
36
2
9 ‚àí
‚àö
15
15
5
36 ‚àí
‚àö
15
30
1
2
5
36 +
‚àö
15
24
2
9
5
36 ‚àí
‚àö
15
24
1
2 +
‚àö
15
10
5
36 +
‚àö
15
30
2
9 +
‚àö
15
15
5
36
5
18
4
9
5
18
.
3

48
Runge‚ÄìKutta methods
Comments and bibliography
A standard text on numerical integration is Davis & Rabinowitz (1967), while highly readable
accounts of orthogonal polynomials can be found in Chihara (1978) and Rainville (1967). We
emphasize that although the theory of orthogonal polynomials is of tangential importance to
the subject matter of this volume, it is well worth studying for its intrinsic beauty as well as
its numerous applications.
Runge‚ÄìKutta methods have been known for a long time; Runge himself produced the main
idea in 1895.3 Their theoretical understanding, however, is much more recent and associated
mainly with the work of John Butcher. As is often the case with progress in computational
science, an improved theory has spawned new and better algorithms, these in turn have led to
further theoretical comprehension and so on. Lambert‚Äôs textbook (1991) presents a readable
account of Runge‚ÄìKutta methods and requires a relatively modest theoretical base. More
advanced accounts can be found in Butcher (1987, 2003) and Hairer et al. (1991).
Let us present in a nutshell the main idea behind the graph-theoretical approach of
Butcher to the derivation of the order of Runge‚ÄìKutta methods. The few examples of ex-
pansion in Sections 3.2 and 3.3 already demonstrate that the main diÔ¨Éculty rests in the need
to diÔ¨Äerentiate composite functions repeatedly. For expositional reasons only, we henceforth
restrict our attention to scalar, autonomous equations.4 Thus,
y‚Ä≤ = f(y)
‚áí
y‚Ä≤‚Ä≤ = fy(y)f(y)
‚áí
y‚Ä≤‚Ä≤‚Ä≤ = fyy(y)[f(y)]2 + [fy(y)]2f(y)
‚áí
y(iv) = fyyy(y)[f(y)]3 + 4fyy(y)fy(y)[f(y)]2 + [fy(y)]3f(y)
and so on. Although it cannot yet be seen from the above, the number of terms increases
exponentially. This should not deter us from exploring high-order methods, since there is a
great deal of redundancy in the order conditions (recall from the corollary to Theorem 2.7
that it is possible to attain order 2ŒΩ with a ŒΩ-stage method!), but we need an intelligent
mechanism to express the increasingly more complicated derivatives in a compact form.
Such a mechanism is provided by graph theory. BrieÔ¨Çy, a graph is a collection of vertices
and edges: it is usual to render the vertices pictorially as solid circles, while the edges are
the lines joining them.5 For example, two simple Ô¨Åve-vertex graphs are
t
t
t
t
t
@
@




@
@
and
t
t
t
t
t
@
@



@
@
.
The order of a graph is the number of vertices therein: both graphs above are of order 5. We
say that a graph is a tree if each two vertices are joined by a single path of edges. Thus the
second graph is a tree, whereas the Ô¨Årst is not. Finally, in a tree we single out one vertex and
call it the root. This imposes a partial ordering on a rooted tree: the root is the lowest, its
3The monograph of Collatz (1966), and in particular its copious footnotes, is an excellent source
on the life of many past heroes of numerical analysis.
4This restriction leads to loss of generality. A comprehensive order analysis should be done for
systems of equations.
5You will have an opportunity to learn much more about graphs and their role in numerical
calculations in Chapter 11.

Comments and bibliography
49
children (i.e., all vertices that are joined to the root by a single edge) are next in line, then
its children‚Äôs children and so on. We adopt in our pictures the (obvious) convention that
the root is always at the bottom. (Strangely, computer scientists often follow an opposite
convention and place the root at the top.) Two rooted trees of the same order are said to be
equivalent if each exhibits the same pattern of paths from its ‚Äòtop‚Äô to its root ‚Äì the following
picture of three equivalent rooted trees should clarify this concept: the graphs
t
t
t
t
t
@
@

t
t
t
t
t
@
@


t
t
t
t
t
@
@


are all equivalent. We keep just one representative of each equivalence class and, hopefully
without much confusion, refer to members of this reduced set as ‚Äòrooted trees‚Äô. We denote
by Œ≥(ÀÜt ) the product of the order of the tree ÀÜt and the orders of all possible trees that occur
upon consecutive removal of the roots of ÀÜt. For example, for the above tree we have
t
t
t
t
t
@
@

‚áí
t
t
t
t
d
@
@ 

‚áí
d
t
d
d
d
@@ 
(an open circle denotes a vertex that has been removed) and Œ≥(ÀÜt ) = 5 √ó (2 √ó 1 √ó 1) √ó 1 = 10.
As we have seen above, the derivatives of y can be expressed as linear combinations of
products of derivatives of f.
The latter are called elementary diÔ¨Äerentials and they can
be assigned to rooted trees according to the following rule: to each vertex of a rooted tree
corresponds a derivative fyy...y, where the suÔ¨Éx occurs the same number of times as the
number of children of the vertex, and the elementary diÔ¨Äerential corresponding to the whole
tree is a product of these terms. For example,
t
t
t
t
t
t
t
t
@
@
@
@




‚áí
f
f
fyy
fyyy
fy
fy
f
f
@
@

@

‚áí
fyyyfyyf 2
yf 4.
To every rooted tree there corresponds an order condition, which we can express in terms
of the RK matrix A and the RK weights b. This is best demonstrated by an example. We
assign an index to every vertex of a tree ÀÜt, e.g. the tree
l
i
l
j
l
‚Ñì
l
k
@


50
Runge‚ÄìKutta methods
corresponds to the condition
ŒΩ

‚Ñì,j,i,k=1
b‚Ñìa‚Ñì,ja‚Ñì,kaj,i =
1
Œ≥(ÀÜt ) = 1
8.
The general rule is clear ‚Äì we multiply b‚Ñìby all components aq,r, where q and r are the
indices of a parent and a child respectively, sum up for all indices ranging in {1, 2, . . . , ŒΩ}
and equate to the reciprocal of Œ≥(ÀÜt ). The main result linking rooted trees and Runge‚ÄìKutta
methods is that the scheme (3.9) (or, for that matter, (3.5)) is of order p if and only if the
above order conditions are satisÔ¨Åed for all rooted trees of order less than or equal to p.
The graph-theoretical technique, often formalized as the theory of B-series, is the stan-
dard tool in the construction of Runge‚ÄìKutta schemes and in the investigation of their
properties. It is, in particular, of great importance in the investigation of the behaviour of
structure-preserving Runge‚ÄìKutta methods that we will encounter in Chapter 5.
By one of these quirks of fate that make the study of mathematics so entrancing, the
graph-theoretical interpretation of Runge‚ÄìKutta methods has recently acquired an unex-
pected application at an altogether diÔ¨Äerent corner of the mathematical universe. It turns
out that the abstract structure underlying this interpretation is a Hopf algebra of a special
kind, which can be applied in mathematical physics to gain valuable insight into certain
questions in quantum mechanics.
The alternative approach of collocation is less well known, although it is presented in
more recent texts, e.g. Hairer et al. (1991). Of course, only a subset of all Runge‚ÄìKutta
methods are equivalent to collocation and the technique is of little value for ERK schemes.
It is, however, possible to generalize the concept of collocation to cater for all Runge‚ÄìKutta
methods.
Butcher, J.C. (1987), The Numerical Analysis of Ordinary DiÔ¨Äerential Equations, John Wi-
ley, Chichester.
Butcher, J.C. (2003), Numerical Methods for Ordinary DiÔ¨Äerential Equations, John Wiley,
Chichester.
Chihara, T.S. (1978), An Introduction to Orthogonal Polynomials, Gordon and Breach, New
York.
Collatz, L. (1966), The Numerical Treatment of DiÔ¨Äerential Equations (3rd edn), Springer-
Verlag, Berlin.
Davis, P.J. and Rabinowitz, P. (1967), Numerical Integration, Blaisdell, London.
Hairer, E., N√∏rsett, S.P. and Wanner, G. (1991), Solving Ordinary DiÔ¨Äerential Equations I:
NonstiÔ¨ÄProblems (2nd edn), Springer-Verlag, Berlin.
Lambert, J.D. (1991), Numerical Methods for Ordinary DiÔ¨Äerential Systems, Wiley, London.
Rainville, E.D. (1967), Special Functions, Macmillan, New York.
Exercises
3.1
Find the order of the following quadrature formulae:
a
 1
0
f(œÑ) dœÑ = 1
6f(0) + 2
3f( 1
2) + 1
6f(1)
(the Simpson rule);

Exercises
51
b
 1
0
f(œÑ) dœÑ = 1
8f(0) + 3
8f( 1
3) + 3
8f( 2
3) + 1
8f(1)
(the three-eighths rule);
c
 1
0
f(œÑ) dœÑ = 2
3f( 1
4) ‚àí1
3f( 1
2) + 2
3f( 3
4);
d
 ‚àû
0
f(œÑ)e‚àíœÑ dœÑ = 5
3f(1) ‚àí3
2f(2) + f(3) ‚àí1
6f(4).
3.2
Let us deÔ¨Åne
Tn(cos Œ∏) := cos nŒ∏,
n = 0, 1, 2, . . . ,
‚àíœÄ ‚â§Œ∏ ‚â§œÄ.
a Show that each Tn is a polynomial of degree n and that the Tn satisfy the
three‚Äìterm recurrence relation
Tn+1(t) = 2t Tn(t) ‚àíTn‚àí1(t),
n = 1, 2, . . .
b Prove that Tn is an nth orthogonal polynomial with respect to the weight
function œâ(t) = (1 ‚àít)‚àí1/2, ‚àí1 < t < 1.
c Find the explicit values of the zeros of Tn, thereby verifying the statement
of Lemma 3.2, namely that all the zeros of an orthogonal polynomial reside
in the open support of the weight function.
d Find b1, b2, c1, c2 such that the order of the quadrature
 1
‚àí1
f(œÑ)
dœÑ
‚àö
1 ‚àíœÑ 2 ‚âàb1f(c1) + b2f(c2)
is four.
(The Tns are known as Chebyshev polynomials and they have many appli-
cations in mathematical analysis. We will encounter them again in Chap-
ter 10.)
3.3
Construct the Gaussian quadrature formulae for the weight function œâ(t) ‚â°
1, 0 ‚â§t ‚â§1, of orders two, four and six.
3.4
Restricting your attention to scalar autonomous equations y ‚Ä≤ = f(y), prove
that the ERK method with tableau
0
1
2
1
2
1
2
0
1
2
1
0
0
1
1
6
1
3
1
3
1
6
is of order 4.

52
Runge‚ÄìKutta methods
3.5
Suppose that a ŒΩ-stage ERK method of order ŒΩ is applied to the linear scalar
equation y‚Ä≤ = Œªy. Prove that
yn =
 ŒΩ

k=0
1
k!(hŒª)k
n
y0,
n = 0, 1, . . .
3.6
Determine all choices of b, c and A such that the two-stage IRK method
c
A
b‚ä§
is of order p ‚â•3.
3.7
Write the theta method, (1.13), as a Runge‚ÄìKutta method.
3.8
Derive the three-stage Runge‚ÄìKutta method that corresponds to the collo-
cation points c1 = 1
4, c2 = 1
2, c3 = 3
4 and determine its order.
3.9
Let Œ∫ ‚ààR\{0} be a given constant. We choose collocation nodes c1, c2, . . . , cŒΩ
as zeros of the polynomial ÀúPŒΩ +Œ∫ ÀúPŒΩ‚àí1. ( ÀúPm is the mth-degree Legendre poly-
nomial, shifted to the interval (0, 1). In other words, the ÀúPm are orthogonal
there with respect to the weight function œâ(t) ‚â°1.)
a Prove that the collocation method (3.12) is of order 2ŒΩ ‚àí1.
b Let Œ∫ = ‚àí1 and Ô¨Ånd explicitly the corresponding IRK method for ŒΩ = 2.

4
StiÔ¨Äequations
4.1
What are stiÔ¨ÄODEs?
Let us try to solve the seemingly innocent linear ODE
y‚Ä≤ = Œõy,
y(0) = y0,
where
Œõ =
 ‚àí100
1
0
‚àí1
10

,
(4.1)
by Euler‚Äôs method (1.4). We obtain
y1 = y0 + hŒõy0 = (I + hŒõ)y0,
y2 = y1 + hŒõy1 = (I + hŒõ)y1 = (I + hŒõ)2y0
(where I is the identity matrix) and, in general, it is easy to prove by elementary
induction that
yn = (I + hŒõ)ny0,
n = 0, 1, 2, . . .
(4.2)
Since the spectral factorization (A.1.5.4) of Œõ is
Œõ = V DV ‚àí1,
where
V =
 1
1
0
999
10

and
D =
 ‚àí100
0
0
‚àí1
10

,
we deduce that the exact solution of (4.1) is
y(t) = etŒõ = V etDV ‚àí1y0,
t ‚â•0,
where
etD =
 e‚àí100t
0
0
e‚àít/10

.
In other words, there exist two vectors, x1 and x2, say, dependent on y0 but not on
t, such that
y(t) = e‚àí100tx1 + e‚àít/10x2,
t ‚â•0.
(4.3)
The function g(t) = e‚àí100t decays exceedingly fast: g
 1
10

‚âà4.54 √ó 10‚àí5 and g(1) ‚âà
3.72 √ó 10‚àí44, while the decay of e‚àít/10 is a thousandfold more sedate. Thus, even for
small t > 0 the contribution of x1 is nil to all intents and purposes and y(t) ‚âàe‚àít/10x2.
What about the Euler solution {yn}‚àû
n=0, though? It follows from (4.2) that
yn = V (I + hD)nV ‚àí1y0,
n = 0, 1, . . .
and, since
(I + hD)n =
 (1 ‚àí100h)n
0
0
(1 ‚àí1
10h)n

,
53

54
StiÔ¨Äequations
0
5
10
15
20
25
4
6
8
10
12
14
16
18
20
 
Figure 4.1
The logarithm of the Euclidean norm ‚à•yn‚à•of the Euler steps, as
applied to the equation (4.1) with h =
1
10 and an initial condition identical to the
second (i.e., the ‚Äòstable‚Äô) eigenvector. The divergence is thus entirely due to roundoÔ¨Ä
error!
it follows that
yn = (1 ‚àí100h)nx1 + (1 ‚àí1
10h)nx2,
n = 0, 1, . . .
(4.4)
(it is left to the reader to prove in Exercise 4.1 that the constant vectors x1 and x2
are the same in (4.3) and (4.4)). Suppose that h >
1
50. Then |1‚àí100h| > 1 and it is a
consequence of (4.4) that, for suÔ¨Éciently large n, the Euler iterates grow geometrically
in magnitude, in contrast with the asymptotic behaviour of the true solution.
Suppose that we choose an initial condition identical to an eigenvector correspond-
ing to the eigenvalue ‚àí0.1, for example
y0 =

1
999
10

.
Then, in exact arithmetic, x1 = 0, x2 = y0 and yn =

1 ‚àí1
10h
n y0, n = 0, 1, . . . ;
the latter converges to 0 as n ‚Üí‚àûfor all reasonable values of h > 0 (speciÔ¨Åcally,
for h < 20). Hence, we might hope that all will be well with the Euler method. Not
so! Real computers produce roundoÔ¨Äerrors and, unless h <
1
50, sooner or later these
are bound to attribute a nonzero contribution to an eigenvector corresponding to the
eigenvalue ‚àí100. As soon as this occurs, the unstable component grows geometrically,
as (1 ‚àí100h)n, and rapidly overwhelms the true solution.
Figure 4.1 displays ln ‚à•yn‚à•, n = 0, 1, . . . , 25, with the above initial condition and
the time step h =
1
10. The calculation was performed on a computer equipped with

4.1
What are stiÔ¨ÄODEs?
55
the ubiquitous IEEE arithmetic,1 which is correct (in a single algebraic operation) to
about 15 decimal digits. The norm of the Ô¨Årst 17 steps decreases at the right pace,
dictated by

1 ‚àí1
10h
n =
 99
100
n. However, everything then breaks down and, after
just two steps, the norm increases geometrically, as |1 ‚àí100h|n = 9n. The reader is
welcome to check that the slope of the curve in Fig. 4.1 is indeed ln 99
100 ‚âà‚àí0.0101
initially but becomes ln 9 ‚âà2.1972 in the second, unstable, regime.
The choice of y0 as a ‚Äòstable‚Äô eigenvector is not contrived. Faced with an equation
like (4.1) (with an arbitrary initial condition) we are likely to employ a small step size
in the initial transient regime, in which the contribution of the ‚Äòunstable‚Äô eigenvector
is still signiÔ¨Åcant. However, as soon as this has disappeared and the solution is com-
pletely described by the ‚Äòstable‚Äô eigenvector, it is tempting to increase h. This must be
resisted: like a malign version of the Cheshire cat, the rogue eigenvector might seem
to have disappeared, but its hideous grin stays and is bound to thwart our endeavours.
It is important to understand that this behaviour has nothing to do with the local
error of the numerical method; the step size is depressed not by accuracy considerations
(to which we should be always willing to pay heed) but by instability.
Not every numerical method displays a similar breakdown in stability. Thus, solv-
ing (4.1) with the trapezoidal rule (1.9), we obtain
y1 =
	I + 1
2hŒõ
I ‚àí1
2hŒõ

y0,
y2 =
	I + 1
2hŒõ
I ‚àí1
2hŒõ

y1 =
	I + 1
2hŒõ
I ‚àí1
2hŒõ

2
y0,
noting that since (I ‚àí1
2hŒõ)‚àí1 and

I + 1
2hŒõ

commute the order of multiplication
does not matter, and, in general,
yn =
	I + 1
2hŒõ
I ‚àí1
2hŒõ

n
y0,
n = 0, 1, . . .
(4.5)
Substituting for Œõ from (4.1) and factorizing, we deduce, in the same way as for (4.4),
that
yn =
	1 ‚àí50h
1 + 50h

n
x1 +
	1 ‚àí1
20h
1 + 1
20h

n
x2,
n = 0, 1, . . .
Thus, since

1 ‚àí50h
1 + 50h
 ,

1 ‚àí1
20h
1 + 1
20h
 < 1
for every h > 0, we deduce that limn‚Üí‚àûyn = 0. This recovers the correct asymptotic
behaviour of the ODE (4.1) (cf. (4.3)) regardless of the size of h.
In other words, the trapezoidal rule does not require any restriction in the step
size to avoid instability. We hasten to say that this does not mean, of course, that
any h is suitable. It is necessary to choose h > 0 small enough to ensure that the
local error is within reasonable bounds and the exact solution is adequately approxi-
mated. However, there is no need to decrease h to a minuscule size to prevent rogue
components of the solution growing out of control.
1The current standard of computer arithmetic on workstations and personal computers.

56
StiÔ¨Äequations
The equation (4.1) is an example of a stiÔ¨ÄODE. Several attempts at a rigorous
deÔ¨Ånition of stiÔ¨Äness appear in the literature, but it is perhaps more informative to
adopt an operative (and slightly vague) designation. Thus, we say that an ODE system
y‚Ä≤ = f(t, y),
t ‚â•t0,
y(t0) = y0,
(4.6)
is stiÔ¨Äif its numerical solution by some methods requires (perhaps in a portion of the
solution interval) a signiÔ¨Åcant depression of the step size to avoid instability. Needless
to say this is not a proper mathematical deÔ¨Ånition, but then we are not aiming to
prove theorems of the sort ‚Äòif a system is stiÔ¨Äthen . . . ‚Äô. The main importance of
the above concept is in helping us to choose and implement numerical methods ‚Äì a
procedure that, anyway, is far from an exact science!
We have already seen the most important mechanism generating stiÔ¨Äness, namely,
that modes with vastly diÔ¨Äerent scales and ‚Äòlifetimes‚Äô are present in the solution. It
is sometimes the practice to designate the quotient of the largest and the smallest
(in modulus) eigenvalues of a linear system (and, for a general system (4.6), the
eigenvalues of the Jacobian matrix) as the stiÔ¨Äness ratio. The stiÔ¨Äness ratio of (4.1)
is 103. This concept is helpful in elucidating the behaviour of many ODE systems
and, in general, it is a safe bet that if (4.6) has a large stiÔ¨Äness ratio then it is stiÔ¨Ä.
Having said this, it is also valuable to stress the shortcomings of linear analysis and
emphasize that the stiÔ¨Äness ratio might fail to elucidate the behaviour of a nonlinear
ODE system.
A large proportion of the ODEs that occur in practice are stiÔ¨Ä. Whenever equa-
tions model several processes with vastly diÔ¨Äerent rates of evolution, stiÔ¨Äness is not far
away. For example, the diÔ¨Äerential equations of chemical kinetics describe reactions
that often proceed on very diÔ¨Äerent time scales (think of the diÔ¨Äerence in time scales
of corrosion and explosion); a stiÔ¨Äness ratio of 1017 is quite typical. Other popular
sources of stiÔ¨Äness are control theory, reactor kinetics, weather prediction, mathemat-
ical biology and electronics: they all abound with phenomena that display variation
at signiÔ¨Åcantly diÔ¨Äerent time scales. The world record, to the author‚Äôs knowledge,
is held, unsurprisingly perhaps, by the equations that describe the cosmological Big
Bang: the stiÔ¨Äness ratio is 1031.
One of the main sources of stiÔ¨Äequations is numerical analysis itself. As we will
see in Chapter 16, parabolic partial diÔ¨Äerential equations are often approximated by
large systems of stiÔ¨ÄODEs.
4.2
The linear stability domain and A-stability
Let us suppose that a given numerical method is applied with a constant step size
h > 0 to the scalar linear equation
y‚Ä≤ = Œªy,
t ‚â•0,
y(0) = 1,
(4.7)
where Œª ‚ààC. The exact solution of (4.7) is, of course, y(t) = eŒªt, hence limt‚Üí‚àûy(t) =
0 if and only if Re Œª < 0. We say that the linear stability domain D of the underlying
numerical method is the set of all numbers hŒª ‚ààC such that limn‚Üí‚àûyn = 0. In other

4.2
The linear stability domain and A-stability
57
words, D is the set of all hŒª for which the correct asymptotic behaviour of (4.7) is
recovered, provided that the latter equation is stable.2
Let us commence with Euler‚Äôs method (1.4).
We obtain the solution sequence
identically to the derivation of (4.2),
yn = (1 + hŒª)n,
n = 0, 1, . . .
(4.8)
Therefore {yn}n=0,1,... is a geometric sequence and limn‚Üí‚àûyn = 0 if and only if
|1 + hŒª| < 1. We thus conclude that
DEuler = {z ‚ààC : |1 + z| < 1}
is the interior of a complex disc of unit radius, centred at z = ‚àí1 (see Fig. 4.2).
Before we proceed any further, let us ponder brieÔ¨Çy the rationale behind this
sudden interest in a humble scalar linear equation. After all, we do not need numerical
analysis to solve (4.7)! However, for Euler‚Äôs method and for all other methods that
have been the theme of Chapters 1‚Äì3 we can extrapolate from scalar linear equations to
linear ODE systems. Thus, suppose that we solve (4.1) with an arbitrary d √ó d matrix
Œõ. The solution sequence is given by (4.2). Suppose that Œõ has a full set of eigenvectors
and hence the spectral factorization Œõ = V DV ‚àí1, where V is a nonsingular matrix of
eigenvectors and D = diag (Œª1, Œª2, . . . , Œªd) contains the eigenvalues of Œõ. Exactly as
in (4.4), we can prove that there exist vectors x1, x2, . . . , xd ‚ààCd, dependent only on
y0, not on n, such that
yn =
d

k=1
(1 + hŒªk)n xk,
n = 0, 1, . . .
(4.9)
Let us suppose that the exact solution of the linear system is asymptotically stable.
This happens if and only if Re Œªk < 0 for all k = 1, 2, . . . , d. To mimic this behaviour
with Euler‚Äôs method, we deduce from (4.9) that the step size h > 0 must be such that
|1 + hŒªk| < 1, k = 1, 2, . . . , d: all the products hŒª1, hŒª2, . . . , hŒªd must lie in DEuler.
This means in practice that the step size is determined by the stiÔ¨Äest component of
the system!
The restriction to systems with a full set of eigenvectors is made for ease of ex-
position only. In general, we may use a Jordan factorization (A.1.5.6) in place of a
spectral factorization; see Exercise 4.2 for a simple example. Moreover, the analysis
can be extended easily to inhomogeneous systems y‚Ä≤ = Œõy + a, and this is illustrated
by Exercise 4.3.
The importance of D ranges well beyond linear systems. Given a nonlinear ODE
system
y‚Ä≤ = f(t, y),
t ‚â•t0,
y(t0) = y0,
where f is diÔ¨Äerentiable with respect to y, it is usual to require that in the nth step
hŒªn,1, hŒªn,2, . . . , hŒªn,d ‚ààD,
2Our interest in (4.7) with Re Œª > 0 is limited, since the exact solution rapidly becomes very large.
However, for nonlinear equations there is an intense interest, which we will not pursue in this volume,
in those equations for which a counterpart of Œª, namely the Liapunov exponent, is positive.

58
StiÔ¨Äequations
ÀÜr1/0(z) = 1 + z
ÀÜr1/1(z) = 1 + 1
2z
1 ‚àí1
2z
ÀÜr3/0(z) = 1 + z + 1
2z2 + 1
6z3
ÀÜr1/2(z) =
1 + 1
3z
1 ‚àí2
3z + 1
6z2
Figure 4.2
Stability domains (the unshaded areas) for various rational approxi-
mations. Note that ÀÜr1/0 corresponds to the Euler method, while ÀÜr1/1 corresponds
both to the trapezoidal rule and the implicit midpoint rule. The ÀÜrŒ±/Œ≤ notation is
introduced in Section 4.3.
where the complex numbers Œªn,1, Œªn,2, . . . , Œªn,d are the eigenvalues of the Jacobian
matrix Jn := ‚àÇf(tn, yn)/‚àÇy. This is based on the assumption that the local behaviour
of the ODE is modelled well by the variational equation y‚Ä≤ = yn + Jn(y ‚àíyn). We
hasten to emphasize that this practice is far from exact. Naive translation of any
linear theory to a nonlinear setting can be dangerous and the correct approach is to
embrace a nonlinear framework from the outset. Although in its full generality this
ranges well beyond the material of this book, we provide a few pointers to modern
nonlinear stability theory in Chapter 5.
Let us continue our investigation of linear stability domains. Replacing Œõ by Œª
from (4.7) in (4.5) and bearing in mind that y0 = 1, we obtain
yn =
	1 + 1
2hŒª
1 ‚àí1
2hŒª

n
,
n = 0, 1, . . .
(4.10)

4.3
A-stability of Runge‚ÄìKutta methods
59
Again, {yn}n=0,1,... is a geometric sequence. Therefore, we obtain for the linear sta-
bility domain in the case of the trapezoidal rule,
DTR =
#
z ‚ààC :

1 + 1
2z
1 ‚àí1
2z
 < 1
$
.
It is trivial to verify that the inequality within the braces is identical to Re z < 0.
In other words, the trapezoidal rule mimics the asymptotic stability of linear ODE
systems without any need to decrease the step size, a property that we have already
noticed in a special example in Section 4.1.
The latter feature is of suÔ¨Écient importance to deserve a name of its own. We say
that a method is A-stable if
C‚àí:= {z ‚ààC : Re z < 0} ‚äÜD.
In other words, whenever a method is A-stable, we can choose the step size h (at least,
for linear systems) on accuracy considerations only, without paying heed to stability
constraints.
The trapezoidal rule is A-stable, whilst Euler‚Äôs method is not. As is evident from
Fig. 4.2, the graph labelled ÀÜr1/2(z) ‚Äì but not the one labelled ÀÜr3/0(z) ‚Äì corresponds to
an A-stable method. It is left to the reader to ascertain in Exercise 4.4 that the theta
method (1.13) is A-stable if and only if 0 ‚â§Œ∏ ‚â§1
2.
4.3
A-stability of Runge‚ÄìKutta methods
Applying the Runge‚ÄìKutta method (3.9) to the linear equation (4.7), we obtain
Œæj = yn + hŒª
ŒΩ

i=1
aj,iŒæi,
j = 1, 2, . . . , ŒΩ.
Denote
Œæ :=
‚é°
‚é¢‚é¢‚é¢‚é£
Œæ1
Œæ2
...
ŒæŒΩ
‚é§
‚é•‚é•‚é•‚é¶,
1 :=
‚é°
‚é¢‚é¢‚é¢‚é£
1
1
...
1
‚é§
‚é•‚é•‚é•‚é¶‚ààRŒΩ;
then Œæ = 1yn + hŒªAŒæ and the exact solution of this linear algebraic system is
Œæ = (I ‚àíhŒªA)‚àí11yn.
Therefore, assuming that I ‚àíhŒªA is nonsingular,
yn+1 = yn + hŒª
ŒΩ

j=1
bjŒæj =
%
1 + hŒªb‚ä§(I ‚àíhŒªA)‚àí11
&
yn,
n = 0, 1, . . .
(4.11)
We denote by PŒ±/Œ≤ the set of all rational functions ÀÜp/ÀÜq, where ÀÜp ‚ààPŒ± and ÀÜq ‚ààPŒ≤.

60
StiÔ¨Äequations
Lemma 4.1
For every Runge‚ÄìKutta method (3.9) there exists r ‚ààPŒΩ/ŒΩ such that
yn = [r(hŒª)]n,
n = 0, 1, . . .
(4.12)
Moreover, if the Runge‚ÄìKutta method is explicit then r ‚ààPŒΩ.
Proof
It follows at once from (4.11) that (4.12) is valid with
r(z) := 1 + zb‚ä§(I ‚àízA)‚àí11,
z ‚ààC,
(4.13)
and it remains to verify that r is indeed a rational function (a polynomial for an
explicit scheme) of the stipulated type.
We represent the inverse of I ‚àízA using a familiar formula from linear algebra,
(I ‚àízA)‚àí1 = adj(I ‚àízA)
det(I ‚àízA),
where adj C is the adjugate of the ŒΩ √ó ŒΩ matrix C: the (i, j)th entry of the adjugate
(also known as the ‚Äòadjunct‚Äô and abbreviated in the same way) is the determinant of
the (j, i)th principal minor, multiplied by (‚àí1)i+j. Since each entry of I ‚àízA is linear
in z, we deduce that each element of adj(I ‚àízA), being (up to a sign) a determinant
of a (ŒΩ ‚àí1) √ó (ŒΩ ‚àí1) matrix, is in PŒΩ‚àí1. We thus conclude that
b‚ä§adj(I ‚àízA)1 ‚ààPŒΩ‚àí1,
therefore det(I ‚àízA) ‚ààPŒΩ implies r ‚ààPŒΩ/ŒΩ.
Finally, if the method is explicit then A is strictly lower triangular and I ‚àízA is,
regardless of z ‚ààC, a lower triangular matrix with ones along the diagonal. Therefore
det(I ‚àízA) ‚â°1 and r is a polynomial.
Lemma 4.2
Suppose that an application of a numerical method to the linear equa-
tion (4.7) produces a geometric solution sequence, yn = [r(hŒª)]n, n = 0, 1, . . . , where
r is an arbitrary function. Then
D = {z ‚ààC : |r(z)| < 1}.
(4.14)
Proof
This follows at once from the deÔ¨Ånition of the set D.
Corollary
No explicit Runge‚ÄìKutta (ERK) method (3.5) can be A-stable.
Proof
Given an ERK method, Lemma 4.1 states that the function r is a poly-
nomial and (4.13) implies that r(0) = 1.
No polynomial, except for the constant
function r(z) ‚â°c ‚àà(‚àí1, 1), may be uniformly bounded by the value unity in C‚àí, and
this excludes A-stability.
For both Euler‚Äôs method and the trapezoidal rule we have observed already that
the solution sequence obeys the conditions of Lemma 4.2. This is hardly surprising,
since both methods can be written in a Runge‚ÄìKutta formalism.

4.3
A-stability of Runge‚ÄìKutta methods
61
3 The function r for speciÔ¨Åc IRK schemes
Let us consider the methods
0
1
4
‚àí1
4
2
3
1
4
5
12
1
4
3
4
and
1
3
5
12
‚àí1
12
1
3
4
1
4
3
4
1
4
.
We have already encountered both in Chapter 3: the Ô¨Årst is (3.10), whereas
the second corresponds to collocation at c1 = 1
3, c2 = 1.
Substitution into (4.13) conÔ¨Årms that the function r is identical for the two
methods:
r(z) =
1 + 1
3z
1 ‚àí2
3z + 1
6z2 .
(4.15)
To check A-stability we employ (4.14). Representing z ‚ààC in polar coordi-
nates, z = œÅeiŒ∏, where œÅ > 0 and |Œ∏+œÄ| < 1
2œÄ, we query whether |r(œÅeiŒ∏)| < 1.
This would be equivalent to
1 + 1
3œÅeiŒ∏2 <
1 ‚àí2
3œÅeiŒ∏ + 1
6œÅ2e2iŒ∏2
and hence to
1 + 2
3œÅ cos Œ∏ + 1
9œÅ2 < 1 ‚àí4
3œÅ cos Œ∏ + œÅ2  1
3 cos 2Œ∏ + 4
9

‚àí2
9œÅ3 cos Œ∏ + 1
36œÅ4.
Rearranging terms, the condition for œÅeiŒ∏ ‚ààD becomes
2œÅ

1 + 1
9œÅ2
cos Œ∏ < 1
3œÅ2(1 + cos 2Œ∏) + 1
36œÅ4 = 2
3œÅ2 cos2 Œ∏ + 1
36œÅ4,
and this is obeyed for all z ‚ààC‚àísince cos Œ∏ < 0 for all such z. Both methods
are therefore A-stable.
A similar analysis can be applied to the Gauss‚ÄìLegendre methods of Sec-
tion 3.4, but the calculations become increasingly labour intensive for large
values of ŒΩ. Fortunately, we are just about to identify a few shortcuts that
render this job signiÔ¨Åcantly easier.
3
Our Ô¨Årst observation is that there is no need to check every z ‚ààC‚àíto verify that
a given rational function r originates in an A-stable method (such an r is called A-
acceptable).
Lemma 4.3
Let r be an arbitrary rational function that is not a constant. Then
|r(z)| < 1 for all z ‚ààC‚àíif and only if all the poles of r have positive real parts and
|r(it)| ‚â§1 for all t ‚ààR.
Proof
If |r(z)| < 1 for all z ‚ààC‚àíthen, by continuity, |r(z)| ‚â§1 for all z ‚ààcl C‚àí.
In particular, r is not allowed to have poles in the closed left half-plane and |r(it)| ‚â§1,
t ‚ààR.
To prove the converse we note that, provided its poles reside to the right of iR,
the rational function r is analytic in the closed set cl C‚àí. Therefore, and since r is

62
StiÔ¨Äequations
not constant, it attains its maximum along the boundary. In other words |r(it)| ‚â§1,
t ‚ààR, implies |r(z)| < 1, z ‚ààC‚àí, and the proof is complete.
The beneÔ¨Åts of the lemma are apparent in the case of the function (4.15): the poles
reside at 2 ¬± i
‚àö
2, hence at the open right half-plane. Moreover |r(it)| ‚â§1, t ‚ààR, is
equivalent to
1 + 1
3it
2 ‚â§
1 ‚àí2
3it ‚àí1
6t22 ,
t ‚ààR,
and hence to
1 + 1
9t2 ‚â§1 + 1
9t2 + 1
36t4,
t ‚ààR.
The gain is even more spectacular for the two-stage Gauss‚ÄìLegendre method, since
in this case
r(z) = 1 + 1
2z + 1
12z2
1 ‚àí1
2z + 1
12z2
(although it is possible to evaluate this from the RK tableau in Section 3.4, a consid-
erably easier derivation follows from the proof of the corollary to Theorem 4.6). Since
the poles 3 ¬± i
‚àö
3 are in the open right half-plane and |r(it)| ‚â°1, t ‚ààR, the method
is A-stable.
Our next result focuses on the kind of rational functions r likely to feature in
(4.12).
Lemma 4.4
Suppose that the solution sequence {yn}‚àû
n=0, which is produced by
applying a method of order p to the linear equation (4.7) with a constant step size,
obeys (4.12). Then necessarily
r(z) = ez + O

zp+1
,
z ‚Üí0.
(4.16)
Proof
Since yn+1 = r(hŒª)yn and the exact solution, subject to the initial con-
dition y(tn) = yn, is ehŒªyn, the relation (4.16) follows from the deÔ¨Ånition of order.
We say that a function r that obeys (4.16) is of order p. This should not be confused
with the order of a numerical method: it is easy to construct pth-order methods with
a function r whose order exceeds p, in other words, methods that exhibit superior
order when applied to linear equations.
The lemma narrows down considerably the Ô¨Åeld of rational functions r that might
occur in A-stability analysis. The most important functions exploit all available de-
grees of freedom to increase the order.
Theorem 4.5
Given any integers Œ±, Œ≤ ‚â•0, there exists a unique function ÀÜrŒ±/Œ≤ ‚àà
PŒ±/Œ≤ such that
ÀÜrŒ±/Œ≤ = ÀÜpŒ±/Œ≤
ÀÜqŒ±/Œ≤
,
ÀÜqŒ±/Œ≤(0) = 1

4.4
A-stability of multistep methods
63
and ÀÜrŒ±/Œ≤ is of order Œ± + Œ≤. The explicit forms of the numerator and the denominator
are respectively
ÀÜpŒ±/Œ≤(z) =
Œ±

k=0
	Œ±
k

(Œ± + Œ≤ ‚àík)!
(Œ± + Œ≤)!
zk,
ÀÜqŒ±/Œ≤(z) =
Œ≤

k=0
	Œ≤
k

(Œ± + Œ≤ ‚àík)!
(Œ± + Œ≤)!
(‚àíz)k = ÀÜpŒ≤/Œ±(‚àíz).
(4.17)
Moreover ÀÜrŒ±/Œ≤ is (up to a rescaling of the numerator and the denominator by a non-
zero multiplicative constant) the only member of PŒ±/Œ≤ of order Œ± + Œ≤, and no function
in PŒ±/Œ≤ may exceed this order.
The functions ÀÜrŒ±/Œ≤ are called Pad¬¥e approximations to the exponential. Most of
the functions r that have been encountered so far are of this kind; thus (compare with
(4.8), (4.10) and (4.15))
ÀÜr1/0(z) = 1 + z,
ÀÜr1/1 = 1 + 1
2z
1 ‚àí1
2z ,
ÀÜr1/2(z) =
1 + 1
3z
1 ‚àí2
3z + 1
6z2 .
Pad¬¥e approximations can be classiÔ¨Åed according to whether they are A-acceptable.
Obviously, we need Œ± ‚â§Œ≤ otherwise ÀÜrŒ±/Œ≤ cannot be bounded in C‚àí. Surprisingly, the
latter condition is not suÔ¨Écient. It is not diÔ¨Écult to prove, for example, that ÀÜr0/3 is
not A-acceptable!
Theorem 4.6 (The Wanner‚ÄìHairer‚ÄìN√∏rsett theorem)
The Pad¬¥e approxima-
tion ÀÜrŒ±/Œ≤ is A-acceptable if and only if Œ± ‚â§Œ≤ ‚â§Œ± + 2.
Corollary
The Gauss‚ÄìLegendre IRK methods are A-stable for every ŒΩ ‚â•1.
Proof
We know from Section 3.4 that a ŒΩ-stage Gauss‚ÄìLegendre method is of or-
der 2ŒΩ. By Lemma 4.1 the underlying function r belongs to PŒΩ/ŒΩ and, by Lemma 4.4,
it approximates the exponential function to order 2ŒΩ. Therefore, according to Theo-
rem 4.5, r = ÀÜrŒΩ/ŒΩ, a function that is A-acceptable by Theorem 4.6. It follows that the
Gauss‚ÄìLegendre method is A-stable.
4.4
A-stability of multistep methods
Attempting to extend the deÔ¨Ånition of A-stability to the multistep method (2.8),
we are faced with a problem: the implementation of an s-step method requires the
provision of s values and only one of these is supplied by the initial condition. We
will see in Chapter 7 how such values are derived in realistic computation. Here we
adopt the attitude that a stable solution of the linear equation (4.7) is required for
all possible values of y1, y2, . . . , ys‚àí1. The justiÔ¨Åcation of this pessimistic approach
is that otherwise, even were we somehow to choose ‚Äògood‚Äô starting values, a small
perturbation (e.g., a roundoÔ¨Äerror) might well divert the solution trajectory toward
instability. The reasons are similar to those already discussed in Section 4.1 in the
context of the Euler method.

64
StiÔ¨Äequations
Let us suppose that the method (2.8) is applied to the solution of (4.7).
The
outcome is
s

m=0
amyn+m = hŒª
s

m=0
bmyn+m,
n = 0, 1, . . . ,
which we write in the form
s

m=0
(am ‚àíhŒªbm)yn+m = 0,
n = 0, 1, . . .
(4.18)
The equation (4.18) is an example of a linear diÔ¨Äerence equation,
s

m=0
gmxn+m = 0,
n = 0, 1, . . . ,
(4.19)
and it can be solved similarly to the more familiar linear diÔ¨Äerential equation
s

m=0
gmx(m) = 0,
t ‚â•t0,
where the superscript indicates diÔ¨Äerentiation m times.
SpeciÔ¨Åcally, we form the
characteristic polynomial
Œ∑(w) :=
s

m=0
gmwm.
Let the zeros of Œ∑ be w1, w2, . . . , wq, say, with multiplicities k1, k2, . . . , kq respectively,
where q
i=1 ki = s. The general solution of (4.19) is
xn =
q

i=1
‚éõ
‚éù
ki‚àí1

j=0
ci,jnj
‚éû
‚é†wn
i ,
n = 0, 1, . . .
(4.20)
The s constants ci,j are uniquely determined by the s starting values x0, x1, . . . , xs‚àí1.
Lemma 4.7
Let us suppose that the zeros (as a function of w) of
Œ∑(z, w) :=
s

m=0
(am ‚àíbmz)wm,
z ‚ààC,
are w1(z), w2(z), . . . , wq(z)(z), while their multiplicities are k1(z), k2(z), . . . , kq(z)(z)
respectively. The multistep method (2.8) is A-stable if and only if
|wi(z)| < 1,
i = 1, 2, . . . , q(z)
for every
z ‚ààC‚àí.
(4.21)
Proof
As for (4.20), the behaviour of yn is determined by the magnitude of the
numbers wi(hŒª), i = 1, 2, . . . , q(hŒª). If all reside inside the complex unit disc then
their powers decay faster than any polynomial in n, therefore yn ‚Üí0. Hence, (4.21)
is suÔ¨Écient for A-stability.

4.4
A-stability of multistep methods
65
Adams‚ÄìBashforth, s = 2
Adams‚ÄìMoulton, s = 2
Adams‚ÄìBashforth, s = 3
Adams‚ÄìMoulton, s = 3
Figure 4.3
Linear stability domains D of Adams methods, explicit on the left and
implicit on the right.
However, if |w1(hŒª)| ‚â•1, say, then there exist starting values such that c1,0 Ã∏= 0;
therefore it is impossible for yn to tend to zero as n ‚Üí‚àû. We deduce that (4.21) is
necessary for A-stability and so conclude the proof.
Instead of a single geometric component in (4.11), we have now a linear combina-
tion of several (in general, s) components to reckon with. This is the quid pro quo
for using s ‚àí1 starting values in addition to the initial condition, a practice whose
perils have been highlighted already in the introduction to Chapter 2. According to
Exercise 2.2, if a method is convergent then one of these components approximates
the exponential function to the same order as the order of the method: this is similar
to Lemma 4.4. However, the remaining zeros are purely parasitic: we can attribute
no meaning to them so far as approximation is concerned.
Fig. 4.3 displays the linear stability domains of Adams methods, all at the same
scale. Notice Ô¨Årst how small they are and that they are reduced in size for the larger

66
StiÔ¨Äequations
s = 2
s = 4
s = 3
s = 5
Figure 4.4
Linear stability domains D of BDF methods of orders s = 2, 3, 4, 5,
shown at the same scale. Note that only s = 2 is A-stable.
s value. Next, pay attention to the diÔ¨Äerence between the explicit Adams‚ÄìBashforth
and the implicit Adams‚ÄìMoulton. In the latter case the stability domain, although not
very impressive compared with those for other methods of Section 4.3, is substantially
larger than for the explicit counterpart. This goes some way toward explaining the
interest in implicit Adams methods, but more important reasons will be presented in
Chapter 6.
However, as already mentioned in Chapter 2, Adams methods were never intended
to cope with stiÔ¨Äequations. After all, this was the motivation for the introduction of
backward diÔ¨Äerentiation formulae in Section 2.3. We turn therefore to Fig. 4.4, which
displays linear stability domains for BDF methods ‚Äì and are disappointed . . . True,
the set D is larger than was the case for, say, the Adams‚ÄìMoulton method. However,
only the two-step method displays any prospects of A-stability.
Let us commence with the good news: the BDF is indeed A-stable in the case
s = 2. To demonstrate this we require two technical lemmas, which will be presented

4.4
A-stability of multistep methods
67
with a comment in lieu of a complete proof.
Lemma 4.8
The multistep method (2.8) is A-stable if and only if bs > 0 and
|w1(it)|, |w2(it)|, . . . , |wq(it)(it)| ‚â§1,
t ‚ààR,
where w1, w2, . . . , wq(z) are the zeros of Œ∑(z, ¬∑ ) from Lemma 4.7.
Proof
On the face of it, this is an exact counterpart of Lemma 4.3: bs > 0 implies
analyticity in cl C‚àíand the condition on the moduli of zeros extends the inequality on
|r(z)|. This is deceptive, since the zeros of Œ∑(z, ¬∑ ) do not reside in the complex plane
but in an s-sheeted Riemann surface over C. This does not preclude the application
of the maximum principle, except that somewhat more sophisticated mathematical
machinery is required.
Lemma 4.9 (The Cohn‚ÄìSchur criterion)
Both zeros of the quadratic Œ±w2+Œ≤w+
Œ≥, where Œ±, Œ≤, Œ≥ ‚ààC, Œ± Ã∏= 0, reside in the closed complex unit disc if and only if
|Œ±| ‚â•|Œ≥|,
|Œ±|2 ‚àí|Œ≥|2 ‚â•
Œ±¬ØŒ≤ ‚àíŒ≤¬ØŒ≥

and
Œ± = Œ≥ Ã∏= 0
‚áí
|Œ≤| ‚â§2|Œ±|.
(4.22)
Proof
This is a special case of a more general result, the Cohn‚ÄìLehmer‚ÄìSchur
criterion. The latter provides a Ô¨Ånite algorithm to check whether a given complex
polynomial (of any degree) has all its zeros in any closed disc in C.
Theorem 4.10
The two-step BDF (2.15) is A-stable.
Proof
We have
Œ∑(z, w) = (1 ‚àí2
3z)w2 ‚àí4
3w + 1
3.
Therefore b2 = 2
3 and the Ô¨Årst A-stability condition of Lemma 4.8 is satisÔ¨Åed. To
verify the second condition we choose t ‚ààR and use Lemma 4.9 to ascertain that
neither of the moduli of the zeros of Œ∑(it, ¬∑ ) exceeds unity. Consequently Œ± = 1 ‚àí2
3it,
Œ≤ = ‚àí4
3, Œ≥ = 1
3 and we obtain
|Œ±|2 ‚àí|Œ≥|2 = 4
9(2 + t2) > 0
and
(|Œ±|2 ‚àí|Œ≥|2)2 ‚àí|Œ±¬ØŒ≤ ‚àíŒ≤¬ØŒ≥|2 = 16
81t4 ‚â•0.
Consequently, (4.22) is satisÔ¨Åed and we deduce A-stability.
Unfortunately, not only the ‚Äòpositive‚Äô deduction from Fig. 4.4 is true. The absence
of A-stability in the BDF for s ‚â•2 (of course, s ‚â§6, otherwise the method would
not be convergent and we would never use it!) is a consequence of a more general and
fundamental result.
Theorem 4.11 (The Dahlquist second barrier)
The highest order of an A-stable
multistep method (2.8) is 2.
Comparing the Dahlquist second barrier with the corollary to Theorem 4.6, it is
diÔ¨Écult to escape the impression that multistep methods are inferior to Runge‚ÄìKutta

68
StiÔ¨Äequations
methods when it comes to A-stability. This, however, does not mean that they should
not be used with stiÔ¨Äequations! Let us look again at Fig. 4.4. Although the cases
s = 3, 4, 5 fail A-stability, it is apparent that for each stability domain D there exists
Œ± ‚àà(0, œÄ] such that the inÔ¨Ånite wedge
VŒ± :=

œÅeiŒ∏ : œÅ > 0, |Œ∏ + œÄ| < Œ±

‚äÜC‚àí
belongs to D. In other words, provided that all the eigenvalues of a linear ODE system
reside in VŒ±, no matter how far away they are from the origin, there is no need to
depress the step size in response to stability restrictions. Methods with VŒ± ‚äÜD are
called A(Œ±)-stable.3 All BDF methods for s ‚â§6 are A(Œ±)-stable: in particular s = 3
corresponds to Œ± = 86‚ó¶2‚Ä≤; as Fig. 4.4 implies, almost all the region C‚àíresides in the
linear stability domain.
Comments and bibliography
DiÔ¨Äerent aspects of stiÔ¨Äequations and A-stability form the theme of several monographs
of varying degrees of sophistication and detail. Gear (1971) and Lambert (1991) are the
most elementary, whereas Hairer & Wanner (1991) is a compendium of just about everything
known in the subject area circa 1991. (No text, however, for obvious reasons, abbreviates
the phrase ‚Äòlinear stability domain‚Äô . . . )
Before we comment on a few themes connected with stability analysis, let us mention
brieÔ¨Çy two topics which, while tangential to the subject matter of this chapter, deserve proper
reference. Firstly, the functions ÀÜrŒ±/Œ≤, which have played a substantial role in Section 4.3,
are a special case of general Pad¬¥e approximation. Let f be an arbitrary function that is
analytic in the neighbourhood of the origin. The function ÀÜr ‚ààPŒ±/Œ≤ is said to be an [Œ±/Œ≤]
Pad¬¥e approximant of f if
ÀÜr(z) = f(z) + O
zŒ±+Œ≤+1
,
z ‚Üí0.
Pad¬¥e approximations possess a beautiful theory and have numerous applications, not just
in the more obvious Ô¨Åelds ‚Äì the approximation of functions, numerical analysis etc. ‚Äì but
also in analytic number theory: they are a powerful tool in many transcendentality proofs.
Baker & Graves-Morris (1981) presented a useful account of the Pad¬¥e theory. Secondly, the
Cohn‚ÄìSchur criterion (Lemma 4.9) is a special case of a substantially more general body
of knowledge that allows us to locate the zeros of polynomials in speciÔ¨Åc portions of the
complex plane by a Ô¨Ånite number of operations on the coeÔ¨Écients (Marden, 1966). A familiar
example is the Routh‚ÄìHurwitz criterion, which tests whether all the zeros reside in C‚àíand
is an important tool in control theory.
The characterization of all A-acceptable Pad¬¥e approximations to the exponential function
was the subject of a long-standing conjecture. Its resolution in 1978 by Gerhard Wanner,
Ernst Hairer and Syvert N√∏rsett introduced the novel technique of order stars and was one of
the great heroic tales of modern numerical mathematics. This technique can be also used to
prove a far-reaching generalization of Theorem 4.11, as well as many other interesting results
in the numerical analysis of diÔ¨Äerential equations. A comprehensive account of order stars
features in Iserles & N√∏rsett (1991).
As far as A-stability for multistep equations is concerned, Theorem 4.11 implies that
not much can be done. One obvious alternative, which has been mentioned in Section 4.4,
3Numerical analysts, being (mostly) human, tend to express Œ± in degrees rather than radians.

Comments and bibliography
69
‚àí2.0
‚àí1.5
‚àí1.0
‚àí0.5
0
0.5
1.0
1.5
2.0
‚àí2.0
‚àí1.5
‚àí1.0
‚àí0.5
0
0.5
1.0
1.5
2.0
‚àí2.0
‚àí1.5
‚àí1.0
‚àí0.5
0
0.5
1.0
1.5
2.0
‚àí2.0
‚àí1.5
‚àí1.0
‚àí0.5
0
0.5
1.0
1.5
2.0
Figure 4.5
Phase planes for the damped oscillator y‚Ä≤‚Ä≤ +y‚Ä≤ +sin y = 0 (on the left)
and the undamped oscillator y‚Ä≤‚Ä≤ + sin y = 0 (on the right).
is to relax the stability requirement, in which case the order barrier disappears altogether.
Another possibility is to combine the multistep rationale with the Runge‚ÄìKutta approach
and possibly to incorporate higher derivatives as well. The outcome, a general linear method
(Butcher, 2006), circumvents the barrier of Theorem 4.11.
We have mentioned in Section 4.2 that the justiÔ¨Åcation of the linear model, which has led
us into the concept of A-stability, is open to question when it comes to nonlinear equations. It
is, however, a convenient starting point. The stability analysis of discretized nonlinear ODEs
is these days a thriving industry! One model of nonlinear stability analysis is addressed in
the next chapter but we make no pretence that it represents anything but a taster for a
considerably more extensive theory.
And this is a convenient moment for a confession. StiÔ¨ÄODEs might seem ‚ÄòdiÔ¨Écult‚Äô and
indeed have been considered as such for a long time. Yet, once you get the hang of them,
use the right methods and take care of stability issues, you are highly unlikely ever to go
wrong. To understand why is this so and to get yourself in the right frame of mind for the
next chapter, examine the phase plane of the damped nonlinear oscillator y‚Ä≤‚Ä≤ + y‚Ä≤ + sin y = 0
on the left of Fig. 4.5.4 (Of course, we convert this second-order ODE into a system of two
coupled Ô¨Årst-order ODEs y‚Ä≤
1 = y2, y‚Ä≤
2 = ‚àísin y1 ‚àíy2.) No matter where we start within the
displayed range, the destination is the same, the origin. Now, applying a numerical method
means that our next step is typically misdirected to a neighbouring trajectory in the phase
plane, but it is obvious from the Ô¨Ågure that the Ô¨Çow itself is ‚Äòself correcting‚Äô. Unless we
are committing errors which are both large and biased, a hallmark of an unstable method,
ultimately our global picture will be at the very least of the right qualitative character: the
numerical trajectory will tend to the origin. Small errors will correct themselves, provided
that the method is stable enough.
Compare this with the undamped nonlinear oscillator y‚Ä≤‚Ä≤ + sin y = 0 on the right of
Fig. 4.5.
Except when it starts at the origin, in which case not much happens, the Ô¨Çow
4This system is not stiÔ¨Äbut even this gentle damping is suÔ¨Écient to convey our point, while a
real stiÔ¨Äsystem, e.g. y‚Ä≤‚Ä≤ + 1000y‚Ä≤ + sin y = 0, would have led to a plot that was considerably less
intelligible.

70
StiÔ¨Äequations
(again, within the range of displayed initial values) progresses in periodic orbits. Now, no
matter how accurate our method and no matter how stable it is, small errors can ‚Äòkick‚Äô us
to the wrong trajectory; and repeated ‚Äòkicks‚Äô, no matter how minute, are likely to produce
ultimately a numerical trajectory that exhibits completely the wrong qualitative behaviour.
Instead of a periodic orbit, the numerical solution might tend to a Ô¨Åxed point, diverge to
inÔ¨Ånity or, if our step size is too large, even exhibit spurious chaotic behaviour.
StiÔ¨ÄdiÔ¨Äerential equations allow the possibility of redemption. As long as you recognise
your sinful ways, correct your behaviour and adopt the right method and the right step size,
your misdemeanours wil be forgiven and your solution will prosper. Not so the nonlinear
oscillator y‚Ä≤‚Ä≤ + sin y = 0. Your numerical sins stay forever with you and accumulate forever.
Or at least until you learn in the next chapter how to deal with this situation.
Baker, G.A. and Graves-Morris, P. (1981), Pad¬¥e Approximants, Addison‚ÄìWesley, Reading,
MA.
Butcher, J.C. (2006), General linear methods, Acta Numerica 15, 157‚Äì256.
Gear, C.W. (1971), Numerical Initial Value Problems in Ordinary DiÔ¨Äerential Equations,
Prentice‚ÄìHall, Englewood CliÔ¨Äs, NJ.
Hairer, E. and Wanner, G. (1991), Solving Ordinary DiÔ¨Äerential Equations II: StiÔ¨ÄProblems
and DiÔ¨Äerential-Algebraic Equations, Springer-Verlag, Berlin.
Iserles, A. and N√∏rsett, S.P. (1991), Order Stars, Chapman & Hall, London.
Lambert, J.D. (1991), Numerical Methods for Ordinary DiÔ¨Äerential Systems, Wiley, London.
Marden, M. (1966), Geometry of Polynomials, American Mathematical Society, Providence,
RI.
Exercises
4.1
Let y‚Ä≤ = Œõy, y(t0) = y0, be solved (with a constant step size h > 0) by a one-
step method with a function r that obeys the relation (4.12). Suppose that a
nonsingular matrix V and a diagonal matrix D exist such that Œõ = V DV ‚àí1.
Prove that there exist vectors x1, x2, . . . , xd ‚ààRd such that
y(tn) =
d

j=1
etnŒªjxj,
n = 0, 1, . . . ,
and
yn =
d

j=1
[r(hŒª)]nxj,
n = 0, 1, . . . ,
where Œª1, Œª2, . . . , Œªd are the eigenvalues of Œõ. Deduce that the values of x1
and of x2, given in (4.3) and (4.4) are identical.
4.2‚ãÜ
Consider the solution of y‚Ä≤ = Œõy where
Œõ =

Œª
1
0
Œª

,
Œª ‚ààC‚àí.

Exercises
71
a Prove that
Œõn =

Œªn
nŒªn‚àí1
0
Œªn

,
n = 0, 1, . . .
b Let g be an arbitrary function that is analytic about the origin. The 2 √ó 2
matrix g(Œõ) can be deÔ¨Åned by substituting powers of Œõ into the Taylor
expansion of g. Prove that
g(tŒõ) =

g(tŒª)
tg‚Ä≤(tŒª)
0
g(tŒª)

.
c By letting g(z) = ez prove that limt‚Üí‚àûy(t) = 0.
d Suppose that y‚Ä≤ = Œõy is solved with a Runge‚ÄìKutta method, using a con-
stant step size h > 0. Let r be the function from Lemma 4.1. Letting g = r,
obtain the explicit form of [r(hŒõ)]n, n = 0, 1, . . .
e Prove that if hŒª ‚ààD, where D is the linear stability domain of the Runge‚Äì
Kutta method, then limn‚Üí‚àûyn = 0.
4.3‚ãÜ
This question is concerned with the relevance of the linear stability domain
to the numerical solution of inhomogeneous linear systems.
a Let Œõ be a nonsingular matrix. Prove that the solution of y‚Ä≤ = Œõy + a,
y(t0) = y0, is
y(t) = e(t‚àít0)Œõy0 + Œõ‚àí1[e(t‚àít0)Œõ ‚àíI]a,
t ‚â•t0.
Thus, deduce that if Œõ has a full set of eigenvectors and all its eigenvalues
reside in C‚àíthen limt‚Üí‚àûy(t) = ‚àíŒõ‚àí1a.
b Assuming for simplicity‚Äôs sake that the underlying equation is scalar, i.e.
y‚Ä≤ = Œªy +a, y(t0) = y0, prove that a single step of the Runge‚ÄìKutta method
(3.9) results in
yn+1 = r(hŒª)yn + q(hŒª),
n = 0, 1, . . . ,
where r is given by (4.13) and
q(z) := hab‚ä§(I ‚àízA)‚àí11 ‚ààP(ŒΩ‚àí1)/ŒΩ,
z ‚ààC.
c Deduce, by induction or otherwise, that
yn = [r(hŒª)]ny0 +
#[r(hŒª)]n ‚àí1
r(hŒª) ‚àí1
$
q(hŒª),
n = 0, 1, . . .
d Assuming that hŒª ‚ààD, prove that limn‚Üí‚àûyn exists and is bounded.
4.4
Determine all values of Œ∏ such that the theta method (1.13) is A-stable.

72
StiÔ¨Äequations
4.5
Prove that for every ŒΩ-stage explicit Runge‚ÄìKutta method (3.5) of order ŒΩ
it is true that
r(z) =
ŒΩ

k=0
1
k!zk,
z ‚ààC.
4.6
Evaluate explicitly the function r for the following Runge‚ÄìKutta methods:
a
0
0
0
2
3
1
3
1
3
1
4
3
4
,
b
1
6
1
6
0
5
6
2
3
1
6
1
2
1
2
,
c
0
0
0
0
1
2
1
4
1
4
0
1
0
1
0
1
6
2
3
1
6
.
Are these methods A-stable?
4.7
Prove that the Pad¬¥e approximation ÀÜr0/3 is not A-acceptable.
4.8
Determine the order of the two-step method
yn+2 ‚àíyn = 2
3h

f(tn+2, yn+2) + f(tn+1, yn+1) + f(tn, yn)

, n = 0, 1, . . .
Is it A-stable?
4.9
The two-step method
yn+2 ‚àíyn = 2hf(tn+1, yn+1),
n = 0, 1, . . .
(4.23)
is called the explicit midpoint rule.
a Denoting by w1(z) and w2(z) the zeros of the underlying function Œ∑(z, ¬∑ ),
prove that w1(z)w2(z) ‚â°‚àí1 for all z ‚ààC.
b Show that D = ‚àÖ.
c We say that ÀúD is a weak linear stability domain of a numerical method
if, when applied to the scalar linear test equation, it produces a uniformly
bounded solution sequence. (It is easy to see that ÀúD = cl D for most methods
of interest.) Determine explicitly ÀúD for the method (4.23).
The method (4.23) will feature again in Chapters 16 and 17, in the guise of
the leapfrog scheme.
4.10
Prove that if the multistep method (2.8) is convergent then 0 ‚àà‚àÇÀúD.

5
Geometric numerical integration
5.1
Between quality and quantity
If mathematics is the language of science and engineering, diÔ¨Äerential equations form
much of its grammar. A myriad of facts originating in the laboratory, in an astronom-
ical observatory or on a Ô¨Åeld trip, Ô¨Çashes of enlightenment and sudden comprehension,
the poetry of nature and the miracle of the human mind can all be phrased in the
language of mathematical models coupling the behaviour of a physical phenomenon
with its rate of change: diÔ¨Äerential equations. No wonder, therefore, that research
into diÔ¨Äerential equations is so central to contemporary mathematics. Mathematical
disciplines from functional analysis to algebraic geometry, from operator theory and
harmonic analysis to diÔ¨Äerential geometry, algebraic topology, analytic function the-
ory, spectral theory, nonlinear dynamical systems and beyond are, once you delve into
their origins and ramiÔ¨Åcations, mostly concerned with adding insight into the great
mystery of diÔ¨Äerential equations.
Modern mathematics is extraordinarily useful in deriving a wealth of qualitative
information about diÔ¨Äerential equations, information that often has profound physical
signiÔ¨Åcance. Yet, except for particularly simple situations, it falls short of actually
providing the solution in an explicit form. The task of Ô¨Çeshing out numbers on the
mathematical bones falls to numerical analysis. And here looms danger . . . The stan-
dard rules of engagement of numerical analysis are simple: deploy computing power
and algorithmic ingenuity to minimize error. Yet it is possible that, in our quest for
the best quantity, we might sacriÔ¨Åce quality. Features of the exact solution that have
been derived with a great deal of mathematical ingenuity (and which might have im-
portant signiÔ¨Åcance in applications) might well be lost in our quest to derive the most
accurate solution with the least computing eÔ¨Äort.
Painting with a broad brush, as one is bound to do in a textbook, we can distinguish
two kinds of qualitative feature of a time-evolving diÔ¨Äerential equation, the dynamic
and the geometric. The dynamic attributes of a diÔ¨Äerential equation have to do with
the ultimate destination of its solution. As time increases to inÔ¨Ånity will the solution
tend to a Ô¨Åxed point? Will it be periodic? Or will it exhibit more ‚Äòexotic‚Äô behaviour,
e.g. chaos? The geometric characteristics of a diÔ¨Äerential equation, however, typically
refer to features which are invariant in time. Typical invariants include Ô¨Årst integrals
‚Äì thus, some diÔ¨Äerential equations conserve energy, angular momentum or (as we will
see below) orthogonality. Other invariants are more elaborate and cannot be easily
phrased just in terms of the solution trajectory, yet they often have deep mathematical
73

74
Geometric numerical integration
0
100
200
300
400
500
600
700
800
900
1000
0
0.5
1.0
y1
0
100
200
300
400
500
600
700
800
900
1000
‚àí0.5
0
0.5
y2
0
100
200
300
400
500
600
700
800
900
1000
‚àí1
‚àí0.5
0
0.5
1.0
y3
‚àí0.4
‚àí0.2
0
0.2
0.4
0.6
0.8
1.0
1.2
‚àí0.8
‚àí0.6
‚àí0.4
‚àí0.2
0
0.2
0.4
0.6
0.8
y1
y2
Figure 5.1
(a) The solution of the ODE system (5.1) for t ‚â§1000 with initial
value y(0) = (
‚àö
3
3 ,
‚àö
3
3 ,
‚àö
3
3 ) and (b) the phase plane (y1, y2).
(a)
(b)
and physical signiÔ¨Åcance. A case in point, upon which we will elaborate at greater
length later, is the conservation of symplectic form by Hamiltonian systems.
An innocent-looking ODE system exhibiting a wealth of dynamical and geometric
features is
y‚Ä≤
1 = y2y3 sin t ‚àíy1y2y3,
y‚Ä≤
2 = ‚àíy1y3 sin t + 1
20y1y3,
(5.1)

5.1
Between quality and quantity
75
y‚Ä≤
3 = y2
1y2 ‚àí1
20y1y2,
whose solution is displayed in Fig. 5.1. The solution is bounded, highly oscillatory
and clearly switches between two modes that are suggestively periodic.
Are these
modes periodic?
Are the switches chaotic?
Good questions, but the system (5.1)
has been designed solely for the purpose of our exposition and not much is known
about it, except for one feature that can be proved with ease. Since y1y‚Ä≤
1 + y2y‚Ä≤
2 +
y3y‚Ä≤
3 = 0 it follows at once that as t increases the Euclidean norm y(t) = [y2
1(t) +
y2
2(y) + y2
3(t)]1/2 remains constant. In other words, the solution of (5.1) evolves on
the two-dimensional unit sphere embedded in R3. It makes sense, at least intuitively,
to compute it while respecting this feature, but simple numerical experiments using
methods from Chapters 2 and 3 mostly exhibit a drift away from the sphere.
There is a priori no reason whatsoever why numerical methods should respect
invariants or have the correct asymptotic behaviour.
Does it matter?
It depends
and indeed is heavily sensitive to the nature of the application that our numerical
solution is attempting to elucidate. Often the correct rendition of qualitative features
is of lesser importance or an optional extra, but sometimes it is absolutely essential
that we model the geometry or dynamics correctly. An obvious example is when the
entire purpose of the computation is to shed light on the asymptotic behaviour of the
solution as t ‚Üí‚àû. In that case we are concerned very little with errors committed in
Ô¨Ånite time, but we cannot allow any infelicities insofar as the dynamics is concerned.
Another example occurs when the conservation of a geometric feature is central to the
entire purpose of the computation.
3 Isospectral Ô¨Çows
On the face of it, there is little about the ODE system
y‚Ä≤
1 = 2y2
4,
y‚Ä≤
2 = 2y2
5‚àí2y2
4,
y‚Ä≤
3 = ‚àí2y2
5,
y‚Ä≤
4 = (y2‚àíy1)y4,
y‚Ä≤
5 = (y3‚àíy2)y5
that meets the eye. However, once we arrange the Ô¨Åve unknowns in a sym-
metric tridiagonal matrix
Y =
‚é°
‚é£
y1
y4
0
y4
y2
y5
0
y5
y3
‚é§
‚é¶,
we can rewrite the system in the form
Y ‚Ä≤ = B(Y )Y ‚àíY B(Y ),
where
B(Y ) =
‚é°
‚é£
0
y4
0
‚àíy4
0
y5
0
‚àíy5
0
‚é§
‚é¶.
The solution of this matrix ODE stays symmetric and tridiagonal for every
t ‚â•0 but, more remarkably, it has a striking feature: the eigenvalues stay put
as the solution evolves!
Had this been true just for one innocent-looking ODE system, this might
have merited little interest. However, our system can be generalized, whence
it becomes of considerably greater interest. Thus, let Y0 be an arbitrary real

76
Geometric numerical integration
symmetric d√ód matrix and suppose that the Lipschitz function B maps such
a matrix into real, skew-symmetric d √ó d matrices. The matrix ODE system
Y ‚Ä≤ = B(Y )Y ‚àíY B(Y ),
t ‚â•0,
Y (0) = Y0,
(5.2)
is said to be isospectral: the eigenvalues of Y (t) coincide with these of Y0
for all t ‚â•0. The proof is important because, as we will see later, it can
be readily translated into a numerical method. Thus, we seek a solution of
the form Y (t) = Q(t)Y0Q‚àí1(t), where Q(t) is a d √ó d matrix function. Since
dQ‚àí1/dt = ‚àíQ‚àí1Q‚Ä≤Q‚àí1, substitution into (5.2) readily conÔ¨Årms that this is
indeed the case, provided that Q itself satisÔ¨Åes the diÔ¨Äerential equation
Q‚Ä≤ = B(QY0Q‚àí1)Q,
t ‚â•0,
Q(0) = I.
(5.3)
Therefore the matrices Y (t) and Y0 share the same eigenvalues.
Actually, this is not the end of the story!
Let Z(t) = Q(t)Q‚ä§(t).
Direct
diÔ¨Äerentiation and the skew-symmetry of B imply that Z obeys the matrix
ODE
Z‚Ä≤ = Q‚Ä≤Q‚ä§+ Q(Q‚ä§)‚Ä≤ = Q‚Ä≤Q‚ä§+ Q(Q‚Ä≤)‚ä§= BQQ‚ä§+ QQ‚ä§B‚ä§= BZ ‚àíZB
with the initial condition Z(0) = I. But the only possible solution of this
equation is Z(t) ‚â°I, and we thus deduce that QQ‚ä§= I. In other words, the
solution of (5.3) is an orthogonal matrix! We Ô¨Åle this important fact for future
use, noting for the present the implication that Y = QY0Q‚àí1 = QY0Q‚ä§is
indeed symmetric.
It is possible to show that for some choices of the matrix function B the
solution of (5.2) invariably tends to a Ô¨Åxed point ÀÜY as t ‚Üí‚àûand also that ÀÜY
is a diagonal matrix. Because of our discussion, it is clear that the diagonal
of ÀÜY consists of the eigenvalues of Y0 and, conceivably, we could solve (5.2)
as a means to their computation. However, for this approach to make sense,
it is crucial that our numerical method renders the eigenvalues correctly. The
bad news is that all the methods that we have mentioned so far in this book
are unequal to this task!
3
Think again about the example of isospectral Ô¨Çows. A numerical method is bound to
commit an error: this is part and parcel of a numerical solution. Our requirement,
though, is that (within roundoÔ¨Ä) this error is nil insofar as eigenvalues are concerned!
Isospectral Ô¨Çows are but one example of cases where the conservation of ‚Äògeometry‚Äô
is an issue. Many other invariants are important for physical or mathematical reasons.
Moreover, the distinction between dynamics and geometry in long-time integration is
fairly moot.
In important cases it is possible to prove that the maintenance of a
geometric feature guarantees the computation of the correct dynamics.
The part of the numerical analysis of diÔ¨Äerential equations concerned with com-
putation in a way that respects dynamic and geometric features is called ‚Äògeometric
numerical integration‚Äô (GNI). This is a fairly new theory, which has already led to
an important change of focus, more in the numerical analysis of ODEs than in the

5.2
Monotone equations
77
computation of PDEs (where the theory is much more incomplete). In this chapter
we restrict our narrative to three examples of GNI in action. A more comprehensive
treatment of the subject must be relegated to specialized monographs.
We have mentioned two reasons why it might be good to conserve dynamic or
geometric features: their intrinsic mathematical importance (recall eigenvalues and
isospectral Ô¨Çows) and their signiÔ¨Åcance in applications. Intriguingly, there is a third
reason, and it has to do with numerical analysis itself.
It is possible to prove for
important categories of equations that, once certain geometric invariants are respected
under discretization, numerical error accumulates much more slowly. This becomes
very important in long-term computations.
5.2
Monotone equations and algebraic stability
We have already seen in Chapter 4 a simple linear model concerned with the conserva-
tion of the dynamics. To employ the terminology of the current chapter, we observed
that A-stable methods render correctly the dynamics of linear ODE systems y‚Ä≤ = Ay
when all the eigenvalues of the matrix A reside in the left half-plane. In this section
we present a simple model for the analysis of computational dynamics in a nonlinear
setting.
Let ‚ü®¬∑ , ¬∑ ‚ü©be an inner product in Cd and ‚à•¬∑ ‚à•the corresponding norm. We say
that the ODE system
y‚Ä≤ = f(t, y),
t ‚â•0,
(5.4)
is monotone (with respect to the given inner product) if the function f satisÔ¨Åes the
inequality
Re ‚ü®u ‚àív, f(t, u) ‚àíf(t, v)‚ü©‚â§0,
t ‚â•0,
u, v ‚ààCd.
(5.5)
The importance of monotonicity follows from the next result.
Lemma 5.1
Subject to the monotonicity condition (5.5), the ODE (5.4) is dissi-
pative: given two solutions, u and v, say, with initial conditions u(0) = u0 and
v(0) = v0, the function ‚à•u(t) ‚àív(t)‚à•decreases monotonically for t ‚â•0.
Proof
Let œÜ(t) = 1
2‚à•u(t) ‚àív(t)‚à•2. It then follows from (5.4) and (5.5) that
œÜ‚Ä≤(t) = 1
2
d
dt ‚ü®u(t) ‚àív(t), u(t) ‚àív(t)‚ü©
= 1
2 ‚ü®u‚Ä≤(t) ‚àív‚Ä≤(t), u(t) ‚àív(t)‚ü©+ 1
2 ‚ü®u(t) ‚àív(t), u‚Ä≤(t) ‚àív‚Ä≤(t)‚ü©
= ‚ü®u(t) ‚àív(t), f(t, u(t)) ‚àíf(t, v(t))‚ü©‚â§0.
This proves the lemma.
The intuitive interpretation of Lemma 5.1 is that diÔ¨Äerent solution trajectories of
(5.4) never depart from each other. From the dynamical point of view, this means
that small perturbations remain forever small.
3 Even scalar equations can be interesting!
Consider the scalar equation
y‚Ä≤ = 1
8 ‚àíy3 and its fairly mild perturbation y‚Ä≤ = 1
8 + 1
6y ‚àíy3. It is easy to

78
Geometric numerical integration
0
1
2
3
4
5
6
7
8
9
10
‚àí1.0
‚àí0.5
0
0.5
1.0
0
1
2
3
4
5
6
7
8
9
10
‚àí1.0
‚àí0.5
0
0.5
1.0
Figure 5.2
Solution trajectories for a monotone (top plot) scalar cubic equation
and its nonmonotone perturbation.
see that, while the Ô¨Årst obeys (5.4) and is monotone, this is not true for the
second equation.
Solution trajectories for a range of initial values are plotted for both equations
in Fig. 5.2. On the face of it, they are fairly similar and it is easy to verify
that in both cases all solutions approach a unique Ô¨Åxed point for t ‚â´1. Yet,
while for the monotone equation the trajectories always bunch up, it is easy
to discern in the bottom plot examples of trajectories that depart from each
other even if only for a while.
This behaviour has intriguing implications once these equations are discretized.
A numerical solution bears an error which, no matter how small (unless we are
extraordinarily lucky and there is no error at all!), means that our next step
resides on a nearby trajectory. Now, if that trajectory does not take us from
the correct solution and if the numerical method is ‚Äòstable‚Äô (in a sense which,
for the time being, we leave vague), this does not matter much. If, however,
the new trajectory takes us further from the correct one, it is possible that
the next step will land us on yet another trajectory, even more remote from
the correct one, and so on: the error cascades and in short order the solution
loses its accuracy.
3
Clearly, it is important to examine whether, once a diÔ¨Äerential equation satisÔ¨Åes (5.4)
and is monotone, the numerical methods of Chapters 2 and 3 conform with Lemma 5.1

5.2
Monotone equations
79
and therefore possess the ‚Äòstability‚Äô mentioned in the last example. In what follows
we address this issue insofar as Runge‚ÄìKutta methods are concerned. The treatment
of multistep methods within this context is much more complicated and outside the
scope of this book.
We say that the Runge‚ÄìKutta method (3.9) is algebraically stable if, subject to
the inequality (5.5), it produces dissipative solutions. In other words, if un and vn
are separate solution sequences (with the same step sizes), corresponding to the initial
values u0 and v0 respectively, then, necessarily,
‚à•un+1 ‚àívn+1‚à•‚â§‚à•un ‚àívn‚à•,
n = 0, 1, . . . .
(5.6)
The main new object in our analysis (and in the rest of this chapter) is a matrix which,
thanks to its surprising ubiquity in many diÔ¨Äerent corners of GNI, is usually referred
informally as ‚Äòthe famous matrix M‚Äô: its elements are given by
mk,‚Ñì= bkak,‚Ñì+ b‚Ñìa‚Ñì,k ‚àíbkb‚Ñì,
k, ‚Ñì= 1, 2, . . . , ŒΩ,
where ak,‚Ñìand bk are the RK matrix elements and the RK weights of the method
(3.9). Note that the ŒΩ √ó ŒΩ matrix M is symmetric.
Theorem 5.2
If the matrix M is positive semideÔ¨Ånite and the weights b1, b2, . . . , bŒΩ
are nonnegative then the Runge‚ÄìKutta method (3.9) is algebraically stable.
Proof
We need to look at detail at a single step of the method (3.9), applied at
tn to the initial vectors un and vn. We denote the internal stages by r1, r2, . . . , rŒΩ
and s1, s2, . . . , sŒΩ respectively, and let
œÅj = un + h
ŒΩ

i=1
aj,iri,
œÉj = vn + h
ŒΩ

i=1
aj,isi,
j = 1, 2, . . . , ŒΩ.
(5.7)
Thus,
rj = f(tn + cjh, œÅj),
sj = f(tn + cjh, œÉj),
j = 1, 2, . . . , ŒΩ
(5.8)
and
un+1 = un + h
ŒΩ

j=1
bjrj,
vn+1 = vn + h
ŒΩ

j=1
bjsj.
(5.9)
We need to prove that the conditions of the theorem imply the inequality (5.6),
namely that
‚à•un+1 ‚àívn+1‚à•2 ‚àí‚à•un ‚àívn‚à•2 ‚â§0.
(5.10)
But, by (5.9),
‚à•un+1 ‚àívn+1‚à•2 =
+
un ‚àívn + h
ŒΩ

j=1
bj(rj ‚àísj), un ‚àívn + h
ŒΩ

j=1
bj(rj ‚àísj)
,
=
+
un ‚àívn + h
ŒΩ

j=1
bjdj, un ‚àívn + h
ŒΩ

j=1
bjdj
,
= ‚à•un ‚àívn‚à•2 + 2h Re
+
un ‚àívn,
ŒΩ

j=1
bjdj
,
+ h2
------
ŒΩ

j=1
bjdj
------
2
,

80
Geometric numerical integration
where dj = rj ‚àísj, j = 1, 2, . . . , ŒΩ. Thus, (5.10) is equivalent to
2 Re
+
un ‚àívn,
ŒΩ

j=1
bjdj
,
+ h
------
ŒΩ

j=1
bjdj
------
2
‚â§0.
(5.11)
Using (5.7) to replace un and vn by
œÅj ‚àíh
ŒΩ

i=1
aj,iri
and
œÉj ‚àíh
ŒΩ

i=1
aj,isi
respectively, we obtain
Re
+
un ‚àívn,
ŒΩ

j=1
bjdj
,
=
ŒΩ

j=1
bj
+
œÅj ‚àíœÉj ‚àíh
ŒΩ

i=1
aj,idi, dj
,
=
ŒΩ

j=1
bj Re ‚ü®œÅj ‚àíœÉj, dj‚ü©‚àíh
ŒΩ

j=1
ŒΩ

i=1
bjaj,i Re ‚ü®di, dj‚ü©.
By our assumption, though, the system (5.4) is monotone and, using (5.8) and the
nonnegativity of the weights, it follows that
ŒΩ

j=1
bj Re ‚ü®œÅj ‚àíœÉj, dj‚ü©=
ŒΩ

j=1
bj Re ‚ü®œÅj ‚àíœÉj, f(tn + cjh, œÅj) ‚àíf(tn + cjh, œÉj)‚ü©‚â§0,
consequently
Re
+
un ‚àívn,
ŒΩ

j=1
bjdj
,
‚â§‚àíh
ŒΩ

i=1
ŒΩ

j=1
bjaj,i Re ‚ü®dj, di‚ü©
and, swapping indices,
Re
+
un ‚àívn,
ŒΩ

j=1
bjdj
,
‚â§‚àíh
ŒΩ

i=1
ŒΩ

j=1
biai,j Re ‚ü®di, dj‚ü©.
Therefore,
2 Re
+
un ‚àívn,
ŒΩ

j=1
bjdj
,
+ h
------
ŒΩ

j=1
bjdj
------
2
‚â§h
ŒΩ

i=1
ŒΩ

j=1
(bibj ‚àíbjaj,i ‚àíbiai,j) Re ‚ü®di, dj‚ü©= ‚àíh
ŒΩ

i=1
ŒΩ

j=1
mi,j Re ‚ü®di, dj‚ü©.
We deduce that (5.11), and hence (5.6), are true if
ŒΩ

i=1
ŒΩ

j=1
mi,j Re ‚ü®di, dj‚ü©‚â•0,
d1, d2, . . . , dŒΩ ‚ààCd.

5.2
Monotone equations
81
Recall our assumption that the matrix M is positive semideÔ¨Ånite. Therefore it can be
written in the form M = WŒõW ‚ä§, where W is orthogonal, and where Œõ is diagonal
and Œªk = Œõk,k ‚â•0, k = 1, 2, . . . , ŒΩ. Since mi,j = ŒΩ
k=1 Œªkwi,kwj,k, i, j = 1, 2, . . . , ŒΩ,
we deduce that
ŒΩ

i=1
ŒΩ

j=1
mi,j Re ‚ü®di, dj‚ü©=
ŒΩ

i=1
ŒΩ

j=1
ŒΩ

k=1
Œªkwi,kwj,k Re ‚ü®di, dj‚ü©
=
ŒΩ

k=1
Œªk Re
+ ŒΩ

i=1
wi,jdi,
ŒΩ

j=1
wj,kdj
,
=
ŒΩ

k=1
Œªk
------
ŒΩ

j=1
wj,kdj
------
2
‚â•0.
This completes the proof: since the above argument applies to all monotone equations,
the Runge‚ÄìKutta method in question is indeed algebraically stable.
Which RK methods can satisfy the conditions of Theorem 5.2?
DeÔ¨Ånitely not
explicit methods, since then mk,k = ‚àíb2
k, k = 1, 2, . . . , ŒΩ and ŒΩ
k=1 bk = 1 (necessary
for order p ‚â•1), which in tandem are inconsistent with the positive semideÔ¨Åniteness
of M.
But we do not need Theorem 5.2 in order to rule out explicit methods! A special
case of a monotone equation is the scalar test equation (4.7) with Re Œª < 0, the
cornerstone of the linear stability analysis of Chapter 4. Therefore, for an algebraically
stable method it is necessary that the complex left half-plane resides within the linear
stability domain: precisely the deÔ¨Ånition of A-stability! We thus deduce that only
A-stable methods are candidates for algebraic stability.
Yet, algebraic stability is a stronger concept than A-stability. For example, the
three-stage method
0
0
0
0
1
2
5
24
1
3
‚àí1
24
1
1
6
2
3
1
6
1
6
2
3
1
6
is of order 4 (prove!) and A-stable (prove!). However, it is a matter of trivial calcula-
tion to demonstrate that the matrix
M = 1
36
‚é°
‚é£
‚àí1
1
0
1
0
‚àí1
0
‚àí1
1
‚é§
‚é¶
is not positive semideÔ¨Ånite.
Given an RK method
c
A
b‚ä§,
we say that it is B(r) if
ŒΩ

i=1
bick‚àí1
i
= 1
k ,
k = 1, 2, . . . , r

82
Geometric numerical integration
and C(r) if
ŒΩ

j=1
ai,jck‚àí1
j
= ck
i
k ,
i = 1, 2, . . . , ŒΩ,
k = 1, 2, . . . , r.
Lemma 5.3
If c1, . . . , , cŒΩ are distinct and a Runge‚ÄìKutta method is both B(2ŒΩ) and
C(ŒΩ) then M = O, the zero matrix.
Proof
The Vandermonde matrix V , where vk,‚Ñì= ck‚àí1
‚Ñì
, k, ‚Ñì= 1, 2, . . . , ŒΩ, is non-
singular (A.1.2.3). Therefore M = O if and only if Àú
M = O, where Àú
M = V ‚ä§MV . But,
using the conditions B(2ŒΩ) and C(ŒΩ) where necessary,
Àúmk,‚Ñì=
ŒΩ

i=1
ŒΩ

j=1
ck‚àí1
i
mi,jc‚Ñì‚àí1
j
=
ŒΩ

i=1
ŒΩ

j=1
ck‚àí1
i
(biai,j + bjaj,i ‚àíbibj)c‚Ñì‚àí1
j
=
ŒΩ

i=1
bick‚àí1
i
ŒΩ

j=1
ai,jc‚Ñì‚àí1
j
+
ŒΩ

j=1
bjc‚Ñì‚àí1
j
ŒΩ

i=1
aj,ick‚àí1
i
‚àí
ŒΩ

i=1
bick‚àí1
i
ŒΩ

j=1
bjc‚Ñì‚àí1
j
= 1
‚Ñì
ŒΩ

i=1
bick+‚Ñì‚àí1
i
+ 1
k
ŒΩ

j=1
bjck+‚Ñì‚àí1
j
‚àí1
k‚Ñì=
	1
‚Ñì+ 1
k

1
k + ‚Ñì‚àí1
k‚Ñì= 0
for all k, ‚Ñì= 1, 2, . . . , ŒΩ. Hence Àú
M = O, and so M = O.
Corollary
The Gauss‚ÄìLegendre methods from Chapter 3 are algebraically stable for
all ŒΩ ‚â•1.
Proof
We recall that each ŒΩ-stage Gauss‚ÄìLegendre RK is a collocation method
of order 2ŒΩ. In particular, the underlying quadrature formula is itself of order 2ŒΩ (it
is the Gaussian quadrature of Theorem 3.3). This implies that
ŒΩ

i=1
bick‚àí1
i
=
 1
0
xk‚àí1 dx = 1
k ,
k = 1, 2, . . . , 2ŒΩ,
and hence that the Runge‚ÄìKutta method is B(2ŒΩ). Moreover, according to (3.13),
ak,‚Ñì=
 ck
0
q‚Ñì(œÑ)
q‚Ñì(c‚Ñì) dœÑ,
k, ‚Ñì= 1, 2, . . . , ŒΩ,
where q(t) = "ŒΩ
j=1(t ‚àícj) and q‚Ñì(t) = q(t)/(t ‚àíc‚Ñì). Therefore
ŒΩ

j=1
ai,jck‚àí1
j
=
 ci
0
‚éõ
‚éù
ŒΩ

j=1
qj(œÑ)
qj(cj)ck‚àí1
j
‚éû
‚é†dœÑ.
Using an argument similar to that in the proof of Lemma 3.5, the integrand is the
Lagrange interpolation polynomial of œÑ k‚àí1. Therefore, since k ‚â§ŒΩ, it equals œÑ k‚àí1 and
so
ŒΩ

j=1
ai,jck‚àí1
j
=
 ci
0
œÑ k‚àí1 dœÑ = ck
i
k ,
i, k = 1, . . . , ŒΩ.

5.3
Quadratic invariants
83
Therefore the condition C(ŒΩ) is met and now we can use Lemma 5.3 to argue that
M = O.
It remains to prove that the weights b1, b2, . . . , bŒΩ are nonnegative.
Let k =
1, 2, . . . , ŒΩ and f(x) = [qk(x)/qk(ck)]2.
Since f is a polynomial of degree 2ŒΩ ‚àí2,
it is integrated exactly by Gaussian quadrature. Moreover, f(ck) = 0 and f(c‚Ñì) = 0
for ‚ÑìÃ∏= k. Therefore
bk =
ŒΩ

‚Ñì=1
b‚Ñìf(c‚Ñì) =
 1
0
f(œÑ) dœÑ > 0.
We deduce that the Gauss‚ÄìLegendre RK method is algebraically stable.
5.3
From quadratic invariants to orthogonal Ô¨Çows
Our concern in this section is with diÔ¨Äerential equations endowed with a quadratic
invariant. SpeciÔ¨Åcally, we consider systems (5.4) such that, for every initial value y0,
y‚ä§(t)Sy(t) ‚â°y‚ä§
0 Sy0,
t ‚â•0,
(5.12)
where S is a nonzero symmetric d √ó d matrix.
(We restrict our attention to real
equations, while mentioning in passing that generalization to a complex setting is
straightforward.)
The invariant (5.12) means that the solution of the diÔ¨Äerential equation is restricted
to a lower-dimensional manifold in Rd. If all the eigenvalues of S are of the same sign
then this manifold is a generalized ellipsoid but we will not explore this issue further.
We commence our numerical analysis of quadratic invariants by asking which
Runge‚ÄìKutta methods produce a solution consistent with (5.12), in other words with
y‚ä§
n+1Syn+1 = y‚ä§
n Syn,
n = 0, 1, . . .
(5.13)
The framework is surprisingly similar to that of the last section and, indeed, we will
use similar ideas and notation: if the truth be told, we have already done all the heavy
lifting in the proof of Theorem 5.2.
Theorem 5.4
Suppose that a Runge‚ÄìKutta method is applied to an ODE (5.4) with
quadratic invariant (5.12). If M = O then the method satisÔ¨Åes (5.13) and is consistent
with the invariant.
Proof
We denote the internal stages of the method by r1, r2, . . . , rŒΩ and let
œÅk = f(tn + ckh, yn + h ŒΩ
j=1 ak,jrj), k = 1, 2, . . . , ŒΩ.
Similarly to the proof of
Theorem 5.2, we calculate
y‚ä§
n+1Syn+1 =

yn + h
ŒΩ

k=1
bkrk
‚ä§
S

yn + h
ŒΩ

‚Ñì=1
b‚Ñìr‚Ñì

= y‚ä§
n Syn + h
 ŒΩ

k=1
bkr‚ä§
k Syn + y‚ä§
n S
ŒΩ

‚Ñì=1
b‚Ñìr‚Ñì

+ h2
ŒΩ

k=1
ŒΩ

‚Ñì=1
bkb‚Ñìr‚ä§
k Sr‚Ñì.

84
Geometric numerical integration
Letting yn = œÅk ‚àíh ŒΩ
‚Ñì=1 ak,‚Ñìr‚Ñì, we have
r‚ä§
k Syn = r‚ä§
k SœÅk ‚àíh
ŒΩ

‚Ñì=1
ak,‚Ñìr‚ä§
k Sr‚Ñì,
k = 1, 2, . . . , ŒΩ.
However, diÔ¨Äerentiating (5.12) and using the symmetry of S yields
0 = y‚ä§(t)Sy‚Ä≤(t) = y‚ä§(t)Sf(t, y(t)),
t ‚â•0.
The above identity still holds when we replace y(t) by an arbitrary vector x ‚ààRd,
because y0 ‚ààRd is itself arbitrary. In particular, it is true for x = œÅk and, since
rk = f(tn + ckh, œÅk), we deduce that r‚ä§
k SœÅk = œÅ‚ä§
k Srk = 0. Therefore
r‚ä§
k Syn = ‚àíh
ŒΩ

‚Ñì=1
ak,‚Ñìr‚ä§
k Sr‚Ñì,
k = 1, 2, . . . , ŒΩ,
and, by the same token,
y‚ä§
n Sr‚Ñì= ‚àíh
ŒΩ

k=1
a‚Ñì,kr‚ä§
k Sr‚Ñì,
‚Ñì= 1, 2, . . . , ŒΩ.
Assembling all this gives
y‚ä§
n+1Syn+1 = y‚ä§
n Syn ‚àíh2
ŒΩ

k=1
ŒΩ

‚Ñì=1
(bkak,‚Ñì+ b‚Ñìa‚Ñì,k ‚àíbkb‚Ñì)r‚ä§
k Sr‚Ñì
= y‚ä§
n Syn ‚àíh2
ŒΩ

k=1
ŒΩ

‚Ñì=1
mk,‚Ñìr‚ä§
k Sr‚Ñì= y‚ä§
n Syn
and, as required, we have recovered (5.13).
We deduce from the corollary to Lemma 5.3 that Gauss‚ÄìLegendre methods con-
serve quadratic invariants.
Linear invariants are trivially satisÔ¨Åed by all multistep and Runge‚ÄìKutta methods,
yet they are not terribly interesting. Quadratic invariants are probably the simplest
conservation laws that have deeper signiÔ¨Åcance in applications and they include the
important case of diÔ¨Äerential equations evolving on a sphere (e.g. the system (5.1)).
A profound generalization of (5.12) is represented by matrix ODEs of the form
Y ‚Ä≤ = A(t, Y )Y,
t ‚â•0,
Y (0) = Y0 ‚ààO(d),
(5.14)
where A is a Lipschitz function, taking [0, ‚àû) √ó O(d) to so(d); here O(d) is the set
of d √ó d real orthogonal matrices while so(d) denotes the set of d √ó d skew-symmetric
matrices. (The system (5.3) is an example.) It follows at once from the skew-symmetry
of A(t, Y ) that
d
dtY ‚ä§Y = Y ‚Ä≤‚ä§Y + Y ‚ä§Y ‚Ä≤ = Y ‚ä§A‚ä§(t, Y )Y + Y ‚ä§A(t, Y )Y = O.

5.3
Quadratic invariants
85
Therefore Y ‚ä§(t)Y (t) ‚â°I and we deduce that the solution of (5.14) is an orthogonal
matrix. This justiÔ¨Åes the name of orthogonal Ô¨Çow, which we bestow on (5.14). Note
that the invariant Y ‚ä§Y = I generalizes (5.12) since it represents a set of 1
2d(d + 1)
quadratic invariants.
Orthogonal Ô¨Çows feature in numerous applications, underlying the centrality of or-
thogonal matrices in mathematical physics (every physical law must be invariant with
respect to rotation of the frame of reference, and this corresponds to multiplication
by an orthogonal matrix) and in numerical algebra (because working with orthogonal
matrices is the safest and best-conditioned strategy in computation-intensive settings).
Furthermore, being able to solve (5.14) while respecting orthogonality aÔ¨Äords us with
a powerful tool that can be applied to many other problems. Recall, for example,
the isospectral Ô¨Çow (5.2). As we have already seen, its solution can be represented
in the form Y (t) = Q(t)Y0Q‚ä§(t), where the matrix Q is a solution of an orthogonal
Ô¨Çow. Thus, solve (5.3) orthogonally and, subject to simple manipulation, you have an
isospectral solution of (5.2). Another example when an equation is (to use a technical
term) ‚Äòacted‚Äô upon by an orthogonal Ô¨Çow is presented by the three-dimensional system
y‚Ä≤ = a(t, y) √ó y,
(5.15)
where b √ó c is the vector product of b ‚ààR3 and c ‚ààR3: those unaware (or, more
likely, forgetful) of vector analysis might just use the formula
b √ó c = (b2c3 ‚àíb3c2)e1 ‚àí(b1c3 ‚àíb3c1)e2 + (b1c2 ‚àíb2c1)e3,
where e1, e2, e3 ‚ààR3 are unit vectors. Verify that an alternative way of writing the
system (5.15) is
y‚Ä≤ =
‚é°
‚é£
0
‚àía3(t, y)
a2(t, y)
a3(t, y)
0
‚àía1(t, y)
‚àía2(t, y)
a1(t, y)
0
‚é§
‚é¶y = A(t, y)y
(5.16)
and note that the 3 √ó 3 matrix function A is skew-symmetric! We have already seen
an example, namely the system (5.1), for which
a‚ä§(t, y) = [ ‚àí1
20y1
‚àíy1y2
‚àíy3 sin t ].
Solutions of (5.15) evolve on a sphere: if ‚à•y(0)‚à•= 1 then a unit norm is maintained
by the solution y(t) for all t ‚â•0. This follows at once from the skew-symmetry of A,
replicating the argument that we used to analyse the ODE system (5.1). Thus,
1
2
d
dt‚à•y‚à•2 = y‚ä§y‚Ä≤ = y‚ä§A(y)y = 0.
Now, given any two points Œ± and Œ≤ on the unit sphere there exists a matrix R ‚ààO(3)
such that Œ≤ = RŒ±. This justiÔ¨Åes the following construction: we seek a suÔ¨Éciently
smooth function Q such that Q(t) ‚ààO(d) and y(t) = Q(t)y0. Substitution into (5.16)
demonstrates easily that Q‚Ä≤ = A(t, Qy0)Q, Q(0) = I, which Ô¨Åts the pattern (5.14) of
orthogonal Ô¨Çows.

86
Geometric numerical integration
Suppose that we have a numerical method that is guaranteed to respect the orthog-
onal structure of an orthogonal Ô¨Çow. Then immediately, and at no extra cost (well,
almost none) we have a method that respects the geometric structure of all equations
that are ‚Äòacted‚Äô upon by orthogonality, e.g. isospectral Ô¨Çows and equations on spheres.
This motivates a strong interest in discretization methods with this feature.
Which Runge‚ÄìKutta methods can solve (5.14) while keeping the solution ortho-
gonal? No prizes for guessing: the condition is again M = O and the proof is identical
to that of Theorem 5.4: just replace bold-faced by capital letters and S by I, and
everything follows in short order.
3 Lie-group equations
Think for a moment about the set of all d √ó d real
orthogonal matrices O(d). Such a set has two important features, analytic
and algebraic. From the analytic standpoint it is a manifold, a portion of Rd2
(since d√ód matrices can be embedded in Rd2) which can be locally linearized
and such that the resulting ‚Äòlinearizing mappings‚Äô can be smoothly stitched
together. Algebraically, it is a group: if U, V ‚ààO(d) then U ‚àí1, UV ‚ààO(d)
and it is easy to verify the standard group axioms. A manifold endowed with
a group structure is called a Lie group.
Lie groups are an important mathematical concept and their applications
range from number theory all the way to mathematical physics. Perhaps their
most important use is as a powerful searchlight to illuminate and analyse
symmetries of diÔ¨Äerential equations. Many Lie groups, like the orthogonal
group O(d) or the special linear group SL(d) of all d √ó d real matrices with
unit determinant, are composed of matrices.
Numerous diÔ¨Äerential equations of interest evolve on Lie groups: an orthog-
onal Ô¨Çow is just one example.
Such equations can be characterized in a
manner similar to (5.14).
Dispensing altogether with proofs, we consider
all diÔ¨Äerentiable curves X(t) evolving on a matrix Lie group G and passing
through the identity I ‚ààG.
Each such curve can be written in the form
X(t) = I +tA+O

t2
. We denote the set of all such As by g. It is possible to
prove that g is a linear space, closed under a skew-symmetric operation which,
in the case of matrix Lie groups, is the familiar commutation operation: if
A, B ‚ààg then [A, B] = AB ‚àíBA ‚ààg. Such a set is known as a Lie algebra.
In particular, the Lie algebra corresponding to O(d) is so(d) (can you prove
it?), while the Lie algebra of SL(d) comprises the set of d √ó d real matrices
with zero trace.
It is possible to prove that an equation of the form (5.14) evolves in G, provided
that Y0 ‚ààG and A : [0, ‚àû) √ó G ‚Üíg. It is seductive to believe, thus, that a
Runge‚ÄìKutta method is bound to respect any Lie-group structure, provided
that M = O. Unfortunately, this is not true and so we may not infer in this
manner from orthogonal Ô¨Çows to general Lie-group equations. Methods that,
by design, respect an arbitrary Lie-group structure are outside the scope of
this book, although we comment upon them further later in this chapter.
3

5.4
Hamiltonian systems
87
5.4
Hamiltonian systems
A huge number of ODE systems in applications ranging from mechanics to molecular
dynamics, Ô¨Çuid mechanics, quantum mechanics, image processing, celestial mechanics,
nuclear engineering and beyond can be formulated as Hamiltonian equations
p‚Ä≤ = ‚àí‚àÇH(p, q)
‚àÇq
,
q‚Ä≤ = ‚àÇH(p, q)
‚àÇp
.
(5.17)
Here the scalar function H is the Hamiltonian energy.
Both p and q are vector
functions of d variables. In typical applications, d is the number of degrees of freedom
of a mechanical system while q and p correspond to generalized positions and momenta
respectively.
Lemma 5.5
The Hamiltonian energy H(p(t), q(t)) remains constant along the so-
lution trajectory.
Proof
By straightforward diÔ¨Äerentiation of H(p(t), q(t)) and substitution of the
ODEs (5.17) we obtain
d
dtH(p, q) =
	‚àÇH
‚àÇp

‚ä§
p‚Ä≤ +
	‚àÇH
‚àÇq

‚ä§
q‚Ä≤ = ‚àí
	‚àÇH
‚àÇp

‚ä§	‚àÇH
‚àÇq

+
	‚àÇH
‚àÇq

‚ä§	‚àÇH
‚àÇp

= 0.
As a consequence of the lemma, Hamiltonian systems evolve along surfaces of
constant Hamiltonian energy H, and this is demonstrated vividly in Fig. 5.3, where
we display phase planes of the equations y‚Ä≤‚Ä≤ + sin y = 0 and y‚Ä≤‚Ä≤ + y sin y = 0. Both are
examples of harmonic oscillators
y‚Ä≤‚Ä≤ + a(y) = 0
and each can be easily converted into a Hamiltonian system with a single degree of
freedom by letting p = y‚Ä≤, q = y and H(p, q) = 1
2p2 ‚àí
! q
0 a(Œæ) dŒæ. The Ô¨Ågures indicate
a great deal of additional structure, in particular that (except when the initial value is
a Ô¨Åxed point of the equation) the motion is periodic. This is true for many, although
by no means all, Hamiltonian systems.
By this stage we might be tempted to utilize the lesson we have learnt from the
previous section. We have an invariant (and one important enough to deserve the
grand name of ‚ÄòHamiltonian energy‚Äô): let us seek numerical methods to preserve it!
However, rushing headlong into this course of action will be a mistake, because Hamil-
tonian systems (5.17) have another geometric feature, which is even more important:
they are symplectic.
Before we deÔ¨Åne symplecticity it is a good idea to provide some geometric intuition,
hence see Fig. 5.4. Given an autonomous diÔ¨Äerential equation y‚Ä≤ = f(y), we say that
the Ô¨Çow map œït(y0) is the function taking the initial value y0 to the vector y(t).

88
Geometric numerical integration
‚àí3
‚àí2
‚àí1
0
1
2
3
‚àí3
‚àí2
‚àí1
0
1
2
3
‚àí6
‚àí4
‚àí2
0
2
4
6
8
‚àí6
‚àí4
‚àí2
0
2
4
6
Figure 5.3
Phase planes of two nonlinear harmonic oscillators, y‚Ä≤‚Ä≤ + sin y = 0 (on
the left) and y‚Ä≤‚Ä≤ + y sin y = 0 (on the right).
The deÔ¨Ånition of a Ô¨Çow map can be extended from vectors in Rd to measurable sets
‚Ñ¶‚äÇRd (roughly speaking, a subset of Rd is measurable if its volume is well deÔ¨Åned).
Thus,
œït(‚Ñ¶) = {y(t) : y(0) ‚àà‚Ñ¶}.
Let ‚Ñ¶= {y ‚ààR2 : (y1‚àí8
5)2+y2
2 ‚â§2
5}. In Fig. 5.4 we display œït(‚Ñ¶) for t = 0, 1, . . . , 6,
for the Hamiltonian equation y‚Ä≤‚Ä≤ + sin y = 0. The blobs march clockwise and become
increasingly distorted. However, their area stays constant! This is a one-degree-of-
freedom manifestation of symplecticity: the Ô¨Çow map for Hamiltonian systems with
d = 1 is area-preserving.
With greater generality (and a moderate amount of hand-waving) we say that a
function œï : ‚Ñ¶‚ÜíR2d, where ‚Ñ¶‚äÜR2d, is symplectic if Œ¶‚ä§(y)JŒ¶(y) = J for every
y ‚àà‚Ñ¶, where
Œ¶(y) = ‚àÇœï(y)
‚àÇy
and
J =

O
I
‚àíI
O

.
The interpretation of symplecticity (and here hand waving comes in!) is as follows. If
d = 1 then it corresponds to the preservation of the (oriented) area of a measurable
set ‚Ñ¶. If d ‚â•2, the situation is somewhat more complicated: defying intuition, area
does not translate into volume! Instead, deÔ¨Åne the d two-dimensional sets
‚Ñ¶k =
#
œâk
œâk+d

œâ ‚àà‚Ñ¶
$
,
k = 1, . . . , d.
Then symplecticity corresponds to the conservation of
area ‚Ñ¶1 + area ‚Ñ¶2 + ¬∑ ¬∑ ¬∑ + area ‚Ñ¶d.

5.4
Hamiltonian systems
89
‚àí2.0
‚àí1.5
‚àí1.0
‚àí0.5
0
0.5
1.0
1.5
2.0
‚àí2.0
‚àí1.5
‚àí1.0
‚àí0.5
0
0.5
1.0
1.5
2.0
Figure 5.4
The blob story: a disc at the phase plane of the nonlinear pendulum
equation y‚Ä≤‚Ä≤ + sin y = 0, centred at ( 8
5, 0) with radius
2
5, is mapped by unit time
intervals with the Ô¨Çow map. The outcome is a progression of blobs, all having the
same area.
Theorem 5.6 (The Poincar¬¥e theorem)
If H is twice continuously diÔ¨Äerentiable
then the Ô¨Çow map œït of the Hamiltonian system (5.17) is symplectic.
Proof
It is convenient to rewrite the Hamiltonian system (5.17) in the form
y‚Ä≤ = J‚àí1‚àáH(y),
where
y =

p
q

.
(5.18)
We denote the Jacobian of œï(y) by Œ¶t(y) and observe that
dœït(y)
dt
= J‚àí1‚àáH(œït(y))
implies that
dŒ¶t(y)
dt
= J‚àí1‚àá2H(œït(y))Œ¶t(y).
Therefore
d
dt(Œ¶‚ä§
t JŒ¶t) =
	 dŒ¶t
dt

‚ä§
JŒ¶t + Œ¶‚ä§
t J
	 dŒ¶t
dt

= Œ¶‚ä§
t ‚àá2H(œït)J‚àí‚ä§JŒ¶t + Œ¶‚ä§
t JJ‚àí1‚àá2H(œït)Œ¶t
= ‚àíŒ¶‚ä§
t ‚àá2H(œït)Œ¶t + Œ¶‚ä§
t ‚àá2H(œït)Œ¶t = O,
since J‚àí‚ä§J = ‚àíI. Therefore
Œ¶‚ä§
t JŒ¶t ‚â°Œ¶‚ä§
0 JŒ¶0 = J,

90
Geometric numerical integration
because œÜ0 = I. The theorem follows.
On the face of it, symplecticity is an obscure concept: why should we care that
a sum of two-dimensional areas is conserved? Is it not more important that (5.17)
conserves Hamiltonian energy? Or, for that matter, the (2d)-dimensional volume of
‚Ñ¶? (Yes, it conserves it.) However, symplecticity trumps all the many other geometric
features of Hamiltonian systems for the simple reason that it is, in a deep sense, the
same as Hamiltonicity! It is possible to prove that if œït is a symplectic map then it is
the Ô¨Çow of some Hamiltonian system (5.17).
This becomes crucially important when a Hamiltonian system is discretized by a
numerical method. If this method is symplectic (in other words, the function œàh,
where yn+1 = œàh(yn), is a symplectic map) then it is an exact solution of some
Hamiltonian system. Hopefully, this ‚Äònumerical Hamiltonian‚Äô shares enough properties
of the original system, hence symplecticity ensures good rendition of other geometric
features.
To demonstrate this, we will solve the nonlinear pendulum equation y‚Ä≤‚Ä≤ + sin y =
0 with two RK schemes: the Gauss‚ÄìLegendre method (also known as the implicit
midpoint rule)
1
2
1
2
1 ,
(5.19)
which is symplectic and of order 2; and the Nystrom method
0
2
3
2
3
2
3
0
2
3
1
4
3
8
3
8
,
of order 3 but, alas, not symplectic. The solutions produced by both methods, em-
ploying a constant step size h =
1
10, are displayed in Fig. 5.5 and, on the face of it,
they look virtually identical. However, the entire picture changes once we examine
the conservation of Hamiltonian energy in long-term integration.
In Fig. 5.6 we plot the numerical Hamiltonian energy produced by both methods
in 10 000 steps of size h =
1
10. As rendered by the Nystrom method, the energy slopes
rapidly and soon leaves the plot altogether: this is obviously wrong. The implicit
midpoint rule, however, produces an energy which, although not constant, is almost
so. It oscillates within a very tight band centred on the exact constant energy ‚àícos 1
corresponding to an intial value y(0) = [1, 0]‚ä§. Thus, although the numerical energy
is not constant, it is almost so, and this behaviour persists for a very long time.
The lesson of the humble nonlinear pendulum is not over, however. In Fig. 5.7 we
plot the absolute errors accumulated by the Nystrom and implicit midpoint methods,
again applied with a constant step size h =
1
10.
We note that the error of Nys-
trom is larger, although the method is of higher order.
More careful examination
of the Ô¨Ågure illuminates the reason underlying this surprising diÔ¨Äerence. While the
error in the Nystrom method accumulates quadratically, the implicit midpoint rule
yields linear error growth. Of course, a Ô¨Ågure proves nothing yet it does manifest

5.4
Hamiltonian systems
91
0
10
20
30
40
50
60
70
80
90
100
‚àí1.0
‚àí0.5
0
0.5
1.0
0
10
20
30
40
50
60
70
80
90
100
‚àí1.0
‚àí0.5
0
0.5
1.0
Figure 5.5
Numerical solution, with constant step h =
1
10, of y‚Ä≤‚Ä≤ + sin y = 0 with
the implicit midpoint rule (upper plot) and with the Nystrom method (lower plot).
behaviour that can be analysed and proved. Symplectic methods in general accumu-
late error more slowly. Unfortunately, the mathematical techniques underlying this
phenomenon, i.e. the Kolmogorov‚ÄìArnold‚ÄìMoser theory, backward error analysis and
the theory of modulated Fourier expansions, are beyond the scope of this textbook.
Symplectic methods thus possess a number of important advantages ranging be-
yond the formal conservation of the symplectic invariant. Yet, there is a catch. To
reap the advantages of numerical symplecticity, we must solve the equation with a
constant step size, in deÔ¨Åance of all the words of wisdom and error-control strategies
that you will read in Chapters 6 and 7. (There do exist, as a matter of fact, strategies
for variable-step implementations which maintain the beneÔ¨Åts of symplecticity, but
they are fairly complicated and of limited utility.)
So far, except for an ex cathedra claim that the implicit midpoint method (5.19) is
symplectic, we have said absolutely nothing with regard to identifying symplecticity.
As in the case of algebraic stability and of the conservation of quadratic invariants,
we need a clear, easily veriÔ¨Åable, criterion to tell us whether a Runge‚ÄìKutta method
is symplectic. Careful readers of this chapter might suspect by now that the famous
matrix M is just about to make its appearance, and such readers will not be disap-
pointed.
Theorem 5.7
If M = O then the Runge‚ÄìKutta method is symplectic.
Proof
There are several ways of proving this assertion, e.g. by using exterior prod-
ucts or a generating function. Here we limit ourselves to familiar tools and method-

92
Geometric numerical integration
0
100
200
300
400
500
600
700
800
900
1000
‚àí0.5420
‚àí0.5418
‚àí0.5416
‚àí0.5414
‚àí0.5412
‚àí0.5410
‚àí0.5408
‚àí0.5406
‚àí0.5404
‚àí0.5402
Figure 5.6
The Hamiltonian energy
1
2y2
n,2 ‚àícos yn,1, as rendered by the im-
plicit midpoint rule (the narrow band extending across the top of the Ô¨Ågure) and the
Nystrom method (the steeply sloping line).
ology: indeed, under several layers of makeup, replicate the proof of Theorem 5.4.
Thus, we apply the RK method
Œæk = f

tn + ckh, yn + h
ŒΩ

‚Ñì=1
ak,lŒæ‚Ñì

,
k = 1, 2, . . . , ŒΩ,
yn+1 = yn + h
ŒΩ

k=1
bkŒæk
to a Hamiltonian system written in the form (5.18). Letting Œ®n = ‚àÇyn/‚àÇy0, symplec-
ticity means that
Œ®‚ä§
n+1JŒ®n+1 = Œ®‚ä§
n JŒ®n,
n = 0, 1, . . .
(5.20)
Let
Œûk = ‚àÇŒæk
‚àÇy0
,
Gk = ‚àá2H

tn + ckh, yn + h
ŒΩ

‚Ñì=1
ak,‚ÑìŒæ‚Ñì

,
k = 1, 2, . . . , ŒΩ
and assume, to make matters simpler, that the symmetric matrices G1, . . . , GŒΩ are
nonsingular. Now
Œ®n+1 = Œ®n + h
ŒΩ

k=1
bkŒûk
and therefore
Œ®‚ä§
n+1JŒ®n+1 =

Œ®n + h
ŒΩ

k=1
bkŒûk
‚ä§
J

Œ®n + h
ŒΩ

k=1
bkŒûk


5.4
Hamiltonian systems
93
0
200
400
600
800
1000
1200
‚àí2
‚àí1
0
1
2
The Nystrom method
0
200
400
600
800
1000
1200
‚àí2
‚àí1
0
1
2
The implicit midpoint method
Figure 5.7
The absolute error for the nonlinear pendulum, as produced by the
Nystrom method and the implicit midpoint method.
= Œ®‚ä§
n JŒ®n + h
ŒΩ

k=1
bkŒû‚ä§
k JŒ®n + h
ŒΩ

‚Ñì=1
b‚ÑìŒ®‚ä§
n JŒû‚Ñì+ h2
ŒΩ

k=1
ŒΩ

‚Ñì=1
bkb‚ÑìŒû‚ä§
k JŒû‚Ñì.
By direct diÔ¨Äerentiation in the RK method and using the special form of (5.18),
Œûk = J‚àí1Gk

Œ®n + h
ŒΩ

‚Ñì=1
ak,‚ÑìŒû‚Ñì

,
thus
Œ®n = G‚àí1
k JŒûk ‚àíh
ŒΩ

‚Ñì=1
ak,‚ÑìŒûl.
Therefore,
ŒΩ

k=1
bkŒû‚ä§
k JŒ®n =
ŒΩ

k=1
bkŒû‚ä§
k J

G‚àí1
k JŒûk ‚àíh
ŒΩ

‚Ñì=1
ak,‚ÑìŒû‚Ñì

=
ŒΩ

k=1
bkŒû‚ä§
k JG‚àí1
k JŒûk ‚àíh
ŒΩ

k=1
ŒΩ

‚Ñì=1
bkak,‚ÑìŒû‚ä§
k JŒû‚Ñì.

94
Geometric numerical integration
Likewise,
ŒΩ

‚Ñì=1
b‚ÑìŒ®‚ä§
n JŒû‚Ñì=
ŒΩ

‚Ñì=1
b‚Ñì

Œû‚ä§
k J‚ä§G‚àí1
‚Ñì
‚àíh
ŒΩ

k=1
a‚Ñì,kŒû‚ä§
k

JŒû‚Ñì
= ‚àí
ŒΩ

‚Ñì=1
b‚ÑìŒû‚ä§
‚ÑìJG‚àí1
‚ÑìJŒû‚Ñì‚àíh
ŒΩ

k=1
ŒΩ

‚Ñì=1
b‚Ñìa‚Ñì,kŒû‚ä§
k JŒû‚Ñì,
since J‚ä§= ‚àíJ. Therefore
Œ®‚ä§
n+1JŒ®n+1 = Œ®‚ä§
n JŒ®n ‚àíh2
ŒΩ

k=1
ŒΩ

‚Ñì=1
(bkak,‚Ñì+ b‚Ñìa‚Ñì,k ‚àíbkb‚Ñì)Œû‚ä§
k JŒû‚Ñì
= Œ®‚ä§
n JŒ®n ‚àíh2
ŒΩ

k=1
ŒΩ

‚Ñì=1
mk,‚ÑìŒû‚ä§
k JŒû‚Ñì
and symplecticity (5.20) follows, since M = O.
There are many symplectic methods that do not Ô¨Åt the pattern (3.9) of Runge‚Äì
Kutta methods. An important subset of Hamiltonian problems, which deserves spe-
cialized methods of its own, includes systems (5.17) with separable Hamiltonian energy,
H(p, q) = T(p) + V (q),
in other words,
p‚Ä≤ = ‚àí‚àÇV (q)
‚àÇq
,
q‚Ä≤ = ‚àÇT(p)
‚àÇp
.
(5.21)
Such systems are ubiquitous in mechanics, where T and V correspond to the kinetic
and potential energy respectively of a mechanical system.
It is possible to discretize (5.21) using two distinct Runge‚ÄìKutta methods: one
applied to the p‚Ä≤ equation and the other to the q‚Ä≤ equation. The great beneÔ¨Åt of this
approach is that there exist corresponding partitioned RK methods, which are both
symplectic and explicit, making symplectic computation considerably more aÔ¨Äordable.
Another useful technique, providing a means of deriving higher-order symplectic
methods from lower-order ones, is composition. We illustrate it with a simple example,
without any proof. Recall that the implicit midpoint rule, that is, the one-stage Gauss‚Äì
Legendre Runge‚ÄìKutta method
yn+1 = yn + hf( 1
2(yn + yn+1)),
(5.22)
is symplectic. Unfortunately, it is of order 2 and in practice we might wish to employ
a higher-order method. The Yoshida method involves three steps of (5.22), according
to the pattern
t
t
*

j
tn
tn+1

Comments and bibliography
95
Thus, we advance from tn with a step Œ±h, say, where Œ± > 1, then turn and time-step
backwards with a step (2Œ± ‚àí1)h and, Ô¨Ånally, advance again with a step Œ±h. By the
end of this ‚ÄòYoshida shuÔ¨Ñe‚Äô, we are at tn+1. Moreover, provided that we were clever
enough to choose Œ± = 1/(2 ‚àí
3‚àö
2), by the end of the journey we have a fourth-order
symplectic method!
There is a notable absentee at our feast: multistep methods. Indeed, these methods
are of little use when the conservation of geometric structure is at issue. This, needless
to say, does not detract from their many other uses in the numerical solution of ODEs.
Comments and bibliography
In an ideal world, we would have kicked oÔ¨Äwith a chapter on computational dynamics, fol-
lowed by one on diÔ¨Äerential algebraic equations. Then we would have laid down meticulously,
in the language of diÔ¨Äerential geometry, the mathematical foundations of geometric numerical
integration (GNI) and followed this with chapters on Lie-group methods and on Hamiltonian
systems. But then, in an ideal mathematical world, on Planet Pedant, all books have at least
100 chapters and undergraduate studies extend for 15 years. Down on Planet Earth we are
forced to compromise, condense and occasionally even hand-wave. Thus, if your impression
by the end of this chapter is that you know enough of GNI to comprehend what it is roughly
all about but not enough to claim real expertise, we have struck the right note.
The understanding that computational dynamics is important was implicit in numerical
ODE research from the early days. True, as we saw in Chapter 4, the standard classical
stability model was linear but there was the realization that this was just an initial step. The
monotone model (5.5) was the Ô¨Årst framework for the rigorous numerical analysis of nonlinear
ODEs.
As with most other fundamental ideas in numerical ODEs, it was formulated by
Germund Dahlquist, who proceeded to analyse multistep methods in this setting; essentially,
he proved that A-stability is suÔ¨Écient for the stable solution of monotone equations by
multistep methods. Insofar as Runge‚ÄìKutta methods are concerned, numerical lore has it
that in 1975 John Butcher heard for the Ô¨Årst time of the monotone model, at a conference
in Scotland. He then proved Theorem 5.2 during the Ô¨Çight from London back to his native
New Zealand: a triumph of the mathematical mind over the discomforts of long-haul travel.
The monotone model was a convenient focus for nonlinear stability analysis for a decade,
until numerical analysts (together with everybody else) became aware of nonlinear dynamical
systems. This has led to many more powerful models for nonlinear behaviour, e.g.
‚ü®u, f(u)‚ü©‚â§Œ± ‚àíŒ≤‚à•u‚à•,
u ‚ààRd,
(5.23)
where Œ±, Œ≤ > 0. If (5.4) satisÔ¨Åes this inequality and g(t) = 1
2‚à•y(t)‚à•2 then
g‚Ä≤(t) = ‚ü®y(t), f(y(t))‚ü©‚â§Œ± ‚àíŒ≤‚à•y(t)‚à•2 = Œ± ‚àí2Œ≤g(t).
Hence g ‚â•0 satisÔ¨Åes the diÔ¨Äerential inequality g‚Ä≤ ‚â§‚àí2Œ≤g + Œ±; thus
g(t) ‚â§Œ±
2Œ≤ +

g(0) ‚àíŒ±
2Œ≤

e‚àí2Œ≤t,
t ‚â•0
and g(t) ‚â§max{Œ±/(2Œ≤), g(0)}. Therefore the solution y(t) evolves within a bounded ball in
Rd. However, within this ball there is a great deal of freedom for the solution to do things
strange and wonderful: like a monotone equation it might tend to a Ô¨Åxed point, but there
is nothing to stop it from being periodic, quasi-periodic or chaotic. So, what is the magic

96
Geometric numerical integration
condition which makes Runge‚ÄìKutta methods respect (5.23)? If your guess is M = O, you
are right on the money . . . The monograph of Stuart and Humphries (1996) is a detailed
compendium of the computational dynamics of numerical methods and includes a long list
of diÔ¨Äerent nonlinear stability models.
Numerical analysts found it natural to adopt the ideas of computational dynamics, since
they chimed with standard numerical theory. This was emphatically not the case with GNI.
With the single honourable exception of Feng Kang and his group at the Chinese Academy
of Sciences in Beijing, numerical analysts were too besotted with accuracy as the main or-
ganizing principle of computation to realize that qualitative and geometric attributes are
important too.
And important they were, since researchers in quantum chemistry, celes-
tial mechanics and reactor physics had been using rudimentary GNI methods for decades,
compelled by the nature of their diÔ¨Äerential equations. Organized, concerted research into
GNI commenced only in the late 1980s although, in fairness, it soon became the mainstay of
contemporary ODE research (GNI methods for PDEs are at a more tentative stage).
An alternative to the manifold-hugging methods of Section 5.4 is the formalism of diÔ¨Äe-
rential-algebraic equations (DAEs). Without striving at generality, a typical DAE might be
of the form
y‚Ä≤ = f(y, x),
0 = g(y, x),
t ‚â•0,
y(0) = y0 ‚ààRd1,
x(0) = x0 ‚ààRd2,
(5.24)
where the Jacobian ‚àÇg/‚àÇx is nonsingular. One interpetation of (5.24) is that the solution
evolves on the manifold determined by the level set g = 0.
Alternatively, establishing a
connection with the material of Section 5.4, DAEs can be interpreted as the limiting case of
the ODEs
y‚Ä≤ = f(y, x),
Œµx‚Ä≤ = g(y, x),
t ‚â•0,
y(0) = y0 ‚ààRd1,
x(0) = x0 ‚ààRd2
for Œµ ‚Üí0; in other words, DAEs are stiÔ¨Äequations with inÔ¨Ånite stiÔ¨Äness. Following this logic
leads us to BDF methods (see Lemma 2.3), which are in a sense ideally adjusted to ‚ÄòinÔ¨Ånite
stiÔ¨Äness‚Äô and which can be extended to the DAEs (5.24). A good reference on DAEs is Hairer
et al. (1991).
The narrative of Section 5.4 is centred around orthogonal Ô¨Çows, but we have commented
already on the considerably more general framework of Lie groups. Thus, assume that a
matrix function Y satisÔ¨Åes the ODE (5.14), except that Y0 ‚ààG and the matrix function A
maps G to its Lie algebra g. An important fact about matrix Lie groups and Lie algebras
is that if X ‚ààg then eX ‚ààG; the exponential of a matrix was deÔ¨Åned in Chapter 2. Now
suppose that, in place of (5.14), we formulate an equation evolving in a Lie algebra. Unlike
G, the Lie algebra g is a linear space! As long as we restrict ourselves to linear combinations
and to the computation of matrix commutators (recall that g is closed under commutation),
we cannot go wrong and, no matter what we do, we will stay in a Lie algebra ‚Äì whence
exponentiation takes us back to the Lie group and our numerical solution stays in G. The
challenge is thus to reformulate (5.14) in a Lie-algebraic setting, and it is answered by the
‚Äòdexpinv‚Äô equation
‚Ñ¶‚Ä≤
=
A(t, e‚Ñ¶Y0) ‚àí1
2[‚Ñ¶, A(t, e‚Ñ¶Y0)] +
1
12[‚Ñ¶, [‚Ñ¶, A(t, e‚Ñ¶Y0)]]
‚àí
1
720[‚Ñ¶, [‚Ñ¶, [‚Ñ¶, [‚Ñ¶, A(t, e‚Ñ¶Y0)]]]] + ¬∑ ¬∑ ¬∑ ,
t ‚â•0,
‚Ñ¶(0) = O.
(5.25)
Note that here we are indeed using only the permitted operations of commutation and linear
combination. Once we have computed ‚Ñ¶, we have Y (t) = e‚Ñ¶(t)Y0. The idea, known as the
Runge‚ÄìKutta‚ÄìMunthe-Kaas method, abbreviated to the somewhat more melodic acronym
RKMK, is to apply a Runge‚ÄìKutta method to an appropriately truncated equation (5.25). As

Comments and bibliography
97
an example, consider the Nystrom method of Section 3.2. Applied directly to the Lie-group
equation (5.14), it reads
Œû1 = hA(tn, Yn)Yn,
Œû2 = hA(tn + 2
3h, Yn + 2
3Œû1)(Yn + 2
3hŒû1),
Œû3 = hA(tn + 2
3h, Yn + 2
3Œû2)(Yn + 2
3hŒû2),
Yn+1 = Yn + 1
4Œû1 + 3
8Œû2 + 3
8Œû3,
and there is absolutely no reason why should it evolve in the Lie group. However, at the
Lie-algebra level, when applied to (5.25) the same method becomes
Œû1 = hA(tn, Yn),
F1 = Œû1,
Œò2 = ‚àí2
3Œû1,
Œû2 = hA(tn + 2
3h, eŒò2Yn),
F2 = Œû2 ‚àí1
2[Œò2, Œû2],
Œò3 = ‚àí2
3Œû2,
Œû3 = hA(tn + 2
3h, eŒò3Yn),
F3 = Œû3 ‚àí1
2[Œò3, Œû3],
Yn+1 = eF1/4+3F2/8+3F3/8Yn.
The method is explicit, of order 3, requires just three function evaluations of A and is
guaranteed to stay in any Lie group. This is but one example of the many Lie-group methods
reviewed in Iserles et al. (2000).
Hamiltonian equations are central to research into mechanics and most relevant ODEs in
this area are Hamiltonian, although often phrased in the equivalent Lagrangian formulation.
Marsden & Ratiu (1999) is a good introduction to this fascinating area but beware: you will
need to master some diÔ¨Äerential-geometric formalism to understand what is going on! In
this volume we have tried to avoid any mention of diÔ¨Äerential geometry beyond that which a
reasonable undergraduate at a reasonable university would have encountered, but any serious
treatment of this subject is bound to employ more sophisticated terminology. This is the
moment to remind long-suÔ¨Äering students that ‚Äòheavy-duty‚Äô mathematical formalism might
be tough to master but, once understood, makes life much easier! Proving Theorem 5.7 with
exterior products would have been easy, virtually a repeat of the proof of Theorem 5.4.
An early, yet very readable exposition of the numerical aspects of Hamiltonian equa-
tions is Sanz-Serna & Calvo (1994). The most comprehensive and authoritative treatment
of the subject is Hairer et al. (2006), while Leimkuhler & Reich (2004) focuses on the vital
connection between numerical theory and the practical applications of Hamiltonian systems.
The satisfactory implementation of a numerical method for a diÔ¨Écult problem consists of
much more than just pulling an algorithm oÔ¨Äthe shelf. It is imperative to understand the
application just as much as we understand the computation, in order to ask the right ques-
tions, ascertain the correct requirements and ultimately produce a computational solution
that really addresses the problem at hand.
Runge‚ÄìKutta methods are but one (exceedingly eÔ¨Äective) means for computing Hamilto-
nian equations symplectically. The list below gives a selection of other techniques that have
attracted much attention in the last few years.
‚Ä¢ The generating-function method is natural within the diÔ¨Äerential-geometric Hamilto-
nian formalism (which is precisely why we do not propose to explain it here).
Its
disadvantage is its lesser generality: essentially, each Hamiltonian requires a separate
expansion. This has an important application to the production of Ph.D. dissertations
and scientiÔ¨Åc papers, less so to practical computation.
‚Ä¢ We have already mentioned partitioned Runge‚ÄìKutta methods. An elementary exam-
ple of such methods, applicable to Hamiltonians of the form H(p, q) = 1
2p‚ä§p + V (q),

98
Geometric numerical integration
is the St¬®ormer‚ÄìVerlet method, to which we return in Chapter 16 in a diÔ¨Äerent con-
text (and with an abbreviated name, the St¬®ormer method; Verlet proved that it is
symplectic):
qn+1 ‚àí2qn + qn‚àí1 + h2 ‚àÇV (qn)
‚àÇq
= 0.
(5.26)
Now, before you exclaim ‚Äòbut this is a multistep method!‚Äô
or query where the p
variables have gone, note Ô¨Årst that our Hamiltonian equations p‚Ä≤ = ‚àÇV (q)/‚àÇq, q‚Ä≤ = p
easily yield the second-order system q‚Ä≤‚Ä≤ + ‚àÇV (q)/‚àÇq = 0, which is solved by (5.26).
Moreover, the multistep scheme (5.26) can be written in a one-step formulation. Thus,
deÔ¨Åning the numerical momenta as
pn = qn+1 ‚àíqn‚àí1
2h
and
pn+1/2 = qn+1 ‚àíqn
h
,
we can convert (5.26) into pn+1/2 = pn‚àí1/2‚àíh‚àÇV (qn)/‚àÇq. But pn‚àí1/2+pn+1/2 = 2pn
and eliminating pn‚àí1/2 from these two expressions leads to the one-step explicit scheme
pn+ 1
2 = pn ‚àí1
2h‚àÇV (qn)
‚àÇq
,
qn+1 = qn + hpn+1/2,
(5.27)
pn+1 = pn+ 1
2 ‚àí1
2h‚àÇV (qn+1)
‚àÇq
,
a partitioned second-order symplectic Runge‚ÄìKutta method.
The St¬®ormer‚ÄìVerlet
method is probably the most popular symplectic integrator in quantum chemistry
and celestial mechanics, as well as an ideal testing bed for all the diÔ¨Äerent phenomena,
tools and tricks of the trade of computational Hamiltonian dynamics (Hairer et al.,
2003).
‚Ä¢ There is much more to composition methods than our brief mention of the Yoshida
trick at the end of Section 5.4. It is possible to employ similar ideas to boost further
the order of symplectic methods and reduce their error. The beneÔ¨Åts of this approach
are not restricted to the Hamiltonian setting and similar ideas have been applied to the
conservation of volume and to the conservation of arbitrary Ô¨Årst integrals of diÔ¨Äerential
systems (McLachlan & Quispel, 2002).
‚Ä¢ An altogether diÔ¨Äerent approach to the solution of Hamiltonian problems is provided
by variational integrators (Marsden & West, 2001). We have mentioned already the
Lagrangian formulation of Hamiltonian problems: essentially, it is possible to convert
a Hamiltonian problem into a variational one, of the kind that will be considered in
Chapter 9. Now, variational integrators are Ô¨Ånite element methods that act within
the Lagrangian formulation in a manner which, back in the Hamiltonian realm, is
symplectic. This is a very Ô¨Çexible approach with many advantages.
An important spin oÔ¨Äof GNI is a numerical theory of highly oscillatory diÔ¨Äerential
equations. Once an ODE oscillates very rapidly, standard methods (no matter how stable)
force us to use step sizes which are smaller than the shortest period, and this can impose huge
costs on the calculation. (If you do not believe me, try solving the humble Airy equation
y‚Ä≤‚Ä≤ + ty = 0 for large t with any Matlab ODE solver.)
Geometric numerical integration
methods have completely revolutionised the computation of such problems, but this is work
in progress.
Dekker, K. and Verwer, J.G. (1984), Stability of Runge‚ÄìKutta Methods for StiÔ¨ÄNonlinear
DiÔ¨Äerential Equations, North-Holland, Amsterdam.

Exercises
99
Hairer, E. and Wanner, G. (1991), Solving Ordinary DiÔ¨Äerential Equations II: StiÔ¨ÄProblems
and DiÔ¨Äerential-Algebraic Equations, Springer-Verlag, Berlin.
Hairer, E., Lubich, C. and Wanner, G. (2003), Geometric numerical integration illustrated
by the St¬®ormer‚ÄìVerlet method, Acta Numerica 12, 399‚Äì450.
Hairer, E., Lubich, C. and Wanner, G. (2006), Geometric Numerical Integration (2nd edn),
Springer Verlag, Berlin.
Iserles, A., Munthe-Kaas, H.Z., N√∏rsett, S.P. and Zanna, A. (2000), Lie-group methods, Acta
Numerica 9, 215‚Äì365.
Leimkuhler, B. and Reich, S. (2004), Simulating Hamiltonian Dynamics, Cambridge Univer-
sity Press, Cambridge.
Marsden, J.E. and Ratiu, T.S. (1999), Introduction to Mechanics and Symmetry: A Basic
Exposition of Classical Mechanical Systems (2nd edn), Springer Verlag, New York.
Marsden, J.E. and West, M. (2001), Discrete mechanics and variational integrators, Acta
Numerica 10, 357‚Äì514.
McLachlan, R.I. and Quispel, G.R.W. (2002), Splitting methods, Acta Numerica 11, 341‚Äì
434.
Sanz-Serna, J.M. and Calvo, M.P. (1994), Numerical Hamiltonian Problems, Chapman &
Hall, London.
Stuart, A.M. and Humphries, A.R. (1996), Dynamical Systems and Numerical Analysis,
Cambridge University Press, Cambridge.
Exercises
5.1
Consider the linear ODE with variable coeÔ¨Écients
y‚Ä≤ = A(t)y,
t ‚â•0,
where A is a real d √ó d matrix function.
a Prove that the above ODE is monotone if and only if all the eigenvalues
¬µ1(t), . . . , ¬µd(t) of the symmetric matrix B(t) = 1
2[A(t)+A‚ä§(t)] are nonpos-
itive.
b Assuming for simplicity that A(t) ‚â°A, a constant matrix, demonstrate by
a counterexample that it is not enough for all the eigenvalues of A to reside
in the closed left complex half-plane for the equation to be monotone.
c Again let A be a constant matrix and assume further that its eigenvalues are
all real. (This is not necessary for our statement but renders the proof much
easier.) Prove that, provided that the ODE is monotone, all the eigenvalues
of A are in the closed left complex half-plane cl C‚àí.
(You might commence by expanding an eigenvector of A in the basis of eigen-
vectors of B = 1
2(A + A‚ä§).)

100
Geometric numerical integration
5.2
Let y‚Ä≤ = Ay, where A is a constant d√ód matrix. As in the previous exercise,
let B = 1
2(A+A‚ä§). The spectral abscissa of B is ¬µ[B] = max{Œª : Œª ‚ààœÉ(B)}.
(Thus the previous exercise amounted to proving that ¬µ[B] ‚â§0 is necessary
and suÔ¨Écient for monotonicity.)
a Prove that the function œÜ(t) = ‚à•y(t)‚à•2, where ‚à•¬∑ ‚à•is the standard Euclidean
norm, obeys the diÔ¨Äerential inequality œÜ‚Ä≤ ‚â§2¬µ[B]œÜ, and thereby deduce that
‚à•y(t)‚à•‚â§et¬µ[B]‚à•y(0)‚à•.
b Prove that Œ± = ¬µ[B] is the least possible constant such that ‚à•y(t)‚à•‚â§
etŒ±‚à•y(0)‚à•for all possible initial values y(0).
5.3
For which of the following three-stage Runge‚ÄìKutta methods is it true that
the matrix M is positive semideÔ¨Ånite?
a The Ô¨Åfth-order Radau IA method
0
1
9
‚àí1
18 ‚àí
‚àö
6
18
‚àí1
18 +
‚àö
6
18
3
5 ‚àí
‚àö
6
10
1
9
11
45 + 7
‚àö
6
360
11
45 ‚àí43
‚àö
6
360
3
5 +
‚àö
6
10
1
9
11
45 + 43
‚àö
6
360
11
45 ‚àí7
‚àö
6
360
1
9
4
9 +
‚àö
6
36
4
9 ‚àí
‚àö
6
36
,
b The fourth-order Lobatto IIIB method
0
1
6
‚àí1
6
0
1
2
1
6
1
3
0
1
1
6
5
6
0
1
6
2
3
1
6
,
c The fourth-order Lobatto IIIC method
0
1
6
‚àí1
3
1
6
1
2
1
6
5
12
‚àí1
12
1
1
6
2
3
1
6
1
6
2
3
1
6
.
5.4
Consider the ODE y‚Ä≤ = S(y)‚àág(y), where S is a d √ó d skew-symmetric
matrix function and g is a continuously diÔ¨Äerentiable scalar function. Prove
that g is a Ô¨Årst integral of this equation, i.e. that g(y(t)) stays constant for
all t ‚â•0.
(This ‚Äòskew-gradient equation‚Äô is at the root of certain discretization methods
that can be made to respect an arbitrary Ô¨Årst integral g.)
5.5
Our point of departure is the matrix diÔ¨Äerential equation
Y ‚Ä≤ = BY + Y B‚ä§,
t ‚â•0,
Y (0) = Y0,
(5.28)
where the matrix B has zero trace, d
k=1 bk,k = 0.

Exercises
101
a Prove that the solution of (5.28) can be expressed in the form Y (t) =
V (t)Y0V ‚ä§(t), t ‚â•0, where the matrix V is the solution of the ODE
V ‚Ä≤ = BV,
t ‚â•0,
V (0) = I.
(5.29)
b Using the fact that the trace of B is zero, prove that the determinant is an
invariant of (5.29), namely that det V (t) ‚â°1.
c Deduce that det Y (t) ‚â°det Y0 for all t ‚â•0.
5.6
Consider again equation (5.29), recalling that the trace of B is zero and that
the exact solution has unit determinant for all t ‚â•0. Assume that we are
solving it with a Runge‚ÄìKutta method (3.9).
a Suppose that the rational function r is given by the formula (4.13). Prove
that Yn+1 = r(hB)Yn, n = 0, 1, . . .
b Deduce that the condition for det Yn+1 = det Yn is that
d

k=1
r(hŒªk) = 1,
where Œª1, . . . , Œªd are the eigenvalues of B.
(You may assume that B has a full set of eigenvectors.)
c Supposing that the RK method is of order p ‚â•1, prove that
d

k=1
r(hŒªk) = 1 + chp+1
d

k=1
Œªp+1
k
+ O

hp+2
,
c Ã∏= 0.
d Provided that d ‚â•3, demonstrate that there exists a matrix B, consistent
with our assumptions, for which det Yn+1 Ã∏= det Yn for suÔ¨Éciently small step
size h > 0.
5.7
The solution of the linear matrix ODE
Y ‚Ä≤ = A(t)Y,
t ‚â•0,
Y (0) = Y0 ‚ààG,
evolves in the Lie group G, subject to the assumption that A(t) ‚ààg, t ‚â•0,
where g is the corresponding Lie algebra.
a Consider the method
Yn+1 = exp
	 tn+1
tn
A(œÑ) dœÑ

Yn,
n = 0, 1, . . . ,
where exp(¬∑ ¬∑ ¬∑) is the standard matrix exponential. Prove that the method
is of order 2 and that Yn ‚ààG, n = 0, 1, . . .

102
Geometric numerical integration
b Suppose that the integral above is discretized by Gaussian quadrature with
a single node,
 tn+1
tn
A(œÑ) dœÑ ‚âàhA(tn + 1
2h).
Prove that the new method is also of order 2 and that it evolves in G.
c‚ãÜProve that
Yn+1 = exp
	 tn+1
tn
A(œÑ) dœÑ ‚àí1
2
 tn+1
tn
	 œÑ
tn
A(Œ∂) dŒ∂, A(œÑ)

dœÑ

Yn
for n = 0, 1, . . . is a fourth-order method and, again, Yn ‚ààG, n = 0, 1, . . .
5.8
Show that the H¬¥enon‚ÄìHeiles system
p‚Ä≤
1 = ‚àíq1 ‚àí2q2q2,
p‚Ä≤
2 = ‚àíq2 + 2
3q2,
q‚Ä≤
1 = p1,
q‚Ä≤
2 = p2
is Hamiltonian and identify explicitly its Hamiltonian energy.
(The H¬¥enon‚ÄìHeiles system is a famous example of an ODE with chaotic
solutions.)
5.9
Let
c1
a1,1
¬∑ ¬∑ ¬∑
a1,ŒΩ
...
...
...
cŒΩ
aŒΩ,1
¬∑ ¬∑ ¬∑
aŒΩ,ŒΩ
b1
¬∑ ¬∑ ¬∑
bŒΩ
and
Àúc1
Àúa1,1
¬∑ ¬∑ ¬∑
Àúa1,ÀúŒΩ
...
...
...
ÀúcÀúŒΩ
ÀúaÀúŒΩ,1
¬∑ ¬∑ ¬∑
ÀúaÀúŒΩ,ÀúŒΩ
Àúb1
¬∑ ¬∑ ¬∑
ÀúbÀúŒΩ
be two Runge‚ÄìKutta methods. We apply them to the Hamiltonian system
(5.17) for a separable Hamiltonian H(p, q) = T(p) + V (q); the Ô¨Årst method
to the momenta p and the second to the positions q. Thus
rk = pn ‚àíh
ŒΩ

‚Ñì=1
ak,‚Ñì
‚àÇV (s‚Ñì)
‚àÇq
,
k = 1, 2, . . . , ŒΩ,
sk = qn + h
ÀúŒΩ

‚Ñì=1
Àúak,‚Ñì
‚àÇT(r‚Ñì)
‚àÇp
,
k = 1, 2, . . . , ÀúŒΩ,
pn+1 = pn ‚àíh
ŒΩ

k=1
bk
‚àÇV (sk)
‚àÇq
,
qn+1 = qn + h
ÀúŒΩ

k=1
Àúbk
‚àÇT(rk)
‚àÇp
.

Exercises
103
Further assuming that T(p) = 1
2p‚ä§p, prove that the St¬®ormer‚ÄìVerlet method
(5.27) can be written as a partitioned RK method with
1
2
1
2
0
1
2
1
2
0
1
2
1
2
and
0
0
0
1
1
2
1
2
1
2
1
2
.
5.10
The symplectic Euler method for the Hamiltonian system (5.17) reads
pn+1 = pn ‚àíh‚àÇH(pn+1, qn)
‚àÇq
,
qn+1 = qn + h‚àÇH(pn+1, qn)
‚àÇp
.
a Show that this is a Ô¨Årst-order method.
b Prove from basic principles that, as implied by its name, the method is
indeed symplectic.
c Assuming that the Hamiltonian is separable, H(p, q) = T(p) + V (q), show
that the method can be implemented explicitly.


6
Error control
6.1
Numerical software vs. numerical mathematics
There comes a point in every exposition of numerical analysis when the theme shifts
from the familiar mathematical progression of deÔ¨Ånitions, theorems and proofs to
the actual ways and means whereby computational algorithms are implemented. This
point is sometimes accompanied by an air of anguish and perhaps disdain: we abandon
the palace of the Queen of Sciences for the lowly shop Ô¨Çoor of a software engineer.
Nothing could be further from the truth!
Devising an algorithm that fulÔ¨Åls its goal
accurately, robustly and economically is an intellectual challenge equal to the best in
mathematical research.
In Chapters 1‚Äì5 we have seen a multitude of methods for the numerical solution
of the ODE system
y‚Ä≤ = f(t, y),
t ‚â•t0,
y(t0) = y0.
(6.1)
In the present chapter we are about to study how to incorporate a method into a
computational package.
It is important to grasp that, when it comes to software
design, a time-stepping method is just one ‚Äì albeit very important ‚Äì component.
A good analogy is the design of a motor car. The time-stepping method is like the
engine: it powers the vehicle along. A car with just an engine is useless: a multitude of
other components ‚Äì wheels, chassis, transmission ‚Äì are essential for its operation. Now,
the diÔ¨Äerent parts of the system should not be optimized on their own but as a part
of an integrated plan; there is little point in Ô¨Åtting a Formula 1 racing car engine into
a family saloon. Moreover, the very goal of optimization is problem-dependent: do we
want to optimize for speed? economy? reliability? marketability? In a well-designed
car the right components are combined in such a way that they operate together as
required, reliably and smoothly. The same is true for a computational package.
A user of a software package for ODEs, say, typically does not (and should not!)
care about the particular choice of method or, for that matter, the other ‚Äòoperating
parts‚Äô ‚Äì error and step-size control, solution of nonlinear algebraic equations, choice
of starting values and of the initial step, visualization of the numerical solution etc.
As far as a user is concerned, a computational package is simply a tool.
The tool designer ‚Äì be it a numerical analyst or a software engineer ‚Äì must adopt
a more discerning view. The package is no longer a black box but an integrated system,
105

106
Error control
which can be represented in the following Ô¨Çowchart:
software
'
&
$
%
?
??
?
?
f( ¬∑ , ¬∑ )
t0
tend
y0
Œ¥
?
{(tn, yn)}n=0,1,...,nend
The inputs are not just the function f, the starting point t0, the initial value y0 and
the endpoint tend but also the error tolerance Œ¥ > 0; we wish the numerical error in,
say, the Euclidean norm to be within Œ¥. The output is the computed solution sequence
at the points t0 < t1 < ¬∑ ¬∑ ¬∑ < tend, which, of course, are not equi-spaced.
We hasten to say that the above is actually the simplest possible model for a com-
putational package, but it will do for expositional purposes. In general, the user might
be expected to specify whether (6.1) is stiÔ¨Äand to express a range of preferences with
regard to the form of the output. An increasingly important component in the design
of a modern software package is visualization; it is diÔ¨Écult to absorb information from
long lists of numbers and so its display in the form of time series, phase diagrams,
Poincar¬¥e sections etc. often makes a great deal of diÔ¨Äerence.
Altogether, writing,
debugging, testing and documenting modern, advanced, broad-purpose software for
ODEs is a highly professional and time-demanding enterprise.
In the present chapter we plan to elaborate a major component of any computa-
tional package, the mechanism whereby numerical error is estimated in the course of
solution and controlled by means of step size changes. Chapter 7 is devoted to an-
other aspect, namely solution of the nonlinear algebraic systems that occur whenever
implicit methods are applied to the system (6.1).
We will describe a number of diÔ¨Äerent devices for the estimation of the local error,
i.e. the error incurred when we integrate from tn to tn+1 under the assumption that
yn is ‚Äòexact‚Äô. This should not be confused with the global error, namely the diÔ¨Äerence
between yn and y(tn) for all n = 0, 1, . . . , nend. Clever procedures for the estimation
of global error are fast becoming standard in modern software packages.
The error-control devices of this chapter will be applied to three relatively simple
systems of the type (6.1): the van der Pol equation
y‚Ä≤
1 = y2,
y‚Ä≤
2 = (1 ‚àíy2
1)y2 ‚àíy1,
0 ‚â§t ‚â§25,
y1(0) =
1
2,
y2(0) =
1
2;
(6.2)

6.2
The Milne device
107
the Mathieu equation
y‚Ä≤
1 = y2,
y‚Ä≤
2 = ‚àí(2 ‚àícos 2t)y1,
0 ‚â§t ‚â§30,
y1(0) = 1,
y2(0) = 0;
(6.3)
and the Curtiss‚ÄìHirschfelder equation
y‚Ä≤ = ‚àí50(y ‚àícos t),
0 ‚â§t ‚â§10,
y(0) = 1.
(6.4)
The Ô¨Årst two equations are not stiÔ¨Ä, while (6.4) is moderately so. The solution of
the van der Pol equation (which is more commonly written as a second-order equation
y‚Ä≤‚Ä≤ ‚àíŒµ(1 ‚àíy2)y‚Ä≤ + y = 0; here we take Œµ = 1) models electrical circuits connected with
triode oscillators. It is well known that, for every initial value, the solution tends to a
periodic curve (see Fig. 6.1). The Mathieu equation (which, likewise, is usually written
in the second-order form y‚Ä≤‚Ä≤ + (a ‚àíb cos 2t)y = 0, here with a = 2 and b = 1) arises in
the analysis of the vibrations of an elliptic membrane and also in celestial mechanics.
Its (nonperiodic) solution remains forever bounded, without approaching a Ô¨Åxed point
(see Fig. 6.2). Finally, the solution of the Curtiss‚ÄìHirschfelder equation (which has
no known signiÔ¨Åcance except as a good test case for computational algorithms) is
y(t) = 2500
2501 cos t +
50
2501 sin t +
1
2501 e‚àí50t,
t ‚â•0,
and approaches a periodic curve at an exponential speed (see Fig. 6.3).
We assume throughout this chapter that f is as smooth as required.
6.2
The Milne device
Let
s

m=0
amyn+m = h
s

m=0
bmf(tn+m, yn+m),
n = 0, 1, . . . ,
as = 1,
(6.5)
be a given convergent multistep method of order p. The goal of assessing the local
error in (6.5) is attained by employing another convergent multistep method of the
same order, which we write in the form
s

m=q
Àúamxn+m = h
s

m=q
Àúbmf(tn+m, xn+m),
n ‚â•max{0, ‚àíq},
Àúas = 1.
(6.6)
Here q ‚â§s ‚àí1 is an integer, which might be of either sign; the main reason for
allowing negative q is that we wish to align the two methods so that they approx-
imate at the same point tn+s in the nth step. Of course, xn+m = yn+m for m =
min{0, q}, min{0, q} + 1, . . . , s ‚àí1.
According to Theorem 2.1, the method (6.5), say, is of order p if and only if
œÅ(w) ‚àíœÉ(w) ln w = c(w ‚àí1)p+1 + O

|w ‚àí1|p+2
,
w ‚Üí1,

108
Error control
where c Ã∏= 0 and
œÅ(w) =
s

m=0
amwm,
œÉ(w) =
s

m=0
bmwm.
By expanding œà one term further in the proof of Theorem 2.1, it is easy to demonstrate
that, provided yn, yn+1, . . . , yn+s‚àí1 are assumed to be error free,
y(tn+s) ‚àíyn+s = chp+1y(p+1)(tn+s) + O

hp+2
,
h ‚Üí0.
(6.7)
The number c is termed the (local) error constant of the method (6.5).1 Let Àúc be the
error constant of (6.6) and assume that we have selected the method in such a way
that Àúc Ã∏= c. Therefore
y(tn+s) ‚àíxn+s = Àúchp+1y(p+1)(tn+s) + O

hp+2
,
h ‚Üí0.
We subtract this expression from (6.7) and disregard the O

hp+2
terms. The outcome
is xn+s ‚àíyn+s ‚âà(c ‚àíÀúc)hp+1y(p+1)(tn+s), hence
hp+1y(p+1)(tn+s) ‚âà
1
c ‚àíÀúc(xn+s ‚àíyn+s).
Substitution into (6.7) yields an estimate of the local error, namely
y(tn+s) ‚àíyn+s ‚âà
c
c ‚àíÀúc(xn+s ‚àíyn+s).
(6.8)
This method of assessing the local error is known as the Milne device. Recall that our
critical requirement is to maintain the local error at less than the tolerance Œ¥. A naive
approach is error control per step, namely to require that the local error Œ∫ satisÔ¨Åes
Œ∫ ‚â§Œ¥,
where
Œ∫ =

c
c ‚àíÀúc
 ‚à•xn+s ‚àíyn+s‚à•
originates in (6.8). A better requirement, error control per unit step, incorporates a
crude global consideration into the local estimate. It is based on the assumption that
the accumulation of global error occurs roughly at a constant pace and is allied to
our observation in Chapter 1 that the global error behaves like O(hp). Therefore, the
smaller the step size, the more stringent requirement must we place upon Œ∫; the right
inequality is
Œ∫ ‚â§hŒ¥.
(6.9)
This is the criterion that we adopt in the remainder of this exposition.
Suppose that we have executed a single time step, thereby computing a candidate
solution yn+s. We use (6.9), where Œ∫ has been evaluated by the Milne device (or
by other means), to decide whether yn+s is an acceptable approximation to y(tn+s).
1The global error constant is deÔ¨Åned as c/œÅ‚Ä≤(1), for reasons that are related to the theme of
Exercise 2.2 but are outside the scope of our exposition.

6.2
The Milne device
109
If not, the time step is rejected: we go back to tn+s‚àí1, halve h and resume time-
stepping. If, however, (6.9) holds then the new value is acceptable and we advance to
tn+s. Moreover, if Œ∫ is signiÔ¨Åcantly smaller than hŒ¥ ‚Äì for example, if Œ∫ <
1
10hŒ¥ ‚Äì we
take this as an indication that the time step is too small (hence, wasteful) and double
it. A simple scheme of this kind might be conveniently represented in Ô¨Çowchart form:
set h
?
evaluate new y
?




is Œ∫ ‚â§hŒ¥?
?
Y
N
6
halve h
remesh(2)
?




is t ‚â•tend?
?
Y
END
@
@




@
@
N
6
'
&
$
%
is Œ∫ ‚â§
1
10hŒ¥?
6
Y
N
double h
advance t
remesh(3)
-
advance t
remesh(1)
?
?
except that each box in the Ô¨Çowchart hides a multitude of sins! In particular, observe
the need to ‚Äòremesh‚Äô the variables: multistep methods require that starting values for
each step are provided on an equally spaced grid and this is no longer the case when h
is amended. We need then to approximate the starting values, typically by polynomial
interpolation (A.2.2.3‚ÄìA.2.2.5). In each iteration we need ÀÜs := s+max{0, ‚àíq} vectors
yn+min{0,q}, . . . , yn+s‚àí1, which we rename w1, w2, . . . , wÀÜs respectively.
There are
three possible cases:
(1) h is unamended
We let wnew
j
= wj+1, j = 1, 2, . . . , ÀÜs‚àí1, and wnew
ÀÜs
= yn+s.
(2) h is halved
The values wnew
ÀÜs‚àí2j = wÀÜs‚àíj, j = 0, 1, . . . , ‚åäÀÜs/2‚åã‚àí1, survive, while
the rest, which approximate values at the midpoints of the old grid, need to be
computed by interpolation.
(3) h is doubled
Here wnew
ÀÜs
= yn+s+1 and wnew
ÀÜs‚àíj = wÀÜs‚àí2j+1, j = 1, 2, . . . , ÀÜs ‚àí1.
This requires an extra ÀÜs‚àí2 vectors, w‚àíÀÜs+3, . . . , w0, which have not been deÔ¨Åned

110
Error control
above. The remedy is simple, at least in principle: we need to carry forward in
the previous two remeshings at least 2ÀÜs ‚àí1 vectors to allow the scope for step-
size doubling. This procedure may impose a restriction on consecutive step-size
doublings.
A glance at the Ô¨Çowchart aÔ¨Érms our claim in Section 6.1 that the speciÔ¨Åc method
used to advance the time-stepping (the ‚Äòevaluate new y‚Äô box) is just a single instrument
in a large orchestra.
It might well be the Ô¨Årst violin, but the quality of music is
determined not by any one instrument but by the harmony of the whole orchestra
playing in unison.
It is the conductor, not the Ô¨Årst violinist, whose name looms
largest on the billboard!
3 The TR‚ÄìAB2 pair
As a simple example, let us suppose that we employ
the two-step Adams‚ÄìBashforth method
xn+1 ‚àíxn = 1
2h[3f(tn, xn) ‚àíf(tn‚àí1, xn‚àí1)]
(6.10)
to monitor the error of the trapezoidal rule
yn+1 ‚àíyn = 1
2h[f(tn+1, yn+1) + f(tn, yn)].
(6.11)
Therefore ÀÜs = 2, the error constants are c = ‚àí1
12, Àúc =
5
12 and (6.9) becomes
‚à•xn+1 ‚àíyn+1‚à•‚â§6hŒ¥.
Interpolation is required upon step-size halving at a single value of t, namely
at the midpoint between (the old values of) tn‚àí1 and tn:
wnew
1
= 1
8(3w2 + 6w1 ‚àíyn‚àí2).
Figure 6.1 displays the solution of the van der Pol equation (6.2) by the TR‚Äì
AB2 pair with tolerances Œ¥ = 10‚àí3, 10‚àí4. The sequence of step sizes attests to
a marked reluctance on the part of the algorithm to experiment too frequently
with step-doubling. This is healthy behaviour, since an excess of optimism is
bound to breach the inequality (6.9) and is wasteful. Note, by the way, how
strongly the step-size sequence correlates with the size of y‚Ä≤
2, which, for the
van der Pol equation, measures the ‚Äòawkwardness‚Äô of the solution ‚Äì it is easy
to explain this feature from the familiar phase portrait of (6.2).
The global error, as displayed in the bottom two graphs, is of the right order
of magnitude and, as we might expect, slowly accumulates with time. This is
typical of non-stiÔ¨Äproblems like (6.2).
Similar lessons can be drawn from the Mathieu equation (6.3) (see Fig. 6.2).
It is perhaps more diÔ¨Écult to Ô¨Ånd a single characteristic of the solution that
accounts for the variation in h, but it is striking how closely the step sequences
for Œ¥ = 10‚àí3 and Œ¥ = 10‚àí4 correlate. The global error accumulates markedly
faster but is still within what can be expected from the general theory and
the accepted wisdom. The goal being to incur an error of at most Œ¥ in a unit-
length interval, a Ô¨Ånal error of (tend ‚àít0)Œ¥ is to be expected at the right-hand
endpoint of the interval.

6.2
The Milne device
111
0
5
10
15
20
25
0
0.05
0.10
0
5
10
15
20
25
0
0.02
0.04
0
5
10
15
20
25
‚àí3
‚àí2
‚àí1
0
1
2
0
5
10
15
20
25
0
2
4
6 x 10 ‚àí4
0
5
10
15
20
25
0
2
4
6 x 10 ‚àí3
Figure 6.1
The top Ô¨Ågure displays the two solution components of the
van der Pol equation (6.2) in the interval [0, 25].
The other Ô¨Ågures re-
late to the Milne device, applied with the pair (6.10), (6.11) to this equa-
tion. The second and third Ô¨Ågures each feature the sequence of step sizes
for tolerances Œ¥ equal to 10‚àí3 and 10‚àí4 respectively, while the lowest two
Ô¨Ågures show the (exact) global error for these values of Œ¥.

112
Error control
0
5
10
15
20
25
30
0
0.05
0.10
0
5
10
15
20
25
30
0
0.05
0.10
0
5
10
15
20
25
30
‚àí1
0
1
0
5
10
15
20
25
30
0
0.005
0.010
0.015
0.020
0.025
0
5
10
15
20
25
30
0
0.5
1.0
1.5
2.0
2.5 x 10 ‚àí3
Figure 6.2
The top Ô¨Ågure displays the two solution components of the
Mathieu equation (6.3) in the interval [0, 30]. The other Ô¨Ågures are con-
cerned with the Milne device, applied with the pair (6.10), (6.11). The
second and the third Ô¨Ågures each feature the sequence of step sizes for tol-
erances Œ¥ equal to 10‚àí3 and 10‚àí4 respectively, while the lowest two Ô¨Ågures
show the (exact) global error for these values of Œ¥.

6.3
Embedded Runge‚ÄìKutta methods
113
Finally, Fig. 6.3 displays the behaviour of the TR‚ÄìAB2 pair for the mildly
stiÔ¨Äequation (6.4).
The Ô¨Årst interesting observation, looking at the step
sequences, is that the step sizes are quite large, at least for Œ¥ = 10‚àí3. Had
we tried to solve this equation with the Euler method, we would have needed
to impose h <
1
25 to prevent instabilities, whereas the trapezoidal rule chugs
along happily with h occasionally exceeding 1
3. This is an important point to
note since the stability analysis of Chapter 4 has been restricted to constant
steps.
It is worthwhile to record that, at least in a single computational
example, A-stability allows the trapezoidal rule to select step sizes solely in
pursuit of accuracy.
The accumulation of global errors displays a pattern characteristic of stiÔ¨Ä
equations. Provided that the method is adequately stable, global error does
not accumulate at all and is often signiÔ¨Åcantly smaller than Œ¥! (The occasional
jumps in the error in Fig. 6.3 are probably attributable to the increase in h
and might well have been eliminated altogether with suÔ¨Écient Ô¨Åne-tuning of
the computational scheme.)
Note that, of course, (6.10) has exceedingly poor stability characteristics (see
Fig. 4.3).
This is not a handicap, since the Adams‚ÄìBashforth method is
used solely for local error control.
To demonstrate this point, in Fig. 6.4
we display the error for the Curtiss‚ÄìHirschfelder equation (6.4) when, in lieu
of (6.10), we employ the A-stable backward diÔ¨Äerentiation formula method
(2.15). Evidently, not much changes!
3
We conclude this section by remarking again that execution of a variable-step code
requires a multitude of choices and a great deal of Ô¨Åne-tuning. The need for brevity
prevents us from discussing, for example, an appropriate procedure for the choice of
the initial step size and starting values y1, y2, . . . , ys‚àí1.
6.3
Embedded Runge‚ÄìKutta methods
The comfort of a single constant whose magnitude reÔ¨Çects (at least for small h) the
local error Œ∫ is denied us in the case of Runge‚ÄìKutta methods, (3.9). In order to
estimate Œ∫ we need to resort to a diÔ¨Äerent device, which again is based upon running
two methods in tandem ‚Äì one, of order p, to provide a candidate for solution and the
other, of order Àúp ‚â•p + 1, to control the error.
In line with (6.5) and (6.6), we denote by yn+1 the candidate solution at tn+1
obtained from the pth-order method, whereas the solution at tn+1 obtained from the
higher-order scheme is xn+1. We have
yn+1 = Àúy(tn+1) + ‚Ñìhp+1 + O

hp+2
,
(6.12)
xn+1 = Àúy(tn+1) + O

hp+2
,
h ‚Üí0,
(6.13)
where ‚Ñìis a vector that depends on the equation (6.1) (but not upon h) and Àúy is the
exact solution of (6.1) with initial value Àúy(tn) = yn. Subtracting (6.13) from (6.12),

114
Error control
0
1
2
3
4
5
6
7
8
9
10
0
0.2
0.4
0
1
2
3
4
5
6
7
8
9
10
0
0.05
0
1
2
3
4
5
6
7
8
9
10
‚àí1.0
‚àí0.5
0
0.5
1.0
0
1
2
3
4
5
6
7
8
9
10
0
0.05
0.01
0
1
2
3
4
5
6
7
8
9
10
0
0.5
1.0 x 10 ‚àí4
0
1
2
3
4
5
6
7
8
9
10
0
1
2
3
x 10 ‚àí5
0
1
2
3
4
5
6
7
8
9
10
0
2
4 x 10 ‚àí5
Figure 6.3
The top Ô¨Ågure displays the solution of the Curtiss‚ÄìHirschfelder equa-
tion (6.4) in the interval [0, 10]. The other Ô¨Ågures are concerned with the Milne
device, applied with the pair (6.10), (6.11). The second to fourth Ô¨Ågures feature
the sequence of step sizes for tolerances Œ¥ equal to 10‚àí3, 10‚àí4 and 10‚àí5 respectively,
while the bottom three Ô¨Ågures show the (exact) global error for these values of Œ¥.

6.3
Embedded Runge‚ÄìKutta methods
115
0
1
2
3
4
5
6
7
8
9
10
0
1
2 x 10 ‚àí4
0
1
2
3
4
5
6
7
8
9
10
0
1
2
3
x 10 ‚àí5
0
1
2
3
4
5
6
7
8
9
10
0
1
2
3
x 10 ‚àí5
Figure 6.4
Global errors for the numerical solution of the Curtiss‚ÄìHirschfelder
equation (6.4) by the TR‚ÄìBDF2 pair with Œ¥ = 10‚àí3, 10‚àí4, 10‚àí5.
we obtain ‚Ñìhp+1 ‚âàyn+1 ‚àíxn+1, the outcome being the error estimate
Œ∫ = ‚à•yn+1 ‚àíxn+1‚à•.
(6.14)
Once Œ∫ is available, we may proceed as in Section 6.2 except that, having opted for
one-step methods, we are spared all the awkward and time-consuming minutiae of
remeshing each time h is changed.
A naive application of the above approach requires the doubling, at the very least,
of the expense of calculation, since we need to compute both yn+1 and xn+1. This is
unacceptable since, as a rule, the cost of error control should be marginal in comparison
with the cost of the main scheme.2 However, when an ERK method (3.5) is used for
the main scheme it is possible to choose the two methods in such a way that the extra
expense is small. Let us thus denote by
c
A
b‚ä§
and
Àúc
ÀúA
Àúb
‚ä§
the pth-order method and the higher-order method, of ŒΩ and ÀúŒΩ stages respectively.
The main idea is to choose
Àúc =

c
ÀÜc

,
ÀúA =
 A
O
ÀÜA

,
2Note that we have used in Section 6.2 an explicit method, the Adams‚ÄìBashforth scheme, to
control the error of the implicit trapezoidal rule. This is consistent with the latter remark.

116
Error control
where ÀÜc ‚ààRÀúŒΩ‚àíŒΩ and ÀÜA is a (ÀúŒΩ ‚àíŒΩ) √ó ŒΩ matrix, so that ÀúA is strictly lower triangular.
In this case the Ô¨Årst ŒΩ vectors Œæ1, Œæ2, . . . , ŒæŒΩ will be the same in both methods and
the cost of the error controller is virtually the same as the cost of the higher-order
method. We say the the Ô¨Årst method is embedded in the second and that, together,
they form an embedded Runge‚ÄìKutta pair. The tableau notation is
Àúc
ÀúA
b‚ä§
Àúb
‚ä§
.
A well-known example of an embedded RK pair is the Fehlberg method, with p = 4,
Àúp = 5, ŒΩ = 5, ÀúŒΩ = 6 and tableau
0
1
4
1
4
3
8
3
32
9
32
12
13
1932
2197
‚àí7200
2197
7296
2197
1
439
216
‚àí8
3680
513
‚àí845
4104
1
2
‚àí8
27
2
‚àí3544
2565
1859
4104
‚àí11
40
25
216
0
1408
2565
2197
4104
‚àí1
5
16
135
0
6656
12825
28561
56430
‚àí9
50
2
55
.
3 A simple embedded RK pair
The RK pair
0
2
3
2
3
2
3
0
2
3
1
4
3
4
1
4
3
8
3
8
(6.15)
has orders p = 2, Àúp = 3. The local error estimate becomes simply
Œ∫ = 3
8
--f

tn + 2
3h, Œæ3

‚àíf

tn + 2
3h, Œæ2
-- .
We applied a variable-step algorithm based on the above error controller to
the problems (6.2)‚Äì(6.4), within the same framework as the computational
experiments using the Milne device in Section 6.2. The results are reported
in Figs 6.5‚Äì6.7.
Comparing Figs 6.1 and 6.5 demonstrates that, as far as error control is con-
cerned, the performance of (6.15) is roughly similar to the Milne device for
the TR‚ÄìAB2 pair. A similar conclusion can be drawn by comparing Figs 6.2
and 6.6. On the face of it, this is also the case for our single stiÔ¨Äexample

6.3
Embedded Runge‚ÄìKutta methods
117
0
5
10
15
20
25
0
0.002
0.004
0.006
0.008
0.010
0
5
10
15
20
25
0
2
4
6
x 10 ‚àí4
Figure 6.5
Global errors for the numerical solution of the van der Pol
equation (6.2) by the embedded RK pair (6.15) with Œ¥ = 10‚àí3, 10‚àí4.
0
5
10
15
20
25
30
0
0.005
0.010
0.015
0
5
10
15
20
25
30
0
0.5
1.0
1.5
2.0
x 10 ‚àí3
Figure 6.6
Global errors for the numerical solution of the Mathieu
equation (6.3) by the embedded RK pair (6.15) with Œ¥ = 10‚àí3, 10‚àí4.

118
Error control
0
1
2
3
4
5
6
7
8
9
10
0
0.02
0.04
0
1
2
3
4
5
6
7
8
9
10
0
0.01
0.02
0
1
2
3
4
5
6
7
8
9
10
0
1
2
x 10 ‚àí4
0
1
2
3
4
5
6
7
8
9
10
0
1
2
x 10 ‚àí4
Figure 6.7
The step sequences (in the top two graphs) and global errors
for the numerical solution of the Curtiss‚ÄìHirschfelder equation (6.4) by the
embedded RK pair (6.15) with Œ¥ = 10‚àí3, 10‚àí4.
from Fig. 6.3 and 6.7.
However, a brief comparison of the step sequences
conÔ¨Årms that the precision for the embedded RK pair has been attained at
the cost of employing minute values of h. Needless to say, the smaller the
step size the longer the computation and the higher the expense. It is easy
to apportion the blame. The poor performance of the embedded pair (6.15)
in the last example can be attributed to the poor stability properties of the
‚Äòinner‚Äô method
0
2
3
2
3
1
4
3
4
.
Therefore r(z) = 1 + z + 1
2z2 (cf. Exercise 3.5) and it is an easy exercise to
show that D ‚à©R = (‚àí2, 0). In constant-step implementation we would have
thus needed 50h < 2, hence h <
1
25, to avoid instabilities. A bound of roughly
similar magnitude is consistent with Fig. 6.7. The error-control mechanism
can cope with stiÔ¨Äness, but it does so at the price of drastically depressing
the step size.
3
Exercise 6.5 demonstrates that the technique of embedded RK pairs can be gener-
alized, at least to some extent, to cater for implicit methods, thereby rendering it
more suitable for stiÔ¨Äequations. It is fair, though, to point out that, in practical
implementations, the use of embedded RK pairs is almost always restricted to explicit
Runge‚ÄìKutta schemes.

Comments and bibliography
119
Comments and bibliography
A classical approach to error control is to integrate once with step h and to integrate again,
along the same interval, with two steps of
1
2h. Comparison of the two candidate solutions
at the new point yields an estimate of Œ∫.
This is clearly inferior to the methods of this
chapter within the narrow framework of error control. However, the ‚Äòone step, two half-steps‚Äô
technique is a rudimentary example of extrapolation, which can be used both to monitor the
error and to improve locally the quality of the solution (cf. Exercise 6.6). See Hairer et al.
(1991) for an extensive description of extrapolation techniques.
We wish to mention two further means whereby local error can be monitored. The Ô¨Årst
is a general technique due to Zadunaisky, which can ride piggyback on any time-stepping
method
yn+1 = Y(f; h; (t0, y0), (t1, y1), . . . , (tn, yn))
of order p (Zadunaisky, 1976). It proceeds as follows. We are solving numerically the ODE
system (6.1) and assume the availability of p+1 past values, yn‚àíp, yn‚àíp+1, . . . , yn. They need
not correspond to equally spaced points. Let us form a pth degree interpolating polynomial
œà such that œà(ti) = yi, i = n ‚àíp, n ‚àíp + 1, . . . , n, and consider the ODE
z‚Ä≤ = f(t, z) + 
œà‚Ä≤(t) ‚àíf(t, œà(t))
,
t ‚â•tn,
z(tn) = yn.
(6.16)
Two observations are crucial. Firstly, (6.16) is merely a small perturbation of the original
system (6.1): since the numerical method is of order p and œà interpolates at p + 1 points,
it follows that œà(t) = y(t) + O
hp+1
, therefore, as long as f is suÔ¨Éciently smooth, œà‚Ä≤(t) ‚àí
f(t, œà(t)) = O(hp). Secondly, the exact solution of (6.16) is nothing other than z = œà; this
is veriÔ¨Åed at once by substitution.
We now use the underlying numerical method to approximate the solution of (6.16) at
tn+1, using exactly the same ingredients as were used in the computation of yn+1: the
same starting values, the same approach to solving nonlinear algebraic systems, an identical
stopping criterion . . . ÀôThe outcome is
zn+1 = Y(g; h; (t0, y0), (t1, y1), . . . , (tn, yn)),
where g(t, z) = f(t, z)+[œà‚Ä≤(t)‚àíf(t, œà(t))]. Since g ‚âàf, we act upon the assumption (which
can be Ô¨Årmed up mathematically) that the error in zn+1 is similar to the error in yn+1, and
this motivates the estimate
Œ∫ = ‚à•œà(tn+1) ‚àízn+1‚à•.
Our second technique for error control, the Gear automatic integration approach, is much
more than simply a device to assess the local error.
It is an integrated approach to the
implementation of multistep methods that not only controls the growth of the error but also
helps us to choose (on a local basis) the best multistep formula out of a given range.
The actual estimate of Œ∫ in Gear‚Äôs method is probably the least important detail. Recall-
ing from (6.7) that the principal error term is of the form chp+1y(p+1)(tn+s), we interpolate
the yi by a polynomial œà of suÔ¨Éciently high degree and replace y(p+1)(tn+s) by œà(p+1)(tn+s).
This, however, is only the beginning! Suppose, for example, that the underlying method is
the pth-order Adams‚ÄìMoulton. We subsequently form similar local-error estimates for its
neighbours, Adams‚ÄìMoulton methods of orders p ¬± 1. Instead of doubling (or, if the error
estimate for the pth method falls short of the tolerance, halving) the step size, we ask our-
selves which of the three methods would have attained, on the basis of our estimates, the
requisite tolerance Œ¥ with the largest value of h? We then switch to this method and this

120
Error control
step size, advancing if the present step is acceptable or otherwise resuming the integration
from the former point tn+s‚àí1.
This brief explanation does no justice to a complicated and sophisticated assembly of
techniques, rules and tricks that makes the Gear approach the method of choice in many
leading computational packages.
The reader is referred to Gear (1971) and to Shampine
& Gordon (1975) for details. Here we just comment on two important features. Firstly, a
tremendous simpliÔ¨Åcation of the tedious minutiae of interpolation and remeshing occurs if,
instead of storing past values of the solution, the program deals with their Ô¨Ånite diÔ¨Äerences,
which, in eÔ¨Äect, approximate the derivatives of y at tn+s‚àí1. This is called the Nordsieck rep-
resentation of the multistep method (6.5). Secondly, the Gear automatic integration obviates
the need for an independent (and tiresome) derivation of the requisite number of additional
starting values, which is characteristic of other implementations of multistep methods (and
which is often accomplished by a Runge‚ÄìKutta scheme). Instead, the integration can be
commenced using a one-step method, allowing the algorithm to increase order only when
enough information has accumulated for that purpose.
It might well be, gentle reader, that by this stage you are disenchanted with your prospects
of programming a competitive computational package that can hold its own against the best
in the Ô¨Åeld. If so, this exposition has achieved its purpose! Modern high-quality software
packages require years of planning, designing, programming, debugging, testing, debugging
again, documenting and testing again, by whole teams of Ô¨Årst class experts in numerical
analysis and software engineering. It is neither a job for amateurs nor an easy alternative to
proving theorems.
All standard numerical software packages, for example MATLAB and symbolic packages
like Maple and Mathematica that cater also for numerical calculations, have a number of well-
written and well-tested ODE solvers, mostly following the ideas described in this chapter.
Good and reliable software for ODEs is available from commercial companies that specialize in
numerical software, e.g. IMSL and NAG, as well as from NetLib, a depository of free software
managed by University of Tennesee at Knoxville and Oak Ridge National Laboratory (the
current URL address is http://www.netlib.org/).
General purpose software for partial
diÔ¨Äerential equations is more problematic, for reasons that should be apparent later in this
volume, but well-written, reliable and superbly documented packages exist for various families
of such equations, often in a form suitable for speciÔ¨Åc applications such as computational Ô¨Çuid
dynamics, electrical engineering etc.
A useful and (relatively) up-to-date guide to state-of-the-art mathematical software is
available at the website http://gams.nist.gov/, courtesy of the (American) National Insti-
tute of Standards and Technology. An impressive source of free software is Ernst Hairer‚Äôs
website, http://www.unige.ch/Àúhairer/software.html. However, the number of ftp and
websites is expanding so fast as to render a more substantive list of little lasting value. Given
the volume of traÔ¨Éc along the information superhighway, it is likely that the ideal program
for your problem exists somewhere. It is a moot point, however, whether it is easier to locate
it or to write one of your own . . .
Gear, C.W. (1971), Numerical Initial Value Problems in Ordinary DiÔ¨Äerential Equations,
Prentice‚ÄìHall, Englewood CliÔ¨Äs, NJ.
Hairer, E., N√∏rsett, S.P. and Wanner, G. (1991), Solving Ordinary DiÔ¨Äerential Equations I:
NonstiÔ¨ÄProblems (2nd edn), Springer-Verlag, Berlin.
Shampine, L.F. and Gordon, M.K. (1975), Computer Solution of Ordinary DiÔ¨Äerential Equa-
tions, W.H. Freeman, San Francisco.

Exercises
121
Zadunaisky, P.E. (1976), On the estimation of errors propagated in the numerical integration
of ordinary diÔ¨Äerential equations, Numerische Mathematik 27, 21‚Äì39.
Exercises
6.1
Find the error constants for the Adams‚ÄìBashforth method (2.7) and for
Adams‚ÄìMoulton methods with s = 2, 3.
6.2‚ãÜ
Prove that the error constant of the s-step backward diÔ¨Äerentiation formula
is ‚àíŒ≤/(s + 1), where Œ≤ was deÔ¨Åned in (2.14).
6.3
Instead of using (6.10) to estimate the error in the multistep method (6.11),
we can use it to increase the accuracy.
a Prove that the formula (6.7) yields
y(tn+1) ‚àíyn+1 = ‚àí1
12h3y‚Ä≤‚Ä≤‚Ä≤(tn+1) + O

h4
,
y(tn+1) ‚àíxn+1 = ‚àí5
12h3y‚Ä≤‚Ä≤‚Ä≤(tn+1) + O

h4
.
(6.17)
b Neglecting the O

h4
terms, solve the two equations for the unknown y(tn+1)
(in contrast with the Milne device, where we solve for y‚Ä≤‚Ä≤‚Ä≤(tn+1)).
c Substituting the approximate expression back into (6.7) results in a two-step
implicit multistep method. Derive it explicitly and determine its order. Is
it convergent? Can you identify it?3
6.4
Prove that the embedded RK pair
0
1
2
1
2
1
‚àí1
2
0
1
1
6
2
3
1
6
combines a second-order and a third-order method.
6.5
Consider the embedded RK pair
0
1
1
2
1
2
1
2
3
8
1
8
1
2
1
2
1
6
1
6
2
3
.
Note that the ‚Äòinner‚Äô two-stage method is implicit and that the third stage
is explicit. This means that the added cost of error control is marginal.
3It is always possible to use the method (6.6) to boost the order of (6.5) except when the outcome
is not convergent, as is often the case.

122
Error control
a Prove that the ‚Äòinner‚Äô method is of order 2, while the full three-stage method
is of order 3.
b Show that the ‚Äòinner‚Äô method is A-stable. Can you identify it as a familiar
method in disguise?
c Find the function r associated with the three-stage method and verify that,
in line with Lemma 4.4, r(z) = ez + O

z4
, z ‚Üí0.
6.6
Let yn+1 = Y(f, h, yn), n = 0, 1, . . . , be a one-step method of order p.
We assume (consistently with Runge‚ÄìKutta methods, cf. (6.12)) that there
exists a vector ‚Ñìn, independent of h, such that
yn+1 = Àúy(tn+1) + ‚Ñìnhp+1 + O

hp+2
,
h ‚Üí0,
where Àúy is the exact solution of (6.1) with initial condition Àúy(tn) = yn. Let
xn+1 := Y

f, 1
2h, Y

f, 1
2h, yn

.
Note that xn+1 is simply the result of traversing [tn, tn+1] with the method
Y in two equal steps of 1
2h.
a Find a real constant Œ± such that
xn+1 = Àúy(tn+1) + Œ±‚Ñìnhp+1 + O

hp+2
,
h ‚Üí0.
b Determine a real constant Œ≤ such that the linear combination zn+1 := (1 ‚àí
Œ≤)yn+1 + Œ≤xn+1 approximates Àúy(tn+1) up to O

hp+2
. (The procedure of
using the enhanced value zn+1 as the approximation at tn+1 is known as
extrapolation. It is of widespread application.)
c Let Y correspond to the trapezoidal rule (1.9) and suppose that the above
extrapolation procedure is applied to the scalar linear equation y‚Ä≤ = Œªy,
y(0) = 1, with a constant step size h. Find a function r such that zn+1 =
r(hŒª)zn = [r(hŒª)]n+1, n = 0, 1, . . . Is the new method A-stable?

7
Nonlinear algebraic systems
7.1
Functional iteration
From the point of view of a numerical mathematician, which we adopted in Chapters
1‚Äì5, the solution of ordinary diÔ¨Äerential equations is all about analysis ‚Äì i.e. conver-
gence, order, stability and an endless progression of theorems and proofs. The outlook
of Chapter 6 parallels that of a software engineer, being concerned with the correct
assembly of computational components and with choosing the step sequence dynam-
ically. Computers, however, are engaged neither in analysis nor in algorithm design
but in the real work concerned with solving ODEs, and this consists in the main of
the computation of (mostly nonlinear) algebraic systems of equations.
Why not ‚Äì and this is a legitimate question ‚Äì use explicit methods, whether mul-
tistep or Runge‚ÄìKutta, thereby dispensing altogether with the need to calculate al-
gebraic systems? The main reason is computational cost. This is obvious in the case
of stiÔ¨Äequations, since, for explicit time-stepping methods, stability considerations
restrict the step size to an extent that renders the scheme noncompetitive and down-
right ineÔ¨Äective. When stability questions are not at issue, it often makes very good
sense to use explicit Runge‚ÄìKutta methods. The accepted wisdom is, however, that,
as far as multistep methods are concerned, implicit methods should be used even for
non-stiÔ¨Äequations since, as we will see in this chapter, the solution of the underlying
algebraic systems can be approximated with relative ease.
Let us suppose that we wish to advance the (implicit) multistep method (2.8) by
a single step. This entails solving the algebraic system
yn+s = hbsf(tn+s, yn+s) + Œ≥,
(7.1)
where the vector
Œ≥ = h
s‚àí1

m=0
bmf(tn+m, yn+m) ‚àí
s‚àí1

m=0
amyn+m
is known. As far as implicit Runge‚ÄìKutta methods (3.9) are concerned, we need to
solve at each step the system
Œæ1 = yn + h[a1,1f(tn + c1h, Œæ1) + a1,2f(tn + c2h, Œæ2) + ¬∑ ¬∑ ¬∑ + a1,ŒΩf(tn + cŒΩh, ŒæŒΩ)],
Œæ2 = yn + h[a2,1f(tn + c1h, Œæ1) + a2,2f(tn + c2h, Œæ2) + ¬∑ ¬∑ ¬∑ + a2,ŒΩf(tn + cŒΩh, ŒæŒΩ)],
...
ŒæŒΩ = yn + h[aŒΩ,1f(tn + c1h, Œæ1) + aŒΩ,2f(tn + c2h, Œæ2) + ¬∑ ¬∑ ¬∑ + aŒΩ,ŒΩf(tn + cŒΩh, ŒæŒΩ)].
123

124
Nonlinear algebraic systems
This system looks considerably more complicated than (7.1). Both, however, can be
cast into a standard form, namely
w = hg(w) + Œ≤,
w ‚ààR
Àúd,
(7.2)
where the function g and the vector Œ≤ are known. Obviously, for the multistep method
(7.1) we have g( ¬∑ ) = bmf(tn+s, ¬∑ ), Œ≤ = Œ≥ and Àúd = d. The solution of (7.2) then
becomes yn+s. The notation is slightly more complicated for Runge‚ÄìKutta methods,
although it can be simpliÔ¨Åed a great deal by using Kronecker products. However, we
provide an example for the case ŒΩ = 2 where, mercifully, no Kronecker products are
required. Thus, Àúd = 2d and
g(w) =

a1,1f(tn + c1h, w1) + a1,2f(tn + c2h, w2)
a2,1f(tn + c1h, w1) + a2,2f(tn + c2h, w2)

,
where
w =

w1
w2

.
Moreover,
Œ≤ =

yn
yn

.
Provided that the solution of (7.2) is known, we then set Œæj = wj, j = 1, 2, hence
yn+1 = yn + h[b1f(tn + c1h, w1) + b2f(tn + c2h, w2)].
Let us assume that g is nonlinear (if g were linear and the number of equations
moderate, we could solve (7.2) by familiar Gaussian elimination; the numerical solu-
tion of large linear systems is discussed in Chapters 11‚Äì15). Our intention is to solve
the algebraic system by iteration;1 in other words, we need to make an initial guess
w[0] and provide an algorithm
w[i+1] = s(w[i]),
i = 0, 1, . . . ,
(7.3)
such that
(1) w[i] ‚ÜíÀÜw, the solution of (7.2);
(2) the cost of each step (7.3) is small;
and
(3) the progression to the limit is rapid.
The form (7.2) emphasizes two important aspects of this iterative procedure. Firstly,
the vector Œ≤ is known at the outset and there is no need to recalculate it in every
iteration (7.3). Secondly, the step size h is an important parameter and its magnitude
is likely to determine central characteristics of the system (7.2); since the exact solution
is obvious when h = 0, clearly the problem is likely to be easier for small h > 0.
Moreover, although in principle (7.2) may possess many solutions, it follows at once
from the implicit function theorem that, provided g is continuously diÔ¨Äerentiable,
1The reader will notice that two distinct iterative procedures are taking place: time-stepping, i.e.
yn+s‚àí1 ‚Üíyn+s, and ‚Äòinner‚Äô iteration, w[i] ‚Üíw[i+1]. To prevent confusion, we reserve the phrase
‚Äòiteration‚Äô for the latter.

7.1
Functional iteration
125
nonsingularity of the Jacobian matrix I ‚àíh‚àÇg(Œ≤)/‚àÇw for h ‚Üí0 implies the existence
of a unique solution for suÔ¨Éciently small h > 0.
The most elementary approach to the solution of (7.2) is the functional iteration
s(w) = hg(w) + Œ≤, which, using (7.3) can be expressed as
w[i+1] = hg(w[i]) + Œ≤,
i = 0, 1, . . .
(7.4)
Much beautiful mathematics has been produced in the last few decades in con-
nection with functional iteration, concerned mainly with the fractal nature of basins
of attraction in the complex case. For practical purposes, however, we resort to the
tried and trusted Banach Ô¨Åxed-point theorem, which will now be stated and proved in
a formalism appropriate for the recursion (7.4).
Given a vector norm ‚à•¬∑ ‚à•and w ‚ààR
Àúd, we denote by BœÅ(w) the closed ball of
radius œÅ > 0 centred at w:
BœÅ(w) =

u ‚ààR
Àúd : ‚à•u ‚àíw‚à•‚â§œÅ

.
Theorem 7.1
Let h > 0, w[0] ‚ààR
Àúd, and suppose that there exist numbers Œª ‚àà(0, 1)
and œÅ > 0 such that
(i)
‚à•g(v) ‚àíg(u)‚à•‚â§Œª
h‚à•v ‚àíu‚à•for every v, u ‚ààBœÅ(w[0]);
(ii)
w[1] ‚ààB(1‚àíŒª)œÅ(w[0]).
Then
(a)
w[i] ‚ààBœÅ(w[0]) for every i = 0, 1, . . .;
(b)
ÀÜw := limi‚Üí‚àûw[i] exists, obeys equation (7.2) and ÀÜw ‚ààBœÅ(w[0]);
(c)
no other point in BœÅ(w[0]) is a solution of (7.2).
Proof
We commence by using induction to prove that
‚à•w[i+1] ‚àíw[i]‚à•‚â§Œªi(1 ‚àíŒª)œÅ
(7.5)
and that w[i+1] ‚ààBœÅ(w[0]) for all i = 0, 1, . . .
Part (a) is certainly true for i = 0 because of condition (ii) and the deÔ¨Ånition of
BœÅ(w[0]). Now, let us assume that the statement is true for all m = 0, 1, . . . , i ‚àí1.
Then, by (7.2) and assumption (i),
‚à•w[i+1] ‚àíw[i]‚à•= ‚à•[hg(w[i]) + Œ≤] ‚àí[hg(w[i‚àí1]) + Œ≤]‚à•
= h‚à•g(w[i]) ‚àíg(w[i‚àí1])‚à•‚â§Œª‚à•w[i] ‚àíw[i‚àí1]‚à•,
i = 1, 2, . . .
This carries forward the induction for (7.5) from i ‚àí1 to i.

126
Nonlinear algebraic systems
The following sum,
w[i+1] ‚àíw[0] =
i

j=0
(w[ j+1] ‚àíw[ j]),
i = 0, 1, . . . ,
telescopes; therefore, by the triangle inequality (A.1.3.3)
‚à•w[i+1] ‚àíw[0]‚à•=
-----
i

j=0
(w[j+1] ‚àíw[j])
----- ‚â§
i

j=0
‚à•w[j+1] ‚àíw[j]‚à•,
i = 0, 1, . . .
Exploiting (7.5) and summing the geometric series, we thus conclude that
‚à•w[i+1] ‚àíw[0]‚à•‚â§
i

j=0
Œªi(1 ‚àíŒª)œÅ = (1 ‚àíŒªi+1)œÅ ‚â§œÅ,
i = 0, 1, . . .
Therefore w[i+1] ‚ààBœÅ(w[0]). This completes the inductive proof and we deduce that
(a) is true.
Again telescoping series, the triangle inequality and (7.5) can be used to argue
that
‚à•w[i+k] ‚àíw[i]‚à•=
-----
k‚àí1

j=0
(w[i+j+1] ‚àíw[i+j])
----- ‚â§
k‚àí1

j=0
‚à•w[i+j+1] ‚àíw[i+j]‚à•
‚â§
k‚àí1

j=0
Œªi+j(1 ‚àíŒª)œÅ = Œªi(1 ‚àíŒªk)œÅ,
i = 0, 1, . . . ,
k = 1, 2, . . .
Therefore, Œª ‚àà(0, 1) implies that for every i = 0, 1, . . . and Œµ > 0 we may choose k
large enough that
‚à•w[i+k] ‚àíw[i]‚à•< Œµ.
In other words, {w[i]}i=0,1,... is a Cauchy sequence.
The set BœÅ(w[0]) being compact (i.e., closed and bounded), the Cauchy sequence
{w[i]}i=0,1,... converges to a limit within the set. This proves the existence of ÀÜw ‚àà
BœÅ(w[0]).
Finally, let us suppose that there exists w‚ãÜ‚ààBœÅ(w[0]), w‚ãÜÃ∏= ÀÜw, such that w‚ãÜ=
hg(w‚ãÜ) + Œ≤. Then ‚à•w‚ãÜ‚àíÀÜw‚à•> 0 implies that
‚à•w‚ãÜ‚àíÀÜw‚à•= ‚à•[hg(w‚ãÜ) + Œ≤] ‚àí[hg( ÀÜw) + Œ≤]‚à•= h‚à•g(w‚ãÜ) ‚àíg( ÀÜw)‚à•
‚â§Œª‚à•w‚ãÜ‚àíÀÜw‚à•< ‚à•w‚ãÜ‚àíÀÜw‚à•.
This is impossible and we deduce that the Ô¨Åxed point ÀÜw is unique in BœÅ(w[0]), thereby
concluding the proof of the theorem.
If g is smoothly diÔ¨Äerentiable then, by the mean value theorem, for every v and u
there exists œÑ ‚àà(0, 1) such that
g(v) ‚àíg(u) = ‚àÇg(œÑv + (1 ‚àíœÑ)u)
‚àÇw
(v ‚àíu).

7.2
The Newton‚ÄìRaphson algorithm and its modiÔ¨Åcation
127
Therefore, assumption (i) of Theorem 7.1 is nothing other than a statement on the
magnitude of the step size h in relation to ‚à•‚àÇg/‚àÇw‚à•. In particular, if (7.2) originates
in a multistep method then we need, in eÔ¨Äect, h|bs| ¬∑ ‚à•‚àÇf(tn+s, yn+s)/‚àÇy‚à•< 1. (A
similar inequality applies to Runge‚ÄìKutta methods.) The meaning of this restriction
is phenomenologically similar to stiÔ¨Äness, as can be seen in the following example.
3 The trapezoidal rule and functional iteration
The iterative scheme
(7.4), as applied to the trapezoidal rule (1.9), reads
w[i+1] = 1
2hf(tn+1, w[i]) +

yn + 1
2hf(tn, yn)

,
i = 0, 1, . . .
Let us suppose that the underlying ODE is linear, i.e. of the form y‚Ä≤ = Œõy,
where Œõ is symmetric. As long as we are employing the Euclidean norm, it
is true that ‚à•Œõ‚à•= œÅ(Œõ), the spectral radius of Œõ (A.1.5.2). The outcome
is the restriction hœÅ(Œõ) < 2, which imposes similar constraints on a stable
implementation of the Euler method (1.4). Provided œÅ(Œõ) is small and stiÔ¨Äness
is not an issue, this makes little diÔ¨Äerence. However, to retain the A-stability
of the trapezoidal rule for large œÅ(Œõ) we must restrict h > 0 so drastically
that all the beneÔ¨Åts of A-stability are lost ‚Äì we might just as well have used
Adams‚ÄìBashforth, say, in the Ô¨Årst place!
3
We conclude that a useful rule of a thumb is that we may use the functional iteration
(7.4) for non-stiÔ¨Äproblems but we need an alternative when stiÔ¨Äness becomes an issue.
7.2
The Newton‚ÄìRaphson algorithm and its
modiÔ¨Åcation
Let us suppose that the function g is twice continuously diÔ¨Äerentiable. We expand
(7.2) about a vector w[i]:
w = Œ≤ + hg(w[i] + (w ‚àíw[i]))
= Œ≤ + hg(w[i]) + h‚àÇg(w[0])
‚àÇw
(w ‚àíw[i]) + O

‚à•w ‚àíw[i]‚à•2
.
(7.6)
Disregarding the O

‚à•w ‚àíw[i]‚à•2
term, we solve (7.6) for w ‚àíw[i]. The outcome,

I ‚àíh‚àÇg(w[i])
‚àÇw

(w ‚àíw[i]) ‚âàŒ≤ + hg(w[i]) ‚àíw[i],
suggests the iterative scheme
w[i+1] = w[i] ‚àí

I ‚àíh‚àÇg(w[i])
‚àÇw
‚àí1 %
w[i] ‚àíŒ≤ ‚àíhg(w[i])
&
,
i = 0, 1, . . .
(7.7)
This is (under a mild disguise) the celebrated Newton‚ÄìRaphson algorithm.
The Newton‚ÄìRaphson method has motivated several profound theories and at-
tracted the attention of some of the towering mathematical minds of the twentieth

128
Nonlinear algebraic systems
century ‚Äì Leonid Kantorowitz and Stephen Smale, to mention just two. We do not
propose in this volume to delve into this issue, whose interest is tangential to our main
theme. Instead, and without further ado, we merely comment on several features of
the iterative scheme (7.7).
Firstly, as long as h > 0 is suÔ¨Éciently small the rate of convergence of the Newton‚Äì
Raphson algorithm is quadratic: it is possible to prove that there exists a constant
c > 0 such that, for suÔ¨Éciently large i,
‚à•w[i+1] ‚àíÀÜw‚à•‚â§c‚à•w[i] ‚àíÀÜw‚à•2,
where ÀÜw is a solution of (7.2).
This is already implicit in the fact that we have
neglected an O

‚à•w ‚àíw[i]‚à•2
term in (7.6).
It is important to comment that the
‚ÄòsuÔ¨Écient smallness‚Äô of h > 0 is of a diÔ¨Äerent order of magnitude to the minute
values of h > 0 that are required when the functional iteration (7.4) is applied to stiÔ¨Ä
problems. It is easy to prove, for example, that (7.7) terminates in a single step when
g is a linear function, regardless of any underlying stiÔ¨Äness (see Exercise 7.2).
Secondly, an implementation of (7.7) requires computation of the Jacobian matrix
at every iteration. This is a formidable ordeal since, for a d-dimensional system, the
Jacobian matrix has d2 entries and its computation ‚Äì even if all requisite formulae are
available in an explicit form ‚Äì is expensive.
Finally, each iteration requires the solution of a linear system of algebraic equa-
tions. It is highly unusual for such a system to be singular or ill conditioned (i.e.,
‚Äòclose‚Äô to singular) in a realistic computation, regardless of stiÔ¨Äness; the reasons, in
the (simpler) case of multistep methods, are that bs > 0 for all methods with reason-
ably large linear stability domains, the eigenvalues of ‚àÇf/‚àÇy reside in C‚àíand it is
easy to prove that all the eigenvalues of the matrix in (7.7) are bounded away from
zero. However, the solution of even a well-conditioned nonsingular algebraic system
is a nontrivial and potentially costly task.
Both shortcomings of Newton‚ÄìRaphson ‚Äì the computation of the Jacobian matrix
and the need to solve linear systems in each iteration ‚Äì can be alleviated by using
the modiÔ¨Åed Newton‚ÄìRaphson instead. The quid pro quo, however, is a signiÔ¨Åcant
slowing-down of the convergence rate. Before we introduce the modiÔ¨Åcation of (7.7),
let us comment brieÔ¨Çy on an important special case when the ‚Äòfull‚Äô Newton‚ÄìRaphson
can (and should) be used.
A signiÔ¨Åcant proportion of stiÔ¨ÄODEs originate in the semi-discretization of para-
bolic partial diÔ¨Äerential equations by Ô¨Ånite diÔ¨Äerence methods (Chapter 16). In such
cases the Newton‚ÄìRaphson method (7.7) is very eÔ¨Äective indeed, since the Jacobian
matrix is sparse (an overwhelming majority of its elements vanish): it has just O(d)
nonzero components and usually can be computed with relative ease. Moreover, most
methods for the solution of sparse algebraic systems confer no advantage for the spe-
cial form (7.9) of the modiÔ¨Åed equations, an exception being the direct factorization
algorithms of Chapter 11.
3 The reaction‚ÄìdiÔ¨Äusion equation
A quasilinear parabolic partial diÔ¨Äer-
ential equation with many applications in mathematical biology, chemistry

7.2
The Newton‚ÄìRaphson algorithm and its modiÔ¨Åcation
129
and physics is the reaction‚ÄìdiÔ¨Äusion equation
‚àÇu
‚àÇt = ‚àÇ2u
‚àÇx2 + œï(u),
0 < x < 1,
t ‚â•0,
(7.8)
where u = u(x, t). It is given with the initial condition u(x, 0) = u0(x), 0 <
x < 1, and (for simplicity) zero Dirichlet boundary conditions u(0, t), u(1, t) ‚â°
0, t ‚â•0.
Among the many applications of (7.8) we single out two for special mention.
The choice œï(u) = cu, where c > 0, models the neutron density in an atom
bomb (subject to the assumption that the latter is in the form of a thin
uranium rod of unit length), whereas œï(u) = Œ±u+Œ≤u2 (the Fisher equation) is
used in population dynamics: the terms Œ±u and Œ≤u2 correspond respectively
to the reproduction and interaction of a species while ‚àÇ2u/‚àÇx2 models its
diÔ¨Äusion in the underlying habitat.
A standard semi-discretization (that is, an approximation of a partial diÔ¨Äer-
ential equation by an ODE system, see Chapter 16) of (7.8) is
y‚Ä≤
k =
1
(‚àÜx)2 (yk‚àí1 ‚àí2yk + yk+1) + œï(yk),
k = 1, 2, . . . , d,
t ‚â•0,
where ‚àÜx = 1/(d + 1) and y0, yd+1 ‚â°0. Suppose that œï‚Ä≤ is easily available,
e.g. that œï is a polynomial. The Jacobian matrix
	‚àÇf(t, y)
‚àÇy

k,‚Ñì
=
‚éß
‚é™
‚é™
‚é™
‚é™
‚é®
‚é™
‚é™
‚é™
‚é™
‚é©
‚àí
2
(‚àÜx)2 + œï‚Ä≤(yk),
k = ‚Ñì,
1
(‚àÜx)2 ,
|k ‚àí‚Ñì| = 1,
0
otherwise,
k, ‚Ñì= 1, 2, . . . , d,
is fairly easy to evaluate and store.
Moreover, as will be apparent in the
forthcoming discussion in Chapter 11, the solution of algebraic linear systems
with tridiagonal matrices is very easy and fast. We conclude that in this case
there is no need to trade oÔ¨Äthe superior speed of Newton‚ÄìRaphson for ‚Äòeasier‚Äô
alternatives.
3
Unfortunately, most stiÔ¨Äsystems do not share the features of the above example and
for these we need to modify the Newton‚ÄìRaphson iteration. This modiÔ¨Åcation takes
the form of a replacement of the matrix ‚àÇg(w[i])/‚àÇw by another matrix, J, say, that
does not vary with i. A typical choice might be
J = ‚àÇg(w[0])
‚àÇw
,
but it is not unusual, in fact, to retain the same matrix J for a number of time steps.
In place of (7.7) we thus have
w[i+1] = w[i] ‚àí(I ‚àíhJ)‚àí1 %
w[i] ‚àíŒ≤ ‚àíhg(w[i])
&
,
i = 0, 1, . . . .
(7.9)

130
Nonlinear algebraic systems
This modiÔ¨Åed Newton‚ÄìRaphson scheme (7.9) confers two immediate advantages. The
Ô¨Årst is obvious: we need to calculate J only once per step (or perhaps per several
steps). The second is realized when the underlying linear algebraic system is solved
by Gaussian elimination ‚Äì the method of choice for small or moderate d.
In its
LU formulation (A.1.4.5), Gaussian elimination for the linear system Ax = b, where
b ‚ààRd, consists of two stages. Firstly, the matrix A is factorized in the form LU, where
L and U are lower triangular and upper triangular matrices respectively. Secondly, we
solve Lz = b, followed by Ux = z. While factorization entails O

d3
operations (for
non-sparse matrices), the solution of two triangular d √ó d systems requires just O

d2
operations and is considerably cheaper. In the case of the iterative scheme (7.9) it is
enough to factorize A = I ‚àíhJ just once per time step (or once per re-evaluation of
J and/or per change in h). Therefore, the cost of each single iteration goes down by
an order of magnitude, as compared with the original Newton‚ÄìRaphson scheme!
Of course, quadratic convergence is lost. As a matter of fact, modiÔ¨Åed Newton‚Äì
Raphson is simply functional iteration except that instead of hg(w) + Œ≤ we iterate
the new function hÀúg(w) + ÀúŒ≤, where
Àúg(w) := (I ‚àíhJ)‚àí1[g(w) ‚àíJw],
ÀúŒ≤ := (I ‚àíhJ)‚àí1Œ≤.
(7.10)
The proof is left to the reader in Exercise 7.3.
There is nothing to stop us from using Theorem 7.1 to explore the convergence of
(7.9). It follows from (7.10) that
Àúg(v) ‚àíÀúg(u) = (I ‚àíhJ)‚àí1{[g(v) ‚àíg(u)] ‚àíJ(v ‚àíu)}.
(7.11)
Recall, however, that, subject to the suÔ¨Écient smoothness of g, there exists a point z
on the line segment joining v and u such that
g(v) ‚àíg(u) = ‚àÇg(z)
‚àÇw (v ‚àíu).
Given that we have chosen
J = ‚àÇg( Àúw)
‚àÇw
(for example, Àúw = w[0]), it follows from (7.11) that
‚à•Àúg(v) ‚àíÀúg(u)‚à•
‚à•v ‚àíu‚à•
‚â§
-----

I ‚àíh‚àÇg( Àúw)
‚àÇw
‚àí1----- √ó
----
‚àÇg(z)
‚àÇw
‚àí‚àÇg( Àúw)
‚àÇw
---- .
Unless the Jacobian matrix varies very considerably as a function of t, the second
term on the right is likely to be small. Moreover, if all the eigenvalues of J are in C‚àí
then ‚à•(I ‚àíhJ)‚àí1‚à•is likely also to be small; stiÔ¨Äness is likely to help, not hinder, this
estimate! Therefore, it is possible in general to satisfy assumption (i) of Theorem 7.1
for large œÅ > 0.
7.3
Starting and stopping the iteration
Theorem 7.1 quantiÔ¨Åes an important point that is equally valid for every iterative
method for nonlinear algebraic equations (7.2), not just for the functional iteration

7.3
Starting and stopping the iteration
131
(7.4) and the modiÔ¨Åed Newton‚ÄìRaphson method (7.9): good performance hinges to a
large extent on the quality of the starting value w[0].
Provided that g is Lipschitz, condition (i) is always valid for a given h > 0 and
suÔ¨Éciently small œÅ > 0. However, small œÅ means that, to be consistent with condition
(ii), we must choose an exceedingly good starting condition w[0]. Viewed in this light,
the main purpose in replacing (7.4) by (7.9) is to allow convergence from imperfect
starting values.
Even if the choice of an iterative scheme provides for a large basin of attraction of
ÀÜw (the set of all w[0] ‚ààR
Àúd for which the iteration converges to ÀÜw), it is important
to commence the iteration with a good initial guess. This is true for every nonlinear
algebraic system but our problem here is special ‚Äì it originates in the use of a time-
stepping method for ODEs. This is an important advantage.
Supposing for example that the underlying ODE method is a pth-order multistep
scheme, let us recall the meaning of the solution of (7.2), namely yn+s. To paraphrase
the last paragraph, it is an excellent policy to seek a starting condition w[0] near to
the vector yn+s. The latter is, of course, unknown, but we can obtain a good guess
by using a diÔ¨Äerent, explicit, multistep method of order p. This multistep method,
called the predictor, provides the platform upon which the iterative scheme seeks the
solution of the implicit corrector.
It is not enough to start an iterative procedure; we must also provide a stopping
criterion, which terminates the iterative process. This might appear as a relatively
straightforward task: iterate until ‚à•w[i+1] ‚àíw[i]‚à•< Œµ for a given threshold value
Œµ (distinct from, and probably signiÔ¨Åcantly smaller than, the tolerance Œ¥ that we
employ in error control). However, this approach ‚Äì perfectly sensible for the solution
of general nonlinear algebraic systems ‚Äì misses an important point: the origin of (7.2)
is in a time-stepping computational method for ODEs, implemented with step size h.
If convergence is slow we have two options, either to carry on iterating or to stop the
procedure, abandon the current step size and commence time-stepping with a smaller
value of h > 0. In other words, there is nothing to prevent us from using the step size
both to control the error and to ensure rapid convergence of the iterative scheme.
The traditional attitude to iterative procedures, namely to proceed with perhaps
thousands of iterations until convergence takes place (to a given threshold) is com-
pletely inadequate. Unless the process converges in a relatively small number of itera-
tions ‚Äì perhaps ten, perhaps fewer ‚Äì the best course of action is to stop, decrease the
step size and recommence time-stepping. However, this does not exhaust the range of
all possible choices. Let us remember that the goal is not to solve a nonlinear algebraic
system per se but to compute a solution of an ODE system to a given tolerance. We
thus have two options.
Firstly, we can iterate for i = 0, 1, . . . , iend, where iend = 10, say.
After each
iteration we check for convergence. Unless it is attained (within the threshold Œµ) we
decide that h is too large and abandon the current step. This is called iteration to
convergence.
The second option is to identify the predictor‚Äìcorrector pair with the two methods
(6.6) and (6.5) that were used in Chapter 6 to control the error by means of the
Milne device. We perform just a single iteration of the corrector and substitute w[1],

132
Nonlinear algebraic systems
instead of yn+s, into the error estimate (6.8). If Œ∫ ‚â§hŒ¥ then all is well and we let
yn+s = w[1]. Otherwise we abandon the step. Note that we are accepting a value
of yn+s that solves neither the nonlinear algebraic equation nor, as a matter of fact,
the implicit multistep method. This, however, is of no consequence since our yn+s
passes the error test ‚Äì and that is all that matters! This approach is called the PECE
iteration.2
The choice between PECE iteration and iteration to convergence hinges upon the
relative cost of performing a single iteration and changing the step size. If the cost of
changing h is negligible, we might just as well abandon the iteration unless w[1], or
perhaps w[2] (in which case we have a PE(CE)2 procedure), satisÔ¨Åes the error criterion.
If, though, this cost is large we should carry on with the iteration considerably longer.
Another consideration is that a PECE iteration is likely to cause severe contraction
of the linear stability domain of the corrector. In particular, no such procedure can
be A-stable3 (see Exercise 7.4).
We recall the dichotomy between stiÔ¨Äand non-stiÔ¨ÄODEs. If the ODE is non-
stiÔ¨Äthen we are likely to employ the functional iteration (7.2), which costs nothing
to restart. The only cost of changing h is in remeshing, which, although diÔ¨Écult to
program, carries a very modest computational price tag. Since shrinkage of the linear
stability domain is not an important issue for non-stiÔ¨ÄODEs, the clear conclusion
is that the PECE approach is superior in this case.
Moreover, if the equation is
stiÔ¨Äthen we should use the modiÔ¨Åed Newton‚ÄìRaphson iteration (7.9). In order to
change the step size, we need to redo the LU factorization of I ‚àíhJ (since h has
changed). Moreover, it is a good policy to re-evaluate J as well, unless it has already
been computed at yn+s‚àí1: it might well be that the failure of the iterative procedure
follows from poor approximation of the Jacobian matrix. Finally, stability is deÔ¨Ånitely
a crucial consideration and we should be unwilling to reconcile ourselves to a collapse in
the size of the linear stability domain. All these reasons mean that the right approach
is to iterate to convergence.
Comments and bibliography
The computation of nonlinear algebraic systems is as old as numerical analysis itself. This
is not necessarily an advantage, since the theory has developed in many directions which,
mostly, are irrelevant to the theme of this chapter.
The basic problem admits several equivalent formulations: Ô¨Årstly, we may regard it as
Ô¨Ånding a zero of the equation h1(x) = 0; secondly, as computing a Ô¨Åxed point of the system
x = h2(x), where h2 = x + Œ±h1(x) for some Œ± ‚ààR \ {0}; thirdly, as minimizing ‚à•h1(x)‚à•.
(With regard to the third formulation, minimization is often equivalent to the solution of
a nonlinear system: provided the function œà : Rd ‚ÜíR is continuously diÔ¨Äerentiable, the
problem of Ô¨Ånding the stationary values of œà is equivalent to solving the system ‚àáœà(x) = 0.)
Therefore, in a typical library nonlinear algebraic systems and their numerical analysis appear
2Predict, Evaluate, Correct, Evaluate.
3in a formal sense. A-stability is deÔ¨Åned only for constant steps, whereas the whole raison d‚ÄôÀÜetre of
the PECE iteration is that it is operated within a variable-step procedure. However, experience tells
us that the damage to the quality of the solution in an unstable situation is genuine in a variable-step
setting also.

Exercises
133
under several headings, probably on diÔ¨Äerent shelves. Good sources are Ortega & Rheinboldt
(1970) and Fletcher (1987) ‚Äì the latter has a pronounced optimization Ô¨Çavour.
Modern numerical practice has moved a long way from the old days of functional iter-
ation and the Newton‚ÄìRaphson method and its modiÔ¨Åcations. The powerful algorithms of
today owe much to tremendous advances in numerical optimization, as well as to the recent
realization that certain acceleration schemes for linear systems can be applied with telling
eÔ¨Äect to nonlinear problems (for example, the method of conjugate gradients, the theme of
Chapter 14). However, it appears that, as far as the choice of nonlinear algebraic algorithms
for practical implementation of ODE algorithms is concerned, not much has happened in the
last three decades. The texts of Gear (1971) and of Shampine & Gordon (1975) represent,
to a large extent, the state of the art today.
This conservatism is not necessarily a bad thing. After all, the test of the pudding is in the
eating and, as far as we are aware, functional iteration (7.4) and modiÔ¨Åed Newton‚ÄìRaphson
(7.9), applied correctly and to the right problems, discharge their duty very well indeed. We
cannot emphasize enough that the task in hand is not simply to solve an arbitrary nonlinear
algebraic system but to compute a problem that arises in the calculation of ODEs. This
imposes a great deal of structure, highlights the crucial importance of the parameter h and, at
each iteration, faces us with the question ‚ÄòShould we continue to iterate or, rather, abandon
the step and decrease h?‚Äô. The transplantation of modern methods for general nonlinear
algebraic systems into this framework requires a great deal of work and Ô¨Åne-tuning. It might
well be a worthwhile project, though: there are several good dissertations here, awaiting
authors!
One aspect of functional iteration familiar to many readers (and to the general public,
through the agency of the mass media, exhibitions and coÔ¨Äee-table volumes) is the fractal
sets that arise when complex functions are iterated. It is only fair to mention that, behind the
fa¬∏cade of beautiful pictures, there lies some truly beautiful mathematics: complex dynamics,
automorphic forms, Teichm¬®uller spaces . . . This, however, is largely irrelevant to the task in
hand. It is a constant temptation of the wanderer in the mathematical garden to stray from
the path and savour the sheer beauty and excitement of landscapes strange and wonderful.
Although it may be a good idea occasionally to succumb to temptation, on this occasion we
virtuously stay on the straight and narrow.
Fletcher, R. (1987), Practical Methods of Optimization (2nd edn), Wiley, London.
Gear, C.W. (1971), Numerical Initial Value Problems in Ordinary DiÔ¨Äerential Equations,
Prentice‚ÄìHall, Englewood CliÔ¨Äs, NJ.
Ortega, J.M. and Rheinboldt, W.C. (1970), Iterative Solution of Nonlinear Equations in
Several Variables, Academic Press, New York.
Shampine, L.F. and Gordon, M.K. (1975), Computer Solution of Ordinary DiÔ¨Äerential Equa-
tions, W.H. Freeman, San Francisco.
Exercises
7.1
Let g(w) = Œõw + a, where Œõ is a d √ó d matrix.
a Prove that the inequality (i) of Theorem 7.1 is satisÔ¨Åed for Œª = h‚à•Œõ‚à•and
œÅ = ‚àû. Deduce a condition on h that ensures that all the assumptions of

134
Nonlinear algebraic systems
the theorem are valid.
b Let ‚à•¬∑ ‚à•be the Euclidean norm (A.1.3.3). Show that the above value of
Œª is the best possible, in the following sense: there exist no œÅ > 0 and
0 < Œª < h‚à•Œõ‚à•such that
‚à•g(v) ‚àíg(u)‚à•‚â§Œª
h‚à•v ‚àíu‚à•
for all v, u ‚ààBœÅ(w[0]). (Hint: Recalling that
‚à•Œõ‚à•= max
#‚à•Œõx‚à•
‚à•x‚à•
: x ‚ààRd, x Ã∏= 0
$
,
prove that for every Œµ > 0 there exists xŒµ such that ‚à•Œõ‚à•= ‚à•ŒõxŒµ‚à•/ ‚à•xŒµ‚à•and
‚à•xŒµ‚à•= Œµ. For any œÅ > 0 choose v = w[0] + xœÅ and u = w[0].)
7.2
Let g(w) = Œõw + a, where Œõ is a d √ó d matrix.
a Prove that the Newton‚ÄìRaphson method (7.7) converges (in exact arith-
metic) in a single iteration.
b Suppose that J = Œõ in the modiÔ¨Åed Newton‚ÄìRaphson method (7.9). Prove
that also in this case just a single iteration is required.
7.3
Prove that the modiÔ¨Åed Newton‚ÄìRaphson iteration (7.9) can be written as
the functional iteration scheme
w[i+1] = hÀúg(w[i]) + ÀúŒ≤,
i = 0, 1, . . . ,
where Àúg and ÀúŒ≤ are given by (7.10).
7.4
Let the two-step Adams‚ÄìBashforth method (2.6) and the trapezoidal rule
(1.9) be respectively the predictor and the corrector of a PECE scheme.
a Applying the scheme with a constant step size to the linear scalar equation
y‚Ä≤ = Œªy, y(0) = 1, prove that
yn+1 ‚àí

1 + hŒª + 3
4(hŒª)2
yn + 1
4(hŒª)2yn‚àí1 = 0,
n = 1, 2, . . .
(7.12)
b Prove that, unlike the trapezoidal rule itself, the PECE scheme is not A-
stable. (Hint: Let h|Œª| ‚â´1. Prove that every solution of (7.12) is of the
form yn ‚âàc
 3
4(hŒª)2n for large n.)
7.5
Consider the PECE iteration
xn+3 = ‚àí1
2yn + 3yn+1 ‚àí3
2yn+2 + 3hf(tn+2, yn+2),
yn+3 =
1
11[2yn ‚àí9yn+1 + 18yn+2 + 6hf(tn+3, xn+3)].
a Show that both methods are third order and that the Milne device gives an
estimate
6
17(xn+3 ‚àíyn+3) of the error of the corrector formula.

Exercises
135
b Let the method be applied to scalar equations, let the cubic polynomial pn+2
interpolate ym at m = n, n + 1, n + 2 and let p‚Ä≤
n+2(tn+2) = f(tn+2, yn+2).
Verify that the predictor and corrector are equivalent to the formulae
xn+1 = pn+2(tn+3)
= pn+2(tn+2) + hp‚Ä≤
n+2(tn+2) + 1
2h2p‚Ä≤‚Ä≤
n+2(tn+2) + 1
6h3p‚Ä≤‚Ä≤‚Ä≤
n+2(tn+2),
yn+1 = pn+2(tn+2) + 5
11hp‚Ä≤
n+2(tn+2) ‚àí1
22h2p‚Ä≤‚Ä≤
n+2(tn+2) ‚àí7
66p‚Ä≤‚Ä≤‚Ä≤
n+2(tn+2)
+ 6
11hf(tn+3, xn+3)
respectively. These formulae make it easy to change the value of h at tn+2
if the Milne estimate is unacceptably large.


P A R T II
The Poisson equation


8
Finite diÔ¨Äerence schemes
8.1
Finite diÔ¨Äerences
The opening line of Anna Karenina, ‚ÄòAll happy families resemble one another, but each
unhappy family is unhappy in its own way‚Äô,1 is a useful metaphor for the computation
of ordinary diÔ¨Äerential equations (ODEs) as compared with that of partial diÔ¨Äerential
equations (PDEs). Ordinary diÔ¨Äerential equations are a happy family; perhaps they
do not resemble each other but, at the very least, we can write them in a single
overarching form y‚Ä≤ = f(t, y) and treat them by a relatively small compendium of
computational techniques. (True, upon closer examination, even ODEs are not all the
same: their classiÔ¨Åcation into stiÔ¨Äand non-stiÔ¨Äis the most obvious example. How
many happy families will survive the deconstructing attentions of a mathematician?)
Partial diÔ¨Äerential equations, however, are a huge and motley collection of prob-
lems, each unhappy in its own way. Most students of mathematics will be aware of
the classiÔ¨Åcation into elliptic, parabolic and hyperbolic equations, but this is only the
Ô¨Årst step in a long journey. As soon as nonlinear ‚Äì or even quasilinear ‚Äì PDEs are
admitted for consideration, the subject is replete with an enormous number of diÔ¨Äer-
ent problems and each problem clamours for its own brand of numerics. No textbook
can (or should) cover this enormous menagerie. Fortunately, however, it is possible
to distil a small number of tools that allow for a well-informed numerical treatment
of several important equations and form a sound basis for the understanding of the
subject as a whole.
One such tool is the classical theory of Ô¨Ånite diÔ¨Äerences. The main idea in the
calculus of Ô¨Ånite diÔ¨Äerences is to replace derivatives with linear combinations of dis-
crete function values. Finite diÔ¨Äerences have the virtue of simplicity and they account
for a large proportion of the numerical methods actually used in applications. This
is perhaps a good place to stress that alternative approaches abound, each with its
own virtue: Ô¨Ånite elements, spectral and pseudospectral methods, boundary elements,
spectral elements, particle methods, meshless methods . . . Chapter 9 is devoted to the
Ô¨Ånite element method and Chapter 10 to spectral methods.
It is convenient to introduce Ô¨Ånite diÔ¨Äerences in the context of real (or complex)
sequences z = {zk}‚àû
k=‚àí‚àûindexed by all the integers. Everything can be translated to
Ô¨Ånite sequences in a straightforward manner, except that the notation becomes more
cumbersome.
1Leo Tolstoy, Anna Karenina, Translated by L. & A. Maude, Oxford University Press, London
(1967).
139

140
Finite diÔ¨Äerence schemes
We commence by deÔ¨Åning the following Ô¨Ånite diÔ¨Äerence operators, which map the
space RZ of all such sequences into itself. Each operator is deÔ¨Åned in terms of its
action on individual elements of the sequence z:
the shift operator,
(Ez)k = zk+1;
the forward diÔ¨Äerence operator,
(‚àÜ+z)k = zk+1 ‚àízk;
the backward diÔ¨Äerence operator,
(‚àÜ‚àíz)k = zk ‚àízk‚àí1;
the central diÔ¨Äerence operator,
(‚àÜ0z)k = zk+ 1
2 ‚àízk‚àí1
2 ;
the averaging operator,
(Œ•0z)k =
1
2(zk‚àí1
2 + zk+ 1
2 ).
The Ô¨Årst three operators are deÔ¨Åned for all k = 0, ¬±1, ¬±2, . . . Note, however, that the
last two operators, ‚àÜ0 and Œ•0, do not, as a matter of fact, map z into itself. After
all, the values zk+1/2 are meaningless for integer k. Having said this, we will soon see
that, appropriately used, these operators can be perfectly well deÔ¨Åned.
Let us assume further that the sequence z originates in the sampling of a function
z, say, at equispaced points. In other words, zk = z(kh) for some h > 0. Stipulating
(for the time being) that z is an entire function, we deÔ¨Åne
the diÔ¨Äerential operator,
(Dz)k = z‚Ä≤(kh).
Our Ô¨Årst observation is that all these operators are linear: given that
T ‚àà{E, ‚àÜ+, ‚àÜ‚àí, ‚àÜ0, Œ•0, D},
and that w, z ‚ààRZ, a, b ‚ààR, it is true that
T (aw + bz) = aT w + bT z.
The superposition of Ô¨Ånite diÔ¨Äerence operators is deÔ¨Åned in an obvious manner, e.g.
‚àÜ+E2zk = ‚àÜ+ (E(Ezk)) = ‚àÜ+(Ezk+1) = ‚àÜ+zk+2 = zk+3 ‚àízk+2.
Note that we have just introduced a notational shortcut: T zk stands for (T z)k, where
T is an arbitrary Ô¨Ånite diÔ¨Äerence operator.
The purpose of the calculus of Ô¨Ånite diÔ¨Äerences is, ultimately, to approximate
derivatives by linear combinations of function values along a grid. We wish to get rid
of D by expressing it in the currency of the other operators. This, however, requires
us Ô¨Årst to deÔ¨Åne formally general functions of Ô¨Ånite diÔ¨Äerence operators.
Because of our assumption that zk = z(kh), k = 0, ¬±1, ¬±2, . . . , Ô¨Ånite diÔ¨Äerence
operators depend upon the parameter h.
Let g(x) = ‚àû
j=0 ajxj be an arbitrary
analytic function, given in terms of its Taylor series. Noting that
E ‚àíI, Œ•0 ‚àíI, ‚àÜ+, ‚àÜ‚àí, ‚àÜ0, hD
h‚Üí0+
‚àí‚Üí
O,
where I is the identity, we can formally expand g about E ‚àíI, Œ•0 ‚àíI, ‚àÜ+ etc. For
example,
g(‚àÜ+)z =
‚éõ
‚éù
‚àû

j=0
aj‚àÜj
+
‚éû
‚é†z =
‚àû

j=0
aj(‚àÜj
+z).

8.1
Finite diÔ¨Äerences
141
It is not our intention to argue here that the above expansions converge (although
they do) but merely to use them in a formal manner to deÔ¨Åne functions of operators.
3 The operator E1/2
What is the square root of the shift operator? One
interpretation, which follows directly from the deÔ¨Ånition of E, is that E1/2 is
a ‚Äòhalf-shift‚Äô, which takes zk to zk+1/2; this we can deÔ¨Åne as z((k + 1
2)h). An
alternative expression exploits the power series expansion
‚àö
1 + x = 1 +
‚àû

j=1
(‚àí1)j‚àí1
22j‚àí1
 (2j ‚àí2)!
(j ‚àí1)!j!xj
to argue that
E1/2 = I ‚àí2
‚àû

j=1
(2j ‚àí2)!
(j ‚àí1)!j!

‚àí1
4(E ‚àíI)
j .
Needless to say, the two deÔ¨Ånitions coincide, but the proof of this would
proceed at a tangent to the theme of this chapter.
Readers familiar with
Newton‚Äôs interpolation formula might seek a proof by interpolating z(x + 1
2)
on the set {x + jh}‚Ñì
j=0 and letting ‚Ñì‚Üí‚àû.
3
Recalling the purpose of our analysis, we next express all Ô¨Ånite diÔ¨Äerence operators in
a single currency, as functions of the shift operator E. It is trivial that ‚àÜ+ = E ‚àíI
and ‚àÜ‚àí= I ‚àíE‚àí1, while the interpretation of E1/2 as a ‚Äòhalf-shift‚Äô implies that
‚àÜ0 = E1/2 ‚àíE‚àí1/2 and Œ•0 = 1
2(E‚àí1/2 + E1/2). Finally, to express D in terms of the
shift operator, we recall the Taylor theorem: for any analytic function z it is true that
Ez(x) = z(x + h) =
‚àû

j=0
1
j!
 djz(x)
dxj

hj =
‚é°
‚é£
‚àû

j=0
1
j!(hD)j
‚é§
‚é¶z(x) = ehDz(x),
and we deduce that E = ehD.2 Formal inversion yields
hD = ln E.
(8.1)
We conclude that, each having been expressed in terms of E, all six Ô¨Ånite diÔ¨Äerence
operators commute. This is a useful observation since it follows that we need not
bother with the order of their action whenever they are superposed.
The above operator formulae can be (formally) inverted, thereby expressing E in
terms of ‚àÜ+ etc. It is easy to verify that E = I + ‚àÜ+ = (I ‚àí‚àÜ‚àí)‚àí1. The expression
for ‚àÜ0 is a quadratic equation for E1/2,
(E1/2)2 ‚àí‚àÜ0E1/2 ‚àíI = O,
with two solutions,
1
2‚àÜ0 ¬±
/
1
4‚àÜ2
0 + I. Letting h ‚Üí0, we deduce that the correct
formula is
E =
	
1
2‚àÜ0 +
/
I + 1
4‚àÜ2
0

2
.
2We have already encountered a similar construction in Chapter 2.

142
Finite diÔ¨Äerence schemes
We need not bother to express E in terms of Œ•0, since this serves no useful purpose.
Combining (8.1) with these expressions, we next write the diÔ¨Äerential operator in
terms of other Ô¨Ånite diÔ¨Äerence operators,
hD = ln(I + ‚àÜ+)
(8.2)
hD = ‚àíln(I ‚àí‚àÜ‚àí)
(8.3)
hD = 2 ln
	
1
2‚àÜ0 +
/
I + 1
4‚àÜ2
0

.
(8.4)
Recall that the purpose of the exercise is to approximate the diÔ¨Äerential operator D
and its powers (which, of course, correspond to higher derivatives). The formulae
(8.2)‚Äì(8.4) are ideally suited to this purpose. For example, expanding (8.2) we obtain
D = 1
h ln(I + ‚àÜ+) = 1
h

‚àÜ+ ‚àí1
2‚àÜ2
+ + 1
3‚àÜ3
+ + O

‚àÜ4
+

= 1
h

‚àÜ+ ‚àí1
2‚àÜ2
+ + 1
3‚àÜ3
+

+ O

h3
,
h ‚Üí0,
where we exploit the estimate ‚àÜ+ = O(h), h ‚Üí0. Operating s times, we obtain an
expression for the sth derivative, s = 1, 2, . . . ,
Ds = 1
hs

‚àÜs
+ ‚àí1
2s‚àÜs+1
+
+ 1
24s(3s + 5)‚àÜs+2
+

+ O

h3
,
h ‚Üí0.
(8.5)
The meaning of (8.5) is that the linear combination
1
hs

‚àÜs
+ ‚àí1
2s‚àÜs+1
+
+ 1
24s(3s + 5)‚àÜs+2
+

zk
(8.6)
of the s + 3 grid values zk, zk+1, . . . , zk+s+2 approximates dsz(kh)/ dxs up to O

h3
.
Needless to say, truncating (8.5) a term earlier, for example, we obtain order O

h2
,
whereas higher order can be obtained by expanding the logarithm further.
Similarly to (8.5), we can use (8.3) to express derivatives in terms of grid points
wholly to the left,
Ds = (‚àí1)s
hs
[ln(I ‚àí‚àÜ‚àí)]s = 1
hs

‚àÜs
‚àí+ 1
2s‚àÜs+1
‚àí
+ 1
24s(3s + 5)‚àÜs+2
‚àí

+O

h3
,
h ‚Üí0.
However, does it make much sense to approximate derivatives solely in terms of grid
points that all lie to one side? Sometimes we have little choice ‚Äì more about this later
‚Äì but in general it is a good policy to match the numbers of points on the left and on
the right. The natural candidate for this task would be the central Ô¨Ånite diÔ¨Äerence
operator ‚àÜ0 except that now, having at last started to discuss approximation on a grid,
not just operators in a formal framework, we can no longer loftily disregard the fact
that ‚àÜ0z is not a proper grid sequence. The crucial observation is that even powers
of ‚àÜ0 map the set RZ of grid sequences to itself! Thus, ‚àÜ2
0zn = zn‚àí1 ‚àí2zn + zn+1
and the proof for all even powers follows at once from the trivial observation that
‚àÜ2s
0 =

‚àÜ2
0
s.

8.1
Finite diÔ¨Äerences
143
Recalling (8.4), we consider the Taylor expansion of the function g(Œæ) := ln(Œæ +
0
1 + Œæ2). By the generalized binomial theorem, we have
g‚Ä≤(Œæ) =
1
0
1 + Œæ2 =
‚àû

j=0
(‚àí1)j
	2j
j

  1
2Œæ
2j ,
where
	2j
j

is a binomial coeÔ¨Écient equal to (2j)!/(j!)2. Since g(0) = 0 and the
Taylor series converges uniformly for |Œæ| < 1, integration yields
g(Œæ) = g(0) +
 Œæ
0
g‚Ä≤(œÑ) dœÑ = 2
‚àû

j=0
(‚àí1)j
2j + 1
	2j
j

  1
2Œæ
2j+1 .
Letting Œæ = 1
2‚àÜ0, we thus deduce from (8.4) the formal expansion
D = 2
h g
 1
2‚àÜ0

= 4
h
‚àû

j=0
(‚àí1)j
2j + 1
	2j
j

  1
4‚àÜ0
2j+1 .
(8.7)
Unfortunately, the expression (8.7) is of exactly the wrong kind ‚Äì all the powers of
‚àÜ0 therein are odd! However, since even powers of odd powers are themselves even,
raising (8.7) to an even power yields
D2s =
1
h2s

(‚àÜ2
0)s ‚àís
12(‚àÜ2
0)s+1 + s(11 + 5s)
1440
(‚àÜ2
0)s+2
(8.8)
‚àís(382 + 231s + 35s2)
362880
(‚àÜ2
0)s+3

+ O

h8
,
h ‚Üí0.
Thus, for example, the linear combination
1
h2s

(‚àÜ2
0)s ‚àís
12(‚àÜ2
0)s+1 + s(11 + 5s)
1440
(‚àÜ2
0)s+2

zk
(8.9)
approximates d2sz(kh)/ dx2s to O

h6
.
How eÔ¨Äective is (8.9) in comparison with (8.6)? To attain O

h2p
, (8.6) requires
2s + 2p adjacent grid points and (8.9) just 2s + 2p ‚àí1, a relatively modest saving.
Central diÔ¨Äerence operators, however, have smaller error constants (see Exercises 8.3
and 8.4). More importantly, they are more convenient to use and usually lead to more
tractable linear algebraic systems (see Chapters 11‚Äì15).
The expansion (8.8) is valid only for even derivatives.
To reap the beneÔ¨Åts of
central diÔ¨Äerencing for odd derivatives, we require a simple, yet clever, trick. Let us
thus pay attention to the averaging operator Œ•0, which has until now had only a silent
part in the proceedings.
We express Œ•0 in terms of ‚àÜ0. Since Œ•0 = 1
2(E1/2+E‚àí1/2) and ‚àÜ0 = E1/2‚àíE‚àí1/2,
it follows that
4Œ•2
0 = E + 2I + E‚àí1,
‚àÜ2
0 = E ‚àí2I + E‚àí1

144
Finite diÔ¨Äerence schemes
and, subtracting, we deduce that 4Œ•0 ‚àí‚àÜ2
0 = 4I. We conclude that
Œ•0 = (I + 1
4‚àÜ2
0)1/2.
(8.10)
The main idea now is to multiply (8.7) by the identity I, which we craftily disguise
by using (8.10),
I = Œ•0

I + 1
4‚àÜ2
0
‚àí1/2 = Œ•0
‚àû

j=0
(‚àí1)j
	2j
j

  1
16‚àÜ2
0
j .
The outcome,
D = 1
h(Œ•0‚àÜ0)
‚é°
‚é£
‚àû

j=0
(‚àí1)j
	2j
j

  1
16‚àÜ2
0
j
‚é§
‚é¶
 ‚àû

i=0
(‚àí1)i
2i + 1
	2i
i

  1
16‚àÜ2
0
i

,
(8.11)
might look messy, but has one redeeming virtue: it is constructed exclusively from
even powers of ‚àÜ0 and Œ•0‚àÜ0. Since
Œ•0‚àÜ0zk = Œ•0

zk+ 1
2 ‚àízk‚àí1
2

= 1
2 (zk+1 ‚àízk‚àí1) ,
we conclude that (8.11) is a linear combination of terms that reside on the grid.
The expansion (8.11) can be raised to a power but this is not a good idea, since
such a procedure is wasteful in terms of grid points; an example is provided by
D2 = 1
h2 (Œ•0‚àÜ0)2 
I ‚àí1
3‚àÜ2
0

+ O

h4
.
Since
(Œ•0‚àÜ0)2 
I ‚àí1
3‚àÜ2
0

zk =
1
12(‚àízk‚àí3 + 5zk‚àí2 + zk‚àí1 ‚àí10zk + zk+1 + 5zk+2 ‚àízk+3),
we need seven points to attain O

h4
, while (8.9) requires just Ô¨Åve points. In general,
a considerably better idea is Ô¨Årst to raise (8.7) to an odd power and then to multiply
it by I = Œ•0(I + 1
4‚àÜ2
0)1/2. The outcome,
D2s+1 =
1
h2s+1 (Œ•0‚àÜ0)

(‚àÜ2
0)s ‚àí1
12(s + 2)(‚àÜ2
0)s+1
+
1
1440(s + 3)(5s + 16)(‚àÜ2
0)s+2
+ O

h5
,
h ‚Üí0,
(8.12)
lives on the grid and, other things being equal, is the recommended approximation of
odd derivatives.
3 A simple example . . .
Figure 8.1 displays the (natural) logarithm of the
error in the approximation of the Ô¨Årst derivative of z(x) = x ex. The Ô¨Årst row
corresponds to the forward diÔ¨Äerence approximations
1
h‚àÜ+,
1
h

‚àÜ+ ‚àí1
2‚àÜ2
+

and
1
h

‚àÜ+ ‚àí1
2‚àÜ2
+ + 1
3‚àÜ3
+

,

8.1
Finite diÔ¨Äerences
145
‚àí1
0
1
‚àí8
‚àí6
‚àí4
‚àí2
0
 h  = 1/10
forward differences
‚àí1
0
1
‚àí12
‚àí10
‚àí8
‚àí6
‚àí4
central differences
‚àí1
0
1
‚àí10
‚àí8
‚àí6
‚àí4
‚àí2
 h = 1/20
‚àí1
0
1
‚àí15
‚àí10
‚àí5
‚àí1
0
1
‚àí14
‚àí12
‚àí10
‚àí8
‚àí6
‚àí4
 h = 1/100
‚àí1
0
1
‚àí22
‚àí20
‚àí18
‚àí16
‚àí14
‚àí12
‚àí10
‚àí8
Figure
8.1
The
error
(on
a
logarithmic
scale)
in
the
approximation
of
z‚Ä≤, where z(x)
=
xex, ‚àí1
‚â§
x
‚â§
1.
Forward diÔ¨Äerences of size O(h)
(solid line), O(h2) (broken line) and O(h3) (broken-and-dotted line) feature
in the Ô¨Årst row, while the second row presents central diÔ¨Äerences of size O(h2)
(solid line) and O(h4) (broken line).
with h =
1
10 and h =
1
20 in the Ô¨Årst and in the second column respectively.
The second row displays the central diÔ¨Äerence approximations
1
hŒ•0‚àÜ0
and
1
hŒ•0‚àÜ0

I ‚àí1
6‚àÜ2
0

.
What can we learn from this Ô¨Ågure?
If the error behaves like chp, where
c Ã∏= 0, then its logarithm is approximately p ln h + ln |c|. Therefore, for small
h, one can expect each ‚Ñìth curve in the top row to behave like the Ô¨Årst curve,
scaled by ‚Ñì(since p = ‚Ñìfor the ‚Ñìth curve). This is not the case in the Ô¨Årst
two columns, since h > 0 is not small enough, but the pattern becomes more
visible when h decreases; the reader could try h =
1
1000 to conÔ¨Årm that this
asymptotic behaviour indeed takes place. However, replacing h by 1
2h should
lower each curve by an amount roughly equal to ln 2 ‚âà0.6931, and this can
be observed by comparing the Ô¨Årst two columns. Likewise, the curves in the
third column are each lowered by about ln 5 ‚âà1.6094 in comparison with the
second column.

146
Finite diÔ¨Äerence schemes
0
2
4
‚àí10
‚àí5
0
 h = 1/10
forward differences
0
2
4
‚àí15
‚àí10
‚àí5
0
central differences
0
2
4
‚àí10
‚àí5
0
 h = 1/20
0
2
4
‚àí20
‚àí15
‚àí10
‚àí5
0
2
4
‚àí15
‚àí10
‚àí5
0
 h = 1/100
0
2
4
‚àí25
‚àí20
‚àí15
‚àí10
‚àí5
Figure 8.2
The error (on a logarithmic scale) in the approximation of z‚Ä≤‚Ä≤, where
z(x) = 1/(1 + x), ‚àí1
2 ‚â§x ‚â§4. For the meaning of the curves, see the caption to
Fig. 8.1.
Similar information is displayed in Fig. 8.2, namely the logarithm of the error
in approximating z‚Ä≤‚Ä≤, where z(x) = 1/(1+x), by forward diÔ¨Äerences (in the top
row) and central diÔ¨Äerences (in the second row). The speciÔ¨Åc approximants
1
h2 ‚àÜ2
+,
1
h2 (‚àÜ2
+ ‚àí‚àÜ3
+),
1
h2 (‚àÜ2
+ ‚àí‚àÜ3
+ + 11
12‚àÜ4
+)
and
1
h2 ‚àÜ2
0,
1
h2 (‚àÜ2
0 ‚àí1
12‚àÜ4
0)
can be easily derived from (8.6) and (8.9) respectively. The pattern is similar,
except that the singularity at x = ‚àí1 means that the quality of approximation
deteriorates at the left end of the scale; it is always important to bear in mind
that estimates based on Taylor expansions break down near singularities.
3
Needless to say, there is no bar on using several Ô¨Ånite diÔ¨Äerence operators in a single
formula (see Exercise 8.5). However, other things being equal, in such cases we usually
prefer to employ central diÔ¨Äerences.

8.2
The Ô¨Åve-point formula for ‚àá2u = f
147
There are two important exceptions. Firstly, realistic grids do not in fact extend
from ‚àí‚àûto ‚àû; this was just a convenient assumption, which has simpliÔ¨Åed the
notation a great deal.
Of course, we can employ Ô¨Ånite diÔ¨Äerences on Ô¨Ånite grids,
except that the procedure might break down near the boundary. ‚ÄòOne-sided‚Äô Ô¨Ånite
diÔ¨Äerences possess obvious advantages in such situations. Secondly, for some PDEs
the exact solution of the equation displays an innate ‚Äòpreference‚Äô toward one spatial
direction over the other, and in this case it is a good policy to let the approximation
to the derivative reÔ¨Çect this fact. This behaviour is displayed by certain hyperbolic
equations and we will encounter it in Chapter 17.
Finally, it is perfectly possible to approximate derivatives on non-equidistant grids.
This, however, is by and large outside the scope of this book, except for a brief
discussion of approximation near curved boundaries in the next section.
8.2
The Ô¨Åve-point formula for ‚àá2u = f
Perhaps the most important and ubiquitous PDE is the Poisson equation
‚àá2u = f,
(x, y) ‚àà‚Ñ¶,
(8.13)
where
‚àá2 = ‚àÇ2
‚àÇx2 + ‚àÇ2
‚àÇy2 ,
f = f(x, y) is a known continuous function and the domain ‚Ñ¶‚äÇR2 is bounded, open
and connected and has a piecewise-smooth boundary. We hasten to add that this is
not the most general form of the Poisson equation ‚Äì in fact we are allowed any number
of space dimensions, not just two, ‚Ñ¶need not be bounded and its boundary, as well
as the function f, can satisfy far less demanding smoothness requirements. However,
the present framework is suÔ¨Écient for our purpose.
Like any partial diÔ¨Äerential equation, for its solution (8.13) must be accompanied
by a boundary condition. We assume the Dirichlet condition, namely that
u(x, y) = œÜ(x, y),
(x, y) ‚àà‚àÇ‚Ñ¶.
(8.14)
An implementation of Ô¨Ånite diÔ¨Äerences always commences by inscribing a grid into
the domain of interest. In our case we impose on cl ‚Ñ¶a square grid ‚Ñ¶‚àÜx parallel to
the axes, with an equal spacing of ‚àÜx in both spatial directions (Fig. 8.3). In other
words, we choose ‚àÜx > 0, (x0, y0) ‚àà‚Ñ¶and let ‚Ñ¶‚àÜx be the set of all points of the form
(x0 + k‚àÜx, y0 + ‚Ñì‚àÜx) that reside in the closure of ‚Ñ¶. We denote
I‚àÜx :=

(k, ‚Ñì) ‚ààZ2 : (x0 + k‚àÜx, y0 + ‚Ñì‚àÜx) ‚ààcl ‚Ñ¶

,
I‚ó¶
‚àÜx :=

(k, ‚Ñì) ‚ààZ2 : (x0 + k‚àÜx, y0 + ‚Ñì‚àÜx) ‚àà‚Ñ¶

,
and, for every (k, ‚Ñì) ‚ààI‚ó¶
‚àÜx, we let uk,‚Ñìstand for the approximation to the solution
u(x0 + k‚àÜx, y0 + ‚Ñì‚àÜx) of the Poisson equation (8.13) at the relevant grid point. Note
that, of course, there is no need to approximate grid points (k, ‚Ñì) ‚ààI‚àÜx \ I‚ó¶
‚àÜx, since
they lie on ‚àÇ‚Ñ¶and there exact values are given by (8.14).

148
Finite diÔ¨Äerence schemes
c
c
c
c
s
s
c
c
s
s
s
c
c
s
s
s
c
c
s
s
c
c
s
s
s
c
c
s
s
s
c
c
s
s
s
c
√ó
c
c
Figure 8.3
An example of a computational grid for a two-dimensional domain ‚Ñ¶.
s, internal points; c, near-boundary points; √ó, a boundary point.
Wishing to approximate ‚àá2 by Ô¨Ånite diÔ¨Äerences, our Ô¨Årst observation is that we are
no longer allowed the comfort of sequences that stretch all the way to ¬±‚àû; whether
in the x- or the y-direction, ‚àÇ‚Ñ¶acts as an impenetrable barrier and we cannot use
grid points outside cl ‚Ñ¶to assist in our approximation.
Our Ô¨Årst Ô¨Ånite diÔ¨Äerence scheme approximates ‚àá2u at the (k, ‚Ñì)th grid point as
a linear combination of the Ô¨Åve values uk,‚Ñì, uk¬±1,‚Ñì, uk,‚Ñì¬±1 and it is valid only if the
immediate horizontal and vertical neighbours of (k, ‚Ñì), namely (k ¬± 1, ‚Ñì) and (k, ‚Ñì¬± 1)
respectively, are in I‚àÜx. We say, for the purposes of our present discussion, that such
a point (x0 + k‚àÜx, y0 + ‚Ñì‚àÜx) is an internal point. In general, the set ‚Ñ¶‚àÜx consists
of three types of points: boundary points, which lie on ‚àÇ‚Ñ¶and whose value is known
by virtue of (8.14); internal points, which soon will be subjected to our scrutiny;
and the near-boundary points, where we can no longer employ Ô¨Ånite diÔ¨Äerences on an
equidistant grid so that a special approach is required. Needless to say, the deÔ¨Ånition
of the internal and near-boundary points changes if we employ a diÔ¨Äerent conÔ¨Åguration
of points in our Ô¨Ånite diÔ¨Äerence scheme (cf. Section 8.3).
Let us suppose that (k, ‚Ñì) ‚ààI‚ó¶
‚àÜx corresponds to an internal point. Following our
recommendation from Section 8.1, we use central diÔ¨Äerences. Of course, our grid is
now two dimensional and we can use diÔ¨Äerences in either coordinate direction. This
creates no diÔ¨Éculty, as long as we distinguish clearly the space coordinate with respect
to which our operator acts. We do this by appending a subscript, e.g. ‚àÜ0,x.
Let v = v(x, y), (x, y) ‚ààcl ‚Ñ¶, be an arbitrary suÔ¨Éciently smooth function.
It

8.2
The Ô¨Åve-point formula for ‚àá2u = f
149
follows at once from (8.9) that, for every internal grid point,
‚àÇ2v
‚àÇx2
x=x0+k‚àÜx,
y=y0+‚Ñì‚àÜx
=
1
(‚àÜx)2 ‚àÜ2
0,xvk,‚Ñì+ O

(‚àÜx)2
,
‚àÇ2v
‚àÇy2
x=x0+k‚àÜx,
y=y0+‚Ñì‚àÜx
=
1
(‚àÜx)2 ‚àÜ2
0,yvk,‚Ñì+ O

(‚àÜx)2
,
where vk,‚Ñìis the value of v at the (k, ‚Ñì)th grid point. Therefore,
1
(‚àÜx)2 (‚àÜ2
0,x + ‚àÜ2
0,y)
approximates ‚àá2 to order O

(‚àÜx)2
. This motivates the replacement of the Poisson
equation (8.13) by the Ô¨Åve point Ô¨Ånite diÔ¨Äerence scheme
1
(‚àÜx)2 (‚àÜ2
0,x + ‚àÜ2
0,y)uk,‚Ñì= fk,‚Ñì
(8.15)
at every pair (k, ‚Ñì) that corresponds to an internal grid point.
Of course, fk,‚Ñì=
f(x0 + k‚àÜx, y0 + ‚Ñì‚àÜx). More explicitly, (8.15) can be written in the form
uk‚àí1,‚Ñì+ uk+1,‚Ñì+ uk,‚Ñì‚àí1 + uk,‚Ñì+1 ‚àí4uk,‚Ñì= (‚àÜx)2fk,‚Ñì,
(8.16)
and this motivates its name, the Ô¨Åve-point formula. In lieu of the Poisson equation,
we have a linear combination of the values of u at an (internal) grid point and at the
immediate horizontal and vertical neighbours of this point.
Another way of depicting (8.16) is via a computational stencil (also known as a
computational molecule). This is a pictorial representation that is self-explanatory
(and becomes indispensable for more complicated Ô¨Ånite diÔ¨Äerence schemes, which
involve a larger number of points), as follows:










‚àí4
1
1
1
1
uk,‚Ñì= (‚àÜx)2fk,‚Ñì
Thus, the equation (8.16) links Ô¨Åve values of u in a linear fashion. Unless they lie on
the boundary, these values are unknown. The main idea of the Ô¨Ånite diÔ¨Äerence method
is to associate with every grid point having an index in I‚ó¶
‚àÜx (that is, every internal
and near-boundary point) a single linear equation, for example (8.16). This results in
a system of linear equations whose solution is our approximation u := (uk,‚Ñì)(k,‚Ñì)‚ààI‚ó¶
‚àÜx.
Three questions are critical to the performance of Ô¨Ånite diÔ¨Äerences.
‚Ä¢ Is the linear system nonsingular, so that the Ô¨Ånite diÔ¨Äerence solution u exists
and is unique?

150
Finite diÔ¨Äerence schemes
‚Ä¢ Suppose that a unique u = u‚àÜx exists for all suÔ¨Éciently small ‚àÜx, and let
‚àÜx ‚Üí0. Is it true that the numerical solution converges to the exact solution
of (8.13)? What is the asymptotic magnitude of the error?
‚Ä¢ Are there eÔ¨Écient and robust means to solve the linear system, which is likely
to consist of a very large number of equations?
We defer the third question to Chapters 11‚Äì15, where the theme of the numerical solu-
tion of large sparse algebraic linear systems will be debated at some length. Meantime,
we address ourselves to the Ô¨Årst two questions in the special case when ‚Ñ¶is a square.
Without loss of generality, we let ‚Ñ¶= {(x, y) : 0 < x, y < 1}. This leads to consider-
able simpliÔ¨Åcation since, provided we choose ‚àÜx = 1/(m + 1), say, for an integer m,
and let x0 = y0 = 0, all grid points are either internal or boundary (Fig. 8.4).
√ó
√ó
√ó
√ó
√ó
√ó
√ó
√ó
s
s
s
s
s
√ó
√ó
s
s
s
s
s
√ó
√ó
s
s
s
s
s
√ó
√ó
s
s
s
s
s
√ó
√ó
s
s
s
s
s
√ó
√ó
√ó
√ó
√ó
√ó
√ó
√ó
Figure 8.4
Computational grid for a unit square. As in Fig. 8.3, internal and
boundary points are denoted by solid circles and crosses, respectively.
3 The Laplace equation
Prior to attempting to prove theorems on the
behaviour of numerical methods, it is always a good practice to run a few
simple programs and obtain a ‚Äòfeel‚Äô for what we are, after all, trying to prove.
The computer is the mathematical equivalent of a physicist‚Äôs laboratory!
In this spirit we apply the Ô¨Åve-point formula (8.15) to the Laplace equation

8.2
The Ô¨Åve-point formula for ‚àá2u = f
151
‚àá2u = 0 in the unit square (0, 1)2, subject to the boundary conditions
u(x, 0) ‚â°0,
u(x, 1) =
1
(1 + x)2 + 1,
0 ‚â§y ‚â§1,
u(0, y) =
y
1 + y2 ,
u(1, y) =
y
4 + y2 ,
0 ‚â§x ‚â§1.
Figure 8.5 displays the exact solution of this equation,
u(x, y) =
y
(1 + x)2 + y2 ,
0 ‚â§x, y ‚â§1,
as well as its numerical solution by means of the Ô¨Åve-point formula with m = 5,
m = 11 and m = 23; this corresponds to ‚àÜx = 1
6, ‚àÜx =
1
12 and ‚àÜx =
1
24
respectively. The size of the grid halves in each consecutive numerical trial
and it is evident from the Ô¨Ågure that the error decreases by a factor of 4.
This is consistent with an error decay of O

(‚àÜx)2
, which is hinted at in our
construction of (8.15) and will be proved in Theorem 8.2.
3
Recall that we wish to address ourselves to two questions. Firstly, is the linear system
(8.16) nonsingular? Secondly, does its solution converge to the exact solution of the
Poisson equation (8.13) as ‚àÜx ‚Üí0? In the case of a square, both questions can be
answered by employing a similar construction.
The function uk,‚Ñìis deÔ¨Åned on a two-dimensional grid and, to write the linear
equations (8.16) formally in a matrix‚Äìvector notation, we need to rearrange uk,‚Ñìinto
a one-dimensional column vector u ‚ààRs, where s = m2. In other words, for any
permutation {(ki, ‚Ñìi)}i=1,2,...,s of the set {(k, ‚Ñì)}k,‚Ñì=1,2,...,m we can let
u =
‚é°
‚é¢‚é¢‚é¢‚é£
uk1,‚Ñì1
uk2,‚Ñì2
...
uks,‚Ñìs
‚é§
‚é•‚é•‚é•‚é¶
and write (8.16) in the form
Au = b,
(8.17)
where A is an s √ó s matrix, while b ‚ààRs includes both the inhomogeneous part
(‚àÜx)2fk,‚Ñì, similarly ordered, and the contribution of the boundary values.
Since
any permutation of the s grid points provides for a diÔ¨Äerent arrangement, there are
s! = (m2)! distinct ways of deriving (8.17). Fortunately, none of the features that are
important to our present analysis depends on the speciÔ¨Åc ordering of the uk,‚Ñì.
Lemma 8.1
The matrix A in (8.17) is symmetric and the set of its eigenvalues is
œÉ(A) = {ŒªŒ±,Œ≤ : Œ±, Œ≤ = 1, 2, . . . , m} ,
where
ŒªŒ±,Œ≤ = ‚àí4
#
sin2

Œ±œÄ
2(m + 1)

+ sin2

Œ≤œÄ
2(m + 1)
$
,
Œ±, Œ≤ = 1, 2, . . . , m.
(8.18)

152
Finite diÔ¨Äerence schemes
0
0.2
0.4
0.6
0.8
1.0
0
0.5
1.0
0
0.5
0
0.5
1.0
0
0.5
1.0
0
0.5
1.0
x 10
‚àí4
 
0
0.5
1.0
0
0.5
1.0
0
2.0
x 10
‚àí4
 
0
0.5
1.0
0
0.5
1.0
0
0.5
1.0
x 10
‚àí3
 
m = 5
m = 11
m = 23
Figure 8.5
The exact solution of the Laplace equation discussed in the text and
the errors of the Ô¨Åve-point formula for m = 5, m = 11 and m = 23, with 25, 121
and 529 grid points respectively.
Proof
To prove symmetry, we notice by inspection of (8.16) that all elements of
A must be ‚àí4, 1 or 0, according to the following rule. All diagonal elements aŒ≥,Œ≥
equal ‚àí4, whereas an oÔ¨Ä-diagonal element aŒ≥,Œ¥, Œ≥ Ã∏= Œ¥, equals 1 if (iŒ≥, jŒ≥) and (iŒ¥, jŒ¥)
are either horizontal or vertical neighbours and 0 otherwise. Being a neighbour is,
however, a commutative relation: if (iŒ≥, jŒ≥) is a neighbour of (iŒ¥, jŒ¥) then (iŒ¥, jŒ¥) is a
neighbour of (iŒ≥, jŒ≥). Therefore aŒ≥,Œ¥ = aŒ¥,Œ≥ for all Œ≥, Œ¥ = 1, 2, . . . , s.
To Ô¨Ånd the eigenvalues of A we disregard the exact way in which the matrix has
been composed ‚Äì after all, symmetric permutations conserve eigenvalues ‚Äì and, in-
stead, go back to the equations (8.16). Suppose that we can demonstrate the existence
of a nonzero function (vk,‚Ñì)k,‚Ñì=0,1,...,m+1 such that vk,0 = vk,m+1 = v0,‚Ñì= vm+1,‚Ñì= 0,
k, ‚Ñì= 1, 2, . . . , m, and such that the homogeneous set of linear equations
vk‚àí1,‚Ñì+ vk+1,‚Ñì+ vk,‚Ñì‚àí1 + vk,‚Ñì+1 ‚àí4vk,‚Ñì= Œªvk,‚Ñì,
k, ‚Ñì= 1, 2, . . . , m,
(8.19)

8.2
The Ô¨Åve-point formula for ‚àá2u = f
153
is satisÔ¨Åed for some Œª. It follows that, up to rearrangement, (vk,‚Ñì) is an eigenvector
and Œª is a corresponding eigenvalue of A.
Given Œ±, Œ≤ ‚àà{1, 2, . . . , m}, we let
vk,‚Ñì= sin
	 kŒ±œÄ
m + 1

sin
	 ‚ÑìŒ≤œÄ
m + 1

,
k, ‚Ñì= 0, 1, . . . , m + 1.
Note that, as required, vk,0 = vk,m+1 = v0,‚Ñì= vm+1,‚Ñì= 0, k, ‚Ñì= 1, 2, . . . , m. Substi-
tuting into (8.19), we obtain
vk‚àí1,‚Ñì+ vk+1,‚Ñì+ vk,‚Ñì‚àí1 + vk,‚Ñì+1 ‚àí4vk,‚Ñì
=
#
sin
(k ‚àí1)Œ±œÄ
m + 1

+ sin
(k + 1)Œ±œÄ
m + 1
$
sin
	 ‚ÑìŒ≤œÄ
m + 1

(8.20)
+ sin
	 kŒ±œÄ
m + 1

#
sin
(‚Ñì‚àí1)Œ≤œÄ
m + 1

+ sin
(‚Ñì+ 1)Œ≤œÄ
m + 1
$
‚àí4 sin
	 kŒ±œÄ
m + 1

sin
	 ‚ÑìŒ≤œÄ
m + 1

.
We exploit the trigonometric identity
sin(Œ∏ ‚àíœà) + sin(Œ∏ + œà) = 2 sin Œ∏ cos œà
to simplify (8.20), obtaining for the right-hand side
2 sin
	 kŒ±œÄ
m + 1

cos
	 Œ±œÄ
m + 1

sin
	 ‚ÑìŒ≤œÄ
m + 1

+ 2 sin
	 kŒ±œÄ
m + 1

sin
	 ‚ÑìŒ≤œÄ
m + 1

cos
	 Œ≤œÄ
m + 1

‚àí4 sin
	 kŒ±œÄ
m + 1

sin
	 ‚ÑìŒ≤œÄ
m + 1

= ‚àí2

2 ‚àícos
	 Œ±œÄ
m + 1

‚àícos
	
Œ≤œÄ
m + 1


sin
	 kŒ±œÄ
m + 1

sin
	 ‚ÑìŒ≤œÄ
m + 1

= ‚àí4
#
sin2

Œ±œÄ
2(m + 1)

+ sin2

Œ≤œÄ
2(m + 1)
$
vk,‚Ñì,
k, ‚Ñì= 1, 2, . . . , m.
Note that we have used in the last line the trigonometric identity
1 ‚àícos Œ∏ = 2 sin2
	Œ∏
2

.
We have thus demonstrated that (8.19) is satisÔ¨Åed by Œª = ŒªŒ±,Œ≤, and this completes
the proof of the lemma.
Corollary
The matrix A is negative deÔ¨Ånite and, a fortiori, nonsingular.
Proof
We have just shown that A is symmetric, and it follows from (8.18) that
all its eigenvalues are negative.
Therefore (see A.1.5.1) it is negative deÔ¨Ånite and
nonsingular.
3 Eigenvalues of the Laplace operator
Before we continue with the orderly
Ô¨Çow of our exposition, it is instructive to comment on how the eigenvalues and
eigenvectors of the matrix A are related to the eigenvalues and eigenfunctions
of the Laplace operator ‚àá2 in the unit square.

154
Finite diÔ¨Äerence schemes
The function v, not identically zero, is said to be an eigenfunction of ‚àá2 in a
domain ‚Ñ¶and Œª is the corresponding eigenvalue if v vanishes along ‚àÇ‚Ñ¶and
satisÔ¨Åes within ‚Ñ¶the equation ‚àá2v = Œªv. The linear system (8.19) is nothing
other than a Ô¨Åve-point discretization of this equation for ‚Ñ¶= (0, 1)2.
The eigenfunctions and eigenvalues of ‚àá2 can be evaluated easily and explic-
itly in the unit square. Given any two positive integers Œ±, Œ≤, we let v(x, y) =
sin(Œ±œÄx) sin(Œ≤œÄy), x, y ‚àà[0, 1]. Note that, as required, v obeys zero Dirichlet
boundary conditions. It is trivial to verify that ‚àá2v = ‚àí(Œ±2+Œ≤2)œÄ2v; hence v
is indeed an eigenfunction and the corresponding eigenvalue is ‚àí(Œ±2 + Œ≤2)œÄ2.
It is possible to prove that all eigenfunctions of ‚àá2 in (0, 1)2 have this form.
The vector vk,‚Ñìfrom the proof of Lemma 8.1 can be obtained by sampling of
the eigenfunction v at the grid points

k
m+1,
‚Ñì
k+1

k,‚Ñì=0,1,...,m+1 (for Œ±, Œ≤ =
1, 2, . . . , m only; the matrix A, unlike ‚àá2, acts on a Ô¨Ånite-dimensional space!),
whereas (‚àÜx)‚àí2ŒªŒ±,Œ≤ is a good approximation to ‚àí(Œ±2 +Œ≤2)œÄ2 provided Œ± and
Œ≤ are small in comparison with m. Expanding sin2 Œ∏ in a power series and
bearing in mind that (m + 1)‚àÜx = 1, we readily obtain
ŒªŒ±,Œ≤
(‚àÜx)2 = ‚àí4
1
Œ±œÄ
2(m + 1)
2
‚àí1
3

Œ±œÄ
2(m + 1)
4
+ ¬∑ ¬∑ ¬∑
2
+
1
Œ≤œÄ
2(m + 1)
2
‚àí1
3

Œ≤œÄ
2(m + 1)
4
+ ¬∑ ¬∑ ¬∑
2
= ‚àí(Œ±2 + Œ≤2)œÄ2 + 1
12(Œ±4 + Œ≤4)œÄ4(‚àÜx)2 + O

(‚àÜx)4
.
Hence, (m + 1)2ŒªŒ±,Œ≤ is a good approximation of an exact eigenvalue of the
Laplace operator for small Œ±, Œ≤, but the quality of approximation deteriorates
rapidly as soon as (Œ±4 + Œ≤4)(‚àÜx)2 becomes nonnegligible.
3
Let u be the exact solution of (8.13) in a unit square. We set Àúuk,‚Ñì= u(k‚àÜx, ‚Ñì‚àÜx) and
denote by ek,‚Ñìthe error of the Ô¨Åve-point formula (8.15) at the (k, ‚Ñì)th grid point, ek,‚Ñì=
uk,‚Ñì‚àíÀúuk,‚Ñì, k, ‚Ñì= 0, 1, . . . , m + 1. Let the Ô¨Åve-point equations (8.15) be represented in
the matrix form (8.17) and let e denote an arrangement of {ek,‚Ñì} into a vector in Rs,
s = m2, whose ordering is identical to that of u. We are measuring the magnitude of
e by the Euclidean norm ‚à•¬∑ ‚à•(A.1.3.3).
Theorem 8.2
Subject to suÔ¨Écient smoothness of the function f and the boundary
conditions, there exists a number c > 0, independent of ‚àÜx, such that
‚à•e‚à•‚â§c(‚àÜx)2,
‚àÜx ‚Üí0.
(8.21)
Proof
Since (‚àÜx)‚àí2(‚àÜ2
0,x + ‚àÜ2
0,y) approximates ‚àá2 locally to order O

(‚àÜx)2
, it
is true that
Àúuk‚àí1,‚Ñì+ Àúuk+1,‚Ñì+ Àúuk,‚Ñì‚àí1 + Àúuk,‚Ñì+1 ‚àí4Àúuk,‚Ñì= (‚àÜx)2fk,‚Ñì+ O

(‚àÜx)4
(8.22)

8.2
The Ô¨Åve-point formula for ‚àá2u = f
155
for ‚àÜx ‚Üí0. We subtract (8.22) from (8.16) and the outcome is
ek‚àí1,‚Ñì+ ek+1,‚Ñì+ ek,‚Ñì‚àí1 + ek,‚Ñì+1 ‚àí4ek,‚Ñì= O

(‚àÜx)4
,
‚àÜx ‚Üí0,
or, in vector notation (and paying due heed to the fact that uk,‚Ñìand Àúuk,‚Ñìcoincide
along the boundary)
Ae = Œ¥‚àÜx,
(8.23)
where Œ¥‚àÜx ‚ààRm2 is such that ‚à•Œ¥‚àÜx‚à•= O

(‚àÜx)4
. It follows from (8.23) that
e = A‚àí1Œ¥‚àÜx.
(8.24)
Recall from Lemma 8.1 that A is symmetric. Hence so is A‚àí1 and its Euclidean norm
‚à•A‚àí1‚à•is the same as its spectral radius œÅ(A‚àí1) (A.1.5.2). The latter can be computed
at once from (8.18), since Œª ‚ààœÉ(B) is the same as Œª‚àí1 ‚ààœÉ(B‚àí1) for any nonsingular
matrix B. Thus, bearing in mind that (m + 1)‚àÜx = 1,
œÅ(A‚àí1) =
max
Œ±,Œ≤=1,2,...,m
1
4
#
sin2

Œ±œÄ
2(m + 1)

+ sin2

Œ≤œÄ
2(m + 1)
$‚àí1
=
1
8 sin2 1
2‚àÜxœÄ .
Since
lim
‚àÜx‚Üí0

(‚àÜx)2
8 sin2 1
2‚àÜxœÄ

=
1
2œÄ2 ,
it follows that for any constant c1 > (2œÄ2)‚àí1 it is true that
‚à•A‚àí1‚à•= œÅ(A‚àí1) ‚â§c1(‚àÜx)‚àí2,
‚àÜx ‚Üí0.
(8.25)
Provided that f and the boundary conditions are suÔ¨Éciently smooth,3 u is itself
suÔ¨Éciently diÔ¨Äerentiable and there exists a constant c2 > 0 such that ‚à•Œ¥(‚àÜx)‚à•‚â§
c2(‚àÜx)4 (recall that Œ¥ depends solely on the exact solution). Substituting this and
(8.25) into the inequality (8.24) yields (8.21) with c = c1c2.
Our analysis can be generalized to rectangles, L-shaped domains etc., provided the
ratios of all sides are rational numbers (cf. Exercise 8.7). Unfortunately, in general
the grid contains near-boundary points, at which the Ô¨Åve-point formula (8.15) cannot
be implemented. To see this, it is enough to look at a single coordinate direction;
without loss of generality let us suppose that we are seeking to approximate ‚àá2 at the
point P in Fig. 8.6.
Given z(x), we Ô¨Årst approximate z‚Ä≤‚Ä≤ at P ‚àºx0 (we disregard the variable y,
which plays no part in this process) as a linear combination of the values of z at P,
Q ‚àºx0 ‚àí‚àÜx and T ‚àºx0 + œÑ‚àÜx. Expanding z in a Taylor series about x0, we can
easily show that
1
(‚àÜx)2

2
œÑ + 1z(x0 ‚àí‚àÜx) ‚àí2
œÑ z(x0) +
2
œÑ(œÑ + 1)z(x0 + œÑ‚àÜx)

= z‚Ä≤‚Ä≤(x0) + 1
3(œÑ ‚àí1)z‚Ä≤‚Ä≤‚Ä≤(x0)‚àÜx + O

(‚àÜx)2
.
3We prefer not to be very speciÔ¨Åc here, since the issues raised by the smoothness and diÔ¨Äeren-
tiability of Poisson equations are notoriously diÔ¨Écult. However, these requirements are satisÔ¨Åed in
most cases of practical interest.

156
Finite diÔ¨Äerence schemes
t
t
t
t
d
d
√ó
√ó
√ó
P
Q
V
S
R
T
3
45
6
‚àÜx
3 45 6
œÑ‚àÜx
Figure 8.6
Computational grid near a curved boundary.
Unless œÑ = 1, when everything reduces to the central diÔ¨Äerence approximation, the
error is just O(‚àÜx).
To recover order O

(‚àÜx)2
, consistently with the Ô¨Åve-point
formula at internal points, we add the function value at V ‚àºx0 ‚àí2‚àÜx to the linear
combination, whereby expansion in a Taylor series about x0 yields
z‚Ä≤‚Ä≤(x0) =
1
(‚àÜx)2
œÑ ‚àí1
œÑ + 2z(x0 ‚àí2‚àÜx) + 2(2 ‚àíœÑ)
œÑ + 1 z(x0 ‚àí‚àÜx) ‚àí3 ‚àíœÑ
œÑ
z(x0)
+
6
œÑ(œÑ + 1)(œÑ + 2)z(x0 + œÑ‚àÜx)

+ O

(‚àÜx)2
.
A good approximation to ‚àá2u at P should involve, therefore, six points, P, Q, R, S, T
and V . Assuming that P corresponds to the grid point (k0, ‚Ñì0), say, we obtain the
linear equation
œÑ ‚àí1
œÑ + 2uk0‚àí2,‚Ñì0 + 2(2 ‚àíœÑ)
œÑ + 1 uk0‚àí1,‚Ñì0 +
6
œÑ(œÑ + 1)(œÑ + 2)uk0+œÑ,‚Ñì0 + uk0,‚Ñì0‚àí1
+ uk0,‚Ñì0+1 ‚àí3 + œÑ
œÑ
uk0,‚Ñì0 = (‚àÜx)2fk0,‚Ñì0,
(8.26)
where (k0 + œÑ, ‚Ñì0) corresponds to the boundary point T, whose value is provided from
the Dirichlet boundary condition. Note that if œÑ = 1 and P becomes an internal point
then this reduces to the Ô¨Åve-point formula (8.16).
A similar treatment can be used in the y-direction. Of course, regardless of direc-
tion, we need ‚àÜx small enough that we have suÔ¨Écient information to implement (8.26)

8.2
The Ô¨Åve-point formula for ‚àá2u = f
157
or other O

(‚àÜx)2
approximants to ‚àá2 at all near-boundary points. The outcome,
in tandem with (8.16) at internal points, is a linear algebraic equation for every grid
point ‚Äì whether internal or near-boundary ‚Äì where the solution is unknown.
We will not extend Theorem 8.2 here and prove that the rate of decay of the error
is O

(‚àÜx)2
but will set ourselves a less ambitious goal: to prove that the linear
algebraic system is nonsingular. First, however, we require a technical lemma that is
of great applicability in many branches of matrix analysis.
Lemma 8.3 (The GerÀásgorin criterion)
Let B = (bk,‚Ñì) be an arbitrary irreducible
(A.1.2.5) complex d √ó d matrix. Then
œÉ(B) ‚äÇ
d7
i=1
Si,
where
Si =
‚éß
‚é®
‚é©z ‚ààC : |z ‚àíbi,i| ‚â§
d

j=1, jÃ∏=i
|bi,j|
‚é´
‚é¨
‚é≠
and œÉ(B) is the set containing the eigenvalues of B. Moreover, Œª ‚ààœÉ(B) may lie
on ‚àÇSi0 for some i0 ‚àà{1, 2, . . . , d} only if Œª ‚àà‚àÇSi for all i = 1, 2, . . . , d. The Si are
known as GerÀásgorin discs.
Proof
This is relegated to Exercise 8.8, where it is broken down into a number
of easily manageable chunks.
There is another part of the GerÀásgorin criterion that plays no role whatsoever in
our present discussion but we mention it as a matter of independent mathematical
interest. Thus, suppose that
{1, 2, . . . , d} = {i1, i2, . . . , ir} ‚à™{j1, j2, . . . , jd‚àír}
such that SiŒ± ‚à©SiŒ≤ Ã∏= ‚àÖ, Œ±, Œ≤ = 1, 2, . . . , r and SiŒ± ‚à©SjŒ≤ = ‚àÖ, i = 1, 2, . . . , r, j =
1, 2, . . . , d ‚àír. Let S := ‚à™r
Œ±=1SiŒ±. Then the set S includes exactly r eigenvalues of B.
Theorem 8.4
Let Au = b be the linear system obtained by employing the Ô¨Åve-
point formula (8.16) at internal points and the formula (8.26) or its reÔ¨Çections and
extensions (catering for the case when one horizontal and one vertical neighbour are
missing) at near-boundary points. Then A is nonsingular.
Proof
No matter how we arrange the unknowns into a vector, thereby determin-
ing the ordering of the rows and the columns of A, each row of A corresponds to a
single equation. Therefore all the elements along the ith row vanish, except for those
that feature in the linear equation that is obeyed at the grid point corresponding
to this row. It follows from an inspection of (8.16) and (8.26) that ai,i < 0, that

jÃ∏=i |ai,j| + ai,i ‚â§0 and that the inequality is sharp at a near-boundary point. (This
is trivial for (8.16), while, since œÑ ‚àà(0, 1], some oÔ¨Ä-diagonal components in (8.26)
might be negative. Yet, simple calculation conÔ¨Årms that the sum of absolute values of
all oÔ¨Ä-diagonal elements along a row is consistent with the above inequality. Of course,

158
Finite diÔ¨Äerence schemes
we must remember to disregard the contribution of boundary points.) It follows that
the origin may not lie in the interior of the GerÀásgorin disc Sj. Thus, by Lemma 8.3,
0 ‚ààœÉ(A) only if 0 ‚àà‚àÇSi for all rows i.
At least one equation has a neighbour on the boundary. Let this equation cor-
respond to the i0th row of A. Then 
jÃ∏=i0 |ai0,j| < |ai0,i0|, therefore 0 Ã∏‚ààSi0. We
deduce that it is impossible for 0 to lie on the boundaries of all the discs Si; hence, by
Lemma 8.3, it is not an eigenvalue. This means that A is nonsingular and the proof
is complete.
8.3
Higher-order methods for ‚àá2u = f
The Laplace operator ‚àá2 has a key role in many important equations of mathematical
physics, to mention just two, the parabolic diÔ¨Äusion equation
‚àÇu
‚àÇt = ‚àá2u,
u = u(x, y, t),
and the hyperbolic wave equation
‚àÇ2u
‚àÇt2 = ‚àá2u,
u = u(x, y, t).
Therefore, the Ô¨Åve-point approximation formula (8.15) is one of the workhorses of
numerical analysis. This all pervasiveness, however, motivates a discussion of higher-
order computational schemes.
Truncating (8.8) after two terms, we obtain in place of (8.15) the scheme
1
(‚àÜx)2

‚àÜ2
0,x + ‚àÜ2
0,y ‚àí1
12

‚àÜ4
0,x + ‚àÜ4
0,y

uk,‚Ñì= fk,‚Ñì.
(8.27)
More economically, this can be written as the computational stencil


















‚àí1
12
‚àí1
12
‚àí1
12
‚àí1
12
4
3
4
3
4
3
4
3
‚àí5
uk,‚Ñì= (‚àÜx)2fk,‚Ñì
Although the error is O

(‚àÜx)4
, (8.27) is not a popular method. It renders too many
points near-boundary, even in a square grid, which means that they require laborious
special treatment. Worse, it gives linear systems that are considerably more expensive

8.3
Higher-order methods for ‚àá2u = f
159
to solve than, for example, those generated by the Ô¨Åve-point scheme. In particular,
the fast solvers from Sections 15.1 and 15.2 cannot be implemented in this setting.
A more popular alternative is to approximate ‚àá2u at the (k, ‚Ñì)th grid point by
means of all its eight nearest neighbours: horizontal, vertical and diagonal.
This
results in the nine-point formula
1
(‚àÜx)2

‚àÜ2
0,x + ‚àÜ2
0,y + 1
6‚àÜ2
0,x‚àÜ2
0,y

uk,‚Ñì= fk,‚Ñì,
(8.28)
more familiarly known in the computational stencil notation


















‚àí10
3
2
3
2
3
2
3
2
3
1
6
1
6
1
6
1
6
uk,‚Ñì= (‚àÜx)2fk,‚Ñì
To analyse the error in (8.28) we recall from Section 8.1 that ‚àÜ0 = E1/2 ‚àíE‚àí1/2
and E = e‚àÜxD. Therefore, expanding in a Taylor series in ‚àÜx,
‚àÜ2
0 = E ‚àí2I + E‚àí1 = e‚àÜxD ‚àí2I + e‚àí‚àÜxD
= (‚àÜx)2D2 + 1
12(‚àÜx)4D4 + O

(‚àÜx)6
.
Since this is valid for both spatial variables and Dx, Dy commute, substitution into
(8.28) yields
1
(‚àÜx)2

‚àÜ2
0,x + ‚àÜ2
0,y + 1
6‚àÜ2
0,x‚àÜ2
0,y

=
1
(‚àÜx)2

(‚àÜx)2D2
x + 1
12(‚àÜx)4D4
x + O

(‚àÜx)6
+

(‚àÜx)2D2
y + 1
12(‚àÜx)4D4
y
+ O

(‚àÜx)6
+ 1
6

(‚àÜx)2D2
x + O

(‚àÜx)4 
(‚àÜx)2D2
y + O

(‚àÜx)4
= (D2
x + D2
y) + 1
12(‚àÜx)2(D2
x + D2
y)2 + O

(‚àÜx)4
= ‚àá2 + 1
12(‚àÜx)2‚àá4 + O

(‚àÜx)4
.
(8.29)
In other words, the error in the nine-point formula is of exactly the same order of
magnitude as that of the Ô¨Åve-point formula. Apparently, nothing is gained by incor-
porating the diagonal neighbours!
Not giving in to despair, we return to the example of Fig. 8.5 and recalculate it
with the nine-point formula (8.28). The results can be seen in Fig. 8.7 and, as can
be easily ascertained, they are inconsistent with our claim that the error decays as
O

(‚àÜx)2
! Bearing in mind that ‚àÜx decreases by a factor of 2 in each consecutive
graph, we would have expected the error to attenuate by a factor of 4 ‚Äì instead it
attenuates by a factor of 16. Too good to be true?
The reason for this spectacular behaviour is, to put it bluntly, our sloth. Had
we attempted to solve a Poisson equation with a nontrivial inhomogeneous term the

160
Finite diÔ¨Äerence schemes
0
0.5
1.0
0
0.5
1.0
0
1
2
x 10
‚àí10
 
0
0.5
1.0
0
0.5
1.0
0
1
x 10
‚àí8
 
0
0.5
1.0
0
0.5
1.0
0
0.5
1.0
x 10
‚àí6
 
m = 5
m = 11
m = 23
Figure 8.7
The errors in the solution of the Laplace equation from Fig. 8.5 by
the nine-point formula, for m = 5, m = 11 and m = 23.
decrease in the error would have been consistent with our error estimate (see Fig. 8.8).
Instead, we applied the nine-point scheme to the Laplace equation, which, as far as
(8.28) is concerned, is a special case. We now give a hand-waving explanation of this
phenomenon.
It follows from (8.29) that the nine-point scheme is an approximation of order
O

(‚àÜx)4
to the ‚Äòequation‚Äô

‚àá2 + 1
12(‚àÜx)2‚àá4
u = f.
(8.30)
Setting aside the dependence of (8.30) on ‚àÜx and the whole matter of boundary
conditions (we are hand-waving after all!), the operator M‚àÜx := I +
1
12(‚àÜx)2‚àá2 is
invertible for suÔ¨Éciently small ‚àÜx.
Multiplying (8.30) by its inverse while letting
f ‚â°0 indicates that the nine-point scheme bears an error of O

(‚àÜx)4
when applied
to the Laplace equation. This explains the rapid decay of the error in Fig. 8.7.
The Laplace equation has many applications and this superior behaviour of the
nine-point formula is a matter of interest. Remarkably, the logic that has led to an
explanation of this phenomenon can be extended to cater for Poisson equations as
well. Thus suppose that ‚àÜx > 0 is small enough that M‚àí1
‚àÜx exists, and act with this
operator on both sides of (8.30). Then we have
‚àá2u = M‚àí1
‚àÜxf,
(8.31)
a new Poisson equation for which the nine-point formula produces an error of order
O

(‚àÜx)4
. The only snag is that the right-hand side diÔ¨Äers from that in (8.13), but
this is easy to put right. We replace f in (8.31) by a function Àúf such that
Àúf(x, y) = f(x, y) + 1
12(‚àÜx)2‚àá2f(x, y) + O

(‚àÜx)4
,
(x, y) ‚àà‚Ñ¶.
Since M‚àí1
‚àÜx Àúf = f + O

(‚àÜx)4
, the new equation (8.31) diÔ¨Äers from (8.13) only in its

8.3
Higher-order methods for ‚àá2u = f
161
0
0.5
1.0
0
0.2
0.4
0.6
0.8
1.0
0
5
10
15
20
25
0
0.5
1.0
0
0.5
1.0
0
0.02
0.04
0.06
0
0.5
1.0
0
0.5
1.0
0
0.1
0.2
0
0.5
1.0
0
0.5
1.0
0
0.005
0.010
0.015
0
0.5
1.0
0
0.5
1.0
0
2
4
6
x 10
‚àí4
0
0.5
1.0
0
0.5
1.0
0
1
2
x 10
‚àí4
0
0.5
1.0
0
0.5
0
2
4
x 10
‚àí5
0
0.5
1.0
0
0.5
1.0
0
1
2
3
x 10
‚àí5
0
0.5
1.0
0
0.5
1.0
0
2
4
6
x 10
‚àí7
0
0.5
1.0
0
0.5
1.0
0
0.5
1.0
x 10
‚àí8
exact solution
Ô¨Åve-point
nine-point
modiÔ¨Åed nine-point
m = 5
m = 11
m = 23
Figure 8.8
The exact solution of the Poisson equation (8.33) and the errors with
the Ô¨Åve-point, nine-point and modiÔ¨Åed nine-point schemes.

162
Finite diÔ¨Äerence schemes
O

(‚àÜx)4
terms. In other words, the nine-point formula, when applied to ‚àá2u = Àúf,
yields an O

(‚àÜx)4
approximation to the original Poisson equation (8.13) with the
same boundary conditions.
Although it is sometimes easy to derive Àúf by symbolic diÔ¨Äerentiation, perhaps
computer-assisted, for simple functions f, it is easier to produce it by Ô¨Ånite diÔ¨Äerences.
Since
1
(‚àÜx)2 (‚àÜ2
0,x + ‚àÜ2
0,y) = ‚àá2 + O

(‚àÜx)2
,
it follows that
Àúf :=

I + 1
12(‚àÜ2
0,x + ‚àÜ2
0,y)

f
is of just the right form. Therefore, the scheme
1
(‚àÜx)2

‚àÜ2
0,x + ‚àÜ2
0,y + 1
6‚àÜ2
0,x‚àÜ2
0,y

uk,‚Ñì=

I + 1
12

‚àÜ2
0,x + ‚àÜ2
0,y

fk,‚Ñì,
(8.32)
which we can also write as


















‚àí10
3
2
3
2
3
2
3
2
3
1
6
1
6
1
6
1
6
uk,‚Ñì= (‚àÜx)2










1
12
1
12
1
12
1
12
2
3
fk,‚Ñì,
is O

(‚àÜx)4
as an approximation of the Poisson equation (8.13).
We call it the
modiÔ¨Åed nine-point scheme.
The extra cost incurred in the modiÔ¨Åcation of the nine-point formula is minimal,
since the function f is known and so the cost of forming Àúf is extremely low. The
rewards, however, are very rich indeed.
3 The modiÔ¨Åed nine-point scheme in action . . .
Figure 8.8 displays the
solution of the Poisson equation
‚àá2u = x2 + y2,
0 < x, y < 1,
(8.33)
u(x, 0) ‚â°0,
u(x, 1) = 1
2x2,
0 ‚â§y ‚â§1,
u(0, y) = sin œÄy,
u(1, y) = eœÄ sin œÄy + 1
2y2,
0 ‚â§x ‚â§1.
The second line in the Ô¨Ågure displays the solution of (8.33) using the Ô¨Åve-point
formula (8.15). The error is quite large, but the exact solution
u(x, y) = eœÄx sin œÄy + 1
2(xy)2,
0 ‚â§x, y ‚â§1,
can be as much as eœÄ + 1
2 ‚âà23.6407, and so even the numerical solution for
m = 5 comes within 1% of it. The most important observation, however, is
that the error is attenuated by roughly a factor of 4 each time ‚àÜx is halved.

Comments and bibliography
163
The outcome of the calculation with the nine-point formula (8.28) is displayed
in the third line and, without any doubt, it is much better ‚Äì about 200 times
smaller than the corresponding values for the Ô¨Åve-point scheme. There is a
good reason: the error expansion of the Ô¨Åve-point formula is
1
(‚àÜx)2 (‚àÜ2
0,x +‚àÜ2
0,y)‚àí‚àá2 =
1
12(‚àÜx)2(‚àá4 ‚àí2D2
xD2
y)+O

(‚àÜx)4
,
‚àÜx ‚Üí0,
(veriÔ¨Åcation is left to the reader in Exercise 8.9) while the error expansion of
the nine-point formula can be deduced at once from the above expression,
1
(‚àÜx)2

‚àÜ2
0,x + ‚àÜ2
0,y + 1
6‚àÜ2
0,x‚àÜ2
0,y

‚àí‚àá2 = 1
12(‚àÜx)2‚àá4+O

(‚àÜx)4
,
‚àÜx ‚Üí0.
As far as the Poisson equation (8.33) is concerned, we have
(‚àá4 ‚àíD2
xD2
y)u
=
2œÄ4eœÄx sin œÄy,
‚àá4u
‚â°
4,
0 ‚â§x, y ‚â§1.
Hence the principal error term for the Ô¨Åve-point formula can be as much as
1127 times larger than the corresponding term for the nine-point scheme in
(0, 1)2. This is not a general feature of the underlying numerical methods!
Perhaps less striking, but nonetheless an important observation, is that the
error associated with the nine-point scheme decays in Fig. 8.8 by roughly a
factor of 4 with each halving of ‚àÜx, consistently with the general theory and
similarly to the behaviour of the Ô¨Åve-point formula.
The bottom line in Fig. 8.8 displays the outcome of the calculation with the
modiÔ¨Åed nine-point formula (8.32). It is evident not just that the absolute
magnitude of the error is vastly smaller (we do better with 25 grid points than
the ‚Äòplain‚Äô nine-point formula with 529 grid points!) but also that its decay,
by roughly a factor 64 whenever ‚àÜx is halved, is consistent with the expected
error O

(‚àÜx)4
.
3
Comments and bibliography
The numerical solution of the Poisson equation by means of Ô¨Ånite diÔ¨Äerences features in many
texts, e.g. Ames (1977).
Diligent readers who wish Ô¨Årst to acquaint themselves with the
analytic theory of the Poisson equation (and more general elliptic equations) might consult
the classical text of Agmon (1965).
The best reference, however, is probably Hackbusch
(1992), since it combines the analytic and numerical aspects of the subject.
The modiÔ¨Åed nine-point formula (8.32) is not as well known as it ought to be, and part
of the blame lies perhaps in the original name, Mehrstellenverfahren (Collatz, 1966). We
prefer to avoid this sobriquet in the text, to deter overenthusiastic instructors from making
its spelling an issue in examinations.
Anticipating the discussion of Fourier transforms in Chapter 15, it is worth remarking
brieÔ¨Çy on the connection of the latter with Ô¨Ånite diÔ¨Äerence operators. Denoting the Fourier

164
Finite diÔ¨Äerence schemes
transform of a function f by ÀÜf,4 it is easy to prove that
;
Ef(Œæ) = eiŒæh ÀÜf(Œæ),
Œæ ‚ààC;
therefore
<
‚àÜ+f(Œæ) = (eiŒæh ‚àí1) ÀÜf(Œæ),
<
‚àÜ‚àíf(Œæ) = (1 ‚àíe‚àíiŒæh) ÀÜf(Œæ),
<
‚àÜ0f(Œæ) = (eiŒæh/2 ‚àíe‚àíiŒæh/2) ÀÜf(Œæ) = 2i

sin Œæh
2

ÀÜf(Œæ),
<
Œ•0f(Œæ) =
1
2(eiŒæh/2 + eiŒæh/2) ÀÜf(Œæ) =

cos Œæh
2

ÀÜf(Œæ).
Œæ ‚ààC.
Likewise,
;
Df(Œæ) = iŒæ ÀÜf(Œæ),
Œæ ‚ààC.
The calculus of Ô¨Ånite diÔ¨Äerences can be relocated to Fourier space. For example, the Fourier
transform of h‚àí2[f(x + h) ‚àí2f(x) + f(x ‚àíh)] is
1
h2 <
‚àÜ2
0f(Œæ) = eiŒæh ‚àí2 + e‚àíiŒæh
h2
ÀÜf(Œæ) = 
‚àíŒæ2 +
1
12h2Œæ4 ‚àí¬∑ ¬∑ ¬∑ ÀÜf(Œæ),
which we recognize as the transform of f ‚Ä≤‚Ä≤ ‚àí
1
12h2f (4) + ¬∑ ¬∑ ¬∑.
The subject matter of this chapter can be generalized in numerous directions.
Much
of it is straightforward intellectually, although it might require lengthy manipulation and
unwieldy algebra. An example is the generalization of methods for the Poisson equation to
more variables. An engineer recognizes three spatial variables, while the agreed number of
spatial dimensions in theoretical physics changes by the month (or so it seems), but numerical
analysis can cater for all these situations (see Exercises 8.10 and 8.11). Of course, the cost
of linear algebra is bound to increase with dimension, but such considerations are easier to
deal with in the framework of Chapters 11‚Äì15.
Finite diÔ¨Äerences extend to non-rectangular meshes. Although, for example, a honeycomb
mesh such as the following,
is considerably more diÔ¨Écult to use in practical programs, it occasionally confers advantages
in dealing with curved boundaries. An example of a Ô¨Ånite diÔ¨Äerence scheme in a honeycomb
is provided by









TT
4
3
4
3
4
3
‚àí4
uk,‚Ñì= (‚àÜx)2fk,‚Ñì.
4If you do not know the deÔ¨Ånition of the Fourier transform, skip what follows, possibly returning
to it later.

Comments and bibliography
165
A slightly more complicated grid features in Exercise 8.13. There are, however, limits to
the practicability of fancy grids ‚Äì Escher‚Äôs tessellations and Penrose tiles are deÔ¨Ånitely out
of the question!
A more wide-ranging approach to curved and complicated geometries is to use nonuniform
meshes. This allows a snug Ô¨Åt to diÔ¨Écult boundaries and also opens up the possibility of
‚Äòzooming in‚Äô on portions of the set ‚Ñ¶where, for some reason, the solution is more problematic
or error-prone, and employing a Ô¨Åner grid there. This is relatively easy in one dimension (see
Fornberg & Sloan (1994) for a very general approach) but, as far as Ô¨Ånite diÔ¨Äerences are
concerned, virtually hopeless in several spatial dimensions. Fortunately, the Ô¨Ånite element
method, which is reviewed in Chapter 9, provides a relatively accessible means of working
with multivariate nonuniform meshes.
Yet another possibility, increasingly popular because of its eÔ¨Éciency in parallel computing
architectures, is domain decomposition (Chan & Mathew, 1994; Le Tallec, 1994). The main
idea is to tear the set ‚Ñ¶into smaller subsets, where the Poisson (or another) equation can be
solved on a distinct grid (and possibly even on a diÔ¨Äerent computer), subsequently ‚Äògluing‚Äô
diÔ¨Äerent bits together by solving smaller problems along the interfaces.
The scope of this chapter has been restricted to Dirichlet boundary conditions, but Pois-
son equations can be computed just as well with Neumann or mixed conditions. Moreover,
Ô¨Ånite diÔ¨Äerences and tricks like the Mehrstellenverfahren can be applied to other linear elliptic
equations. Most notably, the computational stencil


























1
1
1
1
2
2
2
2
‚àí8
‚àí8
‚àí8
‚àí8
20
uk,‚Ñì=(‚àÜx)4fk,‚Ñì
(8.34)
represents an O
(‚àÜx)2
method for the biharmonic equation
‚àá4u = f,
(x, y) ‚àà‚Ñ¶
(see Exercise 8.12).
However, no partial diÔ¨Äerential equation competes with the Poisson equation in regard
to its importance and pervasiveness in applications. It is no wonder that there exist so many
computational algorithms for (8.13), and not just Ô¨Ånite diÔ¨Äerence, but also Ô¨Ånite element
(Chapter 9), spectral (Chapter 10), and boundary element methods. The fastest to date is
the multipole method, originally introduced by Carrier et al. (1988).
This chapter would not be complete without a mention of a probabilistic method for
solving the Ô¨Åve-point system (8.15) in a special case, the Laplace equation (i.e., f ‚â°0).
Although solution methods for linear algebraic systems belong in Chapters 11‚Äì15, we prefer
to describe this method here, so that nobody mistakes it for a viable algorithm. Assume for
example a computational grid in the shape of Manhattan Island, New York City, with streets
and avenues forming rows and columns, respectively. (We need to disregard the Broadway
and a few other thoroughfares that fail to conform with the grid structure, but it is the
principle that matters!) Suppose that a drunk emerges from a pub at the intersection of
Sixth Avenue and Twentieth Street. Being drunk, our friend turns at random in one of the

166
Finite diÔ¨Äerence schemes
four available directions and staggers ahead. Having reached the next intersection, the drunk
again turns at random, and so on and so forth. Disregarding the obvious perils of muggers,
New York City drivers, pollution etc., the person in question can terminate this random walk
(for this is what it is in mathematical terminology) only by reaching the harbour and falling
into the water ‚Äì an event that is bound to take place in Ô¨Ånite time with probability one.
Depending on the exact spot where our imbibing friend takes a dip, a Ô¨Åne is paid to the New
York Harbor Authority. In other words, we have a ‚ÄòÔ¨Åne function‚Äô œÜ(x, y), deÔ¨Åned for all (x, y)
along Manhattan‚Äôs waterfront, and the Harbor Authority is paid US$ œÜ(x0, y0) if the drunk
falls into the water at the point (x0, y0).
Suppose next that n drunks emerge from the same pub and each performs an independent
meander through the city grid. Let Àúun be the average Ô¨Åne paid by the n drunks. It is then
possible to prove that
Àúu := lim
n‚Üí‚àûÀúun
is the solution of the Ô¨Åve-point formula for the Laplace equation (with the Dirichlet boundary
condition œÜ) at the grid point (Twentieth Street, Sixth Avenue).
Before trying this algorithm (hopefully, playing the part of the Harbor Authority), the
reader had better be informed that the speed of convergence of Àúun to Àúu is O
1/‚àön
. In other
words, to obtain four signiÔ¨Åcant digits we need 108 drunks.
This is an example of a Monte Carlo method (Kalos & Whitlock, 1986) and we hasten
to add that such algorithms can be very valuable in other areas of scientiÔ¨Åc computing. In
particular, if you are interested in solving the Laplace equation in several hundred space
dimensions, a popular pastime in Ô¨Ånancial mathematics, just about the only viable approach
is Monte Carlo.
Agmon, S. (1965), Lectures on Elliptic Boundary Value Problems, Van Nostrand, Princeton,
NJ.
Ames, W.F. (1977), Numerical Methods for Partial DiÔ¨Äerential Equations (2nd ed.), Aca-
demic Press, New York.
Carrier, J., Greengard, L. and Rokhlin, V. (1988), A fast adaptive multipole algorithm for
particle simulations, SIAM Journal of ScientiÔ¨Åc and Statistical Computing 9, 669‚Äì686.
Chan, T.F. and Mathew, T.P. (1994), Domain decomposition algorithms, Acta Numerica 3,
61‚Äì144.
Collatz, L. (1966), The Numerical Treatment of DiÔ¨Äerential Equations (3rd edn), Springer-
Verlag, Berlin.
Fornberg, B. and Sloan, D.M. (1994), A review of pseudospectral methods for solving partial
diÔ¨Äerential equations, Acta Numerica 3, 203‚Äì268.
Hackbusch, W. (1992), Elliptic DiÔ¨Äerential Equations: Theory and Numerical Treatment,
Springer-Verlag, Berlin.
Kalos, M.H. and Whitlock, P.A. (1986), Monte Carlo Methods, Wiley, New York.
Le Tallec, P. (1994), Domain decomposition methods in computational mechanics, Compu-
tational Mechanics Advances 1, 121‚Äì220.
Exercises
8.1
Prove the identities
‚àÜ‚àí+ ‚àÜ+ = 2Œ•0‚àÜ0,
‚àÜ‚àí‚àÜ+ = ‚àÜ2
0.

Exercises
167
8.2
Show that formally
E =
‚àû

j=0
‚àÜj
‚àí.
8.3
Demonstrate that for every s ‚â•1 there exists a constant cs Ã∏= 0 such that
ds
dxs z(x) ‚àí1
hs

‚àÜs
+ ‚àí1
2s‚àÜs+1
+

z(x) = cs
ds+2
dxs+2 z(x)h2 + O

h3
,
h ‚Üí0,
for every suÔ¨Éciently smooth function z. Evaluate cs explicitly for s = 1, 2.
8.4
For every s ‚â•1 Ô¨Ånd a constant ds Ã∏= 0 such that
d2s
dx2s z(x) ‚àí1
h2s ‚àÜ2s
0 z(x) = ds
d2s+2
dx2s+2 z(x)h2 + O

h4
,
h ‚Üí0,
for every suÔ¨Éciently smooth function z. Compare the sizes of d1 and c2;
what does this tell you about the errors in the forward diÔ¨Äerence and central
diÔ¨Äerence approximations?
8.5
In this exercise we consider Ô¨Ånite diÔ¨Äerence approximations to the derivative
that use one point to the left and s ‚â•1 points to the right of x.
a Determine constants Œ±j, j = 1, 2, . . . , such that
D = 1
h

Œ≤‚àÜ‚àí+
‚àû

j=1
Œ±j‚àÜj
+

,
where Œ≤ ‚ààR is given.
b Given an integer s ‚â•1, show how to choose the parameter Œ≤ so that
D = 1
h

Œ≤‚àÜ‚àí+
s

j=1
aj‚àÜj
+

+ O

hs+1
,
h ‚Üí0.
8.6
Determine the order (in the form O((‚àÜx)p)) of the Ô¨Ånite diÔ¨Äerence approx-
imation to ‚àÇ2/‚àÇx‚àÇy given by the computational stencil
1
(‚àÜx)2


















1
4
1
4
‚àí1
4
‚àí1
4
0
0
0
0
0
8.7
The Ô¨Åve-point formula (8.15) is applied in an L-shaped domain of the form

168
Finite diÔ¨Äerence schemes
and we assume that all grid points are either internal or boundary (this
is possible if the ratios of the sides are rational).
Prove without relying
on Theorem 8.4 or on its method of proof that the underlying matrix is
nonsingular.
(Hint: Nothing prevents you from relying on the method of
proof of Lemma 8.1.)
8.8
In this exercise we prove, step by step, the GerÀásgorin criterion, which was
stated in Lemma 8.3.
a Let C = (ci,j) be an arbitrary d √ó d singular complex matrix. Then there
exists x ‚ààCd \ {0} such that Cx = 0. Choose ‚Ñì‚àà{1, 2, . . . , d} such that
|x‚Ñì| =
max
j=1,2,...,d |xj| > 0.
By considering the ‚Ñìth row of Cx, prove that
|c‚Ñì,‚Ñì| ‚â§
d

j=1, jÃ∏=‚Ñì
|c‚Ñì,j|.
(8.35)
b Let B be a d√ód matrix and choose Œª ‚ààœÉ(B), where œÉ(B) is the set containing
the eigenvalues of B. Substituting C = B ‚àíŒªI in (8.35) prove that Œª ‚ààS‚Ñì
(the GerÀásgorin discs Si were deÔ¨Åned in Lemma 8.3). Hence deduce that
œÉ(B) ‚äÇ
d7
i=1
Si.
c Suppose that the matrix B is irreducible (A.1.2.5) and that the inequality
(8.35) holds as an equality for some ‚Ñì‚àà{1, . . . , d}; show that this equality
implies that
|ck,k| =
d

j=1, jÃ∏=k
|ck,j|,
k = 1, 2, . . . , d.
Deduce that if Œª ‚ààœÉ(B) lies on ‚àÇS‚Ñìfor one ‚Ñìthen Œª ‚àà‚àÇSk for all k =
1, 2, . . . , d.
8.9
Prove that
1
(‚àÜx)2 (‚àÜ2
0,x+‚àÜ2
0,y)‚àí‚àá2 =
1
12(‚àÜx)2(‚àá4‚àí2D2
xD2
y)+O

(‚àÜx)4
,
‚àÜx ‚Üí0.

Exercises
169
8.10
Consider the d-dimensional Laplace operator
‚àá2 =
d

j=1
‚àÇ2
‚àÇx2
j
.
Prove a d-dimensional generalization of (8.15),
1
(‚àÜx)2
d

j=1
‚àÜ2
0,xj = ‚àá2 + O

(‚àÜx)2
.
8.11‚ãÜ
Let ‚àá2 again denote the d-dimensional Laplace operator and set
L‚àÜx = ‚àí2
3I + 2
3
d

j=1
‚àÜ2
0,xj + 2
3
d

j=1

I + 1
2‚àÜ2
0,xj

,
M‚àÜx = I + 1
12
d

j=1
‚àÜ2
0,xj,
where I is the identity operator.
a Prove that
1
(‚àÜx)2 L‚àÜx = ‚àá2 + 1
12(‚àÜx)2‚àá4 + O

(‚àÜx)4
,
‚àÜx ‚Üí0
and
M‚àÜx = I + 1
12(‚àÜx)2‚àá2 + O

(‚àÜx)4
,
‚àÜx ‚Üí0.
b Deduce that the method
L‚àÜxuk1,k2,...,kd = (‚àÜx)2M‚àÜxfk1,k2,...,kd
solves the d-dimensional Poisson equation ‚àá2u = f with an order-O

(‚àÜx)4
error.
(This is the multivariate generalization of the modiÔ¨Åed nine-point
formula (8.32).)
8.12
Prove that the computational stencil (8.34) for the solution of the biharmonic
equation ‚àá4u = f is equivalent to the Ô¨Ånite diÔ¨Äerence representation
(‚àÜ2
0,x + ‚àÜ2
0,y)2uk,‚Ñì= (‚àÜx)4fk,‚Ñì,
and thereby deduce the order of the error.

170
Finite diÔ¨Äerence schemes
8.13‚ãÜ
Find the equivalent of the Ô¨Åve-point formula in a computational grid of
equilateral triangles,
Your scheme should couple each internal grid point with its six nearest neigh-
bours. What is the order of the error?

9
The Ô¨Ånite element method
9.1
Two-point boundary value problems
The Ô¨Ånite element method (FEM) presents to all those who were weaned on Ô¨Ånite
diÔ¨Äerences an entirely new outlook on the computation of a numerical solution for
diÔ¨Äerential equations. Although it is often encapsulated in a few buzzwords ‚Äì ‚Äòweak
solution‚Äô, ‚ÄòGalerkin‚Äô, ‚ÄòÔ¨Ånite element functions‚Äô ‚Äì an understanding of the FEM calls not
just for a diÔ¨Äerent frame of mind but also for the comprehension of several principles.
Each principle is important but it is their combination that makes the FEM into such
an eÔ¨Äective computational tool.
Instead of commencing our exposition from the deep end, let us Ô¨Årst examine in
detail a simple example, the Poisson equation in just one space variable. In principle,
such an equation is u‚Ä≤‚Ä≤ = f, but this is clearly too trivial for our purposes since it
can be readily solved by integration. Instead, we adopt a more ambitious goal and
examine linear two-point boundary value problems
‚àíd
dx

a(x) du
dx

+ b(x)u = f,
0 ‚â§x ‚â§1,
(9.1)
where a, b and f are given functions, a is diÔ¨Äerentiable and a(x) > 0, b(x) ‚â•0,
0 < x < 1. Any equation of the form (9.1) must be speciÔ¨Åed in tandem with proper
initial or boundary data. For the time being, we assume Dirichlet boundary conditions
u(0) = Œ±,
u(1) = Œ≤.
(9.2)
Two-point boundary problems (9.1) abound in applications, e.g. in mechanics,
and their numerical solution is of independent interest. However, in the context of
this section, our main motivation is to use them as a paradigm for a general linear
boundary value problem and as a vehicle for the description of the FEM.
Throughout this chapter we extensively employ the terminology of linear spaces,
inner products and norms. The reader might wish to consult appendix section A.2.1
for the relevant concepts and deÔ¨Ånitions.
Instead of estimating the solution of (9.1) on a grid, a practice that has underlain
the discourse of previous chapters, we wish to approximate u by a linear combination
of functions in a Ô¨Ånite-dimensional space. We choose a function œï0 that obeys the
boundary conditions (9.2) and a set of m linearly independent functions œï1, œï2, . . . , œïm
that satisfy the zero boundary conditions œï‚Ñì(0) = œï‚Ñì(1) = 0, ‚Ñì= 1, 2, . . . , m.
In
addition, these functions need to satisfy certain smoothness conditions, but it is best
171

172
The Ô¨Ånite element method
to leave this question in abeyance for a while. Our goal is to represent u approximately
in the form
um(x) = œï0(x) +
m

‚Ñì=1
Œ≥‚Ñìœï‚Ñì(x),
0 ‚â§x ‚â§1,
(9.3)
where Œ≥1, Œ≥2, . . . , Œ≥m are real constants. In other words, let
‚ó¶
Hm:= Sp {œï1, œï2, . . . , œïm},
the span of œï1, œï2, . . . , œïm, be the set of all linear combinations of these functions.
Since the œï‚Ñì, ‚Ñì= 1, 2, . . . , m, are linearly independent,
‚ó¶
Hm is an m-dimensional linear
space. An alternative phrasing of (9.3) is that we seek
um ‚àíœï0 ‚àà
‚ó¶
Hm
such that um approximates, in some sense, the solution of (9.1). Note that every
member of
‚ó¶
Hm obeys zero boundary conditions.
Hence, by design, um is bound
to satisfy the boundary conditions (9.2). For future reference, we record this Ô¨Årst
principle of the FEM.
‚Ä¢ Approximate the solution in a Ô¨Ånite-dimensional space.
What might we mean, however, by the phrase ‚Äòapproximates the solution of (9.1)‚Äô?
One possibility, that we have already seen in Chapter 3, is collocation: we choose
Œ≥1, Œ≥2, . . . , Œ≥m so as to satisfy the diÔ¨Äerential equation (9.1) at m distinct points in
[0, 1]. Here, though, we shall apply a diÔ¨Äerent and more general line of reasoning. For
any choice of Œ≥1, Œ≥2, . . . , Œ≥m consider the defect
dm(x) := ‚àíd
dx

a(x) dum(x)
dx

+ b(x)um(x) ‚àíf(x),
0 < x < 1.
Were um the solution of (9.1), the defect would be identically zero. Hence, the nearer
dm is to the zero function, the better we can expect our candidate solution to be.
In the fortunate case when dm itself lies in the space
‚ó¶
Hm for all Œ≥1, Œ≥2, . . . , Œ≥m, the
problem is simple. The zero function in an inner product space is identiÔ¨Åed by being
orthogonal to all members of that space. Hence, we equip
‚ó¶
Hm with an inner product
‚ü®¬∑ , ¬∑ ‚ü©and seek parameters Œ≥0, Œ≥1, . . . , Œ≥m such that
‚ü®dm, œïk‚ü©= 0,
k = 1, 2, . . . , m.
(9.4)
Since {œï1, œï2, . . . , œïm} is, by design, a basis of
‚ó¶
Hm, it follows from (9.4) that dm is
orthogonal to all members of
‚ó¶
Hm, hence that it is the zero function. The orthogonality
conditions (9.4) are called the Galerkin equations.
In general, however, dm Ã∏‚àà
‚ó¶
Hm and we cannot expect to solve (9.1) exactly from
within the Ô¨Ånite-dimensional space. Nonetheless, the principle of the last paragraph
still holds good: we wish dm to obey the orthogonality conditions (9.4). In other
words, the goal that underlies our discussion is to render the defect orthogonal to the

9.1
Two-point boundary value problems
173
Ô¨Ånite-dimensional space
‚ó¶
Hm.1 This, of course, represents valid reasoning only if
‚ó¶
Hm
approximates well the inÔ¨Ånite-dimensional linear space of all the candidate solutions
of (9.1); more about this later. The second principle of the FEM is thus as follows.
‚Ä¢ Choose the approximation so that the defect is orthogonal to the space
‚ó¶
Hm.
Using the representation (9.3) and the linearity of the diÔ¨Äerential operator, the
defect becomes
dm = ‚àí

(aœï‚Ä≤
0)‚Ä≤ +
m

‚Ñì=1
Œ≥‚Ñì(aœï‚Ä≤
‚Ñì)‚Ä≤

+ b

œï0 +
m

‚Ñì=1
Œ≥‚Ñìœï‚Ñì

‚àíf,
and substitution in (9.4) results, after an elementary rearrangement of terms, in
m

‚Ñì=1
Œ≥‚Ñì[‚ü®‚àí(aœï‚Ä≤
‚Ñì)‚Ä≤, œïk‚ü©+ ‚ü®bœï‚Ñì, œïk‚ü©] = ‚ü®f, œïk‚ü©‚àí[‚ü®‚àí(aœï‚Ä≤
0)‚Ä≤, œïk‚ü©+ ‚ü®bœï0, œïk‚ü©] ,
k = 1, 2, . . . , m.
(9.5)
On the face of it, (9.4) is a linear system of m equations for the m unknowns
Œ≥1, Œ≥2, . . . , Œ≥m. However, before we rush to solve it, we must Ô¨Årst perform a crucial
operation, integration by parts.
Let us suppose that ‚ü®¬∑ , ¬∑ ‚ü©is the standard Euclidean inner product over functions
(the L2 inner product; see A.2.1.4),
‚ü®v, w‚ü©=
 1
0
v(œÑ)w(œÑ) dœÑ.
It is deÔ¨Åned over all functions v and w such that
 1
0
|v(œÑ)|2 dœÑ,
 1
0
|w(œÑ)|2 dœÑ < ‚àû.
(9.6)
Hence, (9.5) assumes the form
m

‚Ñì=1
Œ≥‚Ñì
	
‚àí
 1
0
{‚àí[a(œÑ)œï‚Ä≤
‚Ñì(œÑ)]‚Ä≤œïk(œÑ)} dœÑ +
 1
0
b(œÑ)œï‚Ñì(œÑ)œïk(œÑ) dœÑ

=
 1
0
f(œÑ)œïk(œÑ) dœÑ ‚àí
	
‚àí
 1
0
{‚àí[a(œÑ)œï‚Ä≤
0(œÑ)]‚Ä≤œïk(œÑ)} dœÑ +
 1
0
b(œÑ)œï0(œÑ)œïk(œÑ) dœÑ

,
k = 1, 2, . . . , m.
Since for k = 1, 2, . . . , m the function œïk vanishes at the endpoints, integration by
parts may be carried out with great ease:
 1
0
{‚àí[a(œÑ)œï‚Ä≤
‚Ñì(œÑ)]‚Ä≤} œïk(œÑ) dœÑ = ‚àía(œÑ)œï‚Ñì(œÑ)œïk(œÑ)
1
0
+
 1
0
a(œÑ)œï‚Ä≤
‚Ñì(œÑ)œï‚Ä≤
k(œÑ) dœÑ
=
 1
0
a(œÑ)œï‚Ä≤
‚Ñì(œÑ)œï‚Ä≤
k(œÑ) dœÑ,
‚Ñì= 0, 1, . . . , m.
1Collocation Ô¨Åts easily into this formulation, provided the inner product is properly deÔ¨Åned (see
Exercise 9.1).

174
The Ô¨Ånite element method
The outcome,
m

‚Ñì=1
ak,‚ÑìŒ≥‚Ñì=
 1
0
f(œÑ)œïk(œÑ) dœÑ ‚àíak,0,
k = 1, 2, . . . , m,
(9.7)
where
ak,‚Ñì:=
 1
0
[a(œÑ)œï‚Ä≤
‚Ñì(œÑ)œï‚Ä≤
k(œÑ) + b(œÑ)œï‚Ñì(œÑ)œïk(œÑ)] dœÑ,
k = 1, 2, . . . , m, ‚Ñì= 0, 1, . . . , m,
is a form of the Galerkin equations suitable for numerical work.
The reason why (9.7) is preferable to (9.5) lies in the choice of the functions œï‚Ñì,
which will be discussed soon.
The whole point is that good choices of the basis
functions (within the FEM framework) possess quite poor smoothness properties ‚Äì in
fact, the more we can lower the diÔ¨Äerentiability requirements, the wider the class of
desirable basis functions that we might consider. Integration by parts takes away one
derivative, hence we need no longer insist that the œï‚Ñìare twice-diÔ¨Äerentiable in order
to satisfy (9.6). Even once-diÔ¨Äerentiability is, in fact, too strong. Since the value of
an integral is independent of the values that the integrand assumes on a Ô¨Ånite set
of points, it is suÔ¨Écient to choose basis functions that are piecewise diÔ¨Äerentiable in
[0, 1]. We will soon see that it is this lowering of the smoothness requirements through
the agency of an integration by parts that confers important advantages on the Ô¨Ånite
element method. This is therefore our third principle.
‚Ä¢ Integrate by parts to depress to the maximal extent possible the diÔ¨Äerentiability
requirements of the space
‚ó¶
Hm.
The importance of lowered smoothness and integration by parts ranges well beyond
the FEM and numerical analysis.
3 Weak solutions
Let us pause and ponder for a while the meaning of the
term ‚Äòexact solution of a diÔ¨Äerential equation‚Äô, which we are using so freely
and with such apparent abandon on these pages. Thus, suppose that we have
an equation of the form Lu = f, where L is a diÔ¨Äerential operator ‚Äì ordinary
or partial, in one or several dimensions, linear or nonlinear ‚Äì, provided with
the right boundary and/or initial conditions. An obvious candidate for the
term ‚Äòexact solution‚Äô is a function u that obeys the equation and the ‚Äòside‚Äô
conditions ‚Äì the classical solution. However, there is an alternative. Suppose
that we are given an inÔ¨Ånite-dimensional linear space
‚ó¶
H that is rich enough
to include in its closure all functions of interest. Given a ‚Äòcandidate function‚Äô
v ‚àà
‚ó¶
H, we deÔ¨Åne the defect as d(v) := Lv ‚àíf and say that v is the weak
solution of the diÔ¨Äerential equation if ‚ü®d(v), w‚ü©= 0 for every w ‚àà
‚ó¶
H.
On the face of it, we have not changed anything much ‚Äì the naive point of
view is that if the defect is orthogonal to the whole space then it is the zero
function, hence v obeys the diÔ¨Äerential equation in the classical sense. This
is a fallacy, inherited from a Ô¨Ånite-dimensional intuition! The whole point is

9.1
Two-point boundary value problems
175
that, astutely integrating by parts, we are usually able to lower the diÔ¨Äeren-
tiability requirements of
‚ó¶
H to roughly half those in the original equation. In
other words, it is entirely possible for an equation to possess a weak solution
that is neither a classical solution nor, indeed, can even be subjected to the
action of the diÔ¨Äerential operator L in a naive way.
The distinction between classical and weak solutions makes little sense in the
context of initial value problems for ODEs, since there the Lipschitz condition
ensures the existence of a unique classical solution (which, of course, is also a
weak solution). This is not the case with boundary value problems, and much
of modern PDE analysis hinges upon the concept of a weak solution.
3
Suppose that the coeÔ¨Écients ak,‚Ñìin (9.7) have been evaluated, whether explicitly
or by means of quadrature (see Section 3.1). The system (9.7) comprises m linear
equations in m unknowns and our next step is to employ a computer to solve it,
thereby recovering the coeÔ¨Écients Œ≥1, Œ≥2, . . . , Œ≥m that render the best (in the sense of
the underlying inner product) linear combination (9.3).
To introduce the FEM, we need another crucial ingredient, namely a speciÔ¨Åc choice
of the set
‚ó¶
Hm. There are, in principle, two objectives that we might seek in this choice.
On the one hand, we might wish to choose the œï1, œï2, . . . , œïm that, in some sense,
are the most ‚Äòdense‚Äô in the inÔ¨Ånite-dimensional space inhabited by the exact (weak)
solution of (9.1). In other words, we might wish
‚à•um ‚àíu‚à•= ‚ü®um ‚àíu, um ‚àíu‚ü©1/2
to be the smallest possible in (0, 1). This is a perfectly sensible choice, which results
in the spectral methods that will be considered in the next chapter. Here we adopt
a diÔ¨Äerent goal. The dimension of the Ô¨Ånite-dimensional space from which we are
seeking the solution will be often very large, perhaps not in the particular case of
(9.7), when m ‚âà100 is at the upper end of what is reasonable, but deÔ¨Ånitely so in
a multivariate case.
To implement (9.7) (or its multivariate brethren) we need to
calculate approximately m2 integrals and solve a linear system of m equations. This
might be a very expensive process and thus a second reasonable goal is to choose
œï1, œï2, . . . , œïm so as to make this task much easier.
The penultimate and crucial principle of the FEM is thus designed to save com-
putational cost.
‚Ä¢ Choose each function œïk so that it vanishes along most of (0, 1), thereby ensuring
that œïkœï‚Ñì‚â°0 for most choices of k, ‚Ñì= 1, 2, . . . , m.
In other words, each function œïk is supported on a relatively small set Ek ‚äÇ(0, 1),
say, and Ek ‚à©E‚Ñì= ‚àÖfor as many k, ‚Ñì= 1, 2, . . . , m as possible.
Recall that, by virtue of integration by parts, we have narrowed the diÔ¨Äerentiability
requirements so much that piecewise linear functions are perfectly acceptable in
‚ó¶
Hm.

176
The Ô¨Ånite element method
Setting h = 1/(m + 1), we choose
œïk(x) =
‚éß
‚é™
‚é™
‚é®
‚é™
‚é™
‚é©
1 ‚àík + x
h,
(k ‚àí1)h ‚â§x ‚â§kh,
1 + k ‚àíx
h,
kh ‚â§x ‚â§(k + 1)h,
0,
|x ‚àíkh| ‚â•h,
k = 1, 2, . . . , m.
In other words, œïk = œà(x/h ‚àík), k = 1, 2, . . . , m, where œà is the chapeau function
(also known as the hat function), represented as follows:



@
@
@
@
‚àí1
+1
‚é´
‚é¨
‚é≠1
It can also be written in the form
œà(x) = (x + 1)+ ‚àí2(x)+ + (x ‚àí1)+ = (1 ‚àí|x|)+,
x ‚ààR,
(9.8)
where
(t)+ :=
#
t,
t ‚â•0,
0,
t < 0,
t ‚ààR.
The advantages of this cryptic notation will become clear later. At present we draw
attention to the fact that Ek = ((k ‚àí1)h, (k + 1)h), and therefore
Ek ‚à©E‚Ñì=
‚éß
‚é™
‚é™
‚é™
‚é®
‚é™
‚é™
‚é™
‚é©
((k ‚àí1)h, (k + 1)h),
k = ‚Ñì,
((k ‚àí1)h, kh),
k = ‚Ñì+ 1,
(kh, (k + 1)h),
k = ‚Ñì‚àí1,
‚àÖ,
|k ‚àí‚Ñì| ‚â•2,
k, ‚Ñì= 1, 2, . . . , m.
Therefore the matrix in (9.7) becomes tridiagonal. This means Ô¨Årstly that we need to
evaluate just O(m), rather than O

m2
, integrals and secondly that the solution of
triangular linear systems is very easy indeed (see Section 11.1).
3 Spectral methods vs. the FEM
We attempt to solve the equation
‚àíd
dx

(1 + x) du
dx

+
1
1 + xu =
2
1 + x,
(9.9)
accompanied by the Dirichlet boundary conditions
u(0) = 0,
u(1) = 1,
using two distinct choices of the functions œï1, œï2, . . . , œïm. Firstly, we let
œïk(x) = sin kœÄx,
0 ‚â§x ‚â§1,
k = 1, 2, . . . , m
(9.10)

9.1
Two-point boundary value problems
177
and force the boundary conditions by setting
œï0(x) = sin œÄx
2 ,
0 ‚â§x ‚â§1.
This is an example of a spectral method, although we hasten to confess that it
is blatantly biased, since the boundary conditions are not of the right sort for
spectral methods: more even-handed treatment of this construct must await
Chapter 10.
The sprinter in FEM colours is the piecewise linear approximation that we
have just described, i.e.,
œïk(x) = œà((m + 1)x ‚àík),
0 ‚â§x ‚â§1
k = 1, 2, . . . , m.
(9.11)
Boundary conditions are recovered by choosing
œï0(x) = œà((m + 1)(x ‚àí1)),
0 ‚â§x ‚â§1;
note that œï0(0) = 0, œï0(1) = 1, as required. Of course, the support of œï0
extends beyond (0, 1) but this is of no consequence, since its integration is
restricted to the interval (m/(m + 1), 1). The errors for (9.10) and (9.11) are
displayed in Figs. 9.1 and 9.2, respectively. At a Ô¨Årst glance, both methods are
performing well but the FEM has a slight edge, because of the large wiggles
in Fig. 9.1 near the endpoints. This Gibbs eÔ¨Äect is the penalty that we endure
for attempting to approximate the non-periodic solution
u(x) =
2x
1 + x,
0 ‚â§x ‚â§1,
with trigonometric functions. If we disregard the vicinity of the endpoints,
the spectral method performs marginally better.
The error decays roughly quadratically in both Ô¨Ågures, the latter consistently
with the estimate ‚à•um ‚àíu‚à•= O

m‚àí2
(which, as we will see in Section 9.2,
happens to be the correct order of magnitude). Had we played to the strengths
of spectral methods by taking periodic boundary conditions, the error would
have decayed at an exponential speed and the FEM would have been left out
of the running altogether. Leaping to the defence of the FEM, we remark
that it took more than a hundredfold in terms of computer time to produce
Fig. 9.1 in comparison with Fig. 9.2.
3
All the essential ingredients that together make the FEM are in now place, except
for one. To clarify it, we describe an alternative methodology leading to the Galerkin
equations (9.7).

178
The Ô¨Ånite element method
0
0.5
1.0
‚àí4
‚àí2
0
2 x 10
‚àí3
 
0
0.5
1.0
‚àí10
‚àí5
0
5 x 10
‚àí4
 
0
0.5
1.0
‚àí2
0
2 x 10
‚àí4
 
0
0.5
1.0
‚àí10
‚àí5
0
5 x 10
‚àí5
 
m = 5
m = 10
m = 20
m = 40
Figure 9.1
The error in (9.3) when equation (9.9) is solved using the spectral
method (9.10).
0
0.5
1.0
‚àí6
‚àí4
‚àí2
0
2 x 10
‚àí4
 
0
0.5
1.0
‚àí1.5
‚àí1.0
‚àí0.5
0 x 10
‚àí4
 
0
0.5
1.0
‚àí4
‚àí3
‚àí2
‚àí1
0
1 x 10
‚àí5
 
0
0.5
1.0
‚àí10
‚àí5
0
5 x 10
‚àí6
 
m = 5
m = 10
m = 20
m = 40
Figure
9.2
The
error
in
(9.3)
when
the
equation
(9.9)
is
solved
using
the FEM (9.11).

9.1
Two-point boundary value problems
179
Many diÔ¨Äerential equations of practical interest start their life as variational prob-
lems and only subsequently are converted to a more familiar form by use of the
Euler‚ÄìLagrange equations. This gives little surprise to physicists, since the primary
truth about physical models is not that derivatives (velocity, momentum, acceleration
etc.) are somehow linked to the state of a system but that they arrange themselves
according to the familiar principles of least action and least expenditure of energy.
It is only mathematical ingenuity that renders this in the terminology of diÔ¨Äerential
equations!
In a general variational problem we are given a functional J : H ‚ÜíR, where H is
some function space, and we wish to Ô¨Ånd a function u ‚ààH such that
J (u) = min
v‚ààH J (v).
Let us consider the following variational problem. Three functions, a, b and f are
given in the interval (0, 1) in which we stipulate a(x) > 0 and b(x) ‚â•0. The space H
consists of all functions v that obey the boundary conditions (9.2) and
 1
0
v2(œÑ) dœÑ,
 1
0
[v‚Ä≤(œÑ)]2 dœÑ < ‚àû,
and we let
J (v) :=
 1
0

a(œÑ)[v‚Ä≤(œÑ)]2 + b(œÑ)[v(œÑ)]2 ‚àí2f(œÑ)v(œÑ)

dœÑ,
v ‚ààH.
(9.12)
It is possible to prove that infv‚ààH J (v) > ‚àí‚àû(see Exercise 9.7). Moreover, since the
space H is complete,2 every inÔ¨Åmum is attainable within it and the operations inf and
min become equal. Hence our variational problem always possesses a solution.
The space H is not a linear function space since (unless Œ± = Œ≤ = 0) it is not closed
under addition or multiplication by a scalar. However, choose an arbitrary u ‚ààH and
let
‚ó¶
H = {v ‚àíu : v ‚àà
‚ó¶
H}. Therefore, all functions in
‚ó¶
H obey zero boundary conditions.
Unlike H, the set
‚ó¶
H is a linear space and it is trivial to prove that each function
v ‚ààH can be written in a unique way as v = u + w, where w ‚àà
‚ó¶
H. We denote this by
H = u +
‚ó¶
H and say that H is an aÔ¨Éne space.
Let us suppose that u ‚ààH minimizes J . In other words, and bearing in mind that
H = u +
‚ó¶
H,
J (u) ‚â§J (u + v),
v ‚àà
‚ó¶
H .
(9.13)
We choose v ‚àà
‚ó¶
H \{0} and a real number Œµ Ã∏= 0. Then
J (u + Œµv) =
 1
0

a(u‚Ä≤ + Œµv‚Ä≤)2 + b(u + Œµv) ‚àí2f(u + Œµv)

dœÑ
=
 1
0

a

(u‚Ä≤)2 + 2Œµu‚Ä≤v‚Ä≤ + Œµ2(v‚Ä≤)2
+ b

u2 + 2Œµuv + Œµ2v2
‚àí2f(u + Œµv)

dœÑ
2Unless you know functional analysis, do not try to prove this ‚Äì accept it as an act of faith. . . This
is perhaps the place to mention that H is rich enough to contain all piecewise diÔ¨Äerentiable functions
in (0, 1) but, in order to be complete, it must contain many other functions as well.

180
The Ô¨Ånite element method
=
 1
0

a(u‚Ä≤)2 + bu2 ‚àí2fu

dœÑ + 2Œµ
 1
0
(au‚Ä≤v‚Ä≤ + buv ‚àífv) dœÑ
+ Œµ2
 1
0

a(v‚Ä≤)2 + bv2
dœÑ
= J (u) + 2Œµ
 1
0
(au‚Ä≤v‚Ä≤ + buv ‚àífv) dœÑ + Œµ2
 1
0

a(v‚Ä≤)2 + bv2
dœÑ.
(9.14)
To be consistent with (9.13), we require, replacing v by Œµv,
2Œµ
 1
0
(au‚Ä≤v‚Ä≤ + buv) dœÑ + Œµ2
 1
0

a(v‚Ä≤)2 + bv2
dœÑ ‚â•0.
As |Œµ| > 0 can be made arbitrarily small, we can make the second term negligible,
thereby deducing that
Œµ
 1
0
(au‚Ä≤v‚Ä≤ + buv ‚àífv) dœÑ ‚â•0.
Recall that no assumptions have been made with regard to Œµ, except that it is nonzero
and that its magnitude is adequately small. In particular, the inequality is valid when
we replace Œµ by ‚àíŒµ, and we therefore deduce that
 1
0
[a(œÑ)u‚Ä≤(œÑ)v‚Ä≤(œÑ) + b(œÑ)u(œÑ)v(œÑ) ‚àíf(œÑ)v(œÑ)] dœÑ = 0.
(9.15)
We have just proved that (9.15) is necessary for u to be the solution of the varia-
tional problem, and it is easy to demonstrate that it is also suÔ¨Écient. Thus, assuming
that the identity is true, (9.14) (with Œµ = 1) gives
J (u + v) = J (u) +
 1
0

a(v‚Ä≤)2 + bv2
dœÑ ‚â•J (u),
v ‚àà
‚ó¶
H .
Since H = u +
‚ó¶
H, it follows that u indeed minimizes J in H.
Identity (9.15) possesses a further remarkable property: it is the weak form (in
the Euclidean norm) of the two-point boundary value problem (9.1). This is easy to
ascertain using integration by parts in the Ô¨Årst term. Since v ‚àà
‚ó¶
H, it vanishes at the
endpoints and (9.15) becomes
 1
0

‚àí[a(œÑ)u‚Ä≤(œÑ)]‚Ä≤ + b(œÑ)u(œÑ) ‚àíf(œÑ)

v(œÑ) dœÑ = 0,
v ‚àà
‚ó¶
H .
In other words, the function u is a solution of the variational problem (9.12) if and
only if it is the weak solution of the diÔ¨Äerential equation (9.1).3 We thus say that the
two-point boundary value problem (9.1) is the Euler‚ÄìLagrange equation of (9.12).
Traditionally, variational problems have been converted into their Euler‚ÄìLagrange
counterparts but, so far as obtaining a numerical solution is concerned, we may at-
tempt to approximate (9.12) rather than (9.1). The outcome is the Ritz method.
3This proves, incidentally, that the solution of (9.12) is unique, but you may try to prove uniqueness
directly from (9.14).

9.1
Two-point boundary value problems
181
Let œï1, œï2, . . . , œïm be linearly independent functions in
‚ó¶
H and choose an arbitrary
œï0 ‚ààH.
As before,
‚ó¶
Hm is the m-dimensional linear space spanned by œïk, k =
1, 2, . . . , m. We seek a minimum of J in the m-dimensional aÔ¨Éne space œï0 +
‚ó¶
Hm. In
other words, we seek a vector Œ≥ = [ Œ≥1
Œ≥2
¬∑ ¬∑ ¬∑
Œ≥m ]‚ä§‚ààRm that minimizes
Jm(Œ¥) := J

œï0 +
m

‚Ñì=1
Œ¥‚Ñìœï‚Ñì

,
Œ¥ ‚ààRm.
The functional Jm acts on just m variables and its minimization can be accom-
plished, by well-known rules of calculus, by letting the gradient equal zero. Since Jm
is quadratic in its variables,
Jm(Œ¥) =

1
0
‚é°
‚é£a

œï‚Ä≤
0 +
m

‚Ñì=1
Œ¥‚Ñìœï‚Ä≤
‚Ñì
2
+ b

œï0 +
m

‚Ñì=1
Œ¥‚Ñìœï‚Ñì
2
‚àí2f

œï0 +
m

‚Ñì=1
Œ¥‚Ñìœï‚Ñì
‚é§
‚é¶dœÑ,
the gradient is easy to calculate. Thus,
1
2
‚àÇJm(Œ¥)
‚àÇŒ¥k
=
m

‚Ñì=1
 1
0
(aœï‚Ä≤
‚Ñìœï‚Ä≤
k + bœï‚Ñìœï‚Ñì) dœÑ +
 1
0
(aœï‚Ä≤
0œï‚Ä≤
k + bœï0œïk) dœÑ ‚àí
 1
0
fœïk dœÑ.
Letting ‚àÇJm/‚àÇŒ¥k = 0 for k = 1, 2, . . . , m recovers exactly the form (9.7) of the Galerkin
equations.
A careful reader will observe that setting the gradient to zero is merely a neces-
sary condition for a minimum. For suÔ¨Éciency we require in addition that the Hessian
matrix

‚àÇ2Jm/‚àÇŒ¥k‚àÇŒ¥j
m
k,j=1 is nonnegative deÔ¨Ånite. This is easy to prove (see Exer-
cise 9.6).
What have we gained from the Ritz method? On the face of it, not much except for
some additional insight, since it results in the same linear equations as the Galerkin
method. This, however, ceases to be true for many other equations; in these cases
Ritz and Galerkin result in genuinely diÔ¨Äerent computational schemes. Moreover the
variational formulation provides us with an important clue about how to deal with
more complicated boundary conditions.
There is an important mismatch between the boundary conditions for variational
problems and those for diÔ¨Äerential equations. Each diÔ¨Äerential equation requires the
right amount of boundary data. For example, (9.1) requires two conditions, of the
form
Œ±0,iu(0) + Œ±1,iu‚Ä≤(0) + Œ≤0,iu(1) + Œ≤1,iu‚Ä≤(1) = Œ≥i,
i = 1, 2,
such that
rank
 Œ±0,1
Œ±1,1
Œ≤0,1
Œ≤1,1
Œ±0,2
Œ±1,2
Œ≤0,2
Œ≤1,2

= 2.
Observe that (9.2) is a simple special case. However, a variational problem happily
survives with less than a full complement of boundary data. For example, (9.12) can be
deÔ¨Åned with just a single boundary value, u(0) = Œ±, say. The rule is to replace in the
Euler‚ÄìLagrange equations each ‚Äòmissing‚Äô boundary condition by a natural boundary

182
The Ô¨Ånite element method
condition.
For example, we complement u(0) = Œ± with u‚Ä≤(1) = 0.
(The proof is
virtually identical to the reasoning that led us from (9.12) to the corresponding Euler‚Äì
Lagrange equation (9.1), except that we need to use the natural boundary condition
when integrating by parts.)
In the Ritz method we traverse the avenue connecting variational problems and
diÔ¨Äerential equations in the opposite direction, from (9.1) to (9.15), say. This means
that, whenever the two-point boundary value problem is provided with a natural
boundary condition, we disregard it in the formation of the space H. In other words,
the function œï0 need obey only the essential boundary conditions that survive in the
variational problem. The quid pro quo for the disappearance of, say, u‚Ä≤(1) = 0 is that
we need to add œïm+1 (deÔ¨Åned consistently with (9.11)) to our space and an extra
equation, for k = m + 1, to the linear system (9.7); otherwise, by default, we are
imposing u(1) = 0, which is wrong.
3 A natural boundary condition
Consider the equation
‚àíu‚Ä≤‚Ä≤ + u = 2e‚àíx,
0 ‚â§x ‚â§1,
(9.16)
given in tandem with the boundary conditions
u(0) = 0,
u‚Ä≤(1) = 0.
The exact solution is easy to Ô¨Ånd: u(x) = xe‚àíx, 0 ‚â§x ‚â§1.
Fig. 9.3 displays the error in the numerical solution of (9.16) using the piece-
wise linear chapeau functions (9.8). Note that there is no need to provide the
‚Äòboundary function‚Äô œï0 at all. It is evident from the Ô¨Ågure that the algorithm,
as expected, is clever enough to recover the correct natural boundary condi-
tion at x = 1. Another observation, which the Ô¨Ågure shares with Fig. 9.2, is
that the decay of the error is consistent with O

m‚àí2
.
Why not, one may ask, impose the natural boundary condition at x = 1? The
obvious reason is that we cannot employ a chapeau function for that purpose,
since its derivative will be discontinuous at the endpoint. Of course, we might
instead use a more complicated function but, unsurprisingly, such functions
complicate matters needlessly.
3
A natural boundary condition is just one of several kinds of boundary data that
undergo change when diÔ¨Äerential equations are solved with the FEM. We do not wish
to delve further into this issue, which is more than adequately covered in specialized
texts.
However, and to remind the reader of the need for proper respect towards
boundary data, we hereby formulate our last principle of the FEM.
‚Ä¢ Retain only essential boundary conditions.
Throughout this section we have identiÔ¨Åed several principles that combine to
give the Ô¨Ånite element method.
Let us repeat them with some reformulation and

9.1
Two-point boundary value problems
183
0
0.5
1.0
‚àí2
0 x 10
‚àí4
 
0
0.5
1.0
‚àí1.0
‚àí0.5
0 x 10
‚àí3
 
0
0.5
1.0
‚àí6
‚àí4
‚àí2
0 x 10
‚àí5
 
0
0.5
1.0
‚àí4
‚àí2
0 x 10
‚àí6
 
0
0.5
1.0
‚àí1.5
‚àí1.0
‚àí0.5
0 x 10
‚àí5
 
0
0.5
1.0
‚àí1.0
‚àí0.5
0 x 10
‚àí6
 
m = 5
m = 10
m = 20
m = 40
m = 80
m = 160
Figure 9.3
The error in the solution of the equation (9.16) with boundary data
u(0) = 0, u‚Ä≤(1) = 0, by the Ritz‚ÄìGalerkin method with chapeau functions.
also some reordering.
‚Ä¢ Approximate the solution in a Ô¨Ånite-dimensional space œï0 +
‚ó¶
Hm‚äÇH.
‚Ä¢ Retain only essential boundary conditions.
‚Ä¢ Choose the approximant so that the defect is orthogonal to
‚ó¶
Hm or, alternatively,
so that a variational problem is minimized in
‚ó¶
Hm.
‚Ä¢ Integrate by parts to depress to the maximal extent possible the diÔ¨Äerentiability
requirements of the space
‚ó¶
Hm.
‚Ä¢ Choose each function in a basis of
‚ó¶
Hm in such a way that it vanishes along much
of the spatial domain of interest, thereby ensuring that the intersection between
the supports of most of the basis functions is empty.
Needless to say, there is much more to the FEM than these Ô¨Åve principles. In par-
ticular, we wish to specify
‚ó¶
Hm so that for suÔ¨Éciently large m the numerical solution
converges to the exact (weak) solution of the underlying equation ‚Äì and, preferably,
converges fairly fast. This is a subject that has attracted enough research to Ô¨Åll many

184
The Ô¨Ånite element method
a library shelf. The next section presents a brief review of the FEM in a more general
setting, with an emphasis on the choice of
‚ó¶
Hm that ensures convergence to the exact
solution.
9.2
A synopsis of FEM theory
In this section we present an outline of Ô¨Ånite element theory. We mostly dispense
with proofs. The reason is that an honest exposition of the FEM needs to be based
on the theory of Sobolev spaces and relatively advanced functional-analytic concepts.
Several excellent texts on the FEM are listed at the end of this chapter and we refer
the more daring and inquisitive reader to these.
The object of our attention is the boundary value problem
Lu = f,
x ‚àà‚Ñ¶,
(9.17)
where u = u(x), the function f = f(x) is bounded and ‚Ñ¶‚äÇRd is an open, bounded,
connected set with suÔ¨Éciently smooth boundary; L is a linear diÔ¨Äerential operator,
L =
2ŒΩ

k=0

i1+i2+¬∑¬∑¬∑+id=k
i1,i2,...,id‚â•0
ci1,i2,...,id(x)
‚àÇk
‚àÇxi1
1 ‚àÇxi2
2 ¬∑ ¬∑ ¬∑ ‚àÇxid
d
.
The equation (9.17) is accompanied by ŒΩ boundary conditions along ‚àÇ‚Ñ¶‚Äì some might
be essential, others natural, but we will not delve further into this issue.
Let H be the aÔ¨Éne space of all functions which act in ‚Ñ¶, whose ŒΩth derivative is
square-integrable4 and which obey all essential boundary conditions along ‚àÇ‚Ñ¶. We
let
‚ó¶
H = H ‚àíu, where u ‚ààH is arbitrary, and note that
‚ó¶
H is a linear space of functions
that satisfy zero boundary conditions.
We equip ourselves with the Euclidean inner product
‚ü®v, w‚ü©=

‚Ñ¶
v(œÑ)w(œÑ) dœÑ,
v, w ‚ààH,
and the inherited Euclidean norm
‚à•v‚à•= {‚ü®v, v‚ü©}1/2 ,
v ‚ààH.
Note that we have designed
‚ó¶
H so that terms of the form ‚ü®Lv, w‚ü©make sense for every
v, w ‚ààH, but this is true only subject to integration by parts ŒΩ times, to depress the
degree of derivatives inside the integral from 2ŒΩ down to ŒΩ. If d ‚â•2 we need to use
various multivariate counterparts of integration by parts, of which perhaps the most
useful are the divergence theorem

‚Ñ¶
‚àá¬∑ [a(x)‚àáv(x)]w(x) dx =

‚àÇ‚Ñ¶
a(s)w(s)‚àÇv(s)
‚àÇn
ds ‚àí

‚Ñ¶
a(x)[‚àáv(x)] ¬∑ [‚àáw(x)] dx,
4As we have already seen in Section 9.1, this does not mean that the ŒΩth derivative exists every-
where in ‚Ñ¶.

9.2
A synopsis of FEM theory
185
and Green‚Äôs formula

‚Ñ¶
[‚àá2v(x)]w(x) dx +

‚Ñ¶
[‚àáv(x)] ¬∑ [‚àáw(x)] dx =

‚àÇ‚Ñ¶
‚àÇv(s)
‚àÇn w(s) ds.
Here ‚àá=

‚àÇ/‚àÇx1
‚àÇ/‚àÇx2
¬∑ ¬∑ ¬∑
‚àÇ/‚àÇxd
‚ä§, while ‚àÇ/‚àÇn is the derivative in the direc-
tion of the outward normal to the boundary ‚àÇ‚Ñ¶.5 Both the divergence theorem and
the Green formula are special cases of Stokes‚Äôs theorem, which is outside the scope of
our exposition.
Given a linear diÔ¨Äerential operator L from (9.17), we deÔ¨Åne a bilinear form Àúa( ¬∑ , ¬∑ )
such that Àúa(v, w) = ‚ü®Lv, w‚ü©for suÔ¨Éciently smooth functions v and w (i.e. v ‚àà
C2ŒΩ(cl ‚Ñ¶), w ‚ààH) and note that Àúa(v, w), unlike ‚ü®Lv, w‚ü©, remains meaningful when
v, w ‚ààH. The operator L is said to be
self-adjoint
if Àúa(v, w) = Àúa(w, v) for all v, w ‚àà
‚ó¶
H;
elliptic
if Àúa(v, v) > 0 for all v ‚àà
‚ó¶
H;
and
positive deÔ¨Ånite
if it is both self-adjoint and elliptic.
An important example of a positive deÔ¨Ånite operator is
L = ‚àí
d

i=1
‚àÇ
‚àÇxi
d

j=1
bi,j(x) ‚àÇ
‚àÇxj
,
(9.18)
where the matrix B(x) = (bi,j(x)), i, j = 1, 2, . . . , d, is symmetric and positive deÔ¨Ånite
for every x ‚àà‚Ñ¶. To prove this we use a variant of the divergence theorem. Since w ‚àà
‚ó¶
H,
it vanishes along the boundary ‚àÇ‚Ñ¶and it is easy to verify that
‚ü®Lv, w‚ü©= ‚àí

‚Ñ¶
‚éß
‚é®
‚é©
d

i=1
‚àÇ
‚àÇxi
‚é°
‚é£
d

j=1
bi,j(x)‚àÇv(x)
‚àÇxj
‚é§
‚é¶
‚é´
‚é¨
‚é≠w(x) dx
=

‚Ñ¶
d

i=1
d

j=1
‚àÇv(x)
‚àÇxi

bi,j(x)
‚àÇw(x)
‚àÇxj

dx.
(9.19)
Note that, while the formal term ‚ü®Lv, w‚ü©above requires v to be twice diÔ¨Äerentiable,
integration by parts converts the integral into a form in which v, w ‚àà
‚ó¶
H is allowed:
this is precisely our bilinear form Àúa. Since bi,j ‚â°bj,i, i, j = 1, 2, . . . , d, we deduce that
the last expression is symmetric in v and w. Therefore Àúa(v, w) = Àúa(w, v) and L is
self-adjoint. To prove ellipticity we let w = v Ã∏‚â°0 in (9.19); then
Àúa(v, v) =

‚Ñ¶
d

i=1
d

j=1
‚àÇv(x)
‚àÇxi

bi,j(x)
‚àÇv(x)
‚àÇxj

dx > 0
5By rights, this means that the Laplace operator should be denoted by ‚àá‚ä§‚àá, ‚àá¬∑ ‚àáor div grad,
rather than ‚àá2 (a distinction which becomes crucial in algebraic topology), and that, faithful to our
convention, we should really use boldface to remind ourselves that ‚àáis a vector. Regretfully, and
with a heavy sigh, pedantry yields to convention.

186
The Ô¨Ånite element method
by deÔ¨Ånition of the positive deÔ¨Åniteness of matrices (A.1.3.5).
Note that both the negative of the Laplace operator, ‚àí‚àá2, and the one-dimensional
operator
‚àíd
dx

a(x) d
dx

+ b(x),
(9.20)
where a(x) > 0 and b(x) ‚â•0 in the interval of interest, are special cases of (9.18);
therefore they are positive deÔ¨Ånite.
Whenever a diÔ¨Äerential operator L is positive deÔ¨Ånite, we can identify the diÔ¨Äeren-
tial equation (9.17) with a variational problem, thereby setting the stage for the Ritz
method.
Theorem 9.1
Provided that the operator L is positive deÔ¨Ånite, (9.17) is the Euler‚Äì
Lagrange equation of the variational problem
J (v) := Àúa(v, v) ‚àí2‚ü®f, v‚ü©,
v ‚ààH.
(9.21)
The weak solution of Lu = f is therefore the unique minimum of J in H.6
Proof
We generalize an argument that has already been set out in Section 9.1
for the special case of the two-point boundary value problem (9.20).
Because of ellipticity, the variational functional J possesses a minimum (see Ex-
ercise 9.7). Let us denote a local minimum by u ‚ààH. Therefore, for any given v ‚àà
‚ó¶
H
and suÔ¨Éciently small |Œµ| we have
J (u) ‚â§J (u + Œµv) = Àúa(u + Œµv, u + Œµv) ‚àí2‚ü®f, u + Œµv‚ü©.
The form Àúa being linear, this results in
J (u) ‚â§[Àúa(u, u) ‚àí2‚ü®f, u‚ü©] + Œµ[Àúa(v, u) + Àúa(u, v) ‚àí2‚ü®f, v‚ü©] + Œµ2Àúa(v, v)
and self-adjointness together with linearity yield
J (u) ‚â§J (u) + 2Œµ[Àúa(u, v) ‚àí‚ü®f, v‚ü©] + Œµ2Àúa(v, v).
In other words,
2Œµ[Àúa(v, v) ‚àí‚ü®f, v‚ü©] + Œµ2Àúa(v, v) ‚â•0
(9.22)
for all v ‚àà
‚ó¶
H and suÔ¨Éciently small |Œµ|.
Suppose that u is not a weak solution of (9.17). Then there exists v ‚àà
‚ó¶
H, v Ã∏‚â°0,
such that Àúa(u, v) ‚àí‚ü®f, v‚ü©Ã∏= 0. We may assume without loss of generality that this
inner product is negative, otherwise we replace v by ‚àív. It follows that, choosing
suÔ¨Éciently small Œµ > 0, we may render the expression on the left of (9.22) negative.
Since this is forbidden by the inequality, we deduce that no such v ‚àà
‚ó¶
H exists and u is
indeed a weak solution of (9.17).
6An unexpected (and very valuable) consequence of this theorem is the existence and uniqueness
of the solution of (9.17) in H. Therefore Theorem 9.1 ‚Äì like much of the material in this section ‚Äì is
relevant to both the analytic and numerical aspects of elliptic PDEs.

9.2
A synopsis of FEM theory
187
Assume, though, that J has several local minima in
‚ó¶
H and denote two such distinct
functions by u1 and u2. Repeating our analysis with Œµ = 1 whilst replacing v by
u2 ‚àíu1 ‚àà
‚ó¶
H results in
J (u2) = J (u1 + (u2 ‚àíu1))
= J (u1) + 2[Àúa(u1, u2 ‚àíu1) ‚àí‚ü®f, u2 ‚àíu1‚ü©] + Àúa(u2 ‚àíu1, u2 ‚àíu1).
(9.23)
We have just proved that Àúa(u1, u2 ‚àíu1) ‚àí‚ü®f, u2 ‚àíu1‚ü©= 0, since u1 locally minimizes
J . Moreover L is elliptic and u2 Ã∏= u1, therefore Àúa(u2 ‚àíu1, u2 ‚àíu1) > 0. Substitution
into (9.23) yields the contradictory inequality J (u1) < J (u1), thereby leading us to
the conclusion that J possesses a single minimum in
‚ó¶
H.
3 When is a zero really a zero?
An important yet subtle point in the
theory of function spaces is the identity of the zero function. In other words,
when are u1 and u2 really diÔ¨Äerent? Suppose for example that u2 is the same
as u1, except that it has a diÔ¨Äerent value at just one point. This, clearly,
will pass unnoticed by our inner product, which consists of integrals. In other
words, if u1 is a minimum of J (and a weak solution of (9.17)), then so is u2;
in this sense there is no uniqueness. In order to be distinct in the sense of the
function space
‚ó¶
H, u1 and u2 need to satisfy ‚à•u2 ‚àíu1‚à•> 0. In the language of
measure theory, they must diÔ¨Äer on a set of positive Lebesgue measure.
The truth, seldom spelt out in elementary texts, is that a normed function
space (i.e., a linear function space equipped with a norm) sometimes consists
not of functions but of equivalence classes of functions: u1 and u2 are in the
same equivalence class if ‚à•u2 ‚àíu1‚à•= 0 (that is, if u2 ‚àíu1 is of measure
zero). This is an artefact of function spaces deÔ¨Åned on continua that has no
counterpart in the more familiar vector spaces such as Rd. Fortunately, as
soon as this point is comprehended, we can, like everybody else, go back to
our habit of referring to the members of
‚ó¶
H as ‚Äòfunctions‚Äô.
3
The Ritz method for (9.17) (where L is presumed positive deÔ¨Ånite) is a straightfor-
ward generalization of the corresponding algorithm from the last section. Again, we
choose œï0 ‚ààH, let m linearly independent vectors œï1, œï2, . . . , œïm ‚àà
‚ó¶
H span a Ô¨Ånite-
dimensional linear space
‚ó¶
H and seek a vector Œ≥ =

Œ≥1
Œ≥2
¬∑ ¬∑ ¬∑
Œ≥m
‚ä§‚ààRm that
will minimize
Jm(Œ¥) := J

œï0 +
m

‚Ñì=1
Œ¥‚Ñìœï‚Ñì

,
Œ¥ ‚ààRm.
We set the gradient of Jm to 0, and this results in the m linear equations (9.7), where
ak,‚Ñì= Àúa(œïk, œï‚Ñì),
k = 1, 2, . . . , m,
‚Ñì= 0, 1, . . . , m.
(9.24)
Incidentally, the self-adjointness of L means that ak,‚Ñì= a‚Ñì,k, k, ‚Ñì= 1, 2, . . . , m. This
saves roughly half the work of evaluating integrals.
Moreover, the symmetry of a
matrix often simpliÔ¨Åes the task of its numerical solution.

188
The Ô¨Ånite element method
The general Galerkin method is also an easy generalization of the algorithm pre-
sented in Section 9.1 for the ODE (9.1). Again, we seek Œ≥ such that
Àúa

œï0 +
m

‚Ñì=1
Œ≥‚Ñìœï‚Ñì, œïk

‚àí‚ü®f, œïk‚ü©= 0,
k = 1, 2, . . . , m.
(9.25)
In other words, we endeavour to approximate a weak solution from a Ô¨Ånite-dimensional
space.
We have stipulated that L is linear, and this means that (9.25) is, again, nothing
other than the linear system (9.7) with coeÔ¨Écients deÔ¨Åned by (9.24). However, (9.25)
makes sense even for nonlinear operators.
The existence and uniqueness of the solution of the Ritz‚ÄìGalerkin equations (9.7)
has already been addressed in Theorem 9.1.
Another important statement is the
Lax‚ÄìMilgram theorem, which requires more than ellipticity but considerably less than
self-adjointness. Moreover, it also provides a most valuable error estimate.
Given any v ‚ààH, we let
‚à•v‚à•H :=

‚à•v‚à•2 + [Àúa(v, v)]1/2
.
It is possible to prove that ‚à•¬∑ ‚à•H is a norm ‚Äì in fact, this is a special case of the famed
Sobolev norm and it is the correct way of measuring distances in H. We say that the
bilinear form Àúa is
bounded
if there exists Œ¥ > 0 such that |Àúa(v, w)| ‚â§Œ¥‚à•v‚à•H √ó‚à•w‚à•H for every
v, w ‚ààH;
and
coercive
if there exists Œ∫ > 0 such that Àúa(v, v) ‚â•Œ∫‚à•v‚à•2
H for every v ‚ààH.
Theorem 9.2 (The Lax‚ÄìMilgram theorem)
Let L be linear, bounded and coer-
cive and let V be a closed linear subspace of
‚ó¶
H. There exists a unique Àúu ‚ààœï0 +V such
that
Àúa(Àúu, v) ‚àí‚ü®f, v‚ü©= 0,
v ‚ààV
and
‚à•Àúu ‚àíu‚à•H ‚â§Œ¥
Œ∫ inf {‚à•v ‚àíu‚à•H : v ‚ààœï0 + V} ,
(9.26)
where œï0 ‚ààH is arbitrary and u is a weak solution of (9.17) in H.
The inequality (9.26) is sometimes called the C¬¥ea lemma.
The space V need not be Ô¨Ånite dimensional.
In fact, it could be the space
‚ó¶
H
itself, in which case we would deduce from the Ô¨Årst part of the theorem that the
weak solution of (9.17) exists and is unique.
Thus, exactly like Theorem 9.1, the
Lax‚ÄìMilgram theorem can be used for analytic, as well as numerical, ends.
A proof of the coercivity and boundedness of L is typically much more diÔ¨Écult
than a proof of its positive deÔ¨Åniteness. It suÔ¨Éces to say here that, for most domains
of interest, it is possible to prove that the operator ‚àí‚àá2 satisÔ¨Åes the conditions of

9.2
A synopsis of FEM theory
189
the Lax‚ÄìMilgram theorem. An essential step in this proof is the Poincar¬¥e inequality:
there exists a constant c, dependent only on ‚Ñ¶, such that
‚à•v‚à•‚â§c
-----
d

i=1
‚àÇv
‚àÇxi
----- ,
v ‚àà
‚ó¶
H .
As far as the FEM is concerned, however, the error estimate (9.26) is the most
valuable consequence of the theorem. On the right-hand side we have a constant, Œ¥/Œ∫,
which is independent of the choice of
‚ó¶
Hm = V and of the norm of the distance of the
exact solution u from the aÔ¨Éne space œï0 +
‚ó¶
Hm. Of course, infv‚ààœï0+
‚ó¶
Hm ‚à•v ‚àíu‚à•H is
unknown, since we do not know u. The one piece of information, however, that is
deÔ¨Ånitely true about u is that it lies in H = œï0 +
‚ó¶
H. Therefore the distance from u to
œï0 +
‚ó¶
Hm can be bounded in terms of the distance of an arbitrary member w ‚ààœï0 +
‚ó¶
H
from œï0 +
‚ó¶
Hm.
The Ô¨Ånal observation is that œï0 makes no diÔ¨Äerence to our esti-
mates and we hence deduce that, subject to linearity, boundedness and coercivity, the
estimation of the error in the Galerkin method can be replaced by an approximation-
theoretical problem: given a function w ‚àà
‚ó¶
H Ô¨Ånd the distance infv‚àà
‚ó¶
Hm ‚à•w ‚àív‚à•H.
In particular, the question of the convergence of the FEM reduces, subject to
the conditions of Theorem 9.2, to the following question in approximation theory.
Suppose that we have an inÔ¨Ånite sequence of linear spaces
‚ó¶
Hm1,
‚ó¶
Hm2, . . . ‚äÇ
‚ó¶
H, where
dim
‚ó¶
Hmi= mi and the sequence {mi}‚àû
i=1 ascends monotonically to inÔ¨Ånity. Is it true
that
lim
i‚Üí‚àû‚à•umi ‚àíu‚à•H = 0,
where umi is the Galerkin solution in the space œï0 +
‚ó¶
Hmi? In the light of the inequality
(9.26) and of our discussion, a suÔ¨Écient condition for convergence is that for every
v ‚àà
‚ó¶
H it is true that
lim
i‚Üí‚àû
inf
w‚àà
‚ó¶
Hmi
‚à•v ‚àíw‚à•H = 0.
(9.27)
It now pays to recall, when talking of the FEM, that the spaces
‚ó¶
Hmi are spanned
by functions with small support. In other words, each
‚ó¶
Hmi possesses a basis
œï[i]
1 , œï[i]
2 , . . . , œï[i]
mi ‚àà
‚ó¶
Hmi
such that each œï[i]
j is supported on the open set E[i]
j ‚äÇ
‚ó¶
Hmi and E[i]
k ‚à©E[i]
‚Ñì= ‚àÖfor most
choices of k, ‚Ñì= 1, 2, . . . , mi. In practical terms, this means that the d-dimensional
set ‚Ñ¶needs to be partitioned as follows:
cl ‚Ñ¶=
ni
7
Œ±=1
cl ‚Ñ¶[i]
Œ± ,
where
‚Ñ¶[i]
Œ± ‚à©‚Ñ¶[i]
Œ≤ = ‚àÖ,
Œ± Ã∏= Œ≤.
Each ‚Ñ¶[i]
Œ± is called an element, hence the name ‚ÄòÔ¨Ånite element method‚Äô. We allow
each support E[i]
j
to extend across a small number of elements.
Hence, E[i]
k ‚à©E[i]
‚Ñì

190
The Ô¨Ånite element method
consists exactly of the sets ‚Ñ¶[i]
Œ± (and possibly their boundaries) that are shared by
both supports. This implies that an overwhelming majority of intersections is empty.
Recall the solution of (9.7) using chapeau functions. In that case mi = i, ni = i+1,
œï[i]
k = œà
	
x ‚àí
k
i + 1

,
k = 1, 2, . . . , i
(œà having been deÔ¨Åned in (9.8)),
‚Ñ¶[i]
Œ± =
	Œ± ‚àí1
i + 1 ,
Œ±
i + 1

,
Œ± = 1, 2, . . . , i + 1,
and
E[i]
j = ‚Ñ¶[i]
j‚àí1 ‚à™‚Ñ¶[i]
j ,
j = 1, 2, . . . , i.
Further examples, in two spatial dimensions, feature in Section 9.3.
What are reasonable requirements for a ‚ÄòÔ¨Ånite element space‚Äô
‚ó¶
Hmi? Firstly, of
course,
‚ó¶
Hmi‚äÇ
‚ó¶
H, and this means that all members of the set must be suÔ¨Éciently
smooth to be subjected to the weak form (i.e., after integration by parts) of action
by L. Secondly, each set ‚Ñ¶[i]
Œ± must contain functions œï[i]
j
of suÔ¨Écient number and
variety to be able to approximate well arbitrary functions; recall (9.27). Thirdly, as
i increases and the partition is being reÔ¨Åned, we wish to ensure that the diameters
of all elements ultimately tend to zero. It is usual to express this as the requirement
that limi‚Üí‚àûhi = 0, where
hi =
max
Œ±=1,2,...,ni diam ‚Ñ¶[i]
Œ±
is the diameter of the ith partition.7 This does not mean that we need to reÔ¨Åne all
elements at an equal speed ‚Äì an important feature of the FEM is that it lends itself
to local reÔ¨Ånement, and this confers important practical advantages. Our fourth and
Ô¨Ånal requirement is that, as i ‚Üí‚àû, the geometry of the elements does not become
too ‚ÄòdiÔ¨Écult‚Äô: in practical terms, the elements are likely to be polytopes (for example,
polygons in R2) and we wish all their angles to be bounded away from zero as i ‚Üí‚àû.
The latter two conditions are relatively simple to formulate and enforce, but the
Ô¨Årst two require further attention and elaboration. As far as the smoothness of œï[i]
j ,
j = 1, 2, . . . , mi, is concerned, the obvious diÔ¨Éculty is likely to be smoothness across
element boundaries, since it is in general easy to specify arbitrarily smooth functions
within each ‚Ñ¶[i]
Œ± . However, ‚Äòapproximability‚Äô of the Ô¨Ånite element space
‚ó¶
Hmi is all
about what happens inside each element.
Our policy in the remainder of this chapter is to use elements ‚Ñ¶[i]
Œ± that are all linear
translates of the same ‚Äòmaster element‚Äô, in the same way as the chapeau function (9.8)
is deÔ¨Åned in the interval [‚àí1, 1] and then translated to arbitrary intervals. SpeciÔ¨Åcally,
for d = 2 our interest centres on the translates of triangular elements (not necessarily
all with identical angles) and quadrilateral elements. We choose functions œï[i]
j that are
7The quantity diam U, where U is a bounded set, is deÔ¨Åned as the least radius of a ball into which
this set can be inscribed. It is called the diameter of U.

9.2
A synopsis of FEM theory
191
polynomial within each element ‚Äì obviously the question of smoothness is relevant only
across element boundaries. Needless to say, our reÔ¨Ånement condition limi‚Üí‚àûhi = 0
and our ban on arbitrarily acute angles are strictly enforced.
We say that the space
‚ó¶
Hmi is of smoothness q if each function œï[i]
j , j = 1, 2, . . . , mi,
is q ‚àí1 times smoothly diÔ¨Äerentiable in ‚Ñ¶and q times diÔ¨Äerentiable inside each ele-
ment E[i]
Œ± , Œ± = 1, 2, . . . , ni. (The latter requirement is automatically satisÔ¨Åed within
our framework, since we have already required all functions œï[i]
j
to be polynomials.
It is stated for the sake of conformity with more general Ô¨Ånite element spaces.) Fur-
thermore, the space
‚ó¶
Hmi is of accuracy p if, within each element ‚Ñ¶[i]
Œ± , the functions
œï[i]
j
span the set P d
p [x] of all d-dimensional polynomials of total degree p. The latter
encompasses all functions of the form

‚Ñì1+¬∑¬∑¬∑+‚Ñìd‚â§p
‚Ñì1,...,‚Ñìd‚â•0
c‚Ñì1,‚Ñì2,...,‚Ñìdx‚Ñì1
1 x‚Ñì2
2 ¬∑ ¬∑ ¬∑ x‚Ñìd
d ,
where c‚Ñì1,‚Ñì2,...,‚Ñìd ‚ààR for all ‚Ñì1, ‚Ñì2, . . . , ‚Ñìd.
Let us illustrate the above concepts for the case of (9.1) with chapeau functions.
Firstly, each translate of (9.8) is continuous throughout ‚Ñ¶= (0, 1) but not diÔ¨Äeren-
tiable throughout the whole interval, hence q ‚àí1 = 0 and we deduce a smoothness
q = 1. Secondly, each element ‚Ñ¶[i]
Œ± is the support of both œï[i]
Œ±‚àí1 and œï[i]
Œ± (with obvious
modiÔ¨Åcation for Œ± = 1 and Œ± = i + 1). Both are linear functions, the Ô¨Årst increasing,
with slope +i, and the second decreasing with slope ‚àíi. Hence linear independence
allows the conclusion that every linear function can be expressed inside ‚Ñ¶[i]
Œ± as a linear
combination of œï[i]
Œ±‚àí1 and œï[i]
Œ± . Since P 1
1 [x] = P1[x], the set of all univariate linear
functions, it follows that
‚ó¶
Hm1 is of accuracy p = 1.
Much eÔ¨Äort has been spent in the last few pages in arguing that there is an in-
timate connection between smoothness, accuracy and the error estimate (9.26). Un-
fortunately, this is as far as we can go without venturing into much deeper waters of
functional analysis ‚Äì except for stating, without any proof, a theorem that quantiÔ¨Åes
this connection in explicit terms.
Theorem 9.3
Let L obey the conditions of Theorem 9.2 and suppose that we solve
equation (9.17) by the FEM, subject to the aforementioned restrictions (the shape of
the elements, limm‚Üí‚àûhi = 0 etc.), with smoothness and accuracy q = p ‚â•ŒΩ (ŒΩ is
half the number of derivatives in L, cf. (9.18)). Then there exists a constant c > 0,
independent of i, such that
‚à•um ‚àíu‚à•H ‚â§chp+1‚àíŒΩ
i
‚à•u(p+1)‚à•,
i = 1, 2, . . .
(9.28)
Returning to the chapeau functions and their solution of the two-point boundary
value equation (9.1), we use the inequality (9.28) to conÔ¨Årm our impression from
Figs 9.2 and 9.3, namely that the error is O(hi).
Theorem 9.3 is just a sample of the very rich theory of the FEM. Error bounds
are available in a variety of norms (often with more profound signiÔ¨Åcance to the

192
The Ô¨Ånite element method
underlying problem than the Euclidean norm) and subject to diÔ¨Äerent conditions.
However, inequality (9.28) is suÔ¨Écient for the applications to the Poisson equation in
the next section.
9.3
The Poisson equation
As we saw in the last section, the operator L = ‚àí‚àá2 is positive deÔ¨Ånite, being a
special case of (9.18). Moreover, we have claimed that, for most realistic domains ‚Ñ¶,
it is coercive and bounded. The coeÔ¨Écients (9.24) of the Ritz‚ÄìGalerkin equations are
simply given by
ak,‚Ñì=

‚Ñ¶
(‚àáœïk) ¬∑ (‚àáœï‚Ñì) dx,
k, ‚Ñì= 1, 2, . . . , m.
(9.29)
Letting d = 2, we assume that the boundary ‚àÇ‚Ñ¶is composed of a Ô¨Ånite number
of straight segments and partition ‚Ñ¶into triangles. The only restriction is that no
vertex of one triangle may lie on an edge of another; vertices must be shared. In other
words, a conÔ¨Åguration like
s
s
s
s
s
	
	
	
	
	
	B
B
B
B
B
B



	
	
	
	
	
	
(where the position of a vertex is emphasized by ‚Äò s‚Äô) is not allowed. Figs 9.5 and 9.6
display a variety of triangulations that conform with this rule.
In light of (9.28), we require for convergence that p, q ‚â•1, where p and q are the
accuracy and smoothness respectively. This is similar to the situation that we have
already encountered in Section 9.1 and we propose to address it with a similar remedy,
namely by choosing œï1, œï2, . . . , œïm as piecewise linear functions. Each function in P 2
1
can be represented in the form
g(x, y) = Œ± + Œ≤x + Œ≥y
(9.30)
for some Œ±, Œ≤, Œ≥ ‚ààR. Each function œïk supported by an element ‚Ñ¶j, j = 1, 2, . . . , n,
is consequently of the form (9.30). Thus, to be accurate to order p = 1, each element
must support at least three linearly independent functions. Recall that smoothness q =
1 means that every linear combination of the functions œï1, œï2, . . . , œïm is continuous
in ‚Ñ¶and this, obviously, need be checked only across element boundaries.
We have already seen in Section 9.1 one construction that provides both for accu-
racy p = 1 and for continuity with piecewise linear functions. The idea is to choose a
basis of piecewise linear functions that vanish at all the vertices, except that at each
vertex one function equals +1. Chapeau functions are an example of such cardinal
functions and they have counterparts in R2. Fig. 9.4 displays three examples of pyra-
mid functions, the planar cardinal functions, within their support (the set of all values

9.3
The Poisson equation
193
of the argument for which they are nonzero). Unfortunately, it also demonstrates that
using cardinal functions in R2 is, in general, a poor idea. The number of elements
in each support may change from vertex to vertex and the description of each cardi-
nal function, although easy in principle, is quite messy and inconvenient for practical
work.
Figure 9.4
Pyramid functions for diÔ¨Äerent conÔ¨Ågurations of vertices.
The correct procedure is to represent the approximation inside each ‚Ñ¶j by data at
its vertices. As long as we adopt this approach, how many diÔ¨Äerent triangles meet at
each vertex is of no importance and we can apply the same algorithm to all elements.
Let the triangle in question be
t
t
t





A
A
A
A
(x1, y1)
(x2, y2)
(x3, y3)
We determine the piecewise linear approximation s by interpolating at the three ver-
tices. According to (9.30), this results in the linear system
Œ± + x‚ÑìŒ≤ + y‚ÑìŒ≥ = g‚Ñì,
‚Ñì= 1, 2, 3,
where g‚Ñìis the interpolated value at (x‚Ñì, y‚Ñì). Since the three vertices are not collinear,
the system is nonsingular and can be solved with ease. This procedure (which, for-
mally, is completely equivalent to the use of cardinal functions) ensures accuracy of
order p = 1.
We need to prove that the above approach produces a function that is continuous
throughout ‚Ñ¶, since this is equivalent to q = 1, the required degree of smoothness.
This, however, follows from our construction. Recall that we need to prove continuity
only across element boundaries.
Suppose, without loss of generality, that the line

194
The Ô¨Ånite element method
segment joining (x1, y1) and (x2, y2) is not part of ‚àÇ‚Ñ¶(otherwise there would be
nothing to prove). The function s reduces along a straight line to a linear function
in one variable, hence it is determined uniquely by interpolation of the two values g1
and g2 at the endpoints. Since these endpoints are shared by the triangle that adjoins
along this edge, it follows that s is continuous there. A similar argument extends to
all internal edges of the triangulation. We conclude that p = q = 1, hence the error
(in a correct norm) decays like O(h), where h is the diameter of the triangulation.
A practical solution using the FEM requires us to assemble the stiÔ¨Äness matrix
A = (ak,‚Ñì)m
k,‚Ñì=1 .
The dimension being d = 2, (9.29) formally becomes
ak,‚Ñì=
 
‚Ñ¶
	‚àÇœïk
‚àÇx
‚àÇœï‚Ñì
‚àÇx + ‚àÇœïk
‚àÇy
‚àÇœï‚Ñì
‚àÇy

dx dy
=
n

j=1
 
‚Ñ¶j
	‚àÇœïk
‚àÇx
‚àÇœï‚Ñì
‚àÇx + ‚àÇœïk
‚àÇy
‚àÇœï‚Ñì
‚àÇy

dx dy
=
n

j=1
ak,‚Ñì,j,
k, ‚Ñì= 1, 2, . . . , m.
Inside the jth element the quantity ak,‚Ñì,j vanishes, unless both œïk and œï‚Ñìare supported
there. In the latter case, each is a linear function and, at least in principle, all integrals
can be calculated (probably using quadrature). This, however, fails to take account
of the subtle change of basis that we have just introduced in our characterization of
the approximant inside each element in terms of its values on the vertices. Of course,
except for vertices that happen to lie on ‚àÇ‚Ñ¶, these values are unknown and their
computation is the whole purpose of the exercise. The values at the vertices are our
new unknowns and we thereby rephrase the Ritz problem as follows: out of all possible
piecewise linear functions that are consistent with our partition (i.e. linear inside each
element), Ô¨Ånd the one that minimizes the functional
J (v) =

‚Ñ¶
	‚àÇv
‚àÇx

2
+
	‚àÇv
‚àÇy

2
dx dy ‚àí2

‚Ñ¶
fv dx dy
=
n

j=1

‚Ñ¶j
	‚àÇv
‚àÇx

2
+
	‚àÇv
‚àÇy

2
dx dy ‚àí2
n

j=1

‚Ñ¶j
fv dx dy.
Inside each ‚Ñ¶j the function v is linear, v(x, y) = Œ±j + Œ≤jx + Œ≥jy, and explicitly

‚Ñ¶j
	‚àÇv
‚àÇx

2
+
	‚àÇv
‚àÇy

2
dx dy =

Œ≤2
j + Œ≥2
j

area ‚Ñ¶j.
As far as the second integral is concerned, we usually discretize it by quadrature and
this, again, results in a function of Œ±j, Œ≤j and Œ≥j.
With a little help from elementary analytic geometry, this can be expressed in
terms of the values of v at the vertices.
Let these be v1, v2, v3, say, and assume

9.3
The Poisson equation
195
that the corresponding (inner) angles of the triangle are Œ∏1, Œ∏2, Œ∏3 respectively, where
Œ∏1 + Œ∏2 + Œ∏3 = œÄ. Letting œÉk = 1/(2 tan Œ∏k), k = 1, 2, 3, we obtain

‚Ñ¶j
	‚àÇv
‚àÇx

2
+
	‚àÇv
‚àÇy

2
dx dy =
%
v1
v2
v3
&
‚é°
‚é£
œÉ2 + œÉ3
‚àíœÉ3
‚àíœÉ2
‚àíœÉ3
œÉ1 + œÉ3
‚àíœÉ1
‚àíœÉ2
‚àíœÉ1
œÉ1 + œÉ2
‚é§
‚é¶
‚é°
‚é£
v1
v2
v3
‚é§
‚é¶.
Meting out a similar treatment to the second integral and repeating this procedure
for all elements in the triangulation, we Ô¨Ånally represent the variational functional,
acting on piecewise linear functions, in the form
J (v) = 1
2v‚ä§ÀúAv ‚àíf ‚ä§v,
(9.31)
where v is the vector of the values of the function v at the m internal vertices (the
number of such vertices is the same as the dimension of the space ‚Äì why?). The m√óm
stiÔ¨Äness matrix
ÀúA = (Àúak,‚Ñì)m
k,‚Ñì=1 is assembled from the contributions of individual
vertices. Obviously, Àúak,‚Ñì= 0 unless k and ‚Ñìare indices of neighbouring vertices. The
vector f ‚ààRm is constructed similarly, except that it also contains the contributions
of boundary vertices.
Setting the gradient of (9.31) to zero results in the linear algebraic system
ÀúAv = f,
which we need to solve, e.g. by the methods of Chapters 11‚Äì15.
Our extensive elaboration of the construction of (9.31) illustrates the point that it
is substantially more diÔ¨Écult to work with the FEM than with Ô¨Ånite diÔ¨Äerences. The
extra eÔ¨Äort, however, is the price that we pay for extra Ô¨Çexibility.
Figure 9.5 displays the solution of the Poisson equation (8.33) on three meshes.
These meshes are hierarchical ‚Äì each is constructed by reÔ¨Åning the previous one ‚Äì and
of increasing Ô¨Åneness. The graphs on the left display the meshes, while the shapes on
the right are the numerical solutions as constructed from linear pieces (compare with
the exact solution at the top of Fig. 8.8).
The advantages of the FEM are apparent if we are faced with diÔ¨Écult geometries
and, even more profoundly, when it is known a priori that the solution is likely to
be more problematic in part of the domain of interest and we wish to ‚Äòzoom in‚Äô on
the triangulation there. For example, suppose that a Poisson equation is given in a
domain with a re-entrant corner (for example, an L-shaped domain). We can expect
the solution to be more diÔ¨Écult near such a corner and it is a good policy to reÔ¨Åne
the triangulation there.
As an example, let us consider the equation
‚àá2u + 2œÄ2 sin œÄx sin œÄy = 0,
(x, y) ‚àà‚Ñ¶= (‚àí1, 1)2 \ [0, 1]2,
(9.32)
with zero Dirichlet boundary conditions along ‚àÇ‚Ñ¶.
The exact solution is simply
u(x, y) = sin œÄx sin œÄy.
Figure 9.6 displays the triangulations and underlying numerical solutions for three
meshes that are increasingly reÔ¨Åned. The triangulation is substantially Ô¨Åner near the
re-entrant corner, as it should be, but perhaps the most important observation is that

196
The Ô¨Ånite element method
this does not require more eÔ¨Äort than, say, the uniform tessellation of Fig. 9.5. In
fact, both Ô¨Ågures were produced by an identical program, but with diÔ¨Äerent input!
Although writing such a program is more of a challenge than coding Ô¨Ånite diÔ¨Äerences,
the rewards are very rich indeed . . .
The error in Figs. 9.5 and 9.6 is consistent with the bound ‚à•um ‚àíu‚à•H ‚â§ch‚à•u‚Ä≤‚Ä≤‚à•
(where h is the diameter of the triangulation), and this, in turn, is consistent with
(9.28). In particular, its rate of decay (as a function of h) in Fig. 9.5 is similar to
those of the Ô¨Åve-point formula and the (unmodiÔ¨Åed) nine-point formula in Fig. 8.8.
At Ô¨Årst glance, this might perhaps seem contradictory; did we not state in Chapter 8
that the error of the Ô¨Åve-point formula (8.15) is O

h2
? True enough, except that
here we have been using diÔ¨Äerent criteria to measure the error. Suppose, thus, that
uk,‚Ñì‚âàu(k‚àÜx, ‚Ñì‚àÜx)+ck,‚Ñìh2 at all the grid points. Provided that h = O

m‚àí1/2
(note
that the number m means here the total number of variables in the whole grid), that
there are O(m) grid points and that the error coeÔ¨Écients ck,‚Ñìare of roughly similar
order of magnitude, it is easy to verify that
1
1
m

(k,‚Ñì) in the grid
[uk,‚Ñì‚àíu(k‚àÜx, ‚Ñì‚àÜx)]2
21/2
= O(h) .
This corresponds to the Euclidean norm in the Ô¨Ånite element space. Although the
latter is distinct from the Sobolev norm ‚à•¬∑ ‚à•H of inequality (9.28), our argument
indicates why the two error estimates are similar.
As was the case with Ô¨Ånite diÔ¨Äerence schemes, the aforementioned accuracy some-
times falls short of that desired. This motivates a discussion of function bases having
superior smoothness and accuracy properties. In one dimension this is straightfor-
ward, at least on the conceptual level: we need to replace piecewise linear functions
with splines, functions that are kth-degree polynomials, say, in each element and pos-
sess k ‚àí1 smooth derivatives in the whole interval of interest. A convenient basis for
kth-degree splines is provided by B-splines, which are distinguished by having the least
possible support, extending across k + 1 consecutive elements. In a general partition
Œæ0 < Œæ1 < ¬∑ ¬∑ ¬∑ < Œæn, say, a kth degree B-spline is deÔ¨Åned explicitly by the formula
B[k]
j (x) =
k+j+1

‚Ñì=j
‚éõ
‚éù
k+j+1

i=j, iÃ∏=‚Ñì
1
Œæi ‚àíŒæ‚Ñì
‚éû
‚é†(x ‚àíŒæ‚Ñì)k
+ .
Comparison with (9.8) ascertains that chapeau functions are nothing than linear B-
splines.
The task in hand is more complicated in the case of two-dimensional triangulation,
because of our dictum that everything needs to be formulated in terms of function
values in an individual element and across its boundary. As a matter of fact, we have
used only the values at the boundary ‚Äì speciÔ¨Åcally, at the vertices ‚Äì but this is about
to change.
A general quadratic in R2 is of the form
s(x, y) = Œ± + Œ≤x + Œ≥y + Œ¥x2 + Œ∑xy + Œ∂y2;

9.3
The Poisson equation
197
Figure 9.5
The solution of the Poisson equation (8.33) in a square domain with
various triangulations.

198
The Ô¨Ånite element method
Figure 9.6
The solution of the Poisson equation (9.32) in an L-shaped domain
with various triangulations.

9.3
The Poisson equation
199
we note that it has six parameters.
Likewise, a general cubic has ten parameters
(verify!). We need to specify the correct number of interpolation points in the (closed)
triangle. Two choices that give orders of accuracy p = 2 and p = 3 are
s
s
s
s
s
s
	
	
	
	
	
	
B
B
B
B
B
B
and
s
s
s
s
s
s
s
s
s
s
	
	
	
	
	
	
B
B
B
B
B
B
respectively.
Unfortunately, their smoothness q is just 1 since, although a unique
univariate quadratic or cubic, respectively, can be Ô¨Åtted along each edge (hence en-
suring continuity), a tangental derivative might well be discontinuous.
A superior
interpolation pattern is
s
s
s
s
f
f
f
	
	
	
	
	
	
B
B
B
B
B
B
(9.33)
where ‚Äò sf‚Äô means that we interpolate both function values and both spatial deriva-
tives. We require altogether ten data items, and this is exactly the number of degrees
of freedom in a bivariate cubic. Moreover, it is possible to show that the Hermite
interpolation of both function values and (directional) derivatives along each edge
results in both function and derivative smoothness there, hence q = 2.
Interpolation patterns like (9.33) are indispensable when, instead of the Laplace
operator we consider the biharmonic operator ‚àá4, since then ŒΩ = 2 and we need q ‚â•2
(see Exercise 9.5).
We conclude this chapter with a few words on piecewise linear interpolation with
quadrilateral elements. The main problem in this context is that the bivariate linear
function has three parameters ‚Äì exactly right for a triangle but problematic in a
quadrilateral. Recall that we must place interpolation points so as to attain continuity
in the whole domain, and this means that at least two such points must reside along
each edge. The standard solution of this conundrum is to restrict one‚Äôs attention to
rectangles (aligned with the axes) and interpolate with functions of the form
s(x, y) = s1(x)s2(y),
where
s1(x) := Œ± + Œ≤x,
s2(y) := Œ≥ + Œ¥y.
Obviously, piecewise linear functions are a proper subset of the functions s, but now
we have four parameters, just right for interpolating at the four corners:
s
s
s
s

200
The Ô¨Ånite element method
Along both horizontal edges s2 is constant and s1 is uniquely speciÔ¨Åed by the values
at the corners. Therefore, the function s along each horizontal edge is independent of
the interpolated values elsewhere in the rectangle. Since an identical statement is true
for the vertical edges, we deduce that the interpolant is continuous and that q = 1.
Comments and bibliography
Weak solutions and Sobolev spaces are two inseparable themes that permeate the modern
theory of linear elliptic diÔ¨Äerential equations (Agmon, 1965; Evans, 1998; John, 1982). The
capacity of the FEM to Ô¨Åt snugly into this framework is not just a matter of √¶sthetics.
Also, as we have had a chance to observe in this chapter, it provides for truly powerful error
estimates and for a computational tool that can cater for a wide range of diÔ¨Écult situations ‚Äì
curved geometries, problems with internal interfaces, solutions with singularities . . . Yet, the
FEM is considerably less popular in applications than the Ô¨Ånite diÔ¨Äerence method. The two
reasons are the considerably more demanding theoretical framework and the more substantial
eÔ¨Äort required to program the FEM. If all you need is to solve the Poisson equation in a
square, say, with nice boundary conditions, then probably there is absolutely no need to
bother with the FEM (unless oÔ¨Ä-the-shelf FEM software is available), since Ô¨Ånite diÔ¨Äerences
will do perfectly well. More diÔ¨Écult problems, e.g. the equations of elasticity theory, the
Navier‚ÄìStokes equations etc. justify the additional eÔ¨Äort involved in mastering and using
Ô¨Ånite elements.
It is legitimate, however, to query how genuine weak solutions are. Anybody familiar with
the capacity of mathematicians to generalize from the mundane yet useful to the beautiful
yet useless has every right to feel sceptical. The simple answer is that they occur in many
application areas, in linear as well as nonlinear PDEs and in variational problems. Moreover,
seemingly ‚Äònice‚Äô problems often have weak solutions. For a simple example, borrowed from
Gelfand & Fomin (1963), we turn to the calculus of variations. Let
J (v) :=
 1
‚àí1
v2(œÑ)[2œÑ ‚àív‚Ä≤(œÑ)]2 dœÑ,
v(‚àí1) = 0,
v(1) = 1;
this is a nice cosy problem which, needless to say, should have a nice cosy solution. And it
does! The exact solution can be written down explicitly,
u(x) =
# x2,
0 ‚â§x ‚â§1,
0,
‚àí1 ‚â§x ‚â§0.
However, the underlying Euler‚ÄìLagrange equation is
y

4x2 + 2y ‚àíy‚Ä≤2 ‚àíyy‚Ä≤‚Ä≤
= 0
(9.34)
and includes a second derivative, while the function u fails to be twice diÔ¨Äerentiable at the
origin. The solution of (9.34) exists only in a weak sense!
Lest the last example sounds a mite artiÔ¨Åcial (and it is ‚Äì artiÔ¨Åciality is the price of
simplicity!), let us add that many equations of profound interest in applications can be
investigated only in the context of weak solutions and Sobolev spaces. A thoroughly modern
applied mathematician must know a great deal of mathematical analysis.
An unexpected luxury for students of the FEM is the abundance of excellent books in
the subject, e.g. Axelsson & Barker (1984); Brenner & Scott (2002); Hackbusch (1992);
Johnson (1987); Mitchell & Wait (1977).
Arguably, the most readable introductory text

Exercises
201
is Strang & Fix (1973) ‚Äì and it is rare for a book in a fast-moving subject to stay at
the top of the hit parade for more than 30 years! The most comprehensive exposition of
the subject, short of research papers and specialized monographs, is Ciarlet (1976). The
reader is referred to this FEM feast for a thorough and extensive exposition of themes upon
which we have touched brieÔ¨Çy ‚Äì error bounds, the design of Ô¨Ånite elements in multivariate
spaces ‚Äì and many themes that have not been mentioned in this chapter. In particular, we
encourage interested readers to consult more advanced monographs on the generalization of
Ô¨Ånite element functions to d ‚â•3, on the attainment of higher smoothness conditions and
on elements with curved boundaries. Things are often not what they seem to be in Sobolev
spaces and it is always worthwhile, when charging the computational ramparts, to ensure
adequate pure-mathematical covering Ô¨Åre.
These remarks will not be complete without mentioning recent work that blends concepts
from the Ô¨Ånite element, Ô¨Ånite diÔ¨Äerence and spectral methods. A whole new menagerie of
concepts has emerged in the last two decades: boundary element methods, the h-p formulation
of the FEM, hierarchical bases . . . Only the future will tell how much will survive and Ô¨Ånd its
way into textbooks, but these are exciting times at the frontiers of the FEM.
Agmon, S. (1965), Lectures on Elliptic Boundary Value Problems, Van Nostrand, Princeton,
NJ.
Axelsson, O. and Barker, V.A. (1984), Finite Element Solution of Boundary Value Problems:
Theory and Computation, Academic Press, Orlando, FL.
Brenner, S.C. and Scott, L.R. (2002), The Mathematical Theory of Finite Element Methods
(2nd edn), Springer-Verlag, New York.
Ciarlet, P.G. (1976), Numerical Analysis of the Finite Element Method, North-Holland, Am-
sterdam.
Evans, L.C. (1998), Partial DiÔ¨Äerential Equations, American Mathematical Society, Provi-
dence, RI.
Gelfand, I.M. and Fomin, S.V. (1963), Calculus of Variations, Prentice‚ÄìHall, Englewood
CliÔ¨Äs, NJ.
Hackbusch, W. (1992), Elliptic DiÔ¨Äerential Equations: Theory and Numerical Treatment,
Springer-Verlag, Berlin.
John, F. (1982), Partial DiÔ¨Äerential Equations (4th edn), Springer-Verlag, New York.
Johnson, C. (1987), Numerical Solution of Partial DiÔ¨Äerential Equations by the Finite Ele-
ment Method, Cambridge University Press, Cambridge.
Mitchell, A.R. and Wait, R. (1977), The Finite Element Method in Partial DiÔ¨Äerential Equa-
tions, Wiley, London.
Strang, G. and Fix, G.J. (1973), An Analysis of the Finite Element Method, Prentice‚ÄìHall,
Englewood CliÔ¨Äs, NJ.
Exercises
9.1
Demonstrate that in the interval [tn, tn+1] the collocation method (3.12) Ô¨Ånds
an approximation to the weak solution of the ordinary diÔ¨Äerential system

202
The Ô¨Ånite element method
y‚Ä≤ = f(t, y), y(tn) = yn, from the space PŒΩ of ŒΩth-degree polynomials,
provided that we employ the inner product
‚ü®v, w‚ü©=
ŒΩ

j=1
v(tn + cjh)‚ä§w(tn + cjh),
where h = tn+1 ‚àítn. (Strictly speaking, ‚ü®¬∑ , ¬∑ ‚ü©is a semi-inner product, since
it is not true that ‚ü®v, v‚ü©= 0 implies v ‚â°0.)
9.2
Find explicitly the coeÔ¨Écients ak,‚Ñì, k, ‚Ñì= 1, 2, . . . , m, for the equation ‚àíy ‚Ä≤‚Ä≤+
y = f, assuming that the space
‚ó¶
Hm is spanned by chapeau functions on an
equidistant grid.
9.3
Suppose that the equation (9.1) is solved by the Galerkin method with cha-
peau functions on a non-equidistant grid.
In other words, we are given
0 = t0 < t1 < t2 < ¬∑ ¬∑ ¬∑ < tm < tm+1 = 1 such that each œïj is supported in
(tj‚àí1, tj+1), j = 1, 2, . . . , m. Prove that the linear system (9.7) is nonsingu-
lar. (Hint: Use the GerÀásgorin criterion (Lemma 8.3).)
9.4
Let a be a given positive univariate function and
L := ‚àÇ2
‚àÇx2

a(x) ‚àÇ2
‚àÇx2

.
Assuming zero Dirichlet boundary conditions, prove that L is positive deÔ¨Å-
nite in the Euclidean norm.
9.5
Prove that the biharmonic operator ‚àá4, acting in a parallelepiped in Rd, is
positive deÔ¨Ånite in the Euclidean norm.
9.6
Let J be given by (9.21), suppose that the operator L is positive deÔ¨Ånite
and let
Jm(Œ¥) := J

œï0 +
m

‚Ñì=1
Œ¥‚Ñìœï‚Ñì

,
Œ¥ ‚ààRm.
Prove that the matrix
	‚àÇ2Jm(Œ¥)
‚àÇŒ¥k‚àÇŒ¥‚Ñì

k,‚Ñì=1,2,...,m
is positive deÔ¨Ånite, thereby deducing that the solution of the Ritz equations
is indeed the global minimum of Jm.
9.7
Let L be an elliptic diÔ¨Äerential operator and f a given bounded function.
a Prove that the numbers
c1 := min
v‚àà
‚ó¶
H
‚à•v‚à•=1
Àúa(v, v)
and
c2 := max
v‚àà
‚ó¶
H
‚à•v‚à•=1
‚ü®f, v‚ü©
are bounded and that c1 > 0.

Exercises
203
b Given w ‚ààH, prove that
Àúa(w, w) ‚àí2‚ü®f, w‚ü©‚â•c1‚à•w‚à•2 ‚àí2c2‚à•w‚à•.
(Hint: Write w = Œ∫v, where ‚à•v‚à•= 1 and |Œ∫| = ‚à•w‚à•.)
c Deduce that
Àúa(w, w) ‚àí2‚ü®f, w‚ü©‚â•‚àíc2
2
c1
,
w ‚ààH,
thereby proving that the functional J from (9.21) has a bounded minimum.
9.8
Find explicitly a cardinal piecewise linear function (a pyramid function) in a
domain partitioned into equilateral triangles (cf. the graph in Exercise 8.13).
9.9
Nine interpolation points are speciÔ¨Åed in a rectangle:
s
s
s
s
s
s
s
s
s
Prove that they can be interpolated by a function of the form s(x, y) =
s1(x)s2(y), where both s1 and s2 are quadratics. Find the orders of the
accuracy and of the smoothness of this procedure.
9.10
The Poisson equation is solved in a square partition by the FEM in the
manner described in Section 9.3. In each square element the approximant is
the function s(x, y) = s1(x)s2(y), where s1 and s2 are linear, and it is being
interpolated at the vertices. Derive explicitly the entries Àúak,‚Ñìof the stiÔ¨Äness
matrix ÀúA from (9.31).
9.11
Prove that the four interpolatory conditions speciÔ¨Åed at the vertices of the
three-dimensional tetrahedral element
s
s
s
s
can be satisÔ¨Åed by a piecewise linear function.


10
Spectral methods
10.1
Sparse matrices vs. small matrices
In the previous two chapters we have introduced methods based on completely dif-
ferent principles: Ô¨Ånite diÔ¨Äerences rest upon the replacement of derivatives by linear
combinations of function values but the idea behind Ô¨Ånite elements is to approximate
an inÔ¨Ånite-dimensional expansion of the solution in a Ô¨Ånite-dimensional space. Yet
the implementation of either approach ultimately leads to the solution of a system of
algebraic equations. The bad news about such a system is that it tends to be very
large indeed; the good news is that it is highly structured, usually very sparse, hence
lending itself to eÔ¨Äective algorithms for the solution of sparse linear algebraic systems,
the theme of Chapters 11‚Äì15.
In other words, both Ô¨Ånite diÔ¨Äerences and Ô¨Ånite elements converge fairly slowly
(hence the matrices are large) but the weak coupling between the variables results in
sparsity and in practice algebraic systems can be computed notwithstanding their size.
Once we formulate the organizing principle of both kinds of method in this manner,
it immediately suggests an enticing alternative: methods that produce small matrices
in the Ô¨Årst place. Although we are giving up sparsity, the much smaller size of the
matrices renders their solution aÔ¨Äordable.
How do we construct such ‚Äòsmall matrix‚Äô methods? The large size of the matrices
in Chapters 8 and 9 was caused by slow convergence of the underlying approximations,
which resulted in a large number of parameters (grid points or Ô¨Ånite element functions).
Thus, the key is to devise approximation methods that exhibit considerably faster
convergence, hence requiring much smaller number of parameters.
Before we thus approximate solutions of diÔ¨Äerential equations, we need to look
at the approximation of functions. In Fig. 10.1 we display the error incurred when
the function e‚àíx is approximated in [‚àí1, 1] by piecewise linear functions (the chapeau
functions of Chapter 9) with equally spaced nodes k/N, k = ‚àíN/2, . . . , N/2; here and
elsewhere in this chapter N ‚â•2 is an even integer. The local error of piecewise linear
approximation is O

N ‚àí2
and so we expect this to be roughly divided by four each
time N is doubled. This is indeed conÔ¨Årmed by the Ô¨Ågure. Likewise, in Fig. 10.2 we
display the error when the function e‚àícos œÄx is approximated by cheapau functions.
Again, the error decays fairly predictably: the reason why we are so keen on this Ô¨Ågure
will become clear later.
205

206
Spectral methods
‚àí1.0
‚àí0.5
0
0.5
1.0
0
0.5
1.0
1.5
2.0
2.5
3.0
x 10
‚àí3
N  = 20
‚àí1.0
‚àí0.5
0
0.5
1.0
0
2
4
6
8
x 10
‚àí4
N  = 40
‚àí1.0
‚àí0.5
0
0.5
1.0
0
1
2
x 10
‚àí4
N  = 80
‚àí1.0
‚àí0.5
0
0.5
1.0
0
1
2
3
4
5
x 10
‚àí5
N  = 160
Figure 10.1
The error in approximating the function e‚àíx in [‚àí1, 1] by piecewise
linear functions with N degrees of freedom.
We will compare the chapeau-function approximation
f(x) ‚âà
N/2

n=‚àíN/2+1
œà(Nn ‚àíx)f
	2n
N

(see (9.8) for the deÔ¨Ånition of the chapeau function œà) with the truncated Fourier
approximation:
f(x) ‚âàœïN(x) =
N/2

n=‚àíN/2+1
ÀÜfneiœÄnx,
where
ÀÜfn = 1
2
 1
‚àí1
f(œÑ)e‚àíiœÄnœÑ dœÑ,
n ‚ààZ.
(10.1)
Before looking at numerical results, it is useful to recall a basic fact on Fourier series
and their convergence.
Theorem 10.1 (The de la Vall¬¥ee Poussin theorem)
If the function f is Rie-
mann integrable and ÀÜfn = O

n‚àí1
for |n| ‚â´1 then œïN(x) = f(x) + O

N ‚àí1
as
N ‚Üí‚àûfor every point x ‚àà(‚àí1, 1) where f is Lipschitz.
Note that if f is smoothly diÔ¨Äerentiable then, integrating by parts,
ÀÜfn = ‚àí(‚àí1)n
2iœÄn [f(1) ‚àíf(‚àí1)] ‚àí
1
2iœÄn
=f ‚Ä≤n = O

n‚àí1
,
|n| ‚â´1.

10.1
Sparse matrices vs. small matrices
207
‚àí1.0
‚àí0.5
0
0.5
1.0
‚àí0.03
‚àí0.02
‚àí0.01
0
0.01
N  = 20
‚àí1.0
‚àí0.5
0
0.5
1.0
‚àí8
‚àí6
‚àí4
‚àí2
0
2
x 10
‚àí3
N  = 40
‚àí1.0
‚àí0.5
0
0.5
1.0
‚àí20
‚àí15
‚àí10
‚àí5
0
5
x 10
‚àí4
N  = 80
‚àí1.0
‚àí0.5
0
0.5
1.0
‚àí4
‚àí2
0
2
x 10
‚àí4
N  = 160
Figure 10.2
The same as Fig. 10.1, except that now we approximate the peri-
odic function e‚àícos œÄx.
Since such a function f is Lipschitz in (‚àí1, 1), we deduce that œïN converges to f there.
(It follows from standard theorems of calculus that this convergence is uniform in every
closed subinterval.) However, we can guarantee the convergence of O

N ‚àí1
, though
this is very slow.
Even more importantly, there is nothing to make œïN converge
to f at the endpoints. As a matter of fact, it is possible to show that œïN(¬±1) ‚Üí
1
2[f(‚àí1)+f(1)]: unless f is periodic, we fail to converge to the correct function values
at the endpoints. (Not a great surprise, since œïN itself is periodic.) This implies,
in addition, that the error is likely to be unacceptably large near ¬±1, where the
approximation oscillates wildly, a phenomenon known as the Gibbs eÔ¨Äect.
All this is vividly illustrated by Fig. 10.3. Note that we have plotted the error only
in the subinterval [‚àí9
10, 9
10], since œïN(¬±1) ‚Üícosh 1, an altogether wrong value. But
even in the open interval (‚àí1, 1) the news is not good: the convergence of O

N ‚àí1
is
excruciatingly slow. Doubling the number of points increases the accuracy barely by
a factor of 2.
We thus approach our second function, e‚àícos œÄx, with very modest expectations.
Yet, even brief examination of Fig. 10.4 (where again we plot in [‚àí1, 1]) reveals some-
thing truly amazing. Taking N = 10 gives an accuracy comparable to N = 160 with
chapeau functions (compare Fig. 10.2). Doubling N results in ten signiÔ¨Åcant digits,
while for N = 30 we have exhausted the accuracy of MATLAB computer arithmetic
and the plot displays randomness, a tell-tale sign of roundoÔ¨Äerror. All this is true
not just inside the interval but also on the boundary.
This is precisely the rapid
convergence that we have sought!

208
Spectral methods
‚àí0.5
0
0.5
‚àí0.2
‚àí0.1
0
0.1
0.2
N  = 20
‚àí0.5
0
0.5
‚àí0.10
‚àí0.05
0
0.05
0.10
N  = 40
‚àí0.5
0
0.5
‚àí0.04
‚àí0.02
0
0.02
0.04
N  = 80
‚àí0.5
0
0.5
‚àí0.02
‚àí0.01
0
0.01
0.02
N  = 160
Figure 10.3
The error in approximating the function e‚àíx in [‚àí9
10, 9
10] by Fourier
expansion with N terms.
So what makes the Fourier series approximation to e‚àícos œÄx so eÔ¨Äective in compar-
ison to e‚àíx? The brief answer is periodicity. In general, suppose that f is an analytic
function in [‚àí1, 1] that can be extended analytically to a closed complex domain ‚Ñ¶
such that [‚àí1, 1] ‚äÇ‚Ñ¶and to its boundary. In addition, we stipulate that f is periodic
with period 2. Therefore f (m)(‚àí1) = f (m)(1) for all m = 0, 1, . . . We again integrate
by parts, but this time we do not stop with f ‚Ä≤:
ÀÜfn = ‚àí
1
2œÄin
=f ‚Ä≤n =
	
‚àí
1
2œÄin

2
;
f ‚Ä≤‚Ä≤n =
	
‚àí
1
2œÄin

2
;
f ‚Ä≤‚Ä≤‚Ä≤n = ¬∑ ¬∑ ¬∑ .
We thus have
ÀÜfn =
	
‚àí
1
2œÄin

m
<
f (m)n,
m = 0, 1, . . .
(10.2)
How large is | <
f (m)n|? Letting Œ≥ be the positively oriented boundary of ‚Ñ¶and denoting
by Œ±‚àí1 > 0 the minimal distance betweeen Œ≥ and [‚àí1, 1], the Cauchy theorem of
complex analysis states that
f (m)(x) = m!
2œÄi

Œ≥
f(z) dz
(z ‚àím)m+1 ,
x ‚àà[‚àí1, 1] :
therefore, letting Œ∫ = max{|f(z)| : z ‚ààŒ≥} < ‚àû,
|f (m)(x)| ‚â§m!
2œÄ

Œ≥
|f(z)| | dz|
|z ‚àíx|m+1 ‚â§Œ∫ length Œ≥
2œÄ
m! Œ±m+1.

10.1
Sparse matrices vs. small matrices
209
‚àí1.0
‚àí0.5
0
0.5
1.0
‚àí4
‚àí2
0
2
4 x 10
‚àí4
N  = 10
‚àí1.0
‚àí0.5
0
0.5
1.0
‚àí3
‚àí2
‚àí1
0
1
2
3
x 10
‚àí10
N  = 20
‚àí1.0
‚àí0.5
0
0.5
1.0
‚àí2
‚àí1
0
1
2
3
4 x 10
‚àí15
N  = 30
Figure 10.4
The same as Fig. 10.3, except that now we approximate the peri-
odic function e‚àícos œÄx and employ smaller values of N.
It follows that we can bound | <
f (m)n| ‚â§cm!Œ±m, m = 0, 1, . . . , for some c > 0. Conse-
quently, using (10.2) and the above upper bound of | <
f (m)n|,
|œïN(x) ‚àíf(x)| =
œïN(x) ‚àí
‚àû

n=‚àí‚àû
ÀÜfneiœÄnx
 ‚â§
‚àíN/2

n=‚àí‚àû
| ÀÜfn| +
‚àû

n=N/2+1
| ÀÜfn|
=
‚àíN/2

n=‚àí‚àû
| <
f (m)n|
(‚àí2œÄn)m +
‚àû

n=N/2+1
| <
f (m)n|
(2œÄn)m
‚â§cm!Œ±m
(2œÄ)m
‚é°
‚é£
1
(N/2)m + 2
‚àû

n=N/2+1
1
nm
‚é§
‚é¶.
However, for any r = 1, 2, . . . ,
‚àû

n=r
1
nm ‚â§
 ‚àû
r
dœÑ
œÑ m =
1
m ‚àí1r‚àím+1,
and we deduce that
|œïN(x) ‚àíf(x)| ‚â§cm!
 Œ±
2œÄ
m 
1
(N/2)m +
2
m ‚àí1
1
(N/2 + 1)m

‚â§3cm!
 Œ±
œÄN
m
.
(10.3)

210
Spectral methods
We have a competition: while Œ±/(œÄN) can be made as small as we want for large N,
so that [Œ±/(œÄN)]m approaches zero fast when m grows, factorial m! rapidly becomes
large in these circumstances. Fortunately, this is one contest which the good guys win.
According to the well-known Stirling formula,
m! ‚âà
‚àö
2œÄmm+1/2e‚àím,
we have
m!
 Œ±
œÄN
m
‚âà
‚àö
2œÄm
 Œ±m
œÄeN
m
and the latter becomes very small for large N. It thus follows from (10.3) that the
error |œïN ‚àíf| decays pointwise in [‚àí1, 1] faster than O(N ‚àíp) for any p = 1, 2, . . .
Since, in our setting, a rate of convergence of O(N ‚àíp) corresponds to order p,
we deduce that the Fourier approximation of analytic periodic functions is of inÔ¨Ånite
order. Such very rapid convergence deserves a name: we say that œïN tends to f at
spectral speed.
As a matter of fact, it is possible to prove that there exist c1, œâ > 0 such that
|œïN(x)‚àíf(x)| ‚â§c1e‚àíœâN for all N = 0, 1, . . ., uniformly in [‚àí1, 1]. Thus, convergence
is at least at an exponential rate, and this explains the extraordinary accuracy evident
in Fig. 10.4.
1.29
1.27
1.25
1.31
1.30
1.28
1.26
100
90
80
70
60
50
40
30
20
10
Figure 10.5
Scaled logarithm of Fourier coeÔ¨Écients ‚àí(log | ÀÜfn|)/n for n
=
1, 2, . . . , 100 and the periodic function f(x) = (1 + 1
2 cos œÄx)‚àí1.
As we have seen, spectral convergence is all about the fast decay of Fourier coeÔ¨É-
cients. In Fig. 10.5 we illustrate this with the function f(x) = (1 + 1
2 cos œÄx)‚àí1. It is
easy to check that it is indeed periodic and that it can be extended analytically away
from [‚àí1, 1], its nearest singularities residing at ¬± log(2 +
‚àö
3)/(œÄi). The coeÔ¨Écients
(we need to compute ÀÜfn only for n ‚â•0, since f is even and ÀÜf‚àín = ÀÜfn) decay very fast:
ÀÜf100 ‚âà7.37 √ó 10‚àí58 (and we needed more than 120 signiÔ¨Åcant digits to compute this!)
and the plot indicates that ÀÜfn ‚âàe‚àí1.32n.

10.2
The algebra of Fourier expansions
211
We have accumulated enough evidence to make the case that Fourier expansions
converge exceedingly fast for periodic functions. The challenge now is to utilize this
behaviour in the design of discretization methods that lead to relatively small linear
algebraic systems.
10.2
The algebra of Fourier expansions
We denote by A the set of all complex-valued functions f that are analytic in [‚àí1, 1],
are periodic there with period 2, and can be extended analytically into the complex
plane; such functions, as we saw in the last section, have rapidly convergent Fourier
expansions.
What sort of animal is A?
It is a linear space: if f, g ‚ààA and a ‚ààC then
f + g, af ‚ààA. Moreover, identifying functions in A with their (convergent) Fourier
expansion, given by
f(x) =
‚àû

n=‚àí‚àû
ÀÜfneiœÄnx,
g(x) =
‚àû

n=‚àí‚àû
ÀÜgneiœÄnx,
implies that
f(x) + g(x) =
‚àû

n=‚àí‚àû
( ÀÜfn + ÀÜgn)eiœÄnx,
af(x) =
‚àû

n=‚àí‚àû
a ÀÜfneiœÄnx.
(10.4)
Thus, the algebra of A can be easily expressed in terms of Fourier coeÔ¨Écients. More-
over, simple calculation conÔ¨Årms that A is also closed with regard to multiplication:
f(x)g(x) =
‚àû

n=‚àí‚àû

‚àû

m=‚àí‚àû
ÀÜfn‚àímÀÜgm

eiœÄnx.
(10.5)
The inner convergent inÔ¨Ånite sum above is called the convolution of the complex
sequences ÀÜf = { ÀÜfn} and ÀÜg = {ÀÜgn} and is written as
ÀÜf ‚àóÀÜg = ÀÜh,
where
ÀÜhn =
‚àû

m=‚àí‚àû
ÀÜfn‚àímÀÜgm,
n ‚ààZ.
(10.6)
Therefore f(x)g(x) = (f ‚àóg)(x), where the relationship between the Fourier series is
expressed at the level of functions by h = f ‚àóg.
Our calculus with Fourier series requires, in the context of the numerical analysis
of diÔ¨Äerential equations, a means of diÔ¨Äerentiating functions. This is fairly straight-
forward: f ‚ààA implies that f ‚Ä≤ ‚ààA and
f ‚Ä≤(x) = iœÄ
‚àû

n=‚àí‚àû
n ÀÜfneiœÄn.
All this extends to higher derivatives.
This is the moment to recall that, in our
setting, the sequence { ÀÜfn} decays faster than O(n‚àíp) for p ‚ààZ+, and this provides

212
Spectral methods
an alternative demonstration that all derivatives of f have rapidly convergent Fourier
expansions.
3 A simple spectral method
How does all this help in our quest to compute
diÔ¨Äerential equations? Consider the two-point boundary value problem
y‚Ä≤‚Ä≤ + a(x)y‚Ä≤ + b(x)y = f(x),
‚àí1 ‚â§x ‚â§1,
y(‚àí1) = y(1),
(10.7)
where a, b, f ‚ààA.
Substituting Fourier expansions and using (10.4) and
(10.5), we obtain an inÔ¨Ånite-dimensional system of linear algebraic equations
‚àíœÄ2n2ÀÜyn + iœÄ
‚àû

m=‚àí‚àû
mÀÜan‚àímÀÜym +
‚àû

m=‚àí‚àû
ÀÜbn‚àímÀÜym = ÀÜfn,
n ‚ààZ,
(10.8)
for the unknowns ÀÜy. Knowing that the sequences ÀÜa, ÀÜb, ÀÜf decay at spectral
speed, we can truncate (10.8) into the N-dimensional system
‚àíœÄ2n2ÀÜyn + iœÄ
N/2

m=‚àíN/2+1
mÀÜan‚àímÀÜym +
N/2

m=‚àíN/2+1
ÀÜbn‚àímÀÜym = ÀÜfn
for
n = ‚àíN/2 + 1, . . . , N/2.
(10.9)
In the language of signal processing, we are approximating the solution by a
band-limited function, one that can be described as a linear combination of a
Ô¨Ånite number of Fourier coeÔ¨Écients.
Note that, to avoid a needless clutter of notation, we denote both the exact
Fourier coeÔ¨Écients in (10.8) and their N-dimensional approximation in (10.9)
by ÀÜyn. This should not cause any confusion.
The matrix of system (10.9) is in general dense, but our theory predicts that
fairly small values of N, hence very small matrices, are suÔ¨Écient for high
accuracy.
By a way of example, we will choose a(x) = f(x) = cos œÄx and b(x) = sin 2œÄx.
This, incidentally, leads to a sparse matrix, because a and b contain just two
nonzero Fourier harmonics each. Yet, the purpose of this example is not to
investigate matrices but to illustrate the rate of convergence. Fig. 10.6 shows
that N = 16 yields an accuracy of more than ten digits, while for N = 22
we have already hit the buÔ¨Äers of computer arithmetic and roundoÔ¨Äerror.
Needless to say, the direct solution of a 22 √ó 22 linear algebraic system with
Gaussian elimination is so fast that it is pointless to seek an alternative.
3
In the last example we were prescient enough to choose functions a, b and f with
known Fourier coeÔ¨Écients. This is not the case for most realistic scenarios and, if we
really expect spectral methods for diÔ¨Äerential equations to be a serious competitor to
Ô¨Ånite diÔ¨Äerences and Ô¨Ånite elements, we must have an eÔ¨Äective means of computing
ÀÜfn for n = ‚àíN/2 + 1, . . . , N/2.
Fortunately, Fourier coeÔ¨Écients can be computed very accurately indeed with re-
markably small computational cost. In general, suppose that h ‚ààA and we wish to

10.2
The algebra of Fourier expansions
213
‚àí1.0
‚àí0.8
‚àí0.6
‚àí0.4
‚àí0.2
0
0.2
0.4
0.6
0.8
1.0
‚àí6
‚àí4
‚àí2
0
2
4
6
x 10
11
N = 16
‚àí1.0
‚àí0.8
‚àí0.6
‚àí0.4
‚àí0.2
0
0.2
0.4
0.6
0.8
1.0
‚àí1.0
‚àí0.5
0
0.5
1.0
x 10
15
N = 22
Figure 10.6
The error in solving (10.7) with a(x) = f(x) = cos œÄx and
b(x) = sin 2œÄx using the spectral method (10.9) with N = 16 and N = 22 coeÔ¨Écients
respectively.
compute its integral in [‚àí1, 1]. We do so by means of the deceptively simple Riemann
sum
 1
‚àí1
h(œÑ) dœÑ ‚âà2
N
N/2

k=‚àíN/2+1
h
	2k
N

.
(10.10)
Let
œâN = e2œÄi/N
be the Nth primitive root of unity. Substituting its Fourier expansion into (10.10) in
place of h, we obtain
2
N
N/2

k=‚àíN/2+1
h
	2k
N

= 2
N
N/2

k=‚àíN/2+1
‚àû

n=‚àí‚àû
ÀÜhne2œÄink/N
= 2
N
‚àû

n=‚àí‚àû
ÀÜhn
N/2

k=‚àíN/2+1
œânk
N = 2
N
‚àû

n=‚àí‚àû
ÀÜhn
N‚àí1

k=0
œân(k+1‚àíN/2)
N
= 2
N
‚àû

n=‚àí‚àû
ÀÜhnœâ‚àín(N/2‚àí1)
N
N‚àí1

k=0
œânk
N .

214
Spectral methods
Since œâN
N = 1, it follows at once by summing the geometric series that
N‚àí1

k=0
œâkn
N =
1
N,
n ‚â°0
(mod N),
0,
n Ã∏‚â°0
(mod N).
Moreover, if n ‚â°0 (mod N), i.e. n is an integer multiple of N, then necessarily
œâ‚àín(N/2‚àí1)
N
= 1 (recall the deÔ¨Åniton of œâN and that N is even). We thus deduce that
2
N
N/2

k=‚àíN/2+1
h
	2k
N

= 2
‚àû

r=‚àí‚àû
ÀÜhNr;
consequently the error of (10.10) is
2
N
N/2

k=‚àíN/2+1
h
	2k
N

‚àí
 1
‚àí1
h(œÑ) dœÑ =
‚àû

r=1
(ÀÜhNr + ÀÜhNr).
We now recall that h ‚ààA, hence its Fourier coeÔ¨Écients decay at a spectral rate, and
deduce that the error of (10.10) also decays spectrally as a function of N.
In particular, letting h(x) = f(x)e‚àíiœÄmx, we obtain a spectral method for the
calculation of Fourier coeÔ¨Écients. SpeciÔ¨Åcally, given that we wish to evaluate the N
coeÔ¨Écients ÀÜfn, n = ‚àíN/2 + 1, . . . , N/2, say, we need to compute
ÀÜfn ‚âà2
N
N/2

k=‚àíN/2+1
f
	2k
N

œâ‚àínk
N
,
n = ‚àíN/2 + 1, . . . , N/2.
(10.11)
To calculate (10.11) we Ô¨Årst evaluate h at N equidistant points on a grid in [‚àí1, 1] and
then multiply the outcome with a matrix whose elements are (2/N)œâ‚àínk
N
. On the face
of it, such a multiplication requires O

N 2
operations. However, the special structure
of this matrix lends itself to perhaps the most remarkable computational algorithm
ever, the fast Fourier transform (FFT), the subject of our next section. The outcome
is a method that computes the leading N Fourier coeÔ¨Écients (up to spectrally small
error) in just O(N log2 N) operations.
10.3
The fast Fourier transform
Let N be a positive integer and denote by Œ†N the set of all complex sequences x =
{xj}‚àû
j=‚àí‚àûwhich are periodic with period N, i.e., which are such that xj+N = xj,
j ‚ààZ. It is an easy matter to demonstrate that Œ†N is a linear space of dimension N
over the complex numbers C (see Exercise 10.3).
Recall that œâN = exp (2œÄi/N) stands for the primitive root of unity of degree N.
A discrete Fourier transform (DFT) is a linear mapping FN deÔ¨Åned for every x ‚ààŒ†N
by
y = FNx
where
yj = 1
N
N‚àí1

‚Ñì=0
œâ‚àíj‚Ñì
N x‚Ñì,
j ‚ààZ.
(10.12)

10.3
The fast Fourier transform
215
Lemma 10.2
The DFT FN, as deÔ¨Åned in (10.12), maps Œ†N into itself. The map-
ping is invertible and
x = F‚àí1
N y
where
x‚Ñì=
N‚àí1

j=0
œâj‚Ñì
N yj,
‚Ñì‚ààZ.
(10.13)
Moreover, FN is an isomorphism of Œ†N onto itself (A.1.4.2, A.2.1.9).
Proof
Since œâN is a root of unity, it is true that œâN
N = œâ‚àíN
N
= 1. Therefore it
follows from (10.12) that
yj+N = 1
N
N‚àí1

‚Ñì=0
œâ‚àí(j+N)‚Ñì
N
x‚Ñì= 1
N
N‚àí1

‚Ñì=0
œâ‚àíj‚Ñì
N x‚Ñì= yj,
j ‚ààZ,
and we deduce that y ‚ààŒ†N. Therefore FN indeed maps elements of Œ†N into elements
of Œ†N.
To prove the stipulated form of the inverse, we denote w := FNx, where x was
deÔ¨Åned in (10.13). Our Ô¨Årst observation is that if y ‚ààŒ†N then it is also true that
x ‚ààŒ†N (just change the minus to a plus in the above proof). Moreover, also changing
the order of summation,
wm = 1
N
N‚àí1

‚Ñì=0
œâ‚àím‚Ñì
N
x‚Ñì= 1
N
N‚àí1

‚Ñì=0
œâ‚àím‚Ñì
N
‚éõ
‚éù
N‚àí1

j=0
œâj‚Ñì
N yj
‚éû
‚é†
= 1
N
N‚àí1

j=0
N‚àí1

‚Ñì=0
œâ(j‚àím)‚Ñì
N

yj,
m ‚ààZ.
Within the parentheses is a geometric series that can be summed explicitly. If j Ã∏= m
then we have
N‚àí1

‚Ñì=0
œâ(j‚àím)‚Ñì
N
= 1 ‚àíœâ(j‚àím)N
N
1 ‚àíœâj‚àím
N
= 0,
because œâsN
N
= 1 for every s ‚ààZ \ {0}, whereas in the case j = m we obtain
N‚àí1

‚Ñì=0
œâ(j‚àím)N
N
=
N‚àí1

‚Ñì=0
1 = N.
We thus conclude that wm = ym, m ‚ààZ, hence that w = y. Therefore, (10.13) indeed
describes the inverse of FN.
The existence of an inverse for every x ‚ààŒ†N shows that FN is an isomorphism. To
conclude the proof and demonstrate that this DFT maps Œ†N onto itself, we suppose
that there exists a Àúy ‚ààŒ†N that cannot be the destination of FNx for any x ‚ààŒ†N.
Let us deÔ¨Åne Àúx in terms of (10.13). Then, as we have already observed, Àúx ‚ààŒ†N
and it follows from our proof that Àúy = FN Àúx. Therefore Àúy is in the range of FN, in

216
Spectral methods
contradiction to our assumption, and we conclude that the DFT FN is, indeed, an
isomorphism of Œ†N onto itself.
It is of interest to mention an alternative proof that FN is onto. According to a
classical theorem from linear algebra, a linear mapping T from a Ô¨Ånite-dimensional
linear space V to itself is onto if and only if its kernel ker T consists just of the zero
element of the space (ker T is the set of all w ‚ààV such that Tw = 0; see A.1.4.2).
Letting x ‚ààker FN, (10.12) yields
N‚àí1

‚Ñì=0
œâ‚àíj‚Ñì
N x‚Ñì= 0,
j = 0, 1, . . . , N ‚àí1.
(10.14)
This is a homogeneous linear system of N equations in the N unknowns x0, . . . , xN‚àí1.
Its matrix is a Vandermonde matrix (A.1.2.5) and it is easy to prove that its deter-
minant satisÔ¨Åes
det
‚é°
‚é¢‚é¢‚é¢‚é¢‚é£
1
œâ‚àí1
N
¬∑ ¬∑ ¬∑
œâ‚àí(N‚àí1)
N
1
œâ‚àí2
N
¬∑ ¬∑ ¬∑
œâ‚àí2(N‚àí1)
N
...
...
...
1
œâ‚àíN
N
¬∑ ¬∑ ¬∑
œâ‚àíN(N‚àí1)
N
‚é§
‚é•‚é•‚é•‚é•‚é¶
=
N

‚Ñì=1
‚Ñì‚àí1

j=0
(œâ‚àí‚Ñì
N ‚àíœâ‚àíj
N ) Ã∏= 0.
Therefore the only possible solution of (10.14) is x0 = x1 = ¬∑ ¬∑ ¬∑ = xN‚àí1 = 0 and we
deduce that ker FN = 0 and the mapping is onto Œ†N.
3 Applications of the DFT
It is diÔ¨Écult to overstate the importance of the
discrete Fourier transform in a multitude of applications ranging from numer-
ical analysis to control theory, from computer science to coding theory signal
processing, time series analysis . . . Later, both in this chapter and in Chap-
ter 15, we will employ it to provide a fast solution to discretized diÔ¨Äerential
equations.
We commence with the issue that motivated us at the Ô¨Årst place, the compu-
tation of Fourier coeÔ¨Écients. Note that œânN/2
N
= (‚àí1)n (verify!) implies that,
in (10.11),
ÀÜfn = 2(‚àí1)nœâ‚àín
N √ó 1
N
N

‚Ñì=0
f
	2‚Ñì+ 2 ‚àíN
N

œâ‚àín‚Ñì.
Since f is periodic, this immediately establishes a connection between the
approximation of Fourier coeÔ¨Écients in (10.11) and the DFT.
This can be extended without diÔ¨Éculty to functions f deÔ¨Åned in the interval
[a, b], where a < b, that are periodic and of period b‚àía. The Fourier transform
of f is the sequence { ÀÜfn}‚àû
n=‚àí‚àû, where
ÀÜfn =
1
b ‚àía
 b
a
f(œÑ) exp
	
‚àí2iœÄnœÑ
b ‚àía

dœÑ,
n ‚ààZ.
(10.15)

10.3
The fast Fourier transform
217
Fourier transforms feature in numerous branches of mathematical analysis
and its applications: the library shelf labelled ‚Äòharmonic analysis‚Äô is, to a very
large extent, devoted to Fourier transforms and their ramiÔ¨Åcations. More to
the point, as far as the subject matter of this book is concerned they are
crucial in the stability analysis of numerical methods for PDEs of evolution
(see Chapters 15 and 16).
The computation of Fourier transforms is not the only application of the DFT,
although arguably it is the most important. Other applications include the
computation of conformal mappings, interpolation by trigonometric polyno-
mials, the incomplete factoring of polynomials, the fast multiplication of large
integers . . . In Chapter 15 we utilize it in the solution of specially structured
linear algebraic systems that occur in methods for the calculation of the Pois-
son equation with Dirichlet boundary conditions in a square.
3
On the face of it, the evaluation of the DFT (10.12) (or of its inverse) requires O

N 2
operations since, owing to periodicity, it is obtained by multiplying a vector in CN by a
N √ó N complex matrix. It is one of the great wonders of computational mathematics,
however, that this operation count can be reduced a very great deal. Let us assume for
simplicity that N = 2n, where n is a nonnegative integer. It is convenient to replace
FN by the mapping F‚àó
N := NFN; clearly, if we can compute F‚àó
Nx cheaply then just
O(N) operations will convert the result to FNx.
Let us deÔ¨Åne, for every x ‚ààŒ†N, ‚Äòeven‚Äô and ‚Äòodd‚Äô sequences
x[e] := {x2j}‚àû
j=‚àí‚àû
and
x[o] := {x2j+1}‚àû
j=‚àí‚àû.
Since x[e], x[o] ‚ààŒ†N/2, we can make the mappings
y[e] = F‚àó
N/2x[e]
and
y[o] = F‚àó
N/2x[o].
Let y = F‚àó
Nx. Then, by (10.12),
yj =
N‚àí1

‚Ñì=0
œâ‚àíj‚Ñì
N x‚Ñì=
2n‚àí1

‚Ñì=0
œâ‚àíj‚Ñì
2n x‚Ñì
=
2n‚àí1‚àí1

‚Ñì=0
œâ‚àí2j‚Ñì
2n
x2‚Ñì+
2n‚àí1‚àí1

‚Ñì=0
œâ‚àíj(2‚Ñì+1)
2n
x2‚Ñì+1,
j = 0, 1, . . . , 2n ‚àí1.
However,
œâ2s
2n = œâs
2n‚àí1,
s ‚ààZ;
therefore
yj =
2n‚àí1‚àí1

j=0
œâ‚àíj‚Ñì
2n‚àí1x2‚Ñì+ œâ‚àíj
2n
2n‚àí1‚àí1

j=0
œâ‚àíj‚Ñì
2n‚àí1x2‚Ñì+1
= y[e]
j + œâ‚àíj
2n y[o]
j ,
j = 0, 1, . . . , 2n ‚àí1.
(10.16)

218
Spectral methods
In other words, provided that y[e] and y[o] are already known, we can synthesize them
into y in O(N) operations.
Incidentally ‚Äì and before proceeding any further ‚Äì we observe that the number of
operations can be reduced signiÔ¨Åcantly by exploiting the identity œâ‚àís
2s = ‚àí1, s ‚â•1.
Hence, (10.16) yields
yj = y[e]
j + œâ‚àíj
2n y[o]
j ,
yj+2n‚àí1 = y[e]
j+2n‚àí1 + œâ‚àíj‚àí2n‚àí1
2n
y[o]
j+2n‚àí1 = y[e]
j ‚àíœâ‚àíj
2n y[o]
j ,
j = 0, 1, . . . , 2n‚àí1 ‚àí1
(recall that y[e], y[o] ‚ààŒ†2n‚àí1, therefore y[e]
j+2n‚àí1 = y[e]
j etc.). In other words, to combine
y[e] and y[o] we need to form just 2n‚àí1 products œâ‚àíj
2n‚àí1y[o]
j , subsequently adding or
subtracting them, as required, from y[e]
j
for j = 0, 1, . . . , 2n‚àí1.
All this, needless to say, is based on the premise that y[e] and y[o] are known,
which, as things stand, is false. Having said this, we can form, in a similar fashion to
that above, y[e] from F‚àó
N/4x[ee], F‚àó
N/4x[eo] ‚ààŒ†N/4, where
x[ee] = {x4j}‚àû
j=‚àí‚àû,
x[eo] = {x4j+2}‚àû
j=‚àí‚àû.
Likewise, y[o] can be obtained from two transforms of length N/4. This procedure can
be iterated until we reach transforms of unit length, which, of course, are the variables
themselves.
Practical implementation of this procedure, the famous fast Fourier transform
(FFT), proceeds from the other end: we commence from 2n transforms of length 1
and synthesize them into 2n‚àí1 transforms of length 2. These are, in turn, combined
into 2n‚àí2 transforms of length 22, then into 2n‚àí3 transforms of length 23 and so on,
until we reach a single transform of length 2n, the object of this whole exercise.
Assembling 2n‚àís+1 transforms of length 2s‚àí1 into 2n‚àís transforms of double the
length costs O(2n‚àís √ó 2s) = O(N) operations. Since there are n such ‚Äòlayers‚Äô, the
total expense of the FFT is a multiple of 2nn = N log2 N operations.
For large
values of N this results in a very signiÔ¨Åcant saving in comparison with naive matrix
multiplication.
The order of assembly of one set of transforms into new transforms of twice the
length is important ‚Äì we do not just combine any two arbitrary ‚Äòstrands‚Äô! The correct
arrangement is displayed in Fig. 10.7 in the case n = 4 and the general rule is obvious.
It can be best understood by expressing the index ‚Ñìin a binary representation, but
we choose not to dwell further on this.
It is elementary to generalize the DFT (10.12) to two (or more) dimensions. Thus,
let {xk,j}‚àû
k,j=‚àí‚àûbe N-periodic in each index, xk+N,j = xk,j = xk,j+N for k, j ‚ààZ.
We set
y = FNx
where
yk,j =
1
N 2
N‚àí1

‚Ñì=0
N‚àí1

m=0
œâ‚àí(k‚Ñì+jm)
N
x‚Ñì,m,
k, j ‚ààZ.
The FFT can be extended to this case by acting separately on each index. This leads
to an algorithm bearing the price tag of O

N 2 log2 N

operations.

10.4
Second-order elliptic PDEs
219
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
length 1
length 2
length 4
length 8
length 16
Figure 10.7
The assembly pattern in FFT for N = 16 = 24.
10.4
Second-order elliptic PDEs
Spectral methods owe their success to the conÔ¨Çuence of three bits of mathematical
magic:
‚Ä¢ the spectral convergence of Fourier expansions of analytic periodic functions;
‚Ä¢ the spectral convergence of a DFT approximation to Fourier coeÔ¨Écients of ana-
lytic periodic functions; and
‚Ä¢ the low-cost calculation of a DFT by the fast Fourier transform.
In the previous two chapters we were concerned with solving the Poisson equation
‚àá2u = f in a square, and it is tempting to test a spectral method in this case. However,
as we will see, this would not be very representative, since the special structure of a
Poisson equation with periodic boundary conditions confers an unfair advantage on
spectral methods.
SpeciÔ¨Åcally, consider the Poisson equation
‚àá2u = f,
‚àí1 ‚â§x, y ‚â§1,
(10.17)
where the analytic function f obeys the periodic boundary conditons f(‚àí1, y) =
f(1, y), ‚àí1 ‚â§y ‚â§1 and f(x, ‚àí1) = f(x, 1), ‚àí1 ‚â§x ‚â§1. We equip (10.17) with the
periodic boundary conditions
u(‚àí1, y) = u(1, y),
ux(‚àí1, y) = ux(1, y),
‚àí1 ‚â§y ‚â§1,
u(x, ‚àí1) = u(x, 1),
uy(x, ‚àí1) = uy(x, 1),
‚àí1 ‚â§x ‚â§1,
(10.18)
but a moment‚Äôs reÔ¨Çection demonstrates that they deÔ¨Åne the solution of (10.17) only
up to an additive constant: if u(x, y) solves this equation and obeys periodic boundary

220
Spectral methods
conditions, then so does u(x, y) + c for any c ‚ààR. We thus need another condition to
pin the constant c down and so stipulate that
 1
‚àí1
 1
‚àí1
u(x, y) dx dy = 0.
(10.19)
Note from (10.18) that, integrating the equation (10.17), we can prove easily that the
forcing term f must also obey the normalization condition (10.19).
We have the two-dimensional spectrally convergent Fourier expansion
f(x, y) =
‚àû

k=‚àí‚àû
‚àû

‚Ñì=‚àí‚àû
ÀÜfk,‚ÑìeiœÄ(kx+‚Ñìy)
and seek the Fourier expansion of u,
u(x, y) =
‚àû

k=‚àí‚àû
‚àû

‚Ñì=‚àí‚àû
ÀÜuk,‚ÑìeiœÄ(kx+‚Ñìy),
whose existence is justiÔ¨Åed by the periodic boundary conditions (10.18). Note that
the normalization condition (10.19) amounts to ÀÜu0,0 = 0. Therefore
‚àá2u(x, y) = ‚àíœÄ2
‚àû

k=‚àí‚àû
‚àû

‚Ñì=‚àí‚àû
(k2 + ‚Ñì2)ÀÜuk,‚ÑìeiœÄ(kx+‚Ñìy),
together with (10.17), implies at once that
ÀÜuk,‚Ñì= ‚àí
1
(k2 + ‚Ñì2)œÄ2 ÀÜfk,‚Ñì,
k, ‚Ñì‚ààZ,
(k, ‚Ñì) Ã∏= (0, 0).
We have obtained the Fourier coeÔ¨Écients of the solution in an explicit form, without
any need to solve linear algebraic equations. The explanation is simple: what we have
done is to recreate in a numerical setting the familiar technique of the separation of
variables. The trigonometric functions œïk,‚Ñì(x, y) = eiœÄ(kx+‚Ñìy) are eigenfunctions of
the Laplace operator, ‚àá2œïk,‚Ñì= ‚àíœÄ2(k2 + ‚Ñì2)œïk,‚Ñì, and they obey periodic boundary
conditions.
A fairer impression is gained by considering a case where spectral methods do not
enjoy an unfair advantage. Therefore, let us examine the second-order linear elliptic
PDE,
‚àá¬∑ (a‚àáu) = f,
‚àí1 ‚â§x, y ‚â§1,
(10.20)
where the positive analytic function a is periodic, as is the forcing term f. We again im-
pose the periodic boundary conditions (10.18) and the normalization condition (10.19).
Writing
‚àá¬∑ (a‚àáu) = a‚àá2u + axux + ayuy,

10.5
Chebyshev methods
221
we thus have
‚àíœÄ2

‚àû

k=‚àí‚àû
‚àû

‚Ñì=‚àí‚àû
ÀÜak,‚ÑìeiœÄ(kx+‚Ñìy)
 
‚àû

k=‚àí‚àû
‚àû

‚Ñì=‚àí‚àû
(k2 + ‚Ñì2)ÀÜuk,‚ÑìeiœÄ(kx+‚Ñìy)

‚àíœÄ2

‚àû

k=‚àí‚àû
‚àû

‚Ñì=‚àí‚àû
kÀÜak,‚ÑìeiœÄ(kx+‚Ñìy)
 
‚àû

k=‚àí‚àû
‚àû

‚Ñì=‚àí‚àû
kÀÜuk,‚ÑìeiœÄ(kx+‚Ñìy)

‚àíœÄ2

‚àû

k=‚àí‚àû
‚àû

‚Ñì=‚àí‚àû
‚ÑìÀÜak,‚ÑìeiœÄ(kx+‚Ñìy)
 
‚àû

k=‚àí‚àû
‚àû

‚Ñì=‚àí‚àû
‚ÑìÀÜuk,‚ÑìeiœÄ(kx+‚Ñìy)

=
‚àû

k=‚àí‚àû
‚àû

‚Ñì=‚àí‚àû
ÀÜfk,‚ÑìeiœÄ(kx+‚Ñìy)
where
a(x, y) =
‚àû

k=‚àí‚àû
‚àû

‚Ñì=‚àí‚àû
ÀÜak,‚ÑìeiœÄ(kx+‚Ñìy).
Next we replace products by convolutions ‚Äì it is trivial to generalize (10.5) to the
bivariate case by replacing one summation by two. Finally, we truncate the inÔ¨Ånite-
dimensional system to ‚àíN/2+1 ‚â§k, ‚Ñì‚â§N/2 and impose the normalization condition
ÀÜu0,0 = 0. All this results in a system of N 2 ‚àí1 linear algebraic equations in the N 2 ‚àí1
unknowns uk,‚Ñì, k, ‚Ñì= ‚àíN/2 + 1, . . . , N/2, (k, ‚Ñì) Ã∏= (0, 0). Typically, such a system is
devoid of any useful structure and is not sparse. Yet its size is substantially smaller
than anything we would have obtained, expecting similar precision, by the methods
of Chapters 8 and 9.
This is, however, not the only means of constructing a spectral method for (10.20).
Since Lu = ‚àí‚àá¬∑ (a‚àáu) is a positive-deÔ¨Ånite operator with respect to the standard
Euclidean complex valued norm
‚ü®u, v‚ü©=
 1
‚àí1
 1
‚àí1
u(x, y)v(x, y) dx dy,
we can use the Ô¨Ånite element methods of Chapter 9 to construct a linear algebraic
system, except that instead of the Ô¨Ånite element basis (leading to a large, sparse
matrix) we use the spectral basis vk,‚Ñì(x, y) = eiœÄ(kx+‚Ñìy), ‚àíN/2 + 1 ‚â§k, ‚Ñì‚â§N/2
(leading to small dense matrix). In particular, integrating by parts and using periodic
boundary conditions, we have
‚ü®Lvk,‚Ñì, vm,j‚ü©=
 1
‚àí1
 1
‚àí1
a(‚àávk,‚Ñì) ¬∑ (‚àávm,j) dx dy
= œÄ2(km + ‚Ñìj)
 1
‚àí1
 1
‚àí1
a(x, y)eiœÄ[(k‚àím)x+(‚Ñì‚àíj)y] dx dy
= 4œÄ2(km + ‚Ñìj)ÀÜak‚àím,‚Ñì‚àíj.
(Note that there is no need to resort to the formalism of bilinear forms, since our
functions are smooth enough for the straightforward action of the operator L.) In a
similar way we obtain ‚ü®f, vk,‚Ñì‚ü©= 4 ÀÜfk,‚Ñì(verify!). Therefore, using an FFT the Ritz
equations (9.24) can be constructed fairly painlessly.

222
Spectral methods
10.5
Chebyshev methods
And now to the bad news . . . The eÔ¨Écacy of spectral methods depends completely
on the analyticity and periodicity of the underlying problem, inclusive of boundary
conditions and coeÔ¨Écients. Take away either and the rate of convergence drops to
polynomial.1 To obtain reasonable accuracy we then need a large number of variables
and hence end up with a large matrix system but, unlike in the cases of Ô¨Ånite diÔ¨Äer-
ences or Ô¨Ånite elements, the matrix is not sparse, so we have the worst of all worlds.
Relatively few problems originating in real applications are genuinely periodic and
this renders spectral methods of limited applicability: when they are good they are
very very good but when they are bad, they are horrid.2
Once we wish spectral methods to be available in a more general setting, we need
a framework that allows nonperiodic functions. This brings us to Chebyshev polyno-
mials.
We let Tn(x) = cos(n arccos x), n ‚â•0; therefore
T0(x) ‚â°1,
T1(x) = x,
T2(x) = 2x2 ‚àí1,
T3(x) = 4x3 ‚àí3x,
. . .
It is easy to verify (see Exercise 3.2) that each Tn is a polynomial of degree n: it is called
the nth Chebyshev polynomial (of the Ô¨Årst kind). Moreover, Chebyshev polynomials
are orthogonal with respect to the weight function (1 ‚àíx2)‚àí1/2 in (‚àí1, 1),
 1
‚àí1
Tm(x)Tn(x)
dx
‚àö
1 ‚àíx2 =
‚éß
‚é™
‚é®
‚é™
‚é©
œÄ,
m = n = 0,
1
2œÄ,
m = n ‚â•1,
0,
m Ã∏= n,
m, n ‚ààZ,
(10.21)
and they obey the three-term recurrence relation
Tn+1(x) = 2xTn(x) ‚àíTn‚àí1(x),
n = 1, 2, . . .
We consider the expansion of a general integrable function f in the orthogonal
sequence {Tn}‚àû
n=0:
f(x) =
‚àû

n=0
ÀòfnTn(x).
(10.22)
Multiplying (10.22) by Tm(x)(1 ‚àíx2)‚àí1/2, integrating for x ‚àà(‚àí1, 1) and using the
orthogonality conditions (10.21) results in
Àòf0 = 1
œÄ
 1
‚àí1
f(x)
dx
‚àö
1 ‚àíx2 ,
Àòfn = 2
œÄ
 1
‚àí1
f(x)Tn(x)
dx
‚àö
1 ‚àíx2 ,
n = 1, 2, . . .
1Not strictly true:
if the data is C‚àûrather than analytic then spectral convergence can be
recovered, a subject to which we will return before the end of this chapter. But in fact the diÔ¨Äerence
between analytic and C‚àûdata is mostly a matter of mathematical nicety, while piecewise-smooth
data is fairly popular in applications.
2Applied mathematics and engineering departments abound in researchers forcing periodic condi-
tions on their models, to render them amenable to spectral methods. This results in fast algorithms,
nice pictures, but arguably only tenuous relevance to applications.

10.5
Chebyshev methods
223
Letting x = cos Œ∏, a simple change of variables conÔ¨Årms that
 1
‚àí1
f(x)Tn(x)
dx
‚àö
1 ‚àíx2 =
 œÄ
0
f(cos Œ∏) cos nŒ∏ dŒ∏ = 1
2
 œÄ
‚àíœÄ
f(cos Œ∏) cos nŒ∏ dŒ∏.
Given that cos nŒ∏ = 1
2(einŒ∏ + e‚àíinŒ∏), the connection with Fourier expansions stands
out. SpeciÔ¨Åcally, letting g(x) = f(cos x) in place of f in (10.15), we have
ÀÜgn = 1
2œÄ
 œÄ
‚àíœÄ
g(œÑ)e‚àíinœÑ dœÑ,
n ‚ààZ.
Therefore
 1
‚àí1
f(x)Tn(x)
dx
‚àö
1 ‚àíx2 = œÄ
2 (ÀÜg‚àín + ÀÜgn)
and we deduce that
Àòf =
1
ÀÜg0,
n = 0,
ÀÜg‚àín + ÀÜgn,
n = 1, 2, . . . .
(10.23)
The computation of the expansion (10.22) is therefore equivalent to the Fourier
expansion of the function g. However, the latter is periodic with period 2œÄ; therefore
we can use a DFT to compute the Àòfn while enjoying all the beneÔ¨Åts of periodic func-
tions. In particular, if f can be extended analytically into an open neighbourhood of
[‚àí1, 1] then the error decays at a spectral rate. Moreover, thanks to (10.23), Cheby-
shev coeÔ¨Écients Àòfn also decay spectrally fast for n ‚â´1. Therefore we reap all the
beneÔ¨Åts of the rapid convergence of spectral methods without ever assuming that f is
periodic!
Before we can apply Chebyshev expansions (10.22) in spectral-like methods for
nonperiodic analytic functions, we must develop a toolbox for the algebraic and an-
alytic manipulation of these expansions, along the lines of Section 10.2. Thus, let B
denote the set of all analytic functions in [‚àí1, 1] that can be extended analytically
into the complex plane. We identify each such function with its Chebyshev expansion.
Like the set A, we see that B is a linear space and is closed under multiplication. To
derive an alternative to the convolution (10.5), we note that
Tm(x)Tn(x) = cos(m arccos x) cos(n arccos x)
= 1
2{cos[(m ‚àín) arccos x] + cos[(m + n) arccos x]}
= 1
2[T|m‚àín|(x) + Tm+n(x)].
Therefore, after elementary algebra, we obtain
f(x)g(x) =
‚àû

m=0
ÀòfmTm(x)
‚àû

n=0
ÀògnTn(x)
= 1
2
‚àû

m=0
‚àû

n=0
ÀòfmÀògn[T|m‚àín|(x) + Tm+n(x)]
= 1
2
‚àû

n=0
‚àû

m=0
Àòfm(Àòg|m‚àín| + Àògm+n)Tn(x).

224
Spectral methods
Finally, we need to express derivatives of functions in B as Chebyshev expansions.
The analogous task was easy in A, since eiœÄnx is an eigenfunction of the diÔ¨Äerential
operator. In B this is somewhat more complicated. We note, however, that T ‚Ä≤
n is a
polynomial of degree n‚àí1 and hence can be expressed in the basis {T0, T1, . . . , Tn‚àí1}.
Moreover, each Tn is of the same parity as n (that is, T2m is an even and T2m+1 an
odd function); therefore T ‚Ä≤
n is of opposite parity and the only surviving terms in the
linear combination are Tn‚àí1, Tn‚àí3, Tn‚àí5, . . .
Lemma 10.3
The derivatives of Chebyshev polynomials can be expressed explicitly
as the linear combinations
T ‚Ä≤
2n(x) = 4n
n‚àí1

‚Ñì=0
T2l+1(x),
(10.24)
T ‚Ä≤
2n+1(x) = (2n + 1)T0(x) + 2(2n + 1)
n

‚Ñì=1
T2‚Ñì(x).
(10.25)
Proof
We will prove only (10.24), since (10.25) follows by an identical argument.
Thus,
sin Œ∏ T ‚Ä≤
2n(cos Œ∏) = 2n sin 2nŒ∏
while, on telescoping series,
4n sin Œ∏
n‚àí1

‚Ñì=0
T2l+1(cos Œ∏) = 4n
n‚àí1

‚Ñì=0
sin Œ∏ cos(2‚Ñì+ 1)Œ∏
= 2n
n‚àí1

‚Ñì=0
[sin(2‚Ñì+ 2)Œ∏ ‚àísin 2‚ÑìŒ∏] = 2n sin 2nŒ∏.
This proves (10.24).
The recursions (10.24) and (10.25) can be used to express the derivative of any
term in B as a Chebyshev expansion. Moreover, they can be iterated in an obvious
way to express arbitrarily high derivatives in this form.
Chebyshev methods can be assembled exactly like standard spectral methods, using
the linearity of B and the product rules of Lemma 10.3. However, we must remem-
ber to translate correctly the computation of Chebyshev coeÔ¨Écients to the Fourier
realm. In order to use the FFT, we sample the function g at N equidistant points
in [‚àíœÄ, œÄ]. Back in [‚àí1, 1], the world of the original function values, this means com-
puting the function f at the Chebyshev points cos(2œÄk/N), k = ‚àíN/2 + 1, . . . , N/2.
(This, of course, can be translated linearly from [‚àí1, 1] into any other bounded inter-
val.) Likewise, in two dimensions we need to sample our functions on the Chebyshev
grid (cos(2œÄk/N), cos(2œÄ‚Ñì/N)), k, ‚Ñì= ‚àíN/2 + 1, ‚àíN/2 + 2, . . . , N/2, which has the

Comments and bibliography
225
following form (for N = 16):
rr
r
r
r
r
r
rr
rr
r
r
r
r
r
rr
rr
r
r
r
r
r
rr
rr
r
r
r
r
r
rr
rr
r
r
r
r
r
rr
rr
r
r
r
r
r
rr
rr
r
r
r
r
r
rr
rr
r
r
r
r
r
rr
rr
r
r
r
r
r
rr
Thus, the computed function values are denser toward the edges. Insofar as the com-
putation of elliptic problems, e.g. the Poisson equation and its generalization (10.20),
are concerned, this is not problematic. However (and we comment further upon this
below), sampling on the above grid can play havoc with numerical stability once we
apply Chebyshev methods to initial value PDEs.
Comments and bibliography
There are many good texts on Fourier expansions, spanning the full range from the purely
mathematical to the applied. For an introductory yet comprehensive exposition of the sub-
ject, painting on a broad canvass and conveying not just mathematical foundations but also
the beauty and excitement of Fourier analysis, one can hardly do better than K¬®orner (1988).
Within their own frame of reference ‚Äì everything analytic, everything periodic ‚Äì it is
diÔ¨Écult to improve upon Fourier expansions and spectral methods. The question is, how
much can we relax analyticity and periodicity while retaining the substantive advantages of
Fourier expansions?
We can salvage spectral convergence once analyticity is replaced by the requirement that
f ‚ààC‚àû(‚àí1, 1), in other words that f (m)(x) exists for all x ‚àà(‚àí1, 1) and m = 0, 1, 2 . . . For
example, in Fig. 10.8 we plot ‚àí(log | ÀÜfn|)/n0.44 for n = 1, 2, . . . , 100 and the function f(x) =
exp
‚àí1/(1 ‚àíx2)
. (Note that f is even, therefore ÀÜf‚àín = ÀÜfn and it is enough to examine
the Fourier coeÔ¨Écients for n ‚â•0.) Note also that, while all derivatives of f exist in (‚àí1, 1),
this (periodic) function cannot be extended analytically because of essential singularities at
¬±1. Yet, the Ô¨Ågure indicates that, asymptotically, the scaled logarithm oscillates about a
constant value. An easy calculation shows that asymptotically | ÀÜfn| ‚àºO[exp(‚àícnŒ±)], where
c > 0 and Œ± ‚âà0.44. This is slower than the exponential decay of genuinely analytic functions,
yet faster than O
n‚àíp
for any integer p. Hence ‚Äì and this is generally true for all periodic
C‚àûfunctions ‚Äì we have spectral convergence.
The diÔ¨Äerence between analytic and C‚àûfunctions is mostly of little relevance to real-
life computing.
Not so the diÔ¨Äerence between periodic and nonperiodic functions.
Most
diÔ¨Äerential equations are naturally equipped with Dirichlet or Neumann boundary conditions.
Forcing periodicity on the solution is wrong since boundary conditions are not some sort of

226
Spectral methods
100
90
80
70
60
4.5
3.0
2.5
4.0
20
3.5
10
0
40
50
30
Figure 10.8
The quantity ‚àílog | ÀÜfn|/n0.44 for f(x) = exp
‚àí1/(1 ‚àíx2)
oscillates
around a constant value, therefore illustrating the (roughly) O
exp(‚àícn0.44)
decay
of the Fourier coeÔ¨Écients.
optional extra: they are at the very heart of modelling nature by mathematics, so replacing
them by periodic boundary conditions by Ô¨Åat makes no sense.
Unfortunately, periodicity is necessary for spectral convergence. Even if f is analytic,
Fourier series converge as O
N ‚àí1
unless f(‚àí1) = f(+1); thus they are the equivalent of
a Ô¨Årst-order Ô¨Ånite diÔ¨Äerence or Ô¨Ånite element method. (Fourier series also converge when
analyticity fails, subject to weaker conditions, but convergence might be even slower.) There
are several techniques to speed up convergence. They (e.g. Gegenbauer Ô¨Åltering) are mostly
outside the scope of our exposition, but we will mention one fairly elementary approach that,
although coming short of inducing spectral convergence, speeds convergence up to O
N ‚àíp
for p ‚â•2: polynomial subtraction.
Suppose that an analytic function f is not periodic, yet f(‚àí1) = f(+1) (this is not
contradictory since periodicity might fail with regard to its higher derivatives). Integrating
by parts,
ÀÜfn = ‚àí1
iœÄn[f(1) ‚àíf(‚àí1)] +
1
iœÄn
=f ‚Ä≤
n =
1
iœÄn
=f ‚Ä≤
n = O
n‚àí2
,
|n| ‚â´1,
since =f ‚Ä≤
n = O
n‚àí1
. Thus, we gain one order: using the analysis of Section 10.1 we can
show that the rate of convergence is O
N ‚àí2
.
In general, of course, f(‚àí1) Ã∏= f(1), but we can force the values at the endpoints to be
equal. Set f(x) = 1
2(1‚àíx)f(‚àí1)+ 1
2(1+x)f(+1)+g(x), where g(x) = f(x)‚àí1
2(1‚àíx)f(‚àí1)‚àí
1
2(1 + x)f(+1). It is trivial to verify that g(¬±1) = 0 and that if f is analytic then so is g.

Comments and bibliography
227
15.0
7.5
2.5
0
‚àí25
10.0
5.0
50
‚àí50
25
12.5
Figure 10.9
The logarithmic tree: the values of log | ÀÜfn| (bottom branch), log |ÀÜgn|
(middle branch) and log |ÀÜhn| for f(x) = (2 + x)‚àí1.
The idea is now to represent f as a linear function plus the Fourier expansion of g:
f(x) = 1
2(1 ‚àíx)f(‚àí1) + 1
2(1 + x)f(+1) +
‚àû

n=‚àí‚àû
ÀÜgneiœÄnx.
In principle there is nothing to stop us iterating this idea. Thus, setting
h(x) = f(x) ‚àí1
4(1 ‚àíx)2(2 + x)f(‚àí1) ‚àí1
4(1 ‚àíx)2(1 + x)f ‚Ä≤(‚àí1) ‚àí1
4(1 + x)2(2 ‚àíx)f(+1)
+ 1
4(1 + x)2(1 ‚àíx)f ‚Ä≤(+1),
it is trivial to verify that h(¬±1), h‚Ä≤(¬±1) = 0 and consequently ÀÜhn = O
n‚àí3
, |n| ‚â´1. Setting
f(x) = 1
4(1 ‚àíx)2(2 + x)f(‚àí1) + 1
4(1 ‚àíx)2(1 + x)f ‚Ä≤(‚àí1) + 1
4(1 + x)2(2 ‚àíx)f(+1)
‚àí1
4(1 + x)2(1 ‚àíx)f ‚Ä≤(+1) +
‚àû

n=‚àí‚àû
ÀÜhneiœÄnx,
we thus have an O
N ‚àí3
rate of convergence.
Convergence can be accelerated further
in the same way.
Of course, the quid pro quo is a rapid increase in complexity once we
attempt to implement polynomial subtraction in tandem with spectral methods for PDEs.
And, no matter how many levels of polynomial subtraction we employ, the convergence is
never spectral. Figure 10.9 displays the logarithms of | ÀÜfn|, |ÀÜgn| and |ÀÜhn| for the function
f(x) = (1 + 2x)‚àí1.

228
Spectral methods
Our analysis predicts that
log | ÀÜfn| ‚àºc1 ‚àílog |n|,
log |ÀÜgn| ‚àºc2 ‚àí2 log |n|,
log |ÀÜhn| ‚àºc3 ‚àí3 log |n|,
|n| ‚â´1,
and this is clearly consistent with the Ô¨Ågure.
The basis of practical implementations of Fourier expansions is the fast Fourier transform.
As we have already remarked in Section 10.3, the FFT is an almost miraculous computational
device, used in a very wide range of applications. Arguably, no other computational algorithm
has ever changed the practice of science and engineering as much as this rather simple trick
‚Äì already implicit in the writings of Gauss, discovered by Lanczos (and forgotten) and, at a
more opportune moment, rediscovered by Cooley and Tukey in 1965.
Henrici‚Äôs review (1979) of the FFT and its mathematical applications is a must for every
open-minded applied mathematician. This survey is comprehensive, readable and inspiring
‚Äì if you plan to read just one mathematical paper this year, Henrici‚Äôs review will be a very
rewarding choice!
The FFT comes in many Ô¨Çavours, as do its close relatives, the fast sine transform and the
fast cosine transform. It is the standard tool whenever signals or waveforms are processed
or transmitted, and hence at the very core of electrical engineering applications: radio,
television, telephony and the Internet.
Spectral methods have been a subject of much attention since the 1970s, and the text
of Gottlieb & Orszag (1977) is an excellent and clearly written introduction to the state of
the art in the early days. Modern expositions of spectral methods, exhibiting a wide range
of outlooks on the subject, include the books of Canuto et al.
(2006), Fornberg (1995),
Hesthaven et al. (2007) and Trefethen (2000).
The menagerie of spectral methods is substantially greater than our very brief and ele-
mentary exposition would suggest (Canuto et al., 2006). Let us mention brieÔ¨Çy an important
relative of spectral techniques, pseudospectral methods (Fornberg, 1995).
Suppose that a
function u is given on an equally spaced grid kh, k = ‚àíM, . . . , M, where h = 1/M. (We
do not assume that u is periodic.) Assume further that we wish to approximate u‚Ä≤ at the
grid points. This is the stuÔ¨Äof Chapter 8 and we already know how to do it using Ô¨Ånite
diÔ¨Äerences. This, however, is likely to lead to low-order methods.
As in Chapter 8, we wish to approximate u‚Ä≤(mh) as a linear combination of the values
of u(kh) for k = m ‚àír, . . . , m . . . , m + s, where r, s ‚â•0. The highest-order approximation of
this kind is (see Exercise 16.2)
u‚Ä≤(mh) ‚âà1
h
s

k=‚àír
Œ±ku((m + k)h),
(10.26)
where
Œ±k = (‚àí1)k‚àí1
k
r!s!
(r + k)!(s ‚àík)!,
k Ã∏= 0,
Œ±0 = ‚àí
s

j=‚àír
jÃ∏=0
(‚àí1)j‚àí1
j
r!s!
(r + j)!(s ‚àíj)!.
It is possible to show that, for suÔ¨Éciently smooth u, the error is O
hr+s
= O
M ‚àír‚àís
.
How to choose r and s? The logic of Ô¨Ånite diÔ¨Äerences tells us to choose the same r and
s everywhere except perhaps very near the edges, where it is no longer true that r ‚â§M + k,
s ‚â§M ‚àík. However, there is nothing to compel us to follow this Ô¨Ånite diÔ¨Äerence logic;
we could choose diÔ¨Äerent r and s values at diÔ¨Äerent grid points. In particular, note that
at each k ‚àà{‚àíM, ‚àíM + 1, . . . , M} we have M + k points to our left and M ‚àík points to
the right that can be legitimately employed in (10.26). We use all these points! In other

Comments and bibliography
229
words, we approximate u‚Ä≤
k as a linear combination of all the values of uj on the grid, taking
r = rk = M +k, s = sk = M ‚àík. The result is a method whose error behaves like O
M ‚àíM
,
in other words it decays at spectral speed. True, the matrix is dense but we can choose a
relatively small M value while obtaining high accuracy.
Sounds spectacular: spectral convergence without periodicity! Needless to say, we can
repeat the trick for higher derivatives, construct numerical methods in this form . . . An expe-
rienced reader will rightly expect a catch! Indeed, the problem is that rapidly we are running
into very large numbers. Expressing (10.26) as a matrix‚Äìvector product, u‚Ä≤ ‚âàh‚àí1Du, say,
it is possible to show that the norm of D grows very fast.
Thus, for M = 32 we have
‚à•D‚à•‚âà2.18 √ó 1017, while for M = 64 ‚à•D‚à•‚âà1.69 √ó 1036 (we are using the Euclidean matrix
norm). In general, ‚à•D‚à•increases exponentially fast in M.
We have just one means of salvaging exponential convergence of the pseudospectral ap-
proach while keeping the size of the diÔ¨Äerentiation matrices reasonably small: abandon the
assumption of equally spaced grid points. We can think about the generation of a diÔ¨Äeren-
tiation matrix as a two-step process: Ô¨Årst interpolate u at the grid points by a polynomial
(see A.2.2.1) and subsequently diÔ¨Äerentiate the polynomial in question at the grid points.
Seen from this perspective, the size of ‚à•D‚à•is likely to be small once the interpolating poly-
nomial approximates u well. It is known, though, that while equidistant points represent a
very poor choice of interpolation points, an excellent choice is presented by the Chebyshev
points cos(œÄk/M), k = ‚àíM + 1, . . . , M: it is possible to prove that in this case ‚à•D‚à•‚âà9
4M 2.
For example, for M = 32 we obtain ‚à•D‚à•‚âà2.2801 √ó 103 and for M = 64 the norm is
‚à•D‚à•‚âà9.0619 √ó 103: the improvement is amazing. We are able to retain spectral conver-
gence while keeping the matrices reasonably well conditioned. However, in this particular
case we obtain a scheme equivalent to the Chebyshev method from Section 10.5. In more
complicated settings, the Chebyshev and pseudospectral methods part company and lead to
genuinely diÔ¨Äerent algorithms.
In Chapters 15 and 16 we consider the solution of time-dependent PDEs, with an empha-
sis on Ô¨Ånite diÔ¨Äerences methods. A major focus of attention in that setting is the stability
(a fundamental concept that will be deÔ¨Åned in due course). Spectral methods can be ap-
plied for time-dependent problems and, again, stability considerations are of central impor-
tance. Fourier-based methods do very well in this regard, but their applicability is restricted
to periodic boundary conditions. Chebyshev-based and pseudospectral methods are more
problematic but modern practice allows them to be used eÔ¨Éciently within this setting also
(Fornberg, 1995; Hesthaven et al., 2007).
Canuto, C., Hussaini, M.Y., Quarteroni, A. and Zang, T.A. (2006), Spectral Methods. Fun-
damentals in Single Domains, Springer Verlag, Berlin.
Fornberg, B. (1995), A Practical Guide to Pseudospectral Methods, Cambridge University
Press, Cambridge.
Gottlieb, D. and Orszag, S.A. (1977), Numerical Analysis of Spectral Methods: Theory and
Applications, SIAM, Philadelphia.
Henrici, P. (1979), Fast Fourier methods in computational complex analysis, SIAM Review
21, 481‚Äì527.
Hesthaven, J.S., Gottlieb, S. and Gottlieb, D. (2007), Spectral Methods for Time-Dependent
Problems, Cambridge University Press, Cambridge.
K¬®orner, T.W. (1988), Fourier Analysis, Cambridge University Press, Cambridge.
Trefethen, L.N. (2000), Spectral methods in MATLAB, SIAM, Philadelphia.

230
Spectral methods
Exercises
10.1
Given an analytic function f,
a prove that
ÀÜfn = (‚àí1)n‚àí1
2œÄin
[f(1) ‚àíf(‚àí1)] +
1
œÄin
=f ‚Ä≤n,
n ‚ààZ \ {0},
b deduce that for every s = 1, 2, . . . it is true for every n ‚ààZ \ {0} that
ÀÜfn = (‚àí1)n‚àí1
2
s‚àí1

m=0
1
(œÄin)m+1 [f (m)(1) ‚àíf (m)(‚àí1)] +
1
(œÄin)s <
f (s)n.
10.2
Unless f is analytic, the rate of decay of its Fourier harmonics can be very
slow, certainly slower than O

N ‚àí1
. To explore this, let f(x) = |x|‚àí1/2.
a Prove that ÀÜfn = g(‚àín) + g(n), where g(n) =
! 1
0 eiœÄnœÑ 2 dœÑ.
b The error function is deÔ¨Åned as the integral
erf z =
2
‚àöœÄ
 z
0
e‚àíœÑ 2 dœÑ,
z ‚ààC.
Show that its Fourier coeÔ¨Écients are
ÀÜfn = erf(
‚àö
iœÄn)
2
‚àö
in
+ erf(‚àö‚àíiœÄn)
2‚àö‚àíin
.
c Using without proof the asymptotic estimate erf(
‚àö
ix) = 1 + O

x‚àí1
for
x ‚ààR, |x| ‚â´1, or otherwise, prove that
ÀÜfn = O

n‚àí1/2
,
|n| ‚â´1.
(It is possible to prove that this Fourier series converges to f, except at the
origin. The proof is not easy.)
10.3
Prove that Œ†N satisÔ¨Åes all the axioms of a linear space (A.2.1.1). Find a
basis of Œ†N, thereby demonstrating that dim Œ†N = N.
10.4
Consider the solution of the two-point boundary value problem
(2 ‚àícos œÄx)u‚Ä≤‚Ä≤ + u = 1,
‚àí1 ‚â§x ‚â§1,
u(‚àí1) = u(1),
using the spectral method.
a Plugging the Fourier expansion of u into this diÔ¨Äerential equation, show that
the ÀÜun obey a three-term recurrence relation.

Exercises
231
b Computing ÀÜu0 separately and using the fact that ÀÜu‚àín = ÀÜun (why?), prove
that the computation of ÀÜun for ‚àíN/2 + 1 ‚â§n ‚â§N/2 (assuming that ÀÜun = 0
outside this range of n) reduces to the solution of an (N/2) √ó (N/2) tridiag-
onal system of algebraic equations.
10.5
Let a(x, y) = cos œÄx+cos œÄy and f(x, y) = sin œÄx+sin œÄy. Construct explic-
itly the linear algebraic system that needs to be computed once the equation
‚àá¬∑ (a‚àáu) = f, equipped with periodic boundary conditions, is solved for
‚àí1 ‚â§x, y ‚â§1 by a spectral method.
10.6
Supposing that B ‚àãu = ‚àû
n=0 ÀòunTn, express u‚Ä≤ in an explicit form as a
Chebyshev expansion.
10.7
The two-point ODE u‚Ä≤‚Ä≤ +u = 1, u(‚àí1) = u(1) = 0, is solved by a Chebyshev
method.
a Show that the odd coeÔ¨Écients are zero and that u(x) = ‚àû
n=0 Àòu2nT2n(x).
Express the boundary conditions as a linear condition of the coeÔ¨Écients Àòu2n.
b Express the diÔ¨Äerential equation as an inÔ¨Ånite set of linear algebraic equa-
tions in the coeÔ¨Écients Àòu2n.
c Discuss how to truncate the linear system and implement it as a proper,
well-deÔ¨Åned numerical method.
d Since u(‚àí1) = u(1), the solution is periodic. Yet we cannot expect a stan-
dard spectral method to converge at spectral speed. Why?


11
Gaussian elimination for sparse linear
equations
11.1
Banded systems
Whether the objective is to solve the Poisson equation using Ô¨Ånite diÔ¨Äerences, Ô¨Ånite
elements or a spectral method, the outcome of discretization is a set of linear algebraic
equations, e.g. (8.16) or (9.7). The solution of such equations ultimately constitutes
the lion‚Äôs share of computational expenses. This is true not just with regard to the
Poisson equation or even elliptic PDEs since, as will become apparent in Chapter 16,
the practical computation of parabolic PDEs also requires the solution of linear alge-
braic systems.
The systems (8.16) and (9.7) share two important characteristics. Our Ô¨Årst obser-
vation is that in practical situations such systems are likely to be very large. Thus,
Ô¨Åve-point equations in an 81 √ó 81 grid result in 6400 equations.
Even this might
sound large to the uninitiated but it is, actually, relatively modest compared to what
is encountered on a daily basis in real-life situations. Consider the equations of mo-
tion of Ô¨Çuids or solids, for example. The universe is three-dimensional and typical
GFD (geophysical Ô¨Çuid dynamics) codes employ 14 variables ‚Äì three each for position
and velocity, one each for density, pressure, temperature and, say, the concentrations
of Ô¨Åve chemical elements. (If you think that 14 variables is excessive, you might be
interested to learn that in combustion theory, say, even this is regarded as rather mod-
est.) Altogether, and unless some convenient symmetries allow us to simplify the task
in hand, we are solving equations in a three-dimensional parallelepiped. Requiring
81 grid points in each spatial dimension spells 14 √ó 803 = 7 168 000 coupled linear
equations!
The cost of computation using the familiar Gaussian elimination is O

d3
for
a d √ó d system, and this renders it useless for systems of size such as the above.1
Even were we able to design a computer that can perform (64 000 000)3 ‚âà2.6 √ó 1023
operations, say, in a reasonable time, the outcome is likely to be useless because of an
accumulation of roundoÔ¨Äerror.2
1A brief remark about the O( ) notation. Often, the meaning of ‚Äòf(x) = O(xŒ±) as x ‚Üíx0‚Äô is that
limx‚Üíx0 x‚àíŒ±f(x) exists and is bounded. The O( ) notation in this section can be formally deÔ¨Åned in
a similar manner, but it is perhaps more helpful to interpret O
d3
, say, in a more intuitive fashion:
it means that a quantity equals roughly a constant times d3.
2Of course, everybody knows that there are no such computers.
Are they possible, however?
Assuming serial computer architecture and considering that signals travel (at most) at the speed of
light, the distance between the central processing unit and each random access memory cell should
be at an atomic level.
233

234
Gaussian elimination
Fortunately, linear systems originating in Ô¨Ånite diÔ¨Äerences or Ô¨Ånite elements have
one redeeming grace: they are sparse.3 In other words, each variable is coupled to just
a small number of other variables (typically, neighbouring grid points or neighbouring
vertices) and an overwhelming majority of elements in the matrix vanish. For example,
in each row and column of a matrix originating in the Ô¨Åve-point formula (8.16) at
most four oÔ¨Ä-diagonal elements are nonzero. This abundance of zeros and the special
structure of the matrix allow us to implement Gaussian elimination in a manner
that brings systems with 802 equations into the realm of microcomputers and allows
the suÔ¨Éciently rapid solution of 7 168 000-variable systems on (admittedly, parallel)
supercomputers.
The subject of our attention is the linear system
Ax = b,
(11.1)
where the d √ó d real matrix A and the vector b ‚ààRd are given. We assume that A is
nonsingular and well conditioned; the latter means, roughly, that A is suÔ¨Éciently far
from being singular that its numerical solution by Gaussian elimination or its variants
is always viable and does not require any special techniques such as pivoting (A.1.4.4).
Elements of A, x and b will be denoted by ak,‚Ñì, xk and b‚Ñìrespectively, k, ‚Ñì= 1, 2, . . . , d.
The size of d will play no further direct role, but it is always important to bear in
mind that it motivates the whole discussion.
We say that A is a banded matrix of bandwidth s if ak,‚Ñì= 0 for every k, ‚Ñì‚àà
{1, 2, . . . , d} such that |k ‚àí‚Ñì| > s. Familiar examples are tridiagonal (s = 1) and
quindiagonal (s = 2) matrices.
Recall that, subject to mild restrictions, a d √ó d matrix A can be factorized into
the form
A = LU
(11.2)
where
L =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é£
1
0
¬∑ ¬∑ ¬∑
0
‚Ñì2,1
1
...
...
...
...
...
0
‚Ñìd,1
¬∑ ¬∑ ¬∑
‚Ñìd,d‚àí1
1
‚é§
‚é•‚é•‚é•‚é•‚é¶
and
U =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é£
u1,1
u1,2
¬∑ ¬∑ ¬∑
u1,d
0
u2,2
...
...
...
...
...
ud‚àí1,d
0
¬∑ ¬∑ ¬∑
0
ud,d
‚é§
‚é•‚é•‚é•‚é•‚é¶
SpeciÔ¨Åcally, a back-of-the-envelope computation indicates that, were all the expense just in com-
munication (at the speed of light!) and were the whole calculation to be completed in less than 24
hours on a serial computer ‚Äì a reasonable requirement, e.g. in calculations originating in weather
prediction ‚Äì the average distance between the CPU and every memory cell should be roughly 10‚àí7
millimetres, barely twice the radius of a hydrogen atom.
This, needless to say, is in the realm of fantasy. Even the bravest souls in the miniaturization
business dare not contemplate realistic computers of this size and, anyway, quantum eÔ¨Äects are
bound to make an atomic-sized computer an uncertain (in Heisenberg‚Äôs sense) proposition.
There is an important caveat to this emphatic statement: quantum computers are based upon
diÔ¨Äerent principles, which do not preclude this sort of mind-boggling speed. Yet, as things stand,
quantum computers exist only in theory.
3Systems originating in spectral methods are dense, but much smaller: what is lost on the swings
is regained on the roundabouts . . .

11.1
Banded systems
235
are lower triangular and upper triangular respectively (A.1.4.5). In general, it costs
O

d3
operations to calculate L and U. However, if A has bandwidth s then this can
be signiÔ¨Åcantly reduced.
To demonstrate that this is indeed the case (and, incidentally, to measure exactly
the extent of the savings) we assume that a1,1 Ã∏= 0, and we let
‚Ñì:=
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
1
a2,1/a1,1
a3,1/a1,1
...
ad,1/a1,1
‚é§
‚é•‚é•‚é•‚é•‚é•‚é¶
,
u‚ä§=

a1,1
a1,2
¬∑ ¬∑ ¬∑
a1,d

and set ÀúA := A ‚àí‚Ñìu‚ä§. Regardless of the bandwidth of A, the matrix ÀúA has zeros
along its Ô¨Årst row and column, and we Ô¨Ånd that
L =

‚Ñì
0‚ä§
ÀÜL

,
U =

u‚ä§
0
ÀÜU

,
where ÀÜA = ÀÜL ÀÜU, the matrix ÀÜA having been obtained from ÀúA by deleting the Ô¨Årst
row and column. Setting the Ô¨Årst column of L and the Ô¨Årst row of U to ‚Ñìand u‚ä§
respectively, we therefore reduce the problem of LU-factorizing the d √ó d matrix A to
that of an LU factorization of the (d ‚àí1) √ó (d ‚àí1) matrix ÀÜA. For a general matrix A
it costs O

d2
operations to evaluate ÀÜA, but the operation count is smaller if A is of
bandwidth s. Since just s + 1 top components of ‚Ñìand s + 1 leftward components of
u‚ä§are nonzero, we need to form just the top (s + 1) √ó (s + 1) minor of ‚Ñìu‚ä§. In other
words, O

d2
is replaced by O

(s + 1)2
.
Continuing by induction we obtain progressively smaller matrices and, after d ‚àí1
such steps, derive an operation count O

(s + 1)2d

for the LU factorization of a banded
matrix. We assume, of course, that the pivots a1,1, ÀÜa1,1, . . . never vanish, otherwise
the above procedure could not be completed, but mention in passing that substantial
savings accrue even when there is a need for pivoting (see Exercise 11.1).
The matrices L and U share the bandwidth of A. This is a very important ob-
servation, since a common mistake is to regard the diÔ¨Éculty of solving (11.1) with
very large A as being associated solely with the number of operations. Storage plays
a crucial role as well! In place of d2 storage ‚Äòunits‚Äô required for a dense d √ó d matrix,
a banded matrix requires only about (2s+1)d. Provided that s ‚â™d, this often makes
as much diÔ¨Äerence as the reduction in the operation count. Since L and U also have
bandwidth s and we obviously have no need to store known zeros, or for that matter
known ones along the diagonal of L, we can reuse computer memory that has been
devoted to the storing of A (in a sparse representation!) to store L and U instead.
Having obtained the LU factorization, we can solve (11.1) with relative ease by
solving Ô¨Årst Ly = b and then Ux = y.
On the face of it, this sounds like yet
another mathematical nonsense ‚Äì instead of solving one d √ó d linear system, we solve
two! However, since both L and U are banded, this can be done considerably more

236
Gaussian elimination
cheaply. In general, for a dense matrix A the operation count is O

d2
, but this can
be substantially reduced for banded matrices. Writing Ly = b in a form that pays
heed to sparsity, we have
y1 = b1,
‚Ñì2,1y1 + y2 = b2,
‚Ñì3,1y1 + ‚Ñì3,2y2 + y3 = b3,
...
‚Ñì1,sy1 + ¬∑ ¬∑ ¬∑ + ‚Ñìs‚àí1,sys‚àí1 + ys = bs,
‚Ñìk‚àís,kyk‚àís + ¬∑ ¬∑ ¬∑ + ‚Ñìk‚àí1,kyk‚àí1 + yk = bk,
k = s + 1, s + 2, . . . , d,
and hence O(sd) operations. A similar argument applies to Ux = y.
Let us count the blessings of bandedness. Firstly, LU factorization ‚Äòcosts‚Äô O

s2d

operations, rather than O

d3
. Secondly, the storage requirement is O((2s + 1)d),
compared with d2 for a dense matrix. Finally, provided that we have already derived
the factorization, the solution of (11.1) entails just O(sd) operations in place of O

d2
.
3 A few examples of banded matrices
The savings due to the exploitation
of bandedness are at their most striking in the case of tridiagonal matrices.
Then we need just O(d) operations for the factorization and a similar number
for the solution of triangular systems, and just 4d ‚àí2 real numbers (inclusive
of the vector x) need be stored at any one time.
The implementation of
banded LU factorization in the case s = 1 is sometimes known as the Thomas
algorithm.
A more interesting case is presented by the Ô¨Åve-point equations (8.16) in a
square. To present them in the form (11.1) we need to rearrange, at least
formally, the two-dimensional m √ó m grid from Fig. 8.4 into a vector in Rd,
d = m2. Although there are d! distinct ways of doing this, the most obvious
is simply to append the columns of an array to each other, starting from the
leftmost. Appropriately, this is known as natural ordering:
x =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
u1,1
u2,1
...
um,1
u1,2
...
um,2
...
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
,
Àúb = (‚àÜx)2
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
f1,1
f2,1
...
fm,1
f1,2
...
fm,2
...
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
.
The vector b is composed of Àúb and the contribution of the boundary points is
b1 = Àúb1 ‚àí[u(‚àÜx, 0) + u(0, ‚àÜx)],
b2 = Àúb2 ‚àíu(2‚àÜx, 0),
. . . ,
bm+1 = Àúbm+1 ‚àíu(0, 2‚àÜx),
bm+2 = Àúbm+2,
. . .

11.1
Banded systems
237
It is convenient to represent the matrix A as composed of m blocks, each of
size m √ó m. For example, m = 4 results in the 16 √ó 16 matrix
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
‚àí4
1
0
0
1
0
0
0
0
0
0
0
0
0
0
0
1
‚àí4
1
0
0
1
0
0
0
0
0
0
0
0
0
0
0
1
‚àí4
1
0
0
1
0
0
0
0
0
0
0
0
0
0
0
1
‚àí4
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
‚àí4
1
0
0
1
0
0
0
0
0
0
0
0
1
0
0
1
‚àí4
1
0
0
1
0
0
0
0
0
0
0
0
1
0
0
1
‚àí4
1
0
0
1
0
0
0
0
0
0
0
0
1
0
0
1
‚àí4
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
‚àí4
1
0
0
1
0
0
0
0
0
0
0
0
1
0
0
1
‚àí4
1
0
0
1
0
0
0
0
0
0
0
0
1
0
0
1
‚àí4
1
0
0
1
0
0
0
0
0
0
0
0
1
0
0
1
‚àí4
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
‚àí4
1
0
0
0
0
0
0
0
0
0
0
0
1
0
0
1
‚àí4
1
0
0
0
0
0
0
0
0
0
0
0
1
0
0
1
‚àí4
1
0
0
0
0
0
0
0
0
0
0
0
1
0
0
1
‚àí4
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
.
(11.3)
In general, it is easy to see that A has bandwidth s = m. In other words, we
need just O

m4
operations to LU-factorize it ‚Äì a large number, yet signiÔ¨Å-
cantly smaller than O

m6
, the operation count for ‚Äòdense‚Äô LU factorization.
Note that a matrix might possess a large number of zeros inside the band;
(11.3) is a case in point. These zeros will, in all likelihood, be destroyed (or
‚ÄòÔ¨Ålled in‚Äô) through LU factorization. The banded algorithm guarantees only
that zeros outside the band are retained!
3
Whenever a matrix originates in a one-dimensional arrangement of a multivariate grid,
the exact nature of the ordering is likely to have a bearing on the bandwidth. Indeed,
the secret of eÔ¨Écient implementation of banded LU factorization for matrices that
originate in a planar (or higher-dimensional) grid is in Ô¨Ånding a good arrangement
of grid points. An example of a bad arrangement is provided by red‚Äìblack ordering,
which, as far as the matrix (11.3) is concerned, is


1


2


3


4


5


6


7


8
9
10
11
12
13
14
15
16
In other words, the grid is viewed as a chequerboard and black squares are selected

238
Gaussian elimination
before the red ones. The outcome,
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
√ó
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
√ó
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
√ó
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
√ó
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
√ó
√ó
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
√ó
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
√ó
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
√ó
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
√ó
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
(11.4)
is of bandwidth s = 10 and is quite useless as far as sparse LU factorization is con-
cerned. (Here and in the sequel we adopt the notation whereby ‚Äò√ó‚Äô and ‚Äò‚ó¶‚Äô stand for
the nonzero and zero components respectively; after all, the exact numerical values
of these components have no bearing on the underlying problem.) We mention in
passing that, although red‚Äìblack ordering is an exceedingly poor idea if the goal is to
minimize the bandwith, it has certain virtues in the context of iterative methods (see
Chapter 12).
In many situations it is relatively easy to Ô¨Ånd a conÔ¨Åguration that results in a small
bandwidth, but occasionally this might present quite a formidable problem. This is
in particular the case with linear equations that originate from tessalations of two-
dimensional sets into irregular triangles. There exist combinatorial algorithms that
help to arrange arbitrary sets of equations into matrices having a ‚Äògood‚Äô bandwidth,4
but they are outside the scope of this exposition.
11.2
Graphs of matrices and perfect Cholesky
factorization
Let A be a symmetric d √ó d matrix. We say that a Cholesky factorization of A is
LL‚ä§, where L is a d √ó d lower triangular matrix. (Note that the diagonal elements
of L need not be ones.) Cholesky shares all the advantages of an LU factorization
but requires only half the number of operations to evaluate and half the memory to
store. Moreover, as long as A is positive deÔ¨Ånite, a Cholesky factorization always
exists (A.1.4.6). We assume for the time being that A is indeed positive deÔ¨Ånite.
4A ‚Äògood‚Äô bandwidth is seldom the smallest possible, but this is frequently the case with combina-
torial algorithms. ‚ÄòGood‚Äô solutions are often relatively cheap to obtain, but Ô¨Ånding the best solution
is often much more expensive than solving the underlying equations with even the most ineÔ¨Écient
ordering.

11.2
Graphs of matrices and perfect Cholesky factorization
239
It follows at once from the proof of Theorem 8.4 that every matrix A obtained from
the Ô¨Åve-point formula and reasonable boundary schemes is negative deÔ¨Ånite. A similar
statement is true in regard to matrices that arise when the Ô¨Ånite element method is
applied to the Poisson equation, provided that piecewise linear basis functions are
used and that the geometry is suÔ¨Éciently simple. Since we can solve ‚àíAx = ‚àíb in
place of Ax = b, the stipulation of positive deÔ¨Åniteness is not as contrived as it might
perhaps seem at Ô¨Årst glance.
We have already seen in Section 11.1 that the secret of good LU (and, for that
matter, Cholesky) factorization is in the ordering of the equations and variables. In
the case of a grid, this corresponds to ordering the grid points, but remember from
the discussion in Chapter 8 that each such point corresponds to an equation and
to a variable! Given a matrix A, we wish to Ô¨Ånd an ordering of equations and an
ordering of variables such that the outcome is amenable to eÔ¨Écient factorization. Any
rearrangement of equations (hence, of the rows of A) is equivalent to the product PA,
where P is a d √ó d permutation matrix (A.1.2.5). Likewise, relabelling the variables
is tantamount to rearranging the columns of A, hence to the product AQ, where Q
is a permutation matrix. (Bearing in mind the purpose of the whole exercise, namely
the solution of the linear system (11.1), we need to replace b by Pb and x by Qx;
see Exercise 11.5.) The outcome, PAQ, retains symmetry and positive deÔ¨Åniteness if
Q = P ‚ä§, hence we assume herewith that rows and columns are always reordered in
unison. In practical terms, it means that if the (k, ‚Ñì)th grid point corresponds to the
jth equation, the variable uk,‚Ñìbecomes the jth unknown.
The matrix A is sparse and the purpose of a good Cholesky factorization is to retain
as many zeros as possible. More formally, in any particular (symmetric) ordering of
equations, we say that the Ô¨Åll-in is the number of pairs (i, j), 1 ‚â§j < i ‚â§d, such
that ai,j = 0 and ‚Ñìi,j Ã∏= 0. Our goal is to devise an ordering that minimizes Ô¨Åll-in.
In particular, we say that a Cholesky factorization of a speciÔ¨Åc matrix A is perfect if
there exists an ordering that yields no Ô¨Åll-in whatsoever: every zero is retained.
An example of a perfect factorization is provided by a banded symmetric matrix,
where we assume that no components vanish within the band.
This is clear from
the discussion in Section 11.1, which generalizes at once to a Cholesky factorization.
Another example, at the other extreme, is a completely dense matrix (which, of course,
is banded with a bandwidth of s = d ‚àí1, but to say this is to abuse the spirit, if not
the letter, of the deÔ¨Ånition of bandedness).
A convenient way of analysing the sparsity patterns of symmetric matrices is af-
forded by graph theory. We have already mentioned graphs in a less formal setting,
in the discussion at the end of Chapter 3.
Formally, a graph is the set G = {V, E}, where V = {1, 2, . . . , d} and E ‚äÜV2
consists of pairs of the form (i, j), i < j. The elements of V and E are said to be the
vertices and edges of G, respectively.
We say that G is the graph of a symmetric d √ó d matrix A if (i, j) ‚ààE if and only
if ai,j Ã∏= 0, 1 ‚â§i < j ‚â§d. In other words, G displays the sparsity pattern of A, which
we have presented in (11.4) using the symbols ‚Äò‚ó¶‚Äô and ‚Äò√ó‚Äô.
Although a graph can be represented by listing all the edges one by one, it is
considerably more convenient to illustrate it pictorially in a self-explanatory manner.

240
Gaussian elimination
Therefore we will now give a few examples of matrices (represented by their sparsity
pattern) and their graphs.
tridiagonal:
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
‚áí












1
2
3
4
5
6
quindiagonal:
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
√ó
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
√ó
√ó
‚ó¶
‚ó¶
√ó
√ó
√ó
√ó
√ó
‚ó¶
‚ó¶
√ó
√ó
√ó
√ó
√ó
‚ó¶
‚ó¶
√ó
√ó
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
√ó
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
‚áí












1
2
3
4
5
6








arrowhead:
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
√ó
√ó
√ó
√ó
√ó
√ó
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
‚áí












1
2
3
4
5
6
Q
Q
Q
Q
Q
S
S
S



cyclic:
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
‚áí












1
2
3
4
5
6 
@@
@
@


The graph of a matrix often reveals at a single glance its structure, which might
not be evident from the sparsity pattern. Thus, consider the matrix
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
√ó
‚ó¶
√ó
‚ó¶
√ó
‚ó¶
‚ó¶
√ó
‚ó¶
√ó
√ó
‚ó¶
√ó
‚ó¶
√ó
‚ó¶
‚ó¶
√ó
‚ó¶
√ó
‚ó¶
√ó
‚ó¶
√ó
√ó
√ó
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
‚ó¶
√ó
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
.
At a Ô¨Årst glance, there is nothing to link it to any of the four matrices that we have
just displayed, but its graph,

11.2
Graphs of matrices and perfect Cholesky factorization
241












1
2
3
4
5
6




T
T
TT
T
T
T
T



HHHHH





tells a diÔ¨Äerent story ‚Äì it is nothing other than the cyclic matrix in disguise! To see
this, just relabel the vertices as follows
1 ‚Üí1,
2 ‚Üí5,
3 ‚Üí2,
4 ‚Üí4,
5 ‚Üí6,
6 ‚Üí3.
This, of course, is equivalent to reordering (simultaneously) the equations and variables.
An ordered set of edges {(ik, jk)}ŒΩ
k=1 ‚äÜE is called a path joining the vertices Œ±
and Œ≤ if Œ± ‚àà{i1, j1}, Œ≤ ‚àà{iŒΩ, jŒΩ} and for every k = 1, 2, . . . , ŒΩ ‚àí1 the set {ik, jk} ‚à©
{ik+1, jk+1} contains exactly one member. It is a simple path if it does not visit any
vertex more than once. We say that G is a tree if each two members of V are joined by
a unique simple path. Both tridiagonal and arrowhead matrices correspond to trees,
but this is not the case with either quindiagonal or cyclic matrices when ŒΩ ‚â•3.
Given a tree G and an arbitrary vertex r ‚ààV, the pair T = ‚ü®G, r‚ü©is called a rooted
tree, while r is said to be the root. Unlike an ordinary graph, T admits a natural partial
ordering, which can best be explained by an analogy with a family tree. Thus, the
root r is the predecessor of all the vertices in V \ {r} and these vertices are successors
of r. Moreover, every Œ± ‚ààV \ {r} is joined to v by a simple path and we designate
each vertex along this path, except for r and Œ±, as a predecessor of Œ± and a successor
of r. We say that the rooted tree T is monotonically ordered if each vertex is labelled
before all its predecessors; in other words, we label the vertices from the top of the
tree to the root. (As we have already said it, relabelling a graph is tantamount to
permuting the rows and the columns of the underlying matrix.)
Every rooted tree can be monotonically ordered and, in general, such an ordering
is not unique. We now give three monotone orderings of the same rooted tree:














Z
Z
Z

@
@


1
2
3
4
5
6
7














Z
Z
Z

@
@


4
1
5
3
2
6
7














Z
Z
Z

@
@


2
1
4
5
3
6
7
Theorem 11.1
Let A be a symmetric matrix whose graph G is a tree. Choose a
root r ‚àà{1, 2, . . . , d} and assume that the rows and columns of A have been arranged
so that T = ‚ü®G, r‚ü©is monotonically ordered.
Given that A = LL‚ä§is a Cholesky
factorization, it is true that
‚Ñìk,j = ak,j
‚Ñìj,j
,
k = j + 1, j + 2, . . . , d,
j = 1, 2, . . . , d ‚àí1.
(11.5)

242
Gaussian elimination
Therefore ‚Ñìk,j = 0 whenever ak,j = 0 and the matrix A can be Cholesky-factorized
perfectly.
Proof
The coeÔ¨Écients of L can be written down explicitly (A.1.4.5). In particu-
lar,
‚Ñìk,j =
1
‚Ñìj,j

ak,j ‚àí
j‚àí1

i=1
‚Ñìk,i‚Ñìj,i

,
k = j + 1, j + 2, . . . , d,
j = 1, 2, . . . , d ‚àí1. (11.6)
It follows at once that the statement of the theorem is true with regard to the Ô¨Årst
column, since (11.6) yields ‚Ñìk,1 = ak,1/‚Ñì1,1, k = 2, 3, . . . , d.
We continue by in-
duction on j. Suppose thus that the theorem is true for j = 1, 2, . . . , q ‚àí1, where
q ‚àà{2, 3, . . . , d ‚àí1}. The rooted tree T is monotonically ordered, and this means that
for every i = 1, 2, . . . , d ‚àí1 there exists a unique vertex Œ≥i ‚àà{i + 1, i + 2, . . . , d} such
that (i, Œ≥i) ‚ààE.
Now, choose any k ‚àà{q + 1, q + 2, . . . , d}. If ‚Ñìk,i Ã∏= 0 for some i ‚àà{1, 2, . . . , q ‚àí1}
then, by the induction assumption, ak,i Ã∏= 0 also. This implies (i, k) ‚ààE, hence k = Œ≥i.
We deduce that q Ã∏= Œ≥i, therefore (i, q) Ã∏‚ààE. Consequently ai,q = 0 and, exploiting
again the induction assumption, ‚Ñìi,q = 0.
By an identical argument, if ‚Ñìq,i Ã∏= 0 for some i ‚àà{1, 2, . . . , q ‚àí1} then q = Œ≥i,
hence k Ã∏= Œ≥i for all k = q + 1, q + 2, . . . , d, and this implies in turn that ‚Ñìk,i = 0.
We let j = q in (11.6). Since, as we have just proved, ‚Ñìk,i‚Ñìq,i = 0 for all i =
1, 2, . . . , q ‚àí1 and k = i + 1, i + 2, . . . , d, the sum in (11.6) vanishes and we deduce
that ‚Ñìk,q = ak,q/‚Ñìq,q, k = q + 1, q + 2, . . . , d. This inductive proof of the theorem is
thus complete.
An important observation is that the expense of Cholesky factorization of a matrix
consistently with the conditions of Theorem 11.1 is proportional to the number of
nonzero elements under the main diagonal. This is certainly true as far as ‚Ñìk,j, 1 ‚â§
j < k ‚â§d, is concerned and it is possible to verify (see Exercise 11.8) that this is also
the case for the calculation of ‚Ñì1,1, ‚Ñì2,2, . . . , ‚Ñìd,d.
Monotone ordering can lead to spectacular savings and a striking example is the
arrowhead matrix. If we factorized it in a naive fashion we could easily cause total Ô¨Åll-
in but, provided that we rearrange the rows and columns to correspond with monotone
ordering, the factorization is perfect.
Unfortunately, very few matrices of interest are symmetric and positive deÔ¨Ånite
and have a graph that is a tree. Perhaps the only truly useful example is provided by
a (symmetric, positive deÔ¨Ånite) tridiagonal matrix ‚Äì and we do not need graph theory
to tell us that it can be perfectly factorized!
Positive deÔ¨Åniteness is, however, not strictly necessary for our argument and we
have used it only as an cast-iron guarantee that no pivots ‚Ñì1,1, ‚Ñì2,2, . . . , ‚Ñìd‚àí1,d‚àí1 ever
vanish. Likewise, we can dispense ‚Äì up to a point ‚Äì with symmetry. All that matters
is a symmetric sparsity structure, namely ak,j Ã∏= 0 if and only if aj,k Ã∏= 0 for all
d = 1, 2, . . . , d. We have LU factorization in place of Cholesky factorization, but a
generalization of Theorem 11.1 presents no insurmountable diÔ¨Éculties.
The one truly restrictive assumption is that the graph is a tree and this renders
Theorem 11.1 of little immediate interest in applications. There are three reasons

Comments and bibliography
243
why we nevertheless attend to it. Firstly, it provides the Ô¨Çavour of considerably more
substantive results on matrices and graphs.
Secondly, it is easy to generalize the
theorem to partitioned trees, graphs where we have a tree-like structure, provided that
instead of vertices we allow subsets of V. An example illustrating this concept and its
application to perfect factorization is presented in the comments below. Finally, the
idea of using graphs to investigate the sparsity structure and factorization of matrices
is such a beautiful example of lateral thinking in mathematics that its presentation
can surely be justiÔ¨Åed on purely √¶sthetic grounds.
We complete this brief review of graph theory and the factorization of sparse
matrices by remarking that, of course, there are many matrices whose graphs are not
trees, yet which can be perfectly factorized. We have seen already that this is the
case with a quindiagonal matrix and a less trivial example will be presented in the
comments below. It is possible to characterize all graphs that correspond to matrices
with a perfect (Cholesky or LU) factorization, but that requires considerably deeper
graph theory.
Comments and bibliography
The solution of sparse algebraic systems is one of the main themes of modern scientiÔ¨Åc
computing. Factorization that exploits sparsity is just one, and not necessarily the preferred,
option and we shall examine alternative approaches ‚Äì speciÔ¨Åcally, iterative methods and fast
Poisson solvers ‚Äì in Chapters 12‚Äì15.
References on sparse factorization (sometimes dubbed direct solution, to distinguish it
from iterative methods) abound and we refer the reader to DuÔ¨Äet al. (1986); George &
Liu (1981); Tewarson (1973). Likewise, many textbooks present graph theory and we single
out Harary (1969) and Golumbic (1980).
The latter includes an advanced treatment of
graph-theoretical methods in sparse matrix factorization, including the characterization of
all matrices that can be factorized perfectly.
It is natural to improve upon the concept of a banded matrix by allowing the bandwidth
to vary. For example, consider the 8 √ó 8 matrix
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
√ó
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
The portion of the matrix enclosed between the solid lines is called the envelope. It is easy
to demonstrate that LU factorization can be performed in such a manner that Ô¨Åll-in cannot
occur outside the envelope; see Exercise 11.2. It is evident even in this simple example that
‚Äòenvelope factorization‚Äô might result in considerable savings over ‚Äòbanded factorization‚Äô.
Sometimes it is easy to Ô¨Ånd a good banded structure or a good envelope of a given
matrix by inspection ‚Äì a procedure that might involve the rearrangement of equations and
variables. In general, however, this is a formidable task, which needs to be accomplished by
a combinatorial algorithm. An eÔ¨Äective, yet relatively simple, such method is the reverse

244
Gaussian elimination
Cuthill‚ÄìMcKee algorithm. We will not dwell further on this theme, which is explained well
in George & Liu (1981).
Throughout our discussion of banded and envelope algorithms we have tacitly assumed
that the underlying matrix A is symmetric or, at the very least, has a symmetric sparsity
structure. As we have already commented, this is eminently sensible whenever we consider,
for example, the equations that occur when the Poisson equation is approximated by the Ô¨Åve-
point formula. However, it is possible to extend much of the theory to arbitrary matrices; a
good deal of the extension is trivial. For example, if there are nonzero elements in the set
{ak,j : k ‚àí2 ‚â§j ‚â§k +1} then, assuming, as always, that the underlying factorization is well
conditioned, A = LU, where ‚Ñìk,j = 0 for j ‚â§k ‚àí3 and uk,j = 0 for j ‚â•k + 2. However, the
question of how to arrange elements of a nonsymmetric matrix so that it is amenable to this
kind of treatment is substantially more formidable. Note that the correspondence between
graphs and matrices assumes symmetry. Where the more advanced concepts of graph theory
‚Äì speciÔ¨Åcally, directed graphs ‚Äì have been applied to nonsymmetric sparsity structures, the
results so far have met with only modest success.
To appreciate the power of graph theory in revealing the sparsity pattern of a symmetric
matrix, thereby permitting its intelligent exploitation, consider the following example. At
Ô¨Årst glance, the matrix
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
√ó
√ó
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
√ó
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
.
might appear as a completely unstructured mishmash of noughts and crosses, but we claim
nonetheless that it can be perfectly factorized. This becomes more apparent upon an exam-
ination of its graph:


7


1


3


11


8


6


13


5


12


2


9


10


4
@
@


Q
Q
Q
Q
Q

Z
Z
Z


H
H
H
Although this is not a tree, a tree-like structure is apparent. In fact, it is a partitioned tree,

Comments and bibliography
245
a ‚Äòsuper-graph‚Äô which can be represented as a tree of of ‚Äòsuper-vertices‚Äô that are themselves
graphs.
The equations and unknowns are ordered so that the graph is traversed from top to
bottom. This can be done in a variety of diÔ¨Äerent ways and we herewith choose an order-
ing that keeps all vertices in each ‚Äòsuper-vertex‚Äô together; this is not really necessary but
makes the exposition simpler. The outcome of this permutation is displayed in a block form,
corresponding to the structure of the partitioned tree, as follows:
1
4
8
9
12
7
11
3
2
5
10
13
6
1
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
4
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
8
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
9
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
12
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
7
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
11
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
√ó
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
3
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
2
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
√ó
√ó
‚ó¶
5
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
√ó
√ó
‚ó¶
10
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
√ó
√ó
‚ó¶
13
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
√ó
√ó
√ó
6
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
It is now clear how to proceed. Firstly, factorize the vertices 1, 4, 8, 9 and 12. This obviously
causes no Ô¨Åll-in. If you cannot see it at once, consider the equivalent problem of Gaussian
elimination. In the Ô¨Årst column we need to eliminate just a single nonzero component and we
do this by subtracting a multiple of row 1 from row 7. Likewise, we eliminate one component
in the Ô¨Årst row (remember that we need to maintain symmetry!). Neither operation causes
Ô¨Åll-in and we proceed similarly with 4, 8, 9, 12.
Having factorized the aforementioned Ô¨Åve rows and columns, we are left with an 8 √ó 8
problem.
We next factorize 7, 11, 3, in this order; again, there is no Ô¨Åll-in.
Finally, we
factorize 2, 5, 10 and, then, 13. The outcome is a perfect Cholesky factorization.
The last example, contrived as it might be, provides some insight into one of the most
powerful techniques in the factorization of sparse matrices, the method of partitioned trees.
Given a sparse matrix A with a graph G = {V, E}, we will partition the set of vertices
V =
s7
i=1
Vi
such that
Vi ‚à©Vj = ‚àÖ
for every
i, j = 1, 2, . . . , s,
i Ã∏= j.
Letting ÀúV := {1, 2, . . . , s}, we construct the set ÀúE ‚äÜÀúV √ó ÀúV by assigning to it every pair
(i, j) for which i < j and there exist Œ± ‚ààVi and Œ≤ ‚ààVj such that either (Œ±, Œ≤) ‚ààE or
(Œ≤, Œ±) ‚ààE. The set ÀúG := {ÀúV, ÀúE} is itself a graph. Suppose that we have partitioned V so
that ÀúG is a tree. In that case we know from Theorem 11.1 that, by selecting a root in ÀúV and
imposing monotone ordering on the partitioned tree, we can factorize the matrix without
any Ô¨Åll-in taking place between partitioned vertices. There might well be Ô¨Åll-in inside each
set Vi, i = 1, 2, . . . , s. However, provided that these sets are either small (as in the extreme
case s = d, when each Vi is a singleton) or fairly dense (in our example, all the sets Vi are
completely dense), the Ô¨Åll-in is likely to be modest.
Sometimes it is possible to Ô¨Ånd a good partitioned tree structure for a graph just by
inspecting it, but there exist algorithms that produce good partitions automatically. Such

246
Gaussian elimination
methods are extremely unlikely to produce the best possible partition (in the sense of min-
imizing the Ô¨Åll-in), but it should be clear by now that, when it comes to combinatorial
algorithms, the best is often the mortal enemy of the good.
We conclude these remarks with few sobering thoughts. Most numerical analysts regard
direct factorization methods as pass¬¥e and inferior to iterative algorithms. Bearing in mind the
power of modern iterative schemes for linear equations ‚Äì multigrid, preconditioned conjugate
gradients, generalized minimal residuals (GMRes) etc. ‚Äì this is probably a sensible approach.
However, direct factorization has its place, not just in the obvious instances such as banded
matrices; it can also, as we will note in Chapter 15, join forces with the (iterative) method of
conjugate gradients to produce one of the most eÔ¨Äective solvers of linear algebraic systems.
DuÔ¨Ä, I.S., Erisman, A.M. and Reid, J.K. (1986), Direct Methods for Sparse Matrices, Oxford
University Press, Oxford.
George, A. and Liu, J.W.-H. (1981), Computer Solution of Large Sparse Positive DeÔ¨Ånite
Systems, Prentice‚ÄìHall, Englewood CliÔ¨Äs, NJ.
Golumbic, M.C. (1980), Algorithmic Graph Theory and Perfect Graphs, Academic Press,
New York.
Harary, F. (1969), Graph Theory, Addison‚ÄìWesley, Reading, MA.
Tewarson, R.P. (1973), Sparse Matrices, Academic Press, New York.
Exercises
11.1
Let A be a d √ó d nonsingular matrix with bandwidth s ‚â•1 and suppose
that Gaussian elimination with column pivoting is used to solve the system
Ax = b. This means that, before eliminating all nonzero compenents under
the main diagonal in the jth column, where j ‚àà{1, 2, . . . , d ‚àí1}, we Ô¨Årst
Ô¨Ånd |akj,j| := maxi=j,j+1,...,d |ai,j| and next exchange the jth and the kjth
rows of A, as well as the corresponding components of b (see A.1.4.4).
a Identify all the coeÔ¨Écients of the intermediate equations that might be Ô¨Ålled
in during this procedure.
b Prove that the operation count of LU factorization with column pivoting is
O

s2d

.
11.2
Let A be a d √ó d symmetric positive deÔ¨Ånite matrix and for every j =
1, 2, . . . , d‚àí1 deÔ¨Åne kj ‚àà{1, 2, . . . , j} as the least integer such that akj,j Ã∏= 0.
We may assume without loss of generality that k1 ‚â§k2 ‚â§¬∑ ¬∑ ¬∑ ‚â§kd‚àí1, since
otherwise A can be brought into this form by row and column exchanges.
a Prove that the number of operations required to Ô¨Ånd a Cholesky factorization
of A is
O
‚éõ
‚éù
d‚àí1

j=1
(j ‚àíkj + 1)
‚éû
‚é†.

Exercises
247
b Demonstrate that the result of an operation count for a Cholesky factoriza-
tion of banded matrices is a special case of this formula.
11.3
Find the bandwidth of the m2 √ó m2 matrix that is obtained from the nine-
point equations (8.28) with natural ordering.
11.4
Suppose that a square is triangulated in the following fashion:
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
@
The Poisson equation in a square is discretized using the FEM and piecewise
linear functions with this triangulation. Observe that every vertex can be
identiÔ¨Åed with an unknown in the linear system (9.7).
a Arranging equations in an m√óm grid in natural ordering, Ô¨Ånd the bandwidth
of the m2 √ó m2 matrix.
b What is the graph of this matrix?
11.5
Let P and Q be two d√ód permutation matrices (A.1.2.5) and let ÀúA := PAQ,
where the d√ód matrix A is nonsingular. Prove that if Àúx ‚ààRd is the solution
of ÀúAÀúx = Pb then x = QÀúx solves (11.1).
11.6
We say that a graph G = {V, E} is connected if any two vertices in V can be
joined by a path of edges from E; otherwise it is disconnected. Let A be a
d √ó d symmetric matrix with graph G. Prove that if G is disconnected then,
after rearrangement of rows and columns, A can be written in the form
A =

A1
O
O
A2

,
where Aj is a matrix of size dj √ó dj, j = 1, 2, and d1 + d2 = d.

248
Gaussian elimination
11.7
Construct the graphs of the matrices with the following sparsity patterns:
a
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
√ó ‚ó¶
‚ó¶
√ó ‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
√ó ‚ó¶
√ó
√ó
√ó ‚ó¶
‚ó¶
‚ó¶
√ó ‚ó¶
√ó ‚ó¶
‚ó¶
√ó
√ó ‚ó¶
‚ó¶
√ó ‚ó¶
‚ó¶
√ó ‚ó¶
‚ó¶
√ó
√ó ‚ó¶
√ó ‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó ‚ó¶
‚ó¶
‚ó¶
√ó ‚ó¶
√ó
‚ó¶
√ó ‚ó¶
√ó ‚ó¶
‚ó¶
√ó ‚ó¶
‚ó¶
‚ó¶
√ó ‚ó¶
‚ó¶
√ó ‚ó¶
√ó
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
;
b
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
√ó ‚ó¶
‚ó¶
‚ó¶
√ó
√ó ‚ó¶
‚ó¶
‚ó¶
√ó ‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
√ó
√ó
√ó ‚ó¶
√ó ‚ó¶
‚ó¶
‚ó¶
√ó
√ó ‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó ‚ó¶
√ó ‚ó¶
√ó ‚ó¶
‚ó¶
√ó
√ó ‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó ‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó ‚ó¶
‚ó¶
‚ó¶
√ó ‚ó¶
‚ó¶
√ó ‚ó¶
‚ó¶
√ó ‚ó¶
‚ó¶
√ó
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
;
c
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
√ó ‚ó¶
‚ó¶
‚ó¶
√ó ‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó ‚ó¶
√ó
√ó
√ó ‚ó¶
√ó
‚ó¶
‚ó¶
√ó ‚ó¶
√ó ‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó ‚ó¶
√ó ‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
√ó ‚ó¶
√ó ‚ó¶
√ó ‚ó¶
‚ó¶
√ó ‚ó¶
‚ó¶
‚ó¶
√ó ‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó ‚ó¶
√ó ‚ó¶
‚ó¶
√ó ‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
;
d
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
√ó ‚ó¶
√ó
√ó ‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó ‚ó¶
‚ó¶
√ó
√ó
√ó
√ó
√ó ‚ó¶
√ó ‚ó¶
‚ó¶
‚ó¶
√ó ‚ó¶
√ó ‚ó¶
‚ó¶
√ó ‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
√ó ‚ó¶
‚ó¶
√ó ‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó ‚ó¶
‚ó¶
‚ó¶
√ó ‚ó¶
‚ó¶
‚ó¶
√ó
√ó ‚ó¶
‚ó¶
‚ó¶
√ó ‚ó¶
‚ó¶
√ó ‚ó¶
√ó ‚ó¶
‚ó¶
‚ó¶
√ó
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
.
Identify all trees. Suggest a monotone ordering for each tree.
11.8‚ãÜ
Prove that a symmetric positive deÔ¨Ånite matrix with the sparsity pattern
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
√ó
√ó
√ó
√ó
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
possesses a perfect Cholesky factorization.
11.9‚ãÜ
Theorem 11.1 states that the number of operations needed to form ‚Ñìk,j,
1 ‚â§j < k ‚â§d, is proportional to the number of sparse components under-
neath the diagonal of A. The purpose of this question is to prove a similar
statement with regard to the diagonal terms ‚Ñìk,k, k = 1, 2, . . . , d. To this

Exercises
249
end one might use the explicit formula
‚Ñì2
k,k = ak,k ‚àí
k‚àí1

i=1
‚Ñì2
k,i,
k = 1, 2, . . . , d,
and count the total number of nonzero terms ‚Ñì2
k,i in all the sums.


12
Classical iterative methods for sparse
linear equations
12.1
Linear one-step stationary schemes
The theme of this chapter is iterative solution of the linear system
Ax = b,
(12.1)
where A is a d √ó d real nonsingular matrix and b ‚ààRd. The most general iterative
method is a rule that for every k = 0, 1, . . . and x[0], x[1], . . . , x[k] ‚ààRd generates a
new vector x[k+1] ‚ààRd. In other words, it is a family of functions {hk}‚àû
k=0 such that
hk :
k+1 times
5
63
4
Rd √ó Rd √ó ¬∑ ¬∑ ¬∑ √ó Rd ‚ÜíRd,
k = 0, 1, . . . ,
and
x[k+1] = hk(x[0], x[1], . . . , x[k]),
k = 0, 1, . . .
(12.2)
The most fundamental question with regard to the scheme (12.2) is about its
convergence. Firstly, does it converge for every starting value x[0] ‚ààRd?1 Secondly,
provided that it converges, is the limit bound to be the true solution of the linear
system (12.1)?
Unless (12.2) always converges to the true solution the scheme is, obviously, un-
suitable. However, not all convergent iterative methods are equally good. Our main
consideration being to economize on computational cost, we must consider how fast
convergence takes place and what is the expense of each iteration.
An iterative scheme (12.2) is said to be linear if each hk is linear in all its ar-
guments.
It is m-step if hk depends solely on x[k‚àím+1], x[k‚àím+2], . . . , x[k], k =
m ‚àí1, m, . . . Finally, an m-step method is stationary if the function hk does not
vary with k for k ‚â•m ‚àí1. Each of these three concepts represents a considerable
simpliÔ¨Åcation, and it makes good sense to focus our eÔ¨Äort on the most elementary
model possible: a linear one-step stationary scheme. In that case (12.2) becomes
x[k+1] = Hx[k] + v,
(12.3)
1We are content when methods for nonlinear systems, e.g. functional iteration or the Newton‚Äì
Raphson algorithm (cf. Chapter 7), converge for a suitably large set of starting values. When it comes
to linear systems, however, we are more greedy!
251

252
Classical iterative methods
where the d√ód iteration matrix H and v ‚ààRd are independent of k. (Of course, both
H and v must depend on A and b, otherwise convergence is impossible.)
Lemma 12.1
Given an arbitrary linear system (12.1), a linear one-step stationary
scheme (12.3) converges to a unique bounded limit ÀÜx ‚ààRd, regardless of the choice
of starting value x[0], if and only if œÅ(H) < 1, where œÅ( ¬∑ ) denotes the spectral radius
(A.1.5.2). Provided that œÅ(H) < 1, ÀÜx is the correct solution of the linear system (12.1)
if and only if
v = (I ‚àíH)A‚àí1b.
(12.4)
Proof
Let us commence by assuming œÅ(H) < 1. In this case we claim that
lim
k‚Üí‚àûHk = O.
(12.5)
To prove this statement, we make the simplifying assumption that H has a complete
set of eigenvectors, hence that there exist a nonsingular d√ód matrix V and a diagonal
d√ód matrix D such that H = V DV ‚àí1 (A.1.5.3 and A.1.5.4). Hence H2 =

V DV ‚àí1
√ó

V DV ‚àí1
= V D2V ‚àí1, H3 = V D3V ‚àí1 and, in general, it is trivial to prove by
induction that Hk = V DkV ‚àí1, k = 0, 1, 2, . . . Therefore, passing to the limit,
lim
k‚Üí‚àûHk = V
	
lim
k‚Üí‚àûDk

V ‚àí1.
The elements along the diagonal of D are the eigenvalues of H, hence œÅ(H) < 1 implies
Dk k‚Üí‚àû
‚àí‚ÜíO and we deduce (12.5).
If the set of eigenvectors is incomplete, (12.5) can be proved just as easily by using
a Jordan factorization (see A.1.5.6 and Exercise 12.1).
Our next assertion is that
x[k] = Hkx[0] + (I ‚àíH)‚àí1(I ‚àíHk)v,
k = 0, 1, 2, . . . ;
(12.6)
note that œÅ(H) < 1 implies 1 Ã∏‚ààœÉ(H), where œÉ(H) is the set of all eigenvalues (the
spectrum) of H, therefore the inverse of I ‚àíH exists.
The proof is by induction. It is obvious that (12.6) is true for k = 0. Hence, let us
assume it for k ‚â•0 and attempt its veriÔ¨Åcation for k + 1. Using the deÔ¨Ånition (12.3)
of the iterative scheme in tandem with the induction assumption (12.6), we readily
obtain
x[k+1] = Hx[k] + v = H
%
Hkx[0] + (I ‚àíH)‚àí1(I ‚àíHk)v
&
+ v
= Hk+1x[0] +

(I ‚àíH)‚àí1(H ‚àíHk+1) + (I ‚àíH)‚àí1 (I ‚àíH)

v
= Hk+1x[0] + (I ‚àíH)‚àí1(I ‚àíHk+1)v
and the proof of (12.6) is complete.
Letting k ‚Üí‚àûin (12.6), (12.5) implies at once that the iterative process converges,
lim
k‚Üí‚àûx[k] = ÀÜx := (I ‚àíH)‚àí1v.
(12.7)

12.1
Linear one-step stationary schemes
253
We next consider the case œÅ(H) ‚â•1. Provided that 1 Ã∏‚ààœÉ(H), the matrix I ‚àíH
is invertible and ÀÜx = (I ‚àíH)‚àí1v is the only possible bounded limit of the iterative
scheme. For, suppose the existence of a bounded limit ÀÜy. Then
ÀÜy = lim
k‚Üí‚àûx[k+1] = H lim
k‚Üí‚àûx[k] + v = HÀÜy + v,
(12.8)
therefore ÀÜy = ÀÜx.
Even if 1 ‚ààœÉ(H), it remains true that every possible limit ÀÜy must obey (12.8).
To see this, let w be an eigenvector corresponding to the eigenvalue 1. Substitution
into (12.8) veriÔ¨Åes that ÀÜy + w is also a solution. Hence either there is no limit or the
limit is not unique and depends on the starting value; both cases are categorized as
‚Äòabsence of convergence‚Äô. We thus assume that 1 Ã∏‚ààœÉ(H).
Choose Œª ‚ààœÉ(H) such that |Œª| = œÅ(H) and let w be a unit-length eigenvector
corresponding to Œª: Hw = Œªw and ‚à•w‚à•= 1. (‚à•¬∑ ‚à•denotes here ‚Äì and elsewhere in
this chapter ‚Äì the Euclidean norm.)
We need to show that there always exists a starting value x[0] ‚ààRd for which the
scheme (12.3) fails to converge.
Case 1
Let Œª ‚ààR. Note that since Œª is real, so is w. We choose x[0] = w + ÀÜx and
claim that
x[k] = Œªkw + ÀÜx,
k = 0, 1, . . .
(12.9)
As we have already seen, (12.9) is true when k = 0. By induction,
x[k+1] = H

Œªkw + ÀÜx

+ v = ŒªkHw + (I ‚àíH)‚àí1[H + (I ‚àíH)]v
= Œªk+1w + ÀÜx,
k = 0, 1, . . . ,
and we deduce (12.9).
Because |Œª| ‚â•1, (12.9) implies that
‚à•x[k] ‚àíÀÜx‚à•= |Œª|k ‚â•1,
k = 0, 1, . . .
Therefore it is impossible for the sequence {x[k]}‚àû
k=0 to converge to ÀÜx.
Case 2
Suppose that Œª is complex. Therefore ¬ØŒª is also an eigenvalue of H (the
bar denotes complex conjugation). Since Hw = Œªw, complex conjugation implies
H ¬Øw = ¬ØŒª ¬Øw, hence ¬Øw must be a unit-length eigenvector corresponding to the eigenvalue
¬ØŒª. Furthermore ¬ØŒª Ã∏= Œª, hence w and ¬Øw must be linearly independent otherwise they
would correspond to the same eigenvalue. We deÔ¨Åne a function
g(z) := ‚à•zw + ¬Øz ¬Øw‚à•,
z ‚ààC.
It is trivial to verify that g : C ‚ÜíR is continuous, hence it attains its minimum in
every closed, bounded subset of C, in particular, in the unit circle. Therefore,
inf
‚àíœÄ‚â§Œ∏‚â§œÄ ‚à•eiŒ∏w + e‚àíiŒ∏ ¬Øw‚à•=
min
‚àíœÄ‚â§Œ∏‚â§œÄ ‚à•eiŒ∏w + e‚àíiŒ∏ ¬Øw‚à•= ŒΩ ‚â•0,
say. Suppose that ŒΩ = 0. Then there exists Œ∏0 ‚àà[‚àíœÄ, œÄ] such that
‚à•eiŒ∏0w + e‚àíiŒ∏0 ¬Øw‚à•= 0,

254
Classical iterative methods
therefore ¬Øw = e2iŒ∏0w, in contradiction to the linear independence of w and ¬Øw. Con-
sequently ŒΩ > 0.
The function g is homogeneous,
g(reiŒ∏) = r‚à•eiŒ∏w + e‚àíiŒ∏ ¬Øw‚à•= rg(eiŒ∏),
r > 0,
|Œ∏| ‚â§œÄ,
hence
g(z) = |z| g
	 z
|z|

‚â•ŒΩ|z|,
z ‚ààC \ {0}.
(12.10)
We let x[0] = w + ¬Øw + ÀÜx ‚ààRd. An inductive argument identical to the proof of
(12.9) aÔ¨Érms that
x[k] = Œªkw + ¬ØŒªk ¬Øw + ÀÜx,
k = 0, 1, . . .
Therefore, substituting into the inequality (12.10),
‚à•x[k] ‚àíÀÜx‚à•= g(Œªk) ‚â•|Œª|kŒΩ ‚â•ŒΩ > 0,
k = 0, 1, . . .
As in case 1, we obtain a sequence that is bounded away from its only possible limit,
ÀÜx; therefore it cannot converge.
To complete the proof, we need to demonstrate that (12.4) is true, but this is
trivial: the exact solution of (12.1) being x = A‚àí1b, (12.4) follows by substitution
into (12.8).
3 Incomplete LU factorization
Suppose that we can write the matrix A in
the form A = ÀúA‚àíE, the underlying assumption being that LU factorization of
the nonsingular matrix ÀúA can be evaluated with ease. For example, ÀúA might
be banded or (in the case of a symmetric sparsity structure) have a graph that
is a tree; see Chapter 11. Moreover, we assume that E is small in comparison
with ÀúA. Writing (12.1) in the form
ÀúAx = Ex + b
suggests the iterative scheme
ÀúAx[k+1] = Ex[k] + b,
k = 0, 1, . . . ,
(12.11)
incomplete LU factorization (ILU). Its implementation requires just a single
LU (or Cholesky, if ÀúA is symmetric) factorization, which can be reused in each
iteration.
To write (12.11) in the form (12.3), we let H = ‚àíÀúA‚àí1E and v = ÀúA‚àí1b.
Therefore
(I ‚àíH)A‚àí1b = (I ‚àíÀúA‚àí1E)( ÀúA ‚àíE)‚àí1b = ÀúA‚àí1( ÀúA ‚àíE)( ÀúA ‚àíE)‚àí1b
= ÀúA‚àí1b = v,
consistently with (12.4). Note that this deÔ¨Ånition of H and v is purely formal.
In reality we never compute them explicitly; we use (12.11) instead.
3

12.1
Linear one-step stationary schemes
255
The ILU iteration (12.11) is an example of a regular splitting. With greater generality,
we make the splitting A = P ‚àíN, where P is a nonsingular matrix, and consider the
iterative scheme
Px[k+1] = Nx[k] + b,
k = 0, 1, . . .
(12.12)
The underlying assumption is that a system having matrix P can be solved with ease,
whether by LU factorization or by other means. Note that, formally, H = P ‚àí1N =
P ‚àí1(P ‚àíA) = I ‚àíP ‚àí1A and v = P ‚àí1b.
Theorem 12.2
Suppose that both A and P + P ‚ä§‚àíA are symmetric and positive
deÔ¨Ånite. Then the method (12.12) converges.
Proof
Let Œª ‚ààC be an arbitrary eigenvalue of the iteration matrix H and suppose
that w is a corresponding eigenvector. Recall that H = I ‚àíP ‚àí1A, therefore
(I ‚àíP ‚àí1A)w = Œªw.
We multiply both sides by the matrix P, and this results in
(1 ‚àíŒª)Pw = Aw.
(12.13)
Our Ô¨Årst conclusion is that Œª Ã∏= 1, otherwise (12.13) implies Aw = 0, which contradicts
our assumption that A, being positive deÔ¨Ånite, is nonsingular.
We deduce further from (12.13) that
¬Øw‚ä§Aw = (1 ‚àíŒª) ¬Øw‚ä§Pw.
(12.14)
However, A is symmetric, therefore ¬Øy‚ä§Ay is real for every y ‚ààCd. Therefore, taking
conjugates in (12.14),
¬Øw‚ä§Aw = (1 ‚àíŒª) ¬Øw‚ä§Pw = (1 ‚àí¬ØŒª) ¬Øw‚ä§P ‚ä§w.
This, together with (12.14) and Œª Ã∏= 1, implies the identity
	
1
1 ‚àíŒª
+
1
1 ‚àí¬ØŒª
‚àí1

¬Øw‚ä§Aw = ¬Øw‚ä§(P + P ‚ä§‚àíA)w.
(12.15)
We note Ô¨Årst that
1
1 ‚àíŒª
+
1
1 ‚àí¬ØŒª
‚àí1 = 2 ‚àí2 Re Œª ‚àí|1 ‚àíŒª|2
|1 ‚àíŒª|2
= 1 ‚àí|Œª|2
|1 ‚àíŒª|2 ‚ààR.
Next, we let w = wR + iwI, where both wR and wI are real vectors, and we take the
real part of (12.15). On the left-hand side we obtain
Re

(wR ‚àíiwI)‚ä§A(wR + iwI)

= w‚ä§
RAwR + w‚ä§
I AwI
and a similar identity is true on the right-hand side with A replaced by P + P ‚ä§‚àíA.
Therefore
1 ‚àí|Œª|2
|1 ‚àíŒª|2

w‚ä§
RAwR + w‚ä§
I AwI

= w‚ä§
R(P +P ‚ä§‚àíA)wR+w‚ä§
I (P +P ‚ä§‚àíA)wI.
(12.16)

256
Classical iterative methods
Recall that both A and P + P ‚ä§‚àíA are positive deÔ¨Ånite. It is impossible for both wR
and wI to vanish (since this would imply that w = 0), therefore
w‚ä§
RAwR + w‚ä§
I AwI, w‚ä§
R(P + P ‚ä§‚àíA)wR + w‚ä§
I (P + P ‚ä§‚àíA)wI > 0.
We therefore conclude from (12.16) that
1 ‚àí|Œª|2
|1 ‚àíŒª|2 > 0,
hence |Œª| < 1. This is true for every Œª ‚ààœÉ(H), consequently œÅ(H) < 1 and we use
Lemma 12.1 to argue that the iterative scheme (12.12) converges.
3 Tridiagonal matrices
A relatively simple demonstration of the power of
Theorem 12.2 is provided by tridiagonal matrices. Thus, let us suppose that
the d √ó d symmetric matrix
A =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
Œ±1
Œ≤1
0
¬∑ ¬∑ ¬∑
0
Œ≤1
Œ±2
Œ≤2
...
...
0
...
...
...
0
...
...
Œ≤d‚àí2
Œ±d‚àí1
Œ≤d‚àí1
0
¬∑ ¬∑ ¬∑
0
Œ≤d‚àí1
Œ±d
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
is positive deÔ¨Ånite. Our claim is that the regular splittings
P =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é£
Œ±1
0
¬∑ ¬∑ ¬∑
0
0
Œ±2
...
...
...
...
...
0
0
¬∑ ¬∑ ¬∑
0
Œ±d
‚é§
‚é•‚é•‚é•‚é•‚é¶
,
N = ‚àí
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
0
Œ≤1
0
¬∑ ¬∑ ¬∑
0
Œ≤1
0
Œ≤2
...
...
0
...
...
...
0
...
...
Œ≤d‚àí2
0
Œ≤d‚àí1
0
¬∑ ¬∑ ¬∑
0
Œ≤d‚àí1
0
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
(12.17)
and
P =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
Œ±1
0
¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
0
Œ≤1
Œ±2
0
...
0
...
...
...
...
...
...
Œ≤d‚àí2
Œ±d‚àí1
0
0
¬∑ ¬∑ ¬∑
0
Œ≤d‚àí1
Œ±d
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
,
N = ‚àí
‚é°
‚é¢‚é¢‚é¢‚é¢‚é£
0
Œ≤1
¬∑ ¬∑ ¬∑
0
0
0
...
...
...
...
...
Œ≤d‚àí1
0
¬∑ ¬∑ ¬∑
0
0
‚é§
‚é•‚é•‚é•‚é•‚é¶
(12.18)
‚Äì the Jacobi splitting and the Gauss‚ÄìSeidel splitting respectively ‚Äì result in
convergent schemes (12.12).
Since A is positive deÔ¨Ånite, Theorem 12.2 and the positive deÔ¨Åniteness of the
matrix Q := P + P ‚ä§‚àíA imply convergence. For the splitting (12.17) we

12.1
Linear one-step stationary schemes
257
readily obtain
Q =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
Œ±1
‚àíŒ≤1
0
¬∑ ¬∑ ¬∑
0
‚àíŒ≤1
Œ±2
‚àíŒ≤2
...
...
0
...
...
...
0
...
...
‚àíŒ≤d‚àí2
Œ±d‚àí1
‚àíŒ≤d‚àí1
0
¬∑ ¬∑ ¬∑
0
‚àíŒ≤d‚àí1
Œ±d
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
.
Our claim is that Q is indeed positive deÔ¨Ånite, and this follows from the
positive deÔ¨Åniteness of A. SpeciÔ¨Åcally, A is positive deÔ¨Ånite if and only if
x‚ä§Ax > 0 for all x ‚ààRd, x Ã∏= 0 (A.1.3.5). But
x‚ä§Ax =
d

j=1
Œ±2
jx2
j + 2
d‚àí1

j=1
Œ≤jxjxj+1 =
d

j=1
Œ±2
jy2
j ‚àí2
d‚àí1

j=1
Œ≤jyjyj+1 = y‚ä§Qy,
where yj = (‚àí1)jxj, j = 1, 2, . . . , d.
Therefore y‚ä§Qy > 0 for every y ‚àà
Rd \ {0} and we deduce that the matrix Q is indeed positive deÔ¨Ånite.
The proof for (12.18) is, if anything, even easier, since Q is simply the diagonal
matrix
Q =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é£
Œ±1
0
¬∑ ¬∑ ¬∑
0
0
Œ±2
...
...
...
...
...
0
0
¬∑ ¬∑ ¬∑
0
Œ±d
‚é§
‚é•‚é•‚é•‚é•‚é¶
.
Since A is positive deÔ¨Ånite and Œ±j = e‚ä§
j Aej > 0, j = 1, 2, . . ., where ej ‚ààRd
is the jth unit vector, j = 1, 2, . . . , d, it follows at once that Q also is positive
deÔ¨Ånite.
Figure 12.1 displays the error in the solution of a d √ó d tridiagonal system
with
Œ±1 = d,
Œ±j = 2j(d ‚àíj) + d,
j = 2, 3, . . . , d,
Œ≤j = ‚àíj(d ‚àíj),
j = 1, 2, . . . , d ‚àí1,
bj ‚â°1,
x[0]
j
‚â°0,
j = 1, 2, . . . , d.
(12.19)
It is trivial to use the GerÀásgorin criterion (Lemma 8.3) to prove that the
underlying matrix A is positive deÔ¨Ånite.
The system has been solved with both the Jacobi splitting (12.17) (upper
row in the Ô¨Ågure) and the Gauss‚ÄìSeidel splitting (12.18) (lower row) for d =
10, 20, 30.
Even superÔ¨Åcial examination of the Ô¨Ågure reveals a number of
interesting features.
‚Ä¢ Both Jacobi and Gauss‚ÄìSeidel converge. This should come as no sur-
prise since, as we have just proved, provided A is tridiagonal its positive
deÔ¨Åniteness is suÔ¨Écient for both methods to converge.

258
Classical iterative methods
0
200
400
600
800
1000
0
1
2
3
4
5
6
Jacobi
Linear scale
0
200
400
600
800
1000
‚àí30
‚àí25
‚àí20
‚àí15
‚àí10
‚àí5
0
Logarithmic scale
0
200
400
600
800
1000
0
1
2
3
4
5
6
Gauss‚àíSeidel
0
200
400
600
800
1000
‚àí30
‚àí25
‚àí20
‚àí15
‚àí10
‚àí5
0
Figure 12.1
The error vs. the number of iterations in the Jacobi and Gauss‚ÄìSeidel
splittings for the system (12.19) with d = 10 (dotted line), d = 20 (broken-and-
dotted line) and d = 40 (solid line). The Ô¨Årst column displays the error (in the
Euclidean norm) on a linear scale; the second column shows its logarithm.
‚Ä¢ Convergence proceeds at a geometric speed; this is obvious from the
second column, since the logarithm of the error is remarkably close to a
linear function. This is not very surprising either since it is implicit in
the proof of Lemma 12.1 (and made explicit in Exercise 12.2) that, at
least asymptotically, the error decays like [œÅ(H)]k.
‚Ä¢ The rate of convergence is slow and deteriorates markedly as d increases.
This is a worrying feature since in practical computation we are inter-
ested in equations of considerably larger size than d = 40.
‚Ä¢ Gauss‚ÄìSeidel is better than Jacobi. Actually, careful examination of the
rate of decay (which, obviously, is more transparent in the logarithmic
scale) reveals that the error of Gauss‚ÄìSeidel decays at twice the speed of
Jacobi! In other words, we need just half the steps to attain the speciÔ¨Åed
accuracy.
In the next section we will observe that the disappointing rate of decay of
both methods, as well as the better performance of Gauss‚ÄìSeidel, represent

12.2
Classical iterative methods
259
a fairly general state of aÔ¨Äairs, rather than being just a feature of the linear
system (12.19).
3
12.2
Classical iterative methods
Let A be a real d √ó d matrix. We split it as shown below:
A
=
D
‚àí
L0
‚àí
U0
‚é°
‚é¢‚é¢‚é£
‚é§
‚é•‚é•‚é¶
‚é°
‚é¢‚é¢‚é£
@
@
@
@
‚é§
‚é•‚é•‚é¶
‚é°
‚é¢‚é¢‚é£
@
@
@@
‚é§
‚é•‚é•‚é¶
‚é°
‚é¢‚é¢‚é£
@
@
@@
‚é§
‚é•‚é•‚é¶.
Here the d √ó d matrices D, L0 and U0 are the diagonal, minus the strictly lower-
triangular and minus the strictly upper-triangular portions of A, respectively. We
assume that aj,j Ã∏= 0, j = 1, 2, . . . , d. Therefore D is nonsingular and we let
L := D‚àí1L0,
U := D‚àí1U0.
The Jacobi iteration is deÔ¨Åned by setting in (12.3)
H = B := L + U,
v := D‚àí1b
(12.20)
or, equivalently, considering a regular splitting (12.12) with P = D, N = L0 + U0.
Likewise, we deÔ¨Åne the Gauss‚ÄìSeidel iteration by specifying
H = L := (I ‚àíL)‚àí1U,
v := (I ‚àíL)‚àí1D‚àí1b,
(12.21)
and this is the same as the regular splitting P = D ‚àíL0, N = U0. Observe that
(12.17) and (12.18) are nothing other than the Jacobi and Gauss‚ÄìSeidel splittings,
respectively, as applied to tridiagonal matrices.
The list of classical iterative schemes would be incomplete without mentioning the
successive over-relaxation (SOR) scheme, which is deÔ¨Åned by setting
H = Lœâ := (I ‚àíœâL)‚àí1[(1 ‚àíœâ)I + œâU],
v = œâ(I ‚àíœâL)‚àí1D‚àí1b,
(12.22)
where œâ ‚àà[1, 2) is a parameter. Although this might not be obvious at a glance, the
SOR scheme can be represented alternatively as a regular splitting with
P = 1
œâ D ‚àíL0,
N =
	 1
œâ ‚àí1

D + U0
(12.23)
(see Exercise 12.4).
Note that Gauss‚ÄìSeidel is simply a special case of SOR, with œâ = 1. However, it
makes good sense to single it out for special treatment.
All three methods (12.20)‚Äì(12.22) are consistent with (12.4), therefore Lemma 12.1
implies that if they converge, the limit is necessarily the true solution of the linear
system.

260
Classical iterative methods
The ‚ÄòH‚Äìv‚Äô notation is helpful within the framework of Lemma 12.1 but on the
whole it is somewhat opaque. The three methods can be presented in a much simpler
manner. Thus, writing the system (12.1) in the form
d

j=1
a‚Ñì,jxj = b‚Ñì,
‚Ñì= 1, 2, . . . ,
the Jacobi iteration reads
‚Ñì‚àí1

j=1
a‚Ñì,jx[k]
j
+ a‚Ñì,‚Ñìx[k+1]
‚Ñì
+
d

j=‚Ñì+1
a‚Ñì,jx[k]
j
= b‚Ñì,
‚Ñì= 1, 2, . . . , d,
k = 0, 1, . . .
while the Gauss‚ÄìSeidel scheme becomes
‚Ñì

j=1
a‚Ñì,jx[k+1]
j
+
d

j=‚Ñì+1
a‚Ñì,jx[k]
j
= b‚Ñì,
‚Ñì= 1, 2, . . . , d,
k = 0, 1, . . .
In other words, the main diÔ¨Äerence between Jacobi and Gauss‚ÄìSeidel is that in the
Ô¨Årst we always express each new component of x[k+1] solely in terms of x[k], while
in the latter we use the elements of x[k+1] whenever they are available. This is an
important distinction as far as implementation is concerned. In each iteration (12.20)
we need to store both x[k] and x[k+1] and this represents a major outlay in terms of
computer storage ‚Äì recall that x is a ‚Äòstretched‚Äô computational grid. (Of course, we do
not store or even generate the matrix A if it originates in highly sparse Ô¨Ånite diÔ¨Äerence
or Ô¨Ånite element equations. Instead, we need to know the ‚Äòrule‚Äô for constructing each
linear equation, e.g. the Ô¨Åve-point formula. If, however, A originates in a spectral
method, we generate A and multiply it by vectors in the usual manner ‚Äì but recall
that for spectral methods the matrices are signiÔ¨Åcantly smaller!) Clever programming
and exploitation of the sparsity pattern can reduce the required amount of storage
but this cannot ever compete with (12.21): in Gauss‚ÄìSeidel we can throw away any
‚Ñìth component of x[k] as soon as x[k+1]
‚Ñì
has been generated, so both quantities can
share the same storage.
The SOR iteration (12.22) can be also written in a similar form.
Multiplying
(12.23) by œâ results in
œâ
‚Ñì‚àí1

j=1
a‚Ñì,jx[k+1]
j
+ a‚Ñì,‚Ñìx[k+1]
‚Ñì
+ (œâ ‚àí1)a‚Ñì,‚Ñìx[k]
‚Ñì
+ œâ
d

j=‚Ñì+1
a‚Ñì,jx[k]
j
= œâb‚Ñì,
‚Ñì= 1, 2, . . . , d,
k = 0, 1, . . .
Although precise estimates depend on the sparsity pattern of A, it is apparent that
the cost of a single SOR iteration is not substantially larger than its counterpart for
either Jacobi or Gauss‚ÄìSeidel. Moreover, SOR shares with Gauss‚ÄìSeidel the important
virtue of requiring just a single copy of x to be stored at any one time.
The SOR iteration and its special case, the Gauss‚ÄìSeidel method, share another
feature. Their precise deÔ¨Ånition depends upon the ordering of the equations and the

12.2
Classical iterative methods
261
unknowns. As we have already seen in Chapter 11, the rearrangement of equations
and unknowns is tantamount to acting on A with permutation matrices on the left
and on the right respectively, and these two operations, in general, result in diÔ¨Äerent
iterative schemes. It is entirely possible that one of these arrangements converges,
while the other fails to do so!
We have already observed in Section 12.1 that both Jacobi and Gauss‚ÄìSeidel con-
verge whenever A is a tridiagonal symmetric positive deÔ¨Ånite matrix and it is not
diÔ¨Écult to verify that this is also the case with SOR for every 1 ‚â§œâ < 2 (cf. Exer-
cise 12.5).
As far as convergence is concerned, Jacobi and Gauss‚ÄìSeidel share similar be-
haviour for a wide range of linear systems. Thus, let A be strictly diagonally dominant.
This means that
|a‚Ñì,‚Ñì| ‚â•
d

j=1
jÃ∏=‚Ñì
|a‚Ñì,j|,
‚Ñì= 1, 2, . . . , d
(12.24)
and the inequality is sharp for at least one ‚Ñì‚àà{1, 2, . . . , d}. (Some deÔ¨Ånitions require
sharp inequality for all ‚Ñì, but the present, weaker, deÔ¨Ånition is just perfect for our
purposes.)
Theorem 12.3
If the matrix A is irreducible and strictly diagonally dominant then
both the Jacobi and Gauss‚ÄìSeidel methods converge.
Proof
According to (12.20) and (12.24),
d

j=1
|b‚Ñì,j| =
d

j=1, jÃ∏=‚Ñì
|b‚Ñì,j| =
1
|a‚Ñì,‚Ñì|
d

j=1, jÃ∏=‚Ñì
|a‚Ñì,j| ‚â§1,
‚Ñì= 1, 2, . . . , d
and the inequality is sharp for at least one ‚Ñì. Therefore œÅ(B) < 1 by the GerÀásgorin
criterion (Lemma 8.3) and the Jacobi iteration converges.
The proof for Gauss‚ÄìSeidel is slightly more complicated; essentially, we need to
revisit the proof of Lemma 8.3 (i.e., Exercise 8.8) in a diÔ¨Äerent framework. Choose
Œª ‚ààœÉ(L1) with an eigenvector w. Therefore, multiplying L and v from (12.21) Ô¨Årst
by I ‚àíL and then by D,
U0w = Œª(D ‚àíL0)w.
It is convenient to rewrite this in the form
Œªa‚Ñì,‚Ñìw‚Ñì=
d

j=‚Ñì+1
a‚Ñì,jwj ‚àíŒª
‚Ñì‚àí1

j=1
a‚Ñì,jwj,
‚Ñì= 1, 2, . . . , d.
Therefore, by the triangle inequality,
|Œª| |a‚Ñì,‚Ñì| |w‚Ñì| ‚â§
d

j=‚Ñì+1
|a‚Ñì,j| |wj| + |Œª|
‚Ñì‚àí1

j=1
|a‚Ñì,j| |wj|
(12.25)
‚â§
‚éõ
‚éù
d

j=‚Ñì+1
|a‚Ñì,j| + |Œª|
‚Ñì‚àí1

j=1
|a‚Ñì,j|
‚éû
‚é†
max
j=1,2,...,d |wj|,
j = 1, 2, . . . , d.
(12.26)

262
Classical iterative methods
Let Œ± ‚àà{1, 2, . . . , d} be such that
|wŒ±| =
max
j=1,2,...,d |wj| > 0.
Substituting into (12.26), we have
|Œª| |aŒ±,Œ±| ‚â§
d

j=Œ±+1
|aŒ±,j| + |Œª|
Œ±‚àí1

j=1
|aŒ±,j|.
Let us assume that |Œª| ‚â•1. We deduce that
|aŒ±,Œ±| ‚â§
d

j=1
jÃ∏=Œ±
|aŒ±,j|,
and this can be consistent with (12.24) only if the weak inequality holds as an equality.
Substitution in (12.25), in tandem with |Œª| ‚â•1, results in
|Œª|
d

j=1
jÃ∏=Œ±
|aŒ±,j| |wŒ±| ‚â§
d

j=Œ±+1
|aŒ±,j| |wj| + |Œª|
Œ±‚àí1

j=1
|aŒ±,j| |wj| ‚â§|Œª|
d

j=1
jÃ∏=Œ±
|aŒ±,j| |wj|.
This, however, can be consistent with |wŒ±| = max |wj| only if |w‚Ñì| = |wŒ±|, ‚Ñì=
1, 2, . . . , d. Therefore, every ‚Ñì‚àà{1, 2, . . . , d} can play the role of Œ± and
|a‚Ñì,‚Ñì| =
d

j=1
jÃ∏=‚Ñì
|a‚Ñì,j|,
‚Ñì= 1, 2, . . . , d,
in deÔ¨Åance of the deÔ¨Ånition of strict diagonal dominance. Therefore, having been led
to a contradiction, our assumption that |Œª| ‚â•1 must be wrong.
We deduce that
œÅ(L1) < 1, hence Lemma 12.1 implies convergence.
Another, less trivial, example where Jacobi and Gauss‚ÄìSeidel converge in tandem
is provided in the following theorem, which we state without proof.
Theorem 12.4 (The Stein‚ÄìRosenberg theorem)
Suppose that a‚Ñì,‚ÑìÃ∏= 0, ‚Ñì=
1, 2, . . . , d, and that all the components of B are nonnegative. Then one of the Ô¨Çoowing
holds:
œÅ(L1) = œÅ(B) = 0
or
œÅ(L1) < œÅ(B) < 1
or
œÅ(L1) = œÅ(B) = 1
or
œÅ(L1) > œÅ(B) > 1.
Hence, the Jacobi and Gauss‚ÄìSeidel methods are either simultaneously convergent or
simultaneously divergent.
3 An example of divergence
Lest there should be an impression that
iterative methods are bound to converge or that they always share similar

12.2
Classical iterative methods
263
behaviour with regard to convergence, we give here a trivial counterexample,
A =
‚é°
‚é£
3
2
1
2
3
2
1
2
3
‚é§
‚é¶.
The matrix is symmetric and positive deÔ¨Ånite; its eigenvalues are 2 and
1
2(7 ¬±
‚àö
33) > 0. It is easy to verify, either directly or from Theorem 12.2
or Exercise 12.3, that œÅ(L1) < 1 and the Gauss‚ÄìSeidel method converges.
However, œÅ(B) = 1
6(1 +
‚àö
33) > 1 and the Jacobi method diverges.
An interesting variation on the last example is provided by the matrix
A =
‚é°
‚é£
3
1
2
‚àí1
3
‚àí2
‚àí2
2
3
‚é§
‚é¶.
The spectrum of B is {0, ¬±i}, therefore the Jacobi method diverges marginally.
Gauss‚ÄìSeidel, however, proudly converges, since œÉ(L1) =

0, 1
54(‚àí23 ¬±
‚àö
97)

‚àà(0, 1).
Let us exchange the second and third rows and the second and third columns
of A. The outcome is
‚é°
‚é£
3
2
1
‚àí2
3
2
‚àí1
‚àí2
3
‚é§
‚é¶.
The spectral radius of Jacobi remains intact, since it does not depend upon
ordering. However, the eigenvalues of the new L1 are 0 and
1
54(‚àí31¬±
‚àö
1393),
the spectral radius exceeds unity and the iteration diverges.
This demonstrates not just the sensitivity of (12.21) to ordering but also
that the Jacobi iteration need not be the underachieving sibling of Gauss‚Äì
Seidel; by replacing 3 with 3 + Œµ, where 0 < Œµ ‚â™1, along the diagonal, we
render Jacobi convergent (this is an immediate consequence of the GerÀásgorin
criterion), while continuity of the eigenvalues of L1 as a function of Œµ means
that Gauss‚ÄìSeidel (in the second ordering) still diverges.
3
To gain intuition with respect to the behaviour of classical iterative methods, let us
Ô¨Årst address ourselves in some detail to the matrix
A =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
‚àí2
1
0
¬∑ ¬∑ ¬∑
0
1
‚àí2
1
...
...
0
...
...
...
0
...
...
1
‚àí2
1
0
¬∑ ¬∑ ¬∑
0
1
‚àí2
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
.
(12.27)
It is clear why such an A is relevant to our discussion: it is obtained from a central
diÔ¨Äerence approximation to a second derivative.

264
Classical iterative methods
A d √ó d matrix T = (tk,‚Ñì)d
k,‚Ñì=1 is said to be Toeplitz if it is constant along all its
diagonals, in other words, if there exist numbers œÑ‚àíd+1, œÑ‚àíd+2, . . . , œÑ0, . . . , œÑd‚àí1 such
that
tk,‚Ñì= œÑk‚àí‚Ñì,
k, ‚Ñì= 1, 2, . . . , d.
The matrix A, (12.27), is a Toeplitz matrix with
œÑ‚àíd+1 = ¬∑ ¬∑ ¬∑ = œÑ‚àí2 = 0,
œÑ‚àí1 = 1,
œÑ0 = ‚àí2,
œÑ1 = 1,
œÑ2 = ¬∑ ¬∑ ¬∑ = œÑd‚àí1 = 0.
We say that a matrix is TST if it is Toeplitz, symmetric and tridiagonal. Therefore,
T is TST if
œÑj = 0,
|j| = 2, 3, . . . , d ‚àí1,
œÑ‚àí1 = œÑ1.
Matrices that are TST are important both for fast solution of the Poisson equation
(see Chapter 14) and for the stability analysis of discretized PDEs of evolution (see
Chapter 16) but, in the present context, we merely note that the matrix A, (12.27),
is TST.
Lemma 12.5
Let T be a d √ó d TST matrix and Œ± := t0, Œ≤ := t‚àí1 = t1. Then the
eigenvalues of T are
Œªj = Œ± + 2Œ≤ cos
	 œÄj
d + 1

,
j = 1, 2, . . . , d,
(12.28)
each with corresponding orthogonal eigenvector qj, where
qj,‚Ñì=
>
2
d + 1 sin
	 œÄj‚Ñì
d + 1

,
j, ‚Ñì= 1, 2, . . . , d.
(12.29)
Proof
Although it is an easy matter to verify (12.28) and (12.29) directly from
the deÔ¨Ånition of a TST matrix, we adopt a more roundabout approach since this will
pay dividends later in this section.
We assume that Œ≤ Ã∏= 0, otherwise T reduces to a multiple of the identity matrix and
the lemma is trivial. Let us suppose that Œª is an eigenvalue of T with corresponding
eigenvector q. Letting q0 = qd+1 = 0, we can write Aq = Œªq in the form
Œ≤q‚Ñì‚àí1 + Œ±q‚Ñì+ Œ≤q‚Ñì+1 = Œªq‚Ñì,
‚Ñì= 1, 2, . . . , d
or, after a minor rearrangement,
Œ≤q‚Ñì+1 + (Œ± ‚àíŒª)q‚Ñì+ Œ≤q‚Ñì‚àí1 = 0,
‚Ñì= 1, 2, . . . , d.
This is a special case of a diÔ¨Äerence equation (4.19) and its general solution is
q‚Ñì= aŒ∑‚Ñì
+ + bŒ∑‚Ñì
‚àí,
‚Ñì= 0, 1, . . . , d + 1,
where Œ∑¬± are the zeros of the characteristic polynomial
Œ≤Œ∑2 + (Œ± ‚àíŒª)Œ∑ + Œ≤ = 0.

12.2
Classical iterative methods
265
In other words,
Œ∑¬± = 1
2Œ≤

Œª ‚àíŒ± ¬±
0
(Œª ‚àíŒ±)2 ‚àí4Œ≤2

.
(12.30)
The constants a and b are determined by requiring q0 = qd+1 = 0. The Ô¨Årst condition
yields a + b = 0, therefore q‚Ñì= a(Œ∑‚Ñì
+ ‚àíŒ∑‚Ñì
‚àí) where a Ã∏= 0 is arbitrary. To fulÔ¨Ål the
second condition we need
Œ∑d+1
+
= Œ∑d+1
‚àí
.
There are d + 1 roots to this equation, namely
Œ∑+ = Œ∑‚àíexp
	 2œÄij
d + 1

,
j = 0, 1, . . . , d,
(12.31)
but we can discard at once the case j = 0, since it corresponds to Œ∑‚àí= Œ∑+, hence to
q‚Ñì‚â°0.
We multiply (12.31) by exp[‚àíœÄij/(d + 1)] and substitute the values of Œ∑¬± from
(12.30). Therefore

Œª ‚àíŒ± +
0
(Œª ‚àíŒ±)2 ‚àí4Œ≤2

exp
	 ‚àíœÄij
d + 1

=

Œª ‚àíŒ± ‚àí
0
(Œª ‚àíŒ±)2 ‚àí4Œ≤2

exp
	 œÄij
d + 1

for some j ‚àà{1, 2, . . . , d}. Rearrangement yields
0
(Œª ‚àíŒ±)2 ‚àí4Œ≤2 cos
	 œÄj
d + 1

= (Œª ‚àíŒ±)i sin
	 œÄj
d + 1

and, squaring, we deduce

(Œª ‚àíŒ±)2 ‚àí4Œ≤2
cos2
	 œÄj
d + 1

= ‚àí(Œª ‚àíŒ±)2 sin2
	 œÄj
d + 1

.
Therefore
(Œª ‚àíŒ±)2 = 4Œ≤2 cos2
	 œÄj
d + 1

and, taking the square root, we obtain
Œª = Œ± ¬± 2Œ≤ cos
	 œÄj
d + 1

.
Taking the plus sign we recover (12.28) with Œª = Œªj, while the minus repeats Œª =
Œªd+1‚àíj and can be discarded. This concurs with the stipulated form of the eigenvalues.
Substituting (12.28) into (12.30), we readily obtain
Œ∑¬± = cos
	 œÄj
d + 1

¬± i sin
	 œÄj
d + 1

= exp
	 ¬±œÄij
d + 1

,
therefore
qj,‚Ñì= a(Œ∑‚Ñì
+ ‚àíŒ∑‚Ñì
‚àí) = 2ai sin
	 œÄj‚Ñì
d + 1

,
j, ‚Ñì= 1, 2, . . . , d.

266
Classical iterative methods
This will demonstrate that (12.29) is true if we can determine a value of a such that
d

‚Ñì=1
q2
j,‚Ñì= 1
(note that symmetry implies that the eigenvectors are orthogonal, A.1.3.2). It is an
easy exercise to show that
d

‚Ñì=1
sin2
	 œÄj‚Ñì
d + 1

= 1
2(d + 1),
j = 1, 2, . . . , d,
thereby providing a value of a that is consistent with (12.29).
Corollary
All d √ó d TST matrices commute with each other.
Proof
According to (12.29), all such matrices share the same set of eigenvectors,
hence they commute (A.1.5.4).
Let us now return to the matrix A and to our discussion of classical iterative
methods. It follows at once from (12.20) that the iteration matrix B is also a TST
matrix, with Œ± = 0 and Œ≤ = 1
2. Therefore
œÅ(B) = cos
	
œÄ
d + 1

‚âà1 ‚àíœÄ2
2d2 < 1.
(12.32)
In other words, the Jacobi method converges; but we already know this from Sec-
tion 12.1. However, (12.32) gives us an extra morsel of information, namely the speed
of convergence. The news is not very good, unfortunately: the error is attenuated
by O

d‚àí2
in each iteration. In other words, if d is large, convergence up to any
reasonable tolerance is very slow indeed.
Instead of debating Gauss‚ÄìSeidel, we next leap all the way to the SOR scheme ‚Äì
after all, Gauss‚ÄìSeidel is nothing other than SOR with œâ = 1. Although the matrix
Lœâ is no longer Toeplitz, symmetric or tridiagonal, the method of proof of Lemma 12.5
is equally eÔ¨Äective. Thus, let Œª ‚ààœÉ(Lœâ) and denote by q a corresponding eigenvector.
It follows from (12.22) that
[(1 ‚àíœâ)I + œâU] q = Œª(I ‚àíœâL)q.
Therefore, letting q0 = qd+1 = 0, we obtain
‚àí2(1 ‚àíœâ)q‚Ñì‚àíœâq‚Ñì+1 = Œª(œâq‚Ñì‚àí1 ‚àí2q‚Ñì),
‚Ñì= 1, 2, . . . , d,
which we again rewrite as a diÔ¨Äerence equation,
œâq‚Ñì+1 ‚àí2(Œª ‚àí1 + œâ)q‚Ñì+ œâŒªq‚Ñì‚àí1 = 0,
‚Ñì= 1, 2, . . . , d.
The solution is once more
q‚Ñì= a(Œ∑‚Ñì
+ ‚àíŒ∑‚Ñì
‚àí),
‚Ñì= 0, 1, . . . , d + 1,

12.2
Classical iterative methods
267
except that (12.30) needs to be replaced by
Œ∑¬± = Œª ‚àí1 + œâ ¬±
0
(Œª ‚àí1 + œâ)2 ‚àíœâ2Œª.
(12.33)
We set Œ∑d+1
+
= Œ∑d+1
‚àí
and proceed as in the proof of Lemma 12.5. Substitution of
the values of Œ∑¬± from (12.33) results in
(Œª ‚àí1 + œâ)2 = œâ2Œ∫Œª,
(12.34)
where
Œ∫ = cos2
	 œÄ‚Ñì
d + 1

for some ‚Ñì‚àà{1, 2, . . . , d}.
In the special case of Gauss‚ÄìSeidel, (12.34) yields
Œª2 = œâ2Œ∫Œª
and we deduce that
0, cos2
	
œÄ
d + 1

, cos2
	 2œÄ
d + 1

, . . . , cos2
	 dœÄ
d + 1

‚äÜœÉ(L1) ‚äÜ{0} ‚à™
#
cos2
	 œÄ‚Ñì
d + 1

: ‚Ñì= 1, 2, . . . , d
$
.
In particular,
œÅ(L1) = cos2
	
œÄ
d + 1

‚âà1 ‚àíœÄ2
d2 .
(12.35)
Comparison with (12.32) demonstrates that, as far as the speciÔ¨Åc matrix A is con-
cerned, Gauss‚ÄìSeidel converges at exactly twice the rate of Jacobi. In other words,
each iteration of Gauss‚ÄìSeidel is, at least asymptotically, as eÔ¨Äective as two iterations
of Jacobi!
Recall that Gauss‚ÄìSeidel also has important advantages over Jacobi in
terms of storage, while the number of operations in each iteration is identical in the
two schemes. Thus, remarkably, it appears that Gauss‚ÄìSeidel wins on every score.
There is, however, an important reason why the Jacobi iteration is of interest and we
address ourselves to this theme later in this section. At present, we wish to debate
the convergence of SOR for diÔ¨Äerent values of œâ ‚àà[1, 2). Note that our goal is not
merely to check the convergence and assess its speed. The whole point of using SOR
with an optimal value of œâ, rather than Gauss‚ÄìSeidel, rests in the exploitation of the
parameter to accelerate convergence. We already know from (12.35) that a particular
choice of œâ is associated with convergence; now we seek to improve upon this result
by identifying œâopt, the optimal value of œâ.
We distinguish between the following cases.
Case 1
Œ∫œâ2 ‚â§4(œâ ‚àí1), hence the roots of (12.34) form a complex conjugate pair. It
is easy, substituting the explicit value of Œ∫, to verify that this is indeed the case when
2 1 ‚àí
 sin[œÄ‚Ñì/(d + 1)]

cos2[œÄ‚Ñì/(d + 1)]
‚â§œâ ‚â§2 1 +
 sin[œÄ‚Ñì/(d + 1)]

cos2[œÄ‚Ñì/(d + 1)]
.

268
Classical iterative methods
Moreover,
1 +
 sin[œÄ‚Ñì/(d + 1)]

cos2[œÄ‚Ñì/(d + 1)]
‚â•1
and we restrict our attention to œâ ‚â§2. Therefore case 1 corresponds to
Àúœâ := 2 1 ‚àí
 sin[œÄ‚Ñì/(d + 1)]

cos2[œÄ‚Ñì/(d + 1)]
‚â§œâ < 2.
(12.36)
The two solutions of (12.34) are
Œª = 1 ‚àíœâ + 1
2Œ∫œâ2 ¬±
/
1 ‚àíœâ + 1
2Œ∫œâ22 ‚àí(1 ‚àíœâ)2;
(12.37)
consequently
|Œª|2 =

1 ‚àíœâ + 1
2Œ∫œâ22 +
%
(1 ‚àíœâ)2 ‚àí

1 ‚àíœâ + 1
2Œ∫œâ22&
= (œâ ‚àí1)2
and we obtain
|Œª| = œâ ‚àí1.
(12.38)
Case 2
Œ∫œâ2 ‚â•4(œâ ‚àí1) and both zeros of (12.34) are real. DiÔ¨Äerentiating (12.34)
with respect to œâ yields
2(Œª ‚àí1 + œâ)(Œªœâ + 1) = 2Œ∫œâŒª + Œ∫œâ2Œªœâ,
where Œªœâ = dŒª/ dœâ. Therefore Œªœâ may vanish only for
Œª = 1 ‚àíœâ
1 ‚àíŒ∫œâ .
Substitution into (12.34) results in Œ∫(1 ‚àíœâ) = 1 ‚àíŒ∫œâ, hence in Œ∫ = 1 ‚Äì but this is
impossible, because Œ∫ = cos2[œÄ‚Ñì/(d + 1)] ‚àà(0, 1). Therefore Œªœâ Ã∏= 0 in
1 < œâ ‚â§Àúœâ
(cf. (12.36)) and the zeros of (12.34) are monotone in this interval. Since they are
continuous functions of œâ, it follows that, to track the zero of largest magnitude, it is
enough to restrict attention to the endpoints 1 and Àúœâ.
At œâ = 1 we are back to the Gauss‚ÄìSeidel case and the spectral radius is given by
(12.35). At the other endpoint (which is the meeting point of cases 1 and 2) we have
Œ∫œâ2 ‚àí4œâ + 4 = 0, hence
œâ = 2

1 ‚àí‚àö1 ‚àíŒ∫

Œ∫
.
We have taken the minus sign rather than the plus sign in front of the square root,
otherwise œâ Ã∏‚àà[1, 2). Therefore, by (12.38),
|Œª| = œÅ(Œ∫) := 21 ‚àí‚àö1 ‚àíŒ∫
Œ∫
‚àí1.

12.2
Classical iterative methods
269
2
4
6
0
2000
4000
6000
8000
10000
12000
 
2
4
6
0
1000
2000
3000
4000
5000
6000
 
2
4
6
0
20
40
60
80
100
120
140
160
180
200
 
Jacobi
Gauss‚ÄìSeidel
SOR
Figure 12.2
The number of iterations in the solution of Ax = b, where A is given
by (12.27), x[0] = 0 and b = 1, required to reach accuracy up to a given tolerance.
The x-axis displays the number of correct signiÔ¨Åcant digits, while the number
of iterations can be read from the y-axis. The broken-and-dotted line corresponds to
d = 25 and the solid line to d = 50.
The above is true for every Œ∫ = cos2[œÄ‚Ñì/(d + 1)], ‚Ñì= 1, 2, . . . , d. It is, however,
elementary to verify that œÅ increases strictly monotonically as a function of Œ∫. Thus,
the maximum is attained for the largest value of Œ∫, i.e. when ‚Ñì= 1, and we deduce
that
œâopt = 21 ‚àísin[œÄ/(d + 1)]
cos2[œÄ/(d + 1)]
(12.39)
and
œÅ(Lœâopt) = œâopt ‚àí1 =
#1 ‚àísin[œÄ/(d + 1)]
cos[œÄ/(d + 1)]
$2
‚âà1 ‚àí2œÄ
d .
(12.40)
Casting our eyes at the expressions (12.32), (12.35) and (12.40), for the spectral
radii of Jacobi, Gauss‚ÄìSeidel and SOR (with œâopt) respectively, it is diÔ¨Écult not to
notice the drastic improvement inherent in the SOR scheme.
This observation is
vividly demonstrated in Fig. 12.2, where the three methods are employed to solve

270
Classical iterative methods
1.0
1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8
1.9
2.0
0
200
400
600
800
1000
1200
1400
1600
1800
 
Figure 12.3
The number of iterations required to attain accuracy of 10‚àí2 (dot-
ted line), 10‚àí4 (broken-and-dashed line) and 10‚àí6 (solid line) with SOR for diÔ¨Äerent
values of œâ (d = 25).
Ax = b; A is given by (12.27), b = 1 and x[0] = 0. Gauss‚ÄìSeidel is, predictably, twice
as eÔ¨Écient as Jacobi but SOR leaves both far behind!
Figure 12.3 displays the number of iterations required to approach the solution
of Ax = b (for d = 25, with the same b and x[0] as in Fig. 12.2) to within a given
accuracy. The sensitivity of the rate of convergence to the value of œâ is striking.
Although our analysis of the performance of classical iteration schemes has been
restricted to a very special matrix A, (12.27), its conclusions are relevant in a sub-
stantially wider framework. It is easy, for example, to generalize it to an arbitrary
TST matrix, provided the underlying Jacobi iteration converges (in the terminology
of Lemma 12.5 this corresponds to 2|Œ≤| ‚â§|Œ±|; cf. Exercise 12.6).
In the next section we present an outline of a more general theory. This answers the
question of convergence and its rate for classical iterative methods, and identiÔ¨Åes the
optimal SOR parameter, for an extensive family of matrices occurring in the numerical
solution of partial diÔ¨Äerential equations.
12.3
Convergence of successive over-relaxation
In this section we address ourselves to matrices that possess a speciÔ¨Åc sparsity pattern.
For brevity and in order to steer clear of complicated algebra, we have omitted several
proofs, and the section should be regarded as no more than a potted outline of general
SOR theory.
In Chapter 11 we have had already an opportunity to observe that the terminology
of graph theory confers a useful means to express sparsity patterns. As we wish at

12.3
Convergence of successive over-relaxation
271
present to discuss matrices that neither are symmetric nor conform with symmetric
sparsity patterns, we say that G = {V, E} is the digraph (an abbreviation for directed
graph) of a d √ó d matrix A if V = {1, 2, . . . , d} and (m, ‚Ñì) ‚ààE for m, ‚Ñì‚àà{1, 2, . . . , d},
m Ã∏= ‚Ñì, if and only if am,‚ÑìÃ∏= 0.
To distinguish between (‚Ñì, m) ‚ààE and (m, ‚Ñì) ‚ààE, a pictorial representation of
a digraph uses arrows to denote the ‚Äòdirection‚Äô.
Thus, (‚Ñì, m) is represented by a
curve joining the ‚Ñìth and the mth vertex, with an arrow pointing at the latter. The
restriction ‚ÑìÃ∏= m is often dropped in a deÔ¨Ånition of a digraph. We insist on it in the
present context because it simpliÔ¨Åes the notation to some extent.
Given a d √ó d matrix A, we say that j ‚ààZd is an ordering vector if |j‚Ñì‚àíjm| = 1
for every (‚Ñì, m) ‚ààE. Moreover, j is a compatible ordering vector if, in addition,
‚Ñì‚â•m + 1
=‚áí
j‚Ñì‚àíjm = +1,
‚Ñì‚â§m ‚àí1
=‚áí
j‚Ñì‚àíjm = ‚àí1.
Lemma 12.6
If the matrix A has an ordering vector then there exists a permutation
matrix P such that the matrix ÀúA := PAP ‚àí1 has a compatible ordering vector.
Proof
Each similarity transformation by a permutation matrix is merely a rela-
belling of variables; this theme underlies the discussion in Section 11.2.2 Therefore the
graph of ÀúA is simply ÀúG = {œÄ(V), œÄ(E)}, where œÄ is the corresponding permutation of
{1, 2, . . . , d}; in other words, œÄ(V) = {œÄ(1), œÄ(2), . . . , œÄ(d)} and (œÄ(‚Ñì), œÄ(m)) ‚ààœÄ(E)
if and only if (‚Ñì, m) ‚ààE.
Let j be the ordering vector whose existence has been stipulated in the statement of
the lemma and set i‚Ñì:= jœÄ‚àí1(‚Ñì), ‚Ñì= 1, 2, . . . , d, where œÄ‚àí1 is the inverse permutation
of œÄ.
It is easy to verify that i is an ordering vector of ÀúA since (‚Ñì, m) ‚ààœÄ(E)
implies (œÄ‚àí1(‚Ñì), œÄ‚àí1(m)) ‚ààE.
Therefore, by the deÔ¨Ånition of an ordering vector,
|jœÄ‚àí1(‚Ñì) ‚àíjœÄ‚àí1(m)| = 1. Recalling the way we have constructed the vector i, it follows
that |i‚Ñì‚àíim| = 1 and that i is indeed an ordering vector of ÀúA.
Let us choose a permutation œÄ such that i1 ‚â§i2 ‚â§¬∑ ¬∑ ¬∑ ‚â§id; this, of course,
corresponds to jœÄ‚àí1(1) ‚â§jœÄ‚àí1(2) ‚â§¬∑ ¬∑ ¬∑ ‚â§jœÄ‚àí1(d) and can always be done.
Given
(‚Ñì, m) ‚ààœÄ(E), ‚Ñì‚â•m + 1, we then obtain i‚Ñì‚àíim ‚â•0; therefore, i being an ordering
vector, i‚Ñì‚àíim = +1. Likewise (‚Ñì, m) ‚ààœÄ(E), ‚Ñì‚â§m ‚àí1, implies that i‚Ñì‚àíim = ‚àí1.
We thus deduce that i is a compatible ordering vector of ÀúA.
3 Ordering vectors
Consider a matrix A with the following sparsity pattern:
‚é°
‚é¢‚é¢‚é¢‚é¢‚é£
√ó
‚ó¶
√ó
‚ó¶
√ó
‚ó¶
√ó
√ó
‚ó¶
‚ó¶
√ó
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
√ó
‚ó¶
‚ó¶
√ó
√ó
‚é§
‚é•‚é•‚é•‚é•‚é¶
.
Here E = {(1, 3), (3, 1), (1, 5), (5, 1), (2, 3), (3, 2), (4, 5), (5, 4)}, and it is easy to
2It is trivial to prove that P ‚àí1 is also a permutation matrix and that it reverses the action of P.

272
Classical iterative methods
verify that
j =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é£
2
2
1
2
3
‚é§
‚é•‚é•‚é•‚é•‚é¶
is an ordering vector. However, it is not a compatible ordering vector since,
for example, (3, 1) ‚ààE and j3 ‚àíj1 = ‚àí1.
To construct a compatible ordering vector i for a permutation œÄ, we require
jœÄ‚àí1(1) ‚â§jœÄ‚àí1(2) ‚â§jœÄ‚àí1(3) ‚â§jœÄ‚àí1(4) ‚â§jœÄ‚àí1(5). This will be the case if we let,
for example, œÄ‚àí1(1) = 3, œÄ‚àí1(2) = 1, œÄ‚àí1(3) = 2, œÄ‚àí1(4) = 4 and œÄ‚àí1(5) = 5,
in other words,
œÄ =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é£
2
3
1
4
5
‚é§
‚é•‚é•‚é•‚é•‚é¶
,
P =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é£
0
0
1
0
0
1
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
1
‚é§
‚é•‚é•‚é•‚é•‚é¶
and
ÀúA =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é£
√ó
√ó
√ó
‚ó¶
‚ó¶
√ó
√ó
‚ó¶
‚ó¶
√ó
√ó
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
‚ó¶
√ó
‚ó¶
√ó
√ó
‚é§
‚é•‚é•‚é•‚é•‚é¶
.
Incidentally, i is not the only possible compatible ordering vector. To demon-
strate this and, incidentally, that a compatible ordering vector need not be
unique, we render the digraph of A pictorially,


4


5


1


3


2
j
j
j
j
Y
Y
Y
Y
and observe that A is itself a permutation of a tridiagonal matrix.
It is
easy to identify a compatible ordering vector for any tridiagonal matrix with
nonvanishing oÔ¨Ä-diagonal elements ‚Äì a task that is relegated to Exercise 12.8
‚Äì hence producing yet another compatible ordering vector for a permutation
of A.
3
The existence of a compatible ordering vector confers on a matrix several interesting
properties which are of great relevance to the behaviour of classical iterative methods.
Lemma 12.7
If the matrix A has a compatible ordering vector then the function
g(s, t) := det
	
tL0 + 1
t U0 ‚àísD

,
s ‚ààR,
t ‚ààR \ {0}
is independent of t.
Proof
Since A and H(s, t) := tL0 +(1/t)U0 ‚àísD share the same sparsity pattern
for all t Ã∏= 0, it follows from the deÔ¨Ånition that every compatible ordering vector of
A is also a compatible ordering vector of H(s, t). In particular, we deduce that the
matrix H(s, t) possesses a compatible ordering vector.

12.3
Convergence of successive over-relaxation
273
By the deÔ¨Ånition of a determinant,
g(s, t) =

œÄ‚ààŒ†d
(‚àí1)|œÄ|
d

i=1
hi,œÄ(i)(s, t),
where H(s, t) = (hj,‚Ñì(s, t))d
j,‚Ñì=1, Œ†d is the set of all permutations of {1, 2, . . . , d} and
|œÄ| is the sign of œÄ ‚ààŒ†d. Since
hj,‚Ñì(s, t) = ‚àítœÉj‚àí‚Ñìs1‚àí|œÉj‚àí‚Ñì|aj,‚Ñì,
j, ‚Ñì= 1, 2, . . . , d,
where
œÉm =
‚éß
‚é®
‚é©
+1,
m > 0,
‚àí1,
m < 0,
0,
m = 0,
we deduce that
g(s, t) = (‚àí1)d 
œÄ‚ààŒ†d
(‚àí1)|œÄ|tdL(œÄ)‚àídU(œÄ)sd‚àídL(œÄ)‚àídU(œÄ)
d

i=1
ai,œÄ(i),
(12.41)
where dL(œÄ) and dU(œÄ) denote the number of elements ‚Ñì‚àà{1, 2, . . . , d} such that
‚Ñì> œÄ(‚Ñì) and ‚Ñì< œÄ(‚Ñì) respectively.
Let j be the compatible ordering vector of H(s, t), whose existence we have already
deduced from the statement of the lemma, and choose an arbitrary œÄ ‚ààŒ†d such that
a1,œÄ(1), a2,œÄ(2), . . . , ad,œÄ(d) Ã∏= 0.
It follows from the deÔ¨Ånition that
dL(œÄ) =
d

‚Ñì=1
œÄ(‚Ñì)<‚Ñì
[j‚Ñì‚àíjœÄ(‚Ñì)],
dU(œÄ) =
d

‚Ñì=1
œÄ(‚Ñì)>‚Ñì
[jœÄ(‚Ñì) ‚àíj‚Ñì],
hence
dL(œÄ) ‚àídU(œÄ) =
d

‚Ñì=1
œÄ(‚Ñì)Ã∏=‚Ñì
[j‚Ñì‚àíjœÄ(‚Ñì)] =
d

‚Ñì=1
[j‚Ñì‚àíjœÄ(‚Ñì)] =
d

‚Ñì=1
j‚Ñì‚àí
d

‚Ñì=1
jœÄ(‚Ñì).
Recall, however, that œÄ is a permutation of {1, 2, . . . , d}; therefore
d

‚Ñì=1
jœÄ(‚Ñì) =
d

‚Ñì=1
j‚Ñì
and dL(œÄ) ‚àídU(œÄ) = 0 for every œÄ ‚ààŒ†d such that a‚Ñì,œÄ(‚Ñì) Ã∏= 0, ‚Ñì= 1, 2, . . . , d.
Therefore
tdL(œÄ)‚àídU(œÄ)
d

i=1
ai,œÄ(i) =
d

i=1
ai,œÄ(i),
œÄ ‚ààŒ†d,

274
Classical iterative methods
and it follows from (12.41) that g(s, t) is indeed independent of t ‚ààR \ {0}.
Theorem 12.8
Suppose that the matrix A has a compatible ordering vector and let
¬µ ‚ààC be an eigenvalue of the matrix B, the iteration matrix of the Jacobi method.
Then also
(i) ‚àí¬µ ‚ààœÉ(B) and the multiplicities of +¬µ and ‚àí¬µ (as eigenvalues of B) are
identical;
(ii) given any œâ ‚àà(0, 2), every Œª ‚ààC that obeys the equation
Œª + œâ ‚àí1 = œâ¬µŒª1/2
(12.42)
belongs to œÉ(Lœâ);3
(iii) for every Œª ‚ààœÉ(Lœâ), œâ ‚àà(0, 2), there exists ¬µ ‚ààœÉ(B) such that the equation
(12.42) holds.
Proof
According to Lemma 12.7, the presence of a compatible ordering vector of
A implies that
det(L0 + U0 ‚àí¬µD) = g(¬µ, 1) = g(¬µ, ‚àí1) = det(‚àíL0 ‚àíU0 ‚àí¬µD).
Moreover, det(‚àíC) = (‚àí1)d det C for any d √ó d matrix C and we thus deduce that
det(L0 + U0 ‚àí¬µD) = (‚àí1)d det(L0 + U0 + ¬µD).
(12.43)
By the deÔ¨Ånition of an eigenvalue, ¬µ ‚ààœÉ(B) if and only if det(B ‚àí¬µI) = 0 (see
A.1.5.1). But, according to (12.20) and (12.43),
det(B ‚àí¬µI) = det

D‚àí1(L0 + U0) ‚àí¬µI

= det[D‚àí1(L0 + U0 ‚àí¬µD)]
=
1
det D det(L0 + U0 ‚àí¬µD) = (‚àí1)d
det D det(L0 + U0 + ¬µD)
= (‚àí1)d det(B + ¬µI).
This proves (i).
The matrix I ‚àíœâL is lower triangular with ones across the diagonal, therefore
det(I ‚àíœâL) ‚â°1. Hence, it follows from the deÔ¨Ånition (12.22) of Lœâ that
det(Lœâ ‚àíŒªI) = det

(I ‚àíœâL)‚àí1[œâU + (1 ‚àíœâ)I] ‚àíŒªI

=
1
det(I ‚àíœâL) det[œâU + œâŒªL ‚àí(Œª + œâ ‚àí1)I]
= det[œâU + œâŒªL ‚àí(Œª + œâ ‚àí1)I].
(12.44)
3The SOR iteration was deÔ¨Åned in (12.22) for œâ ‚àà[1, 2), while now we allow œâ ‚àà(0, 2). This
should cause no diÔ¨Éculty whatsoever.

12.3
Convergence of successive over-relaxation
275
Suppose that Œª = 0 lies in œÉ(Lœâ). Then (12.44) implies that det[œâU ‚àí(œâ‚àí1)I] = 0.
Recall that U is strictly upper triangular, therefore
det[œâU ‚àí(œâ ‚àí1)I] = (1 ‚àíœâ)d
and we deduce œâ = 1. It is trivial to check that (Œª, œâ) = (0, 1) obeys the equation
(12.42). Conversely, if Œª = 0 satisÔ¨Åes (12.42) then we immediately deduce that œâ = 1
and, by (12.44), 0 ‚ààœÉ(L1). Therefore, (ii) and (iii) are true in the special case Œª = 0.
To complete the proof of the theorem, we need to discuss the case Œª Ã∏= 0. According
to (12.44),
1
œâdŒªd/2 det(Lœâ ‚àíŒªI) = det
	
Œª1/2L + Œª‚àí1/2U ‚àíŒª + œâ ‚àí1
œâŒª1/2
I

and, again using Lemma 12.7,
1
œâdŒªd/2 det(Lœâ ‚àíŒªI) = det
	
L + U ‚àíŒª + œâ ‚àí1
œâŒª1/2
I

= det
	
B ‚àíŒª + œâ ‚àí1
œâŒª1/2
I

.
(12.45)
Let ¬µ ‚ààœÉ(B) and suppose that Œª obeys the equation (12.42). Then
¬µ = Œª + œâ ‚àí1
œâŒª1/2
and substitution in (12.45) proves that det(Lœâ ‚àíŒªI) = 0, hence Œª ‚ààœÉ(Lœâ). However,
if Œª ‚ààœÉ(Lœâ), Œª Ã∏= 0, then, according to (12.45), (Œª + œâ ‚àí1)/(œâŒª1/2) ‚ààœÉ(B). Conse-
quently, there exists ¬µ ‚ààœÉ(B) such that (12.42) holds; thus the proof of the theorem
is complete.
Corollary
Let A be a tridiagonal matrix and suppose that aj,‚ÑìÃ∏= 0, |j ‚àí‚Ñì| ‚â§1,
j, ‚Ñì= 1, 2, . . . Then œÅ(L1) = [œÅ(B)]2.
Proof
We have already mentioned that every tridiagonal matrix with a nonva-
nishing oÔ¨Ä-diagonal has a compatible ordering vector, a statement whose proof was
consigned to Exercise 12.8. Therefore (12.42) holds and, œâ being unity, reduces to
Œª = ¬µŒª1/2.
In other words, either Œª = 0 or Œª = ¬µ2. If Œª = 0 for all Œª ‚ààœÉ(L1) then part (iii) of
the theorem implies that all the eigenvalues of B vanish as well.
Hence œÅ(L1) =
[œÅ(B)]2 = 0. Otherwise, there exists Œª Ã∏= 0 in œÉ(L1) and, since Œª = ¬µ2, parts (ii) and
(iii) of the theorem imply that œÅ(L1) ‚â•[œÅ(B)]2 and œÅ(L1) ‚â§[œÅ(B)]2 respectively. This
proves the corollary.
The statement of the corollary should not come as a surprise, since we have already
observed behaviour consistent with œÅ(L1) = [œÅ(B)]2 in Fig. 12.1 and proved it in
Section 12.2 for a speciÔ¨Åc TST matrix.
The importance of Theorem 12.8 ranges well beyond a comparison of the Jacobi and
Gauss‚ÄìSeidel schemes. It comes into its own when applied to SOR and its convergence.
Theorem 12.9
Let A be a d√ód matrix. If œÅ(Lœâ) < 1 and the SOR iteration (12.22)
converges then necessarily œâ ‚àà(0, 2). Moreover, if A has a compatible ordering vector

276
Classical iterative methods
and all the eigenvalues of B are real then the iteration converges for every œâ ‚àà(0, 2)
if and only if œÅ(B) < 1 and the Jacobi method converges for the same matrix.
Proof
Let œÉ(Lœâ) = {Œª1, Œª2, . . . , Œªd}, therefore
det Lœâ =
d

‚Ñì=1
Œª‚Ñì.
(12.46)
Using a previous argument, see (12.44), we obtain
det Lœâ = det[œâU ‚àí(œâ ‚àí1)I] = (1 ‚àíœâ)d
and substitution in (12.46) leads to the inequality
œÅ(Lœâ) =
max
‚Ñì=1,2,...,d |Œª‚Ñì| ‚â•

d

‚Ñì=1
Œª‚Ñì

1/d
= |1 ‚àíœâ|.
Therefore œÅ(Lœâ) < 1 is inconsistent with either œâ ‚â§0 or œâ ‚â•2 and the Ô¨Årst statement
of the theorem is true.
We next suppose that A possesses a compatible ordering vector. Thus, according
to Theorem 12.8, for every Œª ‚ààœÉ(Lœâ) there exists ¬µ ‚ààœÉ(B) such that p(Œª1/2) = 0,
where p(z) := z2 ‚àíœâ¬µz + (œâ ‚àí1) (equivalently, Œª is a solution of (12.42)).
Recall the Cohn‚ÄìSchur criterion (Lemma 4.9): Both zeros of the quadratic Œ±w2 +
Œ≤w + Œ≥, Œ± Ã∏= 0, reside in the closed complex unit disc if and only if |Œ±|2 ‚â•|Œ≥|2 and
(|Œ±|2 ‚àí|Œ≥|2)2 ‚â•|Œ±¬ØŒ≤ ‚àíŒ≤¬ØŒ≥|2. Similarly, it is possible to prove that both zeros of this
quadratic reside in the open unit disc if and only if
|Œ±|2 > |Œ≥|2
and
(|Œ±|2 ‚àí|Œ≥|2)2 > |Œ±¬ØŒ≤ ‚àíŒ≤¬ØŒ≥|2.
Letting Œ± = 1, Œ≤ = ‚àíœâ¬µ, Œ≥ = œâ ‚àí1 and bearing in mind that ¬µ ‚ààR, these two
conditions become (œâ ‚àí1)2 < 1 (which is the same as œâ ‚àà(0, 2)) and ¬µ2 < 1.
Therefore, provided œÅ(B) < 1, it is true that |¬µ| < 1 for all ¬µ ‚ààœÉ(B), therefore
|Œª|1/2 < 1 for all Œª ‚ààœÉ(Lœâ) and œÅ(Lœâ) < 1 for all œâ ‚àà(0, 2). Likewise, if œÅ(Lœâ) < 1
then part (iii) of Theorem 12.8 implies that all the eigenvalues of B reside in the open
unit disc.
The condition that all the zeros of B are real is satisÔ¨Åed in the important special
case where B is symmetric. If it fails, the second statement of the theorem need not
be true; see Exercise 12.10, where the reader can prove that œÅ(B) < 1 and œÅ(Lœâ) > 1
for œâ ‚àà(1, 2) for the matrix
A =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
2
1
0
¬∑ ¬∑ ¬∑
0
‚àí1
2
1
...
...
0
...
...
...
0
...
...
‚àí1
2
1
0
¬∑ ¬∑ ¬∑
0
‚àí1
2
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
,

12.3
Convergence of successive over-relaxation
277
provided that d is suÔ¨Éciently large.
Within the conditions of Theorem 12.9 there is not much to choose between our
three iterative procedures regarding convergence: either they all converge or they all
fail on that score. The picture changes when we take the speed of convergence into
account. Thus, the corollary to Theorem 12.8 aÔ¨Érms that Gauss‚ÄìSeidel is asymptot-
ically twice as good as Jacobi. Bearing in mind that Gauss‚ÄìSeidel is but a special
case of SOR, we thus expect to improve the rate of convergence further by choosing a
superior value of œâ.
Theorem 12.10
Suppose that A possesses a compatible ordering vector, that œÉ(B) ‚äÇ
R and that Àú¬µ := œÅ(B) < 1. Then
œÅ(Lœâopt) < œÅ(Lœâ),
œâ ‚àà(0, 2) \ {œâopt},
where
œâopt :=
2
1 +
0
1 ‚àíÀú¬µ2 = 1 +

Àú¬µ
1 +
0
1 ‚àíÀú¬µ2
2
‚àà(1, 2).
(12.47)
Proof
Although it might not be immediately obvious, the proof is but an elab-
oration of the detailed example from Section 12.2. Having already done all the hard
work, we can allow ourselves to proceed at an accelerated pace.
Solving the quadratic (12.42) yields
Œª = 1
4
%
œâ¬µ ¬±
0
(œâ¬µ)2 ‚àí4(œâ ‚àí1)
&2
.
According to Theorem 12.8, both roots reside in œÉ(Lœâ).
Since ¬µ is real, the term inside the square root is nonpositive when
Àúœâ := 2(1 ‚àí
0
1 ‚àí¬µ2)
¬µ2
‚â§œâ < 2.
In this case Œª ‚ààC \ R and it is trivial to verify that |Œª| = œâ ‚àí1.
In the remaining portion of the range of œâ both roots Œª are positive and the larger
one equals 1
4[f(œâ, |¬µ|)]2, where
f(œâ, t) := œât +
0
(œât)2 ‚àí4(œâ ‚àí1),
œâ ‚àà(0, Àúœâ],
t ‚àà[0, 1).
It is an easy matter to ascertain that for any Ô¨Åxed œâ ‚àà(0, Àúœâ] the function f(œâ, ¬∑ )
increases strictly monotonically for t ‚àà[0, 1). Likewise, Àúœâ increases strictly monoton-
ically as a function of ¬µ ‚àà[0, 1). Therefore the spectral radius of Lœâ in the range
œâ ‚àà

0, 2(1 ‚àí
0
1 ‚àíÀú¬µ2)/Àú¬µ2
is 1
4f(œâ, Àú¬µ). Note that the endpoint of the interval is
21 ‚àí
0
1 ‚àíÀú¬µ2
Àú¬µ2
=
2
1 +
0
1 ‚àíÀú¬µ2 = œâopt,
as given in (12.47).
The function f( ¬∑ , t) decreases strictly monotonically in œâ ‚àà
(0, œâopt) for any Ô¨Åxed t ‚àà[0, 1), thereby reaching its minimum at œâ = œâopt.

278
Classical iterative methods
0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
¬µ = 0.6
0.7
0.8
0.9
Figure 12.4
The graph of œÅ(Lœâ) for diÔ¨Äerent values of Àú¬µ.
As far as the interval [œâopt, 2) is concerned, the modulus of each Œª corresponding
to ¬µ ‚ààœÉ(B), |¬µ| = Àú¬µ, equals œâ ‚àí1 and is at least as large as the magnitude of any
other eigenvalues of Lœâ. We thus deduce that
œÅ(Lœâ) =
1
1
4
%
œâÀú¬µ +
0
(œâÀú¬µ)2 ‚àí4(œâ ‚àí1)
&2
,
œâ ‚àà(0, œâopt],
œâ ‚àí1,
œâ ‚àà[œâopt, 2)
(12.48)
and, for all œâ ‚àà(0, 2), œâ Ã∏= œâopt, it is true that
œÅ(Lœâ) > œÅ(Lœâopt) =

Àú¬µ
1 +
0
1 ‚àíÀú¬µ2
2
.
Figure 12.4 displays œÅ(Lœâ) for diÔ¨Äerent values of Àú¬µ for matrices that are consistent
with the conditions of the theorem. As apparent from (12.48), each curve is composed
of two smooth portions, joining at œâopt.
In practical computation the value of Àú¬µ is frequently estimated rather than derived
in an explicit form. An important observation from Fig. 12.4 is that it is always a
sound policy to overestimate (rather than underestimate) œâopt, since the curve has
a larger slope to the left of the optimal value and so overestimation is punished less
severely.
The Ô¨Ågure can be also employed as an illustration of the method of proof. Thus,
instead of visualizing each individual curve as corresponding to a diÔ¨Äerent matrix,

12.3
Convergence of successive over-relaxation
279
think of them as plots of |Œª|, where Œª ‚ààœÉ(Lœâ), as a function of œâ. The spectral radius
for any given value of œâ is provided by the top curve ‚Äì and it can be observed in
Fig. 12.4 that this top curve is associated with Àú¬µ.
Neither Theorem 12.8 nor Theorem 12.9 requires the knowledge of a compatible
ordering vector ‚Äì it is enough that such a vector exists. Unfortunately, it is not a
trivial matter to verify directly from the deÔ¨Ånition whether a given matrix possesses
a compatible ordering vector.
We have established in Lemma 12.6 that, provided A has an ordering vector, there
exists a rearrangement of its rows and columns that possesses a compatible ordering
vector. There is more to the lemma than meets the eye, since its proof is constructive
and can be used as a numerical algorithm in a most straightforward manner.
In
other words, it is enough to Ô¨Ånd an ordering vector (provided that it exists) and the
algorithm from the proof of Lemma 12.6 takes care of compatibility!
A d √ó d matrix with a digraph G = {V, E} is said to possess property A if there
exists a partition V = S1 ‚à™S2, where S1 ‚à©S2 = ‚àÖ, such that for every (j, ‚Ñì) ‚ààE either
j ‚ààS1 and ‚Ñì‚ààS2 or ‚Ñì‚ààS1 and j ‚ààS2.
3 Property A
As often in matters involving sparsity patterns, pictorial rep-
resentation conveys more information than many a formal deÔ¨Ånition.
Let us consider, for example, a 6 √ó 6 matrix with a symmetric sparsity struc-
ture, as follows:
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
√ó
√ó
‚ó¶
√ó
‚ó¶
‚ó¶
√ó
√ó
√ó
‚ó¶
√ó
‚ó¶
‚ó¶
√ó
√ó
√ó
‚ó¶
√ó
√ó
‚ó¶
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
√ó
√ó
‚ó¶
‚ó¶
√ó
‚ó¶
√ó
√ó
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
We claim that S1 = {1, 3, 5}, S2 = {2, 4, 6} is a partition consistent with
property A. To conÔ¨Årm this, write the digraph G in the following fashion:


5


3


1


6


4


2
-

-

-


















HHHHHHHH
j
H
H
H
H
H
H
H
H
Y

*









HHHHHHHH
j
H
H
H
H
H
H
H
H
Y
S1
S2
(In the interests of clarity, we have replaced each pair of arrows pointing in
opposite directions by a single, ‚Äòtwo-sided‚Äô arrow.) Evidently, no edges join
vertices in the same set, be it S1 or S2, and this is precisely the meaning of
property A.

280
Classical iterative methods
An alternative interpretation of property A comes to light when we rearrange
the matrix in such a way that rows and columns corresponding to S1 pre-
cede those of S2. In our case, we permute rows and columns in the order
1, 3, 5, 2, 4, 6 and the resultant sparsity pattern is then
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
√ó
‚ó¶
‚ó¶
√ó
√ó
‚ó¶
‚ó¶
√ó
‚ó¶
√ó
√ó
√ó
‚ó¶
‚ó¶
√ó
√ó
‚ó¶
√ó
√ó
√ó
√ó
√ó
‚ó¶
‚ó¶
√ó
√ó
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
√ó
√ó
‚ó¶
‚ó¶
√ó
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
.
In other words, the partitioned sparsity pattern has two diagonal blocks along
the main diagonal.
3
The importance of property A is encapsulated in the following result.
Lemma 12.11
A matrix possesses property A if and only if it has an ordering vector.
Proof
Suppose Ô¨Årst that a d √ó d matrix has property A and set
j‚Ñì=
#
1,
‚Ñì‚ààS1,
2,
‚Ñì‚ààS2,
‚Ñì= 1, 2, . . . , d.
For any (‚Ñì, m) ‚ààE it is true that ‚Ñìand m belong to diÔ¨Äerent sets, therefore j‚Ñì‚àíjm = ¬±1
and we deduce that j is an ordering vector.
To establish the proof in the opposite direction assume that the matrix has an
ordering vector j and let
S1 := {‚Ñì‚ààV : j‚Ñìis odd},
S2 := {‚Ñì‚ààV : j‚Ñìis even}.
Clearly, S1 ‚à™S2 = V and S1 ‚à©S2 = ‚àÖ, therefore {S1, S2} is indeed a partition of V.
For any ‚Ñì, m ‚ààV such that (‚Ñì, m) ‚ààE it follows from the deÔ¨Ånition of an ordering
vector that j‚Ñì‚àíjm = ¬±1. In other words, the integers j‚Ñìand jm are of diÔ¨Äerent parity,
hence it follows from our construction that ‚Ñìand m belong to diÔ¨Äerent partition sets.
Consequently, the matrix has property A.
An important example of a matrix with property A follows from the Ô¨Åve-point
equations (8.16). Each point in the grid is coupled with its vertical and horizontal
neighbours, hence we need to partition the grid points in such a way that S1 and
S2 separate neighbours.
This can be performed most easily in terms of red‚Äìblack
ordering, which we have already mentioned in Chapter 11. Thus, we traverse the grid
as in natural ordering except that all grid points (‚Ñì, m) such that ‚Ñì+ m is odd, say,
are consigned to S1 and all other points to S2.
An example, corresponding to a Ô¨Åve-point formula in a 4 √ó 4 square, is presented
in (11.4). Of course, the real purpose of the exercise is not simply to verify property
A or, equivalently, to prove that an ordering vector exists. Rather, our goal is to
identify a permutation that yields a compatible ordering vector. As we have already
mentioned, this can be performed by the method of proof of Lemma 12.6. However,

12.4
The Poisson equation
281
in the present circumstances we can single out such a vector directly for the natural
ordering. For example, as far as (11.4) is concerned we associate with every grid point
(which, of course, corresponds to an equation and a variable in the linear system) an
integer as follows:


1


3


3


5


3


5


5


7


2


4


2


4


4


6


4


6
As can be easily veriÔ¨Åed, natural ordering results in a compatible ordering vector. All
this can be easily generalized to rectangular grids of arbitrary size (see Exercise 12.11).
The exploitation of red‚Äìblack ordering in the search for property A is not restricted
to rectangular grids. Thus, consider the L-shaped grid
2
2
2
2
c
c
c
c
c
c
c
2
2
2
2
2
2
2
c
c
c
c
c
c
c
2
2
2
2
2
2
c
c
c
c
c
2
2
(12.49)
where ‚Äò2‚Äô and ‚Äò c‚Äô denote vertices in S1 and S2, respectively. Note that (12.49) serves
a dual purpose: it is both the depiction of the computational grid and the graph of a
matrix. It is quite clear that here this underlying matrix has property A. The task of
Ô¨Ånding explicitly a compatible ordering vector is relegated to Exercise 12.12.
12.4
The Poisson equation
Figure 12.5 displays the error attained by four diÔ¨Äerent iterative methods, when ap-
plied to the Poisson equation (8.33) on a 16 √ó 16 grid. The Ô¨Årst row depicts the line
relaxation method (a variant of the incomplete LU factorization (12.11) ‚Äì read on
for details), the second corresponds to the Jacobi iteration (12.20), next comes the
Gauss‚ÄìSeidel method (12.21) and, Ô¨Ånally, the bottom row displays the error in the
successive over-relaxation (SOR) method (12.22) with optimal choice of the parameter
œâ. Each column corresponds to a diÔ¨Äerent number of iterations, speciÔ¨Åcally 50, 100
and 150, except that there is little point in displaying the error for ‚â•100 iterations

282
Classical iterative methods
for SOR since, remarkably, the error after 50 iterations is already close to machine
accuracy!4
Our Ô¨Årst observation is that, evidently, all four methods converge. This is hardly
a surprise in the case of Jacobi, Gauss‚ÄìSeidel and SOR since we have already noted
in the last section that the underlying matrix possesses property A. The latter feature
explains also the very diÔ¨Äerent rate of convergence: Gauss‚ÄìSeidel converges twice as
fast as Jacobi while the speed of convergence of SOR is of a diÔ¨Äerent order of magnitude
altogether.
Another interesting observation pertains to the line relaxation method, a version
of ILU from Section 12.1, where ÀúA is the tridiagonal portion of A. Fig. 12.5 suggests
that line relaxation and Gauss‚ÄìSeidel deliver very similar performances and we will
prove later that this is indeed the case. We commence our discussion, however, with
classical iterative methods.
Because the underlying matrix has a compatible ordering vector, as noted in Sec-
tion 12.3, we need to determine Àú¬µ = œÅ(B); and, by virtue of Theorems 12.8 and 12.10,
Àú¬µ determines completely both œâopt and the rates of convergence of Gauss‚ÄìSeidel and
SOR.
Let V = (vj,‚Ñì)m
j,‚Ñì=1 be an eigenvector of the matrix B from (12.20) and let Œª be the
corresponding eigenvalue. We assume that the matrix A originates in the Ô¨Åve-point
formula (7.16) in a m √ó m square. Formally, V is a matrix; to obtain a genuine vector
v ‚ààRm2 we would need to stretch the grid, but in fact this will not be necessary.
Setting v0,‚Ñì, vm+1,‚Ñì, vk,0, vk,m+1 := 0, where k, ‚Ñì= 1, 2, . . . , m, we can express
Av = Œªv in the form
vj‚àí1,‚Ñì+ vj+1,‚Ñì+ vj,‚Ñì‚àí1 + vj,‚Ñì+1 = 4Œªvj,‚Ñì,
j, ‚Ñì= 1, 2, . . . , m.
(12.50)
Our claim is that
vj,‚Ñì= sin
	 œÄpj
m + 1

sin
	 œÄq‚Ñì
m + 1

,
j, ‚Ñì= 1, 2, . . . , m,
where p and q are arbitrary integers in {1, 2, . . . , m}. If this is true then
vj‚àí1,‚Ñì+ vj+1,‚Ñì=
#
sin
œÄp(j ‚àí1)
m + 1

+ sin
œÄp(j + 1)
m + 1
$
sin
	 œÄq‚Ñì
m + 1

= 2vj,‚Ñìcos
	
œÄp
m + 1

,
vj,‚Ñì‚àí1 + vj,‚Ñì+1 = sin
	 œÄpj
m + 1

 #
sin
œÄq(‚Ñì‚àí1)
m + 1

+ sin
œÄq(‚Ñì+ 1)
m + 1
$
= 2vk,‚Ñìcos
	
œÄq
m + 1

,
and substitution into (12.50) conÔ¨Årms that
Œª = Œªp,q = 1
2

cos
	
œÄp
m + 1

+ cos
	
œÄq
m + 1


4To avoid any misunderstanding, at this point we emphasize that by ‚Äòerror‚Äô we mean departure
from the solution of the corresponding Ô¨Åve-point equations (8.16) not departure from the exact
solution of the Poisson equation.

12.4
The Poisson equation
283
0
0.5
1.0
0
0.5
1.0
0
0.1
0.2
line relaxation
0
0.5
1.0
0
0.5
1.0
0
2
4
6
x 10
‚àí3
0
0.5
1.0
0
0.5
1.0
0
1
2
x 10
‚àí4
0
0.5
1.0
0
0.5
1.0
0
0.5
1.0
Jacobi
0
0.5
1.0
0
0.5
1.0
0
0.1
0.2
0
0.5
1.0
0
0.5
1.0
0
0.01
0.02
0.03
0
0.5
1.0
0
0.5
1.0
0
0.1
0.2
Gauss‚àíSeidel
0
0.5
1.0
0
0.5
1.0
0
2
4
6
x 10
‚àí3
0
0.5
1.0
0
0.5
1.0
0
1
2
x 10
‚àí4
0
0.5
1.0
0
0.5
1.0
0
0.5
1.0
1.5
x 10 ‚àí13
SOR
Figure 12.5
The error in the line relaxation, Jacobi, Gauss‚ÄìSeidel and SOR (with
œâopt) methods for the Poisson equation (8.33) for m = 16 after 100, 200 and 300
iterations. Note the diÔ¨Äerences in scale.

284
Classical iterative methods
is an eigenvalue of A for every p, q = 1, 2, . . . , m. This procedure yields all m2 eigen-
values of A and we therefore deduce that
Àú¬µ = œÅ(B) = cos
	
œÄ
m + 1

‚âà1 ‚àíœÄ2
2m2 .
We next employ Theorem 12.8 to argue that
œÅ(L1) = Àú¬µ2 = cos2
	
œÄ
m + 1

‚âà1 ‚àíœÄ2
m2 .
(12.51)
Finally, (12.47) produces the optimal SOR parameter,
œâopt =
2
1 + sin[œÄ/(m + 1)] = 2 {1 ‚àísin[œÄ/(m + 1)]}
cos2[œÄ/(m + 1)]
,
and
œÅ(Lœâopt) = 1 ‚àísin[œÄ/(m + 1)]
1 + sin[œÄ/(m + 1)] ‚âà1 ‚àí2œÄ
m .
(12.52)
Note, incidentally, that (replacing m by d) our results are identical to the corre-
sponding quantites for the TST matrix from Section 12.2; cf. (12.32), (12.35), (12.39)
and (12.40). This is not a coincidence, since the TST matrix corresponds to a one-
dimensional equivalent of the Ô¨Åve-point formula.
The diÔ¨Äerence between (12.51) and (12.52) amounts to just a single power of m
but glancing at Fig. 12.5 ascertains that this seemingly minor distinction causes a
most striking improvement in the speed of convergence.
Finally, we return to the top row of Fig. 12.5, to derive the rate of convergence
of the line relaxation method and explain its remarkable similarity to that for the
Gauss‚ÄìSeidel method.
In our implementation of the incomplete LU method in Section 12.1 we have split
the matrix A into a tridiagonal portion and a remainder ‚Äì the iteration is carried out
on the tridiagonal part and, for reasons that were clariÔ¨Åed in Chapter 11, is very low
in cost. In the context of Ô¨Åve-point equations this splitting is termed line relaxation.
Provided the matrix A has been derived from the Ô¨Åve-point formula in a square,
we can write it in a block form that has been already implied in (11.3), namely
A =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
C
I
O
¬∑ ¬∑ ¬∑
O
I
C
I
...
...
O
...
...
...
O
...
...
I
C
I
O
¬∑ ¬∑ ¬∑
O
I
C
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
,
(12.53)

12.4
The Poisson equation
285
where I and O are the m √ó m identity and zero matrices respectively, and
C =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
‚àí4
1
0
¬∑ ¬∑ ¬∑
0
1
‚àí4
1
...
...
0
...
...
...
0
...
...
1
‚àí4
1
0
...
0
1
‚àí4
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
.
In other words, A is block-TST and each block is itself a TST matrix.
Line relaxation (12.11) splits A into a tridiagonal part ÀúA and a remainder ‚àíE.
The matrix C being itself tridiagonal, we deduce that ÀúA is block-diagonal, with C‚Äôs
along the main diagonal, while E consists of the oÔ¨Ä-diagonal blocks. Let Œª and v be an
eigenvalue and a corresponding eigenvector of the iteration matrix ÀúA‚àí1E. Therefore
Ev = Œª ÀúAv
and, rendering as before the vector v ‚ààRm2 as an m √ó m matrix V , we obtain
vj,‚Ñì‚àí1 + vj,‚Ñì+1 + Œª(vj‚àí1,‚Ñì‚àí4vj,‚Ñì+ vj+1,‚Ñì) = 0,
j, ‚Ñì= 1, 2, . . . , d.
(12.54)
As before, we have assumed zero ‚Äòboundary values‚Äô: vj,0, vj,m+1, v0,‚Ñì, vm+1,‚Ñì= 0,
j, ‚Ñì= 1, 2, . . . , d.
Our claim (which, with the beneÔ¨Åt of experience, was hardly surprising) is that
vj,‚Ñì= sin
	 œÄpj
m + 1

sin
	 œÄq‚Ñì
m + 1

,
j, ‚Ñì= 1, 2, . . . , m,
for some p, q ‚àà{1, 2, . . . , m}. Since
sin
œÄp(j ‚àí1)
m + 1

‚àí4 sin
	 œÄpj
m + 1

+ sin
œÄp(j + 1)
m + 1

= 2

cos
	 œÄp
m + 1

‚àí2

sin
	 œÄpj
m + 1

,
sin
œÄq(‚Ñì‚àí1)
m + 1

+ sin
œÄq(‚Ñì+ 1)
m + 1

= 2 cos
	
œÄq
m + 1

sin
	 œÄq‚Ñì
m + 1

,
substitution in (12.54) results in
Œª = Œªp,q = ‚àícos[œÄq/(m + 1)]
2 ‚àícos[œÄp/(m + 1)].
Letting p, q range across {1, 2, . . . , m}, we recover all m2 eigenvalues and, in particular,
determine the spectral radius of the iteration matrix:
œÅ( ÀúA‚àí1E) =
cos[œÄ/(m + 1)]
2 ‚àícos[œÄ/(m + 1)] ‚âà1 ‚àíœÄ2
m2 .
(12.55)
Comparison of (12.55) with (12.51) veriÔ¨Åes our observation from Fig. 12.5 that line
relaxation and Gauss‚ÄìSeidel have very similar rates of convergence. As a matter of
fact, it is easy to prove that Gauss‚ÄìSeidel is marginally better, since
cos2 œï <
cos œï
2 ‚àícos œï < cos2 œï + œï2
2 ,
0 < œï < œÄ
2 .

286
Classical iterative methods
0
50
100
150
200
250
300
‚àí10
‚àí5
0
5
Figure 12.6
The logarithm of the error in the Euclidean norm, log ‚à•x[k] ‚àíÀÜx‚à•, for
Jacobi (broken-and-dotted line), Gauss‚ÄìSeidel (dotted line), SOR (broken line) and
line relaxation (solid line) for m = 16 in the Ô¨Årst 300 iterations. The starting vector
is x[0] = 0.
Fig 12.6 displays (on a logarithmic scale) the decay of the Euclidean norm of
the error after a given number of iterations ‚Äì thus, the information in each three-
dimensional surface from Fig. 12.5 is reduced to a single number. Having analysed and
understood the four methods in some detail, we can again observe and compare their
features. There is however, an interesting new detail in Fig. 12.6. In principle, the size
of the spectral radius determines the speed of convergence only in an asymptotic sense
and there is nothing in our analysis to tell how soon ‚Äì or how late ‚Äì the asymptotic
regime occurs. However, we can observe in Fig. 12.6 (and, for that matter, though
for a diÔ¨Äerent equation, in Fig. 12.1) that the onset of asymptotic behaviour is pretty
rapid for a general starting vector x[0].
Comments and bibliography
Numerical mathematics is an old art and its history is replete with the names of intellectual
giants ‚Äì Newton, Euler, Lagrange, Legendre, Gauss, Jacobi . . . It is fair, however, to observe
that the most signiÔ¨Åcant milestone in its long journey has been the invention of the electronic
computer. Numerical linear algebra, including iterative methods for sparse linear systems, is
a case in point. A major research eÔ¨Äort in the 1950s ‚Äì the dawn of the computer era ‚Äì led to
an enhanced understanding of classical iterative methods and forms the cornerstone of our
exposition.

Comments and bibliography
287
A large number of textbooks and monographs deal with the theme of this chapter and we
single out the books of Axelsson (1994), Varga (1962) and Young (1971); see also Hageman
& Young (1981). Readers who are at home with the terminology of functional analysis will
also enjoy the concise monograph of Nevanlinna (1993).
The theory of SOR can be developed signiÔ¨Åcantly beyond the material of Section 12.3.
This involves much beautiful and intricate mathematics but is, arguably, of mainly theoretical
interest, for reasons that will become clearer in Chapter 13 ‚Äì in a nutshell, we describe there
how to accelerate Gauss‚ÄìSeidel iteration in a manner that is much more powerful than SOR.
Classical iterative methods are neither the only nor, indeed, the best means to solve
sparse linear systems by iteration, and we wish to single out three other approaches.
Recall the line relaxation method (12.11). We split the matrix A, which originated from
the Ô¨Åve-point discretization of the Poisson equation in a square, into ÀúA ‚àíE, where ÀúA is its
tridiagonal part, subsequently iterating ÀúAx[k+1] = Ex[k] +b. Suppose, however, that instead
of ordering the grid by columns (the natural ordering), we do so by rows and apply line
relaxation to the new system. This is just as logical ‚Äì or illogical ‚Äì as employing a column-
wise ordering but leads to a diÔ¨Äerent iterative scheme (note that the matrix A stays intact
but both x and b are permuted as a consequence of the row-wise rearrangement). Which
variant should we adopt, ordering by column or by row? A natural approach is to alternate.
In other words, we write A = Ax + Ay, where Ax and Ay originate in central diÔ¨Äerencing in
the x- and y- directions respectively. Column-wise line relaxation (12.11) reads
(Ay ‚àí2I)x[k+1] = ‚àí(Ax + 2I)x[k] + b,
k = 0, 1, . . . ,
while a row-wise rearrangement results in
(Ax ‚àí2I)x[k+1] = ‚àí(Ay + 2I)x[k] + b,
k = 0, 1, . . .
For greater generality, we choose parameters Œ±0, Œ±1, . . . and iterate
(Ax ‚àíŒ±2kI)x[2k+1] = ‚àí(Ay + Œ±2k)x[2k] + b,
(Ay ‚àíŒ±2k+1I)x[2k+2] = ‚àí(Ax + Œ±2k+1)x[2k+1] + b,
k = 0, 1, . . .
This is the alternate directions implicit (ADI) method (Wachspress, 1966).
As often in
numerical analysis, the devil is in the parameter. However, it is known how to choose {Œ±k}
so as to accelerate ADI a great deal. The outcome, at least in certain cases, e.g. the Poisson
equation in a square, is an iterative method that clearly outperforms SOR. This, however,
falls outside the scope of the present volume.
Another example of iterative nonstationary schemes are the Krylov subspace methods,
in particular the method of conjugate gradients. They are of such fundamental importance
and wide-ranging applicability that they deserve a chapter all of their own in our book; see
Chapter 14.
Axelsson, O. (1994), Iterative Solution Methods, Cambridge University Press, Cambridge.
Hageman, L.A. and Young, D.M. (1981), Applied Iterative Methods, Academic Press, New
York.
Nevanlinna, O. (1993), Convergence of Iterations for Linear Equations, Birkh¬®auser, Basel.
Varga, R.S. (1962), Matrix Iterative Analysis, Prentice‚ÄìHall, Englewood CliÔ¨Äs, NJ.
Wachspress, E.L. (1966), Iterative Solution of Elliptic Systems, and Applications to the Neu-
tron DiÔ¨Äusion Equations of Reactor Physics, Prentice‚ÄìHall, Englewood CliÔ¨Äs, NJ.
Young, D.M. (1971), Iterative Solution of Large Linear Systems, Academic Press, New York.

288
Classical iterative methods
Exercises
12.1
Let Œª ‚ààC be such that |Œª| < 1 and deÔ¨Åne
J =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
Œª
1
0
¬∑ ¬∑ ¬∑
0
0
Œª
...
...
...
...
...
...
1
0
...
...
Œª
1
0
¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
0
Œª
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
.
Prove that limk‚Üí‚àûJk = O.
12.2
Suppose that the d√ód matrix H has a full set of eigenvectors, i.e., that there
exist d linearly independent vectors w‚Ñì‚ààCd and numbers Œª1, Œª2, . . . , Œªd ‚ààC
such that Hw‚Ñì= Œª‚Ñìw‚Ñì, ‚Ñì= 1, 2, . . . , d. We consider the iterative scheme
(12.3). Let r[k] := (I ‚àíH)x[k] ‚àív be the residual in the kth iteration, and
suppose that
r[0] =
d

‚Ñì=1
Œ±‚Ñìw‚Ñì
(such Œ±1, Œ±2, . . . , Œ±d always exist ‚Äì why?). Prove that
r[k] =
d

‚Ñì=1
Œ±‚ÑìŒªk
‚Ñìw‚Ñì,
k = 0, 1, . . .
Outline an alternative proof of Lemma 12.1 using this representation.
12.3
Prove that the Gauss‚ÄìSeidel iteration converges whenever the matrix A is
symmetric and positive deÔ¨Ånite.
12.4
Show that the SOR method is a regular splitting (12.12) with
P = œâ‚àí1D ‚àíL0,
N = (œâ‚àí1 ‚àí1)D + U0.
12.5
Let A be a symmetric tridiagonal positive deÔ¨Ånite matrix. Prove that the
SOR method converges for this matrix and for 0 < œâ < 2.
12.6
Let A be a TST matrix such that a1,1 = Œ± and a1,2 = Œ≤. Show that the
Jacobi iteration converges if 2|Œ≤| < |Œ±|. Moreover, prove that if convergence
is required for all d ‚â•1 then this inequality is necessary as well as suÔ¨Écient.
12.7
Demonstrate that
d

‚Ñì=1
sin2
	 œÄj‚Ñì
d + 1

= 1
2(d + 1),
j = 1, 2, . . . , d,
thereby verifying (12.29).

Exercises
289
12.8
Let
A =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
Œ±1
Œ≤1
0
¬∑ ¬∑ ¬∑
0
Œ≥1
Œ±2
Œ≤2
...
...
0
...
...
...
0
...
...
Œ≥d‚àí2
Œ±d‚àí1
Œ≤d‚àí1
0
¬∑ ¬∑ ¬∑
0
Œ≥d‚àí1
Œ±d
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
,
where Œ≤‚Ñì, Œ≥‚ÑìÃ∏= 0, ‚Ñì= 1, 2, . . . , d ‚àí1.
Prove that j, where j‚Ñì= ‚Ñì, ‚Ñì=
1, 2, . . . , d, is a compatible ordering vector of A.
12.9
Find an ordering vector for a matrix with the following sparsity pattern:
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
√ó
√ó
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
√ó
√ó
√ó
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
√ó
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
√ó
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
√ó
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
√ó
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
√ó
√ó
√ó
‚ó¶
‚ó¶
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
√ó
√ó
√ó
‚ó¶
‚ó¶
‚ó¶
‚ó¶
‚ó¶
√ó
‚ó¶
√ó
√ó
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
.
12.10‚ãÜ
We consider the d √ó d tridiagonal Toeplitz matrix
A =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
2
1
0
¬∑ ¬∑ ¬∑
0
‚àí1
2
1
...
...
0
...
...
...
0
...
...
‚àí1
2
1
0
¬∑ ¬∑ ¬∑
0
‚àí1
2
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
.
a Prove that A possesses a compatible ordering vector.
b Find explicitly all the eigenvalues of the Jacobi iteration matrix B. (Hint:
Solve explicitly the diÔ¨Äerence equation obeyed by the components of an eigen-
vector of B.) Conclude that this iterative scheme diverges.
c Using Theorem 12.8, or otherwise, show that œÅ(Lœâ) > 1 and that the SOR
iteration diverges for all choices of œâ ‚àà(0, 2).
12.11
Let A be an m2 √ó m2 matrix that originates in the implementation of the
Ô¨Åve-point formula in a square m √ó m grid. For every grid point (r, s) we let
j(r,s) := m + s ‚àír,
r, s, = 1, 2, . . . , m,

290
Classical iterative methods
and we construct a vector j by assembling the components in the same order
as that used in the matrix A. Prove that j is an ordering vector of A and
identify a permutation for which j is a compatible ordering vector.
12.12
Find a compatible ordering vector for the L-shaped grid (12.49).

13
Multigrid techniques
13.1
In lieu of a justiÔ¨Åcation . . .
How good is the Gauss‚ÄìSeidel iteration (12.21) at solving the Ô¨Åve-point equations on
an m √ó m grid? On the face of it, posing this question just after we have completed
a whole chapter devoted to iterative methods is neither necessary nor appropriate.
According to (12.51), the spectral radius of the iteration matrix is cos2[œÄ/(m + 1)] ‚âà
1 ‚àíœÄ2m‚àí2 and inspection of the third row of Fig. 12.5 will convince us that this
presents a fair estimate of the behaviour of the scheme. Yet, by its very nature, the
spectral radius displays the asymptotic attenuation rate of the error and it is entirely
legitimate to query how well (or badly) Gauss‚ÄìSeidel performs before the onset of its
asymptotic regime.
Figure 13.1 displays the logarithm of the Euclidean norm of the residual for m =
10, 20, 40, 80; we remind the reader that, given the equation
Ax = b
(13.1)
and a sequence of iterations {x[k]}‚àû
i=0, the residual is deÔ¨Åned as r[k] = Ax[k] ‚àíb,
k ‚â•0.1 The emerging picture is startling: the norm drops dramatically in the Ô¨Årst
few iterations! Only after a while does the rate of attenuation approach the linear
curve predicted by the spectral radius of L1. Moreover, this phenomenon ‚Äì unlike
the asymptotic rate of decay of ln ‚à•r[k]‚à•‚Äì appears to be fairly independent of the
magnitude of m.
A similar lesson can be drawn from Fig. 13.2, where we have displayed in detail
the residuals for the Ô¨Årst six even numbers of iterations for m = 20. Evidently, a great
deal of the error disappears very fast indeed, while after about ten iterations nothing
much changes and each further iteration removes roughly cos2(œÄ/21) ‚âà0.9778 of the
remaining residual.
The explanation of this phenomenon is quite interesting, as far as the understand-
ing of Gauss‚ÄìSeidel is concerned. More importantly, it provides a clue about how
to accelerate iterative schemes for linear algebraic equations that originate in Ô¨Ånite
diÔ¨Äerence and Ô¨Ånite element discretizations.
We hasten to confess that limitations of space and of the degree of mathematical so-
phistication that we allow ourselves in this book preclude us from providing a compre-
1There exists an intimate connection between r[k] and the error Œµ[k] = x[k] ‚àíÀÜx, where ÀÜx is the
solution of (13.1) ‚Äì see Exercise 13.1.
291

292
Multigrid techniques
5
10
15
20
0
1
2
3
5
10
15
20
1.0
1.5
2.0
2.5
3.0
5
10
15
20
1.0
1.5
2.0
2.5
3.0
3.5
5
10
15
20
1.5
2.0
2.5
3.0
3.5
4.0
10 √ó 10
20 √ó 20
40 √ó 40
80 √ó 80
ln ‚à•r[k]‚à•
ln ‚à•r[k]‚à•
Figure 13.1
The logarithm of the norm of the residual in the Ô¨Årst 20 Gauss‚ÄìSeidel
iterations for the Ô¨Åve-point discretization of the Poisson equation (8.33).
0
0.5
1.0
0
0.5
1.0
0
2
4
0
0.5
1.0
0
0.5
1.0
0
2
4
0
0.5
1.0
0
0.5
1.0
0
2
4
0
0.5
1.0
0
0.5
1.0
0
2
4
0
0.5
1.0
0
0.5
1.0
0
2
4
0
0.5
1.0
0
0.5
1.0
0
2
4
k = 2
k = 4
k = 6
k = 8
k = 10
k = 12
Figure 13.2
The residual after various small numbers k of Gauss‚ÄìSeidel iterations
for the Ô¨Åve-point discretization of the Poisson equation (8.33) with m = 20.

13.1
In lieu of a justiÔ¨Åcation . . .
293
hensive explanation of the aforementioned phenomenon. Instead, we plan to indulge
in a great deal of mathematical hand-waving, our excuse being that it conveys the
spirit, if not the letter, of a complete analysis and sets us on the right path to exploit
the phenomenon in presenting superior iterative schemes.
Let us subtract the exact Ô¨Åve-point equations,
uj‚àí1,‚Ñì+ uj,‚Ñì‚àí1 + uj+1,‚Ñì+ uj,‚Ñì+1 ‚àí4uj,‚Ñì= (‚àÜx)2fj,‚Ñì,
j, ‚Ñì= 1, 2, . . . , m,
from the Gauss‚ÄìSeidel scheme
u[k+1]
j‚àí1,‚Ñì+ u[k+1]
j,‚Ñì‚àí1 + u[k]
j+1,‚Ñì+ u[k]
j,‚Ñì+1 ‚àí4u[k+1]
j,‚Ñì
= (‚àÜx)2fj,‚Ñì,
j, ‚Ñì= 1, 2, . . . , m.
The outcome is
Œµ[k+1]
j‚àí1,‚Ñì+ Œµ[k+1]
j,‚Ñì‚àí1 + Œµ[k]
j+1,‚Ñì+ Œµ[k]
j,‚Ñì+1 ‚àí4Œµ[k+1]
j,‚Ñì
= 0,
j, ‚Ñì= 1, 2, . . . , m,
(13.2)
where Œµ[k]
j,‚Ñì:= u[k]
j,‚Ñì‚àíuj,‚Ñìis the error after k iterations at the (j, ‚Ñì)th grid point. Since
we assume Dirichlet boundary conditions, uj,‚Ñìand u[k]
j,‚Ñìare identical at all boundary
grid points, therefore Œµ[k]
j,‚Ñì= 0 there.
Let
p[k](Œ∏, œà) =
m

j=1
m

‚Ñì=1
Œµ[k]
j,‚Ñìei(jŒ∏+‚Ñìœà),
0 ‚â§Œ∏, œà ‚â§2œÄ,
be a bivariate Fourier transform of the sequence {Œµ[k]
j,‚Ñì}m
j,‚Ñì=1. We have already con-
sidered Fourier transforms in a more formal setting in Section 12.3 and subsequently
we will employ them (to entirely diÔ¨Äerent ends) in Chapters 15‚Äì17. In the present
section our treatment of Fourier transfroms is therefore perfunctory and we will hint
at proofs rather than providing any degree of detail.
We measure the magnitude of p[k] in the Euclidean norm2
|||g||| =
 1
4œÄ2
 œÄ
‚àíœÄ
 œÄ
‚àíœÄ
|g(Œ∏, œà)|2 dŒ∏ dœà
1/2
.
Therefore
|||p[k]|||2 =
1
4œÄ2
 œÄ
‚àíœÄ
 œÄ
‚àíœÄ

m

j=1
m

‚Ñì=1
Œµ[k]
j,‚Ñìei(jŒ∏+‚Ñìœà)

2
dŒ∏ dœà
=
1
4œÄ2
m

j1=1
m

j2=1
m

‚Ñì1=1
m

‚Ñì2=1
Œµ[k]
j1,‚Ñì1 ¬ØŒµ[k]
j2,‚Ñì2
 œÄ
‚àíœÄ
ei(j1‚àíj2)Œ∏ dŒ∏
 œÄ
‚àíœÄ
ei(‚Ñì1‚àí‚Ñì2)œà dœà
=
m

j=1
m

‚Ñì=1
|Œµ[k]
j,‚Ñì|2 = ‚à•Œµ[k]‚à•2,
2Here for convenience we use triple verticals to indicate the Euclidean norm.

294
Multigrid techniques
where
‚à•y‚à•=

m

j=1
m

‚Ñì=1
|yj,‚Ñì|2
1/2
is the standard Euclidean norm on vectors in Cm2 (which, for convenience, we arrange
in m √ó m arrays. Here ‚à•¬∑ ‚à•should not be mistaken for the Euclidean matrix norm;
see A.1.3.4). We have outlined a proof of a remarkable identity, which is at the root
of many applications of Fourier transforms: provided that we measure both vectors
and their transforms in the corresponding Euclidean norms, their magnitudes are the
same.3 Recalling our goal, to measure the rate of decay of the residuals, we deduce
that monitoring |||p[k]||| or ‚à•Œµ[k]‚à•is equivalent.
We multiply (13.2) by ei(jŒ∏+‚Ñìœà) and sum for j, ‚Ñì= 1, 2, . . . , d. Since
m

j=1
m

‚Ñì=1
Œµ[k+1]
j‚àí1,‚Ñìei(jŒ∏+‚Ñìœà) =
m‚àí1

j=0
m

‚Ñì=1
Œµ[k+1]
j,‚Ñì
ei((j+1)Œ∏+‚Ñìœà) = eiŒ∏p[k](Œ∏, œà) ‚àíei(m+1)Œ∏
m

‚Ñì=1
Œµi‚Ñìœà
m,‚Ñì,
applying similar algebra to the other terms in (13.2) we obtain the identity
(4 ‚àíeiŒ∏ ‚àíeiœà)p[k+1](Œ∏, œà) = (e‚àíiŒ∏ + e‚àíiœà)p[k](Œ∏, œà)
‚àí
1
ei(m+1)Œ∏
m

‚Ñì=1
Œµ[k+1]
m,‚Ñìei‚Ñìœà + ei(m+1)œà
m

j=1
Œµ[k+1]
j,m eijŒ∏
+
m

‚Ñì=1
Œµ[k]
1,‚Ñìei‚Ñìœà +
m

j=1
Œµ[k]
j,1eijŒ∏
2
.
This is a moment when we commit a mathematical crime and assume that the term in
the curly brackets is so small in comparison with p[k] and p[k+1] that it can be harm-
lessly neglected. Our half-hearted excuse is that this term sums over m components,
whereas p[k], say, sums over m2, and that if boundary conditions were periodic rather
than Dirichlet, it would have disappeared altogether. However, the true justiÔ¨Åcation,
as for most other crimes, is that it pays.
The main idea now is to consider how fast the Gauss‚ÄìSeidel iteration attentuates
each individual wavenumber (Œ∏, œà).
In other words, we are interested in the local
attenuation factor
œÅ[k](Œ∏, œà) :=

p[k+1](Œ∏, œà)
p[k](Œ∏, œà)
 ,
|Œ∏|, |œà| ‚â§œÄ.
Having agreed that
(4 ‚àíeiŒ∏ ‚àíeiœà)p[k+1](Œ∏, œà) ‚âà(e‚àíiŒ∏ + e‚àíiœà)p[k](Œ∏, œà),
|Œ∏|, |œà| ‚â§œÄ,
we can make the following estimate
œÅ[k](Œ∏, œà) ‚âàÀúœÅ(Œ∏, œà) :=

eiŒ∏ + eiœà
4 ‚àíeiŒ∏ ‚àíeiœà
 ,
|Œ∏|, |œà| ‚â§œÄ.
(13.3)
3Lemma 16.9 provides a more formal statement of this important result, as well as a complete proof
in a single dimension. The generalization to bivariate ‚Äì indeed, multivariate ‚Äì Fourier transforms is
straightforward.

13.1
In lieu of a justiÔ¨Åcation . . .
295
‚àí2
0
2
‚àí2
0
2
0
0.5
1.0
‚àí2
0
2
‚àí2
0
2
0
0.5
1.0
ÀúœÅ
Figure 13.3
The function ÀúœÅ as a three-dimensional surface and as a contour plot.
The left-hand column displays the whole square [‚àíœÄ, œÄ] √ó [‚àíœÄ, œÄ], while the right-
hand column displays only the set O0.
Note that the function ÀúœÅ is independent of k. On the left in Fig. 13.3 the whole square
[‚àíœÄ, œÄ] √ó [‚àíœÄ, œÄ] is displayed and there are no surprises there; thus, ÀúœÅ(Œ∏, œà) ‚â§1 for
all |Œ∏|, |œà| ‚â§œÄ.
Considerably more interesting is the right-hand column of Fig. 13.3, where the
function ÀúœÅ is displayed just in the set
O0 :=

(Œ∏, œà) :
1
2œÄ ‚â§max{|Œ∏|, |œà|} ‚â§œÄ

of oscillatory wavenumbers. A remarkable feature emerges: provided only such wave-
numbers are considered, the function ÀúœÅ peaks at the value 1
2.
Forearmed with this observation, we formally evaluate the maximum of ÀúœÅ within
the set O0. It is not diÔ¨Écult to verify that
max
(Œ∏,œà)‚ààO0
ÀúœÅ(Œ∏, œà) = ÀúœÅ
 œÄ
2 , tan‚àí1 3
4

= 1
2.
This conÔ¨Årms our observation: as soon as we disregard non-oscillatory wavenum-
bers, the amplitude of the error is halved in each iteration!
This at last explains
the phenomenon that we observe in Figs. 13.1 and 13.2.
To start with, the error
is typically a linear combination of many wavenumbers, oscillatory as well as non-
oscillatory.
A Gauss‚ÄìSeidel iteration attenuates the oscillatory components much
faster, and this means that the contribution of the latter is, to all practical purposes,

296
Multigrid techniques
0
0.5
1.0
0
0.5
1.0
‚àí50
0
50
0
0.5
1.0
0
0.5
1.0
‚àí50
0
50
0
0.5
1.0
0
0.5
1.0
‚àí50
0
50
0
0.5
1.0
0
0.5
1.0
‚àí50
0
50
0
0.5
1.0
0
0.5
1.0
‚àí50
0
50
0
0.5
1.0
0
0.5
1.0
‚àí50
0
50
imaginary
real
k = 2
k = 4
k = 6
Figure 13.4
Real and imaginary components of Œµ[k] for m = 20 and k = 2, 4, 6.
Note the rapid elimination rate of the highly oscillatory components.
completely eliminated after only a few iterations. The non-oscillatory terms, how-
ever, are left and they account for the sedate and plodding rate of attenuation in the
asymptotic regime.
To rephrase this state of aÔ¨Äairs, the Gauss‚ÄìSeidel scheme is a smoother: its eÔ¨Äect
after a few iterations is to Ô¨Ålter out high frequencies from the ‚Äòsignal‚Äô. This becomes
apparent in Fig. 13.4, where the real and imaginary parts of Œµ[k] are displayed for
a 20 √ó 20 grid and k = 2, 4, 6. This is perhaps the place to emphasize that not all
iterative methods from Chapter 12 are smoothers; far from it. In Exercise 13.2, for
example, it is demonstrated that this attribute is absent from the Jacobi method.
Had this been all there were to it, the smoothing behaviour of Gauss‚ÄìSeidel would
be not much more than a mathematical curiosity. Suppose that we wish to solve the
Ô¨Åve-point equations to a given tolerance Œ¥ > 0. The fast attenuation of the highly
oscillatory components does not advance perceptibly the instant when ‚à•Œµ[k]‚à•< Œ¥
(or, in a realistic computer program, ‚à•r[k]‚à•< Œ¥); it is the straggling non-oscillatory
wavenumbers that dictate the rate of convergence. Figure 12.5 does not lie: Gauss‚Äì
Seidel, in complete agreement with the theory of Chapter 12, will perform just twice
as well as Jacobi (which, according to Exercise 13.2, is not a smoother). Fortunately,
there is much more to the innocent phrase ‚Äòhighly oscillatory components‚Äô, and this

13.1
In lieu of a justiÔ¨Åcation . . .
297
0
0.2
0.4
0.6
0.8
1.0
‚àí1
0
1
0
0.2
0.4
0.6
0.8
1.0
‚àí1
0
1
0
0.2
0.4
0.6
0.8
1.0
‚àí1
0
1
coarse
Ô¨Åne
continuum
Figure 13.5
Now you see it, now you don‚Äôt . . . : A highly oscillatory component
and its restrictions to a Ô¨Åne and to a coarse grid.
forms our Ô¨Ånal clue about how to accelerate the Gauss‚ÄìSeidel iteration.
Let us ponder for a moment the meaning of ‚Äòhighly oscillatory components‚Äô. A grid
‚Äì any grid ‚Äì is a set of peepholes to the continuum, say [0, 1] √ó [0, 1]. The continuum
supports all possible frequencies and wavenumbers, but this is not the case with a
grid. Suppose that the frequency is so high that a wave oscillates more than once
between grid points ‚Äì this high oscillation will be invisible on the grid! More precisely,
observing the continuum through the narrow slits of the grid, we will, in all probability,
register the wave as non-oscillatory. An example is presented in Fig. 13.5 where, for
simplicity, we have conÔ¨Åned ourselves to a single dimension. The top graph displays
the highly oscillatory wave sin 20œÄx, x ‚àà[0, 1]. In the middle graph the signal has been
sampled at 23 equidistant points, and this renders faithfully the oscillatory nature of
the sinusoidal wave. However, in the bottom graph we have thrown away every second
point. The new graph, with 12 points, completely misses the high frequency!
The concept of a ‚Äòhigh oscillation‚Äô is, thus, a feature of a speciÔ¨Åc grid.
This
means that on grids of diÔ¨Äerent spacing the Gauss‚ÄìSeidel iteration attenuates diÔ¨Äer-
ent wavenumbers rapidly. Suppose that we coarsen a grid by taking out every second
point, the outcome being a new square grid in [0, 1] √ó [0, 1] but with ‚àÜx replaced by
2‚àÜx. The range of the former high frequencies O0 is no longer visible on the coarse
grid. Instead, the new grid has its own range of high frequencies, on which Gauss‚Äì
Seidel performs well ‚Äì as far as the Ô¨Åne grid is concerned, these correspond to the

298
Multigrid techniques
O0
O1
O2
-
6
Œ∏
œà
Figure 13.6
Nested sets Os ‚äÇ[‚àíœÄ, œÄ], denoted by diÔ¨Äerent shading.
wavenumbers
O1 :=

(Œ∏, œà) :
1
4œÄ ‚â§max{|Œ∏|, |œà|} ‚â§1
2œÄ

.
Needless to say, there is no need to stop with just a single coarsening. In general,
we can cover the whole range of frequencies by a hierarchy of grids, embedded into
each other, whose (grid-speciÔ¨Åc) high frequencies correspond, as far as the Ô¨Åne grid is
concerned, to the sets
Os :=

(Œ∏, œà) : 2‚àís‚àí1œÄ ‚â§max{|Œ∏|, |œà|} ‚â§2‚àísœÄ

,
s = 1, 2, . . . , ‚åälog2(m + 1)‚åã.
The sets Os nest inside each other (see Fig. 13.6) and their totality is the whole of
[‚àíœÄ, œÄ] √ó [‚àíœÄ, œÄ].
In the next section we describe a computational technique that
sweeps across the sets Os, damping the highly oscillatory terms and using Gauss‚Äì
Seidel in its ‚Äòfast‚Äô mode throughout the entire iterative process.
13.2
The basic multigrid technique
Let us suppose for simplicity that m = 2s ‚àí1 and let us embed our grid (and from
here on we designate it as the Ô¨Ånest grid) in a hierarchy of successively coarser grids,
as indicated in Fig. 13.7.
The main idea behind the multigrid technique is to travel up and down the grid
hierarchy, using Gauss‚ÄìSeidel iterations to dampen the (locally) highly oscillating
components of the error. Coarsening means that we are descending down the hierar-
chy to a coarser grid (in other words, getting rid of every other point), while reÔ¨Ånement

13.2
The basic multigrid technique
299












































































































































































































Ô¨Ånest grid
coarsest grid
?
coarsening
6
reÔ¨Ånement
Figure 13.7
Nested grids, from the Ô¨Ånest to the coarsest.
is the exact opposite, ascending from a coarser to a Ô¨Åner grid. Our goal is to solve
the Ô¨Åve-point equations on the Ô¨Ånest grid ‚Äì the coarser grids are just a means to that
end.
In order to describe a multigrid algorithm we need to explain exactly how each
coarsening or reÔ¨Ånement step is performed, as well as to specify the exact strategy
of how to start, when to coarsen, when to reÔ¨Åne and when to terminate the entire
procedure.
To describe reÔ¨Ånement and coarsening it is enough to assume just two grids, one
Ô¨Åne and one coarse. Suppose that we are solving the equation
Afxf = vf
(13.4)
on the Ô¨Åne grid. Having performed a few Gauss‚ÄìSeidel iterations, so as to smooth the
high frequencies, we let rf := Afxf ‚àívf be the residual. This residual needs to be
translated into the coarser grid. This is done by means of a restriction matrix R such
that
rc = Rrf.
(13.5)

300
Multigrid techniques
Remember the whole idea behind the multigrid technique: the vector rf is constructed
from low-frequency components (relative to the Ô¨Åne grid). Hence it makes sense to
go on smoothing the coarsened residual rc on the coarser grid.4 To that end we set
vc := ‚àírc, and so solve
Acxc = ‚àírc.
(13.6)
The matrix Ac is, of course, the matrix of the original system (in our case, the matrix
originating from the Ô¨Åve-point scheme (8.16)) restricted to the coarser grid.
To move in the opposite direction, from coarse to Ô¨Åne, suppose that xc is an
approximate solution of (13.6), an outcome of Gauss‚ÄìSeidel iterations on this and yet
coarser grids. We translate xc into the Ô¨Åne grid in terms of the prolongation matrix
P, where
yf = Pxc
(13.7)
and update the old value of xf,
x
new
f
= x
old
f
+ yf.
(13.8)
Let us evaluate the residual rnew
f
under the assumption that xc is the exact solution
of (13.6). Since
r
new
f
= Afx
new
f
‚àívf = Af(x
old
f
+ yf) ‚àívf,
(13.7) and (13.8) yield
r
new
f
= r
old
f
+ Afyf = r
old
f
+ AfPxc.
Therefore, by (13.6),
r
new
f
= r
old
f
‚àíAfPA‚àí1
c rc.
Finally, invoking (13.5), we deduce that
r
new
f
= (I ‚àíAfPA‚àí1
c R) r
old
f .
(13.9)
Thus, the sole contribution to the new residual comes from replacing the Ô¨Åne grid by a
coarser one. Similar reasoning is valid even if xc is an approximate solution of (13.6),
provided that some bandwidths of wavenumbers have been eliminated in the course of
the iteration. Moreover, suppose that (other) bandwidths of wavenumbers have been
already Ô¨Åltered out of the residual rold
f . Upon the update (13.8), the contribution of
both bandwidths is restricted to the minor ill eÔ¨Äects of the restriction and prolongation
matrices.
Both the restriction and prolongation matrices are rectangular, but it is a very poor
idea to execute them naively as matrix products. The proper procedure is to describe
their eÔ¨Äect on individual components of the grid, since this provides a convenient and
cheap algorithm as well as clarifying what are we trying to do in mathematical terms.
Let wf = Pwc, where wc = (wc
j,‚Ñì)m
j,‚Ñì=1 (a subscript has just been promoted to a
4To be exact, we have advanced an argument to justify this assertion for the error, rather than
the residual. However, it is clear from Exercise 13.1 that the two assertions are equivalent. Of course,
the residual, unlike the error, has an important virtue: we can calculate it without knowing the exact
solution of the linear system. . .

13.2
The basic multigrid technique
301
superscript, for notational convenience) and wf = (wf
j,‚Ñì)2m+1
j,‚Ñì=1 . The simplest way of
restricting a grid is injection,
w
c
j,‚Ñì= w
f
2j,2‚Ñì,
j, ‚Ñì= 1, 2, . . . , m,
(13.10)
but a popular alternative is full weighting
w
c
j,‚Ñì= 1
4w
f
2j,2‚Ñì+ 1
8(w
f
2j‚àí1,2‚Ñì+ w
f
2j,2‚Ñì‚àí1 + w
f
2j+1,2‚Ñì+ w
f
2j,2‚Ñì+1) + 1
16(w
f
2j‚àí1,2‚Ñì‚àí1
+ w
f
2j+1,2‚Ñì‚àí1 + w
f
2j‚àí1,2‚Ñì+1 + w
f
2j+1,2‚Ñì+1),
j, ‚Ñì= 1, 2, . . . , m.
(13.11)
The latter can be rendered as a computational stencil (see Section 8.2) in the form


















1
16
1
16
1
16
1
16
1
8
1
8
1
8
1
8
1
4
wc =
wf
Why bother with (13.11), given the availability of the more natural injection
(13.10)?
One reason is that in the latter case R =
1
4P ‚ä§for the prolongation P
that we are just about to introduce in (13.12) (the factor 1
4 originates in the fourfold
decrease in the number of grid points in coarsening), and this has important theoreti-
cal and practical advantages. Another is that in this manner all points from the Ô¨Åner
grid contribute equally.
There is just one sensible way of prolonging a grid: linear interpolation. The exact
equations are
w
f
2j‚àí1,2‚Ñì‚àí1 = w
c
j,‚Ñì,
j, ‚Ñì= 1, 2, . . . , m;
w
f
2j‚àí1,2‚Ñì= 1
2(w
c
j,‚Ñì+ w
c
j,‚Ñì+1),
j = 1, 2, . . . , m ‚àí1,
‚Ñì= 1, 2, . . . , m;
w
f
2j,2‚Ñì‚àí1 = 1
2(w
c
j,‚Ñì+ w
c
j+1,‚Ñì),
j = 1, 2, . . . , m,
‚Ñì= 1, 2, . . . , m ‚àí1;
w
f
2j,2‚Ñì= 1
4(w
c
j,‚Ñì+ w
c
j,‚Ñì+1
+ w
c
j+1,‚Ñì+ w
c
j+1,‚Ñì+1),
j, ‚Ñì= 1, 1, . . . , m ‚àí1.
(13.12)
The values of wf along the boundary are, of course, zero; recall that we are dealing
with residuals!
Having learnt how to travel across the hierarchy of nested grids, we now need to
specify an itinerary. There are many distinct multigrid strategies and here we mention
just the simplest (and most popular), the V-cycle
c
c
c
c
c
c
c
c
c
c
c
c
c
J
JJ^
J
JJ^
J
JJ^ 










 J
JJ^
J
JJ^
J
JJ^ 











Ô¨Ånest
coarsest
coarsening
reÔ¨Ånement

302
Multigrid techniques
The whole procedure commences and ends at the Ô¨Ånest grid. To start with, we
stipulate an initial condition, let vf = b (the original right-hand side of the linear
system (13.1)) and iterate a small number of times ‚Äì nr, say ‚Äì with Gauss‚ÄìSeidel.
Subsequently we evaluate the residual rf, restrict it to the coarser grid, perform nr
further Gauss‚ÄìSeidel iterations, evaluate the residual, again restrict and so on, until
we reach the coarsest grid, with just a single grid point, which we solve exactly. (In
principle, it is possible to stop this procedure earlier, deciding that a 15 √ó 15 system,
say, can be solved directly without further coarsening.) Having reached this stage,
we have successively damped the inÔ¨Çuence of error components in the entire range of
wavenumbers supported by the Ô¨Ånest grid, except that a small amount of error might
have been added by restriction.
When we reach the coarsest grid, we need to ascend all the way back to the Ô¨Ånest.
In each step we prolong, update the residual on the new grid and perform np Gauss‚Äì
Seidel iterations to eliminate errors (corresponding to highly oscillatory wavenumbers
on the grid in question) that might have been introduced by past prolongations.
Having returned to the Ô¨Ånest grid, we have completed the V-cycle. It is now, and
only now, that we check for convergence, by measuring the size of the residual vector.
Provided that the error is below the required tolerance, the iteration is terminated;
otherwise the V-cycle is repeated. This completes the description of the multigrid
algorithm in its simplest manifestation.
13.3
The full multigrid technique
An obvious Achilles heel of all iterative methods is the choice of the starting vector
x[0]. Although the penalty for a wrong choice is not as drastic as in methods for
nonlinear algebraic equations (see Chapter 7), it is nonetheless likely to increase the
cost a great deal. By the same token, an astute choice of x[0] is bound to lead to
considerable savings.
So far, throughout Chapters 12 and 13, we have assumed that x[0] = 0, a choice
which is likely to be as good or as bad as many others for most iterative methods.
The logic of the multigrid approach ‚Äì working in unison on a whole hierarchy of
embedded grids ‚Äì can be complemented by a superior choice of starting value. Why
not use an approximate solution from a coarser grid as the starting value on the
Ô¨Ånest? Of course, at the beginning of the iteration, exactly when the starting value is
required, we have no solution available on the coarser grid, since the V-cycle iteration
commences from the Ô¨Ånest.
The obvious remedy is to start from the coarsest grid and ascend by prolon-
gation, performing np Gauss‚ÄìSeidel iterations on each grid.
This leads to a tech-
nique known as the full multigrid, whereby, upon its arrival at the Ô¨Ånest grid (where
the V-cycles commence), the starting value has been already cleansed of a sub-
stantial proportion of smooth error components.
The self-explanatory pattern is

13.4
Poisson by multigrid
303
illustrated by the graph
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c



 J
JJ^ 






 J
JJ^
J
JJ^ 










 J
JJ^
J
JJ^
J
JJ^ 











Ô¨Ånest
coarsest
The speed-up in convergence of the full multigrid technique, as will be evidenced in
the results of Section 13.4, is spectacular.
The full multigrid combines two ideas: the Ô¨Årst is the multigrid concept of using
Gauss‚ÄìSeidel, say, to smooth the highly oscillatory components by progressing from
Ô¨Åne to coarse grids; the second is nested iteration. The latter uses estimates from a
coarse grid as a starting value for an iteration on a Ô¨Åne grid. In principle, nested itera-
tion can be used whenever an iterative scheme is applied in a grid, without necessarily
any reference to multigrid. An example is provided by the solution of nonlinear alge-
braic equations by means of functional iteration or Newton‚ÄìRaphson (see Chapter 7).
However, it comes into its own in conjunction with multigrid.
13.4
Poisson by multigrid
This chapter is short on theory and, to remedy the situation, we have made it long
on computational results.
Since Chapter 8 we have used a particular Poisson equation, the problem (8.33),
as a yardstick to measure the behaviour of numerical methods, and we will continue
this practice here. The Ô¨Ånest grid used is always 63 √ó 63 (that is, with ‚àÜx =
1
64. We
measure the performance of the methods by the size of the error at the end of each
V-cycle (disregarding, in the case of full multigrid, all but the ‚Äòcomplete V-cycles‚Äô,
from the Ô¨Ånest to the coarsest grid and back again). It is likely that, in practical
error estimation, the residual rather than the error is calculated. This might lead to
diÔ¨Äerent numbers but it will give the same qualitative picture.
We have tested three diÔ¨Äerent choices of the pair (nr, np) for both the ‚Äòregular‚Äô
multigrid from Section 13.2 and the full multigrid technique, Section 13.3. The results
are displayed in Figs. 13.8 and 13.9.
Each Ô¨Ågure displays three detailed iteration strategies: (a) nr = 1, np = 1; (b)
nr = 2, np = 1; and (c) nr = 3, np = 2. We have not printed the outcome of seven
V-cycles (four in Fig. 13.9), since the error is so small that it is likely to be a roundoÔ¨Ä
artefact.
To assess the cost of a single V-cycle, we disregard the expense of restriction and
prolongation, counting just the number of smoothing (i.e., Gauss‚ÄìSeidel) iterations.
The latter are performed on grids of vastly diÔ¨Äerent sizes, but this can be easily in-
corporated into our estimate by observing that the cost of Gauss‚ÄìSeidel is linear in

304
Multigrid techniques
0
0.5
1.0
0
0.5
1.00
2
4
x 10 ‚àí6
0
0.5
1.0
0
0.5
1.0
0
1
2
0
0.5
1.0
0
0.5
1.00
0.01
0.02
0
0.5
1.0
0
0.5
1.0
0
1
2
x 10 ‚àí4
0
0.5
1.0
0
0.5
1.0
0
1
2
3
x 10 ‚àí5
0
0.5
1.0
0
0.5
1.0
0
0.5
1.0
1.5
0
0.5
1.0
0
0.5
1.00
2
4
6
x 10 ‚àí3
0
0.5
1.0
0
0.5
1.00
0.5
1.0
1.5
x 10 ‚àí6
0
0.5
1.0
0
0.5
1.0
0
2
4
x 10 ‚àí6
0
0.5
1.0
0
0.5
1.0
0
0.2
0.4
0.6
0.8
0
0.5
1.0
0
0.5
1.00
0.5
1.0
1.5
x 10 ‚àí3
(a)
nr = 1,
np = 1
(b)
nr = 2,
np = 1
(c)
nr = 3,
np = 2
cycle 1
cycle 3
cycle 5
cycle 7
cycle 1
cycle 3
cycle 5
cycle 7
cycle 1
cycle 3
cycle 5
Figure 13.8
The V-cycle multigrid method for the Poisson equation (8.33).

13.4
Poisson by multigrid
305
0
0.5
1.0
0
0.5
1.0
0
0.5
1.0
x 10 ‚àí3
0
0.5
1.0
0
0.5
1.00
0.5
1.0
1.5
x 10 ‚àí4
0
0.5
1.0
0
0.5
1.0
0
0.5
1.0
1.5
x 10 ‚àí5
0
0.5
1.0
0
0.5
1.00
1
2
3
x 10 ‚àí6
0
0.5
1.0
0
0.5
1.0
0
2
4
6
x 10 ‚àí4
0
0.5
1.0
0
0.5
1.00
2
4
x 10 ‚àí5
0
0.5
1.0
0
0.5
1.0
0
2
4
6
x 10 ‚àí6
0
0.5
1.0
0
0.5
1.00
0.5
1.0
1.5
x 10 ‚àí6
0
0.5
1.0
0
0.5
1.0
0
1
2
3
x 10 ‚àí4
0
0.5
1.0
0
0.5
1.00
0.5
1.0
1.5
x 10 ‚àí5
0
0.5
1.0
0
0.5
1.0
0
0.5
1.0
1.5
x 10 ‚àí6
(a)
nr = 1,
np = 1
(b)
nr = 2,
np = 1
(c)
nr = 3,
np = 2
cycle 1
cycle 2
cycle 3
cycle 4
cycle 1
cycle 2
cycle 3
cycle 4
cycle 1
cycle 2
cycle 3
Figure 13.9
The full multigrid method for the Poisson equation (8.33).

306
Multigrid techniques
the number of grid points, hence a single coarsening decreases its operations count by
a factor 4. Let œñ denote the cost of a single Gauss‚ÄìSeidel iteration on the Ô¨Ånest grid.
Then the cost of one V-cycle is given by
	
1 + 1
4 + 1
42 + 1
43 + ¬∑ ¬∑ ¬∑

(nr + np) œñ ‚âà4
3(nr + np) œñ.
Remarkably, the cost of a V-cycle is linear in the number of grid points on the Ô¨Ånest
grid! 5 Incidentally, the initial phase of full multigrid is even cheaper: it ‚Äòcosts‚Äô about
4
9(nr + np) œñ.
There is no hiding the vastly superior performance of both the basic and the full
versions of multigrid, in comparison with, say, the ‚Äòplain‚Äô Gauss‚ÄìSeidel method. Let
us compare Fig. 12.5 with Fig. 13.8, even though in the Ô¨Årst we have 152 = 225
equations, while the second comprises 632 = 3969. To realize how much slower the
‚Äòplain‚Äô Gauss‚ÄìSeidel method would have been with m = 63, let us compare the spectral
radii (12.51) of its iteration matrices: we Ô¨Ånd ‚âà0.961 939 766 255 64 for m = 16 and
‚âà0.997 592 363 336 10 for m = 63. Given that Gauss‚ÄìSeidel spends almost all its
eÔ¨Äorts in its asymptotic regime, this means that the number of iterations in Fig. 12.5
needs to be multiplied by ‚âà16 to render comparison with Figs. 13.8 and 13.9 more
meaningful.
It is perhaps fairer to compare multigrid with SOR. The spectral radius of the
latter‚Äôs iteration matrix (for m = 63) is, according to (12.52), ‚âà0.906 454 701 582 76,
and this is a great improvement upon Gauss‚ÄìSeidel. Yet the residual after the eighth
V-cycle of ‚Äòplain‚Äô multigrid (with nr = np = 1) is ‚âà2.62 √ó 10‚àí5 and we need 243 SOR
iterations to attain this value. (By comparison, Gauss‚ÄìSeidel requires 6526 iterations
to reduce the residual by a similar amount. Conjugate gradients, the subject of the
next chapter, are marginally better than SOR, requiring just 179 iterations, but this
number can be greatly reduced with good preconditioners.)
Comparison of Figs. 13.8 and 13.9 also conÔ¨Årms that, as expected, full multigrid
further enhances the performance. The reason ‚Äì and this should have been expected
as well ‚Äì is not a better rate of convergence but a superior starting value (on the
Ô¨Ånest grid): in case (a) both versions of multigrid attenuate the error by roughly a
factor of ten per V-cycle. As a matter of interest, and in comparison with the previous
paragraph, the residual of full multigrid (with nr = np = 1) is ‚âà5.8510‚àí6 after Ô¨Åve
V-cycles.
We conclude this ‚Äòiterative olympics‚Äô with a reminder that the errors in Figs. 13.8
and 13.9 (and in Figs. 12.5 and 12.7 also) display the departure of the iterates from
the solution of the Ô¨Åve-point equations (8.16), not from the exact solution of the
Poisson problem (8.33). Given that we are interested in solving the latter by means
of the former, it makes little sense to iterate with any method beyond the theoretical
accuracy of the Ô¨Åve-point approximation. This is not as straightforward as it may
seem, since, as we have already mentioned, practical convergence estimation employs
residuals rather than errors. Having said this, seeking a residual lower than 10‚àí5 (for
m = 63), is probably of no practical signiÔ¨Åcance.
5Our assumption is that all the calculations are performed in a serial, as distinct from a parallel,
computer architecture. Otherwise the results are likely to be even more spectacular.

Comments and bibliography
307
Comments and bibliography
The idea of using a hierarchy of grids has been around for a while, mainly in the context of
nested iteration, but the Ô¨Årst modern treatment of the multigrid technique was presented by
Brandt (1977).
There exist a number of good introductory texts on multigrid techniques, e.g. Briggs
(1987); Hackbusch (1985) and Wesseling (1992). Convergence and complexity (in the present
context complexity means the estimation of computational cost) are addressed in the book
of Bramble (1993) and in a survey by Yserentant (1993). It is important to emphasize that,
although multigrid techniques can be introduced and explained in an elementary fashion,
their convergence analysis is fairly challenging from a mathematical point of view.
The
reason is that the multigrid is an example of a multiscale phenomenon, which coexists along
a hierarchy of diÔ¨Äerent scales. Such phenomena occur in applications (the ingredients of a
physical model often involve diÔ¨Äerent orders of magnitude in both space and time) and are
playing an increasingly greater role in modern scientiÔ¨Åc computation.
Our treatment of multigrid has centred on just one version, the V-cycle, and we mention
in passing that other strategies are perfectly viable and often preferable, e.g. the W-cycle:
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
J
JJ^
J
JJ^
J
JJ^ 


 J
JJ^ 






 J
JJ^
J
JJ^ 


 J
JJ^ 











Ô¨Ånest
coarsest
The number of diÔ¨Äerent strategies and implementations of multigrid is a source of major pre-
occupation to professionals, although it might be at times slightly baÔ¨Ñing to other numerical
analysts and to users of computational algorithms.
Gauss‚ÄìSeidel is not the only smoother, although neither the Jacobi iteration nor SOR
(with œâ Ã∏= 1) possess this welcome property (see Exercise 13.2). An example of a smoother
is provided by a version of the incomplete LU factorization (not the Jacobi line relaxation
from Section 12.1, though; see Exercise 13.3). Another example is Jacobi over-relaxation
(JOR), an iterative scheme that is to Jacobi what SOR is to Gauss‚ÄìSeidel, with a particular
parameter value.
Multigrid methods would be of little use were their applicability restricted to the Ô¨Åve-
point equations in a square. Indeed, possibly the greatest virtue of multigrid is its versatility.
Provided linear equations are speciÔ¨Åed in one grid and we can embed this into a hierarchy
of nested grids of progressive coarseness, multigrid confers an advantage over a single-grid
implementation of iterative methods. Indeed, we use the word ‚Äògrid‚Äô in a loose sense, since
multigrid is, if anything, even more useful for Ô¨Ånite elements than for Ô¨Ånite diÔ¨Äerences!
Bramble, J.H. (1993), Multigrid Methods, Longman, Harlow, Essex.
Brandt, A. (1977), Multi-level adaptive solutions to boundary-value problems, Mathematics
of Computation 31, 333‚Äì390.
Briggs, W.L. (1987), A Multigrid Tutorial, SIAM, Philadelphia.

308
Multigrid techniques
Hackbusch, W. (1985), Multi-Grid Methods and Applications, Springer-Verlag, Berlin.
Wesseling, P. (1992), An Introduction to Multigrid Methods, Wiley, Chichester.
Yserentant, H. (1993), Old and new convergence proofs for multigrid methods, Acta Numerica
2, 285‚Äì326.
Exercises
13.1
Let ÀÜx be the solution of (13.1), Œµ[k] := x[k] ‚àíÀÜx and r[k] := Ax[b] ‚àíb.
Show that r[k] = AŒµ[k]. Further supposing that A is symmetric and that its
eigenvalues reside in the interval [Œª‚àí, Œª+], prove that the inequality
min{|Œª‚àí|, |Œª+|}‚à•Œµ[k]‚à•‚â§‚à•r[k]‚à•‚â§max{|Œª‚àí|, |Œª+|}‚à•Œµ[k]‚à•
holds in the Euclidean norm.
13.2
Apply the analysis of Section 13.1 to the Jacobi iteration (12.20) instead of
the Gauss‚ÄìSeidel iteration.
a Finding an approximate recurrence relation for the Jacobi equivalent of the
function p[k](Œ∏, œà), prove that the local attenuation of the wavenumber (Œ∏, œà)
is approximately
ÀúœÅ(Œ∏, œà) = 1
2| cos Œ∏ + cos œà| =
cos 1
2(Œ∏ + œà) cos 1
2(Œ∏ ‚àíœà)
 .
b Show that the best upper bound on ÀúœÅ in O0 is unity and hence that the
Jacobi iteration does not smooth highly oscillatory terms.
13.3
Using the same method as in the last exercise, show that the line relaxation
method from Section 12.4 is not a good smoother. You should prove that
ÀúœÅ(Œ∏, œà) =
| cos œà|
2 ‚àícos Œ∏
and that it can attain unity in the set O0.
13.4
Assuming that wf
j,‚Ñì= g(j, ‚Ñì), where g is a linear function of both its ar-
guments, and that wc has been obtained by the fully weighted restriction
(13.11), prove that wc
j,‚Ñì= g(2j, 2‚Ñì).

14
Conjugate gradients
14.1
Steepest, but slow, descent
Our approach to iterative methods in Chapter 12 was based, at least implicitly, on
dynamical systems. The solution of the linear system
Ax = b,
(14.1)
where A is a d √ó d real nonsingular matrix and b ‚ààRd, was formulated as an iterated
map
x[k+1] = h(x[k]),
k = 0, 1, 2, . . . ,
(14.2)
where h : Rd ‚ÜíRd. The convergence of this recursive procedure was a consequence
of basic features of the map h: its contractivity (in the spirit of Section 7.1) and Ô¨Åxed
points. Indeed, much of the eÔ¨Äort required to design, analyse and understand methods
of this kind is a reÔ¨Çection of the tension between mathematical attributes of the map
h, which ensure convergence to the right limit, and numerical desiderata that each
iteration should be cheap and that convergence should occur rapidly.
The basic pattern of one-step stationary iteration (14.2) can be generalized by
the inclusion of past values of x[k] or by allowing h to vary.
In this chapter we
intend to adopt a diÔ¨Äerent point of departure altogether and view the problem from
the standpoint of the theory of optimization. The main underlying idea is to restate
(14.1) as the minimization of some function f : Rd ‚ÜíR and apply an optimization
algorithm.
Let us assume for the time being that the matrix A in (14.1) is symmetric and
positive deÔ¨Ånite.
Lemma 14.1
The unique minimum of the function
f(x) = 1
2x‚ä§Ax ‚àíb‚ä§x,
x ‚ààRd,
(14.3)
is the solution of the linear system (14.1).
Proof
We note that ‚àáf(x) = Ax ‚àíb, therefore (14.3) has a unique stationary
point x which is the solution of (14.1). Moreover ‚àá2f(x) = A is positive deÔ¨Ånite,
therefore x is indeed a minimum of f.
We are concerned with iterative algorithms of the following general form. We pick
a starting vector x[0] ‚ààRd. For any k = 0, 1, . . . the calculation stops if the residual
309

310
Conjugate gradients
‚à•‚àáf(x[k])‚à•= ‚à•Ax[k] ‚àíb‚à•is suÔ¨Éciently small. (We are using here the usual Euclidean
norm.) Otherwise, we seek a search direction d[k] ‚ààRd \ {0} that satisÔ¨Åes the descent
condition
df(x[k] + œâd[k])
dœâ

œâ=0
= ‚àáf(x[k])‚ä§d[k] < 0.
(14.4)
In other words,
f(x[k] + œâd[k]) = f(x[k]) + œâ‚àáf(x[k])‚ä§d[k] + O

œâ2
implies that f(x[k] + œâd[k]) < f(x[k]) for a suÔ¨Éciently small step œâ > 0.
The obvious way forward is to choose such a ‚ÄòsuÔ¨Éciently small œâ > 0‚Äô and let
x[k+1] = x[k] + œâd[k]. This will create a monotonically decreasing sequence of non-
negative values f(x[k]) and, according to an elementary theorem of calculus, such a
sequence descends to a limit. However, we can do better and choose the best value of
œâ. Note that
f(x[k] + œâd[k]) = f(x[k]) + œâ‚àáf(x[k])‚ä§d[k] + 1
2œâ2d[k]‚ä§Ad[k]
is a quadratic function. Therefore, we can easily Ô¨Ånd the value of œâ that minimizes
f(x[k] + œâd[k]) by setting its derivative to zero. Letting g[k] = ‚àáf(x[k]), we thus have
œâ[k] = ‚àíd[k]‚ä§g[k]
d[k]‚ä§Ad[k]
.
(14.5)
(Observe that d[k]‚ä§Ad[k] > 0 for d[k] Ã∏= 0, because A is positive deÔ¨Ånite and we are
indeed at a minimum.) In other words,
x[k+1] = x[k] + œâ[k]d[k] = x[k] ‚àíd[k]‚ä§g[k]
d[k]‚ä§Ad[k]
d[k].
(14.6)
The description of our method is not complete without a means of choosing ‚Äògood‚Äô
directions d[k] which ensure that the target function f decays rapidly in each iteration.
The obvious idea is to choose the search direction d[k] for which the function f
decays the fastest at x[k]. Since the gradient there is g[k] Ã∏= 0 (if the gradient vanishes
we are already at the minimum and our labour is over!), we can take d[k] = ‚àíg[k].
This is known as the steepest descent method.1
Although this choice of steepest descent is natural, it leads to a method with
unacceptably slow convergence. As an example, we take a 20√ó20 TST matrix A such
that ak,k = 2 and ak,k+1 = ak+1,k = ‚àí1 (it follows at once from Lemma 12.5 that
this matrix is positive deÔ¨Ånite) and b ‚ààR20 with bk = cos[(k ‚àí1)œÄ/19], k = 1, . . . , 20.
(There is special signiÔ¨Åcance in this particular matrix, but it will be revealed only in
Section 14.3.) The upper plot in Fig. 14.1 displays the Ô¨Årst 100 values of f(x[k]) and,
on the face of it, all is Ô¨Åne: the values decrease monotonically and clearly tend to
1This must not be confused with the identically named, but totally diÔ¨Äerent, method of steepest
descent in the theory of asymptotic expansions of highly oscillatory integrals.

14.1
Steepest descent
311
0
10
20
30
40
50
60
70
80
90
100
‚àí50
‚àí40
‚àí30
‚àí20
‚àí10
0
0
10
20
30
40
50
60
70
80
90
100
‚àí1.0
‚àí0.5
0
0.5
Figure 14.1
The values of f(x[k]) (upper plot) and of the logarithm of the residual
norm log10 ‚à•Ax[k] ‚àíb‚à•(lower plot), for the steepest descent method.
a limit. Unfortunately, this is only half the story and the lower plot is rather more
disheartening. It exhibits the behaviour of log10 ‚à•Ax[k] ‚àíb‚à•. The norm of the residual
‚à•Ax[k] ‚àíb‚à•evidently does not decay monotonically (and there is absolutely no reason
why should it, since in our choice of œâ[k] we have arranged for monotone decay of
f(x[k]), not of the residual) but evidently it does decay on average at an exponential
rate. Even so, the speed is excruciatingly slow and the graph demonstrates that after
100 iterations we cannot expect even two signiÔ¨Åcant digits of accuracy.
The reason for this sluggish performance of the steepest descent method is that if
the ratio of the greatest and smallest eigenvalues of A is large then the level sets of
the function f are exceedingly elongated hyperellipsoids with steep faces. Instead of
travelling down to the bottom of a hyperellipsoid, the iterates bounce ping-pong-like
across the valley. This is demonstrated vividly in Fig. 14.2, where we have taken d = 2
and
A =
 100
1
1
1

,
b =
 20
0

;
note that the ratio of the eigenvalues of A is large. The upper plot describes the
sequence (x[k]
1
‚àíx[k‚àí1]
1
)/(x[k]
2
‚àíx[k‚àí1]
2
), which evidently bounces up and down: very
similar directions are repeated in this back-and-forth journey. (This Ô¨Ågure does not
describe the size of a step, only its direction.) Indeed, it is evident from the lower plot
that the distances ‚à•x[k] ‚àíx[k‚àí1]‚à•do decrease after a while and tend to zero, albeit not
very rapidly. It is, however, the zig-zag pattern of directions that makes the method
so ineÔ¨Äective.
It is possible to prove that the method of steepest descent converges, but this is of
little comfort. It should be apparent by now, having studied Chapter 12, that we seek

312
Conjugate gradients
0
1
2
3
4
5
6
7
8
9
10
‚àí40
‚àí30
‚àí20
‚àí10
0
0
1
2
3
4
5
6
7
8
9
10
0
0.2
0.4
0.6
0.8
1.0
1.2
Figure 14.2
The zig-zag pattern of directions for steepest descent: the sequences
(x[k]
1 ‚àíx[k‚àí1]
1
)/(x[k]
2 ‚àíx[k‚àí1]
2
) (upper plot) and ‚à•x[k] ‚àíx[k‚àí1]‚à•(lower plot).
rapid convergence. There is little point in designing or describing a new approach,
unless it can compete with other leading methods.
The problem is not, we hasten to say, with the general idea of minimizing the
function f. Moreover, the approach of choosing a descent direction d[k], employing
a line search to pick the optimal œâ[k] and updating the iteration according to (14.6)
is perfectly sound. The problem lies in our intuitive and rash choice of the steepest
direction d[k] = ‚àíg[k]. It should be clear by this stage in this book that the right
criteria in the choice of computational methods are overwhelmingly global.
What
looks locally good is often globally disastrous!
14.2
The method of conjugate gradients
To eliminate the root cause of the sluggishness in the steepest descent method we are
compelled to use directions that, rather than repeating themselves, are set well apart.
Recalling that the matrix A is positive deÔ¨Ånite, we say that the vectors u, v ‚ààRd
are conjugate with respect to A if they are nonzero and satisfy u‚ä§Av = 0.2
Multiplying x[k+1] = x[k] + œâ[k]d[k] from the left by the matrix A and subtracting
b from both sides, we have
Ax[k+1] ‚àíb = Ax[k] ‚àíb + œâ[k]Ad[k].
Since g[k] = ‚àáf(x[k]) = Ax[k] ‚àíb, we thus deduce that
g[k+1] = g[k] + œâ[k]Ad[k].
(14.7)
2Conjugacy is a generalization of the more familiar concept of orthogonality (A.1.3.2).

14.2
The method of conjugate gradients
313
Lemma 14.2
Let us suppose that d[k] is conjugate to any vector a ‚ààRd which is
orthogonal to g[k] (in other words, such that a‚ä§g[k] = 0). Then g[k+1] is orthogonal
to a.
Proof
We multiply (14.7) by a‚ä§from the left.
The lemma follows because
a‚ä§g[k] = 0 (orthogonality) and a‚ä§Ad[k] = 0 (conjugacy).
The main idea of the method of conjugate gradients (CG) is to select search direc-
tions d[k] which are conjugate to each other,
d[k]‚ä§Ad[‚Ñì] = 0,
k, ‚Ñì= 0, 1, . . . ,
k Ã∏= ‚Ñì.
(14.8)
SpeciÔ¨Åcally, we commence the iterative procedure with the steepest descent direction,
d[0] = ‚àíg[0], while choosing
d[k+1] = ‚àíg[k+1]+Œ≤[k]d[k],
where
Œ≤[k] = g[k+1]‚ä§Ad[k]
d[k]‚ä§Ad[k] ,
k = 0, 1, . . .
(14.9)
We note that
d[k+1]‚ä§Ad[k] = (‚àíg[k+1] + Œ≤[k]d[k])‚ä§Ad[k] = ‚àíg[k+1]‚ä§Ad[k] + Œ≤[k]d[k]‚ä§Ad[k] = 0,
because of (14.9).
Therefore d[k+1] is conjugate to d[k].
We will soon prove that
the substantially stronger statement (14.8) is true. First, however, we argue that the
direction deÔ¨Åned in (14.9) obeys the descent conditon (14.4).
Using (14.7) and substituting the value of œâ[k] from (14.5), we have
d[k]‚ä§g[k+1] = d[k]‚ä§(g[k] + œâ[k]Ad[k]) = d[k]‚ä§g[k] + œâ[k]d[k]‚ä§Ad[k] = 0.
(14.10)
The deÔ¨Ånition (14.9) of the new search direction, in tandem with (14.10), implies that
d[k+1]‚ä§g[k+1] = (‚àíg[k+1] + Œ≤[k]d[k])‚ä§g[k+1] = ‚àí‚à•g[k+1]‚à•2 < 0
(recall that g[k+1] Ã∏= 0, otherwise we are already at the minimum and the iterative
process terminates) and that d[k+1] is indeed a descent direction.
We wish to prove that (14.8) holds and the directions are conjugate. This will be
done as part of a larger technical theorem, exploring a number of important features
of the CG algorithm. We commence by deÔ¨Åning for each k = 0, 1, . . . the linear spaces
Dk = Sp

d[0], d[1], . . . , d[k]
,
Gk = Sp

g[0], g[1], . . . , g[k]
,
where the span Sp of a set of vectors in Rd was deÔ¨Åned in Section 9.1.
Theorem 14.3
The following assertions are true for all k = 1, 2, . . .
Assertion 1. The linear spaces Dk‚àí1 and Gk‚àí1 are the same.

314
Conjugate gradients
Assertion 2. The direction d[k‚àí1] is conjugate to d[j] for k ‚â•2 and j = 0, 1, . . . , k‚àí2.
Assertion 3. The gradients satisfy the orthogonality condition g[j]‚ä§g[k] = 0, j =
0, 1, . . . , k ‚àí1.
Proof
All three assertions are trivial for k = 1: the Ô¨Årst follows from d[0] = ‚àíg[0],
the second is immediate and the last comes from g[0] = ‚àíd[0] by letting k = 0 in
(14.10).
We continue by induction. Suppose that the assertions of the theorem are true for
k. The Ô¨Årst assertion is easy. Since g[k] ‚ààGk and, by induction, d[k‚àí1] ‚ààDk‚àí1 =
Gk‚àí1 ‚äÇGk, it follows from (14.9) that
d[k] = ‚àíg[k] + Œ≤[k‚àí1]d[k‚àí1] ‚ààGk,
therefore Dk ‚äÜGk. Likewise, since d[k‚àí1], d[k] ‚ààDk, we again deduce from (14.9) that
g[k] = Œ≤[k‚àí1]d[k‚àí1] ‚àíd[k] ‚ààDk,
therefore Gk ‚äÜDk. Consequently Dk = Gk and the Ô¨Årst assertion of the theorem is
true for k + 1.
We turn our attention to the second assertion and note that we have already shown
that d[k]‚ä§Ad[k‚àí1] = 0. Therefore, to advance the inductive argument we need to show
that d[k]‚ä§Ad[j] = 0 for j = 0, 1, . . . , k ‚àí2. (If k = 1 then there is nothing to show!)
According to (14.9), this is equivalent to
‚àíg[k]‚ä§Ad[j] + Œ≤[k‚àí1]d[k‚àí1]‚ä§Ad[j] = 0,
j = 0, 1, . . . , k ‚àí2,
but according to the induction assumption d[k‚àí1]‚ä§Ad[j] = 0 within this range. There-
fore it is enough to demonstrate that g[k]‚ä§Ad[j] = 0 for j = 0, 1, . . . , k ‚àí2.
It follows from (14.7), replacing k by j, that g[j+1] ‚àíg[j] = œâ[j]Ad[j]. Therefore
œâ[j]g[k]‚ä§Ad[j] = œâ[j]g[k]‚ä§(g[j+1] ‚àíg[j]) = 0
for j = 0, 1, . . . , k ‚àí2, because the third assertion and the inductive argument mean
that g[k]‚ä§g[j] = 0 for j = 0, 1, . . . , k ‚àí1.
Since œâ[j] Ã∏= 0 (actually, œâ[j] > 0, be-
cause d[j]‚ä§g[j] < 0 and d[j]‚ä§Ad[j] > 0, cf. (14.5)), it follows that g[k]‚ä§Ad[j] = 0,
j = 0, 1, . . . , k ‚àí2 and we have proved that the second assertion is valid for k + 1.
All that remains is to prove that we can advance the third assertion from k to
k + 1, i.e. that g[j]‚ä§g[k+1] = 0, j = 0, 1, . . . , k. However, we have already proved that
Gk = Dk, therefore this is equivalent to d[j]‚ä§g[k+1] = 0, j = 0, 1, . . . , k. Furthermore,
(14.10) implies that the latter is true for j = k, therefore we need to check just the
range j = 0, 1, . . . , k ‚àí1.
According to the induction assumption applied to the third assertion, it is true
that d[j]‚ä§g[k] = 0, j = 0, 1, . . . , k ‚àí1. Therefore
d[j]‚ä§g[k+1] = 0
‚áê‚áí
d[j]‚ä§(g[k+1] ‚àíg[k]) = 0,
j = 0, 1, . . . , k ‚àí1,

14.2
The method of conjugate gradients
315
and it is the claim on the right that we now prove. Because of (14.7), this statement
is identical to
œâ[k]d[j]‚ä§Ad[k] = 0,
j = 0, 1, . . . , k ‚àí1,
which follows at once from the conjugacy of d[k] and d[j], j = 0, 1, . . . , k‚àí1, the second
assertion of the theorem, which we have already proved.
This completes the inductive step and the proof of the theorem.
Note the clever way in which the second and third assertions are intertwined in
the proof of the theorem: we need the third assertion to advance the induction for
the second, but this is repaid by the second assertion, which is required to prove the
third.
Corollary
Once the CG method is applied in exact arithmetic, it terminates in at
most d steps.
Proof
Because of the third assertion of the theorem, the sequence {g[0], g[1], . . .}
consists of mutually orthogonal nonzero vectors unless g[r] = 0 for some r ‚â•0, whence
the method terminates.
Since there cannot be more than d mutually orthogonal
nonzero vectors in Rd, we deduce that the iterative procedure terminates and, in
addition, r ‚â§d.
Real computers work in Ô¨Ånite-precision arithmetic. Once d is large, as it inevitably
is in the problems of concern in this book, roundoÔ¨Äerrors accumulate and cause
gradual deterioration in the orthogonality of the g[k].
Thus the method does not
necessarily terminate in at most d (or any Ô¨Ånite number of) steps.
Even so, its
convergence represents a vast improvement upon the method of steepest descent.
3 A simple example of conjugate gradients
We now revisit the linear
system from Section 14.1 that demonstrated the sluggishness of the method
of steepest descent. Thus, A is a 20 √ó 20 TST matrix with ak,k = 2 and
ak+1,k = ak,k+1 = ‚àí1, while b ‚ààR20 is deÔ¨Åned by bk = cos(k ‚àí1)œÄ/19.
Figure 14.3 depicts the values of f(x[k]) and, in the lower plot, the decimal
logarithm of the norm of the residual, the same information that we have
already reported in Fig. 14.1 for the method of steepest descent (except that
now we stop after just 20 iterations). The diÔ¨Äerence could not be greater!
The logarithm of the norm (which roughly corresponds to the number of exact
decimal digits in the solution) decreases gently for a while and then, in the
ninth iteration, drops suddenly down to the least value allowed by machine
accuracy. Not much happens afterwards: the iterative procedure delivers all
it can in nine iterations.
Note another interesting point. The corollary to Theorem 14.3 stated that in
exact arithmetic we need at least d = 20 steps, but in reality we have reached
the exact solution (up to machine precision) in half that number of iterations.
This is not an accident of fate but a structural feature of the CG method,
which we will exploit to good eÔ¨Äect in the next section.
3

316
Conjugate gradients
0
2
4
6
8
10
12
14
16
18
20
‚àí50
‚àí45
‚àí40
‚àí35
‚àí30
‚àí25
0
2
4
6
8
10
12
14
16
18
20
‚àí15
‚àí10
‚àí5
0
Figure 14.3
The values of f(x[k]) (upper plot) and of the logarithm of the residual
norm, log10 ‚à•Ax[k] ‚àíb‚à•(lower plot), for the conjugate gradients method.
The time has come to gather all the strands together and present the CG algorithm
in a convenient form. To this end we let r[k] = ‚àíg[k] = b ‚àíAx[k], k = 0, 1, . . . , be the
residual. Putting together (14.7) and (14.9), we have
Œ≤[k] = g[k+1]‚ä§Ad[k]
d[k]‚ä§Ad[k]
= g[k+1]‚ä§
g[k+1] ‚àíg[k]
d[k]‚ä§
g[k+1] ‚àíg[k] .
However, by Theorem 14.3 the g[k] are orthogonal, therefore
g[k+1]‚ä§(g[k+1] ‚àíg[k]) = ‚à•g[k+1]‚à•2 = ‚à•r[k+1]‚à•2.
Moreover, by (14.10) we have d[k]‚ä§g[k+1] = d[k‚àí1]‚ä§g[k] = 0. Therefore it follows from
(14.9) that
d[k]‚ä§(g[k+1] ‚àíg[k]) = ‚àíd[k]‚ä§g[k] = ‚àí(‚àíg[k] + Œ≤[k‚àí1]d[k‚àí1])‚ä§g[k] = ‚à•g[k]‚à•2 = ‚à•r[k]‚à•2.
We deduce the somewhat neater form
Œ≤[k] = ‚à•r[k+1]‚à•2
‚à•r[k]‚à•2 .
The standard form of the CG algorithm
The ‚Äòplain vanilla‚Äô conjugate gradients
method consists of the following steps.
Step 1. Set x[0] = 0 ‚ààRd, r[0] = b and d[0] = r[0]. Let k = 0.

14.3
Krylov subspaces and preconditioners
317
Step 2. Stop when ‚à•r[k]‚à•is acceptably small.
Step 3. If k ‚â•1 (i.e., except for the initial step) set Œ≤[k‚àí1] = ‚à•r[k]‚à•2/‚à•r[k‚àí1]‚à•2 and
d[k] = r[k] + Œ≤[k‚àí1]d[k‚àí1].
Step 4. Calculate the matrix‚Äìvector product v[k] = Ad[k], subsequently letting œâ[k] =
‚à•r[k]‚à•2/

d[k]‚ä§v[k]
.
Step 5. Form the new iteration x[k+1] = x[k] +œâ[k]d[k] and the new residual r[k+1] =
r[k] ‚àíœâ[k]v[k].
Step 6. Increase k by one and go back to step 2.
Perhaps the most remarkable feature of the CG algorithm is not apparent at Ô¨Årst
glance: the only way the matrix A enters into the calculation (and the only compu-
tationally signiÔ¨Åcant part of the algorithm) is in the formation of the auxiliary vector
v[k] in step 4. This has two important implications.
Firstly, often we do not need even to form the matrix A explicitly in order to
execute the matrix‚Äìvector product. It is enough to have a constructive rule to formu-
late it! Thus, if A originates in the Ô¨Åve-point formula (8.16) then the rule in forming
v = Ad is ‚Äòfor every (k, ‚Ñì) on the grid add the component di value of d corresponding
to the vertical and horizontal neighbours of the point and subtract four times the di
corresponding to the grid point‚Äô. This use of a ‚Äòmultiplication rule‚Äô rather than direct
matrix mutiplication has been already evident in the iterative methods of Chapter 12;
it allows a drastic reduction in cost. Thus, an m √ó m grid results in d = m2 equations
and naive matrix‚Äìvector multiplication would require O

m4
operations, whereas us-
ing the above rule results in O

m2
operations. It is precisely this sort of reasoning
that converts computational methods from ugly ducklings to fully Ô¨Çedged swans.
The second implication is that we can often lift the restrictive condition that A is
symmetric and positive deÔ¨Ånite. Suppose thus that we wish to solve the linear system
Bx = c, where the d √ó d matrix B is nonsingular. We convert it to the form (14.1) by
letting A = B‚ä§B and b = B‚ä§c: note that A is indeed symmetric and positive deÔ¨Ånite.
Of course, in practical applications, and bearing in mind the previous paragraph, we
never actually form the matrix A, a fairly costly procedure. Instead, to calculate v[k]
we Ô¨Årst use the ‚Äòmultiplication rule‚Äô to form u = Bd[k] and next employ the transpose
of that rule to evaluate v[k] = B‚ä§u.
14.3
Krylov subspaces and preconditioners
There is more to Fig. 14.3 than meets the eye. The rapid drop in error, down to
machine accuracy, after just nine iterations is not accidental; it is implicit in our
choice of the matrix A and the vector b. The right terminology in which to express
this behaviour and harness it to accelerate the CG method is the formalism of Krylov
subspaces.
Given a d √ó d matrix A (which need be neither symmetric nor positive deÔ¨Ånite), a
vector v ‚ààRd \ {0} and a natural number m, we call the linear space
Km(A, v) = Sp{Ajv : j = 0, 1, . . . , m ‚àí1}

318
Conjugate gradients
the mth Krylov subspace of Rd. It is trivial to verify that Km(A, v) is indeed a linear
space.
Lemma 14.4
Let ‚Ñìm be the dimension of the Krylov subspace Km(A, v). The se-
quence {‚Ñìm}m=0,1,... increases monotonically. Moreover, there exists a natural number
s with the following property: for every m = 1, 2, . . . , s it is true that ‚Ñìm = m, while
‚Ñìm = s for all m ‚â•s.
Suppose further that v = r
i=1 ciwi, where w1, w2, . . . , wr are eigenvectors of A
corresponding to distinct eigenvalues and c1, c2, . . . , cr Ã∏= 0. Then s = r.
Proof
Since it follows from the deÔ¨Ånition of Krylov subspaces that Km(A, v) ‚äÜ
Km+1(A, v), we deduce that ‚Ñìm ‚â§‚Ñìm+1, m = 0, 1, . . .: we indeed have a monotonically
increasing sequence. Moreover, ‚Ñìm ‚â§d, because Km(A, v) ‚äÜRd, while v Ã∏= 0 implies
that ‚Ñì1 = 1. Finally, since Km(A, v) is spanned by m vectors, necessarily ‚Ñìm ‚â§m. To
sum up,
1 = ‚Ñì1 ‚â§‚Ñì2 ‚â§‚Ñì3 ‚â§¬∑ ¬∑ ¬∑ ‚â§‚Ñìm ‚â§min{m, d},
m ‚â•3.
Let s be the greatest integer such that ‚Ñìs = s and note that s ‚â•1. Since ‚Ñìm ‚â§m,
we deduce that ‚Ñìm ‚â§m ‚àí1 for m ‚â•s + 1, in particular ‚Ñìs+1 ‚â§s. However, by the
deÔ¨Ånition of s, it is true that s = ‚Ñìs ‚â§‚Ñìs+1. Therefore ‚Ñìs+1 = ‚Ñìs and we deduce
that Ks+1(A, v) = Ks(A, v). This means that Asv ‚ààKs(A, v), hence that there exist
Œ±0, Œ±1, . . . , Œ±s‚àí1 such that Asv = s‚àí1
i=0 Œ±iAiv. Multiplying both sides by Aj for any
j = 0, 1, . . . , we have
As+jv =
s‚àí1

i=0
Œ±iAi+jv.
Therefore, if Ajv, Aj+1v, . . . , Aj+s‚àí1v ‚ààKs(A, v) then necessarily also Aj+sv ‚àà
Ks(A, v). Since, as we have just seen, this is true for j = 0, it follows by induc-
tion that Ajv ‚ààKs(A, v) for all j = 0, 1, . . . , hence that Km(A, v) = Ks(A, v) and
‚Ñìm = ‚Ñìs for all m ‚â•k.
To complete the proof, we assume that v can be written as a linear combina-
tion of w1, . . . , wr, eigenvectors of A corresponding to distinct eigenvalues Œª1, . . . , Œªr
respectively,
v =
r

i=1
ciwi,
where the coeÔ¨Écients c1, . . . , cr are all nonzero. Therefore Ajv = r
i=1 ciŒªj
iwi, j =
0, 1, . . . , and we conclude that
Ks(A, v) = Sp {v, Av, . . . , As‚àí1v} ‚äÜSp {w1, w2, . . . , wr}.
Eigenvectors corresponding to distinct eigenvalues are linearly independent and we
thus deduce that s ‚â§r.
Assume next that s < r. Then, by the deÔ¨Ånition of s, it is necessarily true that
‚Ñìr = ‚Ñìs = s and this means that the r vectors Ajv, j = 0, 1, . . . , r ‚àí1, are linearly
dependent: there exist scalars Œ≤0, Œ≤1, . . . , Œ≤r‚àí1, not all zero, such that r‚àí1
j=1 Œ≤jAjv = 0.

14.3
Krylov subspaces and preconditioners
319
Therefore
0 =
r‚àí1

j=0
Œ≤jAjv =
r‚àí1

j=0
Œ≤jAj
r

i=1
ciwi =
r‚àí1

j=0
Œ≤j
r

i=1
ciŒªj
iwi =
r

i=1
ci
‚éõ
‚éù
r‚àí1

j=0
Œ≤jŒªj
i
‚éû
‚é†wi.
Since eigenvectors are linearly independent and c1, c2, . . . , cr Ã∏= 0, we thus deduce that
p(Œªi) = 0,
i = 1, 2, . . . , r,
where
p(z) =
r‚àí1

j=0
Œ≤jzj.
Now, p is a polynomial of degree r ‚àí1 and it is not identically zero. But we have just
proved that it vanishes at the r distinct points Œª1, Œª2, . . . , Œªr. This is a contradiction,
following from our assumption that s < r. Therefore this assumption must be false,
r = s and the proof is complete.
Many methods in linear algebra can be phrased in the terminology of Krylov
subspaces and this often leads to their better understanding ‚Äì and, once we understand
methods, we can often improve them!
Theorem 14.5
Each residual r[m] generated by the method of conjugate gradients
belongs to the Krylov subspace Km+1(A, b), m = 0, 1, . . .
Proof
The Ô¨Årst three residuals are explicitly
r[0] = b ‚ààK1(A, b),
r[1] = r[0] ‚àíœâ[0]Ad[0] = (I ‚àíœâ[0]A)b ‚ààK2(A, b),
r[2] = r[1] ‚àíœâ[1]Ad[1] = r1 ‚àíœâ[1]A(r[1] + Œ≤[0]b)
= [(I ‚àíœâ[1]A)(I ‚àíœâ[0]A) ‚àíœâ[1]Œ≤[0]A]b ‚ààK3(A, b).
Thus, the claim of the theorem is true for m = 0, 1, 2 and we note that the Ô¨Årst
assertion of Theorem 14.3 now implies also that d[m] ‚ààKm+1(A, b) for m = 0, 1, 2.
We continue by induction. Assume that r[j], d[j] ‚ààKj+1(A, b) for j ‚â§m. Since
r[m+1] = r[m] ‚àíœâ[m]Ad[m], it follows from the deÔ¨Ånition of Krylov subspaces that
r[m+1] ‚ààKm+2(A, b). Hence, according to the Ô¨Årst assertion of Theorem 14.3, the
same is true for d[m+1] and our proof is complete.
Corollary
The CG method in exact arithmetic terminates in at most ‚Ñìd steps,
where ‚Ñìd is the dimension of Kd(A, b).
Proof
According to the third assertion of Theorem 14.3 the residuals r[m] are
orthogonal to each other. Therefore the number of nonzero residuals is bounded by
the dimension of Kd(A, b).
The diÔ¨Äerence between the corollary to Theorem 14.3 (convergence in at most d
iterations) and the corollary to Theorem 14.5 (convergence in at most ‚Ñìd iterations) is
the key to improving upon conjugate gradients. If only we can make ‚Ñìd signiÔ¨Åcantly
smaller than d, we can expect the method to perform signiÔ¨Åcantly better.

320
Conjugate gradients
0
2
4
6
8
10
12
14
16
18
20
0
0.5
1.0
1.5
2.0
2.5
3.0
Figure 14.4
The coeÔ¨Écients |uk| for the example from Figs 14.1 and 14.3.
Sometimes ‚Ñìd is small by good fortune.
In the example that we have already
considered in Figs 14.1 and 14.3, the right-hand side b can be expressed as a linear
combination of just ten eigenvectors. Thus let A = WDW ‚àí1, where W is the matrix
of the eigenvectors of A (which is orthogonal, since A is symmetric) and the diagonal
matrix D comprises of its eigenvalues (A.1.5.4). Then
b =
20

k=1
ukwk,
where
u = W ‚àí1b = W ‚ä§b.
Figure 14.4 displays the quantities |uk| and it is evident that u2k+1 = 0 for k =
0, 1, . . . , 9. Therefore, resorting to the notation of Lemma 14.4, we can express v = b as
a linear combination of just ten eigenvectors ck = u2k and the dimension of K20(A, b)
is just ‚Ñì20 = 10. This explains the lower plot in Fig. 14.3.
In general, we can hardly expect the matrix A and the vector b to be in such a
perfect relationship: serendipity can take us only so far! It is a general rule in life and
numerical analysis that, to be lucky, we must make our own luck.
Consider the problems
Bz = g
and
B‚ä§z = g,
(14.11)
where g ‚ààRd is arbitrary while the d √ó d matrix B is nonsingular. Assume further
that either of the systems (14.11) can be solved very easily and cheaply: for example,
B might be tridiagonal or banded. The idea is to incorporate repeated solution of
systems of the form (14.1) into the CG method, to accelerate it. This procedure is
known as preconditioning and the outcome is the method of preconditioned conjugate
gradients (PCG).

14.3
Krylov subspaces and preconditioners
321
0
50
100
150
200
250
300
350
400
450
‚àí12
‚àí10
‚àí8
‚àí6
‚àí4
‚àí2
0
2
Conjugate gradients
0
10
20
‚àí12
‚àí10
‚àí8
‚àí6
‚àí4
‚àí2
0
2
PCG: second preconditioner
0
50
100
150
200
‚àí12
‚àí10
‚àí8
‚àí6
‚àí4
‚àí2
0
2
PCG: first preconditioner
Figure 14.5
The logarithms of the residuals log10 ‚à•r[k]‚à•for ‚Äòplain‚Äô conjugate gra-
dients and for two PCG methods, applied to a 400 √ó 400 TST matrix.
We set h = B‚àí1b, y = B‚ä§x and C = B‚àí1AB‚àí‚ä§. Then
Ax = b
‚áí
AB‚àí‚ä§y = Bh
‚áí
Cy = h.
In place of A and b, we apply the CG method with C and h. Note that we need to
change just two ingredients of the CG algorithm. In step 1 we calculate r[0] = h by
solving the linear system Bh = b, while in step 4 we compute v[k] = Cd[k] in two
stages: Ô¨Årstly we evaluate u ‚ààRd such that B‚ä§u = d[k] and subsequently Ô¨Ånd v[k]
by solving the linear system Bv[k] = Au. All these calculations involve the solution
of linear systems of the form (14.11), which we have assumed is easy. In addition we
need to add a Ô¨Ånal step, to recover x = B‚àí‚ä§y.
3 The TST matrix, again . . .
We consider again a TST matrix A with
2‚Äôs along the main diagonal and ‚àí1‚Äôs in the Ô¨Årst oÔ¨Ä-diagonal, except that we
now let d = 400. The vector b is deÔ¨Åned by bk = 1/
‚àö
k, k = 1, 2, . . . , 400.
The upper plot in Fig. 14.5 depicts the size (on a logarithmic scale) of the
residual for the ‚Äòplain vanilla‚Äô CG method. As predicted by our theory, the
iteration lumbers along for 400 steps, decaying fairly gently, and then in a
single step the error drops down to eleven signiÔ¨Åcant digits ‚Äì as much as
computer arithmetic will allow.
How to precondition our system? Our Ô¨Årst shot is to choose B as the lower-
triangular portion of A, i.e. a matrix with 2‚Äôs along the diagonal and ‚àí2‚Äôs

322
Conjugate gradients
in the subdiagonal. Note that each linear system in (14.11) can be solved in
O(d) operations: it is as cheap to solve each as to multiply a vector by the
matrix B! The behaviour of log10 ‚à•r[k]‚à•is exhibited in the lower left plot in
Fig. 14.5 and it can be seen that we reach the solution, within the limitations
imposed by computer arithmetic, in little more than 150 steps.3
We can do much better, though, with a cleverer choice of preconditioner.
Thus, we choose again B as a bidiagonal matrix but let bk,k = 1, bk+1,k = ‚àí1
and bk,‚Ñì= 0 otherwise. The outcome is displayed in the lower right plot in
Fig. 14.5, and it is astonishing: we attain convergence in just a single step!
The reason has to do with the number of distinct eigenvalues of the matrix
C. Thus, suppose that Œª is an eigenvalue and w a corresponding nonzero
eigenvector of C. Letting u = B‚àí‚ä§v,
B‚àí1AB‚àí‚ä§w = Œªw
‚áí
A(B‚àí‚ä§w) = ŒªBw
‚áí
Au = Œª(BB‚ä§)u
‚áí
(BB‚ä§)‚àí1Au = Œªu.
A simple calculation (a special case of Exercise 14.6) shows, though, that BB‚ä§
coincides with A except at the (1, 1) entry. SpeciÔ¨Åcally, BB‚ä§= A ‚àíe1e‚ä§
1 ,
where e1 ‚ààR400 is the Ô¨Årst coordinate vector. Therefore
F := (BB‚ä§)‚àí1A = I ‚àí(BB‚ä§)‚àí1e1e‚ä§
1 = I ‚àíŒ≥e‚ä§
1 ,
where Œ≥ = (BB‚ä§)‚àí1e1, a rank-1 perturbation of the identity matrix. It is
now a trivial exercise to verify that all the eigenvalues of F, except for one,
are equal to unity. (The remaining eigenvalue is 1 ‚àíŒ≥1.) But the eigenvalues
of F and C coincide, and so we deduce that the matrix C has just two distinct
eigenvalues. Therefore, by Lemma 14.4, the dimension of Km(C, h) is at most
2 and convergence in a single step follows from the corollary to Theorem 14.5. 3
Our example looks, and indeed is, too good to be true. (Anyway, we do not need iter-
ative methods to solve tridiagonal systems, the direct method of Chapter 11 will do!)
In general, even the cleverest preconditioner cannot reduce the number of iterations
down to one or two. Our example is artiÔ¨Åcial, yet it emphasizes the potential beneÔ¨Åts
that follow from a good choice of preconditioner.
How in general should we choose a good preconditioner? The purpose being to
reduce the maximal dimension of Km(B‚àí1AB‚àí‚ä§, B‚àí1b), we note that for every j =
0, 1, . . . it is true that
(B‚àí1AB‚àí‚ä§)jB‚àí1 = B‚àí1(AB‚àí‚ä§B‚àí1)j = B‚àí1(AS‚àí1)‚àí1,
where S = BB‚ä§. Therefore
y ‚ààKm(AS‚àí1, b)
‚áî
B‚àí1y ‚ààKm(B‚àí1AB‚àí‚ä§, B‚àí1b)
3Note that, unlike in the case of plain conjugate gradients, here computer arithmetic has a very
minor eÔ¨Äect on accuracy. The reason is simply that the entire procedure requires less computation,
hence generates less roundoÔ¨Äerror.

14.4
Poisson by conjugate gradients
323
and, since B is nonsingular, we deduce that the dimensions of Km(B‚àí1AB‚àí‚ä§, B‚àí1b)
and Km(AS‚àí1, b) are the same.
A popular technique is to choose a symmetric positive deÔ¨Ånite matrix S such that
‚à•A ‚àíS‚à•is small and S can be Cholesky-factorized easily. Yet, an insistence on small
‚à•A ‚àíS‚à•might be misleading, since the dimension of Km(AS‚àí1, b) does not change
when S is replaced by aS for any a > 0.
An obvious choice of preconditioner, which we have used already in the above
example, is to take B as the lower triangular part of A. Another option is to choose S
as a banded portion of A (provided that it is positive deÔ¨Ånite) and use the approach
of Section 11.1 to factorize it into the form S = BB‚ä§, where B is lower triangular.
This, of course, means that the systems (14.11) can be solved rapidly, as is necessary
for preconditioning.
A more sophisticated approach adopts the graph-theoretical elimination methods
of Section 11.2. Suppose that the graph corresponding to the matrix A is not a tree
but that we can convert it to a tree by setting to zero a small number of entries.
We obtain in this manner a matrix S (of course, we need to check that it is positive
deÔ¨Ånite) which can be subjected to perfect Gaussian elimination while being very close
to the original matrix A.
An alternative to preconditioners based upon direct methods is to mix conjugate
gradients and classical iterative methods. For example, we could use a preconditioner
that consists of a number of Jacobi (or Gauss‚ÄìSeidel, or SOR) iterations, or (if we
really feel sophisticated and brave) a multigrid preconditioner.
14.4
Poisson by conjugate gradients
The CG method was applied to the Poisson problem (8.33) on a 20 √ó 20 grid, hence
with d = 400. The results are reported at the top of Fig. 14.6. We display there
log10 ‚à•r[k]‚à•, the accuracy (in decimal digits) of the residual. Evidently, the residual
decreases at a fairly even pace for about 85 iterations, during which time the iterative
procedure converges within the limitations of Ô¨Ånite computer arithmetic. Recall that
d = 400 implies convergence in at most 400 steps, but the situation in Fig. 14.6 is
typical: convergence occurs more rapidly than in the worst-case scenario.
How should we precondition the matrix A originating in the Ô¨Åve-point formula?
One possibility is to choose S as the tridiagonal portion of A. (Note that since we
are choosing S we need to verify that it is positive deÔ¨Ånite, something which is trivial
in this case. Had we started by choosing any nonsingular B, the positive deÔ¨Åniteness
of S = BB‚ä§would have been assured.)
To obtain B we Cholesky-factorize S, a
procedure which can be accomplished very rapidly (see Section 11.1). Moreover, B
is a bidiagonal lower triangular matrix and both systems (14.11) can be solved with
great ease. The result features in the second graph of Fig. 14.6 and is only marginally
better than plain CG, not really worth the eÔ¨Äort.
An alternative to the tridiagonal preconditioner is to take B as equal to the lower
triangular part of A, a choice that allows for rapid solution of both systems (14.11)
by back substitution. The outcome is displayed in the bottom graph of Fig. 14.6 and
we can see that the number of iterations is cut by more than a factor of 2, while each

324
Conjugate gradients
0
10
20
30
40
50
60
70
80
90
100
‚àí10
‚àí5
0
Plain conjugate gradients
0
10
20
30
40
50
60
70
80
90
100
‚àí10
‚àí5
0
PCG: first preconditioner
0
10
20
30
40
50
60
70
80
90
100
‚àí10
‚àí5
0
PCG: second preconditioner
Figure 14.6
The logarithm of the residual for the CG method and two diÔ¨Äerent
PGC methods, applied to the Poisson equation (8.33) on a 20 √ó 20 grid.
iteration is of similar cost to that of the ‚Äòplain vanilla‚Äô CG method.
All three methods converge within the conÔ¨Ånes of Ô¨Ånite computer arithmetic, but
even here the second preconditioner beats the competition and delivers roughly 14
signiÔ¨Åcant digits. The reason is clear: the less computation we have, the smaller the
accumulation of roundoÔ¨Äerror.
Why is the second preconditioner so much better than the Ô¨Årst? A useful approach
is to examine the eigenvalues of the matrix, whether A (for the plain GC method) or
C. A good preconditioner typically ‚Äòsquashes‚Äô eigenvalues and renders small the ratio
of the largest and the smallest (denoted by Œ∫(A) or Œ∫(C), respectively, and known
as the spectral condition number). (Remember that all eigenvalues are positive, since
the matrix in question is positive deÔ¨Ånite.) Figure 14.7 displays histograms of the
eigenvalues of the relevant matrix, A or C. The eigenvalues of A Ô¨Åt snugly into the
interval [0, 8] and, since the least eigenvalue is fairly small, the spectral conditioning
number is large, Œ∫(A) ‚âà178.06. Matters are somewhat better for the Ô¨Årst precon-
ditioner, yet the presence of small eigenvalues renders the spectral condition number

Comments and bibliography
325
0
1
2
3
4
5
6
7
8
0
5
10
15
Plain conjugate gradients
0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
0
10
20
PCG: first preconditioner
0
0.05
0.10
0.15
0.20
0.25
0
20
40
60
80
PCG: second preconditioner
Figure 14.7
Histograms of the eigenvalues of the underlying matrices for the CG
method and two diÔ¨Äerent PGC methods, applied to the Poisson equation (8.33) on
a 20 √ó 20 grid.
large, Œ∫(C) ‚âà89.53. For the second preconditioner, however, we have Œ∫(C) ‚âà23.11,
a much smaller number. This does not necessarily prove that the second precondi-
tioner is superior; nevertheless, minimizing the spectral condition number is a very
convenient rule of thumb.
The second preconditioner is by no means the best we can do (just changing bk,k
to 5
2 decreases the number of iterations to 30), but then the role of this discussion was
not to describe the state of the art in the application of conjugate gradients to the
Poisson equation but to highlight the importance of good preconditioning.
Comments and bibliography
Much of the narrative of this chapter, in particular Theorem 14.3 and Lemma 14.4, is based
on lecture notes for our Cambridge numerical analysis course, originally compiled by my
colleague and friend Michael J.D. Powell. There are many alternative proofs of conjugacy
and of the essential features of a Krylov subspace but, to my mind, Mike Powell‚Äôs approach
is the most beautiful and it is pleasure to share it, with due acknowledgement, with readers
outside Cambridge.
Many iterative methods lend themselves to the formalism of Krylov subspaces. Thus, the
kth iterate of the standard one-step stationary scheme x[k+1] = Hx[k] + v from Section 12.1,
with starting value x[0] = 0, lives in Kk(H, v).
More interesting in the context of conjugate gradients are two of its generalizations to

326
Conjugate gradients
nonsymmetric matrices, both couched in the language of Krylov subspaces. We have already
mentioned, at the end of Section 14.2, that a nonsymmetric system Ax = b can be sym-
metrized and rendered positive deÔ¨Ånite by multiplying both sides with A‚ä§. However, there
are two enticing alternatives to this approach which do not require symmetrization and the
attendant loss of sparsity. They both generalize an important feature of the CG method,
namely that, assuming x[0] = 0,
‚à•r[m]‚à•A‚àí1 = ‚à•b ‚àíAx[m]‚à•A‚àí1 =
min
x‚ààKm(A,r[0])
‚à•b ‚àíAx‚à•A‚àí1,
m = 0, 1, . . . ,
(14.12)
where ‚à•B‚à•G, where G is symmetric and positive deÔ¨Ånite, is the matrix norm induced by the
inner product ‚ü®a, b‚ü©G = a‚ä§Gb. In other words, the residual produced by CG is the least (in
the ‚à•¬∑ ‚à•A‚àí1 norm) possible residual in the Krylov subspace Km(A, r[0]).
How can we generalize this to nonsymmetric A‚Äôs (or to symmetric A‚Äôs which are not
positive deÔ¨Ånite)? In such cases ‚ü®¬∑ , ¬∑ ‚ü©A‚àí1 is no longer an inner product, because the non-
negativity axiom (A.1.3.1) no longer holds. We have two options for replacing (14.12) by an
alternative condition that retains the gist of this minimization result while being applicable
even when A is no longer symmetric. One option is to choose ‚à•x[m]‚à•such that
‚à•r[m]‚à•2 = ‚à•b ‚àíAx[m]‚à•2 =
min
x‚ààKm(A,r[0])
‚à•b ‚àíAx‚à•2,
m = 0, 1, . . . ,
where ‚à•¬∑ ‚à•2 is the Euclidean matrix norm. This results in the method of minimal residuals
(MR). An alternative to MR is the method of orthogonal residuals (OR), in the spirit of the
Galerkin methods from Chapter 9. Thus, we seek x[m] ‚ààKm(A, b) such that
a‚ä§r[m] = 0,
a ‚ààKm(A, b).
There exist many algorithmic implementations of both the MR and OR approaches and,
needless to say, there has been a great deal of work on their preconditioners. Good references
are Axelsson (1994), Golub & Van Loan (1996) and Greenbaum (1997), but perhaps the
most readable, brief and gentle introduction to the subject is Freud et al. (1992). Here we
outline because of its importance the famed GMRes (generalized minimal residuals) method,
which implements the MR approach. Our point of departure is the Arnoldi iteration, which
is deÔ¨Åned by the following algorithmic steps.
Step 1. Choose x[0] ‚ààRd, compute r[0] and, assuming that it is nonzero (otherwise we
terminate!), let m = 1 and v[0] = r[0]/|r[0]‚à•2.
Step 2. For every k = 0, 1, . . . , m ‚àí1 compute hk,m = v[k]‚ä§Av[m].
Step 3. Set Àúv[m] = Av[m‚àí1] ‚àím‚àí1
k=0 hk,m‚àí1v[k] and let hm,m‚àí1 = ‚à•Àúv[m]‚à•2.
Step 4. If hm,m‚àí1 = 0 (or is of suitably small magnitude) then stop. Otherwise let v[m] =
h‚àí1
m,m‚àí1Àúv[m], subsequently stepping m up by one, and go to step 2.
Now, once we have v[0], . . . , v[m‚àí1], we let
x[m] = x[0] +
m‚àí1

j=0
Œ±jv[j],
where the vector Œ± ‚ààRm minimizes ‚à•œÜ[m] ‚àíH[m]Œ±‚à•2 with
œÜ[m] = ‚à•r[0]‚à•2em+1,
H[m] =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
h0,0
¬∑ ¬∑ ¬∑
h0,m‚àí2
h0,m‚àí1
h1,0
...
h1,m‚àí1
...
...
...
...
0
¬∑ ¬∑ ¬∑ hm‚àí1,m‚àí2
hm‚àí1,m‚àí1
0
¬∑ ¬∑ ¬∑
0
hm,m‚àí1
‚é§
‚é•‚é•‚é•‚é•‚é•‚é¶
.

Exercises
327
Note that the (m + 1) √ó m matrix H[m] is of rank m.
The menagerie of diÔ¨Äerent iterative methods that can be expressed in a Krylov subspace
formalism is very extensive indeed.
We do not propose to dwell further on the alphabet
soup of BCG, BI-CGSTAB, CGNE, CGNR, CGS, GCG, GMRes, MINRES, MR, OR, QMR,
SYMMBK, SYMMLQ and TFQMR (only a partial list) as well as the more humanely named
Arnoldi, Chebyshev and Lanczos methods, or on their diverse preconditioners.
Axelsson, O. (1994), Iterative Solution Methods, Cambridge University Press, Cambridge.
Freund, R.W., Golub, G.H. and Nachtigal, N.M. (1992), Iterative solution of linear systems,
Acta Numerica 1, 57‚Äì100.
Golub, G.H. and Van Loan, C.F. (1996), Matrix Computations (3rd edn), Johns Hopkins
Press, Baltimore.
Greenbaum, A. (1997), Iterative Methods for Solving Linear Systems, SIAM, Philadelphia.
Exercises
14.1
We consider the one-step stationary method Mx[k+1] = Nx[k] + b, where
M ‚àíN = A and the matrix M is nonsingular.
a Prove that x[k] ‚àíx = Hke[0], where H = M ‚àí1N is the iteration matrix,
e[0] = x[0] ‚àíx and x is the exact solution of the linear system.
b Given m ‚â•1, we form a new candidate solution y[m] by the linear combina-
tion
y[m] =
m

k=0
ŒΩkx[k],
where
m

k=0
ŒΩk = 1.
Prove that y[m] ‚àíx = m
k=0 ŒΩkHke[0], and thus deduce that
‚à•y[m] ‚àíx‚à•2 ‚â§‚à•pm(H)‚à•2‚à•e[0]‚à•2,
where
p(z) =
m

k=0
ŒΩkzk.
c Suppose that it is known that all the eigenvalues of H are real and reside in
the interval [Œ±, Œ≤] and that the matrix has a full set of eigenvectors. Prove
that
‚à•y[m] ‚àíx‚à•2 ‚â§‚à•V ‚à•2‚à•V ‚àí1‚à•2‚à•e[0]‚à•2 max
x‚àà[Œ±,Œ≤] |pm(x)|,
where V is the matrix of the eigenvectors of H.
d We now use our freedom of choice of the parameters ŒΩ0, . . . , ŒΩm, hence of
the polynomial pm such that pm(1) = 1 (why this condition?), to minimize
|p(x)| for x ‚àà[Œ±, Œ≤]. To this end prove that the Chebyshev polynomial Tm
(see Exercise 3.2) satisÔ¨Åes the inequality |Tm(x)| ‚â§1, x ‚àà[‚àí1, 1]. (Since

328
Conjugate gradients
Tn(1) = 1, this inequality cannot be improved by any other polynomial q
such that maxx‚àà[‚àí1,1] |q(x)| = 1.) Deduce that the best choice of pm is
pm(x) = Tm(2(x ‚àíŒ±)/(Œ≤ ‚àíŒ±) ‚àí1)
Tm(2(1 ‚àíŒ±)/(Œ≤ ‚àíŒ±) ‚àí1) .
e Show that this algorithm can be formulated in a Krylov subspace formalism.
(This is the famed Chebyshev iterative method. Note, however, that naive
implementation of this iterative procedure is problematic.)
14.2
Apply the plain conjugate gradient method to the linear system
‚é°
‚é£
1
0
0
0
2
0
0
0
3
‚é§
‚é¶x =
‚é°
‚é£
1
1
1
‚é§
‚é¶,
starting as usual with x[0] = 0. Verify that the residuals r[0], r[1] and r[2]
are mutually orthogonal, that the search directions d[0], d[1] and d[2] are
mutually conjugate and that x[3] satisÔ¨Åes the linear system.
14.3
Let the plain conjugate gradient method be applied when A is positive def-
inite.
Express d[k] in terms of r[j] and Œ≤[j], j = 0, 1, . . . , k ‚àí1.
Then,
commencing with the formula x[k+1] = k
j=0 œâ[j]d[j], from œâ[j] > 0 and
with Theorem 14.3, deduce in a few lines that the sequence {‚à•x[j]‚à•: j =
0, 1, . . . , k + 1} increases monotonically.
14.4
The polynomial p(x) = xm + m‚àí1
l=0 clxl is the minimal polynomial of the
d√ód matrix A if it is the polynomial of lowest degree that satisÔ¨Åes p(A) = O.
Note that m ‚â§d holds because of the Cayley‚ÄìHamilton theorem from linear
algebra.
a Give an example of a 3 √ó 3 symmetric matrix with a quadratic minimal
polynomial.
b Prove that (in exact arithmetic) the conjugate gradient method requires at
most m iterations to calculate the exact solution of Av = b, where m is the
degree of the minimal polynomial of A.
14.5
Let A = I +B be a symmetric positive deÔ¨Ånite matrix and suppose that the
rank of B is s. Prove that the CG algorithm converges in at most s steps.
14.6
Let A be a d √ó d TST matrix with ak,k = Œ± and ak,k+1 = ak+1,k = Œ≤.
a Verify that Œ± ‚â•2|Œ≤| > 0 implies that the matrix is positive deÔ¨Ånite.
b Now we precondition the CG method for Ax = b with the Toeplitz lower-
triangular bidiagonal matrix B,
bk,‚Ñì=
‚éß
‚é™
‚é®
‚é™
‚é©
Œ≥,
k = ‚Ñì,
Œ¥,
k = ‚Ñì+ 1,
0,
otherwise.

Exercises
329
Determine real numbers Œ≥ and Œ¥ such that BB‚ä§diÔ¨Äers from A in just the
(1, 1) coordinate.
c Prove that with this choice of Œ≥ and Œ¥ the PCG method converges in a single
step.
14.7
Find the spectral condition number Œ∫(A) when the matrix A corresponds to
the Ô¨Åve-point method in an m √ó m grid.
14.8
Let
A =
 A1
A2
A‚ä§
2
A3

,
S =
 A1
O
O
A2

,
where A1, A3 are symmetric d √ó d matrices and the rank of the d √ó d matrix
A2 is r ‚â§d‚àí1. We further stipulate that the (2d)√ó(2d) matrix A is positive
deÔ¨Ånite.
a Let A1 = L1LT
1 , A3 = L3L‚ä§
3 be Cholesky factorizations and assume that
the preconditioner B is the lower-triangular Cholesky factor of S (hence
BB‚ä§= S). Prove that
C = B‚àí1AB‚àí‚ä§=

I
F
F ‚ä§
I

,
where
F = L‚àí1
1 A2L‚àíT
3
.
b Supposing that the eigenvalues of C are Œª1, . . . , Œª2d, while the eigenvalues of
FF ‚ä§are ¬µ1, . . . , ¬µd ‚â•0, prove that, without loss of generality,
Œªk = 1 ‚àí‚àö¬µk,
Œªd+k = 1 + ‚àö¬µk,
k = 1, 2, . . . , d.
c Prove that the rank of FF ‚ä§is r, thereby deducing that C has at most
2r + 1 distinct eigenvalues. What does this tell you about the number of
steps before the PCG method terminates in exact arithmetic?


15
Fast Poisson solvers
15.1
TST matrices and the Hockney method
This chapter is concerned with yet another approach to the solution of the linear
equations that occur when the Poisson equation is discretized by Ô¨Ånite diÔ¨Äerences.
This approach is an alternative to the direct methods of Chapter 11 and to the iterative
schemes of Chapters 12‚Äì14. We intend to present two techniques for the very fast
approximation of ‚àá2u = f, one in a rectangle and the other in a disc. These techniques
share two features. Firstly, they originate in numerical solution of the Poisson equation
‚Äì hence their sobriquet, fast Poisson solvers. Secondly, the secret of their eÔ¨Écacy rests
in a clever use of the fast Fourier transform (FFT).
In the present section we assume that the Poisson equation (8.13) with Dirichlet
boundary conditions (8.14) is solved in a rectangle with either the Ô¨Åve-point formula
(8.15) or the nine-point formula (8.28) (or, for that matter, the modiÔ¨Åed nine-point
formula (8.32) ‚Äì the matrix of the linear system does not depend on whether the nine-
point method has been modiÔ¨Åed). In either case we assume that the linear equations
have been assembled in natural ordering.
Suppose that the grid is m1 √óm2. The linear system Ax = b can be written in the
block-TST form (recall from Section 12.2 that ‚ÄòTST‚Äô stands for ‚ÄòToeplitz, symmetric
and tridiagonal‚Äô)
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
S
T
O
¬∑ ¬∑ ¬∑
O
T
S
T
...
...
O
...
...
...
O
...
...
T
S
T
O
¬∑ ¬∑ ¬∑
O
T
S
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
‚é°
‚é¢‚é¢‚é¢‚é£
x1
x2
...
xm2
‚é§
‚é•‚é•‚é•‚é¶=
‚é°
‚é¢‚é¢‚é¢‚é£
b1
b2
...
bm2
‚é§
‚é•‚é•‚é•‚é¶,
(15.1)
where x‚Ñìand b‚Ñìcorrespond to the variables and to the portion of b along the ‚Ñìth
column of the grid, respectively:
x‚Ñì=
‚é°
‚é¢‚é¢‚é¢‚é£
u1,‚Ñì
u2,‚Ñì
...
um1,‚Ñì
‚é§
‚é•‚é•‚é•‚é¶,
b‚Ñì=
‚é°
‚é¢‚é¢‚é¢‚é£
b1,‚Ñì
b2,‚Ñì
...
bm1,‚Ñì
‚é§
‚é•‚é•‚é•‚é¶,
‚Ñì= 1, 2, . . . , m2.
331

332
Fast Poisson solvers
Both S and T are themselves m1 √ó m1 TST matrices:
S =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
‚àí4
1
0
¬∑ ¬∑ ¬∑
0
1
‚àí4
1
...
...
0
...
...
...
0
...
...
1
‚àí4
1
0
¬∑ ¬∑ ¬∑
0
1
‚àí4
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
and
T =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
1
0
¬∑ ¬∑ ¬∑
0
0
1
...
...
...
...
...
0
0
¬∑ ¬∑ ¬∑
0
1
‚é§
‚é•‚é•‚é•‚é•‚é•‚é¶
for the Ô¨Åve-point formula and
S =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
‚àí10
3
2
3
0
¬∑ ¬∑ ¬∑
0
2
3
‚àí10
3
2
3
...
...
0
...
...
...
0
...
...
2
3
‚àí10
3
2
3
0
¬∑ ¬∑ ¬∑
0
2
3
‚àí10
3
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
and
T =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
2
3
1
6
0
¬∑ ¬∑ ¬∑
0
1
6
2
3
1
6
...
...
0
...
...
...
0
...
...
1
6
2
3
1
6
0
¬∑ ¬∑ ¬∑
0
1
6
2
3
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
in the case of the nine-point formula.
We rewrite (15.1) in the form
Tx‚Ñì‚àí1 + Sx‚Ñì+ Tx‚Ñì+1 = b‚Ñì,
‚Ñì= 1, 2, . . . , m2,
(15.2)
where x0, xm2+1 := 0 ‚ààRm1, and recall from Lemma 12.5 that the eigenvalues and
eigenvectors of TST matrices are known and that all TST matrices of similar dimension
share the same eigenvectors. In particular, according to (12.29) we have
S = QDSQ,
T = QDT Q,
(15.3)
where
qj,‚Ñì=
>
2
m1 + 1 sin
	
œÄj‚Ñì
m1 + 1

,
j, ‚Ñì= 1, 2, . . . , m1.
(Note that Q is both orthogonal and symmetric; thus, for example, S = QDSQ‚àí1 =
QDSQ‚ä§= QDSQ.
Such a matrix is called an orthogonal involution.)
Both DS
and DT are m1 √ó m1 diagonal matrices whose diagonal components consist of the
eigenvalues of S and T respectively, Œª(S)
1
, Œª(S)
2
, . . . , Œª(S)
m1 and Œª(T )
1
, Œª(T )
2
, . . . , Œª(T )
m1 , say;
cf. (12.28). We substitute (15.3) into (15.2) and multiply with Q = Q‚àí1 from the left.
The outcome is
DT y‚Ñì‚àí1 + DSy‚Ñì+ DT y‚Ñì+1 = c‚Ñì,
‚Ñì= 1, 2, . . . , m2,
(15.4)
where
y‚Ñì:= Qx‚Ñì,
c‚Ñì:= Qb‚Ñì,
‚Ñì= 1, 2, . . . , m2.
The crucial diÔ¨Äerence between (15.2) and (15.4) is that in the latter we have
diagonal, rather than TST, matrices. To exploit this, we recall that x and b (and,

15.1
TST matrices and the Hockney method
333
indeed, the matrix A) have been obtained from a natural ordering of a rectangular
grid by columns. Let us now reorder the y‚Ñìand the c‚Ñìby rows. Thus,
Àúyj :=
‚é°
‚é¢‚é¢‚é¢‚é£
yj,1
yj,2
...
yj,m2
‚é§
‚é•‚é•‚é•‚é¶,
Àúcj :=
‚é°
‚é¢‚é¢‚é¢‚é£
cj,1
cj,2
...
cj,m2
‚é§
‚é•‚é•‚é•‚é¶,
j = 1, 2, . . . , m1.
To derive linear equations that are satisÔ¨Åed by the Àúyj, let us consider the Ô¨Årst equation
in each of the m2 blocks in (15.4),
Œª(S)
1
y1,1 + Œª(T )
1
y1,2 = c1,1,
Œª(T )
1
y1,‚Ñì‚àí1 + Œª(S)
1
y1,‚Ñì+ Œª(T )
1
y1,‚Ñì+1 = c1,‚Ñì,
‚Ñì= 2, 3, . . . , m2 ‚àí1,
Œª(T )
1
y1,m2‚àí1 + Œª(S)
1
y1,m2 = c1,m2,
or, in a matrix notation,
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
Œª(S)
1
Œª(T )
1
0
¬∑ ¬∑ ¬∑
0
Œª(T )
1
Œª(S)
1
Œª(T )
1
...
...
0
...
...
...
0
...
...
Œª(T )
1
Œª(S)
1
Œª(T )
1
0
¬∑ ¬∑ ¬∑
0
Œª(T )
1
Œª(S)
1
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
Àúy1 = Àúc1.
Likewise, collecting together the jth equation from each block in (15.4) results in
Œìj Àúyj = Àúcj,
j = 1, 2, . . . , m1,
(15.5)
where
Œìj :=
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
Œª(S)
j
Œª(T )
j
0
¬∑ ¬∑ ¬∑
0
Œª(T )
j
Œª(S)
j
Œª(T )
j
...
...
0
...
...
...
0
...
...
Œª(T )
j
Œª(S)
j
Œª(T )
j
0
¬∑ ¬∑ ¬∑
0
Œª(T )
j
Œª(S)
j
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
,
j = 1, 2, . . . , m2.
Hence, switching from column-wise to row-wise ordering uncouples the (m1m2) √ó
(m1m2) system (15.4) into m2 systems, each of dimension m1 √ó m1.
The matrices Œìj are also TST; hence their eigenvalues and eigenvectors are known
and, in principle, can be used to solve (15.5). This, however, is a bad idea, since
it is considerably easier to compute these linear systems by banded LU factorization
(see Chapter 11). This costs just O(m1) operations per system, altogether O(m1m2)
operations.

334
Fast Poisson solvers
3 Counting operations
Measuring the cost of numerical calculation is a
highly uncertain business.
At the dawn of the computer era (not such a
long time ago!) it was usual to count multiplications, since they were sig-
niÔ¨Åcantly more expensive than additions or subtractions (divisions were even
more expensive, avoided if at all possible). As often in the computer busi-
ness, technological developments have rendered this point of view obsolete. In
modern processors, operations like multiplication ‚Äì and, for that matter, expo-
nentiation, square-rooting, the evaluation of logarithms and of trigonometric
functions ‚Äì are built into the hardware and can be performed exceedingly fast.
Thus, a more up-to-date measure of computational cost is a Ô¨Çop, an abbrevi-
ation for Ô¨Çoating point operation. A single Ô¨Çop is considered the equivalent of
a FORTRAN statement
A(I) = B(I,J) * C(J) + D(J)
or alternative statements in Pascal, C++ or any other high-level computer
language. Note that a Ô¨Çop involves a product, an addition and a number of
calculations of indices ‚Äì none of these operations is free and the above state-
ment, whose form is familiar even to the novice scientiÔ¨Åc programmer, com-
bines them in a useful manner. The reader might have also encountered Ô¨Çops
as a yardstick of computer speed, in which case Ô¨Çops (kiloÔ¨Çops, megaÔ¨Çops,
gigaÔ¨Çops, teraÔ¨Çops and perhaps, one day, petaÔ¨Çops) are measured per second.
Even Ô¨Çops, though, are increasingly uncertain as measures of computational
cost, because modern computers ‚Äì or, at least, the large computers used for
macho applications of scientiÔ¨Åc computing ‚Äì typically involve parallel archi-
tectures having diÔ¨Äerent conÔ¨Ågurations. This means that the cost of compu-
tation varies between computers and no single number can provide a complete
comparison. Matters are complicated further by the fact that calculation is
not the only time-consuming task of parallel multi-processor computers. The
other is communication and message-passing among the diÔ¨Äerent processors.
To make a long story short ‚Äì and to avoid excessive departures from the
main theme of this book ‚Äì we thereafter provide only the order of magnitude
(indicated by the O( ¬∑ ) notation) of the cost of an algorithm. This can be
easily converted to a ‚Äòmultiplication count‚Äô or to Ô¨Çops, but, for all intents and
purposes, the order of magnitude is illustrative enough. All our counts are in
serial architecture ‚Äì parallel processing is likely to change everything! Having
said this, we hasten to add that, regardless of the underlying architecture,
most good methods remain good and most bad methods remain bad. It is
still better to use sparse LU factorization, say, for banded matrices, multigrid
still outperforms Gauss‚ÄìSeidel and fast Poisson solvers are still . . . fast.
3
Having solved the tridiagonal systems, we end up with the vectors Àúy1, Àúy2, . . . , Àúym1,
which we ‚Äòtranslate‚Äô back to y1, y2, . . . , ym2 by rearranging rows to columns. Note
that this rearrangement is free of any computational cost since, in reality, we are (or,
at least, should) hold the information in the form of a m1 √ó m2 array, corresponding
to the computational grid.
Column-wise or row-wise natural orderings are purely
notational devices! Finally, we reverse the eÔ¨Äects of the multiplication by Q and let
x‚Ñì= Qy‚Ñì, ‚Ñì= 1, 2, . . . , m2 (recall that Q = Q‚àí1).

15.1
TST matrices and the Hockney method
335
Let us review brieÔ¨Çy the stages in this, the Hockney method, estimating their
computational cost.
(1) At the outset, we have the vectors b1, b2, . . . , bm2 ‚ààRm1 and we form the prod-
ucts c‚Ñì= Qb‚Ñì, ‚Ñì= 1, 2, . . . , m2. This costs O

m2
1m2

operations.
(2) We rearrange columns into rows, i.e., c‚Ñì‚ààRm1, ‚Ñì= 1, 2, . . . , m2, into Àúcj ‚àà
Rm2, j = 1, 2, . . . , m1. This is purely a change in notation and is free of any
computational cost.
(3) The tridiagonal systems Œìj Àúyj = Àúcj, j = 1, 2, . . . , m1, are solved by banded LU
factorization, and the cost of this procedure is O(m1m2).
(4) We next rearrange rows into columns, i.e., Àúyj ‚ààRm2, j = 1, 2, . . . , m1 into
y‚Ñì‚ààRm1, ‚Ñì= 1, 2, . . . , m2. Again, this costs nothing.
(5) Finally, we Ô¨Ånd the solution of the discretized Poisson equation by forming the
products x‚Ñì= Qy‚Ñì, ‚Ñì= 1, 2, . . . , m2, at the cost of O

m2
1m2

operations.
Provided both m1 and m2 are large, matrix multiplications dominate the computa-
tional cost. Our Ô¨Årst, trivial observation is that, the expense being O

m2
1m2

, it is
a good policy to choose m1 ‚â§m2 (because of symmetry, we can always rotate the
rectangle, in other words proceed from row to columns and to rows again). However,
a considerably more important observation is that the special form of the matrix Q
can be exploited to make products of the form s = Qp, say, substantially cheaper
than O

m2
1

. Because of (12.29), we have
s‚Ñì= c
m1

j=1
pj sin
	
œÄj‚Ñì
m1 + 1

= c Im
‚é°
‚é£
m1

j=0
pj exp
	 œÄij‚Ñì
m1 + 1

‚é§
‚é¶,
‚Ñì= 1, 2, . . . , m1,
(15.6)
where c = [2/(m1 + 1)]1/2 is a multiplicative constant.
And this is the very point when the Hockney method becomes a powerful compu-
tational tool, rather than a matter of mathematical curiosity: the sum on the right of
(15.6) is a discrete Fourier transform! Therefore, using the FFT (see Section 10.3),
it can be computed in O(m1 log2 m1) operations. The cost of each of steps 1 and 5,
which dominates the Hockney method, drops from O

m2
1m2

to O(m1m2 log2 m1).
The DFT and the FFT were introduced in Chapter 10 in the context of Fourier
expansions and the computation of their coeÔ¨Écients.
There are no overt Fourier
expansions here!
It is the special form of the eigenvectors of TST matrices that
renders them amenable to a technique which we introduced earlier in a very diÔ¨Äerent
context.
An important remark is that in this section we have not used the fact that we are
solving the Poisson equation! The crucial feature of the underlying linear system is
that it is block-TST and each of the blocks is itself a TST matrix. There is nothing
to prevent us from using the same approach for other equations that possess this
structure, regardless of their origin. Moreover, it is an easy matter to extend this
approach to, say, Poisson equations in three variables with Dirichlet conditions along

336
Fast Poisson solvers
the boundary of a parallelepiped: the matrix partitions into blocks of block-TST
matrices and each such block-TST matrix is itself composed of TST matrices.
15.2
Fast Poisson solver in a disc
Let us suppose that the Poisson equation ‚àá2u = g0 is given in the open unit disc
D = {(x, y) ‚ààR2 : x2 + y2 < 1},
together with Dirichlet boundary conditions along the unit circle,
u(cos Œ∏, sin Œ∏) = œÜ(Œ∏),
0 ‚â§Œ∏ ‚â§2œÄ,
(15.7)
where œÜ(0) = œÜ(2œÄ). It is convenient to translate the equation from Cartesian to polar
coordinates. Thus, we let
v(r, Œ∏) = u(r cos Œ∏, r sin Œ∏),
g(r, Œ∏) = g0(r cos Œ∏, r sin Œ∏),
0 < r < 1,
0 ‚â§Œ∏ ‚â§2œÄ.
The form of ‚àá2 in polar coordinates readily gives us
‚àÇ2v
‚àÇr2 + 1
r
‚àÇv
‚àÇr + 1
r2
‚àÇ2v
‚àÇŒ∏2 = g,
0 < r < 1,
0 ‚â§Œ∏ ‚â§2œÄ.
(15.8)
The boundary conditions, however, are more delicate. Switching from Cartesian to
polar means, in essence, that the disc D is replaced by the square
?D = {(r, Œ∏) : 0 < r < 1, 0 ‚â§Œ∏ ‚â§2œÄ}.
Unlike D, which has just one boundary ‚Äòsegment‚Äô ‚Äì its circumference ‚Äì the set ?D
boasts four portions of boundary and we need to allocate appropriate conditions at
all of them.
The segment r = 1, 0 ‚â§Œ∏ ‚â§2œÄ, is the easiest, being the destination of the original
boundary condition (15.7). Hence, we set
v(1, Œ∏) = œÜ(Œ∏),
0 ‚â§Œ∏ ‚â§2œÄ.
(15.9)
Next in order of diÔ¨Éculty are the line segments 0 < r < 1, Œ∏ = 0, and 0 < r < 1,
Œ∏ = 2œÄ. They both correspond to the same segment, namely 0 < x < 1, y = 0, in the
original disc D. The value of u on this segment is, of course, unknown, but, at the
very least, we know that it is the same whether we assign it to Œ∏ = 0 or Œ∏ = 2œÄ. In
other words, we have the periodic boundary condition
v(r, 0) = v(r, 2œÄ),
0 < r < 1.
(15.10)
Finally, we pay attention to the remaining portion of ‚àÇ?D, namely r = 0, 0 ‚â§Œ∏ ‚â§2œÄ.
This whole line corresponds to just a single point in D, namely the origin x = y = 0

15.2
Fast Poisson solver in a disc
337
Figure 15.1
Computational grids (x, y) in the disc D and (r, Œ∏) in the square ?D,
associated by the Cartesian-to-polar transformation.
(see Fig. 15.1). Therefore, v is constant along that line or, to express it in a more
manageable form, we obtain the Neumann boundary condition
‚àÇ
‚àÇŒ∏v(0, Œ∏) = 0,
0 ‚â§Œ∏ ‚â§2œÄ.
(15.11)
We can approximate the solution of (15.8) with boundary conditions (15.9)‚Äì(15.11)
by inscribing a square grid into ?D, approximating ‚àÇv/‚àÇr, ‚àÇ2v/‚àÇr2 and ‚àÇ2v/‚àÇŒ∏2 by
central diÔ¨Äerences, say, at the grid points and taking adequate care of the boundary
conditions. The outcome is certainly preferable by far to imposing a square grid on
the original disc D, a procedure that leads to excessively unpleasant equations at the
near-boundary grid points. Having solved the Poisson equation in ?D, we can map the
outcome to the disc, i.e., to the concentric grid of Fig. 15.1. This, however, is not the
fast solver that is the goal of this section. To calculate (15.8) considerably faster, we
again resort to FFTs.
We have already deÔ¨Åned in Chapter 10 the Fourier transform (10.15) of an ar-
bitrary complex-valued integrable periodic function g in R. Because of the periodic
boundary condition (15.10) we can Fourier-transform v(r, ¬∑ ), and this results in the
sequence
ÀÜvm(r) = 1
2œÄ
 2œÄ
0
v(r, Œ∏)e‚àíimŒ∏ dŒ∏,
m ‚ààZ.
Our goal is to convert the PDE (15.8) into an inÔ¨Ånite set of ODEs that are satisÔ¨Åed
by the Fourier coeÔ¨Écients {vm}‚àû
m=‚àí‚àû. It is easy to deduce from (10.15) that
<
	‚àÇv
‚àÇr

m
= v‚Ä≤
m(r)
and
<
	‚àÇ2v
‚àÇr2

m
= v‚Ä≤‚Ä≤
m(r),
m ‚ààZ,

338
Fast Poisson solvers
but the second derivative with respect to Œ∏ is less trivial ‚Äì fortunately not by much!
Integrating twice by parts and exploiting periodicity readily leads to
<
	‚àÇ2v
‚àÇŒ∏2

m
= 1
2œÄ
 2œÄ
0
‚àÇ2v(r, Œ∏)
‚àÇŒ∏2
e‚àíimŒ∏ dŒ∏
= 1
2œÄ

e‚àíimŒ∏ ‚àÇv(r, Œ∏)
‚àÇŒ∏
2œÄ
0 + im
 2œÄ
0
‚àÇv(r, Œ∏)
‚àÇŒ∏
e‚àíimŒ∏ dŒ∏

= im
2œÄ
 2œÄ
0
‚àÇv(r, Œ∏)
‚àÇŒ∏
e‚àíimŒ∏ dŒ∏
= im
2œÄ

e‚àíimŒ∏v(r, Œ∏)
2œÄ
0 + im
 2œÄ
0
v(r, Œ∏)e‚àíimŒ∏ dŒ∏

= ‚àím2ÀÜvm(r),
m ‚ààZ.
We now multiply (15.8) by e‚àíimŒ∏, integrate from 0 to 2œÄ and divide by 2œÄ. The
outcome is the ODE
ÀÜv‚Ä≤‚Ä≤
m + 1
r ÀÜv‚Ä≤
m ‚àím2
r2 ÀÜvm = ÀÜgm,
(15.12)
which is obeyed by each Fourier coeÔ¨Écient ÀÜvm, m ‚ààZ, for 0 < r < 1. The right-hand
side is the mth Fourier coeÔ¨Écient of the inhomogeneous term g.
What are the boundary conditions associated with the diÔ¨Äerential equation (15.12)?
Firstly, the periodic conditions (15.10), having played their role in validating the use
of the Fourier transform, disappear in tandem with the variable Œ∏.
Secondly, the
right-hand condition (15.9) translates at once into
ÀÜvm(1) = ÀÜœÜm,
m ‚ààZ.
(15.13)
Finally, we must render the natural condition (15.11) in the language of Fourier coef-
Ô¨Åcients. This presents more of a challenge.
We commence by using the inverse Fourier transform formula (10.13), which allows
us to synthesize v from its coeÔ¨Écients:
v(r, Œ∏) =
‚àû

m=‚àí‚àû
ÀÜvm(r)eimŒ∏,
0 ‚â§r ‚â§1,
0 ‚â§Œ∏ ‚â§2œÄ.
Next, we diÔ¨Äerentiate with respect to Œ∏ and set r = 0. Comparison with (15.11) yields
i
‚àû

m=‚àí‚àû
mÀÜvm(0)eimŒ∏ ‚â°0,
0 ‚â§Œ∏ ‚â§2œÄ.
The trigonometric functions eimŒ∏, m ‚ààZ, are linearly independent. Therefore, when-
ever their linear combination vanishes identically, all its components must be zero.1
The outcome is the boundary condition
ÀÜvm(0) = 0,
m ‚ààZ \ {0}.
(15.14)
1The argument is slightly more complicated, since the standard theorem about linear independence
refers to a Ô¨Ånite number of components. For reasons of brevity, we take for granted the generalization
to linear combinations having an inÔ¨Ånite number of components.

15.2
Fast Poisson solver in a disc
339
This leaves us with just a single missing item of information, namely the boundary
value for the zeroth harmonic. The Cartesian-to-polar transformation,
v(r, Œ∏) = u(r cos Œ∏, r sin Œ∏)
implies
‚àÇv(0, Œ∏)
‚àÇr
= cos Œ∏ ‚àÇu(0, 0)
‚àÇx
+ sin Œ∏ ‚àÇu(0, 0)
‚àÇy
.
Therefore
ÀÜv‚Ä≤(0) = 1
2œÄ
 2œÄ
0
‚àÇv(0, Œ∏)
‚àÇr
dŒ∏ = 1
2œÄ
 2œÄ
0
cos Œ∏ dŒ∏‚àÇu(0, 0)
‚àÇx
+ 1
2œÄ
 2œÄ
0
sin Œ∏ dŒ∏‚àÇu(0, 0)
‚àÇy
= 0,
since
 2œÄ
0
cos Œ∏ dŒ∏,
 2œÄ
0
sin Œ∏ dŒ∏ = 0.
We thus obtain the missing boundary condition,
ÀÜv‚Ä≤(0) = 0.
(15.15)
The crucial fact about the ODEs (15.12) (with initial conditions (15.13)‚Äì(15.15))
is that the Fourier transform uncouples the harmonics ‚Äì the equation for each ÀÜvm is a
two-point boundary value problem, whose solution is independent of all other Fourier
coeÔ¨Äcients!
The two-point boundary problem (15.12) can be solved easily by Ô¨Ånite diÔ¨Äerences
(see Exercise 15.4 for an alternative that uses the Ô¨Ånite element method and, if really
daring, design a Chebyshev spectral method for this problem). Thus, we choose a
positive integer d and cover the interval [0, 1] with d subintervals of length ‚àÜr = 1/d.
Adopting the usual notation from Chapters 1‚Äì7, ÀÜvm,k denotes an approximation to
ÀÜvm(k‚àÜr). Employing the simplest central diÔ¨Äerence approximation,
ÀÜv‚Ä≤
m(k‚àÜr) ‚âà
1
2‚àÜr (ÀÜvm,k+1 ‚àíÀÜvm,k‚àí1) ,
ÀÜv‚Ä≤‚Ä≤
m(k‚àÜr) ‚âà
1
(‚àÜr)2 (ÀÜvm,k+1 ‚àí2ÀÜvm,k + ÀÜvm,k‚àí1) ,
the ODE (15.12) leads to the diÔ¨Äerence equation
1
(‚àÜr)2 (ÀÜvm,k+1 ‚àí2ÀÜvm,k + ÀÜvm,k‚àí1)+
2
2k(‚àÜr)2 (ÀÜvm,k+1 ‚àíÀÜvm,k‚àí1)‚àí
m2
k2(‚àÜr)2 ÀÜvm,k = ÀÜgm,k,
where ÀÜgm,k = ÀÜgm(k‚àÜr) and k ranges in {1, 2, . . . , d‚àí1}. Rearranging terms, we arrive
at
	
1 ‚àí1
2k

ÀÜvm,k‚àí1 ‚àí
	
2 + m2
k2

ÀÜvm,k +
	
1 + 1
2k

ÀÜvm,k+1 = (‚àÜr)2ÀÜgm,k,
k = 1, 2, . . . , d ‚àí1.
(15.16)

340
Fast Poisson solvers
We complement (15.16) with boundary values. Thus, (15.13) yields
ÀÜvm,d = ÀÜœÜm,
m ‚ààZ,
and (15.14) results in
ÀÜvm,0 = 0,
m ‚ààZ \ {0}.
Finally, we use forward diÔ¨Äerences to approximate (15.15): for example,
‚àí5
4 ÀÜv0,0 + 3
2 ÀÜv0,1 ‚àí1
4 ÀÜv0,2 = 0.
The outcome is a tridiagonal linear system for every m Ã∏= 0 and an almost tridiag-
onal system for m = 0, with just one ‚Äòrogue‚Äô element outside a three-diagonal band.2
Such systems can be solved with minimal eÔ¨Äort by sparse LU factorization, see Chap-
ter 11.
The practical computation of (15.16) should be conÔ¨Åned, needless to say, to a
Ô¨Ånite subset of m ‚ààZ, for example ‚àím‚àó+ 1 ‚â§m ‚â§m‚àó. We have already observed in
Section 10.2 that as long as g and œÜ are analytic their periodicity implies the spectral
decay of Fourier coeÔ¨Écients. Therefore the error decays roughly like O

e‚àícm‚àó
for
some c > 0. We can use small values of m‚àó, and this translates into small linear
algebraic systems.
Let us turn our attention to the nuts and bolts of a fast Poisson solver in the
disc D. We commence by choosing a positive integer n and let m‚àó= 2n‚àí1. Next we
approximate {ÀÜgm}m‚àó
m=‚àím‚àó+1 and {ÀÜœÜm}m‚àó
m=‚àím‚àó+1 with FFTs, a task that carries a com-
putational price tag of O(m‚àólog2 m‚àó) operations. As we commented in Section 10.3,
the error in such a procedure is very small and, provided that g and œÜ are suÔ¨Éciently
well behaved, it decays at spectral speed as a function of m‚àó.
Having calculated the two transforms and chosen a positive integer d, we proceed
to solve the linear systems (15.16) for the relevant range of m by employing sparse
LU factorization. The total cost of this procedure is O(dm‚àó) operations.
Finally, having evaluated ÀÜvm,k for ‚àím‚àó+ 1 ‚â§m ‚â§m‚àóand k = 1, 2, . . . , d ‚àí1,
we employ d ‚àí1 inverse FFTs to produce values of v on a d √ó (2m‚àó) square grid, or,
alternatively, on a concentric grid in D (see Fig. 15.1). SpeciÔ¨Åcally, we use Fourier
coeÔ¨Écients to reconstruct the function, in line with the formula (10.13):
u (k‚àÜr cos(œÄ‚Ñì/m‚àó), k‚àÜr sin(œÄ‚Ñì/m‚àó)) = v(k‚àÜx, œâ‚Ñì
2m‚àó) =
m‚àó

m=‚àím‚àó+1
ÀÜvm,kœâ‚Ñìm
2m‚àó
for k = 1, 2, . . . , d ‚àí1 and m = 0, 1, . . . , 2m‚àó‚àí1.3 Here œâr = exp(2œÄi/r) is the rth
primitive root of unity (cf. Section 10.2). This is the most computationally intense
part of the algorithm and the cost totals O(dm‚àólog2 m‚àó). It is comparable, though,
with the expense of the Hockney method from Section 15.1 (which, of course, acts in a
diÔ¨Äerent geometry) and very modest indeed in comparison with other computational
alternatives.
2Of course, we have d + 1 unknowns and a matching number of equations when m = 0.
3We do not need to perform an inverse FFT for d = 0 since, obviously, u(0, 0) = v(0, Œ∏) = ÀÜv0,0.

15.2
Fast Poisson solver in a disc
341
Why is the Poisson problem in a disc so important as to deserve a section all its
own? According to the conformal mapping theorem, given any simply connected open
set B ‚äÇC with a suÔ¨Éciently smooth boundary, there exists an analytic univalent (that
is, one-to-one) function œá that maps B onto the complex unit disc and ‚àÇB onto the
unit circle. (There are many such functions but to attain uniqueness it is enough, for
example, to require that an arbitrary z0 ‚ààB is mapped into the origin with a positive
derivative.) Such a function œá is called a conformal mapping of B on the complex unit
disc.
Suppose that the Poisson equation
‚àá2w = f
(15.17)
is given for all (x, y) ‚ààB‚ãÜ, the natural projection of B on R2, where
(x, y) ‚ààB‚ãÜ
if and only if
x + iy ‚ààB.
We accompany (15.17) with Dirichlet boundary conditions w = œà across ‚àÇB‚ãÜ.
Provided that a conformal mapping œá from B onto the complex unit disc is known,
it is possible to translate the problem of numerically solving (15.17) into the unit disc.
Being one-to-one, œá possesses an inverse Œ∑ = œá‚àí1.
We let
u(x, y) = w

Re Œ∑(x + iy), Im Œ∑(x + iy)

,
(x, y) ‚ààcl D.
Therefore
‚àÇ2u
‚àÇx2 =
	‚àÇ2 Re Œ∑
‚àÇx2

 ‚àÇw
‚àÇx +
	‚àÇ2 Im Œ∑
‚àÇx2

 ‚àÇw
‚àÇy +
	‚àÇRe Œ∑
‚àÇx

2 ‚àÇ2w
‚àÇx2 +
	‚àÇIm Œ∑
‚àÇx

2 ‚àÇ2w
‚àÇy2 ,
‚àÇ2u
‚àÇy2 =
	‚àÇ2 Re Œ∑
‚àÇy2

 ‚àÇw
‚àÇx +
	‚àÇ2 Im Œ∑
‚àÇy2

 ‚àÇw
‚àÇy +
	‚àÇRe Œ∑
‚àÇy

2 ‚àÇ2w
‚àÇx2 +
	‚àÇIm Œ∑
‚àÇy

2 ‚àÇ2w
‚àÇy2
and we deduce that
‚àá2u =
	‚àÇRe Œ∑
‚àÇx

2
+
	‚àÇRe Œ∑
‚àÇy

2
‚àÇ2w
‚àÇx2 +
	‚àÇIm Œ∑
‚àÇx

2
+
	‚àÇIm Œ∑
‚àÇy

2
‚àÇ2w
‚àÇy2
+ (‚àá2 Re Œ∑)‚àÇw
‚àÇx + (‚àá2 Im Œ∑)‚àÇw
‚àÇy .
(15.18)
The function Œ∑ is the inverse of a univalent analytic function, hence it is itself
analytic and obeys the Cauchy‚ÄìRiemann equations
‚àÇRe Œ∑
‚àÇx
= ‚àÇIm Œ∑
‚àÇy
,
‚àÇRe Œ∑
‚àÇy
= ‚àí‚àÇIm Œ∑
‚àÇx
,
(x, y) ‚ààcl D.
We conclude that
‚àÇ2 Re Œ∑
‚àÇy2
= ‚àÇ
‚àÇy
	
‚àí‚àÇIm Œ∑
‚àÇx

= ‚àí‚àÇ
‚àÇx
	‚àÇIm Œ∑
‚àÇy

= ‚àí‚àÇ2 Re Œ∑
‚àÇx2
,

342
Fast Poisson solvers
consequently ‚àá2 Re Œ∑ = 0. Likewise ‚àá2 Im Œ∑ = 0 and we deduce the familiar theorem
that the real and imaginary parts of an analytic function are harmonic (i.e., they obey
the Laplace equation).
Another outcome of the Cauchy‚ÄìRiemann equations is that
	‚àÇRe Œ∑
‚àÇx

2
+
	‚àÇRe Œ∑
‚àÇy

2
=
	‚àÇIm Œ∑
‚àÇx

2
+
	‚àÇIm Œ∑
‚àÇy

2
:= Œ∫(x, y),
say. Therefore, substitution in (15.18), in tandem with the Poisson equation (15.17),
yields
‚àá2u = Œ∫(x, y) ‚àá2w

Re Œ∑(x + iy), Im Œ∑(x + iy)

= Œ∫(x, y)f

Re Œ∑(x + iy), Im Œ∑(x + iy)

:= g0(x, y),
(x, y) ‚ààD
and we are back to the Poisson equation in a unit disc! The boundary condition is
u(x, y) = œà(Re Œ∑(x + iy), Im Œ∑(x + iy)), (x, y) ‚àà‚àÇD.
Even if œá is unknown, all is not lost since there are very eÔ¨Äective numerical methods
for its approximation. Their eÔ¨Écacy is based ‚Äì again ‚Äì on a clever use of the FFT
technique. Moreover, approximate maps œá and Œ∑ are typically expressible as DFTs,
and this means that, having solved the equation in a unit disc, we return to B‚ãÜwith
an FFT . . . The outcome is a numerical solution on a curved grid, the image of the
concentric grid under the function Œ∑ (see Fig. 15.2).
Comments and bibliography
Golub‚Äôs survey (1971) and Pickering‚Äôs monograph (1986) are probably the most comprehen-
sive surveys of fast Poisson solvers, although some methods appear also in Henrici‚Äôs survey
(1979) and elsewhere.
The name ‚ÄòPoisson solver‚Äô is frequently a misnomer. While the rationale behind Sec-
tion 15.2 is intimately linked with the Laplace operator, this is not the case with the Hockney
method of Section 15.1 and, indeed, with many other ‚Äòfast Poisson solvers‚Äô. A more appro-
priate name, in line with the comments that conclude Section 15.1, would be ‚Äòfast block-TST
solvers‚Äô. See Exercise 15.2 for an application of a fast block-TST solver to the Helmholtz
equation.
We conclude these remarks with a brief survey of a fast Poisson (or, again, a fast block-
TST) solver that does not employ the FFT: cyclic odd‚Äìeven reduction and factorization.
The starting point for our discussion is the block-TST equations (15.2), where both S and
T are m1 √ó m1 matrices. Neither S nor T need be TST, but we assume that they commute
(an assumption which, of course, is certainly true when they are TST). We assume that
m2 = 2n for some n ‚â•1. For every ‚Ñì= 1, 2, . . . , 2n‚àí1 we multiply the (2‚Ñì‚àí1)th equation by
T, the (2‚Ñì)th equation by S and the (2‚Ñì+ 1)th equation by T. Therefore
T 2x2‚Ñì‚àí2 + TSx2‚Ñì‚àí1 + T 2x2‚Ñì
= Tb2‚Ñì‚àí1,
‚àíSTx2‚Ñì‚àí1 ‚àíS2x2‚Ñì‚àíSTx2‚Ñì+1
= ‚àíSb2‚Ñì,
T 2x2‚Ñì+ TSx2‚Ñì+1 + T 2x2‚Ñì+2 = Tb2‚Ñì+1
and summation, in tandem with ST = TS, results in
T 2x2(‚Ñì‚àí1) + (2T 2 ‚àíS2)x2‚Ñì+ T 2x2(‚Ñì+1) = T(b2‚Ñì‚àí1 + b2‚Ñì+1) ‚àíSb2‚Ñì,
‚Ñì= 1, 2, . . . , 2n‚àí1.
(15.19)

Comments and bibliography
343
Figure 15.2
Conformal mappings and induced grids for three subsets of R2. The
corresponding mapppings are z		
 3
2 ‚àíz2
, z(4 + z2)1/2 and (2 + z3)1/2 		
 7
4 + z3/2.
The linear system (15.19) is also block-TST and it possesses exactly half the number of blocks
of (15.2). Moreover, T 2 and 2T 2 ‚àíS2 commute. Provided that the solution of (15.19) is
known, we can easily recover the missing components by solving the m1 √ó m1 linear systems
Sx2‚Ñì‚àí1 = b2‚Ñì‚àí1 ‚àíT(x2‚Ñì‚àí2 + x2‚Ñì),
‚Ñì= 1, 2, . . . , 2n‚àí1.
Our choice of m2 = 2n already gives the game away ‚Äì we continue by repeating this
procedure again and again, each time reducing the size of the system. Thus, we let
S[0] := S,
S[r+1] := 2(T [r])2 ‚àí(S[r])2,
T [0] := T,
T [r+1] := (T [r])2,
b[0]
‚Ñì
:= b,
b[r+1] := T [r](b[r]
‚Ñì‚àí2r + b[r]
‚Ñì+2r) ‚àíS[r]b[r]
‚Ñì
and recover the missing components by iterating backwards:
S[r‚àí1]xj2r‚àí2r‚àí1 = b[r‚àí1]
j2r‚àí2r‚àí1 ‚àíT [r‚àí1](xj2r + x(j‚àí1)2r),
j = 1, 2, . . . , 2n‚àír‚àí1.

344
Fast Poisson solvers
We hasten to warn that, as presented, this method is ill conditioned since each ‚Äòiteration‚Äô
S[r] ‚ÜíS[r+1], T [r] ‚ÜíT [r+1] not only destroys sparsity but also produces matrices that
are progressively less amenable to numerical manipulation.
It is possible to stabilize the
algorithm, but this is outside the scope of this brief survey.
There exists an intriguing common thread in multigrid methods, the FFT technique and
the cyclic odd‚Äìeven reduction and factorization method. All these organize their ‚Äòmedium‚Äô ‚Äì
whether a grid, a sequence or a system of equations ‚Äì into a hierarchy of nested subsystems.
This procedure is increasingly popular in modern computational mathematics and other
examples are provided by wavelets and by the method of hierarchical bases in the Ô¨Ånite
element method.
Golub, G.H. (1971), Direct methods for solving elliptic diÔ¨Äerence equations, in Symposium
on the Theory of Numerical Analysis (J.L. Morris, editor), Lecture Notes in Mathematics
193, Springer-Verlag, Berlin.
Henrici, P. (1979), Fast Fourier methods in computational complex analysis, SIAM Review
21, 481‚Äì527.
Pickering, M. (1986), An Introduction to Fast Fourier Transform Methods for Partial DiÔ¨Äer-
ential Equations, with Applications, Research Studies Press, Herts.
Exercises
15.1
Show how to modify the Hockney method to evaluate numerically the solu-
tion of the Poisson equation in the three-dimensional cube
{(x1, x2, x3) : 0 ‚â§x1, x2, x3 ‚â§1}
with Dirichlet boundary conditions.
15.2
The Helmholtz equation
‚àá2u + Œªu = g
is given in a rectangle in R2, accompanied by Dirichlet boundary conditions.
Here Œª cannot be an eigenvalue of the operator ‚àí‚àá2 (cf. Section 8.2), because
if it were then in general a solution would not exist. Amend the Hockney
method so that it can provide fast numerical solution of this equation.
15.3‚ãÜ
An alternative to solving (15.12) by the Ô¨Ånite diÔ¨Äerence equations (15.16) is
the Ô¨Ånite element method.
a Find a variational problem whose Euler‚ÄìLagrange equation is (15.12).
b Formulate explicitly the Ritz equations.
c Discuss a choice of Ô¨Ånite element functions that is likely to produce an ac-
curacy similar to the Ô¨Ånite diÔ¨Äerence method (15.16).

Exercises
345
15.4
The Poisson integral formula
v(r, Œ∏) = 1
œÄ
 2œÄ
0

1 ‚àír2
1 ‚àí2r cos(Œ∏ ‚àíœÑ) + r2

g(œÑ) dœÑ
confers an alternative to the natural boundary condition (15.15).
a Find explicitly the value of v(0, Œ∏).
b Deduce the value of ÀÜv0(0). (Hint: Express v(0, Œ∏) as a linear combination of
Fourier coeÔ¨Écients, in line with (10.13).)
15.5
Amend the fast Poisson solver from Section 15.2 to approximate the solution
of ‚àá2u = g0 in the unit disc, but with (15.7) replaced by the Neumann
boundary condition
‚àÇu(cos Œ∏, sin Œ∏)
‚àÇx
cos Œ∏ + ‚àÇu(cos Œ∏, sin Œ∏)
‚àÇy
sin Œ∏ = œÜ(Œ∏),
0 ‚â§Œ∏ ‚â§2œÄ,
where œÜ(0) = œÜ(2œÄ) and
 2œÄ
0
œÜ(Œ∏) dŒ∏ = 0.
15.6
Describe a fast Poisson solver for the Poisson equation ‚àá2u = g0 with Dirich-
let boundary conditions in the annulus
{(x, y) : œÅ < x2 + y2 < 1},
where œÅ ‚àà(0, 1) is given.
15.7‚ãÜ
Generalize the fast Poisson solver from Section 15.2 from the unit disc to
the three-dimensional cylinder
{(x1, x2, x3) : x2
1 + x2
2 < 1, 0 < x3 < 1}.
You may assume Dirichlet boundary conditions.


P A R T III
Partial diÔ¨Äerential equations of evolution


16
The diÔ¨Äusion equation
16.1
A simple numerical method
It is often useful to classify partial diÔ¨Äerential equations into two kinds: steady-state
equations, where all the variables are spatial, and evolutionary equations, which com-
bine diÔ¨Äerentiation with respect to space and to time. We have already seen some
examples of steady-state equations, namely the Poisson equation and the biharmonic
equation. Typically, equations of this type describe physical phenomena whose be-
haviour depends on the minimization of some quantity, e.g. potential energy, and they
are ubiquitous in mechanics and elasticity theory.1 Evolutionary equations, however,
model systems that undergo change as a function of time and they are important inter
alia in the description of wave phenomena, thermodynamics, diÔ¨Äusive processes and
population dynamics.
It is usual in the theory of PDEs to distinguish between elliptic, parabolic and hy-
perbolic equations. We do not wish to pursue here this formalism ‚Äì or even provide the
requisite deÔ¨Ånitions ‚Äì except to remark that elliptic equations are of the steady-state
type whilst both parabolic and hyperbolic PDEs are evolutionary. A brief explanation
of this distinction rests in the diÔ¨Äerent kind of characteristic curves admitted by the
three types of equations.
Evolutionary diÔ¨Äerential equations are, in a sense, reminiscent of ODEs. Indeed,
one can view ODEs as evolutionary equations without space variables. We will see
in what follows that there are many similarities between the numerical treatment of
ODEs and of evolutionary PDEs and that, in fact, one of the most eÔ¨Äective means
to compute the latter is by approximate conversion to an ODE system. However,
this similarity is deceptive. The numerical solution of evolutionary PDEs requires
us to discretize both in time and in space and, in a successful algorithm, these two
procedures are not independent. The concepts underlying the numerical analysis of
PDEs of evolution might sound familiar but they are often surprisingly more intricate
and subtle than the comparable concepts from Chapters 1‚Äì3.
Our Ô¨Årst example of an evolutionary equation is the diÔ¨Äusion equation,
‚àÇu
‚àÇt = ‚àÇ2u
‚àÇx2 ,
0 ‚â§x ‚â§1,
t ‚â•0,
(16.1)
also known as the heat conduction equation. The function u = u(x, t) is accompanied
1This minimization procedure can be often rendered in the language of the theory of variations,
and this provides a bridge to the material of Chapter 9.
349

350
The diÔ¨Äusion equation
by two kinds of ‚Äòside condition‚Äô, namely an initial condition
u(x, 0) = g(x),
0 ‚â§x ‚â§1,
(16.2)
and the boundary conditions
u(0, t) = œï0(t),
u(1, t) = œï1(t),
t ‚â•0
(16.3)
(of course, g(0) = œï0(0) and g(1) = œï1(0)).
As its name implies, (16.1) models
diÔ¨Äusive phenomena, e.g. in thermodynamics, epidemiology, Ô¨Ånancial mathematics
and image processing.2
The equation (16.1) is the simplest form of a diÔ¨Äusion equation and it can be
generalized in several ways:
‚Ä¢ by allowing more spatial variables, giving
‚àÇu
‚àÇt = ‚àá2u,
(16.4)
where u = u(x, y, t), say;
‚Ä¢ by adding to (16.1) a forcing term f, giving
‚àÇu
‚àÇt = ‚àÇ2u
‚àÇx2 + f,
(16.5)
where f = f(x, t);
‚Ä¢ by allowing a variable diÔ¨Äusion coeÔ¨Écient a, giving
‚àÇu
‚àÇt = ‚àÇ
‚àÇx

a(x)‚àÇu
‚àÇx

,
(16.6)
where a = a(x) is a diÔ¨Äerentiable function such that 0 < a(x) < ‚àûfor all
x ‚àà[0, 1];
‚Ä¢ by letting x range in an arbitrary interval of R. The most important special case
is the Cauchy problem, where ‚àí‚àû< x < ‚àûand the boundary conditions (16.3)
are replaced by the requirement that the solution u( ¬∑ , t) is square integrable for
all t, i.e.,
 ‚àû
‚àí‚àû
[u(x, t)]2 dx < ‚àû,
t ‚â•0.
Needless to say, we can combine several such generalizations.
We commence from the most elementary framework but will address ourselves
hereafter to various generalizations. Our intention being to approximate (16.1) by
Ô¨Ånite diÔ¨Äerences, we choose a positive integer d and inscribe into the strip
{(x, t) : x ‚àà[0, 1], t ‚â•0}
2This ability to look beyond the obvious and discover similar structural patterns across diÔ¨Äerent
physical and societal phenomena ‚Äì in this instance, a Ô¨Çow of ‚ÄòstuÔ¨Ä‚Äô across a medium from high-
concentration to low-concentration areas ‚Äì is exactly what makes mathematics into such a powerful
tool in the mission to understand the world.

16.1
A simple numerical method
351
a rectangular grid
{(‚Ñì‚àÜx, n‚àÜt),
‚Ñì= 0, 1, . . . , d + 1, n ‚â•0},
where ‚àÜx = 1/(d + 1). The approximation of u(‚Ñì‚àÜx, n‚àÜt) is denoted by un
‚Ñì. Observe
that in the latter, n is a superscript not a power ‚Äì we employ this notation to establish a
Ô¨Årm and clear distinction between space and time, a central leitmotif in the numerical
analysis of evolutionary equations.
Let us replace the second spatial derivative and the Ô¨Årst temporal derivative re-
spectively by the central diÔ¨Äerence
‚àÇ2u(x, t)
‚àÇx2
‚âà
1
(‚àÜx)2 [u(x ‚àí‚àÜx, t) ‚àí2u(x, t) + u(x + ‚àÜx, t)] + O

(‚àÜx)2
,
‚àÜx ‚Üí0,
and the forward diÔ¨Äerence
‚àÇu(x, t)
‚àÇt
= 1
‚àÜt[u(x, t + ‚àÜt) ‚àíu(x, t)] + O((‚àÜt)) ,
‚àÜt ‚Üí0.
Substitution into (16.1) and multiplication by ‚àÜt results in the Euler method
un+1
‚Ñì
= un
‚Ñì+ ¬µ(un
‚Ñì‚àí1 ‚àí2un
‚Ñì+ un
‚Ñì+1),
‚Ñì= 1, 2, . . . , d,
n = 0, 1, . . . ,
(16.7)
where the ratio
¬µ =
‚àÜt
(‚àÜx)2
is important enough to be given a name all of its own, the Courant number.
To launch the recursive procedure (16.7) we use the initial condition (16.2), setting
u0
‚Ñì= g(‚Ñì‚àÜx),
‚Ñì= 1, 2, . . . , d.
Note that the calculation of (16.7) for ‚Ñì= 1 and ‚Ñì= d requires us to substitute
boundary values from (16.3), namely un
0 = œï0(n‚àÜt) and un
d+1 = œï1(n‚àÜt) respectively.
How accurate is the method (16.7)? In line with our deÔ¨Åniton of the order of a
numerical scheme for ODEs, we observe that
u(x, t + ‚àÜt) ‚àíu(x, t)
‚àÜt
‚àíu(x ‚àí‚àÜx, t) ‚àí2u(x, t) + u(x + ‚àÜx, t)
(‚àÜx)2
= O

(‚àÜx)2, ‚àÜt

(16.8)
for ‚àÜx, ‚àÜt ‚Üí0. Let us assume that ‚àÜx and ‚àÜt approach zero in such a manner
that ¬µ stays constant ‚Äì it will be seen later that this assumption makes perfect sense!
Therefore ‚àÜt = ¬µ(‚àÜx)2 and (16.8) becomes
u(x, t + ‚àÜt) ‚àíu(x, t)
‚àÜt
‚àíu(x ‚àí‚àÜx, t) ‚àí2u(x, t) + u(x + ‚àÜx, t)
(‚àÜx)2
= O

(‚àÜx)2
for ‚àÜx ‚Üí0. We say that the Euler method (16.7) is of order 2.3
3There is some room for confusion here, since for ODE methods an error of O
hp+1
means an
error of order p. The reason for the present deÔ¨Ånition of the order will be made clear in the proof of
Theorem 16.1.

352
The diÔ¨Äusion equation
The concept of order is important in studying how well a Ô¨Ånite diÔ¨Äerence scheme
models a continuous diÔ¨Äerential equation but ‚Äì as was the case with ODEs in Chapter 2
‚Äì our main concern is convergence, not order. We say that (16.7) (or, for that matter,
any other Ô¨Ånite diÔ¨Äerence method) is convergent if, given any t‚àó> 0, it is true that
lim
‚àÜx‚Üí0

lim
‚Ñì‚Üíx/‚àÜx
	
lim
n‚Üít/‚àÜt un
‚Ñì


= u(x, t)
for all
x ‚àà[0, 1],
t ‚àà[0, t‚àó].
As before, ¬µ = ‚àÜt/(‚àÜx)2 is kept constant.
Theorem 16.1
If ¬µ ‚â§1
2 then the method (16.7) is convergent.
Proof
Let t‚àó> 0 be an arbitrary constant and deÔ¨Åne
en
‚Ñì:= un
‚Ñì‚àíu(‚Ñì‚àÜx, n‚àÜt),
‚Ñì= 0, 1, . . . , d + 1,
n = 0, 1, . . . , n‚àÜt,
where n‚àÜt = ‚åät‚àó/‚àÜt‚åã= ‚åät‚àó/(¬µ(‚àÜx)2)‚åãis the right-hand endpoint of the range of n.
The deÔ¨Ånition of convergence can be expressed in the terminology of the variables en
‚Ñì
as
lim
‚àÜx‚Üí0

max
‚Ñì=0,1,...,d+1
	
max
n=0,1,...,n‚àÜt |en
‚Ñì|


= 0.
Letting
Œ∑n :=
max
‚Ñì=0,1,...,d+1 |en
‚Ñì|,
n = 0, 1, . . . , n‚àÜt,
we rewrite this as
lim
‚àÜx‚Üí0
	
max
n=0,1,...,n‚àÜt Œ∑n

= 0.
(16.9)
Since
un+1
‚Ñì
= un
‚Ñì+ ¬µ(un
‚Ñì‚àí1 ‚àí2un
‚Ñì+ un
‚Ñì+1),
Àúun+1
‚Ñì
= Àúun
‚Ñì+ ¬µ(Àúun
‚Ñì‚àí1 ‚àí2Àúun
‚Ñì+ Àúun
‚Ñì+1) + O

(‚àÜx)4
,
‚Ñì= 0, 1, . . . , d + 1,
n = 0, 1, . . . , n‚àÜt ‚àí1,
where Àúun
‚Ñì= u(‚Ñì‚àÜx, n‚àÜt), subtraction results in
en+1
‚Ñì
= en
‚Ñì+ ¬µ(en
‚Ñì‚àí1 ‚àí2en
‚Ñì+ un
‚Ñì+1) + O

(‚àÜx)4
,
‚Ñì= 0, 1, . . . , d + 1,
n = 0, 1, . . . , n‚àÜt ‚àí1.
In the same way as in the proof of Theorem 1.1, we may now deduce that, provided u is
suÔ¨Éciently smooth (as it will be, provided that the initial and boundary conditions are
ssuÔ¨Éciently smooth; but we choose not to elaborate this point further), there exists a
constant c > 0, independent of ‚àÜx, such that, for every ‚Ñì= 0, 1, . . . , d + 1,
|en+1
‚Ñì
‚àíen
‚Ñì‚àí¬µ(en
‚Ñì‚àí1 ‚àí2en
‚Ñì+ en
‚Ñì+1)| ‚â§c(‚àÜx)4,
‚Ñì= 0, 1, . . . , d + 1,
n = 0, 1, . . . , n‚àÜt ‚àí1.

16.1
A simple numerical method
353
Therefore, by the triangle inequality and the deÔ¨Ånition of Œ∑n,
|en+1
‚Ñì
| ‚â§
en
‚Ñì+ ¬µ(en
‚Ñì‚àí1 ‚àí2en
‚Ñì+ en
‚Ñì+1)
 + c(‚àÜx)4
‚â§¬µ|en
‚Ñì‚àí1| + |1 ‚àí2¬µ| |en
‚Ñì| + ¬µ|en
‚Ñì+1| + c(‚àÜx)4
‚â§(2¬µ + |1 ‚àí2¬µ|)Œ∑n + c(‚àÜx)4,
n = 0, 1, . . . , n‚àÜt ‚àí1.
Because ¬µ ‚â§1
2, we may deduce that
Œ∑n+1 =
max
‚Ñì=0,1,...,d+1 |en+1
‚Ñì
| ‚â§Œ∑n + c(‚àÜx)4,
n = 0, 1, . . . , n‚àÜt ‚àí1.
Thus, by induction
Œ∑n+1 ‚â§Œ∑n + c(‚àÜx)4 ‚â§Œ∑n‚àí1 + 2c(‚àÜx)4 ‚â§Œ∑n‚àí2 + 3c(‚àÜx)4 ‚â§¬∑ ¬∑ ¬∑
and we conclude that
Œ∑n ‚â§Œ∑0 + nc(‚àÜx)4,
n = 0, 1, . . . , n‚àÜt.
Since Œ∑0 = 0 (because the initial conditions at the grid points match for the exact and
the discretized equation) and n(‚àÜx)2 = n‚àÜt/¬µ ‚â§t‚àó/¬µ, we deduce that
Œ∑n ‚â§ct‚àó
¬µ (‚àÜx)2,
n = 0, 1, . . . , n‚àÜt.
Therefore lim‚àÜx‚Üí0 Œ∑n = 0 for all n, and comparison with (16.9) completes the proof
of convergence.
Note that the error in Œ∑n in the proof of the theorem behaves like O

(‚àÜx)2
. This
justiÔ¨Åes the statement that the method (16.7) is second order.
3 A numerical example
Let us consider the diÔ¨Äusion equation (16.1) with
the initial and boundary conditions
g(x) = sin 1
2œÄx + 1
2 sin 2œÄx,
0 ‚â§x ‚â§1,
(16.10)
œï0(t) ‚â°0,
œï1(t) = e‚àíœÄ2t/4,
t ‚â•0,
respectively. Its exact solution is, incidentally,
u(x, t) = e‚àíœÄ2t/4 sin 1
2œÄx + 1
2e‚àí4œÄ2t sin 2œÄx,
0 ‚â§x ‚â§1,
t ‚â•0.
Fig. 16.1 displays the error in the solution of this equation by the Euler method
(16.7) with two choices of ¬µ, one at the edge of the interval (0, 1
2] and one
outside.
It is evident that, while for ¬µ =
1
2 the solution looks right, for
the second choice of the Courant number it soon deteriorates into complete
nonsense.
A diÔ¨Äerent aspect of the solution is highlighted in Fig. 16.2, where ¬µ is kept
constant (and within a ‚Äòsafe‚Äô range), while the size of the spatial grid is dou-
bled. The error can be observed to be roughly divided by 4 with each dou-
bling of d, a behaviour entirely consistent with our statement that (16.7) is a
second-order method.
3

354
The diÔ¨Äusion equation
0
0.2
0.4
0.6
0.8
1.0
0
0.5
1.0
‚àí5
0
5
x 10
‚àí3
0
0.2
0.4
0.6
0.8
1.0
0
0.5
1.0
‚àí0.5
0
0.5
¬µ = 0.5
¬µ = 0.509
Figure 16.1
The error in the solution of the diÔ¨Äusion equation (16.10) by the
Euler method (16.7) with d = 20 and two choices of ¬µ, 0.5 and 0.509.
Two important remarks are in order. Firstly, unless a method converges it should
not be used ‚Äì the situation is similar to the numerical analysis of ODEs, with one
important exception, as follows. An ODE method is either convergent or not, whereas
a method for evolutionary PDEs (e.g. for the diÔ¨Äusion equation (16.1)) possesses a
parameter ¬µ and it is entirely possible that it converges only for some values of ¬µ. (We
will see later examples of methods that converge for all ¬µ > 0 and, in Exercise 16.13,
a method which diverges for all ¬µ > 0.)
Secondly, ‚Äòkeeping ¬µ constant‚Äô means in practice that each time we reÔ¨Åne ‚àÜx, we
need to amend ‚àÜt so that the quotient ¬µ = ‚àÜt/(‚àÜx)2 remains constant. This implies
that ‚àÜt is likely to be considerably smaller than ‚àÜx; for example, d = 20 and ¬µ = 1
2
yields ‚àÜx =
1
20 and ‚àÜt =
1
800, leading to a very large computational cost.4
For
example, the lower right-hand surface in Fig. 16.2 was produced with
‚àÜx =
1
160
and
‚àÜt =
1
64 000.
Much of the eÔ¨Äort associated with designing and analysing numerical methods for
the diÔ¨Äusion equation is invested in circumventing such restrictions and attaining
4To be fair, we have not yet proved that ¬µ > 1
2 is bound to lead to a loss of convergence, although
Fig. 16.1 certainly seems to indicate that this is likely. The proof of the necessity of ¬µ ‚â§1
2 is deferred
to Section 16.5.

16.2
Order, stability and convergence
355
0
0.5
1.0
0
0.1
0.2
‚àí5
0
5
x 10
‚àí3
0
0.5
1.0
0
0.1
0.2
‚àí1
0
1
x 10
‚àí3
0
0.5
1.0
0
0.1
0.2
‚àí2
0
2
x 10
‚àí4
0
0.5
1.0
0
0.1
0.2
‚àí5
0
5
x 10
‚àí5
d = 20
d = 40
d = 80
d = 160
Figure 16.2
The numerical error in the solution of the diÔ¨Äusion equation (16.10)
by the Euler method (16.7) with d = 20, 40, 80, 160 and ¬µ = 2
5.
convergence in regimes of ‚àÜx and ‚àÜt that are of a more comparable size.
16.2
Order, stability and convergence
In the present section we wish to discuss the numerical solution by Ô¨Ånite diÔ¨Äerences
of a general linear PDE of evolution,
‚àÇu
‚àÇt = Lu + f,
x ‚ààU,
t ‚â•0,
(16.11)
where U ‚äÇRs, u = u(x, t), f = f(t, x) and L is a linear diÔ¨Äerential operator,
L =

i1+i2+¬∑¬∑¬∑+is‚â§r
ai1,i2,...,is
‚àÇi1+i2+¬∑¬∑¬∑+is
‚àÇxi1
1 ‚àÇxi2
2 ¬∑ ¬∑ ¬∑ ‚àÇxis
s
.
We assume that the equation (16.11) is, as usual, provided with an initial value
u(x, 0) = g(x), x ‚ààU, as well as appropriate boundary conditions.
We express the solution of (16.11) in the form u(x, t) = E(t)g(x), where E is the
evolution operator of L. In other words, E(t) takes an initial value and maps it to the
solution at time t. As an aside, note that E(0) = I, the identity operator, and that

356
The diÔ¨Äusion equation
E(t1 + t2) = E(t1) E(t2) = E(t2) E(t1) for all t1, t2 ‚â•0. An operator with the latter
two properties is called a semigroup.
Let H be a normed space (A.2.1.4) of functions acting in U which possess suÔ¨Écient
smoothness ‚Äì we prefer to leave the latter statement intentionally vague, mentioning
in passing that the requisite smoothness is closely linked to the analysis in Chapter 9.
Denote by
¬∑
the norm of H and recall (A.2.1.8) that every function norm induces
an operator norm. We say that the equation (16.11) is well posed (with regard to the
space H) if, letting both boundary conditions and the forcing term f equal zero, for
every t‚àó> 0 there exists 0 < c(t‚àó) < ‚àûsuch that
E(t)
‚â§c(t‚àó) uniformly for all
0 ‚â§t ‚â§t‚àó.
Intuitively speaking, if an equation is well posed this means that its solution de-
pends continuously upon its initial value and is uniformly bounded in any compact
interval. This is a very important property and we restrict our attention in what
follows to well-posed equations.
The restriction to zero boundary values and f ‚â°0 is not essential for such con-
tinuous dependence upon an initial value. It is not diÔ¨Écult to prove that the latter
remains true (for well-posed equations) provided both the boundary values and the
forcing term are themselves continuous and uniformly bounded.
3 Well-posed and ill-posed equations
We commence with the diÔ¨Äusion
equation (16.1). It is possible to prove by the standard technique of sepa-
ration of variables that, provided the initial condition g possesses a Fourier
expansion,
g(x) =
‚àû

m=1
Œ±m sin œÄmx,
0 ‚â§x ‚â§1,
the solution of (16.1) can be written explicitly in the form
u(x, t) =
‚àû

m=1
Œ±me‚àíœÄ2m2t sin œÄmx,
0 ‚â§x ‚â§1,
t ‚â•0.
(16.12)
Note that u does indeed obey zero boundary conditions.
Suppose that
¬∑
is the familiar Euclidean norm,
f =
# 1
0
[f(x)]2 dx
$1/2
.
Then, according to (16.12) (and allowing ourselves to exchange the order of
summation and integration)
E(t)g 2 =
 1
0
[u(x, t)]2 dx =
 1
0
 ‚àû

m=1
Œ±me‚àíœÄ2m2t sin œÄmx
2
dx
=
‚àû

m=1
‚àû

j=1
Œ±mŒ±je‚àíœÄ2(m2+j2)t
 1
0
sin œÄmx sin œÄjx dx.

16.2
Order, stability and convergence
357
But
 1
0
sin œÄmx sin œÄjx dx =
1 1
2,
m = j,
0,
otherwise;
consequently
E(t)g 2 = 1
2
‚àû

m=1
Œ±2
me‚àí2œÄ2m2t ‚â§1
2
‚àû

m=1
Œ±2
m = g 2.
Therefore E(t) ‚â§1 for every t ‚â•0 and we deduce that (16.1) is well posed.
Another example of a well-posed equation is provided by the advection equa-
tion
‚àÇu
‚àÇt = ‚àÇu
‚àÇx,
which, for simplicity, we deÔ¨Åne for all x ‚ààR; therefore, there is no need to
specify boundary conditions although, of course, we still need to deÔ¨Åne u at
t = 0. We will encounter this equation time and again in Chapter 17.
The exact solution of the advection equation is a unilateral shift, u(x, t) =
g(x + t) (verify this!). Therefore, employing again the Euclidean norm, we
have
Eg 2 =
 ‚àû
‚àí‚àû
[g(x + t)]2 dx =
 ‚àû
‚àí‚àû
[g(x)]2 dx = g 2,
and the equation is well posed.
For an example of an ill-posed problem we resort to the ‚Äòreversed-time‚Äô diÔ¨Äu-
sion equation
‚àÇu
‚àÇt = ‚àí‚àÇ2u
‚àÇx2 .
Its solution, obtained by separation of variables, is almost identical to (16.12),
except that we need to replace the decaying exponential by an increasing one.
Therefore
E(t) sin œÄmx = eœÄ2m2t sin œÄmx ,
m = 1, 2, . . . ,
and it is easy to ascertain that E
is unbounded.
That the ‚Äòreversed-time‚Äô diÔ¨Äusion equation is ill posed is intimately linked to
one of the main principles of thermodynamics, namely that it is impossible to
tell the thermal history of an object from its present temperature distribution. 3
There are, basically, two avenues toward the design of Ô¨Ånite diÔ¨Äerence schemes for
the PDE (16.11). Firstly, we can replace the derivatives with respect to each of the
variables t, x1, x2, . . . , xs, by Ô¨Ånite diÔ¨Äerences.
The outcome is a linear recurrence
relation that allows us to advance from t = n‚àÜt to t = (n+1)‚àÜt; the method (16.7) is
a case in point. Arranging all the components at the time level n‚àÜt in a vector un
‚àÜx,
we can write a general full discretization (FD) of (16.11) in the form
un+1
‚àÜx = A‚àÜxun
‚àÜx + kn
‚àÜx,
n = 0, 1, . . . ,
(16.13)

358
The diÔ¨Äusion equation
where the vector kn
‚àÜx contains the contributions of the forcing term f and the inÔ¨Çuence
of the boundary values. The elements of the matrix A‚àÜx and of the vector kn
‚àÜx may
depend upon ‚àÜx and the Courant number ¬µ = ‚àÜt/(‚àÜx)r (recall that r is the largest
order of diÔ¨Äerentiation in L).
3 The Euler method as FD
The Euler method (16.7) can be written in
the form (16.13) with
A‚àÜx =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
1 ‚àí2¬µ
¬µ
0
¬∑ ¬∑ ¬∑
0
¬µ
1 ‚àí2¬µ
¬µ
...
...
0
...
...
...
0
...
...
¬µ
1 ‚àí2¬µ
¬µ
0
¬∑ ¬∑ ¬∑
0
¬µ
1 ‚àí2¬µ
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
,
(16.14)
and kn
‚àÜx ‚â°0. It might appear that neither A‚àÜx nor kn
‚àÜx depend upon ‚àÜx
and that we have followed the bad habit of excessive mathematical nitpicking.
Not so! ‚àÜx expresses itself via the dimension of the space, since (d+1)‚àÜx = 1. 3
Let us denote the exact solution of (16.11) at the time level n‚àÜt, arranged into a
similar vector, by Àúun
‚àÜx. We say that the FD method (16.13) is of order p if, for every
initial condition,
Àúun+1
‚àÜx ‚àíA‚àÜxÀúun
‚àÜx ‚àíkn
‚àÜx = O

(‚àÜx)p+r
,
‚àÜx ‚Üí0,
(16.15)
for all n ‚â•0 and if there exists at least one initial condition g for which the O((‚àÜx)p+r)
term on the right-hand side does not vanish. The reason why the exponent p+r, rather
than p, features in the deÔ¨Ånition, is nontrivial and it will be justiÔ¨Åed in the proof of
Lemma 16.2.
We equip the underlying linear space with the Euclidean norm
‚à•g‚àÜx‚à•‚àÜx =

‚àÜx

|gj|21/2
,
where the sum ranges over all the grid points.
3 Why the factor ‚àÜx?
Before we progress further, this is the place to
comment on the presence of the mysterious factor (‚àÜx)1/2 in our deÔ¨Ånition
of the vector norm. Recall that in the present context vectors approximate
functions and suppose that g‚àÜx,‚Ñì= g(‚Ñì‚àÜx), ‚Ñì= 1, 2, . . . , d, where (d+1)‚àÜx =
1. Provided that g is square integrable and letting ‚àÜx tend to zero in the
Riemann sum, it follows from elementary calculus that
lim
‚àÜx‚Üí0 ‚à•g‚àÜx‚à•‚àÜx =
# 1
0
[g(x)]2 dx
$1/2
= g .
Thus, scaling by (‚àÜx)1/2 provides for continuous passage from a vector to a
function norm.
3

16.2
Order, stability and convergence
359
A method is convergent if for every initial condition and all t‚àó> 0 it is true that
lim
‚àÜx‚Üí0
	
max
n=0,1,...,‚åät‚àó/‚àÜt‚åã‚à•un
‚àÜx ‚àíÀúun
‚àÜx‚à•‚àÜx

= 0.
We stipulate that the Courant number is constant as ‚àÜx, ‚àÜt ‚Üí0, hence ‚àÜt = ¬µ(‚àÜx)r.
Lemma 16.2
Let ‚à•A‚àÜx‚à•‚àÜx ‚â§1 and suppose that the order condition (16.15) holds.
Then, for every t‚àó> 0, there exists c = c(t‚àó) > 0 such that
‚à•un
‚àÜx ‚àíÀúun
‚àÜx‚à•‚àÜx ‚â§c(‚àÜx)p,
n = 0, 1, . . . , ‚åät‚àó/‚àÜt‚åã,
for all suÔ¨Éciently small ‚àÜx > 0.
Proof
We subtract (16.15) from (16.13), therefore
en+1
‚àÜx = A‚àÜxen
‚àÜx + O

(‚àÜx)p+r
,
‚àÜx ‚Üí0,
where en
‚àÜx := un
‚àÜx‚àíÀúun
‚àÜx, n ‚â•0. The errors en
‚àÜx obey zero initial conditions as well as
zero boundary conditions. Hence, and provided that ‚àÜx > 0 is small and the solution
of the diÔ¨Äerential equation is suÔ¨Éciently smooth (the latter condition depends solely
on the requisite smoothness of initial and boundary conditions), there exists c = c(t‚àó)
such that
‚à•en+1
‚àÜx ‚àíA‚àÜxen
‚àÜx‚à•‚àÜx ‚â§c(‚àÜx)p+r,
n = 0, 1, . . . , n‚àÜt ‚àí1,
where, as in the proof of Theorem 16.1, n‚àÜt := ‚åät‚àó/‚àÜt‚åã. We deduce that
‚à•en+1
‚àÜx ‚à•‚àÜx ‚â§‚à•A‚àÜx‚à•‚àÜx √ó ‚à•en
‚àÜx‚à•‚àÜx + c(‚àÜx)p+r,
n = 0, 1, . . . , n‚àÜt ‚àí1;
induction, in tandem with e0
‚àÜx = 0‚àÜx, then readily yields
‚à•en
‚àÜx‚à•‚àÜx ‚â§c

1 + ‚à•A‚àÜx‚à•‚àÜx + ¬∑ ¬∑ ¬∑ + ‚à•A‚àÜx‚à•n‚àí1
‚àÜx

(‚àÜx)p+r,
n = 0, 1, . . . , n‚àÜt.
Since ‚à•A‚àÜx‚à•‚àÜx ‚â§1, this leads to
‚à•en
‚àÜx‚à•‚àÜx ‚â§cn(‚àÜx)p+r ‚â§c t‚àó
‚àÜt(‚àÜx)p+r = ct‚àó
¬µ (‚àÜx)p,
n = 0, 1, . . . , n‚àÜt,
and the proof of the lemma is complete.
We mention in passing that, with little extra eÔ¨Äort, the condition ‚à•A‚àÜx‚à•‚àÜx ‚â§1 in
the above lemma can be replaced by the weaker condition that the underlying method
is stable ‚Äì although, of course, we have not yet said what is meant by stability! This
is a good moment to introduce this important concept.
Let us suppose that f ‚â°0 and that the boundary conditions are continuous and
uniformly bounded. Reiterating that ‚àÜt = ¬µ(‚àÜx)r, we say that (16.13) is stable if for
every t‚àóthere exists a constant c(t‚àó) > 0 such that
‚à•un
‚àÜx‚à•‚àÜx ‚â§c(t‚àó),
n = 0, 1, . . . , ‚åät‚àó/‚àÜt‚åã,
‚àÜx ‚Üí0.
(16.16)

360
The diÔ¨Äusion equation
Suppose further that the boundary values vanish. In that case kn
‚àÜx ‚â°0‚àÜx, the solution
of (16.13) is un
‚àÜx = An
‚àÜxu0
‚àÜx, n = 0, 1, . . . , and (16.16) becomes equivalent to
lim
‚àÜx‚Üí0
	
max
n=0,1,...,‚åät‚àó/‚àÜt‚åã‚à•An
‚àÜx‚à•‚àÜx

‚â§c(t‚àó).
(16.17)
Needless to say, (16.16) and (16.17) each require that the Courant number be kept
constant.
An important feature of both order and stability is that they are not attributes of
any single numerical scheme (16.13) but of the totality of such schemes as ‚àÜx ‚Üí0.
This distinction, to which we will return in Chapter 17, is crucial to the understanding
of stability.
Before we make use of this concept, it is only fair to warn the reader that there
is no connection between the present concept of stability and the notion of A-stability
from Chapter 4. Mathematics is replete with diverse concepts bearing the identical
sobriquet ‚Äòstability‚Äô and a careful mathematician should always verify whether a casual
reference to ‚Äòstability‚Äô has to do with stable ultraÔ¨Ålters in logic, with stable Ô¨Çuid Ô¨Çow,
stable dynamical systems or, perhaps, with (16.17).
The purpose of our deÔ¨Ånition of stability is the following theorem. Without much
exaggeration, it can be singled out as the lynchpin of the modern numerical theory of
evolutionary PDEs.
Theorem 16.3 (The Lax equivalence theorem)
Provided that the linear evolu-
tionary PDE (16.11) is well posed, the fully discretized numerical method (16.13) is
convergent if and only if it is stable and of order p ‚â•1.
The last theorem plays a similar role to the Dahlquist equivalence theorem (The-
orem 2.2) in the theory of multistep numerical methods for ODEs. Thus, on the one
hand the concept of convergence might be the central goal of our analysis but it is, in
general, diÔ¨Écult to verify from Ô¨Årst principles ‚Äì Theorem 16.1 is almost the exception
that proves the rule! On the other hand, it is easy to derive the order and, as we will
see in Sections 16.4 and 16.5 and Chapter 17, a number of powerful techniques are
available to determine whether a given method (16.13) is stable. Exactly as in Theo-
rem 2.2, we replace an awkward analytic requirement by more manageable algebraic
conditions.
Discretizing all the derivatives in line with the recursion (16.13) is not the only
possible ‚Äì or, indeed, useful ‚Äì approach to the numerical solution of evolutionary
PDEs.
An alternative technique follows by subjecting only the spatial derivatives
to Ô¨Ånite diÔ¨Äerence discretization. This procedure, which we term semi-discretization
(SD), converts a PDE into a system of coupled ODEs. Using a similar notation to
that for FD schemes, in particular ‚Äòstretching‚Äô grid points into a long vector, we write
an archetypical SD method in the form
v‚Ä≤
‚àÜx = P‚àÜxv‚àÜx + h‚àÜx(t),
t ‚â•0,
(16.18)
where h‚àÜx consists of the contributions of the forcing term and the boundary values.
Note that the use of a prime to denote a derivative is unambiguous: since we have
replaced all spatial derivatives by diÔ¨Äerences, only the temporal derivative is left.

16.2
Order, stability and convergence
361
Needless to say, having derived (16.18) we next solve the ODEs, putting to use
the theory of Chapters 1‚Äì7. Although the outcome is a full discretization ‚Äì at the
end of the day, both spatial and temporal variables are discretized ‚Äì it is, in general,
simpler to derive an SD scheme Ô¨Årst and then apply to it the considerable apparatus of
numerical ODE methods. Moreover, instead of using Ô¨Ånite diÔ¨Äerences to discretize in
space, there is nothing to prevent us from employing Ô¨Ånite elements (via the Galerkin
approach), spectral methods or other means, e.g. boundary element methods. Only
limitations of space (no pun intended) prevent us from debating these issues further.
The method (16.18) is occasionally termed the method of lines, mainly in the
more traditional numerical literature, to reÔ¨Çect the fact that each component of v‚àÜx
describes a variable along the line t ‚â•0.
To prevent confusion and for assorted
√¶sthetic reasons we will not use this name.
The concepts of order, convergence and stability can be generalized easily to the
SD framework. Denoting by Àúv‚àÜx(t) the vector of exact solutions of (16.11) at the
(spatial) grid points, we say that the method (16.18) is of order p if for all initial
conditions it is true that
Àúv‚Ä≤
‚àÜx(t) ‚àíP‚àÜxÀúv‚àÜx(t) ‚àíh‚àÜx(t) = O((‚àÜx)p) ,
‚àÜx ‚Üí0,
t ‚â•0,
(16.19)
and if the error is precisely O((‚àÜx)p) for some initial condition. It is convergent if for
every initial condition and all t‚àó> 0 it is true that
lim
‚àÜx‚Üí0
	
max
t‚àà[0,t‚àó] ‚à•v‚àÜx(t) ‚àíÀúv‚àÜx(t)‚à•‚àÜx

= 0.
The semi-discretized method is stable if, whenever f ‚â°0 and the boundary values are
uniformly bounded, for every t‚àó> 0 there exists a constant c(t‚àó) > 0 such that
‚à•v‚àÜx(t)‚à•‚àÜx ‚â§c(t‚àó),
t ‚àà[0, t‚àó].
(16.20)
Now suppose that the boundary values vanish, in which case h‚àÜx ‚â°0‚àÜx and the
solution of (16.18) is
v‚àÜx(t) = etP‚àÜxv‚àÜx(0),
t ‚â•0.
We recall that the exponential of an arbitrary square matrix B is deÔ¨Åned by means of
the Taylor series
eB =
‚àû

k=0
1
k!Bk,
which always converges (see Exercise 16.4). Therefore, (16.20) becomes equivalent to
lim
‚àÜx‚Üí0
	
max
t‚àà[0,t‚àó] ‚à•etP‚àÜx‚à•‚àÜx

‚â§c(t‚àó).
(16.21)
Theorem 16.4 (The Lax equivalence theorem for SD schemes)
Provided
that the linear evolutionary PDE (16.11) is well posed, the semi-discretized numerical
method (16.18) is convergent if and only if it is stable and of order p ‚â•1.
Approximating a PDE by an ODE is, needless to say, only half the computational
job and the eÔ¨Äect of the best semi-discretization can be undone by an inappropriate
choice of ODE solver for the equations (16.18). We will return to this issue later.

362
The diÔ¨Äusion equation
The two equivalence theorems establish a Ô¨Årm bedrock and a starting point for a
proper theory of discretized PDEs of evolution. It is easy to discretize a PDE and to
produce numbers, but only methods that adhere to the conditions of these theorems
allow us to regard such numbers with a modicum of trust.
16.3
Numerical schemes for the diÔ¨Äusion equation
We have already seen one method for the equation (16.1), namely the Euler scheme
(16.7). In the present section we follow a more systematic route toward the design
of numerical methods ‚Äì semi-discretized and fully discretized alike ‚Äì for the diÔ¨Äusion
equation (16.1) and some of its generalizations.
Let
v‚Ä≤
‚Ñì=
1
(‚àÜx)2
Œ≤

k=‚àíŒ±
akv‚Ñì+k,
‚Ñì= 1, 2, . . . , d,
(16.22)
be a general SD scheme for (16.1).
Note, incidentally, that, unless Œ±, Œ≤ ‚â§1, we
need somehow to provide additional information in order to implement (16.22). For
example, if Œ± = 2 we could require a value for v‚àí1 (which is not provided by the
boundary conditions (16.3)).
Alternatively, we need to replace the Ô¨Årst equation
in (16.22) by a diÔ¨Äerent scheme. This procedure is akin to boundary eÔ¨Äects for the
Poisson equation (see Chapter 8) and, more remotely, to the requirement for additional
starting values to launch a multistep method (see Chapter 2).
Now set
a(z) :=
Œ≤

k=‚àíŒ±
akzk,
z ‚ààC.
Theorem 16.5
The SD method (16.22) is of order p if and only if there exists a
constant c Ã∏= 0 such that
a(z) = (ln z)2 + c(z ‚àí1)p+2 + O

|z ‚àí1|p+3
,
z ‚Üí1.
(16.23)
Proof
We employ the terminology of Ô¨Ånite diÔ¨Äerence operators from Section 8.1,
except that we add to each operator a subscript that denotes the variable. For example,
Dt = d/ dt, whereas Ex stands for the shift operator along the x-axis. Letting
Àúv‚Ñì= u(‚Ñì‚àÜx, ¬∑ ),
‚Ñì= 0, 1, . . . , d + 1,
we can thus write the error in the form
Àúe‚Ñì:= Àúv‚Ä≤
‚Ñì‚àí
1
(‚àÜx)2
Œ≤

k=‚àíŒ±
akÀúv‚Ñì+k =

Dt ‚àí
1
(‚àÜx)2 a(Ex)

Àúv‚Ñì,
‚Ñì= 1, 2, . . . , d.
Recall that the function Àúv‚Ñìis the solution of the diÔ¨Äusion equation (16.1) at x = ‚Ñì‚àÜx.
In other words,
DtÀúv‚Ñì= ‚àÇu(‚Ñì‚àÜx, t)
‚àÇt
= ‚àÇ2u(‚Ñì‚àÜx, t)
‚àÇx2
= D2
xÀúv‚Ñì,
‚Ñì= 1, 2, . . . , d.

16.3
Numerical schemes for the diÔ¨Äusion equation
363
Consequently,
Àúe‚Ñì=

D2
x ‚àí
1
(‚àÜx)2 a(Ex)

Àúv‚Ñì,
‚Ñì= 1, 2, . . . , d.
According to (8.2), however, it is true that
Dx =
1
‚àÜx ln Ex,
allowing us to deduce that
Àúe =
1
(‚àÜx)2

(ln Ex)2 ‚àía(Ex)
 Àúv,
(16.24)
where Àúe = [ Àúe1
Àúe2
¬∑ ¬∑ ¬∑
Àúed ]‚ä§.
Since, formally, Ex = I + O(‚àÜx), it follows that (16.23) is equivalent to
[(ln Ex)2 ‚àía(Ex)]Àúv = c(‚àÜx)p+2Dp+2
x
Àúv + O

(‚àÜx)p+3
,
‚àÜx ‚Üí0,
provided that the solution u of (16.1) is suÔ¨Éciently smooth. In particular, substitution
into (16.24) gives
Àúe = c(‚àÜx)pDp+2
x
Àúv + O

(‚àÜx)p+1
,
‚àÜx ‚Üí0.
It now follows from (16.19) that the SD scheme (16.22) is indeed of order p.
3 Examples of SD methods
Our Ô¨Årst example is
v‚Ä≤
‚Ñì=
1
(‚àÜx)2 (v‚Ñì‚àí1 ‚àí2v‚Ñì+ v‚Ñì+1),
‚Ñì= 1, 2, . . . , d.
(16.25)
In this case a(z) = z‚àí1 ‚àí2 + z and, to derive the order, we let z = eiŒ∏; hence
a(eiŒ∏) = e‚àíiŒ∏ ‚àí2 + eiŒ∏ = ‚àí4 sin2 1
2Œ∏ = ‚àíŒ∏2 + 1
12Œ∏4 + ¬∑ ¬∑ ¬∑ ,
Œ∏ ‚Üí0,
while (ln eiŒ∏)2 = (iŒ∏)2 = ‚àíŒ∏2. Therefore (16.25) is of order 2.
Bearing in mind that a(Ex) is nothing other than a Ô¨Ånite diÔ¨Äerence approxi-
mation of (‚àÜxDx)2, the form of (16.25) is not very surprising, once we write
it in the language of Ô¨Ånite diÔ¨Äerence operators:
v‚Ä≤
‚Ñì=
1
(‚àÜx)2 ‚àÜ2
0,xv‚Ñì,
‚Ñì= 1, 2, . . . , d.
(16.26)
Likewise, we can use (8.8) as a starting point for the SD scheme
v‚Ä≤
‚Ñì=
1
(‚àÜx)2

‚àÜ2
0,x ‚àí1
12‚àÜ4
0,x

v‚Ñì
(16.27)
= ‚àí1
12v‚Ñì‚àí2 + 4
3v‚Ñì‚àí1 ‚àí5
2v‚Ñì+ 4
3v‚Ñì+1 ‚àí1
12v‚Ñì+2,
‚Ñì= 1, 2, . . . , d,
where, needless to say, at ‚Ñì= 1 and ‚Ñì= d a special ‚ÄòÔ¨Åx‚Äô might be required to
cover for the missing values. It is left to the reader in Exercise 16.5 to verify
that (16.27) is of order 4.

364
The diÔ¨Äusion equation
Both (16.25) and (16.27) were constructed using central diÔ¨Äerences and their
coeÔ¨Écients display an obvious spatial symmetry. We will see in Section 16.4
that this state of aÔ¨Äairs confers an important advantage. Other things being
equal, we prefer such schemes and this is the rule for equation (16.1).
In
Chapter 17, though, while investigating diÔ¨Äerent equations we will encounter
a situation where ‚Äòother things‚Äô are not equal.
3
The method (16.22) can be easily amended to cater for (16.4), the diÔ¨Äusion equation in
several space variables, and it can withstand the addition of a forcing term. Examples
are the counterpart of (16.25) in a square,
v‚Ä≤
k,‚Ñì=
1
(‚àÜx)2 (vk‚àí1,‚Ñì+ vk,‚Ñì‚àí1 + vk+1,‚Ñì+ vk,‚Ñì+1 ‚àí4vk,‚Ñì),
k, ‚Ñì= 1, 2, . . . , d (16.28)
(unsurprisingly, the terms on the right-hand side are the Ô¨Åve-point discretization of
the Laplacian ‚àá2), and an SD scheme for (16.5), the diÔ¨Äusion equation with a forcing
term,
v‚Ä≤
‚Ñì=
1
(‚àÜx)2 (v‚Ñì‚àí1 ‚àí2v‚Ñì+ v‚Ñì+1) + f‚Ñì(t),
‚Ñì= 1, 2, . . . , d.
(16.29)
Both (16.28) and (16.29) are second-order discretizations.
Extending (16.22) to the case of a variable diÔ¨Äusion coeÔ¨Écient, e.g. to equation
(16.6), is equally easy if done correctly. We extend (16.25) by replacing (16.26) with
v‚Ä≤
‚Ñì=
1
(‚àÜx)2 ‚àÜ0,x (a‚Ñì‚àÜ0,xv‚Ñì) ,
‚Ñì= 1, 2, . . . , d,
where aŒ≥ = a(Œ∫‚àÜx), Œ∫ ‚àà[0, d + 1]. The outcome is
v‚Ä≤
‚Ñì=
1
(‚àÜx)2 ‚àÜ0,x[a‚Ñì(v‚Ñì+1/2 ‚àív‚Ñì‚àí1/2)]
=
1
(‚àÜx)2 [a‚Ñì‚àí1/2v‚Ñì‚àí1 ‚àí(a‚Ñì‚àí1/2 + a‚Ñì+1/2)v‚Ñì+ a‚Ñì+1/2v‚Ñì+1],
‚Ñì= 1, 2, . . . , d,
(16.30)
and it involves solely the values of v on the grid. Again, it is easy to prove that,
subject to the requisite smoothness of a, the method is second order.
The derivation of FD schemes can proceed along two distinct avenues, which we
explore in the case of the ‚Äòplain-vanilla‚Äô diÔ¨Äusion equation (16.1). Firstly, we may
combine the SD scheme (16.22) with an ODE solver. Three ODE methods are of
suÔ¨Écient interest in this context to merit special mention.
‚Ä¢ The Euler method (1.4), that is
yn+1 = yn + ‚àÜtf(n‚àÜt, yn),
yields the similarly named Euler scheme
un+1
‚Ñì
= un
‚Ñì+ ¬µ
Œ≤

k=‚àíŒ±
akun
‚Ñì+k,
‚Ñì= 1, 2, . . . , d,
n ‚â•0.
(16.31)

16.3
Numerical schemes for the diÔ¨Äusion equation
365
‚Ä¢ An application of the trapezoidal rule (1.9),
yn+1 = yn + 1
2‚àÜt[f(n‚àÜt, yn) + f((n + 1)‚àÜt, yn+1)]
results, after minor manipulation, in the Crank‚ÄìNicolson scheme
un+1
‚Ñì
‚àí1
2¬µ
Œ≤

k=‚àíŒ±
akun+1
‚Ñì+k = un
‚Ñì+ 1
2¬µ
Œ≤

k=‚àíŒ±
akun
‚Ñì+k,
‚Ñì= 1, 2, . . . , d,
n ‚â•0.
(16.32)
Unlike (16.31), the Crank-Nicolson method is implicit ‚Äì to advance the recursion
by a single step, we need to solve a system of linear equations.
‚Ä¢ The explicit midpoint rule
yn+2 = yn + 2‚àÜtf((n + 1)‚àÜt, yn+1)
(see Exercise 2.5), in tandem with (16.22), yields the leapfrog method
un+2
‚Ñì
= 2¬µ
Œ≤

k=‚àíŒ±
akun+1
‚Ñì+k + un
‚Ñì,
‚Ñì= 1, 2, . . . , d,
n ‚â•1.
(16.33)
The leapfrog scheme is multistep (speciÔ¨Åcally, two-step). This is not very sur-
prising, given that the explicit midpoint method itself requires two steps.
Suppose that the SD scheme is of order p1 and the ODE solver is of order p2. Hence, the
contribution of the semi-discretization to the error is ‚àÜt O((‚àÜx)p1) = O

(‚àÜx)p1+2
,
while the ODE solver adds O

(‚àÜt)p2+1
= O

(‚àÜx)2p2+2
. Altogether, according to
(16.15), the order of the FD method is thus
p = min{p1, 2p2}
(16.34)
(see also Exercise 16.6).
3 FD from SD
Let us marry the SD scheme (16.25) with the ODE solvers
(16.29)‚Äì(16.31).
In the Ô¨Årst instance this yields the Euler method (16.7).
Since, according to Theorem 16.5, p1 = 2 and since, of course, p2 = 1, we
deduce from (16.34) that the order is 2 ‚Äì a result that is implicit in the proof
of Theorem 16.1.
Putting (16.25) into (16.32) yields the Crank‚ÄìNicolson scheme
‚àí1
2¬µun+1
‚Ñì‚àí1 + (1 + ¬µ)un+1
‚Ñì
‚àí1
2¬µun+1
‚Ñì+1 = 1
2¬µun
‚Ñì‚àí1 + (1 ‚àí¬µ)un
‚Ñì+ 1
2¬µun
‚Ñì+1. (16.35)
Since p1 = 2 (the trapezoidal rule is second order, see Section 1.3) and p2 = 2,
we have order 2. The superior order of the trapezoidal rule has not helped in
improving the order of Crank‚ÄìNicolson beyond that of Euler‚Äôs method (16.7).
Bearing in mind that (16.35) is, as well as everything else, implicit, it is fair to
query why should we bother with it in the Ô¨Årst place. The one-word answer,
which will be discussed at length in Sections 16.4 and 16.5, is its stability.

366
The diÔ¨Äusion equation
The explicit midpoint rule is also of order 2, and so is the order of the leapfrog
scheme
un+2
‚Ñì
= 2¬µ(un+1
‚Ñì+1 ‚àí2un+1
‚Ñì
+ un+1
‚Ñì‚àí1 ) + un
‚Ñì.
(16.36)
Similar reasoning can be applied to more general versions of the diÔ¨Äusion
equation and to the SD schemes (16.26)‚Äì(16.28).
3
An alternative technique in designing FD schemes follows similar logic to Theorems 2.1
and 16.5, identifying the order of a method with the order of approximation to a certain
function. In line with (16.22), we write a general FD scheme for the diÔ¨Äusion equation
(16.1) in the form
Œ¥

k=‚àíŒ≥
bk(¬µ)un+1
‚Ñì+k =
Œ≤

k=‚àíŒ±
ck(¬µ)un
‚Ñì+k,
‚Ñì= 1, 2, . . . , d,
n ‚â•0,
(16.37)
where, as before, a diÔ¨Äerent type of discretization might be required near the boundary
if max{Œ±, Œ≤, Œ≥, Œ¥} ‚â•2. We assume that the identity
Œ¥

k=‚àíŒ≥
bk(¬µ) ‚â°1
(16.38)
holds and that b‚àíŒ≥, bŒ¥, c‚àíŒ±, cŒ≤ Ã∏‚â°0. Otherwise the coeÔ¨Écients bk(¬µ) and ck(¬µ) are, for
the time being, arbitrary. If Œ≥ = Œ¥ = 0 then (16.37) is explicit, otherwise the method
is implicit.
We set
Àúa(z, ¬µ) :=
Œ≤
k=‚àíŒ± ck(¬µ)zk
Œ¥
k=‚àíŒ≥ bk(¬µ)zk ,
z ‚ààC,
¬µ > 0.
Theorem 16.6
The method (16.37) is of order p if and only if
Àúa(z, ¬µ) = e¬µ(ln z)2 + c(¬µ)(z ‚àí1)p+2 + O

|z ‚àí1|p+3
,
z ‚Üí1,
(16.39)
where c Ã∏‚â°0.
Proof
The argument is similar to the proof of Theorem 16.5, hence we will just
present its outline. Thus, applying (16.37) to the exact solution, we obtain
Àúe n
‚Ñì=
Œ¥

k=‚àíŒ≥
bk(¬µ)Àúun+1
‚Ñì+k ‚àí
Œ≤

k=‚àíŒ±
ck(¬µ)Àúun
‚Ñì+k
=

Et
Œ¥

k=‚àíŒ≥
bk(¬µ)Ek
x ‚àí
Œ≤

k=‚àíŒ±
ck(¬µ)Ek
x

Àúun
‚Ñì.
We deduce from the diÔ¨Äerential equation (16.1) and the Ô¨Ånite diÔ¨Äerence calculus in
Section 8.1 that
Et = e(‚àÜt)Dt = e¬µ(‚àÜxDx)2 = e¬µ(ln Ex)2,

16.4
Stability analysis I: Eigenvalue techniques
367
and this renders Àúe n
‚Ñìin the language of Ex:
Àúe n
‚Ñì=

e¬µ(ln Ex)2
Œ¥

k=‚àíŒ≥
bk(¬µ)Ek
x ‚àí
Œ≤

k=‚àíŒ±
ck(¬µ)Ek
x

Àúun
‚Ñì.
Next, we conclude from (16.39), from Ex = I + O(‚àÜx) and from the normalization
condition (16.38) that
e¬µ(ln Ex)2
Œ¥

k=‚àíŒ≥
bk(¬µ)Ek
x ‚àí
Œ≤

k=‚àíŒ±
ck(¬µ)Ek
x = O

(‚àÜx)p+2
and comparison with (16.15) completes the proof.
3 FD from the function Àúa
We commence by revisiting methods that have
already been presented in this chapter. As we saw earlier in this section, it is
a useful practice to let z = eiŒ∏, so that z ‚Üí1 is replaced by Œ∏ ‚Üí0.
In the case of the Euler method (16.7) we have
Àúa(z, ¬µ) = 1 + ¬µ(z‚àí1 ‚àí2 + z);
therefore
Àúa(eiŒ∏) = 1 ‚àí4¬µ sin2 1
2Œ∏ = 1 ‚àí¬µŒ∏2 + 1
12¬µŒ∏4 + O

Œ∏6
= e‚àí¬µŒ∏2 + O

Œ∏4
,
Œ∏ ‚Üí0,
and we deduce order 2 from (16.39).
For the Crank‚ÄìNicolson method (16.35) we have
Àúa(z, ¬µ) = 1 + 1
2¬µ(z‚àí1 ‚àí2 + z)
1 ‚àí1
2¬µ(z‚àí1 ‚àí2 + z)
(note that (16.38) is satisÔ¨Åed), hence
Àúa(eiŒ∏) = 1 ‚àí2¬µ sin2 1
2Œ∏
1 + 2¬µ sin2 1
2Œ∏ = 1 ‚àí¬µŒ∏2 +
 1
3¬µ + 1
4¬µ2
Œ∏4 + O

Œ∏6
= e‚àí¬µŒ∏2 + O

Œ∏4
,
Œ∏ ‚Üí0.
Again, we obtain order 2.
The leapfrog method (16.36) does not Ô¨Åt into the framework of Theorem 16.6,
but it is not diÔ¨Écult to derive order conditions along the lines of (16.39) for
two-step methods, a task left to the reader.
3
Using the approach of Theorems 16.5 and 16.6, it is possible to express order conditions
as a problem in approximation in the two-dimensional case also.
This is not so,
however, for a variable diÔ¨Äusion coeÔ¨Écient; the quickest practical route toward FD
schemes for (16.6) lies in combining the SD method (16.30) with, say, the trapezoidal
rule.

368
The diÔ¨Äusion equation
16.4
Stability analysis I: Eigenvalue techniques
Throughout this section we will restrict our attention, mainly for the sake of simplicity
and brevity, to the case of zero boundary conditions. Therefore, the relevant stability
requirements are (16.17) and (16.21) for FD and SD schemes respectively.
A real square matrix B is normal if BB‚ä§= B‚ä§B (A.1.2.5). Important special
cases are symmetric and skew-symmetric matrices.
Two properties of normal matrices are relevant to the material of this section.
Firstly, every d √ó d normal matrix B possesses a complete set of unitary eigenvectors;
in other words, the eigenvectors of B span a d-dimensional linear space and ¬Øw‚ä§
j w‚Ñì= 0
for any two distinct eigenvectors wj, w‚Ñì‚ààCd (A.1.5.3). Secondly, all normal matrices
B obey the identity ‚à•B‚à•= œÅ(B), where ‚à•¬∑ ‚à•is the Euclidean norm and œÅ is the
spectral radius. The proof is easy and we leave it to the reader (see Exercise 16.10).
We denote the usual Euclidean inner product by ‚ü®¬∑ , ¬∑ ‚ü©, hence
‚ü®x, y‚ü©= x‚ä§y,
x, y ‚ààRd.
(16.40)
Theorem 16.7
Let us suppose that the matrix A‚àÜx is normal for every suÔ¨Éciently
small ‚àÜx > 0 and that there exists ŒΩ ‚â•0 such that
œÅ(A‚àÜx) ‚â§eŒΩ‚àÜt,
‚àÜx ‚Üí0.
(16.41)
Then the FD method (16.13) is stable.5
Proof
We choose an arbitrary t‚àó> 0 and, as before, let n‚àÜt := t‚àó/‚àÜt. Hence it
is true for every vector w‚àÜx Ã∏= 0‚àÜx that
‚à•An
‚àÜxw‚àÜx‚à•2
‚àÜx = ‚ü®An
‚àÜxw‚àÜx, An
‚àÜxw‚àÜx‚ü©‚àÜx
= ‚ü®w‚àÜx, (An
‚àÜx)‚ä§An
‚àÜxw‚àÜx‚ü©‚àÜx,
n = 0, 1, . . . , n‚àÜt.
Note that we have used here the identity ‚ü®Bx, y‚ü©= ‚ü®x, B‚ä§y‚ü©, which follows at once
from (16.40).
It is trivial to verify by induction, using the normalcy of A‚àÜx, that
(An
‚àÜx)‚ä§An
‚àÜx = (A‚ä§
‚àÜxA‚àÜx)n,
n = 0, 1, . . . , n‚àÜt.
Therefore, by the triangle inequality (A.1.3.3) and the deÔ¨Ånition of a matrix norm
(A.1.3.4), we have
‚à•An
‚àÜxw‚àÜx‚à•2
‚àÜx = ‚ü®w‚àÜx, (A‚ä§
‚àÜxA‚àÜx)nw‚àÜx‚ü©‚àÜx
‚â§‚à•w‚àÜx‚à•‚àÜx √ó ‚à•(A‚ä§
‚àÜxA‚àÜx)nw‚àÜx‚à•‚àÜx
‚â§‚à•w‚àÜx‚à•2
‚àÜx √ó ‚à•(A‚ä§
‚àÜxA‚àÜx)n‚à•‚àÜx
‚â§‚à•w‚àÜx‚à•2
‚àÜx √ó ‚à•A‚àÜx‚à•2n
‚àÜx
5We recall that ‚àÜt ‚Üí0 as ‚àÜx ‚Üí0 and that the Courant number remains constant.

16.4
Stability analysis I: Eigenvalue techniques
369
for n = 0, 1, . . . , n‚àÜt. Recalling that A‚àÜx is normal, hence that its norm and spectral
radius coincide, we deduce from (16.41) the inequality
‚à•An
‚àÜxw‚àÜx‚à•‚àÜx
‚à•w‚àÜx‚à•‚àÜx
‚â§[ œÅ(A‚àÜx)]n ‚â§eŒΩn‚àÜt ‚â§eŒΩt‚àó,
n = 0, 1, . . . , n‚àÜt.
(16.42)
The crucial observation about (16.42) is that it holds uniformly for ‚àÜx ‚Üí0. Since by
the deÔ¨Ånition of a matrix norm
‚à•An
‚àÜx‚à•‚àÜx =
max
w‚àÜxÃ∏=0‚àÜx
‚à•An
‚àÜxw‚àÜx‚à•‚àÜx
‚à•w‚àÜx‚à•‚àÜx
,
it follows that (16.17) is satisÔ¨Åed by c(t‚àó) = eŒΩt‚àóand the method (16.13) is stable.
It is of interest to consider an alternative proof of the theorem, which highlights
the role of normalcy and clariÔ¨Åes why, in its absence, the condition (16.41) may not be
suÔ¨Écient for stability. Suppose, thus, that A‚àÜx has a complete set of eigenvectors but
is not necessarily normal. We can factorize A‚àÜx as V‚àÜxD‚àÜxV ‚àí1
‚àÜx , where V‚àÜx is the
matrix of the eigenvectors, while D‚àÜx is a diagonal matrix of eigenvalues (A.1.5.4). It
follows that, for every n = 0, 1, . . . , n‚àÜt,
‚à•An
‚àÜx‚à•= ‚à•(V‚àÜxD‚àÜxV ‚àí1
‚àÜx )n‚à•‚àÜx = ‚à•V‚àÜxDn
‚àÜxV ‚àí1
‚àÜx ‚à•‚àÜx
‚â§‚à•V‚àÜx‚à•‚àÜx √ó ‚à•Dn
‚àÜx‚à•‚àÜx √ó ‚à•V ‚àí1
‚àÜx ‚à•‚àÜx.
The matrix D‚àÜx is diagonal and its diagonal components, dj,j, say, are the eigenvalues
of A‚àÜx. Therefore
‚à•Dn
‚àÜx‚à•‚àÜx = max
j
|dn
j,j| = (max
j
|dj,j|)n = [œÅ(A‚àÜx)]n
and we deduce that
‚à•An
‚àÜx‚à•‚â§Œ∫‚àÜx[œÅ(A‚àÜx)]n,
(16.43)
where
Œ∫‚àÜx := ‚à•V‚àÜx‚à•‚àÜx √ó ‚à•V ‚àí1
‚àÜx ‚à•‚àÜx
is the spectral condition number of the matrix V‚àÜx.
On the face of it, we could have continued from (16.43) in a manner similar to the
proof of Theorem 16.7, thereby proving the inequality
‚à•An
‚àÜx‚à•‚àÜx ‚â§Œ∫‚àÜxeŒΩt‚àó,
n = 0, 1, . . . , n‚àÜt.
This looks deceptively like a proof of stability without assuming normalcy in the
process. The snag, of course, is in the number Œ∫‚àÜx: as ‚àÜx tends to zero, it is entirely
possible that Œ∫‚àÜx becomes inÔ¨Ånite! However, if A‚àÜx is normal then its eigenvectors
are orthogonal, therefore ‚à•V‚àÜx‚à•‚àÜx, ‚à•V ‚àí1
‚àÜx ‚à•‚àÜx ‚â°1 for all ‚àÜx (A.1.3.4) and we can
indeed use (16.43) to construct an alternative proof of the theorem.

370
The diÔ¨Äusion equation
Using the same approach as in Theorem 16.7, we can prove a stability condition
for SD schemes with normal matrices.
Theorem 16.8
Let the matrix P‚àÜx be normal for every suÔ¨Éciently small ‚àÜx > 0.
If there exists Œ∑ ‚ààR such that
Re Œª ‚â§Œ∑
for every
Œª ‚ààœÉ(P‚àÜx)
and
‚àÜx ‚Üí0
(16.44)
then the SD method (16.18) is stable.
Proof
Let t‚àó> 0 be given. Because of the normalcy of P‚àÜx, it follows along
similar lines to the proof of Theorem 16.7 that
‚à•etP‚àÜxw‚àÜx‚à•2
‚àÜx = ‚ü®etP‚àÜxw‚àÜx, etP‚àÜxw‚àÜx‚ü©‚àÜx =
@
w‚àÜx, (etP‚àÜx)‚ä§etP‚àÜxw‚àÜx
A
‚àÜx
=
B
w‚àÜx, etP ‚ä§
‚àÜxetP‚àÜxw‚àÜx
C
‚àÜx =
B
w‚àÜx, et(P ‚ä§
‚àÜx+P‚àÜx)w‚àÜx
C
‚àÜx
‚â§‚à•w‚àÜx‚à•2
‚àÜx √ó ‚à•et(P ‚ä§
‚àÜx+P‚àÜx)‚à•‚àÜx = ‚à•w‚àÜx‚à•2
‚àÜx œÅ(et(P ‚ä§
‚àÜx+P‚àÜx))
= ‚à•w‚àÜx‚à•2
‚àÜx max

e2tRe Œª : Œª ‚ààœÉ(P‚àÜx)

‚â§‚à•w‚àÜx‚à•2
‚àÜxe2Œ∑t‚àó,
t ‚àà[0, t‚àó].
We leave it to the reader to verify that for all normal matrices B and for t ‚â•0 it is
true that
(etB)‚ä§= etB‚ä§
etB‚ä§etB = et(B‚ä§+B)
(note that the second identity might fail unless B is normal!) and that
œÉ(et(B‚ä§+B)) = {e2tRe Œª, Œª ‚ààœÉ(P‚àÜx)}.
We deduce stability from the deÔ¨Ånition (16.21).
The spectral abscissa of a square matrix B is the real number
ÀúŒ±(B) := max {Re Œª : Œª ‚ààœÉ(B)}.
We can rephrase Theorem 16.8 by requiring ÀúŒ±(P‚àÜx) ‚â§Œ∑ for all ‚àÜx ‚Üí0.
The great virtue of Theorems 16.7 and 16.8 is that they reduce the task of deter-
mining stability to that of locating the eigenvalues of a normal matrix. Even better, to
establish stability it is often suÔ¨Écient to bound the spectral radius or the spectral ab-
scissa. According to a broad principle mentioned in Chapter 2, we replace the analytic
‚Äì and diÔ¨Écult ‚Äì stability conditions (16.17) and (16.21) by algebraic requirements.
3 Eigenvalues and stability of methods for the diÔ¨Äusion equation
The
matrix associated with the SD method (16.25) is (in the natural ordering
of grid points, from left to right) TST and, according to Lemma 12.5, its
eigenvalues are ‚àí4 sin2[œÄ‚Ñì/(d + 1)], ‚Ñì= 1, 2, . . . , d. Therefore
ÀúŒ±(P‚àÜx) = ‚àí4 sin2(œÄ‚àÜx) ‚â§0,
‚àÜx > 0
(recall that (d + 1)‚àÜx = 1) and the method is stable.

16.4
Stability analysis I: Eigenvalue techniques
371
Next we consider Euler‚Äôs FD scheme (16.7). The matrix A‚àÜx is again TST
and its eigenvalues are 1 ‚àí4¬µ sin2[œÄ‚Ñì/(d + 1)], ‚Ñì= 1, 2, . . . , d. Therefore
œÅ(A‚àÜx) ‚â°|1 ‚àí4¬µ|,
‚àÜx > 0.
Consequently, (16.41) is satisÔ¨Åed by ŒΩ = 0 for ¬µ ‚â§1
2, whereas no ŒΩ will do for
¬µ > 1
2. This, in tandem with the Lax equivalence theorem (Theorem 16.3)
and our observation from Section 16.3 that (16.7) is a second-order method,
provides a brief alternative proof of Theorem 16.1.
The next candidate for our attention is the Crank‚ÄìNicolson scheme (16.35),
which we also render in a vector form. Disregarding for a moment our as-
sumption that the forcing term and boundary contributions vanish, we have
A[+]
‚àÜxun+1
‚àÜx = A[‚àí]
‚àÜxun
‚àÜx + Àúk
n
‚àÜx,
n ‚â•0,
where the matrices A[¬±]
‚àÜx are TST while the vector Àúk
n
‚àÜx contains the contri-
bution of both forcing and boundary terms. Therefore A‚àÜx = A[+]
‚àÜx
‚àí1A[‚àí]
‚àÜx
and kn
‚àÜx = A[+]
‚àÜx
‚àí1Àúk
n
‚àÜx. (We insist on the presence of the forcing terms Àúk
n
‚àÜx
before eliminating them for the sake of stability analysis, since this procedure
illustrates how to construct the form (16.13) for general implicit FD schemes.)
According to Lemma 12.5, TST matrices of the same dimension share the
same set of eigenvectors. Moreover, these eigenvactors span the whole space,
consequently A¬±
‚àÜx = V‚àÜxD[¬±]
‚àÜxV ‚àí1
‚àÜx , where V‚àÜx is the matrix of the eigenvectors
and D[¬±]
‚àÜx are diagonal. Therefore
A‚àÜx = V‚àÜxD[+]
‚àÜx
‚àí1D[‚àí]
‚àÜxV ‚àí1
‚àÜx
and the eigenvalues of the quotient matrix of two TST matrices ‚Äì itself, in
general, not TST ‚Äì are the quotients of the eigenvalues of A[¬±]
‚àÜx. Employing
again Lemma 12.5, we write down explicitly the eigenvalues of the latter,
œÉ(A[¬±]
‚àÜx) =
#
1 ¬± 2¬µ sin2
	
œÄ‚Ñì
2(d + 1)

: ‚Ñì= 1, 2, . . . , d
$
,
hence
œÉ(A‚àÜx) =
#1 ‚àí2¬µ sin2{œÄ‚Ñì/[2(d + 1)]}
1 + 2¬µ sin2{œÄ‚Ñì/[2(d + 1)]} : ‚Ñì= 1, 2, . . . , d
$
and we deduce that
œÅ(A‚àÜx) = |1 ‚àí2¬µ sin2(œÄ‚àÜx/2)|
1 + 2¬µ sin2(œÄ‚àÜx/2) ‚â§1.
Therefore the Crank‚ÄìNicolson scheme is stable for all ¬µ > 0.
All three aforementioned examples make use of TST matrices, but this tech-
nique is, unfortunately, of limited scope.
Consider, for example, the SD

372
The diÔ¨Äusion equation
scheme (16.30) for the variable diÔ¨Äusion coeÔ¨Écient PDE (16.6). Writing this
in the form (16.18), we obtain for (‚àÜx)2P‚àÜx the following matrix:
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
‚àía‚àí1/2 ‚àía1/2
a1/2
0
¬∑ ¬∑ ¬∑
0
a1/2
‚àía1/2 ‚àía3/2
a3/2
p p p
...
0
p p p
p p p
p p p
0
...
p p p
ad‚àí3/2 ‚àíad‚àí3/2 ‚àíad‚àí1/2
ad‚àí1/2
0
¬∑ ¬∑ ¬∑
0
ad‚àí1/2
‚àíad‚àí1/2 ‚àíad+1/2
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
.
Clearly, in general P‚àÜx is not a Toeplitz matrix and so we are not allowed to
use Lemma 12.5. However, it is symmetric, hence normal, and we are within
the conditions of Theorem 16.8.
Although we cannot Ô¨Ånd the eigenvalues of P‚àÜx, we can exploit the GerÀásgorin
criterion (Lemma 8.3) to derive enough information about their location to
prove stability.
Since a(x) > 0, x ‚àà[0, 1], it follows at once that all the
GerÀásgorin discs Si, i = 1, 2, . . . , d, lie in the closed complex left half-plane.
Therefore ÀúŒ±(P‚àÜx) ‚â§0, hence we have stability.
3
16.5
Stability analysis II: Fourier techniques
We commence this section by assuming that we are solving an evolutionary PDE given
(in a single spatial dimension) for all x ‚ààR and that in place of boundary conditions,
say, (16.3) we impose the requirement that the function u( ¬∑ , t) is square-integrable
in R for all t ‚â•0. As we mentioned in Section 16.1, this is known as the Cauchy
problem.
The technique of the present section is valid whenever a Cauchy problem for an
arbitrary linear PDE of evolution with constant coeÔ¨Écients is solved by a method ‚Äì
either SD or FD ‚Äì that employs an identical formula at each grid point. For simplicity,
however, we restrict ourselves here to the diÔ¨Äusion equation and to the general SD and
FD schemes (16.22) and (16.37) respectively (except that the range of ‚Ñìnow extends
across all Z). The reader should bear in mind, however, that special properties of the
diÔ¨Äusion equation ‚Äì except in the narrowest technical sense, e.g. with regard to the
power of ‚àÜx in (16.22) and (16.37) ‚Äì are never used in our exposition. This makes for
entirely straightforward generalization.
The deÔ¨Ånition of stability depends on the underlying norm and throughout this
section we consider exclusively the Euclidean norm over bi-inÔ¨Ånite sequences. The set
‚Ñì[Z] is the linear space of all complex sequences, indexed over the integers, that are
bounded in the Euclidean vector norm. In other words,
w = {wm}‚àû
m=‚àí‚àû‚àà‚Ñì2[Z]
if and only if
‚à•w‚à•:=

‚àû

m=‚àí‚àû
|wm|2
1/2
< ‚àû.

16.5
Stability analysis II: Fourier techniques
373
Note that throughout the present section we omit the factor (‚àÜx)1/2 in the deÔ¨Ånition
of the Euclidean norm, mainly to unclutter the notation and to bring it into line with
the standard terminology of Fourier analysis. We also allow ourselves the liberating
convention of dropping the subscripts ‚àÜx in our formulae, the reason being that ‚àÜx
no longer expresses the reciprocal of the number of grid points ‚Äì which is inÔ¨Ånite for
all ‚àÜx. The only inÔ¨Çuence of ‚àÜx on the underlying equations is expressed in the
multiplier (‚àÜx)‚àí2 for SD equations and ‚Äì most importantly ‚Äì in the spacing of the
grid along which we are sampling the initial condition g.
We let L[0, 2œÄ] denote the set of all complex, square-integrable functions in [0, 2œÄ],
equipped with the Euclidean function norm:
w ‚ààL[0, 2œÄ]
if and only if
|||w||| =
 1
2œÄ
 2œÄ
0
|w(Œ∏)|2 dŒ∏
1/2
< ‚àû.
Solutions of either (16.22) or (16.37) live in ‚Ñì[Z] (remember that the index ranges
across all integers!), consequently we phrase their stability in terms of the norm in
that space. As it turns out, however, it is considerably more convenient to investigate
stability in L[0, 2œÄ]. The opportunity to abandon ‚Ñì[Z] in favour of L[0, 2œÄ] is con-
ferred by the Fourier transform. We have already encountered a similar concept in
Chapters 10, 13 and 15 in a diÔ¨Äerent context. For our present purpose, we choose a
deÔ¨Ånition diÔ¨Äerent from that in Chapter 10, letting
ÀÜw(Œ∏) =
‚àû

m=‚àí‚àû
wme‚àíimŒ∏,
w = (wm)‚àû
m=‚àí‚àû‚àà‚Ñì[Z].
(16.45)
Lemma 16.9
The mapping (16.45) takes ‚Ñì[Z] onto L[0, 2œÄ]. It is an isomorphism
(i.e., a one-to-one mapping) and its inverse is given by
wm = 1
2œÄ
 2œÄ
0
ÀÜw(Œ∏)eimŒ∏ dŒ∏,
m ‚ààZ,
ÀÜw ‚ààL[0, 2œÄ].
(16.46)
Moreover, (16.45) is an isometry:
||| ÀÜw||| = ‚à•w‚à•,
w ‚àà‚Ñì[Z].
(16.47)
Proof
We combine the proof that ÀÜw ‚ààL[0, 2œÄ] (hence, that (16.45) indeed takes
‚Ñì[Z] to L[0, 2œÄ]) with the proof of (16.47), by evaluating the norm of ÀÜw:
||| ÀÜw|||2 = 1
2œÄ
 2œÄ
0
‚àû

m=‚àí‚àû
‚àû

j=‚àí‚àû
wm ¬Øwjei(j‚àím)Œ∏ dŒ∏
= 1
2œÄ
‚àû

m=‚àí‚àû
‚àû

j=‚àí‚àû
wm ¬Øwj
 2œÄ
0
ei(j‚àím)Œ∏ dŒ∏ =
‚àû

m=‚àí‚àû
|wm|2 = ‚à•w‚à•2.
Note our use of the identity
1
2œÄ
 2œÄ
0
eikŒ∏ dŒ∏ =
# 1,
k = 0,
0,
otherwise,
k ‚ààZ.

374
The diÔ¨Äusion equation
The argument required to prove that the mapping w ‚ÜíÀÜw is an isomorphism onto
L[0, 2œÄ] and that its inverse is given by (16.46) is an almost exact replica of the proof
of Lemma 10.2.
We will call ÀÜw the Fourier transform of w. This is at variance with the terminology
of Chapter 10 ‚Äì by rights, we should call ÀÜw the inverse Fourier transform of w. The
present usage, however, has the advantage of brevity.
The isomorphic isometry of the Fourier transform is perhaps the main reason for its
importance in a wide range of applications. A Euclidean norm typically measures the
energy of physical systems and a major consequence of (16.47) is that, while travelling
back and forth between ‚Ñì[Z] and L[0, 2œÄ] by means of the Fourier transform and its
inverse, the energy stays intact.
We commence our analysis with the SD scheme (16.22), recalling that the index
ranges across all ‚Ñì‚ààZ. We multiply the equation by e‚àíi‚ÑìŒ∏ and sum over ‚Ñì; the outcome
is
‚àÇÀÜv(Œ∏, t)
‚àÇt
=
‚àû

‚Ñì=‚àí‚àû
v‚Ä≤
‚Ñìe‚àíi‚ÑìŒ∏ =
1
(‚àÜx)2
‚àû

‚Ñì=‚àí‚àû
Œ≤

k=‚àíŒ±
akv‚Ñì+ke‚àíi‚ÑìŒ∏
=
1
(‚àÜx)2
Œ≤

k=‚àíŒ±
ak
‚àû

‚Ñì=‚àí‚àû
v‚Ñì+ke‚àíi‚ÑìŒ∏ =
1
(‚àÜx)2
Œ≤

k=‚àíŒ±
ak
‚àû

‚Ñì=‚àí‚àû
v‚Ñìe‚àíi(‚Ñì‚àík)Œ∏
=
1
(‚àÜx)2
Œ≤

k=‚àíŒ±
akeikŒ∏
‚àû

‚Ñì=‚àí‚àû
v‚Ñìe‚àíi‚ÑìŒ∏ = a(eiŒ∏)
(‚àÜx)2 ÀÜv(Œ∏, t),
where the function a( ¬∑ ) was deÔ¨Åned in Section 16.3. The crucial step in the above
argument is the shift of the index from ‚Ñìto ‚Ñì‚àík without changing the endpoints of
the summation, a trick that explains why we require that ‚Ñìshould range across all the
integers.
We have just proved that the Fourier transform ÀÜv = ÀÜv(Œ∏, t) obeys, as a function of
t, the linear ODE
‚àÇÀÜv
‚àÇt = a(eiŒ∏)
(‚àÜx)2 ÀÜv,
t ‚â•0,
Œ∏ ‚àà[0, 2œÄ].
with initial condition ÀÜv(Œ∏, 0) = ÀÜg, where gm = u(m‚àÜx, 0), m ‚ààZ, is the projection on
the grid of the initial condition of the PDE. The solution of the ODE can be written
down explicitly:
ÀÜv(Œ∏, t) = ÀÜg(Œ∏) exp
a(eiŒ∏)t
(‚àÜx)2

,
t ‚â•0,
Œ∏ ‚àà[0, 2œÄ].
(16.48)
Suppose that
Re a(eiŒ∏) ‚â§0,
Œ∏ ‚àà[0, 2œÄ].
(16.49)
In that case it follows from (16.48) that
|||ÀÜv|||2 = 1
2œÄ
 2œÄ
0
|ÀÜg(Œ∏)|2 exp
2 Re a(eiŒ∏)
(‚àÜx)2

dŒ∏ ‚â§1
2œÄ
 2œÄ
0
|ÀÜg(Œ∏)|2 dŒ∏ = |||ÀÜg|||2,

16.5
Stability analysis II: Fourier techniques
375
Therefore, according to (16.47),
‚à•v(t)‚à•‚â§‚à•v(0)‚à•
for all possible initial conditions v ‚àà‚Ñì[Z]. We thus conclude that, according to (16.20),
the method is stable.
Next, we consider the case when the condition (16.49) is violated, in other words,
when there exists Œ∏0 ‚àà[0, 2œÄ] such that Re a(eiŒ∏0) > 0. Since a(eiŒ∏) is continuous in Œ∏,
there exist Œµ > 0 and 0 ‚â§Œ∏‚àí< Œ∏+ < 2œÄ such that
Re a(eiŒ∏) > Œµ,
Œ∏ ‚àà[Œ∏‚àí, Œ∏+].
We choose an initial condition g such that ÀÜg is a characteristic function of the interval
[Œ∏‚àí, Œ∏+]:
ÀÜg(Œ∏) =
#
1,
Œ∏ ‚àà[Œ∏‚àí, Œ∏+],
0
otherwise
(it is possible to identify easily a square-integrable initial condition g with the above
ÀÜg). It follows from (16.48) that
|||ÀÜv|||2 = 1
2œÄ
 Œ∏+
Œ∏‚àí
exp
2 Re a(eiŒ∏)t
(‚àÜx)2

dŒ∏ ‚â•1
2œÄ
 Œ∏+
Œ∏‚àí
exp
 2Œµt
(‚àÜx)2

dŒ∏
= Œ∏+ ‚àíŒ∏‚àí
2œÄ
exp
 2Œµt
(‚àÜx)2

.
Therefore |||ÀÜv||| cannot be uniformly bounded for t ‚àà[0, t‚àó] (regardless of the size of
t‚àó> 0) as ‚àÜx ‚Üí0. We will again exploit isometry to argue that (16.22) is unstable.
Theorem 16.10
The SD method (16.22), when applied to a Cauchy problem, is
stable if and only if the inequality (16.49) is obeyed.
Fourier analysis can be applied with similarly telling eÔ¨Äect to the FD scheme
(16.37) ‚Äì again, with ‚Ñì‚ààZ. The argument is almost identical, hence we present it
with greater brevity.
Theorem 16.11
The FD method (16.37), when applied to a Cauchy problem, is
stable for a speciÔ¨Åc value of the Courant number ¬µ if and only if
|Àúa(eiŒ∏, ¬µ)| ‚â§1,
Œ∏ ‚àà[0, 2œÄ],
(16.50)
where
Àúa(z, ¬µ) =
Œ≤
k=‚àíŒ± ck(¬µ)zk
Œ¥
k=‚àíŒ≥ bk(¬µ)zk ,
z ‚ààC.
Proof
We multiply both sides of (16.37) by e‚àíi‚ÑìŒ∏ and sum over ‚Ñì‚ààZ.
The
outcome is the recursive relationship
ÀÜun+1 = Àúa(eiŒ∏, ¬µ)ÀÜun,
n ‚â•0,

376
The diÔ¨Äusion equation
between the Fourier transforms in adjacent time levels. Iterating this recurrence results
in the explicit formula
ÀÜun =

Àúa(eiŒ∏, ¬µ)
n ÀÜu0,
n ‚â•0,
where, of course, ÀÜu0 = ÀÜg. Therefore
‚à•un‚à•2 = |||ÀÜun|||2 = 1
2œÄ
 2œÄ
0

Àúa(eiŒ∏)
n ÀÜu0(Œ∏) dŒ∏,
n ‚â•0.
(16.51)
If (16.50) is satisÔ¨Åed we deduce from (16.51) that
‚à•un‚à•‚â§‚à•u0‚à•,
n ‚â•0.
Stability follows from (16.16) by virtue of isometry.
The course of action when (16.50) fails is identical to our analysis of SD methods.
We have Œµ > 0 such that |Àúa(eiŒ∏, ¬µ)| ‚â•1 + Œµ for all Œ∏ ‚àà[Œ∏‚àí, Œ∏+]. Picking ÀÜu0 = ÀÜg as the
characteristic function of the interval [Œ∏‚àí, Œ∏+], we exploit (16.51) to argue that
‚à•un‚à•2 = |||ÀÜun|||2 = 1
2œÄ
 2œÄ
0
Àúa(eiŒ∏, ¬µ)
2n g(Œ∏) dŒ∏ ‚â•Œ∏+ ‚àíŒ∏‚àí
2œÄ
(1 + Œµ)n,
n ‚â•0.
This concludes the proof of instability.
3 The Fourier technique in practice
It is trivial to use Theorem 16.10 to
prove that the SD method (16.25) is stable, since a(eiŒ∏) = ‚àí4 sin2 1
2Œ∏. Let us
attempt a more ambitious goal, the fourth-order SD scheme (16.27). We have
a(eiŒ∏) = ‚àí1
12e‚àí2iŒ∏ + 4
3e‚àíiŒ∏ ‚àí5
2 + 4
3eiŒ∏ ‚àí1
12e2iŒ∏
= ‚àí7
3 + 8
3 cos Œ∏ ‚àí1
3 cos2 Œ∏ = ‚àí1
3(1 ‚àícos Œ∏)(7 ‚àícos Œ∏) ‚â§0
for all Œ∏ ‚àà[0, 2œÄ], hence stability.
Whenever the Fourier technique can be put to work, results are easily obtained
and this is also true with regard to FD schemes. The Euler method (16.7)
yields
Àúa(eiŒ∏, ¬µ) = 1 ‚àí4¬µ sin2 1
2Œ∏,
Œ∏ ‚àà[0, 2œÄ],
and it is trivial to deduce that (16.50) implies stability if and only if ¬µ ‚â§1
2.
Likewise, for the Crank‚ÄìNicolson method we have
Àúa(eiŒ∏, ¬µ) = 1 ‚àí2¬µ sin2 1
2Œ∏
1 + 2¬µ sin2 1
2Œ∏ ‚àà[‚àí1, 1],
Œ∏ ‚àà[0, 2œÄ],
and hence stability for all ¬µ > 0.
Let us set ourselves a fairer challenge. Solving the SD scheme (16.25) with the
Adams‚ÄìBashforth method (2.6) results in the second-order two-step scheme
un+2
‚Ñì
= un+1
‚Ñì
+ 3
2¬µ(un+1
‚Ñì‚àí1 ‚àí2un+1
‚Ñì
+ un+1
‚Ñì+1 ) ‚àí1
2(un
‚Ñì‚àí1 ‚àí2un
‚Ñì+ un
‚Ñì+1). (16.52)

16.5
Stability analysis II: Fourier techniques
377
It is not diÔ¨Écult to extend the Fourier technique to multistep methods. We
multiply (16.52) by e‚àíi‚ÑìŒ∏ and sum for all ‚Ñì‚ààZ; the outcome is the three-term
recurrence relation
ÀÜun+2 ‚àí

1 ‚àí6¬µ sin2 1
2Œ∏

ÀÜun+1 ‚àí2¬µ

sin2 1
2Œ∏

ÀÜun = 0,
n ‚â•0.
(16.53)
The general solution of the diÔ¨Äerence equation (16.53) is
ÀÜun = q‚àí(Œ∏)[œâ‚àí(Œ∏)]n + q+(Œ∏)[œâ+(Œ∏)]n,
n = 0, 1, . . . ,
where œâ¬± are zeros of the characteristic equation
œâ2 ‚àí

1 ‚àí6¬µ sin2 1
2Œ∏

œâ ‚àí2¬µ sin2 1
2Œ∏ = 0
and q¬± depend on the starting values (see Section 4.4 for the solution of
comparable diÔ¨Äerence equations). As before, the condition for stability is uni-
form boundedness of the set {|||ÀÜun|||}n=0,1..., since this implies that the vectors
{‚à•un‚à•}n=0,1,... are uniformly bounded. Evidently, the Fourier transforms are
uniformly bounded for all q¬± if and only if |œâ¬±(Œ∏)| ‚â§1 for all Œ∏ ‚àà[0, 2œÄ] and,
whenever |œâ¬±(Œ∏)| = 1 for some Œ∏, the two zeros are distinct ‚Äì in other words,
the root condition all over again!
We use Lemma 4.9 to verify the root condition and this, after some trivial yet
tedious algebra, results in the stability condition ¬µ ‚â§2
5.
Another example of a two-step method, the leapfrog scheme (16.36), features
in Exercise 16.13.
3
The scope of the Fourier technique can be generalized in several directions. The easiest
is from one to several spatial dimensions and this requires a multivariate counterpart
of the Fourier transform (16.45).
More interesting is a relaxation of the ban on boundary conditions in Ô¨Ånite time
‚Äì after all, most physical objects subjected to mathematical modelling possess Ô¨Ånite
size! In Chapter 17 we will mention brieÔ¨Çy periodic boundary conditions, which lend
themselves to the same treatment as the Cauchy problem. Here, though, we address
ourselves to the Dirichlet boundary conditions (16.3), which are more characteristic
of parabolic equations. Without going into any proofs we simply state that, provided
that an SD or an FD method uses just one point from the right and one from the left
(in other words, max{Œ±, Œ≤, Œ≥, Œ¥} = 1), the scope of the Fourier technique extends to
Ô¨Ånite intervals. Thus, the outcome of the Fourier analysis for the SD (16.25), the Euler
method (16.7), the Crank‚ÄìNicolson scheme (16.35) and, indeed, the Adams‚ÄìBashforth
two-step FD (16.52) extends in toto to Dirichlet boundary conditions, but this is not
the case with the fourth-order SD scheme (16.27). There, everything depends on our
treatment of the ‚Äòmissing‚Äô values near the boundary. This is an important subject
‚Äì admittedly more important in the context of hyperbolic diÔ¨Äerential equations, the
theme of Chapter 17 ‚Äì which requires a great deal of advanced mathematics and is
well outside the scope of this book.
This section would not be complete without the mention of a remarkable connec-
tion, which might have already caught the eye of a vigilant reader. Let us consider a

378
The diÔ¨Äusion equation
simple example, the ‚Äòbasic‚Äô SD (16.25). The Fourier condition for stability is
Re a(eiŒ∏) = ‚àí4 sin2 1
2Œ∏ ‚â§0,
Œ∏ ‚àà[0, 2œÄ],
while the eigenvalue condition is nothing other than
Re a(œâ‚Ñì
d+1) = ‚àí4 sin2
	 œÄ‚Ñì
d + 1

‚â§0,
‚Ñì= 1, 2, . . . , d,
where œâd = exp[2iœÄ/(d + 1)] is the dth root of unity. A similar connection exists for
Euler‚Äôs method and Crank‚ÄìNicolson. Before we get carried away, we need to clarify
that this coincidence is restricted, at least in the context of the Cauchy problem,
mostly to methods that are constructed from TST matrices.
16.6
Splitting
Even the stablest and the most heavily analysed method must be, ultimately, run on a
computer. This can be even more expensive for PDEs of evolution than for the Poisson
equation; in a sense, using an implicit method for (16.1) in two spatial dimensions, say,
and with a forcing term is equivalent to solving a Poisson equation in every time step.
Needless to say, by this stage we know full well that eÔ¨Äective solution of the diÔ¨Äusion
equation calls for implicit schemes; otherwise, we would need to advance with such a
miniscule step ‚àÜt as to render the whole procedure unrealistically expensive.
The emphasis on two (or more) space dimensions is important, since in one di-
mension the algebraic equations originating in the Crank‚ÄìNicolson scheme, say, are
fairly small and tridiagonal (cf. (16.32)) and can be easily solved with banded LU
factorization from Chapter 11.6
We restrict our analysis to the diÔ¨Äusion equation
‚àÇu
‚àÇt = ‚àá(a‚àáu),
0 ‚â§x, y ‚â§1,
(16.54)
where the diÔ¨Äusion coeÔ¨Écient a = a(x, y) is bounded and positive in [0, 1] √ó [0, 1].
The starting point of our discussion is an extension of the SD equations (16.29) to
two dimensions,
v‚Ä≤
k,‚Ñì=
1
(‚àÜx)2
%
ak‚àí1/2,‚Ñìvk‚àí1,‚Ñì+ ak,‚Ñì‚àí1/2vk,‚Ñì‚àí1 + ak+1/2,‚Ñìvk+1,‚Ñì+ ak,‚Ñì+1/2vk,‚Ñì+1
‚àí(ak‚àí1/2,‚Ñì+ ak,‚Ñì‚àí1/2 + ak+1/2,‚Ñì+ ak,‚Ñì+1/2)vk,‚Ñì
&
+ hk,‚Ñì,
k, ‚Ñì= 1, . . . , d,
where hk,‚Ñìincludes the contribution of the boundary values. (We could have also
added a forcing term without changing the equation materially.) We commence by
6Even in two space dimensions we can obtain small ‚Äì although dense ‚Äì algebraic systems using
spectral methods. If they are too small for your liking, try three dimensions instead.

16.6
Splitting
379
assuming that hk,‚Ñì= 0 for all k, ‚Ñì= 1, 2, . . . , d and, employing natural ordering, write
the method in a vector form,
v‚Ä≤ =
1
(‚àÜx)(Bx + By)v,
t ‚â•0,
v(0) given.
(16.55)
Here Bx and By are d2√ód2 matrices that contain the contribution of the diÔ¨Äerentiation
in the x- and y- variables respectively. In other words, By is a block-diagonal matrix
and its diagonal is constructed from the tridiagonal d √ó d matrices:
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
‚àí(b1/2 + b3/2)
b3/2
0
¬∑ ¬∑ ¬∑
0
b3/2
‚àí(b3/2 + b5/2)
b5/2
p p p
...
0
p p p
p p p
p p p
0
...
p p p
bd‚àí3/2
‚àí(bd‚àí3/2 + bd‚àí1/2)
bd‚àí1/2
0
¬∑ ¬∑ ¬∑
0
bd‚àí1/2
‚àí(bd‚àí1/2 + bd+1/2)
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
,
where b‚Ñì= ak,‚Ñìand k = 1, 2, . . . , d. The matrix Bx contains all the remaining terms. A
crucial observation is that its sparsity pattern is also block-diagonal, with tridiagonal
blocks, provided that the grid is ordered by rows rather than by columns.
Letting vn := v(n‚àÜt), n ‚â•0, the solution of (16.55) can be written explicitly as
vn+1 = e¬µ(Bx+By)vn,
n ‚â•0.
(16.56)
It might be remembered that the exponential of a matrix has been already deÔ¨Åned in
Section 16.2 (see also Exercise 16.4). To solve (16.56) we can discretize the exponential
by means of the Pad¬¥e approximation ÀÜr1/1 (Theorem 4.5). The outcome,
un+1 = ÀÜr1/1(¬µ(Bx+By))un =

I ‚àí1
2¬µ(Bx + By)
‚àí1 
I + 1
2¬µ(Bx + By)

un,
n ‚â•0,
is nothing other than the Crank‚ÄìNicolson method (in two dimensions) in disguise.
Advancing the solution by a single time step is tantamount to solving a linear algebraic
system by use of the matrix I ‚àí1
2¬µ(Bx + By), a task which can be quite expensive,
even with the fast methods of Chapters 13 and 14, when repeated for a large number
of steps.
An exponential, however, is a very special function. In particular, we are all aware
of the identity ez1+z2 = ez1ez2, where z1, z2 ‚ààC. Were this identity true for matrices,
so that
et(Q+S) = etQetS,
t ‚â•0,
(16.57)
for all square matrices Q and S of equal dimension, we could replace Crank‚ÄìNicolson
by
un+1 = ÀÜr1/1(¬µBx)ÀÜr1/1(¬µBy)un
(16.58)
=

I ‚àí1
2¬µBx
‚àí1 
I + 1
2¬µBx
 
I ‚àí1
2¬µBy
‚àí1 
I + 1
2¬µBy

un,
n ‚â•0.

380
The diÔ¨Äusion equation
The implementation of (16.58) would have a great advantage over the unadulter-
ated form of Crank‚ÄìNicolson. We would need to solve two linear systems to advance
one step, but the second matrix, I‚àí1
2¬µBy, is tridiagonal whilst the Ô¨Årst, I‚àí1
2¬µBx, can
be converted into a tridiagonal form by reordering the grid by rows. Hence, (16.58)
could be solved by sparse LU factorization in O

d2
operations!
Unfortunately, in general the identity (16.57) is false. Thus, let [Q, S] := QS ‚àíSQ
be the commutator of Q and S. Since
etQetS ‚àíet(Q+S) =

I + tQ + 1
2t2Q2 + ¬∑ ¬∑ ¬∑
 
I + tS + 1
2t2S2 + ¬∑ ¬∑ ¬∑

(16.59)
‚àí

I + t(Q + S) + 1
2t2(Q + S)2 + ¬∑ ¬∑ ¬∑

= 1
2t2[S, Q] + O

t3
,
we deduce that (16.57) cannot be true unless Q and S commute. If a ‚â°1 and (16.54)
reduces to (16.4) then [Bx, By] = O (see Exercise 16.14) and we are fully justiÔ¨Åed in
using (16.58), but this will not be the case when the diÔ¨Äusion coeÔ¨Écient ¬µ is allowed
to vary.
As with every good policy, the rule that mathematical injunctions must always
be followed has its exceptions.
For instance, were we to disregard for a moment
the breakdown in commutativity and use (16.58) with a variable diÔ¨Äusion coeÔ¨Écient
0
5
10
15
20
25
30
35
40
45
50
0
2
4
6
8 x 10
‚àí3
0
5
10
15
20
25
30
35
40
45
50
0
0.5
1.0
1.5
2.0 x 10
‚àí3
e¬µBxe¬µBy
e¬µBx/2e¬µBye¬µBx/2
‚à•error‚à•
‚à•error‚à•
¬µ
Figure 16.3
The norm of the error in approximating exp ¬µ(Bx + By) by the
‚Äònaive‚Äô splitting e¬µBxe¬µBy and by the Strang splitting e¬µBx/2e¬µBye¬µBx/2 for a(x, y) =
1 + 1
4(x ‚àíy) and d = 10.

Comments and bibliography
381
a, the diÔ¨Äerence would hardly register. The reason is explained by Fig. 16.3, where
we plot (in the upper graph) the error ‚à•e¬µBxe¬µBy ‚àíe¬µ(Bx+By)‚à•for a speciÔ¨Åc variable
diÔ¨Äusion coeÔ¨Écient. Evidently, the loss of commutativity does not necessarily cause a
damaging loss of accuracy. Part of the reason is that, according to (16.59), e¬µBxe¬µBy ‚àí
e¬µ(Bx+By) = O

¬µ2
; but this hardly explains the phenomenon, since we are interested
in large values of ¬µ. Another clue is that, provided Bx, By and Bx + By have their
eigenvalues in the left half-plane, all the exponents vanish for ¬µ ‚Üí‚àû(cf. Section 4.1).
More justiÔ¨Åcation is provided in Exercise 16.16. In any case, the error in the splitting
e¬µBxe¬µBy ‚âàe¬µ(Bx+By)
is suÔ¨Éciently small to justify the use of (16.58) even in the absence of commutativity.
Even better is the Strang splitting
e¬µBx/2e¬µBye¬µBx/2 ‚âàe¬µ(Bx+By).
It can be observed from Fig. 16.3 that it produces a smaller error ‚Äì Exercise 16.15 is
devoted to proving that this is O

¬µ3
.
In general, splitting presents an aÔ¨Äordable alternative to the ‚Äòfull‚Äô Crank‚ÄìNicolson
and the technique can be generalized to other PDEs of evolution and diverse compu-
tational schemes.
We have assumed zero boundary conditions in our exposition, but this is not strictly
necessary. Let us add to (16.54) nonzero boundary conditions and, possibly, a forcing
term. Thus, in place of (16.55) we now have
v‚Ä≤ =
1
(‚àÜx)2 (Bx + By)v + h(t),
t ‚â•0,
v(0) given,
an equation whose explicit solution is
vn+1 = e¬µ(Bx+By)vn + ‚àÜt
 1
0
e(1‚àíœÑ)¬µ(Bx+By)h((n + œÑ)‚àÜt) dœÑ,
n ‚â•0.
We replace the integral using the trapezoidal rule ‚Äì a procedure whose error is within
the same order of magnitude as that of the original SD scheme. The outcome is
Àúvn+1 = e¬µ(Bx+By)Àúvn + 1
2‚àÜt
%
e¬µ(Bx+By)h(n‚àÜt) + h((n + 1)‚àÜt)
&
= e¬µ(Bx+By) Àúvn + 1
2‚àÜt h(n‚àÜt)

+ 1
2‚àÜt h((n + 1)‚àÜ),
n ‚â•0,
and we form an FD scheme by splitting the exponential and approximating it with
the ÀÜr1/1 Pad¬¥e approximation.
Comments and bibliography
Numerical theory for PDEs of evolution is sometimes presented in a deceptively simple way.
On the face of it, nothing could be more straightforward: discretize all spatial derivatives by
Ô¨Ånite diÔ¨Äerences and apply a reputable ODE solver, without paying heed to that fact that,

382
The diÔ¨Äusion equation
actually, one is attempting to solve a PDE. This nonsense has, unfortunately, taken root in
many textbooks and lecture courses, which, not to mince words, propagate shoddy mathe-
matics and poor numerical practice. Reputable literature is surprisingly scarce, considering
the importance and the depth of the subject. The main source and a good reference to much
of the advanced theory is the monograph of Richtmyer & Morton (1967). Other surveys of
Ô¨Ånite diÔ¨Äerences that get stability and convergence right are Gekeler (1984), Hirsch (1988)
and Mitchell & GriÔ¨Éths (1980), while the slim volume of Gottlieb & Orszag (1977), whose
main theme is entirely diÔ¨Äerent, contains a great deal of useful material applicable to the
stability of Ô¨Ånite diÔ¨Äerence schemes for evolutionary PDEs.
Both the eigenvalue approach and the Fourier technique are often ‚Äì and confusingly ‚Äì
termed in the literature ‚Äòthe von Neumann method‚Äô. While paying due homage to John von
Neumann, who originated both techniques, we prefer a more descriptive and less ambiguous
terminology.
Modern stability theory ranges far and wide beyond the exposition of this chapter. A
useful technique, the energy method, will be introduced in Chapter 17. Perhaps the most
signiÔ¨Åcant eÔ¨Äort in generalizing the framework of stability theory has been devoted to bound-
ary conditions in the Fourier technique. This is perhaps more signiÔ¨Åcant in the context of
hyperbolic equations, the theme of Chapter 17; our only remark here is that these are very
deep mathematical waters. The original reference, not for the faint-hearted, is GustaÔ¨Äson et
al. (1972).
Another interesting elaboration on the theme of stability is connected with the Kreiss
matrix theorem, its generalizations and applications (Gottlieb & Orszag, 1977; van Dorsselaer
et al., 1993).
The splitting algorithms of Section 16.6 are a popular means of solving multivariate PDEs
of evolution and they have much in common with the composition methods from Section 5.4.
An alternative, Ô¨Årst pioneered by electrical engineers and subsequently adopted and enhanced
by numerical analysts, is waveform relaxation. Like splitting, it is concerned with eÔ¨Äective
solution of the ODEs that occur in the course of semi-discretization. Let us suppose that an
SD method can be written in the form
v‚Ä≤ =
1
(‚àÜx)2 Pv + h(t),
t ‚â•0,
v(0) = v0,
and that we can express P as a sum of two matrices, P = Q + S, say, such that it is easy
to solve linear ODE systems with the matrix Q; for example, Q might be diagonal (Jacobi
waveform relaxation) or lower triangular (Gauss‚ÄìSeidel waveform relaxation). We replace
the ODE system by the recursion

v[k+1]‚Ä≤ =
1
(‚àÜx)2 Qv[k+1] +
1
(‚àÜx)2 Sv[k] + h(t),
t ‚â•0,
v[k+1] = v0,
k = 0, 1, . . . ,
(16.60)
where v[k+1](0) ‚â°v0. In each kth iteration we apply a standard ODE solver, e.g. a multistep
method, to (16.60) until the procedure converges to our satisfaction.7 This idea might appear
to be very strange indeed ‚Äì to replace a single ODE by an inÔ¨Ånite (in principle) system of such
equations. However, solving the original, unamended, ODE by conversion into an algebraic
system and employing an iterative procedure from Chapters 12‚Äì14 also replaces a single
equation by an inÔ¨Ånite recursion . . .
There exists a respectable theory that predicts the rate of convergence of (16.60) with
k, similar in spirit to the convergence theory from Sections 12.2 and 12.3. An important
7This brief description does little justice to a complicated procedure. For example, for ‚Äòinterme-
diate‚Äô values of k there is no need to solve the implicit equations with high precision, and this leads
to substantial savings (Vandewalle, 1993).

Exercises
383
advantage of waveform relaxation is that it can easily be programmed in a way that takes
full advantage of parallel computer architectures, and it can also be combined with multigrid
techniques (Vandewalle, 1993).
Gekeler, E. (1984), Discretization Methods for Stable Initial Value Problems, Springer-Verlag,
Berlin.
Gottlieb, D. and Orszag, S.A. (1977), Numerical Analysis of Spectral Methods: Theory and
Applications, SIAM, Philadelphia,
Gustafsson, B., Kreiss, H.-O. and Sundstr¬®om, A. (1972), Stability theory of diÔ¨Äerence ap-
proximations for mixed initial boundary value problems, Mathematics of Computation 26,
649‚Äì686.
Hirsch, C. (1988), Numerical Computation of Internal and External Flows, Vol. I: Funda-
mentals of Numerical Discretization, Wiley, Chichester.
Mitchell, A.R. and GriÔ¨Éths, D.F. (1980), The Finite DiÔ¨Äerence Method in Partial DiÔ¨Äerential
Equations, Wiley, London.
Richtmyer, R.D. and Morton, K.W. (1967), DiÔ¨Äerence Methods for Initial-Value Problems,
Interscience, New York.
Vandewalle, S. (1993), Parallel Multigrid Waveform Relaxation for Parabolic Problems, B.G.
Teubner, Stuttgart.
van Dorsselaer, J.L.M., Kraaijevanger, J.F.B.M. and Spijker, M.N. (1993), Linear stability
analysis in the numerical solution of initial value problems, Acta Numerica 2, 199‚Äì237.
Exercises
16.1
Extend the method of proof of Theorem 16.1 to furnish a direct proof that
the Crank‚ÄìNicolson method (16.32) converges.
16.2
Let
un+1
‚Ñì
= un
‚Ñì+ ¬µ(un
‚Ñì‚àí1 ‚àí2un
‚Ñì+ un
‚Ñì+1) ‚àí1
2b¬µ‚àÜx(un
‚Ñì+1 ‚àíun
‚Ñì‚àí1)
be an FD scheme for the convection‚ÄìdiÔ¨Äusion equation
‚àÇu
‚àÇt = ‚àÇ2u
‚àÇx2 ‚àíb‚àÇu
‚àÇx,
0 ‚â§x ‚â§1,
t ‚â•0,
where b > 0 is given.
Prove from Ô¨Årst principles that the method con-
verges. (You can take for granted that the convection‚ÄìdiÔ¨Äusion equation is
well posed.)
16.3
Let
c(t) := ‚à•u( ¬∑ , t)‚à•=
# 1
0
[u(x, t)]2 dx
$1/2
,
t ‚â•0,
be the Euclidean norm of the exact solution of the diÔ¨Äusion equation (16.1)
with zero boundary conditions.

384
The diÔ¨Äusion equation
a Prove that c‚Ä≤(t) ‚â§0, t ‚â•0, hence c(t) ‚â§c(0), t ‚â•0, thereby deducing an
alternative proof that (16.1) is well posed.
b Let un = (un
‚Ñì)d+1
‚Ñì=0 be the Crank‚ÄìNicolson solution (16.32) and deÔ¨Åne
cn := ‚à•un‚à•‚àÜx =

‚àÜx
d+1

‚Ñì=0
|un
‚Ñì|2
1/2
,
n ‚â•0,
as the discretized counterpart of the function c. Demonstrate that
(cn+1)2 = (cn)2 ‚àí‚àÜt
2‚àÜx
d

‚Ñì=1

un+1
‚Ñì
+ un
‚Ñì‚àíun+1
‚Ñì‚àí1 ‚àíun
‚Ñì‚àí1
2 .
Consequently cn ‚â§c0, n ‚â•0, and this furnishes yet another proof that the
Crank‚ÄìNicolson method is stable. (This is an example of the energy method,
which we will encounter again in Chapter 17.)
16.4
The exponential of a d √ó d matrix B is deÔ¨Åned by the Taylor series
eB =
‚àû

k=0
1
k!Bk.
a Prove that the series converges and that
‚à•eB‚à•‚â§e‚à•B‚à•.
(This particular result does not depend on the choice of a norm and you
should be able to prove it directly from the deÔ¨Ånition of the induced matrix
norm in A.1.3.3.)
b Suppose that B = V DV ‚àí1, where V is nonsingular. Prove that
etB = V etDV ‚àí1,
t ‚â•0.
Deduce that, provided B has distinct eigenvalues Œª1, Œª2, . . . , Œªd, there exist
d √ó d matrices E1, E2, . . . , Ed such that
etB =
d

m=1
etŒªmEm,
t ‚â•0.
c Prove that the solution of the linear ODE system
y‚Ä≤ = By,
t ‚â•t0,
y(t0) = y0,
is
y(t) = e(t‚àít0)By0,
t ‚â•0.

Exercises
385
d Generalize the result from c, proving that the explicit solution of
y‚Ä≤ = By + p(t),
t ‚â•t0,
y(t0) = y0,
is
y(t) = e(t‚àít0)By0 +
 t
t0
e(t‚àíœÑ)Bp(œÑ) dœÑ,
t ‚â•t0.
e Let ‚à•¬∑ ‚à•be the Euclidean norm and let B be a normal matrix. Prove that
‚à•etB‚à•‚â§etÀúŒ±(B), t ‚â•0, where ÀúŒ±( ¬∑ ), the spectral abscissa, was deÔ¨Åned in
Section 16.4.
16.5
Prove that the SD method (16.27) is of order 4.
16.6
Suppose that an SD scheme of order p1 for the PDE (16.11) is computed
with an ODE solver of order p2, and that this results in an FD method
(possibly multistep). Show that this method is of order min{p1, rp2}.
16.7
The diÔ¨Äusion equation (16.1) is solved by the fully discretized scheme
un+1
‚Ñì
‚àí1
2(¬µ‚àíŒ∂)

un+1
‚Ñì‚àí1 ‚àí2un+1
‚Ñì
+ un+1
‚Ñì+1

= un
‚Ñì+ 1
2(¬µ+Œ∂)

un
‚Ñì‚àí1 ‚àí2un
‚Ñì+ un
‚Ñì+1

,
(16.61)
where Œ∂ is a given constant. Prove that (16.61) is a second-order method for
all Œ∂ Ã∏= 1
6, while for the choice Œ∂ = 1
6 (the Crandall method) it is of order 4.
16.8
Determine the order of the SD method
v‚Ä≤
‚Ñì=
1
(‚àÜx)2
 11
12v‚Ñì‚àí1 ‚àí5
3v‚Ñì+ 1
2v‚Ñì+1 + 1
3v‚Ñì+2 ‚àí1
12v‚Ñì+3

for the diÔ¨Äusion equation (16.1). Is it stable? (Hint: Express the function
Re a(eiŒ∏) as a cubic polynomial in cos Œ∏.)
16.9
The SD scheme (16.30) for the diÔ¨Äusion equation with a variable coeÔ¨Écient
(16.6) is solved by means of the Euler method.
a Write down the fully discretized equations.
b Prove that the FD method is stable, provided that ¬µ ‚â§1/(2amin), where
amin = min{a(x) : 0 ‚â§x ‚â§1} > 0.
16.10
Let B be a d √ó d normal matrix and let y ‚ààCd be an arbitrary vector such
that ‚à•y‚à•= 1 (in the Euclidean norm).
a Prove that there exist numbers Œ±1, Œ±2, . . . , Œ±d such that y = d
k=1 Œ±kwk,
where w1, w2, . . . , wd are the eigenvectors of B. Express ‚à•y‚à•2 explicitly in
terms of Œ±k, k = 1, 2, . . . , d.
b Let Œª1, Œª2, . . . , Œªd be the eigenvalues of B, Bwk = Œªkwk, k = 1, 2, . . . , d.
Prove that
‚à•By‚à•2 =
d

k=1
|Œ±kŒªk|2.

386
The diÔ¨Äusion equation
c Deduce that ‚à•B‚à•= œÅ(B).
16.11
Apply the Fourier stability technique to the FD scheme
un+1
‚Ñì
= 1
2(2 ‚àí5¬µ + 6¬µ2)un
‚Ñì+ 2
3¬µ(2 ‚àí3¬µ)(un
‚Ñì‚àí1 + un
‚Ñì+1)
‚àí1
12¬µ(1 ‚àí6¬µ)(un
‚Ñì‚àí2 + un
‚Ñì+2),
‚Ñì‚ààZ.
You should Ô¨Ånd that stability occurs if and only if 0 ‚â§¬µ ‚â§2
3. (We have not
speciÔ¨Åed which equation ‚Äì if any ‚Äì the scheme is supposed to solve, but this,
of course, has no bearing on the question of stability.)
16.12
Investigate the stability of the FD scheme (16.61) (see Exercise 16.7) for
diÔ¨Äerent values of Œ∂ using both the eigenvalue and the Fourier technique.
16.13‚ãÜ
Prove that the leapfrog scheme (16.33) for the diÔ¨Äusion equation is unstable
for every choice of ¬µ > 0. (An experienced student of mathematics will not
be surprised to hear that this was the Ô¨Årst-ever discretization method for the
diÔ¨Äusion equation to be published in the scientiÔ¨Åc literature. Sadly, it is still
occasionally used by the unwary ‚Äì those who forget the history of mathematics
are condemned to repeat its mistakes. . . )
16.14‚ãÜ
Prove that the matrices Bx and By from Section 16.6 commute when a ‚â°
constant. (Hint: Employ the techniques from Section 12.1 to factorize these
matrices and demonstrate that they share the same eigenvalues.)
16.15
Prove that
etQ/2etSetQ/2 = et(Q+S) + O

t3
,
t ‚Üí0,
for any d √ó d matrices Q and S, thereby establishing the order of the Strang
splitting.
16.16‚ãÜ
Let E(t) := etQetS, t ‚â•0, where Q and S are d √ó d matrices.
a Prove that
E‚Ä≤ = (Q + S)E + [etQ, S ]etS,
t ‚â•0.
b Using the explicit formula from Exercise 16.4d ‚Äì or otherwise ‚Äì show that
E(t) = et(Q+S) +
 t
0
e(t‚àíœÑ)(Q+S)[eœÑQ, S ]eœÑS dœÑ,
t ‚â•0.
c Let Q, S and Q + S be symmetric negative deÔ¨Ånite matrices. Prove that
‚à•etQetS ‚àíet(Q+S)‚à•‚â§2‚à•S‚à•
 t
0
exp {(t ‚àíœÑ)ÀúŒ±(Q + S) + œÑ[ÀúŒ±(Q) + ÀúŒ±(S)]} dœÑ
for t ‚â•0, where ÀúŒ±( ¬∑ ), the spectral abscissa, was deÔ¨Åned in Section 16.4.
(Hint: Use the estimate from Exercise 16.4e.)

17
Hyperbolic equations
17.1
Why the advection equation?
Much of the discussion in this chapter is centred upon the advection equation
‚àÇu
‚àÇt + ‚àÇu
‚àÇx = 0,
0 ‚â§x ‚â§1,
t ‚â•0,
(17.1)
which is speciÔ¨Åed in tandem with an initial value
u(x, 0) = g(x),
0 ‚â§x ‚â§1,
(17.2)
as well as the boundary condition
u(0, t) = œï0(t),
t ‚â•0,
(17.3)
where g(0) = œï0(0).
The Ô¨Årst and most natural question pertaining to any mathematical construct
should not be ‚Äòhow?‚Äô (the knee-jerk reaction of many a trained mathematical mind)
but ‚Äòwhy?‚Äô.
This is a particularly pointed remark with regard to equation (17.1),
whose exact solution is both well-known and trivial:
u(x, t) =
#
g(x ‚àít),
t ‚â§x,
œï0(t ‚àíx),
x ‚â§t.
(17.4)
Note that (17.4) can be veriÔ¨Åed at once by direct diÔ¨Äerentiation and that it makes
clear why the single boundary condition (17.3) is suÔ¨Écient.
There are three reasons why numerical study of the advection equation (17.1) is of
interest. Firstly, by its very simplicity, it aÔ¨Äords an insight into a multitude of com-
putational phenomena that are speciÔ¨Åc to hyperbolic PDEs. It is a Ô¨Åtting counterpart
of the linear ODE y ‚Ä≤ = Œªy, which was so fruitful in Chapter 4 in elucidating the be-
haviour of ODE solvers for stiÔ¨Äequations. Secondly, various generalizations of (17.1)
lead to PDEs that are crucial in many applications for which in practice we require
numerical solutions: for example the advection equation with a variable coeÔ¨Écient,
‚àÇu
‚àÇt + œÑ(x)‚àÇu
‚àÇx = 0,
0 ‚â§x ‚â§1,
t ‚â•0,
(17.5)
the advection equation in two dimensions,
‚àÇu
‚àÇt + ‚àÇu
‚àÇx + ‚àÇu
‚àÇy = 0,
0 ‚â§x, y ‚â§1,
t ‚â•0,
(17.6)
387

388
Hyperbolic equations
and the wave equation
‚àÇ2u
‚àÇt2 = ‚àÇ2u
‚àÇx2 ,
‚àí1 ‚â§x ‚â§1,
t ‚â•0.
(17.7)
Equations (17.5)‚Äì(17.7) need to be equipped with appropriate initial and boundary
conditions and we will address this problem later. Here we just mention the connec-
tion that takes us from (17.1) to (17.7). Consider two coupled advection equations,
speciÔ¨Åcally
‚àÇu
‚àÇt + ‚àÇv
‚àÇx = 0,
‚àÇv
‚àÇt + ‚àÇu
‚àÇx = 0,
0 ‚â§x ‚â§1,
t ‚â•0.
(17.8)
It follows that
‚àÇ2u
‚àÇt2 = ‚àÇ
‚àÇt
	‚àÇu
‚àÇt

= ‚àÇ
‚àÇt
	
‚àí‚àÇv
‚àÇx

= ‚àí‚àÇ
‚àÇx
	‚àÇv
‚àÇt

= ‚àí‚àÇ
‚àÇx
	
‚àí‚àÇu
‚àÇx

= ‚àÇ2u
‚àÇx2
and so u obeys the wave equation. The system (17.8) is a special case of the vector
advection equation
‚àÇu
‚àÇt + A‚àÇu
‚àÇx = 0,
0 ‚â§x ‚â§1,
t ‚â•0,
(17.9)
where the matrix A is diagonalizable and has only real eigenvalues.
The third, and perhaps the most interesting, reason why the humble advection
equation is so important leads us into the realm of the nonlinear hyperbolic equations
that are pervasive in wave theory and in quantum mechanics, e.g. the Burgers equation
‚àÇu
‚àÇt + 1
2
‚àÇu2
‚àÇx = 0,
‚àí‚àû< x < ‚àû,
t ‚â•0
(17.10)
and the Korteweg‚Äìde-Vries equation
‚àÇu
‚àÇt + Œ∫‚àÇu
‚àÇx + 3Œ∫
4Œ∑
‚àÇu2
‚àÇx + Œ∫Œ∑2
6
‚àÇ3u
‚àÇx3 = 0,
‚àí‚àû< x < ‚àû,
t ‚â•0,
(17.11)
whose name is usually abbreviated to KdV. Both display a wealth of nonlinear phe-
nomena of a kind that we have not encountered previously in this volume.
Figure 17.1 displays the evolution of the solution of the Burgers equation (17.10)
in the interval 0 ‚â§x ‚â§2œÄ with periodic boundary condition u(2œÄ, t) = u(0, t), t ‚â•0.
The initial condition is g(x) = 5
2 +sin x, 0 ‚â§x ‚â§2œÄ and, as t increases from the origin,
g(x) is transported with unit speed to the right ‚Äì as we can expect from the original
advection equation ‚Äì and simultaneously evolves into a function with an increasingly
sharper proÔ¨Åle which, after a while, looks (and is!) discontinuous. The same picture
emerges even more vividly from Fig. 17.2, where six ‚Äòsnapshots‚Äô of the solution are

17.1
Why the advection equation?
389
0
1
2
3
4
5
6
0
0.2
0.4
0.6
0.8
1.0
‚àí1.0
‚àí0.5
0
0.5
1.0
Figure 17.1
The solution of the Burgers equation (17.10) with periodic boundary
conditions and u(x, 0) = 5
2 + sin x, x ‚àà[0, 2œÄ).
0
2
4
6
1.5
2.0
2.5
3.0
0
2
4
6
1.5
2.0
2.5
3.0
0
2
4
6
1.5
2.0
2.5
3.0
0
2
4
6
1.5
2.0
2.5
3.0
0
2
4
6
1.5
2.0
2.5
3.0
0
2
4
6
1.5
2.0
2.5
3.0
‚Üê
t
x
‚Üí
u
t = 0
t = 1
5
t = 2
5
t = 3
5
t = 4
5
t = 1
Figure 17.2
The solution of the Burgers equation from Fig. 17.1 at times t =
i/5, i = 1, 2, . . . , 5.

390
Hyperbolic equations
‚àí1.0
‚àí0.5
0
0.5
1.0
‚àí2
0
2
4
‚àí1.0
‚àí0.5
0
0.5
1.0
‚àí2
0
2
4
‚àí1.0
‚àí0.5
0
0.5
1.0
‚àí2
0
2
4
‚àí1.0
‚àí0.5
0
0.5
1.0
‚àí2
0
2
4
‚àí1.0
‚àí0.5
0
0.5
1.0
‚àí2
0
2
4
‚àí1.0
‚àí0.5
0
0.5
1.0
‚àí2
0
2
4
‚àí1.0
‚àí0.5
0
0.5
1.0
‚àí2
0
2
4
‚àí1.0
‚àí0.5
0
0.5
1.0
‚àí2
0
2
4
t = 1
t = 3
2
t = 2
t = 5
2
t = 3
t = 7
2
t = 4
t = 9
2
Figure 17.3
The solution of the KdV equation (17.11) with Œ∫ =
1
10, Œ∑ = 1
5, an
initial condition g(x) = cos œÄx, x ‚àà[‚àí1, 1), and periodic boundary conditions.
displayed for increasing t. This is a remarkable phenomenon, characteristic of (17.10)
and similar nonlinear hyperbolic conservation laws: a smooth solution degenerates into
a discontinuous one. In Section 17.5 we brieÔ¨Çy explain this behaviour and present a
simple numerical method for the Burgers equation.
Not less intricate is the behaviour of the KdV equation (17.11). It is possible to
show that, for every periodic boundary condition, a nontrivial solution is made up
of a Ô¨Ånite number of active modes. Such modes, which can be described in terms
of Riemann theta functions, interact in a nonlinear fashion. They move at diÔ¨Äerent
speeds and, upon colliding, coalesce yet emerge after a brief delay to resume their
former shape and speed of travel. A ‚ÄòKdV movie‚Äô is displayed in Fig. 17.3, and it
makes this concept more concrete. We will not pursue further the interesting theme
of modelling KdV and other equations with such soliton solutions.
Although ‚Äì hopefully ‚Äì we have argued to the satisfaction of even the most dis-
cerning reader why numerical schemes for the humble advection equation might be

17.1
Why the advection equation?
391
of interest, the task of motivating the present chapter is not yet complete. For, have
we not just spent a whole chapter deliberating in some detail how to discretize evo-
lutionary PDEs by Ô¨Ånite diÔ¨Äerences and discussing questions of stability and imple-
mentation? According to this comforting point of view, we just need to employ Ô¨Ånite
diÔ¨Äerence operators to construct a numerical method, evaluate an eigenvalue or two to
prove stability . . . and the task of computing the solution of (17.1) will be complete.
Nothing could be further from the truth!
To convince a sceptical reader (and all good readers ought to be sceptical!) that
hyperbolic equations require a subtly diÔ¨Äerent approach, we prove a theorem.
Its
statement might sound at Ô¨Årst quite incredible ‚Äì as, of course, it is. Having carefully
studied Chapter 16, the reader should be adequately equipped to verify ‚Äì or reject ‚Äì
the veracity of the following statement.
‚ÄòTheorem‚Äô
1 = 2.
Proof
We construct the simplest possible genuine Ô¨Ånite diÔ¨Äerence method for
(17.1) by replacing the time derivative by forward diÔ¨Äerences and the space derivative
by backward diÔ¨Äerences. The outcome is the Euler scheme
un+1
‚Ñì
= un
‚Ñì‚àí¬µ(un
‚Ñì‚àíun
‚Ñì‚àí1) = ¬µun
‚Ñì‚àí1 + (1 ‚àí¬µ)un
‚Ñì,
‚Ñì= 1, 2, . . . , d,
n ‚â•0, (17.12)
where
¬µ = ‚àÜt
‚àÜx
is the Courant number. Assuming for even greater simplicity that the boundary value
œï0 is identically zero, we pose the question ‚ÄòWhat is the set of all numbers ¬µ that
bring about stability?‚Äô
We address this problem by two diÔ¨Äerent techniques, based on eigenvalue analysis
(Section 16.4) and on Fourier transforms (Section 16.5) respectively. Firstly, we write
(17.12) in the vector form
un+1 = Aun,
n ‚â•0,
where
A =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
1 ‚àí¬µ
0
¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
0
¬µ
1 ‚àí¬µ
...
...
0
...
...
...
...
...
...
¬µ
1 ‚àí¬µ
0
0
¬∑ ¬∑ ¬∑
0
¬µ
1 ‚àí¬µ
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
.
Since A is lower triangular, its eigenvalues all equal 1 ‚àí¬µ. Requiring |1 ‚àí¬µ| ‚â§1 for
stability, we thus deduce that
stability
‚áê‚áí
¬µ ‚àà(0, 2].
(17.13)
Next we turn our attention to the Fourier approach. Multiplying (17.12) by e‚àíi‚ÑìŒ∏
and summing over ‚Ñìwe easily deduce that the Fourier transform obeys the recurrence
ÀÜun+1 = [¬µe‚àíiŒ∏ + (1 ‚àí¬µ)]ÀÜun,
Œ∏ ‚àà[0, 2œÄ],
n ‚â•0.

392
Hyperbolic equations
Therefore, the Fourier stability condition is
|1 ‚àí¬µ(1 ‚àíe‚àíiŒ∏)| ‚â§1,
Œ∏ ‚àà[0, 2œÄ].
Straightforward algebra renders this in the form
1 ‚àí|1 ‚àí¬µ(1 ‚àíe‚àíiŒ∏)|2 = 4¬µ(1 ‚àí¬µ) sin2 1
2Œ∏ ‚â•0,
Œ∏ ‚àà[0, 2œÄ],
hence ¬µ(1 ‚àí¬µ) ‚â•0 and we conclude that
stability
‚áê‚áí
¬µ ‚àà(0, 1].
(17.14)
Comparison of (17.13) with (17.14) proves the assertion of the theorem.
Before we get carried away by the last theorem, it is fair to give the game away
and confess that it is, after all, just a prank. It is a prank with a point, though; more
precisely, three points. Firstly, its ‚Äòproof‚Äô is entirely consistent with several books
of numerical analysis. Secondly, it is the author‚Äôs experience that a fair number of
professional numerical analysts fail to spot exactly what is wrong. Thirdly, although
a rebuttal of the proof should be apparent after a careful reading of Chapter 16, it
aÔ¨Äords us an opportunity to emphasize the very diÔ¨Äerent ground rules that apply for
hyperbolic PDEs and their discretizations.
Which part of the proof is wrong? In principle, both, except that the second part
can be amended with relative ease while the Ô¨Årst rests upon on a blunder, pure and
simple.
The Fourier stability technique from Section 16.5 is based on the assumption that
we are analysing a Cauchy problem: the range of x is the whole real axis and there
are no boundary conditions except for the requirement that the solution is square
integrable. In the proof of the theorem, however, we have stipulated that (17.1) holds
in [0, 1] with zero boundary conditions at x = 0. This can be easily amended and
we can convert this equation into a Cauchy problem without changing the nonzero
portion of the solution of (17.12). To that end let us deÔ¨Åne
un
‚Ñì= 0,
‚Ñì‚ààZ \ {1, 2, . . . , d},
(17.15)
and let the index ‚Ñìin (17.12) range in Z rather than just {1, 2, . . . , d}. We denote the
new solution sequence by Àòun and claim that ‚à•Àòun‚à•‚â•‚à•un‚à•, n ‚â•0. This is obvious
from the following diagram, describing the Ô¨Çow of information in the scheme (17.12).
This diagram also clariÔ¨Åes why only a left-hand side boundary condition is required
to implement this method. We denote by ‚Äò s‚Äô a point that belongs to the original grid
and by ‚Äò c‚Äô any value that we have set to zero, whether as a consequence of letting
œï0 = 0 or of (17.15) or of the FD scheme. Finally, we denote by ‚Äò cq ‚Äô any value of un
‚Ñì
for ‚Ñì‚â•d + 1 that is rendered nonzero by (17.12).
c
c
c
c
c
c
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
c
c
c
c
c
c
c
c
c
q
q
q
q
q
q
c
c
c
q
q
q
‚Üë
‚Üë
‚Üë
‚Üë
‚Üë
‚Üë
‚Üë
‚Üë
‚Üë
‚Üë
‚Üë
‚Üë
‚Üë
‚Üë
‚Üë
‚Üë
‚Üë
‚Üë
‚Üë
‚Üë
‚Üë
‚Üë
‚Üë
‚Üë
‚Üë
‚Üë
‚Üë
‚Üó‚Üó‚Üó‚Üó‚Üó‚Üó
‚Üó‚Üó‚Üó‚Üó‚Üó
‚Üó‚Üó‚Üó‚Üó
‚Üó‚Üó‚Üó‚Üó‚Üó
‚Üó‚Üó‚Üó‚Üó
‚Üó‚Üó‚Üó
‚Ñì= 0
‚Ñì= d
|
|
. . .

17.1
Why the advection equation?
393
The arrows denote the Ô¨Çow of information, which is from each un
‚Ñìto un+1
‚Ñì
and to un
‚Ñì+1
‚Äì and we can see at once that padding u0 with zeros does not introduce any changes
in the solution for ‚Ñì= 1, 2, . . . , d. Therefore, ‚à•Àòun‚à•‚â•‚à•un‚à•for all n ‚â•0 and we have
thereby deduced the stability of the original problem by Fourier analysis.
Before advancing further, we should perhaps use this opportunity to comment
that often it is more natural to solve (17.1) in x ‚àà[0, ‚àû) (or, more speciÔ¨Åcally, in
x ‚àà[0, 1 + t), t ‚â•0), since typically it is of interest to follow the wave-like phenomena
modelled by hyperbolic PDEs throughout their evolution and at all their destinations.
Thus, it is Àòun, rather than un, that should be measured for the purposes of stability
analysis, in which case (17.14) is valid, as an ‚Äòif and only if‚Äô statement, by virtue of
Theorem 16.11.
Unlike (17.14), the stability condition (17.13) is false. Recall from Section 16.4 that
using eigenvalues and spectral radii to prove stability is justiÔ¨Åed only if the underlying
matrix A is normal, which is not so in the present case. It might seem that this clear
injunction should be enough to deter anybody from ‚Äòproving‚Äô stability by eigenvalues.
Unfortunately, most students of numerical analysis are weaned on the diÔ¨Äusion equa-
tion (where all reasonable Ô¨Ånite diÔ¨Äerence schemes are symmetric) and then given a
brief treatment of the wave equation (where, as we will see in Section 17.4, all reason-
able Ô¨Ånite diÔ¨Äerence schemes are skew-symmetric). Sooner or later the limited scope
of eigenvalue analysis is likely to be forgotten . . .
It is easy to convince ourselves that the present matrix A is not normal by verifying
that A‚ä§A Ã∏= AA‚ä§. It is of interest, however, to go back to the theme of Section 16.4 and
see exactly what goes wrong with this matrix. The purpose of stability analysis is to
deduce uniform bounds on norms, while eigenvalue analysis delivers spectral radii. Had
A been normal, it would have been true that œÅ(A) = ‚à•A‚à•and, in greater generality,
œÅ(An) = ‚à•An‚à•. Let us estimate the norm of A = A‚àÜx for (17.12), demonstrating that
it is consistent with the Fourier estimate rather than the eigenvalue estimate.
In greater generality, we let
Sd =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
s
0
¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
0
q
s
...
...
0
...
...
...
...
...
...
q
s
0
0
¬∑ ¬∑ ¬∑
0
q
s
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
be a bidiagonal d √ó d matrix and assume that s, q Ã∏= 0. (Letting s = 1 ‚àí¬µ and q = ¬µ
recovers the matrix A.) To evaluate ‚à•Sd‚à•(in the usual Euclidean norm) we recall
from A.1.5.2 that ‚à•B‚à•= [œÅ(B‚ä§B)]1/2 for any real square matrix B. Let us thus form
the product
S‚ä§
dSd =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
s2 + q2
sq
0
¬∑ ¬∑ ¬∑
0
sq
s2 + q2
sq
...
...
0
...
...
...
0
...
...
sq
s2 + q2
sq
0
¬∑ ¬∑ ¬∑
0
sq
s2
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
.

394
Hyperbolic equations
This is almost a TST matrix ‚Äì just a single rogue element prevents us from applying
Lemma 12.5 to determine its norm! Instead, we take a more roundabout approach.
Firstly, it readily follows from the GerÀásgorin criterion (Lemma 8.3) that
‚à•Sd‚à•2 = œÅ(S‚ä§
d Sd) ‚â§max{s2 + |sq|, s2 + q2 + 2|sq|} = (|s| + |q|)2.
(17.16)
Secondly, set wd,‚Ñì:= (sgn s/q)‚Ñì‚àí1, ‚Ñì= 1, 2, . . . , d and wd = (wd,‚Ñì)d
‚Ñì=1. Since
S‚ä§
d Sdwd =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
s2 + |sq|
(|s| + |q|)2
...
(|s| + |q|)2
s2 + |sq| + q2
‚é§
‚é•‚é•‚é•‚é•‚é•‚é¶
wd = (|s| + |q|)2wd ‚àí
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
wd,1|sq|
0
...
0
q2 + (2 ‚àíwd,1)|sq|
‚é§
‚é•‚é•‚é•‚é•‚é•‚é¶
,
it follows from the deÔ¨Ånition of a matrix norm (A.1.3.4) that
‚à•Sd‚à•2 = ‚à•S‚ä§
d Sd‚à•= max
yÃ∏=0
‚à•S‚ä§
d Sdy‚à•
‚à•y‚à•
‚â•‚à•S‚ä§
d Sdwd‚à•
‚à•wd‚à•
= (|s|+|q|)2+O

d‚àí1/2
,
d ‚Üí‚àû.
Comparison with (17.16) demonstrates that
‚à•Sd‚à•= |s| + |q| + O

d‚àí1/2
,
d ‚Üí‚àû
which, returning to the matrix A, becomes
‚à•A‚à•= |1 ‚àí¬µ| + ¬µ + O

(‚àÜx)1/2
,
‚àÜx ‚Üí0.
Hence |1 ‚àí¬µ| + ¬µ ‚â§1 is suÔ¨Écient for stability, as we have already deduced by Fourier
analysis.
Hopefully, we have made the case that the numerical solution of hyperbolic equa-
tions deserves further elaboration and eÔ¨Äort.
17.2
Finite diÔ¨Äerences for the advection equation
We are concerned with semi-discretizations of the form
v‚Ä≤
‚Ñì+ 1
‚àÜx
Œ≤

k=‚àíŒ±
akvk+‚Ñì= 0
(17.17)
and with fully discretized schemes
Œ¥

k=‚àíŒ≥
bk(¬µ)un+1
‚Ñì+k =
Œ≤

k=‚àíŒ±
ck(¬µ)un
‚Ñì+k,
n ‚â•0,
(17.18)
where
Œ¥

k=‚àíŒ≥
bk(¬µ) ‚â°1

17.2
Finite diÔ¨Äerences for the advection equation
395
(cf. (16.22) and (16.37) respectively), when applied to the advection equation (17.1).
To address the question of stability, we will need to augment (17.1) by boundary
conditions; we plan to devote most of our attention to the Cauchy problem and to
periodic boundary conditions. For the time being we focus on the orders of (17.17)
and of (17.18), a task for which it is not yet necessary to specify the exact range of ‚Ñì.
Theorem 17.1
The SD method (17.17) is of order p if and only if
a(z) :=
Œ≤

k=‚àíŒ±
akzk = ln z + c(z ‚àí1)p+1 + O

|z ‚àí1|p+2
,
z ‚Üí1,
(17.19)
where c Ã∏= 0, while the FD scheme (17.18) is of order p for Courant number ¬µ = ‚àÜt/‚àÜx
if and only if there exists c(¬µ) Ã∏= 0 such that
Àúa(z, ¬µ) :=
Œ≤
k=‚àíŒ± ck(¬µ)zk
Œ¥
k=‚àíŒ≥ bk(¬µ)zk = z‚àí¬µ + c(¬µ)(z ‚àí1)p+1 + O

|z ‚àí1|p+2
,
z ‚Üí1.
(17.20)
Proof
Our analysis is similar to that in Section 16.3 but, if anything, easier.
Letting Àúv‚Ñì(t) := u(‚Ñì‚àÜx, t) and Àúun
‚Ñì:= u(‚Ñì‚àÜx, n‚àÜt) stand for the exact solution at the
grid points, we have
Àúv‚Ä≤
‚Ñì+ 1
‚àÜx
Œ≤

k=‚àíŒ±
akÀúvk+‚Ñì=

Dt + 1
‚àÜx
Œ≤

k=‚àíŒ±
akEk
x

Àúv‚Ñì.
As far as the exact solution of the advection equation is concerned, we have Dt = ‚àíDx
and, by (8.1), Dx = (‚àÜx)‚àí1 ln Ex. Therefore
Àúv‚Ä≤
‚Ñì+ 1
‚àÜx
Œ≤

k=‚àíŒ±
akÀúvk+‚Ñì= ‚àí1
‚àÜx[ln Ex ‚àía(Ex)]v‚Ñì
and we deduce, using a method similar to that in the proof of Theorem 16.5, that
(17.19) is necessary and suÔ¨Écient for order p.
The order condition for FD schemes is based on the same argument and its deriva-
tion proceeds along the lines of Theorem 16.6. Thus, brieÔ¨Çy,
Œ¥

k=‚àíŒ≥
bk(¬µ)Àúun+1
‚Ñì+k ‚àí
Œ≤

k=‚àíŒ±
ck(¬µ)Àúun
‚Ñì+k =
‚é°
‚é£Et
Œ¥

k=‚àíŒ≥
bk(¬µ)Ek
x ‚àí
Œ≤

k=‚àíŒ±
ck(¬µ)Ek
x
‚é§
‚é¶Àúu‚Ñì
while, by (17.1) and Section 8.1,
Et = e(‚àÜt)Dt = e‚àí(‚àÜt)Dx = e‚àí¬µ(‚àÜx)Dx = e‚àí¬µ ln Ex = E‚àí¬µ
x .
Hence
Œ¥

k=‚àíŒ≥
bk(¬µ)Àúun+1
‚Ñì+k ‚àí
Œ≤

k=‚àíŒ±
ck(¬µ)Àúun
‚Ñì+k =
‚é°
‚é£E‚àí¬µ
x
Œ¥

k=‚àíŒ≥
bk(¬µ)Ek
x ‚àí
Œ≤

k=‚àíŒ±
ck(¬µ)Ek
x
‚é§
‚é¶Àúu‚Ñì.

396
Hyperbolic equations
This and the normalization of the denominator of Àúa are now used to complete the
proof that the pth-order condition is indeed (17.20).
3 Examples of methods and their order
It is possible to show that, given
any Œ±, Œ≤ ‚â•0, Œ± + Œ≤ ‚â•1, there exists for the advection equation a unique SD
method of order Œ±+Œ≤ and that no other method may attain this bound. The
coeÔ¨Écients of such a method are not diÔ¨Écult to derive explicitly, a task that
is deferred to Exercise 17.2. Here we present four such schemes for future
consideration. In each case we specify the function a. The schemes are as
follows:
Œ± = 1,
Œ≤ = 0,
a(z) = ‚àíz‚àí1 + 1;
(17.21)
Œ± = 0,
Œ≤ = 1,
a(z) = z ‚àí1;
(17.22)
Œ± = 1,
Œ≤ = 1,
a(z) = ‚àí1
2z‚àí1 + 1
2z;
(17.23)
Œ± = 3,
Œ≤ = 1,
a(z) = ‚àí1
12z‚àí3 + 1
2z‚àí2 ‚àí3
2z‚àí1 + 5
6 + 1
4z.
(17.24)
To demonstrate the power of Theorem 17.1 we address ourselves to the most
complicated method above, (17.24), verifying that its order is indeed 4. Let-
ting z = eiŒ∏, (17.19) becomes equivalent to
a(eiŒ∏) = iŒ∏ + O

Œ∏p+1
,
Œ∏ ‚Üí0.
For (17.24) we have
a(eiŒ∏) = ‚àí1
12e‚àí3iŒ∏ + 1
2e‚àí2iŒ∏ ‚àí3
2e‚àíiŒ∏ + 5
6 + 1
4eiŒ∏
= ‚àí1
12

1 ‚àí3iŒ∏ ‚àí9
2Œ∏2 + 9
2iŒ∏3 + 27
8 Œ∏4
+ 1
2

1 ‚àí2iŒ∏ ‚àí2Œ∏2 + 4
3iŒ∏3 + 2
3Œ∏4
‚àí3
2

1 ‚àíiŒ∏ ‚àí1
2Œ∏2 + 1
6iŒ∏3 + 1
24Œ∏4
+ 5
6
+ 1
4

1 + iŒ∏ ‚àí1
2Œ∏2 ‚àí1
6Œ∏3 + 1
24Œ∏4
+ O

Œ∏5
= iŒ∏ + O

Œ∏5
,
Œ∏ ‚Üí0,
hence the method is of order 4. It is substantially easier to check that the
orders of both (17.21) and (17.22) are 1 and that (17.23) is a second-order
scheme.
As was the case with the diÔ¨Äusion equation in Chapter 16, the easiest tech-
nique in the design of FD schemes is the combination of an SD method with
an ODE solver (typically, of at least the same order, cf. Exercise 16.6). Thus,
pairing (17.21) with the Euler method (1.4) results in the FD scheme (17.12),
which we have already encountered in Section 17.1 in somewhat strange cir-
cumstances. The marriage of (1.4) and (17.22) yields
un+1
‚Ñì
= un
‚Ñì‚àí¬µ(un
‚Ñì+1 ‚àíun
‚Ñì) = (1 + ¬µ)un
‚Ñì‚àí¬µun
‚Ñì+1,
n ‚â•0,
(17.25)
a method that looks very similar to (17.12) ‚Äì but, as we will see later, is quite
diÔ¨Äerent.
The SD scheme (17.23) is of order 2 and we consider two popular schemes
that are obtained when it is combined with second-order ODE schemes. Our
Ô¨Årst example is the Crank‚ÄìNicolson method
‚àí1
4¬µun+1
‚Ñì‚àí1 + un+1
‚Ñì
+ 1
4¬µun+1
‚Ñì+1 = 1
4¬µun
‚Ñì‚àí1 + un
‚Ñì‚àí1
4¬µun
‚Ñì+1,
n ‚â•0,
(17.26)

17.2
Finite diÔ¨Äerences for the advection equation
397
which originates in the trapezoidal rule. Although we can deduce directly
from Exercise 16.6 that it is of order 2, we can also prove it by using (17.20).
To that end, we again exploit the substitution z = eiŒ∏. Since
Àúa(eiŒ∏, ¬µ) =
1
4¬µe‚àíiŒ∏ + 1 ‚àí1
4¬µeiŒ∏
‚àí1
4¬µe‚àíiŒ∏ + 1 + 1
4¬µeiŒ∏ = 1 ‚àí1
2i¬µ sin Œ∏
1 + 1
2i¬µ sin Œ∏
=

1 ‚àí1
2i¬µ sin Œ∏
 
1 ‚àí1
2i¬µ sin Œ∏ ‚àí1
4¬µ2 sin2 Œ∏ + 1
8i¬µ3 sin3 Œ∏ + ¬∑ ¬∑ ¬∑

= 1 ‚àíi¬µŒ∏ ‚àí1
2¬µ2Œ∏2 + 1
12i¬µ(2 + 3¬µ2)Œ∏3 + O

Œ∏4
,
Œ∏ ‚Üí0,
and
e‚àíi¬µŒ∏ = 1 ‚àíi¬µŒ∏ ‚àí1
2¬µ2Œ∏2 + 1
6i¬µ3Œ∏3 + O

Œ∏4
,
Œ∏ ‚Üí0,
we deduce that
Àúa(eiŒ∏, ¬µ) = e‚àíi¬µŒ∏ + 1
12i¬µ(2 + ¬µ2)Œ∏3 + O

Œ∏4
,
Œ∏ ‚Üí0.
Thus, Crank‚ÄìNicolson is a second-order scheme.
Another popular scheme originates when (17.23) is combined with the explicit
midpoint rule from Exercise 2.5. The outcome, the leapfrog method, uses two
steps:
un+2
‚Ñì
= ¬µ(un+1
‚Ñì‚àí1 ‚àíun+1
‚Ñì+1 ) + un
‚Ñì,
n ‚â•0
(17.27)
(cf. (16.33)). Although we have addressed ourselves in Theorem 17.1 to one-
step FD methods, a generalization to two steps is easy: since
e‚àí2i¬µŒ∏ ‚àí¬µ(e‚àíiŒ∏ ‚àíeiŒ∏)e‚àíi¬µŒ∏ ‚àí1 = ‚àí1
3i¬µ(1 ‚àí¬µ2)Œ∏3 + O

Œ∏4
,
Œ∏ ‚Üí0,
this method is also of order 2. Note that the error constant c(¬µ) = ‚àí1
3i¬µ(1 ‚àí
¬µ2) vanishes at ¬µ = 1 and so the method is of superior order for this value of
the Courant number ¬µ. An explanation of this phenomenon is the theme of
Exercise 17.3.
Not all interesting FD methods can be derived easily from semi-discretized
schemes; an example is the angled derivative method
un+2
‚Ñì
= (1 ‚àí2¬µ)(un+1
‚Ñì
‚àíun+1
‚Ñì‚àí1 ) + un
‚Ñì‚àí1,
n ‚â•0.
(17.28)
It can be proved that the method is of order 2, a task that we relegate to Ex-
ercise 17.4. Exercise 17.4 also includes the order analysis of the Lax‚ÄìWendroÔ¨Ä
scheme
un+1
‚Ñì
= 1
2¬µ(1 + ¬µ)un
‚Ñì‚àí1 + (1 ‚àí¬µ2)un
‚Ñì‚àí1
2¬µ(1 ‚àí¬µ)un
‚Ñì+1,
n ‚â•0.
(17.29)
3
Proceeding next to the stability analysis of the advection equation (17.1) and paying
heed to the lesson of the ‚Äòtheorem‚Äô from Section 17.1, we choose not to use eigenvalue
techniques.1 Our standard tool in the remainder of this section is Fourier analysis.
1Eigenvalues retain a marginal role, since some methods yield normal matrices; see Exercise 17.5.

398
Hyperbolic equations
It is of little surprise, thus, that we commence by considering the Cauchy problem,
where the initial condition is given on the whole real line. Not much change is needed
in the theory of Section 16.5 ‚Äì as far as stability analysis is concerned, the exact
identity of the PDE is irrelevant! The one obvious exception is that, since the space
derivative in (17.1) is on the left-hand side, the inequality in the stability condition
for SD schemes needs to be reversed. Without further ado, we thus formulate an
equivalent of Theorems 16.10 and 16.11 appropriate to the current discussion.
Theorem 17.2
The SD method (17.17) is stable (for a Cauchy problem) if and only
if
Re a(eiŒ∏) ‚â•0,
Œ∏ ‚àà[0, 2œÄ],
(17.30)
where the function a is deÔ¨Åned in (17.19). Likewise, the FD method (17.18) is stable
(for a Cauchy problem) for a given Courant number ¬µ ‚ààR if
|Àúa(eiŒ∏, ¬µ)| ‚â§1,
Œ∏ ‚àà[0, 2œÄ];
(17.31)
the function Àúa is deÔ¨Åned in (17.20).
It is easy to observe that (17.21) and (17.23) are stable, while (17.22) is not. This
is an important point, intimately connected to the lack of symmetry in the advection
equation. Since the exact solution is a unilateral shift, each value is transported to
the right at a constant speed. Hence, numerical methods have a privileged direction
and it is popular to choose schemes ‚Äì whether SD or FD ‚Äì that employ more points
to the left than to the right of the current point, a practice known under the name of
upwinding. Both (17.21) and (17.24) are upwind schemes, (17.23) is symmetric and
the downwind scheme (17.22) seeks information at the wrong venue.
Being upwind is not a guarantee of stability, but it certainly helps. Thus, (17.24)
is stable, since
Re a(eiŒ∏) = Re

‚àí1
12e‚àí3iŒ∏ + 1
2e‚àí2iŒ∏ ‚àí3
2e‚àíiŒ∏ + 5
6 + 1
4eiŒ∏
= ‚àí1
12 cos 3Œ∏ + 1
2 cos 2Œ∏ ‚àí5
4 cos Œ∏ + 5
6
= ‚àí1
12(4 cos3 Œ∏ ‚àí3 cos Œ∏) + 1
2(2 cos2 Œ∏ ‚àí1) ‚àí5
4 cos Œ∏ + 5
6
= ‚àí1
3 cos3 Œ∏ + cos2 Œ∏ ‚àícos Œ∏ + 1
3
= 1
3(1 ‚àícos Œ∏)2 ‚â•0,
Œ∏ ‚àà[0, 2œÄ].
Fully discretized schemes lend themselves to Fourier analysis just as easily. We
have already seen that the Euler method (17.12) is stable for ¬µ ‚àà(0, 1]. The outlook
is less promising with regard to the downwind scheme (17.25) and, indeed,
|Àúa(eiŒ∏, ¬µ)|2 = |1 + ¬µ ‚àí¬µeiŒ∏|2 = 1 + 4¬µ(1 + ¬µ) sin2 1
2Œ∏,
Œ∏ ‚àà[0, 2œÄ],
exceeds unity for every Œ∏ Ã∏= 0 or 2œÄ and ¬µ > 0.
Before we discard this method,
however, let us pause for a while and recall the vector equation (17.9). Suppose that
A = V DV ‚àí1, where D is diagonal. The elements along the diagonal of D are real
since, as we have already mentioned, œÉ(A) ‚äÇR, but they might be negative or positive

17.2
Finite diÔ¨Äerences for the advection equation
399
(in particular, in the important case of the wave equation (17.8), one is positive and
the other negative). Letting w(x, t) := V ‚àí1u(x, t), equation (17.9) factorizes into
‚àÇw
‚àÇt + D‚àÇw
‚àÇx = 0,
t ‚â•0,
and hence into
‚àÇwk
‚àÇt + Œªk
‚àÇwk
‚àÇx = 0,
t ‚â•0,
k = 1, 2, . . . , m,
(17.32)
where m is the dimension of u and Œª1, Œª2, . . . , Œªm are the eigenvalues of A (and form
the diagonal of D). A similar transformation can be applied to a numerical method,
replacing un by, say, wn, and it is obvious that the two solution sequences are uni-
formly bounded (or otherwise) in norm for the same values of ¬µ. Let us suppose that
M ‚äÜR is the set of all numbers (positive, negative or zero) such that ¬µ ‚ààM implies
that an FD method is stable for equation (17.1). If we wish to apply this FD scheme
to (17.9), it follows from (17.32) that we require
Œªk¬µ ‚ààM,
k = 1, 2, . . . , m.
(17.33)
We recognize a situation, familiar from Section 4.2, in which the interval M plays a
role similar to the linear stability domain of an ODE solver. In most cases of interest,
M is a closed interval, which we denote by [¬µ‚àí, ¬µ+].
Provided all eigenvalues are positive, (17.33) merely rescales ¬µ by œÅ(A). If they
are all negative, the method (17.25) becomes stable (for appropriate values of ¬µ),
while (17.12) loses its stability. More interesting, though, is the situation, as in (17.8),
when some eigenvalues are positive and others negative since then, unless ¬µ‚àí< 0 and
0 < ¬µ+, no value of ¬µ may coexist with stability. Both (17.12) and (17.21) fail in this
situation, but this is not the case with Crank‚ÄìNicolson, since
|Àúa(eiŒ∏, ¬µ)|2 =

1 ‚àí1
2i sin Œ∏
1 + 1
2i sin Œ∏
 ‚â°1,
Œ∏ ‚àà[0, 2œÄ];
hence we have stability for all ¬µ ‚àà(‚àí‚àû, ‚àû)! Another example is the Lax‚ÄìWendroÔ¨Ä
scheme, whose explicit form confers important advantages in comparison with Crank‚Äì
Nicolson. Since
|Àúa(eiŒ∏, ¬µ)|2 =
 1
2¬µ(1 + ¬µ)e‚àíiŒ∏ + (1 ‚àí¬µ2) ‚àí1
2¬µ(1 ‚àí¬µ)eiŒ∏2
= |1 ‚àí¬µ(1 ‚àícos Œ∏) ‚àíi¬µ sin Œ∏|2
= 1 ‚àí4¬µ2(1 ‚àí¬µ2) sin4 1
2Œ∏,
Œ∏ ‚àà[0, 2œÄ],
we obtain ¬µ‚àí= ‚àí1, ¬µ+ = 1.
Periodic boundary conditions, our next theme, are important in the context of the
wave-like phenomena that are typically described by hyperbolic PDEs. Thus, let us
complement the advection equation (17.1) with, say, the boundary condition
u(0, t) = u(1, t),
t ‚â•0.
(17.34)

400
Hyperbolic equations
The exact solution is no longer (17.4) but is instead periodic in t: the initial condition
is transported to the right with unit speed but, as soon as it disappears through x = 1,
it reappears from the other end; hence u(x, 1) = g(x), 0 ‚â§x ‚â§1.
To emphasize the diÔ¨Äerence between Dirichlet and periodic boundary conditions we
write the Lax‚ÄìWendroÔ¨Äscheme (17.29) in a matrix form, un+1 = Aun, say. Assuming
(zero) Dirichlet conditions, we have
A = A[D] :=
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
1 ‚àí¬µ2
1
2¬µ(¬µ ‚àí1)
0
¬∑ ¬∑ ¬∑
0
1
2¬µ(1 + ¬µ)
1 ‚àí¬µ2
1
2¬µ(¬µ ‚àí1)
...
...
0
...
...
...
0
...
...
1
2¬µ(1 + ¬µ)
1 ‚àí¬µ2
1
2¬µ(¬µ ‚àí1)
0
¬∑ ¬∑ ¬∑
0
1
2¬µ(1 + ¬µ)
1 ‚àí¬µ2
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
,
while periodic boundary conditions yield
A = A[p] :=
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
1 ‚àí¬µ2
1
2¬µ(¬µ ‚àí1)
0
¬∑ ¬∑ ¬∑
0
1
2¬µ(1 + ¬µ)
1
2¬µ(1 + ¬µ)
1 ‚àí¬µ2
1
2¬µ(¬µ ‚àí1)
...
...
0
0
1
2¬µ(1 + ¬µ)
...
...
0
...
...
0
...
...
1
2¬µ(¬µ ‚àí1)
0
0
...
...
1
2¬µ(1 + ¬µ)
1 ‚àí¬µ2
1
2¬µ(¬µ ‚àí1)
1
2¬µ(¬µ ‚àí1)
0
¬∑ ¬∑ ¬∑
0
1
2¬µ(1 + ¬µ)
1 ‚àí¬µ2
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
.
The reason for the discrepancies in the top right-hand and lower left-hand corners is
that, in the presence of periodic boundary conditions, each time we need a value from
outside the set {0, 1, . . . , d ‚àí1} at one end, we borrow it from the other end.2
The diÔ¨Äerence between A[D] and A[p] does seem minor ‚Äì just two entries in what are
likely to be very large matrices. However, as we will see soon, these two matrices could
hardly be more dissimilar in their properties. In particular, while stability analysis
with Dirichlet boundary conditions, a subject to which we will have returned brieÔ¨Çy
by the end of this section, is very intricate, periodic boundary conditions surrender
their secrets much more easily. In fact, we have the unexpected comfort that both
eigenvalue and Fourier analysis are absolutely straightforward in the periodic case!
The matrix A[p] is a special case of a circulant ‚Äì the latter being a d √ó d matrix C
whose jth row, j = 2, 3, . . . , d, is a ‚Äòright-rotated‚Äô (j ‚àí1)th row,
C = C(Œ∫) =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
Œ∫0
Œ∫1
Œ∫2
¬∑ ¬∑ ¬∑
Œ∫d‚àí1
Œ∫d‚àí1
Œ∫0
Œ∫1
¬∑ ¬∑ ¬∑
Œ∫d‚àí2
Œ∫d‚àí2
Œ∫d‚àí1
Œ∫0
¬∑ ¬∑ ¬∑
Œ∫d‚àí3
...
...
...
...
Œ∫1
Œ∫2
Œ∫3
¬∑ ¬∑ ¬∑
Œ∫0
‚é§
‚é•‚é•‚é•‚é•‚é•‚é¶
;
(17.35)
2We have just tacitly adopted the convention that the unknowns in a periodic problem are the
points with spatial coordinates ‚Ñì‚àÜx, ‚Ñì= 0, 1, . . . , d ‚àí1, where ‚àÜx = 1/d. This makes for a somewhat
less unwieldy notation.

17.2
Finite diÔ¨Äerences for the advection equation
401
speciÔ¨Åcally, Œ∫0 = 1 ‚àí¬µ2, Œ∫1 = 1
2¬µ(¬µ ‚àí1), Œ∫2 = ¬∑ ¬∑ ¬∑ = Œ∫d‚àí2 = 0 and Œ∫d‚àí1 = 1
2¬µ(1 + ¬µ).
Lemma 17.3
The eigenvalues of C(Œ∫) are Œ∫(œâj
d), j = 0, 1, . . . , d ‚àí1, where
Œ∫(z) :=
d‚àí1

‚Ñì=0
Œ∫‚Ñìz‚Ñì,
z ‚ààC
and œâd = exp(2œÄi/d) is the dth primitive root of unity. To each Œªj = Œ∫(œâj
d) there
corresponds the eigenvector
wj =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
1
œâj
d
œâ2j
d...
œâ(d‚àí1)j
d
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
,
j = 0, 1, . . . , d ‚àí1.
Proof
We show directly that C(Œ∫)wj = Œªjwj for all j = 0, 1, . . . , d ‚àí1. To that
end we observe that in (17.35) the mth row of C(Œ∫) is
[ Œ∫d‚àím
Œ∫d‚àím+1
¬∑ ¬∑ ¬∑
Œ∫d‚àí1
Œ∫0
Œ∫1
¬∑ ¬∑ ¬∑
Œ∫d‚àím‚àí1 ],
hence the mth component of C(Œ∫)wj is
d‚àí1

‚Ñì=0
cm,‚Ñìwj,‚Ñì=
m‚àí1

‚Ñì=0
Œ∫d‚àím+‚Ñìœâ j‚Ñì
d +
d‚àí1

‚Ñì=m
Œ∫‚Ñì‚àímœâ j‚Ñì
d .
Let us replace the summation indices on the right by ‚Ñì‚Ä≤ = d‚àím+‚Ñì, ‚Ñì= 1, 2, . . . , m‚àí1
and ‚Ñì‚Ä≤ = ‚Ñì‚àím, ‚Ñì= m, m + 1, . . . , d ‚àí1 respectively. Since œâd
d = 1, the outcome
(dropping the prime from the index ‚Ñì‚Ä≤) is
d‚àí1

‚Ñì=0
cm,‚Ñìwj,‚Ñì=
d‚àí1

‚Ñì=d‚àím
Œ∫‚Ñìœâ j(‚Ñì‚àíd+m)
d
+
d‚àí1‚àím

‚Ñì=0
Œ∫‚Ñìœâ j(‚Ñì+m)
d
=
d‚àí1

‚Ñì=0
Œ∫‚Ñìœâ j‚Ñì
d

œâ jm
d
= Œªjwj,m,
m = 0, 1, . . . , d ‚àí1.
We conclude that the wj are indeed eigenvectors corresponding to the eigenvalues
Œ∫(œâj
d), j = 0, 1, . . . , d ‚àí1, respectively.
The lemma has several interesting consequences. For example, since the matrix of
the eigenvectors is exactly the inverse discrete Fourier transform (10.13), the theory
of Section 10.3 demonstrates that multiplying an arbitrary d √ó d circulant by a vector
can be executed very fast by FFT. More interestingly from our point of view, the
eigenvectors of C(Œ∫) do not depend on Œ∫ at all: all d √ó d circulants share the same
eigenvectors, hence all such matrices commute.

402
Hyperbolic equations
The matrix of eigenvectors,
[ w0
w1
¬∑ ¬∑ ¬∑
wd‚àí1 ],
is unitary since, trivially,
‚ü®wj, w‚Ñì‚ü©= ¬Øw‚ä§
j w‚Ñì= 0,
j, ‚Ñì= 0, 1, . . . , d ‚àí1,
j Ã∏= ‚Ñì.
Therefore every circulant is normal (A.1.2.5). An alternative proof is left to Exer-
cise 17.9. As we already know from Theorems 16.7 and 16.8, the stability of Ô¨Ånite
diÔ¨Äerence schemes with normal matrices can be completely speciÔ¨Åed in terms of the
eigenvalues of the latter. Since these eigenvalues were fully described in Lemma 17.3,
stability analysis becomes almost as easy as painting by numbers. Thus, for Lax‚Äì
WendroÔ¨Ä,
Œ∫0 = 1 ‚àí¬µ2,
Œ∫1 = 1
2¬µ(¬µ ‚àí1),
Œ∫d‚àí1 = 1
2¬µ(¬µ + 1)
‚áí
Œªj = (1 ‚àí¬µ2) + 1
2¬µ(¬µ ‚àí1)œâ j
d + 1
2¬µ(¬µ + 1)œâ(d‚àí1)j
d
= (1 ‚àí¬µ2) + 1
2¬µ(¬µ ‚àí1) exp(2œÄij/d) + 1
2¬µ(¬µ + 1) exp(‚àí2œÄij/d)
= Àúa(exp(2œÄij/d), ¬µ),
j = 0, . . . , d ‚àí1,
where Àúa has been already encountered in the context of both order and Fourier stability
analysis.
There is nothing special about the Lax‚ÄìWendroÔ¨Äscheme; it is the presence of
periodic boundary conditions that makes the diÔ¨Äerence. The identity
Œªj = Àúa(exp(2œÄij/d), ¬µ),
j = 0, 1, . . . , d ‚àí1,
is valid for all FD methods (17.18). Letting d ‚Üí‚àû, we can now use Theorem 16.7
(or a similar analysis, in tandem with Theorem 16.8, in the case of SD schemes) to
extend the scope of Theorem 17.2 to the realm of periodic boundary conditions.
Theorem 17.4
Let us assume the periodic boundary conditions (17.34). The SD
method (17.17) is stable subject to the inequality (17.30), and the FD scheme (17.18)
is stable subject to the inequality (17.31).
An alternative route to Theorem 17.4 proceeds via Fourier analysis with a discrete
Fourier transform (DFT). It is identical in both content and consequences; as far as
circulants are concerned, the main diÔ¨Äerence between Fourier and eigenvalue analysis
is just a matter of terminology.
The stability analysis for a Dirichlet boundary problem is considerably more com-
plicated and the conditions of Theorem 17.2, say, are necessary but often far from
suÔ¨Écient. The Euler method (17.12) is a double exception. Firstly, the Fourier con-
ditions are both necessary and suÔ¨Écient for stability. Secondly, the statement in the
previous sentence can be proved by elementary means (cf. Section 17.1). In general,
even if (17.30) or (17.31) are suÔ¨Écient to attain stability, the proof is far from elemen-
tary.

17.3
The energy method
403
Let us consider the solution of (17.1) with the initial condition g(x) = sin 8œÄx,
x ‚àà[0, 1], and the Dirichlet boundary condition œï0(t) = ‚àísin 8œÄt, t ‚â•0. The exact
solution, according to (17.4), is u(x, t) = sin 8œÄ(x ‚àít), x ‚àà[0, 1], t ‚â•0.
To illustrate the diÔ¨Éculty of stability analysis in the presence of the Dirichlet
boundary conditions, we now solve this equation with the leapfrog method (17.27),
evaluating the Ô¨Årst step with the Lax‚ÄìWendroÔ¨Äscheme (17.29). However, in attempt-
ing to implement the leapfrog method, it is soon realized that a vital item of data is
missing: since there is no boundary condition at x = 1, (17.27) cannot be executed
for ‚Ñì= d. So, let us simply substitute
un+1
d
= 0,
n ‚â•0,
(17.36)
which seems a safe bet ‚Äì what could be more stable than zero?! Figure 17.4 displays
the solution for d = 40 and d = 80 in the interval t ‚àà[0, 6] and it is quite apparent that
it looks nothing like the expected sinusoidal curve. Worse, the solution deteriorates
when the grid is reÔ¨Åned, a hallmark of instability.
The mechanism that causes instability and deterioration of the solution is indeed
the rogue ‚Äòboundary scheme‚Äô (17.36). This perhaps becomes more evident upon an
examination of Fig. 17.5, which displays snapshots of un for time intervals of equal
length. The oscillatory overlay on the (correct) sinusoidal curve at t = 0.5 gives the
game away: the substitution (17.36) allows for increasing oscillations that enter the
interval [0, 1] at x = 1 and travel leftwards, in the wrong direction.
This amazing sensitivity to the choice of just one point (whose value does not
inÔ¨Çuence at all the exact solution in [0, 1)) is further emphasized in Fig. 17.6, where
the very same leapfrog has been used to solve an identical equation, except that, in
place of (17.36), we have used
un+1
d
= un
d‚àí1,
n ‚â•0.
(17.37)
Like Wordsworth‚Äôs ‚ÄòDaÔ¨Äodils‚Äô, the sinusoidal curves of Fig. 17.6 are ‚Äòstretch‚Äôd in a
never-ending line‚Äô, perfectly aligned and stable. This is already apparent from a cur-
sory inspection of the solution, while the four ‚Äòsnapshots‚Äô are fully consistent with the
numerical error of O

(‚àÜx)‚àí2
that is to be expected from a second-order convergent
scheme.
The general rules governing stability in the presence of boundaries are far too com-
plicated for an introductory text; they require sophisticated mathematical machinery.
Our simple example demonstrates that adding boundaries is a genuine issue, not sim-
ply a matter of mathematical nitpicking, and that a wrong choice of a ‚Äòboundary Ô¨Åx‚Äô
might well corrupt a stable scheme.
17.3
The energy method
Both the eigenvalue technique and Fourier analysis are, as should have been amply
demonstrated, of limited scope. Sooner or later ‚Äì sooner if we set our mind on solving
nonlinear PDEs ‚Äì we are bound to come across a numerical scheme that deÔ¨Åes both
methods. The one means left is the recourse of the desperate, the energy method. The

404
Hyperbolic equations
0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
‚àí5
0
5
0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
‚àí5
0
5
d = 40
d = 80
‚Üê
t
x
‚Üí
‚Üê
t
x
‚Üí
Figure 17.4
A leapfrog solution of (17.1) with Dirichlet boundary conditions.
0
0.5
1.0
‚àí5
0
5
0
0.5
1.0
‚àí5
0
5
0
0.5
1.0
‚àí5
0
5
0
0.5
1.0
‚àí5
0
5
0
0.5
1.0
‚àí5
0
5
0
0.5
1.0
‚àí5
0
5
0
0.5
1.0
‚àí5
0
5
0
0.5
1.0
‚àí5
0
5
t = 0.5
t = 1.2
t = 1.9
t = 2.6
t = 3.3
t = 4.0
t = 4.7
t = 5.4
Figure 17.5
Evolution of un for the leapfrog method with d = 80.

17.3
The energy method
405
0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
‚àí1
0
1
0
0.5
1.0
‚àí1
0
1
0
0.5
1.0
‚àí1
0
1
0
0.5
1.0
‚àí1
0
1
0
0.5
1.0
‚àí1
0
1
d = 80
‚Üê
t
x
‚Üí
t = 1.2
t = 2.6
t = 4.0
t = 5.4
Figure 17.6
A leapfrog solution of (17.1) with the stable artiÔ¨Åcial boundary
condition (17.37).
truth of the matter is that, far from being a coherent technique, the energy method is
essentially a brute force approach toward proving the stability conditions (16.16) or
(16.20) by direct manipulation of the underlying scheme.
We demonstrate the energy method by a single example, namely numerical solution
of the variable-coeÔ¨Écient advection equation (17.5), with zero boundary condition
œï0 ‚â°0, by the SD scheme
v‚Ä≤
‚Ñì=
œÑ‚Ñì
2‚àÜx(vn
‚Ñì‚àí1 ‚àívn
‚Ñì+1),
‚Ñì= 1, 2, . . . , d,
t ‚â•0,
(17.38)
where œÑ‚Ñì:= œÑ(‚Ñì‚àÜx), ‚Ñì= 1, 2, . . . , d. Being a generalization of (17.23), we note that
this method is of order 2, but our current goal is to investigate its stability.
Fourier analysis is out of the question; the whole point about this technique is
that it requires exactly the same diÔ¨Äerence scheme at every ‚Ñì, and a variable function
œÑ‚Ñìrenders this impossible. It takes more eÔ¨Äort to demonstrate that the eigenvalue
technique is not up to the task either. The matrix of the SD system (17.38) is
P‚àÜx =
1
2‚àÜx
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
0
‚àíœÑ1
0
¬∑ ¬∑ ¬∑
0
œÑ2
0
‚àíœÑ2
...
...
0
...
...
...
0
...
...
œÑd‚àí1
0
‚àíœÑd‚àí1
0
¬∑ ¬∑ ¬∑
0
œÑd
0
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
,

406
Hyperbolic equations
where (d + 1)‚àÜx = 1.
It is an easy yet tedious task to prove that, subject to œÑ
being twice diÔ¨Äerentiable, the matrix P‚àÜx cannot be normal for ‚àÜx ‚Üí0 unless œÑ is
a constant. The proof is devoid of intrinsic interest and has no connection with our
discussion; hence let us, without further ado, take this result for granted. It means
that we cannot use eigenvalues to deduce stability.
Let us assume that the function œÑ obeys the Lipschitz condition
|œÑ(x) ‚àíœÑ(y)| ‚â§Œª|x ‚àíy|,
x, y ‚àà[0, 1],
(17.39)
for some constant Œª ‚â•0. Recall from Chapter 16 that we are measuring the magnitude
of v‚àÜx in the Euclidean norm
‚à•w‚àÜx‚à•‚àÜx =

‚àÜx
d

‚Ñì=1
w2
‚Ñì
1/2
.
DiÔ¨Äerentiating ‚à•v‚àÜx‚à•2
‚àÜx yields
d
dt‚à•v‚àÜx‚à•2
‚àÜx = ‚àÜx d
dt
d

‚Ñì=1
v2
‚Ñì= 2‚àÜx
d

‚Ñì=1
v‚Ñìv‚Ä≤
‚Ñì.
Substituting the value of v‚Ñìfrom the SD equations (17.38) and changing the order of
summation, we obtain
d
dt‚à•v‚àÜx‚à•2
‚àÜx =
d

‚Ñì=1
œÑ‚Ñìv‚Ñì(v‚Ñì‚àí1 ‚àív‚Ñì+1) =
d‚àí1

‚Ñì=0
œÑ‚Ñì+1v‚Ñìv‚Ñì+1 ‚àí
d

‚Ñì=1
œÑ‚Ñìv‚Ñìv‚Ñì+1
=
d

‚Ñì=1
(œÑ‚Ñì+1 ‚àíœÑ‚Ñì)v‚Ñìv‚Ñì+1.
Note that we have used the zero boundary condition v0 = 0. Observe next that the
Lipschitz condition (17.39) implies
|œÑ‚Ñì+1 ‚àíœÑ‚Ñì| = |œÑ((‚Ñì+ 1)‚àÜx) ‚àíœÑ(‚Ñì‚àÜx)| ‚â§Œª‚àÜx;
therefore
d
dt‚à•v‚àÜx‚à•2
‚àÜx ‚â§

d

‚Ñì=1
(œÑ‚Ñì+1 ‚àíœÑ‚Ñì)v‚Ñìv‚Ñì+1
 ‚â§
d

‚Ñì=1
|œÑ‚Ñì+1 ‚àíœÑ‚Ñì| |v‚Ñìv‚Ñì+1| ‚â§Œª‚àÜx
d

‚Ñì=1
|v‚Ñìv‚Ñì+1|.
Finally, we resort to the Cauchy‚ÄìSchwarz inequality (A.1.3.1) to deduce that
d
dt‚à•v‚àÜx‚à•2
‚àÜx ‚â§Œª‚àÜx

d

‚Ñì=1
v2
‚Ñì
1/2
d

‚Ñì=1
v2
‚Ñì+1
1/2
‚â§Œª‚à•v‚àÜx‚à•2
‚àÜx.
(17.40)
It follows at once from (17.40) that
‚à•v‚àÜx(t)‚à•2
‚àÜx ‚â§eŒªt‚à•v‚àÜx(0)‚à•2
‚àÜx,
t ‚àà[0, t‚àó].

17.4
The wave equation
407
Since
lim
‚àÜx‚Üí0 ‚à•v‚àÜx(0)‚à•‚àÜx = ‚à•g‚à•< ‚àû,
where g is the initial condition (recall our remark on Riemann sums in Section 16.2!), it
is possible to bound ‚à•v‚àÜx(0)‚à•‚àÜx ‚â§c, say, uniformly for suÔ¨Éciently small ‚àÜx, thereby
deducing the inequality
‚à•v‚àÜx(t)‚à•2
‚àÜx ‚â§c2eŒªt‚àó,
t ‚àà[0, t‚àó].
This is precisely what is required for (16.20), the deÔ¨Ånition of stability for SD schemes,
and we thus deduce that the scheme (17.38) is stable.
17.4
The wave equation
As we have already noted in Section 17.1, the wave equation can be expressed as a
system of advection equations (17.8). At least in principle, this enables us to exploit
the theory of Sections 17.2 and 17.3 to produce Ô¨Ånite diÔ¨Äerence schemes for the wave
equation. Unfortunately, we soon encounter two practical snags. Firstly, the wave
equation is equipped with two initial conditions, namely
u(x, 0) = g0(x),
‚àÇu(x, 0)
‚àÇt
= g1(x),
0 ‚â§x ‚â§1,
(17.41)
and, typically, two Dirichlet boundary conditions,
u(0, t) = œï0(t),
u(1, t) = œï1(t),
t ‚â•0
(17.42)
(we will return later to the matter of boundary conditions). Rendering (17.41) and
(17.42) in the terminology of a vector advection equation (17.9) makes for strange-
looking conditions that needlessly complicate the exposition. The second diÔ¨Éculty
comes to light as soon as we attempt to generalize the SD method (17.23), say, to
cater for the system (17.9). On the face of it, nothing could be easier: just replace
(‚àÜx)‚àí1 by (‚àÜx)‚àí1A, thereby obtaining
v‚Ä≤
‚Ñì+
1
2‚àÜxA(v‚Ñì+1 ‚àív‚Ñì‚àí1) = 0.
(17.43)
This is entirely reasonable so far as a general matrix A is concerned. However, choosing
A =

0
1
1
0

converts (17.43) into
v‚Ä≤
1,‚Ñì= ‚àí
1
2‚àÜx(v2,‚Ñì+1 ‚àív2,‚Ñì‚àí1),
v‚Ä≤
2,‚Ñì= ‚àí
1
2‚àÜx(v1,‚Ñì+1 ‚àív1,‚Ñì‚àí1).

408
Hyperbolic equations
According to Section 17.1, v‚Ñì:= v1,‚Ñìapproximates the solution of the wave equation.
Further diÔ¨Äerentiation helps us to eliminate the second coordinate,
v‚Ä≤‚Ä≤
‚Ñì= ‚àí
1
2‚àÜx(v‚Ä≤
2,‚Ñì+1 ‚àív‚Ä≤
2,‚Ñì‚àí1) = ‚àí
1
2‚àÜx

‚àí
1
2‚àÜx(v‚Ñì+2 ‚àív‚Ñì) +
1
2‚àÜx(v‚Ñì‚àív‚Ñì‚àí2)

,
and results in the SD scheme
v‚Ä≤‚Ä≤
‚Ñì=
1
4(‚àÜx)2 (v‚Ñì‚àí2 ‚àí2v‚Ñì+ v‚Ñì+2).
(17.44)
Although (17.44) is a second-order scheme, it makes very little sense. There is
absolutely no good reason to make v‚Ä≤‚Ä≤
‚Ñìdepend on v‚Ñì¬±2 rather than on v‚Ñì¬±1 and we
have at least one powerful incentive for the latter course ‚Äì it is likely to make the
numerical error signiÔ¨Åcantly smaller. A simple trick can sort this out: replace (17.43)
by the formal scheme
v‚Ä≤
‚Ñì+ 1
‚àÜxA(v‚Ñì+1/2 ‚àív‚Ñì‚àí1/2).
In general this is nonsensical, but for the present matrix A the outcome is the SD
scheme
v‚Ä≤‚Ä≤
‚Ñì=
1
(‚àÜx)2 (v‚Ñì‚àí1 ‚àí2v‚Ñì+ v‚Ñì+1),
(17.45)
which is of exactly the right form.
An alternative route leading to (17.45) is to
discretize the second spatial derivative using central diÔ¨Äerences, exactly as we did in
Chapters 8 and 15.
Choosing to follow the path of analytic expansion, along the lines of Sections 16.3
and 17.2, we consider the general SD method
v‚Ä≤‚Ä≤
‚Ñì=
1
(‚àÜx)2
Œ≤

k=‚àíŒ±
akv‚Ñì+k.
(17.46)
The only diÔ¨Äerence from (16.22) and (17.7) is that this scheme possesses a second time
derivative, hence being of the right form to satisfy both the initial conditions (17.41).
Letting Àúv‚Ñì(t) := u(‚Ñì‚àÜx, t), t ‚â•0, and engaging without any further ado in the
already familiar calculus of Ô¨Ånite diÔ¨Äerence operators, we deduce from (17.8) that
Àúv‚Ä≤‚Ä≤
‚Ñì‚àí
1
(‚àÜx)2
Œ≤

k=‚àíŒ±
akÀúv‚Ñì+k =
1
(‚àÜx)2

(ln Ex)2 ‚àí
Œ≤

k=‚àíŒ±
akEk
x

Àúv‚Ñì.
Therefore (17.46) is of order p if and only if
a(z) :=
Œ≤

k=‚àíŒ±
akzk = (ln z)2 + c(z ‚àí1)p+2 + O

|z ‚àí1|p+3
,
z ‚Üí1,
(17.47)
for some c Ã∏= 0.

17.4
The wave equation
409
It is an easy matter to verify that both (17.44) and (17.45) are of order 2. A little
more eÔ¨Äort is required to demonstrate that the SD scheme
v‚Ä≤‚Ä≤
‚Ñì=
1
(‚àÜx)2

‚àí1
12v‚Ñì‚àí2 + 4
3v‚Ñì‚àí1 ‚àí5
2v‚Ñì+ 4
3v‚Ñì+1 ‚àí1
12v‚Ñì+2

is fourth order.
Our next step consists of discretizing the ODE system (17.46) and it aÔ¨Äords us
an opportunity to discuss the numerical solution of second-order ODEs with greater
generality. Consider thus the equations
z‚Ä≤‚Ä≤ = f(t, z),
t ‚â•t0,
z(t0) = z0,
z‚Ä≤(t0) = z‚Ä≤
0,
(17.48)
where f is a given function. Note that we can easily cast the semi-discretized scheme
(17.46) in this form.
The easiest way of solving (17.48) numerically is to convert it into a Ô¨Årst-order
system having twice the number of variables. It can be veriÔ¨Åed at once that, subject
to the substitution y1(t) := z(t), y2(t) := z‚Ä≤(t), t ‚â•t0, (17.48) is equivalent to the
ODE system
y‚Ä≤
1 = y2,
y‚Ä≤
2 = f(t, y1),
t ‚â•t0,
(17.49)
with the initial condition
y1(t0) = z0,
y2(t0) = z‚Ä≤
0.
On the face of it, we may choose any ODE scheme from Chapters 1‚Äì3 and apply it to
(17.49). The outcome can be surprising . . .
Suppose, thus, that (17.49) is solved with Euler‚Äôs method (1.4), hence, in the
notation of Chapters 1‚Äì7, we have
y1,n+1 = y1,n + hy2,n,
y2,n+1 = y2,n + hf(tn, y1,n),
n ‚â•0.
According to the Ô¨Årst equation,
y2,n = 1
h(y1,n+1 ‚àíy1,n).
Substituting this twice (once with n and once with n + 1) into the second equation
allows us to eliminate y2,n altogether:
1
h(y1,n+2 ‚àíy1,n+1) = 1
h(y1,n+1 ‚àíy1,n) + hf(tn, y1,n).
The outcome is the two-step explicit method
zn+2 ‚àí2zn+1 + zn = h2f(tn, zn),
n ‚â•0.
(17.50)

410
Hyperbolic equations
A considerably cleverer approach is to solve the Ô¨Årst set of equations in (17.49)
with the backward Euler method (1.15), while retaining the usual Euler method for
the second set. This yields
y1,n+1 = y1,n + hy2,n+1,
y2,n+1 = y2,n + hf(tn, y1,n),
n ‚â•0.
Substitution of
y2,n+1 = 1
h(y1,n+1 ‚àíy1,n)
in the second equation and a shift in the index results in the St¬®ormer method
zn+2 ‚àí2zn+1 + zn = h2f(tn+1, zn+1),
n ‚â•0,
(17.51)
which we encountered in a diÔ¨Äerent context in (5.26). Although we have used the
backward Euler method in its construction, the St¬®ormer method is explicit.
A numerical method for the ODE (17.49) is of order p if substitution of the exact
solution results in a perturbation of O

hp+2
. As can be expected, the method (17.50)
is of order 1 ‚Äì after all, it is nothing other than the Euler scheme. However, as far
as (17.51) is concerned, symmetry and the subtle interplay between the forward and
backward Euler methods mean that its order is increased and its performance improved
signiÔ¨Åcantly in comparison with what we might have naively expected. Let Àúzn = z(tn),
n ‚â•0, be the exact solution of (17.48). Substitution of this into (17.51), expansion
about the point tn+1 and the diÔ¨Äerential equation (17.48) yield
Àúzn+2 ‚àí2Àúzn+1 + Àúzn ‚àíh2f(tn+1, Àúzn+1)
=
Àúzn+1 + hÀúz‚Ä≤
n+1 + 1
2h2Àúz‚Ä≤‚Ä≤
n+1 + 1
6h3Àúz‚Ä≤‚Ä≤‚Ä≤
n+1 + O

h4
‚àí2Àúzn+1
+
Àúzn+1 ‚àíhÀúz‚Ä≤
n+1 + 1
2h2Àúz‚Ä≤‚Ä≤
n+1 ‚àí1
6h3Àúz‚Ä≤‚Ä≤‚Ä≤
n+1 + O

h4
‚àíh2Àúz‚Ä≤‚Ä≤
n+1
= O

h4
,
and thus we see that the St¬®ormer method is of order 2. Both the methods (17.50)
and (17.51) are two-step and explicit. Their implementation is likely to entail a very
similar expense. Yet, (17.51) is of order 2, while (17.50) is just Ô¨Årst-order ‚Äì yet another
example of a free lunch in numerical mathematics.3
Applying St¬®ormer‚Äôs method (17.51) to the semi-discretized scheme (17.45) results
in the following two-step FD recursion, the leapfrog method,
un+2
‚Ñì
‚àí2un+1
‚Ñì
+ un
‚Ñì= ¬µ2(un+1
‚Ñì‚àí1 ‚àí2un+1
‚Ñì
+ un+1
‚Ñì+1 ),
n ‚â•0,
(17.52)
where ¬µ = ‚àÜt/‚àÜx.
Being composed of a second-order space discretization and a second-order approx-
imation in time, (17.52) is itself a second-order method. To analyse its stability we
commence by assuming a Cauchy problem and proceeding as in our investigation of the
Adams‚ÄìBashforth-like method (16.52) in Section 16.5. Relocating to Fourier space,
straightforward manipulation results in
ÀÜun+2 ‚àí2(1 ‚àí2¬µ2 sin2 1
2Œ∏)ÀÜun+1 + ÀÜun = 0,
Œ∏ ‚àà[0, 2œÄ].
3To add insult to injury, (17.50) leads to an unstable FD scheme for all ¬µ > 0 (see Exercise 17.12).

17.4
The wave equation
411
All solutions of this three-term recurrence are uniformly bounded (and the underlying
leapfrog method is stable) if and only if the zeros of the quadratic
œâ2 ‚àí2(1 ‚àí2¬µ2 sin2 1
2Œ∏)œâ + 1 = 0
both reside in the closed unit disc for all Œ∏ ‚àà[0, 2œÄ]. Although we could now use
Lemma 8.3, it is perhaps easier to write the zeros explicitly,
œâ¬± = 1 ‚àí2¬µ sin2 1
2Œ∏ ¬± 2i¬µ sin 1
2Œ∏

1 ‚àí¬µ2 sin2 1
2Œ∏
1/2 .
Provided 0 < ¬µ ‚â§1, both œâ+ and œâ‚àíare of unit modulus, while if ¬µ exceeds unity
then so does the magnitude of one of the zeros. We deduce that the leapfrog method is
stable for all 0 < ¬µ ‚â§1 insofar as the Cauchy problem is concerned. However, as in the
Euler method for the advection equation in Section 17.1, we are allowed to infer from
Cauchy to Dirichlet. For, suppose that the wave equation (17.7) is given for 0 ‚â§x ‚â§1
with zero boundary conditions œï0, œï1 ‚â°0. Since the leapfrog scheme (17.52) is explicit
and each un+2
‚Ñì
is coupled to just the nearest neighbours on the spatial grid, it follows
that, as long as un
0, un
d+1 ‚â°0, n ‚â•0, we can assign arbitrary values to un
‚Ñìfor ‚Ñì‚â§‚àí1
and ‚Ñì‚â•d + 1 without any inÔ¨Çuence upon un
1, un
2, . . . , un
d. In particular, we can embed
a Dirichlet problem into a Cauchy one by padding with zeros, without amending the
Euclidean norm of un. Thus we have stability in a Dirichlet setting for 0 < ¬µ ‚â§1.
To launch the Ô¨Årst iteration of the leapfrog method we Ô¨Årst need to derive the
vector u1 by other means. Recall that both u and ‚àÇu/‚àÇt are speciÔ¨Åed along x = 0,
and this can be exploited in the derivation of a second-order approximation at t = ‚àÜt.
Let Àòu0
‚Ñì= g1(‚Ñì‚àÜx), ‚Ñì= 1, 2, . . . , d. Expanding about (‚Ñì‚àÜx, 0) in a Taylor series, we
have
u(‚Ñì‚àÜx, ‚àÜt) = u(‚Ñì‚àÜx, 0) + ‚àÜt‚àÇu(‚Ñì‚àÜx, 0)
‚àÇt
+ 1
2(‚àÜt)2 ‚àÇ2u(‚Ñì‚àÜx, 0)
‚àÇt2
+ O

(‚àÜt)3
.
Substituting initial values and the second derivative from the SD scheme (17.45) results
in
u1
‚Ñì= u0
‚Ñì+ (‚àÜt)Àòu0
‚Ñì+ 1
2¬µ2(u0
‚Ñì‚àí1 ‚àí2u0
‚Ñì+ u0
‚Ñì+1),
‚Ñì= 1, 2, . . . , d,
(17.53)
a scheme whose order is consistent with the leapfrog method (17.52).
The Dirichlet boundary conditions (17.42) are not the only interesting means
for determining the solution of the wave equation.
It is a well-known peculiar-
ity of hyperbolic diÔ¨Äerential equations that, for every t0 ‚â•0, an initial condition
along an interval [x‚àí, x+], say, determines uniquely the solution for all (x, t) in a set
Dt0
x‚àí,x+ ‚äÇR √ó [t0, ‚àû), the domain of dependence. For example, it is easy to deduce
from (17.4) that the domain of dependence of the advection equation is the parallelo-
gram
Dt0
x‚àí,x+ = {(x, t) : t ‚â•t0, x‚àí+ t ‚àít0 ‚â§x ‚â§x+ + t ‚àít0 }.
The domain of dependence of the wave equation, also known as the Monge cone, is
the triangle
Dt0
x‚àí,x+ =

(x, t) : t0 ‚â§t ‚â§t0 + 1
2(x+ ‚àíx‚àí), x‚àí+ t ‚àít0 ‚â§x ‚â§x+ ‚àít + t0

.

412
Hyperbolic equations
In other words, provided that we specify u(x, t0) and ‚àÇu(x, t0)/‚àÇt in [x‚àí, x+], the
solution of (17.6) can be uniquely determined in the Monge cone without any need for
boundary conditions.
This dependence of hyperbolic PDEs on local data has two important implications
in their numerical analysis. Firstly, consider an explicit FD scheme of the form
un+1
‚Ñì
=
Œ≤

k=‚àíŒ±
ck(¬µ)un
‚Ñì+k,
n ‚â•0,
(17.54)
where c‚àíŒ±(¬µ), cŒ≤(¬µ) Ã∏= 0.
Remember that each um
j
approximates the solution at
(j‚àÜx, m¬µ‚àÜx) and suppose that for suÔ¨Éciently many ‚Ñìand n in the region of interest
it is true that

‚Ñì‚àÜx, (n + 1)¬µ‚àÜx

Ã∏‚ààDn¬µ‚àÜx
(‚Ñì‚àíŒ±)‚àÜx,(‚Ñì+Œ≤)‚àÜx.
As ‚àÜx ‚Üí0, the points that we wish to determine stay persistently outside their
domain of dependence, hence the numerical solution cannot converge there. In other
words, a necessary condition for the stability of explicit FD schemes (and, for that
matter, SD schemes) for hyperbolics is that ¬µ should be small enough that the new
point ‚Äòalmost always‚Äô Ô¨Åts into the domain of dependence of the ‚Äòfootprint‚Äô.4 This is
the celebrated Courant‚ÄìFriedrichs‚ÄìLewy condition, usually known under its acronym,
the CFL condition.
As an example, let us consider again the method (17.12) for the advection equation.
Comparing the Ô¨Çow of information (see the diagrams soon after (17.15)) with the shape
of the domain of dependence, we obtain
s
s
s

3 6
#
‚àÜx

¬µ‚àÜx
where the domain of dependence is enclosed between the parallel dotted lines. We
deduce at once that stability requires ¬µ ‚â§1, which, of course, we already know.
The CFL condition, however, is more powerful than that!
Thus suppose that, in
an explicit method for the advection equation un+s
‚Ñì
depends on un+i
‚Ñì
and un+i
‚Ñì+1 for
i = 0, 1, . . . , s ‚àí1. No matter how we choose the coeÔ¨Écients, the above geometrical
argument demonstrates at once that stability is inconsistent with ¬µ > 1.
In the case of the wave equation and the leapfrog method (17.52), the diagram is
as follows:
s
s
s
s
s

3
Q
Q
Q
k
6
#
‚àÜx

¬µ‚àÜx
4We do not propose to elaborate on the meaning of ‚Äòalmost always‚Äô here. It is enough to remark
that for both the advection equation and the wave equation the condition is either obeyed for all ‚Ñì
and n or violated for all ‚Ñìand n ‚Äì a clear enough distinction.

17.5
The Burgers equation
413
and, again, we need ¬µ ‚â§1 otherwise the method overruns the Monge cone.
Suppose that the wave equation is speciÔ¨Åed with the initial conditions (17.41) but
without boundary conditions. Since
D0
0,1 =

(x, t) : 0 ‚â§t ‚â§1
2, t ‚â§x ‚â§1 ‚àít

,
it makes sense to use the leapfrog method (17.52), in tandem with the starting method
(17.53), to derive the solution there. Each new point depends on its immediate neigh-
bours at the previous time level, and this, together with the value of ¬µ ‚àà(0, 1], restricts
the portion of D0
0,1 that can be reached with the leapfrog method. This is illustrated
in the following diagram, for ¬µ = 3
5:
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
Here, the reachable points are shown as solid and we can observe that they leave a
portion of the Monge cone out of reach of the numerical method.
17.5
The Burgers equation
The Burgers equation (17.10) is the simplest nonlinear hyperbolic conservation law and
its generalization gives us the Euler equations of inviscid compressible Ô¨Çuid dynamics.
We have already seen in Figs 17.1 and 17.2 that its solution displays strange behaviour
and might generate discontinuities from an arbitrarily smooth initial condition. Before
we take up the challenge of numerically solving it, let us Ô¨Årst devote some attention
to the analytic properties of the solution.
3 Why analysis?
The assembling of analytic information before even con-
sidering computation is a hallmark of good numerical analysis, while the ugly
instinct of ‚Äòdiscretizing everything in sight and throwing it on the nearest
computer‚Äô is the worst kind of advice. Partial diÔ¨Äerential equations are com-
plicated constructs and, as soon as nonlinearities are allowed, they may exhibit
a great variety of diÔ¨Écult phenomena. Before we even pose the question of
how well a numerical algorithm is performing, we need to formulate more
precisely what ‚Äòwell‚Äô means!
In reality, it is a two-way traÔ¨Éc between analysis and computation since, when
it comes to truly complicated equations, the best way for a pure mathemati-
cian to guess what should be proved is by using numerical experimentation.
Recent advances in the understanding of nonlinear behaviour in PDEs have
been not the work of narrow specialists in hermetically sealed intellectual com-
partments but the outcome of collaboration between mathematical analysts,

414
Hyperbolic equations
computational experts, applied mathematicians and even experimenters in
their laboratories. There is little doubt that future advances will increasingly
depend on a greater permeability of discipline boundaries.
3
Throughout this section we assume a Cauchy problem,
u(x, 0) = g(x),
‚àí‚àû< x < ‚àû,
where
 ‚àû
‚àí‚àû
[g(x)]2 dx < ‚àû
and g is diÔ¨Äerentiable, since this allows us to disregard boundaries and simplify the
notation. As a matter of fact, introducing Dirichlet boundary conditions leaves most
of our conclusions unchanged.
Let us choose (x, t) ‚ààR √ó [0, ‚àû) and consider the algebraic equation
x = Œæ + g(Œæ)t.
(17.55)
Supposing that a unique solution Œæ = Œæ(x, t) exists for all (x, t) in a suitable subset of
R √ó [0, ‚àû), we set there
w(x, t) := g(Œæ(x, t)).
Therefore
‚àÇw
‚àÇx = g‚Ä≤(Œæ) ‚àÇŒæ
‚àÇx,
‚àÇw
‚àÇt
= g‚Ä≤(Œæ)‚àÇŒæ
‚àÇt
(17.56)
(it can be easily proved that, provided the solution of (17.55) is unique, the function
Œæ is diÔ¨Äerentiable with respect to x and t). The partial derivatives of Œæ can be readily
obtained by diÔ¨Äerentiating (17.55):
1 = ‚àÇŒæ
‚àÇx + g‚Ä≤(Œæ)t ‚àÇŒæ
‚àÇx
‚áí
‚àÇŒæ
‚àÇx =
1
1 + g‚Ä≤(Œæ)t,
0 = ‚àÇŒæ
‚àÇt + g‚Ä≤(Œæ)t‚àÇŒæ
‚àÇt + g(Œæ)
‚áí
‚àÇŒæ
‚àÇt = ‚àí
g(Œæ)
1 + g‚Ä≤(Œæ)t.
Substituting in (17.56) gives
‚àÇw
‚àÇt + 1
2
‚àÇw2
‚àÇx =
‚àÇŒæ
‚àÇt + g(Œæ) ‚àÇŒæ
‚àÇx

g‚Ä≤(Œæ) =

‚àí
g(Œæ)
1 + g‚Ä≤(Œæ)t +
g(Œæ)
1 + g‚Ä≤(Œæ)t

g‚Ä≤(Œæ) = 0,
thus proving that the function w obeys the Burgers equation.
Since the solution of (17.55) for t = 0 is Œæ = x, it follows that w(x, 0) = g(x). In
other words, the function w obeys both the correct equation and the required initial
conditions, hence within its domain of deÔ¨Ånition it coincides with u. (We have just
used ‚Äì without a proof ‚Äì the uniqueness of the solution of (17.10). In fact, and as we
are about to see, the solution need not be unique for general (x, t), but it is so within
the domain of deÔ¨Ånition of w.)
Let us examine our conclusion that u(x, t) = g(Œæ(x, t)) in a new light. We choose
an arbitrary Œæ ‚ààR. It follows from (17.55) that for every 0 ‚â§t < tŒæ, say, it is true
that u(Œæ + g(Œæ)t, t) = g(Œæ). In other words, at least for a short while, the solution of
the Burgers equation is constant along a straight line.

17.5
The Burgers equation
415
‚àí1.0
‚àí0.8
‚àí0.6
‚àí0.4
‚àí0.2
0
0.2
0.4
0.6
0.8
1.0
0.2
0.6
1.0
g
Characteristics
x
Figure 17.7
Characteristics for g(x) = e‚àí5x2 and shock formation.
We have already seen an example of similar behaviour in the case of the advection
equation, since, according to (17.4), its solution is constant along straight lines of slope
+1. The crucial diÔ¨Äerence is that for the Burgers equation the slopes of the straight
lines depend on an initial condition g and, in general, vary with Œæ. This immediately
creates a problem: what if the straight lines collide? At such a point of collision,
which might occur for an arbitrarily small t ‚â•0, we cannot assign an unambiguous
value to the solution ‚Äì it has a discontinuity there. Figure 17.7 displays the straight
lines (under their proper name, characteristics) for the function g(x) = e‚àí5x2. The
formation of a discontinuity is apparent and it is easy to ascertain (cf. Exercise 17.16)
that it starts to develop from the very beginning, at the point x = 0.
Another illustration of how discontinuities develop in the solution of the Burgers
equation is seen in Figs. 17.1 and 17.2.
A discontinuity that originates in a collision of characteristics is called a shock and
its position is determined by the requirement that characteristics must always Ô¨Çow
into a shock and never emerge from it. Representing the position of a shock at time t
by Œ∑(t), say, it is not diÔ¨Écult to derive from elementary geometric considerations the
Rankine‚ÄìHugoniot condition
Œ∑‚Ä≤(t) = 1
2(uL + uR),
(17.57)
‚àí0.5
‚àí0.4
‚àí0.3
‚àí0.2
‚àí0.1
0
0.1
0.2
0.3
0.4
0.5
0
0.5
1.0
g
Characteristics
x
Figure 17.8
Characteristics for a step function and formation of a rarefaction
region.

416
Hyperbolic equations
where uL and uR are the values ‚Äòcarried‚Äô by the characteristics to the left and the right
of the shock respectively.
No sooner have we explained the mechanism of shock formation than another prob-
lem comes to light. If discontinuities are allowed then it is possible for characteristics
to depart from each other, leaving a void which is reached by none of them. Such a
situation is displayed in Fig. 17.8, where the initial condition is already a discontinuous
step function.
A domain that is left alone by the characteristics is called a rarefaction region.
(The terminology of ‚Äòshocks‚Äô and ‚Äòrarefactions‚Äô originates in the shock-tube problem
of gas dynamics.) For valid physical reasons, it is important to Ô¨Åll such a rarefaction
region by imposing the entropy condition
1
2
‚àÇu2
‚àÇt + 1
3
‚àÇu3
‚àÇx ‚â§0.
(17.58)
The origin of (17.58) is the Burgers equation with artiÔ¨Åcial viscosity,
‚àÇu
‚àÇt + 1
2
‚àÇu2
‚àÇx = ŒΩ ‚àÇ2u
‚àÇx2 ,
where ŒΩ > 0 is small. The addition of the parabolic term ŒΩ‚àÇ2u/‚àÇx2 causes dissipation
and the solution is smooth, with neither shocks nor rarefaction regions. Letting ŒΩ ‚Üí0,
it is possible to derive the inequality (17.58). More importantly, it is possible to prove
that, subject to the Rankine‚ÄìHugoniot condition (17.57) and the entropy condition
(17.58), the Burgers equation possesses a unique solution.
There are many numerical schemes for solving the Burgers equation but we restrict
our exposition to perhaps the simplest algorithm that takes on board the special
structure of (17.10) ‚Äì the Godunov method.
The main idea behind this approach is to approximate locally the solution by a
piecewise-constant function. Since, as we will see soon, the Godunov method amends
the step size, we can no longer assume that ‚àÜt is constant. Instead, we denote by
‚àÜtn the step that takes us from the nth to the (n + 1)th time level, n ‚â•0, and let
un
‚Ñì‚âàu(‚Ñì‚àÜx, tn), where t0 = 0 and tn+1 = tn + ‚àÜtn, n ‚â•0.
We let
u0
‚Ñì=
1
‚àÜx
 (‚Ñì+1/2)‚àÜx
(‚Ñì‚àí1/2)‚àÜx
g(x) dx
(17.59)
for all ‚Ñì-values of interest. Supposing that the un
‚Ñìare known, we construct a piecewise-
constant function w[n](¬∑, tn) by letting it equal un
‚Ñìin each interval I‚Ñì:= (x‚Ñì‚àí1/2, x‚Ñì+1/2]
and evaluate the exact solution of this so-called Riemann problem ahead of t = tn.
The idea is to let each interval I‚Ñì‚Äòpropagate‚Äô in the direction determined by its char-
acteristics.
Let us choose a point (x, t), t ‚â•tn. There are three possibilities.
(1) There exists a unique ‚Ñìsuch that the point is reached by a characteristic from
I‚Ñì. Since characteristics propagate constant values, the solution of the Riemann
problem at this point is un
‚Ñì.

17.5
The Burgers equation
417
(2) There exists a unique ‚Ñìsuch that the point is reached by characteristics from
the intervals I‚Ñìand I‚Ñì+1. In this case, as the two intervals ‚Äòpropagate‚Äô in time,
they are separated by a shock. It is trivial to verify from (17.57) that the shock
advances along a straight line that commences at (‚Ñì+ 1
2)‚àÜx and whose slope
is the average of the slopes in I‚Ñìand I‚Ñì+1 ‚Äì in other words, it is 1
2(un
‚Ñì+ un
‚Ñì+1).
Let us denote this line by œÅ‚Ñì.5 The value at (x, t) is un
‚Ñìif x < œÅ‚Ñì(t) and un
‚Ñì+1 if
x > œÅ‚Ñì(t). (We disregard the case when x = œÅ‚Ñì(t) and the point resides on the
shock, since it makes absolutely no diÔ¨Äerence to the algorithm.)
(3) Characteristics from more than two intervals reach the point (x, t). In this case
we cannot assign a value to the point.
Simple geometrical considerations demonstrate that case (3), which we must avoid,
occurs (for some x) for t > Àút, where Àút > tn is the least solution of the equation
œÅ‚Ñì(t) = œÅ‚Ñì+1(t) for some ‚Ñì. This becomes obvious upon an examination of Fig. 17.9.
Let us consider the vertical lines rising from the points (‚Ñì+ 1
2)‚àÜx. Unless the
original solution is identically zero, sooner or later one such line is bound to hit one
of the segments œÅj. We let Àòt be the time at which the Ô¨Årst such encounter takes place,
choose tn+1 ‚àà(tn, Àòt ] and set ‚àÜtn = tn+1‚àítn. Since tn+1 ‚àà(tn, Àút ] (see Fig. 17.9), cases
(1) and (2) can be used to construct a unique solution w[n](x, t) for all tn ‚â§t ‚â§tn+1.
We choose the un+1
‚Ñì
as averages of w[n]( ¬∑ , tn+1) along the intervals I‚Ñì,
un+1
‚Ñì
=
1
‚àÜx
 (‚Ñì+1/2)‚àÜx
(‚Ñì‚àí1/2)‚àÜx
w[n](x, tn+1) dx.
(17.60)
Our description of the Godunov method is complete, except for an important
remark: the integral in (17.60) can be calculated with great ease. Disregarding shocks,
the function w[n] obeys the Burgers equation for t ‚àà[tn, tn+1]. Therefore, integrating
in t,
‚àÇw[n]
‚àÇt
+ 1
2
‚àÇ[w[n]]2
‚àÇx
= 0
‚áí
w[n](x, tn+1) = w[n](x, tn)‚àí1
2
 tn+1
tn
‚àÇ[w[n](x, t)]2
‚àÇx
dt.
Substitution into (17.60) results in
un+1
‚Ñì
=
1
‚àÜx
 (‚Ñì+1/2)‚àÜx
(‚Ñì‚àí1/2)‚àÜx
#
w[n](x, tn) ‚àí1
2
 tn+1
tn
‚àÇ[w[n](x, t)]2
‚àÇx
dt
$
dx.
Since the un
‚Ñìhave been obtained by an averaging procedure as given in (17.60) (this
is the whole purpose of (17.59)), we have, after changing of the order of integration,
un+1
‚Ñì
= un
‚Ñì‚àí
1
2‚àÜx
 tn+1
tn
 (‚Ñì+1/2)‚àÜx
(‚Ñì‚àí1/2)‚àÜx
‚àÇ[w[n](x, t)]2
‚àÇx
dx dt
= un
‚Ñì‚àí
1
2‚àÜx
 tn+1
tn
#%
w[n] 
(‚Ñì+ 1
2)‚àÜx, t
&2
‚àí
%
w[n] 
(‚Ñì‚àí1
2)‚àÜx, t
&2$
dt.
5Not every œÅ‚Ñìis a shock, but this makes no diÔ¨Äerence to the method.

418
Hyperbolic equations
‚àí2.0
‚àí1.5
‚àí1.0
‚àí0.5
0
0.5
1.0
1.5
2.0
0
0.2
0.4
Piecewise-constant approximation
The segments œÅ‚Ñì
Àút


+
Àòt



x
Figure 17.9
The graph shows a piecewise-constant approximation; the upper dia-
gram shows the line segments œÅ‚Ñìand the Ô¨Årst vertical line to collide with œÅ‚Ñì(dotted).
Let us now recall our deÔ¨Ånition of tn+1. No vertical line segments ((‚Ñì+ 1
2)‚àÜx, t),
t ‚àà[tn, tn+1], may cross the discontinuities œÅj, therefore the value of w[n] across each
such segment is constant ‚Äì equalling either un
‚Ñìor un
‚Ñì+1 (depending on the slope of œÅ‚Ñì:
if it points rightwards it is un
‚Ñì, otherwise un
‚Ñì+1). Let us denote this value by œá‚Ñì+1/2;
then
un+1
‚Ñì
= un
‚Ñì‚àí1
2¬µn(œá2
‚Ñì+1/2 ‚àíœá2
‚Ñì‚àí1/2),
(17.61)
where
¬µn := ‚àÜtn
‚àÜx .
The Godunov method is a Ô¨Årst-order approximation to the solution of the Burgers
equation, since the only error that we have incurred comes from replacing the values
along each step by piecewise-constant approximants. It satisÔ¨Åes the Rankine‚ÄìHugoniot
condition and it is possible to prove that it is stable. However, more work, outside
the scope of this exposition, is required to ensure that the entropy condition (17.58)
is obeyed as well.
It is possible to generalize the Godunov method to more complicated nonlinear
hyperbolic conservation laws
‚àÇu
‚àÇt + ‚àÇf(u)
‚àÇx
= 0,
where f is a general diÔ¨Äerentiable function, as well as to systems of such equations.
In each case we obtain a recursion of the form (17.61), except that the deÔ¨Ånition of
the Ô¨Çux œá‚Ñì+1/2 needs to be amended and is slightly more complicated. In the special
case f(u) = u we are back to the advection equation and the Godunov method (17.61)
becomes the familiar scheme (17.12) with 0 < ¬µ ‚â§1.
Comments and bibliography
Fluid and gas dynamics, relativity theory, quantum mechanics, aerodynamics ‚Äì this is just a
partial list of subjects that need hyperbolic PDEs to describe their mathematical foundations.

Comments and bibliography
419
Such equations ‚Äì the Euler equations of inviscid compressible Ô¨Çow, the Schr¬®odinger equation
of wave mechanics, Einstein‚Äôs equations of general relativity etc. ‚Äì are nonlinear and gener-
ally multivariate and multidimensional, and their numerical solution presents a formidable
challenge. This perhaps explains the major eÔ¨Äort that has gone into the computation of
hyperbolic PDEs in the last few decades. A bibliographical journey through the hyperbolic
landscape might commence with texts on their theory, mainly in a nonlinear setting ‚Äì thus
Drazin & Johnson (1988) on solitons, Lax (1973) and LeVeque (1992) on conservation laws
and Whitham (1974) for a general treatment of wave theory. The next destination might
be the classic volume of Richtmyer & Morton (1967), still the best all-round volume on the
foundations of the numerical treatment of evolutionary PDEs, followed by more specialized
sources, LeVeque (1992), Morton & Sonar (2007) or Hirsch (1988) on numerical conservation
laws. Finally, there is an abundance of texts on themes that bear some relevance to the
subject matter: GustaÔ¨Äson et al. (1972) on the inÔ¨Çuence of boundary conditions on stability;
Trefethen (1992) on an alternative treatment, by means of pseuodospectra, of numerical sta-
bility in the absence of normalcy; Iserles & N√∏rsett (1991) on how to derive optimal schemes
for the advection equation, a task that bears a striking similarity to some of the themes from
Chapter 4; and Davis (1979) on circulant matrices.
As soon as we concern ourselves with computational wave mechanics (which, in a way,
is exactly what the numerical solution of hyperbolic PDEs is all about), there are additional
considerations besides order and stability. In Fig. 17.10 we display a numerical solution of
the advection equation (17.1) with initial condition
g(x) = e‚àí100(x‚àí1/2)2 sin 20œÄx,
‚àí‚àû‚â§x ‚â§‚àû.
(17.62)
The function g is a wave packet ‚Äì a highly oscillatory wave modulated by a sharply decay-
ing exponential so that, for all intents and purposes, it vanishes outside a small support.
The exact solution of (17.1) and (17.62) at time t is the function g, unilaterally translated
rightwards by a distance t. In Fig. 17.10 we can observe what happens to the wave packet
under the inÔ¨Çuence of discretization by three stable FD schemes. Firstly, the leapfrog scheme
evidently moves the wave packet at the wrong speed, distorting it in the process. The energy
of the packet ‚Äì that is, the Euclidean norm of the solution ‚Äì stays constant, as it does in the
exact solution: the leapfrog is a conservative method. However, the energy is transported at
an altogether wrong speed. This wrong speed of propagation depends on the wavenumber:
the higher the oscillation (in comparison with ‚àÜx ‚Äì recall from Chapter 13 that frequencies
larger than œÄ/‚àÜx are ‚Äòinvisible‚Äô on a grid scale), the more false the reading and, for suÔ¨É-
ciently high frequencies, a wave can be transported in the wrong direction altogether. Of
course, we can always decrease ‚àÜx so as to render the frequency of any particular wave small
on the grid scale, although this, obviously, increases the cost of computation. Unfortunately,
if the initial condition is discontinuous then its Fourier transform (i.e., its decomposition as
a linear combination of periodic ‚Äòwaves‚Äô) contains all frequencies that are ‚Äòvisible‚Äô in a grid
and this cannot be changed by decreasing ‚àÜx; see Fig. 17.11.
The behaviour of the Lax‚ÄìWendroÔ¨Ämethod as shown in Fig. 17.10 poses another diÔ¨É-
culty. Not only does the wave packet lag somewhat; the main problem is that it has almost
disappeared! Its energy has decreased by about a factor 3 and this is unacceptable. Lax‚Äì
WendroÔ¨Äis dissipative, rather than conservative. The dissipation is governed by the size of
‚àÜx and it disappears as ‚àÜx ‚Üí0, yet it might be highly problematic in some applications.
Unlike either leapfrog or Lax‚ÄìWendroÔ¨Ä, the angled derivative method (17.28) displays
the correct qualitative behaviour: virtually no dissipation; little dispersion; high frequencies
are transported at roughly the right speed (Trefethen, 1982).

420
Hyperbolic equations
0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
‚àí1
0
1
0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
‚àí1
0
1
0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
‚àí1
0
1
Leapfrog
Lax‚ÄìWendroÔ¨Ä
Angled derivative
x
Figure 17.10
Numerical wave propagation by three FD schemes. The dotted line
presents the position of the exact solution at time t = 1. We have used ‚àÜx =
1
80
and ¬µ = 2
3.
A similar picture emerges from Fig. 17.11, where we have displayed the evolution of the
piecewise-constant function
g(x) =
1
1,
1
4 ‚â§x < 3
4,
0,
otherwise.
Leapfrog emerges the worst, both degrading the shock front and transporting some waves
too slowly or, even worse, in the wrong direction. Lax‚ÄìWendroÔ¨Äis much better: although
it also smoothes the sharp shock front, the ‚Äòmissing mass‚Äô is simply dissipated, rather than
reappearing in the wrong place. The angled derivative method displays the sharpest proÔ¨Åle,
but the quid pro quo is spurious oscillations at high wavenumbers. This dead heat between
Lax‚ÄìWendroÔ¨Äand angled derivative emphasizes that no method is perfect and diÔ¨Äerent
methods often possess contrasting advantages and disadvantages.
By this stage, the reader should be well aware why the correct propagation of shock
fronts is so important. Fig. 17.11 reaÔ¨Érms a principle that underlies much of the discussion
of hyperbolic PDEs: methods should follow characteristics.
In the particular context of conservation laws, ‚Äòfollowing characteristics‚Äô means upwind-
ing. This is easy for the advection equation but becomes a more formidable task when the
characteristics change direction, e.g. for the Burgers equation (17.10). Seen in this light,
the Godunov method from Section 17.5 is all about the local determination of the upwind

Comments and bibliography
421
0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
‚àí0.5
0
0.5
1.0
0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
‚àí0.5
0
0.5
1.0
0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
‚àí0.5
0
0.5
1.0
Leapfrog
Lax‚ÄìWendroÔ¨Ä
Angled derivative
x
Figure 17.11
Numerical shock propagation by three FD schemes. The dotted line
presents the position of the exact solution at time t = 1. We have used ‚àÜx =
1
100
and ¬µ = 2
3.
direction. Another popular choice of an upwinding technique is the use of Engquist‚ÄìOsher
switches
f‚àí(y) := [min{y, 0}]2,
f+(y) := [max{y, 0}]2,
y ‚ààR,
to form the SD scheme
u‚Ä≤
‚Ñì+
1
‚àÜx [‚àÜ+f‚àí(u‚Ñì) + ‚àÜ‚àíf+(u‚Ñì)] = 0.
If u‚Ñì‚àí1, u‚Ñìand u‚Ñì+1 are all positive and the characteristics propagate rightwards then we have
‚àÜ+f‚àí(u‚Ñì) = 0 and ‚àÜ‚àíf+(u‚Ñì) = [u‚Ñì]2 ‚àí[u‚Ñì‚àí1]2, while if all three values are negative then
‚àÜ+f‚àí(u‚Ñì) = [u‚Ñì+1]2 ‚àí[u‚Ñì]2 and ‚àÜ‚àíf+(u‚Ñì) = 0. Again, the scheme determines an upwind
direction on a local basis.
Numerical methods for nonlinear conservation laws are among the great success stories
of modern numerical analysis.
They come in many shapes and sizes ‚Äì Ô¨Ånite diÔ¨Äerences,
Ô¨Ånite elements, particle methods, Ô¨Ånite volume methods, spectral methods, . . . , but all good
schemes have in common elements of upwinding and attention to shock propagation.
Davis, P.J. (1979), Circulant Matrices, Wiley, New York.
Drazin, P.G. and Johnson, R.S. (1988), Solitons: An Introduction, Cambridge University
Press, Cambridge.
Gustafsson, B., Kreiss, H.-O. and Sundstr¬®om, A. (1972), Stability theory of diÔ¨Äerence ap-
proximations for mixed initial boundary value problems, Mathematics of Computation 26,
649‚Äì686.

422
Hyperbolic equations
Hirsch, C. (1988), Numerical Computation of Internal and External Flows, Vol. I: Funda-
mentals of Numerical Discretization, Wiley, Chichester.
Iserles, A. and N√∏rsett, S.P. (1991), Order Stars, Chapman & Hall, London.
Lax, P.D. (1973), Hyperbolic Systems of Conservation Laws and the Mathematical Theory of
Shock Waves, SIAM, Philadelphia.
LeVeque, R.J. (1992), Numerical Methods for Conservation Laws, Birkh¬®auser, Basel.
Morton, K.W. and Sonar, T. (2007), Finite volume methods for hyperbolic conservation laws,
Acta Numerica 16, 155‚Äì238.
Richtmyer, R.D. and Morton, K.W. (1967), DiÔ¨Äerence Methods for Initial-Value Problems,
Interscience, New York.
Trefethen, L.N. (1982), Group velocity in Ô¨Ånite diÔ¨Äerence schemes, SIAM Review 24, 113‚Äì
136.
Trefethen, L.N. (1992), Pseudospectra of matrices, in Numerical Analysis 1991 (D.F. GriÔ¨Éths
and G.A. Watson, editors), Longman, London, 234‚Äì266.
Whitham, G. (1974), Linear and Nonlinear Waves, Wiley, New York.
Exercises
17.1
In Section 17.1 we proved that the Fourier stability analysis of the Euler
method (17.12) for the advection equation can be translated from the real
axis (where it is a Cauchy problem) to the interval [0, 1] and a zero boundary
condition (17.3). Can we use the same technique of proof to analyse the
stability of the Euler method (16.7) for the diÔ¨Äusion equation (16.1) with
zero Dirichlet boundary conditions (16.3)?
17.2‚ãÜ
Let Œ± and Œ≤ be nonnegative integers, Œ± + Œ≤ ‚â•1, and set
pŒ±,Œ≤(z) =
Œ≤

k=‚àíŒ±, kÃ∏=0
Àòakzk + qŒ±,Œ≤,
z ‚ààC,
where
Àòak = (‚àí1)k‚àí1
k
Œ±!Œ≤!
(Œ± + k)!(Œ≤ ‚àík)!,
k = ‚àíŒ±, ‚àíŒ± + 1, . . . , Œ≤,
k Ã∏= 0,
and the constant qŒ±,Œ≤ is such that pŒ±,Œ≤(1) = 0.
a Evaluate p‚Ä≤
Œ±,Œ≤, proving that
p‚Ä≤
Œ±,Œ≤(z) = 1
z + O

|z ‚àí1|Œ±+Œ≤
,
z ‚Üí1.
b Integrate the last expression, thereby demonstrating that
pŒ±,Œ≤(z) = ln z + O

|z ‚àí1|Œ±+Œ≤+1
,
z ‚Üí1.

Exercises
423
c Determine the order of the SD scheme
u‚Ä≤
‚Ñì+ 1
‚àÜx

‚àí1

k=‚àíŒ±
Àòaku‚Ñì+k + qŒ±,Œ≤u‚Ñì+
Œ≤

k=1
Àòaku‚Ñì+k

= 0
for the advection equation (17.1).
17.3
Show that the leapfrog method (17.27) recovers the exact solution of the
advection equation when the Courant number ¬µ equals unity. (It should be
noted that this is of little or no relevance to the solution of the system (17.9)
or to nonlinear equations.)
17.4
Analyse the order of the following FD methods for the advection equa-
tion:
a the angled derivative scheme (17.28);
b the Lax‚ÄìWendroÔ¨Äscheme (17.29);
c the Lax‚ÄìFriedrichs scheme
un+1
‚Ñì
= 1
2(1 + ¬µ)un
‚Ñì‚àí1 + 1
2(1 ‚àí¬µ)un
‚Ñì+1,
n ‚â•0.
17.5
Carefully justifying your arguments, use the eigenvalue technique from Sec-
tion 16.4 to prove that the SD scheme (17.23) is stable.
17.6
Find a third-order upwind SD method (17.17) with Œ± = 3, Œ≤ = 0 and prove
that it is unstable for the Cauchy problem.
17.7
Determine the range of Courant numbers ¬µ for which
a the leapfrog scheme (17.27) and
b the angled derivative scheme (17.28)
are stable for the Cauchy problem. Use the method of proof from Section 17.1
to show that the result for the angled derivative scheme can be generalized
from the Cauchy problem to a zero Dirichlet boundary condition.
17.8
Find the order of the box method
(1 ‚àí¬µ)un+1
‚Ñì‚àí1 + (1 + ¬µ)un+1
‚Ñì
= (1 + ¬µ)un
‚Ñì‚àí1 + (1 ‚àí¬µ)un
‚Ñì,
n ‚â•0.
Determine the range of Courant numbers ¬µ for which the method is stable.
17.9
Show that the transpose of a circulant matrix is itself a circulant and use
this to prove that every circulant matrix is normal.
17.10
Find the order of the method
un+1
j,‚Ñì
= 1
2¬µ(¬µ ‚àí1)un
j+1,‚Ñì+1 + (1 ‚àí¬µ2)un
j,‚Ñì+ 1
2¬µ(¬µ + 1)un
j‚àí1,‚Ñì‚àí1,
n ‚â•0,
for the bivariate advection equation (17.6). Here ‚àÜx = ‚àÜy, ‚àÜt = ¬µ‚àÜx and
un
j,‚Ñìapproximates u(j‚àÜx, ‚Ñì‚àÜx, n‚àÜt).

424
Hyperbolic equations
17.11
Determine the order of the Numerov method
zn+2 ‚àí2zn+1 + zn =
1
12h2 [f(tn+2, zn+2)
+ 10f(tn+1, zn+1) + f(tn, zn)] ,
n ‚â•0,
for the second-order ODE system (17.48).
17.12
Application of the method (17.50) to the second-order ODE system (17.45)
results in a two-step FD scheme for the wave equation.
Prove that this
method is unstable for all ¬µ > 0 by two alternative techniques:
a by Fourier analysis;
b directly from the CFL condition.
17.13
Determine the order of the scheme
un+2
‚Ñì
‚àíun+1
‚Ñì
+ un
‚Ñì=
1
12¬µ2(‚àíun+1
‚Ñì‚àí2 + 16un+1
‚Ñì‚àí1
‚àí30un+1
‚Ñì
+ 16un+1
‚Ñì+1 ‚àíun+1
‚Ñì+2 ),
n ‚â•0,
for the solution of the wave equation (17.8) and Ô¨Ånd the range of Courant
numbers ¬µ for which the method is stable for the Cauchy problem.
17.14
Let Àúun
‚Ñì= u(‚Ñì‚àÜx, n‚àÜx), ‚Ñì= 1, 2, . . . , d+1, n ‚â•0, where (d+1)‚àÜx = 1 and u
is the solution of the wave equation (17.8) with the initial conditions (17.41)
and the boundary conditions (17.42).
a Prove the identity
Àúun+2
‚Ñì
‚àíÀúun
‚Ñì= Àúun+1
‚Ñì+1 ‚àíÀúun+1
‚Ñì‚àí1 ,
‚Ñì= 1, 2, . . . , d,
n ‚â•0.
(Hint: You might use without proof the d‚ÄôAlembert solution of the wave
equation
u(x, t) = g(x ‚àít) + h(x + t)
for all 0 ‚â§x ‚â§1 and t ‚â•0.)
b Suppose that the wave equation is solved using the FD method (17.52) with
Courant number ¬µ = 1 and write en
‚Ñì:= un
‚Ñì‚àíÀúun
‚Ñì, ‚Ñì= 0, 1, . . . , d + 1, n ‚â•0.
Prove by induction that
en
‚Ñì=
n‚àí1

j=0
(‚àí1)je1
‚Ñì+n‚àí2j‚àí1,
‚Ñì= 1, 2, . . . , d,
n ‚â•1,
where we let e1
‚Ñì= 0 for ‚ÑìÃ∏‚àà{1, 2, . . . , d}.
c The leapfrog method cannot be used to obtain u1
‚Ñìso, instead, we use the
scheme (17.53). Assuming that it is known that |e1
‚Ñì| ‚â§Œµ, ‚Ñì= 1, 2, . . . , d,
prove that
|en
‚Ñì| ‚â§min{n, ‚åä1
2(d + 1)‚åã}Œµ,
n ‚â•0.
(17.63)
(Naive considerations provide a geometrically increasing upper bound on the
error, but (17.63) demonstrates that it is much too large and that the increase
in the error is at most linear.)

Exercises
425
17.15
Prove that it is impossible for any explicit FD method (17.54) for the advec-
tion equation to be convergent for ¬µ > Œ±.
17.16
Let g be a continuous function and suppose that x0 ‚ààR is its strict (local)
maximum. The Burgers equation (17.10) is solved with the initial condition
u(x, 0) = g(x), x ‚ààR. Prove that a shock propagates from the point x0.
17.17
Show that the entropy condition is satisÔ¨Åed by a solution of the Burgers
equation as an equality in any portion of R √ó [0, ‚àû) that is neither a shock
nor a rarefaction region. (Hint: Multiply the equation by u.)
17.18‚ãÜ
Amend the Godunov method from Section 17.5 to solve the advection equa-
tion (17.1) rather than the Burgers equation. Prove that the result is the
Euler scheme (17.12) with ¬µ automatically restricted to the stable range
(0, 1].


Appendix
BluÔ¨Äer‚Äôs guide to useful mathematics
HEALTH WARNING
This is not a review of undergraduate mathematics or a distillation of the wis-
dom of many lecture courses into a few pages. Certainly, nobody should use it to
understand new material. Mathematics is not learnt from crib-sheets and brief com-
pendia but by careful study of deÔ¨Ånitions, theorems and ‚Äì most importantly, perhaps
‚Äì proofs, by elucidating the intuition behind ideas and grasping the interconnectedness
between what might seem disparate concepts at Ô¨Årst glance. There are no shortcuts
and no cherry-tasting knowledge capsules to help you along your path . . .
A conscious attempt has been made throughout the volume not to take for granted
any knowledge that an advanced mathematics undergraduate is unlikely to possess. If
we need it, we explain it. However, every book has to start from somewhere.
Unless you have a basic knowledge of the Ô¨Årst two years of university or college
mathematics, this appendix will not help you and, indeed, this is the wrong book for
you. However, it is not unusual for students to attend a lecture course, study material,
absorb it, pass an exam with Ô¨Çying colours ‚Äì and yet, a year or two later, a concept
is perhaps not entirely forgotten but resides so deep in the recesses of memory that
it cannot be used here and now. In these circumstances a virtuous reader consults
another textbook or perhaps her lecture notes. A less virtuous reader usually means
to do so ‚Äì not just yet ‚Äì but in the meantime plunges ahead with a decreased level
of comprehension. This appendix has been written in recognition of the poverty and
scarcity of virtue.
While trying to read a mathematical textbook, nothing can be worse than gradually
losing the thread, progressively understanding less and less. This can happen either
because the reader fails to understand the actual material ‚Äì and the fault may well
rest with the author ‚Äì or when she encounters unfamilar mathematical constructs.
If in this volume you occasionally come across a mathematical concept and, for
the life of you, simply cannot recall exactly what it means (or perhaps are not sure of
the Ô¨Åner details of its deÔ¨Ånition), do glance in this appendix ‚Äì you might Ô¨Ånd it here!
However, if these glances become a habit, rather than an exception, perhaps you had
better use a proper textbook!
There are two sections to this appendix, one on linear algebra and the second on
analysis. Neither is complete ‚Äì they both endeavour to answer possible queries arising
from this book, rather than providing a potted summary of a subject.
There is nothing on basic calculus. Unless you are familiar with calculus then, I
am afraid, you are trying to dance the samba before you can walk.
427

428
BluÔ¨Äer‚Äôs guide to useful mathematics
A.1
Linear algebra
A.1.1
Vector spaces
A.1.1.1
A vector space or a linear space V over the Ô¨Åeld F (which in our case will
be either R, the reals, or C, the complex numbers) is a set of elements closed with
respect to addition, i.e.,
x1, x2 ‚ààV
implies
x1 + x2 ‚ààV,
and multiplication by a scalar, i.e.,
Œ± ‚ààF, x ‚ààV
implies
Œ±x ‚ààV.
Addition obeys the axioms of an abelian group: x1 + x2 = x2 + x1 for all x1, x2 ‚ààV
(commutativity); (x1 + x2) + x3 = x1 + (x2 + x3), x1, x2, x3 ‚ààV (associativity);
there exists an element 0 ‚ààV (the zero element) such that x + 0 = x, x ‚ààV; and
for every x1 ‚ààV there exists an element x2 ‚ààV (the inverse) such that x1 + x2 = 0
(we write x2 = ‚àíx1). Multiplication by a scalar is also commutative: Œ±(Œ≤x) = (Œ±Œ≤)x
for all Œ±, Œ≤ ‚ààF, x ‚ààV, and multiplication by the unit element of F leaves x ‚ààV
intact, 1x = x. Moreover, addition and multiplication by a scalar are linked by the
distributive laws Œ±(x1 + x2) = Œ±x1 + Œ±x2 and (Œ± + Œ≤)x1 = Œ±x1 + Œ≤x1, Œ±, Œ≤ ‚ààF,
x1, x2 ‚ààV.
The elements of V are sometimes called vectors.
If V1 ‚äÜV2, where both V1 and V2 are vector spaces, we say that V1 is a subspace
of V2.
A.1.1.2
The vectors x1, x2, . . . , xm ‚ààV are linearly independent if
‚àÉŒ±1, Œ±2, . . . , Œ±m ‚ààF
such that
m

‚Ñì=1
Œ±‚Ñìx‚Ñì= 0
=‚áí
Œ±1, Œ±2, . . . , Œ±m = 0.
A vector space V is of dimension dim V = d if there exist d linearly indepen-
dent elements y1, y2, . . . , yd ‚ààV such that for every x ‚ààV we can Ô¨Ånd scalars
Œ≤1, Œ≤2, . . . , Œ≤d ‚ààF for which
x =
d

‚Ñì=1
Œ≤‚Ñìy‚Ñì.
The set {y1, y2, . . . , yd} ‚äÇV is then said to be a basis of V. In other words, all the
elements of V can be expressed by forming linear combinations of its basis elements.
A.1.1.3
The vector space Rd (over F = R, the reals) consists of all real d-tuples
x =
‚é°
‚é¢‚é¢‚é¢‚é£
x1
x2
...
xd
‚é§
‚é•‚é•‚é•‚é¶,

A.1
Linear algebra
429
with addition and multiplication by a scalar deÔ¨Åned by
‚é°
‚é¢‚é¢‚é¢‚é£
x1
x2
...
xd
‚é§
‚é•‚é•‚é•‚é¶+
‚é°
‚é¢‚é¢‚é¢‚é£
y1
y2
...
yd
‚é§
‚é•‚é•‚é•‚é¶=
‚é°
‚é¢‚é¢‚é¢‚é£
x1 + y1
x2 + y2
...
xd + yd
‚é§
‚é•‚é•‚é•‚é¶
and
Œ±
‚é°
‚é¢‚é¢‚é¢‚é£
x1
x2
...
xd
‚é§
‚é•‚é•‚é•‚é¶=
‚é°
‚é¢‚é¢‚é¢‚é£
Œ±x1
Œ±x2
...
Œ±xd
‚é§
‚é•‚é•‚é•‚é¶
respectively. It is of dimension d with a canonical basis
e1 =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
1
0
0
...
0
‚é§
‚é•‚é•‚é•‚é•‚é•‚é¶
,
e2 =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
0
1
0
...
0
‚é§
‚é•‚é•‚é•‚é•‚é•‚é¶
,
. . . ,
ed =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
0
0
...
0
1
‚é§
‚é•‚é•‚é•‚é•‚é•‚é¶
of unit vectors.
Likewise, letting F = C we obtain the vector space Cd of all complex d-tuples, with
similarly deÔ¨Åned operations of addition and multiplication by a scalar. It is again of
dimension d and possesses an identical basis {e1, e2, . . . , ed} of unit vectors.
In what follows, unless explicitly stated, we restrict our review to Rd. Transplan-
tation to Cd is straightforward.
A.1.2
Matrices
A.1.2.1
A matrix A is an d √ó n array of real numbers,
A =
‚é°
‚é¢‚é¢‚é¢‚é£
a1,1
a1,2
¬∑ ¬∑ ¬∑
a1,n
a2,1
a2,2
¬∑ ¬∑ ¬∑
a2,n
...
...
...
ad,1
ad,2
¬∑ ¬∑ ¬∑
ad,n
‚é§
‚é•‚é•‚é•‚é¶.
It is said to have d rows and n columns. The addition of two d √ó n matrices is deÔ¨Åned
by
‚é°
‚é¢‚é¢‚é¢‚é£
a1,1 a1,2 ¬∑ ¬∑ ¬∑ a1,n
a2,1 a2,2 ¬∑ ¬∑ ¬∑ a2,n
...
...
...
ad,1 ad,2 ¬∑ ¬∑ ¬∑ ad,n
‚é§
‚é•‚é•‚é•‚é¶+
‚é°
‚é¢‚é¢‚é¢‚é£
b1,1 b1,2 ¬∑ ¬∑ ¬∑ b1,n
b2,1 b2,2 ¬∑ ¬∑ ¬∑ b2,n
...
...
...
bd,1 bd,2 ¬∑ ¬∑ ¬∑ bd,n
‚é§
‚é•‚é•‚é•‚é¶=
‚é°
‚é¢‚é¢‚é¢‚é£
a1,1 + b1,1 a1,2 + b1,2 ¬∑ ¬∑ ¬∑ a1,n + b1,n
a2,1 + b2,1 a2,2 + b2,2 ¬∑ ¬∑ ¬∑ a2,n + b2,n
...
...
...
ad,1 + bd,1 ad,2 + bd,2 ¬∑ ¬∑ ¬∑ ad,n + bd,n
‚é§
‚é•‚é•‚é•‚é¶
and multiplication by a scalar is deÔ¨Åned by
Œ±
‚é°
‚é¢‚é¢‚é¢‚é£
a1,1
a1,2
¬∑ ¬∑ ¬∑
a1,n
a2,1
a2,2
¬∑ ¬∑ ¬∑
a2,n
...
...
...
ad,1
ad,2
¬∑ ¬∑ ¬∑
ad,n
‚é§
‚é•‚é•‚é•‚é¶=
‚é°
‚é¢‚é¢‚é¢‚é£
Œ±a1,1
Œ±a1,2
¬∑ ¬∑ ¬∑
Œ±a1,n
Œ±a2,1
Œ±a2,2
¬∑ ¬∑ ¬∑
Œ±a2,n
...
...
...
Œ±ad,1
Œ±ad,2
¬∑ ¬∑ ¬∑
Œ±ad,n
‚é§
‚é•‚é•‚é•‚é¶.

430
BluÔ¨Äer‚Äôs guide to useful mathematics
Given an m √ó d matrix A and an d √ó n matrix B, the product C = AB is the m √ó n
matrix
C =
‚é°
‚é¢‚é¢‚é¢‚é£
c1,1
c1,2
¬∑ ¬∑ ¬∑
c1,n
c2,1
c2,2
¬∑ ¬∑ ¬∑
c2,n
...
...
...
cm,1
cm,2
¬∑ ¬∑ ¬∑
cm,n
‚é§
‚é•‚é•‚é•‚é¶,
where
ci,j =
d

‚Ñì=1
ai,‚Ñìb‚Ñì,j,
i = 1, 2, . . . , m,
j = 1, 2, . . . , n.
Any x ‚ààRd is itself an d √ó 1 matrix. Hence, the matrix‚Äìvector product y = Ax,
where A is m √ó d, is an element of Rm such that
yi =
d

‚Ñì=1
ai,‚Ñìx‚Ñì,
i = 1, 2, . . . , m.
In other words, any m √ó d matrix A is a linear transformation that maps Rd to Rm.
A.1.2.2
The identity matrix is the d √ó d matrix
I =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é£
1
0
¬∑ ¬∑ ¬∑
0
0
1
...
...
...
...
...
0
0
¬∑ ¬∑ ¬∑
0
1
‚é§
‚é•‚é•‚é•‚é•‚é¶
.
It is true that IA = A and BI = B for any d √ó n matrix A and m √ó d matrix B
respectively.
A.1.2.3
A matrix is square if it is d √ó d for some d ‚â•1. The determinant of a
square matrix A can be deÔ¨Åned by induction. If d = 1, so that A is simply a real
number, det A = A. Otherwise
det A =
d

j=1
(‚àí1)d+jad,j det Aj,
where A1, A2, . . . , Ad are (d ‚àí1) √ó (d ‚àí1) matrices given by
Aj =
‚é°
‚é¢‚é¢‚é¢‚é£
a1,1
¬∑ ¬∑ ¬∑
a1,j‚àí1
a1,j+1
¬∑ ¬∑ ¬∑
a1,d
a2,1
¬∑ ¬∑ ¬∑
a2,j‚àí1
a2,j+1
¬∑ ¬∑ ¬∑
a2,d
...
...
...
...
ad‚àí1,1
¬∑ ¬∑ ¬∑
ad‚àí1,j‚àí1
ad‚àí1,j+1
¬∑ ¬∑ ¬∑
ad‚àí1,d
‚é§
‚é•‚é•‚é•‚é¶,
j = 1, 2, . . . , d.
An alternative means of deÔ¨Åning det A is as as follows:
det A =

œÉ
(‚àí1)|œÉ|
d

j=1
aj,œÉ(j),

A.1
Linear algebra
431
where the summation is carried across all d! permutations œÉ of the numbers 1, 2, . . . , d.
The parity |œÉ| of the permutation œÉ is the minimal number of two-term exchanges
that are needed to convert it to the unit permutation i = (1, 2, . . . , d).
Provided that det A Ã∏= 0, the d √ó d matrix A possesses a unique inverse A‚àí1; this
is a d √ó d matrix such that A‚àí1A = AA‚àí1 = I. An explicit deÔ¨Ånition of A‚àí1 is
A‚àí1 = adj A
det A,
where the (i, j)th component bi,j of the d √ó d adjugate matrix adj A is
bi,j = (‚àí1)i+j det
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
a1,1
¬∑ ¬∑ ¬∑
a1,j‚àí1
a1,j+1
¬∑ ¬∑ ¬∑
a1,d
...
...
...
...
ai‚àí1,1
¬∑ ¬∑ ¬∑ ai‚àí1,j‚àí1
ai‚àí1,j+1
¬∑ ¬∑ ¬∑ ai‚àí1,d
ai+1,1
¬∑ ¬∑ ¬∑
ai+1,j‚àí1
ai+1,j+1
¬∑ ¬∑ ¬∑
ai+1,d
...
...
...
...
ad,1
¬∑ ¬∑ ¬∑
ad,j‚àí1
ad,j+1
¬∑ ¬∑ ¬∑
ad,d
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
,
i, j = 1, 2, . . . , d.
A matrix A such that det A Ã∏= 0 is nonsingular; otherwise it is singular.
A.1.2.4
The transpose A‚ä§of a d √ó n matrix A is an n √ó d matrix such that
A‚ä§=
‚é°
‚é¢‚é¢‚é¢‚é£
a1,1
a2,1
¬∑ ¬∑ ¬∑
an,1
a1,2
a2,2
¬∑ ¬∑ ¬∑
an,2
...
...
...
a1,d
a2,d
¬∑ ¬∑ ¬∑
an,d
‚é§
‚é•‚é•‚é•‚é¶.
A.1.2.5
A square d √ó d matrix A is
‚Ä¢ diagonal if aj,‚Ñì= 0 for every j Ã∏= ‚Ñì, j, ‚Ñì= 1, 2, . . . , d.
‚Ä¢ symmetric if A‚ä§= A;
‚Ä¢ Hermitian or self-adjoint if A is complex and ¬ØA‚ä§= A;
‚Ä¢ skew-symmetric (or anti-symmetric) if A‚ä§= ‚àíA;
‚Ä¢ skew-Hermitian if A is complex and ¬ØA‚ä§= ‚àíA;
‚Ä¢ orthogonal if A‚ä§A = I, the identity matrix, which is equivalent to
d

‚Ñì=1
a‚Ñì,ia‚Ñì,j =
# 1,
i = j,
0,
i Ã∏= j,
i, j = 1, 2, . . . , d.
Note that in this case A‚àí1 = A‚ä§and that A‚ä§is also orthogonal;
‚Ä¢ unitary if A is complex and ¬ØA‚ä§A = I;

432
BluÔ¨Äer‚Äôs guide to useful mathematics
‚Ä¢ a permutation matrix if all its elements are either 0 or 1 and there is exactly one 1
in each row and column. The matrix‚Äìvector product Ax permutes the elements
of x ‚ààRd, while A‚ä§y causes an inverse permutation ‚Äì therefore A‚ä§= A‚àí1 and
A is orthogonal;
‚Ä¢ tridiagonal if ai,j = 0 for every |i ‚àíj| ‚â•2, in other words
A =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
a1,1
a1,2
0
¬∑ ¬∑ ¬∑
0
a2,1
a2,2
a2,3
...
...
0
...
...
...
0
...
...
ad‚àí1,d‚àí2
ad‚àí1,d‚àí1
ad‚àí1,d
0
¬∑ ¬∑ ¬∑
0
ad,d‚àí1
ad,d
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
;
‚Ä¢ a Vandermonde matrix if
A =
‚é°
‚é¢‚é¢‚é¢‚é£
1
Œæ1
Œæ2
1
¬∑ ¬∑ ¬∑
Œæd‚àí1
1
1
Œæ2
Œæ2
2
¬∑ ¬∑ ¬∑
Œæd‚àí1
2
...
...
...
...
1
Œæd
Œæ2
d
¬∑ ¬∑ ¬∑
Œæd‚àí1
d
‚é§
‚é•‚é•‚é•‚é¶
for some Œæ1, Œæ2, . . . , Œæd ‚ààC. It is true in this case that
det A =
d

i=2
i‚àí1

j=1
(Œæi ‚àíŒæj),
hence a Vandermonde matrix is nonsingular if and only if Œæ1, Œæ2, . . . , Œæd are dis-
tinct numbers;
‚Ä¢ reducible if {1, 2, . . . , d} = I1 ‚à™I2 such that I1 ‚à©I2 = ‚àÖand ai,j = 0 for all i ‚ààI1,
j ‚ààI2. Otherwise it is irreducible;
‚Ä¢ normal if A‚ä§A = AA‚ä§. Symmetric, skew-symmetric and orthogonal matrices
are all normal.
A.1.3
Inner products and norms
A.1.3.1
An inner product, also known as a scalar product, is a function ‚ü®¬∑ , ¬∑ ‚ü©:
Rd √ó Rd ‚ÜíR with the following properties.
(1)
Nonnegativity:
‚ü®x, x‚ü©‚â•0 for every x ‚ààRd and ‚ü®x, x‚ü©= 0 if and only if x = 0,
the zero vector.
(2)
Linearity:
‚ü®Œ±x + Œ≤y, z‚ü©= Œ±‚ü®x, z‚ü©+ Œ≤‚ü®y, z‚ü©for all Œ±, Œ≤ ‚ààR and x, y, z ‚ààRd.
(3)
Symmetry:
‚ü®x, y‚ü©= ‚ü®y, x‚ü©for all x, y ‚ààRd.
(4)
The Cauchy‚ÄìSchwarz inequality:
For every x, y ‚ààRd it is true that
|‚ü®x, y‚ü©| ‚â§[‚ü®x, x‚ü©]1/2[‚ü®y, y‚ü©]1/2.

A.1
Linear algebra
433
A particular example is the Euclidean (or ‚Ñì2) inner product
‚ü®x, y‚ü©= x‚ä§y,
x, y ‚ààRd.
In the case of the complex-valued vector space Cd, the symmetry axiom of the
inner product needs to be replaced by ‚ü®x, y‚ü©= ‚ü®y, x‚ü©, x, y ‚ààCd, where the bar
denotes conjugation, while the complex Euclidean inner product is
‚ü®x, y‚ü©= ¬Øx‚ä§y,
x, y ‚ààCd.
A.1.3.2
Two vectors x, y ‚ààRd such that ‚ü®x, y‚ü©= 0 are said to be orthogonal.
A basis {y1, y2, . . . , yd} constructed from orthogonal vectors is called an orthogonal
basis, or, if ‚ü®y‚Ñì, y‚Ñì‚ü©= 1, ‚Ñì= 1, 2, . . . , d, an orthonormal basis. The canonical basis
{e1, e2, . . . , ed} is orthonormal with respect to the Euclidean norm.
A.1.3.3
A vector norm is a function ‚à•¬∑ ‚à•: Rd ‚ÜíR that obeys the following axioms.
(1)
Nonnegativity:
‚à•x‚à•‚â•0 for every x ‚ààRd and ‚à•x‚à•= 0 if and only if x = 0.
(2)
Rescaling:
‚à•Œ±x‚à•= |Œ±|‚à•x‚à•for every Œ± ‚ààR and x ‚ààRd.
(3)
The triangle inequality:
‚à•x + y‚à•‚â§‚à•x‚à•+ ‚à•y‚à•for every x, y ‚ààRd.
Any inner product ‚ü®¬∑ , ¬∑ ‚ü©induces a norm ‚à•x‚à•= [‚ü®x, x‚ü©]1/2, x ‚ààRd. In particular,
the Euclidean inner product induces the ‚Ñì2 norm, also known as the Euclidean norm,
the least squares or the energy norm, ‚à•x‚à•= (x‚ä§x)1/2, x ‚ààRd (or ‚à•x‚à•= (¬Øx‚ä§x)1/2,
x ‚ààCd).
Not every vector norm is induced by an inner product. Well-known and useful
examples are the ‚Ñì1 norm (also known as the Manhattan norm)
‚à•x‚à•=
d

‚Ñì=1
|x‚Ñì|,
x ‚ààRd,
and the ‚Ñì‚àûnorm (also known as the Chebyshev norm, the uniform norm, the max
norm or the sup norm),
‚à•x‚à•=
max
‚Ñì=1,2,...,d |x‚Ñì|,
x ‚ààRd.
A.1.3.4
Every vector norm ‚à•¬∑ ‚à•acting on Rd can be extended to a norm on d √ó d
matrices, the induced matrix norm, by letting
‚à•A‚à•=
max
x‚ààRd, xÃ∏=0
‚à•Ax‚à•
‚à•x‚à•=
max
x‚ààRd, ‚à•x‚à•=1
‚à•Ax‚à•.
It is always true that
‚à•Ax‚à•‚â§‚à•A‚à•√ó ‚à•x‚à•,
x ‚ààRd
and
‚à•AB‚à•‚â§‚à•A‚à•√ó ‚à•B‚à•,

434
BluÔ¨Äer‚Äôs guide to useful mathematics
where both A and B are d √ó d matrices.
The Euclidean norm of an orthogonal matrix always equals unity.
A.1.3.5
A d √ó d symmetric matrix A is said to be positive deÔ¨Ånite if
‚ü®Ax, x‚ü©> 0
x ‚ààRd \ {0}
and negative deÔ¨Ånite if the above inequality is reversed. It is positive semideÔ¨Ånite or
negative semideÔ¨Ånite if
‚ü®Ax, x‚ü©‚â•0
or
‚ü®Ax, x‚ü©‚â§0
for all x ‚ààRd, respectively.
A.1.4
Linear systems
A.1.4.1
The linear system
a1,1x1 + a1,2x2 + ¬∑ ¬∑ ¬∑ + a1,dxd = b1,
a2,1x1 + a2,2x2 + ¬∑ ¬∑ ¬∑ + a2,dxd = b2,
...
ad,1x1 + ad,2x2 + ¬∑ ¬∑ ¬∑ + ad,dxd = bd,
is written in vector notation as Ax = b. It possesses a unique solution x = A‚àí1b if
and only if A is nonsingular.
A.1.4.2
If a square matrix A is singular then there exists a nonzero solution to the
homogeneous linear system Ax = 0. The kernel of A, denoted by ker A, is the set of
all x ‚ààRd such that Ax = 0. If A is nonsingular then ker A = {0}, otherwise ker A is
a subspace of Rd of dimension ‚â•1.
Recall that a d √ó d matrix A is a linear transformation mapping Rd into itself. If
A is nonsingular (‚áîdet A Ã∏= 0 ‚áîker A = {0}) then this mapping is an isomorphism
(in other words, it has a well-deÔ¨Åned and unique inverse linear transformation A‚àí1,
acting on the image ARd := {Ax : x ‚ààRd} and mapping it back to Rd) on Rd (i.e.,
the image ARd is all Rd). However, if A is singular (‚áîdet A = 0 ‚áîdim ker A ‚â•1)
then ARd is a proper vector subspace of Rd and dim(ARd) = d ‚àídim ker A ‚â§d ‚àí1.
A.1.4.3
The practical solution of linear systems such as the above can be performed
by Gaussian elimination. We commence by subtracting from the ‚Ñìth equation, ‚Ñì=
2, 3, . . . , d, the product of the Ô¨Årst equation by the real number a‚Ñì,1/a1,1. This does
not change the solution x of the linear system, while setting zeros in the Ô¨Årst column,
except in the Ô¨Årst equation, and replacing the system by
a1,1x1 + a1,2x2 + ¬∑ ¬∑ ¬∑ + a1,dxd = b1,
Àúa2,2x2 + ¬∑ ¬∑ ¬∑ + Àúa2,d = Àúb2,
...
Àúad,2x2 + ¬∑ ¬∑ ¬∑ + Àúad,d = Àúbd,

A.1
Linear algebra
435
where
Àúa‚Ñì,j = a‚Ñì,j ‚àía1,1
a‚Ñì,1
a1,j,
j = 2, 3, . . . , d,
Àúb‚Ñì= b‚Ñì‚àía1,1
a‚Ñì,1
b1,
‚Ñì= 2, 3, . . . , d.
The unknown x1 does not feature in equations 2, 3, . . . , d because it has been elim-
inated; the latter thereby constitute a set of d ‚àí1 equations in d ‚àí1 unknowns.
Continuing this process by induction results in an upper triangular linear system:
a(1)
1,1x1 + a(1)
1,2x2 + a(1)
1,3x3 + ¬∑ ¬∑ ¬∑ + a(1)
1,dxd = b(1)
1 ,
a(2)
2,2x2 + a(2)
2,3x3 + ¬∑ ¬∑ ¬∑ + a(2)
2,dxd = b(2)
2 ,
a(3)
3,3x3 + ¬∑ ¬∑ ¬∑ + a(3)
3,dxd = b(3)
3
...
a(d)
d,dxd = b(d)
d ,
where a(1)
1,j = a1,j, a(2)
2,j = Àúa2,j, a(3)
3,j = a(2)
3,j ‚àí

a(2)
2,2/a(2)
3,2

a(2)
2,j etc.
The upper triangular system is solved successively from the bottom:
xd =
1
a(d)
d,d
b(d)
d ,
xd‚àí1 =
1
a(d‚àí1)
d‚àí1,d‚àí1
%
b(d‚àí1)
d‚àí1
‚àía(d)
d‚àí1,dxd
&
,
...
x1 =
1
a(1)
1,1
‚é°
‚é£b(1)
1
‚àí
d

j=2
a(1)
1,jxj
‚é§
‚é¶.
The whole procedure depends on the pivots a(‚Ñì)
‚Ñì,‚Ñìbeing nonzero, otherwise it cannot
be carried out in this fashion. It is perfectly possible for a pivot to vanish even if A
is nonsingular and (with few important exceptions) it is impractical to determine
whether a pivot vanishes by inspecting the elements of A. Moreover, if some pivots
are exceedingly small (in modulus), even if none vanishes, large rounding errors can
be introduced by computer arithmetic, thereby destroying the precision of Gaussian
elimination and rendering it unusable.
A.1.4.4
A standard means of preventing pivots becoming small is column pivoting.
Instead of eliminating the ‚Ñìth equation at the ‚Ñìth stage, we Ô¨Årst search for m ‚àà
{‚Ñì, ‚Ñì+ 1, . . . , d} such that |a(‚Ñì)
m,‚Ñì| ‚â•|a(‚Ñì)
i,‚Ñì| for all i = ‚Ñì, ‚Ñì+ 1, . . . , d, interchange the ‚Ñìth
and the ith equations and only then eliminate.
A.1.4.5
An alternative formulation of Gaussian elimination is by means of the
LU factorization A = LU, where the d √ó d matrices L and U are lower and upper

436
BluÔ¨Äer‚Äôs guide to useful mathematics
triangular, respectively, and all diagonal elements of L equal unity,
L =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
1
0
¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
0
‚Ñì2,1
1
0
...
...
...
...
...
...
‚Ñìd‚àí1,1 ¬∑ ¬∑ ¬∑ ‚Ñìd‚àí1,d‚àí2
1
0
‚Ñìd,1
¬∑ ¬∑ ¬∑
‚Ñìd,d‚àí2
‚Ñìd,d‚àí1 1
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
and U =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
u1,1 u1,2
¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
u1,d
0
u2,2 u2,3
...
...
...
...
...
...
0
¬∑ ¬∑ ¬∑
0
ud‚àí1,d‚àí1 ud‚àí1,d
0
¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
0
ud,d
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
.
Provided that an LU factorization of A is known, we replace the linear system Ax = b
by the two systems Ly = b and Ux = y. The Ô¨Årst is lower triangular, hence
y1 = b1,
y2 = b2 ‚àí‚Ñì2,1y1,
...
yd = bd ‚àí
d‚àí1

j=1
‚Ñìd,jyj,
while the second is upper triangular ‚Äì and we have already seen in A.1.4.3 how to
solve upper triangular linear systems.
The practical evaluation of an LU factorization can be done explicitly, by letting,
consecutively for k = 1, 2, . . . , d,
uk,j = ak,j ‚àí
j‚àí1

i=1
‚Ñìk,iui,j,
j = k, k + 1, . . . , d,
‚Ñìj,k =
1
uk,k

aj,k ‚àí
k‚àí1

i=1
‚Ñìj,iui,k

,
j = k + 1, k + 2, . . . , d.
As always, empty sums are nil and empty number ranges are disregarded.
LU factorization, like Gaussian elimination, is prone to failure due to small values
of the pivots u1,1, u2,2, . . . , ud,d and the remedy ‚Äì column pivoting ‚Äì is identical. After
all, LU factorization is but a recasting of Gaussian elimination into a form that is
more convenient for various applications.
A.1.4.6
A positive deÔ¨Ånite matrix A can be factorized into the product A = LL‚ä§,
where L is lower triangular with ‚Ñìj,j > 0, j = 1, 2, . . . , d; this is the Cholesky factor-
ization. It can be evaluated explicitly via
‚Ñìk,j =
1
‚Ñìj,j

ak,j ‚àí
j‚àí1

i=1
‚Ñìk,i‚Ñìj,i

,
j = 1, 2, . . . , k ‚àí1,
‚Ñìk,k =

ak,k ‚àí
k‚àí1

i=1
‚Ñì2
k,i
1/2

A.1
Linear algebra
437
for k = 1, 2, . . . , d.
Having evaluated a Cholesky factorization, the linear system Ax = b can be solved
by a sequential evaluation of two tridiagonal systems, Ô¨Årstly computing Ly = b and
then L‚ä§x = y.
An advantage of Cholesky factorization is that it requires half the storage and half
the computational cost of an LU factorization.
A.1.5
Eigenvalues and eigenvectors
A.1.5.1
We say that Œª ‚ààC is an eigenvalue of the d√ód matrix A if det(A‚àíŒªI) = 0.
The set of all eigenvalues of a square matrix A is called the spectrum and denoted by
œÉ(A).
Each d √ó d matrix has exactly d eigenvalues. All the eigenvalues of a symmetric
matrix are real, all the eigenvalues of a skew-symmetric matrix are pure imaginary and,
in general, the eigenvalues of a real matrix are either real or form complex conjugate
pairs.
If all the eigenvalues of a symmetric matrix are positive then it is positive deÔ¨Å-
nite. A similarly worded statement extends to negative, semipositive and seminegative
matrices.
We say that an eigenvalue is of algebraic multiplicity r ‚â•1 if it is a zero of
multiplicity r of the characteristic polynomial p(z) = det(A ‚àízI); in other words, if
p(Œª) = dp(Œª)
dz
= ¬∑ ¬∑ ¬∑ = dr‚àí1p(Œª)
dzr‚àí1
= 0,
drp(Œª)
dzr
Ã∏= 0.
An eigenvalue of algebraic multiplicity 1 is said to be distinct.
A.1.5.2
The spectral radius of A is a nonnegative number
œÅ(A) = max{|Œª| : Œª ‚ààœÉ(A)}.
It always obeys the inequality
œÅ(A) ‚â§‚à•A‚à•
(where ‚à•¬∑ ‚à•is the Euclidean norm) but œÅ(A) = ‚à•A‚à•for a normal matrix A. It is
possible to express the Euclidean norm of a general square matrix A in the form
‚à•A‚à•= [œÅ(A‚ä§A)]1/2.
A.1.5.3
If Œª ‚ààœÉ(A) it follows that dim ker (A‚àíŒªI) ‚â•1, therefore there are nonzero
vectors in the eigenspace ker (A ‚àíŒªI). Each such vector is called an eigenvector of A
corresponding to the eigenvalue Œª. An alternative formulation is that v ‚ààRd \ {0} is
an eigenvector of A, corresponding to Œª ‚ààœÉ(A), if Av = Œªv. Note that even if A is
real, its eigenvectors ‚Äì like its eigenvalues ‚Äì may be complex.
The geometric multiplicity of Œª is the dimension of its eigenspace and it is always
true that
1 ‚â§geometric multiplicity ‚â§algebraic multiplicity.

438
BluÔ¨Äer‚Äôs guide to useful mathematics
If the geometric and algebraic multiplicities are equal for all its eigenvalues, A is
said to have a complete set of eigenvectors. Since diÔ¨Äerent eigenspaces are linearly
independent and the sum of algebraic multiplicities is always d, a matrix possessing
a complete set of eigenvectors provides a basis of Rd formed by its eigenvectors ‚Äì
speciÔ¨Åcally, the union over all bases of its eigenspaces.
If all the eigenvalues of A are distinct then it has a complete set of eigenvectors.
A normal matrix also shares this feature and, moreover, it always has an orthogonal
basis of eigenvectors.
A.1.5.4
If a d √ó d matrix A has a complete set of eigenvectors then it possesses the
spectral factorization
A = V DV ‚àí1.
Here D is a diagonal matrix and d‚Ñì,‚Ñì= Œª‚Ñì, œÉ(A) = {Œª1, Œª2, . . . , Œªd}, the ‚Ñìth column
of the d √ó d matrix V is an eigenvector in the eigenspace of Œª‚Ñìand the columns of V
are selected so that det V Ã∏= 0, in other words so that the columns form a basis of Rd.
This is possible according to A.1.5.3.
If A is normal, it is possible to normalize its eigenvectors (speciÔ¨Åcally, by letting
them be of unit Euclidean norm) so that V is an orthogonal matrix.
Let two d √ó d matrices A and B share a complete set of eigenvectors; then A =
V DAV ‚àí1, B = V DBV ‚àí1, say. Since diagonal matrices always commute,
AB = (V DAV ‚àí1)(V DBV ‚àí1) = (V DA)(V ‚àí1V )(DBV ‚àí1) = V (DADB)V ‚àí1
= V (DBDA)V ‚àí1 = (V DB)(V ‚àí1V )(BAV ‚àí1) = (V DBV ‚àí1)(V DAV ‚àí1) = BA
and the matrices A and B also commute.
A.1.5.5
Let
f(z) =
‚àû

k=0
fkzk,
z ‚ààC,
be an arbitrary power series that converges for all |z| ‚â§œÅ(A), where A is a d √ó d
matrix. The matrix function
f(A) :=
‚àû

k=0
fkAk
then converges. Moreover, if Œª ‚ààœÉ(A) and v is in the eigenspace of Œª then f(A)v =
f(Œª)v. In particular,
œÉ(f(A)) = {f(Œªj) : Œªj ‚ààœÉ(A),
j = 1, 2, . . . , d}.
If A has a spectral factorization A = V DV ‚àí1 then f(A) factorizes as follows:
f(A) = V f(D)V ‚àí1.
A.1.5.6
Every d √ó d matrix A possesses a Jordan factorization
A = WŒõW ‚àí1,

A.2
Analysis
439
where det W Ã∏= 0 and the d √ó d matrix Œõ can be written in block form as
Œõ =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é£
Œõ1
O
¬∑ ¬∑ ¬∑
O
O
Œõ2
...
...
...
...
...
O
O
¬∑ ¬∑ ¬∑
O
Œõs
‚é§
‚é•‚é•‚é•‚é•‚é¶
.
Here Œª1, Œª2, . . . , Œªs ‚ààœÉ(A) and the kth Jordan block is
Œõk =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
Œªk
1
0
¬∑ ¬∑ ¬∑
0
0
Œªk
1
...
...
...
...
...
...
0
...
...
Œªk
1
0
¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
0
Œªk
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
,
k = 1, 2, . . . , s.
Bibliography
Halmos, P.R. (1958), Finite-Dimensional Vector Spaces, van Nostrand‚ÄìReinhold, Princeton,
NJ.
Lang, S. (1987), Introduction to Linear Algebra (2nd edn), Springer-Verlag, New York.
Strang, G. (1987), Linear Algebra and its Applications (3rd edn), Harcourt Brace Jovanovich,
San Diego, CA.
A.2
Analysis
A.2.1
Introduction to functional analysis
A.2.1.1
A linear space is an arbitrary collection of objects closed under addition
and multiplication by a scalar. In other words, V is a linear space over the Ô¨Åeld F (a
scalar Ô¨Åeld) if there exist operations + : V √ó V ‚ÜíV and ¬∑ : F √ó V ‚ÜíV, consistent
with the following axioms (for clarity we denote the elements of V as vectors):
(1)
Commutativity:
x + y = y + x for every x, y ‚ààV.
(2)
Existence of zero:
There exists a unique element 0 ‚ààV such that x + 0 = x
for every x ‚ààV.
(3)
Existence of inverse:
For every x ‚ààV there exists a unique element ‚àíx ‚ààV
such that x + (‚àíx) = 0.
(4)
Associativity:
(x + y) + z = x + (y + z) for every x, y, z ‚ààV
(These four
axioms mean that V is an abelian group with respect to addition).
(5)
Interchange of multiplication:
Œ±(Œ≤x) = (Œ±Œ≤)x for every Œ±, Œ≤ ‚ààF, x ‚ààV.

440
BluÔ¨Äer‚Äôs guide to useful mathematics
(6)
Action of unity:
1x = x for every x ‚ààV, where 1 is the unit element of F.
(7)
Distributivity: Œ±(x+y) = Œ±x+Œ±y and (Œ±+Œ≤)x = Œ±x+Œ≤x for every Œ±, Œ≤ ‚ààF,
x, y ‚ààV.
If V1, V2 are both linear spaces over the same Ô¨Åeld F and V1 ‚äÜV2, the space V1
is said to be a subspace of V2.
A.2.1.2
We say that a linear space is of dimension d < ‚àûif it has a basis of d
linearly independent elements (cf. A.1.1.2). If no such basis exists, the linear space is
said to be of inÔ¨Ånite dimension.
A.2.1.3
We assume for simplicity that F = R, although generalization to the com-
plex Ô¨Åeld presents no diÔ¨Éculty.
A familiar example of a linear space is the d-dimensional vector space Rd (cf.
A.1.1.1). Another example is the set PŒΩ of all polynomials of degree ‚â§ŒΩ with real
coeÔ¨Écients. It is not diÔ¨Écult to verify that dim PŒΩ = ŒΩ + 1.
More interesting examples of linear spaces are the set C[0, 1] of all continuous
real functions in the interval [0, 1], the set of all real power series with a positive
radius of convergence at the origin and the set of all Fourier expansions (that is,
linear combinations of 1 and cos nx, sin nx for n = 1, 2, . . .). All these spaces are
inÔ¨Ånite-dimensional.
A.2.1.4
Inner products and norms over linear spaces are deÔ¨Åned exactly as in
A.1.3.1 and A.1.3.3 respectively.
A linear space equipped with a norm is called a normed space. If a normed space
V is closed (that is, all Cauchy sequences converge in V), it is said to be a Banach
space.
An important example of a normed space is provided when a function is measured
by a p-norm. The latter is deÔ¨Åned by
‚à•f‚à•p =
	
‚Ñ¶
|f(x)|p dx

1/p
,
1 ‚â§p < ‚àû,
‚à•f‚à•‚àû= supx‚àà‚Ñ¶|f(x)|,
f ‚ààV,
where ‚Ñ¶is the domain of deÔ¨Ånition of the functions (in general multivariate).
A
normed space equipped with the p-norm is denoted by Lp(‚Ñ¶). If ‚Ñ¶is a closed set,
Lp(‚Ñ¶) is a Banach space.
A.2.1.5
A closed linear space equipped with an inner product is called a Hilbert
space. An important example is provided by the inner product
‚ü®f, g‚ü©=

‚Ñ¶
f(x)g(x) dx,
f, g ‚ààV,
where ‚Ñ¶is a closed set (if the space is over complex numbers, rather than reals, g(x)
needs to be replaced by g(x)). It induces the Euclidean norm (the 2-norm)
‚à•f‚à•=
	
‚Ñ¶
|f(x)|2 dx

1/2
,
f ‚ààV.

A.2
Analysis
441
A.2.1.6
The Hilbert space L2(‚Ñ¶) is said to be separable if it has either a Ô¨Ånite or a
countable orthogonal basis, {œïi}, say. (In inÔ¨Ånite-dimensional spaces the set {œïi} is a
basis if each element lies in the closure of linear combinations from {œïi}. The closure
is, of course, deÔ¨Åned by the underlying norm.)
The space L2(‚Ñ¶) (denoted also by L(‚Ñ¶) or L[‚Ñ¶]) is separable with a countable
basis.
A.2.1.7
Let V be a Hilbert space. Two elements f, g ‚ààV are orthogonal if ‚ü®f, g‚ü©= 0.
If
‚ü®f, g‚ü©= 0,
g ‚ààV1,
where V1 is a subspace of the Hilbert space V and f ‚ààV, then f is said to be orthogonal
to V1.
A.2.1.8
A mapping from a linear space V1 to a linear space V2 is called an operator.
An operator T is linear if
T (x + y) = T x + T y,
T (Œ±x) = Œ±T x,
Œ± ‚ààR,
x, y ‚ààV.
Let T be a linear operator from a Banach space V to itself. The norm of T is
deÔ¨Åned as
‚à•T ‚à•=
sup
x‚ààV, xÃ∏=0
‚à•T x‚à•
‚à•x‚à•=
sup
x‚ààV, ‚à•x‚à•=1
‚à•T x‚à•.
It is always true that
‚à•T x‚à•‚â§‚à•T ‚à•√ó ‚à•x‚à•,
x ‚ààV,
and
‚à•T S‚à•‚â§‚à•T ‚à•√ó ‚à•S‚à•,
where both T and S are linear operators from V to itself.
A.2.1.9
The domain of a linear operator T : V1 ‚ÜíV2 is the Banach space V1,
while its range is
T V1 = {T x : x ‚ààV1} ‚äÜV2.
‚Ä¢ If T V1 = V2, the operator T is said to map V1 onto V2.
‚Ä¢ If for every y ‚ààT V1 there exists a unique x ‚ààV1 such that T x = y then T is
said to be an isomorphism (or an injection) and the linear operator T ‚àí1y = x
is the inverse of T .
‚Ä¢ If T is an isomorphism and T V1 = V2 then it is said to be an isomorphism onto
V2 or a bijection.
A.2.1.10
A mapping from a linear space to the reals (or to the complex numbers) is
called a functional. A functional L is linear if L(x+y) = Lx+Ly and L(Œ±x) = Œ±Lx
for all x, y ‚ààV and scalar Œ±.

442
BluÔ¨Äer‚Äôs guide to useful mathematics
A.2.2
Approximation theory
A.2.2.1
Denote by PŒΩ the set of all polynomials with real coeÔ¨Écients of degree
‚â§ŒΩ. Given ŒΩ + 1 distinct points Œæ0, Œæ1, . . . , ŒæŒΩ and f0, f1, . . . , fŒΩ, there exists a unique
polynomial p ‚ààPŒΩ such that
p(Œæ‚Ñì) = f‚Ñì,
‚Ñì= 0, 1, . . . , ŒΩ.
It is called the interpolation polynomial.
A.2.2.2
Suppose that f‚Ñì= f(Œæ‚Ñì), ‚Ñì= 0, 1, . . . , ŒΩ, where f is a ŒΩ + 1 times diÔ¨Äer-
entiable function. Let a = mini=0,1,...,ŒΩ Œæi and b = maxi=0,1,...,ŒΩ Œæi. Then for every
x ‚àà[a, b] there exists Œ∑ = Œ∑(x) ‚àà[a, b] such that
p(x) ‚àíf(x) =
1
(ŒΩ + 1)!f (ŒΩ+1)(Œ∑)
ŒΩ

k=0
(x ‚àíŒæk).
(This is an extension of the familiar Taylor remainder formula and it reduces to the
latter if Œæ0, Œæ1, . . . , ŒæŒΩ ‚ÜíŒæ‚àó‚àà(a, b).)
A.2.2.3
An obvious way of evaluating an interpolation polynomial is by solving the
interpolation equations. Let p(x) = ŒΩ
k=0 pkxk. The interpolation conditions can be
written as
ŒΩ

k=0
pkŒæk
‚Ñì= f‚Ñì,
‚Ñì= 0, 1, . . . , ŒΩ,
and this is a linear system with a nonsingular Vandermonde matrix (A.1.2.5).
An explicit means of writing down the interpolation polynomial p is provided by
the Lagrange interpolation formula
p(x) =
ŒΩ

k=0
pk(x)fk,
x ‚ààR,
where each Lagrange polynomial pk ‚ààPŒΩ is deÔ¨Åned by
pk(x) =
ŒΩ

j=0, jÃ∏=k
x ‚àíxj
xk ‚àíxj
,
k = 0, 1, . . . , ŒΩ,
x ‚ààR.
A.2.2.4
An alternative method of evaluating the interpolation polynomial is the
Newton formula
p(x) =
ŒΩ

k=0
f[Œæ0, Œæ1, . . . , Œæk]
k‚àí1

j=0
(x ‚àíŒæk),
x ‚ààR,
where the deÔ¨Ånition of the divided diÔ¨Äerences f[Œæi0, Œæi1, . . . , Œæik] is given by recursion,
f[Œæi] = fi,
i = 0, 1, . . . , ŒΩ,
f[Œæi0, Œæi1, . . . , Œæik] = f[Œæi1, Œæi2, . . . , Œæik] ‚àíf[Œæi0, Œæi1, . . . , Œæik‚àí1]
Œæik ‚àíŒæi0
;

A.2
Analysis
443
here i0, i1, . . . , ik ‚àà{0, 1, . . . , ŒΩ} are pairwise distinct.
An equivalent deÔ¨Ånition of divided diÔ¨Äerences is that f[Œæ0, Œæ1, . . . , ŒæŒΩ] is the coeÔ¨É-
cient of xŒΩ in the interpolation polynomial p.
A.2.2.5
The practical evaluation of f[Œæ0, Œæ1, . . . , Œæk], k = 0, 1, . . . , ŒΩ is done in a
recursive fashion and it employs a table of divided diÔ¨Äerences, shown below. Only the
underlined divided diÔ¨Äerences are required for the Newton interpolation formula.
f[Œæ0]
f[Œæ1]
f[Œæ2]
f[Œæ3]
...
f[ŒæŒΩ]
f[Œæ0, Œæ1]
f[Œæ1, Œæ2]
f[Œæ2, Œæ3]
...
f[Œæ0, Œæ1, Œæ2]
f[Œæ1, Œæ2, Œæ3]
...
f[Œæ0, Œæ1, Œæ2, Œæ3]
...
XXX
z

:
XXX
z

:
XXX
z

:
XXX
z

:
XXX
z

:
XXX
z

:
The cost is O

ŒΩ2
operations.
An added advantage of the above procedure is that only 2ŒΩ numbers need be
stored, provided that overwriting is used.
A.2.2.6
A diÔ¨Äerentiable function f, deÔ¨Åned for x ‚àà(a, b), is said to be of variation
V [f] =
 b
a
|f ‚Ä≤(x)| dx.
The set of all f‚Äôs whose variation is bounded forms a linear space, denoted by V[a, b].
Let L be a linear functional from V[a, b]. We assume that f ‚ààCŒΩ+1[a, b], the linear
space of functions that are deÔ¨Åned in the interval [a, b] and possess ŒΩ + 1 continuous
derivatives there. Let us further suppose that
L
 b
a
f(x, Œæ) dŒæ =
 b
a
Lf(x, Œæ) dŒæ
for any bivariate function f such that f( ¬∑ , Œæ), f(x, ¬∑ ) ‚ààCŒΩ+1[a, b] and that L annihi-
lates all polynomials of degree ‚â§ŒΩ,
Lp = 0,
p ‚ààPŒΩ.
The Peano kernel of L is the function
k(Œæ) := L[(x ‚àíŒæ)ŒΩ
+],
Œæ ‚àà[a, b],

444
BluÔ¨Äer‚Äôs guide to useful mathematics
where
tm
+ :=
#
tm,
t ‚â•0,
0,
t < 0.
The Peano kernel theorem states that, as long as k is itself in V[a, b], it is true that
Lf = 1
ŒΩ!
 b
a
k(Œæ)f (ŒΩ+1)(Œæ) dŒæ,
f ‚ààCŒΩ+1[a, b].
The following bounds on the magnitude of Lf can be deduced from the Peano
kernel theorem:
|Lf| ‚â§1
ŒΩ!‚à•k‚à•1 √ó ‚à•f (ŒΩ+1)‚à•‚àû,
|Lf| ‚â§1
ŒΩ!‚à•k‚à•‚àû√ó ‚à•f (ŒΩ+1)‚à•1,
|Lf| ‚â§1
ŒΩ!‚à•k‚à•2 √ó ‚à•f (ŒΩ+1)‚à•2,
where ‚à•¬∑ ‚à•1, ‚à•¬∑ ‚à•2 and ‚à•¬∑ ‚à•‚àûdenote the 1-norm, the 2-norm and the ‚àû-norm,
respectively (A.2.1.4).
A.2.2.7
In practice, the main application of the Peano kernel theorem is in esti-
mating approximation errors. For example, suppose that we wish to make the approx-
imation
 1
0
f(Œæ) dŒæ ‚âà1
6

f(0) + 4f( 1
2) + f(1)

.
Letting
Lf =
 1
0
f(Œæ) dŒæ ‚àí1
6

f(0) + 4f( 1
2) + f(1)

we verify that L annihilates P3, therefore ŒΩ = 3. The Peano kernel is
k(Œæ) = L[(x ‚àíŒæ)3
+] =
 1
Œæ
(x ‚àíŒæ)3 dx ‚àí1
6

4( 1
2 ‚àíŒæ)2
+ + (1 ‚àíŒæ)3
=
1‚àí1
12Œæ3(2 ‚àí3Œæ),
0 ‚â§Œæ ‚â§1
2,
‚àí1
12(1 ‚àíŒæ)3(3Œæ ‚àí1),
1
2 ‚â§Œæ ‚â§1.
Therefore
‚à•k‚à•1 =
1
480,
‚à•k‚à•2 =
‚àö
14
1344,
‚à•k‚à•‚àû=
1
192
and we derive the following upper bounds on the error,
|Lf| ‚â§
1
1152‚à•f (iv)‚à•1,
‚àö
14
8064‚à•f (iv)‚à•2,
1
2880‚à•f (iv)‚à•‚àû,
f ‚ààC4[0, 1].

A.2
Analysis
445
A.2.3
Ordinary diÔ¨Äerential equations
A.2.3.1
Let the function f : [t0, t0 + a] √ó U ‚ÜíRd, where U ‚äÜRd, be continuous in
the cylinder
S = {(t, x) : t ‚àà[t0, t0 + a], x ‚ààRd, ‚à•x ‚àíy0‚à•‚â§b}
where a, b > 0 and the vector norm ‚à•¬∑ ‚à•is given. Then, according to the Peano
theorem (not to be confused with the Peano kernel theorem), the ordinary diÔ¨Äerential
equation
y‚Ä≤ = f(t, y),
t ‚àà[t0, t0 + Œ±],
y(t0) = y0 ‚ààRd,
where
Œ± = min
#
a, b
¬µ
$
and
¬µ =
sup
(t,x)‚ààS
‚à•f(t, x)‚à•,
possesses at least one solution.
A.2.3.2
We employ the same notation as in A.2.3.1.
A function f : [t0, t0 + a] √ó U ‚ÜíRd, where U ‚äÜRd, is said to be Lipschitz
continuous (with respect to a vector norm ‚à•¬∑ ‚à•acting on Rd) if there exists a number
Œª ‚â•0, a Lipschitz constant, such that
‚à•f(t, x) ‚àíf(t, y)‚à•‚â§Œª‚à•x ‚àíy‚à•,
x, y ‚ààS.
The Picard‚ÄìLindel¬®of theorem states that, subject to both continuity and Lipschitz
continuity of f in the cylinder S, the ordinary diÔ¨Äerential equation has a unique
solution in [t0, t0 + Œ±].
If f is smoothly diÔ¨Äerentiable in S then we may set
Œª = max
(t,x)‚ààS
----
‚àÇf(t, x)
‚àÇx
---- ;
therefore smooth diÔ¨Äerentiability is suÔ¨Écient for the existence and uniqueness of the
solution.
A.2.3.3
The linear system
y‚Ä≤ = Ay,
t ‚â•t0,
y(t0) = y0,
always has a unique solution.
Suppose that the d √ó d matrix A possesses the spectral factorization A = V DV ‚àí1
(A.1.5.4). Then there exist vectors Œ±1, Œ±2, . . . , Œ±d ‚ààRd such that
y(t) =
d

‚Ñì=1
eŒª‚Ñì(t‚àít0)Œ±‚Ñì,
t ‚â•t0,
where Œª1, Œª2, . . . , Œªd are the eigenvalues of A.

446
BluÔ¨Äer‚Äôs guide to useful mathematics
Bibliography
BirkhoÔ¨Ä, G. and Rota, G.-C. (1989), Ordinary DiÔ¨Äerential Equations (4th edn), Wiley, New
York.
Bollob¬¥as, B. (1990), Linear Analysis: An Introductory Course, Cambridge University Press,
Cambridge.
Boyce, W.E. and DiPrima, R.C. (2001), Elementary DiÔ¨Äerential Equations (7th edn), Wiley,
New York.
Davis, P.J. (1975), Interpolation and Approximation, Dover, New York.
Powell, M.J.D. (1981), Approximation Theory and Methods, Cambridge University Press,
Cambridge.
Rudin, W. (1990), Functional Analysis (2nd edn), McGraw‚ÄìHill, New York.

Index
A-acceptability, 63, 68, 72
A-stability, 56‚Äì68, 69, 71, 72, 77, 113,
122, 127, 132, 134, 360
A(Œ±)-stability, 68
abelian group, 428
acceleration schemes, 133
accuracy, see Ô¨Ånite elements
active modes, 390
Adams method, 19‚Äì20, 28, 66
Adams‚ÄìBashforth, 20, 21, 23, 25,
26, 28, 31, 66, 110, 115, 116,
121, 127, 134, 376, 377, 410
Adams‚ÄìMoulton, 26, 31, 66, 119,
121
advection equation, 357, 387‚Äì403, 411,
412, 415, 418‚Äì420, 423, 425
several space variables, 387, 423
variable coeÔ¨Écient, 387, 405
vector, 388, 407
aerodynamics, 418
Airy equation, 98
Alekseev‚ÄìGr¬®obner lemma, 45
algebraic geometry, 73
algebraic stability, 77‚Äì83
algebraic topology, 73
alternate directions implicit method, 287
analytic function, 3, 140, 141, 341, 342
theory, 73
analytic geometry, 194
analytic number theory, 68
angled derivative method, 397, 419‚Äì421,
423
Anna Karenina, 139
approximation theory, 35, 68, 189, 442‚Äì
444
Arnoldi method, 326
arrowhead matrix, 240‚Äì242
artiÔ¨Åcial viscosity, 416
atom bomb, 129
automorphic form, 133
autonomuous ODE, 18, 40
B-spline, see spline
backward diÔ¨Äerentiation formula (BDF),
26‚Äì28, 32, 67, 96, 113, 121
backward error analysis, 91
Banach Ô¨Åxed point theorem, 125
band-limited function, 212
banded systems, see Gaussian elimina-
tion
bandwidth, 234, 235, 237‚Äì239, 243, 247
basin of attraction, 131
basis, 230, 428, 438, 440
orthogonal, 433
biharmonic
equation, 165, 169, 349
operator, 199, 202
bijection, see isomorphism
bilinear form, 185, 188, 221
binary representation, 218
boundary conditions, 181, 339, 340, 350‚Äì
353, 358, 360‚Äì362, 377, 378,
381, 387, 388, 392, 395, 419
artiÔ¨Åcial, 405
Dirichlet, 129, 147, 151, 154, 156,
165, 166, 171, 176, 195, 202,
225, 293, 294, 331, 335, 336,
341, 344, 345, 377, 400, 402‚Äì
403, 407, 411, 414, 422, 423
essential, 182‚Äì184
mixed, 165
natural, 181, 182, 184, 338, 345
Neumann, 165, 225, 337, 345
periodic, 177, 219, 225, 231, 294,
336‚Äì338, 377, 388‚Äì390, 395,
399‚Äì402
447

448
Index
zero, 179, 356, 359, 360, 368, 383,
391, 392, 400, 405, 406, 411,
422
boundary element method, 361
boundary elements, 139, 165, 201
box method, 423
Burgers equation, 388‚Äì390, 413‚Äì418, 420,
425
Butcher, John, 95
C++, 334
cardinal function, 192, 193
pyramid function, 192, 203
Cartesian coordinates, 336, 337, 339
Cauchy
problem, 350, 372, 375, 377, 392,
395, 398, 410, 411, 414, 422‚Äì
424
sequence, 126, 440
Cauchy‚ÄìRiemann equations, 341, 342
Cauchy‚ÄìSchwarz inequality, 406, 433
Cayley‚ÄìHamilton theorem, 328
C¬¥ea lemma, 188
celestial mechanics, 96, 98, 107
chaotic solution, 73, 75
chapeau function, 176, 182, 183, 190,
191, 196, 202, 205‚Äì207
characteristic
equation, 377
function, 375, 376
polynomial, 264, 437
characteristics, 415‚Äì417, 420, 421
Chebyshev method, see spectral method
Chebyshev polynomial, see orthogonal
polynomials
chemical kinetics, 56
chequerboard, 237
Cholesky factorization, see Gaussian
elimination
circulant, see matrix
classical solution, 174, 175
coarsening, 298, 299
coding theory, 35, 216
coercive operator, see linear operator
Cohn‚ÄìSchur criterion, 67, 68, 276
Cohn‚ÄìLehmer‚ÄìSchur criterion, 67
collocation, 43‚Äì47, 50, 52, 172, 201
parameters, 43
combinatorial algorithm, 238, 243
combinatorics, 35
combustion theory, 233
commutator, 380
completeness, 179
complex dynamics, 133
complexity, 307
composition method, 94, 98
computational dynamics, 77, 95
computational Ô¨Çuid dynamics, 120
computational grid, 148, 150, 155, 165,
170, 260, 281, 334, 337
curved, 342
honeycomb, 164
computational stencil, 149, 159, 165,
167, 301
computer science, 216
conformal mapping, 341, 343
theorem, 341
conjugate direction, 312‚Äì315, 328
conjugate gradients (CGs), 133, 246,
287, 306, 309‚Äì327, 328
preconditioned (PCGs), 246, 320‚Äì
323, 324, 325, 329
standard form, 316, 317
conservation
law, see nonlinear hyperbolic
conservation law
volume, 98
conservative method, 419
contractivity, 309
control theory, 56, 68, 216
convection‚ÄìdiÔ¨Äusion equation, 383
convergence, 360
Ô¨Ånite elements, 183, 189
linear algebraic systems, 251‚Äì256,
258, 261, 262, 270, 275, 277,
284
locally uniform, 30
multigrid, 302, 307
ODEs, 6, 9, 10, 14, 23‚Äì25
PDEs, 352‚Äì354, 359‚Äì361, 382, 383,
403, 425

Index
449
waveform relaxation, 382
convolution, 221
corrector, 131, 134
cosmological Big Bang equations, 56
Courant‚ÄìFriedrichs‚ÄìLewy (CFL)
condition, 412, 424
Courant number, 351, 353, 358‚Äì360, 368,
375, 391, 395, 397, 398, 423,
424
Crandall method, 385
Crank‚ÄìNicolson method
advection equation, 396, 397, 399
diÔ¨Äusion equation, 365, 367, 371,
376‚Äì381, 383, 384
Curtiss‚ÄìHirschfelder equation, 107, 113,
114, 118
curved
boundary, 147, 155, 156, 164, 165
geometry, 200
Cuthill‚ÄìMcKee algorithm, 244
cyclic matrix, 240, 241
cyclic odd‚Äìeven reduction and
factorization, 342, 344
Dahlquist, Germund, 30, 95
Dahlquist
equivalence theorem, 25, 30, 360
Ô¨Årst barrier, 25, 30
second barrier, 67
defect, 45, 172‚Äì175, 183
descent condition, 310
determinant, 60, 430
dexpinv equation, 96
diameter, 190, 194, 196
diÔ¨Äerence equation, see linear diÔ¨Äerence
equation
diÔ¨Äerential algebraic equation, 95, 96
diÔ¨Äerential geometry, 73, 97
diÔ¨Äerential operator, see Ô¨Ånite diÔ¨Äer-
ences, linear operator
diÔ¨Äusion equation, 158, 347‚Äì386, 393
forcing term, 350, 364
‚Äòreversed-time‚Äô, 357
several space variables, 350, 364,
367, 378‚Äì381
variable coeÔ¨Écient, 350, 364, 367,
372, 380, 385
diÔ¨Äusive processes, 349
digraph, 271, 272, 279
dimension, 428, 440
direct solution, see Gaussian elimina-
tion
discrete Fourier transform (DFT), 214‚Äì
217, 219, 335, 342, 402
dissipative
ODE, 77
PDE, 419
divergence theorem, 184, 185
divided diÔ¨Äerences, 443
table, 443
domain decomposition, 165
domain of dependence, 411‚Äì413
downwind scheme, see upwind scheme
eigenfunction, 153, 154
eigenspace, 437, 438
eigenvalue, 437‚Äì439
analysis, 368‚Äì372, 382, 386, 391,
393, 397, 402, 403, 405, 423
Einstein equations, 419
elasticity theory, 200, 349
electrical engineering, 107, 120
element, 189‚Äì193
curved boundary, 201
quadrilateral, 190, 199‚Äì200, 203
tetrahedral, 203
triangular, 190, 192‚Äì199, 203, 247
elementary diÔ¨Äerential, 49
elliptic membrane, 107
elliptic operator, see linear operator
elliptic PDEs, 139, 165, 186, 200, 219‚Äì
221, 233, 349
energy, 419
energy method, 382, 384, 403‚Äì407
Engquist‚ÄìOsher switches, 421
entropy condition, 416, 418, 425
envelope, 243, 244
epidemiology, 350
equivalence class, 187
ERK, see Runge‚ÄìKutta method
error constant, see local error

450
Index
error control, 46, 105‚Äì122
per step, 108
per unit step, 108
error function, 230
Escher‚Äôs tessalations, 165
Euler, Leonhard, 286
Euler equations, 413, 419
Euler‚ÄìLagrange equation, 178, 180‚Äì182,
186, 200, 344
Euler‚ÄìMaclaurin formula, 15
Euler method, 4‚Äì8, 10‚Äì13, 15, 16, 19,
20, 53‚Äì57, 59, 60, 63, 113, 127,
364, 396, 409, 410
advection equation, 391, 398, 402,
411, 422, 425
backward, 15, 27, 410
diÔ¨Äusion equation, 351‚Äì355, 358,
362, 364, 365, 367, 371, 377,
378, 385, 422
evolution operator, 355
evolutionary PDEs, 349, 351, 354‚Äì362,
372, 378, 381, 382, 391, 419
explicit method
ODEs, 10, 21, 123, 131
PDEs, see full discretization
exponential
of a matrix, 29, 379, 384‚Äì386
of an operator, 29, 30
exterior product, 91, 97
extrapolation, 15, 119, 122
fast cosine transform, 228
fast Fourier transform (FFT), 214‚Äì218,
221, 224, 228, 331, 337, 340,
342, 344, 401
fast Poisson solver, 243, 331‚Äì345
fast sine transform, 228
Feng, Kang, 96
Ô¨Åll-in, see Gaussian elimination
Ô¨Ånancial mathematics, 350
Ô¨Ånite diÔ¨Äerence method, 120, 128, 139‚Äì
170, 171, 196, 200, 201, 205,
212, 222, 226, 228, 233, 234,
260, 291, 307, 331, 339, 344,
350, 352, 355‚Äì363, 366, 381,
382, 391, 393‚Äì403, 407, 421
Ô¨Ånite diÔ¨Äerence operator, 140‚Äì147, 166,
167, 362, 363, 391, 408
averaging, 140, 141, 143‚Äì145
backward diÔ¨Äerence, 140‚Äì142, 391
central diÔ¨Äerence, 140‚Äì146, 156, 167,
169, 263, 287, 337, 339, 351,
364, 408
diÔ¨Äerential, 29, 30, 140‚Äì142, 173‚Äì
175
forward diÔ¨Äerence, 140‚Äì142, 145,
146, 167, 351, 391
shift, 29, 30, 140, 141, 362
Ô¨Ånite element method (FEM),
98,
139,
165, 171‚Äì203, 205, 212, 221,
222, 226, 233, 234, 239, 260,
291, 307, 339, 344, 421
accuracy, 191‚Äì193, 196, 199, 203
basis functions, 174, 183
piecewise diÔ¨Äerentiable, 174, 179
piecewise linear, 176, 177, 182,
192‚Äì196, 199, 203, 239, 247
error bounds, 201
for PDEs of evolution, 361
functions, 171, 344
h-p formulation, 201
in multivariate spaces, 201
smoothness, 191‚Äì193, 196, 199, 200,
203
theory, 184‚Äì192
Ô¨Ånite volume method, 421
Fisher equation, 129
Ô¨Åve-point formula, 149‚Äì158, 159, 161‚Äì
163, 165, 167, 169, 196, 234,
236, 239, 244, 260, 280, 284,
287, 289, 291, 293, 296, 299,
300, 306, 307, 323, 331, 332,
364
Ô¨Åxed point, 309
Ô¨Çop, 334
Ô¨Çow
isospectral, 76, 85
map, 87, 89
orthogonal, 85, 86, 96
Ô¨Çuid dynamics, 413, 418

Index
451
Ô¨Çux, 418
forcing term, 350, 356, 358, 360, 371,
378, 381
FORTRAN, 334
Fourier
analysis, 225, 372‚Äì378, 382, 386,
391‚Äì394, 397, 398, 400, 402,
403, 405, 422, 424
boundary conditions, 377, 382
approximation, 210
coeÔ¨Écients, 210‚Äì212, 214, 219, 220,
226, 230, 337‚Äì340, 345
expansion,
206, 208, 210‚Äì214,
219,
220, 223, 225, 227, 228, 230,
356, 440
series, 206, 207, 211, 226, 230, 340
space, 410
transform, 163, 164, 217, 294, 337‚Äì
340, 373, 374, 376, 377, 391,
419
bivariate, 293, 294
inverse, 338, 374, 401
fractals, 125, 133
full discretization (FD), 357, 358, 360‚Äì
362, 364‚Äì368, 371, 372, 375,
377, 381, 383, 385, 386, 392,
394‚Äì399, 402, 410, 412, 419‚Äì
421, 423, 424
explicit, 366, 425
implicit, 365, 366, 371, 378
multistep, 365, 376, 377, 397, 410
full multigrid, see multigrid
functional, 179, 181, 194, 195, 203, 442
linear, 442, 443
functional analysis, 35, 73, 184, 191,
287, 439‚Äì442
functional iteration, 123‚Äì127, 128, 130‚Äì
134, 251, 303
Galerkin
equations, 172, 174, 181, 188, 192
method, 171, 183, 188, 189, 202,
361
solution, 189
gas dynamics, 416, 418
Gauss, Carl Friedrich, 228, 286
Gauss‚ÄìSeidel method, 259‚Äì270, 275, 277,
281‚Äì288, 291‚Äì300, 302, 303,
306‚Äì308, 323, 334
tridiagonal matrices, 256‚Äì258
Gaussian elimination, 124, 128, 130, 212,
233‚Äì248, 434‚Äì437
banded system,
233‚Äì239, 243,
244,
246, 247, 333, 334, 340, 378,
380
Cholesky factorization, 238‚Äì243, 246,
248, 254, 323, 329, 436, 437
Ô¨Åll-in, 237, 239, 242, 243, 245
LU factorization, 130, 132, 235‚Äì
239, 242, 243, 246, 254, 255,
436, 437
perfect factorization,
239, 242‚Äì
245,
248, 323
pivoting, 234, 235, 242, 246, 435,
436
storage, 235, 236, 238
Gaussian quadrature, see quadrature
Gear automatic integration, 119‚Äì120
Gegenbauer Ô¨Åltering, 226
general linear method, 69
generalized binomial theorem, 143
generalized minimal residuals (GMRes),
246, 326
geometric numerical integration, 73‚Äì98
geophysical Ô¨Çuid dynamics, 233
GerÀásgorin
criterion, 157, 168, 202, 257, 261,
263, 372, 394
disc, 158, 168, 372
Gibbs eÔ¨Äect, 177, 207
global error, 8, 106, 108, 110, 113
constant, 108
Godunov method, 416‚Äì418, 420, 425
graph, 245
connected, 247
directed, 244, see digraph
disconnected, 247
edge, 48, 239, 241, 245, 247, 279

452
Index
of a matrix, 239‚Äì245, 247, 254, 271,
323
order of, 49
path, 241, 247
simple, 241
tree, 49‚Äì50, 241, 242, 244, 245, 248,
254, 323
equivalent trees, 49
monotonically ordered, 241, 242,
245, 248
partitioned, 242, 245
root, 241, 245
rooted, 49, 241, 242
vertex, 48, 239, 241, 242, 245, 247,
279
predecessor, 241
successor, 241
graph theory, 238‚Äì244, 270
for RK methods, 41, 42, 48‚Äì50
Green formula, 185
grid ordering, 236, 237, 239, 333
natural, 236, 247, 280, 281, 287,
331, 333, 334, 370, 379
red‚Äìblack, 237, 238, 280, 281
group action, 85
Hamiltonian
energy, 87, 90, 102
kinetic, 94
potential, 94
separable, 94, 102, 103
generating functions, 91, 97
in Lagrangian formulation, 97, 98
system, 74, 87‚Äì95, 97, 98, 102
harmonic
analysis, 73, 217
function, 342
oscillator, 87
harmonics, see Fourier analysis
hat function, see chapeau function
heat conduction equation, see diÔ¨Äusion
equation
Heisenberg‚Äôs uncertainty principle, 234
Helmholtz equation, 342, 344
H¬¥enon‚ÄìHeiles equations, 102
Hessian matrix, 181
hierarchical
bases, 201, 344
grids, 298, 299, 301, 302, 307
mesh, 195, 197, 198
highly oscillatory
component, 295‚Äì298, 302, 303, 308
diÔ¨Äerential equation, 98
Hockney method, 331‚Äì336, 340, 342,
344
Hopf algebra, 50
hydrogen atom, 234
hyperbolic PDEs,
139, 147, 158, 349,
377, 382, 387‚Äì425
IEEE arithmetic, 55
ill-conditioned system, 128
ill-posed equation, 356, 357
image processing, 350
implicit function theorem, 14, 30, 125
implicit method
ODEs, 10, 21, 121, 123, 131
PDEs, see full discretization
implicit midpoint rule, 90
incomplete LU factorization (ILU), 254,
281, 282, 284, 307
injection, see isomorphism
inner product, 35, 171, 172, 175, 186,
187, 432‚Äì434, 440
Euclidean, 173, 184, 368, 433
semi-inner product, 202
integration by parts, 173, 174, 180, 182,
183, 190
internal interfaces, 200
interpolation, see polynomial interpo-
lation
invariant, 73, 76
Ô¨Årst integral, 73, 98, 100
quadratic, 83‚Äì86
symplectic, 91
irregular tessalation, 238
isometry, 373‚Äì376
isomorphism, 215, 216, 373, 374, 434,
441
iterated map, 309
iteration matrix, 252, 255, 274, 285,
289, 291, 306, 327

Index
453
iteration to convergence, 131, 132
iterative methods, see conjugate gra-
dients, Gauss‚ÄìSeidel method,
Jacobi
method, successive over-relaxation
Chebyshev, 328
linear, 251, 252
linear systems, 238, 243, 246, 251‚Äì
290
nonlinear systems, 123‚Äì135, 251
one-step, 251, 252, 309, 325, 327
stationary, 251, 252, 309, 325, 327
Jacobi, Carl Gustav Jacob, 286
Jacobi method, 259‚Äì270, 274‚Äì277, 281‚Äì
283, 286, 288, 289, 296, 307,
308, 323
tridiagonal matrices, 256‚Äì258
Jacobi over-relaxation (JOR), 307
Jacobian matrix, 56, 57, 125, 128‚Äì130,
132
Jordan
block, 439
factorization, 57, 70, 252, 288, 439
KdV equation, 388, 390
kernel, 216, 434
Kolmogorov‚ÄìArnold‚ÄìMoser (KAM)
theory, 91
Korteweg‚Äìde-Vries equation, see KdV
Kreiss matrix theorem, 382
Kronecker product, 124
L2, see inner product, space
‚Ñì2, see inner product, space
L-shaped domain, 155, 167, 195, 281,
290
Lagrange, Joseph Louis, 286
Lagrange polynomial, see polynomial
interpolation
Lanczos, Cornelius, 228
Laplace
equation, 150‚Äì151, 160, 165, 166,
342
operator, 153, 154, 158, 169, 185,
186, 188, 192, 199, 342, 364
eigenfunctions, 220
eigenvalues, 153
Lax equivalence theorem, 360‚Äì362, 371
Lax‚ÄìFriedrichs method, 423
Lax‚ÄìMilgram theorem, 188
Lax‚ÄìWendroÔ¨Ämethod, 397, 399, 400,
402, 403, 419‚Äì421, 423
leapfrog method, 72
advection equation, 397, 403‚Äì405,
419‚Äì421, 423
diÔ¨Äusion equation, 365‚Äì367, 377,
386
wave equation, 410‚Äì413, 424
least action principle, 179
Lebesgue measure, 187
Legendre, Adrian Marie, 286
Liapunov exponent, 57
Lie algebra, 86, 96, 101
Lie group, 86, 96, 101
equation, 86, 96
method, 95
orthogonal group, 86
special linear group, 86
line relaxation, 281‚Äì287, 307, 308
linear operator
inverse, 441
positive deÔ¨Ånite, 187
linear algebraic systems, 434‚Äì437
homogeneous, 434
sparse, see sparsity
linear diÔ¨Äerence equation, 64, 264, 266,
339, 377
linear independence, 428
linear ODEs, 28, 45, 53‚Äì59, 62, 70, 72,
122, 127, 374, 384, 387, 445
inhomogeneous, 71
linear operator, 140, 186, 188, 441
bounded, 30, 188, 189, 192
coercive, 188, 189, 192
diÔ¨Äerential, 184, 185, 188, 355
domain, 441
elliptic, 185‚Äì188, 202
positive deÔ¨Ånite, 185, 186, 188, 202,
221
range, 441

454
Index
self-adjoint, 185‚Äì188
linear space, see space
linear stability domain, 56‚Äì59, 60, 68,
70‚Äì72, 118, 128, 132, 399
weak, 72
linear transformation, 430, 434
Lipschitz
condition, 3, 6, 9, 11, 12, 15, 175,
406
constant, 3, 445
continuity, 445
function, 6, 17, 19, 131, 206, 207
local attenuation factor, 294, 308
local error, 106, 113, 119
constant, 108, 121, 397
local reÔ¨Ånement, 190
LU factorization, see Gaussian
elimination
manifold, 83, 86, 96
Maple, 120
Mathematica, 120
mathematical biology, 56, 129
mathematical physics, 35, 85, 86, 158
Mathieu equation, 107, 110, 112, 117
MATLAB, 120, 207
matrix, 429‚Äì432
adjugate, 60, 431
bidiagonal, 393
circulant, 400, 419, 423
matrix (cont.)
diagonal, 431
Hermitian, 431
identity, 430
inverse, 431
involution, 332
irreducible, 261
normal, 368‚Äì370, 372, 385, 393, 397,
402, 406, 419, 423, 432, 438
orthogonal, 85, 431
positive deÔ¨Ånite, 238, 239, 242, 246,
248, 255‚Äì257, 261, 309, 310,
312, 317, 323, 326, 328, 434
quindiagonal, 234, 240, 241, 243
self-adjoint, 431
skew-Hermitian, 431
skew-symmetric, 431
strictly diagonally dominant, 261,
262
symmetric, 431
Toeplitz, 264, 266, 289, 372
Toeplitz, symmetric and tridiago-
nal (TST), 261, 264, 266, 270,
275, 284, 285, 288, 310, 315,
321, 328, 331‚Äì336, 342, 370,
371, 378, 394
block, 285, 331, 335, 336, 342,
343
eigenvalues, 264, 332
tridiagonal, 129, 176, 234, 236, 240‚Äì
242, 256‚Äì259, 261, 264, 266,
272, 275, 285, 288, 289, 335,
340, 378‚Äì380, 432
unitary, 402, 431
mean value theorem, 126
measure theory, 187
mechanical system, 94
mechanics, 171, 349
Mehrstellenverfahren, see nine-point
formula
meshless method, 139
method of lines, see semi-discretization
metric, 30
microcomputer, 234
midpoint rule
explicit, 72, 365, 366, 397
implicit, 13, 16, 47, 58
Milne device, 107‚Äì113, 116, 121, 131,
134
Milne method, see multistep method
minimal residuals method (MR), 326
modulated Fourier expansion, 91
Monge cone, see domain of dependence
monotone equation, 77, 78, 81, 95, 99
monotone ordering, see graph
Monte Carlo method, 165‚Äì166
multigrid, 246, 298‚Äì308, 323, 334, 344,
383
full multigrid, 302‚Äì303, 305, 306
V-cycle, 301‚Äì304, 306, 307
W-cycle, 307

Index
455
multiplicity
algebraic, 437, 438
geometric, 438
multipole method, 165
multiscale, 307
multistep method, 4, 19‚Äì32, 69, 72, 79,
84, 95, 98, 107‚Äì113, 119‚Äì121,
123, 124, 127, 128, 131, 134,
360, 362
A-stability, 63‚Äì68
Adams, see Adams method
explicit midpoint rule, 32
Milne, 26, 32
Nystrom, 26, 32
three-eighths scheme, 32
natural ordering, see grid ordering
natural projection, 341
Navier‚ÄìStokes equations, 200
nested iteration, 303, 307
nested subsystems, 344
New York Harbor Authority, 166
Newton, Sir Isaac, 286
Newton interpolation formula,
see polynomial interpolation
Newton‚ÄìRaphson method, 127‚Äì130, 133,
134, 251, 303
modiÔ¨Åed, 128‚Äì134
nine-point formula, 159‚Äì163, 196, 247,
331, 332
modiÔ¨Åed, 162‚Äì163, 165, 169, 331
nonlinear
algebraic equations, 42
algebraic systems, 123‚Äì135, 302
dynamical system, 73, 95
hyperbolic conservation law, 390,
413, 418‚Äì421
operator, 188
PDEs, 139, 200, 403, 413, 414, 419
stability theory, 58, 69, 95
nonuniform mesh, 165
Nordsieck representation, 120
norm, 6, 171, 384, 393, 399, 432‚Äì434
Chebyshev, 433
Euclidean, 54, 127, 134, 154, 155,
180, 184, 192, 196, 202, 253,
286, 291, 293, 294, 308, 310,
356‚Äì358, 368, 372‚Äì374, 383,
385, 393, 406, 411, 419, 433,
434, 437, 438, 441
function, 356, 358, 373
Manhattan, 433
matrix, 16, 368, 369, 394, 434
operator, 356, 441
p-norm, 440
Sobolev, 188, 196
vector, 125, 358, 372, 445
normal matrix, see matrix
number theory, 86
Numerov method, 424
Nystrom method, see multistep method
ODE theory, 445
operator theory, 73
optimization, 309
optimization theory, 133
order
ODEs, 8, 351, 365, 366, 385, 396
Adams‚ÄìBashforth methods, 20
Euler method, 8
multistep methods, 21‚Äì26, 108
Runge‚ÄìKutta methods, 39‚Äì42, 44,
46‚Äì48, 50‚Äì52
PDEs, 352, 355‚Äì362, 395, 419
Euler method, 351, 353, 365, 371
full discretization, 358‚Äì360, 365‚Äì
367, 385, 395, 397, 402, 403,
410, 411, 423, 424
semi-discretization, 361‚Äì365, 376,
385, 395, 396, 405, 408‚Äì410
quadrature, 34, 46, 50, 51
rational functions, 62, 366
second-order ODEs, 410, 424
order stars, 68
ordering vector, 271, 272, 279, 280, 289,
290
compatible, 271‚Äì277, 279‚Äì282, 289,
290
orthogonal polynomials, 35‚Äì37, 47, 48
Chebyshev, 35, 51, 222, 224, 327
expansion, 223, 224

456
Index
points, 224, 229
classical, 35
Hermite, 35
Jacobi, 35
Laguerre, 35
Legendre, 35, 52
orthogonal residuals method (OR), 326
orthogonal vector, 433
eigenvector, 264, 369
orthogonality, 172, 173, 175, 183, 222,
313‚Äì315, 328, 441
Pad¬¥e approximation, 62, 68
to ez, 62‚Äì63, 72, 379, 381
parabolic PDEs, 56, 128, 139, 158, 233,
349, 377
quasilinear, 129
parallel computer, 234, 334, 383
particle method, 139, 421
partition, 190, 192, 279, 280
Pascal, 334
Peano
kernel theorem, 7, 15, 33, 443‚Äì445
theorem, 445
PECE iteration, 131, 134
PE(CE)2, 132
Penrose tiles, 165
perfect elimination, see Gaussian
elimination
permutation, 151, 152, 241, 245, 271‚Äì
273, 431
parity of, 431
permutation matrix, 239, 247, 261, 271,
432
phase diagram, 106
Picard‚ÄìLindel¬®of theorem, 445
pivot, see Gaussian elimination
Poincar¬¥e
inequality, 188
section, 106
theorem, 89
point
boundary, 148, 168
internal, 148, 149, 156, 157, 168,
170
near-boundary,
148, 149, 155,
157,
159, 337
Poisson equation, 147‚Äì171, 192‚Äì200, 203,
225, 233, 239, 244, 247, 281‚Äì
287, 292, 303‚Äì306, 323‚Äì325,
331, 335, 341, 349, 362, 378
analytic theory, 163
in a cylinder, 345
in a disc, 336‚Äì342, 345
in an annulus, 345
in three variables, 335, 344, 345
Poisson integral formula, 345
polar coordinates, 336, 337, 339
polygon, 190
polynomial
minimal, 328
multivariate, 191
total degree, 191
polynomial interpolation, 20, 29, 119,
193, 442‚Äì443
Hermite, 199
in three dimensions, 203
in two dimensions, 193, 196, 199,
203
interpolation equations, 442
Lagrange, 20, 34, 43, 82, 442
Newton, 141, 442, 443
polynomial subtraction, 226
polytope, 190
population dynamics, 349
Powell, Michael James David, 325
preconditioned (PCGs), see conjugate
gradients (CGs)
predictor, 131, 134
prolongation matrix, 300, 301
linear interpolation, 301
property A, 279‚Äì282
pseudospectral method, 139, 228, 229
pseudospectrum, see spectrum
pyramid function, see cardinal function
quadrature, 33‚Äì37, 38, 45, 46, 48, 51,
175, 194, 381
Gaussian, 37, 42, 51, 82, 102
Newton‚ÄìCotes, 34

Index
457
nodes, 33, 46
weights, 33
quantum
chemistry, 96, 98
computers, 234
groups, 35
mechanics, 388, 418
quasilinear PDEs, 139
Rankine‚ÄìHugoniot condition, 415, 416,
418
rarefaction region, 415, 416, 425
rational function, 59‚Äì61
reaction‚ÄìdiÔ¨Äusion equation, 129
reactor kinetics, 56
reactor physics, 96
red‚Äìblack ordering, see grid ordering
re-entrant corner, 195
reÔ¨Ånement, 298, 299
regular splitting, 255, 256, 259, 288
relativity theory, 418
representation of groups, 35
residual, 291
restriction matrix, 299, 300
full weighting, 301, 308
injection, 301
Richardson‚Äôs extrapolation, 15
Riemann
problem, 416
sum, 213, 358, 407
surface, 67
theta function, 390
Ritz
equations, 181, 188, 192, 202, 221,
344
method, 180‚Äì183, 186, 187
problem, 194
root condition, 24, 25, 28, 377
root of unity, 213‚Äì215, 378, 401
roundoÔ¨Äerror, 23, 54, 63, 233, 315, 324
Routh‚ÄìHurwitz criterion, 68
Runge‚ÄìKutta method (RK), 4, 13, 32‚Äì
52, 69‚Äì72, 79, 81‚Äì86, 91, 93,
97, 98, 100‚Äì102, 113, 122‚Äì124,
127
A-stability, 59‚Äì63, 68
classical, 40
embedded, 113‚Äì118, 121
explicit (ERK), 38‚Äì41, 51, 52, 60,
72
Fehlberg, 116
Gauss‚ÄìLegendre, 47, 61‚Äì63, 82‚Äì84,
90, 94
implicit (IRK), 41‚Äì47, 52, 123
Lobatto, 100
Munthe-Kaas, 96
Nystrom, 41, 90
partitioned, 94, 97, 98
Radau, 100
RK matrix, 38, 49
RK nodes, 38
RK tableau, 39, 42
RK weights, 38, 49
symplectic, see symplectic method
scalar Ô¨Åeld, 439
Schr¬®odinger equation, 419
second-order ODEs, 409‚Äì410, 424
semi-discretization (SD), 128, 129, 360‚Äì
368, 370‚Äì378, 381, 382, 385,
394‚Äì398, 402, 405‚Äì412, 423
semigroup, 356
separation of variables, 356, 357
shift operator, see Ô¨Ånite diÔ¨Äerence method
shock, 415‚Äì417, 420, 421, 425
shock tube problem, 416
signal processing, 212, 216
similarity transformation, 271
skew-gradient equation, 100
smoother, 296, 308
smoothness, see Ô¨Ånite elements
Sobolev space, see space
software, 105‚Äì107
solitons, 390, 419
space
aÔ¨Éne, 179, 181, 184, 189
Banach, 440, 441
Ô¨Ånite element, 190, 191, 196
function, 187
Hilbert, 440, 441
separable, 441
inner product, 172

458
Index
Krylov, 317‚Äì323, 325, 326, 328
linear, 171, 172, 174, 179, 184, 189,
214, 216, 230, 313, 358, 368,
428‚Äì429, 439, 440, 442
normed, 187, 356, 440
Sobolev, 184, 200, 201
subspace, 428, 440, 441
Teichm¬®uller, 133
span, 172, 313
sparsity, 128, 150, 205, 221, 234, 235,
243, 244
sparsity pattern, 239, 240, 243, 247,
248, 260, 270‚Äì272, 279, 280,
289, 379
sparsity structure, symmetric, 242, 244,
254, 271, 279
spectral
abscissa, 370, 385, 386
basis, 221
condition number, 369
convergence, 210, 212, 220, 222,
225, 228, 229
elements, 139
factorization, 53, 57, 438, 445
method, 139, 165, 175‚Äì178, 201,
205‚Äì229, 231, 260, 361, 421
Chebyshev method, 222‚Äì225, 229,
231, 339
radius, 127, 155, 252, 253, 263, 268,
269, 277, 279, 285, 286, 291,
306, 368‚Äì370, 386, 393, 437
theory, 73
spectrum, 252, 263, 437‚Äì439
pseudospectrum, 419
spline, 196
splitting, 378‚Äì381
Gauss‚ÄìSeidel, see Gauss‚ÄìSeidel
method
Jacobi, see Jacobi method
Strang, 381, 386
stability, see A(Œ±)-stability, A-stability,
root condition
stability (cont.)
in dynamical systems, 360
in Ô¨Çuid dynamics, 360
in logic, 360
PDEs of evolution, 217, 355‚Äì362,
365, 368‚Äì378, 382, 384‚Äì386,
391, 393‚Äì395, 397‚Äì407, 410‚Äì
412, 418, 419, 422‚Äì424
statistics, 35
steady-state PDEs, 349
steepest descent method, 310, 311, 312,
315
Stein‚ÄìRosenberg theorem, 262
stiÔ¨Äequations, 15, 28, 53‚Äì70, 96, 387
stiÔ¨Äness, 55, 113, 116, 123, 127, 128,
130, 132, 139
stiÔ¨Äness matrix, 194, 195, 203
stiÔ¨Äness ratio, 56
Stirling formula, 210
Stokes‚Äôs theorem, 185
storage, 260, 267
St¬®ormer method, 103, 410
St¬®ormer‚ÄìVerlet method, see St¬®ormer
method
successive over-relaxation (SOR), 259‚Äì
283, 286‚Äì289, 306, 307
optimal œâ, 269, 277‚Äì279, 281, 282,
284
supercomputer, 234
symbolic diÔ¨Äerentiation, 162
symplectic
form, 74
function, 88
map, 89, 90, 92, 94
method, 90, 91, 94, 97, 98
Taylor
method, 18
remainder formula, 442
thermodynamics, 349, 350, 357
theta method, 13‚Äì15, 16, 52, 59, 71
Thomas algorithm, 236
three-term recurrence, 222
time series, 106
analysis, 216
Toeplitz matrix, see matrix
trapezoidal rule, 8‚Äì13, 14, 15, 17, 55,
58‚Äì60, 110, 115, 116, 122, 127,
134, 365, 367, 397

Index
459
tree, see graph
triangle inequality, 126, 261, 368
triangulation, see element
trigonometric function, 177, 338
two-point boundary value problem, 171,
180, 182, 186, 191, 230, 231,
339
uniform tessellation, 195
unit vector, 257, 429
unitary eigenvector, 368
univalent function, 341
upwind scheme, 398, 420, 421, 423
upwinding, see upwind scheme
V-cycle, see multigrid
van der Pol equation, 106, 110, 111,
117
Vandermonde
determinant, 216
matrix, 34, 82, 432, 442
system, 36
variation, 443
variational
integrator, 98
problem, 178‚Äì183, 186, 200, 344,
349
vector, 428
vector analysis, 85
vector space, see space
visualization of solution, 105‚Äì106
von Neumann, John, 382
W-cycle, see multigrid
wave
equation, 158, 388, 393, 399, 407‚Äì
413, 424
d‚ÄôAlembert solution, 424
packet, 419
theory, 349, 388, 419
waveform relaxation, 382
Gauss‚ÄìSeidel, 382
Jacobi, 382
wavelets, 344
wavenumber, 294‚Äì297, 300, 302, 308,
419
weak
form, 180, 190
solution, 171, 174, 175, 180, 183,
186‚Äì188, 200
weather prediction, 56
weight function, 33, 46, 52
well-conditioning, 234, 244
well-posed equation, 356, 357, 360, 361,
383, 384
Yoshida method, 94, 98
Zadunaisky device, 119


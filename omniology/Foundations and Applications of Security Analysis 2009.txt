Lecture Notes in Computer Science
5511
Commenced Publication in 1973
Founding and Former Series Editors:
Gerhard Goos, Juris Hartmanis, and Jan van Leeuwen
Editorial Board
David Hutchison
Lancaster University, UK
Takeo Kanade
Carnegie Mellon University, Pittsburgh, PA, USA
Josef Kittler
University of Surrey, Guildford, UK
Jon M. Kleinberg
Cornell University, Ithaca, NY, USA
Alfred Kobsa
University of California, Irvine, CA, USA
Friedemann Mattern
ETH Zurich, Switzerland
John C. Mitchell
Stanford University, CA, USA
Moni Naor
Weizmann Institute of Science, Rehovot, Israel
Oscar Nierstrasz
University of Bern, Switzerland
C. Pandu Rangan
Indian Institute of Technology, Madras, India
Bernhard Steffen
University of Dortmund, Germany
Madhu Sudan
Microsoft Research, Cambridge, MA, USA
Demetri Terzopoulos
University of California, Los Angeles, CA, USA
Doug Tygar
University of California, Berkeley, CA, USA
Gerhard Weikum
Max-Planck Institute of Computer Science, Saarbruecken, Germany

Pierpaolo Degano Luca Viganò (Eds.)
Foundations and
Applications of
Security Analysis
Joint Workshop on Automated Reasoning
for Security Protocol Analysis
and Issues in the Theory of Security, ARSPA-WITS 2009
York, UK, March 28-29, 2009
Revised Selected Papers
1 3

Volume Editors
Pierpaolo Degano
Università di Pisa, Dipartimento di Informatica
Largo Bruno Pontecorvo, 3, 56127 Pisa, Italy
E-mail: degano@di.unipi.it
Luca Viganò
Università di Verona, Dipartimento di Informatica
Strada Le Grazie 15, 37134 Verona, Italy
E-mail: luca.vigano@univr.it
Library of Congress Control Number: 2009933043
CR Subject Classiﬁcation (1998): D.4.6, K.6.5, C.2, H.2.7, K.4.4
LNCS Sublibrary: SL 4 – Security and Cryptology
ISSN
0302-9743
ISBN-10
3-642-03458-6 Springer Berlin Heidelberg New York
ISBN-13
978-3-642-03458-9 Springer Berlin Heidelberg New York
This work is subject to copyright. All rights are reserved, whether the whole or part of the material is
concerned, speciﬁcally the rights of translation, reprinting, re-use of illustrations, recitation, broadcasting,
reproduction on microﬁlms or in any other way, and storage in data banks. Duplication of this publication
or parts thereof is permitted only under the provisions of the German Copyright Law of September 9, 1965,
in its current version, and permission for use must always be obtained from Springer. Violations are liable
to prosecution under the German Copyright Law.
springer.com
© Springer-Verlag Berlin Heidelberg 2009
Printed in Germany
Typesetting: Camera-ready by author, data conversion by Scientiﬁc Publishing Services, Chennai, India
Printed on acid-free paper
SPIN: 12725595
06/3180
5 4 3 2 1 0

Preface
The Joint Workshop on “Automated Reasoning for Security Protocol Analysis
and Issues in the Theory of Security” (ARSPA-WITS 2009) was held in York,
UK, March 28–29, 2009, in association with ETAPS 2009.
ARSPA is a series of workshops on “Automated Reasoning for Security Pro-
tocol Analysis,” bringing together researchers and practitioners from both the
security and the formal methods communities, from academia and industry, who
are working on developing and applying automated reasoning techniques and
tools for the formal speciﬁcation and analysis of security protocols. The ﬁrst
two ARSPA workshops were held as satellite events of the Second International
Joint Conference on Automated Reasoning (IJCAR 2004) and of the 32nd Inter-
national Colloquium on Automata, Languages and Programming (ICALP 2005),
respectively. ARSPA then joined forces with the workshop FCS (Foundations of
Computer Security): FCS-ARSPA 2006 was aﬃliated with LICS 2006, in the
context of FLoC 2006, and FCS-ARSPA 2007 was aﬃliated with LICS 2007 and
ICALP 2007.
WITS is the oﬃcial annual workshop organized by the IFIP WG 1.7 on “The-
oretical Foundations of Security Analysis and Design,” established to promote
the investigation on the theoretical foundations of security, discovering and pro-
moting new areas of application of theoretical techniques in computer security
and supporting the systematic use of formal techniques in the development of
security-related applications. This is the ninth meeting in the series. In 2008,
ARSPA and WITS joined with the workshop on Foundations of Computer Se-
curity FCS for a joint workshop, FCS-ARSPA-WITS 2008, associated with LICS
2008 and CSF 21.
In 2009, ARSPA and WITS again joined forces with the aim to provide a
forum for continued activity in diﬀerent areas of computer security, bringing
computer security researchers in closer contact with the ETAPS community and
giving ETAPS attendees an opportunity to talk to experts in computer security,
on the one hand, and to contribute to bridging the gap between logical methods
and computer security foundations, on the other.
There were 27 submissions of high quality, from countries in Asia, Europe,
and North America. All the submissions were evaluated by at least three referees
and the Program Committee then selected the 15 research contributions that
were presented at the workshop. Out of these, 12 were further revised by their
authors and are included in this volume. The workshop program was enriched
by invited talks by Peter Ryan and David Sands, whose contributions are also
included here.
We would like to thank all the people who contributed to the organization of
the ARSPA-WITS 2009 Workshop, and acknowledge the support from the IFIP
WG 1.7, the AVANTSSAR Project (FP7-ICT-2007-1, Project No. 216471), and

VI
Preface
the SENSORIA Project (EU-FETPI Global Computing Project IST-2005-16004).
In particular, we are deeply indebted to the other members of the Program
Committee and the additional referees, who allowed us to review the papers in
a very short time, while maintaining a very high standard: their help has been
invaluable. We are also grateful to Andrei Voronkov, who allowed us to use the
free conference software system EasyChair, which greatly simpliﬁed the work of
the Program Committee. Last but not least, warm thanks to the organizers of
ETAPS 2009.
June 2009
Pierpaolo Degano
Luca Vigan`o

Organization
Program Committee
Lujo Bauer
CMU, USA
Luca Compagna
SAP Research, France
Veronique Cortier
LORIA INRIA-Lorraine, France
Pierpaolo Degano (Chair)
Universit`a di Pisa, Italy
Sandro Etalle
Technical University of Eindhoven,
The Netherlands
Riccardo Focardi
Universit`a di Venezia, Italy
Dieter Gollman
Technische Universit¨at Hamburg-Harburg,
Germany
Roberto Gorrieri
Universit`a di Bologna, Italy
Joshua Guttman
MITRE, USA
Jerry den Hartog
Technical University of Eindhoven,
The Netherlands
Jan J¨urjens
The Open University, UK
Gavin Lowe
Oxford University, UK
Catherine Meadows
Naval Research Laboratory, USA
Jonathan Millen
MITRE, USA
Sebastian M¨odersheim
IBM Zurich Research Lab, Switzerland
Mark Ryan
University of Birmingham, UK
Luca Vigan`o (Chair)
Universit`a di Verona, Italy
Additional Reviewers
Misha Aizatulin
Alessandro Aldini
Andreas Bauer
Giampaolo Bella
Mario Bravetti
Roberto Carbone
Kostas Chatzikokolakis
St´ephanie Delaune
Alessandra Di Pierro
Francois Dupressoir
Deepak Garg
Claudio Guidi
Volkmar Lotz
Ilaria Matteucci
Toby Murray
Arnab Roy
Theoodor Scholte
Boris Skoric
Ben Smyth
Bruno Pontes
Soares Rocha
Fred Spiessens
Angelo Troina
Mathieu Turuani
Tjark Weber

Table of Contents
A Policy Model for Secure Information Flow. . . . . . . . . . . . . . . . . . . . . . . . .
1
Adedayo O. Adetoye and Atta Badii
A General Framework for Nondeterministic, Probabilistic, and
Stochastic Noninterference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
Alessandro Aldini and Marco Bernardo
Validating Security Protocols under the General Attacker . . . . . . . . . . . . .
34
Wihem Arsac, Giampaolo Bella, Xavier Chantry, and
Luca Compagna
Usage Automata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
Massimo Bartoletti
Static Detection of Logic Flaws in Service-Oriented Applications . . . . . . .
70
Chiara Bodei, Linda Brodo, and Roberto Bruni
Improving the Semantics of Imperfect Security. . . . . . . . . . . . . . . . . . . . . . .
88
Niklas Broberg and David Sands
Analysing PKCS#11 Key Management APIs with Unbounded Fresh
Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
92
Sibylle Fr¨oschle and Graham Steel
Transformations between Cryptographic Protocols . . . . . . . . . . . . . . . . . . .
107
Joshua D. Guttman
Formal Validation of OFEPSP+ with AVISPA. . . . . . . . . . . . . . . . . . . . . . .
124
Jorge L. Hernandez-Ardieta, Ana I. Gonzalez-Tablas, and
Benjamin Ramos
On the Automated Correction of Protocols with Improper Message
Encoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
138
Dieter Hutter and Ra´ul Monroy
Finite Models in FOL-Based Crypto-Protocol Veriﬁcation . . . . . . . . . . . . .
155
Jan J¨urjens and Tjark Weber
Towards a Type System for Security APIs. . . . . . . . . . . . . . . . . . . . . . . . . . .
173
Gavin Keighren, David Aspinall, and Graham Steel
Separating Trace Mapping and Reactive Simulatability Soundness: The
Case of Adaptive Corruption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
193
Laurent Mazar´e and Bogdan Warinschi

X
Table of Contents
How Many Election Oﬃcials Does It Take to Change an Election? . . . . .
211
P.Y.A. Ryan
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
223

A Policy Model for Secure Information Flow
Adedayo O. Adetoye and Atta Badii
School of Systems Engineering, University of Reading, Whiteknights, Berkshire, RG6 6AY, UK
a.o.adetoye@reading.ac.uk, atta.badii@reading.ac.uk
Abstract. When a computer program requires legitimate access to conﬁdential
data, the question arises whether such a program may illegally reveal sensitive in-
formation. This paper proposes a policy model to specify what information ﬂow
is permitted in a computational system. The security deﬁnition, which is based on
a general notion of information lattices, allows various representations of infor-
mation to be used in the enforcement of secure information ﬂow in determinis-
tic or nondeterministic systems. A ﬂexible semantics-based analysis technique is
presented, which uses the input-output relational model induced by an attacker’s
observational power, to compute the information released by the computational
system. An illustrative attacker model demonstrates the use of the technique to
develop a termination-sensitive analysis. The technique allows the development
of various information ﬂow analyses, parametrised by the attacker’s observational
power, which can be used to enforce what declassiﬁcation policies.
1
Introduction
The problem of secure information ﬂow arises when a computer program must be
granted legitimate access to conﬁdential data. When such a program, which might have
access to a network or that might otherwise be able to transmit conﬁdential information
to unauthorised observers, is executed, we want assurances that only the information
that we wish to reveal is released. An information ﬂow policy expresses our security
concern about the information release that we consider as safe. This leads to the ques-
tion of how to specify what information release is safe. The traditional approach to the
speciﬁcation of information release, or rather, the lack of it, is through the noninterfer-
ence requirement [7]. Noninterference prevents any ﬂow of secret information to public
areas in a multi-level security system, where information must not ﬂow from high to
low. Thus, noninterference is very restrictive and its usefulness in general practice has
been argued [13]. In practice, for example, during encryption, authentication, or statis-
tical analysis, we often want to release some level of information. This requires a more
general policy model by which we can specify what is the safe level of information to
be released. This paper proposes a lattice model to capture this property.
In [16], a taxonomy of declassiﬁcation mechanisms is introduced based on what,
where, when and by whom information is released. This paper is concerned about the
what dimension of information ﬂow, where we want to express the property that the
information released by a system does not exceed certain allowed limits. Based on
this observation, a deﬁnition of security is given, which captures the idea that a given
information ﬂow is safe.
P. Degano and L. Vigan`o (Eds.): ARSPA-WITS 2009, LNCS 5511, pp. 1–17, 2009.
© Springer-Verlag Berlin Heidelberg 2009

2
A.O. Adetoye and A. Badii
1.1
Contributions
This paper contributes to the theory of secure information ﬂow through a systematic
study of lattices of information as a tool for the enforcement of what declassiﬁca-
tion policies. Although lattice-based approaches are often used in language-based secu-
rity [14], these lattices are usually of security classes in a multi-level security system,
rather than lattices of information. We demonstrate that various representations of in-
formation such as partial equivalence relations, families of sets, information-theoretic
characterisation, and closure operators ﬁt into the lattice model of information, unify-
ing the various deﬁnitions under the lattice model. This means that the same partial
order-based enforcement technique can be applied to all the representations.
Another contribution to the theory is an input-output relation model, presented as
a primitive for the semantic analysis of information ﬂow. A systematic approach to
deriving the relational model from the operational semantics, which is parametric to
a chosen attacker’s observational power, is presented. The relational model accounts
well for information ﬂow due to nontermination, and the speciﬁc termination-sensitive
analysis presented demonstrates the correct analysis of diverging programs by using the
relational model.
1.2
Plan of the Paper
In Section 2 the lattice model of information is motivated, and a security deﬁnition is
given which uses the lattice model to enforce what declassiﬁcation policies. Section 3
introduces the relational model primitive as a tool for studying information ﬂow in mod-
els of deterministic or nondeterministic systems. Section 4 uses the relational model
to develop a representation of information, based on PERs, for the analysis of deter-
ministic system models. A language-based analysis technique is presented for While
programs with outputs to illustrate how to derive the relational model under a given
attacker model in a language-based setting. Similarly to Section 4, Section 5 applies the
relational model technique to develop a representation of information, based on fami-
lies of sets, which captures the information that the attacker may gain when the system
can be run repeatedly under ﬁxed inputs. An extension of the While language with a
nondeterministic construct shows the use of this information representation for infor-
mation ﬂow analysis in a nondeterministic language setting. We compare our approach
with related works in Section 6. Section 7 concludes the paper.
2
Secure Information Flow
The concept of secure information ﬂow suggests an understanding of the notions of
information and information ﬂow. A fundamental property of information is the intu-
itive notion of information levels, where we say that one piece of information is greater
or more informative than another. This suggests an ordering of information, which we
shall exploit in our information model and security deﬁnition. For this reason we shall
model information as lattices, where the associated partial order captures the notion of
information levels.

A Policy Model for Secure Information Flow
3
Deﬁnition 1 (Information and Information Flow). We deﬁne information as elements
of a complete lattice
  ,
 , where the associated partial order
  models the relative
degree of informativeness of the elements of
 , and the join operation
  models the
combination of information.
Information ﬂow with respect to the lattice
  is deﬁned as an extensive and mono-
tone self map on
 . Deﬁne
lows

f

   f is extensive and monotone
 to be the
set of all information ﬂows on
 .
The lattice join operation, which models information combination, is idempotent, com-
mutative, and associative. These properties agree with natural intuitions about informa-
tion, since idempotency says that the combination of a piece of information with itself
should yield the same information [10]. Similarly, the commutativity and associativity
properties respectively agree with the intuitions that the order and grouping of infor-
mation combination should not matter to the end result. Furthermore, for any s,s
 
 ,
the lattice property, s
  s
  iff s
  s
  s
 , agrees with the idea that the combination of
a lesser information with a greater one yields the greater information - where s
  s
  is
interpreted to mean that the information s is less than or at most equal to s
 .
The notion of information ﬂow models how the knowledge of an attacker changes
due to information release. For any initial knowledge s

  that the attacker might
have, f
s represents the ﬁnal knowledge of the attacker due to the information ﬂow
f

lows that the attacker receives. The extensivity property of f (that is,
s

 ,s
 f
s) intuitively means that an attacker’s knowledge may only increase by gaining
information, and the monotonicity (that is,
s,s
 
 ,s
  s
 

f
s
  f
s
 )
means that the greater the initial knowledge of the attacker before information release
the greater the knowledge afterwards.
Using this ordered structure of information, we can now deﬁne what it means for
a system to have secure information ﬂow with respect to what information the system
releases and a given information ﬂow policy1.
Deﬁnition 2 (What Policies and Security). Given the lattice
  ,
  of information and
the set
lows of information ﬂows over this lattice. An information ﬂow policy P with
respect to the lattice
  is a subset of
lows which speciﬁes the permitted information
ﬂows. An information ﬂow f

lows is said to be permitted or allowed by a policy
P

lows iff there exists a ﬂow function f
  P such that f
  f
 .
Let P be a program, modelling a system. Furthermore, let
P

 
lows be the
information ﬂow property of the system modelled by P, which describes the information
ﬂows caused by this system. The system satisﬁes, and is said to be secure with respect
to a policy P

lows iff for all f

P

  there exists f
  P such that f
  f
 .
The order f
  f
  between ﬂow functions is the usual pointwise ordering of functions
induced by the partial order
  on the lattice
 . This partial order regulates the level of
information that we allow a system to release. Intuitively, this deﬁnition says that the
program P (or the system it models) is secure (with respect to the policy P) only if
every ﬂow f

P

  that is caused by the system is permitted by the policy ( f
  P
such that f
  f
 ). This extensional view of policy enforcement abstractly describes, in
terms of the information lattice order, what information ﬂows arepermitted in the system.
1 We shall sometimes refer to an information ﬂow policy simply as policy or security policy.

4
A.O. Adetoye and A. Badii
Since
  is a complete lattice, it is easy to show that
lows also forms a complete
lattice under the pointwise function ordering. In particular, the noninterference policy,
which prevents any ﬂow of information to the attacker may be modelled by the identity
map (id
 ) on the lattice
 . This means that a system that satisﬁes the noninterference
policy
id
  has the information ﬂow property that regardless of the attacker’s previous
level of knowledge, the ﬁnal knowledge remains unchanged and the attacker is unable
to beneﬁt information by observing this noninterfering system. The baseline nature of
the noninterference policy model
id
  is established by the fact that id
  is the least
element of the lattice
lows of information ﬂows. However, it is clear that other less
restrictive policies than
id
  may be speciﬁed. Let us examine some example policy
patterns under the lattice model of information ﬂow.
2.1
Information Flow Policy Patterns
We examine in this section, policy patterns, other than the noninterference pattern,
which allow deliberate, but controlled, release of information.
We may wish to have partial (but unconditional) release of information s
 
  but not
more in a system. The required information ﬂow property is captured by the policy P

f

s

 ,f
s
 s
  s, which allows the attacker to combine its knowledge s with the
declassiﬁed information s
 , but the attacker may not learn more than this by observing
the system which is secure with respect to P. An example of this scenario arises during
password authentication, where we wish to release (unconditionally) the information
about the equality or not of the stored password and the user-supplied password.
Another scheme is the conditional release pattern, where information (s
 ) is released
based on having some initial knowledge (s
  ). This is modelled by the policy
f
 where
s

 ,f
s
 s
   s if s
    s, and f
s
 s otherwise. Under this scheme, the attacker
gains some information on the condition that the attacker has at least a given initial
information s
  . A scenario where such a policy is needed is during decryption in a
symmetric key system, where the plaintext may be learnt (the knowledge s
 ) only when
the decryption key is known (the knowledge s
  ).
Another pattern, called disjunctive ﬂow policy - after the disjunctive ﬂow pattern
of [16], is
f,f
 f
	
  f
 ,f
 	
  f


lows. This policy permits at most one of f or f
  to
be released but not both at the same time. It is clear that the notion of disjunctive infor-
mation ﬂow is only meaningful for incomparable information and information ﬂows,
because whenever two information are comparable then the greater already contains
the lesser information. An information ﬂow f
  
lows is permitted by the disjunctive
policy
f,f
  f
	
  f
 ,f
 	
  f
, when f
   is smaller than or equal to at most one of f and
f
  - since f and f
  are incomparable. Also, a ﬂow f
   f
  f
 , which contains both f
and f
  is not permitted since there is no such f1

f,f
  f
	
  f
 ,f
 	
  f
 for which
f
    f1.
In Deﬁnition 2 we deﬁned the security property of a system, with respect to a policy,
in terms of the system’s information ﬂow property. In the next section we shall show a
way to derive the information ﬂow property of a system from a relation which describes
how the system transforms its inputs to publicly observable outputs.

A Policy Model for Secure Information Flow
5
3
The Relational System Model
The question that we want to answer is whether a system that processes conﬁdential data
is secure with respect to a given security policy. So, using a suitable representation of
information, we want to check whether the information released conforms to our policy
requirement. The particular choice of representation of information may depend on the
model of the system being analysed or the kind of information that we are interested in
modelling. In this paper we shall consider information ﬂow analysis under deterministic
and nondeterministic system models using qualitative representations of information
and show that the representations ﬁt into the lattice model of information. We note that it
is possible to consider quantitative representations of information, such as entropy and
other information-theoretic measures, under the lattice model of information because
the measures, which are numbers, are naturally ordered.
We shall model a computational system, by an input-output relation: S
 Σ

,
where the system accepts inputs taken from the set Σ and produces public outputs
(with respect to an attacker model) in the set
. The relation S models what the attacker
observes given the supplied input. Depending on the attacker model, the system model
may be deterministic, in which case S is a function. More generally, however, the model
S of the system is said to be nondeterministic (with respect to the attacker’s view) when
the attacker’s observation is not necessarily unique for a given input. In the sequel, we
shall sometimes refer to the system modelled by the relation S simply as “system S”.
Deﬁnition 3 (The Relational Model). The input-output relational model of a system
is deﬁned as a relation S
 Σ

, over the set Σ of the system’s inputs and the set

of observable outputs, where for all input σ
 Σ and possible output v

, σ S v holds
iff the system can produce the output v when supplied with the input σ.
The inverse image of the relation S at v

 is denoted by S
1
v


σ
 Σ
 σ S v
.
When the relational model f is a function, we write f
 Σ
  and for any σ
 Σ, f
σ

stands for the unique output observed when the input σ is supplied. We refer to f as the
functional model of the system.
4
Analysis for Deterministic System Models
We shall use Partial Equivalence Relations2 (PERs) as a qualitative representation of
information for the deterministic system model. We start by motivating the use of PERs
for information representation in this setting. Since the deterministic system model
is a (total) function g
 Σ
 , where for any input σ
 Σ supplied to the system,
the attacker observes g
σ


, the knowledge gained by the attacker can be de-
scribed by the ability to distinguish which inputs might have been supplied based on
the observed output. Thus, given the observed output v

 of the system, the attacker
knows that the input to the system has been taken from the set g
1
v

 Σ. However,
based on this observation the attacker cannot distinguish between the inputs σ and σ
 2 A partial equivalence relation is a symmetric and transitive binary relation. If in addition the
relation is also reﬂexive, then it is an equivalence relation.

6
A.O. Adetoye and A. Badii
if σ,σ
  g
1
v
. More generally, we can model all the input pairs that the attacker
cannot distinguish by the kernel κg of g, which is the equivalence relation given by
σ,σ
  Σ,σ κg σ
 
 g
σ

 g
σ
 . Thus, κg describes the information (or more
directly, the ignorance) of the attacker based on the indistinguishability of input pairs
after observing the system’s outputs. An input pair is distinguishable by the attacker if
the pair is not related by the equivalence relation κg.
We can generalise this idea a bit further from equivalence relations to PERs to obtain
certain expressive powers. Similarly to equivalence relations, we say that a PER can-
not distinguish a pair if the pair is related by that PER, but any value that is not in the
domain of deﬁnition of the PER is considered as not possible and therefore distinguish-
able. For example, we can represent the knowledge of parity of an integer secret via
the equivalence relation Par deﬁned as
n,m
 Z,n Par m iff n mod 2
 m mod 2,
which distinguishes a pair of integers with different parity. However, the PER Par
 over
integers, deﬁned as
n,m
 Z,n
Par m iff n Par m and n,m
 0, represents the
knowledge of parity and sign - since it only relates natural numbers (negative integers
are not possible). The use of equivalence relations, and PERs in general, to describe the
security property of programs is not new [11,15]. In this paper we are interested in the
lattice properties for the enforcement of secure information ﬂow.
Deﬁnition 4. Let Σ be a set. Deﬁne PERΣ to be the set of all PERs over the set Σ
and deﬁne the information order relation
  on PERs such that for any R,R
  PERΣ,
R
  R
  iff for all σ,σ
  Σ, σ R
  σ
  implies σ R σ
 . The associated information
combination, or join operation
 , on PERΣ is deﬁned as σ
R
  R
  σ
  iff σ R σ
 and σ R
  σ
 . More generally, for any subset

 PERΣ let the join of
 be the PER
 , deﬁned for all σ,σ
  Σ as σ
  σ
  iff
R

, σ R σ
 .
We know from the deﬁnition of
  that R
  R
  means that R
  can distinguish whatever
R can, and thus R
  contains more information than R. For example, Par
  Par agrees
with the intuition of the relative information content of these two PERs. The ordering
of PERs by their information content forms a complete lattice of information.
Theorem 1. The partially ordered set
 PERΣ,
 ,
  is a complete lattice.
Deﬁnition 5 (Deterministic Information ﬂow). Let
  PERΣ be the lattice of in-
formation for a system S whose relational model is given by a function gS
 Σ
 .
The information ﬂow property of this system may be deﬁned as
S

 
f

R

PERΣ,f
R

 R
  κgS
, where κgS is the kernel of the function gS.
4.1
Language-Based Analysis
In this section, we shall demonstrate the application of the analysis approach presented
above in a language-based setting. We shall use the core deterministic imperative While
language of Fig. 1, which has outputs, as our basis. The language is similar to the
language of [8], and its operational semantics is fairly standard.
While expressions may be boolean-valued (with values taken from B

tt,ﬀ
),
or integer-valued (taken from Z). Program states, are maps from variables to values.
The evaluation of the expression e at the state σ is summarised as σ
e . Expression

A Policy Model for Secure Information Flow
7
c
    skip
  z
   e
  write e
  c;c
  if(b)then c else c
  while
b do c
Fig. 1. The While Language
evaluations are performed atomically and have no side-effect on state. Program ac-
tions, ranged over by a, can either be internal ε, which is not observable ordinarily, or
output (via a write command), where the expression value can be observed. The opera-
tional semantics is presented via transition relations between expression conﬁgurations
(  e,σ

ε

  σ
e ,σ
) and command conﬁgurations ( c,σ

a

  c
 ,σ
 ). A special, ter-
minal command conﬁguration
 ,σ
 indicates the termination of a command in state σ.
The operational semantics is shown in Fig. 2.
skip,σ

ε
 
,σ

z
   e,σ

ε
 
,σ
z
 σ
e 
write e,σ

σ
 e
 
,σ

c1,σ

a
 
,σ
 
c1;c2,σ

a
 
c2,σ
 
c1,σ

a
 
c
 1,σ
 
c1;c2,σ

a
 
c
 1;c2,σ
 
b,σ

ε
 
tt,σ

c1,σ

a
 
c
 1,σ
 
if
b then c1 else c2,σ

a
 
c
 1,σ
 
b,σ

ε
 
ﬀ,σ

c2,σ

a
 
c
 2,σ
 
if
b then c1 else c2,σ

a
 
c
 2,σ
 
b,σ

ε
 
ﬀ,σ

while
b do c,σ

ε
 
,σ

b,σ

ε
 
tt,σ

c,σ

a
 
c
 ,σ
 
while
b do c,σ

a
 
c
 ;while
b do c,σ
 
Fig. 2. Operational Semantics of While
4.2
The Attacker’s Observational Power
Let Σ be the set of all states of a program P. The trace of P, starting from the state
σ
 Σ, is denoted by t
 P,σ


 P,σ

a0

  P1,σ1

a1

 , according to the operational
semantics. A trace of P at the state σ is said to terminate if there is a natural num-
ber n such that t
 P,σ


 P,σ

a0

 
an  1

  ,σ
 , otherwise, the trace is said to be
nonterminating and P diverges at σ.
We introduce the notion of an attacker’s observational power (obs) as a map from
program traces to what the attacker observes. The relative powers of two attackers A
and A
 , modelled respectively by the observational powers obsA and obsA
, may be
compared under the proposed model, where we say that the attacker A
  is more powerful
than the attacker A if there exists a function f such that obsA
 f
 obsA
. A more
powerful attacker will gain at least as much information as a less powerful attacker.
In this paper we shall consider an attacker model, whose observational power is the
function obs
, which is able to observe only outputs generated by write statements
and is also able to observe the termination or not of a program. The latter assumption
about the attacker appears to be strong, and is usually not modelled in language-based

8
A.O. Adetoye and A. Badii
security. However, given the source code of a program, an attacker may be able to
determine when a given program trace will not terminate without actually observing
it. Modelling the ability to “observe” nontermination is also important as it may be
possible to leak arbitrary amount of information via nontermination channels [2].
Deﬁnition 6 (The Semantic Attacker). Let Σ be the set of all states of a While pro-
gram P and let
ε

  be the reﬂexive, transitive closure of the transition relation
ε

 .
Furthermore, let t
 P,σ


 P,σ

ε

 
 P
 ,σ
 
a1

  P1,σ1

ε

 
 P
 1,σ
 1

a2

  be a
canonical representation of the trace of P at σ
 Σ, where for all i, ai
 ε. Deﬁne the
semantic attacker’s observation of this trace as
obs
t
 P,σ










 a1,a2,
,

if P diverges at σ
 a1,a2,
,

otherwise.
The set of all possible observations that the attacker can make is given by


obs
t
 P,σ


 σ
 Σ. The functional model induced by this attacker’s observational
power is thus given by gP
 Σ
 , deﬁned as gP
σ

 obs
t
 P,σ

 which maps the
supplied input to the observed output of P under the attacker model.
The information gained by the semantic attacker from P is thus given by the equiv-
alence relation
TP
 over states, deﬁned such that for any pair of states σ,σ
  Σ,
σ
TP
 σ
  iff obs
t
 P,σ


 obs
t
 P,σ


 - the kernel of gP . The information ﬂow prop-
erty of P, over the lattice
  PERΣ may therefore be deﬁned as
P

 
f

R

PERΣ,f
R

 R
 TP
.
The deﬁnition of obs
 formalises the idea that the semantic attacker cannot observe
ε

  transitions. For nonterminating traces, the token
 is introduced which, in addition
to the sequence of output tokens observed on the trace, signals the divergence of the
program. Similarly, the token
 identiﬁes a terminating trace. Although the operational
semantics deﬁnition does not have a direct notion of nontermination, obs can account
for it since it is deﬁned over the trace.
To illustrate the deﬁnition’s termination properties, let loop
 while
tt do skip,
and consider the programs P1
if(h)then skip else loop and P2
writeh;loop.
Both P1 and P2 insecurely reveal the boolean secret h. The analysis shows this because
TP1


TP2
 is the identity equivalence relation on h. As the analysis of P2 demon-
strates, we can easily show that by appending a trailing loop to any program P that
always terminates, as in P
  P;loop, the information released is the same because
TP


TP

. However, the analysis of P3
 loop;write h shows that P3 is safe be-
cause, as the semantics shows:
 P3,σ

ε

 
ε

  P3,σ

ε

 , the trailing write h
is never executed, and hence
TP3
 relates all states, revealing no information.
In the next section we shall demonstrate the use of what policies for the enforcement
of secure information ﬂow in While programs.
4.3
Policies For Encryption
Encryption is an important security primitive which is used widely as a security foun-
dation in many systems. However, in order to protect the secrecy of sensitive data used

A Policy Model for Secure Information Flow
9
during encryption, we require policies that permit encryption to be used safely. Nonin-
terference policies cannot be used for encryption since the resulting (public) cyphertext
in an encryption scheme will depend on the supplied plaintext and key which are con-
sidered secret. Thus, there remains the problem of the speciﬁcation of policies that
allow safe use of encryption in programs. We demonstrate in this section how our ap-
proach can be used to specify and enforce policies that allow encryption to be used
safely.
We start by considering an encryption function
1
 K
 M
  C, which accepts a
key chosen from the set K of keys and message or plaintext chosen from the set M,
and produces a cyphertext in the set C. Now suppose that
1 is considered secure and
that its implementation, which we shall denote by the expression enck,m, is cor-
rect, so that under any state σ
enc
k,m

1
σ
k
,σ
m. Therefore, we allow
the attacker to observe the cyphertext enck,m for any choice k
 K of key and
plaintext m
 M values. Hence, we can deﬁne an equivalence relation
1
 which cap-
tures this intentional information release, where

1
 relates every pair of states σ and
σ
 , where
1
σ
k
,σ
m

1
σ
 k
,σ
 m. The resulting information ﬂow policy
P
1

f
 R
 PER Σ,f
R

 R
 1
 allows the attacker to observe the cipher-
text generated by a correct implementation of the encryption function. Firstly, since the
implementation enc is secure, it satisﬁes the policy P
1.
Now consider a secure (and insecure) data backup scenario (adapted from [1]) as
shown in the program listings of Fig. 3. The LHS program P1 securely releases the
encrypted data (ctxt) to a public output channel after encrypting the data (data) with
the key (k). However, the RHS implementation P2 is insecure because the program-
mer releases the plaintext data instead of the ciphertext. The analysis detects that the
RHS program violates the policy because
TP2

	
 
1
 as required by P
1 - unless
the encryption function by deﬁnition reveals the encrypted data, which violates initial
assumption that it is secure. Thus, the analysis detects this ﬂaw. This would be useful,
for example, to a programmer who can avoid such programming error by checking his
or her implementation against the desired policy.
The reason why the noninterference policy cannot be used for encryption lies in the
fact that noninterference prohibits any sort of variation in the observed output from
being induced by a variation in the secret input to the encryption function. However,
one of the reasons why encryption is widely used as a security primitive is the fact that
the security lies in the ability to protect secret data even when the encryption function
is known. Thus, a good encryption function is already designed so that it is not easily
invertible into its constituent arguments, although a variation in its input would cause
a variation in its output for the function to be useful. The safe input-to-output varia-
tion caused by the deﬁnition of the encryption function is captured by the equivalence
relation
1
 in the example above, which allows only the output variations due to the
deﬁnition of the encryption function to be observed by the attacker.
ctxt : = enck,data ;
write ctxt;
ctxt : = enck,data
;
write data;
Fig. 3. Secure versus Insecure Data Backup

10
A.O. Adetoye and A. Badii
Nondeterministic Encryption. In nondeterministic encryption, such as cipher-block
chaining encryption mode, an initialisation vector (iv) is used along with the key and
plaintext such that if a different iv is used, a different ciphertext is generated under the
same key and plaintext pair. The term “nondeterministic” refers to the fact that the im-
plementation of such encryption algorithms generally have the property that encrypting
the same plaintext several times using the same key would yield different ciphertexts.
Let the function
2
 IV
 K
 M
  C denote such an encryption scheme, where IV
is the set of initialisation vectors. We shall represent by the expression enc

iv,k,m a
correct implementation of
2. Similarly to the encryption policy above, we deﬁne
2

as the equivalence relation which relates all states which evaluate enc

iv,k,m to the
same ciphertext, and the required declassiﬁcation policy is similarly deﬁned.
Now, a known problem with declassiﬁcation schemes is that of occlusion [16], where
a legitimately declassiﬁed information masks the release of other secrets. Being a what
policy, our policy enforcement prevents such a ﬂow by permitting only the informa-
tion release that is explicitly allowed by the policy. This problem is illustrated by the
program listing of Fig. 4 (adapted from [1]). Suppose that we have declassiﬁed the en-
cryption result, then revealing the content of l1 and l2 through the write statements in
the program is permitted, however the value of the boolean secret h will be released ad-
ditionally by this program because the inequality of l1 and l2 will reveal the fact that the
then branch was executed. The analysis shows this. Let us call this program P, its analy-
sis
TP
 has the property that whenever it relates any two states σ and σ
  which disagree
on the observed ciphertexts, then they must both agree to a value σ
h 
 σ
 h 
 tt
of h - revealing h. Hence P violates the declassiﬁcation policy ( TP

	
 
2
) because
2
 requires, for example, that σ
h
 ﬀ
 must be indistinguishable from σ, which
TP
 distinguishes in this case.
l1
   enc

iv1,k,m ;
i f (h) then l2
   enc

iv2,k,m ; else l2
   l1;
write l1; write l2;
Fig. 4. The Occlusion Problem
5
Analysis for Nondeterministic System Models
We now turn our attention to the security analyses of information ﬂow under nonde-
terministic system models. We start by motivating the use of families of sets as a rep-
resentation of information in the nondeterministic setting, which generalises the PER
representation used earlier for the deterministic system model.
Consider a system, whose relational model is given by S
 Σ

. We can describe
the information that the attacker gains on observing the output v

 of the system
by the inverse image of v under S. The inverse image S
1
v
 of v represents the set
of all possible inputs that can produce the output v in the system modelled by S, and
thus describes the attacker’s uncertainty about the inputs given the observation of v. It
is thus easy to see that the family of sets
S
1
v

 v


 models the uncertainties of
the attacker under the observation of individual outputs of the system modelled by S.

A Policy Model for Secure Information Flow
11
In the special case that S models a deterministic system, in which case S is a function,
it is clear that
S
1
v

 v


 corresponds to the set of partitions of the kernel of
the function S, which uniquely identiﬁes the equivalence relation over Σ used to de-
scribe the information released in the previous sections. In this sense, the family of sets
representation generalises the PER representation.
However, unlike the deterministic model presented earlier, where for any v,v
 
,
v
 v
  implies S
1
v

	 S
1
v
 


, the inverse images are not necessarily dis-
joint under a nondeterministic model since the outputs resulting from any given in-
put may not necessarily be unique. This leads to an avenue of information release in
nondeterministic systems. The property that the nondeterministic system modelled by
S does not necessarily partition its domain introduces the possibility that an attacker
might gain further information by repeated execution of the system under a ﬁxed in-
put. To illustrate this, suppose S
 Σ

 models a nondeterministic system, where
Σ

σ1,σ2,σ3
 and


v1,v2
 and where the graph of the relation S is given
by graph
S


σ1,v1
,
σ2,v1
,
σ2,v2
,
σ3,v2
. The model is nondeterministic
since the input σ2 can produce outputs v1 or v2. By observing an output v1 the attacker
learns that the input must be one of σ1 and σ2, as suggested by S
1
v1


σ1,σ2
.
Similarly, on observing the output v2, the attacker learns that the input is in the set
S
1
v2


σ2,σ3
. However, if under a ﬁxed input the attacker observes outputs v1
and v2 in different runs of the system, then the attacker conﬁrms that the input to the
system must be σ2 - derived by taking the intersection S
1
v1

	 S
1
v2
. This avenue
of information leakage is not available under the deterministic system model since for
a ﬁxed input, the output of the system always remains the same. This leads us to a
deﬁnition of information based on families of sets.
Deﬁnition 7 (Lattice of Possibilistic Information). Let ΣJ

Σj
 Σ
 j
 J
 be a
family of subsets of Σ indexed by a set J. Deﬁne the operation
  
 on families of subsets
of Σ as
  ΣJ



K
 J

ΣK
, which closes the family under intersections. Deﬁne the
possibilistic information set overΣ asFAMΣ

  ΣJ

ΣJ is a family of subsets of Σ
to be the information contained in families of sets over Σ. For any
  ΣJ

,
  ΣK



FAMΣ deﬁne the join operation as
  ΣJ


   ΣK



  ΣJ
K

 and deﬁne the partial
order
  to be the subset ordering of families in FAMΣ.
The intuition behind the partial ordering
  ΣJ


   ΣK

 is that every information token
in
  ΣJ

 (which is derivable by taking the intersection of some elements of ΣJ) is
also present in
  ΣK

. This leads us to a description of information that can be derived
about the nondeterministic system modelled by S
 Σ

 as

S



  S
1
v

 v





which identiﬁes the minimal sets of inputs that can produce a given output in the system.
Based on this, and choosing
  FAMΣ, we can deﬁne the possibilistic information
ﬂow as
S

 
f

F
 FAMΣ,f
F

 F
 
S


.
The ordering of information content of elements of FAMΣ forms a complete lattice
of information.
Theorem 2. The family of sets
 FAMΣ,
 ,
  over Σ representing the set of possi-
bilistic information is a complete lattice.

12
A.O. Adetoye and A. Badii
5.1
Language-Based Instantiation
We make a conservative extension to the While language, through the introduction of
a nondeterministic choice constructor  to obtain the language While-ND (While with
NonDeterminism) which exhibits nondeterministic behaviour. In While-ND, the pro-
gram c1 c2 makes an invisible but arbitrary choice in the execution either command c1
or command c2. Consequently, the operational semantics of While-ND extends that of
While as shown in Fig. 5.
c1  c2,σ

ε
 
c1,σ

c1  c2,σ

ε
 
c2,σ

Fig. 5. Extending While with Possibilistic Nondeterminism
To deal with the fact that the trace of a While-ND program P is no more unique for a
given starting state, the observational power is extended to sets of traces so that obs


is the set of observations for a given starting state, obtained by applying obs to all the
traces that can result from that state. This produces a relational model SP of P, which
relates σ to v iff there exists v
 obs

t
 P,σ

. The resulting possibilistic analysis is also
termination-sensitive.
Suppose the integer (secret) h is a parameter to the While-ND program given by
P
 if(h
 0)then skip  loop else skip. This program may either terminate
or loop indeﬁnitely when the secret value h is chosen to be zero. Thus, it is easy to
see that the attacker may learn the value of h to be zero when the program fails to
terminate. The set of possible observation of P is given by
P

 ,
 
 where
 
corresponds to the observation during the terminating traces and
  corresponds to the
observation of the diverging trace. If we represent the set of program states as one-
tuples:
n 
 n
 Z, then the relational model of P is SP
 Σ

P , whose graph is
given by
0 ,
 
,
n ,
 
 n
 Z. Thus, we have the following inverse images:
S
1
P
 

0  and S
1
P
 

 Σ, and

SP



0 ,Σ reﬂecting the fact that the
attacker can learn when the secret value is zero. Now consider another program PA

if(h
 0)then skip  loop else loop. In this case, the observation of termination
reveals to the attacker that the value of the integer secret h is zero. The analysis is
similar to that of P, but now we have graphSPA


0 ,
 
,
n ,
 

n
 Z and
S
1
PA
 

0  and S
1
PA
 
 Σ. Thus,

SPA



0 ,Σ, and it is intuitive
PA releases the same information as P.
We can deﬁne a noninterference policy, which prevents any information from be-
ing gained about h in the examples above. This policy is given by PΣ

f

F

FAMΣ,f
F

 F
, which is modelled by the identity map on the lattice FAMΣ.
We see that both P and PA above violate this policy. This is clear, for example, given
an initial knowledge
Σ of the attacker representing lack of information, the attacker
learns
0 ,Σ
	
 Σ through these programs.
Now consider the program PB
 if(h
 0)then skip  loop else skip 
loop which may or may not terminate regardless of the chosen value of h. Intuitively,
this program should not reveal any information to the attacker as its behaviour is
independent of the choice of h. This is conﬁrmed by the analysis because

A Policy Model for Secure Information Flow
13
graph
SPB


n ,
 
,
n ,
 
 n
 Z. Thus, S
1
PB
 
 S
1
PB
 
 Σ. This
means that

SPB



Σ, showing the fact that the attacker learns nothing by observing
the execution of PB. Thus, the program PB satisﬁes the noninterference policy PΣ
deﬁned above.
Now suppose that PC is a While-ND program which always terminates. Similarly
to the analysis under deterministic programs, the information ﬂow of PC is preserved
in the program P
  PC;loop. This is easy to see because there is set isomorphism
between the sets
PC and
P
 of outputs of PC and P
  respectively, which appends

to all
 a

PC such that
σ
 Σ,σ SPC
 a

 σ SP

 a,
 where SPC
 Σ

PC
and SP

 Σ

P
 are respectively the relational models of PC and P
 . Hence, we
have

SPC




SP


. Finally, let PD be a While-ND program such that P
  loop;PD.
Like the deterministic analysis, this program reveals no information since for all σ

Σ,σ SP

  holds and hence

SP




Σ.
5.2
Information-Theoretic Representation
We can perform information-theoretic analyses under the relational model S
 Σ


of a system by assigning probability distributions to the input space Σ to model the
attacker’s uncertainty about the choice of inputs, and by considering the probability
distribution of
 induced by the execution of the system. Because of space restrictions,
speciﬁc information-theoreticanalysis techniques are not shown in this paper. However,
we note that quantitative measures, such as Shannon’s entropy measure, which describe
the attacker’s uncertainty about the input distribution before and after observing pro-
gram outputs can be arranged on a lattice according to the order of the quantitative
amount of information that is released about the system inputs. This observation allows
us to use the lattice-based approach to enforce security under this setting, where the
amount of information that is permitted to be released about a given secret is captured
by the numerical order relation
 on the quantitative information measure.
6
Related Work
This paper falls into the area of language-based security, which is an established and
active research area [14]. We have proposed a lattice-theoretic approach to the en-
forcement of what declassiﬁcation policies [16,17]. Several approaches, such as in
[4,5,6,9,11,15], have been applied to study the what dimension of information release.
While these approaches study the properties of speciﬁc representation of information
used, we study the problem from a more general viewpoint of information as a lattice
structure which can be used to enforce what policies. The resulting lattice-based policy
model has been shown to be capable of handling controlled information release, such
as during encryption, and it does not suffer from the occlusion problem.
In [15], PERs are used to describe the security properties of a program. Given a func-
tion f
 A
  B which is a Scott-style denotation of a program P (information ﬂow due
to nontermination is handled by requiring that secret inputs do not affect the termination
behaviour), the security property of P is stated as a PER-transformer f
 RA  RB,
where RA
 PER
A and RB
 PER
B
. The property f
 RA  RB holds iff for all

14
A.O. Adetoye and A. Badii
a,a
  A,a RA a
 
 f
a RB f
a
 . Thus, f
 RA  RB may be interpreted as
a policy which P satisﬁes. If we assume that A
 B
 Σ
 H
 L represents the set
of all states of P, partitioned to the high-secret (H) and low-public (L) parts, then P is
said to be (noninterference) secure iff f
 all
 id  all
 id, where σ
 L is the projec-
tion of the state σ to L and for all σ,σ
  Σ,σ all
 id σ
  iff σ
 L
 σ
  L. The meaning
of f
 all
 id  all
 id is that the attacker which can only observe the public part of
state before P
 s execution and on its termination cannot distinguish two runs which
agree on the initial L-input. This corresponds to an observational model under our ap-
proach, which for any initial input state σ is given by obs
t
 P,σ



 σ
 L,f
σ

 L
.
Consequently, the information released is characterised by the PER deﬁned such that
σ,σ
  Σ, σ
T
 P,
σ
 
 obs
t
 P,σ


 obs
t
 P,σ


. This deﬁnition computes
the least PER
T
 P,
, that is, the most reﬁned policy for which P is secure under
the attacker model obs
, because f
 R
 id  all
 id


T
 P,

  R
 id.
For any R
 id that is strictly less than
T
 P,
, P will produce outputs that can be
distinguished by obs
 under some variation of the H-projection of inputs that are
related by R.
The abstract noninterference deﬁnition of [6] introduces attacker models as abstract
interpretations which can observe only properties of data in the concrete domain. The
concrete domain is partitioned into two sets H and L, which represent the domain of
secret and public values respectively, and state is modelled as tuples in Σ
 H
 L. The
attacker is modelled as a pair of abstractions
 η,ρ, where η,ρ
 uco
L  are upper
closure operators (extensive, monotone, and idempotent maps) on the powerset lattice
of public values ordered by subset inclusion. The closure operators η and ρ model what
the attacker can observe about the program’s public inputs and outputs respectively.
The concrete semantics of the program P is formalised using angelic denotational se-
mantics, which associates an input-output function,
P

 Σ
  Σ, with P and ignores
nontermination. Furthermore, the observation of (public) values occur at the beginning
of program execution and on program termination. To slightly simplify the notations,
we shall denote the concrete semantics of P as a map
P

 H
 L
  L, throwing away
the H projection of state on termination, which is not used.
Our observational model is more general since we place no restriction on the nature
of the observational power function, as opposed to the requirement in [6] where they
must be closure operators. Furthermore, our observational model is not restricted to the
observation of values at the beginning and end of program execution. In particular, the
attacker
 η,ρ may be obtained under our model by deﬁning an observational power
function on traces, where for any σ, ˆσ
 Σ, and trace t
 P,σ


 P,σ

a

 
a


  , ˆσ

we have obs
 η,ρ 
t
 P,σ



 η σ
 L
,ρˆσ
 L
. This deﬁnition says that the attacker
only observes the η-property of the L-projection of the initial state and the ρ-property of
the L-projection of the terminating state of P. Consequently, the information released
under this observational model is the PER
TP
 η,ρ
 over Σ deﬁned such that for any
σ,σ
  Σ, σ
TP
 η,ρ
 σ
  iff obs
 η,ρ 
t
 P,σ


 obs
 η,ρ 
t
 P,σ


. It is thus clear that for
any σ,σ
  Σ, where
 P,σ

a1

 
an

  , ˆσ
 and
 P,σ
 
a

1

 
a

m

  , ˆσ
  we have

A Policy Model for Secure Information Flow
15
σ
TP
 η,ρ
 σ
 
 η σ
 L

 η σ
  L

 ρˆσ
 L

 ρˆσ
  L


 η σ
 L

 η σ
  L


 ρˆσ
 L

 ρˆσ
  L
.
By this we immediately obtain the narrow abstract noninterference (NANI) deﬁnition:
η P
ρ


σ,σ
  Σ,η σ
 L

 η σ
  L


 ρP
σ

 ρP
σ
 .
Thus,
TP
 η,ρ
 is the least PER over states for which any pair of states that it relates
satisﬁes NANI in P.
The NANI deﬁnition causes what is referred to as “deceptive ﬂows”, when η-
undistinguished public input values cause a variation which makes P to violate NANI.
In order to deal with this problem, abstractions of L values are passed as program pa-
rameters and another abstraction φ
 uco
H
 is introduced on the input secret
values. This results in the abstract noninterference (ANI) property,
η P
φ
 ρ,
of [6].
Let σ
 Σ be a state, and deﬁne the set Ση,φ
σ
of L-projections of the terminating
states of P due to the execution of P from any starting state which agrees with σ on the
η-property of the L-projection and on the φ-property of the H-projection to be
Ση,φ
σ










ˆσ
 L













σ
   Σ. P,σ
  
a

 
a


  , ˆσ
.
η σ
   L

 η σ
 L
,φσ
   H

 φσ
 H
.









In [6] the ANI property,
η P
φ
 ρ, is now deﬁned to hold iff for all σ,σ
  Σ,
η σ
 L

 η σ
  L


 ρΣη,φ
σ

 ρΣη,φ
σ

.
We can obtain this observational model under our framework by deﬁning an observa-
tional power function on traces, such that for any σ
 Σ, and terminating trace t
 P,σ
 we
have obs
 η,φ,ρ 
t
 P,σ



η σ
 L
,ρΣη,φ
σ

. This deﬁnition requires that no public
output can be distinguished by ρ for any initial state which is L-indistinguishable from
to σ under η and H-indistinguishable from σ under φ. Thus, as usual, the information
released under our relational model is the PER
TP
 η,φ,ρ
 over Σ deﬁned such that for
any σ,σ
  Σ, σ
TP
 η,φ,ρ
 σ
  iff obs
 η,φ,ρ 
t
 P,σ


 obs
 η,φ,ρ 
t
 P,σ


. Hence, for
all σ,σ
  Σ, we have that
σ
TP
 η,φ,ρ
 σ
 
 η σ
 L

 η σ
  L

 ρΣη,φ
σ

 ρΣη,φ
σ



 η σ
 L

 η σ
  L


 ρΣη,φ
σ

 ρΣη,φ
σ

.
By this we obtain ANI property
η P
φ
 ρ, where
TP
 η,φ,ρ
 is the least PER
over Σ, for which any pair of states that it relates satisﬁes ANI in P. We note that this
property, which prevents the attacker from gaining information φ about secret inputs
(see [12]), can also be arranged on a lattice of information, in particular, because φ

uco
H
 forms a complete lattice.
In [3,8], action-based operational semantics approaches are used in deterministic lan-
guage settings to check program security, similar to our labelled-transition semantics.

16
A.O. Adetoye and A. Badii
However, the analyses are not termination-sensitive. Furthermore, these approaches as-
sume a ﬁxed attacker model, whereas analysis is parametrised by a chosen attacker’s
observational power under our approach. With respect to the model of information,
the gradual release knowledge of [3] is modelled by sets Σ
 Σ, which represent the
knowledge (more precisely, the uncertainty) of the attacker at any point in time. These
sets shrink over time and the knowledge is monotone, which agrees with the extensiv-
ity and monotonicity properties our information ﬂow function deﬁnition. However, the
PER over Σ representation generalises the subsets of Σ representation, since for any
Σ
 Σ, there is a PER RΣ which encodes this set, where σ RΣ σ
  holds iff σ,σ
  Σ.
Hence, every instantiation of knowledge in [3] can be encoded as PERs.
7
Conclusion
We have presented a policy model for secure information ﬂow based on lattices of
information to enforce what declassiﬁcation policies. We demonstrated that various
representations of information such as PERs, families of sets, information-theoretic
measures, and closure operators may be uniﬁed under the lattice model of informa-
tion, providing us with a uniform way to enforce policies based on the lattice order. A
termination-sensitive analysis method was also presented, which is parametric to a cho-
sen attacker model, and which derives a system’s information ﬂow property from the
operational semantics. A static analysis technique and type system is currently being
developed as an application of the relational model approach. An area of future work is
to study the integration of the lattice-of-information model with other mechanisms such
as security classes in a multi-level security system for the enforcement of secure infor-
mation ﬂow. Such an integration would allow us to express not only what properties in
policies, but also who properties.
References
1. Askarov, A., Hedin, D., Sabelfeld, A.: Cryptographically-masked ﬂows. Theoretical Com-
puter Science 402(2-3), 82–101 (2008)
2. Askarov, A., Hunt, S., Sabelfeld, A., Sands, D.: Termination-insensitive noninterference
leaks more than just a bit. In: Jajodia, S., Lopez, J. (eds.) ESORICS 2008. LNCS, vol. 5283,
pp. 333–348. Springer, Heidelberg (2008)
3. Askarov, A., Sabelfeld, A.: Gradual release: Unifying declassiﬁcation, encryption and key
release policies. In: IEEE Symposium on Security and Privacy, pp. 207–221. IEEE Computer
Society, Los Alamitos (2007)
4. Clark, D., Hunt, S., Malacaria, P.: A static analysis for quantifying information ﬂow in a
simple imperative language. Journal of Computer Security 15(3), 321–371 (2007)
5. Cohen, E.S.: Information transmission in computational systems. In: SOSP 1977: Proceed-
ings of the sixth ACM symposium on Operating systems principles, pp. 133–139. ACM
Press, New York (1977)
6. Giacobazzi, R., Mastroeni, I.: Abstract non-interference: parameterizing non-interference by
abstract interpretation. In: Proceedings of the 31st ACM SIGPLAN-SIGACT symposium on
Principles of programming languages, pp. 186–197. ACM Press, New York (2004)

A Policy Model for Secure Information Flow
17
7. Goguen, J.A., Meseguer, J.: Security policies and security models. In: Proceedings of the
IEEE Symposium on Research in Security and Privacy, Oakland, CA, pp. 11–20. IEEE Com-
puter Society Press, Los Alamitos (1982)
8. Le Guernic, G., Banerjee, A., Jensen, T.P., Schmidt, D.A.: Automata-based conﬁdential-
ity monitoring. In: Okada, M., Satoh, I. (eds.) ASIAN 2006. LNCS, vol. 4435, pp. 75–89.
Springer, Heidelberg (2006)
9. Joshi, R., Leino, K.R.M.: A semantic approach to secure information ﬂow. Science of Com-
puter Programming 37(1-3), 113–138 (2000)
10. Kohlas, J.: Information Algebras: Generic Structures for Inference. Springer, Heidelberg
(2003)
11. Landauer, J., Redmond, T.: A lattice of information. In: Proceedings of the Computer Se-
curity Foundations Workshop VI (CSFW 1993), Washington, Brussels, Tokyo, pp. 65–70.
IEEE, Los Alamitos (1993)
12. Mastroeni, I.: On the Rˆole of abstract non-interference in language-based security. In: Yi, K.
(ed.) APLAS 2005. LNCS, vol. 3780, pp. 418–433. Springer, Heidelberg (2005)
13. Ryan, P., McLean, J., Millen, J., Gligor, V.: Non-interference, who needs it? In: 14th IEEE
Computer Security Foundations Workshop (CSFW 2001), Washington, Brussels, Tokyo, pp.
237–240. IEEE, Los Alamitos (2001)
14. Sabelfeld, A., Myers, A.C.: Language-based information-ﬂow security. IEEE Journal on Se-
lected Areas in Communications 21(1), 5–19 (2003)
15. Sabelfeld, A., Sands, D.: A per model of secure information ﬂow in sequential programs.
Higher-Order and Symbolic Computation 14(1), 59–91 (2001)
16. Sabelfeld, A., Sands, D.: Dimensions and principles of declassiﬁcation. In: CSFW 2005:
Proceedings of the 18th IEEE Computer Security Foundations Workshop (CSFW 2005),
Washington, DC, USA, pp. 255–269. IEEE Computer Society, Los Alamitos (2005)
17. Sabelfeld, A., Sands, D.: Declassiﬁcation: Dimensions and principles. Journal of Computer
Security (2007)

A General Framework for Nondeterministic,
Probabilistic, and Stochastic Noninterference
Alessandro Aldini and Marco Bernardo
University of Urbino “Carlo Bo” – Italy
Information Science and Technology Institute
Abstract. We introduce a notion of stochastic noninterference aimed
at extending the classical approach to information ﬂow analysis with
ﬁne-grain information describing the temporal behavior of systems. In
particular, we refer to a process algebraic setting that joins durational
activities expressing time passing through exponentially distributed
random variables, zero duration activities allowing for prioritized/
probabilistic choices, and untimed activities with unspeciﬁed duration. In
this setting unifying time, priority, probability, and nondeterminism, we
highlight the expressive power of stochastic noninterference with respect
to the existing deﬁnitions of nondeterministic and probabilistic noninter-
ference. From this comparison, we obtain that stochastic noninterference
turns out to be very strict and limiting in real-world applications and,
therefore, requires the use of relaxation techniques. Among them we ad-
vocate performance evaluation as a means for achieving a reasonable
balance between security requirements and quality.
1
Fine-Grain Models and Noninterference
Information ﬂow analysis is a basic approach to the veriﬁcation of security prop-
erties of systems which, in general, require to control who has access to what and
when. Among the several conditions that describe the characteristics of unau-
thorized information ﬂows, called covert channels, one of the most interesting,
for its intuitive and wide-used idea, is the noninterference requirement [11]. Very
brieﬂy, in a multi-level secure system, the user at the high security level should
not be able to aﬀect what the user at the low security level can observe. Inde-
pendently of the speciﬁc formalization of this notion, the underlying approach
is based on the idea that checking noninterference is actually checking the indis-
tinguishability of the diﬀerent low-level views of the system that are obtained
by changing the high-level behavior, see e.g. [9,17,22] and the references therein.
Along the 90s generalized notions of noninterference were designed to analyze
deterministic and nondeterministic systems. However, it was immediately clear
that moving to a quantitative framework including ﬁne-grain information, such
as probability distributions associated with event execution, augments the dis-
tinguishing power of the observer [3,14,18,19]. More recently, the awareness that
perfect noninterference is diﬃcult to achieve has urged to estimate the infor-
mation ﬂow by employing this quantitative information and a relaxed notion of
P. Degano and L. Vigan`o (Eds.): ARSPA-WITS 2009, LNCS 5511, pp. 18–33, 2009.
c
⃝Springer-Verlag Berlin Heidelberg 2009

Nondeterministic, Probabilistic, and Stochastic Noninterference
19
noninterference, see e.g. [4] and the references therein. A quantitative notion of
noninterference can be based not only on probabilistic information, but also on
temporal information. In particular, in this paper we refer to the representation
of time passing that uses non-negative random variables, which is particularly
appropriate when the time taken by an event ﬂuctuates according to some prob-
ability distribution. Among the many probability distributions that can be used
to model event timing, we concentrate on exponential distributions, which are
equipped with a widely developed theory.
As known in the probabilistic setting, the more information is added to the
system, the higher the number of potential vulnerabilities revealed through ﬁne-
grain notions of noninterference. Hence, a noninterference property taking into
account the timing of events, described through exponentially distributed ran-
dom variables, would be sensitive to stochastically timed covert channels. In the
literature, some proposals concerned with discrete-time extensions of noninter-
ference have been proposed, e.g. in the setting of real-time process algebra [8]
and probabilistic timed automata [7,12], while other time-based aspects of non-
interference have been investigated in the general setting of Shannon information
theory, see e.g. [13]. However, to the best of our knowledge, no formal deﬁnition
of noninterference has been proposed in the case of stochastic time.
This paper faces the problem of deﬁning noninterference in the setting of a
stochastic process algebra encompassing nondeterminism, probability, priority,
and stochastic time. The semantics of this paradigm is deﬁned in terms of a
ﬁne-grain notion of bisimulation that extends the classical bisimulation relation
of Milner [15]. In this general framework, it is possible to formalize not only
stochastic noninterference properties, but also to evaluate the expressive power
that is gained when adding ﬁne-grain information in an incremental way. The
result we obtain is that when moving from nondeterministic noninterference to
ﬁne-grain deﬁnitions of noninterference, security constraints become severe and
hard to accomplish in practice. In the most restrictive scenario where temporal
behavior and stochastic noninterference come into play, we will see that these
constraints may be impractical, so that in real-world applications we need re-
laxation techniques aiming at trading the amount of (unavoidable) information
leakage with the complexity of the adopted securing countermeasures.
We will discuss the expressive power of the diﬀerent notions of noninterference
through a client-server example with two access clearance levels, high and low.
The clients compete for the resource on the basis of their access clearance (as-
sume that high-level clients take precedence over low-level clients). The service
center is composed of two buﬀers of capacity 1 – one for each security level –
and one shared server providing a service to the clients of both levels. This toy
example is more than enough to highlight the severeness of the constraints to
which a completely secure system should be subjected.
The paper is organized as follows. In Sect. 2 we describe the stochastic process
algebra framework, on the base of which in Sect. 3 we introduce the stochastic
version of noninterference together with a comparison with the nondeterminis-
tic/probabilistic setting. Some conclusions are drawn in Sect. 4.

20
A. Aldini and M. Bernardo
2
Stochastic Process Algebra Framework
The general framework for the analysis of ﬁne-grain noninterference we use is
based on a Markovian process calculus extended with prioritized/weighted im-
mediate actions, in which multiway communication is based on a mixture of the
generative and reactive models of [10]. All these features, which are equipped
with a widely developed theory, provide the expressiveness needed to model
ﬁne-grain details of real systems.
In the following, we explain the syntax and semantics of this calculus, how
to derive nondeterministic and probabilistic sub-calculi, and the bisimulation
semantics.
2.1
Markovian Process Calculus
The basic elements of the calculus are durational actions. They are represented
as pairs of the form <a, ˜λ>, where a is the action name and ˜λ is the action rate.
There are three kinds of actions: exponentially timed, immediate, and passive.
Exponentially timed actions are of the form <a, λ> with λ ∈R>0. The du-
ration of each such action is exponentially distributed with parameter equal to
the action rate (hence its average duration is the inverse of its rate). Whenever
several exponentially timed actions are enabled, the race policy is adopted, hence
the action that is executed is the fastest one. As a consequence, the execution
probability of an exponentially timed action is proportional to its rate.
Immediate actions are of the form <a, ∞l,w>, where l ∈N>0 is the priority
level and w ∈R>0 is the weight. Each immediate action has duration zero and
takes precedence over exponentially timed actions, which are assumed to have
priority level 0. Whenever several immediate actions are enabled, the generative
preselection policy is adopted. This means that the lower priority immediate
actions are discarded, whereas each of the highest priority immediate actions is
given an execution probability equal to the action weight divided by the sum of
the weights of all the highest priority immediate actions.
Passive actions are of the form <a, ∗l′
w>, where l′ ∈N is the priority con-
straint and w ∈R>0 is the weight. Each passive action with priority constraint
l′ can synchronize only with another passive action having the same name and
the same priority constraint, or with a non-passive action having the same name
and priority level l such that l = l′. Whenever several passive actions are enabled,
the reactive preselection policy is adopted. This means that, within every set of
enabled passive actions having the same name, each such action is given an exe-
cution probability equal to the action weight divided by the sum of the weights
of all the actions in the set. Instead, the choice among passive actions having dif-
ferent names is nondeterministic. Likewise, the choice between a passive action
and a non-passive action is nondeterministic.
Deﬁnition 1. Let Act = Name × Rate be a set of actions, with Name being a
set of action names containing a distinguished symbol τ for the invisible action
and Rate = R>0 ∪{∞l,w | l ∈N>0 ∧w ∈R>0} ∪{∗l′
w | l′ ∈N ∧w ∈R>0} being

Nondeterministic, Probabilistic, and Stochastic Noninterference
21
a set of action rates (ranged over by ˜λ). The set L of process terms is generated
by the following syntax:
P ::= 0 | <a, ˜λ>.P | P + P | A | P/L | P ∥S P
where L, S ⊆Name −{τ} and A is a process constant deﬁned through the (pos-
sibly recursive) equation A
∆= P.
The semantics for the set P of closed and guarded process terms of L is deﬁned
in the usual operational style. The behavior of each term P is given by a labeled
multitransition system [[P]], whose states correspond to process terms and whose
transitions – each of which has a multiplicity equal to the number of proofs of
its derivation – are labeled with actions.
The null term 0 cannot execute any action, hence the corresponding semantics
is given by a state with no transitions. The action preﬁx term <a, ˜λ>.P can
execute an action with name a and rate ˜λ and then behaves as P:
<a, λ>.P
a,λ
−−−→P
<a, ∞l,w>.P
a,∞l,w
−−−→P
<a, ∗l′
w>.P
a,∗l′
w
−−−→P
The alternative composition P1 + P2 behaves as either P1 or P2 depending on
whether P1 or P2 executes an action ﬁrst:
P1
a,˜λ
−−−→P ′
P1 + P2
a,˜λ
−−−→P ′
P2
a,˜λ
−−−→P ′
P1 + P2
a,˜λ
−−−→P ′
The process constant A behaves as the right-hand side process term in its deﬁning
equation:
P
a,˜λ
−−−→P ′
A
∆= P
A
a,˜λ
−−−→P ′
The hiding term P/L behaves as P with the diﬀerence that the name of every
action executed by P that belongs to L is turned into τ:
P
a,˜λ
−−−→P ′
a ∈L
P/L
τ,˜λ
−−−→P ′/L
P
a,˜λ
−−−→P ′
a /∈L
P/L
a,˜λ
−−−→P ′/L
The parallel composition P1 ∥S P2 behaves as P1 in parallel with P2 as long as
actions are executed whose name does not belong to S:
P1
a,˜λ
−−−→P ′
1
a /∈S
P1 ∥S P2
a,˜λ
−−−→P ′
1 ∥S P2
P2
a,˜λ
−−−→P ′
2
a /∈S
P1 ∥S P2
a,˜λ
−−−→P1 ∥S P ′
2
Generative-reactive synchronizations are forced between any non-passive action
executed by one term and any passive action executed by the other term that
have the same name belonging to S and the same priority level/constraint:

22
A. Aldini and M. Bernardo
P1
a,λ
−−−→P ′
1
P2
a,∗0
w
−−−→P ′
2
a ∈S
P1 ∥S P2
a,λ·
w
weight(P2,a,0)
−−−−−−−−−−−−−−−→P ′
1 ∥S P ′
2
P1
a,∗0
w
−−−→P ′
1
P2
a,λ
−−−→P ′
2
a ∈S
P1 ∥S P2
a,λ·
w
weight(P1,a,0)
−−−−−−−−−−−−−−−→P ′
1 ∥S P ′
2
P1
a,∞l,v
−−−→P ′
1
P2
a,∗l
w
−−−→P ′
2
a ∈S
P1 ∥S P2
a,∞l,v·
w
weight(P2,a,l)
−−−−−−−−−−−−−−−→P ′
1 ∥S P ′
2
P1
a,∗l
w
−−−→P ′
1
P2
a,∞l,v
−−−→P ′
2
a ∈S
P1 ∥S P2
a,∞l,v·
w
weight(P1,a,l)
−−−−−−−−−−−−−−−→P ′
1 ∥S P ′
2
where weight(P, a, l) = {| w | ∃P ′ ∈P. P
a,∗l
w
−−−→P ′ |}. Following [6], the adopted
synchronization discipline mixes generative and reactive probabilistic models.
Among all the enabled timed actions whose names belong to a synchronization
set, the proposal of an action name is generated after a selection based on the
rates of those actions. Then the enabled passive actions that have the same name
as the proposed one react by means of a selection based on their weights. Finally,
the timed action winning the generative selection and the passive action winning
the reactive selection synchronize with each other.
Reactive-reactive synchronizations are forced between any two passive actions
of the two terms that have the same name belonging to S and the same priority
constraint:
P1
a,∗l
w1
−−−→P ′
1
P2
a,∗l
w2
−−−→P ′
2
a ∈S
P1 ∥S P2
a,∗l
w1
weight(P1,a,l) ·
w2
weight(P2,a,l) ·(weight(P1,a,l)+weight(P2,a,l))
−−−−−−−−−−−−−−−−−−−−−−−−−−−→
P ′
1 ∥S P ′
2
We will use the abbreviation P\S to stand for P ∥S 0, which intuitively de-
scribes the behavior of a restriction operator. Moreover, if P
a1,˜λ1
−−−→. . .
an,˜λn
−−−→P ′,
with n ∈N, then we say that P ′ is a derivative of P and we denote with Der(P)
the set of derivatives of P. We denote with Ppc the set of performance closed
process terms of P, i.e. those terms with no passive transitions. The stochastic
process underlying P ∈Ppc is a continuous-time Markov chain (CTMC) pos-
sibly extended with immediate transitions. States having outgoing immediate
transitions are called vanishing as the sojourn time in these states is zero. In
order to retrieve a pure CTMC stochastically equivalent to an extended CTMC,
all the vanishing states can be adequately eliminated.
Example 1. Let us exemplify the use of the operators through the running ex-
ample modeling the client-server system. The arrival process for clients of level
k ∈{L, H}, where L stands for low access level and H stands for high access
level, can be modeled as:
AL
∆= <aL, λL>.AL
and
AH
∆= <aH, λH>.AH

Nondeterministic, Probabilistic, and Stochastic Noninterference
23
where λk is the parameter of the Poisson arrival process for clients of access level
k. The buﬀer for clients of access level k – which we assume to have capacity 1
– can be modeled as a completely passive entity as follows:
Bk
∆= <ak, ∗0
w>.<dk, ∗f(k)
w
>.Bk
where, for instance, we assume f(L) = 1 and f(H) = 2 to denote that high-level
requests pre-empt low-level ones. The shared server can be modeled as follows:
S
∆= 
k∈{L,H} <dk, ∞f(k),w>.<sk, µk>.S
where µk is the service rate for clients of access level k. Hence, the overall client-
server system can be modeled as follows:
CS
∆= (AL ∥∅AH) ∥{aL,aH} (BL ∥∅BH) ∥{dL,dH} S
2.2
Nondeterministic and Probabilistic Sub-calculi
From the calculus above it is straightforward to derive two sub-calculi: a non-
deterministic one and a probabilistic one. The former is obtained by removing
all the ﬁne-grain information related to time, probabilities, and priorities. Syn-
tactically, durational actions of the form <a, λ> and <a, ∞l,w> are replaced by
the output action a, while passive actions of the form <a, ∗l
w> become the input
action a∗. Semantically, the result is a classical calculus with CSP-like commu-
nication, whose underlying model is a labeled transition system. We denote with
nd(P) the nondeterministic version of process P obtained in this way.
The latter sub-calculus is obtained by removing all the ﬁne-grain informa-
tion related to time and priorities. Syntactically, the exponentially timed ac-
tion <a, λ> is mapped to the immediate action <a, ∞0,λ>. In this way, the
action rate is interpreted as the action weight that governs the execution prob-
ability of the action. The immediate action <a, ∞l,w> is simply mapped to
<a, ∞0,w>, thus losing the information about its priority. Similarly, the passive
action <a, ∗l
w> is turned into the action <a, ∗0
w>. Semantically, the result is a
calculus where the execution probabilities of the actions are calculated on the
basis of the associated weights. The underlying model for this calculus is a mix-
ture of the generative and reactive probabilistic transition systems. A variant of
this calculus, where the probabilities are associated with the operators rather
than attached to the actions as weights, has been introduced in [6] and used in [3]
to deﬁne probabilistic noninterference. We denote with pr(P) the probabilistic
version of process P obtained in this way.
2.3
Bisimulation Semantics
We now recall from [5] an extension of Markovian bisimilarity, whose basic idea
is to compare the exit rates of the process terms by taking into account the
various kinds of actions (exponentially timed, immediate, and passive). This is
accomplished by parameterizing the notion of exit rate with respect to a number
in Z representing the priority level of the action, which is 0 if the action is
exponentially timed, l if the action rate is ∞l,w, −l′ −1 if the action rate is ∗l′
w.

24
A. Aldini and M. Bernardo
Deﬁnition 2. Let P ∈P, a ∈Name, l ∈Z, and C ⊆P. The exit rate of P
when executing actions with name a and priority level l that lead to C is deﬁned
through the following non-negative real function:
rate(P, a, l, C) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
{| λ | ∃P ′ ∈C. P
a,λ
−−−→P ′ |}
if l = 0
{| w | ∃P ′ ∈C. P
a,∞l,w
−−−→P ′ |}
if l > 0
{| w | ∃P ′ ∈C. P
a,∗−l−1
w
−−−→P ′ |}
if l < 0
where each sum is taken to be zero whenever its multiset is empty.
Extended Markovian bisimilarity compares the process term exit rates for all
possible action names and priority levels, except for those actions that will al-
ways be pre-empted by higher priority actions of the form <τ, ∞l,w>. We denote
by priτ
∞(P) the priority level of the highest priority internal immediate action
enabled by P, and we set priτ
∞(P) = 0 if P does not enable any internal im-
mediate action. Moreover, given l ∈Z, we use no-pre(l, P) to denote that no
action of level l can be pre-empted in P. Formally, this is the case whenever
l ≥priτ
∞(P) or −l −1 ≥priτ
∞(P).
Deﬁnition 3. An equivalence relation B ⊆P × P is an extended Markovian
bisimulation iﬀ, whenever (P1, P2) ∈B, then for all action names a ∈Name,
equivalence classes C ∈P/B, and priority levels l ∈Z such that no-pre(l, P1)
and no-pre(l, P2):
rate(P1, a, l, C) = rate(P2, a, l, C)
Extended Markovian bisimilarity, denoted by ∼EMB, is the union of all the ex-
tended Markovian bisimulations.
We relax the deﬁnition of exit rate by means of a notion of reachability that
involves the internal actions with zero duration <τ, ∞l,w>, which are unobserv-
able. The idea is that, if a given class of process terms is not reached directly
after executing an action with a certain name and level, then we have to explore
the possibility of reaching that class indirectly via a ﬁnite-length unobservable
path starting from the term reached after executing the considered action.
Deﬁnition 4. Let P ∈P and l ∈N>0. We say that P is l-unobservable iﬀ
priτ
∞(P) = l and P does not enable any immediate non-τ-action with priority
level l′ ≥l, nor any passive action with priority constraint l′ ≥l.
Deﬁnition 5. Let n ∈N>0 and P1, P2, . . . , Pn+1 ∈P. A path π of length n:
P1
τ,∞l1,w1
−−−→P2
τ,∞l2,w2
−−−→. . .
τ,∞ln,wn
−−−→Pn+1
is unobservable iﬀfor all i = 1, . . . , n process term Pi is li-unobservable. In that
case, the probability of executing path π is given by:
prob(π) =
n
i=1
wi
rate(Pi,τ,li,P)
Deﬁnition 6. Let P ∈P, a ∈Name, l ∈Z, and C ⊆P. The weak exit rate at
which P executes actions of name a and level l that lead to C is deﬁned through
the following non-negative real function:

Nondeterministic, Probabilistic, and Stochastic Noninterference
25
ratew(P, a, l, C) =

P ′∈Cw
rate(P, a, l, {P ′}) · probw(P ′, C)
where Cw is the weak backward closure of C:
Cw = C ∪{Q ∈P −C | Q can reach C via unobservable paths}
and probw is a R]0,1]-valued function representing the sum of the probabilities of
all the unobservable paths from a term in Cw to C:
probw(P ′, C)=

1
if P ′ ∈C
{| prob(π) | π unobs. path from P ′ to C |}
if P ′ ∈Cw −C
When comparing process term weak exit rates, besides taking pre-emption into
account, we also have to skip the comparison for classes that contain certain
unobservable terms. More precisely, we distinguish among observable, initially
unobservable, and fully unobservable terms. An observable term is a term that
enables a non-τ-action that cannot be pre-empted by any enabled immediate
τ-action. An initially unobservable term is a term in which all the enabled non-
τ-actions are pre-empted by some enabled immediate τ-action, but at least one of
the paths starting at this term with one of the higher priority enabled immediate
τ-actions reaches an observable term. A fully unobservable term is a term in
which all the enabled non-τ-actions are pre-empted by some enabled immediate
τ-action, and all the paths starting at this term with one of the higher priority
enabled immediate τ-actions are unobservable (note that 0 is fully unobservable).
The weak exit rate comparison with respect to observable and fully unob-
servable classes must obviously be performed. In order to maximize the abstrac-
tion power in the presence of quantitative information attached to immediate
τ-actions, the comparison should be conducted with respect to the whole set
Pfu of fully unobservable process terms of P. By contrast, the comparison with
respect to initially unobservable classes should be skipped as each of them can
reach an observable class.
Deﬁnition 7. An equivalence relation B ⊆P ×P is a weak extended Markovian
bisimulation iﬀ, whenever (P1, P2) ∈B, then for all action names a ∈Name and
levels l ∈Z such that no-pre(l, P1) and no-pre(l, P2):
ratew(P1, a, l, C) = ratew(P2, a, l, C)
for all observable C ∈P/B
ratew(P1, a, l, Pfu) = ratew(P2, a, l, Pfu)
Weak extended Markovian bisimilarity, denoted by ≈EMB, is the union of all the
weak extended Markovian bisimulations.
As examples of weakly extended Markovian bisimilar process terms we mention:
P1 ≡<a, λ>.<τ, ∞l,w>.<b, µ>.0
P2 ≡<a, λ>.<b, µ>.0
and also:
P3 ≡<a, λ>.(<τ, ∞l,w1>.<b, µ>.0 + <τ, ∞l,w2>.<c, γ>.0)
P4 ≡<a, λ ·
w1
w1+w2 >.<b, µ>.0 + <a, λ ·
w2
w1+w2 >.<c, γ>.0
which is related to vanishing state elimination. We also point out that ≈EMB can
abstract not only from intermediate immediate τ-actions, but also from interme-
diate unobservable self-loops, consistently with the fact that the probability to
escape from them is 1. Moreover, ≈EMB cannot abstract from initial immediate

26
A. Aldini and M. Bernardo
τ-actions, otherwise compositionality with respect to the alternative composi-
tion operator would be broken. The careful classiﬁcation of states on the basis
of their functional and performance observability is a key ingredient thanks to
which congruence and axiomatization can be achieved for ≈EMB. In particular,
compositionality with respect to parallel composition is preserved by restricting
to a well-prioritized subset of non-divergent process terms of P [5].
3
Noninterference Properties
In a typical two-level access system we distinguish between the high security
access level and the low security access level. Information ﬂowing from low level
to high level is authorized, while the inverse ﬂow is not. The noninterference
analysis aims at revealing direct and indirect information ﬂows, called covert
channels, in the unauthorized path. In a process algebraic setting this classiﬁca-
tion of security levels is realized by assuming that the set Name of action names
includes a set NameL ⊆Name of low-level names and a set NameH ⊆Name
of high-level names, such that NameL ∩NameH = ∅. These sets describe the
interactions between the system and the low-level and high-level parts of the
environment, respectively. We can also have that NameL ∪NameH = Name. If
this is not the case, we assume that the remaining action names can be disre-
garded as they are not related to interactions with the users at diﬀerent access
clearances. For instance, they could represent synchronizing activities performed
by diﬀerent system components that cooperate to handle the service requests.
Hence, they will be hidden by turning them into unobservable actions.
In the following, we describe two noninterference properties relying on this
classiﬁcation and on the bisimulation semantics, named strong noninterference
property and strong local noninterference property.
3.1
Bisimulation-Based Strong Noninterference
The requirement at the base of the lack of any interference from high level to low
level can be easily expressed by a property called Strong Nondeterministic Nonin-
terference, which informally says that a system is secure if its observable low-level
behavior is the same in the presence and in the absence of high-level interac-
tions. The stochastic version of this property is called Bisimulation-based Strong
Stochastic Noninterference (BSSNI ) and is deﬁned as follows, where P/NameH
expresses the system view in the presence of high-level actions, which are hidden
because they cannot be explicitly seen by a low-level observer, while P\NameH
represents the system view whenever the high-level actions are absent, i.e. they
are prevented from execution by means of the restriction operator.
Deﬁnition 8. (BSSNI) P ∈Ppc is secure iﬀP/NameH ≈EMB P\NameH.
For brevity, we assume that P is performance closed. For the formal treatment of
open systems with respect to high-level inputs, the interested reader is referred
to [3]. The nondeterministic version of BSSNI , termed BSNNI [9], is obtained

Nondeterministic, Probabilistic, and Stochastic Noninterference
27
  

 
  
 
  
 
  
  
  
  
 
 
  

  

 
    

  
L
a
L
a
L
a
L
a
L
a
L
a
L
s
L
s
L
s
L
s
L
a
L
a
L
s
L
s
L
a
L
s
τ
τ
τ
τ
τ
τ
τ
τ
τ
τ
τ
τ
τ
τ
τ
τ
τ
τ
τ
H
s
H
a
H
a
H
a
H
a
H
a
H
a
H
s
H
s
L
L
L
H
s
L
a
L
a
L
a
L
a
L
a
s
s
s
high−level
with
high−level
without
Fig. 1. Nondeterministic version of the client-server system and its low-level views
in this framework by replacing ≈EMB with the Milner’s weak bisimilarity [15],
denoted by ≈B.
Deﬁnition 9. (BSNNI) nd(P) is secure iﬀnd(P)/NameH ≈B nd(P)\NameH.
Similarly, the probabilistic version of BSSNI, termed BSPNI [3], is deﬁned in
the same framework by replacing ≈EMB with the weak probabilistic bisimilarity
of [6], denoted by ≈PB.
Deﬁnition 10.(BSPNI)pr(P) is secure iﬀpr(P)/NameH ≈PB pr(P)\NameH.
Example 2. We now discuss an incremental analysis of the client-server system
introduced in Ex. 1. This is done by considering the three ﬁne-grain notions
of noninterference in order to show which kind of information ﬂow can be re-
vealed and what strategies should be implemented to circumvent the related

28
A. Aldini and M. Bernardo
covert channels. We start by classifying the activities performed by the client-
server system into low level and high level. The low-level client is represented
by process AL, thus the action name modeling the arrival process, aL, and the
action name modeling the delivered service, sL, are the unique action names in
NameL. Similarly, given that AH represents the high-level client, we assume that
NameH = {aH, sH}. The communications between the buﬀers and the server
represent internal activities among components of the service center. Thus, they
should not be observed by any client. Hence, the noninterference check is applied
to system CS/{dL, dH} even if, for brevity, we will continue to refer to CS.
First, consider the property BSNNI and the system nd(CS). The underlying
semantic model is represented by the labeled transition system [[nd(CS)]] in
the upper part of Fig. 1, which also includes in its lower part the two labeled
transition systems to compare according to the noninterference property. The
weak bisimulation relating these two sub-systems is illustrated by graphically
representing in the same way the states that belong to the same class. Hence,
it is easy to see that the noninterference check is satisﬁed and the functional
behavior of the system is secure. Intuitively, the availability to serve the low-level
requests is never compromised independently of the behavior of the high-level
client.
Second, consider the property BSPNI and the system pr(CS). The semantics
of this system is obtained from [[nd(CS)]] by enriching every edge in Fig. 1 with
a weight. The intuition is that with this extension we add ﬁne-grain informa-
tion which allow nondeterministic choices to be solved according to probability
distributions modeling the quantitative behavior of system activities. In spite of
this enriched, detailed description, the noninterference check based on BSPNI
is satisﬁed and the system turns out to be secure. As a sketch of the proof of
this result, consider the semantics of the sub-system pr(CS)\NameH.
From the unique state of class • it is possible to reach, through the action aL,
class □with probability 1. Then, after an internal move within the same class,
the unique probabilistic choice of this sub-system governs the choice among the
action sL leading to class • and the action aL leading to class ◦. Finally, from class
◦the unique enabled transition, which is labeled with sL, leads to class □with
probability 1. Now, consider the semantics of the sub-system pr(CS)/NameH.
From states of class • it is possible to pass, through the weak transition τ∗aL, to
class □with probability 1. With the same probability, we then reach, by following
unobservable paths, one of two states of class □enabling the same probabilistic
choice between sL and aL that characterizes the former sub-system. Finally, the
two states of class ◦lead with probability 1 to states of class □through the weak
transition τ ∗sL. Summing up, adding probabilistic information to the system
does not increase the discriminating power of the low-level observer, because any
high-level activity is not able to alter the unique low-level probabilistic choice.
Third, consider the property BSSNI and the system CS. The intuition is that
with this extension the clients observe the passage of time and the priority-based
behavior of the server. The introduction of this ﬁne-grain information causes an
interfering information ﬂow from high level to low level, which is revealed by the

Nondeterministic, Probabilistic, and Stochastic Noninterference
29
violation of the BSSNI condition. In the following, we show the nature of these
covert channels and how the diagnostic information provided by ≈EMB has been
exploited to avoid them.
On the one hand, the high-level process AH is immediately revealed by the
low-level client. Indeed, AH/NameH describes a working process that, accord-
ing to the race policy, competes with the other durational processes, while
AH\NameH does not (ﬁrst interference). On the other hand, from the view-
point of the low-level client, the time spent by the server to deliver the high-
level services describes an observable busy waiting phase (second interference).
From a performance-oriented perspective, these interferences aﬀect the low-level
throughput of the client-server system, in terms of number of sL actions exe-
cuted per unit of time, which is higher in the absence of high-level interactions.
The removal of the two interferences requires strong control mechanisms that,
as expected, aim at degrading the performance of the system in order to make
the behavior of the high-level client transparent to the low-level client.
As far as the ﬁrst interference is concerned, it is necessary to conﬁne the
behavior of the high-level client in order to hide its impact on the timing of the
system activities. This can be done by employing a sort of high-level box which,
somehow, limits and controls the activities performed by the high-level client.
Formally, we replace process AH with the following one:
H box
∆= <τ, ∞l,w>.
(<aH, ∞h+1,w>.<aH, λH>.H box + <τ, ∞h,w>.<τ, λH>.H box)
The action <τ, ∞l,w> represents the activation of the box and is technically
needed because it allows ≈EMB to abstract from the subsequent intermediate
immediate τ-actions. The branch guarded by the action <τ, ∞h,w> is enabled
if and only if the high-level client is absent. Its role is to simulate the presence
of such a client in a way that makes the high-level behavior invisible to the
low-level client. Intuitively, process H box is a black box acting as an interface
between the system and the high-level client, who can interact with such a box
by pushing a button whenever he decides to communicate with the system.
As far as the second interference is concerned, making the server transparent
to the low-level client is achieved by following an approach borrowed by round-
robin scheduling strategies. In practice, the intuition is similar to that underlying
the deﬁnition of process H box. The service activities are divided into temporal
slots, each one dedicated to a class of clients in a round-robin fashion. Inde-
pendently of the presence of a pending request from a client of the currently
managed class, the temporal slot is spent. In this way a low-level client cannot
deduce whether the high-level slot has been actively exploited by the high-level
client. Formally, we replace process S with the following one:
SL
∆= <dL, ∞f(L),w>.<sL, µL>.SH + <τ, µL>.SH
SH
∆= <dH, ∞f(H),w>.<sH, µH>.SL + <τ, µH>.SL
This is not enough to hide completely the interference of the high-level client.
Indeed, whenever such a client is blocked because the related buﬀer is not avail-
able to accept further requests, then process H box does not compete for the
resource time. This observable behavior would reveal to the low-level client that

30
A. Aldini and M. Bernardo
the high-level buﬀer is full. The covert channel can be avoided by changing the
high-level buﬀer as follows:
BH
∆= <aH, ∗0
w>.B′
H
B′
H
∆= <dH, ∗f(H)
w
>.BH + <τ, λH>.B′
H
3.2
Bisimulation-Based Strong Local Noninterference
The introduction of ﬁne-grain information about time makes the satisfaction of
noninterference properties a very hard task, which can be accomplished through
invasive strategies aiming at controlling the temporal behavior of the system.
This claim is strengthened by the fact that we employed one of the least re-
strictive noninterference properties in the literature. As an example, we ignored
the interference caused by a high-level user that blocks the high-level output
modeling the service delivery, because this problem can be easily avoided by
making such a communication asynchronous [1]. This and more subtle problems
can be revealed by security properties that have been deﬁned in the literature
with the aim of capturing more information ﬂows with respect to the noninter-
ference property surveyed above. For instance, the strongest property of [9] is the
Strong Bisimulation Nondeducibility on Compositions, which corresponds to the
Strong Local Noninterference property that has been separately deﬁned in [16].
In our framework, we consider such a property under the name Bisimulation-
based Strong Stochastic/Probabilistic/Nondeterministic Local Noninterference,
respectively, depending on the nature of the ﬁne-grain information we take into
consideration. The underlying intuition states that the absence of any interfer-
ence is ensured when the low-level user cannot distinguish which, if any, high-
level event has occurred at some point in the past.
Deﬁnition 11. (BSSLNI) P is secure if and only if ∀P ′ ∈Der(P) and ∀P ′′
such that P ′
a,˜λ
−−−→P ′′, with a ∈NameH, then P ′\NameH ≈EMB P ′′\NameH.
In essence, this property states that the low-level view of the system is not
aﬀected by the execution of a high-level action. The nondeterministic version,
BSNLNI , and the probabilistic version, BSPLNI , can be derived as before. How-
ever, the introduction of the temporal aspect changes the nature of the relations
among these properties. The reason stems from the ﬁne-grain information asso-
ciated with the high-level actions. The intuition is that the strong local nonin-
terference property analyzes the low-level view of the system before and after
the execution of a high-level action without taking into account neither the time
spent by its execution (if it is durational) nor its priority (if it is immediate). In
practice, in the stochastic setting the original notions of strong noninterference
and strong local noninterference cannot be compared.
Theorem 1. (Inclusion relations)
(i)
BSNLNI ⊂BSNNI
(ii) BSPLNI ⊂BSPNI
(iii) BSSLNI ̸⊂BSSNI

Nondeterministic, Probabilistic, and Stochastic Noninterference
31
Proof. (i) and (ii) are standard results shown in [9] and [3], respectively. To show
(iii), consider the process term P7 ≡<h, λ>.<l, µ>.0+<l, µ>.0, which satisﬁes
BSSLNI , because its low-level view is <l, µ>.0 before and after the execution of
the high-level action. However, P7 is not BSSNI secure, because <l, µ>.0 and
P8 ≡<τ, λ>.<l, µ>.0 + <l, µ>.0 are not weakly extended Markovian bisimilar.
The motivation is given by the race policy between the two exponentially timed
actions. Similarly, consider the process term P9 ≡<h, ∞2,w>.P10 + P10, where
P10 ≡<l, ∞1,w>.0+<l′, ∞2,w>.0. P9 is clearly BSSLNI secure, but not BSSNI
secure: in P9/NameH the low-level action with name l is initially pre-empted,
thus altering the probability of executing the two low-level actions.
Example 3. As far as the client-server system is concerned, nd(CS) and pr(CS)
satisfy BSNLNI and BSPLNI , respectively, provided that the delivery of the
high-level service is made asynchronous. Such a relaxation is not enough to make
process CS a BSSLNI-secure system. For this purpose, all the exponentially
timed actions executed by process H box must be made invisible in order to keep
them out of the control of the high-level client. Then, process H box should be
further complicated in such a way that the (priority, probabilistic, and temporal)
low-level view of the system must be the same before and after the execution of
action <aH, ∞h,w>.
The stochastic noninterference check pinpoints the covert channels that af-
fect the quantitative behavior of the system and, therefore, the delivered quality
of service. The low-level throughput of the original client-server system oﬀers
diﬀerent results depending on the presence of high-level interactions. This dif-
ferent quantitative behavior that is exhibited by the two sub-systems is formally
captured by the ≈EMB-based check as a diagnostic information revealing the in-
terference. Estimating the metric above in the presence and in the absence of the
high-level client is a way to measure the covert channel bandwidth. The same
performance metric reveals the quality of service degradation that is paid by
making the system completely secure through the high-level black box and the
revised versions of the service center components. Hence, a balanced trade-oﬀ
between security and performance strictly depends on the value of this metric.
From the analysis standpoint, since the semantic model underlying the system
CS is a CTMC, such a metric can be easily evaluated through reward-based
steady-state numerical analysis in order to obtain an estimation of the informa-
tion leakage on the long run [20].
In conclusion, strong notions of noninterference are hard to accomplish when
modeling real-world systems that turn out to be much more complex than the
client-server example. Adding ﬁne-grain information such as time opens new sce-
narios where covert channels cannot be completely eliminated without severely
limiting the system behavior. On the basis of the discussed strategies that are
needed to pass the BSSNI-based check, it is easy to observe that the minimiza-
tion of the covert channel bandwidth corresponds to a reduction of the quality
of service. Hence, an approach towards the mitigation of the strict, unrealis-
tic constraints imposed by the noninterference properties should be based on
tolerance thresholds, which are expressed in terms of negligible diﬀerence with

32
A. Aldini and M. Bernardo
respect to a family of performance metrics related to quality of service. A sim-
ilar approach is used in [2], where the impact of the high-level strategies (and
of the related countermeasures adopted by the system) is estimated from the
performance standpoint.
4
Conclusions
The deﬁnition and analysis of stochastic noninterference is not only yet another
extension of classical nondeterministic noninterference. In fact, it allowed us to
highlight some interesting relations between the noninterference approach to in-
formation ﬂow analysis and the quality of the service delivered by real-world
systems. In a framework where time is important, the objective is not to guar-
antee the complete absence of any kind of interference, which is a task that is
either impossible to achieve or impractical because of the strong functional limi-
tations that should be imposed. Instead, the attention should pass to the analysis
of the performance measures of interest that are aﬀected by the high-level in-
terferences. The goal is to estimate whether such interferences are negligible
(e.g. because they are revealed if and only if the system execution is observed
for a long time) and, therefore, cause a tolerable amount of information leakage
in terms of sensitive data that are revealed on the long run.
From a practical standpoint, we intend to use the notion of stochastic non-
interference to support a methodology for the analysis of a balanced trade-oﬀ
between the security degree of the system and the impact of diﬀerent securing
strategies on the system quality of service. The intuition is that the stronger
the noninterference validation is, the more complicated and invasive the related
securing strategy should be. From a theoretical standpoint, it would be useful
to investigate properties stronger than BSSNI which do not imply the explicit
analysis of every possible high-level user, as instead required, e.g., by the Nond-
educibility on Compositions property.
Acknowledgement. The authors thank the anonymous referees for their valu-
able comments. This work has been funded by MIUR-PRIN project PaCo –
Performability-Aware Computing: Logics, Models, and Languages.
References
1. Aldini, A.: Classiﬁcation of security properties in a Linda-like process algebra. J.
of Science of Computer Programming 63, 16–38 (2006)
2. Aldini, A., Bernardo, M.: A formal approach to the integrated analysis of security
and QoS. J. of Reliability Engineering & System Safety 92, 1503–1520 (2007)
3. Aldini, A., Bravetti, M., Gorrieri, R.: A process-algebraic approach for the analysis
of probabilistic noninterference. J. of Computer Security 12, 191–245 (2004)
4. Aldini, A., Bravetti, M., Di Pierro, A., Gorrieri, R., Hankin, C., Wiklicky, H.: Two
formal approaches for approximating noninterference properties. In: Focardi, R.,
Gorrieri, R. (eds.) FOSAD 2001. LNCS, vol. 2946, pp. 1–43. Springer, Heidelberg
(2004)

Nondeterministic, Probabilistic, and Stochastic Noninterference
33
5. Bernardo, M., Aldini, A.: Weak Markovian bisimilarity: abstracting from priori-
tized/weighted internal immediate actions. In: 10th Italian Conf. on Theoretical
Computer Science, pp. 39–56. World Scientiﬁc, Singapore (2007)
6. Bravetti, M., Aldini, A.: Discrete time generative-reactive probabilistic processes
with diﬀerent advancing speeds. Theoretical Comp. Science 290, 355–406 (2003)
7. Di Pierro, A., Wiklicky, H.: Quantifying timing leaks and cost optimisation. In:
Chen, L., Ryan, M.D., Wang, G. (eds.) ICICS 2008. LNCS, vol. 5308, pp. 81–96.
Springer, Heidelberg (2008)
8. Focardi, R., Martinelli, F., Gorrieri, R.: Information ﬂow analysis in a discrete-time
process algebra. In: IEEE Computer Security Foundations Workshop, pp. 170–184
(2000)
9. Focardi, R., Gorrieri, R.: A classiﬁcation of security properties. J. of Computer
Security 3, 5–33 (1995)
10. van Glabbeek, R.J., Smolka, S.A., Steﬀen, B.: Reactive, generative and stratiﬁed
models of probabilistic processes. Information and Comput. 121, 59–80 (1995)
11. Goguen, J.A., Meseguer, J.: Security policy and security models. In: IEEE Symp.
on Security and Privacy, pp. 11–20 (1982)
12. Lanotte, R., Maggiolo-Schettini, A., Troina, A.: A classiﬁcation of time and/or
probability dependent security properties. In: 3rd Int. Workshop on Quantitative
Aspects of Programming Languages. ENTCS, vol. 153, pp. 177–193 (2005)
13. Mantel, H., Sudbrock, H.: Comparing countermeasures against interrupt-related
covert channels in an information-theoretic framework. In: IEEE Computer Secu-
rity Foundations Symposium, pp. 326–340 (2007)
14. McLean, J.: Security models and information ﬂow. In: IEEE Symp. on Research
in Security and Privacy, pp. 180–187 (1990)
15. Milner, R.: Communication and Concurrency. Prentice Hall, Englewood Cliﬀs
(1989)
16. Roscoe, A.W., Reed, G.M., Forster, R.: The successes and failures of behavioural
models. Millenial Perspectives in Computer Science (2000)
17. Ryan, P.Y.A., Schneider, S.A.: Process algebra and non-interference. J. of Com-
puter Security 9, 75–103 (2001)
18. Sabelfeld, A., Sands, D.: Probabilistic noninterference for multi-threaded programs.
In: IEEE Computer Security Foundations Workshop, pp. 200–214 (2000)
19. Smith, G.: Probabilistic noninterference through weak probabilistic bisimulation.
In: IEEE Computer Security Foundations Workshop, pp. 3–13 (2003)
20. Stewart, W.J.: Introduction to the numerical solution of Markov chains. Princeton
University Press, Princeton (1994)
21. Volpano, D., Smith, G.: Probabilistic noninterference in a concurrent language. In:
IEEE Computer Security Foundations Workshop, pp. 34–43 (1998)
22. Wittbold, J.T., Johnson, D.M.: Information ﬂow in nondeterministic systems. In:
IEEE Symp. on Research in Security and Privacy, pp. 144–161 (1990)

Validating Security Protocols under
the General Attacker
Wihem Arsac1, Giampaolo Bella2, Xavier Chantry1, and Luca Compagna1
1 SAP Research Labs, 805 Avenue du Dr Maurice Donat, 06250 Mougins, France
{wihem.arsac,xavier.chantry,luca.compagna}@sap.com
2 Dipartimento di Matematica e Informatica,
Universit`a di Catania, Viale A.Doria 6, 95125 Catania, Italy
giamp@dmi.unict.it
Abstract. Security protocols have been analysed using a variety of tools
and focusing on a variety of properties. Most ﬁndings assume the ever
so popular Dolev-Yao threat model. A more recent threat model called
the Rational Attacker [1] sees each protocol participant decide whether
or not to conform to the protocol upon their own cost/beneﬁt analysis.
Each participant neither colludes nor shares knowledge with anyone, a
feature that rules out the applicability of existing equivalence results in
the Dolev-Yao model. Aiming at mechanical validation, we abstract away
the actual cost/beneﬁt analysis and obtain the General Attacker threat
model, which sees each principal blindly act as a Dolev-Yao attacker.
The analysis of security protocols under the General Attacker threat
model brings forward yet more insights: retaliation attacks and anticipa-
tion attacks are our main ﬁndings, while the tool support can scale up
to the new analysis at a negligible price. The general threat model for
security protocols based on set-rewriting that was adopted in AVISPA [2]
is leveraged so as to express the General Attacker. The state-of-the-art
model checker SATMC [3] is then used to automatically validate a pro-
tocol under the new threats, so that retaliation and anticipation attacks
can automatically be found.
1
Introduction
The analysis of security protocols stands among the most attractive niches of
research in computer science, as it has attracted eﬀorts from many communities.
It is diﬃcult to even provide a satisfactory list of citations, which would have to
at least include process calculi [4,5], strand spaces [6,7], the inductive method
[8,9] and advanced model checking techniques [2,10].
Any meaningful statement about the security of a system requires a clear
speciﬁcation of the threats that the system is supposed to withstand. Such a
speciﬁcation is usually referred to as threat model. Statements that hold under a
threat model may no longer hold under other models. For example, if the threat
model only accounts for attackers that are outsiders, then Lowe’s famous attack
on the Needham-Schroeder Public-Key (NSPK) protocol [11] cannot succeed,
P. Degano and L. Vigan`o (Eds.): ARSPA-WITS 2009, LNCS 5511, pp. 34–51, 2009.
  Springer-Verlag Berlin Heidelberg 2009

Validating Security Protocols under the General Attacker
35
and the protocol may be claimed secure. But the protocol is notoriously insecure
under a model that allows the attacker as a registered principal.
The standard threat model for symbolic protocol analysis is the Dolev-Yao
model (DY in brief), which sees a powerful attacker control the whole network
traﬃc. The usual justiﬁcation is that a protocol secure under DY certainly is
secure under a less powerful, perhaps more realistic attacker. By contrast, a large
group of researchers consider DY insuﬃcient because a DY attacker cannot do
cryptanalysis, and their probabilistic reasoning initiated with a foundational pa-
per [12]. This sparked oﬀa research thread that has somewhat evolved in parallel
with the DY research line, although some eﬀorts exist in the attempt to con-
jugate them [13,14,15]. The present paper is not concerned with probabilistic
protocol analysis. Our main argument is that security protocols may still hide
important subtleties even after they are proved correct under DY. These sub-
tleties can be discovered by symbolic protocol analysis under a new threat model
that adheres to the present real world more strictly than DY does.
The new model we develop here is the General Attacker (GA in brief), which
features each protocol participant as a DY attacker who does not collude or share
knowledge with anyone else. In GA, it is meaningful to continue the analysis of a
protocol after an attack is mounted, or to anticipate the analysis by looking for
extra ﬂaws before that attack, something that has never been seen in the relevant
literature. This can assess whether additional attacks can be mounted either by
the same attacker or by diﬀerent attackers. Even novel scenarios whereby princi-
pals attack each other become possible. A signiﬁcant scenario is that of retalia-
tion [16], where an attack is followed by a counterattack. This paper recasts that
scenario into the new GA threat model. Also, a completely new scenario is de-
ﬁned, that of anticipation, where an attack is anticipated, before its termination,
by another attack by some other principal.
As its main contribution, our research tailors an existing formalism suited for
model checking to accommodate the GA threat model. This makes it possible
to analyse protocol subtleties that go beyond standard security properties such
as conﬁdentiality and authentication. We begin by extending an existing set-
rewriting formalisation of the classical DY model to capture the GA model.
Then, we leverage an established model checking tool for security protocols to
tackle the validation problems arisen from the new threat model. Finally, we run
the tool over the NSPK protocol and its variants to investigate retaliation and
anticipation attacks.
The structure of this manuscript is simple. The GA threat model is introduced
in Section 2 and then our formalisation and analysis results are described in
Sections 3 and 4. Some conclusions terminate the presentation.
2
The General Attacker
DY can be considered the standard threat model to study security protocols
[17]. The DY attacker controls the entire network although he cannot perform
cryptanalysis. Some historical context is useful here. The model was deﬁned

36
W. Arsac et al.
in the late 1970s when remote computer communications were still conﬁned
to military/espionage contexts. It was then natural to imagine that the entire
world would collude joining their forces against a security protocol session be-
tween two secret agents of the same country. The DY model has remarkably
favoured the discovery of signiﬁcant protocol, but the prototype attacker is sig-
niﬁcantly changed today. To become an attacker has never been so easy as in
the present technological setting because hardware is inexpensive, while secu-
rity skill is at hand—malicious exploits are even freely downloadable from the
web.
A seminal threat model called BUG [16], which dates back to 2002 [18],
must be recalled here. The name is a permuted acronym for the “Good”, the
“Bad” and the “Ugly”. This model attempts stricter adherence than DY’s to
the changed reality by partitioning the participants in a security protocol into
three groups. The Good principals would follow the protocol, the Bad would in
addition try to subvert it, and the Ugly would be ready to either behaviour.
This seems the ﬁrst account in the literature of formal protocol veriﬁcation on
the chance that attackers may attempt to attack each other without sharing
knowledge. More recently, Bella observed [1] that the partition of the princi-
pals had to be dynamically updated very often, in principle at each event of
a principal’s sending or receiving a message, depending on whether the princi-
pal respected the protocol or not. Thus, BUG appeared overly detailed, and he
simpliﬁed it as the Rational Attacker threat model: each principal may at any
time make cost/beneﬁt decisions to either behave according to the protocol or
not [1]. After BUG’s inception [18], homologous forms of rational attackers were
speciﬁcally carved out in the area of game theory [19,20] and therefore are, as
such, not directly related to our symbolic analysis.
Analyzing a protocol under the Rational Attacker requires specifying each
principal’s cost and beneﬁt functions, but this still seems out of reach, espe-
cially for classical model checking. By abstracting away the actual cost/beneﬁt
analysis, we derive the following simpliﬁed model:
The General Attacker (GA) threat model: each principal is a Dolev-Yao
attacker.
The change of perspective in GA with respect to DY is clear: principals do
not collude for a common aim but, rather, each of them acts for his own personal
sake. Although there is no notion of collusion constraining this model, the human
protocol analyser can deﬁne some for their particular investigations. The GA
model has each principal endowed with the entire potential of a DY attacker.
So, each principal may at any stage send any of the messages he can form to
anyone. Of course, such messages include both the legal ones, conforming to the
protocol in use, and the illegal, forged ones.
As we shall see, analysing the protocols under the GA threat model yields
unknown scenarios featuring retaliation or anticipation attacks. This paves the
way for a future analysis under the Rational Attacker. For example, if an attack
can be retaliated under GA, such a scenario will not occur under the Rational
Attacker because the cost of attacking clearly overdoes its beneﬁt, and hence the

Validating Security Protocols under the General Attacker
37
1.
A→B : {Na, A}Kb
2.
B→A : {Na, Nb}Ka
3.
A→B : {Nb}Kb
4a.
A→B : {“Transfer X1  from A’s account to Y 1’s”}⟨Na,Nb⟩
4b.
B→A : {“Transfer X2  from B’s account to Y 2’s”}⟨Na,Nb⟩
Fig. 1. NSPK++: the NSPK protocol terminated with the completion steps
attacker will not attack in the ﬁrst place. This argument is after all not striking:
even a proper evaluation of the “realism” of classical attacks found under DY
would have required a proper cost/beneﬁt analysis.
Our research suggests that if in the real world an attacker can mount an attack
that can be retaliated, then he may rationally opt for not attacking in the ﬁrst
place. In consequence, even a deployed protocol suﬀering an attack that can be
retaliated may perhaps be kept in place.
The BUG threat model was demonstrated over the public-key Needham-Schro-
eder protocol [16]. In the sequel of this section, we recast that analysis under the
GA model. The protocol version studied here, which we address as NSPK++, is its
original design terminated with the completion steps for reciprocal, authenticated
money transfers (Figure 1). It can be seen that principal A issues a fresh nonce Na
in message 1, which she sees back in message 2. Because message 1 is encrypted
under B’s public key, the nonce was fresh and cryptanalysis cannot be broken by
assumption, A learns that B acted at the other end of the network. Messages 2 and
3 give B the analogous information about A by means of nonce Nb. This protocol
version is subject to Lowe’s attack [11], as described in Figure 2. It can be seen how
the attacker C masquerades as A with B to carry out an illegal money transfer at B
(which intuitively is a bank) from A’s to C’s account. It is known that the problems
originated with the conﬁdentiality attack upon the nonce Nb. Another observation
is that there is a second nonce whose conﬁdentiality is violated—by B, not by C—
in this scenario: it is Na. Although it is invented by A to be only shared with C, also
B learns it. This does not seem to be an issue in the DY model, where all principals
except C followed the protocol like soldiers follow orders. What one of them could
do with a piece of information not meant for him therefore became uninteresting.
To what extent this is appropriate to the current real world, where there often are
various attackers with targets of their own, is at least questionable. Strictly speak-
ing, B’s learning of Na is a new attack because it violates the conﬁdentiality policy
upon the nonces, which are later used to form a session key. It can be easily cap-
tured in the GA threat model, where more than one principal may act illegally at
the same time for their own sake.
We are facing a new perspective of analysis. Principal B did not have to act to
learn a nonce not meant for him, therefore this is named an indeliberate attack
[18]. To use a metaphor, B does not know which lock the key Na can open.
This is not an issue in the GA threat model, where each principal just sends out
anything he can send to anyone. Nevertheless, there are at least four methods
to help B practically evaluate the potential of Na [21].

38
W. Arsac et al.
1.
A→C : {Na, A}Kc
1′.
C(A) →B : {Na, A}Kb
2′.
B →A : {Na, Nb}Ka
2.
C→A : {Na, Nb}Ka
3.
A→C : {Nb}Kc
3′.
C(A) →B : {Nb}Kb
4a′.
C(A) →B : {“Transfer 1000  from A’s account to C’s”}⟨Na,Nb⟩
Fig. 2. Lowe’s attack to the NSPK++ protocol
4b.
B(C) →A : {“Transfer 2000  from C’s account to B’s”}⟨Na,Nb⟩
Fig. 3. Retaliation attack following Lowe’s attack
The natural consequence of B’s learning Na is the retaliation attack [16] in
Figure 3. Note that the ﬁrst method indicated above gives B a reasonable set
of target principals to try retaliation against, while the second one gives him
a probabilistic answer originated from traﬃc analysis. However, the remaining
two methods exactly tell him who the target for retaliation is.
It is worth remarking once more that a retaliation attack cannot be captured
in the standard DY threat model, where all potential attackers merely collude
to form a super-potent one. However, the GA model can support this notion.
An important ﬁnding that will be detailed below is that B learns Na be-
fore C learns Nb (Figure 2). This may lead to the unknown scenario that sees
B steal money by step 4b from Figure 3 before C does it by step 4a′ from
Figure 2. The more quickly does B use any of the ﬁrst three methods given
above to evaluate Na and pinpoint C, the more realistic this scenario. Poten-
tially, B’s illegal activity may even succeed before message 3 reaches C disclosing
Nb. This attack will be addressed as anticipation attack.
This section attempted to motivate our interest in studying protocols beyond
the DY model. It is well known that machine support is dramatically needed
for protocol analysis. The next sections show how to tailor established model-
checking techniques to validate protocols under the GA threat model.
3
Extending the Validation Method over the General
Attacker
We have used one of the AVISPA backends to perform our experiments: the
SAT-based model checker SATMC. This tool has successfully tackled the prob-
lem of determining whether the concurrent execution of a ﬁnite number of ses-
sions of a protocol enjoys certain security properties in spite of the DY attacker

Validating Security Protocols under the General Attacker
39
[22,23]. Leveraging on that work, we aim at relaxing the assumption of a single,
super-potent attacker to specify the GA threat model, where principals can even
compete each other. It was already stated above that our aim is to study novel
protocol subtleties that go beyond the violation of standard security properties
such as conﬁdentiality and authentication. We are currently focusing on retali-
ation attacks and anticipation attacks. Also these notions can be recast into a
model checking problem, as we shall see below.
3.1
Basics of SAT-Based Model Checking
This subsection outlines the basic deﬁnitions and concepts underlying SAT-based
model checking. The reader who is familiar with such concepts can skip this. Let
us recall that a model checking problem can be stated as
M |= G
(1)
where M is a labelled transition system modelling the initial state of the system
and the behaviours of the principals (including their malicious activity) and G
is an LTL formula expressing the security property to be checked.
The states of M are represented as sets of facts i.e. ground atomic formulas
of a ﬁrst-order language with sorts. If S is a state, then its facts are interpreted
as the propositions holding in the state represented by S, all other facts being
false in that state (closed-world assumption). A state is written down by the
convenient syntax of a list of facts separated by the  symbol, as we shall see.
The transitions of M are represented as set-rewriting rules that deﬁne map-
pings between states. Each rule has a label expressing what the rule is there for:
for example, the label stepi is for a rule that formalises the i-th legal protocol
step, the label overhear is for a rule whereby an attacker reads some traﬃc, and
so on. Each rule label is parameterised by the rule variables or proper instances
of them, and we will encounter a number of self-explaining rule labels below.
Let (L
ρ−→R) be (an instance of) a rewriting rule and S be a set of facts. If
L ⊆S then we say that ρ is applicable in S and that S′ = appρ(S) = (S \L)∪R
is the state resulting from the execution of ρ in S. A path π is an alternating
sequence of states and rules S0ρ1S1 . . . Sn−1ρnSn such that Si = appρi(Si−1)
(i.e. Si is a state resulting from the execution of ρi in Si−1), for i = 1, . . . , n. Let
I be the initial state of the transition system; if S0 ⊆I, then we say that the
path is initialised. Let π = S0ρ1S1 . . . Sn−1ρnSn be a path. We deﬁne π(i) = Si
and πi = Siρi+1Si+1 . . . Sn−1ρnSn. Therefore, π(i) and πi are the i-th state of
the path and the suﬃx of the path starting with the i-th state respectively. Also,
it is assumed that paths have inﬁnite length. This can be always obtained by
adding stuttering transitions to the transition system.
The language of LTL used here has facts and equalities over ground terms
as atomic propositions, the usual propositional connectives (namely, ¬, ∨) and
the temporal operators X (next), F (eventually) and O (once). Let π be an
initialised path of M, an LTL formula φ is valid on π, written π |= φ, if and only
if (π, 0) |= φ, where (π, i) |= φ (φ holds in π at time i) is inductively deﬁned as:

40
W. Arsac et al.
(π, i) |= f
iﬀf ∈π(i) (f is a fact)
(π, i) |= (t1 = t2) iﬀt1 and t2 are the same terms
(π, i) |= ¬φ
iﬀ(π, i) ̸|= φ
(π, i) |= (φ1 ∨φ2) iﬀ(π, i) |= φ1 or (π, i) |= φ2
(π, i) |= Xφ
iﬀ(π, i + 1) |= φ
(π, i) |= Fφ
iﬀ∃j ∈[i, ∞).(π, j) |= φ
(π, i) |= Oφ
iﬀ∃j ∈[0, i].(π, j) |= φ
In the sequel we use (φ1∧φ2), (φ1 ⇒φ2) and Gφ as abbreviations of ¬(¬φ1∨¬φ2),
(¬φ1 ∨φ2) and ¬F¬φ respectively.
In Section 3.2 we show how the behaviours of the principals can be speciﬁed
using a set-rewriting formalism. This amounts to specifying the model M of (1).
In Section 3.3 we show how interesting protocol properties can be formalised by
means of LTL formulae, which correspond to G in the problem (1)).
3.2
Formalising the General Attacker
We conveniently adopt the IF language as it can specify inputs to the AVISPA
backends and more speciﬁcally to SATMC, a successful SAT-based model checker
[22] that will be used in the ﬁnal validation phase. The following syntactical
conventions are adopted in the sequel.
– Lower-case typewriter fonts, such as na, denote IF constants.
– Upper-case typewriter fonts, such as A, denote IF variables.
– Lower-case italics fonts of 0 arity, such as s indicating a session identiﬁer,
compactly denote IF terms.
– Lower-case italics fonts of positive arity, such as attack(a, v, s), denote meta-
predicates that aim at improving the readability of this manuscript, but in
fact do not belong to the current IF formalisation.
– Upper-case italics fonts serve diverse purposes, as speciﬁed each time.
To specify the GA threat model, a number of new facts must be deﬁned in our
language. They are summarised with their informal meaning in Table 1.
If S is a set of facts representing a state, then the state of principal a is
represented by the facts of form stater(j, a, es, s) (called state-facts) and of
form ak(a, m) occurring in S. It is assumed that for each session s and for each
principal a there exists at most one fact of the form stater(j, a, es, s) in S. This
does not prevent a principal from playing diﬀerent roles in diﬀerent sessions.
The dedicated account on the attacker’s knowledge in DY must be extended
as a general account on principals’ knowledge in GA. In practice, what was the
ik fact to represent the DY attacker knowledge is now replaced by ak, which
has as an extra parameter the principal’s identity whose knowledge is being
deﬁned. Incidentally, we conveniently write down the set of facts in a state
by enumerating the facts and interleaving them a dot. Here is the deﬁnition
of ak:

Validating Security Protocols under the General Attacker
41
Table 1. New facts and their informal meaning
Fact
Holds when
stater(j, a, es, s)
Principal a, playing role r, is ready to execute step
j in session s of the protocol, and has internal state
es, which is a list of expressions aﬀecting her future
behaviour.
ak(a, m)
Principal a knows message m.
nt(a)
Principal a is not trustworthy.
c(n)
Term n is the current value of the counter used to issue
fresh terms, and is incremented as s(n) every time a
fresh term is issued.
msg(rs, b, a, m)
Principal rs has sent message m to principal a pre-
tending to be principal b.
contains(db, m)
Message m is contained into set db. Sets are used, e.g.,
to share data between honest principals.
confidential(m, g)
Message m is a secret shared among the group of prin-
cipals g (the set g is clearly populated through occur-
rences of contains(g, a) for each principal a intended
to be in g).
transferred(rs, a, b, c, x, s) rs, in the disguise of a, transferred x   from a’s account
to c′s at b (which intuitively is a bank) in session s.
ak(a, m)  ak(a, k)
encrypt(VARS (a,k,m))
−−−−−−−−−−−−−−−→
ak(a, {m}k)  LHS
ak(a, {m}k)  ak(a, k)
decrypt(VARS (a,k,m))
−−−−−−−−−−−−−−−→
ak(a, m)  LHS
ak(a, m1)  ak(a, m2)
pairing(VARS (a,m1,m2))
−−−−−−−−−−−−−−−−−→
ak(a, ⟨m1, m2⟩)  LHS
ak(a, ⟨m1, m2⟩)
decompose(VARS (a,m1,m2))
−−−−−−−−−−−−−−−−−−→ak(a, m1)  ak(a, m2)  LHS
where VARS (t1, . . . , th) and LHS abbreviate in each rule, respectively, all the
IF variables occurring in the IF terms represented by t1, . . . , th and the set of
facts occurring in the left hand side of the rule. Also, k and k are the inverse
keys of one another.
Under the GA threat model any principal may behave as a DY attacker.
We may nonetheless need to formalise protocols that encompass trusted third
parties, or where a principal loses her trustworthiness due to a certain event.
Reading through Table 1, it can be seen that our machinery features the nt(a)
predicate, holding of a principal a that is not to be trusted. It may conveniently
be used either statically, if added to the initial state of principals, or dynamically
when introduced by rewriting rules. It is understood that if all principals but
one are declared as trustworthy, then we are back to the DY threat model.
The fact c(n) holds of the current counter n used to generate fresh nonces. For
example, c(0) holds in the initial state, and c(s(0)) after the generation of the
ﬁrst fresh nonce, which takes the value 0. More generally, if the fact c(na) holds,
a fresh nonce na can be issued producing another state with counter c(s(na)).

42
W. Arsac et al.
The initial state of the system deﬁnes the initial knowledge and the state-
facts of all principals involved in the considered protocol sessions. Its standard
deﬁnition is omitted here but can be found elsewhere [3]. Appropriate rewriting
rules specify the evolution of the system. Those for honest principals, which also
serve to demonstrate the remaining facts enumerated in Table 1, are of the form:
msg(rs, b1, a, m1)  stater(i, a, es, s)
stepl(VARS (a,b1,b2,rs,es,es′,m1,m2,s))
−−−−−−−−−−−−−−−−−−−−−−−−−−→
msg(a, a, b2, m2)  stater(j, a, es′, s)  ak(a, m1)  ak(a, m2)
(2)
where l is the step label, i and j are integers and r is a protocol role (e.g., the
NSPK++ has two roles Alice and Bob, also said Initiator and Responder roles).
Rule 2 models the reception by a principal of a message and the principal’s
sending of the next message according to the protocol. More precisely, it states
that if principal a, who is playing role r at step i with internal state es in session
s of the protocol, has received message m1 supposedly from b1 (while the real
sender is rs), then she can honestly send message m2 to b2. In doing so, a updates
her internal state as es′ and her knowledge accordingly, that is the new state
registers the facts ak(a, m1) and ak(a, m2). Note that rule 2 may take slightly
diﬀerent forms depending on the protocol step it models. For example, if j = 1
and a sends the ﬁrst message of the protocol, the fact msg(rs, b1, a, m1) does
not appear in the left hand side of the rule, reﬂecting a’s freedom to initiate the
protocol at anytime. Similarly, for generating and sending a fresh term, c(N) is
included in the left hand side of the rule, while c(s(N)) and ak(a, N) appear in the
right hand side to express the incremented counter and the principal’s learning
the fresh term. A further variant is necessary when the step involves either a
membership test or an update of a set of elements. In this case, facts of the form
contains(db, m) must be properly deﬁned. Facts such as confidential(m, g)
or transferred(rs, b, a, c, x, s) added to the right hand side express respectively
conﬁdentiality for a group of principals, and a successful transfer of money.
To illustrate the speciﬁcation of concrete protocol rules we consider two steps
of the NSPK++ protocol. The transition in which B receives the ﬁrst message of
the protocol (supposedly) from A and replies with the second protocol message
is modelled by the following rule:
msg(RS, A,B,{⟨A, NA⟩}KB) statebob(1, B, [A, KA, KB, G], S)  c(NB)
step2(B,A,RS,KA,KB,NA,NB,G,S)
−−−−−−−−−−−−−−−−−→
msg(B, B, A,{⟨NA, NB⟩}KA)  statebob(3, B,[A, KA, KB, NA, NB, G], S)  ak(B,{⟨A, NA⟩}KB) 
ak(B, NB)  contains(G, A)  contains(G, B)  confidential(NB, G)  c(s(NB))
where G represents the group of principals that are allowed to share the freshly
generated nonce NB. This also illustrates our diﬀerent treatment of conﬁdentiality
with respect to DY’s. While DY reduced conﬁdentiality of a message to keeping
the message conﬁdential from the attacker, GA requires the original, subtler
and unsimpliﬁed, deﬁnition of conﬁdentiality: “conﬁdentiality is the protection of
information from disclosure to those not intended to receive it”[24]. For example,

Validating Security Protocols under the General Attacker
43
it regards the conﬁdentiality of a message as compromised if ever anyone beyond
its intended peers learns it.
To provide another example of a protocol rule, the completion step 4b in which
A receives and then executes a money transfer from B is modeled by two rules,
one for B’s sending and one for A’s reception. Here is the latter:
msg(RS, B, A, {tr(B, C, X)}⟨NA,NB⟩)  statealice(4, A, [B, KA, KB, NA, NB, G], S)
step4b rec(A,B,C,RS,KA,KB,NA,NB,X,G,S)
−−−−−−−−−−−−−−−−−−−−−−→
statealice(4, A, [B, KA, KB, NA, NB, G], S)  ak(A, {tr(B, C, X)}⟨NA,NB⟩)  ak(A, X) 
transferred(RS, B, A, C, X, S)
The rule states the fact transferred(RS, B, A, C, X, S) to record that RS, who is
not necessarily B, transferred X   from B’s account to C′s at A (which intuitively
is a bank) in session S. This is not to be confused with {tr(B, C, X)}⟨NA,NB⟩, which
is a message expressing the request of a transfer.
The malicious behaviour of each principal C acting as a DY attacker can be
speciﬁed by the following rules:
nt(C)  ak(C, M)  ak(C, A)  ak(C, B)
fake(C,A,B,M)
−−−−−−−−→
msg(C, A, B, M) LHS
nt(C)  msg(RS, A, B, M)
overhear(RS,A,B,C,M)
−−−−−−−−−−−−→ak(C, M)  LHS
nt(C)  msg(RS, A, B, M)
intercept(RS,A,B,C,M)
−−−−−−−−−−−−−→ak(C, M)  nt(C)
Although the model outlined so far is accurate, it is not the most appropriate to
perform automatic analysis eﬃciently. This is not a big issue for validating the
NSPK++protocol in particular, but it is in general. The main problem is the speci-
ﬁcationofthemaliciousbehaviourofprincipals.Itallowsfor theforging ofmessages
that will clearly not help to attack the protocol, as they do not correspond to the
forms that the protocol prescribes. In other words, forging messages that no one
will ever accept is of no use to any attacker. Eﬃciency of the analysis improves by
adopting a reﬁned model of malicious activity, for example by introducing a forged
message only if it conforms to one of the forms that belong to the protocol. More
narrow-scoped, though realistic, impersonate rules detailed elsewhere [25,
3.2.4]
can easily be recast in the GA threat model. For example, the following imperson-
ate rule models a principal C who, pretending to be A, sends the ﬁrst message of our
example protocol to B, who is exactly waiting for a message of that form:
nt(C)  statebob(2, B, [A, KA, KB, G], S)  ak(C, A)  ak(C, NA)  ak(C, KB)
impersonate2(C,A,B,KA,KB,NA,G,S)
−−−−−−−−−−−−−−−−−−−→
msg(C, A, B, {⟨A, NA⟩}KB)  ak(C, {⟨A, NA⟩}KB)  LHS
3.3
Formalising Protocol Properties under the General Attacker
One may expect that standard security properties such as conﬁdentiality and
authentication can be routinely speciﬁed and checked under the GA threat model
using a model checker. Yet, conﬁdentiality deserves particular consideration in
what the GA model diﬀers from the DY one.

44
W. Arsac et al.
In general, the conﬁdentiality of a message m w.r.t. a group of principals g
is guaranteed if and only if m is only known to principals in g. This property
is clearly violated anytime a principal a outside g learns, for any reason, m.
However, under DY, where all principals but the attacker meticulously follow
the protocol, the violation is by design not considered so unless a is the attacker.
There are many real scenarios—ranging from a betrayed person who publishes
his/her partner’s credit card details, to a scenario where a ﬁred employee dis-
closes sensitive information about its former employer—in which this violation
is signiﬁcant and therefore not negligible. In the GA model this conﬁdentiality
breach is not overlooked, as it can be checked by the following meta-predicate
formalising a violation of conﬁdentiality:
voc(a, m, g) = F(confidential(m, g) ∧ak(a, m) ∧¬ contains(g, a) )
(3)
The negation of the meta-predicate given in Deﬁnition 3 corresponds to G in
the model checking problem 1 stated in Section 3.1, and represents the conﬁden-
tiality property.
What is more interesting is that the GA threat model paves the way to in-
vestigate subtler protocol properties than conﬁdentiality and authentication. In
Section 2, we observed that retaliation is a common practice in the real world.
(Even anyone who is most peaceful may react under attack—whether this is for-
tunate or unfortunate lies outside our focus.) Depending on who reacts against
the attacker, retaliation can be named diﬀerently [16]: if it is the victim, then
there is direct retaliation; if it is someone else, then there is indirect retaliation.
Let attack(c, a, b) be a meta-predicate holding if and only if an attacker c has
successfully attacked a victim a (being a ̸= c and a ̸= b) with the (unaware)
support of b (if any). Of course, “has successfully attacked” denotes the vio-
lation of the speciﬁc property that the protocol under analysis is supposed to
achieve. Any violation of a protocol property should make the predicate hold,
and therefore the predicate should be deﬁned by cases. If we focus for simplicity
on the didactic attack to the NSPK++ protocol seen above (Figure 2), which is
an illegal money transfer, then the meta-predicate can be deﬁned as:
attack(c, a, b) = transferred(c, a, b, r, x, s) ∧c ̸= a
(4)
Clearly, this deﬁnition can be extended to check other kinds of attacks. Because
not all money transfers are illegal, the second conjunct in the formula is crucial:
the illegal transfers are only those that are not requested by the account holder.
Direct and indirect retaliation can be respectively modelled as the following
meta-predicates:
direct retaliation(a, c) = F( attack(c, a, b1) ∧X F attack(a, c, b2) )
(5)
indirect retaliation(a, c, b) = F( attack(c, a, b) ∧X F attack(b, c, a) )
(6)
It can be seen that the formula deﬁned by Deﬁnition 5 is valid on those paths
where an attacker hits a victim who hits the attacker back. Each attack can
be carried out with the help of potentially diﬀerent supporters b1 and b2. By

Validating Security Protocols under the General Attacker
45
contrast, Deﬁnition 6 of indirect retaliation shows that who hits the attacker
back is not the victim but the supporter, who perhaps realises what he has just
done and decides to rebel against the attacker.
The deﬁnition of anticipation attack is deferred to the next section because it
is best demonstrated upon the actual experiments.
4
Validating the NSPK++ Protocol under the General
Attacker
The previous Section outlined the model checking problem in general, and de-
scribed how to formalise the GA threat model for the validation of security
protocols. For the sake of demonstration, it presented the formalisation of a
step of the NSPK++ protocol. It concluded with the speciﬁcation of protocol
properties under the GA threat model.
Having digested the innovative aspects, deriving the full formalisation of the
NSPK++ protocol under the General Attacker, which is omitted here, became
an exercise. That formalisation was fed to SATMC, a state-of-the-art SAT-based
model checker for security protocols, to carry out the ﬁrst protocol validation
experiments under GA. The details of SATMC appear elsewhere [3,23]. Its core is
a procedure that automatically generates a propositional formula. The satisfying
assignments of this formula, if any exist, correspond to counterexamples (i.e.
execution traces of M that falsify G) of length bounded by some integer k,
which can be iteratively deepened. Finding violations (of length k) on protocol
properties boils down to solving propositional satisﬁability problems. SATMC
accomplishes this task by invoking state-of-the-art SAT solvers, which can handle
satisﬁability problems with hundreds of thousands of variables and clauses.
In running SATMC over the formalisation of the NSPK++ protocol, we con-
sidered the classical scenario in which a wants to talk with b in one session
and with c in another session. Because the formalisation accounted for the new
threat model, we were pleased to observe that it passed simple sanity checks:
SATMC outputs the known conﬁdentiality attack whereby c learns nb, and the
known authentication attack whereby c impersonates a with b. As these are well
known, they are omitted here. More importantly, the tool also reported what are
our major ﬁndings: b’s conﬁdentiality attack upon na, and interesting retaliation
attacks, which are detailed in the following. We argue that this is the ﬁrst mech-
anised treatment of these subtle properties, laying the ground for much more
computer-assisted analysis of security protocols.
0: [step_1(a,c,ka,kc,na,nb,set_ac,1)]
1: [overhear(c,a,a,c,{na,a}kc)]
2: [decrypt(c,inv(kc),{na,a})]
3: [impersonate_2(c,a,b,ka,kb,na,set_ab,2)]
4: [step_2(b,a,c,ka,kb,na,nb,set_ab,2)]
5: [decrypt(b,inv(kb),{na,a})]
Fig. 4. Trace of NSPK++ featuring b’s conﬁdentiality attack

46
W. Arsac et al.
6: [impersonate_3(b,c,a,ka,kc,na,nb,set_ac,1)]
7: [step_3(a,c,b,ka,kc,na,nb,set_ac,1)]
8: [overhear(c,a,a,c,{nb}kc)]
9: [decrypt(c,inv(kc),{nb})]
10: [impersonate_3_rec(c,a,b,ka,kb,nb,set_ab)]
11: [step_3_rec(b,a,c,ka,kb,na,nb,set_ab,2)]
12: [impersonate_4a_rec(c,a,b,c,ka,kb,na,nb,set_ab,2)]
13: [step_4a_rec(b,a,c,c,ka,kb,na,nb,1K,set_ab,2)]
14: [impersonate_4b_rec(b,c,a,b,ka,kc,na,nb,set_ac,1)]
15: [step_4b_rec(a,c,b,b,ka,kc,na,nb,2K,set_ac,1)]
Fig. 5. Trace of NSPK++ (continuation) featuring b’s indirect retaliation
Figure 4 reports the protocol trace that the tool outputs when checking a
violation of conﬁdentiality by the formula voc(A, M, G). Precisely, the trace is
obtained by mildly polishing the partial-order plan returned by SATMC. Each
trace element reports the label of the rule that ﬁred, and hence the trace can be
interpreted as the history of events leading to the conﬁdentiality attack.
A full description of the trace requires a glimpse at the protocol formalisa-
tion —each rule label in a trace element must be matched to the actual rule—
but we shall see that the trace can be automatically converted into a more
user-friendly version. Element 0 means that principal a initiates the protocol
with principal c. Elements 1, 2 and 3 show c’s illegal activities respectively
of getting the message, decrypting it and forging a well-formed one for prin-
cipal b. Although c does not need in practice to overhear a message meant
for him, element 1 is due to our monolithic formalisation of the acts of receiv-
ing a message and of sending out its protocol-prescribed reply. Therefore, for
a principal to abuse a message, the principal must ﬁrst overhear it. Element
3 reminds that c’s forging of the message for b counts as an attempt to im-
personate a. The last two elements signal b’s legal participation in the proto-
col by receiving the message meant for him, and his subsequent deciphering of
the nonce. SATMC now returns because voc(b, na, set ac) holds (where the set
set ac = {a, c}).
To illustrate the detection of a retaliation attack, SATMC can be launched
on a signiﬁcant property such as indirect retaliation(A, B, C). It returns a trace
that continues as in Figure 5 the one seen in Figure 4. The impersonate rules
show that both b and c are acting illegally in this trace, a development that has
never been observed by previous analyses under DY. Elements 6 and 7 show that
b is impersonating c with a, who naively replies to c. The next three elements
conﬁrm c’s attempt at fooling b, who legally replies as element 11 indicates. Now
c can ﬁnalise his attack as in elements 12 and 13. The latter element indicates the
ﬁring of rule 4a, which makes attack(c, a, b) hold. The last two elements witness
b’s retaliation attack by making attack(b, c, a) hold. Therefore, by Deﬁnition 6,
the property indirect retaliation(a, c, b) holds, indicating that the tool reports
the retaliation attack described above (Figure 3).

Validating Security Protocols under the General Attacker
47
Legend
Fig. 6. Graphics of b’s indirect retaliation in NSPK++
A graphical illustration of this retaliation attack is in Figure 6, and can be
easily built. First, we run a procedure to transform the output of the tool into a
more readable and intuitive version. Then, we coherently associate the signiﬁcant
phrases of this version to graphical elements, and hence build the image. The
behaviours and interactions of the principals can be observed by looking at the
ﬁgure from top to bottom. The overhear and impersonate icons emphasise the
signiﬁcant number of illegal steps taken by both principals c and b).
We run the tool repeatedly on the protocol model, each time adjusting the
property to check. In particular, we tried an alternative deﬁnition of indirect
retaliation where the X operator was left out (see Deﬁnition 6). Somewhat to
our surprise, the tool returned a trace leading to a state where both attacks
held, as if retaliation did not need be triggered by another attack. This called
for more attention at the trace.
In consequence, we observed from element 3 in Figure 4 that c initialises
his attack very early—precisely, with his impersonation of a based upon the
repetition to b of a’s nonce na. This means that b learns na when c has not
yet learned nb and hence cannot attack yet. We thus deﬁned the self-explaining
fact nonce leak(c, a, b). In the GA threat model, it is plausible that b exploits
his knowledge before c does. This can be interpreted as a scenario in which a
potential victim realises that he is going to be attacked, and therefore reacts
successfully before being actually attacked. We name such a successful reaction
anticipation attack and deﬁne it as a meta-predicate below. For the sake of

48
W. Arsac et al.
0: [step_1(a,c,ka,kc,na,nb,set_ac,1)]
1: [overhear(c,a,a,c,{na,a}kc)]
2: [decrypt(c,inv(kc),{na,a})]
3: [impersonate_2(c,a,b,ka,kb,na,set_ab,2)]
4: [overhear(b,c,a,b,{na,a}kb)]
5: [decrypt(b,inv(kb),{na,a})]
6: [i_got_you(c,a,b,na,set_ac)]
7: [impersonate_3(b,c,a,ka,kc,na,nb,set_ac,1)]
8: [step_3(a,c,b,ka,kc,na,nb,set_ac,1)]
9: [impersonate_4b_rec(b,c,a,b,ka,kc,na,nb,set_ac,1)]
10: [step_4b_rec(a,c,b,b,ka,kc,na,nb,2K,set_ac,1)]
Fig. 7. Trace of NSPK++ featuring b’s anticipation attack
eﬃciency, we decided to add a rule that introduces the fact nonce leak(C, A, B)
following c’s attack initialisation:
confidential(NA, G)  ak(B, NA)  ¬ contains(G, B) 
ak(B, KB−1)  msg(C, A, B, {⟨A, NA⟩}KB)
i got you(C,A,B,NA,G)
−−−−−−−−−−−−→
nonce leak(C, A, B)
A meta-predicate attack init(c, a, b) must be introduced to formalise some c’s
act of initialising an attack, which is yet to be carried out, while interleaving
sessions with some a and b. In the same fashion of Deﬁnition 4 of attack(c, a, b),
we provide a deﬁnition that is appropriate for the attack under analysis, and
corresponds to the nonce leak:
attack init(c, a, b) = nonce leak(c, a, b)
The deﬁnition of anticipation attack can be given now. It insists that an attack
initiated by someone, such as c, is followed by an actual attack carried out by
someone else, such as b:
anticipation attack(c, a, b) = F( attack init(c, a, b) ∧
X F attack(b, c, a) )
(7)
SATMC validated our intuition. When run on anticipation attack(C, A, B) as a
goal, the tool produced the partial order plan reported in Figure 7. Element 6 in
the trace indicates that c leaked a nonce created by a by sending it to b. So, the
meta-predicate attack init(c, a, b) holds. Moreover, the last two elements witness
b’s anticipation attack by making attack(b, c, a) hold. Therefore, by Deﬁnition
7, we have that anticipation attack(c, a, b) is true, which signiﬁes that the tool
reports the anticipation attack described above (Figure 8).
Our various experiments produced another interesting ﬁnding—SATMC re-
ported a trace describing the following scenario. In a session, a discloses to c her
nonce generated for b. In another session, a discloses to b another nonce of hers
generated for c. In consequence, both b and c become capable of attacking each

Validating Security Protocols under the General Attacker
49
Ă
Ă
Đ
ď
с/ŵƉĞƌƐŽŶĂƚĞсKǀĞƌŚĞĂƌ
сDŽŶĞǇdƌĂŶƐĨĞƌ
Legend
Fig. 8. Graphics of b’s anticipation attack in NSPK++
other with a’s support if needed. Moreover, b and c may be initially unaware of
each other’s capability of attack. This reﬂects the real-world situation in which
someone creates strife in a couple that starts ﬁghting.
5
Conclusions
The General Attacker threat model seems most appropriate to the present so-
cial/technological setting. Reasoning that was impossible under DY can now be
carried out, highlighting protocol niceties that are routinely overseen. Our work
required care in formalising the protocols under the new threat model, but no
changes to the state-of-the-art model checker SATMC.
Retaliation. teaches us that we can perhaps live with ﬂawed protocols. We are
used to go back to design when a protocol is found ﬂawed, even if already
deployed. However, an attack that can be retaliated may in practice con-
vince an attacker to refrain from attacking in the ﬁrst place. If the “cost” of
attacking overdoes its “beneﬁts” then the attacker will not be carried out.
Retaliation makes that precondition hold.
Anticipation. teaches us to ponder the entire sequence of events underlying an
attack. An attack typically is an interleaving of legal and illegal steps rather
than a single illegal action. Therefore, we may face a scenario, unreported
so far, where a principal mounts an attack by successfully exploiting for his
own sake the illegal activity initiated but not yet ﬁnalised by someone else.
This is routine for the present hackers’ community.

50
W. Arsac et al.
So used as we are, as protocol analysers, to reasoning in terms of a DY at-
tacker, we might think of our novel investigations as unrealistic. However, scenar-
ios of retaliation and anticipation often appear in our era of computer networks,
notably in the area of intrusion management. With the increasing awareness
of computer security issues, successful attacks rarely are instantaneous moves
without consequences. Rather, they often involve a combination of smaller il-
legal achievements or the breaking of subsequent protection levels to succeed.
These are typically monitored by either administrators or other attackers, and
hence attacks can be anticipated or retaliated. The ﬁeld of protocol analysis
seems bound to develop much further.
Acknowledgements. This work was supported by the FP7-ICT-2007-1 Project
no. 216471, “AVANTSSAR: Automated Validation of Trust and Security of
Service-oriented Architectures” (www.avantssar.eu).
References
1. Bella, G.: The Rational Attacker (2008) (invited talk at SAP Research France,
Sophia Antipolis),
http://www.dmi.unict.it/~giamp/Seminars/rationalattackerSAP08.pdf
2. Armando, A., Basin, D.A., Boichut, Y., Chevalier, Y., Compagna, L., Cu´ellar, J.,
Drielsma, P.H., H´eam, P.C., Kouchnarenko, O., Mantovani, J., M¨odersheim, S., von
Oheimb, D., Rusinowitch, M., Santiago, J., Turuani, M., Vigan`o, L., Vigneron, L.:
The AVISPA Tool for the Automated Validation of Internet Security Protocols and
Applications. In: Etessami, K., Rajamani, S.K. (eds.) CAV 2005. LNCS, vol. 3576,
pp. 281–285. Springer, Heidelberg (2005)
3. Armando, A., Compagna, L.: SAT-based Model-Checking for Security Protocols
Analysis. International Journal of Information Security 7(1), 3–32 (2008)
4. Abadi, M., Gordon, A.: A calculus for cryptographic protocols: the spi calculus.
Information and Computation 148(1), 1–70 (1999)
5. Ryan, P.Y.A., Schneider, S., Goldsmith, M., Lowe, G., Roscoe, A.W.: Modelling
and Analysis of Security Protocols. In: AW (2001)
6. F´abrega, F.J.T., Herzog, J.C., Guttman, J.D.: Strand spaces: Proving security
protocols correct. Journal of Computer Security 7, 191–230 (1999)
7. Caleiro, C., Vigan`o, L., Basin, D.: Relating strand spaces and distributed temporal
logic for security protocol analysis. Logic Journal of the IGPL 13(6), 637–663 (2005)
8. Bella, G.: Formal Correctness of Security Protocols. In: Information Security and
Cryptography. Springer, Heidelberg (2007)
9. Paulson, L.C.: The inductive approach to verifying cryptographic protocols. Jour-
nal of Computer Security 6, 85–128 (1998)
10. Blanchet, B.: Automatic veriﬁcation of cryptographic protocols: a logic program-
ming approach. In: Proceedings of the 5th International ACM SIGPLAN Confer-
ence on Principles and Practice of Declarative Programming, Uppsala, Sweden,
August 27-29, pp. 1–3 (2003)
11. Lowe, G.: Breaking and Fixing the Needham-Shroeder Public-Key Protocol Using
FDR. In: Margaria, T., Steﬀen, B. (eds.) TACAS 1996. LNCS, vol. 1055, pp. 147–
166. Springer, Heidelberg (1996)

Validating Security Protocols under the General Attacker
51
12. Bellare, M., Rogaway, P.: Provably secure session key distribution– the three party
case. In: Proceedings 27th Annual Symposium on the Theory of Computing, pp.
57–66. ACM, New York (1995)
13. Abadi, M., Rogaway, P.: Reconciling two views of cryptography (the computa-
tional soundness of formal encryption). In: Watanabe, O., Hagiya, M., Ito, T., van
Leeuwen, J., Mosses, P.D. (eds.) TCS 2000. LNCS, vol. 1872, pp. 3–22. Springer,
Heidelberg (2000)
14. Gollmann, D.: On the veriﬁcation of cryptographic protocols — a tale of two com-
mittees. In: Proc. of the Workshop on Secure Architectures and Information Flow.
ENTCS, vol. 32. Elsevier Science, Amsterdam (2000)
15. Backes, M., Pﬁtzmann, B.: Relating symbolic and cryptographic secrecy. In: IEEE
Symposium on Security and Privacy (2005)
16. Bella, G., Bistarelli, S., Massacci, F.: Retaliation: Can we live with ﬂaws? In:
Essaidi, M., Thomas, J. (eds.) Proc. of the Nato Advanced Research Workshop
on Information Security Assurance and Security. Nato through Science, vol. 6, pp.
3–14. IOS Press, Amsterdam (2006),
http://www.iospress.nl/loadtop/load.php?isbn=9781586036782
17. Dolev, D., Yao, A.: On the Security of Public-Key Protocols. IEEE Transactions
on Information Theory 2(29) (1983)
18. Bella, G., Bistarelli, S.: Conﬁdentiality levels and deliberate/indeliberate protocol
attacks. In: Christianson, B., Crispo, B., Harbison, W.S., Roe, M. (eds.) Security
Protocols 2002. LNCS, vol. 2845, pp. 104–119. Springer, Heidelberg (2004)
19. Aiyer, A.S., Alvisi, L., Clement, A., Dahlin, M., Martin, J.P., Porth, C.: Bar fault
tolerance for cooperative services. ACM SIGOPS Operating Systems Review 39(5),
45–58 (2005)
20. Butty´an, L., Hubaux, J.P., ˇCapkun, S.: A formal model of rational exchange and
its application to the analysis of syverson’s protocol. Journal of Computer Secu-
rity 12(3-4), 551–587 (2004)
21. Bella, G.: What is Correctness of Security Protocols? Springer Journal of Universal
Computer Science 14(12), 2083–2107 (2008)
22. Armando, A., Compagna, L.: SAT-based Model-Checking for Security Protocols
Analysis. International Journal of Information Security 6(1), 3–32 (2007)
23. Armando, A., Carbone, R., Compagna, L.: LTL Model Checking for Security Proto-
cols. In: Proceedings of the 20th IEEE Computer Security Foundations Symposium
(CSF20), Venice, Italy, July 6-8. LNCS. Springer, Heidelberg (2007)
24. Neuman, B.C., Ts’o, T.: Kerberos: An authentication service for computer net-
works, from IEEE communications magazine. In: Stallings, W. (ed.) Practical
Cryptography for Data Internetworks, September 1994. IEEE Press, Los Alamitos
(1996)
25. Compagna, L.: SAT-based Model-Checking of Security Protocols. Phd, Universit`a
degli Studi di Genova, Italy, and University of Edinburgh, Scotland (2005)

Usage Automata
Massimo Bartoletti
Dipartimento di Matematica e Informatica, Universit`a degli Studi di Cagliari
Abstract. Usage automata are an extension of ﬁnite stata automata,
with some additional features (e.g. parameters and guards) that improve
their expressivity. Usage automata are expressive enough to model secu-
rity requirements of real-world applications; at the same time, they are
simple enough to be statically amenable, e.g. they can be model-checked
against abstractions of program usages. We study here some founda-
tional aspects of usage automata. In particular, we discuss about their
expressive power, and about their eﬀective use in run-time mechanisms
for enforcing usage policies.
1
Introduction
Security is a major concern in the design and implementation of modern pro-
gramming languages. For instance, both Java and C# oﬀer a whole range of
security features, from the “safety pillars” of bytecode veriﬁcation and secure
class loading, to more high-level defence mechanisms, like e.g. stack inspection
and cryptography APIs [10].
However, mainstream programming languages typically do not oﬀer any facil-
ities to specify and enforce custom policies on the usage of resources. Therefore,
it is common practice to renounce to separating duties between functionality
and security, and to implement the needed enforcement mechanism with local
checks explicitly inserted into the code by programmers. Since forgetting even a
single check might compromise the security of the whole application, program-
mers have to inspect their code very carefully. This may be cumbersome even
for small programs, and it may also lead to unnecessary checking.
History-based security has been repeatedly proposed as a replacement for
stack inspection [1,8,17]. Clearly, the ability of checking the whole execution
history, instead of the call stack only, places history-based mechanisms a step
forward stack inspection, from the expressivity viewpoint. However, since many
possible history-based models can be devised, it is crucial to choose one which
wisely conciliates the expressive power with the theoretical properties enjoyed.
It is also important that the enforcement mechanism can be implemented trans-
parently to programmers, and with an acceptable run-time overhead.
In this paper we study usage automata as a formalism for deﬁning and en-
forcing usage policies. Usage automata extend ﬁnite state automata, by allowing
edges to carry variables and guards. Variables represent universally quantiﬁed
resources. Since variables may range over an inﬁnite set of resources, usage au-
tomata are a suitable formalism for expressing policies with parameters ranging
P. Degano and L. Vigan`o (Eds.): ARSPA-WITS 2009, LNCS 5511, pp. 52–69, 2009.
c
⃝Springer-Verlag Berlin Heidelberg 2009

Usage Automata
53
over inﬁnite domains (e.g. the formal parameters of a procedure). For instance, a
usage automaton might state that “for all ﬁles x, you can read or write x only if
you have opened x, and not closed it by the while”. Guards represent conditions
among variables (and resources). For instance, “a ﬁle y cannot be read if some
other ﬁle x ̸= y has been read in the past”.
Usage automata have been proved expressive enough to model security re-
quirements of real-world applications. For instance, we used them to specify the
typical set of policies of a bulletin board system [13]. At the same time, usage
automata are simple enough to be statically amenable, e.g. they can be model-
checked against abstractions (namely, history expressions) of program usages [4].
In this paper we investigate some foundational issues about usage automata.
In Section 2 we formally deﬁne their semantics, and we show that usage au-
tomata recognize safety properties enforceable through ﬁnite state automata.
In Section 3 we turn our attention to expressivity issues. We show that remov-
ing guards diminishes the expressive power of usage automata, while removing
polyadic events (i.e. only allowing for monadic ones) keeps it unaltered. We then
prove that the expressive power of usage automata increases as the number of
variables therein increases. To do that, we introduce a proof technique (a sort
of “deﬂating lemma”), stating that a usage automaton with k variables cannot
distinguish among more than k resources.
In Section 4 we discuss the feasibility of run-time monitoring policies deﬁned
by usage automata. This question naturally arises from the deﬁnition of the
semantics of usage automata, which requires an unbounded amount of space
to decide whether a program trace respects a policy or not. We deﬁne an en-
forcement mechanism based on usage automata, which only requires a bounded
amount of space. Some possible optimizations of the enforcement mechanism, as
well as some implementation issues, are then highlighted.
We conclude with Section 5, by discussing some related work.
2
Usage Automata
Assume a (possibly inﬁnite) set of resources r, r′, . . . ∈Res, which can be ac-
cessed through a given ﬁnite set of actions α, α′, . . . ∈Act. Each action α has
a given arity |α|, that is the number of resources it operates upon. An event
α(r1, . . . , rk) ∈Ev models the action α (with arity k) being ﬁred on the tar-
get resources r1, . . . , rk. Traces, typically denoted by η, η′, . . . ∈Ev∗, are ﬁnite
sequences of events. A usage policy is a set of traces.
In this paper we study usage automata as a formalism for deﬁning and en-
forcing usage policies. Usage automata can be seen as an extension of ﬁnite
state automata (FSA), where the labels on the edges may contain variables
and guards. Variables x, y, . . . ∈Var represent universally quantiﬁed resources.
Guards g, g′, . . . ∈G represent conditions among variables (and resources).
Before introducing usage automata, we formally deﬁne the syntax and seman-
tics of guards.

54
M. Bartoletti
Deﬁnition 1. Guards
Let ρ, ρ′, . . . ∈Res ∪Var. We inductively deﬁne the set G of guards as follows:
G ::= true | ρ = ρ′ | ¬G | G ∧G
For all guards g ∈G, we deﬁne var(g) as the set of variables occurring in g.
Let g be a guard, and let σ : var(g) →Res. Then, we write σ |= g when:
– g = true, or
– g = (ρ = ρ′) and ρσ = ρ′σ, or
– g = ¬g′ and it is not the case that σ |= g′, or
– g = g′ ∧g′′, and σ |= g′ and σ |= g′′.
We feel free to write ρ ̸= ρ′ for ¬(ρ = ρ′), and g ∨g′ for ¬(¬g ∧¬g′).
We now deﬁne the syntax of usage automata. They are much alike ﬁnite state
automata, but with a diﬀerent input alphabet. Instead of plain symbols, the
edges of usage automata have the form α(ρ) : g, where ρ ∈(Res ∪Var)|α| and
g ∈G. The formal deﬁnition is in Def. 2.
Deﬁnition 2. Usage automata
A usage automaton ϕ is a 5-tuple ⟨S, Q, q0, F, E⟩, where:
– S ⊆Act × (Res ∪Var)∗is the input alphabet,
– Q is a ﬁnite set of states,
– q0 ∈Q \ F is the start state,
– F ⊂Q is the set of ﬁnal “oﬀending” states,
– E ⊆Q × S × G × Q is a ﬁnite set of edges, written q
α(ρ): g
−−−−⊸q′
Example 1. Let DIFF(k) be the usage policy stating that the action α (with
arity 1) can be ﬁred at most on k diﬀerent resources, i.e.:
DIFF(k) = Ev∗\ { η0α(r0)η1 · · · ηkα(rk)ηk+1 | ∀i ̸= j ∈0..k : ri ̸= rj }
The usage automaton ϕDIFF(k) in Fig. 1 denotes the usage policy DIFF(k).
⊓⊔
Each usage automaton ϕ denotes a usage policy P, i.e. a set of traces η ∈P
that obey the policy. To deﬁne the semantics of ϕ, we consider all the possible
instantiations of its variables var(ϕ) to actual resources. This operation yields a
(possibly inﬁnite) set of automata with ﬁnite states and possibly inﬁnite edges.
q0
q1
q2
α(x0)
α(x1) : x1 ̸= x0
α(xk) : xk ̸= xk−1 ∧· · · ∧xk ̸= x0
qk+1
Fig. 1. The usage automaton ϕDIFF(k) models the policy DIFF(k)

Usage Automata
55
The usage policy denoted by ϕ is then the union of the (complemented) lan-
guages recognized by the automata resulting from the instantiation of ϕ (the
complement is needed because the ﬁnal states of a usage automaton ϕ denote a
violation of ϕ). We formally deﬁne the set of traces η that respect ϕ in Def. 3.
Deﬁnition 3. Policy compliance
Let ϕ = ⟨S, Q, q0, F, E⟩be a usage automaton. For all R ⊆Res and for all
σ : var(ϕ) →R we deﬁne the automaton Aϕ(σ, R) = ⟨Σ, Q, q0, F, δ⟩as follows:
Σ = { α(r) | α ∈Act and r ∈R|α| }
δ = complete({ q
α(ρσ)
−−−−→q′ | q
α(ρ): g
−−−−⊸q′ ∈E and σ |= g }, R)
where complete(X, R) is deﬁned as follows:
complete(X, R) = X ∪{ q
α(r)
−−−→q| r ∈R|α| and ∄q′ ∈Q : q
α(r)
−−−→q′ ∈X }
We say that η respects Aϕ(σ, R) when η is not in the language of Aϕ(σ, R).
In such a case, we write η ◁Aϕ(σ, R).
We say that η respects ϕ when η◁Aϕ(σ, Res) for all σ : var(ϕ) →Res. In such
a case, we write η |= ϕ; otherwise, we say η violates ϕ, and we write η ̸|= ϕ.
Example 2. Consider the implementation of a list datatype which allows for
iterating over the elements of the list. In this scenario, a relevant usage policy is
that preventing list modiﬁcations when the list is being iterated (this is actually
implemented by Java collections through checks inserted in the source code).
The usage automaton ϕList recognizing this policy is depicted in Fig. 2, left. The
relevant events are start(l), for starting the iterator of the list l, next(l), for
retrieving the next element of l, and modify(l), that models adding/removing
elements from l. The automaton ϕList has three states: q0, q1, and fail. In the
state q0 both modiﬁcations of the list and iterations are possible. However, a
modiﬁcation (i.e. an add or a remove) done in q0 will lead to q1, where it is
no longer possible to iterate on l, until a start(l) resets the situation. Let now:
η = start(l0) next(l0) start(l1) next(l1) modify(l1) modify(l0) next(l0). The trace
η violates the policy, because the last event attempts to iterate on the list l0,
after l0 has been modiﬁed. To check that ϕList correctly models this behaviour,
consider the instantiation A0 = AϕList({x →l0}, {l0, l1}) depicted in Fig. 2, right.
When supplied with the input η, the FSA A0 reaches the oﬀending state fail,
therefore by Def. 3 it follows that η ̸|= ϕList.
⊓⊔
Our ﬁrst result about usage automata is Lemma 1, the intuition behind which
is more evident if the statement is read contrapositively. Let res(η) be the set
of resources occurring in η. If η is recognized as oﬀending by some instantiation
Aϕ(σ, R′) with R′ ⊃res(η), then η will be oﬀending for all the instantiations
Aϕ(σ, R) with R ⊇res(η). In other words, the only relevant fact about the sets R
used in instantiations is that R includes res(η).

56
M. Bartoletti
q0
q1
modify(x)
fail
next(x)
start(x)
next(l0)
start(l0)
start(l1)
next(l1)
modify(l1)
start(l0)
modify(l0)
start(l1)
next(l1)
modify(l1)
start(l0)
next(l0)
modify(l0)
start(l1)
next(l1)
modify(l1)
q1
q0
next(l0)
modify(l0)
fail
start(l0)
Fig. 2. The usage automaton ϕList (left) and the FSA AϕList({x →l0}, {l0, l1}) (right)
Lemma 1. For all usage automata ϕ, for all σ : var(ϕ) →Res, and traces η:
∃R ⊇res(η) : η ◁Aϕ(σ, R) =⇒∀R′ ⊇res(η) : η ◁Aϕ(σ, R′)
Proof. We prove the contrapositive. Assume there exists R′ ⊇res(η) such that
η ̸◁Aϕ(σ, R′), i.e. there is a run q0
η−→qk of Aϕ(σ, R′) where qk ∈F. For all
R ⊇res(η), we reconstruct an oﬀending run of Aϕ(σ, R) on η. Let δR and
δR′ be the transition relations of Aϕ(σ, R) and Aϕ(σ, R′), respectively, and let
δ∗
R and δ∗
R′ be their (labelled) reﬂexive and transitive closures. We prove by
induction on the length of η that δ∗
R′({q0}, η) ⊆δ∗
R({q0}, η). The base case
η = ε is trivial. For the inductive case, note ﬁrst that the transitions in the
set X = { q
α(ρσ)
−−−−→q′ | q
α(ρ): g
−−−−⊸q′ ∈E and σ |= g } do not depend on R, R′:
indeed, R and R′ only aﬀect the self-loops added by complete. Let η = η′α(r).
By the induction hypothesis, δ∗
R′({q0}, η′) ⊆δ∗
R({q0}, η′). Since r ∈res(η) and
both R ⊇res(η) and R′ ⊇res(η), it follows that q
α(r)
−−−→q′ ∈complete(X, R) if
and only if q
α(r)
−−−→q′ ∈complete(X, R′), which implies the thesis.
⊓⊔
Note that usage automata can be non-deterministic, in the sense that for all
states q and input symbols α(ρ), the set of states:
{ q′ | q
α(ρ): g
−−−−⊸q′ }
is not required to be a singleton. Given a trace η, we want that all the paths
of the instances Aϕ(σ, R) comply with ϕ, i.e. they lead to a non-ﬁnal state.
Recall that we used ﬁnal states to represent violations to the policy. An alter-
native approach would be the “default-deny” one, that allows for specifying the
permitted usage patterns, instead of the denied ones. We could deal with this
approach be regarding the ﬁnal states as accepting. Both the default-deny and
the default-allow approaches have known advantages and drawbacks. We think
this form of diabolic non-determinism is particularly convenient when designing
usage automata, since one can focus on the sequences of events that lead to vi-
olations, while neglecting those that do not aﬀect the compliance to the policy.
All the needed self-loops will be added by complete(X, R) in Def. 3.

Usage Automata
57
q1
q0
q1
write(z3)
read(z4)
fail
write(z2)
read(y)
write(z1)
read(x)
read(y)
q0
read(x) : x ̸= y
read(y)
read(x) : x ̸= y
fail
Fig. 3. A usage automaton (left), and a wrong attempt to expose its self-loops (right)
Example 3. Note that, in general, it is not possible to make all the self-loops
explicit, so that complete(X, R) adds no further self loops. Consider for instance
the policy “a ﬁle x cannot be read if some other ﬁle y ̸= x has been read
in the past”. This is modelled by the usage automaton in Fig. 3, left, which
recognizes e.g. the trace read(f0)write(f0)read(f1) as oﬀending, and the trace
read(f0)write(f0)read(f0) as permitted. The usage automaton depicted on the
left has implicit self-loops: in order to correctly classify the above traces, it relies
on complete(X, R) in Def. 3, which adds the needed self-loops (e.g. those labelled
write(r) for all states and resources r). Any attempt to make all the self-loops
explicit will fail, because it would require an unbounded number of variables
in the edges whenever R is an inﬁnite set. Consider e.g. the usage automaton
in Fig. 3, right. For all the choices of σ, complete(X, R) will add e.g. all the
self-loops of q0 labelled write(r), for all r ̸= σ(z1).
⊓⊔
According to Def. 3, to check η |= ϕ we should instantiate a possibly inﬁnite
number of automata with ﬁnite states, one for each substitution σ : var(ϕ) →Res
(recall that Res can be an inﬁnite set). Interestingly enough, the compliance of
a trace with a usage automaton is a decidable property: the next lemma shows
that checking η |= ϕ can be decided through a ﬁnite set of ﬁnite state automata.
Lemma 2. Let η be a trace, let ϕ be a usage automaton, let res(ϕ) be the set
of resources mentioned in the edges of ϕ (both in events and in guards), let
k = |var(ϕ)|, and let #1, . . . , #k be arbitrary resources in Res\(res(η)∪res(ϕ)),
with #i ̸= #j for all i ̸= j. Then, η |= ϕ if and only if:
∀σ : var(ϕ) →res(η) ∪res(ϕ) ∪{#1, . . . , #k}
:
η ◁Aϕ(σ, res(η))
Proof. For the “only if” part, assume that η |= ϕ, i.e. that η ◁Aϕ(σ′, Res), for
all σ′ : var(ϕ) →Res. Since res(η) ∪res(ϕ) ∪{#1, . . . , #k} ⊆Res, then it is also
the case that η ◁Aϕ(σ, Res). By Lemma 1, it follows that η ◁Aϕ(σ, res(η)).
For the “if” part, we prove the contrapositive. Assume that η ̸|= ϕ, i.e.
η ̸◁Aϕ(σ′, Res) for some σ′ : var(ϕ) →Res. We prove that there exists some
σ : var(ϕ) →res(η) ∪res(ϕ) ∪{#1, . . . , #k} such that η ̸◁Aϕ(σ, res(η)). Let
var(ϕ) = {x1, . . . , xk}, and let σ′(var(ϕ)) \ (res(η) ∪res(ϕ)) = {r1, . . . , rk′},
where k′ ≤k and ri ̸= rj for all i ̸= j. Then, deﬁne σ as follows:
σ(xj) =

#h
if σ′(xj) = rh
σ′(xj)
otherwise
(1)

58
M. Bartoletti
Let η = α1(r1) · · · αn(rn), and consider an oﬀending run q0
α1(r1)
−−−−→· · ·
αn(rn)
−−−−→qn
of Aϕ(σ′, Res) on η. We now construct an oﬀending run of Aϕ(σ, res(η)) on η. Let
δAϕ(σ′) = complete(X′, Res) and δAϕ(σ) = complete(X, res(η)) be the transition
sets of these automata, as given by Def. 3. We will show that, for all i ∈1..n:
δAϕ(σ′)(qi, αi(ri)) ⊆δAϕ(σ)(qi, αi(ri))
(2)
Let q′ ∈δAϕ(σ′)(qi, αi(ri)). Consider ﬁrst the case that q′ has not been added as
a self-loop, i.e. q′ ∈X′(qi, αi(ri)). Then, there exist ρ and g such that:
qi
αi(ρ): g
−−−−−⊸q′ ∈E
ρσ′ = ri
σ′ |= g
Since each component of ri is in res(η), then by (1) it follows that ρσ = ρσ′ = ri.
By induction on the structure of g, we prove that:
σ |= g
if and only if
σ′ |= g
(3)
The base case g = true is trivial. The other base case is when g has the form
ρ0 = ρ1. One of the following four subcases hold.
– ρ0, ρ1 ∈Res. The thesis follows from ρ0σ = ρ0 = ρ0σ′ and ρ1σ = ρ1 = ρ1σ′.
– ρ0 ∈Var, ρ1 ∈Res. Then, σ(ρ1) = ρ1 = σ′(ρ1). Let ρ0 = xa ∈var(ϕ).
By (1), we have that:
σ(xa) =

#ha
if σ′(xa) = rha
(a1)
σ′(xa)
otherwise
(a2)
If σ′ |= ρ0 = ρ1, then σ′(xa) = ρ1. We are in the case (a2), because ρ1 ∈
res(ϕ) ̸∋#ha. Therefore, σ(xa) = σ′(xa) = ρ1, which proves σ |= ρ0 = ρ1.
If σ |= ρ0 = ρ1, then σ(xa) = ρ1. Again, we are in the case (a2), because
ρ1 ̸= #ha. Therefore, σ′(xa) = σ(xa) = ρ1, which proves σ′ |= ρ0 = ρ1.
– ρ0 ∈Res, ρ1 ∈Var. Similar to the previous case.
– ρ0, ρ1 ∈Var. Assume ρ0 = xa and ρ1 = xb, for some a, b ∈1..k with a ̸= b
(otherwise the thesis trivially follows). By (1), we have that:
σ(xa) =

#ha
if σ′(xa) = rha
(a1)
σ′(xa)
otherwise
(a2)
σ(xb) =

#hb
if σ′(xb) = rhb
(b1)
σ′(xb)
otherwise
(b2)
If σ′ |= ρ0 = ρ1, then σ′(xa) = σ′(xb). Let r = σ′(xa). If r ∈res(η) ∪res(ϕ),
then we are in the case (a2,b2), and so σ(xa) = σ′(xa) = σ′(xb) = σ(xb).
Otherwise, if r ̸∈res(η) ∪res(ϕ), then r = rha = rhb. Since r1, . . . , rk′ are
pairwise distinct, then ha = hb, and so by (1), σ(xa) = #ha = #hb = σ(xb).
In both cases, we have proved that σ |= ρ0 = ρ1.
Conversely, if σ |= ρ0 = ρ1, then σ(xa) = σ(xb). Only the cases (a1,b1) and
(a2,b2) are possible, because {#1, . . . , #k} is disjoint from res(η)∪res(ϕ). In
the case (a1,b1), we have that σ(xa) = #ha = #hb = σ(xb). Since #1, . . . , #k
are pairwise distinct, then ha = hb, from which σ′(xa) = rha = rhb = σ′(xb).
In the case (a2,b2), we have that σ′(xa) = σ(xa) = σ(xb) = σ′(xb). In both
cases, we have proved that σ′ |= ρ0 = ρ1.

Usage Automata
59
For the inductive case, the guard g can have either the form ¬g0 or g0 ∧g1, for
some g0 and g1. If g = ¬g0, then by the induction hypothesis of (3) it follows
that σ |= g0 if and only if σ′ |= g0, from which the thesis is straightforward. If
g = g0 ∧g1, the thesis follows directly from the induction hypothesis.
This concludes the proof of (3), which we have shown to imply q′∈X′(qi, αi(ri)).
Consider now the case q′ ∈(complete(X′, Res) \ X′)(qi, αi(ri)), i.e. q′ = qi has
been added as a self-loop. By Def. 3, we have that:
∄q ∈Q : qi
αi(ri)
−−−−→q ∈X′
We prove that q′ ∈δAϕ(σ)(qi, αi(ri)) by showing that the above implies:
∄q ∈Q : qi
αi(ri)
−−−−→q ∈X
We proceed contrapositively, i.e. we show:
∃q ∈Q : qi
αi(ri)
−−−−→q ∈X =⇒∃q ∈Q : qi
αi(ri)
−−−−→q ∈X′
Assume that qi
αi(ri)
−−−−→q ∈X. Then, there exist ρ and g such that:
qi
αi(ρ): g
−−−−−⊸q ∈E
ρσ = ri
σ |= g
Similarly to the previous case, this implies ρσ′ = ri, and by (3) it also follows
that σ′ |= g. Therefore, qi
αi(ri)
−−−−→q ∈X′. Summing up, we have proved (2), from
which the thesis directly follows.
⊓⊔
Following [16], we can characterize the set of policies recognized by our usage
automata as a subset of safety properties. Usage automata recognize properties,
because they deﬁne policies of the form P = { η | η |= ϕ }. They recognize safety
properties because, by Def. 3, for all traces η and η′, if ηη′ |= ϕ, then η |= ϕ.
We conclude this section by presenting some further relevant policies express-
ible through usage automata.
Example 4. Consider the policy requiring that, in a trace α(r1)α(r2)α(r3), the
three resources r1, r2, r3 must be distinct. This is modelled by the usage automa-
ton ϕ3 in Fig. 4. A straightforward generalisation allows for keeping distinct k
resources, by using a policy with arity k −1.
Example 5. To justify the extra resources #1, . . . , #k used in Lemma 2, let ϕ
be the usage automaton with a single edge q0
α(x) : x̸=y
−−−−−−⊸fail. When a trace η
q0
q1
q3
α(x)
α(y)
α(x)
α(x)
α(y)
q2
Fig. 4. The usage automaton ϕ3 allows for keeping distinct three resources

60
M. Bartoletti
contains some event α(r), there exists an instantiation Aϕ(σ, res(η)) that rec-
ognizes η as oﬀending, e.g. take σ = {x →r, y →r′} for some r′ ̸= r. So, ϕ
actually forbids any α actions. Consider now η = α(r0) β(r0). If Lemma 2 would
not allow the range of σ to include any resource except r0, then there would
exist a single choice for σ, i.e. σ0 = {x →r0, y →r0}. This would be wrong,
because η ◁Aϕ(σ0, {r0}), while we know by Def. 3 that η violates ϕ. The re-
sources #1, #2 allows for choosing e.g. σ1 = {x →r0, y →#1}, which yields
η ̸◁Aϕ(σ1, {r0}), from which Lemma 2 correctly infers that η ̸|= ϕ.
⊓⊔
Example 6. A classical security policy used in commercial and corporate busi-
ness services is the Chinese Wall policy [7]. In such scenarios, it is usual to
assume that all the objects which concern the same corporation are grouped
together into a company dataset, e.g. bankA, bankB, oilA, oilB, etc. A conﬂict of
interest class groups together all company datasets whose corporations are in
competition, e.g. Bank containing bankA, bankB and Oil containing oilA, oilB.
The Chinese Wall policy then requires that accessing an object is only permitted
in two cases. Either the object is in the same company dataset as an object al-
ready accessed, or the object belongs to a diﬀerent conﬂict of interest class. E.g.,
the trace read(oilA, Oil) read(bankA, Bank) read(oilB, Oil) violates the policy,
because reading an object in the dataset oilB is not permitted after having ac-
cessed oilA, which is in the same conﬂict of interests class Oil. The Chinese Wall
policy is speciﬁed by ϕCW in Fig. 5. The edge from q0 to q1 represents accessing
the company dataset x in the conﬂict of interests class y. The edge leading from
q1 to the oﬀending state q2 means that a dataset x′ diﬀerent from x has been
accessed in the same conﬂict of interests class y.
⊓⊔
Example 7. As a more substantial playground for studying the applicability of
usage automata to express and enforce real-world policies, we consider a bulletin
board system inspired by phpBB, a popular open-source Internet forum. A bul-
letin board consists of a set of users and a set of forums. Users can create new
topics within forums. A topic represents a discussion, to be populated by posts
inserted by users. Users may belong to three categories: guests, registered users,
and, within these, moderators. We call regular users those who are not modera-
tors. Each forum is tagged with a visibility level: PUB if everybody can read and
write, REG if everybody can read and registered users can write, REGH if only
registered users can read and write, MOD if everybody can read and moderators
can write, MODH if only moderators can read and write. Additionally, we assume
there exists a single administrator, which is the only user who can create new
forums and delete them, set their visibility level, and lock/unlock them. Mod-
erators can edit and delete the posts inserted by other users, can move topics
q2
q0
y conﬂict of interest class
ϕCW
x, x′ company datasets
q1
read(x′, y) : x ̸= x′
read(x, y)
Fig. 5. A usage automaton ϕCW for the Chinese Wall policy

Usage Automata
61
fail
q1
q0
demote(∗, u)
promote(u, ∗) : u ̸= admin
promote(∗, u)
demote(u, ∗) : u ̸= admin
Fig. 6. Usage automaton for “only moderators can promote and demote other users”
through forums, can delete and lock/unlock topics. Both the administrator and
the moderators can promote users to moderators, and vice-versa; the adminis-
trator cannot be demoted. Usage automata can be used to specify a substantial
subset of the policies implemented (as local checks hard-wired in the code) by
the server-side of phpBB, among which:
– only registered users can post to a REG or REGH forum; only moderators can
post to a MOD or MODH forum.
– only registered users can browse a REGH forum; only moderators can browse
a MODH forum.
– the session identiﬁer used when interacting with the bulletin board must be
the one obtained at login.
– a message can be edited/deleted by its author or by a moderator, only.
– a regular user cannot edit a message after it has been edited by a moderator.
– a regular user cannot delete a message after it has been replied.
– nobody can post on a locked topic.
– only moderators can promote/demote users and lock/unlock topics;
– only the administrator can lock/unlock forums.
Consider for instance the policy “only moderators can promote and demote
other users”. This is modelled by the usage automaton in Fig. 6. The state q0
models u being a regular user, while q1, is for when u is a moderator. The ﬁrst two
transitions actually represent u being promoted and demoted. In the state q0, u
cannot promote/demote anybody, unless u is the administrator. For example, the
trace η = promote(admin, alice)promote(alice, bob)demote(bob, alice) respects
the policy, while η promote(alice, carl) violates it. Note that in Fig. 6 we use the
wildcard ∗to denote an arbitrary user. This has the same meaning of substituting
a fresh variable for each occurrence of ∗.
⊓⊔
3
On the Expressive Power of Usage Automata
In the previous section we showed usage automata suitable for expressing a wide
variety of policies, also from real-world scenarios. In this section we try to answer
some interesting questions about their expressive power.
We start by investigating two restricted forms of usage automata. First, we
consider usage automata without guards, and we show them less expressive than
unrestricted ones. Then, we restrict the events α(ρ) on the edges of usage au-
tomata to be monadic, i.e. with |ρ| = 1. Unlike in the previous case, this restric-
tion does not aﬀect the expressive power of usage automata.
Lemma 3. Removing guards decreases the expressive power of usage automata.

62
M. Bartoletti
Proof. Recall from Ex. 1 the usage policy DIFF(1), preventing the action α
to be ﬁred on k > 1 distinct resource. We shall prove that there exists no
usage automaton without guards that recognizes DIFF(1). By contradiction, as-
sume that ϕ has no guards and recognizes DIFF(1). Let η = α(r)α(r′), where
r, r′ ̸∈res(ϕ) and r ̸= r′. Since η ̸∈DIFF(1), by Lemma 2 it follows that
η ̸◁Aϕ(σ, {r, r′}) for some σ : var(ϕ) →{r, r′} ∪res(ϕ) ∪{#1, . . . , #|var(ϕ)|}.
Also, since α(r) ∈DIFF(1), then α(r) ◁Aϕ(σ, {r, r′}). Any run of Aϕ(σ, {r, r′})
on η will then have the form:
q0
α(r)
−−−→q1
α(r′)
−−−→q2
where q0, q1 ̸∈F and q2 ∈F
Since q1 ̸∈F and q2 ∈F, the transition from q1 to q2 cannot be a self-loop, and
so there exists q1
α(x)
−−−⊸q2 ∈E such that σ(x) = r′. Also, it must be q0 ̸= q1. To
show that, assume by contradiction that q0 = q1. Since q0 = q1
α(x)
−−−⊸q2 ∈E,
then the trace α(r) would violate ϕ, which cannot be the case by the hypothesis
that ϕ recognizes DIFF(1). Thus, q0 ̸= q1, and so there exists q0
α(y)
−−−⊸q1 ∈E
such that σ(y) = r. Consider now the trace η′ = α(r)α(r) ∈DIFF(1). We have
that η′ ̸◁Aϕ({x →r, y →r}, {r}), and so by Lemma 2 it follows that η′ ̸|= ϕ –
contradiction, because we assumed ϕ to recognize DIFF(1).
⊓⊔
We now consider the second restriction, i.e. only allowing for monadic events in
the edges of usage automata. To show that this restriction does not aﬀect their
expressive power, we deﬁne a transformation that substitutes monadic events
for polyadic ones in usage automata (Def. 4). We then show in Lemma 4 that
the original and the transformed usage automata recognize the same policy. Of
course, one needs to make monadic also the events in traces. To do that, we ﬁrst
deﬁne the set Act1 of monadic actions as follows:
Act1 = { α1, . . . , α|α| | α ∈Act }
Then, we deﬁne an injective transformation slice : (Act×Res∗)∗→(Act1 ×Res)∗
of traces as follows:
slice(ε) = ε
slice(η α(r1, . . . , rk)) = slice(η) α1(r1) · · · αk(rk)
In Def. 4 below we transform usage automata in order to make them recognize
policies on traces of monadic events.
Deﬁnition 4. Slicing of polyadic events
Let ϕ = ⟨S, Q, q0, F, E⟩be a usage automaton. We deﬁne the usage automaton
slice(ϕ) = ⟨S′, Q′, q0, F, E′⟩as follows:
S′ = { α1(ρ1), . . . , αk(ρk) | α(ρ1, . . . , ρk) ∈S }
E′ = { q
α1(ρ1): g
−−−−−−⊸q1
α2(ρ2)
−−−−⊸· · · qk−1
αk(ρk)
−−−−⊸q′ | q
α(ρ1,...,ρk): g
−−−−−−−−−⊸q′ ∈E }
Q′ = Q ∪{ qi | q
αi(ρi)
−−−−⊸qi ∈E′ }

Usage Automata
63
The following lemma states that polyadic events do not augment the expressive
power of usage automata. Formally, it shows that making monadic the events of
a usage automaton ϕ preserves the policy deﬁned by ϕ, i.e. ϕ and slice(ϕ) deﬁne
the same policy (modulo the injective transformation of traces).
Lemma 4. For all usage automata ϕ and for all traces η, η |= ϕ if and only if
slice(η) |= slice(ϕ).
Proof. Straightforward by induction on the length of η.
⊓⊔
We now turn our attention to the arity of usage automata, deﬁned as |ϕ| =
|var(ϕ)|. Unlike the arity of events, which we showed above unrelated to the
expressive power, this is not the case for the arity of usage automata. Indeed,
we will prove that each increment of the arity of usage automata corresponds to
an increment of the usage policies they can recognize.
To show that, we exploit a proof technique provided by Lemma 5 below.
Intuitively, it states that a usage automaton ϕ cannot distinguish among more
than |ϕ| + |res(ϕ)| + 1 distinct resources. More formally, if η ̸|= ϕ, then one can
“deﬂate” to |ϕ|+|res(ϕ)|+1 the number of distinct resources in η, and still obtain
a violation. Deﬂating to n the number of resources in η amounts to applying to
η a substitution τ : res(η) →res(η) ∪{#} with |ran(τ)| = n, where # is a
distinguished resource not appearing in res(η) and ran(τ) = { xτ | x ∈res(η) }.
Lemma 5. For all usage automata ϕ and for all traces η:
(∀τ : res(η)→res(η) ∪{#}:|ran(τ)| ≤|ϕ| + |res(ϕ)| + 1 =⇒ητ |= ϕ) =⇒η |= ϕ
Proof. Let n = |ϕ| + |res(ϕ)| + 1. If |res(η)| ≤n, there is nothing to prove
(it suﬃces to choose τ be the identity). Let then |res(η)| = n′ > n. We proceed
contrapositively. By contradiction, assume that η ̸|= ϕ. We prove that there exists
τ : res(η) →res(η) ∪{#} with |ran(τ)| ≤n, such that ητ ̸|= ϕ. By Lemma 2,
η ̸|= ϕ implies that there exists σ : var(ϕ) →res(η) ∪res(ϕ) ∪{#1, . . . , #|ϕ|}
such that η ̸◁Aϕ(σ). For all r ∈res(η), let τ be deﬁned as follows:
τ(r) =

r
if r ∈ran(σ) or r ∈res(ϕ)
#
otherwise
(4)
Note that |ran(τ)| ≤|ran(σ)| + |res(ϕ)| + 1 ≤|var(ϕ)| + |res(ϕ)| + 1 = n. By
induction on the length of η, we prove that:
δ∗
Aϕ(σ)(q0, η) ⊆δ∗
Aϕ(τ◦σ)(q0, ητ)
(5)
That is, each transition taken by Aϕ(σ) on η can be replayed on the trace ητ.
Since δAϕ(σ)(q0, η) ∈F, from (5) it will follow that δAϕ(σ)(q0, ητ) ∈F, from
which Lemma 2 implies the thesis ητ ̸|= ϕ.
We now prove (5). W.l.o.g. we assume ϕ has monadic events (see Lemma 4).
The base case η = ε is trivial. For the inductive case, let η = η′α(r). By the
induction hypothesis, δ∗
Aϕ(σ)(q0, η′) ⊆δ∗
Aϕ(σ)(q0, η′τ). Let now q ∈δ∗
Aϕ(σ)(q0, η′),

64
M. Bartoletti
and let q′ ∈δAϕ(σ)(q, α(r)). Consider ﬁrst the case that q′ has not been added
as a self-loop, i.e. q′ ∈X(q, α(r)). Then, by Def. 3 it follows that:
∃ρ, g :
q
α(ρ) : g
−−−−⊸q′ ∈E
ρσ = r
σ |= g
To prove that q′ ∈δAϕ(σ)(q, α(rτ)), just note that since r = ρσ, then either
r ∈ran(σ) or r = ρ ∈res(ϕ), and so by (4) it follows that rτ = r.
Consider now the case q′ ∈(complete(X, res(η)) \ X)(q, α(r)), i.e. q′ = q has
been added as a self-loop. By Def. 3, we have that:
∄q′′ ∈Q : q
α(r)
−−−→q′′ ∈X
We prove that q ∈δAϕ(σ)(q, α(rτ)) by showing that the above implies:
∄q′′ ∈Q : q
α(rτ)
−−−→q′′ ∈X
There are two cases, according to (4). If rτ = r, then the thesis trivially follows.
Otherwise, we have that rτ = #, and since # is a distinguished resource – in
particular, diﬀerent from any resource in ran(σ) and res(ϕ) – then α(#) cannot
be a label in X. Summing up, we have proved (5), which implies the thesis.
⊓⊔
Example 8. To justify the term “+1” in |ran(τ)| ≤|ϕ| + |res(ϕ)| + 1, consider
the usage automaton ϕ deﬁned below, and the trace η = α(r0)α(r1):
q0
q1
α(x)
α(x)
We have that ϕ ̸|= η, e.g. η ̸◁Aϕ(σ) with σ = {x →r0}. Since |ϕ| = 1 and
res(ϕ) = 0, Lemma 5 states that it is possible to obtain a violation by deﬂating
η to 2 resources. Without the term “+1”, we could have incorrectly deﬂated η
to just 1 resource. Actually, all the possible “deﬂations” of η to only 1 resource
– e.g. α(r0)α(r0) and α(r1)α(r1) – do obey ϕ, while η ̸|= ϕ.
⊓⊔
We can now prove that the policy DIFF(k) of Ex. 1 cannot be expressed by
a usage automaton with arity k′ < k, in the case Res is an inﬁnite set. By
contradiction, assume that there exists ϕ with arity k′ such that η |= ϕ if and
only if η ∈DIFF(k), for all η. W.l.o.g. we can assume that res(ϕ) = ∅. Let:
ηk = α(r0)α(r1) · · · α(rk)
with ri ̸= rj for all i ̸= j ∈0..k
(6)
Since k′ < k, we have that ηkτ |= ϕ for all τ : res(η) →res(η) ∪{#} such
that |ran(τ)| ≤k′ + 1. By Lemma 5, this implies that ηk |= ϕ. But this is a
contradiction, because by (6) it follows that ηk ̸∈DIFF(k). Therefore, DIFF(k)
cannot be expressed by any usage automaton with arity less then k.

Usage Automata
65
4
Run-Time Enforcement of Usage Policies
In this section we discuss the feasibility of exploiting usage automata to imple-
ment run-time monitors for usage policies. The starting point is the observation
that the speciﬁcation of policy compliance of Def. 3 is not suitable for run-time
monitors. Indeed, it requires the whole trace η, to decide whether η |= ϕ or
not. Since the trace of a running program will typically grow unbound, it is not
feasible for a concrete run-time monitor to record it as a whole.
A realistic run-time monitor should only require a ﬁnite, bounded amount
of memory to decide policy compliance. In particular, we aim at an execution
monitor that does not need to record (any part of) the execution trace. Our
monitor will just observe one event at a time, as it is ﬁred at run-time, and use
this event to update its (bounded) internal state.
We now deﬁne an enforcement mechanism for usage policies, that is suitable
for run-time monitoring. Without loss of generality, we assume that events are
monadic (see Lemma 4). The conﬁgurations of our mechanism have the form:
Q = {σ0 →Q0, . . . , σk →Qk}
Intuitively, each σi : var(ϕ) →Res represents a possible instantiation of ϕ into a
ﬁnite state automaton Aϕ(σi). The set Qi ⊆Q associated with σi represents the
states reachable by the instantiation Aϕ(σi) upon the event trace seen so far.
The number of instantiations recorded by the mechanism grows as new resources
are discovered at run-time (yet, with the optimizations discussed below, it needs
a bounded amount of space under the hypothesis that the number of resources
“live” at any moment is bounded). The conﬁguration Q of the mechanism is
updated whenever an event is ﬁred. The formal speciﬁcation is in Def. 5.
Deﬁnition 5. Enforcement mechanism for usage automata
INPUT: a usage automaton ϕ = ⟨S, Q, q0, F, E⟩and a trace η.
OUTPUT: true if η |= ϕ, false otherwise.
1. Q := { σ →{q0} | σ : var(ϕ) →{#1, . . . , #p} }, where p = |ϕ| and
#1, . . . , #p are arbitrary resources in Res \ (res(η) ∪res(ϕ)).
2. while η is not empty, let η = α(r) η′, do η := η′, and:
(a) if r ̸∈res(Q), update Q as follows. For all σ occurring in Q and for
all σ′ : var(ϕ) →res(Q) ∪{r} such that, for all x ∈var(ϕ), either
σ′(x) = σ(x) or σ(x) = #j:
Q := Q [σ′ →Q(σ)]
(b) Let step(q) = { q′ | q ∈Qi ∧q
α(ρ): g
−−−−⊸q′ ∈E ∧ρσi = r ∧σi |= g }.
Let step′(q) = (if step(q) = ∅then {q} else step(q)).
Then, for all (σi →Qi) ∈Q, update Q as follows:
Q := Q [σi →
q∈Qi step′(q)]
3. return true if, for all (σi →Qi) ∈Q, Qi ∩F = ∅. Otherwise, return false.

66
M. Bartoletti
The enforcement mechanism in Def. 5 actually deﬁnes a relation η ⊢ϕ, that
holds whenever the above algorithm outputs true upon the inputs ϕ and η. The
following lemma relates the “speciﬁcation” of policy compliance in Def. 3 with
its “implementation” provided by Def. 5. As expected, the two notions coincide.
Lemma 6. For all traces η and usage automata ϕ, η |= ϕ if and only if η ⊢ϕ.
Proof. Let η = α1(r1) · · · αn(rn), and let Q0
α1(r1)
−−−−→· · ·
αn(rn)
−−−−→Qn be the
sequence of conﬁgurations of the enforcement mechanisms upon input η, where
Q0 is the initial conﬁguration. By Lemma 2, η |= ϕ if and only if η ◁Aϕ(σ),
for all σ : var(ϕ) →res(η) ∪res(ϕ) ∪{#1, . . . , #p}. Let ηi be the preﬁx of η
containing exactly i events. We deﬁne the preorder ≤as follows:
σ ≤σ′
iﬀ
dom(σ) = dom(σ′)
and ∀x : σ(x) = #i ∨σ(x) = σ′(x)
and ∀x, y : σ(x) = σ(y) ⇐⇒σ′(x) = σ′(y)
By induction on i, it is easy to prove that:
σ ≤σ ∧σ ≤σ′ =⇒Qi(σ) = Qi(σ′)
(7)
For all i ∈0..n, choose then σi be such that:
σi ∈max { σ′ ∈dom(Qi) | σ′ ≤σ }
By (7) it follows that the image of Qi on any maximum element of the set
{ σ′ ∈dom(Qi) | σ′ ≤σ } is the same. We shall then prove that, for all i ∈0..n:
δ∗
Aϕ(σ)(q0, ηi) = Qi(σi)
(8)
The proof then proceeds by a straightforward induction on i.
⊓⊔
We now discuss some more concrete issues, building upon our experience with
the implementation of a run-time monitor that enforces usage policies to Java
programs [13]. A key issue is how to keep the size of the mechanism state small
enough to make acceptable the overhead due to run-time monitoring. Some ob-
servations may help in achieving that.
First, observe that in real-world scenarios, the variables of usage automata
will be typed, e.g. in the Chinese Wall policy of Ex. 6, the variables x, x′ will
have type “company dataset”, while y will have type “conﬂict of interests class”.
Therefore, only those σ that respect types will need to be considered.
Second, the number of variables used in a policy can be reduced by means of
wildcards. For instance, in [13] we used the wildcard ∗to denote “any resource”,
and −to denote “any resource diﬀerent from the other resources mentioned in
the policy”. Since the number of possible instantiations grows exponentially in
the number of variables, using wildcards may drastically reduce the size of the
enforcement mechanism.
Third, some optimizations of the algorithm in Def. 5 – besides using smart
data structures – are possible. For instance, there is no need to allocate in step

Usage Automata
67
(a) a new instance Aϕ(σ), unless Aϕ(σ) can take a transition in step (b). Also,
when a resource r is garbage-collected, we can discard all the instantiations
Aϕ(σ) with r ∈ran(σ); it suﬃces to record in Q the states of Aϕ(σ) in a special
entry σ† →Q† for disposed objects.
5
Conclusions
We have studied a model for policies that control the usage of resources. Usage
automata were originally introduced in [3]. The current work thoroughly im-
proves and simpliﬁes them. Usage automata can now have an arbitrary number
of parameters, which augments their expressive power and allows for modelling
signiﬁcant real-world policies (see e.g. the Jalapa Project [13]). Also, the guards
featured here allow for specifying policies that were not expressible through the
negation operator (¯x, meaning “any resource diﬀerent from x”) proposed in [3].
For instance, usage automata with guards can recognize DIFF(k) for all k, while
those with negated variables cannot do that for k > 1.
The present work provides a deeper understanding of some foundational as-
pects of usage automata. From the point of view of expressivity, we have devised
a proof technique (the “deﬂating lemma”) through which showing that certain
policies cannot be expressed by usage automata having less than a given number
of variables. Also, we have proved that restricting the events to be monadic has
no impact on the expressive power. From the point of view of implementability,
we have proposed an enforcement mechanism to decide policy compliance. This
mechanism only requires a bounded amount of memory.
Usage automata have been used in [2] to deﬁne a uniﬁed framework for the
veriﬁcation of history-based policies of programs. There, programs are terms
in a λ-calculus enriched with primitives to create and use resources, and with
a lexical mechanism to deﬁne the scope of usage policies (speciﬁed by usage
automata). A type and eﬀect analysis infers sound approximations of the set of
run-time traces, and a model-checking algorithm veriﬁes whether these traces
comply with the policies at hand.
Related work. A lot of recent research is concerned with the study of usage
policies and of their enforcement mechanisms.
A characterization of the class of policies enforceable through run-time mon-
itoring systems is given in [16]. There, the policy language is that of security
automata, a class of B¨uchi automata that recognize safety properties. To handle
the case of parameters ranging over inﬁnite domains (e.g. the formal param-
eters of a method), security automata resort to a countable set of states and
input symbols. While this makes security automata suitable to provide a com-
mon semantic framework for policy languages (e.g. the semantics of our usage
automata can be given in terms of a mapping into security automata) it also
makes security automata hardly usable as an eﬀective formalism for expressing
policies. In [11] a characterization is given of the policies that can be enforced
through program rewriting techniques. In [5] a kind of automata that can delete
and insert events in the execution trace is considered. Polymer [6] is a language

68
M. Bartoletti
for specifying, composing and enforcing (global) security policies. In the lines of
edit automata [5], a policy can intervene in the program trace, to insert or sup-
press some events. Policy composition can then be unsound, because the events
inserted by a policy may interfere with those monitored by another policy. To
cope with that, the programmer must explicitly ﬁx the order in which policies
are applied. The access control model of Java is enhanced in [15], by specifying
ﬁne-grained constraints on the execution of mobile code. A method invocation
is denied when a certain condition on the dynamic state of the system is false.
Security policies are modeled as process algebras in [14]. A custom Java Virtual
Machine is used, with an execution monitor that traps system calls and ﬁres
them concurrently to the policy. When a trapped system call is not permitted
by the policy, the execution monitor tries to force a corrective event – if possible
– otherwise it aborts the system call. Unlike our usage automata, the models
of [6,14,15] are Turing-equivalent, and so they are able to recognize more poli-
cies than ours. Yet, this ﬂexibility comes at a cost. First, the process of deciding
whether an action must be denied or not might loop forever. Second, non-trivial
static optimizations are unfeasible, unlike in our approach. In particular, no
static guarantee can be given about the compliance of a program with the im-
posed policy: run-time monitoring is then needed to enforce the policy, while our
usage automata are model-checkable [4], and so may avoid this overhead.
Shallow history automata are investigated in [9]. These automata can keep
track of the set of past access events, rather than the sequence of events. Al-
though shallow history automata can express some interesting security proper-
ties, they are clearly less expressive than our usage automata.
In [12], policies are modelled as sets of permitted usage patterns, to be at-
tached to resources upon creation. Our usage automata are not hard-wired to
resources, yet they are parametric over resources. For instance, a usage automa-
ton ϕ with variables x, y means that, for all the possible instantiations of x and
y to actual resources, the obligation expressed by ϕ must be obeyed. This is
particularly relevant in mobile code scenarios, where you need to impose con-
straints on how external programs access the resources created in your local
environment, without being able to alter the code.
Acknowledgments. I thank Roberto Zunino for his insightful comments. Research
partially supported by EU-FETPI Global Computing Project IST-2005-16004
Sensoria (Software Engineering for Service-Oriented Overlay Computers) and
by PRIN Project SOFT (Tecniche Formali Orientate alla Sicurezza).
References
1. Abadi, M., Fournet, C.: Access control based on execution history. In: Proceedings
of the 10th Annual Network and Distributed System Security Symposium, San
Diego, California, USA, The Internet Society (2003)
2. Bartoletti, M., Degano, P., Ferrari, G.L., Zunino, R.: Local policies for resource
usage analysis. To appear in ACM Tran. Programming Languages and Systems

Usage Automata
69
3. Bartoletti, M., Degano, P., Ferrari, G.-L., Zunino, R.: Types and eﬀects for resource
usage analysis. In: Seidl, H. (ed.) FOSSACS 2007. LNCS, vol. 4423, pp. 32–47.
Springer, Heidelberg (2007)
4. Bartoletti, M., Degano, P., Ferrari, G.L., Zunino, R.: Model checking usage poli-
cies. In: Proceedings of the 4th Trustworthy Global Computing, Barcelona, Spain.
LNCS, vol. 5474, pp. 19–35. Springer, Heidelberg (2009)
5. Bauer, L., Ligatti, J., Walker, D.: More enforceable security policies. In: Proceed-
ings of the Workshop on Foundations of Computer Security (FCS) (2002)
6. Bauer, L., Ligatti, J., Walker, D.: Composing security policies with Polymer. In:
Proceedings of the ACM SIGPLAN 2005 Conference on Programming Language
Design and Implementation (PLDI), Chicago, USA, pp. 305–314. ACM, New York
(2005)
7. Brewer, D.F.C., Nash, M.J.: The Chinese Wall security policy. In: Proceedings of
the 1989 IEEE Symposium on Security and Privacy (1989)
8. Edjlali, G., Acharya, A., Chaudhary, V.: History-based access control for mobile
code. In: Vitek, J. (ed.) Secure Internet Programming. LNCS, vol. 1603. Springer,
Heidelberg (1999)
9. Fong, P.W.: Access control by tracking shallow execution history. In: Proceedings
of the IEEE Symposium on Security and Privacy (S&P 2004), Berkeley, CA, USA,
May 9-12, pp. 43–55. IEEE Computer Society, Los Alamitos (2004)
10. Gong, L.: Inside Java 2 platform security: architecture, API design, and implemen-
tation. Addison-Wesley, Reading (1999)
11. Hamlen, K.W., Morrisett, J.G., Schneider, F.B.: Computability classes for enforce-
ment mechanisms. ACM Trans. on Programming Languages and Systems 28(1),
175–205 (2006)
12. Igarashi, A., Kobayashi, N.: Resource usage analysis. In: Proceedings of the 29th
Annual Symposium on Principles of Programming Languages (POPL), pp. 331–
342. ACM, New York (2002)
13. Jalapa: Securing Java with Local Policies, http://jalapa.sourceforge.net
14. Martinelli, F., Mori, P.: Enhancing java security with history based access control.
In: Aldini, A., Gorrieri, R. (eds.) FOSAD 2007. LNCS, vol. 4677, pp. 135–159.
Springer, Heidelberg (2007)
15. Pandey, R., Hashii, B.: Providing ﬁne-grained access control for java programs.
In: Guerraoui, R. (ed.) ECOOP 1999. LNCS, vol. 1628, pp. 449–473. Springer,
Heidelberg (1999)
16. Schneider, F.B.: Enforceable security policies. ACM Trans. on Information and
System Security 3(1), 30–50 (2000)
17. Skalka, C., Smith, S.: History eﬀects and veriﬁcation. In: Chin, W.-N. (ed.) APLAS
2004. LNCS, vol. 3302, pp. 107–128. Springer, Heidelberg (2004)

Static Detection of Logic Flaws in
Service-Oriented Applications⋆
Chiara Bodei1, Linda Brodo2, and Roberto Bruni1
1 Dipartimento di Informatica, Universit`a di Pisa, Italy
{chiara,bruni}@di.unipi.it
2 Dipartimento di Scienze dei Linguaggi, Universit`a di Sassari, Italy
brodo@uniss.it
Abstract. Application or business logic, used in the development of
services, has to do with the operations that deﬁne the application func-
tionalities and not with the platform ones. Often security problems can
be found at this level, because circumventing or misusing the required
operations can lead to unexpected behaviour or to attacks, called ap-
plication logic attacks. We investigate this issue, by using the CaSPiS
calculus to model services, and by providing a Control Flow Analysis
able to detect and prevent some possible misuses.
1
Introduction
More and more web surfers use applications based on web service technology
for their transactions, such as bank operations or e-commerce purchases. The
increasing availability of information exchange over e-services comes at the price
of new security threats: new clever forms of attacks can come out at a rate that
is growing with growth in usage.
Among the many diﬀerent kinds of attacks that a malicious hacker can launch
against web services applications, we here focus on the so-called application logic
attacks, which are tailored to exploit the vulnerabilities of the speciﬁc function-
alities of the application rather than the more general ones provided by the used
platform, i.e. they violate application or business logic (see e.g., [11,19]). This
logic represents the functions or the services that a particular site provides, in
terms of the steps required to ﬁnalise a business goal, e.g. in an e-shop, the ap-
plication logic can establish that customers’ personal data necessary to complete
an order must be provided only after the shopping basket is completed. Logic
bugs in the application design may open dangerous loopholes that allow a user
to do something that isn’t allowed by the business, just by abusing or misusing
the functions of the application, even without modifying them. Unfortunately,
often security is not considered from the very beginning of the application de-
velopment: the focus is on what the user is expected and allowed to do and not
on the possible pathological usage scenarios that a goofy user may encounter
⋆Research supported by the EU FET-GC2 Project IST-2005-016004 Sensoria, by
the Italian PRIN Project “SOFT” and by the Italian FIRB Project Tocai.it.
P. Degano and L. Vigan`o (Eds.): ARSPA-WITS 2009, LNCS 5511, pp. 70–87, 2009.
c
⃝Springer-Verlag Berlin Heidelberg 2009

Static Detection of Logic Flaws in Service-Oriented Applications
71
or a malicious user may exploit. For example, take a conference management
system handling blind peer reviews: a malicious author could insert the names
of “unfriendly” Program Committee members as co-authors of her/his paper to
make them in conﬂict and exclude their opinions from the discussion phase. The
ﬁctitious co-authoring could then be removed in case the paper got accepted,
without compromising the overall consistency of the conference management
system and review process. Of course to prevent this misuse it could suﬃce to
notify all authors about the submission as part of the application logic.
Logical vulnerabilities are subtle, application speciﬁc, and therefore diﬃcult
to detect. As usual, in the development of complex systems resorting to formal
methods can be helpful. In particular, we try to transfer and adapt some tech-
niques used in the ﬁeld of network security (see e.g.,[8,10]). We develop a Control
Flow Analysis for analysing the close-free fragment of CaSPiS [13,15], a pro-
cess calculus recently introduced for modelling service oriented computing. The
key features of CaSPiS are a disciplined, built-in management of long-running
dyadic (and possibly nested) sessions between services and their clients, together
with data-ﬂow and orchestration primitives such as the pipeline operator and
pattern-matching primitives that are suited, e.g., to deal with XML-like data
typical of web service scenarios. The analysis statically approximates the be-
haviour of CaSPiS processes, in terms of the possible service and communication
synchronisations. More precisely, what the analysis predicts encompasses every-
thing that may happen, while what the analysis does not predict corresponds
to something that cannot happen. The session mechanism is particularly valu-
able for the kind of analysis we use, because it guarantees that sibling sessions
established between diﬀerent instances of the same service and the correspond-
ing clients do not interfere one with the other by leaking information, with two
main consequences: ﬁrst, our analysis can focus on each client-server conversa-
tion separately and second, we can focus on the application logic, neither having
to commit on any speciﬁc implementation of sessioning over a certain platform
nor worrying about the analysis of such a realisation.
The analysis we propose borrows some ideas from [7,20], because it exploits
the similarity between the nesting hierarchies introduced by session primitives
and the nesting hierarchies used in Ambients-like calculi. Furthermore to take
care of malicious users, we modify the classical notion of Dolev-Yao attacker [17]:
the attacker we are interested to model, that we call malicious customer, is an
insider or, more precisely, an accredited user of a service that has no control of
the communication channels, but that does not follow the intended rules of the
application protocol, e.g., (s)he can cheat or introduce inconsistent data.
We apply our framework to an example inspired by a known logic-ﬂawed ap-
plication for e-commerce, where an abuse of functionality is possible, in which
the attacker unexpectedly alters data, therefore modifying the application be-
haviour. The CyberOﬃce shopping cart [23] could be attacked by modifying the
hidden ﬁeld used for the price, within the web order form. The web page could
be indeed downloaded normally, edited unexpectedly outside the browser and
then submitted regularly with the prices set to any desired value, included zero

72
C. Bodei, L. Brodo, and R. Bruni
or even a negative value. If no data-consistency control was performed by the
server when the form was returned then the attack could be successful. Weak
forms of validation could lead to similar problems, like when checking all item
prices but not the total, or when checking goods price, but not the expedition
costs. Intuitively, the information exchange between the customer C and the e-
shop service S (and the data base DBI storing item prices) can be represented
by the following informal protocol narration (steps from 1. to 4.), that we borrow
from network security literature.
1.
C →S :
ItemA
2. S →DBI : Code, ItemA
3. DBI →C : OrderForm(Code, ItemA, PriceA)
4.
C →S :
PaymentForm(Code, ItemA, PriceA, Name, Cc)
...
Narration of the Protocol between Customer and E-shop Service
4′.
C →S :
PaymentForm′(Code, ItemA, FakedPriceA, Name, Cc)
Attack on Fourth Step
The customer chooses an item ItemA, receives its price PriceA inside an
OrderForm and can ﬁnalise the order by ﬁlling in a PaymentForm with per-
sonal data like Name and credit card information Cc. In the same form are
reported: the transaction Code, the item and its price (for simplicity, we assume
expedition expenses included). In case of pathological usage (step 4′), the re-
quired information is added on a forged copy of the payment form, where the
attacker has altered the price ﬁeld, using the forged price FakedPriceA, instead
of the one received from DBI within the OrderForm.
When modelled in CaSPiS, our analysis of the e-shop service is able to detect
the possible price modiﬁcation in harshly designed processes. The attack relies
on the fact that S does not check that the third ﬁeld of the received form has
the correct value. This is because the application logic relies on step 4 to acquire
personal data and credit card information of the customer that are considered
as good enough credentials for establishing the “circle of trust” over the pending
commercial transaction and it does not expect that misuses may still arise. Fur-
thermore, to save on the number of exchanged messages, it delegates the DBI
service (likely running on a separate, dedicated server) to communicate the price
directly to the customer, so that S cannot perform any validation over it when
the form is returned. A possible ﬁx to the problem would consists in having the
price information sent to S ﬁrst and then redirected to the customer, so that
it could be easy for the server to match the price included in PaymentForm
against the one received by DBI, as shown below.
2′′. S →DBI : Code, ItemA
3′′. DBI →S : Code, ItemA, PriceA
4′′.
S →C :
OrderForm(Code, ItemA, PriceA)
5′′.
C →S :
PaymentForm(Code, ItemA, PriceA, Name, Cc)
...
Alternative Narration

Static Detection of Logic Flaws in Service-Oriented Applications
73
Related Work. Recently some works have faced the issue of security in the com-
position and veriﬁcation of web applications. In [5] the authors propose security
libraries, automatically analysed with ProVerif [6], using the WS-Security poli-
cies of [22]. WS ReliableMessaging is instead analysed in [24] with the AVISPA [3]
veriﬁcation toolkit, initially developed to check properties of cryptographic pro-
tocols. Recently, in [12], security properties have been considered in an extension
of the π-calculus with session types and correspondence assertions [25]. In [1],
behavioral types are used to statically approximate the behavior of processes.
Building on [2,15], the type system [18] for CaSPiS is instead introduced to ad-
dress the access control properties related to security levels. Safety properties of
service behaviour is faced in [4]. Services can enforce security policies locally and
can invoke other services respecting given security contracts. Formal reasoning
about systems is obtained by means of dynamic and static semantics.
The focus of our interest is in formalising the logic used to develop an ap-
plication and discovering its intrinsic weaknesses, due to a design practice that
does not consider security as a ﬁrst-class concern.
Plan of the Paper. In Section 2, we present the calculus. In Section 3, we intro-
duce the Control Flow Analysis and the analysis of the malicious customer or
attacker. In Section 4, we apply our framework to the above shown example of
logic-ﬂawed application for e-commerce. Section 5 concludes the paper. Proofs
of theorems and lemmata presented throughout the paper are reported in [9].
2
The Calculus
We introduce here the fragment of CaSPiS that is suﬃcient to handle our mod-
elling needs (i.e., without the constructs for handling session termination): it
is essentially the one considered in [15]. Due to space limitation, we refer the
interested reader to [13,14] for the motivation around CaSPiS design choices, its
detailed description and many examples that witness its ﬂexibility.
Syntax. Let N ∋n, n′, ... be a countable set of names, that includes the set
Nsrv ∋s, s′, ... of service names and the set Nsess ∋r, r′, ... of session names,
with Nsrv ∩Nsess = ∅. We also let x range over variables (for service names and
data). We distinguish here between deﬁnition occurrences and use occurrences
of variables. A deﬁnition variable occurrence ?x is when x gets its binding value,
while a use occurrence x is when the value has been bound. We assume that a
variable cannot occur in both forms inside the same input, as in (..., ?x, ..., x, ...).
For the sake of simplicity, we let v range over values, i.e. names and use variables
and ˜v on tuples. In the second part of the paper, we shall use a more general
kind of input, including pattern matching. The syntax of CaSPiS is presented in
Fig. 1, where the operators occur in decreasing order of precedence.
As usual, the empty summation is the nil process 0 (whose trailing is mostly
omitted), parallel composition is denoted by P|Q and restriction by (νn)P. The
construct rp ▷P indicates a generic session side with polarity p (taking values
in {+, −}). Sessions are mostly intended as run-time syntax. In fact, diﬀerently

74
C. Bodei, L. Brodo, and R. Bruni
P, Q ::=
processes
| s.P
service deﬁnition
| v.P
service invocation
| Σi∈IπiPi
guarded sum
| rp ▷P
session (considered as run-time syntax)
| P > (?˜x)Q
pipeline (written P > ˜x > Q in the literature)
| (νn)P
restriction
| P|Q
parallel composition
| !P
replication
p, q
::= +|−
polarities
π, π′ ::=
action preﬁxes
| (?˜x)
input
| ⟨˜v⟩
output
| ⟨˜v⟩↑
return
Fig. 1. Syntax of CaSPiS
from other languages that provide primitives for explicit session naming and
creation, here all sessions are transparent to programmers as they can be built
automatically, resulting in a more elegant and disciplined style of writing pro-
cesses. A fresh session name r and two polarised session ends r−▷P and r+ ▷Q
are generated (on client and service sides, resp.) upon each service invocation
s.P of the service s.Q. We say r−▷P is the dual session side of r+ ▷Q and
vice versa. As P and Q share a session, their I/O communications are directed
toward the dual session side. We let p, q range over polarities and let p denote
the opposite polarity of p, where + = −and −= +. The preﬁx ⟨˜v⟩↑is used to
output values to the enclosing parent session and the pipe P > (?˜x)Q is a con-
struct that spawns a fresh instance Q[˜v/˜x] of Q on any value ˜v produced by P.
Note that we use a slight modiﬁcation of the pipeline P > Q introduced in [13],
similar to the variation considered in [15] and closer to Orc’s sequencing [16]: the
variables ˜x to be bound after pipeline synchronisation are included in a special
input (?˜x), called pipeline input preceding the right branch process.
We assume processes are designed according to some typical well-formedness
criteria: (i) the containment relation between session identiﬁers is acyclic (i.e. we
cannot have processes like rp ▷(P|rp ▷Q)); (ii) for each session identiﬁer r, each
r+ and r−occurs once in the process and never in the scope of a dynamic
operator; (iii) in any summation Σiπi, all preﬁxes πi are of one and the same
kind (all outputs or all inputs or all returns).
Semantics. The reduction semantics of CaSPiS exploits a rather standard struc-
tural congruence ≡on processes, deﬁned as the least congruence satisfying the
clauses in Fig. 2. The binders for the calculus are (˜x).P for ˜x in P and (νn)P
for n in P, with standard notions of free names (fn) and bound names (bn) of
a process. Processes are considered equivalent up to the α-renaming of bound
names. We present the operational semantics of the calculus by means of re-
duction contexts. The one-hole context C ·  is useful to insert a process P, the

Static Detection of Logic Flaws in Service-Oriented Applications
75
– (P/≡, |, 0) is a commutative monoid;
– !P ≡P| !P;
– (νn)0 ≡0, (νn)(νn′)P ≡(νn′)(νn)P, (νn)(P | Q) ≡P | (νn)Q
if n ̸∈fn(P);
– rp ▷(νn)P ≡(νn)(rp ▷P) if r ̸= n;
– ((νn)P) > (?x)Q ≡(νn)(P > (?x)Q) if n ̸∈fn(Q);
Fig. 2. Structural Congruence Laws
result being denoted C P  (process P replaces the hole inside the context), into
an arbitrary nesting of operators. Before describing the reduction rules, we need
to ﬁx some terminology. Let us call dynamic operators any service deﬁnition
s. · , service invocation s. · , preﬁx π · , left-sided pipeline P > (?x) ·  and
replication ! · . The remaining operators are called static. The contexts we are
interested in are called static, and characterised by the fact that the hole occurs
in an actively running position and it is ready to interact (e.g. it is not under a
preﬁx): formally, we say that a context is static if its holes do not occur in the
scope of a dynamic operator. Moreover, we say that a context is session-immune
if its hole does not occur under a session operator, and pipeline-immune if its
hole does not occur under a right-sided pipeline operator. In the following we
let C ·  range over static contexts, S ·  over static session-immune contexts,
and P ·  over contexts that are static, session-immune and pipeline-immune.
Roughly, a static session-immune context S ·  can “intercept” concretion pre-
ﬁxes but not abstraction and return preﬁxes, while a static, session-immune and
pipeline-immune context P ·  cannot “intercept” any preﬁx. Analogous deﬁni-
tions apply to the case of two-holes contexts C ·, · . Below we let Cr ·, ·  be
a context of the form C rp ▷P · , rp ▷S ·   (for some P ·  and S · ), which
helps us to characterise the most general situation in which intra-session com-
munication can happen, and we write Srp ·  for a context of the form rp ▷S · 
The reduction rules of CaSPiS are given in Fig. 3, where we assume that r
is fresh in Sync and that |˜x| = |˜v|. It can be shown that reductions preserve all
well-formedness criteria mentioned above, in the sense that well-formed processes
always reduce to well-formed processes.
Naming conventions. To distinguish among diﬀerent occurrences of the same
service, we assume to annotate each of them with a diﬀerent index, as in s@k.
As a consequence, we can uniquely identify the fresh name used in case of syn-
chronisation on the same service, e.g., when the synchronisation happens on the
service s, on the occurrences s@k and s@m, then the session name is rp
s@m:k.
When unambiguous, we simply use s, s and rp
s.
To distinguish among diﬀerent pipeline constructs, we annotate each pipeline
operator with a diﬀerent label l ∈L, as in >l. Also, we identify the left branch
with a label l0 and the right branch with l1. The same annotation l1 enriches
the variables ˜x aﬀected by the pipeline input in the right branch of the pipeline,
as in P >l (?˜xl1)Q. Note that these annotations do not aﬀect the semantics.
Furthermore, to simplify the deﬁnition of our Control Flow Analysis in Section
3, we discipline the α-renaming of bound values and variables. To do it in a simple
and “implicit” way, we partition all the names used by a process into ﬁnitely

76
C. Bodei, L. Brodo, and R. Bruni
Sync
C s.P, s.Q  →(νr)C r−▷P, r+ ▷Q 
S Sync
Cr ⟨˜v⟩P + 
i πiPi, (?˜x)Q + 
j πjQj  →Cr P, Q[˜v/˜x] 
S Sync Ret Cr S′
r′p ⟨˜v⟩↑P + 
i πiPi , (?˜x)Q + 
j πjQj  →
Cr S′
r′p P , Q[˜v/˜x] 
P Sync
C P ⟨˜v⟩P + 
i πiPi  > (?˜x)Q  →C Q[˜v/˜x]|(P P  > (?˜x)Q) 
P Sync Ret C P Srp ⟨˜v⟩P + 
i πiPi   > (?˜x)Q  →
C Q[˜v/˜x]|(P Srp P   > (?˜x)Q) 
Struct
P ≡P ′ ∧P ′ →Q′ ∧Q′ ≡Q ⇒P →Q
Fig. 3. Reduction Semantics of CaSPiS
many equivalence classes and we use the names of the equivalence classes instead
of the actual names. This partition works in such a way that names from the
same equivalence class are assigned a common canonical name and consequently
there are only ﬁnitely many canonical names in any execution of a given process.
This is enforced by assigning the same canonical name to every name generated
by the same restriction. The canonical name ⌊n⌋is for a name n; similarly ⌊x⌋
is for a variable x. In this way, we statically maintain the identity of values and
variables that might be lost by freely applying α-conversions. Hereafter, when
unambiguous, we shall simply write n (resp. x) for ⌊n⌋(resp. ⌊x⌋).
Examples. Consider the following simpliﬁed speciﬁcation of a bank credit request
service, where req is the service deﬁnition of the bank B.
B
def
= req@1.(?yba)val@3.⟨yba⟩(?wans)⟨wans⟩↑
V
def
= val@4.(?zba)⟨Ans⟩
After the client invocation req@1, the bank waits for the client balance asset Ba
(as input to ?yba) that should be passed to the Validation service V , through
the service invocation val@3. The validation answer Ans is sent to B (as input
to ?wans) and forwarded to the client. Thus a typical client C can be deﬁned as
C
def
= req@2.⟨Ba⟩(?xans)⟨xans⟩↑
After the service invocation req, the overall system S
def
= C|B|V becomes S′,
S →S′ def
= (νrreq@1:2)( r−
req@1:2 ▷⟨Ba⟩(?xans)⟨xans⟩↑|
r+
req@1:2 ▷(?yba)val@3.⟨yba⟩(?wans)⟨wans⟩↑) | V
where rreq@1:2 is the freshly generated session and where the client protocol is
running on the left (the session side with negative polarity r−) and the service
protocol on the right (the session side with positive polarity r+). The two pro-
tocols running on opposite sides of the same session can now exchange data,
leading to S′′:
S′ →S′′ def
= (νrreq@1:2)( r−
req@1:2 ▷(?xans)⟨xans⟩↑|
r+
req@1:2 ▷val@3.⟨Ba⟩(?wans)⟨wans⟩↑) | V

Static Detection of Logic Flaws in Service-Oriented Applications
77
The computation continues as follows:
S′′ →(νrreq@1:2)(r−
req@1:2 ▷(?xans)⟨xans⟩↑|
(νrval@3:4)(r+
req@1:2 ▷r−
val@3:4 ▷⟨Ba⟩(?wans)⟨wans⟩↑) | r+
val@3:4 ▷(?zba)⟨Ans⟩))
→(νrreq@1:2)(r−
req@1:2 ▷(?xans)⟨xans⟩↑|
(νrval@3:4)(r+
req@1:2 ▷r−
val@3:4 ▷(?wans)⟨wans⟩↑)|r+
val@3:4 ▷⟨Ans⟩))
→(νrreq@1:2)(r−
req@1:2 ▷(?xans)⟨xans⟩↑|
(νrval@3:4)(r+
req@1:2 ▷r−
val@3:4 ▷⟨Ans⟩↑)|r+
val@3:4 ▷0))
→(νrreq@1:2)(r−
req@1:2 ▷⟨Ans⟩↑|(νrval@3:4)(r+
req@1:2 ▷r−
val@3:4 ▷0|r+
val@3:4 ▷0))
Since the bank and validation service must typically handle more requests, one
can use their replicated versions !B and !V .
Now suppose several bank services B1, ..., Bn are available (together with
suitable veriﬁcation services V1, ..., Vm, possibly shared by diﬀerent banks), and
that a client wants to contact them all and be notiﬁed by email about their
answers Ansj (with j ∈[1, m]) exploiting a suitable service email. Then the
client could be written as
(C1 | · · · | Cn) >l (?xl1
any−ans)email@0.⟨xany−ans⟩
where each Ci is the request to a speciﬁc bank service, i.e. it has the form
reqi.⟨Ba⟩(?xi)⟨xi⟩↑.
3
The Control Flow Analysis
We develop a Control Flow Analysis for the close-free fragment of CaSPiS,
borrowing some ideas from [7,20]. Session primitives indeed introduce a nesting
hierarchy that resembles the ones used in Ambients-like calculi. Our analysis
uses the notion of enclosing scope, recording the current scope due to services,
sessions or pipelines. We say that P is in the scope of s@k if s@k is the immediate
enclosing service deﬁnition. Similarly for s@k, rp
s@m:k, and for l0 or l1. The aim of
the analysis is over-approximating all the possible behaviour of a CaSPiS process.
In particular, our analysis keeps track of the possible contents of scopes, in terms
of communication and service synchronizations. The result of analysing a process
P is a pair (I, R), called estimate for P, that satisﬁes the judgements deﬁned by
the axioms and rules in the upper (lower, resp.) part of Table 1. The analysis is
deﬁned in the ﬂavour of Flow Logic [21]. The ﬁrst component I gives information
on the contents of a scope. The second component R gives information about
the set of values to which names can be bound. Moreover, let σ, σ′ be scope
identiﬁers, ranged over by s@k, s@k, rp
s@m:k, l0, l1.
To validate the correctness of a proposed estimate (I, R) we state a set
of clauses operating upon judgements for analysing processes I, R |=σ P. The
judgement expresses that when P is enclosed within the scope identiﬁed by σ,
then (I, R) correctly captures the behaviour of P, i.e. the estimate is valid also
for all the states P ′, passed through a computation of P. More precisely:

78
C. Bodei, L. Brodo, and R. Bruni
– I : (⌊Nsrv⌋∪⌊Nsess⌋) ∪L →℘(⌊Nsrv⌋∪⌊Nsess⌋∪L) ∪⌊Pref⌋, where ⌊S⌋
is the set of canonical names in S, ℘(S) stands for the power-set of the set
S and ⌊Pref⌋is the set of action and service preﬁxes, deﬁned on canonical
names. Here, σ ∈I(σ′) means that the scope identiﬁed by σ′ may contain
the one identiﬁed by σ; π ∈I(σ′) means that the action π may occur in the
scope identiﬁed by σ′.
– R : ⌊N⌋→℘(⌊N⌋) is the abstract environment that maps a variable to the
set of names it can be bound to, i.e. if v ∈R(n) then n may take the value
v. We assume that for each free name n, we have that n ∈R(n). Moreover,
we write R(x1, ..., xn) as a shorthand for R(x1), ..., R(xn). Without loss of
generality, we suppose to have all the variables distinct.
Validation. Following [20], the analysis is speciﬁed in two phases. First, we check
that (I, R) describes the initial process. This is done in the upper part of Table 1,
where the clauses amount to a structural traversal of process syntax.
The clause for service deﬁnition checks that whenever a service s@k is deﬁned
in s@k.P, then the relative hierarchy position w.r.t. the enclosing scope must
be reﬂected in I, i.e. s@k ∈I(σ). Furthermore, when inspecting the content
P, the fact that the new enclosing scope is s@k is recorded, as reﬂected by the
judgement I, R |=s@k P. Similarly for service invocation x@k: the only diﬀerence
is that when x is a variable, the analysis checks for every actual value s that can
be bound to x that s@k ∈I(σ) and I, R |=s@k P. The clauses for input, output
and return check that the corresponding preﬁxes are included in I(σ) and that
the analysis of the continuation processes hold as well. There is a special rule
for pipeline input preﬁx, that allows us to distinguish it from the standard input
one. Note that the current scope has the same identiﬁer carried by the variables.
Similarly, there is a rule for output preﬁxes occurring inside the scope of a left
branch of a pipeline. The corresponding possible outputs are annotated with
the label l0. The rule for session, modelled as the ones for service, just checks
that the the relative hierarchy position of the session identiﬁer rp
s@m:k w.r.t. the
enclosing scope must be reported in I, i.e. rp
s@m:k ∈I(σ). It is used in analysing
the possible continuations of the initial process.
All the clauses dealing with a compound process check that the analysis also
holds for its immediate sub-processes. In particular, the analysis of !P and that
of (νn)P are equal to the one of P. This is an obvious source of imprecision
(in the sense of over-approximation). The clause for pipeline deserves a speciﬁc
comment. It checks that whenever a pipeline >l is met, then the analysis of
the left and the right branches is kept distinct by the introduction of two sub-
indexes l0 for the left one and l1 for the right one. This allows us to predict
possible communication over the two sides of the same pipeline. Furthermore,
the analysis contents of the two scopes must be included in the enclosing scope
identiﬁed by σ. This allows us to predict also the communications due to I/O
synchronisations, involving preﬁxes occurring inside the scope of a pipeline.
In the second phase, we check that (I, R) also takes into account the dynamics
of the process under analysis, i.e. the synchronizations due to communications,
services and pipelines. This is expressed by the closure conditions in the lower

Static Detection of Logic Flaws in Service-Oriented Applications
79
part of Table 1 that mimic the semantics, by modelling, without exceeding the
precision boundaries of the analysis, the semantic preconditions and the conse-
quences of the possible actions. More precisely, preconditions check, in terms of
I, for the possible presence of the redexes necessary for actions to be performed.
The conclusion imposes the additional requirements on I and R, necessary to
give a valid prediction of the analysed action. In the clause for Service Synch,
we have to make sure that the precondition requirements hold, i.e.:
– there exists an occurrence of service deﬁnition: s@k ∈I(σ);
– there exists an occurrence of the corresponding invocation s@m ∈I(σ′);
If the precondition requirements are satisﬁed, then the conclusions of the clause
express the consequences of performing the service synchronisation. In this case,
we have that I must reﬂect that there may exist a session identiﬁed by r+
s@m:k
inside σ and by r−
s@m:k inside σ′ , such that the contents (scopes, preﬁxes) of
s@m:k and of s@m may also be inside I(r+
s@m:k) and I(r−
s@m:k), resp..
Similarly, in the clause for I/O Synch, if the following preconditions hold:
– there exists an occurrence of output in I(rp
s@m:k);
– there exists an occurrence of the corresponding input in the sibling session
I(rp
s@m:k).
then the values sent can be bound to the corresponding input variables: i.e.,
a possible communication is predicted here. Note that the rule correctly does
not consider outputs in the form ⟨˜v⟩l0, because they possibly occur inside a left
branch of a pipeline and therefore they are not available for I/O synchronisations.
The clause for Ret Synch is similar. The return preﬁx must be included in
rp
s@m:k, in turn included in I(rp′
s′@n:q), while the corresponding input must be
included in rp′
s′@n:q, i.e. in the sibling session scope of the enclosing session.
In the clause for Pipe I/O Synch, the communication can be predicted, when-
ever the output and the pipeline input preﬁxes occur in the scope of the same
session identiﬁer (same side, too), and furthermore the output occurs in the left
branch of a pipeline, while the pipeline input occurs in the right part of the same
pipeline. Note that only pipeline input preﬁxes are considered here.
Similarly, in the clause for Pipe Ret Synch. The only diﬀerence is that the
return preﬁx must occur in the session identiﬁed by rp
s@m:k, included in the same
scope that includes the corresponding pipeline input.
Example 1. Now, we can show how the analysis works on our running example:
S
def
= B|V |C
B
def
= req@1.(?yba)val@3.⟨yba⟩(?wans)⟨wans⟩↑
V
def
= val@4.(?zba)⟨Ans⟩
C
def
= req@2.⟨Ba⟩(?xans)⟨xans⟩↑
The main entries of the analysis are reported in Fig. 4, where ∗identiﬁes the
ideal outermost scope in which the system top-level service scopes are. It is

80
C. Bodei, L. Brodo, and R. Bruni
Table 1. Analysis for CaSPiS Processes
I, R |=σ s@k.P
iﬀs@k ∈I(σ) ∧I, R |=s@k P
I, R |=σ x@k.P
iﬀ∀s@m ∈R(x) : s@k ∈I(σ) ∧I, R |=s@k P
I, R |=σ (?˜x).P
iﬀ(?˜x) ∈I(σ) ∧I, R |=σ P
I, R |=l1 (?˜xl1).P
iﬀ(?˜xl) ∈I(l1) ∧I, R |=l1 P
I, R |=σ ⟨˜x⟩.P
iﬀ∀˜v ∈R(˜x) ⟨˜v⟩∈I(σ) ∧I, R |=σ P
I, R |=l0 ⟨˜x⟩.P
iﬀ∀˜v ∈R(˜x) ⟨˜v⟩l0 ∈I(l0) ∧I, R |=l0 P
I, R |=σ ⟨˜x⟩↑.P
iﬀ∀˜v ∈R(x) ⟨˜v⟩↑∈I(σ) ∧I, R |=σ P
I, R |=σ Σi∈IπiPi
iﬀ∀i ∈I : I, R |=σ πiPi
I, R |=σ P|Q
iﬀI, R |=σ P ∧I, R |=σ Q
I, R |=σ !P
iﬀI, R |=σ P
I, R |=σ (νn)P
iﬀI, R |=σ P
I, R |=σ P >l (?˜xl1)Q iﬀl0, l1 ∈I(σ) ∧I, R |=l0 P ∧I, R |=l1 (?˜xl)Q ∧
I(l0), I(l1) ⊆I(σ)
I, R |=σ rp
s@m:k ▷P
iﬀrp
s@m:k ∈I(σ) ∧I, R |=rp
s@m:k P
(Service Synch)
s@m ∈I(σ) ∧s@k ∈I(σ′)
⇒r+
s@m:k ∈I(σ) ∧I(s@m) ⊆I(r+
s@m:k) ∧
r−
s@m:k ∈I(σ′) ∧I(s@k) ⊆I(r−
s@m:k)
(I/O Synch)
⟨˜v⟩∈I(rp
s@m:k) ∧(?˜x) ∈I(rp
s@m:k)
⇒˜v ∈R(˜x)
(Ret Synch)
⟨˜v⟩↑∈I(rp
s@m:k) ∧rp
@m:k ∈I(rp′
s′@n:q) ∧(?˜x) ∈I(rp′
s′@n:q)
⇒˜v ∈R(˜x)
(Pipe I/O Synch) ⟨˜v⟩l0 ∈I(l0) ∧(?˜xl1) ∈I(l1)
⇒˜v ∈R(˜x)
(Pipe Ret Synch)
⟨˜v⟩↑∈I(rp
s@m:k) ∧rp
s@m:k ∈I(l0)
∧(?˜xl1) ∈I(l1)
⇒˜v ∈R(˜x)
easy to check that (I, R) is a valid estimate, i.e., that I, R |=∗S, by following
the two stages explained above: we just illustrate some steps. We have indeed
that I, R |=∗S holds if I, R |=∗B, I, R |=∗V and I, R |=∗C. In particular,
I, R |=∗A holds because req@1 ∈I(∗) and I, R |=∗C because req@2 ∈I(∗).
Now, by (Service Synch), we have that req@1, req@2 ∈I(∗) implies that
r+
req@1:2 ∈I(∗) ∧I(req@1) ⊆I(r+
req@1:2) ∧
r−
req@1:2 ∈I(∗) ∧I(req@2) ⊆I(r−
req@1:2)
Furthermore, by (I/O Synch), we have that Ba ∈R(yba), because
⟨Ba⟩∈I(r−
req@1:2) (?yba) ∈I(r+
req@1:2)
Similarly, we can obtain that R(xany−ans) includes Ansj, for each j ∈[1, m].

Static Detection of Logic Flaws in Service-Oriented Applications
81
I(∗) ∋req@1, req@2, val@4
I(∗) ∋r+
req@1:2, r−
req@1:2, rval@3:4
I(req@2), I(r−
req@1:2) ∋⟨Ba⟩, (?xans), ⟨xans⟩↑
I(req@1), I(r+
req@1:2) ∋val@3, (?yba)
I(val@3), I(r−
val@3:4) ∋⟨yba⟩, (?wba), ⟨wba⟩↑
I(val@4), I(r+
va@3:4) ∋(?zba), ⟨Ans⟩
R(yba) ∋Ba, R(zba) ∋Ba
R(wans) ∋Ans, R(xans) ∋Ans
Fig. 4. Some Entries of the Example Analysis
Semantic Correctness. Our analysis is correct w.r.t. the given semantics, i.e. a
valid estimate enjoys the following subject reduction property.
Theorem 1. (Subject Reduction)
If P →Q and I, R |=σ P then also I, R |=σ Q.
This result depends on the fact that the analysis is invariant under the structural
congruence, as stated below.
Lemma 1. (Invariance of Structural Congruence) If P ≡Q and I, R |=σP
then also I, R |=σ Q.
The above results are handled and proved in the extended version of the calculus
[9]. Currently, our analysis has not been implemented yet, but it could, e.g., along
the lines of the one in [20].
Modelling the Malicious Customer. We need to model a new kind of attacker,
diﬀerent from the classical Dolev-Yao one [17], on which rely many systems for
the veriﬁcation of security protocols. A Dolev-Yao attacker acts on an open
network, where principals exchange their messages. He/she can intercept and
generate messages, provided that the necessary information is in his/her knowl-
edge, which increases while interacting with the network. Our setting calls for a
diﬀerent model, because our attacker is an accredited customer of a service that
has no control of the communication channels, apart from the ones established
by the sessions in which he/she is involved. Nevertheless, our attacker, that we
call malicious customer or M, does not necessarily follow the intended rules of
the application protocol and can try to use the functions of the service in an
unintended way, e.g., by sending messages in the right format, but with contents
diﬀerent from the expected ones. Under this regard, our attacker is less powerful
of the Dolev-Yao one: M just tries to do everything that the application does
not prevent him/her to do. More precisely, M has a knowledge made of all the
public information and increased by the messages received from the service: the
attacker can use this knowledge to produce messages to be sent to the server.
We assume M is smart enough to send only messages in the expected format
in each information exchange. We extend our framework in order to implicitly

82
C. Bodei, L. Brodo, and R. Bruni
Table 2. Analysis for CaSPiS Processes in the Presence of a Malicious Customer
I, R, K |=σ (?˜x)M.P
iﬀ(?˜x)M ∈I(σ) ∧I, R, K |=σ P
I, R, K |=σ (?˜xl)M.P
iﬀ(?˜xl)M ∈I(σ) ∧I, R, K |=σ P
I, R, K |=σ ⟨˜x⟩M.P
iﬀ∀˜v ∈R(˜x) ⟨˜v⟩M ∈I(σ) ∧I, R, K |=σ P
I, R, K |=σ ⟨˜x⟩↑M.P
iﬀ∀˜v ∈R(˜x) ⟨˜v⟩↑M ∈I(σ) ∧I, R, K |=σ P
(I/O Synch)
⟨˜v⟩a ∈I(rp
s@m:k) ∧(?˜x)b ∈I(rp
s@m:k)
⇒˜v ∈R(˜x)
a = M ⇒∀	v′ ∈K : 	v′ ∈R(˜x)
b = M ⇒˜v ∈K
(Pipe I/O Synch)
⟨˜v⟩↑a ∈I(l0) ∧(?˜xl1b) ∈I(l1)
⇒˜v ∈R(˜x)
a = M ⇒∀	v′ ∈K : 	v′ ∈R(˜xl1)
b = M ⇒˜v ∈K
(Knowledge Rule 1) v ∈Nfn ⇒v ∈K
(Knowledge Rule 2) v1, ..., vk ∈K ⇒⟨v1, ..., vk⟩∈K
(Knowledge Rule 3) ⟨v1, ..., vk⟩∈K ⇒v1, ..., vk ∈K
consider the possible behaviour of such an attacker or malicious customer. We
statically approximate the malicious customer knowledge, by representing it by
a new analysis component K. Intuitively, the clauses acting on K implicitly take
the attacker possible actions into account. The component K contains all the
free names, all the messages that the customer can receive, and all the messages
that can be computed from them, e.g. if v and v′ belong to K, then also the tuple
(v, v′) belongs to K and, vice versa, if (v, v′) belongs to K, then also v and v′
belong to K. Furthermore, all the messages in K can be sent by the customer. To
distinguish the customer actions, we annotate the corresponding preﬁxes with
M, as in πM, and we use a and b both for M or the empty annotation ϵ. As
a consequence, we need to slightly change the rules in Table 1, as shown in
Table 2, where only the more signiﬁcant rules are reported. Consider the rule
(I/O Synch). Whenever the analysis predicts that a customer may send a certain
message ˜v, the analysis predicts that the same, possibly malicious, customer can
also send every other message 	v′, obtained by synthetising the information in K,
provided that it is in the same format of ˜v. Whenever a customer may receive
an input, then the analysis predicts that the same, possibly malicious, customer
can also acquire the received message in K.
Analysis at Work. In the following, we refer to the version of CaSPiS that
includes pattern matching into the input construct, as deﬁned by the following
syntax (for a similar treatment see also [8]).
π, π′ ::=
Preﬁxes
(D1, ..., Dk)
input
⟨E1, ..., Ek⟩
output
⟨E1, ..., Ek⟩↑
return
E ::=
Terms
n
names
x
variables
D ::=
Deﬁnition Terms
E
terms
?x
deﬁnition variables

Static Detection of Logic Flaws in Service-Oriented Applications
83
Our patterns are tuples of deﬁnition terms (D1, · · · , Dk) that have to be matched
against tuples of terms (E1, · · · , Ek), upon input. Note that, at run-time, each
Ei is a closed term. Intuitively, the matching succeeds when the closed terms,
say Di, elementwise match to the corresponding terms Ei, and its eﬀect is to
bind the remaining terms Ej to the remaining variables. Actually, deﬁnition
terms in (D1, . . . , Dk) can be partitioned into closed terms to be matched and
deﬁnition variables to be bound. We make the partition above explicit, by using
the auxiliary functions Term(D1, . . . , Dk) and Var(D1, . . . , Dk). They work on
the position of deﬁnition terms within the tuples in such a way that if i ∈
Term(D1, . . . , Dk), then Di is a closed term, while if i ∈Var(D1, . . . , Dk), then
Di is a deﬁnition variable. The new semantic rules for message exchange take
pattern matching into account, e.g., the (S Sync) rule becomes:
Cr ⟨E1, ..., Ek⟩P + 
i πiPi, (D1, ..., Dk)Q + 
j πjQj  →Cr P, Q[ ˜E/ ˜D] 
if
∧i∈Term(D1,...,Dk) Ei = Di
Suppose to have the following process
(νr)(r−▷⟨A, MA⟩.P)|(r+ ▷(A, ?yB).Q)
In the input tuple (A, yB), we have that Term(A, ?yB) = 1 and Var(A, ?yB) =
2. Here the synchronisation succeeds, because the matching on the ﬁrst term A
succeeds. As a consequence, the second term yB is bound to MA in the contin-
uation process (νr)(r−▷P)|(r+ ▷Q[MA/yB]).
Extending the presented analysis in order to capture this kind of input con-
struct is quite standard (see e.g. [8,10]). We recall the new CFA rules in Table 3,
where we use R(D) as a shorthand for R(x) when D =?x. For the sake of space,
we only report some of them.
Table 3. Analysis for CaSPiS Processes
I, R |=σ (D1, ..., Dk).P iﬀ(D1, ..., Dk) ∈I(σ) ∧I, R |=σ P
I, R |=σ ⟨E1, ..., Ek⟩.P iﬀ∀v1, ..vk : vi ∈R(Ei) ⟨v1, ..vk⟩∈I(σ) ∧I, R |=σ P
(I/O Synch) ⟨v1, ..vk⟩∈I(rp
s@m:k) ∧(D1, ..., Dk) ∈I(rp
s@m:k)
∧
i∈Term(D1,...,Dk) vi = Di
⇒∧j∈Var(D1,...,Dk) vj ∈R(Dj)
We assume that, in each information exchange, a malicious customer sends
messages in the expected format and successful w.r.t. the required pattern match-
ing, as can stated by the rule (I/O Synch) in the presence of the Malicious
Customer, modiﬁed to take the pattern matching into account.

84
C. Bodei, L. Brodo, and R. Bruni
(I/O Synch) ⟨v1, ..vk⟩a ∈I(rp
s@m:k) ∧(D1, ..., Dk)b ∈I(rp
s@m:k)
∧
i∈Term(D1,...,Dk) vi = Di
⇒∧j∈Var(D1,...,Dk) vj ∈R(Dj)
...
a = M ⇒∀⟨v′
1, ..v′
k⟩a ∈K :
∧
i∈Term(D1,...,Dk) v′
i = Di
⇒∧j∈Var(D1,...,Dk) v′
j ∈R(Dj)
Price Modiﬁcation Example. We are now ready to model and analyse the exam-
ple informally introduced in Section 1. The global system is composed by two
processes put in parallel, the e-shop service S and the customer C. Also a data
base DBI storing item prices is modeled:
(S | DBI) | C
Essentially, the e-shop S allows costumers to choose among several items itemi;
when the costumer returns its selection, S asks the DBI service for the price
of the selected item. In the ﬁrst speciﬁcation, shown below, the service S does
not check if the form sent by the costumer contains the right price, i.e. the one
computed by the DBI service, because the DBI sends the price directly to
the client, without sending it also to the bank. For the sake of readability, we
distinguish the part of the output tuples relative to the order form (payment
form, resp.) by writing: order form(...) (payment form(...), resp.).
S
= !selling. 
i((itemi)(ν code)
(price db ⟨itemi⟩(?xpricei)⟨order form(code, itemi, xpricei)⟩↑
|
(ok, payment form(code, itemi, ?ypricei, ?yname, ?ycc)).PAY +
(no payment)))
DBI = !price db 
i((itemi)⟨pricei⟩)
C
= selling. ⟨itemi⟩M(order form(?z code, itemi, ?zpricei))M
⟨ok, payment form(z code, itemi, zpricei, name, cc)⟩M+
⟨no payment⟩M
In the second formulation, the service S′ matches the price sent by the customer
against the one provided by the DBI. The other processes are unmodiﬁed.
S′ = !selling. 
i((itemi)(ν code)price db. ⟨itemi⟩(?xpricei)⟨xpricei⟩↑>l
(?yl1
pricei) ⟨form(code, itemi, ypricei)⟩
(ok, payment form(code, itemi, ypricei, ?yname, ?ycc))) +
(no payment)
The main entries of the analysis of the ﬁrst formulation are reported in
Fig 5. The variable ?yprice, used by S, may be bound to any value the cos-
tumer sends, in particular to any possible faked price value. This depends on
the fact that there is no pattern matching on the values received; more gen-
erally, no control on this part of input is made. Furthermore, note that since

Static Detection of Logic Flaws in Service-Oriented Applications
85
I(selling), I(r+
sell)
∋itemi, price db
I(price db), I(r−
price) ∋⟨itemi⟩, (?xpricei), ⟨order form(code, itemi, pricei)⟩↑,
(ok, payment form(code, itemi, pricei, name, cc)),
(ok, payment form(code, itemi, faked price, name, cc)),
(no payment)
I(selling), I(r−
sell)
∋⟨item⟩M, (order form(code, itemi, pricei))M,
⟨ok, payment form(code, itemi, pricei, name, cc)⟩M
⟨no payment⟩M
I(price db), I(r+
price) ∋(itemi), ⟨pricei⟩
R(xpricei), R(zpricei) ∋pricei
R(ypricei)
∋pricei, faked price
K
∋pricei, faked price, payment form(code, itemi, pricei, name, cc),
payment form(code, itemi, faked price, name, cc)
Fig. 5. Some Analysis Entries of (S | DBI) | C
I(selling), I(r+
sell)
∋(itemi), l0
I(l0)
∋⟨itemi⟩, price db
I(price db), I(r−
price) ∋⟨itemi⟩(?xprice)⟨price⟩↑,
I(l1)
∋(?yl1
price), ⟨form(code, item, pricei)⟩,
(ok, payment form(code, itemi, pricei, ?yname, ?ycc)),
(no payment)
R(xpricei)
∋pricei
R(yl1
pricei)
∋pricei
Fig. 6. Some Analysis Entries of (S′ | DBI) | C
⟨ok, payment form(code, itemi, pricei, name, cc)⟩M belongs to I(selling) and
faked price ∈K, then ⟨ok, payment form(code, itemi, faked price, name, cc)⟩
can synchronise with the input (ok, payment form(code, itemi, pricei,name,cc)).
In the second formulation, the problem does not arise, because: (i) the DBI
sends the price to the bank that, in turn, forwards it to the client; (ii) the shop
service checks if the price returned by the costumer matches against the one
returned by the DBI component, thus avoiding the attack. The ﬁrst ﬁx has
to do with the overall design of the service. The second has to do with input
validation and, technically, is obtained by using pattern matching on the third
value received in the payment form, that should match with the price yl1
price, as
correctly predicted by the analysis of the second formulation, shown in Fig 6.
4
Conclusion
Often, component-based applications, i.e. those that are mostly implemented by
connected existing applications, as web services, only limit the analysis of their
component applications to the functionalities they actually use. A good practice
could be to consider all the functionalities an application oﬀers, before including
it in the composition, in order to check all the possible inputs the overall web
service can eventually provide and to keep trace where data-validation is applied
or is missing.

86
C. Bodei, L. Brodo, and R. Bruni
We applied a Control Flow Analysis to an example inspired by a known
logic-ﬂawed application for e-commerce, speciﬁed in service oriented calculus
as CaSPiS. Ours is a proof-of-concept work: we chose to detect the application
misuse, by analysing the static approximation of the behaviour of the system.
Our analysis can be easily specialised to capture speciﬁc properties of interest,
e.g., data integrity in this example. We do not describe the specialisation process
here, but only remark that the core of the analysis here introduced is preserved
(for a similar process, see, e.g., the analyses for the LySa calculus in [8,10]).
Our proposal shows that the chosen level of abstraction of services and of their
features is suitable to investigate logic ﬂaws. In fact, a calculus like CaSPiS ab-
stractly expresses the key aspects of service oriented computing as primitives,
without requiring any further codiﬁcation, thus allowing us to focus on the se-
curity ﬂaws that arise at the level of services, and to ignore those arising at
the level of the underlying protocols. Indeed diﬀerent forms of basic interactions
are distinguished and regulated on their own (services are globally available,
while ordinary I/O communications are context sensitive); and sessions are an
implicit mechanism for enclosing the communications between a caller and its
callee, avoiding external interferences.
References
1. Acciai, L., Boreale, M.: Type Abstractions of Name-Passing Processes. In: Arbab,
F., Sirjani, M. (eds.) FSEN 2007. LNCS, vol. 4767, pp. 302–317. Springer, Heidel-
berg (2007)
2. Acciai, L., Boreale, M.: A Type System for Client Progress in a Service-Oriented
Calculus. In: Degano, P., De Nicola, R., Meseguer, J. (eds.) Concurrency, Graphs
and Models. LNCS, vol. 5065, pp. 642–658. Springer, Heidelberg (2008)
3. Armando, A., Basin, D., Boichut, Y., Chevalier, Y., Compagna, L., Cuellar, J.,
Drielsma, P.H., He´am, P.C., Kouchnarenko, O., Mantovani, J., M¨odersheim, S., von
Oheimb, D., Rusinowitch, M., Santiago, J., Turuani, M., Vigan`o, L., Vigneron, L.:
The AVISPA Tool for the Automated Validation of Internet Security Protocols and
Applications. In: Etessami, K., Rajamani, S.K. (eds.) CAV 2005. LNCS, vol. 3576,
pp. 281–285. Springer, Heidelberg (2005)
4. Bartoletti, M., Degano, P., Ferrari, G.L., Zunino, R.: Semantics-Based Design for
Secure Web Services. IEEE Transactions on Software Engineering 34(1), 33–49
(2008)
5. Bhargavan, K., Fournet, C., Gordon, A.D.: Veriﬁed Reference Implementations of
WS-Security Protocols. In: Bravetti, M., N´u˜nez, M., Zavattaro, G. (eds.) WS-FM
2006. LNCS, vol. 4184, pp. 88–106. Springer, Heidelberg (2006)
6. Blanchet, B.: An Eﬃcient Cryptographic Protocol Veriﬁer Based on Prolog Rules.
In: Computer Security Foundations Workshop (CSFW) (2001)
7. Bodei, C., Bracciali, A., Chiarugi, D.: Control Flow Analysis for Brane Calculi.
ENTCS, vol. 227, pp. 59–75. Elsevier, Amsterdam (2009)
8. Bodei, C., Brodo, L., Degano, P., Gao, H.: Detecting and Preventing Type Flaws
at Static Time. To appear in Journal of Computer Security (2009)
9. Bodei, C., Brodo, L., Bruni, R.: Static Detection of Logic Flaws in Service Appli-
cations. Technical Report, Dipartimento di Informatica, Universit`a di Pisa (2009)

Static Detection of Logic Flaws in Service-Oriented Applications
87
10. Bodei, C., Buchholtz, M., Degano, P., Nielson, F., Nielson, H.R.: Static Validation
of Security Protocols. Journal of Computer Security 13(3), 347–390 (2005)
11. Bond, M., Clulow, J.: Extending Security Protocol Analysis: New Challenges.
ENTCS, vol. 125(1), pp. 13–24. Elsevier, Amsterdam (2005)
12. Bonelli, E., Compagnoni, A., Gunter, E.: Typechecking Safe Process Synchroniza-
tion. In: Proc. Foundations of Global Ubiquitous Computing. ENTCS, vol. 138(1),
pp. 3–22. Elsevier, Amsterdam (2005)
13. Boreale, M., Bruni, R., De Nicola, R., Loreti, M.: Sessions and Pipelines for Struc-
tured Service Programming. In: Barthe, G., de Boer, F.S. (eds.) FMOODS 2008.
LNCS, vol. 5051, pp. 19–38. Springer, Heidelberg (2008)
14. Bruni, R.: Calculi for service-oriented computing. In: Proc. of 9th International
School on Formal Methods for the Design of Computer, Communication and Soft-
ware Systems: Web Services (SFM 2009). LNCS, vol. 5569, pp. 1–41. Springer,
Heidelberg (2009)
15. Bruni, R., Mezzina, L.G.: Types and Deadlock Freedom in a Calculus of Services,
Sessions and Pipelines. In: Meseguer, J., Ro¸su, G. (eds.) AMAST 2008. LNCS,
vol. 5140, pp. 100–115. Springer, Heidelberg (2008)
16. Kitchin, D., Cook, W.R., Misra, J.: A language for task orchestration and its
semantic properties. In: Baier, C., Hermanns, H. (eds.) CONCUR 2006. LNCS,
vol. 4137, pp. 477–491. Springer, Heidelberg (2006)
17. Dolev, D., Yao, A.C.: On the Security of Public Key Protocols. IEEE TIT, IT-
29(12), 198–208 (1983)
18. Kolundzija, M.: Security Types for Sessions and Pipelines. In: Proc. of the 5th
International Workshop on Web Services and Formal Methods (WS-FM 2008).
LNCS, vol. 5387, pp. 175–189. Springer, Heidelberg (2009)
19. Nabi, F.: Secure business application logic for e-commerce systems. Computers &
Security 24(3), 208–217 (2005)
20. Nielson, F., Riis Nielson, H., Priami, C., Schuch da Rosa, D.: Control Flow Analysis
for BioAmbients. ENTCS, vol. 180(3), pp. 65–79. Elsevier, Amsterdam (2007)
21. Riis Nielson, H., Nielson, F.: Flow Logic: a multi-paradigmatic approach to static
analysis. In: Mogensen, T.Æ., Schmidt, D.A., Sudborough, I.H. (eds.) The Essence
of Computation. LNCS, vol. 2566, pp. 223–244. Springer, Heidelberg (2002)
22. OASIS Technical Commitee. Web Services Security (WS-Security) (2006)
23. Neohapsis Archives. Price modiﬁcation possible in CyberOﬃce Shopping Cart,
http://archives.neohapsis.com/archives/bugtraq/2000-10/0011.html
24. Backes, M., M¨odersheim, S., Pﬁtzmann, B., Vigan`o, L.: Symbolic and Crypto-
graphic Analysis of the Secure WS-ReliableMessaging Scenario. In: Aceto, L.,
Ing´olfsd´ottir, A. (eds.) FOSSACS 2006. LNCS, vol. 3921, pp. 428–445. Springer,
Heidelberg (2006)
25. Woo, T.Y.C., Lam, S.S.: A semantic model for authentication protocols. In: Proc.
of IEEE Symposium on Security and Privacy (1993)

Improving the Semantics of Imperfect Security
Niklas Broberg and David Sands
Chalmers University of Technology, Sweden
Information ﬂow policies that evolve over time (including, for example, declassiﬁca-
tion) are widely recognised as an essential ingredient in useable information ﬂow con-
trol system. In previous work ([BS06a, BS06b]) we have shown one approach to such
policies, ﬂow locks, which is a very general and ﬂexible system capable of encoding
many other proposed approaches.
However, any such policy approach is only useful if we have a precise speciﬁcation –
a semantic model – of what we are trying to enforce. A semantic model gives us insight
into what a policy actually guarantees, and deﬁnes the precise goals of any enforcement
mechanism. Unfortunately, semantic models of declassiﬁcation can be both inaccurate
and difﬁcult to understand. This was deﬁnitely the case for the ﬂow locks system as
presented in [BS06a, BS06b], and we have found that the main problem is one common
to most proposed models to date. We will start by discussing the problem in general,
and then go on to sketch its solution for the ﬂow locks system speciﬁcally.
The Flow Sensitivity Problem. The most commonly used semantic deﬁnition of secure
information ﬂow deﬁnes security by comparing any two runs of a system in environ-
ments that only differ in their secrets. A system is secure or noninterfering if any two
such runs are indistinguishable to an attacker. Many semantic models for declassiﬁca-
tion are built from adaptations of such a two-run noninterference condition.
Such adaptations are problematic for a number of reasons, but the most fundamental
problem is that they lack a formal and intuitive attacker model. An attacker would not
observe two runs and try to deduce something from their differences, which is what a
two-run model inherently claims.
There are also technical problems arising from the use of a two-run system. Consider
the ﬁrst point in a run at which a declassiﬁcation occurs. From this point onwards,
two runs may very well produce different observable outputs. A semantic model for
declassiﬁcation must constrain the difference at the declassiﬁcation point in some way,
and further impose some constraint on the remainder of the computation. The prevailing
approach (e.g. [MS04, EP05, EP03, AB05, Dam06, MR07, BCR08, LM08]) to give
meaning to declassiﬁcation is to reset the environments of the systems so as to restore
the low-equivalence of environments at the point after a declassiﬁcation. We refer to
this as the resetting approach to declassiﬁcation semantics.
The down-side of the resetting approach is that it is ﬂow insensitive. This implies
that the security of a program P containing a reachable subprogram Q requires that Q
be secure independently of P. Another instance of the problem is that dead code can be
viewed as semantically signiﬁcant, so that a program will be rejected because of some
insecure dead code. Note that ﬂow insensitivity might be a perfectly reasonable property
for a particular enforcement mechanism such as a type system – but in a sequential
setting it has no place as a fundamental semantic requirement.
P. Degano and L. Vigan`o (Eds.): ARSPA-WITS 2009, LNCS 5511, pp. 88–91, 2009.
c⃝Springer-Verlag Berlin Heidelberg 2009

Improving the Semantics of Imperfect Security
89
A Knowledge-based Approach. Comparing two runs is fairly intuitive for standard non-
interference, but in the presence of policy changes such as the opening or closing of
locks (in our work) or declassiﬁcation (in other work) it can be hard to see how the
semantic deﬁnition really relates to what we can say about an attacker.
One recent alternative to deﬁning the meaning of declassiﬁcation is Gradual Release
[AS07], which uses a more explicit attacker model whereby one reasons about what an
attacker learns about the initial inputs to a system as computation progresses.
The basic idea can be explained when considering the simple case of noninterference
between an initial memory state, which is considered secret, and public outputs. The
model assumes that the attacker knows the program itself P.
Suppose that the attacker has observed some (possibly empty) trace of public out-
puts t. In such a case the attacker can, at best, deduce that the possible initial state is one
of the following: K1 = {N | Running P on N can yield trace t }. Now suppose that
after observing t the attacker observes the further output u. Then the attacker knowl-
edge is K2 = {N | Running P on N can yield trace t followed by u }. For the program to
be considered noninterfering, in all such cases we must have K1 = K2. The gradual re-
lease idea weakens noninterference by allowing K1 ̸= K2, but only when u is explicitly
labelled as a declassiﬁcation event.
This style of deﬁnition is the key to our new ﬂow lock semantics. The core challenge
is then to determine what part of the knowledge must remain constant on observing the
output u by viewing the trace from the perspective of the lock-state in effect at that time.
Flow locks: the basic idea. Flow locks are a simple mechanism for interfacing between
information ﬂow policies given to data, and the code that processes that data. Security
policies are associated with the storage locations in a program. In general a policy p is a
set of clauses, where each clause of the form Σ ⇒α states the circumstances (Σ) under
which actor α may view the data governed by this policy. Σ is a set of locks, and for α
to view the data all the locks in Σ must be open.
A lock is a special variable in the sense that the only interaction between the program
and the lock is via the instructions to open or close the lock. In this way locks can be
seen as a purely compile-time entity used to specify the information ﬂow policy. We
say that x is visible to α at ∆if the policy of x contains Σ ⇒α for some Σ ⊆∆.
One aim of the ﬂow locks approach is to provide a general language into which a
variety of information ﬂow mechanisms can be encoded, to let ﬂow locks serve as a
unifying framework for various policy mechanisms. As a simple example of such an
encoding from [BS06a], consider a system with data marked with ”high” or ”low”,
and a single statement ℓ:= declassify(h) taking high data in h and downgrading
it to low variable ℓ. We can encode this using ﬂow locks by letting low variables be
marked with {high; low} and high variables with {Decl ⇒high; low}. The statement
ℓ:= declassify(h) can then be encoded with the following sequence of statements:
open Decl; ℓ:= h; close Decl.
In common with many of the approaches cited above, the previous semantic security
model for ﬂow locks is deﬁned using a resetting approach. All the problems discussed
above are thus manifested in the old ﬂow locks security model – no intuitive attacker
model leading to an imperfect notion of knowledge gained, and a ﬂow insensitive notion
of security that rules out many perfectly secure programs for purely technical reasons.

90
N. Broberg and D. Sands
A new semantics for Flow Locks. First we need a suitable attacker model. The model
given in [AS07] is very simple, to match the simple declassiﬁcation mechanism they
consider. For ﬂow locks we need a slightly more complex model, both since we deal
with multiple actors, but also since ﬂows are more ﬁne-grained, so that a secret may be
declassiﬁed for an actor through a series of steps.
To verify that a program is allowed, we need to validate the ﬂows at each ”level”
that a secret may ﬂow to, where a level corresponds to a certain set of locks guarding
a location from a given actor. We note that these levels correspond to the points in the
lattice Actors × P(Locks), which leads us to our formal attacker model: An attacker A
is a pair of an actor α and a set of locks ∆, formally A = (α, ∆) ∈Actors ×P(Locks).
We refer to the lockstate component of an attacker as his capability, and say that a
location x is visible to A = (α, ∆) iff x is visible to α at ∆.
The ﬂows that we need to validate are the assignments, so each assignment must be
registered in the “trace” that the program generates when run. An output u in the trace
is visible to attacker A if the variable being assigned to is, and an A-observable trace is
the restriction of a full trace to only include the outputs visible to A.
To reason about attacker knowledge we also need to be able to focus on the parts of
a memory which are visible to a given attacker. Given an attacker A, we say a memory
L is A-low if dom(L) = {x | A can see x}. We say that two memories M and N are
A-equivalent, written M ∼A N if their A-low projections are identical – i.e. they agree
on all variables that A can see. The knowledge gained by an attacker A from observing
a sequence of outputs ⃗w of a program c starting with a A-low memory L is then deﬁned
to be the set of all possible starting memories that could have lead to that observation:
kA(⃗w, c, L) = {M|M ∼A L, running c on M can yield A-visible trace ⃗w}
Intuitively, for a program to be ﬂow lock secure we must consider the perspective
of each possible attacker A, and how his knowledge of the initial memory evolves as
he observes successive outputs. The requirement for each output thus observed is that
knowledge of the initial memory only increases if the attacker’s inherent capabilities
are weaker than the program lockstate in effect at the time of the output. The intuition
here is that an attacker whose capability includes the program lock state in effect should
already be able to see the locations used when computing the value that is output. Thus
no knowledge should be gained by such an attacker.
For convenience we introduce the notion of a run, which is just an output trace to-
gether with the lockstate in effect at the time of the last output in the sequence:
RunA(Σ, c, L) = {(⃗ww, Ω)|M ∼A L,
running c with starting memory M and starting lockstate Σ yields
A-visible output trace ⃗ww where Ωis the lockstate in effect at output w}
Finally we can deﬁne our security requirement in terms of runs. A program c is said
to be Σ-ﬂow lock secure, written FLS(Σ, c) iff for all attackers A = (α, ∆), all A-low
memories L, and all runs (⃗ww, Ω) ∈RunA(Σ, c, L) such that Ω⊆∆we have
kA(⃗ww, c, L) = kA(⃗w, c, L)

Improving the Semantics of Imperfect Security
91
This deﬁnition directly captures the intuition that we started out with. An attacker whose
capabilities includes the current lockstate in effect at the time of the output should learn
nothing new when observing that output. Attackers who do not fulﬁll this criterion have
no constraint on what they may learn at this step. But note that this cannot lead to
unchecked ﬂows because we quantify over all attackers including, in particular, those
with sufﬁcient capabilities.
Further reading. This extended abstract accompanies an invited talk, and what we
describe here is still work in progress. The slides and latest version of a full paper are
available at: http://www.cs.chalmers.se/˜dave/ARSPA-WITS09
References
[AB05]
Almeida Matos, A., Boudol, G.: On declassiﬁcation and the non-disclosure policy.
In: Proc. IEEE Computer Security Foundations Workshop, June 2005, pp. 226–240
(2005)
[AS07]
Askarov, A., Sabelfeld, A.: Gradual release: Unifying declassiﬁcation, encryption
and key release policies. In: Proc. IEEE Symp. on Security and Privacy, May 2007,
pp. 207–221 (2007)
[BCR08]
Barthe, G., Cavadini, S., Rezk, T.: Tractable enforcement of declassiﬁcation policies.
In: Proc. IEEE Computer Security Foundations Symposium (2008)
[BS06a]
Broberg, N., Sands, D.: Flow locks: Towards a core calculus for dynamic ﬂow poli-
cies. In: Sestoft, P. (ed.) ESOP 2006. LNCS, vol. 3924, pp. 180–196. Springer, Hei-
delberg (2006)
[BS06b]
Broberg, N., Sands, D.: Flow locks: Towards a core calculus for dynamic ﬂow poli-
cies. Technical report, Chalmers University of Technology and G¨oteborgs University
(May 2006); Extended version of [BS06a]
[Dam06]
Dam, M.: Decidability and proof systems for language-based noninterference rela-
tions. In: Proc. ACM Symp. on Principles of Programming Languages (2006)
[EP03]
Echahed, R., Prost, F.: Handling harmless interference. Technical Report 82, Labora-
toire Leibniz, IMAG (June 2003)
[EP05]
Echahed, R., Prost, F.: Security policy in a declarative style. In: Proceedings of the
7th International Conference on Principles and Practice of Declarative Programming
(PPDP 2005), Lisboa, Portugal (July 2005)
[LM08]
Lux, A., Mantel, H.: Who can declassify? In: Preproceedings of the Workshop on
Formal Aspects in Security and Trust (FAST) (2008)
[MR07]
Mantel, H., Reinhard, A.: Controlling the what and where of declassiﬁcation in
language-based security. In: De Nicola, R. (ed.) ESOP 2007. LNCS, vol. 4421, pp.
141–156. Springer, Heidelberg (2007)
[MS04]
Mantel, H., Sands, D.: Controlled declassiﬁcation based on intransitive noninterfer-
ence. In: Chin, W.-N. (ed.) APLAS 2004. LNCS, vol. 3302, pp. 129–145. Springer,
Heidelberg (2004)

Analysing PKCS#11 Key Management
APIs with Unbounded Fresh Data
Sibylle Fr¨oschle1 and Graham Steel2
1 School of Informatics, University of Oldenburg, Germany
2 LSV, ENS Cachan & CNRS & INRIA, France
Abstract. We extend Delaune, Kremer and Steel’s framework for anal-
ysis of PKCS#11-based APIs from bounded to unbounded fresh data.
We achieve this by: formally deﬁning the notion of an attribute policy;
showing that a well-designed API should have a certain class of policy we
call complete; showing that APIs with complete policies may be safely
abstracted to APIs where the attributes are ﬁxed; and proving that these
static APIs can be analysed in a small bounded model such that secu-
rity properties will hold for the unbounded case. We automate analysis
in our framework using the SAT-based security protocol model checker
SATMC. We show that a symmetric key management subset of the Era-
com PKCS#11 API, used in their ProtectServer product, preserves the
secrecy of sensitive keys for unbounded numbers of fresh keys and han-
dles, i.e. pointers to keys. We also show that this API is not robust: if an
encryption key is lost to the intruder, SATMC ﬁnds an attack whereby
all the keys may be compromised.
1
Introduction
RSA Laboratories standard PKCS#11 deﬁnes Cryptoki, a general purpose API
for cryptographic devices such as smartcard security tokens and cryptographic
hardware security modules (HSMs) [8]. It has been widely adopted in industry.
As well as providing cryptographic functionality, PKCS#11 is also supposed to
enforce certain security properties. In particular, it is stated in the standard
that even if the device is connected to an untrusted machine where the operat-
ing system and device drivers might be compromised, keys marked “sensitive”
cannot be compromised [8, §7]. However, in 2003, Clulow presented a number of
attacks, i.e. sequences of valid PKCS#11 commands, which result in sensitive
keys being revealed in the clear [2]. A typical one is the so-called ‘key separa-
tion attack’. The name refers to the fact that a key may have conﬂicting roles.
Clulow gives the example of a key conﬁgured for decryption of ciphertexts, and
for ‘wrapping’, i.e. encryption of other keys for secure transport. To determine
the value of a sensitive key, the attacker calls the ‘wrap’ command and then the
‘decrypt’ command, as shown in Figure 1. Here (and subsequently) h(n1, k1) is
what we call a handle binding, i.e. a function application modelling the fact that
n1 is a handle for (or pointer to) key k1 on the device. The symmetric encryption
of k1 under key k2 is represented by the term senc(k1, k2). We model commands
P. Degano and L. Vigan`o (Eds.): ARSPA-WITS 2009, LNCS 5511, pp. 92–106, 2009.
c
⃝Springer-Verlag Berlin Heidelberg 2009

Analysing PKCS#11 Key Management APIs with Unbounded Fresh Data
93
by giving the inputs on the left of the arrow, and the output on the right. The
result of the attack is to reveal the value of k1 in the clear.
Delaune, Kremer and Steel have proposed a formal model for the operation
of the PKCS#11 API [5], which together with a model checker allowed various
conﬁgurations of the API to be examined and several new attacks to be found.
However, their model requires a bound on the number of freshly generated names,
used to model new keys and handles. This means that when the model is shown
to be secure up to the bound, we cannot be sure that there is not an attack
on a larger model of the API. Moreover, they were only able to ﬁnd secure
conﬁgurations by severely restricting the functionality of the interface - in eﬀect
restricting the update of long-term keys to an operation requiring the device
to be connected to a trusted machine. In this paper, we motivate a restriction
to a certain class of APIs, showing how APIs outside this class are likely to be
problematic because of the wrapping and unwrapping operations intrinsic to the
API. We prove theorems showing that if there is an attack on an API in our
class in the unbounded model, then there is an attack in a particular bounded
model of a size suitable for treatment with a model checker. We use version 3.0
of the SAT-based model checker for security protocols SATMC [1] to carry out
experiments on these APIs.
The rest of the paper is organised as follows. We ﬁrst deﬁne our formal model
(§2). We make explicit the idea of an attribute policy, which speciﬁes what roles
a key may have, and insist that API rules give rise to a certain class of attribute
policies, called complete policies (§3). We explain why this is a reasonable de-
mand, and show that an API with a complete policy can be mapped to an
equivalent (with respect to security) API with a static policy, i.e. one where the
attributes are non-mutable. We then show how security analysis of APIs with
static policies for unbounded numbers of keys and handles can be performed by
model checking carefully designed small bounded models (§4). Finally, we prove
by model checking the secrecy of sensitive keys for a small PKCS#11 based API,
for unbounded numbers of keys and handles (§5). This API is a subset of the
implementation of PKCS#11 used by Eracom/SafeNet in their ProtectServer
product [7]. We also show how the loss of a key with the encrypt attribute set
can lead to the loss of all the keys, thanks to an attack found by SATMC. We
show how to modify the API to prevent the attack. Thus ﬁnally we are able to
prove some robust guarantees of security for a functional subset of PKCS#11,
albeit in our symbolic model. We conclude in section 6.
2
Formal Model
In Cryptoki’s logical view a “cryptographic token” (or simply “token”) is a device
that stores objects and can perform cryptographic functions with these objects.
For the subset of Cryptoki functions that we will model, the objects will always
be cryptographic keys. Each object will be referenced by a handle, which (like
a ﬁle handle) can be thought of as a pointer to the object. Several instances
of the same object are possible. Attributes are characteristics that distinguish a

94
S. Fr¨oschle and G. Steel
Initial knowledge: The intruder knows h(n1, k1) and
h(n2, k2). The name n2 has the attributes wrap and
decrypt set whereas n1 has the attribute sensitive and
extract set.
Trace:
Wrap:
h(n2, k2), h(n1, k1) →senc(k1, k2)
SDecrypt: h(n2, k2), senc(k1, k2) →k1
Fig. 1. Decrypt/Wrap attack
particular instance of an object. They enable or restrict the way functions can be
applied to the instance of the object. For example, the instance of a key can only
be used to decrypt a ciphertext (provided by the application programmer) if the
attribute decrypt is set. The combinations of attributes that an instance of an
object is allowed to have, and how the attributes can be set and unset, depends
on the particular conﬁguration of the token. We will model this by deﬁning the
concept of an attribute policy. The API commands and their semantics will as
usual be modelled by a rewriting system over a term algebra.
Our model has two main diﬀerences to that of Delaune, Kremer and Steel
(DKS) [5]. First, justiﬁed by results therein [5, Theorem 1] we will work with
a typed term algebra, silently insisting that all APIs considered are well-moded
with respect to our types. Second, our concept of deﬁning our APIs relative to a
attribute policy is new. Following DKS [5] we will model handles by nonces, and
the reference of an object k by a nonce n by the function application h(n, k),
where h is a symbol unknown to the attacker. However, to avoid ambiguity in
informal discussions, we will call such a function application a handle binding
rather than just a handle as in [5].
2.1
Term Algebra
We consider the following types: Cipher, Key, Nonce, and HBinding where Key
and Nonce are atomic types. We assume four countably inﬁnite sets: a set N
of ground terms of type Nonce; a set XN of variables of type Nonce; a set K of
ground terms of type Key; and a set XK of variables of type Key. All other terms
are built up from the atomic terms by the following operators:
– encryption enc : Key × Key →Cipher, and
– handle binding h : Nonce × Key →HBinding.
We denote the set of terms by T , and the set of ground terms, i.e., terms that do
not contain any variables, by GT . We deﬁne the set of template terms, denoted
by T T , to be the set of terms that do not contain any subterm of N ∪K. Thus,
a template term is a term that does not contain any ground nonce or key but
may use variables to represent them. We deﬁne key terms as TK = K ∪XK, and
nonce terms as TN = N ∪XN respectively. Given a term t ∈T , let nonces(t) be

Analysing PKCS#11 Key Management APIs with Unbounded Fresh Data
95
the set of nonce terms occurring in t. We extend this notation to sets of terms
in the obvious way.
We also consider a ﬁnite set A of predicate symbols, disjoint from the other
symbols, which we call attributes. In this paper we work with the following
restricted set of attributes:
A = {encrypt, decrypt, wrap, unwrap}.
The set of attribute terms is deﬁned as
AT = {att(n) | att ∈A & n ∈TN }.
Attribute terms will be interpreted as propositions. A literal is an expression a
or ¬a where a ∈AT . Ground attribute terms and literals, and template attribute
terms and literals are deﬁned analogously to above. We denote template literals
by T L.
2.2
Attribute Policies and Attribute States
An attribute valuation is a partial function a : A →{⊥, ⊤}. If an attribute
valuation is total then it is an object state. We write S for the set of object states.
An attribute policy is a ﬁnite directed graph P = (SP , →P ) where SP ⊆S is
the set of allowable object states, and →P ⊆SP × SP is the set of allowable
transitions between the object states.
A token state is a partial function A : N →S which assigns an object state
to each nonce in dom(A), and thus to the keys represented by the nonces in
dom(A). Given an attribute policy P, we say A conforms to P iﬀran(A) ⊆SP .
Every token state induces a valuation VA for the set of attribute terms over
dom(A), deﬁned by:
VA(att(n)) =
 ⊤
if A(n)(att) = ⊤
⊥
otherwise,
VA(¬att(n)) =
 ⊤
if A(n)(att) = ⊥
⊥
otherwise.
2.3
API Rewrite Systems
We model two types of API commands: key management commands and the
command SetAttributeValue, which allows the API programmer to manipulate
the token state. The actions of the intruder will be modelled by a ﬁxed set of
intruder rules.
Syntax and informal semantics. Key management commands may generate
new objects, in which case the token state will be extended by assigning an
object state to each new object. The execution of a key management command
may be subject to whether the objects referenced have certain attributes set or
unset. Thus, formally, such commands are described by key management rules
of the form
R :
T ; L
new ˜x
−−−→T ′ ; Anew

96
S. Fr¨oschle and G. Steel
where T ⊆T T is a set of template terms, L ⊆T L is a set of template literals
such that vars(L) ⊆vars(T ), ˜x ⊆XN ∪XK is a set of key and nonce variables
such that ˜x ∩vars(T ) = ∅, T ′ ⊆T T is a set of template terms such that
vars(T ′) ⊆vars(T ) ∪˜x, and Anew is a template token state with dom(Anew) =
nonces(˜x).
The SetAttributeValue command is modelled by the parameterized attribute
rule set(n, a) where n ∈N, and a is an attribute valuation. Whether an instance
of this rule can indeed be applied at a given token state will depend on the
attribute policy.
An API rewrite system is a pair (P, R) where P is an attribute policy and R
is a set of key management rules that conform to P in the following way: every
rule that generates a new handle is parameterized by an object state a ∈SP such
that the new handle is assigned object state a in Anew. Note that the attribute
rule set(n, a) is assumed to be part of all APIs modelled here and does not need
to be speciﬁed explicitly.
Example 1. As an example consider the rewrite system (P, R) where R is given
by the rules in Figure 2, and P is deﬁned by:
– SP = {{encrypt, decrypt}, {wrap, unwrap}}, and
– →P = (SP , SP × SP ).
As in this example we often specify the allowed object states concisely as sets
of attributes rather than functions. Note that due to our concept of attribute
policy we do not need to specify rules for setting or unsetting attributes.
Intruder capability. The following two rules represent the deduction capabil-
ities of the intruder. We will denote the intruder theory by I.
I-SEncrypt :
ke, k →senc(k, ke)
I-SDecrypt :
senc(k, ke), ke →k
Note that the intruder rules can be considered to be in the same format as the
key management rules. We will make use of this when deﬁning the semantics.
Wrap : h(nw, kw), h(n, k); wrap(nw) →senc(k, kw)
For all a ∈SP :
Unwrap(a) :
h(nw, kw), senc(k, kw); unwrap(nw)
new n
−−−→h(n, k); {n →a}
KeyGenerate(a) :
new n,k
−−−−→h(n, k); {n →a}
SEncrypt :
h(ne, ke), k; encrypt(ne) →senc(k, ke)
SDecrypt :
h(ne, ke), senc(k, ke); decrypt(ne) →k
Fig. 2. PKCS#11 symmetric key management subset relative to an attribute policy P

Analysing PKCS#11 Key Management APIs with Unbounded Fresh Data
97
Semantics. Let (P, R) be an API rewrite system. A state of (P, R) is a pair
(Q, A) where Q ⊆GT is a set of ground terms, and A is a token state such that
dom(A) = nonces(Q) and A conforms to P. Given a state (Q, A) and a rule
R ∈R ∪I of the form T ; L
new ˜x
−−−→T ′, Anew, we say R can be applied at (Q, A)
under substitution θ if
1. θ is grounding for R and assigns to the variables in ˜x distinct nonces and
keys that do not occur in Q,
2. T θ ⊆Q, and
3. VA(Lθ) = ⊤.
The successor state of (Q, A) under R, θ is then deﬁned as (Q′, A′) where Q′ =
Q∪T ′θ, and A′ = A∪Anewθ. This gives rise to a transition (Q, A) ⇝R,θ (Q′, A′).
Given a state (Q, A) and an instance of the attribute rule R of the form
set(n, a), we say R can be applied at (Q, A) if
1. n ∈nonces(Q), and
2. A(n) →P a′, where a′ is deﬁned by
a′(att) =
a(att)
if att ∈dom(a),
A(n)(att)
otherwise.
The successor state of (Q, A) under R is then (Q, A′) where A′ is deﬁned by
A′(n) = a′, and A′(n′) = A(n′) for all n′ ∈dom(A) such that n′ ̸= n. This gives
rise to a transition (Q, A) ⇝R (Q′, A′).
We write (Q, A) ⇝(Q′, A′) when there is some transition from (Q, A) to
(Q′, A′). We write ⇝∗for the reﬂexive and transitive closure of ⇝. We call deriva-
tion a sequence of rule applications D = (S0, A0) ⇝(S1, A1) · · · ⇝(Sn, An).
We call (S0, A0) the initial state of D and (Sn, An) the ﬁnal state of D.
Queries. A query is a pair (T, L) where T is a set of template terms and L is
a set of template literals. A state (Q, A) satisﬁes a query (T, L) iﬀthere exists
a substitution θ grounding for (T, L) and such that T θ ⊆Q and VA(Lθ) = ⊤.
A derivation D satisﬁes a query (T, L) relative to an initial state (T0, L0) iﬀthe
initial state of D satisﬁes (T0, L0) and the ﬁnal state of D satisﬁes (T, L).
3
Towards Static Attribute Policies
An attribute policy is called static if it rules out any change of object state.
Formally, an attribute policy P = (S, →) is static if →= ∅. It is clear that,
in general, a given set of API rules is much easier to analyse and verify when
the associated attribute policy is static: in an informal analysis one does not
need to consider any side eﬀects that may arise from moving between allowable
object states; in a formal analysis one does not need to consider any attribute
set/unset rules; thereby the state space is reduced and remains monotonic. Even
though static attribute policies may seem very restrictive, we will argue that
well-designed attribute policies should satisfy a criterion we call completeness,

98
S. Fr¨oschle and G. Steel
and that complete attribute policies may be safely abstracted to static attribute
policies.
Due to limitation of space we will only consider API rewrite systems whose
rewrite rules are positive in that all the tests of attributes are positive. In a
future version of this paper we will show how our results can be carried over to
all APIs.
API rewrite systems in positive form. A rule T ; L
new ˜x
−−−→T ′, Anew is positive
if all literals l ∈L are positive. An API rewrite system (P, R) is in positive form
if all the rules in R are positive.
Given an API rewrite system in positive form we adopt the following simpli-
ﬁcations. An object state a : A →{⊤, ⊥} can be viewed as the set of attributes
{att | a(att) = ⊤}, and a token state A : TN →S as the set of attribute terms
{att(n) | n ∈dom(A) & att ∈A(n)}. The valuation induced by an attribute
state A, VA, then simpliﬁes to VA(att(n)) = ⊤if and only if att(n) ∈A. (We
never need to consider valuations of negative literals.)
One could adopt similar conventions for API rewrite systems in general. How-
ever, due to our restriction to positive form we have: for two object states a ⊆a′
the object state a′ enables at least as many commands as a, and similarly for
token states.
Complete attribute policies. In the following let R be a set of positive API
rules. Assume we wish to obtain a secure attribute policy for the commands
modelled by R. Typically we will start out with an idea of which attributes are
conﬂicting. For example, the attack in Figure 1 tells us that no object should
ever have both wrap and decrypt set. In this way we can induce the set of al-
lowable object states. It is clear from previously discovered attacks, however,
that deﬁning a safe transition relation between allowable states is non-trivial.
For example, one might try to prevent the attack in Figure 1 by disallowing
the attributes wrap and decrypt from being set on the same handle (which is
illustrated by the policy of Example 1). The attack in Figure 3 shows that this
will not suﬃce. To address this, one might decide to declare wrap and decrypt as
‘sticky’ attributes, i.e. attributes which cannot be unset. However, the fact that
Initial knowledge: The intruder knows h(n1, k1) and
h(n2, k2). The handle n2 has the attribute wrap set.
Trace:
Wrap:
h(n2, k2), h(n1, k1) →senc(k1, k2)
Unset wrap:
set(n2, [wrap →⊥])
Set decrypt:
set(n2, [decrypt →⊤])
SDecrypt:
h(n2, k2), senc(k1, k2) →k1
Fig. 3. Decrypt/Wrap attack II [9]

Analysing PKCS#11 Key Management APIs with Unbounded Fresh Data
99
the intruder can generally wrap and unwrap keys in order to obtain multiple
handles for the same key means the attack can still be performed [5, Fig. 4].
For the rest of this paper, we will consider a restricted class of policies:
Deﬁnition 1. An attribute policy P = (S, →) is complete if P consists of a
collection of disjoint, disconnected cliques, and for each clique C, c0, c1 ∈C ⇒
c0 ∪c1 ∈C.
The intuition behind this is that we do not expect to be able to prevent the
intruder from making copies of an object via the import and export mechanisms,
as in certain known attacks [5, Fig. 4]. This means that for policy P = (S, →),
∀s1, s2, s3 ∈S, if s1 →s2 and s1 →s3 are in P, then by copying a handle in
state s1, the intruder can obtain what is eﬀectively a handle for an object in
state s2 ∪s3 . A well designed policy should take this into account. We further
require the transition relation to be symmetric. We believe that the results in this
paper could be extended to relax this restriction, but observe that our current
deﬁnition has the advantage of giving a simple and intuitive rule for attribute
policy design. Of course not all complete policies will be secure: consider a trivial
policy in which all object states, including conﬂicting ones, are connected (such
as the policy of Example 1). However, complete policies are amenable to analysis,
as we will now show.
End point abstractions. Let P = (S, →) be a complete attribute policy. We
call the object states of S that are maximal in S with respect to set inclusion end
points of P, denoted by ε(P). Such object states are end points in the following
sense: once an attacker has reached an end point for an object he does not gain
anything by leaving it: any object state that can be reached from an end point
will have less enabling power.
P naturally gives rise to a static attribute policy where the allowable object
states are taken to be the end points of P. Formally, we deﬁne the end point
abstraction of P, denoted by EP(P), to be the attribute policy (ε(P), ∅). In The-
orem 1 below we prove that EP(P) provides a sound and complete abstraction
of P.
Given a ∈S, deﬁne ε(a) to be the uniquely given end point e such that a ⊆e.
Given an object state A, deﬁne ε(A) to be the object state that results when
replacing every map [n →a] ∈A by [n →ε(a)]. If a rule R ∈R generates a new
handle with object state a then ε(R) is the rule that results from R by replacing
a by ε(a). (Recall that ε(R) ∈R by deﬁnition of API rewrite systems.)
Proposition 1.
1. For all standard rules R, we have:
if (Q1, A1) ⇝R (Q2, A2) then (Q1, ε(A1)) ⇝ε(R) (Q2, ε(A2)).
2. For all attribute rule instances R, we have:
if (Q1, A1) ⇝R (Q2, A2) then ε(A1) = ε(A2) (and Q1 = Q2 as usual).
Proof. (1) is a consequence of the deﬁnition of ε(a) and our restriction to positive
rules. (2) follows since by deﬁnition of complete attribute policies if a1 →a2 is
a transition in P then ε(a1) = ε(a2).

100
S. Fr¨oschle and G. Steel
Theorem 1. If there is a derivation under P from (Q0, A0) to (Qm, Am) then
there is a derivation under EP(P) from (Q0, ε(A0) to (Qm, ε(Am)).
Conversely, if there is a derivation under EP(P) from (Q0, A0) to (Qm, Am)
then there is a derivation under P from (Q0, A0) to (Qm, Am).
Proof. To prove the ﬁrst part assume a derivation D under P. By Prop. 1 we can
transform D into a derivation under EP(P) in the following way: replace each
state (Qi, Ai) occurring in D by (Qi, ε(Ai)), and remove all attribute rule tran-
sitions from D. The converse direction is immediate since EP(P) is a subgraph
of P.
Altogether our results mean that for our class of APIs it suﬃces to analyse
security under a static attribute policy. As we will demonstrate in the next
section, together with further insights, this will lead us to reducing two variants
of the PKCS#11 API to a bounded model.
4
Reducing APIs with Static Attribute Policies to
Bounded Models
We consider the symmetric key fragment of the PKCS#11 key management API
as modelled in Figure 2, and the Eracom version of the API to be introduced
in this section. We show that for both these rewrite systems, when we assume a
static attribute policy, it is suﬃcient to work with a bounded number of freshly
generated values.
In the following we assume an API rewrite system (P, R) where P = (E, ∅) is
any static attribute policy and R will be speciﬁed in each paragraph. Derivations
will always be assumed to start from a given initial state (Q0, A0). Since we work
with static attribute policies only, the object state of every handle stays constant:
Proposition 2. Let (Q0, A0) ⇝(Q1, A1) · · · ⇝(Qm, Am) be a derivation. Let
n be a nonce, and i such that n is generated by the ith transition. Then for all
j ∈[i, m], Aj(n) is deﬁned and Ai(n) = Aj(n).
Justiﬁed by this, in the context of a derivation as above, for every nonce n
occurring in D we write A(n) for the one object state it can assume. Also note
that we no longer need to consider any attribute rule instances.
Atom substitutions. One key insight behind our reductions is that we can
eliminate freshly generated keys and handles whenever we are able to replace
them by already existing keys and handles that provide the same functionality.
Formally, we will require the concept of atom substitutions, following [6]. We use
Atoms to denote the set of ground atomic terms, i.e., N ∪K.
An atom substitution is a partial function δ : Atoms →Atoms that respects
the type of the atoms. We extend atom substitutions to sets, relations, etc. in
the usual way. Given a token state A, we say atom substitution δ, is deﬁned for
A if for all n1, n2 ∈dom(A) we have:

Analysing PKCS#11 Key Management APIs with Unbounded Fresh Data
101
1. n1 →n2 ∈δ & n2 ̸∈dom(δ) =⇒A(n1) = A(n2), and
2. n1 →n, n2 →n ∈δ for some n
=⇒A(n1) = A(n2).
Proposition 3. If A is a token state and δ is an atom substitution deﬁned for
A then we have:
1. A δ is a token state with dom(A δ) = dom(A) δ, and
2. for all literals l, if VA(l) is deﬁned then VAδ(lδ) is deﬁned and VA(l) =
VAδ(lδ).
The following two propositions show that atom substitutions preserve rule ap-
plications as well as queries in a natural way. Both propositions are routine to
prove by Prop. 3 and inspection of the deﬁnitions.
Proposition 4. Let R be a rule of the form T ; L
new ˜x
−−−→T ′, Anew. If (Q, A) ⇝R,θ
(Q′, A′) and δ is an atom substitution deﬁned for A such that θ(˜x) ∩(dom(δ) ∪
ran(δ)) = ∅then we have (Q, A)δ ⇝R,θδ (Q′, A′)δ.
Proposition 5. If a state (Q, A) satisﬁes a query (T, L) by a substitution θ,
and δ is an atom substitution deﬁned for A, then (Q, A)δ satisﬁes (T, L) by θδ.
PKCS#11 rewrite system. Consider the symmetric key fragment of the
standard PKCS#11 API as modelled in Figure 2. There are only two rules that
generate fresh values: KeyGenerate and Wrap.
It is intuitive that a pair k, n generated by KeyGenerate with, say, the ob-
ject state of n set to ε provides the same functionality as any other pair k′,
n′ generated by KeyGenerate with n′ set to the same object state ε. Thus, it
is plausible that to check whether a query is satisﬁed we need to consider at
most one instance of KeyGenerate for each allowable object state. The following
proposition gives us the means to successively delete instances of KeyGenerate
from any derivation until we arrive at a derivation that is bounded in this way.
Proposition 6. Assume a derivation
(Q0, A0) · · · ⇝Ri (Qi, Ai) · · · ⇝Rj (Qj, Aj) · · · ⇝Rn (Qn, An)
such that for some ni, nj, ki, kj,
1. Ri is an instance of KeyGenerate producing h(ni, ki),
2. Rj is an instance of KeyGenerate producing h(nj, kj), and
3. A(ni) = A(nj).
Then
(Q0, A0) · · · ⇝Ri (Qi, Ai) · · · ⇝Rj−1 (Qj−1, Aj−1) ⇝Rj+1δ (Qj+1, Aj+1)δ · · ·
· · · ⇝Rnδ (Qn, An)δ
is a derivation, where δ = [nj →ni, kj →ki].

102
S. Fr¨oschle and G. Steel
Proof. This follows by Prop. 4 when considering that (Qj−1, Aj−1) = (Qj, Aj)δ,
and that δ satisﬁes the conditions of Prop. 4 with respect to each transition
⇝Rm, m ≥j.
It can be read from the rules that, given a key k, whenever two handles for k, say
n1 and n2, have the same object state then they can be employed in exactly the
same way. Hence, it is plausible that for each key we need at most one handle per
allowable object state. The following proposition gives us the means to eliminate
instances of Unwrap until we arrive at a derivation that is reduced in this way.
Proposition 7. Assume a derivation
(Q0, A0) · · · ⇝Ri (Qi, Ai) · · · ⇝Rn (Qn, An)
such that for some n1, n2, k
1. h(n1, k) ∈Qi−1 , and
2. Ri is an instance of Unwrap producing h(n2, k) with A(n1) = A(n2).
(Q0, A0) · · · ⇝Ri−1 (Qi−1, Ai−1) ⇝Ri+1δ (Qi+1, Ai+1)δ · · · ⇝Rnδ (Qn, An)δ
is a derivation, where δ = [n2 →n1].
Proof. This follows similarly to Prop. 6.
Altogether we obtain:
Lemma 1. If there is a derivation of a query (T, L) then there is a derivation
D′ of the same query such that, in D′, the following holds:
1. for all pairs k1, n1 and k2, n2 generated by KeyGenerate, k1 ̸= k2 implies
A(n1) ̸= A(n2);
2. for every key k and all handles n1, n2 with bindings h(n1, k), h(n2, k), n1 ̸=
n2 implies A(n1) ̸= A(n2).
Proof. The lemma easily follows by successively applying the above two propo-
sitions and because by Prop. 5 the transformations preserve the query.
The lemma implies that for the PKCS#11 rewrite rules we can reduce the static
model to a bounded model. We also make use of the fact that in a model with
no disequalities on keys, an attacker is as powerful when given one key (without
a handle) in his initial knowledge as when given many such keys.
Theorem 2. In the PKCS#11 rewrite system with attribute policy (E, ∅), if
there is a derivation of a query (T, L) then there is a derivation of the same
query using
1. at most 1 + |E| keys: at most 1 key in Q0, and at most |E| keys generated by
KeyGenerate, and
2. at most |E| × (1 + |E|) handles.

Analysing PKCS#11 Key Management APIs with Unbounded Fresh Data
103
For all e ∈E:
KeyGenerate(e) :
new n,k
−−−−→h(n, k); e(n)
Wrap(e) :
h(nw, kw), h(n, k); wrap(nw), e(n)
new km
−−−−→enc(k, kw), enc(mk, kw), hmackm(k, e)
Unwrap(e) :
h(nw, kw), enc(k, kw), enc(km, kw), hmackm(k, e); unwrap(kw)
new n
−−−→h(n, k); e(n)
SEncrypt :
h(ne, ke), k; encrypt(ne) →enc(k, ke)
SDecrypt : h(ne, ke), enc(k, ke); decrypt(ke) →k
Fig. 4. Symmetric Key Management subset of the Eracom PKCS#11 API. e(n) is a
short-hand for “n is in object state e”.
Eracom rewrite system. Consider the API rules given in Figure 4, derived
from the symmetric key management commands of the Eracom ProtectServer
[7]. Here, an HMAC is used to bind the attributes to the wrapped key, to prevent
an attacker from re-importing several copies of a key with diﬀerent attributes.
Formally, we have the new type mac and the function symbol hmac : Key ×
Key × att →mac. Although the proof of Lemma 1 carries over to this set of
rules, Theorem 2 no longer holds, since the wrap rule gives an additional way of
generating fresh keys (the key to be used as MAC-key). However, we can recover
the result with a simple abstraction: if there is an attack, then there is an attack
where the MAC-key generated by the wrap command is constant:
Proposition 8. Given a derivation under the rules of Figure 4, we can map in
the obvious way to a derivation under rules that are like those of Figure 4 apart
from that the wrap rule always uses a constant mac-key mK.
This gives us an abstraction, not an equivalence. It may in theory lead to false
attacks, but it is sound for proofs since in our queries we have no disequalities
on keys. In this model, we have:
Theorem 3. In the abstracted Eracom rewrite system with attribute policy
(E, ∅), if there is a derivation of a query (T, L) then there is a derivation of
the same query using
1. at most 1 + |E| + 1 keys:
at most 1 key in Q0,
at most |E| keys generated by KeyGenerate, and
at most 1 key used by Wrap; and
2. at most |E| × (2 + |E|) handles.
In fact we can recover an exact version (i.e. without the abstraction) of Theo-
rem 2 for the Eracom rewrite system involving a slightly larger number of keys
and handles, but due to lack of space we leave this for a future longer version of
the paper.

104
S. Fr¨oschle and G. Steel
5
Experiments
Having established the validity of model checking a small bounded model of our
Eracom-based PKCS#11 API (Fig. 4), we can now investigate security proper-
ties for unbounded keys and handles. We assume an endpoint attribute policy
P = ({ed, wu}, ∅), where ed represents encrypt and decrypt and wu represents
wrap and unwrap. All keys are assumed to be sensitive. SATMC includes the
usual rules for encryption and decryption by known keys. First we investigate
the stated property required of PKCS#11 in the speciﬁcation [8, §7].
Deﬁnition 2. A known key is a key k such that the intruder knows the plaintext
value k and the intruder has a handle h(n, k).
Property 1. If an intruder starts with no known keys, he cannot obtain any
known keys.
In the API of Figure 4, this property is veriﬁed by SATMC in 0.4 seconds.
A further desirable property of such an API is that if a session key (i.e. an
encryption/decryption key) is lost to the intruder by some means beyond the
scope of the model, then no further keys are compromised.
Property 2. If an intruder starts with a known key ki with handle h(ni, ki), and
ed(ni) is true, then he cannot obtain any further known keys.
Unfortunately this does not hold for the Eracom-based API. An attack is found
by SATMC in 0.4 seconds. We give the attack in Figure 5. The intruder ﬁrst
wraps his key ki, then fakes an HMAC for it using ki as the MAC key. This allows
him to re-import ki as a wrap/unwrap key. One way to prevent this attack is
to add the wrapping key inside the HMAC (see Figure 6). SATMC veriﬁes this
property in about 0.5 seconds.
Full details of our model checking experiments, including all relevant model
ﬁles, are available at http://www.lsv.ens-cachan.fr/~steel/pkcs11/.
Initial knowledge: Handles h(n1, k1), h(n2, k2), and h(ni, ki). Key ki. Attributes
ed(n1), wu(n2), ed(ni). The HMAC key is k3.
Trace:
Wrap: (ed)
h(n2, k2), h(ni, ki) →senc(ki, k2), senc(k3, k2), hmack3(ki, ed)
Unwrap: (wu) h(n2, k2), senc(ki, k2), senc(ki, k2), hmacki(ki, wu) →h(n2, ki)
Wrap: (ed)
h(n2, ki), h(n1, k1) →senc(k1, ki), senc(k3, ki), hmack3(k1, ed)
Decrypt:
ki, senc(k1, ki) →k1
Fig. 5. Lost session key attack

Analysing PKCS#11 Key Management APIs with Unbounded Fresh Data
105
Wrap :
h(x1, y1), h(x2, y2); wrap(x1), A(x2)
new mk
−−−−→enc(y2, y1), enc(mk, y1), hmacmk(y2, A, y1)
Unwrap :
h(x1, y2), enc(y1, y2), enc(xm, y2), hmacxm(y1, A, y2); unwrap(x1)
new n1
−−−−→h(n1, y1); A(n1)
Fig. 6. Revised Wrap/Unwrap Mechanism for the Eracom API
6
Conclusions
We have presented our framework for analysing PKCS#11 based APIs in an
unbounded model. We described an attack on a version of the API used by Era-
com, discovered using our model and the model checker SATMC. We suggested
a ﬁx and proved the secrecy of sensitive keys for the ﬁxed version of the API,
for unbounded numbers of keys, handles and command calls. An extension to
asymmetric cryptography is our ﬁrst priority, and then experiments with further
proprietary implementations of PKCS#11.
We have explained how our work extends previous work by Delaune, Kremer
and Steel [5]. There have been other attempts to analyse PKCS#11, but these
were also for bounded models [9,10], and included further approximations such
as monotonic global state. With our endpoint abstractions, we also obtain a
monotonic global state, however we have formally justiﬁed this in the presence
of a complete attribute policy. In fact, our analysis suggests that robust attribute
policies can be reduced to static ones. There have also been proofs of security
for other APIs, such as revised versions of the IBM Common Cryptographic
Architecture, again in bounded models [3,4]. We plan to adapt our method to
this API.
References
1. Armando, A., Compagna, L.: SAT-based model-checking for security protocols
analysis. Int. J. Inf. Sec. 7(1), 3–32 (2008), http://www.ai-lab.it/satmc; Cur-
rently developed under the AVANTSSAR project, http://www.avantssar.eu
2. Clulow, J.: On the security of PKCS#11. In: Walter, C.D., Ko¸c, C¸.K., Paar, C.
(eds.) CHES 2003. LNCS, vol. 2779, pp. 411–425. Springer, Heidelberg (2003)
3. Cortier, V., Keighren, G., Steel, G.: Automatic analysis of the security of XOR-
based key management schemes. In: Grumberg, O., Huth, M. (eds.) TACAS 2007.
LNCS, vol. 4424, pp. 538–552. Springer, Heidelberg (2007)
4. Courant, J., Monin, J.-F.: Defending the bank with a proof assistant. In: Proceed-
ings of the 6th International Workshop on Issues in the Theory of Security (WITS
2006), Vienna, Austria, March 2006, pp. 87–98 (2006)
5. Delaune, S., Kremer, S., Steel, G.: Formal analysis of PKCS#11. In: Proceedings
of the 21st IEEE Computer Security Foundations Symposium (CSF 2008), Pitts-
burgh, PA, USA, pp. 331–344. IEEE Computer Society Press, Los Alamitos (2008)

106
S. Fr¨oschle and G. Steel
6. Fr¨oschle, S.: The insecurity problem: Tackling unbounded data. In: Proceedings of
the 20th IEEE Computer Security Foundations Symposium (CSF 2007), Venice,
Italy, pp. 370–384. IEEE Computer Society Press, Los Alamitos (2007)
7. Krhovj´ak, J.: PKCS #11 based APIs. Talk given at the Analysis of Security APIs
Workshop (ASA-1) (July 2007),
http://homepages.inf.ed.ac.uk/gsteel/asa/slides/
8. RSA Security Inc., v2.20. PKCS #11: Cryptographic Token Interface Standard
(June 2004)
9. Tsalapati, E.: Analysis of PKCS#11 using AVISPA tools. Master’s thesis, Univer-
sity of Edinburgh (2007)
10. Youn, P.: The analysis of cryptographic APIs using the theorem prover Otter.
Master’s thesis, Massachusetts Institute of Technology (2004)

Transformations between Cryptographic
Protocols⋆
Joshua D. Guttman
The MITRE Corporation
Abstract. A transformation F between protocols associates the mes-
sages sent and received by participants in a protocol Π1 with messages
sent and received in some Π2. Transformations are useful for modeling
protocol design, protocol composition, and the services that protocols
provide.
A protocol transformation determines a map from partial behaviors
A1 of Π1—which we call “skeletons”—to skeletons F(A1) of Π2. Good
transformations should act as functors, preserving homomorphisms
(information-preserving maps) from one Π1-skeleton to another. Thus,
if H : A1 →A2 is a homomorphism between Π1-skeletons, then there
should be a homomorphism F(H): F(A1) →F(A2) between their im-
ages in Π2.
We illustrate protocol transformation by examples, and show that our
deﬁnition ensures that transformations act as functors.
1
Introduction
A protocol transformation F from a protocol Π1 to a protocol Π2 maps message
transmissions and receptions of roles of Π1 to transmissions and receptions of
roles of Π2. F may be used to show how Π1 and Π2 exhibit one of a number of
relations, among them:
– Π1 may be a choreography—typically deﬁned without cryptography—which
the cryptography protocol Π2 is intended to implement faithfully, despite
the presence of malicious parties;
– Π1 may be a stage in designing the fuller protocol Π2;
– Π1 may be a simpliﬁcation of an implemented protocol Π2 [12]; or
– Π2 may be a composition of Π1 with other protocols [2,8,10].
In this paper, our goals are to give:
1. a deﬁnition of transformations ﬂexible enough for these purposes, which
requires us to avoid giving an excessively syntactic criterion;
2. examples to show how it accommodates these purposes; and
3. three key results ensuring that protocol transformations are well-behaved.
⋆Supported by the MITRE-Sponsored Research program.
P. Degano and L. Vigan`o (Eds.): ARSPA-WITS 2009, LNCS 5511, pp. 107–123, 2009.
c
⃝Springer-Verlag Berlin Heidelberg 2009

108
J.D. Guttman
Skeletons and the homomorphisms between skeletons are central to our ap-
proach. We have previously used them to develop an algorithm for protocol
analysis [6]; to prove this algorithm covers all the possibilities [5]; and to give
a criterion ensuring that a protocol composition will respect the security goals
achieved by the protocols being combined [8].
A skeleton A describes the behavior of the regular (non-adversarial) partici-
pants in some set of executions. It contains some regular behavior; the executions
A describes must also contain this behavior. Transmission and reception events
are related by a partial order expressing causal precedence. The skeleton may
stipulate that some keys are assumed uncompromised, and that some keys or
other (nonce-like) values are freshly generated. Some skeletons are realized in the
sense that they contain all the regular behavior needed for a complete execution.
This means that for every message that the skeleton says was delivered, either
this message should itself have been previously transmitted, or else the adversary
can synthesize it using its own creations and messages that were previously sent.
Naturally, adversary behavior must respect the freshness and noncompromise
assumptions of the skeleton. A protocol transformation F determines a map
lifting skeletons A of Π1 to skeletons F(A) of Π2.
A homomorphism is an information-preserving map H : A0 →A1 between
skeletons of one protocol. The executions that a skeleton A0 describes are all
the realized skeletons Ar such that, for some homomorphism H, H : A0 →Ar.
An information-preserving map H : A0 →A1 cannot increase (loosen) the set
of executions described; instead, H’s target A1 should describe a subset of the
executions described by its source A0. This follows from the fact that the com-
position of homomorphisms is a homomorphism.
The three theorems of this paper state:
1. A transformation acts as a functor (Thm. 1). If H is a homomorphism
H : A0 →A1 on skeletons of Π1, F should determine a (unique) homo-
morphism between their images in Π2, i.e. F(H): F(A0) →F(A1).
2. F, regarded as a functor on homomorphisms, does not introduce new ones
(Thm. 2). If G: F(A0) →F(A1) is a homomorphism on Π2 skeletons that
are the images of Π1 skeletons A0, A1, then G = F(H) for some H : A0 →A1.
3. If B is any Π2 skeleton, then there is a (unique) maximum A1 such that
F(A1) may be mapped injectively into B (Thm. 3). A1 deﬁnes the maximum
Π1 content that is contained in B.
In all of these results, uniqueness is to within isomorphism.
Our criterion on transformations F involves, besides a few bookkeeping con-
straints, three main requirements. First, suppose a parameter of a role ρ1 of
Π1 originates on its image F(ρ1). This means that it is transmitted without
having previously been received. Then this parameter should also have origi-
nated on ρ1. Second, is an analogous condition on parameters of a role ρ1 of Π1
that are simply transmitted on its image F(ρ1), rather than being used only as
keys for encryption or decryption. Finally, suppose that two roles ρ0, ρ1 of Π1
have a substitution instance s in common. Then their images F(ρ0), F(ρ1) in Π2

Transformations between Cryptographic Protocols
109
should have in common the image F(s). This condition is necessary to ensure
that F(ρi) does not commit to one branch of a branching behavior while ρ0 and
ρ1 are uncommitted.
Section 2 presents a variety of examples. In Section 3, we pause to summarize
the current strand space treatment of the algebra of messages, skeletons, and
homomorphisms, following [6,8]. Section 4 presents the main deﬁnition and shows
that F(A) is well determined, and Section 5 shows that homomorphisms are
preserved. Section 6 mentions some key related work and concludes.
2
Some Examples
We discuss here several variants of a transaction in which a principal A answers a
question asked by another principal B. First, in the choreography description [1],
Fig. 1, A starts by sending a message saying that he is ready; B responds with a
query message containing the parameter Q, representing the question; and then
A branches. Either A decides to answer the question aﬃrmatively, and sends the
constant yes, or else A decides to answer the question negatively, and sends the
constant no.
The local behaviors of the individual parties are of four kinds. Two of them,
the answerer A’s behaviors C1, C2, are shown in Fig. 2. We refer to the lo-
cal sequence of actions of one principal in one run of the protocol—possibly
incomplete—as a strand. The questioner B’s strands C3, C4 are dual, interchang-
ing message sends and receives. In C1, the ﬁnal message transmission is the af-
ﬁrmative form yes while in C2, it is no. In either case, the query can carry any
value of the parameter Q. A strand which is only partly complete—because only
the ﬁrst step or the ﬁrst two steps have occurred—is the same whether it is a C1
behavior or a C2 behavior. The execution has not yet committed to one branch
or the other if it has not yet performed the last step. When one observes only
A
rdy


B

•
 













•
qry Q

 













•
yes

•
no

•
•
Fig. 1. A Question-Answering Choreography
C1 :
A

	
•
 •
	
C2 :
A

	
•
 •
	
rdy
qry Q

yes
rdy
qry Q

no
Fig. 2. Question-Answering Choreography: A’s Parametric Strands

110
J.D. Guttman
A
{|R ˆ A|}B


B

•
 













•
{|R ˆ Q ˆ Y ˆ N ˆ B|}A

 













•
Y

•
N

•
•
Fig. 3. Question-Answering Cryptoprotocol
the ﬁrst two nodes of any of these behaviors, one cannot distinguish whether the
last node will be aﬃrmative or negative.
The matching behaviors C3, C4 are not shown since they are simply the duals.
A may later bill B a fee for this service. With so valuable a service, there are
of course security considerations. A may not want to disclose to an adversary
sniﬃng the network the answer to B’s question; B may not want to disclose what
question he is asking; and both parties may wish to ensure that the adversary
cannot alter B’s question Q to a diﬀerent question Q′, or alter A’s answer from
aﬃrmative to negative or the reverse. To defeat these (standard) attacks, we may
implement our choreography using a cryptographic protocol (Fig 3). Here, each
principal is assumed to know a public (asymmetric) encryption key to which
only the partner holds the matching decryption key; we write t encrypted in a
form that only principal P can read as {|t|}P . The parameters R, Y, N are nonces,
i.e. randomly chosen bitstrings that are extremely unlikely to be used more than
once. The last message is unencrypted. A transmits the ﬁrst of the two nonces
Y and N created by B to indicate the answer yes, and the second to indicate
the answer no. This conveys the answer to B, while conveying nothing whatever
to any principal that did not prepare {|RˆQˆY ˆN ˆB|}A and cannot decrypt
it. The nonces themselves are perfectly arbitrary.
Each local behavior of the principal A is summarized in one of the strands in
Fig. 4. Each one has the parameters A, B, R, Q, Y, N, and the instances of each
strand are generated by substituting each possible name, nonce, or question for
these parameters, respecting types. Again, the strands P3, P4 of B are dual.
Looking at the strands of Figs. 2, 4, we can see that we could perform a map-
ping in either direction. One could map the transmission and reception events of
the cryptoprotocol strands to send and receive events in the choreography, dis-
carding superﬂuous parameters. In this scheme, each jth node of Pi would map to
P1 :
A

	
•
 •
	
P2 :
A

	
•
 •
	
t1
t2

Y
t1
t2

N
t1 = {|RˆA|}B
t2 = {|RˆQˆY ˆN ˆB|}A
Fig. 4. Question-Answering Cryptoprotocol: Parametric Strands

Transformations between Cryptographic Protocols
111
A
 













B
qry Q

 













•
yes

•
no

•
•
C′
1 :
A
 •
	
C′
2 :
A
 •
	
qry Q

yes
qry Q

no
Fig. 5. A Question-Answering Service and Its Strands
the jth node of Ci. Alternatively, one could map the events of the choreography
to events on strands of the cryptoprotocol, allowing the additional parameters
to be freely chosen. In this scheme, each jth node of Ci would map to the jth
node of Pi.
In either case, we will map nodes on answerer strands at one level to nodes
on answerer strands at the other level, and questioner nodes to questioner nodes
at the other level. We will map aﬃrmative nodes at one level to aﬃrmative
nodes at the other, and likewise for negative nodes. Along each strand, each
successive transmission or reception must be associated with the corresponding
transmission or reception of the strand at the other level. Thus, our only choice
is whether to map from Pi to the Ci, or vice versa.
This example is, however, peculiar in that the two protocols have corresponding
strands of the same length. Consider next the choreography of Fig. 5. It is more
natural than the choreography of Fig. 1, since the questioner acts as client and the
answerer acts as server;the rdy messageat the beginning of Fig. 1 inverts the client-
server roles. However, in Fig. 5, we can no longer map the jth node of C′
i to the jth
node of Pi. Instead, it correspondsto the j+1st node of Pi. For this same reason, we
cannot speak of a transformationin the other direction; since the Ci →Pi mapping
is not surjective, the Pi →Ci mapping would not be total.
Moreover, we can also think of protocol transformations where both source
and target protocols involve cryptography. For instance, we can regard the cryp-
toprotocol of Fig. 3 as derived from the choreography of Fig. 5 by a succession
of steps that progressively introduce suitable cryptography. As a ﬁrst step, we
may introduce the ﬁrst readying message of Fig. 3, with the nonce R returned
piggybacked with the question Q. We include these two elements within an en-
cryption so that an adversary cannot reassociate the nonce R with a diﬀerent
question Q′. Thus, we obtain the intermediate form given in Fig. 6. The ﬁrst
two messages provide an “outgoing test” [6] that allows A to authenticate this
question Q as having been submitted by B. If the private decryption keys of A, B
are uncompromised, then only B could liberate R from {|RˆA|}B, and moreover
no third party could liberate R from any other message {|RˆQ′ˆB|}A. Thus, no
third party could have repackaged R with the Q that A actually received.

112
J.D. Guttman
A
{|R ˆ A|}B


B

•
 













•
{|R ˆ Q ˆ B|}A

 













•
yes

•
no

•
•
Fig. 6. From Choreography to Cryptoprotocol, Step 1
A
{|R ˆ A|}B


B

•
 













•
{|R ˆ Q ˆ Y ˆ B|}A

 













•
Y

•
no

•
•
Fig. 7. From Choreography to Cryptoprotocol, Step 2
•

{|N|}K 
{|N|}K •

•
N

•
N

Fig. 8. The Outgoing (Nonce) Authentication Test
Using the outgoing test idea again, B can supply a new nonce Y within
the second message {|RˆQˆY ˆB|}A; A may liberate this nonce to indicate an
aﬃrmative answer to Q (Fig 7). Repeating this idea with another nonce N for
the negative case completes the protocol design process for Fig. 3.
Indeed, we can express the abstract schema of an “outgoing test” [6], in which
a nonce is transmitted in encrypted form and received back later outside of that
encryption, as a sort of germ protocol (Fig. 8). It allows the sender to infer
that either the decryption key is compromised, or else a regular participant
has received the encrypted unit and transformed its content. If we perform a
renaming here, mapping N to R and K to pubk(B), then we can map the left
strand to the ﬁrst two nodes on the left of Fig. 6 and the right strand to the
ﬁrst two nodes on the right. This appears to show Fig. 6 as a sort of “join”
of the choreography of Fig. 5 with an instance of the outgoing test protocol.
We can now repeat this process with a diﬀerent renaming of Fig. 8 to produce
Fig. 7 as a successive join, producing the ﬁnal cryptoprotocol in a similar third
step. These maps appear to characterize how the protocol of Fig. 4 achieves its
goals.

Transformations between Cryptographic Protocols
113
3
Messages, Protocols, Skeletons
In this section, we provide an overview of the current strand space framework;
cf. [8] or the extended version of [6] for more detail. Let A0 be an algebra equipped
with some operators and a set of homomorphisms η: A0 →A0. We call members
of A0 atoms.
For the sake of deﬁniteness, we will assume here that A0 is the disjoint union
of inﬁnite sets of nonces, atomic keys, names, and texts. The operator sk(a) maps
names to (atomic) signature keys, and K−1 maps an asymmetric atomic key to
its inverse, and a symmetric atomic key to itself. Homomorphisms η are maps
that respect sorts, and act homomorphically on sk(a) and K−1.
Let X be an inﬁnite set disjoint from A0; its members—called indetermi-
nates—act like unsorted variables. A is freely generated from A0 ∪X by two
operations: encryption {|t0|}t1 and tagged concatenation tag t0ˆt1, where the
tags tag
are drawn from some set TAG. For a distinguished tag nil, we write
nil
t0ˆt1 as t0ˆt1 with no tag. In {|t0|}t1, a non-atomic key t1 is a symmetric
key. Members of A are called messages.
A homomorphism α = (η, χ): A →A consists of a homomorphism η on atoms
and a function χ: X →A. It is deﬁned for all t ∈A by the conditions:
α(a) = η(a),
if a ∈A0
α({|t0|}t1) = {|α(t0)|}α(t1)
α(x) = χ(x),
if x ∈X
α(tag t0ˆt1) = tag α(t0)ˆα(t1)
Thus, atoms serve as typed variables, replaceable only by other values of the same
sort, while indeterminates x are untyped. Indeterminates x serve as blank slots,
to be ﬁlled by any χ(x) ∈A. Indeterminates and atoms are jointly parameters.
This A has the most general uniﬁer property, which we will rely on. That is,
suppose that for v, w ∈A, there exist α, β such that α(v) = β(w). Then there
are α0, β0, such that α0(v) = β0(w), and whenever α(v) = β(w), then α and β
are of the forms γ ◦α0 and γ ◦β0.
Messages are abstract syntax trees in the usual way:
1. Let ℓand r be the partial functions such that for t = {|t1|}t2 or t = tag t1ˆt2,
ℓ(t) = t1 and r(t) = t2; and for t ∈A0, ℓand r are undeﬁned.
2. A path p is a sequence in {ℓ, r}∗. We regard p as a partial function, where
⟨⟩= Id and cons(f, p) = p ◦f. When the rhs is deﬁned, we have: 1. ⟨⟩(t) = t;
2. cons(ℓ, p)(t) = p(ℓ(t)); and 3. cons(r, p)(t) = p(r(t)).
3. p traverses a key edge in t if p1(t) is an encryption, where p = p1⌢⟨r⟩⌢p2.
4. p traverses a member of S if p1(t) ∈S, where p = p1⌢p2 and p2 ̸= ⟨⟩.
5. t0 is an ingredient of t, written t0 ⊑t, if t0 = p(t) for some p that does not
traverse a key edge in t.
6. t0 appears in t, written t0 ≪t, if t0 = p(t) for some p.
Strands and origination. A single local session of a protocol at a single
principal is a strand, containing a linearly ordered sequence of transmissions and
receptions that we call nodes. In Figs. 2, 4, and 5, the vertical columns of nodes
connected by double arrows ⇒are strands.

114
J.D. Guttman
We write s ↓i for the ith node on strand s, using 1-based indexing. We write
n ⇒m when n, m are successive nodes on the same strand, i.e. when for some
s, i, n = s ↓i and m = s ↓i + 1. We write msg(n) for the message sent or
received on the node n.
A message t0 originates at a node n1 if (1) n1 is a transmission node; (2)
t0 ⊑msg(n1); and (3) whenever n0 ⇒+ n1, t0 ̸⊑msg(n0).
Thus, t0 originates when it was transmitted without having been either re-
ceived or transmitted previously on the same strand. Values assumed to originate
only on one node in an execution—uniquely originating values—formalize the
idea of freshly chosen, unguessable values. Values assumed to originate nowhere
may be used to encrypt or decrypt, but are never sent as message ingredients.
They are called non-originating values. For a non-originating value K, K ̸⊑t
for any transmitted message t. However, K ≪{|t0|}K ⊑t possibly, which is why
we distinguish ⊑from ≪. See [11,6] for more details.
In the tree model of messages, to apply a homomorphism, we walk through,
copying the tree, but inserting α(a) every time an atom a is encountered, and
inserting α(x) every time that an indeterminate x is encountered.
Protocols. A protocol Π is a ﬁnite set of strands, representing the roles of the
protocol. Four of the roles of the yes-no cryptoprotocol are the strands shown in
Fig. 4. Their instances result by replacing A, B, R, Q, Y, N, etc., by any names,
nonce, question, etc. The ﬁfth role is the listener role Lsn[y] with a single recep-
tion node in which y is received. The instances of Lsn[y] are used to document
that values are available without cryptographic protection. For instance, Lsn[K]
would document that K is compromised. Every protocol contains the role Lsn[y].
Indeterminates represent syntactically unconstrained messages received from
protocol peers, or passed down as parameters from higher-level protocols. Thus,
we require an indeterminate to be received as an ingredient before appearing in
any other way:
If n1 is a node on ρ ∈Π, with an indeterminate x ≪msg(n1),
then ∃n0, n0 ⇒∗n1, where n0 is a reception node and x ⊑msg(n0).
A principal executing a role such as P3 in Fig. 4 may be partway through its run;
for instance, it may have executed the ﬁrst reception event and ﬁrst transmission,
without “yet” having executed the ﬁnal reception event. Thus, it can not yet be
distinguished from the ﬁrst two nodes of an instance of role P4. When n1 ⇒n2
is the beginning of a strand α(P3), we also regard it as an instance of the role
P4, since nothing that has happened shows that it is not:
Deﬁnition 1. Node n is a role node of Π if n = ρ ↓j lies on some ρ ∈Π.
Let ρ ↓j be a role node of Π. Node mj is an instance of ρ ↓j if, for some
homomorphism α, the strand of mj, up to mj, takes the form: α(ρ ↓1) ⇒. . . ⇒
α(ρ ↓j) = mj.
⊓⊔
That is, messages and their directions—transmission or reception—must agree
up to node j. However, any remainders of the two strands beyond node j are
unconstrained. They need not be compatible. When a protocol allows a principal

Transformations between Cryptographic Protocols
115
to decide between diﬀerent behaviors after step j, based on the message contents
of their run, then this deﬁnition represents branching [7,9]. At step j, one doesn’t
yet know which branch will be taken.
If s is a strand, then t0 ≪s iﬀfor some j, t0 ≪msg(s ↓j); and similarly
t0 ⊑s iﬀfor some j, t0 ⊑msg(s ↓j).
Skeletons. A skeleton A consists of (possibly partially executed) role instances,
i.e. a ﬁnite set of nodes, nodes(A), with two additional kinds of information:
1. A partial ordering ⪯A on nodes(A);
2. Finite sets uniqueA, nonA of atomic values assumed uniquely originating and
respectively non-originating in A.
nodes(A) and ⪯A must respect the strand order, i.e. if n1 ∈nodes(A) and n0 ⇒
n1, then n0 ∈nodes(A) and n0 ⪯A n1. If a ∈nonA, then a must originate nowhere
in nodes(A), though a or a−1 may be the key encrypting some e ≪msg(n) for
n ∈nodes(A). If a ∈uniqueA, then a must originate at most once in nodes(A).
A is realized if it is a possible run without additional activity of regular par-
ticipants; i.e., for every reception node n, the adversary can construct msg(n)
via the Dolev-Yao adversary actions,1 using as inputs:
1. all messages msg(m) where m ≺A n and m is a transmission node;
2. any atomic values a such that a ̸∈(nonA ∪uniqueA), or such that a ∈uniqueA
but a originates nowhere in A.
Two skeletons A0, A1 are shown in Fig. 9. They are skeletons of the protocol
shown in Fig. 7, i.e. the second step in the derivation of the Question-Answering
Cryptoprotocol. Of these skeletons, only A1 is realized. It is a realized skeleton
because every message to be received may be derived by the adversary from
messages previously sent. In fact, every message received was itself previously
sent: i.e. the trivial adversary derivation suﬃces. However, A0 is not realized.
Y ′ is not derivable from its predecessors in A0, since the adversary can not use
K−1
A , nor re-originate Y ′.
The initiator strand in A0 is only of height 2, rather than 3, so that it has
not yet committed to branch to the aﬃrmative or negative answer. Thus, it is a
common instance of both initiator strands of its protocol. Whether we regard the
not-yet-occurred third node as an aﬃrmative one or a negative node makes no
diﬀerence. Indeed, the map on skeletons that replaces the ﬁrst two nodes of an
aﬃrmative strand by the ﬁrst two nodes of a negative strand is an isomorphism.
Given any skeleton A0, assume that ψ: nodesA0 →nodesA1. [ψ, α] is an equiv-
alence class of pairs ψ′, α′. A pair ψ′, α′ is in [ψ, α] if (1) ψ′ = ψ; and (2)
α(a) = α′(a), for every a such that a ≪msg(n) for any n ∈nodes(A0). This
deﬁnition for [ψ, α] implies that the action of α on atoms not mentioned in A0
is irrelevant.
1 The Dolev-Yao adversary actions are: concatenating messages and separating the
pieces of a concatenation; encrypting a given plaintext using a given key; and de-
crypting a given ciphertext using the matching decryption key.

116
J.D. Guttman
A

t1 
t′
1
 B

•
≻
t2

•

t′
2
Y ′
 •
A0
H0
−→
A

t1
≺
t1
 B

•

≻
t2

•

t2
•
Y 
≺
Y
 •
A1
t1 = {|RˆA|}B
t′
1 = {|R′ˆA|}B
t2 = {|RˆQˆY ˆB|}A
t′
2 = {|R′ˆQ′ˆY ′ˆB|}A
uniqueA0 = {Y ′}
uniqueA1 = {Y }
nonA0 = {K−1
A }
nonA1 = {K−1
A , K−1
B }
Fig. 9. Homomorphism H0 = [ψ0, α0]: A0 →A1
Deﬁnition 2. Let α be a homomorphism on messages of A, and ψ: nodesA0 →
nodesA1, for skeletons A0, A1. H = [ψ, α] is a skeleton homomorphism, written
H : A0 →A1, if:
1a. For all n ∈A0, msg(ψ(n)) = α(msg(n))
1b. ψ(n) is a transmission node iﬀn is a transmission node;
1c. ψ acts strand-by-strand; i.e.
∀s ∃s′ ∀j . s ↓j ∈nodes(A)
implies
ψ(s ↓j) = s′ ↓j;
2. n ⪯A0 m implies ψ(n) ⪯A1 ψ(m);
3. α(nonA0) ⊆nonA1;
4a. α(uniqueA0) ⊆uniqueA1;
4b. If a originates at n ∈nodesA0 for a ∈uniqueA0, then α(a) originates at ψ(n).
H is an isomorphism if (as usual) for some G: A1 →A0, G ◦H = IdA0.
We sometimes write H(n) for ψ(n) or H(a) for α(a), when H = [ψ, α]. As an
example of a homomorphism between skeletons, consider the map H0 between
skeletons shown in Fig. 9.
In H0, α0(R′) = R, α0(Q′) = Q, and α0(Y ′) = Y ; the remaining parameters
are unchanged. The function ψ0 on nodes maps successive nodes on the left
strand of A0 to their counterparts on the left strand of A1; it maps nodes on the
right strand of A0 to their counterparts on the right strand of A1. It also enriches
the ordering. Speciﬁcally, the topmost nodes are ordered in A1 but not in A0;
and the second node of the left strand precedes the last node of the right strand
in A1, although they are not ordered in A0. Moreover, α0(uniqueA0) = uniqueA1,
although α0(nonA0) ⊊nonA1.
When, for a homomorphism H = [ψ, α]: A →A′, ψ is an injective function
from nodes of A to nodes of A′, we call H a node-injective homomorphism.

Transformations between Cryptographic Protocols
117
We proved in [6] that if H : A →A′ and G: A′ →A are node-injective, then
A and A′ are isomorphic. Thus, we write A ≤N A′ to mean that for some node-
injective H, H : A →A′. Regarding ≤N as a relation on skeletons factored by
isomorphism, it is a well-founded partial order. In fact, there are only ﬁnitely
many isomorphism classes below the isomorphism class of any skeleton A.
4
Transformations
We deﬁne our notion of transformation via ﬁve clauses. Clauses 1–3 are simple
bookkeeping requirements. Clause 4 says that a transformation respects origi-
nation and ⊑.
Clause 5 says that a transformation respects the instances of roles in the
sense of Def. 1. Essentially, Clause 5 says that the transformed protocol does
not commit to one branch any earlier than the source, whenever the source roles
can branch.
Deﬁnition 3. Suppose that Π1 and Π2 are protocols, and F is a map from the
strands ρ1 ∈Π1 to pairs F(ρ1) = ρ2, g, where ρ2 ∈Π2 and g: N →N is a
function on natural numbers.
F is a protocol transformation iﬀ, for all ρ1 ∈Π1, letting F(ρ1) = ρ2, g:
1. g is an increasing function, i.e. i < j implies g(i) < g(j).
2. g(length(ρ1)) ≤length(ρ2).
3. ρ1 ↓j is a transmission node [resp. a reception node] iﬀ
ρ2 ↓g(j) is a transmission node [resp. a reception node].
4. For each role node n = ρ1 ↓i, and each parameter a such that a ≪msg(n):
(a) If a originates on ρ2 ↓k, with k ≤g(j), then a originates on some ρ1 ↓j′
with j′ ≤j; and
(b) If a ⊑msg(ρ2 ↓k), with k ≤g(j), then a ⊑msg(ρ1 ↓j′) with j′ ≤j.
5. Suppose for some ρ1, ρ′
1 and α, α′, and all j ≤k,
α(ρ1 ↓j) = α′(ρ′
1 ↓j).
Let F(ρ′
1) = ρ′
2, g′. For every i ≤k, g(i) = g′(i); and for every j ≤g(k),
α(ρ2 ↓j) = α′(ρ′
2 ↓j).
Node n is an image of m iﬀfor some ρ1, α, j, letting F(ρ1) = ρ2, g, we have
m = α(ρ1 ↓j) and n = α(ρ2 ↓g(j)).
⊓⊔
We use the indeﬁnite article, an image of m, because there are many substitu-
tions α′ such that m = α′(ρ1 ↓j), i.e. all those that diﬀer only for parameters not
appearing in ρ1 ↓j. For instance, in Fig. 5, the parameters of C′
2 ↓1 are A, B, Q,
so this node is unaﬀected by the action of α′ on R, Y, N, and unaﬀected by the
choice of the encryption keys pubk(A), pubk(B). However, diﬀerent instances of
P2 ↓2 in Fig. 4 result as these parameters vary.

118
J.D. Guttman
According to this deﬁnition, we have numerous transformations among the
examples given in Section 2. The two-step choreography service of Fig. 5 may
be transformed into any of the other choreographies and protocols of Figs. 1–7.
Each jth node of the source is mapped to the j + 1th node of the target, so that
the functions g(j) = j + 1. The ﬁrst “ready” message is not in the range of the
transformations.
Indeed, an alternate transformation maps the aﬃrmative strands of Fig. 5
to negative strands of the targets; this represents an inversion of the conven-
tion about the meanings of the outcomes. Indeed, the deﬁnition also allows
the “nonsensical” maps that send aﬃrmative initiator strands to negative re-
sponder strands, and negative initiator strands to aﬃrmative responder strands.
These nonsensical transformations, however, have the property that the image
of a well-formed choreography execution is not a realized skeleton of the crypto
protocol.
All of the three-step protocols have transformations to all of the others, which
in some cases introduce additional parameters and cryptographic structure; in
the reverse cases, the transformation “forgets” parameters. If P1 and P1 sent
their ﬁrst messages in incompatible forms, then Clause 5 would imply that the
map from the choreography in Fig. 1 to the resulting cryptoprotocol would not
be a transformation. It would force commitment to one branch too early. This
corresponds to the (pointless) service in which A allows B to ask a question to
which the answer is yes, and then answers yes, or allows B to ask a question to
which the answer is no, and then answers no.
Clause 4 prohibits the map from Fig. 7 to a variant of Fig. 4 in which the
public or private key of A is included in one of the messages as part of its
plaintext. It is a parameter of the source, while not being an ingredient in the
plaintext of the messages, and Clause 4(b) requires that this be preserved under
transformation. Likewise, a variant of Fig. 4 in which the responder originates R
in message 2 without having received it in message 1 would violate Clause 4(a).
When a transformation introduces parameters, then nodes of the source proto-
col have many possible images under the transformation. Naturally some choices
may be unnecessarily constraining, when they select values for the newly intro-
duced parameters that equal other parameters unnecessarily.
Suppose that X is a set of parameters, e.g. the parameters appearing in ρ1 ↓j.
Some parameters of ρ2 ↓g(j) may not be in X, for instance R if ρ2 = P2 and
j = 1. Among substitutions α′ that agree with α on X, some are unnecessarily
speciﬁc, for instance those that map two parameters ̸∈X to the same value.
Selecting α′(Y ) ̸= α′(N) would be more general than forcing α′(Y ) = α′(N).
Likewise, forcing a parameter not in X to agree with one in X discards generality,
e.g. forcing α′(Y ) = α′(Q).
Let α0 be a substitution that agrees with α on X, and where α0(x) = α0(x′)
implies x, x′ ∈X or x = x′. Then α0 is maximally general among substitutions
that agree with α on X, in the following sense. If α1 is any substitution agreeing
with α on X, then any β ◦α1 = γ ◦α0 for exactly one γ. As a consequence, if
α1 also satisﬁes the same property as α0, then α0 and α1 diﬀer by a renaming.

Transformations between Cryptographic Protocols
119
Thus, for any set of parameters X, α0 is universal for X if α0(x) = α0(x′)
implies x, x′ ∈X or x = x′. Such an α is however unique only up to isomorphism,
and the same will remain true for our functions to lift skeletons A. That is, F(A)
is determined only to within isomorphism.
In some circumstances (cf. e.g. [13]), it would be desirable to relax the order
preserving requirement of Def. 3, Clause 1. In this case, letting
g+(k) = max
1≤i≤k g(i),
we would rewrite Clause 2 as g+(length(ρ1)) ≤length(ρ2). Similarly, the occur-
rences of g(j) in Clause 4(a) and (b) would become g+(j), and the quantiﬁcation
over all i ≤g(k) in Clause 5 becomes i ≤g+(k). The main results of Section 5
are not substantially changed.
A strand of A is any s where n ∈nodes(A), for at least s’s ﬁrst node n = s ↓1.
Deﬁnition 4. Let F transform Π1 to Π2, and let A be a Π1-skeleton.
1. A Π2-skeleton B is an F-image of A iﬀthere is a bijection ϕ between strands
of A and strands of B such that:
(a) For every strand s of A, letting s = α(ρ1) and F(ρ1) = ρ2, g:
ϕ(s) = α(ρ2)
and
heightB(ϕ(s)) = g(heightA(s));
(b) If s ↓j ⪯A s′ ↓k, then (ϕ(s) ↓g(j)) ⪯B (ϕ(s′) ↓g′(k)), where
s = α(ρ1)
F(ρ1) = ρ2, g
s′ = α′(ρ′
1)
F(ρ′
1) = ρ′
2, g′.
(c) Uniquely originating and non-originating values are preserved:
uniqueA ⊆uniqueB and nonA ⊆nonB
2. We write F(A) = B if and only if B ≤N B′, for every F-image B′ of A.
Lemma 1. Let F transform Π1 to Π2, and let A be a Π1-skeleton. There is—to
within isomorphism—a unique B such that F(A) = B.
Proof sketch. We may regard the strands that contribute nodes to A as a family
{αi(ρi
1)}i∈I, where for each a representation has been chosen, as an instance of
a particular role ρi
1 under a particular substitution αi. In some cases, more than
one choice of role ρi
1 may be compatible with the nodes that actually appear to
A, which may be only an initial segment of the whole role. Deﬁnition 3, Clause 5
ensures that this choice cannot aﬀect the outcome. Moreover, αi may freely vary
for all parameters that do not appear in nodes of this segment of ρi
1. By the
discussion above, we may require that the αi are chosen such that:
1. for atoms or indeterminates v not appearing in ρi
1, αi(v) = αj(w) implies
w = v and i = j;
2. for atoms a not appearing in ρi
1, αi(a) ̸∈uniqueA ∪nonA;
3. for indeterminates x not appearing in ρi
1, αi(x) is an indeterminate.

120
J.D. Guttman
A

t3 
t′
3
 B

•
≻
t4

•

t′
4
Y ′
 •
F(A0)
F (H0)
−→
A

t3
≺
t3
 B

•

≻
t4

•

t′′
4
•
Y 
≺
Y
 •
F(A1)
t3 = {|RˆA|}B
t′
3 = {|R′ˆA|}B
t4 = {|RˆQˆY ˆN ˆB|}A
t′
4 = {|R′ˆQ′ˆY ′ˆN ′ˆB|}A
t′′
4 = {|RˆQˆY ˆN ′ˆB|}A
uniqueF (A0) = {Y ′}
uniqueF (A1) = {Y }
nonF (A0) = {K−1
A }
nonF (A1) = {K−1
A , K−1
B }
Fig. 10. Lifted Homomorphism F(H0): F(A0) →F(A1)
We now construct B by (a) taking images of each strand αi(ρi
1) using αi(ρi
2)
up to height g(k), where k is the height of αi(ρi
1) in A and F(ρi
1) = ρi
2, g; (b)
selecting ⪯B to be a minimal extension of ϕ(⪯A) compatible with the strand
ordering in B; and (c,d) letting nonB = nonA and uniqueB = uniqueA.
B is a Π2 skeleton: The conditions on the nodes and ordering are immediate
from the deﬁnitions. Moreover, Deﬁnition 3, Clause 4 ensures that any node in B
in which a ∈nonB appears as an ingredient is an image of a node in A in which
it already appeared as an ingredient. Thus, the condition on non-origination
is satisﬁed in B because it was satisﬁed in A. Similarly, Deﬁnition 3, Clause 4
ensures that any node in B in which a ∈uniqueB originates is an image of a node
in A in which it already originated. Thus, the condition on unique origination is
satisﬁed in B because it was satisﬁed in A.
Moreover, by the choice of the substitutions αi, B has a nodewise injective
homomorphism to any other image of A.
⊓⊔
As an example, in Fig. 10, we provide the images of the skeletons A0, A1 from Fig. 9.
The new parameters N, N ′ have been chosen to be diﬀerent from each other, and
diﬀerent from any other value appearing in the skeletons. Despite the fact that the
other primed parameters disappear from A1, N ′ remains; generality would have
been lost if it were forced to agree with N. Hence, F(A1) is not realized, although
the result of the substitution N ′ →N would yield a realized skeleton.
5
Transformations and Homomorphisms
There are three main facts that show that our notion of transformation is reason-
able, and these concern the way that transformations relate the homomorphisms
among Π1-skeletons with those among Π2-skeletons. The ﬁrst shows that trans-
formations lift each Π1-homomorphism to a Π2-homomorphism which is unique

Transformations between Cryptographic Protocols
121
A0 
H

ϕ0
	
A1
ϕ1
	
F(A0) 
G
 F(A1)
Fig. 11. Homomorphisms H, G
to within isomorphism. We write this G subsequently as F(H). See Fig. 11. An
example appears in Fig. 10.
Theorem 1. Let F transform Π1 to Π2, and suppose H : A0 →A1 is a ho-
momorphism on Π1-skeletons. There is a homomorphism G on Π2-skeletons
G: F(A0) →F(A1) such that (letting ϕi satisfy the conditions of Def. 4 for Ai):
1. ϕ1(H(n)) = G(ϕ0(n)) for every n ∈nodes(A0); and
2. G(v) = H(v) for every atom or indeterminate v appearing in A0.
Moreover, if G and G′ both satisfy this property, then they diﬀer by an isomor-
phism I, i.e. G′ = I ◦G. G is node-injective iﬀH is.
Proof sketch. Suppose that the homomorphism H = [ψ1, β]. We may regard the
strands contributing nodes to A0 as a family αi(ρi
1) in such as way that the
strands of A1 in the image of A0 under ψ are all of the form (β ◦αi)(ρi
1). A1
may restrict which roles are chosen more tightly than A0, since they may have
greater height in A1. Let β be general for parameters not appearing in A0.
Thus, we deﬁne G = [ψ2, β], where ψ2 = φ1 ◦ψ1 ◦φ−1
0 . G is a homomorphism
because each strand αi(ρi
2) in F(A0) is mapped by ψ2 to β ◦αi(ρi
2) as desired,
and the origination constraints follow as in Lemma 1.
If G′ = [ψ′
2, β′] also satisﬁes the conditions, then let φ′
1 a function such that, for
n ∈nodesA1, φ′
1(n) is (1) φ1(n) if n is not in the range of ψ1; and (2) ψ′
2(φ0(m))
for any m such that ψ1(m) = n, otherwise. By the uniqueness condition on φ1,
φ′
1 ◦(φ1)−1 is the desired isomorphism on F(A1).
The node-injectiveness property is immediate from the deﬁnition of ψ2 using
the bijections φ1, φ−1
0 .
⊓⊔
Retaining the notation of Fig. 11, we can also go in the other direction:
Theorem 2. Let F transform Π1 to Π2, and suppose G: F(A0) →F(A1).
G = F(H), for some H : A0 →A1. H is unique to within isomorphism.
Proof sketch. Let G = [ψ2, β]. We deﬁne ψ1 = φ−1
1
◦ψ2 ◦φ0.
⊓⊔
Finally, each Π2-skeleton B has a decomposition into a maximal part of the form
F(A), to which a node-injective mapping is applied.
Theorem 3. Let F transform Π1 to Π2, and let B be a Π2-skeleton. Let
S = {F(A0): A0 is a Π1-skeleton and F(A0) ≤N B}.
Then S has a ≤N-maximum member, i.e. there is a A1 such that F(A1) ∈S
and for every B0 ∈S, B0 ≤N F(A1).

122
J.D. Guttman
Proof sketch. For each strand s with nodes in nodesB, consider the nodes in B
that equal the nodes of any F(αi(ρi
1)). For each s, choose a ρi
1 that maximizes
the k such that s is of the form F(αi(ρi
1)) up to g(k). Let B1 be the subskeleton
of B where:
1. nodesB1 consists of the nodes of B of height less than this g(k) for each s;
2. ⪯B1 is the weakest order generated from the strand orderings and ⪯B re-
stricted to nodes of the form s ↓g(j);
3. nonB1 is the subset of nonB which are parameters appearing in some αi(ρi
1),
and likewise for uniqueB1.
This B1 is of the form F(A1).
⊓⊔
6
Conclusion
In this paper we have simply provided a deﬁnition, and shown that the deﬁni-
tion is well-behaved. Namely, we show that the deﬁnition of transformation ﬁts
the skeletons-and-homomorphisms framework of strand spaces. However, this in
itself does not tell us when a transformation respects security properties.
For this, we believe that our work on protocol composition via the authen-
tication tests provides the crucial hint [8]. We show there that if adding a new
protocol Π2 to an existing protocol Π1 produces no new ways to solve the
challenge-response patterns on which Π1’s security goals depend, then Π2 pre-
serves security goals that Π1 achieves. We also provided a syntactic way to deﬁne
and validate the “no new solutions” property. We believe that a rewriting of that
property to our current framework provides a suﬃcient criterion for preserving
security properties through protocol elaboration.
Such a result would complement the protocol transformation techniques de-
veloped by Datta, Derek, Mitchell, and Pavlovic in an outstanding series of
papers including [3,4]. The authors explore a variety of protocols with common
ingredients, showing how they form a sort of family tree, related by a number of
operations on protocols.
Our deﬁnition of multiprotocol from [8] covers both [4]’s parallel composition
and its sequential composition. Reﬁnement enriches the message structure of a
protocol. Their transformation moves information between protocol messages,
either to reduce the number of messages or to provide a tighter binding among
parameters. Our notion of transformation appears to cover both reﬁnement and
their transformation, although the “no new solutions” property appears more
likely to work with reﬁnements than with transformations in their sense.
Despite their rich palette of operations, their main results are restricted to
parallel and sequential composition [4, Thms. 4.4, 4.8]. Each result applies to
particular proofs of particular security goals G1. Each proof relies on a set Γ of
invariant formulas that Π1 preserves. If a secondary protocol Π2 respects Γ, then
G1 holds of the parallel composition Π1 ∪Π2 (Thm. 4.4). Thm 4.8, on sequential
composition, is more elaborate but comparable. By contrast, the key theorem
of [8] is one uniform assertion about all security goals, independent of their

Transformations between Cryptographic Protocols
123
proofs. It ensures that Π2 will respect all usable invariants of Π1. This syntactic
property, checked once, suﬃces permanently, without looking for invariants to
re-establish.
References
1. Carbone, M., Honda, K., Yoshida, N.: Structured communication-centred program-
ming for web services. In: De Nicola, R. (ed.) ESOP 2007. LNCS, vol. 4421, pp.
2–17. Springer, Heidelberg (2007)
2. Cortier, V., Delaitre, J., Delaune, S.: Safely composing security protocols. In:
Arvind, V., Prasad, S. (eds.) FSTTCS 2007. LNCS, vol. 4855, pp. 352–363.
Springer, Heidelberg (2007)
3. Datta, A., Derek, A., Mitchell, J.C., Pavlovic, D.: Abstraction and reﬁnement in
protocol derivation. In: Proceedings of Computer Security Foundations Workshop.
IEEE CS Press, Los Alamitos (2004)
4. Datta, A., Derek, A., Mitchell, J.C., Pavlovic, D.: A derivation system and compo-
sitional logic for security protocols. Journal of Computer Security 13(3), 423–482
(2005)
5. Doghmi, S.F., Guttman, J.D., Thayer, F.J.: Completeness of the authentication
tests. In: Biskup, J., Lopez, J. (eds.) ESORICS 2007. LNCS, vol. 4734, pp. 106–
121. Springer, Heidelberg (2007)
6. Doghmi, S.F., Guttman, J.D., Thayer, F.J.: Searching for shapes in cryptographic
protocols. In: Grumberg, O., Huth, M. (eds.) TACAS 2007. LNCS, vol. 4424, pp.
523–537. Springer, Heidelberg (2007), http://eprint.iacr.org/2006/435
7. Fr¨oschle, S.: Adding branching to the strand space model. In: Proceedings of EX-
PRESS 2008. Electronic Notes in Theoretical Computer Science. Elsevier, Amster-
dam (2008) (to appear)
8. Guttman, J.D.: Cryptographic protocol composition via the authentication tests.
In: de Alfaro, L. (ed.) Foundations of Software Science and Computation Structures
(FOSSACS). LNCS, vol. 5504, pp. 303–317. Springer, Heidelberg (2009)
9. Guttman, J.D., Herzog, J.C., Ramsdell, J.D., Sniﬀen, B.T.: Programming cryp-
tographic protocols. In: De Nicola, R., Sangiorgi, D. (eds.) TGC 2005. LNCS,
vol. 3705, pp. 116–145. Springer, Heidelberg (2005)
10. Guttman, J.D., Thayer, F.J.: Protocol independence through disjoint encryption.
In: Proceedings, 13th Computer Security Foundations Workshop, IEEE Computer
Society Press, Los Alamitos (2000)
11. Guttman, J.D., Thayer, F.J.: Authentication tests and the structure of bundles.
Theoretical Computer Science 283(2), 333–380 (2002); Conference version appeared
in IEEE Symposium on Security and Privacy (May 2000)
12. Hui, M.L., Lowe, G.: Fault-preserving simplifying transformations for security pro-
tocols. Journal of Computer Security 9(1/2), 3–46 (2001)
13. Mostrous, D., Yoshida, N., Honda, K.: Global principal typing in partially commu-
tative asynchronous sessions. In: ESOP Proceedings. LNCS. Springer, Heidelberg
(2009)

Formal Validation of OFEPSP+ with AVISPA⋆
Jorge L. Hernandez-Ardieta, Ana I. Gonzalez-Tablas, and Benjamin Ramos
Department of Computer Science, University Carlos III of Madrid
Avda. de la Universidad 30, 28911 Leganes, Spain
jlopez.ha@gmail.com, aigonzal@inf.uc3m.es, benja1@inf.uc3m.es
Abstract. Formal validation of security protocols is of utmost impor-
tance before they gain market or academic acceptance. In particular,
the results obtained from the formal validation of the improved Opti-
mistic Fair Exchange Protocol based on Signature Policies (OFEPSP+)
are presented. OFEPSP+ ensures that no party gains an unfair advan-
tage over the other during the protocol execution, while substantially
reducing the probability of a successful attack on the protocol due to
a compromise of the signature creation environment. We have used the
Automated Validation of Internet Security Protocols and Applications
(AVISPA) and the Security Protocol ANimator for AVISPA (SPAN),
two powerful automated reasoning technique tools to formally specify
and validate security protocols for the Internet.
Keywords: Fair exchange, security protocol, formal validation, AVISPA,
SPAN.
1
Introduction
Formal validation of security protocols is of utmost importance before they gain
market or academic acceptance. Some standard and widely used security proto-
cols for the Internet have been proved to suﬀer from critical design ﬂaws that
an attacker can exploit to subvert their security. The reason is that their secu-
rity goals were merely informally evaluated, obviating potential attack paths.
Automated reasoning techniques are commonly used to evaluate the protocols
in a formal way, increasing the assurance respecting the purported security. In
this sense, the Automated Validation of Internet Security Protocols and Appli-
cations (AVISPA) [2] and the Security Protocol ANimator for AVISPA (SPAN)
[12] tools have been used to validate the correctness and safety of the improved
Optimistic Fair Exchange Protocol based on Signature Policies (OFEPSP+).
OFEPSP [13], the protocol from which OFEPSP+ has been designed, is a
protocol oriented to Internet transactions where two parties need to exchange
information in a fair and secure manner. The origin sends a signed message to
⋆The authors would like to thank the AVISPA project team, and specially Laurent
Vigneron and Luca Vigan`o, for their useful comments on the preliminary versions of
OFEPSP+ HLPSL speciﬁcation. The authors wish also to thank the reviewers for
their valuable remarks.
P. Degano and L. Vigan`o (Eds.): ARSPA-WITS 2009, LNCS 5511, pp. 124–137, 2009.
c
⃝Springer-Verlag Berlin Heidelberg 2009

Formal Validation of OFEPSP+ with AVISPA
125
the receiver while the receiver sends back a proof of receipt of the message.
Therefore, both parties are making a commitment in the transaction: the origin
cannot repudiate having sent the message and the receiver cannot repudiate its
reception. As a fair exchange protocol, OFEPSP ensures that no party gains
an unfair advantage over the other during the protocol execution. Therefore,
either both parties obtain the expected information or none of them obtains any
useful information from the other. As an optimistic protocol, a Trusted Third
Party (TTP) is included in the design but participating only when a party’s
misbehavior or protocol error occurs.
Many fair exchange protocols found in literature are designed using symmetric
encryption, assuring the undisclosure of the message sent by the origin until the
receiver has made a commitment in the transaction [16]. In our case, OFEPSP
is based on signature policies [10]. A signature policy is a document that col-
lects a set of rules to create and validate electronic signatures, and under which
an electronic signature can be determined to be valid in a particular transac-
tion context. Signature policies are deﬁned both in human readable form and
structured form using an agreed syntax and encoding, like ASN.1 or XML. The
signature policy reference is included as a signed property in each signature per-
formed by the parties, allowing any veriﬁer to ascertain if the signature matches
the requirements imposed in the policy. Therefore, signature creation and veriﬁ-
cation processes can be completely carried out in an automatic and transparent
way in accordance with the signature policy rules.
In order to tie down the origin and receiver respecting the exchanged infor-
mation, most protocols use digital signatures. Due to the cryptographic basis of
digital signatures, a correctly veriﬁed signature is known to have been computed
with a particular private key. The common (and mistaken) established assump-
tion is that the entity which owns the private key cannot repudiate having per-
formed such a signature as nobody else could have computed it. If applied to
fair exchange protocols, the signed information sent by the origin and the proof
of receipt cannot be repudiated by the corresponding parties. However, several
feasible and practical attacks on the signature creation environment are found
in the literature [1,11,14,15,17]. Therefore, an attacker gaining access to the ori-
gin’s private key or just the signature creation environment could impersonate
her in a transaction, no matter how the underlying protocol is designed.
To reduce this risk, we improved OFEPSP, dividing the origin’s environment
in a Signature Creation Environment (SCE) and a Transaction Conﬁrmation
Environment (TCE). This division substantially reduces the probability of a
successful attack on the protocol as both environments are needed to create the
valid evidence. The security of both the SCE and the TCE can be compromised.
However, the probability of a successful distributed attack on SCE and TCE by
two diﬀerent malwares, and which collaborate in order to undermine the protocol
security, is, in any case, substantially lower that the probability of compromising
the security of a single environment. OFEPSP+ assures that an attacker will not
gain any beneﬁt from a potential security compromise on either the SCE or the
TCE, enforcing the non-repudiation property of the evidence generated during a

126
J.L. Hernandez-Ardieta, A.I. Gonzalez-Tablas, and B. Ramos
protocol run. To the best of our knowledge, this is the ﬁrst time a fair exchange
protocol takes the security of the parties’ environments into consideration for the
overall protocol design. OFEPSP+ can be applied to a variety of scenarios like
contract signing or e-commerce transactions, where the origin corresponds to an
end user using a potentially compromised SCE, like her Personal Computer.
In this article, OFEPSP+ is brieﬂy presented and the formal validation of
the protocol detailed. The preliminary results are promising, but some work
must still be done to obtain a high assurance of the protocol security. In this
ﬁrst approach, the formal validation covered masquerading and integrity attacks.
Fairness could not be modeled yet taking into account standard AVISPA fea-
tures. Nonetheless, future work will cover the use of additional predicates and
special goal formulas in order to incorporate fairness in the validation.
The article is organized as follows. The next Sect. 2 describes OFEPSP+ pro-
tocol. Section 3 covers the formal analysis of the protocol by means of AVISPA
and SPAN tools. And ﬁnally, we conclude the article in Sect. 4.
2
The Improved Optimistic Fair Exchange Protocol
Based on Signature Policies
This Sect. describes the improved Optimistic Fair Exchange Protocol based on
Signature Policies (OFEPSP+). The basic notation and deﬁnitions are given
ﬁrst. Afterwards, OFEPSP+ set of protocols is explained in “Alice & Bob”
notation.
2.1
Basic Notation and Deﬁnitions
Following notation is necessary for the correct understanding of further Sects.:
SP
Signature policy used during the protocol
X →Y : m Party X sends message m to party Y
X ←Z : SP Party X retrieves SP from a repository located at party Z
Sx (m)
Signature of party x generated on message m
Sx (m|SP)
Signature of party x generated on message m under SP conditions
POO = SO (m, ℓ, tpl id|SP)
Proof of origin of message m.
POR = SR (m, ℓ, tpl id|SP)
Proof of receipt of message m.
NRO = SO (POR|SP)
Non-repudiation evidence of origin of message m generated on POR.
NRR = SR (NRO|SP)
Non-repudiation evidence of receipt of message m generated on NRO.
NRAO = SO (NRR|SP)
Non-repudiation evidence of acknowledgment generated by origin on NRR.
This is the valid evidence that completes the protocol.

Formal Validation of OFEPSP+ with AVISPA
127
NRAT T P = ST T P (NRR|SP)
Non-repudiation evidence of acknowledgment generated by the TTP on NRR.
This is the valid evidence when the recovery protocol has been executed.
Each protocol run is identiﬁed by a protocol identiﬁer ℓ. A template is used
by the parties in order to ﬁx the information to be sent by the origin. This
template is referenced by the template identiﬁer tpl id. The template must be
deﬁned by the receiver according to the transaction needs. The message m sent
by the origin must be further processed by the receiver taking into account the
template information.
2.2
Main Protocol
OFEPSP+ consists of one main protocol, explained in this Subsect., and two
subprotocols, called recovery subprotocol and abort subprotocol, which are fur-
ther explained. During the exchange process, the origin must use two diﬀerent
platforms, called the Signature Creation Environment (SCE) and the Transac-
tion Conﬁrmation Environment (TCE). SCE is the platform where the signature
application is installed and used by the origin to create electronic signatures on
her behalf. The origin must use a hardware cryptographic device with sign-
ing capabilities (e.g. smart card) for that purpose. This cryptographic device
is regarded as part of the SCE platform. On the other hand, the origin ac-
knowledges the performed signatures by means of a second cryptographic device
(and thus diﬀerent signing key) used in a diﬀerent platform with another signa-
ture application (e.g. mobile device with cryptographic capabilities), that is, the
TCE.
In the main protocol, the origin initiates the transaction by sending the signed
message to the receiver by means of SCE. Afterwards, the origin conﬁrms the
initiated transaction by using TCE. The receiver will exchange several inter-
mediate evidences with both the SCE and TCE, until the ﬁnal valid evidence
NRA is generated. Next, the main protocol is formalized using the notation of
SubSect. 2.1 above:
(1) OSCE ←R : tpl [tpl id] , SR (tpl [tpl id])
First, the origin (O) requests the template identiﬁed by tpl id to the receiver
(R) (1) by means of the SCE platform.
(2) OSCE ←TTP-SP: SP, ST T P −SP (SP)
In next step (2) the origin retrieves the signature policy SP necessary to com-
municate with the receiver. Once the origin has obtained the template and the
signature policy, she can produce the message and POO taking into account this
information.
(3) OSCE →R : m, ℓ, tpl id, POO
Afterwards (3), the origin starts the protocol itself by sending the message m, a
unique protocol identiﬁer ℓ, the template reference and the POO.

128
J.L. Hernandez-Ardieta, A.I. Gonzalez-Tablas, and B. Ramos
(4) R ←TTP-SP: SP, ST T P −SP (SP)
(5) R →OT CE : m, ℓ, tpl id, POO, POR
The receiver retrieves the signature policy (4), if not obtained yet, and validates
the received POO. Afterwards, the receiver generates and sends the POR to the
origin’s TCE (5). The receiver must also send all the information received from
the origin in step (3) in order to allow her to validate the initiated transaction
using the second platform TCE. Step (4) can be avoided for eﬃciency purposes
if the receiver accesses the TTP-SP once and afterwards manages a local copy
of the signature policy, provided that it is within its validity period.
(6) OT CE →R : NRO
The origin must generate the NRO only if the information received in step (5)
corresponds to a desired transaction and POO and POR are correctly veriﬁed.
(7) R →OSCE : NRR
Once the origin has conﬁrmed the transaction by means of the NRO, the receiver
sends the NRR to the origin’s SCE platform (7).
(8) OSCE →R : NRAO
In the last step (8) the origin completes the transaction by sending the NRA to
the receiver.
Although not shown above, evidences POO, POR, NRO, NRR and NRA must
be time-stamped. The time-stamping procedure must be carried out according to
known standards, and implies the participation of a Time-Stamping Authority
(TSA). The time-stamp is an assertion of proof given by the TSA that the datum
existed before the speciﬁed time.
2.3
Recovery Subprotocol
The recovery subprotocol allows the receiver to obtain evidence NRA in case
of a protocol interruption or origin’s misbehavior, and must be executed if the
receiver does not receive the NRA within a speciﬁc time interval. OFEPSP+
recovery subprotocol consists of next steps:
(1) R →T T P : H (m, ℓ, tpl id) , ℓ, POO, POR, NRO, NRR
if (protocol aborted) then
(2a)
T T P →R : ST T P (SO (abort, ℓ|SP) |SP)
else
(2b)
T T P ←TTP-SP: SP
(3b)
T T P →R, OSCE, OT CE : NRAT T P
In (1) the receiver sends the produced evidences POO, POR, NRO and NRR
to the TTP. Also, and in order to protect the privacy of the parties, the infor-
mation signed in POO and POR - the message, the protocol identiﬁer and the
template reference - is not sent in plain text but the hash of their concatenated

Formal Validation of OFEPSP+ with AVISPA
129
values. Yet the TTP is still able to verify POO and POR by directly using the
hash, provided that a digital signature scheme based on public key cryptography
is used (i.e. RSA, DSA, ECDSA). The TTP must decrypt POO and POR - using
the corresponding public keys -, obtaining the hash of the signed information,
which must correspond to the value of H (m, ℓ, tpl id). ℓis sent in (1) to allow
the TTP to retrieve and update the information associated to the transaction.
If the protocol has already been aborted, the TTP merely forwards the abort
evidence to the receiver (2a). On the other hand, the TTP generates the NRAT T P
taking into account the referenced signature policy - (2b) and (3b) -, but only in
the ﬁrst request. The evidence must be stored in a local database along with the
received information. Subsequently, the TTP will reuse it, improving the eﬃciency.
It is important to remark that the signatures that correspond to the evidences
generated in the protocol (POO, POR, NRO, NRR and NRA) must be gener-
ated according to electronic signature standards (please refer to [13] for further
information). Thereby, a reference to the signature policy used in the protocol is
included as a signed property in the speciﬁc electronic signature format chosen
for the transaction (i.e. XAdES, CAdES). This permits the TTP to know and
retrieve the signature policy and avoids an attacker to modify the referenced
signature policy.
2.4
Abort Subprotocol
The abort subprotocol allows the origin to abort the protocol execution if a re-
ceiver’s malicious behavior is suspected or an error during the protocol run has
occurred. OFEPSP+ abort subprotocol consists of next steps:
(1) OSCE|T CE →T T P : abort, ℓ, SO (abort, ℓ|SP)
if (recovery protocol executed) then
(2a)
T T P →OSCE|T CE : NRAT T P
else
(2b)
T T P ←TTP-SP: SP
(3b)
T T P →OSCE|T CE : ST T P (SO (abort, ℓ|SP) |SP)
For eﬃciency purposes, ST T P (SO (abort, ℓ|SP) |SP) is only generated in the
ﬁrst time (2b) and (3b), being reused in subsequent executions of the abort
subprotocol. On the other hand, if the protocol has been recovered (2a), the
TTP just retrieves the NRAT T P from its data base, forwarding it to the origin.
3
OFEPSP+ Validation with AVISPA and SPAN
This Sect. covers the validation process using the Automated Validation of In-
ternet Security Protocols and Applications tool (AVISPA) and the Security Pro-
tocol ANimator for AVISPA (SPAN).
AVISPA [2,3] provides a suite of applications for building and analyzing for-
mal models of security protocols. AVISPA incorporates four backends: the On-
the-Fly Model-Checker (OFMC) [6], the Constraint-Logic-based model-checker

130
J.L. Hernandez-Ardieta, A.I. Gonzalez-Tablas, and B. Ramos
(CL-AtSe) [18], the SAT-based Model-Checker (SATMC) [4], and the Tree Au-
tomata based Automatic Approximations for the Analysis of Security Protocols
(TA4SP) [7]. These modules implement diﬀerent automated reasoning techniques
to formally analyze the protocol speciﬁcation. On the other hand, SPAN [12] of-
fers a graphical user interface that allows the protocol designer to easily interact
with AVISPA capabilities.
Protocol models must be written in the High Level Protocol Speciﬁcation
Language (HLPSL) [5,8], which allows the protocol designer to describe the
security protocol and specify its intended security properties. HLPSL details
have been omitted for lack of space.
AVISPA adopts the standard intruder model of Dolev and Yao (DY model)
[9], in which the intruder has complete control over the network but cannot break
cryptography. The intruder may intercept, analyze, and/or modify messages (as
far as he knows the required keys), and send any message he composes to whoever
he pleases, posing as any other agent. The goal of OFEPSP+ validation was to
check the correctness and safety of the protocol respecting DY model.
The validation methodology followed can be summarized in next 3 steps:
1. OFEPSP+ speciﬁcation in HLPSL.
2. HLPSL correctness veriﬁcation.
3. OFEPSP+ security validation.
3.1
OFEPSP+ Speciﬁcation in HLPSL
OFEPSP+ complexity lies in the existence of four entities. The ﬁrst two entities,
SCE and TCE, are managed by the origin of the protocol, but, in HLPSL, had
to be considered as two diﬀerent roles played by a diﬀerent agent each. The other
two entities, Receiver and TTP, were modeled as the receiver and the TTP roles,
respectively, played by the corresponding agents. Each role implemented its own
state transition system according to the steps indicated in the protocol described
in Sect. 2. Thanks to the set structure available in HLPSL, the TTP’s evidence
database could be modeled and the TTP behavior accurately deﬁned.
Restrictions Applied
The protocol steps described in Sect. 2 could be modeled, except next four issues.
Signature Policy-based Design
OFEPSP+ fairness property is enforced by the signature policy-based design,
assuring that an incomplete evidence (POO, POR, NRO or NRR) does not tie
down any of the parties involved. The existence of NRA is imperative to prove
the commitment made by both parties, and its creation (either by the origin or
the TTP) must fulﬁll the policy constraints and requirements. We found that
the usage of signature policies could not be modeled in HLPSL. HLPSL allows
translating an Alice & Bob chart into a more detailed and expressive language.
However, not every protocol behavior can be mapped in HLPSL. Therefore, the
protocol steps related to the policy retrieval were discarded during the HLPSL
deﬁnition.

Formal Validation of OFEPSP+ with AVISPA
131
Time-Stamping
Time-stamps are needed in the protocol to allow the parties to know when
the recovery or abort subprotocols must be executed. The trigger is a timeout
deﬁned in the signature policy for each case. However, HLPSL only allows the
establishment of a generic timeout as an incoming message to a role. This feature
avoided us to model the speciﬁc timeouts, and thus, the time-stamping processes
were obviated. We have checked that this constraint has not modiﬁed neither
the protocol behavior nor its security goals fulﬁllment.
Template Usage
In OFEPSP+ the origin must create the message m according to the tem-
plate constraints. This measure restricts the semantics of the signed informa-
tion, avoiding most of semantic attacks currently developed [1,14,15]. However,
and as previously mentioned, in this ﬁrst approach we wanted to evaluate the
security measures against DY model. For that reason, the template retrieval by
the origin in the ﬁrst step of the main protocol was discarded as well.
Server Role Capabilities
Finally, the TTP (server) role was initially too complex for the backend analyzers
of AVISPA, due to the number of transition combinations considered during the
validation process. Note that our TTP is designed as an e-notary, storing and
managing the evidences generated during a protocol run in which the TTP
takes part. Besides, there were three situations where the TTP could intervene:
an abort requested by both the SCE and the TCE, and a recovery requested by
the receiver. This led to a huge role deﬁnition with 9 transitions. Taking into
account that CL-AtSe considers, by default, that each transition can be applied
at most 3 times, the backend had to manage 27 possible transitions during the
analysis. It seemed to be too complex.
For that reason, we simpliﬁed the role server, excluding the TCE from making
abort requests. The origin is still able to abort the protocol by using the SCE.
Thus, the server role was modeled considering next three states: 0 as the initial
state; 1 as the state when an abort has been done ﬁrst; and 2 as the state when
a recovery has been done ﬁrst. In each state, the server can receive both abort
and recovery requests, leading to 6 deﬁned transitions.
As a result of previous modiﬁcations applied, the server role cannot respond
to several parallel sessions. The transitions are sequential, that is, once a trans-
action has been aborted or recovered, the server will stay in that state (1 or
2, respectively) for the rest of requests, no matter if they come from another
session. Nonetheless, and as will be seen further, we were able to test the proto-
col with parallel sessions between SCE, TCE and Receiver, looking for possible
attacks though the server behaved in this way.
We know that we have limited the possible space of attacks, but the decision
was made in order to allow the backends to correctly analyze the protocol.
Analysis Scenarios
HLPSL must cover the deﬁnition of two special roles: session and environment.
Respecting the session role, we just instantiated the roles sce, tce and receiver

132
J.L. Hernandez-Ardieta, A.I. Gonzalez-Tablas, and B. Ramos
with the adequate information. Mention that the agents SCE and TCE, since
both are managed by the same origin, own a pre-shared knowledge: the message
to be sent. The template reference is also a pre-shared knowledge between the
SCE, TCE and the Receiver agents.
One of the most important parts of the HLPSL is the initial knowledge al-
located to the intruder. In this sense, and for test purposes, we deﬁned ﬁve
diﬀerent template references (tpl id, tpl id2, tpl id3, tpl id4) and four diﬀerent
messages (msg, msg2, msg3, msg4), assigning the subset {msg2, msg3, tpl id2,
tpl id3, tpl id4} as knowledge to the intruder.
Afterwards, we deﬁned several analysis scenarios with diﬀerent sessions con-
ﬁgurations (see section 3.3 for details). Due to the constraints applied to the
server role capabilities commented above, we instantiated just one server role in
each scenario.
Security Goals
AVISPA supports three types of goals so far: secrecy of, (strong) authentica-
tion on and weak authentication on. In the latter, the piece of data used to
authenticate the agent can be reused by an attacker, so reply attacks are not
considered by the analyzers. As AVISPA does not explicitly support fairness and
non-repudiation security goals, some fair exchange/non-repudiation protocols
modeled with AVISPA used secrecy of goal to achieve it. However, OFEPSP+
does not provide conﬁdentiality on any item. Fairness is achieved by means of
the signature policy fulﬁllment, as explained above. Therefore, currently we have
only modeled authentication goal respecting the exchanged evidences.
3.2
HLPSL Correctness Veriﬁcation
In this step, the aim was twofold: verify the syntactic/semantic correctness and
executability of the HLPSL speciﬁcation; and that the HLPSL speciﬁcation im-
plemented the intended protocol behavior.
For the syntactic, semantic and executability veriﬁcation, next AVISPA and
SPAN tools were used:
HLPSL2IF: This tool translates the HLPSL speciﬁcation into the Intermediate
Format (IF). A successful translation implies that the syntax of the protocol is
correct. OFEPSP+ HLPSL ﬁle was correctly translated into the corresponding
IF.
Protocol simulation: By simulating the protocol with SPAN, the semantic of the
protocol’s HLPSL is veriﬁed and a Message Sequence Chart (MSC) visualized. In
our case, the semantic was correctly veriﬁed but the MSC could not be shown. It
frequently happens when the transition labels and state values are not perfectly
set. In any case, it does not imply an error in the speciﬁcation.
OFMC search tree option: OFMC oﬀers the possibility to browse the search tree
through a path indicated by the indexes of the successors to follow. As a result,
one can decide which choice point take in a speciﬁc point of the search tree
deduced from the IF. This option allows the tester to check if every transition

Formal Validation of OFEPSP+ with AVISPA
133
can be taken during a protocol run. In our case, every transition could be chosen
at some time in the search tree.
CL-AtSe no executability option: CL-AtSe oﬀers the possibility of tracing the
protocol speciﬁcation without being analyzed. The output shows the so called
Initial System State, representing both the intruder and honest participants
states in CL-AtSe just after reading and interpreting the IF ﬁle. While the
intruder state is just represented by a list of knowledges, the honest partici-
pants are described by a set of instantiated roles, called Interpreted protocol
speciﬁcation. This option is useful to check that CL-AtSe interprets the protocol
transitions as expected. Each role consists in a tree where unary nodes are proto-
col steps and n-ary nodes are choice points. In our case, each possible transition
was represented in the tree.
SATMC check only executability: With this option, SATMC checks on exe-
cutability of actions/rules without any intruder, allowing the tester to debug
the speciﬁcation. The output trace showed that every rule could be executed.
Session compilation with OFMC: With session compilation (sessco), OFMC ﬁnds
a replay attack even without a second parallel session. It ﬁrst simulates a run
of the whole system and in a second run, it lets the intruder take advantage of
the knowledge learned in the ﬁrst run. Sessco is also handy for a quick check of
executability. However, as stated in AVISPA documentation, if one role can loop
(i.e. remain in the same control state forever and make inﬁnitely many steps),
sessco is not possible, and OFMC aborts with an error message. That is our case
in some transitions of role server, and thus, we could not use this option.
CL-AtSe no executability option and OFMC search tree option helped us also to
ascertain that the HLPSL speciﬁcation matched the intended protocol behavior.
3.3
OFEPSP+ Security Validation
The results obtained from validating OFEPSP+ with OFMC and CL-AtSe back-
ends are shown in next Tables 1, 2 and 3. In case of SATMC, the result was always
“Inconclusive”. Tests reports showed us that SATMC did not ﬁnd an attack, but
it warned that, with SATMC backend, intruder is not allowed to generate fresh
terms (as in sce role). As a consequence, attacks based on such an ability would
not be reported. TA4SP was not used because it does not support sets up to
now. The analysis scenarios referred in Tables below are described in Table 41.
Conﬁgurations applied in Table 1 were aimed at ﬁnding attacks in a normal
session (cfg1) or sessions when the intruder impersonates one of the legitimate
agents - SCE (cfg2), TCE (cfg3) or Receiver (cfg4).
Conﬁgurations shown in Table 22 were focused on violating the security goals
when two coherent parallel sessions are executed (cfg5) and when a legitimate
1 Intruder is denoted as ’i’. We did not instantiated any session with the intruder
playing the role of the server because we consider the TTP to be honest.
2 OFMC executed with maximum search depth of 17. CL-AtSe executed with -nb 1
option (maximum 1 loop iteration in any trace) for test marked with (*).

134
J.L. Hernandez-Ardieta, A.I. Gonzalez-Tablas, and B. Ramos
Table 1. Validation results with OFMC and CL-AtSe respecting a single session with
legitimate agents and single sessions with intruder playing the role of a legitimate agent
Analysis scenario OFMC CL-AtSe
cfg1
SAFE
SAFE
cfg2
SAFE
SAFE
cfg3
SAFE
SAFE
cfg4
SAFE
SAFE
Table 2. Validation results with OFMC and CL-AtSe respecting parallel sessions with
legitimate agents playing diﬀerent roles
Analysis scenario OFMC CL-AtSe
cfg5
SAFE SAFE(*)
cfg6
SAFE
SAFE
cfg7
SAFE
SAFE
cfg8
SAFE
SAFE
Table 3. Validation results with OFMC and CL-AtSe respecting parallel sessions with
intruder playing as legitimate agent(s)
Analysis scenario OFMC CL-AtSe
cfg9
SAFE(*)
SAFE
cfg10
SAFE(*)
SAFE
cfg11
SAFE(*)
SAFE
cfg12
SAFE
SAFE
cfg13
SAFE
SAFE
cfg14
SAFE
SAFE
party is playing a role for which is not intended to in case of parallel sessions
(cfg6, cfg7 and cfg8). We realized that each participant’s identiﬁer had to be
included in each evidence generated in order to avoid this sort of attack. In
particular, we used the public keys of SCE, TCE, Receiver, and TTP.
Table 33 contains the set of conﬁgurations where an intruder is playing the
role of legitimate agent(s) when two parallel sessions are executed. Note that
the knowledge own by the intruder in each conﬁguration diﬀers. The aim was
to ﬁnd possible security goals violations in a session when carried out from an
intruder running in another diﬀerent session. Mention that when the intruder
plays a legitimate role in a session, the goals involving him are not considered
by AVISPA (otherwise, he could always achieve an attack).
Based on the results obtained from the tests, our protocol fulﬁlls the security
goals G2 - Message authentication (includes message integrity), G17 - Account-
ability, G18 - Proof of Origin and G19 - Proof of Delivery for evidences POO,
3 Results marked with (*) mean that OFMC was launched with maximum search
depth of 17.

Formal Validation of OFEPSP+ with AVISPA
135
Table 4. Analysis scenario conﬁgurations for the tests
Analysis scenario
sessions conﬁguration
cfg1
session (sce, tce, r, s, ..., msg,tpl id)
cfg2
session (i, tce, r, s, ..., msg, tpl id)
cfg3
session (sce, i, r, s, ..., msg, tpl id)
cfg4
session (sce, tce, i, s, ..., msg, tpl id)
cfg5
session (sce, tce, r, s, ..., msg,tpl id)
session (sce, tce, r, s, ..., msg,tpl id)
cfg6
session (sce, tce, r, s, . . . , msg, tpl id)
session (r, tce, sce, s, . . . , msg, tpl id)
cfg7
session (sce, tce, r, s, . . . , msg, tpl id)
session (sce, r, tce, s, . . . , msg, tpl id)
cfg8
session (sce, tce, r, s, . . . , msg, tpl id)
session (tce, sce, r, s, . . . , msg, tpl id)
cfg9
session (sce, tce, r, s, . . . , msg, tpl id)
session (i, tce, r, s, . . . , msg,tpl id)
cfg10
session (sce, tce, r, s, . . . , msg, tpl id)
session (sce, i, r, s, . . . , msg3, tpl id3)
cfg11
session (sce, tce, r, s, . . . , msg, tpl id)
session (sce, tce, i, s, . . . , msg4, tpl id4)
cfg12
session (i, tce, r, s, . . . , msg2, tpl id2)
session (sce, i, r, s, . . . , msg3, tpl id3)
cfg13
session (i, tce, r, s, . . . , msg2, tpl id2)
session (sce, tce, i, s, . . . , msg3, tpl id3)
cfg14
session (sce, i, r, s, . . . , msg,tpl id)
session (sce, tce, i, s, . . . , msg, tpl id)
POR, NRO, NRR and NRA. Goals description are given in Deliverable 6.1 “List
of selected problems” in AVISPA project [2].
Due to our protocol design, evidences are not protected against reply attacks
(G3), and thus goal Entity Authentication (G1) is not achieved either. For that
reason, only weak authentication on goal was assigned. We think that including
a nonce in the requests generated by the receiver would enforce goals G1 and
G3 for NRO, NRR and NRA. However, tests conducted including that nonce
did not reach a conclusion, maybe due to the huge number of transitions the
backends had to analyze. As a result, our assumption could not be proved.
4
Conclusions and Work in Progress
In this article an improved Optimistic Fair Exchange Protocol based on Signature
Policies (OFEPSP+), which evolves a previously presented protocol [13], has
been brieﬂy presented and analyzed.
The protocol has been evaluated with the Automated Validation of Inter-
net Security Protocols and Applications (AVISPA) and the Security Protocol
ANimator for AVISPA (SPAN) against the intruder model of Dolev-Yao. The

136
J.L. Hernandez-Ardieta, A.I. Gonzalez-Tablas, and B. Ramos
methodology followed to specify the protocol in the High Level Protocol Spec-
iﬁcation Language (HLPSL) and to validate it by means of AVISPA backends
has been presented along with the results obtained. We found several prob-
lems during the speciﬁcation stage, like modeling the intended protocol behav-
ior in HLPSL, or checking that the protocol features that could not be deﬁned
in HLPSL did not aﬀect the security properties evaluated (e.g. the signature
policy-based design).
The preliminary results are promising, but some work must still be done. We
are currently working on modifying the transition labels and state values in order
to get a protocol simulation, what will allow us to interact in a protocol run as
an active intruder. Afterwards, we will enrich the test scenarios to increase the
level of assurance respecting the security goals fulﬁllment. Finally, and as the
biggest challenge, fairness will be speciﬁed by means of special predicates and
goal formulas. We hope that these results and current work will allow us to
implement a proved secure fair exchange protocol.
Acknowledgments. This research has been partially supported by the Min-
istry of Industry, Tourism and Trade of Spain, in the framework of the project
CENIT-Segur@, reference CENIT-2007 2004. (https://www.cenitsegura.es)
References
1. Alsaid, A., Mitchel, C.J.: Dynamic content attacks on digital signatures. Informa-
tion Management & Computer Security 13(4), 328–336 (2005)
2. AVISPA: Automated validation of internet security protocols and applications.
FET Open Project IST-2001-39252 (2003), http://www.avispa-project.org/
3. Armando, A., Basin, D., Boichut, Y., Chevalier, Y., Compagna, L., Cuellar, J.,
Hankes Drielsma, P., Heam, P.-C., Kouchnarenko, O., Mantovani, J., Modersheim,
S., von Oheimb, D., Rusinowitch, M., Santos Santiago, J., Turuani, M., Vigano, L.,
Vigneron, L.: The AVISPA Tool for the automated validation of internet security
protocols and applications. In: Etessami, K., Rajamani, S.K. (eds.) CAV 2005.
LNCS, vol. 3576, pp. 281–285. Springer, Heidelberg (2005)
4. Armando, A., Compagna, L.: SATMC: A SAT-based model checker for security
protocols. In: Alferes, J.J., Leite, J. (eds.) JELIA 2004. LNCS (LNAI), vol. 3229,
pp. 730–733. Springer, Heidelberg (2004)
5. AVISPA. Deliverable 2.1: The High-Level Protocol Speciﬁcation Language (2003),
http://www.avispa-project.org/publications.html
6. Basin, D.A., Sebastian, M., Vigano, L.: Ofmc: A symbolic model checker for secu-
rity protocols. Int. J. Inf. Sec. 4(3), 181–208 (2005)
7. Boichut, Y., Heam, P.-C., Kouchnarenko, O., Oehl, F.: Improvements on the Genet
and Klay Technique to Automatically Verify Security Protocols. In: Proc. of Au-
tomated Veriﬁcation of Inﬁnite States Systems (AVIS 2004). ENTCS, pp. 1–11
(2004)
8. Chevalier, Y., Compagna, L., Cuellar, J., Hankes Drielsma, P., Mantovani, J., Mod-
ersheim, S., Vigneron, L.: A High Level Protocol Speciﬁcation Language for Indus-
trial Security-Sensitive Protocols. In: Proc. SAPS 2004. Austrian Computer Society
(2004)

Formal Validation of OFEPSP+ with AVISPA
137
9. Dolev, D., Yao, A.: On the Security of Public-Key Protocols. IEEE Transactions
on Information Theory 2(29) (1983)
10. ETSI TR 102 041 v1.1.1. Signatures policies report. European Telecommunications
Standards Institute (2002)
11. Girard, P., Giraud, J.-L.: Software attacks on smart cards. Information Security
Technical Report 8(1), 55–66 (2003)
12. Glouche, Y., Genet, T., Heen, O., Courtay, O.: A Security Protocol Animator Tool
for AVISPA. In: ARTIST2 Workshop on Security Speciﬁcation and Veriﬁcation of
Embedded Systems, Pisa (2006)
13. Hernandez-Ardieta, J.L., Gonzalez-Tablas, A.I., Alvarez, B.R.: An Optimistic Fair
Exchange Protocol based on Signature Policies. Computers & Security 27(7-8),
309–322 (2008)
14. Jsang, A., Povey, D., Ho., A.: What You See is Not Always What You Sign. In:
The Proceedings of the Australian UNIX User Group, Melbourne (2002)
15. Kain, K.: Electronic Documents and Digital Signatures. Master Thesis (2003)
16. Kremer, S., Markowitch, O., Zhou, J.: An intensive survey of fair non-repudiation
protocols. Computer Communications 25, 1601–1621 (2002)
17. Spalka, A., Cremers, A.B., Langweg, H.: Trojan Horse Attacks on Software for
Electronic Signatures. Informatica 26(2), 191–203 (2002)
18. Turuani, M.: The CL-Atse Protocol Analyser. In: Pfenning, F. (ed.) RTA 2006.
LNCS, vol. 4098, pp. 277–286. Springer, Heidelberg (2006)

On the Automated Correction of Protocols with
Improper Message Encoding
Dieter Hutter1 and Ra´ul Monroy2
1 DFKI, Cartesium 2.41, Enrique Schmidt Str. 5, D-28359 Bremen, Germany
hutter@dfki.de
2 Tecnol´ogico de Monterrey, Campus Estado de M´exico
Carr. Lago de Guadalupe Km. 3.5, Atizap´an, Estado de M´exico, 52926, M´exico
raulm@itesm.mx
Abstract. Security protocols are crucial to achieve trusted computing.
However, designing security protocols is not easy and so security pro-
tocols are typically faulty and have to be repaired. Continuing previous
work we present ﬁrst steps to automate this repairing process, especially
for protocols that are susceptible to type-ﬂaw attacks. To this end, we
extend the notion of strand spaces by introducing an implementation
layer for messages and extending the capabilities of a penetrator to swap
messages that share the same implementation. Based on this framework
we are able to track type ﬂaw attacks to incompatibilities between the
way messages are implemented and the design of concrete security pro-
tocols. Heuristics are given to either change the implementation or the
protocol to avoid these situations.
1
Introduction
A security protocol is a protocol that aims to establish one or more security
goals, often a combination of integrity, authentication or conﬁdentiality. Secu-
rity protocols are critical applications, since they are crucial to achieve trusted
computing. So they are thoroughly studied to guarantee that there does not
exist an interleaving of protocol runs violating a security goal, called an attack.
Designing correct security protocols, however, has proven to be problematic,
since it is diﬃcult to anticipate what an adversary can learn from observing
the simultaneous execution of an arbitrary number of protocols, given some
initial knowledge. Under the formal approach to security protocol veriﬁcation,
the adversary usually is Dolev-Yao: he is able to delay messages or prevent them
reach their destination; he can analyse messages without breaking cryptography;
he can build messages using his initial knowledge or reusing components from
messages previously intercepted; he can send messages as if they were of his own
or somebody else’s.
Protocol ﬂaws can usually be understood as violations to well-known design
guidelines, such as those given in [1,2,17]. These kinds of principles capture
prudent practises in security protocol design, while pinpointing features that
make protocols diﬃcult to verify or possibly susceptible to an attack, and should
P. Degano and L. Vigan`o (Eds.): ARSPA-WITS 2009, LNCS 5511, pp. 138–154, 2009.
c
⃝Springer-Verlag Berlin Heidelberg 2009

On the Automated Correction of Protocols with Improper Message Encoding
139
thus be avoided. Principles in [1] spot ﬂaws caused by overlooking information
security ﬂow, while those in [2,17] spot ﬂaws, commonly referred to as protocol
failures, caused by misusing cryptosystems. In this paper, attention is restricted
only to the design principles given in [1].
Guidelines for protocol design should be treated carefully, though. This is
partly because they capture prudent practises of the 1990’s, when aspects like
resilience or denial-of-service attacks were not considered yet. But this is also
partly because some guidelines are rather cunbersome. For example, some prin-
ciples are unnecessarily normative, suggesting one should add full information to
every protocol message (protocol ID, session ID, etc.) or suggesting one should
use diﬀerent encryption systems or add hashings to each message, e.g. [3,6].
Protocol designers are often unwilling to follow these guidelines. They aim
at achieving security guarantees by applying a combination of cryptographic
primitives.
A type ﬂaw attack is a kind of replay attack1 where a principal accepts a
message of one type as a message of another. Heather et al. [11] have shown that
it is possible, under certain circustamces and protocol assumptions, to prevent
type ﬂaw attacks by tagging every protocol message, and elements thereof, with
a string indicating its intended type. However, as noticed by Malladi and Alves-
Foss [13], message tagging makes it easier to elaborate a password guessing
attack. This is because each tag provides the adversary a means for identifying
a hit, since the tag, which is a meaningful string, might become readable after
a code cracking attempt.
In this paper, we shall prove that blind message tagging is unnecessary. We
shall argue that to prevent a type ﬂaw attack, it is enough to adopt good prac-
tises of message encoding, as proposed by [1]. In particular, upon reception of a
protocol message, an agent should be able to verify that the message is associ-
ated to a particular protocol step and to a particular run (see design principles
6—8 and 10). We shall see that our approach can provide a protocol message
with as much tagging as necessary but also as less tagging as possible without
getting faulty.
Continuing previous work [12], we propose a method that aims at automati-
cally ﬁxing security protocols that are susceptible to a replay attack. We rely on
existing state-of-the-art tools, such as OFMC [4,5] or Cl-Atse [7,8], capable of
ﬁnding a type ﬂaw attack to an input faulty protocol. Our method then analyses
the protocol and the attack to identify the faulty steps of the protocol and syn-
thesises appropriate changes to ﬁx them. This yields an improved version of the
protocol that should be analysed and potentially patched again until no further
ﬂaws can be detected.
To capture type ﬂaw attacks, we extend the notion of strand spaces [19] (see
Section 2). Crucial to our approach is the distinction of a protocol speciﬁcation
at an abstract level and the implementation of messages at a concrete level.
A type ﬂaw attack is feasible only if the penetrator is capable of supplying a
1 A replay attack is a form of attack where a data transmission is repeated or delayed.

140
D. Hutter and R. Monroy
honest principal a camouﬂaged message whose implementation equals that of
the message being spoofed.
We discuss on the soundness of the extended version of strand spaces (see
Section 3) and provide a method for automatic protocol repair (see Section 4).
2
Strands for Type Flaw Attacks
2.1
Message Terms
Abstract messages, ranged over by M1, M2, . . ., are also called terms. The set of
terms, A, is freely generated from two disjoint sets, the set of texts (T) and the set
of keys (K), by means of concatenation, M1; M2, and encryption, {|M|}K (K ∈
K). T contains Nonce, the set of nonces, Na, Nb, . . .; Timestamps, the set of
timestamps, Ta, Tb, . . .; Agent, the set of principals, A, B, . . .; and Tags, the set of
tags, ℓa, ℓb, . . .. There are two functions, one maps principals, A, B, . . ., to their
public keys, K+
a , K+
b , . . ., and the other a pair of principals, ⟨A, B⟩, to their
symmetric shared key, Kab. K comes with an inverse operator mapping each
member of a key pair for an asymmetric cryptosystem to the other, (K+
a )−1 =
K−
a , and each symmetric key to itself, (Kab)−1 = Kab.
The subterm relation, ⊑, which we borrow from [19], is the least relation
such that: M ⊑M; M ⊑{|M1|}K if M ⊑M1; M ⊑M1; M2 if M ⊑M1 or
M ⊑M2. Notice that, for any K ∈K, K ⊑{|M|}K only if K ⊑M. We extend
⊑to a homomorphism over sets of terms in the expected manner. A message is
atomic if it is not an encrypted term or a concatenated one. A message M0 is
a component of M, if M0 ⊑M, M0 is not a concatenated term, and for every
M1 ̸= M0 such that M0 ⊑M1 ⊑M implies that M1 is a concatenated term.
Deﬁnition 1. Let R be a set of concrete, object-level messages and let I be a
mapping from A to R. Further, let M1 ≈M2 abbreviate I(M1) = I(M2). Then, I
is an implementation of A iﬀthe following holds:
∀M, M1, M2 ∈A. M1 ≈M2 →M1; M ≈M2; M
(1)
∀M, M1, M2 ∈A. M1 ≈M2 →M; M1 ≈M; M2
(2)
∀M1, M2 ∈A. ∀K ∈K. M1 ≈M2 →{|M1|}K ≈{|M2|}K
(3)
∀K, K′ ∈K. ∀M ∈A. K ≈K′ →{|M|}K ≈{|M|}K′
(4)
∀M1, M2 ∈Agent. M1 = M2 ↔M1 ≈M2
(5)
∀M1, M2 ∈Nonce. M1 = M2 ↔M1 ≈M2
(6)
∀M1, M2 ∈Timestamps. M1 = M2 ↔M1 ≈M2
(7)
∀M1, M2 ∈K. M1 = M2 ↔M1 ≈M2
(8)
Axioms (5)—(8) specify that the identiﬁcation of diﬀerent (abstract) messages
can only occur between messages of diﬀerent types. If two nonces are considered
to be diﬀerent on the abstract level then we also assume that their implementa-
tions are diﬀerent. However, we can still identify, for instance, the implementa-
tion of a nonce with the implementation of an encrypted message or a timestamp.

On the Automated Correction of Protocols with Improper Message Encoding
141
2.2
Strands, Strand Spaces and Bundles
A strand is a sequence of nodes. Every node in a strand denotes a communicating
event, where transmission (respectively reception) of a term M is denoted as +M
(respectively −M). Accordingly, we will call a node n either positive or negative
and will use msg(n) and sign(n) to respectively obtain the node term and the
communication event thereby denoted.
Let s be a strand and let ⟨s, i⟩and ⟨s, i + 1⟩denote the i-th and the i + 1-
th nodes of s. Then ⟨s, i⟩⇒⟨s, i + 1⟩. ⇒+ and ⇒∗are used to respectively
denote the transitive and the transitive-reﬂexive closure of ⇒. ⟨ss, i⟩→⟨sr, j⟩
denotes inter-strand communication, ss ̸= sr. It requires the nodes to be both
complementary, in terms of polarity, sign(⟨ss, i⟩) = + while sign(⟨sr, j⟩) = −, and
matching, in terms of the message being exchanged, msg(⟨ss, i⟩) = msg(⟨sr, j⟩).
A strand represents a protocol run from the local perspective of a participant.
If the participant is honest, the strand, as well as each individual strand node, is
said to be regular and penetrator otherwise. A strand space Σ is a set of strands,
where ⇒and →impose a graph structure on the nodes of Σ. The pair (Σ, P) is
said to be an inﬁltrated strand space, whenever Σ is a strand space and P ⊆Σ,
such that every p ∈P is a penetrator strand.
A penetrator strand hooks together several penetrator traces, which are ac-
tions that characterise the abilities of the penetrator, according to the standard
Dolev-Yao model. Our extension to the penetrator model consists of adding the
I action, which captures the penetrator’s ability of causing a type ﬂaw attack.
Deﬁnition 2 (Penetrator Trace). Let KP denote the keys initially known to
the penetrator; then, a penetrator trace is (as given in [19]) one of the following:
(M)essage: ⟨+M⟩, where M ∈Agent or M ∈Nonce;
(K)ey: ⟨+K⟩, where K ∈KP;
(F)lush: ⟨−M⟩;
(T)ee: ⟨−M, +M, +M⟩;
(C)oncatenation: ⟨−M1, −M2, +M1; M2⟩;
(S)eparation: ⟨−M1; M2, +M1, +M2⟩;
(E)ncryption: ⟨−K, −M, + {|M|}K⟩; and
(D)ecryption: ⟨−{|M|}K , +K, +M⟩;
plus the action I deﬁned by:
(I)mplementation: ⟨−M, +M ′⟩, provided that M ≈M ′ (The messages
M and M ′ are respectively called camouﬂaged and spoofed).
In an inﬁltrated strand space, (Σ, P), penetrator traces of type M are assumed
to be unable to model unguessable nonces and so the penetrator can build mas-
querading messages using M, K, F, T, C, S, E, D and I, only. Thus, to persuade
a honest principal that a camouﬂaging message can be accepted to spoof some
other, the penetrator has the obligation of proving that, at the object level, the
implementation of both messages are equal.

142
D. Hutter and R. Monroy
Deﬁnition 3. A ﬁnite, acyclic graph, B = ⟨N, (→∪⇒)⟩, is a bundle if for
every n2 ∈N, i) if sign(n2) = −, then there is a unique n1 ∈N with n1 →n2;
and ii) if n1 ⇒n2 then n1 ∈N and n1 = ⟨s, i⟩and n2 = ⟨s, i + 1⟩. Let B be
a bundle, then ≺B and ⪯B denote respectively the transitive and the transitive-
reﬂexive closure of (→∪⇒).
A bundle is said to be regular if it contains no penetrator strands and penetrator
otherwise. A message M is said to be a component of a node n if it is a component
of msg(n). A node n is said to be an M, K, etc. node if n lies on a penetrator
strand with a trace of kind M, K, etc.
In the original approach, the notion of a node originating a term is a syntac-
tical property of strands [19]:
“Let B = ⟨N, (→∪⇒)⟩be a bundle. An unsigned term M originates
at a node n ∈N if sign(n) = +; M ⊑msg(n); and M ̸⊑msg(n′), for
every n′ ⇒+ n. M is said to be uniquely originating if it originates on a
unique n ∈N.”
So, if for instance the term is a nonce, depending on whether the nonce occurs
ﬁrst in a received or in a sent message of a strand, the associated principal can
either reuse the nonce received from elsewhere or create a fresh one. Here, we
have to lift this notion of nodes that originate terms to match our extended
version of strand spaces. This is because when considering the concrete repre-
sentation of messages it is possible that diﬀerent (abstract) messages share the
same implementation. Hence, considering the reception of a message by a prin-
cipal on the abstract level allows us to conceal the fact that that principal gets
also knowledge about other messages sharing the same implementation. The only
principal caring about this ambiguity at the implementation level is, of course,
the penetrator, since he may want to tunnel a message camouﬂaged as a diﬀerent
one along a protocol run. Upon the reception of the camouﬂaged message he is
also aware of the spoofed message and the I-rule allows him to switch between
these diﬀerent (abstract) messages.
The following deﬁnition allows us to collect the information about the cam-
ouﬂage actions that the penetrator has performed up to a speciﬁc node in the
bundle.
Deﬁnition 4. Let B = ⟨N, (→∪⇒)⟩be a bundle and let n1 and n2 be nodes
in B (n1, n2 ∈N). Whenever there is an I trace in B with nodes n1 and
n2 having signed terms −M and +M ′, respectively, with M ̸≡M ′, we write
itrB(n1, −M, n2, +M ′).
Deﬁnition 5 (Scope). Let B = ⟨N, (→∪⇒)⟩be a bundle and let n ∈N.
Then, the set of spoofed actions up to node n, called the scope of n, wrt B,
written ScB(n), is given by:
ScB(n) = {M ≈M ′ : ∃n1, n2 such that itrB(n1, −M, n2, +M ′) and n2 ≺B n}
The knowledge of the camouﬂaged messages is crucial for the reﬁned notion of
originating nodes:

On the Automated Correction of Protocols with Improper Message Encoding
143
Deﬁnition 6. Let B = ⟨N, (→∪⇒)⟩be a bundle and let n ∈N be a node in
B. An atomic term M originates at n iﬀ:
– sign(n) = +;
– M ⊑msg(n);
– M ̸⊑msg(n′) for every n′ ⇒+ n and
– M ̸⊑ScB(n′) if ∃n′, M1, M2 such that itrB(n′, −M1, n, +M2).
Before concluding this section, we illustrate the notions above with example
descriptions of two protocol attack.
2.3
Woo and Lam’s Protocol Attack
For brevity, protocols will be speciﬁed using Alice and Bob notation. So a pro-
tocol is given by a sequence of steps, each of the form q. A →B : M, meaning
that, at step q, agent A sends message M to agent B, which B receives. We use
S and Spy to refer to the server and the penetrator, respectively. Consider the
Woo and Lam π1 Protocol [20]:
1. A →B : A
2. B →A : Nb
3. A →B : {|A; B; Nb|}Kas
4. B →S :
A; B; {|A; B; Nb|}Kas

Kbs
5. S →B : {|A; B; Nb|}Kbs
The Woo and Lam π1 protocol exhibits a type ﬂaw attack, which is based
on the property that the implementation of nonces can be confused with the
implementation of encrypted messages. We capture this property by reﬁning our
theory of implementation for this protocol with the corresponding additional
axiom:
∀N ∈Nonce.∃K ∈K, M ∈A.
N ≈{|M|}K
(9)
Then, the penetrator bundle illustrating a type ﬂaw attack on this protocol is
as follows:
Spy
B
p1 (+A)
−−−−−−→s1 (−A)
⇓
⇓
p2 (−Nb)
←−−−−−−s2 (+Nb)
I −trace ⇓
⇓
p3 (+ {|M|}K)
−−−−−−→s3 (−{|M|}K)
⇓
p4 (−{|A; B; {|M|}K|}Kbs) ←−−−−−−s4 (+ {|A; B; {|M|}K|}Kbs)
I −trace ⇓
⇓
p5 (+ {|A; B; Nb|}Kbs)
−−−−−−→s5 (−{|A; B; Nb|}Kbs)
This example penetrator bundle contains two I traces: the ﬁrst is ⟨−Nb, + {|M|}K⟩
and the second ⟨−{|A; B; {|M|}K|}Kbs , + {|A; B; Nb|}Kbs⟩.

144
D. Hutter and R. Monroy
The node p3 with msg(p3) = {|M|}K is an originating node for M and K
because both messages do not occur in the previous message, Nb, of the ﬁrst I
trace (when read from top to bottom). Nor do they occur in the scope of p2, with
msg(p2) = Nb (which is actually empty). In order to justify the use of the I trace,
we have to guarantee that Nb ≈{|M|}K. Since M and K originate at this I trace
we do not care about the resulting values of M and K in the spoofed message.
Selecting the witnesses (skolem-terms) of the existential quantiﬁed variables in
(9) the proof obligation is a simple instance of (9).
By contrast consider the second I trace, even though Nb does not occur in
msg(p4), the node p5 with msg(p5) = {|A; B; Nb|}Kbs is not an originating node
for Nb. However, Nb occurs in the scope of node p5:
ScB(p5) = {Nb ≈{|M|}K}.
This context knowledge in combination with axiom (2) guarantees the property
{|A; B; Nb|}Kbs ≈{|A; B; {|M|}K|}Kbs
and therefore the soundness of the I-rule application.
2.4
The KP Protocol
Snekkenes [18] discusses the KP protocol, which is subject to a type ﬂaw attack.
The protocol is as follows:
1.
A →B
: A; Na; B
2.
B →KDC : A; Na; B; Nb
3. KDC →B
:
KS; A; Nb; {|A; B; Na; KS|}Kas

Kbs
4.
B →A
: {|A; B; Na; KS|}Kas ; {|Na; Nc; B|}KS
5.
A →B
: {|Nc; A|}KS
The attack is based on the facts that:
1. on step 3, the principal B extracts {|A; B; Na; KS|}Kas from the component
encrypted under Kbs and then, on step 4, sends this extracted message to
A;
2. keys share common implementations with agents and encrypted messages;
and
3. therefore the implementation of the message in step 3 can be misinterpreted
(in a diﬀerent run) as the implementation of the ﬁrst component of the
message in step 4.
Similar to the previous example, an appropriate theory of message implemen-
tations contain axioms to deduce the fact that keys (generated dynamically by
the server) and agents (or encrypted messages, respectively) potentially share
the same implementation:
∀A ∈Agent. ∃K ∈K.
A ≈K
(10)
∀K ∈K. ∃K′ ∈K, M ∈A.
K ≈{|M|}K′
(11)

On the Automated Correction of Protocols with Improper Message Encoding
145
KDC
B
Spy
A
p1(+B; N ′
b; A)
−−−→s′
1(−B; N ′
b; A)
⇓
⇓
p2(−B; N ′
b; A; N ′
a)
←−−−s′
2(+B; N ′
b; A; N ′
a)
. . .
s1(−A; N ′
a; B)
←−−−p3(+A; N ′
a; B)
⇓
k1(−. . .) ←−−−s2(+A; N ′
a; B; N ′
b)
⇓
⇓
k2(−. . .) −−−→s3(−
n˛˛˛KS; A; Nb; {|A; B; N ′
a; KS|}Kas
˛˛˛
o
Kbs
)
⇓
s4(+ {|A; B; N ′
a; KS|}Kas ; {|N ′
a; Nd; B|}KS)
−−−→p4(−{|A; B; n′
A; KS|}KA ; {|N ′
a; Nd; B|}KS)
. . .
p5(+ {|A; B; N ′
a; KS|}Kas)
↓
p6(−{|A; B; N ′
a; KS|}Kas)
I −trace ⇓
p7(+ {|K′
S; B; N ′
a; KS|}Kas)
↓
p8(−{|K′
S; B; N ′
a; KS|}Kas)
I −trace ⇓
p9(+
˘˛˛K′
S; B; N ′
a; {|M|}K
˛˛¯
Kas)
−−−→s′
3(−
˘˛˛K′
S; B; N ′
a; {|M|}K
˛˛¯
Kas)
⇓
p10(−{|M|}K ; {|N ′
b; N ′
d; A|}K′
S)
←−−−s′
4(+ {|M|}K ; {|N ′
b; N ′
d; A|}K′
S)
I −trace ⇓
p11(+KS; {|N ′
b; N ′
d; A|}K′
S)
. . .
s5(−{|Nd; A|}KS)
←−−−p12(+ {|Nd; A|}KS)
Fig. 1. Type ﬂaw attack in the KP-protocol in [18]

146
D. Hutter and R. Monroy
Fig. 1 illustrates the resulting attack in the strand space notation using our
notion of I-traces. This example contains three I-traces. Node p7 originates the
key K′
S on the penetrator strand. Since this key has not been seen by the regular
principles, the penetrator is free to choose its value and axiom (10) applies.
Similar arguments hold for p9 originating M and K using axiom (11). The scope
of p10 is {A ≈K′
S, KS ≈{|M|}K} which trivially implies that the messages in
p10 and p11 share the same implementation.
3
Feasibility of Type Flaw Attacks
3.1
Extending Strand Space Theory
In the last section we extended the strand space notation by an axiomatic speciﬁ-
cation of messages at an implementation level and by an additional I penetrator
strand connecting this implementation level with the abstract level of strand
spaces. While the original purpose of this extension was to provide a uniform
representation language for protocol runs (potentially containing type ﬂaw at-
tacks), it is also interesting to investigate how the veriﬁcation methodology of
strand spaces can be lifted to our extended approach.
A central part of this methodology are outgoing (or ingoing, respectively)
authentication tests [10]. Suppose, a nonce N is only known by an honest princi-
pal. If this principal sends this nonce N as part of an encrypted message {|M|}K
around and later on receives the same nonce in clear text (i.e. outside its original
encryption by K) then somebody must have decrypted the message possessing
the appropriate key K−1. If this key is only knowledgeable to a single person
(and has not been sent around) then we can conclude that this particular person
has been involved in processing the message.
The question arises whether this property holds also including I-traces. Sup-
pose, N occurs outside of its encryption by K in the second node of an I-trace.
We consider two cases. First, suppose N originates at the second node of the
I-trace. In this case the message used to create N has been created without any
knowledge of N since otherwise N would be part of the scope of the particular
I-trace node, which would contradict our assumption that N originates at the
I-trace. Now, suppose that N does not originate at the I-trace node. Either N
is (unencrypted) also part of the received ﬁrst message of the I-trace and we can
use the argument of the original strand space theorem that this cannot happen
or there is a type confusion going on and both messages of the I-trace share the
same interpretation. Since the equality of implementation has to be implied by
the scope of the node (and by assumption N occurs only encrypted with K in
the scope), this can only be true iﬀthe implementation of the encryption satisﬁes
some non-standard properties (i.e. it can be simulated by some other operation
on messages without K−1).
3.2
Implementation
In our approach we separate the protocol level operating on abstract messages
from its implementation level. This is in contrast to many other approaches that

On the Automated Correction of Protocols with Improper Message Encoding
147
encode implementation details in an equality theory on (abstract) messages.
The beneﬁt is that we can use (arbitrary) algebraic speciﬁcations to formalise
properties (in particular equality and inequality) of the message implementa-
tion. This knowledge about the implementation is used to verify side condi-
tions of I-traces. In order to apply I-traces in the penetrator bundle we have
to make sure that both messages of the trace share the same implementation.
This is a task that can be given to an automated theorem prover or to spe-
cialised deduction system incorporating domain speciﬁc knowledge (e.g. SMT-
provers).
Axioms (1)—(8) reﬂect only the minimal requirements to an implementation.
Typically, one would extend this set of axioms by a more detailed speciﬁcation
of how messages and their operations are implemented. It is easy to formalise
a message implementation theory that takes care of the length of messages, i.e.
that assumes that all types of atomic messages have a speciﬁc length. Then, rea-
soning about such a theory requires arithmetic (and in the worst case properties
of least common multiples). Another possibility is use Meadow’s probabilistic ap-
proach [15] to reason about the equality of the implementations of two messages.
However, this is still future work.
The axiomatisation given in our examples is idealistic in a sense that we deﬁne
possible type ﬂaws regardless of the concrete instances of a nonce or key. It might
be the case that there are some nonces that do not share the implementation
with any encrypted message. However, incorporating this sort of probabilistic
information would cause a far more complex speciﬁcation of the implementation
theory and therefore we stick to a generalisation of such properties which result
in a worst case analysis.
The execution of the I-rule is trivial as it only forwards a received message
duping the receiver of the message that it would be a diﬀerent abstract message.
The crucial question is then: how likely is it that a penetrator can betray the
receiver in such a way? The philosophy behind the design of strand spaces was
that the independent choice of two nonces will result in two diﬀerent nonces.
This philosophy is implemented in demanding that nonces have to be uniquely
originating in bundles. Similarly, we typically assume that choosing indepen-
dently two diﬀerent abstract messages will also result in two diﬀerent messages
on the implementation level.
I traces impose the restriction on the occurring messages that their implemen-
tations have to be equal. In contrast to standard bundles, as introduced in [19],
the property of being a bundle depends on the theory of implementations and for
each proposed instance ⟨−t, +t′⟩of an I trace we have to verify the constraints
that t ≈t′. An atomic message originating in t′ can be “freely” chosen by the
penetrator who selects an appropriate instance the implementation of which is
equal to the corresponding bits of the implementation of t. If the message does
not originate in t′ its value is ﬁxed by the bundle at some other node and we
have to prove that its implementation is equal to the corresponding bits of the
implementation of t.

148
D. Hutter and R. Monroy
4
A Method for Protocol Repair
Repairing a security protocol susceptible to a type ﬂaw attack can be, accord-
ingly, achieved at the abstract level, at the concrete level or at a combination
thereof. Protocol repair at the abstract level involves changing the structure of
the camouﬂaged message so that it can no longer spoof the other (or vice versa).
By contrast, protocol repair at the concrete level is achieved by modifying the
theory of implementation underlying the input faulty protocol; these changes
ought to guarantee that each application of the I-rule is no longer sound. We
now elaborate on protocol repair at the abstract level.
4.1
Protocol Repair at the Abstract Level
The intruder can elaborate a type ﬂaw attack on a protocol whenever he is able
to make a protocol principal accept a message of one type, M, as a message of
another, M ′. In that case, M and M ′ share the same implementation, M ≈M ′.
Thus, to remove the protocol type ﬂaw, it suﬃces to break M ≈M ′.
Following [12], we approach protocol repair by translating some of Abadi and
Needham’s informal principles for the design of security protocols [1] into formal
requirements on sets of protocol steps. In particular, here we focus on principle
10: A message is properly encoded if it is possible to deduce that the message
belongs to a protocol, including the protocol step, and in fact to a particular run
of the protocol [1].
We attempt to achieve proper encoding of a message M by rearranging its
structure while keeping its meaning intact. If this operation does not suﬃce to
break the confusion between M and M ′, we may then insert into M vacuous
terms, tags actually, as in [11]. Finally, when incurring on these changes, we
make sure that the new protocol message does not clash with some other. The
following deﬁnitions are used in the deﬁnition of our patch method.
Deﬁnition 7 (Visible Content). The visible content ctS(M) of a message M
wrt a set of keys S is given by:
ctS(M) = {M}
if M is atomic
ctS(M1; M2) = ctS(M1) ∪ctS(M2)
ctS({|M|}K) =
ctS(M)
if K−1 ∈S
∅
if K−1 /∈S
Deﬁnition 8. Two messages M and M ′ are equivalent under rearrangement,
M ≡M ′ for short, iﬀctS(M) = ctS(M ′) for all sets of keys S.
Example 1. {|A; B; M|}Kas ≡{|M; A; B|}Kas, because either Kas ̸∈S then
ctS({|A; B; M|}Kas) = ∅= ctS({|M; A; B|}Kas)
or Kas ∈S then
ctS({|A; B; M|}Kas) = ctS(A; B; M) = {A, B} ∪ctS(M)
= ctS(M) ∪{A, B} = ctS(M; A; B) = ctS({|M; A; B|}Kas)

On the Automated Correction of Protocols with Improper Message Encoding
149
Name: type ﬂaw
Input: P ∈Σ, B, BR
Preconditions:
% Spy reuses cypher-text {|M|}K:
∃i, i′, j, M, K.
⟨sr, i⟩≺B ⟨sp, i′⟩⪯B ⟨sq, j⟩∧
{|M|}K ⊑msg(⟨sr, i⟩) ∧{|M|}K ⊑msg(⟨sp, i′⟩) ∧{|M|}K ⊑msg(⟨sq, j⟩)
% Strand sp corresponds to the penetrator, while
% Strand sq to the principal being deceived.
∧⟨sr, i⟩and ⟨sq, j⟩are regular but ⟨sp, i′⟩is not
∧sign(⟨sr, i⟩) = + = sign(⟨sp, i′⟩) but sign(⟨sq, j⟩) = −
% The message being replayed lies on an I-trace
∧itrB(sp, −{|M|}K , s′
p, + {|M ′|}K′) with s′
p →sq
Repair:
% Break similarity of {|M|}K:
select M ′′ such that {|M|}K ≤A0 {|M ′′|}K, where A0 ⊆Tags is a minimal
set of tags, and such that {|M ′′|}K is collision free with respect to
L = {{|M ′|}K′ : {|M ′|}K′ ⊑msg(n), n ∈BR}.
Replace {|M ′′|}K for {|M|}K in P.
Fig. 2. The type-ﬂaw abstract-level patch method
Deﬁnition 9 (Monotonicity). Let M, M ′ be messages and let A0 ⊆Tags be a
set of tags. M ≤A0 M ′ iﬀctS(M) ⊆ctS(M ′) and ctS(M ′) ⊆ctS(M) ∪ctS(A0)
for all S ⊆K.
Deﬁnition 10 (Collision Freeness). Let S be a set of keys, let M be a mes-
sage and let A′ ⊆A be a set of messages. M is collision free with respect to A′
iﬀfor all M ′ ∈ctS(M) and for all M ′′ ∈A′ it is the case that M ′ ̸≈M ′′.
Our method for the repair of a security protocol susceptible to a replay attacks is
shown in Fig. 2.2 It is given as a patch method [12]. A patch method is a 4-tuple
(name, input, preconditions, patch). The ﬁrst component is the name of the
method. The second component is the input, the description of faulty protocol
P, a bundle B describing the attack, and a representative bundle BR describing
the intended run of the protocol. The third component is the preconditions, a
formula written in a meta-logic that the input objects must satisfy. We use these
preconditions to predict whether the associated patch will make the protocol no
longer susceptible to the attack. The fourth component is the patch, a procedure
specifying how to mend the input protocol.
When input the Woo and Lam π1 protocol, and upon the proviso that:
∀A ∈Agent, N ∈Nonce. A ̸≈N
(12)
2 Notice that we consider encrypted message components only, since a replay attack
builds on the re-use of non-trivial messages.

150
D. Hutter and R. Monroy
∀M1, M2, M3, M4 ∈A. M1 ≈M3 ∧M2 ≈M4 →M1; M2 ≈M3; M4
(13)
the abstract-level type ﬂaw method repairs it, yielding:3
1. A →B : A
2. B →A : Nb
3. A →B : {|A; B; Nb|}Kas
4. B →S : {| {|A; B; Nb|}Kas; A; B |}Kbs
5. S →B : {|A; B; Nb|}Kbs
To be able to apply the I rule with this mended protocol, the penetrator would
have to prove:
∀A, B, Nb. ∃M ′, K′. {|A; B; Nb|}Kbs ≈{|{|M ′|}K′ ; A; B|}Kbs
which, by (13), requires in turn that:
∀A, B, Nb. ∃M ′, K′. (A ≈{|M ′|}K′ ∧B = A ∧Nb ≈B)
Which contradicts (12). We now elaborate on protocol repair at the concrete
level.
4.2
Protocol Repair at the Concrete Level
We assume an underlying theory of message implementation, I. I accommodates
axioms (1)—(8) as well as others that are particular to a concrete development
of messages. Example of these kinds of additional axioms may involve:
∀A ∈Agent, K ∈K. A ̸≈K
Disjunction of Agent 1
∀A ∈Agent, N ∈Nonce. A ̸≈N
Disjunction of Agent 2
∀A ∈Agent, T ∈Timestamps. A ̸≈T
Disjunction of Agent 3
∀A ∈Agent, M ∈A, K ∈K. A ̸≈{|M|}K
Disjunction of Agent 4
∀M, M ′ ∈A. M ̸≈M; M ′
Monotonicity of Concatenation 1
∀M, M ′ ∈A. M ̸≈M ′; M
Monotonicity of Concatenation 2
In general, axioms other than basic are either about type disjunction or about
properties of compound messages, such as type length, c.f. (13).
Clearly, at the implementation level, a type ﬂaw attack is possible because I
contains axioms that overlook a means of explicitly providing type disjunction.
For instance, the type ﬂaw attack on the Woo and Lam π1 protocol is justiﬁed
when I contains axiom (9), where an encrypted message can be confused with
a nonce. Repairing a protocol at the concrete level can thus be achieved by
removing from I these kinds of overlooking axioms while explicitly adding their
negation. This way the I trace is no longer applicable. The method presented in
Fig. 3 follows this idea. It makes use of the following deﬁnitions:
3 Protocol changes are enclosed within a solid box to ease reference.

On the Automated Correction of Protocols with Improper Message Encoding
151
Name: type ﬂaw
Input: P ∈Σ, B, BR
Preconditions:
% Spy reuses cypher-text {|M|}K:
∃i, i′, j, M, K.
⟨sr, i⟩≺B ⟨sp, i′⟩⪯B ⟨sq, j⟩∧
{|M|}K ⊑msg(⟨sr, i⟩) ∧{|M|}K ⊑msg(⟨sp, i′⟩) ∧{|M|}K ⊑msg(⟨sq, j⟩)
% Strand sp corresponds to the penetrator, while
% Strand sq to the principal being deceived.
∧⟨sr, i⟩and ⟨sq, j⟩are regular but ⟨sp, i′⟩is not
∧sign(⟨sr, i⟩) = + = sign(⟨sp, i′⟩) but sign(⟨sq, j⟩) = −
% The message being replayed lies on an I-trace
∧itrB(sp, −{|M|}K , s′
p, + {|M ′|}K′) with s′
p →sq
Repair:
Let A={Ax : Ax ⊢(M ≈M ′)↓RB∪RC, with Ax ∈I and M ≈M ′ ∈ScB(s′
p)}
Let I ←(I \ A) ∪{Cl(¬Mtrx(Ax)) : Ax ∈A}
Fig. 3. The type-ﬂaw concrete-level patch method
Deﬁnition 11. A term-rewriting system (TRS), R, is a set of rewrite-rules,
each of which is of the form L :⇒R. A rule L :⇒R applies to a term t (in A) if
a subterm s of t matches L with some substitution σ of terms (in A) for variables
appearing in L. The rule is applied by replacing s in t with the corresponding
right-hand side Rσ of the rule. We write t :⇒t′ to indicate that the term t is
transformed into t′ by an application of this rewriting. A TRS is terminating iﬀ
the sequence t1 :⇒. . . :⇒tn is ﬁnite. tn is the normal form of t1. Let R be a
terminating TRS; then, we write t↓R to denote the normal form of t wrt R.
Deﬁnition 12. Let A be a ﬁrst-order logic formula. Then Mtrx(A) is the matrix
of A, a formula just as A except that it contains no quantiﬁers, while Cl(A)
returns the closure of A, a formula obtained from A by attaching a universal
quantiﬁer to it for each of its free variables.
Since we reason backwards, from a formula to the axioms, implications can be
used as rewrite-rules from right to left. We use RB and RC to respectively
denote the set of rewrite-rules gained from (1)—(8) and from the axioms stating
properties of compound messages, such as (13).
When input the KP protocol, and under the assumption that we use (1)—(8)
and (13) as rewrites, the concrete-level type ﬂaw patch method will remove (10)
and (11) from I, while inserting into it the following axioms:
∀A ∈Agent, K ∈K. A ̸≈K
∀K, K′ ∈K, M ∈A. K ̸≈{|M|}K′

152
D. Hutter and R. Monroy
5
Related Work
In the past several approaches have been developed to deal with potential type-
ﬂaw attacks. This work can be classiﬁed into two diﬀerent areas. The ﬁrst area
is concerned with changing the representation of messages to prevent type-ﬂaw
attacks in the ﬁrst place. Heather et al. [11] use a tagging of messages to identify
the type of a message in a unique way. Considering original messages combined
with their tagging as new atomic entities, such messages constitute a generated
algebraic datatype with non-overlapping ranges of the constructors satisfying the
most important properties of the abstract message theory. Since tags themselves
will reveal information about a message to a penetrator there are several reﬁne-
ments of this tagging approach to minimize the set of subterms of a message
that have to be tagged (e.g. [13,14]).
The second area is concerned with the veriﬁcation of protocols that may
contain type-ﬂaw attacks. A prominent approach is to replace the standard
representation of messages as a freely generated datatype by a more involved
datatype dealing with equations between constructor terms. As a consequence
terms representing messages have to be uniﬁed modulo a theory modelling the
equality relation on messages [4,7,9]. While this approach assumes that only en-
tire messages can be interpreted in diﬀerent ways, Meadows [15,16] investigated
the problem that the implementation of a message could be cut into pieces and
one of such pieces might be used to mock another message, e.g. some part of a
bit string representing an encrypted message is reused as a nonce. In her model
messages are represented as bit streams. Based on an information ﬂow analy-
sis of the protocol and the knowledge of how abstract messages are represented
as bit streams probabilities are computed as to how likely a message could be
guessed (or constructed) by a penetrator. This is in contrast to our possibilis-
tic approach in which we abstract from unlikely events, e.g. that independently
guessing a key and a nonce will result in messages that share the same imple-
mentation. This is reﬂected in our notion of originating messages occurring on
I traces. The strand space approach excludes protocol runs in which messages
are not uniquely originating resulting in a possibilitic approach in a somewhat
idealised world. In our approach, for instance, a nonce can be only camouﬂaged
by a message which itself is camouﬂaged at some point with the help of the same
nonce (c.f. deﬁnition 6).
6
Conclusions
We presented an extension of the strand space approach to deal with type ﬂaw
attacks. We separated the level of abstract protocols from the implementation
level the properties of which are speciﬁed by algebraic speciﬁcations and thus can
be explored by automated theorem provers. While in the ﬁrst place we developed
this extension as an underlying notation for repairing faulty protocols, it seems
to be promising also to investigate how the theory of protocol veriﬁcation based
on strand spaces can be carried over to our extended notion. The extension of

On the Automated Correction of Protocols with Improper Message Encoding
153
strand spaces allows us to formulate heuristics to repair faulty protocols (due
to type ﬂaw attacks). It is interesting to see that there are typically two ways
to overcome type ﬂaw attacks. On one hand we can change the implementation
in order to avoid the particular equalities on implemented messages and on the
other hand we can change the protocol on the abstract level in order avoid
situations in which one can exploit these properties in the implementation.
References
1. Abadi, M., Needham, R.: Prudent engineering practice for cryptographic protocols.
IEEE Transactions on Software Engineering 22(1), 6–15 (1996)
2. Anderson, R., Needham, R.: Robustness principles for public key protocols. In:
Coppersmith, D. (ed.) CRYPTO 1995. LNCS, vol. 963, pp. 236–247. Springer,
Heidelberg (1995)
3. Aura, T.: Strategies against replay attacks. In: Proceedings of the 10th IEEE Com-
puter Security Foundations Workshop, CSFW 1997, pp. 59–69. IEEE Computer
Society, Los Alamitos (1997)
4. Basin, D., M¨odersheim, S., Vigan`o, L.: Algebraic intruder deductions. In: Sutcliﬀe,
G., Voronkov, A. (eds.) LPAR 2005. LNCS, vol. 3835, pp. 549–564. Springer, Hei-
delberg (2005)
5. Basin, D.A., M¨odersheim, S., Vigan`o, L.: Ofmc: A symbolic model checker for secu-
rity protocols. International Journal of Information Security 4(3), 181–208 (2005)
6. Carlsen, U.: Cryptographic protocols ﬂaws. In: Proceedings of the 7th IEEE Com-
puter Security Foundations Workshop, CSFW 1994, pp. 192–200. IEEE Computer
Society, Los Alamitos (1994)
7. Chevalier, Y., Rusinowitch, M.: Combining intruder theories. In: Caires, L., Ital-
iano, G.F., Monteiro, L., Palamidessi, C., Yung, M. (eds.) ICALP 2005. LNCS,
vol. 3580, pp. 639–651. Springer, Heidelberg (2005)
8. Chevalier, Y., Vigneron, L.: Automated unbounded veriﬁcation of security pro-
tocols. In: Brinksma, E., Larsen, K.G. (eds.) CAV 2002. LNCS, vol. 2404, pp.
324–337. Springer, Heidelberg (2002)
9. Gao, H., Bodei, C., Degano, P.: A formal analysis of complex type ﬂaw attacks on
security protocols. In: Meseguer, J., Ro¸su, G. (eds.) AMAST 2008. LNCS, vol. 5140,
pp. 167–183. Springer, Heidelberg (2008)
10. Guttman, J.-D., Thayer, F.-J.: Authentication tests and the structure of bundles.
Theoretical Computer Science 283(2), 333–380 (2002)
11. Heather, J., Lowe, G., Schneider, S.: How to prevent type ﬂaw attacks on secu-
rity protocols. In: Proceedings of the 13th IEEE Computer Security Foundations
Workshop, CSFW 2000, pp. 255–268. IEEE Computer Society, Los Alamitos (2000)
12. L´opez-Pimentel, J.C., Monroy, R., Hutter, D.: On the automated correction of
security protocols susceptible to a replay attack. In: Biskup, J., L´opez, J. (eds.)
ESORICS 2007. LNCS, vol. 4734, pp. 594–609. Springer, Heidelberg (2007)
13. Malladi, S., Alves-Foss, J.: How to prevent type-ﬂaw guessing attacks on password
protocols. In: Proceedings of the 2003 Workshop on Foundations of Computer
Security (FCS 2003), pp. 1–12. Technical Report of University of Ottawa (2003)
14. Malladi, S., Alves-Foss, J., Heckendorn, R.: On preventing replay attacks on se-
curity protocols. In: Proceedings of the International Conference on Security and
Management, ICSM 2002, pp. 77–83 (2002),
http://citeseer.ist.psu.edu/malladi02preventing.html (retrieved February
1, 2007)

154
D. Hutter and R. Monroy
15. Meadows, C.: Identifying potential type confusion in authenticated messages. In:
Foundations of Computer Security 2002 (2002)
16. Meadows, C.: A procedure for verifying security against type confusion attacks. In:
Proceedings of the 16th IEEE Computer Security Foundations Workshop, CSFW
2003, pp. 62–74. IEEE Computer Society, Los Alamitos (2003)
17. Moore, J.H.: Protocol failures in cryptosystems. Proceedings of the IEEE 76(5),
594–602 (1988); reprinted in: Simmons, G.J. (ed.)Contemporary Cryptology: The
Science of Information Integrity, pp. 541–558. IEEE Computer Science Press, Los
Alamitos (1991)
18. Snekkenes, E.: Roles in cryptographic protocols. In: Proceedings of the 1992 IEEE
Computer Society Symposium on Research in Security and Privacy, pp. 105–119.
IEEE Computer Society Press, Los Alamitos (1992)
19. Thayer, F.-J., Herzog, J.-C., Guttman, J.-D.: Strand spaces: Proving security pro-
tocols correct. Journal of Computer Security 7(2-3), 191–230 (1999)
20. Woo, T.Y.C., Lam, S.S.: A lesson on authentication protocol design. Operating
Systems Review 28(3), 24–37 (1994)

Finite Models in FOL-Based
Crypto-Protocol Veriﬁcation
Jan J¨urjens1 and Tjark Weber2
1 Open University (UK), Microsoft Research, Cambridge,
and Robinson College (Univ. Cambridge)
http://www.jurjens.de/jan
2 Computer Laboratory, University of Cambridge
tw333@cam.ac.uk
Abstract. Cryptographic protocols can only be secure under certain
inequality assumptions. Axiomatizing these inequalities explicitly is prob-
lematic: stating too many inequalities may impair soundness of the ver-
iﬁcation approach. To address this issue, we investigate an alternative
approach (based on ﬁrst-order logic) that does not require inequalities
to be axiomatized. A derivation of the negated security property exhibits
a protocol attack, and absence of a derivation amounts to absence of the
investigated kind of attack.
We establish a fragment of FOL strictly greater than Horn formulas in
which the approach is sound. We then show how to use ﬁnite model gen-
eration in this context to prove the absence of attacks. To demonstrate
its practicality, the approach is applied to several well-known protocols,
including ones relying on non-trivial algebraic properties. We show that
it can be used to deal with inﬁnitely many principals (and thus sessions).
1
Introduction
Cryptographic protocol analysis often models operations on messages in terms
of a free algebra. Encryption of a message m with a key k, for instance, may
be represented as the term e(k, m). Decryption can be represented either im-
plicitly, by including a rule that says that if a principal knows a (symmetric)
key k and an encrypted message e(k, m), then he can also learn m, or explic-
itly, using a decryption operator d that is assumed to satisfy a cancellation rule
d(k, e(k, m)) = m. The knowledge of a Dolev-Yao attacker is deﬁned inductively:
he knows any messages that are transmitted over an insecure channel, and he
can learn new messages by performing, e.g., encryption, decryption (provided
he knows the key), and by sending messages to other principals to learn their
response.
Typically automated or interactive theorem proving is used with the free alge-
bra model to establish security properties of protocols [1]. Security proofs, how-
ever, must make crucial use of freeness: to show that transmitting a message m
over an insecure channel does not reveal a secret s, we of course have to assume
that s is distinct from m. Since the attacker can encrypt, we also have to as-
sume that s is distinct from e(m, m), e(m, e(m, m)), etc. Moreover, to show that
P. Degano and L. Vigan`o (Eds.): ARSPA-WITS 2009, LNCS 5511, pp. 155–172, 2009.
c
⃝Springer-Verlag Berlin Heidelberg 2009

156
J. J¨urjens and T. Weber
the attacker cannot learn s, we have to assume that the attacker’s only means
of gaining knowledge are according to the rules of the inductive deﬁnition, i.e.,
that his knowledge is understood as a least ﬁxed point—because without this
understanding, the attacker might know everything. Theorem provers usually
implement an “open world” semantics, where only those statements are prov-
ably false that hold in no model of the axioms. To show secrecy, distinctness and
least ﬁxed-point axioms therefore have to be asserted explicitly in these systems.
This is sometimes done automatically [2]. Otherwise, however, it can be te-
dious and error-prone. Stating too few inequalities may impair completeness of
the veriﬁcation approach, and stating too many may impair soundness. One has
to be careful not to assert erroneous axioms, which might render the protocol
formalization inconsistent and allow one to prove anything (e.g., security of a
protocol that is in fact insecure).
In this paper we investigate the alternative approach that one can negate
the conjecture about a protocol’s security: instead of showing that the attacker
does not know a secret s, we attempt to prove that he knows s. A proof of this
conjecture corresponds to an attack, and the proven absence of a proof (equiv-
alently, by soundness and completeness of ﬁrst-order logic, a counterexample to
the conjecture) corresponds to security of the protocol.1
Note that this approach allows us to establish security of the protocol without
stating any freeness or ﬁxed-point axioms. It is related to the famous “closed
world assumption”, which was popularized by Prolog and is now also used in,
e.g., the ProVerif protocol veriﬁer [4]. The closed world assumption gives rise to
the treatment of negation as failure: every statement that cannot be proved is
considered false.
Thus model generation can be used to prove protocols secure: a simple 2-
element model (in which m and s are interpreted diﬀerently) suﬃces to show
that transmission of m does not necessarily reveal the secret s to the attacker.
In Sect. 2, we will show that this is in fact suﬃcient to conclude secrecy of s in
the free algebra model.
In Sect. 3, we demonstrate practicality of the approach by applying it to
abstract ﬁrst-order formalizations of three well-known protocols: the RSA Prob-
abilistic Signature Scheme (RSA-PSS), the Wired Equivalent Privacy (WEP)
protocol, and the Needham-Schroeder-Lowe (NSL) authentication protocol. Sec-
tion 4 concludes.
2
FOL-Based Crypto-Protocol Veriﬁcation: Derivations
vs. Models
We consider protocol formalizations in ﬁrst-order logic, as e.g. in [4,5]. We re-
strict ourselves to secrecy properties in this paper. Blanchet [6] discusses how
authentication can be treated in this framework.
1 A similar approach was taken in [3], although without investigating the questions
we consider in this paper.

Finite Models in FOL-Based Crypto-Protocol Veriﬁcation
157
Central to the formalization of a protocol is a unary predicate knows (deﬁned
inductively) that describes which messages are known to a Dolev-Yao attacker.
Showing security of a protocol then amounts to showing that the attacker does
not know a certain secret in the free algebra model.
However, when giving a proof that the axioms of a protocol formalization
imply that the attacker does not know the secret, we implicitly have to consider
all models of the axioms. In some of these models, equalities may hold that one
would expect an implementation of the protocol to avoid: e.g., that the secret
is equal to a publicly known value. To consider the free algebra model only,
traditionally further axioms must be asserted: namely freeness of operations (to
prevent unwanted confusion in the model), and a least ﬁxed-point axiom for
the knows predicate (to prevent the attacker from knowing messages without
reason).
Stating these axioms explicitly seems inelegant. It introduces the risk of in-
cluding too few distinctness axioms, impairing completeness of the veriﬁcation
approach by giving rise to spurious attacks (based on accidental equalities be-
tween diﬀerent terms). Including too many axioms, on the other hand, might
lead to an inconsistent protocol speciﬁcation, thereby impairing soundness of the
veriﬁcation approach. We would like to avoid these dangers.
There is an alternative to asserting these axioms, and consequently proving
security in the free algebra model. We will show that for many protocols, security
can be established by exhibiting just one (arbitrary) model in which the attacker
does not know the secret.
2.1
Model-Theoretic Foundations
Note that automated theorem provers implicitly consider every possible model
satisfying the given axioms to see whether it satisﬁes the given conjecture, not
only the quotients of the free algebra under the axioms. This means that in the
models considered, additional properties not following from the given axioms
may hold. In the case of cryptographic protocols, this may mean that a secret
key coincides with a public value and therefore becomes known to the adversary.
This is of course something which one would assume an implementation of the
protocol to avoid, and therefore one would like to analyze the protocol under
the assumption that this does not happen. There are two ways to deal with this
situation:
1. If one wants to formulate the conjecture in a way that a proof of the conjec-
ture means that the protocol is secure, one needs to explicitly include axioms
which prevent an unwanted collapse of the model (for example, axioms re-
quiring each constant in the model describing a secret value to be diﬀerent
from any other value).
2. Alternatively, one can formulate the conjecture in a negated way so that a
proof of the conjecture corresponds to an attack, and the proven absence
of a proof (equivalently, by soundness and completeness of FOL, a counter-
example to the formula) corresponds to the security of the protocol. This

158
J. J¨urjens and T. Weber
makes sure that, when considering a given protocol execution (i.e., a given
instantiation of the message variables), all models of the formulas have to
fulﬁll the attack conjecture in order for an attack to be detected, in particular
also the quotient of the free algebra under the formula, which does not satisfy
any equations that would be assumed not to hold in an implementation of
the protocol (for example, between a secret key and a public value) and
which we call a confusion-free model (to be deﬁned precisely below). That
way, false positives arising in this way can be avoided.
We would like to avoid having to introduce the distinctness axioms in the ﬁrst
option, since it would introduce the risk of including too few distinctness axioms
(again giving rise to false counter-examples to the security of the speciﬁcation),
or too many (which may lead to an inconsistency of the formula on the whole).
The second option, however, raises an issue with respect to the completeness
of the analysis: Note that in general it is not the case that any formula which
holds in the quotient of a free algebra under given axioms also holds in every
other model of the axioms. Therefore, in the second approach explained above,
ﬁnding a counter-example which satisﬁes the axioms but not the conjecture does
not in general have to mean that the confusion-free models of the axioms do not
satisfy the conjecture either. This, however, is what one would like to establish
here to show that the program is secure in a certain sense. Intuitively speaking,
having a counter-example just means that there exists an implementation of the
protocol which happens to be secure against the conjecture, maybe only because
certain non-standard equalities happen to hold, whereas one would like to show
that any reasonable implementation of the protocol, in particular the ones which
correspond to the confusion-free models, is secure.
Therefore, we would like to establish under which conditions on the set of
axioms and which conditions on the conjecture it is indeed the case that the
following logical equivalence holds: There exists a model which satisﬁes the ax-
ioms but not the conjecture if and only if the confusion-free models satisfying
the axioms do not satisfy the conjecture.
We consider the negated security conjecture, i.e., that the attacker knows the
secret. A proof of this “insecurity conjecture” corresponds to an attack on the
protocol. Since the proof implicitly considers all models of the protocol axioms,
the attack cannot depend on accidental equalities; it will be possible in the free
algebra model as well.
Recall that if a conjecture cannot be derived from a given set of axioms,
this does not mean that one can derive the negation of the conjecture from the
axioms. The conjecture may just be independent of the axioms, i.e., it may hold
in some models satisfying the axioms, but not in others. In general it is not the
case that any formula which holds in the free algebra model also holds in every
other model of the axioms.
We now identify conditions on protocol formalizations that are suﬃcient to
conclude security (i.e., that the attacker does not know the secret in the free
algebra model) from the proven absence of a proof of the “insecurity conjec-
ture” (or equivalently, by soundness and completeness of ﬁrst-order logic, from

Finite Models in FOL-Based Crypto-Protocol Veriﬁcation
159
a counterexample). Note that this is closely related to the treatment of negation
as failure, which is justiﬁed in, e.g., Prolog by the “closed world assumption”.2
Let us recall some concepts from mathematical logic. We observe that a for-
malization of security protocols in ﬁrst-order logic (or more precisely, of the
corresponding knows predicate describing the attacker’s knowledge) can often
be given by strict Horn clauses. The following deﬁnition is standard [9].
Deﬁnition 1 (Strict Horn Clause). A strict Horn clause is a formula that
consists of universal (ﬁrst-order) quantiﬁers followed by a quantiﬁer-free formula
of the form ϕ (a fact), or ϕ1 ∧. . . ∧ϕn =⇒ϕ (a rule), where the formulas ϕ1,
. . . , ϕn, ϕ are all atomic.
Theories consisting of strict Horn clauses always have an initial model. This is
known as the initial model theorem [9].
Theorem 1 (Initial Model Theorem). Let T be a theory consisting of strict
Horn clauses. Then T has a model A with the property that for every model B
of T there is a unique homomorphism from A to B. (Such a model A is called
an initial model of T . It is unique up to isomorphism.)
The initial model satisﬁes the no junk and no confusion properties; its universe is
indeed the freely generated term algebra of messages. Moreover, the initial model
(also known as the least Herbrand model) satisﬁes only those atomic sentences
that are derivable from T . Thus, the initial model is precisely the free algebra
model that we are interested in. Moreover, if we can ﬁnd any model B in which
the attacker does not know the secret, then the existence of a homomorphism
from the initial model A to B implies that the attacker does not know the
secret in the initial model either. (Recall that a homomorphism h from A to B,
by deﬁnition, preserves satisfaction: if A |= knows(s), then B |= knows(h(s)).
Therefore, by contraposition, if B ̸|= knows(h(s)), then A ̸|= knows(s).)
Theorem 1 also applies to theories that contain equality. (For a proof sketch,
note that equality is a predicate that can be axiomatized by strict Horn clauses.
We then obtain the initial model by taking equivalence classes of elements. A de-
tailed proof can be found in [9].) Therefore the theorem covers both implicit and
explicit decryption (cf. Sect. 1). More generally it covers varieties, i.e., algebraic
structures deﬁned by identities.
We would like to investigate now to what extent Thm. 1 can be strengthened
to cover a larger fragment of FOL exceeding strict Horn clauses, in particular
wrt. confusion-freeness in crypto-protocol veriﬁcation.
For the purposes of this paper, we are only concerned with the additional
knowledge that the adversary may gain from exploiting certain equalities in spe-
ciﬁc models of the axioms (which correspond to implementations of the protocol
speciﬁcation). One could also investigate other kind of relations besides equal-
ities which the adversary may exploit to gain knowledge, although we believe
2 It is interesting to note that there are non-standard interpretations of classical logic
formulas such as inductive logic [7] that also enforce certain “closed world” assump-
tions and that have recently been used for crypto-protocol veriﬁcation [8].

160
J. J¨urjens and T. Weber
that in many cases these could be modeled by equalities, if necessary introducing
extra function operators.
Therefore, we are interested in the models M of a given set A of axioms in
the (logical) signature Σ (containing function and relation symbols) that are
confusion-free in the following sense: for all terms t1, t2 in Σ with free variables
x, if the formula ∀x. t1 = t2 holds in M, then it holds in all models of A.
Suppose we are given a signature Σ and a set A of axioms and a conjecture c.
We would like to establish suﬃcient conditions on A and c that imply existence
of a set G of confusion-free models such that if all models in G satisfy c, then
each model of A satisﬁes c.
Our approach is to ﬁnd a way to construct all models of A out of the models
in G in a way that preserves satisfaction of c. We recall a few deﬁnitions from
algebraic model theory [10]. A limit sentence is a sentence of the form
(∀x1, . . . , xn)(φ(x1, . . . , xn) =⇒(∃!y1, . . . , ym)ψ(x1, . . . , xn, y1, . . . , ym)).
Note that, in particular, universal Horn sentences are limit sentences. However,
limit sentences are strictly more expressive than universal Horn sentences: for
example, one can show that the class of algebras satisfying the limit formula
∀x. (α(x)∧β(x) =⇒∃!y. ρ(x, y)) is not deﬁnable by a universal Horn theory [10,
p.210].
A subclass S of a class M of models is called reﬂective if for every structure
M in M there exists a structure S in S and a homomorphism r : M →S (the
reﬂection homomorphism) such that for every structure S′ in S and for every
homomorphism f : M →S′ there exists a unique homomorphism f ′ : S →S′
such that f = f ′ ◦r.
Theorem 2. Suppose that A is a set of limit sentences, c a universally quan-
tiﬁed conjunction of atomic formulas, and G the set of reﬂections of the free
Σ-structures on ﬁnitely many generators under the sentences in A. Then the
structures in G are confusion-free models such that if all models in G satisfy c,
then each model of A satisﬁes c.
Proof. Let T be a set of limit sentences and A the class of models of T . One can
show that the class of A-structures is closed under limits and directed colimits in
the class of Σ-structures. By [10, 2.48], this implies that the class of A-structures
is reﬂective in the class of Σ-structures.3 Therefore, the set G of reﬂections of
the free Σ-structures on ﬁnitely many generators under the sentences in A is
confusion-free. Note that every Σ-structure is a directed colimit of the free Σ-
structures on ﬁnitely many generators. This implies that every A-structure is a
directed colimit of the structures in G. Also, it follows that every A-structure
satisﬁes any universally quantiﬁed conjunction c of atomic formulas satisﬁed
by all structures in G (because these are preserved by quotients and, as limit
sentences, by directed colimits). This proves the above theorem.
3 Note that it follows from [11] that the converse does not hold, that is a reﬂective
subclass of a class of structures which is closed under colimits need not be deﬁnable
by a limit theory.

Finite Models in FOL-Based Crypto-Protocol Veriﬁcation
161
Theorem 2 covers approaches such as [12] (formalizing BAN logic),4 as well as
[3,13] (that use similar formalizations as ours here). [13] also remarks that a sig-
niﬁcant number of protocols and security requirements can in fact be expressed
using Horn formulas. [14] uses the (arguably inelegant) approach that the pro-
tocol speciﬁer has to explicitly introduce relevant inequalities (such as diﬀerent
public keys being unequal). The paper does not comment on how to deal with
a possible incompleteness or inconsistency of the resulting axioms. All of the
above approaches do not explicitly consider the issue of soundness with respect
to cryptographic inequalities raised above.
It would be worthwhile to consider whether Thm. 2 can be generalized from
limit theories to basic theories deﬁned in [10, 5.31], but we do not need this here.
Note that the restrictions on A cannot be relaxed arbitrarily. For instance,
with A = {e(k, a) ̸= e(k, b) =⇒knows(s)} and c = knows(s) one obtains a
model which satisﬁes the axiom (by falsifying its premise) but not the conjecture,
although the corresponding protocol should be regarded as insecure (because
one would usually assume an implementation to satisfy e(k, a) ̸= e(k, b), unless
a = b). The reason is that limit sentences do not admit negation.
In cryptographic protocols, the usage of negated equations in the protocol for-
malization applies for example to the error treatment when a cryptographic cer-
tiﬁcate veriﬁcation fails. Certiﬁcate veriﬁcation is typically performed by check-
ing an equation c = c′, where c and c′ are cryptographic expressions. For error
handling one would need to specify a behavior that is executed when c = c′ fails
to hold, i.e., when c ̸= c′ holds.
To deal with the fact that this is not supported by limit sentences, we simply
leave out the precondition c ̸= c′ from the formalization. This abstraction is
safe (because the attacker needs to do less work to achieve his goal), and it
is not overly unrealistic, because one would usually assume that the attacker
would anyhow be able to provoke a failed certiﬁcate check (simply by producing
a wrong certiﬁcate) to try to exploit a possible security weakness in the error
treatment. Nevertheless, we are currently considering whether one can make use
of ideas on elimination of negation in term algebras in this context [15,16].
Also, the restrictions on c cannot be relaxed arbitrarily, for example because
quotients of free algebras do not in general preserve inequalities and implications.
Thus, for A = ∅and c = (e(k, a) = e(k, b) =⇒knows(s)) a counterexample
is found, although one would usually assume an implementation to satisfy c
(vacuously, since a and b are distinct because nothing forces them to be equal
here).
However, in our practical examples it has so far always been possible to for-
malize the conjecture in a way that the restrictions do not become a problem.
If they would, one could still take the approach of considering the security con-
jecture directly (without negating) and asserting suitable distinctness axioms. If
4 Note that there a conjecture formalizes a security (rather than insecurity) property,
but our theorem easily transforms to that case, since BAN logic does not formalize
the adversary knowledge but rather the protocol participants assurance and can thus
be treated in a dual way.

162
J. J¨urjens and T. Weber
the model generator then ﬁnds counterexamples to both the conjecture and its
negation, one knows that the conjecture is independent of the axioms; thus, more
distinctness axioms should be introduced. However, this has not happened yet in
our usage of this approach in a variety of examples and application case-studies.
2.2
Using Finite Model Generation for Verifying Crypto-Protocols
In Sect. 2.1, we have given suﬃcient conditions under which security of a protocol
(i.e., that the attacker does not know the secret in the free algebra model) can
be concluded from existence of just one (arbitrary) model in which the attacker
does not know the secret. Unlike the free algebra model, which is necessarily
inﬁnite, this model may even be ﬁnite. Thus ﬁnite model generation [17] can be
used to search for it.
A ﬁnite model generator is an automatic software tool that attempts to build
a ﬁnite model of a (typically ﬁrst-order) formula. In a sense, model genera-
tion is dual to theorem proving: while the latter establishes validity, the former
establishes satisﬁability (and may provide counterexamples by considering the
negation of a formula).
Note that some formulas have inﬁnite models only. Thus, failure to ﬁnd a
ﬁnite model does not prove unsatisﬁability. Validity and ﬁnite satisﬁability of
ﬁrst-order formulas are semi-decidable, general satisﬁability however is not. In
the following Sect. 3 we will see that these theoretical restrictions are of limited
importance in practice.
3
Case Studies
To further illustrate the approach for protocol veriﬁcation discussed in this pa-
per, and to demonstrate its practicality, we have applied the approach to three
well-known (and frequently studied) protocols: the RSA Probabilistic Signature
Scheme (RSA-PSS) [18], the Wired Equivalent Privacy (WEP) protocol [19],
and the Needham-Schroeder-Lowe (NSL) authentication protocol [20].
We present abstract protocol formalizations in TPTP [21] syntax. The TPTP
library is a collection of standard benchmark problems for ﬁrst-order theorem
provers. The concrete syntax uses & for conjunction, => for implication, and !
(followed by a list of variables in square brackets) for universal quantiﬁcation.
Vampire 10.0, an automatic theorem prover for ﬁrst-order logic, was used
to ﬁnd attacks, and Paradox 2.3 [22], a ﬁnite model generator, was employed
to search for models that show security. Both Vampire (which was invoked via
Sutcliﬀe’s “System on TPTP” [23] web interface) and Paradox support TPTP
syntax as their input format.
We have also formalized these protocols in Isabelle/HOL [2], an interac-
tive theorem prover and model generator for higher-order logic. Since the Is-
abelle/HOL formalization diﬀers from the TPTP formalization only in terms of
concrete syntax, we do not show it in this paper.
As is often done in protocol analysis, we assume perfect cryptography and
consider abstract versions of these protocols only, i.e., we do not aim to verify

Finite Models in FOL-Based Crypto-Protocol Veriﬁcation
163
an actual implementation, nor the mathematics underlying the cryptographic
primitives (except where stated otherwise).
3.1
RSA-PSS
RSA-PSS [18] is a digital signature scheme that follows the usual “hash-then-
sign” paradigm. RSA refers to the now classic algorithm for public-key cryptogra-
phy devised by Rivest, Shamir, and Adleman [24]. PSS stands for “Probabilistic
Signature Scheme”, ﬁrst described by Bellare and Rogaway [25]. Starting with
a message m that is to be signed, the RSA-PSS protocol—at a very abstract
level—proceeds in two steps:
1. Apply a one-way hash function to the message m to produce an encoded
message hash(m).
2. Apply a signature function to the encoded message, using a private key k,
to produce a signature sign(hash(m), k).
The message m is then sent together with its signature, sign(hash(m), k). The
signature can be veriﬁed by the receiver using the sender’s public key k−1. Note
that RSA-PSS is not an encryption algorithm; m becomes publicly known.
A detailed Isabelle/HOL formalization of the RSA-PSS protocol by Linden-
berg and Wirt is available [26]. For our purposes, however, it will be suﬃcient to
model hashing and signing as uninterpreted functions. Our analysis is therefore
not speciﬁc to PSS hashing. A third function, conc, forms the concatenation of
two messages.
We assume a naive implementation of the RSA signature function that suf-
fers from an undesirable homomorphism property, which allows the attacker to
compute the signature of concatenated messages from signatures for their com-
ponents (and vice versa):
sign(conc(a, b), k) = conc(sign(a, k), sign(b, k)).
(1)
If we consider a modiﬁed protocol without PSS hashing, then it is easy to show
from (1) that the attacker can forge the signature for conc(b, a) if he knows the
signature for conc(a, b). Vampire ﬁnds a proof of this result in less than a second.
Our goal is rather simple: we want to show that PSS hashing breaks this
homomorphism property, thereby improving security of the signature scheme.
We consider a protocol run where the message conc(a, b) is hashed, signed with
some private key k, and then transmitted over an insecure connection. The ﬁrst-
order formulas that model the RSA-PSS protocol and the abilities of a potential
Dolev-Yao attacker are shown in Fig. 1. Note that we do not assume sign to
satisfy a homomorphism property similar to (1) wrt. hash.
Can we conclude from these axioms that the attacker knows the signature
sign(hash(conc(b, a)), k)? (In our formalization, it is certainly possible that the
attacker knows this signature—since the knows predicate, as discussed in Sect. 1,
may be true everywhere—but is it also necessary?) The answer is: No. Paradox
ﬁnds a counterexample with just four elements (shown in Fig. 2) in about two

164
J. J¨urjens and T. Weber
fof(knows_hash, axiom,(![X]:(knows(X)=>knows(hash(X))))).
fof(knows_sign,axiom,(![X,K]:((knows(X)&knows(K))=>knows(sign(X,K))))).
fof(remove_sign,axiom,(![X,K]:((knows(sign(X,K))&knows(invs(K)))=>knows(X)))).
fof(knows_conc,axiom,(![X,Y]:((knows(X)&knows(Y))=>knows(conc(X,Y))))).
fof(remove_conc,axiom,(![X,Y]:(knows(conc(X,Y))=>(knows(X)&knows(Y))))).
fof(sign_hom,axiom,(![X,Y,K]:(sign(conc(X,Y),K)=conc(sign(X,K),sign(Y,K))))).
fof(protocol_msg,axiom,(knows(conc(conc(a,b),sign(hash(conc(a,b)),k))))).
fof(public_key,axiom,(knows(invs(k)))).
fof(attack,conjecture,(knows(sign(hash(conc(b,a)),k)))).
Fig. 1. TPTP encoding of the RSA-PSS protocol
hash(1)
= 2
hash(2)
= 4
hash(3)
= 3
hash(4)
= 4
invs(1)
= 3
invs(2)
= 3
invs(3)
= 1
invs(4)
= 3
sign(1, 1) = 2
sign(1, 2) = 2
sign(1, 3)
= 1
sign(1, 4) = 2
sign(2, 1) = 2
sign(2, 2) = 2
sign(2, 3)
= 3
sign(2, 4) = 1
sign(3, 1) = 1
sign(3, 2) = 1
sign(3, 3)
= 3
sign(3, 4) = 2
sign(4, 1) = 2
sign(4, 2) = 2
sign(4, 3)
= 4
sign(4, 4) = 1
conc(1, 1) = 1
conc(1, 2) = 2
conc(1, 3) = 3
conc(1, 4) = 1
conc(2, 1) = 2
conc(2, 2) = 2
conc(2, 3) = 3
conc(2, 4) = 2
conc(3, 1) = 3
conc(3, 2) = 3
conc(3, 3) = 3
conc(3, 4) = 3
conc(4, 1) = 4
conc(4, 2) = 2
conc(4, 3) = 3
conc(4, 4) = 1
a
= 4
b
= 1
k
= 3
knows(1)
knows(2)
¬knows(3)
knows(4)
Fig. 2. Model showing security of RSA-PSS hashing
seconds on a current personal computer. This proves that the attack mentioned
above (exploiting (1)) is no longer possible with hashing.5 Of course the inter-
pretation of, e.g., conc is not the usual one in this ﬁnite model. By Thm. 1,
however, the result also holds in the free algebra model (modulo (1)).
3.2
Wired Equivalent Privacy
The Wired Equivalent Privacy (WEP) protocol [19] was introduced in 1997 to
provide conﬁdentiality for wireless networks. Later serious weaknesses were iden-
tiﬁed, and the protocol is now considered deprecated. In the context of this paper
the protocol is interesting because it employs the “exclusive or” function, which
can be characterized algebraically by associativity, commutativity, existence of
a neutral element, and nilpotency:
xor(xor(x, y), z) = xor(x, xor(y, z)),
xor(x, 0) = x,
xor(x, y) = xor(y, x),
xor(x, x) = 0.
Note that all four axioms are universally quantiﬁed identities.
5 We have also shown that this remains true even if the hashing function is reversible,
i.e., if we add an axiom ![X]:(knows(hash(X))=>knows(X)).

Finite Models in FOL-Based Crypto-Protocol Veriﬁcation
165
fof(xor_assoc,axiom,(![X,Y,Z]:(xor(xor(X,Y),Z)=xor(X,xor(Y,Z))))).
fof(xor_comm,axiom,(![X,Y]:(xor(X,Y)=xor(Y,X)))).
fof(xor_neutral,axiom,(![X]:(xor(X,zero)=X))).
fof(xor_nilpotent,axiom,(![X]:(xor(X,X)=zero))).
fof(knows_xor,axiom,(![X,Y]:((knows(X)&knows(Y))=>knows(xor(X,Y))))).
fof(knows_zero,axiom,(knows(zero))).
fof(knows_rc4,axiom,(![X,Y]:((knows(X)&knows(Y))=>knows(rc4(X,Y))))).
fof(knows_c,axiom,(![X,Y]:(knows(X)=>knows(c(X))))).
fof(knows_conc,axiom,(![X,Y]:((knows(X)&knows(Y))=>knows(conc(X,Y))))).
fof(remove_conc,axiom,(![X,Y]:(knows(conc(X,Y))=>(knows(X)&knows(Y))))).
fof(protocol_msg,axiom,(knows(conc(v,xor(conc(m,c(m)),rc4(v,k)))))).
% an arbitrary delta message
fof(knows_d,axiom,(knows(d))).
% homomorphism property for c
fof(c_hom,axiom,(![X,Y]:(c(xor(X,Y))=xor(c(X),c(Y))))).
% homomorphism property for conc
fof(conc_hom,axiom,(![X1,Y1,X2,Y2]:(xor(conc(X1,Y1),conc(X2,Y2))
=conc(xor(X1,X2),xor(Y1,Y2))))).
% the protocol is malleable
fof(attack,conjecture,
(knows(conc(v,xor(conc(xor(m,d),c(xor(m,d))),rc4(v,k)))))).
Fig. 3. TPTP encoding of the Wired Equivalent Privacy protocol
Our formalization is based on [27, Sect. 3.3]. The WEP protocol uses the RC4
algorithm, which generates a sequence of pseudo-random bits from an initial
vector v and a shared secret key k. Moreover, it uses a checksum algorithm c,
e.g., cyclic redundancy check. We model both RC4 and c by uninterpreted
functions.
To encrypt a message m, principal A chooses an initial vector v, computes
RC4(v, k) and c(m), encrypts conc(m, c(m)) with RC4(v, k) (using xor), and sends
both v and the ciphertext to principal B. To decrypt, B (who must know the
shared key k) then computes RC4(v, k), obtains conc(m, c(m)) from the cipher-
text (again using xor), and veriﬁes the checksum c(m).
If both conc and c satisfy a homomorphism property with respect to xor (see
Fig. 3), the protocol becomes malleable: a Dolev-Yao attacker can modify a
ciphertext arbitrarily without disrupting the checksum [28]. Vampire automati-
cally proves the attack in about three seconds.
The attack is no longer possible if we drop the conc hom axiom. Paradox then
ﬁnds a counterexample (of size 8) to the attack conjecture in less than a second.
We omit the model for space reasons.
If we instead drop the c hom axiom, Paradox—perhaps surprisingly—fails to
refute the attack. We used it to exhaustively check all models of size 15 or
less (which took several hours on a current personal computer); none of them
constitute a counterexample to the conjecture. Possibly, all counterexamples are
inﬁnite.

166
J. J¨urjens and T. Weber
3.3
Needham-Schroeder-Lowe with ECB
The Needham-Schroeder-Lowe (NSL) protocol [20] is perhaps the most fre-
quently studied authentication protocol of all. Our formalization is based on [27,
Sect. 3.5]. The NSL protocol consists of three messages.
1. First, principal A sends encrypt(conc(NA, A), pk(B)) to principal B, where
pk(B) is B’s public key, and NA is a fresh (e.g., random) value.
2. B decrypts this message, using his secret key sk(B), to learn NA. He then
sends encrypt(conc(NA, conc(NB, B)), pk(A)) to A, where NB is again a fresh
value.
3. Third, A decrypts B’s message to learn NB, and replies to B with
encrypt(NB, pk(B)). B veriﬁes receipt of this message.
At the end of the protocol, A and B are convinced to talk with each other, and
to share the secrets NA and NB. The protocol, however, is ﬂawed if Electronic
Code Book (ECB) is used for encryption. ECB is a block cipher mode that simply
encrypts each message block separately. ECB (formalized by the uninterpreted
function encrypt below) renders the NSL protocol insecure because it satisﬁes a
homomorphism property:
encrypt(conc(a, b), k) = conc(encrypt(a, k), encrypt(b, k)).
(2)
The attack works as follows. Assuming that principal A initiates a protocol
session S1 with the intruder by sending encrypt(conc(NA, A), pk(I)), the intruder
can then
1. impersonate A to initiate a session S2 with principal B:
(S2.1)
I(A) →B:
encrypt(conc(NA, A), pk(B)) ,
2. learn the secret NB by abusing A to decrypt B’s response:
(S2.2)
B →I(A):
encrypt(conc(NA, conc(NB, B)), pk(A))
(S1.2)
I →A :
encrypt(conc(NA, conc(NB, I)), pk(A))
(S1.3)
A →
I :
encrypt(NB, pk(I)),
3. and ﬁnally use NB to trick B (who is actually communicating with the
intruder) into believing that he is communicating with A:
(S2.3)
I(A) →B:
encrypt(NB, pk(B)).
The ECB homomorphism property (2) is used in step 2 of the attack.
The protocol formalization in TPTP syntax is shown in Fig. 4. We assume
that exactly one principal, A, initiates a session with the intruder. To model
freshness of NB, we use a Skolem function (rather than a constant) nb that takes
three arguments, which correspond to the three parameters in the protocol’s ﬁrst
message, i.e., NA, A, and B. Vampire automatically proves the attack in about
two seconds.

Finite Models in FOL-Based Crypto-Protocol Veriﬁcation
167
fof(knows_encrypt,axiom,(![X,K]:((knows(X)&knows(K))=>knows(encrypt(X,K))))).
fof(remove_encrypt,axiom,(![X,Y]:((knows(encrypt(X,pk(Y)))&knows(sk(Y)))
=>knows(X)))).
fof(knows_conc,axiom,(![X,Y]:((knows(X)&knows(Y))=>knows(conc(X,Y))))).
fof(remove_conc,axiom,(![X,Y]:(knows(conc(X,Y))=>(knows(X)&knows(Y))))).
% principal A initiates a session with the intruder I
fof(protocol_msg_1,axiom,(knows(encrypt(conc(na,a),pk(i))))).
fof(protocol_msg_2,axiom,(![Na,A,B]:(knows(encrypt(conc(Na,A),pk(B)))
=>knows(encrypt(conc(Na,conc(nb(Na,A,B),B)),pk(A)))))).
% principal A will respond to the intruder’s reply
fof(protocol_msg_3,axiom,(![Nb]:(knows(encrypt(conc(na,conc(Nb,i)),pk(a)))
=>knows(encrypt(Nb,pk(i)))))).
fof(knows_i,axiom,(knows(i))).
fof(knows_pka,axiom,(knows(pk(a)))).
fof(knows_pkb,axiom,(knows(pk(b)))).
fof(knows_pki,axiom,(knows(pk(i)))).
fof(knows_ski,axiom,(knows(sk(i)))).
% ECB homomorphism property
fof(ecb_hom,axiom,(![X,Y,K]:(encrypt(conc(X,Y),K)
=conc(encrypt(X,K),encrypt(Y,K))))).
fof(attack,conjecture,(knows(nb(na,a,b)))).
Fig. 4. TPTP encoding of the Needham-Schroeder-Lowe protocol
On the other hand, if we do not assume (2), the attacker can no longer gain
knowledge of nb(NA, A, B). Paradox ﬁnds a counterexample of size 4 in just
about a second. We omit the model for space reasons.
Our formalization does not model the state of principal A. In reality, the intruder
can abuse A to decrypt a value only once. In our formal model, the intruder is more
powerful: there appears to be an unbounded number of protocol sessions initiated
by A available to him. However, this is not used in Vampire’s proof of the attack,
and it does not aﬀect security of the protocol when (2) is omitted.
Inﬁnitely Many Principals
We can model a potentially inﬁnite number of principals by introducing a unary
function next and a predicate principal, and requiring principal(i) as well as ∀x.
(principal(x) =⇒principal(next(x))). Note that these assertions are strict Horn
clauses. If we modify the NSL formalization accordingly (by replacing protocol
msg 1, protocol msg 3, and knows pkX from Fig. 4 with suitably quantiﬁed ver-
sions), Paradox still ﬁnds a counterexample of size 4. Thus, the NSL protocol is (by
Thm. 1) secure even in the presence of inﬁnitely many principals.
4
Conclusion
In this paper, we have proposed an approach to crypto-protocol veriﬁcation that
proceeds by negating the security conjecture, and then employs ﬁnite model gener-
ation to show security by means of a single (counter-) model. This idea was

168
J. J¨urjens and T. Weber
independently pioneered in an earlier paper by Selinger [29]. Here we have estab-
lished conditions under which the approach is sound, thereby perhaps remedying
the “uneasy feeling” that Selinger in that paper mentioned that he had because of
the models’ potential simplicity. Our result also shows that approaches such as [3]
are sound in the sense that our Theorem 2 applies to it. We have also shown that
the search for models can be automated not just “in principle”, but that current
model generators are extremely valuable for this task.
Theorem proving, in this approach, can be used to ﬁnd attacks. Unlike in the
traditional free algebra approach, no freeness axioms or least ﬁxed-point axioms
for the attacker’s knowledge are required.
In contrast to ProVerif, we do not merely claim security of protocols, but pro-
duce “security certiﬁcates”: models that can be veriﬁed independently. This has
recently been explored further by Goubault-Larrecq [30], who translates models
into Coq proofs of protocol correctness. Furthermore, our use of a standard, highly
eﬃcient theorem prover and model generator leads to eﬀortless support for a larger
fragment of ﬁrst-order logic, including, e.g., equality.
Theproposedtechniquehasboththeoreticalandpracticallimitations.Itissound
for protocols that can be formalized by strict Horn clauses (including identities),
and more generally, limit theories (cf. Sect. 2). On the practical side, ﬁnite model
generators may fail to ﬁnd a counterexample (which would demonstrate security
of the protocol) because of resource (i.e., runtime, memory) constraints, or sim-
ply because no ﬁnite model exists. Therefore the approach is not complete; it may
fail to demonstrate the security of a secure protocol. Nevertheless, its successful
application to three well-known protocols in this paper provides evidence that the
approach is both suﬃciently versatile and practical. It can simplify protocol for-
malizations and security proofs signiﬁcantly.
In this paper, we only considered secrecyproperties. However, thegeneralresults
carry over to other properties that can be formalized within limit theories (such as
authentication by correspondence properties).
In future work, it would be interesting to investigate how the ideas discussed
here could be used in the context of verifying implementations, rather than speci-
ﬁcations of crypto protocols, e.g., in the context of [31], or the veriﬁcation of secure
information ﬂow, e.g., based on [32].
References
1. Paulson, L.C.: The inductive approach to verifying cryptographic protocols. Journal
of Computer Security 6(1-2), 85–128 (1998)
2. Nipkow, T., Paulson, L.C., Wenzel, M.: Isabelle/HOL – A Proof Assistant for Higher-
Order Logic. LNCS, vol. 2283. Springer, Heidelberg (2002)
3. Weidenbach, C.: Towards an automatic analysis of security protocols in ﬁrst-order
logic. In: Ganzinger, H. (ed.) CADE 1999. LNCS, vol. 1632, pp. 314–328. Springer,
Heidelberg (1999)
4. Blanchet, B.: An Eﬃcient Cryptographic Protocol Veriﬁer Based on Prolog Rules.
In: CSFW-14, pp. 82–96. IEEE Computer Society, Los Alamitos (2001)

Finite Models in FOL-Based Crypto-Protocol Veriﬁcation
169
5. J¨urjens, J.: A domain-speciﬁc language for cryptographic protocols based on
streams. Journal of Logic and Algebraic Programming (JLAP) (2009)
6. Blanchet, B.: From secrecy to authenticity in security protocols. In: Hermenegildo,
M.V., Puebla, G. (eds.) SAS 2002. LNCS, vol. 2477, pp. 342–359. Springer, Heidel-
berg (2002)
7. Comon, H., Nieuwenhuis, R.: Induction = I-axiomatization + ﬁrst-order consistency.
Technical report, ENS Cachan (1998)
8. Steel, G., Bundy, A., Maidl, M.: Attacking a protocol for group key agreement by
refuting incorrect inductive conjectures. In: Basin, D., Rusinowitch, M. (eds.) IJCAR
2004. LNCS, vol. 3097, pp. 137–151. Springer, Heidelberg (2004)
9. Hodges, W.: Model Theory. Cambridge University Press, Cambridge (1993)
10. Ad´amek, J., Rosick´y, J.: Locally Presentable and Accessible Categories. London
Math. Soc. Lect. Note Ser., vol. 189. Cambridge University Press, Cambridge (1994)
11. J¨urjens, J.: On a problem of Gabriel and Ulmer. Journal of Pure and Applied Alge-
bra 158, 183–196 (2001)
12. Schumann, J.: Automatic veriﬁcation of cryptographic protocols with SETHEO. In:
CADE (1999)
13. Comon-Lundh, H., Cortier, V.: Security properties: two agents are suﬃcient. Sci.
Comput. Program. 50(1-3), 51–71 (2004)
14. Cohen, E.: First-order veriﬁcation of cryptographic protocols. Journal of Computer
Security 11(2), 189–216 (2003)
15. Lassez, J.L., Maher, M.J., Marriott, K.: Elimination of negation in term algebras.
In: Tarlecki, A. (ed.) MFCS 1991. LNCS, vol. 520, pp. 1–16. Springer, Heidelberg
(1991)
16. Comon, H., Fern´andez, M.: Negation elimination in equational formulae. In: Havel,
I.M., Koubek, V. (eds.) MFCS 1992. LNCS, vol. 629. Springer, Heidelberg (1992)
17. Weber, T.: SAT-based Finite Model Generation for Higher-Order Logic. PhD thesis,
Technische Universit¨at M¨unchen (2008)
18. RSA Laboratories: PKCS #1: RSA Cryptography Standard Version 2.1. (June 2002)
19. Institute of Electrical and Electronics Engineers: IEEE Std 802.11-1997 (1997)
20. Lowe, G.: An attack on the Needham-Schroeder public key authentication protocol.
Information Processing Letters 56(3), 131–136 (1995)
21. Sutcliﬀe, G., Suttner, C.B.: The TPTP problem library: CNF release v1.2.1. Journal
of Automated Reasoning 21(2), 177–203 (1998)
22. Claessen, K., S¨orensson, N.: New techniques that improve MACE-style ﬁnite model
ﬁnding. In: CADE, Workshop W4 (2003)
23. Sutcliﬀe, G.: System on TPTP,
http://www.cs.miami.edu/~tptp/cgi-bin/SystemOnTPTP (accessed January 9,
2009)
24. Rivest, R., Shamir, A., Adleman, L.: A method for obtaining digital signatures and
public-key cryptosystems. Communications of the ACM 21(2), 120–126 (1978)
25. Bellare, M., Rogaway, P.: The exact security of digital signatures: How to sign with
RSA and Rabin. In: Maurer, U.M. (ed.) EUROCRYPT 1996. LNCS, vol. 1070, pp.
399–416. Springer, Heidelberg (1996)
26. Lindenberg, C., Wirt, K.: SHA1, RSA, PSS and more. In: Klein, G., Nipkow, T.,
Paulson, L. (eds.) The Archive of Formal Proofs (May 2005),
http://afp.sourceforge.net/entries/RSAPSS.shtml,
Formal proof development

170
J. J¨urjens and T. Weber
27. Cortier, V., Delaune, S., Lafourcade, P.: A survey of algebraic properties used in
cryptographic protocols. Journal of Computer Security 14(1), 1–43 (2006)
28. Borisov, N., Goldberg, I., Wagner, D.: Intercepting mobile communications: The in-
security of 802.11. In: MOBICOM, pp. 180–188 (2001)
29. Selinger, P.: Models for an adversary-centric protocol logic. Electr. Notes Theor.
Comput. Sci. 55(1) (2001)
30. Goubault-Larrecq, J.: Towards producing formally checkable security proofs, auto-
matically. In: Computer Security Foundations (CSF), pp. 224–238 (2008)
31. J¨urjens, J.: Security analysis of crypto-based java programs using automated theo-
rem provers. In: ASE, pp. 167–176. IEEE Computer Society, Los Alamitos (2006)
32. J¨urjens, J.: Secure information ﬂow for concurrent processes. In: Palamidessi, C. (ed.)
CONCUR 2000. LNCS, vol. 1877, pp. 395–409. Springer, Heidelberg (2000)

Finite Models in FOL-Based Crypto-Protocol Veriﬁcation
171
A
Appendix
This appendix contains ﬁnite models that demonstrate security of the Needham-
Schroeder-Lowe protocol (in Fig. 5) and of the Wireless Equivalent Privacy proto-
col (in Fig. 6). These models were mentioned in Sect. 3.3 and Sect. 3.2, respectively,
of the paper. They were omitted from the paper for space reasons.
conc(1, 1)
= 1
conc(1, 2)
= 2
conc(1, 3)
= 1
conc(1, 4)
= 2
conc(2, 1)
= 2
conc(2, 2)
= 2
conc(2, 3)
= 2
conc(2, 4)
= 2
conc(3, 1)
= 3
conc(3, 2)
= 2
conc(3, 3)
= 1
conc(3, 4)
= 4
conc(4, 1)
= 4
conc(4, 2)
= 2
conc(4, 3)
= 2
conc(4, 4)
= 4
encrypt(1, 1) = 3
encrypt(1, 2) = 4
encrypt(1, 3) = 1
encrypt(1, 4) = 4
encrypt(2, 1) = 4
encrypt(2, 2) = 4
encrypt(2, 3) = 2
encrypt(2, 4) = 4
encrypt(3, 1) = 3
encrypt(3, 2) = 1
encrypt(3, 3) = 3
encrypt(3, 4) = 4
encrypt(4, 1) = 4
encrypt(4, 2) = 3
encrypt(4, 3) = 3
encrypt(4, 4) = 4
nb(1, 1, 1)
= 1
nb(1, 1, 2)
= 3
nb(1, 1, 3)
= 3
nb(1, 1, 4)
= 1
nb(1, 2, 1)
= 1
nb(1, 2, 2)
= 1
nb(1, 2, 3)
= 2
nb(1, 2, 4)
= 1
nb(1, 3, 1)
= 3
nb(1, 3, 2)
= 1
nb(1, 3, 3)
= 3
nb(1, 3, 4)
= 1
nb(1, 4, 1)
= 1
nb(1, 4, 2)
= 3
nb(1, 4, 3)
= 4
nb(1, 4, 4)
= 2
nb(2, 1, 1)
= 1
nb(2, 1, 2)
= 4
nb(2, 1, 3)
= 1
nb(2, 1, 4)
= 1
nb(2, 2, 1)
= 4
nb(2, 2, 2)
= 2
nb(2, 2, 3)
= 4
nb(2, 2, 4)
= 2
nb(2, 3, 1)
= 1
nb(2, 3, 2)
= 4
nb(2, 3, 3)
= 2
nb(2, 3, 4)
= 3
nb(2, 4, 1)
= 1
nb(2, 4, 2)
= 2
nb(2, 4, 3)
= 1
nb(2, 4, 4)
= 2
nb(3, 1, 1)
= 4
nb(3, 1, 2)
= 1
nb(3, 1, 3)
= 1
nb(3, 1, 4)
= 3
nb(3, 2, 1)
= 3
nb(3, 2, 2)
= 1
nb(3, 2, 3)
= 4
nb(3, 2, 4)
= 3
nb(3, 3, 1)
= 1
nb(3, 3, 2)
= 4
nb(3, 3, 3)
= 1
nb(3, 3, 4)
= 1
nb(3, 4, 1)
= 1
nb(3, 4, 2)
= 1
nb(3, 4, 3)
= 4
nb(3, 4, 4)
= 4
nb(4, 1, 1)
= 4
nb(4, 1, 2)
= 4
nb(4, 1, 3)
= 2
nb(4, 1, 4)
= 3
nb(4, 2, 1)
= 4
nb(4, 2, 2)
= 1
nb(4, 2, 3)
= 2
nb(4, 2, 4)
= 1
nb(4, 3, 1)
= 2
nb(4, 3, 2)
= 2
nb(4, 3, 3)
= 2
nb(4, 3, 4)
= 1
nb(4, 4, 1)
= 4
nb(4, 4, 2)
= 1
nb(4, 4, 3)
= 3
nb(4, 4, 4)
= 4
pk(1)
= 3
pk(2)
= 4
pk(3)
= 1
pk(4)
= 2
sk(1)
= 2
sk(2)
= 4
sk(3)
= 1
sk(4)
= 4
a
= 1
b
= 1
i
= 3
NA
= 3
knows(1)
¬knows(2)
knows(3)
¬knows(4)
Fig. 5. Model showing security of NSL

172
J. J¨urjens and T. Weber
xor(1, 1)
= 5
xor(1, 2)
= 7
xor(1, 3)
= 4
xor(1, 4)
= 3
xor(1, 5)
= 1
xor(1, 6)
= 8
xor(1, 7)
= 2
xor(1, 8)
= 6
xor(2, 1)
= 7
xor(2, 2)
= 5
xor(2, 3)
= 8
xor(2, 4)
= 6
xor(2, 5)
= 2
xor(2, 6)
= 4
xor(2, 7)
= 1
xor(2, 8)
= 3
xor(3, 1)
= 4
xor(3, 2)
= 8
xor(3, 3)
= 5
xor(3, 4)
= 1
xor(3, 5)
= 3
xor(3, 6)
= 7
xor(3, 7)
= 6
xor(3, 8)
= 2
xor(4, 1)
= 3
xor(4, 2)
= 6
xor(4, 3)
= 1
xor(4, 4)
= 5
xor(4, 5)
= 4
xor(4, 6)
= 2
xor(4, 7)
= 8
xor(4, 8)
= 7
xor(5, 1)
= 1
xor(5, 2)
= 2
xor(5, 3)
= 3
xor(5, 4)
= 4
xor(5, 5)
= 5
xor(5, 6)
= 6
xor(5, 7)
= 7
xor(5, 8)
= 8
xor(6, 1)
= 8
xor(6, 2)
= 4
xor(6, 3)
= 7
xor(6, 4)
= 2
xor(6, 5)
= 6
xor(6, 6)
= 5
xor(6, 7)
= 3
xor(6, 8)
= 1
xor(7, 1)
= 2
xor(7, 2)
= 1
xor(7, 3)
= 6
xor(7, 4)
= 8
xor(7, 5)
= 7
xor(7, 6)
= 3
xor(7, 7)
= 5
xor(7, 8)
= 4
xor(8, 1)
= 6
xor(8, 2)
= 3
xor(8, 3)
= 2
xor(8, 4)
= 7
xor(8, 5)
= 8
xor(8, 6)
= 1
xor(8, 7)
= 4
xor(8, 8)
= 5
zero
= 5
RC4(1, 1) = 5
RC4(1, 2)
= 8
RC4(1, 3)
= 2
RC4(1, 4)
= 5
RC4(1, 5) = 1
RC4(1, 6)
= 1
RC4(1, 7)
= 2
RC4(1, 8)
= 7
RC4(2, 1) = 5
RC4(2, 2)
= 2
RC4(2, 3)
= 3
RC4(2, 4)
= 2
RC4(2, 5) = 2
RC4(2, 6)
= 5
RC4(2, 7)
= 1
RC4(2, 8)
= 7
RC4(3, 1) = 5
RC4(3, 2)
= 1
RC4(3, 3)
= 1
RC4(3, 4)
= 1
RC4(3, 5) = 5
RC4(3, 6)
= 1
RC4(3, 7)
= 5
RC4(3, 8)
= 7
RC4(4, 1) = 1
RC4(4, 2)
= 2
RC4(4, 3)
= 2
RC4(4, 4)
= 3
RC4(4, 5) = 5
RC4(4, 6)
= 5
RC4(4, 7)
= 1
RC4(4, 8)
= 7
RC4(5, 1) = 5
RC4(5, 2)
= 4
RC4(5, 3)
= 1
RC4(5, 4)
= 5
RC4(5, 5) = 1
RC4(5, 6)
= 4
RC4(5, 7)
= 1
RC4(5, 8)
= 7
RC4(6, 1) = 5
RC4(6, 2)
= 5
RC4(6, 3)
= 2
RC4(6, 4)
= 4
RC4(6, 5) = 5
RC4(6, 6)
= 7
RC4(6, 7)
= 1
RC4(6, 8)
= 7
RC4(7, 1) = 6
RC4(7, 2)
= 1
RC4(7, 3)
= 1
RC4(7, 4)
= 2
RC4(7, 5) = 1
RC4(7, 6)
= 7
RC4(7, 7)
= 6
RC4(7, 8)
= 7
RC4(8, 1) = 7
RC4(8, 2)
= 7
RC4(8, 3)
= 7
RC4(8, 4)
= 7
RC4(8, 5) = 7
RC4(8, 6)
= 7
RC4(8, 7)
= 7
RC4(8, 8)
= 7
c(1)
= 5
c(2)
= 2
c(3)
= 1
c(4)
= 1
c(5)
= 5
c(6)
= 7
c(7)
= 2
c(8)
= 7
conc(1, 1) = 1
conc(1, 2) = 2
conc(1, 3) = 7
conc(1, 4) = 8
conc(1, 5) = 1
conc(1, 6) = 3
conc(1, 7) = 2
conc(1, 8) = 8
conc(2, 1) = 3
conc(2, 2) = 8
conc(2, 3) = 7
conc(2, 4) = 7
conc(2, 5) = 4
conc(2, 6) = 3
conc(2, 7) = 4
conc(2, 8) = 2
conc(3, 1) = 2
conc(3, 2) = 8
conc(3, 3) = 7
conc(3, 4) = 8
conc(3, 5) = 2
conc(3, 6) = 4
conc(3, 7) = 2
conc(3, 8) = 2
conc(4, 1) = 8
conc(4, 2) = 7
conc(4, 3) = 7
conc(4, 4) = 7
conc(4, 5) = 2
conc(4, 6) = 3
conc(4, 7) = 4
conc(4, 8) = 2
conc(5, 1) = 5
conc(5, 2) = 8
conc(5, 3) = 6
conc(5, 4) = 2
conc(5, 5) = 5
conc(5, 6) = 2
conc(5, 7) = 2
conc(5, 8) = 7
conc(6, 1) = 8
conc(6, 2) = 2
conc(6, 3) = 4
conc(6, 4) = 2
conc(6, 5) = 3
conc(6, 6) = 4
conc(6, 7) = 6
conc(6, 8) = 7
conc(7, 1) = 6
conc(7, 2) = 7
conc(7, 3) = 3
conc(7, 4) = 6
conc(7, 5) = 2
conc(7, 6) = 2
conc(7, 7) = 3
conc(7, 8) = 7
conc(8, 1) = 7
conc(8, 2) = 7
conc(8, 3) = 7
conc(8, 4) = 7
conc(8, 5) = 7
conc(8, 6) = 7
conc(8, 7) = 7
conc(8, 8) = 7
m
= 2
v
= 1
k
= 2
d
= 1
knows(1)
¬knows(2)
¬knows(3)
¬knows(4)
knows(5)
¬knows(6)
¬knows(7)
¬knows(8)
Fig. 6. Model showing security of WEP

Towards a Type System for Security APIs
Gavin Keighren1, David Aspinall1, and Graham Steel2
1 Laboratory for Foundations of Computer Science
School of Informatics, The University of Edinburgh
Informatics Forum, 10 Crichton Street, Edinburgh, EH8 9AB, UK
gavin.keighren@ed.ac.uk, david.aspinall@ed.ac.uk
2 LSV, INRIA & CNRS & ENS de Cachan
61, avenue du Pr´esident Wilson
94235 CACHAN Cedex – France
graham.steel@lsv.ens-cachan.fr
Abstract. Security API analysis typically only considers a subset of an
API’s functions, with results bounded by the number of function calls.
Furthermore, attacks involving partial leakage of sensitive information
are usually not covered.
Type-based static analysis has the potential to alleviate these short-
comings. To that end, we present a type system for secure information
ﬂow based upon the one of Volpano, Smith and Irvine [1], extended with
types for cryptographic keys and ciphertext similar to those in Sumii and
Pierce [2]. In contrast to some other type systems, the encryption and
decryption of keys does not require special treatment.
We show that a well-typed sequence of commands is non-interferent,
based upon a deﬁnition of indistinguishability where, in certain circum-
stances, the adversary can distinguish between ciphertexts that corre-
spond to encrypted public data.
1
Introduction
It is common for computer systems which store, process and manipulate sensitive
data to use a dedicated security hardware device (e.g., IBM 4758 [3] and nCipher
nShield [4]). The set of functions provided by a security device is termed its
security API, as they are intended to enforce a security policy as well as provide
an interface to the device. A security policy describes the restrictions on the
access to, use of, and propagation of data in the system. These restrictions,
therefore, must follow as a direct consequence of the API functions which are
available to users of the security device.
The analysis of security APIs has traditionally been carried out by enumer-
ating the set of data items which the adversary (a malicious user) can obtain
through repeated interactions with the API. While this approach has had rea-
sonable success (e.g., [5,6,7,8,9]), results are typically bounded by the number
of API calls, do not consider data integrity, and only detect ﬂaws which involve
the release of sensitive data items in their entirety.
P. Degano and L. Vigan`o (Eds.): ARSPA-WITS 2009, LNCS 5511, pp. 173–192, 2009.
c
⃝Springer-Verlag Berlin Heidelberg 2009

174
G. Keighren, D. Aspinall, and G. Steel
In contrast, static analysis has the potential to provide unbounded results,
identify ﬂaws which allow for sensitive data to be leaked via covert control-ﬂow
channels, and also deal with data integrity. The type system presented in this
paper is the foundation of one such approach, although it does not yet deal with
integrity.
Our work builds upon the information-ﬂow analysis capabilities of Volpano,
Smith and Irvine’s type system [1] by including cryptographic types similar
to those from Sumii and Pierce’s system for analysing security protocols [2]. Al-
though there are many similarities between security APIs and security protocols,
analysis methods for the latter are typically designed to deal with ﬁxed-length
speciﬁed interactions, and therefore generally do not scale well when applied to
arbitrary sequences of interactions.
2
Background
Hardware Security Modules (HSMs) comprise some memory and a processor
inside a tamper-proof enclosure which prevents the memory contents from being
physically read — any breach causes the memory to be erased within a few
micro-seconds. Additional storage is provided by the host system to which the
HSM is attached. This leads to a natural partition of memory locations: those
inside the HSM are high security, and those on the host system are low security.
Memory locations on the host system are deemed low security, since the attack
model for security API analysis assumes that the adversary has full control of
the host system. In addition, the adversary is assumed to be capable of calling
certain functions provided by the HSM’s security API (because, for example,
they have hijacked a user’s session, or they are a legitimate user themselves).
Figure 1 shows the interactions between the adversary, HSM API, and mem-
ory locations in the standard attack scenario. HSM functions may access any
memory locations, while the adversary can only access the low security loca-
tions. A similar setup applies in the case of software APIs, where the adversary
is a malicious client program and the high memory locations correspond to those
which are hidden by the API.
High
Low
HSM API
y
@

@
@

@

@

@
@

6




@
@

Read/Write Accesses
-
API Function Calls
-
Insecure Data Flow
Fig. 1. The interactions between the adversary, the HSM API, and the memory loca-
tions

Towards a Type System for Security APIs
175
n
∈
N
Names
a
∈
A
Locations
l
∈
L
Security levels (where ⊥,⊤∈L)
e
::= n | !a | senc(e, e) | sdec(e, e) | junk(e)
Expressions
c
::= a := e | c ; c | ϵ
Commands
u
::= n | senc(u, u)
Non-junk values
v
::= u | junk(u)
Values
E
::= l data | l key | enc(E)
Security types for expressions
A ::= E loc
Security type for locations
C ::= cmd
Security type for commands
T ::= E | A | C
All security types
φ
::= φ , a →v | ϵ
Store
∆::= ∆, n : l | ∆, a : l | ϵ
Security levels environment
Γ ::= Γ, n : l data | Γ, n : l key | Γ, a : A | ϵ
Security types environment
Fig. 2. Fundamental syntax deﬁnitions
The adversary’s goal is to execute a series of API function calls such that
sensitive data is unintentionally written to the low security memory, or that
sensitive data can be inferred from the API’s low security output. The aim of
security API analysis is to detect such insecure data ﬂows, or to guarantee that
no such ﬂows exist.
3
Type System
Figure 2 presents the fundamental syntax deﬁnitions upon which our type sys-
tem is built. The set N comprises names representing regular data items and
cryptographic keys; A is the set of abstract memory locations, and L is the set of
security levels which may be associated to names and locations.1 Although our
type system allows for any number of security levels (where l∈L →⊥≤l ≤⊤),
in this paper we only consider ⊥and ⊤(i.e., low and high) in order to simplify
the presentation and discussion.
An expression can be a name, the contents of a memory location, the result of
a (symmetric) encryption or decryption, or ‘junk’ which denotes an incorrectly
decrypted term. Junk terms contain the expression which would have resulted
had the correct decryption key(s) been used, so we can ensure that a junk ex-
pression is treated in the same way as the intended non-junk term. A command
is zero or more instances of the assignment operation. A value is a name, a fully
evaluated ciphertext, or a junk value.
1 Security levels are for analysis purposes only — they do not exist in practice (and
even if they did, the adversary would not gain anything from knowing a term’s
security level).

176
G. Keighren, D. Aspinall, and G. Steel
The security types for regular data items and cryptographic keys, l data and
l key respectively, associate a security level with their terms. The level associated
with regular data denotes the conﬁdentiality level of that data item, whereas the
one associated with a cryptographic key denotes the maximum security level of
expressions which that key may encrypt. The diﬀerent semantics are necessary
because we allow the security level of expressions to be arbitrarily increased and
therefore cannot determine what a key may encrypt based solely on its security
level. For example, a low security key can be considered a high security key and
thus could be used to encrypt other high security expressions. This approach
also allows a high security key to encrypt and decrypt low security expressions
without forcing the result of the decryption to be high security.
To recover the precise type of encrypted data when it is subsequently de-
crypted, we use a type operator; enc(E) is the type of an encrypted expression
of type E. This means that no type information is lost as a result of the encryp-
tion/decryption process and also allows us to handle nested encryptions. The
security type for locations denotes the most general type of expression which
may be stored in that location. We do not associate a security level with the
command type, although we will do so in future work.
The store, φ, maps locations to values; the security levels environment, ∆,
contains the security levels of names and locations (used dynamically), and the
security types environment, Γ, contains the security types of names and locations
(used statically). We assume there is no overlap between the identiﬁers used for
names and for locations.
3.1
Operational Semantics
Figure 3 presents the operational semantics for command sequences which we
consider. We wish to enforce the restriction that only ciphertext is decrypted,
therefore any term which evaluates to the decryption of non-encrypted data will
get ‘stuck.’ That is, the term cannot be fully evaluated under the operational
semantics.
Other terms which will get stuck include the encryption of data with a key
whose security level is too low, the assignment of a value to a location whose
security level is too low, and the dereferencing of something other than a location.
The ﬁrst two of these correspond to cases where continued evaluation would
result in a security breach:
∆= {a : ⊥, vk : ⊥, vm : ⊤}
a := senc(vk, vm)
(1)
a := vm
(2)
Getting stuck in case (1) guarantees that, if the adversary is able to decrypt
some given piece of ciphertext, the result will not be sensitive, while getting stuck
in case (2) guarantees that sensitive data cannot be written directly to a low
security memory location. This latter property is known as ‘no-write down,’ and
is enforced by the rule E-Assign2. The ‘no read-up’ property follows from the
assumption that an observer is only able to read the contents of locations whose

Towards a Type System for Security APIs
177
Expressions
e→∆φ e′
ek →∆φ ek
′
senc(ek, em) →∆φ senc(ek
′, em)
E-SEnc1
e →∆φ e′
senc(v, e) →∆φ senc(v, e′)
E-SEnc2
senc(u, junk(u′)) →∆φ junk(senc(u, u′)) E-SEnc3
senc(junk(u), v) →∆φ junk(senc(u, v)) E-SEnc4
ek →∆φ ek
′
sdec(ek, em) →∆φ sdec(ek
′, em)
E-SDec1
e →∆φ e′
sdec(v, e) →∆φ sdec(v, e′)
E-SDec2
u′
k ̸= uk
∆⊢u′
k,uk : lk
∆⊢um : lm
lm ≤lk
sdec(u′
k, senc(uk, um)) →∆φ junk(um)
E-SDec4
sdec(u, junk(u′)) →∆φ junk(sdec(u, u′))
E-SDec5
sdec(junk(u), v) →∆φ junk(sdec(u, v))
E-SDec6
Commands
⟨φ, c⟩→∆⟨φ′, c′⟩
⟨φ, c1⟩→∆⟨φ′, c′
1⟩
⟨φ, c1; c2⟩→∆⟨φ′, c′
1; c2⟩
E-Cmds1
⟨φ, ϵ ; c⟩→∆⟨φ, c⟩
E-Cmds2
e →∆φ e′
⟨φ, a := e⟩→∆⟨φ, a := e′⟩
E-Assign1
∆⊢a : la ∆⊢v : lv
lv ≤la
⟨φ, a := v⟩→∆⟨φ[a →v], ϵ⟩
E-Assign2
∆⊢uk : lk
∆⊢um : lm lm ≤lk
sdec(uk, senc(uk, um)) →∆φ um
E-SDec3
!a →∆φ φ(a) E-Deref
e→∆φ e′
junk(e) →∆φ junk(e′)
E-Junk1
junk(junk(u)) →∆φ junk(u)
E-Junk2
Security Levels of Values
n : l ∈∆
∆⊢n : l
a : l ∈∆
∆⊢a : l
∆⊢uk : lk
∆⊢um : lm lm ≤lk
∆⊢senc(uk, um) : ⊥
∆⊢u : l
∆⊢junk(u) : l
Fig. 3. The operational semantics for command sequences
associated security level is low enough. This is a legitimate assumption, since the
sensitive locations will be those which are inside the tamper-proof HSM whose
security API is being analysed, or in the case of software APIs, those locations
which are hidden from client programs.
The junk term is returned when a piece of ciphertext is decrypted with a
diﬀerent key from the one which was used to create it (E-SDec4), or when the
key or message in an encryption or decryption operation is junk (E-SEnc3, E-
SEnc4, E-SDec5 and E-SDec6). In all cases, the expression within the junk
term is that which would have been returned had the correct decryption key(s)
been used.
Encryption requires that the security level of the key is at least as high as that
of the message. However, this restriction is not enforced when the ciphertext is
actually created, but rather when it is subsequently decrypted or assigned to a

178
G. Keighren, D. Aspinall, and G. Steel
Commands
Γ ⊢c1 : cmd
Γ ⊢c2 : cmd
Γ ⊢c1 ; c2 : cmd
T-Cmds
Γ ⊢a : E loc
Γ ⊢e : E
Γ ⊢a := e : cmd
T-Assign
Γ ⊢ϵ : cmd
T-Empty
Expressions
n : E ∈Γ
Γ ⊢n : E
T-Name
Γ ⊢e : E
Γ ⊢junk(e) : E
T-Junk
a : E loc ∈Γ
Γ ⊢a : E loc
T-Loc
Γ ⊢a : E loc
Γ ⊢!a : E
T-Deref
Γ ⊢ek : l key Γ ⊢em : E lvl(E) = l
Γ ⊢senc(ek, em) : enc(E)
T-SEnc
Γ ⊢ek : l key Γ ⊢em : enc(E) lvl(E) = l
Γ ⊢sdec(ek, em) : E
T-SDec
Subtyping
Γ ⊢t : T′
T′ <: T
Γ ⊢t : T
T-Sub
T <: T
T <: T′′
T′′ <: T′
T <: T′
l ≤l′
l data <: l′ data
E <: E′
enc(E) <: enc(E′)
l key <: ⊤data
enc(E) <: ⊥data
Security Levels of Types
lvl(cmd) = ⊥
lvl(l data) = l
lvl(l key) = ⊤
lvl(enc(E)) = ⊥
lvl(E loc) = lvl(E)
Fig. 4. The typing rules of our system
location (i.e., when the security level of the ciphertext has to be determined).
The security level of ciphertext is ⊥since encryption is used primarily as a means
of securely declassifying sensitive data. If the result of an encryption should itself
be sensitive, then this can be achieved simply by assigning the ciphertext to a
location which stores sensitive data and returning a reference to that location.
3.2
Typing Rules
Figure 4 presents the rules of our type system. As noted previously, a location’s
type denotes the most general type of values which can be stored in that location.
By design, the more general a type is, the greater its security level (i.e., E <:
E′ →lvl(E) ≤lvl(E′)). Therefore, the typing rule for assignment (T-Assign)
guarantees the ‘no write-down’ property since the security level associated with
the location will be no lower than the one associated with the expression.
Junk terms can have any expression type (T-Junk), since they are generated
only as the result of a decryption with the wrong key, and we wish to consider a
junk term as being equivalent to the intended result, had the correct decryption
key been used. This is to prevent insecure information ﬂows which may otherwise
result from the use of an incorrect decryption key.
The contents of a location are given the type of the most general expression
that can be stored in that location (T-Deref). Thus, any security result is
independent of the values which are actually stored in each memory location.
For encryption, the key used must be able to encrypt messages which are at
least as secure as the actual message (T-SEnc). For decryption, the message

Towards a Type System for Security APIs
179
must be ciphertext and the security level associated with the key must be no
lower than the security level associated with the result (T-SDec).
Currently, we restrict keys to having the highest security level, and commands
to having the lowest security level, since our focus is on security APIs with secret
keys and public functions. Relaxing these restrictions will form part of our future
work.
To prove the theorems presented in this paper, we require a couple of stan-
dard type-theoretic lemmas. The proofs are quite straightforward and have been
omitted.
Lemma 1. Generation Lemma (Inversion of the Typing Relation)
1. If Γ ⊢n :T then T :> E and n : E ∈Γ.
2. If Γ ⊢a :T then T ≡E loc and a : E loc ∈Γ.
3. If Γ ⊢!a :T then T :> E and Γ ⊢a : E loc.
4. If Γ ⊢senc(e1, e2) : T then T :> enc(E), Γ ⊢e1 : l key, Γ ⊢e2 : E and lvl(E)
= l.
5. If Γ ⊢sdec(e1, e2) : T then T :> E, Γ ⊢e1 : l key, Γ ⊢e2 : enc(E) and lvl(E)
= l.
6. If Γ ⊢junk(e) :T then Γ ⊢e :T.
7. If Γ ⊢a := e : T then T ≡cmd, Γ ⊢a : E loc, and Γ ⊢e : E.
8. If Γ ⊢ϵ :T then T ≡cmd.
9. If Γ ⊢c1 ; c2 : T then T ≡cmd, Γ ⊢c1 : cmd, and Γ ⊢c2 : cmd.
Proof. Follows from induction on the typing derivations.
Lemma 2. Canonical Forms Lemma
1. If Γ ⊢v : enc(E) then v ≡senc(uk, um) or junk(senc(uk, um)).
2. If Γ ⊢v : l key then v ≡n or junk(n).
3. If Γ ⊢v : l data then v ≡n, senc(uk, um), junk(n) or junk(senc(uk, um)).
Proof. Follows from inspection of the typing rules and fundamental deﬁnitions.
4
Progress and Preservation
The standard way to establish type safety for a type system with respect to
an operational semantics is to show that the progress and preservation proper-
ties hold. Preservation establishes that the type of a term is not changed by
the evaluation rules, while progress demonstrates that well-typed terms will not
get ‘stuck.’ Stuck terms represent certain error conditions that may arise during
evaluation. In our system, for example, a term becomes stuck whenever further
evaluation would result in a security leak. Such leaks are prevented in the oper-
ational semantics by checks carried out on the security levels in a number of the
evaluation rules.

180
G. Keighren, D. Aspinall, and G. Steel
For the progress and preservation properties to hold, the initial store φ must
be well-typed, and the security levels environment ∆must be level-consistent
with respect to the typing context Γ. Informally, φ is well-typed if every value
in φ has the type predicted by Γ, while ∆is level-consistent with respect to Γ if
every name and location in ∆has the same security level as given to it by Γ.
Deﬁnition 1. A store φ is well-typed with respect to a typing context Γ, writ-
ten Γ ⊢φ, if dom(φ) = dom( Γ | loc) and, ∀a ∈dom(φ), ∃E. Γ ⊢φ(a) : E ∧
Γ ⊢a : E loc.
Deﬁnition 2. A security levels environment ∆is level-consistent with respect
to a typing context Γ, written Γ ⊢∆, if dom(∆) = dom(Γ), and
• ∀n ∈dom(Γ | nam), n :E ∈Γ →n : lvl(E) ∈∆
• ∀a ∈dom(Γ | loc), a : E loc ∈Γ →a : lvl(E) ∈∆
Here, S | nam and S | loc denote the subsets of S containing only those elements
which are names and locations respectively.
Corollary 1. If Γ ⊢∆, Γ ⊢v :E and ∆⊢v : l, then l ≤lvl(E).
Proof. By deﬁnition, v ≡n, junk(n), senc(uk, um) or junk(senc(uk, um)). If v ≡
n or junk(n) then, by Lemma 1, n : E′ ∈Γ, where E′ <: E. By Γ ⊢∆, n : lvl(E′)
∈∆therefore ∆⊢v : lvl(E′) and l = lvl(E′). It then follows from E′ <: E that
lvl(E′) ≤lvl(E), so the result holds. If v ≡senc(uk, um) or junk(senc(uk, um))
then l = ⊥, so the result holds.
⊓⊔
Theorem 1. Progress
i) If Γ ⊢t : E, then either t is a value, or else for any security levels environ-
ment ∆and store φ such that Γ ⊢∆and Γ ⊢φ, there exists some t′ such
that t →∆φ t′.
ii) If Γ ⊢t : C, then either t is the empty command ϵ or else, for any security
types environment ∆and store φ such that Γ ⊢∆and Γ ⊢φ, there exists
some t′ and φ′ such that ⟨φ , t⟩→∆⟨φ′, t′⟩.
Proof. By induction on Γ ⊢t : E and Γ ⊢t : C:(selected cases only)
• Case T-Deref:
t : E ≡!a : E
a : E loc
The rule E-Deref applies (it follows from Γ ⊢φ that a ∈φ).
• Case T-SEnc:
t : E ≡senc(ek, em) : enc(E′)
ek : l key
em : E′
lvl(E′)
= l
By the induction hypothesis, either ek is a value, or else for any ∆and φ such
that Γ ⊢∆and Γ ⊢φ, there exists some e′
k such that ek →∆φ e′
k. Similarly
for em. If ek is not a value then E-SEnc1 applies; if em is not a value (but
ek is) then E-SEnc2 applies; if ek is a junk value then E-SEnc4 applies;
if em is a junk value (and ek is a non-junk value) then E-SEnc3 applies; if
both ek and em are non-junk values then t is a value.

Towards a Type System for Security APIs
181
• Case T-SDec: t : E ≡sdec(ek, em) : E
ek : l key
em : enc(E)
lvl(E) = l
By the induction hypothesis, either ek is a value, or else for any ∆and φ such
that Γ ⊢∆and Γ ⊢φ, there exists some e′
k such that ek →∆φ e′
k. Similarly
for em. If ek is not a value then E-SDec1 applies, and if em is not a value
(but ek is) then E-SDec2 applies. If ek is a value then, by Lemma 2, it
must be of the form n or junk(n). The former case is covered by the rules
E-SDec3, E-SDec4 and E-SDec5 as described below; in the latter case,
E-SDec6 applies. If em is a value then, by Lemma 2, it must be of the
form senc(uk, um) or junk(senc(uk, um)). In the ﬁrst case, it follows from
Lemma 1 that Γ ⊢uk : l′ key, Γ ⊢um : E′ and lvl(E′) = l′, where enc(E) :>
enc(E′) (therefore E :> E′). If ek = uk then E-SDec3 applies and if ek ̸= uk
then E-SDec4 applies. In the second case, where em ≡junk(senc(uk, um)),
the rule E-SDec5 applies. For rules E-SDec3 and E-SDec4, the inequality
lm ≤lk will be satisﬁed because it follows from Lemma 1 that ek : l key ∈Γ,
and from Γ ⊢∆that ek : ⊤∈∆, thus lk = ⊤.
• Case T-Assign:
t : C ≡a := e : cmd
a : E loc
e : E
By the induction hypothesis for Part(i), either e is a value, or else for any
∆and φ such that Γ ⊢∆and Γ ⊢φ, there exists some e′ such that e →∆φ e′
. If e is a value, then E-Assign2 applies, otherwise E-Assign1 applies. In
the former case, the inequality will hold because, by Lemma 1, a : E loc ∈Γ,
by Γ ⊢∆, a : lvl(E) ∈∆therefore la = lvl(E), and by Cor. 1, lv ≤lvl(E).
• Case T-Cmds:
t : C ≡c1 ; c2 : cmd
c1 : cmd
c2 : cmd
By the induction hypothesis, either c1 is the empty command ϵ or else, for
any ∆and φ such that Γ ⊢∆and Γ ⊢φ, there exists some c′
1 and φ′ such
that ⟨φ, c1⟩→∆⟨φ′, c′
1⟩. If c1 ≡ϵ then the rule E-Cmds2 applies, otherwise
the rule E-Cmds1 applies.
⊓⊔
Theorem 2. Preservation
i) If Γ ⊢t : E, Γ ⊢∆, φ and there exists some t′ such that t →∆φ t′, then
Γ ⊢t′: E.
ii) If Γ ⊢t : C, Γ ⊢∆, φ and there exists some t′ and φ′ such that ⟨φ , t⟩→∆⟨φ′, t′⟩,
then Γ ⊢φ′ and Γ ⊢t′: C.
Proof. By induction on Γ ⊢t : E and Γ ⊢t : C:(selected cases only)
• Case T-Deref:
t : E ≡!a : E
a : E loc
E-Deref is the only evaluation rule which may apply, therefore t′ ≡φ(a).
By Γ ⊢φ, ∃E′ such that Γ ⊢a : E′ loc and Γ ⊢φ(a) : E′. It therefore follows
that E′ ≡E and so the result holds.
• Case T-SEnc: t : E ≡senc(ek, em) : enc(E)
ek : l key
em : E
lvl(E) = l
There are four evaluation rules which correspond to the transition t →∆φ t′:
E-SEnc1 through E-SEnc4. Subcase E-SEnc2 has a similar proof to sub-
case E-SEnc1, and subcase E-SEnc4 has a similar proof to subcase E-
SEnc3.

182
G. Keighren, D. Aspinall, and G. Steel
• Subcase E-SEnc1:
ek →∆φ e′
k
t′ ≡senc(e′
k, em)
The T-SEnc rule has a subderivation whose conclusion is ek : l key and
the induction hypothesis gives us e′
k : l key. Therefore, in conjunction with
em : E and lvl(E) = l, we can apply the rule T-SEnc to conclude that
senc(e′
k, em) : enc(E).
• Subcase E-SEnc3:
ek ≡uk
em ≡junk(um)
t′ ≡
junk(senc(uk, um))
The T-SEnc rule has a subderivation whose conclusion is junk(um) : E,
and by Lemma 1 we get um : E. Therefore, in conjunction with uk : l key
and lvl(E) = l, we can apply T-SEnc and T-Junk to conclude that
junk(senc(uk, um)) : enc(E).
• Case T-SDec: t : E ≡sdec(ek, em) : E
ek : l key
em : enc(E)
lvl(E) = l
There are six evaluation rules which correspond to the transition t →∆φ t′:
E-SDec1 through E-SDec6. Subcases E-SDec1 and E-SDec2 have similar
proofs to subcase E-SEnc1 above; subcases E-SDec5 and E-SDec6 have
a similar proof to subcase E-SEnc3 above.
• Subcase E-SDec3:
ek ≡n
em ≡senc(n, t′)
TheT-SDecrulehasa subderivationwhoseconclusionissenc(n,t′) : enc(E).
It follows from Lemma 1 that Γ ⊢t′ : E′ and enc(E′) <: enc(E). Thus E′<: E,
and we can apply the T-Sub rule to conclude that Γ ⊢t′ : E.
• Subcase E-SDec4:
em ≡senc(uk, um)
ek ̸= uk
t′ = junk(um)
The
T-SDec
rule
has
a
subderivation
whose
conclusion
is
senc(uk, um) : enc(E). It follows from Lemma 1 that Γ ⊢um : E′and enc(E′)
<: enc(E). Thus E′ <: E, and we can apply T-Sub and T-Junk to con-
clude that Γ ⊢junk(um) : E.
• Case T-Assign:
t : C ≡a := e : cmd
a : E loc
e : E
Two evaluation rules may correspond to the transition ⟨φ, t⟩→∆⟨φ′, t′⟩: E-
Assign1 and E-Assign2. The proof for the latter is trivial.
• Subcase E-Assign1:
⟨φ, e⟩→∆⟨φ, e′⟩
t′ = a := e′
The T-Assign rule has a subderivation whose conclusion is e : E. Ap-
plying the induction hypothesis to this subderivation gives us Γ ⊢e′ : E.
In conjunction with the other subderivation Γ ⊢a : E loc, we can apply
T-Assign to conclude that Γ ⊢a := e′ : cmd. Γ ⊢φ′ follows immediately
from the fact that φ = φ′.
• Case T-Cmds:
t : C ≡c1 ; c2 : cmd
c1 : cmd
c2 : cmd
Two evaluation rules may correspond to the transition ⟨φ, t⟩→∆⟨φ′, t′⟩: E-
Cmds1 and E-Cmds2. The proof for the latter is trivial.
• Subcase E-Cmds1:
⟨φ, c1⟩→∆⟨φ′, c′
1⟩
t′ = c′
1 ; c2
The T-Cmds rule has a subderivation whose conclusion is c1 : cmd and
the induction hypothesis gives us Γ ⊢φ′ and Γ ⊢c′
1 : cmd. Using the
latter of these, in conjunction with the other subderivation Γ ⊢c2 : cmd,
we can apply the rule T-Cmds to conclude that Γ ⊢c′
1 ; c2 : cmd.
⊓⊔

Towards a Type System for Security APIs
183
The following lemma states that the type of an expression is preserved under
evaluation with respect to a well-typed store, independent of the actual val-
ues contained in the locations of that store, and is required to prove our non-
interference result.
Lemma 3. If (Γ, a : E loc) ⊢e : E ′, Γ ⊢v : E, (Γ, a : E loc) ⊢∆, φ and e →∆φ′
∗
v′,
where φ′ ≡φ[a →v], then Γ ⊢v′: E ′.
Proof. By induction on (Γ, a : E loc) ⊢e : E′:(selected cases only)
• Case T-Deref:
e : E′ ≡!a′ : E′
a′ : E′ loc
!a′ →∆φ′ φ′(a′). By Lemma 1, a′ : E′ loc ∈Γ. If a = a′ then v′ = v and E′ ≡E,
thus the result holds. If a ̸= a′ then v′ = φ(a′) and the result follows from
(Γ, a : E loc) ⊢φ.
• Case T-SEnc:
e : E′ ≡senc(ek, em) : enc(E′′)
ek : l key
em : E′′
lvl(E′′) = l
senc(ek, em) →∆φ′
∗
senc(vk, vm) →∆φ′
∗
v′, where ek →∆φ′
∗
vk and em →∆φ′
∗
vm.
By the induction hypothesis, Γ ⊢vk : l key and Γ ⊢vm : E′′. If vk and vm are
both non-junk values, then v′ ≡senc(vk, vm) and the the result follows from
T-SEnc. Otherwise, vk ≡junk(uk) and/or vm ≡junk(um), therefore v′ ≡
junk(senc(uk, um)) and the result follows from T-Junk and T-SEnc.
• Case T-SDec:
e : E′ ≡sdec(ek, em) : E′
ek : l key
em : enc(E′)
sdec(ek, em) →∆φ′
∗
sdec(vk, vm) →∆φ′
∗
v′ where ek →∆φ′
∗
vk and em →∆φ′
∗
vm.
By the induction hypothesis, Γ ⊢vk : l key and Γ ⊢vm : enc(E′), and by
Lemma 2, vk is of the form n or junk(n), and vm is of the form senc(uk, um)
or junk(senc(uk, um)). In both cases for vm, it follows from Lemma 1 that
Γ ⊢um : E′′, where E′′ <: E′. By inspection of the evaluation rules, v′ will
be of the form um or junk(um). In the ﬁrst case, we can apply T-Sub to
Γ ⊢um : E′′ and E′′ <: E′ to conclude that Γ ⊢um : E′; in the second case, the
result follows from T-Sub and T-Junk.
⊓⊔
5
Indistinguishability
Our type system is intended for analysing systems with ciphers that are repeti-
tion concealing and which-key concealing — also known as type-1 ciphers ([10],
Sec. 4.2). Repetition concealing means that it is not possible to say whether two
messages encrypted under the same key are equal. Which-key concealing means
that it is not possible to say whether two keys used to encrypt the same message
are equal. Both of these properties are possessed by standard block ciphers, such
as DES and AES, when used in CBC or CTR mode ([10], Sec. 4.4). However,
these deﬁnitions assume that the adversary is unable to correctly decrypt the
ciphertexts. This is not strictly the case with security APIs: the API functions
can be used to decrypt ciphertexts whose contents are public, whilst keeping the
actual values of the keys secret. As a result, we have to capture the ability of the

184
G. Keighren, D. Aspinall, and G. Steel
adversary to distinguish between ciphertexts which contain public data, under
certain circumstances.
We use the notation Γ ⊢v1 ∼l v2 : E to denote that the values v1 and v2 both
have type E and are indistinguishable at the security level l, and the notation
Γ ⊢φ ∼l φ′ to denote that the stores φ and φ′ are indistinguishable at the security
level l. In both cases, l denotes the maximum security level associated with the
locations that an observer can read directly.
Deﬁnition 3. We deﬁne the indistinguishability of two values, v1 and v2, with
respect
to
a
typing
environment
Γ
and
observation
level
l,
denoted
Γ ⊢v1 ∼l v2 : E, as the least symmetric relation closed under the following rules,
where Γ ⊢v1,v2 : E:
• Γ ⊢n1 ∼l n2 : l′ data iﬀ( l ≥l′) →(n1 = n2)
• Γ ⊢n1 ∼l n2 : l′ key iﬀ( l ≥l′) →(n1 = n2)
• Γ ⊢senc(uk, um) ∼l senc(u′
k, u′
m) : enc(E) iﬀ(Γ ⊢um ∼l u′
m : E) ∧
(Γ ⊢junk(um) ∼l u′
m : E ∨Γ ⊢uk ∼l u′
k: lvl(E) key)
• Γ ⊢junk(u) ∼l junk(u′) : E
• Γ ⊢junk(n) ∼l n′ : l′ data iﬀ( l < l′)
• Γ ⊢junk(n) ∼l n′ : l′ key iﬀ( l < l′)
• Γ ⊢junk(senc(uk, um)) ∼l senc(u′
k, u′
m) : enc(E) iﬀΓ ⊢junk(um) ∼l u′
m : E
If a value has a type which permits it to be observed by the adversary, we
must assume that this will eventually occur. It then follows that unencrypted
data items which can be observed must be equal for them to be considered
indistinguishable. Keys will be distinguishable if the output from their use is
distinguishable. That is, by encrypting a known value with each key, decrypting
each ciphertext with both keys, then comparing the ﬁnal results to the original
input: if any of the outputs are distinguishable from the input, then the two keys
cannot be the same, and are thus distinguishable.
Ciphertexts are indistinguishable if their messages are indistinguishable, and
the keys must also be indistinguishable if the observer could otherwise determine
when one of the ciphertexts has been incorrectly decrypted. That is, if the keys
have a type which allows them to encrypt observable data, then we must assume
that the adversary is able to correctly decrypt each ciphertext, and can thus
determine whether or not the required keys are the same whenever he can predict
the correct output. It follows from the deﬁnition that keys which operate on non-
observable data are indistinguishable.
Two junk values are indistinguishable, since they are both essentially just
random bit-strings. For this reason also, junk names are distinguishable from
observable non-junk names. Junk ciphertext is indistinguishable from non-junk
ciphertext if the results of decrypting each one cannot be distinguished.
Deﬁnition 4. We deﬁne the indistinguishability of two stores, φ1 and φ2, with
respect to a typing environment Γ and observation level l, denoted Γ ⊢φ1 ∼l φ2,
as the least relation closed under the following rules:

Towards a Type System for Security APIs
185
• Γ ⊢ϵ ∼l ϵ
• Γ ⊢(φ, a →v) ∼l (φ′, a →v′) iﬀΓ ⊢φ ∼l φ′, Γ ⊢v, v′ : E and Γ ⊢v ∼l v′ : E
This deﬁnition states that two stores are indistinguishable if their domains are
equal, and the values stored in equivalent locations are indistinguishable.
6
Non-interference
Informally, non-interference states that changes to non-observable inputs should
have no eﬀect on observable outputs. For expressions, this means that given
two indistinguishable stores (which diﬀer in the contents of at least one non-
observable location), the ﬁnal values obtained by fully evaluating the same ex-
pression with respect to those stores should be indistinguishable. For command
sequences, this means that given two indistinguishable stores (which again diﬀer
in the contents of at least one non-observable location), the stores which result
from fully evaluating the same command sequence with respect to those stores
should also be indistinguishable.
Theorem 3. Non-Interference
i) If
(Γ, a : E loc)
⊢
e : E ′,
Γ ⊢v1, v2 : E
and
Γ ⊢∆,φ1,φ2,
such
that
Γ ⊢v1 ∼l v2 : E and Γ ⊢φ1 ∼l φ2, then it follows from e →∆φ′
1
∗v′
1 and
e →∆φ′
2
∗v′
2 that Γ ⊢v′
1 ∼l v′
2 : E ′, where φ′
i ≡φi[a →vi].
ii) If
(Γ, a : E loc)
⊢
c : C,
Γ ⊢v1, v2 : E
and
Γ ⊢∆,φ1,φ2,
such
that
Γ ⊢v1 ∼l v2 : E and Γ ⊢φ1 ∼l φ2, then it follows from ⟨c, φ′
1⟩→∆
∗⟨ϵ, φ′′
1⟩
and ⟨c, φ′
2⟩→∆
∗⟨ϵ, φ′′
2⟩that Γ ⊢φ′′
1 ∼l φ′′
2, where φ′
i ≡φi[a →vi].
Proof. By induction on (Γ, a : E loc) ⊢e : E′ and (Γ, a : E loc) ⊢c : C:(selected cases
only)
• Case T-Deref:
e : E′ ≡!a′ : E′
a′ : E′ loc
!a′ →∆φ′
i φ′
i(a′). If a′ = a, then v′
i = vi and E′ ≡E, thus the result follows
from Γ ⊢v1 ∼l v2 : E. If a′ ̸= a, the result follows from Γ ⊢φ1 ∼l φ2.
• Case T-SEnc:
e : E′ ≡senc(ek, em) : enc(E′′)
ek : l′ key
em : E′′
lvl(E′′) = l′
senc(ek, em) →∆φ′
1
∗
senc(vk, vm) →∆φ′
1
∗
v′
1 where ek →∆φ′
1
∗
vk and em →∆φ′
1
∗
vm.
senc(ek, em) →∆φ′
2
∗
senc(v′
k, v′
m) →∆φ′
2
∗
v′
2 where ek →∆φ′
2
∗
v′
k and em →∆φ′
2
∗
v′
m.
It follows from Lemma 3 that Γ ⊢vk, v′
k : l′ key and Γ ⊢vm, v′
m : E′′, by
Lemma 2, vk ≡n or junk(n), and v′
k ≡n′ or junk(n′), and by deﬁnition,
vm ≡um or junk(um), and v′
m ≡u′
m or junk(u′
m). If vk ≡n and vm ≡um
then v′
1 ≡senc(n, um) [A]; if vk ≡junk(n) and vm ≡um then, by E-SEnc4,
v′
1 ≡junk(senc(n, um)) [B]; if vk ≡n and vm ≡junk(um) then, by E-SEnc3,
v′
1 ≡junk(senc(n, um)) [C], and if vk ≡junk(n) and vm ≡junk(um) then, by
E-SEnc4, E-Junk1, E-SEnc3 and E-Junk2, v′
1 ≡junk(senc(n, um)) [D].
The equivalent outcomes for v′
2 are denoted by [E] through [H]. There are
16 cases for Γ ⊢v′
1 ∼l v′
2 : E′ which we need to consider (resulting from the
cross product of [A,B,C,D] and [E,F,G,H]):

186
G. Keighren, D. Aspinall, and G. Steel
• Subcase [A]×[E]:
Γ ⊢senc(n, um) ∼l senc(n′, u′
m) : enc(E′′)
By the induction hypothesis, Γ ⊢n ∼l n′ : l′ key and Γ ⊢um ∼l u′
m : E′′,
and since lvl(E′′) = l′, the result follows immediately from Def. 3.
• Subcase [A]×[F]:
Γ ⊢senc(n, um) ∼l junk(senc(n′, u′
m)) : enc(E′′)
By the induction hypothesis, Γ ⊢n ∼l junk(n′) : l′ key thus, by Def. 3, l < l′.
Γ ⊢senc(n, um) ∼l junk(senc(n′, u′
m)) : enc(E′′) iﬀΓ ⊢um ∼l junk(u′
m) : E′′
and this holds when l < lvl(E′′). The result then follows from lvl(E′′) = l′
and l < l′.
• Subcases [A]×[G,H]:
Γ ⊢senc(n, um) ∼l junk(senc(u′
k, u′
m)) : enc(E′′)
By the induction hypothesis, Γ ⊢um ∼l junk(u′
m) : E′′, thus the result fol-
lows immediately from Def. 3.
• Subcases[B,C,D]×[F,G,H]:Γ ⊢junk(senc(n, um)) ∼l junk(senc(n′, u′
m)) : enc(E′′)
The result follows immediately from Def. 3.
Subcase [B]×[E] is similar to subcase [A]×[F] and subcases [C,D]×[E] are
similar to subcases [A]×[G,H].
• Case T-SDec:
e : E′ ≡sdec(ek, em) : E′
ek : l′ key
em : enc(E′)
lvl(E′) = l′
sdec(ek, em) →∆φ′
1
∗sdec(vk, vm) →∆φ′
1
∗v′
1, where ek →∆φ′
1
∗vk and em →∆φ′
1
∗vm.
sdec(ek, em) →∆φ′
2
∗sdec(v′
k, v′
m) →∆φ′
2
∗v′
2, where ek →∆φ′
2
∗v′
k and em →∆φ′
2
∗v′
m.
It follows from Lemma 3 that Γ ⊢vk, v′
k : l′ key and Γ ⊢vm, v′
m : enc(E′), and
by Lemma 2, vk ≡n or junk(n), v′
k ≡n′ or junk(n′), vm ≡senc(ua, ub) or
junk(senc(ua, ub)), and v′
m ≡senc(u′
a, u′
b) or junk(senc(u′
a, u′
b)). If vk ≡n
and vm ≡senc(n, ub) then, by E-SDec3, v′
1 = ub [A]; if vk ≡n and vm ≡
senc(ua, ub) where ua ̸= n then, by E-SDec4, v′
1 = junk(ub) [B]; if vk ≡
junk(n) and vm ≡senc(ua, ub) then, by E-SDec5, v′
1 = junk(ub) [C]; if vk ≡
n and vm ≡junk(senc(ua, ub)) then, by E-SDec6, v′
1 = junk(ub) [D], and
if vk ≡junk(n) and vm ≡junk(senc(ua, ub)) then, by E-SDec6, E-Junk1,
E-SDec5 and E-Junk2, v′
1 = junk(ub) [E]. The equivalent outcomes for v′
2
are denoted by [F] through [J]. There are 25 subcases for Γ ⊢v′
1 ∼l v′
2 : E′
which we need to consider (resulting from the cross product of [A,B,C,D,E]
and [F,G,H,I,J]):
• Subcase [A]×[F]:
Γ ⊢ub ∼l u′
b : E′
By the induction hypothesis, Γ ⊢senc(n, ub) ∼l senc(n′, u′
b) : enc(E′), there-
fore it follows from Def. 3 that Γ ⊢ub ∼l u′
b : E′.
• Subcase [A]×[G]:
Γ ⊢ub ∼l junk(u′
b) : E′
Bytheinductionhypothesis,wehaveΓ ⊢senc(n, ub) ∼lsenc(u′
a, u′
b) : enc(E′)
and Γ ⊢n ∼l n′: l′ key. By Def. 3, it follows from the second of these that
l < l′ or n = n′. From the ﬁrst one, it follows that Γ ⊢ub ∼l u′
b : E′ as well
as either Γ ⊢n ∼l u′
a : lvl(E′) key or Γ ⊢ub ∼l junk(u′
b) : E′. In the latter
case, the result is immediate. In the former case, it follows from Def. 3
that l < lvl(E′) or n = u′
a. However, since lvl(E′) = l′, it must be the case
that l < lvl(E′), otherwise we would have n = n′ = u′
a which is prevented

Towards a Type System for Security APIs
187
by the requirement for [G] which states that n′ ̸= u′
a. The result then
follows from Def. 3.
• Subcases [A]×[H,J]:
Γ ⊢ub ∼l junk(u′
b) : E′
By the induction hypothesis, Γ ⊢n ∼l junk(n′) : l′ key, therefore it follows
from Def. 3 that l < l′. Since l′ = lvl(E′), the result follows from Def. 3.
• Subcase [A]×[I]:
Γ ⊢ub ∼l junk(u′
b) : E′
By the induction hypothesis, Γ ⊢senc(n, ub) ∼l junk(senc(v′
a, u′
b)) : enc(E′)
and so it follows from Def. 3 that Γ ⊢ub ∼l junk(u′
b) : E′.
• Subcases [B,C,D,E]×[G,H,I,J]:
Γ ⊢junk(ub) ∼l junk(u′
b) : E′
The result follows immediately from Def. 3.
Subcase [B]×[F] is similar to subcase [A]×[G]; subcases [C,E]×[F] are similar
to subcases [A]×[H,J], and subcase [D]×[F] is similar to subcase [A]×[I].
• Case T-Assign:
c : C ≡a′ := e : cmd
a′ : E loc
e : E
⟨a′ := e, φ′
1⟩
→∆
∗
⟨ϵ, φ′
1[a′ →v]⟩
where e →∆φ′
1
∗v,
and
⟨a′ := e, φ′
2⟩
→∆
∗
⟨ϵ, φ′
2[a′ →v′]⟩where e →∆φ′
2
∗v′. By the induction hypothesis for part (i),
Γ ⊢v ∼l v′ : E and thus, in conjunction with Γ ⊢φ′
1 ∼l φ′
2 and Γ ⊢v1 ∼l v2 : Γ,
the result follows from Def. 4.
• Case T-Cmds:
c : C ≡c1 ; c2 : cmd
c1 : cmd
c2 : cmd
⟨c1 ; c2, φ′
1⟩→∆
∗⟨ϵ ; c2, φ′′′
1 ⟩→∆⟨c2, φ′′′
1 ⟩→∆
∗⟨ϵ, φ′′
1⟩where ⟨c1, φ′
1⟩→∆
∗⟨ϵ, φ′′′
1 ⟩
and ⟨c1 ; c2, φ′
2⟩→∆
∗⟨ϵ ; c2, φ′′′
2 ⟩→∆⟨c2, φ′′′
2 ⟩→∆
∗⟨ϵ, φ′′
2⟩where ⟨c1, φ′
2⟩→∆
∗
⟨ϵ, φ′′′
2 ⟩. The result then follows by two applications of the induction hypoth-
esis.
⊓⊔
Theorem 3 guarantees that well-typed expressions and command sequences are
non-interferent. As an example, consider the following presentation of an API
function for encrypting low security data stored in msg loc with a key that is
itself encrypted (by a master key, km) and stored in a low security location,
ekey loc:2
km : ⊤key, ekey loc : enc(⊥key) loc,
Γ =

key loc : ⊥key loc, msg loc : ⊥data loc, res loc : enc(⊥data) loc

key loc := sdec(km, !ekey loc) ; res loc := senc(!key loc, !msg loc) : cmd
The non-interference theorem tells us that the above well-typed command se-
quence will not leak any information about the values of km and !key loc into
the low security locations.
7
Example: Wrap/Decrypt Attack
The wrap/decrypt attack ([11], Sec. 2.3) is one of the most basic attacks which a
key management API can be susceptible to. In short, a sensitive key is altered in
2 Recall that we treat all keys as high security values, and the security level associated
with a key’s type denotes the level of data that it may encrypt.

188
G. Keighren, D. Aspinall, and G. Steel
[⇒l = ⊤]
[⇒E′ ≡⊤key]
k1 : ⊤key ∈Γ
k2 : ⊤key ∈Γ
[ Holds ]
[⇒E ≡enc(⊤key)]
Γ ⊢k1 : l key
Γ ⊢k2 : E′
lvl(E′) = l
[ Holds ]
x : enc(⊤key) loc ∈Γ
Γ ⊢senc(k1, k2) : enc(E′)
enc(E′) <: E
Γ ⊢x : E loc
Γ ⊢senc(k1, k2) : E
Γ ⊢x := senc(k1, k2) : cmd
Fig. 5. Successful typing derivation for the wrap command
[⇒E′ ≡⊤key]
[⇒l = ⊤]
x : enc(⊤key) loc ∈Γ
k1 : ⊤key ∈Γ
Γ ⊢x : enc(⊤key) loc
[ Holds ]
[⇒E ≡⊥data]
Γ ⊢k1 : l key
Γ ⊢!x : enc(E′)
lvl(E′) = l
⎡
⎣
Does
Not
Hold
⎤
⎦
y : ⊥data loc ∈Γ
Γ ⊢sdec(k1, !x) : E′
E′ <: E
Γ ⊢y : E loc
Γ ⊢sdec(k1, !x) : E
Γ ⊢y := sdec(k1, !x) : cmd
Fig. 6. Failed typing derivation for the decrypt command
such a way as to be able to wrap (encrypt) other sensitive keys and also decrypt
public data. This typically involves altering the key’s ‘type’ so that it is accepted
by each of the two required API functions. Alternatively, two copies of the key
can be obtained such that each copy has one of the two necessary types. Both
of these requirements can be quite straightforward to achieve (e.g., as discussed
in [7]). The outcome is that a sensitive key can be discovered by ﬁrst wrapping
it, then decrypting the result:
x := senc(k1, k2)
. . .‘wrap’ k2 with k1
y := sdec(k1, !x)
. . . recover k2
Our type system can be applied to these commands as follows:
k1 : ⊤key, k2 : ⊤key,
x := senc(k1, k2) : cmd
Γ =

x : enc(⊤key) loc, y : ⊥data loc

y := sdec(k1, !x) : cmd
Figure 5 shows the typing derivation for the wrap command, and Fig. 6 shows
the typing derivation for the decrypt command (unnecessary instances of the
T-Sub rule have been omitted in both cases).
The ﬁrst command type-checks, since lvl(E′) = l and enc(E′) <: E both hold,
but the second command does not, since E′ <: E does not hold. The ﬂaw
is that a sensitive piece of data is written to a public location — the failed
subtype condition indicates that the security level of the data is greater than
that of the location. Note that using the deﬁnition x : enc(⊥data) loc instead of
x : enc(⊤key) loc in the above example makes the second command type-check,

Towards a Type System for Security APIs
189
[⇒l′′ = ⊤]
[⇒E ≡l key]
key : ⊤key ∈Γ
wkey : enc(l key) ∈Γ
[ Holds ]
[⇒E′ ≡l′ key]
Γ ⊢key : l′′ key
Γ ⊢wkey : enc(E)
lvl(E) = l′′
⎡
⎣
May
Not
Hold
⎤
⎦
res : l′ key loc ∈Γ
Γ ⊢sdec(key, wkey) : E
E <: E′
Γ ⊢res : E′ loc
Γ ⊢sdec(key, wkey) : E′
Γ ⊢res := sdec(key, wkey) : cmd
Fig. 7. The unwrap command is only secure when l key <:l′ key (i.e., when l = l′)
but it prevents the ﬁrst command from type-checking, since enc(E′) <: E no
longer holds.
The wrap/decrypt attack is one of a number of attacks which initially require
the type of a key to be altered, therefore our type system should be able to
identify when an API command may allow this to occur. One such command is
‘unwrap,’ which takes an existing key and ciphertext corresponding to a second
key encrypted under the ﬁrst one, and then decrypts the ciphertext before storing
the result. Figure 7 shows that our type system is indeed able to identify that
the following instantiation of that command is insecure:
key : ⊤key,
Γ =

wkey : enc(l key), res : l′ key loc

res := sdec(key, wkey) : cmd
Since the security level associated with the type of a key restricts what that
key can be used to encrypt and decrypt, and the instantiation of the ‘unwrap’
command given above allows this level to be changed (i.e., when l ̸= l′), then it
is clearly insecure. This particular ﬂaw can be prevented in practice by including
usage information for the key within the ciphertext, thereby making it possible
to carry out a check which is equivalent to ensuring that l and l′ are equal.
However, it is then necessary to ensure that no API command allows this usage
information to be modiﬁed unintentionally.
8
Related Work
Vaughan and Zdancewic [12] give a security typed language in which valid pro-
grams are guaranteed to be non-interfering; a result which is achieved via a
combination of static and dynamic checks. However, they require that encrypted
messages adhere to a strict format which prevents their system from being used
to analyse many existing security APIs.
Laud [13] presents a weakened variant of non-interference termed ‘computa-
tional independence,’ using static analysis to track dependencies between
variables. Security is guaranteed when the public outputs are computationally
independent from all of the sensitive inputs. Encryption is probabilistic and
assumed to be secure with respect to a polynomially-bounded adversary. Key
cycles are permitted, as the rules will identify the resulting cyclic dependencies.

190
G. Keighren, D. Aspinall, and G. Steel
Focardi and Centenaro [14] give a type system for enforcing non-interference
in multi-threaded distributed programs which share common memory locations.
They use confounders (unique values associated with each new ciphertext) as
an abstraction of probabilistic encryption, and give a deﬁnition of equivalence
for low security values based on the notion of patterns [10]. If the confounder is
uniquely determined by the message and key, then their deﬁnition of indistin-
guishability for ciphertexts is equivalent to the one given in this paper. Their
deﬁnition for memories is stronger than our one since we do not distinguish be-
tween copies of the same ciphertext and diﬀerent ciphertexts created from the
same key and message (doing so is only necessary when considering condition-
als). However, because they deal with distributed systems where restrictions on
key usage cannot be enforced, they do not associate a secondary security level
with cryptographic keys which means that if a high security key is used to en-
crypt some low security data, the result of the subsequent decryption is forced
to be high.
Bengtson et al. [15] have developed an extended typechecker for F# code that
is annotated with reﬁnement types. A reﬁnement type includes a logical formula
which places restrictions upon the associated term. They consider an active
adversary and use a generalised version of the symbolic cryptography model.
The focus of their research is on authentication and authorisation properties for
security protocols, but the ﬂexibility aﬀorded by reﬁnement types means that the
technique may be applicable to related domains such as security API analysis.
However, due to the diﬀerent target domain, the underlying type system that
Bengtson et al. employ is quite diﬀerent from the one which we give in this
paper.
9
Conclusions and Future Work
Using typing rules for analysing the security properties of cryptographic systems
is not new (e.g., [16]), but it is common for restrictions to be placed upon the
use of encryption and decryption, as well as on any keys involved. Consequently,
certain security APIs cannot be analysed using some of these existing systems.
For example, the IBM 4758 [3] has one internal master key that is used to encrypt
all other keys which are then stored on the attached host, therefore rule sets in
which the result of a decryption cannot be used as a key (e.g., [17]) are unable
to analyse the security API for that device.
In this paper, we have presented the foundations of a type system that is de-
signed to deal with common features of security APIs such as encrypted keys and
nested encryptions. We gave a deﬁnition of indistinguishability which captures
the potential for an adversary to determine that the keys used in two cipher-
texts are diﬀerent, even though their actual values remain unknown. We then
proved that well-typed command sequences are non-interferent with respect to
this deﬁnition. We also proved the type-safety of our system meaning that the
type information can be ignored at run-time.
The next stage of our research is to extend our type system to include addi-
tional features present in Volpano, Smith and Irvine’s original type system [1]

Towards a Type System for Security APIs
191
and Volpano and Smith’s extension [18] — speciﬁcally procedures, primitive op-
erations and conditional statements. This will allow us to analyse more accurate
representations of functions in widely used security APIs such as PKCS#11 [20].
Adding conditionals will require a modiﬁed deﬁnition of the indistinguishability
of stores, similar to the one given by Focardi and Centenaro [14]. It should be
noted that such a change will not aﬀect our results for the indistinguishability
of expressions.
Further ahead, we plan to extend our type system to deal with data integrity,
since this is equally as important as data conﬁdentiality for key management
APIs, as well as permitting explicit declassiﬁcation thus allowing our system to
analyse an additional class of security APIs.
References
1. Volpano, D.M., Smith, G., Irvine, C.E.: A Sound Type System for Secure Flow
Analysis. Journal of Computer Security 4(3), 167–187 (1996)
2. Sumii, E., Pierce, B.C.: Logical Relations for Encryption. In: Proceedings of the
14th IEEE Computer Security Foundations Workshop (CSFW-14 2001), pp. 256–
269. IEEE Computer Society Press, Los Alamitos (2001)
3. IBM 4758 PCI Cryptographic Coprocessor,
http://www-03.ibm.com/security/cryptocards/pcicc/overview.shtml
4. nCipher nShield Hardware Security Module,
http://www.ncipher.com/en/Products/Hardware%20Security%20Modules/
nShield.aspx
5. Cortier, V., Keighren, G., Steel, G.: Automatic Analysis of the Security of XOR-
Based Key Management Schemes. In: Grumberg, O., Huth, M. (eds.) TACAS 2007.
LNCS, vol. 4424, pp. 538–552. Springer, Heidelberg (2007)
6. Courant, J., Monin, J.F.: Defending the Bank with a Proof Assistant. In: Proceed-
ings of the 6th International Workshop on Issues in the Theory of Security (WITS
2006), pp. 87–98 (2006)
7. Delaune, S., Kremer, S., Steel, G.: Formal Analysis of PKCS #11. In: [19], pp.
331–344
8. Youn, P.: The Analysis of Cryptographic APIs using the Theorem Prover Otter.
Master’s thesis, Massachusetts Institute of Technology (May 2004)
9. Youn, P., Adida, B., Bond, M.K., Clulow, J., Herzog, J., Lin, A., Rivest, R.L.,
Anderson, R.J.: Robbing the Bank with a Theorem Prover. Technical Report 644,
University of Cambridge Computer Laboratory (August 2005)
10. Abadi, M., Rogaway, P.: Reconciling Two Views of Cryptography (The Computa-
tional Soundness of Formal Encryption). In: Watanabe, O., Hagiya, M., Ito, T., van
Leeuwen, J., Mosses, P.D. (eds.) TCS 2000. LNCS, vol. 1872, pp. 3–22. Springer,
Heidelberg (2000)
11. Clulow, J.S.: On the Security of PKCS #11. In: Walter, C.D., Ko¸c, C¸.K., Paar, C.
(eds.) CHES 2003. LNCS, vol. 2779, pp. 411–425. Springer, Heidelberg (2003)
12. Vaughan, J.A., Zdancewic, S.: A Cryptographic Decentralized Label Model. In:
Proceedings of the 2007 IEEE Symposium on Security and Privacy, pp. 192–206.
IEEE Computer Society Press, Los Alamitos (2007)
13. Laud, P.: Handling Encryption in an Analysis for Secure Information Flow. In:
Degano, P. (ed.) ESOP 2003. LNCS, vol. 2618, pp. 159–173. Springer, Heidelberg
(2003)

192
G. Keighren, D. Aspinall, and G. Steel
14. Focardi, R., Centenaro, M.: Information Flow Security of Multi-threaded Dis-
tributed Programs. In: Proceedings of the 3rd ACM SIGPLAN Workshop on Pro-
gramming Languages and Analysis for Security (PLAS 2008), pp. 113–124. ACM
Press, New York (2008)
15. Bengtson, J., Bhargavan, K., Fournet, C., Gordon, A.D., Maﬀeis, S.: Reﬁnement
Types for Secure Implementations. In: [19], pp. 17–32
16. Abadi, M.: Secrecy by Typing in Security Protocols. In: Ito, T., Abadi, M. (eds.)
TACS 1997. LNCS, vol. 1281, pp. 611–638. Springer, Heidelberg (1997)
17. Laud, P., Vene, V.: A Type System for Computationally Secure Information Flow.
In: Li´skiewicz, M., Reischuk, R. (eds.) FCT 2005. LNCS, vol. 3623, pp. 365–377.
Springer, Heidelberg (2005)
18. Volpano, D.M., Smith, G.: A Type-Based Approach to Program Security. In:
Bidoit, M., Dauchet, M. (eds.) CAAP 1997, FASE 1997, and TAPSOFT 1997.
LNCS, vol. 1214, pp. 607–621. Springer, Heidelberg (1997)
19. Proceedings of the 21st IEEE Computer Security Foundations Symposium (CSF
2008). IEEE Computer Society Press, Los Alamitos (June 2008)
20. PKCS #11: Cryptographic Token Interface Standard,
http://www.rsa.com/rsalabs/node.asp?id=2133

Separating Trace Mapping and Reactive
Simulatability Soundness:
The Case of Adaptive Corruption⋆
Laurent Mazar´e1 and Bogdan Warinschi2
1 LexiFI S.A.S.
laurent.mazare@polytechnique.org
2 University of Bristol Department of Computer Science, University of Bristol,
Merchant Venturers Building, Woodland Road,
Bristol BS8 1UB, United Kingdom
bogdan@cs.bris.ac.uk
Abstract. Computational soundness is the research direction that aims
to translate security guarantees with respect to Dolev-Yao models into
guarantees with resepect to the stronger computational models of mod-
ern cryptography. There are essentially two diﬀerent approaches that
aim to achieve computational soundness. One approach is based on the
so-called trace mapping theorems, and one based on reactive simulata-
bility. In a recent paper, Backes, D¨urthmuth, and K¨usters have shown
that the stronger requirements needed for reactive simulatability-based
soundness imply that a trace mapping theorem also holds. It was left
as an open problem whether there exists interesting settings where the
simulatability framework breaks down but mapping theorems still exist.
In this paper we describe one such setting, and thus give a separation
between the two frameworks. Speciﬁcally, we show that adaptive corrup-
tion of symmetric encryption keys (a problematic setting for simulation-
based frameworks) can be smoothly treated in a mapping theorem-based
soundness framework.
A crucial ingredient of our proof, and a result of independent interest,
is a new (indistinguishability based) security notion for encryption. The
central feature of our deﬁnition is that in addition to standard chosen-
ciphertext attacks in multi-user settings, it also directly accounts for
adaptive corruption of decryption keys. We show that our notion satis-
ﬁes the intuitively appealing property that it is equivalent to standard
security requirements on encryption.
1
Introduction
Computational soundness is an important research direction in security that
aims to justify the high abstraction used by Dolev-Yao models with respect
to the more concrete computational ones. Two frameworks for computational
⋆This work was partially funded by the European Commission through the ICT pro-
gramme under Contract ICT-2007-216646 ECRYPT II.
P. Degano and L. Vigan`o (Eds.): ARSPA-WITS 2009, LNCS 5511, pp. 193–210, 2009.
c
⃝Springer-Verlag Berlin Heidelberg 2009

194
L. Mazar´e and B. Warinschi
soundness have been developed in the past few years: the trace mapping-based
and the reactive simulatability-based framework. Each of these frameworks es-
tablishes a connection between concrete and symbolic executions of protocols. In
turn these connections allow for security properties proved in the more abstract
models to be transfered to the more concrete ones.
The reactive simulatability approach (developed by Backes, Pﬁtzmann, and
Waidner [BPW03a, BPW03b, BP04]) uses the well-established notion of simula-
tion. Under this paradigm, one shows that concrete implementations can replace
idealized (symbolic) versions of cryptographic primitives inconspicuously, i.e.
without introducing any additional attacks. Such results are established through
a simulator which, interacting solely with the abstract version of the primitives,
oﬀers lexactly the same behavior to an external arbitrary environment (in partic-
ular, a protocol executed in the presence of an adversary) as a concrete (secure)
implementation. In the case of encryption, to emulate the ciphertexts created by
honest parties, the simulator usually encrypts some arbitrary, randomly selected
string of appropriate length. The idea is that if encryption is indeed secure, an
external environment would not be able to notice the diﬀerence between simu-
lated and honest executions.
The trace mapping approach (Micciancio and Warinschi [MW04]) is based on
a weaker, thus easier to establish link between symbolic and concrete executions.
Typically, one shows that for any protocol in a certain class, any concrete attack
produces an execution trace which is the image of a trace produced by a symbolic
attacker, unless the concrete attacker breaks one of the primitives used in the
execution of the protocol.
Reactive simulatability imposes a stricter relation between computational and
symbolic models than the trace mapping approach. Unsurprisingly, the imple-
mentation requirements needed to enforce a reactive simulatability relation are
more demanding, allow for less ﬂexibility than those for trace mapping, and
require more complex symbolic models. On the ﬂip side, reactive simulatabil-
ity comes with ”built-in” compositionality results that are missing for the trace
mapping approach. Therefore, understanding the trade-oﬀoﬀered by the two
frameworks is important: it allows choosing between simpler vs. complex secu-
rity models, symbolic proofs, and implementations with an eye on the desired
security guarantees.
An important step was recently taken by Backes, D¨urmuth, and K¨usters who
established the ﬁrst formal relation between the two frameworks [BDK07]. They
show that a reactive simulatability framework for computational soundness also
implies a trace mapping theorem. In this paper we further clarify the relation
between the two frameworks. We identify an important and interesting setting
where the tools and techniques of reactive simulatability fail, but a trace mapping
theorem can still be proved. We discuss the details of our results and their
signiﬁcance next.
Adaptive corruption and trace mapping soundness. Technically, our
main result is a computational soundness setting based on trace mapping. The
novelty of the setting consists in the setting that we use: in addition to standard

Separating Trace Mapping and Reactive Simulatability Soundness
195
attacks, the adversary may adaptively decide to corrupt the symmetric keys of
honest parties even after those keys had been used to produce encryptions.
Our trace mapping soundness theorem holds for a restricted class of protocols:
we do not allow for encryption keys to be sent encrypted (they can be still sent
in clear), and we do not allow ciphertext forwarding. These restrictions prevent
the adversary from obtaining key-dependent encryptions (an issue we do not
address in this paper) and thus in particular avoids the occurrence of encryption
cycles (and their associated diﬃculties) [AR00, BRS03, BPS07]. Nevertheless,
the class of protocols that we consider is still quite large.
More important than our technical result is its conceptual importance. A
soundness result via reactive simulatability for adaptive corruption of keys seems
very diﬃcult and perhaps impossible. Indeed, an environment can easily be used
to mimic the game that deﬁnes security of encryption under selective decryption
attacks, a known diﬃcult problem [DNRS03], and thus soundness would seem
to require solving the selective decryption problem.
More problematic is that in simulation based settings, adaptive corruption of
keys raises the so-called commitment problem. Recall that in such settings the
simulator has to produce a simulation of the computational execution of the
protocol. In doing so however, it only has access to the idealized execution. In
particular, it may have to produce encryption of values that it does not apriorily
know. As sketched above, in such situations the simulator may simply encrypt a
random string of appropriate length. An attacker would not be able to observe
the diﬀerence, unless the encryption scheme is not secure. The problem raised by
adaptive corruption of keys is now clear: an adversary that corrupts a key that
had been used to encrypt ”garbage” would be able to tell the diﬀerence between
real and simulated executions. This problem with adaptive corruption had been
formally captured by Nielsen who proves that in such settings, secure systems
require keys that are as long as the data to be encrypted [Nie02]. Therefore, our
soundness result also entails the ﬁrst separation result between trace mapping
and reactive simulatability based soundness.
Security of encryption against adaptive corruption. The various se-
curity notions for encryption developed so far, capture the idea that ciphertexts
reveal no partial information about the plaintext under a range of possible at-
tacks [NY90, RS92, DDN00, BBM00]. The tacit hope underlying existent def-
initions is that the strongest such notion (generally accepted to be IND-CCA)
captures security under all possible attacks that encryption should withstand
when used in higher-level protocols. Unfortunately this is not the case. Existent
deﬁnitions do not explicitly consider the possibility that an adversary can obtain
some of the keys of the honest parties (through corruption, or possibly through
legitimate means).
A second result of this paper is a novel security deﬁnition for encryption that
overcomes the above problem. The central feature of our deﬁnition is that in
addition to standard chosen-ciphertext attacks in multi-user settings, it also di-
rectly accounts for corruption of decryption keys. Importantly, the corruption
model that we consider is adaptive: the adversary can choose which keys to

196
L. Mazar´e and B. Warinschi
corrupt after seeing ciphertexts under those keys. We call the resulting notions
indistinguishability under chosen-plaintext (respectively, chosen-ciphertext) at-
tacks with adaptive corruptions IND-ACCPA (respectively, IND-ACCCA) security,
in short. Our deﬁnitions are given for symmetric encryption, but can be adapted
for the asymmetric case straightforwardly.
Finally, we relate our notion to standard security notions for encryption. We
prove the intuitively appealing property that the security of plaintexts encrypted
under uncorrupted keys is not aﬀected by corruption of other keys (as long as
the corrupted keys do not encrypt related plaintexts). More formally, we prove
via a tight reduction that an IND-CPA (respectively IND-CCA) secure encryption
scheme is also IND-ACCPA (respectively IND-ACCCA) secure. The proof uses a
subtle modiﬁcation of the hybrid argument used in [BBM00].
Our new deﬁnition is a crucial ingredient in the soundness result described
above. Furthermore, we expect the deﬁnition to have further applications to se-
curity proofs of larger protocols that use encryption. To clarify what we mean
by this, consider some arbitrary protocol that uses encryption and imagine the
following security proof by reduction. (Similar issues occur in our soundness
result, so the discussion bellow is also relevant there.) Given some adversary
A against the protocol, one constructs an adversary B against the encryp-
tion. The latter adversary uses access to the encryption/decryption oracles in
the game deﬁning the security of encryption to simulate the execution of the
protocol for A. During this simulation B needs to handle the corruption re-
quests of A. In the case when corruption is static (i.e. A decides whom to cor-
rupt priorly to the actual execution), adversary B can generate the keys for
all corrupt parties on its own and hand them over as responses to A’s cor-
rupt requests. For the simulation of the remaining honest parties B uses its
oracles.
The case of adaptive corruption is signiﬁcantly more complicated since there
is no way for B to a priori determine which subset of keys will be corrupted by
the adversary. A simple minded simulation where B guesses this subset succeeds
only with negligible probability, so some more elaborate techniques should be
employed.1 It is clear that the simulation sketched can beneﬁt greatly from a se-
curity game for encryption that provides B with the ability to corrupt keys – that
part of the simulation would then become trivial – and this is precisely what our
new deﬁnition oﬀers. Our deﬁnition and equivalence result oﬀers thus the follow-
ing proof methodology: show the security via a reduction to IND-ACCPA security
of the underlying encryption scheme (the simulation now can take advantage of
the key corruption capabilities) and then conclude, via our equivalence proof,
security of the system based on IND-CCA security. In essence, this methodology
avoids repeating the hybrid argument for the equivalence between IND-CCA and
IND-ACCCA (which had been done once and for all).
1 Notice that in this discussion we assume that the reduction is to the security of
encryption deﬁned in the multi-user setting. A reduction to the security of a single
key immediately lands into trouble if the adversary B needs to encrypt the same
message under the challenge key, and under the key of some other honest party.

Separating Trace Mapping and Reactive Simulatability Soundness
197
2
Related Work
Computational soundness. Security models for encryption that enable com-
putationally sound symbolic analysis were recently analyzed by Backes, Pﬁtz-
mann, and Scedrov [BPS07] in the general setting of a universally composable
library [BPW03b, BP04]. The attacker model that they develop combines key-
related messages with dynamic corruption of keys and leads to a notion that
is incomparable to ours: under their attack model the attacker can not corrupt
a key after it had been used. The resulting soundness results are also diﬀerent
since they concern incomparable classes of protocols: unlike [BPS07] we allow
adaptive corruption of keys, but use some restrictions that are not needed in
that work.
Common and Cortier [CCL08] include symmetric encryption in a process cal-
culus for security for which they prove computational soundness of an associ-
ated process equivalence. Although they do allow for nested encryption, adaptive
corruption is forbidden in their calculus. A soundness result for symmetric en-
cryption had previously been obtained by Laud [Lau04]. The framework that
he considers can cope with protocols that send symmetric encryption keys after
they had been used, but does not permit adaptive corruption of such keys. Fur-
thermore, his results do not use a mapping lemma technique and thus do not
yield a separation result akin to the one here.
The issue of adaptive corruption in computational soundness had also been ad-
dressed by Gupta and Shmatikov [GS06]. Their result however does not consider
fully adaptive corruptions since corruption must occur prior to parties actually
using their encryption key. Our security notion for encryption, and our compu-
tational soundness result, permit corruptions based on the values of ciphertexts
under the secret encryption key.
Adaptively secure encryption. The security of encryption in settings where
the adversary can obtain adaptively some of the decryption keys has been recog-
nized early on as a very important cryptographic issue, and has since generated
quite a bit of research. The main diﬀerence between the deﬁnition that we de-
veloped and those used in prior research on the subject is that we use indistin-
guishability, game-based deﬁnitions, while the latter follow simulatability-based
formulations. Although quite popular due to the strength and uniformity of the
resulting deﬁnitions as well as their intuitive appeal, simulation-based deﬁnitions
may lead to notions which are unsatisﬁable [DNRS03, Nie02].
A lot of research has been devoted to circumventing the diﬃculties associ-
ated to this adaptive corruption in simulation-based settings. Beaver and Haber
propose a solution that uses interaction and memory erasure [BH92]. Canetti
et. al. [CFGN96], followed by Beaver [Bea97], and Damgard and Nielsen [DN00]
develop the concept of non-committing encryption (roughly, a ciphertext may
be decrypted to arbitrary plaintexts, for appropriately chosen keys) which cir-
cumvents the commitment problem. Finally, a more recent proposal of Canetti,
Halevi, and Katz [CHK05] shows that adaptive security of encryption schemes
can be achieved if the decryption key is allowed to somehow evolve over time.

198
L. Mazar´e and B. Warinschi
Given that the security requirements on encryption under simulation based
deﬁnitions are stronger than those captured by our deﬁnition, and in light of
the negative result in [Nie02], it should come as no surprise that the solutions
outlined above, are all rather ineﬃcient, and do not follow from standard assump-
tions on encryption. In contrast, our game-based notion imposes less stringent
requirements on the security of encryption and can be shown to be a conse-
quence of existing requirements on encryption. Clearly, we make no claim that
we deal with adaptive security of encryption in its full generality, since the
results of [Nie02] apply. We do claim however that our deﬁnition captures a
reasonable level of security which still suﬃces for many applications where se-
curity is not deﬁned via simulations. In some sense the situation resembles that
of commitment schemes. While it is widely known that universally composable
commitments do not exist [CF01], other, perfectly reasonable game-based secu-
rity deﬁnitions for commitments can both be achieved and used in a wide range
of applications [Gol99].
More recently, Backes, Pﬁtzmann, and Scedrov proposed a security deﬁnition
for encryption based on indistinguishability in which they consider adversaries
who can both observe key-dependent ciphertexts, and perform corruption of
secret keys [BPS07]. Our deﬁnition does not consider the key-dependent message
capabilities. Also, the corruption model there seems to be weaker, as it does not
permit an adversary to corrupt keys which had been used to encrypt.
3
Adaptive Corruption in Indistinguishability-Based
Security of Encryption
In this section we give and motivate our deﬁnition for the security of encryption
when the adversary can adaptively corrupt keys.
Notation and preliminaries. We write [k] for the set {1, 2, . . ., k}. If A is a
probabilistic algorithm we write x
$←AO1,O2,...(input) for the result of running
algorithm A on input input, with access to oracles O1, O2, . . ., all initialized with
fresh coins.
We recall the standard deﬁnition for symmetric encryption schemes. Let SE
be a symmetric encryption scheme deﬁned by three algorithms KG, E and D.
The key generation algorithm KG takes as input a security parameter η and
outputs a key k. We write k
$←KG(η) for the process of generating key k. The
encryption algorithm E is randomized. It takes as input a bit-string m and a key
k and returns the encryption of m using k. We write c
$←E(k, m) for the process
of producing the encryption c of m under k. Finally, the decryption algorithm D
takes as input a bit-string c representing a ciphertext and a key k and outputs
the corresponding plain-text. It is required that D(k, c) = m.
Next, we recall the standard notion of IND-CCA security for symmetric en-
cryption schemes. Let SE = (KG, E, D) be a symmetric encryption scheme.
Let b ∈{0, 1} and η ∈N a security parameter. Let A be an adversary that
has access to a left-right encryption oracle E(k, LR(·, ·, b)), parametrized by a

Separating Trace Mapping and Reactive Simulatability Soundness
199
Experiment ExpIND-CCA-b
SE,A
(η)
k
$←KG(η)
d
$←AE(k,LR(·,·,b)),D(k,·)(η)
Return d
Experiment ExpIND-ACCCA-b
A,SE,p
(η)
For i = 1 to p(η) do ki ←KG(η)
keys = (k1, k2, . . . , kp(η))
d
$←AE(keys,·),E(keys,LR(·,·,b)),C(keys),D(keys,·)(η)
Return d
Fig. 1. Experiments for deﬁning IND-CCA and IND-ACCCA security of symmetric en-
cryption scheme SE. In the IND-CCA experiment the adversary is not allowed to query
the decryption oracle with a ciphertext it obtains from its encryption oracle. In the
IND-ACCCA experiment, the adversary is not allowed to query the decryption oracle
with (i, c) if c was the result of an encryption query using key ki. CPA versions of the
above experiments are obtained by removing the decryption oracle D.
bit b and keyed with key k. The oracle accepts as input pairs of equal-length
messages (m0, m1) and returns the encryption of mb. In addition, the adversary
also has access to a decryption oracle D(k, ·) which on a query c returns D(k, c).
We give the experiment ExpIND-CCA-b
SE,A
(η) for IND-CCA security in Figure 1.
We deﬁne the advantage of A against IND-CCA security of SE by:
AdvIND-CCA
SE,A
(η) = Pr

ExpIND-CCA-1
SE,A
(η) = 1

−Pr

ExpIND-CCA-0
SE,A
(η) = 1

We say that the encryption scheme SE is IND-CCA secure if for all probabilistic
polynomial time adversaries A, AdvIND-CCA
SE,A
(η) is a negligible function of η.
3.1
Security of Encryption with Adaptive Corruption
Our starting point is the security deﬁnition that Bellare, Boldyreva, and Micali
give for the security of encryption in a multi-user setting [BBM00]. We adapt
their deﬁnition from the asymmetric to the symmetric case, and extend it to
capture key corruptions.
In their security model, an adversary has access to polynomially many left-
right encryption oracles, parametrized by the same hidden bit b, but which use
independently generated encryption keys. The adversary also has access to corre-
sponding decryption oracles. Importantly, this setup allows the adversary to see
encryptions of the same message under diﬀerent keys. The goal of the adversary
is to determine b.
In addition to the standard attacks captured as above, we would like to allow
the adversary to corrupt some of the encryption keys. One possibility is to keep
the same setup, and let the adversary corrupt some of the keys used by the left-
right oracles, as long as the adversary makes no queries to those oracles (since
otherwise, such queries combined with key corruption lead to trivial attacks).
The resulting notion would not be strong enough as the adversary cannot de-
cide to corrupt a key once that key had been used to encrypt. Our solution is
to provide the adversary with access to standard encryption oracles under the
keys used by the left-right oracles which the adversary can query at will. The

200
L. Mazar´e and B. Warinschi
restriction that we impose is that ciphertexts returned by a left-right oracle are
not sent to the corresponding decryption oracle.
We next formally deﬁne our new security notions. We use several oracles,
which use as parameter an ordered list of keys keys = (k1, k2, . . .). Oracle
E(keys, ·) allows the adversary to see encryptions of any message under the keys
in keys. It accepts queries of the form (i, m) ∈(N × {0, 1}∗) and returns an en-
cryption E(ki, m) if i is a valid index in the list keys. Oracle E(keys, LR(·, ·, b)) is
a left-right encryption oracle parametrized by keys and a bit b. When it receives
a query of the form (i, m0, m1) ∈(N×{0, 1}∗×{0, 1}∗) this oracle returns an en-
cryption E(ki, mb) if i is a valid index in set of keys, and m0 and m1 have the same
length. The corruption oracle C(keys) gets as input an index i and returns the
key ki if the key is deﬁned. In the chosen-ciphertext attack version, the adversary
also has access to a decryption oracle OD which on input a pair (i, c) ∈N×{0, 1}∗
returns D(k, c). The formal experiment for deﬁning IND-ACCCA security is given
in Figure 1. As before, the experiment for IND-ACCPA is obtained by removing
access to the decryption oracle.
Deﬁnition 1. For an adversary A against SE in the IND-ACCCA experiment
in Figure 1 we deﬁne its advantage by:
AdvACCCA
A,SE,p(η) = Pr

ExpIND-ACCCA-1
A,SE,p
(η) = 1

−Pr

ExpIND-ACCCA-0
A,SE,p
(η) = 1

We say that scheme SE is IND-ACCCA secure if for any polynomial p, and
any probabilistic polynomial-time adversaries A, AdvACCCA
A,SE,p(η) is a negligible
function.
Relating IND-ACCCA and IND-CCA security. Although apparently the no-
tion that we deﬁne is stronger, due to the extra corruption capabilities, we now
show that this is not the case. More precisely we prove the following theorem.
Theorem 1. Let SE be a symmetric encryption scheme and p be a polynomial.
Encryption scheme SE is IND-CCA secure if and only if it is IND-ACCCA secure.
Clearly, security of encryption in the IND-CCA sense is a particular case of
IND-ACCCA where the size of the list keys is 1, and the adversary does not
have access to the corruption oracle OC so any IND-ACCCA secure scheme is also
IND-CCA secure. We prove that the converse also holds. More precisely, we show
that for any adversary A against IND-ACCCA of SE, there exists an adversary
B against IND-CCA and a polynomial p such that
AdvIND-ACCCA
A,SE,p
(η) ≤p(η) · AdvIND-CCA
B,SE
(η)
The proof of the theorem uses a hybrid argument and is given in Appendix A.
Furthermore, we can only prove that the reduction factor in the above theorem
is optimal. We postpone the result for the full version of this paper.

Separating Trace Mapping and Reactive Simulatability Soundness
201
4
Computational Soundness with Adaptive Corruptions
In this section we give our computational soundness result. As explained in the
introduction, our goal is to show that adaptive corruption of ciphertexts can
be treated by symbolic methods in a computationally sound manner. The re-
sult that we present in this section uses the trace mapping approach introduced
Micciancio and Warinschi [MW04]. We do not attempt a strict extension of pre-
vious results that use this framework (e.g. the extension that treats asymmetric
encryption and digital signature of Cortier and Warinschi [CW05]). The reason
is that nested symmetric and asymmetric encryption combined with adaptive
corruptions raises speciﬁc problems that are outside the goals of this paper.
Instead, we consider a simpliﬁed framework in which symmetric keys have
already been distributed to parties. The class of protocols that we consider uses
these keys, together with random nonces, party identities, and symmetric en-
cryption. Furthermore, we consider protocols that do not send encryption keys
over the network. We stress that keys used by protocol participants can be
corrupted by the adversary. We prove that the computational traces of such
protocols are the images of valid symbolic traces, with overwhelming probability
over the choice of randomness used in the system. It had been shown in pre-
vious work how to use this soundness theorem to prove transference theorem
of large classes of security properties [MW04, CW05]. The framework follows
closely [MW04, CW05]. We highlight the diﬀerences that are due to adaptive
corruption.
Protocol Syntax. We consider protocols that allow parties to exchange mes-
sages built from identities and randomly generated nonces using symmetric en-
cryption. Consider an algebraic signature Σ with sorts ID, Key, Nonce, Ciphertext,
and Pair for respectively agent identities, keys, nonces, ciphertexts, and pairs.
The sort Term is a supersort containing all other sorts, except Key. There are
three operations: sk is deﬁned on ﬁnite sets of the sort ID and return the
symmetric key shared by the input identities. The other operations that we
consider are pairing ⟨
, ⟩: Term × Term →Pair and symmetric encryption
{ } : Term×Key →Ciphertext. Protocols are speciﬁed using the algebra of terms
constructed over the above signature from a set X of sorted variables. Speciﬁ-
cally, X = X.n ∪X.a ∪X.c ∪X.s ∪X.l, where X.n, X.a, X.c are sets of variables
of sort nonce, agent, ciphertext respectively. Furthermore, X.a and X.n are as
follows. If k ∈N is some ﬁxed constant representing the number of protocol par-
ticipants, w.l.o.g. we ﬁx the set of agent variables to be X.a = {A1, A2, . . . , Ak},
and partition the set of nonce variables, by the party that generates them. For-
mally: X.n = ∪A∈X.aXn(A) and Xn(A) = {Xj
A | j ∈N}. This partition avoids
to specify later, for each role, which variables stand for generated nonces and
which variables stand for expected nonces.
The messages that are sent by participants are speciﬁed using terms in TΣ(X),
the free algebra generated by X over the signature Σ. The individual behavior
of each protocol participant is deﬁned by a role that describes a sequence of
message receptions/transmissions. A k-party protocol is given by k such roles.

202
L. Mazar´e and B. Warinschi
Deﬁnition 2 (Roles and protocols). The set Roles of roles for protocol par-
ticipants is deﬁned by Roles = (({Πi} ∪TΣ(X)) × (TΣ(X) ∪{stop}))∗. A k-party
protocol is a mapping Π : [k] →Roles.
We assume that a protocol speciﬁcation is so that Π(j) = ((lj
1, rj
1), (lj
2, rj
2), . . .),
the j’th role in the deﬁnition of the protocol being executed by player Aj. Each
sequence ((l1, r1), (l2, r2), . . .) ∈Roles speciﬁes the messages to be sent/received
by the party executing the role: at step i, the party expects to receive a message
conforming to li and returns message ri. We stress that terms lj
i , rj
i are not
actual messages but specify how the message that is received and the message
that is output should look like.
Hypothesis over protocols. We restrict our results to the class of executable
protocols, i.e. protocols where each role can be implemented by an executable
program, using only the local knowledge of the corresponding agent. This re-
quires in particular that any sent message (corresponding to some rj
k) is always
deducible from the previously received messages (corresponding to lj
1, . . . , lj
k). We
further restrict the class of protocols so that roles may not use nested encryptions
(i.e. encryptions of encryptions) and may not send the keys, and encryptions of
the keys used in the protocol. Notice that in particular ciphertext forwarding is
not permitted as an adversary could use this ability to create nested ciphertexts.
This also means that each message received by a party can be parsed completely.
Execution Models. For the protocols speciﬁed in the language described
above we give symbolic and computational execution models. As usual, the sym-
bolic execution is carried out using symbolic messages, and in the presence of
a Dolev-Yao adversary. This adversary can inject messages into the communi-
cation, but can only “compute” new messages obeying the standard Dolev-Yao
restrictions. In the concrete execution model, the messages that are exchanged
are bit-strings and the honest parties and the adversary are p.p.t. Turing ma-
chines.
Formal Execution Model. Messages that parties exchange in the symbolic
execution model are elements of the algebra T f deﬁned by:
T f ::= ai | sk(a1, . . . , an) | n(a, j, s) | ⟨T f , T f⟩| {T f}sk(a1,...,an)
where a, a1, . . . , an ∈ID, j, s ∈N.
Intuitively, the ﬁrst three items correspond to identities, shared symmetric
keys, and random nonces. We deﬁne the initial knowledge of an agent A by
kn(A) = {sk(X) | A ∈X} ∪Xn(A) i.e. an agent knows its shared keys, and also
knows the nonces that it generates during the protocol execution.
Formally, the execution of the protocol in the symbolic world is deﬁned by
a state transition system. A global state of the system is given by (SId, f, H)
where H is a set of terms of T f representing the messages sent on the network
and f maintains the local states of all sessions ids SId. Session ids are tuples
of the form (n, j, (a1, a2, . . . , ak)) ∈(N × N × IDk), where n ∈N identiﬁes the
session, a1, a2, . . . , ak are the identities of the parties that are involved in the

Separating Trace Mapping and Reactive Simulatability Soundness
203
Initial knowledge:
t ∈S
S ⊢t
b ∈X.a
S ⊢b
Pairing and unpairing:
S ⊢t1
S ⊢t2
S ⊢⟨t1 , t2⟩
S ⊢⟨t1 , t2⟩
i ∈{1, 2}
S ⊢ti
Encryption and decryption:
S ⊢t
S ⊢{t}sk(a1...an)
S ⊢{t}sk(a1...an)S ⊢sk(a1 . . . an)
S ⊢t
Fig. 2. Deduction rules for the formal adversary
protocol and j is the index of the role that is executed in this session. Mathe-
matically, f is a function f : SId →([X →T f] × N × N), where f(sid) = (σ, i, p)
is the local state of session sid. The function σ is a partial instantiation of
the variables occurring in role Π(i) with elements from T f and p ∈N is the
control point of the program. The transition between global states are deter-
mined by actions of the adversary. The adversary can corrupt parties, trigger
the initialization of new sessions, and send messages to these sessions. The se-
mantics of these actions is straightforward and is identical to prior work. We
stress that as described in the introduction we now deal with adversaries who
can adaptively corrupt parties. This is exhibited by the fact that the adver-
sary can issue at any point a corruption request (which contrasts with prior
frameworks where the corruption request was issued only in the beginning of the
execution).
We write SID = N × N × IDk for the set of all session ids. We deﬁne the set of
symbolic execution traces by
SymbTr =

2SID ×

SID→

X →T f
× N × N

× 2T f ∗
As usual in symbolic models [DY83], we restrict the messages that the ad-
versary can send. More precisely, we consider the deduction relation ⊢deﬁned
in Figure 2, and require that the messages sent by the adversary are computed
from its knowledge, and the messages that it had seen so far using this deduction
relation. In the ﬁgure, S ⊢t means that the adversary is able to compute the
term t from the set of terms S.
We remark that the rule that deﬁnes encryption is not standard in symbolic
models and reﬂects a real capacity that computational adversary may possess: it
may be possible for an adversary to create ciphertexts under an encryption key
that it does not know, even if the encryption scheme is, say, IND-CCA secure. An
alternative is to use the standard deﬁnition for symbolic security of encryption,
but require that the scheme that is used in the implementation is both IND-CCA
secure, and satisﬁes a version of authentication, i.e. IND-CTXT [BN00].

204
L. Mazar´e and B. Warinschi
Deﬁnition 3. An execution trace (SId1, f1, H1), . . . , (SIdn, fn, Hn) is said to be
valid if the terms sent by the adversary are computed by Dolev-Yao operations.
Formally, it is required that whenever the adversary sends some message t, then
it is the case that Hi ⊢t. Given a protocol Π, we write Execs(Π) for the set of
its valid symbolic execution traces.
Concrete Execution Model. In concrete execution, the messages that are
exchanged by parties are actual bit-strings that depend on the security parame-
ter η (which determines for example the length of random nonces). We write Cη
for the set of valid messages. The set Cη has subsets that contain values for agent
identities, nonces, keys, ciphertexts, and pairs. We write Cη.a, Cη.n, Cη.s, Cη.c,
and Cη.p for these subsets, respectively. We assume that the implementation
is such that each bit-string in Cη has a unique type which can be eﬃciently
recovered via the function type : Cη →{a, n, s, c, p}. In the concrete implemen-
tation, encryption is implemented with a symmetric scheme SE = (Ke, Enc, Dec)
which we ﬁx throughout this section. Pairing is implemented by some standard
(eﬃciently invertible) encoding function ⟨· , ·⟩: Cη × Cη →Cη.p.
We represent the global state of the execution by a pair (SId, f). Here SId is
the set of all session ids for the session currently executed, and f is a function
that keeps track of the local state of each such session. More formally, session
ids are tuples (n, i, (a1, a2, . . . , al)), where n ∈N is a unique session identiﬁer,
i is the index of the role executed in this session and a1, a2, . . . ak ∈Cη are
the names of the agents involved in running this session. The state function
f : SId →[X →Cη]×N×N, given a session id sid returns f(sid) = (σ, i, p) where
σ assigns values to the variables of the program executed in this session (see the
discussion regarding the execution of individual roles), i is the index of the role
executed in this session and p is the program counter that keeps track of the
next step to be executed in this session. Similarly to the symbolic setting, we
represent the execution in this setting via transitions between global states. As
in the symbolic model the adversary can corrupt parties, initiate new sessions,
and send and receive messages to these sessions. Due to space limitations we
defer the details for the full version of this paper.
The execution model that we described above uses randomization: the ad-
versary is probabilistic, and the honest parties use randomization for generat-
ing nonces and encryptions. It can be shown that if the adversary A runs in
polynomial-time, then the honest parties use a number of coins that is a poly-
nomial in the security parameter. In the following, for a ﬁxed adversary A we
denote by {0, 1}pA(η), resp. by {0, 1}gA(η), the spaces from where the adversary,
resp. the honest parties, draw the coins used in the execution. Notice that each
pair of random coins (RA, RΠ) ∈{0, 1}pA(η) × {0, 1}gA(η) determines a unique
sequence of global states (SId1, f1), (SId2, f2), . . ., called the concrete trace deter-
mined by random coins (RΠ, RA). We write ExecΠ(RΠ),A(RA)(η) for this trace.
We write SId = N × [k] × (Cη.a)k for the set of all possible session ids. The set
of all possible concrete traces is
ConcTr =

η

2SId × (SId →([X →Cη] × N × N))

.∗

Separating Trace Mapping and Reactive Simulatability Soundness
205
Soundness Result. First, observe that concrete traces can be regarded as
instantiations of formal traces by appropriate mappings from the abstract to
the concrete representations. If we let ts = (SIds
1, f1, H1), . . . , (SIds
n, fn, Hn) and
tc = (SIdc
1, g1), . . . , (SIdc
n, gn) be a symbolic and a concrete execution trace, then
we say that trace tc is a concrete instantiation of ts (or alternatively ts is a
symbolic representation of tc) and we write ts ⪯tc if there exists an injective
function c : T f →Cη such that for every i ∈[n] it holds that SIds
i = SIdc
i and
for every sid ∈SIds
i if fi(sid) = (σsid, isid, psid) and gi(sid) = (τ sid, jsid, qsid) then
τsid = c ◦σsid, isid = jsid and psid = qsid.
Informally, the soundness result that we obtain states that concrete traces of
probabilistic polynomial adversaries are in fact instantiations of valid symbolic
traces. This idea is captured formally by the following theorem.
Theorem 2. Let Π be an executable protocol. If the concrete implementation
scheme SE = (KG, E, D) is IND-CCA secure and for any bit-string bs D(k, bs)
and D(k′, bs) cannot both succeed if k ̸= k′ then for any p.p.t. algorithm A
Pr

∃ts ∈Execs(Π) | ts ⪯Execc
Π(RΠ),A(RA)(η)

is overwhelming. The probability is over the choice of randomness (RΠ, RA) used
by the honest participants and the adversary.
Proof sketch: The proof of the above proposition does not use any conceptually
novel ideas over prior work on trace mapping soundness [MP05, CW05]. Here we
discuss it at a high level and highlight the technical role played by our security
deﬁnition for encryption.
The proof proceeds by contradiction. Assuming the existence of a computa-
tional adversary A which with non-negligible probability produces an execution
trace which is the image of a non Dolev-Yao trace, we show how to construct an
adversary B who breaks IND-CCA security for SE. We use Theorem 1 and we
construct B against IND-ACCCA.
We proceed as follows: in a non Dolev-Yao trace (and thus in its computational
counterpart), the adversary outputs at some point some data which is supposedly
secret (it is only sent encrypted under keys that the adversary does not know).
Due to the restrictions on the protocol, it can be easily seen that this secret is
a nonce (had we allowed for nested encryption, this secret could have also been
a ciphertext). Adversary B ﬁrst guesses which of the secret nonces used during
the execution of A will actually be produced by A (there are only polynomially
many nonces, so the guess succeeds with suﬃciently high probability), and then
B proceeds to oﬀer to A a simulation of the protocol with which A is supposed
to interact. The simulation is such that the keys of the honest parties coincide
with the keys used by the oracles of B (recall that B is against IND-ACCCA). B
uses its decryption oracle to parse the messages sent by the adversary (in the
case when these messages contain encryptions), and uses its encryption oracle
to produce appropriate responses. The crucial aspect of the simulation is that
ciphertexts which contain the secret nonce are produced using the left-right

206
L. Mazar´e and B. Warinschi
encryption oracle. Adversary B passes to the oracle two messages containing
diﬀerent values for the nonce, and obtains the encryption of the message selected
according to the internal bit of the left-right oracle. Clearly, the ciphertexts
obtained this way never need to be decrypted and the nonce never needs to be
sent in clear (since in both cases the adversary would learn the nonce from the
honest execution). At some point the adversary outputs the message that is not
obtained using only Dolev-Yao computations, and which in particular contains
the secret nonce. Adversary A then obtains the value of the nonce, and thus
determines the internal bit of the left-right oracle.
During the simulation adversary A may decide to corrupt encryption keys.
It is here where the enhanced capabilities of the adversary against encryption
comes into play: B can easily answer such queries using access to its own key
corruption oracle. Clearly, adversary A never asks to corrupt a key which was
used to encrypt the secret nonce (as otherwise the nonce would not be secret),
so all its corruption queries can be answered.
It has been already shown how to use this result to obtain general trans-
ference theorems for large classes of trace-based security notions [MW04] (e.g.
entity authentication). We note that the proof of the above proposition can
also be modiﬁed to yield transfer of secrecy properties from the symbolic to the
computational world [CW05].
References
[AR00]
Abadi, M., Rogaway, P.: Reconciling two views of cryptography (the com-
putational soundness of formal encryption). In: Watanabe, O., Hagiya,
M., Ito, T., van Leeuwen, J., Mosses, P.D. (eds.) TCS 2000. LNCS,
vol. 1872, p. 3. Springer, Heidelberg (2000)
[BBM00]
Bellare, M., Boldyreva, A., Micali, S.: Public-key encryption in a multi-
user setting: Security proofs and improvements. In: Preneel, B. (ed.) EU-
ROCRYPT 2000. LNCS, vol. 1807, pp. 259–274. Springer, Heidelberg
(2000)
[BDK07]
Backes, M., D¨urmuth, M., K¨usters, R.: On simulatability soundness and
mapping soundness of symbolic cryptography. In: Arvind, V., Prasad, S.
(eds.) FSTTCS 2007. LNCS, vol. 4855, pp. 108–120. Springer, Heidelberg
(2007)
[Bea97]
Beaver, D.: Plug and play encryption. In: Kaliski Jr., B.S. (ed.) CRYPTO
1997. LNCS, vol. 1294, pp. 75–89. Springer, Heidelberg (1997)
[BH92]
Beaver, D., Haber, S.: Cryptographic protocols provably secure against
dynamic adversaries. In: Rueppel, R.A. (ed.) EUROCRYPT 1992. LNCS,
vol. 658, pp. 307–323. Springer, Heidelberg (1992)
[BN00]
Bellare, M., Namprempre, C.: Authenticated encryption: Relations
among notions and analysis of the generic composition paradigm. In:
Okamoto, T. (ed.) ASIACRYPT 2000. LNCS, vol. 1976, pp. 531–545.
Springer, Heidelberg (2000)
[BP04]
Backes, M., Pﬁtzmann, B.: Symmetric encryption in a simulatable Dolev-
Yao style cryptographic library. In: CSFW 2004: Proceedings of the 17th
IEEE Computer Security Foundations Workshop (CSFW 2004), Wash-
ington, DC, USA, p. 204. IEEE Computer Society, Los Alamitos (2004)

Separating Trace Mapping and Reactive Simulatability Soundness
207
[BPS07]
Backes, M., Pﬁtzmann, B., Scedrov, A.: Key-dependent message security
under active attacks - BRSIM/UC-soundness of symbolic encryption with
key cycles. In: Proceedings of 20th IEEE Computer Security Foundation
Symposium (CSF) (June 2007); preprint on IACR ePrint 2005/421
[BPW03a]
Backes, M., Pﬁtzmann, B., Waidner, M.: A universally composable cryp-
tographic library (2003)
[BPW03b]
Backes, M., Pﬁtzmann, B., Waidner, M.: A composable cryptographic
library with nested operations. In: CCS 2003: Proceedings of the 10th
ACM conference on Computer and communications security, pp. 220–
230. ACM Press, New York (2003)
[BRS03]
Black, J., Rogaway, P., Shrimpton, T.: Encryption-scheme security in the
presence of key-dependent messages. In: Nyberg, K., Heys, H.M. (eds.)
SAC 2002. LNCS, vol. 2595, pp. 62–75. Springer, Heidelberg (2003)
[CCL08]
Cortier, V., Comon-Lundh, H.: Computational soundness of observa-
tional equivalence. In: CCS 2008: Proceedings of the 15th ACM con-
ference on Computer and communications security, pp. 109–118 (2008)
[CF01]
Canetti, R., Fischlin, M.: Universally composable commitments. In: Kil-
ian, J. (ed.) CRYPTO 2001. LNCS, vol. 2139, pp. 19–40. Springer, Hei-
delberg (2001)
[CFGN96]
Canetti, R., Feige, U., Goldreich, O., Naor, M.: Adaptively secure multi-
party computation. In: SToC 1996: Proceedings of the 28th annual ACM
Symposium on Theory of Computing, pp. 639–648. ACM Press, New
York (1996)
[CHK05]
Canetti, R., Halevi, S., Katz, J.: Adaptively-secure, non-interactive
public-key encryption. In: Kilian, J. (ed.) TCC 2005. LNCS, vol. 3378,
pp. 150–168. Springer, Heidelberg (2005)
[CW05]
Cortier, V., Warinschi, B.: Computationally sound, automated proofs for
security protocols. In: Sagiv, M. (ed.) ESOP 2005. LNCS, vol. 3444, pp.
157–171. Springer, Heidelberg (2005)
[DDN00]
Dolev, D., Dwork, C., Naor, M.: Nonmalleable cryptography. SIAM J.
Comput. 30(2), 391–437 (2000)
[DN00]
Damgaard, I., Nielsen, J.B.:
Improved non-committing encryption
schemes based on a general complexity assumption. In: Bellare, M. (ed.)
CRYPTO 2000. LNCS, vol. 1880, pp. 432–450. Springer, Heidelberg
(2000)
[DNRS03]
Dwork, C., Naor, M., Reingold, O., StockmeyerMagic, L.: Magic func-
tions. Journal of the ACM 50(6), 852–921 (2003)
[DY83]
Dolev, D., Yao, A.: On the security of public key protocols. IEEE Trans-
actions on Information Theory 29(2), 198–208 (1983)
[Gol99]
Goldreich, O.: Modern cryptography, probabilistic proofs and pseudoran-
domness. Springer, Berlin (1999)
[GS06]
Gupta, P., Shmatikov, V.: Key conﬁrmation and adaptive corruptions in
the protocol security logic. In: Proceedings of FLOC Joint Workshop on
Foundations of Computer Security and Automated Reasoning for Secu-
rity Protocol Analysis – FCS-ARSPA 2006 (2006)
[Lau04]
Laud, P.: Symmetric encryption in automatic analyses for conﬁdentiality
against active adversaries. In: Proc. 25th IEEE Symposium on Security
& Privacy, pp. 71–85 (2004)

208
L. Mazar´e and B. Warinschi
[MP05]
Micciancio, D., Panjwani, S.: Adaptive security of symbolic encryption.
In: Kilian, J. (ed.) TCC 2005. LNCS, vol. 3378, pp. 169–187. Springer,
Heidelberg (2005)
[MW04]
Micciancio, D., Warinschi, B.: Soundness of formal encryption in the pres-
ence of active adversaries. In: Naor, M. (ed.) TCC 2004. LNCS, vol. 2951,
pp. 133–151. Springer, Heidelberg (2004)
[Nie02]
Nielsen, J.B.: Separating random oracle proofs from complexity theo-
retic proofs: The non-committing encryption case. In: Yung, M. (ed.)
CRYPTO 2002. LNCS, vol. 2442, pp. 111–126. Springer, Heidelberg
(2002)
[NY90]
Naor, M., Yung, M.: Public-key cryptosystems provably secure against
chosen ciphertext attacks. In: ACM Symposium on Theory of Computing,
pp. 427–437 (1990)
[RS92]
Rackoﬀ, C., Simon, D.R.: Non-interactive zero-knowledge proof of knowl-
edge and chosen ciphertext attack. In: Feigenbaum, J. (ed.) CRYPTO
1991. LNCS, vol. 576, pp. 433–444. Springer, Heidelberg (1992)
A
Proof of Theorem 1
We prove the theorem in two steps. First, we show that the IND-CCA and
IND-ACCCA notions are equivalent whenever the IND-ACCCA adversary is given
access to encryption/decryption oracles that use a single key. Then, we leverage
on this result to prove the general case.
Lemma 1. Let SE be a symmetric encryption scheme. Then for any polynomial
time adversary A against IND-ACCCA, there exists a polynomial time adversary
B against IND-CCA such that:
AdvIND-ACCCA
A,SE,1
(η) = AdvIND-CCA
B,SE
(η)
Proof. Let A be an adversary against IND-ACCCA. We construct an adversary
B against IND-CCA as follows. Adversary B (which has access to the left-right
encryption oracle E(k, LR(·, ·, b)) and corresponding decryption oracle D(k, ·))
runs A as a subroutine and answers its queries as follows.
1. When A sends a query (m0, m1) to its left-right oracle, B sends (1, m0, m1)
to its own oracle and forwards the answer to A.
2. When A sends a query m to its normal encryption oracle, adversary B for-
wards (m, m) to its left-right encryption oracle and obtains some answer c.
It then records the pair (m, c) and forwards the answer to A.
3. When A sends c to its decryption oracle, then A searches its list of recorded
pairs for an entry (m, c). If such an entry exists then it sends m to B. Oth-
erwise, it sends c to its decryption oracle and forward the answer to A.
4. Finally, if A queries its corruption oracle, then B outputs 1 as its guess and
ﬁnishes its execution.
Let Evb be the event that A uses its corruption oracle in the ExpIND-ACCCA-b
game and let Evb the complementary event. The ﬁrst important observation is

Separating Trace Mapping and Reactive Simulatability Soundness
209
that Evb does not depend on b. This is true because A is not allowed to make use
of its left-right oracle if it corrupts the key used by this oracle. In the remainder
of this proof we write Ev for both Ev0 and Ev1. Due to the same reason, it is
the case that Pr

ExpIND-ACCCA-1
A,SE
= 1 | Ev

= Pr

ExpIND-ACCCA-0
A,SE
= 1 | Ev

. We
therefore have that:
AdvIND-ACCCA
A,SE,1
(η) =
= Pr[Ev] · Pr

ExpIND-ACCCA-1
A,SE
(η) = 1 | Ev

−Pr[Ev] · Pr

ExpIND-ACCCA-0
A,SE
(η) = 1 | Ev

+
Pr[Ev] · Pr

ExpIND-ACCCA-1
A,SE
(η) = 1 | Ev

−Pr[Ev] · Pr

ExpIND-ACCCA-0
A,SE
(η) = 1 | Ev

= Pr[Ev] · Pr

ExpIND-ACCCA-1
A,SE
(η) = 1 | Ev

−Pr[Ev] · Pr

ExpIND-ACCCA-0
A,SE
(η) = 1 | Ev

To ﬁnish up the analysis, we calculate the advantage of adversary B in the
IND-CCA game:
AdvIND-CCA
B,SE
(η) =
= Pr

ExpIND-CCA-1
B,SE
(η) = 1

−Pr

ExpIND-CCA-0
B,SE
(η) = 1

= Pr[Ev] · Pr

ExpIND-CCA-1
B,SE
(η) = 1 | Ev

−Pr[Ev] · Pr

ExpIND-CCA-0
B,SE
(η) = 1 | Ev

+ Pr[Ev] · Pr

ExpIND-CCA-1
B,SE
(η) = 1 | Ev

−Pr[Ev] · Pr

ExpIND-CCA-0
B,SE
(η) = 1 | Ev

= Pr[Ev] −Pr[Ev] +
Pr[Ev] · Pr

ExpIND-ACCCA-1
A,SE
(η) = 1 | Ev

−Pr[Ev] · Pr

ExpIND-ACCCA-0
A,SE
(η) = 1 | Ev

= AdvIND-ACCCA
A,SE,1
(η)
Lemma 2. Let SE be a symmetric encryption scheme and p be a polynomial.
Then for any probabilistic polynomial time adversary A against IND-ACCCA,
there exists a probabilistic polynomial time adversary B against IND-ACCCA such
that:
AdvIND-ACCCA
A,SE,p
(η) ≤p(η) · AdvIND-ACCCA
B,SE,1
(η)
Proof. Let SE be a symmetric encryption scheme and p be a polynomial. Then
we build p(η) adversaries Bj against IND-CPA that use A as a sub-routine (j
ranges between 1 and p(η)).
Adversary Bj works as follows. Recall that Bj works in the ExpIND-ACCCA-b
Bj,SE,1
(η)
experiment, so it has access to a left-right encryption oracle keyed with some
key k, a standard encryption oracle keyed with the same key, a decryption oracle
keyed with k, and a corruption oracle (which when called returns k). Bj generates
p(η) keys k1, k2, . . . , kp(η), using KG and simulates the environment of A. The
queries of A are answered as follows:
1. On query E(i, m) for a standard encryption, Bj proceeds as follows. If i ̸= j
then it simply sends an encryption E(ki, m). Otherwise, it sends (1, m) to
its own standard encryption oracle and forwards the answer to Aj.

210
L. Mazar´e and B. Warinschi
2. On a corruption query i, Bj returns ki if i ̸= j. Otherwise it calls its own
corruption oracle and forwards the answer to A.
3. On a decryption oracle (i, c), Bj returns D(ki, c) if i ̸= j; otherwise it sends
the query (1, c) to its own decryption oracle and forwards the answer to A.
4. Finally, on a left-right query (i, (m0, m1)), Bj answers with E(ki, m0) if i < j,
and with E(ki, m1) if i > j. Otherwise (i.e. when i = j) Bj sends (1, (m0, m1))
to its left-right encryption oracle and forwards the answer to A.
When A halts and outputs a bit d, then Bj outputs d and halts.
If follows from the way we set up the simulation that the output of
ExpIND-ACCCA-1
B1,SE,1
(η) is identically distributed with ExpIND-ACCCA-1
A,SE,p(η)
(η), that is:
Pr

ExpIND-ACCCA-b
B1,SE,1
(η) = 1

= Pr

ExpIND-ACCCA-b
A,SE,p(η)
(η) = 1

and similarly, Bp(η) where bit b is 0 is identical to the experiment involving A
where bit b equals 0.
Pr

ExpIND-ACCCA-0
Bp(η),SE,1
(η) = 1

= Pr

ExpIND-ACCCA-0
A,SE,p(η)
(η) = 1

Moreover, we have that:
Pr

ExpIND-ACCCA-0
Bj,SE,1
(η) = 1

= Pr

ExpIND-ACCCA-1
Bj+1,SE,1
(η) = 1

We then get that:
AdvIND-ACCCA
A,SE,p
(η) =
= Pr

ExpIND-ACCCA-1
A,SE,p
(η) = 1

−Pr

ExpIND-ACCCA-0
A,SE,p
(η) = 1

= Pr

ExpIND-CCA-1
B1,SE,1
(η) = 1

−Pr

ExpIND-CCA-0
Bp(η),SE,1(η) = 1

= Pr

ExpIND-CCA-1
B1,SE,1
(η) = 1

−Pr

ExpIND-CCA-0
B1,SE,1
(η) = 1

+
Pr

ExpIND-CCA-1
B2,SE,1
(η) = 1

−Pr

ExpIND-CCA-0
B2,SE,1
(η) = 1

+
. . .
Pr

ExpIND-CCA-1
Bp(η),SE,1(η) = 1

−Pr

ExpIND-CCA-0
Bp(η),SE,1(η) = 1

=
p(η)

j=1
AdvIND-CCA
Bj,SE,1 (η) ≤p(η) · AdvIND-CCA
B,SE,1 (η)
In the above, adversary B is the adversary that randomly samples an integer j
in [1, p(η)] and executes adversary Bj.

How Many Election Oﬃcials Does It Take to
Change an Election?
P.Y.A. Ryan
University of Luxembourg
Abstract. Electronic voting technologies have been quite widely de-
ployed in a number of democracies. In the US for example, approximately
30% of the electorate used Direct Recorded Electronic (DRE) machines
in the 2004 and 2008 presidential election. These systems have come in
for signiﬁcant criticism leading many experts to conclude that all digi-
tal technology used in voting must be ﬂawed. While these critiques are
fully justiﬁed and documented, to conclude that all voting technology
must be irredeemably ﬂawed is not in our view justiﬁed. The purpose
of this note is to argue that, while it is undoubtedly true that careless
introduction of information technology can undermine democracy, high
assurance, veriﬁable voting schemes do exist and are worthy of serious
consideration. Furthermore, such veriﬁable schemes can provide higher
levels of trustworthiness than traditional pencil and paper, hand count-
ing systems. The challenge remains to convince the various stakeholders
of the trustworthiness of such schemes.
1
Introduction
The history of democracy is littered with instances of the outcome of elections
being being altered by corruption of the process. Human ingenuity knows no
bounds when it comes to devising ways to subvert voting systems. The US has
a long, and not so proud, history of technological innovation in voting systems
dating back to the end of the 18th century with the lever machines and followed
by punch-cards, optical scanners, touch screens. For every one of these people
quickly devised techniques to subvert the outcome. Excellent descriptions of
this can be found in [7] and [8]. Indeed, Gumbel speaks of the “fallacy of the
technological ﬁx”.
More recently, information technology has been widely deployed in various
stages of elections: in maintaining an electoral role, in vote capture and for vote
counting. In the US for example, approximately 30% of the electorate used so-
called DRE (Directly Recorded Electronic) devices to cast and record their votes.
Here, the voters choices are (supposedly) directly recorded in computer memory.
Many experts have voiced serious concerns about such devices: the quality of
the security engineering of all such systems that have been made available for
examination has been shown to be abysmal, for example [11]. Indeed the whole
approach can be argued to be fundamentally ﬂawed: the accuracy of the count
depends critically on the correctness of the software running on the devices at the
P. Degano and L. Vigan`o (Eds.): ARSPA-WITS 2009, LNCS 5511, pp. 211–221, 2009.
c
⃝Springer-Verlag Berlin Heidelberg 2009

212
P.Y.A. Ryan
time of the election. Rivest and Wack, [15], have argued applying the principle
of software independence to voting systems: in essence that no change in the
software should result in an undetectable change in the outcome.
The suppliers of such DRE devices typically make claims about the veriﬁcation
and testing of the code but the openness and credibility of certiﬁcation process
is questionable. Typically the code is proprietary and copyright protected and
so no available for general scrutiny. The recent California Secretary of State’s
“Top-to-bottom” report demonstrates that a number of voting machines that
had been “certiﬁed” were not ﬁt for purpose. This throws into question the
whole certiﬁcation process for voting technologies, in the US at least.
Many experts have gone on to conclude from these observations that any use
of information technology in voting systems must undermine their trustworthi-
ness. We fully concur with the view that introducing systems whose integrity
depends on proprietary code that is not available for inspection must be inad-
missible. However, we take issue with the claim that the lack of trustworthiness
of many existing systems necessarily implies that no information technology can
provide trustworthy elections. Indeed, we will argue that trustworthy and prac-
tical voting schemes already exist and that they can provide greater levels of
assurance with weaker trust assumptions that the traditional paper and hand
counting approach.
1.1
Verify the Election, Not the System
In summary, we are advocating the serious consideration of voting schemes de-
signed to provide a high degree of transparency and auditability, within the
constraints of the privacy requirements. The correctness of the outcome of the
election should not depend on the correct behaviour of the code running during
the election. Rather it should be possible to detect any error that could lead to
a change in the tally.
Good candidates for such veriﬁable voting schemes do exist, several of which
are conceptually and technologically suﬃciently simple to be viable for use in
real elections. In the remainder of the paper I will outline a number of the most
interesting and promising such schemes.
1.2
Remote Voting
To be clear, I am not advocating remote, e.g., internet voting. Virtually any form
of remote voting, whether it be postal, phone, internet, is vulnerable to threats
of coercion and vote buying. It is extremely diﬃcult to enforce the isolation of
the booth in the remote context. Ingenious technical approaches to countering
coercion threats in remote voting have been proposed, for example [10]. Even
if some of these might be regarded as sound from a technical point of view, it
is not clear that the electorate at large would feel suﬃciently comfortable with
them for them to be truly eﬀective. A (technically) coercion resistent scheme
that is not understood and trusted by the majority of the electorate is not in
fact coercion resistent.

How Many Election Oﬃcials Does It Take to Change an Election?
213
1.3
Resilience
I am not advocating a further point to be made clear is that, whilst I assert that
the integrity of the count must not rely on the correct behaviour of the code, we are
not advocating ignoring veriﬁcation and testing. On the contrary, veriﬁcation and
testing must be our ﬁrst line of defence and it is essential that every eﬀort be made
to ensure that the election runs smoothly, in particular, to make it as diﬃcult as
possible to undermine the smooth accurate execution of the election.
Thus, veriﬁcation and testing is necessary but not suﬃcient: we need a second
line and more solid line of defence based on veriﬁcation of the election itself.
2
The Challenge of High-Assurance Voting Systems
Designing voting systems that are both trustworthy and trusted is immensely
challenging. We need to ensure that two conﬂicting requirements are meet si-
multaneously: accuracy and ballot privacy. Clearly either would be trivial in
isolation: for accuracy a simple show of hands is enough, for secrecy a constant
function that always return the same outcome regardless of votes cast is enough.
Alternatively, if we are prepared to place our trust in a third party, then again
the problem is quite trivial. However, our goal will be to guarantee both these
properties with minimal trust assumptions. Ideally we would like to eliminate
trust, in the sense of dependence, entirely. In practice it seems that it is not
feasible to reduce dependence to zero.
The challenge is made even more daunting by also requiring the systems to be
suﬃciently simple to easily usable and readily trusted by the majority of voters.
Advances have been made in recent years, mainly in schemes that make heavy
use of cryptographic techniques to reconcile these conﬂicting requirements. It is
essential that voters have suﬃcient understanding of the security mechanisms
for them to trust the guarantees they provide, and be motivated to contribute
to the dependability. Cryptography, by its very nature is rather sinister and
forbidding to the majority of the human race. A system that a cryptographer
claims is “transparent”, in the sense of highly auditable, may be totally opaque
to most voters. Thus we must ﬁnd ways to either avoid the use of cryptography,
or ensure that it’s use is simple enough and can be suﬃciently clearly explained,
perhaps vis suitable metaphors, to instill widespread trust.
Another fundamental problem with elections, in contrast to most other critical
systems, is that there is no “god’s eye” view that would tell us the correct answer.
3
A Brief History of Veriﬁable Voting
In this section I give a very brief outline of recent developments in veriﬁable
voting. This is necessarily very brief and makes no pretense of scholarship.
Veriﬁable, secret elections can be regarded as a special case of the secure,
distributed computation problem: a number of entities each know secret values
and they wish to compute some function of these values in a veriﬁable fashion in

214
P.Y.A. Ryan
such a way as not to reveal any of the secret values. Some very elegant schemes
have been proposed that take this approach but these typically require each
voter to have computational, in particular cryptographic capabilities. This is
ﬁne in theory and perhaps reasonable in some contexts, e.g. elections for oﬃcers
of the International Association for Cryptologic Research perhaps, but not a
reasonable assumption for most political elections. More recent developments
have striven to come up with schemes that minimise the capabilities required of
the voters and make the voter experience as simple and familiar as possible. It
is these schemes that I will focus on here.
The ﬁrst suggestion that cryptographic techniques could be applied to vot-
ing systems appears to be Chaum’s 1981 paper that introduced the idea of
anonymising mixes, [3]. In 1994, Benaloh and Tunistra, [2], introduce the no-
tion of coercion-resistance along with a scheme using homomorphic tabula-
tion that satisﬁes it. Later, Chaum [4] and Neﬀ[13] introduced schemes that
could be regarded as more practical than previous schemes. The original Prˆet `a
Voter scheme, [17] and [16] was inspired by the Chaum scheme, replacing the
visual cryptography by the candidate permutation concept. Chaum has subse-
quently adopted this concept in his new PunchScan scheme, [1]. Chaum sub-
sequently proposed Scantegrity II, [6] which will be outlined below. Recently,
Rivest has proposed ThreeBallot, [14] that provides voter-veriﬁability without
using cryptography.
4
Evaluating Secure Voting Systems
The question of the title is of course rather tongue-in-cheek, but it does point to
a serious issue: how does one evaluate and compare often vastly diﬀerent voting
systems and technologies? One possible rough measure is to estimate the number
of people who would have to collude in order to corrupt an election. In practice
coming up with such estimates is extremely diﬃcult: we need good threat models,
people fall into diﬀerent categories (voters, election oﬃcials, supplier technicians,
cryptographic experts, etc.). Voting systems are examples par excellence of socio-
technical systems: it is not enough to examine the technical core, the code, the
cryptographic algorithms etc, we need also to take account of the behaviour of
the humans and their interactions with the technical mechanisms.
In this section we discuss the strengths and weakenesses of a representative
set of voting systems.
4.1
Pen-and-Paper
In the case of a conventional pen-and-paper system with hand counting, like the
one used in the UK, it can be argued that it would take quite a large number of
colluding agents to alter the outcome, especially given the First-Past-The-Post
system, though the number drops as the election margin drops. In the UK anyone
can request to be an observer of the election process. This lends a signiﬁcant level
of transparency to the process, but clearly no observer will be able to observe

How Many Election Oﬃcials Does It Take to Change an Election?
215
more that a tiny fraction of execution of a general election. Many ways have been
found to corrupt votes in a virtually undetectable fashion. Ballot boxes can be
manipulated in transit or storage. Jones, [9], observes that virtually all ballot
box locks and seals are quite easy to manipulate and fake. A piece of pencil lead
secreted under a thumb nail can be used to surreptitiously spoil ballot forms
during counting, and so on.
4.2
Touch Screen Machines
For the touch screen devices used in recent US presidential elections, it appears
that just one corrupt technician or oﬃcial could quite easily swing an election.
Indeed it has been argued that with some systems, a voter with a little knowhow
could corrupt an arbitrary number of votes. Furthermore, if done with a little
subtlety, the rigging will be virtually undetectable.
Until recently, such vulnerabilities were essentially theoretical, with no docu-
mented exploitation. Recently however we have seen the ﬁrst documented case
of ballot corruption with electronic voting machines. In Clay County Kentucky
a number of oﬃcials have been indited for large scale corruption of votes, [20].
The attack was socio-technical in nature, exploiting weaknesses in the design
of the interface and altering guidance to the voters to fool them into failing to
ﬁnalise their vote, allowing the oﬃcials to go into the booth after and “correct”
and ﬁnalise the vote for them.
4.3
Voter-Veriﬁable Paper Audit Trails
A common response to the concerns about the lack of auditability of touch screen
machines is to propose the incorporation of a Voter-Veriﬁable Paper Audit Trail
(VVPAT) mechanism. This is essentially a printer attached to the side of the
machine that prints the ballot in a way that the voter can examine and approve
if correct, or challenge if incorrect. Typically the printed ballot is shown under
glass to the voter to prevent tampering, the so called Mercuri method, [12].
This indeed has the beneﬁt of providing an independent audit trail that can be
invoked if the electronic count is called into question. Such mechanisms have
been incorporated in a number of DRE style products. In fact it has a number
of serious shortcoming:
– fallability of a paper audit
– failure of voters to properly verify the ballot
– challenging the printed ballot may violate the voter’s privacy
– lack of clarity regarding when the paper audit trail should be invoked
Nonetheless, VVPAT schemes are regarded as falling within the deﬁnition of
software independent.
4.4
Cryptographic Schemes
The ﬁnal category I want to discuss is that of cryptographically based schemes.
These strive to provide a notion of Voter-veriﬁability though the term is

216
P.Y.A. Ryan
potentially misleading as it is quite diﬀerent from the sense used in VVPAT
systems. The idea here is to provide voters with the means to conﬁrm to their
own satisfaction that their vote is accurately included in the count while not
providing a way to prove to a third party how they voted. A ﬁrst glance this
may seem impossible and indeed some pretty nifty cryptographic magic is re-
quired to pull it oﬀ. The key idea is to provide the voter with a receipt that
carries their vote in encrypted or encoded form. This is done in such a way
that nobody can extract the vote but a threshold set of Trustees can extract
the votes and perform the count in a way that maintains ballot secrecy. I will
outline a number of such schemes below, for the moment I make a number of
observations:
Such schemes appear to give a high degree of trustworthiness as a consequence
of the high degree of auditability of the whole process. Assurance arguments for
voting systems can usefully be broken down into three elements:
– cast as intended
– recorded as cast
– counted as recorded
For cryptographic schemes what this means is that:
Votes must be faithfully encoded in the receipts. Arguably this is the most
delicate part of the process. Voters cannot be expected to check the correct
encryption of their vote in there receipt, nor would we wish to give them this
capability as this would enable them to prove their vote to a coercer or vote
buyer. Consequently we must use indirect means to convince the voters of the
correctness of the encryption. I will discuss some ways to achieve this in the sec-
tions on particular schemes, but note that this is arguably the most challenging
aspect of the whole endeavor. We need mechanisms that are technically sound
but also suﬃciently simple for voters to understand and trust.
We need to ensure that all legitimately cast receipts, and only these, are
included in the tabulation phase. This is where the voter veriﬁcation comes into
play: all receipts should be posted to some form of public Bulletin Board (BB)
and voters can check that their receipt appears accurately. If it does not they can
protest and they hold a receipt to substantiate their complaint. Leaving aside
for the moment issues of voter diligence in checking their receipts and ballot
stuﬃng of the BB, this should serve to ensure that all legitimately cast votes are
entered into the tabulation.
Finally, we need to ensure that all posted receipts are correctly translated
back into votes. This phase is almost totally technical in nature and rather
well understood. A number of established techniques are known for veriﬁably
tabulating encrypted ballots in an anonymous way. In a sense, by introducing
the encrypted receipts we have transformed the problem into one that is purely
mathematical and hence fully veriﬁable. The problems of course remain at the
edges: in ensuring that the transformation into the purely mathematical domain
is faithful, and furthermore, universally seen as faithful.

How Many Election Oﬃcials Does It Take to Change an Election?
217
5
Voter-Veriﬁable Schemes
In this section I outline a number of recent proposals for practical, veriﬁable
voting.
5.1
Prˆet `a Voter
The Prˆet `a Voter approach to veriﬁable voting, [5,18,19], is distinguished by the
particularly simple way that the voter makes their choice and this is translated
into an encrypted receipt. The key innovation of the Prˆet `a Voter approach is the
way votes are encoded using a randomised candidate list. An important observa-
tion about this way of encoding the vote is that, in contrast to previous schemes,
there is no need for the voter to communicate their vote to an encryption device.
What is encrypted is the information that deﬁnes the frame of reference for any
given ballot form, and this can be computed in advance, before the ballot has
been associated with any voter let alone a vote choice.
At the polling station, the voter pre-registers and chooses at random a ballot
form from a pile of forms individually sealed in envelopes. An example form is
shown in Figures 1.
In the booth, the voter removes the ballot from its envelope and makes her
selection in the usual way by placing a cross in the right hand column against the
candidate of choice, or, in the case of a Single Transferable Vote (STV) system
for example, she marks her ranking against the candidates. Once the selection
has been made, she detaches and discards the left hand strip that carries the
candidate order. The remaining right hand strip now constitutes the receipt, as
shown in Figure 2.
Candidates Vote
Obelix
Ideﬁx
Asterix
Panoramix
7rJ94K
Destroy
Retain
Fig. 1. Prˆet `a Voter ballot form
Your Vote
X
7rJ94K
Retain
Fig. 2. Prˆet `a Voter ballot receipt (encoding a vote for “Ideﬁx”)

218
P.Y.A. Ryan
Anne now exits the booth with this receipt, registers with an oﬃcial and
casts her receipt in the presence of the oﬃcial. The ballot receipt is placed
against an optical reader or similar device that records the cryptographic value
at the bottom of the strip, that we will refer to henceforth as the ballot onion,
and an index value i indicating the cell into which the X was marked. A digital
signature is computed over the onion and index values and this signature is
printed on Anne’s receipt.
The digitized copies of the receipts are transmitted to a central tabulation
server which posts them to a secure WBB. Voters are encouraged to visit this
WBB and conﬁrm that their receipt appears correctly and, if their receipt does
not appear, or appears incorrectly (i.e., with the X in the wrong position), they
can appeal.
After a suitable period, assuming that checks on the posted receipts are sat-
isfactory or at least suitably resolved, we can start the tabulation process. For
this we employ standard, veriﬁable, anonymising tabulation techniques. I omit
the details here, they can be found in, for example, [19].
5.2
Three-Ballot
ThreeBallot is an interesting new scheme has proposed by Rivest, [14], that
achieves voter-veriﬁability and unconditional privacy without using any cryp-
tography. In essence, voters cast three ballots in such a way as to allocate two
votes to their candidate of choice and exactly one vote to all other candidates.
All ballots carry unique, pure random serial numbers.
All three ballots are cast, recorded and posted to a secure WBB. At the time
of casting, the voter retains a copy of one out of their three ballots, chosen
arbitrarily. The copy should be created in such a way to ensure that the system
does not learn which has been retained. This chosen form they can check against
the WBB. As a result, an attempt to corrupt a ballot stands a 1/3 chance of
being detected.
Obelix
Obelix
X Obelix
Ideﬁx
Ideﬁx
Ideﬁx
X
Asterix
X Asterix
Asterix
X
Panoramix
Panoramix X Panoramix
762067
4567023
3633209
Fig. 3. ThreeBallot ballot form (showing a vote for Asterix)
Obelix
X
Ideﬁx
Asterix
Panoramix X
4567023
Fig. 4. ThreeBallot ballot receipt

How Many Election Oﬃcials Does It Take to Change an Election?
219
The tabulation is easy to perform, involves no cryptography and universally
veriﬁable. Suppose that there are n voters. Votes for each candidate are totalled
over all the 3n posted ballots. n is subtracted from the total for each candidate
to give the votes cast for each candidate. Note that a receipt does not reveal
how the vote was cast.
This scheme is remarkable in that it achieves a degree of voter-veriﬁability
without the use of cryptography. Unfortunately, it seems that the scheme, at
least as it currently conceived, is probably not suitable for real elections. The
voting rules are not trivial and clearly they need to be enforced: if a voter is
allowed to cast three votes against a candidate or indeed none against another,
fairness will be violated. It is not clear how to enforce the voting rules in such a
way as to maintain vote secrecy. It is also not clear how to provide the voter with
a copy of just one ballot whilst ensuring that the system cannot learn which the
voter has chosen.
The scheme is also vulnerable to subtle forms of coercion, for a example a
coercer might require a voter to cast their vote according to a certain pattern.
If ballots corresponding to this pattern do not appear on the WBB then pun-
ishment follows. This can be countered by not allowing the voter the freedom
to choose an initial distribution of “null” crosses by having a device allocate a
random distribution to which the voter then adds their own, casting X. This too
has problems, for example, how to ensure that the device really does allocate
the initial Xs randomly and does not record any information.
Thus, the scheme is of immense theoretical importance but probably not prac-
tical as it stands.
5.3
Scantegrity II
Scantegrity II is a recent scheme proposed by Chaum, [6], that provides veriﬁ-
ability while being compatible with existing US optical scanner machines. The
concept is interesting in that here the voter is not provided with a receipt, as
with previous voter-veriﬁable schemes. Rather the voter gets to learn a random
code associated with her choice which she can note down and subsequently use
to check that her vote has been correctly recorded. An ingenious use of invisible
ink ensures that the voter only learns the code associated on her ballot with
her choice and no other codes on the ballot. Scratch strips could also be used
to ensure that only the code for the chosen candidate is revealed. The point of
this is that if she ﬁnds that a diﬀerent code is recorded for her ballot and she
protests, there is a mechanism to reveal the alternative codes for her ballot. If it
turns out the code she claims to have noted is indeed a valid code for this ballot
then it is likely that the system has tried to corrupt her vote. Alternatively, if
the code she claims is not a valid code for her ballot then it is likely that she is
merely trying to discredit the voting system by making up an alternative code.
Thus, the fact that she is denied knowledge of the alternate codes means that
if she claims a diﬀerent valid code than that recorded then her claim should be
regarded as evidence of corruption.

220
P.Y.A. Ryan
The ballot forms are essentially the standard forms used in optical scan sys-
tems with the diﬀerence that random codes are written in invisible ink in the
bubbles against the candidates. When a voter ﬁlls in the bubble of her choice,
using a specially provided pen, the code is revealed. All other codes should re-
main hidden as long as only one bubble is ﬁlled. If more than one bubble (for
a given race) is ﬁlled this is detected by the optscan software and the ballot
rejected.
Information deﬁning the codes is stored in a secret data base which is used
to perform the tabulation against recorded codes in a veriﬁable fashion, details
in [6]. The scheme has a number of advantages: it is compatible with existing
technology in the US and so for voters already familiar with opscan machines
the voting experience is virtually unchanged, aside from the optional step of
verifying their codes. The outcome can be computed from the codes but the
scheme also produces as a side eﬀect a VVPAT that can be invoked if necessary.
6
Conclusions
There has been much discussion about the merits or demerits of introducing
digital technology into the process of voting. Many argue that moving to digital
technology is simply too dangerous and it is better to stick with tried and tested
pen-and-paper along with hand counting. Whilst the critiques of many existing
voting products and technologies are well-founded, the extrapolation of these
observations to conclude that all technologies must inevitably be ﬂawed is un-
founded. Indeed many of these polemics appear unaware of, or prefer to ignore,
the existence of high-assurance, veriﬁable schemes. There is also a tendency to
conﬂate the problems of information technologies and those of remote forms of
voting, in particular internet voting. In this note I have sought to clarify these
issues and put the case for software-independent, veriﬁable voting systems.
References
1. http://punchscan.org/index.php
2. Beneloh, J., Tuinstra, D.: Receipt-free secret-ballot elections. In: Symposium on
Theory of Computing, pp. 544–553. ACM, New York (1994)
3. Chaum, D.: Untraceable mail, return addresses and digital pseudonyms. Commu-
nications of the ACM 24(2), 84–88 (1981)
4. Chaum, D.: Secret-ballot receipts: True voter-veriﬁable elections. IEEE Security
and Privacy 2(1), 38–47 (2004)
5. Chaum, D., Ryan, P.Y.A., Schneider, S.: A practical, voter-veriﬁable election
scheme. In: di Vimercati, S.d.C., Syverson, P.F., Gollmann, D. (eds.) ESORICS
2005. LNCS, vol. 3679, pp. 118–139. Springer, Heidelberg (2005)
6. Chaum, D., et al.: Scantegrity II: End-to-End Veriﬁability for Optical Scan Election
Systems using Invisible Ink Conﬁrmation Codes. Technical report. In: Proceedings
of USENIX-ACCURATE EVT (2008)
7. Gumbel, A.: Steal this vote! Nation Books (2005)

How Many Election Oﬃcials Does It Take to Change an Election?
221
8. Jones, D.W.: A brief illustrated history of voting (2003),
http://www.cs.uiowa.edo/~jones/voting/pictures
9. Jones, D.W.: Threats to voting systems (2005),
http://vote.nist.gov/threats/papers/threatstovotingsystems.pdf
10. Juels, A., Catalano, D., Jakobsson, M.: Coercion-resistant Electronic Elections.
In: Proceedings of the 2005 ACM workshop on Privacy in the electronic society
(November 2005)
11. Kohno, T., Stubbleﬁeld, A., Rubin, A.D., Wallach, D.S.: Analysis of an electronic
voting system. In: Symposium on Security and Privacy. IEEE, Los Alamitos (2004)
12. Mercuri, R.: A better ballot box? IEEE Spectrum Online (October 2002)
13. Neﬀ, A.: A veriﬁable secret shuﬄe and its application to e-voting. In: Conference
on Computer and Communications Security, pp. 116–125. ACM, New York (2001)
14. Rivest, R.L.: The three ballot voting system (2006),
http://theory.lcs.mit.edu/~rivest/Rivest-TheThreeBallotVotingSystem.pdf
15. Rivest, R.L., Wack, J.P.: On the notion of “software independence” in voting sys-
tems. Transactions of the Royal Society (2008)
16. Ryan, P.Y.A.: Towards a dependability case for the chaum voting scheme. In:
DIMACS Workshop on Electronic Voting – Theory and Practice (2004)
17. Ryan, P.Y.A.: A variant of the chaum voting scheme. Technical Report CS-TR-864,
University of Newcastle upon Tyne (2004)
18. Ryan, P.Y.A.: A variant of the chaum voting scheme. In: Proceedings of the Work-
shop on Issues in the Theory of Security, pp. 81–88. ACM, New York (2005)
19. Ryan, P.Y.A., Schneider, S.: Prˆet `a Voter with Re-encryption Mixes. In: Gollmann,
D., Meier, J., Sabelfeld, A. (eds.) ESORICS 2006. LNCS, vol. 4189, pp. 313–326.
Springer, Heidelberg (2006)
20. Schneier, B.: Election fraud in kentucky (2009)

Author Index
Adetoye, Adedayo O.
1
Aldini, Alessandro
18
Arsac, Wihem
34
Aspinall, David
173
Badii, Atta
1
Bartoletti, Massimo
52
Bella, Giampaolo
34
Bernardo, Marco
18
Bodei, Chiara
70
Broberg, Niklas
88
Brodo, Linda
70
Bruni, Roberto
70
Chantry, Xavier
34
Compagna, Luca
34
Fr¨oschle, Sibylle
92
Gonzalez-Tablas, Ana I.
124
Guttman, Joshua D.
107
Hernandez-Ardieta, Jorge L.
124
Hutter, Dieter
138
J¨urjens, Jan
155
Keighren, Gavin
173
Mazar´e, Laurent
193
Monroy, Ra´ul
138
Ramos, Benjamin
124
Ryan, P.Y.A.
211
Sands, David
88
Steel, Graham
92, 173
Warinschi, Bogdan
193
Weber, Tjark
155



COMMON ERRORS IN STATISTICS
(AND HOW TO AVOID THEM)

COMMON ERRORS IN STATISTICS
(AND HOW TO AVOID THEM)
Second Edition
Phillip I. Good
Huntington Beach, CA
James W. Hardin
Columbia, SC
A JOHN WILEY & SONS, INC., PUBLICATION

Copyeright © 2006 by John Wiley & Sons, Inc.  All rights reserved.
Published by John Wiley & Sons, Inc., Hoboken, New Jersey.
Published simultaneously in Canada.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted in
any form or by any means, electronic, mechanical, photocopying, recording, scanning, or
otherwise, except as permitted under Section 107 or 108 of the 1976 United States
Copyright Act, without either the prior written permission of the Publisher, or authorization
through payment of the appropriate per-copy fee to the Copyright Clearance Center, Inc.,
222 Rosewood Drive, Danvers, MA 01923, (978) 750-8400, fax (978) 750-4470, or on the
web at www.copyright.com. Requests to the Publisher for permission should be addressed to
the Permissions Department, John Wiley & Sons, Inc., 111 River Street, Hoboken, NJ
07030, (201) 748-6011, fax (201) 748-6008, or online at
http://www.wiley.com/go/permission.
Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their
best efforts in preparing this book, they make no representations or warranties with respect
to the accuracy or completeness of the contents of this book and speciﬁcally disclaim any
implied warranties of merchantability or ﬁtness for a particular purpose.  No warranty may be
created or extended by sales representatives or written sales materials.  The advice and
strategies contained herein may not be suitable for your situation.  You should consult with a
professional where appropriate.  Neither the publisher nor author shall be liable for any loss
of proﬁt or any other commercial damages, including but not limited to special, incidental,
consequential, or other damages.
For general information on our other products and services or for technical support, please
contact our Customer Care Department within the United States at (800) 762-2974, outside
the United States at (317) 572-3993 or fax (317) 572-4002.
Wiley also publishes its books in a variety of electronic formats. Some content that appears in
print may not be available in electronic formats. For more information about Wiley products,
visit our web site at www.wiley.com.
Library of Congress Cataloging-in-Publication Data:
ISBN-13: 978-0-471-79431-8
ISBN-10: 0-471-79431-7
Printed in the United States of America.
10 9 8 7 6 5 4 3 2 1

CONTENTS
v
Contents
Preface
ix
PART I
FOUNDATIONS
1
1. Sources of Error
3
Prescription
4
Fundamental Concepts
4
Ad Hoc, Post Hoc Hypotheses
7
2. Hypotheses: The Why of Your Research
13
Prescription
13
What Is a Hypothesis?
13
How precise must a hypothesis be?
14
Found Data
16
Null hypothesis
16
Neyman–Pearson Theory
17
Deduction and Induction
21
Losses
22
Decisions
24
To Learn More
25
3. Collecting Data
27
Preparation
27
Measuring Devices
28
Determining Sample Size
31
Fundamental Assumptions
36
Experimental Design
38
Four Guidelines
39

Are Experiments Really Necessary?
42
To Learn More
42
PART II
HYPOTHESIS TESTING AND ESTIMATION
45
4. Estimation
47
Prevention
47
Desirable and Not-So-Desirable Estimators
47
Interval Estimates
51
Improved Results
55
Summary
56
To Learn More
56
5. Testing Hypotheses: Choosing a Test Statistic
57
Comparing Means of Two Populations
59
Comparing Variances
67
Comparing the Means of K Samples
71
Higher-Order Experimental Designs
73
Contingency Tables
79
Inferior Tests
80
Multiple Tests
81
Before You Draw Conclusions
81
Summary
84
To Learn More
84
6. Strengths and Limitations of Some Miscellaneous Statistical 
Procedures
87
Bootstrap
88
Bayesian Methodology
89
Meta-Analysis
96
Permutation Tests
99
To Learn More
99
7. Reporting Your Results
101
Fundamentals
101
Tables
104
Standard Error
105
p-Values
110
Conﬁdence Intervals
111
Recognizing and Reporting Biases
113
Reporting Power
115
Drawing Conclusions
115
vi
CONTENTS

Summary
116
To Learn More
116
8. Interpreting Reports
119
With A Grain of Salt
119
Rates and Percentages
122
Interpreting Computer Printouts
123
9. Graphics
125
The Soccer Data
125
Five Rules for Avoiding Bad Graphics
126
One Rule for Correct Usage of Three-Dimensional Graphics
133
The Misunderstood Pie Chart
135
Two Rules for Effective Display of Subgroup Information
136
Two Rules for Text Elements in Graphics
140
Multidimensional Displays
141
Choosing Graphical Displays
143
Summary
143
To Learn More
144
PART III
BUILDING A MODEL
145
10. Univariate Regression
147
Model Selection
147
Estimating Coefﬁcients
155
Further Considerations
157
Summary
160
To Learn More
162
11. Alternate Methods of Regression
163
Linear vs. Nonlinear Regression
164
Least Absolute Deviation Regression
164
Errors-in-Variables Regression
165
Quantile Regression
169
The Ecological Fallacy
170
Nonsense Regression
172
Summary
172
To Learn More
172
12. Multivariable Regression
175
Caveats
175
Factor Analysis
178
General Linearized Models
178
Reporting Your Results
181
CONTENTS
vii

A Conjecture
182
Decision Trees
183
Building a Successful Model
185
To Learn More
186
13. Validation
187
Methods of Validation
188
Measures of Predictive Success
191
Long-Term Stability
193
To Learn More
194
Appendix A
195
Appendix B
205
Glossary, Grouped by Related but Distinct Terms
219
Bibliography
223
Author Index
243
Subject Index
249
viii
CONTENTS

PREFACE
ix
Preface
ONE OF Dr. Good’s very ﬁrst statistical applications was an analysis of
leukemia cases in Hiroshima, Japan after World War II; on August 7, 1945
this city was the target site of the ﬁrst atomic bomb dropped by the
United States. Was the high incidence of leukemia cases among survivors
the result of exposure to radiation from the atomic bomb? Was there a
relationship between the number of leukemia cases and the number of sur-
vivors at certain distances from the atomic bomb’s epicenter?
To assist in the analysis, Dr. Good had an electric (not an electronic)
calculator, reams of paper on which to write down intermediate results,
and a prepublication copy of Scheffé’s Analysis of Variance. The work took
several months, and the results were somewhat inclusive, mainly because
he could never seem to get the same answer twice—a consequence of
errors in transcription rather than the absence of any actual relationship
between radiation and leukemia.
Today, of course, we have high-speed computers and prepackaged statis-
tical routines to perform necessary calculations. Yet access to statistical
software will no more make one a statistician, than access to a chainsaw
will make one a lumberjack. Allowing these tools to do our thinking for us
is a sure recipe for disaster—just ask any emergency room physician.
Pressed by management or by funding needs, too many research
workers have no choice but to go forward with data analysis regardless of
the extent of their statistical training. Alas, although a semester or two of
undergraduate statistics may sufﬁce to develop familiarity with the names
of some statistical methods, it is not enough to ensure awareness of all the
circumstances under which these methods may be applicable.
The purpose of the present text is to provide a mathematically rigorous
but readily understandable foundation for statistical procedures. Here for
the second time are such basic statistical concepts as null and alternative
hypotheses, p-value, signiﬁcance level, and power. Reprints from the statis-

tical literature provide illustration as we reexamine sample selection, linear
regression, the analysis of variance, maximum likelihood, Bayes’ theorem,
meta-analysis and the bootstrap.
For the second edition, we’ve added material from online courses we
offer at statistics.com. This new material is devoted to unbalanced designs,
report interpretation, and alternative modeling methods.
More good news for technophobes: Dr. Good’s articles on women’s
sports have appeared in the San Francisco Examiner, Sports Now, and Vol-
leyball Monthly, Twenty-two of his short stories are also in print. If you
can read the sports page, you’ll ﬁnd the presentation of material in this
text easy to read and to follow. Lest the statisticians among you believe
this book is too introductory, we point out the existence of hundreds of
citations in statistical literature calling for the comprehensive treatment we
have provided. Regardless of past training or current specialization, this
book will serve as a useful reference; you will ﬁnd applications for the
information contained herein whether you are a practicing statistician or a
well-trained scientist who just happens to apply statistics in the pursuit of
other science.
The primary objective of the opening chapter is to describe the main
sources of error and provide a preliminary prescription for avoiding them.
The hypothesis formulation—data gathering—hypothesis testing and esti-
mate cycle is introduced, and the rationale for gathering additional data
before attempting to test after-the-fact hypotheses is detailed.
Chapter 2 places our work in the context of decision theory. We empha-
size the importance of providing an interpretation of each and every
potential outcome in advance of consideration of actual data.
Chapter 3 focuses on study design and data collection, for failure at the
planning stage can render all further efforts valueless. The work of Berger
and his colleagues on selection bias is given particular emphasis.
Desirable features of point and interval estimates are detailed in Chapter
4 along with procedures for deriving estimates in a variety of practical situ-
ations. This chapter also serves to debunk several myths surrounding esti-
mation procedures.
Chapter 5 reexamines the assumptions underlying testing hypotheses.
We review the impacts of violations of assumptions and detail the proce-
dures to follow when making 2- and k-sample comparisons. In addition,
we cover the procedures for analyzing contingency tables and 2-way
experimental designs if standard assumptions are violated.
Chapter 6 is devoted to the value and limitations of Bayes’ theorem,
meta-analysis, and resampling methods.
Chapter 7 lists the essentials of any report that will utilize statistics,
debunks the myth of the “standard” error, and describes the value and
limitations of p-values and conﬁdence intervals for reporting results. Prac-
x
PREFACE

tical signiﬁcance is distinguished from statistical signiﬁcance and induction
is distinguished from deduction. Chapter 8 covers much the same mater-
ial, but the viewpoint is that of the report reader rather than the report
writer. Of particular importance is a section on interpreting computer
output.
Twelve rules for more effective graphic presentations are given in
Chapter 9 along with numerous examples of the right and wrong ways 
to maintain reader interest while communicating essential statistical 
information.
Chapters 10 through 13 are devoted to model building and to the
assumptions and limitations of a multitude of regression methods and data
mining techniques. A distinction is drawn between goodness of ﬁt and
prediction, and the importance of model validation is emphasized. Seminal
articles by David Freedman and Gail Gong are reprinted.
Finally, for the further convenience of readers, we provide a glossary
grouped by related but contrasting terms, an annotated bibliography, and
subject and author indexes.
Our thanks to William Anderson, Leonardo Auslender, Vance Berger,
Peter Bruce, Bernard Choi, Tony DuSoir, Cliff Lunneborg, Mona Hardin,
Gunter Hartel, Fortunato Pesarin, Henrik Schmiediche, Marjorie Stine-
spring, and Peter A. Wright for their critical reviews of portions of this
text. Doug Altman, Mark Hearnden, Elaine Hand, and David Parkhurst
gave us a running start with their bibliographies. Brian Cade, David
Rhodes, and, once again, Cliff Lunneborg helped us complete the second
edition.
We hope you soon put this text to practical use.
Sincerely yours,
Phillip Good
brother_unknown@yahoo.com
Huntington Beach CA.
James Hardin
jhardin@gwm.sc.edu
Columbia, SC.
July 2003/2005
PREFACE
xi

Part I
FOUNDATIONS
Don’t think—use the computer.
Dyke (tongue in cheek) (1997)

Chapter 1
Sources of Error
CHAPTER 1
SOURCES OF ERROR
3
Common Errors in Statistics (and How to Avoid Them), 2e, by Phillip I. Good and James W. Hardin.
Copyright © 2006 John Wiley & Sons, Inc.
STATISTICAL PROCEDURES FOR HYPOTHESIS TESTING, ESTIMATION, AND MODEL
building are only a part of the decision-making process. They should
never be quoted as the sole basis for making a decision (yes, even those
procedures that are based on a solid deductive mathematical foundation).
As philosophers have known for centuries, extrapolation from a sample or
samples to a larger incompletely examined population must entail a leap 
of faith.
The sources of error in applying statistical procedures are legion and
include all of the following:
•
Using the same set of data both to formulate hypotheses and to
test them
•
Taking samples from the wrong population or failing to specify
the population(s) about which inferences are to be made in
advance
•
Failing to draw random, representative samples
•
Measuring the wrong variables or failing to measure what you’d
hoped to measure
•
Using inappropriate or inefﬁcient statistical methods
•
Failing to validate models
But perhaps the most serious source of error lies in letting statistical
procedures make decisions for you.
In this chapter, as throughout this text, we offer ﬁrst a preventive pre-
scription, followed by a list of common errors. If these prescriptions are
followed carefully, you will be guided to the correct, proper, and effective
use of statistics and avoid the pitfalls.

PRESCRIPTION
Statistical methods used for experimental design and analysis should be
viewed in their rightful role as merely a part, albeit an essential part, of the
decision-making procedure.
Here is a partial prescription for the error-free application of statistics.
1. Set forth your objectives and the use you plan to make of your
research before you conduct a laboratory experiment, a clinical
trial, or survey or analyze an existing set of data.
2. Deﬁne the population to which you will apply the results of your
analysis.
3. List all possible sources of variation. Control them or measure
them to avoid their being confounded with relationships among
those items that are of primary interest.
4. Formulate your hypothesis and all of the associated alternatives.
(See Chapter 2.) List possible experimental ﬁndings along with the
conclusions you would draw and the actions you would take if
this or another result should prove to be the case. Do all of these
things before you complete a single data collection form, and before
you turn on your computer.
5. Describe in detail how you intend to draw a representative sample
from the population. (See Chapter 3.)
6. Use estimators that are impartial, consistent, efﬁcient, robust, and
minimum loss. (See Chapter 4.) To improve results, focus on sufﬁ-
cient statistics pivotal statistics, and admissible statistics, and use
interval estimates. (See Chapters 4 and 5.)
7. Know the assumptions that underlie the tests you use. Use 
those tests that require the minimum of assumptions and are 
most powerful against the alternatives of interest. (See 
Chapter 5.)
8. Incorporate in your reports the complete details of how the
sample was drawn and describe the population from which it was
drawn. If data are missing or the sampling plan was not followed,
explain why and list all differences between data that were present
in the sample and data that were missing or excluded. (See
Chapter 7.)
FUNDAMENTAL CONCEPTS
Three concepts are fundamental to the design of experiments and surveys:
variation, population, and sample.
A thorough understanding of these concepts will forestall many errors in
the collection and interpretation of data.
If there were no variation, if every observation were predictable, a mere
repetition of what had gone before, there would be no need for 
statistics.
4
PART I
FOUNDATIONS

Variation
Variation is inherent in virtually all our observations. We would not expect
outcomes of two consecutive spins of a roulette wheel to be identical. One
result might be red, the other black. The outcome varies from spin to
spin.
There are gamblers who watch and record the spins of a single roulette
wheel hour after hour, hoping to discern a pattern. A roulette wheel is,
after all, a mechanical device, and perhaps a pattern will emerge. But even
those observers do not anticipate ﬁnding a pattern that is 100% determin-
istic. The outcomes are just too variable.
Anyone who spends time in a schoolroom, as a parent or as a child, can
see the vast differences among individuals. This one is tall, today, that one
short. Half an aspirin and Dr. Good’s headache is gone, but his wife
requires four times that dosage for relief.
There is variability even among observations on deterministic formula-
satisfying phenomena such as the position of a planet in space or the
volume of gas at a given temperature and pressure. Position and volume
satisfy Kepler’s laws and Boyle’s law, respectively, but the observations we
collect will depend on the measuring instrument (which may be affected
by the surrounding environment) and the observer. Cut a length of 
string and measure it three times. Do you record the same length each
time?
In designing an experiment or survey we must always consider the 
possibility of errors arising from the measuring instrument and from the
observer. It is one of the wonders of science that Kepler was able to for-
mulate his laws given the relatively crude instruments at his disposal.
Population
The population(s) of interest must be clearly deﬁned before we begin to
gather data.
From time to time, someone will ask us how to generate conﬁdence inter-
vals (see Chapter 7) for the statistics arising from a total census of a popu-
lation. Our answer is no, we cannot help. Population statistics (mean,
median, 30th percentile) are not estimates. They are ﬁxed values and will
be known with 100% accuracy if two criteria are fulﬁlled:
1. Every member of the population is observed.
2. All the observations are recorded correctly.
Conﬁdence intervals would be appropriate if the ﬁrst criterion is vio-
lated, for then we are looking at a sample, not a population. And if the
second criterion is violated, then we might want to talk about the conﬁ-
dence we have in our measurements.
CHAPTER 1
SOURCES OF ERROR
5

Debates about the accuracy of the 2000 United States Census arose
from doubts about the fulﬁllment of these criteria.1 “You didn’t count the
homeless,” was one challenge. “You didn’t verify the answers,” was
another. Whether we collect data from a sample or an entire population,
equivalents of both of the previously mentioned challenges can and should
be made.
Kepler’s “laws” of planetary movement are not testable by statistical
means when applied to the original planets (Jupiter, Mars, Mercury, and
Venus) for which they were formulated. But when we make statements
such as “Planets that revolve around Alpha Centauri will also follow
Kepler’s laws,” then we begin to view our original population, the planets
of our sun, as a sample of all possible planets in all possible solar systems.
A major problem with many studies is that the population of interest is
not adequately deﬁned before the sample is drawn. Don’t make this
mistake. A second major source of error is that the sample proves to have
been drawn from a different population than was originally envisioned. We
consider this problem in the next section and again in Chapters 2, 5, 
and 6.
Sample
A sample is any (proper) subset of a population.
Small samples may give a distorted view of the population. For example,
if a minority group comprises 10% or less of a population, a jury of 12
persons selected at random from that population fails to contain any
members of that minority at least 28% of the time.
As a sample grows larger, or as we combine more clusters within a
single sample, the sample will grow to more closely resemble the popula-
tion from which it is drawn.
How large a sample must be to obtain a sufﬁcient degree of closeness
will depend on the manner in which the sample is chosen from the popu-
lation. Are the elements of the sample drawn at random, so that each unit
in the population has an equal probability of being selected? Are the 
elements of the sample drawn independently of one another?
If either of these criteria is not satisﬁed, then even a very large sample
may bear little or no relation to the population from which it was drawn.
An obvious example is the use of recruits from a Marine boot camp as
representatives of the population as a whole or even as representatives of
all Marines. In fact, any group or cluster of individuals who live, work,
6
PART I
FOUNDATIONS
1 City of New York v. Department of Commerce, 822 F. Supp. 906 (E.D.N.Y, 1993). The
arguments of four statistical experts who testiﬁed in the case may be found in Volume 34 of
Jurimetrics, 1993, 64–115.

study, or pray together may fail to be representative for any or all of the
following reasons (Cummings and Koepsell, 2002):
1. Shared exposure to the same physical or social environment
2. Self-selection in belonging to the group
3. Sharing of behaviors, ideas, or diseases among members of the
group
A sample consisting of the ﬁrst few animals to be removed from a cage
will not satisfy these criteria either, because, depending on how we grab,
we are more likely to select more active or more passive animals. Activity
tends to be associated with higher levels of corticosteroids, and corticos-
teroids are associated with virtually every body function.
Sample bias is a danger in every research ﬁeld. For example, Bothun
(1998) documents the many factors that can bias sample selection in
astronomical research.
To forestall sample bias in your studies, determine before you begin the
factors that can affect the study outcome (gender and lifestyle, for
example). Subdivide the population into strata (males, females, city
dwellers, farmers) and then draw separate samples from each stratum.
Ideally, you would assign a random number to each member of the
stratum and let a computer’s random number generator determine which
members are to be included in the sample.
Surveys and Long-Term Studies
Being selected at random does not mean that an individual will be willing
to participate in a public opinion poll or some other survey. But if survey
results are to be representative of the population at large, then pollsters
must ﬁnd some way to interview nonresponders as well. This difﬁculty is
only exacerbated in long-term studies, as subjects fail to return for follow-
up appointments and move without leaving a forwarding address. Again, if
the sample results are to be representative, some way must be found to
report on subsamples of the nonresponders (those who never participate)
and the dropouts (those who stop participating at some point).
AD HOC, POST HOC HYPOTHESES
Formulate and write down your hypotheses before you examine the data.
Patterns in data can suggest, but cannot conﬁrm, hypotheses unless these
hypotheses were formulated before the data were collected.
Everywhere we look, there are patterns. In fact, the harder we look 
the more patterns we see. Three rock stars die in a given year. Fold the
United States twenty-dollar bill in just the right way and not only the
CHAPTER 1
SOURCES OF ERROR
7

Pentagon but the Twin Towers in ﬂames are revealed. It is natural for us
to want to attribute some underlying cause to these patterns. But those
who have studied the laws of probability tell us that more often than not
patterns are simply the result of random events.
Put another way, ﬁnding at least one cluster of events in time or in
space has a greater probability than ﬁnding no clusters at all (equally
spaced events).
How can we determine whether an observed association represents an
underlying cause-and-effect relationship or is merely the result of chance?
The answer lies in our research protocol. When we set out to test a spe-
ciﬁc hypothesis, the probability of a speciﬁc event is predetermined. But
when we uncover an apparent association, one that may well have arisen
purely by chance, we cannot be sure of the association’s validity until we
conduct a second set of controlled trials. A very interesting exploration of
patterns and cause-effect is illustrated in the study described online at
http://www.cis.gsu.edu/~dstraub/Courses/Grandma.htm, where the
author investigates whether it really is true that a student’s grandparent is
more likely to die just before the student takes an exam than at any other
time during the year.
In the International Study of Infarct Survival (1988), patients born
under the Gemini or Libra astrological birth sign did not survive as long
when their treatment included aspirin. By contrast, aspirin offered appar-
ent beneﬁcial effects (longer survival time) to study participants from all
other astrological birth signs.
Except for those who guide their lives by the stars, there is no hidden
meaning or conspiracy in this result. When we describe a test as signiﬁcant
at the 5% or 1-in-20 level, we mean that 1 in 20 times we’ll get a signiﬁ-
cant result even though the hypothesis is true. That is, when we test to
see whether there are any differences in the baseline values of the control
and treatment groups, if we’ve made 20 different measurements, we can
expect to see at least one statistically signiﬁcant difference; in fact, we will
see this result almost two-thirds of the time. This difference will not repre-
sent a ﬂaw in our design but simply chance at work. To avoid this undesir-
able result—that is, to avoid attributing statistical signiﬁcance to an
insigniﬁcant random event, a so-called Type I error, we must distinguish
between the hypotheses with which we began the study and those that
came to mind afterward. We must accept or reject these hypotheses at 
the original signiﬁcance level while demanding additional corroborating
evidence for those exceptional results (such as a dependence of an
outcome on astrological sign) that are uncovered for the ﬁrst time during
the trials.
No reputable scientist would ever report results before successfully
reproducing the experimental ﬁndings twice, once in the original 
8
PART I
FOUNDATIONS

laboratory and once in that of a colleague.2 The latter experiment can be
particularly telling, as all too often some overlooked factor not controlled
in the experiment—such as the quality of the laboratory water—proves
responsible for the results observed initially. Better to be found wrong in
private than in public. The only remedy is to attempt to replicate the ﬁnd-
ings with different sets of subjects, replicate, then replicate again.
Persi Diaconis (1978) spent some years as a statistician investigating
paranormal phenomena. His scientiﬁc inquiries included investigating the
powers linked to Uri Geller, the man who claimed he could bend spoons
with his mind. Diaconis was not surprised to ﬁnd that the hidden
“powers” of Geller were more or less those of the average nightclub magi-
cian, down to and including forcing a card and taking advantage of ad
hoc, post hoc hypotheses.
When three buses show up at a stop simultaneously, or three rock stars
die in the same year, or a stand of cherry trees is found amid a forest of
oaks, a good statistician remembers the Poisson distribution. This distribu-
tion applies to relatively rare events that occur independently of one
another. The calculations performed by Siméon-Denis Poisson reveal that
if there is an average of one event per interval (in time or in space), then
although more than a third of the intervals will be empty, at least a
quarter of the intervals are likely to include multiple events. Poisson’s
original investigations examined the number of Prussian soldiers who died
each year as a result of being kicked by a horse and the number of suicides
among women and children. Perhaps it is the morbid nature of these
studies that led to the association of his name with the formula even
though Abraham de Moivre had previously discovered the distribution.
Anyone who has played poker will concede that one out of every two
hands contains “something” interesting. Don’t allow naturally occurring
results to fool you nor lead you to fool others by shouting, “Isn’t this
incredible?”
The purpose of a recent set of clinical trials was to see whether blood
ﬂow and distribution in the lower leg could be improved by carrying out a
simple surgical procedure before the administration of standard prescrip-
tion medicine.
The results were disappointing on the whole, but one of the marketing
representatives noted that the long-term prognosis was excellent when a
marked increase in blood ﬂow was observed just after surgery. She 
CHAPTER 1
SOURCES OF ERROR
9
2 Remember “cold fusion?” In 1989, two University of Utah professors told the newspapers
they could fuse deuterium molecules in the laboratory, solving the world’s energy problems
for years to come. Alas, neither those professors nor anyone else could replicate their ﬁnd-
ings, although true believers abound, http://www.ncas.org/erab/intro.htm.

suggested we calculate a p-value3 for a comparison of patients with an
improved blood ﬂow versus patients who had taken the prescription medi-
cine alone.
Such a p-value would be meaningless. Only one of the two samples of
patients in question had been taken at random from the population (those
10
PART I
FOUNDATIONS
3 A p-value is the probability under the primary hypothesis of observing the set of observa-
tions we have in hand. We can calculate a p-value once we make a series of assumptions
about how the data were gathered. These days, statistical software does the calculations, but
it’s still up to us to verify that the assumptions are correct.
109
65
22
3
1
0
0
20
40
60
80
100
120
0
1
2
3
4
5 or more
Number of deaths
Frequency
FIGURE 1.1
Frequency plot of the number of deaths in the Prussian army as a
result of being kicked by a horse (there are 200 total observations).
Hand
Probability
Straight ﬂush
0.0000
4-of-a-kind
0.0002
Full house
0.0014
Flush
0.0020
Straight
0.0039
3-of-a-kind
0.0211
Two pairs
0.0475
Pair
0.4226
Total
0.4988
TABLE 1.1 Probability of Finding Something Interesting
in a Five Card Hand

patients who received the prescription medicine alone). The other sample
(those patients who had increased blood ﬂow after surgery) was deter-
mined after the fact. To extrapolate results from the samples in hand to a
larger population, the samples must be taken at random from, and be rep-
resentative of, that population.
The preliminary ﬁndings clearly called for an examination of surgical
procedures and of patient characteristics that might help forecast successful
surgery. But the generation of a p-value and the drawing of any ﬁnal con-
clusions had to wait on clinical trials speciﬁcally designed for that purpose.
This doesn’t mean that one should not report anomalies and other
unexpected ﬁndings. Rather, one should not attempt to provide p-values
or conﬁdence intervals in support of them. Successful researchers engage
in a cycle of theorizing and experimentation so that the results of one
experiment become the basis for the hypotheses tested in the next.
A related, extremely common error whose resolution we discuss at
length in Chapters 12 and 13 is to use the same data to select variables for
inclusion in a model and to assess their signiﬁcance. Successful model
builders develop their frameworks in a series of stages, validating each
model against a second independent data set before drawing conclusions.
CHAPTER 1
SOURCES OF ERROR
11

Chapter 2
Hypotheses: The Why of 
Your Research
CHAPTER 2
HYPOTHESES: THE WHY OF YOUR RESEARCH
13
Common Errors in Statistics (and How to Avoid Them), 2e, by Phillip I. Good and James W. Hardin.
Copyright © 2006 John Wiley & Sons, Inc.
IN THIS CHAPTER, WE REVIEW HOW TO FORMULATE a hypothesis that is
testable by statistical means, the appropriate use of the null hypothesis, the
Neyman–Pearson theory, the two types of error, and the more general
theory of decisions and losses.
PRESCRIPTION
Statistical methods used for experimental design and analysis should be
viewed in their rightful role as merely a part, albeit an essential part, of the
decision-making procedure.
1. Set forth your objectives and the use you plan to make of your
research before you conduct a laboratory experiment, a clinical
trial, or a survey or analyze an existing set of data.
2. Formulate your hypothesis and all of the associated alternatives.
List possible experimental ﬁndings along with the conclusions you
would draw and the actions you would take if this or another
result should prove to be the case. Do all of these things before
you complete a single data collection form and before you turn on
your computer.
WHAT IS A HYPOTHESIS?
A well-formulated hypothesis will be both quantiﬁable and testable, that
is, involve measurable quantities or refer to items that may be assigned to
mutually exclusive categories.
A well-formulated statistical hypothesis takes one of the forms “Some
measurable characteristic of a population takes one of a speciﬁc set of

values” or “Some measurable characteristic takes different values in differ-
ent populations, the difference(s) taking a speciﬁc pattern or a speciﬁc set
of values.”
Examples of well-formed statistical hypotheses include the following:
•
“For males over 40 suffering from chronic hypertension, a 100-
mg daily dose of this new drug lowers diastolic blood pressure an
average of 10mmHg.”
•
“For males over 40 suffering from chronic hypertension, a daily
dose of 100mg of this new drug lowers diastolic blood pressure
an average of 10mmHg more than an equivalent dose of 
metoprolol.”
•
“Given less than 2 hours per day of sunlight, applying from 1 to
10lbs of 23-2-4 fertilizer per 1000 square feet will have no effect
on the growth of fescues and Bermuda grasses.”
“All redheads are passionate” is not a well-formed statistical hypothesis,
not merely because “passionate” is ill deﬁned but because the word “All”
indicates that the phenomenon is not statistical in nature.
Similarly, logical assertions of the form “Not All,” “None,” or “Some”
are not statistical in nature. The restatement “80% of redheads are passion-
ate” would remove this latter objection.
The restatements “Doris J. is passionate,” or “Both Good brothers are
5′10″ tall” also are not statistical in nature as they concern speciﬁc individ-
uals rather than populations (Hagood, 1941).
If we quantify “passionate” to mean “has an orgasm more than 95% of
the time consensual sex is performed,” then the hypothesis “80% of red-
heads are passionate” becomes testable. Note that deﬁning “passionate” to
mean “has an orgasm every time consensual sex is performed” would not
be provable, as it is a statement of the “all or none” variety.
Finally, note that until someone succeeds in locating unicorns, the
hypothesis “80% of unicorns are passionate” is not testable.
Formulate your hypotheses so they are quantiﬁable, testable, and 
statistical in nature.
HOW PRECISE MUST A HYPOTHESIS BE?
The chief executive of a drug company may well express a desire to test
whether “our anti-hypertensive drug can beat the competition.” But to
apply statistical methods, a researcher will need precision on the order of
“For males over 40 suffering from chronic hypertension, a daily dose of
100mg of our new drug will lower diastolic blood pressure an average 
of 10mmHg more than an equivalent dose of metoprolol.”
The researcher may want to test a preliminary hypothesis on the order
of “For males over 40 suffering from chronic hypertension, there is a daily
14
PART I
FOUNDATIONS

dose of our new drug which will lower diastolic blood pressure an average
of 20mmHg.” But this hypothesis is imprecise. What if the necessary dose
of the new drug required taking a tablet every hour? Or caused liver mal-
function? Or even death? First, the researcher would conduct a set of clini-
cal trials to determine the maximum tolerable dose (MTD) and then test
the hypothesis, “For males over 40 suffering from chronic hypertension, a
daily dose of one-third to one-fourth the MTD of our new drug will
lower diastolic blood pressure an average of 20mmHg.”
CHAPTER 2
HYPOTHESES: THE WHY OF YOUR RESEARCH
15
A BILL OF RIGHTS
•
Scientists can and should be encouraged to make subgroup analyses.
•
Physicians and engineers should be encouraged to make decisions 
utilizing the ﬁndings of such analyses.
•
Statisticians and other data analysts can and should rightly refuse to
give their imprimatur to related tests of signiﬁcance.
In a series of articles by Horwitz et al. (1998), a physician and his col-
leagues strongly criticize the statistical community for denying them (or so
they perceive) the right to provide a statistical analysis for subgroups not
contemplated in the original study protocol. For example, suppose that in
a study of the health of Marine recruits, we notice that not one of the
dozen or so women who received the vaccine contracted pneumonia. Are
we free to provide a p-value for this result?
Statisticians Smith and Egger (1998) argue against hypothesis tests of
subgroups chosen after the fact, suggesting that the results are often likely
to be explained by the “play of chance.” Altman (1998; p. 301–303),
another statistician, concurs, noting that “. . . [T]he observed treatment
effect is expected to vary across subgroups of the data . . . simply through
chance variation” and that “Doctors seem able to ﬁnd a biologically plau-
sible explanation for any ﬁnding.” This leads Horwitz et al. to the incor-
rect conclusion that Altman proposes we “dispense with clinical biology
(biologic evidence and pathophysiologic reasoning) as a basis for forming
subgroups.” Neither Altman nor any other statistician would quarrel with
the assertion of Horwitz et al. that physicians must investigate “how do
we [physicians] do our best for a particular patient.” 
Scientists can and should be encouraged to make subgroup analyses.
Physicians and engineers should be encouraged to make decisions based
on them. Few would deny that in an emergency, satisﬁcing (coming up
with workable, fast-acting solutions without complete information) is
better than optimizing.1 But, by the same token, statisticians should not
1 Chiles (2001, p. 61).

be pressured to give their imprimatur to what, in statistical terms, is clearly
an improper procedure, nor should statisticians mislabel suboptimal proce-
dures as the best that can be done.2
We concur with Anscombe (1963), who writes . . . [T]he concept of
error probabilities of the ﬁrst and second kinds . . . has no direct relevance
to experimentation. . . . The formation of opinions, decisions concerning
further experimentation and other required actions, are not dictated . . . by
the formal analysis of the experiment, but call for judgment and imagina-
tion. . . . It is unwise for the experimenter to view himself seriously as a
decision-maker. . . . The experimenter pays the piper and calls the tune he
likes best; but the music is broadcast so that others might listen. . . .
FOUND DATA
p-Values should not be computed for hypotheses based on “found data”
as, of necessity, all hypotheses related to found data are after the fact. This
rule does not apply if the observer ﬁrst divides the data into sections
where one section is studied and conclusions drawn and then the resultant
hypotheses are tested on the remaining sections. Even then, the tests are
valid only if the found data can be shown to be representative of the 
population at large.
NULL HYPOTHESIS
A major research failing seems to be the exploration of uninteresting or even
trivial questions. . . . In the 347 sampled articles in Ecology containing null
hypotheses tests, we found few examples of null hypotheses that seemed
biologically plausible. Anderson, Burnham, and Thompson (2000)
Test only relevant null hypotheses.
The “null hypothesis” has taken on a mythic role in contemporary statis-
tics. Obsession with the “null” has been allowed to shape the direction of
our research. We’ve let the tool use us instead of our using the tool.3
Although a null hypothesis can facilitate statistical inquiry—an exact per-
mutation test is impossible without it—it is never mandated. In any event,
virtually any quantiﬁable hypothesis can be converted into null form.
There is no excuse and no need to be content with a meaningless null.
16
PART I
FOUNDATIONS
2 We are reminded of the dean, several of them in fact, who asked one of us to alter his
grades.
“But that is something you can do as easily as I.”
“Why Dr. Good, I would never dream of overruling one of my instructors.”
3 See, for example, Hertwig and Todd (2000).

To test that the mean value of a given characteristic is three, subtract
three from each observation and then test the “null hypothesis” that the
mean value is zero.
Often, we want to test that the size of some effect is inconsequential,
not zero but close to it, smaller than d, say, where d is the smallest biolog-
ical, medical, physical or socially relevant effect in your area of research.
Again, subtract d from each observation, before proceeding to test a null
hypothesis. In Chapter 5, we discuss an alternative approach using conﬁ-
dence intervals for tests of equivalence.
To test that “80% of redheads are passionate,” we have two choices
depending on how “passion” is measured. If “passion” is an all or none
phenomenon, then we can forget about trying to formulate a null hypoth-
esis and instead test the binomial hypothesis that the probability p that a
redhead is passionate is 80%. If “passion” can be measured on a seven-
point scale and we deﬁne “passionate” as “passion” greater than or equal
to 5, then our hypothesis becomes “The 20th percentile of redhead
passion exceeds 5.” As in the ﬁrst example above, we could convert this to
a “null hypothesis” by subtracting 5 from each observation. But the effort
is unnecessary.
NEYMAN–PEARSON THEORY
Formulate your alternative hypotheses at the same time you set forth your
principal hypothesis.
When the objective of our investigations is to arrive at some sort of con-
clusion, then we need to have not only a hypothesis in mind but one or
more potential alternative hypotheses.
The cornerstone of modern hypothesis testing is the Neyman–Pearson
lemma. To get a feeling for the working of this lemma, suppose we are
testing a new vaccine by administering it to half of our test subjects and
giving a supposedly harmless placebo to each of the remainder. We
proceed to follow these subjects over some ﬁxed period and to note which
subjects, if any, contract the disease that the new vaccine is said to offer
protection against.
We know in advance that the vaccine is unlikely to offer complete pro-
tection; indeed, some individuals may actually come down with the disease
as a result of taking the vaccine. Depending on the weather and other
factors over which we have no control, our subjects, even those who
received only placebo may not contract the disease during the study
period. All sorts of outcomes are possible.
The tests are being conducted in accordance with regulatory agency
guidelines. From the regulatory agency’s perspective, the principal hypoth-
CHAPTER 2
HYPOTHESES: THE WHY OF YOUR RESEARCH
17

esis H is that the new vaccine offers no protection. Our alternative
hypothesis A is that the new vaccine can cut the number of infected indi-
viduals in half. Our task before the start of the experiment is to decide
which outcomes will rule in favor of the alternative hypothesis A and
which in favor of the null hypothesis H.
The problem is that because of the variation inherent in the disease
process each and every one of the possible outcomes could occur regard-
less of which hypothesis is true. Of course, some outcomes are more likely
if H is true, for example, 50 cases of pneumonia in the placebo group and
48 in the vaccine group, and others are more likely if the alternative
hypothesis is true, for example, 38 cases of pneumonia in the placebo
group and 20 in the vaccine group.
Following Neyman and Pearson, we order each of the possible out-
comes in accordance with the ratio of its probability or likelihood when
the alternative hypothesis is true to its probability when the principal
hypothesis is true. When this likelihood ratio is large, we shall say the
outcome rules in favor of the alternative hypothesis. Working downward
from the outcomes with the highest values, we continue to add outcomes
to the rejection region of the test—so-called because these are the out-
comes for which we would reject the primary hypothesis—until the total
probability of the rejection region under the null hypothesis is equal to
some predesignated signiﬁcance level.
To see that we have done the best we can do, suppose we replace 
one of the outcomes we assigned to the rejection region with one we 
did not. The probability that this new outcome would occur if the
primary hypothesis is true must be less than or equal to the probability
that the outcome it replaced would occur if the primary hypothesis is 
true. Otherwise, we would exceed the signiﬁcance level. Because of how
we assigned outcome to the rejection region, the likelihood ratio of the
new outcome is smaller than the likelihood ratio of the old outcome. 
Thus the probability that the new outcome would occur if the alternative
hypothesis is true must be less than or equal to the probability that the
outcome it replaced would occur if the alternative hypothesis is true. 
That is, by swapping outcomes we have reduced the power of our test. 
By following the method of Neyman and Pearson and maximizing the
likelihood ratio, we obtain the most powerful test at a given signiﬁcance
level.
To take advantage of Neyman and Pearson’s ﬁnding, we need to have
an alternative hypothesis or alternatives ﬁrmly in mind when we set up a
test. Too often in published research, such alternative hypotheses remain
unspeciﬁed or, worse, are speciﬁed only after the data are in hand. We
must specify our alternatives before we commence an analysis, preferably at
the same time we design our study.
18
PART I
FOUNDATIONS

Are our alternatives one-sided or two-sided? Are they ordered or
unordered? The form of the alternative will determine the statistical proce-
dures we use and the signiﬁcance levels we obtain.
Decide beforehand whether you wish to test against a one-sided or a two-
sided alternative.
One-Sided or Two-Sided
Suppose on examining the cancer registry in a hospital, we uncover the
following data that we put in the form of a 2 × 2 contingency table.
CHAPTER 2
HYPOTHESES: THE WHY OF YOUR RESEARCH
19
Survived
Died
Total
Men
9
1
10
Women
4
10
14
Total
13
11
24
The 9 denotes the number of males who survived, the 1 denotes the
number of males who died, and so forth. The four marginal totals or mar-
ginals are 10, 14, 13, and 11. The total number of men in the study is 10,
and 14 denotes the total number of women, and so forth.
The marginals in this table are ﬁxed because, indisputably, there are 11
dead bodies among the 24 persons in the study and 14 women. Suppose
that before completing the table we lost the subject IDs, so that we could
no longer identify which subject belonged in which category. Imagine you
are given two sets of 24 labels. The ﬁrst set has 14 labels with the word
“woman” and 10 labels with the word “man.” The second set of labels
has 11 labels with the word “dead” and 12 labels with the word “alive.”
Under the null hypothesis, you are allowed to distribute the labels to sub-
jects independently of one another. One label from each of the two sets
per subject, please.
There are a total of 
ways you could assign the labels. 
of the assignments result in tables that are as extreme as our original table, 
(that is, in which 90% of the men survive) and 
in tables that are
more extreme (100% of the men survive). This is a very small fraction of
the total, so we conclude that a difference in survival rates of the two
sexes as extreme as the difference we observed in our original table is very
unlikely to have occurred by chance alone. We reject the hypothesis that
the survival rates for the two sexes are the same and accept the alternative
hypothesis that, in this instance at least, males are more likely to proﬁt
from treatment.
14
11
10
0







14
10
10
1







24
10





In terms of the relative survival rates of the two sexes, the ﬁrst of these
tables is more extreme than our original table. The second is less extreme.
In the preceding example, we tested the hypothesis that survival rates
do not depend on sex against the alternative that men diagnosed with
cancer are likely to live longer than women similarly diagnosed. We
rejected the null hypothesis because only a small fraction of the possible
tables were as extreme as the one we observed initially. This is an example
of a one-tailed test. But is it the correct test? Is this really the alternative
hypothesis we would have proposed if we had not already seen the data?
Wouldn’t we have been just as likely to reject the null hypothesis that men
and women proﬁt the same from treatment if we had observed a table of
the following form?
20
PART I
FOUNDATIONS
TABLE 2.1 Survival Rates of Men and Women
Survived
Died
Total
Men
10
0
10
Women
3
11
14
Total
13
11
24
Survived
Died
Total
Men
8
2
10
Women
5
9
14
Total
13
11
24
Survived
Died
Total
Men
0
10
10
Women
13
1
14
Total
13
11
24
Of course, we would! In determining the signiﬁcance level in the
present example, we must add together the total number of tables that lie
in either of the two extremes or tails of the permutation distribution.
The critical values and signiﬁcance levels are quite different for one-
tailed and two-tailed tests, and, all too often, the wrong test has been
employed in published work. McKinney et al. (1989) reviewed some 70
plus articles that appeared in six medical journals. In over half of these
articles, Fisher’s exact test was applied improperly. Either a one-tailed test
had been used when a two-tailed test was called for or the authors of the
paper simply hadn’t bothered to state which test they had used.

Of course, unless you are submitting the results of your analysis to a
regulatory agency, no one will know whether you originally intended a
one-tailed test or a two-tailed test and subsequently changed your mind.
No one will know whether your hypothesis was conceived before you
started or only after you’d examined the data. All you have to do is lie.
Just recognize that if you test an after-the-fact hypothesis without identify-
ing it as such, you are guilty of scientiﬁc fraud.
When you design an experiment, decide at the same time whether you
wish to test your hypothesis against a two-sided or a one-sided alternative.
A two-sided alternative dictates a two-tailed test; a one-sided alternative
dictates a one-tailed test.
As an example, suppose we decide to do a follow-on study of the cancer
registry to conﬁrm our original ﬁnding that men diagnosed as having
tumors live signiﬁcantly longer than women similarly diagnosed. In this
follow-on study, we have a one-sided alternative. Thus we would analyze
the results with a one-tailed test rather than the two-tailed test we applied
in the original study.
Determine beforehand whether your alternative hypotheses are ordered or
unordered.
Ordered or Unordered Alternative Hypotheses?
When testing qualities (number of germinating plants, crop weight, etc.)
from k samples of plants taken from soils of different composition it is
often routine to use the F-ratio of the analysis of variance. For contin-
gency tables, many routinely use the chi-square test to determine whether
the differences among samples are signiﬁcant. But the F-ratio and the chi-
square are what are termed omnibus tests, designed to be sensitive to all
possible alternatives. As such, they are not particularly sensitive to ordered
alternatives such “as more fertilizer more growth” or “more aspirin faster
relief of headache.” Tests for such ordered responses at k distinct treat-
ment levels should properly use the Pitman correlation described by Frank,
Trzos, and Good (1976) when the data are measured on a metric scale
(e.g., weight of the crop). Tests for ordered responses in 2 × C contin-
gency tables (e.g., number of germinating plants) should use the trend
test described by Berger, Permutt, and Ivanova (1998). We revisit this
topic in more detail in Chapter 2.
DEDUCTION AND INDUCTION
When we determine a p-value as we did in the example above, we apply a
set of algebraic methods and deductive logic to deduce the correct value.
The deductive process is used to determine the appropriate size of resistor
CHAPTER 2
HYPOTHESES: THE WHY OF YOUR RESEARCH
21

to use in an electric circuit, to determine the date of the next eclipse of
the moon, and to establish the identity of the criminal (perhaps from the
fact that a dog did not bark on the night of the crime). Find the formula,
plug in the values, turn the crank, and out pops the result (or it does for
Sherlock Holmes4, at least).
When we assert that for a given population a percentage of samples will
have a speciﬁc composition, this also is a deduction. But when we make an
inductive generalization about a population based on our analysis of a
sample we are on shakier ground. It is one thing to assert that if an obser-
vation comes from a normal distribution with mean zero, the probability
is one-half that it is positive. It is quite another if, on observing that half
the observations in the sample are positive, we assert that half of all the
possible observations that might be drawn from that population will be
positive also.
Newton’s law of gravitation provided an almost exact ﬁt (apart from
measurement error) to observed astronomical data for several centuries;
consequently, there was general agreement that Newton’s generalization
from observation was an accurate description of the real world. Later, as
improvements in astronomical measuring instruments extended the range
of the observable universe, scientists realized that Newton’s law was only a
generalization and not a property of the universe at all. Einstein’s Theory
of Relativity gives a much closer ﬁt to the data, a ﬁt that has not been
contradicted by any observations in the century since its formulation. But
this still does not mean that relativity provides us with a complete, correct,
and comprehensive view of the universe.
In our research efforts, the only statements we can make with God-like
certainty are of the form “our conclusions ﬁt the data.” The true nature of
the real world is unknowable. We can speculate, but never conclude.
LOSSES
In our ﬁrst advanced course in statistics, we read in the ﬁrst chapter of
Lehmann (1986) that the “optimal” statistical procedure would depend
on the losses associated with the various possible decisions. But on day
one of our venture into the real world of practical applications, we were
taught to ignore this principle.
At that time, the only computationally feasible statistical procedures
were based on losses that were proportional to the square of the difference
between estimated and actual values. No matter that the losses really
might be proportional to the absolute value of those differences, or the
22
PART I
FOUNDATIONS
4 See “The Adventure of Silver Blaze” by A. Conan-Doyle, The Strand, December 1892.

cube, or the maximum over a certain range. Our options were limited by
our ability to compute.
Computer technology has made a series of major advances in the past
half-century. What required days or weeks to calculate forty years ago
takes only milliseconds today. We can now pay serious attention to this
long-neglected facet of decision theory: the losses associated with the
varying types of decision.
Suppose we are investigating a new drug: We gather data, perform a
statistical analysis, and draw a conclusion. If chance alone is at work yield-
ing exceptional values and we opt in favor of the new drug, we’ve made
an error. We also make an error if we decide there is no difference and the
new drug really is better. These decisions and the effects of making them
are summarized in Table 2.2.
We distinguish the two types of error because they have the quite differ-
ent implications described in Table 2.2. As a second example, Fears,
Tarone, and Chu (1977) use permutation methods to assess several stan-
dard screens for carcinogenicity. As shown in Table 2.3, their Type I error,
a false positive, consists of labeling a relatively innocuous compound as
carcinogenic. Such an action means economic loss for the manufacturer
and the denial to the public of the compound’s beneﬁts. Neither conse-
CHAPTER 2
HYPOTHESES: THE WHY OF YOUR RESEARCH
23
The Facts
Our Decision
No difference.
No difference.
Drug is better.
Type I Error:
Manufacturer wastes money
developing ineffective drug.
Drug is better.
Type II Error:
Manufacturer misses
opportunity for proﬁt.
Public denied access to
effective treatment.
TABLE 2.2 Decision-Making Under Uncertainty
The Facts
Fears et al.’s Decision
Not a carcinogen.
Not a carcinogen.
Is a carcinogen.
Type I Error:
Manufacturer misses
opportunity for proﬁt.
Public denied access to
effective treatment.
Is a carcinogen.
Type II Error:
Patients die; families suffer;
Manufacturer sued.
TABLE 2.3 Decision-Making Under Uncertainty

quence is desirable. But a false negative, a Type II error, is much worse, as
it would mean exposing a large number of people to a potentially lethal
compound.
What losses are associated with the decisions you will have to make?
Specify them now before you begin.
DECISIONS
The hypothesis-alternative duality is inadequate in most real-life situations.
Consider the pressing problems of global warming and depletion of the
ozone layer. We could collect and analyze yet another set of data and
then, just as is done today, make one of three possible decisions: reduce
emissions, leave emission standards alone, sit on our hands and wait for
more data to come in. Each decision has consequences as shown in 
Table 2.4.
As noted at the beginning of this chapter, it’s essential that we specify in
advance the actions to be taken for each potential result. Always suspect
are after-the-fact rationales that enable us to persist in a pattern of conduct
despite evidence to the contrary. If no possible outcome of a study will be
sufﬁcient to change our mind, then perhaps we ought not undertake such
a study in the ﬁrst place.
Every research study involves multiple issues. We might want to know
not only whether a measurable, biologically (or medically, physically, or
sociologically) signiﬁcant effect takes place but also what the size of the
effect is and the extent to which the effect varies from instance to
instance. We would also want to know what factors, if any, will modify the
size of the effect or its duration.
We may not be able to address all these issues with a single data set. A
preliminary experiment might tell us something about the possible exis-
tence of an effect, along with rough estimates of its size and variability.
Hopefully, we glean enough information to come up with doses, environ-
mental conditions, and sample sizes to apply in collecting and evaluating
the next data set. A list of possible decisions after the initial experiment
24
PART I
FOUNDATIONS
The Facts
President’s Decision on Emissions
Reduce emissions.
Gather more data.
Change 
unnecessary.
No effect.
Economy disrupted.
Sampling cost
Burning of
Sampling cost
Decline in quality 
fossil fuels is
Decline in quality of
of life
responsible.
life (irreversible?)
(irreversible?)
TABLE 2.4 Effect of Global Warming

includes “Abandon this line of research,” “Modify the environment and
gather more data,” “Perform a large, tightly controlled, expensive set of
trials.” Associated with each decision is a set of potential gains and losses.
Common sense dictates we construct a table similar to Tables 2.2 or 2.3
before we launch a study.
For example, in clinical trials of a drug we might begin with some
animal experiments, then progress to Phase I clinical trials in which, with
the emphasis on safety, we look for the maximum tolerable dose. Phase I
trials generally involve only a small number of subjects and a one-time or
short-term intervention. An extended period of several months may be
used for follow-up purposes. If no adverse effects are observed, we might
decide to go ahead with a further, or Phase II, set of trials in the clinic in
which our objective is to determine the minimum effective dose. Obvi-
ously, if the minimum effective dose is greater than the maximum tolera-
ble dose, or if some dangerous side effects are observed that we didn’t
observe in the ﬁrst set of trials, we’ll abandon the drug and go on to some
other research project. But if the signs are favorable, then and only then
will we go to a set of Phase III trials involving a large number of subjects
observed over an extended time period. Then and only then will we hope
to get the answers to all our research questions.
Before you begin, list all the consequences of a study and all the actions
you might take. Persist only if you can add to existing knowledge.
TO LEARN MORE
For more thorough accounts of decision theory, the interested reader is
directed to Berger (1986), Blyth (1970), Cox (1958), DeGroot (1970),
and Lehmann (1986). For an applied perspective, see Clemen (1991),
Berry (1995), and Sox, Blatt, Higgins, and Marton (1988).
Over 300 references warning of the misuse of null hypothesis testing
can be accessed online at the URL http://www.cnr.colostate.edu/
~anderson/thompson1.html. Alas, the majority of these warnings are ill
informed, stressing errors that will not arise if you proceed as we recom-
mend and place the emphasis on the why, not the what of statistical proce-
dures. Use statistics as a guide to decision-making rather than a mandate.
Neyman and Pearson (1933) ﬁrst formulated the problem of hypothesis
testing in terms of two types of error. Extensions and analyses of their
approach are given by Lehmann (1986) and Mayo (1996). For more work
along the lines proposed here, see Selike, Bayarri, and Berger (2001).
Clarity in hypothesis formulation is essential; ambiguity can only yield
controversy. See, for example, Kaplan (2001).
CHAPTER 2
HYPOTHESES: THE WHY OF YOUR RESEARCH
25

Chapter 3
Collecting Data
CHAPTER 3
COLLECTING DATA
27
Common Errors in Statistics (and How to Avoid Them), 2e, by Phillip I. Good and James W. Hardin.
Copyright © 2006 John Wiley & Sons, Inc.
THE VAST MAJORITY OF ERRORS IN STATISTICS, and, not incidentally, in most
human endeavors, arises from a reluctance (or even an inability) to plan.
Some demon (or demonic manager) seems to be urging us to cross the
street before we’ve had the opportunity to look both ways. Even on those
rare occasions when we do design an experiment, we seem more obsessed
with the mechanics than with the concepts that underlie experimental
design.
In this chapter we review the fundamental concepts of experimental
design, the determination of sample size, the assumptions that underlie
most statistical procedures, and the precautions necessary to ensure that
they are satisﬁed and that the data you collect will be representative of the
population as a whole. We do not intend to replace a text on experiment
or survey design, but to supplement it, providing examples and solutions
that are often neglected in courses on the subject.
PREPARATION
The ﬁrst step in data collection is to have a clear, preferably written state-
ment of your objectives. In accordance with Chapter 1, you will have
deﬁned the population or populations from which you intend to sample
and have identiﬁed the characteristics of these populations you wish to
investigate.
You developed one or more well-formulated hypotheses (the topic of
Chapter 2) and have some idea of the risks you will incur should your
GIGO: Garbage in; garbage out.
Fancy statistical methods will not rescue garbage data.
Course notes of Raymond J. Carroll (2001)

analysis of the collected data prove to be erroneous. You will need to
decide what you wish to observe and measure, and how you will go about
observing it.
Good practice is to draft the analysis section of your ﬁnal report based
on the conclusions you would like to make. What information do you
need to justify these conclusions? All such information must be collected.
The next section is devoted to the choice of measuring devices, fol-
lowed by sections on determining sample size and preventive steps to
ensure your samples will be analyzable by statistical methods.
MEASURING DEVICES
Know what you want to measure. Collect exact values whenever possible.
Know what you want to measure. Will you measure an end point such as
death or a surrogate such as the presence of HIV antibodies? The regres-
sion slope describing the change in systolic blood pressure (in mmHg) per
100mg of calcium intake is strongly inﬂuenced by the approach used for
assessing the amount of calcium consumed (Cappuccio et al., 1995). The
association is small and only marginally signiﬁcant with diet histories
[slope −0.01 (−0.003 to −0.016)] but large and highly signiﬁcant when
food frequency questionnaires are used [−0.15 (−0.11 to −0.19)]. With
studies using 24-hour recall an intermediate result emerges [−0.06 (−0.09
to −0.03)]. Diet histories assess patterns of usual intake over long periods
of time and require an extensive interview with a nutritionist, whereas 24-
hour recall and food frequency questionnaires are simpler methods that
reﬂect current consumption (Block, 1982).
Before we initiate data collection, we must have a ﬁrm idea of what we
will measure.
A second fundamental principle is also applicable to both experiments
and surveys: Collect exact values whenever possible. Worry about group-
ing them in interval or discrete categories later.
A long-term study of buying patterns in New South Wales illustrates
problems caused by grouping prematurely. At the beginning of the study,
the decision was made to group the incomes of survey subjects into cate-
gories: under $20,000, $20,000 to $30,000, and so forth. Six years of
steady inﬂation later and the organizers of the study realized that all the
categories had to be adjusted. An income of $21,000 at the start of the
study would only purchase $18,000 worth of goods and housing at 
the end (see Figure 3.1.) The problem was that those surveyed toward the
end had ﬁlled out forms with exactly the same income categories. Had
income been tabulated to the nearest dollar, it would have been easy to
correct for increases in the cost of living and convert all responses to the
28
PART I
FOUNDATIONS

same scale. But the study designers hadn’t considered these issues. A
precise and costly survey was now a matter of guesswork.
You can always group your results (and modify your groupings) after a
study is completed. If after-the-fact grouping is a possibility, your design
should state how the grouping will be determined; otherwise there will be
the suspicion that you chose the grouping to obtain desired results.
Experiments
Measuring devices differ widely both in what they measure and the preci-
sion with which they measure it. As noted in the next section of this
chapter, the greater the precision with which measurements are made, the
smaller the sample size required to reduce both Type I and Type II errors
below speciﬁc levels.
Before you rush out and purchase the most expensive and precise mea-
suring instruments on the market, consider that the total cost C of an
experimental procedure is S + nc, where n is the sample size and c is the
cost per unit sampled.
CHAPTER 3
COLLECTING DATA
29
1970
1975
1980
1985
1990
1995
2000
2005
$0.00
$0.50
$1.00
$1.50
$2.00
$2.50
$3.00
$3.50
$4.00
$4.50
$5.00
$5.50
$1.00
$1.39
$2.12
$2.77
$3.37
$3.93
$4.44
$5.02
Year
FIGURE 3.1
Equivalent Purchasing Powers Over Time Using Consumer
Price Index Calculations. Each year shows the cost of the equivalent goods/
services.

The startup cost S includes the cost of the measuring device; c is made
up of the cost of supplies and personnel costs. The latter includes the time
spent not only on individual measurements but in preparing and calibrat-
ing the instrument for use.
Less obvious factors in the selection of a measuring instrument include
impact on the subject, reliability (personnel costs continue even when an
instrument is down), and reusability in future trials. For example, one of
the advantages of the latest technology for blood analysis is that less blood
needs to be drawn from patients. Less blood means happier subjects, fewer
withdrawals, and a smaller initial sample size.
Surveys
Although no scientist would dream of performing an experiment without
ﬁrst mastering all the techniques involved, an amazing number will
blunder into the execution of large-scale and costly surveys without a pre-
liminary study of all the collateral issues a survey entails.
We know of one institute that mailed out some 20,000 questionnaires
(didn’t the post ofﬁce just raise its rates again?) before discovering that
half the addresses were in error and that the vast majority of the remain-
der were being discarded unopened before prospective participants had
even read the “sales pitch.”
Fortunately, there are texts such as those by Bly (1990, 1996) that will
tell you how to word a “sales pitch” and the optimal colors and graphics
to use along with the wording. They will tell you what “hooks” to use on
the envelope to ensure attention to the contents and what premiums to
offer to increase participation.
There are other textbooks such as those by Converse and Presser
(1986), Fowler and Fowler (1995), and Schroeder (1987) to assist you in
wording questionnaires and in pretesting questions for ambiguity before
you begin. We’ve only two paragraphs of caution to offer:
1. Be sure your questions don’t reveal the purpose of your study, else
respondents shape their answers to what they perceive to be your
needs. Contrast “How do you feel about compulsory pregnancy?”
with “How do you feel about abortions?”
2. With populations ever more heterogeneous, questions that work
with some ethnic groups may repulse others (see, for example,
Choi, 2000).
Recommended are web-based surveys with initial solicitation by mail
(letter or post card) and e-mail. Not only are both costs and time to com-
pletion cut dramatically, but also the proportion of missing data and
incomplete forms is substantially reduced. Moreover, web-based surveys
are easier to monitor, and forms may be modiﬁed on the ﬂy. Web-based
30
PART I
FOUNDATIONS

entry also offers the possibility of displaying the individual’s prior
responses during follow-up surveys.
Three other precautions can help ensure the success of your survey:
1. Award premiums only for fully completed forms.
2. Continuously tabulate and monitor submissions; don’t wait to be
surprised.
3. A quarterly newsletter sent to participants will substantially
increase retention (and help you keep track of address changes).
DETERMINING SAMPLE SIZE
Determining optimal sample size is simplicity itself once we specify
whether we wish to test a hypothesis or provide a conﬁdence interval.
To determine the optimal sample size for testing a hypothesis, we need
to specify all of the following:
•
Desired power and signiﬁcance level
•
Distributions of the observables
•
Statistical test(s) that will be employed
•
Whether each comparison is formulated as a one-tailed or a two-
tailed test
To determine the optimal sample size for providing a conﬁdence inter-
val, we need to specify the following:
•
Desired level of conﬁdence
•
Desired width of the interval
•
Distributions of the observables.
In either case, we will need to specify and correct for the anticipated losses
due to nonresponders, noncompliant participants, and dropouts.
Power and Signiﬁcance Level
Sample size must be determined for each experiment; there is no univer-
sally correct value. We need to understand and make use of the relation-
ships among sample size, signiﬁcance level, power, and precision of our
measuring instruments.
Increase the precision (and hold all other parameters ﬁxed), and we can
decrease the required number of observations. Decreases in any or all of
the intrinsic and extrinsic sources of variation will also result in a decrease
in the required sample size.
Permit a greater number of Type I or Type II errors (and hold all 
other parameters ﬁxed), and we can decrease the required number of
observations.
CHAPTER 3
COLLECTING DATA
31

Explicit formula for power and signiﬁcance level are available when the
underlying observations are binomial, the results of a counting or Poisson
process, or normally distributed. Several off-the-shelf computer programs
including nQuery AdvisorTM, Pass 2005TM, and StatXactTM are available to
do the calculations for us.
To use these programs, we need to have some idea of the location
(mean) and scale parameter (variance) of the distribution both when the
primary hypothesis is true and when an alternative hypothesis is true.
Because there may well be inﬁnite alternatives in which we are interested,
power calculations should be based on the worst case or boundary value.
For example, if we are testing a binomial hypothesis p = 1/2 against the
alternatives p ≤2/3, we would assume that p = 2/3.
A recommended rule of thumb is to specify the alternative as the small-
est effect that is of practical signiﬁcance.
When determining sample size for data drawn from the binomial or any
other discrete distribution, one should always display the power curve.
The explanation for why we routinely include this graphic lies in the saw-
toothed nature of the curve (Chernick and Liu, 2002). As a result of
inspecting the power curve by eye, it may be possible to discern a less-
expensive solution than offered by the software.
If the data do not come from a well-tabulated distribution, then one
might use a bootstrap to estimate the power and signiﬁcance level.
In preliminary trials of a new device, test results of 7.0 were observed in
11 of 12 cases and 3.3 in 1 of 12 cases. Industry guidelines speciﬁed that
any population with a mean test result greater than 5 would be acceptable.
A worst-case or boundary-value scenario would include one in which the
test result was 7.0 3/7th of the time, 3.3 3/7th of the time, and 4.1
1/7th of the time.
32
PART I
FOUNDATIONS
Type I error (α)
Probability of falsely rejecting the hypothesis when it is 
true
Type II error (1 −β[A])
Probability of falsely accepting the hypothesis when an 
alternative hypothesis A is true; Depends on the 
alternative A
Power = β[A]
Probability of correctly rejecting the hypothesis when an 
alternative hypothesis A is true; depends on the 
alternative A.
Distribution functions
F[(x −µ)σ]; e.g., normal distribution
Location parameters
For both hypothesis and alternative hypothesis, µ1, µ2
Scale parameters
For both hypothesis and alternative hypothesis, σ1, σ2
Sample sizes
May be different for different groups in an experiment 
with more than one group
TABLE 3.1 Ingredients in a Sample Size Calculation

The statistical procedure required us to reject if the sample mean of the
test results were less than 6. To determine the probability of this event for
various sample sizes, we took repeated samples with replacement from the
two sets of test results. Some bootstrap samples consisted of all 7s, some,
taken from the worst-case distribution, only of 3.3s. Most were a mixture.
Table 3.2 illustrates the results; for example, in our trials, 23% of the
bootstrap samples of size 3 from our starting sample of test results had
medians less than 6. If instead we drew our bootstrap samples from the
hypothetical “worst-case” population, then 84% had medians less than 6.
If you want to try your hand at duplicating these results, simply take the
test values in the proportions observed, stick them in a hat, draw out
CHAPTER 3
COLLECTING DATA
33
Exact Power
120
118
116
114
112
110
108
106
104
102
100
36.00
35.00
34.00
33.00
32.00
31.00
30.00
29.00
Power (%)
Sample Size
Power plot
FIGURE 3.2
Power as a Function of Sample Size. Test of the hypothesis that
the prevalence in a population of a characteristic is 0.46 rather than 0.40.
Sample Size
Test Mean < 6
α
Power
3
0.23
0.84
4
0.04
0.80
5
0.06
0.89
TABLE 3.2

bootstrap samples with replacement several hundred times, compute the
sample means, and record the results. Or you could use the StataTM boot-
strap procedure, as we did.1
34
PART I
FOUNDATIONS
One Tail or Two?
A one-sided alternative (“Regular brushing with a ﬂouride toothpaste will
reduce cavities”) requires a one-tailed or one-sided test. A two-sided alter-
native (“The efﬁcacy of this new drug is different from the old drug”)
requires a two-tailed test.
However, in practical situations it is often difﬁcult to decide whether
the real alternatives are one-sided or two-sided. L. A. Moye provides a
particularly horrifying illustration in his text Statistical Reasoning in Medi-
cine (p. 145–148), which, in turn, was extracted from Thomas Moore’s
Deadly Medicine (p. 203–204). The particular example concerns a study of
cardiac arrhythmia suppression, in which a widely used but untested
therapy was at last tested in a series of controlled, randomized, double-
blind clinical trials (Greene et al., 1992).
The study had been set up as a sequential trial. At various checkpoints,
the trial would be stopped if efﬁcacy were demonstrated or if it became
evident that the treatment was valueless. Although no preliminary studies
had been made, the investigators did not plan for the possibility that the
already widely used but untested treatment might actually be harmful. It
was.
Fortunately, clinical trials have multiple end points, an invaluable
resource if investigators choose to look at the data. In this case, a Data
and Safety Monitoring Board (consisting of scientists not directly afﬁliated
with the trial or the trial’s sponsers) noted that of 730 patients random-
ized to the active therapy, 56 died, whereas of the 725 patients random-
DON’T LET THE SOFTWARE DO THE THINKING
Many researchers rely on menu-driven software to perform power and
sample size calculations. Most such software comes with default settings—
for example, α = 0.05 or tails = 2. These settings are readily altered, if, that
is, investigators bother to take the time.
Among the errors made by participants in a recent workshop on sample
size determination was allowing the software to calculate necessary sample
size for a two-sample, two-tailed test for the hypothesis that 50% or less
of subjects would behave in a certain way versus the alternative that 60%
or more of them would.
1 Chapters 4–7 have more information on the use of the bootstrap and its limitations.

ized to placebo there were 22 deaths. They felt free to perform a two-
sided test despite the original formulation of the problem as one-sided.
Prepare for Missing Data
The relative ease with which a program like Stata or StatXact can produce
a sample size may blind us to the fact that the number of subjects with
which we begin a study may bear little or no relation to the number with
which we conclude it.
A midsummer hailstorm, an early frost, or an insect infestation can lay
waste to all or part of an agricultural experiment. In the National Institute
of Aging’s ﬁrst years of existence, a virus wiped out the entire primate
colony, destroying a multitude of experiments in progress.
Large-scale clinical trials and surveys have a further burden, the subjects
themselves. Potential subjects can and do refuse to participate. (Don’t
forget to budget for a follow-up study, bound to be expensive, of respon-
ders versus nonresponders.) Worse, they agree to participate initially, but
drop out at the last minute (see Fig. 3.3).
CHAPTER 3
COLLECTING DATA
35
Examined
800
Randomized
700
Excluded
100
New
340
Control
360
Postprocedure
328 
Dropouts
12
Postprocedure
345 
Dropouts
15 
1-mo follow-up
324
Dropouts
4 
1-mo follow-up
344
Dropouts
1 
FIGURE 3.3
A Typical Clinical Trial. Dropouts and noncompliant patients
occur at every stage. Reprinted from the Manager’s Guide to Design and Conduct
of Clinical Trials with the permission of John Wiley & Sons, Inc.

They move without a forwarding address before a scheduled follow-up.
Or simply don’t bother to show up for an appointment. We lost 30% of
the patients in the follow-ups to a life-saving cardiac procedure. (We can’t
imagine not going in to see our surgeon, but then we may not be typical
participants.)
The key to a successful research program is to plan for such dropouts in
advance and to start the trials with more than the number required to
achieve a given power and signiﬁcance level.
In a recent attempt to reduce epidemics at its training centers, the U.S.
Navy vaccinated 50,000 recruits with an experimental vaccine and 50,000
others with a harmless saline solution. But at the halfway mark, with
50,000 inoculated and 50,000 to go, fewer than 500 recruits had con-
tracted the disease of interest. The bottom line: It’s the sample you end
with, not the sample you designed for, that determines the power of your
tests.
Nonresponders
An analysis of those who did not respond to a survey or a treatment can
sometimes be as or more informative than the survey itself. See, for
example, Mangel and Samaniego (1984) as well as the sections on the
Behrens–Fisher problem and on the premature drawing of conclusions in
Chapter 5. Be sure to incorporate in your sample design and in your
budget provisions for sampling nonresponders.
Sample from the Correct Population
Be sure you are sampling from the population as a whole rather than from
an nonrepresentative subset of the population. The most famous blunder
along these lines was basing the forecast of Landon over Roosevelt in the
1936 U.S. presidential election on a telephone survey: Those who owned
a telephone and responded to the survey favored Landon; those who
voted did not. An economic study may be ﬂawed because we have over-
looked the homeless,2 an astrophysical study ﬂawed because of overlooking
galaxies whose central surface brightness was very low.3
FUNDAMENTAL ASSUMPTIONS
Most statistical procedures rely on two fundamental assumptions: that the
observations are independent of one another and that they are identically
distributed. If your methods of collection fail to honor these assumptions,
then your analysis must fail also.
36
PART I
FOUNDATIONS
2 City of New York v. Dept of Commerce, 822 F. Supp. 906 (E.D.N.Y., 1993).
3 Bothun (1999, p. 249).

Independent Observations
To ensure the independence of responses in a return-by-mail or return-by-
web survey, no more than one form per household should be accepted. If
a comparison of the responses within a household is desired, then the
members of the household should be interviewed separately, outside of
each other’s hearing and with no opportunity to discuss the survey in
between. People care what other people think and when asked about an
emotionally charged topic may or may not tell the truth. In fact, they are
unlikely to tell the truth if they feel that others may overhear or somehow
learn of their responses.
To ensure independence of the observations in an experiment, deter-
mine in advance what constitutes the experimental unit.
In the majority of cases, the unit is obvious: One planet means one
position in space, one container of gas means one volume and one pres-
sure to be recorded, one runner on one ﬁxed race course means one
elapsed time.
In a clinical trial, each individual patient corresponds to a single set of
observations. Or does she? Suppose we are testing the effects of a topical
ointment on pink eye. Is each eye a separate experimental unit or each
patient?
It is common in toxicology to examine a large number of slides. But
regardless of how many are examined in the search for mutagenic and
toxic effects, if all slides come from a single treated animal, then the total
size of the sample is one.
We may be concerned with the possible effects a new drug might have
on a pregnant woman and, as critically, on her children. In our prelimi-
nary tests, we’ll be working with mice. Is each fetus in the litter a separate
experimental unit? Or each mother?
If the mother is the one treated with the drug, then the mother is the
experimental unit, not the fetus. A litter of six or seven corresponds only
to a sample of size one.
As for the topical ointment, although more precise results might be
obtained by treating only one eye with the new ointment and recording
the subsequent difference in appearance between the treated and untreated
eyes, each patient still yields only one observation, not two.
Identically Distributed Observations
If you change measuring instruments during a study or change observers,
then you will have introduced an additional source of variation and the
resulting observations will not be identically distributed.
The same problems will arise if you discover during the course of a
study (as is often the case) that a precise measuring instrument is no
longer calibrated and readings have drifted. To forestall this, any measur-
CHAPTER 3
COLLECTING DATA
37

ing instrument should have been exposed to an extensive burn-in before
the start of a set of experiments and should be recalibrated as frequently as
the results of the burn-in or prestudy period dictate.
Similarly, one doesn’t just mail out several thousand copies of a survey
before performing an initial pilot study to weed out or correct ambiguous
and misleading questions.
The following groups are unlikely to yield identically distributed obser-
vations: the ﬁrst to respond to a survey, those who only respond after
been offered an inducement, and nonresponders.
EXPERIMENTAL DESIGN
Statisticians have found three ways for coping with individual-to-individual
and observer-to-observer variation:
1. Controlling. The fewer the extrinsic sources of variation, the
smaller the sample size required. Make the environment for the
study—the subjects, the manner in which the treatment is adminis-
tered, the manner in which the observations are obtained, the
apparatus used to make the measurements, and the criteria for
interpretation—as uniform and homogeneous as possible.
2. Blocking. A clinician might stratify the population into subgroups
based on such factors as age, sex, race, and the severity of the con-
dition and restricting comparisons to individuals who belong to
the same subgroup. An agronomist would want to stratify on the
basis of soil composition and environment.
3. Randomizing. This consists of randomly assigning patients to
treatment within each subgroup so that the innumerable factors
that can neither be controlled nor observed directly are as likely to
inﬂuence the outcome of one treatment as another.
Steps 1 and 2 are trickier than they appear at ﬁrst glance. Do the phe-
nomena under investigation depend on the time of day, as with body tem-
perature and the incidence of mitosis? On the day of the week, as with
retail sales and the daily mail? Will the observations be affected by the sex
of the observer? Primates (including you) and hunters (tigers, mountain
lions, domestic cats, dogs, wolves, and so on) can readily detect the
observer’s sex.4
Blocking may be mandatory as even a randomly selected sample may
not be representative of the population as a whole. For example, if a
minority comprise less than 10% of a population, then a jury of 12 persons
selected at random from that population will fail to contain a single
member of that minority at least 28% of the time.
38
PART I
FOUNDATIONS
4 The hair follicles of redheads, genuine not dyed, are known to secrete a prostaglandin
similar to an insect pheromone.

Groups to be compared may differ in other important ways even before
any intervention is applied. These baseline imbalances cannot be attributed
to the interventions, but they can interfere with and overwhelm the com-
parison of the interventions.
One good after-the-fact solution is to break the sample itself into strata
(men, women, Hispanics) and to extrapolate separately from each stratum
to the corresponding subpopulation from which the stratum is drawn.
The size of the sample we take from each block or stratum need not,
and in some instances should not, reﬂect the block’s proportion in the
population. The latter exception arises when we wish to obtain separate
estimates for each subpopulation. For example, suppose we are studying
the health of Marine recruits and wish to obtain separate estimates for
male and female Marines as well as for Marines as a group. If we want to
establish the incidence of a relatively rare disease, we will need to oversam-
ple female recruits to ensure that we obtain a sufﬁciently large number. To
obtain a rate R for all Marines we would then take the weighted average
pFRF + pMRM of the separate rates for each sex, where the proportions pM
and pF are those of males and females in the entire population of Marine
recruits.
FOUR GUIDELINES
In the next few sections on experimental design, we may well be preach-
ing to the choir, for which we apologize. But there is no principle of
experimental design, however obvious, however intuitive, that someone
will not argue can be ignored in his or her special situation:
•
Physicians feel they should be allowed to select the treatment that
will best affect their patient’s condition (but who is to know in
advance what this treatment is?).
•
Scientists eject us from their laboratories when we suggest that
only the animal caretakers be permitted to know which cage
houses the control animals.
•
Engineers at a ﬁrm that specializes in refurbishing medical devices
objected when Dr. Good suggested they purchase and test some
new equipment for use as controls. “But that would cost a
fortune.”
The statistician’s lot is not a happy one. The opposite sex ignores us
because we are boring,5 managers hate us because all our suggestions 
seem to require an increase in the budget. But controls will save money in
CHAPTER 3
COLLECTING DATA
39
5 Dr. Good told his wife he was an author—it was the only way he could lure someone that
attractive to his side. Dr. Hardin is still searching for an explanation for his own good
fortune.

the end. Blinding is essential if our results are to have credence, and care
in treatment allocation is mandatory if we are to avoid bias.
Randomize
Permitting treatment allocation by either experimenter or subject will
introduce bias.
Controls
To guard against the unexpected, as many or more patients should be
assigned to the control regimen as are assigned to the experimental one.
This sounds expensive and is. But shit happens. You get the ﬂu. You get a
headache or the runs. You have a series of colds that blend one into the
other until you can’t remember the last time you were well. So you blame
your silicone implants. Or, if you are part of a clinical trial, you stop
taking the drug. It’s in these and similar instances that experimenters are
grateful they’ve included controls, because when the data are examined,
experimenters learn that as many of the control patients came down with
the ﬂu as those who were on the active drug. And that those women
without implants had exactly the same incidence of colds and headaches as
those who had implants.
Reﬂect on the consequences of not using controls. The ﬁrst modern sili-
cone implants (Dow Corning’s Silastic mammary prosthesis) were placed
in 1962. In 1984, a jury awarded $2 million to a recipient who com-
plained of problems resulting from the implants. Award after award fol-
lowed, the largest being more than $7 million. A set of controlled
randomized trials was ﬁnally begun in 1994. The verdict: Silicon implants
have no adverse effects on recipients. Tell this to the stockholders of bank-
rupt Dow Corning.
Use positive controls.
There is no point in conducting an experiment if you already know the
answer.6 The use of a positive control is always to be preferred. A new
anti-inﬂammatory should be tested against aspirin or ibuprofen. And there
can be no justiﬁcation whatever for the use of placebo in the treatment of
a life-threatening disease (Barbui et al., 2000; Djulbegovic et al., 2000).
Blind Observers
Observers should be blinded to the treatment allocation.
Patients often feel better solely because they think they ought to feel
better. A drug may not be effective if the patient is aware it is the old or
40
PART I
FOUNDATIONS
6 The exception being to satisfy a regulatory requirement.

less-favored remedy. Nor is the patient likely to keep taking a drug on
schedule if he or she feels the pill contains nothing of value. She is also
less likely to report any improvement in her condition, if she feels the
doctor has done nothing for her. Vice versa, if a patient is informed that
she has the new treatment she may think it necessary to “please the
doctor” by reporting some diminishment in symptoms. These sorts of
behavioral phenomena are precisely the reason why clinical trials must
include a control.
A double-blind study in which neither the physician nor the patient
knows which treatment is received is preferable to a single-blind study in
which only the patient is kept in the dark (Ederer, 1975; Chalmers et al.,
1983; Vickers et al., 1997).
Even if a physician has no strong feelings one way or the other concern-
ing a treatment, she may tend to be less conscientious about examining
patients she knows belong to the control group. She may have other
unconscious feelings that inﬂuence her work with the patients and the
patients themselves. Exactly the same caveats apply in work with animals
and plants: Units subjected to the existing, less-important treatment may
be handled more carelessly, less thoroughly examined.
We recommend that you employ two or even three individuals, one to
administer the intervention, one to examine the experimental subject, and
a third to observe and inspect collateral readings such as angiograms, labo-
ratory ﬁndings, and X rays that might reveal the treatment.
Conceal Treatment Allocation
Without allocation concealment, selection bias can invalidate study results
(Schultz, 1995; Berger and Exner, 1999). If an experimenter could predict
the next treatment to be assigned, he might exercise an unconscious bias
in the treatment of that patient; he might even defer enrollment of a
patient he considers less desirable. In short, randomization alone, without
allocation concealment, is insufﬁcient to eliminate selection bias and
ensure the internal validity of randomized clinical trials.
Lovell et al. (2000) describe a study in which four patients were ran-
domized to the wrong stratum and, in two cases, the treatment received
was reversed. For an excruciatingly (and embarrassingly) detailed analysis
of this experiment by an FDA regulator, see http://www.fda.gov/
cder/biologics/review/etanimm052799r2.pdf.
Vance Berger and Costas Christophi offer the following guidelines for
treatment allocation:
•
Generate the allocation sequence in advance of screening any
patients.
•
Conceal the sequence from the experimenters.
CHAPTER 3
COLLECTING DATA
41

•
Require the experimenter to enroll all eligible subjects in the order
in which they are screened.
•
Verify that the subject actually received the assigned treatment.
•
Conceal the proportions that have already been allocated (Schultz,
1996).
•
Conceal treatment codes until all patients have been randomized
and the database is locked.
•
Do not permit enrollment discretion when randomization may be
triggered by some earlier response pattern.
Blocked Randomization, Restricted Randomization, and 
Adaptive Designs
All the above caveats apply to these procedures as well. The use of an
advanced statistical technique does not absolve its users from the need to
exercise common sense. Observers must be kept blinded to the treatment
received.
ARE EXPERIMENTS REALLY NECESSARY?
In the case of rare diseases and other rare events, it is tempting to begin
with the data in hand, that is, the records of individuals known to have
the disease, rather than to draw a random and expensive sample from the
population at large. There is a right way and a wrong way to conduct such
studies.
The wrong way is to reason backwards from effect to cause. Suppose
that the majority of victims of pancreatic cancer are coffee drinkers. Does
this mean that coffee causes pancreatic cancer? Not if the majority of indi-
viduals in the population in which the cases occurred are coffee drinkers
also.
To be sure, suppose we create a set of case controls, matching each indi-
vidual in the pancreatic cancer database with a cancer-free individual in the
population at large. The matching of the individuals ensures identical race,
sex, and age and as many other near-matching characteristics as the data
can provide. We could then compare the incidence of coffee drinkers in
the cancer database with the incidence in the matching group of case 
controls.
TO LEARN MORE
Good (2005) provides a series of anecdotes concerning the mythical 
Bumbling Pharmaceutical and Device Company that amply illustrate the
results of inadequate planning. See also Andersen (1990) and Elwood
(1998).
42
PART I
FOUNDATIONS

Deﬁnitions and a further discussion of the interrelation among power
and signiﬁcance level may be found in Lehmann (1986), Casella and
Berger (1990), and Good (2001). You’ll also ﬁnd discussions of optimal
statistical procedures and their assumptions.
Shuster (1993) offers sample size guidelines for clinical trials. A detailed
analysis of bootstrap methodology is provided in Chapters 3 and 7.
For further insight into the principles of experimental design, light on
math and complex formula but rich in insight are the lessons of the
masters: Fisher (1925, 1935) and Neyman (1952). If formulas are what
you desire, see Thompson and Seber (1996), Rosenbaum (2002), Jenni-
son and Turnbull (1999), and Toutenburg (2002).
Among the many excellent texts on survey design are Fink and Kosecoff
(1998), Rea, Parker, and Shrader (1997), and Cochran (1977). For tips
on formulating survey questions, see Converse and Presser (1986), Fowler
and Fowler (1995), and Schroeder (1987). For tips on improving the
response rate, see Bly (1990, 1996).
CHAPTER 3
COLLECTING DATA
43

Part II
HYPOTHESIS
TESTING AND
ESTIMATION

Chapter 4
Estimation
CHAPTER 4
ESTIMATION
47
Common Errors in Statistics (and How to Avoid Them), 2e, by Phillip I. Good and James W. Hardin.
Copyright © 2006 John Wiley & Sons, Inc.
ACCURATE, RELIABLE ESTIMATES ARE ESSENTIAL to effective decision-making.
In this chapter, we review preventive measures and list the properties to
look for in an estimation method. Several robust semiparametric 
estimators are considered along with one method of interval estimation,
the bootstrap.
PREVENTION
The vast majority of errors in estimation stem from a failure to measure
what one wanted to measure or what one thought one was measuring.
Misleading deﬁnitions, inaccurate measurements, errors in recording and
transcription, and confounding variables plague results.
To forestall such errors, review your data collection protocols and pro-
cedure manuals before you begin, run several preliminary trials, record
potential confounding variables, monitor data collection, and review the
data as they are collected.
DESIRABLE AND NOT-SO-DESIRABLE ESTIMATORS
“The method of maximum likelihood is, by far the most popular tech-
nique for deriving estimators” (Casella and Berger, 1990, p. 289). The
proper starting point for the selection of the “best” method of estimation
is with the objectives of our study: What is the purpose of our estimate? If
our estimate is θ* and the actual value of the unknown parameter is θ,
what losses will we be subject to? It is difﬁcult to understand the popular-

ity of the method of maximum likelihood and other estimation procedures
that do not take these losses into consideration.
The majority of losses will be monotone nondecreasing in nature, that
is, the further apart the estimate θ* and the true value θ, the larger our
losses are likely to be. Typical forms of the loss function are the absolute
deviation |θ* −θ|, the squared deviation (θ* −θ)2, and the jump, that is,
no loss if |θ* −θ| < δ and a big loss otherwise. Or the loss function may
resemble the square deviation but take the form of a step function increas-
ing in discrete increments.
Desirable estimators share the following properties: impartiality, consis-
tency, efﬁciency, robustness, and minimum loss.
Impartiality
Estimation methods should be impartial. Decisions should not depend on
the accidental and quite irrelevant labeling of the samples. Nor should
decisions depend on the units in which the measurements are made.
Suppose we have collected data from two samples with the object of
estimating the difference in location of the two populations involved.
Suppose further that the ﬁrst sample includes the values a, b, c, d, and e,
the second sample includes the values f, g, h, i, j, k, and our estimate of
the difference is θ*. If the observations are completely reversed, that is, 
if the ﬁrst sample includes the values f, g, h, i, j, k and the second sample
the values a, b, c, d, and e, our estimation procedure should declare the
difference to be −θ*.
The units we use in our observations should not affect the resulting
estimates. We should be able to take a set of measurements in feet,
convert to inches, make our estimate, convert back to feet, and get
absolutely the same result as if we’d worked in feet throughout. Similarly,
where we locate the zero point of our scale should not affect the 
conclusions.
Finally, if our observations are independent of the time of day, the
season, and the day on which they were recorded (facts that ought to be
veriﬁed before proceeding further), then our estimators should be inde-
pendent of the order in which the observations were collected.
Consistency
Estimators should be consistent, that is, the larger the sample, the greater
the probability that the resultant estimate will be close to the true popula-
tion value.
Efﬁciency
One consistent estimator certainly is to be preferred to another if the ﬁrst
consistent estimator can provide the same degree of accuracy with fewer
48
PART II
HYPOTHESIS TESTING AND ESTIMATION

observations. To simplify comparisons, most statisticians focus on the
asymptotic relative efﬁciency (ARE), deﬁned as the limit with increasing
sample size of the ratio of the number of observations required for each 
of two consistent statistical procedures to achieve the same degree of 
accuracy.
Robustness
Estimators that are perfectly satisfactory for use with symmetric normally
distributed populations may not be as desirable when the data come from
nonsymmetric or heavy-tailed populations, or when there is a substantial
risk of contamination with extreme values.
When estimating measures of central location, one way to create a more
robust estimator is to trim the sample of its minimum and maximum
values (the procedure used when assigning scores in ice-skating or gym-
nastics). On the downside, as information is thrown away, a larger sample
size is required. In many instances, LAD (least absolute deviation) estima-
tors are more robust then their least square counterparts.1 This ﬁnding is
in line with our discussion of the F-statistic in the preceding chapter.
Many semiparametric estimators are not only robust but provide for
high ARE with respect to their parametric counterparts.
As an example of a semiparametric estimator, suppose the {Xi} are inde-
pendent identically distributed (i.i.d.) observations with probability distrib-
ution Pr{Xi ≤x} = F[y −∆] and we want to estimate the location
parameter ∆without having to specify the form of the distribution F. If F
is normal and the loss function is proportional to the square of the estima-
tion error, then the arithmetic mean is optimal for estimating ∆. Suppose,
on the other hand, that F is symmetric but more likely to include very
large or very small values than a normal distribution. Whether the loss
function is proportional to the absolute value or the square of the estima-
tion error, the median, a semiparametric estimator, is to be preferred. The
median has an ARE relative to the mean that ranges from 0.64 (if the
observations really do come from a normal distribution) to values well in
excess of 1 for distributions with higher proportions of very large and very
small values (Lehmann, 1999, p. 242). Still, if the unknown distribution
were “almost” normal, the mean would be far preferable.
If we are uncertain whether F is symmetric, then our best choice is the
Hodges–Lehmann estimator deﬁned as the median of the pairwise 
averages
ˆ
.
∆=
+
(
)
≤
mediani j
j
i
X
X
2
CHAPTER 4
ESTIMATION
49
1 See, for example, Yoo (2001).

Its ARE relative to the mean is 0.97 when F is a normal distribution
(Lehmann, 1999, p. 246). With little to lose with respect to the mean if F
is near normal, and much to gain if F is not, the Hodges–Lehmann esti-
mator is recommended.
Suppose {Xi} and {Yj} are i.i.d. with distributions Pr{Xi ≤x} = F[x] and
Pr{Yj ≤y} = F[y −∆] and we want to estimate the shift parameter ∆
without having to specify the form of the distribution F. For a normal dis-
tribution F, the optimal estimator with least-square losses is
the arithmetic average of the mn differences Yj −Xi. Means are highly
dependent on extreme values; a more robust estimator is given by
Minimum Loss
The value taken by an estimate, its accuracy, that is, the degree to which 
it comes close to the true value of the estimated parameter, and the 
associated losses will vary from sample to sample. A minimum loss 
estimator is one that minimizes the losses when the losses are averaged
over the set of all possible samples. Thus its form depends on all of the
following: the loss function, the population from which the sample is
drawn, and the population characteristic that is being estimated. An 
estimate that is optimal in one situation may only exacerbate losses in
another.
Minimum loss estimators in the case of least-square losses are widely
and well documented for a wide variety of cases. Linear regression with an
LAD loss function is discussed in Chapter 11.
Mini-Max Estimators
It’s easy to envision situations in which we are less concerned with the
average loss than with the maximum possible loss we may incur by using a
particular estimation procedure. An estimate that minimizes the maximum
possible loss is termed a mini-max estimator. Alas, few off-the-shelf mini-
max solutions are available for practical cases, but see Pilz (1991) and
Pinelis (1988).
Other Estimation Criteria
The expected value of an unbiased estimator is the population 
characteristic being estimated. Thus unbiased estimators are also 
consistent estimators.
ˆ
.
∆=
−
(
)
median ij
j
i
X
Y
∆=
−
(
) =
−
∑
∑
1
mn
X
X
j
i
j
i
Y
Y
50
PART II
HYPOTHESIS TESTING AND ESTIMATION

Minimum variance estimators provide relatively consistent results from
sample to sample. Although minimum variance is desirable, it may be of
practical value only if the estimator is also unbiased. For example, 6 is a
minimum variance estimator but offers few other advantages.
Plug-in estimators, in which one substitutes the sample statistic for the
population statistic, the sample mean for the population mean, or the
sample’s 20th percentile for the population’s 20th percentile, are consis-
tent, but they are not always unbiased nor minimum loss.
Always choose an estimator that will minimize losses.
Myth of Maximum Likelihood
The popularity of the maximum likelihood estimator is hard to compre-
hend other than as a vehicle for an instructor to ﬂaunt knowledge of the
calculus. In many cases, this estimator may be completely unrelated to the
loss function. The maximum likelihood estimator has as its sole justiﬁca-
tion that it corresponds to that value of the parameter that makes the
observations most probable—providing, that is, they are drawn from a
speciﬁc predetermined distribution even though the observations might
have resulted from a thousand other a priori possibilities.
A common and lamentable fallacy is that the maximum likelihood esti-
mator has many desirable properties—that it is unbiased and minimizes
the mean squared error. But this is true only for the maximum likelihood
estimator of the mean of a normal distribution.2
Statistics instructors would be well advised to avoid introducing
maximum likelihood estimation and to focus instead on methods for
obtaining minimum loss estimators for a wide variety of loss functions.
INTERVAL ESTIMATES
Point estimates are seldom satisfactory in and of themselves. First, if the
observations are continuous, the probability is zero that a point estimate
will be correct and equal the estimated parameter. Second, we still require
some estimate of the precision of the point estimate.
In this section, we consider one form of interval estimate derived from
bootstrap measures of precision. A second form, derived from tests of
hypotheses, will be considered in Chapter 5.
A common error is to specify a conﬁdence interval in the form (estimate
−k∗standard error, estimate +k∗standard error). This form is applicable
CHAPTER 4
ESTIMATION
51
2 It is also true in some cases for very large samples. How large the sample must be in each
case will depend on both the parameter being estimated and the distribution from which the
observations are drawn.

only when an interval estimate is desired for the mean of a normally dis-
tributed random variable. Even then k should be determined from tables
of the Student’s t-distribution and not from tables of the normal 
distribution.
Nonparametric Bootstrap
The bootstrap can help us obtain an interval estimate for any aspect of a
distribution—a median, a variance, a percentile, or a correlation coefﬁ-
cient—if the observations are independent and all come from distributions
with the same value of the parameter to be estimated. This interval pro-
vides us with an estimate of the precision of the corresponding point 
estimate.
We resample with replacement repeatedly from the original sample, 1000
times or so, computing the sample statistic for each bootstrap sample.
For example, here are the heights of a group of 22 adolescents, mea-
sured in centimeters and ordered from shortest to tallest.
137.0 138.5 140.0 141.0 142.0 143.5 145.0 147.0 148.5 150.0 153.0 154.0
155.0 156.5 157.0 158.0 158.5 159.0 160.5 161.0 162.0 167.5
The median height lies somewhere between 153 and 154 centimeters. If
we want to extend this result to the population, we need an estimate of
the precision of this average.
Our ﬁrst bootstrap sample, arranged in increasing order of magnitude
for ease in reading, might look like this:
138.5 138.5 140.0 141.0 141.0 143.5 145.0 147.0 148.5 150.0 153.0 154.0
155.0 156.5 157.0 158.5 159.0 159.0 159.0 160.5 161.0 162.0
Several of the values have been repeated, not surprising as we are sampling
with replacement, treating the original sample as a stand-in for the much
larger population from which the original sample was drawn. The
minimum of this bootstrap sample is 138.5, higher than that of the origi-
nal sample; the maximum at 162.0 is less than the original, while the
median remains unchanged at 153.5.
137.0 138.5 138.5 141.0 141.0 142.0 143.5 145.0 145.0 147.0 148.5 148.5
150.0 150.0 153.0 155.0 158.0 158.5 160.5 160.5 161.0 167.5
In this second bootstrap sample, again we ﬁnd repeated values; this time
the minimum, maximum, and median are 137.0, 167.5, and 148.5,
respectively.
52
PART II
HYPOTHESIS TESTING AND ESTIMATION

The medians of ﬁfty bootstrapped samples drawn from our sample
ranged between 142.25 and 158.25 with a median of 152.75 (see Fig.
4.1). These numbers provide an insight into what might have been had
we sampled repeatedly from the original population.
We can improve on the interval estimate {142.25, 158.25} if we are
willing to accept a small probability that the interval will fail to include the
true value of the population median. We will take several hundred boot-
strap samples, instead of a mere 50, and use the 5th and 95th percentiles
of the resulting bootstrap distribution to establish the boundaries of a 90%
conﬁdence interval.
This method might be used equally well to obtain an interval estimate
for any other population attribute: the mean and variance, the 5th per-
centile or the 25th, and the interquartile range. When several observations
are made simultaneously on each subject, the bootstrap can be used to
estimate covariances and correlations among the variables. The bootstrap is
particularly valuable when trying to obtain an interval estimate for a ratio
or for the mean and variance of a nonsymmetric distribution.
Unfortunately, such intervals have two deﬁciencies:
1. They are biased, that is, they are more likely to contain certain
false values of the parameter being estimated than the true one
(Efron, 1987).
2. They are wider and less efﬁcient than they could be (Efron,
1987).
Two methods have been proposed to correct these deﬁciencies; let us
consider each in turn.
The ﬁrst method consists of the Hall–Wilson (1991) corrections in
which the bootstrap estimate is Studentized. For the one-sample case, we
want an interval estimate based on the distribution of (θˆ b −θˆ)/sb, where θˆ
and θˆ b are the estimates of the unknown parameter based on the original
and bootstrap sample, respectively, and sb denotes the standard deviation
of the bootstrap sample. An estimate σˆ of the population variance is
required to transform the resultant interval into one about θ (see 
Carpenter and Bithell, 2000).
For the two-sample case, we want a conﬁdence interval based on the
distribution of
CHAPTER 4
ESTIMATION
53
FIGURE 4.1
Scatterplot of 50 Bootstrap Medians Derived from a Sample
of Heights.

where n, m and snb, smb denote the sample sizes and standard deviations,
respectively, of the bootstrap samples. Applying the Hall–Wilson correc-
tions, we obtain narrower interval estimates that are more likely to contain
the true value of the unknown parameter.
The bias-corrected and accelerated BCa interval due to Efron and Tib-
shirani (1986) also represents a substantial improvement, although for
samples under size 30, the interval is still suspect. The idea behind these
intervals comes from the observation that percentile bootstrap intervals are
most accurate when the estimate is symmetrically distributed about the
true value of the parameter and the tails of the estimate’s distribution drop
off rapidly to zero. The symmetric, bell-shaped normal distribution
depicted in Figure 7.1 represents this ideal.
Suppose θ is the parameter we are trying to estimate, θˆ is the estimate,
and we are able to come up with a monotone increasing transformation m
such that m(θ) is normally distributed about m(θˆ). We could use this
normal distribution to obtain an unbiased conﬁdence interval, and then
apply a back-transformation to obtain an almost unbiased conﬁdence 
interval.3
Even with these modiﬁcations, we do not recommend the use of the
nonparametric bootstrap with samples of fewer than 100 observations.
Simulation studies suggest that with small sample sizes the coverage is far
from exact and the end points of the intervals vary widely from one set of
bootstrap samples to the next. For example, Tu and Zhang (1992) report
that with samples of size 50 taken from a normal distribution, the actual
coverage of an interval estimate rated at 90% using the BCα bootstrap is
88%. When the samples are taken from a mixture of two normal distribu-
tions (a not uncommon situation with real-life data sets) the actual cover-
age is 86%. With samples of only 20 in number, the actual coverage is
80%.
More serious when trying to apply the bootstrap is that the end points
of the resulting interval estimates may vary widely from one set of boot-
strap samples to the next. For example, when Tu and Zhang drew samples
of size 50 from a mixture of normal distributions, the average of the left
limit of 1000 bootstrap samples taken from each of 1000 simulated data
ˆ
ˆ
θ
θ
nb
mb
nb
mb
n
s
m
s
n
m
n
m
−
(
)
−
(
)
+
−
(
)
+
−
+
(
)
1
1
2
1
1
2
2
,
54
PART II
HYPOTHESIS TESTING AND ESTIMATION
3 StataTM provides for bias-corrected intervals via its bstrap command. R and S-Plus both
include BCa functions. A SAS macro is available at
http://ftp.sas.com/techsup/download/stat/jackboot.sas.

sets was 0.72 with a standard deviation of 0.16 and the average and stan-
dard deviation of the right limit were 1.37 and 0.30, respectively.
Parametric Bootstrap
Even when we know the form of the population distribution, the use 
of the parametric bootstrap to obtain interval estimates may prove 
advantageous either because the parametric bootstrap provides more 
accurate answers than textbook formulas or because no textbook formulas
exist.
Suppose we know that the observations come from a normal distribu-
tion and want an interval estimate for the standard deviation. We would
draw repeated bootstrap samples from a normal distribution the mean of
which is the sample mean and the variance of which is the sample variance.
As a practical matter, we would draw an element from a N(0,1) popula-
tion, multiply by the sample standard deviation, and then add the sample
mean to obtain an element of our bootstrap sample. By computing the
standard deviation of each bootstrap sample, an interval estimate for the
standard deviation of the population may be derived.
IMPROVED RESULTS
In many instances, we can obtain narrower interval estimates that have a
greater probability of including the true value of the parameter by focus-
ing on sufﬁcient statistics, pivotal statistics, and admissible statistics.
A statistic T is sufﬁcient for a parameter if the conditional distribution of
the observations given this statistic T is independent of the parameter. If
the observations in a sample are exchangeable, then the order statistics of
the sample are sufﬁcient; that is, if we know the order statistics x(1) ≤x(2) ≤
. . . ≤x(n), then we know as much about the unknown population distribu-
tion as we would if we had the original sample in hand. If the observa-
tions are on successive independent binomial trials that end in either
success or failure, then the number of successes is sufﬁcient to estimate the
probability of success. The minimal sufﬁcient statistic that reduces the
observations to the fewest number of discrete values is always preferred.
A pivotal quantity is any function of the observations and the unknown
parameter that has a probability distribution that does not depend on the
parameter. The classic example is Student’s t, whose distribution does not
depend on the population mean or variance when the observations come
from a normal distribution.
A decision procedure d based on a statistic T is admissible with respect
to a given loss function L, providing there does not exist a second proce-
dure d* whose use would result in smaller losses whatever the unknown
population distribution.
CHAPTER 4
ESTIMATION
55

The importance of admissible procedures is illustrated in an expected
way by Stein’s paradox. The sample mean, which plays an invaluable role
as an estimator of the population mean of a normal distribution for a
single set of observations, proves to be inadmissible as an estimator when
we have three or more independent sets of observations to work with.
Speciﬁcally, if {Xij} are independent observations taken from four or more
distinct normal distributions with means θi and variance 1, and losses are
proportional to the square of the estimation error, then the estimators
have smaller expected losses than the individual sample means, regardless
of the actual values of the population means (see Efron and Morris,
1977).
SUMMARY
Desirable estimators are impartial, consistent, efﬁcient, robust, and
minimum loss. Interval estimates are to be preferred to point estimates;
they are less open to challenge for they convey information about the esti-
mate’s precision.
TO LEARN MORE
Selecting more informative end points is the focus of Berger (2002) and
Bland and Altman (1995).
Lehmann and Casella (1998) provide a detailed theory of point 
estimation.
Robust estimators are considered by Huber (1981), Maritz (1996), and
Bickel et al. (1993). Additional examples of both parametric and nonpara-
metric bootstrap estimation procedures may be found in Efron and Tibshi-
rani (1993). Shao and Tu (1995, Section 4.4) provide a more extensive
review of bootstrap estimation methods along with a summary of empiri-
cal comparisons.
Carroll and Ruppert (2000) show how to account for differences in
variances between populations; this is a necessary step if one wants to take
advantage of Stein–James–Efron–Morris estimators.
Bayes estimators are considered in Chapter 6.
ˆ
[
]
(
)
..
.
..
.
..
θi
i
i
i
k
X
k
S
X
X
S
X
X
=
+
−
−
(
)
−
=
−
(
)
=
∑
1
3
2
2
2
1
, where
,
56
PART II
HYPOTHESIS TESTING AND ESTIMATION

Chapter 5
Testing Hypotheses:
Choosing a Test Statistic
CHAPTER 5
TESTING HYPOTHESES: CHOOSING A TEST STATISTIC
57
Common Errors in Statistics (and How to Avoid Them), 2e, by Phillip I. Good and James W. Hardin.
Copyright © 2006 John Wiley & Sons, Inc.
EVERY STATISTICAL PROCEDURE RELIES ON CERTAIN ASSUMPTIONS for 
correctness. Errors in testing hypotheses come about either because the
assumptions underlying the chosen test are not satisﬁed or because the
chosen test is less powerful than other competing procedures. We shall
study each of these lapses in turn.
First, the signiﬁcance level of each test and whether the test is to be
one-sided or two-sided must be determined before the test is performed
and before the data are examined.
Second, virtually all statistical procedures rely on the assumption that
the observations are independent.
Third, virtually all statistical procedures require at least one of the 
following successively weaker assumptions be satisﬁed under the null
hypothesis:
1. The observations are identically distributed.
2. The observations are exchangeable, that is, their joint distribution
is the same for any relabeling.
Forget “large-sample” methods. In the real world of experiments
samples are so nearly always “small” that it is not worth making
any distinction, and small-sample methods are no harder to apply.
George Dyke (1997)
Statistical tests should be chosen before the data are analyzed, and
the choice should be based on the study design and distribution of
the data, not the results.
Cara H. Olsen

3. The observations are drawn from populations in which a speciﬁc
parameter is the same across the populations.
The ﬁrst assumption is the strongest assumption. If it is true, the subse-
quent two assumptions are also true. The ﬁrst assumption must be true
for a parametric test to provide an exact signiﬁcance level. If the second
assumption is true, the third assumption is also true. The second assump-
tion must be true for a permutation test to provide an exact signiﬁcance
level.
The third assumption is the weakest assumption. It must be true for a
bootstrap test to provide an exact signiﬁcance level asymptotically.
An immediate consequence of the ﬁrst two assumptions is that if obser-
vations come from a multiparameter distribution, then all parameters, not
just the one under test, must be the same for all observations under the
null hypothesis. For example, a t-test comparing the means of two popula-
tions requires that the variation of the two populations be the same.
For parametric and parametric bootstrap tests, under the null hypothe-
sis, the observations must all come from a distribution of a speciﬁc form.
Let us now explore the implications of these assumptions in a variety of
practical testing situations including comparing the means of two popula-
tions, comparing the variances of two populations, comparing the means
of three or more populations, and testing for signiﬁcance in two-factor
and higher-order experimental designs.
58
PART II
HYPOTHESIS TESTING AND ESTIMATION
Test Type
Deﬁnition
Example
Exact
Stated signiﬁcance level is exact, 
t-Test when observations 
not approximate.
are i.i.d. normal; 
permutation test when
observations are 
exchangeable
Parametric
Obtains cut-off points from 
t-Test
speciﬁc parametric
distribution.
Semiparametric
Obtains cut-off points from 
Bootstrap
percentiles of bootstrap 
distribution of parameter.
Parametric
Obtains cut-off points from 
Bootstrap
percentiles of parameterized 
bootstrap distribution of 
parameter.
Permutation
Obtains cut-off points from 
Tests may be based on the
distribution of test statistic 
original observations, on 
obtained by rearranging 
ranks, on normal or 
labels.
Savage scores, or on 
U-statistics.
TABLE 5.1 Types of Statistical Tests of Hypotheses

In each instance, before we choose1 a statistic, we check which assump-
tions are satisﬁed, which procedures are most robust to violation of these
assumptions, and which are most powerful for a given signiﬁcance level
and sample size. To ﬁnd the most powerful test, we determine which pro-
cedure requires the smallest sample size for given levels of Type I and
Type II error.
CHAPTER 5
TESTING HYPOTHESES: CHOOSING A TEST STATISTIC
59
1 Whether Republican or Democrat, Liberal or Conservative, male or female, we have the
right to choose, and need not be limited by what textbook, half-remembered teacher pro-
nouncements, or software dictates.
2 Here and throughout this text, we deliberately ignore the many exceptional cases, the
delight of the true mathematician, that one is unlikely to encounter in the real world.
VERIFY THE DATA
The ﬁrst step in any analysis is to verify that the data have been entered
correctly. As noted in Chapter 3: GIGO. A short time ago, a junior biosta-
tistician came into Dr. Good’s ofﬁce asking for help with covariate adjust-
ments for race. “The data for race don’t make sense,” she said. Indeed the
proportions of the various races did seem incorrect. No “adjustment”
could be made. Nor was there any reason to believe that race was the
only variable affected. The ﬁrst and only solution was to do a thorough
examination of the database and, where necessary, trace the data back to
their origins until all the bad data had been replaced with good.
The SAS programmer’s best analysis tool is PROC MEANS. By merely
examining the maximum and minimum values of all variables, it often is
possible to detect data that were entered in error. Some years ago, Dr.
Good found that the minimum value of one essential variable was zero. 
He brought this to the attention of a domain expert who told him a zero
was impossible. As it turns out, the data were full of zeros, the explanation
being that the executive in charge had been faking results. Of the 150
subjects in the database, only 50 were real.
Before you begin any analysis, verify that the data have been entered 
correctly.
COMPARING MEANS OF TWO POPULATIONS
The most common test for comparing the means of two populations is
based on the Student’s t-distribution. For Student’s t-test to provide sig-
niﬁcance levels that are exact rather than approximate, all the observations
must be independent and, under the null hypothesis, all the observations
must come from identical normal distributions.
Even if the distribution is not normal, the signiﬁcance level of the t-test
is almost exact for sample sizes greater than 12; for most of the distribu-
tions one encounters in practice,2 the signiﬁcance level of the t-test is

usually within a percent or so of the correct value for sample sizes between
6 and 12.
There are more powerful tests than the t-test for testing against nonnor-
mal alternatives. For example, a permutation test replacing the original
observations with their normal scores is more powerful than the t-test
(Lehmann, 1975).
Permutation tests are derived by looking at the distribution of values
the test statistic would take for each of the possible assignments of treat-
ments to subjects. For example, if in an experiment two treatments were
assigned at random to six subjects so that three subjects got one treatment
and three the other, there would have been a total of 20 possible assign-
ments of treatments to subjects.3 To determine a p-value, we compute for
the data in hand each of the 20 possible values the test statistic might have
taken. We then compare the actual value of the test statistic with these 20
values. If our test statistic corresponds to the most extreme value, we say
that p = 1/20 = 0.05 (or 1/10 = 0.10 if this is a two-tailed permutation
test).
Against speciﬁc normal alternatives, this two-sample permutation test
provides a most powerful unbiased test of the distribution-free hypothesis
that the centers of the two distributions are the same (Lehmann, 1986, p.
239). For large samples, its power against normal alternatives is almost the
same as Student’s t-test (Albers, Bickel, and van Zwet, 1976). Against
other distributions, by appropriate choice of the test statistic, its power can
be superior (Lambert, 1985; Maritz, 1997).
Options
Alas, more and more individuals seem content to let their software do
their thinking. It won’t.
The t-test is designed for use with continuous measurements; it can be
used to analyze ratings on a Likert scale, but it is not appropriate for use
with counts or survival data.
Your ﬁrst fundamental decision is to decide whether you are doing a
one-tailed or a two-tailed test. If you are testing against a one-sided alter-
native, no difference versus improvement, for example, then you require a
one-tailed or one-sided test. If you are doing a head-to-head compari-
son—which alternative is best?—then a two-tailed test is required.
Note that in a two-tailed test, the tails need not be equal in size but
should be portioned out in accordance with the relative losses associated
with the possible decisions (Moye, 2000, p. 152–157).
60
PART II
HYPOTHESIS TESTING AND ESTIMATION
3 Interested readers may want to verify this for themselves by writing out all the possible
assignments of six items into two groups of three, 1 2 3 / 4 5 6, 1 2 4 / 3 5 6, and so
forth.

You must decide whether your observations are paired (as would be the
case when each individual serves as its own control) or unpaired, and use
the paired or unpaired t-test.
Although your instructor may have spent several lessons devoted to
tables of the normal distribution, in practice, the population variance is
unknown and must be estimated. Thus cut-off values and conﬁdence
limits should be obtained from the Student’s t-distribution with the
appropriate degrees of freedom.
Testing Equivalence
When the logic of a situation calls for demonstration of similarity rather
than differences among responses to various treatments, then equivalence
tests are often more relevant than tests with traditional no-effect null
hypotheses (Anderson and Hauck, 1986; Dixon, 1998; p. 257–301).
Two distributions F and G such that G[x] = F[x −δ] are said to be
equivalent, providing |δ| < ∆, where ∆is the smallest difference of clinical
signiﬁcance. To test for equivalence, we obtain a conﬁdence interval for δ,
rejecting equivalence only if this interval contains values in excess of ∆.
The width of a conﬁdence interval decreases as the sample size increases;
thus a very large sample may be required to demonstrate equivalence just
as a very large sample may be required to demonstrate a clinically signiﬁ-
cant effect.
Unequal Variances
If the variances of the two populations are not the same, neither the t-test
nor the permutation test will yield exact signiﬁcance levels despite pro-
nouncements to the contrary of numerous experts regarding the permuta-
tion tests.
Rule 1: If the underlying distribution is known, make use of it.
Some older textbooks recommend an arcsine transformation when the
data are drawn from a binomial distribution and a square root transforma-
tion when the data are drawn from a Poisson distribution. The resulting p-
values are still only approximations and, in any event, lead to suboptimal
tests.
The optimal test for comparing two binomial outcomes is Fisher’s exact
test and the optimal test for comparing two Poisson outcomes is based on
the binomial (see, for example, Lehmann, 1986, Chapter 4, Section 5).
Rule 2: More important than comparing the means can be determining why
the variances of the populations are different.
There are numerous possible solutions for the Behrens–Fisher problem of
unequal variances in the treatment groups. These include the following:
CHAPTER 5
TESTING HYPOTHESES: CHOOSING A TEST STATISTIC
61

•
Wilcoxon test. The use of the ranks in the combined sample
reduces the impact (though not the entire effect) of the difference
in variability between the two samples.
•
Generalized Wilcoxon test. See O’Brien (1988).
•
Procedure described in Manly and Francis (1999).
•
Procedure described in Chapter 7 of Weerahandi (1995).
•
Procedure described in Chapter 10 of Pesarin (2001).
•
Bootstrap. See the section on dependent observations in what
follows.
•
Permutation test. Phillip Good conducted simulations for sample
sizes between 6 and 12 drawn from normally distributed popula-
tions. The populations in these simulations had variances that dif-
fered by up to a factor of ﬁve, and nominal p-values of 5% were
accurate to within 1.5%.
Hilton (1996) compared the power of the Wilcoxon test, O’Brien’s test,
and the Smirnov test in the presence of both location shift and scale (vari-
ance) alternatives. As the relative inﬂuence of the difference in variances
grows, the O’Brien test is most powerful. The Wilcoxon test loses power
in the face of different variances. If the variance ratio is 4:1, the Wilcoxon
test is not trustworthy.
One point is unequivocal. William Anderson writes,
The ﬁrst issue is to understand why the variances are so different, and what
does this mean to the patient. It may well be the case that a new treatment is
not appropriate because of higher variance, even if the difference in means is
favorable. This issue is important whether or not the difference was antici-
pated. Even if the regulatory agency does not raise the issue, I want to do so
internally.
David Salsburg agrees:
If patients have been assigned at random to the various treatment groups,
the existence of a signiﬁcant difference in any parameter of the distribution
suggests that there is a difference in treatment effect. The problem is not
how to compare the means but how to determine what aspect of this differ-
ence is relevant to the purpose of the study.
Since the variances are signiﬁcantly different, I can think of two situations
where this might occur:
1. In many measurements there are minimum and maximum
values that are possible, e.g., the Hamilton Depression Scale,
or the number of painful joints in arthritis. If one of the treat-
ments is very effective, it will tend to push values into one of
the extremes. This will produce a change in distribution from a
relatively symmetric one to a skewed one, with a correspond-
ing change in variance.
2. The experimental subjects may represent a mixture of popula-
tions. The difference in variance may occur because the effec-
62
PART II
HYPOTHESIS TESTING AND ESTIMATION

tive treatment is effective for only a subset of the population.
A locally most powerful test is given in Conover and Salsburg
(1988).
Dependent Observations
The preceding statistical methods are not applicable if the observations are
interdependent. There are ﬁve cases in which, with some effort, analysis
may still be possible: repeated measures, clusters, known or equal pairwise
dependence, a moving average or autoregressive process,4 and group ran-
domized trials.
Repeated Measures. Repeated measures on a single subject can be dealt
with in a variety of ways including treating them as a single multivariate
observation. Good (2001, Section 5.6) and Pesarin (2001, Chapter 11)
review a variety of permutation tests for use when there are repeated 
measures.
Another alternative is to use one of the standard modeling approaches
such as random- or mixed-effects models or generalized estimating equa-
tions (GEEs). See Chapter 12 for a full discussion.
Clusters. Occasionally, data will have been gathered in clusters from fami-
lies and other groups who share common values, work, or leisure habits. If
stratiﬁcation is not appropriate, treat each cluster as if it were a single
observation, replacing individual values with a summary statistic such as an
arithmetic average (Mosteller and Tukey, 1977).
Cluster-by-cluster means are unlikely to be identically distributed,
having variances, for example, that will depend on the number of individu-
als that make up the cluster. A permutation test based on these means
would not be exact.
If there are a sufﬁciently large number of such clusters in each treatment
group, the bootstrap deﬁned in Chapter 3 is the appropriate method of
analysis.
With the bootstrap, the sample acts as a surrogate for the population.
Each time we draw a pair of bootstrap samples from the original sample,
we compute the difference in means. After drawing a succession of such
samples, we’ll have some idea of what the distribution of the difference in
means would be were we to take repeated pairs of samples from the popu-
lation itself.
As a general rule, resampling should reﬂect the null hypothesis, accord-
ing to Young (1986) and Hall and Wilson (1991). Thus, in contrast to
the bootstrap procedure used in estimation (see Chapter 3), each pair of
bootstrap samples should be drawn from the combined sample taken from
CHAPTER 5
TESTING HYPOTHESES: CHOOSING A TEST STATISTIC
63
4 For a discussion of these latter, see Brockwell and Davis (1987).

the two treatment groups. Under the null hypothesis, this will not affect
the results; under an alternative hypothesis, the two bootstrap sample
means will be closer together than they would if drawn separately from the
two populations. The difference in means between the two samples that
were drawn originally should stand out as an extreme value.
Hall and Wilson (1991) also recommend that the bootstrap be applied
only to statistics that, for very large samples, will have distributions that do
not depend on any unknowns.5 In the present example, Hall and Wilson
(1991) recommend the use of the t-statistic, rather than the simple differ-
ence of means, as leading to a test that is both closer to exact and more
powerful.
Suppose we draw several hundred such bootstrap samples with replace-
ment from the combined sample and compute the t-statistic each time. We
would then compare the original value of the test statistic, Student’s t in
this example, with the resulting bootstrap distribution to determine what
decision to make.
Pairwise Dependence. If the covariances are the same for each pair of
observations, then the permutation test described previously is an exact
test if the observations are normally distributed (Lehmann, 1986) and is
almost exact otherwise.
Even if the covariances are not equal, if the covariance matrix is nonsin-
gular, we may use the inverse of this covariance matrix to transform the
original (dependent) variables to independent (and hence exchangeable)
variables. After this transformation, the assumptions are satisﬁed so that a
permutation test can be applied. This result holds even if the variables are
collinear. Let R denote the rank of the covariance matrix in the singular
case. Then there exists a projection onto an R-dimensional subspace 
where R normal random variables are independent. So if we have an N-
dimensional (N > R) correlated and singular multivariate normal distri-
bution, there exists a set of R linear combinations of the original N
variables so that the R linear combinations are each univariate normal 
and independent.
The preceding is only of theoretical interest unless we have some inde-
pendent source from which to obtain an estimate of the covariance matrix.
If we use the data at hand to estimate the covariances, the estimates will
be interdependent and so will the transformed observations.
Moving Average or Autoregressive Process. These cases are best
treated by the same methods and are subject to the caveats as described in
Part III of this text.
64
PART II
HYPOTHESIS TESTING AND ESTIMATION
5 Such statistics are termed asymptotically pivotal.

Group Randomized Trials.6 Group randomized trials (GRTs) in public
health research typically use a small number of randomized groups with a
relatively large number of participants per group. Typically, some naturally
occurring groups are targeted: work sites, schools, clinics, neighborhoods,
even entire towns or states. A group can be assigned to either the inter-
vention or the control arm but not both; thus the group is nested within
the treatment. This contrasts with the approach used in multicenter clini-
cal trials, in which individuals within groups (treatment centers) may be
assigned to any treatment.
GRTs are characterized by a positive correlation of outcomes within a
group and the small number of groups. “There is positive intraclass corre-
lation (ICC), between the individuals’ target-behavior outcomes within
the same group. This can be due in part to the differences in characteris-
tics between groups, to the interaction between individuals within the
same group, or (in the presence of interventions) to commonalities of the
intervention experienced by an entire group. Although the size of the ICC
in GRTs is usually very small (e.g., in the Working Well Trial, between
0.01–0.03 for the four outcome variables at baseline), its impact on the
design and analysis of GRTs is substantial.”
“The sampling variance for the average responses in a group is (σ2/n) ·
[1 + (n −1)σ], and that for the treatment average with k groups and n
individuals per group is (σ2/n) · [1 + (n −1)σ], not the traditional σ2/n
and σ2/(nk), respectively, for uncorrelated data.”
“The factor 1 + (n −1)σ is called the variance inﬂation factor (VIF), or
design effect. Although σ in GRTs is usually quite small, the VIFs could
still be quite large because VIF is a function of the product of the correla-
tion and group size n.”
“For example, in the Working Well Trial, with σ = 0.03 for daily
number of fruit and vegetable servings, and an average of 250 workers per
work site, VIF = 8.5. In the presence of this deceivingly small ICC, an
8.5-fold increase in the number of participants is required in order to
maintain the same statistical power as if there were no positive correlation.
Ignoring the VIF in the analysis would lead to incorrect results: variance
estimates for group averages that are too small.”
To be appropriate, an analysis method of GRTs must acknowledge both
the ICC and the relatively small number of groups. Three primary
approaches are used:
1. Generalized linear mixed models (GLMM). This approach, imple-
mented in SAS Macro GLIMMIX and SAS PROC MIXED, relies
on an assumption of normality.
CHAPTER 5
TESTING HYPOTHESES: CHOOSING A TEST STATISTIC
65
6 This section has been abstracted (with permission from Annual Reviews) from Feng et al.
(2001), from whom all quotes in this section are taken.

2. Generalized estimating equations (GEE). Again, this approach
assumes asymptotic normality for conducting inference, a good
approximation only when the number of groups is large.
3. Randomization-based inference. Unequal sized groups will result
in unequal variances of treatment means resulting in misleading p-
values. To be fair, “Gail et al. (1996) demonstrate that in GRTs,
the permutation test remains valid (exact or near exact in nominal
levels) under almost all practical situations, including unbalanced
group sizes, as long as the number of groups are equal between
treatment arms or equal within each block if blocking is used.”
The drawbacks of all three methods, including randomization-based
inference if corrections are made for covariates, are the same as those for
other methods of regression as detailed in Chapters 10 and 11.
Nonsystematic Dependence. If the observations are interdependent and
fall into none of the preceding categories, then the experiment is fatally
66
PART II
HYPOTHESIS TESTING AND ESTIMATION
Method 102 βˆ (102 SE)
p-value
pˆ
Fruit/vegetable
GLIM (independent)
−6.9 (2.0)
0.0006
GEE (exchangeable)
−6.8 (2.4)
0.0052
0.0048
GLMM (random intercept)
−6.7 (2.6)
0.023
0.0077
df D 12b
Permutation
−6.1 (3.4)
0.095
t-Test (group-level)
−6.1 (3.4)
0.098
Permutation (residual)
−6.3 (2.9)
0.052
Smoking
GLIM (independent)
−7.8 (12)
0.53
GEE (exchangeable)
−6.2 (20)
0.76
0.0185
GLMM (random intercept)
−13 (21)
0.55
0.020
df D 12b
Permutation
−12 (27)
0.66
t-Test (group-level)
−12 (27)
0.66
Permutation (residual)
−13 (20)
0.53
TABLE 5.2 Comparison of different analysis methods for inference on treatment
effect bˆ a
a Using Seattle 5-a-day data with 26 work sites (K = 13) and an average of 87 (ni ranges
from 47 to 105) participants per work site. The dependent variables are ln (daily servings
of fruit and vegetable C1) and smoking status. The study design is matched pair, with
two cross-sectional surveys at baseline and 2-year follow-up. Pairs identiﬁcation, work
sites nested within treatment, intervention indicator, and baseline worksite mean fruit
and vegetable intake are included in the model. Pairs and work sites are random effects
in GLMM (generalized linear mixed models). We used SAS PROC GENMOD for GLIM
(linear regression and generalized linear models) and GEE (generalized estimating
equations) (logistic model for smoking data) and SAS PROC MIXED (for fruit/vegetable
data) or GLMMIX (logistic regression for smoking data) for GLMM; permutation tests
(logit for smoking data) were programmed in SAS.
b Degrees of freedom (df) = 2245 in SAS output if work site is not deﬁned as being
nested within treatment.
Source: Reprinted (with permission from Annual Reviews) from Feng et al. (2001).

ﬂawed. Your efforts would be best expended on the design of a cleaner
experiment. Or, as J. W. Tukey remarked on more than one occasion, “If
a thing is not worth doing, it is not worth doing well.”
The Analysis of Variance
The principal weakness of the analysis of variance is that the various tests
of signiﬁcance, based on statistics that share a common denominator, are
not independent of one another. Although synchronized permutations do
in theory provide independent tests (see, for example, Good, 2004), soft-
ware to perform those tests is not yet available.
Additional problems arise when one comes to interpret the output of
three-way and four-way designs. Suppose a second- or higher-order inter-
action is statistically signiﬁcant: How is this to be given a practical inter-
pretation? Some authors suggest that one write, “Factor C moderates the
effect of Factor A on Factor B,” as if this phrase actually had discernible
meaning. Among the obvious alternative interpretations of a statistically
signiﬁcant higher-order interaction are the following:
•
An example of a Type I error
•
A defect in the formulation of the additive model; perhaps one
ought to have employed f (X) in place of X or g(X,Y) in place of
X ·Y.
Still, it is clear there are situations in which higher-order interactions
have real meaning. For example, plants require nitrogen, phosphorus, and
potassium in sufﬁcient concentration in order to grow. Remove any one
component and the others will prove inadequate to sustain growth—a
clear-cut example of a second-order interaction.
To avoid ambiguities, one must either treat multifactor experiments
purely as pilot efforts and guides to further experimentation or undertake
such experiments only after one has gained a thorough understanding of
interactions via one- and two-factor experiments. See the discussion in
Chapter 12 on Building a Successful Model.
COMPARING VARIANCES
Testing for the equality of the variances of two populations is a classic
problem with many not-quite-exact, not-quite-robust, not-quite-powerful-
enough solutions. Sukhatme (1958) lists 4 alternative approaches and adds
a ﬁfth of his own; Miller (1968) lists 10 alternatives and compares 4 of
these with a new test of his own; Conover, Johnson, and Johnson (1981)
list and compare 56 tests; and Balakrishnan and Ma (1990) list and
compare 9 tests with one of their own.
CHAPTER 5
TESTING HYPOTHESES: CHOOSING A TEST STATISTIC
67

None of these tests proves satisfactory in all circumstances, for each
requires that two or more of the following four conditions be satisﬁed:
1. The observations are normally distributed.
2. The location parameters of the two distributions are the same or
differ by a known quantity.
3. The two samples are equal in size.
4. The samples are large enough that asymptotic approximations to
the distribution of the test statistic are valid.
As an example, the ﬁrst published solution to this classic testing
problem is the z-test proposed by Welch (1937) based on the ratio of the
two sample variances. If the observations are normally distributed, this
ratio has the F-distribution, and the test whose critical values are deter-
mined by the F-distribution is uniformly most powerful among all unbi-
ased tests (Lehmann, 1986, section 5.3). But with even small deviations
from normality, signiﬁcance levels based on the F-distribution are grossly
in error (Lehmann, 1986, section 5.4).
Box and Anderson (1955) propose a correction to the F-distribution for
“almost” normal data, based on an asymptotic approximation to the per-
mutation distribution of the F-ratio. Not surprisingly, their approximation
is close to correct only for normally distributed data or for very large
samples. The Box–Anderson statistic results in an error rate of 21%, twice
the desired value of 10%, when two samples of size 15 are drawn from a
gamma distribution with four degrees of freedom.
A more recent permutation test (Bailor, 1989) based on complete 
enumeration of the permutation distribution of the sample F-ratio is exact
only when the location parameters of the two distributions are known or
are known to be equal.
The test proposed by Miller (1968) yields conservative Type I errors,
less than or equal to the declared error, unless the sample sizes are
unequal. A 10% test with samples of sizes 12 and 8 taken from normal
populations yielded Type I errors 14% of the time.
Fligner and Killeen (1976) propose a permutation test based on the
sum of the absolute deviations from the combined sample mean. Their
test may be appropriate when the medians of the two populations are
equal but can be virtually worthless otherwise, accepting the null hypothe-
sis up to 100% of the time. A proposed test based on permutations of the
absolute deviations from the individual sample medians is only asymptoti-
cally exact and even then only for approximately equal sample sizes, as
shown by Baker (1995).
To compute the primitive bootstrap introduced by Efron (1979), we
would take successive pairs of samples—one of n observations from the
sampling distribution Fn, which assigns mass 1/n to the values {Xi: i = 1,
68
PART II
HYPOTHESIS TESTING AND ESTIMATION

. . . n}, and one of m observations from the sampling distribution Gm,
which assigns mass 1/m to the values {Xj: j = n + 1, . . . n + m}, and
compute the ratio of the sample variances
We would use the resultant bootstrap distribution to test the hypothesis
that the variance of F equals the variance of G against the alternative that
the variance of G is larger. Under this test, we reject the null hypothesis if
the 100(1 −α) percentile is less than 1.
This primitive bootstrap and the associated conﬁdence intervals are close
to exact only for very large samples with hundreds of observations. More
often, the true coverage probability is larger than the desired value.
Two corrections yield vastly improved results. First, for unequal-sized
samples, Efron (1982) suggests that more accurate conﬁdence intervals
can be obtained using the test statistic
Second, applying the bias and acceleration corrections described in
Chapter 3 to the bootstrap distribution of R′ yields almost exact intervals.
Lest we keep you in suspense, a distribution-free exact and more power-
ful test for comparing variances can be derived based on the permutation
distribution of Aly’s statistic.
This statistic, proposed by Aly (1990), is
where X(1) ≤X(2) ≤. . . ≤X(m) are the order statistics of the ﬁrst sample.
Suppose we have two sets of measurements; 121, 123, 126, 128.5, 129
and, in a second sample, 153, 154, 155, 156, 158. We replace these with
the deviations z1i = X(i+1) −X(i) or 2, 3, 2.5, .5 for the ﬁrst sample and 
z2i = 1, 1, 1, 2 for the second.
The original value of the test statistic, is 8 + 18 + 15 + 2 = 43. Under
the hypothesis of equal dispersions in the two populations, we can
exchange labels between z1i and z2i for any or all of the values of i. One
possible rearrangement of the labels on the deviations puts {2, 1, 1, 2} in
the ﬁrst sample, which yields a value of 8 + 6 + 6 + 8 = 28.
There are 24 = 16 rearrangements of the labels in all, of which only one
{2, 3, 2.5, 2} yields a larger value of Aly’s statistic than the original obser-
vations. A one-sided test would have 2 of 16 rearrangements as or more
δ =
−
(
)
−
(
)
=1
−
+
(
)
( )
∑i m
i X
X
i
m
i
i
1
1
′ =
R
s
n
s
m
n
m
2
2
R
s
n
s
m
n
m
=
−
(
)
−
(
)
2
2
1
1
.
CHAPTER 5
TESTING HYPOTHESES: CHOOSING A TEST STATISTIC
69

extreme than the original; a two-sided test would have 4. In either case,
we would accept the null hypothesis, although the wiser course would be
to defer judgment until we have taken more observations.
If our second sample is larger than the ﬁrst, we have to resample in two
stages. First, we select a subset of m values at random without replacement
from the n observations in the second, larger sample and compute the
order statistics and their differences. Last, we examine all possible values of
Aly’s measure of dispersion for permutations of the combined sample as
we did when the two samples were equal in size and compare Aly’s
measure for the original observations with this distribution. We repeat this
procedure several times to check for consistency.
70
PART II
HYPOTHESIS TESTING AND ESTIMATION
MATCH SIGNIFICANCE LEVELS BEFORE PERFORMING 
POWER COMPARISONS.
When we studied the small-sample properties of parametric tests based on
asymptotic approximations that had performed well in previously pub-
lished power comparisons, we uncovered another major error in statistics:
the failure to match signiﬁcance levels before performing power compari-
sons. Asymptotic approximations to cutoff values were used rather than
exact values or near estimates.
When a statistical test takes the form of an interval, that is, if we reject
when S < c and accept otherwise, then power is a non-decreasing function
of signiﬁcance level; a test based on an interval may have greater power at
the 10% signiﬁcance level than a second different test evaluated at the 5%
signiﬁcance level, even though the second test is uniformly more powerful
than the ﬁrst. To see this, let H denote the primary hypothesis and K an
alternative hypothesis:
If Pr{S < c|H} = α < α′ = Pr{S < c′|H), then c < c′, and β = Pr{S < c|K} ≤
Pr{S < c′|K} = β′.
Consider a second statistical test depending on S via the monotone
increasing function h, where we reject if h[S] < d and accept otherwise. If
the cutoff values d < d′ correspond to the same signiﬁcance levels α < α′,
then β < Pr{h[S] < d|K} < β′. Even though the second test is more powerful
than the ﬁrst at level α, this will not be apparent if we substitute an
approximate cutoff point c′ for an exact one c when comparing the two
tests.
To ensure matched signiﬁcance levels in your own power comparisons,
proceed in two stages: First, use simulations to derive exact cutoff values.
Then use these derived cutoff values in determining power. Using this
approach, we were able to show that an exact permutation test based on
Aly’s statistic was more powerful for comparing variances than any of the
numerous published inexact parametric tests.

COMPARING THE MEANS OF K SAMPLES
Although the traditional one-way analysis of variance based on the 
F-ratio
is highly robust, it has four major limitations:
1. Its signiﬁcance level is dependent on the assumption of normality.
Problems occur when data are drawn from distributions that are
highly skewed or heavy in the tails. Still, the F-ratio test is
remarkably robust to minor deviations from normality.
2. Not surprisingly, lack of normality also affects the power of the
test, rendering it suboptimal.
3. The F-ratio is optimal for losses that are proportional to the
square of the error and is suboptimal otherwise.
4. The F-ratio is an omnibus statistic offering all-round power
against many alternatives but no particular advantage against any
speciﬁc one of them. For example, it is suboptimal for testing
against an ordered dose response when a test based on the correla-
tion would be preferable.
Normality is a myth; there never has, and never will be a normal distribution.
Geary (1947, p. 241)
A permutation test is preferred for the k-sample analysis. These tests are
distribution free (though the variances must be the same for all 
treatments). They are as powerful or more powerful than the analysis of
variance; they are more powerful when the design is unbalanced (Good
and Lunneborg, 2006), as it so often is as a result of missing data. And
you can choose the test statistic that is optimal for a given alternative and
loss function and not be limited by the availability of tables.
We take as our model Xij = αi + εjj where i = 1, . . . I denotes the 
treatment and j = 1, . . . , ni. We assume that the error terms {εjj} are 
independent and identically distributed.
We consider two loss functions: one in which the losses associated with
overlooking a real treatment effect, a Type II error, are proportional to
the sum of the squares of the treatment effects αi
2 (LS) and the other in
which the losses are proportional to the sum of the absolute values of the
treatment effects, |αi| (LAD).
Our hypothesis, a null hypothesis, is that the differential treatment
effects, the {αi}, are all zero. We will also consider two alternative hypothe-
ses: KU that at least one of the differential treatment effects αi is not zero
and KO that exactly one of the differential treatment effects αi is not zero.
n X
X
I
X
X
N
L
i
i
i
I
ij
i
j
n
i
I
i
.
..
.
−
(
)
−
(
)
−
(
)
−
(
)
=
=
=
∑
∑
∑
2
1
2
1
1
1
CHAPTER 5
TESTING HYPOTHESES: CHOOSING A TEST STATISTIC
71

For testing against KU with the LS loss function, Good (2002, p. 126) 
recommends the use of the statistic 
, which is equivalent 
to the F-ratio once terms that are invariant under permutations are 
eliminated.
We compared the parametric and permutation versions of this test when
the data were drawn from a mixture of normal distributions. The 
difference in power between the two tests is exacerbated when the design
is unbalanced. For example, the following experiment was simulated 4000
times:
•
A sample of size 3 was taken from a mixture of 70%N(0,1) and
30%N(1,1).
•
A sample of size 4 was taken from a mixture of 70%N(0.5,1) and
30%N(1.5,1.5).
•
A sample of size 5 was taken from a mixture of 70%N(1,1) and
30%N(2,2).
Note that such mixtures are extremely common in experimental work. The
parametric test in which the F-ratio is compared with an F-distribution
had a power of 18%. The permutation test in which the F-ratio is 
compared with a permutation- distribution had a power of 31%. For testing
against KU with the LAD loss function, Good (2002, p. 126) recommends 
the use of the statistic 
.
For testing against KO, ﬁrst denote by X¯i the mean of the ith sample
and by X¯i the mean of all observations excluding the ith observation. 
A possible test statistic would be the maximum of the differences 
|X¯i −X¯i|.
A permutation test based on the original observations is appropriate
only if one can assume that under the null hypothesis the observations are
identically distributed in each of the populations from which the samples
are drawn. If we cannot make this assumption, we will need to transform
the observations, throwing away some of the information about them so
that the distributions of the transformed observations are identical.
For example, for testing against KO, Lehmann (1999, p. 372) 
recommends the use of the Jonckheere–Terpstra statistic, the number of
pairs in which an observation from one group is less than an observation
from a higher-dose group. The penalty we pay for using this statistic and
ignoring the actual values of the observations is a marked reduction in
power for small samples and a less pronounced loss for larger ones.
If there are just two samples, the test based on the Jonckheere–Terpstra
statistic is identical with the Mann–Whitney test. For very large samples,
with identically distributed observations in both samples, 100 observations
would be needed with this test to obtain the same power as a permutation
F
Xij
j
i
1 =
∑
∑
F
X ij
j
i
2
2
=
(
)
∑
∑
72
PART II
HYPOTHESIS TESTING AND ESTIMATION

test based on the original values of 95 observations. This is not a price
one would want to pay in human or animal experiments.
Subjective Data
Student’s t-test and the analysis of variance are based on mathematics that
require the dependent variable to be measured on an interval or ratio scale
so that its values can be meaningfully added and subtracted. But what
does it mean if one subtracts the subjective data value “Indifferent” from
the subjective data value “Highly preferable.” The mere fact that we have
entered the data into the computer on a Likert scale, such as a “1” for
“Highly preferable” and a “3” for “Indifferent” does not actually endow
our preferences with those relative numeric values.
Unfortunately, the computer thinks it does, and if asked to compute a
mean preference it will add the numbers it has stored and divide by the
sample size. It will even compute a t-statistic and a p-value if such is
requested. But this does not mean that either is meaningful.
Of course, you are welcome to ascribe numeric values to subjective data,
providing that you spell out exactly what you have done and realize that
the values you ascribe may be quite different from the ones that I or some
other investigator might attribute to precisely the same data.
Independence Versus Correlation
Too often we treat a test of the correlation between two variables X and 
Y as if it were a test of their independence. But X and Y can have a zero
correlation coefﬁcient, yet be totally dependent.
For example, even when the expected value of Y is independent of the
expected value of X, the variance of Y might be directly proportional to
the variance of X. Of course, if we’d plotted the data, we’d have spotted
this right away.
Many variables exhibit circadian rhythms. Yet the correlation of such a
variable with time when measured over the course of 24 hours would be
zero. This is because correlation really means “linear correlation” and the
behavior of diurnal rhythm is far from linear. Of course, this too would
have been obvious had we drawn a graph rather than letting the computer
do the thinking for us.
Yet another, not uncommon, example would be when X is responsible
for the size of a change in Y, but a third variable, not part of the study,
determines the direction of the change.
HIGHER-ORDER EXPERIMENTAL DESIGNS
Recent simulations (Good, 2006) reveal that the parametric analysis of
variance is remarkably robust, particularly when there are 3 or more rows,
CHAPTER 5
TESTING HYPOTHESES: CHOOSING A TEST STATISTIC
73

3 or more columns, and 3 or more observations per cell in the design.
The ANOV tests for main effects and interactions are almost exact and
almost most powerful even when the data is drawn from mixed normal
populations or Weibull distributions (as is the case with time-to-event data.)
As with the k-sample comparison, it should be remembered that the
tests for main effects in the analysis of variance are omnibus statistics offer-
ing all-round power against many alternatives but no particular advantage
against any speciﬁc one of them. Judicious use of contrasts can provide
more powerful tests. For example, one can obtain a one-sided test of the
row effect in a 2xCx . . . design by testing the contrast X¯1. . . −X¯2. . . or a
test of an ordered row effect in an RxCx . . . design by testing the contrast
ΣjajX¯j. . . where Σjaj = 0 and the aj are increasing in j. Note: These contrasts
must be speciﬁed in advance of examining the data. Otherwise, there will be
a loss of power due to the need to correct for multiple tests.
Two addition caveats apply to the parametric ANOVA approach to the
analysis of the two-factor experimental design:
1. The sample sizes must be the same in each cell; that is, the design
must be balanced.
2. A test for interaction must precede any test for main effects.
Alas, these same caveats apply to the permutation tests. Let us see why.
Imbalance in the design will result in the confounding of main effects
with interactions. Consider the following two-factor model for crop yield:
Now suppose that the observations in a two-factor experimental design are
normally distributed as in the following diagram taken from Cornﬁeld and
Tukey (1956).
There are no main effects in this example—both row means and both
column means have the same expectations—but there is a clear interaction
represented by the two nonzero off-diagonal elements.
If the design is balanced, with equal numbers per cell, the lack of 
signiﬁcant main effects and the presence of a signiﬁcant interaction should
and will be conﬁrmed by our analysis. But suppose that the design is not
in balance, that for every 10 observations in the ﬁrst column, we have
only 1 observation in the second. Because of this imbalance, when we use
the F-ratio or equivalent statistic to test for the main effect, we will
N
N
N
N
0 1
2 1
2 1
0 1
,
,
,
,
(
)
(
)
(
)
(
)
Xijk
i
j
ij
ijk
=
+
+
+
+
µ
α
β
γ
ε .
74
PART II
HYPOTHESIS TESTING AND ESTIMATION

uncover a false “row” effect that is actually due to the interaction between
rows and columns. The main effect is confounded with the interaction.
If a design is unbalanced as in the preceding example, we cannot test
for a “pure” main effect or a “pure” interaction. But we may be able to
test for the combination of a main effect with an interaction by using the
statistic that we would use to test for the main effect alone. This 
combined effect will not be confounded with the main effects of other
unrelated factors.
Whether or not the design is balanced, the presence of an interaction
may zero out a cofactor-speciﬁc main effect or make such an effect 
impossible to detect. More important, the presence of a signiﬁcant interac-
tion may render the concept of a single “main effect” meaningless. For
example, suppose we decide to test the effect of fertilizer and sunlight on
plant growth. With too little sunlight, a fertilizer would be completely
ineffective. Its effects only appear when sufﬁcient sunlight is present.
Aspirin and warfarin can both reduce the likelihood of repeated heart
attacks when used alone; you don’t want to mix them!
Gunter Hartel offers the following example: Using ﬁve observations per
cell and random normals as indicated in Cornﬁeld and Tukey’s diagram, a
two-way ANOVA without interaction yields the following results:
Source
df
Sum of Squares
F-Ratio
Prob > F
Row
1
0.15590273
0.0594
0.8104
Col
1
0.10862944
0.0414
0.8412
Error
17
44.639303
Adding the interaction term yields:
Source
df
Sum of Squares
F-Ratio
Prob > F
Row
1
0.155903
0.1012
0.7545
Col
1
0.108629
0.0705
0.7940
Row*col
1
19.986020
12.9709
0.0024
Error
16
24.653283
Expanding the ﬁrst row of the experiment to have 80 observations
rather than 10, the main effects-only table becomes:
Source
df
Sum of Squares
F-Ratio
Prob > F
Row
1
0.080246
0.0510
0.8218
Col
1
57.028458
36.2522
<0.0001
Error
88
138.43327
CHAPTER 5
TESTING HYPOTHESES: CHOOSING A TEST STATISTIC
75

But with the interaction term it is:
Source
df
Sum of Squares
F-Ratio
Prob > F
Row
1
0.075881
0.0627
0.8029
Col
1
0.053909
0.0445
0.8333
Row*col
1
33.145790
27.3887
<0.0001
Error
87
105.28747
We must also recognize that the various effect tests provided by the
analysis of variance are positively correlated as they share a common
denominator, the within-cell variance. Thus a real non-zero main effect
may be obscured by the presence of a spuriously signiﬁcant interaction.
The permutation tests for main effects and interactions in a multi-factor
design are also correlated as the residuals in a two-way complete experi-
mental design are not exchangeable even if the design is balanced
(Lehmann and D’Abrera, 1988). To see this, suppose our model is Xijk =
µ + αi + βj + γij + εijk, where
Eliminating the main effects in the traditional manner, that is, setting
X′ijk = Xijk −X¯i.. −X¯.j. + X¯ . . . , one obtains the test statistic
ﬁrst derived by Still and White (1981). A permutation test based on 
the statistic I will not be exact, for even if the error terms {εijk} are
exchangeable, the residuals X′ijk = εijk −ε¯i.. −ε¯.j. + ε¯ . . . are weakly 
correlated, the correlation depending on the subscripts.
The negative correlation between permutation test statistics works to
the advantage of the permutation test when there is a single effect present.
The analysis of variance is to be preferred if there are multiple effects.
This result also extends to those permutation tests based on the ranks of
the observations, e.g., the Kruskall-Wallace test, that may be found in
many statistics software packages.
Synchronized Permutations
The recent efforts of Salmaso (2002) and Pesarin (2001) have resulted in
a breakthrough that extends to higher-order designs. The key lies in the
concept of weak exchangeability with respect to a subset of the possible
permutations. The simpliﬁed discussion of weak exchangeability presented
here is abstracted from Good (2002).
Think of the set of observations {Xijk} in terms of a rectangular lattice L
with K colored, shaped balls at each vertex. All the balls in the same
I
X ijk
k
j
i
=
′
(
)
∑
∑
∑
2
α
β
γ
γ
i
j
ij
i
ij
j
∑
∑
∑
∑
=
=
=
= 0
76
PART II
HYPOTHESIS TESTING AND ESTIMATION

column have the same color initially, a color that is distinct from the color
of the balls in any other column. All the balls in the same row have the
same shape initially, a shape that is distinct from the shape of the balls in
any other row. See Fig. 5.1.
Let P denote the set of rearrangements or permutations that preserve
the number of balls at each row and column of the lattice. P is a group.7
Let PR denote the set of exchanges of balls among rows and within
columns that (a) preserve the number of balls at each row and column of
the lattice and (b) result in the numbers of each shape within each row
being the same in each column. PR is the basis of a subgroup of P. See
Fig. 5.2.
Let PC denote the set of exchanges of balls among columns and within
rows that (a) preserve the number of balls at each row and column of the
lattice and (b) result in the numbers of each color within each column
being the same in each row. PC is the basis of a subgroup of P. See Fig.
5.3.
Let PRC denote the set of exchanges of balls that preserve the number of
balls at each row and column of the lattice and result in (a) an exchange
of balls between both rows and columns (or no exchange at all), (b) the
numbers of each color within each column being the same in each row,
CHAPTER 5
TESTING HYPOTHESES: CHOOSING A TEST STATISTIC
77
FIGURE 5.1
A 2 ¥ 3 Design with Three Observations per Cell.
FIGURE 5.2
A 2 ¥ 3 Design with Three Observations per Cell after p Œ
PR.
7 See Hungerford (1974) or http://members.tripod.com/~dogschool/ for a thorough 
discussion of algebraic group properties.

(c) the numbers of each shape within each row being the same in each
column. PRC is the basis of a subgroup of P.
The only element these three subgroups PRC, PR, and PC have in
common is the rearrangement that leaves the observations with the same
row and column labels they had to begin with. As a result, tests based on
these three different subsets of permutations are independent of one
another.
For testing H3: γij = 0 for all i and j, determine the distribution of the
values of S = ΣiΣj(Σkxijk)2 with respect to the rearrangements in PRC. If the
value of S for the observations as they were originally labeled is not an
extreme value of this permutation distribution, then we can accept the
hypothesis H3 of no interactions and proceed to test for main effects.
For testing H1: αi = 0 for all i, choose one of the following test statistics
as we did in the section on one-way analysis, F12 = Σi(ΣjΣkxijk)2, F11 =
Σi|ΣjΣkxijk|, or R1 = Σig[i]ΣjΣkxijk, where g[i] is a monotone function of i,
and determine the distribution of its values with respect to the 
rearrangements in PR.
For testing H2: βj = 0 for all j, choose one of the following test statistics
as we did in the section on one-way analysis, F22 = Σj(ΣiΣkxijk)2, F21 =
Σj|ΣiΣkxijk|, or R2 = Σjg[j]ΣiΣkxijk, where g[j] is a monotone function of j,
and determine the distribution of its values with respect to the 
rearrangements in PC.
Tests for the parameters of three-way and higher-order experimental
designs can be obtained via the same approach; use a multidimensional
lattice and such additional multivalued properties of the balls as charm and
spin. A general exact solution for unbalanced factorial designs is described
in Pesarin (2001, p. 237).
Unbalanced Designs
Unbalanced designs with unequal numbers per cell may result from 
unanticipated losses during the conduct of an experiment or survey (or
from an extremely poor initial design). There are two approaches to their
analysis.
78
PART II
HYPOTHESIS TESTING AND ESTIMATION
FIGURE 5.3
A 2 ¥ 3 Design with Three Observations per Cell after p Œ
PC.

First, if we have a large number of observations and only a small
number are missing, we might consider imputing values to the missing
observations, recognizing that the results may be somewhat tainted.
Second, we might bootstrap along one of the following lines:
•
If only one or two observations are missing, create a balanced
design by discarding observations at random; repeat to obtain a
distribution of p-values (Baker, 1995).
•
If there are actual holes in the design, so that there are missing
combinations, create a test statistic that does not require the
missing data. Obtain its distribution by bootstrap means. See
Good (2000, p. 68–70) for an example.
CONTINGENCY TABLES
A major source of error in the analysis of contingency tables is associating
the Pearson chi-square statistic, a quite useful measure of the difference
between observed and expected values, with the chi-square distribution.
The latter is the distribution of Z 2, where Z has the normal distribution.
Just as the means of very large samples have almost normal distributions,
so the means of very large numbers of squared values tend to almost 
chi-square distributions. Pearson’s chi-square statistic is no exception to
the rule. If the probabilities of an observation falling in a particular cell of
a contingency table are roughly the same for all rows and columns, then
convergence of the chi-square distribution can be quite rapid. But for
sparse tables, the chi-square distribution can be quite misleading 
(Delucchi, 1983).
We recommend using an exact permutation procedure, particularly now
that software for a variety of testing situations is commercially and freely
available.8 As in Fisher (1935), we determine the proportion of tables 
with the same marginals that are as extreme or more extreme than our
original table.
The problem lies in deﬁning what is meant by “extreme.” The errors lie
in failing to report how we arrived at our deﬁnition.
For example, in obtaining a two-tailed test for independence in a 2 × 2
contingency table, we can treat each table strictly in accordance with its
probability under the multinomial distribution (Fisher’s method) or
weight each table by the value of the Pearson chi-square statistic for that
table. The situation is even more complicated with general R × C tables
where a dozen different statistics compete for our attention.
The chief errors in practice lie in failing to report all of the following:
CHAPTER 5
TESTING HYPOTHESES: CHOOSING A TEST STATISTIC
79
8 Examples include StatXact® from http://www.cytel.com, RT from www.west-inc.com,
NPC Test from www.methodologica.it., and R (freeware) from http://www.r-project.org/.

•
Whether we used a one-tailed or two-tailed test and why
•
Whether the categories are ordered or unordered
•
Which statistic was employed and why
Chapter 10 contains a discussion of a ﬁnal, not inconsiderable, source of
error, the neglect of confounding variables that may be responsible for
creating an illusory association or concealing an association that actually
exists.
INFERIOR TESTS
Violation of assumptions can affect not only the signiﬁcance level of a test
but the power of the test, as well; see Tukey and MacLaughlin (1963) and
Box and Tiao (1964). For example, although the signiﬁcance level of the
t-test is robust to departures from normality, the power of the t-test is
not. Thus the two-sample permutation test may always be preferable.
If blocking including matched pairs was used in the original design,
then the same division into blocks should be employed in the analysis.
Confounding factors such as sex, race, and diabetic condition can easily
mask the effect we hoped to measure through the comparison of two
samples. Similarly, an overall risk factor can be totally misleading 
(Gigerenzer, 2002). Blocking reduces the differences between subjects so
that differences between treatment groups stand out, if, that is, the appro-
priate analysis is used. Thus paired data should always be analyzed with
the paired t-test or its permutation equivalent, not with the group t-test.
To analyze a block design, (for example, where we have sampled 
separately from whites, blacks, and Hispanics), the permutation test 
statistic is 
, where xbj is the jth observation in the control 
sample in the bth block, and the rearranging of labels between control and
treated samples takes place separately and independently within each of the
B blocks (Good, 2001, p. 124).
Blocking can also be used after the fact if you suspect the existence of
confounding variables and if you measured the values of these variables as
you were gathering data.9
Always be sure your choice of statistic is optimal against the alternative
hypotheses of interest for the appropriate loss function.
To avoid using an inferior, less sensitive, and possibly inaccurate statisti-
cal procedure, pay heed to another admonition from George Dyke
(1997): “The availability of ‘user-friendly’ statistical software has caused
authors to become increasingly careless about the logic of interpreting
S
xbj
j
b
B
=
∑
∑=1
80
PART II
HYPOTHESIS TESTING AND ESTIMATION
9 This recommendation applies only to a test of efﬁcacy for all groups (blocks) combined. 
p-Values for subgroup analyses performed after the fact are still suspect; see Chapter 1.

their results, and to rely uncritically on computer output, often using the
‘default option’ when something a little different (usually, but not always,
a little more complicated) is correct, or at least more appropriate.”
MULTIPLE TESTS
When we perform multiple tests in a study, there may not be journal room
(or interest) to report all the results, but we do need to report the total
number of statistical tests performed so that readers can draw their own
conclusions as to the signiﬁcance of the results that are reported.
We may also wish to correct the reported signiﬁcance levels by using
one of the standard correction methods for independent tests (e.g., 
Bonferroni as described in Hsu, 1996; for resampling methods, see 
Westfall and Young, 1993). But see also Saville (1990, 2003).
Several statistical packages—SAS is a particular offender—print out the
results of several dependent tests performed on the same set of data, for
example, the t-test and the Wilcoxon. We are not free to pick and choose.
We must decide before we view the printout which test we will employ.
Let Wα denote the event that the Wilcoxon test rejects a hypothesis at
the α signiﬁcance level. Let Pα denote the event that a permutation test
based on the original observations and applied to the same set of data
rejects a hypothesis at the α signiﬁcance level. Let Tα denote the event
that a t-test applied to the same set of data rejects a hypothesis at the α
signiﬁcance level.
It is possible that Wα may be true when Pα and Tα are not, and so forth.
As Pr{Wα or Pα or Tα|H} ≤Pr{Wα|H} = α, we will have inﬂated the Type 
I error by picking and choosing after the fact which test to report. Vice
versa, if our intent was to conceal a side effect by reporting that the
results were not signiﬁcant, we will inﬂate the Type II error and deﬂate
the power β of our test, by an after-the-fact choice as β = Pr {not (Wα and
Pα and Tα)|K} ≤Pr{Wα|K}.
To repeat, we are not free to pick and choose among tests; any such
conduct is unethical. Both the comparison and the test statistic must
be speciﬁed in advance of examining the data.
BEFORE YOU DRAW CONCLUSIONS
Insigniﬁcance
If the p-value you observe is greater than your predetermined signiﬁcance
level, this may mean any or all of the following:
1. You’ve measured the wrong thing, gone about measuring it the
wrong way, or used an inappropriate test statistic.
CHAPTER 5
TESTING HYPOTHESES: CHOOSING A TEST STATISTIC
81

2. Your sample size was too small to detect an effect.
3. The effect you are trying to detect is not statistically signiﬁcant.
Practical vs. Statistical Signiﬁcance
If the p-value you observe is less than your predetermined signiﬁcance
level, it does not necessarily mean the effect you’ve detected is of practical
signiﬁcance—see, for example, the section on measuring equivalence. For
this reason, as we discuss in Chapter 7, it is essential that you follow up
any signiﬁcant result by computing a conﬁdence interval, so readers can
judge for themselves whether the effect you’ve detected is of practical 
signiﬁcance.
And do not forget that at the α-percent signiﬁcance level, α percent of
your tests will be statistically signiﬁcant by chance alone.
Missing Data
Before you draw conclusions, be sure you have accounted for all missing
data, interviewed nonresponders, and determined whether the data were
missing at random or were speciﬁc to one or more subgroups.
During the Second World War, a group was studying planes returning
from bombing Germany. They drew a rough diagram showing where the
bullet holes were and recommended that those areas be reinforced. A 
statistician, Abraham Wald (1980),10 pointed out that essential data were
missing from the sample they were studying: What about the planes that
didn’t return from Germany?
When we think along these lines, we see that the two areas of the plane
that had almost no bullet holes (where the wings and the tail joined the
fuselage) are crucial. Bullet holes in a plane are likely to be at random,
occurring over the entire plane. Their absence in those two areas in
returning bombers was diagnostic. Do the data missing from your 
experiments and surveys also have a story to tell?
Induction
Behold! human beings living in an underground den, which has a mouth open
towards the light and reaching all along the den; here they have been from
their childhood, and have their legs and necks chained so that they cannot
move, and can only see before them, being prevented by the chains from
turning round their heads. Above and behind them a ﬁre is blazing at a 
distance, and between the ﬁre and the prisoners there is a raised way; and
you will see, if you look, a low wall built along the way, like the screen which
marionette players have in front of them, over which they show the puppets.
82
PART II
HYPOTHESIS TESTING AND ESTIMATION
10 This reference may be hard to obtain. Alternatively, see Mangel and Samaniego (1984).

And they see only their own shadows, or the shadows of one another, which
the ﬁre throws on the opposite wall of the cave.
To them, I said, the truth would be literally nothing but the shadows of the
images.
The Allegory of the Cave (Plato, The Republic, Book VII).
Never assign probabilities to the true state of nature, but only to the 
validity of your own predictions.
A p-value does not tell us the probability that a hypothesis is true, nor
does a signiﬁcance level apply to any speciﬁc sample; the latter is a 
characteristic of our testing in the long run. Likewise, if all assumptions
are satisﬁed, a conﬁdence interval will in the long run contain the true
value of the parameter a certain percentage of the time. But we cannot 
say with certainty in any speciﬁc case that the parameter does or does not
belong to that interval (Neyman, 1961, 1977).
When we determine a p-value, we apply a set of algebraic methods and
deductive logic to deduce the correct value. The deductive process is used
to determine the appropriate size of resistor to use in an electric circuit, to
determine the date of the next eclipse of the moon, and to establish the
identity of the criminal (perhaps from the fact that a dog did not bark on
the night of the crime). Find the formula, plug in the values, turn the
crank and out pops the result (or it does for Sherlock Holmes11, at least).
When we assert that for a given population a percentage of samples will
have a speciﬁc composition, this is a deduction also. But when we make an
inductive generalization about a population based upon our analysis of a
sample we are on shakier ground. Newton’s law of gravitation provided an
exact ﬁt to observed astronomical data for several centuries; consequently,
there was general agreement that Newton’s generalization from observation
was an accurate description of the real world. Later, as improvements in
astronomical measuring instruments extended the range of the observable
universe, scientists realized that Newton’s law was only a generalization
and not a property of the universe at all. Einstein’s Theory of Relativity
gives a much closer ﬁt to the data, a ﬁt that has not been contradicted by
any observations in the century since its formulation. But this still does
not mean that relativity provides us with a complete, correct, and 
comprehensive view of the universe.
In our research efforts, the only statements we can make with God-like
certainty are of the form “our conclusions ﬁt the data.” The true nature of
the real world is unknowable. We can speculate, but never conclude.
CHAPTER 5
TESTING HYPOTHESES: CHOOSING A TEST STATISTIC
83
11 See “The Adventure Silver Blaze” by A. Conan-Doyle, The Strand, December 1892.

The gap between the sample and the population will always require a
leap of faith. We understand only in so far as we are capable of 
understanding (Lonergan, 1961).
SUMMARY
Know your objectives in testing. Know your data’s origins. Know the
assumptions you feel comfortable with. Never assign probabilities to 
the true state of nature, but only to the validity of your own predictions.
Collecting more and better data may be your best alternative.
TO LEARN MORE
For commentary on the use of wrong or inappropriate statistical methods,
see Avram et al. (1985), Badrick and Flatman (1999), Berger et al.
(2002), Bland and Altman (1995), Cherry (1998), Dar, Serlin, and Omer
(1997), Elwood (1998), Felson, Cupples, and Meenan (1984), Fienberg
(1990), Gore, Jones, and Rytter (1977), Lieberson (1985), MacArthur
and Jackson (1977), McGuigan (1995), McKinney et al. (1989), Miller
(1986), Padaki (1989), Welch and Gabbe (1996), Westgard and Hunt
(1973), White (1979), and Yoccuz (1991).
Guidelines for reviewers are provided by Altman (1998), Bacchetti
(2002), Finney (1997), Gardner, Machin, and Campbell (1986), 
George (1985), Goodman, Altman, and George (1998), International
Committee of Medical Journal Editors (1997), Light and Pillemer (1984),
Mulrow (1987), Murray (1988), Schor and Karten (1966), and Vaisrub
(1985).
For additional comments on the effects of the violation of assumptions,
see Box and Anderson (1955), Friedman (1937), Gastwirth and Rubin
(1971), Glass, Peckham, and Sanders (1972), and Pettitt and Siskind
(1981).
For the details of testing for equivalence, see Dixon (1998). For a
review of the appropriate corrections for multiple tests, see Tukey 
(1991).
For true tests of independence, see Romano (1989). There are many
tests for the various forms of dependence, such as quadrant dependence
(Fisher’s exact test), trend (correlation), and serial correlation (see, for
example, Maritz, 1996 and Manly, 1997).
For procedures with which to analyze factorial and other multi-factor
experimental designs, see Chapter 8 of Pesarin (2001).
Most of the problems with parametric tests reported here extend to and
are compounded by multivariate analysis. For some solutions, see Chapter
5 of Good (2000) and Chapter 6 of Pesarin (2001).
84
PART II
HYPOTHESIS TESTING AND ESTIMATION

For a contrary view on adjustments of p-values in multiple comparisons,
see Rothman (1990). For a method for allocating Type I error among
multiple hypotheses, see Moye (2000).
Venn (1866) and Reichenbach (1949) are among those who’ve
attempted to construct a mathematical bridge between what we observe
and the reality that underlies our observations. To the contrary, 
extrapolation from the sample to the population is not a matter of 
applying Holmes-like deductive logic but entails a leap of faith. A careful
reading of Locke (1700), Berkeley (1710), Hume (1748), and Lonergan
(1992) is an essential prerequisite to the application of statistics.
For more on the contemporary view of induction, see Berger (2002)
and Stern and Smith (2001). The former notes that, “Dramatic illustration
of the non-frequentist nature of p-values can be seen from the applet 
available at www.stat.duke.edu/~berger. The applet assumes one faces a
series of situations involving normal data with unknown mean θ and
known variance, and tests of the form H: θ = 0 versus K: θ ≠0. The
applet simulates a long series of such tests, and records how often H is
true for p-values in given ranges.”
CHAPTER 5
TESTING HYPOTHESES: CHOOSING A TEST STATISTIC
85

Chapter 6
Strengths and Limitations 
of Some Miscellaneous 
Statistical Procedures
CHAPTER 6
SOME MISCELLANEOUS STATISTICAL PROCEDURES
87
Common Errors in Statistics (and How to Avoid Them), 2e, by Phillip I. Good and James W. Hardin.
Copyright © 2006 John Wiley & Sons, Inc.
THE GREATEST ERROR ASSOCIATED WITH THE USE of statistical procedures is
to make the assumption that one single statistical methodology can sufﬁce
for all applications.
From time to time, a new statistical procedure will be introduced or an
old one revived along with the assertion that at last the deﬁnitive solution
has been found. As is so often the case with religions, at ﬁrst the new
methodology is reviled, even persecuted, until, growing in the number of
its adherents, it can begin to attack and persecute the adherents of other
more established dogma in its turn.
During the preparation of the ﬁrst edition of this text, an editor of a
statistics journal rejected an article of one of the authors on the sole
grounds that it made use of permutation methods.
“I’m amazed that anybody is still doing permutation tests. . . .” wrote
the anonymous reviewer. “There is probably nothing wrong technically
with the paper, but I personally would reject it on grounds of irrelevance
to current best statistical practice.” To which the editor sought ﬁt to add,
“The reviewer is interested in estimation of interaction or main effects in
the more general semiparametric models currently studied in the literature.
It is well known that permutation tests preserve the signiﬁcance level but
that is all they do is answer yes or no.”1
But one methodology can never be better than another, nor can estima-
tion replace hypothesis testing or vice versa. Every methodology has a
1 A double untruth. First, permutation tests also yield interval estimates; see, for example,
Garthwaite (1996). Second, semiparametric methods are not appropriate for use with small-
sample experimental designs, the topic of the submission.

proper domain of application and another set of applications for which it
fails. Every methodology has its drawbacks and its advantages, its assump-
tions and its sources of error. Let us seek the best from each statistical
procedure.
The balance of this chapter is devoted to exposing the frailties of four of
the “new” (and revived) techniques: Bayesian methods, bootstrap, meta-
analysis, and permutation tests.
BOOTSTRAP
Many of the procedures discussed in this chapter fall victim to the erro-
neous perception that one can get more out of a sample or series of
samples than one actually puts in. One bootstrap expert learned he was
being considered for a position because management felt “your knowledge
of the bootstrap will help us to reduce the cost of sampling.”
Michael Chernick, author of Bootstrap Methods: A Practitioner’s Guide,
Wiley, 1999, has documented six myths concerning the bootstrap:
1. Allows you to reduce your sample size requirements by replacing
real data with simulated data—Not.
2. Allows you to stop thinking about your problem, the statistical
design and probability model—Not.
3. No assumptions necessary—Not.
4. Can be applied to any problem—Not.
5. Only works asymptotically—Necessary sample size depends on the
context.
6. Yields exact signiﬁcance levels—Never.
Of course, the bootstrap does have many practical applications, as
witness its appearance in six of the chapters in this text.2
Limitations
As always, to use the bootstrap or any other statistical methodology effec-
tively, one has to be aware of its limitations. The bootstrap is of value in
any situation in which the sample can serve as a surrogate for the 
population.
If the sample is not representative of the population because the sample
is small, biased, or not selected at random, or its constituents are not inde-
pendent of one another, then the bootstrap will fail.
Canty et al. (2000) also list data outliers, inconsistency of the bootstrap
method, incorrect resampling model, wrong or inappropriate choice of
88
PART II
HYPOTHESIS TESTING AND ESTIMATION
2 If you’re counting, we meet the bootstrap again in Chapters 12 and 13.

statistic, nonpivotal test statistics, nonlinearity of the test statistic, and dis-
creteness of the resample statistic as potential sources of error.
One of the ﬁrst proposed uses of the bootstrap, illustrated in Chapter 4,
was in providing an interval estimate for the sample median. Because the
median or 50th percentile is in the center of the sample, virtually every
element of the sample contributes to its determination. As we move out
into the tails of a distribution, to determine the 20th percentile or the
90th, fewer and fewer elements of the sample are of assistance in making
the estimate.
For a given size sample, bootstrap estimates of percentiles in the tails
will always be less accurate than estimates of more centrally located per-
centiles. Similarly, bootstrap interval estimates for the variance of a distrib-
ution will always be less accurate than estimates of central location such as
the mean or median, as the variance depends strongly on extreme values
in the population.
One proposed remedy is the tilted bootstrap3 in which, instead of sam-
pling each element of the original sample with equal probability, we
weight the probabilities of selection so as to favor or discourage the selec-
tion of extreme values.
If we know something about the population distribution in advance, for
example, if we know that the distribution is symmetric or that it is chi-
square with six degrees of freedom, then we may be able to take advan-
tage of a parametric or semiparametric bootstrap as described in Chapter
4. Recognize that in doing so, you run the risk of introducing error
through an inappropriate choice of parametric framework.
Problems due to the discreteness of the bootstrap statistic are usually
evident from plots of bootstrap output. They can be addressed by using a
smooth bootstrap as described in Davison and Hinkley (1997, Section
3.4).
BAYESIAN METHODOLOGY
Since being communicated to the Royal Society in 1763,4 Bayes’ theorem
has exerted a near fatal attraction on those exposed to it.5 Much as a bell
placed on the cat would magically resolve so many of the problems of the
average house mouse, Bayes’ straightforward, easily grasped mathematical
formula would appear to provide the long-awaited basis for a robotic
judge free of human prejudice.
CHAPTER 6
SOME MISCELLANEOUS STATISTICAL PROCEDURES
89
3 See, for example, Hinkley and Shi (1989) and Phipps (1997).
4 Philos Trans 1763; 53:376–398. Reproduced in: Biometrika 1958; 45: 293–315.
5 The interested reader is directed to Keynes (1921) and Redmayne (1998) for some
accounts.

On the plus side, Bayes’ theorem offers three main advantages:
1. Simpliﬁes the combination of a variety of different kinds of evi-
dence, lab tests, animal experiments, and clinical trials and serves
as an effective aid to decision-making.
2. Permits evaluating evidence in favor of a null hypothesis. And
with very large samples, a null hypothesis is not automatically
rejected.
3. Provides ﬂexibility during the conduct of an experiment; sample
sizes can be modiﬁed, measuring devices altered, subject popula-
tions changed, and end points redeﬁned.
Suppose we have in hand a set of evidence E = {E1, E2, . . . , En}, and
thus have determined the conditional probability Pr{A|E} that some event
A is true. A might be the event that O.J. killed his ex-wife, that the
captain of the Valdez behaved recklessly, or some other incident whose
truth or falsehood we wish to establish. An additional piece of evidence
En+1 now comes to light. Bayes’ theorem tell us that
where ~A, read not A, is the event that A did not occur. Recall that
Pr{A} + Pr{~A} = 1. Pr{A|E1, . . . , En} is the prior probability of A, and
Pr{A|E1, . . . , En, En+1} is the posterior probability of A once the item of
evidence En+1 is in hand. Gather sufﬁcient evidence and we shall have an
automatic verdict.
The problem with the application of Bayes’ theorem in practice comes
at the beginning when we have no evidence in hand and n = 0. What is
the prior probability of A then?
Applications in the Courtroom6
Bayes’ theorem has seen little use in criminal trials as ultimately the
theorem relies on unproven estimates rather than known facts.7 Tribe
(1971) states several objections, including the argument that a jury might
actually use the evidence twice, once in its initial assessment of guilt—that
is, to determine a prior probability—and a second time when it applies
Bayes’ theorem. A further objection to the theorem’s application is that if
Pr
...
Pr
Pr
...
Pr
Pr
...
Pr
~
Pr ~
...
A E
E
E
E
A
A E
E
E
A
A E
E
E
A
A E
E
n
n
n
n
n
n
n
n
1
1
1
1
1
1
1
1
,
,
,
,
,
,
,
,
,
+
+
+
+
{
} =
{
}
{
}
{
}
{
} +
{
}
{
}
90
PART II
HYPOTHESIS TESTING AND ESTIMATION
6 The majority of this section is reprinted with permission from Applying Statistics in the
Courtroom, by Phillip Good, Copyright 2001 by CDC Press Inc.
7 See, for example, People v. Collins, 68 Cal2d 319, 36 ALR3d 1176 (1968).

a man is innocent until proven guilty, the prior probability of his guilt
must be zero; by Bayes’ theorem the posterior probability of his guilt
would be zero also, rendering a trial unnecessary. The courts of several
states have remained unmoved by this argument.8
In State v. Spann,9 showing the defendant had fathered the victim’s
child was key to establishing a charge of sexual assault. The State’s expert
testiﬁed that only 1% of the presumed relevant population of possible
fathers had the type of blood and tissue that the father had and, further,
that the defendant was included within that 1%. In other words, 99% of
the male population at large was excluded. Next, she used Bayes’ theorem
to show that the defendant had a posterior probability of fathering the
victim’s child of 96.5%.
The expert testifying that the probability of defendant’s paternity was 96.5%
knew absolutely nothing about the facts of the case other than those
revealed by blood and tissues tests of defendant, the victim, and the 
child. . . .10
In calculating a ﬁnal probability of paternity percentage, the expert relied in
part on this 99% probability of exclusion. She also relied on an assumption of
a 50% prior probability that defendant was the father. This assumption [was]
not based on her knowledge of any evidence whatsoever in this case . . . [she
stated] “everything is equal . . . he may or may not be the father of the
child.”11
Was the expert’s opinion valid even if the jury disagreed with the assumption
of .5 [50%]? If the jury concluded that the prior probability is .4 or .6, for
example, the testimony gave them no idea of the consequences, no knowl-
edge of what the impact (of such a change in the prior probability) would be
on the formula that led to the ultimate opinion of the probability of
paternity.12
. . . [t]he expert’s testimony should be required to include an explanation to
the jury of what the probability of paternity would be for a varying range of
such prior probabilities, running for example, from .1 to .9.13
In other words, Bayes’ theorem might prove applicable if regardless of the
form of the a priori distribution, one came to more or less the same 
conclusion.
CHAPTER 6
SOME MISCELLANEOUS STATISTICAL PROCEDURES
91
8 See, for example, Davis v. State, 476 N.E.2d 127 (Ind.App.1985) and Grifﬁth v. State of
Texas, 976 S.W.2d 241 (1998).
9 130 N.J. 484 (1993).
10 Id. 489.
11 Id. 492.
12 Id. 498.
13 Id. 499.

Courts in California,14 Illinois, Massachusetts,15 Utah,16 and Virginia17
also have challenged the use of the ﬁfty-ﬁfty assumption. In State v.
Jackson,18 the expert did include a range of prior probabilities in her testi-
mony, but the court ruled that the trial judge had erred in allowing the
expert to testify as to the conclusions of Bayes’ theorem in stating a con-
clusion that the defendant was “probably” the father of the victim’s child.
In Cole v. Cole,19 a civil action, the court rejected the admission of an
expert’s testimony of a high probability of paternity derived via Bayes’
formula because there was strong evidence that the defendant was sterile
as a result of a vasectomy.
The source of much controversy is the statistical formula generally used to
calculate the provability of paternity: the Bayes Theorem. . . . Brieﬂy, the Bayes
Theorem shows how new statistical information alters a previously established
probability. . . . When a laboratory uses the Bayes Theorem to calculate a
probability of paternity it must ﬁrst calculate a “prior probability of pater-
nity”. . . . This prior probability usually has no connection to the case at hand.
Sometimes it reﬂects the previous success of the laboratory at excluding false
fathers. Traditionally, laboratories use the ﬁgure 50% which may or may not
be appropriate in a given case.
Critics suggest that this prior probability should take into account the circum-
stances of the particular case. For example if the woman has accused three
men of fathering her child or if there are reasons to doubt her credibility, or if
there is evidence that the husband is infertile, as in the present case, then the
prior probability should be reduced to less than 50%.20
The question remains as to what value to assign the prior probability.
And whether absent sufﬁcient knowledge to pin down the prior probabil-
ity with any accuracy we can make use of Bayes’ theorem at all. At trial, an
expert called by the prosecution in Plemel v. Walter21 used Bayes’ theorem
to derive the probability of paternity.
If the paternity index or its equivalents are presented as the probability of
paternity, this amounts to an unstated assumption of a prior probability of 50
percent. . . . the paternity index will equal the probability of paternity only
when the other evidence in this case establishes prior odds of paternity of
exactly one.22
92
PART II
HYPOTHESIS TESTING AND ESTIMATION
14 State v. Jackson, 320 NC 452, 358 S.E.2d 679 (1987).
15 Commonwealth v. Beausoleil, 397 Mass. 206 (1986).
16 Kofford v. Flora, 744 P.2d 1343, 1351–2 (1987).
17 Bridgeman v. Commonwealth, 3 Va. App 523 (1986).
18 320 N.C. 452 (1987).
19 74 N.C.App. 247, aff’d. 314 N.C. 660 (1985).
20 Id. 328.
21 303 Or. 262 (1987).
22 Id. 272.

. . . the expert is unqualiﬁed to state that any single ﬁgure is the accused’s
“probability of paternity.” As noted above, such a statement requires an esti-
mation of the strength of other evidence presented in the case (i.e., an esti-
mation of the “prior the probability of paternity”), an estimation that the
expert is no better position to make than the trier of fact.23
Studies in Poland and New York City have suggested that this assumption [a
50 percent prior probability] favors the putative father because in an esti-
mated 60 to 70 percent of paternity cases the mother’s accusation of pater-
nity is correct. Of course, the purpose of paternity litigation is to determine
whether the mother’s accusation is correct and for that reason it would be
both unfair and improper to apply the assumption in any particular case.24
A remedy proposed by the court is of interest to us:
If the expert testiﬁes to the defendant’ paternity index or a substantially
equivalent statistic, the expert must, if requested, calculate the probability
that the defendant is the father by using more than a single assumption
about the strength of the other evidence in the case. . . . If the expert uses
various assumptions and makes these assumptions known, the fact ﬁnder’s
attention will be directed to the other evidence in the case, and it will not be
misled into adopting the expert’s assumption as to the correct weight to be
assigned the other evidence. The expert should present calculations based on
assumed prior probabilities of 0, 10, 20, . . . , 90 and 100 percent.25
The courts of many other states have followed Plemmel. “The better
practice may be for the expert to testify to a range of prior probabilities,
such as 10, 50 and 90 percent, and allow the trier of fact to determine
which to use.26
Applications to Experiments and Clinical Trials
Outside the courtroom, where the rules of evidence are less rigorous, we
have much greater latitude in the adoption of a priori distributions for the
unknown parameter(s). Two approaches are common:
1. Adopting some synthetic distribution—a normal or a Beta
2. Using subjective probabilities
The synthetic approach, although common among the more computa-
tional, is difﬁcult to justify. The theoretical basis for an observation having
a normal distribution is well known—the observation will be the sum of a
CHAPTER 6
SOME MISCELLANEOUS STATISTICAL PROCEDURES
93
23 Id. 275.
24 Id. 276, fn 9.
25 Id. 279. See also Kaye (1988).
26 County of El Dorado v. Misura, 33 Cal. App.4th 73 (1995) citing Plemel, supra, at p.
1219; Peterson (1982 at p. 691, fn. 74), Paternity of M.J.B., 144 Wis.2d 638, 643, State v.
Jackson, 320 N.C.452, 455 (1987), and Kammer v. Young, 73 Md. App. 565, 571 (1988).
See also State v. Spann, 130 N.J. 484 at p. 499 (1993).

large number of factors each of which makes only a minute contribution
to the total. But could such a description be applicable to a population
parameter?
Here is an example of this approach taken from a report by D.A.
Berry27:
A study reported by Freireich et al.28 was designed to evaluate the effective-
ness of a chemotherapeutic agent 6-mercaptopurine (6-MP) for the treatment
of acute leukemia. Patients were randomized to therapy in pairs. Let p be the
population proportion of pairs in which the 6-MP patient stays in remission
longer than the placebo patient. (To distinguish probability p from a probabil-
ity distribution concerning p, I will call it a population proportion or a propen-
sity.) The null hypothesis H0 is p = 1/2: no effect of 6-MP. Let H1 stand for the
alternative hypothesis that p > 1/2. There were 21 pairs of patients in the
study, and 18 of them favored 6-MP.
Suppose that the prior probability of the null hypothesis is 70 percent and
that the remaining probability of 30 percent is on the interval (0,1) uniformly.
. . . So under the alternative hypothesis H1, p has a uniform(0,1) distribution.
This is a mixture prior in the sense that it is 70 percent discrete and 30
percent continuous.
The uniform(0,1) distribution is also the beta(1,1) distribution. Updating the
beta(a,b) distribution after s successes and f failures is easy, namely, the new
distribution is beta(a + s, b + f ). So for s = 18 and f = 3, the posterior distribu-
tion under H1 is beta(19,4).
The subjective approach places an added burden on the experimenter.
As always, she must specify each of the following:
•
Maximum acceptable frequency of Type I errors (that is, the sig-
niﬁcance level)
•
Alternative hypotheses of interest
•
Power desired against each alternative
•
Losses associated with Type I and Type II errors
With the Bayesian approach, she must also provide a priori probabilities.
Arguing in favor of the use of subjective probabilities is the fact that
they permit incorporation of expert judgment in a formal way into infer-
ences and decision-making. Arguing against them, in the words of the late
Edward Barankin, “How are you planning to get these values—beat them
out of the researcher?” More appealing, if perhaps no more successful,
approaches are described by Good (1950) and Kadane et al. (1980).
94
PART II
HYPOTHESIS TESTING AND ESTIMATION
27 The full report, titled “Using a Bayesian Approach in Medical Device Development,” may
be obtained from Donald A. Berry at the Institute of Statistics & Decision Sciences and
Comprehensive Cancer Center, Duke University, Durham NC 27708.
28 Blood 1963; 21:699–716.

Bayes’ Factor
An approach that allows us to take advantage of the opportunities Bayes’
Theorem provides while avoiding its limitations and the objections raised
in the courts is through the use of the minimum Bayes’ factor introduced
by Edwards et al. [1963].
The Bayes factor is a measure of the degree to which the data from a
study moves us from our initial position. Let B denote the odds we put on
the primary hypothesis before we examine the data, and let A be the odds
we assign after seeing the data; the Bayes factor is deﬁned as A/B.
If the Bayes factor is equal to 1/10th, it means that the study results
have decreased the relative odds assigned to the primary hypothesis by
tenfold. For example, suppose the probability of the primary hypothesis
with respect to the alternate hypothesis was high to begin with, say 9 to 1.
A tenfold decrease would mean a change to odds of 9 to 10, a probability
of 47%. A further independent study with a Bayes factor of 1/10th would
mean a change to a posteriori odds of 9 to 100, less than 9%.
The minimum Bayes factor is calculated from the same information used
to determine the p value, and it can easily be derived from standard ana-
lytic results. In the words of Goodman [2001], “If a statistical test is
based on a Gaussian approximation, the strongest Bayes factor against the
null hypothesis is exp(−Z 2/2), where Z is the number of standard errors
from the null value. If the log-likelihood of a model is reported, the
minimum Bayes factor is simply the exponential of the difference between
the log-likelihoods of two competing models (i.e., the ratio of their
maximum likelihoods).”
The minimum Bayes factor does not involve a speciﬁc prior probability
distribution, rather, it is a global minimum over all prior distributions.
Bayarri and Berger [1998] and Berger and Sellke [1987] provide a 
simple formula for the minimum Bayes factor in the situation where the
prior probability distribution is symmetric and descending around 
the null value. This is −exp p ln(p), where p is the ﬁxed-sample-size 
p value.
As Goodman [2001] notes, “even the strongest evidence against the
null hypothesis does not lower its odds as much as the p-value magnitude
might lead people to believe. More importantly, the minimum Bayes
factor makes it clear that we cannot estimate the credibility of the null
hypothesis without considering evidence outside the study.”
For example, while a p value of 0.01 is usually termed “highly signiﬁ-
cant,” it actually represents evidence for the primary hypothesis of some-
where between 1/25 and 1/8.29 Put another way, the relative odds of the
CHAPTER 6
SOME MISCELLANEOUS STATISTICAL PROCEDURES
95
29 See Table B.1, Goodman [2001].

primary hypothesis versus any alternative given a p value of 0.01 are at
most 8–25 times lower than they were before the study. If one is going to
claim that a hypothesis is highly unlikely (e.g., less than 5%), one must
already have evidence outside the study that the prior probability of the
hypothesis is no greater than 60%. Conversely, even weak evidence in
support of a highly plausible relationship may be enough for an author to
make a convincing case.
Two caveats:
1. Bayesian methods cannot be used in support of after-the-fact
hypotheses for, by deﬁnition, an after-the-fact-hypothesis has zero
a priori probability and, thus, by Bayes’ rule, zero a posteriori
probability.
2. One hypothesis proving of greater predictive value than another in
a given instance may be suggestive but is far from deﬁnitive in the
absence of collateral evidence and proof of causal mechanisms. See,
for example, Hodges (1987).
96
PART II
HYPOTHESIS TESTING AND ESTIMATION
30 Reprinted with permission from the BMJ Publishing Group.
WHEN USING BAYESIAN METHODS
Do not use an arbitrary prior.
Never report a p-value.
Incorporate potential losses in the decision.
Report the Bayes factor.
META-ANALYSIS
Meta-analysis should be viewed as an observational study of the evidence.
The steps involved are similar to any other research undertaking: formulation
of the problem to be addressed, collection and analysis of the data, and
reporting of the results. Researchers should write in advance a detailed
research protocol that clearly states the objectives, the hypotheses to be
tested, the subgroups of interest, and the proposed methods and criteria for
identifying and selecting relevant studies and extracting and analysing 
information.
Egger, Smith, and Phillips (1997)30
Too many studies end with inconclusive results because of the relatively
small number of observations that were made. The researcher can’t quite
reject the null hypothesis, but isn’t quite ready to embrace the null
hypothesis, either. As we saw in Chapter 1, a post hoc subgroup analysis

can suggest an additional relationship, but the relationship cannot be
subject to statistical test in the absence of additional data.
Meta-analysis is a set of techniques that allow us to combine the results
of a series of small trials and observational studies. With the appropriate
meta-analysis, we can, in theory, obtain more precise estimates of main
effects, test a priori hypotheses about subgroups, and determine the
number of observations needed for large-scale randomized trials.
By putting together all available data, meta-analyses are also better
placed than individual trials to answer questions about whether an overall
study result varies among subgroups—for example, among men and
women, older and younger patients, or subjects with different degrees of
severity of disease.
In performing a meta-analysis, we need to distinguish between observa-
tional studies and randomized trials.
Confounding and selection bias can easily distort the ﬁndings from
observational studies. Egger et al. (1998) note,
An important criterion supporting causality of associations is a dose-response
relation. In occupational epidemiology the quest to show such an association
can lead to very different groups of employees being compared. In a meta-
analysis that examined the link between exposure to formaldehyde and
cancer, funeral directors and embalmers (high exposure) were compared with
anatomists and pathologists (intermediate to high exposure) and with indus-
trial workers (low to high exposure, depending on job assignment). There is a
striking deﬁcit of deaths from lung cancer among anatomists and pathologists
[standardised mortality ratio 33 (95% conﬁdence interval 22 to 47)], which is
most likely to be due to a lower prevalence of smoking among this group. In
this situation few would argue that formaldehyde protects against lung
cancer. In other instances, however, such selection bias may be less obvious.31
On the other hand, much may be gained by a careful examination of
possible sources of heterogeneity between the results from observational
studies.
Publication and selection bias also plague the meta-analysis of com-
pletely randomized trials. Inconclusive or negative results seldom appear in
print (Götzsche, 1987; Chalmers et al., 1990; Eastebrook et al., 1991)
and are unlikely even to be submitted for publication. One can’t analyze
what one doesn’t know about.
Similarly, the decision as to which studies to incorporate can dramati-
cally affect the results. Meta-analyses of the same issue may reach opposite
conclusions, as shown by assessments of low-molecular-weight heparin in
the prevention of perioperative thrombosis (Nurmohamed et al., 1992;
Leizorovicz et al., 1992) and of second-line antirheumatic drugs in the
CHAPTER 6
SOME MISCELLANEOUS STATISTICAL PROCEDURES
97
31 Reprinted with permission from the BMJ Publishing Group.

treatment of rheumatoid arthritis (Felson et al., 1990; Götzsche et al.,
1992). Meta-analyses showing beneﬁt of statistical signiﬁcance and clinical
importance have been contradicted later by large randomized trials (Egger
et al., 1997).
Where there are substantial differences between the different studies
incorporated in a meta-analysis (their subjects or their environments), or
substantial quantitative differences in the results from the different trials, a
single overall summary estimate of treatment beneﬁt has little practical
applicability (Horowitz, 1995). Any analysis that ignores this heterogene-
ity is clinically misleading and scientiﬁcally naive (Thompson, 1994). Het-
erogeneity should be scrutinized, with an attempt to explain it (Bailey,
1987; Berkey et al., 1995; Chalmers, 1991; Victor, 1995).
Bayesian Methods
Bayesian methods can be effective in meta-analyses; see, for example,
Mosteller and Chalmers (1992). In such situations the parameters of
various trials are considered to be random samples from a distribution of
trial parameters. The parameters of this higher-level distribution are called
hyperparameters, and they also have distributions. The model is called
hierarchical. The extent to which the various trials reinforce each other is
determined by the data. If the trials are very similar, the variation of the
hyperparameters will be small, and the analysis will be very close to a
classic meta-analysis. If the trials do not reinforce each other, the conclu-
sions of the hierarchical Bayesian analysis will show a very high variance in
the results.
A hierarchical Bayesian analysis avoids the necessity of a prior decision as
to whether or not the trials can be combined; the extent of the combina-
tion is determined purely by the data. This does not come for free; in con-
trast to the meta-analyses discussed above, all the original data (or at least
the sufﬁcient statistics) must be available for inclusion in the hierarchical
model. The Bayesian method is also vulnerable to all the selection bias
issues discussed above.
Guidelines for a Meta-Analysis
•
A detailed research protocol for the meta-analysis should be pre-
pared in advance. Criteria for inclusion and the statistical method
employed should be documented in the materials and methods
section of the subsequent report.
•
Meta-analysis should be restricted to randomized controlled trials.
•
Heterogeneity in the trial results should be documented and
explained.
•
Do not attempt to compare treatments investigated in unrelated
trials. (Suppose, by way of a counterexample, that Old were given
98
PART II
HYPOTHESIS TESTING AND ESTIMATION

as always to low-risk patients in one set of trials, whereas New was
given to high-risk patients in another.)
•
Individual patient data, rather than published summary statistics,
often are required for meaningful subgroup analyses. This is a
major reason why we favor the modern trend in journals of insist-
ing that all data reported on within their pages be made available
by website to all investigators.
Kepler was able to formulate his laws only because (1) Tycho Brahe had
made over 30 years of precise (for the time) astronomical observations and
(2) Kepler married Brahe’s daughter and thus gained access to his data.
PERMUTATION TESTS
Permutation tests are often lauded erroneously in the literature as
“assumption-free” “panaceas.” Nothing could be further from the truth.
Permutation tests only yield exact signiﬁcance levels if the labels on the
observations are weakly exchangeable under the null hypothesis. Thus they
cannot be successfully applied to the coefﬁcients in a multivariate 
regression.
On the other hand, if the observations are weakly exchangeable under
the null hypothesis, then permutation tests are the method of choice for k-
sample comparisons and contingency tables, whenever there are 12 or
fewer observations in each subsample. Moreover, permutation methods
can be used both to test hypotheses and to obtain interval estimates of
parameters.
TO LEARN MORE
Potential ﬂaws in the bootstrap approach are considered by Schenker
(1985), Wu (1986), Diciccio and Romano (1988), Efron (1988, 1992),
Knight (1989), and Gine and Zinn (1989). Canty et al. (2000) provide a
set of diagnostics for detecting and dealing with potential error sources.
Berry and Stangl (1996) include a collection of case studies in Bayesian
biostatistics. Kass and Rafferty (1995) discuss the problem of establishing
priors along with a set of practical examples. The Bayes factor can be used
as a test statistic; see Good (1992).
For more on the strengths and limitations of meta-analysis, see Egger
and Smith (1997), Egger, Smith, and Phillips (1997), Smith, Egger, and
Phillips (1997), Smith and Egger (1998), Gillett (2001), Gurevitch and
Hedges (1993), Horowitz (1995), and Smeeth, Haines, and Ebrahim
(1999). To learn about the appropriate statistical procedures, see Adams,
Gurevitch, and Rosenberg (1997), Berlin et al. (1989), and Hedges and
Olkin (1985).
CHAPTER 6
SOME MISCELLANEOUS STATISTICAL PROCEDURES
99

For practical, worked-through examples of hierarchical Bayesian analysis,
see Harley and Myers (2001) and Su, Adkison, and Van Alen (2001).
Theoretical development may be found in Mosteller and Chalmers (1992)
and Carlin and Louis (1996).
The lack of access to the raw data underlying published studies is a
matter of ongoing concern. See Moher et al. (1999), Eysenbach and Sa
(2001), and Hutchon (2001).
Permutation methods and their applications are described in Good
(2001), Manley (1997), Mielke and Berry (2001), and Pesarin (2001).
For a description of some robust permutation tests, see Lambert (1985)
and Maritz (1996). Berger (2000) reviews the pros and cons of permuta-
tion tests.
100
PART II
HYPOTHESIS TESTING AND ESTIMATION

Chapter 7
Reporting Your Results
CHAPTER 7
REPORTING YOUR RESULTS
101
Common Errors in Statistics (and How to Avoid Them), 2e, by Phillip I. Good and James W. Hardin.
Copyright © 2006 John Wiley & Sons, Inc.
THE FOCUS OF THIS CHAPTER IS ON WHAT TO REPORT and how to report it.
Reportable elements include the experimental design and its objectives, the
analysis, and the sources and amounts of missing data. Guidelines for table
construction are provided. The bootstrap is proposed as an alternative to
the standard error as a measure of precision. The value and limitations of p-
values and conﬁdence intervals are summarized. Practical signiﬁcance is dis-
tinguished from statistical signiﬁcance and induction from deduction.
FUNDAMENTALS
Few experimenters fail to list number of subjects, doses administered, and
dose intervals in their reports. But many fail to provide the details of
power and sample size calculations. Feng et al. (2001) found that such
careless investigators also report a higher proportion of nonsigniﬁcant
intervention effects, indicating underpowered studies.
Too often inadequate attention is given to describing treatment alloca-
tion and the ones who got away. We consider both topics in what follows.
Treatment Allocation1
Allocation details should be fully described in your reports including dic-
tated allocation versus allocation discretion, randomization, advance prepa-
ration of the allocation sequence, allocation concealment, ﬁxed versus
Cut out the appropriate part of the computer output and paste it
onto the draft of the paper.
George Dyke (tongue in cheek) (1997).
1 This material in this section relies heavily on a personal communication from 
Vance W. Berger and Costas A. Christophi.

varying allocation proportions, restricted randomization, masking, simulta-
neous versus sequential randomization, enrollment discretion, and the pos-
sibility of intent to treat.
Allocation discretion may be available to the investigator, the patient,
both, or neither (dictated allocation). Were investigators permitted to
assign treatment based on patient characteristics? Could patients select
their own treatment from among a given set of choices?
Was actual (not virtual, quasi-, or pseudo-) randomization employed?
Was the allocation sequence predictable? (For example, patients with even
accession numbers or patients with odd accession numbers receive the
active treatment; the others receive the control.)
Was randomization conventional, that is, was the allocation sequence
generated in advance of screening any patients?
Was allocation concealed before its being executed? As Vance W. Berger
and Costas A. Christophi relate in a personal communication, “This is not
itself a reportable design feature, so a claim of allocation concealment
should be accompanied by speciﬁc design features. For example, one may
conceal the allocation sequence; and instead of using envelopes, patient
enrollment may involve calling the baseline information of the patient to
be enrolled in to a central number to receive the allocation.”
Was randomization restricted or unrestricted? Randomization is unre-
stricted if a patient’s likelihood of receiving either treatment is independent
of all previous allocations and is restricted otherwise. If both treatment
groups must be assigned equally often, then prior allocations determine
the ﬁnal ones. Were the proportions also hidden?
Were treatment codes concealed until all patients had been randomized
and the database locked? Were there instances of codes being revealed
accidentally? Senn (1995) warns, “Investigators should delude neither
themselves, nor those who read their results, into believing that simply
because some aspects of their trial were double-blind that therefore all the
virtues of such trials apply to all their conclusions.” Masking can rarely, if
ever, be ensured; see also Day (1998).
Was randomization simultaneous, block simultaneous, or sequential? A
blocked randomization is block simultaneous if all patients within any given
block are identiﬁed and assigned accession numbers before any patient in
that block is treated.
And, not least, was intent to treat permitted?
Missing Data2
Every experiment or survey has its exceptions. You must report the raw
numbers of such exceptions and, in some instances, provide additional
102
PART II
HYPOTHESIS TESTING AND ESTIMATION
2 Material in this section is reprinted with permission from The Manager’s Guide to Design
and Conduct of Clinical Trials, Wiley, 2002, 6.

analyses that analyze or compensate for them. Typical exceptions include
the following:
Did Not Participate. Subjects who were eligible and available but did
not participate in the study—This group should be broken down further
into those who were approached but chose not to participate and those
who were not approached. With a mail-in survey, for example, we would
distinguish between those whose envelopes were returned “address
unknown” and those who simply did not reply.
Ineligibles. In some instances, circumstances may not permit deferring
treatment until the subject’s eligibility can be determined.
For example, an individual arrives at a study center in critical condition;
the study protocol calls for a series of tests, the results of which may not
be back for several days, but in the opinion of the examining physician
treatment must begin immediately. The patient is randomized to treat-
ment, and only later is it determined that the patient is ineligible.
The solution is to present two forms of the ﬁnal analysis, one incor-
porating all patients and the other limited to those who were actually 
eligible.
Withdrawals. Subjects who enrolled in the study but did not complete it,
including both dropouts and noncompliant patients—These patients might
be subdivided further based on the point in the study at which they
dropped out.
At issue is whether such withdrawals were treatment related or not. For
example, the gastrointestinal side effects associated with erythromycin are
such that many patients (including both authors) may refuse to continue
with the drug. Traditional statistical methods are not applicable when
withdrawals are treatment related.
Crossovers. If the design provided for intent-to-treat, a noncompliant
patient may still continue in the study after being reassigned to an alter-
nate treatment. Two sets of results should be reported: the ﬁrst for all
patients who completed the trials (retaining their original treatment
assignments for the purpose of analysis), the second restricted to the
smaller number of patients who persisted in the treatment groups to
which they were originally assigned.
Missing Data. Missing data are common, expensive, and preventable in
many instances.
The primary end point of a recent clinical study of various cardiovascu-
lar techniques was based on the analysis of follow-up angiograms.
Although more than 750 patients were enrolled in the study, only 523
had the necessary angiograms. Almost a third of the monies spent on the
CHAPTER 7
REPORTING YOUR RESULTS
103

trials had been wasted. This result is not atypical. Capaldi and Patterson
(1987) uncovered an average attrition rate of 47% in studies lasting 4 to
10 years.
You need to analyze the data to ensure that the proportions of missing
observations are the same in all treatment groups. Again, traditional statis-
tical methods are applicable only if missing data are not treatment related.
Deaths and disabling accidents and diseases, whether or not directly
related to the condition being treated, are common in long-term trials in
elderly and high-risk populations. Or individuals are simply lost to sight
(“no forwarding address”) in highly mobile populations.
Lang and Secic (1997, p. 22) suggest a chart such as that depicted in
Figure 3.1 as the most effective way to communicate all the information
regarding missing data. Censored and off-scale measurements should be
described separately and their numbers indicated in the corresponding
tables.
TABLES
Is text, a table, or a graph the best means of presenting results? Dyke
(1997) would argue, “Tables with appropriate marginal means are often
the best method of presenting results, occasionally replaced (or supple-
mented) by diagrams, usually graphs or histograms.” vanBelle (2002)
warns that aberrant values often can be more apparent in graphical form.
Arguing in favor of the use of ActivStats® for exploratory analysis is the
fact that one can so easily go back and forth from viewing the table to
viewing the graph.
A sentence structure should be used for displaying 2 to 5 numbers, as in
“The blood type of the population of the United States is approximately
45% O, 40% A, 11% B, and 4% AB.”3 Note that the blood types are
ordered by frequency.
Marginal means may be omitted only if they have already appeared in
other tables.4 Sample sizes should always be speciﬁed.
Among our own worst offenses is the failure to follow vanBelle’s advice
to “Use the table heading to convey critical information. Do not stint.
The more informative the heading, the better the table.”5
Consider adding a row (or column, or both) of contrasts; “for example,
if the table has only two rows we could add a row of differences, row 1
minus row 2: if there are more than two rows, some other contrast might
104
PART II
HYPOTHESIS TESTING AND ESTIMATION
3 vanBelle (2002, p. 154).
4 Dyke (1997). Reprinted with permission from Elsevier Science.
5 vanBelle (2002, p. 154).

be useful, perhaps ‘mean haploid minus mean diploid,’ or ‘linear compo-
nent of effect of N-fertilizer’.”6 Indicate the variability of these contrasts.
Tables dealing with two-factor arrays are straightforward, provided con-
ﬁdence limits, least standard deviations, and standard errors are clearly
associated with the correct set of ﬁgures. Tables involving three or more
factors are not always immediately clear to the reader and are best
avoided.
Are the results expressed in appropriate units? For example, are parts per
thousand more natural in a speciﬁc case than percentages? Have we
rounded off to the correct degree of precision, taking account of what we
know about the variability of the results and considering whether they will
be used by the reader, perhaps by multiplying by a constant factor, or by
another variate, for example, % dry matter?
Dyke (1997) also advises us that “Residuals should be tabulated and
presented as part of routine analysis; any [statistical] package that does not
offer this option was probably produced by someone out of touch with
research workers, certainly with those working with ﬁeld crops.” Best of
all is a display of residuals aligned in rows and columns as the plots were
aligned in the ﬁeld.
A table of residuals (or tables, if there are several strata) can alert us to
the presence of outliers and may also reveal patterns in the data not con-
sidered previously.
STANDARD ERROR
One of the most egregious errors in statistics, one encouraged, if not
insisted on by the editors of journals in the biological and social sciences,
is the use of the notation “mean ± standard error” to report the results of
a set of observations.
Presumably, the editors of these journals (and the reviewers they select)
have three objectives in mind: To communicate some idea of
1. The “correct” result
2. The precision of the estimate of the correct result
3. The dispersion of the distribution from which the observations
were drawn
Let’s see to what degree any or all of these objectives might be realized
in real life by the editor’s choice.
For small samples of 3–5 observations, summary statistics are virtually
meaningless. Reproduce the actual observations; this is easier to do and
more informative.
CHAPTER 7
REPORTING YOUR RESULTS
105
6 Dyke (1997). Reprinted with permission from Elsevier Science.

For many variables, regardless of sample size, the arithmetic mean can
be very misleading. For example, the mean income in most countries is far
in excess of the median income or 50th percentile to which most of us can
relate. When the arithmetic mean is meaningful, it is usually equal to or
close to the median. Consider reporting the median in the ﬁrst place.
The geometric mean is more appropriate than the arithmetic mean in
three sets of circumstances:
1. When losses or gains can best be expressed as a percentage rather
than a ﬁxed value
2. When rapid growth is involved, as in the development of a bacter-
ial or viral population
3. When the data span several orders of magnitude, as with the con-
centration of pollutants
Because bacterial populations can double in number in only a few
hours, many government health regulations utilize the geometric rather
than the arithmetic mean.7 A number of other government regulations
also use it, although the sample median would be far more appropriate.8
Whether you report a mean or a median, be sure to report only a sensi-
ble number of decimal places. Most statistical packages can give you 9 or
10. Don’t use them. If your observations were to the nearest integer, your
report on the mean should include only a single decimal place. For guides
to the appropriate number of digits, see Ehrenberg (1977) and for per-
centages, vanBelle (2002, Table 7.4).
The standard error is a useful measure of population dispersion if the
observations come from a normal or Gaussian distribution. If the observa-
tions are normally distributed as in the bell-shaped curve depicted in
Figure 7.1, then in 95% of the samples we would expect the sample mean
to lie within two standard errors of the mean of our original sample.
But if the observations come from a nonsymmetric distribution like an
exponential or a Poisson, or a truncated distribution like the uniform, or a
mixture of populations, we cannot draw any such inference.
Recall that the standard error equals the standard deviation divided by
the square root of the sample size,
SE
x
x
n
i
=
−
(
)
∑
2
106
PART II
HYPOTHESIS TESTING AND ESTIMATION
7 See, for example, 40 CFR part 131, 62 Fed. Reg. 23004 at 23008 (28 April 1997).
8 Examples include 62 Fed. Reg. 45966 at 45983 (concerning the length of a hospital stay)
and 62 Fed. Reg. 45116 at 45120 (concerning sulfur dioxide emissions).

As the standard error depends on the squares of individual observations,
it is particularly sensitive to outliers. A few extra large observations will
have a dramatic impact on its value.
If you can’t be sure your observations come from a normal distribution,
then consider reporting your results either in the form of a histogram as in
Figure 7.2 or a box and whiskers plot, Figure 7.3. See also Lang and Secic
(1997, p. 50).
If your objective is to report the precision of your estimate of the mean
or median, then the standard error may be meaningful providing the mean
of your observations is normally distributed.
CHAPTER 7
REPORTING YOUR RESULTS
107
.4
0
probability density
normally distributed variable
–3
3
FIGURE 7.1
Bell-Shaped Symmetric Curve of a Normal Distribution.
135
170
height in cm
6
0
Frequency
FIGURE 7.2
Histogram of Heights in a Sixth-Grade Class.

The good news is that the sample mean often will have a normal distri-
bution even when the observations themselves do not come from a
normal distribution. This is because the sum of a large number of random
variables each of which makes only a small contribution to the total is a
normally distributed random variable.9 And in a sample mean based on n
observations, each contributes only 1/nth the total. How close the ﬁt is
to a normal will depend on the size of the sample and the distribution
from which the observations are drawn.
The distribution of a uniform random number U[0,1] is a far cry from
the bell-shaped curve of Figure 7.1. Only values between 0 and 1 have a
positive probability, and in stark contrast to the normal distribution, no
range of values between zero and one is more likely than another of the
same length. The only element the uniform and normal distributions have
in common is their symmetry about the population mean. Yet to obtain
normally distributed random numbers for use in simulations a frequently
employed technique is to generate 12 uniformly distributed random
numbers and then take their average.
Apparently, 12 is a large enough number for a sample mean to be nor-
mally distributed when the variables come from a uniform distribution.
108
PART II
HYPOTHESIS TESTING AND ESTIMATION
+
+
Control
X537a
Treatment
40
30
20
10
0
Length
FIGURE 7.3
Box and Whiskers Plot. The box encompasses the middle 50%
of each sample whereas the “whiskers” lead to the smallest and largest values. The
line through the box is the median of the sample, that is, 50% of the sample is
larger than this value, and 50% is smaller. The plus sign indicates the sample mean.
Note that the mean is shifted in the direction of a small number of very large
values.
9 This result is generally referred to as the central limit theorem. Formal proof can be found
in a number of texts including Feller (1966, p. 253).

But take a smaller sample of observations from a U[0,1] population and
the distribution of its mean would look less like a bell-shaped curve.
A loose rule of thumb is that the mean of a sample of 8 to 25 observa-
tions will have a distribution that is close enough to the normal for the
standard error to be meaningful. The more nonsymmetric the original dis-
tribution, the larger the sample size required. At least 25 observations are
needed for a binomial distribution with p = 0.1.
Even the mean of observations taken from a mixture of distributions
(males and females, tall Zulu and short Bantu)—visualize a distribution
curve resembling a camel with multiple humps—will have a normal distri-
bution if the sample size is large enough. Of course, this mean (or even
the median) conceals the fact that the sample was taken from a mixture of
distributions.
If the underlying distribution is not symmetric, the use of the ±SE nota-
tion can be deceptive as it suggests a nonexistent symmetry. For samples
from nonsymmetric distributions of size 6 or less, tabulate the minimum,
the median, and the maximum. For samples of size 7 and up, consider
using a box and whiskers plot. For samples of size 16 and up, the boot-
strap, described in Chapters 4 and 5, may provide the answer you need.
As in Chapters 4 and 5, we would treat the original sample as a stand-in
for the population and resample from it repeatedly, 1000 times or so, with
replacement, computing the sample statistic each time to obtain a distribu-
tion similar to that depicted in Figure 7.4. To provide an interpretation
compatible with that given the standard error when used with a sample
from a normally distributed population, we would want to report the
values of the 16th and 84th percentiles of the bootstrap distribution along
with the sample statistic.
When the estimator is other than the mean, we cannot count on the
central limit theorem to ensure a symmetric sampling distribution. We rec-
ommend that you use the bootstrap whenever you report an estimate of a
ratio or dispersion.
If you possess some prior knowledge of the shape of the population dis-
tribution, you should take advantage of that knowledge by using a para-
metric bootstrap (see Chapter 4). The parametric bootstrap is particularly
recommended for use in determining the precision of percentiles in the
tails (P20, P10, P90, and so forth).
CHAPTER 7
REPORTING YOUR RESULTS
109
FIGURE 7.4
Rugplot of 50 Bootstrap Medians Derived from a Sample of
Sixth Graders’ Heights.

p-VALUES
Before interpreting and commenting on p-values, it’s well to remember
that, in contrast to the signiﬁcance level, the p-value is a random variable
that varies from sample to sample. There may be highly signiﬁcant differ-
ences between two populations and yet the samples taken from those pop-
ulations and the resulting p-value may not reveal that difference.
Consequently, it is not appropriate for us to compare the p-values from
two distinct experiments, or from tests on two variables measured in the
same experiment, and declare that one is more signiﬁcant than the other.
If we agree in advance of examining the data that we will reject the
hypothesis if the p-value is less than 5%, then our signiﬁcance level is 5%.
Whether our p-value proves to be 4.9% or 1% or 0.001%, we will come to
the same conclusion. One set of results is not more signiﬁcant than
another; it is only that the difference we uncovered was measurably more
extreme in one set of samples than in another.
Note: After examining the data, it is unethical to alter the signiﬁcance level
or to interpret a two-tailed test as if one had intended it to be one-tailed.
p-Values need not reﬂect the strength of a relationship. Duggan and
Dean (1968) reviewed 45 articles that had appeared in sociology journals
between 1955 and 1965 in which the chi-square statistic and distribution
had been employed in the analysis of 3 × 3 contingency tables and com-
pared the resulting p-values with association as measured by Goodman and
Kruskal’s gamma. Table 7.1 summarizes their ﬁndings.
p-Values derived from tables are often crude approximations, particularly
for small samples and tests based on a speciﬁc distribution. They and the
stated signiﬁcance level of our test may well be in error.
The vast majority of p-values produced by parametric tests based on the
normal distribution are approximations. If the data are “almost” normal,
the associated p-values will be almost correct. As noted in Chapter 6, the
stated signiﬁcance values for Student’s t are very close to exact. Of course,
a stated p-value of 4.9% might really prove to be 5.1% in practice. The sig-
niﬁcance values associated with the F-statistic can be completely inaccurate
for nonnormal data (1% rather than 10%). And the p-values derived from
the chi-square distribution for use with contingency tables also can be off
by an order of magnitude.
110
PART II
HYPOTHESIS TESTING AND ESTIMATION
p-Value
Gamma
<.30
.30–.70
>.70
<.01
8
11
5
0.1 to .05
7
0
0
>.10
8
0
0
TABLE 7.1 p-Value and Association

The good news is that there exists a class of tests, the permutation tests
described in Chapter 5, for which the signiﬁcance levels are exact if the
observations are independent and identically distributed under the null
hypothesis or their labels are otherwise exchangeable.
Regardless of which test one uses, it is the height of foolishness to report
p-values with excessive precision: 0.06 and 0.052 are both acceptable, but
0.05312 suggests you’ve let your software do the thinking for you.
CONFIDENCE INTERVALS
If p-values are misleading, what are we to use in their place? Jones (1955,
p. 407) was among the ﬁrst to suggest that “an investigator would be
misled less frequently and would be more likely to obtain the information
he seeks were he to formulate his experimental problems in terms of the
estimation of population parameters, with the establishment of conﬁdence
intervals about the estimated values, rather than in terms of a null hypoth-
esis against all possible alternatives.” See also Gardner and Altman (1996)
and Poole (2001).
Conﬁdence intervals can be derived from the rejection regions of our
hypothesis tests, whether the latter are based on parametric or nonpara-
metric methods. Suppose A(θ′) is a 1 −α level acceptance region for
testing the hypothesis θ = θ′, that is, we accept the hypothesis if our test
statistic T belongs to the acceptance region A(θ′) and reject it otherwise.
Let S(X) consist of all the parameter values θ* for which T[X] belongs to
the acceptance region A(θ*). Then S(X) is an 1 −α level conﬁdence inter-
val for θ based on the set of observations X = {x1,x2, . . . , xn}.
The probability that S(X) includes θo when θ = θo is equal to Pr{T[X] ∈
A(θo) when θ = θo} ≥1 −α.
As our conﬁdence 1 −α increases, from 90% to 95%, for example, the
width of the resulting conﬁdence interval increases. Thus a 95% conﬁ-
dence interval is wider than a 90% conﬁdence interval.
By the same process, the rejection regions of our hypothesis tests can be
derived from conﬁdence intervals. Suppose our hypothesis is that the odds
ratio for a 2 × 2 contingency table is 1. Then we would accept this null
hypothesis if and only if our conﬁdence interval for the odds ratio includes
the value 1.
A common error is to misinterpret the conﬁdence interval as a state-
ment about the unknown parameter. It is not true that the probability
that a parameter is included in a 95% conﬁdence interval is 95%. What is
true is that if we derive a large number of 95% conﬁdence intervals, we
can expect the true value of the parameter to be included in the computed
intervals 95% of the time. (That is, the true values will be included if the
assumptions on which the tests and conﬁdence intervals are based are sat-
CHAPTER 7
REPORTING YOUR RESULTS
111

isﬁed 100% of the time.) Like the p-value, the upper and lower conﬁdence
limits of a particular conﬁdence interval are random variables, for they
depend on the sample that is drawn.
112
PART II
HYPOTHESIS TESTING AND ESTIMATION
IMPORTANT TERMS
Acceptance Region, A(θo). Set of values of the statistic T[X] for which we
would accept the hypothesis H: θ = θo. Its complement is called the rejec-
tion region.
Conﬁdence Region, S(X). Also referred to as a conﬁdence interval (for a
single parameter) or a conﬁdence ellipse (for multiple parameters). Set of
values of the parameter θ for which given the set of observations X =
{x1,x2, . . . , xn} and the statistic T[X] we would accept the corresponding
hypothesis.
Conﬁdence intervals can be used to evaluate and report on both the
precision of estimates (see Chapter 4) and the signiﬁcance of hypothesis
tests (see Chapter 5). The probability that the interval covers the true
value of the parameter of interest and the method used to derive the inter-
val must also be reported.
In interpreting a conﬁdence interval based on a test of signiﬁcance, it is
essential to realize that the center of the interval is no more likely than
any other value, and the conﬁdence to be placed in the interval is no
greater than the conﬁdence we have in the experimental design and statis-
tical test it is based on. (As always, GIGO.)
Multiple Tests
Whether we report p-values or conﬁdence intervals, we need to correct for
multiple tests as described in Chapter 5. The correction should be based
on the number of tests we perform, which in most cases will be larger than
the number on which we report. See Westfall and Young (1992) and Hsu
(1996) for a discussion of some of the methods that can be employed to
obtain more accurate p-values.
Analysis of Variance
Don’t ignore signiﬁcant interactions. The guidelines on reporting the results
of a multifactor analysis are clearcut and too often ignored. If the interac-
tion between A and B is signiﬁcant, then the main effects of A should be
calculated and reported separately for several levels of the factor B.
Or, to expand on the quote from George Dyke with which we opened
this chapter, “Don’t just cut out the appropriate part of the computer
output and paste it onto the draft of the paper, but read it through and
conduct what additional calculations are suggested by the original analysis.”

Don’t Just Copy Your Computer Output
Don’t just copy your computer output and paste it into your reports. The
results can be absurd as shown in Table 7.2, taken from a working report
posted on the web, where the mean and standard deviation of a yes/no
variable (Electronic Voting) are recorded.
This problem might easily have been avoided, to say nothing of the
reduction in transcription errors, had Electronic Voting been recorded as a
character variable, Y/N or E/P, in the ﬁrst place.
And don’t ever report the mean and standard deviation of interaction
terms such as B2*Elec Voting whether or not one or the other of the
variables is dichotomous. On the other hand, when interaction terms are
statistically signﬁcant, and one of the variables is discrete, one ought
report the mean and standard deviation separately for each level of the dis-
crete variable. In the present example, this would indicate reporting the
mean and standard deviation of B2 both when electronic voting was used
and when printed ballots were used.
RECOGNIZING AND REPORTING BIASES
Very few studies can avoid bias at some point in sample selection, study
conduct, and results interpretation. We focus on the wrong end points;
participants and coinvestigators see through our blinding schemes; the
effects of neglected and unobserved confounding factors overwhelm and
outweigh the effects of our variables of interest. With careful and pro-
longed planning, we may reduce or eliminate many potential sources of
bias, but seldom will we be able to eliminate all of them. Accept bias as
inevitable and then endeavor to recognize and report all exceptions that
do slip through the cracks.
CHAPTER 7
REPORTING YOUR RESULTS
113
N
Mean
SD
Min
Max
Change in % voting
67
0.037
0.029
−0.03
0.107
% B2
67
0.563
0.093
0.315
0.755
% B2 squared
67
0.325
0.103
0.099
0.569
% D1
67
0.507
0.084
0.288
0.712
Turnout Change
67
24236
31692
663
116327
Vote Total(K)
67
111
159
2997
714362
Median Income
67
35385
6343
26032
52244
Hispanic Population
67
0.085
0.100
0.015
0.573
Electronic Voting
67
0.224
0.420
0
1
% B2*Elec Voting
67
0.118
0.226
0
0.702
% B2*B2*Elec Voting
67
0.064
0.130
0
0.493
TABLE 7.2 Description of Variables

Most biases occur during data collection, often as a result of taking
observations from an unrepresentative subset of the population rather than
from the population as a whole. The example of the erroneous forecast of
Landon over Roosevelt was cited in Chapter 3. In Chapter 5, we consid-
ered a study that was ﬂawed because of a failure to include planes that did
not return from combat.
When analyzing extended time series in seismological and neurological
investigations, investigators typically select speciﬁc cuts (a set of consecu-
tive observations in time) for detailed analysis, rather than trying to
examine all the data (a near impossibility). Not surprisingly, such “cuts”
usually possess one or more intriguing features not to be found in run-of-
the-mill samples. Too often, theories evolve from these very biased selec-
tions. We expand on this point in Chapter 10 in our discussion of the
limitations on the range over which a model may be applied.
Limitations in the measuring instrument such as censoring at either end
of the scale can result in biased estimates. Current methods of estimating
cloud optical depth from satellite measurements produce biased results
that depend strongly on satellite viewing geometry. In this and in similar
cases in the physical sciences, absent the appropriate nomograms and con-
version tables, interpretation is impossible.
Over- and underreporting plague meta-analysis (discussed in Chapter 6).
Positive results are reported for publication, negative ﬁndings are sup-
pressed or ignored. Medical records are known to underemphasize condi-
tions such as arthritis for which there is no immediately available treatment
while overemphasizing the disease of the day. (See, for example, Callaham
et al., 1998.)
Collaboration between the statistician and the domain expert is essential
if all sources of bias are to be detected and corrected for, as many biases
are speciﬁc to a given application area. In the measurement of price
indices, for example, the three principle sources are substitution bias,
quality change bias, and new product bias.10
Two distinct kinds of statistical bias effects arise with astronomical dis-
tance indicators (DIs), depending on the method used.11
In one approach, the redshifts of objects whose DI-inferred distances are
within a narrow range of some value d are averaged. Subtracting d from the
resulting mean redshift yields a peculiar velocity estimate; dividing the mean
redshift by d gives an estimate of the parameter of interest. These estimates
will be biased because the distance estimate d itself is biased and is not the
mean true distance of the objects in question.
114
PART II
HYPOTHESIS TESTING AND ESTIMATION
10 Otmar Issing in a speech at the CEPR/ECB Workshop on issues in the measurement of
price indices, Frankfurt am Main, 16 November 2001.
11 These next paragraphs are taken with minor changes from Willick (1999, Section 9).

This effect is called homogeneous Malmquist bias. It tells us that, typically,
objects lie further away than their DI-inferred distances. The physical cause is
more objects “scatter in” from larger true distances (where there is more
volume) than “scatter out” from smaller ones.
A second sort of bias comes into play because some galaxies are too faint or
small to be in the sample; in effect, the large-distance tail of P(d|r) is cut off. It
follows that the typical inferred distances are smaller than those expected at
a given true distance r. As a result, the peculiar velocity model that allows
true distance to be estimated as a function of redshift is tricked into returning
shorter distances. This bias goes in the same sense as Malmquist bias, but is
fundamentally different.
It results not from volume/density effects, but from the same sort of
sample selection effects that were discussed earlier in this section.
Selection bias can be minimized by working in the “inverse direction.”
Rather than trying to predict absolute magnitude (Y) given a value of the
velocity width parameter (X), instead one ﬁts a line by regressing the
widths X on the magnitudes Y.
Finally, bias can result from grouping or averaging data. Bias if group
randomized trials are analyzed without correcting for cluster effects was
reported by Feng et al. (1999); see Chapter 5. The use of averaged rather
than end-of-period data in ﬁnancial research results in biased estimates of
the variance, covariance, and autocorrelation of the ﬁrst- as well as higher-
order changes. Such biases can be both time varying and persistent
(Wilson, Jones, and Lundstrum, 2001).
REPORTING POWER
Statisticians are routinely forced to guess at the values of population para-
meters in order to make the power calculations needed to determine
sample size. It’s tempting, once the data are in hand, to redo these same
power calculations. Don’t. Post hoc calculations invariably inﬂate the
actual power of the test (Zumbo and Hubley, 1998).
Post hoc power calculations can be of value in designing follow-up
studies but should not be used in reports.
DRAWING CONCLUSIONS
Found data (nonrandom samples) can be very useful in suggesting models
and hypotheses for further exploration. But without a randomized study,
formal inferential statistical analyses are not supported (Greenland, 1990;
Rothman, 1990). The concepts of signiﬁcance level, power, p-value, and
conﬁdence interval apply only to data that have arisen from carefully
designed and executed experiments and surveys.
CHAPTER 7
REPORTING YOUR RESULTS
115

A vast literature has grown up around the unease researchers feel in
placing too much reliance on p-values. Examples include Selvin (1957),
Yoccuz (1991), Badrick and Flatman (1999), Feinstein (1998), Johnson
(1999), Jones and Tukey (2000), McBride, Loftis, and Adkins (1993),
Nester (1996), Parkhurst (2001), and Suter (1996).
The vast majority of such caveats are unnecessary providing we treat p-
values as merely one part of the evidence to be used in decision-making.
They need to be viewed and interpreted in the light of all the surrounding
evidence, past and present. No computer should be allowed to make deci-
sions for you.
A failure to reject may result from insensitive or inappropriate measure-
ments, or too small a sample size.
A difference that is statistically signiﬁcant may be of no practical inter-
est. Take a large enough sample and we will always reject the null hypoth-
esis; take too small a sample and we will always accept. To say nothing of
“signiﬁcant” results that arise solely because their authors chose to test a
“null” hypothesis rather than one of practical interest. (See Chapter 4.)
Many researchers would argue that there are always three regions to
which a statistic may be assigned: acceptance, rejection, and indifference.
When a statistic falls in the latter, intermediate region it may suggest a
need for additional experiments. The p-value is only one brick in the wall;
all our other knowledge must and should be taken into consideration
(Horwitz et al., 1998).
SUMMARY
•
Provide details of power and sample size calculations.
•
Describe treatment allocation.
•
Detail exceptions including withdrawals and other sources of
missing data.
•
Use meaningful measures of dispersion.
•
Use conﬁdence intervals in preference to p-values.
•
Report sources of bias.
•
Formal statistical inference is appropriate only for randomized
studies and predetermined hypotheses.
TO LEARN MORE
The text by Lang and Secic (1997) is must reading; reporting criteria for
meta-analyses are given on p. 177ff. See Tufte (1983) on the issue of table
versus graph. For more on the geometric versus arithmetic mean see
Parkhurst (1998). For more on reporting requirements, see Begg et al.
(1996), Bailar and Mosteller (1988), Grant (1989), Altman et al. (2001;
116
PART II
HYPOTHESIS TESTING AND ESTIMATION

the revised CONSORT statement), and International Committee of
Medical Journal Editors (1997).
Mosteller (1979) and Anderson and Hauck (1986) warn against the
failure to submit reports of negative or inconclusive studies and the failure
of journal editors to accept them. To address this issue the Journal of
Negative Results in Biomedicine has just been launched at
http://www.jnrbm.com/start.asp.
On the proper role of p-values, see Neyman (1977), Cox (1977),
www.coe.tamu.edu/~bthompson, www.indiana.edu/~stigsts,
www.nprc.ucgs.gov/perm/hypotest, and Poole (1987, 2001).
To learn more about decision theory and regions of indifference, see
Duggan and Dean (1968) and Hunter and Schmidt [1997].
CHAPTER 7
REPORTING YOUR RESULTS
117
REQUESTED MANUSCRIPT FORMATS
For submission to Academic Emergency Medicine
As posted at http://www.aemj.org/misc/reqmanfor.shtml
Study Protocol
Describe the method of patient enrollment (i.e., consecutive, convenience,
random, population sampling); discuss any consent process; note any inter-
ventions used; describe any blinding or randomization regarding treat-
ments, purpose of the study, or data collection; discuss if and how
standard treatment was administered (describe such standard treatment
separate from interventions used speciﬁcally for study purposes), and
placebo speciﬁcs (how prepared, delivered) and the reasoning for such
(especially if the study is an analgesia trial).
Measurements
Discuss the data collection. Clarify who collected the data. Describe any
special data collection techniques or instruments. Provide manufacturer’s
name and address along with brand name and model number for equip-
ment used in the study. Denote what instructions or training the data col-
lectors were given.
Data Analysis
Summarize how the major outcome variables were analyzed (clearly deﬁne
outcome measures). If multiple deﬁnitions must be provided, include a sep-
arate subheading for deﬁnitions. Note which outcomes were analyzed with
which statistical tests. Clearly deﬁne any criterion standards (do not use
the phrase “gold” standard). Note any important subgroup analyses and
whether these were planned before data collection or arose after initial
data evaluation. Denote any descriptive statistics used. Provide 95% conﬁ-
dence intervals for estimates of test performance where possible; they
should be described as 95% CI = X to X. Discuss sample size estimates.
Note signiﬁcance levels used.

Chapter 8
Interpreting Reports
CHAPTER 8
INTERPRETING REPORTS
119
Common Errors in Statistics (and How to Avoid Them), 2e, by Phillip I. Good and James W. Hardin.
Copyright © 2006 John Wiley & Sons, Inc.
CHAPTER 7 WAS AIMED AT PRACTITIONERS WHO MUST PREPARE REPORTS. This
chapter is aimed at those who must read them.
WITH A GRAIN OF SALT
Critics may complain that we advocate interpreting reports not merely
with a grain of salt but with an entire shaker; so be it. Internal as well as
published reports are the basis of our thought processes, not just our pub-
lications. Neither society nor we can afford to be led down false pathways.
The Authors
Begin with the authors’ afﬁliations: Who conducted this study? What is
their personal history in conducting other studies? What personal interest
might they have in the outcome?
Who funded the study? What is their history regarding other studies,
and what is their interest in the outcome?
The Samples
What population(s) was/were sampled from? Were these the same popula-
tions to which the report(s) conclusions were applied?
Was the sample random? Stratiﬁed? Clustered? What was the survey
unit? Was the sample representative? Can you verify this from the informa-
tion provided in the report?
How was the sample size determined? Was the anticipated power stated
explicitly?
What method of allocation to subgroups was used? What type of blind-
ing was employed, if any?
Smoking is one of the leading causes of statistics.—Fletcher Knebel

With regard to surveys, what measures were taken to ensure that
responses were independent? To detect lies and careless responses? Were
nonresponders contacted and interviewed?
Are the authors attempting to compare or combine samples that 
have been collected subject to different restrictions and by different
methods?
Experimental Design
Are all potential confounding factors listed and accounted for?
Descriptive Statistics
Is all the necessary information present? Measures of dispersion (variation)
as well as measures of central tendency? Was the correct and appropriate
measure used in each instance: Mean (arithmetic? or geometric?) or
median? Standard deviation or standard error or bootstrap CI?
Are missing data accounted for? Does the frequency of missing data
vary among treatment groups?
Tests
Authors must describe which statistical test they used, report the effect
size (the appropriate measure of the magnitude of the difference, usually
the difference or ratio between groups; a conﬁdence interval would be
best), and give a measure of signiﬁcance, usually a p-value, or a conﬁdence
interval for the difference.
Can you tell which tests were used? Were they one-sided or two-sided?
How many tests? In a study by Olsen [2003] of articles in Infection and
Immunity the most common error was a failure to adjust or account for
multiple comparisons.
Was the test appropriate for the experimental design? (For example, was
a matched-pairs t-test used when the subjects were not matched?)
Note to Journal Editors: The raw data should always be available on a
website for those who may want to run their own tests.
Correlation and Regression
Always look for conﬁdence intervals about the line. If they aren’t there,
distrust the results unless you can get hold of the raw data and run the
regressions and a bootstrap validation yourself (see Chapters 12 and 13).
Graphics
Beware of missing baselines as in Figure 8.1. Be wary of extra dimensions
that inﬂate relative proportions (see Chapter 9). Distrust curves that
extend beyond the plotted data. Check to see that charts include all data
points, not just some of them.
120
PART II
HYPOTHESIS TESTING AND ESTIMATION

Conclusions
Is an attempt made to extend results beyond the populations that were
studied? Are potential biases described?
Were any of the tests and subgroup analyses performed after the data
were examined, thereby rendering the associated p-values meaningless?
And again, one must ask, were all potential confounding factors accounted
for either by blocking or by treatment as covariates? (See, for example, the
discussion of Simpson’s paradox in Chapter 10.)
Be wary of extrapolations, particularly in multifactor analyses. As the
small print reads on a stock prospectus, past performance is no guarantee
of future success.
Are nonsigniﬁcant results taken as proof of lack of effect? Are practical
and statistical signiﬁcance distinguished?
Few journals publish negative ﬁndings, so avoid concluding “most
studies show.”
CHAPTER 8
INTERPRETING REPORTS
121
25% more active ingredient
FIGURE 8.1
THE COURTS EXAMINE THE SAMPLING UNIVERSE
The U.S. Equal Employment Opportunities Commission alleged that Eagle
Iron Works assigned African-Americans to unpleasant work tasks because
of their race and discharged African-Americans in greater numbers than
Caucasians, again because of their race.1 The EEOC was able to identify
only 1200 out of 2000 past and present employees by race, although all
250 current employees could be identiﬁed. The court rejected the con-
tention that the 250 current employees were a representative sample of all
2000; it also rejected the EEOC’s unsubstantiated contention that all
unidentiﬁed former workers were Caucasian. “The lack of a satisfactory
basis for such an opinion and the obvious willingness of the witness to
attribute more authenticity to the statistics than they possessed, cast
doubts upon the value of opinions.”
The plaintiff’s survey was rejected in Bristol Meyers v. FTC,2 as there was
no follow up of the 80% who did not respond.
1 Eagle Iron Works, 424 F. Supp, 240, 246–7 (S.D. Ia. 1946).
2 185 F.2d 258 (4th Cir. 1950).

RATES AND PERCENTAGES
Consider the statement “Sixty percent of the children in New York City
read below grade level!” Some would say we can’t tell whether this per-
centage is of practical signiﬁcance without some means of comparison.
How does New York City compare with other cities its size? What about
racial makeup? What about other environmental factors compared with
other similar cities?
In the U.S. in 1985, there were 2.1 million deaths from all causes, com-
pared to 1.7 million in 1960. Does this mean it was safer to live in the U.S.
in the ’60s than in the ’80s? We don’t know the answer, because we don’t
know the relative sizes of the population of the U.S. in 1960 and 1985.
If a product had a 10% market share in 1990 and 15% today, is this a
50% increase or a 5% increase? Not incidentally, note that market share
may increase even when total sales decline.
How are we to compare rates? If a population consists of 12% African-
Americans, and a series of jury panels contain only 4%, the absolute dispar-
ity is 8%, but the comparative disparity is 66%.
In Davis v. City of Dallas,4 the court observed that a “7% difference
between 97% and 90% ought not to be treated the same as a 7% difference
between, e.g., 14% and 7%, since the latter ﬁgure indicates a much greater
degree of disparity.” Not so, for pass rates of 97% and 90% immediately
imply failure rates of 3% and 10%.
122
PART II
HYPOTHESIS TESTING AND ESTIMATION
Amstar Corporation3 claimed that “Domino’s Pizza” was too easily con-
fused with its own use of the trademark “Domino” for sugar. The U.S.
Circuit Court of Appeals found that the surveys both parties used to
support their claims were substantially defective.
“In undertaking to demonstrate likelihood of confusion in a trademark
infringement case by use of survey evidence, the appropriate universe
should include a fair sampling of those purchasers most likely to partake of
the alleged infringer’s goods or service.”
Amstar conducted and offered in evidence a survey of heads of house-
holds in 10 cities. But Domino’s Pizza had no stores or restaurants in eight
of these cities and, in the remaining two their outlets had been open less
than three months. Only women were interviewed by Amstar, and only
those women who were at home during daylight hours, that is, grocery
shoppers rather than the young and the single that compose the majority
of pizza eaters. Similarly, the court rejected Domino Pizza’s own survey
conducted in its pizza parlors. Neither plaintiff nor defendant had sampled
from a sufﬁciently complete universe.
3 205 U.S.P.Q. 128 (N.D. GA. 1979), zeuld, 615 F.2d 252 5th (iv. 1980).
4 487 F.Supp 389 (N.D. Tex 1980).

The consensus among statisticians is that one ought use the odds ratio
for such comparisons deﬁned as the percentage of successes divided by the
percentage of failures. In the present example, one would compare
97%/3% = 32.3 versus 90%/10% = 9.
INTERPRETING COMPUTER PRINTOUTS
Many of our reports come to us directly from computer printouts. Even
when we’re the ones who’ve collected the data, these reports are often a
mystery. One such report, generated by SAS PROC TTEST, is reproduced
and annotated below. We hope our annotations will inspire you to do the
same with the reports your software provides you. (Hint: Read the manual.)
First, a confession: We’ve lopped off many of the decimal places that
were present in the original report. They were redundant as the original
observations only had two decimal places. In fact, the fourth decimal place
is still redundant.
Second, we turn to the foot of the report, where we learn that a highly
signiﬁcant difference was detected between the dispersions (variances) of
the two treatment groups. We’ll need to conduct a further investigation to
uncover why this is true.
Conﬁning ourselves to the report in hand, unequal variances mean we
need to use the Satterthwaite degrees of freedom adjusted t-test for which
Pr > |t| = 0.96, that is, the values of RIG for the New and Standard treat-
ment groups are not signiﬁcantly different from a statistical point of view.
Finally, the seventh line of the report tells us that the difference in the
means of the two groups is somewhere in the interval (−0.05, +0.05). (The
report does not specify what the conﬁdence level of this conﬁdence interval
is, and we need to refer to the SAS manual to determine that it is 95%.)
CHAPTER 8
INTERPRETING REPORTS
123
The TTEST Procedure
Statistics
Lower CL
Upper CL
Lower CL
Upper CL
Var’le
treat
N
Mean
Mean
Mean
Std Dev
Std Dev
Std Dev
Std Err
RIG
New
121
0.5527
0.5993
0.6459
0.2299
0.2589
0.2964
0.0235
RIG
Stand
127
0.5721
0.598
0.6238
0.1312
0.1474
0.1681
0.0131
RIG
Diff
(1–2)
−0.051
0.0013
0.0537
0.1924
0.2093
0.2296
0.0266
T-Tests
Variable
Method
Variances
DF
t Value
Pr > |t|
RIG
Pooled
Equal
246
0.05
0.9608
RIG
Satterthwaite
Unequal
188
0.05
0.9613
Equality of Variances
Variable
Method
Num DF
Den DF
F Value
Pr > F
RIG
Folded F
120
126
3.09
<.0001

124
PART II
HYPOTHESIS TESTING AND ESTIMATION
CRITICIZING REPORTS
Commenting on an article by Rice and Grifﬁn (2004), David Hershey
(http://www.fortunecity.com/greenﬁeld/clearstreets/84/hornworm.htm)
cites the following ﬂaws:
1. Using the arithmetic average (linear interpolation) of two values that do
not fall on a straight line
2. Plotting curves without plotting the corresponding conﬁdence intervals
3. Failure to match treatment groups based on baseline data. As a result,
such factors as the weight of the subject were confounded with 
treatment
4. No explanation provided for the missing data (occasioned by the
deaths of the experimental organisms)
5. No breakdown of missing data by treatment
6. Too many signiﬁcant ﬁgures in tables and equations
7. Extrapolation leading to a physiologically impossible end point
8. Concluding that detecting a signiﬁcant difference provided conﬁrmation
of the validity of the experimental method

Chapter 9
Graphics
CHAPTER 9
GRAPHICS
125
Common Errors in Statistics (and How to Avoid Them), 2e, by Phillip I. Good and James W. Hardin.
Copyright © 2006 John Wiley & Sons, Inc.
What is the dimension of the information you will illustrate? Do you need
to illustrate repeated information for several groups? Is a graphical illustra-
tion the best vehicle for communicating information to the reader? How do
you select from a list of competing choices? How do you know whether
the graphic is effectively communicating the desired information?
GRAPHICS SHOULD EMPHASIZE AND HIGHLIGHT SALIENT FEATURES. They
should reveal data properties and coherently summarize large quantities of
information. Although graphics provide a break from dense prose, authors
must not forget that these illustrations should be scientiﬁcally informative
as well as decorative. In this chapter, we outline mistakes in selection, cre-
ation, and execution of graphics and discuss improvements for each of
these areas.
Graphical illustrations should be simple and pleasing to the eye, but the
presentation must remain scientiﬁc. In other words, we want to avoid
having too many graphical features that are purely decorative while
keeping a critical eye open for opportunities to enhance the scientiﬁc infer-
ence we expect from the reader. A good graphical design utilizes a large
proportion of the ink for communicating scientiﬁc information in the
overall display.
THE SOCCER DATA
Dr. Hardin coaches youth soccer (players of age 5), and he collected the
total number of goals for the top ﬁve teams during the eight-game Spring
2001 season. The total number of goals scored per team was 16 (Team
KISS—Keep It Simple, but Scientiﬁc
Emanuel Parzen

1), 22 (Team 2), 14 (Team 3), 11 (Team 4), and 18 (Team 5). There are
many ways we can describe this set of outcomes to the reader. In the text
above, we simply communicated the results in words.
A more effective presentation would be to write “The total number of
goals scored by Teams 1 through 5 was 16, 22, 14, 11, and 18 respec-
tively.” The College Station Soccer Club assigned the ofﬁcial team names
as Team 1, Team 2, etc. These labels show the remarkable lack of imagina-
tion that we encounter in many data collection efforts.1 Improving on this
textual presentation, we could also write “The total number of goals with
the team number identiﬁed by subscript was 222, 185, 161, 143, and 114.”
This presentation improves communication by ordering the outcomes, as
the reader will naturally want to know the order in this case.
FIVE RULES FOR AVOIDING BAD GRAPHICS
There are a number of choices in presenting the soccer outcomes in
graphical form. Many of these are poor choices; they hide information,
make it difﬁcult to discern actual values, or inefﬁciently use the space
within the graphic. Open almost any newspaper and you will see a bar
chart graphic similar in form to Figure 9.1 illustrating the soccer data. In
126
PART II
HYPOTHESIS TESTING AND ESTIMATION
1 To be fair, the children had their own informal names such as Fireballs, but not all of these
names were available at data collection time.
0
5
10
15
20
25
1
2
3
4
5
FIGURE 9.1
Total Number of Goals Scored by Teams 1 through 5. The x-
axis indicates the team number, and the y-axis indicates the number of goals scored
by the respective team.
Problem: The false third dimension makes it difﬁcult to discern values. The
number of goals for Team 3 appears to be 13 rather than the correct value of 14.

CHAPTER 9
GRAPHICS
127
22
21
20
19
18
17
16
15
14
13
12
11
10
1
2
3
4
5
FIGURE 9.2
Total Number of Goals Scored by Teams 1 through 5. The x-
axis indicates the team number, and the y-axis indicates the number of goals scored
by the respective team.
Problem: The false third dimension makes it difﬁcult to discern values.
Solution: Compute the vertical distance from the back right bottom corner of a bar
to the ﬁrst vertical value. Transfer this value to the top of the back face of a bar.
This height may then be compared to the added gridlines so that the correct value
(14) may be inferred from the graphic.
this section, we provide ﬁve important rules for generating correct graph-
ics. Subsequent sections will augment this list with other speciﬁc examples.
Figure 9.1 includes a false third dimension: a depth dimension that does
not correspond to any information in the data. Furthermore, the resulting
ﬁgure makes it difﬁcult to discern the actual values presented. Can you tell
by looking at Figure 9.1 that Team 3 scored 14 goals, or does it appear
that they scored 13 goals? The reader must focus on the top back corner
of the three-dimensional rectangle because that part of the three-
dimensional bar is (almost) at the same level as the grid lines on the plot;
actually, the reader must ﬁrst focus on the ﬂoor of the plot to initially
discern the vertical distance of the back right corner of the rectangular bar
from the corresponding grid line at the back (these are at the same
height). The viewer must then mentally transfer this difference to the top
of the rectangular bars in order to accurately infer the correct value. To
highlight the difﬁculty caused by the false third dimension, look at Figure
9.2, where we have provided additional grid lines. This plot illustrates the
previously described technique for how to infer values from this type of
graphic. The reality is that most people focus on the front face of the 
rectangle and will subsequently misinterpret this data representation.

Figure 9.3 also includes a false third dimension. As before, the resulting
illustration makes it difﬁcult to discern the actual values presented. This
illusion is further complicated by the fact that the depth dimension has
been eliminated at the top of the three-dimensional pyramids so that it’s
nearly impossible to correctly ascertain the plotted values. Focus on the
result of Team 4, compare it to the illustration in Figure 9.1, and judge
whether you think the plots are using the same data (they are).
Other types of plots that confuse the audience with false third dimen-
sions include point plots with shadows and line plots in which the data are
connected with a three-dimensional line or ribbon. The only sure-ﬁre way
to ﬁx the problems in Figure 9.3 is to include text values atop each
pyramid or a small tabular legend with the values.
The lesson from these graphics is that we must avoid illustrations 
that utilize more dimensions than exist in the data. Clearly, a better 
presentation would indicate only two dimensions, where one dimension
identiﬁes the teams and the other dimension identiﬁes the number of
goals scored.
Rule 1: Don’t produce graphics illustrating more dimensions than exist in the
data.
128
PART II
HYPOTHESIS TESTING AND ESTIMATION
0
5
10
15
20
25
1
2
3
4
5
FIGURE 9.3
Total Number of Goals Scored by Teams 1 through 5. The x-
axis indicates the team number, and the y-axis indicates the number of goals scored
by the respective team.
Problem: The false third dimension makes it difﬁcult to discern the values in the
plot. Because the back face is the most important for interpreting the values, the
fact that the decorative object comes to a point makes it impossible to correctly
read values from the plot.

Figure 9.4 is an improvement over three-dimensional displays. It is
easier to discern the outcomes for the teams, but the axis label obscures
the outcome of Team 4. Axes should be moved outside of the plotting
area with enough labels so that the reader can quickly scan the illustration
and identify values.
Rule 2: Don’t superimpose labeling information on the graphical elements of
interest. Labels can add information to the plot, but should be placed in 
(otherwise) unused portions of the plotting region.
Figure 9.5 is a much better display of the information of interest. The
problem illustrated is that there is too much empty space in the graphic.
Choosing to begin the vertical axis at zero means that about 40% of the
plotting region is empty. Unless there is a scientiﬁc reason compelling you
to include a speciﬁc baseline in the graph, the presentation should be
limited to the range of the information at hand. You can ignore this rule if
you want to include zero as the baseline to admit a relative comparison of
the values as well as an absolute comparison. Note how the symbol for
Team 2 is twice as high as the symbol for Team 4 in Figure 4, but in
Figure 9.6 this is no longer true because we eliminate the zero range of
the data. There are several instances where axis range can exceed the infor-
mation at hand, and we will illustrate those in a presentation.
CHAPTER 9
GRAPHICS
129
0
5
10
15
20
25
0
1
2
3
4
5
6
FIGURE 9.4
Total Number of Goals Scored by Teams 1 through 5. The x-
axis indicates the team number, and the y-axis indicates the number of goals scored
by the respective team.
Problem: Placing the axes inside of the plotting area effectively occludes data infor-
mation. This violates the simplicity goal of graphics. The reader should be able to
easily see all of the numeric labels in the axes and plot region.

130
PART II
HYPOTHESIS TESTING AND ESTIMATION
0
5
10
15
20
25
0
1
2
3
4
5
6
FIGURE 9.5
Total Number of Goals Scored by Teams 1 through 5. The x-
axis indicates the team number, and the y-axis indicates the number of goals scored
by the respective team.
Problem: By allowing the y-axis to range from zero, the presentation reduces the
proportion of the plotting area in which we are interested. Less than half of the
vertical area of the plotting region is used to communicate data.
10
12
14
16
18
20
22
24
0
1
2
3
4
5
6
FIGURE 9.6
Total Number of Goals Scored by Teams 1 through 5. The x-
axis indicates the team number, and the y-axis indicates the number of goals scored
by the respective team.
Problem: This graph correctly scales the y-axis but still uses a categorical variable
denoting the team on the x-axis. Labels 0 and 6 do not correspond to a team
number, and the presentation appears as if the x-axis is a continuous range of
values when in fact it is merely a collection of labels. Although this is a reasonable
approach to communicating the desired information, we can still improve on this
presentation by changing the numeric labels on the x-axis to string labels corre-
sponding to the actual team names.

Rule 3: Don’t allow the range of the axes labels to signiﬁcantly decrease the
area devoted to data presentation. Choose axis limits wisely and do not
accept default values for the axes that are far outside of the range of data
unless relative as well as absolute comparisons should be made by the reader.
Figure 9.6 eliminates the extra space included in Figure 9.5 where the
vertical axis is allowed to more closely match the range of the outcomes.
The presentation is ﬁne, but it could be made better. The data of interest
in this case involve a continuous and a categorical variable. This presenta-
tion treats the categorical variable as numeric for the purposes of organiz-
ing the display, but this is not necessary.
Rule 4: Carefully consider the nature of the information underlying the axes.
Numeric axis labels imply a continuous range of values that can be confusing
when the labels actually represent discrete values of an underlying categorical
variable.
Figures 9.6 and 9.7 are further improvements of the presentation. The
graph region, the area of the illustration devoted to the data, is illustrated
with axes that more closely match the range of the data. Figure 9.7 con-
nects the point information with a line that may help visualize the differ-
ence between the values but also indicates a nonexistent relationship; the
CHAPTER 9
GRAPHICS
131
10
12
14
16
18
20
22
24
1
2
3
4
5
FIGURE 9.7
Total Number of Goals Scored by Teams 1 through 5. The x-
axis indicates the team number, and the y-axis indicates the number of goals scored
by the respective team.
Problem: The inclusion of a polyline connecting the 5 outcomes helps the reader
to visualize changes in scores. However, the categorical values are not ordinal, and
the polyline indicates an interpolation of values that does not exist across the cate-
gorical variable denoting the team number. In other words, there is no reason that
Team 5 is to the right of Team 3 other than we ordered them that way, and there
is no Team 3.5 as the presentation seems to suggest.

horizontal axis is discrete rather than continuous. Even though these pre-
sentations vastly improve the illustration of the desired information, we are
still using a two-dimensional presentation. In fact, our data are not really
two-dimensional, and the ﬁnal illustration more accurately reﬂects the true
nature of the information.
Rule 5: Do not connect discrete points unless there is either a scientiﬁc
meaning to the implied interpolation or a collection of proﬁles for group level
outcomes.
Rules 4 and 5 are aimed at the practice of substituting numbers for
labels and then treating those numeric labels as if they were in fact
numeric. Had we included the word “Team” in front of the labels, there
would be no confusion as to the nature of the labels. Even when nomina-
tive labels are used on an axis, we must consider the meaning of values
between the labels. If the labels are truly discrete, data outcomes should
not be connected or they may be misinterpreted as implying a continuous
rather than discrete collection of values.
Figure 9.8 is an excellent, and spatially economical, illustration of the
soccer data. There are no false dimensions, the range of the graphic is close
to the range of the data, there is no difﬁculty interpreting the values indi-
cated by the plotting symbols, and the legend fully explains the material.
Table 9.1 succinctly presents the relevant information in tabular form.
Tables and ﬁgures have the advantage over in-text descriptions that the
132
PART II
HYPOTHESIS TESTING AND ESTIMATION
Team 2
Team 4
Team 3
Team 1
Team 5
10
12
14
16
18
20
22
24
FIGURE 9.8
Total Number of Goals Scored by Teams 1 through 5. The 
x-axis indicates with a square the number of goals scored by the respective team.
The associated team name is indicated above the square. Labeling the outcomes
addresses the science of the KISS speciﬁcation given at the beginning of the
chapter.
Team 4
Team 3
Team 1
Team 5
Team 2
11
14
16
18
22
TABLE 9.1 Total number of goals scored by Teams 1
through 5 ordered by lowest total to highest totala
a These totals are for the Spring 2001 season. The
organization of the table correctly sorts on the numeric
variable. That the team labels are not sorted is far less
important because these labels are merely nominal; were it
not for the fact that we labeled with integers, the team
names would have no natural ordering.

information is more easily found while scanning through the containing
document. If the information is summary in nature, we should make that
information easy to ﬁnd for the reader and place it in a ﬁgure or table. If
the information is ancillary to the discussion, it can be left in text.
Choosing Between Tabular and Graphical Presentations
In choosing between tabular and graphical presentations, there are two
issues to consider, the size (density) of the resulting graphic and the scale
of the information. If the required number of rows for a tabular presenta-
tion would require more than one page, the graphical representation is
preferred. Usually, if the amount of information is small, a table is pre-
ferred. If the scale of the information makes it difﬁcult to discern other-
wise signiﬁcant differences, a graphical presentation is better.
ONE RULE FOR CORRECT USAGE OF 
THREE-DIMENSIONAL GRAPHICS
As illustrated in the previous section, the introduction of superﬂuous
dimensions in graphics should be avoided. The prevalence of turnkey solu-
tions in software that implement these decorative presentations is alarm-
ing. At one time, these graphics were limited to business-oriented software
and presentations, but this is no longer true. Misleading illustrations are
starting to appear in scientiﬁc talks. Partly, this is due to the introduction
of business-oriented software in university service courses (usually
demanded by the served departments). Errors abound when increased
license costs for scientiﬁc and business-oriented software lead departments
to eliminate the more scientiﬁcally oriented software packages.
The reader should not necessarily interpret these statements as a
mandate to avoid business-oriented software. Many of these maligned
packages are perfectly capable of producing scientiﬁc plots. Our warning is
that we must educate ourselves in the correct software speciﬁcations.
Three-dimensional perspective plots are very effective but require speci-
ﬁcation of a viewpoint. Experiment with various viewpoints to highlight
the properties of interest. Mathematical functions lend themselves to
three-dimensional surface-type plots, whereas raw data are typically better
illustrated with contour plots. This is especially true for map data, such as
surface temperatures, or surface wind (where arrows can denote direction
and the length of the arrow can denote the strength, which effectively
adds a fourth dimension of information to the plot).
In Figures 9.9 and 9.10, we illustrate population density of children for
Harris County, Texas. Illustration of the data on a map is a natural
approach, and a contour plot reveals the pockets of dense and sparse pop-
ulations. Further contour plots of vegetation, topography, roads, and other
CHAPTER 9
GRAPHICS
133

134
PART II
HYPOTHESIS TESTING AND ESTIMATION
No. children per
region
0-1000
1000-2000
2000-3000
3000-4000
4000-5000
FIGURE 9.9
Distribution of Child Population in Harris County, Texas,
USA. The x-axis is the longitude (−96.04 to −94.78 degrees), and the y-axis is the
latitude (29.46 to 30.26 degrees).
0
500
1000
1500
2000
2500
3000
3500
4000
4500
4000-4500
3500-4000
3000-3500
2500-3000
2000-2500
1500-2000
1000-1500
500-1000
0-500
FIGURE 9.10
Population Density of the Number of Children in Harris
County, Texas, USA. The x-axis is the longitude (−96.04 to −94.78 degrees) 
and the y-axis is the latitude (29.46 to 30.26 degrees). The x-y axis is rotated 35
degrees from Figure 9.9.

information may then be sandwiched to reveal spatial dependencies among
various sources of information.
Although the contour plot in Figure 9.9 lends itself to comparison of
maps, the perspective plot in Figure 9.10 is more difﬁcult to interpret.
The surface is more clearly illustrated, but the surface itself prevents
viewing all of the data.
Rule 6: Use a contour plot over a perspective plot if a good viewpoint is not
available. Always use a contour plot over the perspective plot when the axes
denote map coordinates.
Although the contour plot is generally a better representation of
mapped data, a desire to improve Figure 9.9 would lead us to suggest that
the grid lines should be drawn in a lighter font so that they have less
emphasis than lines for the data surface. Another improvement to data
illustrated according to real-world maps is to overlay the contour plot
where certain known places or geopolitical distinctions may be marked.
The graphic designer must weigh the addition of such decorative items
with the improvement in inference that they bring.
THE MISUNDERSTOOD PIE CHART
The pie chart is undoubtedly the graphical illustration with the worst rep-
utation. Wilkinson (1999) points out that the pie chart is simply a bar
chart that has been converted to polar coordinates. Therein lies the
problem: Most humans naturally think in Cartesian coordinates.
Focusing on Wilkinson’s point makes it easier to understand that the
conversion of the bar height to an angle on the pie chart is most effective
when the bar height represents a proportion. If the bars do not have
values where the sum of all bars is meaningful, the pie chart is a poor
choice for presenting the information (cf. Fig. 9.11).
Rule 7: Do not use pie charts unless the sum of the entries is scientiﬁcally
meaningful and of interest to the reader.
On the other hand, the pie chart is an effective display for illustrating
proportions. This is especially true when we want to focus on a particular
slice of the graphic that is near 25% or 50% of the data because we
humans are adept at judging these size portions. Including the actual
value as a text element decorating the associated pie slice effectively allows
us to communicate the raw number along with the visual clue of the pro-
portion of the total that the category represents. A pie chart intended to
display information on all sections where some sections are very small is
very difﬁcult to interpret. In these cases, a table or bar chart is to be 
preferred.
CHAPTER 9
GRAPHICS
135

Additional research has addressed whether the information should be
ordered before placement in the pie chart display. There are no general
rules to follow other than to repeat that humans are fairly good at identi-
fying pie shapes that are approximately one-half or one-quarter of the total
display. As such, a good ordering of outcomes that included such approxi-
mate values would strive to place the leading edge of 25% and 50% pie
slices along one of the major north-south or east-west axes. Reordering
the set of values may lead to confusion if all other illustrations used a dif-
ferent ordering, so the graphic designer may ultimately feel compelled to
reproduce those illustrations as well.
TWO RULES FOR EFFECTIVE DISPLAY OF 
SUBGROUP INFORMATION
Graphical displays are very effective for communication of subgroup infor-
mation, for example, when we wish to compare changes in median family
income over time of African-Americans and Hispanics. With a moderate
number of subgroups, a graphical presentation can be much more effective
than a similar tabular display. Labels, stacked bar displays, or a tabular
arrangement of graphics can effectively display subgroup information.
Each of these approaches has its limits, as we will see in the following 
sections.
136
PART II
HYPOTHESIS TESTING AND ESTIMATION
16
22
14
11
18
1
2
3
4
5
FIGURE 9.11
Total Number of Goals Scored by Teams 1 through 5. The
legend indicates the team number and associated slice color for the number of
goals scored by the respective team. The actual number of goals is also included.
Problem: The sum of the individual values is not of interest so that the treatment
of the individuals as proportions of a total is not correct.

In Figure 9.12, separate connected polylines easily separate the sub-
group information. Each line is further distinguished with a different plot-
ting symbol. Note how easy it is to confuse the information because of the
inverted legend. To avoid this type of confusion, ensure that the order of
entries (top to bottom) matches that of the graphic.
Rule 8: Put the legend items in the same order they appear in the graphic
whenever possible. You may not know this order until after the graphic has
been produced, so check the consistency of this information.
Clearly, there are other illustrations that would work even better for
these particular data. When one subgroup is always greater than the other
subgroup, we can use vertical bars between each measurement instead of
two separate polylines. Using data from Table 9.2, a bar chart using sub-
groups is illustrated in Figure 9.13. Such a display not only points out the
discrepancies in the data but also allows easier inference as to whether the
discrepancy is static or changes over time. An improvement in the graphi-
cal display appears in Figure 9.14, where more emphasis on the values is
achieved by altering the scale of the vertical axis.
The construction of a table such as Table 9.2 effectively reduces the
number of dimensions from two to one. This presentation makes it more
CHAPTER 9
GRAPHICS
137
Ratio of median family income to median Anglo-American income
0.8
0.75
0.7
0.65
0.6
0.55
0.5
1974
1976
1978
1980
1982
1984
1986
1988
1990
Year
Family income ratio
African-American
Hispanic
FIGURE 9.12
Median Family Income of African-American and Hispanics
Divided by the Median Family Income for Anglo-American Families for Years
1976–1988.
Problem: The legend identiﬁes the two ethnic groups in reverse order of how they
appear in the plot. It is easy to confuse the polylines because of the discrepancy in
organizing the identiﬁers. The rule is that if the data follow a natural ordering in
the plotting region, the legend should honor that order.

difﬁcult for the reader to discern the subgroup information that the analy-
sis emphasizes. Although this organization matches the input to most sta-
tistical packages for correct analysis, it is not the best presentation for
humans to discern the groups.
Keep in mind that tables are simply text-based graphics. All of the rules
presented for graphical displays apply equally to textual displays.
138
PART II
HYPOTHESIS TESTING AND ESTIMATION
Fat
Surfactant
Volume
1
1
5.57
1
2
6.20
1
3
5.90
2
1
6.80
2
2
6.20
2
3
6.00
3
1
6.50
3
2
7.20
3
3
8.30
TABLE 9.2 Volume of a Mixture Based on the Included
Fat and Surfactant Typesa
a Problem: The two categorical variables are equally of
interest, but the table uses only one direction for
displaying the values of the categories. This demonstrates
that table generation is similar to graphics generation, and
we should apply the same graphical rules honoring
dimensions to tables.
0
2
4
6
8
10
1
2
3
Fat type
Volume
Surfactant 1
Surfactant 2
Surfactant 3
FIGURE 9.13
Volume of a Mixture Based on the Included Fat and Surfac-
tant Types.
Problem: As with a scatterplot, the arbitrary decision to include zero on the y-axis
in a bar plot detracts from the focus on the values plotted.

The proper organization of the table in two dimensions clariﬁes the sub-
group analysis. Tables may be augmented with decorative elements just as
we augment graphics. Effective additions to the table are judged on their
ability to focus attention on the science; otherwise, these additions serve
as distracters. Speciﬁc additions to tables include horizontal and vertical
lines to differentiate subgroups and font/color changes to distinguish
headings from data entries.
Specifying a y-axis that starts at zero obscures the differences of the
results and violates Rule 3 seen previously. If we focus on the actual values
of the subgroups, we can more readily see the differences.
CHAPTER 9
GRAPHICS
139
5
5.5
6
6.5
7
7.5
8
8.5
9
1
2
3
Fat type
Volume
Surfactant 1
Surfactant 2
Surfactant 3
FIGURE 9.14
Volume of a Mixture Based on the Included Fat and Surfac-
tant Types. Drawing the bar plot with a more reasonable scale clearly distin-
guishes the values for the reader.
Surfactant
1
2
3
Fat
1
5.57
6.20
5.90
2
6.80
6.20
6.00
3
6.50
7.20
8.30
TABLE 9.3 Volume of a mixture based on the included
fat and surfactant typesa
a The two categorical variables are equally of interest.
With two categorical variables, the correct approach is to
allow one to vary over rows and the other to vary over
columns. This presentation is much better than the
presentation of Table 9.2 and probably easier to interpret
than any graphical representation.

Rule 9. Use plain language in your legends and text—not “computerese.”
An example violating this rule can be seen in the working paper posted
at http://www.yuricareport.com/ElectionAftermath04/BerkeleyElec-
tion04_WP.pdf where the authors use the phrase “% Democrat Vote Esti-
mated if Electronic Voting = 0” in place of the far preferable “Estimated %
Vote for Democrats When Printed Ballots are Used.”
TWO RULES FOR TEXT ELEMENTS IN GRAPHICS
If a picture were worth a thousand words, then the graphics we produce
would considerably shorten our written reports. Although attributing “a
thousand words” for each graphic is an exaggeration, it remains true that
the graphic is often much more efﬁcient at communicating numeric infor-
mation than equivalent prose. This efﬁciency is in terms of the amount of
information successfully communicated and not necessarily any space
savings.
If the graphic is a summary of numeric information, then the caption is
a summary of the graphic. This textual element should be considered part
of the graphic design and should be carefully constructed rather than
placed as an afterthought. Readers, for their own use, often copy graphics
and tables that appear in articles and reports. Failure on the part of the
graphic designer to completely document the graphic in the caption can
result in gross misrepresentation when the graphic or table is copied and
used as a summary in another presentation or report. It is not the presen-
ter who copied the graph who suffers, but the original author who gener-
ated the graphic. Tufte (1983) advises that graphics “should be closely
integrated with the statistical and verbal descriptions of the data set,” and
the caption of the graphic clearly provides the best avenue for ensuring
this integration.
Rule 10: Captions for your graphical presentations must be complete. Do not
skimp on your descriptions.
The most effective method for writing a caption is to show the graphic
to a third party. Allow them to question the meaning and information
presented. Finally, take your explanations and write them all down as a
series of simple sentences for the caption. Readers rarely, if ever, complain
that the caption is too long. If they do complain that the caption is too
long, it is a clear indication that the graphic design is poor. Were the
graphic more effective, the associated caption would be of a reasonable
length.
Depending on the purpose of your report, editors may challenge the
duplication of information within the caption and within the text.
140
PART II
HYPOTHESIS TESTING AND ESTIMATION

Although we may not win every skirmish with those that want to abbrevi-
ate our reports, we are reminded that it is common for others to repro-
duce only tables and graphics from our reports for other purposes.
Detailed captions help alleviate misrepresentations and other out-of-
context references we certainly want to avoid. Thus we endeavor to win 
as many of these battles with editors as possible.
Other text elements that are important in graphical design are the axis
labels, title, and symbols that can be replaced by textual identiﬁers. Recog-
nizing that the plot region of the graph presents numerical data, the axis
must declare associated units of measure. If the axis is transformed (log or
otherwise), the associated label must present this information as well. The
title should be short, and it serves as the quick reference for the graphic
and associated caption. By itself, the title usually does not contain enough
information to fully interpret the graphic in isolation.
When symbols are used to denote points from the data that can be
identiﬁed by meaningful labels, there are a few choices to consider for
improving the information content of the graphic. First, we can replace 
all symbols with associated labels if such replacement results in a readable
(nonoverlapping) presentation. If our focus highlights a few key points, 
we can substitute labels for only those values.
When replacing (or decorating) symbols with labels results in an 
overlapping indecipherable display, a legend is an effective tool providing
there are not too many legend entries. Producing a graphical legend 
with 100 entries is not an effective design. It is an easy task to design
these elements when we stop to consider the purpose of the graphic. 
It is wise to consider two separate graphics when the amount of 
information overwhelms our ability to document elements in legends and
the caption.
Too many line styles or plotting points can be visually confusing and
can prevent inference on the part of the reader. You are better off splitting
the single graphic into multiple presentations when there are too many
subgroups. An ad hoc rule of thumb is to limit the number of colors or
symbols to less than eight.
Rule 11: Keep the number of line styles, colors, and symbols to a minimum.
MULTIDIMENSIONAL DISPLAYS
Representing several distinct measures for a collection of points is prob-
lematic in both text and graphics. The construction of tables for this
display is difﬁcult because of the necessity of effectively communicating
the array of subtabular information. The same is true in graphical displays,
but the distinction of the various quantities is somewhat easier.
CHAPTER 9
GRAPHICS
141

Choosing Effective Display Elements
As Cleveland and McGill (1988) emphasize, graphics involve both encod-
ing of information by the graphic designer and decoding of the informa-
tion by the reader. Various psychological properties affect the decoding of
the information in terms of the reader’s graphical perception. For example,
when two or more elements are presented, the reader will also envision
by-products such as implied texture and shading. These by-products can
be distracting and even misleading.
Graphical displays represent a choice on the part of the designer in
terms of the quantitative information that is highlighted. These decisions
are based on the desire to assist the analyst and reader in discerning per-
formance and properties of the data and associated models ﬁtted to the
data. Although many of the decisions in graphical construction simply
follow convention, the designer is still free to choose geometric shapes to
represent points, color or style for lines, and shading or textures to repre-
sent areas. Cleveland and McGill (1988) included a helpful study in which
various graphical styles were presented to readers. The ability to discern
the underlying information was measured for each style, and an ordered
list of effective elementary design choices was inferred. The ordered list
for illustrating numeric information is presented in Table 9.4. The goal 
of the list is to allow the reader to effectively differentiate among several
values.
142
PART II
HYPOTHESIS TESTING AND ESTIMATION
Rank
Graphical Elementb
1
Positions along a common scale
2
Positions along identical, nonaligned scales
3
Lengths
4
Angles
4–10a
Slopes
6
Areas
7
Volumes
8
Densities
9
Color saturations
10
Color hues
TABLE 9.4 Rank-Ordered List of Elementary Design
Choices for Conveying Numeric Information
a Slopes are given a wide range of ranks because they can
be very poor choices when the aspect ratio of the plot
does not allow distinction of slopes. Areas and volumes
introduce false dimensions to the display that prevent
readers from effective interpretation of the underlying
information.
b Graphical elements ordered from most (1) to least (10)
effective.

CHOOSING GRAPHICAL DISPLAYS
When relying completely on the ability of software to produce scientiﬁc
displays, many authors are limited by their mastery of the software. Most
software packages will allow users either to specify in advance the desired
properties of the graph or to edit the graph to change individual items in
the graph. Our ability to follow the guidelines outlined in this chapter is
directly related to the time we spend learning to use the more advanced
graphics features of software.
SUMMARY
•
Examine the data and results to determine the number of dimen-
sions in the information to be illustrated. Limit your graphic to
that many dimensions.
•
Limit the axes to exactly (or closely) match the range of data in
the presentation unless a zero axis limit admits desired relative
comparisons of the depicted values.
•
Do not connect points in a scatterplot unless there is an underly-
ing interpolation that makes scientiﬁc sense.
•
Recognize that readers of your reports will copy tables and ﬁgures
for their own use. Ensure that you are not misquoted by com-
pletely describing your graphics and tables in the associated
legends. Do not skimp on these descriptions or you force readers
to scan the entire document for needed explanations.
•
If readers are to accurately compare two different graphics for
values (instead of shapes or predominant placement of outcomes),
use the same axis ranges on the two plots.
•
Use pie charts only when there are a small number of categories
and the sum of the categorical values has scientiﬁc meaning.
•
Tables are text-based graphics. Therefore, the rules governing
organization and scientiﬁc presentation of graphics should be
honored for the tables that we present. Headings should be differ-
entiated from data entries by font weight or color change. Refrain
from introducing multiple fonts in the tables and instead use one
font in which differences are denoted in weight (boldness), style
(slanted), and size.
•
Numeric entries in tables should be in the same number of signiﬁ-
cant digits. Furthermore, they should be right justiﬁed so that
they line up and allow easy interpretation while scanning columns
of numbers.
•
Many of the charts could beneﬁt from the addition of grid lines.
Bar charts especially can beneﬁt from horizontal grid lines from
the y-axis labels. This is especially true of wider displays, but grid
lines should be drawn in a lighter shade than the lines used to
draw the major features of the graphic.
CHAPTER 9
GRAPHICS
143

•
Criticize your graphics and tables after production by isolating
them with their associated caption. Determine whether the salient
information is obvious by asking a colleague to interpret the
display. If we are serious about producing efﬁcient communicative
graphics, we must take the time to ensure that our graphics are
interpretable.
TO LEARN MORE
Wilkinson (1999) presents a formal grammar for describing graphics, but
more importantly (for our purposes), the author lists graphical element
hierarchies from best to worst. Cleveland (1995) focuses on the elements
of common illustrations where he explores the effectiveness of each
element in communicating numeric information. A classic text is Tukey
(1977), where the author lists both graphical and text-based graphical
summaries of data. More recently, Tufte (1983) and Tufte (1990) orga-
nized much of the previous work and combined that work with modern
developments. For speciﬁc illustrations, subject speciﬁc texts can be con-
sulted for particular displays in context; cf. Hardin and Hilbe (2002, p.
143–167) illustrate the use of graphics for assessing model accuracy.
144
PART II
HYPOTHESIS TESTING AND ESTIMATION

Part III
BUILDING A
MODEL

Chapter 10
Univariate Regression
CHAPTER 10
UNIVARIATE REGRESSION
147
Common Errors in Statistics (and How to Avoid Them), 2e, by Phillip I. Good and James W. Hardin.
Copyright © 2006 John Wiley & Sons, Inc.
THE SIMPLEST EXAMPLE OF A MODEL, the relationship between exactly two
variables, illustrates at least ﬁve of the many complications that can inter-
fere with the task of model building:
1. Limited scope—The model we develop may be applicable for only
a portion of the range of each variable.
2. Ambiguous form of the relationship—A variable may give rise to 
a statistically signiﬁcant linear regression without the underlying
relationship being a straight line.
3. Confounding—Undeﬁned confounding variables may create the
illusion of a relationship or may mask an existing one.
4. Assumptions—The assumptions underlying the statistical proce-
dures we use may not be satisﬁed.
5. Inadequacy—Goodness of ﬁt is not the same as prediction.
We consider each of these error sources in turn along with a series of pre-
ventive measures. Our discussion is divided into problems connected with
model selection and difﬁculties that arise during the estimation of model
coefﬁcients.
MODEL SELECTION
Limited Scope
Almost every relationship has both a linear and a nonlinear portion where
the nonlinear portion is increasingly evident for both extremely large and
Are the data adequate? Does your data set cover the entire range
of interest? Will your model depend on one or two isolated data
points?

extremely small values. One can think of many examples from physics such
as Boyle’s law, which fails at high pressures, and particle symmetries that
are broken as the temperature falls. In medicine, radioimmunoassay fails to
deliver reliable readings at very low dilutions even though for virtually
every drug there will always be an increasing portion of nonresponders as
the dosage drops. In fact, almost every measuring device—electrical, elec-
tronic, mechanical, or biological—is reliable only in the central portion of
its scale.
We need to recognize that although a regression equation may be used
for interpolation within the range of measured values, we are on shaky
ground if we try to extrapolate, to make predictions for conditions not
previously investigated. The solution is to know the range of application
and to recognize, even if we do not exactly know the range, that our
equations will be applicable to some but not all possibilities.
Ambiguous Relationships
Think why rather than what.
The exact nature of the formula connecting two variables cannot be deter-
mined by statistical methods alone. If a linear relationship exists between
two variables X and Y, then a linear relationship also exists between Y and
any monotone (nondecreasing or nonincreasing) function of X. Assume X
can only take positive values. If we can ﬁt Model I: Y = α + βX + ε to the
data, we also can ﬁt Model II: Y = α′ + β′log[X] + ε and Model III: Y =
α″ + β′X + γX2 + ε. It can be very difﬁcult to determine which model if
any is the “correct” one in either a predictive or mechanistic sense.
A graph of Model I is a straight line (see Fig. 10.1). Because Y includes
a stochastic or random component ε, the pairs of observations (x1, y1), (x2,
y2), . . . will not fall exactly on this line but above and below it. The func-
tion log[X] does not increase as rapidly as X does; when we ﬁt Model II
to these same pairs of observations, its graph rises above that of Model I
for small values of X and falls below that of Model I for large values.
Depending on the set of observations, Model II may give just as good a
ﬁt to the data as Model I.
How Model III behaves will depend upon whether β″ and α″ are both
positive or whether one is positive and the other negative. If β″ and α″ are
both positive, then the graph of Model III will lie below the graph of
Model I for small positive values of X and above it for large values. If β″ is
positive and α″ is negative, then Model III will behave more like Model
II. Thus Model III is more ﬂexible than either Models I or II and can
usually be made to give a better ﬁt to the data, that is, to minimize some
function of the differences between what is observed “yi” and what is pre-
dicted by the model “Y[xi].”
148
PART III
BUILDING A MODEL

The coefﬁcients α, β, γ for all three models can be estimated by a tech-
nique known (to statisticians) as linear regression. Our knowledge of this
technique should not blind us to the possibility that the true underlying
model may require nonlinear estimation as in
This latter model may have the advantage over the ﬁrst three in that it
ﬁts the data over a wider range of values.
Which model should we choose? At least two contradictory rules apply:
•
The more parameters the better the ﬁt; thus Models III and IV
are to be preferred.
•
The simpler, more straightforward model is more likely to 
be correct when we come to apply it to data other than the 
observations in hand; thus Models I and II are to be preferred.
Again, the best rule of all is not to let statistics do your thinking for
you, but to inquire into the mechanisms that give rise to the data and that
might account for the relationship between the variables X and Y. An
example taken from physics is the relationship between volume V and
temperature T of a gas. All of the preceding four models could be used 
to ﬁt the relationship. But only one, the model V = a + KT, is consistent
with kinetic molecular theory.
Model IV:
.
Y =
+
+
−
+
α
β
γ
δ
φ
ε
X
X
X
2
CHAPTER 10
UNIVARIATE REGRESSION
149
x
y
 Fitted values
0
2
–.2
0
.2
.4
.6
.8
1
FIGURE 10.1
A Straight Line Appears to Fit the Data.

Inappropriate Models
An example in which the simpler, more straightforward model is not
correct comes when we try to ﬁt a straight line to what is actually a
higher-order polynomial. For example, suppose we tried to ﬁt a straight
line to the relationship Y = (X −1)2 over the range X = (0, +2). We’d get
a line with slope 0 similar to that depicted in Figure 10.2. With a correla-
tion of 0, we might even conclude in error that X and Y were not related.
Figure 10.2 suggests a way we can avoid falling into a similar trap. Always
plot the data before deciding on a model.
The data in Figure 10.3 are taken from Mena et al. (1995). These
authors reported in their abstract, “The correlation . . . between IL-6 and
TNF-α was .77 . . . statistically signiﬁcant at a p-value less than .01.”
Would you have reached the same conclusion?
With more complicated models, particularly those like Model IV that
are nonlinear, it is advisable to calculate several values that fall outside the
observed range. If the results appear to defy common sense (or the laws of
physics, market forces, etc.) the nonlinear approach should be abandoned
and a simpler model utilized.
Often it can be difﬁcult to distinguish which variable is the cause and
which the effect. But if the values of one of the variables are ﬁxed in
advance, then this variable should always be treated as the so-called inde-
pendent variable or cause, the X in the equation Y = a + bX + ε. Here is
why:
When we write Y = a +bx + ε, we actually mean Y = E(Y|x) + ε, where
E(Y|X) = a + bx is the expected value of an indeﬁnite number of indepen-
150
PART III
BUILDING A MODEL
x
z
 Fitted values
0
2
–.2
0
.2
.4
.6
.8
1
FIGURE 10.2
Fitting an Inappropriate Model.

dent observations of Y when X = x. If X is ﬁxed, the inverse equation 
x = (E(x|Y) −a)/b + ε′ = makes little sense.
Nonuniqueness
Although a model may provide a good ﬁt to a set of data, one ought
refrain from inferring any causal connection. The reason is that a single
model is capable of ﬁtting many disparate data sets. Consider that one line
Y = 3 + 0.5X ﬁts the four sets of paired observations depicted in Figure
10.4 with R2 = 0.67 in each case.
The four data sets arise as follows:
X1 = X2 = X3 = c(10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5)
Y1 = c(8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68)
Y2 = c(9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74)
Y3 = c(7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73)
X4 = c(8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 19)
Y4 = c(6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 5.56, 7.91, 6.89, 12.50)
Confounding Variables
If the effects of additional variables other than X on Y are suspected, these
additional effects should be accounted for either by stratifying or by per-
forming a multivariate regression.
CHAPTER 10
UNIVARIATE REGRESSION
151
0
100
200
300
400
500
600
700
TNF-α
0
100
200
300
400
500
600
700
800
900
IL-6
FIGURE 10.3
Relation Between Two Inﬂammatory Reaction Mediators in
Response to Silicone Exposure. Data taken from Mena et al. (1995).

152
PART III
BUILDING A MODEL
0
2
4
6
8
10
12
14
0
2
4
6
8
10
12
14
0
2
4
6
8
10
12
14
16
18
20
0
2
4
6
8
10
12
14
16
18
20
0
2
4
6
8
10
12
14
16
18
20
0
2
4
6
8
10
12
14
16
18
20
0
2
4
6
8
10
12
14
0
2
4
6
8
10
12
14
FIGURE 10.4
The best ﬁtting (regression) line for each of these four
datasets is y = 3 + .5x where each regression is characterized by R2 = 0.67.
The upper left plot is a reasonable data set for which the linear model is applied.
The upper right plot illustrates a possible quadratic relationship not accounted for
by our linear model; the lower left plot demonstrates the effect of a possibly 
miscoded outcome value yielding a slope that is somewhat higher than it otherwise
would be, while the lower right plot demonstrates an outlier with very high 
leverage.
SOLVE THE RIGHT PROBLEM
Don’t be too quick to turn on the computer. Bypassing the brain to
compute by reﬂex is a sure recipe for disaster.
Be sure of your objectives for the model: Are you trying to uncover cause
and effect mechanisms? Or derive a formula for use in predictions? If the
former is your objective, standard regression methods may not be 
appropriate.
A researcher studying how neighborhood poverty levels affect violent
crime rates hit an apparent statistical roadblock. Some important crimino-
logical theories suggest that this positive relationship is curvilinear with an
accelerating slope, whereas other theories suggest a decelerating slope.
As the crime data are highly variable, previous analyses had used the loga-
rithm of the primary end point—violent crime rate—and reported a signiﬁ-
cant negative quadratic term (poverty * poverty) in their least-squares
models. The researcher felt such results were suspect, that the log trans-
formation alone might have biased the results toward ﬁnding a signiﬁcant
negative quadratic term for poverty.

Correlations can be deceptive. Variable X can have a statistically signiﬁcant
correlation with variable Y, solely because X and Y are both dependent on
a third variable Z. A fall in the price of corn is inversely proportional to
the number of hay fever cases only because the weather that produces a
bumper crop of corn generally yields a bumper crop of ragweed as well.
Even if the causal force X under consideration has no inﬂuence on the
dependent variable Y, the effects of unmeasured selective processes can
produce an apparent test effect. Children were once taught that storks
brought babies. This juxtaposition of bird and baby makes sense (at least
to a child), for where there are houses there are both families and chim-
neys where storks can nest. The bad air or miasma model (“common
sense” two centuries ago) works rather well at explaining respiratory ill-
nesses and not at all at explaining intestinal ones. An understanding of 
the role that bacteria and viruses play unites the two types of illness and
enriches our understanding of both.
We often try to turn such pseudocorrelations to advantage in our
research, using readily measured proxy variables in place of their less easily
measured “causes.” Examples are our use of population change in place of
economic growth, M2 for the desire to invest, arm cuff blood pressure
measurement in place of the width of the arterial lumen, and tumor 
size for mortality. At best such surrogate responses are inadequate (as in
attempting to predict changes in stock prices); in other instances they may
actually point in the wrong direction.
At one time, the level of CD-4 lymphocytes in the blood appeared to
be associated with the severity of AIDS; the result was that a number of
clinical trials used changes in this level as an indicator of disease status.
Reviewing the results of 16 sets of such trials, Fleming (1995) found that
the concentration of CD-4 rose to favorable levels in 13 instances even
though clinical outcomes were only favorable in 8.
CHAPTER 10
UNIVARIATE REGRESSION
153
But quadratic terms and log transforms are irrelevancies, artifacts resulting
from an attempt to squeeze the data into the conﬁnes of a linear regres-
sion model. The issue appears to be whether the rate of change of crime
rates with poverty levels is a constant, increasing, or decreasing function of
poverty levels. Resolution of this issue requires a totally different approach.
Suppose Y denotes the variable you are trying to predict and X the predic-
tor. Replace each of the y[i] by the slope y*[i] = (y[i + 1] −y[i])/(x[i + 1] −
x[i]). Replace each of the x[i] by the midpoint of the interval over which the
slope is measured, x*[i] = (x[i + 1] −x[i])/2. Use the permutation methods
described in Chapter 5 to test for the correlation if any between y* and x*.
A positive correlation means an accelerating slope, a negative correlation a
decelerating slope.

Stratiﬁcation
Gender discrimination lawsuits based on the discrepancy in pay between
men and women could be defeated once it was realized that pay was
related to years in service and that women who had only recently arrived
on the job market in great numbers simply didn’t have as many years on
the job as men.
These same discrimination lawsuits could be won once the gender com-
parison was made on a years-in-service basis, that is, when the salaries of
new female employees were compared with those of newly employed men,
the salaries of women with three years of service with those of men with
the same time in grade, and so forth. Within each stratum, men always
had the higher salaries.
If the effects of additional variables other than X on Y are suspected,
they should be accounted for either by stratifying or by performing a mul-
tivariate regression as described in Chapter 11.
The two approaches are not equivalent unless all terms are included in
the multivariate model. Suppose we want to account for the possible
effects of gender. Let I[] be an indicator function that takes the value 1 
if its argument is true and 0 otherwise. Then to duplicate the effects of
stratiﬁcation, we would have to write the multivariate model in the follow-
ing form:
In a study by Kanarek et al. (1980) whose primary focus is the relation
between asbestos in drinking water and cancer, results are stratiﬁed by sex,
race, and census tract. Regression is used to adjust for income, education,
marital status, and occupational exposure.
Liberson (1985) warns that if the strata differ in the levels of some third
unmeasured factor that inﬂuences the outcome variable, the results may be
bogus.
Simpson’s Paradox
A third omitted variable may also result in two variables appearing to be
independent when the opposite is true. Consider the following table, an
example of what is termed Simpson’s paradox:
Y =
[
] +
−[
]
(
) +
[
]
+
−[
]
(
) +
a
male
a
male
b
male
b
male
e
m
f
m
f
I
I
I
X
I
1
1
.
154
PART III
BUILDING A MODEL
Treatment Group
Control
Treated
Alive
6
20
Dead
6
20

We don’t need a computer program to tell us the treatment has no effect
on the death rate. Or does it? Consider the following two tables that
result when we examine the males and females separately:
CHAPTER 10
UNIVARIATE REGRESSION
155
Males
Control
Treated
Alive
4
8
Dead
3
5
Females
Control
Treated
Alive
2
12
Dead
3
15
In the ﬁrst of these tables, treatment reduces the male death rate from 3
of 7 to 5 of 13, or from 0.43 to 0.38. In the second, the female death
rate is reduced from 3 of 5 to 15 of 27, or from 0.60 to 0.55. Both sexes
show a reduction, yet the combined population does not. Resolution of
this paradox is accomplished by avoiding a knee-jerk response to statistical
signiﬁcance when association is involved. One needs to think deeply about
underlying cause-and-effect relationships before analyzing data. Thinking
about cause-and-effect in the preceding example might have led us to
thinking about possible sexual differences, and to stratifying the data by
sex before analyzing it.
ESTIMATING COEFFICIENTS
Write down and conﬁrm your assumptions before you begin.
In this section we consider problems and solutions associated with three
related challenges:
1. Estimating the coefﬁcients of a model
2. Testing hypotheses concerning the coefﬁcients
3. Estimating the precision of our estimates
The techniques we employ will depend on the following:
1. The nature of the regression function (linear, nonlinear, logistic)
2. The nature of the losses associated with applying the model

3. The distribution of the error terms in the model, that is, the ε’s
4. Whether these error terms are independent or dependent
The estimates we obtain will depend on our choice of ﬁtting function.
Our choice should not be dictated by the software but by the nature of
the losses associated with applying the model. Our software may specify a
least-squares ﬁt—most commercially available statistical packages do—but
our real concern may be with minimizing the sum of the absolute values
of the prediction errors or the maximum loss to which one will be
exposed. A solution is provided in Chapter 11.
In the univariate linear regression model, we assume that
where E denotes the mathematical expectation of Y given x and could be
any deterministic function of x in which the parameters appear in linear
form; ε, the error term; stands for all the other unaccounted for factors
that make up the observed value y.
How accurate our estimates are and how consistent they will be from
sample to sample will depend on the nature of the error terms. If none of
the many factors that contribute to the value of ε makes more than a small
contribution to the total, then ε will have a Gaussian distribution. If the
{εi} are independent and normally distributed (Gaussian), then the ordi-
nary least-squares estimates of the coefﬁcients produced by most statistical
software will be unbiased and have minimum variance.
These desirable properties, indeed the ability to obtain coefﬁcient values
that are of use in practical applications, will not be present if the wrong
model has been adopted. They will not be present if successive observa-
tions are dependent. The values of the coefﬁcients produced by the soft-
ware will not be of use if the associated losses depend on some function 
of the observations other than the sum of the squares of the differences
between what is observed and what is predicted. In many practical prob-
lems, one is more concerned with minimizing the sum of the absolute
values of the differences or with minimizing the maximum prediction
error. Finally, if the error terms come from a distribution that is far from
Gaussian, a distribution that is truncated, ﬂattened or asymmetric, the p-
values and precision estimates produced by the software may be far from
correct.
Alternatively, we may use permutation methods to test for the signiﬁ-
cance of the resulting coefﬁcients. Providing that the {εi} are independent
and identically distributed (Gaussian or not), the resulting p-values will be
exact. They will be exact regardless of which goodness-of-ﬁt criterion is
employed.
y
x
= (
) +
E Y
ε
156
PART III
BUILDING A MODEL

Suppose that our hypothesis is that yi = a + bxi + εi for all i and b = bo.
First, we substitute y′I = yi −boxi in place of the original observations yi.
Our translated hypothesis is y′i = a + b′xi + εi for all i and b′ = 0 or, equiv-
alently, ρ = 0, where ρ is the correlation between the variables Y′ and X.
Our test for correlation is based on the permutation distribution of the
sum of the cross-products y′ixi (Pitman, 1938). Alternative tests based on
permutations include those of Cade and Richards (1996) and MRPP LAD
regression (Mielke and Berry, 1997). For large samples, these tests are
every bit as sensitive as the least-squares test described in the previous
paragraph even when all the conditions for applying that test are satisﬁed
(Mielke and Berry, 2001, Section 5.4).
If the errors are dependent and normally distributed and the covariances
are the same for every pair of errors, then we may also apply any of the
permutation methods described above. If the errors are dependent and
normally distributed, but we are reluctant to make such a strong assump-
tion about the covariances, then our analysis may call for dynamic regres-
sion models (Pankratz, 1991).1
FURTHER CONSIDERATIONS
Bad Data
The presence of bad data can completely distort regression calculations.
When least-squares methods are employed a single outlier can inﬂuence
the entire line to pass closely to the outlier. Although a number of
methods exist for detecting the most inﬂuential observations (see, for
example, Mosteller and Tukey, 1977), “inﬂuential” does not automatically
mean that the data point is in error. Measures of inﬂuence encourage
review of data for exclusion. Statistics do not exclude data, analysts do.
And they only exclude data when presented ﬁrm evidence that the data are
in error.
The problem of bad data is particularly acute in two instances:
1. When most of the data are at one end of the line, so that a few
observations at the far end can have undue inﬂuence on the esti-
mated model
2. When there is no causal relationship between X and Y
The Washington State Department of Social and Health Services extrap-
olates its audit results on the basis of a regression of over- and under-
charges against the dollar amount of the claim. As the frequency of errors
CHAPTER 10
UNIVARIATE REGRESSION
157
1 In the SAS manual, these are called ARIMAX techniques and are incorporated in Proc
ARIMA.

depends on the amount of paper work involved and not on the dollar
amount of the claim, no linear relationship exists between overcharges and
the amount of the claim. The slope of the regression line can vary widely
from sample to sample; the removal or addition of a very few samples to
the original audit can dramatically effect the amount claimed by the state
in overcharges.
Recommended is the delete-one approach in which the regression coefﬁ-
cients are recomputed repeatedly, deleting a single pair of observations
from the original data set each time. These calculations provide conﬁdence
intervals for the estimates along with an estimate of the sensitivity of the
regression to outliers. When the number of data pairs exceeds a hundred,
a bootstrap might be used instead.
To get an estimate of the precision of the estimates and the sensitivity of
the regression equation to bad data, recompute the coefﬁcients leaving
out a different data pair each time.
Convenience
More often than we would like to admit, the variables and data that go
into our models are chosen for us. We cannot directly measure the vari-
ables we are interested in, so we make do with surrogates. But such surro-
gates may or may not be directly related to the variables of interest. Lack
of funds and or/the necessary instrumentation limit the range over which
observations can be made. Our census overlooks the homeless, the unco-
operative, the less luminous. (See, for example, City of New York v. Dept 
of Commerce;2 Bothun, 1998, Chapter 6.)
The presence of such bias does not mean we should abandon our
attempts at modeling, but that we should be aware of and report our 
limitations.
Stationarity
An underlying assumption of regression methods is that relationships
among variables remain constant during the data collection period. If not,
if the variables we are measuring undergo seasonal or other detectable
changes then we need to account for them. A multivariate approach is
called for, as described in Chapter 11.
Practical vs. Statistical Signiﬁcance
An association can be of statistical signiﬁcance without being of the least
practical value. In the study by Kanarek et al. (1980) referenced above, a
100-fold increase in asbestos ﬁber concentration is associated with perhaps
158
PART III
BUILDING A MODEL
2 822 F. Supp. 906 (E.D.N.Y., 1993).

a 5% increase in lung cancer rates. Do we care? Perhaps, for no life can be
considered unimportant. But courts traditionally have looked for at least a
two fold increase in incidence before awarding damages. (See, for
example, the citations in Chapter 6 of Good, 2001b.) And in this particu-
lar study, there is reason to believe there might be other hidden cofactors
that are at least as important as the presence of asbestos ﬁber.
Goodness-of-ﬁt vs. Prediction
As noted above, we have a choice of “ﬁtting methods.” We can minimize
the sum of the squares of the deviations between the observed and model
values, or we can minimize the sum of the absolute values of these devia-
tions, or we can minimize some entirely different function. Suppose that
we have followed the advice given above and have chosen our goodness-
of-ﬁt criterion to be identical with our loss function.
For example, suppose the losses are proportional to the square of the
prediction errors, and we have chosen our model’s parameters so as to
minimize the sum of squares of the differences yi −M[xi] for the historical
data. Unfortunately, minimizing this sum of squares is no guarantee that
when we continue to make observations we will continue to minimize the
sum of squares between what we observe and what our model predicts. If
you are a businessman whose objective is to predict market response, this
distinction can be critical.
There are at least three reasons for the possible disparity:
1. The original correlation was spurious.
2. The original correlation was genuine, but the sample was not 
representative.
3. The original correlation was genuine, but the nature of the 
relationship has changed with time (as a result of changes in the
underlying politic, market, or environment, for example). We take
up this problem again in our chapter on prediction error.
And lest we forget, association does not “prove” causation; it can only
contribute to the evidence.
Indicator Variables
The use of an indicator (yes/no) or a nonmetric ordinal variable
(improved, much improved, no change) as the sole independent (X) vari-
able is inappropriate. The 2-sample and k-sample procedures described in
Chapter 5 should be employed.
Transformations
It is often the case that the magnitude of the residual error is proportional
to the size of the observations, that is, y = E(Y|x)ε. A preliminary log
CHAPTER 10
UNIVARIATE REGRESSION
159

transformation will restore the problem to linear form log(y) = logE(Y|x)
+ ε′. Unfortunately, even if ε is normal, ε′ is not, and the resulting conﬁ-
dence intervals must be adjusted (Zhou and Gao, 1997).
Curve-Fitting and Magic Beans
Until recently, what distinguished statistics from the other branches of
mathematics was that at least one aspect of each analysis was ﬁrmly
grounded in reality. Samples were drawn from real populations, and, in
theory, one could assess and validate ﬁndings by examining larger and
larger samples taken from that same population.
In this reality-based context, modeling has one or possibly both of the
following objectives:
1. To better understand the mechanisms leading to particular
responses
2. To predict future outcomes
Failure to achieve these objectives has measurable losses. Although these
losses cannot be eliminated because of the variation inherent in the under-
lying processes, hopefully, by use of the appropriate statistical procedure,
they can be minimized.
By contrast, the goals of curve ﬁtting (nonparametric or local regres-
sion)3 are aesthetic in nature; the resultant graphs, although pleasing to
the eye, may bear little relation to the processes under investigation. To
quote Green and Silverman (1994, p. 50), “There are two aims in curve
estimation, which to some extent conﬂict with one another, to maximize
goodness-of-ﬁt and to minimize roughness.”
The ﬁrst of these aims is appropriate if the loss function is mean square
error.4 The second creates a strong risk of overﬁtting.
Validation is essential, yet most of the methods discussed in Chapter 13
do not apply. Validation via a completely independent data set cannot
provide conﬁrmation, as the new data would entail the production of a
completely different, unrelated curve. The only effective method of valida-
tion is to divide the data set in half at random, ﬁt a curve to one of the
halves, and then assess its ﬁt against the entire data set.
SUMMARY
Regression methods work well with physical models. The relevant variables
are known, and so are the functional forms of the equations connecting
160
PART III
BUILDING A MODEL
3 See, for example Green and Silverman (1994) and Loader (1999).
4 Most published methods also require that the residuals be normally distributed.

them. Measurement can be done to high precision, and much is known
about the nature of the errors—in the measurements and in the equations.
Furthermore, there is ample opportunity for comparing predictions to
reality.
Regression methods can be less successful for biological and social
science applications. Before undertaking a univariate regression, you
should have a fairly clear idea of the mechanistic nature of the relationship
(and thus the form the regression function will take). Look for deviations
from the model, particularly at the extremes of the variable range. A plot
of the residuals can be helpful in this regard; see, for example, Davison
and Snell (1991) and Hardin and Hilbe (2002, p. 143–159).
A preliminary multivariate analysis (the topic of Chapters 11 and 12)
will give you a fairly clear notion of which variables are likely to be con-
founded so that you can correct for them by stratiﬁcation. Stratiﬁcation
will also allow you to take advantage of permutation methods, which are
to be preferred in instances where “errors” or model residuals are unlikely
to follow a normal distribution.
It’s also essential that you have ﬁrmly in mind the objectives of your
analysis, and the losses associated with potential decisions, so that you can
adopt the appropriate method of goodness of ﬁt. The results of a regres-
sion analysis should be treated with care; as Freedman (1999) notes.
Even if signiﬁcance can be determined and the null hypothesis rejected or
accepted, there is a much deeper problem. To make causal inferences, it must
in essence be assumed that equations are invariant under proposed interven-
tions. . . . [I]f the coefﬁcients and error terms change when the variables on
the right hand side of the equation are manipulated rather than being pas-
sively observed, then the equation has only a limited utility for predicting the
results of interventions.
Statistically signiﬁcant ﬁndings should serve as a motivation for further cor-
roborative and collateral research rather than as a basis for conclusions.
Checklist: Write down and conﬁrm your assumptions before you begin.
•
Data cover an adequate range. Slope of line not dependent on a
few isolated values.
•
Model is plausible and has or suggests a causal basis.
•
Relationships among variables remained unchanged during the
data collection period and will remain unchanged in the near
future.
•
Uncontrolled variables are accounted for.
•
Loss function is known and will be used to determine the good-
ness of ﬁt criteria.
•
Observations are independent or the form of the dependence is
known or is a focus of the investigation.
CHAPTER 10
UNIVARIATE REGRESSION
161

•
Regression method is appropriate for the types of data involved
and the nature of the relationship.
•
Is the distribution of residual errors known?
TO LEARN MORE
David Freedman’s (1999) article on association and causation is must
reading. Lieberson (1985) has many examples of spurious association.
Friedman, Furberg, and DeMets (1998) cite a number of examples of clin-
ical trials using misleading surrogate variables.
Mosteller and Tukey (1977) expand on many of the points raised here
concerning the limitations of linear regression. Distribution-free methods
for comparing regression lines among strata are described by Good (2001,
p. 168–169).
For more on Simpson’s paradox, see
http://www.cawtech.freeserve.co.uk/simpsons.2.html. For a real-world
example, search under Simpson’s paradox for an analysis of racial bias in
New Zealand jury service at http://www.stats.govt.nz.
162
PART III
BUILDING A MODEL

Chapter 11
Alternate Methods of
Regression
CHAPTER 11
ALTERNATE METHODS OF REGRESSION
163
Common Errors in Statistics (and How to Avoid Them), 2e, by Phillip I. Good and James W. Hardin.
Copyright © 2006 John Wiley & Sons, Inc.
IN CHAPTER 10, WE FOCUSED EXCLUSIVELY ON LEAST-SQUARES LINEAR regres-
sion (OLS), both because it is the most common modeling technique and
because the limitations and caveats we outlined there apply to virtually 
all modeling techniques. But OLS is not the only modeling 
technique.
If your primary interest is in classiﬁcation or if the majority of your pre-
dictors are dichotomous, then you ought consider the use of classiﬁcation
and regression trees (CART) discussed in Chapter 12.
If distinct strata exist, one ought consider developing separate regression
models for each stratum, a technique known as ecological regression dis-
cussed in the next-to-last section of the present chapter.
If one’s primary interest is not in the expected value of the dependent
variable but in its extremes (the number of bacteria that will survive treat-
ment, the number of individuals who will fall below the poverty line),
then one ought consider the use of quantile regression.
If it is not clear which variable should be viewed as the predictor and
which the dependent variable, as is the case when evaluating two methods
of measurement, then one ought to employ Deming or errors-in-variables
(EIV) regression.
Imagine how statisticians might feel about the powerful statistics
programs that are now in our hands. It is so easy to key-in a set of
data and calculate a wide variety of statistics—regardless what
those statistics are or what they mean. There also is a need to check
that things are done correctly in the statistical analyses we
perform in our laboratories.
James O. Westgard (1998)

If one wants to diminish the effect of outliers and treat prediction errors
as proportional to their absolute magnitude rather than their squares, one
should use least absolute deviation (LAD) regression.
LINEAR VS. NONLINEAR REGRESSION
Linear regression is a much misunderstood and mistaught concept. If a
linear model provides a good ﬁt to data, this does not imply that a plot 
of the dependent variable with respect to the predictor would be a straight
line, only that a plot of the dependent variable with respect to some not
necessarily monotonic function of the predictor would be a line.
For example, y = A + Blog[x] and y = Acos(x) + Bsin(x) are both linear
models whose coefﬁcients A and B might be derived by OLS or LAD
methods. Y = Ax5 is a linear model. Y = xA is nonlinear.
LEAST ABSOLUTE DEVIATION REGRESSION
The two most popular linear regression methods for estimating model
coefﬁcients are referred to as ordinary least-squares (OLS) and least
absolute deviation (LAD) goodness of ﬁt. Because they are popular, a
wide selection of computer software is available to help us do the 
calculations.
With least-squares goodness of ﬁt, we seek to minimize the sum
where Yi denotes the variable we wish to predict and Xi the corresponding
value of the predictor on the ith occasion. With the LAD method, we
seek to minimize the sum of the absolute deviations between the observed
and the predicted value
Those who’ve taken calculus know the OLS minimum is obtained when
that is, when
b
RM
R
R
R
M
M
R
R
i
i
i
=
(
)
( )
=
−
(
)
−
(
)
−
(
)
∑
∑
covariance
variance
2
Y
Y
i
i
i
i
i
i
a
bX b
a
bX
−
−
(
) =
−
−
(
) =
∑
∑
0
0
and
,
Yi
i
i
a
bX
−
−
∑
.
Yi
i
i
a
bX
−
−
(
)
∑
2,
164
PART III
BUILDING A MODEL

and
LAD regression attempts to correct one of the major ﬂaws of OLS, that
of giving sometimes excessive weight to extreme values. The LAD method
solves for those values of the coefﬁcients in the regression equation for
which the sum of the absolute deviations Σ|yi −R[xi]| is a minimum.
Finding the LAD minimum is more complicated and requires linear
programming. But as there is plenty of commerically available software to
do the calculations for us, we needn’t worry about their complexity.
Algorithms for LAD regression are given in Barrodale and Roberts
(1973). The qreg function of Stata provides for LAD regression, as does
R’s quantreg package.
LAD regression should be used in preference to OLS (ordinary least
squares) in four circumstances:
1. To reduce the inﬂuence of outliers
2. If the losses associated with errors in prediction are additive,
rather than large errors being substantially more important than
small ones
3. If the conditional distribution of Y|X = x is not symmetric and we
wish to estimate the median of Y|X = x rather than its mean value
4. If the conditional distribution of Y|X = x is heavy in the tails
Figure 11.1 depicts systolic blood pressure as a function of age. Each
circle corresponds to a pair of observations on a single individual. The
solid line is the LAD regression line. The dotted line is the OLS regres-
sion line. A single individual, a 47 year old with a systolic blood pressure
of 220, is responsible for the difference between the two lines. Which line
do you feel it would be better to use for prediction purposes?
Drawbacks of LAD Regression
On the downside, LAD is unstable in the sense that a small change in the
data can cause a relatively large change in the ﬁtted plane. Ellis (1998)
studied plots of blood fenuramine versus blood norfenuramine levels for 
a group of 99 research subjects in a fenuramine challenge experiment. He
found that a change in the value of just one observation by a mere
1/20,000th of its interquartile range resulted in a dramatic shift in the
LAD ﬁt.
ERRORS-IN-VARIABLES REGRESSION
The need for errors-in-variables (EIV) or Deming regression is best illus-
trated by the struggles of a small medical device ﬁrm to bring its product
a
M
bR
=
−
.
CHAPTER 11
ALTERNATE METHODS OF REGRESSION
165

to market. First, they must convince regulators that their long-lasting
device provides equivalent results to those of a less efﬁcient device already
on the market. In other words, they need to show that the values V
recorded by their device bear a linear relation to the values W recorded by
their competitor, that is, that E(V) = a + bW.
But the errors inherent in measuring W (the so-called predictor) are as
large if not larger than the variation inherent in the output V of the new
device. The EIV regression method used to demonstrate equivalence
differs in two respects from that of OLS:
1. With OLS, we are trying to minimize the sum of squares 
Σ(yoi −ypi)2 where yoi is the ith observed value of Y and ypi is the
ith predicted value. With EIV, we are trying to minimize the 
sums of squares of errors, going both ways: Σ(yoi −ypi)2/VarY +
Σ(xoi −xpi)2/VarX.
2. The coefﬁcients of the EIV regression line depend on the ratio 
λ = VarX/VarY.
Unfortunately, in cases involving only single measurements by each
method, the ratio λ may be unknown and is often assigned a default value
of 1. In a simulated comparison of two electrolyte methods, Linnet
(1998) found that misspeciﬁcation of λ produced a bias that amounted to
two-thirds of the maximum bias of the OLS regression method. Standard
errors and the results of hypothesis testing also became misleading. In a
simulated comparison of two glucose methods, Linnet found that a mis-
166
PART III
BUILDING A MODEL
35
40
45
50
55
60
65
120
140
160
180
200
220
Age
SBP
FIGURE 11.1
Systolic Blood Pressure (SBP) as a Function of Age (Years).
LAD-ﬁtted line (solid) and OLS-ﬁtted line (dotted).

speciﬁed error ratio resulted only in a negligible bias. Given a short range
of values in relation to the measurement errors, it is important that λ is
correctly estimated either from duplicate sets of measurements or, in the
case of single measurement sets, speciﬁed from quality-control data. Even
with a misspeciﬁed error ratio, Linnet found that Deming regression
analysis is likely to perform better than least-squares regression analysis.
CHAPTER 11
ALTERNATE METHODS OF REGRESSION
167
WHEN DOES THIS DIFFERENCE MATTER?
When the relative errors for the two methods are similar and the correla-
tion coefﬁcient is greater than 0.8, the OLS regression slope can be
approximated as:
where ρ is the correlation coefﬁcient. This means that the regular slope
routinely underestimates the actual slope of the data. For ρ less than 0.8,
the relationship no longer is as accurate. However differences of 20% and
more continue to exist between the slopes calculated by the two methods.
For many clinical chemistry procedures ρ is greater than 0.995, and there is
very little difference between OLS and Deming regression. However, for
analytes such as electrolytes and many hematology parameters (especially
the white cells), ρ can easily be less than 0.95, and sometimes in the range
of 0.2 to 0.8. In these cases, the use of Deming statistics makes a large dif-
ference in the results.
One such example, depicted in Figure 11.2, arises when activated partial
thromboplastin time (APTT) is used to determine the correct dose of
heparin (a blood thinner). Either too much or too little heparin could seri-
ously impair a patient’s health. But which line are we to use?
ρ = (
) (
)
OLS slope
Deming slope
In practice, Stöckl, Dewitte, and Thienpont [1998] ﬁnd that it is not
the statistical model but the quality of the analytical input data that is
crucial for interpretation of method comparison studies.
Correlation versus Slope of Regression Line. Perfect correlation (ρ2 =
1) does not imply that two variables are identical but rather that one of
them, Y, say, can be written as a linear function of the other, Y = a + bX,
where b is the slope of the regression line and a is the intercept.
How Big Should the Sample Be? In method comparison studies, we
need to be sure that differences of medical importance are detected. As
discussed in Chapter 2, for a given difference, the necessary number of
samples depends on the range of values and the analytical standard devia-
tions of the methods involved.

Linnet (1999) ﬁnds that the sample sizes of 40–100 conventionally used
in method comparison studies often are inadequate. A main factor is the
range of values, which should be as wide as possible for the given analyte.
For a range ratio (maximum value divided by minimum value) of 2544
samples are required to detect one standardized slope deviation; the
number of required samples decreases to 64 at a range ratio of 10 (pro-
portional analytical error). For electrolytes having very narrow ranges of
values, very large sample sizes usually are necessary. In case of proportional
analytical error, application of a weighted approach is important to ensure
an efﬁcient analysis; for example, for a range ratio of 10, the weighted
approach reduces the requirement of samples by more than 50%.
168
PART III
BUILDING A MODEL
40
60
80
100
APTT
0.0
0.2
0.4
0.6
0.8
1.0
Heparin
FIGURE 11.2
Solid line has slope and intercept estimated by OLS. Dashed
line has slope and intercept estimated by EIV. λ = 1600.
NINE GUIDELINES
1. Use statistics to provide estimates of errors, not as indicators of
acceptability.
2. Recognize that the main purpose of the method comparison experi-
ment is to obtain an estimate of systematic error or bias.
3. Obtain estimates of systematic error at important medical decision 
concentrations.
4. When there is a single medical decision concentration, make the esti-
mate of systematic error near the mean of the data.

QUANTILE REGRESSION
Linear regression techniques (OLS, LAD, or EIV) are designed to help us
predict expected values, as in E(Y) = µ + βX. But what if our real interest
is in predicting extreme values, if, for example, we would like to character-
ize the observations of Y that are likely to lie in the upper and lower tails
of Y’s distribution. This would certainly be the case for economists and
welfare workers who want to predict the number of individuals whose
incomes will place them below the poverty line; physicians, bacteriologists,
and public health ofﬁcers who want to estimate the proportion of bacteria
that will remain untouched by various doses of an antibiotic; ecologists
and nature lovers that want to estimate the number of species that might
perish in a toxic waste spill; and industrialists and retailers who want to
know what proportion of the population might be interested in and can
afford their new product.
In estimating the τth quantile1, we try to ﬁnd that value of β for which
Even when expected values or medians lie along a straight line, other
quantiles may follow a curved path. Koenker and Hallock (2001) applied
the method of quantile regression to data taken from Ernst Engel’s study
in 1857 of the dependence of household food expenditure on household
income. As Figure 11.3 reveals, not only was an increase in food expendi-
k
k
k,
is a minimum where
if
if
ρ
β
ρ
τ
τ
τ
τ
y
f x
x
x
x
x
x
−[
]
(
)
[ ] =
>
=
−
(
)
≤
∑
,
.
0
1
0
CHAPTER 11
ALTERNATE METHODS OF REGRESSION
169
5. When there are two or more medical decision concentrations, use the
correlation coefﬁcient, r, to assess whether the range of data is ade-
quate for using ordinary regression analysis.
6. When the correlation coefﬁcient exceeds 0.975, use the comparison
plot along with ordinary linear regression statistics.
7. When the correlation coefﬁcient is close to zero, improve the data or
change the statistical technique.
8. When in doubt about the validity of the statistical technique, see
whether the choice of statistics changes the outcome or decision on
acceptability.
9. Plan the experiment carefully and collect the data appropriate for the
statistical technique to be used.
Abstracted from Westgard (1998).
1 τ is pronounced “tau.”

tures observed as expected when household income was increased, but the
dispersion of the expenditures increased also.
Some precautions are necessary. As Brian Cade notes, the most common
errors associated with quantile regression include:
1. Failing to evaluate whether the model form is appropriate, for
example, forcing linear ﬁt through an obvious nonlinear response.
(Of course, this is also a concern with mean regression, OLS,
LAD, or EIV.)
2. Trying to overinterpret a single quantile estimate (say 0.85) with a
statistically signiﬁcant nonzero slope (p < 0.05) when the majority
of adjacent quantiles (say 0.5–0.84, and 0.86–0.95) are clearly
zero (p > 0.20).
3. Failing to use all the information a quantile regression provides.
Even if you think you are only interested in relations near
maximum (say 0.90–0.99), your understanding will be enhanced
by having estimates (and sampling variation via conﬁdence inter-
vals) across a wide range of quantiles (say 0.01–0.99).
THE ECOLOGICAL FALLACY
The court wrote in NAACP v. City Of Niagara Falls, “Simple regression
does not allow for the effects of racial differences in voter turnout; it
assumes that turnout rates between racial groups are the same.”2 When-
170
PART III
BUILDING A MODEL
500
1000
1500
2000
2500
500
1000
1500
2000
household income
food expenditure
FIGURE 11.3
Dependence of Household Food Expenditure on Household
Income. Based on data gathered by Ernst Engel, 1857.
2 65 F.3d 1002, n2 (2nd Cir. 1994).

ever distinct strata exist, one ought to develop separate regression models
for each stratum. Failure to do so constitutes the ecological fallacy.
In the 2004 election for governor of the state of Washington, of the
over 2.8 million votes counted, just 261 votes separated the two leading
candidates, Christine Gregoire and Dino Rossi, with Mr. Rossi in the lead.
Two recounts later, Ms. Gregoire was found to be ahead by 129 votes.
There were many problems with the balloting, including the discovery
that some 647 felons voted despite having lost the right to vote. Borders
et al. v. King County et al. represents an attempt to overturn the results,
arguing that if the illegal votes were deducted from each precinct propor-
tional to the relative number of votes cast for each candidate, Mr. Rossi
would have won the election.
The Court ﬁnds that the method of proportionate deduction and the assump-
tion relied upon by Professors Gill and Katz are a scientiﬁcally unaccepted use
of the method of ecological inference. In particular, Professors Gill and Katz
committed what is referred to as the ecological fallacy in making inferences
about a particular individual’s voting behavior using only information about
the average behavior of groups; in this case, voters assigned to a particular
precinct. The ecological fallacy leads to erroneous and misleading results.
Election results vary signiﬁcantly from one similar precinct to another, from
one election to another in the same precinct and among different candidates
of the same party in the same precinct. Felons and others who vote illegally
are not necessarily the same as others in the precinct.
. . . The Court ﬁnds that the statistical methods used in the reports of 
Professors Gill and Katz ignore other signiﬁcant factors in determining how a
person is likely to vote. In this case, in light of the candidates, gender may be
as signiﬁcant or a more signiﬁcant factor than others. The illegal voters were
disproportionately male and less likely to have voted for the female 
candidate.3
To see how stratiﬁed regression would be applied in practice, consider 
a suit4 to compel redistricting to create a majority Hispanic district in Los
Angeles County. The plaintiffs offered in evidence two regression equa-
tions to demonstrate differences in the voting behavior of Hispanics and
non-Hispanics:
where Yhi, Yti are the predicted proportions of voters in the ith precinct
for the Hispanic candidate, and for all candidates respectively; Ch, Ct are
Y
Y
hi
h
h
hi
hi
ti
t
t
hi
ti
C
b X
C
b X
=
+
+
=
+
+
ε
ε
CHAPTER 11
ALTERNATE METHODS OF REGRESSION
171
3 Quotations are from a transcript of the decision by Chelan County Superior Court Judge
John Bridges, June 6, 2005.
4 Garza et al v. County of Los Angeles, 918 F.2d 763 (9th Cir), cert. denied.

the percentages of non-Hispanic voters who voted for the Hispanic candi-
date, any candidate; bh, bt are the added percentages of Hispanic voters
who voted for the Hispanic candidate, any candidate, Xhi is the percentage
of registered voters in the ith precinct who are Hispanic; and εhi, εti are
random or otherwise unexplained ﬂuctuations.
If there were no differences in the voting behavior of Hispanics and
non-Hispanics, then we would expect our estimates of bh, bt to be close 
to zero. Instead, the plaintiffs showed that the best ﬁt to the data was
provided by the equations
Of course, other estimates of the C’s and b’s are possible, as only the
X’s and Y’s are known with certainty; it is conceivable, though unlikely,
that few if any of the Hispanics actually voted for the Hispanic candidate.
NONSENSE REGRESSION
Nonlinear regression methods are appropriate when the form of the non-
linear model is known in advance. For example, a typical pharmacological
model will have the form Aexp[bX]+ Cexp[dW]. The presence of numer-
ous locally optimal but globally suboptimal solutions creates challenges,
and validation is essential. See, for example, Gallant (1987) and Carroll 
et al. (1995).
To be avoided are a recent spate of proprietary algorithms available solely
in software form that guarantee to ﬁnd a best-ﬁtting solution. In the words
of John von Neumann, “With four parameters I can ﬁt an elephant and
with ﬁve I can make him wiggle his trunk.” Goodness of ﬁt is no guarantee
of predictive success, a topic we take up repeatedly in subsequent chapters.
SUMMARY
In this chapter, we distnguished linear from nonlinear regression and
described a number of alternatives to ordinary least-squares (OLS) regres-
sion, including least absolute deviation (LAD) regression, and quantile
regression. We also noted the importance of using separate regression
equations for each identiﬁable stratum.
TO LEARN MORE
Consider using LAD regression when analyzing software data sets
(Miyazaki et al., 1994) or meteorological data (Mielke et al., 1996).
Y
Y
h
h
t
h
X
X
=
+
=
−
7 4
110
42 5
048
. %
.
. %
.
172
PART III
BUILDING A MODEL

Only iteratively reweighed general Deming regression produces statisti-
cally unbiased estimates of systematic bias and reliable conﬁdence intervals
of bias. For details of the recommended technique, see Martin (2000).
Mielke and Berry (2001, Section 5.4) provide a comparison of MRPP,
Cade–Richards, and OLS regression methods. Stöckl et al. (1998)
compare ordinary linear regression, Deming regression, standardized 
principal component analysis, and Passing–Bablok regression.
For more on quantile regression, download Blossom and its 
accompanying manual from http://www.fort.usgs.gov/products/
software/blossom/blossom.asp. Stata software also has a number of 
routines for quantile regression; see http://www.stata.com. For R code 
to implement any of the preceding techniques, see Good (2005).
CHAPTER 11
ALTERNATE METHODS OF REGRESSION
173

Chapter 12
Multivariable Regression
CHAPTER 12
MULTIVARIABLE REGRESSION
175
Common Errors in Statistics (and How to Avoid Them), 2e, by Phillip I. Good and James W. Hardin.
Copyright © 2006 John Wiley & Sons, Inc.
CAVEATS
Multivariable regression is plagued by the same problems univariate
regression is heir to, plus many more of its own. Is the model correct? Are
the associations spurious?
In the univariate case, if the errors were not normally distributed, we
could take advantage of permutation methods to obtain exact signiﬁcance
levels in tests of the coefﬁcients. Exact permutation methods do not exist
in the multivariable case.
When selecting variables to incorporate in a multivariable model, we are
forced to perform repeated tests of hypotheses, so that the resultant p-
values are no longer meaningful. One solution, if sufﬁcient data are avail-
able, is to divide the data set into two parts, using the ﬁrst part to select
variables and the second part to test these same variables for signiﬁcance.
If choosing the correct functional form of a model in a univariate case
presents difﬁculties, consider that in the case of k variables, there are k
linear terms (should we use logarithms? should we add polynomial terms?)
and k(k −1) ﬁrst-order cross-products of the form xixk. Should we include
any of the k(k −1)(k −2) second-order cross-products?
Should we use forward stepwise regression? Or backward? Or some
other method for selecting variables for inclusion? The order of selection
can result in major differences in the ﬁnal form of the model (see, for
example, Roy, 1958 and Goldberger, 1961).
David Freedman (1983) searched for and found a large and highly sig-
niﬁcant R2 among totally independent normally distributed random vari-
ables. This article is reproduced in its entirety in Appendix A, and we urge
you to read this material more than once. What led him to such an experi-

ment in the ﬁrst place? How could he possibly have guessed at the results
he would obtain?
The Freedman article demonstrates how testing multiple hypotheses, a
process that typiﬁes the method of stepwise regression, can only exacer-
bate the effects of spurious correlation. As he notes in the introduction to
the article, “If the number of variables is comparable to the number of
data points, and if the variables are only imperfectly correlated among
themselves, then a very modest search procedure will produce an equation
with a relatively small number of explanatory variables, most of which
come in with signiﬁcant coefﬁcients, and a highly signiﬁcant R2. Such an
equation is produced even if Y is totally unrelated to the X’s.”
Freedman used computer simulation to generate 5100 independent nor-
mally distributed random “observations.” He put these observations into a
data matrix in the format required by the SAS regression procedure. His
organization of the values deﬁned 100 “observations” on each of 51
random variables. Arbitrarily, the ﬁrst 50 variables were designated as
“explanatory” and the 51st as the dependent variable Y.
In the ﬁrst of two passes through the “data,” all 50 of the explanatory
variables were used: 15 coefﬁcients of the 50 were signiﬁcant at the 25%
level, and one of the 50 was signiﬁcant at the 5% level.
Focusing attention on the “explanatory” variables that proved signiﬁcant
on the ﬁrst pass, a second model was constructed using only those vari-
ables. The resulting model had an R2 of 0.36, and the model coefﬁcients
of six of the “explanatory” (but completely unrelated) variables were sig-
niﬁcant at the 5% level. Given these ﬁndings, how can we be sure whether
the statistically signiﬁcant variables we uncover in our own research regres-
sion models are truly explanatory or are merely the result of chance?
A partial answer may be found in an article by Gail Gong published in
1986 and reproduced in its entirety in Appendix B.
Gail Gong was among the ﬁrst, if not the ﬁrst, student to have the
bootstrap as the basis of her doctoral dissertation. Reading her article,
reprinted here with the permission of the American Statistical Association,
we learn that the bootstrap can be an invaluable tool for model validation,
a result we explore at greater length in Chapter 13. We also learn not to
take for granted the results of a stepwise regression.
Gong (1986) constructed a logistic regression model based on observa-
tions Peter Gregory made on 155 chronic hepatitis patients, 33 of whom
died. The object of the model was to identify patients at high risk. In 
contrast to the computer simulations David Freedman performed, the 19
explanatory variables were real, not simulated, derived from medical histo-
ries, physical examinations, X rays, liver function tests, and biopsies.
If one or more extreme values can inﬂuence the slope and intercept of a
univariate regression line, think how much more impact, and how subtle
176
PART III
BUILDING A MODEL

the effect, these values might have on a curve drawn through 20-dimen-
sional space.1
Gong’s logistic regression models were constructed in two stages. At
the ﬁrst stage, each of the explanatory varlables was evaluated on a univari-
ate basis. Thirteen of these variables proved signiﬁcant at the 5% level
when applied to the original data. A forward multiple regression was
applied to these 13 variables, and 4 were selected for use in the predictor
equation.
When she took bootstrap samples of the 155 patients, the R2 values of
the ﬁnal models associated with each bootstrap sample varied widely. Not
reported in this article, but far more important, is that although two of
the original four predictor variables always appeared in the ﬁnal model
derived from a bootstrap sample of the patients, ﬁve other variables were
incorporated in only some of the models.
We strongly urge you to adopt Dr. Gong’s bootstrap approach to vali-
dating multivariable models. Retain only those variables that appear consis-
tently in the bootstrap regression models. Additional methods for model
validation are described in Chapter 13.
Correcting for Confounding Variables
When your objective is to verify the association between predetermined
explanatory variables and the response variable, multiple linear regression
analysis permits you to provide for one or more confounding variables that
could not be controlled otherwise.
Keep It Simple
It is always best to keep things simple: Fools rush in where angels fear to
tread. Multivariate regression should be attempted only for exploratory
purposes (hoping to learn more from fewer observations) or as the ﬁnal
stage in a series of modeling attempts of increasing complexity.
After the 2004 Presidential elections in the United States, critics noted
that (1) the ﬁnal tabulated results differed sharply from earlier exit polls
and (2) many of the more ﬂagrant discrepancies occurred when ballots
were recorded electronically rather than via a paper ballot (Loo, 2005).
The straightforward way to prove or disprove that the electronically
recorded ballots were tampered with would be to compare the discrepan-
cies between the exit polls and the ﬁnal tabulations of precincts that
recorded ballots solely by electronic means with the discrepancies observed
in a matched set of case controls selected from precincts where paper
ballots were used. Surprisingly, Hout et al. (2005) chose instead to build a
CHAPTER 12
MULTIVARIABLE REGRESSION
177
1 That’s 1 dimension for risk of death, the dependent variable, and 19 for the explanatory
variables.

multivariate regression model in which the dependent variable was the
2004 ﬁnal count for Bush and the independent variables included the
2000 ﬁnal count for Bush, the square of this count, the 1996 ﬁnal count
for Dole, the change in voter turnout, the median income, the proportion
of the population that was Hispanic, and whether or not electronic voting
machines were used.
FACTOR ANALYSIS
“The procedures that are involved in factor analysis (FA) as used by psycholo-
gists today have several features in common with the procedures for adminis-
tering Rorschach inkblots. In both procedures, data are ﬁrst gathered
objectively and in quantity; subsequently, the data are analysed according to
rational criteria that are time-honoured while not fully understood. . . .”
Chris Brand
Alas, the ad hoc nature of factor analysis is such that one cannot perform
the analysis without displeasing somebody. For example, although one
group of researchers might argue that a majority of variables should end
up identiﬁed principally with just one factor, an equally vociferous opposi-
tion consider it folly to break up clear g factors by an obsessional search
for simple structure.
The sole advice we can offer is that a factor analysis ought be given the
same scrutiny as any other modeling procedure and validated as described
in Chapter 13.
GENERALIZED LINEAR MODELS
Today, most statistical software incorporates new advanced algorithms for
the analysis of generalized linear models (GLMs)2 and extensions to panel
data settings including ﬁxed-, random- and mixed-effects models, logistic-,
Poisson, and negative-binomial regression, generalized estimating equation
models (GEEs), and hierarchical linear models (HLMs). These models
take the form Y = g−1[βX] + ε, where β is a vector of to-be-determined
coefﬁcients, X is a matrix of explanatory variables, and ε is a vector of
identically distributed random variables. These variables may be normal, or
gamma, or Poisson depending on the speciﬁed variance of the GLM. The
nature of the relationship between the outcome variable and the coefﬁ-
cients depends on the speciﬁed link function g of the GLM. Panel data
models include the following:
178
PART III
BUILDING A MODEL
2 As ﬁrst deﬁned by Nelder and Wedderburn (1972).

Fixed effects. An indicator variable for each subject is added and used to
ﬁt the model. Although often applied to the analysis of repeated measures,
this approach has bias that increases with the number of subjects. If data
include a very large number of subjects, the associated bias of the results
can make this a very poor model choice.
Conditional ﬁxed effects are commonly applied in logistic regression,
Poisson regression, and negative binomial regression. A sufﬁcient statistic
for the subject effect is used to derive a conditional likelihood such that
the subject level effect is removed from the estimation.
Although conditioning out the subject level effect in this manner is
algebraically attractive, interpretation of model results must continue to 
be in terms of the conditional likelihood. This may be difﬁcult, and the
analyst must be willing to alter the original scientiﬁc questions of interest
to questions in terms of the conditional likelihood.
Questions always arise as to whether some function of the independent
variable might be more appropriate to use than the independent variable
itself. For example, suppose X = Z 2, where E(Y|Z) satisﬁes the logistic
equation; then E(Y|X) does not.
Random effects. The choice of a distribution for the random effect is
driven too often by the need to ﬁnd an analytic solution to the problem,
rather than by any scientiﬁc foundation. If we assume a normally distrib-
uted random effect when the random effect is really Laplace, we will
obtain the same point estimates (because both distributions have mean
zero), but we will compute different standard errors. We will not have any
way of comparing the approaches short of ﬁtting both models.
If the true random-effects distribution has a nonzero mean, then the
misspeciﬁcation is more troublesome as the point estimates of the ﬁtted
model are different from those that would be obtained from ﬁtting the
true model. Knowledge of the true random-effects distribution does not
alter the interpretation of ﬁtted model results. Instead, we are limited to
discussing the relationship of the ﬁtted parameters to those parameters we
would obtain if we had access to the entire population of subjects and we
ﬁt that population to the same ﬁtted model. In other words, even given
the knowledge of the true random effects distribution, we cannot easily
compare ﬁtted results to true parameters.
As discussed in Chapter 5 with respect to group-randomized trials, if
the subjects are not independent (say, they all come from the same class-
room) then the true random effect is actually larger. The attenuation of
our ﬁtted coefﬁcient increases as a function of the number of supergroups
containing our subjects as members; if classrooms are within schools and
there is within-school correlation, the attenuation is even greater.
CHAPTER 12
MULTIVARIABLE REGRESSION
179

GEE (Generalized Estimating Equation). Instead of trying to derive the
estimating equation for GLM with correlated observations from a likeli-
hood argument, the within-subject correlation is introduced directly into
the estimating equation of an independence model. The correlation para-
meters are then nuisance parameters and can be estimated separately. (See
also Hardin and Hilbe, 2003.)
Underlying the population-averaged GEE is the assumption that one is
able to specify the correct correlation structure. If one hypothesizes an
exchangeable correlation and the true correlation is time dependent, the
resulting regression coefﬁcient estimator is inefﬁcient. The naive variance
estimates of the regression coefﬁcients will then produce incorrect conﬁ-
dence intervals. Analysts specify a correlation structure to gain efﬁciency in
the estimation of the regression coefﬁcients, but they typically calculate
the sandwich estimate of variance to protect against misspeciﬁcation of the
correlation.3 This variance estimator is more variable than the naive vari-
ance estimator, and many analysts do not pay adequate attention to the
fact that the asymptotic properties depend on the number of subjects (not
the total number of observations).
HLM includes hierarchical linear models, linear latent models, and others.
Although previous models are limited for the most part to a single effect,
HLM allows more than one. Unfortunately, most commercially available
software requires one to assume that each random effect is Gaussian with
mean zero. The variance of each random effect must be estimated.
Mixed models allow both linear and nonlinear mixed-effects regression
(with various links). They allow you to specify each level of repeated 
measures. Imagine districts: schools: teachers: classes: students. In this
description, each of the sublevels is within the previous level and we can
hypothesize a ﬁxed or random effect for each level. We also imagine 
that observations within same levels (any of these speciﬁc levels) are 
correlated.
The caveats revealed in this and Chapter 11 apply to the GLMs. The
most common sources of error are the use of an inappropriate or erro-
neous link function, the wrong choice of scale for an explanatory variable
(for example, using x rather than log[x]), neglecting important variables,
and the use of an inappropriate error distribution when computing conﬁ-
dence intervals and p-values. Firth (1991, p. 74–77) should be consulted
for a more detailed analysis of potential problems.
180
PART III
BUILDING A MODEL
3 See Hardin and Hilbe (2003, p. 28) for a more detailed explanation.

REPORTING YOUR RESULTS
In reporting the results of your modeling efforts you need to be explicit
about the methods used, the assumptions made, the limitations on your
model’s range of application, potential sources of bias, and the method of
validation (see Chapter 13). The section on “Limitations Of The Logistic
Regression” from Bent and Archﬁeld (2002) is ideal in this regard:
The logistic regression equation developed is applicable for stream sites with
drainage areas between 0.02 and 7.00mi2 in the South Coastal Basin and
between 0.14 and 8.94mi2 in the remainder of Massachusetts, because these
were the smallest and largest drainage areas used in equation development
for their respective areas. [The authors go on to subdivide the area.]
The equation may not be reliable for losing reaches of streams, such as for
streams that ﬂow off area underlain by till or bedrock onto an area underlain
by stratiﬁed-drift deposits (these areas are likely more prevalent where hill-
sides meet river valleys in central and western Massachusetts). At this junc-
ture of the different underlying surﬁcial deposit types, the stream can lose
stream ﬂow through its streambed. Generally, a losing stream reach occurs
where the water table does not intersect the streambed in the channel (water
table is below the streambed) during low-ﬂow periods. In these reaches, the
equation would tend to overestimate the probability of a stream ﬂowing
perennially at a site.
The logistic regression equation may not be reliable in areas of Massachusetts
where ground-water and surface-water drainage areas for a stream site differ.
[The authors go on to provide examples of such areas.]
In these areas, ground water can ﬂow from one basin into another; therefore,
in basins that have a larger ground-water contributing area than the surface-
water drainage area the equation may underestimate the probability that
stream is perennial. Conversely, in areas where the ground-water contributing
area is less than the surfacewater-drainage area, the equation may overesti-
mate the probability that a stream is perennial.
This report by Bent and Archﬁeld also illustrates how data quality, selec-
tion, and measurement bias can restrict a model’s applicability.
The accuracy of the logistic regression equation is a function of the quality of
the data used in its development. This data includes the measured perennial
or intermittent status of a stream site, the occurrence of unknown regulation
above a site, and the measured basin characteristics.
The measured perennial or intermittent status of stream sites in Massachu-
setts is based on information in the USGS NWIS database. Stream-ﬂow mea-
sured as less than 0.005ft3/s is rounded down to zero, so it is possible that
several streamﬂow measurements reported as zero may have had ﬂows less
than 0.005ft3/s in the stream. This measurement would cause stream sites to
be classiﬁed as intermittent when they actually are perennial.
Additionally, of the stream sites selected from the NWIS database, 61 of 62
intermittent-stream sites and 89 of 89 perennial-stream sites were repre-
CHAPTER 12
MULTIVARIABLE REGRESSION
181

sented as perennial streams on USGS topographic maps; therefore, the
Statewide database (sample) used in development of the equation may not
be random, because stream sites often selected for streamﬂow measurements
are represented as perennial streams on USGS topographic maps. Also, the
drainage area of stream sites selected for streamﬂow measurements generally
is greater than about 1.0mi2, which may result in the sample not being
random.
The observed perennial or intermittent status of stream sites in the South
Coastal Basin database may also be biased, because the sites were measured
during the summer of 1999. The summer of 1999 did not meet the deﬁnition
of an extended drought; but monthly precipitation near the South Coastal
Basin was less than 50 percent of average in April, less than 25 percent of
average in June, about 75 percent of average in July (excluding one station),
and about 50 percent of average in August (excluding one station). Addition-
ally, Socolow and others (2000) reported streamﬂows and ground-water levels
well below normal throughout most of Massachusetts during the summer of
1999. Consequently, stream sites classiﬁed as intermittent would have been
omitted from the database had this period been classiﬁed as an extended
drought. This climatic condition during the summer of 1999 could bias the
logistic regression equation toward a lower probability of a stream site being
considered perennial in the South Coastal Basin.
Basin characteristics of the stream sites used in the logistic equation develop-
ment are limited by the accuracy of the digital data layers used. In the future,
digital data layers (such as hydrography, surﬁcial geology, soils, DEMs, and
land use) will be at lower scales, such as 1:5,000 or 1:25,000. This would
improve the accuracy of the measured basin characteristics used as 
explanatory variables to predict the probability of a stream ﬂowing 
perennially.
For this study, the area of stratiﬁed-drift deposits and consequently the 
areal percentage of stratiﬁed-drift deposits included areas with sand and
gravel, large sand, ﬁne-grained, and ﬂoodplain alluvium deposits. Future
studies would allow more speciﬁcity in testing the areal percentage of 
surﬁcial deposits as explanatory variables. For example, the areal percentage
of sand and gravel deposits may be an important explanatory variable for
estimating the probability that a stream site is perennial. The accuracy of 
the logistic regression equation also may be improved with the testing of
additional basin characteristics as explanatory variables. These explanatory
variables could include areal percentage of wetlands (forested and non-
forested), areal percentage of water bodies, areal percentage of forested
land, areal percentage of urban land, or mean, minimum, and maximum basin
elevation.
A CONJECTURE
A great deal of publicity has heralded the arrival of new and more power-
ful data mining methods, among them neural networks along with dozens
of unspeciﬁed proprietary algorithms. In our limited experience, none of
these have lived up to expectations; see a report of our tribulations in
182
PART III
BUILDING A MODEL

Good (2001, Section 7.6). Most of the experts we’ve consulted have
attributed this failure to the small size of our test data set, 400 observa-
tions each with 30 variables. In fact, many publishers of data mining soft-
ware assert that their wares are designed solely for use with terabytes of
information
This observation has led to our putting our experience in the form of
the following conjecture:
If m points are required to determine a univariate regression line with
sufﬁcient precision, then it will take at least mn observations and perhaps
n!mn observations to appropriately characterize and evaluate a regression
model with n variables.
DECISION TREES
As the number of potential predictors increases, the method of linear
regression becomes less and less practical. With three potential predictors,
we can have as many as seven coefﬁcients to be estimated: one for the
intercept, three for ﬁrst-order terms in the predictors Pi, two for second-
order terms of the form PiPj, and one third-order term P1P2P3. With k-
variables, we have k ﬁrst-order terms, k(k −1) second-order terms, and 
so forth. Should all these terms be included in our model? Which ones
should be neglected? With so many possible combinations, will a single
equation be sufﬁcient?
We need to consider alternate approaches. If you’re a mycologist, a
botanist, a herpetologist, or simply a nature lover you may have made use
of some sort of a key. For example,
1. Leaves simple?
A. Leaves needle-shaped?
a. Leaves in clusters of 2 to many?
i. Leaves in clusters of 2 to 5, sheathed, persistent for
several years?
Which is to say that one classiﬁes objects according to whether they
possess a particular characteristic. One could accomplish the same result by
means of logistic regression, but the latter seems somewhat contrived.
The classiﬁcation and regression tree (CART) proposed by Brieman and
Friedman is simply a method of automating the process of classiﬁcation, so
that the initial bifurcation, “Leaves simple” in the preceding example, pro-
vides the most effective division of the original sample, and so on. CART
can also be used for the purpose of regression as well as classiﬁcation as
depicted in Figure 12.1.
CART offers two other advantages over multiple regression:
CHAPTER 12
MULTIVARIABLE REGRESSION
183

1. If known, we can weight the result in terms of the proportions of
the various categories in the population at large rather than those
in the sample.
2. We can assign losses or penalties on a one-by-one basis to each
speciﬁc type of misclassiﬁcation, rather than making use of some
potentially misleading aggregate measure such as least-square
error.
Alas, regression trees are also known for their instability (Breiman,
1996). A small change in the training set (see Chapter 13) may lead to a
different choice when building a node, which in turn may represent a dra-
matic change in the tree, particularly if the change occurs in top level
nodes. Branching is also affected by data density and sparseness, with
more branching and smaller bins in data regions where data points are
dense. Moreover, in contrast to the smoothness of an OLS regression
curve, the jagged approximation provided by CART has marked 
discontinuities.
184
PART III
BUILDING A MODEL
      Node 1
RM
Aug = 22.533
W = 506.000
    N = 506
      Node 2
LSTAT
Aug = 19.934
W = 430.000
    N = 430
      Node 3
DIS
Aug = 23.350
W = 255.000
     N = 255
      Node 4
RM
Aug = 22.905
W = 250.000
     N = 250
      Node 5
LSTAT
Aug = 21.630
W = 195.000
     N = 195
      Node 6
TAX
Aug = 27.427
W = 55.000
     N = 55
      Node 8
NOX
Aug = 17.138
W = 101.000
     N = 101
      Node 9
NOX
Aug = 11.978
W = 74.000
     N = 74
      Node 7
CRIM
Aug = 14.955
W = 175.000
     N = 175
     Node 12
CRIM
Aug = 32.113
W = 46.000
     N = 46
     Node 11
RM
Aug = 37.238
W = 76.000
     N = 76
     Node 13
DIS
Aug = 33.349
W = 43.000
     N = 43
     Node 10
LSTAT
Aug = 11.077
W = 62.000
     N = 62
     Node 14
CRIM
Aug = 45.0
W = 30.000
     N = 30
Terminal
Node 11
W = 2.000
Terminal
Node 12
W = 41.000
Terminal
Node 13
W = 3.000
Terminal
Node 14
W = 29.000
Terminal
Node 10
W = 44.000
Terminal
Node 9
W = 18.000
Terminal
Node 8
W = 12.000
Terminal
Node 7
W = 77.000
Terminal
Node 6
W = 24.000
Terminal
Node 5
W = 38.000
Terminal
Node 4
W = 17.000
Terminal
Node 3
W = 152.000
Terminal
Node 2
W = 43.000
Terminal
Node 1
W = 5.000
FIGURE 12.1
Part of a CART Decision Tree for Predicting Median House
Price.
REPEATED OBSERVATIONS
Be wary when developing models based on repeated observations on indi-
viduals. If your software is permitted to do its own random partitioning
you can get wildly optimistic performance results.
To avoid this, assign the individual not the record to a partition, so that all
records belonging to that individual are either all “train” or “test”.

For quantitative prediction, both regression methods and decision trees
have problems. Unthinking use of either approach results in overﬁtting.
With decision trees, this translates into branching rules that seem arbitrary
and unrelated to any theory of causation among the variables. This com-
plexity can be compensated for in part by developing the tree for one set
of data, then cross-validating it on another as described in Chapter 13.
BUILDING A SUCCESSFUL MODEL
“Rome was not built in one day,”4 nor was any reliable model. The only
successful approach to modeling lies in a continuous cycle of hypothesis
formulation—data gathering—hypothesis testing and estimation. How you
navigate through this cycle will depend on whether you are new to the
ﬁeld, have a small data set in hand and are willing and prepared to gather
more until the job is done, or have access to databases containing 
hundreds of thousands of observations. The following prescription, while
directly applicable to the latter case, can be readily modiﬁed to ﬁt any 
situation.
1. A thorough literature search and an understanding of casual mech-
anisms is an essential prerequisite to any study. Don’t let the soft-
ware do your thinking for you.
2. Using a subset of the data selected at random, see which variables
appear to be correlated with the dependent variable(s) of interest.
(As noted in this chapter and Chapter 11, two unrelated variables
may appear to be correlated by chance alone or as a result of con-
founding factors. For the same reasons, two closely related factors
may fail to exhibit a statistically signiﬁcant correlation.)
3. Using a second, distinct subset of the data selected at random, see
which of the variables selected at the ﬁrst stage still appear to be
correlated with the dependent variable(s) of interest. Alternately,
use the bootstrap method describe by Gong (1986) to see which
variables are consistently selected for inclusion in the model.
4. Limit attention to one or two of the most signiﬁcant predictor
variables. Select a subset of the existing data in which the remain-
der of the signiﬁcant variables are (almost) constant. (Alternately,
gather additional data for in which the remainder of the signiﬁ-
cant variables are almost constant.) Decide on a generalized linear
model form that best ﬁts your knowledge of the causal relations
among the few variables on which you are now focusing. (A stan-
dard multivariate linear regression may be viewed as just another
form, albeit a particularly straightforward one, of generalized
linear model.) Fit this model to the data.
CHAPTER 12
MULTIVARIABLE REGRESSION
185
4 John Heywood, Proverbes. Part i. Chap. xi., 16th Century.

5. Select a second subset of the existing data (or gather an additional
data set) for which the remainder of the signiﬁcant variables are
(almost) equal to a second constant. For example, if only men
were considered at stage 4, then you should focus on women at
this stage. Attempt to ﬁt the model you derived at the preceding
stage to this data.
6. By comparing the results obtained at stages 4 and 5, you can
determine whether to continue to ignore or to include variables
previously excluded from the model. Only one or two additional
variables should be added to the model at each iteration of steps 4
through 6.
7. Always validate your results as described in Chapter 13.
If all this sounds like a lot of work, it is. It will take several years to
develop sound models even with or despite the availability of lightning
fast, multifunction statistical software. The most common error in statistics
is to assume that statistical procedures can take the place of sustained
effort.
TO LEARN MORE
Paretz (1981) reviews the effect of autocorrelation on multivariable
regression.
Inﬂation of R2 as a consequence of multiple tests also was considered by
Rencher (1980).
Osborne and Waters (2002) review tests of the assumptions of multi-
variable regression. Harrell, Lee, and Mark (1996) review the effect of
violation of assumptions on GLMs and suggest the use of the bootstrap
for model validation. Hosmer and Lemeshow (2001) recommend the use
of the bootstrap or some other validation procedure before accepting the
results of a logistic regression.
Diagnostic procedures for use in determining an appropriate functional
form are described by Tukey and Mosteller (1977), Therneau and 
Grambsch (2000), Hosmer and Lemeshow (2001), and Hardin and Hilbe
(2002).
Automated construction of a decision tree dates back to Morgan and
Sonquist (1963). Comparisons of the regression and tree approaches were
made by Nurminen (2003) and Perlich, Provost, and Simonoff (2003).
186
PART III
BUILDING A MODEL

Chapter 13
Validation
CHAPTER 13
VALIDATION
187
Common Errors in Statistics (and How to Avoid Them), 2e, by Phillip I. Good and James W. Hardin.
Copyright © 2006 John Wiley & Sons, Inc.
Validate your models before drawing conclusions.
AS WE READ IN THE ARTICLES BY DAVID FREEDMAN AND GAIL GONG
reprinted in Chapter 12, absent a detailed knowledge of causal mecha-
nisms, the results of a regression analysis are highly suspect. Freedman
found highly signiﬁcant correlations between totally independent variables.
Gong resampled repeatedly from the data in hand and obtained a different
set of signiﬁcant variables each time.
A host of advertisements from new proprietary software claim an ability
to uncover relationships previously hidden and to overcome the deﬁcien-
cies of linear regression. But how can we determine whether or not such
claims are true?
Good (2001, Chapter 10) reports on one such claim from the maker of
PolyAnalystTM. He took the 400 records, each of 31 variables PolyAnalyst
provided in an example data set, split the data in half at random, and
obtained completely discordant results with the two halves whether they
were analyzed with PolyAnalyst, CART, or stepwise linear regression. This
was yet another example of a spurious relationship that did not survive the
validation process.
In this chapter, we review the various methods of validation and provide
guidelines for their application.
. . . The simple idea of splitting a sample in two and then develop-
ing the hypothesis on the basis of one part and testing it on the
remainder may perhaps be said to be one of the most seriously
neglected ideas in statistics. If we measure the degree of neglect by
the ratio of the number of cases where a method could help to the
number of cases where it is actually used.
G.A. Barnard in discussion following Stone (1974, p. 133).

METHODS OF VALIDATION
Your choice of an appropriate methodology will depend on your objectives
and the stage of your investigation. Is the purpose of your model to
predict—Will there be an epidemic?, to extrapolate—What might the
climate have been like on the primitive Earth?, or to elicit causal mecha-
nisms—Is development accelerating or decelerating? Which factors are
responsible?
Are you still developing the model and selecting variables for inclusion?
Or are you in the process of estimating model coefﬁcients?
There are three main approaches to validation:
1. Independent veriﬁcation (obtained by waiting until the future
arrives or through the use of surrogate variables)
2. Splitting the sample (using one part for calibration, the other for
veriﬁcation)
3. Resampling (taking repeated samples from the original sample and
reﬁtting the model each time)
Independent Veriﬁcation
Independent veriﬁcation is appropriate and preferable whatever the objec-
tives of your model and whether selecting variables for inclusion or esti-
mating model coefﬁcients.
In soil, geologic, and economic studies, researchers often return to the
original setting and take samples from points that have been bypassed on
the original round. See, for example, Tsai et al. (2001).
In other studies, veriﬁcation of the model’s form and the choice of vari-
ables is obtained by attempting to ﬁt the same model in a similar but dis-
tinct context.
For example, having successfully predicted an epidemic at one army
base, one would then wish to see whether a similar model might be
applied at a second and third almost but not quite identical base.
Stockton and Meko (1983) reconstructed regional-average precipitation
to A.D. 1700 in the Great Plains of the United States with multiple linear
regression models calibrated on the period 1933–77. They validated the
reconstruction by comparing the reconstructed regional percentage-of-
normal precipitation with single-station precipitation for stations with
records extending back as far as the 1870s. Lack of appreciable drop in
correlation between these single-station records and the reconstruction
from the calibration period to the earlier segment was taken as evidence
for validation of the reconstructions.
Graumlich (1993) used a response-surface reconstruction method to
reconstruct 1000 years of temperature and precipitation in the Sierra
188
PART III
BUILDING A MODEL

Nevada. The calibration climatic data were 62 years of observed precipita-
tion and temperature (1928–89) at Giant Forest/Grant Grove. The model
was validated by comparing the predictions with the 1873–1927 segments
of three climate stations 90km to the west in the San Joaquin Valley. The
climatic records of these stations were highly correlated with those at
Giant Forest/Grant Grove. Signiﬁcant correlation of these long-term
station records with the 1873–1927 part of the reconstruction was
accepted as evidence of validation.
Independent veriﬁcation can help discriminate among several models
that appear to provide equally good ﬁts to the data. Independent veriﬁca-
tion can be used in conjunction with either of the two other validation
methods. For example, an automobile manufacturer was trying to forecast
parts sales. After correcting for seasonal effects and long-term growth
within each region, ARIMA techniques were used.1 A series of best-ﬁtting
ARIMA models was derived, one model for each of the nine sales regions
into which the sales territory had been divided. The nine models were
quite different in nature. As the regional seasonal effects and long-term
growth trends had been removed, a single ARIMA model applicable to all
regions, albeit with differing coefﬁcients, was more plausible. Accordingly,
the ARIMA model that gave the best overall ﬁt to all regions was utilized
for prediction purposes.
Independent veriﬁcation also can be obtained through the use of surro-
gate or proxy variables. For example, we may want to investigate past cli-
mates and test a model of the evolution of a regional or worldwide climate
over time. We cannot go back directly to a period before direct measure-
ments on temperature and rainfall were made, but we can observe the
width of growth rings in long-lived trees or measure the amount of
carbon dioxide in ice cores.
Sample Splitting
Splitting the sample into two parts, one for estimating the model parame-
ters, the other for veriﬁcation, is particularly appropriate for validating
time series models where the emphasis is on prediction or reconstruction.
If the observations form a time series, the more recent observations
should be reserved for validation purposes. Otherwise, the data used for
validation should be drawn at random from the entire sample.
Unfortunately, when we split the sample and use only a portion of it,
the resulting estimates will be less precise.
CHAPTER 13
VALIDATION
189
1 For examples and discussion of autoregressive integrated moving average (ARIMA)
processes, see Brockwell and Davis (1987).

Browne (1975) suggests we pool rather than split the sample if
1. The predictor variables to be employed are speciﬁed beforehand
(that is, we do not use the information in the sample to select
them)
2. The coefﬁcient estimates obtained from a calibration sample drawn
from a certain population are to be applied to other members of
the same population
The proportion to be set aside for validation purposes will depend on
the loss function. If both the goodness of ﬁt error in the calibration
sample and the prediction error in the validation sample are based on
mean squared error, Picard and Berk (1990) report that we can minimize
their sum by using between a quarter and a third of the sample for valida-
tion purposes.
A compromise proposed by Moiser (1951) is worth revisiting: The orig-
inal sample is split in half; regression variables and coefﬁcients are selected
independently for each of the subsamples; if they are more or less in
agreement, then the two samples should be combined and the coefﬁcients
recalculated with greater precision.
A further proposal by Subrahmanyam (1972) to use weighted averages
where there are differences strikes us as equivalent to painting over cracks
left by the last earthquake. Such differences are a signal to probe deeper,
to look into causal mechanisms, and to isolate inﬂuential observations that
may, for reasons which need to be explored, be marching to a different
drummer.
Resampling
We saw in the report of Gail Gong (1985), reproduced in Appendix B,
that resampling methods such as the bootstrap may be used to validate
our choice of variables to include in the model. As seen Appendix B, they
may also be used to estimate the precision of our estimates.
But if we are to extrapolate successfully from our original sample to 
the population at large, then our original sample must bear a strong
resemblance to that population. When only a single predictor variable is
involved, a sample of 25 to100 observations may sufﬁce. But when we
work with n variables simultaneously, sample sizes on the order of 25n to
100n may be required to adequately represent the full n-dimensional
region.
Because of dependencies among the predictors, we can probably get by
with several orders of magnitude fewer data points. But the fact remains
that the sample size required for conﬁdence in our validated predictions
grows exponentially with the number of variables.
Five resampling techniques are in general use:
190
PART III
BUILDING A MODEL

1. K-fold, in which we subdivide the data into K roughly equal-sized
parts, then repeat the modeling process K times, leaving one
section out each time for validation purposes
2. Leave-one-out, an extreme example of K-fold, in which we 
subdivide into as many parts as there are observations. We leave
one observation out of our classiﬁcation procedure, and use the
remaining n −1 observations as a training set. Repeating this pro-
cedure n times, omitting a different observation each time, we
arrive at a ﬁgure for the number and percentage of observations
classiﬁed correctly. A method that requires this much computation
would have been unthinkable before the advent of inexpensive
readily available high-speed computers. Today, at worst, we need
to step out for a cup of coffee while our desktop completes its
efforts
3. Jackknife, an obvious generalization of the leave-one-out approach,
where the number left out can range from one observation to half
the sample
4. Delete-d, where we set aside a random percentage d of the obser-
vations for validation purposes, use the remaining 100 −d% as a
training set, then average over 100 to 200 such independent
random samples
5. The bootstrap, which we have already considered at length in
earlier chapters
The correct choice among these methods in any given instance is still a
matter of controversy (though any individual statistician will assure you
the matter is quite settled). See, for example, Wu (1986) and the discus-
sion following and Shao and Tu (1995).
Leave one out has the advantage of allowing us to study the inﬂuence
of speciﬁc observations on the overall outcome.
Our own opinion is that if any of the above methods suggest that the
model is unstable, the ﬁrst step is to redeﬁne the model over a more
restricted range of the various variables. For example, with the data of Fig.
9.3, we would advocate conﬁning attention to observations for which the
predictor (TNF-α) was less than 200.
If a more general model is desired, than many additional observations
should be taken in underrepresented ranges. In the cited example, this
would be values of TNF-α greater than 300.
MEASURES OF PREDICTIVE SUCCESS
Whatever method of validation is used, we need to have some measure of
the success of the prediction procedure. One possibility is to use the sum
of the losses in the calibration and the validation sample. Even this proce-
dure contains an ambiguity that we need resolve. Are we more concerned
with minimizing the expected loss, the average loss, or the maximum loss?
CHAPTER 13
VALIDATION
191

One measure of goodness of ﬁt of the model is SSE = Σ(yi −y*i)2 where
yi and y*i denote the ith observed value and the corresponding value
obtained from the model. The smaller this sum of squares, the better the
ﬁt.
If the observations are independent, then
The ﬁrst sum on the right-hand side of the equation is the total sum of
squares (SST ). Most statistics software uses as a measure of ﬁt R2 = 1 −
SSE/SST. The closer the value of R2 is to 1, the better.
The automated entry of predictors into the regression equation using
R2 runs the risk of over-ﬁtting, as R2 is guaranteed to increase with each
predictor entering the model. To compensate, one may use the adjusted
R2
where n is the number of observations used in ﬁtting the model, p is the
number of estimated regression coefﬁcients and i is an indicator variable
that is 1 if the model includes an intercept and 0 otherwise.
The adjusted R2 has two major drawbacks according to Rencher and
Pun (1980):
1. The adjustment algorithm assumes the predictors are independent;
more often, the predictors are correlated.
2. If the pool of potential predictors is large, multiple tests are per-
formed, and R2 is inﬂated in consequence; the standard algorithm
for adjusted R2 does not correct for this inﬂation.
A preferable method of guarding against overﬁtting the regression
model, proposed by Wilks (1995), is to use validation as a guide for stop-
ping the entry of additional predictors. Overﬁtting is judged to begin
when entry of an additional predictor fails to reduce the prediction error
in the validation sample.
Mielke et al. (1997) propose the following measure of predictive accu-
racy for use with either a mean-square-deviation or a mean-absolute-
deviation loss function:
M
n
y
y
n
y
y
i
i
i
n
i
j
j
n
i
n
=
−
= 1
−
= 1
−
=
=
=
∑
∑
∑
1
1
2
1
1
δ µ
δ
µ
δ
δ
, where
* and
*.
1
1
2
−
−
(
)
−
(
)
(
)
−
(
)
[
]
n
i
R
n
p
y
y
y
y
y
y
i
i
i
i
−
(
) =
−
(
) −
−
(
)
∑
∑
∑
*
*
2
2
2
.
192
PART III
BUILDING A MODEL

Uncertainty in Predictions
Whatever measure is used, the degree of uncertainty in your predictions
should be reported. Error bars are commonly used for this purpose.
The prediction error is larger when the predictor data are far from their
calibration-period means, and vice versa. For simple linear regression, the
standard error of the estimate se and standard error of prediction sy* are
related as follows:
where n is the number of observations and xi is the ith value of the pre-
dictor in the calibration sample, and xp is the value of the predictor used
for the prediction.
The relation between sy* and se is easily generalized to the multivariate
case. In matrix terms, if Y = AX + E and y* = AXp, then s 2
y* = s 2
e {1 +
x T
p(XTX)−1xp}.
This equation is only applicable if the vector of predictors lies inside the
multivariate cluster of observations on which the model was based. An
important question is, How “different” can the predictor data be from the
values observed in the calibration period before the predictions are consid-
ered invalid?
LONG-TERM STABILITY
Time is a hidden dimension in most economic models. Many an airline
has discovered to its detriment that today’s optimal price leads to half-
ﬁlled planes and markedly reduced proﬁts tomorrow. A careful reading of
the newspapers lets them know a competitor has slashed prices, but more
advanced algorithms are needed to detect a slow shifting in tastes of
prospective passengers. The public, tired of being treated no better than
hogs,2 turns to trains, personal automobiles, and teleconferencing.
An army base, used to a slow seasonal turnover in recruits, suddenly
ﬁnds that all inﬁrmary beds are occupied and the morning lineup for sick
call stretches the length of a barracks.
To avoid a pound of cure
•
Treat every model as tentative, best described, as any lawyer will
advise you, as subject to change without notice.
•
Monitor continuously.
s
s
n
n
x
x
x
x
y
e
p
i
i
n
* =
+
(
) +
−
(
)
−
(
)
=
∑
1
2
2
1
CHAPTER 13
VALIDATION
193
2 Or somewhat worse, because hogs generally have a higher percentage of fresh air to
breathe.

Most monitoring algorithms take the following form:
If the actual value exceeds some boundary value (the series mean, for
example, or the series mean plus one standard deviation),
And if the actual value exceeds the predicted value for three observation
periods in a row,
Sound the alarm (if the change like an epidemic is expected to be tem-
porary in nature) or recalibrate the model.
TO LEARN MORE
Almost always, a model developed on one set of data will fail to ﬁt a
second independent sample nearly as well. Mielke et al. (1996) investi-
gated the effects of sample size, type of regression model, and noise-to-
signal ratio on the decrease or shrinkage in ﬁt from the calibration to the
validation data set.
For more on leave-one-out validation see Michaelsen (1987), Weisberg
(1985), and Barnston and van den Dool (1993). Camstra and Boomsma
(1992) and Shao and Tu (1995) review the application of resampling in
regression.
Watterson (1996) reviews the various measures of predictive accuracy.
194
PART III
BUILDING A MODEL

Appendix A
A Note on Screening
Regression Equations
APPENDIX A
A NOTE ON SCREENING REGRESSION EQUATIONS
195
Common Errors in Statistics (and How to Avoid Them), 2e, by Phillip I. Good and James W. Hardin.
Copyright © 2006 John Wiley & Sons, Inc.
DAVID A. FREEDMAN*
Consider developing a regression model in a context where substantive
theory is weak. To focus on an extreme case, suppose that in fact there is
no relationship between the dependent variable and the explanatory vari-
ables. Even so, if there are many explanatory variables, the R2 will be
high. If explanatory variables with small t statistics are dropped and the
equation reﬁtted, the R2 will stay high and the overall F will become
highly signiﬁcant. This is demonstrated by simulation and by asymptotic
calculation.
KEY WORDS: Regression; Screening; R2; F; Multiple testing.
1. INTRODUCTION
When regression equations are used in empirical work, the ratio of data
points to parameters is often low; furthermore, variables with small coefﬁ-
cients are often dropped and the equations reﬁtted without them. Some
examples are discussed in Freedman (1981) and Freedman, Rothenberg,
and Sutch (1982, 1983). Such practices can distort the signiﬁcance levels
of conventional statistical tests. The existence of this effect is well known,
but its magnitude may come as a surprise, even to a hardened statistician.
The object of the present note is to quantify this effect, both through 
* David A. Freedman is Professor, Statistics Department, University of California, Berkeley,
CA 94720. This research developed from a project supported by Dr. George Lady, of the
former Ofﬁce of Analysis Oversight and Access, Energy Information Administration, 
Department of Energy, Washington, D.C. I would like to thank David Brillinger, Peter
Guttorp, George Lady, Thomas Permutt, and Thomas Rothenberg for their help.
Reprinted with permission by The American Statistian.

simulation (Section 2) and through asymptotic calculation (Section 3). For
another discussion, see Rencher and Pun (1980).
To help draw the conclusion explicitly, suppose an investigator seeks to
predict a variable Y in terms of some large and indeﬁnite list of explana-
tory variables X1, X2, . . . . If the number of variables is comparable to the
number of data points, and if the variables are only imperfectly correlated
among themselves, then a very modest search procedure will produce an
equation with a relatively small number of explanatory variables, most of
which come in with signiﬁcant coefﬁcients, and a high signiﬁcant R2. This
will be so even if Y is totally unrelated to the X’s.
To sum up, in a world with a large number of unrelated variables and
no clear a priori speciﬁcations, uncritical use of standard methods will lead
to models that appear to have a lot of explanatory power. That is the
main—and negative—message of the present note. Therefore, only the
null hypothesis is considered here, and only the case where the number of
variables is of the same order as the number of data points.
The present note is in the same spirit as the pretest literature. An early
reference is Olshen (1973). However, there is a real difference in imple-
mentation: Olshen conditions on an F test being signiﬁcant; the present
note screens out the insigniﬁcant variables and reﬁts the equation. Thus,
Olshen has only one equation to deal with; the present note has two. The
results of this note can also be differentiated from the theory of pretest
estimators described in, for example, Judge and Bock (1978). To use the
latter estimators, the investigator must decide a priori which coefﬁcients
may be set to zero; here, this decision is made on the basis of the data.
2. A SIMULATION
A matrix was created with 100 rows (data points) and 51 columns (vari-
ables). All the entries in this matrix were independent observations drawn
from the standard normal distribution. In short, this matrix was pure
noise. The 51st column was taken as the dependent variable Y in a regres-
sion equation; the ﬁrst 50 columns were taken as the independent vari-
ables X1, . . . , X50. By construction, then, Y was independent of the X’s.
Ideally, R2 should have been insigniﬁcant, by the standard F test. Likewise,
the regression coefﬁcients should have been insigniﬁcant, by the standard
t test.
These data were analyzed in two successive multiple regressions. In the
ﬁrst pass, Y was run on all 50 of the X’s, with the following results:
•
R2 = 0.50, P = 0.53;
•
15 coefﬁcients out of 50 were signiﬁcant at the 25 percent level;
•
1 coefﬁcient out of 50 was signiﬁcant at the 5 percent level.
196
APPENDIX A
A NOTE ON SCREENING REGRESSION EQUATIONS

Only the 21 variables whose coefﬁcients were signiﬁcant at the 25
percent level were allowed to enter the equation on the second pass. The
results were as follows:
•
R2 = 0.36, P = 5 ¥ 10-4
•
14 coefﬁcients out of 15 were signiﬁcant at the 25 percent level;
•
6 coefﬁcients out of 15 were signiﬁcant at the 5 percent level.
The results from the second pass are misleading indeed, for they appear
to demonstrate a deﬁnite relationship between Y and the X’s, that is,
between noise and noise. Graphical methods cannot help here; in effect, Y
and the selected X’s follow a jointly normal distribution conditioned on
having signiﬁcant t statistics. The simulation was done 10 times; the
results are shown in Table 1. The 25 percent level was selected to repre-
sent an “exploratory” analysis; 5 percent for “conﬁrmatory.” The simula-
tion was done in SAS on the UC Berkeley IBM 4341 by Mr. Thomas
Permutt, on April 16, 1982.
3. SOME ASYMPTOTICS
An asymptotic calculation is helpful to explain the results of the simulation
experiment. The Y and the X’s are independent; condition X to be con-
stant. There is no reason to treat the intercept separately since the Y’s and
X’s all have expectation zero. Finally, suppose X has orthonormal
columns. The resulting model is
(1)
where Y is an n × 1 random vector, X is a constant n × p matrix with
orthonormal columns, where p  p, while β is a p × 1 vector of parame-
ters, and ε is an n × 1 vector of independent normals, having mean 0 and
common variance σ2. In particular, the rank of X is p. All probabilities are
computed assuming the null hypothesis that β ≡0. Suppose
(2)
Let R2
n be the square of the conventional multiple correlation coefﬁcient,
and Fn the conventional F statistic for testing the null hypothesis β ≡0.
Under these conditions, the next proposition shows that R2
n will be essen-
tially the ratio of the number p of variables to the number n of data
points: the proof is deferred.
Proposition. Assume (1) and (2). Then
(3)
R
F
n
n
2
1
→
→
ρ and 
 in probability.
n
p
p n
→∞
→∞
→
<
<
 and 
 so that 
 where 
ρ
ρ
,
.
0
1
Y =
+
Xβ
ε
APPENDIX A
A NOTE ON SCREENING REGRESSION EQUATIONS
197

198
APPENDIX A
A NOTE ON SCREENING REGRESSION EQUATIONS
First Pass
Second Pass
Repetition
R2
F
Pa
#25%b
#5%c
R2
F
P × 104
#pb
#25%
#5%
1
0.50
0.98
0.53
15
1
0.36
3.13
5
15
14
6
2
0.46
0.84
0.73
9
0
0.15
1.85
700
9
6
2
3
0.52
1.07
0.40
16
4
0.36
2.93
7
16
16
9
4
0.45
0.83
0.75
7
1
0.14
2.13
500
7
5
4
5
0.57
1.35
0.15
17
2
0.44
3.82
0.2
17
17
9
6
0.46
0.84
0.73
12
1
0.22
2.06
300
12
11
2
7
0.41
0.70
0.89
4
0
0.12
3.33
100
4
3
1
8
0.42
0.72
0.88
12
1
0.27
2.66
40
12
11
3
9
0.39
0.64
0.94
8
0
0.20
2.90
60
8
8
4
10
0.63
1.69
0.03
16
4
0.48
4.80
0.008
16
16
9
TABLE 1 Simulation Results
a P is the signiﬁcance level of the F test, scaled up by 104 in the second pass.
b #25% is the number of variables whose coefﬁcients are signiﬁcant at the 25% level; only such variables are entered at the second pass; #p is the
number of such variables, that is, the number of variables in the second pass regression, repeated for ease of reference.
c #5% is the number of variables whose coefﬁcients are signiﬁcant at the 5% level.
Note: The regressions are run without intercepts.

Now consider redoing the regression after dropping the columns of X
that fail to achieve signiﬁcance at level α. Here, 0 < α < 1 is ﬁxed. Let qn,α
be the number of remaining columns. Let R2
n,α be the square of the con-
ventional multiple correlation in this second regression, and let Fn,α be the
F statistic. These are to be computed by the standard formulas, that is,
without any adjustment for the preliminary screening.
To estimate R2
n,α and Fn,α, the following will be helpful. Let Z be stan-
dard normal and Φ(z) = P{|Z| > z}. Analytically,
Choose λ so that Φ(λ) = α. Thus, λ is the cutoff for a two-tailed z test at
level α. Let
For 0  z < ∞, integration by parts shows
(4)
Clearly,
(5)
Then, as intuition demands,
(6)
Let Z λ be Z conditional on |Z| > λ. Put z = λ in (5) and recall that Φ(λ) =
α:
(7)
Using (6) and further integration by parts.
(8)
where
var
,
Z
Z
z
v z
2
2
>
{
} =
+ ( )
g
E Z
Z
E Z
λ
α
λ
λ
( )
=
>
{
} =
{
} >
2
2
1
E Z
Z
z
z
z
2
2
1
2
1
2
1
>
{
} =
+
−




( ) >
π exp
.
Φ
E Z
Z
z
g z
z
2
>
{
} =
( )
( )
Φ
.
g z
z
z
z
( ) =
( ) +
−




Φ
2
1
2
2
π exp
.
g z
Z
z
z
( ) =
<
>
{
}
∫
2
1.
Φ z
u
du
z
( ) =
−




∞∫
2
1
2
2
π
exp
.
APPENDIX A
A NOTE ON SCREENING REGRESSION EQUATIONS
199

(9)
and
In particular, ν is continuous. Intuition suggests that ν be positive. This
fact will not be needed here, but it is true: see Diaconis and Freedman
(1982, (3.15)–(3.16)).
Proposition. Assume (1) and (2). In probability: qn,α/n →αρ and R2
n,α →
g(λ) and
(10)
In the second regression, the t statistic for testing whether a coefﬁcient
vanishes is asymptotically distributed as
These results may be interpreted as follows. The number of variables 
in the ﬁrst-pass regression is p = ρn + o(n); the number in the second 
pass is qn,α = αρn + o(n). That is, as may be expected, α of the variables
are signiﬁcant at level α. Since g(λ) < 1, the R2 in the second-pass 
regression is essentially the fraction g(λ) of R2 in the ﬁrst pass. Likewise,
g(λ) > α, so the asymptotic value of the F statistic exceeds 1. Since the
number of degrees of freedom is growing, off-scale P values will result.
Finally, the real level of the t test may differ appreciably from the nominal
level.
Example. Suppose N = 100 and p = 50, so ρ = 1–2; and α = 0.25 so λ 
1.15. Then g(λ)  0.72, and E{Z 2| |Z| > λ}  2.9. In a regression with
50 explanatory variables and 100 data points, on the null hypothesis R2
should be nearly 1–2.
Next, run the regression again, keeping only the variables signiﬁcant at
the 25 percent level. The new R2 should be around g (λ) = 72 percent of
the original R2. The new F statistic should be around
Z
g
λ
αρ
λ ρ
1
1
−
−
( )
.
F
g
g
n,
.
α
λ
α
λ ρ
αρ
→
( )
−
( )
−
1
1
w z
z
z
z
z
z
( ) =
+
(
) ( ) −
−




3
2
2
2
1
2
Φ
π
exp
.
v z
z
w z
z
z
( ) ( ) =
( )
−




( )
2
2
2
2
1
2
π
exp
Φ
200
APPENDIX A
A NOTE ON SCREENING REGRESSION EQUATIONS

The number of degrees of freedom should be around αρn  12 in the
numerator and 100 −12 = 88 in the denominator. (However, qn,α is still
quite variable, its standard deviation being about 3.) On this basis, a P
value on the order of 10−4 may be anticipated.
What about the t tests? Take λ′ > λ, corresponding to level α′ < α. The
nominal level for the test is α′, but the real level is
Since g(λ) > α, it follows that 1 −αρ > 1 −g(λ)ρ. Keep α = 0.25, so 
λ  1.15; take α′ = 5 percent, so λ′ = 1.96; keep ρ = 1–2. Now
and the real level is 9 percent. This concludes the example.
Turn now to the proofs. Without loss of generality, suppose the ith
column of X has a 1 in the ith position and 0’s everywhere else. Then
and the sum of squares for error in the ﬁrst-pass regression corresponding
to the model (1) is
Thus
and
Now (3) follows from the weak law of large numbers. Of course, E(R2
n)
and var Rn are known: see Kendall and Stuart (1969).
F
p
n
p
n
i
i
p
i
i p
n
=
−
=
= +
∑
∑
1
1
2
1
2
1
Y
Y .
Rn
i
i
p
i
i
n
2
2
1
2
1
=
=
=
∑
∑
Y
Y
Yi
i p
n
2
1
.
= +∑
ˆ
, ..., ,
βi
i
i
p
=
=
Y  for 
1
′
−
−
( )
=
λ
αρ
λ ρ
1
1
2 3
g
.
1
1
1
α
λ
αρ
λ ρ
P
Z
g
>
′
−
−
( )






.
g
g
λ
α
λ ρ
αρ
( )
−
( )
−
=
1
1
4 0
˙
. .
APPENDIX A
A NOTE ON SCREENING REGRESSION EQUATIONS
201

To prove (10), the t statistic for testing βi = 0 is Yi/sn, where
Thus, column i of X enters the second regression iff |Yi/sn| > tα,n−p, the
cutoff for a two-tailed t test at level α, with n −p degrees of freedom.
In what follows, suppose without loss of generality that σ2 = 1. Given sn,
the events
are conditionally independent, with common conditional probability
Φ(tα,n−psn). Of course, tα,n−p →λ and sn →1; so this conditional probability
converges to Φ(λ) = α. The number qn,α of the events Ai that occur is
therefore
by (2). This can be veriﬁed in detail by computing the conditional expec-
tation and variance.
Next, condition on sn and qn,α = q and on the identity of the q columns
going into the second regression. By symmetry, suppose that it is columns
1 through q of X that enter the second regression. Then
and
Now Σn
i=1Yi
2 = n + o(n); and in the denominator of Fn,α,
It remains only to estimate Σq
i=1Yi
2, to within o(n). However, these Yi’s are
conditionally independent, with common conditional distribution: they are
distributed as Z given |Z| > zn, where Z is N(0, 1) and zn = tα,n−p·sn. In
view of (5), the conditional expectation of Σq
i=1Yi
2 is
Y
Y
Y
i
i q
n
i
i
n
i
i
q
2
1
2
1
2
1
= +
=
=
∑
∑
∑
=
−
.
F
q
n
q
n
i
i
q
i
i q
n
,
.
α =
−
=
= +
∑
∑
1
1
2
1
2
1
Y
Y
Rn
i
i
q
i
i
n
,α
2
2
1
2
1
=
=
=
∑
∑
Y
Y
α
αρ
p
o p
n
o n
+ ( ) =
+ ( )
A
t
s
i
i
n p n
=
>
{
}
−
Y
α,
s
n
p
n
j
j
p
n
2
2
1
1
=
−
= +∑Y .
202
APPENDIX A
A NOTE ON SCREENING REGRESSION EQUATIONS

But qn,α = αρn + o(n) and zn →λ. So the last display is, up to o(n),
Likewise, the conditional variance of Σq
i=1Yi
2 is qn,α {2 + ν(zn)} = O(n); the
conditional standard deviation is O(
). Thus
This completes the argument for the convergence in probability. The
assertion about the t statistic is easy to check, using the last display.
REFERENCES
Diaconis P; Freedman D. “On the Maximum Difference Between the Empirical
and Expected Histograms for Sums,” Paciﬁc Journal of Mathematics, 1982;
100:287–327.
Freedman D. “Some Pitfalls in Large-Scale Econometric Models: A Case Study,”
University of Chicago Journal of Business, 1981; 54:479–500.
Freedman D; Rothenberg T; Sutch R. “A Review of a Residential Energy End Use
Model,” Technical Report No. 14, University of California, Berkeley, Dept. of
Statistics, 1982.
—— “On Energy Policy Models,” Journal of Business and Economic Statistics,
1983; 1:24–32.
Judge G; Bock M. The Statistical Implications of Pre-Test and Stein-Rule Estimators
in Econometrics, Amsterdam: North-Holland, 1978.
Kendall MG; Stuart A. The Advanced Theory of Statistics, London: Grifﬁn, 1969.
Olshen RA. “The Conditional Level of the F-Test,” Journal of the American 
Statistical Association, 1973; 68, 692–698.
Rencher AC; Pun FC. “Inﬂation of R2 in Best Subsets Regression,” Technometrics,
1980; 22:49–53.
1
1
1
1
2
1
n
q
g
o
i
i q
n
−
=
−
( )
−
+ ( )
= +∑Y
λ ρ
αρ
.
1
1
2
1
q
g
o
i
i
n
Y
=∑
=
( )
+ ( )
λ
α
,
Yi
i
q
g
n
o n
2
1
=∑
=
( )
+ ( )
λ ρ
,
n
αρ
λ
α
λ ρ
ng( )
=
( )
g
n.
q
g z
z
n
n
n
,
.
α (
)
(
)
Φ
APPENDIX A
A NOTE ON SCREENING REGRESSION EQUATIONS
203

Appendix B
Cross-Validation, the
Jackknife, and the Bootstrap:
Excess Error Estimation in
Forward Logistic Regression
APPENDIX B
EXCESS ERROR ESTIMATION IN FORWARD LOGISTIC REGRESSION
205
Common Errors in Statistics (and How to Avoid Them), 2e, by Phillip I. Good and James W. Hardin.
Copyright © 2006 John Wiley & Sons, Inc.
GAIL GONG*
Given a prediction rule based on a set of patients, what is the probability of
incorrectly predicting the outcome of a new patient? Call this probability the
true error. An optimistic estimate is the apparent error, or the proportion of
incorrect predictions on the original set of patients, and it is the goal of this
article to study estimates of the excess error, or the difference between the
true and apparent errors. I consider three estimates of the excess error:
cross-validation, the jackknife, and the bootstrap. Using simulations and real
data, the three estimates for a speciﬁc prediction rule are compared. When
the prediction rule is allowed to be complicated, overﬁtting becomes a real
danger, and excess error estimation becomes important. The prediction rule
chosen here is moderately complicated, involving a variable-selection
procedure based on forward logistic regression.
KEY WORDS: Prediction; Error rate estimation; Variables selection.
1. INTRODUCTION
A common goal in medical studies is prediction. Suppose we observe n
patients, x1 = (t1, y1), . . . , xn = (tn, yn), where yi is a binary variable indicat-
ing whether or not the ith patient dies of chronic hepatitis and ti is a
vector of explanatory variables describing various medical measurements
* Gail Gong is Assistant Professor, Department of Statistics, Carnegie-Mellon University,
Pittsburgh, PA 15217.
Reprinted with permission by the American Statistical Association.

on the ith patient. These n patients are called the training sample. We
apply a prediction rule η to the training sample x = (x1, . . . , xn) to form
the realized prediction rule ηx. Given a new patient whose medical mea-
surements are summarized by the vector t0, we predict whether or not he
will die of chronic hepatitis by ηx(i0), which takes on values “death” or
“not death.” Allowing the prediction rule to be complicated, perhaps
including transforming and choosing from many variables and estimating
parameters, we want to know: What is the error rate, or the probability of
predicting a future observation incorrectly?
A possible estimate of the error rate is the proportion of errors that ηx
makes when applied to the original observations x1, . . . , xn. Because the
same observations are used for both forming and assessing the prediction
rule, this proportion, which I call the apparent error, underestimates the
error rate.
To correct for this bias, we might use cross-validation, the jackknife, or
the bootstrap for estimating excess errors (e.g., see Efron 1982). We study
the performance of these three methods for a speciﬁc prediction rule.
Excess error estimation is especially important when the training sample is
small relative to the number of parameters requiring estimation, because
the apparent error can be seriously biased. In the chronic hepatitis
example, if the dimension of ti is large relative to n, we might use a pre-
diction rule that selects a subset of the variables that we hope are strong
predictors. Speciﬁcally, I will consider a prediction rule based on forward
logistic regression. I apply this prediction rule to some chronic hepatitis
data collected at Stanford Hospital and to some simulated data. In the
simulated data, I compare the performance of the three methods and ﬁnd
that cross-validation and the jackknife do not offer signiﬁcant improve-
ment over the apparent error, whereas the improvement given by the
bootstrap is substantial.
A review of required deﬁnitions appears in Section 2. In Section 3, I
discuss a prediction rule based on forward logistic regression and apply it
to the chronic hepatitis data. In Sections 4 and 5, I apply the rule to 
simulated data. Section 6 concludes.
2. DEFINITIONS
I brieﬂy review the deﬁnitions that will be used in later discussions. These
deﬁnitions are essentially those given by Efron (1982). Let x1 = (t1, y1), . . . ,
xn = (tn, yn) be independent and identically distributed from an unknown
distribution F, where ti is a p-dimensional row vector of real-valued
explanatory variables and yi is a real-valued response. Let 
be the empirical
distribution function that puts mass 1/n at each point x1, . . . , xn. We apply
a prediction rule η to this training sample and form the realized prediction
ˆF
206
APPENDIX B
EXCESS ERROR ESTIMATION IN FORWARD LOGISTIC REGRESSION

rule η (t0). Let Q(y0, η (t0)) be the criterion that scores the discrepancy
between an observed value y0 and its predicted value η (t0). The form of
both the prediction rule η and the criterion Q are given a priori. I deﬁne
the true error of η to be the expected error that η makes on a new 
observation x0 = (t0, y0) from F,
In addition, I call the quantity
the apparent error of η . The difference
is the excess error of η . The expected excess error is
where the expectation is taken over , which is obtained from x1, . . . , xn
generated by F. In Section 4, I will clarify the distinction between excess
error and expected excess error. I will consider estimates of the expected
excess error, although what we would rather have are estimates of the
excess error.
I will consider three estimates (the bootstrap, the jackknife, and cross-
validation) of the expected excess error. The bootstrap procedure for esti-
mating r = E
~FR( , F) replaces F with . Thus
where * is the empirical distribution function of a random sample x*1, . . . ,
x*n from
. Since
is known, the expectation can in principle be calcu-
lated. The calculations are usually too complicated to perform analytically,
however, so we resort to Monte Carlo methods.
1. Generate x*1, . . . , x*n, a random sample from . Let * be the
empirical distribution of x*1, . . . , x*n.
2. Construct η
*, the realized prediction rule based on x*1, . . . , 
x*n.
3. Form
ˆF
ˆF
ˆF
ˆF
ˆF
ˆF
ˆ
ˆ , ˆ ,
ˆ ~ ˆ
r
E
R F
F
F
F
boot =
∗
(
)
∗
ˆF
ˆF
ˆF
ˆF
r
E
R F F
F F
=
(
)
ˆ~
ˆ,
,
ˆF
R F F
q F F
q F F
ˆ,
ˆ,
ˆ, ˆ
(
) = (
) −(
)
ˆF
ˆ
ˆ, ˆ
,
,
ˆ
ˆ
ˆ
q
q F F
E
Q y
t
n
Q y
t
x
F
F
i
F
i
i
n
app = (
) =
( )
(
) =
( )
(
)
−
=∑
0
0
0
1
1
η
η
q
q F F
E
Q y
t
x
F
F
= (
) =
( )
(
)
ˆ,
,
.
~
ˆ
0
0
0
η
ˆF
ˆF
ˆF
ˆF
ˆF
APPENDIX B
EXCESS ERROR ESTIMATION IN FORWARD LOGISTIC REGRESSION
207

(2.1)
4. Repeat 1–3 a large number B times to get R*1, . . . , R*B. The boot-
strap estimate of expected excess error is
See Efron (1982) for more details.
The jackknife estimate of expected excess error is
where 
(i) is the empirical distribution function of (x1, . . . , xi−1, xi+1, . . . ,
xn), and
Efron (1982) showed that the jackknife estimate can be reexpressed as
The cross-validation estimate of expected excess error is
Let the training sample omit patients one by one. For each omission,
apply the prediction rule to the remaining sample and count the number
(0 or 1) of errors that the realized prediction rule makes when it predicts
the omitted patient. In total, we apply the prediction rule n times and
predict the outcome of n patients. The proportion of errors made in these
n predictions is the cross-validation estimate of the error rate and is the
ﬁrst term on the right-hand side. [Stone (1974) is a key reference on
cross-validation and has a good historical account. Also see Geisser
(1975).]
ˆ
,
.
,
ˆ
ˆ
r
n
Q y
t
n
Q y
t
i
F
i
i
n
i
F
i
i
n
i
cross =
( )
(
) −
( )
(
)
( )
=
=
∑
∑
1
1
1
1
η
η
ˆ
,
.
,
ˆ
ˆ
r
n
Q y
t
n
n
Q y
t
i
F
i
i
n
i
F
i
j
n
i
n
i
i
jack =
( )
(
) −
( )
(
)
( )
( )
=
=
=
∑
∑
∑
1
1
1
1
1
1
η
η
R
R F
F
R
n
R
R
R F F
i
i
i
i
n
( )
( )
⋅( )
( )
=
=
(
)
=
=
(
)
∑
ˆ
, ˆ ,
,
ˆ
ˆ, ˆ .
1
1
ˆF
ˆ
ˆ ,
.
r
n
R
R
jack =
−
(
)
−
(
)
( )
1
ˆ
.
r
B
Rb
b
B
boot =
∗
=∑
1
1
R
q F
F
q F
F
n
Q y n
t
n
Q y
t
i
F
i
i
F
i
i
n
i
n
∗=
∗
(
) −
∗
∗
(
)
=
( )
(
) −
∗
(
)
(
)
∗
∗
=
=
∗
∑
∑
ˆ , ˆ
ˆ , ˆ
,
,
ˆ
ˆ
1
1
1
1
η
208
APPENDIX B
EXCESS ERROR ESTIMATION IN FORWARD LOGISTIC REGRESSION

3. CHRONIC HEPATITIS: AN EXAMPLE
We now discuss a real prediction rule. From 1975 to 1980, Peter Gregory
(personal communication, 1980) of Stanford Hospital observed n = 155
chronic hepatitis patients, of which 33 died from the disease. On each
patient were recorded p = 19 covariates summarizing medical history, physi-
cal examinations, X rays, liver function tests, and biopsies. (Missing values
were replaced by sample averages before further analysis of the data.) An
effective prediction rule, based on these 19 covariates, was desired to identify
future patients at high risk. Such patients require more aggressive treatment.
Gregory used a prediction rule based on forward logistic regression. We
assume x1 = (t1, y1), . . . , xn = (tn, yn) are independent and identically dis-
tributed such that conditional on ti, yi is Bernoulli with probability of
success θ(ti), where logit θ(ti) = β0 + tiβ, and where β is a column vector 
of p elements. If (
0, ) is an estimate of (β0, β), then 
(t0), such that 
logit 
(t0) =
0 + t0 , is an estimate of θ(t0). We predict death if the 
estimated probability (t0) of death were greater than 1–2.:
(3.1)
Gregory’s rule for estimating (β0, β) consists of three steps.
1. Perform an initial screening of the variables by testing H0: bj = 0
in the simple logistic model, logit q(t0) = b + t0jbj, for j = 1, . . . , p
separately at level a = 0.05. Retain only those variables j for which
the test is signiﬁcant. Applied to Gregory’s data, the initial screen-
ing retained 13 variables, 17, 12, 14, 11, 13, 19, 6, 5, 18, 10, 1,
4, 2, in increasing order of p-values.
2. To the variables that were retained in the initial screening, apply
forward logistic regression that adds variables one at a time in the
following way. Assume variables j1, j2, . . . , jP1 are already added to
the model. For each remaining j, test H0: bj = 0 in the linear logis-
tic model that contains variables j1, j2, . . . , jP1, j together with the
intercept. Rao’s (1973, pp. 417–420) efﬁcient score test requires
calculating the maximum likelihood estimate only under H0. If the
most signiﬁcant variable is signiﬁcant at a = 0.05, we add that
variable to the model as variable jP1+1 and start again. If none of
the remaining variables is signiﬁcant at a = 0.05, we stop. From
the aforementioned 13 variables, forward logistic regression
applied to Gregory’s data chose four variables (17, 11, 14, 2) that
are, respectively, albumin, spiders, bilirubin, and sex.
3. Let (
0, ) be the maximum likelihood estimate based on the
linear logistic model consisting of the variables chosen by forward
logistic regression together with the intercept. On Gregory’s data,
it turned out that
ˆb
ˆb
η
θ
β
β
ˆ
ˆ
,
ˆ
ˆ
F t
t
t
0
0
0
1
1
2
0
0
( ) =
( ) ≥
+
≥
=
if 
 i.e., 
otherwise.
0
ˆθ
ˆβ
ˆβ
ˆθ
ˆθ
ˆβ
ˆβ
APPENDIX B
EXCESS ERROR ESTIMATION IN FORWARD LOGISTIC REGRESSION
209

The realization η of Gregory’s rule on his 155 chronic hepatitis
patients predicts that a new patient with covariate vector t0 will die if his
predicted probability of death 
(t0) is greater than 1–2; that is,
(3.2)
For the dichotomous problem, we use the criterion
The apparent error is 
app = 0.136. Figure 1 shows a histogram of B = 400
bootstrap replications of R* = R( *, ). Recall that each R* was calcu-
lated using (2.1), where η * is the realization of Gregory’s rule on the
bootstrap sample x*1, . . . , x*n. The bootstrap estimate of expected excess
error was
The jackknife and cross-validation estimates were calculated to be
Adding expected excess error estimates to the apparent error gives bias-
corrected estimates of the error:
All three estimates require substantial computing time. FORTRAN 
programs for performing the preceding calculations and the ones in the
following section were developed on a PDP-11/34 minicomputer. The
cross-validation and jackknife estimates were computed in 11–2 hours,
whereas the 400 bootstrap replications required just under 6 hours. Com-
puters are becoming faster and cheaper, however, and even now it is possi-
ble to compute these estimates on very complicated prediction rules, such
as Gregory’s rule.
Are B = 400 bootstrap replications enough? Notice that R*1, . . . , R*B is a
random sample from a population with mean
ˆ
.
,
ˆ
.
,
ˆ
.
.
q
q
q
boot
jack
cross
=
=
=
0 175
0 159
0 145
ˆ
.
,
ˆ
.
.
r
r
jack
cross
=
=
0 023
0 019
ˆ
~
.
.
r
B
Rb
b
B
boot
1
0 039
1
∗=
=∑
ˆF
ˆF
ˆF
ˆq
Q y
y
,
,
η
η
(
) =
≠
=
1
0
if 
otherwise.
logit ˆ
.
.
.
.
.
.
.
.
,
,
θ t
t
t
t
t
0
0 17
0 11
0 14
0 3
12 17
1 83
1 58
0 56
5 17
0
( ) =
−
−
+
−
≥
ˆθ
ˆF
ˆ , ˆ
, ˆ , ˆ
, ˆ
.
,
.
,
.
, .
,
.
β
β
β
β
β
0
17
11
14
2
12 17
1 83
1 58 0 56
5 17
(
) =
−
−
−
(
)
210
APPENDIX B
EXCESS ERROR ESTIMATION IN FORWARD LOGISTIC REGRESSION

and variance, say, σ 2. Figure 1 shows that this population is close to
normal, so
with high probability. Approximating σ 2 with
gives
so with high probability, 
400 is within 0.0027 of 
∞=
boot.
ˆr
ˆr
ˆr
ˆ
ˆ
.
.
;
r
r
400
1 2
2 0 027
400
0 0027
−
≤(
)
=
∞
ˆ
ˆ
.
σ400
2
400
2
1
400
1
400
1
0 027
2
=
−
∗−
[
] = (
)
−∑R
r
b
b
ˆ
ˆ
,
r
r
400
1 2
2
400
−
≤
∞
σ
E
R F
F
r
r
F
F
ˆ ~ ˆ
ˆ , ˆ
ˆ
ˆ ,
∗
∞
∗
(
) =
=
boot
APPENDIX B
EXCESS ERROR ESTIMATION IN FORWARD LOGISTIC REGRESSION
211
0
10
20
30
40
50
–0.052
–0.045
–0.039
–0.032
–0.028
–0.019
–0.013
–0.006
0.000
0.008
0.013
0.019
0.026
0.032
0.039
0.045
0.052
0.058
0.064
0.071
0.077
0.084
0.090
0.097
0.103
0.110
0.116
0.129
*
***
**
**
**
****
********
**********
*****************
**********************
***************************
*******************************
**********************
**********************
****************
***********
****
**
**
**
*
*
**
************
*******
*******
**********************************
**************************************
*******************************************
********************************************
*
FIGURE 1
Histogram of Bootstrap Replications for Gregory’s Rule. The his-
togram summarizes the 400 bootstrap replications of R* that are used in estimat-
ing the expected excess error of Gregory’s rule for predicting death in chronic
hepatitis. Values of R* range from −0.045 to 0.116, with mean 0.039, standard
deviation 0.027, and quantiles R*(.05) = −0.006 and R*(.05) = 0.084.

Before leaving the chronic hepatitis data, I mention that other predic-
tion rules might be used. Examples include more complicated forms of
variable selection such as best subset regression and alternative models
such as discriminant analysis. Friedman (1977) applied recursive partition-
ing to these data to obtain a binary-decision tree. I chose to focus atten-
tion on the rule based on forward logistic regression because it is the rule
actually proposed and used by Gregory, the experimenter. The question of
choosing an optimal prediction rule was not my goal.
4. THE PERFORMANCE OF CROSS-VALIDATION, THE
JACKKNIFE, AND THE BOOTSTRAP IN SIMULATIONS
In the previous section we saw the cross-validation, jackknife, and boot-
strap estimates of expected excess error for Gregory’s rule. These estimates
give bias corrections to the apparent error. Do these corrections offer real
improvements? Introduce the “estimators” 
app ≡0, the zero-correction
estimate corresponding to the apparent error, and 
ideal ≡E(R), the best
constant estimate if we knew the expected excess error E(R). To compare
cross, 
jack, 
boot against these worst and best cases 
app and 
ideal, we
perform some simulations.
To judge the performance of estimators in the simulations, we use two
criteria:
the root mean squared error (RMSE) about the excess error, and
the root mean squared error about the expected excess error. Notice that
since
RMSE1(
) also measures the performance of the bias-corrected estimate 
app +
as an estimator of the true error 
app + R.
I pause to clarify the distinction between excess error and expected
excess error. In the chronic hepatitis problem, the training sample that
Gregory observed led to a particular realization (3.2). The excess error is
the difference between the true and apparent error of the realized rule η
based on this training sample. The expected excess error averages the
ˆF
ˆq
ˆR
ˆq
ˆR
E R
R
E q
R
q
R
ˆ
ˆ
ˆ
ˆ
ˆ
,
−
[
] =
+
(
) −
+
(
)
[
]
2
2
app
app
RMSE2
2
1 2
ˆ
ˆ
,
R
E R
E R
( ) =
−( )
[
]




RMSE1
2
1 2
ˆ
ˆ
,
R
E R
R
( ) =
−
[
]




ˆr
ˆr
ˆr
ˆr
ˆr
ˆr
ˆr
212
APPENDIX B
EXCESS ERROR ESTIMATION IN FORWARD LOGISTIC REGRESSION

excess error over the many training samples that Gregory might have
observed and, therefore, over many realizations of this prediction rule.
Because 
cross, 
jack, 
boot average over many realizations, they are, strictly
speaking, estimates of the expected excess error. Gregory, however, would
much rather know the excess error of his particular realization.
It is perhaps unfair to think of 
cross, 
jack, 
boot as estimators of the
excess error. A simple analogy may be helpful. Suppose X is an observa-
tion from the distribution Fζ, and T(X) estimates ζ. The bias is the
expected difference E[T(X) −ζ] and is analogous to the expected excess
error. The difference T(X) −ζ is analogous to the excess error. Getting a
good estimate of the bias is sometimes possible, but getting a good esti-
mate of the difference T(X) −ζ would be equivalent to knowing ζ.
In the simulations, the underlying model was the logistic model that
assumes x1 = (t1, y1), . . . , xn = (tn, yn) are independent and identically 
distributed such that yi conditional on ti is Bernoulli with probability 
of success θ(ti), where
(4.1)
where ti = (ti1, . . . , tip) is p-variate normal with zero mean and a speciﬁed
covariance structure Σ.
I performed two sets of simulations. In the ﬁrst set (simulations 1.1,
1.2, 1.3) I let the sample sizes be, respectively, n = 20, 40, 60; the dimen-
sion of ti be p = 4; and
(4.2)
where τ = 0.80. We would expect a good prediction rule to choose vari-
ables t1 and t2, and due to the correlation between variables t2 and t3, a
prediction rule choosing t1 and t3 would probably not be too bad. In the
second set of simulations (simulations 2.1, 2.2, 2.3, the sample sizes were
again n = 20, 40, 60; the dimension of ti was increased to p = 6; and
Σ =
















=
=
















1
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
1
0
0
0
0
0
0
1
0
1
1
1
2
0
0
0
τ
τ
β
β
,
,
.
Σ =










=
=










1
0
0
0
0
0
0
0
1
0
0
0
0
1
0
1
2
0
0
0
τ
τ
β
β
,
,
,
logit θ
β
β
t
t
i
i
( ) =
+
0
,
ˆr
ˆr
ˆr
ˆr
ˆr
ˆr
APPENDIX B
EXCESS ERROR ESTIMATION IN FORWARD LOGISTIC REGRESSION
213

Each of the six simulations consisted of 400 experiments. The results of
all 400 experiments of simulation 1.1 are summarized in Table 1. In each
experiment, we estimate the excess error R by evaluating the realized pre-
diction rule on a large number (5,000) of new observations. We estimate
the expected excess error by the sample average of the excess errors in the
400 experiments. To compare the three estimators, I ﬁrst remark that in
the 400 experiments, the bootstrap estimate was closest to the true excess
error 210 times. From Table 1 we see that since
are all close, 
cross and 
jack are nearly unbiased estimates of the expected
excess error E(R), whereas 
boot with expectation E( boot) = 0.0786 is
biased downwards. [Actually, since we are using the sample averages of the
excess errors in 400 experiments as estimates of the expected excess errors,
we are more correct in saying that a 95% conﬁdence interval for E( cross) is
(0.0935), 0.1143), which contains E(R), and a 95% conﬁdence interval
for E( jack) is (0.0866, 1036), which also contains E(R). On the other
hand, a 95% conﬁdence interval for E( boot) is (0.0761, 0.0811), which
does not contain E(R).] However, 
corss and 
jack have enormous standard
deviations, 0.1060 and 0.0864, respectively, compared to 0.0252, the
standard deviation of 
boot. From the column for RMSE1,
with RMSE1(
boot) being about one-third of the distance between
RMSE1(
ideal) and RMSE1(
app). The same ordering holds for RMSE2.
Recall that simulations 1.1, 1.2, and 1.3 had the same underlying distri-
bution but differing sample sizes, n = 20, 40, and 60. As sample size
increased, the expected excess error decreased, as did the mean squared
error of the apparent error. We observed a similar pattern in simulations
2.1, 2.2, and 2.3, where the sample sizes were again n = 20, 40, and 60,
ˆr
ˆr
ˆr
RMSE
RMSE
RMSE
RMSE
RMSE
ideal
boot
app
cross
jack
1
1
1
1
1
ˆ
ˆ
ˆ
~
ˆ
~
ˆ
,
r
r
r
r
r
(
) <
(
) <
(
)
(
)
(
)
ˆr
ˆr
ˆr
ˆr
ˆr
ˆr
ˆr
ˆr
ˆr
ˆr
E r
E r
E R
ˆ
.
,
ˆ
.
,
.
cross
jack
(
) =
(
) =
( ) =
0 1039
0 0951
0 1006
214
APPENDIX B
EXCESS ERROR ESTIMATION IN FORWARD LOGISTIC REGRESSION
E(
)
SD(
)
RMSE1(
)
RMSE2(
)
apparent
0.0000
0.0000
0.1354
0.1006
cross
0.1039
0.1060
0.1381
0.1060
jack
0.0951
0.0864
0.1274
0.0865
boot
0.0786
0.0252
0.1078
0.0334
ideal
0.1006
0.0000
0.0906
0.0000
ˆR
ˆR
ˆR
ˆR
ˆR
TABLE 1 The Results of 400 Experiments of Simulation 1.1
Note: RMSE1 is the root mean squared error about the true excess, and RMSE2 is that
about the expected excess error. The expected excess error is E(
) for ideal.
ˆR

and the dimension of ti was increased to p = 6 and Σ, β0, and β given in
(4.3). For larger sample sizes, bias corrections to the apparent error
became less important. It is still interesting, however, to compare mean
squared errors. For all six simulations, I plot RMSE1’s in Figure 2 and
RMSE2’s in Figure 3. It is interersting to note that the ordering noticed in
simulation 1.1 of the root mean squared error of the ﬁve estimates also
held in the other ﬁve simulations. That is,
and RMSE1( boot) is about one-third of the distance between 
RMSE1( ideal)and RMSE1( app). Similar remarks hold for RMSE2. Cross-
validation and the jackknife offer no improvement over the apparent error,
whereas the improvement given by the bootstrap is substantial.
The superiority of the bootstrap over cross-validation has been observed
in other problems. Efron (1983) discussed estimates of excess error and
ˆr
ˆr
ˆr
RMSE
RMSE
RMSE
app
cross
jack
1
1
1
ˆ
~
ˆ
~
ˆ
,
r
r
r
(
)
(
)
(
)
APPENDIX B
EXCESS ERROR ESTIMATION IN FORWARD LOGISTIC REGRESSION
215
C
A
J
B
1.1
1.2
1.3
2.1
2.2
2.3
0.00
0.04
0.08
0.12
0.16
0.20
FIGURE 2
95% (nonsimultaneous) Conﬁdence Intervals for RMSE1. In each set
of simulations, there are ﬁve conﬁdence intervals for, respectively, apparent (A),
cross-validation (C), jackknife (J), bootstrap (B), and ideal (1) estimates of the
excess error. Each conﬁdence interval is indicated by — —. The middle vertical bar
in each conﬁdence interval represents the value of the estimate.

performed several simulations with a ﬂavor similar to mine. I report on
only one of his simulations here. When the prediction rule is the usual
Fisher discriminant and the training sample consists of 14 observations
that are equally likely from N((−1–2, 0), I) or N((+ 1–2, 0), I), then the
RMSE1 of apparent, cross-validation, bootstrap, and ideal estimates are,
respectively, 0.149, 0.144, 0.134, and 0.114. Notice that the RMSE1’s of
cross-validation and apparent estimates are close, whereas the RMSE1 of
the bootstrap estimate is about halfway between that of the ideal and
apparent estimates.
In the remainder of this section, I discuss the sufﬁciency of the number
of bootstrap replications and the number of experiments.
Throughout the simulations, I used B = 100 bootstrap replications for
each experiment. Denote
216
APPENDIX B
EXCESS ERROR ESTIMATION IN FORWARD LOGISTIC REGRESSION
A
C
J
B
1.1
1.2
1.3
2.1
2.2
2.3
0.00
0.04
0.08
0.12
0.16
0.20
FIGURE 3
95% (nonsimultaneous) Conﬁdence Intervals for RMSE2. In each set
of simulations, there are four conﬁdence intervals for, respectively, apparent (A),
cross-validation (C), jackknife (J), and bootstrap (B) estimates of the expected
excess error. Notice that 
app ≡0, so RMSE2(
app) is the expected excess error, 
a constant; the “conﬁdence interval” for RMSE2(
app) is a single value, indicated 
by a single bar. In addition, RMSE2(
ideal) = 0 and its conﬁdence intervals are not
shown. Some of the bootstrap conﬁdence intervals are so small that they are 
indistinguishable from single bars.
ˆr
ˆr
ˆr
ˆr

Using a component-of-variance calculation (Gong 1982), for Simulation
1.1
so if we are interested in comparing root mean squared errors about 
the excess error, we need not perform more than B = 100 bootstrap 
replications.
In each simulation, I included 400 experiments and therefore used the
approximation
where 
e and Re are the estimate and true excess of the eth experiment.
Figure 2 and 3 show 95% nonsimultaneous conﬁdence intervals for
RMSE1’s and RMSE2’s. Shorter intervals for RMSE1’s would be prefer-
able, but obtaining them would be time-consuming. Four hundred experi-
ments of simulation 1.1 with p = 4, n = 20, and B = 100 took 16
computer hours on the PDP-11/34 minicomputer, whereas 400 experi-
ments of simulation 2.3 with p = 6, n = 60, and B = 100 took 72 hours.
Halving the length of the conﬁdence intervals in Figures 2 and 3 would
require four times the number of experiments and four times the com-
puter time. On the other hand, for each simulation in Figure 3, the conﬁ-
dence interval for RMSE2(
ideal) is disjoint from that of RMSE2(
boot), 
and both and disjoint from the conﬁdence intervals for RMSE2(
jack),
RMSE2(
cross), and RMSE2(
app). Thus, for RMSE2, we can convincingly
argue that the number of experiments is sufﬁcient.
5. THE RELATIONSHIP BETWEEN CROSS-VALIDATION
AND THE JACKKNIFE
Efron (1982) conjectured that the cross-validation and jackknife estimates
of excess error are asymptotically close. Gong (1982) proved Efron’s con-
jecture. Unfortunately, the regularity conditions stated there do not hold
for Gregory’s rule. The conjecture seems to hold for Gregory’s rule,
however, as evidenced in Figure 4, a scatterplot of the jackknife and cross-
validation estimates of the ﬁrst 100 experiments of simulation 1.1. The
plot shows points hugging the 45° line, whereas a scatterplot of the boot-
strap and cross-validation exhibits no such behavior.
ˆr
ˆr
ˆr
ˆr
ˆr
ˆr
MSE1
2
2
11
400
1
400
ˆ
ˆ
~
ˆ
,
r
E r
R
r
R
e
e
e
( ) ≡
−
[
]
−
[
]
=∑
M
M
1 2
1 2
0 1070
0 1078
100
∞
( ) =
=
(
)
.
~ .
;
ˆ
,
ˆ .
r
B
R
M B
r
B
b
b
B
B
=
∗
( ) =
( )
=∑
1
1
1
MSE
APPENDIX B
EXCESS ERROR ESTIMATION IN FORWARD LOGISTIC REGRESSION
217

6. CONCLUSIONS
Because complicated prediction rules depend intricately on the data and
thus have grossly optimistic apparent errors, error rate estimation for com-
plicated prediction rules is an important problem. Cross-validation is a
time-honored tool for improving the apparent error. This article compares
cross-validation with two other methods, the jackknife and the bootstrap.
With the help of increasingly available computer power, all three methods
are easily applied to Gregory’s complicated rule for predicting the
outcome of chronic hepatitis. Simulations suggest that whereas the jack-
knife and cross-validation do not offer signiﬁcant improvement over the
apparent error, the bootstrap shows substantial gain.
REFERENCES
Efron B. The Jackknife, the Bootstrap, and Other Resampling Plans, Philadelphia:
Society for Industrial and Applied Mathematics 1982.
—— “Estimating the Error Rate of a Prediction Rule: Improvements on Cross-
Validation,” Journal of the American Statistical Association, 1983; 78:316–331.
Friedman JR. “A Recursive Partitioning Decision Rule for Nonparametric Classiﬁ-
cation,” IEEE Transactions on Computers, C-26, 1977; 404–408.
Geisser S. “The Predictive Sample Reuse Method With Applications,” Journal of
the American Statistical Association, 1975; 70:320–328.
Gong G. “Cross-Validation, the Jackknife, and the Bootstrap: Excess Error Estima-
tion in Forward Logistic Regression,” unpublished Ph.D. thesis, Stanford Uni-
versity 1982.
Rao CR. Linear Statistical Inference and Its Applications, New York: John Wiley
1973.
Stone M. “Cross-Validatory Choice and Assessment of Statistical Predictions,”
Journal of the Royal Statistical Society, 1974; 36:111–147.
218
APPENDIX B
EXCESS ERROR ESTIMATION IN FORWARD LOGISTIC REGRESSION
*
*
*
*
0.40
0.30
0.20
0.10
0.00
0.40
0.30
0.20
0.10
0.00
0.00
0.10
0.20
0.30
0.40
cross
0.00
0.10
0.20
0.30
0.40
cross
jack
boot
5
5
8
2
2
2
1
2
2
1
2
1
1
2
1
1
1
1
1
1
4
1
5
5
5
5
22
1
1
1
8
7
43
3
4
6
1
7
4
1
2
1
1
1
1
1
1
1
7
9
FIGURE 4
Scatterplots to Compare
corss, 
jack and 
boot. The scatterplots sum-
marize the relationships among the three estimates for the ﬁrst 100 experiments 
of simulation 1.1. The numerals indicate the number of observations; * indicates
greater than 9.
ˆr
ˆr
ˆr

Glossary, Grouped 
by Related but 
Distinct Terms
GLOSSARY, GROUPED BY RELATED BUT DISTINCT TERMS
219
Common Errors in Statistics (and How to Avoid Them), 2e, by Phillip I. Good and James W. Hardin.
Copyright © 2006 John Wiley & Sons, Inc.
ACCURACY AND PRECISION
An accurate estimate is close to the estimated quantity. A precise interval
estimate is a narrow one. Precise measurements made with a dozen or
more decimal places may still not be accurate.
DETERMINISTIC AND STOCHASTIC
A phenomenon is deterministic when its outcome is inevitable and all
observations will take speciﬁc value.1 A phenomenon is stochastic when its
outcome may take different values in accordance with some probability
distribution.
DICHOTOMOUS, CATEGORICAL, ORDINAL, 
METRIC DATA
Dichotomous data have two values and take the form “yes or no,” “got
better or got worse.”
Categorical data have two or more categories such as yes, no, and unde-
cided. Categorical data may be ordered (opposed, indifferent, in favor) or
unordered (dichotomous, categorical, ordinal, metric).
Preferences can be placed on an ordered or ordinal scale such as
strongly opposed, opposed, indifferent, in favor, strongly in favor.
Metric data can be placed on a scale that permits meaningful subtrac-
tion; for example, while “in favor” minus “indifferent” may not be mean-
ingful, 35.6 pounds minus 30.2 pounds is.
1 These observations may be subject to measurement error.

Metric data can be grouped so as to evaluate it by statistical methods
applicable to categorical or ordinal data. But to do so would be to throw
away information, and reduce the power of any tests and the precision of
any estimates.
DISTRIBUTION, CUMULATIVE DISTRIBUTION,
EMPIRICAL DISTRIBUTION, LIMITING DISTRIBUTION
Suppose, we were able to examine all the items in a population and record
a value for each one to obtain a distribution of values. The cumulative dis-
tribution function of the population F[x] denotes the probability that an
item selected at random from this population will have a value less than or
equal to x. 0 ≤F[x] ≤1. Also, if x < y then F[x] ≤F[y].
The empirical distribution, usually represented in the form of a cumula-
tive frequency polygon or a bar plot, is the distribution of values observed
in a sample taken from a population. If Fn[x] denotes the cumulative dis-
tribution of observations in a sample of size n, then as the size of the
sample increases Fn[x] →F[x].
The limiting distribution for very large samples of a sample statistic such
as the mean or the number of events in a large number of very small
intervals often tends to a distribution of known form such as the Gaussian
for the mean or the Poisson for the number of events.
Be wary of choosing a statistical procedures which is optimal only for a
limiting distribution and not when applied to a small sample. For small
sample, the empirical distribution may be a better guide.
HYPOTHESIS, NULL HYPOTHESIS, ALTERNATIVE
The dictionary deﬁnition of a hypothesis is a proposition, or set of proposi-
tions, put forth as an explanation for certain phenomena.
For statisticians, a simple hypothesis would be that the distribution from
which an observation is drawn takes a speciﬁc form. For example, F[x] =
N(0,1). In the majority of cases, a statistical hypothesis will be compound
rather than simple, for example, that the distribution from which an obser-
vation is drawn has a mean of zero.
Often, it is more convenient to test a null hypothesis, for example, that
there is no or null difference between the parameters of two populations.
There is no point in performing an experiment or conducting a survey
unless one also has one or more alternate hypotheses in mind. If the alter-
native is one-sided, for example, the difference is positive rather than zero,
then the corresponding test will be one-sided. If the alternative is two-
sided, for example, the difference is not zero, then the corresponding test
will be two-sided.
220
GLOSSARY, GROUPED BY RELATED BUT DISTINCT TERMS

PARAMETRIC, NON-PARAMETRIC, AND 
SEMI-PARAMETRIC MODELS
Models can be subdivided into two components, one systematic and one
random. The systematic component can be a function of certain pre-
determined parameters (a parametric model), be parameter free 
(nonparametric), or be a mixture of the two types (semi-parametric). 
The deﬁnitions in the following section apply to the random component.
PARAMETRIC, NON-PARAMETRIC, AND 
SEMI-PARAMETRIC STATISTICAL PROCEDURES
Parametric statistical procedures concern the parameters of distributions of
a known form. One may want to estimate the variance of a normal distri-
bution or the number of degrees of freedom of a chi-square distribution.
Student’s t, the F-ratio, and maximum likelihood are typical parametric
procedures.
Non-parametric procedures concern distributions whose form is unspec-
iﬁed. One might use a nonparametric procedure like the bootstrap to
obtain an interval estimate for a mean or a median or to test that the dis-
tributions of observations drawn from two different populations are the
same. Non-parametric procedures are often referred to as distribution-free,
though not all distribution-free procedures are non-parametric in nature.
Semi-parametric statistical procedures concern the parameters of distri-
butions whose form is not speciﬁed. Permutation methods and U-statistics
are typically employed in a semi-parametric context.
RESIDUALS AND ERRORS
A residual is the difference between a ﬁtted value and what was actually
observed. An error is the difference between what is predicted based on a
model and what is actually observed.
SIGNIFICANCE LEVEL AND p-VALUE
The signiﬁcance level is the probability of making a Type I error. It is a
characteristic of a statistical procedure.
The p-value is a random variable that depends both upon the sample
and the statistical procedure that is used to analyze the sample.
If one repeatedly applies a statistical procedure at a speciﬁc signiﬁcance
level to distinct samples taken from the same population when the hypoth-
esis is true and all assumptions are satisﬁed, then the p-value will be less
than or equal to the signiﬁcance level with the frequency given by the 
signiﬁcance level.
GLOSSARY, GROUPED BY RELATED BUT DISTINCT TERMS
221

TYPE I AND TYPE II ERROR
A Type I error is the probability of rejecting the hypothesis when it is
true. A Type II error is the probability of accepting the hypothesis when
an alternative hypothesis is true. Thus, a Type II error depends on the
alternative.
TYPE II ERROR AND POWER
The power of a test for a given alternative hypothesis is the probability of
rejecting the original hypothesis when the alternative is true. A Type II
error is made when the original hypothesis is accepted even though the
alternative is true. Thus, power is one minus the probability of making a
Type II error.
222
GLOSSARY, GROUPED BY RELATED BUT DISTINCT TERMS

Bibliography
BIBLIOGRAPHY
223
Common Errors in Statistics (and How to Avoid Them), 2e, by Phillip I. Good and James W. Hardin.
Copyright © 2006 John Wiley & Sons, Inc.
Adams DC; Gurevitch J; Rosenberg MS. Resampling tests for meta-analysis of eco-
logical data. Ecology. 1997; 78:1277–1283.
Albers W; Bickel PJ; Van Zwet WR. Asymptotic expansions for the power of distri-
bution-free tests in the one-sample problem. Ann. Statist. 1976; 4:108–156.
Altman DG. Statistics in medical journals. Statist. Med. 1982; 1:59–71.
Altman DG. Randomisation. BMJ. 1991; 302:1481–1482.
Altman DG. The scandal of poor medical research. BMJ. 1994; 308:283–284.
Altman DG. Statistics in medical journals: developments in the 1980s. Statist. Med.
1991; 10:1897–1913.
Altman DG. Statistical reviewing for medical journals. Statist. Med. 1998;
17:2662–2674.
Altman DG. Commentary: Within trial variation—A false trail? J. Clin. Epidemiol.
1998; 51:301–303.
Altman DG. Statistics in medical journals: some recent trends. Statist. Med. 2000;
19:3275–3289.
Altman DG. Poor quality medical research: what can journals do? JAMA. 2002.
Altman DG; De Stavola BL; Love SB; Stepniewska KA. Review of survival analyses
published in cancer journals. Br. J. Cancer. 1995; 72:511–518.
Altman DG; Lausen B; Sauerbrei W; Schumacher M. Dangers of using “optimal”
cutpoints in the evaluation of prognostic factors. [Commentary] JNCI. 1994;
86:829–835.
Altman DG; Schulz KF; Moher D; Egger M; Davidoff F; Elbourne D; Gøtzsche
PC; Lang T for the CONSORT Group. The revised consort statement for
reporting randomized trials: explanation and elaboration. Annals Internal Med.
2001; 134:663–694.
Aly E-E AA. Simple test for dispersive ordering. Statist. Prob. Letters. 1990;
9:323–325.
Andersen B. Methodological Errors in Medical Research. Blackwell, Oxford, 1990.

Anderson DR; Burnham KP; Thompson WL. Null hypothesis testing: Problems,
prevalence, and an alternative. J. Wildlife Management. 2000; 64:912–923.
Anderson S; Hauck WW. A proposal for interpreting and reporting negative
studies: Statist. Med. 1986; 5:203–209.
Anscombe F. Sequential medical trials (book review). JASA. 1963; 58:365.
Armitage P. Test for linear trend in proportions and frequencies. Biometrics. 1955;
11:375–386.
Avram MJ; Shanks CA; Dykes MHM; Ronai AK; Stiers WM. Statistical methods in
anesthesia articles: An evaluation of two American journals during two six-
month periods. Anesthesia and Analgesia. 1985; 64:607–611.
Bacchetti P. Peer review of statistics in medical research: the other problem. BMJ.
2002; 324:1271–1273.
Badrick TC; Flatman RJ. The inappropriate use of statistics. NZ. J. Med. Lab. Sci.
1999; 53:95–103.
Bailar JC; Mosteller F. Guidelines for statistical reporting in articles for medical
journals. Ampliﬁcations and explanations. Annals of Internal Medicine. 1988;
108:66–73.
Bailey KR. Inter-study differences: how should they inﬂuence the interpretation
and analysis of results? Statist. Med. 1987; 6:351–358.
Bailor AJ. Testing variance equality with randomization tests. Statist. Comp. Simul.
1989; 31:1–8.
Balakrishnan N; Ma CW. A comparative study of various tests for the equality of
two population variances. Statist. Comp. Simul. 1990; 35:41–89.
Baker RD. Two permutation tests of equality of variance. Statist. Comput. 1995;
5(4):289–296.
Barbui C; Violante A; Garattini S. Does placebo help establish equivalence in trials
of new antidepressants? Eur. Psychiatry. 2000; 15:268–273.
Barnston AG; van den Dool HM. A degeneracy in cross-validated skill in 
regression-based forecasts. J. Climate. 1993; 6:963–977.
Barrodale I; Roberts FDK. An improved algorithm for discrete l1 linear approxima-
tions. Soc. Industr. Appl. Math. J. Numerical Anal. 1973; 10:839–848.
Bayarri MJ; Berger J. Quantifying surprise in the data and model veriﬁcation. In:
Bernado et al, eds. Bayesian Statistics. Oxford: Oxford University Press, 1998;
53–82.
Bayes T. An essay toward solving a problem in the doctrine of chances. Philosophi-
cal Transactions of the Royal Society. 1763; 53:370–418.
Begg CB; Cho M; Eastwood S; Horton R; Moher D; Olkin I; Pitkin R; Rennie 
D; Schulz KF; Simel D; Stroup DF. Improving the quality of reporting of 
randomized controlled trials: the CONSORT Statement. JAMA. 1996;
276:637–639.
Bent GC; Archﬁeld SA. A logistic regression equation for estimating the probabil-
ity of a stream ﬂowing perennially in Massachusetts USGC. Water-Resources
Investigations Report 02-4043.
Berger JO. Statistical Decision Theory and Bayesian Analysis; 2nd ed.; Springer-
Verlag: New York. 1986.
224
BIBLIOGRAPHY

Berger JO. Could Fisher, Jefferies, and Neyman have agreed on testing? Statist.
Sci. In press.
Berger JO; Berry DA. Statistical analysis and the illusion of objectivity. The Ameri-
can Scientist. 1988; 76:159–165.
Berger JO; Sellke T. Testing a point null hypothesis: The irreconcilability of P-
values and evidence. JASA. 1987; 82:112–122.
Berger V. Pros and cons of permutation tests. Statist. Med. 2000; 19:1319–1328.
Berger VW. Improving the information content of endpoints in clinical trials. 
Controlled Clinical Trials. 2002; In press.
Berger VW; Exner DV. Detecting selection bias in randomized clinical trials. 
Controlled Clinical Trials. 1999; 20:319–327.
Berger VW; Lunneborg C; Ernst MD; Levine JG. Parametric analyses in random-
ized clinical trials. J. Modern Appl. Statist. Meth. 2002; 1:74–82.
Berger VW; Ivanova A. Bias of linear rank tests for stochastic order in ordered cat-
egorical data. J. Statist. Planning and Inference. 2002; 107: In press.
Berger VW; Permutt T; Ivanova A. Convex hull test of ordered categorical data.
Biometrics. 1998; 54:1541–1550.
Berkeley G. Treatise Concerning the Principles of Human Knowledge. Oxford Uni-
versity Press. 1710.
Berkey C; Hoaglin D; Mosteller F; Colditz G. A random effects regression model
for meta-analysis. Statist. Med. 1995; 14:395–411.
Berkson J. Tests of signiﬁcance considered as evidence. JASA. 1942; 37:325–335.
Berlin JA; Laird NM; Sacks HS; Chalmers TC. A comparison of statistical methods
for combining event rates from clinical trials. Statist. Med. 1989; 8:141–151.
Berry DA. Decision analysis and Bayesian methods in clinical trials. In Recent
Advances in Clinical Trial Design and Analysis. 125–154. Kluwer Press, New
York. (Ed: Thall P). 1995.
Berry DA. Statistics: A Bayesian Perspective. Duxbury Press: Belmont; California.
1996.
Berry DA; Stangl DK. Bayesian Biostatistics. Marcel Dekker; New York. 1996.
Bickel P; Klassen CA; Ritov Y; Wellner. Efﬁcient and Adaptive Estimation for
Semi-parametric Models. Johns Hopkins University Press.: Baltimore. 1993.
Bland JM; Altman DG. Comparing methods of measurement: why plotting 
difference against standard method is misleading. Lancet. 1995;
346:1085–1087.
Block G. A review of validations of dietary assessment methods. Am. J. Epidemiol.
1982; 115:492–505.
Bly RW. Power-Packed Direct Mail: How to Get More Leads and Sales by Mail.
Henry Holt 1996.
Bly RW. The Copywriter’s Handbook: A Step-By-Step Guide to Writing Copy That
Sells. Henry Holt. 1990.
Blyth CR. On the inference and decision models of statistics (with discussion).
Ann. Statist. 1970; 41:1034–1058.
Bothun G. Modern Cosmological Observations and Problems. Taylor and Francis:
London. 1998.
BIBLIOGRAPHY
225

Box GEP; Anderson SL. Permutation theory in the development of robust criteria
and the study of departures from assumptions. JRSS-B. 1955; 17:1–34.
Box GEP; Tiao GC. A note on criterion robustness and inference robustness. 
Biometrika. 1964; 51:169–173.
Breiman L. Bagging Predictors. Machine Learning. 1996; 24:123–140.
Breiman L; Friedman JH; Olshen RA; Stone CJ. Classiﬁcation and Regression
Trees. Wadsworth and Brooks: Monterey, CA. 1984.
Brockwell PJ; Davis RA. Time Series: Theory and Methods. Springer-Verlag: New
York. 1987.
Browne MW. A comparison of single sample and cross-validation methods for 
estimating the mean squared error of prediction in multiple linear regression.
British J. Math. Statistist Psychol. 1975; 28:112–120.
Buchanan-Wollaston H. The philosophic basis of statistical analysis. J. Int. Council
Explor. Sea. 1935; 10:249–263.
Cade B; Richards L. Permutation tests for least absolute deviation regression. Bio-
metrics. 1996; 52:886–902.
Callaham ML; Wears RL; Weber EJ; Barton C; Young G. Positive-outcome bias
and other limitations in the outcome of research abstracts submitted to a scien-
tiﬁc meeting. JAMA. 1998; 280:254–257.
Camstra A; Boomsma A. Cross-validation in regression and covariance structure
analysis. Sociological Methods and Research. 1992; 21:89–115.
Canty AJ; Davison AC; Hinkley DV; Ventura V. Bootstrap diagnostics.
http://www.stat.cmu.edu/www/cmu-stats/tr/tr726/tr726.html
Cappuccio FP; Elliott P; Allender PS; Pryer J; Follman DA; Cutler JA. Epidemio-
logic association between dietary calcium intake and blood pressure: a meta-
analysis of published data. Am. J. Epidemiol. 1995; 142:935–945.
Carlin BP; Louis TA. Bayes and Empirical Bayes Methods For Data Analysis.
Chapman and Hall: London, U.K. 1996.
Carleton RA; Lasater TM; Assaf AR; Feldman HA; McKinlay S; et al. The Paw-
tucket Heart Health Program: Community changes in cardiovascular risk factors
and projected disease risk. Am. J. Public Health. 1995; 85:777–785
Carmer SG; Walker WM. Baby bear’s dilemma: A statistical tale. Agronomy
Journal. 1982; 74:122–124.
Carpenter J; Bithell J. Bootstrap conﬁdence intervals. Statist. Med. 2000;
19:1141–1164.
Carroll RJ; Ruppert D. Transformations in regression: a robust analysis. Techno-
metrics. 1985; 27:1–12.
Carroll RJ; Ruppert D. Transformation and Weighting in Regression. CRC. 2000.
Carroll RJ; Ruppert D; Stefanski LA. Measurement Error in Nonlinear Models,
Chapman and Hall, New York. 1995.
Casella G; Berger RL. Statistical Inference. Paciﬁc Grove CA: Wadsworth &
Brooks. 1990.
Chalmers TC. Problems induced by meta-analyses. Statist. Med. 1991;
10:971–980.
Chalmers TC; Frank CS; Reitman D. Minimizing the three stages of publication
bias. JAMA. 1990; 263:1392–1395.
226
BIBLIOGRAPHY

Charlton BG. The future of clinical research: from megatrials towards method-
ological rigour and representative sampling. J. Eval. Clin. Pract. 1996;
2:159–169.
Chernick MR; Liu CY. The saw-toothed behavior of power versus sample size and
software solutions: single binomial proportion using exact methods. American
Statistician. 2002; 56:149–155 .
Cherry S. Statistical tests in publications of The Wildlife Society, Wildlife Society
Bulletin. 1998; 26:947–953.
Chiles JR. Inviting Disaster: Lessons from the Edge of Technology. Harper-Collins:
New York. 2001.
Choi BCK. Development of indicators for occupational health and safety surveil-
lance. Asian-Paciﬁc Newsletter. 2000; 7.
http://www.ttl.ﬁ/Internet/English/Information/Electronic+journals/Asian-
Paciﬁc+Newsletter/2000-01/04.htm
Clemen RT. Combining forecasts: A review and annotated bibliography. Interna-
tional Journal of Forecasting. 1989; 5:559–583.
Clemen RT. Making Hard Decisions. PWS-Kent; Boston. 1991.
Clemen RT; Jones SK; Winkler RL. Aggregating forecasts: an empirical evaluation
of some Bayesian methods. In Bayesian Analysis in Statistics and Econometrics.
(Ed: Berry DA; Chaloner K) pp. 3–13. John Wiley & Sons: New York. 1996.
Cleveland WS. The Elements of Graphing Data. Hobart Press: Summit, NJ. 1985,
1994.
Cleveland WS; McGill ME. Dynamic Graphics Statistics. London: Press. 1988.
Cochran WG. Sampling Techniques (3rd ed.) New York: Wiley: NY. 1977.
Cohen J. Things I have learned (so far). American Psychologist. 1990;
45:1304–1312.
Collins R; Keech A; Peto R; Sleight P; Kjekshus J; Wilhelmsen L; et al. Choles-
terol and total mortality: need for larger trials. BMJ. 1992; 304:1689.
Conover W; Salsburg D. Biometrics. 1988; 44:189–196.
Conover WJ; Johnson ME; Johnson MM. Comparative study of tests for homo-
geneity of variances: with applications to the outer continental shelf bidding
data. Technometrics. 1981; 23:351–361.
Converse JM; Presser S. Survey Questions: Handcrafting the Standardized Ques-
tionaire. Sage. 1986. 
Cooper HM; Rosenthal R. Statistical versus traditional procedures for summarising
research ﬁndings. Psychol. Bull. 1980; 87:442–449.
Copas JB; Li HG. Inference for non-random samples (with discussion). JRSS.
1997; 59:55–95.
Cornﬁeld J; Tukey JW. Average values of mean squares in factorials. Ann. Math.
Statist. 1956; 27:907–949.
Cox DR. Some problems connected with statistical inference. Ann. Math. Statist.
1958; 29:357–372.
Cox DR. The role of signiﬁcance tests. Scand J. Statist. 1977; 4:49–70.
Cox DR. Seven Common Errors in Statistics and Causality. JRSS A. 1992;
155:291.
BIBLIOGRAPHY
227

Cox DR. Some remarks on consulting. Liaison (Statistical Society of Canada).
1999; 13:28–30.
Cummings P; Koepsell TD. Statistical and design issues in studies of groups. Inj.
Prev. 2002; 8:6–7.
Dar R; Serlin; Omer H. Misuse of statistical tests in three decades of psychother-
apy research. J. Consult. Clin. Psychol. 1994; 62:75–82.
Davision AC; Hinkley DV. Bootstrap Methods and Their Application. Cambridge
University Press. 1997.
Davision AC; Snell EJ. Residuals and diagnostics. In Statistical Theory and Model-
ling, DV. Hinkley, N. Reid, and EJ Shell, eds. Chapman and Hall: London. 
p. 83. 1991.
Day S. Blinding or masking. In Encyclopedia of Biostatistics, v1, P. Armitage and T.
Colton, Editors, Johns Wiley and Sons, Chichester. 1998.
Delucchi KL. The use and misuse of chisquare. Lewis and Burke revisited. Psych.
Bull. 1983; 94:166–176.
Diaconis P. Statistical problems in ESP research. Science. 1978; 201:131–136.
Diciccio TJ; Romano JP. A review of bootstrap conﬁdence intervals (with discus-
sion). JRSS B. 1988; 50:338–354.
Dietmar SD; Dewitte K; Thienpont LM. Validity of linear regression in method
comparison studies: is it limited by the statistical model or the quality of the
analytical input data? Clinical Chemistry. 1998; 44:2340–2346.
Dixon PM. Assessing effect and no effect with equivalence tests. In Newman MC,
Strojan CL, eds. Risk Assessment: Logic and Measurement. Chelsea (MI): Ann
Arbor Press. 1998.
Djulbegovic B; Lacevic M; Cantor A; Fields KK; Bennett CL; Adams JR; Kuderer
NM; Lyman GH. The uncertainty principle and industry-sponsored research.
Lancet. 2000; 356:635–638.
Donner A; Brown KS; Brasher P. A methodological review of non-therapeutic
intervention trials employing cluster randomization, 1979–1989. Int. J. Epi-
demiol. 1990; 19:795–800.
Duggan TJ; Dean CW. Common misinterpretations of signiﬁcance levels in socio-
logical Journals. Amer. Sociologist. 1968; February: 45–46.
Dyke G. How to avoid bad statistics. Field Crops Research. 1997; 51:165–197.
Easterbrook PJ; Berlin JA; Gopalan R; Matthews DR. Publication bias in clinical
research. Lancet. 1991; 337:867–72.
Edwards W; Lindman H; Savage L. Bayesian statistical inference for psychological
research. Psychol Rev. 1963; 70:193–242.
Efron B. Bootstrap methods, another look at the jackknife. Annals Statist. 1979;
7:1–26.
Efron B. The Jackknife, the Bootstrap, and Other Resampling Plans. Philadelphia:
SIAM. 1982.
Efron B. Better bootstrap conﬁdence intervals, with discussion). JASA. 1987;
82:171–200.
Efron B. Bootstrap conﬁdence intervals: good or bad? (with discussion). Psychol.
Bull. 1988; 104:293–296.
228
BIBLIOGRAPHY

Efron B. Six questions raised by the bootstrap. R. LePage and L. Billard, eds.
Exploring the Limits of the Bootstrap. New York: Wiley; 1992.
Efron B; Morris C. Stein’s paradox in statistics. Sci. Amer. 1977; 236:119–127.
Efron B; Tibshirani R. Bootstrap methods for standard errors, conﬁdence intervals,
and other measures of statistical accuracy. Statist. Sci. 1986; 1:54–77.
Efron B; Tibshirani R. An Introduction to the Bootstrap. New York: Chapman and
Hall; 1993.
Egger M; Smith GD. Meta-analysis: Potentials and promise. BMJ. 1997;
315:1371–1374.
Egger M; Smith GD; Phillips AN. Meta-analysis: Principles and procedures. BMJ.
1997; 315:1533–1537.
Egger M; Schneider M; Smith GD. Spurious precision? Meta-analysis of observa-
tional studies. British Med. J. 1998; 316:140–143.
Ehrenberg ASC. Rudiments of numeracy. JRSS Series A. 1977; 140:277–297.
Ellis SP. Instability of least squares, least absolute deviation and least median of
squares linear regression. Statist. Sci. 1998; 13:337–350.
Elwood JM. Critical Appraisal Of Epidemiological Studies And Clinical Trials. 2nd
ed. New York: Oxford University Press. 1998.
Eysenbach G; Sa E-R. Code of conduct is needed for publishing raw data. BMJ.
2001; 323:166.
Fears TR; Tarone RE; Chu KC. False-positive and false-negative rates for carcino-
genicity screens. Cancer Res. 1977; 37:1941–1945.
Feinstein AR. P-values and conﬁdence intervals: two sides of the same unsatisfac-
tory coin. J. Clin. Epidem. 1998; 51:355–360.
Feinstein AR; Concato J. The quest for “power”: contradictory hypotheses and
inﬂated sample sizes. J. Clin. Epidem. 1998; 51:537–545.
Feller W. An Introduction to Probability Theory and Its Applications. v2. Wiley:
New York. 1966.
Felson DT; Anderson JJ; Meenan RF. The comparative efﬁcacy and toxicity of
second-line drugs in rheumatoid arthritis. Arthritis and Rheumatism. 1990;
33:1449–1461.
Felson DT; Cupples LA; Meenan RF. Misuse of statistical methods in Arthritis
and Rheumatism. 1982 versus 1967–68. Arthritis and Rheumatism. 1984;
27:1018–1022.
Feng Z; Grizzle J. Correlated binomial variates: properties of estimator of ICC
and its effect on sample size calculation. Statist. Med. 1992; 11:1607–1614.
Feng Z; McLerran D; Grizzle J. A comparison of statistical methods for clustered
data analysis with Gaussian error. Statist. Med. 1996; 15:1793–1806.
Feng Z; Diehr P; Peterson A; McLerran D. Selected statistical issues in group ran-
domized trials. Annual Rev. Public Health. 2001; 22:167–187.
Fienberg SE. Damned lies and statistics: misrepresentations of honest data. In:
Editorial Policy Committee. Ethics and Policy in Scientiﬁc Publications. Council
of Biology Editors. 1990. 202–206.
Fink A; Kosecoff JB. How to Conduct Surveys: A Step by Step Guide. Sage. 1988.
BIBLIOGRAPHY
229

Finney DJ. The responsible referee. Biometrics. 1997; 53:715–719.
Firth D. General linear models. In Statistical Theory and Modelling, DV. Hinkley,
N. Reid, and EJ Shell, eds. Chapman and Hall: London. p. 55. 1991.
Fisher NI; Hall P. On bootstrap hypothesis testing. Australian J. Statist. 1990;
32:177–190.
Fisher NI; Hall P. Bootstrap algorithms for small samples. J. Statist. Plan Infer.
1991; 27:157–169.
Fisher RA. Design of Experiments. New York: Hafner; 1935.
Fisher RA. Statistical Methods and Scientiﬁc Inference. 3rd ed. New York: Macmil-
lan, 1973.
Fleming TR. Surrogate markers in AIDs and cancer trials. Statist. Med. 1995;
13:1423–1435.
Fligner MA; Killeen TJ. Distribution-free two-sample tests for scale. JASA. 1976;
71:210–212.
Fowler FJ Jr; Fowler FJ. Improving Survey Questions: Design and Evaluation. Sage.
1995.
Frank D; Trzos RJ; Good P. Evaluating drug-induced chromosome alterations.
Mutation Res. 1978; 56:311–317.
Freedman DA. A note on screening regression equations. Amer. Statist. 1983;
37:152–155.
Freedman DA. From association to causation. Statist. Sci. 1999.
Freedman DA; Navidi W; Peters SC. On the impact of variable selection in ﬁtting
regression equations. In Dijkstra TK ed., On Model Uncertainty and Its Statisti-
cal Implications. Springer: Berlin. 1988. pp. 1–16.
Friedman LM; Furberg CD; DeMets DL. Fundamentals Of Clinical Trials. 3rd ed.
St. Louis: Mosby. 1996.
Friedman M. The use of ranks to avoid the assumption of normality implicit in the
analysis of variance. JASA. 1937; 32:675–701.
Freiman JA; Chalmers TC; Smith H; Kuebler RR. The importance of beta; the
type II error; and sample size in the design and interpretation of the random-
ized controlled trial. In: Bailar JC; Mosteller F; eds. Medical uses of statistics.
Boston; MA: NEJM Books; 1992; 357.
Fritts HC; Guiot J; and Gordon GA. Veriﬁcation; in Cook ER; and Kairiukstis LA;
eds; Methods of Dendrochronology; Applications in the Environmental Sciences:
Kluwer Academic Publishers. 1990. pp. 178–185.
Gail MH; Byar DP; Pechacek TF; Corle DK. Aspects of statistical design for the
Community Intervention Trial for Smoking Cessation (COMMIT). Cont. Clin.
Trials. 1992; 123:6–21
Gail MH; Mark SD; Carroll R; Green S; Pee D. On design considerations and ran-
domization-based inference for community intervention trials. Statist. Med.
1996; 15:1069–1092
Gail MH; Tan WY; Piantadosi S. Tests for no treatment effect in randomized clini-
cal trials. Biometrika. 1988; 75:57–64.
Gallant AR. Nonlinear Statistical Models. Wiley: New York. 1987.
Gardner MJ; Altman DG. Conﬁdence intervals rather than P values: Estimation
rather than hypothesis testing. BMJ. 1996; 292:746–750.
230
BIBLIOGRAPHY

Gardner MJ. and Bond J. An exploratory study of statistical assessment of papers
published in the Journal of the American Medical Association. JAMA. 1990;
263:1355–1357.
Gardner MJ; Machin D; Campbell MJ. Use of check lists in assessing the statistical
content of medical studies. BMJ. 1986; 292:810–812.
Garthwaite PH. Conﬁdence intervals from randomization tests. Biometrics. 1996;
52:1387–1393.
Gastwirth JL; Rubin H. Effect of dependence on the level of some one-sample
tests. JASA. 1971; 66:816–820.
Gavarret J. Principes Généraux de Statistique Medicale. Libraires de la Faculte de
Medecine de Paris, Paris. 1840.
Geary RC. Testing normality. Biometrika. 1947; 34:241.
George SL. Statistics in medical journals: a survey of current policies and proposals
for editors. Medical and Pediatric Oncology. 1985; 13:109–112.
Geweke JK; DeGroot MH. Optimal Statistical Decisions. McGraw-Hill: New York.
1970.
Gigerenzer G. Calculated Risks: How To Know When Numbers Deceive You. Simon
& Schuster: NY. 2002.
Gill J. Whose variance is it anyway? Interpreting empirical models with state-level
data. State Politics and Policy Quarterly. Fall 2001; 318–338.
Gillett R. Meta-analysis and bias in research reviews. Journal of Reproductive and
Infant Psychology. 2001; 19:287–294.
Gine E; Zinn J. Necessary conditions for a bootstrap of the mean. Ann. Statist.
1989; 17:684–691.
Glantz S. Biostatistics: how to detect: correct: and prevent errors in the medical
literature. Circulation. 1980; 61:1–7.
Glass GV; Peckham PD; Sanders JR. Consequences of failure to meet the assump-
tions underlying the ﬁxed effects analysis of variance and covariance. Reviews in
Educational Research. 1972; 42:237–288.
Goldberger, Arthur S. Note on stepwise least squares. JASA. 1981; 56–293.
105–110.
Gong G. Cross-validation, the jackknife and the bootstrap: Excess error in forward
logistic regression. JASA. 1986; 81:108–113.
Good IJ. Probability and the Weighing of Evidence. London: Grifﬁn. 1950.
Good IJ. The Bayes/non-Bayes compromise: a brief review. JASA. 1992; 87;
597–606.
Good PI. Permutation Tests. 2nd ed. New York: Springer. 2000.
Good PI. Resampling Methods. 2nd ed. Boston: Birkhauser. 2001.
Good PI. Extensions of the concept of exchangeability and their applications to
testing hypotheses. J. Modern Stat. Anal. 2002; 2.
Goodman SN. Towards evidence-based medical statistics. II. The Bayes Factor.
Ann. Intern. Med. 1999; 130:1005–1013.
Goodman SN. Of p-values and Bayes: a modest proposal. Epidemiology. 2001;
12:295–297.
Goodman SN; Altman DG; George SL. Statistical reviewing policies of medical
journals: Caveat lector? J. Gen. Intern. Med. 1998; 13:753–756.
BIBLIOGRAPHY
231

Gore S; Jones IG; Rytter EC. Misuse of statistical methods: critical assessment of
articles in BMJ from January to March 1976. BMJ. 1977; 1:85–87.
Götzsche PC. Reference bias in reports of drug trials. BMJ. 1987; 295:654–656.
Götzsche PC; Podenphant J; Olesen M; Halberg P. Meta-analysis of second-line
antirheumatic drugs: sample size bias and uncertain beneﬁt. J. Clin. Epidemiol.
1992; 45:587–594.
Grant A. Reporting controlled trials. British J. Obstetrics and GynaEcology. 1989;
96:397–400.
Graumlich L. A 1000-year record of temperature and precipitation in the Sierra
Nevada, Quaternary Research. 1993; 39:249–255.
Green PJ; Silverman BW. Nonparametric Regression and Generalized Linear
Models. Chapman and Hall: London. 1994.
Greene HL; Roden DM; Katz RJ; et al. The Cardiac Arrhythmia Suppression
Trial: ﬁrst CAST . . . then CAST II. J. Am. Coll. Cardiol. 1992; 19:894–898.
Greenland S. Modeling and variable selection in epidemiologic analysis. Am. J.
Public Health. 1989; 79:340–349.
Greenland S. Randomization, statistics, and causal inference, Epidemiology. 1990;
1:421–429.
Greenland S. Probability logic and probabilistic induction [see comments]. Epi-
demiology. 1998; 9:322–332.
Gurevitch J; Hedges LV. Meta-analysis: combining the results of independent
studies in experimental Ecology. Pages 378–398 in S. Scheiner and J. Gurevitch;
editors. The Design And Analysis Of Ecological Experiments. Chapman & Hall:
London. 1993.
Guthery FS; Lusk JJ; Peterson MJ. The fall of the null hypothesis: liabilities and
opportunities. J. Wildlife Management. 2001; 65:379–384.
Guttorp P. Stochastic Modeling of Scientiﬁc Data. Chapman & Hall: London. 1995.
Hagood MJ. Statistics for Sociologists. NY: Reynal and Hitchcock. 1941.
Hall P; Wilson SR. Two guidelines for bootstrap hypothesis testing. Biometrics.
1991; 47:757–762.
Hardin JW; Hilbe JM. Generalized Estimating Equations. Chapman and
Hall/CRC: London. 2003.
Harley SJ; Myers RA. Hierarchical Bayesian models of length-speciﬁc catchability
of research trawl surveys. Canadian J. Fisheries Aquatic Sciences. 2001;
58:1569–1584.
Harrell FE; Lee KL. A comparison of the discrimination of discriminant analysis
and logistic regression under multivariate normality. In Biostatistics: Statistics 
in Biomedical; Public Health; and Environmental Sciences. The Bernard G. 
Greenberg Volume. ed. PK Sen. New York: North-Holland. 1985. pp. 333–343.
Harrell FE; Lee KL; Mark DB. Multivariable prognostic models: Issues in develop-
ing models; evaluating assumptions and adequacy; and measuring and reducing
errors. Statist. Med. 1996; 15:361–387.
Hastie T; Tibshirani R; Friedman JH. The Elements of Statistical Learning : Data
Mining, Inference, and Prediction. Springer. 2001.
Hedges LV; Olkin I. Statistical Methods For Meta-Analysis. Academic Press: New
York. 1985.
232
BIBLIOGRAPHY

Hertwig R; Todd PM. Biases to the left, fallacies to the right: stuck in the middle
with null hypothesis signiﬁcance testing. Psycoloquy. 2000; 11: #28. (with 
discussion)
Hilton J. The appropriateness of the Wilcoxon test in ordinal data. Statist. Med.
1996; 15:631–645.
Hinkley DV; Shi S. Importance sampling and the nested bootstrap. Biometrika.
1989; 76:435–446.
Hoenig JM; Heisey DM. The abuse of power: The pervasive fallacy of power cal-
culations for data analysis. Amer. Statist. 2001; 55:19–24.
Horowitz RI. Large scale randomised evidence; large simple trials and overviews of
trials: discussion—a clinician’s perspective on meta-analysis. J. Clin. Epidemiol.
1995; 48:41–44.
Horwitz RI; Singer BH; Makuch RW; Viscolia CM. Clinical versus statistical con-
siderations in the design and analysis of clinical research. J. Clinical Epidemiol-
ogy. 1998; 51:305–307.
Hosmer DW; Lemeshow SL. Applied Logistic Regression. Wiley: NY. 2001.
Hsu JC. Multiple Comparisons: Theory and Methods. Chapman & Hall/CRC,
1996.
Huber PJ. Robust Statistics. Wiley: New York. 1981.
Hume D. An Enquiry Concerning Human Understanding. Oxford University
Press. 1748.
Hungerford TW. Algebra. Holt, Rinehart, and Winston: NY. 1974.
Hunter JE; Schmidt FL. Eight common but false objections to the discontinuation
of signiﬁcance testing in the analysis of research data. Pages 37–64 in L. L.
Harlow; S. A. Mulaik; and J. H. Steiger, eds. What If There Were No Signiﬁcance
Tests? Lawrence Erlbaum Assoc.: Mahwah, NJ. 1997.
Hurlbert SH. Pseudoreplication and the design of ecological ﬁeld experiments.
Ecological Monographs. 1984; 54:198–211.
Husted JA; Cook RJ; Farewell VT; Gladman DD. Methods for assessing respon-
siveness: a critical review and recommendations. J. Clinical Epidemiology. 2000;
53:459–468.
Hutchon DJR. Infopoints: Publishing raw data and real time statistical analysis on
e-journals. BMJ. 2001; 322:530.
International Committee of Medical Journal Editors. Uniform requirements for
manuscripts submitted to biomedical journals. JAMA. 1997; 277;927–934.
International Study of Infarct Survival Collaborative Group. Randomized trial of
intravenous streptokinase, oral aspirin, both or neither, among 17187 cases of
suspected acute myocardial infarction. ISIS-2. Lancet. 1988; 2:349–362.
Jennison C; Turnbull BW. Group Sequential Methods with Applications to Clinical
Trials. CRC. 1999.
Johnson DH. The insigniﬁcance of statistical signiﬁcance testing. J. Wildlife Man-
agement. 1999; 63:763–772.
Jones LV. Statistics and research design. Annual Review Psych. 1955; 6:405–430.
Jones LV; Tukey JW. A sensible formulation of the signiﬁcance test. Psychol. Meth.
2000; 5:411–416.
BIBLIOGRAPHY
233

Kadane IB; Dickey J; Winklcr R; Smith W; Peters S. Interactive elicitation of
opinion for a normal linear model. JASA. 1980; 75:845–854.
Kanarek MS; Conforti PM; Jackson LA; Cooper RC; Murchio JC. Asbestos in
drinking water and cancer incidence in the San Francisco Bay Area. Amer. J.
Epidemiol. 1980; 112:54–72.
Kaplan J. Misuses of statistics in the study of intelligence: the case of Arthur
Jensen (with disc). Chance. 2001; 14:14–26.
Kass R; Raftery A. Bayes factors. JASA. 1995; 90:773–795.
Keynes JM. A Treatise on Probability. Macmillan: London. 1921.
Knight K. On the bootstrap of the sample mean in the inﬁnite variance case.
Annal Statist. 1989; 17:1168–1173.
Koenker R; Hallock KF. Quantile regression. J. Economic Perspectives. 2001;
15:143–156.
Lachin JM. Sample size determination. In Encyclopedia of Biostatistics, 5. Armitage
P; Colton T. (editors). John Wiley and Sons: Chichester. 1998. pp. 3892–3903.
Lambert D. Robust two-sample permutation tests. Ann. Statist. 1985; 13:606–25.
Lang TA; Secic M. How to Report Statistics in Medicine. American College of
Physicians. Philadelphia. 1997.
Lehmann EL. Testing Statistical Hypotheses. 2nd ed. New York: John Wiley and
Sons; 1986. (pp. 203–213 on robustness)
Lehmann EL. The Fisher, Neyman-Pearson theories of testing hypotheses: one
theory or two? JASA. 1993; 88:1242–1249.
Lehmann EL; Casella G. Theory of Point Estimation. Springer: New York. 2nd ed.
1998.
Lehmann EL; D’Abrera HJM. Nonparametrics: Statistical Methods Based on Ranks.
McGraw-Hill: New York. 2nd ed. 1988.
Leizorovicz A; Haugh MC; Chapuis F-R; Samama MM; Boissel J-P. Low molecu-
lar weight heparin in prevention of perioperative thrombosis. BMJ. 1992;
305:913–20.
Lettenmaier DP. Space-time correlation and its effect on methods for detecting
aquatic ecological change. Canadian J. Fisheries Aquatic Science. 1985;
42:1391–1400. Correction-1986; 43:1680.
Lewis D; Burke CJ. Use and misuse of the chi-square test. Psych. Bull. 1949;
46:433–489.
Lieberson S. Making it Count. University of California Press, Berkeley. 1985.
Light RJ; Pillemer DB. Summing Up: The Science of Reviewing Research. Harvard
University Press: Cambridge; Massachusetts. 1984.
Lindley DV. The choice of sample size. The Statistician. 1997; 46:129–138;
163–166.
Lindley D. The philosophy of statistics (with discussion). The Statistician. 2000;
49:293–337.
Linnet K., Performance of Deming regression analysis in case of misspeciﬁed ana-
lytical error ratio in method comparison studies. Clinical Chemistry. 1998;
44:1024–1031.
234
BIBLIOGRAPHY

Linnet K. Necessary sample size for method comparison studies based on regres-
sion analysis. Clinical Chemistry. 1999; 45:882–894.
Lissitz RW; Chardos S. A study of the effect of the violation of the assumption of
independent sampling upon the type I error rate of the two group t-test.
Educat. Psychol. Measurement. 1975; 35:353–359.
Little RJA; Rubin DB. Statistical Analysis with Missing Data. John Wiley and
Sons: New York. 1987.
Loader C. Local Regression and Likelihood. Springer: NY. 1999.
Locke J. Essay Concerning Human Understanding. Prometheus Books. 4th ed. 1700.
Lonergan JF. Insight: A Study of Human Understanding. Univ of Toronto Press.
1992.
Lord FM. Statistical adjustment when comparing preexisting groups. Psych. Bull.
1969; 72:336–337.
Lovell DJ; Giannini EH; Reiff A; Cawkwell GD; Silverman ED; Nocton JJ; Stein
LD; Gedalia A; Ilowite NT; Wallace CA; Whitmore J; Finck BK. The Pediatric
Reumatology Collaborative Study Group. Etanercept in children with polyartic-
ular juvenile rheumatoid arthritis. New Engl. J. Med. 2000; 342:763–769.
MacArthur RD. and Jackson GG. An evaluation of the use of statistical methodol-
ogy in the Journal of Infectious Diseases. J. Infectious Diseases. 1984;
149:349–354.
Malone KM; Corbitt EM; Li S; Mann JJ. Prolactin response to fenuramine and
suicide attempt lethality in major depression. British J. Psychiatry. 1996;
168:324–329.
Mangel M; Samaniego FJ. Abraham Wald’s work on aircraft survivability. JASA.
1984; 79:259–267.
Manly BFJ. Randomization, Bootstrap and Monte Carlo Methods in Biology. (2nd
ed.). London: Chapman & Hall; 1997.
Manly B; Francis C. Analysis of variance by randomization when variances are
unequal. Aust. New Zeal. J. Statist. 1999; 41:411–30.
Maritz JS. Distribution Free Statistical Methods. (2nd ed.) London: Chapman &
Hall; 1996.
Martin RF. General Deming regression for estimating systematic bias and its conﬁ-
dence interval in method-comparison studies. Clinical Chemistry. 2000;
46:100–104.
Matthews JNS; Altman DG. Interaction 2: Compare effect sizes not P values.
BMJ. 1996; 313:808.
Mayo DG. Error and the Growth of Experimental Knowledge. University of Chicago
Press. 1996.
McBride GB; Loftis JC; Adkins NC. What do signiﬁcance tests really tell us about
the environment? Environ. Manage. 1993; 17:423–432. (erratum. 19, 317).
McGuigan SM. The use of statistics in the British Journal of Psychiatry. British J.
Psychiatry. 1995; 167:683–688.
McKinney PW; Young MJ; Hartz A; Bi-Fong Lee M. The inexact use of Fisher’s
exact test in six major medical journals. JAMA. 1989; 261:3430–3433.
Mena EA; Kossovsky N; Chu C; Hu C. Inﬂammatory intermediates produced by
tissues encasing silicone breast prostheses. J. Invest. Surg. 1995; 8:31–42.
BIBLIOGRAPHY
235

Michaelsen J. Cross-validation in statistical climate forecast models. J. Climate and
Applied Meterorology. 1987; 26;1589–1600.
Mielke PW; Berry KJ. Permutation Methods: A Distance Function Approach.
Springer: NY. 2001.
Mielke PW; KJ Berry. Permutation covariate analyses of residuals based on Euclid-
ean distance. Psychological Reports. 1997; 81:795–802.
Mielke PW; Berry KJ; Landsea CW; Gray WM. Artiﬁcial skill and validation in
meteorological forecasting. Weather and Forecasting. 1996; 11:153–169.
Mielke PW; Berry KJ; Landsea CW; Gray WM. A single sample estimate of 
shrinkage in meteorological forecasting. Weather and Forecasting. 1997;
12:847–858.
Miller ME; Hui SL; Tierney WM. Validation techniques for logistic regression
models. Statist. Med. 1991; 10:1213–1226.
Miller RG. Jackkniﬁng variances. Annals Math. Statist. 1968; 39:567–582.
Miller RG. Beyond Anova: Basics of Applied Statistics. Wiley: NY. 1986.
Miyazaki Y; Terakado M; Ozaki K; Nozaki H. Robust regression for developing
software estimation models. J. Systems Software. 1994; 27:3–16.
Moher D; Cook DJ; Eastwood S; Olkin I; Rennie D; Stroup D. for the
QUOROM Group. Improving the quality of reports of meta-analyses of 
randomised controlled trials: the QUOROM statement. Lancet. 1999;
354:1896–1900.
Moiser CI. Symposium: the need and means of cross-validation, I: problems and
design of cross-validation. Educat. Psych. Measure. 1951; 11:5–11.
Montgomery DC; Myers RH. Response Surface Methodology: Process and Product
Optimization Using Designed Experiments. Wiley. 1995.
Moore T. Deadly Medicine: Why Tens of Thousands of Heart Patients Died in
America’s Worst Drug Disaster. Simon & Schuster. 1995.
Morgan JN; Sonquist JA. Problems in the analysis of surveuy data and a proposal.
JASA. 1963; 58:415–434.
Morris RW. A statistical study of papers in the J. Bone and Joint Surgery BR. J.
Bone and Joint Surgery BR. 1988; 70-B:242–246.
Morrison DE; Henkel RE. The Signiﬁcance Test Controversy. Aldine: Chicago.
1970.
Mosteller F. Problems of omission in communications. Clinical Pharmacology and
Therapeutics. 1979; 25:761–764.
Mosteller F; Chalmers TC. Some progress and problems in meta-analysis of clinical
trials. Stat. Sci. 1992; 7:227–236.
Mosteller F; Tukey JW. Data Analysis and Regression: a second course in statistics.
Addison-Wesley: Menlo Park. 1977.
Moyé LA. Statistical Reasoning in Medicine: The Intuitive P-Value Primer.
Springer: NY. 2000.
Mulrow CD. The medical review article: state of the science. Ann Intern Med
1987; 106:485–488.
Murray GD. Statistical guidelines for the British Journal of Surgery. British J.
Surgery. 1991; 78:782–784.
236
BIBLIOGRAPHY

Murray GD. The task of a statistical referee. British J. Surgery. 1988; 75:664–667.
Nelder JA; Wedderburn RWM. Generalized linear models. JRSS A. 1972;
135:370–384.
Nester M. An applied statistician’s creed. Appl. Statist. 1996; 45:401–410.
Neyman J. Lectures and conferences on mathematical statistics and probability. 2nd
ed., Washington, Graduate School, U.S. Dept. of Agriculture, 1952.
Neyman J. Silver jubilee of my dispute with Fisher. J. Operations Res. Soc. Japan.
1961; 3:145–154.
Neyman J. Frequentist probability and frequentist statistics. Synthese. 1977;
36:97–131.
Neyman J; Pearson ES. On the testing of speciﬁc hypotheses in relation to proba-
bility a priori. Proc. Cambridge Phil. Soc. 1933; 29:492–510.
Neyman J; Pearson ES. On the problem of the most efﬁcient tests of statistical
hypotheses. Phil. Trans. Roy. Soc. A. 1933; 231:289–337.
Nurminen M. Prognostic models for predicting delayed onset of renal allograft
function. Internet Journal of Epidemiology. 2003; 1:1.
Nurmohamed MT; Rosendaal FR; Bueller HR; Dekker E; Hommes DW; Vanden-
broucke JP; et al. Low-molecular-weight heparin versus standard heparin in
general and orthopaedic surgery: a meta-analysis. Lancet. 1992; 340:152–156.
O’Brien PC. The appropriateness of analysis of variance and multiple-comparison
procedures. Biometrics. 1983; 39:787–788.
O’Brien P. Comparing two samples: extension of the t, rank-sum, and log-rank
tests. JASA. 1988; 83:52–61.
Oldham PD. A note on the analysis of repeated measurements of the same sub-
jects. J. Chron. Dis. 1962; 15:969–977.
Olsen CH. Review of the use of statistics in Infection and Immunity. Infection and
Immunity. 2003; 71:6689–6692.
Osborne J; Waters E. Four assumptions of multiple regression that researchers
should always test. Practical Assessment, Research & Evaluation. 2002; 8(2).
Padaki PM. Inconsistencies in the use of statistics in horticultural research. Hort.
Sci. 1989; 24:415.
Palmer RF; Graham JW; White EL; Hansen WB. Applying multilevel analytic
strategies in adolescent substance use prevention research. Prevent. Med. 1998;
27:328–336.
Pankratz A. Forecasting with Dynamic Regression Models. New York: John Wiley &
Sons, Inc. 1991.
Parkhurst DF. Arithmetic versus geometric means for environmental concentration
data. Environmental Science and Technology. 1998; 32:92A–98A.
Parkhurst DF. Statistical signiﬁcance tests: Equivalence and reverse tests should
reduce misinterpretation. Bioscience. 2001; 51:1051–1057.
Perlich C; Provost F; Simonoff JS. Tree Induction vs. Logistic Regression: A
Learning-Curve Analysis. Journal of Machine Learning Research. 2003;
4:211–255.
Pesarin F. Multivariate Permutation Tests. NewYork: Wiley. 2001.
BIBLIOGRAPHY
237

Pettitt AN; Siskind V. Effect of within-sample dependence on the Mann-Whitney-
Wilcoxon statistic. Biometrika. 1981; 68:437–441.
Phipps MC. Small samples and the tilted bootstrap. Theory of Stochastic Processes.
1997; 19:355–362.
Picard RR; Berk KN. Data splitting. American Statistician. 1990; 44:140–147.
Picard RR; Cook RD. Cross-validation of regression models. JASA. 1984;
79:575–583.
Pierce CS. Values in a University of Chance. (Wiener PF ed.) Doubleday Anchor
Books: NY. 1958.
Pilz J. Bayesian Estimation And Experimental Design In Linear Regression Models.
2nd ed, Wiley: New York. 1991.
Pinelis IF. On minimax risk. Theory Prob. Appl. 1988; 33:104–109.
Pitman EJG. Signiﬁcance tests which may be applied to samples from any popula-
tion. Roy. Statist. Soc. Suppl. 1937; 4:119–130, 225–232.
Pitman EJG. Signiﬁcance tests which may be applied to samples from any 
population. Part III. The analysis of variance test. Biometrika. 1938;
29:322–335.
Poole C. Beyond the conﬁdence interval. Amer. J. Public Health. 1987;
77:195–199.
Poole C. Low p-values or narrow conﬁdence intervals: which are more durable?
Epidemiology. 2001; 12:291–294.
Praetz P. A note on the effect of autocorrelation on multiple regression statistics.
Australian J. Statist. 1981; 23:309–313.
Proschan MA; Waclawiw MA. Practical guidelines for multiplicity adjustment in
clinical trials. Controlled Clinical Trials. 2000; 21:527–539.
Ravnskov U. Cholesterol lowering trials in coronary heart disease: frequency of
citation and outcome. BMJ. 1992; 305:15–19.
Rea LM; Parker RA; Shrader A. Designing and Conducting Survey Research: A
Comprehensive Guide. Jossey-Bass. 2nd ed. 1997.
Redmayne M. Bayesianism and Proof, in Science in Court, M. Freeman, Reece H.
eds., Ashgate: Brookﬁeld, MA. 1998.
Reichenbach H. The Theory of Probability. University of California Press: Berkeley.
1949.
Rencher AC; Pun F-C. Inﬂation of R2 in best subset regression. Technometrics.
1980; 22:49–53.
Rice SA; Grifﬁn JR. The hornworm assay: Useful in mathematically-based biologi-
cal investigations. American Biology Teacher. 2004; 66:487–491.
Rosenbaum PR. Observational Studies. Springer, 2nd ed. 2002.
Rosenberger W; Lachin JM. Randomization in Clinical Trials: Theory and Prac-
tice. Wiley: New York. 2002.
Rothman KJ. Epidemiologic methods in clinical trials. Cancer. 1977;
39:1771–1775.
Rothman KJ. No adjustments are needed for multiple comparisons. Epidemiology.
1990; 1:43–46.
238
BIBLIOGRAPHY

Rothman KJ. Statistics in nonrandomized studies. Epidemiology. 1990; 1:417–418.
Roy J. Step-down procedure in multivariate analysis. Ann. Math. Stat. 1958;
29(4):1177–1187.
Royall RM. Statistical Evidence: A Likelihood Paradigm. Chapman and Hall: New
York. 1997.
Rozeboom W. The fallacy of the null hypothesis signiﬁcance test. Psychol. Bull.
1960; 57:416–428.
Salmaso L. Synchronized permutation tests in 2k factorial designs. Int. J. Non
Linear Model. Sci. Eng. 2002; 3(In press).
Savage LJ. The Foundations of Statistics. Dover Publications, 1972.
Saville DJ. Multiple comparison procedures: The practical solution. American Sta-
tistician. 1990; 44:174–180.
Saville DJ. Basic statistics and the inconsistency of multiple comparison procedures.
Canadian J. Exper. Psych. 2003; 57(3):167–175.
Schmidt FL. Statistical signiﬁcance testing and cumulative knowledge in 
psychology: implications for training of researchers. Psychol. Meth. 1996;
1:115–129.
Schenker N. Qualms about bootstrap conﬁdence intervals. JASA. 1985;
80:360–361.
Schor S; Karten I. Statistical evaluation of medical manuscripts. JASA. 1966;
195:1123–1128.
Schroeder YC. The procedural and ethical ramiﬁcations of pretesting survey ques-
tions. Amer. J. of Trial Advocacy. 1987; 11:195–201.
Schulz KF. Randomised trials, human nature, and reporting guidelines. Lancet.
1996; 348:596–598.
Schulz KF. Subverting randomization in controlled trials. JAMA. 1995;
274:1456–1458.
Schulz KF; Chalmers I; Hayes R; Altman DG. Empirical evidence of bias. Dimen-
sions of methodological quality associated with estimates of treatment effects in
controlled trials. JAMA. 1995; 273:408–412.
Schulz KF; Grimes DA. Blinding in randomized trials: hiding who got what.
Lancet. 2002; 359:696–700.
Seidenfeld T. Philosophical Problems of Statistical Inference. Reidel: Boston. 1979.
Selike T; Bayarri MJ; Berger JO. Calibration of p-values for testing precise null
hypotheses. Amer. Statist. 2001; 55:62–71.
Selvin H. A critique of tests of signiﬁcance in survey research. Amer. Soc. Rev.
1957; 22:519–527.
Senn S. A personal view of some controversies in allocating treatment to patients
in clinical trials. Statist. Med. 1995; 14:2661–2674.
Shao J; Tu D. The Jacknife and the Bootstrap. New York: Springer. 1995.
Sharp SJ; Thompson SG; Altman DG. The relation between treatment beneﬁt and
underlying risk in meta-analysis. BMJ. 1996; 313:735–738.
Sharp SJ; Thompson SG. Analysing the relationship between treatment effect and
underlying risk in meta-analysis: comparison and development of approaches.
Statist. Med. 2000; 19:3251–3274.
BIBLIOGRAPHY
239

Shuster JJ. Practical Handbook of Sample Size Guidelines for Clinical Trials. CRC:
Boca Raton, FL. 1993.
Simpson JM; Klar N; Donner A. Accounting for cluster randomization: a review of
primary prevention trials; 1990 through 1993. Am. J. Public Health. 1995;
85:1378–1383.
Smeeth L; Haines A; Ebrahim S. Numbers needed to treat derived from meta-
analysis—sometimes informative; usually misleading. BMJ. 1999;
318:1548–1551.
Smith GD; Egger M. Commentary: Incommunicable knowledge? Interpreting and
applying the results of clinical trials and meta-analyses. J. Clin. Epidemiol. 1998;
51:289–295.
Smith GD; Egger M; Phillips AN. Meta-analysis: Beyond the grand mean? BMJ.
1997; 315:1610–1614.
Smith TC; Spiegelhalter DJ; Parmar MKB. Bayesian meta-analysis of randomized
trials using graphical models and BUGS. In Bayesian Biostatistics. Ed: Berry DA;
Stangl DK. 411–427. Marcel Dekker: New York. 1996.
Snee RD. Validation of regression models: methods and examples. Technometrics.
1977; 19:415–428.
Sox HC; Blatt MA; Higgins MC; Marton KI. Medical Decision Making. Butter-
worth and Heinemann: Boston. 1988.
Spiegelhalter DJ. Probabilistic prediction in patient management. Statist. Med.
1986; 5:421–433.
Sterne JAC; Smith GD; Cox DR. Sifting the evidence—what’s wrong with signiﬁ-
cance tests? Another comment on the role of statistical methods. BMJ. 2001;
322:226–231.
Dietmar Stöckl D; Dewitte K; Thienpont LM. Validity of linear regression in
method comparison studies: is it limited by the statistical model or the quality of
the analytical input data? Clinical Chemistry. 1998; 44:2340–2346.
Stockton CW; Meko DM. Drought recurrence in the Great Plains as reconstructed
from long-term tree-ring records: J. of Climate and Applied Climatology. 1983;
22:17–29.
Stone M. Cross-validatory choice and assessment of statistical predictions. JRSS B.
1974; 36:111–147.
Su Z; Adkison MD; Van Alen BW. A hierarchical Bayesian model for estimating
historical salmon escapement and escapement timing. Canadian J. Fisheries and
Aquatic Sciences. 2001; 58:1648–1662.
Subrahmanyam M. A property of simple least squares estimates. Sankha. 1972;
34B:355–356.
Sukhatme BV. A two sample distribution free test for comparing variances: Bio-
metrika. 1958; 45:544–548.
Suter GWI. Abuse of hypothesis testing statistics in ecological risk assessment.
Human and Ecological Risk Assessment. 1996; 2:331–347.
Teagarden JR. Meta-analysis: whither narrative review? Pharmacotherapy. 1989;
9:274–284.
Therneau TM; Grambsch PM. Modeling Survival Data. Springer: New York.
2000.
240
BIBLIOGRAPHY

Thompson SG. Why sources of heterogeneity in meta-analysis should be investi-
gated. BMJ. 1994; 309:1351–1355.
Thompson SK; Seber GAF. Adaptive Sampling. Wiley. 1996.
Thorn MD; Pulliam CC; Symons MJ; Eckel FM. Statistical and research quality of
the medical and pharmacy literature. American J. Hospital Pharmacy. 1985;
42:1077–1082.
Tiku ML; Tan WY; Balakrishnan N. Robust Inference. New York and Basel: Marcel
Dekker. 1990.
Tsai C-C; Chen Z-S; Duh C-T; Horng F-W. Prediction of soil depth using a soil-
landscape regression model: a case study on forest soils in southern Taiwan.
Proc. Natl. Sci. Counc. ROC(B). 2001; 25:34–39.
Tu D; Zhang Z. Jackknife approximations for some nonparametric conﬁdence
intervals of functional parameters based on normalizing transformations.
Comput. Statist. 1992; 7:3–5.
Tufte ER. The Visual Display of Quantitative Information. Graphics Press:
Cheshire, CT. 1983.
Tufte ER. Envisioning Data. Graphics Press. Graphics Press: Cheshire, CT. 1990.
Tukey JW. Exploratory Data Analysis. Addison-Wesley: Reading, MA. 1977.
Tukey JW. The philosophy of multiple comparisons. Statist. Sci. 1991; 6:100–116.
Tukey JW; McLaughlin DH. Less vulnerable conﬁdence and signiﬁcance proce-
dures for location based on a single sample; Trimming/Winsorization 1.
Sankhya. 1963; 25:331–352.
Tversky A; Kahneman D. Belief in the law of small numbers. Psychol. Bull. 1971;
76:105–110.
Toutenburg H. Statistical Analysis of Designed Experiments. Springer-Verlag: New
York. 2nd Ed. 2002.
Tyson JE; Furzan JA; Reisch JS; Mize SG. An evaluation of the quality of thera-
peutic studies in perinatal medicine. J. Pediatrics. 1983; 102:10–13.
Vaisrub N. Manuscript review from a statisticians perspective. JAMA. 1985;
253:3145–3147.
van Belle G. Statistical Rules of Thumb. Wiley: New York. 2002.
Venn J. The Logic of Chance. MacMillan: London. 1888.
Victor N. The challenge of meta-analysis: discussion. J. Clin. Epidemiol. 1995;
48:5–8.
Wainer H. Rounding tables. Chance. 1998; 11:46–50.
Watterson IG. Nondimensional measures of climate model performance. Int. J.
Climatology. 1966; 16:379–391.
Weerahandi S. Exact Statistical Methods for Data Analysis. Springer Verlag: Berlin.
1995.
Weisberg S. Applied Linear Regression. 2nd ed. Wiley: New York. 1985.
Welch BL. On the z-test in randomized blocks and Latin squares. Biometrika.
1937; 29:21–52.
Welch GE; Gabbe SG. Review of statistics usage in the American J. Obstetrics 
and Gynecology. American J. Obstetrics and Gynecology. 1996; 175:
1138–1141.
BIBLIOGRAPHY
241

Westfall DH; Young SS. Resampling-Based Multiple Testing: Examples and Methods
for p-value Adjustment. Wiley: New York. 1993.
Westgard JO. Points of care in using statistics in method comparison studies. Clin-
ical Chemistry. 1998; 44:2240–2242.
Westgard JO; Hunt MR. Use and interpretation of common statistical tests in
method comparison studies. Clin. Chem. 1973; 19:49–57.
White SJ. Statistical errors in papers in the British J. Psychiatry. British J. Psychiatry.
1979; 135:336–342.
Wilkinson L. The Grammar of Graphics. Springer-Verlag: New York. 1999.
Wilks DS. Statistical Methods in the Atmospheric Sciences. Academic Press. 1995.
Willick JA. Measurement of galaxy distances. In Formation of Structure in the Uni-
verse, eds. A. Dekel and J. Ostriker. Cambridge University Press. 1999.
Wilson JW; Jones CP; Lundstrum LL. Stochastic properties of time-averaged
ﬁnancial data: explanation and empirical demonstration using monthly stock
prices. Financial Review. 2001; 36:3.
Wu CFJ. Jackknife, bootstrap, and other resampling methods in regression analysis
(with discuss.) Annals Statist. 1986; 14:1261–1350.
Wulf HR; Andersen B; Brandenhof P; Guttler F. What do doctors know about 
statistics? Statistics in Medicine. 1987; 6:3–10.
Yandell BS. Practical Data Analysis for Designed Experiments. Chapman and Hall:
London. 1997.
Yoccuz NG. Use, overuse, and misuse of signiﬁcance tests in evolutionary biology
and Ecology. Bull Ecol. Soc. Amer. 1991; 72:106–111.
Yoo S-H. A robust estimation of hedonic price models: least absolute deviations
estimation. Applied Economics Letters. 2001; 8:55–58.
Young A. Conditional data-based simulations: some examples from geometric sta-
tistics. Int. Statist. Rev. 1986; 54:1–13.
Zhou X-H; Gao S. Conﬁdence intervals for the log-normal mean. Statist. Med.
1997; 17:2251–2264.
242
BIBLIOGRAPHY

AUTHOR INDEX
243
Adams DC, 100
Adkins NC, 115
Adkison MD, 100
Albers W, 60
Altman DG, 15, 56, 84, 84, 111, 
116
Aly E-E AA, 62
Andersen B, 42
Anderson DR, 16
Anderson S, 61, 116
Anderson SL, 68, 84
Anderson W, 62
Anscombe F, 16
Archﬁeld SA, 181
Avram MJ, 84
Bacchetti P, 84
Badrick TC, 84, 115
Bailar JC, 116
Bailey KR, 98
Bailor AJ, 68
Baker RD, 68, 79
Balakrishnan N, 67
Barbui C, 40
Barnard GA, 187
Barnston AG, 194
Barrodale I, 137, 165
Bayarri MJ, 86
Bayes T, 89
Begg CB, 116
Bent GC, 181
Berger JO, 25, 43, 47, 85
Berger VW, 19, 41, 56, 84, 101–102
Berk KN, 190
Berkeley G, 85
Berkey C, 98
Berlin JA, 100
Berry DA, 25, 94
Berry KJ, 99, 157, 162, 173
Bickel P, 56, 60
Bithell J, 53
Bland JM, 56, 84
Block G, 28
Bly RW, 30, 43
Blyth CR, 25
Bock M, 196, 203
Boomsma A, 194
Bothun G, 36, 158
Box GEP, 68, 84
Brand C, 178
Breiman L, 184
Brockwell PJ, 189
Browne MW, 189
Burnham KP, 16
Cade B, 157, 170
Callaham ML, 114
Campbell MJ, 84
Camstra A, 194
Canty AJ, 88, 99
Author Index
Common Errors in Statistics (and How to Avoid Them), 2e, by Phillip I. Good and James W. Hardin.
Copyright © 2006 John Wiley & Sons, Inc.

244
AUTHOR INDEX
Capaldi D, 104
Cappuccio FP, 28
Carlin BP, 100
Carpenter J, 53
Carroll RJ, 27, 56, 172
Casella G, 43, 47, 56
Chalmers TC, 41, 98
Chernick MR, 32, 88
Cherry S, 84
Chiles JR, 15
Christophi C, 41, 102
Chu KC, 23
Clemen RT, 25
Cleveland WS, 142, 144
Conan-Doyle A, 22, 83
Conover WJ, 63, 67
Converse JM, 30
Cornﬁeld J, 74
Cox DR, 25, 85, 117
Cummings P, 7
Cupples LA, 84
D’Abrera HJM, 60, 76
Dar R, 84
Davis RA, 189
Davision AC, 89, 161
Day S, 102
Dean CW, 110, 117
DeGroot MH, 25
Delucchi KL, 79
DeMets DL, 162
Diaconis P, 9, 200, 203
Diciccio TJ, 99
Dixon PM, 61, 84
Djulbegovic B, 40
Duggan TJ, 110, 117
Dyke G, 3, 51, 80, 91, 105
Easterbrook PJ, 98
Ebrahim S, 100
Ederer F, 41
Edwards W, 95
Efron B, 53–54, 56, 68, 99, 206, 215,
217, 218
Egger M, 15, 96–99
Ehrenberg ASC, 106
Ellis SP,
Elwood JM, 42, 84
Engel E, 169
Eysenbach G, 100
Fears TR, 23
Feinstein AR, 115
Feller W, 108
Felson DT, 84, 98
Feng Z, 65, 66, 101,115
Fienberg SE, 84
Fink A, 43
Finney DJ, 84
Firth D, 180
Fisher RA, 43, 79
Flatman RJ, 84
Fleming TR, 153
Fligner MA, 68
Fowler FJ, 30
Francis C, 62
Frank D, 21
Freedman DA, 161–162, 175, 187,
195, 203
Freireich EJ, 94
Friedman JH, 212, 218
Friedman LM, 162
Friedman M, 84
Furberg CD, 162
Gabbe SG, 84
Gail MH, 66
Gallant AR, 172
Gao S, 160
Gardner MJ, 84, 111
Garthwaite PH, 15, 87
Geary RC, 71
Geisser S, 208, 218
George SL, 84
Gigerenzer G, 80
Gillett R, 100
Gine E, 99
Glass GV, 84
Goldberger AS, 175
Gong G, 177, 185, 187, 190, 205, 218
Good IJ, 94, 99
Good PI, 21, 42–43, 63, 71, 73–74,
76, 84, 90, 100, 159,162, 183
Goodman SN, 84, 95, 86
Gore S, 84
Gotzsche PC, 98
Grambsch PM, 186
Grant A, 116
Graumlich L, 188
Green PJ, 160
Greene HL, 34

AUTHOR INDEX
245
Greenland S, 115
Gregory P, 176, 209
Grifﬁn JR, 124
Gurevitch J, 100
Hagood MJ, 14
Haines A, 100
Hall P, 53, 63, 64
Hallock KF, 169
Hardin J, 144, 161, 180, 186
Harley SJ, 100
Harrell FE, 186
Hartel G, 75
Hauck WW, 61, 116
Hedges LV, 100
Hershey D, 124
Hertwig R, 16
Heywood J, 185
Hilbe JM, 144, 161, 180, 186
Hilton J, 62
Hodges JS, 86
Horwitz RI, 15, 98, 100, 116
Hosmer DW, 186
Hout M, 177
Hsu JC, 81, 112
Huber PJ, 56
Hubley AM, 115
Hume D, 85
Hungerford TW, 77
Hunt MR, 84
Hunter JE, 117
Hutchon DJR, 100
International Committee of Medical
Journal Editors, 84, 116
International Study of Infarct Survival
Collaborative Group, 8
Ivanova A, 21
Jackson GG, 84
Jennison C, 43
Johnson MG, 67
Johnson MM, 67
Jones CP, 115
Jones IG, 84
Jones LV, 111,115
Judge G, 196, 203
Kadane IB, 94
Kanarek MS, 154, 158
Kaplan J, 25
Karten I, 84
Kass R, 99
Kaye DH, 93
Kendall M, 201, 203
Keynes JM, 89
Killeen TJ, 68
Knight K, 99
Koenker R, 169
Koepsell TD, 7
Kosecoff JB, 43
Lambert D, 60, 100
Lang TA, 104, 107, 116
Lee KL, 186
Lehmann EL, 23, 25, 43, 50, 56, 60,
64, 68, 72,76
Leizorovicz A, 98
Lemeshow SL, 186
Lieberson S, 84, 154, 162
Light RJ, 84
Linnet K, 166, 168
Liu CY, 32
Loader C, 160
Locke J, 85
Loftis JC, 115
Lonergan JF, 84–85
Loo D, 177
Louis TA, 100
Lovell DJ, 41
Lundstrum LL, 115
Lunneborg C, 71
Ma CW, 67
MacArthur RD, 84
Machin D, 84
Malone KM,
Mangel M, 36, 82
Mangels L,
Manly BFJ, 62, 100
Maritz JS, 56, 60, 100
Mark SD, 186
Martin RF, 173
Mayo DG, 25
McBride GB, 115
McGill ME, 142
McGuigan SM, 84
McKinney PW, 20, 84
McLaughlin DH, 80
Meenan RF, 84

246
AUTHOR INDEX
Meko DM, 188
Mena EA, 150–1
Michaelsen J, 194
Mieike PW, 100, 157, 162, 172, 192,
194
Miller RG, 67, 68, 84
Miyazaki Y, 172
Moher D, 100
Moiser CI, 190
Moore T, 34
Morgan JN, 186
Morris RW, 56
Mosteller F, 63, 98, 100, 116, 116,
157, 162,186
Moye L, 34
Mulrow CD, 84
Murray GD, 84
Myers RA, 100
Nelder JA, 178
Nester M, 115
Neyman J, 25, 43, 83, 117
Nurmohamed MT, 98
Nurminen M, 186
O’Brien PC, 62
Oikin I, 100
Olsen CH,
Olshen RA, 196, 203
Omer H, 84
Osborne J, 186
Padaki PM, 84
Pankratz A, 157
Parker RA, 43
Parkhurst DF, 115
Parzen E, 107
Patterson GR, 104
Perlich C, 186
Permutt T, 21
Pesarin F, 62, 63, 76–78, 84, 100
Phillips AN, 99
Phipps MC, 89, 97
Picard RR, 190
Pillemer DB, 84
Pilz J, 50
Pinelis IF, 50
Pitman EJG, 157
Plato, 83
Poole C, 111
Presser S, 30
Provost F, 186
Pun F-C, 192, 196, 203
Raftery A, 99
Rao CR, 209, 218
Rea LM, 43
Redmayne M, 89
Reichenbach H, 85
Rencher AC, 186, 192, 196, 203
Rice SA, 124
Richards L, 157
Roberts FDK, 137, 165
Romano JP, 100
Rosenbaum PR, 43
Rosenberg MS, 100
Rothenberg T, 195, 203
Rothman KJ, 84, 115
Roy J, 175
Rubin H, 84
Ruppert D, 56
Rytter EC, 84
Sa ER, 100
Salmaso L, 76
Salsburg D, 62, 63
Samaniego FJ, 36, 82
Sanders JR, 84
Saville DJ, 81
Schenker N, 99
Schmidt FL, 117
Schor S, 84
Schroeder YC, 30, 43
Schultz KF, 41–42
Seber GAP, 43
Secic M, 104, 107, 116
Selike T, 25, 86
Selvin H, 115
Senn S, 102
Serlin OH, 84
Shao J, 56, 191, 194
Shi S, 84
Shuster JJ, 43
Silverman BW, 160
Simonoff JS, 186
Siskind V, 84
Smeeth L, 100
Smith GD, 15, 84, 97, 99
Snell EJ, 161
Sonquist JA, 186

AUTHOR INDEX
247
Sox HC, 25
Stangi DK, 100
Sterne JAC, 85
Still AW, 76
Stockton CW, 188
Stone CJ, 187
Stone M, 208, 218
Stuart A, 201, 203
Su Z, 100
Subrahmanyam M, 190
Sukhatme BV, 67
Sutch R, 195, 203
Suter GWI, 115
Tarone RE, 23
Terakado M,
Therneau TM, 186
Thompson SG, 16, 98
Thompson SK, 43
Tiao GC, 80
Tibshirani R, 54, 56
Todd PM, 16
Toutenburg H, 43
Tribe L, 90
Trzos R, 21
Tsai C-C, 188
Tu D, 54, 56, 191, 194
Tufte ER, 116, 140, 144
Tukey JW, 63, 68, 74, 80, 115,144,
157, 162
Turnbull BW, 43
Vaisrub N, 84
van Alen BW, 100
van Belle G, 104, 106
van den Dool HM, 194
van Zwet WR, 60
Venn J, 85
Vickers A, 41
Victor N, 98
von Neumann, 172
Wald A, 82
Waters E, 186
Watterson IG, 194
Wedderburn RWM, 178
Weerahandi S, 62
Weisberg S, 194
Welch GE, 84
Westgard JO, 84, 163, 169
White SJ, 76, 84
Wilkinson L, 135, 144
Wilks DS, 192
Willick JA, 114
Wilson JW, 115
Wilson SR, 53, 63, 64
Wu CFJ, 99, 191
Yoccuz NG, 84, 115
Young A, 63, 81
Zhang Z, 54
Zhou X-H, 160
Zinn J, 99
Zumbo BD, 115

SUBJECT INDEX
249
a prior distribution, 91–93
a prior probability, 96
Acceptance region, 111–112
Accuracy vs. precision, 219
Agronomy, 38, 74
Algorithms, 192, 193
Allocation (of treatment), see
Treatment allocation
Aly’s statistic, 69–70
Analysis of variance, 71, 73–74, 113
Angiograms, 103
Animals, 7
Antibodies, 28
ARIMA, 189
Arithmetic vs. geometric mean, 106
Aspirin, 40, 74
Association,
spurious, 143
versus causation, 154, 160
Assumptions, 4, 36, 57, 68, 99, 111,
147, 181
Astronomy, 8, 83, 114
Asymptotic, 57, 165
approximation, 68
relative efﬁciency (ARE), 49–50
Audit, 157
Authors, 119
Autocorrelation, 115
Autoregressive process, see time series
Axis
label, 130,141
range, 130
Bacteria, 106
Balanced design, 74
Bar chart, 126, 135
Baseline, 65
Bayes
analysis, 98
factor, 95
Theorem, 89–90
Behrens-Fisher problem, 61
Bias
estimation, 53, 114, 179, 206
observer, 41
sample, 7, 39–40, 97, 113–115, 
158
sources, 121, 166, 181
systematic error, 168
Bias-corrected and accelerated, 54, 69
Blinding, 41, 102
Blocks, 38, 66, 80
Blood, 9, 30, 83
Blood pressure, 28
Bonferroni correction, 81
Bootstrap, 55, 79, 90, , 205
limitations, 88
nonparametric, 52
parametric, 55, 109
percentile, primitive, 68
sample, 33, 52, 64, 177
test, 57–63
tilted, 89
Box and whiskers plot, 107–108
Boyle’s Law, 5, 148
Subject Index
Common Errors in Statistics (and How to Avoid Them), 2e, by Phillip I. Good and James W. Hardin.
Copyright © 2006 John Wiley & Sons, Inc.

250
SUBJECT INDEX
Cancer, 19, 23, 94, 97, 158
Caption, 140
CART, 187
Case controls, 42
Categories, 28, 80
Cause and effect, 149, 153, 185, 187,
190
Censored data, 104, 114
Census, 5, 158
Central limit theorem, 108
Chi-square
statistic, 110
statistic vs. distribution, 79, 111
test, 21
Classiﬁcation and regression tree
(CART), 183
Clinical
chemistry, 167
signiﬁcance, 61
trials, 9, 24, 34–35, 40, 90, 94, 98,
103,143
Clinician, 38
Cluster, 6, 63
Cold fusion, 8
Computer, 20
output, 113, 123
simulation, see Simulation
Conﬁdence interval, 31, 53, 69, 83,
111–112, 120, 124
Confounded effects, 4, 79, 97, 112,
120, 147, 151, 177
Contingency table, 18, 79, 99, 110
Contour plot, 133, 135
Contrast, 73
Controls, 38–40
positive, 40
Correlation, 51, 153
spurious, 159, 176
vs. slope, 167
Corticosteroids, 7
Cost, 29
Cost-beneﬁt analysis, 23
Covariances, 64
Covariates, 66
Criminology, 152
Criteria, 117
Crossovers, 103
Cross-products, 175
Cross-validation, 205
Curve-ﬁtting, 160
Cutoff value, 70, 110
Cuts, 114
Data collection, 28, 47, 113, 117
Data mining, 182
Deaths, 104
Decimal places, 106
Decision tree, see CART
Decision theory, 24, 47, 106, 117
Deduction, 21, 83
Delete-one, 158, 191
Density, 133
Descriptive statistics, 120
Deterministic vs. stochastic, 5, 219
Discrimination, 121, 154, 143
Disease process, 17
Dispersion, 105–106
Distribution, 31, 49, 156, 220
a prior, 94
cumulative, 220
empirical, 220
exponential, 86
F, 64
function, 9
heavy-tailed, 49
mixture, 54, 58, 74,109
multiparameter, 54
multivariate normal, 62
nonsymmetric, 49, 54, 106
normal, 32, 106, 109
Poisson, 106
sampling, 68
symmetric, 49
uniform, 108
Weibull, 74
Diurnal rhythm, 73
Dropouts, 35, 36
Drugs, 23, 37, 103
Ecological fallacy, 170
Effects, 179
Elections, 171
Eligibility, 103
Emissions, 24
Endpoints, 112
Epidemiology, 97, 188
Equivalence, 61, 75
Error
common, 80, 112, 120
frequency, 158

SUBJECT INDEX
251
probabilities, 16
terms, 72, 156, 161
Estimate, 4,
consistent, 48
efﬁcient, 48
interval vs. point, 51
least-squares, 51
mini-max, 50
minimum loss, 50
minimum variance, 51, 156
optimal, 49
plug-in, 51
semiparametric, 48
unbiased, 51, 156
Exact, 74
Experimental design, 69
unbalanced, 58
Experimental unit, 37
Exploratory analysis, 177
Extrapolate, 9
Factors, 7, 24
Factor analysis, 178
False dimension, 127
False negative, see Type I error
False positive, see Type II error
F-distribution, 68
Fisher’s exact test, 20
Forecast, 112
Found data, 16, 115
F-ratio, 21, 71–74, 111
Fraud, 8, 21, 59
Frequency plot, 10
GEE, 63, 66, 180
Geometric mean, 106
Generalized Linear Models (GLM), 65,
178
Goodness of ﬁt, 159, 190
Grammar, 143
Graphics, 104, 120, 143
Grid line, 127, 135, 143
Ground water, 181
Group, 77
Group randomized trials, 65, 115, 179
Growth, 106
Heterogeneity, 98–99
Hierarchical models, 98
Histogram, 107
HLM, 180
Hodges-Lehmann estimator, 49
Hypertension, 14
Hypothesis, 4, 27, 161,
alternative, 18–21, 71–72, 94, 220
null, 16, 64, 72, 90, 94, 220
ordered, 21
post-hoc, 7
primary, 18, 70, 86
Hypothesis testing, 17
Immunology, 151, 153
Income, 28, 170
Independence vs. correlation, 73
Indifference region, 116
Induce, 83
Induction, 21
Interaction, 74–78
Interpolation, 124
Interquartile range, 53
Interval estimate, 55, 89
Intraclass correlation, 65
Jackknife, 191, 205
Jonckeere-Terpsira statistic, 72
Kepler’s Law, 5
k-fold resampling, 190
Kinetic molecular theory, 149
Kruskal’s gamma, 110
k-sample analysis, 71, 99
Labels, 141
Lattice, 77
Least-absolute deviation, 49
Least-squares, 49
Legal applications, 6, 90–93, 121, 154,
158, 170–1
Legend, 137
Likelihood, 18
Likert scale, 60, 73
Linear vs. nonlinear, 147
Link function, 178
Litter, 37
Location parameter, 32
Logical assertion, 14
Logistic regression, 181, 212
Losses
absolute deviation, 22, 48, 50, 72,
157, 160, 161, 164

252
SUBJECT INDEX
jump, 48
monotone, 48
square deviation, 48, 50, 161
step function, 48
Mail, 37
Main effect, 74, 78
Malmquist bias, 114
Mann-Whitney test, 72
Marginals, 19, 79
Maximum, 52, 109
Maximum likelihood, 47, 51
Maximum tolerable dose, 15
Mean absolute deviation, 192
Mean, arithmetic, 16, 32, 50, 55, 63,
104–7
Measurements, 28, 29, 48, 114, 105,
117, 148
Median, 33, 49–52, 89, 105–109
Medical device, 39, 164
Medicine, 15, 205–206
Meta-analysis, 97, 90, 114
Minimum, 52, 109
Minimum effective dose, 25
Missing data, 4, 78, 82, 103, 124
Mitosis, 38
Model,
choosing, 3, 147, 150
nonlinear, 149
nonunique, 151
range of application, 191
general linear, see GLM
parametric vs. non-parametric, 220
physical, 160, 181
Monotone function, 70, 78, 148
MRPP, 157
Multiple
end points, 34
tests, 74
Multivariate analysis, 161, 175
Negative ﬁndings, 106
Neural network, 182
Neurology, 114
Newton’s Law, 22, 83
Neyman-Pearson theory, 17
Nominative label, 132
Nonsigniﬁcant results, 97, 116, 121
Nonresponders, 36, 82
Normal alternatives, 60
Normal distribution, 80, 107
Normal scores, 60
Nuisance parameters, 180
Objectives, 4, 13
O’Brien’s test, 62
Observational studies, 97
Observations
dependent, 63,
exchangeable, 57
identically-distributed, 37
independent, 51
independent, identically distributed,
37, 57,111
multivariate, 63
transformed, 72
Odds, 95
One-sided vs. two-sided, 19, 31, 34
Ordinal data, 132, 219
Outliers, 88, 157, 164
Overﬁtting, 192
Paired observations, 61
Parameters, 51, 55, 58, 62, 82, 98,
111,149
location, 32, 49
nuisance, 180
scale, 32
shift, 50
Paternity, 91
Patterns, 7, 24
Percentages, 122
Percentiles, 51, 53, 89, 109
Permutation test, 16, 21, 23, 58–63,
66–71, 76,79–80,87, 99
Permutations, 78, 157
Perspective plot, 133, 135
Phase III trials, 25
Physician, 39
Pie chart, 135–6, 143
Pitman correlation, 72
Pivotal quantity, 55, 64, 88
Placebo, 40
Plotting region, 129, 139
Plotting symbol, 137, 141
Poker, 9–10
Polar coordinates, 135
Polyline, 131, 137
Polynomial, 150
Population, 3–5, 27, 36–38, 83

SUBJECT INDEX
253
Population statistics, 5
Poverty, 152
Power, 18, 31–36, 60, 62, 70, 74, 81,
94
post-hoc, 115
related to sample size, 31, 101
Precision, 52, 105, 107, 155, 158
Prediction, 159, 191–193, 205
Pricing, 114
ProcARIMA, 157
Proc Means, 59
Proc Mixed, 65
Protocol, 8, 15, 97, 117
Proxy variable, 153
p-value, 10–11, 60, 81–83, 110–112,
116
Radioimmune assay, 148
Rain, 188
Random number, 7, 108
Randomizing, 40–42, 60, 94
Ranks, 58
Rates, 122
Ratio, 53
Raw data, 90
Recruitment, 30, 41
Recruits, 6, 15, 39
Redheads, 14, 17, 38
Redshift, 115
Regression
coefﬁcients, 155–7
Deming (EIV), 165
LAD, 163
linear vs. nonlinear, 149, 163
methods, 163
multivariable,
OLS, 164, 166
quantile, 165
scope, 147
stepwise, 176
stratiﬁed, 171
Regulatory agency, 17, 40
Rejection region, 18
Relationship, 148
Relativity, 22, 83
Repeated measures, 63, 184
Report, 28
Resampling, 188
Residuals, 76, 105, 160, 162
Robust, 58, 80, 90
R-squared, 95
Rugplot, 109
Sales, 38
Sample, 6
representative, 4, 7, 9, 88
splitting, 189
size, 31–36, 41, 58, 104, 109, 105,
119, 167
universe, 121
Scale alternative, 62
Scale parameter, 32
Scatterplot, 138, 143
Scope, 147
Seismology, 114
Self-selection, 7
Semiparametric models, 87
Sex, 14, 19, 154
Shading, 142
Shift alternative, 62
Signiﬁcance
practical, 16, 61, 105
statistical, 155
Signiﬁcance level, 18, 31–33, 57–59,
68, 70, 80,110–111
Signiﬁcance level vs. p-value, 221
Silicon implants, 40
Simpson’s paradox, 121,154, 162
Simulations, 70, 74, 176, 196, 207
Slope, 150, 152
Smirnov test, 62
Sociology, 110
Software, 32, 34, 156
Stacked bar chart, 138
Standard error, 105–106, 193
Stationarity, 158
Statistic, 88
Stein’s paradox, 56
Stepwise regression, 176
Stochastic, 148
Strata, 7, 41, 154, 143
Studentize, 54
Subgroups, 15, 38, 80, 97, 121
Subjective data, 73
Sufﬁcient statistic, 55
Surgery, 9
Surrogates, 153, 158, 189
Surveys, 7, 28–31, 37–38, 78, 102,
120
Survival, 19

254
SUBJECT INDEX
Tables, 105
Testing hypotheses, 156
Tests
analysis of variance, 71, 74, 76
bootstrap, 58
chi-square, 21
Fisher’s exact, 20
for equality of variances, 67
for equivalence, 75
for independence, 79
F-test, 21, 58, 68, 71
inferior, 80
k-sample, 72
locally most powerful, 63
Mann-Whitney, 72
most powerful, 58
multiple, 81, 112, 176
one- vs. two-tailed, 21, 60
permutation, 16, 21, 23, 57–63,
66–70,76,79–80, 87,99
t-test, 58, 73
Texture, 142
Time series, 57, 114
Time-to-event data, 74
Title, 140
Toxicology, 37
Transformations, 159
Treatment allocation, 41, 101, 119
t-test, 58–60, 64, 73, 81, 111, 123
Type I and II errors, 23, 29, 32, 58,
81, 94, 221
Type I error, 8, 68
Type II error vs. power, 222
Unbalanced vs balanced design, 
74
U-statistic, 58
Vaccine, 17
Validation, 3, 160, 186, 188
Variable
categorical, 131
continuous, 131
confounding, 39
exchangeable, 64
explanatory, 176
indicator, 159
predictor vs. dependent, 163
selection of, 10, 175
Variance, 51, 55, 61, 67–68, 115
estimator,
inﬂation factor, 65
Variation, 4, 5, 18, 24, 58, 105
Viewpoint, 133, 135
Virus, 36
Voting, electronic, 113
War, 81
Weak exchangeability, 76, 99
Weather, 17, 153
Wilcoxon test, 62
Withdrawals, 103



Modern Recording Techniques
Modern Recording Techniques is the bestselling, authoritative guide to sound and
music recording. Whether you’re just starting out or are looking for a step-up
in the industry, Modern Recording Techniques provides an in-depth read on the
art and technologies of music production. It’s a must-have reference for all audio
bookshelves. Using its familiar and accessible writing style, this ninth edition
has been fully updated, presenting the latest production technologies and
includes an in-depth coverage of the DAW, networked audio, MIDI, signal
processing and much more.
A robust companion website features video tutorials, web-links, an online
glossary, ﬂashcards, and a link to the author’s blog. Instructor resources include
a test bank and an instructor’s manual.
The ninth edition includes:
n Updated tips, tricks and insights for getting the best out of your studio
n An introduction to the Apple iOS in music production
n Introductions to new technologies and important retro studio techniques
n The latest advancements in DAW systems, signal processing, mixing and
mastering
David Miles Huber is a 4-time Grammy-nominated producer and musician
who works in the electronic music and surround sound genres. He is widely
acclaimed in the recording industry as a consultant, author and guest lecturer
on the subject of digital audio and recording technology.

AUDIO ENGINEERING SOCIETY PRESENTS . . . 
http://www.aes.org/
Editorial Board
Chair: Francis Rumsey, Logophon Ltd. 
Hyun Kook Lee, University of Huddersﬁeld
Natanya Ford, University of West England
Kyle Snyder, Ohio University
Other titles in the Series:
Handbook for Sound Engineers, 5th Edition
Edited by Glen Ballou
Audio Production and Critical Listening, 2nd Edition
Authored by Jason Corey
Recording Orchestra and Other Classical Music Ensembles
Authored by Richard King
Recording Studio Design, 4th Edition
Authored by Philip Newell
Modern Recording Techniques, 9th Edition
Authored by David Miles Huber
Immersive Sound: The Art and Science of Binaural and Multi-Channel Audio
Edited by Agnieszka Roginska and Paul Geluso

Modern Recording 
Techniques
Ninth Edition
David Miles Huber
Robert E. Runstein

Ninth edition published 2018
by Routledge
711 Third Avenue, New York, NY 10017
and by Routledge
2 Park Square, Milton Park, Abingdon, Oxon OX14 4RN
Routledge is an imprint of the Taylor & Francis Group, an informa business
© 2018 Taylor & Francis
The right of David Miles Huber and Robert E. Runstein to be identiﬁed as the
authors of this work has been asserted by them in accordance with sections
77 and 78 of the Copyright, Designs and Patents Act 1988.
All rights reserved. No part of this book may be reprinted or reproduced or
utilized in any form or by any electronic, mechanical, or other means, now
known or hereafter invented, including photocopying and recording, or in
any information storage or retrieval system, without permission in writing
from the publishers.
Trademark notice: Product or corporate names may be trademarks or registered
trademarks, and are used only for identiﬁcation and explanation without
intent to infringe.
First edition published 1974 by H.W. Sams
Eighth edition published 2014 by Focal Press
Library of Congress Cataloging-in-Publication Data
Names: Huber, David Miles, author. | Runstein, Robert E., author.
Title: Modern recording techniques / David Miles Huber and 
Robert E. Runstein.
Description: Edition 9. | New York ; London : Routledge, 2017.
Identiﬁers: LCCN 2016055885| ISBN 9781138203679 (hardback) | 
ISBN 9781138954373 (pbk.)
Subjects: LCSH: Sound—Recording and reproducing. | Magnetic recorders 
and recording. | Digital audiotape recorders and recording.
Classiﬁcation: LCC TK7881.4 .H783 2017 | DDC 621.389/3—dc23
LC record available at https://lccn.loc.gov/2016055885
ISBN: 978–1–138–20367–9 (hbk)
ISBN: 978–1–138–95437–3 (pbk)
ISBN: 978–1–315–66695–2 (ebk)
Typeset in Giovanni Book
by Florence Production Ltd, Stoodleigh, Devon, UK
Visit the companion website: www.routledge.com/cw/huber

Several months before Prince’s death, I took the cover shot for this 
book at London Bridge Studio in Seattle. The black theme combined
with the retro look of their vintage Neve 8048 console intrigued me,
and I immediately envisioned making MRT9 “The Purple Rain Edition,”
complete with purple highlights. After losing him and so many other
great artists in 2016, it seemed that it would be a natural tribute to
those who used, currently use and will use the art of modern recording
technology to its fullest. I hope you enjoy and learn from it.
All the best!
DMH


ACKNOWLEDGMENTS..................................................................................................................xxv
CHAPTER 1
Introduction ...........................................................1
The Professional Studio Environment..........................................................3
The Professional Recording Studio.....................................................................................4
The Control Room.....................................................................................................................6
The Retro Revolution ..............................................................................................................9
The Changing Faces of the Music Studio Business .......................................................10
The Project Studio.........................................................................................11
Making the Project Studio Pay for Itself...........................................................................13
The Portable Studio .....................................................................................14
The iRevolution........................................................................................................................15
Live/On-location Recording: A Different Animal.......................................16
Audio for Video and Film..............................................................................16
Audio for Games............................................................................................17
The DJ............................................................................................................18
The Times, They’ve Already Changed: Multimedia and the Web ..............18
Power to the People!....................................................................................19
Whatever Works for You......................................................................................................20
The People Who Make It All Happen...........................................................20
The Artist ..................................................................................................................................21
Studio Musicians and Arrangers .........................................................................................21
The Producer............................................................................................................................21
The Engineer ...........................................................................................................................22
Assistant Engineer.................................................................................................................23
Maintenance Engineer ..........................................................................................................23
Mastering Engineer ...............................................................................................................24
Studio Management..............................................................................................................24
Music Law................................................................................................................................24
Women and Minorities in the Industry ..............................................................................25
Behind the Scenes .................................................................................................................25
Career Development....................................................................................26
Self-Motivation.......................................................................................................................27  
Networking: “Showing Up Is Huge”...................................................................................27
vii
Contents

Contents
The Recording Process ................................................................................29
1. Preparation..........................................................................................................................29
The Power of Preparation .................................................................................30
2. Recording ............................................................................................................................30
3. Overdubbing .......................................................................................................................34
Options in Overdubbing ...................................................................................36
4. Mixdown...............................................................................................................................37
5. Mastering.............................................................................................................................39
6. Product Manufacturing and/or Downloadable Distribution ..................................40
7. Marketing and Sales..........................................................................................................41
CHAPTER 2 
Sound and Hearing..............................................43
The Transducer.............................................................................................43
The Basics of Sound .....................................................................................45
Waveform Characteristics....................................................................................................47
Amplitude .......................................................................................................47
Frequency .......................................................................................................48
Frequency Response........................................................................................49
Velocity...........................................................................................................50
Wavelength.....................................................................................................51
Reﬂection of Sound ........................................................................................51
Diffraction of Sound.......................................................................................52
Phase..............................................................................................................53
Phase Shift.......................................................................................................54
Harmonic Content...........................................................................................55
Envelope .........................................................................................................59
Loudness Levels: The Decibel.....................................................................60
Logarithmic Basics................................................................................................................60
The Decibel...............................................................................................................................61
Sound-Pressure Level........................................................................................61
Voltage ...........................................................................................................62
Power .............................................................................................................63
The “Simple” Heart of the Matter......................................................................63
The Ear..........................................................................................................64
Threshold of Hearing.............................................................................................................65
Threshold of Feeling..............................................................................................................65
Threshold of Pain ...................................................................................................................65
Taking Care of Your Hearing ...............................................................................................65
Psychoacoustics.....................................................................................................................66
Auditory Perception..........................................................................................66
Beats ..............................................................................................................68
Combination Tones..........................................................................................68
Masking ....................................................................................................................................69
Perception of Direction.....................................................................................69
viii

Perception of Space ..........................................................................................71
Direct Sound ...................................................................................................72
Early Reﬂections..............................................................................................72
Reverberation..................................................................................................73
CHAPTER 3 
Studio Acoustics and Design..............................75
Studio Types.................................................................................................76
The Professional Recording Studio...................................................................................76
The Audio-for-Visual Production Environment..............................................................77
The Audio-for-Gaming Production Environment ...........................................................77
The Project Studio.................................................................................................................78
Primary Factors Governing Studio and Control Room Acoustics.............79
Acoustic Isolation ..................................................................................................................80
Walls..............................................................................................................82
Floors .............................................................................................................84
Risers..............................................................................................................85
Ceilings ..........................................................................................................86
Windows and Doors.........................................................................................87
Iso-Rooms and Iso-Booths .................................................................................88
Acoustic Partitions ...........................................................................................89
Noise Isolation Within the Control Room...........................................................90
Symmetry in Control Room Design....................................................................................91
Frequency Balance................................................................................................................92
Reﬂections.......................................................................................................93
Absorption.......................................................................................................96
High-Frequency Absorption...........................................................................98
Low-Frequency Absorption ............................................................................99
Room Reﬂections and Acoustic Reverberation........................................101
Acoustic Echo Chambers....................................................................................................103
CHAPTER 4
Microphones: Design and Application.............105
Microphone Design....................................................................................106
The Dynamic Microphone ..................................................................................................106
The Ribbon Microphone .....................................................................................................107
Further Developments in Ribbon Technology.....................................................108
The Condenser Microphone ..............................................................................................109
Powering a Condenser Mic .............................................................................110
External Power Supply..................................................................................110
Phantom Power ............................................................................................111
The Electret-Condenser Microphone...............................................................................112
Microphone Characteristics .......................................................................112
Directional Response...........................................................................................................112
Frequency Response ...........................................................................................................116
Transient Response..............................................................................................................117
Output Characteristics.........................................................................................................117
ix
Contents

Contents
Sensitivity Rating...........................................................................................117
Equivalent Noise Rating .................................................................................118
Overload Characteristics.................................................................................118
Microphone Impedance...................................................................................118
Balanced/Unbalanced Lines ..............................................................................................119
Microphone Preamps..................................................................................121
Microphone Techniques.............................................................................122
Other Microphone Pickup Issues......................................................................................123
Low-Frequency Rumble...................................................................................123
Proximity Effect .............................................................................................123
Popping.........................................................................................................124
Off-Axis Pickup .............................................................................................124
Pickup Characteristics as a Function of Working Distance .......................................125
Close Microphone Placement...........................................................................126
Leakage..........................................................................................................128
Recording Direct............................................................................................131
Distant Microphone Placement .......................................................................132
Room Microphone Placement.....................................................................134
Room Pickup in the Studio..........................................................................135
The Boundary Effect .....................................................................................136
“Reamping it” in the Mix .............................................................................137
Accent Microphone Placement.........................................................................139
Stereo and Immersive Mic Techniques...........................................................................139
Spaced Pair ...................................................................................................140
X/Y...............................................................................................................140
M/S..............................................................................................................141
Decca Tree ....................................................................................................143
Surround Miking Techniques ..........................................................................143
Ambient/Room Surround Mics ...................................................................144
Surround Decca Tree ....................................................................................144
Microphone Placement Techniques .........................................................144
Brass Instruments................................................................................................................146
Trumpet........................................................................................................146
Trombone......................................................................................................147
Tuba.............................................................................................................147
French Horn..................................................................................................148
Guitar.......................................................................................................................................148
Acoustic Guitar..............................................................................................148
Miking Near the Sound Hole.......................................................................149
Room and Surround Guitar Miking ............................................................149
The Electric Guitar.........................................................................................149
Miking the Guitar Amp ................................................................................149
Recording Direct ...........................................................................................150
The Electric Bass Guitar .................................................................................151
Keyboard Instruments .........................................................................................................151
Grand Piano .................................................................................................151
x

Separation .....................................................................................................152
Upright Piano................................................................................................153
Electronic Keyboard Instruments......................................................................153
Percussion..............................................................................................................................154
Drum Set......................................................................................................154
Miking The Drum Set ...................................................................................156
Kick Drum.....................................................................................................157
Snare Drum...................................................................................................158
Overheads .....................................................................................................158
Rack-Toms.....................................................................................................159
Floor-Tom .....................................................................................................160
Hi-Hat............................................................................................................160
Tuned Percussion Instruments.........................................................................160
Congas and Hand Drums ............................................................................160
Xylophone, Vibraphone and Marimba .......................................................161
Stringed Instruments ...........................................................................................................161
Violin and Viola ............................................................................................161
Cello.............................................................................................................162
Double Bass ..................................................................................................162
Voice........................................................................................................................................162
Mic Tools for The Voice ..................................................................................163
Woodwind Instruments ......................................................................................................164
Clarinet ........................................................................................................164
Flute.............................................................................................................164
Saxophone.....................................................................................................165
Harmonica....................................................................................................166
Microphone Selection................................................................................166
Shure SM57...........................................................................................................................166
Telefunken M81....................................................................................................................166
AKG D112 ...............................................................................................................................167
Royer Labs R-121.................................................................................................................167
Beyerdynamic M-160 .........................................................................................................168
AEA A440.............................................................................................................................169
Shure PGA181.......................................................................................................................169
AKG C214...............................................................................................................................170
Neumann TLM102...............................................................................................................170
ADK Z-251..............................................................................................................................171
Townsend Labs Sphere L22™...........................................................................................171
Telefunken U47, C12 and ELA M251E............................................................................173
CHAPTER 5 
The Analog Tape Recorder ...............................175
To Commit or Not to Commit it to Tape?....................................................176
Magnetic Recording and its Media............................................................176
The Professional Analog ATR...........................................................................................178
The Tape Transport ........................................................................................178
The Magnetic Tape Head................................................................................180
xi
Contents

Contents
Equalization..................................................................................................182
Bias Current..................................................................................................182
Monitoring Modes..........................................................................................183
To Punch or Not to Punch .................................................................................................184
Tape, Tape Speed and Head Conﬁgurations.................................................................185
Print-Through................................................................................................186
Analog Tape Noise.........................................................................................187
Cleanliness....................................................................................................189
Degaussing....................................................................................................189
Editing Magnetic Tape........................................................................................................189
Backup and Archive Strategies.........................................................................................191
Backing Up Your Analog Project......................................................................191
Archive Strategies ..........................................................................................192
C.L.A.S.P.................................................................................................................................192
Tape Emulation Plug-Ins....................................................................................................193
CHAPTER 6 
Digital Audio Technology .................................195
The Language of Digital .............................................................................195
Digital Basics.........................................................................................................................197
Sampling ......................................................................................................197
Quantization.................................................................................................198
The Devil’s in the Details....................................................................................................199
The Nyquist Theorem .....................................................................................200
Oversampling ................................................................................................201
Signal-to-Error Ratio......................................................................................201
Dither...........................................................................................................202
Fixed- vs. Floating-Point Processing .................................................................203
The Digital Recording/Reproduction Process.........................................203
The Recording Process .....................................................................................................204
The Playback Process........................................................................................................205
Sound File Basics.......................................................................................206
Sound File Sample Rates..................................................................................................206
Sound File Bit Depths ........................................................................................................207
Professional Sound File Formats ...................................................................................208
Regarding Digital Audio Levels.......................................................................................209
Digital Audio Transmission................................................................................................210
AES/EBU......................................................................................................210
S/PDIF .........................................................................................................211
SCMS...........................................................................................................212
MADI...........................................................................................................212
ADAT Lightpipe.............................................................................................213
TDIF ............................................................................................................214
Signal Distribution .....................................................................................214
What Is Jitter? .......................................................................................................................215
Wordclock ..............................................................................................................................216
xii

CHAPTER 7 The Digital Audio Workstation ..........................219
Integration Now—Integration Forever! ...................................................220
DAW Hardware...........................................................................................221
The Desktop Computer ......................................................................................................225
The Laptop Computer.........................................................................................................225
System Interconnectivity..........................................................................226
USB.........................................................................................................................................226
FireWire.................................................................................................................................228
Thunderbolt ..........................................................................................................................228
Audio Over Ethernet...........................................................................................................228
The Audio Interface ...................................................................................229
Audio Driver Protocols........................................................................................................231
Latency ...................................................................................................................................231
Need Additional I/O? ..........................................................................................................232
DAW Controllers ..................................................................................................................233
Hardware Controllers.....................................................................................233
Instrument Controllers....................................................................................234
Touch Controllers...........................................................................................234
Large-Scale Controllers...................................................................................235
Sound File Formats ....................................................................................235
Sound File Sample and Bit Rates....................................................................................236
Sound File Interchange and Compatibility Between DAWs.....................................236
DAW Software............................................................................................237
Sound Recording and Editing...........................................................................................238
Fixing Sound with a Sonic Scalpel ..................................................................................243
Comping.................................................................................................................................245
MIDI Sequencing and Scoring..........................................................................................245
Support for Video and Picture Sync...............................................................................246
Real-Time, On-Screen Mixing..........................................................................................246
DSP Effects ..........................................................................................................................247
DSP Plug-Ins........................................................................................................................248
Accelerator Processing Systems ........................................................................249
Fun with Effects.............................................................................................249
Equalization..................................................................................................249
Dynamics ......................................................................................................249
Delay..............................................................................................................251
Pitch and Time Change ................................................................................252
ReWire....................................................................................................................................253
Mixdown and Effects Automation...................................................................................254
Exporting a Final Mixdown to File ..................................................................................254
Power to the Processor . . . Uhhh, People! ...............................................255
1. Get a Computer That’s Powerful Enough..................................................................255
2. Make Sure You Have Enough Fast Memory............................................................256
3. Keep Your Production Media Separate.....................................................................256
4. Update Your Drivers—With Caution!..........................................................................257
xiii
Contents

Contents
5. Going (At Least) Dual Monitor.....................................................................................257
6. Keeping Your Computer Quiet ....................................................................................258
7. Backup, Archive and Networking Strategies...........................................................259
Computer Networking ....................................................................................260
8. Session Documentation................................................................................................262
Documenting Within the DAW ......................................................................262
Make Documentation Directories ....................................................................263
9. Accessories and Accessorize.......................................................................................263
In Closing....................................................................................................264
CHAPTER 8 
Groove Tools and Techniques..........................265
The Basics ..................................................................................................266
Pitch Shift Algorithms ........................................................................................................267
Warping .......................................................................................................268
Beat Slicing...........................................................................................................................268
Audio to MIDI .......................................................................................................................269
Groove Hardware ......................................................................................270
Groove Software ........................................................................................271
Looping Your DAW ..............................................................................................................271
Loop-based Audio Software.............................................................................................272
ReWire .........................................................................................................275
Groove and Loop-based Plug-Ins......................................................................276
Drum and Drum Loop Plug-Ins.......................................................................276
Pulling Loops into a DAW Session ............................................................277
Groove Controllers ....................................................................................278
DJ Software..........................................................................................................................278
Obtaining Loop Files from the Great Digital Wellspring .........................279
CHAPTER 9 MIDI and Electronic Music Technology .............281
The Power Of MIDI.....................................................................................282
MIDI Production Environments................................................................282
What Is MIDI?.............................................................................................284
What MIDI Isn’t..........................................................................................286
System Interconnections ..........................................................................287
The MIDI Cable.....................................................................................................................287
MIDI Phantom Power....................................................................................288
Wireless MIDI...............................................................................................288
MIDI Jacks.............................................................................................................................289
MIDI Echo....................................................................................................290
Typical Conﬁgurations .......................................................................................................290
The Daisy Chain............................................................................................291
The Multiport Network...................................................................................292
Exploring the Spec.....................................................................................293
The MIDI Message ..............................................................................................................293
xiv

MIDI Channels .............................................................................................294
MIDI Modes .................................................................................................296
Channel Voice Messages.................................................................................297
Explanation of Controller ID Parameters......................................................................300
System Messages ...............................................................................................................303
MIDI and the Computer .............................................................................306
Connecting to the Peripheral World ...............................................................................306
The MIDI Interface........................................................................................307
Electronic Instruments..............................................................................308
Inside the Toys.....................................................................................................................308
Instrument and Systems Plug-Ins.....................................................................310
Keyboards..............................................................................................................................310
The Synth .....................................................................................................310
Samplers.................................................................................................................................312
Sample Libraries and DIY Sampling ................................................................314
The MIDI Keyboard Controller..........................................................................................314
Percussion Instruments and Controllers.........................................................................317
The Drum Machine .......................................................................................317
MIDI Drum Controllers .................................................................................318
Drum Replacement ........................................................................................319
Other MIDI Instrument and Controller Types................................................................319
Sequencing.................................................................................................320
Integrated Hardware Sequencers...................................................................................320
Software Sequencers.........................................................................................................320
Basic Introduction to Sequencing..............................................................321
Recording...............................................................................................................................322
Setting a Session Tempo .................................................................................323
Changing Tempo ...........................................................................................323
Click Track....................................................................................................323
Multitrack MIDI Recording ............................................................................324
Punching In and Out .....................................................................................324
Step Time Entry.............................................................................................325
Drum Pattern Entry .......................................................................................326
MIDI to Audio........................................................................................................................327
Audio to MIDI........................................................................................................................327
Saving Your MIDI Files ......................................................................................................328
Documentation.....................................................................................................................329
Editing ....................................................................................................................................329
Practical Editing Techniques ...........................................................................330
Transposition ................................................................................................330
Quantization.................................................................................................330
Humanizing..................................................................................................331
Slipping in Time ...........................................................................................331
Editing Controller Values.............................................................................331
xv
Contents

Contents
Playback.................................................................................................................................332
Mixing a Sequence...............................................................................................................333
Music Printing Programs...........................................................................334
CHAPTER 10 The iOS in Music Production.............................337
Audio Inside the iOS...................................................................................338
Core Audio on the iOS........................................................................................................338
AudioBus ...............................................................................................................................339
Audio Units for the iOS ......................................................................................................339
Connecting the iOS to the Outside World.................................................340
Audio Connectivity..............................................................................................................340
MIDI Connectivity.................................................................................................................341
Recording Using iOS...................................................................................341
Handheld Recording Using iOS........................................................................................341
Mixing With iOS ...................................................................................................................342
iDAWs.....................................................................................................................................342
Taking Control of Your DAW Using the iOS..................................................................343
The iOS on Stage ........................................................................................345
iOS and the DJ .....................................................................................................................346
iOS as a Musical Instrument......................................................................346
The Ability to Accessorize.........................................................................347
CHAPTER 11 Multimedia and the Web..................................349
The Multimedia Environment ...................................................................350
The Computer.......................................................................................................................350
Television and the Home Theater ...................................................................................350
Delivery Media............................................................................................351
Networking.............................................................................................................................351
The Web..................................................................................................................................351
The Cloud .....................................................................................................352
Physical Media ...........................................................................................353
The CD ....................................................................................................................................353
The DVD.................................................................................................................................353
Blu-ray....................................................................................................................................355
The Flash Card and Memory USB Stick.........................................................................355
Media Delivery Formats............................................................................356
Uncompressed Sound File Formats ...............................................................................356
PCM Audio File Formats ................................................................................357
DSD Audio ...................................................................................................357
Compressed Codec Sound File Formats .......................................................................359
Perceptual Coding..........................................................................................359
MP3 ...............................................................................................................360
MP4 ...............................................................................................................361
WMA..............................................................................................................361
xvi

AAC................................................................................................................362
FLAC ..............................................................................................................362
Tagged Metadata...........................................................................................362
MIDI.............................................................................................................363
Standard MIDI Files............................................................................................................364
General MIDI.........................................................................................................................365
Graphics......................................................................................................367
Video...........................................................................................................368
Multimedia in the “Need for Speed” Era ..................................................369
On a Final Note ...........................................................................................370
CHAPTER 12 Synchronization ................................................371
Timecode ....................................................................................................372
Timecode Word.....................................................................................................................373
Sync Information Data...................................................................................374
Timecode Frame Standards.............................................................................374
Timecode Within Digital Media Production...................................................................376
Broadcast Wave File Format ...........................................................................376
MIDI Timecode .....................................................................................................................376
MIDI Timecode Messages ...............................................................................377
SMPTE/MTC Conversion ...............................................................................378
Timecode Production in the Analog Audio and Video Worlds ................379
Timecode Refresh and Jam Sync.....................................................................................379
Synchronization Using SMPTE Timecode....................................................................380
SMPTE Offset Times ..........................................................................................................381
Distribution of SMPTE Signals........................................................................................382
Timecode Levels.............................................................................................382
Real-World Applications Using Timecode and MIDI Timecode...............................383
Master/Slave Relationship...............................................................................383
Video’s Need for a Stable Timing Reference................................................384
Digital Audio’s Need for a Stable Timing Reference...................................385
Video Workstation or Recorder........................................................................385
Digital Audio Workstations.............................................................................385
Routing Timecode to and from Your Computer .................................................386
Analog Audio Recorders..................................................................................387
A Simple Caveat ............................................................................................387
Keeping Out of Trouble......................................................................................................387
CHAPTER 13 Ampliﬁers..........................................................389
Ampliﬁcation..............................................................................................389
The Operational Ampliﬁer..................................................................................................391
Preampliﬁers .................................................................................................392
Equalizers .....................................................................................................392
Summing Ampliﬁers ............................................................................................................393
xvii
Contents

Contents
Distribution Ampliﬁers........................................................................................................393
Power Ampliﬁers..................................................................................................................393
Voltage and Digitally Controlled Ampliﬁers..................................................................394
CHAPTER 14 Power- and Ground-Related Issues ................397
Grounding Considerations.........................................................................397
Power Conditioning...................................................................................399
Multiple-Phase Power.......................................................................................................400
Balanced Power .............................................................................................401
Hum, Radio Frequency (RF) and Electro-Magnetic 
Induction (EMI)......................................................................................401
CHAPTER 15 Signal Processing.............................................403
The Wonderful World of Analog, Digital or Whatever ............................403
The Whatever.......................................................................................................................403
Analog....................................................................................................................................404
Analog Recall ................................................................................................404
Digital.....................................................................................................................................405
Plug-Ins ......................................................................................................405
Plug-In Control and Automation.....................................................................................406
Signal Paths in Effects Processing...........................................................407
Insert Routing ......................................................................................................................407
External Control Over an Insert Effect’s Signal Path .........................................408
Send Routing .......................................................................................................................409
Viva La Difference ..............................................................................................................409
Side Chain Processing ....................................................................................410
Effects Processing......................................................................................411
Hardware and Plug-In Effects in Action..........................................................................411
Equalization..................................................................................................411
Peaking Filters...............................................................................................412
Shelving Filters..............................................................................................413
High-Pass and Low-Pass Filters....................................................................413
Equalizer Types.............................................................................................414
Applying Equalization..................................................................................416
EQ in Action!................................................................................................416
Sound-Shaping Effects Devices and Plug-Ins..............................................419
Dynamic Range.............................................................................................420
Dynamic Range Processors..............................................................................421
Compression..................................................................................................421
Multiband Compression ..............................................................................427
Limiting........................................................................................................428
Expansion .....................................................................................................429
The Noise Gate..............................................................................................431
Time-Based Effects............................................................................................................432
xviii

Delay............................................................................................................432
Delay in Action: Less than 15 ms ................................................................432
Delay in Action: 15 to 35 ms .......................................................................433
Delay in Action: More than 35 ms ..............................................................434
Reverb ..........................................................................................................434
Reverb Types .................................................................................................436
Psychoacoustic Enhancement...........................................................................437
Pitch Shifting ................................................................................................437
Time and Pitch Changes...............................................................................438
Automatic Pitch Correction .........................................................................439
Multiple-Effects Devices..................................................................................440
Dynamic Effects Automation and Editing ................................................441
CHAPTER 16 Noise Reduction ...............................................443
Digital Noise Reduction.............................................................................443
Fast Fourier Transform.....................................................................................................444
Restoration....................................................................................................446
Single-Ended Noise-Reduction Process ......................................................................446
The Noise Gate..............................................................................................447
CHAPTER 17 The Art and Technology of Mixing ..................449
The Art of Mixing.......................................................................................449
Listening................................................................................................................................450
Ear Training..................................................................................................450
Preparation............................................................................................................................451
Fixing It in the Mix........................................................................................451
Preparing for the Mix.....................................................................................451
Providing a Reference Track.........................................................................452
Gain Structure...............................................................................................452
Human Factors..............................................................................................453
A Review of the Recording Process .........................................................454
Recording..............................................................................................................................454
Monitoring....................................................................................................456
Overdubbing .........................................................................................................................457
The Technology of Mixing.........................................................................458
Understanding the Underlying Concept of “The Mixing Surface” .........460
The Mixer/Console Input Strip .........................................................................................461
Gain Level Optimization ................................................................................462
1. Channel Input (Preamp) ............................................................................463
Hardware Console/Mixer Insert Point.........................................................465
Virtual DAW Insert Point .............................................................................465
2. Auxiliary Send Section................................................................................466
3. Equalization..............................................................................................467
4. Dynamics Section ......................................................................................469
xix
Contents

Contents
5. Monitor Section .........................................................................................469
In-Line Monitoring.......................................................................................470
Direct Insert Monitoring ..............................................................................471
Separate Monitor Section.............................................................................471
6. Channel Fader...........................................................................................472
7. Output Section ..........................................................................................473
8. Channel Assignment ..................................................................................474
9. Grouping...................................................................................................475
10. Main Output Mix Bus..............................................................................477
11. Monitor Level Section...............................................................................477
12. Patch Bay................................................................................................478
13. Metering .................................................................................................479
The Finer Points of Metering........................................................................480
The VU Meter ................................................................................................482
The Average/Peak Meter ...............................................................................483
Digital Console and DAW Mixer/Controller Technology...........................................483
The Virtual Input Strip...................................................................................484
The DAW Software Mixer Surface...................................................................485
Mix-Down Level and Effects Automation .........................................................486
Write Mode ...................................................................................................487
Read Mode ....................................................................................................487
Drawn (Rubber Band) Automation.............................................................487
The Finer Points of Mixing........................................................................488
Mixing and Balancing Basics............................................................................................489
A Final Footnote on the Art of Mixing.......................................................491
CHAPTER 18 Monitoring ........................................................493
Speaker and Room Considerations..........................................................494
Monitor Speaker Types.....................................................................................................496
Far-Field Monitoring......................................................................................496
Near-Field Monitoring ...................................................................................497
Small Speakers ..............................................................................................499
Headphones ..................................................................................................500
Earbuds ........................................................................................................500
In-Ear Monitoring..........................................................................................500
Your Car.......................................................................................................501
Speaker Design.....................................................................................................................501
Active Powered vs. Passive Speaker Design .......................................................502
Speaker Polarity..................................................................................................................503
Balancing Speaker Levels.................................................................................................504
Monitoring..................................................................................................505
Monitor Level Control ........................................................................................................506
Monitor Volume ............................................................................................507
Spectral Reference .............................................................................................................507
Monitoring Conﬁgurations in the Studio .......................................................................508
xx

1.0 Mono......................................................................................................508
2.0 Stereo .....................................................................................................509
2+1 Stereo + Sub ...........................................................................................509
Monitoring in the Recording Space ..................................................................................511
Headphones in the Studio...............................................................................511
Playback Speakers in the Studio.......................................................................513
CHAPTER 19 Immersive Audio (5.1 and Beyond) ..................515
Immersive Audio: Past to the Present ......................................................516
Stereo Comes to Television...............................................................................................517
Theaters Hit Home...............................................................................................................518
Today’s Immersive Audio Experience.............................................................................518
Mixing in Surround .....................................................................................518
Surround Hardware/Software.........................................................................................520
Surround Interfacing......................................................................................520
Surround Channel Assignments.......................................................................521
Monitoring in 5.1 and Beyond ....................................................................521
5.1 Speaker Placement and Setup ..................................................................................522
The LFE........................................................................................................523
Bass Management in a Surround System..........................................................524
Practical 5.1 Placement..................................................................................524
5.0 Surround Minus an LFE ...........................................................................525
7.1 Speaker Placement ...................................................................................525
9.1 and 11.1 Height Channels—Adding to the Experience ................................525
Dolby Atmos.................................................................................................525
Auro Technologies Auro3D..........................................................................526
Active/Passive Monitors in Surround ............................................................................526
Noise Calibration..................................................................................................................527
Desert Island Reference Files/Discs..............................................................................528
Surround Final Mastering and Delivery Formats ....................................528
Uncompressed PCM Audio...............................................................................................528
Dolby Digital (AC3) .............................................................................................................529
DTS .........................................................................................................................................529
MP4 ........................................................................................................................................529
FLAC.......................................................................................................................................529
Down-Mix/Up-Mix ..............................................................................................................530
Down-Mix from 5.1.......................................................................................530
Up-Mix to 5.1 ...............................................................................................530
Authoring for Immersive............................................................................531
Reissuing Back Catalog Material...............................................................531
CHAPTER 20 Mastering ..........................................................533
The Final Mix ..............................................................................................534
Getting Too Close to the Mix ............................................................................................535
xxi
Contents

Contents
Hiring a Professional Mastering Engineer ...............................................535
To Master or Not to Master—Was That the Question?.............................537
“Pre”-paration ......................................................................................................................538
Providing Stems...................................................................................................................539
Providing a Reference Track............................................................................................540
To Be There, Or Not to Be There....................................................................................540
Sequencing: The Natural Order of Things.....................................................................540
To Master or Not to Master the Project Yourself—That’s the Next
Question! ................................................................................................541
The Zen of Self Self-Mastering? .....................................................................................542
Desert Island Mixes........................................................................................543
Two-Step or One-Step (Integrated) Mastering Option ............................................543
Understanding the Signal Chain ......................................................................................544
Mastering the Details of a Project ............................................................545
Sound File Volume..............................................................................................................545
Sound File Resolution........................................................................................................546
Dither......................................................................................................................................546
Relative Volumes.................................................................................................................547
EQ............................................................................................................................................547
Dynamics...............................................................................................................................548
Compression in Mastering ..............................................................................549
Limiting in Mastering ....................................................................................550
Multiband Dynamic Processing in Mastering....................................................550
Mid/Side Processing .......................................................................................551
Mastering Plug-Ins...............................................................................................................551
On a Final Note.....................................................................................................................552
CHAPTER 21 Product Manufacture and Distribution ...........553
Product Manufacture.................................................................................555
The CD ....................................................................................................................................555
The CD Manufacture Process..........................................................................556
CD Burning ..................................................................................................558
DVD and Blu-ray Burning..................................................................................................559
Optical Disc Handling and Care........................................................................................560
Vinyl.........................................................................................................................................561
Disc Cutting..................................................................................................561
Disc-Cutting Lathe........................................................................................562
Cutting Head.................................................................................................562
Pitch Control.................................................................................................563
The LP Mastering Process .............................................................................564
Vinyl Disc Plating and Pressing ...................................................................564
Product Distribution..................................................................................565
Online Distribution...............................................................................................................565
Uploading to Stardom ....................................................................................566
Build Your Own Website................................................................................567
xxii

Thoughts on Being (and Getting Heard) in Cyberspace .....................................568
Streaming .....................................................................................................568
Free Streaming Services..................................................................................569
Internet Radio ...............................................................................................569
Money for Nothin’ and the Chicks . . . .............................................................569
Legal Issues ..........................................................................................................................569
Royalties and Other Business Issues .................................................................570
Ownership of the Masters ...............................................................................571
Registering Your Work ...................................................................................571
Form SR.........................................................................................................571
Form PA.........................................................................................................571
Collecting the $$$ .........................................................................................572
Further Reading...................................................................................................................572
CHAPTER 22 Studio Tips and Tricks.......................................573
Preparation and the Details ......................................................................573
What’s a Producer and Do You Really Need One?......................................................574
Do You Need a Music Lawyer?.........................................................................................575
A Short, Step-by-Step Overview of the Production Process..................576
1. Long Before Going into the Studio..............................................................................576
2. Before Going into the Studio........................................................................................577
3. Going into the Studio......................................................................................................578
4. Setting Up ........................................................................................................................580
5. Session Documentation................................................................................................580
6. Recording ..........................................................................................................................581
7. Mix-Down..........................................................................................................................583
8. Mastering..........................................................................................................................583
9. Backup and Archive Strategies...................................................................................584
10. Household Tips..............................................................................................................585
11. Personal Skills and Tools.............................................................................................586
12. Protect Your Investment ............................................................................................588
13. Protect Your Hardware...............................................................................................588
14. Update Your Software ................................................................................................588
15. Read Your Manuals ......................................................................................................589
16. A Word on Professionalism........................................................................................589
In Conclusion..............................................................................................589
CHAPTER 23 Yesterday, Today and Tomorrow ....................591
Yesterday....................................................................................................591
Today ..........................................................................................................595
Tomorrow...................................................................................................598
Happy Trails ...............................................................................................598
INDEX.................................................................................................................................................601
xxiii
Contents


I’d like to thank my life partner, Daniel Butler, for putting up with the general
rantin’, ravin’ and all-round craziness that goes into writing a never-ending book
project. I’d also like to express my thanks to all of my friends and family in the
United States and Europe, as well as to my music collaborators who help me
reach new heights, both in the studio and on-stage. I’d like to give special thanks
to Emiliano Caballero Fraccaroli (Amsterdam/Belgium); the folks at Easy Street
Records in West Seattle; Galaxy Studios (Belgium); Dominik Trampf, nhow
Hotel, Drew, Michael, Yvonne and Uli (Berlin); Sara, Steve and my buddies at
Big Arts Labs (LA); Maurice Pastist (PMC Speakers, US); The Recording Academy
(the Grammy folks in LA/Paciﬁc NW); Greg & the folks at Steinberg North
America (LA); Native Instruments (LA/Berlin); Ableton (Berlin) and Zerodebug
(Berlin).
A sincere thanks to you all for your kindness and support!
David Miles Huber
www.davidmileshuber.com
xxv
Acknowledgments


The world of modern music and sound production is multifaceted. It’s an
exciting world of creative individuals: musicians, engineers, producers, managers,
manufacturers and business-people who are experts in such ﬁelds as music,
acoustics, electronics, sales, production, broadcast media, multimedia, market -
ing, graphics, law and the day-to-day workings of the business of music. The
combined efforts of these talented people work together to create a single end
product: music that can be marketed to the masses. The process of turning a
creative spark into a ﬁnal product takes commitment, talent, a creative pro -
duction team, a marketing strategy and, often, money. Throughout the history
of recorded sound, the process of capturing music and transforming it into a
marketable product has always been driven by changes in the art of music,
production technology and cultural tastes.
In the past, the process of turning one’s own music into a ﬁnal product required
the use of a commercial recording studio, which was (and still is) equipped
with specialized equipment and a professionally skilled staff. With the
introduction of the large-scale integrated (LSI) circuit, mass production and mass
marketing (three of the most powerful forces in the Information Age) another
option has arrived on the scene: the radical idea that musicians, engineers
and/or producers can produce music in their own facility or home . . . on their
own time. Along with this concept comes the realization that almost anyone
can afford, construct and learn to master their own personal audio production
facility. In short, we’re living in the midst of a techno-artistic revolution that
puts more power, artistic control and knowledge directly into the hands of artists
and creative individuals from all walks of life . . . a fact that assures that the
industry will forever be a part of the creative life-force of change.
Those who are new to the world of modern digital audio and multitrack
production, musical instrument digital interface (MIDI), mixing, remixing and
the studio production environment should be aware that years of dedicated
practice are often required to develop the skills that are needed to successfully
master the art and application of these technologies. In short, it takes time to
master the craft. A person new to the recording or project studio environment
1
CHAPTER 1
Introduction

Introduction
(Figures 1.1 and 1.2) might easily be overwhelmed by the amount and variety
of equipment that’s involved in the process; however, as you become familiar
with the tools, toys and techniques of the recording process, a deﬁnite order to
the studio’s makeup will soon begin to emerge—with each piece of equipment
and personal approach to production being designed to play a role in the
overall scheme of making music and quality audio.
The goal of this book is to serve as a guide and reference tool to help you become
familiar with the recording and production process. When used in conjunction
with mentors, lots of hands-on experience, further reading, Web searching, soul
2
FIGURE 1.1
The historic (but newly
renovated) Capitol Records
Recording Studios, Los
Angeles, CA. (a) Studio A
control room. (Courtesy of
PMC Ltd., www.pmc-
speakers.com) (b) Studio A.
(Courtesy of Capitol
Records, www.capitol
records.com)
FIGURE 1.2 
One of the many, many
possible examples of a
project studio. (Courtesy of
Bernd Scholl
Musikproduktionen, 
www.moonbooter.de, 
© foto by moonbooter.de)

searching and simple common sense, I hope this book will help introduce you
to the equipment and day-to-day practices of the studio. Although it’s taken
the modern music studio over a hundred years to evolve to its current level of
technological sophistication, we have moved into an important evolutionary
phase in the business of music and its production: the digital age. Truly, this
is an amazing time in production history, when we can choose between an
array of powerful tools for fully realizing our creative and human potential in
a cost-effective way. As always, patience and a nose-to-the-grindstone attitude
are needed in order to learn how to use them effectively, but today’s technology
can free you up for the really important stuff: making music and audio
productions. In my opinion, these are deﬁnitely the good ol’ days!
3
Introduction  CHAPTER 1
This book, by its very nature, is an overview of
recording technology and production. It’s a very in-
depth one, but there’s absolutely no way that it can
fully devote itself to all of the topics. However, we’re
lucky enough to have the Web at our disposal to
help us dig deeper into a particular subject that we
might not fully understand, or simply want to know
more about. Giga-tons of sites can be found that are
dedicated to even the most off-beat people, places,
toys and things . . . and search engines can even
help you ﬁnd obscure information on how to ﬁx a
self-sealing stem-bolt on a 1905 sonic-driven
nutcracker. As such, I strongly urge you to use the
Web as an additional guide. For example, if there’s a
subject that you just don’t get, look it up on
www.wikipedia.org or simply Google it. 
Of course, there’s a wealth of info that can be found 
by searching the innumerable www.youtube.com
videos that relate to any number of hardware
systems, software toys and production techniques.
Further information relating to this book and the
recording industry at large can also be found at
www.modrec.com. Digging deeper into the Web will
certainly provide you with a different viewpoint or
another type of explanation, and having that “AH
HA!” light bulb go off (as well as the “hokey pokey”)
is deﬁnitely what it’s all about.
David Miles Huber (www.davidmileshuber.com)
Try This: Diggin’ Deep into the Web
D I Y
 do  it  yourself
THE PROFESSIONAL STUDIO ENVIRONMENT
The commercial music studio is made up of one or more acoustic spaces that
are specially designed and tuned for the purpose of capturing the best possible
sound onto a recorded medium. In addition, these facilities are often structurally
isolated in order to keep outside sounds from entering the room and being
recorded (as well as to keep inside sounds from leaking out and disturbing the
surrounding neighbors). In effect, the most important characteristics that go
into the making and everyday workings of such a facility include:

The Professional Studio Environment
4
n A professional staff
n Professional equipment
n Professional, yet comfortable working environment
n Optimized acoustic and recording environment
n Optimized control room mixing environment
The Professional Recording Studio
Professional recording studio spaces vary in size, shape and acoustic design
(Figures 1.3 through 1.5) and usually reﬂect the personal taste of the owner or
are designed to accommodate the music styles and production needs of clients,
as shown by the following examples:
n A studio that records a wide variety of music (ranging from classical to
rock) might have a large main room with smaller, isolated rooms off to
the side for unusually loud or soft instruments, vocals, etc.
n A studio designed for orchestral ﬁlm scoring might be larger than other
studio types. Such a studio will often have high ceilings to accommodate
the large sound buildups that are often generated by a large number of
studio musicians.
n A studio used to produce audio for video, ﬁlm dialogue, vocals and
mixdown might consist of only a single, small recording space located off
the control room for overdub purposes.
FIGURE 1.3 
Alicia Keys’ Oven Studios,
New York. (Courtesy of
Walters-Storyk Design
Group, www.wsdg.com)
FIGURE 1.4 
Trilogy Studios, San
Francisco. (Courtesy of
Walters-Storyk Design
Group, www.wsdg.com)

5
Introduction  CHAPTER 1
FIGURE 1.5 
Main hall at Galaxy Hall,
Galaxy Studios, Mol,
Belgium. (Courtesy of
Galaxy Studios,
www.galaxy.be)
In fact, there is no secret formula for determining the perfect studio design.
Each studio design (Figures 1.6 and 1.7) has its own sonic character, layout,
feel and decor that are based on the personal tastes of its owners, the designer
(if one was involved) and the going studio rates (based on the studio’s return
on investment and the supporting market conditions).
FIGURE 1.7 
Floor plan of Paisley Park’s
Studio A, Chanhassen, MN.
(Courtesy of Paisley Park
Studios)
FIGURE 1.6 
Basic floor plan for KMR
Audio Germany, Berlin.
(Courtesy of KMR Audio,
www.kmraudio.de; studio
design by Fritz Fey,
www.studioplan.de)

The Professional Studio Environment
6
During the 1970s, studios were generally small. Because of the new development
of (and over-reliance on) artiﬁcial reverb and delay devices, they tended to be
acoustically “dead” in that the absorptive materials tended to suck the life right
out of the room. The basic concept was to eliminate as much of the original
acoustic environment as possible and replace it with artiﬁcial ambience.
Fortunately, as tastes began to change and music-makers grew tired of relying
entirely upon artiﬁcial ambience, rooms (both large and smaller) began to revert
back to the idea of basing their acoustics upon a combination of absorption
and natural acoustic reﬂections. This use of balanced acoustics has revived the
art of capturing the room’s original acoustic ambience along with the actual
sound pickup. In fact, through improved studio design techniques, we have
learned how to achieve the beneﬁts of both earlier and modern-day recording
eras by building a room that provides a reasonable-to-maximum amount of
isolation within the room (thereby reducing unwanted leakage from an instru -
ment to other mics in the room), while encouraging higher-frequency reﬂections
that can help give life and ambience to the overall sound. This natural balance
of absorption and reﬂection is used to “liven up” the sound of an instrument
or ensemble when they are recorded at a distance, a technique that has become
popular when recording live rock drums, string sections, electric guitars, choirs,
etc. Using close mic techniques, it’s also possible to use one or more iso-booths
or smaller rooms as a tool, should greater isolation be needed.
In short, it is this combination of the use of acoustic treatment, proper mic tech -
niques and a personal insight into the instruments and artists within a room
(combined with a sense of experimentation, experience and personal preferences)
that can bring out the best in a recording facility.
In certain situations, a studio might not have a large recording space at all but
simply have a small or mid-sized iso-room for recording overdubs (this is often
the case in facilities that are used in audio-for-visual post-production and/or
music remixing). Project studios, mix rooms and newer “concept” studios might
not have a separate recording space at all, opting to create an environment
whereby the artists can record directly within the mixing/production space itself.
The Control Room
A recording studio’s control room (Figures 1.8 through 1.10) serves a number of
purposes in the recording process. Ideally, the control room is acoustically
isolated from the sounds that are produced in the studio, as well as from the
surrounding, outer areas. It is optimized to act as a critical listening environment
that uses carefully placed and balanced monitor speakers. This room also houses
the majority of the studio’s recording, control and effects-related equipment.
At the heart of the control room is the recording console and/or Digital Audio
Workstation.

7
Introduction  CHAPTER 1
The recording console (also referred to as the board or desk, as seen in Figure 1.11a)
can be thought of as an artist’s palette for the artists, producer and/or recording
engineer. The console allows the engineer to combine, control and distribute the
input and output signals of most, if not all, of the devices found in the control
room. The console’s basic function is to allow for any combination of mixing
(variable control over relative amplitude and signal blending between channels),
spatial positioning (left/right or surround-sound control over front, center, rear
and sub), routing (the ability to send any input signal from a source to a desti -
nation) and switching for the multitude of audio input/output signals that are
commonly encountered in an audio production facility . . . not to mention the
fact that a console will also need to work in conjunction with a recording device.
FIGURE 1.8 
In the middle of a session.
(Courtesy of Kris Gorsky,
Gdansk, Poland)
FIGURE 1.9 
Boston Symphony Hall
control room. (Courtesy of
Walters-Storyk Design
Group, www.wsdg.com)
FIGURE 1.10 
Synchron Stage control
room A, Vienna. (Courtesy of
Walters-Storyk Design
Group, www.wsdg.com)

The Professional Studio Environment
8
A digital audio workstation (DAW, as seen in Figure 1.11b) is a multi-channel
media monster (audio, MIDI and video) that can work in conjunction with an
outboard recording console or mixer or it can work entirely on its own. DAWs
are increasingly common ﬁxtures within most control rooms, allowing us to
work in an “in-the-box” stand-alone fashion.
The analog tape machine (24, 16, 8, 4 and 2 tracks, as seen in Figure 1.11c) is
another way to capture sounds in the studio, using a way of working that’s quite
different (both functionally and sonically) than its digital counterpart.
FIGURE 1.11 
The heart(s) of the recording
studio. (a) The recording
console. (b) The digital
audio workstation (DAW). 
(c) The analog tape
recorder.
Tape machines might be located toward the rear of a control room, while a 
DAW might be located at the side of the console or at the functional center 
(if the DAW serves as the room’s main recording/mixing device). Because of the
added noise and heat generated by recorders, computers, power supplies,
ampliﬁers and other devices, it’s becoming more common for equipment to be
housed in an isolated machine room that has a window and door adjoining the
control room for easy access and visibility. In either case, DAW controller surfaces
(which are used for computer-based remote control and mixing functions) and
auto-locator devices (which are used for locating tape and media position cue
points) are often situated in the control room near the engineer for easy access
to all recording, mixing and transport functions. Effects devices (used to
electronically alter and/or augment the character of a sound) and other signal
processors are also placed nearby for easy accessibility (often being designed into
an effects island or bay that’s often located directly behind the console).
As with recording studio designs, every control room will usually have its own
unique sound, feel, comfort factor and studio booking rate. Commercial control
rooms often vary in design and amenities—from a room that’s basic in form
and function to one that is lavishly outﬁtted with the best toys. Again, the 
style and layout are a matter of personal choice; however, as you’ll see through-
out this book, there are numerous guide lines that can help you make the most
of a recording space. It’s really important to keep in mind that although the

9
Introduction  CHAPTER 1
To me, the mainstream acceptance of all things retro goes above and beyond
the reverence and interest in just older equipment—it has ﬁnally begun to
further connect us with our use and interest in past techniques. This includes:
n The use of distance techniques when placing microphones
n The use of analog tape machines (or their modeled plug-in counterparts)
to add an indeﬁnable punch to our mixes
layout and equipment will always be important, it’s the people (the staff, musi -
cians and you) that will almost always play the most prominent role in capturing
the feel of a performance and the heart of your clients.
The Retro Revolution
A revolution that has been making itself increasingly felt over the last decade
at all levels of studio production is the desire for all things retro (Figures 1.12
and 1.13). Perhaps it’s a need to revert back to our steampunk past, or just a
nostalgia for simpler days, but retro is deﬁnitely in. So, what is retro? It’s a
desire to put older devices and techniques back into practice in our current
productions, or to have new things that are designed in the style and function
of yesteryear in our studios. Either way, it's often very cool to make use of these
new, older toys in order to give our productions a fresh sound.
FIGURE 1.12 
Joe Tritschler knows how 
to live the retro life. (Photo
credit: Chris Bell, 2013)
FIGURE 1.13 
Welcome to 1979’s control
room. (Courtesy of Welcome
to 1979, Nashville, TN,
www.welcometo1979.com)

The Professional Studio Environment
10
n The willingness to use older equipment to add a different sonic character
to our sound
n The willingness to place a set of speakers in the studio and mic them, so
as to add “room sound” to a mix
n Placing a speaker/mic combination in the bathroom down the hall to get
“that sound”
n Recording the guitar track in a huge gym down the street with your laptop,
to get a larger-than-life acoustic sound
In short, I think retro is helping us to accept that all things don’t have to be
new in order to be awesome and relevant. It can be a piece of equipment, it
can be a mic technique, it can be expensive or it can cost nothing to use tools
you already have at your disposal. All you need is a sense of adventure and a
willingness to experiment.
The Changing Faces of the Music Studio Business
As we’ve noted, the role of the professional recording studio has begun to change
as a result of upsurges in project studios, audio for video and/or ﬁlm, multimedia
and the Internet. These market forces have made it necessary for certain facilities
to rethink their operational business strategies. Sometimes this means that a
studio will not be able to adapt to the changing times, however, those who are
able to react and diversify in the new digital age, new possibilities can be met
with success, as is illustrated by the following examples:
n Personal production and home project studios have greatly reduced the
need for an artist or producer to have constant and costly access to a
professional facility. As a result, many pro studios now cater to artists and
project studio owners who might have an occasional need for a larger
space or better-equipped recording facility (e.g., for recording big drum
sounds, string overdubs or an orchestral session). In addition, after an
important project has been completed in a private studio, a professional
facility might be needed to mix the production down into its ﬁnal form.
Most business-savvy studios are only too happy to capitalize on these new
and constantly changing market opportunities.
n Upsurges in the need for audio for video, game and ﬁlm postproduction
have created new markets that allow professional recording studios to
provide services to the local, national and international broadcast and
visual production communities. Creative studios often enter into lasting
relation ships with audio-for-visual and broadcast production markets, so
as to thrive in the tough business of music, when music production alone
might not provide enough income to keep a studio aﬂoat.
n Studios are also taking advantage of Internet audio distribution techniques
by offering Web development, distribution and other audio-for-web
services as an added incentive to their clients.

n A number of studios are also jumping directly into the business of music
by offering advisory, business, networking and management services to
artists and bands, sometimes signing the artists and funding tours 
in exchange for a piece of the business pie.
n A studio with several rooms might offer one of the rooms to an established
engineer/mixer, offering it up as “Joe’s Mix Room” in exchange for a roster
of clients that comes with the territory of having “Joe” on board.
These and other aggressive marketing strategies (many of which may be unique
to a particular area) are being widely adopted by commercial music and
recording facilities to meet the changing market demands of new and changing
media. No longer can a studio afford to place all of its eggs in one media basket.
Tapping into changes in market forces and meeting them with new solutions
are important for making it (or simply keeping aﬂoat) in the business of modern
music production and distribution. Make no mistake about it, starting, stafﬁng
and maintaining a production facility, as well as getting the clients’ music heard,
is serious work that requires dedication, stamina, innovation, guts and a deﬁnite
dose of craziness.
OK, now for the hard part. Let’s take a moment to say that all-important word
again: business. With the onset of more creative changes, opportunities and
options, the only thing that stays constant is change, right? For example, there
has been a steady onslaught of technological advances in audio production (such
as new software, portable recording and controller options that come with new
generations of computer, laptop and pad technologies). However, beyond that,
in reality many (if not most) of the technical aspects of music and audio
production have stayed the same. What has drastically changed are the business
aspects of the industry—most notably in how music is distributed, marketed
and consumed by the buying public.
I certainly don’t have to remind you about how the traditional label distribution
models have all changed in the wake of the download era. Over the last decade,
the game has been continuously evolving in a way that keeps even the most
seasoned industry professionals on their toes . . . is this necessarily a bad thing?
I’m not convinced it is. Innovation and ingenuity have always been part of the
creative process . . . it’s what keeps things new and exciting. However, innovation
doesn’t always come easily and is often at a price. Recording studios are con -
stantly being challenged to ride the wave of innovation—some make it, some
are simply unable to adapt to the new world of the personal studio, the Internet
and changing business models. In short, the reality is that the professional studio
business is a tough one that requires that the studio make itself and its expertise
relevant and marketable in the changing world of music and media production.
THE PROJECT STUDIO
With the advent of affordable, high-quality digital audio workstations, plug-
ins, controllers and speakers, it’s a foregone conclusion that the vast majority
11
Introduction  CHAPTER 1

The Project Studio
12
of music and audio recording/production systems are being built and designed
for personal use. The rise of the project studio (Figures 1.14 through 1.16) has
brought about monumental changes in the business of music and professional
audio, in a way that has affected and altered almost every facet of the audio
production community.
FIGURE 1.14 
Happiness is recording 
your own band in the
basement. (Courtesy of
Brooks Callison,
www.callisonic.com)
FIGURE 1.15 
The author’s project 
studio when he’s in Berlin.
(Courtesy of DMH,
www.davidmileshuber.com)
FIGURE 1.16 
Project studio showing
standard screen for
waveform navigation and a
touchscreen at the mixer
position (the author uses a
similar configuration).
(Courtesy of Chad Kelly)
One of the greatest beneﬁts of a project or portable production system centers
on the idea that an artist can choose from a wide range of tools and toys to get
the particular sounds that he or she likes on their own schedule and without
hiring out a pro studio. This technology is often extremely powerful, as the
components combine to create a vast palette of sounds and handle a wide range
of task-speciﬁc functions. Such systems often include a DAW (digital audio
workstation computer for recording, MIDI sequencing, mixing and just about
anything that relates to modern audio production), soft/hardware electronic
instruments, soft/hardware effects devices, as well as speaker monitoring.

13
Introduction  CHAPTER 1
Systems like these are constantly being installed in the homes of working and
aspiring musicians, audio enthusiasts and DJs. Their sizes range from a corner
in an artist’s bedroom to a larger system that has been installed in a dedicated
project studio. All of these system types can be designed to handle a wide range
of tasks and have the important advantage of letting the artist produce his or
her music in a comfortable, cost-effective, at-home environment whenever the
creative mood hits. Such production luxuries, which would have literally cost
a fortune 30 years ago, are now within the reach of almost all working and
aspiring musicians. This revolution has been carried out under the motto “You
don’t have to have a million-dollar studio to make good music.” Truly, the
modern-day project and portable studio systems offer such a degree of cost-
effective power and audio ﬁdelity that they can often match the production
quality of a professional recording facility—all you need to supply is knowledge,
care, dedication, patience and artistry.
Making the Project Studio Pay for Itself
Beyond the obvious advantage of being able to record when, where and how
you want to in your own project studio, there are several additional beneﬁts to
working in a personal environment. Here are ways that a project studio can
help subsidize itself, at any number of levels:
n Setting your own schedule and saving money while you’re at it! An obvious
advantage of a project studio revolves around the idea that you can create
your own music on your own schedule. Part of the expense of using a
professional studio comes from having to be practiced and ready to roll
on a speciﬁc date or range of dates. Having your own project studio frees
you up to lay down practice tracks and/or record when the mood hits,
without having to worry about punching the studio’s time clock.
n For those who are in the business of music, media production or the related
arts business, the equipment, building and utility payments can be written
off as a tax-deductible expense. Do some research and talk with a tax
advisor; there are deﬁnitely advantages to writing off both personal and
business studio deductions from your income tax.
n An individual artist or group might consider pre-producing a project in
their own studio allowing the time and expense billings to be a business
tax deduction.
n The same artists might consider recording part or all of their production at
their own project studio. The money saved (and deducted) could be later
spent on a producer, better mixdown facility, professional freelance engi -
neer, legal issues (such as copyright and contracts) . . . and let's not forget
marketing.
n The “signed artist/superstar approach” refers to the mega-artist who, instead
of blowing their advance royalties on lavish parties in a professional studio
(a sure way never to see any money from your hard work), will spend the
bucks on building their own professional-grade project studio. After the

The Project Studio
14
project has been recorded, the artist will still have a tax-deductible facility
that can be operated as a business enterprise. Then, when the next project
comes along, the artist will still have a personal facility where they can record,
while the saved advance bucks on the new project can be put in the bank.
n The name of all of these games are all about being wise and ﬁnancially
responsible.
THE PORTABLE STUDIO
Of course, as laptops have grown in power, it has become a simple matter to
load them with your favorite DAW software and audio interface, grab your
favorite mics and headphones, put the entire system in your backpack and hit
the road running. These systems (Figure 1.17) have actually gotten so powerful
that they equal and can sometimes surpass large, tower-based studio systems,
allowing you to record, edit, mix and produce on-the-go or in the studio without
any compromises whatsoever. In the bedroom, on the beach or at a remote
seaside island under battery or solar power—there are literally no limits to what
these ever-growing production systems can do.
FIGURE 1.17 
A portable studio can be set
up just about anywhere.
(Courtesy of Universal
Audio, www.uaudio.com 
© 2017 Universal Audio,
Inc. All rights reserved.
Used with permission)
To take these ever-shrinking analogies to the nth degree, newer handheld
recording systems can actually ﬁt in your pocket, allowing you to sample and
record sounds with professional results, using either their internal high-quality
mics or, in some cases, using external professional mics under phantom power.
Truly, it is a small world after all (Figure 1.18)!
FIGURE 1.18 
A portable handheld
recorder can go with you
anywhere to sample or
capture the moment.

The iRevolution
In recent years another studio revolution has taken place—the iOS revolution.
In fact, some of the greatest changes in audio production today are coming
about as a direct result of the introduction of devices like the iPad and other
iOS devices. Of course, the strengths of these systems are that they are extremely
portable, wireless and offer an ever-increasing amount of processing power. A
huge by-product of all this is overall cost-effectiveness. As an example, years
ago, a remote controller device for a DAW would set you back $1,200 or so—
now, it’s a simple matter of getting out your pad, downloading a controller app
from the “store” and you’ll have as many (or more) of the functions of its “wired”
hardware counterpart at a ridiculously small cost, or even for free (Figure 1.19).
15
Introduction  CHAPTER 1
FIGURE 1.19 
A pad can be used as a
wireless control surface for
a DAW in a way that
simulates the functions of a
larger controller at a meager
fraction of the cost.
As new apps (applications) come onto the market on a daily basis, the iOS
revolution continues to make its mark on all forms of media production.
Within the ﬁelds of music and audio production, a pad can be used for such
applications as:
n Audio recording and mixing
n DAW and live mixing
n Electronic instruments
n Systems controllers
n Compositional tools
Obviously, as these devices become more powerful, they can be used for any
number of on-the-go purposes that previously required a larger computer or
laptop . . . it is truly a technological revolution in the making.

Live/On-location Recording
16
LIVE/ON-LOCATION RECORDING: A DIFFERENT 
ANIMAL
Unlike the traditional multitrack recording environment, where overdubs are
often used to build up a song over time, live/on-location recordings are created
on the spot, in real time, often during a single on-stage or in-the-studio
performance, with little or no studio postproduction other than mixdown. 
A live recording might be very simple, possibly being recorded using only a 
few mics that are mixed directly to two or more tracks. Or, a more elabo-
rate gig might call for a full-ﬂedged multitrack setup, requiring the use of a
temporary control room or fully equipped mobile recording van or truck 
(Figure 1.20). A more involved setup will obviously require a great deal of
preparation and expertise, including a knowledge of sound reinforcement
combined with the live recording techniques that are necessary to capture
instruments in a manner that provides enough isolation between the tracks 
so as to yield control over the individual instruments during the mixdown 
phase.
Although the equipment and system setups will be familiar to any studio
engineer, live recording differs from its more controlled studio counterpart in
that it happens in a world where the motto is “you only get one chance.” When
you’re recording an event where the artist is spilling his or her guts to hundreds
or tens of thousands of fans, it’s critical for everything to run smoothly. Live
recording usually requires a unique degree of preparedness, redundancy, system
setup skills, patience and, above all, experience. It’s all about capturing the sound
and feel of the moment, the ﬁrst time—and for a brave few, that’s a very exciting
thing.
AUDIO FOR VIDEO AND FILM
In this day and age, it’s simply impossible to overlook the importance that
quality audio plays in the production of ﬁlm and video. With the introduction
of complex surround playback formats, high-budget music scores and special
effects production, audio-for-ﬁlm has long been an established and specialized
industry that has a dramatic effect on the movie-goer’s experience. Most
FIGURE 1.20 
Studio Metronome’s live
recording audio truck,
Brookline, NH. (Courtesy of
Metronome Media Group,
www.studiometronome.com,
Photo by Bennett Chandler)

17
Introduction  CHAPTER 1
deﬁnitely, audio-for ﬁlm is an art form that has touched and helped shape our
world culture (Figures 1.21 and 1.22).
Prior to the advent of the DVD, Blu-ray and home theater surround sound,
broadcast audio and the home theater experience was almost an afterthought
in a TV tube’s eye. However, with the introduction of these new technologies,
audio has matured to being a highly respected part of video and visual media
production. With the common use of surround sound in the creation of 
movie soundtracks, along with the growing popularity of surround sound in
home and computer entertainment systems, the public has come to expect high
levels of audio quality from their entertainment experience.
In modern-day production, MIDI, hard-disk recording, timecode and synchron -
ization, automated mixdown and advanced effects have become everyday
components of the audio-for-visual environment, requiring that professionals
be highly specialized and skilled in order to meet the demanding schedules and
production complexities.
AUDIO FOR GAMES
Most of the robot-zappin’, daredevil-ﬂyin’, hufﬂepuff-boppin’ addicts who are
reading this book are very aware that one of the largest and most lucrative areas
of multimedia audio production is the ﬁeld of scoring, designing and producing
FIGURE 1.21 
Skywalker Sound main
control room. (Courtesy of
Skywalker Sound a division
of Lucasfilm Ltd.,
www.skysound.com)
FIGURE 1.22 
Auditorium, Galaxy Studios,
Mol, Belgium. (Courtesy of
Galaxy Studios,
www.galaxy.be)

Audio for Games
18
audio for computer games—Zaaaaaaappppppppp! Like most subcategories
within audio production, this ﬁeld of expertise has its own set of technical and
scheduling requirements that center as much around spreadsheets and databases
as they do audio equipment. With the tens of thousands of voice and music cues
that are commonly required to make a game into a fully interactive experience,
an entirely different skillset is often required of a sound technician.
A rather interesting connection between orchestral recording and game audio
has been on the upsurge. Just as ﬁlm makes use of heavy orchestral scoring for
dramatic effect, game audio has also begun to score using big-budget and big-
name orchestras to create a bigger-than-life storyline.
THE DJ
The days when DJs would bring their records to a gig and spin are pretty much
gone (except for those who prefer the retro, hands-on way of working). Now,
on-stage systems are commonly made up of laptops, controllers and other gear
that has just as much in common with a project studio as it does with the stage.
In short, it’s not just a pair of turntables anymore. Actually, a DJ can range from
being someone who plays other people’s music, to one who creates, composes
and combines their own productions, and then combines these sounds with
the works of others.
A new development within the DJ community revolves around the use and
distribution of stems (individual groupings of instruments that can combine
together to make up a recording, or be used on their own to create new and
unique sub-mixes). This allows sub-groups of various projects to be combined
mixed and mutilated in new and different ways.
THE TIMES, THEY’VE ALREADY CHANGED: 
MULTIMEDIA AND THE WEB
With the integration of text, graphics, MIDI, digital audio and digitized video
into almost every facet of the personal computer environment, the ﬁeld of
multimedia audio has become a fast-growing, established industry that represents
an important and lucrative source of income for both creative individuals and
production facilities alike. Of course, the use of audio-for-the-web can take any
number of forms:
n A record label might decide to offer a new release in a download-only
format, requiring that the project be mastered in a way that best suits the
medium.
n An online language dictionary might require that all of the translations
be recorded, so they can be easily pronounced and heard in high quality.
n An online download site might offer any number of music loops that can
be downloaded and made into a personal remix that can be shared over
the web.

n A music sample library might make use of a major recording facility to
record instruments for a sampler plug-in.
n The list is absolutely endless!
For decades, the industry has been crying foul over the breakup of the traditional
record industry as we know it. In the early days of the web, a new kid on the
block came onto the scene . . . the MP3. This “ripping” and playback medium
made it possible for entire song libraries to be compressed (data-wise), uploaded,
downloaded and streamed with relative ease. Such a simple beastie then
progressed into a format that would bring an entire industry to its virtual knees.
With media sharing came the eventual revolution of the social network—
allowing people to connect with each other in totally new ways never before
thought possible. With the sharing of information, came the sharing of music
and visual media, allowing you to see that DMH is currently listening to Mouse
on Mars on his phone, whilst walking on the Oberbaum Brücke in Berlin. It’s
a brave, new world after all, and we’re all part of that big change!
POWER TO THE PEOPLE!
On a more personal and human front, with all of these amazing tools that are
at our disposal, it totally makes sense that artists, producers and aspiring
recording professionals will create art using the toys, tools and techniques that
are affordable. However, technology isn’t enough to create great art—a personal
sense of drive, passion and ingenuity is also required. One more ingredient is
also necessary to ﬁnish off this artistic “mix”, namely, a personal and never-
ending search for knowledge to improve your craft. This all-important ingredient
can be gained by:
n Reading about the equipment choices that are available to you on the
Web or in the numerous trade magazines that are available
n Visiting and talking with others of like (and dissimilar) minds about their
equipment, techniques and personal working styles (conventions, industry
organizations and social networks can be a really effective learning and
networking tool)
n Enrolling in a recording course that best ﬁts your needs, working style and
budget
n Researching the type of equipment and room design that best ﬁts your
needs and budget before you make any purchases and, if possible, getting
your hands on equipment before you make any ﬁnal purchases (i.e.,
checking them out at your favorite music store)
n Experience and time—always the best teacher
The more you take the time to familiarize yourself with the options and
possibilities that are available to you, the less likely you are to be unhappy
about how you’ve spent your hard-earned bucks after-the-fact. It is also important
to point out that having the right equipment for the job isn’t enough—it’s
19
Introduction  CHAPTER 1

Power to the People!
important to take the time to learn how to use your tools to their fullest potential.
Whenever possible read the manual and get your feet wet by taking the various
settings, functions and options for a test spin long before you’re under the time
and emotional constraints of being in a session.
Whatever Works for You
As you begin to research the various types of recording and supporting systems
that can be put to use in a project studio, you’ll ﬁnd that a wide variety of
options are available. There are indeed hundreds, if not thousands, of choices
for recording media, hardware types, software systems, speakers, effects devices—
the list goes on. This should automatically tell us that no one tool is right for
the job. As with everything in art (even the business of an art), there are many
personal choices that can be combined into a working system that’s right for
you. Whether you:
n Work with a DAW or tape-based system
n Choose to use analog or digital effects equipment (or both)
n Are a Mac or PC kind of person (pretty much a nonissue these days)
n Use this type of software or that
It all comes down to the bottom line of how does it sound? Does the music
move you? How does it move the audience? How can it be sold? In truth, no
prospective buyer will turn down a song because it wasn’t recorded on such-n-
such a machine, at such-and-such sample rate, using speakers made by so-n-
so—it’s the feel, baby. It’s the emotion and the art that always seals the deal in
the end.
THE PEOPLE WHO MAKE IT ALL HAPPEN
“ One of the most satisfying things about being in the professional audio [and
music] industry is the sense that you are part of a community.”
Frank Wells, editor, Pro Sound News
When you get right down to the important stuff, the recording ﬁeld is built
around pools of talented individuals and service industries who work together
toward a common goal: producing, selling and enjoying music. As such, it’s the
people in the recording industry who make the business of music happen.
Recording studios and other businesses in the industry aren’t only known for
the equipment that they have, but are more often judged by the quality, know -
ledge, vision and combined personalities of their staff. The following sections
describe but a few of the ways in which a person can be involved in this
multifaceted industry. In reality, the types and descriptions of a job in this
techno-artistic industry are limited only by the imagination. New ways of
expressing a passion for music production and sales are being created every day
and if you see a new opportunity, the best way to make it happen is to roll up
your sleeves and “just do it.”
20

The Artist
The strength of a recorded performance begins and ends with the artist. All of
the technology in the world is of little use without the existence of the central
ingredients of human creativity, emotion and individual technique. Just as the
overall sonic quality of a recording is no better than its weakest link, it’s the
performer’s job to see that music’s main ingredient—its inner soul—is laid out
for all to experience and hear. After all is said and done, a carefully planned
and well-produced recording project is simply a gilded framework for the music’s
original drive, intention and emotion.
Studio Musicians and Arrangers
A project will often require additional musicians to add extra spice and depth
to the artist’s recorded performance. For example:
n An entire group of studio musicians might be called on to provide the
best possible musical support for a high-proﬁle artist or vocalist.
n A project might require musical ensembles (such as a choir, string section
or background vocals) for a particular part or to give a piece a fuller sound.
n If a large ensemble is required, it might be necessary to call in a professional
music contractor to coordinate all of the musicians and make the ﬁnancial
arrangements. The project might also require a music arranger, who can
notate and possibly conduct the various musical parts.
n A member of a group might not be available or be up to the overall musical
standards that are required by a project. In such situations, it’s not
uncommon for a replacement studio musician to be called in to ﬁt the
bill.
In situations like these, a project that’s been recorded in a private studio might
beneﬁt from the expertise of a professional studio that has a larger recording
room, an analog multitrack for that certain sound and/or an engineer that knows
how to better deal with a complicated situation.
The Producer
Beyond the scheduling and budgetary aspects of coordinating a recording project,
it’s the job of a producer to help the artist and record company create the best
possible recorded performance and ﬁnal product that reﬂects the artist’s vision.
A producer can be hired for a project to fulﬁll a number of speciﬁc duties or
might be given full, creative reign to help with any and all parts of the creative
and business side of the process to get the project out to the buying public.
More likely, however, a producer will act collaboratively with an artist or group
to guide them through the recording process to get the best possible ﬁnal
product. This type of producer will often:
n Help the artist (and/or record label) create the best possible recorded
performance and ﬁnal product that reﬂects the artist’s vision. This will
21
Introduction  CHAPTER 1

The People Who Make It All Happen
often include a large dose of musical input, creative insight and mastery
of the recording process
n Assist in the selection of songs
n Help to focus the artistic goals and performance in a way that best conveys
the music to the targeted audience
n Help to translate that performance into a ﬁnal, salable product (with the
technical and artistic help of an engineer and mastering engineer)
It’s interesting to note that because engineers spend much of their working time
with musicians and industry professionals with the intention of making their
clients sound good, it’s not uncommon for an engineer to take on the role of
producer or co-producer (by default or by mutual agreement). Conversely, as
producers and artists alike become increasingly more knowledgeable about
recording technology, it’s increasingly common to ﬁnd them on the other side
of the glass, sitting behind the controls of a console.
Additionally, a producer might also be chosen for his or her ability to understand
the process of selling a ﬁnal recorded project from a business perspective to a
label, ﬁlm licensing entity or to the buying public. This type of producer can
help the artist gain insights into the world of business, business law, budgeting
and sales, always an important ingredient in the process.
Of course, in certain circumstances, a project producer might be chosen for his
or her reputation alone and/or for giving a certain cachet to a project that can
help put a personal “brand” on the project, thereby adding to the project’s stature
and hopefully help grab the public’s attention.
One ﬁnal thing is for certain, the artist and/or label should take time to study
what type of outside producer is needed (if any) and then agree upon his or
her creative and ﬁnancial role in the project before entering into the creative
process.
The Engineer
The role of an engineer can best be described as an interpreter in a techno-
artistic ﬁeld. He or she must be able to express the artist’s music and the
producer’s concepts and intent through the medium of recording technology.
This job is actually best classiﬁed as a techno-art form, because both music and
the recording process itself are totally subjective and artistic in nature and rely
on the tastes, experiences and feelings of those involved. During a recording
session, one or more engineers can be used on a project to:
n Conceptualize the best technological approach for capturing a performance
or music experience
n Translate the needs and desires of the artists and producer into a techno -
logical approach for capturing the music
n Document the process for other engineers or future production use
22

n Place the musicians in the desired studio positions
n Choose and place the microphones or pickup connections
n Set levels and balances on the recording console or DAW mixing interface
n Capture the performance (onto hard disk or tape) in the best way possible
n Overdub additional musical parts into the session that might be needed
at a later time
n Mix the project into a ﬁnal master recording in any number of media
formats (mono, stereo and surround sound)
n Help in meeting the needs for archiving and/or storing the project
n Last, but not least, be helpful, understanding and supportive in a way that
can put those (who are in a stressful situation) at ease
In short, engineers use their talent and artful knowledge of recording media
technology to convey the best possible ﬁnished sound for the intended media,
the client and the buying public.
Assistant Engineer
Many studios often train future engineers (or build up a low-wage staff) by
allowing them to work as assistants or interns who can offer help to staff and
visiting freelance engineers. The assistant engineer might do microphone and
headphone setups, run DAW or tape machine operations, help with session
documentation, do session breakdowns and (in certain cases) perform rough
mixes and balance settings for the engineer on the console. With the proliferation
of freelance engineers (engineers who are not employed by the studio, but are
retained by the artist, producer or record company to work on a particular
project), the role of the assistant engineer has become even more important.
It’s often his or her role to guide freelance engineers through the technical aspects
and quirks that are peculiar to the studio, and to generally babysit the technical
and physical aspects of the place.
Traditionally, this has been a no- or low-wage job that can expose a “newbie”
to a wide range of experiences and situations. With hard work and luck, many
assistants have worked their way into the hot seat whenever an engineer quits
or is unexpectedly ill. As in life, there are no guarantees in this position—you
just never know what surprises are waiting around the next corner for those
who rise to the occasion.
Maintenance Engineer
The maintenance engineer’s job is to see that the equipment in the studio is
maintained in top condition and regularly aligned and repaired when necessary.
Of course, with the proliferation of project studios, cheaper mass-produced
equipment, shrinking project budgets and smaller staffs, most studios will not
have a maintenance engineer on staff. Larger organizations (those with more
than one studio) might employ a full-time staff maintenance engineer, whereas
23
Introduction  CHAPTER 1

The People Who Make It All Happen
outside freelance maintenance engineers and technical service companies are
often called in to service smaller commercial studios in both major and non-
major markets.
Mastering Engineer
Often a ﬁnal master recording will need to be tweaked in terms of level,
equalization (EQ) and dynamics so as to present the ﬁnal “master” recording
in the best possible sonic and marketable light. If the project calls for it, this
job will fall to a mastering engineer, whose job it is to listen to and process the
recording in a specialized, ﬁne-tuned monitoring environment. Of course,
mastering is a techno-artistic ﬁeld in its own right. Beauty is deﬁnitely in the
ear of the beholding client and one mastering engineer might easily have a
completely different approach to the sound and overall feel to a project than
the next dude or dudette. However, make no mistake about it—the mastering
of a project can have a profound impact on the ﬁnal sound of a project, and
the task of ﬁnding the right mastering engineer for the job should never be
taken lightly. Further info on mastering can be found in Chapter 20.
Studio Management
Running a business in the ﬁeld of music and audio production requires the
special talents of businesspeople who are knowledgeable about the inner
workings of promotion, the music studio, the music business and, above all,
the people. It requires constant attention to quirky details that would probably
be totally foreign to someone outside “the biz.” Studio management tasks
include:
n Management: The studio manager (who might or might not be the owner)
is responsible for managerial and marketing decisions for all of the inner
workings of the facility and its business.
n Bookings: This staff person keeps track of most of the details relating to
studio usage and billing.
n Competent administration staff: These assistants keep everyone happy and
everything running as smoothly as possible.
Note, however, that some or all of these functions often vary from studio to
studio. These and other equally important staff are necessary in order to
successfully operate a commercial production facility on a day-to-day basis.
Music Law
It’s never good for an artist, band or production facility to underestimate the
importance of a music lawyer. When entering into important business
relationships, it’s always a good idea to have a professional ally who can help
you, your band or your company navigate the potentially treacherous waters of
a poorly or vaguely written contract. Such a professional can serve a wide range
of purposes, ranging from the primary duties of looking after their clients’
24

25
Introduction  CHAPTER 1
interests and ensuring that they don’t sign their careers away by entering into
a life of indentured, nonproﬁt servitude, all the way to introducing an artist to
the best possible music label or distribution network.
Music lawyers, like many in this business, can be involved in the working of a
business or career in many ways; hence, various fee scales are used. For example,
a new artist might meet up with a friend who knows about a bright, young, 
freshly-graduated music lawyer who has just passed the bar exam. By developing
a relationship early on, there are any number of potential opportunities for
building trust and making special deals that are beneﬁcial to both parties, etc.
On the other hand, a more established lawyer could help solicit and shop a song,
band or artist more effectively within a major music, TV or ﬁlm market. As with
most facets of the biz, answers to these questions are often situational and require
intuition, careful reference checking and the building of trust over time. Again,
it’s important to remember that a good music lawyer can be extremely important
(at the right moment) and is the unsung hero of many a successful career.
Women and Minorities in the Industry
Ever since its inception, males have dominated the recording industry. I
remember many sessions in which the only women on the scene were female
artists, secretaries or studio groupies in short dresses. Fortunately, over the years,
women have begun to play a more prominent role, both in front of and behind
the glass, and in every facet of studio production and the business of music
(Figure 1.23). Fortunately, in recent decades, most of the resistance to including
new and fresh blood based on gender, race or sexual orientation into the
business has greatly reduced. In the end, the most important thing that you can
do to make it in “the biz” is to be sincere and simply be yourself.
FIGURE 1.23 
Women’s Audio Mission, 
an organization formed to
assist women in the
industry. (Courtesy of the
Women’s Audio Mission,
www.womensaudio
mission.org)
Behind the Scenes
In addition to the positions listed earlier, there
are scores of professionals who serve as a
backbone for keeping the business of music
alive and func tioning. Without the many
different facets that con tribute to the making
of the music business, the biz would be very,
very different. A small sampling of the addi tional
pro fessional ﬁelds that help make it happen
includes:
No matter who you are,
where you’re from or what your
race, gender, sexual or planetary
orientation is, remember this universal truth: 
If your heart is in it and you’re willing to work
hard enough, you’ll make it (whatever 
you perceive “ it”  to be). Don’t let 
them tell you (or tell yourself)
otherwise.

n Artist management
n Artist booking agents
n A&R (artist and repertoire)
n Equipment design
n Equipment manufacturing
n Music and print publishing
n Distribution
n Web development
n Graphic arts and layout
n Audio company marketing
n Studio management
n Live sound
n Live sound tour management
n Acoustics
n Audio instruction
n Club management
n Sound system installation for nightclubs, airports, homes, etc.
n . . . and a whole lot more!
This incomplete listing serves as a reminder that the business of making music
is full of diverse possibilities and extends far beyond the notion that in order to
make it in the biz you’ll have to sell your soul or be someone you’re not. 
In short, there are many paths that can be taken in this techno-artistic business.
Once you’ve found the one that best suits your own personal style, you can then
begin the lifelong task of gaining knowledge, experience and pulling together
an interactive network with those who are currently working in the ﬁeld.
The People Who Make It All Happen
26
It’s also important to realize that ﬁnding the career
niche that’s right for you might not happen
overnight. You might try your hand at one aspect of
production, only to ﬁnd that your passion is in
another ﬁeld. This isn’t a bad thing. As the saying
goes, “Wherever you may be, there you are!” Finding
the path that’s best for you is a lifelong on-going
quest; the general idea is to work hard, learn and
enjoy the ride.
CAREER DEVELOPMENT
It’s a sure bet that those who are interested in getting into the business of audio
will quickly ﬁnd out that it can be a tough nut to crack. For every person who
makes it, a large number won’t. In short, there are a lot of people who are
waiting in line to get into what is perceived by many to be a glamorous biz.
So, how do you get to the front of the line? Well, folks, here’s the key:

Self-Motivation
The business of art (the techno-art of recording and music being no exception)
is one that’s generally reserved for self-starters and self-motivated people. Even
if you get a degree from XYZ college or recording school, there’s absolutely no
guarantee that your dream studio will be knocking on the door with an offer
in hand (in fact, they most certainly won’t). It takes a large dose of perseverance,
talent and personality to make it.
This may sound strange, but one of the best ways to get into the biz is to simply
jump in and start. In fact, you might try this little trick . . . ﬁnd a stick (or one
of those Scottish swords, if you have one) and get down on one knee, then
“knight” yourself on the shoulder with the ﬁgurative “sword” and say: “I am
now a ____________ !” (Fill in the blank with whatever you want to be—
engineer, artist, producer . . . whatever) and “arise Sir or Dame ____________ 
. . . you are now a ____________!” Simply become it . . . right there on the spot!
Now, make up a business card, start a business and begin contacting artists to
work with (or make the ﬁrst step toward becoming the creative person you want
to be). All you have to do is work hard, believe in yourself and follow the
Golden Rule.
In fact, there are many ways to get to the top of your own personal mountain.
For example, you could get a diploma from a school of education or from the
school of hard knocks (it usually ends up being from a bit of both) but the
goals and the paths are up to you.
Networking: “Showing Up Is Huge”
The other half of the success equation lies in your ability to network with other
people. Like the venerable expression says: “It’s not [only] what you
know, it’s who you know.” Maybe you have an uncle or a
friend in the business or a friend who has an uncle—
you just never know where help might come from
next. This idea of getting to know someone who
knows someone else is what makes the busi-
ness world go around. So, don’t be afraid to
put your best face forward and start meeting
people. If you want to work at XYZ Studios,
hang out without being in the way. You never
know, the engineer might need some help or might
know someone who can help get you into the proverbial
door. The longer you stick with it, the more people you’ll meet,
and you’ll have a bigger and stronger network than you ever thought could be
possible.
A friend of mine recently added to the above proverb “but you have to be ready!”
If you’re at the right place, at the right time and you’re not ready to step up to
the plate then it’s all been for naught.
27
Introduction  CHAPTER 1
ANCIENT PROVERB
Being “ in the right place at the 
right time”  means being in the wrong 
(or right) place at the wrong time a thousand
times! In short, “ Showing up is
HUGE” !

Career Development
28
So what are some good ways to get started?
n Join an industry association such as The Recording Academy (Grammys),
Grammy U (for students), Audio Engineering Society (AES), etc.
n Attend conventions and industry business functions (both nationally and
in your area)
n Visit a favorite studio and get to know them (and make it easy for them
to get to know you)
n Online social networking
Of course, I’ve been assuming that you want to get into the production side of
the recording business. But for those who just want to learn the tools and toys
of recording technology from an artist standpoint, the above networking tools
apply even more for those who want and need to get your names out to the
music-consuming public. For business professionals, networking is essential—
for the artist, it’s the driving force of your life.
So, when do you start this grand adventure? When do you start building 
your career? The obvious answer is RIGHT NOW. If you’re in school, you have
already started the process. If you’re just hanging out with like-minded biz 
folks and or joined a local or national organization that, too, is an equally
strong start. Whatever you do, don’t wait until you graduate or until some magic
date in the future, because putting it off will just put you that much further
behind.
In addition to the above, make yourself visible. Try not to be afraid when sending
out a resume, demo link or CD of your work or when asking for a job. The
worst thing they can do is say “No.” You might also keep in mind that “No”
could actually mean “No, not right now.” You might actually ask if this is the
case. If so, they might take your persistence into account before saying “No”
two or three times. By picking a market and particular area, blanketing that 
area with resume/press kits and knocking on doors, you just never know what
might happen. I know it’s not easy, and if you fail, simply but pick yourself up
(again), reevaluate your strategies and start pounding the streets (again). Just
remember the self-motivation rule, “failing at something isn’t a bad thing—not
trying is!”
Here are a few additional networking and job placement tips to get you started:
n Make a Facebook or personal Web page (Wordpress is an easy, free and
powerful way to get started).
n Send out lots of resumes or, better yet, make an online bio/resume link
on your page.
n Choose a mentor who you can rely on and talk to (sometimes they fall
out of the sky, sometimes you have to develop the relationship over time).
n Pick the areas you want to live in (if that’s a key factor).
n Pick the companies in that area and target them.

n Contact studios or companies in this area that might be looking for interns.
n Visit these places, just to hang out and see what they are like.
n Use your school counselors for intern placement.
n Always remember to follow up at least once, usually more.
THE RECORDING PROCESS
In this age of pro studios, project studios, digital audio workstations, groove
tools and personal choices, it’s easy to understand how the “different strokes
for different folks” adage equally applies to recording, as the artistic and
technological process can be approached in many different ways. The cost-
effective environment of the project studio has also brought music and audio
production to a much wider audience, thus making the process much more
personal. If we momentarily set aside the monumental process of creating music
in its various styles and forms, the process of capturing sound onto a recorded
medium will generally occur in seven distinct steps:
1. Preparation
2. Recording
3. Overdubbing
4. Mixdown
5. Mastering and song sequence editing
6. Product manufacturing
7. Marketing and sales
1 Preparation
One of the most important aspects of the recording process occurs before the
artist and production team step into any type of studio: preparation. Questions
like the following must be addressed long before you start:
n What is the goal?
n What is the budget?
n What are the estimated studio costs?
n Will there be enough time to work on tracking vocals, mixing, and other
important issues before running out of time or money?
n Will the project be released as physical media or download only?
n How much will it cost to manufacture the CDs and/or records (if any)?
n How will the music be distributed and sold? And to whom?
n Who will do the artwork and what is that budget?
n What will the advertising costs be?
n Is the group practiced enough?
29
Introduction  CHAPTER 1

The Recording Process
n If the project doesn’t have a producer, who will speak for the group when
the going gets rough?
n Are the instruments, voices and attitudes ready for the task ahead?
n Will additional musicians be needed?
n Are there any legal issues to consider (an important question for any
project)?
n How and when will the website be made and up & running?
These questions and a whole lot more will have to be addressed before it comes
time to press the big red record button.
THE POWER OF PREPARATION
Whenever a producer is asked about the importance of preparation, more often
than not they’ll most likely place preparation at or near the top of the list for
capturing a project’s sound, feel and performance. Having a plan (both musically
and technically) to get the job done in a timely fashion, with the highest degree
of artistic motivation and technical preparedness will almost certainly get the
project off to a good start. Details like making sure that the instruments are
tuned and in top form are also extremely important to take care of before the
red light goes on. Making sure that the musicians are well practiced, relaxed
and rested doesn’t hurt either, and don’t forget to have water, fruit and food
on hand to keep everyone at their best. You’d be surprised how the little things
can add to the success of a project—preparation, dude!
Taking the time for the band, artist and producer to meet up with the engineering
staff and to visit the studio beforehand can also help make the project successful.
Assuming that you’ve taken the time to ﬁnd the right studio and staff that will
work best with your musical style and project needs, taking the time to
familiarize the staff with your project can help everyone be as prepared as
possible, while having the added beneﬁt that everyone’s familiar with each other
and a bit more at ease.
2 Recording
It’s the engineer’s job to help capture a project’s sound to DAW or tape during
the recording phase. Recording the best possible sound (both musically and
technically) with a clear vision as to the musical, technical and time needs will
deﬁnitely start the project off on the right track.
In the recording phase, one or more sound sources are picked up by a micro -
phone or directly captured as an electrical signal, which is then recorded to a
track or series of tracks. Of course, hard-disk and multitrack recording tech nologies
have the ﬂexibility of allowing multiple sound sources to be captured onto and
played back from separate tracks in a disk- or tape-based environment. Because
the recorded tracks are isolated from each other, any number of instruments 
can be recorded and re-recorded without affecting the other instruments (with
30

disk-based DAWs offering an almost unlimited track count and analog tape counts
usually being offered in track groups of eight such as 8, 16 & 24). Of course, the
biggest advantage to having individual tracks is that they can be altered, added,
combined and edited at any time and in any order to best suit the production.
By recording a single instrument to a dedicated track (or group of tracks), it’s
possible to vary the level, spatial positioning (such as left/right or surround
panning), EQ and signal processing and routing in mixdown without affecting
the level or tonal qualities of other instruments that are recorded onto an
adjacent track or tracks (Figure 1.24). This isolation allows leakage from nearby
instruments or mic pickups to be reduced to such an insigniﬁcant level that
individual tracks can be rerecorded and/or processed at a later time, without
affecting the overall mix.
31
Introduction  CHAPTER 1
FIGURE 1.24 
Basic representation of how
isolated sound sources can
be recorded to a DAW or
multitrack recorder.
The basic tracks of a session can, of course, be built up in any number of ways.
For example, the foundation of a session might be recorded “all at once” in a
traditional fashion, or it can be built up using basic tracks that involve such
acoustic instruments as drums, guitar, piano and a scratch vocal (used as a rough
guide throughout the session until the ﬁnal vocals can be laid down).
Alternatively, these basic tracks might be made up of basic electronic music
loops, samples or MIDI tracks that have been composed using a digital audio
workstation (DAW). The various combinations of working styles, studio miking,
isolation and instrument arrangements are practically limitless and, whenever
possible, are best discussed or worked out in the early preparation stages.
From a technical standpoint, the microphones for each instrument are selected
either by experience or by experimentation and are then connected to the
desired console or audio interface inputs. Once done, the mic type and track
selection should be noted within the DAW track notepads and/or a track sheet
or piece of paper for easy input and track assignment in the studio or for
choosing the same mic/placement during a subsequent overdub session (taking

The Recording Process
a photo or screenshot of any complicated setups and saving it within the session
directory can also save your butt at a later time).
Some engineers ﬁnd it convenient to standardize on a system that uses the same
console mic input and DAW/tape track number for an instrument type at every
session. For example, an engineer might consistently plug their favorite kick
drum mic into input number 1 and record it onto track 1, the snare mic onto
number 2, and so on. This way, the engineer instinctively knows which track
belongs to a particular instrument without having to think too much about it.
When recording to a DAW, track names, groupings and favorite identifying track
colors can also be selected so as to easily identify the instrument or grouped
type.
Once the instruments, bafﬂes (an optional sound-isolating panel) and mics have
been roughly placed and headphones that are equipped with enough extra cord
to allow free movement have been distributed to each player, the engineer can
now get down to the business of labeling each input strip on the console with
the name of the corresponding instrument.
At this time, the mic/line channels can be assigned to their respective tracks.
Make sure you fully document the assignments and other session info on the
song or project’s track sheet (Figure 1.25). If a DAW is used, make sure each
input track in the session is properly named for easy reference and track
identiﬁcation.
If the console is digital in nature, labeling tracks might be as simple as naming
the track on the DAW which will then be shown on the console’s channel readout
display. If you’re working in the analog world, label strips (which are often
provided just below each channel input fader) can be marked with an erasable
felt marker, or the age-old tactic of rolling out and marking a piece of paper
masking tape could be used. (Ideally, the tape should be the type that doesn’t
leave a tacky residue on the console surface.)
32
FIGURE 1.25 
Studio track sheets. 
(a) Capitol Records.
(Courtesy of Capitol
Records, www.capitol
records.com) (b) John
Vanderslice. (Courtesy of
John Vanderslice and
Tiny Telephone,
www.johnvanderslice.com)

33
Introduction  CHAPTER 1
After all of the assignments and labeling have been completed, the engineer
then can begin the process of setting levels for each instrument and mic input
by asking each musician to play solo or by asking for a complete run-through
of the song and listening one input at a time (using the solo function). By
placing each of the channel and master output faders at their unity (0 dB) setting
and starting with the EQ settings at the ﬂat position, the engineer can then
check each of the track meter readings and adjust the mic preamp gains to their
optimum level while listening for potential preamp overload. If necessary, a
gain pad can be inserted into the path in order to help eliminate distortion.
After these levels have been set, a rough headphone mix can be made so that
the musicians can hear themselves. Mic choice and/or placements can be
changed or EQ settings can be adjusted, if necessary, to obtain the best sound
for each instrument. Also dynamic limiting or compression could be carefully
inserted and adjusted for any channel that requires dynamic attention. It’s
important to keep in mind that it’s easier to change the dynamics of a track
later during mix-down (particularly if the session is being recorded digitally)
than to undo any changes that have been made during the recording phase.
Once this is done, the engineer and producer can again listen for tuning and
extraneous sounds (such as buzzes or hum from guitar ampliﬁers or squeaks
from drum pedals) and adjust or eliminate them. Soloing the individual tracks
can ease the process of selectively listening for such unwanted sounds and for
getting the best sound from an instrument without any listening to the other
tracks. Soloing a track will not distract the musicians, as the monitor feed won’t
be affected. If several mics are to be grouped into one or more tracks, the balance
between them should be carefully set at this time. When recording to a DAW,
organizing your individually recorded tracks into groups, folders, adding color
track identiﬁers, etc. while making session notes is a relatively easy matter.
After this procedure has been followed for all the instruments, the musicians
should do a couple of practice rundown songs so that the engineer and producer
can listen to how the instruments sound together before being recorded. 
(If disk or tape space isn’t a concern, you might strongly consider recording
these practice tracks, because they might turn out to be your best takes—you
just never know!) During the rundown, you might also consider soloing the
various instruments and instrument combinations as a ﬁnal check and then,
monitor all of the instruments together to hear how the combined tracks sound.
Careful changes in EQ can be made at this time, making sure to note these
changes in the DAW notepad or track sheet for future reference. These changes
should be made sparingly, because ﬁnal balance compensations are probably
better made during the ﬁnal mixdown phase.
While the song is being run down, the engineer can also make ﬁnal adjustments
to the recording levels and the headphone monitor mix. He or she can then
check the headphone mix either by putting on a pair of phones connected to
the cue system or by routing the mix to the monitor loudspeakers. If the
musicians can’t hear themselves properly, the mix should be changed to satisfy

The Recording Process
34
their monitoring needs (fortunately, this can usually be done without regard
to the recorded track levels themselves). If several cue systems are available,
multiple headphone mixes might need to be built up to satisfy those with
different balance needs. During a loud session, the musicians might ask you to
turn up their level (or the overall headphone mix), so they can hear themselves
above the ambient room leakage. It’s important to note that high sound-pressure
levels can cause the pitch of instruments to sound ﬂat, so musicians might have
trouble tuning or even singing with their headphones on. To avoid these
problems, tuning shouldn’t be done while listening through phones. The
musicians should play their instruments at levels that they’re accustomed to
and adjust their headphone levels accordingly. For example, they might put
only one cup over an ear, leaving the other ear free to hear the natural room
sound.
The importance of proper headphone levels and a
good cue balance can’t be stressed enough, as they
can either help or hinder a musician’s overall
performance. The same situation exists in the
control room with respect to high monitor-speaker
levels: Some instruments might sound out of tune,
even when they aren’t, and ear fatigue can easily
impair your ability to properly judge sounds and their
relative balance.
During the practice rundown, it’s also a good idea to ask the musician(s) to
play through the entire song so you’ll know where the breaks, bridges and any
other point of particular importance might be. Making notes and even writing
down or entering the timing numbers (into a DAW using session markers or
into an analog recorder’s auto-locator) can help speed up the process of ﬁnding
a section during a take or overdub. You can also pinpoint the loud sections at
this time, so as to avoid any overloads. If compression or limiting is used, you
might keep an ear open to ensure that the instruments don’t trigger an undue
amount of gain reduction (again, if the tracks are recorded digitally, you might
consider applying gain reduction later during mixdown.) Even though an
engineer might ask each musician to play as loudly as possible, they’ll often
play even louder when performing together. This fact may require further
changes in the mic preamp gain, recording level and compression/limiting
thresholds. Soloing each mic and listening for leakage can also help to check
for separation and leakage problems between the instruments. If necessary, the
relative positions of mics, instruments and bafﬂes can be changed one last time.
3 Overdubbing
Once the basic tracks have been laid down, additional instrument and/or vocal
parts can be added later in a process known as overdubbing. During this phase,
additional tracks are added by monitoring the previously recorded tracks (usually
over headphones) while simultaneously recording new, doubled or augmented

instruments and/or vocals onto one or more available tracks of a DAW or
recorder (Figure 1.26). During the overdub phase, individual parts are added
to an existing project until the song or soundtrack is complete. If the artist makes
a mistake, no problem! Simply recue the DAW or rewind the tape back to where
the instrument begins and repeat the process until you’ve captured the best
possible take. If a take goes almost perfectly except for a bad line or a few ﬂubbed
notes, it’s possible to go back and re-record the offending segment onto the
same or a different track in a process known as punching in. If the musician lays
down his or her part properly and the engineer dropped in and out of record
at the correct times (either manually or under automation), the listener won’t
even know that the part was recorded in multiple takes—such is the magic of
the recording process!
While recording onto the same track might make sense when an analog recorder
is used, a DAW, on the other hand, doesn’t have any real track limitations.
Therefore, it often makes sense to record the overdub onto another track and
then combine the edited tracks into a single track or set of separate grouped
tracks. If multiple takes are needed to get the best performance, it’s possible to
record the different takes to a set of new tracks either manually or under a looped
automation function. Once done, the engineer and producer can sit down and
pick the best parts from the various takes and combine them into a ﬁnal per -
formance in a process that’s known as “comping” (making a composite track).
In an overdub session, the same procedure is followed for mic selection, place -
ment, EQ and level, possibly as they occurred during the original recording session
(now you’re beginning to see the need for good documentation). If only one
instrument is to be overdubbed, the problem of having other instrument tracks
leak into the new track won’t exist. However, care should be taken to ensure that
the headphones aren’t too loud or are improperly placed on the artist’s head,
because excessive leakage from the headphone mix into the mic can occur.
Of course, the natural ambience of the session should be taken into account
during an overdub. If the original tracks were made from a natural, roomy
ensemble, it could be distracting to hear an added track that was obviously laid
down in a different (usually deader) room environment.
If an analog recorder is to be used, it should be placed in the master sync mode
to ensure that the previously recorded tracks will play back in sync from the
35
Introduction  CHAPTER 1
FIGURE 1.26 
Overdubbing allows
instruments and/or vocals 
to be added at a later time
to existing tracks on a
multitrack recording
medium.

The Recording Process
record head (see Chapter 5 for more info). This sync mode can be set either at
the recorder or using its auto-locator/remote control. Usually, the tape machine
can automatically switch between monitoring the source (signals being fed to
the recorder or console) and tape/sync (signals coming from the playback or
record/sync heads). When recording to a DAW or tape, the control room monitor
mix should prominently feature the instruments that are being recorded, so
mistakes can be easily heard. During the initial rundown, the headphone cue
mix can be adjusted to ﬁt the musician’s personal taste.
OPTIONS IN OVERDUBBING
At ﬁrst glance, the process of overdubbing seems pretty cut and dried—put a
mic out in the studio, put headphones on the performer and start recording.
In fact, the process can present as many unique challenges and opportunities
as the recording process. For starters, getting the right sound during the process
is extremely important. The “Good Principle” applies as much here as within
any phase. Not asking these and other questions can cause lots of headaches
later in the game:
n Is the performer in the right headspace to do the overdub?
n Is the instrument properly tuned to the track?
n Is the mic’s choice, placement and distance appropriate to the track?
n If an overdub is being made over a previous overdub track, do all of the
above placements and settings match the previous session (again, stressing
the need for pictures and documentation)?
One other option that’s available to the artist, producer and engineer during
an overdub is the wide number of pickup options that can be used in the session
to add effect and various mixdown options to the recording. I’m referring to
the various ways that an instrument can be recorded to additional, separate
tracks to add dramatic options during mixdown. For example:
n A standard close-mic pickup can be used to record the overdub.
n If the instrument is electric or electronic, it can be recorded “direct” to
capture the pure instrument sound.
n Another mic could be placed at a 6’ to 10’ distance from the instrument
(i.e., electric guitar amp) to get a fuller, more distant sound.
n It’s also possible to place an additional distant (stereo or even quad) pickup
mic setup further back in the room to capture the room’s overall acoustic/
reverberant sound.
n If the instrument is electronic and has a MIDI out jack, it’s always wise
to capture the performance to a MIDI track on the DAW.
So why go through all this additional work? Well, let’s say that, at a later time,
the mix engineer calls up your additional distant mics and adds it to the track
and the producer goes wild—she just loves it!—or the track gets picked up as
36

37
Introduction  CHAPTER 1
the soundtrack to a feature ﬁlm and needs to be remixed in surround. By placing
the distant room sounds in the rear speakers, the whole guitar sound just got
super-huge, in a way that ﬁlls out the track amazingly well. On the ﬂip-side,
let’s say that the producer likes the overdub, but decides that amp isn’t right
for the track at all—all you’d have to do is play the direct signal back through
that new Vox stack in the big studio (possibly with new a whole new set of
room mics, as well) and everybody’s super happy—or, you captured the MIDI
tracks from a synth, which made it possible to change the patch to get a whole
new sound, without having to re-perform the track. Are you starting to get the
idea that the name of the game is versatility in production and/or mixdown 
. . . if you’re prepared for it?
4 Mixdown
When all of the tracks of a project have been recorded, assembled and edited,
the time has come to mix the songs into their ﬁnal media form (Figure 1.27).
The mixdown process can occur in various ways by:
n Routing the tracks of an analog tape recorder (ATR) through an analog
console
n Routing the tracks of a DAW to an analog console and mixing in the analog
domain
n Mixing the tracks of a DAW directly within its own software mixer (mixing
“in-the-box”)
No matter which approach is taken, the mixing process is used to shape the
overall tone character and intention of the song or production with respect to:
n Relative level
n Spatial positioning (the physical placement of a sound within a stereo or
surround ﬁeld)
n Equalization (affecting the relative frequency balance of a track)
n Dynamics processing (altering the dynamic range of a track, group or
output bus to optimize levels or to alter the dynamics of a track so that
it “ﬁts” better within a mix)
FIGURE 1.27 
Very basic representation of
the mixdown process. (a)
Using an analog console
and ATR. (b) Mixing in-the-
box.

The Recording Process
n Effects processing (adding reverb-, delay- or pitch-related effects to a mix
in order to augment or alter the piece in a way that is natural, unnatural
or just plain interesting)
If a DAW is being used to create the ﬁnal mix “in-the-box,” the channel faders
can be adjusted for overall level, panning can be used and a basic array of effects
can be entered into the session mixer and, if available, a controller surface can
be used to facilitate the mix by giving you hands-on control.
The ﬁrst job at hand is to set up a rough mix of the song by adjusting the levels
and the spatial pan positions. The artist and/or producer then listens to this
mix and might ask the engineer to make speciﬁc changes. The instruments are
often soloed one by one or in groups, allowing for any necessary EQ changes
that might need to be made. The engineer, producer, and possibly the entire
group can then begin the cooperative process of “building” the mix into its ﬁnal
form. Compression and limiting can be used on individual instruments as
required, either to make them sound fuller, more consistent in level, or to prevent
them from overloading the mix when raised to the desired mix output level.
Once the mix begins to take shape, reverb and other effects types can be added
to shape and add ambience in order to give close-miked sounds a more “live,”
spacious feeling, as well as to help blend the instruments. Once a mix has been
made, it’s then possible for a DAW to virtually mix down (export or bounce)
the overall mix to a speciﬁed mono, stereo or surround sound ﬁle without the
need for an external hardware recording device. It’s also wise to note that various
versions of the mix can be saved to the session directory, allowing you to go
back to any preferred mix version at any time.
If an actual recording console is used in the process, it can be placed into the
mixdown mode (or each input module can be switched to the line or tape
position) and the fader label strips can be labeled with their respective
instrument names. Channel faders should start in the fully down (set at inﬁnity
∞), while the group and master output faders could be set to unity gain (0 dB),
with the monitor section being switched to feed the mixdown signal to the
appropriate speakers. At this point, the console’s automation features, if
available, could be used (if desired).
If the hardware recording console isn’t automation assisted, the fader settings
will obviously have to be changed during the mix in real time. This means that
the engineer will have to memorize the various fader moves (possibly noting
the transport counter to keep track of transition times). If more changes are
needed than the engineer can handle alone, the assistant, producer or artist
(who probably knows the transition times better than anyone) can help by
controlling certain faders or letting you know when a transition is coming up.
It’s usually best, however, if the producer is given as few tasks as possible, so
that he or she can concentrate fully on the music rather than the physical
mechanics of the mix. The engineer then listens to the mix from a technical
standpoint to detect any sounds or noises that shouldn’t be present. If noises
38

39
Introduction  CHAPTER 1
are recorded on tracks that aren’t used during a section of a song, these tracks
can be muted (or gated) until needed. After the engineer practices the song
enough to determine and learn all the changes, the mix can be recorded and
faded at the end. The engineer might not want to fade the song during mixdown,
because it will usually be performed after being transferred to a DAW (which
can perform a fade much more smoothly than even the smoothest hand). Of
course, if automation is available or if the mix is performed using a DAW, all
of these moves can be performed much more easily and with full repeatability
by using the software’s own mix automation.
It’s often important that the overall (average) levels between the various songs
be as consistent as possible. Although the overall levels between songs can be
smoothed out in the mastering process, it’s always wise to be aware of general
level and mix consistencies between songs in the project. In addition, it’s often
wise to monitor at a consistent and possibly moderate listening level. This is
due to the variations in our ear’s frequency response at different sound-pressure
levels, which could result in inconsistencies between song balances. Ideally, the
control room monitor level should be the same as might be heard at home,
over the radio or in the car (between 70 and 90 dB SPL), although certain music
styles will simply “want” to be listened to at higher levels. Once the ﬁnal mix
or completed project master is made, you’ll undoubtedly want to listen to the
mix over different speaker systems (ranging from the smallest to the biggest/
baddest and also from the crappiest to the most accurate you can ﬁnd). It’s
usually wise to run off a few copies for the producer and band members to
listen to in familiar environments (i.e., at home and in their cars). In addition,
the mix should be tested for mono-stereo/surround compatibility to see if any
changes in the overall balance needs to occur. If any changes in the overall
frequency or dynamic balance are needed, some degree of EQ and/or
compression might be added to the ﬁnal output mix bus (making sure these
processors are disabled in the version that goes to the mastering engineer, as
they can re-apply these processors with a greater degree of control and accuracy).
5 Mastering
Once done, the ﬁnal, edited mixdown of a project might be sent to a mastering
engineer (Figure 1.28), so that ﬁne-tuning adjustments can be made to the overall
recording with respect to:
n Relative level balancing between songs within the project
n Dynamic level (altering the dynamics of a song so as to maximize its level
for the intended media or to tighten up the dynamic balance, overall or
within certain frequency bands)
n Equalization
n Overall level
n Song sequence editing

The Recording Process
40
In essence, it’s the job of a qualiﬁed mastering engineer to smooth over any
level and spectral imbalances within a project and to present the ﬁnal, recorded
product in the best possible light for its intended media form. Commonly, the
producer and/or artist will be faced with the question of whether to hire a
qualiﬁed or well-known mastering engineer to put on the ﬁnishing touches, or
to master the project themselves into a ﬁnal product (using the producer’s,
engineer’s or artist’s talents). The question as to whether to master the project
yourself should be thoroughly discussed with the producer and artist in the
preplanning phase, allowing for any on-the-spot change of plans should you
require the services of a professional.
6 Product Manufacturing and/or Downloadable
Distribution
Last but never least in the production chain is the process of manufacturing the
master recording into a ﬁnal, salable product. Whether the ﬁnal product is a
compact disc or digital download, this process should be carefully overseen to
ensure that the ﬁnal product doesn’t compromise all of the blood, sweat, tears
and bucks that have gone into the creation of a project. These manufacturing
phases should be carefully scrutinized, checked and rechecked:
n Creating a manufacture master
n Art layout and printing
n Product packaging
Whenever possible, ask for a proof copy of the ﬁnal duplicated product and
artwork before it is mass produced. Receiving 1,000 or more copies of your hard-
earned project that has a ﬂaw is as bad as being handed an accordion in hell.
Considerations as to its download presence should also be taken into consid -
eration. Does it need to be mastered for downloadable media? How will the
cover and accompanying credit art be produced and presented? Further info on
these topics can found in Chapter 21.
FIGURE 1.28 
Future Disc Mastering,
McMinnville, OR. (Courtesy
of Future Disc Systems,
www.futurediscsystems.
com)

41
Introduction  CHAPTER 1
7 Marketing and Sales
Although this section is mentioned last, it is by far one of the most important
areas to be dealt with when contemplating the time, talent and ﬁnancial effort
involved in creating a recorded product. For starters, the following questions
(and more) should all be answered long before the Record button is pressed
and the ﬁrst downbeat is played:
n Who is the intended audience?
n Will the project be distributed by a record company, or will the band 
(or I) try to sell it?
n What should the ﬁnal product look and sound like?
n Will there be physical product or will it just be available as a download?
n Will we make a promotional video for YouTube?
n Does the group want the title track to get out to media scouts for use in
a movie or TV show?
n What’s my budget and how much is this going to cost me?
n How will it be marketed?
In this short section, I won’t even attempt to fully cover this extremely important
and complex topic, because these subjects have been fully discussed in a number
of well-crafted books and searchable online articles. Whenever possible, talk
with industry professionals (record labels, marketing specialists or simply search
the web for all the info you can ﬁnd). In short, this is the all-important last
phase where the needle hits the record. Getting the public to accept and then
buy your product is by far the hardest part of the business, often requiring the
help of those who are experienced in the school of industry hard knocks. This
is the ﬁeld where the brave go to bat and get things done under extremely difﬁcult
circumstances—but never forget to be careful along the way. As always, surround
yourself with good people and “caveat emptor.”


When we make a recording, in effect we’re actually capturing and storing sound
into a memory media so that an original event or generated signal can be re-
created at a later date. If we start with the idea that sound is actually a concept
that corresponds to the brain’s perception and interpretation of a physical
auditory stimulus, the study of sound can be divided into four areas:
n The basics of sound
n The characteristics of the ear
n How the ear is stimulated by sound
n The psychoacoustics of hearing
Before we delve into the basics of the nature of sound and hearing, however,
let’s take a moment to look at an important concept that’s central to music,
sound, electronics and the art of sound recording: the transducer. If any
conceptual tool can help you to understand the technological and human
underpinnings of sound, art and process of recording, this is probably it!
THE TRANSDUCER
Quite simply, a transducer is any mechanical or electrical device that changes
one form of energy into another corresponding form of energy. For example,
a guitar is a transducer in that it takes the vibrations of picked or strummed
strings (the medium), ampliﬁes them through a body of wood, and converts
these vibrations into corresponding sound-pressure waves, which are then
perceived as sound (Figure 2.1).
A microphone is another example of a transducer. Here, sound-pressure waves
(the medium) act on the mic’s diaphragm and are converted into corresponding
electrical voltages. The electrical signal from the microphone can then be
ampliﬁed (not a process of transduction because the medium stays in its electrical
form) and fed to a recording device. A recorder is a device that changes electrical
voltages into analogous magnetic ﬂux signals on magnetic tape or into
representative digital data that can be encoded onto tape, hard disk or other
43
CHAPTER 2
Sound and Hearing

The Transducer
44
type of digital media. On playback, the stored magnetic signals or digital data
are converted back to their original electrical form, ampliﬁed, mixed and then
fed to a speaker system. The speakers convert the electrical signal back into a
mechanical motion (by way of magnetic induction), which, in turn, recreates
the original air-pressure variations that were picked up by the microphone, and
. . . ta-da . . . we have a group of transducers that work together to produce sound!
As can be gathered from Table 2.1, transducers can be found practically
everywhere in the audio environment, and they tend to have two overriding
characteristics:
n It’s interesting to note that transducers (and the media they use) tend to
be the weakest link in any audio system chain. In general, this process of
changing the energy in one medium into a corresponding form of energy
in another medium can’t be accomplished perfectly (although hi-res digital
coding gets close). Noise, distortion and (often) coloration of the sound
are introduced to some degree and, unfortunately, these effects can only
be minimized, not eliminated.
n Differences in design are another major factor that can affect sound quality.
Even a slight design variation between two microphones, speaker systems,
digital audio converters, guitar pickups or any other transducer can cause
them to sound quite different. This is due to the fact that they’ve been
dreamt up by humans, each having their own ideas of what materials
should be used, what method of operation should be used and how it
will sound in the end.
These factors, combined with the complexity of music and acoustics, helps make
the ﬁeld of recording the subjective and personal art form that it is . . . both
artistically and technically.
It’s also interesting to note that fewer transducers are used in an all or largely
digital recording system. In this situation, the acoustic waveforms that are picked
up by a microphone are converted into electrical signals and then quickly
converted into digital form by an analog-to-digital (A/D) converter. The A/D
FIGURE 2.1 
The guitar and microphone
as transducers.

converter changes these continuous electrical waveforms into corresponding
discrete numeric values that represent the waveform’s instantaneous, analogous
voltage levels. Arguably, digital information has the distinct advantage over
analog in that data can be transferred between electrical, magnetic and optical
media with little or no degradation in quality. This is because the information
is stored in its original, discrete binary form, no transduction process is involved
(i.e., only the medium changes, while the data representing the actual inform -
ation stays in its original digital form). Does this mean that digital’s better? 
Not necessarily. It’s just another way of expressing sound through a medium,
which, in the end, is simply one of the many possible artistic and technological
choices in the making and recording of sound and music. Beauty is, indeed, in
the ear of the beholder.
Having introduced the basic concept of the transducer, we can now go about
the task of understanding the physical nature of sound and the basics of how
the ears change a physical phenomenon into a perceived sensory one (again, a
transducer in action). With this knowledge, we can discover how to use this science
to understand the subjective art forms of music, sound recording and production.
THE BASICS OF SOUND
Sound arrives at the ear in the form of periodic variations in atmospheric
pressure called sound-pressure waves. This is the same atmospheric pressure that’s
measured by the weather service; although, the changes in pressure heard 
by the ear are simply too small in magnitude and ﬂuctuate too rapidly to be
observed on a barometer. An analogy of how sound waves travel in air can be
demonstrated by bursting a balloon in a silent room. Before we stick it with a
pin, the molecular motion of the room’s atmosphere is at a normal resting
pressure. The pressure inside the balloon is much higher, though, and the
molecules are compressed much more tightly together (Figure 2.2a) like people
packed into a crowded subway car. When the balloon is popped—KAPOW!
(Figure 2.2b), the tightly compressed area under high pressure begins to exert
an outward force on their molecular neighbors in an effort to move toward
areas of lower pressure. When the neighboring set of molecules have been 
45
Sound and Hearing  CHAPTER 2
Transducer
From
To
Ear
Sound waves in air
Nerve impulses in the brain
Microphone
Sound waves in air
Electrical signals in wires
Analog record head
Electrical signals in wires
Magnetic ﬂux on tape
Analog playback head
Magnetic ﬂux on tape
Electrical signals in wires
Phonograph cartridge
Grooves cut in disk surface
Electrical signals in wires
Speaker
Electrical signals in wires
Sound waves in air
Table 2.1
Media Used by Transducers in the Studio to Transfer Energy

The Basics of Sound
46
com pressed, they will then exert an outward force on the next set of lower-
pressured neighbors in a continuing ongoing outward motion (Figure 2.2c) that
travels until the  pressure stabilizes and the molecules have used up all their
energy in the form of heat.
Likewise, as a vibrating mass (such as a guitar string, a person’s vocal chords
or a loudspeaker) moves outward from its normal resting state, it squeezes air
molecules into a compressed area, away from the sound source. This causes the
area being acted on to have a greater than normal atmospheric pressure, a process
called compression (Figure 2.3a). As the vibrating mass moves inward from its
normal resting state, an area with a lower-than-normal atmospheric pressure
will be created, in a process called rarefaction (Figure 2.3b). As the vibrating
body cycles through its inward and outward motions, areas of higher and lower
compression states are then generated. These areas of high and low pressure
will cause the oscillating wave to move outward from the sound source in the
same way that the compressed wave moved outward from the burst balloon.
It’s interesting (and important) to note that the molecules themselves don’t
move through air at the velocity of sound—only the sound wave itself moves
through the atmosphere in the form of high-pressure compression waves that
continue to push against areas of lower pressure (in an outward direction). This
outward pressure motion is known as wave propagation, which is the building
block of sound.
FIGURE 2.2 
Wave movement in air as it
moves away from its point
of origin. (a) An intact
balloon contains pressurized
air. (b) When the balloon is
popped, the compressed
molecules exert a force on
outer neighbors in an effort
to move to areas of lower
pressure. (c) The outer
neighbors then exert a force
on the next set of molecules
in an effort to move to areas
of lower pressure and the
process continues.
FIGURE 2.3 
Effects of a vibrating mass
on air molecules and their
propagation. (a)
Compression—air
molecules are forced
together to form a
compression wave. (b)
Rarefaction—as the
vibrating mass moves
inward, an area of lower
atmospheric pressure is
created.

Waveform Characteristics
A waveform is the graphic representation of a sound-pressure level or voltage
level as it moves through a medium over time. In short, a waveform lets us see
and explain the actual phenomenon of wave propagation in our physical
environment and will generally have the following fundamental characteristics:
n Amplitude
n Frequency
n Velocity
n Wavelength
n Phase
n Harmonic content
n Envelope
These characteristics allow one waveform to be distinguished from another. 
The most fundamental of these are amplitude and frequency (Figure 2.4). The
following sections describe each of these characteristics. It’s important to note
that although several math formulas have been included, it is by no means
important that you memorize or worry about them. It’s far more important that
you grasp the basic principles of acoustics rather than fret over the underlying
math.
47
Sound and Hearing  CHAPTER 2
FIGURE 2.4 
Amplitude and frequency
ranges of human hearing.
AMPLITUDE
The distance above or below the centerline of a waveform (such as a pure sine
wave) represents the amplitude level of that signal. The greater the distance or
displacement from that centerline, the more intense the pressure variation,
electrical signal level or physical displacement will be within a medium. Wave -
form amplitudes can be measured in several ways (Figure 2.5). For example,
the measurement of either the maximum positive or negative signal level of a
wave is called its peak amplitude value (or peak level). The combined measurement

The Basics of Sound
48
of the positive and negative peak signal levels is called the peak-to-peak value.
The root-mean-square (rms) value was developed to determine a meaningful
average level of a waveform over time (one that more closely approximates the
level that’s actually perceived by our ears and gives a better real-world measure -
ment of overall signal amplitudes). The rms value of a sine wave can be
calculated by squaring the amplitudes at points along the waveform and then
taking the mathematical average of the combined results. The math isn’t as
important as the basic concept that the rms value of a perfect sine wave is equal
to 0.707 times its instantaneous peak amplitude level. Because the square of a
positive or negative value is always positive, the rms value will always be
positive. The following simple equations show the relationship between a
waveform’s peak and rms values:
FIGURE 2.5 
Graph of a sine wave
showing the various ways to
measure amplitude.
rms voltage = 0.707 × peak voltage
peak voltage = 1.414 × rms voltage
FREQUENCY
The rate at which an acoustic generator, electrical signal or vibrating mass
repeats within a cycle of positive and negative amplitude is known as the
frequency of that signal. As the rate of repeated vibration increases within a given
time period, the frequency (and thus the perceived pitch) will likewise increase
and vice versa. One completed excursion of a wave (which is plotted over the
360º axis of a circle) is known as a cycle (Figure 2.6). The number of cycles that
occur within a second (which determines the frequency) is measured in hertz
(Hz). The diagram in Figure 2.7 shows the value of a waveform as starting at
zero (0º). At time t = 0, this value increases to a positive maximum value and
then decreases negatively as it moves back towards its original zero point, where
the process begins all over again in a repetitive fashion. A cycle can begin at
any angular degree point on the waveform; however, to be complete, it must
pass through a single 360º rotation and end at the same point as its starting
value. For example, the waveform that starts at t = 0 and ends at t = 2 constitutes
a cycle, as does the waveform that begins at t = 1 and ends at t = 3.

49
Sound and Hearing  CHAPTER 2
FIGURE 2.6 
Cycle divided into the 360°
of a circle.
Frequency Response
One of the ways that we can “look” at how an audio device might sound is by
charting its output level over a frequency range using a visual rating called a
frequency response curve. This curve is used to graphically represent how a device
will respond to the audio spectrum (the 20 to 20,000 Hz range of human
hearing) and, thus, how it will affect a signal’s overall sound. This is done by
sending a reference tone that is equal level at all frequencies to the device under
test; any changes in level over frequency at the device’s output will give us an
accurate measure of any volume changes over frequency that the device will
exhibit.
As an example, Figure 2.8 shows the frequency response of several unidentiﬁed
devices. In these and all cases, the x-axis graphically represents frequency, while
the y-axis represents the device’s measured output signal. By feeding the input
of an electrical device with a constant-amplitude reference signal that sweeps
over the entire frequency spectrum, the results can then be charted on an
amplitude vs. frequency graph that can be easily read at a glance. If the measured
signal is the same level at all frequencies, the curve will be drawn as a ﬂat,
straight line from left to right (known as a ﬂat frequency response curve). This
indicates that the device passes all frequencies equally (with no frequency being
FIGURE 2.7 
Graph of waveform
amplitude over time.
Frequency Response 

The Basics of Sound
50
FIGURE 2.8 
Frequency response curves:
(a) curve showing a bass
boost; (b) curve showing a
boost at the upper end; (c)
curve showing a dip in the
midrange.
emphasized or de-emphasized). If the output lowers or increases at certain
frequencies, these changes will easily show up as dips or peaks in the chart.
Just as an electrical device (such as an ampliﬁer, digital audio circuit, etc.) can
be graphed over its input vs. output range, an electro-acoustic device (such as
a microphone, speaker, phono cartridge, etc.) can also be charted. This can be
done in several ways, for example: a high-quality calibration speaker can feed
equal-level test tones in a non-reverberant chamber to a microphone; any
changes in level at the mic’s output can be charted as its response curve. Likewise,
a high-quality calibration microphone can be placed in a non-reverberant
chamber to test the output of a speaker that is fed with equal-level tones over
the spectrum, which will give us a reference vs. test device output frequency
level that can be charted on the graph.
Before continuing on, it should be pointed out that care should be taken when
looking at graphs and all of the other types of specs that can be used to determine
the “quality” of a device. It’s important to keep in mind that sound is an art
form, and that some of the best-sounding and most sought-after devices will
often not stand up against modern-day specs (speciﬁcations). Indeed, the
numbers on the spec sheet will not and cannot tell the whole story. Devices that
“look” perfect on paper may sound dreadful and “dreadful-looking” devices just
might sound amazing. The moral of the story is to use your ears and the opinions
of others that you trust over (or in addition to) the numbers that are printed on
the spec sheet.
VELOCITY
The velocity of a sound wave as it travels through air at 68ºF (20ºC) is
approximately 1130 feet per second (ft/sec) or 344 meters per second (m/sec).
This speed is temperature dependent and increases at a rate of 1.1 ft/sec for
each Fahrenheit degree increase in temperature (2 ft/sec per Celsius degree).

WAVELENGTH
The wavelength of a waveform (frequently represented by the Greek letter lambda,
λ) is the physical distance in a medium between the beginning and the end of
a cycle. The physical length of a wave can be calculated using:
λ = V/f
where 
λ is the wavelength in the medium
V is the velocity in the medium
f is the frequency (in hertz)
The time it takes to complete 1 cycle is called the period of the wave. To illustrate,
a 30-Hz sound wave completes 30 cycles each second or 1 cycle every 1/30th
of a second. The period of the wave is expressed using the symbol T:
T = 1/f
where T is the number of seconds per cycle.
Assuming that sound propagates at the rate of 1130 ft/sec, all you need to do
is divide this ﬁgure by the desired frequency. For example, the simple math for
calculating the wavelength of a 30-Hz waveform would be 1130/30 = 37.6 feet
long, whereas a waveform having a frequency of 300-Hz would be 1130/300
= 3.76 feet long (Figure 2.9). Likewise, a 1000-Hz waveform would work out
as being 1130/1000 = 1.13 feet long, and a 10,000-Hz waveform would be
1130/10,000 = 0.113 feet long. From these calculations, you can see that
whenever the frequency is increased, the wavelength decreases.
51
Sound and Hearing  CHAPTER 2
FIGURE 2.9 
Wavelengths decrease in
length as frequency
increases (and vice versa).
Reﬂection of Sound
Much like a light wave, sound reﬂects off a surface boundary at an angle that’s
equal to (and in the opposite direction of) its initial angle of incidence. This
basic property is one of the cornerstones of the complex study of acoustics. 
For example, Figure 2.10a shows how a sound wave reﬂects off a solid smooth
surface in a simple and straightforward manner (at an equal and opposite
angle). Figure 2.10b shows how a convex surface will splay the sound outward
from its surface, radiating the sound outward in a wide dispersion pattern. 
Reflection of Sound 

The Basics of Sound
52
In Figure 2.10c, a concave surface is used to focus a sound inward toward a
single point, while a 90º corner (as shown in Figure 2.10d) reﬂects patterns
back at angles that are equal to their original incident direction. This holds true
for both the 90º corners of a wall and for intersections where the wall and ﬂoor
meet. These corner reﬂections help to provide insights into how volume levels
often build up in the corners of a room (particularly at bass frequencies at wall-
to-ﬂoor corner intersections).
FIGURE 2.10 
Incident sound waves
striking surfaces with
varying shapes: (a) single-
planed, solid, smooth
surface; (b) convex surface;
(c) concave surface; (d) 90°
corner reflection.
1. Get out your cell phone, handy, mobile or
whatever you call it in your country and begin
playing some music while holding it in the air.
How does it sound? . . . tinny and not that loud,
right?
2. Now, place the cell phone against a ﬂat wall. Did
the sound change? Did it get fuller, louder and
deeper (especially in the low-end)?
3. Now, place it on a ﬂat counter, against the wall.
How did that affect the sound? Did the bass just
get even deeper?
4. Now place it on the ﬂoor, in the corner of the
room. It just got louder and the bass boosted
even further, didn’t it?
5. This is a function of the “boundary effect”,
whereby the reﬂections coming from the wall and
corners combine together to make the overall
acoustic signal louder . . . and often more
pronounced in the low-end.
Try This: Reﬂection of Sound
D I Y
 do  it  yourself
Diffraction of Sound
Sound has the inherent ability to diffract around or through a physical acoustic
barrier. In other words, sound can bend around an object in a manner that
reconstructs the signal back into its original form in both frequency and
amplitude. For example, in Figure 2.11a, we can see how a small obstacle will
scarcely impede a larger acoustic waveform. Figure 2.11b shows how a larger
obstacle can obstruct a larger portion of the waveform; however, past the
Diffraction of Sound 

obstruction, the signal bends around the area in the barrier’s wake and begins
to reconstruct itself. Figure 2.11c shows how the signal is able to radiate through
an opening in a large barrier. Although the signal is greatly impeded (relative
to the size of the opening), it nevertheless begins to reconstruct itself in
wavelength and relative amplitude and begins to radiate outward as though it
were a new point of origin. Finally, Figure 2.11d shows how a large opening
in a barrier lets much of the waveform pass through relatively unimpeded.
PHASE
Because we know that a cycle can begin at any point on a waveform, it follows
that whenever two or more waveforms are involved in producing a sound, their
relative amplitudes can (and almost always will) be different at any one point
in time. For simplicity’s sake, let’s limit our example to two pure tone waveforms
(sine waves) that have equal amplitudes and frequency, but start their cyclic
periods at different times. Such waveforms are said to be out of phase with respect
to each other. Variations in phase, which are measured in degrees (º), can be
described as a time delay between two or more waveforms. These delays are
often said to have differences in relative phase degree angles (over the full
rotation of a cycle, e.g., 90º, 180º, 270º or any angle between 0º and 360º).
The sine wave (so named because its amplitude follows a trigonometric sine
function) is usually considered to begin at 0º with an amplitude of zero; the
waveform then increases to a positive maximum at 90º, decreases back to a
zero amplitude at 180º, increases to a negative maximum value at 270º, and
ﬁnally returns back to its original level at 360º, simply to begin all over again.
Whenever two or more waveforms arrive at a single acoustic location or are
electrically conducted through a cable out of phase, their relative signal levels
will be added together to create a combined amplitude level at that one point
in time. Whenever two waveforms having the same frequency, shape and peak
amplitude are completely in phase (meaning that they have no relative time
difference), the newly combined waveform will have the same frequency, phase
and shape, but will be double in amplitude (Figure 2.12a). If the same two
waves are combined completely out of phase (having a phase difference of 180º),
they will cancel each other out when added, resulting in a ﬁnal relative value
of zero amplitude (Figure 2.12b). If the second wave is only partially out of
phase (by a degree other than 180º), the levels will be added at points where
53
Sound and Hearing  CHAPTER 2
FIGURE 2.11 
The effects of obstacles on
sound radiation and
diffraction. (a) A small
obstacle will scarcely
impede a longer wavelength
signal. (b) A larger obstacle
will obstruct the signal to a
greater extent; the
waveform will also
reconstruct itself in the
barrier’s wake. (c) A small
opening in a barrier will
greatly impede a signal; the
waveform will emanate from
the opening and reconstruct
itself as a new source point.
(d) A larger opening allows
sound to pass unimpeded,
allowing it to quickly diffract
back into its original shape.

The Basics of Sound
54
the combined amplitudes are positive and will be reduced in level where the
combined results subtract from each other (Figure 2.12c).
Phase Shift
Phase shift is a term that describes one waveform’s lead or lag time with respect
to another. Basically, it results from a time delay between two (or more)
waveforms (with differences in acoustic distance being one form of this type
of delay). For example, a 500 Hz wave completes one cycle every 0.002 sec. If
you start with two in-phase, 500 Hz waves and delay one of them by 0.001 sec
(half the wave’s period), the delayed wave will lag the other by one-half a cycle,
or 180º (resulting in a combined-waveform cancellation). Another example
might include a single source that’s being picked up by two microphones that
have been placed at different distances (Figure 2.13), thereby creating a
corresponding time delay when the signals are mixed together. Such a delay
can also occur when a single microphone picks up direct sounds as well as
FIGURE 2.12 
Combining sine waves of
various phase relationships.
(a) The amplitudes of in-
phase waves increase in level
when mixed together. (b)
Waves of equal amplitude
cancel completely (no output)
when mixed 180º out of
phase. (c) When partial phase
angles are mixed, the signals
will increase and decrease
when acoustically or
electrically combined
together.
FIGURE 2.13 
Cancellations can occur
when a single source is
picked up by two
microphones that are mixed
together.
Phase Shift 

those that are reﬂected off of a nearby boundary (such as a ﬂoor or wall). These
signals will be in phase at frequencies where the path-length difference is equal
to the signal’s wavelength, and out of phase at those frequencies where the
multiples fall at or near the half-wavelength distance. In all the above situations,
these boosts and cancellations combine to alter the signal’s overall frequency
response at the pickup. For this and other reasons, you might want to keep
acoustic leakage between microphones and reﬂections from nearby boundaries
to a minimum whenever possible. It’s important to remember, however, that
there are no hard and fast rules. Some of the best engineers in the world make
use of controlled (and sometimes uncontrolled) leakage to add “life” to the
sound of a recording.
55
Sound and Hearing  CHAPTER 2
1. Go to the Tutorial section of www.modrec.com,
click on Phase Tutorial and download the 0º and
180º sound ﬁles.
2. Load the 0º ﬁle onto track 1 of the digital audio
workstation (DAW) of your choice, making sure to
place the ﬁle at the beginning of the track, with
the signal panned center.
3. Load the same 0º ﬁle again into track 2.
4. Load the 180º ﬁle into track 3.
5. Play tracks 1 and 2 (by muting track 3) and listen
to the results. The result should be a summed
signal that is 3 dB louder.
6. Play tracks 1 and 3 (by muting track 2) and listen
to the results. It should cancel, resulting in no
output.
7. Offsetting track 3 (relative to track 1) should
produce varying degrees of cancellation.
8. Feel free to zoom in on the waveforms, mix them
to a ﬁle and view the results. Interesting, huh?
Try This: Phase
D I Y
 do  it  yourself
HARMONIC CONTENT
Up to this point, our discussion has centered on the sine wave, which is
composed of a single frequency that produces a pure sound at a speciﬁc pitch.
Fortunately, musical instruments rarely produce pure sine waves. If they did,
all of the instruments would sound the same, and music would be pretty boring.
The factor that helps us differentiate between instrumental “voicings” is the
presence of frequencies (called partials) that exist in addition to the fundamental
pitch that’s being played. Partials that are higher than the fundamental frequency
are called upper partials or overtones. Overtone frequencies that are whole-number
multiples of the fundamental frequency are called harmonics. For example, the
frequency that corresponds to concert A is 440 Hz (Figure 2.14a). An 880 Hz
wave is a harmonic of the 440 Hz fundamental because it is twice the frequency

The Basics of Sound
56
(Figure 2.14b). In this case, the 440 Hz fundamental is technically the ﬁrst
harmonic because it is 1 times the fundamental frequency, and the 880 Hz
wave is called the second harmonic because it is 2 times the fundamental. 
The third harmonic would be 3 times 440 Hz, or 1320 Hz (Figure 2.14c).
Instruments, such as bells, xylophones and other percussion instruments, will
often contain overtone partials that aren’t harmonically related to the
fundamental at all.
The ear perceives frequencies that are whole, doubled multiples of the
fundamental as being related in a special way (a phenomenon known as the
musical octave). As concert A is 440 Hz (A4), the ear hears 880 Hz (A5) as being
the next highest frequency that sounds most like concert A. The next related
octave above that will be 1760 Hz (A6). Therefore, 880 Hz is said to be one
octave above 440 Hz, and 1760 Hz is said to be two octaves above 440 Hz,
etc. Because these frequencies are even multiples of the fundamental, they’re
known as even harmonics. Not surprisingly, frequencies that are odd multiples
of the fundamental are called odd harmonics. In general, even harmonics are
perceived as creating a sound that is pleasing to the ear, while odd harmonics
will often create a dissonant, harsher tone.
FIGURE 2.14 
An illustration of harmonics:
(a) 440Hz—first harmonic
“fundamental waveform”;
(b) 880Hz—second
harmonic; (c) 1320Hz—
third harmonic.
1. Go to the Tutorial section of www.modrec.com,
click on Harmonics Tutorial and download all of
the sound ﬁles.
2. Load the ﬁrst-harmonic A440 ﬁle onto track 1 of
the digital audio workstation (DAW) of your
choice, making sure to place the ﬁle at the
beginning of the track, with the signal panned
center.
3. Load the second-, third-, fourth- and ﬁfth-
harmonic ﬁles into the next set of consecutive
tracks.
4. Solo the ﬁrst-harmonic track, then solo the ﬁrst-
and second-harmonic tracks. Do they sound
related in nature?
5. Solo the ﬁrst-harmonic track, then solo the ﬁrst-
and third-harmonic tracks. Do they sound more
dissonant?
6
Solo the ﬁrst-, second- and fourth-harmonic
tracks. Do they sound related?
7. Solo the ﬁrst-, third- and ﬁfth-harmonic tracks.
Do they sound more dissonant?
Try This: Harmonics
D I Y
 do  it  yourself

Because musical instruments produce sound waves that contain harmonics with
various amplitude and phase relationships, the resulting waveforms bear little
resemblance to the shape of the single-frequency sine wave. Therefore, musical
waveforms can be divided into two categories: simple and complex. Square,
triangle and sawtooth waves are examples of simple waves that contain a
consistent harmonic structure (Figure 2.15). They are said to be simple because
they’re continuous and repetitive in nature. One cycle of a square wave looks
exactly like the next, and they are symmetrical about the zero line.
Complex waves, on the other hand, represent practically all other sounds that
are produced in music and nature. They almost never repeat and often are not
symmetrical about the zero line. An example of a complex waveform (Figure
2.16) is one that’s created by any naturally occurring sound (such as music or
speech). Although complex waves are rarely repetitive in nature, “all sounds”
can be mathematically broken down into a series of ever-changing combination
of individual sine waves (or re-synthesized from sine waves through a complex
process known as Fourier analysis).
57
Sound and Hearing  CHAPTER 2
Regardless of the shape or complexity of the waveform that reaches the eardrum,
the inner ear is able to perceive these component signals and then transmit the
stimulus to the brain. This can be illustrated by passing a square wave through
a bandpass ﬁlter that’s set to pass only a narrow band of frequencies at any one
time. Doing this would show that the square wave is composed of a fundamental
frequency plus a number of harmonics that are made up of odd-number multiple
frequencies (whose amplitudes decrease as the frequency increases). In Figure
2.17, we see how individual sine-wave harmonics can be combined together to
form a square wave.
If we were to analyze the harmonic content of sound waves that are produced
by a violin and compare them to the content of the waves that are produced
by a viola (with both playing concert A440 Hz), we might come up with results
FIGURE 2.15 
Simple waveforms: (a)
square waves; (b) triangle
waves; (c) sawtooth waves.
FIGURE 2.16 Example of
a complex waveform.

The Basics of Sound
58
Because the relative harmonic balance is so important to an instrument’s sound,
the frequency response of a microphone, ampliﬁer, speaker and all other
elements in the signal path can have an effect on the timbral (tonal) balance
of a sound. If the frequency response isn’t ﬂat, the timbre of the sound will be
changed. For example, if the high frequencies are ampliﬁed less than the low
and middle frequencies, then the sound will be duller than it should be. For
this reason, a choice of mic, equalizer or mic placement can be used as tools
to vary the timbre of an instrument, thereby changing its subjective sound.
FIGURE 2.17 
Breaking a square wave
down into its odd-harmonic
components: (a) square
wave with frequency f; (b)
sine wave with frequency f;
(c) sum of a sine wave with
frequency f and a lower
amplitude sine wave of
frequency 3f; (d) sum of a
sine wave of frequency f
and lower amplitude sine
waves of 3f and 5f, which is
beginning to resemble a
square wave.
FIGURE 2.18 
Harmonic structure of
concert A440: (a) played on
a viola; (b) played on a
violin.
similar to those shown in Figure 2.18. Notice that the violin’s harmonics differ
in both degree and intensity from those of the viola. These harmonics and their
relative intensities are extremely important, as they determine an instrument’s
characteristic sound (which is called the instrument’s timbre). If we changed an
instrument’s harmonic balance, the sonic character of that instrument would
also be changed. For example, if the violin’s upper harmonics were reduced,
the violin would sound more like a viola.
FIGURE 2.19 
Radiation patterns of a cello
over frequency as viewed
from the side (top) and from
overhead (bottom).

In addition to the variations in harmonic balance that can exist between
instruments and their families, it is common for this harmonic balance to vary
with respect to the direction that a sound wave radiates from an instrument.
Figure 2.19 shows the principal radiation patterns as they emanate from a cello
(as seen from both the side and top views).
ENVELOPE
Timbre isn’t the only characteristic that helps us differentiate between instru -
ments. Each one produces a sonic amplitude envelope that works in combination
with timbre to determine its unique and subjective sound. The envelope of an
acoustic or electronically generated waveform can be described as characteristic
variations in amplitude level that occur in time over the duration of a played
note. This envelope (ADSR) is composed of four sections:
n Attack (A) refers to the time taken for a sound to build up to its full volume
when a note is initially sounded
n Decay (D) refers to how quickly the sound levels off to a sustain level after
the initial attack peak
n Sustain (S) refers to the duration of the ongoing sound that’s generated
following the initial attack decay
n Release (R) relates to how quickly the sound will decay once the note is
released
Figure 2.20a illustrates the envelope of a trombone note. The attack, decay times
and internal dynamics produce a smooth, sustaining sound. A cymbal crash
(Figure 2.20b) combines a high-level, fast attack with a longer sustain and decay
that creates a smooth, lingering shimmer. Figure 2.20c illustrates the envelope
of a snare drum. Notice that the initial attack is much louder than the internal
dynamics, while the ﬁnal decay trails off very quickly, resulting in a sharp,
percussive sound.
59
Sound and Hearing  CHAPTER 2
FIGURE 2.20 
Various musical waveform
envelopes: (a) trombone; 
(b) cymbal crash; and (c)
snare drum, where 
A = attack, D = decay, 
S = sustain, and 
R = release.
It’s important to note that the concept of an envelope often relies on peak wave -
form values, while the human perception of loudness is proportional to the average
wave intensity over a period of time (rms value). Therefore, high-amplitude por -
tions of the envelope won’t make an instrument sound loud unless the amplitude
is maintained for a sustained period. Short high-amplitude sections (transients)

The Basics of Sound
60
tend to contribute to a sound’s overall character, rather than to its loudness. By
using a compressor or limiter, an instrument’s character can often be modiﬁed
by changing the dynamics of its envelope without changing its overall timbre.
LOUDNESS LEVELS: THE DECIBEL
The human ear operates over an energy range of approximately 1013:1
(10,000,000,000,000:1), which is an extremely wide range. Since it’s difﬁcult
for us to conceptualize number ranges that are this large, a logarithmic scale
has been adopted to compress the measurements into ﬁgures that are more
manageable. The unit used for measuring sound-pressure level (SPL), signal
level and relative changes in signal level is the decibel (dB), a term that literally
means 1/10th of a Bell (an older telegraph and telephone transmission loss
measur ement unit that was named after Alexander Graham Bell, inventor of
the telephone). In order to develop an understanding of the decibel, we ﬁrst
need to examine logarithms and the logarithmic scale (Figure 2.21). The
logarithm (log) is a mathematical function that reduces large numeric values into
smaller, more manageable numbers. Because logarithmic numbers increase
exponentially in a way that’s similar to how we perceive the doubling of loudness
levels (e.g., 1, 2, 4, 16, 128, 256, 65,536 . . .), it expresses our perceived sense
of volume more precisely than a linear curve can.
FIGURE 2.21 
Linear and logarithmic
curves: (a) linear; (b)
logarithmic.
Before we delve into a deeper study of this important concept and how it deals
with our perceptual senses, let’s take a moment to understand the basic concepts
and building block ideas behind the log scale, so as to get a better understanding
of what examples such as “+3 dB at 10,000 Hz” really mean. I know that this
can be a daunting subject to grasp, but be patient with yourself. Over time, the
concept of the decibel will become as much a part of your working vocabulary
as ounces, gallons and miles per hour (or grams, liters and kilometers).
Logarithmic Basics
In audio, we use logarithmic values to express the differences in intensities
between two levels (often, but not always, comparing a newly-measured level
to a standard reference level). Because the differences between these two levels
can be really, really big, a simpler system makes use of representative values

that are mathematical exponents of 10. To begin, ﬁnding the log of a number
such as 17,386 without a calculator is not only difﬁcult, it’s unnecessary! All
that’s really important to help you along are three simple guidelines:
n The log of the number 2 is 0.3.
n When a number is an integral power of 10 (e.g., 100, 1000, 10,000), the
log can be found simply by adding up the zeros in that number.
n Numbers that are greater than 1 will have a positive log value, while those
less than 1 will have a negative log value.
Again, the ﬁrst one is an easy fact to remember: The log of 2 is 0.3—this will
make sense shortly. The second one is even easier: The logs of numbers such
as 100, 1000 or 10,000,000,000,000 can be arrived at by simply counting up
the zeros. The last guideline relates to the fact that if the measured value is less
than the reference value, the resulting log value will be negative. For example:
log 2 = 0.3
log 1/2 = log 0.5 = –0.3
log 10,000,000,000,000 = 13
log 1000 = 3
log 100 = 2
log 10 = 1
log 0.1 = –1
log 0.01 = –2
log 0.001 = –3
All other numbers can be arrived at by using a scientiﬁc calculator (most
computers and cell phones have one built in); however, it’s unlikely that you’ll
ever need to know any log values beyond understanding the basic concepts that
are listed above.
The Decibel
Now that we’ve gotten past the absolute bare basics, I’d like to break with
tradition again and attempt an explanation of the decibel in a way that’s less
complex and relates more to our day-to-day needs in the sound biz. First off,
the decibel is a logarithmic value that “expresses differences in intensities
between two levels.” From this, we can infer that these levels are expressed by
several units of measure, the most common being sound-pressure level (SPL),
voltage (V) and power (wattage, or W). Now, let’s look at the basic math behind
these three measurements.
SOUND-PRESSURE LEVEL
Sound-pressure level is the acoustic pressure that’s built up within a deﬁned
atmospheric area (usually a square centimeter, or cm2). Quite simply, the higher
the SPL, the louder the perceived sound (Figure 2.22). In this instance, our
61
Sound and Hearing  CHAPTER 2

Loudness Levels
62
measured reference (SPLref) is the threshold of hearing, which is deﬁned as being
the softest sound that an average person can hear. Most conversations will have
an SPL of about 70 dB, while average home stereos are played at volumes ranging
between 80 and 90 dB SPL. Sounds that are so loud as to be painful have SPLs
of about 130 to 140 dB (10,000,000,000,000 or more times louder than the 0
dB reference). We can arrive at an SPL rating by using the formula:
dB SPL = 20log SPL/SPLref
where SPL is the measured sound pressure (in dyne/cm2).
SPLref is a reference sound pressure (the threshold limit of human hearing, 0.02
millipascals = 2 ten-billionths of our atmospheric pressure).
From this, I feel that the major concept that needs to be understood is the idea
that SPL ﬁgures change with the square of the distance (hence, the 20 log part
of the equation). This means that whenever a source/pickup distance is doubled,
the SPL level will be reduced by 6 dB (20 log 1/2 = 20 × –0.3 = –6 dB SPL); as
the distance is halved, it will increase by 6 dB (20 log 2/1 = 20 × 0.3 = 6 dB
SPL), as shown in Figure 2.23.
FIGURE 2.22 
Chart of sound-pressure
levels. (Courtesy of General
Radio Company)
FIGURE 2.23 
Doubling the distance of a
pickup will lower the
perceived signal level by 
6 dB SPL, while halving the
distance will increase it by 
6 dB SPL.
VOLTAGE
Voltage can be thought of as the pressure behind electrons within a wire. As
with acoustic energy, comparing one voltage level to another level (or reference
level) can be expressed as dBv using the equation:
dBv = 20log V/Vref
where V is the measured voltage, and Vref is a reference voltage (0.775 volts).

POWER
Power is usually a measure of wattage or current and can be thought of as the
ﬂow of electrons through a wire over time. Power is generally associated with
audio signals that are carried throughout an audio production system. Unlike SPL
and voltage, the equation for signal level (which is often expressed in dBm) is:
dBm = 10log P/Pref
where P is the measured wattage, and Pref is referenced to 1 milliwatt (0.001
watt).
THE “SIMPLE” HEART OF THE MATTER
I am going to stick my neck out and state that, when dealing with decibels, it’s
far more common for working professionals to deal with the concept of power.
The dBm equation expresses the spirit of the decibel term when dealing with
the marking and measurements on an audio device or the numeric values in a
computer dialog box. This is due to the fact that power is the unit of measure
that’s most often expressed when dealing with audio equipment controls;
therefore, it’s my personal opinion that the average working stiff only needs to
grasp the following basic concepts, as shown in Figure 2.24.
n A 1 dB change is noticeable to most ears (but not by much).
n Turning something up by 3 dB will double the signal’s level, but it will
only be perceived as being 11⁄4 times as loud (deﬁnitely noticeable, but
not as much an increase in gain as you might think).
n Turning something down by 3 dB will halve the signal’s level (likewise,
halving the signal level won’t decrease the perceived loudness as much as
you might think).
n The log of an exponent of 10 can be easily ﬁgured by simply counting the
zeros (e.g., the log of 1,000 is 3). Given that this ﬁgure is multiplied by
10 (10 log P/Pref), increasing the signal’s level 10-fold will turn something
up by 10 dB (and will be perceived as being twice as loud), 100-fold will
yield a 20 dB increase, 1,000 fold will yield a 30 dB increase, etc.
63
Sound and Hearing  CHAPTER 2
FIGURE 2.24 
Various gain and perceived
loudness changes on a
fader.

Loudness Levels
64
Everyone pretty much knows that it’s unlikely that anyone will ever ask, “Would
you please turn that up a hundred times?” It just won’t happen! However, when
a pro asks his or her assistant to turn the gain up by 20 dB, that assistant will
often instinctively know what 20 dB is—and what it should sound like. I’m
saying that the math really isn’t nearly as important as the ongoing process of
grasping an instinctive “feel” for the decibel and how it relates to relative levels
within audio production.
THE EAR
A sound source produces acoustic waves by alternately compressing and rarefying
the air molecules between it and the listener, causing ﬂuctuations that fall above
and below normal atmospheric pressure. The human ear (Figure 2.25) is an
extremely sensitive transducer that responds to these pressure variations by way
of a series of related processes that occur within the auditory organs—our ears.
When these variations arrive at the listener, sound-pressure waves are collected
in the aural canal by way of the outer ear’s pinna. These are then directed to
the eardrum, a stretched drum-like membrane, where the sound waves are
changed into mechanical vibrations, which are transferred to the inner ear by
way of three bones known as the hammer, anvil and stirrup. These bones act
both as an ampliﬁer (by signiﬁcantly increasing the vibrations that are trans -
mitted from the eardrum) and as a limiting protection device (by reducing the
level of loud, transient sounds, such as thunder or ﬁreworks explosions). The
vibrations are then applied to the inner ear (cochlea)—a tubular, snail-like organ
that contains two ﬂuid-ﬁlled chambers. Within these chambers are tiny hair
receptors that are lined up in a row along the length of the cochlea. These hairs
respond to certain frequencies depending on their placement along the organ,
which results in the neural stimulation that gives us the sensation of hearing.
Permanent hearing loss generally occurs when these hair/nerve combinations
are damaged or as they deteriorate with age.
A. outer ear
B. eardrum
C. cochlea
D. pinna 
D
A
B
C
FIGURE 2.25 
Outer, middle and inner 
ear.

Threshold of Hearing
A convenient sound pressure-level (SPL) reference is the threshold of hearing,
which is the minimum sound pressure that produces the phenomenon of
hearing in most people and is equal to 0.0002 microbar. One microbar is equal
to 1 millionth of normal atmospheric pressure, so you can see that the ear is
an amazingly sensitive instrument. In fact, if the ear were any more sensitive,
the thermal motion of molecules in the air would be audible! When referencing
SPLs to 0.0002 microbar, this threshold level usually is denoted as 0 dB SPL,
which is deﬁned as the level that an average person can hear at a speciﬁc
frequency only 50% of the time.
Threshold of Feeling
An SPL that causes discomfort in a listener 50% of the time is called the threshold
of feeling. It occurs at a level of about 118 dB SPL between the frequencies of
200 Hz and 10 kHz.
Threshold of Pain
The SPL that causes pain in a listener 50% of the time is called the threshold of
pain and corresponds to an SPL of 140 dB in the frequency range between 200
Hz and 10 kHz.
Taking Care of Your Hearing
During the 1970s and early 1980s, recording studio monitoring levels were
often turned up so high as to be truly painful. In the mid-1990s,
a small band of powerful producers and record executives
banded together to get the industry to successfully reduce
these average volumes down to tolerable levels (85 to 95
dB)—a practice that often continues to this day. Live
sound venues and acts often continue the practice of
raising house and stage volumes to chest-thumping levels
(and beyond). Although these levels are exciting, long-term
exposure can lead to temporary or even permanent hearing loss. Here
are the different types of hearing loss:
n Acoustic trauma: This happens when the ear is exposed to a sudden, loud
noise in excess of 140 dB. Such a shock could lead to permanent hearing
loss.
n Temporary threshold shift: The ear can experience temporary hearing loss
when exposed to long-term, loud noise.
n Permanent threshold shift: Extended exposure to loud noises in a speciﬁc
or broad hearing range can lead to permanent hearing loss in those
frequencies. In short, the ear becomes less sensitive to sounds in the
damaged frequency range leading to a reduction in perceived volume and
intelligibility . . . What?
65
Sound and Hearing  CHAPTER 2
A simple 
fact to remember: 
Once your hearing is gone,
it’s gone—for good!

The Ear
66
Here are a few hearing conservation tips (courtesy of the House Ear Institute,
www.hei.org) that can help reduce hearing loss due to long-term exposure of
sounds over 115 dB:
n Avoid hazardous sound environments; if they’re not avoidable, wear
hearing protection devices, such as foam earplugs, custom-molded earplugs
or in-ear monitors.
n Monitor sound-pressure levels at or around 85 dB. The general rule to
follow is if you’re in an environment where you must raise your voice to
be heard, then you’re monitoring too loudly and should limit your
exposure times.
n Take 15-minute “quiet breaks” every few hours if you’re being exposed to
levels above 85 dB.
n Musicians and other live entertainment professionals should avoid
practicing at concert-hall levels whenever possible.
n Have your hearing checked periodically by a licensed audiologist.
Psychoacoustics
The area of psychoacoustics deals with how and why the brain interprets a
particular sound stimulus in a certain way. Although a great deal of study has
been devoted to this subject, the primary device in psychoacoustics is the all-
elusive brain, which is still largely unknown to present-day science.
AUDITORY PERCEPTION
From the outset, it’s important to realize that the ear is a nonlinear device (what’s
received at your ears isn’t always what you’ll hear). It’s also important to note
that the ear’s frequency response (its perception of timbre) changes with the
loudness of the perceived signal. The “loudness” compensation switch found
on many hi-ﬁ preampliﬁers is an attempt to compensate for this decrease in
the ear’s sensitivity to low- and high-frequency sounds at low listening levels.
The Fletcher-Munson equal-loudness contour curves (Figure 2.26) indicate the ear’s
average sensitivity to different frequencies at various levels. These indicate the
sound-pressure levels that are required for our ears to hear frequencies along
the curve as being equal in level to a 1000-Hz reference level (measured in
phons). Thus, to equal the loudness of a 1-kHz tone at 110 dB SPL (a level
typically created by a trumpet-type car horn at a distance of 3 feet), a 40-Hz
tone has to be about 6 dB louder, whereas a 10-kHz tone must be 4 dB louder
in order to be perceived as being equally loud. At 50 dB SPL (the noise level
present in the average private business ofﬁce), the level of a 40-Hz tone must
be 30 dB louder and a 10-kHz tone 13 dB louder than a 1-kHz tone to be
perceived as having the same volume. Thus, if a piece of music is mixed to
sound great at a level of 85 to 95 dB, its bass and treble balance will actually
be boosted when turned up (often a good thing). If the same piece were mixed
at 110 dB SPL, it would sound great at that level, but will be both bass and

67
Sound and Hearing  CHAPTER 2
treble shy when played back at lower levels because no compensation for the
ear’s response was added to the mix. Over the years, it has generally been found
that changes in apparent frequency balance are less apparent when monitoring
at levels of 85 dB SPL. In fact, many of the world’s best Grammy award winning
mixers consistently mix their projects at levels that are even lower than that—
the idea being that most systems (TVs, laptops, etc.) will be playing the music
at these lower levels. If the mixes get turned up, the bass and high end will be
exaggerated and will sound just that much better!
In addition to the above, whenever it is subjected to sound waves that are above
a certain loudness level, the ear can produce harmonic distortion that doesn’t exist
in the original signal. For example, the ear can cause a loud 1-kHz sine wave to
be perceived as being a combination of 1-, 2-, 3-kHz waves, and so on. Although
the ear might hear the overtone structure of a violin (if the listening level is loud
enough), it might also perceive additional harmonics (thus changing the timbre of
the instrument). This is one of several factors that implies that sound monitored
at very loud levels could sound quite different when played back at lower levels.
The loudness of a tone can also affect our ear’s perception of pitch. For example,
if the intensity of a 100-Hz tone is increased from 40 to 100 dB SPL, the ear will
hear a pitch decrease of about 10%. At 500 Hz, the pitch will change about 2%
for the same increase in sound-pressure level. This is one reason why musicians
ﬁnd it difﬁcult to tune their instruments when listening through loud headphones.
As a result of these nonlinearities in the ear’s response, tones will often interact
with each other, rather than being perceived as being separate. Three types of
interaction effects can occur:
n Beats
n Combination tones
n Masking
FIGURE 2.26 
The Fletcher-Munson curve
shows an equal loudness
contour for pure tones as
perceived by humans having
an average hearing acuity.
These perceived loudness
levels are charted relative to
sound-pressure levels at
1000 Hz.

The Ear
68
BEATS
Two tones that differ only slightly in frequency and have approximately the
same amplitude will produce an effect known as beats. This effect sounds like
repetitive volume surges that are equal in frequency to the difference between
these two tones. The phenomenon is often used as an aid for tuning instruments,
because the beats slow down as the two notes approach the same pitch and
ﬁnally stop when the pitches match. In reality, beats are a result of the ear’s
inability to separate closely pitched notes. This results in a third frequency that’s
created from the phase sum and difference values between the two notes.
1. Go to the Tutorial section of www.modrec.com,
click on Beats Tutorial and download all of the
sound ﬁles.
2. Load the 440Hz ﬁle onto track 1 of the digital
audio workstation (DAW) of your choice, making
sure to place the ﬁle at the beginning of the
track, with the signal panned center.
3. Load the 445 and 450 Hz ﬁles into the next two
consecutive tracks.
4. Solo and play the 440 Hz tone.
5. Solo both the 440 and 445 Hz tones and listen to
their combined results. Can you hear the 5 Hz
beat tone? (445 Hz – 440 Hz = 5 Hz)
6. Solo both the 445 and 450 Hz tones and listen to
their combined results. Can you hear the 5 Hz
beat tone? (450 Hz – 445 Hz = 5 Hz)
7. Now, solo both the 440 and 450 Hz tones and
listen to their combined results. Can you hear the
10 Hz beat tone? (450 Hz – 440 Hz = 10 Hz)
Try This: Beats
D I Y
 do  it  yourself
COMBINATION TONES
Combination tones result when two loud tones differ by more than 50 Hz. In
this case, the ear perceives an additional set of tones that are equal to both the
sum and the difference between the two original tones as well as being equal
to the sum and difference between their harmonics. The simple formulas for
computing the fundamental tones are:
Sum tone = f1 + f2
Difference tone = f1 – f2
Difference tones can be easily heard when they are below the frequency of both
tones’ fundamentals. For example, the combination of 2000 and 2500 Hz
produces a difference tone of 500 Hz.

Masking
Masking is the phenomenon by which loud signals prevent the ear from hearing
softer sounds. The greatest masking effect occurs when the frequency of the
sound and the frequency of the masking noise are close to each other. For
example, a 4 kHz tone will mask a softer 3.5 kHz tone but has little effect on
the audibility of a quiet 1000 Hz tone. Masking can also be caused by harmonics
of the masking tone (e.g., a 1 kHz tone with a strong 2 kHz harmonic might
mask a 1900-Hz tone). This phenomenon is one of the main reasons why stereo
placement and equalization are so important to the mixdown process. An
instrument that sounds ﬁne by itself can be completely hidden or changed in
character by louder instruments that have a similar timbre. Volume, equalization,
mic choice or mic placement might have to be altered to make the instruments
sound different enough to overcome any masking effect.
69
Sound and Hearing  CHAPTER 2
1. Go to the Tutorial section of www.modrec.com,
click on Masking Tutorial and download all of the
sound ﬁles.
2. Load the 1000 Hz ﬁle onto track 1 of the digital
audio workstation (DAW) of your choice, making
sure to place the ﬁle at the beginning of the
track, with the signal panned center.
3. Load the 3800 and 4000 Hz ﬁles into the next
two consecutive tracks.
4. Solo and play the 1000 Hz tone.
5. Solo both the 1000 and the 4000 Hz tones and
listen to their combined results. Can you hear
both of the tones clearly?
6. Solo and play the 3800 Hz tone.
7. Solo both the 3800 and the 4000 Hz tones and
listen to their combined results. Can you hear
both of the tones clearly?
Try This: Masking
D I Y
 do  it  yourself
PERCEPTION OF DIRECTION
Although one ear can’t discern the direction of a sound’s origin, two ears can.
This capability of two ears to localize a sound source within an acoustic space
is called spatial or binaural localization. This effect is the result of three acoustic
cues that are received by the ears:
n Interaural intensity differences
n Interaural arrival-time differences
n The effects of the pinnae (outer ears)
Middle to higher frequency sounds originating from the right side (for example)
will reach the right ear at a higher intensity level than the left ear, causing an

The Ear
70
interaural intensity difference. This volume difference occurs because the head
casts an acoustic block or shadow, allowing only reﬂected sounds from surround -
ing surfaces to reach the opposite ear (Figure 2.27a). Because the reﬂected sound
travels farther and loses energy at each reﬂection, the sound perceived by the
left ear will naturally be greatly reduced, resulting in a signal that will be correctly
perceived as originating from the right.
This acoustic blockage is relatively insigniﬁcant at lower frequencies, however,
where wavelengths are large compared to the head’s size, allowing the wave to
easily bend around its acoustic shadow. For this reason, a different method of
localization (known as interaural arrival-time differences) is employed at lower
frequencies (Figure 2.27b). In both Figures 2.27a and b, small time differences
occur because the acoustic path length to the left ear is slightly longer than the
path to the right ear. The sound pressure therefore arrives at the left ear at a
later time than the right. This method of localization (in combination with
interaural intensity differences) helps to give us lateral localization cues over
the entire frequency spectrum.
FIGURE 2.27 
Localization of direction: 
(a) the head casts an
acoustic shadow that helps
with localization at middle
to upper frequencies;
(b) interaural arrival-time
differences allow us to
discriminate direction at
lower frequencies.
Intensity and delay cues allow us to perceive the direction of a sound’s origin,
but not whether the sound originates from the front, behind or below. The pinna
(Figure 2.28), however, makes use of two ridges that reﬂect sound into the ear.
These ridges introduce minute time delays between the direct sound (which
reaches the entrance of the ear canal) and the sound that’s reﬂected from the
ridges (which varies according to source location). It’s interesting to note that
beyond 130º from the front of our face, ridge #1 is able to reﬂect and delay
sounds by 0 and 80 microseconds (μsec), making rear localization possible. Ridge
#2 has been reported to produce delays of between 100 and 330 μsec that help
us to locate sources in the vertical plane. The delayed reﬂections from both ridges
are then combined with the direct sound to produce frequency-response
colorations that are compared within the brain to determine source location.
Small movements of the head can also provide additional position information.

71
Sound and Hearing  CHAPTER 2
FIGURE 2.28 
The pinna and its reflective
ridges for determining
vertical location information.
If there are no differences between what the left and right ears hear, the brain
assumes that the source is the same distance from each ear. This phenomenon
allows us to position sound not only in the left and right loudspeakers but also
monophonically between them. If the same signal is fed to both loudspeakers,
the brain perceives the sound identically in both ears and deduces that the source
must be originating from directly in the center. By changing the proportion
that’s sent to each speaker, the engineer changes the relative interaural intensity
differences and thus creates the illusion of physical positioning between the
speakers. This placement technique is known as panning (Figure 2.29).
FIGURE 2.29 
Pan pot settings and their
relative spatial positions.
PERCEPTION OF SPACE
In addition to perceiving the direction of sound, the ear and brain combine
together to help us perceive the size and physical characteristics of the acoustic
space in which a sound occurs. When a sound is generated, a percentage reaches
the listener directly, without encountering any obstacles. A larger portion,
however, travels out to the many surfaces that exist within an acoustic room or

The Ear
72
enclosure. If these surfaces are reﬂective, the sound is bounced back into the
room and toward the listener. If the surfaces are absorptive, less energy will be
reﬂected back to the listener. Three types of reﬂections are commonly generated
within an enclosed space (Figure 2.30):
n Direct sound
n Early reﬂections
n Reverberation
Direct Sound
In air, sound travels at a constant speed of about 1130 feet per second, so a
wave that travels from the source to the listener will follow the shortest path
and arrive at the listener’s ear ﬁrst. This is called the direct sound. Direct sounds
determine our initial perception of a sound source’s location and size and
conveys the true timbre of the source.
Early Reﬂections
After the initial direct sound, the ear will begin to perceive sound reﬂections
that bounce off of surrounding, large-boundary surfaces within the room. These
waves (which are called early reﬂections) must travel further than the direct
sound to reach the listener and therefore arrive after the direct sound and from
a multitude of directions. Early reﬂections give us clues as to the reﬂectivity,
size and general nature of an acoustic space. These sounds generally arrive at
the ears less than 50 m/sec after the brain perceives the direct sound and are
the result of reﬂections off of the largest, most prominent boundaries within a
room. The time elapsed between hearing the direct sound and the beginning
of the early reﬂections helps to provide information about the size of the
performance room. Basically, the farther the boundaries are from the source
and listener, the longer the delay before it’s reﬂected back to the listener.
FIGURE 2.30 The three
distinct sound field types
that are generated within an
enclosed space.
Direct Sound 
Early Reflections 

73
Sound and Hearing  CHAPTER 2
Another aspect that occurs with early reﬂections is called temporal fusion. These
early reﬂections arriving at the listener within 30 m/sec of the direct sound are
not only audibly suppressed, but are also fused with the direct sound. In effect,
the ear can’t distinguish these closely occurring reﬂections and considers them
to be part of the direct sound. The 30 m/sec time limit for temporal fusion isn’t
absolute; rather, it depends on the sound’s envelope. Fusion breaks down at 
4 m/sec for transient clicks, whereas it can extend beyond 80 m/sec for slowly
evolving sounds (such as a sustained organ note or legato violin passage).
Although these early reﬂections are suppressed and fused with the direct sound,
they still modify our perception of the sound, making it both louder and fuller.
Reverberation
Whenever room reﬂections continue to bounce off of room boundaries, a
randomly decaying set of sounds can often be heard after the source stops in
the form of reverberation (Figure 2.31). A highly reﬂective surface absorbs less
of the wave energy at each reﬂection and allows the sound to persist longer
after the initial sound stops. Sounds reaching the listener 50 m/sec later in time
are perceived as a random and continuous stream of reﬂections that arrive from
all directions. These densely spaced reﬂections gradually decrease in amplitude
and add a sense of warmth and body to a sound. Because it has undergone
multiple reﬂections, the timbre of the reverberation is often quite different from
the direct sound (with the most notable difference being a roll-off of high
frequencies and a slight bass emphasis).
The time it takes for a reverberant sound to decrease to 60 dB below its original
level is called its decay time or reverb time and is determined by the room’s
absorption characteristics. The brain is able to perceive the reverb time and
timbre of the reverberation and uses this information to form an opinion on
the hardness or softness of the surrounding surfaces. The loudness of the
perceived direct sound increases rapidly as the listener moves closer to the source,
while the reverberation levels will often remain the same, because the diffusion
is roughly constant throughout the room. This ratio of the direct sound’s
loudness to the reﬂected sound’s level helps listeners judge their distance from
the sound source.
FIGURE 2.31 
Recording Hall 2 (the
“smaller” hall) at Funkhaus,
Berlin has a reverb time that
can reach up to 4 seconds.
(Courtesy of Funkhaus
NelapeStrasse, Berlin,
www.funkhaus-berlin.net)
Reverberation 

Whenever artiﬁcial reverb and delay units are used, the engineer can generate
the necessary delay cues to convince the brain that a sound was recorded in a
huge, stone-walled cathedral, when, in fact, it was recorded in a small, absorptive
room. To do this, the engineer programs the device to mix the original un-
reverberated signal with the necessary early delays and random reﬂections.
Adjusting the number and amount of delays on an effects processor gives the
engineer control over all of the necessary parameters to determine the perceived
room size, while decay time and frequency balance can help to determine the
room’s perceived surfaces. By changing the proportional mix of direct-to-
processed sound, the engineer/producer can place the sound source at either
the front or the rear of the artiﬁcially created soundscape.
74
The Ear

The Audio Cyclopedia deﬁnes acoustics as “a science dealing with the production,
effects and transmission of sound waves; the transmission of sound waves
through various mediums, including reﬂection, refraction, diffraction, absorption
and interference; the characteristics of auditoriums, theaters and studios, as well
as their design.” We can see from this description that the proper acoustic
design of music recording, project and audio-for-visual or broadcast studios is
often no simple matter. A wide range of complex variables and interrelationships
often come into play in the creation of a successful acoustic and monitoring
design. When designing or redesigning an acoustic space, the following basic
requirements should be considered:
n Acoustic isolation: This prevents external noises from transmitting into the
studio environment through the air, ground or building structure. It can
also prevent feuds that can arise when excessive volume levels leak out
into the surrounding neighborhood.
n Frequency balance: The frequency components of a room shouldn’t adversely
affect the acoustic balance of instruments and/or speakers. Simply stated,
the acoustic environment shouldn’t alter the sound quality of the original
or recorded performance.
n Acoustic separation: The acoustic environment should not interfere with
intelligibility and should offer the highest possible degree of acoustic
separation within the room (often a requirement for ensuring that sounds
from one instrument aren’t unduly picked up by another instrument’s
microphone).
n Reverberation: The control of sonic reﬂections within a space is an important
factor for maximizing the intelligibility of music and speech. No matter
how short the early reﬂections and reverb times are, they will add an im -
portant psychoacoustic sense of “space” in the sense that they can give
our brain subconscious cues as to a room’s size, number of reﬂective
bound aries, distance between the source and listener, and so forth.
n Cost factors: Not the least of all design and construction factors is cost. Multi -
million-dollar facilities often employ studio designers and construction
75
CHAPTER 3
Studio Acoustics 
and Design

Studio Acoustics and Design
teams to create a plush decor that’s been acoustically tuned to ﬁt the needs
of both the owners and their clients. Owners of project studios and budget-
minded production facilities, however, can all take full advantage of the
same basic acoustic principles and construction techniques and apply them
in cost-effective ways.
This chapter will discuss many of the basic acoustic principles and construction
techniques that should be considered in the design of a music or sound
production facility. I’d like to emphasize that any or all of these acoustical topics
can be applied to any type of audio production facility and aren’t only limited
to professional music studio designs. For example, owners of modest project
and bedroom studios should know the importance of designing a control room
that’s symmetrical and hopefully feels and sounds good. It doesn’t cost anything
to know that if one speaker is in a corner and the other is on a wall, the perceived
center image and frequency balance will be screwy. As with many techno-artistic
endeavors, studio acoustics and design are a mixture of fundamental physics
(in this case, mostly dimensional mathematics) with an equally large dose of
common sense and dumb luck. More often than not, acoustics is an artistic
science that melds physics with the art of intuition and experience.
STUDIO TYPES
Although the acoustical fundamentals are the same for most studio design
types, differences will often follow the form, function and budgets required by
the tasks at hand. Some of the more common studio types include:
n Professional music studios
n Audio-for-visual production environments
n Audio-for-gaming production environments
n Project studios
The Professional Recording Studio
The professional recording studio (Figure 3.1) is ﬁrst and foremost a commercial
business, so its design, decor and acoustical construction requirements are often
much more demanding than those of a privately owned project studio. In some
cases, an acoustical designer and experienced construction team are placed in
charge of the overall building phase of a professional facility. In others, the
studio’s budget is just too tight to hire such professionals, which places the
studio owners and staff squarely in charge of designing and constructing the
entire facility. Whether you happen to have the luxury of building a new facility
from the ground up or are renovating a studio within an existing shell, you
could easily beneﬁt from a professional studio designer’s experience and skills.
Such expert advice sometimes proves to be cost effective in the long run, because
errors in design judgment can lead to cost overruns, lost business due to
unexpected delays or the unfortunate state of living with mistakes that could
have been avoided.
76

77
Studio Acoustics and Design  CHAPTER 3
The Audio-for-Visual Production Environment
An audio-for-visual production facility is used for video, ﬁlm post-production
(often simply called “post”) and includes such facets as music recording for
ﬁlm or other media (scoring), score mixdown, automatic dialog replacement
(ADR—the replacement of on- and off-screen dialog to visual media) and 
Foley (the replacement and creation of on- and off-screen sound effects). As
with music studios, audio-for-visual production facilities can range from high-
end facilities that can accommodate the posting needs of network video or
feature ﬁlm productions (Figure 3.2) to a simple, budget-minded project studio
that’s equipped with video and a digital audio work station. As with the music
studio, audio-for-visual construction and design techniques often span a wide
range of styles and scope in order to ﬁt the budget needs at hand.
The Audio-for-Gaming Production Environment
With the ever-increasing popularity of having the gaming experience in the home,
budgets and the need for improved audio in newer game releases, production
facilities have sprung up that deal exclusively with the recording and post-
production aspects of game audio. These can range from high-end facilities that
resemble the high-end music studio, to production houses that deal with the
FIGURE 3.1 
Professional studio
examples. (a) BiCoastal
Music, Ossining, NY.
(Courtesy of Russ Berger
Design Group, www.rbdg.
com) (b) London Bridge
Studios big tracking room,
Seattle. (Courtesy of London
Bridge Studios,
www.londonbridgestudio.co
m, Photo by Christopher
Nelson)

Studio Types
78
day-to-day creation and programming of the hundreds of thousands of audio
clips that go into a modern game. Although the production needs of audio-for-
gaming is quite different from music production, the required skills and need
for attention to technical detail demand a high level of skill and dedication, as
a single production schedule can easily last for several months.
The Project Studio
It goes without saying that the vast majority of audio production studios fall
into the project studio category. This basic deﬁnition of such a facility is open
to interpretation. It’s usually intended as a personal production resource for
recording music, audio-for-visual production, multimedia production, voice -
overs—you name it. Project studios can range from being fully commercial in
nature to smaller setups that are both personal and private (Figure 3.3). All of
these possible studio types have been designed with the idea of giving artists
the ﬂexibility of making their art in a personal, off-the-clock environment that’s
both cost and time effective. Of course, the design and construction consid -
erations for creating a privately owned project studio will often differ from the
design considerations for a professional music facility in two fundamental ways:
n Building constraints
n Cost
FIGURE 3.2 
Radiobu audio production
and post facility. (Courtesy
of Russ Berger Design
Group, Inc., www.rbdg.com)
FIGURE 3.3 
Gettin’ it all going in the
project studio. (a) Courtesy
of Yamaha Corporation of
America,
www.yamaha.com. 
(b) Courtesy of Universal
Audio, www.uaudio.com
© 2017 Universal Audio,
Inc. All rights reserved.
Used with permission.

Generally, a project studio’s room (or series of rooms) is built into an artist’s
home or a rented space where the construction and dimensional details are
already deﬁned. This fact (combined with inherent cost considerations) often
leads the owner/artist to employ cost-effective techniques for sonically treating
any deﬁciencies within the room. Even if the room has little or no treatment,
keep in mind that a basic knowledge of acoustical physics and room design can
be a valuable and cost-effective tool, as your experience, production needs and
business abilities grow.
Modern-day digital audio workstations (DAWs) have squarely placed the Mac
and PC in the middle of almost every pro and home project studio (Figure 3.4).
In fact, in many cases, the DAW is the project studio. With the advent of self-
powered speaker monitors, cost-effective microphones and hardware DAW
controllers, it’s a relatively simple matter to design a powerful production system
into almost any existing space.
79
Studio Acoustics and Design  CHAPTER 3
FIGURE 3.4 
Mark Needham’s production
room which centers around
two Raven MTX touch
screen controllers.
(Courtesy of Slate Pro Audio,
www.slateproaudio.com)
PRIMARY FACTORS GOVERNING STUDIO AND 
CONTROL ROOM ACOUSTICS
Regardless of which type of studio facility is being designed, built and used, a
number of primary concerns should be addressed in order to achieve the best
possible acoustic results. In this section, we’ll take a close look at such important
and relevant aspects of acoustics as:
n Acoustic isolation
n Symmetry in control room and monitoring design
n Frequency balance
n Absorption
n Reﬂection
n Reverberation

Primary Factors
80
Although several mathematical formulas have been
included in the following sections, it’s by no means
necessary that you memorize or worry about them.
By far, I feel that it’s more important that you grasp
the basic principles of acoustics rather than worry
about the underlying math. Remember: More often
than not, acoustics is an artistic science that blends
math with the art of intuition and experience.
Acoustic Isolation
Because most commercial and project studio environments make use of an
acoustic space to record sound, it’s often wise and necessary to employ effective
isolation techniques into their design in order to keep external noises to a
minimum. Whether that noise is transmitted through the medium of air (e.g.,
from nearby auto, train or jet trafﬁc) or through solids (e.g., from air-conditioner
rumbling, underground subways or nearby businesses), special construction
techniques will often be required to dampen these extraneous sounds (Figure 3.5).
FIGURE 3.5 
Various isolation, absorption
and reflective acoustical
treatments for the
construction of a
recording/monitoring
environment. (Courtesy of
Auralex Acoustics,
www.auralex.com)
If you happen to have the luxury of building a studio facility from the ground
up, a great deal of thought should be put into selecting the studio’s location.
If a location has considerable neighborhood noise, you might have to resort to
extensive (and expensive) construction techniques that can “ﬂoat” the rooms
(a process that effectively isolates and decouples the inner rooms from the
building’s outer foundations). If there’s absolutely no choice of studio location
and the studio happens to be located next to a recycling factory, just under 
the airport’s main landing path or over the subway’s uptown line you’ll simply
have to give in to destiny and build acoustical barriers to these outside
interferences.

81
Studio Acoustics and Design  CHAPTER 3
The reduction in the sound-pressure level (SPL) of a sound source as it passes
through an acoustic barrier of a certain physical mass (Figure 3.6) is termed the
transmission loss (TL) of a signal. This attenuation can be expressed (in dB) as:
TL = 14.5 log M + 23
where TL is the transmission loss in decibels, and M is the surface density (or
combined surface densities) of a barrier in pounds per square foot (lb/ft2).
FIGURE 3.6 
Transmission loss refers to
the reduction of a sound
signal (in dB) as it passes
through an acoustic barrier.
Because transmission loss is frequency dependent, the following equation can
be used to calculate transmission loss at various frequencies with some degree
of accuracy:
TL = 14.5 log Mf – 16
where f is the frequency (in hertz).
Both common sense and the preceding two equations tell us that heavier acoustic
barriers will yield a higher transmission loss. For example, Table 3.1 tells us
that a 12-inch-thick wall of dense concrete (yielding a surface density of 150
lb/ft2) offers a much greater resistance to the transmission of sound than can
a 4-inch cavity ﬁlled with sand (which yields a surface density of 32.3 lb/ft2).
From the second equation (TL = 14.5 log Mf – 16), we can also draw the con -
clusion that, for a given acoustic barrier, transmission losses will increase as the
frequency rises. This can be easily illustrated by closing the door of a car that
has its sound system turned up, or by shutting a single door to a music studio’s
control room. In both instances, the high frequencies will be greatly reduced
in level, while the bass frequencies will be impeded to a much lesser extent.
From this, the goal would seem to be to build a studio wall, ﬂoor, ceiling,
window or door out of the thickest and most dense material that’s available;
however, expense and physical space often play roles in determining just how
much of a barrier can be built to achieve the desired isolation. As such, a balance
must usually be struck when using both space- and cost-effective building
materials.

Primary Factors
82
WALLS
When building a studio wall or reinforcing an existing structure, the primary
goal is to reduce leakage (increase the transmission loss) through a wall as much
as possible over the audible frequency range. This is generally done by:
n Building a wall structure that’s as massive as is practically possible (both
in terms of cubic and square foot density)
n Eliminating joints that can easily transmit sound through the barrier
n Dampening structures, so that they are well supported by reinforcement
structures and are free of resonances
The following guidelines can be helpful in the construction of framed walls
that have high transmission losses:
n If at all possible, the inner and outer wallboards should not be directly
attached to the same wall studs. The best way to avoid this is to alternately
stagger the studs along the ﬂoor and ceiling frame (i.e., framing 2 × 4 studs
Material
Thickness
Surface Density
(inches)
(lb/ft2)
Brick
4
40.0
8
80.0
Concrete (lightweight)
4
33.0
12
100.0
Concrete (dense)
4
50.0
12
150.0
Glass
—
3.8
—
7.5
—
11.3
Gypsum wallboard
—
2.1
—
2.6
Lead
1/16
3.6
Particleboard
—
1.7
Plywood
—
2.3
Sand
1
8.1
4
32.3
Steel
—
10.0
Wood
1
2.4
Table 3.1
Surface Densities of Common Building Materials

onto a 2 × 6 frame), so that the front/back facing walls aren’t in physical
contact with each other (Figure 3.7).
n Each wall layer should have a different density to reduce the likelihood
of increased transmission due to resonant frequencies that might be
sympathetic to both sides. For example, one wall might be constructed of
two 5/8-inch gypsum wallboards, while the other wall might be
underplayed with soft ﬁberboard that’s also surfaced with two 3/4-inch
gypsum wallboards.
n If you’re going to attach gypsum wallboards to a single wall face, you can
increase transmission loss by mounting the additional layers (not the ﬁrst
layer) with adhesive caulking rather than by using screws or nails.
n Spacing the studs 24 inches on center instead of using the traditional 16-
inch spacing yields a slight increase in transmission loss.
n To reduce leakage that might make it through the cracks, apply a bead of
non-hardening caulk sealant to the inner gypsum wallboard layer at the
wall-to-ﬂoor, wall-to-ceiling and corner junctions.
Generally, the same amount of isolation is required between the studio and the
control room as is required between the studio’s interior and exterior environ -
ments. The proper building of this wall is important, so that an accurate tonal
balance can be heard over the control-room monitors without promoting leakage
between the rooms or producing resonances within the wall that would audibly
color the signal. Optionally, a specially designed cavity, called a sofﬁt, can be
designed into the front-facing wall of the control room to house the larger studio
monitors. This superstructure allows the main, far ﬁeld studio monitors to be
mounted directly into the wall to reduce reﬂections and resonances in the
monitoring environment.
It’s important for a sofﬁt to be constructed to high standards, using a multiple-
wall or high-mass design that maximizes the density with acoustically tight
construction techniques in order to reduce leakage between the two rooms.
Cutting corners by using substandard (and even standard) construction
techniques in the building of a studio sofﬁt can lead to unfortunate side effects,
such as wall resonances, rattles, and increased leakage. Typical wall construction
materials include:
83
Studio Acoustics and Design  CHAPTER 3
FIGURE 3.7 
Double, staggered stud
construction greatly reduces
leakage by decoupling the
two wall surfaces from each
other: (a) top view showing
walls that are directly tied to
wall studs (allowing sound
to easily pass through); 
(b) top view showing walls
with offset, non-touching
studs (so that sound doesn’t
easily pass from wall to
wall).

Primary Factors
n Concrete: This is the best and most solid material, but it is often expensive
and it’s not always possible to pour cement into an existing design.
n Bricks (hollow-form or solid): This excellent material is often easier to place
into an existing room than concrete.
n Gypsum plasterboard: Building multiple layers of plasterboard onto a double-
walled stud frame is often the most cost- and design-efﬁcient approach
for reducing resonances and maximizing transmission loss. It’s often a
good idea to reduce these resonances by ﬁlling the wall cavities with
Rockwool or ﬁberglass, while bracing the internal structure to add an extra
degree of stiffness.
Studio monitors can be designed into the sofﬁt in a number of ways. In one
expensive approach, the far-ﬁeld speakers’ inner enclosure cavities are literally
the walls of the control room’s front wall concrete pour. Under these conditions,
resonances are completely eliminated. Another less expensive approach has the
studio monitors resting on poured concrete pedestals; in this situation, inserts
can be cast into the pedestals that can accept threaded rebar rods (known as
all-thread). By ﬁling the rods to a chamfer (a sharp point), it’s possible to adjust
the position, slant and height of the monitors for ﬁnal positioning into the
sofﬁt’s wall framing. The most common and affordable approach uses traditional
wood framing in order to design a cavity into which the speaker enclosures can
be designed and positioned. Extra bracing, plasterboard and heavy construction
should be used to reduce resonances.
FLOORS
For many recording facilities, the isolation of ﬂoor-borne noises from room
and building exteriors is an important consideration. For example, a building
that’s located on a busy street and whose concrete ﬂoor is tied to the building’s
ground foundation might experience severe low-frequency rumble from nearby
trafﬁc. Alternatively, a second-ﬂoor facility might experience undue leakage
from a noisy downstairs neighbor or, more likely, might interfere with a quieter
neighbor’s business. In each of these situations, increasing the isolation to
reduce ﬂoor-borne leakage and/or transmission is essential. One of the most
common ways to isolate ﬂoor-related noise is to construct a “ﬂoating” ﬂoor
that’s structurally decoupled from its subﬂoor foundation.
Common construction methods for ﬂoating a professional facility’s ﬂoor uses
either neoprene “hockey puck” isolation mounts, U-Boat ﬂoor ﬂoaters (Figure
3.8a), or a continuous underlay, such as a rubberized ﬂoor mat. In these cases,
the underlay is spread over the existing ﬂoor foundation and then covered with
an overlaid plywood ﬂoor structure. In more extreme situations, this super -
structure could be covered with reinforcing wire mesh and ﬁnally topped with
a 4-inch layer of concrete (Figure 3.8b). In either case, the isolated ﬂoor is then
ready for carpeting, wood ﬁnishing, painting or any other desired surface.
An even more cost- and space-effective way to decouple a ﬂoor involves layering
the original ﬂoor with a rubberized or carpet foam pad. A 1/2- or 5/8-inch layer
84

85
Studio Acoustics and Design  CHAPTER 3
of tongue-and-groove plywood or oriented strand board (OSB) is then laid on
top of the pad. These should not be nailed to the subﬂoor; instead, they can
be stabilized by glue or by locking the pieces together with thin, metal braces.
Another foam pad can then be laid over this structure and topped with carpeting
or any other desired ﬁnishing material (Figure 3.9).
It is important that the ﬂoating superstructure be isolated from both the under-
ﬂooring and the outer wall. Failing to isolate these structures allows sounds to
be transmitted through the walls to the subﬂoor, and vice versa (often defeating
the whole purpose of ﬂoating the ﬂoor). These wall perimeter isolation gaps
can be sealed with pliable decoupling materials such as widths of soft mineral
ﬁberboard, neoprene, silicone or other pliable materials.
Caulk this small gap at each layer
1/2” or 5/8” Particle Board
5/8” Drywall
3/4” Particle Board
Glue these layers 
together & screw them 
to the framing 
members 
Insulate this cavity
Framing member
(2x4 shown)
U-Boat (Duh)
Existing wall
SheetBlok
4” concrete
reinforcing mesh
plastic sheet 6 mil
marine plywood 1/2”
neoprene or fiberglass
mounts
perimiter
isolation
board
(a)
(b)
FIGURE 3.8 
Floor isolation treatments.
(a) U-Boat™ floor beam
float channels can be placed
under a standard 2x4 floor
frame to increase isolation.
Floor floaters should be
placed every 16 inches
under a 2x4 floor joist.
(Courtesy of Auralex
Acoustics, www.auralex.
com) (b) Basic guidelines for
building a concrete floating
floor using neoprene
mounts.
FIGURE 3.9 
An alternative, cost-
effective way to float an
existing floor is by layering
relatively inexpensive
materials.
RISERS
As we saw from the equation TL = 14.5 log Mf – 16, low-frequency sound travels
through barriers much more easily than does high-frequency sound. It stands
to reason that strong, low-frequency energy is transmitted more easily than high-
frequency energy between studio rooms, from the studio to the control room

Primary Factors
86
or to outside locations. In general, the drum set is most likely to be the biggest
leakage offender. By decoupling much of a drum set’s low-frequency energy
from a studio ﬂoor, many of the low-frequency leakage problems can be reduced.
In most cases, the problem can be ﬁxed by using a drum riser. Drum risers are
available commercially (Figure 3.10a), or they can be easily constructed. In 
order to reduce unwanted resonances, drum risers should be constructed 
using 2 × 6-inch or 2 × 8-inch beams for both the frame and the supporting
joists (spaced at 16 or 12 inches on center, as shown in Figure 3.10b). Sturdy
1/2- or 5/8-inch tongue-and-groove plywood panels should be glued to the
supporting frames with carpenter’s glue (or similar wood glue) and then nailed
or screwed down (using heavy-duty, galvanized fasteners). When the frame has
dried, rubber coaster ﬂoat channels or (at the very least) strips of carpeting should
be attached to the bottom of the frame, and the riser will be ready for action.
CEILINGS
Foot trafﬁc and other noises from above a sound studio or production room
are another common source of external leakage. Ceiling noise can be isolated
in a number of ways. If foot trafﬁc is your problem and you’re fortunate enough
to own the ﬂoors above you, you can reduce this noise by simply carpeting the
overhead hallway or by ﬂoating the upper ﬂoor. If you don’t have that luxury,
one approach to isolating ceiling-borne sounds is to hang a false structure from
the existing ceiling or from the overhead joists (as is often done when a new
room is being constructed). This technique can be fairly cost effective when
FIGURE 3.10 
Drum/isolation risers. 
(a) HoverDeckTM 88 isolation
riser. (Courtesy of Auralex
Acoustics, www.auralex.
com) (b) General
construction details for a
homemade drum riser.
FIGURE 3.11 
Ceiling isolator systems. 
(a) RSICSI-1 (Resilient
Sound Isolation Clips).
(Courtesy of PAC
International, Inc; www.
pac-intl.com) (b) Z channels
can be used to hang a
floating ceiling from an
existing overhead structure.

spring or “Z” suspension channels are used (Figure 3.11). Z channels are often
screwed to the ceiling joists to provide a ﬂexible, yet strong support to which
a hanging wallboard ceiling can be attached. If necessary, ﬁberglass or other
sound-deadening materials can be placed into the cavities between the overhead
structures.
WINDOWS AND DOORS
Access to and from a studio or production room area (in the form of windows
and doors) can also be a potential source of sound leakage. For this reason,
strict attention needs to be given to window and door design and construction.
Visibility in a studio is extremely important within a music production environ -
ment. For example, when multiple rooms are involved, good visibility serves
to promote effective communication between the producer or engineer and the
studio musician (as well as among the musicians themselves). For this reason,
windows have been an important factor in studio design since the beginning.
The design and construction details for a window often vary with studio needs
and budget requirements and can range from being deep, double-plate cavities
that are built into double-wall constructions (Figure 3.12) to more modest prefab
designs that are built into a single wall. Other more expensive designs include
ﬂoor-to-ceiling windows that create a virtual “glass wall,” as well as those
impressive ones which are designed into poured concrete sofﬁt walls.
87
Studio Acoustics and Design  CHAPTER 3
FIGURE 3.12 
Detail for a practical window
construction between the
control room and studio. 
(a) Simplified drawing. 
(b) Detailed drawing.
(Courtesy of Russ Berger
Design Group, Inc.,
www.rbdg.com)
Access doors to and from the studio, control room, and exterior areas should
be constructed of solid wood or high-quality acoustical materials (Figure 3.13a),
as solid doors generally offer higher TL values than their cheaper, hollow
counterparts. No matter which door type is used, the appropriate seals, weather
stripping, and doorjambs should be used throughout so as to reduce leakage
through the cracks. Whenever possible, double-door designs should be used to

Primary Factors
88
form an acoustical sound lock (Figure 3.13b). This construction technique
dramatically reduces leakage because the air trapped between the two solid
barriers offers up high TL values.
ISO-ROOMS AND ISO-BOOTHS
Isolation rooms (iso-rooms) are acoustically isolated or sealed areas that are built
into a music studio or just off of a control room (Figure 3.14). These recording
areas can be used to separate louder instruments from softer ones (and vice
versa) in order to reduce leakage and to separate instrument types by volume
to maintain control over the overall ensemble balance. For example:
n To eliminate leakage when recording scratch vocals (a guide vocal track
that’s laid down as a session reference), a vocalist might be placed in a
small room while the rhythm ensemble is placed in the larger studio area.
n A piano or other instrument could be isolated from the larger area that’s
housing a full string ensemble.
n Vocals could be set up in the iso-room, while drums are being laid down
in the main room. The possibilities are endless.
An iso-room can be designed to have any number of acoustical properties. By
having multiple rooms and/or iso-room designs in a studio, several acoustical
environments can be offered that range from being more reﬂective (live) to
FIGURE 3.13 
Door isolation systems. 
(a) A SoundSecureTM studio
door. (Courtesy of ETS-
Lindgren, www.ets-
lindgren.com) (b) Example
of a sound lock door system
design.
FIGURE 3.14 
Studio incorporating
multiple iso-rooms.
(Courtesy of Blade Studios,
www.bladestudios.com)

absorptive (dead), or a speciﬁc room can be designed to better ﬁt the acoustical
needs of a particular instrument (e.g., drums, piano or vocals). These rooms
can be designed as totally separate areas that can be accessed from the main
studio or control room, or they might be directly tied to the main studio by
way of sliding walls or glass sliding doors. In short, their form and function
can be put to use to ﬁt the needs and personality of the session.
Isolation booths (iso-booths) provide the same type of isolation as an iso-room,
but are often much smaller. Often called vocal booths, these mini-studios are
perfect for isolating vocals and single instruments from the larger studio. In
fact, rooms that have been designed and built for the express purpose of mixing
down a recording will often only have an iso-booth . . . and no other recording
room. Using this space-saving option, vocals or single instruments can be easily
overdubbed on site, and should more space be needed a larger studio can be
booked to ﬁt the bill.
ACOUSTIC PARTITIONS
Movable acoustic partitions (also known as ﬂats or gobos) are commonly used
in studios to provide on-the-spot barriers to sound leakage. By partitioning a
musician and/or instrument on one or more sides and then placing the mic
inside the temporary enclosure, isolation can be greatly improved in a ﬂexible
way that can be easily changed as new situations arise. Acoustic partitions are
currently available on the commercial market in various design styles and types
for use in a wide range of studio applications (Figure 3.15). For those on a
budget, or who have particular isolation needs, it’s relatively simple to get out
the workshop tools and make your own ﬂats that are based around wood
frames, ﬁberglass, Rockwool or other acoustically absorptive materials—and then
decorate them with your favorite fabric coverings (Figure 3.16a).
If you can’t get a ﬂat when you need one, you can often improvise using
common studio and household items. For example, a simple partition can be
easily made on the spot by grabbing a mic/boom stand combination and
retracting the boom halfway at a 90º angle to make a T-shape. Simply drape a
89
Studio Acoustics and Design  CHAPTER 3
FIGURE 3.15 
Acoustic partition flat
examples: (a) S5–2L
“Sorber” baffle system.
(Courtesy of ClearSonic
Mfg., Inc., www.clearsonic.
com); (b) piano panel setup.
(Courtesy of Auralex
Acoustics, www.auralex.
com)

Primary Factors
90
blanket or heavy coat over the T-bar and voilà—you’ve built a quick-’n’-dirty
dividing ﬂat (Figure 3.16b).
When using a partition, it’s important to be aware that musicians need to be
able to see each other, the conductor and the producer. Musicality and human
connectivity almost always take precedence over technical issues.
NOISE ISOLATION WITHIN THE CONTROL ROOM
Isolation between rooms and the great outdoors isn’t the only noise-related
issue in the modern-day recording or project studio. The proliferation of
computers, multitrack tape machines and cooling systems has created issues
that present their own Grinch-like types of noise, Noise, NOISE! This usually
manifests itself in the form of system fan noise, transport tape noise and
computer-related sounds from CPUs, case fans, hard drives and the like.
When it comes to isolating tape transport and system fan sounds, budget and
size constraints permitting, it’s often wise to build an iso-machine room or iso-
closet that’s been speciﬁcally designed and ventilated for containing such
equipment. An equipment room that has easy-access doors that provide for
current/future wiring needs can add a degree of peace-’n’-quiet and an overall
professionalism that will make both you and your clients happy.
Within smaller studio or project studio spaces, such a room isn’t always possible,
however, with care and forethought the whizzes and whirrs of the digital era
can be turned into a non-issue that you’ll be proud of. Here are a few examples
of the most common problems and their solutions:
n Place the computer(s) in an isolated case, alcove or room (care needs to
be taken to provide ventilation and to monitor the CPU/case temperatures
so as not to harm your system).
n Connect the studio computers via a high-speed network to a remote server
location.
n Replace fans with quieter ones. By doing some careful Web searching or
by talking to your favorite computer salesperson, it’s often possible to
install CPU and case fans that are quieter than most off-the-shelf models.
FIGURE 3.16 
Examples of a homemade
flat: (a) homemade flat
design; (b) the old “blanket
and a boom” trick.

91
Studio Acoustics and Design  CHAPTER 3
Symmetry in Control Room Design
While many professional studios are built from the ground up using standard
acoustic and architectural guidelines, most budget-minded production and
project studios are often limited by their own unique sets of building, space
and acoustic constraints. Even though the design of a budget, project or bedroom
control room might not be acoustically perfect, if speakers are to be used in the
monitoring environment, certain ground rules of acoustical physics must be
followed in order to create a proper listening environment.
One of the most important acoustic design rules in a monitoring environment
is the need for symmetrical reﬂections on all axes within the design of a control
room or single-room project studio. In short, the center and acoustic imaging
(ability to discriminate placement and balance in a stereo or surround ﬁeld) is
best when the listener, speakers, walls and other acoustical boundaries are sym -
metrically centered about the listener’s position (often in an equilateral triangle).
In a rectangular room, the best low-end response can be obtained by orienting
the console and loudspeakers into the room’s long dimension (Figure 3.17a).
Should space or other room considerations come into play, centering the
listener/monitoring position at a 45º angle within a symmetrical corner (Figure
3.17b) is another example of how the left/right imagery can be reasonably
maintained.
FIGURE 3.17 
Various acceptable
symmetries in a monitoring
environment. (a) Acoustic
reflections must be
symmetrical about the
listener’s position. In
addition, orienting a control
room along the long
dimension can extend the
room’s low-end response.
(b) Placing the listening
environment symmetrically
in a corner is another
example of how the
left/right imagery can be
improved over an off-center
placement.
With regard to setting up any production/monitoring
environment, I’d like to ﬁrst draw your attention to
the need for symmetry in any critical monitoring
environment. A symmetrical acoustic environment
around the central mixing axis can work wonders
toward creating a balanced left/right and surround
image. Fortunately, this generally isn’t a difﬁcult goal
to achieve. An acoustical and speaker placement
environment that isn’t balanced between the left-
hand and right-hand sides will allow for differing
reﬂections, absorption coefﬁcients and variations in
frequency response. This can adversely affect the
imaging and balance of your ﬁnal mix. Further
information on this important subject can be found
later in this chapter, however, consider this your ﬁrst
heads-up on an important topic.

Primary Factors
92
While we’re on the subject of the relationship between the room’s acoustic layout
and speaker placement, it’s always wise to place near-ﬁeld and all other speaker
enclosures at points that are equidistant to the listener in the stereo and surround
ﬁeld. Whenever possible, speaker enclosures should be placed 1 to 2 feet away
from the nearest wall and/or corner, which helps to avoid bass buildups that
acoustically occur at boundary and corner locations. In addition to strategic
speaker placement, homemade or commercially available isolation pads can be
used to reduce resonances that often occur whenever enclosures are placed
directly onto a table or ﬂat surface.
Frequency Balance
Another important factor in room design is the need for maintaining the original
frequency balance of an acoustic signal. In other words, the room should exhibit
a relatively ﬂat frequency response over the entire audio range without adding
its own particular sound coloration. The most common way to control the tonal
character of a room is to use materials and design techniques that govern the
acoustical reﬂection and absorption factors.
FIGURE 3.18 
Center symmetry. 
(a) Placing the monitoring
environment off-center and
in a corner will affect the
audible center image, and
placing one speaker in a
90º corner can cause an off-
center bass buildup and
adversely affect the mix’s
imagery. (b) Shifting the
listener/monitoring position
into the center will greatly
improve the left/right
imagery.
Should any primary boundaries of a control room (especially wall or ceiling
boundaries near the mixing position) be asymmetrical from side to side, sounds
heard by one ear will receive one combination of direct and reﬂected sounds,
while the other ear will hear a different acoustic balance (Figure 3.18). This
condition can drastically alter the sound’s center image characteristics, so that
when a sound is actually panned between the two monitor speakers the sound
will appear to be centered; however, when the sound is heard in another studio
or standard listening environment the imaging may be off center. To avoid this
problem, care should be taken to ensure that both the side and ceiling bound -
aries are largely symmetrical with respect to each other and that all of the speaker
level balances are properly set.

93
Studio Acoustics and Design  CHAPTER 3
REFLECTIONS
One of the most important characteristics of sound as it travels through air is
its ability to reﬂect off a boundary’s surface at an angle that’s equal to (and
opposite of) its original angle of incidence (Figure 3.19). Just as light bounces
off a mirrored surface or multiple reﬂections can appear within a mirrored room,
sound reﬂects throughout room surfaces in ways that are often amazingly
complex. Through careful control of these reﬂections, a room can be altered to
improve its frequency response and sonic character.
FIGURE 3.19 
Sound reflects off a surface
at an angle equal (and
opposite) to its original
angle of incidence, much as
light will reflect off a mirror.
In Chapter 2, we learned that sonic reﬂections can be controlled in ways that
disperse the sound outward in a wide-angled pattern (through the use of a convex
surface) or focus them on a speciﬁc point (through the use of a concave surface).
Other surface shapes, on the other hand, can reﬂect sound back at various other
angles. For example, a 90º corner will reﬂect sound back in the same direction
as its incident source (a fact that accounts for the additive acoustic buildups at
various frequencies at or near a wall-to-corner or corner-to-ﬂoor intersection).
The all-time winner of the “avoid this at all possible cost” award goes to
constructions that include opposing parallel walls in its design. Such conditions
give rise to a phenomenon known as standing waves. Standing waves (also
known as room modes) occur when sound is reﬂected off of parallel surfaces
and travels back on its own path, thereby causing phase differences to interfere
with a room’s amplitude response (Figure 3.20a). Room modes are expressed
as integer multiples of the length, width and depth of the room and indicate
which multiple is being referred to for a particular reﬂection.
Walking around a room with moderate to severe mode problems produces the
sensation of increasing and/or decreasing volume levels at various frequencies
throughout the area. These perceived volume changes are due to amplitude
(phase) cancellations and reinforcements of the combined reﬂected waveforms
at the listener’s position. The distance between parallel surfaces and the signal’s
wavelength determines the nodal points that can potentially cause sharp peaks
or dips at various points in the response curve (up to or beyond 19 dB) at the
affected fundamental frequency (or frequencies) and upper harmonic intervals
(Figure 3.20b). This condition exists not only for opposing parallel walls but
also for all parallel surfaces (such as between the ﬂoor and ceiling or between
two reﬂective ﬂats). From this discussion, it’s obvious that the most effective
way to prevent standing waves is to construct walls, boundaries and ceilings
that are nonparallel.

Primary Factors
94
FIGURE 3.20 
Standing waves within a
room. (a) Reflective parallel
surfaces can potentially
cancel and reinforce
frequencies within the
audible spectrum, causing
changes in its response. 
(b) The reflective, parallel
walls create an undue
number of standing waves,
which occur at various
frequency intervals (f1, f2, f3,
f4, and so on).
If the room in question is rectangular or if further sound-wave dispersion is
desired, diffusers can be attached to the wall and/or ceiling boundaries to help
break up standing waves. Diffusers (Figure 3.21) are acoustical boundaries that
reﬂect the sound wave back at various angles that are wider than the original
incident angle (thereby breaking up the energy-destructive standing waves). In
addition, the use of both nonparallel and diffusion wall construction can reduce
extreme, recurring reﬂections and smooth out the reverberation characteristics
of a room by building more complex acoustical pathways.
FIGURE 3.21 
Diffuser examples: (a)
SpaceArray sound diffusers.
(Courtesy of pArtScience,
www.partscience.com); 
(b) open-ended view of a
PrimacousticTM Razorblade
quadratic diffuser. 
(Courtesy of Primacoustic
Studio Acoustics,
www.primacoustic.com); 
(c) Art Diffusor sound
diffusers Models F, C & E.
(Courtesy of acousticsfirst,
www.acousticsfirst.com);
(d) home-made wooden
diffuser.
Flutter echo (also called slap echo) is a condition that occurs when parallel bound -
aries are spaced far enough apart that the listener is able to discern a number
of discrete echoes. Flutter echo often produces a “boingy,” hollow sound that
greatly affects a room’s sound character as well as its frequency response. A
larger room (which might contain delayed echo paths of 50 m/sec or more)

can have its echoes spaced far enough apart in time that the discrete reﬂections
produce echoes that can actually interfere with the intelligibility of the direct
sound. This will often result in a jumble of noise, and in these cases, a proper
application of absorption and acoustic dispersion becomes critical.
When speaking of reﬂections within a studio control room, one long-held
design concept relates to the concept of designing the room such that the rear
of the room is largely reﬂective and diffuse in nature (acoustically “live”), while
the front of the room is largely or partially absorptive (acoustically “dead”).
This philosophy (Figure 3.22) argues for the fact that the rear of the room should
be largely reﬂective providing for a balanced and diffuse environment that can
help reinforce positive reﬂections which can add acoustic “life” to the mix
experience (Figure 3.23). The front of the room would tend more toward the
absorptive side in a way that reduces standing-waves, ﬂutter reﬂections and
reﬂections from the rear of the speakers that would interfere with the overall
response of the room.
95
Studio Acoustics and Design  CHAPTER 3
FIGURE 3.22 
Control-room layout
showing the live end toward
the back of the room and
the dead end (absorption)
toward the front of the
room.
FIGURE 3.23 
Placing bookshelves along
the rear wall and putting
“stuff” on them can provide
both diffusion and a place
for lots of general storage
for things other than the
non-diffuse surfaces of
books.

Primary Factors
96
ABSORPTION
Another factor that often has a marked effect on an acoustic space involves the
use of surface materials and designs that can absorb unwanted sounds (either
across the entire audible band or at speciﬁc frequencies). The absorption of
acoustic energy is, effectively, the inverse of reﬂection (Figure 3.24). Whenever
sound strikes a material, the amount of acoustic energy that’s absorbed relative
to the amount that’s reﬂected can be expressed as a simple ratio known as the
material’s absorption coefﬁcient. For a given material, this can be represented as:
A = Ia/Ir
where Ia is the sound level (in dB) that is absorbed by the surface (often
dissipated in the form of physical heat), and Ir is the sound level (in dB) that
is reﬂected back from the surface.
It’s important to realize that no two rooms will be
acoustically the same or will necessarily offer the
same design challenges. The one constant is that
careful planning, solid design and ingenuity are the
foundation of any good-sounding room. You should
also keep in mind that numerous studio design and
commercial acoustical product ﬁrms are available
that offer assistance for both large and small
projects. Getting professional advice can be a good
thing.
FIGURE 3.24 
Absorption occurs when
only a portion of the incident
acoustic energy is reflected
back from a material’s
surface.
The factor (1 – a) is a value that represents the amount of reﬂected sound. This
makes the coefﬁcient a decimal percentage value between 0 and 1. If we say
that a surface material has an absorption coefﬁcient of 0.25, we’re actually saying
that the material absorbs 25% of the original acoustic energy and reﬂects 75%
of the total sound energy at that frequency. A sample listing of these coefﬁcients
is provided in Table 3.2.
To determine the total amount of absorption that’s obtained by the sum of all
the absorbers within a total volume area, it’s necessary to calculate the average
absorption coefﬁcient for all of the surfaces together. The average absorption
coefﬁcient (Aave) of a room or area can be expressed as:

97
Studio Acoustics and Design  CHAPTER 3
Aave = s1a1 + s2a2 + . . . snan/S
where s1, s2, . . ., sn are the individual surface areas; a1, a2, . . ., an are the individual
absorption coefﬁcients of the individual surface areas, and S is the total square
surface area.
On the subject of absorption, one common misconception is that the use of
large amounts of sound-deadening materials will reduce room reﬂections and
therefore make a room sound “good”. In fact, the overuse of absorption will
often have the effect of reducing high frequencies, creating a skewed room
response that is dull and bass-heavy, as well as reducing constructive room
reﬂections that are important to a properly designed room. In fact, with regard
Coefﬁcients (Hz)
Material
125
250
500
1000
2000
4000
Brick, unglazed
0.03
0.03
0.03
0.04
0.05
0.07
Carpet (heavy, on concrete)
0.02
0.06
0.14
0.37
0.60
0.65
Carpet (with latex backing, on 
0.03
0.04
0.11
0.17
0.24
0.35
40-oz hair-felt or foam rubber)
Concrete or terrazzo
0.01
0.01
0.015
0.02
0.02
0.02
Wood
0.15
0.11
0.10
0.07
0.06
0.07
Glass, large heavy plate
0.18
0.06
0.04
0.03
0.02
0.02
Glass, ordinary window
0.35
0.25
0.18
0.12
0.07
0.04
Gypsum board nailed to 2 x 4 
0.013
0.015
0.02
0.03
0.04
0.05
studs on 16-inch centers
Plywood (3/8 inch)
0.28
0.22
0.17
0.09
0.10
0.11
Air (sabins/1000 ft3)
—
—
—
—
2.3
7.2
Audience seated in upholstered 
0.08
0.27
0.39
0.34
0.48
0.63
seats
Concrete block, coarse
0.36
0.44
0.31
0.29
0.39
0.25
Light velour (10 oz/yd2 in
0.29
0.10
0.05
0.04
0.07
0.09
contact with wall)
Plaster, gypsum or lime (smooth 
0.44
0.54
0.60
0.62
0.58
0.50
ﬁnish on tile or brick)
Wooden pews
0.57
0.61
0.75
0.86
0.91
0.86
Chairs, metal or wooden, seats 
0.15
0.19
0.22
0.39
0.38
0.30
unoccupied
Note: These coefﬁcients were obtained by measurements in the laboratories of the Acoustical Materials Association. Coefﬁcients for
other materials may be obtained from Bulletin XXII of the association.
Table 3.2
Absorption Coefﬁcients for Various Materials

Primary Factors
98
to the balance between reﬂection, diffusion and absorption, many designers
agree that a balance of 25% absorption and 25% diffuse reﬂections is a good
ratio that can help preserve the “life” of a room, while reducing unwanted
buildups.
High-Frequency Absorption
The absorption of high frequencies is accomplished through the use of dense
porous materials, such as ﬁberglass, Rockwool, dense fabric and carpeting. These
materials generally exhibit high absorption values at higher frequencies, which
can be used to control room reﬂections in a frequency-dependent manner.
Specially designed foam and acoustical treatments are also commercially
available that can be attached easily to recording studio, production room or
control room walls as a means of taming multiple room reﬂections and/or
dampening high-frequency reﬂections.
In addition to buying commercial absorbers, it’s very possible to put your handy
shop tools to work by building your own cost-effective absorber panels (of any
shape, depth and style). One straightforward way of making them is by using
Rockwool as the basic ingredient for your homemade absorber:
1. Measure the dimensions that you’ll need to build your absorbers. Buy
1” × 4” ﬁr boards that add up to your required dimensions (often, the
hardware store will even cut them to suit your needs). You might also
want to buy and measure your Rockwool bats at the same time (this might
help you with determining your overall dimensions).
2. Lay the boards out on your workbench or protected table and drill pilot
holes to the top and bottom frame edges, then using a 2” sheetrock or
other type of screw, screw the frames together.
3. Make your measurements for the amount of fabric that you’ll want to
stretch over the entire surface and around the edges, so that they stretch
around the newly made box. The fabric can be of practically any type, but
a nice, inexpensive fabric of your favorite color works well.
4. It always helps to iron the fabric, before mounting it, just to get the
wrinkles out.
5. Before attaching the fabric, you might want to see how the Rockwool ﬁts
into each frame. If all’s ok, then begin carefully attaching the fabric to the
frame with a heavy-duty staple gun, taking care that the fabric is tight,
straight and looks good.
6. Once the fabric is attached and the Rockwool is inserted, it’s ready to hang
in your control room/studio wall.
When done right, these absorbers (Figure 3.25) can look professional and ﬁt
your speciﬁc needs at a fraction of their commercial equivalents, sometimes
with better results.
High-Frequency Absorption 

99
Studio Acoustics and Design  CHAPTER 3
FIGURE 3.25 
Homemade absorber panel:
(a) showing fabric that’s to
be stretched over a wooden
frame; (b) once made, the
Rockwool is placed inside
the frame (which can be of
any size or form), it can be
hung on the wall, lowered
from the ceiling or placed in
a corner.
Low-Frequency Absorption
As shown in Table 3.2, materials that are absorptive in the high frequency range
often provide little resistance to the low-frequency end of the spectrum (and
vice versa). This occurs because low frequencies are best damped by pliable
materials, meaning that low-frequency energy is absorbed by the material’s
ability to bend and ﬂex with the incident waveform (Figure 3.26). Rooms that
haven’t been built to the shape and dimensions to properly handle the low 
end may need to be controlled in order to reduce the room’s resonance
frequencies.
FIGURE 3.26 
Low-frequency absorption.
(a) A carefully designed
surface that can be “bent”
by oncoming sound waves
can be used to absorb 
low frequencies. 
(b) PrimacousticTM
Polyfuser, a combination
diffuser and bass trap.
(Courtesy of Primacoustic
Studio Acoustics,
www.primacoustic.com)
Another absorber type can be used to reduce low-frequency buildup at speciﬁc
frequencies (and their multiples) within a room. This type of attenuation device
(known as a bass trap) is available in a number of design types:
n Quarter-wavelength trap
n Pressure-zone trap
n Functional trap
n Active trap
Low-Frequency Absorption 

Primary Factors
100
The quarter-wavelength trap: The quarter-wavelength bass trap (Figure 3.27) is an
enclosure with a depth that’s one-fourth the wavelength of the offending
frequency’s fundamental frequency and is often built into the rear facing wall,
ceiling or ﬂoor structure and covered by a metal grating to allow foot trafﬁc.
The physics behind the absorption of a calculated frequency (and many of the
harmonics that fall above it) rests in the fact that the pressure component of a
sound wave will be at its maximum at the rear boundary of the trap when the
wave’s velocity component is at a minimum. At the mouth of the bass trap
(which is at a one-fourth wavelength distance from this rear boundary), the
overall acoustic pressure will be at its lowest, while the velocity component
(molecular movement) will be at its highest potential. Because the wave’s
motion (force) is greatest at the trap’s opening, much of the signal can be
absorbed by placing an absorptive material at that opening point. A low-density
ﬁberglass lining can also be placed inside the trap to increase absorption
(especially at harmonic intervals of the calculated fundamental).
Pressure zone trap: The pressure-zone bass trap absorber (Figure 3.28) works on
the principle that sound pressure is doubled at large boundary points that are
at 90º angles (such as walls and ceilings). By placing highly absorptive material
at a boundary point (or points, in the case of a corner/ceiling intersection), the
built-up pressure can be partially absorbed.
FIGURE 3.27 
A quarter-wavelength bass
trap: (a) physical concept
design; (b) sound is largely
absorbed as heat, since the
particle velocity (motion) is
greatest at the trap’s
quarter-wavelength
opening.
FIGURE 3.28 
Realtrap MegaTraps and
absorption curve. (Courtesy
of RealTraps, LLC,
www.realtraps.com)

Functional trap: Originally created in the 1950s by Harry F. Olson (former
director of RCA Labs), the functional bass trap (Figure 3.29a) uses a material
generally formed into a tube or half-tube structure that is rigidly supported so
as to reduce structural vibrations. By placing these devices into corners, room
boundaries or in a freestanding spot, a large portion of the undesired bass
buildup frequencies can be absorbed. By placing a reﬂective surface over the
portion of the trap that faces into the room, frequencies above 400 Hz can be
dispersed back into the room or focal point.
Active trap: An active bass trap system (Figure 3.29b) makes use of a microphone,
low-frequency driver and a fast acting, band-limited ampliﬁer to effectively
create an inverse pressure wave that electronically “absorbs” low-end frequencies.
Such a unit is actually capable of creating an effective area of absorption that
is up to 40 times greater than its actual size.
ROOM REFLECTIONS AND ACOUSTIC
REVERBERATION
Another criterion for studio design is the need for a desirable room ambience
and intelligibility, which is often contradictory to the need for good acoustic
separation between instruments and their pickup. Each of these factors is
governed by the careful control and tuning of the reverberation constants within
the studio over the frequency spectrum.
Reverberation (reverb) is the persistence of a signal (in the form of reﬂected waves
within an acoustic space) that continues after the original sound has ceased.
The effect of these closely spaced and random multiple echoes give us perceptible
cues as to the size, density and nature of an acoustic space. Reverb also adds
to the perceived warmth and spatial depth of recorded sound and plays an
extremely important role in the perceived enhancement of music.
As was stated in the latter part of Chapter 2, the reverberated signal itself can
be broken down into three components:
101
Studio Acoustics and Design  CHAPTER 3
FIGURE 3.29 
Pressure zone bass trap
examples. (a) A functional
bass trap can be placed in a
corner to prevent bass
buildup. (b) The PSI Audio
AVAA C20 low-frequency
active acoustic absorber.
(Courtesy of PSI Audio,
www.psiaudio.com)

Room Reﬂections and Acoustic Reverberation
102
n Direct sound
n Early reﬂection
n Reverb
The direct signal is made up of the original, incident sound that travels from
the source to the listener. Early reﬂections consist of the ﬁrst few reﬂections that
are projected to the listener off of major boundaries within an acoustic space.
These reﬂections generally give the listener subconscious cues as to the size of
the room. (It should be noted that strong reﬂections off of large, nearby surfaces
can potentially have detrimental cancellation effects that can degrade a room’s
sound and frequency response at the listening position.) The last set of signal
reﬂections makes up the actual reverberation characteristic. These signals are
composed of random reﬂections that travel from boundary to boundary in a
room and are so closely spaced that the brain can’t discern the individual
reﬂections. When combined, they are perceived as a single decaying signal.
Technically, reverb is considered to be the time that’s required for a sound to
die away to a millionth of its original intensity (resulting in a decrease over
time of 60 dB), as shown by the following formula:
RT60 = V × 0.049/AS
where RT is the reverberation time (in sec), V is the volume of the enclosure
(in ft3), A is the average absorption coefﬁcient of the enclosure, and S is the
total surface area (in ft2). As you can see from this equation, reverberation time
is directly proportional to two major factors: the volume of the room and 
the absorption coefﬁcients of the studio surfaces. A large environment with 
a relatively low absorption coefﬁcient (such as a large cathedral) will have a
relatively long RT60 decay time, whereas a small studio (which might incorporate
a heavy amount of absorption) will have a very short RT60.
The style of music and the room application will often determine the optimum
RT60 for an acoustical environment. Reverb times can range from 0.25 sec in
a smaller absorptive recording studio environment to 1.6 sec or more in a larger
music or scoring studio. In certain designs, the RT60 of a room can be altered
to ﬁt the desired application by using movable panels or louvers or by placing
carpets in a room. Other designs might separate a studio into sections that exhibit
different reverb constants. One side of the studio (or separate iso-room) might
be relatively non-reﬂective or dead, whereas another section or room could be
much more acoustically live. The more reﬂective, live section is often used to
bring certain instruments that rely heavily on room reﬂections and reverb, such
as strings or an acoustic guitar, to “life.” The recording of any number of instru -
ments (including drums and percussion) can also greatly beneﬁt from a well-
designed acoustically live environment.
Isolation between different instruments and their pickups is extremely important
in the studio environment. If leakage isn’t controlled, the room’s effectiveness
becomes severely limited over a range of applications. The studio designs of

the 1970s and 1980s brought about the rise of the “sound sucker” era in studio
design. During this time, the absorption coefﬁcient of many rooms was raised
almost to an anechoic (no reverb) condition. With the advent of the music
styles of the 1990s and a return to the respectability of live studio acoustics,
modern studio and control-room designs have begun to increase in size and
“liveness” (with a corresponding increase in the studio’s RT60). This has
reintroduced the buying public to the thick, live-sounding music production of
earlier decades, when studios were larger structures that were more attuned to
capturing the overall acoustics of a recorded instrument or ensemble.
Acoustic Echo Chambers
Another physical studio design that was used extensively in the past (before the
invention of artiﬁcial effects devices) for re-creating room reverberation is the
acoustic echo chamber. A traditional echo chamber is an isolated room that has
highly reﬂective surfaces into which speakers and microphones are placed.
The speakers are fed from an effects send, while the mic’s reverberant pickup
is fed back into the mix via an input strip of effects return. By using one or
more directional mics that have been pointed away from the room’s speakers,
the direct sound pickup can be minimized. Movable partitions also can be used
to vary the room’s decay time. When properly designed, acoustic echo chambers
have a very natural sound quality to them. The disadvantage is that they take
up space and require isolation from external sounds; thus, size and cost often
make it unfeasible to build a new echo chamber, especially those that can match
the caliber and quality of high-end digital reverb devices.
An echo chamber doesn’t have to be an expensive, built-from-the-ground-up
design. Actually, a temporary chamber can be made from a wide range of
available acoustic spaces to pepper your next project with a bit of “acoustic
spice.” For example:
n An ambient-sounding chamber can be built by placing a Blumlein (crossed
ﬁgure-8) pair or spaced stereo pair of mics in the main studio space and
feeding a send to the studio playback monitors.
n A speaker/mic setup could be placed in an empty garage (as could a guitar
amp/mic, for that matter).
n An empty stairwell often makes an excellent chamber.
n Any vocalist could tell you what’ll happen if you place a singer or guitar
speaker/mic setup in the shower.
From the above, it’s easy to see that ingenuity and experimentation are often
the name of the makeshift echo/reverb game. In fact, there’s nothing that says
that the chamber has to be a real-time effect—for example, you could play back
a song’s effects track from a laptop DAW into a church’s acoustic space and
record the space to stereo tracks on the DAW, where they can be later placed
into the mix. The limitless experimental options are totally up to you!
103
Studio Acoustics and Design  CHAPTER 3


A microphone (often called a mic) is usually the ﬁrst device in a recording chain.
A mic is a transducer that changes one form of energy (sound waves) into another
corresponding form of energy (electrical signals). The quality of its pickup will
often depend on external variables (such as placement, distance, instrument
and the acoustic environment), as well as on design variables (such as the
microphone’s operating type, design characteristics and quality). These inter -
related elements work together to affect the signal’s overall sound quality.
In order to deal with the wide range of musical and acoustic situations that
might come your way (not to mention your own personal taste), a large number
of mic types, styles and designs can be pulled out of our “sonic toolbox” to get
the job done. The truth is, microphone designs and types differ greatly from
one to the other. Some may have a certain sonic personality that allows it to
work best with certain instruments and situations, while others may be suited
for a wide range of applications. These “personalities” are carefully chosen and
placed by engineers, producers and artists, using their intuition, experience and
talent to get the best possible sound from an acoustic source that best ﬁts the
application at hand.
The road to considering microphone choice and placement is best traveled by
considering a few simple rules:
n Rule 1: There are no rules, only guidelines. And although guidelines can help
you achieve a good pickup, don’t hesitate to experiment in order to get a
sound that best suits your needs or personal taste.
n Rule 2: The overall sound of an audio signal is no better than the weakest link
in the signal path. If a mic or its placement doesn’t sound as good as it
could, make the changes to improve it before you commit it to disk, tape
or whatever. More often than not, the concept of “ﬁxing it later in the
mix” will often put you in the unfortunate position of having to correct
a situation after the fact, rather than recording the best sound and/or
performance during the initial session.
105
CHAPTER 4
Microphones: Design 
and Application

Microphones: Design and Application
106
n Rule 3: Whenever possible, use the “Good Rule”: Good musician + good instru -
ment + good performance + good acoustics + good mic + good placement = good
sound. This rule refers to the fact that a music track will only be as good
as the performer, instrument, mic, mic placement and the entire signal
chain that follows it. If any of these elements falls short of their potential,
the track will suffer accordingly. However, if all of these links are the best
that they can be, the recording will almost always be something that you
can be proud of!
The miking of vocals and instruments (both in the studio and onstage) is
deﬁnitely an art form. It’s often a balancing act to get the most out of the Good
Rule. Sometimes you’ll have the best of all elements;
at others, you’ll have to work hard to make
lemonade out of a situational lemon. The
best rule of all is to use common sense
and to trust your own instincts.
Before diving into the facts and place -
ment techniques that deal with the ﬁner
points of microphone technology, I’d like to
take a basic look at how microphones (and their
operational characteristics) work. Why do I put this in the
book? Well, from a personal standpoint, I feel that having a basic understanding
of what happens “under the hood” will help you to have a better “mental”
image of how a particular mic or mic technique will sound in a given situation.
An operational understanding of how a mic works can combine with your own
intuition, technical and sonic judgments to make the best artistic judgment at
the time.
MICROPHONE DESIGN
A microphone is a device that converts acoustic energy into corresponding
electrical voltages that can be ampliﬁed and recorded. In audio production, three
transducer mic types can be used to accomplish this:
n Dynamic mic
n Ribbon mic
n Condenser mic
The Dynamic Microphone
In principle, the dynamic mic (Figures 4.1a and b) operates by using electro -
magnetic induction to generate an output signal. The simple theory of electromag -
netic induction states that whenever an electrically conductive metal cuts across
the ﬂux lines of a magnetic ﬁeld, a current of a speciﬁc magnitude and direction
will be generated within that metal.
THE
“GOOD RULE”
Good musician + good instrument + 
good performance + good acoustics + good mic
+ good placement = good sound.

107
Microphones: Design and Application  CHAPTER 4
FIGURE 4.1
The dynamic microphone:
(a) the Shure 58 dynamic
mic. (Courtesy of Shure
Incorporated,
www.shure.com, Images
© 2017, Shure
Incorporated—used with
permission); (b) Telefunken
M81 dynamic microphone.
(Courtesy of Telefunken
Elektroakoustik,
www.telefunken-
elektroakustik.com); (c)
inner workings of a dynamic
microphone.
Dynamic mic designs (Figure 4.1c) generally consist of a stiff Mylar diaphragm
of roughly 0.35-mil thickness. Attached to this diaphragm is a ﬁnely wrapped
coil of wire (called a voice coil) that’s precisely suspended within a strong
magnetic ﬁeld. Whenever an acoustic pressure wave hits the diaphragm’s face,
the attached voice coil is displaced and moves in proportion to the amplitude
and frequency of the wave, causing the coil to cut across the lines of magnetic
ﬂux. According to the above theory, an analogous electrical signal (of a speciﬁc
magnitude and direction) is then induced into the coil and across the output
leads, thus producing an analog audio output signal.
The Ribbon Microphone
Like the dynamic microphone, the ribbon mic also works on the principle of
electromagnetic induction. Older ribbon design types, use a diaphragm of
extremely thin aluminum ribbon (2 microns). Often, this diaphragm is
corrugated along its length and is suspended within a strong ﬁeld of magnetic
ﬂux (Figure 4.2a). Sound-pressure variations between the front and the back of
the diaphragm cause it to move and cut across these ﬂux lines, thereby inducing
a current into the ribbon that’s proportional to the amplitude and frequency
of the acoustic waveform. Because the ribbon generates a small output signal
(when compared to the larger output that’s generated by the multiple wire turns
of a moving coil), its output signal is too low to drive a microphone input stage
directly; thus, a step-up transformer (or amp in the case of an active ribbon
mic) must be used to boost the output signal and impedance to an acceptable
range.
Until recently, traditional ribbon technology could only be found on original,
vintage mics (such as the older RCA and Cole ribbon mics). However, with the
skyrocketing price of vintage mics and a resurgence in the popularity of the
smooth, transient quality of the ribbon “sound”, modern reproductions and
entirely new ribbon mic designs have begun to spring up on the market (Figures
4.2b and c).

Microphone Design
108
FURTHER DEVELOPMENTS IN RIBBON TECHNOLOGY
During the past several decades, certain microphone manufacturers have made
changes to original ribbon technologies by striving to miniaturize and improve
their basic operating characteristics. For example, the popular M160 (Figure 4.3)
and M260 ribbon mics from Beyerdynamic use a rare-earth magnet to produce
a capsule that’s small enough to ﬁt into a 2-inch grill ball (much smaller than
a traditional ribbon-style mic). The ribbon (which is corrugated along its length
to give it added strength and at each end to give it ﬂexibility) is 3 microns thick,
about 0.08 inch wide, 0.85 inch long and weighs only 0.000011 ounce. A plastic
throat is ﬁtted above the ribbon, which houses a pop-blast ﬁlter. Two additional
ﬁlters and the grill greatly reduce the ribbon’s potential for blast and wind
damage, a feature that has made these designs suitable for outdoor and hand-
held use.
FIGURE 4.2
The ribbon microphone: 
(a) cutaway detail of a
ribbon microphone.
(Courtesy of Audio
Engineering Associates,
www.ribbonmics.com); (b)
the AEA A440 ribbon mic.
(Courtesy of Audio
Engineering Associates,
www.ribbonmics.com);
(c) Royer Labs R-121
ribbon microphone.
(Courtesy of Royer Labs,
www.royerlabs.com)
FIGURE 4.3
The Beyerdynamic M160
ribbon mic. (Courtesy of
Beyerdynamic,
www.beyerdynamic.com)
Other alterations to the traditional ribbon technology make use of phantom
power to supply power to an active, internal ampliﬁer, so as to boost the mic’s
output to that of a dynamic or condenser mic, without the need for a passive
transformer (an explanation of phantom power can be found in the next section
on condenser mics).

The Condenser Microphone
Condenser mics (like those having capsules which are shown in Figures 4.4a and
b) operate on an electrostatic principle rather than the electromagnetic principle
used by a dynamic or ribbon mic. The capsule of a basic condenser mic consists
of two plates: one very thin movable diaphragm and one ﬁxed backplate. These
two plates form a capacitor (or condenser, as it is still called in the UK and in
many parts of the world). A capacitor is an electrical device that’s capable of
storing an electrical charge. The amount of charge that it can store is determined
by its capacitance value and the voltage that’s applied to it, according to the
formula:
Q = CV
where Q is the charge (in coulombs), C is the capacitance (in farads) and V is
the voltage (in volts).
At its most basic level, a condenser mic operates when a regulated DC power
supply is applied between its diaphragm plates to create a capacitive charge.
When sound acts upon the movable diaphragm, the varying distance between
the plates will likewise create a change in the device’s capacitance (Figure 4.5a).
According to the above equation, if Q (the power supply charge) is constant
and C (the diaphragm’s capacitance) changes, then V (voltage across the
diaphragm) will change in a proportional and inverse fashion thereby giving
us our output signal.
109
Microphones: Design and Application  CHAPTER 4
FIGURE 4.4
Inner details of a condenser
mic: (a) AKG C214
condenser mic. (Courtesy 
of AKG Acoustics GmbH.,
www.akg.com); (b) exposed
diaphragm. (Courtesy of
ADK, www.adkmic.com;
photograph by K. Bujack)
Since the charge (Q) is known to be constant and
the diaphragm’s capacitance (C) changes with
differences in sound pressure, the voltage (V) must
change in inverse proportion. Given that the
capsule’s voltage now changes in proportion to the
sound waves that act upon it, voilà—we have a
condenser mic that has an audio output signal!

Microphone Design
110
The next trick is to tap into the circuit to capture the changes in output voltage.
This is done by placing a high-value resistor across the circuit. Since the voltage
across the resistor will change in inverse proportion to the capacitance across
the capsule plates, this signal will then become the feed for the mic’s output
(Figure 4.5b).
FIGURE 4.5
Interactions as a result of
diaphragm capacitance
changes: (a) output and
potential relationships as 
a result of changing
capacitance; (b) as a sound
wave decreases the
condenser spacing by d the
capacitance will increase,
causing the voltage to
proportionately fall (and vice
versa).
Since the resulting signal has an extremely high impedance, it must be fed through
a preampliﬁer in order to preserve the mic’s frequency response characteristics.
As this amp must be placed at a point just following the resistor (often at a distance
of 2 inches or less), it is almost always placed within the mic body in order to
prevent hum, noise pickup and signal-level losses. This preamp (in addition to
the need for a polarizing voltage source across the diaphragm leads) means that
a powering voltage/current source must be included in the design.
POWERING A CONDENSER MIC
As you have read, all condenser microphones require a polarizing voltage, as
well as an ampliﬁer that’s required to step the impedance of the capsule down
to a value that will work in everyday production applications. The three systems
for dealing with these power requirements are:
n An external power supply
n Phantom power
n Electret self-charging system
External Power Supply
Older condenser microphones and modern-day reproductions are generally
valued by studios and collectors alike for their “tube” sound, which results from
even-harmonic distortion and “warm-sounding” characteristics that occur
whenever tubes are used. These mics use an external power supply to provide
power to the condenser diaphragm and internal amp circuits that go between
the mic and the console/interface/mic preamp. The supply plugs into a 120/240V
External Power Supply 

111
Microphones: Design and Application  CHAPTER 4
power outlet to provide a high voltage/current supply to a vacuum tube (valve)
that’s designed into the mic’s housing itself. Unfortunately, there is no
standardization for these supplies, meaning that each supply must be connected
to and stored with its associated mic (often leading to a jumble of cables on
the studio ﬂoor, but that’s the price we pay for art).
Phantom Power
Most modern professional condenser (and some ribbon) mics are designed to
be powered directly from the console/interface/mic preamp through the use of
a phantom power supply. Phantom power works by supplying a positive DC
supply voltage of +48V equally through both audio conductors (pins 2 and 3)
of a balanced mic line to the condenser capsule and preamp (Figure 4.6). This
voltage is equally distributed through identical value resistors, so that no voltage
differential exists between the two leads. The ground side of the circuit is then
supplied to the capsule and preamp through the balanced cable grounding wire
(pin 1).
FIGURE 4.6
Schematic drawing of a
phantom power wiring/cable
system.
Since the audio is only affected by potential differences between pins 2 and 3
(and not the ground signal on pin 1), the carefully matched +48V powering
potential at these leads cancel out and is therefore not electrically “visible” to
the input stage of a balanced mic preamp. Instead, only the balanced, alternating
audio signal that’s being simultaneously carried along the two audio leads (pins
2 and 3) will be seen by the audio circuitry.
The resistors (R) used for distributing power to the signal leads should be 1/4W
resistors with a ±1% tolerance and a value of 6.8 kΩwhen using a 48V system.
In addition to precisely matching the supply voltages, these resistors also help
to provide a degree of power isolation between other mic inputs on a console.
If a signal lead were accidentally shorted to ground (which could happen if
defective cables or unbalanced XLR cables were used), the power supply should
still be able to deliver power to other mics in the system. If two or more inputs
were accidentally shorted, however, the phantom voltage could then drop to
levels that would be too low to be usable.
Phantom Power 

Microphone Design
112
The Electret-Condenser Microphone
Electret-condenser mics work on the same operating principles as their externally
polarized counterparts, with the exception that a static polarizing charge has
been permanently set up between the mic’s diaphragm and its backplate (using
a process that works much like the static-cling that occurs when you take socks
out of a dryer). Since the charge (Q) is permanently built into the capsule, no
external source is required to power the diaphragm. However, as with a powered
condenser mic, the capsule’s output impedance is so high that a preamp will
still be required to reduce it to a standard value. As a result, a battery, external
powering source or standard phantom supply must be used to power the low-
current amp.
1. Go to the Tutorial section of www.modrec.com,
click on Mic Types and download the sound ﬁles
(which include examples of each mic operating
type).
2. Listen to the tracks. If you have access to an
editor or digital audio workstation (DAW), import
the ﬁles and look at the waveform amplitudes for
each example.
3. If you’d like to DIY, then—pull out several mics
from each operating type and plug them in (if you
don’t have several types, maybe a studio, your
school or a friend has a few you can borrow). Try
each one on an instrument and/or vocal. Are the
differences between operating types more
noticeable than between models in the same
family?
Try This: Mic Types
D I Y
 do  it  yourself
MICROPHONE CHARACTERISTICS
In order to handle the wide range of applications that are encountered in studio,
project and on-location recording, microphones will often differ in their overall
sonic, electrical and physical characteristics. The following section highlights
many of these characteristics in order to help you choose the best mic for a
given application.
Directional Response
The directional response of a mic refers to its sensitivity (output level) at various
angles of incidence with respect to the front (on-axis) of the microphone (Figure
4.7a). This angular response can be graphically charted in a way that shows a
microphone’s sensitivity with respect to direction and frequency over 360º. Such
a chart is commonly referred to as the mic’s polar pattern. This directionality of
a mic can be classiﬁed into two categories:

n Omnidirectional polar response
n Directional polar response.
An omnidirectional mic (Figure 4.7b) is a pressure-operated device that’s
responsive to sounds which emanate from all directions. In other words, the
diaphragm will react equally to all sound-pressure ﬂuctuations at its surface,
regardless of the source’s location. Pickups that display directional properties are
pressure-gradient devices, meaning that the pickup is responsive to relative
differences in pressure between the front, back and sides of a diaphragm. For
example, a purely pressure-gradient mic will exhibit a bidirectional polar pattern
(commonly called a ﬁgure-8 pattern), as shown in Figure 4.8. Many of the older
ribbon mics exhibit a bidirectional pattern, since their diaphragms are often
exposed to sound waves from both the front and rear axis. As such, they are
equally sensitive to sounds that emanate from either direction (Figures 4.9a
and b). Sounds from the rear will produce a signal that’s 180º out of phase
with an equivalent on-axis signal. Sound waves arriving 90º off-axis produce
equal but opposite pressures at both the front and rear of the ribbon (Figure
4.9c), resulting in a cancellation at the diaphragm and no output signal.
113
Microphones: Design and Application  CHAPTER 4
FIGURE 4.7
Directional axis of a
microphone: (a) 0º is
located at the front of the
mic’s capsule; (b) graphic
representation of a typical
omnidirectional pickup
pattern.
FIGURE 4.8
Graphic representation of a
typical bidirectional pickup
pattern.

Microphone Characteristics
114
Figure 4.10 graphically illustrates how the acoustical combination (as well as
electrical and mathematical combination, for that matter) of a bidirectional
(pressure-gradient) and omnidirectional (pressure) pickup can be combined to
obtain various other directional pattern types. Actually, an inﬁnite number of
directional patterns can be obtained from this mixture, with the most widely
known patterns being the cardioid, supercardioid and hypercardioid polar patterns
(Figure 4.11).
FIGURE 4.9
Sound sources on-axis and
90º off-axis at the ribbon’s
diaphragm. (a) The ribbon is
sensitive to sounds at the
front and (b) at the rear. 
(c) While sound waves from
the sides (90º and 270º) off-
axis are canceled.
FIGURE 4.10
Directional combinations of
various bidirectional and
non-directional pickup
patterns.
FIGURE 4.11
Various polar patterns with
output sensitivity plotted
over the angle of incidence.
Often, dynamic mics achieve a cardioid response (named after its heart-shaped
polar chart, as shown in Figure 4.12) by incorporating a rear port into their
design. This port serves as an acoustic labyrinth that creates an acoustic resistance
(delay). In Figure 4.13a, a dynamic pickup having a cardioid polar response is
shown receiving an on-axis (0º) sound signal. In effect, the diaphragm receives

two signals: the incident signal, which arrives from the front, plus an acoustically
delayed rear signal. In this instance, the on-axis signal exerts a positive pressure
on the diaphragm and begins its travels 90º to a port located on the side of
the pickup. At this port, the signal is delayed by another 90º (using an internal,
acoustically resistive material or labyrinth). In the combined time it takes for
the delayed signal to reach the rear of the diaphragm (180º), the on-axis signal
moves on to the negative (180º) portion of its acoustic cycle and then begins
to exert a negative pressure on the diaphragm (pulling it outward). Since the
delayed rear signal is also 180º out-of-phase at the rear of the diaphragm, it
will begin to push it back outward, resulting in a reinforced positive output
signal. In fact, this signal is actually increased in level due to the combined
signals.
115
Microphones: Design and Application  CHAPTER 4
FIGURE 4.12
Graphic representation of a
typical cardioid pickup
pattern.
Conversely, when a sound arrives at the rear of the mic, it begins its trek around
to the capsule’s front. As the sound travels 90º to the side of the pickup, it is
again delayed by another 90º before reaching the rear of the diaphragm. During
this same delay period, the sound continues its journey around to the front of
the mic. Since the acoustic pressures at the diaphragm’s front and rear sides are
equal and opposite in pressure, the diaphragm will be simultaneously pushed
inward and outward with equal force, resulting in little or no movement and
therefore will have little or no output signal (Figure 4.13b). The attenuation of
such an off-axis signal, with respect to an equal on-axis signal, is known as its
front-to-back discrimination and is rated in decibels.
FIGURE 4.13
The directional properties of
a cardioid microphone. (a)
Signals arriving at the front
(on-axis) of the diaphragm
will produce a full output
level. (b) Signals arriving
from the rear of the
diaphragm (90º – 90º = no
output) will cancel each
other out, resulting in a
greatly reduced output.

Microphone Characteristics
116
By mounting two capsules back-to-back on a central backplate, a condenser mic
can be electrically selected to switch from one polar pattern to another. For
example conﬁguring these dual-capsule systems electrically in phase will create
an omnidirectional pattern, while conﬁguring them out of phase results in a
bidirectional pattern. A number of intermediate patterns (such as cardioid and
hypercardioid) can be created by electrically varying between these two polar
states (in either continuous or stepped degrees), as was seen earlier in Figure
4.10.
Frequency Response
The on-axis frequency-response curve of a microphone is the measurement of its
output over the audible frequency range when driven by a constant, on-axis
input signal. This response curve (which is generally plotted in output level
[dB] over the 20 to 20,000Hz frequency range) will often yield valuable
information and can give clues as to how a microphone will react at speciﬁc
frequencies.
A mic that’s designed to respond equally to all frequencies is said to exhibit a
ﬂat frequency response (shown as the curve in Figure 4.14a). Others can be
made to emphasize or de-emphasize the high-, mid- or low-end response of
the audio spectrum (shown as the boost in the high-end curve in Figure 4.14b)
so as to give it a particular sonic character. The solid frequency-response curves
(as shown in both parts a and b) were measured on-axis and exhibit an
acceptable response. However, the same mics might exhibit a “peaky” or erratic
curve when measured off-axis (shown as the dotted curves). These signal
colorations could affect their sound when operating in an area where off-axis
sound (in the form of leakage) arrives at the pickup. Such colorations will often
result in a tone quality change, when the leaked signal is mixed in with other
properly miked signals.
FIGURE 4.14
Frequency response curves:
(a) response curve of the
AKG C460B/CK61 ULS; (b)
response curve of the AKG
D321. (Courtesy of AKG
Acoustics GmbH.,
www.akg-acoustics.com)
It’s extremely important to keep in mind that there are many other variables
that will determine how a mic will sound, some of which have no measurement
standards. Some of the most coveted mics have response characteristics that are
far from being ﬂat. They might have a high-end peak, that helps to give them
a “present sound,” or it could have other personalities that make it perfect for
a speciﬁc application. In short, looking at the specs can be helpful, but there is
never a substitute for listening and making decision based on what you hear
with your own ears.

117
Microphones: Design and Application  CHAPTER 4
Transient Response
A signiﬁcant piece of data (which currently has no accepted standard of measure)
is the transient response of a microphone (Figure 4.15). Transient response is the
measure of how quickly a mic’s diaphragm will react when it is hit by an
acoustic wave front. This ﬁgure varies wildly among microphones and is a major
reason for the difference in sound quality among the three pickup types. For
example, the diaphragm of a dynamic mic can be quite large (up to 2.5 inches).
With the additional weight of the coil of wire and its core, this combination
can be a very large mass when compared to the power of the sound wave that
drives it. Because of this, a dynamic mic can be very slow in reacting to a
waveform, often giving it a rugged, gutsy, and less accurate sound. By
comparison, the diaphragm of a ribbon mic is much lighter, so its diaphragm
can react more quickly to a sound waveform, resulting in a clearer, more present
sound. The condenser pickup has an extremely light diaphragm, which varies
in diameter from 1 inch to less than 0.25 inch and has a thickness of about
0.0015 inch. This means that the diaphragm offers very little mechanical
resistance to a sound-pressure wave, allowing it to accurately track the wave
over the entire frequency range—potentially giving it a present and clear sound.
FIGURE 4.15
Transient response
characteristics of a
percussive woodblock using
various microphone types:
(a) Shure SM58 dynamic; 
(b) RCA 44BX ribbon; 
(c) AKG C3000 condenser.
Output Characteristics
A microphone’s output characteristics refer to its measured sensitivity, equivalent
noise, overload characteristics, impedance and other output responses.
SENSITIVITY RATING
A mic’s sensitivity rating is the output level (in volts) that a microphone will
produce, given a speciﬁc and standardized acoustic signal at its input (rated in
dB SPL). This ﬁgure will specify the amount of ampliﬁcation that’s required to
raise the mic’s signal to line level (often referenced to –10 dBv or +4 dBm) and
allows us to judge the relative output levels between any two mics. A microphone
with a higher sensitivity rating will produce a stronger output signal voltage
than one with a lower sensitivity.

Microphone Characteristics
EQUIVALENT NOISE RATING
The equivalent noise rating of a microphone can be viewed as a device’s electrical
self-noise. It is expressed in dB SPL or dBA (a weighted curve) as a signal that
would be equivalent to the mic’s self-noise voltage. As a general rule, the mic
itself doesn’t contribute much noise to a system when compared to the mixer’s
ampliﬁcation stages, the recording system or media (whether analog or digital).
However, with recent advances in mic preamp/mixer technologies and overall
reductions in noise levels produced by digital systems, these noise ratings have
become increasingly important. Interestingly enough, the internal noise of a
dynamic or ribbon pickup is actually generated by the electrons that move within
the coil or ribbon itself. Most of the noise that’s produced by a condenser mic
is generated by the built-in preamp. It almost goes without saying that certain
microphone designs will have a higher degree of self-noise than will others;
thus, care should be taken in your microphone choices for critical applications
(such as with classical recording or ﬁlm production techniques).
OVERLOAD CHARACTERISTICS
Just as a microphone is limited at low levels by its inherent self-noise, it’s also
limited at high sound-pressure levels (SPLs) by overload distortion. In terms of
distortion, the dynamic microphone is an extremely rugged pickup, often capable
of an overall dynamic range of 140 dB. Typically, a condenser microphone 
won’t distort, except under the most severe sound-pressure levels; however, the
condenser system differs from the dynamic in that at high acoustic levels the
capsule’s output might be high enough to overload the mic’s preampliﬁer. To
prevent this, most condenser mics offer a switchable attenuation pad that
immediately follows the capsule output and serves to reduce the signal level
before the preamp’s input, thereby reducing or eliminating overload distortion.
When inserting such an attenuation pad into the circuit, keep in mind that the
mic’s signal-to-noise ratio will be degraded by the amount of attenuation;
therefore, it’s always wise to remove the inserted pad when using the microphone
under normal level conditions.
MICROPHONE IMPEDANCE
Microphones are designed to exhibit different output impedances. Output
impedance is a rating that’s used to help you match the output resistance of
one device to the rated input resistance requirements of another device (so as
to provide the best-possible level and frequency response matching).
Impedance is measured in ohms (with its symbol being Ω or Z). The most
commonly used microphone output impedances are 50, 150 and 250 Ω (low)
and 20 to 50 k Ω (high). Each impedance range has its advantages. In the past,
high-impedance mics were used because the input impedances of most tube-
type ampliﬁers were high. A major disadvantage to using high-impedance mics
is the likelihood that their cables will pick up electrostatic noise (like those
caused by motors and ﬂuorescent lights). To reduce such interference, a shielded
118

cable is necessary, although this begins to act as a capacitor at lengths greater
than 20 to 25 feet, which serves to reduce much of the high-frequency
information that’s picked up by the mic. For these reasons, high-impedance
microphones are rarely used in the professional recording process.
Most modern-day systems, on the other hand, are commonly designed to accept
a low-impedance microphone source. The lines of very-low-impedance mics
(50 Ω) have the advantage of being fairly insensitive to electrostatic pickup.
They are, however, sensitive to induced hum pickup from electromagnetic ﬁelds
(such as those generated by AC power lines). This extraneous noise can be 
greatly reduced through the use of a twisted-pair cable, because the interference
that’s magnetically induced into the cable will ﬂow in opposite directions along
the cable’s length and will cancel out at the console or mixer’s balanced
microphone input stage. Mic lines of 150 to 250 Ω are less susceptible to 
signal losses and can be used with cable lengths of up to several thousand feet.
They’re also less susceptible to electromagnetic pickup than the 50 Ω lines but
are more susceptible to electrostatic pickup. As a result, most professional mics
operate with an impedance of 200 Ω, using a shielded twisted-pair cable to
reduce noise.
A number of high-end pre-amps that are now on the market offer a variable
input impedance control, allowing the preamp to match its impedance to the
mic’s design characteristics. This can have an effect over the overall sound and
operating characteristics of the pickup.
Balanced/Unbalanced Lines
In short, a balanced line uses three wires to properly carry audio. Two of the
wires are used to independently carry the audio signal, while a third lead is
used as a neutral ground wire. From a noise standpoint, whenever an electrostatic
or electromagnetic signal is induced across the audio leads, it will be induced
into both of the two audio leads at an equal level and polarity (Figure 4.16a).
Since the input of a balance device will only respond to the alternating voltage
potentials between the two audio leads, the unwanted noise (which has the
same polarity) will be canceled out.
Several connector types are used to route analog, balanced signals between
devices:
n XLR connectors (Figures 4.16b and c) are most commonly used to 
connect microphones to preamp, console and interface systems. They 
are also used to connect line-level connections between professional 
effects and audio systems devices. The two-conductor, balanced, XLR
connector and cable speciﬁes pin 2 as being positive (+ or hot), pin 3 as
being negative (– or neutral), with the cable ground being connected to
pin 1.
n 1/4” balanced connecters (Figure 4.16d) are used to interface between both
professional and semi-pro equipment at line-level. These connectors are
119
Microphones: Design and Application  CHAPTER 4

Microphone Characteristics
120
also sometimes used to provide a line-level balanced or unbalanced
connection. The 1/4” balanced connector and cable speciﬁes the tip as
being positive (+ or hot), the middle sleeve as being negative (– or neutral),
with the cable ground being connected to the connector’s shield.
n TT (or bantam) connectors/cables allow balanced lines to be plugged into
a patch bay for fast and easy access to the various line-level devices within
a production facility.
FIGURE 4.16
Wiring detail of a balanced
microphone cable (courtesy
of Loud Technologies Inc.,
www.mackie.com): (a)
wiring circuit, where the
induced signals travel down
the wires in equal polarities
that cancel at the
transformer, whereby the
AC audio signals are of
opposing polarities that
generate an output signal;
(b) diagram for wiring a
balanced microphone (or
line source) to a balanced
XLR connector; (c) physical
XLR connection male/female
drawings; (d) physical and
diagram wiring for a
balanced 1/4-inch phone
connector.
If the hot and neutral pins of balanced mic cables are haphazardly pinned in
a music or production studio, it’s possible that any number of mics (and other
equipment, for that matter) could be wired in opposite, out-of-phase polarities.
For example, if a single instrument were picked up by two mics using two
improperly phased cables, the instrument might totally or partially cancel when
mixed to mono. For this reason, it’s always wise to use a phase tester or volt-
ohm meter to check the cable wiring throughout a pro or project studio complex.
High-impedance mics and most line-level instrument lines use unbalanced lines
(Figure 4.17) to transmit signals from one device to another. In an unbalanced
FIGURE 4.17
Unbalanced microphone
circuit (courtesy of Loud
Technologies Inc.,
www.mackie.com): (a)
diagram for wiring an
unbalanced microphone (or
line source) to a balanced
XLR connector; (b) diagram
for wiring an unbalanced
1/4-inch phone connector;
(c) physical and diagram
wiring for an unbalanced
1/4-inch phone connector;
(d) diagram for wiring an
unbalanced phono (RCA)
connector.

circuit, a single signal lead carries a positive current potential to a device, while
a second, grounded shield (which is tied to the chassis ground) is used to
complete the circuit’s return path. When working at low signal levels (especially
at mic levels), any noises, hums, buzzes or other types of interference that are
induced into the signal path will be ampliﬁed along with the input signal.
MICROPHONE PREAMPS
Since the output signals of most microphones are at levels far too low to drive
the line-level input stage of most recording systems, a mic preampliﬁer must
be used to boost its signal to acceptable line levels (often by 30 to 70 dB). With
the advent of improved technologies in analog and digital console design, hard-
disk recorders, DAWs, signal processors and the like, low noise and distortion
ﬁgures have become more important than ever. In this day and age, most of
the mic pres (pronounced “preeze”) that are designed into an audio interface
or production console are capable of providing a professional, high-quality
sound. It’s not uncommon, however, for an engineer, producer or artist to prefer
a preamp which has a personal and “special sound” that can be used in a critical
application to produce just the right tone for a particular application. In such
a case, an outboard mic preamp might be chosen instead (Figure 4.18) for its
characteristic sound, low noise or special distortion specs. These devices might
make use of tube, FET and/or integrated circuit technology, and offer advanced
features in addition to the basic variable input gain, phantom power and high-
pass ﬁlter controls. As with most recording tools, the sound, color scheme, retro
style, tube or transistor type and budget level are up to the individual, the
producer and the artist—it’s completely a matter of personal style and taste,
and this includes choosing to use the high-quality preamps that are built into
your interface/mixer.
121
Microphones: Design and Application  CHAPTER 4
FIGURE 4.18
Outboard microphone
preamplifier examples. 
(a) PreSonus DigiMAX D8 
8-channel microphone
preamp with lightpipe.
(Courtesy of Presonus 
Audio Electronics, Inc.,
www.presonus.com) 
(b) Rupert Neve Designs
Portico 5024 4-channel mic
preamp. (Courtesy of Rupert
Neve Designs LLC,
www.rupertneve.com) 
(c) Universal Audio 2–610
Dual Channel Tube
Preamplifier. (Courtesy of
Universal Audio,
www.uaudio.com © 2017
Universal Audio, Inc. All
rights reserved. Used with
permission)

Microphone Techniques
MICROPHONE TECHNIQUES
Most microphones have a distinctive sound character that’s based on its speciﬁc
type and design. A large number of types and models can be used for a variety
of applications, and it’s up to the engineer to choose the right one for the job.
Over the years, I’ve come to the realization that there are two particular paths
that one can take when choosing the types and models of microphones for a
studio’s production toolbox. These can be placed into two categories:
n Select a limited range of mics that are well suited for a wide range of
applications
n Acquire a larger collection of mics that are commonly perceived as being
individually suited for a particular instrument or situation
n Or both
The ﬁrst approach is ideal for the project studio and those who are just starting
out and are on a limited budget. This is also common practice among seasoned
professionals who swear by a limited collection of their favorite mics that are
chosen to cover a wide range of applications. These dynamic, ribbon and/or
condenser mics can be used both in the project studio and in the professional
studio to achieve the best possible sound on a budget.
The second approach (I often refer to it as the “Alan Sides” approach—go
ahead, Google him) is better suited to the professional studio (and to personal
collectors). This path is taken for those who have a need or desire to amass a
large “dream collection” of carefully chosen mics that can be surgically used
for particular applications. In the end, both approaches have their merits—
indeed, it’s usually wise to keep an open mind and choose a range of mic types
that best ﬁt your needs, budget and personal style. I’d like to add, however, that
you might consider buying a matched pair of mics, especially at ﬁrst. This opens
up your options for placing two mics in the room or on an instrument, so you
can record in stereo (overhead drums, stereo XY mics on a guitar, placing room
mics, etc., come to mind just for starters).
Choosing the appropriate mic, however, is only half the story. The placement
of a microphone will often play just as important a role toward getting the right
sound, and it is one of the engineer’s most valued tools. Because mic placement
is an art form, there is no right or wrong. Placement techniques that are currently
considered “bad” might easily be accepted as being standard practice ﬁve years
from now, and as new musical styles develop, new recording techniques will
also tend to evolve, helping to breathe new life into music and production. The
craft of recording should always be open to change and experimentation—two
of the strongest factors that keep the music and the biz of music alive and fresh.
Here are several pieces of practical advice that can help get you started.
n If you’re recording a musician (especially an experienced one) you might
consider asking him or her how they’ve been recorded in the past. Do
they have a favorite mic or technique that’s often worked best for them?
122

This tactic can help to put the artist at ease and give you insights into new
studio miking techniques that can be a helpful production and educational
tool.
n Think carefully before “printing” a recorded signal directly to a track. The
recording of an instrument with effects or dynamics can’t be undone at a
later time—so, unless you’re absolutely sure, it’s often wise to add effects
to the track later during mixdown, or as an option, you could print the
signal to two tracks, one with and one without effects.
n Make use of the various aspects of a room’s acoustics when recording an
instrument. This includes the use of distance as a tool for changing the
size and character of an instrument or group (which could be recorded
to separate tracks for later blending within the mix).
n Although there are no rules for this type of creativity, you might keep in
mind the instrument and traditional pickup style, so as to not go too far
aﬁeld from the expected norm (unless you want to break the rules to lay
a new path).
Other Microphone Pickup Issues
Before we move into the realm of microphone placement and pickup techniques,
let’s take a quick look at a few issues that are common to many pickups and
placement situations. These are:
n Low-frequency rumble
n Proximity effect
n Popping
n Off-axis pickup
LOW-FREQUENCY RUMBLE
When using a mic in the studio or on-location, rumble (low-frequency, high-
level vibrations that occur in the 3 to 25Hz region) can easily be transmitted
from the ﬂoor of a studio, hall or unsupported ﬂoor—through the mic stand
and directly to the mic. Sources such as passing trucks, air conditioners, subways
or fans can be reduced or eliminated in a number of ways, such as:
n Using a shock mount to isolate the mic from the vibrating surface and
ﬂoor stand.
n Choosing a mic that displays a restricted low-frequency response.
n Restricting the response of a wide-range mic by using a low-frequency roll-
off ﬁlter.
PROXIMITY EFFECT
Another low-frequency phenomenon that occurs in most directional mics is
known as proximity effect. This common effect causes an increase in bass response
123
Microphones: Design and Application  CHAPTER 4

Microphone Techniques
124
whenever a directional mic is brought within 1 foot of the sound source. This
bass boost (which is often most noticeable on vocals) proportionately increases
as the distance decreases. To compensate for this effect (which is somewhat
greater for bidirectional mics than for cardioids), a low-frequency roll-off ﬁlter
switch (which is often located on the microphone body) can be used. If none
exists, an external roll-off or equalizer can be used to reduce the low end.
Finally, the directional mic can be swapped with an omni-direction one—yes,
this actually works! Any of these tools can be used to help restore the bass
response to a ﬂat and natural-sounding balance.
On a more positive note, this increase in bass response has long been appreciated
by vocalists and DJs for their ability to give a full, “larger-than-life” quality to
voices that are otherwise thin. In many cases, the use of a directional mic has
become an important part of the engineer, producer and vocalist’s toolbox.
1. Pull out omnidirectional, cardioid and
bidirectional mics (or one that can be switched
between these patterns).
2. Move in on each mic pattern type from distances
of 3 feet to 6 inches (being careful of volume
levels and problems that can occur from
popping).
3. Does the bass response increase as the distance
is decreased with the cardioid? With the
bidirectional? With the omni?
Try This: Proximity Effect
D I Y
 do  it  yourself
POPPING
Another annoying problem that’s generally associated with directional mics that
are exposed to bind or breath blasts is popping. Of course we’ve all experienced
talking into a mic that blasts a loud, low-frequency noise whenever we say or
sing the letter “p” or “b.” The best ways to reduce or eliminate popping is:
n To slip a pop ﬁlter over the head of a mic, which provides resistance to
breath and wind blasts
n To place a blast/pop shield between the artist and the mic
n To place the mic slightly off-axis (off center) to the singer
n To replace the directional microphone with an omnidirectional mic when
working at close distances
OFF-AXIS PICKUP
As we’ve previously seen, when sound arrives at a directional microphone from
off-axis (from the sides, rear, etc.), there are degrees of reduction in level that

125
Microphones: Design and Application  CHAPTER 4
occur due to acoustical or electrical delay phase cancellation at the diaphragm.
These cancellations will vary in level depending upon the emanating direction
of the sound source—meaning that a mic’s off-axis frequency response may
differ from its on-axis response.
This difference in response might not be a problem (or at least something to
watch out for) if it were not for acoustic leakage that arrives off-axis at a pickup
from another source that’s also miked (Figure 4.19). If this leakage is loud
enough, the off-axis pickup would combine with the other instrument’s pickup
to change the sound’s overall tone color—particularly if the original and leaked
sounds are mixed to a single channel. None of this is meant to scare you; it’s
simply to point out that you should be aware of what can happen when an
instrument or sound source is picked up by multiple mics.
FIGURE 4.19
Off-axis leakage can
combine with another direct
mic pickup to alter the tonal
character of a sound.
Pickup Characteristics as a Function of Working Distance
In studio and sound-stage recording, four fundamental styles of microphone
placement are directly related to the working distance of a microphone from
its sound source. These extremely important placement styles are as important
as any tool in the toy box:
n Close miking
n Distant miking
n Accent miking
n Ambient miking
One of the top engineers in the world (Al Schmitt)
makes extensive use of leakage to create a live,
present, “you-are-there” feel to his recordings, often
choosing to use omnidirectional mics (which actually
have far less off-axis coloration, due to the fact that
no directional phase cancellation occurs). Of course,
he uses some of the best musicians in the world and
records in Capitol Studios, but that doesn’t mean
that you can’t get amazing results by experimenting
with the artist and room using alternative mic
techniques.

Microphone Techniques
126
It’s very interesting that with the resurgence of retro gear, the industry is ﬁnally
coming around to the idea that changing the working distance, as well as
making use of a room’s acoustics as an important part or “effect” within the
recording process. As one recording artist put it:
“ You can use the room to change the era of the sound. If you want a modern vocal
sound, get right up on the mic. If you want a Motown vocal sound, step away from
the mic three feet and sing harder.”
—Ben Harper
CLOSE MICROPHONE PLACEMENT
When a close microphone placement is used, the mic is often positioned about
1 inch to 3 feet from a sound source. This commonly used technique generally
yields two results:
n It creates a tight, present sound quality
n It effectively excludes the acoustic environment from being picked up
Because sound diminishes with the square of its distance from the sound source,
a sound that originates 3 inches from the pickup will be much higher in level
than one that originates 6 feet from the mic (Figure 4.20). Therefore, whenever
close miking is used, only the desired on-axis sound will be recorded; extraneous,
distant sounds (for all practical purposes) won’t be picked up. In effect, the
distant pickup will be masked by the closer sounds and/or will be reduced to
such a relative level that it’s well below the main pickup signal.
FIGURE 4.20
Close miking reduces the
effects of picking up the
distant acoustic
environment.
Probably the best analogy that can be given at this point is that of a microscope
or an eyepiece. For this example, let’s place a guitar player in the middle of a
studio (feel free to “Try this” for yourself). If we look through our eyepiece from
a distance of 6 feet (about 2 meters) at the guitar, we’ll see the entire instrument
and probably a good amount of the player. As we move further in, we’ll see
less and less of the player and even the guitar. As we move closer into the

instrument, we’ll start to see only certain parts of the instrument (the strings,
the resonance hole, the players ﬁngers over the strings, etc.). Of course, as we
move further in only a microscopic part in the instrument will be seen.
Conversely, as we move out again past the 2 meter point, we’ll start seeing more
and more of the room, until the player, instrument and the room all combine
together to make up our overall view.
Of course, this visual analysis is directly analogous to how a mic pickup will
hear the combined overall room/instrument sound—and will then sonically
“zoom in” on parts of the instrument, as the pickup is moved closer to the
instrument and then closer still to speciﬁc parts of the instrument.
This analogy works for all types of acoustic spaces and instruments—I urge you
put a single mic and player out in the room and try zooming the pickup in and
out for yourself.
127
Microphones: Design and Application  CHAPTER 4
1. Mic an acoustic instrument (such as a guitar or
piano) at a distance of 1 to 3 inches.
2. Move (or have someone move) the mic over the
instrument’s body as it’s being played, while
listening to variations in the sound. Does the
sound change? What are your favorite and least
favorite positions?
3. Now pull the mic back by a foot or more—then to
a distance of 6 feet (2m) or more. How does the
overall pickup change?
Try This: Close Mic Experimentation
D I Y
 do  it  yourself
Because close mic techniques commonly involve distances of 1 to 6 inches (2.5
to 15cm), the tonal balance (timbre) of an entire sound source often can’t be
picked up; rather, the mic might be so close to the source that only a small
portion of the surface is actually picked up. This gives it a tonal balance that’s
very area speciﬁc (much like hearing the focused parts of an instrument through
an acoustic microscope in our earlier explanation). At these close distances,
moving a mic by only a few inches can easily change the pickup tonal balance.
If this occurs, try using one or more of the following remedies:
n Move the microphone along the surface of the sound source until the
desired balance is achieved.
n Place the mic farther back from the sound source to allow for a wider
angle (thereby picking up more of the instrument’s overall sound).
n Change the mic.
n Equalize the signal until the desired balance is achieved.

Microphone Techniques
128
Leakage
Whenever an instrument’s mic is placed at enough of a distance to also pick
up the sound of a nearby instrument, a condition known as leakage will occur
(Figure 4.21). Whenever a signal is picked up by both its intended mic and a
nearby mic (or mics), it’s easy to see how the signals could be combined together
within the mixdown process. Whenever this occurs, level and phase cancellations
can sometimes make it more difﬁcult to have control over the volume and tonal
character of the involved instruments within a mix.
FIGURE 4.21
Leakage occurs due to the
indirect pickup of a distant
signal source.
In order to avoid the problems that can be associated with leakage, try the
following:
n Place the mics closer to their respective instruments (Figure 4.22a).
n Use directional mics.
n Place an acoustic barrier (known as a ﬂat, gobo, or divider) between the
instruments (Figure 4.22b). Alternatively, mic/instruments can be sur -
rounded on several sides by sound bafﬂes and (if needed) a top can be
draped over them.
n Spread the instruments farther apart.
n An especially loud (or quieter) instrument can be isolated by putting it
in an unused iso-room or vocal or instrument booth. Electric instrument
amps that are played at high volumes can also be recorded in an isolated
room or area. The amp and mic can be covered with a blanket, iso-box
or other sound-absorbing material, so that there’s a clear path between
the ampliﬁer and the mic.
n Separation can be achieved by plugging otherwise loud electronic
instruments directly into the console via a direction injection (DI) box,
thereby bypassing the miked amp.

Obviously, these isolation examples only hint at the number of possibilities
that can occur during a session. For example, you might choose not to isolate
the instruments and instead, place them in an acoustically “live” room. This
approach will require that you carefully place the mics in order to control
leakage; however, the result will often yield a live and present sound. As an
engineer, producer and/or artist, the choices belong to you. Remember, the idea
is to prepare your session, work out the kinks beforehand and simplify tech -
nology as much as possible in the studio because Murphy’s law is always alive
and well in any production room.
Whenever individual instruments are being miked close (or semi-close), it’s
generally wise to follow the 3:1 distance rule.
129
Microphones: Design and Application  CHAPTER 4
FIGURE 4.22
Two methods for reducing
leakage: (a) place the
microphones closer to their
sources; (b) use an acoustic
barrier to reduce leakage.
To reduce leakage and maintain phase integrity, this
rule states that for every unit of distance between a
mic and its source, a nearby mic (or mics) should be
separated by at least three times that distance
(Figure 4.23).
3:1 Distance Rule
FIGURE 4.23
Example of the 3:1
microphone distance rule:
“For every unit of distance
between a mic and its
source, a nearby mic (or
multiple mics) should be
separated by at least three
times that distance.”

Microphone Techniques
Some err on the side of caution and avoid leakage even further by following a
5:1 distance rule. As always, experience will be your best teacher. Although the
close miking of a sound source offers several advantages, a mic should be placed
only as close to the source as is necessary, not as close as possible. Unless care
is taken and careful experimentation is done, miking too close can color the
recorded tone quality of a source.
Again, it should be noted that a bit of “bleed” (a slang word for leakage)
between mics just might be a good thing. With semi-distant and even multiple
mics that are closely spaced, the pickup of a source by several pickups can add
a sense of increased depth and sonic space. Having an overall distant set of
mics in the studio can add a dose of natural ambience that can actually help
to “glue” a mix together. The concept of minute phase cancellations and leakage
in a mix isn’t always something to be feared; it’s simply important that you be
aware of the effects that it can have on a mix and use that knowledge to your
advantage.
In addition to all of the above considerations, the placement of musicians and
instruments will often vary from one studio and/or session to the next because
of the room, people involved, number of instruments, isolation (or lack thereof)
among instruments, and the degree of visual contact that’s needed for creative
communication. If additional isolation (beyond careful microphone placement)
is needed, ﬂats and bafﬂes can be placed between instruments in order to
prevent loud sound sources from spilling over into other open mikes.
Alternatively, the instrument or instruments could be placed into separate
isolation (iso) rooms and/or booths, or they could be overdubbed at a later
time.
During a session that involves several musicians or more, the setup should allow
them to see and interact with each other as much as possible. It’s extremely
important that they be able to give and receive visual cues, so they can better
“feel the vibe.” The instrument/mic placement, bafﬂe arrangement, and possibly
room acoustics (which can often be modiﬁed by placing absorbers in the room)
will depend on the engineer’s and artists’ personal preferences, as well as on
the type of sound the producer wants—so, it’s always a good idea to consult
with the producer and/or band before any studio setup is attempted.
Alternatively, it should be pointed out some of the world’s top engineers/
producers would also say that “leakage can be your friend” in certain
circumstances. This hints at the idea that when the “good rule” is in full effect
and the room’s acoustics match the style of the recording, pulling the care-
fully chosen mics away from the sound source to pick up more of the room
(along with other instrument leakage), just might add life to the overall sound.
A few examples of this that come to mind, is the use of leakage that’s sometimes
used to add “life” to Al Schmitt's jazz recordings that are made at Capitol
Records’ famous studios in LA. Another rather novel approach to positive leakage
was used by Bruce Swedien (Michael Jackson’s engineer) when he would set
up mics in the studio for 12 strings, have 6 players sit in the ﬁrst few rows (with
130

131
Microphones: Design and Application  CHAPTER 4
all the mics open), then have the 6 move to the last rows (again, with all mics
open) and then combine the two overdubs, allowing the leakage to add to the
overall liveness of the sound. Should Micheal J. be required to layer his voice
in multiple layers, Bruce would have him move his position from the mic, so
that the time differences would make for a more life-like combined track. These
are but a few of the many tricks that can be applied to add “life” to a recording
and mix.
RECORDING DIRECT
Should there be a problem with miking an instrument, due to leakage, sonic
preference or bad acoustics in a room, the signal of an electric or electronic
instrument (guitar, keyboard, etc.) can alternatively be “directly injected” into
a console, recorder or DAW without the use of a microphone. This option often
produces a cleaner, more present sound by bypassing the distorted components
of a head/amp combination. In the project or recording studio, the direct injection
(DI) box (Figure 4.24) serves to interface an instrument with an analog output
signal to a console or recorder in the following ways:
n It reduces an electric or electronic instrument’s line-level output signal to
mic level for direct insertion into the console’s mic input jack.
n It changes an instrument’s unbalanced, high-source impedance line to a
balanced, low-source impedance signal that’s needed by the console’s
input stage.
n It often can electrically isolate the audio signal paths between the
instrument and mic/line preamp stages (thereby reducing the potential
for ground-loop hum and buzzes).
FIGURE 4.24
The DI box: (a) radial 
JDI passive direct box.
(Courtesy of Radial
Engineering,
www.radialeng.com); 
(b) Rupert Neve Designs
RNDI Active Transformer
Direct Interface. (Courtesy
of Rupert Neve Designs,
www.rupertneve.com)
Most commonly, the instrument’s output is plugged directly into the DI box
(where it’s stepped down in level and impedance), and the box’s output is then
fed into the mic pres of a console or DAW. If a “dirtier” sound is desired, certain
boxes will allow high-level input signals to be taken directly from the amp’s
speaker output jack. It’s also not uncommon for an engineer, producer and/or

Microphone Techniques
artist to combine the punchy, full sound of a mic with the present crispness of
a direct sound. These signals can then be combined onto a single track or
recorded to separate tracks (thereby giving more ﬂexibility in the mixdown stage).
The ambient image can be “opened up” even further by mixing a semi-distant
or distant mic (or stereo pair) with the direct (and even the close miked amp)
signal. This ambient pickup can be either mixed into a stereo ﬁeld or at the rear
of a surround ﬁeld to ﬁll out the sound.
When recording a guitar, the best tone and lowest hum pickup for a direct
connection occurs when the instrument volume control is fully turned up.
Because guitar tone controls often use a variable treble roll-off, leaving the tone
controls at the treble setting and using a combination of console EQ and
different guitar pickups to vary the tone will often yield the maximum amount
of control over the sound. Note that if the treble is rolled off at the guitar,
boosting the highs with EQ will often increase pickup noise.
DISTANT MICROPHONE PLACEMENT
Within the study of modern recording and production techniques, one of the
lesser understood aspects of miking is distant placement. This is a shame, as
having an insight into distant microphone techniques unlocks the key to a wide
range of tools and tricks that can be useful both in the studio and in the concert
hall.
Let’s begin our study into distance by taking a visual and experimental approach
to the subject:
n If we were to look through a viewﬁnder that was placed very, very close
to an instrument, we might only see an area of black.
n If we pull back by only 2 inches, we might see some wood, but still not
understand what we’re looking at. We could start moving around and see
that we’re looking at an instrument, but we’ll only be getting a small part
of the picture.
n Pulling back to 4 inches, we’ll ﬁnally see that we were looking into the
sound-hole of an acoustic guitar. Taking some time to look around, we
might get more information as to what type, how many strings, etc.
n Pulling back to a foot, we can start seeing more of the instrument and we
might get some sense of its size and type. We might even get a glimpse
of the artist.
n Moving to a distance of 4 feet, we can ﬁnally see the whole instrument
and much of the artist. Maybe we can see part of the room that it is being
played in.
n Finally, moving back to a distance of 12 or more feet, we can see much
of the room, the artist and the instrument. In short, we can see “the big
picture.”
132

133
Microphones: Design and Application  CHAPTER 4
As you might have guessed, this same analogy applies to microphone placement
and technique. Moving in close gives us a microscopic sense of the sound of
an instrument, whereby we can move around and listen for just that certain
sound. Pulling back gives us a better overall sense of the sound of the instrument
as a whole. Pulling back to a greater distance begins to introduce the general
acoustics of the room, giving a sense of the instrument in its natural sounding
environment.
Distant miking techniques can be used to serve two functions:
n To combine an actual acoustic environment with close or modern studio
mic techniques to give a sense of acoustic “life” to a studio recording.
n To capture the recording environment as a whole. That’s to say that the
instrument or ensemble will be captured along with its natural acoustic
environment to create a combined representation of what the listener
would hear in that room.
Distant miking is often used in classical and other traditional styles of recording
to pick up large instrumental ensembles (such as a symphony orchestra or choral
ensemble). In this application, the pickup will largely rely on the acoustic
environment to help achieve a natural, ambient sound. The mic should be placed
at a distance so as to strike an overall balance between the ensemble’s direct
sound and the room’s acoustics. This approach will result in a balance that’s
determined by a number of factors, including the size of the sound source, its
overall volume level, mic distance and placement as well as the acoustic and
reverberant characteristics of the room.
With distant microphone placement (Figure 4.25), multiple mics can be
positioned within the acoustic space at a distance that will pick up the proper
balance between the instrument/ensemble and the room. Often, these pickups
will be spaced in order to capture a good stereo image of the instrument/
ensemble within the room. Larger ensembles or instruments (such as a pipe
organ) might require a greater spacing in order to best capture the “width” of
the event. In other cases, the mics might be placed in a coincident fashion (placed
FIGURE 4.25
One of the “many” possible
distant pickup miking
examples.

Microphone Techniques
very close together in a speciﬁc placement pattern), so as to use directionality
and acoustic levels to capture the width, breadth and imagery of the overall
acoustic sound of being in the room. These coincident stereo and surround
techniques will be covered later in this chapter.
As for microphone placement when using distant techniques, there are several
approaches that can yield natural and excellent results. Let’s start off by taking
a careful look at stereo miking techniques (which are also covered later in this
chapter). An in-depth look at stereo mic techniques is not placed here in the
distant section for the simple reason that stereo mic techniques can be invaluable
(and to many, like myself, indispensible) in all types of close, distant and any
other type of placement settings. Once you’ve looked at stereo and immersive
mic techniques, the next aspect that needs to be considered is the music style,
production approach and application. For example:
n A classical recording might rely strictly on a very simple (but carefully
chosen and laid out) mic choice and placement. This approach assumes
that the recording hall is acoustically matched to your music style, whereby
the musicians and the hall become one perfectly combined sound
experience.
n In a large recording studio room setting, a set of overall distant pickups
might be combined with semi-close (accent) mics that can ﬁll out the
sound and make it more present. This approach is often taken during the
recording of a ﬁlm score, whereby a large room sound needs to be balanced
out with a closer mic technique that can be mixed in a controlled multitrack
setting.
n In a music-for-games setting careful control over the recorded sound 
might be required, allowing sounds to be carefully mixed to match the
game play, while also giving the option to have a distant larger-than-life
music mix.
Room Microphone Placement
Miking a room (room miking) places the pickup at such a distance that the
reverberant character of the room sound will be predominant and can then be
mixed in with the main signal or be recorded to their own tracks for later possible
addition during mixdown. This distant mic technique can be used to enhance
a recording in a number of ways, such as:
n In a studio recording, ambient room microphones can be used in the studio
to add a sense of space or natural acoustics back into the sound.
n In a live concert recording, ambient mics can be placed in a hall to restore
the natural reverberation that is often lost with close miking techniques.
n In a live concert recording, room mics can be placed over the audience to
pick up their reaction and applause.
134
Room Microphone Placement 

135
Microphones: Design and Application  CHAPTER 4
Although overall room miking techniques are often used in classical recording,
it’s never a good idea to underestimate the power and effect that distant miking
can have within a modern production. Recording instruments using standard
close mic techniques deﬁnitely have their own advantages (reduced leakage,
better control over problematic pickups, greater presence and immediacy),
however, these sounds can occasionally end up being a bit too “on-your-face”—
even when artiﬁcial delay, reverb and other effects are added. Adding actual
room sound to the mix can add a degree of realism and depth that often just
can’t be duplicated using artiﬁcial processing.
Room Pickup in the Studio
Placing a distant mic or (even better) a mic pair within a large room or studio
can help bring instruments such as drums, guitars, pianos and other percussion
to life. For example, when recording drums, you might consider placing a stereo
mic pair at a distance within the room, placed fairly high. If the other instruments
(such as bass, guitar, piano) are isolated away—this will give you a distant drum
sound that helps to make the instrument sound really big or even huge! Even
if the instruments are in the same room, this distant pair might just be what’s
needed to give the sound that extra live cohesiveness or live “punch.” Of course,
it goes without saying that these distant room mics will need to be recorded
onto their own separate tracks, thereby allowing decisions to be made later
during mixdown.
Besides the standard, tried-and-true approach to recording that I apply to my
own projects, I’ve come to rely upon several approaches to recording that have
helped me make the best and most open-sounding recordings that I possibly
can. In addition to a deep appreciation for session preparation, here are “my”
personal techniques for approaching a recording:
n I “always” record in stereo (usually XY)—I have practically my entire life.
This approach is done only by a handful of engineers. But the list of those
1. Mic an instrument or its amp (such as an
acoustic or electric guitar) at a traditional
distance of 6 inches to 1 foot.
2. Place a stereo mic pair (in an X/Y and/or spaced
conﬁguration) in the room, placed at 6 to 12 feet
(or further in a large room) away from the
instrument.
3. Mix the two pickup types together. Does it
“open” the sound up and give it more space?
Does it muddy the sound up or breathe new life
into it?
4. If you’re lucky enough to be surround-capable,
place the ambient tracks to the rear. Does it add
an extra dimension of space? Does it alter the
recording’s deﬁnition?
Try This: Ambient Miking
D I Y
 do  it  yourself
Room Pickup in the Studio 

Microphone Techniques
136
who do is impressive. My reasoning is that stereo miking spreads the sound,
giving it a natural, “not coming from one point in the mix” sound. In
short it adds width, depth and ambience to a sound, even when used in
a close mic setting.
n Whenever the instrument is on its own in a room (of any size, including
an iso-room), I will place a distant stereo (and sometimes quad) pair in
the room. These tracks can be mixed in with the instrument during
mixdown to give a larger, fuller, “you-are-there” sound.
n If the instrument has a MIDI out jack I’ll always record that MIDI track
within the DAW’s session. At a later time, I could decide to change the
sound, ﬁx a note or replay the track through a speaker that’s placed in the
studio and re-record it through a huge amp stack. Literally, the sky’s the
limit.
1. Have someone grab their favorite electric
guitar/amp setup and place it in the studio.
2. Place a DI on the guitar and record it direct.
3. Place a close mic at 3–6 inches from the amp
speaker cabinet and send that to a track.
4. Place an ambient mic 6–8 feet from the speaker
and send that to a track.
5. Place a stereo mic pair (XY or Blumlein) in the
room at a good distance and send them to a
stereo track.
6. In mixdown, listen to the pickups individually and
then combine them. How does the sound
develop? How does the room sound affect the
sound?
7. If it’s available, insert an amp plug-in into the
direct track and try various amp combinations.
Does it sound different or better than the actual
amp pickups?
8. If it’s available, insert a room simulation plug-in
into an amp track and blend that in instead of the
actual room mics. How does that affect the
“liveness” of the guitar?
9. Play around with the various combinations and
have fun learning how distance can affect your
overall sound.
Try This: Combining Various Distant/Pickup Techniques
D I Y
 do  it  yourself
The Boundary Effect
If a distant mic is used to pick up a portion of the room sound, placing it at a
random height can result in a hollow sound due to phase cancellations that
occur between the direct sound and delayed sounds that are reﬂected off the
ﬂoor and other nearby surfaces (Figure 4.26). If these delayed reﬂections arrive
at the mic at a time that’s equal to one-half a wavelength (or at odd multiples
thereof), the reﬂected signal will be 180º out of phase with the direct sound.
These reﬂections could produce dips in the signal’s pickup response that might
The Boundary Effect 

137
Microphones: Design and Application  CHAPTER 4
adversely color the signal. Since the reﬂected sound is at a lower level than the
direct sound (as a result of traveling farther and losing energy as it bounces off
a surface), the cancellation will only be partially complete. Raising the mic will
have the effect of reducing reﬂections (due to the increased distances that the
reﬂected sound must travel), while moving the mic close to the ﬂoor will
conversely reduce the path length and raise the range in which the frequency
cancellation occurs. In practice, a height of 1/8 to 1/16 inches will raise the
can cellation above 10 kHz. One such microphone design type, known as a
boundary microphone (Figure 4.27), places an electret-condenser or condenser
diaphragm well within these low height restrictions. For this reason, this mic
type might be a good choice for use as an overall distant pickup, when the mics
need to be out of sight (i.e., when placed on a ﬂoor, wall or large boundary).
FIGURE 4.26
Resulting frequency
response from a
microphone that receives a
direct and delayed sound
from a single source.
FIGURE 4.27
The boundary microphone
system: (a) mic placement;
(b) the PZM-6D boundary
microphone. (Courtesy of
Crown International, Inc.,
www.crownaudio.com)
“Reamping it” in the Mix
Another way to alter the sound of a track that had already been recorded by
injecting a new sense of acoustic space into an existing take is to reamp a track.
The “reamp” process (originally conceived in 1993 by recording engineer 
John Cuniberti and now owned by Radial Engineering; www.radialeng.com)
lets us record a guitar’s signal directly to a track using a DI during the record-
ing session and then play this cleanly recorded track back through a miked
guitar amp/speaker, allowing it to be re-recorded to new tracks at another time
(Figure 4.28).
"Reamping it- in the Mix 

Microphone Techniques
138
FIGURE 4.28
Figure showing how a 
direct recording can be
“reamped” in a studio,
allowing for complete tonal,
mic placement and
acoustical control, after the
fact! (Courtesy of Radial
Engineering,
www.radialeng.com)
The re-recording of an instrument that has already been recorded directly gives
us total ﬂexibility for changing the ﬁnal, recorded amp and mic sound at a later
time. For example, it’s well known that it’s far easier to add an effect to a “dry”
track (one that’s recorded without effects) during mixdown than to attempt to
remove an effect after it’s been printed to track. Whenever reamping is used at
a later time, it’s possible to audition any number of amps, using any number
of effects and/or mic settings, until the desired sound has been found. This
process allows the musician to concentrate solely on getting the best recorded
performance, without having to spend extra time getting the perfect guitar, amp,
mic and room sound. Leakage problems in the studio are also reduced, as no
mikes are used in the process.
Although the concept of recording an instrument directly and playing the track
back through a miked amp at a later time is relatively new, the idea of using a
room’s sound to ﬁll out the sound of a track or mix isn’t. The reamp concept
takes this idea a bit further by letting you go as wild as you like. For example,
you could use the process to re-record a single, close-miked guitar amp and
then go back at a later time and layer a larger stack at a distance. An electronic
guitarist could take the process even further by recording his or her MIDI guitar
directly and to a sequenced MIDI track. In this way, the reamp and synth patch
combinations could be virtually limitless.
1. Have someone grab their favorite electric guitar
and record a track direct, using a DI.
2. After the above recording is done, play the
recorded track back through a guitar amp in the
studio (feel free to turn it up).
3. Place a mic at a distance of 4–8 feet and record
that track (you could also use the various
placement settings that were used in the
previous distant/pickup DIY).
4. When you combine the DI and reamped tracks
together, did that inject new “life” into the track?
5. You might see what happens when you swap
one amp/speaker combo for another and try the
experiment again. The process of reamping in
the studio opens up a lot of post-production
possibilities—have fun!
Try This: Reamp Experiment
D I Y
 do  it  yourself

ACCENT MICROPHONE PLACEMENT
When using distant mic techniques to record an ensemble, sometimes a softer
instrument or one that is to perform an important solo can get lost or slightly
buried within the overall, larger pickup. In such a situation, an accent mic might
be needed (Figure 4.29). This happens by placing a mic at a closer distance to
the solo instrument or simply a softer instrument that needs a little pickup help.
This mic is then carefully mixed in with the overall distant pickups to ﬁll out
the overall sound and bring out the problem instrument.
139
Microphones: Design and Application  CHAPTER 4
FIGURE 4.29
Accent microphone placed
at proper compromise
distance.
As you might expect, care needs to be taken when placing an accent mic, as the
tonal and ambient qualities of a pickup will sound very different when it is
placed close to an instrument versus when it is placed at a distance. For 
example, if a solo instrument within an orchestra needs an extra mic for added
volume and presence, placing the mic too close would result in a pickup that
sounds overly present, unnatural and out of context with the distant, overall
orchestral pickup. To avoid this pitfall, a compromise in distance should be
struck. The microphone should be placed at a reasonably close range to an
instrument or section within a larger ensemble, but not so close as to have an
unnatural sound. The amount of accent signal that’s introduced into the mix
should sound natural relative to the overall pickup, and a good accent mic should
only add presence to a solo passage and not stick out as a separate, identiﬁable
pickup.
Stereo and Immersive Mic Techniques
For the purpose of this discussion, the term stereo and immersive (4.0, 5.1, 7.1
and 9.1 surround sound) miking techniques refers to the use of two or more
microphones, so as to obtain a coherent, multichannel sonic “image” of a
recording. These techniques can be used in close, distant or room miking of
single instruments, vocals, large or small ensembles, within on-location or

Microphone Techniques
140
studio applications—in fact, the only limitation is your imagination. The ﬁve
fundamental multichannel miking techniques are:
n Spaced pair
n X/Y
n M/S
n Decca tree
n Multi array (can be an arrangement of up to 11 mics)
SPACED PAIR
Spaced microphones (Figure 4.30) can be placed in front of an instrument or
ensemble (in a left/right fashion) to obtain an overall stereo image. This tech -
nique places the two mics (of the same type, manufacturer and model) anywhere
from only a few feet to more than 30 feet apart (depending on the size of the
instrument or ensemble) and uses time and amplitude cues in order to create a
stereo image. The primary drawback to this technique is the strong potential for
phase discrepancies between the two channels due to differences in a sound’s
arrival time at one mic relative to the other. When mixed to mono, these phase
discrepancies could result in variations in frequency response and even the partial
cancellation of instruments and/or sound components in the pickup ﬁeld.
FIGURE 4.30
Spaced stereo miking
techniques.
X/Y
X/Y stereo miking is an intensity-dependent system that uses only the cue of
amplitude to discriminate direction. With the X/Y coincident-pair technique
(Figure 4.31), two directional microphones of the same type, manufacture and
model are placed with their grills as close together as possible (without touching)
and facing at angles to each other (generally between 90º and 135º). The
midpoint between the two mics is pointed toward the source, and the mic
outputs are equally panned left and right. Even though the two mics are placed
together, the stereo imaging is excellent—often better than that of a spaced pair.
In addition, due to their proximity, no appreciable phase problems arise. Most
commonly, X/Y pickups use mics that have a cardioid polar pattern, although

141
Microphones: Design and Application  CHAPTER 4
the Blumlein technique is being increasingly used. This technique (which is
named after the unheralded inventor, Alan Dower Blumlein) uses two crossed
bidirectional mics that are offset by 90º to each other. This simple technique
often yields excellent ambient results for the pickup of the overall room
ambience within a studio or concert hall, while also being a good choice for
picking up sources that are placed “in the round.” These ambient pickup styles
can be mixed into a stereo or surround-sound production to provide a natural
reverb and/or ambience.
FIGURE 4.31
X/Y stereo miking patterns:
(a) technique using an X/Y
crossed cardioid pair; 
(b) crossed cardioid pattern;
(c) Blumlein crossed figure-
8 pattern.
Stereo microphones that contain two diaphragms in the same case housing are
also available on the new and used market. These mics are either ﬁxed (generally
in a 90º pattern) or are designed so that the top diaphragm can be rotated by
180º (allowing for the adjustment of various coincident X/Y, Blumlein and M/S
angle conﬁgurations).
M/S
Another coincident-pair system, known as the M/S (or mid-side) technique
(Figure 4.32a), is similar to X/Y in that it uses two closely spaced, matched
pickups. The M/S method differs from the X/Y method, however, in that it
requires the use of an external transformer, active matrix, or software plug-in
in order to work. In the classic M/S stereo miking conﬁguration, one of the
microphone capsules is designated the M (mid) position pickup and is generally
a cardioid pickup pattern that faces forward, toward the sound source. The S
(side) capsule is generally chosen as a ﬁgure-8 pattern that’s oriented sideways
(90º and 270º) to the on-axis pickup (i.e., with the null facing forward, into
the cardioid's main axis). In this way, the mid capsule picks up the direct sound,
while the side ﬁgure-8 capsule picks up ambient and reverberant sound. These
outputs are then combined through a sum-and-difference decoder matrix either
electrically (through a transformer matrix) or mathematically (through a digital
M/S plug-in), which then resolves them into a conventional X/Y stereo signal:
(M + S = left) and (M – S = right).
One advantage of this technique is its absolute monaural compatibility. 
When the left and right signals are combined, the sum of the output will be
(M + S) + (M – S) = 2M. That’s to say, the side (ambient) signal will be canceled,

Microphone Techniques
142
but the mid (direct) signal will be accentuated. Since it is widely accepted that
a mono signal loses its intelligibility with added reverb, this tends to work to
our advantage. Another amazing side beneﬁt of using M/S is the fact that it lets
us continuously vary the mix of mid (direct) to side (ambient) sound that’s
being picked up either during the recording (from the console location), or
even at a later time during mixdown, after it’s been recorded! These are both
possible by simply mixing the ratio of mid to side that’s being sent to the decoder
matrix (Figure 4.32b). In a mix-down scenario, all that’s needed is to record
the mid on one track and the side on another. (It’s often best to use a digital
recorder, because phase delays associated with the analog recording process can
interfere with decoding.) During mixdown, routing the M/S tracks to the decoder
matrix allows you to make important decisions regarding stereo width and depth
at a later, more controlled date.
FIGURE 4.32
M/S stereo microphone
technique: (a) M/S mic
configuration; (b) K-Stereo
Ambience Recovery with
M/S controls highlighted.
(Courtesy of Universal
Audio, www.uaudio.com ©
2017 Universal Audio, Inc.
All rights reserved. Used
with permission); (c) Vertigo
Sound VSM-3 Mix Satellite
Plug-In with M/S controls
highlighted. (Courtesy of
Universal Audio,
www.uaudio.com © 2017
Universal Audio, Inc. All
rights reserved. Used with
permission)
At this point, I’d like to offer up a personal fact on stereo miking: I share a
common working mic practice with both Bruce Swedien (Michael Jackson, etc.)
and Allen Sides (Ella Fitzgerald, Joni Mitchell, Phil Collins, Frank Zappa, Ice-T,
Sheryl Crow, Michael Jackson, Mary J. Blige and too many others to men tion).
We all record everything in stereo—and I mean everything. Why? First off, think
of recording a stereo synthesizer, using only one channel. We all know that the
1. Place a single mic in front of an instrument at a
traditional distance/placement.
2. Place an additional stereo mic pair (near the
same position for an X/Y pickup or at a desired
placement for a spaced conﬁguration).
3. Listen to the mono and stereo pickups in an A/B
fashion. How do the two compare?
Try This: Stereo Mic Test
D I Y
 do  it  yourself

143
Microphones: Design and Application  CHAPTER 4
effects, width and depth of that synth were programmed to come across best
in stereo. The same thing actually occurs when recording acoustic instru ments.
When two mics are used, a larger acoustic width can be picked up when panned
left/right. This often results in a more expansive, “live-sounding” instrument,
voice or soundscape. In Allen Sides’ words—“I still record everything in stereo.
I don’t use pan pots if I can avoid it. It makes things sound more exiting.” 
Go ahead and try it for yourself and see what you think.
DECCA TREE
Although not as commonly used as the preceding stereo techniques, the Decca
tree is a time-tested, classical miking technique that uses both time and amplitude
cues in order to create a coherent stereo image. Attributed originally to Decca
engineers Roy Wallace and Arthur Haddy in 1954, the Decca tree (Figure 4.33)
originally consisted of three omnidirectional mics (originally, Neumann M50
mics were used). In this arrangement, a left and right mic pair is placed 3 feet
apart, and a third mic is placed 1.5 feet out in front and panned in the center
of the stereo ﬁeld. Still favored by many in orchestral situations as a main pickup
pair, the Decca tree is most commonly placed on a tall boom, above and behind
the conductor. According to lore, when Haddy ﬁrst saw the array, he remarked,
“It looks like a bloody Christmas tree!” The name stuck.
FIGURE 4.33
Decca tree microphone
array. (a) Hardware mount.
(Courtesy of Audio
Engineering Associates,
www.ribbonmics.com) (b) In
a studio setting. (Courtesy
of Galaxy Studios,
www.galaxy.be) (c)
Arrangement in the front
segment of a circle, with
optional counterweight at
the rear.
SURROUND MIKING TECHNIQUES
With the advent of 5.1, 7.1 and 9.1 surround-sound production, it’s certainly
possible to make use of a surround console or DAW to use multiple-pickup
surround mic techniques in order to capture the actual acoustic environment and
then translate that into a surround mix. Just as the number of techniques and
personal styles increases when miking in stereo compared to mono, the number
of placement and technique choices will likewise increase when miking a source
and room in immersive surround. Although guidelines have been and will
continue to be set, both placement and mixing styles are deﬁnitely more of an
art than a science. For more info on surround, see Chapter 19, Immersive Audio.

Microphone Techniques
Ambient/Room Surround Mics
As was said earlier in the room pickup section, placing a spaced or coincident
pair of room mics in a studio can work wonders to add a sense of space to a
stereo recording. Well, the same room mics can also come in really handy if a
surround mix is made of the project. By simply panning the room mics to the
rear of the soundscape, the overall ambience will come alive in the mix, giving
an added sense of space to an ensemble group, drum set or instrument overdub.
So, during a recording or an overdub, the addition of a pair of room mics can
come in handy for both your stereo mixes and any immersive mixes that you
might make—Double score!
Surround Decca Tree
One of the most logical techniques for capturing an ensemble or instrument
in a surround setting places ﬁve mics onto a modiﬁed Decca tree (Figure 4.34a).
This ingenious and simple system adds two rear-facing mics to the existing three-
mic Decca tree system. Another simpler approach is to place ﬁve cardioid mics
in a circle, such that the center channel faces toward the source, thereby creating
a simple setup that can be routed L–C–R–Ls–Rs.
One last approach (which doesn’t actually fall under the Decca tree category)
involves the use of four cardioid mics that are spaced at 90º angles, representing
L–R–Ls–Rs, with the on-axis point being placed 45º between the L and R mics.
This “quad” conﬁguration can be easily made by mounting the mics that are
offset by 90º on three (or two crossed) stereo bars (Figure 4.34b).
144
FIGURE 4.34
Surround Mic placements:
(a) five cardioid
microphones can be
arranged in a circular
pattern (with the center
microphone facing toward
the source) to create a
modified, mini-surround
Decca tree; (b) A simple
four-microphone (quad)
approach to surround
miking can be easily made
by using three stereo bars.
MICROPHONE PLACEMENT TECHNIQUES
The following sections are meant to be used as a general guide to mic placement
for various acoustic and popular instruments (most often in close and semi-
close applications). It’s important to keep in mind that these are only guidelines.
Several general application and characteristic notes are detailed in Table 4.1,
and descriptions of several popular mics are outlined toward the end of this
Ambient/Room Surround Mics 
Surround Decca Tree 

chapter in the “Microphone Selection” section to help give insights into place -
ment and techniques that might work best in a particular application.
As a general rule, choosing the best mic for an instrument or vocal will ultimately
depend on the sound you’re searching for. For example, a dynamic mic will
often yield a “rugged” or “punchy” character (which is often further accentuated
by the proximity of bass boost that’s generally associated with a directional
mic). A ribbon mic will often yield a mellow sound that ranges from being
open and clear to slightly “croony” depending on the type and distances
involved. Condenser mics are often characterized as having a clear, present and
full-range sound that varies with mic design, grill options and capsule size. Before
jumping into this section, I’d like to again take time to refer you back to the
“Good Rule”, at the beginning of this chapter, to anyone who wants to be a
better engineer, producer and/or musician:
145
Microphones: Design and Application  CHAPTER 4
Needed Application
Required Microphone Choice and/or Characteristic
Natural, smooth tone quality
Flat frequency response
Bright, present tone quality
Rising frequency response
Extended lows
Dynamic or condenser with extended low-frequency response
Extended highs (detailed sound)
Condenser
Increased “edge” or midrange detail
Dynamic
Extra ruggedness
Dynamic or modern ribbon/condenser
Boosted bass at close working 
Directional microphone
distances
Flat bass response up close
Omnidirectional microphone
Reduced leakage, feedback, and 
Directional microphone or omnidirectional microphone at close 
room acoustics
working distances
Enhanced pickup of room acoustics
Place microphone or stereo pair at greater working distances
Reduced handling noise
Omnidirectional vocal microphone or directional microphone with
shock mount
Reduced breath popping
Omnidirectional or directional microphone with pop ﬁlter
Distortion-free pickup of very loud 
Dynamic or condenser with high maximum SPL rating
sounds
Noise-free pickup of quiet sounds
Condenser with low self-noise and high sensitivity
Table 4.1
Microphone Selection Guidelines
Good musician + good instrument + good performance + good acoustics + good mic + good placement 
= good sound.
THE “GOOD RULE”

Microphone Placement Techniques
Starting with an experienced, rehearsed and ready musician who has a quality
instrument that’s well tuned is the best insurance toward getting the best possible
sound. Let’s think about this for a moment. Say that we have a live rhythm
session that involves drums, piano, bass guitar and scratch vocals. All of the
players are the best around, except for the drummer, who is new to the studio
process. Unfortunately, you’ve now signed on to teach the drummer the ropes
of proper drum tuning, studio interaction and playing under pressure. It goes
without saying that the session might go far less smoothly than it otherwise
would, as you’ll have to take the extra time to work with the player to get the
best possible sound. Once you’re rolling, it’ll also be up to you or the producer
to pull a professional performance out of someone who’s new to the ﬁeld—
and the session will probably suffer for it.
Don’t get me wrong, musicians have to start somewhere, but an experienced,
capable musician who comes into the studio with a great instrument that’s tuned
and ready to go (and who might even clue you in on some sure-ﬁre mic and
placement techniques for the instrument) is simply a joy from a sound,
performance, time and budget-saving standpoint. Simply put, if you and/or the
project’s producer have prepared enough to get all your “goods” lined up, the
track will have a much better chance of being something that everyone can be
proud of. Just as with the art of playing an instrument—preparation, careful
mic choice, placement and “style” in the studio are a few of the fundamental
calling cards of a good engineer. Experience simply comes with time and the
willingness to experiment. Be patient, learn, listen and have fun and you too
will eventually rise to the professional occasion.
Brass Instruments
The following sections describe many of the sound characteristics and miking
techniques that are encountered in the brass family of instruments.
TRUMPET
The fundamental frequency of a trumpet ranges from E3 to D6 (165 to 1175
Hz) and contains overtones that stretch upward to 15 kHz. Below 500 Hz, the
sounds emanating from the trumpet project uniformly in all directions; above
1500 Hz, the projected sounds become much more directional; and above 5
kHz, the dispersion emanates at a tight 30º angle from in front of the bell. The
formants of a trumpet (the relative harmonic and resonance frequencies that
give an instrument its speciﬁc character) lie at around 1 to 1.5 kHz and at 2 to
3 kHz. Its tone can be radically changed by using a mute (a cup-shaped dome
that ﬁts directly over the bell), which serves to dampen frequencies above 2.5
kHz. A conical mute (a metal mute that ﬁts inside the bell) tends to cut back
on frequencies below 1.5 kHz while encouraging frequencies above 4 kHz.
Because of the high sound-pressure levels that can be produced by a trumpet
(up to 130 dB SPL), it’s best to place a mic slightly off the bell’s center at a
distance of 1 foot or more (Figure 4.35). When closer placements are needed,
146

147
Microphones: Design and Application  CHAPTER 4
a –10- to –20-dB pad can help prevent input overload at the mic or console
preamp input. Under such close working conditions, a windscreen can help
protect the diaphragm from windblasts.
TROMBONE
Trombones come in a number of sizes, however, the most commonly used
“bone” is the tenor, which has a fundamental note range spanning from E2 to
C5 (82 to 523 Hz) and produces a series of complex overtones that range from
5 kHz (when played medium loud) to 10 kHz (when overblown). The
trombone’s polar pattern is nearly as tight as the trumpet’s: frequencies below
400 Hz are distributed evenly, whereas its dispersion angle increases to 45º
from the bell at 2 kHz and above. The trombone most often appears in jazz
and classical music. The “Mass in C Minor” by Mozart, for example, has parts
for soprano, alto, tenor and bass trombones. This style obviously lends itself
to the spacious blending that can be achieved by distant pickups within a large
hall or studio. On the other hand, jazz music often calls for closer miking
distances. At 2 to 12 inches, for example, the trombonist should play slightly
to the side of the mic to reduce the chance of overload and wind blasts. In the
miking of a trombone section, a single mic might be placed between two
players, acoustically combining them onto a single channel and/or track.
TUBA
The bass and double-bass tubas are the lowest pitched of the brass/wind
instruments. Although the bass tuba’s range is actually a ﬁfth higher than the
double bass, it’s still possible to obtain a low fundamental of B (31 Hz). A
tuba’s overtone structure is limited; it’s top response ranges from 1.5 to 2 kHz.
The lower frequencies (around 75 Hz) are evenly dispersed; however, as
frequencies rise, their distribution angles reduce. Under normal conditions, this
class of instruments isn’t miked at close distances. A working range of 2 feet or
more, slightly off-axis to the bell, will generally yield the best results.
FIGURE 4.35
Typical microphone
placement for a single
trumpet.

Microphone Placement Techniques
148
FRENCH HORN
The fundamental tones of the French horn range from B1 to B5 (62 to 988 Hz).
Its “oo” formant gives it a round, broad quality that can be found at about 340
Hz, with other frequencies falling between 750 Hz and 3.5 kHz. French horn
players often place their hands inside the bell to mute the sound and promote
a formant at about 3 kHz. A French horn player or section is traditionally placed
at the rear of an ensemble, just in front of a rear, reﬂective stage wall. This wall
serves to reﬂect the sound back toward the listener’s position (which tends to
create a fuller, more deﬁned sound). An effective pickup of this instrument can
be achieved by placing an omni- or bidirectional pickup between the rear,
reﬂecting wall and the instrument bells, thereby receiving both the direct and
reﬂected sound. Alternatively, the pickups can be placed in front of the players,
thereby receiving only the sound that’s being reﬂected from the rear wall.
Guitar
The following sections describe the various sound characteristics and techniques
that are encountered when miking the guitar.
ACOUSTIC GUITAR
The popular steel-strung, acoustic guitar has a bright, rich set of overtones
(especially when played with a pick). Mic placement and distance will often
vary from instrument to instrument and may require experimentation to pick
up the best tonal balance. A balanced pickup can often be achieved by placing
the mic (or an X/Y stereo pair) at a point slightly off-axis and above or below
the sound hole at a distance of between 6 inches and 1 foot (Figure 4.36).
Condenser mics are often preferred for their smooth, extended frequency
response and excellent transient response. The smaller-bodied classical guitar
is normally strung with nylon or gut and is played with the ﬁngertips, giving
it a warmer, mellower sound than its steel-strung counterpart. To make sure
that the instrument’s full range is picked up, place the mic closer to the center
of the bridge, at a distance of between 6 inches and 1 foot.
FIGURE 4.36
Typical microphone
placement for the guitar 
(in either a mono or X/Y
stereo configuration).

Miking Near the Sound Hole
The sound hole (located at the front face of a guitar) serves as a bass port, which
resonates at the lower frequencies (around 80 to 100 Hz). Placing a mic too
close to the front of this port might result in a boomy and unnatural sound;
however, miking close to the sound hole is often popular on stage or around
high acoustic levels because the guitar’s output is highest at this position. To
achieve a more natural pickup under these conditions, the microphone’s output
can be rolled off at the lower frequencies (5 to 10 dB at 100 Hz).
Room and Surround Guitar Miking
An effective way to translate an acoustic guitar to the wide stage of surround
(if a big, full sound is what you’re after) is to record the guitar using X/Y or
spaced techniques stereo (panned front L/R) and pan the guitar’s electric pickup
(or added contact pickup) to the center speaker. Extra ambient mics could also
be used during an overdub to add room ambience to a stereo mix or to be
panned to the rear in a surround mix.
THE ELECTRIC GUITAR
The fundamentals of the average 22-fret guitar extend from E2 to D6 (82 to
1174 Hz), with overtones that extend much higher. All of these frequencies
might not be ampliﬁed, because the guitar chord tends to attenuate frequencies
above 5 kHz (unless the guitar has a built-in low impedance converter or low-
impedance pickups). The frequency limitations of the average guitar loudspeaker
often add to this effect, because their upper limit is generally restricted to below
5 or 6 kHz.
Miking the Guitar Amp
The most popular guitar ampliﬁer used for recording is a small practice-type
amp/speaker system. These high-quality amps often help the guitar’s suffering
high end by incorporating a sharp rise in the response range at 4 to 5 kHz, thus
helping to give it a clean, open sound. High-volume, wall-of-sound speaker
stacks are less commonly used in a session, because they’re harder to control
in the studio and in a mix. By far the most popular mic type for picking up an
electric guitar amp is the cardioid dynamic. A dynamic tends to give the sound
a full-bodied character without picking up extraneous ampliﬁer noises. Often
guitar mics will have a pronounced presence peak in the upper frequency range,
giving the pickup an added clarity. For increased separation, a microphone can
be placed at a working distance of 2 inches to 1 foot. When miking at a distance
of less than 4 inches, mic/speaker placement becomes slightly more critical
(Figure 4.37). For a brighter sound, the mic should face directly into the center
of the speaker’s cone. Placing it off the cone’s center tends to produce a more
mellow sound while reducing ampliﬁer noise. During an overdub or when the
amp is placed in an iso-booth/room, the ambient image can be “opened up”
even further by mixing a semi-distant or distant mic (or stereo pair) with the
149
Microphones: Design and Application  CHAPTER 4
Miking Near the Sound Hole 
Room and Surround Guitar Miking 
Miking the Guitar Amp 

Microphone Placement Techniques
150
FIGURE 4.37
Miking an electric guitar
cabinet directly in front of 
or off-center to the cone. 
A semi-distant mic can be
used to pick up the entire
cabinet, while a room mic
setup can be used to
capture ambience.
direct mic (and even with the close miked amp signal). This ambient pickup
can be either mixed into a stereo ﬁeld or at the rear of a surround ﬁeld to ﬁll
out the sound.
Isolation cabinets have also come onto the market that are literally sealed boxes
that house a speaker or guitar amp/cabinet system, as well as an internal mic
mount. These systems are used to reduce leakage and to provide greater control
over instrument levels within a recording studio or control room during a
session.
Recording Direct
A DI box is often used to feed the output signal of an electric guitar directly
into the mic input stage of a recording console or mixer. By routing the direct
output signal to a track, a cleaner, more present sound can be recorded (Figure
4.38a). This technique also reduces the leakage that results from having a guitar
amp in the studio and even makes it possible for the guitar to be played in the
control room or project studio. A combination of direct and miked signals often
results in a sound that adds the characteristic fullness of a miked amp to the
extra “bite” that a DI tends to give. These may be combined onto a single track
or, whenever possible, can be assigned to separate tracks, allowing for greater
control during mixdown (Figure 4.38b).
FIGURE 4.38
Direct recording of an
electric guitar: (a) direct
recording; (b) combined
direct and miked signal.
Recording Direct 

THE ELECTRIC BASS GUITAR
The fundamentals of an electric bass guitar range from about E1 to F4 (41.2 to
343.2 Hz). If it’s played loudly or with a pick, the added harmonics can range
upward to 4 kHz. Playing in the “slap” style or with a pick gives a brighter,
harder attack, while a “ﬁngered” style will produce a mellower tone. In modern
music production, the bass guitar is often recorded direct for the cleanest possible
sound. As with the electric guitar, the electric bass can be either miked at the
ampliﬁer or picked up through a DI box. If the amp is miked, dynamic mics
usually are chosen for their deep, rugged tones. The large-diaphragm dynamic
designs tend to subdue the high-frequency transients. When combined with a
boosted response at around 100 Hz, these large diaphragm dynamics give a
warm, mellow tone that adds power to the lower register. Equalizing a bass can
sometimes increase its clarity, with the fundamental being affected from 125
to 400 Hz and the harmonic punch being from 1.5 to 2 kHz. A compressor is
commonly used on electric and acoustic basses. It’s a basic fact that the signal
output from the instrument’s notes often varies in level, causing some notes to
stand out while others dip in volume. A compressor having a smooth
input/output ratio of roughly 4:1, a fast attack (8 to 20 milliseconds) and a
slower release time (1/4 to 1/2 second) can often smooth out these levels, giving
the instrument a strong, present and smooth bass line.
Keyboard Instruments
The following sections describe the various sound characteristics and techniques
that are encountered when miking keyboard instruments.
GRAND PIANO
The grand piano is an acoustically complex instrument that can be miked in a
variety of ways, depending on the style and preferences of the artist, producer
and/or engineer. The overall sound emanates from the instrument’s strings,
soundboard and mechanical hammer system. Because of its large surface area,
a minimum miking distance of 4 to 6 feet is needed for the tonal balance to
fully develop and be picked up; however, leakage from other instruments often
means that these distances aren’t practical or possible. As a result, pianos are
often miked at distances that favor such instrument parts as:
n Strings and soundboard, often yielding a bright and relatively natural tone
n Hammers, generally yielding a sharp, percussive tone
n Soundboard holes alone, often yielding a sharp, full-bodied sound
In modern music production, two basic grand piano styles can be found in the
recording studio: the concert grand, which traditionally has a rich and full-bodied
tone (often used for classical music and ranging in size up to 9 feet in length),
and the studio grand, which is more suited for modern music production and
has a sharper, more percussive edge to its tone (often being about 7 feet in
length).
151
Microphones: Design and Application  CHAPTER 4

Microphone Placement Techniques
152
Figure 4.39 shows a number of miking positions that can be used in recording
a grand piano. Although several mic positions are illustrated, it’s important to
keep in mind that these are only guidelines from which to begin. Your own
personal sound can be achieved through mic choice and experimentation with
mic placement.
n Position 1: The mic is attached to the partially or entirely open lid of the
piano. The most appropriate choice for this pickup is the boundary mic,
which can be permanently attached or temporarily taped to the lid. This
method uses the lid as a collective reﬂector and provides excellent pickup
under restrictive conditions (such as on stage and during a live video shoot).
n Position 2: Two mics are placed in a spaced stereo conﬁguration at a
working distance of 6 inches to 1 inch. One mic is positioned over the
low strings and one is placed over the high strings.
n Position 3: A single mic or coincident stereo pair is placed just inside the
piano between the soundboard and its fully or partially open lid.
n Position 4: A single mic or stereo coincident pair is placed outside the
piano, facing into the open lid (this is most appropriate for solo or accent
miking).
n Position 5: A spaced stereo pair is placed outside the lid, facing into the
instrument.
n Position 6: A single mic or stereo coincident pair is placed just over the
piano hammers at a working distance of 4 to 8 inches to give a driving
pop or rock sound.
A condenser or extended-range dynamic mic is most often the preferred choice
when miking an acoustic grand piano, as those types of mics tend to accurately
represent the transient and complex nature of the instrument. Should excessive
leakage be a problem, a close-miked cardioid (or cardioid variation) can be
used; however, if leakage isn’t a problem, backing away to a compromise
distance (3 to 6 feet) can help capture the instrument’s overall tonal balance.
Separation
Separation is often a problem associated with the grand piano whenever it is
placed next to noisy neighbors. Separation, when miking a piano, can be
achieved in the following ways:
FIGURE 4.39
Possible miking
combinations for the grand
piano.

n Place the piano inside a separate isolation room.
n Place a ﬂat (acoustic separator) between the piano and its louder neighbor.
n Place the mics inside the piano and lower the lid onto its short stick. A
heavy moving or other type of blanket can be placed over the lid to further
reduce leakage.
n Overdub the instrument at a later time. In this situation, the lid can be
removed or propped up by the long stick, allowing the mics to be placed
at a more natural-sounding distance.
UPRIGHT PIANO
You would expect the techniques for this seemingly harmless piano type to be
similar to those for its bigger brother. This is partially true. However, because
this instrument was designed for home enjoyment and not performance, the
mic techniques are often very different. Since it’s often more difﬁcult to achieve
a respectable tone quality when using an upright, you might want to try the
following methods:
n Miking over the top: Place two mics in a spaced fashion just over and in
front of the piano’s open top, with one over the bass strings and one over
the high strings. If isolation isn’t a factor, remove or open the front face
that covers the strings in order to reduce reﬂections and, therefore, the
instrument’s characteristic “boxy” quality. Also, to reduce resonances you
might want to angle the piano out and away from any walls.
n Miking the kickboard area: For a more natural sound, remove the kickboard
at the lower front part of the piano to expose the strings. Place a stereo
spaced pair over the strings (one each at a working distance of about 8
inches over the bass and high strings). If only one mic is used, place it
over the high-end strings. Be aware, though, that this placement can pick
up excessive foot-pedal noise.
n Miking the upper soundboard area: To reduce excessive hammer attack, place
a microphone pair at about 8 inches from the soundboard, above both
the bass and high strings. In order to reduce muddiness, the soundboard
should be facing into the room or be moved away from nearby walls.
ELECTRONIC KEYBOARD INSTRUMENTS
Signals from most electronic instruments (such as synthesizers, samplers and
drum machines) are often taken directly from the device’s line-level output(s)
and inserted into a console, either through a DI box or directly into a channel’s
line-level input. Alternatively, the keyboard’s output can be plugged directly
into the recorder or interface line-level inputs. The approach to miking an
electronic organ can be quite different from the techniques just mentioned. 
A good Hammond or other older organ can sound wonderfully “dirty” through
miked loudspeakers. Such organs are often played through a Leslie cabinet
(Figure 4.40), which adds a unique, Doppler-based vibrato. Inside the cabinet
153
Microphones: Design and Application  CHAPTER 4

Microphone Placement Techniques
154
FIGURE 4.40
A Leslie speaker cabinet
creates a unique vibrato
effect by using a set of
rotating speaker baffles that
spin on a horizontal axis. (a)
Modern portable rotary amp
with built-in microphones
and three XLR outputs.
(Courtesy of Motion Sound,
www.motion-sound.com) 
(b) Miking the rotating
speakers of a Leslie cabinet.
is a set of rotating speaker bafﬂes that spin on a horizontal axis and, in turn,
produce a pitch-based vibrato as the speakers accelerate toward and away from
the mics. The upper high-frequency speakers can be picked up by either one or
two mics (each panned left and right), with the low-frequency driver being
picked up by one mic. Motor and bafﬂe noises can produce quite a bit of wind,
possibly creating the need for a windscreen and/or experimentation with
placement.
Percussion
The following sections describe the various sound characteristics and techniques
that are encountered when miking drums and other percussion instruments.
DRUM SET
The standard drum kit (Figure 4.41) is often at the foundation of modern
music, because it provides the “heartbeat” of a basic rhythm track; consequently,
a proper drum sound is extremely important to the outcome of most music
projects. Generally, the drum kit is composed of the kick drum, snare drum,
high-toms, low-tom (one or more), hi-hat and a variety of cymbals. Since a full
FIGURE 4.41
The drum set: (a) Peter
Erskine’s studio drum kit
(Courtesy of Beyerdynamic,
www.beyerdynamic.com);
(b) traditional drum
placement and mic
positioning.

kit is a series of interrelated and closely spaced percussion instruments, it often
takes real skill to translate the proper spatial and tonal balance into a project.
The larger-than-life driving sound of the acoustic rock drum set that we’ve all
become familiar with is the result of an expert balance among playing
techniques, proper tuning and mic placement.
During the past decades, drums have undergone a substantial change with
regard to playing technique, miking technique and choice of acoustic recording
environment. In the 1960s and 1970s, the drum set was placed in a small
isolation room called a drum booth. This booth acoustically isolated the
instrument from the rest of the studio and had the effect of tightening the drum
sound because of the limited space (and often dead acoustics). The drum booth
also physically isolated the musician from the studio, which often caused the
musician to feel removed and less involved in the action. Today, many engineers
and producers have moved the drum set out of smaller iso-rooms and back
into larger open studio areas where the sound can fully develop and combine
with the studio’s own acoustics. In many cases, this effect can be exaggerated
by placing a distant mic pair in the room (a technique that often produces a
fuller, larger-than-life sound, especially in surround).
Before a session begins, the drummer should tune each drum while the mics
and bafﬂes for the other instruments are being set up. Each drumhead should
be adjusted for the desired pitch and for constant tension around the rim by
hitting the head at various points around its edge and adjusting the lugs for the
same pitch all around the head. Once the drums are tuned, the engineer should
listen to each drum individually to make sure that there are no buzzes, rattles,
or resonant after-rings. Drums that sound great in live performance may not
sound nearly as good when being close miked. In a live performance, the rattles
and rings are covered up by the other instruments and are lost before the sound
reaches the listener. Close miking, on the other hand, picks up the noises as
well as the desired sound.
If tuning the drums doesn’t bring the extraneous noises or rings under control,
duct or masking tape can be used to dampen them. Pieces of cloth, dampening
rings, paper towels, or a wallet can also be taped to a head in various locations
(which is determined by experimentation) to eliminate rings and buzzes.
Although head damping has been used extensively in the past, present methods
use this damping technique more discreetly and will often combine dampening
with proper design and tuning styles (all of which are the artist’s personal choice).
During a session, it’s best to remove the damping mechanisms that are built
into most drum sets, because they apply tension to only one spot on the head
and unbalance its tension. These built-in dampeners often vibrate when the
head is hit and are a chief source of rattles. Removing the front head and placing
a blanket or other damping material inside the drum (so that it’s pressing
against the head) can often dampen the kick drum. Adjusting the amount of
material can vary the sound from being a resonant boom to a thick, dull thud.
Kick drums are usually (but not always) recorded with their front heads removed,
155
Microphones: Design and Application  CHAPTER 4

Microphone Placement Techniques
156
while other drums are recorded with their bottom heads either on or off. Tuning
the drums is more difﬁcult if two heads are used because the head tensions
often interact; however, they will often produce a more resonant tone. After the
drums have been tuned, the mikes can be put into position. It’s important to
keep the mics out of the drummer’s way, or they might be hit by a stick or
moved out of position during the performance.
Miking the Drum Set
After the drum set has been optimized for the best sound, the mics can be placed
into their pickup positions (Figure 4.42). Because each part of the drum set is
so different in sound and function, it’s often best to treat each grouping as an
individual instrument. In its most basic form, the best place to start when miking
a drum set is to start with the fundamental “groups.” These include placing:
n Position 1: A mic on the kick
n Position 2: A mic on the snare drum
n Position 3: At an absolute minimum, the entire drum set can be adequately
picked up using only four mics by adding two overhead spaced pickups
n Position 4: Or a coincident pair can be placed over the set
n Position 5: One or two mics placed on the high toms
n Position 6: A mic on the low tom
FIGURE 4.42
Typical microphone
placements for a drum set:
(a) side view showing a
“bare bones” jazz kit;
(b) front view showing a
basic rock setup; (c) top
view showing a basic rock
setup.
A mic’s frequency response, polar response, proximity effect and transient
response should be taken into account when matching it to the various drum
groups. In fact, mic placement is your best friend, when it comes to the getting
the best balance between drum “instruments” (and each is best thought of 
as percussion instruments that work together to make an overall kit). While a
mic’s polar response is another friend, when it comes to isolating a speciﬁc drum,
etc., onto a track with maximum rejection and overall leakage (for gaining better
Miking the Drum Set 

control over the various drums within the overall drum mix). Dynamic range is
another important consideration when miking drums. Since a drum set is
capable of generating extremes of volume and power (as well as softer, more
subtle sounds), the chosen mics must be able to withstand strong peaks without
distorting, and yet still be able to capture the more delicate nuances of a sound.
Since the drum set usually is one of the loudest sound sources in a studio setting,
it’s often wise to place it on a solidly supported riser. This reduces the amount of
low-end “thud” that can otherwise leak through the ﬂoor into other parts of the
studio. Depending on the studio layout, the following drum scenarios may occur:
n The drums could be placed in their own room, isolated from other
instruments.
n To achieve a bigger sound, the drums could be placed in the large studio
room while the other instruments are placed in smaller iso-rooms or are
recorded direct.
n To reduce leakage, the drums could be placed in the studio, while being
enclosed by 4-foot (or higher) divider ﬂats.
Kick Drum
The kick drum adds a low-energy drive or “punch” to a rhythm groove. This
drum has the capability to produce low frequencies at high sound-pressure levels,
so it’s necessary to use a mic that can both handle and faithfully reproduce
these signals. Often the best choice for the job is a large-diaphragm dynamic
mic. Since proximity effect (bass boost) occurs when using a directional mic at
close working distances and because the drum’s harmonics vary over its large
surface area, even a minor change in placement can have a profound effect on
the pickup’s overall sound. Moving the mic closer to the head (Figure 4.43) can
add a degree of warmth and fullness, while moving it farther back often
emphasizes the high-frequency “click.” Placing the mic closer to the beater
emphasizes the hard “thud” sound, whereas an off-center pickup captures more
of the drum’s characteristic skin tone. A dull and loose kick sound can be
tightened to produce a sharper, more deﬁned transient sound by placing a
blanket or other damping material inside the drum shell ﬁrmly against the beater
head. Cutting back on the kick’s equalization at 300 to 600 Hz can help reduce
157
Microphones: Design and Application  CHAPTER 4
FIGURE 4.43
Kick drum mic placements.
(a) Placing a microphone
inside the kick at a close
distance results in a sharp,
deep attack (moving the mic
placement will affect the
tone. (b) Placing the mic at
a distance just outside the
kick drumhead to bring out
the low end and natural
fullness. (c) Do both and
record them to their own
tracks for greater control
and choices during
mixdown.
Kick Drum 

Microphone Placement Techniques
158
the dull “cardboard” sound, whereas boosting from 2.5 to 5 kHz adds a sharper
attack, “click” or “snap.” It’s also often a good idea to have a can of WD-40®
or other light oil handy in case squeaks from some of the moving parts (most
often the kick pedal) gets picked up by the mics.
Snare Drum
Commonly, a snare mic is aimed just inside the top rim of the snare drum at a
distance of about 1 inch (Figure 4.44). The mic should be angled for the best
possible separation from other drums and cymbals. Its rejection angle should
be aimed at either the hi-hat or rack-toms (depending on leakage difﬁculties).
Usually, the mic’s polar response is cardioid, although bidirectional and super -
cardioid responses might offer a tighter pickup angle. With certain musical styles
(such as jazz), you might want a crisp or “bright” snare sound. This can be achieved
by placing an additional mic on the snare drum’s bottom head and then
combining the two mics onto a single track. Because the bottom snare head is
180º out of phase with the top, it’s almost always a wise idea to reverse the bottom
mic’s phase polarity. When playing in styles where the snare springs are turned
off, it’s also wise to keep your ears open for snare rattles and buzzes that can
easily leak into the snare mic (as well as other mics). The continued ringing of an
“open” snare note (or for any other drum type, for that matter) can be dampened
in several ways. Dampening rings, which can be purchased at music stores, are
used to reduce the ring and to deepen the instrument’s tone. If there are no damp -
ening rings around, the tone can be dampened by taping a billfold or similar sized
folded paper towel to the top/side of the drumhead, a few inches off its edge.
FIGURE 4.44
Snare drum mic
placements. (a) With mic
placed over and just inside
the top rim. (b) With the top
mic placed, a lower mic can
be placed and flipped 180º
out of phase with the top
mic, so as to pick up the
snare springs (in engaged).
Overheads
Overhead mics are generally used to pick up the high-frequency transients of
cymbals with crisp, accurate detail while also providing an overall blend of the
entire drum kit. Because of the transient nature of cymbals, a condenser mic is
often chosen for its accurate high-end response. Overhead mic placement can
be very subjective and personal. One type of placement is the spaced pair,
whereby two mics are suspended above the left and right sides of the kit. These
Snare Drum 
Overheads 

mics are equally distributed about the L/R cymbal clusters so as to pick up their
respective instrument components in a balanced fashion (Figure 4.45a). Another
placement method is to suspend the mics closely together in a coincident
fashion (Figure 4.45b). This often yields an excellent stereo overhead image
with a minimum of the phase cancellations that might otherwise result when
using spaced mics. Again, it’s important to remember that there are no rules
for getting a good sound. If only one overhead mic is available, place it at a
central point over the drums. (Note: Now is the time that my friend George
Massenburg would want me to remind you to make sure that the overheads
are equidistant to the kick, so as to avoid phase problems.) Lastly, if you’re
using a number of pickups to close mic individual components of a kit, there
might be times when you won’t need overheads at all (the leakage spillover
just might be enough to do the trick).
Rack-Toms
The upper rack-toms can be miked either individually (Figure 4.46a) or by
placing a single mic between the two at a short distance (Figure 4.46b). When
miked individually, a “dead” sound can be achieved by placing the mic close
to the drum’s top head (about 1 inch above and 1 to 2 inches in from the outer
rim). A sound that’s more “live” can be achieved by increasing the height above
159
Microphones: Design and Application  CHAPTER 4
FIGURE 4.45
Typical stereo overhead
pickup positions: (a) spaced
pair technique; (b) X/Y
coincident technique.
FIGURE 4.46
Miking toms. (a) Individual
miking of a rack-tom. 
(b) Single microphone
placement for picking up
two toms.
Rack-Toms 

Microphone Placement Techniques
the head to about 3 to 6 inches. If isolation or feedback is a consideration, a
hypercardioid pickup pattern can be chosen. Another way to reduce leakage
and to get a deep, driving tone (with less attack) is to remove the tom’s bottom
head and place the mic inside, 1 to 6 inches away from the top head.
Floor-Tom
Floor-toms can be miked similarly to the rack-toms. The mic can be placed 2
to 3 inches above the top and to the side of the head, or it can be placed inside
1 to 6 inches from the head. Again, a single mic can be placed above and between
the two ﬂoor-toms, or each can have its own mic pickup (which often yields a
greater degree of control over panning and tonal color).
Hi-Hat
The “hat” usually produces a strong, sibilant energy in the high-frequency range,
whereas the snare’s frequencies often are more concentrated in the midrange.
Although moving the hat’s mic won’t change the overall sound as much as it
would on a snare, you should still keep the following three points in mind:
n Placing the mic above the top cymbal will help pick up the nuances of
sharp stick attacks.
n The open and closing motion of the hi-hat will often produce rushes of
air; consequently, when miking the hat’s edge, angle the mic slightly above
or below the point where the cymbals meet.
n If only one mic is available (or desired), both the snare and hi-hat can be
simultaneously picked up by carefully placing the mic between the two,
facing away from the rack-toms as much as possible. Alternatively, a ﬁgure-
8 mic can be placed between the two with the null axis facing toward the
cymbals and kick.
TUNED PERCUSSION INSTRUMENTS
The following sections describe the various sound characteristics and techniques
that are encountered when miking tuned percussion instruments.
Congas and Hand Drums
Congas, tumbas, and bongos are single-headed, low-pitched drums that can be
individually miked at very close distances of 1 to 3 inches above the head and
2 inches in from the rim, or the mics can be pulled back to a distance of 1 foot
for a fuller, “live” tone. Alternatively, a single mic or X/Y stereo pair can be
placed at a point about 1 foot above and between the drums (which are often
played in pairs). Another class of single-headed, low-pitched drums (known as
hand drums) isn’t necessarily played in pairs but is often held in the lap or
strapped across the player’s front. Although these drums can be as percussive
as congas, they’re often deeper in tone and often require that the mic(s) be
backed off in order to allow the sound to develop and/or fully interact with
160
Floor-Tom 
Hi-Hat 
Congas and Hand Drums 

the room. In general, a good pickup can be achieved by placing a mic at a
distance of 1 to 3 feet in front of the hand drum’s head. Since a large part of
the drum’s sound (especially its low-end power) comes from its back hole,
another mic can be placed at the lower port at a distance of 6 inches to 2 feet.
Since the rear sound will be 180º out of phase from the front pickup, the mic’s
phase should be reversed whenever the two signals are combined.
Xylophone, Vibraphone and Marimba
The most common way to mic a tuned percussion instrument is to place two
high-quality condenser or extended-range dynamic pickups above the playing
bars at a spaced distance that’s appropriate to the instrument size (following
the 3:1 general rule). A coincident stereo pair can help eliminate possible phase
errors; however, a spaced pair will often yield a wider stereo image.
Stringed Instruments
Of all the instrumental families, stringed instruments are perhaps the most diverse.
Ethnic music often uses instruments that range from being single stringed to those
that use highly complex and developed systems to produce rich and subtle tones.
Western listeners have grown accustomed to hearing the violin, viola, cello and
double bass (both as solo instruments and in an ensemble setting). Whatever
the type, stringed instruments vary in their design type and in construction to
enhance or cut back on certain harmonic frequencies. These variations are what
give a particular stringed instrument its own characteristic sound.
VIOLIN AND VIOLA
The frequency range of the violin runs from 196 Hz to above 10 kHz. For this
reason, a good mic that displays a relatively ﬂat frequency response should be
used. The violin’s fundamental range is from G3 to E6 (196 to 1300 Hz), and
it is particularly important to use a mic that’s ﬂat around the formant frequencies
of 300 Hz, 1 kHz, and 1200 Hz. The fundamental range of the viola is tuned
a ﬁfth lower and contains fewer harmonic overtones. In most situations, the
violin or viola’s mic should be placed within 45º of the instrument’s front face.
The distance will depend on the particular style of music and the room’s acoustic
condition. Miking at a greater distance will generally yield a mellow, well-
rounded tone, whereas a closer position might yield a scratchy, more nasal
quality—the choice will depend on the instrument’s tone quality. The recom -
mended miking distance for a solo instrument is between 3 and 8 feet, over
and slightly in front of the player (Figure 4.47). Under studio conditions, a
closer mic distance of between 2 and 3 feet is recommended. For a ﬁddle 
or jazz/rock playing style, the mic can be placed at a close working distance of 
6 inches or less, as the increased overtones help the instrument to cut through
an ensemble. Under PA (public address) applications, distant working conditions
are likely to produce feedback (since less ampliﬁcation is needed). In this
situation, an electric pickup, contact, or clip-type microphone can be attached
to the instrument’s body or tailpiece.
161
Microphones: Design and Application  CHAPTER 4
Xylophone, Vibraphone and Marimba 

Microphone Placement Techniques
162
CELLO
The fundamental range of the cello is from C2 to C5 (65 to 520 Hz), with
overtones up to 8 kHz. If the player’s line of sight is taken to be 0º, then the
main direction of sound radiation lies between 10º and 45º to the right. A
quality mic can be placed level with the instrument and directed toward the
sound holes. The chosen microphone should have a ﬂat response and be placed
at a working distance of between 6 inches and 3 feet.
DOUBLE BASS
The double bass is one of the orchestra’s lowest-pitched instruments. The
fundamentals of the four-string type reach down to E1 (41 Hz) and up to around
middle C (260 Hz). The overtone spectrum generally reaches upward to 7 kHz,
with an overall angle of high-frequency dispersion being ±15º from the player’s
line of sight. Once again, a mic can be aimed at the f holes at a distance of
between 6 inches and 1.5 feet.
Voice
From a shout to a whisper, the human voice is a talented and versatile sound
source that displays a dynamic and timbrel range that’s matched by few other
instruments. The male bass voice can ideally extend from E2 to D4 (82 to 294
Hz) with sibilant harmonics extending to 12 kHz. The upper soprano voice can
range upward to 1050 Hz with harmonics that also climb to 12 kHz.
When choosing a mic and its proper placement, it’s important to step back for
a moment and remember that the most important “device” in the signal chain
is the vocalist. Let’s assume that the engineer/producer hasn’t made the classic
mistake of waiting until the last minute (when the project goes over budget
and/or into overtime) to record the vocals. Good, now the vocalist can relax
and concentrate on a memorable performance. Next step is to concentrate on
the vocalist’s “creature comforts”: How are the lighting and temperature settings?
Is the vocalist thirsty? Once done, you can go about the task of choosing your
mic and its placement to best capture the performance.
FIGURE 4.47
Miking a violin. (a) Example
of a typical mic placement
for the violin. (b) Example of
miking for a small, 4-piece
string section.

The engineer/producer should be aware of the following traps that are often
encountered when recording the human voice:
n Excessive dynamic range: This can be solved either by mic technique
(physically moving away from the mic during louder passages) or by
inserting a compressor into the signal path. Some vocalists have dynamics
that range from whispers to normal volumes to practically screaming—all
in a single passage. If you optimize your recording levels during a moderate-
volume passage and the singer begins to belt out the lines, then the levels
will become too “hot” and will distort. Conversely, if you set your recording
levels for the loudest passage, the moderate volumes will be buried in the
music. The solution to this dilemma is to place a compressor in the mic’s
signal path. The compressor automatically “rides” the signal’s gain and
reduces excessively loud passages to a level that the system can effectively
handle. (See Chapter 12 for more information about compression and
devices that alter dynamic range.) Don't forget that in the digital domain,
it's often possible to apply compression later within the mixdown stage. 
n Sibilance: This occurs when sounds such as f, s and sh are overly accentuated.
This often is a result of tape saturation and distortion at high levels or
slow tape speeds. Sibilance can be reduced by inserting a frequency-
selective compressor (known as a de-esser) into the chain or through the
use of moderate equalization.
n Excessive bass boost due to proximity effect: This bass buildup often occurs
when a directional mic is used at close working ranges. It can be reduced
or compensated for by increasing the working distance between the source
and the mic, by using an omnidirectional mic (which doesn’t display a
proximity bass buildup), or through the use of equalization.
MIC TOOLS FOR THE VOICE
Some of the most common tools in miking are used for ﬁxing problems that
relate to picking up the human voice and to room isolation.
Explosive popping p and b sounds often result when turbulent air blasts from the
mouth strike the mic diaphragm. This problem can be avoided or reduced by:
n Placing a pop ﬁlter over the mic
n Placing a mesh windscreen between the mic and the vocalist
n Taping a pencil in front of the mic capsule, so as to break up the “plosive”
air blasts
n Using an omnidirectional mic (which is less sensitive to popping, but
might cause leakage issues)
Reducing problems due to leakage and inadequate isolation can be handled in
any number of ways, including:
n Choice of directional pattern (i.e., choosing a tighter cardioid or hyper -
cardioid pattern can help reduce unwanted leakage)
163
Microphones: Design and Application  CHAPTER 4

Microphone Placement Techniques
n Isolating the singer with a ﬂat or portable isolation cage
n Isolating the singer in a separate iso-booth
n Overdubbing the vocals at a later time, keeping in mind that carefully
isolated “scratch” vocals can help glue the band together
Woodwind Instruments
The ﬂute, clarinet, oboe, saxophone, and bassoon combine to make up the
woodwind class of instruments. Not all modern woodwinds are made of wood
nor do they produce sound in the same way. For example, a ﬂute’s sound is
generated by blowing across a hole in a tube, whereas other woodwinds produce
sound by causing a reed to vibrate the air within a tube.
Opening or covering ﬁnger holes along the sides of the instrument controls the
pitch of a woodwind by changing the length of the tube and, therefore, the
length of the vibrating air column. It’s a common misunderstanding that the
natural sound of a woodwind instrument radiates entirely from its bell or
mouthpiece. In reality, a large part of its sound often emanates from the ﬁnger
holes that span the instrument’s entire length.
CLARINET
The clarinet commonly comes in two pitches: the B ﬂat clarinet, with a lower
limit of D3 (147 Hz), and the A clarinet, with a lower limit of C3 (130 Hz).
The highest fundamental is around G6 (1570 Hz), whereas notes an octave
above middle C contain frequencies of up to 150 Hz when played softly. This
spectrum can range upward to 12 kHz when played loudly. The sound of this
reeded woodwind radiates almost exclusively from the ﬁnger holes at frequen-
cies between 800 Hz and 3 kHz; however, as the pitch rises, more of the 
sound emanates from the bell. Often, the best mic placement occurs when 
the pickup is aimed toward the lower ﬁnger holes at a distance of 6 inches to
1 foot (Figure 4.48a).
FLUTE
The ﬂute’s fundamental range extends from B3 to about C7 (247 to 2093 Hz).
For medium loud tones, the upper overtone limit ranges between 3 and 6 kHz.
Commonly, the instrument’s sound radiates along the player’s line of sight for
frequencies up to 3 kHz. Above this frequency, however, the radiated direction
often moves outward 90º to the player’s right. When miking a ﬂute, placement
depends on the type of music being played and the room’s overall acoustics.
When recording classical ﬂute, the mic can be placed on-axis and slightly above
the player at a distance of between 3 and 8 feet. When dealing with modern
musical styles, the distance often ranges from 6 inches to 2 feet. In both
circumstances, the microphone should be positioned at a point 1/3 to 1/2 the
distance from the instrument’s mouthpiece to its footpiece. In this way, the
instrument’s overall sound and tone quality can be picked up with equal
164

165
Microphones: Design and Application  CHAPTER 4
FIGURE 4.48
Typical woodwind
placement. (a) Mic position
for the clarinet. (b) Mic
position for a flute.
intensity (Figure 4.48b). Placing the mic directly in front of the mouthpiece
will increase the level (thereby reducing feedback and leakage); however, the
full overall body sound won’t be picked up and breath noise will be accentuated.
If mobility is important, an integrated contact pickup can be used or a clip mic
can be secured near the instrument’s mouthpiece.
SAXOPHONE
Saxophones vary greatly in size and shape. The most popular sax for rock 
and jazz is the S-curved B-ﬂat tenor sax, whose fundamentals span from B2 to
F5 (123 to 698 Hz), and the E-ﬂat alto, which spans from C3 to G5 (130 
to 784 Hz). Also within this family are the straight-tubed soprano and sopranino,
as well as the S-shaped baritone and bass saxophones. The harmonic content
of these instruments ranges up to 8 kHz and can be extended by breath noises
up to 13 kHz. As with other woodwinds, the mic should be placed roughly in
the middle of the instrument at the desired distance and pointed slightly toward
the bell (Figure 4.49). Keypad noises are considered to be a part of the
instrument’s sound; however, even these can be reduced or eliminated by aiming
the microphone closer to the bell’s outer rim.
FIGURE 4.49
Typical microphone
positions for the saxophone:
(a) standard placement; 
(b) typical “clip-on”
placement.

Microphone Placement Techniques
166
HARMONICA
Harmonicas come in all shapes, sizes and keys—and are divided into two basic
types: the diatonic and the chromatic. Their pitch is determined purely by the
length, width and thickness of the various vibrating metal reeds. The “harp”
player’s habit of forming his or her hands around the instrument is a way to
mold the tone by forming a resonant cavity. The tone can be deepened and a
special “wahing” effect can be produced by opening and closing a cavity that’s
formed by the palms; consequently, many harmonica players carry their preferred
microphones with them rather than being stuck in front of an unfamiliar mic
and stand.
MICROPHONE SELECTION
The following information is meant to provide insights into a limited number
of professional mics that are used for music recording and professional sound
applications. This list is by no means complete, as literally hundreds of mics
are available, each with its own particular design, sonic character and application.
Shure SM57
The SM57 (Figure 4.50) is widely used by engineers, artists, touring sound
companies, etc., for instrumental and remote recording applications. The SM57’s
midrange presence peak and good low-frequency response make it useful for
use with vocals, snare drums, toms, kick drums, electric guitars and keyboards.
FIGURE 4.50
Shure SM57 dynamic
microphone. (Courtesy of
Shure Brothers, Inc.,
www.shure.com, images 
© 2017, Shure
Incorporated—used with
permission)
Speciﬁcations:
n Transducer type: moving-coil dynamic
n Polar response: cardioid
n Frequency response: 40 to 15,000 Hz
n Equivalent noise rating: –7.75 dB (0 dB = 1 V/microbar)
Telefunken M81
The Telefunken M81 (Figure 4.51) has been likened to that of a condenser
microphone and has become a staple for vocal and snare drum applications,
especially in the world of touring and live performance. As an alternative to the
extended top end capabilities of the Telefunken M80, the linear response M81
is a tool that is a bit less specialized, giving the microphone more universal
application ability.

167
Microphones: Design and Application  CHAPTER 4
FIGURE 4.51
Telefunken M81 dynamic
microphone. (Courtesy of
TELEFUNKEN Elektroakustik,
Inc., www.telefunken-
elektroakustik.com)
Speciﬁcations:
n Transducer type: moving-coil dynamic
n Polar response: Super Cardioid
n Capsule: 25 mm
n Frequency response: 50 Hz / 18 kHz
n Sensitivity: 1.54 mV/Pa
n Maximum SPL: 135 dB
AKG D112
Large-diaphragm cardioid dynamic mics, such as the AKG D112 (Figure 4.52),
are often used for picking up kick drums, bass guitar cabinets and other low-
frequency, high-output sources.
FIGURE 4.52
AKG D112 dynamic
microphone. (Courtesy of
AKG Acoustics GmbH.,
www.akg.com)
Speciﬁcations:
n Transducer type: moving-coil dynamic
n Polar response: cardioid
n Frequency response: 30 to 17,000 Hz
n Sensitivity: –54 dB ± 3 dB re. 1 V/microbar
Royer Labs R-121
The R-121 is a ribbon mic with a ﬁgure-8 pattern (Figure 4.53). Its sensitivity
is roughly equal to that of a good dynamic mic, and it exhibits a warm, realistic
tone and ﬂat frequency response. Made using advanced materials and cutting-
edge construction techniques, its response is ﬂat and well balanced; the low
end is deep and full without getting boomy, mids are well deﬁned and realistic
and the high-end response is sweet and natural sounding.

Microphone Selection
168
Speciﬁcations:
n Acoustic operating principle: electrodynamic pressure gradient ribbon
n Polar pattern: ﬁgure 8
n Generating element: 2.5-micron aluminum ribbon
n Frequency response: 30 to 15,000 Hz ± 3 dB
n Sensitivity: –54 dBV re. 1 V/Pa ± 1 dB
n Output impedance: 300 Ω at 1 K (nominal); 200 Ω optional
n Maximum SPL: >135 dB
Beyerdynamic M-160
The Beyer M-160 ribbon microphone (Figure 4.54) is capable of handling high
sound-pressure levels without sustaining damage while providing the trans -
parency that often is inherent in ribbon mics. Its hypercardioid response 
yields a wide-frequency response/low-feedback characteristic for both studio and
stage.
Speciﬁcations:
n Transducer type: ribbon dynamic
n Polar response: hypercardioid
n Frequency response: 40–18,000Hz
n Sensitivity: 52 dB (0 dB = 1 mW/Pa)
n Equivalent noise rating: –145 dB
n Output impedance: 200 Ω
FIGURE 4.53
Royer Labs R-121 
ribbon microphone.
(Courtesy of Royer Labs,
www.royerlabs.com)
FIGURE 4.54
Beyerdynamic M-160 
ribbon microphone.
(Courtesy of Beyerdynamic,
www.beyerdynamic.com)

AEA A440
The AEA model R44 and A440 ribbon microphones (the latter is shown in Figure
4.2b) carry forward the classic tradition of the venerable 1930s RCA-44 into
the twenty-ﬁrst century. The R44 series even uses “new old-stock” RCA ribbon
material and features a re-engineered output transformer that recaptures the
sonic signature of the original RCA-44 and surpasses it in frequency response
and dynamics.
Speciﬁcations:
n Transducer type: ribbon dynamic
n Polar response: bidirectional
n Frequency response: 20 to 20,000 Hz
n Sensitivity: 30 mV (–33.5 dBV)/Pa 1 Pa = 94 dB SPL
n Equivalent noise rating: Signal to Noise Ratio: 88 dB (A) (94 dB SPL minus
equivalent noise)
n Output impedance: 92 Ω
Shure PGA181
The Shure PGA181 (Figure 4.55) is a side-address cardioid condenser micro -
phone with a smooth frequency response with a durable construction. This is
a versatile go-to mic for use with a wide range of applications, including acoustic
and ampliﬁed instruments, vocals and live rehearsal recording and performance.
Speciﬁcations:
n Transducer type: electret condenser
n Polar response: cardioid
n Frequency response: 50 to 20,000 Hz
n Sensitivity: –38 dBV/Pa (12.7 mV)
n Output impedance: 120 Ω
169
Microphones: Design and Application  CHAPTER 4
FIGURE 4.55
Shure PGA181 electret
condenser microphone.
(Courtesy of Shure Brothers,
Inc., www.shure.com,
Images © 2017, Shure
Incorporated—used with
permission)

Microphone Selection
170
AKG C214
The AKG C214 (Figure 4.56) is the younger brother of the legendary C414
condenser. Engineered for highest linearity and neutral sound, this mic uses
the same 1-inch capsule as the C414 in a single-diaphragm, cardioid-only
design.
Speciﬁcations:
n Transducer type: condenser
n Polar response: omnidirectional, wide cardioid, cardioid, hypercardioid,
ﬁgure 8 and four intermediate settings
n Bass cut ﬁlter slope: 12 dB/octave at 40 Hz and 80 Hz; 6 dB/octave at 160
Hz
n Frequency response: 20 to 20,000 Hz
n Sensitivity: 20 mV/Pa
FIGURE 4.56
AKG C 214 condenser
microphone. (Courtesy of
AKG Acoustics GMBH,
www.akg.com)
Neumann TLM102
The TLM102 (Figure 4.57) has a newly developed large-diaphragm capsule
(cardioid) with a maximum sound-pressure level of 144 dB, which permits the
recording of percussion, drums, amps and other very loud sound sources. For
vocals and speech; a slight boost above 6 kHz provides for excellent presence
of the voice in the overall mix. Up to 6 kHz the frequency response is extremely
linear, ensuring minimal coloration and a clearly deﬁned bass range. The capsule
has an elastic suspension for the suppression of structure-borne noise.
FIGURE 4.57
Neumann TLM 102
condenser microphone.
(Courtesy of Georg
Neumann GMBH,
www.neumann.com)

171
Microphones: Design and Application  CHAPTER 4
Speciﬁcations:
n Transducer type: condenser
n Polar response: cardioid
n Frequency response: 20 to 20,000 Hz
n Sensitivity: 11 mV/Pa
ADK Z-251
The ADK Z-Mod Tube Microphones (Figure 4.58) are Class A (Discrete) 9 Polar-
Pattern Tube (Valve) Condenser Microphones hand-built as “One-Offs” using
a custom Belgium-Designed PC Board built with Hi-Fi methods and materials.
Features German Hand-Selected Valve, Top-of-the-Line Components (like a
USA Jensen Transformer) with Australian-Designed GK capsules. Based on ADK’s
Area 51 TT chassis, this is a totally Customized Product which can be custom-
built “One-Off” for each client based on one’s preference for Historical, Vintage
Curves-Microphone Response Characteristics.
Speciﬁcations:
n Acoustical operating principle: 27.5mm large diaphragm pressure gradient
transducer
n Directional pattern: cardioid, ﬁgure 8, omni
n Frequency range: 20Hz ~ 20kHz
n Sensitivity: –35 dB (17.8mV/Pa)
n Output impedance: 200 Ω
n Equivalent noise A-weighted: 11 dB
n S/N ratio (A-weighted, rel 1Pa): 83 dB
n Maximum SPL: 136 dB with 16 dB pad
n Dynamic range: 125 dB
FIGURE 4.58
ADK Z-251 Custom Tube
(Valve) Condenser
Microphone. (Courtesy of
ADK Microphones,
www.adkmic.com)
Townsend Labs Sphere L22™
The Sphere L22 microphone modeling system (Figure 4.59) comprises this
precision dual-channel microphone, plus an audio plug-in for your DAW. The
software plug-in does all of the DSP processing used to create the microphone
models. Using a plug-in, instead of DSP processing built into the mic, has the

Microphone Selection
172
major advantage of allowing the user to change all of the modeling settings,
such as mic type and polar pattern, after the audio is recorded.
The Sphere™ system consists of a high precision dual channel microphone
(front and back capsules can be recorded to separate channels), which when
paired with the included Sphere DSP plug-in (UAD, VST, AU, AAX Native)
accurately models the response of a wide range of mics, including transient
response, harmonics, proximity effect and three-dimensional polar response.
Using a dual-capsule microphone with dual-outputs makes it possible to more
completely capture the sound ﬁeld—including the directional and distance
information otherwise lost with a conventional single-channel microphone. This
allows the Sphere system to precisely reconstruct how different microphones
respond to the sound ﬁeld, allowing you to change mic type, polar pattern, and
other microphone characteristics, even after tracking.
Speciﬁcations:
n Equivalent noise level: 7 dB-A
n Max SPL at 0.5% THD: 140 dB (with –20 dB pad engaged)
n Pad attenuation: –10 db and –20 dB
n Sensitivity: 22 mV/Pa
n Output connector: 5-pin male XLR
n Breakout cable: 10 feet (3m) 5-pin female XLR to dual 3-pin male XLR
n Weight (mic only): 1.7 lbs. (770 grams)
n Dimensions (mic only): 8.9 inches × 2.5 inches (225mm x 63mm)
n Output impedance: 200 Ω
n Recommended load impedance: >1000 Ω
n Phantom power
n Voltage: 44 to 52 V per channel
n Current: 5 mA per channel typical, 8 mA at max SPL
FIGURE 4.59
Townsend Labs Sphere
L22™ microphone system.
(Courtesy of Townsend Labs
Inc., www.townsendlabs.
com and Universal Audio,
www.uaudio.com © 2017
Universal Audio, Inc. All
rights reserved. Used with
permission)

Telefunken U47, C12 and ELA M251E
Telefunken (Figure 4.60) offers up historic recreations of classic microphones
(in addition to their own line of microphones) that are based around the
distinctive tube mic sound, blending vintage style and sound with the reliability
of a modern-day microphone.
173
Microphones: Design and Application  CHAPTER 4
FIGURE 4.60
Telefunken U47, C12 and
ELA M251E classic
condenser microphone
recreations. (Courtesy of
Telefunken Elektroakoustik,
www.telefunken-
elektroakustik.com)


From its inception in Germany in the late 1920s (Figure 5.1) and its American
introduction by Jack Mullin in 1945, the analog tape recorder (or ATR) had steadily
increased in quality and universal acceptance to the point that professional and
personal studios had totally relied upon magnetic media for the storage of analog
sound onto reels of tape. With the dawning of the project studio and computer-
based DAWs, the use of two-channel and multitrack ATRs has steadily dwindled
to the point where no new analog tape machine models are currently being
manufactured. In short, recording to analog tape has steadily become a high-
cost, future-retro, specialty process for getting a “certain sound.” That being said,
the analog recording process is still highly regarded and even sought after by
many studios for its characteristic sound and by others as a raised ﬁst against
the onslaught of the “evil digital empire.” Without delving into the ongoing
debate of the merits of analog versus digital, I think it’s fair to say that each
has its place and its own distinct type of sound and application in audio and
music production. Although professional analog recorders are usually much
more expensive than their digital counterparts, as a general rule, a properly
aligned, professional analog deck will have a particular sound that’s often
175
CHAPTER 5
The Analog Tape 
Recorder
FIGURE 5.1
One of John T. (Jack)
Mullin’s WWII vintage
German Magnetophones,
which was confiscated
during the war and brought
to the United States.
(Courtesy of the late John T.
Mullin)

The Analog Tape Recorder
described as being full, punchy, gutsy and “raw.” In fact, the limitations of tape
are often used as a form of “artistic expression” to get a certain sound. From
this, it’s easy to see and hear why the analog tape recorder isn’t dead yet—and
probably won’t be for some time.
TO COMMIT OR NOT TO COMMIT IT TO TAPE?
Before we delve into the inner workings of the analog tape recorder, let’s take
a moment to discuss ways in which the analog tape sound can be taken
advantage of in the digital and project studio environment. Before you go out
and buy your own deck, however, there are other cost-effective ways to get “that
sound” on your own projects. For example:
n In recent times, a growing number of plug-ins have become available that
can emulate (or approximate) the harmonic and overdriven sound of an
analog tape track. Further info on such tape plug-ins can be found later
in this chapter and in the book’s chapter on signal processing.
n Rent a studio that has an analog multitrack for a few hours or days. You
could record speciﬁc tracks to tape, transfer existing digital tracks to tape
or dump an entire ﬁnal mixdown to tape. For the cost of studio time and
a reel of tape, you could inject your project with an entirely new type of
sound.
n Rent an analog machine from a local studio equipment service. For a rental
fee and basic cartage charges, you could reap the beneﬁts of having an
analog ATR for the duration of a project, without any undue ﬁnancial and
maintenance overhead.
A few guidelines should also be kept in mind when recording and/or transferring
tracks to or from a multitrack recorder:
n Obviously, high recording levels add to that sought-after “overdriven”
analog sound; however, driving a track too hard (hot) can actually kill a
track’s deﬁnition or “air.” The trick is often to ﬁnd a center balance between
the right amount of saturation, distortion and dynamic range.
n Noise reduction can be a good thing, but it can also diminish what is
thought of as that “classic analog sound.” Newer, wide tape width recorders
(such as ATR Services’ ATR-102 1-inch, 2-track and the 108C 2-inch, 8-
track recorder), as well as older 2-inch, 16-track recorders, can provide
improved deﬁnition without the need for noise reduction.
MAGNETIC RECORDING AND ITS MEDIA
At a basic level, an analog audio tape recorder can be thought of as a sound
recording device that has the capacity to store audio information onto a
magnetizable tape-based medium and then play this information back at a later
time. By deﬁnition, analog refers to something that’s “analogous,” similar to or
comparable to something else. An ATR is able to transform an electrical input
176

177
The Analog Tape Recorder  CHAPTER 5
signal directly into a corresponding magnetic energy that can be stored onto
tape in the form of magnetic remnants. Upon playback, this magnetic energy
is then converted back into a corresponding electrical signal that can be
ampliﬁed, mixed, processed and heard.
The recording media itself is composed of several layers of material, each serving
a speciﬁc function (Figure 5.2). The base material that makes up most of a tape’s
thickness is often composed of polyester or polyvinyl chloride (PVC), which is
a durable polymer that’s physically strong and can withstand a great deal of
abuse before being damaged. Bonded to the PVC base is the all-important layer
of magnetic oxide. The molecules of this oxide combine to create some of the
smallest known permanent magnets, which are called domains (Figure 5.3a).
On an unmagnetized tape, the polarities of these domains are randomly oriented
over the entire surface of the tape. The resulting energy force of this random
magnetization at the reproduce head is a general cancellation of the combined
domain energies, resulting in no signal at the recorder’s output (except for the
tape noise that occurs due to the residual domain energy output . . . hisssssssss).
FIGURE 5.2
Structural layers of
magnetic tape.
When a signal is recorded, the resulting magnetization from the record head
polarizes the individual domains (at varying degrees in positive and negative
angular directions) in such a way that their average magnetism produces a much
larger combined magnetic ﬂux (Figure 5.3b). When the tape is pulled across
the playback head at the same, constant speed at which it was recorded, this
alternating magnetic output is then converted back into an alternating signal
that can then be ampliﬁed and further processed for reproduction.
FIGURE 5.3
Orientation of magnetic
domains on unmagnetized
and magnetized recording
tape. (a) The random
orientation of an
unmagnetized tape results
in no output. (b) Magnetized
domains result in an
average flux output at the
magnetic head.

Magnetic Recording and Its Media
178
The Professional Analog ATR
Professional analog ATRs can be found in 2-, 4-, 8-, 16- and 24-track formats.
Each conﬁguration is generally best suited to a speciﬁc production and
postproduction task. For example, a 2-track ATR is generally used to record the
ﬁnal stereo mix of a project (Figure 5.4), whereas 8-, 16- and 24-track machines
are obviously used for multitrack recording (Figure 5.5). Although no pro -
fessional analog machines are currently being manufactured, quite a few decks
can be found on the used market in varying degrees of working condition.
Certain recorders (such as the ATR-108C 2-inch, multitrack/mastering recorder)
can be switched between tape width and track formats, allowing the machine
to be converted to handle a range of multitrack, mixdown and mastering tasks.
FIGURE 5.4
Examples of an ATR. 
(a) ATR-102 1-inch stereo
mastering recorder. 
(b) ATR-108C 2-inch
multitrack/mastering
recorder. (Courtesy of ATR
Service Company,
www.atrservice.com)
FIGURE 5.5
Analog multitrack machines
(right) coexisting with their
digital tape counterparts
(left). (Courtesy of Galaxy
Studios, www.galaxy.be)
THE TAPE TRANSPORT
The process of recording audio onto magnetic tape depends on the transport’s
capability to pass a precise length of tape over the record head at a speciﬁc and
constant speed, with a uniform tension (Figure 5.6). During playback, this
relationship is maintained by again moving the tape across the heads at the
same speed, thereby preserving the program’s original pitch, rhythm and
duration.

179
The Analog Tape Recorder  CHAPTER 5
This constant speed and tension movement of the tape across a head’s path is
initiated by simply pressing the Play button. The drive can be disengaged at
any time by pressing the Stop button, which applies a simultaneous breaking
force to both the left and right reels. The Fast Forward and Rewind buttons
cause the tape to rapidly shuttle in the respective directions in order to locate
a speciﬁc point. Initiating either of these modes engages the tape lifters, which
raise the tape away from the heads (deﬁnitely an ear-saving feature). Once the
play mode has been engaged, pressing the Record button allows audio to be
recorded onto any selected track or tracks.
Beyond these basic controls, you might expect to run into several differences
between transports (often depending on the machine’s age). For example, older
recorders might require that both the Record and Play buttons be simultaneously
pressed in order to go into record mode; while others may begin recording when
the Record button is pressed while already in the Play mode.
On certain older professional transports (particularly those wonderful Ampex
decks from the 1950s and 1960s), stopping a fast moving tape by simply
pressing the Stop button might stretch or destroy a master tape, because the
inertia is simply too much for the electro-mechanical brake to deal with. In
such a situation, a procedure known as “rocking” the tape is used to prevent
tape damage. The deck can be rocked to its stop position by engaging the fast-
wind mode in the direction opposite the current travel direction until the tape
slows down to a reasonable speed . . . at which point it’s safe to press the Stop
button. Go ahead—thread a used, blank tape onto a machine and try it for
yourself.
In recent decades (1980 and later), tape transport designs have made use of
total transport logic (TTL), which places transport and monitor functions under
microprocessor control. This has a number of distinct advantages. For example,
with TTL, the recorder can sense the tape speed and direction and then
automatically rock the transport motors until the tape can safely be stopped or
it might slow the tape to a point where the deck can seamlessly slip into play,
record or stop mode.
Most modern ATRs are equipped with a control that allows the tape to be
shuttled at various wind speeds in either direction. This allows a speciﬁc cue
point to be located by listening to the tape at varying play speeds, or the control
FIGURE 5.6
Relationship of time to the
physical length of recording
tape.

Magnetic Recording and Its Media
can be used to gently and evenly wind the tape onto its reel at a slower speed
for long-term storage. The Edit button (which can be found on certain
professional machines) often has two operating modes: stop-edit and dump-
edit. If the Edit button is pressed while the transport is in the stop mode, the
left and right tape reel brakes are released and the tape sensor is bypassed. This
makes it possible for the tape to be manually rocked back and forth until the
edit point is found. Often, if the Edit button is pressed while in the play mode,
the take-up turntable will be disengaged and the tape sensor is bypassed. This
allows unwanted sections of tape to be spooled off the machine (and into the
trash can) while listening to the material as it’s being discarded during playback.
A safety tape guide switch, which is incorporated into all professional transports,
initiates the stop mode when it senses the absence of tape along its guide path;
thus, the recorder stops automatically at the end of a reel or should the tape
accidentally break. This switch might be built into the tape-tension sensor arm,
or it might exist in the form of a light beam that’s interrupted when tape is
present.
Most newer, professional ATRs are equipped with automatic tape counters that
accurately read out time in hours, minutes, seconds and sometimes frames
(00:00:00:00). Many of these recorders have digital readout displays that double
as tape-speed indicators when in the “vari-speed” mode. This function
incorporates a control that lets you vary the tape speed from ﬁxed industry
standards. On many tape transports, this control can be continuously varied
over a ±20% range from the 7 1/2, 15 or 30 ips (inches per second) standards.
THE MAGNETIC TAPE HEAD
Most professional analog recorders use three magnetic tape heads, each of which
performs a specialized task:
n Record
n Reproduce
n Erase
The function of a record head (Figure 5.7) is to electromagnetically transform
analog electrical signals into corresponding magnetic ﬁelds that can be
permanently stored onto magnetic tape. In short, the input current ﬂows through
coils of wire that are wrapped around the head’s magnetic pole pieces. Since
the theory of magnetic induction states that “whenever a current is injected into
metal, a magnetic ﬁeld is created within that metal,” a magnetic force is caused
to ﬂow through the coil, into the pole pieces and across the head gap. Like
electricity, magnetism ﬂows more easily through some media than through
others. The head gap between poles creates a break in the magnetic ﬁeld, thereby
creating a physical resistance to the magnetic “circuit.” Since the gap is in
physical contact with the moving magnetic tape, the tape’s magnetic oxide offers
a lower resistance path to the ﬁeld than does the non-magnetic gap. Thus, the
ﬂux path travels from one pole piece, into the tape and to the other pole. Since
180

181
The Analog Tape Recorder  CHAPTER 5
the magnetic domains retain their polarity and magnetic intensity as the tape
passes across the gap, the tape now has an analogous magnetic “memory” of
the recorded event.
The reproduce or playback head (Figure 5.8) operates in a way that’s opposite
of the record head. When a recorded tape track passes across the reproduce head
gap, a magnetic ﬂux is induced into the pole pieces. Since the theory of magnetic
induction also states that “whenever a magnetic ﬁeld cuts across metal, a current
will be set up within that metal,” an alternating current is caused to ﬂow through
the pickup coil windings, which can then be ampliﬁed and processed into a
larger output signal.
FIGURE 5.7
The record head.
FIGURE 5.8
The playback head.
It’s important to note that the reproduce head’s output is nonlinear because
this signal is proportional to both the tape’s average ﬂux magnitude and the
rate of change of this magnetic ﬁeld. This means that the rate of change increases
as a direct function of the recorded signal’s frequency. Thus, the output level
of a playback head is effectively doubled for each doubling in frequency,
resulting in a 6 dB increase in output voltage for each increased octave. The
tape speed and head gap width work together to determine the reproduce head’s

Magnetic Recording and Its Media
182
upper-frequency limit, which in turn determines the system’s overall bandwidth.
The wavelength of a signal that’s recorded onto tape is equal to the speed at
which tape travels past the reproduce head, divided by the frequency of the
signal; therefore, the faster the tape speed, the higher the upper-frequency limit.
Likewise, the smaller the head gap, the higher the upper-frequency limit.
The function of the erase head is to effectively reduce the average magnetization
level of a recorded tape track to zero, thereby allowing the tape track to be re-
used or recorded over. After a track is placed into the record mode, a high-
frequency, high-intensity sine-wave signal is fed into the erase head (resulting
in a tape that’s being saturated in both the positive- and negative-polarity
directions). This alternating saturation occurs at such a high speed that it serves
to confuse any magnetic pattern that previously existed on the tape. As the tape
moves away from the erase head, the intensity of the magnetic ﬁeld decreases
over time, leaving the domains in a random orientation, with a resulting average
magnetization or output level that’s as close to zero as tape noise will allow.
EQUALIZATION
Equalization (EQ) is a term that’s used to denote an intentional change in relative
amplitudes at different frequencies. Because the analog recording process isn’t
linear, equalization is needed to achieve a ﬂat frequency-response curve when
using magnetic tape. The 6 dB-per-octave boost that’s inherent in the playback
head’s response curve requires that a complementary equalization cut of 6 dB
per octave be applied within the playback circuit (see Figure 5.9).
FIGURE 5.9
A flat frequency playback
curve results due to
complementary equalization
in the playback circuit.
BIAS CURRENT
In addition to the nonlinear changes that occur in playback level relative to
frequency, another discrepancy in the recording process exists between the
amount of magnetic energy that’s applied to the record head and the amount
of magnetism that’s retained by the tape after the initial recording ﬁeld has been
removed. As Figure 5.10a shows, the magnetization curve of tape is linear
between points A and B, as well as between points C and D. Signals greater

183
The Analog Tape Recorder  CHAPTER 5
than A and D have reached the saturation level and are subject to clipping
distortion. Conversely, signals falling within the B to C range are too low in
ﬂux level to adequately magnetize the domains during the recording process
(i.e., they’re not strong enough to force the individual magnets to change
orientation). For this reason, it’s important that low-level signals be boosted in
level so that they’re pushed into the linear range. This boost is applied by mixing
an AC bias current (Figure 5.10b) with the audio signal. This bias current is
applied by mixing the incoming audio signal with an ultrasonic sine-wave
signal (often between 75 and 150 kHz). The combined signals are then
amplitude modulated in such a way that the overall magnetic ﬂux levels are
given an extra “oomph,” which effectively boosts the signal above the nonlinear
zero-crossover range and into the linear (free of distortion) portion of the curve.
In fact, if this bias signal weren’t added, distortion levels would be so high as
to render the analog recording process useless.
MONITORING MODES
The output signal of a professional ATR channel can be switched between three
working modes:
n Input
n Reproduce
n Sync
In the input (source) mode, the signal at the selected channel output is derived
from its input signal. Thus, with the ATR transport in any mode (including
stop), it’s possible to meter and monitor the signal that’s present at a channel’s
selected input. In the reproduce mode, the output and metering signal is derived
from the playback head. This mode can be useful in two ways: it allows
previously recorded tapes to be played back, and it enables the monitoring of
material off of the tape while in the record mode. The latter provides an
immediate quality check of the ATR’s entire record and reproduce process. 
The sync mode (originally known as selective synchronization, or sel-sync) is a
required feature in analog multitrack ATRs because of the need to record new
material on one or more tracks while simultaneously playing back tracks that
FIGURE 5.10
The effects of bias current
on recorded linearity: 
(a) magnetization curve
showing distortion at lower
levels; (b) after bias is
applied, the signal is
boosted back into the
curve’s linear regions.

Magnetic Recording and Its Media
184
have been previously recorded (during a process called overdubbing). Here’s the
deal—using the record head to lay down one or more tracks while listening to
previously recorded tracks off of the reproduce head would actually cause the
newly recorded track(s) to be out of sync with the others on ﬁnal playback (due
to the physical distance between the two heads, as shown in Figure 5.11a). To
prevent such a time lag, all of the reproduced tracks must be monitored off of
the record head at the same time that the new tracks are being laid down onto
the same head. Since the record head is used for both recording and playback,
there is no physical time lag and, thus, no signal delay (Figure 5.11b).
To Punch or Not to Punch
You’ve all heard the age-old adage “$%& happens.” Well, it happens in the
studio—a lot! Whenever a mistake or bad line occurs during a multitrack
session, it’s often (but not always) possible to punch-in on a speciﬁc track or
set of tracks. Instead of going back and re-recording an entire song or overdub,
performing a punch involves going back and re-recording over a speciﬁc section
in order to ﬁx a bad note, musical line, you name it. This process is done by
cueing the tape at a logical point before the bad section and then pressing play.
Just before the section to be ﬁxed, pressing the record button (or entering record
under automation) will place the track into record mode. At the section’s end,
pressing the play button again will cause the track to smoothly fall back out of
record, thereby preserving the section following the punch. From a monitor
standpoint, the recorder begins playback in the sync mode; once placed into
record, the track switches to monitor the input source. This lets the performers
hear themselves during the punch while listening to playback both before and
after the take.
When performing a punch, it’s often far better to “ﬁx” the track immediately
after the take has been recorded, while the levels, mic positions and performance
vibe are still the same. This also makes it easier to go back and re-record the
entire song or a larger section should the punch not work. If the punch can’t
be performed at that time, however, it’s always a good idea to take detailed
notes about mic selection, placement, preamps and so on to recreate the session’s
setup without having to guess the details from memory.
FIGURE 5.11
The sync mode’s function.
(a) In the monitored
playback mode, the
recorded signal lags behind
the recorded signal, thereby
creating an out-of-sync
condition. (b) In the sync
mode, the record head acts
as both record and playback
head, bringing the signals
into sync.

185
The Analog Tape Recorder  CHAPTER 5
As any experienced engineer/producer knows, performing a punch can be tricky.
In certain situations, it’s a complete no-brainer—for example, when a stretch
of silence the size of a Mack Truck exists both before and after the bad section,
you’ll have plenty of space to punch in and out. At other times, a punch can
be very tight or problematic (e.g., if there’s very little time to punch in or out,
when trying to keep vocal lines ﬂuid and in-context, when it’s hard to feel the
beat of a song or if it has a fast rhythm). In short, punching-in shouldn’t be
taken too lightly nor taken so seriously that you’re afraid of the process. Talk
it over with the producer and/or musicians. Is this an easy punch? Does the
section really need ﬁxing? Do we have the time right now? Or, is it better just
to redo the song? In short, the process is totally situational and requires attention,
skill, experience and sometimes a great deal of luck.
n Before committing the punch to tape, it’s often a wise idea to rehearse the
punch, without actually committing the ﬁx to tape. This has the advantage
of giving both you and the performer a chance to practice beforehand.
n Some analog decks (and almost all DAWs) will let you enter the punch-
in and punch-out times under automation, thereby allowing the punch
to be automatically performed with greater precision.
n If you’re recording onto the same track, a fudged punch may leave you
with few options other than to re-record the entire song or section of a
song. An alternative to this dilemma would be to record the ﬁx into a
separate track and then switch between tracks in mixdown (a process
known as compositing or simply “comping”).
n The track(s) could be transferred to a DAW, where the edits could be
performed in the digital domain.
Tape, Tape Speed and Head Configurations
Professional analog ATRs are currently available in a wide range of track- and
tape-width conﬁgurations. The most common analog conﬁgurations are 2-track
mastering machines that use tape widths of 1/4 inch, 1/2 inch, and even 1 inch,
as well as 16- and 24-track machines that use 2-inch tape. Figure 5.12 details
FIGURE 5.12
Analog track configurations
for various tape widths.

Magnetic Recording and Its Media
many of the tape formats that can be currently found. Optimal tape-to-head
performance characteristics for an analog ATR are determined by several
parameters: track width, head-gap width and tape speed. In general, track widths
are on the order of 0.080 inch for a 1/4-inch 2-track ATR; 0.070 inch for 1/2-
inch 4-track, 1-inch 8-track, and 2-inch 16-track formats or 0.037 inch for the
2-inch 24-track format. As you might expect, the greater the recorded track width,
the greater the amount of magnetism that can be retained by the magnetic tape,
resulting in a higher output signal and an improved signal-to-noise ratio. The
use of wider track widths also makes the recorded track less susceptible to signal-
level dropouts.
The most common tape speeds used in audio production are 15 ips (38 cm/sec)
and 30 ips (76 cm/sec). Although 15 ips will eat up less tape, 30 ips has gained
wide acceptance in recent years for having its own characteristic sound (often
having a tighter bottom end), as well as a higher output and lower noise ﬁgures
(which in certain cases eliminates the need for noise reduction). On the other
hand, 15 ips has a reputation for having a more “gutsy,” rugged sound.
PRINT-THROUGH
A form of deterioration in a recording’s quality, known as print-through, begins
to occur almost immediately after a recording has been made. This effect is the
result of the transfer of a recorded signal from one layer of tape to an adjacent
outer track layer by means of magnetic induction, which gives rise to an audible
false signal or echo on playback. The effects of print-through are greatest when
recording levels are very high, and the effect decreases by about 2 dB for every
1 dB reduction in signal level. The extent of this condition also depends on
such factors as length of storage, storage temperature and tape thickness (tapes
with a thicker base material are less likely to have severe print-through problems).
Because of the effects of print-through, the standard method for professionally
storing a recorded analog tape is in the tails-out position, using the following
method:
1. Professional analog tape should always be stored tails-out (onto the right-
hand take-up reel).
2. Upon playback, the tape should be rewound onto the left-most “supply
reel.”
3. During playback, feed the tape back onto the right-hand take-up reel, after
which time it can again be removed for storage.
4. If the tape has been continuously wound and rewound during the session,
it’s often wise to rewind the tape and then smoothly play or slow-wind
the tape onto the take-up reel, after which time it can be removed for
storage.
So why do we go through all this trouble? When a tape is improperly stored
heads-out, the signal will print through to the outer tape layer. Upon playback,
186

187
The Analog Tape Recorder  CHAPTER 5
the signal will be heard as an unnatural pre-echo that can readily be heard. 
If a tape is properly stored using the tails-out method (Figure 5.13), the print-
through will bleed to the outer layers, causing the signal to bleed in such a way
that the echo will follow the original signal. This will result in a decay that’s
subconsciously perceived by the listener as a natural after-echo instead of as a
distracting pre-echo.
ANALOG TAPE NOISE
The roughly 60-dB signal-to-noise (S/N) limitation that’s imposed on a
conventional analog ATR audio track is dictated by saturation (at the high signal
level end) and tape hiss at the low-end (which is heard when the overall
recorded level of the program is too low). Should an optimum level produce
an unacceptable amount of noise, the engineer is faced with several options:
record at a higher level (with the possibility of increased distortion) or change
the signal’s overall dynamic range by raising low-level signals above the noise
(often with compression or limiting).
Analog tape noise might not be much of a problem when dealing with one or
two tracks in an audio production, but the combined noise and other distortions
that can occur when 8, 16, 24 or 48 tracks are combined can range from being
bothersome to downright unacceptable. The following types of noises are often
major contributors to the problem:
n Tape and ampliﬁer noise
n Crosstalk between tracks
n Print-through
n Modulation noise
Modulation noise is a high-frequency component that causes sonic “fuzziness”
by introducing sideband frequencies that can distort the signal (Figure 5.14).
This noise-based distortion is due to the magnetic and mechanical properties
of the analog recording process itself, and actually increases as recorded levels
rise. This noise is often higher in level than you might expect, and when
combined with asperity noise (additional sideband frequencies that are also
FIGURE 5.13
Recorded analog tapes
should always be stored in
the tails-out position.

Magnetic Recording and Its Media
188
FIGURE 5.14
Modulation noise of a
recorded analog sine wave.
introduced by the analog record/playback process) these distortions deﬁnitely
play a role in what is called the “analog sound.”
These analog-based noises can be reduced to acceptable levels by using different
combinations of the following actions:
n Increased the tape speed in order to record at higher ﬂux levels.
n Using an ATR with wider recorded tracks (i.e., allowing for higher
record/playback levels and reduced crosstalk specs).
n By using noise reduction hardware systems (such as Dolby-A and DBX
NR). These are older systems from “back-in-the-day” when analog was
king. They are not easy to get hold of and may require a bit of setup and
understanding before being inserted into the analog tape chain.
It should be noted that by using tape formulations which combine low noise
and high output (resulting in an increased S/N ratio of 3 dB or more), noise
levels can be reduced even further. However, when all’s said and done, making
most or all of the above improvements might be simply too costly and
impractical. In the end, it’s important to realize that noise is simply an inherent
part of the analog recording process and the goal is to minimize its effects as
much as possible.
1. Feed a 0-VU, 1-kHz test tone to a track on a
professional analog recorder.
2. Listen to the recorder’s source (input) signal
through the monitors at a moderate level.
3. Switch the recorder to monitor the tone from the
track’s playback (tape) head, while in record.
Does it sound different?
Try This: Analog Tape Modulation and Asperity Noise
D I Y
 do  it  yourself

CLEANLINESS
It’s very important for the magnetic recording heads and all moving parts of an
ATR transport deck to be kept free from dirt and oxide shed. Oxide shed occurs
when friction causes small particles of magnetic oxide to ﬂake off and accumulate
on surface contacts. This accumulation is most critical at the surface of the
magnetic recording heads, since even a minute separation between the magnetic
tape and heads can cause high-frequency separation loss. For example, a signal
that’s recorded at 15 ips and has an oxide shed buildup of 1 mil (0.001 inch)
on the playback head will cause a 15 kHz signal to drop by 55 dB below its
standard operating level. Denatured (isopropyl) alcohol or an appropriate
cleaning solution should be used to clean transport tape heads and guides (with
the exception of the machine’s pinch roller and other rubber-like surfaces) at
regular intervals.
DEGAUSSING
Magnetic tape heads are made from a magnetically soft metal, which means
that the alloy is easily magnetized, but once the coil’s current is removed, the
core won’t retain any of this magnetism. Small amounts of residual magnetism,
however, will build up over time, which can actually partially erase high-
frequency signals from a master tape. For this reason, all of the tape heads should
be demagnetized after 10 hours of operation with a professional head
demagnetizer. This handheld device works much like an erase head in that it
saturates the magnetic head with a high-level alternating signal that randomizes
residual magnetic ﬂux. Once a head has been demagnetized (after 5 to 10
seconds), it’s important to move the tool to a safe distance from the tape 
heads at a speed of less than 2 ips before turning it off, so as to avoid induc-
ing a larger magnetic ﬂux back into the head. Before an ATR is aligned, the
magnetic tape heads should always be cleaned and demagnetized in order to
obtain accurate readings and to protect your expensive reference alignment
tapes.
Editing Magnetic Tape
Obviously, editing magnetic tape is “quite” different than editing digital audio
on a DAW. It’s a “very” hands-on thing (all puns intended). The process of
editing tape is not entirely destructive, but it’s close to it, and requires a good
ear, good eye-to-hand coordination and a sharp razor blade.
189
The Analog Tape Recorder  CHAPTER 5
FIGURE 5.15
A tape editing block is used
to cut and splice together
analog tape in the editing
process.

Magnetic Recording and Its Media
The process makes use of a tape edit block (Figure 5.15), special adhesive tape
(that is specially formulated not to “bleed” its sticky adhesive onto tape heads,
rollers and other parts in the tape’s path) and a grease pencil. The process goes
much like this:
n First, locate the in and out edit points on the tape. It’s up to the operator
to determine a point just before where things go wrong and then to ﬁnd
the good part later in the tape, where the music is to be “picked up” in
order to ﬁx the mistake, etc.
n Once the general areas where the edit is to be made are located on the
tape, locate the ﬁrst edit point. The machine can then be placed into “Edit
Mode” (which disengages the reel brakes and tape lifters). This allows the
tape to be manually “rocked” forward and backwards to listen for the edit
point. These audible cues are often a kick, snare or percussive sound; it
can also be a sung word, literally anything. You just keep rocking the tape
back and forth across the playback head, until you’ve zeroed in on the
exact point to be edited.
n At this point, you take your grease pencil (black or white can be used)
and gently, but ﬁrmly mark the tape’s backing at the exact point where
the tape rests over the playback head.
n Next, play or locate the tape to where the edit is to be picked up—manually
locate the part to be kept in the edit and make your out edit mark on the
tape.
n Now, you can wind back and go about the process of locating the ﬁrst
part of the edit, place the exact “edit in point” on the edit block (almost
always over the 90º angle) and using a fresh single-sided razor blade, cut
across the tape with a smooth, clean movement.
n Locate the “edit in point” on the tape and place it on the edit block at a
point that butts up to your previous cut and cut the tape.
n Now, all you need to do is cut off about 1–1.5 inches of the special editing
adhesive tape and bind the two pieces together, rewind the tape (remember
to take the machine out of Edit Mode) to a point before your edit and
listen to your masterpiece.
n Obviously, during this learning phase, it’s important to take your time,
save the unused tape, just in case you miss your marks and, if possible,
do a lot of practice tries with material that’s not important (i.e., record
some of your favorite music onto tape and have fun with it), and possibly
have someone who’s used to cutting tape standing around to help you
get the hang of it (if you can ﬁnd such a person).
190

Backup and Archive Strategies
In this day of hard drives and the cloud, we’ve all come to know the importance
of backing up our data. With important music and media projects, it’s equally
important to create a backup copy of your analog projects in case of an
unforeseen catastrophe or as added insurance that future generations can enjoy
the fruits of your work.
BACKING UP YOUR ANALOG PROJECT
The one basic truth that can be said about analog magnetic tape is that this
medium has withstood the test of time. With care and reconditioning, tapes
that have been recorded in the 1940s have been fully restored, allowing us to
preserve and enjoy some of the best music of the day. On the other hand, digital
data has two points that aren’t exactly in its favor:
n Data that resides on hard drives isn’t the most robust of media for storage
over time. Even optical media (which can be rated to last over 100 years)
haven’t really been proven to last.
n Even if the data remains intact, with the ever-increasing advances in
computer technology, who’s to say that the media, drives, programs,
session formats and ﬁle formats will be around in 10 years, let alone 50!
These warnings aren’t slams against digital, just precautions against the march
of technology versus the need for media preservation. For the above reasons,
media preservation is a top priority for such groups as the Recording Academy’s
Producers and Engineers Wing (P&E Wing), as well as for many major record
labels—so much so that many stipulate in their contracts that multitrack sessions
(no matter what the original medium) must be transferred and archived to 2-
inch multitrack analog tape.
When transferring digital tracks to an analog machine, it’s always wise to make
sure that the recorder has been properly calibrated and that reference tones 
(1 kHz, 10 kHz, 16 kHz and 100 Hz) have been recorded at the beginning of
the tape. When copying from analog to analog, both machines should be
properly calibrated, and the calibration source for the newly recorded tones
should be the master tape. If a SMPTE track is required, be sure to stripe the
copy with a clean, jam-sync code. The backing up of analog tapes and/or digital
data usually isn’t a big problem—unless you’ve lost your original masters. 
In this situation, a proper safety master can be the difference between panic
and peace.
191
The Analog Tape Recorder  CHAPTER 5

Magnetic Recording and Its Media
192
ARCHIVE STRATEGIES
Just as it’s important to back up your media, it’s also important that both the
original and backup media be treated and stored properly. Here are a few
guidelines:
n As stated earlier, always store the tapes tails-out.
n Store the tapes onto the take-up storage reel at slow-wind or play speeds
(to minimize tape edge wear).
n Store the boxes vertically. If they’re stored horizontally, the outer tape edges
could get bent and become damaged.
n Media storage facilities exist that can store your masters or backups for a
fee. If this isn’t an option, store them is an area that’s cool and dry (e.g.,
no temperature extremes, in low humidity, no attics or basements).
n Store your masters and backups in separate locations. In case of a ﬁre or
other disaster one would be lost, but not both (always a good idea with
digital data, as well).
C.L.A.S.P.
One specialized, patented system has been developed that can seamlessly
integrate an analog tape machine (of various type and track conﬁgurations) 
into a DAW, to give a disk-based digital system that “analog feel and sound.” 
The C.L.A.S.P. (Closed Loop Analog Signal Processor) literally inserts a work-
ing analog tape and electronics chain between the recording interface and/or
console and the DAW. This system (which is the only system that has latency-
free analog monitoring) takes the recorded signals from your console, interface
or DAW and records them onto tape. The signal is then routed from the tape
machine’s playback head where specialized time-stamp and time-correction are
used to re-align the audio (with sample accuracy) directly into the DAW session
(Figure 5.16).
FIGURE 5.16
The C.L.A.S.P. (Closed Loop
Analog Signal Processor).
(Courtesy of Endless Analog,
www.endlessanalog.com).

193
The Analog Tape Recorder  CHAPTER 5
FIGURE 5.17
The Ampex ATR 102 and
Studer A800 tape emulation
plug-ins for the Apollo
family of interfaces and the
UAD effects processing
card. (Courtesy of Universal
Audio, www.uaudio.com ©
2017 Universal Audio, Inc.
All rights reserved. Used
with permission)
Tape Emulation Plug-Ins
As DAW technology and methods for precisely modeling hardware devices in
software have advanced, new types of plug-ins that can directly emulate the
sound of analog tape machines have come onto the market. These devices are
capable of closely mimicking the sonic character, tape noise, distortion and
changes with virtual tape formulation and bias settings of any number of tape
machines and model makes “virtually” allowing us to plug-in the sound of our
favorite deck onto any workstation track or bus (Figure 5.17).


There’s absolutely no doubt digital audio technology has changed the way that
all forms of media are produced, gathered and distributed. As a driving force
behind human creativity, expression and connectivity, the impact of digital
media production is simply HUGE, and is an integral part of both the medium
and the message within modern-day communications.
As with most other media, these changes have been brought about by the
integration of the personal computer and portable devices into the modern-day
project studio environment. Newer generations of computers and related
hardware peripherals have been integrated into both the pro and project studio
to record, fold, spindle and mutilate audio with astonishing ease. This chapter
is dedicated to helping you get familiar with the various digital system types
and their relation to the modern-day music production studio.
THE LANGUAGE OF DIGITAL
Although digital audio is a varied and complex ﬁeld of study, the basic theory
behind the magic curtain isn’t all that difﬁcult to understand. At its most
elementary level, it’s simply a process by which numeric representations of
analog signals (in the form of voltage levels) are encoded, processed, stored
and reproduced over time through the use of a binary number system.
Just as English-speaking humans communicate by combining any of 26 letters
together into groupings known as “words” and manipulate numbers using the
decimal (base 10) system, the system of choice for a digital device is the binary
(base 2) system. This numeric system provides a fast and efﬁcient means for
manipulating and storing digital data. By translating the alphabet, base 10
numbers or other form of information into a binary language form, a digital
device (such as a computer or microprocessor) is used to perform calculations
and tasks that would otherwise be cumbersome, less cost effective and/or
downright impossible to perform in the analog domain.
195
CHAPTER 6
Digital Audio 
Technology

The Language of Digital
196
To illustrate, let’s take a look at how a human construct can be translated into
a digital language (and back). If we type the letters D, O and G into a word
processor, the computer will quickly go about the task of translating these
keystrokes into a series of 8-bit digital words represented as [0100 0100], [0100
1111] and [0100 0111]. On their own, these digits don’t mean much, but when
these groups are put together, they form a word that represents a four-legged
animal that’s always glad to see you (Figure 6.1).
FIGURE 6.1
Digital and analog
equivalents for an awesome
four-legged animal (“Liné” 
. . . originally from Turkey,
is now the mascot dog for
The Pesto Dealer, Berlin).
In a similar manner, a digital audio system works by sampling (measuring) the
instantaneous voltage level of an analog signal at very precise intervals over
time, and then converts these samples into a series of encoded “words” which
digitally represents the analogous voltage levels at that time interval. By
successively measuring changes in an analog signal’s voltage level (over time),
this successive stream of representative digital words can then be stored in a
form that accurately represents the original analog signal. Once stored, this data
can be processed and reproduced in ways that continue to change the face of
audio production.
It’s interesting to note that binary data can be encoded as logical 1 “on” or 0
“off” states, using various methods to encode data as:
n Voltage or no voltage (circuitry)
n Magnetic ﬂux or no ﬂux (hard disk or tape)
n Reﬂection off of a surface or no reﬂection (CD, DVD or other optical disc
form)
n Electromagnetic waves (broadcast transmission)
From this, you’ll hopefully begin to get the idea that human forms of
communication (i.e., print, visual and audible media) can be translated into a
digital form that can be easily understood and manipulated by a processor.
Once the data has been recorded, stored and/or processed, the resulting binary
data can be reconverted back into a form that can be easily understood by us
humans (such as a display readout, system control, text-to-speech, controller
interaction, you name it). If you think this process of changing one form of
energy into an analogous form (and then back again) sounds like the general
deﬁnition of a transducer—you’re right!

197
Digital Audio Technology  CHAPTER 6
Digital Basics
The following sections provide a basic overview of the various stages that are
involved in the encoding of analog signals into equivalent digital data, and the
subsequent converting of this data back into its original analog form.
The encoding and decoding phases of the digitization process revolve around
two processes:
n Sampling (the component of time)
n Quantization (the signal-level component)
In a nutshell, sampling is a process that affects the overall bandwidth (frequency
range) that can be encoded within a sound ﬁle, while quantization refers to the
volume level and resolution (overall quality and distortion characteristics) of
an encoded signal compared to the original analog signal at its input.
SAMPLING
In the world of analog audio, signals are recorded, stored and reproduced as
changes in voltage levels that continuously vary over time in a continuous
fashion (Figure 6.2a). The digital recording process, on the other hand, doesn’t
operate in this manner; rather, digital recording operates by taking periodic
samples of an analog audio waveform over time (Figure 6.2b), and then
calculates each of these snapshot samples into grouped binary words that
digitally represent these voltage levels (as accurately as possible) as they change
over time.
FIGURE 6.2
Representations of an audio
signal: (a) an analog signal
is continuous in nature. 
(b) a digital signal makes
use of periodic sampling to
encode information.
During this process, an incoming analog signal is sampled at discrete and
precisely timed intervals (as determined by the sample rate). At each interval,
this analog signal is momentarily “held” (frozen in time), while the converter
goes about the process of determining what the voltage level actually is. This
is done with a degree of accuracy that’s deﬁned by the quality of the converter’s
circuitry and the chosen bit depth. The converter then generates a binary-
encoded word that’s numerically equivalent to the analog voltage level at that
point in time (Figure 6.3). Once this is done, the converter can store the

The Language of Digital
198
FIGURE 6.3
The sampling process. 
(a) The analog signal is
momentarily “held” (frozen
in time), while the converter
goes about the process of
determining the voltage
level at that point in time
and then converts that level
into a binary-encoded word
that’s numerically
equivalent to the original
analog voltage level.
(b) Once this digital
information is processed
and stored, the sample is
released and the next
sample is held, as the
system again goes about
the task of determining the
level of the next sampled
voltage—and so forth, and
so forth, and so forth over
the course of the recording.
representative word into a memory medium (disk, disc, RAM, tape, etc.), release
its hold, and then go about the task of determining the values of the next
sampled voltage. The process is then continuously repeated throughout the
recording process.
Within a digital audio system, the sampling rate is deﬁned as the number of
measurements (samples) that are periodically taken over the course of a second.
It’s reciprocal (sampling time), is the elapsed time that occurs between each
sampling period. For example, a sample rate of 44.1 kHz corresponds to a sample
time of 1/44,100 of a second.
This process can be likened to a photographer who takes a series of action
sequence shots. As the number of pictures taken in a second increases, the
accuracy of the captured event will likewise increase until the resolution is so
great that you can’t tell that the continuous and (hopefully) compelling movie
is really a series of successive, discrete pictures. Since the process of sampling
is tied directly to the component of time, the sampling rate of a system
determines its overall bandwidth (Figure 6.4), meaning that a recording made
at a higher sample rate will be capable of storing a wider range of frequencies
(effectively increasing the signal’s bandwidth at its upper limit).
FIGURE 6.4
Discrete time sampling. (a)
Whenever the sample rate is
set too low, important data
between sample periods will
be lost. (b) As the rate is
increased, more frequency-
related data can be
encoded. (c) Increasing the
sampling frequency further
can encode the recorded
signal with an even higher
bandwidth range.
QUANTIZATION
Quantization represents the amplitude component of the digital sampling
process. It is used to translate the voltage levels of a continuous analog signal
(at discrete sample points over time) into binary digits (bits) for the purpose

of manipulating or storing audio data in the digital domain. By sampling the
amplitude of an analog signal at precise intervals over time, the converter
determines the exact voltage level of the signal (during a sample interval, when
the voltage level is momentarily held), and then outputs this signal level as an
equivalent set of binary numbers (as a grouped word of n-bits length) which
represent the originally sampled voltage level (Figure 6.5). The resulting word
is used to encode the original voltage level with as high a degree of accuracy as
can be permitted by the word’s bit length and the system’s overall design.
Currently, the most common binary word lengths for audio are 16-bit (i.e.,
11111101 10000011, having a theoretical dynamic range of 96.33 dB) and 
24-bit resolutions (i.e., 11111101 10000011 10101110, having a theoretical
dynamic range of 144.49 dB). In addition, computers and signal-processing
devices are capable of performing calculations internally at the 32- and 64-bit
resolution level. This internal calculation headroom at the bit level helps to
reduce errors whenever high track counts are summed together. In addition,
this added internal headroom helps to reduce errors in performance whenever
the multiple data streams are processed in real-time. Since the internal bit depth
is higher, these resolutions can be preserved (instead of being dropped by the
system’s hardware or software processing functions), with a ﬁnal result being
an n-bit data stream that’s relatively free of errors. When implemented properly,
this increase in summing and real-time processing accuracy can result in a
system that’s audibly superior in overall sound performance and reduced
distortion.
The Devil’s in the Details
The details of the digital audio record/playback process can get quite
complicated, however the essential basics are:
n Sampling (in the truest sense of the word) analog voltage levels at precise
intervals in time
n Conversion of these samples into a digital word value that most accurately
represents these voltage levels
n Storing these numeric sample equivalents within a digital memory device
199
Digital Audio Technology  CHAPTER 6
FIGURE 6.5
The instantaneous
amplitude of the incoming
analog signal is broken
down into a series of
discrete voltage steps,
which are then converted
into equivalent binary-
encoded words.

The Language of Digital
200
Upon playback, these digital words are then converted back into discrete voltages
(again, at precise intervals in time), allowing the originally recorded signal
voltages to be re-created, processed and played back.
Although the basic concept behind the sample-and-hold process is relatively
straightforward, delving further into the process can quickly bog you down in
the language of high-level math and physics. Luckily, there are a few additional
details relating to digital audio that can be discussed at a basic level.
THE NYQUIST THEOREM
The Nyquist theorem is a basic rule that relates to the sampling process and
states that:
In order for the desired frequency bandwidth to be
faithfully encoded in the digital domain, the selected
sample rate must be at least twice as high as the
highest frequency to be recorded (sample rate ≥ 2x
highest frequency).
In plain language, should frequencies that are greater than twice the sample
rate be allowed into the sampling process, these frequencies would be higher
than the sample rate can faithfully capture. When this happens, the successive
samples won’t be able to accurately capture these higher frequencies, but instead
would actually be recorded as false or “alias” frequencies that aren’t actually
there—and will be heard as harmonic distortion (Figure 6.6). As such, from a
practical point of view, this would mean that an audio signal with a bandwidth
of 20 kHz would require that the sampling rate be at least 40 kHz samples/sec.
FIGURE 6.6
Frequencies that enter into
the digitization process
above the Nyquist half-
sample frequency limit can
introduce harmonic
distortion. (a) Frequencies
greater than 2x the
sampling rate limit are
passed into the sampling
process. (b) This will result
in “alias” frequencies that
are introduced back into the
audio band as distortion.
In order to eliminate the effects of aliasing, a low-pass ﬁlter is placed into the
circuit before the sampling process takes place. In theory, an ideal ﬁlter would
pass all frequencies up to the Nyquist cutoff frequency and then prevent any
frequencies above this point from passing. In the real world, however, such a
“brick wall” ﬁlter doesn’t really exist. For this reason, a slightly higher sample
rate must be chosen in order to account for the cutoff slope that’s required for

the ﬁlter to be effective (Figure 6.7). As a result, an audio signal with a bandwidth
of 20 kHz will actually be sampled at a standardized rate of 44.1 samples/sec,
while a bandwidth of roughly 22 kHz would require the use of a sampling rate
of at least 48 kHz samples/sec, etc.
OVERSAMPLING
This sampling-related process is commonly used in professional and consumer
digital audio systems to improve the Nyquist ﬁlter’s anti-aliasing characteristics.
Oversampling increases the effective sample rate by factors ranging between 12
and 128 times the original rate. There are three main reasons for this process:
n Nyquist ﬁlters can be expensive and difﬁcult to properly design. By
increasing the effective sample bandwidth, a simpler and less-expensive
ﬁlter can be used.
n Oversampling generally results in a higher-quality analog-to-digital (A/D)
and digital-to-analog (D/A) converter that sounds better.
n Since multiple samples are taken of a single sample-and-hold analog
voltage, the average noise level will be lower.
Following the sample stage, the sampled data is digitally scaled back down to
the target data rate and bandwidth for further processing and/or storage.
SIGNAL-TO-ERROR RATIO
The signal-to-error ratio is used to measure the quantization process. A digital
system’s signal-to-error ratio is closely akin (although not identical) to the analog
concept of signal-to-noise (S/N) ratio. Whereas S/N ratio is used to indicate the
overall dynamic range of an analog system, the signal-to-error ratio of a digital
audio device indicates the degree of accuracy that’s used to capture a sampled
level and its step-related effects.
Although analog signals are continuous in nature, as we’ve read, the process of
quantizing a signal into an equivalent digital word isn’t. Since the number of
discrete steps that can be encoded within a digital word limits the accuracy of the
quantization process, the representative digital word can only be an approximation
(albeit an extremely close one) of the original analog signal level. Given a properly
designed system, the signal-to-error ratio for a signal coded with n bits is:
Signal-to-error ratio = 6n + 1.8(dB)
201
Digital Audio Technology  CHAPTER 6
FIGURE 6.7
Anti-alias filtering. 
(a) An ideal filter would have
an infinite attenuation at the
20 kHz Nyquist cutoff
frequency. (b) Real-world
filters require an additional
frequency “guardband” in
order to fully attenuate
unwanted frequencies that
fall above the half-
bandwidth Nyquist limit.

The Language of Digital
202
DITHER
A process known as dither is commonly used during the recording or conversion
process to increase the overall bit resolution (and therefore low-level noise and
signal clarity) of a recorded signal, when converting from a higher to a lower
bit rate.
Technically, dither is the addition of very small amounts of randomly generated
noise to an existing bit-stream that allows the S/N and distortion ﬁgures to fall
to levels that approach their theoretical limits. The process makes it possible
for low-level signals to be encoded at less than the data’s least signiﬁcant bit
level (less than a single quantization step, as shown in Figure 6.8a). You heard
that right—by adding a small amount of random noise into the A/D path, we
can actually:
n Improve the resolution of the conversion process below the least signiﬁcant
bit level
n Reduce harmonic distortion in a way that greatly improves the signal’s
performance
The concept of dither relies on the fact that noise is random. By adding small
amounts of randomization into the quantization process, there is an increased
probability that the D/A converter will be able to guess the least signiﬁcant bit
of a low-level signal more accurately. This is due to the fact that the noise shapes
the detected sample in such a way that the sample-and-hold (S/H) circuitry can
“guess” the original analog value with greater precision.
Dither is often applied to an application or process to reduce quantization 
errors that result in slight increases in noise and/or fuzziness that could other-
wise creep into a bitstream. For example, when multiple tracks are mixed
together within a DAW, it’s not uncommon for digital data to be internally
processed at 32- and 64-bit depths. In situations like this, dither is often used
to smooth and round the data values, so that the low-level (least signiﬁcant
bit) resolutions won’t be lost when they are interpolated back to their original
target bit depths.
Applications and DAW plug-ins can be used to apply dither to a sound ﬁle or
master mix, so as to reduce the effects of lost resolution due to the truncation
of least signiﬁcant bits (Figure 6.8b). For example, mastering engineers might
experiment with applying dither to a high-resolution ﬁle before saving or
Therefore, the theoretical signal-to-error ratio for the
most common bit rates will yield a dynamic range of:
8-bit word = 49.8 dB
16-bit word = 97.8 dB
20-bit word = 121.8 dB
24-bit word = 145.8 dB
32-bit word = 193.8 dB
64-bit word = 385.8 dB

203
Digital Audio Technology  CHAPTER 6
FIGURE 6.8
Dither in action. 
(a) Values falling below
the “least significant bit”
level cannot be encoded
without the use of dither.
(b) Apogee UV22 dither
plug-in for Steinberg
Cubase and Nuendo.
(Courtesy of Steinberg
Media Technologies
GmbH, a division of
Yamaha Corporation,
www.steinberg.net)
exporting it as a 16-bit ﬁnal master. In this way, noise will be reduced and the
sound ﬁle’s overall clarity can be increased. When in doubt, it has been found
that adding “triangular” dither to a ﬁle that is to be bit-reduced is the safest
overall noise-shaping ﬁlter that can be used.
FIXED- VS. FLOATING-POINT PROCESSING
Many of the newer digital audio and DAW systems make use of ﬂoating-point
arithmetic in order to process, mix and output digital audio. The advantage to
the use of ﬂoating over ﬁxed-point DSP calculations is that the former is able
to use numeric “shorthand” in order to process a wider range of values at any
point in time. In short, it’s able to easily move or “ﬂoat” the decimal point of
a very large number in a way that can represent it as a much smaller value. By
doing so, the processor is able to internally calculate much larger bit depth
values (i.e., 32- or 64-bit) with relative ease and increased data resolution.
THE DIGITAL RECORDING/REPRODUCTION 
PROCESS
The following sections provide a basic overview of the various stages within the
process of encoding analog signals into equivalent digital data (Figure 6.9a)
and then converting this data back into its original analog form (Figure 6.9b).
FIGURE 6.9
The digital audio chain: 
(a) recording;
(b) reproduction.

The Digital Recording/Reproduction Process
204
The Recording Process
In its most basic form, the digital recording chain includes a low-pass ﬁlter, a
sample-and-hold circuit, an analog-to-digital converter and circuitry for signal
coding and error correction. At the input of a digital sampling system, the analog
signal must be band limited with a low-pass ﬁlter, so as to stop frequencies that
are greater than half the sample rate frequency from passing into the A/D
conversion circuitry. Such a stop-band (anti-aliasing) ﬁlter generally makes use
of a sharp roll-off slope at its high-frequency cutoff point (oversampling might
be used to simplify and improve this process).
Following the low-pass ﬁlter, a sample-and-hold (S/H) circuit freezes and measures
the analog voltage level that’s present during the sample period. This period is
determined by the sample rate (i.e., 1/44,100th of a second for a 44.1K rate).
At this point, computations (a series of computerized guessing-games) are
performed to translate the sampled voltage into an equivalent binary word. This
step in the A/D conversion is one of the most critical components of the
digitization process, because the sampled DC voltage level must be quickly and
accurately quantized into an equivalent digital word (to the closest step level).
Once the sampled signal has been converted into its equivalent digital form, the
data must be conditioned for further data processing and storage. This condition -
ing includes data coding, data modulation and error correction. In general, the
binary digits of a digital bit-stream aren’t directly stored onto a recording medium
as raw data; rather, data coding is used to translate the data (along with synchro -
nization and address information) into a form that allows the data to be most
efﬁciently and accurately stored to a memory or storage media. The most common
form of digital audio data coding is pulse-code modulation, or PCM (Figure 6.10a).
The density of stored information within a PCM recording and playback system
is extremely high, so much so that any imperfections (such as dust, ﬁngerprints
or scratches that might adhere to the surface of any magnetic or optical recording
medium) would cause severe or irretrievable data errors. To keep these errors
within acceptable limits, several forms of error correction are used (depending
on the media type). One method uses redundant data in the form of parity bits
and check codes in order to retrieve and/or reconstruct lost data. A second
FIGURE 6.10
Data conditioning in the
digital recording process: 
(a) pulse-code modulation;
(b) example of interleaved
error correction.

method uses error correction that involves interleaving techniques, whereby data
is deliberately scattered across the digital bit-stream, according to a complex
mathematical pattern. The latter has the effect of spreading the data over a larger
surface of the recording media, thereby making the recording media less
susceptible to dropouts (Figure 6.10b). In fact, it’s a simple truth that without
error correction, the quality of most digital audio media would be greatly
reduced or (in the case of the CD and DVD) rendered almost useless.
The Playback Process
In many respects, the digital reproduction chain works in a manner that’s
complementary to the digital encoding process. Since most digital media encodes
data onto media in the form of highly saturated magnetic transition states or
optical reﬂections, the recorded data must ﬁrst be reconditioned in a way that
restores the digital bit-stream back into its original, modulated binary state (i.e.,
a transitional square wave). Once this is done, the encoded data can be de-
interleaved (reassembled) back into its original form, where it can be converted
back into PCM data, and the process of D/A conversion can take place.
Within the D/A conversion process, a stepped resistance network (sometimes
called an R/2R network) is used to convert the representative words back into
their analogous voltage levels for playback. During a complementary S/H period,
each bit within the representative digital word is assigned to a resistance leg in
the network (moving from the most signiﬁcant to the least signiﬁcant bit). Each
“step” in the resistance leg is then designed to successively pass one-half the
reference voltage level as it moves down the ladder toward the LSB (Figure 6.11).
The presence or absence of a logical “1” in each step is then used to turn on
each successive voltage leg. As you might expect, when all the resistance legs
are properly summed together, their voltages equal a precise level that matches
(as closely as possible) the originally recorded voltage during the recorded
sample period. Since these voltages are reproduced over time in precise intervals
(as determined by the sample rate) you end up with successively changing
voltages that make up the system’s playback signal!
Following the conversion process, a ﬁnal, complementary low-pass ﬁlter is
inserted into the signal path. Again, following the principle of the Nyquist
theorem, this ﬁlter is used to smooth out any of the step-level distortions that
are introduced by the sampling process, resulting in a waveform that faithfully
represents the originally recorded analog waveform.
205
Digital Audio Technology  CHAPTER 6
FIGURE 6.11
A stepped resistance
network is a common
device for accomplishing
D/A conversion by assigning
each word bit to a series of
resistors that are scaled by
factors of 2.

Sound File Basics
SOUND FILE BASICS
Within the digital audio community there are, of course, standardized conven -
tions that allow us to uphold general levels of professionalism and ﬁle
interchange between systems. There are deﬁnite ranges of personal choice, as
to what general ﬁle sample rate, bit depth and other specs that might work best
for you. Of course, there is a widespread movement that advocates the use of
sound ﬁle specs that adhere to the general ideas of “High-Deﬁnition Audio.”
This ideology advocates the idea that only high-sample rate, high-bit depth audio
should be used in the production of professional and/or high-quality audio.
These are, of course, lofty goals—to capture, process and playback audio with
as high degree of audio quality and faithfulness to the music as is possible. 
I will leave the general debate as to what rates, bit depths and general specs in
your capable hands, as tons has been written about this subject and by now,
you might have formed your own opinion about how you should work or about
what will work best for you.
On a personal note, I will say this: Most people who really know their ins-and-
out of digital audio conversion will tell you that determining the quality of a
system is not always about “the numbers.” That’s to say it’s not “always” about
how high the sample-rates are or how impressive the digital specs of a system
will look on paper, it’s far more often about the overall design quality of your
system’s converters, or how well your system’s internal system’s jitter is kept to
a minimum, etc.
I know that it’s quite unpopular to say this, but it’s a simple fact that a high-
quality digital audio converter/DAW system running at 24/44.1 (for example)
can sound as good as or better than a high-resolution 24/96 converter/DAW
system that’s less well-designed, or has higher than normal jitter within its design,
or even one that’s been improperly cabled or improperly linked to an external
wordclock. In short, it’s not always about the spec or sample rate numbers or
about how the system’s design looks on paper. In the end, it’s the system’s
overall design quality that will determine how the conversion/DAW system will
“sound.” If you start there, the added idea of capturing your sound with sound
ﬁle rates and settings that match your personal needs (or sonic requirements)
will only put you that much further ahead of the game.
Sound File Sample Rates
The sample rate of a recorded digital audio bit-stream directly relates to the
resolution at which a recorded sound will be digitally captured. Using the ﬁlm
analogy, if you capture more samples (frames) of a moving image as it moves
through time, you’ll end up with a more accurate representation of that recorded
event. If the numbers of samples are too low, the resolution will be “lossy” and
will distort the event. On the other hand, taking too many picture frames could
result in a recorded bandwidth that’s so high that the audience won’t be able
to discriminate any advantages that the extra information has to offer.
206

This analogy relates perfectly to audio because the choices of sample rate will
be determined by the bandwidth (number of overall frequencies that are to be
captured) versus the amount of storage and processing time that’s needed to
either save the data to a memory storage media, or possibly the time that will
be required to up/download a ﬁle through a transmission and/or online data-
stream. The following are the most commonly used in the professional, project
and audio production community:
n 32k: This rate is often used by broadcasters to transmit/receive digital data
via satellite. With its overall 15-kHz bandwidth and reduced data require -
ments, certain devices also use it in order to conserve on memory. Although
the pro community doesn’t generally use this rate, it’s surprising just how
good a sound can be captured at 32k (given a high-quality converter).
n 44.1k: The long-time standard of consumer and basic pro audio pro duc -
tion, 44.1 is the chosen rate of the CD-audio standard. With its overall 20-
kHz bandwidth, the 44.1k rate is generally considered to be the minimum
sample rate for professional high-deﬁnition audio production. Assuming
that high-quality converters are used, this rate is capable of recording
lossless audio, while conserving on memory storage require ments.
n 48k: This standard was adopted early on as a standard sample rate for
professional audio applications (particularly when dealing with early
hardware devices). It’s also the adopted standard rate for use within
professional video, DVD and broadcast production.
n 88.2k: As a simple multiple of 44.1, this rate is often used within pro -
ductions that are intended to be high-resolution products.
n 96k: This rate has been adopted as the de-facto sample rate for high-resolu -
tion recordings and is a common rate for Blu-ray high-deﬁnition audio.
n 192k: This high-resolution rate is increasingly used within pro audio
production, however the storage and media processing requirements are
quite high (but not beyond the limits of modern day computer CPUs).
Sound File Bit Depths
The bit depth of a digitally recorded sound ﬁle directly relates to the number of
quantization steps that are encoded into the bit-stream. As a result, the bit rate
(or bit depth) is directly correlated to the:
n Accuracy by which a sampled level (at one point in time) is encoded
n Signal-to-error ﬁgure . . . and thus the overall dynamic range of the recorded
signal
If the bit rate is too low to accurately encode the sample, the resolution will
lead to quantization errors, which will lead to distortion. Increasing the bit-
depth will often improve quantization errors in the record/playback process,
generally leading to reduced distortion and an increase in the sound’s overall
transparency. The following are the most commonly used within the pro, project
and general audio production community:
207
Digital Audio Technology  CHAPTER 6

Sound File Basics
n 16 bits: The long-time standard of consumer and professional audio
production, 16 bits is the chosen bit depth of the CD-audio standard
(offering a theoretical dynamic range of 97.8 dB). It is generally considered
to be the minimum depth for high-quality professional audio production.
Assuming that high-quality converters are used, this rate is capable of loss-
less audio recording, while conserving on memory storage requirements.
n 20 bits: Before the 24-bit depth came onto the scene, 20 bits was considered
to be the standard for high-bit-depth resolution. Although it’s used less
commonly now, it can still be found in high-deﬁnition audio recordings
(offering a theoretical dynamic range of 121.8 dB).
n 24 bits: Offering a theoretical dynamic range of 145.8 dB, this standard
bit depth is often used in high-deﬁnition audio applications, often in
conjunction with the 96k sample rate (i.e., 24/96).
Further reading on sound ﬁle and compression codec speciﬁcs can be found in
Chapter 11 (Multimedia).
Professional Sound File Formats
Although several formats exist for encoding and storing sound ﬁle data, only
two have been universally adopted by the industry, these are:
n Wave (.wav) format
n AIFF (.aif) format
These standardized formats make it easier for ﬁles to be exchanged between
compatible media devices.
The most common ﬁle type is the Wave (or .wav) format. Developed for the
Microsoft Windows operating system, this universal ﬁle type supports mono,
stereo and multi-channel ﬁles at a wide range of uncompressed resolutions and
sample rates. Wave ﬁles contain pulse-code modulation (PCM) coded audio
that follows the Resource Information File Format (RIFF) spec, which allows
extra user information to be embedded and saved within the ﬁle itself.
The newly adopted Broadcast Wave format, which has been adopted by the
Producers and Engineers wing (www.grammypro.com/producers-engineers-
wing) as the preferred sound ﬁle format for DAW production and music
archiving, allows metadata and timecode-related positioning information to be
directly embedded within the sound ﬁle’s data stream, making it easier for these
wave ﬁles to be placed within music and media productions using precise
timecode position placement.
Apple’s Audio Interchange File (AIFF or .aif) format, likewise supports mono,
stereo and multi-channel imbedded sound ﬁles with 8-, 16- and 24-bitdepths
over a wide range of sample rates. Like Broadcast Wave ﬁles, AIFF ﬁles can also
contain embedded text strings, however, unlike BWF, it’s not capable of
containing time code stamped information within the ﬁle itself.
208

209
Digital Audio Technology  CHAPTER 6
Regarding Digital Audio Levels
Over the decades, the trend toward making recordings that are as loud as
possible has totally pervaded the industry to the point that it has been given
the name of “The Loudness Wars.” Not only is this practice used in mastering
to make a song or project stand out in an on-the-air or in-the-device playlist,
it has also been grandfathered in from the analog tradition of recording a 
track as “hot” as possible to get the best noise ﬁgures and “punch” out of a
track. All of this is arguably well and good, except for the fact that whenever a
track has been recorded at too high a level, you’re not adding extra “punch” to
the track at all—in fact, all you are adding is distortion, and a rather nasty
sounding one at that.
The dynamic range of a digital recording ranges from its theoretical ﬂoor signal
level of 00000000 00000000 00000000, for a 24-bit recording, to its full-scale
(dBFS) headroom ceiling of 11111111 11111111 11111111. It almost goes
without saying that having average or peak levels that go above full scale can
easily ruin a recording by adding clipping distortion. Since the overall dynamic
range of a digital ﬁle can be 97.8 dB for a 16-bit ﬁle and 145.8 dB for a 24-bit
ﬁle, it’s generally a good idea to reduce your levels so that they peak at around
–12 (or even at levels approaching –20 dB). This will generally accurately
capture the peaks without clipping and without adding any appreciable amount
of noise into the mix.
One of the ﬁner points that you should be aware of, regarding overall recording
levels (as well as levels that get summed together whenever all of the tracks in
your mix get added together) is the math that’s used by your DAW to deal with
the general levels and gain structure within your session mix. Not all DAWs are
created equal on this matter. Some DAWs make use of ﬂoating-point math 
(a digital shorthand that allows the software to deal with wide ranges in number
calculations) to increase its overall internal gain structure, while other DAWs
are not quite so forgiving, often leading to increased distortion when the tracks
are summed within a mix. It’s just an FYI point that you might want to be aware
of within the mixing process, when using your DAW.
FIGURE 6.12
Several present-day 
DAW and digital systems
metering options: 
(a) iZotope Insight 
Essential Metering Suite.
(Courtesy of iZotope Inc.,
www.izotope.com); 
(b) Steinberg Cubase 
and Nuendo main output
metering display. 
(Courtesy of Steinberg
Media Technologies GmbH,
a division of Yamaha
Corporation, www.
steinberg.net)

Sound File Basics
Although digital audio level guidelines now exist within the broadcast com -
munity (to some extent), pro and project studio level guidelines are few and
far between. In recent years, metering plug-ins and better DAW output metering
has come onto the market, allowing you to better keep track of your overall
level and metering needs (Figure 6.12).
Digital Audio Transmission
In the digital age, it’s become increasingly common for audio data to be
distributed throughout a connected production system in the digital domain.
In this way, digital audio can be transmitted in its original numeric form and
(in theory) without any degradation throughout a connected path or system.
When looking at the differences between the distribution of digital and analog
audio, it should be kept in mind that, unlike its analog counterpart, the
transmitted bandwidth of digital audio data occurs in the megahertz range;
therefore, it actually has more in common with video signals than with the
lower bandwidth range of analog audio. This means that care must be exercised
to ensure that impedance values are more closely matched and that quick-ﬁx
solutions don’t occur (for example, using a Y-cord to split a digital signal
between two devices is a major no-no). Failure to follow these precautions could
seriously degrade or deform the digital signal, causing increased jitter and
unwanted distortions.
Due to these tight restrictions, several digital transmission standards have been
adopted that allow digital audio data to be quickly and reliably transmitted
between compliant devices. These include such protocols as:
n AES/EBU
n S/PDIF
n SCMS
n MADI
n ADAT lightpipe
n TDIF
AES/EBU
The AES/EBU (Audio Engineering Society and the European Broadcast Union)
protocol has been adopted for the purpose of transmitting digital audio between
professional digital audio devices. This standard (which is most often referred
to as simply an AES digital connection) is used to convey two channels of
interleaved digital audio through a single, three-pin XLR mic or line cable in a
single direction. This balanced conﬁguration connects pin 1 to the signal ground,
while pins 2 and 3 are used to carry signal data. AES/EBU transmission data is
low impedance in nature (typically 110 Ω) and has digital burst amplitudes
that range between 3 and 10 V. These combined factors allow for a maximum
cable length of up to 328 feet (100 meters) at sample rates of less than 50 kHz
without encountering undue signal degradation.
210

211
Digital Audio Technology  CHAPTER 6
FIGURE 6.13
AES/EBU subframe format.
Digital audio channel data and subcode information is transmitted in blocks
of 192 bits that are organized into 24 words (each being 8 bits long). Within
the conﬁnes of these data blocks, two sub-frames are transmitted during each
sample period that convey information and digital synchronization codes for
both channels in an L-R-L-R fashion. Since the data is transmitted as a self-
clocking bi-phase code (Figure 6.13), wire polarity can be ignored. In addition,
whenever two devices are directly connected, the receiving device will usually
derive its reference-timing clock from the digital source device.
In the late 1990s, the AES protocol was amended to include the “stereo 96k
dual AES signal” protocol. This was created to address signal degradations that
can occur when running longer cable runs at sample rates above 50 kHz. To
address the problem, the dual AES standard allows stereo sample rates above
50 kHz (such as 24/96) to be transmitted over two synchronized AES cables
(with one cable carrying the L information and the other carrying the R).
S/PDIF
The S/PDIF (Sony/Phillips Digital Interface) protocol has been widely adopted
for transmitting digital audio between consumer digital audio devices and their
professional counterparts. Instead of using a balanced 3-pin XLR cable, the
popular S/PDIF standard has adopted the single-conductor, unbalanced phono
(RCA) connector (Figure 6.14a), which conducts a nominal peak-to-peak voltage
level of 0.5 V between connected devices, with an impedance of 75 Ω. In
addition to using standard RCA cable connections S/PDIF can also be transmitted
between devices using Toslink optical connection lines (Figure 6.14b), which
are commonly referred to as “lightpipe” connectors.
As with the AES/EBU protocol, S/PDIF channel data and subcode informa-
tion are transmitted in blocks of 192 bits consisting of 12 words that are each
16 bits long. A portion of this information is reserved as a category code that
provides the necessary setup information (sample rate, copy protection status
and so on) to the copying device. Another portion is set aside for transmitting
FIGURE 6.14
S/PDIF connectors: (a) RCA
coax connection; (b) Toslink
optical connection.

Sound File Basics
audio data that’s used to relay track indexing information (such as start ID and
program ID numbers), allowing this relevant information to be digitally
transferred from the master to the copy. It should be noted that the professional
AES/EBU protocol isn’t capable of digitally transmitting these codes during a
copy transfer.
In addition to transmitting two channels in an interleaved L-R-L-R fashion,
S/PDIF is able to communicate multichannel data between devices. Most
commonly, this shows up as a direct 5.1 or 7.1 surround-sound link between
a DVD player and an audio receiver/ampliﬁer playback system (via either an
RCA coax or optical connection).
SCMS
Initially, certain digital recording devices (such as a DAT recorder) were intended
to provide consumers with a way to make high-quality recordings for their own
personal use. Soon after its inception, however, for better or for worse, the
recording industry began to see this new medium as a potential source of lost
royalties due to home copying and piracy practices. As a result, the RIAA
(Recording Industry Association of America) and the former CBS Technology
Center set out to create a “copy inhibitor.” After certain failures and long
industry deliberations, the result of these efforts was a process that has come
to be known as the Serial Copy Management System, or SCMS (pronounced
“scums”). This protocol was incorporated into many consumer digital devices
in order to prohibit the unauthorized copying of digital audio at 44.1 kHz 
(of course, SCMS doesn’t apply to the making of analog copies). Note also that,
with the demise of the DAT format (both as a recording and mass-music
distribution medium), SCMS has fallen completely out of favor with both the
consumer and pro audio communities.
So, what is SCMS? Technically, it’s a digital protection ﬂag that is encoded in
byte 0 (bits 6 and 7) of the S/PDIF subcode area. This ﬂag can have only one
of three possible states:
n Status 00: No copy protection, allowing unlimited copying and subsequent
dubbing
n Status 10: No more digital copies allowed
n Status 11: Allows a single copy to be made of this product, but that copy
cannot be copied.
MADI
The MADI (Multichannel Audio Digital Interface) standard was jointly pro-
posed as an AES standard by representatives of Neve, Sony and SSL as a straight
forward, clutter-free digital interface connection between multitrack devices
(such as a high-end workstation, digital tape recorder or mixing console, as
shown in Figure 6.15). The transmission rate of 100 Mbit/sec provides for an
212

213
Digital Audio Technology  CHAPTER 6
overall bandwidth that’s capable of handling the following number of channels
in a single direction (one sender, one receiver):
n 56 channels at 32 kHz to 48 kHz (with ± 12.5% pitch shift capabilities)
n 64 channels at 32 kHz to 48 kHz (no pitch shift)
n 28 channels at 64 kHz to 96 kHz (with ± 12.5% pitch shift capabilities)
The linearly encoded digital audio is connected via a single 75 Ω, video-grade
coaxial cable at distances of up to 120 feet (50 meters) or at greater distances
whenever a ﬁber-optic cable is used.
In short, MADI makes use of a serial data transmission format that’s compatible
with the AES/EBU twin-channel protocol (whereby the data, Status, User and
parity bit structures are preserved), and sequentially cycles through each channel
(i.e., starting with Ch. 0 and ending with Ch. 55).
ADAT LIGHTPIPE
A wide range of audio interface, mic preamps and older modular digital
multitrack recorders currently use the lightpipe system for transmitting multi -
channel audio via a standardized optical cable link. These connections make
use of standard Toslink connectors and cables to transmit up to eight channels
of uncompressed digital audio at resolutions up to 24 bit at sample rates up to
48k over a sequential, optical bitstream. In what is called the S/MUX IV mode,
a lightpipe connection can also be used to pass up to four channels of digital
audio at the higher sample rates of 88.2 and 96k.
Although these connections are identical to those that are used to optically
transmit S/PDIF stereo digital audio, the data streams are incompatible with
each other. Lightpipe data isn’t bidirectional, meaning that the connection can
only travel from a single source to a destination. Thus, two cables will be needed
to distribute data both to and from a device. In addition, synchronization data
is imbedded within the digital data stream, meaning that no additional digital
audio sync connections are necessary in order to lock device timing clocks;
however, transport and timecode information is not transmitted and will require
additional connections, should transport control (generally over older MDMs)
be needed.
FIGURE 6.15
MADI offers a clutter-free
digital interface connection
between multitrack devices.

Sound File Basics
214
FIGURE 6.16
ADAT optical I/O
Interconnection drawing
between audio interface and
a multi-channel preamp.
It’s interesting to note that, although the Alesis ADAT tape-based format has
virtually disappeared into the sunset, the lightpipe protocol lives on as a preferred
way of passing multichannel digital audio to and from multichannel mic
preamps to a lightpipe-equipped digital audio interface (Figure 6.16). This
allows a stream of up to eight digital outs to be easily inserted into a suitable
audio interface, with a single optical cable, thus increasing the number of inputs
by eight (or 16, should two ADAT I/O interconnections be available).
TDIF
The TDIF (Tascam Digital InterFace) is a proprietary Tascam format that uses a
25-pin D-sub cable to transmit and/or receive up to eight channels of digital
audio between compatible devices. Unlike the lightpipe connection, TDIF is a
bidirectional connection, meaning that only one cable is required to connect
the eight ins and outs of one device to another. Although systems that support
TDIF-1 cannot send and receive sync information (a separate wordclock
connection is required for that), the more recent TDIF-2 protocol is capable of
receiving and transmitting digital audio sync through the existing connection,
without any additional cabling.
SIGNAL DISTRIBUTION
If copies are to be made from a single, digital audio source, or if data is to be
distributed throughout a connected network using AES/EBU, S/PDIF or MADI
digital transmission cables, it’s possible to daisy chain the data from one device
to the next in a straightforward fashion (Figure 6.17a). This method works well
FIGURE 6.17
Digital audio distribution: 
(a) several devices
connected in a daisy-
chain fashion; (b) multiple
devices connected using a
distribution device.

only if a few devices are to be chained together. However, if several devices are
connected together, time-base errors (known as jitter) might be introduced into
the path, with the possible side effects being added noise, distortion and a
slightly “blurred” signal image. One way to reduce the likelihood of such time-
base errors is to use a digital audio distribution device that can route the data
from a single digital audio source to a number of individual device destinations
(Figure 6.17b).
What Is Jitter?
One of the more important and misunderstood characteristics surrounding the
topic of digital audio transmission is the need for a stable timing element
within the digital data stream itself. That is to say, a clean, high-quality digital
audio signal relies upon a timing element (clock) that is highly stable, one that
samples and processes the sample at exactly the right intervals (according to
the chosen sample rate). If an unstable timing element is introduced into the
circuit, signal degradation in the form of reduced audio quality (most likely
heard as a blurred audio image, reduced resolution, increased noise and (in
worst cases) audible ticks and pops.
Let’s take a closer look at how jitter effects digital and our audio production
lives in practice. In a perfectly designed audio circuit that is perfectly inter-
faced to its digital neighbors, the clock would occur at exactly the right timing
intervals. The clock would instruct the analog-to-digital (A/D) converter to 
begin the guessing game of ﬁnding the audio signal’s voltage levels and then
converting that level to an equivalent set of digital audio words—all at a pre-
cisely repeating, periodic rate (Figure 6.18a). The problem is that digital audio,
contrary to popular opinion, is most often not perfect at all, indeed, undesired
variations in the timing circuit of an A/D converter requires a great deal of 
skill and dedication to quality in order to reduce these timings to acceptably
low levels. Systems that have not been carefully designed will often introduce
these timing errors into the A/D conversion process (Figure 6.18b), thereby
capturing digital audio that can range from having a “veiled” sound image to
a sound that can be harsh and even dirty in nature. The unfortunate part is that
whenever a circuit that has an erratic timing circuit is used to capture the 
sound, there is little or nothing that can be done to restore the sound to a
quality state.
This concept of jitter becomes more complicated when you add the idea that
digital connections to the outside world (i.e., cable quality, lengths, impedances,
215
Digital Audio Technology  CHAPTER 6
FIGURE 6.18
Example of time-base
errors: (a) a theoretically
perfect digital signal source;
(b) the same signal with
jitter errors.

Signal Distribution
216
etc.) can have an effect upon the timing of the digital signal (which is often in
the megahertz range). Care should be taken to use quality cabling (within
reason) and to be aware of cable matching impedances.
I’m now going to break with tradition and offer up an observation that I’ve
made over my career (and it happens to coincide with the opinions of many
who are at the top levels of high-quality interface design)—I’m referring to the
fact that the overall design quality of the converters within an interface will be
the most important factor when it comes to capturing a killer-quality audio
signal. I’ve noticed ﬁrst-hand that an awesome sounding converter or audio
interface at lower sampling rates can sound better than an average interface
running at higher resolution sample and even higher bitrates. It’s not always
about the specs, numbers and how high the ﬁle resolution is, it’s primarily about
the quality and care in design of the converters and overall A/D and D/A circuit
design. Therefore, when you take an awesome interface and increase the
sample/bitrate resolution everything jumps up the highest possible level. Again,
it's not always about the numbers, more often it's about how the device
“sounds.”
Wordclock
One aspect of digital audio recording that never seems to get enough attention
is the need for locking (synchronizing) the sample clocks within a connected
digital audio system to a single timing reference. Left unchecked, it’s possible
that such gremlins as clicks, pops and jitter (oh my!) would make their way
into the audio chain. Through the use of a single master timing reference known
as wordclock, the overall sample-and-hold conversion timings during both the
A/D and D/A process for all digital audio channels and devices within the system
will occur at exactly the same point in time.
How can you tell when the wordclock between
multiple devices either isn’t connected or is
improperly set? If you hear ticks, clicks and pops
over the monitors that sound like “pensive monkeys
pounding away at a typewriter” you probably have a
wordclock problem. When this happens, stop and
deal with the issue, as those monkeys will almost
certainly make it into your recordings.
To further illustrate the need for a master timing source, let’s assume that we’re
in a room that has four or ﬁve clocks and none of them read the same time!
In places like this, you’ll never quite know what the time really is—the clocks
could be running at different speeds or they could be running at the same speed
but are set to different times. Trying to accurately keep track of the time would
end up being a jumbled nightmare. On the other hand, when all of these clocks
are locked to a single, master clock, there will be only one master timing
reference for the entire system.

In a manner similar to the distribution of timecode, there can only be one master
wordclock reference within a connected digital distribution network (Figure
6.19). This reference source can be derived from a digital mixer, soundcard or
any desired source that can transmit a stable wordclock. Often, this reference
pulse is chained between the involved devices through the use of BNC and/or
RCA connectors, using low-capacitance cables (often 75 Ω, video-grade coax
cable is used, although this cable grade isn’t always necessary with shorter cable
runs).
It’s interesting to note that wordclock isn’t generally needed when making a
digital copy directly from one device to another (via such protocols as AES,
S/PDIF, MADI or TDIF2), because the timing information will be embedded
within the data bit-stream itself. Only when we begin to connect multiple
devices that share and communicate digital data throughout a production
network do we begin to see the immediate need for wordclock.
It almost goes without saying that there will often be differences in connections
and parameter setups from one system to the next. In addition to proper cabling,
impedance and termination considerations throughout the network, speciﬁc
hardware and software setups may be required in order to get all the device
blocks to communicate properly. In order to better understand your particular
system’s setup (and to keep frustration to a minimum), it’s always a good idea
to keep all of your device’s physical or pdf manuals close at hand.
On a ﬁnal point, external wordclock generators have come onto the market that
provide an extremely stable timing reference for the digital clocking path. This
can be viewed in several ways. It can be viewed as a tool to clean up and improve
the sound of your A/D and D/A circuitry, or it could be viewed as an expensive
device that wouldn’t be necessary, had the interface been properly designed in
the ﬁrst place. I will say this—it’s been my experience that no two interfaces
sound alike and that the choice of interface is an extremely important decision.
It is a crucial transducer in the recording chain (along with your mics, speakers
and your skills). Try to make your choices wisely in a way that best matches
your needs and budget.
217
Digital Audio Technology  CHAPTER 6
FIGURE 6.19
Example of wordclock
distribution showing that
there can be only one
master clock within a digital
production network.


Over the history of digital audio production, the style, form and function of
hard-disk recording has changed to meet the challenges of faster processors,
reduced size, larger drives, improved hardware systems and the ongoing push
of marketing forces to sell, sell, sell! As a result, there are a wide range of system
types that are designed for various purposes, budgets and production styles. As
new technologies and programming techniques continue to turn out new
hardware and software systems at a dizzying pace, many of the long-held
production limitations have vanished as increased track counts, processing
power and affordability have changed the way we see the art of production
itself. In recent years, no single term implies these changes more than the
“DAW.”
In recent years, the digital audio workstation (DAW) has come to signify an
integrated computer-based hard-disk recording system that commonly offers a
wide and ever-changing number of production features such as:
n Advanced multitrack recording, editing and mixdown capabilities
n MIDI sequencing, edit and score capabilities
n Integrated video and/or video sync capabilities
n Integration with peripheral hardware devices such as controllers, DSP
acceleration systems, MIDI and audio interface devices
n Plug-in DSP (digital signal processing) support
n Support for plug-in virtual instruments
n Support for integrating timing, signal routing and control elements with
other production software (ReWire)
Truth of the matter is, by offering an astounding amount of production power
for the buck, these software-based programs (Figures 7.1 through 7.3) and their
associated hardware devices have revolutionized the faces of professional, project
and personal studios in a way that touches almost every life within the sound
production communities.
219
CHAPTER 7
The Digital Audio
Workstation

The Digital Audio Workstation
220
FIGURE 7.1
Pro Tools hard-disk editing
workstation for the Mac or
PC. (Courtesy of Avid
Technology, Inc.,
www.avid.com)
INTEGRATION NOW—INTEGRATION FOREVER!
Throughout the history of music and audio production, we’ve grown used to
the idea that certain devices were only meant to perform a single task: A recorder
records and plays back, a limiter limits and a mixer mixes. Fortunately, the age
of the microprocessor has totally broken down these traditional lines in a way
that has created a breed of digital chameleons that can change their functional
colors as needed to match the task at hand. Along these same lines, the digital
audio workstation isn’t so much a device as a systems concept that can perform
a wide range of audio production tasks with relative ease and speed. Some of
the characteristics that can (or should be) offered by a DAW include:
n Integration: One of the biggest features of a workstation is its ability to
provide centralized control over the digital audio recording, editing,
processing and signal routing functions within the production system. 
FIGURE 7.2
Cubase Media Production
System for the Mac or PC.
(Courtesy of Steinberg
Media Technologies GmbH,
a division of Yamaha
Corporation,
www.steinberg.net)
FIGURE 7.3
Logic DAW for the Mac.
(Courtesy of Apple Inc.,
www.apple.com)

It should also provide for direct communications with production-related
hardware and software systems, as well as transport and sync control
to/from external media devices.
n Communication: A DAW should be able to communicate and distribute
pertinent audio, MIDI and automation-related data throughout the con -
nected network system. Digital timing (wordclock) and synchronization
(SMPTE timecode and/or MTC) should also be supported.
n Speed and ﬂexibility: These are probably a workstation’s greatest assets. After
you’ve become familiar with a particular system, most production tasks
can be tackled in far less time than would be required using similar analog
equipment. Many of the extensive signal processing, automation and
systems communications features would be far more difﬁcult to accomplish
in the analog domain.
n Session recall: Because all of the functions are in the digital domain, the
ability to instantly save and recall a session and to instantly undo a
performed action becomes a relatively simple matter.
n Automation: The ability to automate almost all audio, control and session
functions allows for a great degree of control over almost all of a DAWs
program and session parameters.
n Expandability: Most DAWs are able to integrate new and important
hardware and software components into the system with little or no
difﬁculty.
n User-friendly operation: An important element of a digital audio workstation
is its ability to communicate with its central interface unit: you! The
operation of a workstation should be relatively intuitive and shouldn’t
obstruct the creative process by speaking too much “computerese.”
I’m sure you’ve gathered from the above points that a software system (and its
associated hardware) which is capable of integrating audio, video and MIDI
under a single, multifunctional umbrella can be a major investment, both in
ﬁnancial terms and in terms of the time that’s spent learning to master the
overall program environment. When choosing a system for yourself or your
facility, be sure to take the above considerations into account. Each system has
its own strengths, weaknesses and particular ways of working. When in doubt,
it’s always a good idea to research the system as much as possible before
committing to it. Feel free to contact your local dealer for a salesroom test drive,
or better yet, try the demo. As with a new car, purchasing a DAW and its
associated hardware can be an expensive proposition that you’ll probably have
to live with for a while. Once you’ve taken the time to make the right choice
for you, you can get down to the business of making music.
DAW HARDWARE
Keeping step with the modern-day truism “technology marches on,” the
hardware and software specs of a computer and the connected peripherals
221
The Digital Audio Workstation  CHAPTER 7

DAW Hardware
222
continue to change at an ever-increasing pace. This is usually reﬂected as general
improvements in such areas as their:
n Need for speed (multiple processors and accelerated co-processors)
n Increased computing power
n Increased disk size and speed
n Increased memory size and speed
n Operating system (OS) and peripheral integration
n General connectivity (networking and the Web)
In this day and age, it’s deﬁnitely important that you keep step with the ever-
changing advances in computer-related production technology (Figure 7.4).
That’s not to say you need to update your system every time a new hard- or
soft-whiz-bang comes onto the market. On the contrary, it’s often a wise person
who knows when a system is working just ﬁne for his or her own personal
needs and who does the research to update software and ﬁne-tune the system
(to the best of his or her ability). On the other hand, there will come a time
(and you’ll know all too well when it arrives) that this “march” of technology
will dictate a system change to keep you in step with the times. As with almost
any aspect of technology, the best way to judge what will work best for you
and your system is to research any piece of hard- and software that you’re
considering—quite simply, read the specs, reads the reviews, ask your friends
and then make your best, most informed choice.
FIGURE 7.4
Pro Tools HDX DAW for the
Mac® or PC. (Courtesy of
Avid Technology, Inc.,
www.avid.com)
When buying a computer for audio production, one of the most commonly
asked questions is “Which one—Mac or PC?” The answer as to which operating
system (OS) will work best for you will actually depend on:
n Your preference
n Your needs
n The kind of software you currently have

n The kind of computer platform and software your working associates or
friends have
n Cost: The fact that you might already be heavily invested in either PC or
Mac software, or that you are more familiar with a certain platform will
usually factor into your system choice
n OS: Even this particular question is being sidestepped with the advent of
Apple’s Boot Camp, which allows a Mac to boot up under the Mac or
Windows OS, giving you freedom of choice to have either or both
The truth is, in this day and age there isn’t much of a functional difference
between the two platforms. They both can do the job admirably.
Once you’ve decided which side of the platform tracks you’d like to live on,
the more important questions that you should be asking are:
n Is my computer fast and powerful enough for the tasks at hand?
n Does it have enough hard disk space that’s large and fast enough for my
needs?
n Is there enough random access memory (RAM)?
n Do I have enough monitor space (real estate) to see the important things
at a single glance?
On the “need for speed” front, it’s always a good idea to buy (or build) a com -
puter at the top of its performance range at any given time. Keeping in mind
that technology marches on, the last thing that you’ll want to do is buy a new
computer only to soon ﬁnd out that it’s underpowered for the tasks ahead.
The newer quad and eight-core (multiprocessor) systems allow for faster
calculations. Their tasks are spread across multiple CPUs; for example, a number
of DAWs allow for their odd/even track counts and/or for effects processing to
be split across multiple CPUs to increase the overall processing load, for added
track count and DSP capabilities.
With today’s faster and higher capacity hard drives, it’s a simple matter to install
cost-effective drives with terabyte capacities into a system. These drives can be
internal, or they can be installed in portable drive cases that can be plugged
into either a Thunderbolt®, FireWire® or USB port (preferably USB 3 or C),
making it easy to take your own personal drive with you to the studio or on-
stage.
The speed at which the disc platters turn will often affect a drive’s access time.
Modern drives with a high disc spin rate (7200 rpm or higher) with large
amounts of internal cache memory are often preferable for both audio and video
production. SSD (solid state drives) are also available that don’t have moving
parts at all, but include solid state memory that can have access times that often
blaze at 6Gb per second or faster.
223
The Digital Audio Workstation  CHAPTER 7

DAW Hardware
224
Within a media production system, it’s always a wise
precaution to have a second drive that’s strictly
dedicated to your audio, video and media ﬁles—this
is generally recommended because the operating
system (OS) will often require access to the main
drive in a way that can cause data interruptions and
a slowed response, should both the OS and media
need to access data simultaneously.
Regarding random access memory, it’s always good to install a more than
adequate amount of high-speed RAM into the system. If a system doesn’t have
enough RAM, data will often have to be swapped to the system’s hard drive,
which can seriously slow things down and affect overall real-time DSP
performance. When dealing with music sampling, video and digital imaging
technologies having a sufﬁcient amount of RAM becomes even more of an issue.
With regard to system and application software, it’s often wise to perform an
update to keep your system, well, up-to-date. This holds true even if you just
bought the software, because it’s often hard to tell how long the original
packaging has been sitting on the shelves—and even if it is brand-spanking
new, chances are new revisions will still be available. Updates don’t come
without their potential downfalls, however; given the incredible number of
hardware/software system combinations that are available, it’s actually possible
that an update might do as much harm as good. In this light, it’s actually not
a bad idea to do a bit of research before clicking that update button. Whoever
said that all this stuff would be easy?
Just like there never seems to be enough space around the house or apartment,
having a single, undersized monitor can leave you feeling cramped for visual
“real estate.” For starters, a sufﬁciently large monitor that’s capable of working
at higher resolutions will greatly increase the size of your visual desktop; however,
if one is a good thing, two is always better! Both Windows® and Mac OS offer
support for multiple monitors (Figure 7.5). By adding a commonly available
“dual-head” video card, your system can easily double your working monitor
space for fewer bucks than you might think. I’ve found that it’s truly a joy to
have your edit window, mixer, effects sections, and transport controls in their
own places—all in plain and accessible view over multiple monitors.
FIGURE 7.5
You can never have enough
visual “real estate”!

The Desktop Computer
Desktop computers are often (but not always) too large and cumbersome to
lug around. As a result, these systems are most often found as a permanent
installation in the professional, project and home studio (Figure 7.6). Historic -
ally, desktops have offered more processing power than their portable
counterparts, but in recent times, this distinction has become less and less of
a factor.
225
The Digital Audio Workstation  CHAPTER 7
FIGURE 7.6
The desktop computer.
(a) The Mac ProTM with
Cinema display. (Courtesy
of Apple Computers, Inc.,
www.apple.com)
(b) CS450v5 Creation
Station desktop PC.
(Courtesy of Sweetwater,
www.sweetwater.com)
The Laptop Computer
One of the most amazing characteristics of the digital age is miniaturization.
At the forefront of the studio-on-the-go movement is the laptop computer
(Figure 7.7). From the creation of smaller, lighter and more powerful notebooks
has come the technological Phoenix of the portable DAW and music per -
formance machine. With the advent of USB, FireWire and Thunderbolt audio
interfaces, controllers and other peripheral devices, these systems are now
capable of handling most (if not all) of the edit and processing functions that
can be handled in the studio. In fact, these AC/battery-powered systems are
often powerful enough to handle advanced DAW edit/mixing functions, as well
as happily handling a wide range of plug-in effects and virtual instruments, all
in the comfort of—anywhere!
FIGURE 7.7
The laptop computer. 
(a) The MacBook Pro 15”.
(Courtesy of Apple
Computers, Inc.,
www.apple.com)
(b) PCAudiolabs laptop.
(Courtesy of PCAudiolabs,
www.pcaudiolabs.com)

DAW Hardware
That’s the good news! Now, the downside of all this portability is the fact that,
since laptops are optimized to run off of a battery with as little power drain as
possible, their:
n Processors “may” run slower, so as to conserve on battery power
n BIOS (the important subconscious brains of a computer) might be different
(again, especially with regards to battery-saving features)
n Hard drives “might” not spin as fast (generally they’re shipped with 5400
rpm speed drives, although this and SSD technologies have changed)
n Video display capabilities are sometimes limited when compared to a
desktop (video memory is often shared with system RAM, reducing graphic
quality and refresh rate)
n Internal audio interface usually isn’t so great (but that’s why there are so
many external interface options)
As the last option says, it’s no secret that the internal audio quality of most
laptops range from being quite acceptable to abysmal. As a result, about the
only true choice is to ﬁnd an external audio interface that works best for you
and your applications. Fortunately, there’s a ton of audio interface choices for
connecting via either FireWire, USB or Thunderbolt, ranging from a simple stereo
I/O device to those that include multi-channel audio, MIDI and controller
capabilities in a small, on-the-go package.
SYSTEM INTERCONNECTIVITY
In the not-too-distant past, installing a device into a computer or connecting
between computer systems could’ve easily been a major hassle. With the
development of the USB and other protocols (as well as the improved general
programming of hardware drivers), hardware devices such as mice, keyboards,
cameras, soundcards, modems, MIDI interfaces, CD and hard drives, MP3
players, portable fans, LED Christmas trees and cup warmers can be plugged
into an available port, installed and be up and running in no time—generally
without a hassle. Additionally, with the development of a standardized network
and Internet protocol, it’s now possible to link computers together in a way
that allows for the fast and easy sharing of data throughout a connected system.
Using such a system, artists and businesses alike can easily share and swap ﬁles
on the other side of the world, and pro or project studios can swap sound ﬁles
and video ﬁles over the web with relative ease.
USB
In recent computer history, few interconnection protocols have affected our lives
like the universal serial bus (USB). In short, USB is an open speciﬁcation for
connecting external hardware devices to the personal computer, as well as a
special set of protocols for automatically recognizing and conﬁguring them. Here
are the current USB specs:
226

227
The Digital Audio Workstation  CHAPTER 7
n USB 2.0 (up to 480 megabits/sec = 60 megabytes/sec): For high throughput
and fast transfer over the original USB 1.0 spec
n USB 3.0 (up to 5 gigabits/sec = 640 megabytes/sec): For even higher through -
put and fast transfer of the above applications
n USB C (up to 10 gigabits/sec = 1.28 gigabytes/sec): For even higher throughput
and fast transfer of the above applications and includes a plug that can
be inserted in either orientation
The basic characteristics of USB include:
n Up to 127 external devices can be added to a system without having to
open up the computer. As a result, the industry has largely moved toward
a “sealed case” or “locked-box” approach to computer hardware design.
n Newer operating systems will often automatically recognize and conﬁgure
a basic USB device that’s shipped with the latest device drivers.
n Devices are “hot pluggable,” meaning that they can be added (or removed)
while the computer is on and running.
n The assignment of system resources and bus bandwidth is transparent to
the installer and end user.
n USB connections allow data to ﬂow bidirectionally between the computer
and the peripheral.
n USB cables can be up to 5 meters in length (up to 3 meters for low-speed
devices) and include two twisted pairs of wires, one for carrying signal
data and the other pair for carrying a DC voltage to a “bus-powered” device.
Those that use less than 500 milliamps (1/2 amp) can get their power
directly from the USB cable’s 5-V DC supply, while those having higher
current demands will need to be externally powered. USB C, on the other
hand can supply up to 20 volts or 100 watts through the data cable.
n Standard USB 1 through 3 cables have different connectors at each end.
For example, a cable between the PC and a device would have an “A” plug
at the PC (root) connection and a “B” plug for the device’s receptacle.
Cable distribution and “daisy-chaining” are done via a data “hub” (Figure 7.8).
These devices act as a trafﬁc cop in that they cycle through the various USB
inputs in a sequential fashion, routing the data into a single data output line.
FIGURE 7.8
USB hubs in action.

System Interconnectivity
228
FireWire
Originally created in the mid-1990s as the IEEE-1394 standard, the FireWire
protocol is similar to USB in that it uses twisted-pair wiring to communicate
bidirectional, serial data within a hot-swappable, connected chain. Unlike USB
(which can handle up to 127 devices per bus), up to 63 devices can be connected
within a connected FireWire chain. FireWire most commonly supports two
speed modes:
n FireWire 400 or IEEE-1394a (400 megabits/sec) is capable of delivering
data over cables up to 4.5 meters in length. FireWire 400 is ideal for
communicating large amounts of data to such devices as hard drives,
video camcorders and audio interface devices.
n FireWire 800 or IEEE-1394b (800 megabits/sec) can communicate large
amounts of data over cables up to 100 meters in length. When using ﬁber-
optic cables, lengths in excess of 90 meters can be achieved in situations
that require long-haul cabling (such as within sound stages and studios).
Unlike USB, compatibility between the two modes is mildly problematic,
because FireWire 800 ports are conﬁgured differently from their earlier
predecessor, and therefore require adapter cables to ensure compatibility.
Thunderbolt
Originally designed by Intel and released in 2011, Thunderbolt (Figure 7.9)
combines the Display-Port and PCIe bus into a serial data interface. A single
Thunderbolt port can support a daisy chain of up to six Thunderbolt devices
(two of which can be DisplayPort display devices), which can run at such high
speeds as:
n Thunderbolt 1 (up to 10 gigabits/sec = 1.28 gigabytes/sec)
n Thunderbolt 2 (up to 20 gigabits/sec = 2.56 gigabytes/sec)
n Thunderbolt 3 (up to 40 megabits/sec = 5.12 megabytes/sec)
FIGURE 7.9
Thunderbolt ports on a
MacBook Pro.
Audio Over Ethernet
One of the more recent advances in audio and systems connectivity in the studio
and on stage revolves around the concept of communicating audio over the
Ethernet (AoE). Currently, there are several competing protocols that range from
being open-source (no licensing fees) to those that require a royalty to be
designed into a hardware networking system.

By connecting hardware devices directly together via a standard cat5 Ethernet
cable (Figure 7.10), it’s possible for channel counts of up to 512 x 512 to be
communicated over a single connected network. This straightforward system is
designed to replace bulky snake cables and ﬁxed wiring within large studio
installations, large-scale stage sound reinforcement, convention centers and
other complex audio installations. For example, instead of having an expensive,
multi-cable microphone snake run from a stage to the main mixing console a
single Ethernet cable could be run directly from an A/D mic/line cable box to
the mixer (as well as the on-stage monitor mixer, for that matter)—all under
digital control that often can include a redundant cable/system in case of
unforeseen problems or failures.
In short, AoE allows for complex system setups to be interconnected, digitally
controlled and routed in an extremely ﬂexible manner, and since the system is
connected to the Internet, wireless control via apps and computer software is
often fully implemented.
229
The Digital Audio Workstation  CHAPTER 7
FIGURE 7.10
MOTU AVB (Audio Video
Bridge) Switch and 
AVB Control app for
communicating and
controlling audio over
Ethernet. (Courtesy 
of MOTU, Inc.,
www.motu.com)
THE AUDIO INTERFACE
An important device that deserves careful consideration when putting together
a DAW-based production system is the digital audio interface. These devices can
have a single, dedicated purpose, or they might be multifunctional in nature.
In either case, their main purpose in the studio is to act as a connectivity bridge
between the outside world of analog audio and the computer’s inner world of
digital audio (Figures 7.11 through 7.15). Audio interfaces come in all shapes,
sizes and functionalities; for example, an audio interface can be:
n Built into a computer (although, more often than not, these devices are
often limited in quality and functionality)
n A simple, two-I/O audio device
n A multichannel device, offering many I/Os and numerous I/O expansion
options
n Fitted with one or more MIDI I/O ports
n One that offers digital I/O, wordclock and various sync options
n Fitted with a controller surface (with or without motorized faders) that
provides for direct DAW control integration
n A built-in DSP acceleration for offering assisted plug-in processing

The Audio Interface
230
FIGURE 7.11
Steinberg UR22 MkII 2x2
audio interface. (Courtesy of
Steinberg Media
Technologies GmbH, a
division of Yamaha
Corporation,
www.steinberg.net)
FIGURE 7.12
MOTU Ultralite mk3 Hybrid
FireWire/USB audio
interface with effects.
(Courtesy of MOTU, Inc.,
www.motu.com)
FIGURE 7.13
Presonus Studio 192 26x32
channel audio interface.
(Courtesy of Presonus Audio
Electronics, Inc.,
www.presonus.com)
FIGURE 7.14
Apollo audio interface with
integrated UAD effects
processing. (a) Apollo
FireWire/Thunderbolt. 
(b) Apollo Twin USB.
(Courtesy of Universal
Audio, www.uaudio.com ©
2017 Universal Audio, Inc.
All rights reserved. Used
with permission)
FIGURE 7.15
Burl Audio B80 
Mothership audio interface.
(Courtesy of Burl Audio,
www.burlaudio.com)

These devices are most commonly designed as stand-alone and/or 19” rack
mountable systems that plug into the system via USB, FireWire, Thunderbolt
or AoE. An interface might have as few as two inputs and two outputs, or it
might have more than 24. Recent units offer bit depth/sample rate options that
range up to 24/96 or 24/192. In recent times, pretty much all interfaces will
work with any DAW and platform (even Digidesign has dropped their use of
proprietary hardware/software pairing). For this reason, patience and care should
be taken to weigh the various system and audio quality options in a way that
best suits your needs and budget—as these options could easily affect your future
expansion and systems operation choices.
Audio Driver Protocols
Audio driver protocols are software programs that set standards for allowing
data to be communicated between the system’s software and hardware. A few
of the more common protocols are:
n WDM: This driver allows compatible single-client, multichannel applica -
tions to record and play back through most audio interfaces using Microsoft
Windows. Software and hardware that conform to this basic standard can
communicate audio to and from the computer’s basic audio ports.
n ASIO: The Audio Stream Input/Output architecture (which was developed
by Steinberg and offered free to the industry) forms the backbone of VST.
It does this by supporting variable bit depths and sample rates, multi -
channel operation, and synchronization. This commonly used protocol
offers low latency, high performance, easy setup and stable audio recording
within VST.
n MAS: The MOTU Audio System is a system extension for the Mac that
uses an existing CPU to accomplish multitrack audio recording, mixer,
bussing and real-time effects processing.
n CoreAudio: This driver allows compatible single-client, multichannel appli -
cations to record and play back through most audio interfaces using Mac
OS X. It supports full-duplex recording and playback of 16-/24-bit audio
at sample rates up to 96 kHz (depending on your hardware and CoreAudio
client application).
In most circumstances, it won’t be necessary for you to be familiar with the
protocols—you just need to be sure that your software and hardware are
compatible for use with the driver protocol that works best for you. Of course,
further information can always be found at the respective companies’ websites.
Latency
When discussing the audio interface as a production tool, it’s important that
we touch on the issue of latency. Quite literally, latency refers to the buildup of
delays (measured in milliseconds) in audio signals as they pass through the
audio circuitry of the audio interface, CPU, internal mixing structure and I/O
231
The Digital Audio Workstation  CHAPTER 7

The Audio Interface
232
routing chains. When monitoring a signal directly through a computer’s signal
path, latency can be experienced as short delays between the input and
monitored signal. If the delays are excessive, they can be unsettling enough to
throw a performer off time. For example, when recording a synth track, you
might actually hear the delayed monitor sound shortly after hitting the keys
(not a happy prospect) and latency on vocals can be quite unsettling. However,
by switching to a supported ASIO or CoreAudio driver and by optimizing the
interface/DAW buffer settings to their lowest operating size (without causing
the audio to stutter), these delay values can be reduced down to an unnoticeable
or barely noticeable range.
In response to the above problem, most modern interface drivers include a
function called direct monitoring, which allows for the system to monitor inputs
directly from the monitoring source in a way that bypasses the DAW’s
monitoring circuitry. The result is a monitor (cue) source that is free from latency,
allowing the artist to hear themselves without the distraction of delays in the
monitor path.
Need Additional I/O?
Obviously, there are a wide range of options that should be taken into account
when buying an interface. Near the top of this list (audio quality always being
the top consideration) is the need for having an adequate number of inputs
and outputs (I/O).
Although a number of interface designs include a large number of I/O channels,
by far the most have a limited I/O count, but instead offers access to addition
I/O options should the need arise. This can include such options as:
n Lightpipe (ADAT) I/O, whereby each optical cable can give access to either
8 channels at sample rates of 44 or 48k or 4 channels at 96k (if this option
is available), when used with an outboard lightpipe preamp (Figure 7.16).
n Connecting additional audio interfaces to a single computer. This is
possible whenever several compatible interfaces can be detected by and
controlled from a single interface driver.
n Using an audio over the Ethernet protocol and compatible interface
systems, additional I/O can be added by connecting additional AoE devices
onto the network and patching the audio through the system drivers.
FIGURE 7.16
Outboard lightpipe (ADAT)
preamp. (a) Audient ASP800.
(Courtesy of Audient 
Limited, www.audient.com)
(b) Presonus Digimax D8.
(Courtesy of Presonus
Audio Electronics, Inc.,
www.presonus.com)

It’s often important to fully research your needs and possible hardware options
before you buy an interface. Anticipating your future needs is never an easy task,
but it can save you from future heartaches, headaches and additional spending.
DAW Controllers
Originally, one of the more common complaints against most DAWs (particu -
larly when relating to the use of on-screen mixers) is the lack of hardware control
that gives the user direct, hands-on access. Over the years, this has been addressed
by major manufacturers and third-party companies in the form of:
n Hardware DAW controllers
n MIDI instrument controller surfaces that can directly address DAW controls
n On-screen touch monitor surfaces
n iOS-based controller apps
233
The Digital Audio Workstation  CHAPTER 7
It’s important to note that there are a wide range of
controllers from which to choose—and just because
others feel that the mouse is cumbersome doesn’t
mean that you have to feel that way; for example, 
I have several controllers in my own studio, but the
mouse is still my favorite tool. As always, the choice
of what works best for you is totally up to you.
HARDWARE CONTROLLERS
Hardware controller types (Figure 7.17) generally mimic the design of an audio
mixer in that they offer slide or rotary gain faders, pan pots, solo/mute, channel
select buttons, as well as full transport remote functions. A channel select button
might be used to actively assign a speciﬁc channel to a section that contains a
series of grouped pots and switches that relate to EQ, effects and dynamic
functions, or the layout may be simple in form, providing only the most-often
used direct control functions in a standard channel layout.
Such controllers range in the number of channel control strips that are offered
at one time. They’ll often (but not always) offer direct control over eight input
FIGURE 7.17
Hardware controllers. 
(a) Mackie Control
Universal Pro DAW
controller. (Courtesy of
Loud Technologies, Inc.,
www.mackie.com)
(b) SSL Nucleus DAW
controller and audio
interface. (Courtesy of Solid
State Logic, www.solid-
statelogic.com)

The Audio Interface
234
strips at a time, allowing channel groups to be switched in groups of 8 (1–8,
9–16, 17–24, etc.), any number of the grouped inputs can be accessed on the
controller, as well as on the DAW’s on-screen mixer. These devices will also
often include software function keys that can be programmed to give quick and
easy access to the DAW’s more commonly used program keys.
INSTRUMENT CONTROLLERS
Since all controller commands are transmitted between the controller and audio
editor via MIDI and device-speciﬁc MIDI SysEx messages (see Chapter 9). It
only makes sense that a wide range of MIDI instrument controllers (mostly
keyboard controllers) offer a wide range of controls, performance triggers and
system functionality that can directly integrate with a DAW (Figure 7.18). The
added ability of controlling a mix, as well as remote transport control is a nice
feature, should the keyboard controller be out of arm’s reach of the DAW.
FIGURE 7.18
Keyboard controllers will
often provide direct access
to DAW mixing and function
controls. (a) Komplete
Kontrol S49 keyboard
controller. (Courtesy of
Native Instruments 
GmbH, www.native-
instruments.com) 
(b) KX49 keyboard
controller. (Courtesy of
Yamaha Corporation,
www.yamaha.com)
TOUCH CONTROLLERS
In addition to the wide range of hardware controllers that are available on the
market, an ever-growing number of software-based touch-screen monitor con -
trollers have begun to take over the market. These can take the form of standard
touch-screen monitors that let you have simple, yet direct, control over any
software commands, or they can include software that gives you additional
commands and control over speciﬁc DAWs and/or recording-related software
in an easy-to-use fashion (Figure 7.19a). Since these displays are computer-based
devices themselves, they can change their form, function and entire way of
working with a single, uh, touch.
In addition to medium-to-large touch control screens, a number of Wi-Fi-based
controllers are available for the iPad (Figure 7.19b). These controller “apps”,
FIGURE 7.19
Touch screen controllers. 
(a) The Raven MTi Multi-
touch Audio Production
Console. (Courtesy of Slate
Pro Audio, www.slatepro
audio.com) (b) V-Control Pro
DAW controller for the iPad.
(Courtesy of Neyrinck,
www.vcontrolpro.com)

offer direct control over many of the functions that were available on hardware
controllers that used to cost hundreds or thousands of dollars, but are now
emulated in software and can be purchased from an app “store” for the virtual
cost of a cup of coffee.
LARGE-SCALE CONTROLLERS
Another controller type that is a different type of beastie is the large-scale
controller (Figure 7.20). In fact, these controllers (which might or might not
include analog hardware, such as mic preamps) are far more likely to resemble
a full sized music and media production console than a controller surface. They
allow for direct control and communication with the DAW, but offer a large
number of physical and/or touch-screen controls for easy access during such
tasks as mixing for ﬁlm, television and music production.
235
The Digital Audio Workstation  CHAPTER 7
FIGURE 7.20
Digidesign S6 integrated
controller/console. (Courtesy
of Avid Technology, Inc.,
www.avid.com)
SOUND FILE FORMATS
A wide range of sound ﬁle formats exist within audio and multimedia
production. Here is a list of those that are used in professional audio that don’t
use data compression of any type:
n Wave (.wav): The Microsoft Windows format supports both mono and
stereo ﬁles at a variety of bit and sample rates. WAV ﬁles contain PCM
coded audio (uncompressed pulse-code modulation formatted data) that
follows the Resource Information File Format (RIFF) spec, which allows
extra user information to be embedded and saved within the ﬁle itself.
n Broadcast wave (.wav): In terms of audio content, broadcast wave ﬁles are
the same as regular wave ﬁles; however, text strings for supplying additional
information (most notably, time code data) can be embedded in the ﬁle
according to a standardized data format.
n Apple AIFF (.aif or .snd): This standard sound ﬁle format from Apple
supports mono or stereo, 8-, 16- and 24-bit audio at a wide range of sample
rates. Like broadcast wave ﬁles, AIFF ﬁles can contain embedded text
strings.

Sound File Formats
Sound File Sample and Bit Rates
While the sample rate of a recorded bit stream (samples per second) directly
relates to the resolution at which a recorded sound will be digitally captured,
the bit rate of a digitally recorded sound ﬁle directly relates to the number of
quantization steps that are encoded into the bit stream. It’s important that these
rates be determined and properly set before starting a session. Further reading
on sample and bit rate depths can be found in Chapter 6. Additional info on
sound ﬁles and compression codecs can be found in Chapter 11.
Sound File Interchange and Compatibility Between
DAWs
At the sound ﬁle level, most software editors and DAWs are able to read a wide
range of uncompressed and compressed formats, which can then be saved into
a new DAW session format. At the session level, there are several ways to
exchange data for an entire session from one platform, OS or hardware device
to another. These include the following:
n Open Media Framework Interchange (OMFI) is a platform-independent
session ﬁle format intended for the transfer of digital media between
different DAW applications; it is saved with an .omf ﬁle extension. OMF
(as it is commonly called) can be saved in either of two ways: (1) “export
all to one ﬁle,” when the OMF ﬁle includes all of the sound ﬁles and
session references that are included in the session (be prepared, this ﬁle
will be extremely large), or (2) “export media ﬁle references,” which does
not contain the sound ﬁles themselves but will contain all of the session’s
region, edit and mix settings; effects (relating to the receiving DAW’s
available plug-ins and ability to translate effects routing); and I/O settings.
This second type of ﬁle will be small by comparison; however, the original
sound ﬁles must be transferred into the proper session folders.
n One audio-export only option makes use of the broadcast wave sound ﬁle
format. By using Broadcast wave, many DAWs are able to directly read
the time-stamped data that’s imbedded into the sound ﬁle and then
automatically line them up within the session.
n Software options that can convert session data between DAWs also exist.
Pro Convert from Solid State Logic, for example, is a stand-alone program
that helps you tailor sound ﬁle information, level and other information,
and then transfer one DAW format into another format or readable XML
ﬁle.
Although these systems for allowing ﬁle and session interchangeability between
workstation types can be a huge time and work saver, it should be pointed out
that they are, more often than not, far from perfect. It’s not uncommon for 
ﬁles not to line up properly (or load at all), plug-ins can disappear and/or lose
their settings—Murphy’s law deﬁnitely applies. As a result, the most highly
recommended and sureﬁre way to make sure that a session will load into any
236

DAW platform is to make (print or export) a copy of each track, starting from
the session’s beginning (00:00:00:00) and going to the end of that particular
track. Using this system, all you need to do is load each track into the new
workstation at their respective track beginning points and get to work.
Of course, the above method won’t load any of the plug-in or mixer settings
(often an interchange problem anyway). Therefore, it’s extremely important that
you properly document the original session, making sure that:
n All tracks have been properly named (supplying additional track notes
and documentation, if needed).
n All plug-in names and settings are well documented (a screenshot can go
a long way toward keeping track of these settings).
n Any virtual instrument or MIDI tracks are printed to an audio track. (Make
sure to include the original MIDI ﬁles in the session, and to document all
instrument names and settings—again, screenshots can help.)
n Any special effects or automation moves are printed to the particular track
in question (make sure this is well documented) and you should deﬁnitely
consider providing an additional copy of the track without effects or
automation.
DAW SOFTWARE
By their very nature, digital audio workstations (Figures 7.1 through 7.3, as well
as 7.21 and 7.22) are software programs that integrate with computer hardware
and functional applications to create a powerful and ﬂexible audio production
environment. These programs commonly offer extensive record, edit and
mixdown facilities for such uses in audio production as:
n Extensive sound ﬁle recording, edit and region deﬁnition and placement
n MIDI sequencing and scoring
n Real-time, on-screen mixing
n Real-time effects
n Mixdown and effects automation
n Sound ﬁle import/export and mixdown export
n Support for video/picture playback and synchronization
n Systems synchronization
n Audio, MIDI and sync communications with other audio programs (e.g.,
ReWire)
n Audio, MIDI and sync communications with other effects and software
instruments (e.g., VST technology)
This list is but a small smattering of the functional capabilities that can be offered
by an audio production DAW.
237
The Digital Audio Workstation  CHAPTER 7

DAW Software
238
Sufﬁce to say that these powerful software production tools are extremely
powerful and varied in their form and function. As you can see, even with their
inherent strengths, quirks, and complexities, their basic look, feel, and
operational capabilities have, to some degree, become uniﬁed among the major
DAW competitors. Having said this, there are enough variations in features,
layout, and basic operation that individuals (from aspiring beginner to seasoned
professional) will have their favorite DAW make and model. With the growth
of the DAW and computer industries, people have begun to customize their
computers with features, added power and peripherals that rival their love for
supped-up cars and motorcycles. In the end, though (as with many things in
life), it doesn’t matter which type of DAW you use—it’s how you use it that
counts!
Sound Recording and Editing
Most digital audio workstations are capable of recording sound ﬁles in mono,
stereo, surround or multichannel formats (either as individual ﬁles or as a
single interleaved ﬁle). These production environments graphically display
sound ﬁle information within a main graphic window (Figure 7.23), which
contains drawn waveforms that graphically represent the amplitude of a sound
ﬁle over time in a WYSIWYG (what you see is what you get) fashion. Depending
on the system type, sound ﬁle length and the degree of zoom, the entire
waveform may be shown on the screen, or only a portion will show as it scrolls
over the course of the song or program. Graphic editing differs greatly from the
“razor blade” approach that’s used to cut analog tape, in that the waveform
gives us both visual and audible cues as to precisely where an edit point should
FIGURE 7.21
Reaper DAW software.
(Courtesy of Cockos
Incorporated,
www.reaper.fm)
FIGURE 7.22
Presonus Studio One DAW
software. (Courtesy of
Presonus Audio Electronics,
Inc., www.presonus.com)

be. Using this common display technique, any position, cut/copy/paste, gain
or time changes will be instantly reﬂected in the waveforms on the screen. Almost
always, these edits are nondestructive (a process whereby the original sound
ﬁle isn’t altered—only the way in which the region in/out points are accessed
or the ﬁle is processed will be changed, undone, redone, copied, pasted—
virtually without limit.
Only when a waveform is zoomed-in fully is it possible to see the individual
sample amplitude levels of a sound ﬁle (Figure 7.24). At this zoom level, it
becomes simple to locate zero-crossing points (points where the level is at the
0, center-level line). In addition, when a sound ﬁle is zoomed-in to this level,
the program might allow the sample points to be redrawn in order to remove
potential offenders (such as clicks and pops) or to smooth out amplitude
transitions between loops or adjacent regions.
239
The Digital Audio Workstation  CHAPTER 7
FIGURE 7.23
Main edit window within 
the Cubase audio 
production software.
(Courtesy of Steinberg
Media Technologies GmbH,
a division of Yamaha
Corporation,
www.steinberg.net)
FIGURE 7.24
Zoomed-in edit window
showing individual samples.
The nondestructive edit capabilities of a DAW refer to a disk-based system’s
ability to edit a sound ﬁle without altering the data that was originally recorded
to disk. This important capability means that any number of edits, alterations
or program versions can be performed and saved to disk without altering the
original sound ﬁle data.
Nondestructive editing is accomplished by accessing deﬁned segments of a
recorded digital audio ﬁle (often called regions) and allowing them to be

DAW Software
240
reproduced in a user-deﬁned order, deﬁned segment in/out point or level in a
manner that can be (and often is) different than the originally recorded sound
ﬁle. In effect, when a speciﬁc region is deﬁned, we’re telling the program to
access the sound ﬁle at a point that begins at a speciﬁc memory address on the
hard disk and continues until a speciﬁed ending address has been reached (Figure
7.25). Once deﬁned, these regions can be inserted into a program list (often
called a playlist or edit list) in such a way that they can be accessed and
reproduced in any order and any number of ti-ti-ti-times. For example, Figure
7.26 shows a snippet from Gone With the Wind that contains Rhett’s immortal
words “Frankly, my dear, I don’t give a damn.” By segmenting it into three
regions we could use a DAW editor to output the words in several ways.
FIGURE 7.25
Nondestructive editing
allows a region within a
larger sound file to begin at
a specific in-point and play
until the user-defined end-
point is reached.
FIGURE 7.26
Example of how snippets
from Rhett’s famous Gone
with the Wind dialogue can
be easily rearranged using
standard non-destructive
editing.
When working in a graphic editing environment, regions can usually be deﬁned
by positioning the cursor over the waveform, pressing and holding the mouse
or trackball button and then dragging the cursor to the left or right, which
highlights the selected region for easy identiﬁcation. After the region has been
deﬁned, it can be edited, marked, named, maimed or otherwise processed.
As one might expect, the basic cut-and-paste techniques used in hard-disk
recording are entirely analogous to those used in a word processor or other
graphics-based programs:
n Cut: Places the highlighted region into clipboard memory and deletes the
selected data (Figure 7.27a).

241
The Digital Audio Workstation  CHAPTER 7
n Copy: Places the highlighted region into memory and doesn’t alter the
selected waveform in any way (Figure 7.27b).
n Paste: Copies the waveform data that’s within the system’s clipboard
memory into the sound ﬁle beginning at the current cursor position (Figure
7.27c).
Besides basic nondestructive cut-and-paste editing techniques, the amplitude
processing of a signal is one of the most common types of changes that are
likely to be encountered. These include such processes as gain changing,
normalization and fading.
1. Download a demo copy of your favorite DAW
(these are generally available off the company’s
website for a free demo period).
2. Download the workstation’s manual and
familiarize yourself with its functional operating
basics.
3. Consult the manual regarding the recording of a
sound ﬁle.
4. Assign a track to an interface input sound
source.
5. Name the track! It’s always best to name the
track (or tracks) before going in to record. In this
way, the ﬁle will be saved to disk within the
session folder under a descriptive name instead
of an automatically generated ﬁle name (e.g.,
killerkick.wav instead of track16–01.wav).
6. Save the session and assign the input to another
track, and overdub a track along with the
previously recorded track.
7. Repeat as necessary until you’re having fun!
8. Save your ﬁnal results for the next tutorial.
Try This: Recording a Sound ﬁle to Disk
D I Y
 do  it  yourself
FIGURE 7.27
Standard Cut, Copy & Paste
commands. (a) Cutting
inserts the highlighted
region into memory and
deletes the selected data.
(b) Copying simply places
the highlighted region into
memory without changing
the selected waveform in
any way. (c) Pasting copies
the data within the system’s
clipboard memory into the
sound file at the current
cursor position.

DAW Software
242
Gain changing relates to the altering of a region or track’s overall amplitude
level, such that a signal can be proportionally increased or reduced to a speciﬁed
level (often in dB or percentage value). To increase a sound ﬁle or region’s overall
level, a function known as normalization can be used. Normalization (Figure
7.28) refers to an overall change in a sound ﬁle or deﬁned region’s signal level,
whereby the ﬁle’s greatest amplitude will be set to 100% full scale (or a set
percentage level of full scale), with all other levels in the sound ﬁle or region
being proportionately scaled up or down in gain level.
1. Open the session from the preceding tutorial,
“Recording a Sound File to Disk.”
2. Consult your editor’s manual regarding basic
cut-and-paste commands (which are almost
always the standard PC and Mac commands).
3. Open a sound ﬁle and deﬁne a region that
includes a musical phrase, lyric or sentence.
4. Cut the region and try to paste it into another
point in the sound ﬁle in a way that makes sense
(musical or otherwise).
5. Feel free to cut, copy and paste to your heart’s
desire to create an interesting or totally wacky
sound ﬁle.
Try This: Copy and Paste
D I Y
 do  it  yourself
FIGURE 7.28
Original signal and
normalized signal level.
The fading of a region (either in or out, as shown in Figure 7.29) is accomplished
by increasing or reducing a signal’s relative amplitude over the course of a deﬁned
duration. For example, fading in a ﬁle proportionately increases a region’s gain
from inﬁnity (zero) to full gain. Likewise, a fade-out has the opposite effect of
creating a transition from full gain to inﬁnity. These DSP functions have the
advantage of creating a much smoother transition than would otherwise be
humanly possible when performing a manual fade.

A cross-fade (or X-fade) is often used to smooth the transition between two
audio segments that either are sonically dissimilar or don’t match in amplitude
at a particular edit point (a condition that would otherwise lead to an audible
“click” or “pop”). This useful tool basically overlaps a fade-in and fade-out
between the two waveforms to create a smooth transition from one segment 
to the next (Figure 7.30). Technically, this process averages the amplitude of
the signals over a user-deﬁnable length of time in order to mask the offending
edit point.
243
The Digital Audio Workstation  CHAPTER 7
FIGURE 7.29
Examples of fade-in and
fade-out curves.
FIGURE 7.30
Example of a cross-
fade window. (Courtesy 
of Steinberg Media
Technologies GmbH, 
a division of Yamaha
Corporation,
www.steinberg.net)
Fixing Sound with a Sonic Scalpel
In traditional multitrack recording, should a mistake or bad take be recorded
onto a new track, it’s a simple matter to start over and re-record over the
unwanted take. However, if only a small part of the take was bad, it’s easy to
go back and perform a punch-in (Figure 7.31). During this process, the recorder
or DAW:
n Silently enters into record at a predetermined point
n Records over the unwanted portion of the take
n Silently falls back out of record at a predetermined point.
A punch can be manually performed on most recording systems; however,
DAWs and newer tape machines can be programmed to automatically go into
and fall out of record at a predetermined time.

DAW Software
244
FIGURE 7.31
Punch-ins let you selectively
replace material and correct
mistakes. (Courtesy of
Steinberg Media
Technologies GmbH, a
division of Yamaha
Corporation,
www.steinberg.net)
From a continuity standpoint, it’s often best to punch-in on a section
immediately after the take has been recorded, because changes in mic choice,
mic position or the session’s general “vibe” can lead to a bad punch that simply
doesn’t match the original take’s general sound. If this isn’t possible, make sure
that you match the sounds by carefully documenting the mic choice, placement,
preamp type, etc. You’ll be glad you did.
Naturally, performing a “punch” should always be done with care. In some
non-DAW cases, allowing the track to continue recording after the intended
out- could possibly cut off a section of the following, acceptable track and
likewise require that the following section be redone. Stopping it short could
cut off the natural reverb trail of the ﬁnal note.
It needs to be pointed out that performing a punch using a DAW is often “far”
easier than doing the same on an analog recorder. For example:
n If the overdub wasn’t that great, you can simply click to “undo” it and
start over!
n If the overdub was started early and cut into the good take (or went too
long), the leading and/or trailing edge of the punch can often be manually
adjusted to expose or hide sections after the punch has been performed
(a tape editor’s dream).
These beneﬁcial luxuries can go a long way toward reducing operator error (and
its associated tensions) during a session.
When punching-in, any number of variables can
come into play. If a solo instrument is to be
overdubbed, it’s often easy to punch the track
without fear of any consequences. If an offending
musical section is within a group or ensemble,
leakage from the original instrument could ﬁnd its
way into adjacent tracks, making a punch difﬁcult or
unwise. In such a situation, it’s usually best to re-
record the piece, pick up at a point just before the
bad section and splice (edit insert) it back into the
original recording, or attempt to punch the section
using the entire ensemble.

Comping
When performing a musically or technically complex overdub, most DAWs will
let you comp (short for composite) multiple overdubs together into a ﬁnal, master
take (Figure 7.32). Using this process, a DAW can be programmed to
automatically enter into and out of record at the appropriate points. When placed
into record mode, the DAW will start laying down the overdub into a new and
separate track (called a “lane”). At the end of the overdub, it’ll loop back to the
beginning and start recording the next pass onto a new and separate lane. This
process of laying down consecutive takes will continue, until the best take is
done or the artist simply gets tired of recording. Once done, an entire overdub
might be chosen, or individual segments from the various takes can be assembled
together into a ﬁnal, master composite overdub. Such is the life of a digital
micro-surgeon!
245
The Digital Audio Workstation  CHAPTER 7
FIGURE 7.32
A single composite track
can be created from several
partially acceptable takes.
MIDI Sequencing and Scoring
Most DAWs include extensive support for MIDI (Figure 7.33), allowing electronic
instruments, controllers, effects devices, and electronic music software to be
integrated with multitrack audio and video tracks. This important feature often
includes the full implementation for:
n MIDI sequencing, processing and editing
n Score editing and printing
n Drum pattern and step note editing
FIGURE 7.33
MIDI edit windows with
Steinberg’s Cubase/
Nuendo DAW. (a) Piano roll
edit window. (b) Notation
edit window. (Courtesy 
of Steinberg Media
Technologies GmbH, 
a division of Yamaha
Corporation,
www.steinberg.net)

DAW Software
246
n MIDI signal processing
n Support for linking the timing and I/O elements of an external music
application (often via ReWire)
n Support for software instruments (VSTi and RTAS)
Further reading about the wonderful world of MIDI can be found in Chapter 9.
Support for Video and Picture Sync
Most high-end DAWs also include support for displaying a video track within
a session, both as a video window that can be displayed on the desktop and
in the form of a video thumbnail track (which often appears as a linear guide
track within the edit window). Through the use of SMPTE timecode, MTC and
wordclock, external video players and edit devices can be locked with the
workstation’s timing elements, allowing us to have full “mix to picture”
capabilities (Figure 7.34).
FIGURE 7.34
Most high-end DAW
systems are capable of
importing a video file
directly into the project
session window.
Real-Time, On-Screen Mixing
In addition to their ability to offer extensive region edit and deﬁnition, one of
the most powerful cost- and time-effective features of a digital audio workstation
is the ability to offer on-screen mixing capabilities (Figure 7.35), known as
mixing “in the box.” Essentially, most DAWs include a digital mixer interface
that offers most (if not more) of the capabilities that are offered by larger analog
and/or digital consoles—without the price tag and size. In addition to the 
basic input strip fader, pan, solo/mute and select controls, most DAW software
mixers offer broad support for EQ, effects plug-ins (offering a tremendous
amount of DSP ﬂexibility), routing, spatial positioning (pan and often surround-
sound positioning), total automation (both mixer and plug-in automation),
mixing and transport control from an external surface, support for exporting
(bouncing) a mixdown to a ﬁle—the list goes on and on and on. Further reading
on the mixers, consoles and the process of mixing audio can be found in
Chapter 17.

DSP Effects
In addition to being able to cut, copy and paste regions within a sound ﬁle, it’s
also possible to alter a sound ﬁle, track or segment using digital signal processing
techniques. In short, DSP works by directly altering the samples of a sound ﬁle
or deﬁned region according to a program algorithm (a set of programmed
instructions) in order to achieve a desired result. These processing functions
can be performed either in real time or non-real time (ofﬂine):
n Real-time DSP: Commonly used in most modern-day DAW systems, this
process makes use of the computer’s CPU or additional acceleration
hardware to perform complex DSP calculations during actual playback.
Because no calculations are written to disk in an ofﬂine fashion, signiﬁ-
cant savings in time and disk space can be realized when working with
pro ductions that involve complex or long processing events. In addition,
the automation instructions for real-time processing are embedded 
within the saved session ﬁle, allowing any effect or set of parameters to
be changed, undone and redone without affecting the original sound 
ﬁle.
n Non-real-time DSP: Using this method, signal processing (such as changes
in level, L/R channel swapping, etc.) can be saved as a unique sound ﬁle
in a non-real-time fashion. In this way, the newly calculated ﬁle (containing
an effect, volume change, combined comp. tracks, sub-mix, etc.) will be
played back without the need for additional, real-time CPU processing.
It’s good to know that DAWs will often have a speciﬁc term for tracks or
processing functions that have been written to disk in order to save on
processing—often being called “locking” or “freezing” a ﬁle. These ﬁles
can almost always be unlocked at a later time to revert to real-time DSP
processing. When DSP is “printed” to a new ﬁle in non-real time, it’s almost
always wise to save both the original and the affected sound ﬁles, just in
case you need to make changes at a later time.
n Most DAWs offer an extensive array of DSP options, ranging from options
that are built into the basic I/O path of any input strip (e.g., basic EQ and
gain-related functions) to DSP effects and plug-ins that come bundled with
the DAW package, to third-party effects plug-ins that can be either inserted
247
The Digital Audio Workstation  CHAPTER 7
FIGURE 7.35
DAW on-screen mixer. 
(a) ProTools on-screen
mixer. (Courtesy of Avid
Technology, Inc.,
www.avid.com) 
(b) Nuendo on-screen 
mixer. (Courtesy of
Steinberg Media
Technologies GmbH, a
division of Yamaha
Corporation,
www.steinberg.net)

DAW Software
directly into the signal path (insert) or offered as a (send) that can be
assigned to numerous tracks within a mix. Although the way in which
effects are dealt with in a DAW will vary from one make and model to
the next, the basic fundamentals will be much the same.
DSP Plug-Ins
Workstations often offer a number of stock DSP effects that come bundled with
the program; however, a staggering range of third-party plug-in effects can be
inserted into a signal path which perform functions for any number of tasks
ranging from the straightforward to the wild-’n’-zany. These effects can be
programmed to seamlessly integrate into a host DAW application that conforms
to such plug-in platforms as:
n DirectX: A DSP platform for the PC that offers plug-in support for sound,
music, graphics (gaming) and network applications running under
Microsoft Windows (in its various OS incarnations)
n AU (Audio Units): Developed by Apple for audio and MIDI technologies
in OS X; allows for a more advanced GUI and audio interface
n VST (Virtual Studio Technology): A native plug-in format created by Steinberg
for use on either a PC or Mac; all functions of a VST effect processor or
instrument are directly controllable and automatable from the host
program
n MAS (MOTU Audio System): A real-time native plug-in format for the Mac
that was created by Mark of the Unicorn as a proprietary plug-in format
for Digital Performer; MAS plug-ins are fully automatable and do not
require external DSP in order to work with the host program
n AudioSuite: A ﬁle-based plug-in that destructively applies an effect to a
deﬁned segment or entire sound ﬁle, meaning that a new, affected version
of the ﬁle is rewritten in order to conserve on the processor’s DSP overhead
(when applying AudioSuite, it’s often wise to apply effects to a copy of
the original ﬁle so as to allow for future changes)
n RTAS (Real-Time Audio Suite): A fully automatable plug-in format that was
designed for various ﬂavors of Digidesign’s Pro Tools and runs on the
power of the host CPU (host-based processing) on either the Mac or PC
n TDM (Time Domain Multiplex): A plug-in format that can only be used
with Digidesign Pro Tools systems (Mac or PC) that are ﬁtted with
Digidesign Farm cards; this 24-bit, 256-channel path integrates mixing and
real-time digital signal processing into the system with zero latency and
under full automation
These popular software applications (which are programmed by major manu -
facturers and smaller startup companies alike) have helped to shape the face of
the DAW by allowing us to pick and choose the plug-ins that best ﬁt our
personal production needs. As a result, new companies, ideas and task-oriented
products are constantly popping up on the market, literally on a monthly basis.
248

249
The Digital Audio Workstation  CHAPTER 7
ACCELERATOR PROCESSING SYSTEMS
In most circumstances, the CPU of a host DAW program will have sufﬁcient
power and speed to perform all of the DSP effects and processing needs of a
project. Under extreme production conditions, however, the CPU might run
out of computing steam and choke during real-time playback. Under these
conditions, there are a couple of ways to reduce the workload on a CPU: On
one hand, the tracks could be “frozen,” meaning that the processing functions
would be calculated in non-real time and then written to disk as a separate ﬁle.
On the other hand, an accelerator card (Figure 7.36) that’s capable of adding
extra CPU power can be added to the system, giving it the necessary real-time
power to perform the required effects calculations. Of course, as computers 
have gotten faster and more powerful, native processing packages have come
onto the market, which make use of the computer’s own multi-processor
capabilities.
FIGURE 7.36
The UAD-2 DSP PCIe and
Thunderbolt (Mac) or USB
(Win) DSP processor and
several plug-in examples.
(Courtesy of Universal
Audio, www.uaudio.com, 
© 2017 Universal Audio,
Inc. All rights reserved.
Used with permission)
FUN WITH EFFECTS
The following effects notes describe but a few of the possible effects that can
be plugged into the signal path of DAW; however, further reading on effects
processing can be found in Chapter 15 (Signal Processing).
Equalization
EQ is, of course, a feature that’s often implemented at the basic level of a virtual
input strip (Figure 7.37). Most DAW “strips” also include one that gives full
parametric control over the entire audible range, offering overlapping control
over several bands with a variable degree of bandwidth control (Q). Beyond
the basic EQ options, numerous third-party EQ plug-ins are available on the
market that vary in complexity, musicality and market appeal (Figure 7.38).
Dynamics
Dynamic range processors (Figures 7.39 and 7.40) can be used to change the
signal level of a program. Processing algorithms are available that emulate a
Equalization 
Dynamics 

DAW Software
compressor (a device that reduces gain by a ratio that’s proportionate to the
input signal), limiter (reduces gain at a ﬁxed ratio above a certain input
threshold), or expander (increases the overall dynamic range of a program).
These gain changers can be inserted directly into a channel or group master
track or inserted into the ﬁnal master output path.
In addition to the basic complement of stock and third-party dynamic range
processors, wide assortments of multiband dynamic plug-in processors (Figure
7.41) are available for general and mastering DSP applications. These processors
250
FIGURE 7.37
DAWs offer a stock EQ 
on their channel strip. 
(a) 7-Band Digirack EQIII
plug-in for Pro Tools.
(Courtesy of Avid
Technology, Inc.,
www.avid.com)
(b) EQ plug-in for
Cubase/Nuendo. (Courtesy
of Steinberg Media
Technologies GmbH, a
division of Yamaha
Corporation,
www.steinberg.net)
FIGURE 7.38
EQ plug-ins (a) FabFilter
Pro-Q 24-band EQ plug-in
for mixing and mastering.
(Courtesy of FabFilter,
www.fabfilter.com) 
(b) Sonnox EQ and Filters for
Apollo and the UAD effects
processing card. (Courtesy
of Universal Audio,
www.uaudio.com © 2017
Universal Audio, Inc. All
rights reserved. Used with
permission)
FIGURE 7.39
DAW stock compressor
plug-ins. (a) Compressor/
limiter plug-in for Pro 
Tools. (Courtesy of Avid
Technology, Inc.,
www.avid.com) 
(b) Compressor plug-in
for Cubase/Nuendo.
(Courtesy of Steinberg
Media Technologies GmbH,
a division of Yamaha
Corporation,
www.steinberg.net)

allow the overall frequency range to be broken down into various frequency
bands. For example, a plug-in such as this could be inserted into a DAW’s main
output path, which allows the lows to be compressed while the mids are lightly
limited and the highs are simultaneously de-essed to reduce harsh sibilance in
the mix.
Delay
Another important effects category that can be used to alter and/or augment a
signal revolves around delays and regeneration of sound over time. These time-
based effects use delay (Figure 7.42) to add a perceived depth to a signal or
change the way that we perceive the dimensional space of a recorded sound. A
wide range of time-based plug-in effects exist that are all based on the use of
delay (and/or regenerated delay) to achieve such results as:
n Delay
n Chorus
n Flanging
n Reverb
251
The Digital Audio Workstation  CHAPTER 7
FIGURE 7.40
Various compressor plug-ins
for Apollo and the UAD
effects processing card.
(Courtesy of Universal
Audio, www.uaudio.com 
© 2017 Universal Audio,
Inc. All rights reserved.
Used with permission)
FIGURE 7.41
Multiband compressor 
plug-ins. (a) Multiband
compressor for Pro Tools.
(Courtesy of Avid
Technology, Inc.,
www.avid.com) 
(b) Multiband compressor
for Cubase/Nuendo.
(Courtesy of Steinberg
Media Technologies GmbH,
a division of Yamaha
Corporation,
www.steinberg.net)
Delay 

DAW Software
252
Pitch and Time Change
Pitch change functions make it possible to shift the relative pitch of a deﬁned
region or track either up or down by a speciﬁc percentage ratio or musical
interval. Most systems can shift the pitch of a sound ﬁle or deﬁned region by
determining a ratio between the current and the desired pitch and then adding
(lower pitch) or dropping (raise pitch) samples from the existing region or sound
ﬁle. In addition to raising or lowering a sound ﬁle’s relative pitch, most systems
can combine variable sample rate and pitch shift techniques to alter the duration
of a region or track. These pitch- and time-shift combinations make it possible
for such changes as:
n Pitch shift only: A program’s pitch can be changed while recalculating the
ﬁle so that its length remains the same.
n Change duration only: A program’s length can be changed while shifting
the pitch so that it matches that of the original program.
n Change in both pitch and duration: A program’s pitch can be changed while
also having a corresponding change in length.
When combined with shifts in time (delay), changes in pitch make it possible
for a world of time-based effects, alterations, tempo changes and more to be
created. For example:
n Should a note be played that’s out of pitch—instead of going back and
doing an overdub, it’s a simple matter to simply zoom in on that note
and change its pitch up or down, till it’s right.
n Using pitch shift, it’s a simple matter to perform time stretching to do any
number of tasks. For example:
– Should you be asked to produce a radio commercial that is 30 seconds
long, and the producer tells (after the fact) that it has to be 28 seconds—
it’s a simple matter to time stretch the entire commercial, so as to trim
the 2 seconds off.
– The tempo of an entire song can be globally shifted in time or pitch,
at the touch of a button, to change the entire key or tempo of a song.
– Should you import a musical groove that’s of a different tempo than
your session tempo; most DAWs will let you slip the groove in time,
so that its tempo ﬁts perfectly simply by dragging the boundaries.
Changing the groove’s pitch is likewise a simple matter.
FIGURE 7.42
Various delay plug-ins for
Apollo and the UAD effects
processing card. (Courtesy
of Universal Audio,
www.uaudio.com © 2017
Universal Audio, Inc. All
rights reserved. Used with
permission)
Pitch and Time Change 

– Dedicated plug-ins can also be used to automatically tune a vocal or
instrumental track, so that the intonation is corrected, smoothed out
or exaggerated for effect.
– A process called “warping” can be used to apply micro changes in
musical timing (using time and pitch shift processing) to ﬁt, modify,
shake up or otherwise mangle a section within a passage or groove.
Deﬁnitely fun stuff!
If you’re beginning to get the idea that there are few limitations to the wonderful
world of pitch shifting—you’re right. However, there are deﬁnitely limits and
guidelines that should be adhered to, or at least experimented with. For starters:
n A single program will often have several algorithms that can be applied
to a passage (depending upon if it’s percussive, melodic or continuous in
nature). Not all algorithms are created equal. Also the algorithms of one
program can easily sound totally different than that of another program.
It isn’t often straightforward or set-in-stone, as the processing is simply
often too complex to predict . . . it often will require careful  experimenta -
tion and artistry
n Shifting in time or pitch (two sides of the same coin) by too great a value
can cause audible side effects. You’ll simply have to experiment.
ReWire
ReWire and ReWire2 are special protocols that were co-developed by Propeller-
head Software and Steinberg to allow audio to be streamed between two
simultaneously running computer applications. Unlike a plug-in, where a task-
speciﬁc application is inserted “into” a compatible host program, ReWire allows
the audio and timing elements of an independent client program to be seam -
lessly integrated into another host program. In essence, ReWire provides virtual
patch chords that link the two programs together within the computer. A few
of ReWire’s supporting features include:
n Real-time streaming of up to 64 separate audio channels (256 with
ReWire2) at full bandwidth from one program into its host program
application
n Automatic sample accurate synchronization between the audio in the two
programs
n An ability to allow the two programs to share a single soundcard or
interface
n Linked transport controls that can be controlled from either program
(provided it has some kind of transport functionality)
n An ability to allow numerous MIDI outs to be routed from the host
program to the linked application (when using ReWire2)
n A reduction of the total number of system requirements that would be
required if the programs were run independently
253
The Digital Audio Workstation  CHAPTER 7

DAW Software
254
This useful protocol essentially allows a compatible program to be plugged into
a host program in a tandem fashion. As an example, ReWire could allow
Propellerhead’s Reason (client) to be “ReWired” into Steinberg’s Cubase DAW
(host), allowing all MIDI functions to pass through Cubase into Reason while
patching the audio outs of Reason into Cubase’s virtual mixer inputs (Figure
7.43). For further information on this useful protocol, consult the supporting
program manuals and web videos.
Mixdown and Effects Automation
One of the great strengths of the “in the box” age is how easily all of the mix
and effects parameters can be automated and recalled within a mix. The ability
to change levels, pan and virtually control any parameter within a project makes
it possible for a session to be written to disk, saved and recalled at a second’s
notice. In addition to grabbing a control and moving it manually (either virtually
on-screen or from a physical controller), another interface style for controlling
automation parameters (known as rubber band controls) lets you view, draw
and edit variables as a graphic line that details the various automation moves
over time.
As with any automation moves, these rubber band settings can be undone,
redone or recalled back to a speciﬁc point in the edit stage. Often (but not
always), the fader volume moves within a mix can’t be “undone” and reverted
back to any speciﬁc point in the mix. In any case, one of the best ways to save
(and revert to) a particular mix version (or various alternate mix versions) is
simply to save a speciﬁc mix under a unique, descriptive session ﬁle title 
(e.g., gamma_ultraviolet_radiomix01.ses) and then keep on working. By the
way, it’s always wise to save your mixes on a regular basis (many a great mix
has been lost in a crash because it wasn’t saved or the auto-save function didn’t
work properly); in addition, progressively saving your mixes under various
name or version numbers (mix01.ses, mix02.ses, etc.) can come in handy if
you need to revert to a past version. In short, save often and save regularly!
Exporting a Final Mixdown to File
Once your mix is ready, most DAWs systems are able to export (bounce or print)
part or all of a session to a single ﬁle or set of sound ﬁles (Figure 7.44). 
FIGURE 7.43
ReWire allows a client
program to be inserted into
a host program (often a
DAW) so the programs can
run simultaneously in
tandem.

255
The Digital Audio Workstation  CHAPTER 7
FIGURE 7.44
Most DAWs can export
(bounce) session sound
files, effects and automation
to a final mixdown track.
An entire session or deﬁned region can be exported as a single interleaved ﬁle
(containing multiple channels that are encoded into a single L-R-L-R sound
ﬁle), or can be saved as separate, individual (L.wav and R.wav) sound ﬁles. Of
course, a surround or multichannel mix can be likewise exported as a single
interleaved ﬁle, or as separate ﬁles.
Often, the session can be exported in non-real time (a faster-than-real-time
process that can include all mix, plug-in effects, automation and virtual
instrument calculations) or in real time. Usually, a session can be mixed down
in a number of ﬁnal sound ﬁle and bit/sample rate formats.
POWER TO THE PROCESSOR . . . UHHH, PEOPLE!
Speaking of having enough power and speed to get the job done, there are
deﬁnitely some tips and tricks that can help you get the most out of your digital
audio workstation. Let’s take a look at some of the more important items. It’s
vital to keep in mind that keeping up with technology can have its triumphs
and its pitfalls. No matter which platform you choose to work with, there’s no
substitute for reading, research and talking with your peers about your techno
needs. It’s generally best to strike a balance between our needs, our desires, the
current state of technology and the relentless push of marketing to grab our
money—and it’s usually best to take a few big breaths (days, weeks, etc.) before
making any important decisions.
1 Get a Computer That’s Powerful Enough
With the increased demand for higher bit/sample rate resolution, more tracks,
more plug-ins, more of everything, you’ll obviously want to make sure that your
computer is fast and powerful enough to get the job done in real time without
spitting and sputtering digits. This often means getting the most up-to-date and
powerful computer/processor system that your budget can reasonably handle.
With the advent of 32 and 64-bit OS platforms and quad or eight-core processors
(chips that effectively contain multiple CPUs), you’ll want to make sure that
your hardware will support these features before taking the upgrade plunge.

Power to the Processor
The same goes for your production software and driver availability. If any part
of this hardware, software and driver equation is missing, the system will not
be able to make use of these advances. Therefore, one of the smartest things
you can do is research the type and system requirements that would be needed
to operate your production system, and then make sure that your system exceeds
these ﬁgures by a comfortable margin so as to make allowances for future
technological advances and the additional processing requirements that are
associated with them. If you have the budget to add some of the extra bells and
whistles that go with living on the cutting edge, you should take the time to
research whether or not your system will actually be able to deliver the extra
goods when these advances actually hit the store shelves.
2 Make Sure You Have Enough Fast Memory
It almost goes without saying that your system will need to have an adequate
amount of random access memory (RAM) and hard-disk storage in order for
you to take full advantage of your processor’s potential and your system’s data
storage requirements. RAM is used as a temporary storage area for data that is
being processed and passed to and from the computer’s central processing unit
(CPU). Just as there’s a “need for speed” within the computer’s CPU, it’s usually
best that we install memory with the fastest possible transfer speed that can be
supported by the computer. It’s also important that you install as much memory
as your computer and budget will allow. Installing too little RAM will force the
OS to write this temporary data to and from the hard disk, a process that’s
much slower than transfer to RAM and causes the system’s overall performance
to slow to a crawl. For those who are making extensive use of virtual sampling
technology (whereby samples are transferred to RAM), it’s usually a wise idea
to throw as much RAM into the system as possible.
Hard-disk requirements for a system are certainly an important consideration.
The general considerations include:
n Need for size: Obviously, you’ll want to have drives that are large enough
to meet your production storage needs. With the use of numerous tracks
within a session, often at sample rates of 24/96, data storage requirements
can quickly become an important consideration.
n Need for speed: With the current track count and sample rate requirements
that can commonly be encountered in a DAW session, it’s easy to
understand how slower disk access times (the time that’s required for the
drive heads to move from one place to another on a disk and then output
that data) becomes important.
3 Keep Your Production Media Separate
Whenever possible, it’s important that you keep your program and operating
system data on a separate drive from the one that holds your production 
media data. This is due to the simple fact that a computer periodically has to
check in and interact with both the currently running program and the OS.
256

257
The Digital Audio Workstation  CHAPTER 7
FIGURE 7.45
You can never have 
enough visual real estate:
(a) side-by-side; (b) top
screen shows edit screen,
while the bottom (possibly a
touch screen) displays the
mixer in a traditional layout.
Should the production media be on the same disk, interruptions in audio data
can occur as the disk takes time to go perform program-related tasks, resulting
in a reduction in media and program data access and throughput time (not
good).
4 Update Your Drivers . . . With Caution!
In this day and age of software revisions, it’s always a good idea to go on the
Web and search for the latest update to a piece of software or a hardware driver.
Even if you’ve just bought a product new out of the box, it might easily have
been sitting on a music store shelf for over a year. By going to the company
website and downloading the latest versions, you’ll be assured that it has the
latest and greatest capabilities. In addition, it’s always wise to save these updates
to disk in your backup directories. This way, if you’re without Internet and there’s
a hardware or software problem, you’ll be able to reload the software or drivers
and should be on your way in no time.
5 Going (At Least) Dual Monitor
Q: How do you ﬁt the easy visual reference of multiple programs, documents
and a digital audio workstation onto a single video monitor?
A:
You often don’t. Those of you who rely on your computer for recording
and mixing, surﬁn’, writing, etc., should deﬁnitely think about doubling
your computer’s visual real estate by adding an extra monitor to your
computer system.
Folks who have never seen or thought much about adding a second monitor
(Figure 7.45) might be skeptical and ask, “What’s the big deal?” But, all you
have to do is sit down and start opening programs onto a single screen just to
see how fast your screen can get ﬁlled up. When using a complicated produc-
tion program (such as a professional DAW or a high-end graphics app), 
getting the job done with a single monitor can be an exercise in frustration.
There’s just too much we need to see and not enough screen real estate to show
it on.
Truth is, in this age of Mac and Windows, adding an extra monitor is a fairly
straightforward proposition. Most systems can deal with two or more monitors

Power to the Processor
with little or no fuss. Getting hold of a second monitor could be as simple as
grabbing an unused one from the attic or buying a second monitor.
Once you’ve installed the hardware, the software side of building a dual-monitor
system is relatively straightforward. Simply call up the resolution settings in the
control panel or System Preferences and change the resolution settings and
orientation for each monitor. Once you extend your desktop across both
monitors, you should be well on your way.
Those of you who use a laptop can also enjoy many of these beneﬁts by plugging
the second monitor into the video out and following the setup steps that are
recommended by your computer’s operating system. You should be aware that
many laptops are limited in the way they share video memory and might be
restricted in the resolution levels that can be selected.
This might not seem much like a recording tip, but once you get a dual-monitor
system going, your whole approach to producing content (of any type) on a
computer will instantly change and you’ll quickly wonder how you ever got
along without it!
6 Keeping Your Computer Quiet
Noise! Noise! Noise! It’s everywhere! It’s in the streets, in the car, and even in
our studios. It seems like we spend all those bucks getting the best sound
possible, only to gunk it all up by placing this big computer box that’s full of
noisy fans and whirring hard drives smack in the middle of a critical listening
area. Fortunately, a number of companies have begun to ﬁnd ways to reduce
the problem. Here are a few solutions:
n Whenever possible, use larger, low-rpm fans to reduce noise.
n Certain PC motherboards come bundled with a fan speed utility that can
monitor the CPU and case heat and adjust the fan speeds accordingly.
n Route your internal case cables carefully. They could block the ﬂow of air,
which can add to heat and noise problems.
n A growing number of hard-disk drives are available as quiet drives. Check
the manufacturer’s noise ratings.
n You might consider placing the computer in a well-ventilated area, just
outside the production room. Always pay special attention to ventilation
(both inside and outside the computer box), because heat is a killer that’ll
reduce the life span of your CPU. (Note: When building my own studio
I designed a special alcove/glass door enclosure that houses my main
computer—no muss, no fuss, and almost no noise.)
n Thanks to gamers and audio-aware buyers, a number of companies exist
that specialize in quiet computer cases, fans and components. These are
always fun to check out on the web.
258

259
The Digital Audio Workstation  CHAPTER 7
7 Backup, Archive and Networking Strategies
It’s pretty much always true that it’s not a matter of if an irreplaceable hard
drive will fail, but when. At a time that we least expect it, disaster could strike.
It’s our job to be prepared for the inevitable. This type of headache can, of
course, be partially or completely averted by backing up your active program
and media ﬁles, as well as by archiving your previously created sessions and
then making sure that these ﬁles are also backed up.
As previously stated, it’s generally wise to keep your computer’s operating system
and program data on a separate hard disk (usually the boot drive) and then
store your session ﬁles on a separate media drive. Let’s take this as a practical
and important starting point. Beyond this premise, as most of you are quite
aware, the basic rules of hard-disk management are extremely personal, and
will often differ from one computer user to the next (Figure 7.46). Given these
differences, I’d still like to offer up some basic guidelines:
n It’s important to keep your data (of all types) well organized, using a system
that’s both logical and easy to follow. For example, online updates of a
prog ram or hardware driver downloads can be placed into their own direc -
tories; data relating to your studio can be placed in the “studio” directory
and subdirectories; documents, MP3s, and all the trappings of day-to-day
studio operations can be also placed on the disk, using a system that’s easy
to understand.
n Session data should likewise be logical and easy to ﬁnd. Each project should
reside in its own directory and each song should likewise reside in its own
subdirectory of that session project directory.
n Remember to save various take versions of a mix. If you just added the
vocals to a song, go ahead and save the session under a new version name.
This acts as an “undo” function that lets you go back to a speciﬁc point in
a session. The same goes for mixdown versions. If someone likes a particular
mix version or effect, go ahead and save the mix under a new name or
version number (my greatest song 1 ver15.ses) or (my greatest song 1 ver15
favorite effect.ses). In fact, it’s generally wise to save the various versions
throughout the course of the mix. These session ﬁles are usually small and
might save your butt at a later point in time. As a suggestion, you might
want to create a “mix back” subdirectory in the session/song folder and
move the older session ﬁles there, so you don’t end up being confused with
80 backup take names.
FIGURE 7.46
Data and hard-drive
management (along with a
good backup scheme) are
extremely important facets
of media production.

Power to the Processor
With regards to backup strategies, a number of options exist. In this day and
age, hard drives are the most robust and cost-effective ways of backing up your
precious data. Here are some options, although you may have better options
that work for your own application and working scale:
n Primary data storage drive: Drives (in addition to your main OS drive)
that are within your computer can be used to store your primary (master)
data ﬁles. It’s often good to view a speciﬁc drive as a source where all
information is held (Fort Knox) and that all data, sound ﬁles, etc. need
to eventually make it to that drive.
n Backup drive or drives: External drives or Portable high-capacity (2G and
higher) drives can then be used as to back up your program and media
data. The latter portable drives are physically small and can be used with
your laptop and other computers in a straightforward way.
n Off-site backup drive: It’s almost always important to store a backup drive
off-site.
Having a relatively up-to-date backup that’s stored in a bank vault safety box
or at a second place (anywhere safe and secure), can literally save a crucial part
of your personal life in case of theft or ﬁre. Seriously—your hardware can be
replaced, but your data can’t.
n Cloud: It’s slow and cumbersome, but storing important data on another
cloud network can be an effective backup scheme.
All of the above are simply suggestions. I rarely give opinions like these in a book;
however, they’ve served me so well and for so long that I had to pass them along.
COMPUTER NETWORKING
Beyond the concept of connecting external devices to a single computer, a larger
concept hits at the heart of the connectivity age—networking. The ability to set
up and make use of a local area network (LAN) can be extremely useful in the
home, studio and/or ofﬁce, in that it can be used to link multiple computers
with various data, platforms and OS types. In short, a network can be set up
in a number of different ways, with varying degrees of complexity and admin -
istrative levels. There are two common ways that data can be handled over a
LAN (Figure 7.47):
n The ﬁrst is a system whereby the data that’s shared between linked
computers resides on the respective computers and is communicated back
and forth in a decentralized manner.
n The second makes use of a centralized computer (called a server) that uses
an array of high-capacity hard drives to store all of the data that relates to
the everyday production aspects of a facility. Often, such a system will
have a redundant set of drives (RAID) that actually clones the entire system
on a moment-to-moment basis as a safety backup procedure. In larger
facilities where data integrity is highly critical, a set of backup tapes may
be made on a daily basis for extra insurance and archival purposes.
260

261
The Digital Audio Workstation  CHAPTER 7
No matter what level of complexity is involved, some of the more common
uses for working with a network connection include:
n Sharing ﬁles: Within a connected household, studio or business, a LAN
can be used to share virtually anything (ﬁles, sound ﬁles, video images,
etc.) throughout the connected facility. This means that various production
rooms, studios and ofﬁces can simultaneously share and swap data and/or
media ﬁles in a way that’s often transparent to the users.
n Shared Web connection: One handy aspects of using a LAN is the ability to
share an Internet connection over the network from a single, connected
computer or server. The ability to connect from any computer with ease
is just another reason why you should strongly consider wiring your studio
and/or house with LAN connections.
n Archiving and backup: In addition to the beneﬁts of archiving and backing
up data with a server system—even the simplest LAN can be a true lifesaver.
For example, let’s say that we need to make a backup of a session. In this
situation, we could simply run a backup to the main server that’s connected
to the system, and continue working away on our DAW, without
interruption—or the backups could automatically run in the background
after work hours.
n Accessing sound ﬁles and sample libraries: It goes without saying that sound
and sample ﬁles can be easily accessed from any connected computer.
Actually, if you’re wireless, you could go out to the pool, download or
directly access the needed session ﬁles and soak up the sun while working
on your latest project!
On a ﬁnal note, those who are unfamiliar with networking are urged to learn
about this powerful and easy-to-use data distribution and backup system for
your pro or project studio. For a minimal investment in cables, hubs and
educational reading, you might be surprised at the time, trouble and life-saving
beneﬁts that will be almost instantly realized.
FIGURE 7.47
Local area network (LAN)
connections. (a) Data can 
be shared between
independent computers in 
a home or workplace LAN
environment. (b) Computers
or computer terminals may
be connected to a
centralized server, allowing
data to be stored, shared
and distributed from a
central location and/or on
the web.

Power to the Processor
262
8 Session Documentation
Most of us don’t like to deal with housekeeping. But when it comes to recording
and producing a project, documenting the creative process can save your butt
after the session dust has settled—and help make your postproduction life
much easier (you never know when something will be reissued/remixed). So
let’s discuss how to document the details that crop up before, during and after
the session. After all, the project you save might be your own!
DOCUMENTING WITHIN THE DAW
One of the simplest ways to document and improve a session’s workﬂow is to
name a track before you press the record button, because most DAWs will use
that as a basis for the ﬁle name. For example, by naming a track “Jenny’s lead
voc take 5,” most DAWs will automatically save and place the newly recorded
ﬁle into the session as “Jenny’s lead voc take 5.wav” (or .aif). Locating this track
later would be a lot easier than rummaging through sound ﬁles only to ﬁnd
that the one that you want is “Audio018–05.”
Also, make use of your DAW’s notepad (Figure 7.48). Most programs offer a
scratchpad function that lets you ﬁll in information relating to a track or the
overall project; use this to name a speciﬁc synth patch, note the mic used on a
vocal, and include other information that might come in handy after the session’s
speciﬁcs have been long forgotten.
FIGURE 7.48
Cubase/Nuendo Notepad
apps. (Courtesy of Steinberg
Media Technologies GmbH,
a division of Yamaha
Corporation,
www.steinberg.net)
Markers and marker tracks can also come in super-handy. These tracks can alert
us to mix, tempo and other kinds of changes that might be useful in the
production process. I’ll often place the lyrics into a marker track, so I can sing
the track myself without the need for a lead sheet, or to help indicate phrasings
to another singer.

MAKE DOCUMENTATION DIRECTORIES
The next step toward keeping better track of details is to create a “Song Name
Doc” directory within the song’s session, and ﬁll that folder with documents
and ﬁles that relate to the session such as:
n Your contact info
n Song title and basic production notes (composer, lyricist, label, business
and legal contacts)
n Producer, engineer, assistant, mastering engineer, duplication facility, etc.
(with contact info)
n Original and altered tempos, tempo changes, song key, timecode settings,
etc.
n Original lyrics, along with any changes (changed by whom, etc.)
n Additional production notes
n Artist and supporting cast notes (including their roles, musician costs,
address info, etc.)
n Lists of any software versions and plug-in types, as well as any pertinent
settings (you never know if they’ll be available at a future time, and a
description and screenshot might help you to duplicate it within another
app)
n Lists of budget notes and production dates (billing hours, studio rates and
studio addresses—anything that can help you write off the $$$)
n Scans of copyright forms, session contracts, studio contracts and billings
n Anything else that’s even remotely important
In addition, I’ll often take screenshots of some of my more complicated plug-
in settings and place these into this folder. If I have to redo the track later for
some reason, I refer to the screenshot, so I can start reconstruction. Photos or
movie clips can also be helpful in documenting which type of mic, instrument
and speciﬁc placements were used within a setup. You can even use pictures to
document outboard hardware settings and patch arrangements. Composers can
use the “Doc” folder to hold original scratchpad recordings that were captured
on your cell phone or message machine.
Furthermore, a “Song Name Graphics” directory can hold the elements, pictures
and layouts that relate to the project’s artwork . . . “Song Name Business” and
“Project Name artwork” directory, etc. might also come in handy.
9 Accessories and Accessorize
I know it seems like an afterthought, but there’s an ever-growing list of hardware
and travel accessories that can help you to take your portable rig on the road.
Just a small listing includes:
n Laptop backpacks for storing your computer and gear in a safe, fun case
n Pad stands and cases
263
The Digital Audio Workstation  CHAPTER 7

n Instrument cases and covers
n Flexible LED gooseneck lights that let you view your keyboard in the dark
or on-stage
n Laptop DJ stands for raising your laptop above an equipment-packed table
IN CLOSING
At this time, I’d like to refer you to the many helpful pointers in Chapter 22.
I’m doing this in the hope that you’ll read this section twice (at least)—
particularly the discussion on project preparation, session documentation and
backup/archive strategies. I promise that the time will eventually come when
you’ll be glad you did.
264
Power to the Processor

The expression “getting into the groove” of a piece of music often refers to a
feeling that’s derived from the underlying foundation of music: rhythm. With
the introduction and maturation of MIDI and digital audio, new and wondrous
tools have made their way into the mainstream of music production. These
tools (Figure 8.1) can help us to use technology to forge, fold, mutilate and
create compositions that make direct use of rhythm and other building blocks
of music through the use of looping technology. Of course, the cyclic nature of
loops can be repeat-repeat-repetitive in nature, but new toys and technology
and compositional techniques for looping can inject added ﬂexibility, control
and real-time processing into a project in wondrously expressive ways.
FIGURE 8.1
Probably the most widely
used groove tool on the
planet—GarageBandTM for
the Mac and iDevices.
In this chapter we’ll be touching on many of the approaches and software
packages that have evolved (and continue to evolve) into what is one of the
fastest and most accessible facets of personal music production. It’s literally
impossible to hit on all of the ﬁner operational points of these systems; for
that, I’ll rely on your motivation and ingenuity to:
n Download many of the software demos and apps that are readily available
n Delve into their manuals and working tutorials
265
CHAPTER 8
Groove Tools and
Techniques

Groove Tools and Techniques
n Begin to create your own grooves and songs that can then be integrated
into your music or those of collaborators
If you do these three things, you’ll be shocked and astounded as to how much
you’ll learn, and these experiences will directly translate into skills that’ll widen
your production horizons and possibly change your music.
THE BASICS
The basic idea behind groove-based tools rests with tempo matching, the idea
that various rhythms, grooves, pads and any other imaginable sounds of various
tempos, lengths and often musical keys, can be artfully manipulated and crafted
together into a single, working song.
Because groove-based tools often deal with rhythms and cyclic-based measures
that are pulled from various musical sources, the technical factors that need to
be managed are:
n Sync
n Tempo and length
n Time and pitch change techniques
The aspect of sync relates to the fact that the various loops in a groove project
will need to sync up with each other (or with multiple lengths and timings of
each other). It almost goes without saying that multiple loops which are
successively or simultaneously triggered must have a synchronous timing
relationship to one another—otherwise, it’ll all end up being a jumbled mess
of sound.
The next relationship relates to the aspect of tempo. Just as sync is imperative,
it’s also necessary for the ﬁles to be adjusted in length (time stretching), so that
they precisely match the currently selected tempo (or a relative multiple of the
session’s tempo).
A ﬁnal aspect in groove production is associated with time and pitch change
techniques. This is the process of altering a sound ﬁle (often which is rhythmically
repetitive and short in length) to match the current session tempo and to
synchronously align them within the software by using variable sample- and pitch-
shifting techniques. Using these basic digital signal processing (DSP) tools, it’s
possible to alter a sound ﬁle’s duration (varying the length of a program by raising
or lowering its playback sample rate) and/or to alter its relative pitch (either up
or down). In this way, loops can be matched up or music ally combined by using
any of following time and pitch change combinations:
n Time change: A program’s length can be altered without affecting its pitch.
n Pitch change: A program’s length can remain the same while pitch is shifted
either up or down.
n Both: Both a program’s pitch and length can be altered using resampling
techniques.
266

267
Groove Tools and Techniques  CHAPTER 8
By setting the loop program to a master tempo (or a tempo at that point in the
song), an audio segment or ﬁle can be imported, examined as to its sample rate
or length and then recalculated to a new relative tempo that matches the current
session tempo. Voilà! We now have a deﬁned segment of audio that matches
the tempo of all of the other segments in a project, allowing it to play and
interact in relative sync with the other deﬁned segments and/or loop ﬁles.
(Note: more in-depth reading on pitch and time changing can be found in
Chapter 15.)
Along the same line, these pitch-changing techniques can be applied to change
the relative pitch of the loop so that it matches (or best ﬁts into) the musical
key of the session. This ﬁnal key to the musical jigsaw puzzle allows us to mix
and match sounds or loops of various musical keys in ways that otherwise would
never ﬁt together, allowing new and interesting combination of sounds, textures
and rhythms to be created.
Pitch Shift Algorithms
It’s also important to note that time-shifting processes are created according to
a speciﬁc program algorithm. This means that the sound is shaped according
to a set of basic mathematic/programming calculations. More often than not,
the music program will let you choose between a set of algorithms, according
to the type of sound that’s being processed (Figure 8.2). For example, a basic
drum loop might sound really good when a “beats” detection algorithm is
chosen, while a long, slow pad might sound totally unnatural with the same
settings. In short, it’s always a good idea to be aware of the various time shift
options that are available to you and to take the time to make processing
choices that best match the music or selected segment at hand. Additionally,
these algorithms will also vary from one program to program. For example, the
“pad” (a long, continuous series of chords) setting on one looping program
might sound entirely different from that of another DAW or program—just
something to be aware (and possibly take advantage) of.
FIGURE 8.2
Different DAWs will offer
various pitch shift
algorithms that have their
own sonic characteristics.
(a) Ableton Live. (Courtesy 
of Ableton AG,
www.ableton.com) 
(b) Cubase/Nuendo. 
(Courtesy of Steinberg
Media Technologies GmbH,
a division of Yamaha
Corporation,
www.steinberg.net)

The Basics
268
WARPING
In addition to basic pitch- and time-shifting, most looping tools make use of
a tempo and sound processing technique, called warping. This process uses
various time-shift tools to match the timing elements of a sound ﬁle by detecting
and entering hitpoint markers (Figure 8.3) into the sound ﬁle. These markers
are most often automatically detected at percussive transient points, in a way
that makes it easier for the time and pitch shifting process to best match the
speciﬁc ﬁle to the session tempo (most often by adjusting the transient timings,
so they fall directly on the beat and its subdivisions).
FIGURE 8.3
Hitpoint markers can be
used to show and
manipulate events and
percussive transients in 
a sound file. (Courtesy 
of Ableton AG,
www.ableton.com)
In addition to helping the loops better match the session tempo, warp hitpoint
markers can also be manually moved and manipulated to change the basic “feel”
of a loop. For example, various hitpoints can be moved ahead or back in time
give the loop an entirely new feel or swing. In short, take the time to experiment
and learn about how you can shape and create new sounds by using warp and
hitpoint technology, as there are countless online videos on this subject that
are both insightful and fun.
Beat Slicing
In addition to warp’s use of pitch- and time-stretch techniques, another method,
called beat slicing, makes use of an entirely different process to match the length
and timings of a sound ﬁle segment to the session tempo. Rather than changing
the speed and pitch of a sound ﬁle, the beat slicing process actually breaks an
audio ﬁle into a number of small segments by detecting the transient events
within the loop and then automatically placing the new slices at their appropriate
(and easily adjustable) tempo points, according to automatic or user-deﬁnable
sensitivity and detection controls. The process then simply changes the length
and timing elements of a segment by simply adding or subtracting time between
these slices.
The most universally used format for beat slicing is the REX ﬁle (Figure 8.4),
which was originated by Propellerhead. These ﬁles can be found in countless
sample libraries as pre-formatted beat slice loops that can simply be loaded
into most of the currently-available DAWs (although you may need to install
Propellerhead’s free Rex Shared Library in order to import and work with these
loops). If you would like to edit and create your own loops from your own
sound ﬁles, Propellerhead’s ReCycle program can be used.

269
Groove Tools and Techniques  CHAPTER 8
FIGURE 8.4
ReCycle Groove Editing
Software. (Courtesy of
Propellerhead Software,
www.propellerheads.se)
In short, REX ﬁles don’t change the pitch, length or sound quality of the loop,
in order to match the current DAW’s session tempo. Instead, these transient-
detected hit-point slices are compressed or spread out in time to match the
current tempo. When the tempo is increased, the slices are pulled together in
time and simply overlap. When the tempo is slowed, the slices are spread out
in time. Too slow of a tempo will actually cause audible gaps to fall between
the cracks. In order to reduce this effect, a “stretch” algorithm can be used that
doesn’t actually time-stretch the loop ﬁle, but instead uses a small sustain
section within the loop, reverses it and adds it to the end of the ﬁle, essentially
smoothing out the gaps with quite acceptable results. From all of this, it’s easy
to understand why the beat-slicing process often works best on percussive
sounds which have silence between the individual hitpoints.
Audio to MIDI
Another ingenious way to allow your loops to change to match the session tempo,
key or alter in almost any other way is to convert the audio loop into a MIDI
segment. This can be done using an audio-to-MIDI tool that can be found in many
DAWs. Just as all time stretch algorithms are not created equal and can often
sound quite different from one program to the next; audio-to-MIDI algorithms
can yield drastically different results. Likewise, each software package will often
offer different algorithms for different types of audio material (i.e., for converting
harmony, melody, drum or sustained pad passages, as seen in Figure 8.5).
FIGURE 8.5
Certain DAWs are able to
convert an audio track to a
MIDI track. (Courtesy of
Ableton AG,
www.ableton.com)

The Basics
270
Once you get into the process of converting audio to MIDI, you’ll quickly begin
to notice that it can be a very hit-or-miss process. You might get totally lucky
on your ﬁrst try and the musical line will translate perfectly into a MIDI ﬁle
that can be altered in pitch, time, sound (or almost any other imaginable
parameter)—or—you might ﬁnd that it will need to be edited (notes corrected,
added, removed, quantized, etc.) in order for it to be acceptable for the project.
With patience, however, the ability to alter a wimpy bass line into a powerful
growler that perfectly ﬁts your needs is an awesome and ever-changing tool.
As with so many other things we’ve discussed, the sky (and your imagination)
is the limit when it comes to the tricks and techniques that can be used to
match the tempos and various time-/pitch-shift elements between loops, MIDI
and sound ﬁle segments. I strongly urge you to take the time to read the manuals
and watch videos about the various loop and DAW software packages to learn
more about the actual terms and procedures and then put them into practice.
If you take the time, I guarantee that your production skills and your outlook
on these tools will greatly expand.
For the remainder of this chapter, we’ll be looking at several of the more popular
groove tools and toys. This is by no means a complete listing, and I recommend
that you keep reading the various trade magazines, websites, and other resources,
for new and exciting technologies that regularly come onto the market.
GROOVE HARDWARE
Since the “groove” is so important to the foundation of modern dance and music
production in general, groove-related hardware (Figure 8.6) has been around for
a long time and continues to be used by musicians and producers alike. These
instruments (and they truly are instruments) range in form and function from
being strictly drum and percussion in nature (like the vintage Roland TR-808),
to newer groove keyboards that are both rhythmic and melodic in nature, to 
the European models that dive deep into the dance culture (my personal
favorites). These devices not only offer up a wide range of press-’n’-play sounds
and sequences, but can be user programmed to ﬁt the requirements of a special
composition.
FIGURE 8.6
Groove hardware synths. 
(a) Older Roland MC-303
Groovebox. (b) Roland
TR-8 Rhythm Performer.
(Courtesy of Roland
Corporation,
www.roland.com)

One of the best ways to get into the timing ballpark between your DAW, external
groove or other MIDI hardware devices, is through the use of MIDI timing
messages (MIDI Clock). In short, you can select your DAW to output MIDI
timing clock data, while slaving your external groove or MIDI hardware to this
clock (look in your manual for external MIDI clock). In this way, when the
DAW begins playback, MIDI timing messages will instruct the hardware to begin
playing at the proper time and tempo—deﬁnitely a useful feature.
For those who don’t want to deal with an external MIDI hardware clock, there
is another option. With the advent of powerful time- and pitch-shift processing
within most DAWs, the sounds from these hardware devices can be pulled into
a session without too much trouble. For example, a single groove loop (or
multiple loops) could be recorded into a DAW (at a bpm that’s fairly close to
the session’s tempo), edited and then imported into the session, at which time
the loop could be easily stretched into the proper time sync, allowing it to be
looped in sync with the song. Just remember, necessity is the mother of
invention—patience and creativity are probably your most important tools in
the looping process.
GROOVE SOFTWARE
Software is often the place where grooves come to life. This can take many forms,
including looping capabilities that are (to varying degrees) available within most
DAWs, plug-in instruments and various iOS applications. The possibilities are
practically limitless, allowing combinations and manipulation of beats, patterns,
sequences and pads in ways that can be addicting, fun and will help get the
groove moving in your latest music track.
Looping Your DAW
Most digital audio workstations offer various features that make it possible to
incorporate self-produced and/or imported loops grooves into a session. This
can be as simple as making an edit cut on the beat at the beginning and end
of a 4-, 8- or 16-bar segment and then repeating the newly-created loop multiple
times. Alternatively, a sample library loop can also be imported into a session,
where it can be repeated and manipulated in order to ﬁt the song. If the imported
loop is at a different length and tempo than the song’s session, most DAWs can
use time/pitch-shifting techniques to shift the loop length, so that it matches
the session’s current tempo. Said another way, if a session has been set to a
tempo of 115 bpm, and if a 100 bpm loop is imported at a speciﬁc measure,
by simply calling up the DAW’s snap-to-grid and automatic time-stretch
functions, the segment can be time-stretched until it ﬁts snuggly into the session’s
native tempo (Figure 8.7). Once done, the loop can be manually looped and
manipulated to your heart’s content.
As was mentioned, different DAWs and editing systems will have differing ways
of tackling a time-stretch situation, with varying degrees of ease and accuracy.
271
Groove Tools and Techniques  CHAPTER 8

Groove Software
272
It’s often a good idea to listen to the various stretch algorithms (beat, pad, vocal,
etc.) and manually audition each option, so as to get the best possible sound.
Just remember, there are usually no hard and fast rules. With planning, ingenuity
and your manual’s help, you’ll be surprised at the number of ways that a
looping problem can be turned into an interesting sonic and learning
opportunity. When a studio DAW is used in conjunction with groove loop toys,
MIDI hardware and various instrument plug-ins, each of these tools can work
together to play an important role in the never-ending creative process of
producing modern music.
Loop-based Audio Software
Loop-based audio systems are groove-driven DAWs, music programs or plug-
ins that are designed to let you drag and drop pre-recorded or user-created loops
and audio tracks into a graphic production interface. At their basic level, these
programs differ conceptually from their traditional DAW counterpart in that
the pitch- and time-shift architecture is so variable and dynamic that, even after
the basic rhythmic, percussive and melodic grooves have been created their
tempo, track patterns, pitch, session key, etc., can be quickly and easily changed
at any time. With the help of custom, royalty-free loops (available from many
manufacturer and third-party companies), users can quickly and easily
experiment with setting up grooves, backing tracks and creating a sonic ambience
by simply dragging the loops into the program’s main sound ﬁle view, where
they can be arranged, edited, processed, saved and exported.
One of the most interesting aspects of the loop-based editor is its ability to
match the tempo of a specially programmed loop to the tempo of the current
session. Amazingly enough, this process isn’t that difﬁcult to perform, because
the program extracts the length, native tempo and pitch information from the
imported ﬁles and, using the previously-mentioned digital time- and pitch-
change techniques, adjusts the loop to ﬁt the native time and pitch parameters
of the current session. This means that loops of various tempos and musical
keys can be automatically adjusted in length and pitch so as to ﬁt in time with
previously existing loops—just drag, drop and go!
Of course, the graphic user interfaces (GUIs) between looping software editors
and tools can differ greatly. Some layouts use a track-based system that lets you
FIGURE 8.7
Most production
workstations allow a loop or
sound file to be resized to fit
into the session’s tempo.

273
Groove Tools and Techniques  CHAPTER 8
enter or drag a preprogrammed loop ﬁle into a track and then drag it to the
right in a way that repeats the loop. Again, it’s worth stressing that DAW editors
will often include such looping functions that can be either basic in nature
(requiring manual editing and/or sound ﬁle processing) or advanced (having
any number of automated loop functions). Other loop programs make use of
visual objects that can be triggered and combined into a real-time mix by
clicking on an icon or track-based grid.
By far, one of the most popular groove/looping programs is Live from Ableton
(Figure 8.8). Live is an interactive loop-based program that’s capable of
recalculating the time, pitch and tempo structure of a sound ﬁle or easily deﬁned
segment, and then entering that loop into the session at a deﬁned global tempo.
This means that a segment of any length or tempo that’s been pulled into the
session grid will be recalculated to the master tempo and can be combined,
mixed and processed in perfect sync with all other loops in the project session.
FIGURE 8.8
Ableton Live performance
audio workstation: 
(a) Arrangement View; 
(b) Session View. 
(Courtesy of Ableton AG,
www.ableton.com)
In Live’s Arrangement View, as in all traditional sequencing programs, everything
happens along a ﬁxed song timeline, allowing the loops or tracks to be
manipulated in a standard timeline. The program’s Session View, however,
breaks this traditional paradigm by allowing media ﬁles to be mapped onto a
grid as buttons (called clips). Any clip can be played at any time and in any
order in a random fashion that lends itself to interactive performance both in
the studio and on-stage. Functionally, each vertical column, or “track”, can play
only one clip at a time. Any number of “clips”, which are laid out in horizontal
rows, can be played in a loop fashion by clicking on its launch button. By clicking
on a “scene” launch at the screen’s right, every clip in that row will simul -
taneously begin playback. Of course, Live is additionally capable of fully
incorporating MIDI into a project or live interactive performance, along with a
wide range of editing tools, effects and software instruments (both within the
program and giving access to external plug-in instruments and effects).
Another popular groove and production-related tool is Reason from the folks
at Propellerheads. Reason differs from most sound ﬁle-based looping programs
in that it’s an overall music production environment which includes a MIDI

Groove Software
274
sequencer (Figure 8.9a), as well as a wide range of software instrument modules
(Figure 8.9b) that can be played, mixed and integrated into a comprehensive
music production environment that can be controlled from any external
keyboard or MIDI controller. Reason also includes a large number of signal
processors that can be applied to any instrument or instrument group, under
full automation control.
1. Go to www.acidplanet.com and download their
free version of ACID Xpress; or
2. Go to www.ableton.com and download their trial
version of Ableton Live.
3. Download a free set of ACID 8pack set of loops
from www.acidplanet.com (under “free
downloads” select “free 8pack loops” and
download a few of your favorites), or simply use
the included loops within the Abelton demo.
4. Load the individual loops.
5. Read the program’s manual and/or begin to
experiment with the loops.
6. Mess around with the tempo and musical keys.
7. Copy and duplicate the various loop tracks to
your heart’s content until you’ve made a really
fun song!
8. Import some of your own samples and
incorporate them into the song.
9. Save and export the session to play for your
friends!
Try This: Having Fun with a Loop-Based Editor
D I Y
 do  it  yourself
FIGURE 8.9
Reason Music Production
Software: (a) sequencer 
and mixer window; 
(b) instrument and effects
music production
environment. (Courtesy of
Propellerhead Software,
www.propellerheads.se)
In essence, Reason is a combination of modeled representations of vintage
analog synthesis gear, mixed with the latest of digital synthesis and sampling
technology (Figure 8.10). Combine these with a modular approach to signal
and effects processing, add a generous amount of internal and remote mix and
controller management (via external MIDI controllers), top this off with a
quirky yet powerful sequencer, and you have an integrated software package
that is powerful and convenient.

Once you’ve ﬁnished the outline of a track, the obvious idea is to create new
instrument tracks that can be combined in a traditional multitrack building-
block approach, until a song begins to form. Additional sounds, loops and
patches are widely available for sale or for free as “reﬁlls” that can be added to
your collection to greatly expand the software’s palette.
Feel free to download the demo and take it for a spin. You’ll also want to check
out the Reason Basics video clips. Due to its all-in-one production nature, this
program might take a while to master, but the journey just might open you up
to a whole new world of production possibilities.
REWIRE
In Chapter 7, we learned that ReWire is a special protocol that was developed
by Propellerhead Software and Steinberg, which allows audio to be streamed
between two simultaneously running computer applications. Unlike a plug-in,
where a task-speciﬁc application is inserted into a compatible host program,
ReWire allows the audio and timing elements of a supporting program to be
seamlessly integrated into another host program that also supports ReWire. For
example, DAWs such as Cubase/Nuendo and Pro Tools support ReWire, allowing
production environment programs such as Ableton Live and Reason to be
directly “wired” in software into the production inner-workings of a DAW
session. This allows audio to be routed through the DAW’s virtual mixer or I/O,
while making it possible for the timing elements of the client application to be
synchronously controlled by the host DAW. This capability allows for greatly
expanded instrument and production options within a studio or on-stage
environment.
If you feel up to the task, download a few program demos, consult the involved
program manuals, and try it out for yourself. The most important rule to
remember when using ReWire is that the host program should always be 
opened ﬁrst and then the client (the secondary program that’s being ReWired
into the DAW). When shutting down, the client program should always be 
closed ﬁrst.
275
Groove Tools and Techniques  CHAPTER 8
FIGURE 8.10
Examples of Reason’s
software instruments: 
(a) SubTractor polyphonic
synth module; (b) NN-XT
sampler module. (Courtesy
of Propellerhead Software,
www.propellerheads.se)

Groove Software
276
GROOVE AND LOOP-BASED PLUG-INS
It’s a sure bet that for every hardware looping tool, there are far more software
plug-in groove tools and synths (Figure 8.11) that can be inserted into your
DAW. These amazing software wonders often make life easier by:
n Automatically following the session tempo or tempo map
n Allowing I/O routing to plug directly into the DAW (without the need for
ReWiring)
n Making use of the DAW’s automation and external controller capabilities
n Allowing individual or combined groove loops to be easily imported as
audio into a session
FIGURE 8.11
Groove-based Plug-ins: 
(a) Stylus RMX real-time
groove module. (Courtesy 
of Spectrasonics,
www.spectrasonics.net); 
(b) Steinberg’s LoopMash2
Groove plug-in. (Courtesy 
of Steinberg Media
Technologies GmbH, 
a division of Yamaha
Corporation,
www.steinberg.net)
These software instruments come with a wide range of sounds (and/or added
“sound packs”) that can often be edited and effected using an on-screen user
interface, and can often be directly controlled from an external MIDI controller.
DRUM AND DRUM LOOP PLUG-INS
Virtual software drum machines are also part of the present-day landscape and
can be used in a stand-alone, plugged-in or rewired production environment.
These plug-ins (Figure 8.12) are capable of injecting a wide range of groove and
FIGURE 8.12
Virtual software drum
machines: (a) Battery 3
virtual drum and loop
module. (Courtesy of Native
Instruments GmbH, www.
nativeinstruments.com); 
(b) BFD2 acoustic drum
library module. (Courtesy 
of FXpansion,
www.fxpansion.com)

277
Groove Tools and Techniques  CHAPTER 8
sonic spice options into a digital audio and/or MIDI project at the session’s
current tempo, allowing accompaniment patterns that can range from being
simple and non-varying over time, to individually complex instrument parts
that can be meticulously programmed into a session or performed on the ﬂy.
In addition, most of these tools include multiple signal paths that let you route
individual or grouped voices to a speciﬁc mixer or DAW input. This makes it
possible for isolated voices to be individually mixed, panned or processed
(using equalization, effects, etc.) in a session in new and interesting ways.
PULLING LOOPS INTO A DAW SESSION
When dealing with loops in modern-day production, one concept that needs
to be discussed are the various ways that grooves and loops can be managed
within a DAW session—such as:
n It’s certainly possible to ReWire a supported client program in conjunction
with the host program. This, however, will often require special setup,
making sure that the programs are opened in the right order and that each
are opened with the right session ﬁles. You also should be aware that these
applications might use up valuable computer program and memory
resources.
n You might have a groove/loop plug-in inserted into a DAW session (a far
more automated process, that will always open directly within the session).
n One of the better ways for freeing up resources and for creating an actual
ﬁle that can be processed and backed up, is to export the groove/loop as
a sound ﬁle track. This can be done in several ways (although with
forethought, you might come up with a new way that works best for you):
1. The instrument track can be soloed and exported to an audio track in
a contiguous fashion from the beginning “00:00:00:00” of the session
to the end of the instrument’s performance.
2. In the case of a repetitive loop, a deﬁned segment (often of a precise
length of 4, 8, or more bars that occurs on the metric boundaries) can
be selected for export as a sound ﬁle loop. Once exported, it can be
dropped into the session and then looped, processed and easily
manipulated in any number of straightforward ways.
3. In the case of an instrument or groove tool that has multiple parts or
voices, each part can be soloed and exported to its own looping track.
This will give you a far greater degree of control over variations, effects
and any number of DAW parameters than otherwise would be possible
during production and mixdown.
4. If you export a plug-in, loop tool, etc., it’s always a good idea to save
it within the session (and probably disable it, so that it doesn’t take up
resources), or to take a screenshot of the settings and place it in a “docs”
or “pics” folder within the session (should you wish to manually recall
it at a later time).

Pulling Loops into a DAW Session
278
5. It also goes without saying that you’ll want to properly document your
export, just as you would with any recorded track. You might include
the instrument type/name that was used directly within the sound ﬁle
name or within the track’s note area . . . along with any other identifying
information. The confused brain (not to mention lost time and
frustration that you might save if a change is needed or a problem arises
at a later time) will almost certainly be your own.
n Obviously, the best approach to exporting loops and creating a loop library
is to devise a system that makes sense to you and then stick with that
personal organizational system.
GROOVE CONTROLLERS
An increasing number of groove-based software/controller systems (Figure 8.13)
are appearing on the market that control or work as stand-alone and/or plug-
in based groove software systems. These controllers offer both hardware control
over the various software parameters and direct performance playing surfaces
(often mimicking basic drum machine pad control surfaces). Although some
of these controllers are dedicated to playing and manipulating drum kits and
percussion sounds, others allow for sample ﬁle data to be entered and edited
in the system within a virtually unlimited performance environment.
FIGURE 8.13
Groove-based
software/controller systems:
(a) Maschine groove
hardware/software
production system.
(Courtesy of Native
Instruments GmbH,
www.nativeinstruments.
com); (b) Spark Drum
Machine. (Courtesy of
Arturia, www.arturia.com)
In addition to using the hardware surface to directly control the system’s groove
software (and possibly other software offered by that company), many of these
controllers can be used as a controller for other third-party software plug-ins.
For example, a hardware controller might be able to directly control sampler
plug-ins, synths and other drum machine plug-ins. Often, this will take ingenuity
and a willingness to wade your way through the vast number of software
integration options.
DJ Software
In addition to music production software, there are a growing number of
software players, loopers, groovers, effects and digital devices that are available

on the market for the twenty-ﬁrst century digital DJ. These hardware/software
devices make it possible for digital grooves to be created from a laptop, iPad,
controller, specially ﬁtted turntable or digital turntable (jog/scratch CD player).
Using such hardware-/software-based systems, it’s possible to sync, scratch and
perform with vinyl and/or digital media with an unprecedented amount of live
performance and wireless interactivity that can be used on the ﬂoor, onstage or
in the studio (Figure 8.14).
279
Groove Tools and Techniques  CHAPTER 8
FIGURE 8.14
Digital DJ Software: 
(a) Traktor portable laptop
DJ rig. (Courtesy of Native
Instruments GmbH,
www.native-instruments.
com); (b) Serato DJ.
(Courtesy of Serato Audio
Research, www.serato.com)
OBTAINING LOOP FILES FROM THE GREAT 
DIGITAL WELLSPRING
In this day and age, there’s absolutely no shortage of pre-programmed loops
that can be directly imported into a number of DAWs and groove editors. Some
sound ﬁles might need to be manually edited, shaped or programmed to work
with your DAW system, while others can be directly and straightforwardly
entered into any loop-based program. Either way, these ﬁles can be easily
obtained from any number of sources, such as:
n CDs, DVDs or downloads that are included for free with newly purchased
software
n The Web (both free and for purchase)
n Commercial media
n Files within websites, promotions and CDs that are loaded as royalty-free
demo content
n Rolling your own (creating your own loops can add a satisfying and
personal touch to your project)
It’s important to note that at any point during the creation of a composition,
audio and MIDI tracks (such as vocals or played instruments) can be performed
into a looped session in order to give the performance a ﬂuid, alternative and
interesting feel. It’s even possible to record a live instrument into a session with
a deﬁned tempo and then edit these tracks into loops that can be dropped into
the current and future sessions to add a live performance touch.

As with most music technologies, the ﬁeld of looping and laying tracks into a
groove continues to advance and evolve at an amazing rate. It’s almost a sure
bet that your current system will support looping, in one way or another. Take
time to read the manuals, gather up some loops that ﬁt your style and particular
interests and start working them into a session. It might take you some time to
master the art of looping or, then again, you might be a natural Zen master.
Either way, the journey will help your production style grow and always lots
of fun!
280
Obtaining Loop Files

Today, professional and nonprofessional musicians alike are using the language
of the Musical Instrument Digital Interface (MIDI) to perform an expanding 
range of music and automation tasks within audio production, audio for video,
ﬁlm post, stage production, etc. This industry-wide acceptance can, in large part,
be attributed to the cost effectiveness, power and general speed of MIDI pro -
duction. Once a MIDI instrument or device comes into the production picture,
there may be less need (if any at all) to hire outside musicians for a project.
This alluring factor allows a musician/composer to compose, edit and arrange
a piece in an electronic music environment that’s extremely ﬂexible. By this,
I’m not saying that MIDI replaces, or should replace the need for acoustic instru-
ments, microphones and the traditional performance setting. In fact, it’s a
powerful production tool that assists countless musicians to create music and
audio productions in ways that are both innovative and highly personal. In
short, MIDI is all about control, repeatability, ﬂexibility, cost-effective production
power and fun.
The affordable potential for future expansion and increased control over an
integrated production system has spawned the growth of a production industry
that allows an individual to cost effectively realize a full-scale sound production,
not only in his or her own lifetime but in a relatively short time. For example,
much of modern-day ﬁlm composition owes its very existence to MIDI. Before
this technology, composers were forced to create without the beneﬁts of hearing
their work at all or by creating a reduction score that could only be played on
a piano or small ensemble (due to the cost and politics of hiring a full orchestra).
With the help of MIDI, composers can now hear their work in real time, make
any necessary changes, print out the scores and take a full orchestra into the
studio to record the ﬁnal score version. At the other end of the spectrum, MIDI
can be an extremely personal tool that lets us perform, edit and layer synthesized
and/or sampled instruments to create a song that helps us to express ourselves
to the masses—all within the comfort of the home or personal project studio.
The moral of this story is that today’s music industry would look and sound
very different if it weren’t for this powerful, four-letter production word.
281
CHAPTER 9
MIDI and Electronic 
Music Technology

The Power Of MIDI
THE POWER OF MIDI
In everyday use, MIDI can be thought of as both a compositional tool for creating
a scratch pad of sounds, then over time, help sculpt them into a ﬁnal piece. It’s
an awesome compositional environment that, like the digital world, is extremely
chameleon-like in nature.
n It can be used in straightforward ways, whereby sounds and textures are
created, edited, mixed and blended into a composition.
n It can be used in conjunction with groove and looping tools to augment,
control and shape a production in an endless number of ways and in a
wide range of music genres.
n It can be used as a tool for capturing a performance (as a tip, if an instru-
ment in the studio has a MIDI out jack, it’s always wise to record it to a
MIDI track on your DAW). The ability to edit, change a sound or vary
parameters after-the-fact is a helpful luxury that could save, augment and/or
improve the track.
n MIDI, by its very nature is a “re-amp” beast, the ability to change a sound,
instruments, settings and/or parameters in post-production is what MIDI
is all about, baby. You could even play the instrument back in the studio,
turn it up and re-record it acoustically—there are practically no limits.
n The ability to have real-time and post-production control over music and
effects parameters is literally in MIDI’s DNA. Almost every parameter can
be mangled, mutilated and ﬁnessed to ﬁt your wildest dreams—either
during the composition phase or in post-production.
In short, the name of this game is editability, ﬂexibility and individuality. There
are so many ways of approaching and working with MIDI that it’s very personal
in nature. The ways that a system can be set up, the various approaches that
the tools and toys are used to create music and sounds can be extremely
individualistic. How you use your tools to create your own style of music is
literally up to you, both in production and in post-production. That’s the true
beauty of MIDI.
MIDI PRODUCTION ENVIRONMENTS
One of the more powerful aspects of MIDI production is that a system can be
designed to handle a wide range of tasks with a degree of ﬂexibility and ease
that best suits an artist’s main instrument, playing style and even personal
working habits. By opening up almost any industry-related magazine, you’ll
easily see that a vast number of electronic musical instruments, effects devices,
computer systems and other MIDI-related devices are currently available on the
new and used electronic music market. MIDI production systems exist in all
kinds of shapes and sizes and can be incorporated to match a wide range of
production and budget needs. For example, working and aspiring musicians
commonly install digital audio and MIDI systems in their homes (Figure 9.1).
282

283
MIDI and Electronic Music Technology  CHAPTER 9
These production environments range from ones that take up a corner of an
artist’s bedroom to larger systems that are integrated into a dedicated project
studio. Systems such as these can be specially designed to handle a multitude
of applications and have the important advantage of letting artists produce their
music in a comfortable environment—whenever the creative mood hits. Newer,
laptop-based systems allow us to make music “wherever and whenever” from
the comfort of your trusty backpack. Such production luxuries, that would have
literally cost an artist a fortune in the not-too-distant past, are now within the
reach of almost every musician.
In effect, the true power of MIDI lay in its repeatability and ability to offer
control and edit functions during both production and (even more so) after
the fact in post-production. When combined with digital audio workstations
(DAWs) and modern-day recording technology, much of the music production
process can be pre-planned and rehearsed before you even step into the studio.
In fact, it’s not uncommon for recorded tracks to be laid down before they ever
see the hallowed halls of a pro studio (if they see them at all). In business
jargon, this luxury has reduced the number of billable hours to the artist or
label to a cost-effective minimum—this ﬂexibility, editability and affordability
has placed MIDI production and control squarely at the heart of modern-day
music production.
Since its inception, electronic music has been an indispensable tool for the
scoring and audio post-production of television and radio commercials,
industrial videos and full-feature motion picture sound tracks (Figure 9.2). For
FIGURE 9.1
MIDI production rooms: 
(a) gettin’ it all going in the
bedroom studio. (Courtesy
of Steinberg Media
Technologies GmbH, a
division of Yamaha
Corporation,
www.steinberg.net); 
(b) Moonbooter Studio.
(Courtesy of Moonbooter,
www.moonbooter.de, ©
foto by moonbooter.de)
FIGURE 9.2
Skywalker Sound scoring
stage with orchestra, Marin
County, CA. (Courtesy of
Skywalker Sound,
www.skysound.com)

MIDI Production Environments
284
productions that are on a budget, an entire score can be created in the artist’s
project studio using MIDI, hard-disk tracks and digital recorders—all at a mere
fraction of what it might otherwise cost to hire the musicians and rent a studio.
Electronic music production and MIDI are also very much at home on the stage.
In addition to using synths, samplers, DAWs and drum machines on the stage,
most or all of a MIDI instrument and effects device parameters can be controlled
from a pre-sequenced or real-time source. This means that all the necessary
settings for the next song (or section of a song) can be automatically called up
before being played. Once under way, various instrument patch and controller
parameters can also be changed during a live performance from a stomp box
controller, DAW or other hardware controller.
MIDI also falls squarely under the multimedia banner. General MIDI (GM,
which is discussed in greater detail within Chapter 11), is a standardized spec
that allows any sound-card or GM-compatible device to play back a score using
the originally intended sounds and program settings. A General MIDI sequence
can therefore be played on any laptop, tablet or (last but not least) phone for
use with playing back music and effects within multimedia games and websites.
With the integration of the General MIDI standard into various media devices,
one of the fastest growing MIDI applications, surprisingly, is probably
comfortably resting in your pocket or purse right now—the ring tone on your
cell phone (Figure 9.3). The ability to use MIDI (or digital sound ﬁles) to let
you know who is calling has spawned an industry that allows your cell to be
personalized in a super-fun way. One of my favorite ring tone stories happened
on Hollywood Boulevard in L.A. This tall, lanky man was sitting at a café when
his cell phone started blaring out an “If I Only Had a Brain” MIDI sequence
from The Wizard of Oz. It wouldn’t have been nearly as funny if the guy didn’t
look A LOT like the scarecrow character. Of course, everyone laughed.
FIGURE 9.3
One ringy-dingy . . . MIDI
(as well as digital audio)
helps us to reach out and
touch someone through
phone ring tones.
WHAT IS MIDI?
Simply stated, the Musical Instrument Digital Interface (MIDI) is a digital
communications language and compatible speciﬁcation that allows multiple
hardware and software electronic instruments, performance controllers,
computers and other related devices to communicate with each other over a

connected network (Figure 9.4). MIDI is used to translate performance- or
control-related events (such as playing a keyboard, selecting a patch number,
varying a modulation wheel, triggering a staged visual effect, etc.) into equivalent
digital messages and then transmit these messages to other MIDI devices where
they can be used to control sound generators and other performance/control
parameters. The beauty of MIDI is that its data can be recorded into a DAW or
hardware device (known as a sequencer), where it can then be edited and
communicated between electronic instruments or other devices to create music
or control any number of parameters in a performance- or post-production
setting.
In addition to composing and performing a song, musicians can also act as
techno-conductors, having complete control over a wide palette of sounds, their
timbre (sound and tonal quality), overall blend (level, panning) and other real-
time controls. MIDI can also be used to vary the performance and control
parameters of electronic instruments, recording devices, control devices and
signal processors in the studio, on the road or on the stage.
The term interface refers to the actual data communications link and software/
hardware systems in a connected MIDI and digital audio network. Through the
use of MIDI, it’s possible for all of the electronic instruments and devices within
a network to be addressed through the transmission of real-time performance
and control-related MIDI data messages throughout a system to multiple
instruments and devices through one or more data lines (which can be chained
from device to device). This is possible because a single data cable is capable
of transmitting performance and control messages over 16 discrete channels.
This simple fact allows electronic musicians to record, overdub, mix and play
back their performances in a working environment that loosely resembles the
multitrack recording process. Once mastered, MIDI surpasses this analogy by
allowing a composition to be edited, controlled, altered and called up with
complete automation and repeatability—all of this providing production
challenges and possibilities that are well beyond the capabilities of the traditional
tape-based multitrack recording process.
285
MIDI and Electronic Music Technology  CHAPTER 9
FIGURE 9.4
Example of a typical MIDI
system with the MIDI
network connections
highlighted in solid lines.

What MIDI Isn’t
286
FIGURE 9.5
Example of a typical MIDI
system with the audio
connections highlighted in
solid lines.
WHAT MIDI ISN’T
For starters, let’s dispel one of MIDI’s greatest myths: MIDI DOESN’T com -
municate audio, nor can it create sounds! It is strictly a digital language that
instructs a device or program to create, play back or alter the parameters of
sound or control function. It is a data protocol that communicates on/off
triggering and a wide range of parameters to instruct an instrument or device
to generate, reproduce or control audio or production-related functions. Because
of these differences, the MIDI data path is entirely distinct and separate from
the audio signal paths (Figure 9.5). Even when they digitally share the same
transmission cable (such as FireWire, USB or thunderbolt), the actual data paths
and formats are completely separate in structure.
In short, MIDI communicates information that instructs an instrument to play
or a device to carry out a function. It can be likened to the holes in a player-
piano roll; when we put the paper roll up to our ears, we hear nothing, but
when the cut-out dots pass over the sensors on a player piano, the instrument
itself begins to make music. It’s exactly the same with MIDI. A MIDI ﬁle or data
stream is simply a set of instructions that pass down a wire in a serial fashion,
but when an electronic instrument interprets the data, we begin to hear sound.
It’s worth repeating here—of course, the power of
MIDI lies in its ability to a capture performance, but
its real strength squarely rests with the ability to edit
that data, to manipulate it up, down and inside out,
to control a wide range of musical and non-musical
parameters as well as to alter the performance,
individual notes and instrument sounds or
parameters in almost an inﬁnite way—all in the name
of editing ﬂexibility in post-production. That’s where
MIDI shines.

SYSTEM INTERCONNECTIONS
As a data transmission medium, MIDI is unique in the world of sound
production in that it’s able to transmit 16 discrete channels of performance,
controller and timing information over a cable in one direction, using data
densities that are economically small and easy to manage. In this way, it’s
possible for MIDI messages to be communicated from a speciﬁc source (such
as a keyboard or MIDI sequencer) to any number of devices within a connected
network over a single MIDI data chain. In addition, MIDI is ﬂexible enough
that multiple MIDI data lines can be used to interconnect devices in a wide
range of possible system conﬁgurations; for example, multiple MIDI lines can
be used to transmit data to instruments and devices over 32, 48, 128 or more
discrete MIDI channels!
Of course, those of you who are familiar with the concept of MIDI know that,
over the years, the concept of interconnecting electronic instruments and other
devices together has changed. These days, you’re more likely to connect a device
to a computer by using a USB, FireWire, Thunderbolt or network cable than by
using standard MIDI cables. In recent times, these interconnections are made
“under the virtual hood,” where cable connectivity limitations are rarely an issue.
However—it’s still important that we have an understanding of how these data
connections are made “at a basic level,” because these older rules often still
apply within a new connection environment—therefore, I present to you, the
MIDI cabling system.
The MIDI Cable
A MIDI cable (Figure 9.6) consists of a shielded, twisted pair of conductor wires
that has a male 5-pin DIN plug located at each of its ends. The MIDI speciﬁcation
currently uses only three of the ﬁve pins, with pins 4 and 5 being used as
conductors for MIDI data and pin 2 is used to connect the cable’s shield to
equipment ground. Pins 1 and 3 are currently not in use. The cables use twisted
cable and metal shield groundings to reduce outside interference (such as
electrostatic or radio-frequency interference), both of which can serve to distort
or disrupt the transmission of MIDI messages.
287
MIDI and Electronic Music Technology  CHAPTER 9
FIGURE 9.6
The MIDI cable: (a) wiring
diagram; (b) cable
connectors.

System Interconnections
288
n Pin 1 is not used in most cases; however, it can
be used to provide the V– (ground return) of a
MIDI phantom power supply.
n Pin 4 is a MIDI data line.
n Pin 2 is connected to the shield or ground cable,
which protects the signal from radio and
electromagnetic interference.
n Pin 5 is a MIDI data line.
n Pin 3 is not used in most cases; however, it can
be used to provide the +V (+9 to +15V) of a MIDI
phantom power supply.
MIDI Pin Description
MIDI cables come prefabricated in lengths of 2, 6, 10, 20 and 50 feet and can
commonly be obtained from music stores that specialize in MIDI equipment.
To reduce signal degradations and external interference that tends to occur over
extended cable runs, 50 feet is the maximum length speciﬁed by the MIDI spec.
It should be noted, that in modern-day MIDI
production, it’s become increasingly common for
MIDI data to be transmitted throughout a system
network using USB, FireWire, Thunderbolt, network
or Wi-Fi interconnections. Although the data isn’t
transmitted through traditional MIDI cabling, the
data format still adheres to the MIDI protocol.
MIDI PHANTOM POWER
In December 1989, Craig Anderton (musician and audio guru) submitted an
article to EM proposing an idea that provides a standardized 12-V DC power
supply to instruments and MIDI devices directly through pins 1 and 3 of a basic
MIDI cable. Although pins 1 and 3 are technically reserved for possible changes
in future MIDI applications (which never really came about), over the years
several forward-thinking manufacturers (and project enthusiasts) have begun
to implement MIDI phantom power directly into their studio and on-stage
systems.
WIRELESS MIDI
Wireless MIDI transmitters (Figure 9.7) also make it possible for a battery-
operated MIDI guitar, wind controller, etc., to be footloose and fancy free 
on-stage and in the studio. Working at distances of up to 500 feet, these battery-
powered transmitter/receiver systems introduce very low delay latencies and can
be switched over a number of radio channel frequencies. In recent times,
however, dedicated wireless transmitters have given way to instruments, MIDI
interface and portable iOS devices that communicate system-wide via Wi-Fi and

289
MIDI and Electronic Music Technology  CHAPTER 9
network-based systems. The use of iOS devices for inputting controlling and
directly interfacing with MIDI-based systems has expanded into an important
way to wirelessly communicate in the studio or on-stage with relative ease (more
on this in Chapter 10).
MIDI Jacks
MIDI is distributed from device to device using three types of MIDI jacks: 
MIDI In, MIDI Out and MIDI Thru (Figure 9.8a). These three connectors use
5-pin DIN jacks as a way to connect MIDI instruments, devices and computers
into a music or production network system. As a side note, that these ports
(as strictly deﬁned by the MIDI 1.0 spec) are optically isolated to eliminate
possible ground loops that might occur when connecting numerous devices
together.
n MIDI In jack: The MIDI In jack receives messages from an external source
and communicates this performance, control and timing data to the
device’s internal microprocessor, allowing an instrument to be played or
a device to be controlled. More than one MIDI In jack can be designed
into a system to provide for MIDI merging functions or for devices that
can support more than 16 channels (such as a MIDI interface). Other
devices (such as a controller) might not have a MIDI In jack at all.
n MIDI Out jack: The MIDI Out jack is used to transmit MIDI performance,
control messages or SysEx data from one device to another MIDI
instrument or device. More than one MIDI Out jack can be designed into
a system, giving it the advantage of controlling and distributing data over
multiple MIDI paths using more than 16 channels (i.e., 16 channels × N
MIDI port paths).
n MIDI Thru jack: The MIDI Thru jack retransmits an exact copy of the data
that’s being received at the MIDI In jack. This process is important, because
it allows data to pass directly through an instrument or device to the next
device in the MIDI chain (more on this later). Keep in mind that this jack
is used to relay an exact copy of the MIDI In data stream, and isn’t merged
with the data being transmitted from the MIDI Out jack.
FIGURE 9.7
Yamaha UD-BT01 and 
MD-BT01 Bluetooth
Wireless interface for Mac
and iOS devices. (Courtesy
of Yamaha Corporation,
www.yamaha.com)

System Interconnections
290
MIDI ECHO
Certain MIDI devices may not include a MIDI Thru jack at all. Some of these
devices, however, may have the option of switching the MIDI Out between being
an actual MIDI Out jack or a MIDI Echo jack (Figure 9.8b). As with the MIDI
Thru jack, a MIDI Echo option can be used to retransmit an exact copy of any
information that’s received at the MIDI In port and route this data to the MIDI
Out/Echo jack. Unlike a dedicated MIDI Out jack, the MIDI Echo function can
often be selected to merge incoming data with performance data that’s being
generated by the device itself. In this way, more than one controller can be
placed in a MIDI system at one time. Note that, although performance and
timing data can be echoed to a MIDI Out/Echo jack, not all devices are capable
of echoing SysEx data.
FIGURE 9.8
MIDI ports. (a) MIDI In, Out
and Thru ports, showing the
device’s signal path routing.
(b) MIDI echo configuration.
Typical Configurations
Although electronic studio production equipment and setups are rarely alike
(or even similar), there are a number of general rules that make it easy for MIDI
devices to be connected to a functional network. These common conﬁgurations
allow MIDI data to be distributed in the most efﬁcient and understandable
manner possible.
As a primary rule, there are only two valid ways to
connect one MIDI device to another within a MIDI
cable chain (Figure 9.9):
n The MIDI Out jack of a source device (controller
or sequencer/computer) must be connected to
the MIDI In of a second device in the chain.
n The MIDI Thru jack of the second device must be
connected to the MIDI In jack of the third device
in the chain, following this same Thru-to-In
convention until the end of the chain is reached.

291
MIDI and Electronic Music Technology  CHAPTER 9
THE DAISY CHAIN
One of the simplest and most common ways to distribute data throughout a
MIDI system is through a daisy chain. This method relays MIDI data from a
source device (controller or sequencer/computer) to the MIDI In jack of the
next device in the chain (which receives and acts on this data). This next device
then relays an exact copy of the incoming data at its MIDI In jack to its MIDI
Thru jack, which is then relayed to the next MIDI In within the chain, and so
on through the successive devices. In this way, up to 16 channels of MIDI data
can be chained from one device to the next within a connected data network—
and it’s precisely this concept of stringing multiple data line/channels through
a single MIDI line that makes the whole concept work! Let’s try to understand
this system better by looking at a few examples.
Figure 9.10a shows a simple (and common) example of a MIDI daisy chain
whereby data ﬂows from a controller (MIDI Out jack of the source device) to
a synth module (MIDI In jack of the second device in the chain) then, an exact
copy of the data that ﬂows into the second device is then relayed to its MIDI
Thru jack out to another synth (via the MIDI In jack of the third device in the
chain). If our controller is set to transmit on MIDI channel 3, the second synth
in the chain (which is set to channel 2) will ignore the messages and not play,
while the third synth (which is set to channel 3) will be playing its heart out.
The moral of this story is that, although there’s only one connected data line,
a wide range of instruments and channel voices can be played in a surprisingly
large number of combinations—all by using individual channel assignments
along a daisy chain.
Another example (Figure 9.10b) shows how a computer can easily be designated
as the master source within a daisy chain so that a sequencing program can be
used to control the entire playback and channel routing functions of a daisy-
chained system. In this situation, the MIDI data ﬂows from a master controller/
synth to the MIDI In jack of a computer’s MIDI interface (where the data can
be played into, processed and rerouted through a MIDI sequencer). The MIDI
Out of the interface is then routed back to the MIDI In jack of the master
controller/synth (which receives and acts on this data). The controller then relays
an exact copy of this incoming data out to its MIDI Thru jack (which is then
relayed to the next device in the chain) and so on, until the end of the chain
FIGURE 9.9
The two valid means of
connecting one MIDI device
to another.

System Interconnections
292
FIGURE 9.10
Example of a connected
MIDI system using a daisy
chain: (a) typical daisy chain
hookup; (b) example of how
a computer can be
connected into a daisy
chain.
is reached. When we stop and think about it, we can see that the controller is
essentially used as a “performance tool” for entering data into the MIDI
sequencer, which is then used to communicate this data out to the various instru -
ments throughout the connected MIDI chain.
THE MULTIPORT NETWORK
Another common approach to routing MIDI throughout a production system
involves distributing MIDI data through the multiple 2, 4 and 8
In/Out ports that are available on the newer multiport MIDI
interfaces or through the use of multiple MIDI USB interface
devices.
In larger, more complex MIDI systems, a multiport
MIDI network (Figure 9.11) offers several advantages
over a single daisy chain path. One of the most
important is its ability to address devices within a
complex setup that requires more than 16 MIDI
channels. For example, a 2x2 MIDI interface that has two
independent In/Out paths is capable of simultaneously
addressing up to 32 channels (i.e., port A 1–16 and port B 1–16),
whereas an 8x8 port is capable of addressing up to 128 individual MIDI
channels.
NOTE:
Although the distinction
isn’t overly important, you might
want to keep in mind that a MIDI
“port”  is a virtual data path that’s
processed through a computer,
whereas a MIDI “ jack”  is the
physical connection on a
device itself.
FIGURE 9.11
Example of a multiport
network using two MIDI
interfaces.

This type of multiport MIDI network has a number of advantages. As an example,
port A might be dedicated to three instruments that are set to respond to MIDI
channels 1–6, 7 and ﬁnally channel 11, whereas port B might be transmitting
data to two instruments that are responding to channels 1–4 and 510 and port
C might be communicating SysEx MIDI data to and from a MIDI remote
controller to a digital audio workstation (DAW). In this modern age of audio
interfaces, multiport MIDI interfaces and controller devices that are each ﬁtted
with MIDI ports, it’s a simple matter for a computer to route and synchronously
communicate MIDI data throughout the studio in lots of ingenious and cost-
effective ways. As you might remember, many of the newer devices that are
MIDI capable might also be able to talk with the OS and host software vie USB,
thereby reducing the need for a multiport setup.
EXPLORING THE SPEC
MIDI is a speciﬁed data format that must be strictly adhered to by those who
design and manufacture MIDI-equipped instruments and devices. Because the
format is standardized, you don’t have to worry about whether the MIDI output
of one device will be understood by the MIDI in port of a device that’s made
by another manufacturer. As long as the data ports say and/or communicate
MIDI, you can be assured that the data (at least most of the basic performance
functions) will be transmitted and understood by all devices within the
connected system. In this way, the user need only consider the day-to-day
dealings that are involved with using electronic instruments, without having to
be concerned with compatibility between devices.
The MIDI Message
When using a standard MIDI cable, it’s important to remember that data can
only travel in one direction from a single source to a destination (Figure 9.12a).
In order to make two-way communication possible, a second MIDI data line
must be used to communicate data back to the device, either directly or through
the MIDI chain (Figure 9.12b).
293
MIDI and Electronic Music Technology  CHAPTER 9
FIGURE 9.12
MIDI data can only travel in
one direction through a
single MIDI cable: (a) data
transmission from a single
source to a destination; 
(b) two-way data
communication using two
cables.
MIDI digitally communicates musical performance data between devices as a
string of MIDI messages. These messages are made up of groups of 8-bit words
(known as bytes), which are transmitted in a serial fashion (generally at a speed
of 31,250 bits/sec) to convey a series of instructions to one or all MIDI devices
within a system.

Exploring the Spec
294
Only two types of bytes are deﬁned by the MIDI
speciﬁcation: the status byte and the data byte.
n A status byte is used to identify what type of
MIDI function is to be performed by a device or
program. It is also used to encode channel data
(allowing the instruction to be received by a
device that’s set to respond to the selected
channel).
n A data byte is used to associate a value to the
event that’s given by the accompanying status
byte.
Although a byte is made up of 8 bits, the most signiﬁcant bit (MSB; the leftmost
binary bit within a digital word) is used solely to identify the byte type. The MSB
of a status byte is always 1, while the MSB of a data byte is always 0. For example,
a 3-byte MIDI Note-On message (which is used to signal the beginning of a MIDI
note) might read in binary form as a 3-byte Note-On message of (10010100)
(01000000) (01011001). This particular example transmits instructions that
would be read as: “Transmitting a Note-On message over MIDI channel #5, using
keynote #64, with an attack velocity [volume level of a note] of 89.”
MIDI CHANNELS
Just as a public speaker might single out and communicate a message to one
individual in a crowd, MIDI messages can be directed to communicate
information to a speciﬁc device or range of devices within a MIDI system. This
is done by embedding a channel-related nibble (4 bits) within the status/channel
number byte. This process makes it possible for up to 16 channels of
performance or control information to be communicated to a speciﬁc device,
or a sound generator through a single MIDI data cable (Figure 9.13).
Since this nibble is 4 bits wide, up to 16 discrete MIDI channels can be transmitted through a single MIDI cable or
designated port.
0000 = CH#1
0100 = CH#5
1000 = CH#9
1100 = CH#13
0001 = CH#2
0101 = CH#6
1001 = CH#10
1101 = CH#14
0010 = CH#3
0110 = CH#7
1010 = CH#11
1110 = CH#15
0011 = CH#4
0111 = CH#8
1011 = CH#12
1111 = CH#16
FIGURE 9.13
Up to 16 channels can be
communicated through a
single MIDI cable or data
port.

Whenever a MIDI device or sound generator within a device or program function
is instructed to respond to a speciﬁc channel number, it will only react to
messages that are transmitted on that channel (i.e., it ignores channel messages
that are transmitted on any other channel). For example, let’s assume that we’re
going to create a short song using a synthesizer that has a built-in sequencer (a
device or program that’s capable of recording, editing and playing back MIDI
data), a synthesizer and a sampler (Figure 9.14):
1. We could easily start off by recording a simple drum pattern track into
the master synth on channel 10 (numerous synths are pre-assigned to
output drum/percussion sounds on this channel).
2. Once recorded, the sequence will thereafter transmit the notes and data
over channel 10, allowing the synth’s percussion section to be played.
3. Next, we could set the synth module to channel 3 and instruct the master
synth to transmit on the same channel (since the synth module is set to
respond to data on channel 3, its generators will sound whenever the
master keyboard is played). We can now begin recording a melody line
into the sequencer’s next track.
4. Playing back the sequence will then transmit data to both the master synth
(perc section 10) and the module (melody line 3) over their respective
channels. At this point, our song is beginning to take shape.
5. Now we can set the sampler (or any other instrument type) to respond
to channel 5 and instruct the master synth to transmit on that channel,
allowing us to further embellish the song.
6. Now that the song’s beginning to take shape, the sequencer can play the
musical parts to the instruments on their respective MIDI channels—all
in a “multitrack” environment that gives us complete control over voicing,
volume, panning and a wide range of edit functions over each instrument.
In short, we’ve created a true multichannel production environment.
It goes without saying that the above example is just one of the inﬁnite setup
and channel possibilities that can be encountered in a production environment.
It’s often true, however, that even the most complex MIDI and production rooms
will have a system strategy—a basic channel and overall layout that makes the
day-to-day operation of making music easier. This layout and the basic decisions
that you might make in your own room are, of course, up to you. Streamlining
a system to work both efﬁciently and easily will come with time, experience
and practice.
295
MIDI and Electronic Music Technology  CHAPTER 9
FIGURE 9.14
MIDI setup showing a set of
MIDI channel assignments.

Exploring the Spec
296
MIDI MODES
Electronic instruments often vary in the number of sounds and notes that can
be simultaneously produced by their internal sound-generating circuitry. For
example, certain instruments can only produce one note at a single time (known
as a monophonic instrument), while others can generate 16, 32 and even 64
notes at once (these are known as polyphonic instruments). The latter type can
easily play chords or more than one musical line on a single instrument at a
time.
In addition, some instruments are only capable of producing a single generated
sound patch (often referred to as a “voice”) at any one time. Its generating
circuitry could be polyphonic, allowing the player to lay down chords and bass
or melody lines, but it can only produce these notes using a single, characteristic
sound at any one time (e.g., an electric piano, a synth bass or a string patch).
However, the vast majority of newer synths differ from this in that they’re
multitimbral in nature, meaning that they can generate numerous sound patches
at any one time (e.g., an electric piano, a synth bass and a string patch, as can
be seen in Figure 9.15). That’s to say that it’s common to run across electronic
instruments that can simultaneously generate a number of voices, each offering
its own control over a wide range of parameters. Best of all, it’s also common
for different sounds to be assigned to their own MIDI channels, allowing
multiple patches to be internally mixed within the device to a stereo output
bus or independent outputs.
FIGURE 9.15
Multitimbral instruments are
virtual bands-in-a-box that
can simultaneously
generate multiple patches,
each of which can be
assigned to its own MIDI
channel.
The following list and ﬁgures explain the four modes that are supported by the
MIDI spec:
n Mode 1 (Omni On/Poly): In this mode, an instrument will respond to data
that’s being received on any MIDI channel and then redirect this data to
the instrument’s base channel. In essence, the device will play back
everything that’s presented at its input in a polyphonic fashion, regardless
of the incoming channel designations. As you might guess, this mode is
rarely used.
n Mode 2 (Omni On/Mono): As in Mode 1, an instrument will respond to
all data that’s being received at its input, without regard to channel
designations; however, this device will only be able to play one note at a
time. Mode 2 is used even more rarely than Mode 1, as the device can’t
discriminate channel designations and can only play one note at a time.

n Mode 3 (Omni Off/Poly): In this mode, an instrument will only respond
to data that matches its assigned base channel in a polyphonic fashion.
Data that’s assigned to any other channel will be ignored. This mode is
by far the most commonly used, as it allows the voices within a multi -
timbral instrument to be individually controlled by messages that are
being received on their assigned MIDI channels. For example, each of the
16 channels in a MIDI line could be used to independently play each of
the parts in a 16-voice, multitimbral synth.
n Mode 4 (Omni Off/Mono): As with Mode 3, an instrument will be able to
respond to performance data that’s transmitted over a single, dedicated
channel; however, each voice will only be able to generate one MIDI note
at a time. A practical example of this mode is often used in MIDI guitar
systems, where MIDI data is monophonically transmitted over six
consecutive channels (one channel/voice per string).
CHANNEL VOICE MESSAGES
Channel Voice messages are used to transmit real-time performance data
throughout a connected MIDI system. They’re generated whenever a MIDI
instrument’s controller is played, selected or varied by the performer. Examples
of such control changes could be the playing of a keyboard, pressing of program
selection buttons or movement of modulation or pitch wheels. Each Channel
Voice message contains a MIDI channel number within its status byte, meaning
that only devices that are assigned to the same channel number will respond
to these commands. There are seven Channel Voice message types: Note-On,
Note-Off, Polyphonic Key Pressure, Channel Pressure, Program Change, Pitch
Bend Change and Control Change.
n Note-On messages (Figure 9.16): Indicate the beginning of a MIDI note.
This message is generated each time a note is triggered on a keyboard,
drum machine or other MIDI instrument (by pressing a key, striking a
drum pad, etc.). A Note-On message consists of three bytes of information:
a MIDI channel number, a MIDI pitch number, and an attack velocity
value (messages that are used to transmit the individually played volume
levels [0–127] of each note).
n Note-Off messages: Indicate the release (end) of a MIDI note. Each note
played through a Note-On message is sustained until a corresponding
Note-Off message is received. A Note-Off message doesn’t cut off a sound;
it merely stops playing it. If the patch being played has a release (or ﬁnal
decay) stage, it begins that stage upon receiving this message. It should
be noted that many systems will actually use a Note-On message with a
velocity 0 to denote a Note-Off message.
n Polyphonic Key Pressure messages (Figure 9.17): Transmitted by instruments
that can respond to pressure changes applied to the individual keys of a
keyboard. A Polyphonic Key Pressure message consists of three bytes of infor -
mation: a MIDI channel number, a MIDI pitch number and a pressure value.
297
MIDI and Electronic Music Technology  CHAPTER 9

Exploring the Spec
298
n Channel Pressure (or Aftertouch) messages (Figure 9.18): Transmitted and
received by instruments that respond to a single, overall pressure applied
to the keys. In this way, additional pressure on the keys can be assigned
to control such variables as pitch bend, modulation and panning.
n Program Change messages (Figures 9.19 and 9.20): Change the active voice
(generated sound) or preset program number in a MIDI instrument or
device. Using this message format, up to 128 presets (a user- or factory-
deﬁned number that activates a speciﬁc sound-generating patch or system
setup) can be selected. A Program Change message consists of two bytes
of information: a MIDI channel number (1–16) and a program ID number
(0 –127).
n Pitch Bend Change messages (Figures 9.21 and 9.22): Transmitted by an
instrument whenever its pitch bend wheel is moved in either the positive
(raise pitch) or negative (lower pitch) direction from its central (no pitch
bend) position.
n Control Change messages (Figures 9.23 and 9.24): Transmit information
that relates to real-time control over a MIDI instrument’s performance
parameters (such as modulation, main volume, balance and panning).
Three types of real-time controls can be communicated through control
change messages: continuous controllers, which communicate a con -
tinuous range of control settings, generally with values ranging from
0–127; switches (controls having an ON or OFF state with no intermediate
settings); and data controllers, which enter data either through numerical
keypads or stepped up/down entry buttons.
FIGURE 9.16
Byte structure of a MIDI
Note-On message.
FIGURE 9.17
Byte structure of a MIDI
Polyphonic Key Pressure
message (generated when
additional pressure is
applied to each key that’s
played).
FIGURE 9.18
Byte structure of a MIDI
Channel Pressure message
(simultaneously affect all
notes that are transmitted
over a MIDI channel).

299
MIDI and Electronic Music Technology  CHAPTER 9
FIGURE 9.19
Program Change messages
can be used to change
sound patches from a
sequencer or from a remote
controller.
FIGURE 9.20
Workstations and sequencer
software systems will often
allow patches to be recalled
via Program Change
messages. (a) Cubase/
Nuendo. (Courtesy of
Steinberg Media Tech -
nologies GmbH, a division 
of Yamaha Corporation,
www.steinberg.net) 
(b) Protools. (Courtesy of
Digidesign, a division of
Avid Technology,
www.digidesign.com)
FIGURE 9.21
Byte structure of a Pitch
Bend Change message.
FIGURE 9.22
Pitch bend wheel data value
ranges.
FIGURE 9.23
Control Change message
byte structure.
FIGURE 9.24
Control messages can be
varied in real time or under
automation using a number
of input methods.

Exploring the Spec
300
Explanation of Controller ID Parameters
As you can see in Figure 9.23, the second byte of the Control Change message is
used to denote the controller ID number. This all-important value is used to specify
which of the device’s program or performance parameters are to be addressed.
The following section details the general categories and conventions for assigning
controller ID numbers to an associated parameter (as speciﬁed by the 1995
update of the MMA (MIDI Manufacturers Association, www.midi.org). An
overview of these controllers can be seen in Table 9.1. This is deﬁnitely an
important table to earmark, because these numbers will be an important guide
toward knowing and/or ﬁnding the right ID number that can help you on your
path toward ﬁnding that perfect parameter for controlling a variable.
Control #
Parameter
14-Bit Controllers Coarse/MSB (Most Signiﬁcant Bit)
0
Bank Select 0–127 MSB
1
Modulation Wheel or Lever 0–127 MSB
2
Breath Controller 0–127 MSB
3
Undeﬁned 0–127 MSB
4
Foot Controller 0–127 MSB
5
Portamento Time 0–127 MSB
6
Data Entry MSB 0–127 MSB
7
Channel Volume (formerly Main Volume) 0–127 MSB
8
Balance 0–127 MSB
9
Undeﬁned 0–127 MSB
10
Pan 0–127 MSB
11
Expression Controller 0–127 MSB
12
Effect Control 1 0–127 MSB
13
Effect Control 2 0–127 MSB
14
Undeﬁned 0–127 MSB
15
Undeﬁned 0–127 MSB
16–19
General Purpose Controllers 1–4 0–127 MSB
20–31
Undeﬁned 0–127 MSB
14-Bit Controllers Fine/LSB (Least Signiﬁcant Bit)
32
LSB for Control 0 (Bank Select) 0–127 LSB
33
LSB for Control 1 (Modulation Wheel or Lever) 0–127 LSB
Table 9.1
Listing of Controller ID Numbers, Outlining Both the Deﬁned
Format and Conventional Controller Assignments

301
MIDI and Electronic Music Technology  CHAPTER 9
Control #
Parameter
34
LSB for Control 2 (Breath Controller) 0–127 LSB
35
LSB for Control 3 (Undeﬁned) 0–127 LSB
36
LSB for Control 4 (Foot Controller) 0–127 LSB
37
LSB for Control 5 (Portamento Time) 0–127 LSB
38
LSB for Control 6 (Data Entry) 0–127 LSB
39
LSB for Control 7 (Channel Volume, formerly Main Volume) 0–127 
LSB
40
LSB for Control 8 (Balance) 0–127 LSB
41
LSB for Control 9 (Undeﬁned) 0–127 LSB
42
LSB for Control 10 (Pan) 0–127 ISB
43
LSB for Control 11 (Expression Controller) 0–127 LSB
44
LSB for Control 12 (Effect control 1) 0–127 LSB
45
LSB for Control 13 (Effect control 2) 0–127 LSB
46–47
LSB for Control 14–15 (Undeﬁned) 0–127 LSB
48–51
LSB for Control 16–19 (General Purpose Controllers 1–4) 0–127
LSB
52–63
LSB for Control 20–31 (Undeﬁned) 0–127 LSB
7-Bit Controllers
64
Damper Pedal On/Off (Sustain) <63 off, >64 on
65
Portamento On/Off <63 off, >64 on
66
Sustenuto On/Off <63 off, >64 on
67
Soft Pedal On/Off <63 off, >64 on
68
Legato Footswitch <63 Normal, >64 Legato
69
Hold 2 <63 off, >64 on
70
Sound Controller 1 (default: Sound Variation) 0–127 LSB
71
Sound Controller 2 (default: Timbre/Harmonic Intensity) 0–127 
LSB
72
Sound Controller 3 (default: Release Time) 0–127 LSB
73
Sound Controller 4 (default: Attack Time) 0–127 LSB
74
Sound Controller 5 (default: Brightness) 0–127 LSB
75
Sound Controller 6 (default: Decay Time: see MMA RP-021) 
0–127 LSB
76
Sound Controller 7 (default: Vibrato Rate: see MMA RP-021)
0–127 LSB
Table 9.1
continued

Exploring the Spec
302
Control #
Parameter
77
Sound Controller 8 (default: Vibrato Depth: see MMA RP-021)
0–127 LSB
78
Sound Controller 9 (default: Vibrato Delay: see MMA RP-021)
0–127 LSB
79
Sound Controller 10 (default undeﬁned: see MMA RP-021) 
0–127 LSB
80–83
General Purpose Controller 5–8 0–127 LSB
84
Portamento Control 0–127 LSB
85–90
Undeﬁned
91
Effects 1 Depth (default: Reverb Send Level) 0–127 LSB
92
Effects 2 Depth (default: Tremolo Level) 0–127 LSB
93
Effects 3 Depth (default: Chorus Send Level) 0–127 LSB
94
Effects 4 Depth (default: Celesta [Detune] Depth) 0–127 LSB
95
Effects 5 Depth (default: Phaser Depth) 0–127 LSB
Parameter Value Controllers
96
Data Increment (Data Entry +1)
97
Data Decrement (Data Entry –1)
98
Non-Registered Parameter Number (NRPN): LSB 0–127 LSB
99
Non-Registered Parameter Number (NRPN): MSB 0–127 MSB
100
Registered Parameter Number (RPN): LSB* 0–127 LSB
101
Registered Parameter Number (RPN): MSB* 0–127 MSB
102–119
Undeﬁned
Reserved for Channel Mode Messages
120
All Sound Off 0
121
Reset All Controllers
122
Local Control On/Off 0 off, 127 on
123
All Notes Off
124
Omni Mode Off (+ all notes off)
125
Omni Mode On (+ all notes off)
126
Poly Mode On/Off (+ all notes off)
127
Poly Mode On (+ mono off + all notes off)
Table 9.1
continued

System Messages
As the name implies, System messages are globally transmitted to every MIDI
device in the MIDI chain. This is accomplished because MIDI channel numbers
aren’t addressed within the byte structure of a System message. Thus, any device
will respond to these messages, regardless of its MIDI channel assignment. The
three System message types are:
n System Common messages
n System Real-Time messages
n System Exclusive messages
System-Common messages are used to transmit MIDI timecode, song position
pointer, song select, tune request and end-of-exclusive data messages throughout
the MIDI system or 16 channels of a speciﬁed MIDI port.
n MIDI timecode (MTC) messages: Provide a cost-effective and easily
implemented way to translate SMPTE (a standardized synchronization
time-code) into an equivalent code that conforms to the MIDI 1.0 spec.
It allows time-based codes and commands to be distributed throughout
the MIDI chain in a cheap, stable and easy-to-implement way. MTC
Quarter-Frame messages are transmitted and recognized by MIDI devices
that can understand and execute MTC commands. A grouping of eight
quarter frames is used to denote a complete timecode address (in hours,
minutes, seconds, and frames), allowing the SMPTE address to be updated
every two frames. More in-depth coverage of MIDI timecode can be found
in Chapter 12.
n Song Position Pointer (SPP) messages: Allow a sequencer or drum machine
to be synchronized to an external source (such as a tape machine) from
any measure position within a song. This complex timing protocol isn’t
commonly used, because most users and design layouts currently favor
MTC.
n Song Select messages: Use an identifying song ID number to request a
speciﬁc song from a sequence or controller source. After being selected,
the song responds to MIDI Start, Stop and Continue messages.
n Tune Request messages: Used to request that an equipped MIDI instrument
initiate its internal tuning routine.
n End of Exclusive (EOX) messages: Indicate the end of a System-Exclusive
message.
System Real-Time messages provide the precise timing element required to
synchronize all of the MIDI devices in a connected system. To avoid timing
delays, the MIDI speciﬁcation allows System Real-Time messages to be inserted
at any point in the data stream, even between other MIDI messages.
n Timing Clock messages: The MIDI Timing Clock message is transmitted
within the MIDI data stream at various resolution rates. It is used to
synchronize the internal timing clocks of each MIDI device within the
303
MIDI and Electronic Music Technology  CHAPTER 9

Exploring the Spec
304
system and is transmitted in both the start and stop modes at the currently
deﬁned tempo rate. In the early days of MIDI, these rates (which are
measured in pulses per quarter note [ppq]) ranged from 24 to 128 ppq;
however, continued advances in technology have brought these rates up
to 240, 480 or even 960 ppq.
n Start messages: Upon receipt of a timing clock message, the MIDI Start
command instructs all connected MIDI devices to begin playing from their
internal sequences’ initial start point. Should a program be in midsequence,
the start command will reposition the sequence back to its beginning, at
which point it will begin to play.
n Stop messages: Upon receipt of a MIDI Stop command, all devices within
the system will stop playing at their current position point.
n Continue messages: After receiving a MIDI Stop command, a MIDI Continue
message will instruct all connected devices to resume playing their internal
sequences from the precise point at which it was stopped.
n Active Sensing messages: When in the Stop mode, an optional Active
Sensing message can be transmitted throughout the MIDI data stream every
300 milliseconds. This instructs devices that can recognize this message
that they’re still connected to an active MIDI data stream.
n System Reset messages: A System Reset message is manually transmitted in
order to reset a MIDI device or instrument back to its initial power-up
default settings (commonly mode 1, local control on and all notes off).
System-exclusive (sys-ex) messages allow MIDI manufacturers, programmers and
designers to communicate customized MIDI messages between MIDI devices.
The purpose of these messages is to give manufacturers, programmers and
designers the freedom to communicate any device-speciﬁc data of an unrestricted
length, as they see ﬁt. Most commonly, sys-ex data are used for the bulk trans -
mission and reception of program/patch data and sample data, as well as real-
time control over a device’s parameters. The transmission format of a sys-ex
message (Figure 9.25), as deﬁned by the MIDI standard, includes a sys-ex status
header, manufacturer’s ID number, any number of sys-ex data bytes and an
EOX byte. When a sys-ex message is received, the identiﬁcation number is read
by a MIDI device to determine whether or not the following messages are
relevant. This is easily accomplished by the assignment of a unique 1- or 3-byte
ID number to each registered MIDI manufacturer and make. If this number
doesn’t match the receiving MIDI device, the subsequent data bytes will be
FIGURE 9.25
System-exclusive ID data
and controller format.

ignored. Once a valid stream of sys-ex data has been transmitted, a ﬁnal EOX
message is sent, after which the device will again begin to respond normally to
incoming MIDI performance messages.
In actual practice, the general idea behind sys-ex is that it uses MIDI messages
to transmit and receive program, patch and sample data or real-time parameter
information between devices. It’s sort of like having an instrument or device
that’s a musical chameleon. One moment it can be conﬁgured with a certain
set of sound patches and setup data and then, after it receives a new sys-ex data
dump, you could easily end up with an instrument that’s literally full of new
and hopefully exciting sounds and settings. Here are a few examples of how
sys-ex can be put to good use:
n Transmitting patch data between synths: Sys-ex can be used to transmit patch
and overall setup data between synths of identical make and (most often)
model. Let’s say that we have a Brand X Model Z synthesizer, and, as it
turns out, you have a buddy across town that also has a Brand X Model
Z. That’s cool, except your buddy has a completely different set of sound
patches loaded into her synth—and you want them! Sys-ex to the rescue!
All you need to do is go over and transfer the patch data into your synth
(to make life easier, make sure you take the instruction manual along).
n Backing up your current patch data: This can be done by transmitting a sys-
ex dump of your synth’s entire patch and setup data to disk, to a sys-ex
utility program (often shareware) or to your DAW/MIDI sequencer. This
is important: Back up your factory preset or current patch data before attempting
a sys-ex dump! If you forget and download a sys-ex dump, your previous
settings will be lost until you contact the manufacturer, download the
dump from their website or take your synth back to your favorite music
store to reload the data.
n Getting patch data from the Web: One of the biggest repositories of sys-ex
data is on the Internet. To surf the Web for sys-ex patch data, all you need
to do is log on to your favorite search engine and enter the name of your
synth. You’ll probably be amazed at how many hits will come across the
screen, many of which are chock-full of sys-ex dumps that can be
downloaded into your synth.
n Varying sys-ex controller or patch data in real time: Patch editors or hardware
MIDI controllers can be used to vary system and sound-generating
parameters, in real time. Both of these controller types can ease the job
of experimenting with parameter values or changing mix moves by giving
you physical or on-screen controls that are often more intuitive and easier
to deal with than programming electronic instruments that’ll often leave
you dangling in cursor and 3-inch LCD screen hell.
Before moving on, I should also point out that sys-ex data grabbed from the
Web, disk, disc or any other medium will often be encoded using several sys-ex
ﬁle format styles (unfortunately, none of these are standardized). Unfortunately,
sequencer Y might not recognize a sys-ex dump that was encoded using
305
MIDI and Electronic Music Technology  CHAPTER 9

Exploring the Spec
sequencer Z. For this reason, dumps are often encoded using easily available,
standard sys-ex utility programs for the Mac or PC or as a standard MIDI ﬁle.
At last, it seems that a single uniﬁed standard has begun to emerge from the
fray that’s so simple that it’s amazing it wasn’t universally adopted from the
start. This system simply records a sys-ex dump as data on a single MIDI track
within your DAW. Before recording a dump to a DAW MIDI track, you may
need to consult the manual to make sure that sys-ex ﬁltering is turned off. Once
this is done, simply place the track into record mode, initiate the dump and
save the track in your personal sys-ex dump directory. Using this approach, it
would also be possible to:
n Import the appropriate sys-ex dump track (or set of tracks) into the current
working session so as to automatically program the instruments before
the sequence is played back.
n Import the appropriate sys-ex dump track (or set of tracks) into separate
MIDI tracks that can be muted or unassigned. Should the need arise, the
track(s) can be activated and/or assigned in order to dump the data into
the appropriate instruments.
MIDI AND THE COMPUTER
Besides the coveted place of honor in which most electronic musicians hold
their instruments, the most important device in a MIDI system is undoubtedly
the personal computer. Through the use of software programs and peripheral
hardware, the computer is often used to control, process and distribute
information relating to music performance and production from a centralized,
integrated control position.
Of course, two computer types dominate modern-day music production: the
PC and the Mac. In truth, each brings its own particular set of advantages and
disadvantages to personal computing, although their differences have greatly
dwindled over the years. My personal take on the matter (a subject that’s not
even important enough to debate) is that it’s a dual-platform world. The choice
is yours and yours alone to make. Many professional software and hardware
systems can work on either platform. As I write this, some of my music
collaborators are fully Mac, some are PC and some (like me) use both, and it
doesn’t affect our production styles at all. Coexistence isn’t much of a problem,
either. Living a dual-platform existence can give you the edge of being familiar
with both systems, which can be downright handy in a sticky production pinch.
Connecting to the Peripheral World
An important event in the evolution of personal computing has been the
maturation of hardware and processing peripherals. With the development 
of the USB (www.usb.org), FireWire (www.1394ta.org), Thunderbolt (www.
thunderbolttechnology.net) and Dante (www.audinate.com) protocols, hard -
ware devices such as mice, keyboards, cameras, audio interfaces, MIDI interfaces,
306

307
MIDI and Electronic Music Technology  CHAPTER 9
CD and hard drives, MP3 players and even portable fans can be plugged into
an available port without any need to change frustrating hardware settings or
open up the box. External peripherals are generally hardware devices that are
designed to do a speciﬁc task or range of production tasks. For example, an
audio interface is capable of translating analog audio (and often MIDI, control
and other media) into digital data that can be understood by the computer.
Other peripheral devices can perform such useful functions as printing, media
interfacing (video and MIDI), scanning, memory card interfacing, portable hard
disk storage—the list could ﬁll pages.
THE MIDI INTERFACE
Although computers and electronic instruments both communicate using the
digital language of 1’s and 0’s, computers simply can’t understand the language
of MIDI without the use of a device that translates these serial messages into a
data structure that computers can comprehend. Such a device is known as the
MIDI interface. A wide range of MIDI interfaces currently exist that can be used
with most computer system and OS platforms. For the casual and professional
musician, interfacing MIDI into a production system can be done in a number
of ways. Probably the most common way to access MIDI In, Out and Thru jacks
is on a modern-day USB or FireWire audio interface or controller surface (Figure
9.26), although they usually only offer up a single I/O port.
FIGURE 9.26
Many audio interface
devices include MIDI I/O
ports. (Courtesy of Native
Instruments GmbH,
www.native-
instruments.com)
The next option is to choose a USB MIDI interface that can range from simpler
devices that include a single port to multiple-port systems that can easily handle
up to 64 channels over four I/O ports. The multiport MIDI interface (Figure
9.27) is often the device of choice for most professional electronic musicians
who require added routing and synchronization capabilities. These USB devices
can easily be ganged together to provide eight or more independent MIDI Ins
and Outs to distribute MIDI data through separate lines over a connected
network.
In addition to distributing MIDI data, these systems often include driver software
that can route and process MIDI data throughout the MIDI network. For
example, a multiport interface could be used to merge together several MIDI
Ins (or Outs) into a single data stream, ﬁlter out speciﬁc MIDI message types
(used to block out unwanted commands that might adversely change an

MIDI and the Computer
308
instrument’s sound or performance) or rechannel data being transmitted on
one MIDI channel or port to another channel or port (thereby allowing the
data to be recognized by an instrument or device).
Another important function that can be handled by some multiport interfaces
is synchronization. Synchronization (sync, for short) allows other, external
devices (such as DAWs, video decks and other media systems) to be simul -
taneously played back using the same timing reference. Interfaces that includes
sync features will often read and write SMPTE timecode, convert SMPTE to MIDI
time-code (MTC) and allow recorded timecode signals to be cleaned up when
copying code from one analog device to another (jam sync). Further reading
on synchronization can be found in Chapter 12.
In addition to the above interface types, a number of MIDI keyboard controllers
and synth instruments have been designed with MIDI ports and jacks built right
into them. For those getting started, this useful and cost-saving feature makes it
easy to integrate your existing instruments into your DAW and sequencing
environment.
ELECTRONIC INSTRUMENTS
Since their inception in the early 1980s (www.midi.org/articles/the-history-
of-midi), MIDI-based electronic instruments have played a central and im-
portant role in the development of music technology and production. These
devices (which fall into almost every instrument category), along with the
advent of cost-effective analog and digital audio recording systems, have prob -
ably been the most important technological advances to shape the industry into
what it is today. In fact, the combination of hardware and newer software plug-
in technologies has turned the personal project studio into one of the most
important driving forces behind modern-day music production.
Inside the Toys
Although electronic instruments often differ from one another in looks, form
and function, they almost always share a common set of basic building block
components, including the following:
n Central processing units (CPUs): CPUs are one or more dedicated computing
devices (often in the form of a specially manufactured microprocessor chip)
that contain all of the necessary instructional brains to control the
hardware, voice data and sound-generating capabilities of the entire instru -
ment or device.
n Performance controllers: These include such interface devices as music key -
boards, knobs, buttons, drum pads and/or wind controllers for inputting 
FIGURE 9.27
iConnect MIDI4plus 4x4 
(64 channel) MIDI interface.
(Courtesy of iConnectivity,
www.iConnectivity.com)

performance data directly into the electronic instrument in real time or
for transforming a performance into MIDI messages. Not all instru ments
have a built-in controller. These devices (commonly known as modules)
contain all the necessary processing and sound-generating circuitry;
however, the idea is to save space in a cramped studio by eliminating
redundant keyboards or other controller surfaces.
n Control panel: The control panel is the all-important human interface of
data entry controls and display panels that let you select and edit sounds
and route and mix output signals, as well as control the instrument’s basic
operating functions.
n Memory: Digital memory is used for storing important internal data (such
as patch information, setup conﬁgurations and/or digital waveform data).
This digital data can be encoded in the form of either read-only memory
(ROM; data that can only be retrieved from a factory-encoded chip,
cartridge, or CD/DVD-ROM) or random access memory (RAM; memory
that can be read from and stored to a device’s resident memory, cartridge,
hard disk or recordable media).
n Voice circuitry: Depending on the device type, this section can chain together
digital processing “blocks” to either generate sounds (voices) or process
and reproduce digital samples that are recorded into memory for playback
according to a speciﬁc set of parameters. In short, it’s used to generate or
reproduce a sound patch, which can then be processed, ampliﬁed and
heard via speakers or headphones.
n Auxiliary controllers: These are external controlling devices that can be used
in conjunction with an instrument or controller. Examples of these include
foot pedals (providing continuous-controller data), breath controllers,
and pitch-bend or modulation wheels. Some of these controllers are
continuous in nature, while others exist as a switching function that can
be turned on and off. Examples of the latter include sustain pedals and
vibrato switches.
n MIDI communications ports: These data ports and physical jacks are used
to transmit and/or receive MIDI data.
Generally, no direct link is made between each of these functional blocks; the
data from each of these components is routed and processed through the
instrument’s CPU. For example, should you wish to select a certain sound patch
from the instrument’s control panel, the control panel could be used to instruct
the CPU to recall all of the waveform and sound-patch parameters from memory
that are associated with the particular sound. These instructional parameters
would then be used to modify the internal voice circuitry, so that when a key
on the keyboard is pressed or a MIDI Note-On message is received, the sound
generators will output the desired patch’s note and level values.
For the remainder of this section, we’ll be discussing the various types of MIDI
instruments and controller devices that are currently available on the market.
These instruments can be grouped into such categories as keyboards, percussion,
MIDI guitars and controlling devices.
309
MIDI and Electronic Music Technology  CHAPTER 9

Electronic Instruments
INSTRUMENT AND SYSTEMS PLUG-INS
Of course, one of the wonderful things about living in the digital age is that
many (if not most) of our new toys aren’t hardware at all—they exist as soft-
ware synths, samplers, effects, manglers and musical toys of all types, features
and genres. These systems (which can exist as an instrument or effects plug-in)
include all known types of synths, samplers and pitch- and sound-altering
devices that are capable of communicating MIDI, audio, timing sync and 
control data between the software instrument/effect plug-in and a host DAW
program.
Using an established plug-in communications protocol, it’s possible for most
or all of the audio and timing data to be routed through the host audio appli -
cation, allowing the instrument or application I/O, timing and control
parameters to be seamlessly integrated into the DAW or application. A few of
these protocols include VST, AU, MAS, AudioSuite, RTAS.
ReWire is another type of protocol that allows audio, performance and control
data of an independent audio program to be wired into a host program (usually
a DAW) such that the audio routing and sync timing of the slave program is
locked to the host DAW, effectively allowing them to work in tandem as a single
production environment. Further reading on plug-in protocols and virtual and
plug-in instruments can be found in Chapter 7.
Keyboards
By far, the most common instruments that you’ll encounter in almost any MIDI
production facility will probably belong to the keyboard family. This is due, in
part, to the fact that keyboards were the ﬁrst electronic music devices to gain
wide acceptance; also, MIDI was initially developed to record and control many
of their performance and control parameters. The two basic keyboard-based
instruments are the synthesizer and the digital sampler.
THE SYNTH
A synthesizer (or synth) is an electronic instrument that uses multiple sound
generators, ﬁlters and oscillator blocks to create complex waveforms that can
be combined into countless sonic variations. These synthesized sounds have
become a basic staple of modern music and range from those that sound
“cheesy” to ones that realistically mimic traditional instruments—and all the
way to those that generate otherworldly, ethereal sounds that literally defy
classiﬁcation.
Synthesizers generate sounds using a number of different technologies or
program algorithms. Examples of these include:
n FM synthesis: This technique generally makes use of at least two signal
generators (commonly referred to as “operators”) to create and modify a
voice. It often does this by generating a signal that modulates or changes
310

311
MIDI and Electronic Music Technology  CHAPTER 9
the tonal and amplitude characteristics of a base carrier signal. More
sophisticated FM synths use up to four or six operators per voice, each
using ﬁlters and variable ampliﬁer types to alter a signal’s characteristics.
n Wavetable synthesis: This technique works by storing small segments of
digitally sampled sound into memory media. Various sample-based and
synthesis techniques make use of looping, mathematical interpolation,
pitch shifting and digital ﬁltering to create extended and richly textured
sounds that use a surprisingly small amount of sample memory, allowing
hundreds if not thousands of samples and sound variations to be stored
in a single device or program.
n Additive synthesis: This technique makes use of combined waveforms that
are generated, mixed and varied in level over time to create new timbres
that are composed of multiple and complex harmonics. Subtractive
synthesis makes extensive use of ﬁltering to alter and subtract overtones
from a generated waveform (or series of waveforms).
Of course, synths come in all shapes and sizes and use a wide range of patented
synthesis techniques for generating and shaping complex waveforms, in a
polyphonic fashion using 16, 32 or even 64 simultaneous voices (Figures 9.28
and 9.29). In addition, many synths often include a percussion section that can
play a full range of drum and “perc” sounds, in a number of styles. Reverb and
other basic effects are also commonly built into the architecture of these devices,
reducing the need for using extensive outboard effects when being played on-
stage or out of the box. Speaking of “out of the box,” a number of synth systems
are referred to as being “workstations.” Such beasties are designed (at least in
theory) to handle many of your basic production needs (including basic sound
generation, MIDI sequencing, effects, etc.) all in one neat little package.
FIGURE 9.28
Hardware synths. 
(a) Mopho x4 4-voice analog
synth. (Courtesy of Dave
Smith Instruments,
www.davesmithinstruments
.com) (b) Fantom-X7
Workstation Keyboard.
(Courtesy of Roland
Corporation US,
www.rolandus.com)
FIGURE 9.29
Software synthesizers. 
(a) Absynth 5. (Courtesy 
of Native Instruments
GmbH, www.native-
instruments.com) 
(b) Omnisphere. 
(Courtesy of Spectrasonics,
www.spectrasonics.net)

Electronic Instruments
312
Samplers
A sampler (Figures 9.30 and 9.31) is a device that can convert audio into a
digital form that is then imported into, manipulated and output from internal
random access memory (RAM).
FIGURE 9.30
Hardware samplers. 
(a) Akai MPC1000 Music
Production Center. (Courtesy
of Akai Professional,
www.akaipro.com) 
(b) Fantom-XR sampler.
(Courtesy of Roland
Corporation US,
www.rolandus.com)
FIGURE 9.31
Software samplers. (a)
Kontact Virtual Sampler.
(Courtesy of Native
Instruments GmbH,
www.native-
instruments.com) (b)
Ableton Simpler. (Courtesy
of Ableton AG,
www.ableton.com) (c)
Reason’s NN-19 sample
module. (Courtesy of
Propellerhead Software,
www.propellerheads.se)
Once audio has been sampled or loaded into RAM, segments of audio can then
be edited, transposed, processed and played in a polyphonic, musical fashion.
Additionally, signal processing capabilities, such as basic editing, looping, gain
changing, reverse, sample-rate conversion, pitch change and digital mixing can
also be easily applied to:
n Edit and loop sounds into a usable form
n Vary and modulate envelope parameters (e.g., dynamics over time)
n Apply ﬁltering to alter the shape and feel of the sound.
n Vary processing and playback parameters
A sample can then be played back according to the standard Western musical
scale (or any other scale, for that matter) by altering the reproduced sample
rate over the controller’s note range. In simple terms, a sample can be imported
into the device, where it can be mapped (assigned) to a range of keys (or just
a single key) on the keyboard. When the root key is played, the sample will
playback at its original pitch, however, when notes above or below that key are
played, they will be transposed upwards or downwards in a musical fashion.

By choosing the proper sample-rate ratios, these sounds can be polyphonically
played (whereby multiple notes are sounded at once) at pitches that correspond
to standard musical chords and intervals.
A sampler (or synth) with a speciﬁc number of voices (e.g., 64 voices) simply
means that up to 64 notes can be simultaneously played on a keyboard at any
one time. Each sample in a multiple-voice system can be assigned across a
performance keyboard, using a process known as splitting or mapping. In this
way, a sound can be assigned to play across the performance surface of a
controller over a range of notes, known as a zone (Figure 9.32). In addition to
grouping samples into various zones, velocity can enter into the equation by
allowing multiple samples to be layered across the same keys of a controller,
according to how soft or hard they are played. For example, a single key might
be layered so that pressing the key lightly would reproduce a softly recorded
sample, while pressing it harder would produce a louder sample with a sharp,
percussive attack. In this way, mapping can be used to create a more realistic
instrument or wild set of soundscapes that change not only with the played
keys but with different velocities as well. Most samplers have extensive edit
capabilities that allow the sounds to be modiﬁed in much the same way as a
synthesizer, using such modiﬁers as:
n Velocity
n Panning
n Expression (modulation and user control variations)
n Low-frequency oscillation (LFO)
n Attack, delay, sustain and release (ADSR) and other envelope processing
parameters
n Keyboard scaling
n Aftertouch
313
MIDI and Electronic Music Technology  CHAPTER 9
FIGURE 9.32
Example of a sampler’s
keyboard layout that has
been programmed to
include zones. Notice that
the upper register has been
split into several zones that
are triggered by varying
velocities.
Many sampling systems will often include such features as integrated signal
processing, multiple outputs (offering isolated channel outputs for added live
mixing and signal processing power or for recording individual voices to a
multitrack recording system) and integrated MIDI sequencing capabilities.

Electronic Instruments
SAMPLE LIBRARIES AND DIY SAMPLING
Just as patch data in the form of sys-ex dump ﬁles can have the effect of breathing
new life into your synth, a wide range of free or commercially available samples
are commonly available online or as a purchase package that lets you experiment
with loading new and fresh sounds into your production system. These ﬁles
can exist as unedited sound ﬁle data (which can be imported into any sample
system or DAW track), or as data that has been speciﬁcally programmed by a
professional musician/programmer to contain all the necessary loops, system
commands and sound-generating parameters, so that all you ideally need to do
is load the sample and start having fun.
The mind boggles at the range of styles and production quality that has gone
into producing samples that are just ready and waiting to give your project a
boost. The general production level literally runs the entire amateur-to-pro
gamut—meaning that, whenever possible, it’s wise to listen to examples to
determine their quality and to hear how they might ﬁt into your own personal
or project style before you buy. As a ﬁnal caveat, by now, you’ve probably heard
of the legal battles that have been raging over sampled passages that have been
“ripped” from recordings of established artists. In the fall of 2004, in the case
of Bridgeport Music et al. v. Dimension Films, the 6th Circuit U.S. Court of Appeals
ruled that the digital sampling of a recording without a license is a violation
of copyright, regardless of size or signiﬁcance. This points to the need for tender
loving care when lifting samples off a record, CD or the Web.
It goes without saying that a great deal of satisfaction and individual artistry
can be gained by making your own samples. Lifting sounds from your own
instruments, music projects or simply walking around the house and recording
“things” with a handheld recorder can be super fun and interesting. Editing
them and then importing them into a software sampler is far easier than it used
to be with hardware samplers and playing the sounds of that Tibetan bell that
you recorded in a cabin in the mountains of Austria can help make your music
more personal. Get out, start recording and importing your own sounds and
have fun!
The MIDI Keyboard Controller
As computers, sound modules, virtual software instruments and other types of
digital devices have come onto the production scene, it’s been interesting to
note that fewer and fewer instruments are being made that include a music
keyboard in their design. As a result, the MIDI keyboard controller (Figures 9.33
and 9.34) has gained in popularity as a device that might include a:
n Music keyboard surface
n Variable parameter controls
n Fader, mixing and transport controls
n Switching controls
n Trigger pads
314

315
MIDI and Electronic Music Technology  CHAPTER 9
FIGURE 9.33
MIDI keyboard controller.
(Courtesy of M-Audio, a
division of Avid Technology,
Inc., www.m-audio.com).
As was stated, these devices contain no internal tone generators or sound-
producing elements. Instead they can be used in the studio or on the road as
a simple and straightforward surface for handling MIDI performance, control
and device-switching events in real time.
As you might imagine, controllers vary widely in the number of features that
are offered. For starters, the number of keys can vary from the sporty, portable
25-key models to those having 49 and 61 keys and all the way to the full 88-
key models that can play the entire range of a full-size grand piano. The keys
may be fully or partially weighted and in a number of models the keys might
be much smaller than the full piano key size—often making a performance a
bit difﬁcult. Beyond the standard pitch and modulation wheels (or similar-type
controller), the number of options and general features are up to the manu -
facturers. With the increased need for control over electronic instruments and
music production systems, many model types offer up a wide range of physical
controllers for varying an ever-widening range of expressive parameters.
One of the wonders of using MIDI to directly control any number of devices
via MIDI control messages is that the overall communication between the host
and the controller device is open to being easily conﬁgurable by the user. A
controller message can easily be paired to a physical knob, button or other
controller input type by a simple “learn” command. A few companies go a step
further to make assignments between devices, plug-ins and software instruments
FIGURE 9.34
MIDI keyboard controller.
(Courtesy of Native
Instruments GmbH,
www.native-
instruments.com)

Electronic Instruments
316
A MIDI controller is a device that’s expressly
designed to control other devices (be they for
sound, light or mechanical control) within a
connected MIDI system. As was previously
mentioned, these devices contain no internal tone
generators or sound-producing elements but often
include a high-quality control surface and a wide
range of controls for handling control, trigger and
device-switching events. Since controllers have
become an integral part of music production and are
available in many incarnations to control and
emulate many types of musical instrument types,
don’t be surprised to ﬁnd controllers of various
incarnations popping up all over this book for 
both recording and electronic music production
applications.
A Word about Controllers
that much easier to control and operate, by creating a uniﬁed system for
automatically “mapping” a controller’s hardware directly to the software’s
parameters in an easy to use, preprogrammed fashion.
One such system is AutoMap from Novation. This system allows a music or
hardware controller to directly communicate with a wide range of effects and
music-related plug-ins, allowing the controllers hardware surface to directly and
automatically map to the selected plug-in (Figure 9.35a). Another evolving
standard offered by Native Instruments is called the Native Kontrol Standard
(NKS for short). NKS allows for direct control over a growing number of software
instruments and plug-ins when working within Native Instrument’s Komplete
10 under the control of a Komplete Kontrol or Maschine hardware controller
(Figure 9.35b).
FIGURE 9.35
MIDI controller mapping 
to a software plug-in or
instrument. (a) Novation’s
Nocturn automatically 
maps its controls to various
parameters on the target
plug-in. (Courtesy of
Novation Digital Music
Systems, Ltd.,
www.novationmusic.com)
(b) Native Instruments
Komplete Kontrol maps its
hardware controls to a
growing range of
instruments and plug-ins.
(Courtesy of Native
Instruments GmbH,
www.native-
instruments.com)

317
MIDI and Electronic Music Technology  CHAPTER 9
Percussion Instruments and Controllers
Historically speaking, one of the ﬁrst applications of sample technology made
use of digital audio to record and play back drum and percussion sounds. Out
of this virtual miracle sprang a major class of sample and synthesis technology
that lets an artist (mostly keyboard players) add drum and percussion to their
own compositions with relative ease. Over the years, MIDI has brought sampled
percussion within the grasp of every electronic musician, whatever skill level,
from the frustrated drummer to professional percussionist/programmers—all
of whom use their skills to perform live and/or to sequenced compositions.
THE DRUM MACHINE
In its most basic form, the drum machine uses ROM-based, prerecorded
waveform samples to reproduce high-quality drum sounds from its internal
memory. These factory-loaded sounds often include a wide assortment of drum
sets, percussion sets and rare, wacky percussion hits, and effected drum sets
(e.g., reverberated, gated). Who knows, you might even encounter scream hits
by the venerable King of Soul, James Brown. These pre-recorded samples can
be assigned to a series of playable keypads that are generally located on the
machine’s top face, providing a straightforward controller surface that often
sports velocity and aftertouch dynamics. Sampled voices can be assigned to each
pad and edited using control parameters such as tuning, level, output assignment
and panning position.
Because of new cost-effective technology, many drum machines (Figure 9.36)
now include basic sampling technology, which allows sounds to be imported,
edited and triggered directly from the box. As with the traditional “beat box,”
these samples can be easily mapped and played from the traditionally styled
surface trigger pads. Of course, virtual software drum and groove machines are
part of the present-day landscape and can be used in a standalone, plug-in and
rewired production environment.
FIGURE 9.36
Examples of drum
machines. (a) Alesis 
SR-18 stereo drum
machine. (Courtesy of
Alesis, www.alesis.com) 
(b) Roland 7X7-TR-8
Rhythm Performer.
(Courtesy of Roland
Corporation US,
www.rolandus.com)

Electronic Instruments
318
MIDI DRUM CONTROLLERS
MIDI drum controllers are used to translate the voicing and expressiveness of
a percussion performance into MIDI data. These devices are great for capturing
the feel of a live performance, while giving you the ﬂexibility of automating or
sequencing a live event. These devices range from having larger pads and trigger
points on a larger performance surface to drum-machine-type pads/buttons.
Since the later type of controller pads are generally too small and not durable
enough to withstand drumsticks or mallets, they’re generally played with the
ﬁngers. A few of the many ways to perform and sequence percussion include:
n Drum machine button pads: One of the most straightforward of all drum
controllers is the drum button pad design that’s built into most drum
machines, portable percussion controllers (Figure 9.37), iOS devices and
certain keyboard controllers. By calling up the desired setup and voice
parameters, these small footprint triggers let you go about the business of
using your ﬁngers to do the walking through a performance or sequenced
track.
FIGURE 9.37
Portable drum controllers.
(a) Akai LPD8 portable
USB Drum Controller.
(Courtesy Akai Professional,
LP. www.akaipro.com)
(b) Alesis DM Dock iPad
based drum module.
(Courtesy of Alesis,
www.alesis.com)
n The keyboard as a percussion controller: Since drum machines respond to
external MIDI data, probably the most commonly used device for triggering
percussion and drum voices is a standard MIDI keyboard controller. One
advantage of playing percussion sounds from a keyboard is that sounds
can be triggered more quickly because the playing surface is designed for
fast ﬁnger movements and doesn’t require full hand/wrist motions.
Another advantage is its ability to express velocity over the entire range
of possible values (0–127), instead of the limited number of velocity steps
that are available on certain drum pad models.
n Drum pad controllers: In more advanced MIDI project studios or live stage
rigs, it’s often necessary for a percussionist to have access to a playing
surface that can be played like a real instrument. In these situations, a
dedicated drum pad controller would be better for the job. Drum
controllers vary widely in design. They can be built into a single, semi-
portable case, often having between six and eight playing pads, or the
trigger pads can be individual pads that can be ﬁtted onto a special rack,
traditional drum ﬂoor stand or drum set.

n MIDI drums: Another way to MIDI-fy an acoustic drum is through the use
of trigger technology. Put simply, triggering is carried out by using a
transducer pickup (such as a mic or contract pickup) to change the acoustic
energy of a percussion or drum instrument into an electrical voltage. Using
a MIDI trigger device (Figure 9.38), a number of pickup inputs can be
translated into MIDI so as to trigger programmed sounds or samples from
an instrument for use on stage or in the studio.
319
MIDI and Electronic Music Technology  CHAPTER 9
FIGURE 9.38
By using a MIDI trigger
device, a pickup can be
either directly replaced or
sent via MIDI to another
device or sequenced track
(a) using a mic/instrument
source, or (b) using a
recorded track as a source.
DRUM REPLACEMENT
While we’re still on the subject of triggering MIDI sounds from acoustic sources
such as drums, another option is the use of triggering software to detect and
replace drum sounds from an already recorded DAW track (Figure 9.39). Using
a software drum replacement software, it’s a relatively simple matter to replace
a bad sounding kick, snare or whatever with one of the many sounds that you
might have in your existing drum sample library. These trigger plug-ins can also
be used to simply augment or fatten up an existing percussion track—the options
and possibilities are huge.
FIGURE 9.39
Steven Slate Drums Trigger
2 drum replacement plug-
in. (Courtesy of Steven 
Slate Drums, www.steven
slatedrums.com)
Other MIDI Instrument and Controller Types
There are literally thousands of instruments and controller types out there that
are capable of translating a performance or general body movements into MIDI.
You’d be surprised what you’ll ﬁnd searching the Web for wild and wacky
controllers—both those that are commercially available and those that are made
by soldering iron junkies. A few of the more traditional controllers include MIDI

Electronic Instruments
guitars and basses, wind controllers, MIDI vibraphones—the list goes on and
on. It’ll be worth your while to search the web for something that might work
best and be the most fun for you.
SEQUENCING
With regards to the modern-day project studio, one of the most important tools
within MIDI production is the MIDI sequencer. A sequencer is a digital device
or software application that’s used to record, edit and output MIDI messages
in a sequential fashion. These messages are generally arranged in a track-based
format that follows the modern production concept of having separate
instruments (and/or instrument voices) located on separate tracks. This
traditional track environment makes it easy for us humans to view MIDI data
as isolated tracks, most commonly on our digital audio workstation (DAW).
These sequenced tracks contain MIDI-related performance and control events
that are made up of such channel and system messages as Note-On, Note-Off,
Velocity, Modulation, Aftertouch and Program/Continuous Controller messages.
Once a performance has been recorded into a sequencer’s memory, these events
can be graphically arranged and edited into a musical performance. The data
can then be saved as a MIDI ﬁle or (most likely) within a DAW session and
recalled at any time, allowing the data to be played back in its originally recorded
or edited “sequential” order.
Integrated Hardware Sequencers
A type of keyboard synth and sampler system known as a keyboard workstation
will often include much of the necessary production hardware that’s required
for music production, including effects and an integrated hardware sequencer.
These systems have the advantage of letting you take your instrument and
sequencer on the road without having to drag your whole system along. Similar
to the stand-alone hardware sequencer, a number of these sequencer systems
have the disadvantage of offering few editing tools beyond transport functions,
punch-in/out commands, and other basic edit functions. Newer, more powerful
keyboard systems include a larger, integrated LCD display, and have extensive
features that resemble their software sequencing counterparts. Other types of
palm-sized sequencers offer such features as polyphonic synth voices, drum
machine kits, effects, MIDI sequencing and, in certain cases, facilities for
recording multitrack digital audio in an all-in-one package that ﬁts in the palm
of your hand!
Software Sequencers
By far, the most common sequencer type is the software sequencing section
that exists within all major DAW programs (Figure 9.40). These tracks (which
exist alongside audio tracks within a DAW) take advantage of the hardware and
software versatility that only a computer can offer in the way of speed, hardware
320

321
MIDI and Electronic Music Technology  CHAPTER 9
ﬂexibility, memory management, signal routing and digital signal and MIDI
processing. These sequence tracks offer a multitude of advantages over their
hardware counterparts, such as:
n Increased graphics capabilities (giving us direct control over track and
transport-related record, playback, mix and processing functions)
n Standard computer cut-and-paste edit capabilities
n Ability to easily change note and controller values, one note at a time or
over a deﬁned range
n A graphic user interface environment that allows easy manipulation of
program, controller and edit-related data
n Easy adjustment of performance timing and tempo changes within a
session
n Powerful MIDI routing to multiple ports within a connected system
n Graphic assignment of instrument voices via Program Change messages
n Ability to save and recall ﬁles using standard computer memory media
FIGURE 9.40
The MIDI edit window 
within a DAW. (a) Cubase
audio production software.
(Courtesy of Steinberg
Media Technologies GmbH,
a division of Yamaha
Corporation,
www.steinberg.net) 
(b) Protools. (Courtesy of
Digidesign, a division of
Avid Technology,
www.digidesign.com)
BASIC INTRODUCTION TO SEQUENCING
When dealing with any type of sequencer, one of the most important concepts
to grasp is that these devices don’t store sound directly; instead, they encode
MIDI messages that instruct instruments as to what note is to be played, over
what channel, at what velocity and at what, if any, optional controller values
might be attributed to the messages. In other words, a sequencer simply stores
command instructions that follow in a sequential order. This means that the
amount of encoded data is a great deal less memory intensive than its digital
audio or digital video media counterparts. Because of this, the data overhead
that’s required by MIDI is very small, allowing a computer-based sequencer to
work simultaneously with the playback of digital audio tracks, video images,
Internet browsing, etc., all without unduly slowing down the computer’s CPU.
For this reason, MIDI and the MIDI sequencer provide a media environment
that plays well with other computer-based production media.

Basic Introduction to Sequencing
322
Recording
Commonly, a MIDI sequencer is an application within a DAW workspace for
creating personal and commercial compositions in environments that range from
the bedroom to more elaborate project and professional studios. As with audio
tracks, these systems use a working interface that’s roughly designed to emulate
a traditional multitrack-based environment. A tape-like set of transport controls
lets us move from one location to the next using standard Play, Stop, Fast
Forward, Rewind and Record command buttons. Beyond using the traditional
Record-Enable button to select the track or tracks that we want to record onto,
all we need to do is select the MIDI input (source) port, output (destination)
port, MIDI channel (although most DAWs are also able to select all MIDI inputs
as a source for ease of use), instrument/plug-in patch and other setup require -
ments. Then press the record button and begin laying down the track—it can
be that easy.
1. Pull out a favorite MIDI instrument or call up a
favorite plug-in instrument.
2. Route the instrument’s MIDI and audio cables
(or plug-in routing) to your DAW.
3. Create a MIDI track that can be recorded to.
4. Set the session to a tempo that feels right for
the song.
5. Assign the track’s MIDI input to the port that’s
receiving the incoming MIDI data.
6. Assign the track’s MIDI output to the port and
proper MIDI channel that’ll be receiving the
outgoing MIDI data during playback.
7. If a click track is desired, turn it on (more about
this later).
8. Name the track (always a good idea, as this will
make it easier to identify the MIDI instrument/
patch in the future).
9. Place the track into the Record Ready mode.
10. Play the instrument or controller. Can you hear
the instrument? Do the MIDI activity indicators
light up on the sequencer, track and MIDI
interface? If not, check your cables and run
through the checklist again. If so, press Record
and start laying down your ﬁrst track.
11. Once you’ve ﬁnished a track, you can jump
back to the beginning of the recorded passage
and listen to it. From this point, you could then
“arm” (a term used to denote placing a track
into the record-ready mode) the next track and
go about the process of laying down additional
tracks (possibly with a different instrument or
sound patch) until a song begins to form.
Try This: Tutorial: Setting Up a Session and 
Laying Down a MIDI Track
D I Y
 do  it  yourself

SETTING A SESSION TEMPO
When beginning a MIDI session, one of the ﬁrst aspects to consider is the tempo
and time signature. The beats-per-minute (bpm) value will set the general tempo
speed for the overall session. This is important to set at the beginning of the
session, so as to lock the overall “bars and beats” timing elements to this initial
speed that’s often essential in electronic and modern music production. This
tempo/click element can then be used to lock the timing elements of other
instruments and/or rhythm machines to the session (e.g., a drum machine plug-
in can be pulled into the session that’ll automatically lock to the session’s speed
and timing).
323
MIDI and Electronic Music Technology  CHAPTER 9
To avoid any number of unforeseen obstacles to a
straightforward production, it’s often wise to set
your session tempo (or at least think about these
options) before pressing the record button.
As with most things MIDI, when working strictly in this environment, it’s
extremely easy to change almost all aspects of your song (including the tempo)
provided that you have initially worked to some form of an initial timing element
(such as a base tempo). Although working without a set tempo can give a very
human feel, it’s easy to see how this might be problematic when trying to get
multiple MIDI tracks to work together in any form of musical sync and timing
control. In truth, there are usually ways to pull corrective timing rabbits out of
your technical hat, but the name of the game (as with most things recording)
is forethought and preparation.
CHANGING TEMPO
The tempo of a MIDI production can often be easily changed without wor-
rying about changing the program’s pitch or real-time control parameters. In
short, once you know how to avoid potential conﬂicts and pitfalls, tempo
variations can be made after the fact with relative ease. All you need to do is
alter the tempo of a sequence (or part of a sequence) to best match the 
overall feel of the song. In addition, the tempo of a session can be dynamically
changed over its duration by creating a tempo map that causes the speed to
vary by deﬁned amounts at speciﬁc points within a song. Care and preplanning
should be exercised when a sequence is to be synced to another media form or
device.
CLICK TRACK
When musical timing is important (as is often the case in modern music and
visual media production), a click track can be used as a tempo guide for keeping
the performance as accurately on the beat as possible. A click track can be set

to make a distinctive sound on the measure boundary or (for a more accurate
timing guide) on the ﬁrst beat boundary and on subsequent meter divi-
sions (e.g., tock, tick, tick, tick, tock, tick, tick, tick . . . ). Most sequencers can
output a click track by either using a dedicated beep sound (often outputting
from the device or main speakers) or by sending Note-On messages to a
connected instrument in the MIDI chain. The latter lets you use any sound you
want and often at deﬁnable velocity levels. For example, a kick could sound on
the beat, while a snare sounds out the measure divisions.
The use of a click track is by no means a rule. A strong reason
for using a click track (at least initially) is that it serves as
a rhythmic guide that can improve the timing accuracy
of a performance. However, in certain instances, it 
can lead to a performance that sounds stiff. For
compositions that loosely ﬂow and are legato in
nature, a click track can stiﬂe the passage’s overall
feel and ﬂow. As an alternative, you could turn the
metronome down, have it sound only on the measure
boundary, and then listen through one head phone. As
with most creative decisions, the choice is up to you and
your current circumstance.
MULTITRACK MIDI RECORDING
Although only one MIDI track is commonly recorded at a time, most mid- and
professional-level sequencers let us record multiple tracks at one time. This
feature makes it possible for multiple instruments and performers to be recorded
to a multi-track sequence in one, live pass.
PUNCHING IN AND OUT
Almost all sequencing systems are capable of punching in and out of record
while playing a sequence (Figure 9.41). This commonly used function lets you
drop in and out of record on a selected track (or series of tracks) in real time,
in a way that mimics the traditional multitrack overdubbing process. Although
punch-in and punch-out points can often be manually performed on the ﬂy
from the transport or often from a convenient foot pedal, most sequencers 
can also automatically perform a punch by graphically or numerically entering
in the measure/beat points that mark the in and out location points. Once 
done, the sequence can be rolled back to a point a few measures before the
punch-in point and the artist can then play along while the sequencer
automatically performs the necessary switching functions.
Basic Introduction to Sequencing
324
Care should be 
taken when setting the proper 
time signature at the session’s outset.
Listening to a 4/4 click can be
disconcerting when the song is 
being performed in 3/4 
time.

325
MIDI and Electronic Music Technology  CHAPTER 9
FIGURE 9.41
Automated punch-in 
and punch-out points.
(Courtesy of Steinberg
Media Technologies GmbH,
a division of Yamaha
Corporation,
www.steinberg.net)
1. Create a MIDI track in your DAW and record a
musical passage. Save/name the session.
2. Roll back to the beginning of the take and play
along. Manually punch in and out during a few
bars (simply by pressing the REC button). Now,
undo or revert back to your originally saved
session.
3. Read your DAW/sequencer manual and learn
how to perform an automated punch (placing
your punch-in and punch-out points at a logical
place on the track).
4. Roll back to a point a few bars before the punch
and go into the record mode. Did the track
automatically place itself into record? Was that
easier than doing it manually?
5. Feel free to try other features, such as record
looping or stacking.
Try This: Punching During a Take
D I Y
 do  it  yourself
STEP TIME ENTRY
In addition to laying down a performance track in real time, most sequencers
will allow us to enter note values into a sequence one note at a time. This feature
(known as step time, step input or pattern sequencing) makes it possible for
notes to be entered into a sequence without having to worry about the exact
timing. Upon playback, the sequenced pattern will play back at the session’s
original tempo. Fact is, step entry can be an amazing tool, allowing a difﬁcult
or a blazingly fast passage to be meticulously entered into a pattern and then
be played out or looped with a degree of technical accuracy that would otherwise
be impossible for most of us to play. Quite often, this data entry style is used
with fast, high-tech musical styles where real-time entry just isn’t possible or
accurate enough for the song.

Basic Introduction to Sequencing
326
DRUM PATTERN ENTRY
In addition to real-time and step-time entry, most sequencers will allow for
drum and percussion notes to be entered into a drum pattern grid (Figure 9.42).
This graphical environment is intuitive to most drummers and allows for patterns
to be quickly and easily programmed and then linked together as a string of
patterns that can form the backbone of a song’s beat.
FIGURE 9.42
Drum pattern entry window
in Cubase/Nuendo.
(Courtesy of Steinberg
Media Technologies GmbH,
a division of Yamaha
Corporation,
www.steinberg.net)
For those who want to dive into a whole new world of experimentation and sonic surprises, here’s a possible
guideline that can be used in a live recording session:: “If the instrument supports MIDI, record the performance
data to a DAW MIDI track during each take.” For example:
It’s all up to you—as you might imagine, surprises can deﬁnitely come from experiments like these.
n You might record the MIDI out of a keyboard
performance to a DAW MIDI track. If there’s a
problem in the performance you can simply
change the note (just after it was played or later),
without having to redo the performance—or, if
you want to change the sound, simply pick
another sound.
n Record the sequenced track from a triggered
drum set or controller to a set of DAW MIDI
tracks.
n If a MIDI guitar riff needs some tweaking to ﬁll
out the sound, double the MIDI track with
another sound patch or chord augmentation.
n Acoustic drum recordings can beneﬁt from MIDI
by using a trigger device that can accept audio
from a mic or recorded track and output the
triggered MIDI messages to a sampler or
instrument, in order to replace the bad tracks
with samples that rock the house. Of course,
don't forget to record the trigger outputs to MIDI
tracks on the DAW, just in case you want to edit
or change the sounds at a later time.
n Even if MIDI isn’t involved, a drum replacement
plug-in could be used to replace bad sounds or
to ﬁll out a sound at a later time.
D I Y
 do  it  yourself

MIDI to Audio
When mixing down a session that contains MIDI tracks, many folks prefer not
to mix the sequenced tracks in the MIDI domain. Instead, they’ll often export
(bounce) the hardware or plug-in instruments to an audio track within the
project. Here are a few helpful hints that can make this process go more
smoothly:
n Set the main volume and velocities to a “reasonable” output level, much
as you would with any recorded audio track.
n Solo the MIDI and instrument’s audio input track and take a listen, making
sure to turn off any reverb or other effects that might be on that instrument
track. If you really like the instrument effect, of course, go ahead and record
it; however, you might consider recording the track both with and without
effects, as you might want to make changes in mixdown.
n If any mix-related moves have been programmed into the sequence, you
might want to strip out volume, pan and other controller messages before
exporting the track. This is easily done by making a copy of the existing
track and then stripping the controller values from the copy—an export
can be taken from this track.
n If the instrument has an acoustic element to it (such as a MIDI acoustic
grand piano or MIDI guitar room/amp setup), you might also consider
recording the instrument in the studio and mix these tracks in with the
MIDI tracks This will often allow for a greater sense of acoustic “space”
within the ﬁnal mix.
327
MIDI and Electronic Music Technology  CHAPTER 9
It’s worth mentioning again that it is always wise to
save the original MIDI track or ﬁle within the session.
This makes future changes in the composition
inﬁnitely easier. Failure to save your MIDI ﬁles could
limit your future options or result in major production
headaches down the road.
Various DAWs will offer different options for capturing MIDI tracks as audio
tracks within a session. As always, you might consider consulting your DAW’s
manual for further details.
Audio to MIDI
Another tool that comes under the “You gotta try this!” category is the ability
to take an audio ﬁle and extract relevant MIDI data from the track. The power
in such an option is truly huge! For example, let’s say that we have a really
killer bass loop audio ﬁle that has just the right vibe, but it lacks punch, is too
noisy or sounds wrong. A number of DAWs actually have algorithms for
detecting and extracting MIDI note values that will result in a MIDI loop or
sequence that can be saved as a separate MIDI ﬁle. As with all things MIDI, this
new loop can be routed to a synth, sampler or anything to change the sound

Basic Introduction to Sequencing
in a way that’ll make it stand out. In short, you can now do “anything” with
the sequence. You owe it to yourself to check it out!
There is one thing to keep in mind when extracting MIDI from an audio track
or loop, however. Each DAW will usually use different detection algorithms for
drum/percussion, complex melody patterns and sustained pads. You might
experiment to ﬁnd which will work best with your track, as different DAWs will
often yield completely different results. Experimentation and experience is often
the name of the game here. Lastly, the results might be close to the original (in
pitch and rhythmic pattern), but it might require a bit of human editing. Once
you get it right, the results can be truly fun to experiment with.
Saving Your MIDI Files
Just as it’s crucial that we carefully and methodically back up our program and
production media, it’s important that we save our MIDI and session ﬁles while
we’re in production. This can be done in two ways:
n Periodically save your ﬁles over the course of a production.
n At important points throughout a production, you might choose to save
your session ﬁles under new and unique names (mysong001, mysong002,
etc.), thereby making it possible to easily revert back to a speciﬁc point
in the production. This can be an important recovery tool should the
session take a wrong turn or for re-creating a speciﬁc effect and/or mix.
Personally, I save these session versions under a “mysong_bak” sub -
directory within the project session.
One other precaution will often save your butt: Always save your MIDI ﬁles
within the session.
n When working with MIDI within a DAW session, it’s always a good idea
to save the original MIDI tracks within the session. This makes it easy to
go back and change a note, musical key or sounding voice or to make any
other alterations you want. Not saving these ﬁles could lead to some
major headaches or worse.
n Keeping your DAW’s MIDI ﬁles within a folder called “MIDI” will make
it easier to identify and even hide (collapse) these tracks when they’re not
needed.
MIDI ﬁles can also be converted to and saved as a standard MIDI ﬁle for use
in exporting to and importing from another MIDI program or for distributing
MIDI data for use on the Web, to cell phones, etc. These ﬁles can be saved in
either of two formats:
n Type 0: Saves all of the MIDI data within a session as a single MIDI track.
The original MIDI channel numbers will be retained. The imported data
will simply exist on a single track. Note that if you save a multi-instrument
session as a Type 0, you’ll loose the ability to save the MIDI data to discrete
tracks within the saved sequence.
328

n Type 1: Saves all of the MIDI data within a session onto separate MIDI
tracks that can be easily imported into a sequencer in a multitrack fashion.
Documentation
When it comes to MIDI sequencing, one of the things to deﬁnitely keep in mind
is the need for documenting any and all information that relates to your MIDI
tracks. It’s always a good idea to keep notes about:
n What instrument/plug-in is being used on the track?
n What is the name of the patch?
n What are the settings that are being used (if these settings are not
automatically saved within the session—and even if they are)? In such
cases, you might want to make a screenshot of the plug-in settings, or if
it’s a hardware device, get out your cell phone or camera and take a picture
that can be placed in the documentation directory.
Placing these and any other relevant piece of information into the session
(within the track notes window or in a separate doc ﬁle) can really come in
handy when you have to revisit the ﬁle a year or so later and you’ve totally
forgotten how to rebuild the track. Trust me on this one; you’ll eventually be
glad you did.
Editing
One of the more important features that a sequencer (or MIDI track within 
a DAW) has to offer is its ability to edit sequenced tracks or blocks within 
a track. Of course, these editing functions and capabilities often vary from 
one DAW/sequencer to another. The main track window of a sequencer or 
MIDI track on a DAW is used to display such track information as the exist-
ence of track data, track names, MIDI port assignments for each track, 
program change assignments, volume controller values and other transport
commands.
Depending on the sequencer, the existence of MIDI data on a particular track
at a particular measure point (or over a range of measures) is indicated by the
highlighting of track data in a way that’s extremely visible. For example, back
in Figure 9.40, you’ll notice that the MIDI tracks contain graphical bar display
information. This means that these measures contain MIDI messages, while the
non-highlighted areas don’t.
By navigating around the various data display and parameter boxes, it’s possible
to use cut-and-paste and/or edit techniques to vary note values and parameters
for almost every facet of a musical composition. For example, let’s say that we
really screwed up a few notes when laying down an otherwise killer bass riff.
With MIDI, ﬁxing the problem is totally a no-brainer. Simply highlight each
fudged note and drag it (them) to the proper note location—we can even
329
MIDI and Electronic Music Technology  CHAPTER 9

Basic Introduction to Sequencing
change the beginning and endpoints in the process. In addition, tons of other
parameters can be changed, including velocity, modulation and pitch bend,
note and song transposition, quantization and humanizing (factors that
eliminate or introduce human timing errors that are generally present in a live
performance), in addition to full control over program and continuous controller
messages . . . the list goes on and on.
PRACTICAL EDITING TECHNIQUES
When it comes to learning the Ins, Outs and Thrus of basic sequencing,
absolutely nothing can take the place of diving in and experimenting with your
own setup. Here, I’d like to quote Craig Anderton, who said: “Read the manual
through once when you get the program (or device), then play with the software
and get to know it before you need it. Afterwards, reread the manual to pick
up the system’s ﬁner operating points.” Wise words—although I tend to take
another route and simply start pressing buttons and icons until I learn what I
need. I honestly think there’s something to be said for both approaches.
In this section, we’ll be covering some of the basic techniques that’ll help speed
you on your way to sequencing your own music. Note that there are no rules
to sequencing MIDI. As with all of music production (and the arts, for that
matter), there are as many right ways to perform and play with making music
via MIDI as there are musicians. Just remember that there are no hard and
steadfast rules to music production—but there are always guidelines, tools and
tips that can speed and improve the process.
Transposition
As was mentioned earlier, a sequencer app is capable of altering individual notes
in a number of ways including pitch, start time, length and controller values.
In addition, it’s generally a simple matter for a deﬁned range of notes in a passage
to be altered in ways that could alter the overall key, timing and controller
processes. Changing the pitch of a note or the entire key of a song is extremely
easy to do on a MIDI track. Depending on the system, a song can be transposed
up or down in pitch at the global or deﬁned measure level, thereby affecting
the pitch or musical key of a song. Likewise, a segment can be shifted in pitch
from the main edit, piano roll or notation edit windows by simply highlighting
the bars and tracks that are to be changed and then dragging them or by calling
up the transpose function from the MIDI edit menu.
Quantization
By far, most common timing errors begin with the performer. Fortunately, “to
err is human,” and standard performance timing errors often give a piece a live
and natural feel. However, for those situations when timing goes beyond the
bounds of nature, an important sequencing feature known as quantization can
help correct these timing errors. Quantization allows timing inaccuracies to be
330
Transposition 
Quantization 

adjusted to the nearest desired musical time division (such as a quarter, eighth,
or sixteenth note). For example, when performing a passage where all involved
notes must fall exactly on the quarter-note beat, it’s often easy to make timing
mistakes (even on a good day). Once the track has been recorded, the
problematic passage can be highlighted and the sequencer can recalculate each
note’s start and stop times so they fall precisely on the boundary of the closest
time division.
Humanizing
The humanization process is used to randomly alter all of the notes in a selected
segment according to such parameters as timing, velocity and note duration.
The amount of randomization can often be limited to a user-speciﬁed value or
percentage range, and parameters and can be individually selected or ﬁne-tuned
for greater control. Beyond the obvious advantages of reintroducing human-
like timing variations back into a track, this process can help add expression
by randomizing the velocity values of a track or selected tracks. For example,
humanizing the velocity values of a percussion track that has a limited dynamic
range can help bring it to life. The same type of life and human swing can be
effective on almost any type of instrument.
Slipping in Time
Another timing variable that can be introduced into a sequence to help change
the overall feel of a track is the slip time feature. Slip time is used to move a
selected range of notes either forward or backward in time by a deﬁned number
of clock pulses. This has the obvious effect of changing the advance/retard times
for these notes, relative to the other notes or timing elements in a sequence.
Editing Controller Values
Almost every DAW sequencer allows controller message values to be edited or
changed often by using a simple, graphic window whereby a line or freeform
curve can be drawn that graphically represents the effect that the controller
messages will have on an instrument or voice. By using a mouse or other input
device, it becomes a simple matter to draw a continuous stream of controller
values that correspondingly change such variables as velocity, modulation, pan,
etc. To physically change parameters using a controller, all you need to do is
twiddle a knob, move a fader or graphically draw the variables on-screen in a
WYSIWYG (“what you see is what you get”) fashion.
It almost goes without saying that a range of controller events can be altered
on one or more tracks by allowing a range of MIDI events to be highlighted
and then altered by entering in a parameter or processing function from an edit
dialog box. This ability to deﬁne a range of events often comes in handy for
making changes in pitch/key, velocity, main volume and modulation (to name
a few).
331
MIDI and Electronic Music Technology  CHAPTER 9
Humanizing 
Slipping in Time 
Editing Controller Values 

Basic Introduction to Sequencing
332
Some of the more common controller values that can affect a sequence and/or MIDI mix values include (for the
full listing of controller ID numbers, see Table 9.1 earlier in this chapter):
Control #
Parameter
1
Modulation Wheel
2
Breath Controller
4
Foot Controller
7
Channel Volume (formerly Main Volume)
8
Balance
10
Pan
11
Expression Controller
64
Damper Pedal on/off (Sustain) <63 off, >64 on
Playback
Once a sequence is composed and saved within a session, all of the sequence
tracks can be transmitted through the various MIDI ports and channels to the
instruments or devices to make music, create sound effects for ﬁlm tracks,
control device parameters in real time, etc. Because MIDI data exists as encoded
real-time control commands and not as audio, you can listen to the sequence
and make changes at any time. You could change the patch voices, alter the
1. Read your DAW/sequencer manual and learn its
basic controller editing features.
2. Open a MIDI track or create a new one.
3. Highlight a segment and reduce the overall
Channel Volume levels by 50%. On playback,
did the levels reduce?
4. Now, refer to your DAW/sequencer manual for
how to scale MIDI controller events over time.
5. Select a range of measures and change their
Channel Volume settings (controller 7) over time.
Does the output level change over the segment
when you play it back?
7. Highlight the segment and scale the velocity
values so they have a minimum value of 64 and
a maximum of 96. Could you see and hear the
changes?
9. Again, highlight the segment and draw or
instruct the software to fade it from its current
value to an ending value of 0. Hopefully, you’ve
just created a Channel Volume fade. Did you see
the MIDI channel fader move?
10. Undo the last fade and start a new one with an
initial value of 0 and a current value of 100%
toward the end of the section. Did the segment
fade in?
11. Feel free to make up your own edit moves and
experiment with the track(s).
Try This: Tutorial: Changing Controller Values
D I Y
 do  it  yourself

333
MIDI and Electronic Music Technology  CHAPTER 9
ﬁnal mix or change and experiment with such controllers as pitch bend or
modulation—and as we’ve read, change the tempo and key signature. In short,
this medium is inﬁnitely ﬂexible in the number of versions that can be created,
saved, folded, spindled and mutilated until you’ve arrived at the overall sound
and feel you want. Once done, you’ll have the option of using the medium for
live performance or mixing the tracks down to a ﬁnal recorded product, either
in the studio or at home.
During the summer, in a wonderful small-town tavern in the city where I live,
there’s a frequent performer who’ll wail the night away with his voice, trusty
guitar and a backup band that consists of several electronic synth modules and
a DAW that’s just chock-full of country-’n’-western sequences. His set of songs
for the night is loaded into a song playlist that’s pre-programmed for that night.
Using this, he queues his sequences so that when he’s ﬁnished one song, taken
his bow, introduced the next song and complimented the lady in the red dress,
all he needs to do is press the space bar and begin playing the next song. Such
is the life of a hard workin’, on-the-road sequencer.
Mixing a Sequence
Almost all DAW and sequencer types will let you mix a sequence in the MIDI
domain using various controller message types. This is usually done by simply
integrating the world of MIDI into the DAW’s on-screen mixer. Instead of
directly mixing the audio signals that make up a sequence, however, these
controls are able to access such track controllers as Main Volume (controller
7), Pan (controller 10), and Balance (controller 8), in an environment that
completely integrates into the workstation’s overall mix controls. Therefore, 
even with the most basic DAW, you’ll be able to mix and remix your sequences
with complete automation and total settings recall whenever a new sequence
is opened. As is almost always the case with a DAW’s audio and MIDI graphical
user interface (GUI), the controller and mix interface will almost always 
have moving faders, controls and access to external hardware controllers 
(Figure 9.43).
FIGURE 9.43
MIDI tracks are routinely
added into the DAW mixer
for real-time parameter
control over hardware
and/or software devices.
(Courtesy of Avid
Technology, Inc.,
www.avid.com and
Neyrinck Audio, www.
neyrinck.com)

Music Printing Programs
334
Never forget that you can always export your MIDI
track/instrument as an audio track for easy use and
manipulation within a session. Always remember to
save (and disable) your MIDI track within the
session, just in case any changes need to be made
later.
MUSIC PRINTING PROGRAMS
In recent times, the ﬁeld of transcribing musical scores and arrangements has
been strongly affected by both the computer and MIDI technology. This process
has been greatly enhanced through the use of computer software that make it
possible for music notation data to be entered into a computer either manually
(by placing the notes onto the screen via keyboard or mouse movements) or
by direct MIDI input or by sheet music scanning technology. Once entered,
these notes can be edited in an on-screen environment that lets you change and
conﬁgure a musical score or lead sheet using standard cut-and-paste editing
techniques. In addition, most programs allow the score data to be played directly
from the score by electronic instruments via MIDI. A ﬁnal and important
program feature is their ability to quickly print out hard copies of a score or
lead sheets in a wide number of print formats and styles.
A music printing program (also known as a music notation program) lets you
enter musical data into a computerized score in a number of manual and
automated ways (often with varying degrees of complexity and ease). Programs
of this type (Figure 9.44) offer a wide range of notation symbols and type styles
that can be entered either from a computer keyboard or mouse. In addition to
entering a score manually, most music transcription programs will accept MIDI
input, allowing a part to be played directly into a score. This can be done in
real time (by playing a MIDI instrument/controller or ﬁnished sequence directly
into the program) or in step time (by entering the notes of a score one note at
a time from a MIDI controller) or by entering a standard MIDI ﬁle into the
program (which uses a sequenced ﬁle as the notation source).
FIGURE 9.44
Music printing. (a) Finale
music composition,
arranging and printing
program. (Courtesy of
MakeMusic, Inc.,
www.finalemusic.com) (b)
Steinberg Cubase/Nuendo
score window. (Courtesy of
Steinberg Media
Technologies GmbH, a
division of Yamaha
Corporation,
www.steinberg.net)

In addition to dedicated music printing programs, most DAW or sequencer
packages will often include a basic music notation application that allows the
sequenced data within a track or deﬁned region to be displayed and edited
directly within the program, from which it can be printed in a limited score-
like fashion. However, a number of high-level workstations offer scoring features
that allow sequenced track data to be notated and edited in a professional
fashion into a fully printable music score.
As you might expect, music printing programs often vary widely in their capa -
bilities, ease of use and offered features. These differences often center around
the graphical user interface (GUI), methods for inputting and editing data, the
number of instrumental parts that can be placed into a score, the overall selection
of musical symbols, the number of musical staves (the lines that music notes
are placed onto) that can be entered into a single page or overall score, the
ability to enter text or lyrics into a score, etc. As with most programs that deal
with artistic production, the range of choices and general functionality reﬂect
the style and viewpoints of the manufacturer, so care should be taken when
choosing a professional music notation program, to see which one would be
right for your personal working style.
335
MIDI and Electronic Music Technology  CHAPTER 9


One of the most awe-inspiring inventions in the modern age is mobile
computing—speciﬁcally when it relates to the hundreds of millions of pads and
phones (handys, mobiles, or whatever your country calls them). It has changed
the way we communicate (or not communicate, if we don’t occasionally look
up) and the way that information is dealt with in an on-the-go, on-demand
kind of way.
Of these mobile devices, the class of systems that have affected the audio
production community in a very direct way are the iOS range of devices from
Apple. Those of us in recording, live sound and music production have been
directly touched in many ways with production methods that offer:
n Mobility: As iOS devices are wireless and portable by their very nature,
these mini-computers allow us to record, generate and playback audio;
perform complex control functions over a production computer, be a
mixer or production hardware device—our imagination is the only limit
to what these devices can do from a remote, untethered location.
n Affordability: Quite often, these multi-function devices will replace
dedicated hardware systems that can cost a hundred or thousand times
that of what an equivalent “app” might cost.
n Multi-functionality: Given that these devices are essentially a computer,
their capabilities can be as far reaching and as functional as the pro -
gramming that went into them. Most importantly, they have the ability
to be a chameleon—changing form from being a controller, to a
synthesizer, to a mixer, to a calculator, to a ﬂashlight, to a phone and
social networking device—all in a fraction of a second.
n Touch capabilities: The ability to directly interact with portable devices
through the sense of touch (and voice) is something that we have all come
to take for granted. This direct interaction allows all of the above advantages
to be available at the touch of a virtual button, which, once touched, can
transform the device into another type of phoenix that can raise your
337
CHAPTER 10
The iOS in Music 
Production

The iOS in Music Production
production system from the ashes into something beautiful, simple, cost-
effective and functional.
Of course, it goes without saying that this chapter is simply a quick overview
into a ﬁeld that has expanded into a major industry and ﬁeld of artistry in and
of itself. The number of apps, options and supporting connectivity and control
hardware choices continue to grow on a monthly basis. To better ﬁnd out what
options are available and best suit your needs, I urge you to simply browse the
App Store, search the web for an ever-expanding number of online resources
(e.g., www.iosmidi.com) and wade your way through any number of YouTube
videos and reviews on any iSubject that might be helpful to you and your studio
scenario.
AUDIO INSIDE THE iOS
Apple has had a long history of dealing with properly implementing audio into
their operating systems (for the most-part). One of the major advantages of
dealing with the iOS for portable computing and media players comes down
to two factors:
n The iOS has been developed to pass high-quality audio with a very low
amount of latency (system delay within the audio path).
n Given the fact that the operating system (including its audio program -
ming) is closed to outside third-party developers, audio applications can
be developed in a standardized way that must make use of Apple’s
programming architecture.
The following sections offer a basic glimpse into how the iOS can integrate into
an audio production system with professional results, which have made portable
production more mobile, cost-effective and fun.
Core Audio on the iOS
Audio on the Mac OS and the iOS are handled through an integrated
programming service called Core Audio. This audio architecture (Figure 10.1)
is broken into application-level services that include:
n Audio Cue Services: Used to record, playback, pause, loop and synchronize
audio.
n Audio File, Converter and Codec Services: Used to read and write from
disk (or media memory) and to perform audio data format transformations
(in OS X, custom codecs can also be created).
n Audio Units: Used to host audio units (audio plug-ins) in your application.
n Music Sequencing Services: Used to play MIDI-based control and music
data.
n Core Audio Clock Services: Used for audio and MIDI synchronization and
time format management.
338

339
The iOS in Music Production  CHAPTER 10
n System Sounds: Used to play system sounds and user-interface sound
effects.
Core Audio on the iOS is optimized for the computing resources available in
a battery-powered mobile platform.
FIGURE 10.1
Core Audio’s basic I/O
architecture.
AudioBus
AudioBus (Figure 10.2) is an iOS app that can be downloaded and used to act
as a virtual patch cord for connecting together the ins, outs and throughs of
various AudioBus compatible apps in a way that would otherwise not be
possible. For example, an audio app that has been launched will obviously have
its own input and an output. If we were to launch another audio app, there
would be no way to “patch” the output of one app to the input of another
within the iOS. Using AudioBus, it’s now possible to connect a source device
(input), route this through an effects device (which is not acting as “plug-in”,
but as a stand-alone, processing app) and then patch the output of the effects
app through to a ﬁnal destination app (output).
FIGURE 10.2
AudioBus showing input,
effects and output routing.
(a) Basic screen. 
(b) Showing audio apps that
are available to its input.
Audio Units for the iOS
With the release of the latest versions of the iOS, Apple has allowed for effects
and virtual instrument plug-ins to be directly inserted into an app, much in the
same way that plug-ins can be inserted into programs using the Mac OS. This
protocol, which is universally known across all Apple platforms as Audio Units,
lets us go to the App Store and download a plug-in that might work best in
our situation, then insert it directly into the processing path of an iOS DAW,
video editor or other host app for DSP processing.

Connecting the iOS to the Outside World
340
CONNECTING THE iOS TO THE OUTSIDE WORLD
iOS devices can easily be connected into an audio or DAW system, using cost-
effective external hardware solutions that can be tailored to a wide range of
studio and on-the-go applications.
Audio Connectivity
Audio passes through an iOS device using Core Audio. Connecting to the device
can be as simple as using the internal mic and line/headphone out-jack (or
lightening port) on the unit. Making a higher-quality, multi-channel audio
connection will often require an audio interface. The number of interface options
for connecting an iPad or iPhone to a high-quality mic or integrating these
devices into the studio system continues to grow, however, these options
generally fall into two categories:
n A docking device (Figure 10.3) can be used that is speciﬁcally designed to
connect an iOS device to the outside studio world of audio and possibly
MIDI can be used.
n A standard audio interface that is Class Compliant (cc is a mode that allows
the interface to be directly connected to an iOS device, using a readily-
available Apple Camera Adapter as shown in Figure 10.4), which can
connect to the device, allowing audio (and often MIDI) to pass in a
normal I/O manner.
FIGURE 10.3
Alesis iO Dock iPad-based
audio/MIDI interface.
(Courtesy of Alesis,
www.alesis.com)
FIGURE 10.4
A camera adapter is used to
connect a compliant device
(such as an audio interface
to an iOS device).

341
The iOS in Music Production  CHAPTER 10
MIDI Connectivity
Using Core Audio, all iOS devices are capable of receiving and outputting MIDI
without the need for additional software or drivers. All that’s needed (as always)
is a compatible I/O device that is capable of communicating with it. Currently,
this is possible in any of three ways:
n By way of a docking device (Figure 10.5) that can serve as an interface for
audio and/or MIDI.
n By way of an Apple Camera Adapter that can connect the iOS device to a
class-compliant MIDI interface (Figure 10.6a).
n Through the use of an Apple Camera Adapter that can connect the device
to an audio interface with one or more audio and MIDI I/O ports (Figure
10.6b).
FIGURE 10.5
Alesis DM Dock iPad based
drum module. (Courtesy of
Alesis, www.alesis.com)
FIGURE 10.6
Class-compliant MIDI I/O 
for an iOS Device. (a)
iConnect MIDI4plus 4x4
MIDI interface. (Courtesy 
of iConnectivity,
www.iConnectivity.com)
(b) Rear of Steinberg’s 
UR22 MK II interface
showing MIDI I/O. 
(Courtesy of Steinberg
Media Technologies GmbH,
a division of Yamaha
Corporation,
www.steinberg.net)
RECORDING USING iOS
Given the fact that iOS devices are capable of recording, editing, processing and
outputting audio, it stands to reason that they would also excel at giving us
access to production tools on a bus, in an airplane, by the pool or on our way
to Mars—all in a cost-effective and versatile fashion.
Handheld Recording Using iOS
Another way that an iOS device can come in handy for recording audio is as a
handheld recording device. When used with a suitable mic accessory and
recording app (Figure 10.7), an iPhone or iPad can be used to capture
professional quality audio in an easy-to-use, on-the-go fashion, that can then
be edited or transferred to a DAW for further editing, processing and integration
into a project.

Recording Using iOS
342
Mixing with iOS
Another way that the iOS has integrated itself into the recording and mixing
process is through its pairing with the console or mixer itself (Figure 10.8). This
combination gives an engineer or producer unprecedented remote control over
most mixing, panning, EQ, EFX and monitor control, either in the studio or
on-stage.
FIGURE 10.7
Shure MV88 iOS handheld
microphone. (Courtesy of
Shure Incorporated,
www.shure.com, Images ©
2017, Shure Incorporated—
used with permission)
FIGURE 10.8
The StudioLive 16.4.2
Digital Recording and
Performance Mixer with
remote iPad app. (Courtesy
of Presonus Audio
Electronics, Inc.,
www.presonus.com)
iDAWs
Over the years, iPads have gotten powerful enough and apps have been written
that allow an entire session to be transferred from our main workstation to a
multitrack DAW on the pad (Figure 10.9). It’s easy to see (and hear) how such
a powerful and portable device would be of beneﬁt to the on-the-go musician
or producer. He or she could record, mix and process a mix virtually anywhere.
Offering up a surprising number of DSP options that are compatible with and
can be read by their main DAW system, anyone could put on a pair of in-ear
monitors or noise cancelling headphones and begin mixing (as the iMantra
goes) “virtually anywhere.”

Taking Control of Your DAW Using the iOS
In addition to the many practical uses that are listed above, another huge
contribution that iOS and mobile computing technology has made to music
production is the pad’s ability to serve as a DAW remote control. For example,
in the not-too-distant past, dedicated hardware DAW controllers would easily
cost us over $1,000 and would need both a power and a wired USB connection.
Now, with the introduction of iOS-based DAW controllers (Figure 10.10), the
same basic functionality is available to us for less than $20 (if not for free).
These apps allow the controller to ﬁt in our hand and allow us to work wirelessly
from any place in the studio, giving us the ability to:
n Mix from the studio, from your instrument stand, on-stage, in an audience,
virtually anywhere.
n Remotely control all transport functions from anywhere within the facility
n Studio monitor and headphone sub-mixes can be controlled from within
the DAW, allowing the musician to control their own monitor mix (Figure
10.11), simply by downloading the app, connecting their iOS device to
the studio network and then creating their own personal sub-mix.
The iOS doesn’t stop with the idea of controlling a DAW by using simple
transport, control and mixing functions—when used with a performance-based
DAW (such as Ableton Live), a number of iOS applications can be used to
wirelessly integrate with a DAW allowing a performer to take control of their
343
The iOS in Music Production  CHAPTER 10
FIGURE 10.9
Several iDAW examples: 
(a) Auria Pro. (Courtesy of
WaveMachine Labs, Inc,
www.wavemachinelabs.
com); (b) Steinberg’s
Cubasis. (Courtesy of
Steinberg Media
Technologies GmbH, 
a division of Yamaha
Corporation,
www.steinberg.net)

Recording Using iOS
344
FIGURE 10.11
Musicians can easily mix
their headphone sends
remotely via an iOS DAW
controller app. (Courtesy 
of Steinberg Media
Technologies GmbH, a
division of Yamaha
Corporation,
www.steinberg.net)
FIGURE 10.10
iOS-based DAW controllers.
(a) V-Control Pro DAW
controller. (Courtesy of
Neyrinck, www.vcontrol
pro.com) (b) Cubase iC Pro.
(Courtesy of Steinberg
Media Technologies GmbH,
a division of Yamaha
Corporation,
www.steinberg.net)
Go ahead and go to the App Store and search under
“DAW controllers” and choose one that will best
work for you (as always, it’s important to do a bit of
research before buying—we all know there are
always differences and quirks that will set one app
apart from the rest, for your particular DAW). 
Once you’ve followed the install instructions for your
particular DAW, you’ll be “remoting” around the
studio from your pad and/or phone in no time.
Try This: Installing a DAW Controller
D I Y
 do  it  yourself
system to create a live performance set in real-time (Figure 10.12). Whereas, in
the past, a hardware controller was used to perform with a DAW in a way that
had limited mobility and (more importantly) visual feedback, an iOS device
can give the performer far better tactile and interactive response cues that allow
the performer to clearly see exactly which audio and MIDI loops, effects and
any number of performance controls are available to them—all in a way that
can be interactively played, triggered and controlled live, on-stage. As an
electronic musician who works with such tools in an on-stage environment, I
can tell you that iOS literally changes the game into one that’s far easier to see,
understand and interact with over its hardware counterpart.

THE iOS ON STAGE
Another huge advancement in wireless control comes in the form of the iOS-
based live sound controller (Figures 10.13 and 10.14). These devices are literally
changing the way live sound hands are able to do their jobs. By giving a live
sound mixer the freedom to mix wirelessly from anywhere in the venue, he or
she can walk around the place and make any adjustments that are needed from
the middle of the audience, FOH (Front of House) position, virtually anywhere
(Figure 10.15). Stage monitor mixing (one using a completely separate mix or
sub-mix from the main FOH mixer) can now also be accomplished wirelessly
from a mix app. Depending upon the size and scale of the performance and
venue, these stage mixes can be performed by a dedicated monitor mix person
or by the performers themselves. As with all things wireless, it’s about freedom,
mobility and ﬂexibility.
345
The iOS in Music Production  CHAPTER 10
FIGURE 10.12
Touchable 3 can be used to
wirelessly control Ableton
Live in a practice and
performance setting.
(Courtesy of Zerodebug,
www.touch-able.net)
FIGURE 10.13
Mackie DC16 (iOS) and
ProDX8 mixing systems 
(for iOS and Android).
(Courtesy of Loud
Technologies, Inc.,
www.mackie.com)
FIGURE 10.14
StudioLive RML32AI
wireless mixing system 
for live and recorded sound.
(Courtesy of Presonus Audio
Electronics, Inc.,
www.presonus.com)

The iOS on Stage
346
FIGURE 10.15
Greg “Greedy” Williamson
mixing at Easy Street
Records in West Seattle.
(Courtesy of Greedtone,
www.greedtone.com)
iOS and the DJ
The modern DJ is deﬁnitely no stranger to the power and portability of the iOS.
Full sets can be easily pre-programmed or performed on-the-ﬂy from an iPad
or iPhone, especially when used in conjunction with external control hardware.
Offering up most or all of the control of a laptop DJ system, these devices give
the DJ full freedom to strut their stuff anywhere and anytime.
iOS AS A MUSICAL INSTRUMENT
Electronic instruments and music production tools allow us to quickly save and
work on musical ideas at the drop of a hat, or to integrate an instrument app
into our working studio environment, adding rich and complex musical
expression to a track, at a mere fraction of the cost of its hardware equivalent.
All of this started with GarageBand (Figure 10.16), a musical loop app that
allows audio and MIDI to be dragged into a project timeline in a quick and
easy way, without extensive musical experience. This app allowed musicians to
ﬁrst grasp the concept that the iOS could act as a serious musical instrument.
Offering up a wide range of electronic loops and beats, as well as a set of
reasonably good sounding virtual instruments (piano, guitar, bass, strings, etc.),
FIGURE 10.16
GarageBand for the Mac and
iOS. (Courtesy of Apple Inc.,
www.apple.com)

these instruments could be either sequenced from within the program itself, or
(with the use of a MIDI interface connection) could be played from an external
MIDI sequencer or controller source.
After GarageBand, individual developers and electronic instrument manu -
facturers quickly realized that there was a huge market for the recreation of
classic synths, new synth designs, groove synths, beat generators and other elec -
tronic instrument types that range from being super-simple in their operation,
to being sophisticated programs that equal or rival any hardware counterpart
(Figure 10.17).
347
The iOS in Music Production  CHAPTER 10
FIGURE 10.17
iOS music software. 
(a) Nanologue synth app. 
(b) LoopMash HD Groove
app. (Courtesy of Steinberg
Media Technologies 
GmbH, a division of 
Yamaha Corporation,
www.steinberg.net)
When using an iOS compatible audio and MIDI interface (or simply a camera
adapter/MIDI interface setup and the unit’s 1/8” headphone out jack), an iOS
device can be completely integrated into a DAW-based MIDI setup, literally
allowing the instrument to fully integrate into your musical system.
THE ABILITY TO ACCESSORIZE
Naturally, there are tons of additional accessories that can be added to customize
the look and functionality of your iOS device. These can include:
n Desk stand adapters
n Mic stand adapters
n Docking stations
n Interface systems and adapters
Of course, the list could go on for quite some time. Sufﬁce it to say, that if you
wanted blue ﬂames to shoot out of your device in a way that would spell your
name on the stage—just wait a few days and you’ll be able to get it from the
store. It’s all about customizing your system in a way that’ll make the device
uniquely yours.


It’s no secret that modern-day computers, smart phones, gamestations and even
smart televisions have gotten faster, sleeker, more touchable and sexier in their
overall design. In addition to their ability to act as a multifunctional production
workhorse, one of the crowning achievements of modern work and
entertainment devices is their networking and media integration, which has come
to be universally known by the household buzzword multimedia.
This combination of working and playing with multimedia has found its way
into modern media and computer culture through the use of various hardware
and software systems which combine in a multitasking environment to bring
you an experience that seamlessly involves such media types as:
n Audio and music
n Video and video streaming
n Graphics and gaming
n Musical Instrument Digital Interface (MIDI)
n Text and communication
The obvious reason for creating and integrating these media types is the human
desire to share and communicate one’s experiences with others. This has been
done for centuries in the form of books and, in relatively recent decades, through
movies and television. Obviously, in the here and now, the powerful and
versatile presence of the Internet can be placed at or near the top of this
communications list. Nothing allows individuals and corporate entities to reach
millions (or billions) so easily. Perhaps most importantly, the web is a
multimedia experience that each individual can manipulate, learn from and
even respond to in an interactive fashion. It has indeed unlocked the potential
for experiencing events and information in a way that makes each of us a
participant, and not just a passive spectator. To me, this is the true revolution
that’s occurring at the dawn of the twenty-ﬁrst century!
349
CHAPTER 11
Multimedia and 
the Web

The Multimedia Environment
THE MULTIMEDIA ENVIRONMENT
Although much of recording and music production has matured into a relatively
stable industry, the web, multimedia and the music industry itself is in a full-
speed-ahead tailspin of change. With online social media, on-demand video
and audio streaming, network communications, computer gaming and hundreds
of other media options entering into the marketplace on a weekly basis, it’s no
wonder that things are changing fast!
As with all things tech, I would say that the most important word in multimedia
technology today is “integration.” In fact, the perfect example of multimedia
today is in your pocket or purse. Your cell phone (handy, mobile or whatever
you call it) is a marvel of multimedia technology that can:
n Keep you in touch with friends
n Surf the web to ﬁnd the best restaurant in the area
n Connect to a web-based and/or actual GPS to keep you from getting lost
n Play or stream your favorite music
n Let you watch the latest movie or YouTube video
n Take pictures
n Light your way to the bathroom late at night or remotely turn your home’s
lights on or off
Of course, that’s just the short list. What more could you ever want from a
mobile on-the-go device? I don’t know, but we’re surely to ﬁnd out in the not-
too-distant-future; and, because of these everyday tools, we’ve come to expect
the same or similar experience from other such media devices as:
n Computers
n Televisions
n Cars
The Computer
Obviously, the tool that started the multimedia revolution is the computer. The
fact that it is a lean, mean multitasking machine makes it ideal for delivering
all of the media that we want, all of the time. From multimedia, to gaming, to
music, to video—if you have a good Internet connection—you can pretty much
hold the world in your hands (or at least your lap).
Television and the Home Theater
As you might expect, newer generations of video and home theater systems
incorporate more and more options for offering up “a rich multimedia
experience.” Newer TVs are able to directly connect to the web, allowing us to
stream our favorite movies or listen to Internet radio “on demand.”
So, what holds all of these various media devices together? Media, data distri -
bution and transmission formats!
350

DELIVERY MEDIA
Although media data can be stored and/or transmitted on a wide range of storage
devices, the most commonly found delivery media at the time of this writing
are the:
n Shared networks
n The web
n Physical media
Networking
At its most basic level, a shared data network is a collection of computers and
other hardware devices that are connected by protected data protocol links that
allow for the communication of shared resources, program apps and data. The
most well-known network communications protocols are the Ethernet (a
standard for creating Local Area Networks), and the Internet Protocol Suite
(otherwise known as the World Wide Web).
Within media production, a Local Area Network (or LAN) is a powerful tool that
allows multiple computers to be linked within a production facility. For example,
a central server (a dedicated data delivery and shared storage device) can be used
in a facility to store and share large amounts of data throughout a facility. Simpler
LAN connections could also be used in a project studio to link various computers,
so as to share media and backup data in a simple and cost-effective manner. For
example, a single, remote computer within a connected facility could be used to
store and share the large amounts of data that’s required for video, music and
sample library production, and then backup all of this data in a RAID (redundant
array of independent disks) system, allowing the data and backups to be
duplicated and stored on multiple hard drives (thereby reducing the chance of
system data loss).
In short, it’s always a good idea to become familiar with the strength, protection
and power that a properly designed network can offer a production facility, no
matter how big it is.
The Web
One of the most powerful aspects of multimedia is its ability to communicate
experiences either to another individual or to the masses. For this, you need a
very large network connection. The largest and most common network of all
is the Internet (World Wide Web). Here’s the basic gist of how this beast works:
n The Internet (Figure 11.1) can be thought of as a communications network
that allows your computer (or connected network) to be connected to an
Internet Service Provider (ISP) server (a specialized computer or cluster of
ISP computers that are designed to handle, pass and route data between
other network user connections).
351
Multimedia and the Web  CHAPTER 11

Delivery Media
352
n These ISPs are then connected (through specialized high-speed connec -
tions) to a series of network access points (NAPs), which essentially form
the connected infrastructure of the World Wide Web (www).
Therefore, in its most basic form, the www can be simply thought of as a uniﬁed
array of connected networks.
Internet browsers transmit and receive information on the web via a Uniform
Resource Locator (URL) address. This address is then broken down into three
parts: the protocol (e.g., http:// or https://), the server name (e.g., www.modrec.
com) and the requested page or ﬁle name (e.g., index.htm). The connected server
is able to translate the server name into a speciﬁc Internet Provider (IP) address,
which is then used to connect your computer to the desired server, after which
the requests to receive or send data are communicated and the information is
passed to your computer.
Email works in a similar data transfer fashion, with the exception that an 
email isn’t sent to or requested from a speciﬁc server; rather, it’s commun-
icated through a worldwide server network from one speciﬁc email address 
(e.g., myname@myprovider.com) directly to a destination email address 
(e.g., yourname@yourprovider.com).
THE CLOUD
One of the more current buzz terms on the web is “The Cloud” or “Cloud
Computing.” Put simply, storing data on the cloud refers to data that’s stored
on a remote server system or web-connected drive system. Most commonly,
that server would be operated and maintained by a company that will store
your data at a cost (although many services allow limited amounts of your
personal data to be stored for free). For example, cloud storage companies can:
n Store backup media online in a manual or automated way to keep your
ﬁles secure.
n Store huge amounts of uploadable video data online in a social media
context (i.e., YouTube).
n Store uploadable audio data online in a social media context for promoting
artists and DJs (i.e., BandCamp, ReverbNation, SoundCloud).
n Store program-related data online, reducing the need for installing
programs directly onto your computer.
n Store application and program data online, allowing the user to “subscribe”
to the use of their programs for a monthly or yearly fee.
FIGURE 11.1
The Internet works by
communicating requests
and data from a user’s
computer to connected
servers that are connected
to other network access
points around the world,
which are likewise
connected to other users’
computers.

PHYSICAL MEDIA
Although online data distribution and management is increasingly becoming
the primary way to get information from the distributor to the consumer,
physical media (you know, the kind that we can hold in our hands) still has a
very important role in delivering media to the consumer masses. Beyond the
fun stuff—like vinyl records—the most common physical media formats are
CDs, DVDs and Blu-ray discs.
The CD
Of course, one of the ﬁrst and most important developments in the mass
marketing and distribution of large amounts of digital media was the Compact
Disc (CD), both in the form of the CD-Audio and the CD-ROM. As most are
aware, the CD-Audio disc is capable of storing up to 74 minutes of audio at a
rate of 16 bits/44.1 kHz. Its close optical cousin, the CD-ROM, is most often
capable of storing 700 MB of graphics, video, digital audio, MIDI, text, and raw
data. Consequently, these pre-manufactured and user-encoded media are still
widely used to store large amounts of music, text, video, graphics, etc., largely
due to the fact that you can hold it in your hand and store it away in a safe
place. Table 11.1 details the various CD standards (often affectionately called
the “rainbow book”) that are currently in use.
It’s important to note that Red Book CDs (audio CDs) are capable of encoding
small amounts of user-data that can be used to encode imbedded metadata
(user information). This metadata (called CD-Text) allows the media creator to
enter and encode information such as artist, title, song number, track artist/title,
etc.—all within the disc itself. Since most CD players, computers and media
players are capable of reading this information, it’s always a good idea to
provide the listener with as much information as possible.
Another system for identifying CD and disc-related data is provided by Gracenote
(formerly CDDB or Compact Disc Data Base). In short, Gracenote maintains
and licenses an Internet database that contains CD info, text and images. Many
media devices that are connected to the web are able to access this database
and display the information on your player or device.
The DVD
Similar to their cousin, DVDs (which, after a great deal of industry deliberation,
simply stands for “DVD”) can contain any form of data. These discs are capable
of storing up to 4.7 gigabytes (GB) within a single-sided disc or 8.5 GB on a
double-layered disc. This capacity makes the DVD the good delivery medium
for encoding video (generally in the MPEG-2 encoding format), data-intensive
games, DVD-ROM titles and program installation discs. The increased demand
for multimedia games, educational products, etc., has spawned the computer-
related industry of CD and DVD-ROM authoring. The term authoring refers to
the creative, design and programming aspects of putting together a CD or DVD
353
Multimedia and the Web  CHAPTER 11

Physical Media
354
Format
Description
Red Book
Audio-only standard; also called CD-A (Compact Disc
Audio)
Yellow Book
Data-only format; used to write/read CD-ROM data
Green Book
CD-I (Compact Disc Interactive) format; never gained mass
popularity
Orange Book
CD-R (Compact Disc Recordable) format
White Book
VCD (Video Compact Disc) format for encoding CD-A audio
and MPEG-1 or MPEG-2 video data; used for home video
and karaoke
Blue Book
Enhanced Music CD format (also known as CD Extra or
CD+) can contain both CD-A and data
ISO-9660
A data ﬁle format that’s used for encoding and reading data
from CDs of all types across platforms
Joliet
Extension of the ISO-9660 format that allows for up to 64
characters in its ﬁle name (as opposed to the 8 ﬁle + 3
extension characters allowed by MS-DOS)
Romeo
Extension of the ISO-9660 format that allows for up to 128
characters in the ﬁle name
Rock Ridge
Unix-style extension of the ISO-9660 format that allows for
long ﬁle names
CD-ROM/XA
Allows for extended usage for the CD-ROM format—Mode-1
is strictly Yellow Book, while Mode-2 Form-1 includes error
correction and Mode-2 Form-2 doesn’t allow for error
correction; often used for audio and video data
CD-RFS
Incremental packet writing system from Sony that allows
data to be written and rewritten to a CD or CD-RW (in a way
that appears to the user much like the writing/retrieval of
data from a hard drive)
CD-UDF
UDF (Universal Disc Format) is an open incremental packet
writing system that allows data to be written and rewritten to
a CD or CD-RW (in a way that appears to the user much like
the writing/retrieval of data from a hard drive) according to
the ISO-13346 standard
HDCD
The High-Deﬁnition Compatible Digital system adds 6 dB of
gain to a Red Book CD (when played back on an HDCD-
compatible player) through the use of a special companion
mastering technique
Macintosh HFS
An Apple ﬁle system that supports up to 31 characters in a
ﬁle name; includes a data fork and a resource fork that
identify which application should be used to open the ﬁle
Table 11.1
CD Format Standards

Blu-ray
Although similar in size to the CD and DVD, a Blu-ray disc can store up to
25GB of media-related data onto each data layer (50GB for a dual-layer 
disc). In addition to most of the standard video formats that are commonly
encoded onto a DVD, the Blu-ray format can play back both compressed and
non-compressed PCM audio (Table 11.3) in a multi-channel, high-resolution
environ ment.
The Flash Card and Memory USB Stick
In our on-the-go world, another useful media device is the ﬂash memory card.
More speciﬁcally, the SD (secure digital) card typically ranges in size up to 128
Gb in capacity and comes in various physical sizes. These media cards can be
used for storing audio, video, photo, app and any other type of digital info that
can be used with your laptop, phone, car player, Blu-ray player—you name it!
355
Multimedia and the Web  CHAPTER 11
Format
Sample 
Bit Rate
Bit/s
Ch
Common 
Compression
Rate (kHz)
Format
PCM
48, 96
16, 20, 24
Up to 
1 to 8
48 kHz, 16 bit
None
6.144 Mbps
AC3
48
16, 20, 24
64 to 
1 to 6.1
192 kbps, 
AC3 and 384 kbps, 
448 kbps
stereo
448 kbps
DTS
48, 96
16, 20, 24
64 to 
1 to 7.1
377 or 754 
DTS coherent 
1536 kbps
kbps for stereo 
acoustics
and 754.5 or 
1509.25 kbps 
for 5.1
MPEG-2
48
16, 20
32 to 912 
1 to 7.1
Seldom 
MPEG
kbps
used
MPEG-1
48
16, 20
384 kbps
2
Seldom 
MPEG
used
SDDS
48
16
Up to 
5.1, 7.1
Seldom 
ATRAC
1289 kbps
used
Table 11.2
DVD Video/Audio Formats
project. At its most basic level, a project can be authored, mastered and burned
to disc from a single commercial authoring program. Whenever the stakes are
higher, trained professionals and expensive systems are often called in to
assemble, master and produce the ﬁnal disc for mass duplication and pack-
aging. Table 11.2 details the various DVD video/audio formats that are currently 
in use.

Media Delivery Formats
356
MEDIA DELIVERY FORMATS
Now that we’ve taken a look at the various delivery media, the next most
important aspect of delivering the multimedia experience rests with the data
distribution formats (the coding and technical aspects of data delivery)
themselves.
When creating content for the various media systems, it’s extremely important
that the media format and bandwidth be matched with the requirements of the
content delivery system that’s being used. In other words, it’s always smart to
maximize the efﬁciency of the message (media format and required bandwidth)
to match (and not alienate) your intended audience. The following section
outlines many standard and/or popular formats for delivering media to a target
audience.
Uncompressed Sound File Formats
Digital audio is obviously a component that adds greatly to the multimedia
experience. It can augment a presentation by adding a dramatic music soundtrack,
help us to communicate through speech or give realism to a soundtrack by adding
sound effects. Because of the large amounts of data required to pass video, graphics
and audio from a disc, the Internet or other media, the bit- and sample-rate
structure of an uncompressed audio ﬁle is usually limited compared to that of
a professional-quality sound ﬁle. At the “lo-ﬁ” range, the generally accepted sound
ﬁle standard for older multimedia production is either 8-bit or 16-bit audio at
a sample rate of 11.025 or 22.050 kHz. This standard came about mostly because
LPCM
Dolby 
Dolby
Dolby
DTS
DTS-HD 
DRA
DRA 
Digital
Digital 
TrueHD
Digital
Master
Extension
Plus
(Lossless)
Surround
Audio 
(Lossless)
Max. 
27.648 
640 
4.736
18.64
1.524
24.5
1.5
3.0
Bitrate
Mbit/s
kbit/s
Mbit/s
Mbit/s
Mbit/s
Mbit/s
Mbit/s
Mbit/s
Max. 
8
5.1
7.1
8
5.1
8
5.1
7.1
Channel
(48 kHz, 
(48 kHz, 
(48 kHz, 
96 kHz)
96 kHz),
96 kHz),
6 (192 kHz)
6 (192 kHz)
6 (192 kHz)
Bits/
16, 20, 24
16, 24
16, 24
16, 24
16, 20, 24
16, 24
16
16
sample
Sample 
48 kHz, 
48 kHz
48 kHz
48 kHz, 
48 kHz
48 kHz, 
48 kHz
48 kHz, 
frequency 96 kHz, 
96 kHz, 
96 kHz, 
96 kHz
192 kHz
192 kHz
192 kHz
Table 11.3
Speciﬁcation of BD-ROM Primary audio streams

older CD drive and processor systems generally couldn’t pass the professional
rates of 44.1 kHz and higher. With the introduction of faster processing systems
and better hardware, these limitations have generally been lifted to include
16/44.1 (16bit/44.1 kHz), 24/44.1 and as high as 24/192. Obviously there are
limitations to com mun icating uncompressed professional-rate sound ﬁles over
the Internet or from an optical disc that’s also streaming full-motion video.
Fortunately, with improvements in codec (encode/decode) techniques, hardware
speed and design, the overall sonic and production quality of compressed audio
data has greatly improved.
PCM AUDIO FILE FORMATS
Although several formats exist for encoding and storing sound ﬁle data, only a
few have been universally adopted by the industry. These standardized formats
make it easier for ﬁles to be exchanged between compatible media devices.
In audio, Pulse-Code Modulation (PCM) is the standard system for encoding,
storing and decoding audio. Within a PCM stream, the amplitude of the analog
signal is sampled at precise intervals, with each sample being quantized to the
nearest value within a range of digital steps. This level (amplitude) is then
sampled at precise time intervals (frequency), so as to represent analog audio
in a numeric form.
Probably the most common ﬁle type is the Wave (or .wav) format. Developed
for the Microsoft Windows format, this universal ﬁle type supports both mono
and stereo ﬁles at a wide range of uncompressed resolutions and sample rates.
Wave ﬁles contain PCM coded audio that follows the Resource Information File
Format (RIFF) spec, which allows extra user information to be embedded and
saved within the ﬁle itself. The newly adopted Broadcast Wave format, which has
been adopted by the Producers and Engineers wing (www.grammypro.com/
producers-engineers-wing) as the preferred sound ﬁle format for DAW pro -
duction and music archiving, allows for timecode-related positioning inform -
ation to be directly embedded within the sound ﬁle’s data stream.
In addition to the .wav format, the Audio Interchange File (AIFF; .aif) format
is commonly used to encode digital audio within Apple computers. Like Wave
ﬁles, AIFF ﬁles support mono or stereo, 8-bit, 16-bit and 24-bit audio at a wide
range of sample rates—and like Broadcast Wave ﬁles, AIFF ﬁles can also contain
embedded text strings. Table 11.4 details the differences between uncompressed
ﬁle sizes as they range from the 24-bit/192-kHz rates, all the way down to lo-
voice quality 8-bit/10-kHz ﬁles.
DSD AUDIO
Direct Streaming Digital (DSD) was a joint venture between Sony and Phillips
for encoding audio onto the Super Audio CD (SACD). Although the SACD has
fallen out of favor (with the Blu-ray format’s wide acceptance), the audio format
itself survives as a high-resolution audio format.
357
Multimedia and the Web  CHAPTER 11

Media Delivery Formats
358
Sample Rate
Word Length
No. of 
Date Rate 
MB/min
MB/hour
Channels
(kbps)
192
24
2
1152
69.12
4147.2
192
24
1
576
34.56
2073.6
96
32
2
768
46.08
2764.8
96
32
1
384
23.04
1382.4
96
24
2
576
34.56
2073.6
96
24
1
288
17.28
1036.8
48
32
2
384
23.04
1382.4
48
32
1
192
11.52
691.2
48
24
2
288
17.28
1036.8
48
24
1
144
8.64
518.4
48
16
2
192
11.52
691.2
48
16
1
96
5.76
345.6
44.1
32
2
352
21.12
1267.2
44.1
32
1
176
10.56
633.6
44.1
24
2
264
15.84
950.4
44.1
24
1
132
7.92
475.2
44.1
16
2
176
10.56
633.6
44.1
16
1
88
5.28
316.8
32
16
2
128
7.68
460.8
32
16
1
64
3.84
230.4
22
16
2
88
5.28
316.8
22
16
1
44
2.64
158.4
22
8
1
22
1.32
79.2
11
16
2
44
2.64
158.4
11
16
1
22
1.32
79.2
11
8
1
11
0.66
39.6
Table 11.4
Audio Bit Rate and File Sizes

359
Multimedia and the Web  CHAPTER 11
Unlike PCM, DSD makes use of Pulse-Density Modulation (Figure 11.2) to
encode audio. That’s to say that it doesn’t follow PCMs system of the periodic
sampling of audio at a speciﬁc rate; rather, the level and change of relative gain
levels over time is a result of the density of the bits within the stream. A stream
that has all 0s will have no level at that point in time, while one having all 1s
will have a maximum voltage level. The density of 1s to 0s will determine the
overall change in gain over time at a sampling rate of 2.8224 MHz (or 64 times
the 44.1 kHz sample rate), 5.6448 MHz (DSD128) or higher. Currently, only
a few DAWs are able to work natively in the DSD modulation code.
Compressed Codec SoundFile Formats
As was mentioned earlier, high-quality uncompressed sound ﬁles often present
severe challenges to media delivery systems that are restricted in terms of
bandwidth, download times or memory storage. Although the streaming of
audio data from various media and high-bandwidth networks (including the
web) has improved over the years, memory storage space and other bandwidth
limitations have led to the popular acceptance of compressed audio data formats
known as codecs. These formats can encode audio in a manner that reduces
data ﬁle size and bandwidth requirements and then decode the information
upon playback using a system known as perceptual coding.
PERCEPTUAL CODING
The central idea behind perceptual coding is the psychoacoustic principle that
the human ear will not always be able to hear all of the information that’s
present in a recording. This is largely due to the fact that louder sounds will
often mask sounds that are both lower in level and relatively close in frequency
to another louder signal. These perceptual coding schemes take advantage of
this masking effect by ﬁltering out noises and sounds that can’t be detected by
our ears and removes them from the encoded audio stream.
The perceptual encoding process is said to be “lossy,” because once the ﬁltered
data has been taken away it can’t be replaced or introduced back into the ﬁle.
For the purposes of audio quality, the amount of perceived data compression
reduction can be selected by the user during the encoding process. Higher
bandwidth compression rates will remove less data from a stream (resulting in
a reduced amount of ﬁltering and higher audio quality), while low bandwidth
rates will greatly reduce the data stream (resulting in smaller ﬁle sizes, increased
FIGURE 11.2
Uncompressed audio
coding. (a) PCM encodes the
absolute level at that
sample period and stores
that number within memory.
(b) DSD, on the other hand,
does not encode the level
within samples and words,
but encodes the “density” of
1s to 0s within a period of
time to determine the
signal’s level . . . there is no
coding, per se.

Media Delivery Formats
ﬁltering, increased artifacts and lower audio quality). The amount of ﬁltering
that’s to be applied to a ﬁle will depend on the intended audio quality and the
delivery medium’s bandwidth limitations. Due to the lossy character of these
encoded ﬁles, it’s always a good idea to keep a copy of the original,
uncompressed sound ﬁle in a data archive backup, should changes in content
or future technologies occur (never underestimate Murphy’s law).
Many of the listed codecs are capable of encoding and decoding audio using a
constant bit rate (CBR) and variable bit rate (VBR) structure:
n CBR encoding is designed to work effectively in a streaming scenario
where the end user’s bandwidth is a consideration. With CBR encoding,
the chosen bit rate will remain constant over the course of the ﬁle or stream.
n VBR encoding is designed for use when you want to create a downloadable
ﬁle that has a smaller ﬁle size and bit rate without sacriﬁcing sound and
video quality. This is carried out by detecting which sections will need the
highest bandwidth and adjusting the encode process accordingly. When
lower rates will sufﬁce, the encoder adjusts the processing to match the
content. Under optimum conditions, you might end up with a VBR-
encoded ﬁle that has the same quality as a CBR-encoded ﬁle, but is only
half the ﬁle size.
Perceptual coding schemes that are in most common use include:
n MP3
n MP4
n WMA
n AAC
n RealAudio
n FLAC
MP3
MPEG (which is pronounced “M-peg” and stands for the Moving Picture Experts
Group; www.mpeg.org) is a standardized format for encoding digital audio into
a compressed format for the storage and transmission of various media over
the web. As of this writing, the most popular format is the ISO-MPEG Audio
Level-2 Layer-3, commonly referred to as MP3. Developed by the Fraunhofer
Institute (www.iis.fraunhofer.de) and Thomson Multimedia in Europe, MP3 
has advanced the public awareness and acceptance of compressing and distrib -
uting digital audio by creating a codec that can compress audio by a substan-
tial factor while still maintaining quality levels that approach those of a CD
(depending on which compression levels are used). Although a wide range of
compression rates can be chosen to encode/decode an MP3 ﬁle, the most
common rate for the general masses is 128 kbps (kilo bits per second). Although
this rate is deﬁnitely “lossy” (containing increased distortion, reduced bandwidth
360
MP3 

361
Multimedia and the Web  CHAPTER 11
and sideband artifacts), it allows us to literally put thousands of songs on an
on-the-go player. Higher rates of 160, 192 and 320 kbps offer higher “near CD
sound quality” with the obvious tradeoff being larger ﬁle sizes.
Although faster web connections are capable of streaming MP3 (and higher
rates) in real time, this format is most often downloaded to the end consumer
for storage to disk, disc and SD media for the storage and playback of down -
loaded songs. Once saved, the data can then be transferred to playback devices
(such as phones, pads, etc.). In fact, billions of music tracks are currently being
downloaded every month on the Internet using MP3, practically every personal
computer contains licensed MP3 software and virtually every song has been
encoded into this format—it’s actually hard to imagine how many players there
are out there on the global market. This makes it the web’s most popular audio
compression format by far . . . although, as this book goes to press, Fraunhofer
has decided to stop its support and licensing for this codec.
MP4
Like MP3, the MPEG-4 (MP4) codec is largely used for streaming media data
over the web or for viewing media over portable devices. MP4 is largely based
on Apple’s QuickTime “MOV” format and can be used to encode A/V and audio
only content over a wide range of bitrate qualities with both stereo and
multichannel (surround) capabilities. In addition, this format can employ DRM
(copy protection), so as to restrict copying of the downloaded ﬁle.
WMA
Developed by Microsoft as their corporate response to MP3, Windows Media
Audio (WMA) allows compression rates to encode high-quality audio at low
bit-rate and ﬁle-size settings. Designed for ripping (extracting audio from a CD)
and sound ﬁle encoding/playback from within the popular Window’s Media
Player (Figure 11.3), this format has grown and then fallen in general acceptance
and popularity. In addition to its high quality at low bit rates, WMA also allows
for a wide range of bitrate qualities with both stereo and multichannel
(surround) capabilities, while being able to imbed DRM (copy protection), so
as to restrict copying of the downloaded ﬁle.
FIGURE 11.3
Windows Media Player.
MP4 
WMA 

Media Delivery Formats
AAC
Jointly developed by Dolby Labs, Sony, ATT, and the Fraunhofer Institute, the
Advanced Audio Coding (AAC) scheme is touted as a multichannel-friendly
format for secure digital music distribution over the Internet. Stated as having
the ability to encode CD-quality audio at lower bit rates than other coding
formats, AAC not only is capable of encoding 1, 2 and 5.1 surround sound ﬁles
but can also encode up to 48 channels within a single bitstream at bit/sample
rates of up to 24/96. This format is also SDMI (Secure Digital Music Initiative)
compliant, allowing copyrighted material to be protected against unauthorized
copying and distribution. AAC is the default or standard audio format for
YouTube, iPhone, iPod, iPad, iTunes (it’s the backbone of Apple’s music and
audio media distribution), Nintendo DSi, Nintendo 3DS, DivX Plus Web Player
and PlayStation 3.
FLAC
FLAC (Free Lossless Audio Codec) is a format that makes use of a data
compression scheme that’s capable of reducing an audio ﬁle’s data size by 40%
to 50%, while playing back in a lossless fashion that maintains the sonic
integrity of the original stereo and multichannel source audio (up to 8 channels
bit depths of up to 32 bits at sample rates that range to 655.350 kHz). As the
name suggests, FLAC is a free, open-source codec that can be used by software
developers in a royalty-free fashion.
With the increase in memory storage size and higher download speeds, many
enthusiasts in audio are beginning to demand higher playback quality. As such,
FLAC is growing in popularity as a medium for playing back high quality, loss-
less audio, both in stereo and in various surround formats.
TAGGED METADATA
Within most types of multimedia ﬁle formats it’s possible to embed a wide
range of content identiﬁer data directly into the ﬁle itself or within a web-related
page or event. This “tagged” data (also known as metadata) can identify and
provide extensive and extremely important information that relates to the
content of the ﬁle. For example, let’s say that little Sally is looking to ﬁnd a
song that she wants to download from her favorite artist. Now, Sally consumes
a lot of music and she goes to iTunes to download that song into her phone.
So, how can she ﬁnd her favorite needle in a digital haystack? By searching for
songs under “Mr. Right,” the site is able to ﬁnd several of his latest songs and
BOOM—she’s groovin’ to the beat—all thanks to metadata.
Now that she knows the name of the song, she can ﬁnd it under “Mr. Right”
in her player and she’s groovin’ on the underground or heading to school. On
the ﬂip side, if the song name hasn’t been entered (or was incorrectly entered)
into the metadata database, poor Sally’s song would literally get lost in the
shufﬂe. Sally would be bummed and the record label/artist would lose the sale.
362
AAC 
FLAC 

363
Multimedia and the Web  CHAPTER 11
On another day, let’s say that Sally really wanted to buy a new song. She could
enter her favorite music genre into the ﬁeld and look through the songs that have
been properly tagged with that genre. By clicking the “sounds like” button, songs
that have been tagged in the same genre could pop up that might completely
ﬂip her out—BOOM, again—a sale and (even better) a fan of a new artist is born,
all because the “sounds like” metadata was properly tagged by her new, favorite
band. Are you getting the idea that “tagging” a song, project or artist band data
with the proper metadata can be a make or break deal in the new digital age?
Metadata in all its media and website glory tells the world who it is, the song
title, what genre type, etc. It’s the gateway to telling the world “Hey, I’m here!
Listen to me!” Metadata can also be extracted from an audio using a central
music database and then be automatically entered into a music copy (ripping)
program (Figure 11.4).
FIGURE 11.4
Embedded metadata file
tags can be added to a
media file via various media
libraries, rippers or editors
and then viewed by a media
player or file manager.
Due to the fact that there is no existing set of rules for ﬁlling out metadata,
folks who make extensive use of iTunes, Discogs and other music playlist services
often go nuts when the tags are incorrect, conﬂicting or non-standard. For
example, a user might download an album that might go by the artist name of
“XYZ Band” and then download another album that might have it listed as
“The Band XYZ.” One would show up in a player under “X” while the same
band would also show up under “T”—and it can get A LOT worse than that.
In fact, many people actually go so far as to manually ﬁx their library’s metadata
to their own liking. The moral of this story is to research your metadata and
stick to a single, consistent naming scheme.
It’s worth mentioning here that the Producers and Engineers wing of the
Grammys have been doing extensive work on the subject of metadata (www.
grammy.com/credits), so that tags can be more uniform and extensive, allowing
producer, engineer and other project-related data to be entered into the ofﬁcial
metadata. It is hoped that such credit documentation will help to get royalties
to those who legally deserve to be recognized and paid for their services in the
here-and-now or in the future (when new payment legislations might be passed).
MIDI
One of the unique advantages of MIDI as it applies to multimedia is the rich
diversity of musical instruments and program styles that can be played back in
real time, while requiring almost no overhead processing from the computer’s

MIDI
CPU. This makes MIDI a perfect candidate for playing back soundtracks from
multimedia games or from a phone (MIDI ringtone), Internet, gaming devices,
etc.. As one might expect, MIDI has taken a back seat to digital audio as a serious
music playback format for multimedia. Most likely, this is due to several factors,
including:
n A basic misunderstanding of the medium
n The fact that producing MIDI content often requires a fundamental
knowledge of music
n The frequent difﬁculty of synchronizing digital audio to MIDI in a
multimedia environment
n The fact that soundcards, phones, etc., often include poorly designed FM
synthesizers (although most operating systems now include higher quality
software synths)
Fortunately, a number of companies have taken up the banner of embedding
MIDI within their media projects and Google’s Chrome now includes integrated
MIDI support within the browser itself. All of these factors have helped push
MIDI a bit more into the web mainstream. As a result, it’s becoming more
common for your PC to begin playing back a MIDI score on its own or perhaps
in conjunction with a game or more data-intensive program.
The following information relates to MIDI as it functions within the multimedia
environment. Of course, more in-depth information on the spec. and its use
can be found within Chapter 9: MIDI.
Standard MIDI Files
The accepted format for transmitting music-related data and real-time MIDI
information within multimedia (or between sequencers from different
manufacturers) is the standard MIDI ﬁle. This ﬁle type (which is labeled with
a .mid or .smf extension) is used to distribute MIDI data, song, track, time
signature and tempo information to the general masses. Standard MIDI ﬁles
can support both single and multichannel sequence data and can be loaded
into, edited and then directly saved from almost any sequencer package. When
exporting a standard MIDI ﬁle, keep in mind that they can come in two basic
ﬂavors—type 0 and type 1:
n Type 0 is used whenever all of the tracks in a sequence need to be merged
into a single MIDI track. All of the notes will have a channel number
attached to them (i.e., will play various instruments within a sequence);
however, the data will have no deﬁnitive track assignments. This type might
be the best choice when creating a MIDI sequence for a standard device
or the Internet (where the sequencer or MIDI player application might
not know or care about dealing with multiple tracks).
n Type 1, on the other hand, will retain its original track information
structure and can be imported into another sequencer type with its basic
track information and assignments left intact.
364

General MIDI
One of the most interesting aspects of MIDI production is the absolute
uniqueness of each professional and even semi-pro project studio. In fact, no
two studios will be even remotely alike (unless they’ve been speciﬁcally designed
to be the same or there’s a very unlikely coincidence). Each artist will have 
his or her own favorite equipment, supporting hardware, assigned patches and
way of routing channels/tracks. The fact that each system setup is unique and
personal has placed MIDI at odds with the need for complete compatibility in
the world of multimedia. For example, if you import a MIDI ﬁle over the Net
that’s been created in another studio, the song will most likely attempt to play
with a totally irrelevant set of sound patches (it might sound interesting, but it
won’t sound anything like it was originally intended). If the MIDI ﬁle is loaded
into completely different setups, the sequence will again sound completely
different, and so on.
To eliminate (or at least reduce) the basic differences that exist between systems,
a standardized set of patch settings, known as General MIDI (GM), was created.
In short, General MIDI assigns a speciﬁc instrument patch to each of the 128
available program change numbers. Since all electronic instruments that conform
to the GM format must use these patch assignments, placing GM program
change commands at the header of each track will automatically instruct the
sequence to play with its originally intended sounds and general song settings.
In this way, no matter what synth, sequencer and system setup is used to play
the ﬁle back, as long as the receiving instrument conforms to the GM spec, the
sequence will be heard using its intended instrumentation.
Tables 11.5 and 11.6 detail the program numbers and patch names that conform
to the GM format. These patches include sounds that include synthesizer sounds,
ethnic instruments and sound effects that have been derived from early Roland
synth patch maps. Although the GM spec states that a synth must respond to
all 16 MIDI channels, the ﬁrst 9 channels are reserved for instruments, while
GM restricts the percussion track to MIDI channel 10.
365
Multimedia and the Web  CHAPTER 11
1. Acoustic Grand Piano
44. Contrabass
87. Lead 7 (ﬁfths)
2. Bright Acoustic Piano
45. Tremolo Strings
88. Lead 8 (bass + lead)
3. Electric Grand Piano
46. Pizzicato Strings
89. Pad 1 (new age)
4. Honky-tonk Piano
47. Orchestral Harp
90. Pad 2 (warm)
5. Electric Piano 1
48. Timpani
91. Pad 3 (polysynth)
6. Electric Piano 2
49. String Ensemble 1
92. Pad 4 (choir)
7. Harpsichord
50. String Ensemble 2
93. Pad 5 (bowed)
8. Clavichord
51. SynthStrings 1
94. Pad 6 (metallic)
9. Celesta
52. SynthStrings 2
95. Pad 7 (halo)
Table 11.5
GM Nonpercussion Instrument (Program Change) Patch Map

MIDI
366
10. Glockenspiel
53. Choir Aahs
96. Pad 8 (sweep)
11. Music Box
54. Voice Oohs
97. FX 1 (rain)
12. Vibraphone
55. Synth Voice
98. FX 2 (soundtrack)
13. Marimba
56. Orchestra Hit
99. FX 3 (crystal)
14. Xylophone
57. Trumpet
100. FX 4 (atmosphere)
15. Tubular Bells
58. Trombone
101. FX 5 (brightness)
16. Dulcimer
59. Tuba
102. FX 6 (goblins)
17. Drawbar Organ
60. Muted Trumpet
103. FX 7 (echoes)
18. Percussive Organ
61. French Horn
104. FX 8 (sci-ﬁ)
19. Rock Organ
62. Brass Section
105. Sitar
20. Church Organ
63. SynthBrass 1
106. Banjo
21. Reed Organ
64. SynthBrass 2
107. Shamisen
22. Accordion
65. Soprano Sax
108. Koto
23. Harmonica
66. Alto Sax
109. Kalimba
24. Tango Accordion
67. Tenor Sax
110. Bag pipe
25. Acoustic Guitar (nylon)
68. Baritone Sax
111. Fiddle
26. Acoustic Guitar (steel)
69. Oboe
112. Shanai
27. Electric Guitar (jazz)
70. English Horn
113. Tinkle Bell
28. Electric Guitar (clean)
71. Bassoon
114. Agogo
29. Electric Guitar (muted)
72. Clarinet
115. Steel Drums
30. Overdriven Guitar
73. Piccolo
116. Woodblock
31. Distortion Guitar
74. Flute
117. Taiko Drum
32. Guitar Harmonics
75. Recorder
118. Melodic Tom
33. Acoustic Bass
76. Pan Flute
119. Synth Drum
34. Electric Bass (ﬁnger)
77. Blown Bottle
120. Reverse Cymbal
35. Electric Bass (pick)
78. Shakuhachi
121. Guitar Fret Noise
36. Fretless Bass
79. Whistle
122. Breath Noise
37. Slap Bass 1
80. Ocarina
123. Seashore
38. Slap Bass 2
81. Lead 1 (square)
124. Bird Tweet
39. Synth Bass 1
82. Lead 2 (sawtooth)
125. Telephone Ring
40. Synth Bass 2
83. Lead 3 (calliope)
126. Helicopter
41. Violin
84. Lead 4 (chiff)
127. Applause
42. Viola
85. Lead 5 (charang)
128. Gunshot
43. Cello
86. Lead 6 (voice)
Table 11.5
continued

367
Multimedia and the Web  CHAPTER 11
35.
Acoustic Bass Drum
51.
Ride Cymbal 1
67.
High Agogo
36.
Bass Drum 1
52.
Chinese Cymbal
68.
Low Agogo
37.
Side Stick
53.
Ride Bell
69.
Cabasa
38.
Acoustic Snare
54.
Tambourine
70.
Maracas
39.
Hand Clap
55.
Splash Cymbal
71.
Short Whistle
40.
Electric Snare
56.
Cowbell
72.
Long Whistle
41.
Low Floor Tom
57.
Crash Cymbal 2
73.
Short Guiro
42.
Closed Hi-Hat
58.
Vibraslap
74.
Long Guiro
43.
High Floor Tom
59.
Ride Cymbal 2
75.
Claves
44.
Pedal Hi-Hat
60.
Hi Bongo
76.
Hi Wood Block
45.
Low Tom
61.
Low Bongo
77.
Low Wood Block
46.
Open Hi-Hat
62.
Mute Hi Conga
78.
Mute Cuica
47.
Low-Mid Tom
63.
Open Hi Conga
79.
Open Cuica
48.
Hi-Mid Tom
64.
Low Conga
80.
Mute Triangle
49.
Crash Cymbal 1
65.
High Timbale
81.
Open Triangle
50.
High Tom
66.
Low Timbale
Note: In contrast to Table 11.5, the numbers in Table 11.6 represent the percussion keynote numbers on a MIDI keyboard, not
program change numbers.
Table 11.6
GM Percussion Instrument (Program Key Number) Patch Map (Channel 10)
GRAPHICS
Graphic imaging occurs on the computer screen in the form of pixels. These
are basically tiny dots that blend together to create color images in much the
same way that dots are combined to give color and form to your favorite comic
strip. Just as word length affects the overall amplitude range of a digital audio
signal, the number of bits in a pixel’s word will affect the range of colors that
can be displayed in a graphic image. For example, a 4-bit word only has 16
possible combinations. Thus, a 4-bit word will allow your screen to have a total
of 16 possible colors; an 8-bit word will yield 256 colors; a 16-bit word will
give you 65,536 colors; and a 24-bit word will yield a whopping total of 16.7
million colors! These methods of displaying graphics onto a screen can be
broken down into several categories:
n Raster graphics: In raster graphics, each image is displayed as a series of
pixels. This image type is what is utilized when a single graphic image is
used (i.e., bitmap, JPEG, GIF, PNG or TIFF format). The sense of motion
can come from raster images only by successively stepping through a
number of changing images every second (much in the same way that
standard video images create the sense of motion).

Graphics
n Vector graphics: This method displays still graphic drawings using geometric
shapes (lines, curves and other shapes) that are placed at speciﬁc
coordinates on the screen. Being assigned coordinates, shape, thickness
and ﬁll, these shapes combine together to create an image that can be
simple or complex in nature. This script form reduces a ﬁle’s data size
dramatically and is used with several image programs.
n Vector animation: Much like vector graphics, vector animation can make
use of the above shapes, thickness, ﬁlls, shading and computer-generated
lighting to create a sense of complex motion that moves from frame to
frame (often with a staggering degree of realism). Obviously, with the
increased power of modern computers and supercomputers, this graphic
art form has attained higher degrees of artistry or realism within modern-
day ﬁlm and gaming production and design.
VIDEO
With the proliferation of computers, DVD/Blu-ray players, cell phones, video
interface hardware and editing software systems, desktop and laptop video has
begun to play an increasingly important role in home and corporate multi-
media production and content. In short, video is encoded into a data stream
as a continuous series of successive frames, which are refreshed at rates that
vary from 12 or fewer frames/second (fr/sec) to the standard broadcast rates of
29.97 and 30 fr/sec (or higher for 3D applications). As with graphic ﬁles, a
single full-sized video frame can be made up of a gazillion pixels, which are
themselves encoded as a digital word of n bits. Multiply these ﬁgures by nearly
30 frames and you’ll come up with rather impressive data ﬁle size and
throughput rates.
Obviously, it’s more common to ﬁnd such ﬁle sizes and data throughput rates
on higher-end desktop systems and professional video editing workstations;
however, several options are available to help bring video down to data rates
that are suitable for the Internet and multimedia:
n Window size: The basics of making the viewable picture smaller are simple
enough: Reducing the frame size will reduce the number of pixels in a
video frame, thereby reducing the overall data requirements during
playback.
n Frame rate: Although standard video frame rates run at around 30 
fr/sec (United States and Japan) and 25 fr/sec (Europe), these rates can
be lowered to 12 fr/sec in order to reduce the encoded ﬁle size or
throughput.
n Compression: In a manner similar to that which is used for audio,
compression codecs can be applied to a video frame to reduce the amount
of data that’s necessary to encode the ﬁle. This is done by ﬁltering out and
smoothing over pixel areas that consume data or by encoding data that
doesn’t change from frame to frame into shorthand that reduces data
368

369
Multimedia and the Web  CHAPTER 11
throughput. In situations where high levels of compression are needed,
it’s common to accept degradations in the video’s resolution in order to
reduce the ﬁle size and/or data throughput to levels that are acceptable
for a restrictive medium (e.g., the web).
From all of this, it’s clear that there are many options for encoding a desktop
video ﬁle. When dealing with video clips, tutorials and the like, it’s common
for the viewing window to be medium in size and encoded at a medium to
lower frame rate. This middle ground is often chosen in order to accommodate
the standard data throughput that can be streamed off most of the web. These
ﬁles are commonly encoded using Microsoft’s Audio-Video Interleave (AVI)
format, QuickTime (a common codec developed by Apple that can be played
by either a Mac or PC) or MPEG 1, 2 or 4 (codecs that vary from lower
multimedia resolutions to higher ones that are used to encode DVD movies).
Both the Microsoft Windows and Apple OS platforms include built-in or easily
obtained applications that allow all or most of these ﬁle types to be played
without additional hardware or software.
MULTIMEDIA IN THE “NEED FOR SPEED” ERA
The household phrase “surﬁn’ the web” has become synonymous with jump-
ing onto the Net, browsing the sites and grabbing all of those hot songs, videos,
and graphics that might wash your way. With improved audio and video codecs
and ever-faster data connections (Table 11.7), the ability to search on any
subject, download ﬁles, and stream audio or radio stations from any point in
the world has deﬁnitely changed our perception of modern-day communi -
cations.
Connection
Speed (bps)
Description
56k dial-up
56 Kbps (usually less)
Common modem connection
ISDN
128 Kbps (older technology)
ISDN PRI/E1
1.5 Mbps/1.9 Mbps
DSL
256 Kbps to 20 Mbps
Cable
Up to 85 Mbps
OC-1
52 Mbps
Optical ﬁber
OC-3
155 Mbps
Optical ﬁber
OC-12
622 Mbps
Optical ﬁber
OC-48
2.5 Gbps
Optical ﬁber
Ethernet
10 Mbps (older technology) 
Local-Area Network (LAN), 
up to 100 Gbps
not an Internet connection
Table 11.7
Internet Connection Speeds

ON A FINAL NOTE
One of the most amazing things about multimedia, cyberspace and their related
technologies is the fact that they’re ever-changing. By the time you read this
book, many changes will have occurred. Old concepts will have faded away and
new—possibly better—ones will take over and then begin to take on a new life
of their own. Although I’ve always had a fascination with crystal balls and have
often had a decent sense about new trends in technology, there’s simply no
way to foretell the many amazing things that lie ahead in the ﬁelds of music,
music technology, gaming, visual media, multimedia and especially cyberspace.
As with everything techno, I encourage you to read the trades and surf the web
to keep abreast of the latest and greatest tools that have recently arrived, or are
on the horizon.
370
On a Final Note

Of course, it’s a safe bet to say that music and audio itself, in all its various
forms, is an indispensable part of almost all types of media production. In video
postproduction, digital video editors, digital audio workstations (DAWs), audio
and video transports, automated console systems and electronic musical
instruments routinely work together to help create a ﬁnished soundtrack (Figure
12.1). The underlying technology that allows multiple audio and visual media
to operate in tandem (so as to maintain a direct time relationship) is known
as synchronization or sync.
Strictly speaking, synchronization occurs when two or more related events
happen at precisely the same relative time. With respect to analog audio and
video systems, sync is achieved by interlocking the transport speeds of two or
more machines. For computer-related systems (such as digital video, digital
audio and MIDI), synchronization between devices is often achieved through
the use of a timing clock that can be fed through a separate line or is directly
FIGURE 12.1
Example of an integrated
audio production system.
371
CHAPTER 12
Synchronization

Synchronization
372
embedded within the digital data line itself. Within such an environment, it’s
often necessary for analog and digital devices to be synchronized together;
resulting in a number of ingenious forms of communication and data translation
systems. In this chapter, we’ll explore the various forms of synchronization used
for both digital and analog devices, as well as current methods for maintaining
sync between media types.
TIMECODE
Maintaining relative sync between media devices doesn’t require that all transport
speeds involved in the process be constant; however, it’s critical that they
maintain the same relative speed and position over the course of a program.
Physical analog devices, for example, have a particularly difﬁcult time achieving
this. Due to differences in mechanical design, voltage ﬂuctuations and tape
slippage, it’s a simple fact of life that analog tape devices aren’t able to maintain
a constant playback speed, even over relatively short durations. For this reason,
accurate sync between analog and digital machines would be nearly impossible
to achieve over any reasonable program length without some form of timing
lock. It therefore quickly becomes clear that if production is to utilize, multiple
forms of media and record/playback sync is essential.
The standard method of interlocking audio, video and ﬁlm transports makes
use of a code that was developed by the Society of Motion Picture and Television
Engineers (SMPTE, www.smpte.org). This timecode (or SMPTE timecode)
identiﬁes an exact position within recorded media or onto tape by assigning a
digital address that increments over the course of a program’s duration. This
address code can’t slip in time and always retains its original location, allowing
for the continuous monitoring of tape position to an accuracy of between
1/24th and 1/30th of a second (depending on the media type and frame rates
being used). These divisional segments are called frames, a term taken from ﬁlm
production. Each audio or video frame is tagged with a unique identifying
number, known as a “timecode address.” This eight-digit address is displayed
in the form 00:00:00:00, whereby the successive pairs of digits represent
hours:minutes:seconds:frames or HH:MM:SS:FF (Figure 12.2).
FIGURE 12.2
Readout of a SMPTE
timecode address in
HH:MM:SS:FF.
The recorded timecode address is then used to locate a position on the hard
disk, magnetic tape or any other recorded media, in much the same way that
a letter carrier uses a written address to match up, locate and deliver a letter to
a speciﬁc, physical residence (i.e., by matching up the address, you can then

ﬁnd the desired physical location point, as shown in Figure 12.3a). For example,
let’s suppose that a time-encoded analog multitrack tape begins at time
00:01:00:00, ends at 00:28:19:00 and contains a speciﬁc cue point (such as a
glass shattering) that begins at 00:12:53:19 (Figure 12.3b). By monitoring the
timecode readout, it’s a simple matter to locate the precise position that
corresponds to the cue point on the tape and then perform whatever function
is necessary, such as inserting an effect into the sound track at that speciﬁc
point—CRAAAASH!
Timecode Word
The total of all time-encoded information that’s encoded within each audio or
video sync frame is known as a timecode word. Each word is divided into 80
equal segments, which are numbered consecutively from 0 to 79. One word
covers an entire audio or video frame, such that for every frame there is a unique
and corresponding timecode address. Address information is contained in the
digital word as a series of bits that are made up of binary 1’s and 0’s.
In the case of an analog, a SMPTE signal is electronically encoded in the form
of a modulated square wave. This method of encoding information is known
as bi-phase modulation. Using this code type, a voltage or bit transition in the
middle of a half-cycle of a square wave represents a bit value of 1, while no
transition within this same period signiﬁes a bit value of 0 (Figure 12.4). The
most important feature about this system is that detection relies on shifts within
the pulse and not on the pulse’s polarity or direction. Consequently, timecode
can be read in either the forward or reverse play mode, as well as at fast or slow
shuttle speeds.
373
Synchronization  CHAPTER 12
FIGURE 12.3
Location of relative
addresses: (a) postal
address analogy; 
(b) timecode addresses 
and a cue point on
longitudinal tape.
FIGURE 12.4
Bi-phase modulation
encoding.

Timecode
374
1. Go to the Tutorial section of www.modrec.com,
click on SMPTE Audio Example and play the
timecode sound ﬁle. Not my favorite tune, but it’s
a useful one!
2. The 80-bit timecode word is subdivided into
groups of 4 bits (Figure 12.5), whereby each
grouping represents a speciﬁc coded piece of
information. Each 4-bit segment represents a
binary-coded decimal (BCD) number that ranges
from 0 to 9. When the full frame is scanned, all
eight of these 4-bit groupings are read out as a
single SMPTE frame number (in hours, minutes,
seconds and frames).
Try This: SMPTE Timecode
D I Y
 do  it  yourself
SYNC INFORMATION DATA
An additional form of information that’s encoded into the timecode word is
sync data. This information exists as 16 bits at the end of each timecode address
word. These bits are used to deﬁne the end of each frame. Because timecode
can be read in either direction, sync data is also used to tell the device which
direction the tape or digital device is moving.
FIGURE 12.5
Bi-phase representation of
the SMPTE timecode word.
TIMECODE FRAME STANDARDS
In productions using timecode, it’s important that the readout display be directly
related to the actual elapsed time of a program, particularly when dealing with
the exacting time requirements of broadcasting. Due to historical and technical
differences between countries, timecode frame rates may vary from one medium,
production house or region of origin to another. The following frame-rates are
available:
n 30 fr/sec (monochrome U.S. video): In the case of a black-and-white (mono -
chrome) video signal, a rate of exactly 30 frames per second (fr/sec) is
used. If this rate (often referred to as non-drop code) is used on a black
and-white program, the timecode display, program length and actual
clock-on-the-wall time would all be in agreement.
n 29.97 fr/sec (drop-frame timecode for color NTSC video): The simplicity of
30 fr/sec was eliminated, however, when the National Television Standards

Committee (NTSC) set the frame rate for the color video signal in the
United States and Japan at 29.97 fr/sec. Thus, if a timecode reader that’s
set up to read the monochrome rate of 30 fr/sec were used to read a color
program, the timecode readout would pick up an extra 0.03 frame for
every second that passes. Over the duration of an hour, the timecode
readout would differ from the actual elapsed time by a total of 108 frames
(or 3.6 seconds). To correct for this difference and bring the timecode
readout and the actual elapsed time back into agreement, a series of frame
adjustments was introduced into the code. Because the goal is to drop
108 frames over the course of an hour, the code used for color has come
to be known as drop-frame code. In this system, two frame counts for
every minute of operation are omitted from the code, with the exception
of minutes 00, 10, 20, 30, 40 and 50. This has the effect of adjusting the
frame count, so that it agrees with the actual elapsed duration of a program.
n 29.97 fr/sec (non-drop-frame code): In addition to the color 29.97 drop-
frame code, a 29.97 non-drop-frame color standard can also be found in
video production. When using non-drop timecode, the frame count will
always advance one count per frame, without any drops. As you might
expect, this mode will result in a disagreement between the frame count
and the actual clock-on-the-wall time over the course of the program. Non-
drop, however, has the distinct advantage of easing the time calculations
that are often required in the video editing process (because no frame
compensations need to be taken into account).
n 25 fr/sec EBU (standard rate for PAL video): Another frame rate format that’s
used throughout Europe is the European Broadcast Union (EBU) time-
code. EBU utilizes SMPTE’s 80-bit code word but differs in that it uses a
25 fr/sec frame rate. Because both monochrome and color video EBU
signals run at exactly 25 fr/sec, an EBU drop-frame code isn’t necessary.
n 24 fr/sec (standard rate for ﬁlm work): The medium of ﬁlm differs from all
of these in that it makes use of an SMPTE timecode format that runs at
24 fr/sec.
From the above, it’s easy to understand why confusion often exists as to which
frame rate should be used on a project. If you are working on an in-house project
that doesn’t incorporate time-encoded material that comes from the outside
world, you should choose a rate that both makes sense for you and is likely to
be compatible with an outside facility (should the need arise).
For example, electronic musicians who are working in-house in the US will
often choose to work at 30 fr/sec. Those in Europe have it easy, because on that
continent 25 fr/sec is the logical choice for all music and video productions.
On the other hand, those who work with projects that come through the door
from other production houses will need to take special care to reference their
time-code rates to those used by the originating media house. This can’t be
stressed enough: If care isn’t taken to keep your timecode references at the proper
rate and relative address times (while keeping degradation to a minimum from
375
Synchronization  CHAPTER 12

Timecode
one generation to the next), the various media might have trouble syncing up
when it comes time to put the ﬁnal master together—and that could spell big
trouble.
Timecode Within Digital Media Production
Given that SMPTE exists in a digitally encoded data form, current-day digital
professional media devices are able to accept and communicate SMPTE directly
without too much trouble. Professional camera, ﬁlm, controllers and editing
systems are able to directly chain and synchronize SMPTE using a multitude of
complicated, yet standardized methods that make use of both digital- and
analog-style timecode data streams.
Of course, there are a wide range of approaches that can be taken when media
devices (cameras, video editing software and ﬁeld audio recorders) are to be
synchronized together. These can range from “shoots” that make use of multiple
cameras and separate ﬁeld recorders, which are “locked” to a single timecode
source on a set—all the way down to a simple camera and digital hand recorder,
with audio that can be manually synced up within the digital editor, without
the use of timecode at all. The types of equipment and the ways that they deal
with the technology of sync are ever-changing. Therefore it’s important to keep
abreast of current technology, read the manuals (about how connections and
settings can best be made) and dive into the study of visual media production.
BROADCAST WAVE FILE FORMAT
Although digital media devices and software are able to import, convert and
communicate using the language of SMPTE timecode (in all its various ﬂavors),
the folks at the EBU (European Broadcast Union) saw the need to create a
universal audio ﬁle format that would include timecode data within all forms
of audio and visual media production. The result was the Broadcast Wave
Format (BWF). Broadcast Wave is in most ways completely compatible with its
Microsoft Wave counterpart, with the exception that it is able to embed metadata
(information about the recorded content—photo, take number, date, technical
data, etc.) as well as SMPTE timecode address data. The inclusion of such
important content and timecode information means that the time-related
information will actually be imbedded within the ﬁle itself, allowing sound
ﬁles that are imported into a video or audio editor to automatically snap to
their appropriate timecode position. Obviously, Broadcast Wave can be a huge
time saver within the production and post-production process.
MIDI Timecode
In earlier times, the synchronization of audio devices to other video and/or
audio devices was a very expensive proposition, far beyond the budget of most
project or independent production houses. Today, however, an easy-to-use and
inexpensive standard makes use of MIDI to transmit sync and timecode data
throughout a connected production system (Figure 12.6). This has made it
376

377
Synchronization  CHAPTER 12
possible for even the most budget-minded project studios to be able to
synchronize media devices and software using timecode.
MIDI timecode (MTC) was developed to allow electronic musicians, project
studios, video facilities and virtually all other production environments to cost
effectively and easily translate timecode into time-stamped messages that can
be transmitted over MIDI data lines. Created by Chris Meyer and Evan Brooks,
MIDI timecode allows SMPTE-based timecode to be distributed throughout the
MIDI chain to devices or instruments that are capable of synchronizing to and
executing MTC commands. MIDI timecode is an extension of the MIDI standard,
making use of existing sys-ex message types that were either previously undeﬁned
or were being used for other, non-conﬂicting purposes.
Since most modern recording systems include MIDI in their design, there’s often
no need for external hardware when making direct connections. Simply chain
the MIDI data lines from the master to the appropriate slaves within the system
(via physical cables, USB or virtual internal routing). Although MTC uses a
reasonably small percentage of MIDI’s available bandwidth (about 7.68% at 30
fr/sec), it’s customary (but not at all necessary) to separate these lines from
those that are communicating performance data when using physical MIDI
cables. As with conventional SMPTE, only one master can exist within an MTC
system, while any number of slaves can be assigned to follow, locate and chase
to the master’s speed and position. Because MTC is easy to use and is often
included free in many systems and program designs, this technology has grown
to become the most straightforward and commonly used way to lock together
such devices as DAWs, external devices and basic analog and video setups.
MIDI TIMECODE MESSAGES
The MIDI timecode format can be divided into two parts:
n Timecode
n MIDI cueing
The timecode capabilities of MTC are relatively straightforward and allow devices
to be synchronously locked or triggered to SMPTE timecode. MIDI cueing is a
format that informs a MIDI device of an upcoming event that’s to be performed
at a speciﬁc time (such as load, play, stop, punch-in/out, reset). This protocol
envisions the use of intelligent MIDI devices that can prepare for a speciﬁc event
in advance and then execute the command on cue.
FIGURE 12.6
Many time-based media
devices in the studio can be
cost effectively connected
via MIDI timecode (MTC).

Timecode
378
MIDI timecode is made up of three message types:
n Quarter-frame messages: These are transmitted only while the system is
running in real or variable speed time, in either forward or reverse direction.
True to its name, four quarter-frame messages are generated for each
timecode frame. Since 8 quarter-frame messages are required to encode a
full SMPTE address (in hours, minutes, seconds and frames: 00:00:00:00),
the complete SMPTE address time is updated once every two frames (In
other words, MIDI timecode actually has half the resolution accuracy of
its SMPTE timecode counterpart). Each quarter-frame message contains 2
bytes. The ﬁrst byte is F1, the quarter-frame common header; the second
byte contains a nibble (four hits) that represents the message number (0
through 7) and a nibble for encoding the time ﬁeld digit.
n Full messages: Quarter-frame messages are not sent in the fast-forward,
rewind or locate modes, because this would unnecessarily clog a MIDI
data line. When the system is in any of these shuttle modes, a full message
is used to encode a complete timecode address. After a fast shuttle mode
is entered, the system generates a full address message and then places
itself in a pause mode until the time-encoded slaves have located to the
correct position. Once playback has resumed, MTC will again begin sending
incremental quarter-frame messages.
n MIDI cueing messages: MIDI cueing messages are designed to address indi -
vidual devices or programs within a system. These 13-bit messages can be
used to compile a cue or edit decision list, which in turn instructs one or
more devices to play, punch in, load, stop, and so on, at a speciﬁc time.
Each instruction within a cueing message contains a unique number, time,
name, type and space for additional information. At the present time, only
a small percentage of the possible 128 cueing event types have been deﬁned.
SMPTE/MTC CONVERSION
Although MIDI timecode connections can be directly made between compatible
MIDI devices, a SMPTE-to-MIDI converter is required to read incoming LTC
FIGURE 12.7
SMPTE timecode can often
be generated throughout a
production system, possibly
as either LTC or as MTC via
a capable MIDI or audio
interface.

SMPTE timecode and convert it into MIDI timecode (and vice versa) for other
device types. These conversion systems are available as a stand-alone device or
as an integrated part of an audio interface or multiport MIDI interface/patch
bay/synchronizer system (Figure 12.7).
TIMECODE PRODUCTION IN THE ANALOG AUDIO
AND VIDEO WORLDS
Fortunately for us, most digital editing systems (such as digital video and audio
workstations) are able to communicate timecode in a relatively seamless and
straightforward manner (at least at a basic level, often only requiring that the
various systems be set to the same frame rates, etc.). Synching analog-to-analog
or analog-to-digital devices, on the other hand, is often far less straightforward
and needs to be understood at at least a fundamental level.
Timecode that’s recorded onto an analog audio or video cue track of an older-
style video tape recorder is known as longitudinal timecode (LTC). LTC encodes
a bi-phase timecode signal onto an analog track in the form of a modulated
square wave at a bit rate of 2400 bits/sec. The recording of a perfect square
wave onto a magnetic audio track is difﬁcult, even under the best of conditions.
For this reason, the SMPTE standard has set forth an allowable rise time of 25
±5 microseconds for the recording and reproduction of valid code. This tolerance
requires a signal bandwidth of at least 15 kHz, which is well within the range
of most professional audio recording devices. Variable-speed timecode readers
are often able to decode timecode information at shuttle rates ranging from
1/10th to 100 times normal playing speed, which is often necessary when
monitoring videotape at slow or near-still speeds.
Because LTC can’t be read at speeds slower than 1/10th to 1/20th normal play
speed, therefore whenever a VTR is used (which is uncommon these days), a
character generator will be used to burn time-code addresses directly into the
video image of a work tape copy. This superimposed readout allows the timecode
to be easily seen and identiﬁed, even at very slow or still picture shuttle speeds.
It’s nice to keep in mind, however, that in most modern-day production settings,
LTC code won’t be necessary whenever digital video and audio devices are
involved.
Timecode Refresh and Jam Sync
Longitudinal timecode operates by recording a series of square-wave pulses 
onto magnetic tape. As you now know, it’s somewhat difﬁcult to record a square
waveform onto analog magnetic tape without having the signal suffer moderate
to severe waveform distortion. Although timecode readers are designed to be
relatively tolerant of waveform amplitude ﬂuctuations, such distortions are
severely compounded when code is copied from one analog recorder to another
over one or more generations.
379
Synchronization  CHAPTER 12

Timecode Production
380
Should the quality of a copied SMPTE signal degrade to the point where the
synchronizer can’t differentiate between the pulses, the code will disappear and
the slaves will come to a stop. For this reason, a timecode refresher (Figure
12.8) has been incorporated into most timecode synchronizers and various MIDI
interface devices that have sync capabilities. Basically, this process (known as
jam sync) reads the degraded timecode information from a previously recorded
track and then refreshes and regenerates the square wave back into its original
shape, so it can be freshly recorded to a new track and accurately read by
another device.
Jam sync also refers to the synchronizer’s ability to output the next timecode
value, even though the next valid value has not appeared at its input. The gen -
erator is then said to be working in a freewheeling fashion, since the generated
code may not agree with the actual recorded address values; however, if the
dropout occurs for only a short period, jam sync can often detect or refresh 
the lost signal. (This process is often useful when dealing with dropouts or
undependable code from audio tracks on an analog video machine.) Two forms
of jam sync options are available:
n Freewheeling
n Continuous
In the freewheeling mode, the receipt of timecode causes the generator’s output
to initialize when a valid address number is detected. The generator then begins
to count in an ascending order on its own, ignoring any deterioration or
discontinuity in code and producing fresh, uninterrupted SMPTE address
numbers. Continuous jam sync is used in cases where the original address
numbers must remain intact and shouldn’t be regenerated as a continuously
ascending count. After the reader has been activated, the generator updates the
address count for each frame in accordance with incoming address numbers
and outputs an identical, regenerated copy.
Synchronization Using SMPTE Timecode
In order to achieve a frame-by-frame timecode lock between multiple audio,
video or ﬁlm analog transports, it’s necessary to use a device or integrated
system that’s known as a synchronizer. The basic function of a synchronizer is
FIGURE 12.8
Jam sync is used to restore
distorted SMPTE when
copying code from one
machine to another.

381
Synchronization  CHAPTER 12
to control one or more tape, computer-based or ﬁlm transports (designated as
slave machines) so their speeds and relative positions are made to accurately
follow one speciﬁc transport (designated as the master).
The use of a synchronizer within a project studio environment (Figure 12.9)
often involves a multiport MIDI interface that includes provisions for locking
an analog audio or video transport to a digital audio, MIDI or electronic music
system by translating LTC SMPTE code into MIDI timecode. In this way, one
simple device can cost effectively serve multiple purposes to achieve lock with
a high degree of accuracy. Systems that are used in video production and in
higher levels of production will often require a greater degree of control and
remote-control functions throughout the studio or production facility. Such a
setup will often require a more sophisticated device, such as a control
synchronizer or an edit decision list (EDL) controller.
FIGURE 12.9
Example of timecode sync
production using a simple
MIDI interface synchronizer
(possibly one that’s already
designed into an audio
interface) within a studio
setting.
SMPTE Offset Times
In the real world of audio production, programs or songs don’t always begin
at 00:00:00:00 (as might easily happen when using a video or audio workstation
with an in-house project). Let’s say that you were handed a recording that needed
a synth track to be laid down onto track 7 of a song that goes from 00:11:24:03
to 00:16:09:21. Instead of inserting more than 11 minutes of empty bars into
a MIDI track on your synched DAW, you could simply insert an offset start time
of 00:11:24:03. This means that the sequenced track will begin to increment
from measure 1 at 00:11:24:03 and will maintain relative offset sync throughout
the program.
Offset start times are also useful when synchronizing devices to an analog or
videotape source that doesn’t begin at 00:00:00:00. As you’re probably aware,
it always takes a bit of time for an analog audio transport to settle down and
begin playing (this wait time often quadruples whenever a videotape transport
is involved). If a program’s timecode were to begin at the head of the tape, it’s
extremely unlikely that you would want to start a program at 00:00:00:00, since
playback would be delayed and extremely unstable at points near this time.
Instead, most programming involving an analog audio or video media is striped
with an appropriate pre-roll of anywhere from 10 seconds to 2 minutes. Such

Timecode Production
382
a pre-roll gives any analog transports ample time to begin playback and sync
up to the master time-code source.
In addition, it’s often wise to begin the actual production or ﬁrst song at an
offset or SMPTE start time of 00:01:00:00 (some facilities set the start offset at
01:00:00:00). This minimizes the possibility that rolling over at midnight will
confuse the synchronizer. That’s to say, if the content starts at 00:00:00:00
(midnight), the pre-roll would be in the 23:59:00:00 range and the synchronizer
would try to rewind the tape backwards to ﬁnd 00:00:00:00 (rolling the tape
backwards off the reel) instead of rolling forward. Not always fun in the heat
of a production!
Distribution of SMPTE Signals
Generally, when analog media devices are synced together, connections will
need to be made between each transport and the synchronizer. These include
lines for the LTC timecode reproduce track and the control interface (which
often uses the Sony 9-pin remote protocol for giving the synchronizer full logic
transport and speed-related feedback information). LTC signal lines can be
distributed throughout the production system in much the same way that any
other audio lines are distributed. They can be routed directly from machine to
machine or patched through audio switching systems via balanced, shielded
cables or unbalanced cables. It should be noted that because the timecode signal
is bi-phase or symmetrical, it’s immune to cable polarity problems.
TIMECODE LEVELS
One problem that can plague systems using timecode is crosstalk. This happens
when a high-level signal leaks into adjacent signal paths or analog tape tracks.
Currently, no industry standard levels exist for the recording of timecode onto
magnetic tape or digital tape track; however, the levels shown in Table 12.1 can
help you get a good signal level while keeping distortion and analog crosstalk
to a minimum.
Tape Format
Track Format
Optimum Recording 
Level
ATR
Edge track (highest number)
–5 to –10 VU
Digital device
Highest number track or 
–20 dB
dedicated timecode I/O ports
Note: If the VTR is equipped with automatic gain compensation (AGC), override the AGC and adjust
the signal gain controls manually.
Table 12.1
Optimum Timecode Recording Levels

Real-World Applications Using Timecode and MIDI
Timecode
Before we delve into the many possible ways that a system can be set up to
work in a timecode environment, it needs to be understood that each system
will often have its own particular personality and that the connections, software
and operation of one system might totally differ from those of another. This is
often due to factors such as system complexity and the basic hardware types
that are involved, as well as the type of hardware and software systems that are
installed in a DAW. Larger, more expensive setups that are used to create
television and ﬁlm soundtracks will often involve extensive timecode and system
interconnections that can easily get complicated.
Fortunately, the use of MIDI timecode and digital systems has greatly reduced
the cost and complexity of connecting and controlling a synchronous pro and
project studio system down to levels that can be easily managed by both
experienced and novice users. Having said these things, I’d still like to stress
that solving synchronization problems will often require as much intuition,
perseverance, insight and art as it will technical skill. For the remainder of this
chapter, we’ll be looking into some of the basic concepts and connections that
can be used to get your system up and running. Beyond this, the next best course
of action will be to consult your manuals, seek help from an experienced friend
or call the tech department about the particular hardware or software that’s
giving both you and your system the willies.
MASTER/SLAVE RELATIONSHIP
Since synchronization is based on the timing relationship between two or more
devices, it follows that the logical way to achieve sync is to have one or more
devices (known as slaves) follow the relative movements of a single transport
or device (known as the master). The basic rule to keep in mind is that there
can be only one master in a connected system; however, any number of slaves
can be set to follow the relative movements of a master transport or device
(Figure 12.10).
383
Synchronization  CHAPTER 12
FIGURE 12.10
There can be only one
master in a synchronized
system; however, there can
be any number of slaves.

Timecode Production
384
Generally, the rule for deciding which device will be the master in a production
system (during the pre-planning phase) can best be determined by asking a few
questions:
n What type of media is the master timecode media recorded on?
n Which device will provide the most stable timing reference?
n Which device will most easily and cost-effectively serve as the master?
If the master comes to you from an outside source, asking lots of questions
about the source specs will most likely solve many of your problems. If the
project is in-house and you have total say in the matter, you might want to
research your options more fully, to make the best choice for your facility. The
following sections can help give you insights into which devices will best serve
as the master within a particular system.
Video’s Need for a Stable Timing Reference
Whenever a video signal is copied from one machine to another, it’s essential
that the scanned data (containing timing, video and user information) be copied
in perfect sync from one frame to the next. Failure to do so will result in severe
picture breakup or, at best, the vertical rolling of a black line over the visible
picture area. Copying video from one machine to another generally isn’t a
problem (because the video recording device that’s doing the copying normally
provides sync from the playback machine within the picture itself). Video
postproduction houses, however, often simultaneously use any number of video
and audio workstations, switchers and edit controllers during the production
and editing of a single program. Mixing and switching between these combined
sources without a stable sync source would almost certainly result in chaos—
with the end result being a very unhappy client.
Fortunately, referencing all of the video, audio and timing elements to an
extremely stable timing source (called a black burst or house sync generator)
will generally resolve this sync nightmare. This reference clock serves to
synchronize the video frames and timecode addresses that are received or trans -
mitted by nearly “every” video-related device in a production facility, so the
FIGURE 12.11
Example of a system whose
overall timing elements are
locked to a black burst
reference signal.
Video's Need for a Stable Timing Reference 

leading frame edge of every video signal occurs at exactly the same instant in
time (Figure 12.11). By resolving all video and audio devices to a single black
burst reference, you’re assured that relative frame transitions, speeds and TC
addresses throughout the system will be consistent and stable.
Digital Audio’s Need for a Stable Timing Reference
The process of maintaining a synchronous lock between digital audio devices or
between digital and analog systems differs fundamentally from the process of
maintaining relative speed between analog transports. This is due to the fact that
a digital system generally achieves synchronous lock by adjusting its playback
sample rate (and thus its speed and/or pitch ratio), so as to precisely match the
relative playback speed of the master transport. Therefore, whenever a digital
system is synchronized to a time-encoded master, a stable timing source is
extremely important in order to keep jitter (in this case, an increased distortion
due to rapid pitch shifts) to a minimum. In other words, the source’s program
speed should vary as little as possible to prevent any degradation in the digital
signal’s quality. As such, a digital audio system that’s working within a video
production environment would also beneﬁt from the above mentioned house
sync timing source.
VIDEO WORKSTATION OR RECORDER
Since video is often an extremely stable timing source, a digital video editor (or
even analog video device) would be a stable timing source within a connected
production system. This process still shouldn’t be taken lightly, because the
timecode must (in most cases) conform to the timecode addresses on the
original video or working master. The rule of thumb is: If you’re working on a
project that was created out of house, always use the code that was provided
by the original production team. Striping your own code or erasing over the
original code with your own would render the original timing elements useless,
because the new code wouldn’t relate to the original addresses or include any
timing variations that might be a part of the original master source. In short,
make sure that your working copy includes a SMPTE track that is a regenerated
copy of the original code! Should you overlook this, you might run into timing
and sync troubles, either immediately or later in the post-production phase—
factors that will deﬁnitely lead to premature hair and client loss.
DIGITAL AUDIO WORKSTATIONS
A computer-based DAW can often be set to act as either a master or slave. This
will ultimately depend on the software and the situation, because most
professional workstations can be set to chase (to follow or be triggered by) a
master timecode source, as well as generate timecode (often in the form of MIDI
or SMPTE timecode within a higher-end system).
Most modern DAWs include support for displaying a video track (Figure 12.12)
within a session (both as a separate video screen that can be displayed on the
385
Synchronization  CHAPTER 12
Digital Audio's Need for a Stable Timing Reference 

Timecode Production
386
FIGURE 12.12
Most high-end DAW
systems are capable of
importing a video file
directly into the project
session window. (Courtesy
of Apple Computers, Inc.,
www.apple.com)
monitor desktop and in the form of a video thumbnail track that appears within
the track view). Of course, the video track provides important visual cues for
tracking live music, accurately placing automation moves and effects (sfx) at
speciﬁc hitpoints within the scene or for adding music sweetening. This feature
allows audio to be built up within a DAW environment without the need for
synching to an external device at all. As you might expect, the use of recorded
tracks, software instruments and internal mixing capabilities, tracks can easily
be built up, spotted and mixed—all inside the box.
ROUTING TIMECODE TO AND FROM YOUR COMPUTER
From a connections standpoint, most DAW, MIDI and audio application soft -
ware packages are ﬂexible enough to let you choose from any number of avail -
able sync sources (whether connected to a hardware port, MIDI interface port
or virtual sync driver). All you have to do is assign all of the slaves within the
system to the device driver that’s generating the system’s master code (Figure
12.13). In many cases, it’s best to have your DAW or editor generate the master
code for the system with the appropriate settings and timecode address times.
FIGURE 12.13
Cubase/Nuendo Sync
Preferences dialog box.
(Courtesy of Steinberg
Media Technologies GMBH,
www.steinberg.net)

ANALOG AUDIO RECORDERS
In many audio production situations, whenever an analog tape recorder is
connected in a timecode environment, this machine will most often want to
act as the master in an LTC environment. Although this might be counter-
intuitive, it’s far easier and less expensive for an analog recorder to output a
master SMPTE code, than to be controlled in an external slave relationship. This
is because special and expensive equipment is generally required to continuously
adjust the regulator’s speed (using a DC capstan servo), so as to maintain a
synchronous relationship to the master SMPTE address.
A SIMPLE CAVEAT
The above guidelines are just that—guidelines. As you might expect, each and
every setup will be slightly different, and might require that you come up with
a novel solution to a quirky problem. Again, the Internet is full of insights and
solutions from those who have already gone down that long and treacherous
path. Be warned though. It’s important that you prepare and make your decisions
wisely, lest a problem raise its ugly head at a later and crucial time.
Keeping Out of Trouble
Here are a few guidelines that can help save your butt when using SMPTE and
other timecode translations during a project:
n Familiarize yourself with the hardware and software involved in a project
before the session starts.
n When in doubt about frame rates, special requirements or anything else,
for that matter, ask! You (and your client) will be glad you did.
n Fully document your timecode settings, offsets, start times, etc.
n If the project isn’t to be used in-house, ask the producer what the proper
frame rate should be. Don’t assume or guess it.
n When beginning a new session (when using a tape-based device), always
stripe the master contiguously from the beginning to end before the session
begins. It never hurts to stripe an extra tape, just in case.
n Whenever analog machines are involved, start generating new code at a
point after midnight (i.e., 00:01:00:00 or 01:00:00:00 to allow for a pre-
roll). If the project isn’t to be used in-house, ask the producer what the
start times should be. Don’t assume or guess.
n Never dub (copy) timecode directly. Always make a refreshed (jam
synched) copy of the original timecode (from an analog master) before
the session begins.
n Disable noise reduction and AGC (Automatic Gain Control) on analog
audio tracks (on both audio and video devices).
n Work with copies from the original production video, and make a new
one when sync troubles appear.
387
Synchronization  CHAPTER 12

n It’s not unusual for the timecode to be read incorrectly (when short
dropouts occur on the track, usually on videotape). When this happens,
you might set the synchronizer to freewheel once the transports have
initially locked.
In closing, I’d like to point out that synchronization can be a simple procedure
or it can be an extremely complex one, depending on your requirements and
the type of equipment that’s involved. A number of books and articles have
been written on this subject. If you’re serious about production, I suggest that
you do your best to keep up on it. Although the fundamentals often remain
the same, new technologies and techniques are constantly emerging. As always,
the best way to learn is simply by reading and then jumping in and doing it.
388
Timecode Production

In the world of audio, ampliﬁers have many applications. They can be designed
to amplify, equalize, combine, distribute or isolate a signal. They can even be
used to match signal impedances between devices. At the heart of any ampliﬁer
(amp) system is either a vacuum tube or a semiconductor-type transistor series
of devices. Everyone has heard of these regulating devices, but few have a grasp
of how they operate, so let’s have a basic look into these electronic wonders.
AMPLIFICATION
To best understand how the theoretical process of ampliﬁcation works, let’s
draw on an analogy. The original term for the tube used in early ampliﬁers is
valve (a term that’s still used in England and other Commonwealth countries).
If we hook up a physical water valve to a high-pressure hose, large amounts of
water pressure can be controlled with very little effort, simply by turning the
valve (Figure 13.1). By using a small amount of expended energy, a trickle of
water can be turned into a high-powered gusher and back down again. In
practice, both the vacuum tube and the transistor work much like this valve.
For example, a vacuum tube operates by placing a DC current across its plate
and a heated cathode element (Figure 13.2). A wire mesh grid separating these
two elements acts like a control valve, allowing electrons to pass from the plate
to the cathode. By introducing a small and varying signal at the input onto the
tube’s grid, a much larger electrical signal can be used to correspondingly
regulate the ﬂow of electrons between the plate and the cathode (Figure 13.3a).
FIGURE 13.1
The current through a
vacuum tube or transistor is
controlled in a manner
that’s similar to the way
that a valve tap can control
water pressure through a
water pipe: (a) open valve;
(b) closed valve.
389
CHAPTER 13
Ampliﬁers

Ampliﬁcation
390
The transistor (a term originally derived from “trans-resistor” meaning a device
that can easily change resistance) operates under a different electrical principle
than a tube-based amp, although the valve analogy still applies. Figure 13.3b
shows a basic ampliﬁer schematic with a DC power source that’s placed across
the transistor’s collector and emitter points. As with the valve analogy, by
presenting a small control signal at the transistor’s base, the resistance between
the collector and emitter will correspondingly change. This allows a much larger
analogous signal to be passed to the device’s output.
As a device, the transistor isn’t inherently linear; that is, applying an input signal
to the base won’t always produce a corresponding output change. The linear
operating region of a transistor lies between the device’s lower-end cutoff region
and an upper saturation point (Figure 13.4a). Within this operating region,
however, changes at the input will produce a corresponding (linear) change in
the collector’s output signal. When operating near these cutoff or saturation
points, the base current lines won’t be linear and the output will become
distorted. In order to keep the signal within this linear operating range, a DC
bias voltage signal is applied to the base of the transistor (for much the same
reason a high-frequency bias signal is applied to an analog recording head).
After a corrective voltage has been applied and sufﬁcient ampliﬁer design
characteristics have been met, the amp’s dynamic range will be limited by only
two factors: noise (which results from thermal electron movement within the
transistor and other circuitry) and saturation.
FIGURE 13.2
An example of a triode
vacuum tube.
FIGURE 13.3
A simple amplifier
schematic: (a) showing how
small changes in voltage at
the tube’s grid can produce
much larger, corresponding
amplitude changes between
its cathode and plate; (b)
showing how small changes
in current at the transistor’s
base can produce much
larger, corresponding
amplitude changes through
the emitter and collector to
the output.

391
Ampliﬁers  CHAPTER 13
Ampliﬁer saturation results when the input signal is so large that its DC output
supply isn’t large enough to produce the required, corresponding output signal.
Overdriving an amp in such a way will cause a mild to severe waveform
distortion effect known as clipping (Figure 13.4b). For example, if an amp having
a supply voltage of +24 volts (V) is operating at a gain ratio of 30:1, an input
signal of 0.5 V will produce an output of 15 V. Should the input be raised to
1 V, the required output level would have to be increased to 30 V. However,
since the maximum output voltage is limited to 24 V, levels above this point
will be chopped off or “clipped” at the upper and lower edges of the waveform.
Whenever a transistor and integrated circuit design clips, severe odd-order
harmonics are often introduced that are immediately audible as distortion.
Tube amp designs, on the other hand, tend to lend a more musical sounding,
even-order harmonic aspect to a clipped signal. I’m sure you’re aware that
clipping distortion can be a sought-after part of a tube instrument’s sound
(electric guitars thrive on it); however, it’s rarely a desirable effect in quality
studio and monitoring gear. The best way to avoid undesirable distortion from
either amp type is to be aware of the various device gain stages throughout the
studio’s signal chains.
The Operational Amplifier
An operational ampliﬁer (op-amp) is a stable, high-gain, high-bandwidth amp
that has a high-input impedance and a low-output impedance. These qualities
allow op-amps (Figure 13.5) to be used as a basic building block for a wide
variety of audio and video applications, simply by adding components onto
the basic circuit in a building-block fashion to ﬁt the design’s needs. To reduce
an op-amp’s output gain to more stable, workable levels, a negative feedback
loop is often required. Negative feedback is a technique that applies a portion
of the output signal through a limiting resistor back into the negative or phase-
inverted input terminal. By feeding a portion of the amp’s output back into the
input out of phase, the device’s output signal level is reduced. This has the effect
of controlling the gain (by varying the negative resistor value) in a way that
also serves to stabilize the amp and further reduce distortion.
FIGURE 13.4
Output curves of a
transistor: (a) proper
operating region; (b) a
clipped waveform.

Ampliﬁcation
392
FIGURE 13.5
Basic op-amp circuit and
741-type pin configuration.
PREAMPLIFIERS
One of the mainstay ampliﬁer types found at the input section of most
professional mixer console and outboard devices is the preampliﬁer (preamp).
This amp type is often used in a wide range of applications, such as boosting
a mic’s signal to line level, providing variable gain for various signal types and
isolating input signals and equalization, just to name a few. Preamps are an
important component in audio engineering because they often set the “tone”
of how a device or system will sound. Just as a microphone has its own sonic
character, a preamp design will often have its own “sound.” Questions such as
“Are the op-amps designed from quality components?” “Do they use tubes or
transistors?” and “Are they quiet or noisy?” are all-important considerations
that can greatly affect the overall sound of a device.
EQUALIZERS
You might be surprised to know that basically, an equalizer is nothing more than
a frequency-discrimi nating ampliﬁer. In most analog designs, equalization (EQ)
is achieved through the use of resistor/capacitor networks that are located in an
op-amp’s negative feedback loop (Figure 13.6) in order to boost (amplify) or
cut (attenuate) certain frequencies in the audible spectrum. By changing the circuit
design, complexity and parameters, any number of EQ curves can be achieved.
FIGURE 13.6
Basic E equalizer circuit: 
(a) low-frequency;
(b) high-frequency.

393
Ampliﬁers  CHAPTER 13
Summing Amplifiers
A summing amp (also known as an active combining ampliﬁer) is designed to
combine any number of discrete inputs into a single output signal bus, while
providing a high degree of isolation between them (Figure 13.7). The summing
ampliﬁer is an important component in analog console/mixer design because
the large number of internal signal paths requires a high-degree of isolation in
order to prevent signals from inadvertently leaking into other audio paths.
FIGURE 13.7
A summing amp is used to
provide isolation between
various inputs and/or
outputs in a signal chain.
Distribution Amplifiers
Often, it’s necessary for audio signals to be distributed from one device to several
other devices or signal paths within a recording console or music studio. In this
situation, a distribution amp isn’t used to provide gain but instead will amplify
the signal’s current (power) that’s being delivered to one or more loads (Figure
13.8). Such an amp, for example, could be used to boost the overall signal
power so that a single feed could be distributed to a large number of headphones
during a string or ensemble session.
FIGURE 13.8
Distribution amp.
Power Amplifiers
As you might expect, power ampliﬁers (Figure 13.9) are used to boost the audio
output to a level that can drive one or more loudspeakers at their rated volume
levels. Although these are often reliable devices, power amp designs have their

Ampliﬁcation
394
FIGURE 13.9
Mackie FRS-2800
professional power
amplifier. (Courtesy of Loud
Technologies, Inc.,
www.mackie.com)
own special set of problems. These include the fact that transistors don’t like
to work at the high temperatures that can be generated during continuous, high-
level operation. Such temperatures can also result in changes in the unit’s
response and distortion characteristics or outright failure. This often requires
that protective measures (such as fuse and thermal protection) be taken.
Fortunately, many of the newer ampliﬁer models offer protection under a wide
range of circuit conditions (such as load shorts, mismatched loads and even
open “no-load” circuits) and are usually designed to work with speaker
impedance loads ranging between 4 and 16 ohms (most speaker models are
designed to present a nominal load of 8 ohms). When matching an amp to a
speaker set, the amp should be capable of delivering sufﬁcient power to properly
drive the speakers. If the speaker’s sensitivity rating is too low or the power
rating too high for what the amp can deliver, there could be a tendency to
“overdrive” the amp at levels that could cause the signal to be clipped. In
addition to sounding distorted, clipped signals can contain a high-level DC
component that could potentially damage the speaker’s voice coil drivers.
Voltage and Digitally Controlled Amplifiers
Up to this point, our discussion has largely focused on analog amps whose
output levels are directly proportional to the signal level that’s present at its
input. Several exceptions to this principle are the voltage-controlled ampliﬁer
(VCA) and the digitally-controlled ampliﬁer (DCA). In the case of the VCA, the
overall output gain is a function of an external DC voltage (generally ranging
from 0 to 5 V) that’s applied to the device’s control input (Figure 13.10). As
the control voltage is increased, the analog signal will be proportionately
attenuated. Likewise, a digitally controlled external voltage can be used to
control the amp’s overall gain. Certain older console automation systems,
automated analog signal processors and even newer digital console designs make
use of VCA technology to digitally store and automate levels.
FIGURE 13.10
Simplified example of a
voltage-controlled amplifier.

With the wide acceptance of digital technology in the production studio, it’s
now far more common to ﬁnd devices that use digitally controlled ampliﬁers
to control the gain of an audio signal. Although most digital devices change
the gain of a signal directly within the digital domain, it’s also possible to 
change the gain of an analog signal using an external digital source. Much like
the VCA, the overall gain of an analog amp can be altered by placing a series
of digitally controlled step resistors into its negative feedback loop and digitally
varying the amount of resistance that’s required to achieve the desired gain.
395
Ampliﬁers  CHAPTER 13


Throughout this book, we’ve seen how a production facility, in all its forms,
involves the interconnection of various digital and analog devices to create a
common task—to capture and produce good music and audio without adding
clicks, pops and spurious noises to our tracks. This brings us to two aspects that
are often overlooked in the overall design of a facility:
1. The need for proper grounding techniques (the way that devices
interconnect without introducing outside electrical noises)
2. The need for proper power conditioning (the purity and isolation of a
room’s power from the big, bad outside world)
GROUNDING CONSIDERATIONS
Proper grounding is essential to maintaining equipment safety; however, within
an audio facility, small AC voltage potentials between various devices in an
audio system can leak into a system’s grounding circuit. Although these
potentials are small, they are sometimes large enough to induce noise in the
form of hums, buzzes or radio-frequency (RF) reception that can be injected
(and ampliﬁed) directly into the audio signal path. These unwanted signals
generally occur whenever improper grounding allows a piece of audio equipment
to detect two or more different paths to ground.
Because grounding problems arise as a result of electrical interactions between
any number of equipment combinations, the use of proper grounding
techniques and troubleshooting within an audio production facility are by their
very nature situational and often frustrating. As such, the following procedures
are simply a set of introductory guidelines for dealing with this age-old problem.
There are a great number of technical papers, books, methods and philosophies
on grounding, and it’s recommended that you carefully research the subject
further before tackling any major ground-related problems. When in doubt, an
experienced professional should be contacted, and care should always be taken
not to sacriﬁce safety.
397
CHAPTER 14
Power- and Ground-
Related Issues

Grounding Considerations
n Keep all studio electronics on the same AC electrical circuit—most stray
hums and buzzes occur whenever parts of the sound system are plugged
into outlets from different AC circuits. Plugging into a circuit that’s
connected to such noise-generating devices as air conditioners, refrigerators,
light dimmers, neon lights, etc., will often invite stray noise problems.
Because most project studio devices don’t require a great deal of current
(with the possible exception of power ampliﬁers), it’s often safe to run all
of these devices from a single, properly grounded line from the electrical
circuit panel.
n Try to keep audio wiring away from AC wiring—whenever AC and audio
cables are laid side-by-side, portions of the 60-Hz signal might be induced
into a high-gain, unbalanced circuit as hum. If this occurs, check to see if
separating or shielding the lines helps reduce the noise.
n When all else fails: If you only hear hum coming from one particular input
channel, check that source device for ground-related problems. If the noise
still exists when the console or mixer inputs are turned down, check the
amp circuit or any device that follows the mixer. If the problem continues
to elude you, then . . .
n Disconnect all of the devices (both power and audio) from the console,
mixer or audio interface, then methodically plug them back in one at a
time (it’s often helpful to monitor through a pair of headphones).
n Check the cables for bad connections or improper polarity. It’s also 
wise to keep the cables as short as possible (especially in an unbalanced
circuit).
n Another common path for ground loops is through a chassis into a 19-
inch rack that not only provides a mount for multiple devices, but often
a common ground path. Test this by removing devices from the rack one
at a time. If needed, a device can be electrically isolated from the rack by
using special nylon mounting washers.
n Investigate the use of a balanced power source, if traditional grounding
methods don’t work.
n Lastly, it’s deﬁnitely not a good (and potentially dangerous) idea to get
rid of your ground noise by removing the ground pin leads from your
power connectors. These connections are there to make sure that a
problematic power voltage is routed to earth (ground) and not through
you. Any grounding problems in the system should be able to be corrected
for without resorting to improper wiring techniques.
Troubleshooting a ground-related problem can be tricky and ﬁnding the
problem’s source might be a needle-in-a-haystack situation. When putting on
your troubleshooting hat, it’s usually best to remain calm, be methodical and
consult with others who might know more than you do (or might simply have
a fresh perspective).
398

399
Power- and Ground-Related Issues  CHAPTER 14
POWER CONDITIONING
As was said earlier, one of the best ways to ensure that the power that’s being
delivered to your production system is as “clean” as possible is get power from
a single circuit source. If you have the luxury of building your facility from the
ground-up, one of the best ways to ensure that you have clean and reliable
power is to take care to deliver power directly from the circuit box (over one
or two lines) directly to the studio. If this isn’t possible, then pulling your power
from a single circuit or power socket might be your best bet. The latter scenario
is possible for many home project studios, as the combined power require-
ments are well below the maximum load of a wall socket before the breaker
trips (15A × 120V = 1800 watts or (20A × 120V = 2400 watts).
One of the best and simplest ways to connect all of your studio electronics to
the same AC electrical circuit is by using a power conditioner (Figure 14.1). In
addition to obtaining power from a single source, such a device can regulate,
isolate and protect the voltage supply that’s feeding one of your studio’s most
precious investments (besides you and your staff)—the equipment! Other added
beneﬁts to using power conditioning can be broken down into two important
topics:
n Voltage regulation (ﬂuctuations and surges)
n Keeping the lines quiet (reducing noise and interference)
FIGURE 14.1
Furman M-8Lx power
conditioner. (Courtesy of
Furman Sound, Inc.,
www.furmansound.com)
In an ideal world, the power that’s being fed to your studio outlets should be
very close to the standard reference voltage of the country you are working in
(e.g., 120V, 220V, 240V). The real fact of the matter is that these line voltages
regularly ﬂuctuate from this standard level, resulting in voltage sags (a condition
that can seriously under-power your equipment), surges (rises in voltage that
can harm or reduce the working life of your equipment), transient spikes (sharp,
high-level energy surges from lightning and other sources that can do serious
damage) and brown-outs (long-term sags in the voltage lines). Through the use
of a voltage regulator, high-level, short-term spikes and surge conditions can
be clamped (stopped or reduced), thereby reducing or eliminating the chance
that the main voltage will rise above a standard, predetermined level.
While most power conditioners are meant to work best in a rack-mounted
scenario, a number of power strips on the market offer protection against surges,

Power Conditioning
400
as well as RF and electro-magnetic interference (Figure 14.2). These corded power
strips can offer better protection over cheaper power strips and can be used in
portable settings and go into tight spots where many larger conditioners simply
can’t go.
Certain devices that are equipped with voltage regulation circuitry are able to
deal with power sags, long-term surges and brown-outs by electronically
switching between the multiple voltage level taps of a transformer so as to match
the output voltage to the ideal mains level (or as close to it as possible). One
of the best approaches for regulating voltage ﬂuctuations both above and below
nominal power levels is to use an adequately powered uninterruptible power
supply (UPS). In short, a quality UPS works by using a regulated power supply
to constantly charge a rechargeable battery or bank of batteries. This battery
supply is again regulated and used to feed sensitive studio equipment (such as
a computer, bank of effects devices, etc.) with a clean and constant voltage
supply.
Multiple-Phase Power
Another good defense against noise, power drops and other equipment-related
power problems is to make use of multiple circuits in your overall studio power
design. This approach reduces the amount of potential interference between
power systems by physically placing the major system groups on their own power
circuit. For example, an ideal scenario would place each of the following groups
on their own power phase (separate circuit):
Phase 1:
Equipment, computer (UPS isolated) and studio power
Phase 2:
Lighting
Phase 3:
Air conditioning and heating
The use of such a separate circuit design can greatly help to guard against the
everyday surges, noise and any other number of gremlins that might creep into
your beloved production system.
FIGURE 14.2
Furman SS-6B power
conditioner/strip. (Courtesy
of Furman Sound, Inc.,
www.furmansound.com)

401
Power- and Ground-Related Issues  CHAPTER 14
BALANCED POWER
For those facilities that are located in areas where power lines are overtasked
by heavy machinery, air conditioners and the like, a balanced power source
might be considered. Such a device makes use of a power transformer (Figure
14.3) that has two secondary windings, with a potential to ground on each side
of 60V. Because each side of the circuit is 180º out of phase with the other, a
120V supply is maintained. Also, since the two 60V legs are out of phase, any
hum, noise or RF that’s present at the device’s input will be canceled at the
transformer’s center tap (a null point that’s tied to ground).
FIGURE 14.3
Furman IT-1220 balanced-
output power conditioner
and basic equivalent circuit.
(Courtesy of Furman Sound,
Inc., www.furmansound.
com)
A few important points relating to a balanced power circuit include:
n A balanced power circuit is able to reduce line noise if all of the system’s
gear is plugged into it. As a result, the device must be able to deliver
adequate power.
n Balanced power will not eliminate noise from gear that’s already sensitive
to outside induced hums and buzzes.
n Choosing when to use balanced power is often open to interpretation,
depending on who you talk to. For example, some feel that a balanced
power conditioner should be used only after all other options to eliminate
noise have been explored, while others believe it is a starting point from
which to build a noise-free environment.
HUM, RADIO FREQUENCY (RF) AND ELECTRO-
MAGNETIC INDUCTION (EMI)
In our modern world, where electrical and electronic gear are everywhere and
where radio frequencies literally surround us as urban densities increase, the
need for keeping noise out of our production system becomes more and more
problematic.
The equipment in our system usually has lots of analog ampliﬁers, and most
of them have one task—provide gain to make lower-level signals LOUDER.
Unfortunately, whenever noises, buzzes, pops and ticks make their way into
the signal before this gain stage, they too, are ampliﬁed and quickly become a

nuisance in the audio chain. These pests can be introduced into a system as
hums (noise that occurs as a result of improper grounding or shielding) and
electromagnetic interference (noise from radio and cell phone transmissions,
lights or other electrically induced signals).
Several options for keeping electrically-induced signal out of the audio path
include:
n Use a clean power source (see the above section on power conditioning).
n Make sure that the devices and audio cables are properly shielded to
ground. This can be best done by using quality cables that have metal
shielding.
n Whenever possible, use balanced cable connections (cables that have 2
audio conductors “plus” a ground connection—see the wiring explanations
in Chapter 4 for more info).
n Whenever it’s practical and possible, keep your audio and power signals
separate.
n There is talk that the use of ferrite beads (you know those heavy lumps
that you’ll ﬁnd at the end of USB cables, etc.) can be useful for reducing
high-frequency (MHz) transmission through cable lines. Since there are
deﬁnitely debates on this subject, I’ll let you research, as to whether this
might be helpful in your situation or not.
When talking about power, grounding and interference, one thing’s for certain
—there’s usually no one-size-ﬁts-all answer to problems that might creep up 
in an audio signal’s path. You will probably be called upon to be patient,
methodical, insightful and even a bit psychic, so as to best understand and
correct any of the innumerable problems that you might encounter. Oh, I
almost forgot the last (but not least) element in helping to get this elusive job
done—luck!
402
Hum, RF and EMI

Over the years, signal processing has become an increasingly important part of
audio and music production. It’s the function of a signal processor to change,
augment or otherwise modify an audio signal in either the analog or digital
domain. This chapter offers some insight into the basics of effects processing
and how they can be integrated into a recording or mixdown in ways that sculpt
sound using forms that are subtle, lush, extreme or just plain whimsical and
wacky.
Of course, the processing power of an effects system can be harnessed in either
the hardware or software plug-in domain. Regardless of how you choose to
work with sound, the important rule to remember is that there are no rules;
however, there are a few general guidelines that can help you get the sound
that you want. When using effects, the most important asset you can have is
experience and your own sense of artistry. The best way to learn the art of
processing, shaping and augmenting sound is through experience—and gaining
experience takes time, a willingness to learn and lots of patience.
THE WONDERFUL WORLD OF ANALOG, DIGITAL 
OR WHATEVER
Signal processing devices and their applied practices come in all sizes, shapes
and ﬂavors. These tools and techniques might be analog, digital or even acoustic
in nature. The very fact that early analog processors have made a serious
comeback (in the form of reissued hardware and software plug-in emulations,
as seen in Figure 15.1) points to the importance of embracing past tools and
techniques, while combining them with the technological advances of the day
to make the best possible production.
The Whatever
Although these aren’t the ﬁrst thoughts that come to mind, the use of acoustics
and ambient mic techniques are often the ﬁrst line of defense when dealing
403
CHAPTER 15
Signal Processing

The World of Analog, Digital or Whatever
404
with the processing of an audio signal. For example, as we saw in Chapter 4,
changing a mic or its position might be the better option for changing the
character of a pickup over using EQ. Placing a pair of mics out into a room or
mixing a guitar amp with a second, distant pickup might ﬁll the ambience of
an instrument in a way that a device just might not be able to duplicate. In
short, never underestimate the power of your acoustic environment and your
ingenuity as an effects tool.
Analog
For those wishing to work in the world of analog (Figure 15.2), an enormous
variety of devices can be put to use in a production. Although these generally
relate to devices that alter a source’s relative volume levels (e.g., equalization
and dynamics), there are also a number of analog devices that can be used to
alter effects that are time based. For example, an analog tape machine can be
patched so as to make an analog delay or regenerative echo device. Although
they’re not commonly used, spring and plate reverb units that can add their own
distinctive sound can still be found on the used and sometimes new market.
FIGURE 15.1
1176LN limiter: 
(a) hardware version;
(b) powered software plug-
in. (Courtesy of Universal
Audio, www.uaudio.com.
© 2017 Universal Audio,
Inc. All rights reserved.
Used with permission)
FIGURE 15.2
Analog hardware in the
studio. (Courtesy of
Wisseloord Studios,
www.wisseloord.nl,
acoustics/photo by 
Jochen Veith, www.
jv-acoustics.de)
ANALOG RECALL
Of course, we simply can’t overlook the reason why many seek out analog
hardware devices, especially tube hardware—their sound! Many audio
professionals are on a continual quest for that warm, smooth sound that’s
delivered by tube mics, preamps, amps and even older analog tape machines.
It’s part of what makes it all fun.
The downside of all those warm & fuzzy tools rest with its inability to be saved
and instantly recalled with a session (particularly a DAW session). If that analog
sound is to be part of the recording, then you can use your favorite tube preamp
to “print” just the right sound to a track. If the entire session is to be mixed in

real-time on an analog console, plugging your favorite analog toys into the
channels will also work just ﬁne. If, however, you’re going to be using a DAW
with an analog console (or even in the box, under special circumstances) then,
you’ll need to do additional documentation in order to manually “recall” the
session during the next setup. Of course, you could write the settings into the
track notepads, however, another excellent way to save your analog settings is
to take a picture of the device’s settings and save it within the song’s session
documentation directory.
A number of forward-thinking companies have actually begun to design analog
gear that can be digitally controlled, directly from within the DAW’s session.
The settings on these devices can then be remotely controlled from the DAW
in real-time and then instantly recalled from within the session.
Digital
The world of digital audio, on the other hand, has deﬁnitely set signal processing
on ﬁre by offering an almost unlimited range of effects that are available to the
musician, producer and engineer. One of the biggest advantages to working in
the digital signal processing (DSP) domain is the fact that software can be used
to conﬁgure a processor in order to achieve an ever-growing range of effects
(such as reverb, echo, delay, equalization, dynamics, pitch shifting, gain changing
and signal re-synthesis).
The task of processing a signal in the digital domain is accomplished by
combining logic or programming circuits in a building-block fashion. These logic
blocks follow basic binary computational rules that operate according to a
special program algorithm. When combined, they can be used to alter the
numeric values of sampled audio in a highly predictable way. After a program
has been conﬁgured (from either internal ROM, RAM or system software),
complete control over a program’s setup parameters can be altered and inserted
into a chain as an effected digital audio stream. Since the process is fully digital,
these settings can be saved and precisely duplicated at any time upon recall (often
using MIDI program change messages to recall a programmed setting). Even more
amazing is how the overall quality and functionality have steadily increased while
at the same time becoming more cost effective. It has truly brought an
overwhelming amount of production power to the audio production table.
PLUG-INS
In addition to both analog and digital hardware devices, an ever-growing list
of signal processors are available for the Mac and PC platforms in the form of
software plug-ins. These software applications offer virtually every processing
function that’s imaginable (often at a fraction of the price of their hardware
counterparts and with little or no reduction in quality, capabilities or automation
features). These programs are, of course, designed to be “plugged” into an editor
or DAW production environment in order to perform a particular real-time or
non-real-time processing function.
405
Signal Processing  CHAPTER 15

Plug-Ins
406
Currently, several plug-in standards exist, each of which function as a platform
that serves as a bridge to connect the plug-in, through the computer’s operating
system (OS), to the digital audio production software. This means that any plug-
in (regardless of its manufacturer) will work with an OS and DAW that’s
compatible with that platform standard, regardless of its form, function and/or
manufacturer. As of this writing, the most popular standards are VST (PC/Mac),
AudioSuite (Mac), Audio Units (Mac), MAS (MOTU for PC/Mac), as well as
TDM and RTAS (Digidesign for PC/Mac).
By and large, effects plug-ins operate in a native processing environment (Figure
15.3a). This means that the computer’s main CPU processor carries the
processing load for both the DAW and plug-in DSP functions. With the ever-
increasing speed and power of modern-day CPUs, this has become less and less
of a problem; however, hardware cards and external systems can be added to
your system to “accelerate” the DSP processing power of your computer (Figure
15.3b) by adding additional hardware CPUs that are directly dedicated to
handling the signal processing plug-in functions.
FIGURE 15.3
Signal processing plug-ins. 
(a) Collage of the various
screens within the Cubase
media production DAW.
(Courtesy of Steinberg
Media Technologies GmbH,
www.steinberg.net)
(b) Producer/engineer
Billy Bush mixing Garbage’s
“Not Your Kind of People”
with Universal Audio’s
Apollo and UAD2
accelerated plug-ins.
(Photo courtesy of Universal
Audio/David Goggin,
www.uaudio.com)
Plug-In Control and Automation
A fun and powerful aspect of working with various signal processing plug-ins
on a DAW platform is the ability to control and automate many or all of the
various effects parameters with relative ease and recall. These controls can be
manipulated on-screen (via hands-on or track parameter controls) or from an
external hardware controller (Figure 15.4), allowing the parameters to be
physically controlled in real time.
FIGURE 15.4
External EFX hardware
controllers. (a) Novation’s
Nocturn automatically maps
its controls to various
parameters on the target
plug-in. (Courtesy of
Novation Digital Music
Systems, Ltd.,
www.novationmusic.com)
(b) Native Instruments
Komplete Kontrol system
maps instruments and
plug-ins to its hardware
controls. (Courtesy of
Native Instruments GmbH,
www.native-instruments.
com)

407
Signal Processing  CHAPTER 15
SIGNAL PATHS IN EFFECTS PROCESSING
Before diving into the process of effecting and/or altering sound, we should
ﬁrst take a quick look at an important signal path concept—the fact that a signal
processing device can be inserted into an analog, digital or DAW chain in several
ways. The most common of these are:
n Insert routing
n Send routing
Insert Routing
Insert routing is often used to alter the sonic or effects characteristics of a single
track or channel signal. It occurs whenever a processor is directly inserted into
a signal path in a serial (pass thru) fashion. Using this approach, the audio
source enters into the input path, passes through the inserted signal processor
and then continues on to carry out the record, monitor and/or mix function.
This method of inserting a device is generally used for the processing of a single
instrument, voice or grouped set of signals that are present on a particular
hardware or virtual input strip. Often, but not always, the device tends to be
an amplitude-based processing function (such as an equalizer, compressor or
limiter). In keeping with the “no-rules” concept, however, time- and pitch-
changing devices can also be used to tweak an instrument or voice as an insert.
Here are but a few examples of how an insert can be used:
n A device can be plugged into an input strip’s insert (direct send/return)
point. This approach is used to insert an outboard device directly into the
input signal path of an analog, digital or DAW strip (Figure 15.5a).
n A processor (such as a stereo compressor, limiter, EQ, etc.) could be
inserted into a mixer’s main output bus to affect an overall mix.
n A processor (such as a stereo compressor, limiter, EQ, etc.) could be
inserted into a grouping to affect a sub-mix.
n An effects stomp box could be inserted between a mic preamp and console
input to create a grungy distortion effect.
n A DAW plug-in could be inserted into a strip to process only the signal
on that channel (Figure 15.5b).
An insert is used to “insert” an effect or effects chain
into a single track or group. It tends (but not always),
to be amplitude-based in nature, meaning the
processor is often used to effect amplitude levels
(i.e., compressor, limiter, gate, etc.)

Signal Paths in Effects Processing
408
FIGURE 15.5
Inserting an effect into a
channel strip. (a) Analog
insert. (b) DAW insert.
EXTERNAL CONTROL OVER AN INSERT EFFECT’S SIGNAL PATH
Certain insert effects processors allow for an external audio source to act as a
control for affecting a signal as it passes from the input to the output of a device
(Figure 15.6). Devices that offer an external “key” or “sidechain” input can be
quite useful, allowing a signal source to be used as a control for varying another
audio path. For example:
n A gate (an inﬁnite expander that can control the passing of audio through
a gain device) might take its control input from an external “key” signal
that will determine when a signal will or will not pass in order to reduce
leakage, tighten up an instrument decay or to create an effect.
n A vocal track could be inserted into a vocoder’s control input, so as to
synthetically add a robot-like effect to a track.
n A voice track could be used for vocal ducking at a radio station, as a control
to fade out the music or crowd noise when a narrator is speaking.
n An external keyed input can be used to make a mix “pump” or “breathe”
in a dance production.
It’s important to note that there is no set standard for providing a side-chain
key in software. Some software packages provide native side-chain capability,
others support side-chaining via “multiple input” plug-ins and complex signal
routing and many don’t support side-chaining at all.
FIGURE 15.6
Diagram of a key side-
chain input to a noise gate.
(a) The signal is passed
whenever a signal is
present at the key input.
(b) No signal is passed
when no signal is present
at the key input.

Send Routing
Effects “sends” are often used to augment a signal (generally being used to add
reverb, delay or other time-based effects). This type differs from an insert, in that
instead of inserting a signal-changing device directly into the signal path, a portion
of the signal (which is essentially a combined mix of the desired channels) is
then “sent” to one or more effects devices. Once effected, the signal can then be
proportionately mixed back into the monitor or main out signal path, so as to
add an effects blend of the selected tracks to the overall output mix.
409
Signal Processing  CHAPTER 15
A send is used to “send” a mix of multiple channel
signals to a single effect or effects chain, after
which, it is routed to the monitor or main mix bus. 
It tends (but not always) to be time-based in nature,
meaning the processor effects time functions (i.e.,
reverb, echo, etc.)
As an example, Figure 15.7 shows how sends from any number of channel inputs
can be mixed together and then be sent to an effects device. The effected output
signal is then sent back into either the effects return section or to a set of spare
mixer inputs or directly to the main mixing bus outputs.
FIGURE 15.7
An “aux” sends path flows
in a horizontal fashion to a
send bus. The combined
sum can then be effected
(or sent to another
destination) and the
returned back to the 
monitor or main output bus.
(a) Analog aux send.
(b) Digital aux send.
Viva La Difference
There are a few insights that can help you to understand the difference between
insert and send effects routing. Let’s start by reviewing the basic ways that their
signal paths function:
n First, from Figure 15.5, we can see that the signal ﬂow of an effects insert
is basically “vertical,” moving from the channel input through an inserted
device and then back into the input strip’s signal chain (for mixing,
recording or whatever).
n From Figure 15.7 we can see that an effects send basically functions in a
“horizontal” direction, allowing portions of the various input signals to
be mixed together and sent to an effects device, which can then be routed
back into the mixer signal chain for further effects and mix blending.

Signal Paths in Effects Processing
410
n Finally, it’s important to grasp the idea that when a device is “inserted”
into a signal chain, it’s usually a single, dedicated hardware/plug-in device
(or sometimes chain of devices) that is used to perform a speciﬁc function.
If a large number of track/channels are used in a session or mix, inserting
numerous DSP effects into multiple strips could take up too much
processing power (or are simply unnecessary and hard to manage). Setting
up an effects send can save a great deal of DSP processing overhead by
creating a single effects send “mix” that can then be routed to a single
effects device (or device chain)—it works like this: “why use a whole
bunch of EFX devices to do the same job on every channel, when sending
a signal mix to a single EFX device/chain will do.”
On a ﬁnal note, each effects routing technique has its own set of strengths and
weaknesses—it’s important to play with each, so as to know the difference and
when to make best use of each of them.
SIDE CHAIN PROCESSING
Another effects processing tool that can be used to add “character” to your 
mix is the idea of side chain processing. Essentially, side chain processing (also
known as parallel processing) is a fancy word for setting up a special effects
send and return that can add a degree of spice to your overall mix. It could
quite easily be:
n A processing chain that adds varying degrees of compression, tape
saturation and/or distortion to add a bit of “grunge” to the mix (Figure
15.8).
n A send mix that is routed to loud guitar amp in the studio (or amp
simulation plug-in) that is being picked up by several close and distant
room mics to add character to the mix.
n A room simulation plug-in to add space to an otherwise lifeless mix.
FIGURE 15.8
An example of how side
chain processing can add a
bit of character to a mix.
The options and possible effects combinations are endless. By naming the send
“glue,” “dirt” “grit” or anything you want, a degree of character that is uniquely
yours can be added to the mix. I’d like to add a bit of caution here, however:
Adding grit to your mix can be all well and good (especially if it’s a rock track

411
Signal Processing  CHAPTER 15
that just begs for some extra dirt). However, it’s also possible to overdo it by
adding too much distortion into the mix. Too much of a good thing is just
that—too much.
There is one more control that you should be aware of. A number of hardware
and plug-in devices have an internal “wet/dry mix” control that serves as an
internal side-chain mix control for varying the amount of “dry” (original) signal
to be mixed with the “wet” (effected) signal (usually varied in percentage). This
setting can have an effect on your settings (when used in either an insert or
send setting) and a careful awareness is advised.
EFFECTS PROCESSING
From this point on, this chapter will be taking an in-depth look at many of the
signal processing devices, applications and techniques that have traditionally
been the cornerstone of music and sound production, including systems and
techniques that exert an ever-increasing degree of control over:
n The spectral content of a sound: In the form of equalization and bandpass
ﬁltering
n Amplitude level processing: In the form of dynamic range processing
n Time-based effects: Augmentation or re-creation of room ambience, delay,
time/pitch alterations and tons of other special effects that can range from
being sublimely subtle to “in yo’ face.”
Hardware and Plug-In Effects in Action
The following sections offer some insight into the basics of effects processing
and how they can be integrated into a recording or mixdown. It’s a forgone
conclusion that the power of these effects can be harnessed in hardware or
software plug-in form. An important rule to remember is that there are no rules;
however, there are a few general guidelines that can help you get the sound
that you want. When using effects, the most important asset you can have is
experience and your own sense of artistry. The best way to learn the art of
processing, shaping and augmenting sound is through experience; gaining that
takes time and patience.
EQUALIZATION
An audio equalizer (Figure 15.9) is a circuit, device or plug-in that lets us exert
control over the harmonic or timbral content of a recorded sound. EQ may
need to be applied to a single recorded channel, to a group of channels or to
an entire program.
Equalization refers to the alteration in frequency response of an ampliﬁer so
that the relative levels of certain frequencies are more or less pronounced 
than others. EQ is speciﬁed as either plus or minus a certain number of decibels
at a certain frequency. For example, you might want to boost a signal by 

Effects Processing
412
“+4 dB at 5 kHz.” Although only one frequency was speciﬁed in this example,
in reality a range of frequencies above, below and centered around the speciﬁed
frequency will actually be affected. The amount of boost or cut at frequencies
other than the one named is determined by whether the curve is peaking or
shelving, by the bandwidth of the curve (a factor that’s affected by the Q settings
and determines how many frequencies will be affected around a chosen
centerline), and by the amount of boost or cut at the named frequency. For
example, a +4 dB boost at 1000 Hz might easily add a degree of boost or cut
at 800 and 1200 Hz (Figure 15.10).
FIGURE 15.9
The audio equalizer. 
(a) Manley Massive Passive
Analog Stereo Equalizer.
(Courtesy of Manley
Laboratories, Inc.,
www.manleylabs.com)
(b) Sonnox Oxford EQ.
(Courtesy of Universal
Audio, www.uaudio.com.
©) 2017 Universal Audio,
Inc. All rights reserved.
Used with permission)
FIGURE 15.10
Various boost/cut EQ curves
centered around 1 kHz: 
(a) center frequency,
1-kHz bandwidth 1 octave,
±15 dB boost/cut; (b) center
frequency, 1 kHz bandwidth
3 octaves, ±15 dB
boost/cut.
Older equalizers and newer “retro” systems often base their design around
ﬁlters that use passive components (i.e., inductors, capacitors and resistors) and
employ ampliﬁers only to make up for internal losses in level, called insertion
loss. Most equalization circuits today, however, are of the active ﬁlter type that
change their characteristics by altering the feedback loop of an operational amp.
This is by far the most common analog EQ type and is generally favored over
its passive counterpart due to its low cost, size and weight, as well as its wide
gain range and line-driving capabilities.
Peaking Filters
The most common EQ curve is created by a peaking ﬁlter. As its name implies,
a peak-shaped bell curve can either be boosted or cut around a selected center
frequency. Figure 15.11a shows the curves for a peak equalizer that’s set to 
boost or cut at 1000 Hz. The quality, factor (Q) of a peaking equalizer refers 
to the width of its bell-shaped curve. A curve with a high Q will have a narrow
Peaking Filters 

bandwidth with few frequencies outside the selected bandwidth being affected,
whereas a curve having a low Q is very broadband and can affect many
frequencies (or even octaves) around the center frequency. Bandwidth is a
measure of the range of frequencies that lie between the upper and lower 
–3-dB (half-power) points on the curve (Figure 15.11b). The Q of a ﬁlter is an
inverse measure of the bandwidth (such that higher Q values mean that fewer
frequencies will be affected, and vice versa). To calculate Q, simply divide the
center frequency by the bandwidth. For example, a ﬁlter centered at 1 kHz that’s
a third of an octave wide will have its –3 dB frequency points located at 891
and 1123 Hz, yielding a bandwidth of 232 Hz (1123—891). This EQ curve’s
Q, therefore, will be 1 kHz divided by 232 Hz or 4.31.
Shelving Filters
Another type of equalizer is the shelving ﬁlter. Shelving refers to a rise or drop
in frequency response at a selected frequency, which tapers off to a preset level
and continues at that level to the end of the audio spectrum. Shelving can be
inserted at either the high or low end of the audio range and is the curve type
that’s commonly found on home stereo bass and treble controls (Figure 15.12).
413
Signal Processing  CHAPTER 15
FIGURE 15.11
Peaking equalization curves.
(a) Various Q widths. (b) The
number of hertz between
the two points that are 3 dB
down from the center
frequency determines the
bandwidth of a peaking
filter.
FIGURE 15.12
High/low, boost/cut curves
of a shelving equalizer.
High-Pass and Low-Pass Filters
Equalizer types also include high-pass and low-pass ﬁlters. As their names imply,
this EQ type allows certain frequency bandwidths to be passed at full level while
other sections of the audible spectrum are attenuated. Frequencies that are
attenuated by less than 3 dB are said to be inside the passband; those attenuated
by more than 3 dB are located outside, in the stopband. The frequency at which
the signal is attenuated by exactly 3 dB is called the turnover or cutoff frequency
and is used to name the ﬁlter frequency.
Shelving Filters 
High-Pass and Low-Pass Filters 

Effects Processing
414
Ideally, attenuation would become inﬁnite immediately outside the passband;
however, in practice this isn’t always attainable. Commonly, attenuation is
carried out at rates of 6, 12 and 18 dB per octave. This rate is called the slope
of the ﬁlter. Figure 15.13a, for example, shows a 700-Hz high-pass ﬁlter response
curve with a slope of 6 dB per octave, and Figure 15.13b shows a 700-Hz low-
pass ﬁlter response curve having a slope of 12 dB per octave. High- and low-
pass ﬁlters differ from shelving EQ in that their attenuation doesn’t level off
outside the passband. Instead, the cutoff attenuation continues to increase. A
high-pass ﬁlter in combination with a low-pass ﬁlter can be used to create a
bandpass ﬁlter, with the passband being controlled by their respective turnover
frequencies and the Q by the ﬁlter’s slope (Figure 15.14).
FIGURE 15.13
A 700-Hz filter: (a) high-
pass filter with a slope of 
6 dB per octave; (b) low-
pass filter with a slope of 
12 dB per octave.
Equalizer Types
The four most commonly used equalizer types that can incorporate one or more
of the previously described ﬁlter types are the:
n Selectable frequency equalizer
n Parametric equalizer
n Graphic equalizer
n Notch ﬁlter
The selectable frequency equalizer (Figure 15.15), as its name implies, has a set
number of frequencies from which to choose. These equalizers usually allow a
boost or cut to be performed at a number of selected frequencies with a
predetermined Q. They are most often found on older console designs, certain
low-cost production consoles and outboard gear.
FIGURE 15.14
A bandpass filter is created
by combining a high- and
low-pass filter with different
cutoff frequencies.
Equalizer Types 

The parametric equalizer (Figure 15.16a) lets you adjust most or all of its frequency
parameters in a continuously variable fashion. Although the basic design layout
will change from model to model, each band will often have an adjustment for
continuously varying the center frequency. The amount of boost or cut is also
continuously variable. Control over the center frequency and Q can be either
selectable or continuously variable, although certain manufacturers might not
have provisions for a variable Q.
Generally, each set of frequency bands will overlap into the next band section,
so as to provide smooth transitions between frequency bands or allow for
multiple curves to be placed in nearby frequency ranges. Because of its ﬂexibility
and performance, the parametric equalizer has become the standard design for
most input strips, digital equalizers and workstations.
A graphic equalizer (Figure 15.16b) provides boost and cut level control over a
series of center frequencies that are equally spaced (ideally according to music
intervals). An “octave band” graphic equalizer might, for example, have 12
equalization controls spaced at the octave intervals of 20, 40, 80, 160, 320 and
640 Hz and 1.25, 2.5, 5, 10 and 20 kHz, while 1/3-octave equalizers could have
up to 36 center frequency controls. The various EQ band controls generally use
vertical sliders that are arranged side by side so that the physical positions of
these controls could provide a “graphic” readout of the overall frequency
response curve at a glance. This type is often used in applications that can help
ﬁne-tune a system to compensate for the acoustics in various types of rooms,
auditoriums and studio control rooms.
Notch ﬁlters are often used to zero in on and remove 60- or 50-Hz hum or
other undesirable discrete-frequency noises. They use a very narrow bandwidth
to ﬁne-tune and attenuate a particular frequency in such a way as to have little
effect on the rest of the audio program. Notch ﬁlters are used more in ﬁlm
location sound and broadcast than in studio recording, because severe narrow-
band problems aren’t often encountered in a well-designed studio—hopefully.
415
Signal Processing  CHAPTER 15
FIGURE 15.15
The Warm Audio EQP-WA
selectable frequency tube
equalizer. (Courtesy of
Warm Audio LLC,
www.warmaudio.com)
FIGURE 15.16
Equalizer types: (a) the 
EQF-100 full range,
parametric vacuum tube
equalizer. (Courtesy of
Summit Audio, Inc.,
www.summitaudio.com);
(b) Rane GE 130 single-
channel, 30-band, 1/3-
octave graphic equalizer.
(Courtesy of Rane
Corporation,
www.rane.com)

Effects Processing
416
Applying Equalization
When you get right down to it, EQ is all about compensating for deﬁciencies
in a sound pickup or about reducing extraneous sounds that make their way
into a pickup signal. To start our discussion on how to apply EQ, let’s again
revisit the all-important “Good Rule” from Chapter 4.
Good musician + good instrument + good performance + good acoustics + good mic 
+ good placement = good sound.
The “Good Rule”
Whenever possible, EQ should not be used as a Band-Aid. By this, I mean that it’s
often a good idea to correct for a problem on the spot rather than to rely on the
hope that you can “ﬁx it in the mix” at a later time using EQ and other methods.
When in doubt, it’s often better to deal with a problem as it occurs. This isn’t
always possible, however—therefore, EQ is best used in situations where:
n There’s no time or money left to redo the track 
n The existing take was simply magical and shouldn’t be re-recorded 
n The track was already recorded during a previous session and is in need
of being ﬁxed.
EQ in Action!
Although most equalization is done by ear, it’s helpful to have a sense of which
frequencies affect an instrument in order to achieve a particular effect. On the
whole, the audio spectrum can be divided into four frequency bands: low (20
to 200 Hz), low-mid (200 to 1000 Hz), high-mid (1000 to 5000 Hz) and high
(5000 to 20,000 Hz). When the frequencies in the 20- to 200-Hz (low) range
are modiﬁed, the fundamental and the lower harmonic range of most bass
information will be affected. These sounds often are felt as well as heard, so
boosting in this range can add a greater sense of power or punch to music.
Lowering this range will weaken or thin out the lower frequency range.
The fundamental notes of most instruments lie within the 200- to 1000-Hz
(low-mid) range. Changes in this range often result in dramatic variations in
the signal’s overall energy and add to the overall impact of a program. Because
of the ear’s sensitivity in this range, a minor change can result in an effect that’s
very audible. The frequencies around 200 Hz can add a greater feeling of warmth
to the bass without loss of deﬁnition. Frequencies in the 500- to 1000-Hz range
could make an instrument sound hornlike, while too much boost in this range
can cause listening fatigue.
Applying Equalization 
EQ in Action! 

417
Signal Processing  CHAPTER 15
Higher-pitched instruments are most often affected in the 1000 to 5000 Hz
(high-mid) range. Boosting these frequencies often results in an added sense of
clarity, deﬁnition and brightness. Too much boost in the 1000 to 2000 Hz range
can have a “tinny” effect on the overall sound, while the upper mid-frequency
range (2000 to 4000 Hz) affects the intelligibility of speech. Boosting in this
range can make music seem closer to the listener, but too much of a boost can
also cause listening fatigue.
The 5000–20,000 Hz (high-frequency) region is composed almost entirely of
instrument harmonics. For example, boosting frequencies in this range will often
add sparkle and brilliance to a string or woodwind instrument. Boosting too
much might produce sibilance on vocals and make the upper range of certain
percussion instruments sound harsh and brittle. Boosting at around 5000 Hz
has the effect of making music sound louder. A 6 dB boost at 5000 Hz, for
example, can sometimes make the overall program level sound as though it’s
been doubled in level; conversely, attenuation can make music seem more
distant. Table 15.1 provides an analysis of how frequencies and EQ settings can
interact with various instruments. (For more information, refer to the
Microphone Placement Techniques section in Chapter 4.)
Instrument
Frequencies of Interest
Kick drum
Bottom depth at 60–80 Hz, slap attack at 2.5 kHz
Snare drum
Fatness at 240 Hz, crispness at 5 kHz
Hi-hat/cymbals
Clank or gong sound at 200 Hz, shimmer at 7.5 kHz to 12 kHz
Rack toms
Fullness at 240 Hz, attack at 5 kHz
Floor toms
Fullness at 80–120 Hz, attack at 5 kHz
Bass guitar
Bottom at 60–80 Hz, attack/pluck at 700–1000 Hz, string noise/pop at 2.5 kHz
Electric guitar
Fullness at 240 Hz, bite at 2.5 kHz
Acoustic guitar
Bottom at 80–120 Hz, body at 240 Hz, clarity at 2.5–5 kHz
Electric organ
Bottom at 80–120 Hz, body at 240 Hz, presence at 2.5 kHz
Acoustic piano
Bottom at 80–120 Hz, presence at 2.5–5 kHz, crisp attack at 10 kHz, honky-tonk
sound (sharp Q) at 2.5 kHz
Horns
Fullness at 120–240 Hz, shrill at 5–7.5 kHz
Strings
Fullness at 240 Hz, scratchiness at 7.5–10 kHz
Conga/bongo
Resonance at 200–240 Hz, presence/slap at 5 kHz
Vocals
Fullness at 120 Hz, boominess at 200–240 Hz, presence at 5 kHz, sibilance at 
7.5–10 kHz
Note: These frequencies aren’t absolute for all instruments, but are meant as a subjective guide.
Table 15.1
Instrumental Frequency Ranges of Interest

Effects Processing
418
1. Solo an input strip on a mixer, console or DAW.
Experiment with the settings using the previous
frequency ranges. Can you improve on the
original recorded track or does it take away from
the sound?
2. Using the input strip equalizers on a mixer,
console or DAW, experiment with the EQ settings
and relative instrument levels within an entire mix
using the previous frequency ranges as a guide.
Can you bring an instrument out without
changing the fader gains? Can you alter the
settings of two or more instruments to increase
the mix’s overall clarity?
3. Plug an outboard or plug-in equalizer into the
main output buses of a mixer, console or DAW,
change the program’s EQ settings using the
previous frequency range discussions as a guide.
How does it change the mix?
Try This: Equalization
D I Y
 do  it  yourself
One way to zero in on a particular frequency using an equalizer (especially a
parametric one) is to accentuate or attenuate the EQ level and then vary the
center frequency until the desired range is found. The level should then be scaled
back until the desired effect is obtained. If boosting in one-instrument range
causes you to want to do the same in other frequency ranges, it’s likely that
you’re simply overdoing it. It’s easy to get caught up in the “bigger! Better!
MORE!” syndrome of wanting an instrument to sound louder. If this continues
to happen on a mix, it’s likely that one of the frequency ranges of an instrument
or ensemble is too dominant and requires attenuation. On the subject of laying
down a recorded track with EQ, there are a number of situations and differing
opinions regarding them:
n Some will “track” (record) the sound the mic/instrument/room sound
directly to tape or disk, so that little or no EQ (or any other changes) will
be needed in mixdown.
n Some use EQ liberally to make up for placement and mic deﬁciencies,
whereas others might use it sparingly, if at all. One example where EQ is
used sparingly is when an engineer knows that someone else will be
mixing a particular song or project. In this situation, the engineer who’s
doing the mix might have a very different idea of how an instrument should
sound. If large amounts of EQ were recorded to a track during the session,
the mix engineer might have to work very hard to counteract the original
EQ settings.
n If everything was recorded ﬂat, the producer and artists might have
difﬁculty passing judgment on a performance or hearing the proper balance
during the overdub phase. Such a situation might call for equalization in
the monitor mix, while leaving the recorded tracks alone.

n In situations where several mics are to be combined onto a single track
or channel, the mics can only be individually equalized (exchanged, altered
or moved) during the recording phase. In situations where a project is to
be engineered, mixed and possibly even mastered by the same person, the
engineer might want to discuss in advance the type and amount of EQ
that the producer and/or artist might want.
n Above all, it’s wise that any “sound-shaping” should be determined and
discussed with the producer and/or artist before the sounds are committed
to a track.
In the end, there’s no getting around the fact that an equalizer is a powerful
tool. When used properly, it can greatly enhance or restore the musical and
sonic balance of a signal. Experimentation and experience are the keys to 
proper EQ usage, and no book can replace the trial-and-error process of “just
doing it!”
Before moving on, it’s important to keep one age-old viewpoint in mind—that
an equalizer shouldn’t be regarded as a cure-all for improper mic, playing or
instrument technique; rather, it should be used as a tool for correcting problems
that couldn’t be easily ﬁxed on the spot through mic and/or performance
adjustments. If an instrument is poorly recorded during an initial recording
session, it’s often far more difﬁcult and time consuming to “ﬁx it in the mix”
at a later time. Getting the best possible sound down onto tape or DAW will
deﬁnitely improve your chances for attaining a sound and overall mix that you
can be proud of in the future.
Sound-Shaping Effects Devices and Plug-Ins
Another class of effects devices that aren’t equalizers, but instead affect the overall
tonal character of a track or mix come under the category of sound-shaping
devices. These systems can either be hardware or plug-in in nature and are used
to alter the tonal and/or overtone balance of a signal. For example, a device
that’s been around for decades in the Aphex Aural Exiter. This device is able to
add a sense of presence to a sound by generating additional overtones that are
subdued or not present in the program signal. Other such devices are able to
modify the shape of a sounds transient envelope (Figure 5.17a) or to ﬁlter the
sound in unique ways (Figure 5.17b).
419
Signal Processing  CHAPTER 15
FIGURE 15.17
Sound-shaping plug-ins. 
(a) Oxford Envolution.
(b) Moog multimode filter.
(Courtesy of Universal
Audio, www.uaudio.com.
© 2017 Universal Audio,
Inc. All rights reserved.
Used with permission)
Sound-Shaping Effects Devices and Plug-Ins 

Effects Processing
420
Another class of sound-shaper comes in the form of virtual tape machine plug-
ins that closely model and emulate analog tape recorders—right down to their
sonic character, tape noise, distortion, changes with virtual tape formulation
and bias settings (Figure 15.18).
DYNAMIC RANGE
Like most things in life that get out of hand from time to time, the level of a
signal can vary widely from one moment to the next. For example, if a vocalist
gets caught up in the moment and lets out an impassioned scream following
a soft whispery passage, you can almost guarantee that the mic and preamp
will push the recording chain from its optimum recording level into severe
distortion—OUCH! Conversely, if you set an instrument’s mic to properly
accommodate the loudest level, its signal might be buried in the mix during
the rest of the song. For these and other reasons, it becomes obvious that it’s
sometimes necessary to exert some form of control over a signal’s dynamic range
by using various techniques and dynamic controlling devices. In short, the
dynamics of an audio program’s signal resides somewhere in a continuously
varying realm between three level states:
n Saturation
n Average signal level
n System/ambient noise
As you may remember from various chapters in this book that saturation occurs
when an input signal is so large that an amp’s supply voltage isn’t large enough
to produce the required output current or is so large that a digital converter
reaches full scale (where the A/D output reads as all 1’s). In either case, the
results generally don’t sound pretty and should be avoided in the channel’s
audio chain. The average signal level is where the overall signal level of a mix
often likes to reside. Logically, if an instrument’s level is too low, it can get
buried in the mix—if it’s too high, it can unnecessarily stick out and throw the
entire balance off. It is here that the art of creating an average mix level that’s
high enough to stand out in any playlist, while still retaining enough dynamic
“life,” truly becomes an applied balance of skill and magic.
FIGURE 15.18
Analog tape emulation plug-
in. (a) Oxide Tape Recorder
for the Apollo and the UAD
effects processing card.
(Courtesy of Universal
Audio, www.uaudio.com 
© 2017 Universal Audio,
Inc. All rights reserved.
Used with permission) 
(b) Slate Digital Virtual Tape
Machine. (Courtesy of Slate
Digital, www.slate
digital.com)

421
Signal Processing  CHAPTER 15
DYNAMIC RANGE PROCESSORS
The overall dynamic range of music is potentially on the order of 120 to 140 dB,
whereas the overall dynamic range of a compact disc is often 80 to 90 dB, and
analog magnetic tape is on the order of 60 dB (excluding the use of noise-
reduction systems, which can improve this ﬁgure by 15 to 30 dB). However,
when working with 24-bit digital word lengths, a system, processor or channel’s
overall dynamic range can actually approach or exceed the full range of hearing.
Even with such a wide dynamic range, unless the recorded program is played
back in a noise-free environment, either the quiet passages will get lost in the
ambient noise of the listening area (35 to 45 dB SPL for the average home and
much worse in a car) or the loud passages will simply be too loud to bear.
Similarly, if a program of wide dynamic range were to be played through a
medium with a limited dynamic range (such as the 20- to 30-dB range of an
AM radio or the 40- to 50-dB range of FM), a great deal of information would
get lost in the general background noise. To prevent such problems, the dynamics
of a program can be restricted to a level that’s appropriate for the reproduction
medium (theater, radio, home system, car, etc.) as shown in Figure 15.19. This
gain reduction can be accomplished either by manually riding the fader’s gain
or through the use of a dynamic range processor that can alter the range between
the signal’s softest and loudest passages.
FIGURE 15.19
Dynamic ranges of various
audio media, showing the
noise floor (black), average
level (white) and peak levels
(gray). (Courtesy of Thomas
Lund, tc electronic,
www.tcelectronic.com)
The concept of automatically changing the gain of an audio signal (through the
use of compression, limiting and/or expansion) is perhaps one of the most
misunderstood aspects of audio recording. This can be partially attributed to
the fact that a well-done job won’t be overly obvious to the listener. Changing
the dynamics of a track or overall program will often affect the way in which
it will be perceived (either consciously or unconsciously) by making it “seem”
louder, thereby reducing its volume range to better suit a particular medium or
by making it possible for a particular sound to ride at a better level above other
tracks within a mix.
COMPRESSION
A compressor (Figure 15.20), in effect, can be thought of as an automatic fader.
It is used to proportionately reduce the dynamics of a signal that rises above a

Effects Processing
422
user-deﬁnable level (known as the threshold) to a lesser volume range. This
process is done so that:
n The dynamics can be managed by the electronics and/or ampliﬁers in the
signal chain without distorting the signal chain.
n The range is appropriate to the overall dynamics of a playback or broadcast
medium.
n An instrument or vocal better matches the dynamics of other recorded
tracks within a song or audio program.
FIGURE 15.20
Universal Audio 1176LN
limiting amplifier. 
(Courtesy of Universal
Audio, www.uaudio.com. 
© 2017 Universal Audio,
Inc. All rights reserved.
Used with permission)
Since the signals of a track, group or program will be automatically turned down
(hence the terms compressed or squashed) during a loud passage, the overall level
of the newly reduced signal can now be ampliﬁed upwards to better ﬁt into a
mix or to match the required dynamics of the medium. In other words, once
the dynamics have been reduced downward, the overall level can be boosted
such that the range between the loud and soft levels is less pronounced (Figure
15.21). We’ve not only restored the louder signals back to a prominent level
but we have also turned up the softer signals that would otherwise be buried
in the mix or ambient background noise.
FIGURE 15.21
A compressor reduces input
levels that exceed a
selected threshold by a
specified amount. Once
reduced, the overall signal
can then be boosted in
level, thereby allowing the
softer signals to be raised
above other program or
background sounds.
The most common controls on a compressor (and most other dynamic range
devices) include input gain, threshold, output gain, slope ratio, attack, release
and meter display:

423
Signal Processing  CHAPTER 15
n Input gain: This control is used to determine how much signal will be sent
to the compressor’s input stage.
n Threshold: This setting determines the level at which the compressor will
begin to proportionately reduce the incoming signal. For example, if the
threshold is set to –20 dB, all signals that fall below this level will be un -
affected, while signals above this level will be proportionately attenuated,
thereby reducing the overall dynamics. On some devices, varying the input
gain will correspondingly control the threshold level. In this situation,
raising the input level will lower the threshold point and thus reduce the
overall dynamic range. Most quality compressors offer hard and soft knee
threshold options. A soft knee widens or broadens the threshold range,
making the onset of compression less obtrusive, while the hard knee setting
causes the effect to kick in quickly above the threshold point.
n Output gain: This control is used to determine how much signal will be
sent to the device’s output. It’s used to boost the reduced dynamic signal
into a range where it can best match the level of a medium or be better
heard in a mix.
n Slope ratio: This control determines the slope of the input-to-output gain
ratio. In simpler terms, it determines the amount of input signal (in
decibels) that’s needed to cause a 1 dB increase at the compressor’s output.
For example, linear ampliﬁer has an input-to-output ratio of 1:1 (one-to-
one), meaning for every 1 dB increase at the input, there will be a
corresponding 1 dB increase at the output (Figure 15.22a). When using a
2:1 compression ratio, below the threshold the signal will be linear (1:1),
however, above this level an increase of 2 dB will result in an increase of
only 1 dB at the output (Figure 15.22b). An increase of 4 dB above the
threshold will result in an output gain of 1 dB, when a 4:1 ratio (slope)
is selected. Get the idea?
n Attack: This setting (which is calibrated in milliseconds; 1 msec = 1
thousandth of a second) determines how fast or how slowly the device
will turn down signals that exceed the threshold. It is deﬁned as the time
it takes for the gain to decrease to a percentage (usually 63%) of its ﬁnal
gain value. In certain situations (as might occur with instruments that have
a long sustain, such as the bass guitar), setting a compressor to instantly
FIGURE 15.22
The output ratios of a
compressor. (a) A liner slope
results in an increase of 
1 dB at the output for every
1 dB increase at the input.
(b) A compression slope
follows an input/output gain
reduction ratio above the
threshold, proportionately
reducing signals that fall
above this point.

Effects Processing
424
turn down a signal might be audible (possibly creating a sound that
pumps the signal’s dynamics). In this situation, it would be best to use a
slower attack setting. On the other hand, such a setting might not give
the compressor time to react to sharp, transient sounds (such as a hi-hat).
In this case, a fast attack time would probably work better. As you might
expect, you’ll need to experiment to arrive at the fastest attack setting that
won’t audibly color the signal’s sound.
n Release: Similar to the attack setting, release (which is calibrated in milli -
seconds) is used to determine how slowly or quickly the device will restore
a signal to its original dynamic level once it has fallen below the threshold
point (deﬁned as the time required for the gain to return to 63% of its
original value). Too fast a setting will cause the compressor to change
dynamics too quickly (creating an audible pumping sound), while too
slow a setting might affect the dynamics during the transition from a loud
to a softer passage. Again, it’s best to experiment with this setting to arrive
at the slowest possible release that won’t color the signal’s sound.
n Meter display: This control changes the compressor’s meter display to read
the device’s output or gain reduction levels. In some designs, there’s no
need for a display switch, as readouts are used to simultaneously display
output and gain reduction levels.
As was previously stated, the use of compression (and most forms of dynamics
processing) is often misunderstood, and compression can easily be abused.
Generally, the idea behind these processing systems is to reduce the overall
dynamic range of a track, music or sound program or to raise its overall perceived
level without adversely affecting the sound of the track itself. It’s a well-known
fact that over-compression can actually squeeze the life out of a performance
by limiting the dynamics and reducing the transient peaks that can give life to
a performance. For this reason, it’s important to be aware of the general nuances
of the controls that have been discussed.
During a recording or mixdown session, compression can be used in order to
balance the dynamics of a track to the overall mix or to keep the signals from
overloading preamps, the recording medium and your ears. Compression should
be used with care for any of the following reasons:
n Minimize changes in volume that might occur whenever the dynamics of
an instrument or vocal are too wide for the mix. As a tip, a good starting
point might be a 0 dB threshold setting at a 4:1 ratio, with the attack and
release controls set at their middle positions.
n Smooth out momentary changes in source-to-mic distance.
n Balance out the volume ranges of a single instrument. For example, the
notes of an electric or upright bass often vary in volume from string to
string. Compression can be used to “smooth out” the bass line by matching
their relative volumes (often using a slow attack setting). In addition, some
instruments (such as horns) are louder in certain registers because of the

amount of effort that’s required to produce these notes. Compression is
often useful for smoothing out these volume changes. As a tip, you might
start with a ratio of 5:1 with a medium-threshold setting, medium attack
and slower release time. Over-compression should be avoided to avoid
pumping effects.
n Reduce other frequency bands by inserting a ﬁlter into the compression
chain that causes the circuit to compress frequencies in a speciﬁc band
(multi-band compression). A common example of this is a deesser, which
is used to detect high frequencies in a compressor’s circuit so as to suppress
those “SSSS,” “CHHH” and “FFFF” sounds that can distort or stand out
in a recording.
n Reduce the dynamic range and/or boost the average volume of a mix so
that it appears to be signiﬁcantly louder (as occurs when a song’s volume
sticks out in a playlist or a television commercial seems louder than your
favorite show).
Although it may not always be the most important, this last application often
gets a great deal of attention, because many producers strive to cut their
recordings as “hot” as possible. That is, they want the recorded levels to be as
far above the normal operating level as possible without blatantly distorting.
In this competitive business, the underlying logic behind the concept is that
louder recordings (when placed into a Top 40, podcast, phone or MP3 playlist)
will stand out from the softer recordings and get noticed. In fact, reducing the
dynamic range of a song or program’s dynamic range will actually make the
overall levels appear to be louder. By using a slight (or not-so-slight) amount
of compression and limiting to squeeze an extra 1 or 2 dB gain out of a song,
the increased gain will also add to the perceived bass and highs because of our
ears’ increased sensitivity at louder levels (remember the Fletcher-Munson curve
discussed in Chapter 2?). To achieve these hot levels without distortion,
multiband compressors and limiters often are used during the mastering process
to remove peaks and to raise the average level of the program. You’ll ﬁnd more
on this subject in Chapter 20 (Mastering).
Compressing a mono mix is done in much the same way as one might compress
a single instrument—although greater care should be taken. Adjusting the
threshold, attack, release and ratio controls are more critical in order to prevent
“pumping” sounds or loss of transients (resulting is a lifeless mix). Compressing
a stereo mix gives rise to an additional problem: If two independent compressors
are used, a peak in one channel will only reduce the gain on that channel and
will cause sounds that are centered in a stereo image to shift (or jump) toward
the channel that’s not being compressed (since it will actually be louder). 
To avoid this center shifting, most compressors (of the same make and model)
can be linked as a stereo pair. This procedure of ganging the two channels
together interconnects the signal-level sensing circuits in such a way that a gain
reduction in one channel will cause an equal reduction in the other.
425
Signal Processing  CHAPTER 15

Effects Processing
Before moving on, let’s take a look at a few examples of the use of compression
in various applications. Keep in mind, these are only beginning suggestions—
nothing can substitute for experimenting and ﬁnding the settings that work best
for you and the situation:
n Acoustic guitar: A moderate degree of compression (3 to 8 dB) with a
medium compression ratio can help to pull an acoustic forward in a mix.
A slower attack time will allow the string’s percussive attack to pass through.
n Bass guitar: The electric bass is often a foundation instrument in pop and
rock music. Due to variations in note levels from one note to another on
an electric bass guitar (or upright acoustic, for that matter), a compressor
can be used to even out the notes and add a bit of presence and/or punch
to the instrument. Since the instrument often (but not always) has a
slower attack, it’s often a good idea to start with a medium attack (4:1,
for example) and threshold setting, along with a slower release time setting.
Harder compression of up to 10:1 with gain reductions ranging from 5
to 10 dB can also give a good result.
n Brass: The use of a faster attack (1 to 5 ms) with ratios that range from
6:1 to 15:1 and moderate to heavy gain reduction can help keep the brass
in line.
n Electric guitar: In general, an electric guitar won’t need much compression,
because the sound is often evened out by the amp, the instrument’s natural
sustain character and processing pedals. If desired, a heavier compression
ratio, with 10 or more decibels of compression can add to the instrument’s
“bite” in a mix. A faster attack time with a longer release is often a good
place to start.
n Kick drum and snare: These driving instruments often beneﬁt from added
compression. For the kick, a 4:1 ratio with an attack setting of 10 ms or
slower can help emphasize the initial attack while adding depth and
presence. The snare attack settings might be faster, so as to catch the initial
transients. Threshold settings should be set for a minimum amount of
reduction during a quiet passage, with larger amounts of gain reduction
happening during louder sections.
n Synths: These instruments generally don’t vary widely in dynamic range,
and thus won’t require much (or any) compression. If needed, a 4:1 ratio
with moderate settings can help keep synth levels in check.
n Vocals: Singers (especially inexperienced ones) will often place the mic
close to their mouths. This can cause wide volume swings that change
with small moves in distance. The singer might also shout out a line just
after delivering a much quieter passage. These and other situations lead
to the careful need for a compressor, so as to smooth out variations in
level. A good starting point would be a threshold setting of 0 dB, with a
ratio of 4:1 with attack and release settings set at their midpoints. Gain
reductions that fall between 3 and 6 dB will often sit well in a mix
(although some rock vocalists will want greater compression) be careful
426

427
Signal Processing  CHAPTER 15
of over-compression and its adverse pumping artifacts. Given digital’s
wide dynamic range, you might consider adding compression later in the
mixdown phase, rather than during the actual session.
n Final mix compression: It’s often a common practice to compress an entire
mix during mixdown. If the track is to be professionally mastered, you
should consult with the mastering engineer before the deed is done (or
you might provide him or her with both a compressed and uncompressed
version). When applying bus compression, it is usually a good idea to
start with medium attack and release settings, with a light compression
ratio (say, 4:1). With these or your preferred settings, reduce the threshold
detection until a light amount of compression is seen on the meter display.
Levels of between 3 and 6 dB will provide a decent amount of compression
without audible pumping or artifacts (given that a well-designed unit or
plug-in is used).
1. Go to the Tutorial section of www.modrec.com
and download the tutorial sound ﬁles that relate
to compression (which include instrument/music
segments in various dynamic states).
2. Listen to the tracks. If you have access to an
editor or DAW, import the ﬁles and look at the
waveform amplitudes for each example. If you’d
like to DIY, then:
3. Record or obtain an uncompressed bass guitar
track and monitor it through a compressor or
compression plug-in. Increase the threshold level
until the compressor begins to kick in. Can you
hear a difference? Can you see a difference on
the console or mixer meters?
4. Set the levels and threshold to a level you like
and then set the attack time to a slow setting.
Now, select a faster setting and continue until it
sounds natural. Try setting the release to its
fastest setting. Does it sound better or worse?
Now select a slower setting. Does it sound more
natural?
5. Repeat the above routine and settings using a
snare drum track. Were your ﬁndings any
different?
Try This: Compression
D I Y
 do  it  yourself
Multiband Compression
Multiband compression (Figure 15.23) works by breaking up the audible
spectrum into various frequency bandwidths through the use of multiple
bandpass ﬁlters. This allows each of the bands to be isolated and processed in
ways that strictly minimize the problems or maximize the beneﬁts in a particular
band. Although this process is commonly done in the ﬁnal mastering stage,
multiband techniques can also be used on an instrument or grouping. For
example:
Multiband Compression 

Effects Processing
428
n The dynamic upper range of a slap bass could be lightly compressed, while
heavier amounts of compression could be applied to the instrument’s lower
register.
n An instrument’s high end can be brightened simply by adding a small
amount of compression. This can act as a treble boost while reducing any
sharp attacks that might jump out in a mix.
LIMITING
If the compression ratio is made large enough, a compressor will actually
become a limiter. A limiter (Figure 15.24) is used to keep signal peaks from
exceeding a speciﬁed level in order to prevent the overloading of ampliﬁer
signals, recorded signals onto tape or disc, broadcast transmission signals, and
so on. Most limiters have ratios of 10:1 (above the threshold, for every 10 dB
increase at the input there will be a gain of 1 dB at the output) or 20:1 (Figure
15.25), although some have ratios that can range up to 100:1. Since a large
increase above the threshold at the input will result in a very small increase at
its output, the likelihood of overloading any equipment that follows the limiter
will be greatly reduced. Limiters have three common functions:
n To prevent signal levels from increasing beyond a speciﬁed level: Certain types
of audio equipment (often those used in broadcast transmission) are often
designed to operate at or near their peak output levels. Signiﬁcantly
increasing these levels beyond 100% would severely distort the signal and
possibly damage the equipment. In these cases, a limiter can be used to
prevent signals from signiﬁcantly increasing beyond a speciﬁed output
level.
n To prevent short-term peaks from reducing a program’s average signal level:
Should even a single high-level peak exist at levels above the program’s
rms average, the overall level can be signiﬁcantly reduced. This is especially
true whenever a digital audio ﬁle is normalized at any percentage value,
because the peak level will become the normalized maximum value and
not the average level. Should only a few peaks exist in the ﬁle, they can
easily be zoomed in on and manually reduced in level. If multiple peaks
exist, then a limiter should be considered.
FIGURE 15.23
Multiband compressors. 
(a) Universal Audio’s UAD
Precision Multiband plug-in.
(Courtesy of Universal
Audio, www.uaudio.com.
© 2017 Universal Audio,
Inc. All rights reserved.
Used with permission)
(b) Steinberg multiband
compressor. (Courtesy
of Steinberg Media
Technologies GmbH,
a division of Yamaha
Corporation,
www.steinberg. net)

429
Signal Processing  CHAPTER 15
FIGURE 15.24
Limiter plug-ins. 
(a) Waves L1 Ultramaximizer
limiting/quantization plug-
in. (Courtesy of Waves Ltd.,
www.waves.com) 
(b) Universal Audio’s UAD
Precision Limiter plug-in.
(Courtesy of Universal
Audio, www.uaudio.com. 
© 2017 Universal Audio,
Inc. All rights reserved.
Used with permission)
n To prevent high-level, high-frequency peaks from distorting analog tape: When
recording to certain media (such as cassette and videotape), high-energy,
transient signals actually don’t signiﬁcantly add to the program’s level
however, if allowed to pass, these transients can easily result in distortion
or tape saturation.
Unlike the compression process, extremely short attack and release times are
often used to quickly limit fast transients and to prevent the signal from being
audibly pumped. Limiting a signal during the recording and/or mastering phase
should only be used to remove occasional high-level peaks, as excessive use
would trigger the process on successive peaks and would be noticeable. If the
program contains too many peaks, it’s probably a good idea to reduce the level
to a point where only occasional extreme peaks can be detected.
EXPANSION
Expansion is the process by which the dynamic range of a signal is propor -
tionately increased. Depending on the system’s design, an expander (Figure
15.26) can operate either by decreasing the gain of a signal (as its level falls
below the threshold) or by increasing the gain (as the level rises above it). Most
expanders are of the ﬁrst type, in that as the signal level falls below the expansion
threshold the gain is proportionately decreased (according to the slope ratio),
thereby increasing the signal’s overall dynamic range (Figure 15.27). These
devices can also be used as noise reducers. You can do this by adjusting the
device so that the noise is downwardly expanded during quiet passages, while
FIGURE 15.25
The output ratios of a
limiter. (a) A liner slope
results in an increase of 
1 dB at the output for every
1 dB increase at the input.
(b) A limiter slope follows a
very high input/output gain
reduction ratio (10:1, 20:1
or more) above the
threshold, proportionately
“limiting” signal level, so
that they do not increase
above a set point.

Effects Processing
430
1. Go to the Tutorial section of www.modrec.com,
click on limiting and download the sound ﬁles
(which include instrument/music segments in
various states of limiting).
2. Listen to the tracks. If you have access to an
editor or DAW, import the ﬁles and look at the
waveform amplitudes for each example. If you’d
like to DIY, then:
3. Feed an isolated track or entire mix through a
limiter or limiting plug-in.
4. With the limiter switched out, turn the signal up
until the meter begins to peg (you might want to
turn the monitors down a bit).
5. Now reduce the level and turn it up again—this
time with the limiter switched in. Is there a point
where the level stops increasing, even though
you’ve increased the input signal? What does the
gain reduction meter show? Decrease and
increase the threshold level and experiment with
the signal’s dynamics. What did you ﬁnd out?
Try This: Limiting
D I Y
 do  it  yourself
louder program levels are unaffected or only moderately reduced. As with any
dynamics device, the attack and release settings should be carefully set to best
match the program material. For example, choosing a fast release time for an
instrument that has a long sustain can lead to audible pumping effects.
Conversely, slow release times on a fast-paced, transient instrument could cause
the dynamics to return to its linear state more slowly than would be natural.
As always, the best road toward understanding this and other dynamics processes
is through experimentation.
FIGURE 15.26
The Aphex Model 622
Logic-Assisted
Expander/Gate. (Courtesy 
of Aphex Systems, Inc.,
www.aphex.com
FIGURE 15.27
Commonly, the output of an
expander is linear above the
threshold and follows a low
input/output gain expansion
ratio below this point.

THE NOISE GATE
One other type of expansion device is the noise gate (Figure 15.28). This device
allows a signal above a selected threshold to pass through to the output at unity
gain (1:1) and without dynamic processing; however, once the input signal falls
below this threshold level, the gate acts as an inﬁnite expander and effectively
mutes the signal by fully attenuating it. In this way, the desired signal is allowed
to pass while background sounds, instrument buzzes, leakage or other unwanted
noises that occurs between pauses in the music are muted. Here are a few
examples of where a noise gate might be used:
n To reduce leakage between instruments. Often, parts of a drum kit fall
into this category; for example, a gate can be used on a high-tom track in
order to reduce excessive leakage from the snare.
n To eliminate tape or system noise from an instrument or vocal track during
silent passages.
431
Signal Processing  CHAPTER 15
FIGURE 15.28
Noise gates are 
commonly included within
many dynamic plug-in
processors. (a) Noise Gate
Plug-in. (Courtesy of 
Avid Technology, Inc.,
www.avid.com) 
(b) Noise Gate Plug-in.
(Courtesy of Steinberg
Media Technologies GmbH,
a division of Yamaha
Corporation, www.
steinberg. net) (c) Noise
Gate found on Duality
console. (Courtesy of Solid
State Logic, www.solid-
state-logic.com)
The general rules of attack and release apply to gating as well. Fortunately, these
settings are a bit more obvious during the gating process than with any other
dynamic tool. Improperly set attack and release times will often be immediately
obvious when you’re listening to the instrument or vocal track (either on its
own or within a mix) because the sound will cut in and out at inappropriate
times.
Commonly, a key input (as previously shown in Figure 15.6) is included as a
side-chain path for triggering a noise gate. A key input is an external control
that allows an external analog signal source (such as a miked instrument or
signal generator) to trigger the gate’s audio output path. For example, a mic or
recorded track of a kick drum could be used to key a low-frequency oscillator.
Whenever the kick sounds, the oscillator will be passed through the gate. By
combining the two, you can have a deep kick sound that’ll make the room
shake, rattle and roll.

Effects Processing
432
Time-Based Effects
Another important effects category that can be used to alter or augment a signal
revolves around delays and regeneration of sound over time. These time-based
effects often add a perceived depth to a signal or change the way we perceive
the dimensional space of a recorded sound. Although a wide range of time-
based effects exist, they are all based on the use of delay (and/or regenerated
delay) to achieve such results as:
n Time-delay or regenerated echoes, chorus and ﬂanging
n Reverb
DELAY
One of the most common effects used in audio production today alters the
parameter of time by introducing various forms of delay into the signal path.
Creating a delay circuit is a relatively simple task to accomplish digitally.
Although dedicated delay devices (often referred to as digital delay lines, or
DDLs) are readily available on the market, most multifunction signal processors
and time-related plug-ins are capable of creating this straightforward effect
(Figure 15.29). In its basic form, digital delay is accomplished by storing sampled
audio directly into RAM. After a deﬁned length of time (usually measured in
milliseconds), the sampled audio can be read out from memory for further
processing or direct output (Figure 15.30a). Using this basic concept, a wide
range of effects can be created simply by assembling circuits and program
algorithms into blocks that can introduce delays or regenerated echo loops. Of
course, these circuits will vary in complexity as new processing blocks are
introduced.
FIGURE 15.29
Delay plug-ins. (a) Pro Tools
Mod Delay II. (Courtesy of
Avid Technology, Inc.,
www.avid.com) (b) Galaxy
Tape Echo Plug-in.
(Courtesy of Universal
Audio, www.uaudio.com. 
© 2017 Universal Audio,
Inc. All rights reserved.
Used with permission)
Delay in Action: Less than 15 ms
Probably the best place to start looking at the delay process is at the sample
level. By introducing delays downward into the microsecond (one millionth of
a second) range, control over a signal’s phase characteristics can be introduced
to the point where selective equalization actually begins to occur. In reality,
controlling very short-term delays is actually how EQ is carried out in both the
analog and digital domains!
Delay in Action: Less than 15 ms 

Whenever delays that fall below the 15-ms range are slowly varied over time
and then are mixed with the original undelayed signal, an effect known as
combing is created. Combing is the result of changes that occur when equalized
peaks and dips appear in the signal’s frequency response. By either manually
or automatically varying the time of one or more of these short-term delays, a
constantly shifting series of effects known as ﬂanging can be created. Depending
on the application, this effect (which makes a unique “swishing” sound that’s
often heard on guitars or vocals) can range from being relatively subtle to
having moderate to wild shifts in time and pitch. It’s interesting to note the
differences between the effects of phasing and ﬂanging. Phasing uses all-pass
ﬁlters to create uneven peaks and notches, whereas ﬂanging uses delay lines to
create even peaks and notches although, the results are somewhat similar.
Delay in Action: 15 to 35 ms
By combining two identical (and often slightly delayed) signals that are slightly
detuned in pitch from one another, an effect known as chorusing can be created.
Chorusing is an effects tool that’s often used by guitarists, vocalists and other
musicians to add depth, richness and harmonic structure to their sound.
Increasing delay times into the 15- to 35-ms range will create signals that are
spaced too closely together to be perceived by the listener as being discrete delays.
Instead, these closely spaced delays create a doubling effect when mixed with an
instrument or group of instruments (Figure 15.30b). In this instance, the delays
actually fool the brain into thinking that more instruments are playing than
actually are—subjectively increasing the sound’s density and richness. This effect
can be used on background vocals, horns, string sections and other grouped
instruments to make the ensemble sound as though it has doubled (or even
tripled) its actual size. This effect also can be used on foreground tracks, such
as vocals or instrument solos, to create a larger, richer and fuller sound. Some
“chorus” delay devices introduce slight changes in delay and pitch shifting,
allowing for detunings that can create an interesting, humanized sound.
433
Signal Processing  CHAPTER 15
FIGURE 15.30
Digital delay. (a) A ddl
stores sampled audio into
RAM, where it can be read
out at a later time. (b) In
certain instances, ddl
(doubling or double delay)
can fool the brain into
thinking that more
instruments are playing than
actually are.
Should time or budget be an issue, it’s also possible to create this doubling
effect by actually recording a second pass to a new set of tracks. Using this
method, a 10-piece string section could be made to sound like a much larger
ensemble. In addition, this process automatically gives vocals, strings, keyboards
and other legato instruments a more natural effect than the one you get by
using an electronic effects device. This having been said, these devices can
Delay in Action: 15 to 35 ms 

Effects Processing
434
actually go a long way toward duplicating the effect. Some delay devices even
introduce slight changes in delay times in order to create a more natural,
humanized sound. As always, the method you choose will be determined by
your style, your budget and the needs of your particular project.
Delay in Action: More than 35 ms
When the delay time is increased beyond the 35- to 40-ms point, the listener
will begin to perceive the sound as being a discrete echo. When mixed with the
original signal, this effect can add depth and richness to an instrument or range
of instruments that can really add interest to an instrument within a mix.
Adding delays to an instrument that are tied to the tempo of a song can go
even further toward adding a degree of depth and complexity to a mix. Most
delay-based plug-ins make it easy to insert tempo-based delays into a track. For
hardware delay devices, it’s usually necessary to calculate the tempo math that’s
required to match the session. Here’s the simple math for making these
calculations:
60,000/tempo = time (in ms)
For example, if a song’s tempo is 100 bpm (beats per minute), then the amount of delay needed to match the
tempo at the beat level would be:
60,000/100 = 600 ms
Using divisions of this ﬁgure (300, 150, 75, etc.) would insert delays at 1/2, 1/4, 1/8 measure intervals.
Caution should be exercised when adding delay to an entire musical program,
because the program could easily begin to sound muddy and unintelligible. By
feeding the delayed signal back into the circuit, a repeated series of echo . . .
echo . . . echoes can be made to simulate the delays of yesteryear—you’ll
deﬁnitely notice that Elvis is still in the house.
REVERB
In professional audio production, natural acoustic reverberation is an extremely
important tool for the enhancement of music and sound production. A properly
designed acoustical environment can add a sense of space and natural depth
to a recorded sound that’ll often affect the performance as well as its overall
sonic character. In situations where there is little, no or substandard natural
ambience, a high-quality reverb device or plug-in (Figure 15.31) can be extremely
helpful in ﬁlling the production out and giving it a sense of dimensional space
and perceived warmth. In fact, reverb consists of closely spaced and random
multiple echoes that are reﬂected from one boundary to another within a
determined space (Figure 15.32). This effect helps give us perceptible cues as
Delay in Action: More than 35 ms 

435
Signal Processing  CHAPTER 15
1. Go to the Tutorial section of www.modrec.com
and download the delay tutorial sound ﬁles
(which include segments with varying degrees of
delay).
2. Listen to the tracks. If you’d like to DIY, then:
3. Insert a digital delay unit or plug-in into a
program channel and balance the dry track’s
output-mix, so that the input signal is set equally
with the delayed output signal. (Note: If there is
no mix control, route the delay unit’s output to
another input strip and combine
delayed/undelayed signals at the console.)
4. Listen to the track with the mix set to listen
equally to the dry and effected signal.
5. Vary the settings over the 1- to 10-ms range. Can
you hear any rough EQ effects?
6. Manually vary the settings over the 10- to 35-ms
range. Can you simulate a rough phasing effect?
7. Increase the settings above 35 ms. Can you hear
the discrete delays?
8. If the unit has a phaser setting, turn it on. How
does it sound different?
9. Now change the delay settings a little faster to
create a wacky ﬂange effect. If the unit has a
ﬂange setting, turn it on. Try playing with the
time-based settings that affect its sweep rate.
Fun, huh?
Try This: Delay
D I Y
 do  it  yourself
to the size, density and nature of a space (even though it might have been
artiﬁcially generated). These cues can be broken down into three subcomponents:
n Direct signal
n Early reﬂections
n Reverberation
The direct signal is heard when the original sound wave travels directly from the
source to the listener. Early reﬂections is the term given to those ﬁrst few reﬂections
that bounce back to the listener from large, primary boundaries in a given space.
Generally, these reﬂections are the ones that give us subconscious cues as to
the perception of size and space. The last set of reﬂections makes up the signal’s
FIGURE 15.31
Digital reverb effects
processors. (a) Bricasti M7
Stereo Reverb Processor.
(Courtesy of Bricasti Design
Ltd, www.bracasti.com) 
(b and c) AMS rmx16 and
EMT 140 reverb plug-ins for
the Apollo and the UAD
effects processing card.
(Courtesy of Universal
Audio, www.uaudio.com 
© 2017 Universal Audio,
Inc. All rights reserved.
Used with permission)

Effects Processing
436
reverberation characteristic. These sounds are comprised of zillions of random
reﬂections that travel from boundary to boundary within the conﬁnes of a 
room. These reﬂections are so closely spaced in time that the brain can’t discern
them as individual reﬂections, so they’re perceived as a single, densely decaying
signal.
Reverb Types
By varying program and setting parameters, a digital reverb device can be used
to simulate a wide range of acoustic environments, reverb devices and special
effects. A few popular categories include:
n Hall: Simulates the acoustics of a concert hall. This is often a diffuse, lush
setting with a longer RT60 decay time (the time that’s required for a sound
to decay by 60 dB).
n Chamber: Simulates the acoustics of an echo chamber. Like a live chamber,
these settings often simulate the brighter reﬂectivity of tile or cement
surfaces.
n Room: As you might expect, these settings simulate the acoustics of a mid-
to large-sized room. It’s often best suited to intimate solo instruments or
a chamber atmosphere.
n Live (stage): Simulates a live performance stage. These settings can vary
widely but often simulate long early-delay reﬂections.
n Spring: Simulates the low-ﬁdelity “boingyness” of yesteryear’s spring reverb
devices.
n Plate: Simulates the often-bright diffuse character of yesteryear’s metallic
plate reverb devices. These settings are often used on vocals and percussion
instruments.
n Reverse: These backward-sounding effects are created by reversing the decay
trail’s envelope so that the decay increases in level over time and is quickly
cut off at the tail end, yielding a sudden break effect. This can also be
realistically created in a DAW by reversing a track or segment, applying
reverb, and then reversing it again to yield a true backward reverb trail.
n Gate: Cuts off the decay trail of a reverb signal. These settings are often
used for emphasis on drums and percussion instruments.
FIGURE 15.32
Signal level versus reverb
time.
Reverb Types 

437
Signal Processing  CHAPTER 15
PSYCHOACOUSTIC ENHANCEMENT
A number of signal processors rely on psychoacoustic cues in order to fool the
brain into perceiving a particular effect. The earliest and most common of these
devices are those that enhance the overall presence of a signal or entire recording
by synthesizing upper-range frequency harmonics and inserting them into a
mix in order to brighten the perceived sound. Although the additional harmonics
won’t signiﬁcantly affect the program’s overall volume, the effect is a marked
increase in its perceived presence. Other psychoacoustic devices that make use
of complex harmonic, phase, delay and equalization parameters have become
standard production tools in the ﬁeld of mastering in order to shape the ﬁnal
sound into one that’s interesting, with a sonic character all their own.
In addition to synthesizing harmonics in order to change or enhance a recording
or track, other digital psychoacoustic processors deal exclusively with the subject
of spatialization (the placement of an audio signal within a three-dimensional
acoustic ﬁeld), even though the recording is being played back over stereo
speakers. By varying the parameters of a stereo or multiple input source, this
processing function creates phase and amplitude paths that can fool the brain
into perceiving that the stereo image is actually emanating from a sound ﬁeld
that’s wider than the physical speaker positions. In practice, care should be taken
when using these devices, because the effect is often carried off with degrees of
success that vary from system to system. In addition, the use of phase
relationships to expand the stereo sound ﬁeld can actually cause obvious
cancellation problems when the program is listened to in mono.
PITCH SHIFTING
Ever had a perfectly good vocal take that was spoiled by just one or two ﬂat
notes? Or had a project come in the door with a guitar track that was out of
tune? Or needed to change the key on a 30-second radio spot? It’s times like
these that pitch shifting can save your day! Pitch shifting can be used to vary
the pitch of a signal or sound ﬁle (either upward or downward) in order to
transpose the relative pitch of an audio program without affecting its duration.
This process can take place in either real time or non-real time. Pitch shifting
1. Open up a DAW session that contains various
instruments (such as a snare, guitar and vocal).
2. Insert (or send) a reverb plug-in into each track.
3. Listen to each track, while changing the plug-in
presets between “hall,” “plate,” “small room,”
etc. How do they sound different? Do they sound
different for each instrument?
Try This: Reverb Types
D I Y
 do  it  yourself

Effects Processing
438
works by writing sampled audio data to a temporary memory, where it’s
resampled to either a higher or a lower sample rate (according to the desired
ﬁnal pitch). Once this is done, the processor either adds interpolated samples
to (lowers the pitch) or subtracts them from (raises the pitch) the resampled
data to return it back to the original output rate, while keeping the altered pitch
intact. Figure 15.33 gives two basic examples of how this is often carried out.
FIGURE 15.33
Two pitch-shift examples
with an initial 1-kHz digital
signal and a sample rate of
44.1 kHz: (a) The signal can
be halved in pitch (to 500
Hz) by internally down-
sampling to a new rate of
22.05 k. To return the
output rate to 44.1 (while
retaining the 500-Hz pitch),
new sample points must be
added into each dropped
position. (b) The signal 
can be doubled in pitch 
(to 2 kHz) by internally
upsampling to a new rate of
88.2 k. To return the output
rate to 44.1 (while retaining
the 2-kHz pitch), every other
sample point must be
dropped.
A degree of caution should be used when changing the pitch of a program or
audio segment. Whenever uneven or minute interval changes are made, the
interpolation of samples doesn’t always fall perfectly into place. This can lead
to digital artifacts that adds amounts of harmonic distortion that can range from
slightly noticeable to unacceptable. If the track is in the background, there
shouldn’t be a problem; however, care should be taken with upfront instruments
and vocals. It’s important to keep in mind that large pitch changes might be
more noticeable. As always, your ears are the best judge.
Time and Pitch Changes
By combining variable sample rates and pitch shifting techniques, it’s possible
to create three different variations:
n Time change: A program’s length can be altered, without affecting its pitch,
by raising or lowering its playback sample rate.
n Pitch change: A program’s length can remain the same while pitch is shifted
either up or down.
n Both: Both a program’s pitch and length can be altered by means of simple
resampling techniques.
These functions have become an important part of the signal processing and
music production arsenals that are used by the audio-for-video, ﬁlm and
broadcast industries. They help give producers control over the running time of
ﬁlm video and audio soundtracks while maintaining the original, natural pitch
of voice, music and effects. For example, using a DAW, we could add 5 seconds
onto the end of an existing 25-second public service radio spot simply by time
shifting the 25-second spot to 30 seconds (while keeping the pitch intact).
Time and Pitch Changes 

In addition to the basic time/pitch techniques that are commonly used in music
production (most often by electronic musicians), this technology has allowed
for the huge explosion in loop-based music composition and production. These
popular programs and music plug-ins involve the use of recorded sound ﬁles
that are encoded with headers that include information on their native tempo
and length (in both samples and beats). When you set the loop program to a
master tempo (or a speciﬁc tempo at that point in the score), a loop segment,
once imported, can go about the process of recalculating its pitch and tempo
to match the current session tempo and—voilà! The ﬁle’s in sync with the song!
Further info on loop-based production tools can be found in Chapter 8 (Groove
Tools and Techniques).
439
Signal Processing  CHAPTER 15
It’s important to realize that the algorithms that
control how time and pitch are to be calculated
differ between programs (as well as having various
algorithm options within the same program). It would
be wise to read up on how your favorite programs
deal with these shifting techniques (or better yet,
listen to various percussion and pad-based loops)
and then familiarize yourself with how the different
algorithms sound. The quality of your sound ﬁles will
thank you.
Automatic Pitch Correction
One other pitch correction category makes use of auto-tuning software or plug-
ins that are able to automatically detect off-tune or slight pitch-bend segments
of a track and then correct these inaccuracies (according to user parameters).
These systems (Figure 15.34) are commonly used in modern music production
in situations that range from:
n Simple track pitch corrections to ﬁx the overall tuning of a track.
n Automated and manual micro-tuning to correct bad or out-of-tune notes
in a vocal or instrument performance.
n Using parameters that ﬁx tuning steps that can create an absolutely “perfect”
performance.
FIGURE 15.34
Automatic pitch correction
software. (a) Melodyne
Editor auto pitch correction
system. (Courtesy of
Celemony Software GmbH,
www.celemony.com). 
(b) Auto-Tune pitch
correction system. 
(Courtesy of Antares Audio
Technologies,
www.antarestech.com)
Automatic Pitch Correction 

Effects Processing
n Creating an extreme tuning effect that exaggerates the various tuning steps
in pitch to give a non-human, robotic-like effect to a vocal or instrumental
performance
Often, these pitch editors allow us to change the pitch in a straightforward,
graphical environment, according to musical and/or user-deﬁned parameters.
Single notes or entire chords can be built up, altered or changed, making it
possible to correct a wide range of material. It’s even possible to alter individual
notes within a polyphonic recording. For example, you could ﬁx a single wrong
note in a piano recording or change the notes within a guitar backing track—
all after the recording has been made. In short, automatic pitch correction has
become practically an art in itself, and several have made a career out of it. Of
course, there is too much to fully cover the nuances and intricacies of pitch
correction here, fortunately, there are hundreds (maybe thousands) of video
tutorials on the subject. Dive into them, then dive into the software—experiment
and have fun!
Of course, the use of automatic tuning to ﬁx a track isn’t without its critics. In
1998 Cher released one of the ﬁrst popular songs with Auto-Tune Believe, and
since then, pitch correction has become a mainstream tool for pop, country,
hip hop, you-name-it. As a tool, pitch correction can be carefully used to make
a vocal or track sound more in tune, to many, it’s use (some would say, overuse)
takes away from the artistry and humanity of a performance. As with most tools
of the effects trade, it’s wise to take the time to become familiar with the details,
depth and consequences of pitch correction and then talk with the musicians
and producer about their intentions.
MULTIPLE-EFFECTS DEVICES
Since most digital signal processors are by nature multifunctional chameleons,
it follows that most hardware and certain plug-in processors can be easily
programmed to perform various functions. For this reason, many digital systems
have been designed to perform as multiple-effects devices (Figure 15.35).
Multiple effects, in this case, can have several basic meanings:
440
FIGURE 15.35
Multi-effects processing
devices. (a) Controller for 
tc electronic System 6000
digital effects processor.
(Courtesy of tc electronic,
www.tcelectronic.com) 
(b) Lexicon 224 reverb
plug-in for the Apollo and
the UAD effects processing
card. (Courtesy of Universal
Audio, www.uaudio.com
© 2017 Universal Audio,
Inc. All rights reserved.
Used with permission)

441
Signal Processing  CHAPTER 15
n A single device might offer a wide range of processing functions but allow
only one effect to be called up at a time.
n A single device might offer a range of processing functions that can be
“stacked” to perform a number of simultaneous effects.
n An effects device might have multiple ins and outs, each of which can
perform several processing functions (effectively giving you multiple
processors that can be used in a multichannel mixdown environment).
DYNAMIC EFFECTS AUTOMATION AND EDITING
One of the joys of working with effects is the ability to manipulate and vary
effects parameters in real time over the duration of a song or audio program.
By altering parameters, changing settings and mixing effects levels—the subtle
variations in expression can add a great deal of interest to a project.
The ability to dynamically automate effects settings can be accomplished in any
number of ways, including:
n Via MIDI control and parameter change messages
n Via external hardware controller
n Via DAW automation or other form of automation control
In closing, an almost unlimited degree of effects control is available to us
through most high-level (and many entry-level) digital audio workstations.
Through the use of any of the readily available hardware controllers or on-screen
automation controls, it’s possible to manipulate and automate effects within a
DAW session with an amazing degree of sophistication and ease.
Go ahead, get hold of these fun and effective tools and experiment your heart
out!
The vast majority of plug-in effects can be directly
and dynamically automated within the computer’s
DAW program. It’s my greatest hope that you’ll:
n Take a look at your favorite DAW manual and
start reading!
n Open up a tutorial session or better yet, make
your own.
n Call up some effects on various tracks.
n Call up some automation control parameters and
start grabbing controls.
n Learn how to edit these automation functions, so
as to be able to ﬁnesse your effects in new and
interesting ways.
Try This: DAW Effects Automation
D I Y
 do  it  yourself


With the advent of newer and better digital audio technologies, low-noise
systems, hi-res audio and surround-sound home theaters, an increase in dynamic
range and a demand for better quality sound has steadily been on the rise.
Because of this, it’s more important than ever that those in audio production
pay close attention to the background noise levels that are produced by pre-
amp and ampliﬁer self-noise, synths, analog magnetic tape and the like.
Although the overall dynamic range of human hearing roughly encompasses a
full 140 dB and well-designed digital systems are capable of much wider ranges
in everyday practice, such dynamics often won’t be fully captured, played-back
or appreciated for several reasons:
n An acoustic or electronic weak link in the chain might introduce noise
and/or restricting the program’s dynamic range.
n The medium or device itself might be incapable of capturing a wide
dynamic range.
n Background noises in the environment might mask the subtleties of the
sound.
Not all the blame for added noise can be placed on our older technology
friends. Even though a 16-bit digital recording has a theoretical dynamic range
of 96 dB and a 24-bit system can actually encode 144 dB, noises can (and often
will) crop up from such modern-day gremlins as mic preamps, effects and
outboard gear, analog communication lines and poorly designed digital audio
converters. Honestly, though, one of the biggest noise problems that you’ll often
encounter is the need for restoring tracks that were poorly recorded or were
made under adverse and/or noisy acoustic conditions.
DIGITAL NOISE REDUCTION
As you might expect, in modern music production, digital signal processing
(DSP) is most commonly used to reduce noise levels within a recorded sound
443
CHAPTER 16
Noise Reduction

Digital Noise Reduction
ﬁle. These noises might include artifacts such as tape hiss, hum, obtrusive
background ambience, needle ticks, pops and even certain types of distortion
that are present in the original recording. Although stand-alone digital noise
reduction processors deﬁnitely exist, most of these processors exist as plug-ins
that can be introduced at multiple points into the signal chain of a DAW. For
example, an “NR” plug-in can be inserted into a single track to reduce the amp
noise on a guitar, synth, or other type of track to instantly clean up a mix that
might otherwise be problematic. Likewise, an NR plug-in can be inserted into
a group or ﬁnal output mix stage to cleanup an overall mix that’s overly noisy.
Fast Fourier Transform
The most commonly used noise reduction programs make use of a math -
ematically intense algorithm known as Fast Fourier Transform (FFT). These appli -
cations and plug-ins (Figure 16.1) are able to analyze the amplitude/frequency
domain of an audio signal in order to reduce hum, tape hiss and other extran -
eous noises from your recordings. This digital analysis generally begins by taking
a digital “snapshot” of a short snippet of the offending noise (a brief section
that contains only the noise to be eliminated will yield the best results). This
noise template can then be digitally subtracted from the original sound ﬁle or
segment in varying amounts (and under the control of various program
parameters), such that only the footprint noise is reduced, while (under the
best of conditions) the original program material is left intact and unaffected.
444
FIGURE 16.1
Noise-reduction plug-ins. 
(a) Digidesign DINR plug-in.
(Courtesy of Digidesign, a
division of Avid Technology,
Inc., www.digidesign.com).
(b) Soundness SoundSoap 5
Audio restoration plug-in.
(Courtesy of Soundness
LLC, www.soundness-
llc.com)
In addition, certain FFT noise reduction systems are able to display frequencies
in an easy to detect graphic form that lets the user view the overall spectral
analysis of a recorded section (Figure 16.2). These various frequencies, noises,
pops, etc., can then be easily seen and “drawn out,” and then mathematically
removed from the audio signal. Such a useful tool can also be used to remove
coughs, squeaky chairs, sirens and any unfortunate noises that might make their
way into a recording.
Although FFT algorithms for reducing noise have greatly improved over the years,
it’s still important that we brieﬂy discuss a few of the unfortunate artifacts that

can occur when using (and over using) FFT-based noise reduction. The most
notable of these is “chirping.” This audible artifact most often occurs when too
much FFT processing is applied. It literally sounds like a ﬂock of small chirping
birds that can either be heard in the background or in an obnoxious way that
sounds like a bad Alfred Hitchcock movie. If you ﬁnd yourself running for cover,
it’s best to pull back on the FFT settings (and/or increase the processing quality
level) until the artifacts are less noticeable.
Should chirping and/or bandwidth limitations become a problem, you might
consider using equalization or a single-ended noise reduction device/plug-in
instead. Because single-ended noise reduction (to be discussed later) uses an
adaptive ﬁlter to intelligently change the program’s bandwidth, no chirping
artifacts will be introduced.
Finally, it’s a misconception that an FFT-based noise reduction application can
only be used for reducing noise. Literally, any sound can be used as a sonic
removal footprint, and as a result, vocal formants, snare hits or any sound that
you can imagine, can be pulled from a sound ﬁle to create unique and interesting
effects. The sky’s literally the limit!
445
Noise Reduction  CHAPTER 16
FIGURE 16.2
iZotope’s RX5 audio repair
toolkit is capable of
displaying the spectral
(frequency) content of a
passage, allowing the user
to “draw out” any unwanted
noises from a program.
(Courtesy of iZotope Inc.,
www.izotope.com)
1. Load a track containing an excessive amount of
noise into the DAW of your choice.
2. Call up an FFT-based noise-reduction plug-in,
select a short segment of noise and follow the
application’s user directions.
3. Apply varying amounts of noise reduction in real
time and listen to the results. Can you make it
chirp? Were you able to achieve acceptable
results?
Try This: FFT-Based Noise Reduction
D I Y
 do  it  yourself

Digital Noise Reduction
446
From the above DIY, it’s easy to tell that the noise-reduction process isn’t an
exact science, but more of a balancing act. Because of these and other band-
limiting artifacts, you might consider writing the processed signal to a new
track—while keeping your original sound ﬁle intact—thereby keeping your
options open for future changes, decisions and/or technological advances.
RESTORATION
In addition to removing noise, programs and DAW plug-ins also exist for
removing clicks and pops from vinyl and older recordings (Figure 16.3). Although
FFT analysis is often involved in the process, click removal differs slightly from
FFT noise reduction. This multistep process begins by detecting high-level clicks
(or those exceeding a user-deﬁned threshold) that exist within a sound ﬁle or
deﬁned segment. Once the offending noises are detected, the program performs
a frequency analysis (both before and after the click) and then goes about the
business of making a best plausible “guess” as to what the damaged amplitude/
frequency content should actually sound like. Finally, the calculated sound is
pasted over the nasty offender (ideally rendering it less noticeable or gone) and
then moves on to the next click and restarts the detection/replacement process.
FIGURE 16.3
Click/pop eliminator
applications: (a) within
Adobe’s Audition CC.
(Courtesy of Adobe
Systems, Inc., www.adobe.
com); (b) within iZotope
RX5. (Courtesy of iZotope
Inc., www.izotope.com)
Because click and pop noises can be different in nature from each other (both
in duration and frequency makeup), noise-reduction plug-ins might offer
applications that are speciﬁcally suited to reducing each type.
Single-Ended Noise-Reduction Process
The adaptive ﬁlter or single-ended noise-reduction process differs from the above
digital FFT systems, in that it extracts noise from an audio source by combining
a downward dynamic-range expander with a variable low-pass ﬁlter. These
devices (which can be analog or digital in nature) can be used to dynamically
analyze, process and EQ an existing program to reduce the unwanted noise
content with little or no audible effects (or giving us a best possible compromise,
in extreme cases).

Single-ended noise reduction systems work by breaking up the audio spectrum
into a number of frequency bands, such that whenever the signal level within
each band falls below a user-deﬁned threshold, the signal will be attenuated.
This downward expansion/ﬁltering process accomplishes noise reduction by
taking advantage of two basic psycho-acoustical principles:
n Music is capable of masking noise (the covering up a lower-level noise by
a louder signal) that exists within the same bandwidth.
n Reducing the bandwidth of an audio signal will reduce the perceived
noise.
It’s a psycho-acoustical fact that our ears are more sensitive to noises that contain
a greater number of frequencies than to those containing fewer frequencies.
Thus, whenever the program’s high-frequency content is reduced or restricted
to a certain bandwidth, the dynamic ﬁltering process will sense this and reduce
this bandwidth accordingly (thereby reducing its noise content). When the
program’s high-frequency signal returns, the ﬁlter will again pass the frequency
bandwidth up as far as necessary to pass the signal (allowing the increased
program content will mask the background noise). As you might have guessed,
such a dynamic ﬁlter can also be easily built by using a multi-band dynamics
processor (Figure 16.4), whereby the high-end (or offending) band can be set
to downwardly expand (reduce) the signal in that band when the signal falls
below the set threshold, and not affect the signal when they’re present in this
frequency range.
447
Noise Reduction  CHAPTER 16
FIGURE 16.4
A Multiband Expander/
Gate can effectively be
programmed to become a
single-ended noise
reduction unit. (a) Steinberg
Multiband Envelope Shaper.
(Courtesy of Steinberg
Media Technologies GmbH,
a division of Yamaha
Corporation, www.
steinberg.net) (b) Universal
Audio Precision Multiband
Compressor/Expander/Gate
plug-in. (Courtesy of
Universal Audio,
www.uaudio.com, © 2017
Universal Audio, Inc. All
rights reserved. Used with
permission)
THE NOISE GATE
A noise gate (Figure 16.5) can also be a very simple and effective tool for
eliminating noise from a track or tracks within a mix. This device allows a signal
above a selected threshold to pass through to the output at unity gain (no gain
change) and without dynamic processing. Once the input signal falls below this
threshold level, however, the gate acts as an inﬁnite expander and effectively
mutes background noises and other unwanted sounds by attenuating them.

448
When “gating” a speciﬁc track, it might be necessary to take time out to ﬁne-
tune the device’s attack and release controls. This is done to reduce or eliminate
any unwanted “pumping” or “breathing” as the noise or leakage signal falls and
rises around the threshold point. The general rules for these settings are the
same as those that apply to gain-change processors (see the compressor, limiter
and expander settings section in Chapter 15). Fortunately, these settings are
often audibly more obvious than with any other dynamic tool. Setting the
threshold, attack and release times at inappropriate levels will often be
immediately audible because the sound will cut in and out at inappropriate
times. For this reason, care should be taken when adjusting the settings by both
listening to the track on its own (solo track) and by listening to it within the
context of the full mix.
FIGURE 16.5
Noise Gate plug-ins. 
(a) Steinberg Noise Gate.
(Courtesy of Steinberg
Media Technologies GmbH,
a division of Yamaha
Corporation, www.
steinberg.net) (b) Universal
Audio API Vision Strip plug-
in with gate section
highlighted. (Courtesy of
Universal Audio,
www.uaudio.com, © 2017
Universal Audio, Inc. All
rights reserved. Used with
permission)
Digital Noise Reduction

In the past, almost all commercial music was mixed by an experienced
professional recording engineer under the supervision of a producer and/or artist.
Although this is still true at many levels of high-end production, with the
emergence of the project studio, the vast majority of facilities have become much
more personal and cost effective in nature. Additionally, with the maturation
of the digital revolution, artists, individuals, labels and enthusiasts are taking
the time to gain experience in the artistry, techniques and work-habits of creative
and commercial mixing in their own production workspaces.
Within music, audio-for-visual and audio production, it’s a well-known fact
that professional mixers have to earn their “ears” by logging countless hours
behind the console. Although there’s no substitute for this experience, the
mixing abilities and ears of producers and musicians outside of the pro studio
environment are also steadily improving as equipment quality gets better and
as practitioners become more knowledgeable about proper mixing environments
and techniques—quite often by mixing their own projects and compositions.
THE ART OF MIXING
Remember the old music joke: Q: How do you get to Carnegie Hall? A: Practice,
kid, practice! Well, the same goes for the process of learning how to mix. In
short, mixing is:
n First and foremost, the art of listening.
n The art of making decisions based upon what you hear, and then acting
upon these decisions.
n The process of blending art and a knowledge of audio technology and
signal ﬂow to turn these decisions into a technological reality, so as to
create an artistic vision.
n An art—as such it’s very subjective and individual. There is no right or
wrong way and no two people will mix in exactly the same way. The object
449
CHAPTER 17
The Art and Technology 
of Mixing

The Art of Mixing
of the process is to create a mix that “frames” (shows off) the music in
the best light possible.
Listening
It’s a simple fact that no one learns to play an instrument overnight—at least
not well. The same goes for the art of listening and making complex judgments
within the mixing process. Yet for some reason, we expect ourselves to sit down
at a console or workstation and instantly make a masterpiece. I personally
remember sitting down at my ﬁrst recording console (a Neve) and having a go
at my ﬁrst attempt at a mix. I was really nervous and completely unsure of what
to do next. I remember moving the faders and saying to myself—ok, now what?
Oh yeah, EQ, then I’d start blindly ﬁddling with the EQ. Well, I’m here to tell
you that this is totally normal! You can’t expect to be an expert on your ﬁrst
try, or your second, or your hundredth. Taking time to learn your “instrument”
takes time, patience and persistence—after all of this work, you’ll begin to gain
enough experience to follow your gut instincts towards creating a professional
mix.
EAR TRAINING
Of course, the art of listening requires that you take the time to listen. One of
the things that a mix engineer will be doing throughout his or her career is
listening to a mix, often over and over. This allows us to become familiar with
the nuances of the song and or project. Sometimes, the instant recall aspect of
a DAW gives us the ability to keep going back to a mix and improving it ad-
inﬁnitum. This is often especially true of artists who mix their own work (the
it’s never done until it’s “perfect” category). To this, I say; go easy on yourself.
Music is often a process of self-discovery and expression. If it were perfect on
your ﬁrst try, how can you expect to get better?
In the twenty-ﬁrst century, one of the things that we’ve gotten really good at is
having media readily at hand. Quite often, this media is omnipresent and
totally disposable. You walk into a store—there’s music! A car passes you by—
there’s loud music! You go practically anywhere—well, you know the deal all
too well. My point is that we’re all too used to being passive in the process.
Mixing requires that we become an “active” listener. As such, it’s often a good
idea to sit down with your favorite desert island songs, albums, LPs and play
them over your best speaker system. How does it sound when you actively listen
to them? Then, you might take the time to listen to them over your favorite
headphones. How does this change the experience for you? What can you learn
from the music and their mixes?
In the end, just as with learning an instrument or doing anything well, the fact
remains that as you mix, mix and mix again you will get better at your listening
and mixing skills. It’s a matter of experience matched with a desire to do your
best.
450

Preparation
Just as preparation is one of the best ways to ensure that a recording session
goes well the idea of preparing for a mix can help make the process go more
smoothly and be more enjoyable.
FIXING IT IN THE MIX
The term “ﬁx it in the mix” stems back to the 1980s, when multitrack recording
started to hit its full stride. It refers to the idea that if there’s a mistake or
something that’s not quite right in the recording “don’t worry about it; we don’t
have to make that decision right now; we’ll just ﬁx it later in the mix.” Although
to some degree, this might (or might not) be true, the fact is that this mentality
can come back to haunt you if care isn’t taken.
From a mix standpoint, preparing for this all-important stage deﬁnitely begins
in the recording phase. For example, if the above “ﬁx” isn’t dealt with
beforehand, it might, indeed be able to be dealt with later. The real problem,
however, happens when multiple “ﬁxes” that are meant to be dealt with at a
later time begin to creep into the project. If this happens, the mix can take on
a new life as something that needs to be wrestled to the ground, in order to
sound right instead of being pre-sculpted into a form that will simply need a
ﬁnal polishing in order to shine.
Although, each project has a life of its own, just a few of the ways that a mix
can be prepared is to:
n Strive to capture the artist and performance to disk or tape in a way that
best reﬂects everyone’s artistic intentions. Indeed, decisions such as EQ,
EFX, etc., can be made during the mix—but if the life, spirit and essence
of the musical expression isn’t captured, no amount of processing during
the mix will help.
n Whenever possible, deal with the musical or technical problem as it occurs,
during the recording or production phase.
n During a punch in or comp session (combining multiple takes into a single,
best take), take care to match the tracks as carefully as possible. This might
involve documenting the mic and instrument that was used, its placement
and distance in the room to the artist, as well as any other detail that will
ensure that the tracks properly blend.
n Create a rough mix during the recording phase that will help you get started
toward the ﬁnal mix. This is especially easy to do in this age of the DAW,
as the mix can begin to take its rough form in a way that can be saved
and recalled within the session ﬁle.
PREPARING FOR THE MIX
There are no rules for approaching a mix; however, there are deﬁnitely guidelines.
For example, when listening to a mix of a song, it is often best to listen to its
451
The Art and Technology of Mixing  CHAPTER 17

The Art of Mixing
452
overall blend, texture and “feel.” A common mistake amongst those who are
just beginning their journey into mixing would be to take each instrument
in isolation, EQ it and try to sculpt its sound while listening to
that track alone. When this is done, it’s quite possible to
make each instrument sound absolutely perfect on its
own, but when combined into the overall mix, the
blend might not work at all. This is because of the
subtle interactions that occur when all of the elements
are combined. Thus, it’s often a good idea to ﬁrst listen
to the tracks within the context of the full song and then,
you can go about making any mix changes that might best
serve it.
Preparing for a mix can come in many forms, each of which can save a
great deal of setup time, frustration and help with the overall outcome of the
project. Here are a few things that can be thought through beforehand:
n Is the project ready to be mixed? Has sufﬁcient time and emotional energy
been put into the vocals? Quite often, the vocals are laid on last—leaving
one of the most important elements to the last minute, when there might
be time and budget restraints. This leads us to recording rule number 2—
”Always budget enough time to do the vocals right, without stress.”
n Will you be mixing your own project or will someone else be mixing? If
it’s your own project and it’s in-house, then you’re probably technically
prepared. If the mix will take place elsewhere, then further thought might
be put into the overall process. For example, will the other studio happen
to have the outboard gear that you might or will need? Do they have the
plug-ins that you’re used to or need for the session? If not, then it’s your
job to make sure that you bring the installs and authorizations to get the
session up and running smoothly, or print the effects to another track.
n If you’ll be mixing for another artist and/or producer, it’s often helpful to
fully discuss the project with them. Is there a particular sonic style that
they’re after? Should the mix be aggressive or smooth sounding? Is there
a particular approach to effects that should be taken?
Providing a Reference Track
The band or artist might consider providing the mix engineer with a copy of
their own rough or demo mix. This might give insights into how the song might
best be mixed, effected or approached.
GAIN STRUCTURE
Actually, I lied when I said that there are no rules to the art of mixing. There
is one big one—watching your gain structure. This will be discussed more 
in-depth later in this chapter; however, it’s worth forewarning you about the
perils of setting your record and/or mix faders at levels that will cause distortion.
REMEMBER:
The mix always has to support
the song. It should bring an energy to
the performance that allows the
musical strengths and statements
to shine through.
Providing a Reference Track 

It might be obvious, but the reality of adding just a little bit more here, and a
little bit more there will all begin to add up in a mix. Before you know it, your
track channels, sub-groups and your main outs will start to redline. Simply being
aware of this natural tendency is your best defense again a distorted mix.
HUMAN FACTORS
Given the fact that mixing a song or project is an art, by its very nature it is a
subjective process. This means that our outlook and the very way that we
perceive a song will affect our workﬂow, as we approach the mix. Therefore,
it’s often a good idea to take care of ourselves and our bodies throughout the
process.
n Try to be prepared and rested as you start the mix process. Working yourself
too hard during the recording phase and then jumping right into the mix
just might not be the best approach at that point in time.
n By the same token, over-mixing a song by slaving behind the board for
hours and hours on end can deﬁnitely effect the way that you perceive a
mix. If you’ve gone all blurry-eyed and your ears are tired (the “I can’t
hear anything anymore” syndrome)—obviously, the mix could easily
suffer. This is deﬁnitely a time to point out that you might consider saving
your mixes under different version names (i.e., mymix_023), so that you
can go back to a previous version, should problems arise.
n You might want to take breaks—sometimes looooooongg ones. If you’re
not under any major time constraint, you might even consider coming
back to the mix a day or even a week later. This can give us a fresh
perspective, without ear fatigue or any short-term thoughts that might
cloud our perception. If this is an option, you might try it out and see if
it helps.
A dear friend within the Grammy organization once said to me: “Dave, the one
thing that I’ve found amongst all engineers, is the fact that they are seeking that
‘perfect sound’ . . . it’s something that they ‘hear’ in their heads, but are never
quite able to reach that Holy Grail.” From a personal standpoint, I can say that
this is true. I’m always sonically reaching for that killer sound, that’s often just
beyond reach.
This brings me to: “Wherever you may be, there you are!” By this, I mean; we
all have to start somewhere. If you’re just starting out, your level of mix
sophistication will hopefully be different than after you’ve had several years of
intensive mixing experience under your belt. Be patient with yourself and your
abilities, while always striving to better yourself—always a ﬁne line to walk.
From an equipment point-of-view, it’s obvious that you’re not going to start
out with your dream system. We all have to start the process by learning what
speakers, mics, DAW, etc., will best work for us at our current stage of knowledge,
budget and development. Later, your tastes in tools, studio layout and
production techniques will surely change—this is all part of growing. Your own
453
The Art and Technology of Mixing  CHAPTER 17

The Art of Mixing
personal growth will deﬁnitely affect your choice of tools and toys, as well as
the way that you integrate with them and your acoustic environment. It’s all
part of the personal journey that is called “your life and career.” Be patient with
yourself and enjoy the journey.
A REVIEW OF THE RECORDING PROCESS
Before we continue, let’s take a moment out to review the all-important recording
and overdub stages which must occur before the mixdown process can begin.
Recording
The recording phase involves the physical process of capturing live or sequenced
instruments onto a recorded medium (disk, tape or whatever). Logistically, this
process can be carried out in a number of ways:
n All the instruments to be used in a song or concert can be recorded in a
single live pass, either on the stage or in the studio.
n Musicians can be used to lay down the basic tracks (usually rhythm) of a
song, thereby forming the basic foundation tracks, whereby additional
instruments can be overdubbed and added to at a later time.
n Electronic or groove instruments can be performed or programmed into
the sound ﬁle and/or MIDI sequenced tracks of a DAW in such a way as
to build up the foundation of a song.
The ﬁrst process in this list could involve the recording of a group of musicians
as a single ensemble. This is often the case when recording a classical concert
in a large hall, where the ensemble and the acoustics of the room are treated
as a single entity, which is to be picked up by a microphone array. The recording
of a jazz ensemble would often be recorded in a single, live pass, however, in
this case the mics will often be placed closer to the instruments, so as to allow
for greater control in the mixdown or further production phase. Other styles,
such as a rock ensemble, might also record in a single, live pass; although this
would most often be recorded using close-mic techniques, so as to take advantage
of a maximum amount of isolation during mixdown and further production.
This idea of isolation can also be point of discussion. When recording a classical
ensemble, distant miking techniques rely upon the fact that there is little or no
isolation between the instruments and their acoustic environment. Recording
a classical ensemble for ﬁlm, for example, will often make use of mic placements
that are at a closer, more compromised distance allowing the mics to pick up
leakage (bleed) from other instruments, but to a lesser extent, so as to allow
for greater control during mixdown. One of the top engineers in the world will
often use omnidirectional mics during jazz sessions at a semi-distance to pick
up the instruments, nearby leakage and the room stating that leakage can often
be a good thing, adding to the “liveness” of the recording. The moral to these
examples is that mic distance and isolation are often situational, relying upon
454

455
The Art and Technology of Mixing  CHAPTER 17
the style and intended effect that the room and pickup technique might have
upon the music itself—it’s all part of the art of recording.
The last two of the above procedures are most commonly encountered within
the recording of modern, popular music. In the second approach, the resulting
foundation tracks (to which other tracks can be added at a later time) are called
basic, rhythm or bed tracks. These consist of instruments that provide the rhythmic
foundations of a song and often include drums, bass, rhythm guitar and
keyboards (or any combination thereof). An optional vocal guide (scratch track)
can also be recorded at this time to help the musicians and vocalists capture
the proper tempo and that all-important feel of a song.
When recording live popular music, each instrument is generally recorded onto
separate tracks of an ATR or DAW recorder (Figure 17.1). This is accomplished
by plugging each mic into an input strip on the console (or mic panel that’s
located in the studio), an input on an audio interface or an available input on a
smaller mixer. Once the input is turned on, the input strip/channel gain can be
set to its optimum level and is then assigned to a track on a DAW or tape machine.
FIGURE 17.1
When recording popular-
styled music, each
instrument is generally
recorded onto a separate
track (or stereo tracks) of 
a recording device.
When approaching the production process from an electronic music standpoint,
all bets are off, as there are literally so many possible ways that an artist can
build a song, using loops, MIDI instruments and external hardware/software
systems (Figure 17.2), that the process often becomes quite personal. This
popular form of production can best be understood through a thorough under -
standing of the DAW (Chapter 7), Groove Tools and techniques (Chapter 8) and
MIDI (Chapter 9).
As was said, the beauty behind the modern recording process is that the various
instruments can be recorded separately onto tracks of a DAW or ATR recorder,
thereby allowing as much isolation between the separately recorded tracks as
possible. This is extremely important, as the name of the game is to capture the

A Review of the Recording Process
456
best performance with the highest possible quality, while achieving optimum
isolation between these tracks, and at optimum signal levels (often without
regard to level balances on the other tracks).
This last part of the equation refers to the fact that the signal should be recorded
to the track at a level that is optimized for the medium. For example, a soft
vocal whisper might easily be boosted to recorded levels that are equal to those
of a neighboring electric guitar track, or a toy piano might be boosted to the
same track level as a full grand piano. In this conventional “one-instrument-
per-track” gain setting, each signal should be recorded at a reasonably high level
without overloading the digital or analog track. When recording to analog tape,
recording at an optimal level will result in a good signal-to-noise ratio, so that
tape hiss, preamp noise or other artifacts won’t impair the overall S/N (signal-
to-noise) ratio. Digital tracks, on the other hand, are more forgiving (due to
the increased headroom, especially at higher 24- and 32-bit-depths). In cases
such as these, we might still be in the habit of wanting to record the signal at
as high a level as possible (thereby inviting signal clipping or overload dis -
tortion). In fact, a properly designed digital signal chain is capable of recording
signals over an extremely wide dynamic range, so it’s often a good idea to record
signals to disk at lower recommended levels (i.e., quite often peaking at 12 dB
below the maximum overload point).
MONITORING
As you might expect, the main beneﬁt of recording individual instruments or
instru ment groupings onto isolated tracks at optimum recording levels lay in the
fact that project decisions over relative volumes, effects and placement changes
can be made at any time during the production and/or ﬁnal mixdown stage.
Since the instruments have been recorded at levels that probably won’t relate
to the program’s ﬁnal balance, a separate mix must be made in order for the
artists, producer and engineer to hear the instruments in their proper musical
perspective; for this, a separate mix is often set up for monitoring. As you’ll learn
later in this chapter, a multitrack performance can be monitored in several ways.
No particular method is right or wrong; rather, it’s best to choose a method
that matches your own personal production style or one that matches the
current needs of the session and its musicians. This monitor mix can be created
and made use of in the control room and in the studio (as a feed to the
musicians) in several ways, such as:
FIGURE 17.2
One possible example of 
a midi, hardware and
software setup.

457
The Art and Technology of Mixing  CHAPTER 17
n CR Monitor Mix: The engineer will create a rough mix version of what is
being recorded in the studio, so that he or she can hear the tracks in a
way that’s musically balanced. If the session is being monitored from
within a DAW, digital console or console with automated recall over mix
parameters, this ever-evolving control-room mix will often improve over
the course of the project, sometimes to the point that the mix improve -
ments can begin to represent the ﬁnal mix—often making the ﬁnal
mixdown process just that much easier.
n Studio Monitor Mix: Quite often, this control room mix will be sufﬁcient
for the musicians in the studio to hear themselves over headphones in a
properly balanced way.
n Special Studio Cue Mix: Of course, there are times that the musicians as
a whole or as individuals will need a special, separate mix in order to
properly hear themselves or another instrument from which they will need
to take their cues. For example, a musician’s headphone “cue mix” might
call for more drums, so the artist can better hear the song’s tempo, or it
might call for more piano, so that the vocalist can better stay in tune). It’s
totally situational and, at times, multiple cue mixes might be called for.
In short, the idea is to do whatever is needed to assist the musicians so
that they can best hear themselves in the studio. If they can’t hear
themselves in a setting that’s comfortable and best ﬁts their needs it follows
that it will be that much harder to deliver their best performance and that,
in the end, is the job of a good engineer and producer.
Overdubbing
Instruments that aren’t present during the original performance can be added
at a later time to the existing multitrack project during a process known as
overdubbing (Figure 17.3). At this stage, musicians listen to the previously
recorded tracks over headphones and then play along in sync while recording
new and separate tracks that are added to the basic tracks in order to ﬁll out
and ﬁnish the project.
FIGURE 17.3
Once the basic recorded
tracks have been “laid
down,” additional tracks
can be added at a later time
during the overdub phase.

A Review of the Recording Process
458
Since the overdub is an isolated take that’s being recorded to a new and separate
track, it can be laid down to disk or tape in any number of ways. For example,
with proper preparation and talent, an overdub take might get recorded in a
single pass. However, if the musician has made minor mistakes during an
otherwise good performance, sections of a take could be “punched in” on the
same track to correct a bad overdub section. (As a note: most DAWs can do
this “nondestructively” i.e., without losing any of the previously recorded data
and with the ability to make in/out adjustments at a later time.) For more inform -
ation on overdubbing to a DAW track, please refer to Chapters 1 and 7, while
info on overdubbing in sync to an analog recorder can be found in Chapter 5.
THE TECHNOLOGY OF MIXING
Now that the recording process is complete, we can now return to the technology,
techniques and process of mixing. Here, we’ll gain insights into how the console,
mixer, digital audio workstation and modern production equipment can work
together to improve both your personal working style and your sound.
The basic purpose of an audio production console or mixer (Figures 17.4 through
17.7) is to give us full control over volume, tone, blending and spatial
positioning for any or all signals that are applied to its inputs from microphones,
electronic instruments, effects devices, recording systems and other audio devices.
FIGURE 17.4
At the console. (a) DMH 
and Martin Skibba at nhow
hotel, Berlin. (Courtesy 
of nhow Berlin, www.nhow
hotels.com/berlin/en, Photo
courtesy of Sash) (b) DMH
and Emiliano Caballero
Fraccaroli. (Courtesy of
Galaxy Studios, Mol
Belgium, www.galaxy.be)
FIGURE 17.5
Solid State Logic Duality
Console. (Courtesy of Solid
State Logic, www.solid-
state-logic.com)

An audio production console (which also goes by the name of board, desk or
mixer) should also provide a straightforward way to quickly and reliably route
these signals to any appropriate device in the studio or control room so they
can be recorded, monitored and/or mixed into a ﬁnal product. A console or
mixer can be likened to an artist’s palette in that it provides a creative control
surface that allows an engineer to experiment and blend all the possible variables
onto a sonic canvas.
During the mixdown process, the audio project will be repeatedly played while
adjustments in level, panning, EQ, effects, etc., are made for each track and/or
track grouping. This allows us to get accustomed to the song, soundtrack or
audio program, so we can make the appropriate artistic decisions. Throughout
this process, the individually recorded signals are then blended into a composite
stereo, mono or surround set of output bus tracks that are fed to a master
mixdown recorder or more commonly internally “bounced” or “exported” from
directly within the DAW’s internal software mixer bus tracks to the ﬁnal sound
ﬁle track or tracks. These ﬁnal mixes can then be assembled or mastered (along
with other songs or scenes in the project) into a ﬁnal product.
When a recording is made “in-the-box” using a DAW, the mixdown process is
often streamlined, since the tracks, mixer and effects are all integrated into the
459
The Art and Technology of Mixing  CHAPTER 17
FIGURE 17.6
Smaller analog mixing
consoles. (a) API Model
1608. (Courtesy of
Automated Processes, 
Inc., www.apiaudio.com) 
(b) Rupert Neve Designs
5060 Centerpiece desktop
mixer. (Courtesy of Rupert
Neve Designs,
www.rupertneve.com)
FIGURE 17.7
Digidesign ICON D-
Command Integrated
console. (Courtesy of Avid
Technology, Inc.,
www.avid.com)

The Technology of Mixing
software system. In fact, much of the preparation has probably been long
underway, as basic mix moves were made and saved into the session during
the recording and overdub phases. When working “outside the box,” the process
can be quite different (depending upon the type of system that you’re working
with). Newer hardware console systems that include facilities to control and
communicate directly with the DAW recording system. This integration can be
ﬂuid and straightforward, with many or all of the console level, routing and
effects settings being recalled directly onto the console’s surface. Fully analog
consoles will require that you manually set up and recall all of the session settings
each and every time that you need to return to the session. Obviously, this
means that most non-automated mix sessions must be repeatedly perfected and
“performed” in real-time until a ﬁnal mix is achieved.
Of course, there are several ways that a mix can be recorded to its ﬁnal intended
mix medium.
n The DAW or console outputs can be recorded to a digital or analog
recorder.
n The outputs can be recorded onto a new set of tracks on the DAW.
n An in-the-box mix can be internally exported (bounced) to a ﬁnal stereo
or multichannel mix (either in real or non-real-time).
UNDERSTANDING THE UNDERLYING CONCEPT 
OF “THE MIXING SURFACE”
In order to understand the process of mixing, it’s important that we understand
one of the most important concepts in all of audio technology: the signal chain
(also known as the signal path). As is true with literally any audio system, a
recording mixer or console (DAW, digital or otherwise) can be broken down
into functional components that are chained together into a larger (and
hopefully manageable) number of signal paths. By identifying and examining
the individual components that work together to form this chain, it becomes
easier to understand the basic layout of any mixing system, no matter how large
or complex. In order to gain insights into the layout of a mixer, let’s start with
the concept that it’s built of numerous building-block components, each having
an input that moves to its output, and then to the input of the next functional
block to its output and so forth down the signal path until the end of the chain
is reached. Here are a few important things to keep in mind, regarding any
audio signal chain path:
n Naturally, whenever a link in this source-to-destination path is broken (or
incorrectly routed), no signal will pass. In the heat of production, it’s easy
to plug something into the wrong input or output. Following the “gozinta-
gozouta” approach might seem like a simple concept; however keeping it
always in mind can save your sanity and your butt when paths, devices
and cables that look like tangled piles of spaghetti get out of hand.
460

461
The Art and Technology of Mixing  CHAPTER 17
n Try to run each block in the signal chain at its optimum gain level. Too
little signal will result in increased noise, while too much will result in
increased distortion. We’ll talk more about this later.
n The “Good Rule” deﬁnitely applies to the audio signal path. That’s to say
that the overall signal path is no better than its weakest link. Just something
to keep in mind.
The Mixer/Console Input Strip
Let’s start our quest into the hardware side of mixing by taking a conceptual
look at various systems—from the analog hardware mixer to the general layout
within a virtual mixing environment. In a traditional hardware mixer (which
also goes by the name of board, desk or console) design, the signal ﬂow for
each input travels vertically down a plug-in strip known as an I/O module (Figure
17.8) in a manner that generally ﬂows:
FIGURE 17.8
General anatomies of input
strips on an analog mixing
console: (a) Mackie Onyx 
4-bus. (Courtesy of Loud
Technologies, Inc.,
www.mackie.com); 
(b) Audient sp8024 large
format recording console.
(Courtesy of Audient
Limited, www.audient.com)
Although the layout of a traditional analog hardware mixer generally won’t
match the graphical user interface (GUI) layout of a virtual DAW mixer, the
signal ﬂow will follow along the same or similar paths. Therefore, grasping the
concept of an analog console’s signal chain will also be extremely useful for
grasping the general signal ﬂow concept for smaller mixers and virtual DAW
mixers. Each system type is built from numerous building-block components,
having an input (source) that ﬂows through the signal chain to an output
(destination). The output of each source device must be literally or virtually
connected to the input of the device that follows it, and so on until the end of
the audio path is reached. Keeping this simple concept in mind is important
when paths, plug-ins and virtual paths seem to meld together into a ball of
confusion. When the going gets rough, slow down, take a deep breath, read the
manual (if you have the time and inclination)—and above all be patient and
keep your wits about you.

Understanding “The Mixing Surface”
462
Figure 17.9 shows the general I/O stages of three virtual mixing systems. It’s impor -
tant that you take the time to familiarize yourself with the inner workings of your
own DAW (or those that you might come in contact with) by reading the manual,
pushing buttons and by diving in and having fun with your own projects.
GAIN LEVEL OPTIMIZATION
As we enter our discussion on console and mixer layouts, it’s extremely important
that we touch base on the concept of signal ﬂow or gain level optimization. In fact,
the idea of optimizing levels as they pass from one device to another or from
one functional block in an input strip to the next is one of the more important
concepts to be grasped in order to create professional-quality recordings.
Although it’s possible to go into a great deal of math in this section, I feel that
it’s far more important that you understand the underlying principles of level
optimization, internalize them in everyday practice and let common sense be
your guide. For example, it’s easy to see that, if a mic that’s plugged into an input
strip is overdriven to the point of distortion, the signal following down the entire
path will be distorted. By the same notion, driving the mic preamp at too low
a signal will require that it be excessively boosted at a later point in the chain,
resulting in increased noise. From this, it follows that the best course of action
FIGURE 17.9
Virtual mixer strip layouts:
(a) Pro Tools. (Courtesy of
Avid Technology, Inc.,
www.avid.com); 
(b) Logic. (Courtesy of Apple
Inc., www.apple.com); 
(c) Steinberg’s Cubase and
Nuendo virtual mixer.
(Courtesy of Steinberg
Media Technologies GmbH,
a division of Yamaha
Corporation,
www.steinberg.net)

463
The Art and Technology of Mixing  CHAPTER 17
is to optimize the signal levels at each point along the chain (regardless of whether
the signal path is within an input strip or pertains to input/output (I/O) levels
as they pass from one device to another throughout the studio).
So, now that we have a fundamental idea of how a hardware and software mixing
system is laid out, let’s discuss the various stages in greater detail as they ﬂow
through the process, starting with a channel’s input, through the various
processing and send stages and then out to the ﬁnal mix bus destination.
1 CHANNEL INPUT (PREAMP)
The ﬁrst link in the input chain is the channel input (Figure 17.10). This serves
as a preamp section to optimize the signal gain levels at the input of an I/O
module before the signal is processed and routed. On a hardware console, mixer
or audio interface that has built-in mic preamps, either a mic or line input can
be selected to be the signal source (Figure 17.11). Although these values vary
between designs, mic trims are typically capable of boosting a signal over a range
of +20 to +70 dB, while a line trim can be varied in gain over a range of –15
(15-dB pad) to +45 dB or more. Gain trims are a necessary component in the
signal path, because the output level of a microphone is typically very low (–45
to –55 dB) and requires that a high-quality, low-noise amp be used to raise
and/or match the various mic levels in order for the signal to be passed
throughout the console or DAW at an optimum level (as determined by the
system’s design and standard operating levels).
FIGURE 17.10
Channel input section of the
Solid State Logic Duality
Console. (Courtesy of Solid
State Logic, www.solid-
state-logic.com)
FIGURE 17.11
Analog and DAW interface
input sections. (Courtesy 
of Loud Technologies, Inc.,
www.mackie.com and
Steinberg Media
Technologies GmbH,
www.steinberg.net)

Understanding “The Mixing Surface”
464
Of course, a mic preamp can take many forms. It might be integrated into a
console or mixers input strip (as referred to above), or it might exist as external
hardware “preze” (Figure 17.12) that are carefully chosen for its pristine or
special sound character or it might be directly integrated into our workstation’s
own audio interface. Any of these options are a valid way of boosting the mic’s
signal to a level that can be manipulated, monitored and/or recorded.
When asked about her recommendations for getting
those vintage, quirky analog-type sounds. Sylvia
Massy (Prince, Johnny Cash, System of a Down)
said, “Try some different things, especially with the
mic pres, the front end. The recorder is sorted out,
but it’s the front-end that’s the challenge.”
FIGURE 17.12
Microphone preamps: 
(a) Presonus ADL-600
high-voltage tube preamp.
(Courtesy of Presonus
Audio Electronics, Inc.,
www.presonus.com);
(b) Universal Audio 2–610S
dual channel tube preamp.
(Courtesy of Universal
Audio, www.uaudio.com
© 2017 Universal Audio,
Inc. All rights reserved.
Used with permission)
Whenever a mic or line signal is boosted to levels that cause the preamp’s output
to be overdriven, severe clipping distortion will almost certainly occur. To avoid
the dreaded LED overload light, the input gain must be reduced (by simply
turning down the gain trim or by inserting an attenuation pad into the circuit).
Conversely, signals that are too low in level will unnecessarily add noise into
the signal path. Finding the right levels is often a matter of knowing your
equipment, watching the meter/overload displays and using your experience.
Input attenuation pads that are used to reduce a signal by a speciﬁc amount
(e.g., –10 or –20 dB) may be inserted ahead of the preamp, in order to prevent
input overload. On many consoles, the preamp outputs may be phase-reversed,
via the “Ø” button. This is used to change the signal’s phase by 180º in order
to compensate for polarity problems in mic placement or in cable wiring. High-
and low-pass ﬁlters may also follow the pre-amp, allowing extraneous signals
such as amp/tape hiss or subsonic ﬂoor rumble to be ﬁltered out.
From a practical standpoint, level adjustments usually begin at the mic preamp.
If the pre has LED or other types of metering, setting your gain at a reasonable
level (while being careful not to overload this important ﬁrst major gain stage

465
The Art and Technology of Mixing  CHAPTER 17
in the process) will generally get you off to a good start. Alternatively, you could
set the gain on the main strip fader to 0 dB (unity gain). While monitoring
levels for that channel or channel grouping, turn the mic preamp up until an
acceptable gain is reached. Should the input overload LED light up, back off
on the input level and adjust the output gain structure accordingly. Care should
be taken when inserting devices into the signal chain at a direct insert point,
making sure that the in and out signals are also working at or near their optimum
level. In addition, it’s important to keep in mind that the EQ section can also
cause level overload problems whenever a signal is overly boosted within a
frequency range.
Hardware Console/Mixer Insert Point
Many mixer and certain audio interface designs provide a break in the signal
chain that occurs after the channel input. A direct send/return or insert point
(often referred to simply as direct or insert) can be used to send the strip’s line
level audio signal out to an external gain or effects processing device. The
external device’s output signal can then be inserted back into the signal path,
where it can be mixed back into the audio program. Access to an insert point
on a hardware console or mixer can be found at either a marked set of jacks
on the studio’s patch bay or at the rear of some console/mixers themselves
(Figure 17.13). It’s important to note that plugging a signal processor into an
insert point will only affect the audio on that channel.
FIGURE 17.13
Direct send/return signal
paths. (a) Two jacks can be
used to send signals to and
return signals from an
external device. (b) A single
TRS (stereo) jack can be
used to insert an external
device into an input strip’s
path.
Virtual DAW Insert Point
Within a workstation environment, inserts are extremely important in that they
allow audio or MIDI processing/effects plug-ins to be directly inserted into the
virtual path of that channel (Figure 17.14). Often, a workstation allows multiple
plug-ins to be inserted into a channel in a stacked fashion, allowing complex
and unique effects to be built up. Of course, keep in mind that the exten-
sive use of insert plug-ins can eat up processing power. Should the stacking of
FIGURE 17.14
An effects plug-in can be
easily inserted into the
virtual path of a DAW’s
channel strip. (Courtesy of
Avid Technology, Inc.,
www.avid.com, Steinberg
Media Technologies GmbH,
www.steinberg.net, and
Universal Audio,
www.uaudio.com)
Hardware Console/Mixer Insert Point 
Virtual DAW Insert Point 

Understanding “The Mixing Surface”
466
mul tiple plug-ins become a drain on your CPU (something that can be
monitored by watching your processor usage meter—aka “busy bar”), many
DAWs allow the track to be frozen (committed), meaning that the total sum
of the effects can be written to an audio ﬁle, allowing the track + effects to be
played back without causing any undue strain on the CPU.
2 AUXILIARY SEND SECTION
In a hardware or virtual setting, the auxiliary (aux) sends are used to route and
mix signals from multiple input strip channels to a single effect device/plug-in
and/or monitor/headphone cue send from within the console or DAW. This
section is used to create a mono or stereo sub-mix that’s derived from multiple
console or DAW input signals, and then “send” it to a signal processing,
monitoring or recording destination (Figure 17.15).
FIGURE 17.15
Although a hardware
mixer’s input path generally
flows vertically from top to
bottom, an aux send’s path
flows in a horizontal
fashion, in that the various
channel signals are mixed
together to feed a mono or
stereo send bus. The
combined mix can then be
sent to any device.
(Courtesy of Loud
Technologies, Inc.,
www.mackie.com)
It’s not uncommon for six or more individual aux sends to be found on a
hardware or virtual input strip. An auxiliary send can serve many purposes. For
example, one send could be used to drive a reverb unit, signal processor, etc.,
while another could be used to drive a speaker that’s placed in that great
sounding bathroom down the hall. A pair of sends (or a stereo send) could
also be used to provide a headphone mix for several musicians in the studio,
while another send could feed a separate mix to the drummer that’s having a
hard time hearing the lead guitar. From these and countless other situations,
you can see how a send can be used for virtually any signal routing, effects
processing and/or monitoring task that needs to be handled. How you make
use of a send is up to you, your needs and your creativity.
With regard to a workstation, using an aux send is a great way to use processing
effects, while keeping the processing load on the CPU to a minimum (Figure
17.16). For example, let’s say that we wanted to make wide use of a reverb plug-
in that’s generally known to be a CPU hog. Instead of separately plugging this
reverb into a number of tracks as inserts, we can greatly save on processing
power by plugging the reverb into an aux send bus. This lets us selectively route
and mix audio signals from any number of tracks and then send the summed

467
The Art and Technology of Mixing  CHAPTER 17
FIGURE 17.16
An effects plug-in can be
inserted into an effects send
bus, allowing multiple
channels to share the same
effects processor. (Courtesy
of Avid Technology, Inc.,
www.avid.com, Steinberg
Media Technologies GmbH,
www.steinberg.net and
Universal Audio,
www.uaudio.com)
(mixed) signals to a single plug-in that can then be mixed back into the master
output bus. In effect, we’ve cut down on our power requirements by routing a
creative effects mix to a single device, instead of plugging the device into each
channel. Knowing the functional difference between an insert and a send can
be a powerful engineering tool.
3 EQUALIZATION
The most common form of signal processing is equalization (EQ). The audio
equalizer (Figure 17.17) is a device or processing circuit that lets us control the
relative amplitude of various frequencies within the audible bandwidth. Like
the auxiliary sends, it derives its feed on a hardware console directly from the
channel input section. In short, it exercises tonal control over the harmonic or
timbral content of an input signal. EQ may need to be applied to a single
recorded channel, to a group of channels or to an entire program (often as a
step in the mastering process) for any number of other reasons, including:
n To correct for speciﬁc problems in an instrument or in the recorded sound
(possibly to restore a sound to its natural tone)
n To overcome deﬁciencies in the frequency response of a mic or in the
sound of an instrument
n To allow contrasting sounds from several instruments or recorded tracks
to better blend together in a mix
n To alter a sound purely for musical or creative reasons.
FIGURE 17.17
Equalizer examples.
(Courtesy of Loud
Technologies, Inc.,
www.mackie.com,
Steinberg Media
Technologies GmbH,
www.steinberg.net and
Universal Audio,
www.uaudio.com)

Understanding “The Mixing Surface”
468
When you get right down to it, EQ is all about compensating for deﬁciencies
in a sound pickup, or “shaping” the sound of an instrument so that it doesn’t
interfere with other instruments in a mix or reducing extraneous sounds that
make their way into a track. To start our discussion on how to apply EQ, let’s
take another look at the “Good Rule.”
The “Good Rule”
Good musician + good instrument + good performance + good acoustics + good mic 
+ good placement = good sound.
Let’s say that at some point in the “good” chain something falls short—like, a
mic was placed in a bad spot for a particular instrument during a session that’s
still in progress. Using this example, we now have two options. We can change
the mic position and overdub the track or re-record the entire song—or, we can
decide to compensate by applying EQ. These choices represent an important
philosophy that’s held by many producers and engineers (including myself):
Whenever possible, EQ should NOT be used as a bandage to doctor a track or
session after it’s been completed. By this, I mean that it’s often a good idea to
correct a problem on the spot during the recording (i.e., change the mic or mic
position) rather than rely on the hope that you can ﬁx it later in the mix using
EQ and other corrective methods.
Although it’s usually better to deal with problems as they occur, this simply
isn’t always possible. When a track needs ﬁxing after it’s already been recorded,
EQ can be a good option when:
n There’s no time, money or possibility to redo the track.
n The existing take was simply magical and too much feeling would be lost
if the track were to be redone.
n You have no control over a track that’s already been recorded during a
previous session and the artists are touring on the other side of the planet.
Whenever EQ is applied to a track, bus or signal, the whole idea is to take out
the bad and leave the good. If the signal is excessively EQed, the signal will
often degrade and lead to volumes that often creep up in level. Thus, it’s often
a good idea to use EQ to take away a deﬁciency in the signal and not simply
boost the desirable part of the track. Just a few examples of using EQ to cut
offensive sounds might include:
n Reducing the high end on a bass guitar instead of boosting its primary
bass notes
n Using a peak ﬁlter to pull out the ring of a snare drum (a perfect example
of a problem that should’ve been corrected during the session, by
dampening the drumhead)

n Pulling out a small portion of a vocalist’s upper-midrange to reduce any
problematic nasal sounds
Using EQ might or might not always be the best course of action. Just like life,
use your best judgment—nothing’s ever absolute. A complete
explanation of equalization can be found in Chapter 15 (Signal
Processing).
4 DYNAMICS SECTION
Many top-of-the-line analog consoles offer a dynamics
section on each of their I/O modules (Figure 17.18), and
of course dynamic plug-ins are readily available for all
DAWs. This allows individual signals to be dynamically
processed more easily, without the need to scrounge up 
tons of outboard devices. Often, a full complement of compres-
sion, limiting and expansion (including gating) is also provided. A 
complete explanation of dynamic control can be found in Chapter 15 (Signal
Processing).
469
The Art and Technology of Mixing  CHAPTER 17
It’ s always a good idea
to be patient with your “EQ
style”——especially when you’re 
just starting out. Learning how to 
EQ properly (i.e., not over 
EQ) takes time and 
practice.
FIGURE 17.18
Dynamics section of a Solid
State Logic Duality Console.
(Courtesy of Solid State
Logic, www.solid-state-
logic.com)
5 MONITOR SECTION
During the recording phase, since the audio signals are commonly recorded to
DAW or tape at their optimum levels (without regard to the relative musical
balance on other tracks), a means for creating a separate monitor mix in the
control room is necessary in order to hear a musically balanced version of the
production. Therefore, a separate monitor section (or aux bus) can be used to
provide varying degrees of control over each input’s level and possibly panning,
effects, etc. This mix can be routed to the master CR volume control, as well as
monitor switching between various speaker sets and between mono, stereo or
surround modes in the control room (Figure 17.19). The approach and
techniques for monitoring tracks during a recording will often vary from mixer
to mixer (as well as among individuals). Again, no method is right or wrong.

Understanding “The Mixing Surface”
470
FIGURE 17.19
Monitor mix sections: 
(a) Legacy model SSL
XL9000K monitor mix
section. (Courtesy of Solid
State Logic, www.solid-
statelogic.com);
(b) Software monitor
section (at top) within the
Cubase/Nuendo virtual
mixer. (Courtesy of
Steinberg Media
Technologies GmbH,
www.steinberg.net)
It simply depends on what type of equipment you’re working with, and on your
own personal working style.
Note that during the overdub and general production phases on a large-scale
console or DAW, this idea of using a separate monitor section can be easily
passed over in favor of mixing the tracks directly using the main faders in a
standard mixdown environment. This straightforward process helps us by setting
up a rough mix all through the production phase, allowing us to ﬁnesse the
mix during production under automated recall. By the time the ﬁnal mix rolls
around, many of your “mix as you go” level and automation kinks might easily
have been worked out.
In-Line Monitoring
Many larger console designs incorporate an I/O small fader section that can be
used to directly feed its source signal to the monitor mix or directly to the
DAW/ATR (depending on its selected operating mode). In the standard monitor
mix mode (Figure 17.20a), the small fader is used to adjust the monitor level
for the associated recording track. In the ﬂipped mode (Figure 17.20b), the small
FIGURE 17.20
Flipped-fader monitor
modes: (a) standard monitor
mode; (b) flipped monitor
mode.
In-Line Monitoring 

fader is used to control the signal level that’s being sent to the recording device,
while the larger, main fader is used to control the monitor mix levels. This
useful function allows multitrack record levels (which aren’t often changed
during a session) to be located out of the way, while the more frequently used
monitor levels are assigned to the larger, more accessible master faders.
Direct Insert Monitoring
Another ﬂavor of in-line monitoring that has gained favor over the years is
known as direct insert monitoring. This method (which often makes the most
sense on large-format consoles) makes use of the direct send/returns of each
input strip to insert the recording device (such as a DAW) directly into the input
strip’s signal path. Using this approach (which is closely tied to the Insert Point
section from earlier in the chapter),
n The insert send output for each associated channel (which can be inserted
either before or after the EQ section) is routed to its associated track on
a multitrack DAW or ATR.
n The insert return signal is then routed back from the recording device’s
output back into the console’s input strip return path, where it’s injected
back into the channel strip’s effects send, pan and main fader path.
With this approach, the input signal directly following the mic/line preamp will
be fed to the DAW or ATR input (with record levels being adjusted by the pre-
amp’s gain trim). The return path (from the DAW or ATR) is then fed back into
the input strip’s signal path so it can be mixed (along with volume, pan, effects,
sends, etc.) without regard for the levels that are being recorded to tape, disk
or other medium. This system greatly simpliﬁes the process, since playing back
the track won’t affect the overall monitor mix at all—because the recorder’s
outputs are already being used to drive the console’s monitor mix signals.
Separate Monitor Section
Certain British consoles (particularly those of older design) incorporate a
separate mixing section that’s dedicated speciﬁcally to the task of sending a mix
to the monitor feed. Generally located on the console’s right-hand side (Figure
17.21), the inputs to this section are driven by the console’s multitrack output
and tape return buses, and offer level, pan, effects and “foldback” (an older
British word for headphone monitor control). During mixdown, this type of
design has the distinct advantage of offering a large number of extra inputs that
can be assigned to the main output buses for use with effects returns, electronic
instrument inputs and so on. During a complex recording session, this moni -
toring approach will often require an extra amount of effort and concentration
to avoid confusing the inputs that are being sent to tape or DAW with the
corresponding return strips that are being used for monitoring. This is especially
true when the channel and track numbers do not agree (which is probably why
this design style has fallen out of favor in modern console designs).
471
The Art and Technology of Mixing  CHAPTER 17
Direct Insert Monitoring 
Separate Monitor Section 

Understanding “The Mixing Surface”
472
FIGURE 17.21
Older style English consoles
may have a separate
monitor section (circled),
which is driven by the
console’s multitrack output
and/or tape return buses.
(Courtesy of Buttermilk
Records, www.buttermilk
records.com)
6 CHANNEL FADER
Each input strip contains an associated channel fader, which determines the
strip’s bus output level (Figure 17.22) and pan pot which is often designed into
or near the fader and determines the signal’s left/right placement in the stereo
and/or surround ﬁeld (Figure 17.23). Generally, this section also includes a
solo/mute feature, which performs the following functions:
n Solo: When pressed, the monitor outputs for all other channels will be
muted, allowing the listener to monitor only the selected channel (or
soloed channels) without affecting the multitrack or main stereo outputs
during the recording or mixdown process.
n Mute: This function is basically the opposite of the solo button, as when
it is pressed the selected channel is cut or muted from the main and/or
monitor outputs.
Depending on the hardware mixer or controller interface design, the channel
fader might be motorized, allowing automation moves to be recorded and played
back in the physical motion of moving faders. In the case of some of the high-
end console and audio interface/controller designs, a ﬂip fader mode can be
FIGURE 17.22
Output fader section of the
Solid State Logic Duality
Console. (Courtesy of Solid
State Logic, www.solid-
state-logic.com)

called up that literally reassigns the control of the monitor section’s fader to
that of the main channel fader (as was seen in Figure 17.20). This “ﬂip” allows
the monitoring of levels during the recording process to be controlled from the
larger, long-throw faders. In addition to swapping monitor/channel fader
functions, certain audio interface/controller designs allow a number of functions
such as panning, EQ, effects sends, etc., to be swapped with the main fader,
allowing these controls to be ﬁnely tuned under motorized control.
7 OUTPUT SECTION
In addition to the concept of the signal path as it follows through the chain,
there’s another important signal concept that should be understood: output
bus. From the above input strip discussion, we’ve seen that a channel’s audio
signal by and large follows a downward path from its top to the bottom;
however, when we take the time to follow this path, it’s easy to spot where
audio is sometimes routed off the strip and onto a horizontal output path.
Conceptually (and sometimes literally), we can think of this path (or bus) as
a single electrical conduit that runs the horizontal length of a console or mixer
(Figure 17.24). Signals can be inserted onto or routed off of this bus at multiple
points.
Much like a city transit bus, this signal path follows a speciﬁc route and allows
audio signals to get on or off the line at any point along its path. Aux sends,
473
The Art and Technology of Mixing  CHAPTER 17
FIGURE 17.23
Example of various
hardware and software 
pan pot configurations.
FIGURE 17.24
Example of a master output
bus, whereby multiple
inputs are mixed and routed
to a master output fader.

Understanding “The Mixing Surface”
474
monitor sends, channel assignments, and main outputs are all examples of
signals that are taken from their associated input strips and are injected into
buses for routing to one or more output destinations. The main stereo or
surround buses, (which are used to feed the channel faders and pan positioners)
are then fed to the mixer’s main output bus, where they are combined with the
various effects return signals and ﬁnally routed to the recording device and/or
monitor speakers.
8 CHANNEL ASSIGNMENT
After the channel output fader on a console or larger mixer, the signal is often
routed to the strip’s track assignment matrix (Figure 17.25), which is used to
distribute the signal to any or all tracks of a connected multitrack DAW or ATR
recorder. Although this section electrically follows either the main or small fader
section (depending on the channel’s monitor mode), the track assign buttons
will often be located either at the top of the input strip or designed into the
main output fader (often being placed at the fader’s right-hand side).
Functionally, pressing any or all assignment buttons will route the input strip’s
main signal to the corresponding track output buses. For example, if a vocal
mic is plugged into channel 14, the engineer might assign the signal to track
14 by pressing (you guessed it) the “14” button on the matrix. If a quick
overdub on track 15 is also needed, all the engineer has to do is unpress the
“14” button and reassign the signal to track 15.
FIGURE 17.25
Channel assignment
sections: (a) Onyx 4-bus.
(Courtesy of Loud
Technologies, Inc.,
www.mackie.com); (b) API
1608 Console. (Courtesy of
Automated Processes, Inc.,
www.apiaudio.com); 
(c) ProTools. (Courtesy of
Digidesign, a division of
Avid Technology, Inc.,
www.digidesign.com)
Many newer consoles offer only a single button for even- and odd-paired tracks,
which can then be individually assigned by using the strip’s main output pan
pot. For example, pressing the button marked “5/6” and panning to the left
routes the signal only to output bus 5, while panning to the right routes it to
bus 6. This simple approach accomplishes two things:
n Fewer buttons need to be designed into the input strip (lowering
production costs and reducing the number of moving parts).
n Panning instruments within a stereo sound ﬁeld and then assigning their
outputs to a pair of tracks on the multitrack recorder can easily be used
to build up a stereo sub-mix.

Of course, the system for assigning a track or channel on a DAW varies with
each design, but it’s often straightforward. As always, it’s best to check with the
manual, and become familiar with the software before beginning a session.
9 GROUPING
Many DAWs, consoles and professional mixing systems allow any number of
input channels to be organized into groups. Such groupings allow the overall
relative levels of a series of channels to be interlinked into organized groups
according to instrument or scene change type. This important feature makes it
possible for multiple instruments to retain their relative level balance while
offering control over their overall group level from a single fader or stereo fader
pair. Individual group bus faders often have two functions. They can:
n Vary the overall level of a grouped signal that’s being sent to a recorded
track.
n Vary the overall sub-mix level of a grouped signal that’s being routed to
the master mix bus during mixdown.
The obvious advantage to grouping channels is that it makes it possible to avoid
the dreaded and unpredictable need to manually change each channel volume
individually. Why try to move 20 faders when you can adjust their overall levels
from just one? For example, the numerous tracks of a string ensemble and a
drum mix could each be varied in relative level by assigning them to their own
stereo or surround sound groupings and then moving a single fader—ahhhh,
much easier and far more accurate!
It’s important to keep in mind that there are two methods for grouping signals
together, when using either a hardware console or a DAW. These can be done
by:
n Signals can be grouped together onto the same, combined audio bus.
n An external control signal (such as a DC voltage or digital control) can
be used to control the relative level of various grouped faders, etc., while
keeping the signals separate and discrete.
Using the ﬁrst method, the grouped signals are physically assigned to the same
audio bus, where they are combined into a composite mono, stereo or multi -
channel track. Of course, this means that all of your mix decisions have to be
made at that time, as all of the signals will be electrically combined together,
with little or no recourse for hitting an “‘undo’ button” at a later time. This
physical grouping together of channel signals can be easily done by assigning
the involved channels in the desired group to their own output bus (Figure
17.26). During mixdown, each instrument group bus will be routed to the main
stereo or surround output through the use of pan pots or L/R assignment
buttons.
The second grouping system doesn’t combine the signals, but makes use of a
single voltage or digital control signal to control the “relative” levels of tracks
475
The Art and Technology of Mixing  CHAPTER 17

Understanding “The Mixing Surface”
476
FIGURE 17.26
Simplified anatomy of the
output grouping section on
the Mackie Onyx 4-bus
analog console, whereby the
signals can be combined
into a physical group output
bus. (Courtesy of Loud
Technologies, Inc.,
www.mackie.com)
that are assigned to a group bus. This method (Figure 17.27), allows the 
signals to be discretely isolated and separated from each other, but their relative
levels can be changed in a way that keeps the relative mix balance intact. Quite
simply, it’s a way that multiple faders can be either controlled from a single
“group fader” or can be ganged together to move in relative tandem with each
other.
As you might have noticed, the above section focused largely upon the idea of a hardware console or mixer.
Newer DAWs, on the other hand, now have several options for grouping a signal. Each type has its subtle
strengths and should be tried out to see which one works best for you, or in what situation it would work best in.
1. Grouping track: By using a separate group track,
the digitally summed grouping signal will be
routed through a separate group fader.
Controlling this group fader will control the
relative volumes of the tracks that are assigned
to that group.
2. VCA Grouping: Using this system, the grouped
tracks will not be assigned to an assigned
output, but will be virtually locked in tandem by
a control signal. Moving any fader in that
grouping will move the relative volumes of all the
faders that are assigned to that group.
3. Using color coding: Here, all of the faders in a
grouping can be given a speciﬁc track color.
Of course, this is not a grouping method at all,
however, I mention this as an additional way to
easily keep track of a grouped set of tracks.
FIGURE 17.27
Tracks can be assigned as a
group, so that the relative
track levels can change,
while keeping the track
signals separate in a mix.

477
The Art and Technology of Mixing  CHAPTER 17
FIGURE 17.28
Monitor level section of the
Solid State Logic Duality
Console. (Courtesy of Solid
State Logic, www.solid-
state-logic.com)
10 MAIN OUTPUT MIX BUS
The main output bus of an analog console acts to physically or electronically
sum (add) the signals from all of the channel fader, pan, effects return and any
other signal routing output together into a ﬁnal mono, stereo or surround audio
program output. The output from this audio source can then be processed in
a ﬁnal stage (using compression, limiting or EQ) and then is sent to a master
recording device.
In the digital domain, the main output bus likewise serves to digitally combine
the sample levels from all of the channel fader, pan, effects return and any other
signal routing outputs together into a ﬁnal mono, stereo or surround digital
audio stream that represents the ﬁnal audio program data stream.
At this stage, the primary concern that needs to be kept in mind is the careful
handling of the program’s overall dynamic range. As you might expect, a main
output signal that is too low in level could result in increased noise, while
excessive output levels will almost certainly result in a distortion. This logic,
however, begins to blur when we consider that most of the output signal levels
are digital in nature and make use of higher bit-depths. For example, a 24-bit
signal could have a dynamic range of up to 144 dB, meaning that the main
output could be at lower levels without adding signiﬁcant noise to the mix.
This which would have an added advantage of assuring that the levels will be
low enough that digital clipping distortion will not occur—something to be
kept on mind, should you know that a mastering engineer will be using their
skills to ﬁnesse the mix into a ﬁnal form.
11 MONITOR LEVEL SECTION
Most console, mixing and DAW systems include a central monitor section that
controls levels for the various monitoring functions (such as control room level,
studio level, headphone levels and talkback). This section (Figure 17.28) often
makes it possible to easily switch between multiple speaker sets and can also
provide switching between the various inputs, recording device sources and
various output formats (e.g., surround, stereo and mono output buses; tape
returns; aux send monitoring; solo monitoring).

Understanding “The Mixing Surface”
478
12 PATCH BAY
A patch bay (Figure 17.29) is a panel that’s found in the control room and on
larger consoles which contains accessible jacks that correspond to the various
inputs and outputs of every access point within a mixer or recording console.
Most professional patch bays (also known as patch panels) offer centralized
I/O access to most of the recording, effects and monitoring devices or system
blocks within the production facility (as well as access points that can be used
to connect between different production rooms).
FIGURE 17.29
The patch bay: 
(a) Ultrapatch PX2000 patch
bay. (Courtesy of Behringer
International GmbH,
www.behringer.de);
(b) rough example of a
labeled patch bay layout.
Patch bay systems come in a number of plug and jack types as well as wiring
layouts. For example, prefabricated patch bays are available using tip-ring-
sleeve (balanced) or tip-sleeve (unbalanced) 1/4-inch phone conﬁgurations.
These models will often place interconnected jacks at the panel’s front and rear
so that studio users can reconﬁgure the panel simply by rearranging the plugs
at the rear access points. Other professional systems using the professional
telephone-type (TT or mini Bantam-TT) plugs might require that you hand-wire
the connections in order to conﬁgure or reconﬁgure a bay (usually an amazing
feat of patience, concentration and stamina).
Patch jacks can be conﬁgured in a number of ways to allow for several signal
connection options among inputs, outputs and external devices (Figure 17.30):
n Open: When no plugs are inserted, each I/O connection entering or leaving
the panel is independent of the other and has no electrical connection.
n Half-normaled: When no plugs are inserted, each I/O connection entering
the panel is electrically connected (with the input being routed to the
output). When a jack is inserted into the top jack, the in/out connection
is still left intact, allowing you to tap into the signal path. When a jack is
inserted into the bottom jack, the in/out connection is broken, allowing
only the inserted signal to pass to the input.
n Normaled: When no plugs are inserted, each I/O connection entering the
panel is electrically connected (with the input routing to the output). When
a jack is inserted into the top jack, the in/out connection is broken,
allowing the output signal to pass to the cable. When a jack is inserted
into the bottom jack, the in/out connection is broken, allowing the input
signal to pass through the inserted cable connection.

479
The Art and Technology of Mixing  CHAPTER 17
FIGURE 17.30
Typical patch bay signal
routing schemes. (Courtesy
of Behringer International
GmbH, www.behringer.de)
FIGURE 17.31
A set of LED, light-bar and
VU meter displays.
n Parallel: In this mode, each I/O connection entering the panel is electrically
connected (with the input routing to the output). When a jack is inserted
into either the top or bottom jack, the in/out connection will still be intact,
allowing you to tap into both the signal path’s inputs and outputs.
Breaking a normaled connection allows an engineer to patch different or
additional pieces of equipment into a circuit path. For example, a compressor
might be temporarily patched between a mic preamp output and an equalizer
input. The same preamp/EQ patch point could be used to insert an effect or
other device type. These useful access points can also be used to bypass a
defective component or to change a signal path order. Versatility is deﬁnitely
the name of the game here!
13 METERING
The level of a signal’s strength, at an input, output bus and other console level
point, is often measured by visual meter display (Figure 17.31). Meter and indi -
cator types will often vary from system to system. For example, banks of readouts
that indicate console bus output and tape return levels might use VU metering,
peak program meters (PPMs, found in European designs) or digital/software
readouts. It’s also not uncommon to ﬁnd LED overload indicators on an input
strip’s preamp, which give quick and easy peak indications as to whether you’ve
approached or reached the component’s headroom limits (a sure sign to back
off your levels).
The basic rule regarding levels isn’t nearly as rigid as you might think and will
often vary depending on whether the device or recording medium is analog or
digital. In short, if the signal level is too low, tape, amp and even digital noise

Understanding “The Mixing Surface”
480
could be a problem, because the levels throughout the signal chain might not
be optimized. If the level is too high, overloaded preamps, saturated tape or
clipped digital converters will often result in a distorted signal. Here are a few
rules of thumb:
n In analog recording, the proper recording level is achieved when the
highest reading on the meter is near the zero level, although levels slightly
above or below this might not be a problem, as shown in Figure 17.32.
In fact, overdriving some analog devices and tape machines (slightly) will
often result in a sound that’s “rough” and “gutsy.”
n When recording digitally, noise is often less of a practical concern
(especially when higher bit depths are used). It’s often a good idea to keep
levels at a reasonable level (i.e., peaking at about –12 on the meter), while
keeping a respectful distance from the dreaded clip or “over” indicator.
Unlike analog, digital is generally unforgiving of signals that are clipped
and will generate a grunge sound that’s guaranteed to make you cringe!
Because there is no real standard for digital metering levels beyond these
guidelines, many feel that giving a headroom margin that’s 12 dB below
“0” full scale is usually a wise precaution—especially when recording at
a 24-bit or higher depth.
FIGURE 17.32
VU meter readings for
analog devices that are too
low, too high and just right.
The Finer Points of Metering
Ampliﬁers, magnetic tape and even digital media are limited in the range of
signal levels that they can pass without distortion. As a result, audio engineers
need a basic standard to help determine whether the signals they’re working
with will be stored or transmitted without distortion. The most convenient way
to do this is to use a visual level display, such as a meter. Two types of metering
ballistics (active response times) are encountered in recording sound to either
analog or digital media:
n Average (rms)
n Peak
It’s important to keep in mind that recorded
distortion isn’t always easy (or possible) to ﬁx. You
might not be able to see the peak levels that could
distort your sound ﬁles, but your ears might hear
them after the damage is done. When in doubt, back
off and give your levels some breathing room.
The Finer Points of Metering 

From Chapter 2, we know that the root-mean-square (rms) value was developed
to determine a meaningful average level of a waveform over time. Since humans
perceive loudness according to a signal’s average value (in a way that doesn’t
bear much relationship to a signal’s instantaneous peak level), the displays of
many meters will indicate an average signal-level readout. The total amplitude
measurement of the positive and negative peak signal levels is called the peak-
to-peak value. A readout that measures the maximum amplitude ﬂuctuations of
a waveform is a peak-indicating meter.
One could deﬁnitely argue that both average and peak readout displays have
their own sets of advantages. For example, the ear’s perception of loudness is
largely proportional to the rms (average) value of a signal, not its peak value.
On the other hand, a peak readout displays the actual amplitude at a particular
point in time and not its overall perceived level. For this reason, a peak meter
might show readings that are noticeably higher at a particular point in the
program than the averaged rms counterpart (Figure 17.33). Such a reading will
alert you when short-term peaks are at levels that are above the clipping point,
while the average signal is below the maximum limits. Under such conditions
(where short-duration peaks are above the distortion limit), you might or might
not hear distortion as it often depends on the makeup of the signal that’s being
recorded; for example, the clipped peaks of a bass guitar will not be nearly as
noticeable as the clipped high-end peaks of a cymbal. The recording medium
often plays a part in how a meter display will relate to sonic reality; for example,
recording a signal with clipped peaks onto a tube analog tape machine might
be barely noticeable (because the tubes and the tape medium act to smooth
over these distortions), whereas a DAW or digital recorder might churn out a
hash that’s as ugly as the night (or your current session) is long.
481
The Art and Technology of Mixing  CHAPTER 17
FIGURE 17.33
A peak meter reads higher
at point A than at point B,
even though the average
loudness level is the same.
Getting to the heart of the matter, it goes without saying that, whenever the
signal is too high (hot), it’s an indication for you to grab hold of the channel’s
mic trim, output fader or whatever level control is the culprit in the chain and
turn it down. In doing so, you’ve actually become a dynamic range-changing
device. Additionally, the main channel fader (which can be controlling an input

Understanding “The Mixing Surface”
482
level during recording or a tape track’s level during mixdown) is by far the most
intuitive and most often used dynamic gain-changing device in the studio.
In practice, the difference between the maximum level that can be handled
without incurring distortion and the average operating level of the system is
called headroom. Some studio-quality preampliﬁers are capable of signal outputs
as high as 26 dB above 0 VU and thus are said to have 26 dB of headroom.
With regard to analog tape, the 3% distortion level for analog magnetic tape is
typically only 8 dB above 0 VU. For this reason, the best recording level for
most program material is around 0 VU (although higher levels are possible
provided that short-term peak levels aren’t excessively high). In some
circumstances (i.e., when using higher bias, low-noise/high-output analog tape),
it’s actually possible to record at higher levels without distortion, because the
analog tape formulation is capable of handling higher magnetic ﬂux levels. With
regard to digital media, the guidelines are often far less precise and will often
depend on your currently chosen bit-depth. Since a higher bit-depth (e.g., 24
or 32 bits) directly translates into a wider dynamic range, it’s often a good idea
to back off from the maximum level, because noise generally isn’t a problem.
Now that we’ve gotten a few of the basic concepts out of the way, let’s take a
brief look at two of the most common meter readout displays.
The VU Meter
The traditional signal-level indicator for analog equipment is the VU meter (see
Figure 17.32). The scale chosen for this device is calibrated in “Volume Units”
(hence its name) and is designed to display a signal’s average rms level over
time. The standard operating level for most consoles, mixers and analog tape
machines is considered to be 0 VU. Although VU meters do the job of indicating
rms volume levels, they ignore the short-term peaks that can overload a track.
This means that the professional console systems must often be designed so
that unacceptable distortion doesn’t occur until at least 14 dB above 0 VU.
Typical VU meter speciﬁcations are listed in Table 17.1.
Since recording is an art form, I have to rise to the defense of those who prefer
to record certain instruments (particularly drums and percussion) at levels that
Characteristic
Speciﬁcation
Sensitivity
Reads 0 VU when fed a +4-dBm signal (1.228 V into a
600-⏐circuit)
Frequency response
±0.2 dB from 35 Hz to 10 kHz; ±0.5 dB from 25 Hz to
16 kHz
Overload capability
Can withstand 5 times 0-VU level (approximately +18
dBm) continuously and 10 times 0-VU level (+24 dBm)
for 0.5 sec
Table 17.1
VU Meter Speciﬁcations
TheVU Meter 

bounce or even “pin” VU needles at higher levels than 0 VU. When recording
to a professional analog machine, this might actually give a track a “gutsy” feel
that can add impact to a performance. This is rarely a good idea when recording
instruments that contain high-frequency/high-level signals (such as a snare or
cymbals), because the peak transients will probably distort in a way that’s hardly
pleasing—and it’s almost never a good idea to overload the meters when record -
ing to a digital system. Always be aware that you can often add distortion to a
track at a later time (using any number of ingenious tricks), but you can’t easily
remove it from an existing track (software does exist that can help remove some
distortion after the fact, but it should never be relied upon). As always, it’s wise
to talk such moves over with the producer and artist beforehand.
The Average/Peak Meter
While many analog devices display levels using the traditional VU meter, most
digital hardware and software devices will often display a combination of VU
and peak program-level metering, using an LCD or on-screen readout. This best-
of-both-worlds system makes sense in the digital world as it gives us a traditional
readout that visually corresponds to what our ears are hearing, while providing
a quick-and-easy display of the peak levels at any point in time. Often, the 
peak readout is frozen in position for a few seconds before resetting to a new
level (making it easier to spot the maximum levels), or it can be permanently
held at that level until a higher level comes along to bump it up. Of course,
should a peak level approach the clipping level, a red “clip” indicator will often
light, showing that it’s time to back off the levels.
In recent years, a number of high-quality sound level meters have come onto
the market that allow the user to visually inspect the peak, loudness and spectral
levels within a recording. These tools can be extremely useful for obtaining the
best overall level of a mix, as well as the careful balancing of relative song levels
within a project.
Digital Console and DAW Mixer/Controller Technology
In the last decade, console design, signal processing and signal routing tech -
nology have undergone a tremendous design revolution with the advent of
digital technology. DAWs, digital console and mixers, the iPad and controller
design have found their ways into professional, project and audio production
facilities at an amazing pace. These systems use large-scale integrated circuits
and central processors to convert, process, route and interface to external audio
and computer-related devices with relative ease. In addition, this technology
makes it possible for many of the costly and potentially faulty discrete switches
and level controls that are required for such functions as track selection, gain
and EQ to be replaced by a touch controlled system. The big bonus, however,
is that since routing and other control functions are digitally encoded, it becomes
a simple matter for level, patch and automation settings to be saved into memory
for instantaneous recall at any time.
483
The Art and Technology of Mixing  CHAPTER 17
The Average/Peak Meter 

Understanding “The Mixing Surface”
484
THE VIRTUAL INPUT STRIP
From a functional standpoint, the basic signal ﬂow of a digital console or
DAW’s virtual mixing environment is conceptually similar to that of an analog
console, in that the input must ﬁrst be boosted in level by a mic/line pre-amp
(where it will then be converted into digital data). From this point, the signals
can pass through EQ, dynamics and other signal processing blocks; and various
effects and monitor sends, volume, routing and other sections that you might
expect, such as main and group output fader controls. From a control standpoint,
however, sections of a digital system might be laid out in a manner that looks
and feels completely different.
By the very nature of analog strip design, all of the physical controls must be
duplicated for each strip along the console. This is the biggest contributing 
factor to cost and reliability problems in traditional analog design. Since most
(if not all) of the functions of a digital console pass through the device’s central
processor, this duplication is neither necessary, cost effective or reliable. As a
result, designers have often opted to keep the most commonly used controls
(such as the pan, solo, mute and channel volume fader) in their traditional
input strip position. However, controls such as EQ, input signal processing,
effects sends, monitor levels and (possibly) track assignment have been designed
into a central, virtual control panel (Figure 17.34) that can be used to focus on
and vary a particular channel’s setting parameters. These controls can be quickly
and easily assigned to a particular input strip by pressing the “select” button
on the relevant input strip channel and then making the necessary changes to
that channel.
FIGURE 17.34
Presonus StudioLive 16.0.2
Digital mixing console with
channel parameter section
highlighted. (Courtesy of
Presonus Audio Electronics,
Inc., www.presonus.com)
In certain digital console, mixer and hardware controller designs, each “virtual
input strip” may be ﬁtted with a virtual pot (V-pot) that can be assigned to a
particular parameter for instant access. In others, the main parameter panel can
be multipurpose in its operation, allowing a number of controls and readouts
itself to be reconﬁgured in a chameleon-like fashion to ﬁt the task at hand. In
other cases, touch-screen displays can be used to provide for an inﬁnite degree
of user control, while some might use software “soft” buttons that can easily
reconﬁgure the form and function of the various buttons and dial controls in
the control panel.

From the above, it’s easy to see that digital consoles and hardware controllers
often vary widely in their layout, ease of operation and cost effectiveness. As
with any major system device, it’s often a good idea to familiarize yourself with
the layout and basic functions before buying or taking on a project that involves
such a beastie. As you might expect, operating manuals and basic tutorials are
often available on the various manufacturers’ websites.
THE DAW SOFTWARE MIXER SURFACE
Of course, in this new millennium, the vast majority of mixers that exist in the
world are software mixers that are integrated into our DAW of choice. With the
increased power of the computer-based digital audio workstation, mixing “in
the box” has become the norm, rather than the exception.
Through the use of a traditional (and sometimes not-so-traditional) user graph -
ical interface, these mixers offer functional on-screen control over levels, panning,
EQ, effects, DSP, mix automation and a host of functions that are simply too
long to list here. Often these software mixers (Figure 17.35) emulate their hard -
ware counterparts by offering basic controls (such as fader, solo, mute, select
and routing) in the virtual input strip. Likewise, selecting a channel track will
assign many of the channel’s functions to a wide range of virtual parameters
that can be controlled from a virtual strip or control panel.
485
The Art and Technology of Mixing  CHAPTER 17
FIGURE 17.35
Example of virtual, 
on-screen DAW mixers. 
(a) ProTools. (Courtesy of
Avid Technology, Inc.,
www.avid.com) (b) Logic.
(Courtesy of Apple Inc.,
www.apple.com)
When using a software mixer in conjunction with a DAW’s waveform/edit
window on a single monitor screen, it’s easy to feel squeezed by the lack of
visual “real estate.” For this reason, many opt for a dual-monitor display
arrangement. Whether you are working in a PC or Mac environment, this
working arrangement is easier and more cost effective than you might think
and the beneﬁts will immediately become evident, no matter what audio,
graphics or program environment you’re working on.
When using a DAW controller, the ﬂexibility of the DAW’s software environ-
ment can also be combined with the hands-on control and automation of a
hardware or iPad-based controller surface (Figure 17.36). More information 
on the DAW software mixing environment and hardware controllers can be found
in Chapter 7.

Understanding “The Mixing Surface”
486
FIGURE 17.36
DAW controllers. (a) Mackie
Universal Control. (Courtesy
of Loud Technologies, Inc.,
www.mackie.com) 
(b) V-Control Pro controller
for the iPad. (Courtesy of
Neyrinck Audio, www.
neyrinck.com)
MIX-DOWN LEVEL AND EFFECTS AUTOMATION
One of the greatest strengths of the digital age is how easily all of the mix and
effects parameters can be automated and recalled within a mix. Although many
large-scale consoles are capable of automation, computer-based DAW systems
are particularly strong in this area, allowing complete control of all parameters
throughout all phases of a production. The beauty of being able to set basic
levels within a mix or to mix under automation is that a mix can be built up
over time, allowing multiple mix versions to be saved, so that we can go back
and explore other production avenues or to correct a potential problem. In short,
the job of mixing becomes much less of a chore, giving us the time to pursue
less of the technology of mixing and more of the art of mixing. What could be
bad about that?
Although terminologies and functional offerings will differ from one system to
the next, control over the basic automation functions will be carried out in one
of two operating modes (Figure 17.37):
n Write mode
n Read mode
FIGURE 17.37
Automation mode
selections: (a) Pro Tools
showing auto selectors
within the edit and mixer
screens. (Courtesy of
Digidesign, a division of
Avid Technology, Inc.,
www.digidesign.com); 
(b) Cubase/Nuendo showing
auto selectors within the
edit and mixer screens.
(Courtesy of Steinberg
Media Technologies GmbH,
www.steinberg.net.)

487
The Art and Technology of Mixing  CHAPTER 17
Write Mode
Once the mixdown process has gotten under way, the process of writing the
automation data into the system’s memory can begin (actually, that’s not entirely
true, because basic mix moves will often begin during the recording or overdub
phase). When in the write mode, the system will begin the process of encoding
mix moves for the selected channel or channels in real time. This mode is used
to record all of the settings and moves that are made on a selected strip or strips
(allowing track mixes to be built up individually) or on all of the input strips
(in essence, storing all of the mix moves, live and in one pass). The ﬁrst approach
can help us to focus all of our attention on a difﬁcult or particularly important
part or passage. Once that channel has been ﬁnessed, another channel or group
of channels can then be written into the system’s memory—and then another,
until an initial mix is built up.
Often, modern automation will let us update previously written automation
data by simply grabbing the fader (either on-screen or on the console/controller)
and moving it to the newly desired position. Once the updated move has been
made, the automation will remain at that level or position until a previously
written automation move is initiated, at which point the values will revert to
the existing automation settings.
Read Mode
An automated console or DAW that has been placed into the read mode will
play the mix information from the system’s automation data, allowing the
onscreen and moving faders to follow or match the written mix moves in real
time. Once the ﬁnal mix has been achieved, all you need to do is press play,
sit back and listen to the mix.
Drawn (Rubber Band) Automation
In addition to physically moving on-screen and controller faders under read/
write automation control, one of the most accurate ways to control various
automation parameters on a DAW is through the drawing and editing of 
on-screen rubber bands. These useful tools offer a simple, graphic form of automa-
tion that lets us draw fades and complicated mix automation moves over time.
FIGURE 17.38
Automation rubber bands:
(a) Pro Tools. (Courtesy of
Digidesign, a division of
Avid Technology, Inc.,
www.digidesign.com); 
(b) Cubase/Nuendo.
(Courtesy of Steinberg
Media Technologies GmbH,
www.steinberg.net)
Write Mode 
Read Mode 
Drawn (Rubber Band) Automation 

Understanding “The Mixing Surface”
This user interface is so-named because the graphic lines (that represent the
relative fade, volume, pan and other parameters) can be bent, stretched, and
contorted like a rubber band (Figure 17.38).
Commonly, all that’s needed to deﬁne a new mix point is to click on a point
on the rubber band (at which point a box handle will appear) and drag it to
the desired position. You can change a move simply by clicking on an existing
handle (or range of handles) and moving it to a new position.
THE FINER POINTS OF MIXING
Once all of the tracks of a project have been recorded, assembled and edited,
it’s time to put the above technology to use to mix your tracks into its ﬁnal
media form. The goal of this process is to combine audio, MIDI and effects
tracks into a pleasing form that makes use of such traditional tools as:
n Relative levels
n Spatial positioning (the physical panned placement of a sound within a
stereo or surround ﬁeld)
n Equalization (affecting the relative frequency balance of each track)
n Dynamics processing (altering the dynamic range of a track, group or
output bus to optimize levels or to alter the dynamics of a track so it ﬁts
better within a mix)
n Effects processing (the adding of reverb, delay or pitch-related effects to a
mix in order to augment or alter the piece in a way that’s natural, unnatural
or just plain interesting
Within a well-produced project, sounds can be built up and placed into a sonic
stage through the use of natural, psychoacoustic and processed signal cues to
create a pleasing, interesting and balanced soundscape. Now, it’s pretty evident
that volume can be used to move sound forward and backward within the sound
ﬁeld, and that relative channel levels can be used to position a sound within
that ﬁeld. It’s less obvious that changes in timbre introduced through EQ, delay
and reverb can also be used to move sounds within the stereo or surround ﬁeld.
Of course, all of this sounds simple enough; however, the dedication that’s
required to hone your skills within this evolving art is what mixing careers are
made of.
Before we get started, I’d like to ﬁrst point you in a direction that just might
help your overall mixing technique:
n It’s often a good idea to create your own “desert island” favorite mix disc
or directory—a compilation of several of your favorite songs. By playing
these in your mix room, you can get a general idea of how the room
sounds. If there’s a problem (particularly, if the room’s new to you), it
can help point it out and help you avoid certain pitfalls.
488

n Secondly, you might think about inserting a high-resolution frequency-
analyzer into the output mix bus. This device (or plug-in) can literally
help you see how your mix is “shaping up” and might help pinpoint
possible problem areas.
Okay—now let’s take a moment to understand the process better by walking
through a ﬁctitious mix. Remember, there’s no right or wrong way to mix as
long as you watch your levels along the signal path. Also, there’s no doubt that,
over time, you’ll develop your own sense of style.
Mixing and Balancing Basics
1
Let’s begin building the mix by setting the output volumes on each of the
instruments to a level that’s acceptable to your main mixer or console. From a
practical standpoint, (if a basic mix wasn’t built during the production phase)
you might want to start with all of the faders down. You can then start to bring
the basic tracks up one at a time, until a desirable blend begins to take shape.
You could also take an entirely different approach by setting them all at unity
gain.
2
The next step is to repeatedly listen to the project as it begins to take shape,
making any fader and pan changes that are necessary until it begins to take
shape. If there is a vocal track, it could be brought into the mix at this time to
see how the overall blend works, or you could work on the instrument tracks
and bring the vocal track in later. In any case, the idea is to begin the process
of shaping the mix in a way that sounds pleasing and reinforces the song’s
musical style and intentions.
3
EQ can be applied to any problematic track that doesn’t quite sound good
or ﬁt into the mix properly. A general consensus on EQ is that it’s often best
to equalize a track within the context to the mix (i.e., not by listening to the
lone, soloed track), however, if a problem track does arise, try listening to the
offending instrument on its own, making any corrections and
then listening to how the corrections ﬁt into the mix.
Another approach to sound-shaping using EQ is
called “carving.” This approach uses EQ to
restrict frequencies that are present on a
track, so they don’t conﬂict with other
important instrument tracks that exist in
the same range. For example, a synth track
that has a great deal of additional low-
frequency energy, might be rolled-off at the
low-end, so that it doesn’t “muddy” up the
low end and/or conﬂict with the other bass
instruments.
As with all things sound, letting your ears be your EQ
guide takes time and experience—be patient with yourself.
489
The Art and Technology of Mixing  CHAPTER 17
REMEMBER:
It’s always wise to save your
individual mixes under a new and
identifiable name. You never know when you
might want to revert to a previous mix version.
Creating a “mixback”  directory within the song’s
session directory lets you throw the previous mix
versions (project_bitsamplerates_mixname_001,
etc.) into it without having dozens of previous
version numbers clutter things up in
your song directory.

The Finer Points of Mixing
490
4
Should the mix need to be changed at any point from its initial settings,
you might turn the automation on for that track and begin building up the
mix. You might also want to save your mix at various stages of its development
under a different name songname01, songname02, 02mybestmix, 03jimsnew
approach, etc. (personally, I save these versions within the project’s directory
as songname_mixbacks). Saving previous versions never hurts, as it makes it
easier to return to the point, should you make a mistake or simply need to take
a step back in time.
5
You might want to group various instrumental sections together within the
mix (Figure 17.39). The time savings of this process really can’t be overstated.
For example, by grouping instrument types together, you can:
n Reduce the volume on several tracks by simply grabbing a single fader
instead of moving each track individually.
n When a session contains lots of tracks, soloing the group tracks can help
you ﬁnd a track much more quickly than searching through the tracks—
it really does!
n Effects can be quickly and easily added to an instrument section at the
group level (either as an insert or as a send).
FIGURE 17.39
Figure showing the group
master (at left, entitled
“main”) is used to control
the grouping (at right, which
is assigned to “main”).
6
Just as reverb, delay and other time-based effects can be added to the mix,
should you wish to add a bit of controlled “grit” or “dirt” to a mix, an effects
send could be set up to add a degree of compression, tape emulation saturation
or whatever you might want to add to the mix. Once set up, this dirt, glue or
whatever you want to call it can be tastefully added to a single channel or
grouped set of tracks by mixing it in at the send level. The same goes for room
ambience as a room simulation plug-in (or actual mics in the studio) can be
entered into the mix at the send level to add a degree of openness and acoustic
realism to the mix.
7
When the mix comes together, you can then begin to think about inserting
some form of EQ and/or dynamics processing to the master channel output
bus. This is not a “required” stage in the process, simply an additional tool that
can be used to shape the overall sound of your mix. Of course, this subject

starts many a barroom brawl amongst recording and mastering engineers alike.
Adding too much compression to a mix can take the life out of your recording,
while adding just the right amount can help tame the mix’s overall levels.
Likewise, bad EQ decisions at the mix bus stage can wreck a mix like a barrel
of monkeys. Applying too much EQ to the master bus means that other problems
in your mix might exist—not to mention the fact that most mastering engineers
would prefer that you leave the ﬁnal processing to them. All of these are valid
comments that should be kept in mind, but in the end it’s your mix.
8
This calls to mind a very important aspect of most types of production
(actually it’s a basic tenant in life): Keep it simple! If there’s a trick you can use
to make your project go more smoothly, use it.
9
Once you’re happy with your basic mix, you might want to export it as a
ﬁle to play in your car, in a friend’s studio, put it in your best/worst boom box
and/or have your friends critique it. Take notes and then revisit the mix after a
day. You might want to take the time to compare your new mix to your desert
island disc, to hear how it stacks up against your favorite (or the producer’s
favorite) mixes. Good or bad, it’ll be a learning (and possibly humbling)
experience.
A FINAL FOOTNOTE ON THE ART OF MIXING
Actually, the topic of the art of mixing could easily ﬁll a book (and I’m sure it
has); however, I’d simply like to point out the fact that it is indeed an art form,
and as such is a very personal process. I remember the ﬁrst time I sat down at
a console (an older Neve 1604). I was truly petriﬁed and at a loss as to how to
approach the making of my ﬁrst mix. Am I over-equalizing? Does it sound right?
Will I ever get used to this sea of knobs? Well, folks, as with all things—the
answers come to you when you simply sit down and mix, mix, mix! It’s always
a good idea to watch others in the process of practicing their art and take the
time to listen to the work of others (both the known and the not-so-well
known). With practice, it’s a foregone conclusion that you’ll begin to develop
your own sense of the art and style of mixing, which, after all, is what it’s all
about. It’s all up to you now. Just dive into the deep end—and have fun!
491
The Art and Technology of Mixing  CHAPTER 17


Throughout the entire recording process, our ability to judge and adjust sound
is primarily based on what’s heard through the monitor speakers in a project
studio or professional control room environment (Figure 18.1). In fact, within
the audio and video industries, the word “monitor” refers to a device that acts
as a subjective professional standard or reference by which program material
can be critically evaluated.
493
CHAPTER 18
Monitoring
FIGURE 18.1
Example of a professional
monitoring system: Hit
Factory Criteria, Miami, FL.
(Courtesy of Solid State
Logic LTD, www.solid-
state-logic.com)
Despite steady advances in design, speakers are still one of the weakest links in
the audio chain. This weakness is generally due to potential nonlinearities that
can exist in a speaker system’s frequency response. In addition, interactions with
a room’s acoustic nature and frequency response can often lead to peaks and
dips that affect a speaker’s sonic character in ways that are difﬁcult to predict.
Add to this the factor of personal “tastes” in the sound, size and design types
of the countless speaker systems that are available, and you’ll quickly ﬁnd that
they’re also one of the most subjective tools in a production environment.

Speaker and Room Considerations
SPEAKER AND ROOM CONSIDERATIONS
Unless you have several rooms with precisely matching dimensions, materials
and furnishings (an unlikely scenario), you can bet your bottom buck that
identical speaker systems will sound different in differing room environments.
That’s to say, it will interact with the various complex acoustic and setup factors
to exhibit a unique frequency response curve and delay characteristic.
Although variations between production rooms often play a huge role in giving
a facility its own particular sound, extreme variations in a room’s frequency
response can lead to difﬁculties that can deﬁnitely be heard in the ﬁnal product.
For this reason, certain basic principles (which are covered in the Control Room
Design section in Chapter 3) have become common knowledge to many who
attempt the art of control room design. A few examples include:
n Reducing standing waves to help reduce erratic frequency response
characteristics within a room
n Reducing excessive bass buildup in room corners through the use of bass
traps
n Keeping the room’s acoustic layout symmetrical, so that the left/right and
front/rear imaging is consistent
n Using a careful balance of absorptive, reﬂective and diffusion surfaces to
help “shape” a room’s sonic character
Because of the untold number of acoustic variables that are involved, a project
that’s been recorded in one facility will often sound quite different when played
and/or mixed in another, even when high acoustical construction standards are
followed. Fortunately for us, the basic understanding of how production and
mixdown rooms can best be designed and/or acoustically adjusted has greatly
improved over the last several decades. This is largely due to the increased
availability of quality acoustical products and a better understanding of general
acoustics and how its effects can help shape a room’s sound.
Beyond careful acoustic design and construction, a professional facility might
choose to further reduce variations in frequency response by tuning (equalizing)
its speakers to the room’s acoustics so that the adjusted frequency response curve
will be reasonably as ﬂat as possible and, therefore, reasonably compatible with
most other control rooms. Tuning a speaker system to a room can be carried
out in either of several ways:
n Altering the settings on the speaker itself
n Using external equalizers (or EQ software) to smooth out the monitor
output lines
n Using a speaker system that can “self-tune” to match its response to the
room
One of the simplest ways to alter the acoustic and frequency response of a speaker
system is through the careful control of the basic EQ and system setting controls
494

495
Monitoring  CHAPTER 18
FIGURE 18.2
Rear controls for the Mackie
HR824mk2 active monitor
speaker. (Courtesy of Loud
Technologies, Inc.,
www.mackie.com)
that are found on most actively powered speaker systems (Figure 18.2). These
simple controls let the user roughly match level and EQ settings to best ﬁt their
application or placement layout. Often these settings can be used to:
n Finely match audio balance levels within a stereo and surround system
n Allow for basic high- and low-end tuning
n Partially compensate for bass buildup (whenever speakers are placed in
or near a corner or other large boundary)
n Offer various speaker “emulation” modes
Larger, passive monitors (often a far-ﬁeld pair) can be tuned by placing a 1/3-
octave bandwidth graphic equalizer between each of the console’s control room
monitor outputs and the power ampliﬁer. Of course, there are various ways to
ﬁne-tune a speaker system and room response to improve a studio’s overall
monitoring conditions. The simplest approach is to place a high-quality
omnidirectional mic at the center listening position and insert it into a channel
strip on your DAW. By recording a loop of pink noise (search Wikipedia for
the “colors of noise”) and playing it back equally to each speaker in the system,
basic level matching and frequency measurements can be carefully taken.
It should be noted that the above measurement method is often inaccurate (due
to time-based variables and other acoustical factors). Fortunately, standalone
software (Figure 18.3) can help with accurately measuring such variables as level,
EQ and time delay reﬂections, to help with the accurate tuning within a control
FIGURE 18.3
OmniMic V2 Precision
Measurement System.
(Courtesy of Dayton Audio,
www.daytonaudio.com)

Speaker and Room Considerations
room, studio, performance hall or auditorium. Obviously, these settings will
affect the way that your room sounds. If you have the correct measurement and
hardware tools–and you know what you’re doing or are carefully researching
your steps—you can proceed with caution, otherwise you might seek out
experienced, professional help. Keep in mind that these measurements are often
best interpreted by those who are well versed in acoustics, studio design and
the ﬁne art of common sense—in other words; careful ﬁne-tuning might be best
left to a competent professional or someone who has a professional under -
standing of acoustics and design.
Lastly, a number of professional stereo and surround speaker systems ship with
a measurement mic and built-in soft-/hardware that can automatically analyze
and tune the system’s response to the room. Such an approach might or might
not be an answer to your room’s problems.
In the end, it’s always wise to start by correcting any problems that exist with
a space directly by tackling the acoustical and layout problems that you are
facing ﬁrst. If the room is improperly laid out (for symmetry, improper
reﬂections, absorption, bass buildups at wall boundaries)—no amount of
automated correction will truly help. However, once you’ve tackled the room’s
basic acoustical nightmares, then you can go about the task of making smaller
(but carefully made) setting adjustments. Also, keep in mind that these
corrections and “Ah-HA! insights” don’t often happen overnight . . . it is usually
an ongoing process that improves as your experience and knowledge of your
room grows—as always, patience is a virtue.
Monitor Speaker Types
Since the buying public will be listening to your mixes over a very wide range
of speaker types, under an inﬁnite number of listening conditions, it’s often
wise to listen to a mix over several standardized speaker types during a recording
and/or mixdown session. Quite often, a console or monitor control system will
let you select between speaker/monitor types, with each set commonly having
its own associated controls for level matching. These types generally include
far-ﬁeld, near-ﬁeld, small-speaker and headphone monitoring systems.
FAR-FIELD MONITORING
Far-ﬁeld monitors (Figure 18.4) often involve large, multi-driver loudspeakers
that are capable of delivering relatively accurate sound at moderate to high
volume levels. Because of their large size and basic design, the enclosures are
generally sofﬁt mounted (built into the control room wall to reduce reﬂections
around and behind the enclosure and to increase overall speaker efﬁciency. An
introduction to sofﬁt design and construction can be found in Chapter 3). These
large-driver systems are sometimes used during the recording phase because of
their ability to safely handle high volume levels (which can come in handy
should a microphone drop or a vocalist decide to be cute and scream into an
open mic . . . ouch).
496

497
Monitoring  CHAPTER 18
FIGURE 18.4
Far-field monitor speakers:
(a) Genelec 1236 main
reference monitor. 
(Courtesy of Genelec Inc.,
www.genelec.com); 
(b) PMC QB1-A active
monitor speakers. 
(Courtesy of PMC Ltd.,
www.pmc-speakers.com)
For obvious reasons, this monitor type has never caught on in project studios
and has lost popularity to the easier-to-use and less expensive near-ﬁeld
bookshelf system that is common in most professional studios, as well. That’s
not to say that they can’t be found in many studios as a rock-’n’-roll, hip-hop,
“how does it sound loud?” reference. Of course, it’s important that we all stay
aware of the dangers that can come with long-term exposure to such sound
levels—not to mention the problems that can come with listening to mixes that
were mixed at high levels, but played back at moderate-to-low levels (more on
this later).
NEAR-FIELD MONITORING
Although far-ﬁeld monitors are useful for listening at high levels, few home
systems are equipped with speakers that can deliver “clean” sound at such high
SPLs. For this reason, most professional and project studios use near-ﬁeld monitors
that more realistically represent the type of listening environment that John H.
and Jill Q. Public will most likely have.
The term near-ﬁeld refers to the placement of small to medium-sized bookshelf
speakers on each side of a desktop working environment or on (or slightly
behind) the metering bridge of a production console. These speakers (Figure
18.5) are generally placed at closer working distances, allowing us to hear more
of the direct sound and less of the room’s overall acoustics.
In recent times, near-ﬁelds have become an accepted standard for monitoring
in almost all areas that relate to audio production for the following reasons:
n Quality near-ﬁeld monitors more accurately represent the sound that
would be reproduced by the average home speaker system.
n The placement of these speakers at a position closer to the listening
position reduces unwanted room reﬂections and resonances. In the case
of an untuned room, this helps to create a more accurate monitoring
environment.

Speaker and Room Considerations
n These moderate-sized speaker systems cost signiﬁcantly less than their
larger studio reference counterparts (not to mention the reduced ampliﬁer
cost because less wattage is needed).
One other consideration that should be taken into account when using near-
ﬁeld speakers on a table, desk or console top, are the natural resonances that
can be transmitted from the speaker onto the desktop. These resonances can
add a distinct coloration and added resonances to the sound. For this reason,
several companies have begun to develop foam and other material isolation
pads (Figure 18.6) that act to decouple the speakers from the surface, thereby
reducing resonances. I would like to add that there are other options for
decoupling speakers from working surfaces. For example, I actually hang my
speakers (using standard plumbing pipe ﬁxtures and carefully-designed brackets)
from the ceiling. This has the advantages of eliminating surface resonances, frees
up tons of workspace and it can look really amazing, if you’re careful about it.
As with any type of speaker system, near-ﬁelds vary widely in both construction
and fundamental design philosophy. It almost goes without saying that extreme
care should be taken when choosing the speaker system that best ﬁts your
production needs and personal tastes—they are possibly the most important
tools in your production room (other than you).
498
FIGURE 18.5
Near-field monitor speakers:
(a) Yamaha HS8 studio
monitor speakers. (Courtesy
of Yamaha Corporation of
America, www.yamaha.
com); (b) Mackie HR824mk2
active monitor speaker.
(Courtesy of Loud
Technologies, Inc.,
www.mackie.com);
(c) Neumann KH310 Active
Studio Monitor. (Courtesy
of Georg Neumann GMBH,
www.neumann.com);
(d) PMC two-two active
monitor speakers.
(Courtesy of PMC Ltd.,
www.pmc-speakers.com)

499
Monitoring  CHAPTER 18
SMALL SPEAKERS
Because radio, television, computer and cell phone playback are huge market
forces in audio production and are key in the distribution and sales of recordings,
it’s often good to monitor your ﬁnal mix through a small, inexpensive speaker
set (Figure 18.7). These mimic the nonlinearities, distortion and poor bass
response of these media (although the general specs of such speakers continue
to greatly improve over the years). Making active decisions at low levels over
small speakers will often reveal shortcomings within a mix that can actually
help improve the mix’s low-end response and overall presence, and you know,
having your mix stand out over grocery store speakers or on your laptop can
help the bottom line.
FIGURE 18.6
Speaker isolation pads can
help to reduce speaker/
stand resonances: 
(a) Primacoustic Recoil
Stabilizer pad. (Courtesy 
of Primacoustic,
www.primacoustic.com); 
(b) IsoAcoustics ISO-L8R155
stands. (Courtesy of
lsoAcoustics Inc.,
www.isoacoustics.com)
FIGURE 18.7
Small speaker systems: 
(a) Bose SoundLink®
speaker III. (Courtesy of 
Bose Corporation,
www.bose.com); 
(b) Avantone Active
MixCubes Full-Range Mini
Reference Monitors.
(Courtesy of Avantone Pro,
www.avantonepro.com)
Before listening to a mix over such small speakers or over laptop speakers, it’s
often a smart idea to take a break in order to allow your ears and your brain
to recover from the prolonged exposure of listening to higher sound levels 
over larger speakers. Taking the time to regain your perspective is deﬁnitely 
worth it.

Speaker and Room Considerations
500
HEADPHONES
Headphones (Figure 18.8) are also an important monitoring tool, as they remove
you from the room’s acoustic environment. Headphones offer excellent spatial
positioning in that they let the artist, engineer or producer place a sound source
at critical positions within the stereo ﬁeld without reﬂections or other
environmental interference from the room. Because they’re portable, you can
take your favorite headphones with you to quickly and easily check out a mix
in an unfamiliar environment.
FIGURE 18.8
Listening to the mix over
headphones can give you 
a different, and sometimes
more accurate perspective
(especially if you’re not
familiar with the room): 
(a) Sony MDR-7506
professional dynamic stereo
headphones. (Courtesy of
Sony Electronics, Inc.,
www.sony.com/proaudio);
(b) ATH-M50x Professional
Monitor Headphones.
(Courtesy of Audio-Technica
Corporation, www. audio-
technica.com)
It should be noted that while headphones eliminate the acoustics of a room
from the monitoring situation, they don’t always give a true representation of
how sounds will behave through loudspeakers. Monitoring through headphones
might emphasize low-level sounds like reverb and other effects more than
loudspeakers in a room. As a result, listening to a mix over both monitor types
is usually a good idea.
EARBUDS
Almost certainly, the most listened-to media speaker device in the world is the
earbud. They’re portable, they’re inexpensive (well, at least most of them are)
and they are a relative standard that will usually guarantee a certain level of
audio quality. Of course, it almost goes without saying that earbuds (like the
standard Apple buds) can come in handy in helping to determine how a project
will sound to the masses.
IN-EAR MONITORING
One step beyond the concept of either headphones or earbuds is the concept
of the professional in-ear monitor. These carefully tuned devices can range from
being a generic design that can ﬁt anyone all the way to those that are highly
custom and personalized (Figure 18.9). The latter being molded to precisely ﬁt
the customer’s ear and coming in a variety of driver conﬁgurations that offer a
frequency response curve that matches the user’s preferences and needs. Such
a device can also greatly reduce ambient and stage noise, allowing them to be
used on-stage for professional monitoring purposes. These high-quality, on-stage

501
Monitoring  CHAPTER 18
FIGURE 18.9
Professional in-ear monitors
in an on-stage setting.
(Courtesy of 64 Audio,
www. 64audio.com)
or on-the-go devices can range into the thousands of dollars, contain 12 or
more drivers for each ear and be custom ﬁtted and acoustically tailored to work
only with your ears. I know of one world-class mastering engineer who uses
in-ear monitors to master platinum artists on a regular basis.
YOUR CAR
Last, but not least, your (or your friend’s) car could be a big help in determining
how a mix will sound in another of modern society’s most popular listening
environments. You might take your mix out for a spin on a basic car system,
as well as a supped-up, window-shakin’ bass bomb (Figure 18.10).
FIGURE 18.10
Kickin’ it in the ride—
preferably on Sunset
Boulevard with the top
down!
Speaker Design
Just as the sound of a speaker system will vary when heard in different acoustic
environments, speakers of different designs and operating types will usually
sound very different from one another, even when heard in the same room.
Enclosure size, number of components and driver size, crossover frequencies
and design philosophy contributes greatly to these differences in sound quality.
Monitor speakers are available with high-frequency drivers that are made of
various types of hard and soft plastic domes, metal domes and even ones made

Speaker and Room Considerations
with corrugated metal or plastic ribbon materials. Bass drivers can also be made
from various materials, having various sizes that best match the enclosure size.
Enclosures can incorporate an air suspension (an airtight system that seals the
air in its interior from the outside environment) or a bass reﬂex design (which
uses a tuned bass porthole that’s designed into the front or rear of the speaker
enclosure). Air suspension is often used in smaller “bookshelf” designs, often
producing a strong, “tight” bass response that rolls off at the extreme low end.
Bass reﬂex vented designs, on the other hand, allow the air mass inside the
enclosure to mix freely with the outside air in such a way as to act as a tuned
resonator (which serves to acoustically boost the speaker’s output at the extreme
lower octaves).
Because there are so many variables in speaker and ampliﬁer design, it quickly
becomes clear that there’s no such thing as the “ideal” monitor system. The
ﬁnal choice is often more of a matter of personal taste and current marketing
trends than one of subjective measurements. Monitors that are widely favored
over a long period of time tend to become regarded as the industry standard;
however, this can easily change as preferences vary. Again, the best judge of
what works best for you should be your own ears and personal sense of style.
The overall goal is to:
n Choose the best speaker type, make and sound that best matches your
way of working
n Match it as best you can to the acoustics of the room
n Become accustomed to the sound and use that reference to make the
highest-quality productions that you can
ACTIVE POWERED VS. PASSIVE SPEAKER DESIGN
As you might expect, most of the more popular monitor types that are in use
today incorporate an actively powered ampliﬁer into their design. These cost-
effective systems have become widely accepted by the professionals and project
communities due to their:
n Compact design
n High-quality sound (often these systems use bi- or tri-ampliﬁed circuits)
n Expandability (additional speakers can be cost effectively added for
surround-sound monitoring)
n No need for an external power ampliﬁer
For these reasons, these systems are often ideal for project- and DAW-based
facilities and are steadily increasing in popularity.
Electronic crossover networks (Figure 18.11), called active crossovers, use complex
analog and digital circuitry to split the incoming line-level audio signal into
various frequency bands. Each equalized signal is then fed to its own power
amp, which in turn is used to drive the respective bass, mid-, and/or high-driver 
502

503
Monitoring  CHAPTER 18
FIGURE 18.11
Example of a passive two-
way crossover system,
showing a crossover
frequency of 1500 Hz.
elements. Such a system is generally referred to as being bi-ampliﬁed or tri-
ampliﬁed (depending on the number of crossovers and power amps that are
required by the design). These systems have several advantages:
n The crossover signals are low in level, meaning that inductors (which can
introduce audible ringing and intermodulation distortion) can be
eliminated from the design.
n Power losses (due to the inductive resistance within the passive crossover
network) can also be eliminated.
n Each amp is band-limited (meaning that each speaker will only need to
output frequencies for which it was designed—highs to the tweeter, lows
to the woofer, etc.), thereby increasing the speaker’s overall design and
power efﬁciency.
Speaker Polarity
A common oversight that can drastically affect the sound of a multi-speaker
system is for the cables to be wired out-of-phase with respect to each other. Speaker
polarity is said to be electrically in-phase (Figure 18.12a) whenever a signal that’s
equally applied to both speakers causes their cones to properly move in the
same direction (either positively or negatively). When the speakers are wired
out-of-phase (Figure 18.12b), one speaker cone will move in one direction while
the other will move in the opposite direction, causing an incoherent signal that
has poor center imaging at the listener’s position—not a good thing.
FIGURE 18.12
Relative speaker cone phase
movements: (a) in-phase;
(b) out-of-phase.

Speaker and Room Considerations
Speaker polarities can be easily tested by applying a mono signal to both or all
of the speakers at the same level. If the signal’s image appears to originate from
a single, localized point directly between the speakers, they have been properly
wired (in-phase). If the image is hard to locate and appears to originate beyond
the outer boundaries of a stereo speaker pair, or shifts as the listener moves his
or her head, it’s a good bet that the speakers have been improperly wired (out-
of-phase).
An out-of-phase speaker condition can be easily corrected by checking the
speaker wire polarities. Using powered speakers, you can check the polarity of
the cable using a polarity tester box or you could get out a volt-ohm meter
(every production room should have one) and test that the positive and neutral
leads are wired properly. On a passive speaker design, the “hot” lead (+ or red
post) leading from each amp channel should be secured to the same lead on
its respective speaker/amp connection. Likewise, the negative lead (– or black
post) should be connected to its respective lead for each speaker in the system.
Balancing Speaker Levels
Another important factor that is often overlooked (but should always be taken
into consideration) is the need for balancing your speaker levels, so monitoring
and playback occur at equal and matched levels at the listening position. If the
speaker level settings (at the passive or active power amp) are improperly set,
your ﬁnal mix levels could end up being off-balance—meaning that your center
and overall imaging might lean to the left or right. This will result in a ﬁnal
mix that, if not caught in the mastering phase, will be off-center. You’d be
surprised how many times mastering engineers run into this problem, even with
high-proﬁle releases.
The best way to calibrate your speaker system is to:
n Record a minute or so worth of pink noise, and copy this same ﬁle into
your workstation in a loop fashion onto as many channel tracks as you
have speakers in your system.
n Place a sound level meter on a camera stand directly at the listener’s
position and height (you could use an SPL meter app on your phone or
pad, or an inexpensive sound level meter that is bought online will also
do the trick, as it’s always nice when the meter has a thread for mounting
the unit directly onto a mic stand or camera tripod). Point the meter at
the left speaker (Figure 18.13).
n Play the pink noise tracks one at a time (with each track being assigned
to its associated speaker) at a level of about 80–85 dB SPL and adjust the
sensitivity and gain on both the meter and master monitor gain, so that
the meter reads at “0” on an analog meter or at a speciﬁed level on a
digital readout.
n Swivel the meter, so that it points to the right speaker. Then, set its level
to match until all of the speakers are at equal level at the listening position.
504

505
Monitoring  CHAPTER 18
n You’re now ready to start mixing or listening, knowing that your system
is properly level matched.
n I almost forgot to mention to “switch the SPL meter off”—I can’t tell you
how many times I’ve drained the battery this way.
Once done, you can ﬁnally be sure that the speaker levels are properly matched.
Whenever changes are made at a later time or it you’re in doubt, just call up
the noise “session” and repeat the setup test. It’s always better to be safe than
sorry.
FIGURE 18.13
General setup for balancing
your speaker levels.
MONITORING
When mixing, it’s important that the engineer be seated as closely as possible
to the center of the sound ﬁeld (making allowances for the producer, musicians
and others who are also doing their best to be in the “sweet spot”) and that all
the speaker volumes are adjusted equally. For example, if the engineer is closer
to one speaker than another, that speaker will sound louder and the engineer
may be tempted either to pan the instruments toward the far speaker or boost
that entire side of the mix to equalize the volumes. The resulting mix would
sound properly centered when played in that room, but in another environment,
the mix might be off-center.
Here are a few additional pointers that can help you get the best sound from
your control room monitors:
n Keep all room boundaries and reﬂections in the room as symmetrical as
possible along the L/R and front/back axis of the mixing sound ﬁeld.
n Keep large, direct reﬂections to a minimum within the room. This can be
done by introducing absorptive (25–30%) and diffusive (25–30%)
materials into the room. These will help reduce reﬂections to a level that’s
at least 20 dB down from the direct signal.
n It’s often wise to place absorptive materials behind the speakers (in the
front section of the room). This reduces reﬂections that emanate from the

Monitoring
506
rear of the speakers and keep them from reﬂecting back to the listener’s
position.
n Diffusion is often helpful at the sides and especially in the rear of the
room, to help break up reﬂections that will return to the listener’s position.
n Angle the monitors symmetrically toward the listening position in both
the horizontal and vertical planes.
n Whenever near ﬁeld monitors are used, you might consider placing them
on medium-density foam blocks to reduce console- and desk-borne
vibrations.
A sizable number of the world’s top mixing engineers recommend the following:
1. Monitoring at low levels (commonly at 75–80 dB)
can have a positive impact on how your mixes
will sound. This is especially true for monitoring
and room systems that are acoustically
compromised, as monitoring at lower levels will
reduce the effect that room reﬂections will have
on the overall sound.
2. Listening to a mix from another room (i.e., just
outside of the control room door) can actually
give you an alternative viewpoint as to how the
mix will sound. Literally, stepping out and away
from the mix can be a huge help toward giving
you a new understanding of how it sounds.
In Chapter 17 (Mixing), we talked about creating a high-quality desert island
compilation disc or music directory that’s made up of a number of your favorite
or most respected songs that can be listened to as a reference for evaluating a
studio or listening room and its speaker monitors. It’s always a good idea to
take these songs for a spin when entering a new environment—actually; it’s a
good idea to take a listen from time to time in your own “trusted” room, just
to keep a sense of perspective about how it sounds. Trust me, you’ll never come
across a sound device that’s as subjective and variable as your own sense of
hearing perception.
Monitor Level Control
As monitor systems grow to accommodate various production and playback
formats, controlling the monitor level, adjustments and switching can become
problematic. Many multiple-output consoles and high-end DAW systems are
capable of handling the monitoring requirements of the various monitor formats
(including multiple speaker selection and surround sound); however, even these
can fall short when multiple sources, level trims and straightforward level
control are taken into consideration (although newer interface designs are ﬁnally
offering multichannel level control). For this reason, many have turned to using
a high-quality interface or dedicated studio monitor management system (Figure
18.14) in order to handle the monitoring needs of a professional or project
studio.

MONITOR VOLUME
Before continuing, I’d like to revisit another important factor—volume. During
the record and mixdown stage, it’s important to keep in mind that the Fletcher-
Munson curves will always have a direct effect on the frequency balance of a
mix. Because our ears perceive sound differently at various monitoring levels,
our ears will easily perceive the extreme high and low frequencies in the mix
when monitoring at loud levels (sounds good, doesn’t it?). However, when the
mix is played back at lower levels (such as over the radio, TV or computer), our
ears will be much less sensitive to these frequencies and the bass and extreme
highs will probably be deﬁcient, which can leave the mix sounding dull, distant
and lifeless.
Unlike in earlier years, when excruciatingly high SPLs tended to be the rule in
most studios, recent decades have seen the reduction of monitor levels to a
more moderate 75 to 90 dB SPL (A good rule of thumb is that if you have to
shout to communicate in a room, you’re probably monitoring too loud). In
fact, a number of the top mix engineers and producers (and an ever-growing
number of individuals) are mixing productions at levels that are quite low. The
general idea behind this method is that if the project sounds really good at low
levels, they will sound good at low-to-moderate levels (over laptops, TV, the
Internet, etc.,) but when they are turned up, the Fletcher-Munson Curve will
boost levels on the highs and lows, making it sound even better. Truth is, many
of the industry’s top-charted hits were mixed in this way.
No matter what your own preferences for mixing are, it’s important that we be
aware of potential problems with ear fatigue and hearing damage due to
prolonged exposure to high SPLs. For more information on safe monitor levels
and hearing conservation, contact the House Ear Institute at www.hei.org or
H.E.A.R.® at www.hearnet.com.
Spectral Reference
In addition to our best set of tools—our experience, judgment and ears—a visual
tool known as a spectral analyzer is often used to give visual cues as to an audio
program’s overall frequency balance at any point in time. These applications
507
Monitoring  CHAPTER 18
FIGURE 18.14
Studio monitor management
systems: (a) UR 28M
monitor control/audio
interface. (Courtesy of
Steinberg Media
Technologies GmbH,
www.steinberg.net); 
(b) iD22 audio interface
and monitoring system.
(Courtesy of Audient
Limited, www.audient.com)

Monitoring
508
FIGURE 18.15
Spectral analyzer 
displays: (a) Within the
Cubase/Nuendo EQ window.
(Courtesy of Steinberg
Media Technologies GmbH,
www.steinberg.net); 
(b) iZotope’s Insight
Essential Metering Suite.
(Courtesy of iZotope Inc.,
www.izotope.com)
(which can be found in either a stand-alone or DAW program or suite of plug-
ins) give a visual bar readout of a signal’s level at various frequencies throughout
the audible band (Figure 18.15). Obviously, such a tool can help an engineer
or producer to zero in on an offending or deﬁcient frequency and/or bandwidth
simply by looking at the display over time. During both the record and mix
phases, these tools can help point out and avoid potential spectral problems.
Monitoring Configurations in the Studio
It’s important to remember that a large percentage of your potential customers
may ﬁrst hear your mix over a TV, computer or AM/FM radio in mono. Therefore,
if a recording sounds good in stereo but poor in mono, it might not sell as well
because it failed to take these media into account. The same might go for a
surround-sound mix of a music video or feature release ﬁlm in which proper
attention wasn’t paid to phase cancellation problems in mono and/or stereo
(or vice versa). The moral of this story is simply this:
To prevent potential problems, a mix should be
carefully checked in all its release formats in order to
ensure that it sounds good and that no out-of-phase
components are included that would cancel out
instruments and potentially degrade the balance.
The most commonly accepted speaker conﬁgurations are mono and stereo
(Further reading on 5.1, 7.1 and other immersive audio standards can be found
in Chapter 19).
1.0 MONO
Even in this day and age, a number of people will ﬁrst experience a mix in
monaural (mono) sound (Figure 18.16a). That’s to say, they’ll hear your song
on their phone, in an elevator, over a small radio or an iPad, etc. For this reason,
it’s often a good idea to listen to a mix with the stereo channels combined into
mono for overall sound, phase and compatibility. The days of doing a special

mono mix are pretty much behind us (given the fact that most people experience
music over headphones or computer-related devices), but it’s always wise to
check for mono compatibility. Phase cancellations can cause instruments or
frequencies in the spectrum to simply disappear whenever a mix is summed to
mono. The best tools for reducing phase errors are good mic technique, a phase
plug-in or display and, of course, your ears.
2.0 STEREO
Ever since the practical development of the 45º/45º record cutting process,
stereophonic (stereo) sound (Figure 18.16b) has ruled the turntable. Of course,
over the years, stereo has also grown to rule the computer, Internet, FM radio,
the CD player, TV, auto, and the cell phone. For these reasons, the creation of
a quality stereo mix is extremely important with relation to L/R balance, overall
frequency balance, dynamics, depth and effects.
Some of the basic rules for monitoring in stereo are:
n Try to make your mixing environment as acoustically and physically
symmetrical (within reason) in order to ensure that the L/R balance, effects
balance and overall imagery are accurate within the stereo sound ﬁeld.
n Balance your speaker levels carefully, so that they match in level.
n Use your ears, be inventive and have fun!
509
Monitoring  CHAPTER 18
FIGURE 18.16
Basic monitoring
configurations: (a) mono; 
(b) stereo.
2+1 STEREO + SUB
In actuality, 2+1 isn’t really a standard, but it’s a relatively easy title to remember.
The “+1” represents the addition of a powered sub to a set of stereo speakers
(not a .1 LFE channel). Instead of providing an additional sonic boom “effect”,
the subwoofer is actually used to extend and to help deﬁne the music’s low
end. Extreme care should always be taken when setting up such a system, as a
loud and improperly setup sub and crossover combination can create a low
end that can sound great, but be very inaccurate. If your system creates too
much of a big, bad (but false) bass sound in the mix room your mixes might
actually end up being bass shy and boxy (not quite what you had in mind for
your special mix).

Monitoring
510
The proper use and setup of a subwoofer can help extend the low end, while
adding a tight deﬁnition to the bass that can actually bring a monitor playback
system to life. As you might expect, the response and overall sound of a sub is
often heavily inﬂuenced by the acoustics of the playback environment. In
addition to using a software frequency and acoustics analyzer (that’s capable
of displaying phase relationships as well as frequency response), one of the best
ways to set up and tune a sub is by careful experimentation. When placing the
subwoofer, the following concepts should be kept in mind:
n Because low-frequency audio is basically non-directional, some feel that
a sub can be placed in any spot in the front of the room. However, a
centrally placed position is most likely preferable, as a difference can be
heard with upper-bass deﬁnition and image localization.
n Although a sub’s output will be greater when placed near a wall boundary,
a “muddy” or “boomy” sound might result due to unfavorable room and
corner reﬂections.
n Most active subs will include level and crossover frequency adjustments.
These settings should be used to closely match your sub to the chosen
speaker set or sets.
n Active subs will often include a phase switch that can match the phase
(driver motion) to your main speaker set. Using an analyzer or your ears,
set the phase to a position that sounds best.
Although it’s not commonly known, there are two ways to set up a 2.1 system.
Through careful adjustment and care, either method can work:
n Connect the stereo signal to the sub’s inputs and use the sub’s internal
crossover outputs to feed a bass-limited signal to the main stereo speakers
(Figure 18.17). This traditional system will band-limit the stereo speaker’s
low end and should be carefully tuned using the sub’s crossover and level
settings.
n The active sub (which usually has two inputs) can be combined with the
stereo signal by using a simple “y” connection or summing network. This
method will NOT band-limit the stereo speaker’s low end and should be
tuned with VERY special care, because interference and interactions
between the bass drivers can deﬁnitely cause problems.
FIGURE 18.17
Example of a stereo + sub
system that uses crossovers
within the sub to split the
band-limited signals going
to the sub and monitor
speakers.

All of this simply leads to the fact that sub placement and setup are super-
critical for getting the best sound possible—and it’s worth mentioning again
that an imbalanced system can cause really big problems for you, your room
and your reputation.
Monitoring in the Recording Space
In addition to the need for an accurate monitoring environment in the control
or production studio, musicians often have special needs for playing back or
monitoring in order to best hear the sound that’s being recorded during a
session. This can take the form of a simple cue (American) or foldback (British)
send of the overall mix that the engineer hears, or it can be groupings of several
different cue sub-mix sends that are tailored to individual musicians (i.e., the
drummer needs to hear more of the snare and guitar in order to best keep time,
the vocalist needs to mostly hear herself with a fair amount of reverb and the
pianist needs to hear more of the bass, you get the idea). Truth is, setting up a
proper mix is almost always an important factor during a recording or overdub
session. Without the ability to hear what’s actually going down, the performer
will be struggling and not comfortable during the take—deﬁnitely not a good
situation for an artist who’s trying to do his or her best.
HEADPHONES IN THE STUDIO
Monitoring over headphones in the studio is by far the most common way to
monitor sound during a recording session.
When recording, it’s often best to use sealed headphones to prevent or minimize
the monitor feed from leaking back into the newly recorded track. Sometimes,
however, this isn’t the best way to go. Vocalist and other performers will often
prefer to wear only one side of the headphone, so they can hear the natural
sound and room acoustics, their own voice or instrument along with the
recorded track. Other musicians might bring along their own headphones or
in-ear monitors, so they can best hear the mix in a trusted environment.
The number of headphones will vary with the particular session requirements.
An orchestral ﬁlm overdub, for example, could easily use upward of 50 or more
headphone pairs during a single session. The next session booking might be an
overdub that will call for a single pair, however, sessions that call for several
headphones with a single cue mix or a couple of simple mix versions is far
more likely to be the norm when recording. Truly, the life of an engineer or
setup assistant is rarely a dull one.
Such setup variations also place demands on the distribution of headphone
power and the number of required feeds. As you might imagine, the power that
would be needed to run 50 headphones is quite considerable, requiring that a
power ampliﬁer be used to drive any number of headphone distribution boxes
throughout the room. On the other hand, the power that would be required
to drive one or two headphones in a project studio might be so small that they
511
Monitoring  CHAPTER 18

Monitoring
512
FIGURE 18.18
Headphone distribution
amplifiers: (a) Powerplay
Pro-8 8-channel headphone
distribution amp. (Courtesy
of Behringer Intl GmbH,
www.behringer.com); 
(b) Presonus HP4 4-channel
headphone monitor.
(Courtesy of Presonus
Audio Electronics,
www.presonus.com)
could be driven from the console/mixer’s internal headphone or a basic
headphone distribution amp (Figure 18.18).
For those who wish to allow the musicians to create their own mix, a growing
number of headphone mix/distribution systems (Figure 18.19) exist that can
take the various direct or sub-group mixes of a DAW or console and send these
individual feeds directly to the musician’s headphone amp/control station.
Here, the musician can have complete control over volume, pan, and instrument
mix within his or her headphone mix.
FIGURE 18.19
Headphone personal
mix/distribution systems: 
(a) HRM-16 16 channel
personal headphone mixing
station. (Courtesy of Furman
Sound, Inc., www.furman
sound.com); (b) Hear
Technologies Hear Back
Personal Monitor Mixer
System. (Courtesy of Hear
Technologies,
www.heartechnologies.com)
One last method for monitoring is by far the most commonly used in the
modern DAW era. This makes use of individual monitor sends that can be routed
from within your DAW, directly to an output on a multi-channel audio interface
for routing to the studio’s headphone distribution amps. In this way, the
engineer can assign tracks, groups out any combination within the session to
the headphones, all with total recall at a later time.
With the advent of wireless control over a DAW from a phone or pad, the
musician can also simply download the app that integrates with the DAW, call
up the headphone mix sends and dial in his or her own headphone mix,
wirelessly from their playing position (Figure 18.20).
FIGURE 18.20
Wireless iDevices can be
used in the studio to directly
control a personal
headphone mix.

PLAYBACK SPEAKERS IN THE STUDIO
Often, there’s not enough time for the musicians to leave their instruments and
walk into the studio to hear a playback between takes. For this reason, studio
monitors are commonly mounted in the studio for immediate playback. These
are usually larger monitors, because they will need to be driven to levels that
can adequately ﬁll the studio and will often need to withstand unintentional
abuse (from loud talkback levels, feedback, etc.).
For those musicians who would like to leave their headphones on the shelf and
create a vocal or instrumental take using speakers, it’s actually possible to place
a microphone directly between two, equally spaced speakers (either in the
studio or in the control room) that have been intentionally selected to be out-
of-phase with each other (further info can be found in the section on speaker
polarity earlier in this chapter). Under ideal conditions, this will result in a
180º phase cancellation at the mic position, meaning that the speaker cue signal
can be heard, but will cancel out or be greatly reduced in level at the mic’s
pickup position.
513
Monitoring  CHAPTER 18


At a special media event that was attended by about 500 people, my good friend
George Massenburg asked the crowd, “How many of you truly know the joys
of surround sound playback or production?” In that crowd, about 20 of us raised
our hands. He then went about the business of playing back several very well
known mixes that were remixed in surround and talked about the beneﬁts of
surround-sound music and audio-for-visual media production (Figure 19.1).
Right-on, George!
515
CHAPTER 19
Immersive Audio 
(5.1 and Beyond)
FIGURE 19.1
This Mixing Room isn’t only
set up for 5.1 (notice the
speakers on the ceiling?),
it’s also set up for 9.1
(Auro3D), Galaxy Studios,
Mol, Belgium. (Courtesy of
Galaxy Studios,
http://www.galaxy.be)
Truth is, whether you’re an advocate or an adversary of the concept of surround
sound, one thing is for sure—it exists in the here and now, and it is certain to
play an ever-growing role in the media technologies of tomorrow.
I think that at this point I have to break from my role as a neutral author and
state ﬂat out that I’m a HUGE surround fan and a 4-time Grammy nominee in

Immersive Audio
this category. For me, the ability to compose and mix in surround has been an
uplifting and hugely beneﬁcial experience. I clearly remember as a kid, placing
two album covers behind my ears and listening as the music came to life around
my head (go ahead, give it a try)! The ability to augment music and visual
media by placing sounds within a 360º circle has literally opened up new
dimensions in mixing and effects-placement technologies in a way that keeps
me creatively young.
Most of the people that I know who are ideologically closed to the idea of
immersive audio haven’t worked with or listened to the medium. I urge that
you keep an open mind to the process, watch movies and listen to music in
surround (in any available format) and, if at all possible, take the time and
effort to familiarize yourself with the process of producing sound in surround.
Before I put my neutral writer’s cap back on, I’d like to present the strongest
argument for becoming familiar with the production techniques of surround:
enhanced job opportunities. I have friends that live in the technological heart
of numerous cities who are completely unfamiliar with any and all forms of
surround. It simply never occurred to them that they could increase their
opportunities, client base and perceived prestige in the ﬁelds of mixing music,
soundtracks for movies and gaming by investing in a surround monitoring
system and learning the basic tools and techniques of recording, mixing and
mastering media for multichannel sound. If for absolutely no other reason, the
ability to understand and work in such new and upcoming technologies can
help give your career a marketing edge.
IMMERSIVE AUDIO: PAST TO THE PRESENT
Of course, it all started with the movies. In the pre “talky” days, movie theaters
were anything but silent—organs, chimes and all sorts of percussion clanged
from the front, sides and rear parts of the room behind ornate wall features.
With the introduction of movie sound and the use of the musical score in the
late 1920s, all of this came to a halt when soundtracks were played back by the
only known delivery format of the time: mono.
On November 13, 1940, Walt Disney’s Fantasia opened up the sound ﬁeld to
stereo when it premiered at New York’s Broadway Theater. Although it wasn’t
the ﬁrst ﬁlm that was produced using a “multiple channel recording” process,
Fantasia was the ﬁrst to introduce multichannel sound to the public.
The ﬁnal mix of Fantasia was printed onto four master optical tracks for playback
using a special RCA system called “Fantasound.” (Unlike the two-channel format
that was adopted for home playback, ﬁlm “stereo” sound started out with, and
continues to use, a minimum of four discrete channels.) This multispeaker setup
placed three horns behind the screen and 65 smaller rear-side speakers around
the walls of the theater. Due to the outlandish setup costs (estimated at about
$85,000 for each theater at the time), RCA quickly stopped making this advanced
system.
516

517
Immersive Audio (5.1 and beyond)  CHAPTER 19
In the early 1950s, the ﬁrst commercially successful multichannel sound formats
came onto the scene with the development of CinemaScope (4-track 35 mm)
and Todd-AO (6-track 70 mm). Both of these formats made use of magnetic
tracks that existed alongside the release print picture, and required that the
projector be ﬁtted with special playback heads, amps and speakers.
In the early 1970s, the home consumer stereo market was gaining in popularity
and audio quality. With the development of higher quality amps, speakers and
record turntables came new experimentations in systems design that eventually
led to the development of Quadraphonic Sound (Quad). This playback system
made use of four speakers (Figure 19.2) that were placed in the four corners of
a room, which enveloped the home listener in a L/R/Ls/Rs listening experience.
FIGURE 19.2
Example of a Quadraphonic
(Quad) speaker setup.
Although analog reel-to-reel and cassette tape machines were used in homes,
they were still relatively expensive. By elimination, this meant that playback would
have to be carried out by the most popular medium of the day—the LP record.
Reproducing four channels from a record wasn’t easy. Often, the task of encoding
four channels onto the two walls of a vinyl record was done with relative
phase or by using a complex, high-frequency carrier tone that was used to
modulate the sum and difference channels. However, the real difﬁculties lay in
the wide assortment of incompatible encode/decode formats that were offered
by various manufac turers. Given the fact that your system might not play back
the latest and greatest release from another company, and that discs were both
expensive and prone to deterioration over a short period of time (the high-
frequency signals on modu lated records would literally wear away), the Quad
revolution quickly died away.
Stereo Comes to Television
Since its inception, surround sound had been used in motion picture soundtrack
production with great success. With the introduction of Dolby noise reduction
and multichannel audio in the theater, good sound was not only appreciated,
it was expected! On the other side of the media tracks, television sound was

Immersive Audio: Past to the Present
strictly a lo-ﬁ, mono experience up until the early 1980s and was strictly an
afterthought to the visual image. However, with the adoption of the video
cassette recorder (VCR) and later hi-ﬁ stereo sound from a VCR, discriminating
audiences began to appreciate the higher-quality audio that accompanied the
almighty image. With the dawning of the music video (I want my MTV!), stereo
broadcast television and the stereo VCR, TV was ﬁnally forced into offering a
higher-quality, multichannel, visual experience.
Theaters Hit Home
In 1982, Dolby Labs introduced “Dolby Surround,” an extension of their
professional Dolby Film Sound Project. By 1987, millions of homes were
beginning to be ﬁtted with consumer receivers and high-end audio systems that
were integrated with video. With the introduction of Dolby Pro Logic, a simple
system was put into place that allowed phase information to be extracted from
the two tracks of a stereo program to reproduce the L/R/C/Surround ﬁeld. In
1992, with the introduction of Dolby’s AC3 surround encode/decode system
(Dolby Digital), it became possible for discrete 5.1 surround sound to be
encoded directly with the new visual entertainment medium of the early twenty-
ﬁrst century—the DVD—and now to the newer Blu-ray disc.
Today’s Immersive Audio Experience
Recent advances in immersive audio are still largely being driven by the ﬁlm
industry, but that’s not to say that surround sound television, surround music
and especially the immersive audio experience that’s being produced today for
the gaming industry doesn’t play a big part in driving the economics of media
and music production.
Obviously, one of the biggest markets in immersive technology centers around
the home theater and the idea of enjoying high-deﬁnition movies in the comfort
of your home or even private home screening theater (complete with plush
seats, impressive screen and popcorn popping machine). On the audio ﬁle front,
the sky is also the limit with the high-resolution playback systems that often
accompany high-end home theater systems. 24-bit/96 k (and higher) sound
ﬁles can be downloaded of the latest classical or popular music releases. Last,
but hardly least, the latest high-performance/high-resolution gaming consoles
have advanced the art of the immersive experience to new levels. Here, high-
performance computing teams up with multi-channel audio to place sound
effects, dialog and music onto a larger-than-life soundscape. Often, newer
gaming releases involve huge music budgets that make use of full-size orchestral
scores with live instrumentation in top-ﬂight music halls. In the end, it’s all
about the experience.
MIXING IN SURROUND
As you might imagine, designing an analog console that’s designed to include
surround features is almost always a massive and costly undertaking. Although
518

519
Immersive Audio (5.1 and beyond)  CHAPTER 19
these large-scale consoles do exist (and are most often used in the mixing of
feature ﬁlms), it’s far more common these days to see consoles that are digital
in nature (allowing them to be easily recalled and controlled digitally from
within the session). Nowadays, however, it’s even more likely that this beast
will be a large-scale controller surface, which serves to interface with and control
the DAW directly, thereby bypassing all analog circuitry altogether. Of course,
direct control over the dialog, music and special effects of a movie or media
production lets the production team have direct and recallable access to all level,
panning, effects plug-ins and more—all within a single, contained workstation
session. Alternatively, a number of DAW systems offer “in-the-box” software
mixers that can mix, process and export audio in surround in a powerful and
much-less-costly environment (Figure 19.3). These systems are likewise able to
mix, automate, effect and then export a full-surround mix into a ﬁnal format
in a way that’s often a thing of beauty—and in a way that’s often hard to match
in the hardware domain—regardless of price.
FIGURE 19.3
Surround-sound panners
within the Nuendo mixer.
(Courtesy of Steinberg
Media Technologies 
GmbH, a division of 
Yamaha Corporation,
www.steinberg.net)
After having mixed many projects in surround myself and in talking with many
top engineers and producers, I’ve come to realize that the ﬁrst rule of recording—
”There are no rules, only guidelines”—deﬁnitely applies to surround-sound
mixing.
The ways in which sound can be placed in the soundscape are as varied as those
who place them. For example:
n There are those who believe that the sound should be primarily placed in
the front (L/C/R), while the reverb, natural ambient and/or audience
should be placed in the rear.
n Others feel that there literally are no rules, and that sounds can be placed
anywhere that sound good, interesting or just plain “right” for the material.
n Others will place monophonic sounds at pinpoint positions throughout
the ﬁeld, while others will place multiple stereo ﬁelds throughout the
soundscape (i.e., along any of the front, side or corner axes).

Mixing in Surround
520
n Some won’t use the center speaker, believing that it messes with the center
image along the L/R axis in a way that’s reminiscent of the quadraphonic
era. While others use the center to combine with a phantom center to
create new and interesting interactions.
n Some will not use the LFE (sub) channel at all, believing that it adds a
degree of unnaturalness to the low bass that’s better handled by the full-
range speakers. Most will make use of the LFE to add an extra degree of
bottom end to the mix, although care should be made in deciding how
much bottom end is to be added to the .1 channel, as too much can quickly
wreck a mix.
It’s my strong belief that how you choose to convey your music through the
art of immersive sound mixing is up to you and the muses. It’s an extremely
fun medium that should be experimented with, mastered and experienced to
its fullest.
Surround Hardware/Software
Any device that has mixer which offers multiple bus outputs (typically having
eight or more buses) can be used to create a surround-sound mix. The more
important question in this day and age is: How easily can signals be routed,
panned and affected in a surround environment to create a 5.1 mix without
going nuts with frustration? In short, the ability to pan mono or stereo sources
into a surround soundscape and place effects in the 5.1 scape or higher output
format without difﬁculty can make the difference between a difﬁcult,
compromised mix and one that lifts your spirits. The obvious answer to cost-
and power-effective immersive sound production with the greatest results and
minimum of frustration rests strictly “in-the-box” and, as such, allows for awe-
inspiring projects that can be created over a wide range of budgets and
production levels.
SURROUND INTERFACING
Because surround requires that you have at least six output channels (for a 5.1
setup), it logically follows that your audio interface (Figure 19.4) should be
either:
n A dedicated audio interface with at least two inputs and six outs
n A multichannel audio interface (for example, one having at least 8 ins ×
8 outs)
FIGURE 19.4
Steinberg UR-824 24x24
multichannel audio
interface, showing the 8
monitor output jacks for
surround. (Courtesy of
Steinberg Media
Technologies GmbH, 
a division of Yamaha
Corporation,
www.steinberg.net)

Over the years, a number of audio interfaces have come onto the market that
fully support surround sound. These devices allow audio to be easily and cost
effectively routed from your host DAW or application to an external amp or
active monitor system. These devices include a driver that offers extensive
monitor and function controls for routing the sound to external I/O hardware
(such as an 8x8 lightpipe mic preamp), hardware processing devices and a multi-
speaker setup. Care needs to be taken when choosing an interface for immersive
sound, as not all interfaces take into consideration some of the extra (yet often
simple) options that are needed for surround. Something as simple as not
having a main volume control that can vary the gain of your eight main outputs
(for monitoring) might mean that you’ll need an external monitor controller
(something that could unnecessarily cost you a great deal of money)—and the
list goes on. It’s deﬁnitely worth it to take your time and consider any needs
that might be special to your production setup.
SURROUND CHANNEL ASSIGNMENTS
When creating an immersive multichannel mix for ﬁlm, music, gaming, etc.,
it’s important to know what channel output conﬁguration that should be used
(either as a default that’s set within your DAW or as a track conﬁguration that’s
required by outside clients. For example, a surround conﬁguration that’s used
by Steinberg and other companies has a default channel layout of L/R/C/LFE/
Ls/Rs (Left/Right/Center/LFE/Left Surround/Right Surround), while Pro Tools
defaults to a L/C/R/Ls/Rs/LFE layout.
This conﬁguration isn’t totally critical, however, it’s important to be aware of
its existence and that you lay your production room out according to your chosen
layout. What’s more important is that your ﬁnal exported mixdown tracks be
properly named with the track conﬁguration being clearly identiﬁed within the
ﬁlename (i.e., Directory/Songtitle_Left.wav, Directory/Songtitle_Right.wav,
Directory/Songtitle_Center.wav, etc.). Most surround-capable DAWs are capable
of automatically adding the track sufﬁces to the track names during the ﬁnal
export.
MONITORING IN 5.1 AND BEYOND
With the recent proliferation of surround-sound speaker options for both the
home and studio, options exist at all levels of quality, functionality and cost
effectiveness for installing a surround and immersive system into a studio
monitoring environment. As with most new technologies, it’s important that
your existing facility be taken into account, so as to maximize control over mon -
itor levels and monitor format choices (discrete surround, stereo and mono),
as well as its integration with your current console and/or DAW system. Before
choosing a 5.1 or higher speaker system/setup, it would be extremely wise to
consider the following:
521
Immersive Audio (5.1 and beyond)  CHAPTER 19

Monitoring in 5.1 and Beyond
n What are the commercial advantages to producing audio in surround? Are
there any new clients or business ventures that allow you to make use of
this technology?
n What is the budget for such a system?
n Can your existing speakers be integrated into the surround system? Many
powered monitor systems can be upgraded to surround by adding
matching or similar speakers from the same product line.
n Can your console produce audio in surround sound? If your console has
six or more output buses (8 bus +), your system can output surround in
some manner; however, true surround panning and surround monitor
control are often difﬁcult to pull off when using such systems.
n Can your DAW produce audio in surround sound? As above, certain DAWs
are capable of routing audio to multiple output buses (5.1, 7.1 or higher);
however, most surround-capable workstations are able to integrate tracks,
effects and track exporting into a ﬁnal immersive format with an amazing
degree of versatility.
n How do I plan to monitor in surround? If the console, interface or DAW
offers true surround monitor capabilities, you’re in luck. If not, a hardware
surround monitor control system or a surround preamp might be necessary
(although you could also route the DAW outputs through a console and
control all of the monitor levels together in a ganged fashion).
n What types of surround mastering tools should I invest in? Creating a
surround-sound mix is only part of the battle. Often the real challenge
comes in the mastering of the six tracks into a ﬁnal format that can be
played on a commercial playback system—when it comes to the intricacies
of surround mastering of the various formats (dealt with later in this
chapter), knowledge, attention to detail and patience are of paramount
importance.
Once these and other considerations have been taken into account and you’re
ready to make the jump, the task of choosing and installing a surround system
into the production control room can get underway. This can be a daunting
task, requiring technical expertise and acoustical knowledge, or it can be a
straightforward undertaking that requires only basic placement, system setup
and a big dose of techno-artistic patience.
5.1 Speaker Placement and Setup
When trying to ﬁrst understand and work with immersive systems, it’s always
a good idea to begin at a basic starting point, in this case, let’s start with the
traditional 5.1 surround sound setup.
As deﬁned by the International Telecommunications Union (ITU), the “ofﬁcial”
5.1 speaker setup is made up of ﬁve full-range monitors that are positioned in
a circular arc, with the speakers being placed at equal distances to the listener
(at the center position). Three of the speakers are placed to the front, with the
522

523
Immersive Audio (5.1 and beyond)  CHAPTER 19
center speaker being placed dead center (0º) and the left/right speakers being
placed at 30º arcs to the center point. The surround speakers are then placed
behind the listener at 110º arcs to the center point (Figure 19.5).
FIGURE 19.5
The ITU 5.1 speaker setup
diagram (left) and PMC
TB2+ 5.1 monitor setup
(right). (Courtesy of PMC
Limited, www.pmcspeakers.
com)
THE LFE
Within an immersive system, the LFE channel stands for low frequency effect and
was developed by the ﬁlm industry to give an extra “ummmffff” to the low-
bass end, and add an extra degree of impact, especially during explosions and
low rumbles. Contrary to popular belief, the “.1” aspect of an LFE has little or
nothing to do with the musical bass line of a music program or ﬁlm score.
Instead, it was originally designed to augment full-range monitors as a separate,
band-limited channel (120 Hz for Dolby Digital or 80 Hz for DTS) for adding
10 dB of headroom to the extreme low-end of a ﬁlm. (I recall that it was ﬁrst
used in the ﬁlm Earthquake to add extra realism to the “earth-shaking” score.)
The LFE’s placement and setup within the room should also be taken seriously,
as placing it in a corner or in a position that’s affected by a harmonic node in
the room could greatly affect its response. If possible, the sub should be placed
on the ﬂoor, near the front plane of the room, at a reasonable distance from
the front wall (to prevent excessive bass buildup). Finally, most active subs will
offer full control over gain and crossover frequency, allowing the user to best
match the driver’s bass response to the room and main speaker set.
It should be noted that the 110º arc in the surround
ﬁeld is a debatable point. During a special playback
event, a rather noteworthy group of surround
producers and engineers (including myself)
compared a 110º arc with a wider 130º arc. We
decided on the latter, with popular opinion being
that the head and ears provided less of a barrier to
the wide-angle sound. Having the tweeters at the
outer edge sides will also help. You might want to
perform a DIY and check it out for yourself.

Monitoring in 5.1 and Beyond
524
BASS MANAGEMENT IN A SURROUND SYSTEM
At ﬁrst glance, bass management might look like an LFE channel or a simple
subwoofer system (which is a good reason why bottom-end issues can quickly
get so confusing). In reality, this is not the case at all. A bass management system
(which is used in most home theatre systems and many high-end auto systems)
use ﬁlters to extract low-frequency information from the main channels and
then route this bass to the sub speaker, while the highs are sent to the system’s
tweeters. In short, this method has the advantage of allowing for one bass speaker
and multiple, easier-to-build (stereo or surround) tweeters that are small and
can be placed in conﬁned spaces.
Although these systems are in wide use in home playback and theater systems,
it’s widely held that these speaker systems aren’t suitable for studio monitoring
due to their irregularities in midrange response, poor image localization and
limited sound quality.
PRACTICAL 5.1 PLACEMENT
From a practical standpoint, I have often found 5.1 surround speaker placement
to be a bit more forgiving than the above “spec” suggests. For example, it’s
sometimes not practical to place the three front speakers in an equidistant arc
on most consoles or DAW desks because there simply isn’t room. Usually, this
means placing the speakers in a straight line or smaller arc (while angling the
speakers for the best overall sound ﬁeld coverage).
Placing three matched speakers on the front bridge of a console, on ﬂoor/ceiling
mounts or ﬂush in a sofﬁt generally isn’t difﬁcult. However, I found that placing
the center speaker on a DAW desk is sometimes a challenge, simply because
the computer monitor (or monitors) is commonly placed at this center position.
If matched speakers are used, where can the center speaker be placed? You might
be able to place the center speaker on its side, or you can sometimes get a
There’s also an ongoing debate among top music
mixers as to whether to use the LFE channel at all
within music mixing. Almost everyone agrees that all
of the speakers within a music mixing scenario
should be full range however, there are those who
don’t believe in using the LFE channel at all, while
others will put some degree of low content in this
channel (stating that customers feel cheated out of
their well-earned .1 when they don’t get some
bottom in that channel). Personally, I fall in the camp
of putting a light-to-moderate degree of low-end
content in the LFE channel (while being very careful
that all calibrations are correct). It’s no fun to work
hard on a surround project, only to have the bass
levels playback at too high a level, muddying up
your carefully crafted mix. When in doubt, it’s always
a good idea to calibrate your surround system
according to the Grammy P&E surround setup
guidelines from www.grammy.com/recording_
academy/producers_and_engineers/guidelines. On a
ﬁnal note: Critical musical material should never be
sent to the LFE channel, because Dolby decoders
will completely drop it when folding 5.1 material
down to stereo.

dedicated, low proﬁle center speaker that allows the computer monitor to be
placed on a shelf that can hold the monitor(s). In my room, I actually hang
the speakers from the ceiling, allowing the center speaker to “ﬂoat” above the
monitors.
In certain cases where space, budget and time are a consideration, it might not
be possible to exactly match the rear speakers to those of the front three (which
should be matched). In such cases, use your best judgment to match the general
characteristics of the speaker sets. For example, if a company makes a speaker
series that comes in various sizes, you might try putting a pair of their smaller
monitors in the rear. If this isn’t an option, intuition and ingenuity are your
next best friends.
5.0 SURROUND MINUS AN LFE
Simply put, 5.0 is 5.1 without an LFE big-bang-boom channel. Since LFE is
band-limited to the extreme bass frequencies and is intended to provide only
an added EFFECT!, many surround professionals argue that it’s unnecessary to
add the LFE channel to a surround music project at all. Others, on the other
hand, argue that consumers expect and deserve to get their .1’s worth. As with
much of audio production, surround is an open frontier that’s totally open to
personal interpretation.
7.1 SPEAKER PLACEMENT
Quite simply, 7.1 is carried out by adding two side channels (180º left and
right) to the ofﬁcial 5.1 ITU speaker setup. The idea of this format is to add
more deﬁnition, movement and localization to the surround experience by
giving you sound that comes directly from the left and right.
9.1 AND 11.1 HEIGHT CHANNELS—ADDING TO THE EXPERIENCE
In recent years, several systems have cropped up that add height channels to
an already existing immersive system. Having applications for ﬁlm, game and
music, this type of playback makes it possible for jets to ﬂy literally over your
head or reverb and added ambience can ﬂoat above your head in a mix, adding
a dimension to the sound that truly has to be heard to be believed.
Although these immersive audio media systems are primarily in use in a growing
number of major-release theater houses around the world, these systems are
also beginning to make their way into the home and home theater environment
as well. Currently, there are two systems that add height channels to an existing
surround soundscape: Dolby Atmos® and Auro Technologies Auro3D®.
Dolby Atmos
Dolby Atmos from Dolby Labs is an object-based immersive audio format that
can be scaled to playback in a wide range of theater or home theater systems.
Essentially, a movie or audio mix uses metadata (channel-based information
525
Immersive Audio (5.1 and beyond)  CHAPTER 19
Dolby Atmos 

Monitoring in 5.1 and Beyond
526
that’s imbedded within the maximum number of 128 individual audio data
streams) to “steer” audio from one speaker/channel to the next within the
room. The format is designed to encode the mix data into a single audio stream,
such that the data can be directed and panned between up to 64 speakers (front,
side, rear, height) or the same mix data stream can be scaled down to move
the data over an 11.1, 9.1, 7.1, 5.1 or even a 2.0 soundscape.
Auro Technologies Auro3D
Auro3D from Auro Technologies (Figure 19.6) also uses a single codec data
stream to deliver front, side, rear and height channels to the big screen, home
theater and high-end sound system. Unlike Atmos, the sound is not steered,
but is instead mixed in 11.1, 9.1, 7.1, 5.1 or 2.0 soundscapes in a way that can
be either up- or down-mixed between the various formats, allowing multiple
mix versions to be imbedded within a single data stream.
FIGURE 19.6
Auro-3D mixing theater
showing height channels.
(Courtesy of Auro
Technologies, www.auro-
3d.com)
Active/Passive Monitors in Surround
As you might recall from Chapter 13, active monitors include a powered
ampliﬁer(s) within their design, whereas passive monitors require that an
external power ampliﬁer be used to power their drivers.
One of the beneﬁts of designing your system around a powered speaker setup
is its ability to upgrade a stereo system to an immersive environment by simply
adding extra monitors to a stereo production system.
With regard to the LFE channel, almost all surround sub speakers are actively
self-powered. Although many of these systems provide active crossover outputs
for diverting the low-frequency energy to the sub speakers, while sending the
highs out to all ﬁve speakers (or more likely in a pro system, only the front
stereo channels)—extreme care should be taken when considering the use of
this crossover network. Using the crossover ﬁlter networks in a sub can undercut
the low-frequency reproduction capabilities of your main speakers and send
the lows to the sub, where the bass can be much less controlled and might add
Auro Technologies Auro3D 

a very different type of low-end sound to your mix. Most professional mix setups
route the LFE channel directly to the .1 bass drivers in the sub that sits on your
ﬂoor. In short, read up on the subject of LFE, bass management and sub
crossover system setups and use your ears. Few things can mess with your mix
more than an improperly setup sub/LFE system.
Noise Calibration
As is the case with any speaker setup in a professional monitoring environment,
it’s very important that the speakers within a stereo, surround or immersive
speaker setup be adjusted, so that the acoustic output of each speaker is at the
same level at the listening position. This level matching can be done in several
ways. Although certain surround amp/receiver systems are able to output pink
noise (random noise that has equal energy over the audible range), it’s far more
common to:
n Record a minute or so worth of pink noise, and then to copy this same
ﬁle into your workstation onto as many channel tracks as you have speakers
in your system.
n Place a sound level meter on a camera stand directly at the listener’s
position (you could use an SPL meter app on your phone or pad, or an
inexpensive sound level meter that is bought online will also do the trick—
although it’s always nice when the meter has a thread for mounting the
unit directly onto a tripod). Point the meter at the ﬁrst speaker to be tested
(Figure 19.7). This will usually be the left speaker.
n Play the pink noise through one speaker at a time (with each track being
assigned to its associated speaker) at a level of about 80–85 dB SPL and
adjust the sensitivity and gain on both the meter and master monitor gain,
so that the meter reads at “0” on an analog meter or at a speciﬁed level
on a digital readout.
n Swivel the meter, so that it points to the next speaker being tested. Now,
solo that speaker/track and set the gain on that speaker to match to that
same level. Then, move around the entire setup matching levels until all
of the speakers are at equal level at the listening position.
527
Immersive Audio (5.1 and beyond)  CHAPTER 19
FIGURE 19.7
Speaker noise calibration
tests are used to make sure
that your speakers in a
stereo, 5.1 or immersive
system will play back at
their proper, balanced
levels.

Monitoring in 5.1 and Beyond
n You’re now ready to start mixing or listening, knowing that your system
is properly level matched.
n I almost forgot to mention to “switch the meter off”—I can’t tell you how
many times I’ve drained the battery this way.
It should be noted that setting LFE levels properly is a bit more complicated
(most set the LFE at +4 DB SPL relative to the other speaker levels). However,
if you’re serious about setting up a system properly, I’d strongly recommend
that you consult the surround technical setup guidelines from the Producers
and Engineers Wing of NARAS (the Grammy folks). It’s well worth downloading
this and other freely distributed guidelines from www.grammy.com/recording_
academy/producers_and_engineers/guidelines.
Desert Island Reference Files/Discs
Remember that trusty high-quality desert island compilation disc or music
directory that I’ve been talking about over the course of the book? Well, those
who are serious about immersive sound might consider putting together a series
of favorite discs or ﬁles that can be played as a reference for your and other
surround rooms. As always, you’ll almost certainly be amazed at the difference
in sound and mixing styles that exist between the discs—it simply points out
(in a way that exists in any art) there are no rules.
SURROUND FINAL MASTERING AND DELIVERY 
FORMATS
One of the underlying fallacies of immersive audio production is that you can
take a console (or even a DAW for that matter) with at least six output buses,
connect 5.1 speakers to their outputs, and then believe that you’re ready to fully
produce and distribute audio in surround. In fact, there are many hardware and
software considerations that must ﬁrst be taken into account, including those
that relate to the mastering of these channels into a ﬁnal format that can be
played by both the average and the not-so-average consumer.
This process often involves the conversion of a set of master recorded tracks or
DAW session into a ﬁnal format that can be distributed and reproduced using
existing surround technology (such as on a DVD or Blu-ray disc). Several of the
production formats that are in current use include:
Uncompressed PCM Audio
Most audio media support uncompressed PCM (Pulse Code Modulation) .wav
or .aif audio at sample rates from 44.1kHz to 96kHz at 16 or 24 bit depths.
Although these are supported by the DVD speciﬁcation, they can be more
commonly found on Blu-ray discs.
528

Dolby Digital (AC3)
Dolby Digital (commonly known as AC3) is a popular codec that’s used to encode
digital audio into a multichannel (mono through 5.1). Because this code uses
perceptual coding techniques to eliminate unused audio that’s masked or
imperceptible (in a way that works much like the MP3 codec), the amount of
data that’s encoded within the bit stream can be greatly reduced (often by a
factor of 10), when compared to its uncompressed counterpart.
Dolby Digital is used in the encoding of surround audio on the DVD, Blu-ray
discs, HDTV television production, digital cable and satellite transmissions. The
traditional ﬂavor of Dolby Digital provides for ﬁve full-bandwidth channels in
the L/R/C/LFE/Ls/Rs format, while the LFE channel provides a low-pass channel
(usually cutting off frequencies above 120 Hz). The bit stream is also capable
of down-mixing the surround channel information down to 2.0 to ensure
compatibility with any mono or stereo playback device.
DTS
Digital Theater System (DTS) is an audio format that encodes up to 7.1 channels
of discrete audio into a single data stream for use in cinema sound, DVD/home
theater and multimedia. It’s used to encode audio data at rates of 1.5 Mbit/sec
or 754 kbit/sec (compared to the basic rate of 448 kbit/sec, or the 384 kbit/sec
rate of Dolby Digital). Although far fewer titles have been released in DTS than
in its Dolby Digital or Atmos counterpart, the new 754 kbit/sec rate has allowed
a number of movie production studios to offer soundtracks in both Dolby Digital
5.1 and DTS 5.1 formats.
MP4
MPEG-4 (commonly known as MP4) is essentially based on Apple’s QuickTime
MOV format, and is capable of containing audio, video and subtitle data streams
in various bit stream formats including surround sound. This popular format
allows for the distribution of audio and video content over download and other
data transmission media in a way that can be protected using an optional digital
rights management scheme.
Initially, MPEG-4 was aimed at being a low-bit-rate video medium; however,
its scope was later expanded to be efﬁcient across a variety of bit rates ranging
from a few kilobits per second to tens of megabits per second.
FLAC
FLAC (Free Lossless Audio Codec) is an open-source, royalty-free codec for deliv -
ering lossless audio within a multi-channel bitstream (up to 8 channels). It
supports only ﬁxed-point samples (not ﬂoating-point) and can handle any PCM
bit resolution from 4 to 32 bits per sample at any rate between 1 Hz and 655,350
Hz in 1 Hz increments. The channel count can scale from 1 to 8 (which can be
529
Immersive Audio (5.1 and beyond)  CHAPTER 19

Surrounding Final Mastering and Delivery Formats
grouped together for stereo, 5.1 and 7.1 surround playback). In addition, the
FLAC format does not include any copy prevention features (DRM) of any kind.
As FLAC is a lossless scheme, it’s suitable as an archive format for preserving
their audio collections. If the original media is lost, damaged, or worn out, a
FLAC copy of the audio tracks ensures that an exact duplicate of the original
data can be recovered at any time. This codec is now included in an increasing
number of soft- and hardware media players (including newer Android phones).
Down-Mix/Up-Mix
There are times when movie, sound design and music creators will want to move
between various stereo and immersive surround formats (most notably 5.1). 
As an example, you might want to playback a movie that was encoded in 5.1 on
your laptop using its built-in stereo playback jack. In this case, the mix will need
to be automatically “folded down.”
DOWN-MIX FROM 5.1
The process of folding down audio from a higher-count immersive track to stereo
can be a tricky venture and one that should be carefully considered in the
production of a visual- or music-based project. As an example, the general
default settings for decoding an AC3 (Dolby Digital) track down to stereo is
usually as follows:
n The L and R channels are respectively sent to the L and R tracks
n The Center track is sent to the L and R tracks, with a –3 dB gain reduction
n The Ls and Rs tracks are also sent to the L and R tracks, also with a –3 dB
gain reduction
n LFE is often ignored
When the L, R, C, Ls and Rs tracks are summed together to a single stereo mix
version, there is a big chance that the resulting levels might distort, however,
the decoder is able to automatically control the dynamic range to reduce or
eliminate these possible distortions. In addition, it’s often possible to program
changes into the down-mix metadata that would allow for changes to the above
channel mix levels, if it best suits the program material.
No discussion about down-mixing should be tackled without weighing in on
the decision of whether to simply fold-down the original surround mix, or to
go back to the remix stage and create a separate stereo track that can be placed
into the 2.0 audio stream. It’s a simple fact that there will be a difference in
the ﬁnal sound between a fold-down and a dedicated stereo mix. If the time
and budget exists, it’s usually worth the effort to consider a separate mix.
UP-MIX TO 5.1
Another tool that’s available to us in immersive technology, is the ability to
make use of processing and spatial processing to “up-mix” an existing stereo
530

531
Immersive Audio (5.1 and beyond)  CHAPTER 19
sound ﬁle into a fully discrete set of surround sound ﬁles. With such a tool
(Figure 19.8), it’s possible to take an existing stereo sound ﬁle and (using
various width, timbre, spaciousness and other parameters) split the image into
an immersive soundscape that can (under the best of conditions) be surprisingly
full and convincing for use in ﬁlm, television or music.
AUTHORING FOR IMMERSIVE
Authoring software (for gathering and encoding media that can be encoded
onto an optical disc for personal or distribution use) is, of course, available for
laying out, editing and burning both DVD and Blu-ray titles. These programs
allow edited video and audio to be placed into the media timeline onto multiple
media data streams (main video track, main audio, alternate language tracks,
etc.) with additional options for subtitling, additional material tracks—the
options are numerous.
These programs can range in difﬁculty from being entry-level home media
creation, all the way to high-end professional software for mass-media distribu -
tion. Obviously, it’s important that you take your time and research the various
options that are available in the programs that you’re interested in. Do they
fully support 5.1 or higher surround formats? Is the program easy to operate?
Research, checking out demos (if available) and reading over the manual and
reviews before buying will often save you money and frustration.
REISSUING BACK CATALOG MATERIAL
On a ﬁnal note, one of the unintended by-products of surround is that many
older, classic video and music projects are being reissued with restored video
and surround sound by the media companies. This resurrection, restoration and
the rescue is helping us to save older analog (and digital) masters that are aging
to the point that restoration becomes a monumental task and often a labor of
love.
Regarding efforts to transfer analog music tracks to 24/96 digital archive sound
ﬁles—it’s not uncommon to hear horror stories of 16- and 24-track masters that
FIGURE 19.8
TC Electronic UnWrap 
TDM or System 6000 plug-
in for Stereo to 5.1 Up- and
Down-mixing. (Courtesy 
of TC Electronic A/S,
www.tcelectronic.com)

have to be reconditioned and carefully baked, then played onto an analog
machine, only for the iron oxide to shed off the tape and separate from its
backing as it plays off the reel and onto the ﬂoor during the actual transfer to
DAW. Literally, you would only get one chance to save the master.
Rescue stories like these are varied and awesome (in the truest sense of the
word). Sometimes the ﬁlm and audio masters have already deteriorated past
the point of play-ability and a safety backup must be used. Others must be
tracked down in order to ﬁnd that “right take” that wasn’t properly documented.
Sufﬁce it to say that media re-issues are deﬁnitely doing their part toward
helping to keep the original masters alive and kickin’ into the twenty-ﬁrst
century, not to mention the fact that they are breathing new life into the visual
and music media in the form of a killer, new surround-sound version with
restored sound and possibly picture.
In a nutshell, listening to a well-crafted immersive project over killer speakers
and video monitors in the studio, home playback or a home theater system can
rank way up there with chocolate, motorcycle ridin’ and sex—absolutely the
best!
532
Reissuing Back Catalog Material

The mastering process is an art form that uses specialized, high-quality audio
gear in conjunction with one or more sets of critical ears to help the artist,
producer and/or record label attain a particular sound and feel before the
recording is made into a ﬁnished manufactured product. Working with such
tools, a mastering engineer or experienced user can go about the task of shaping
and arranging the various cuts of a project into a ﬁnal form that can be replicated
into a salable product (Figures 20.1 and 20.2).
In past decades, when vinyl records ruled the airwaves and spun on everyone’s
sound system, the art of transferring high-quality sound from a master tape to
FIGURE 20.1
Darcy Proper in her
mastering room at
Wisseloord Studios,
Hilversum, The Netherlands.
(Courtesy of Wisseloord
Studios, www.wisseloord.nl,
acoustics/photo by Jochen
Veith, www.jv-acoustics.de)
FIGURE 20.2
Mandy Parnell doing her
thing at the mastering desk.
(Courtesy of Black Saloon
Studios, London, UK,
www.blacksaloonstudios.
com)
533
CHAPTER 20
Mastering

Mastering
a vinyl record was as much of (if not more than) a carefully guarded art form
as it was technology. Because this ﬁeld of expertise was (and still is) well beyond
the abilities and affordability of most engineers and producers, the ﬁeld of vinyl
mastering was left to a very select magical few.
The art of preparing sound for transfer to disc, optical media and downloadable
media is also still very much an art form that’s often best left to those who are
familiar with the tools and trade of getting the best sound out of a project.
However, recent advances in computer and effects processing technology have
also made it much easier for producers, engineers and musicians to own high-
quality hardware and software tools that are speciﬁcally crafted to help create
a professional-sounding ﬁnal product in the studio or on a desk/laptop
computer.
THE FINAL MIX
Of course, the process of making a truly professional mix is one that comes
with time and experience. Once you’ve gotten to know your system and your
own personal mixing style, you’ll eventually arrive at a “sound” that’s all yours.
So, now you’ve put in the time on your latest project and you’ve done your
absolute best, hopefully, you’ll be able to sit back and say, “Yeah, that’s it!”
Once you’ve ﬁnished, you might want to:
n Listen to your mixes on various other systems (hopefully, ones you can
trust), just to hear how it sounds
n You might listen to a track next to some of your favorite “desert island”
mixes
n You might also want to get the opinions of people you know and trust
In addition to the concept of capturing the pure music, feel and artistry of a
production, one of the primary goals during the course of recording a project
is the overriding concept that the ﬁnal product should have a certain “sound.”
This sound might be “clean,” “punchy,” “gutsy” or any other sonic adjective
that you, the artist, the producer or label might be striving for. If the ﬁnal
master(s) don’t have a sound that makes everyone happy, you, the band and/or
the producer will be faced with the following:
1. You might say “Well, that’s the best we can do.” Of course, this is always
an option, however, before you sign off on the project; ask yourself if it
will be something that you and the team will be proud of once it’s out
to the public.
2. If you need to keep working (and time permits), you might take a bit of
a break, get some ideas, opinions and a fresh perspective; and then put
your nose to the grindstone until you and your team feel that it’s the best
that it can be—or at least know you’ve given it you best. All of this brings
up an important point. . . .
534

Getting Too Close to the Mix
With all of the above tools and techniques, combined with the time that it takes
to see a good recording and mix through to the end, there is one danger that
lurks in the minds of all musicians (more than anybody, if it’s your own project),
producers and engineers—the fact that we might simply be too close to the mix.
I speak from experience when I say that by the time I’ve neared the ﬁnish of a
project and have made so many careful adjustments to get that right production
and sound, there are deﬁnitely times when I have to throw my hands up and
say “Is it better? I’m so close to the mix and music that I simply can’t tell!”
When this happens to you, don’t fret (too much) it’s all part of being in the
production club. You might simply need a break, some time on a sailboat or
just to get away from it, or you might have to just plow through to get it done
on time. Just realize that this is totally normal.
Once you’ve signed off on the project and have done your best work, you will
be faced with one last worthwhile question (before the actual hard work of
distribution and sales begins):
1. Do I/we put it out as it is?
2. Do I/we hire a mastering engineer to master the project?
3. Do I/we master it ourselves?
HIRING A PROFESSIONAL MASTERING ENGINEER
In order to answer the question as to whether you need a mastering engineer,
it’s always best to understand exactly what a mastering engineer does (or
potentially can do) for your project. In essence, the process of successfully
mastering a project can:
n Help a Project to Sound “Right”
As was mentioned before, a ﬁnal product should have a certain “sound.” That
sound could be completely transparent (as might be sought after in a clean jazz
or faithfully captured classical recording), or it could be a rough and raunchy
as possible, so as to grab the latest elektro-garage-steampunk band’s audience
by their sonic throats. It’s all about the character and attitude of the sound. In
mastering, this is often accomplished through the use of careful level matching,
EQ matching and dynamics processing. As was also previously mentioned, this
process not only takes the right set of processing gear, but also requires
experienced ears who are working in a familiar environment that intuitively
knows how the project will most likely sound under a wide range of playback
conditions.
Just how much a mastering engineer might be involved in shaping the actual
sound can vary, depending upon their level of involvement. The mastering
engineer can simply afﬁrm that the musicians/producer/engineer did a great
job (i.e., “I hardly touched a thing—the original master was practically perfect!”)
535
Mastering  CHAPTER 20

Hiring a Professional Mastering Engineer
or it can require moderate to heavy processing in order to tame the sound into
being what everyone wants. In short, if the ﬁnal mix or mixes aren’t up to snuff,
having an experienced mastering engineer help you to “shape” the project’s sonic
character through the careful use of level balancing, dynamics and EQ could
help save your sonic Technicolor day.
n Help Match the Levels and Overall Character within a Project
The basic truth is that it takes years for a mix engineer to learn the ﬁne art of
blending the songs within a mix, so that they seamlessly combine together into
a project that has a cohesive feel (if that’s what’s called for in the mix). A less
experienced engineer, mixer and/or producer might come up with an albums
worth of songs that abruptly changes in character and “feel” over the course of
the project.
Another factor that can affect a project’s sound is the reality that recordings may
have been recorded and/or mixed in several studios, living rooms, bedrooms
and/or basements over the course of several months or years. This could mean
that the cuts could actually sound very different from each other.
In situations like these, where a uniﬁed, smooth sound might be hard to attain,
it’s even more important that someone who’s experienced at the art of mastering
be sought out. This might involve the careful balancing of levels and dynamics
from song to song, but it will almost certainly involve the careful matching of
EQ to smooth out the sonic imbalances between the cuts, so they don’t stick
out like a sore thumb.
n Help With the Concept of Song Ordering
Have you ever noticed that some of your favorite desert island recordings just
seen to have a natural ﬂow to them? An album that takes you on a speciﬁc
journey is called a “Concept Album,” and one of the signature callings of such
an album is that one song will follow another in a way that just feels right.
This idea of choosing a project’s song order is an art form that’s best done by
the artist and/or producer to convey the overall “feel” of a project, however, if
the production team doesn’t have a good feel for how the album should “ﬂow”
(a very sad thing, indeed), they might seek the mastering engineer’s opinion.
n Help With the Concept of Song Timing
Getting back to our favorite desert island album, have you ever noticed how
one song just seems to ﬂow into another, or that an abrupt cut—followed by
a fast pickup of the next song just seems to ﬁt perfectly with the music? This
was almost certainly no accident. The concept of song timing is an intuitive
process of setting the gap or crossfade times between songs. It can make the
difference between a project that has an awkward pause with one that “ﬂows”
smoothly from one cut to the next. Just remember that automatically setting
the “gaps” to their default 2-second settings (when burning a CD) might not
be in the project’s best interest.
536

Again, if the band/producer/engineer doesn’t have a good sense of how this
timing should go (I’m going to cut them a break here, as it’s actually an art
that’s not that easy), they might consult with the mastering engineer (or at least
make him/her aware that you would like to carefully address the timing issue).
n Help with Overall Levels
The subject of how high to set the levels (or more accurately, relative levels—
dynamics—compression/limiting) is a subject that can (and has) been the
subject of many a heated debate. The major question here being—“Can it ever
be too loud?” In actuality, the question that we’re really asking is, “How much
dynamics can we take out of the music before it becomes too much to bear?”
Traditionally, the industry as a whole tends to set the average level of a project
at the highest possible value. This is often due to the fact that record companies
will always want their music to “stand out” above the rest when played on the
TV, radio, mobile phone or on the Web. This is usually accomplished by
applying compression to the track or overall project. Again, this is an artistic
technique that often requires experience to handle appropriately.
I’m going to stay clear of opinions here, as each project will have its own special
needs and “desires,” but several years ago, the industry became aware of a top-
selling album that was released with virtually no dynamic range at all. All of
the peaks and dynamics were lost and it was pretty much a “ﬂat-wall” of sound.
This brought to light that maybe, just maybe, there might be limits to how loud
a project should be. Of course, everyone wants to be heard in the playlist in an
elevator, on the phone, etc. In reality, some degree of dynamics control is often
necessary to keep your mix sounding present and punchy, however a good
mastering engineer can help you be aware that over-compression can lead to
audible artifacts that can “squash” the life out of your hard-earned sound. In
fact, light to moderate compression or limiting might also be an alternative for
certain types of music. Alternatively, classical music lovers often spend big bucks
to hear a project’s full dynamic range. In the end, it all depends on the content
and the message.
TO MASTER OR NOT TO MASTER—WAS THAT 
THE QUESTION?
As was mentioned, the process of mastering often requires specialized technical
skills, audiophile equipment, a carefully tuned listening environment and
talented, experienced ears in order to pull a sonic rabbit out of a problematic
hat or even one that could simply use some simple dressing up.
A few years back, I had the good fortune of sitting around a big table at a top
LA restaurant with some of the best mastering engineers in the US and UK. As
you might expect, the general consensus was that it’s never a good idea for
artists to master their own project—that artists and even producers are just too
close to the project to be objective. Although these arguments make a strong
537
Mastering  CHAPTER 20

To Master or Not to Master?
case for extreme caution, I’m not sure I agree that this is “always” the case.
However, when approaching the question of whether to master a project yourself
or to have a project professionally mastered, it’s important that you objectively
consider the following questions:
n Are you objective enough to have a critical ear for the sound and general
requirements of the project, or are you just too close to it emotionally,
musically and technically (realizing that the “sound” of a particular project
might follow you throughout your career)?
n Is your equipment and/or listening environment adequate for the task?
n Is the ﬁnal mix adequate (or more than adequate) for its intended purpose,
or would the project beneﬁt most from an outside set of professional ears?
n Does the budget allow for the project to be professionally mastered?
After the above questions have been carefully weighted and answered, if the
services of a professional mastering engineer are sought, the next question to
ask is “Who’ll do the mastering?” In this instance, it’s important that you take
a long hard look at the experience level and musical/technical tastes of the person
who will be doing the job and make it your job to familiarize yourself with
that person’s work and personal style. In fact, it’s probably best to borrow from
the traditional business practice of ﬁnding three of the most appropriate
mastering house/engineer facilities and following due diligence in making your
decision by considering the following:
n Are you familiar with examples of the mastering engineer’s work? If not,
you should deﬁnitely ask for a client list and recent examples of their work,
then, have a critical listening session with the producer and/or band
members. In fact, one of the better ways to search out the best mastering
engineer for the job is to start by seeing who mastered some of your favorite
projects.
n Are they familiar with your music genre as well as the type of “sound”
that you’re hoping to achieve?
n What are their hourly or project rates? Do they ﬁt your budget?
n Are they willing to do a complementary test mastering session on one of
your cuts, or at least discuss how they might approach the process, based
upon a track that you’ve sent them?
Bottom line: Beware of the inexperienced or inappropriate mastering engineer
(especially if that person is you).
“Pre”-paration
Just as one of the best ways to make the recording and mixdown process go
more smoothly is to ﬁx most of your technical problems and issues before you
sit down at the console. Likewise, one of the best ways to ensure that a mastering
session has as few problems as possible is to ask the right questions and deal
with all the technical issues before the mastering engineer even receives your
538

539
Mastering  CHAPTER 20
sound ﬁles. By far, the best way to avoid problems during this phase is to ask
questions ahead of time.
The mastering engineer should be willing to sit down with you or your team
to discuss your needs and pre-planning issues (or at least direct you to a
document checklist that can help you through the process). During this
introductory “getting to know you and your technical requirements” session,
here are just a few sample questions that you might ask:
n What should the ﬁnal master sample rate and bit rate be (usually the
highest sample/bitrate that the session was actually produced at)?
n What should the master’s maximum level be (often –7 to –12 db of full
scale)?
n Should all master compression be turned off on the mix bus (usually yes)?
Would you like for us to supply you a copy with the bus compression
turned on, so you can hear our intended ideas?
n Would you like separate instrument/vocal stem tracks, so they can be
treated separately?
n Are there any special requirements that we should be aware of, or other
issues that we haven’t thought of?
n It’s always wise to consult with the project’s
mastering engineer about the general
speciﬁcations of the ﬁnal product before
beginning the mix-down (or possibly even the
recording) process. For example, that person
might prefer that the ﬁles be recorded and/or
mixed at a certain bit depth and bit rate, as well
as in a speciﬁc format.
n A mastering engineer might prefer that the ﬁnal
master be changed or processed as little as
possible with regard to normalization, fade
changes, overall mix bus dynamic changes
(compression) and applied dither. These
processing functions are best done in the ﬁnal
mastering process by a qualiﬁed engineer.
Important Things to Remember
Providing Stems
A more recent development in how ﬁnal masters are delivered to the mastering
engineer involves the idea of stems. In short, stems are major, musical sub-
groupings of ﬁnal master mix. This might be as simple as providing the mastering
engineer with the music tracks of the song, with a separated vocal track, or, it
might be more complex involving a full drum stem, instrument stem, vocal
stem, etc. The idea behind this method is that it allows the mix engineer to
provide the best individual level, EQ and dynamics processing for the
instruments and vocals separately, and then combine them at the end.

To Master or Not to Master?
There are potential merits to this process, but it’s easy to understand how the
mix engineer and even the producer might have problems with passing control
to the mastering engineer—it begs the question “When does the mix phase
actually end, and who has ﬁnal musical/production control over the mix?”
Providing a Reference Track
As we read, it’s usually best to provide the ﬁnal pre-mastered mix at a reduced
level and with all of the processing effects (EQ, compression, etc.) turned off.
However, you might provide documentation and/or screen shots of these
processing settings to the mastering engineer, so as to give them an idea of what
your intentions are. In addition to this, it’s often a good idea to provide an
additional copy of your ﬁnal mix with everything turned on. This way he or
she can listen to the mastered and processed studio mix to compare the two,
side by side. It can often provide invaluable insights into the artist, producer
and/or engineer’s intentions.
To Be There, Or Not to Be There
One of the next questions that might be asked of the artist or producer is whether
or not they want to sit in on the mastering phase. In the mastering world, this
is not a huge question, simply one that should be addressed. The artist/producer
might feel strongly about sitting in and becoming part of the process, or they
might feel that the mastering engineer was chosen for his or her artistry,
experience and sense of detachment and simply choose to let them do their job
on their own. Another middle approach that is often taken is for the mastering
engineer to master a song and then quickly upload it to the band or producer
before moving on with the project.
No matter which approach is taken, it’s important that the artist and client make
sure that there are several ears around to listen to the ﬁnal mastered project
and listen over several types of systems. And above all, be patient with yourselves,
however be critical of the project’s overall sound and listen to the opinions of
others. Sometimes you get lucky and the mastering process can be quick and
painless; at other times it takes the right gear, keen ears and lots of careful
attention to detail to ﬁnish the job right. In the end, you (artist, producer, label)
are the ones that will have to live with this song or project—strive to keep the
values up during all of the production phases, so it’ll be a project that you can
be proud of.
Sequencing: The Natural Order of Things
Whether the master is to be assembled using a DAW/editor or on analog tape,
the running order in which the songs of a project are played will often affect
the overall ﬂow and tone of a project. The considerations for song order is
inﬁnitely varied, and can only be garnered from experience and having an
artistic “feel” for how their order and interactions will affect the listening ﬁnal
experience. Of course, sequence decisions are probably best made by the artist
540

and/or producer, as they have the best feel for the project. A number of variables
that can directly affect the sequenced order of a project include:
n Total length: How many songs will be included on the disc or album? If
you’ve recorded extra songs, could it include the Bonus Tracks on the disc?
Is it worth adding a weaker song, just to ﬁll up the CD? Will you need to
drop songs for the vinyl release?
n Running order: Which song should start? Which should close? What order
feels best and supports the overall mood and intention of the project?
n Transitions: Altering the transition times between songs can actually make
the difference between an awkward silence that jostles the mood and a
transition that upholds the pace and feel of the project. The Red Book CD
standard calls for 2 seconds of silence as a default setting between tracks.
Although this is necessary before the beginning of the ﬁrst track, it isn’t
at all the law for spacing that falls after that. Most editing programs will
let you alter the index space timings between tracks from 00 seconds (butt
splice) to longer gaps that help to maintain the appropriate mood.
n Cross-fades: In certain situations, the transition from one song to the next
is best served by cross-fading from one track directly to the next. Such a
fade could seamlessly intertwine the two pieces, providing the glue that
can help convey any number of emotional ties.
With the advent of the DAW, the relatively cumbersome process of sequencing
tracks using analog tape has given way to the faster, easier and more ﬂexible
process of editing the ﬁnal masters from hard disk. In this process, all of the
songs can be quickly loaded into a workstation, and then adjusted with respect
to volume, equalization, dynamics, fades and timing (song start and endpoints)
for assembly into a ﬁnal, edited form.
Another way to vary the timing gaps between songs is to do the process manually.
This can be done in the DAW by placing the gap times directly within the song’s
session itself. In this way, the song can be exported (including blank silence at
the end), so that project songs can be imported into any regular CD burning
program or digital playlist without adding any breaks (0 sec) into the CD
burning process.
TO MASTER OR NOT TO MASTER THE PROJECT 
YOURSELF—THAT’S THE NEXT QUESTION!
The question of whether to master the process yourself or to turn your baby
over to another person (highly regarded professional or not) is one that’s
spawning a whole new industry—self-mastering software and plug-ins.
Obviously, there are different production levels for deciding whether or not to
master. Is it a national major release? Is it your ﬁrst self-produced project? Is
there virtually no budget or did the group blow it all on the release party or
worse? These and countless other questions come into play when choosing how
a project is to be mastered.
541
Mastering  CHAPTER 20

To Master or Not to Master the Project Yourself
Having said this, if you decide to master a project (whether it’s your own or
not), there are a few basic concepts that should be understood ﬁrst:
n The basic concept of mastering is to present a song or project in its best
possible sonic “light.” This requires a basic skillset, level of comfortability
and knowledge of your room’s equipment, acoustics and (most of all)
any overall downfalls.
n A basic level of detachment is often required during the mastering process.
This is a lot harder than it sounds, as sound is a very subjective thing. It’s
often easy to fool yourself into thinking that your room, setup and sound
are absolutely top-notch—and then when you go to play it on another
professional system (or any home system)—your hopes can be dashed in
an instant. One of the best ways to guard against this is to listen to your
mix on as many systems as possible and then ask others to critique your
work. How does it sound to them on their system?
n Again, the process of mastering isn’t just about adding EQ and compression
to make your projects sound “AWESOME, DUDE!” Of course, that would
be great, but it’s also about a wide range of artistic decisions that need to
be carefully ﬁnessed in order to master a recording into its ﬁnal, approved
form. Just a few of these decision-making steps include:
– Choosing the proper song order
– The use of level changes to balance the relative track levels and improve
the overall “feel” of the project
– The application of EQ to improve the sound and overall “tone” of the
project
– The judicial use of dynamics to balance out the project’s sound (over
the course of the project) and to increase the project’s overall level
It’s worth saying again, that the biggest obstacle to self-mastering a project
ourselves is ourselves. The problems might lie in our inexperience and under -
standing of the subtleties of EQ, level and dynamics, our subtle “feel” for timing
and sequence order, or any other possible number of things.
Am I calling attention to our possible faults and potential downfalls of making
a good master, because I feel that the process of mastering should be held as
a sacred process, held by a select few? Absolutely not—I do, however, feel that
if you’re aware of the potential pitfalls, you just might be able to follow the
yellow brick road and learn the basic aspects of self-mastering. I know, because
I pretty much did it myself!
The Zen of Self Self-Mastering?
So, after having my projects mastered by several engineers, I actually ended up
feeling that I could do a better (or at least as good) a job, so, I made the
conscience decision to give it a try. After decades, I’ve ﬁnessed my mixing room
into one that I know and trust (as much as anyone can) and have chosen my
542

gear with TLC (I’m particularly happy with my monitor speakers, always a good
place to start), in order to do the best job I can.
The “working with my own music” is always the rough part. It’s tough enough
trying to get the right sound for another band or producer, but when it’s your
own music, the going really gets tough. This is because you’re soooooooooo close
to the music and production that the sounds, the nuances, the “everything”
becomes progressively more difﬁcult to second-guess. This is why I allowed myself
the luxury of learning self-mastering over the course of several years. I knew that
it would not come easily or even overnight—boy, was I right on that one.
One of the ﬁrst steps is to simply be patient with yourself. The process of
understanding the sound that you’re after and then going about getting that
sound might happen sooner than you think, or it might take a lot longer than
you thought. The key is to be patient, to fail, to keep experimenting and
eventually, to understand your own system, sense of art and the sound that
you’re after.
DESERT ISLAND MIXES
The idea of having a personal collection of desert island mixes on hand as a
reference keeps popping up in this book. Of course, this points to how important
it is to have music that you admire and love on hand as a listening reference,
and this also goes for the mastering phase. As you progress, you might compare
your own mixes to a favorite desert island project. It used to be that everyone
would tell you to listen in your car (always a good idea), however, these days
people spend a lot more time listening to music on the system they know best—
their personal phone and headphone/earbud system.
You might listen to one of your fave mixes and then click over to your latest
song mix. How does it compare? Does it make you happy, indifferent or
disappointed? If it’s the latter two, then you know you have more work to do.
After putting those several years of hard, frustrating labor, I can honestly say
that my mixes stand up to many of my favorite mixes—that and 4 Grammy
nominations are things that I can be proud of. If I can do it, I know others can,
it just takes a true dedication, understanding and learning of the craft (just as
it does with anything).
Two-Step or One-Step (Integrated) Mastering Option
One of the concepts of traditional mastering has always felt foreign to me. This
lay in the fact that traditional mastering is (by its very nature) a 2-step process:
mixing and then mastering. The ﬁnal mix is completed by the artist/engineer/
producer and then the masters are sent off to another person to be ﬁnalized in
a separate stage. I know that this is the original beauty of having a professional
who is detached that can take your “baby” and make it even better. To me,
however, the main reason why I started my journey into self-mastering was that
I wanted to integrate the two phases into a ﬁnal DAW project session itself. To
me, this makes so much sense, as it gives us the ability to:
543
Mastering  CHAPTER 20

To Master or Not to Master the Project Yourself
544
n Have complete and total recall over your session. Meaning that you can
go back and make accurate changes to the ﬁnal mix/master version at any
time. This can also be a never-ending pitfall, if you’re not careful.
n Include song timings within the session itself (this is done by deﬁning
your export markings within each song). Whenever each song within the
project is exported, these timings will automatically include the proper
timings. Just drop it into the project—that’s it!
n Most importantly, the ﬁnal mastering chain will be included within each
song, allowing us to add any appropriate amount of ﬁnal master EQ and
compression to each song.
In short, all of the above three options allow us to have complete control and
repeatability (recall) of not only the recording, but also the ﬁnal mastering
phases. However, I’m also a believer that this is not for everybody. If you feel
you’re not qualiﬁed, don’t have the time/interest/equipment to make the
journey, then hiring a pro is your best bet.
Understanding the Signal Chain
Whether you’re a professional mastering engineer or self-master your own
projects, one of the most important concepts to understand in the process of
getting a certain “sound” in the mastering process is the idea of the “signal
chain” (Figure 20.3).
FIGURE 20.3
Example of a possible DAW
mastering signal chain
through the main output
bus.
In short, a “chain” in the mastering process is the chaining together of a series
of hardware and/or software processing stages that combine to shape the ﬁnal
sound of the song or project. It is a very individualized process that will vary
from one mastering artist to another. Indeed, it is often their personal signature
for shaping how a project might sound.
n In a hardware-based system, this would be the chaining together of
hardware EQ devices and dynamics processors in a serial fashion.
n When a separate DAW or mastering software is used as a mastering device,
these processing devices are likewise used in a serial plug-in chain to shape
the ﬁnal sound.
n When used within the actual DAW session, a combination of sound and
dynamics processors can be inserted into the main output bus channel to
shape the ﬁnal mix/production sound.

If you get the idea that each of these ways of working can achieve similar
production results, you would be right. Of course, there will be opinions about
which way works best and about how the ﬁnal results will sound, however, the
concept is the same: the signal chain inserts specialized signal processing devices
into the ﬁnal audio chain to achieve a sonic result.
The above explanation of the mastering signal chain is where all similarities
between one person’s signal chain and another’s stops. In addition, you’ll
remember that I said that mastering takes time. Well, the understandings of the
subtleties (or not so subtle aspects) of how inserting a device and/or processor
into a chain will affect the overall sound often takes time. Additionally, the
understanding of what’s needed to best bring out the best in a piece will often
change from project-to-project. The concept of what it might take for an 
artist to bring his or her songs out into the sonic light will also take time, and
just when you feel that you’ve found your personal chain for your music, you
might very well ﬁnd that that latest and greatest song of yours might call for
something a bit different to make it sound the best it can. You might have
found an awesome signal chain that works for your music, but it’s wise to realize
that it won’t always be a hard and fast rule that your signature chain will always
be the best one for a particular song. As always, your ears, experience and
intuition are the best and most accurate equipment that you have at your
disposal.
MASTERING THE DETAILS OF A PROJECT
As with mixing, mastering is a process of balancing all of the variables that go
into making a project sound as good as it can be. Let’s now take a closer look
into some of the details that can help shape your project into a ﬁnal master
that is ready for distribution, no matter who will be doing the ﬁnal sonic
sculpting. Now, onto the details:
Sound File Volume
While dynamics is a hotly debated topic among mastering and recording
engineers, most agree that it’s never a good idea deliver a sound ﬁle to a master -
ing house, or to begin the process of mastering with a sound ﬁle that has already
been compressed and raised to levels approaching digital full-scale. Doing so
obviously reduces the dynamic choices that can be made in the master ing stage.
For this reason, most mastering engineers will ask that the project be delivered
without any compression or dynamics of any kind (although, you might take
a screenshot or write down any settings that you had, as a possible suggestion)
and that the overall session gain be reduced so that it peaks at a lower level
(–12 dB for example). Again, it’s always a good idea to discuss these and other
details with the mastering engineer beforehand.
545
Mastering  CHAPTER 20

Mastering the Details of a Project
546
Sound File Resolution
In the end, the sample rate and bit rate resolution of a sound ﬁle is a matter
of personal choice, beliefs and working habits. Some believe that sample rates
of 44.1 k and 48 k with a 24 (or even 16) bit-depth will be sufﬁcient to provide
quality audio (given that the interface and converters are very high in quality).
Others believe that rates of 96 k and 192 k with a 24 bit-depth is required to
capture the music with sufﬁcient resolution. I am going to stay out of this
argument and let you make the proper choices for your own situation. 
Whatever your chosen session sample rate will be, it’s almost always best to
deliver the ﬁnal master recording to the mastering engineer at the original native
rate. That’s to say, if the session was recorded and mixed at 24/96, the ﬁnal
mixdown resolution should delivered to the mastering engineer at that rate 
and bit resolution. If it was recorded at 24/48 then it should be delivered at
that rate.
Dither
As was stated in Chapter 6, the addition of small amounts of randomly generated
noise to an existing bit stream can actually increase the overall bit resolution
(and therefore low-level noise and signal clarity) of a recorded signal, when
reducing the ﬁle’s bit-depth to a lower depth (i.e., reducing a 24-bit ﬁle to one
with a 16-bit-depth). Through the careful addition of dither, it’s actually possible
for signals to be encoded at levels that are less than the data’s least signiﬁcant
bit level. You heard that right, by adding a small amount of carefully shaped,
random noise into the A/D path, the resolution of the conversion process can
actually be improved below the least signiﬁcant bit level and thereby reduce a
sound ﬁle’s harmonic distortion.
Within mastering, dither is often manually applied to sound ﬁles that have been
recorded at 24-bit-depths, so as to reduce the effects of lost resolution due to
the truncation of least signiﬁcant bits. For example, mastering engineers might
carefully experiment with applying dither (by inserting a plug-in or by applying
dither within the mastering DAW) to a high-resolution ﬁle before saving or
exporting it as a 16-bit ﬁnal master (Figure 20.4). In this way, distortion is
reduced and the sound ﬁle’s overall clarity is increased.
FIGURE 20.4
Dither Plug-ins. (a) Sonnox
Oxford Limiter plug-in for
the Apollo and the UAD
effects processing card.
(Courtesy of Universal
Audio, www.uaudio.com 
© 2017 Universal Audio,
Inc. All rights reserved.
Used with permission) 
(b) Apogee UV22 dither
plug-in within Cubase/
Nuendo. (Courtesy of
Steinberg Media
Technologies GmbH,
a division of Yamaha
Corporation,
www.steinberg.net)

Relative Volumes
In addition to addressing the volume levels of each song/section within a
project, one of the tasks in the mastering process is to smooth out the relative
volume differences between songs over the course of the entire project. These
differences could occur from a number of sources, including general variations
in mixdown and program content levels, as well as levels between projects that
have been mixed at different studios (possibly with different engineers).
The best cues that can be used to smooth out the relative rms and peak
differences between songs, etc., can be obtained by:
n Using your ears to ﬁne-tune the relative volume levels from song to song
n Carefully watching the master output meters on a recorder or DAW
n Watching the graphic levels of the songs as they line up on a DAW’s screen
This process is also part of the art of mastering, as it generally requires judgment
calls to set the relative levels properly in a way that makes sense. Contrary
to popu lar belief, the use of a standard DAW normalization tool can’t smooth
out these level differences, because this process only detects the peak level
within a sound ﬁle and raises the overall level to a determined value. Since the
average (rms) and peak levels will often vary widely between the songs of a
project, this tool will often lead you astray (although certain editors and
mastering plug-ins provide normalization tools that have more variables which
are more useful and in depth).
EQ
As is the case in the mixdown process, equalization is often an extremely im -
portant tool for boosting, cutting or tightening up the low end, adding presence
to the midrange and tailoring the high end of a song or overall project. EQ can
also be used as a tool to smooth out differences between cuts or for making
changes that affect the overall character of the entire project. Of course, there
are a wide range of hardware and software plug-in EQ systems that are available
547
Mastering  CHAPTER 20
FIGURE 20.5
EQ in mastering. 
(a) EQ processing in Ozone
7. (Courtesy of iZotope Inc.,
www.izotope.com) 
(b) Channel EQ within
Cubase/Nuendo. 
(Courtesy of Steinberg 
Media Technologies GmbH,
a division of Yamaha
Corporation,
www.steinberg.net)

Mastering the Details of a Project
548
for applying the ﬁnal touches both within a professional and project-based
setting (Figure 20.5). It should be stressed that it isn’t always necessary to insert
a third-party EQ device into the chain in order to get the job done. In the case
of a DAW, the EQ on the master output bus can do much of the work (provided
that it’s musical and works for you).
Dynamics
One of the most commonly used (and overused) tools within the mastering
process relates to dynamics processing.
Although the traditional name-of-the-game is to achieve the highest overall
average level within a song or complete project, care must be taken so as not
to apply so much compression that the life gets dynamically sucked out of the
sound (Figure 20.6). As with the ﬁrst rule in recording—“There are no rules”—
the amount of dynamics processing is entirely up to those who are creatively
involved in the ﬁnal mastering process. However, it’s important to keep in mind
the following guidelines:
FIGURE 20.6
Reducing the dynamics of a
passage too much can take
the transients and dynamic
“life” out of a track or
passage (shown at right).
n Depending on the program content and genre, the general dynamic trend
is toward raising the overall levels to as high a point as possible. It should
be noted, however, that there are those in the production community that
are calling for a return to allowing the natural dynamics within the music
to shine through.
n When pushed to an extreme, compression will often have an intended or
unintended side effect of creating a sound that has been “squashed,”
giving a “wall of sound” character that’s thick (a good thing) and/or one
that’s sonically lifeless (a bad thing).
n When compression is not applied (or little is used), the sound levels will
often be lower, thinner (that might be bad) or full of dynamic life (that’s
often extremely good).
From all of this, you’d be correct if you said that the process is entirely subjective
and relative! The use of dynamic reduction can help add a strong presence to
a recording, while overuse can actually kill its dynamic life; therefore it’s
important to use it wisely!

549
Mastering  CHAPTER 20
COMPRESSION IN MASTERING
The purpose of compression in mastering is, of course, to control the overall
dynamics of a song or sound production. A compressor (Figure 20.7) can be
used to help control the peaks within the sound ﬁle, or it can be used to help
tame or control the overall dynamic levels. Before compression (or any other
form of dynamic control, for that matter) is applied, it’s often a good idea to
have a clear (or some form of) vision as to how you want this gain-changing
stage to affect the sound. Without a clear idea of how this tool can help, it can
be easily overused in a way that can take the dynamic life out of your sound
(Figure 20.8).
FIGURE 20.7
Compressor/Limiters: 
(a) Manley Stereo Variable
Mu® mastering outboard
hardware compressor.
(Courtesy of Manley
Laboratories, Inc.,
www.manleylabs.com) 
(b) Neve 33608 Compressor
plug-in for the Apollo and
the UAD effects processing
card. (Courtesy of Universal
Audio, www.uaudio.com 
© 2017 Universal Audio,
Inc. All rights reserved.
Used with permission)
A compressor doesn’t only make things louder, through judicial control it can
be used to make it fuller, richer and can make the mix sound “punchier.” This
is done by setting the compressor’s threshold at a point that’s just above the
music average level. Then by setting the compression ratio at between 2:1 and
8:1, the device will be able to reduce the peak levels without affecting the overall
mix’s sound. By setting the attack and release times to a fast acting setting (with
a higher ratio) and then setting the threshold so that the compression effect is
obvious, you can then go about the task of backing off on the threshold and
ratio settings until it starts to sound natural while still registering a degree of
compression on the signal reduction meter. Again, the desired sound can range
from giving the mix an extra degree of presence or punch—all the way to giving
a natural sound, while reducing the level of the program peaks.
FIGURE 20.8
Figure showing the same
passage with varying
degrees of compression.

Mastering the Details of a Project
550
LIMITING IN MASTERING
One of the more misunderstood (or possibly misused) dynamics processing
functions within the mastering process is limiting. Since, by its very nature, a
limiter has a very high input to output gain reduction ratio (up to 100:1), it is
used to restrict the program output’s signal from exceeding levels above its
threshold level. This means that if it is improperly set at threshold levels that
are too low, the overall dynamic range can be severely reduced in an unnatural
way that can cause signal clipping and distortion.
On the other-hand, when a high-quality, fast-acting hardware or software limiter
(Figure 20.9) is driven by an input signal that has good dynamics and is set at
such a level that any excessive peaks only occasionally occur then, the limiter’s
threshold can be set at a level that will detect just the peak overloads and reduce
them to acceptable levels. Said another way, when a quality limiter is properly
set to reduce occasional program peaks, the results will often be transparent
and inaudible.
FIGURE 20.9
Limiters in the mastering
process. (a) Manley Slam!
(Courtesy of Manley
Laboratories, Inc.,
www.manleylabs.com) 
(b) Precision Limiter plug-in
for the Apollo and the UAD
effects processing card.
(Courtesy of Universal
Audio, www.uaudio.com
© 2017 Universal Audio,
Inc. All rights reserved.
Used with permission)
MULTIBAND DYNAMIC PROCESSING IN MASTERING
The modern-day mastering process often makes use of multiband dynamic
processing (Figure 20.10) in order to break the frequencies of the audio spectrum
into bands that can be individually processed. Depending on the system, up to
ﬁve distinct bands might be available for processing the ﬁnal signal. Such a
hardware or software system could be used to strongly compress the low
frequencies of a song using a speciﬁc set of parameters, while applying only a
small amount of compression to the sibilance at its upper end. Alternatively, 
a multi-band can be used to add spice to the upper end to the mix, while
reducing the sharp upper-end attacks that could be harsh.
FIGURE 20.10
Multiband compression. (a)
ProTools Multiband
Dynamics. (Courtesy of Avid
Technology, Inc.,
www.avid.com) (b) Precision
Multiband Plug-in for the
Apollo and the UAD effects
processing card. (Courtesy
of Universal Audio,
www.uaudio.com © 2017
Universal Audio, Inc. All
rights reserved. Used with
permission)

MID/SIDE PROCESSING
Another mastering tool that has been gaining favor in recent times is M/S
processing (Figure 20.11). This seemingly new technique is actually quite old,
being based on Alan Dower Blumlein’s 1934 mic technique patent.
551
Mastering  CHAPTER 20
FIGURE 20.11
Plug-ins showing M/S
processing functions. 
(a) Barinworx Modus 
EQ bx1. (b) K-Stereo
Ambience Recovery.
(Courtesy of Universal
Audio, www.uaudio.com 
© 2017 Universal Audio,
Inc. All rights reserved.
Used with permission)
In essence, this process mathematically is able to split a stereo signal’s sonic
image into two components, the Mid (M) and the Side (S) processing channels.
The Mid contains all of the in-phase signal that combines in the (L + R) channel
information (containing the direct, non-reverberant, in-your-face sound), while
the Side contains the (L + R) + (L – R) information (containing the ambient,
reverberant, distant sounds). The fact that this information can be extracted
from a stereo signal means that the ratio between the two can be varied, allowing
an overall mix to be brought forward (by increasing the Mid over the Side ratio),
making it more present or made to be a wider and more ambient mix (by
increasing the Side over the Mid ratio). In addition, since the two information
channels are split, they can be individually processed over a range of individual
frequency bands in almost any way, allowing for a greater degree of ﬂexible
control over these aspects of the mix. It should be pointed out, however, that
the use of compression in M/S can cause problems with shifting widths in the
soundscape—just a note of caution. Finally, M/S processing can be an involved
process to fathom, for more information, try searching the web for “What is
Mid/Side Processing?” You’ll ﬁnd loads of information on the subject there.
Mastering Plug-Ins
Of course, with the interest in DIY mastering comes a selection of plug-ins that
are speciﬁcally designed (or bundled) to handle the processing functions and
basic details that goes into mastering. These could include separate plug-ins
that are speciﬁcally tailored to provide the EQ, compression, limiting and
multiband needs of ﬁnessing a master into its ﬁnal mastered form. In essence,
these tools combine together to create a personalized mastering chain.
Overall mastering stand-alone programs or plug-ins have been designed that
provide an all-in-one environment that often includes EQ, dynamics, M/S
processing, restoration and more. These program/plug-ins (Figure 20.12) provide
visual sound ﬁle and setup cues, template suggestions and various other tools
to help you through the process.

552
Finally, the approach that would most likely be taken by a professional mastering
engineer would be to build his or her own ﬁnal mastering chain of hand-chosen
software and hardware options that work together to support the audio program
or music. In the end, all of these options are totally valid, depending upon your
skill level and/or desire to learn the ﬁner points of the craft.
On a Final Note
In closing, it’s always a wise idea to budget in some time to QC (quality control)
the mastered recording before committing it to being a ﬁnal product. I’ve learned
that the ﬁnal process of exporting (bouncing) a mix into its ﬁnal form is always
best done in “real-time.” This lets us listen to the entire track as it's being played
out, allowing us one ﬁnal chance to catch something that might be wrong in
the ﬁnal mix.
If at all possible, take a week and listen to it in your car, on your boom box,
home theater system, in another studio—virtually everywhere! Then revisit the
mastered mix one last time. As a musician, producer or record label, it will be
your calling card for quite some time to come. Once you’re satisﬁed with the
sound of the ﬁnished product, then you can move from the mastering phase
to making the project into a ﬁnal, salable product.
FIGURE 20.12
Izotope Ozone 7 complete
set of mastering plug-ins.
(Courtesy of iZotope Inc.,
www.izotope.com)
Mastering the Details of a Project

Given the huge number of changes that have occurred within the business of
music marketing and media distribution, it stands to reason that an equally large
number of changes have occurred when it comes to planning and considering
how you are going to get your new-born project out to the masses. In short,
many of the rules have changed, and it’s the wise person who takes the time to
put their best foot (and plan) forward to make it all shine. So, having said this—
what are our best options for reaching people in this day and age? Well there’s:
n CD: Some would call this a dying breed, but when you walk into a music
store, you’ll see them everywhere. Fact is, a lot of people still like to have
their music media “in hand.”
n DVD and Blu-ray: Still the preferred medium for visual media (but just
barely). Of course, DVDs are good for movies and for music video/live
concerts (usually with a 5.1/7.1 soundtrack). Blu-ray discs, also tout high-
deﬁnition video, but are also a medium for getting high-resolution
5.1/7.1/9.1 music out to the masses in a physical form.
n Vinyl: We’re all pretty much aware of vinyl’s comeback as a viable music
medium. It’s the perfect in-hand “see what I just got?” product—having
both a huge retro allure and saying to those around you that you care
enough about your music to dare to be different.
n Online (download) distribution: The 700-pound gorilla in the room.
Online music sites exist in the hundreds (probably thousands) although
the biggies (iTunes, CDBaby, Google Play, Amazon Digital Music, etc.) still
are at the top of the digital download heap, allowing us to collect and
consume albums or single tracks when and where we want them.
n Hi-res download distribution: Similar to online music download services,
high-resolution sites allow us to download our favorite classical, new
release or newly-remastered, high-res version of our favorite older classics.
Most often being offered as larger, hi-res 24/96 sound ﬁles, these special
releases offer up the highest-quality versions of a master recording. Again,
553
CHAPTER 21
Product Manufacture 
and Distribution

Product Manufacture and Distribution
554
this is presented to customers that value the quality (or perceived
“specialness”) of their personal hi-res music collection.
n Music streaming: The other 700-pound gorilla that a lot of online distribu -
tion companies are trying to come to grips with. The idea being that paying
a monthly subscription to a music service will give us instant access to their
entire music library in an on-demand fashion—usually without the option
of owning a physical or download copy of our (well, actually—their) music.
It worked for video streaming (i.e., Netﬂix) so, why not for music?
n Free music streaming: One of the biggest music services on the web is (at
the present time) actually free. YouTube has eclipsed many of the streaming
services as an outlet for getting music, DJ playlists and music videos out
to the public.
With all of these distribution methods, one of greatest misconceptions
surrounding the production of music, visual and other media is the idea that
once you ﬁnish your project and have the master ﬁles in hand, your work is
done; that the production process is now over. Of course, this is far from being
the truth. There are many decisions that need to be made long before the
process has been ﬁnished (and most often, long before the project has even
begun). Questions like:
n Can we deﬁne who the market is and take steps to make “the product”
known to this market?
n Will it be self-distributed or will it be released by a music label who knows
the ins-and-outs of the business and how to get “the product” out to the
masses?
n Will it be available in physical form and/or online?
n Will there be a marketing strategy and budget?
All of these will deﬁnitely need to be addressed. One of the biggest mistakes
that can be made during the creation of a project is to think that your adoring
public will be clamoring for your product, website and merchandise without
any marketing strategy or general outreach strategy.
Early in this book, I told you about the ﬁrst rule of recording—that “there are
no rules, only guidelines.” This actually isn’t true. There is one rule:
If you don’t pre-plan, and follow through with your
production and marketing strategies, you can be
fairly sure that your project will sit on a shelf. Or,
worse, you’ll have 1,000 CDs sitting in your
basement that’ll never be heard or downloads that
will be lost in the deep digital ocean—a huge shame
given all the hard work that went into making it.
Now that we’ve gotten the really important idea of pre-planning your strategies
across let’s have a look at the various media. How they are made and how they
can best be utilized to get the word out to the public.

555
Product Manufacture and Distribution  CHAPTER 21
PRODUCT MANUFACTURE
Although downloadable and streaming media are well on their way toward
dominating the music and media industries, the ability to buy, own and gift a
physical media object still warms the hearts of many a media product purchaser.
Often, containing uncompressed, high-quality music, these physical discs and
records are held in regard as something that can be held onto, looked at and
packaged up with a big, bright gift bow. Their major market-share days may be
numbered but don’t count them out any time soon.
The CD
Beyond the process of distributing audio over the Internet (using an online
service or from your own site), as of this writing, the compact disc (CD) is still
a strong and viable medium for distributing music in a physical form. These
120-mm silvery discs (Figure 21.1a) contain digitally encoded information (in
the form of microscopic pits) that’s capable of yielding playing times of up to
74 or 80 minutes at a standard sampling rate of 44.1 kHz with a 16-bit-depth.
The pit of a CD is approximately half a micrometer wide, and a standard
manufactured disc can hold about 2 billion pits. These pits are encoded onto
the disc’s surface in a spiraling fashion, similar to that of a record, except that
60 CD spirals can ﬁt into the single groove of a long-playing record. These spirals
also differ from a record in that they travel outward from the center of the disc,
are impressed into the plastic substrate, and are then covered with a thin coating
of aluminum (or occasionally gold) so that the laser light can be reﬂected back
to a receiver. When the disc is placed in a CD player, a low-level infrared laser
is alternately reﬂected or not reﬂected back to a photosensitive pickup. In this
way, the reﬂected data is modulated so that each pit edge represents a binary
1, and the absence of a pit edge represents a binary 0 (Figure 21.1b). Upon
playback, the data is then demodulated and converted back into an analog form.
FIGURE 21.1
The compact disc. 
(a) Physical disc and
package. (Courtesy of
www.davidmileshuber.com)
(b) Transitions between a
pit edge (binary 1) and the
absence of a pit edge
(binary 0).
Songs or other types of audio material can be grouped on a CD into tracks
known as “indexes.” This is done via a subcode channel lookup table, which
makes it possible for the player to identify and quickly locate tracks with frame
accuracy. Subcodes are event pointers that tell the player how many selections

Product Manufacture
are on the disc and where their beginning address points are located. At present,
eight sub-code channels are available on the CD format, although only two
(the P and Q subcodes) are used.
Functionally, the CD encoding system splits the 16 bits of information into two
8-bit words with error correction (that’s applied in order to correct for lost or
erroneous signals). In fact, without error correction, the CD playback process
would be so fragile and prone to dropouts that it’s doubtful it would’ve ever
become a viable medium. This information is then translated into a data frame,
using a process known as eight-to-fourteen modulation or EFM. Each data frame
contains a frame-synchronization pattern (27 bits) that tells the laser pickup
beam where it is on the disc. This is then followed by a 17-bit subcode word,
12 words of audio data (17 bits each), 8 parity words (17 bits each), 12 more
words of audio, and a ﬁnal 8 words of parity (error correction) data.
THE CD MANUFACTURE PROCESS
In order to translate the raw PCM of a music or audio project into a format
that can be understood by a CD player, a compact disc burning system must
be used. These come in two ﬂavors:
n Specialized hardware/software that’s used by professional mastering and
duplication facilities to mass replicate optical media.
n Disc burning hardware/software systems that allow a personal computer
to easily and cost effectively burn individual or small-run CDs.
Both system types allow audio to be entered into the software, after which the
tracks can be assembled into the proper order and the appropriate gap times
entered between tracks (in the form of index timings). Depending on the system,
sound ﬁles might also be processed using cross-fades, volume, EQ and other
parameters. Once assembled, the project can be “ﬁnalized” into a media form
that can be directly accepted by a CD manufacturing facility. By far, the most
common media that’s received by CD pressing plants for making the ﬁnal
master disc are user-burned CD-Recordable (CD-R) discs, although some
professional services will still accept a special Exabyte-type data tape system.
Note that not all CD-R media are manufactured using high-quality standards.
In fact, some are so low in quality, that the project’s data integrity could be
jeopardized. As a general rule:
n It’s always good to use high-quality “master-grade” CD-Rs to burn the ﬁnal
duplication master (you can sometimes see the difference in pit and optical
dye quality with the naked eye).
n It’s always best to send two identical copies to the manufacturer (just in
case one fails).
n Speaking of failing, you should check that the manufacturer has run a
data integrity check on the ﬁnal master to ensure that there are few to no
errors, before beginning the duplication process.
556

557
Product Manufacture and Distribution  CHAPTER 21
Once the manufacturing plant has received the recorded media, the next stage
in the large-scale replication process is to cut the original CD master disc. The
heart of such a CD cutting system is an optical transport assembly that contains
all the optics necessary to write the digital data onto a reusable glass master
disc that has been prepared with a photosensitive material.
After the glass master has been exposed using a special recording laser, it’s placed
in a developing machine that etches away the exposed areas to create a ﬁnished
master. An alternative process, known as nonphotoresist, etches directly into
the photosensitive substrate of the glass master without the need for a develop -
ment process.
After the glass or CD master disc has been cut, the compact disc manufacturing
process can begin (Figure 21.2). Under extreme clean-room conditions, the glass
disc is electroplated with a thin layer of electro-conductive metal. From this,
the negative metal master is used to create a “metal mother,” which is used to
replicate a number of metal “stampers” (metal plates which contain a negative
image of the CD’s data surface). The resulting stampers make it possible for
machines to replicate clear plastic discs that contain the positive encoded pits,
which are then coated with a thin layer of foil (for increased reﬂectivity) and
encased in clear resin for stability and protection. Once this is done, all that
remains is the screen-printing process and ﬁnal packaging. The rest is in the
hands of the record company, the distributors, marketing and you.
As with any part of the production process, it’s always wise to do a full back -
ground check on a production facility and even compare prices and services
FIGURE 21.2
Various phases of the CD
manufacturing process. (a)
The lab, where the CD
mastering process begins.
(b) Once the graphics are
approved, the project’s
packaging can move onto
the printing phase. (c) While
the packaging is being
printed, the approved
master can be burned onto
a glass master disc. (d)
Next, the master stamper
(or stampers) is placed onto
the production line for CD
pressing.
Note: This figure continues
on the next page.

Product Manufacture
558
from at least three manufacturing houses. Give the company a call, request a
promo pack (which includes product and art samples, service options and a
price sheet), ask questions and try to get a “feel” for their customer service
abilities, their willingness to help with layout questions, etc. You’d be surprised
just how much you can learn in a short time.
Once you’ve settled on a manufacturer, it’s always a good idea to research what
their product and graphic arts needs and specs are before delving
into this production phase. When in doubt about anything,
give them a call and ask; they are there to help you get
the best possible product and to avoid costly or time-
consum ing mistakes. The absolute last thing that you
or the artist wants is to have several thousand discs
arrive on your doorstep that are—WRONG! Receiving
a test pressing and graphic “proof” is almost always well
worth the time and money. It’s never wise to assume that
a manufacturing or duplication process is perfect and doesn’t
make mistakes. Remember, Murphy’s law can pop up anywhere and at any time!
CD BURNING
Software for burning CDs/DVDs/Blu-ray media is available in various forms for
both the Mac and PC. These include the simple burning applications that are
included with the popular operating systems, popular third-party burning
applications and more complex authoring programs that are capable of editing,
mastering and assembling individual cuts into a ﬁnal burned master.
There are numerous ways in which a CD project can be prepared and burned.
For starters, it’s a fairly simple matter to prepare and master individual songs
within a project and then load them into a program for burning (Figure 21.3).
Such a program can be used to burn the audio ﬁles in a straightforward manner
from beginning to end. Keep in mind that the Red Book CD standard speciﬁes
a beginning header silence (pause length) that’s 2 seconds long, however, after
this initial lead-in, any pause length can be user speciﬁed. The default setting
for silence between cuts is 2 seconds, however, these lengths will often vary, as
one song might want to ﬂow directly into another, while the next might want
a longer pause to help set the proper artistic mood.
Whenever
possible, it’s always wise and
extremely important that you be given
art proofs and test pressings 
before the final products are mass
duplicated.
FIGURE 21.2 cont.
(e) The freshly stamped
discs are cooled and
checked for data integrity.
(f) Labels are then silk-
screen printed onto the
CDs, whereafter the printed
CDs are checked before
being inserted into their
finished packaging.
(Courtesy of Disc Makers,
Inc., www.discmakers.com)

Most CD burning programs will also allow you to enter “CD Text” information
(such as title, artist name/copyright and track name ﬁeld code info) that will
then be written directly into the CD’s subcode data ﬁeld. This can be a helpful
tool, as important artist, copyright and track identiﬁers can be directly embedded
within the CD itself and will be automatically displayed on most CD hard- and
software players. Additionally, database services such as Gracenote can be used
to display track and project information to players that are connected to the
web. These databases (which should not be confused with CD Text) allow CD
titles, artist info, song titles, project graphics and other info to appear on most
media-based music players—an important feature for any artist and label.
As always, careful attention to the details should always be taken when creating
a master optical disc, making sure that:
n High-quality media is used
n The media is burned using a stable, high-quality drive
n The media is carefully labeled using a recommended marking pen
n Two copies are delivered to the manufacturer, just in case there are data
problems on one of the discs
With the ever-growing demands of marketing, gigging and general business
practices, many independent musicians are also taking on the task of burning,
printing, packaging and distributing their own physical products from the home
or business workplace. This homespun strategy allows for individual or small
runs to be made in an “on-demand” basis, without tying up ﬁnancial resources
and storage space with unsold CD inventories.
DVD and Blu-ray Burning
Of course, on a basic level, DVD and Blu-ray burning technologies have matured
enough to be available and affordable for the Mac or PC. From a technical
standpoint, these optical discs differ from the standard CD format in several
ways. The most basic of these are:
559
Product Manufacture and Distribution  CHAPTER 21
FIGURE 21.3
EZ CD Audio Converter 
All-in-one multi format
Audio Converter, CD Ripper,
Metadata Editor, and Disc
Burner. (Courtesy of
Poikosoft, www.
poikosoft.com)

Product Manufacture
560
n An increased data density due to a reduction in pit size (Figure 21.4)
n Double-layer capabilities (due to the laser’s ability to focus on two layers
of a single side)
n Double-side capabilities (which again doubles the available data size)
FIGURE 21.4
Detailed relief showing
standard CD, DVD and Blu-
ray pit densities.
In addition to the obvious beneﬁts of increased data density, a DVD (max of
17 Gb) or Blu-ray (max of 50 Gb—dual layer) will also have higher data transfer
rates, making them the ideal media for: 
n The simultaneous decoding of digital video and surround-sound audio
n Multichannel surround sound
n Data- and access-intensive video games
n High-density data storage
As DVD and Blu-ray writable drives have become commonplace, affordable data
backup and mastering software has come onto the market that brought the art
of video and Hi-Def production to the masses. Even high-level optical media
mastering is now possible in a desktop environment. More information on the
ﬁner points of codec data compression and media technologies relating these
technologies can be found in Chapter 11.
Optical Disc Handling and Care
Here are a few basic handling tips for optical media (including the recordable
versions) from the National Institute of Standards and Technology:
Do:
n Handle the disc by the outer edge or center hole (your ﬁngerprints may
be acidic enough to damage the disc over time).
n Use a felt-tip permanent marker to mark the label side of the disc. The
marker should be water or alcohol based. In general, these will be labeled
as being a non-toxic CD/DVD pen. Stronger solvents may eat through the
thin protective layer to the data.
n Keep discs clean. Wipe with a cotton fabric in a straight line from the center
of the disc toward the outer edge. If you wipe in a circle, any scratches may
follow the disc tracks, rendering them unreadable. Use a disc-cleaning or
light detergent to remove stubborn dirt.

n Return discs to their cases immediately after use.
n Store discs upright (book-style) in their cases.
n Open a recordable disc package only when you are ready to record.
n Check the disc surface for scratches, etc. before recording.
Don’t:
n Touch the surface of a disc.
n Bend the disc (as this may cause the layers to separate).
n Use adhesive labels (as they may unbalance or warp the disc).
n Expose discs to extreme heat or high humidity; for example, don’t leave
them in direct sunlight or in on a car’s dash.
n Expose recordable discs to prolonged sunlight or other sources of
ultraviolet light.
n Expose discs to extreme rapid temperature or humidity changes.
Especially Don’t:
n Scratch the label side of the disc (it’s often more sensitive than the
transparent side).
n Use a pen, pencil or ﬁne-tipped marker to write on the disc’s label surface.
n Try to peel off or reposition a label (it could destroy the reﬂective layer
or unbalance the disc).
Vinyl
Obviously, reports of vinyl’s death were very premature. In fact, for consumers
ranging from Dance DJ trip-hopsters to die-hard classical buffs, the record is
making a popular comeback in record stores around the world. However, the
truth remains that only a few record pressing plants are still in existence (they’re
currently working their overtime butts off) and there are far fewer discs mastering
labs that are capable of cutting “master lacquers.” As a result, it may take a bit
longer to ﬁnd a facility that ﬁts your needs, budget and quality standards, but
it’s deﬁnitely not a futile venture.
DISC CUTTING
The ﬁrst stage of production in the disc manufacturing process is disc cutting.
As the master is played from a digital source or analog tape machine, its signal
output is fed through a mastering console to a disc-cutting lathe. Here, the
electrical signals are converted into the mechanical motions of a stylus and are
cut into the surface of a lacquer-coated recording disc.
Unlike the compact disc, a record rotates at a constant angular velocity, such
as 33 1/3 or 45 rpm (revolutions per minute), and has a continuous spiral that
gradually moves from the disc’s outer edge to its center. The recorded time
relationship can be reconstructed by playing the disc on a turntable that has
the same constant angular velocity as the original disc cutter.
561
Product Manufacture and Distribution  CHAPTER 21

Product Manufacture
562
The system that’s used for recording a stereo disc is the 45/45 system. The recording
stylus cuts a groove into the disc surface at a 90º angle, so that each wall of the
groove forms a 45º angle with respect to the vertical axis. Left-channel signals
are cut into the inner wall of the groove and right-channel signals are cut into
the outer wall, as shown in Figure 21.5a. The stylus motion is phased so that
L/R channels that are in-phase (a mono signal or a signal that’s centered between
the two channels) will produce a lateral groove motion (Figure 21.5b), while
out-of-phase signals (containing channel difference information) will produce
a vertical motion that changes the groove’s depth (Figure 21.5c). Because mono
information relies only on lateral groove modulation, an older disc that has been
recorded in mono can be accurately reproduced with a stereo playback cartridge.
Disc-Cutting Lathe
The main components of a vinyl disc-cutting lathe are the turntable, lathe bed
and sled, pitch/depth control computer and cutting head. Basically, the lathe
(Figure 21.6a) consists of a heavy, shock-mounted steel base (a). A weighted turn -
table (b) is isolated from the base by an oil-ﬁlled coupling (c), which reduces
wow and ﬂutter to extremely low levels. The lathe bed (d) allows the cutter
suspension (e) and the cutter head (f) to be driven by a screw feed that slowly
moves the record mechanism along a sled in a motion that’s perpendicular to
the turntable.
Cutting Head
The cutting head translates the electrical signals that are applied to it into
mechanical motion at the recording stylus. The stylus gradually moves in a straight
line toward the disc’s center hole as the turntable rotates, creating a spiral groove
FIGURE 21.5
The 45/45 cutting system.
(a) Stereo waveform signals
can be encoded into the
grooves of a vinyl record in
a 45º/45º vector. (b) In-
phase groove motion.
(c) Out-of-phase groove
motion.
FIGURE 21.6
The disc-cutting lathe: 
(a) lathe with automatic
pitch and depth control.
(Courtesy of Paul
Stubblebine Mastering
and Michael Romanowski
Mastering, San Francisco,
CA, www.paulstubblebine.
com and www.michael
romanowski.com);
(b) simplified drawing of
a stereo cutting head.
Disc-Cutting Lathe 
Cutting Head 

563
Product Manufacture and Distribution  CHAPTER 21
on the record’s surface. This spiral motion is achieved by attaching the cutting
head to a sled that runs on a spiral gear (known as the lead screw), which drives
the sled in a straight track.
The stereo cutting head (Figure 21.6b) consists of a stylus that’s mechanically
connected to two drive coils and two feedback coils (which are mounted in a
perma nent magnetic ﬁeld) and a stylus heating coil that’s wrapped around the tip
of the stylus. When a signal is applied to the drive coils, an alternating current
ﬂows through them creating a changing magnetic ﬁeld that alternately attracts and
repels the permanent magnet. Because the permanent magnet is ﬁxed, the coils
move in proportion to a ﬁeld strength that causes the stylus to move in a plane
that’s 45º to the left or right of vertical (depending on which coil is being driven).
Pitch Control
The head speed determines the “pitch” of the recording and is measured by the
number of grooves, or lines per inch (lpi), that are cut into the disc. As the head
speed increases, the number of lpi will decrease, resulting in a corresponding
decrease in playing time. Varying the lead screw’s rotation can most commonly
change groove pitch by changing the motor’s speed (a common way to vary the
program’s pitch in real-time).
The space between grooves is called the land. Modulated grooves produce a lateral
motion that’s proportional to the in-phase signals between the stereo channels.
If the cutting pitch is too high (causing too many lines per inch, which closely
spaces the grooves) and high-level signals are cut, it’s possible for the groove to
break through the wall into an adjacent groove (causing overcut) or for the grooves
to overlap (twinning). The former is likely to cause the record to skip when played,
while the latter causes either distortion or a signal echo from the adjacent groove
(due to wall deformations). Groove echo can occur even if the walls don’t touch
and is directly related to groove width, pitch and level.
These cutting problems can be eliminated either by reducing the cutting level or
by reducing the lines per inch. A conﬂict can arise here as a louder record will
have a reduced playing time, but will also sound brighter, punchier, and more
present (due to the Fletcher-Munson curve effect). Because record companies
and producers are always concerned about the competitive levels of their discs
relative to those that are cut by others, they’re reluctant to reduce the overall
cutting level.
The solution to these level problems is to continuously vary the pitch so as to
cut more lines per inch during soft passages and fewer lines per inch during loud
passages. This is done by splitting the program material into two paths: undelayed
and delayed. The undelayed signal is routed to the lathe’s pitch/depth control
computer (which determines the pitch needed for each program portion and
varies the lathe’s screw motor speed). The delayed signal (which is usually achieved
by using a high-quality digital delay line) is fed to the cutter head, thereby giving
the pitch/depth control computer enough time to change the lpi to the appro -
priate pitch. Although longer playing times can be attained by reducing levels
Pitch Control 

Product Manufacture
and by varying pitch control, many of the pros recommend keeping playing times
of a “good sounding” LP to about 18 minutes per side or less.
The LP Mastering Process
Once the mastering engineer sets a basic pitch on the lathe, a lacquer (a ﬂat
aluminum disc that’s coated with a ﬁlm of lacquer) is placed on the turntable
and compressed air is used to blow any accumulated dust off the lacquer surface.
A chip suction vacuum is started and a test cut is made on the outside of the
disc to check for groove depth and stylus heat. Once the start button is pressed,
the lathe moves into the starting diameter, lowers the cutting head onto the
disc, starts the spiral and lead-in cuts, and begins playing the master production
tape. As the side is cut, the engineer can ﬁne-tune any changes to the previously
determined console settings. Whenever an analog tape machine is used, a
photocell mounted on the deck senses white leader tape between the selections
on the master tape and signals the lathe to automatically expand the grooves
to produce track bands. After the last selection on the side, the lathe cuts the
lead-out groove and lifts the cutter head off the lacquer.
This master lacquer is never played, because the pressure of the playback stylus
would damage the recorded soundtrack (in the form of high-frequency losses
and increased noise). Reference lacquers (also called reference acetates or simply
acetates) are cut to hear how the master lacquer will sound.
After the reference is approved, the record company assigns each side of the
disc a master (or matrix) number that the cutting room engineer scribes between
the grooves of the lacquer’s ending spiral. This number identiﬁes the lacquer
in order to eliminate any need to play the record, and often carries the mastering
engineer’s personal identity mark. If a disc is remastered for any reason, some
record companies will retain the same master numbers; others add a sufﬁx to
the new master to differentiate it from the previous “cut.”
Vinyl Disc Plating and Pressing
When the ﬁnal master arrives at the plating plant, it is washed to remove any
dust particles and then electroplated with nickel. Once the electroplating is
complete, the nickel plate is pulled away from the lacquer. If something goes
wrong at this point, the master will be damaged, and the master lacquer must
be recut.
The nickel plate that’s pulled off the master (called the matrix) is a negative
image of the master lacquer (Figure 21.7). This negative image is then electro -
plated to produce a nickel positive image called a mother. Because the nickel
is stronger than the lacquer disc, several mothers can be made from a single
matrix. Since the mother is a positive image, it can be played as a test for noise,
skips, and other defects. If it’s accepted, the mother can be electroplated several
times, producing stampers that are negative images of the disc (a ﬁnal plating
stage that’s used to press the record).
564
The LP Mastering Process 
Vinyl Disc Plating and Pressing 

565
Product Manufacture and Distribution  CHAPTER 21
The stampers for the two sides of the record are mounted on the top and bottom
plates of a hydraulic press. A lump of vinylite compound (called a biscuit) is
placed in the press between the labels for the two sides. The press is then closed
and heated by steam to make the vinylite ﬂow around the raised grooves of the
stampers. The resulting pressed record is too soft to handle when hot, so cold
water is circulated through the press to cool it before the pressure is released.
When the press opens, the operator pulls the record off the mold and the excess
(called ﬂash) is trimmed off after the disc is removed from the press. Once
done, the disc’s edge is buffed smooth and the product is ready for packaging,
distribution and sales.
PRODUCT DISTRIBUTION
With the rise of Internet music distribution and the steady breakdown of the
traditional record company distribution system, bands and individual artists
have begun to produce, market and sell their own music on an ever-increasing
scale. This concept of the “grower” selling directly to the consumer is as old as
the town square produce market. By using the global Internet economy,
independent distribution, fanzines, live concert sales, etc., savvy independent
artists are taking matters into their own hands by learning the inner workings
of the music business. In short, artists are being forced to take business matters
more seriously in order to reap the fruits of their labor and craft—something
that has never been and never will be an easy task.
Online Distribution
On the subject of online distribution, besides the important fact that an artist’s
music has the potential to reach a large fan base, the best part about down -
loadable media is that there’s no expensive physical media to manufacture and
distribute. Gone are the days when you have to give up a closet or area in your
basement for storing CDs and LPs. However, the ﬂipside to this is that the artist
will need to research and understand their online distribution options, needs
and legal responsibilities before signing to a download service.
FIGURE 21.7
The various stages in the
plating and pressing
process.

Product Distribution
UPLOADING TO STARDOM
In this day of surﬁng and streaming media off the Web, it almost goes without
saying that the web has become the most important and effective marketing
tool for the musician and labels alike. It allows us to cost-effectively upload
and promote our songs, projects, promotional materials, touring info and liner
notes and distribute them to mass audiences. However, long before the recording
and mix phases of a project have been completed (assuming that you’re also
doing your business and promotion homework), the next and possibly most
important steps to take are:
n Target your audience: Who are your fans and what is your message (music,
communication style, personal brand)?
n Create a “presence”: Develop a web marketing and music distribution
presence and then ﬁnd the best ways to get that message out to your fans.
n Develop a social network system to interact with the fans (if that’s what
you want).
n Broadcast your music: Using terrestrial radio or TV, podcasts, YouTube
videos, you can expand your audience reach.
n Perform live: This can be done on the stage or on the web (in a live or
YouTube live-cast platform).
n Distribute your music: Using a carefully chosen online distributor you can
create a sales presence that is uniquely your own, or you could use a single
distributor that releases your music to most or all of the major online
distribution networks—or both. In this day and age, it’s easy to make the
world your musical oyster.
n BUT—the trick is to get heard above the millions of others that are also
vying for everyone’s attention—that’s the hard part that requires talent,
luck, tenacity and connections.
From a technical standpoint, mastering for the Internet can either be
complicated; requiring professional knowledge and experience, or it can be a
simple and straightforward process that can be carried out from any computer.
It’s a matter of meeting the level of professionalism and development that’s
required by you and your audience’s needs.
Generally, it’s a good idea to keep the quality as high as possible. Most online
distribution companies will offer guidelines for accepted sound ﬁle, sample and
bitrate types and/or will re-encode the sound ﬁle to match their own internal
codec rates and format (making it a simple easy-to-upload process).
One of the more important considerations that should be made before you
start the upload process is to decide upon the proper metadata for your project,
songs or song. Metadata (or tagged information) is the inputting of media
information relating to:
n Project Name
n Artist Name
566

n Song Names
n Music Genre
n Publisher Info
n Copyright Info, etc.
It’s important that you enter this data correctly and with forethought, as the
distribution system will use this information within their search database and
for ﬁnding your music in their system, on the Web, etc. Quite simply, inaccurate
metadata will bury your hard work in the haystack like a proverbial needle (that
won’t get played).
BUILD YOUR OWN WEBSITE
If you build it, they will come! This overly simplistic concept deﬁnitely doesn’t
apply to the web. With an ever-increasing number of dot-whatevers going online
every month, expecting people to come to your personal or music site just
because it’s there simply isn’t realistic. Like anything that’s worthwhile, it takes
connections, persistence, a good product and good ol’-fashioned dumb luck to
be seen as well as heard! If you’re selling your music, T-shirts or whatever at
gigs, on the streets and to family and friends, cyberspace can help increase sales
by making it possible (and even easy) to get your band, music or clients onto
several independent music websites that offer up descriptions, downloadable
samples, direct sales and a link that goes directly to your main website. Such a
site could deﬁnitely help to get the word out to a potentially new public—and
to help educate your audience about you and your music.
Of course, artists aren’t the only ones that have websites. Most folks in ﬁelds
involving the crafts have a site; heck, even some studios have sites for their
studio pets or mascots. Here are a few general guidelines that help your website
work its magic for you and/or your organization.
n Make your site a central hub: It’s always a good idea to direct your social
network friends and fans to your personal or band page. This helps to
inform them about your upcoming projects, older ones that are for sale,
current goings-on in your personal and professional life, etc.
n Give it a strong, uncluttered main page: Give your site a simple front page
that makes a statement about you and your work. From there, they can
dig deeper and be drawn into “all things you.”
n Keep your site simple and straightforward: Try to make your site navigation
easy to use and understand. Don’t use problematic animations or
programming that can easily confuse your audience.
n Make your site personal: Speak directly to your fans: Let your fans know
what makes you tick, what you’re up to, as well as what’s happening in
your professional life.
n Keep it up-to-date: Nothing’s worse than going to a site, only to see that
there’s been no activity on it. Nothing says “I don’t care” like a neglected
site.
567
Product Manufacture and Distribution  CHAPTER 21

Product Distribution
568
THOUGHTS ON BEING (AND GETTING HEARD) IN CYBERSPACE
In the past, there were smaller, separate stores for meats, cheese, candy, etc.,
and the milk would be delivered to your doorstep. It was all pretty much kept
on a smaller, personable scale. Now, in the age of the supermarket, where
everything is wholesaled, processed, packaged and distributed to a single
clearinghouse there are more options, but the scale is so large that older folks
can only shop there with the aid of a motorized shopping cart.
For more than six decades, the music industry has largely worked on a similar
principle: Find artists who’ll ﬁt into an existing marketing formula (or, more
rarely, create a new marketing image), produce and package them according to
that formula and put tons of bucks behind them to get them heard and
distributed, and then put them on all of the Mom ‘n’ Pop Music store shelves
everywhere. This was not a bad thing in and of itself; however, for independent
artists the struggle has been, and continues to be, one of getting themselves
heard, seen and noticed—without the aid of the well-oiled mega-machine. With
the creation of cyberspace, not only are established record industry forces able
to work their way onto your desktop screen (and into your multimedia speakers),
but independent artists now also have a huge medium for getting heard. Through
the creation of dedicated websites, search engines, links from other sites, and
independent music dot-coms, as well as through creative gigging and marketing,
new avenues have begun to open up for the web-savvy independent artist
(Figure 21.8). The only trick is to connect with you fans, so you can get noticed
in that great music supermarket.
FIGURE 21.8
Getting the artist’s work 
out to the masses is hard
work. Here’s my chance to
put my best foot (or music
site www.davidmileshuber.
com) forward. Moral of the
story? Never pass up an
opportunity!
STREAMING
Although music downloads are one of the strongest-growing markets in the
music industry, another market that’s also showing huge growth potential is
the online music and media streaming market. Streaming refers to the concept
that music and video content exists only in the cloud (on web servers) and is
available for listening by the subscriber (for a monthly fee) on a 24/7 basis.
The major difference here is that the listener has no ownership over the music
and can’t download the media ﬁles directly; we’re simply licensed to access and
listen to them.

Companies like Pandora, Amazon Prime and Apple have been working to
perfect a business model that allows the customer to feel comfortable with paying
a monthly fee for access to their music database. One of the greatest and highly
publicized problems is the notoriously low fees that are paid out to musicians
and music labels for the use of their content.
As with downloadable distribution, it’s important that the music content’s
metadata be correctly thought out and entered—in fact, it’s the metadata aspect
of the music that allows the streaming process to work in the ﬁrst place. Again,
inaccurate metadata will bury your hard work in the haystack like a proverbial
needle (that simply won’t get played).
FREE STREAMING SERVICES
Of course, in this day and age when music is literally and inescapably everywhere,
the most popular streaming service is free—of course, we’re talking about
YouTube. Known for its vast resource library of video material, YouTube is also
one of the most (if not the most) popular ways for fans to connect with their
favorite artist or group’s music. How much does the artist get paid for these
free online efforts? Well nothing, or practically nothing.
Internet Radio
Due to the increased bandwidth of many Internet connections and improve -
ments in audio streaming technology, many of the world’s radio stations have
begun to broadcast on the web. In addition to offering a worldwide platform
for traditional radio listening audiences, a large number of corporate and
independent web radio stations have begun to spring up that can help to
increase the fan and listener base of musicians and record labels. Go ahead, get
on the web and listen to your favorite Mexican station, catch the latest dance
craze from Berlin, or chill to reggae rhythms streaming on an island breeze.
MONEY FOR NOTHIN’ AND THE CHICKS . . .
As you might expect, making a living in today’s online marketplace is as difﬁcult
as it ever was. Sure, it’s easy to get your material out there, but getting heard,
seen and surfed above the digital fray requires a great deal of knowledge,
dedication, artistry, time and money. When you factor in the reality that income
on sales for digital streaming can be relatively meager (this is often true with
digital download, as well) it’s no wonder that most popular artists turn to
touring, merchandise and promotional associations to keep their business aﬂoat.
Even with a huge fan base and media buying/streaming public, it can be a
difﬁcult business that requires a great deal of savvy and determination.
Legal Issues
Of course, the subject of legal rights is way beyond the scope of this book, but
in this day and age of self-publishing and distribution, it’s deﬁnitely worth a
mention.
569
Product Manufacture and Distribution  CHAPTER 21
Internet Radio 

Product Distribution
Obviously, the ﬁrst place to begin diving into the complex and (more often
than not) misunderstood subject of music law is by researching the topic on
the web and by reading any number of books that are out on the subject. Keep
in mind that any info in this book and within any other printed materials on
the subject will simply be an opinion and guide. Each artist has his or her own
speciﬁc legal needs and requirements that can range from being simple (with
answers that are stated in easy-to-understand terms) to extremely complex
(having twists and turns that can send the most seasoned music executive to
the local pub for lubrication).
This last sentence hints at the question about whether or not you need a music
lawyer. Again, it depends upon the situation and your speciﬁc needs. If you’re
uploading your own self-published music into a distribution service and the
terms are fairly straightforward, then you will want to take the time to carefully
read the site’s ﬁne print, so as to see if there are any pitfalls or snags that might
jeopardize your ownership of the publishing rights or other unforeseen events.
In this case, you could be just ﬁne researching this legal ﬁne print on your own,
with the help of the web and possibly with trusted advice. If things are more
complicated, for example, if a record label will be acting as the publisher and
the music’s “writer” percentage is split between the band, the lyricist and the
producer, then, you’d deﬁnitely be wise to sit down with a trusted legal source
to hammer out the details that are equitable and free of confusion. In any case,
care and time should “always” be taken before signing your name on the dotted
line.
ROYALTIES AND OTHER BUSINESS ISSUES
In this “business” of music, one of the goals is to get paid for your hard-earned
work. With the added complexities of having so many options for getting an
artist’s music out to the world, come the added complexities for calculating,
comprehending and collecting the cash. Again, an exhaustive overview of the
topic of the music business is far beyond the scope of this book. Fortunately,
many books and on-line resources have been written about “The Biz.” However,
care must be exercised and often counsel (which can come in many forms) can
be sought to help guide you through these enticing and perilous waters. As
always, caveat emptor!
Let’s start here:
n If you are the writer of an original composition and have not signed with
a publishing company then you own the publishing (and all rights) to
that composition.
n If you have signed with a publishing company (one that you or the band
hire to “hopefully” take care of the everyday inner working of collecting
funds and running your business, while you’re busy making and
performing music), then the publishing rights to a composition (and thus
the paid out funds) will traditionally be split between the writer and the
publishing company 50/50.
570

n The “Writer” can be a single artist, singer/songwriter, members of a band
(equal split), a single member of a band (possibly the one who originally
wrote the song) or it can be shared between the lyricist/composer—in short,
it is open to be deﬁned in any way that the production team sees ﬁt.
n A contract is a written legal agreement that can also be written in almost
any way—it is always negotiable before it is executed (signed). If there are
parts that do not serve your needs and cannot be agreed upon, you
probably should seek counsel and think twice (or more times) before
signing.
OWNERSHIP OF THE MASTERS
If you are the artist/composer/chief-bottle-washer of a project and are not signed
to a publisher, then you are the owner of your recordings. Once you pass
through the gates into the realm of dealing with music publishers and record
labels, this can change. Contracts can be worded in such ways that the actual
ownership of the “phonorecord”, that’s to say the actual recorded production,
can be transferred to that entity for a speciﬁed period or forever. In these
situations, you might want to read the print “very” carefully and/or seek counsel.
REGISTERING YOUR WORK
Regarding copyright ownership, the truth is, that once you’ve written or recorded
an original music work, you then own the copyright, however, unless this work
is registered with the Library of Congress, it will be difﬁcult to actually prosecute
and seek damages from someone who you know has copied your music.
Copyrighting your music with the Library of Congress can now be done online
using their eco online registration system (http://www.copyright.gov/eco) and
can involve one of two submission forms:
Form SR
Form SR (http://copyright.gov/forms/formsr.pdf ) is used for registration of
published or unpublished sound recordings, meaning that you are using this
form to register the “phonorecord” or the actual recorded production as it exists
in physical or downloadable media form.
Form PA
Form PA (http://copyright.gov/forms/formpa.pdf) is used to register published
or unpublished works of the performing arts. In the Library’s words: “This class
includes works prepared for the purpose of being “performed” directly before
an audience or indirectly “by means of any device or process.” Works of the
performing arts include: (1) musical works, including any accompanying words;
(2) dramatic works, including any accompanying music; (3) pantomimes and
choreographic works; and (4) motion pictures and other audiovisual works.
571
Product Manufacture and Distribution  CHAPTER 21
FormSR 
FormPA 

COLLECTING THE $$$
Ahhh, the business side of the industry. Being and staying on top of collecting
funds for you, your band or your represented artist isn’t always easy or
straightforward, to say the least.
Let’s start here:
n Direct sales to your fans over a website or at a performance can put $$$
directly into your or the band’s account—very nice.
n Upon uploading your or your band’s music to a music download service
or aggregator (distributor to multiple download services), this company
will collect information as to how you will get paid (i.e., PayPal, direct
deposit, etc.). This is (or should be) fairly straightforward. Care should be
taken to read the percentages and how often these proceeds will be paid
out.
n Once the recording is made and is released to the public, then registration
with a performance rights organization might be in order. PRO’s such as
ASCAP (American Society of Composers, Authors and Publishers), BMI
(Broadcast Music Inc.) and SESAC (Society of European Stage Authors and
Composers) can be found in the U.S. and are used to collect royalties
from music that is paid on terrestrial radio stations, restaurants, retail stores
and the like.
n Soundexchange is a performance rights organization that collects royalties
from music that is played over satellite and Internet radio, cable TV and
spoken word recordings.
n It should be noted that companies (such as CD Baby PRO and TuneCore)
can actually help with signup and collections from several of the above-
mentioned organizations, making the process easier for individuals who
want to follow these collection avenues.
From the above, it’s easy to see that the task of getting paid for your music is
not always a simple task. Many subscribe to the “multiple trickles from many
sources” concept, which holds to the idea that by subscribing to these (and
other sources), the payment returns will be small, but they will come in from
numerous sources in many ways to add up to a sum that could be worth it.
Further Reading
Again, the information in this chapter is simply meant as an introduction. So
much information exists on the subject of product manufacture and (even more
so) on the distribution and business of music, that I urge you to dig deeper, so
as to gain a better understanding and your own personal “edge” on the ever-
changing landscape of the music business.
572
Product Distribution

As we near the end of this book, I’d like to take some time to offer some tips
that can help make a session go more smoothly, both in the project and in the
professional studio environment. Of these, one of the most important insights
to be gained (beyond an understanding of the technology and tools of the trade)
is the fact that there are no rules for the process of recording. This rule holds
true insofar as inventiveness and freshness tend to play a major role in keeping
the creative process of making music alive and exciting. However, there are
guidelines, equipment setup and procedures that can help you have a smoother-
ﬂowing, professional-sounding recording session, or at the very least, help you
solve potential problems when used in conjunction with ﬁve of the best tools
for guiding you toward a successful project:
n Preparation
n Attention to detail
n A creative, open insight
n Common sense
n A good attitude
PREPARATION AND THE DETAILS
You’ve all heard the old adage “The devil’s in the details”? It’s important to
remember that the hallmark of both a good production and a good production
facility rests with the nitty-gritty small stuff that will help you to rise above the
crowd. The glory goes not so much to those who simply do the job, but to
those who take the extra time to get the details of creating a quality project right
(both technically and musically). OK, let’s take some time to look at some of
the details that can help your projects shine. Probably the most important step
toward ensuring that a recording project will become successful and marketable
is careful preparation and planning (Figure 22.1).
573
CHAPTER 22
Studio Tips and 
Tricks

Preparation and the Details
574
By far, the biggest mistake that a musician or group can make is to go into the
studio without practice and preparation, spend a lot of money and time, release
their project on iTunes and YouTube, make a template website—then sit back,
fully expecting an adoring audience to spring out of thin air! It ain’t gonna
happen! Beyond a good dose of business reality and added experience, these
artists will have the dubious distinction of joining the throngs that have simply
been passed over in the ever-present noise of the web. Getting your “product”
heard takes hard work, persistence, marketing and luck.
What’s a Producer and Do You Really Need One?
One of the ﬁrst steps that can help ensure the success of a project is to seek the
advice and expertise of those who have extensive experience in their chosen
ﬁelds. This might include seeking legal counsel for help and advice with legal
matters or business and record label contacts. Another important “advisor” can
come in the form of that all-important title, producer. The producer of a project
can ﬁll one of two roles:
n The ﬁrst type can be likened to a ﬁlm director, in that his or her role is
to be an artistic, psychological and technical guide that can help the band
or artist reach their intended goals of obtaining the best possible song,
album, remix, ﬁlm score, etc. It’s his or her job to stand back and objectively
look at the big picture, and to help organize the various production and
recording stages. Their role can also be to offer up suggestions as to how
to shape and guide the performance, as well as to direct the artist or group
in directions that will result in the best possible ﬁnal product.
n The second type also encompasses the directorial role, but might have the
added responsibilities of being an executive producer. He or she would
be charged with many of the business responsibilities of overall session
budgeting, making arrangements for all studio and session activities,
contracting (should additional musicians and arrangers be needed on the
project), etc. This type of producer might join with a music lawyer to help
ﬁnd the best sales avenue for the artist and negotiate contact relations
with potential record companies or distributors. If you ﬁnd such a person
that ﬁts with your artistic vision count yourself as being very fortunate!
FIGURE 22.1
Workin’ out the kinks
beforehand in the practice
space. (Courtesy of Yamaha
Corporation of America.
www.yamaha.com)

From this, you can see that a producer’s role can be either limited or broad in
scope. This role should be carefully discussed and agreed on long before any
record button is pressed. The importance of ﬁnding a producer that can work
best with your particular personalities, musical style and business/marketing
needs can’t be stressed enough, and ﬁnding the right match can be a rewarding
experience. Here are a few tips to prepare you for the hunt:
n Check out the liner notes of groups or musicians that you love and 
admire. You never know—their producer just might be interested in taking
you on!
n Find a local up-and-coming producer that might be right for your music.
This could help fast-track your reputation.
n Talk with other groups, musicians or even label execs (if you can get in
touch with one). They might be able to recommend someone.
Here are just a few of the questions to ask when searching for a producer:
n Does he or she openly discuss ideas and alternate paths that contribute
to growth and better artistic expression?
n Is he or she a team player, or are the rules laid out in a dictator-like fashion?
n Does the producer know the difference between a creative endeavor and
one that wastes time in the studio?
n Does he or she say “Why?” a lot more often than “Why not give it a try?”
Although many engineers have spent most of their lives with their ears wide
open and have gained a great deal of musical, production and in-studio
experience, it’s generally not a good idea to always assume that the engineer
will or can automatically ﬁll the role of a producer. For starters, he or she will
probably be unfamiliar with the group and their sound, or might not even
appreciate or like their music (although the studio and engineer was hopefully
chosen for their interest in the band/artist’s particular style)! For these and other
reasons, it’s always best to seek out a producer that is familiar with you, your
goals and your style (or is contacted early enough in the game that he or she
has time to become familiar).
Do You Need a Music Lawyer?
It’s important to realize that music in the modern world is a BUSINESS. Once
you get into the phase of getting your band or your client’s band out to the
buying public, you’ll quickly realize just how true this is. Building and
maintaining an audience with an appetite for your product can easily be a full-
time business—one where you’ll encounter well-intentioned people as well as
those who would think nothing of taking advantage of you and/or your client.
Whether you’re selling your products on the street, at gigs, on iTunes or in the
stores through a traditional music label, it’s often wise to “at least” entertain the
idea of seeking out the counsel of a trusted music lawyer. The music industry is
fraught with its own special legal and ﬁnancial language, and having someone
575
Studio Tips and Tricks  CHAPTER 22

Preparation and the Details
on your side that has insight into the language, quirks and inner workings of
this unique business can be an extremely valuable asset. Before we move on, it
should be pointed out that many metropolitan areas have “Lawyers for the Arts”
organizations that regularly offer seminars and events, as well as one-on-one
consultations with artists who are on a tight budget and have need of legal counsel.
A SHORT, STEP-BY-STEP OVERVIEW OF THE 
PRODUCTION PROCESS
Obviously, every recording project will be different. However, as always, there
are general guidelines that can smooth the process of getting your or our band’s
music out as a ﬁnal, marketable product.
As with most things, a road to a successful product is ﬁlled with potholes that
can only be ﬁlled-in with dedication, networking, hard work, seeking out the
advice and/or help of those who are professionals or have successfully walked
the path—and let’s not forget the most important ingredients—talent and blind,
dumb luck!
Beyond these things, here are but a few possible guidelines that can help keep
you on the right path—all the best (especially with the dumb luck thing)!
1 Long Before Going into the Studio
As was emphasized earlier in the chapter (and the book), one of the most
important steps to take when approaching a project that involves a number of
creative and business stages, decisions and risks is preparation. Without a doubt,
the best way to avoid pitfalls and to help get you, your client or your band’s
project off the ground is to discuss and outline the many factors and decisions
that will affect the creation and outcome of that all-important “ﬁnal product.”
Just for starters, here are a number of basic questions that need to be asked
long before anyone presses the “REC” button:
n How are you planning to recoup the production costs?
n Will others share in the costs or ﬁnancial arrangements?
n Do you want to think about pre-sales and/or crowd-funding to help defray
the cost?
n How is it to be distributed to the public? Self-distribution? Digital
download? Indy? Record label?
n Do you need a producer or will you self-produce?
n How much practice will you need? Where and when? Shall the dates be
put on the calendar?
n Should you record it in a band member’s project studio or at a commercial
studio?
n If a project studio is used and it works out, should you mix it at the
commercial studio? Which one and who will do the mix?
576

n Who’s going to keep track of the time and budget? Is that the producer’s
job or will he or she be strictly in charge of creative and contact decisions?
n Are you going to need a music lawyer to help with contacts and contracts?
n Who will be in charge of the artwork and the website?
n Will it be self- or professionally mastered? Who will do the mastering?
n When should the artist or group’s artistic and ﬁnancial goals be discussed
and put down on paper? (Of course, it’s always best to discuss budget
requirements, possible rewards and written contract stipulations as early
as possible in the game!)
These are but “a few” questions that should be asked before tackling a project.
Of course, they’ll change from project to project and will depend on the project’s
scope and purpose. However, in the ﬁnal analysis, asking the right questions
(or ﬁnding someone who can help you ask them) can help keep your project
and career on-track.
2 Before Going into the Studio
Now that you’ve at least asked some or all of the above questions, here’s a to-
do list of tasks that are often wise to tackle well before going into the studio:
n Create a “mission statement” for yourself/your group and the project. This
can help clue your audience into what you are trying to communicate
through your art and music (in general and on this project). This can greatly
beneﬁt your marketing goals. For example, you might want to answer such
questions as these: Who am I/who are we? What am I/are we trying to
communicate through our music? How should the ﬁnished project sound?
What emotions should it evoke?
n Practice, practice and more practice—and while you’re at it, you might
want to record your practices to get used to the process (some of these
tracks could be used on your website as bonus tracks, for the “making 
of” music videos . . . or they might even make it into the ﬁnal master
takes).
n Start working on the project’s artwork, packaging and website ASAP. Do
you want to tackle this yourself, hire a professional or a qualiﬁed friend
who wants to help or could use some extra $$$?
n Copyright your songs. Government forms are readily available for the
copyrighting of your music (identiﬁcation and protection of intellectual
property). The Library of Congress Form PA is used for the registration of
“published or unpublished works of the performing arts.” This class
includes works prepared for the purpose of being “performed” directly
before an audience or indirectly “by means of any device or process.” Works
of the performing arts include: (1) musical works and any accompanying
words; (2) dramatic works, including any accompanying music; (3)
pantomimes and choreographic works; and (4) motion pictures and other
577
Studio Tips and Tricks  CHAPTER 22

Overview of the Production Process
audiovisual works.” In short, it is used to copyright a work that is intended
for public performance or display. Form SR is used for the “registration
of published or unpublished sound recordings. This should be used when
the copyright claim is limited to the sound recording itself, and may also
be used where the same copyright claimant is seeking simultaneous
registration of the underlying musical, dramatic, or literary work embodied
in the “phonorecord.” In other words, it’s used to copyright the recording
itself, while also protecting the underlying performance that is recorded
onto the media. These and other forms can be found at www.copyright.
gov/forms or by searching the Library of Congress at www.loc.gov. Again,
this might be a good time to discuss these matters with a music lawyer.
n Should you wish to use the services of a professional studio during the
recording and/or mixdown phase, it’s ALWAYS wise to take the time to
check out several studios and available engineers in your area. Take the
time to ask about others’ experiences in that studio, listen to tracks that
have come out of it, as well as those that have been recorded by the
engineer. Finding out which studio best ﬁts your style, budget and level
of professionalism is both musically and ﬁnancially an important
decision—the time taken to ﬁnd the best match for you and your music
could be the difference between a happy or potentially not-so-fun
experience.
n “Now” is the time to decide who’ll be the producer and their duties will
be. Will it be a person who is chosen from the outside? Will it be a member
of the group? (If there’s to be no producer on the project, it’s often wise
to pick (or at least consider picking) a spokesman for the group who has
the best production “chops.” He or she could then work closely with the
engineer to create the best possible recording and/or mix.) Alternatively,
production could be done as a group effort. Lastly, in what way will the
producer be paid (as an upfront payment and/or as a percentage of sales)?
Answering these questions now can help avoid emotional disagreements
in the studio.
3 Going into the Studio
Before beginning a recording session (possibly a week or more before), it’s always
good to mentally prepare yourself for what lies ahead by creating a basic checklist
that can help answer:
n What type of instruments and equipment will be needed?
n What the number and type of musicians/instruments are needed for the
session?
n Will any particular miking technique (if any) be used, and where they’ll
be placed?
The best way to do this is for you, your group and the producer (if there is one)
to sit down with the engineer and discuss instrumentation, studio layout,
musical styles and production techniques. This meeting lets everyone know what
578

to expect during the session and lets everyone become familiar with the engineer,
studio and staff. It’s always time well spent, because it will invariably come in
handy during the studio setup and will help get the session off to a good start.
The following tips can also be immensely valuable:
n Record your songs during live gigs or rehearsals. It doesn’t matter if you
record them professionally or not; however, keep the “always press the
record button” adage in mind. If the setup meets basic professional
standards, it’s always possible to import all or part of a “magical” take
into the ﬁnal project.
n You might want to audition the session’s song list before a live audience.
n If possible, work out all of the musical and vocal parts “before” going into
the studio. Unrehearsed music can leave the music standing on shaky
ground; however, leave yourself open to exploring new avenues and
surprises that can be the lifeblood of a magical session.
n Try to leave plenty of time for laying down the ﬁnal vocal tracks. Many a
project has been compromised by spending too much time on “tracking”
the basic instruments and then running short on time and money when
it comes time to lay down the vocals. This almost always leads to increased
tensions and a rushed vocal track. Don’t let this happen to you, as vocals
are often the central focus of a song.
n Rehearse more songs than you plan to record. You never know which
songs will end up sounding best or will have the strongest impact.
n Again, meet with the engineer beforehand. Take time for the producer
and/or group to get to know him or her, so you’ll both know what to
expect on the day of the session.
n Prepare and edit any sequenced, sampled or pre-recorded material
beforehand. In short, be as prepared as possible with your MIDI tools and
toys.
n If it ﬁts the musical style, try working to a metronome (click track) if timing
is an issue. Not using a click track is also totally ok, just be aware of any
timing shortfalls.
n Make sure that the instruments are in good working condition (i.e., bring
new strings).
n Create a checklist of all of the small but important details that can make
or break a session, for example, extension cords, tuners, extra instrument
cords, drum oil, drum tuning lugs, your favorite good luck charm,
comfortable jammies—you name it!
n Take care of your body. Try to relax and get enough sleep before and during
the session. Eat the foods that are best for you (you might bring some
health foods, fruits and plenty of liquids to keep your energy up). Be aware
of your energy levels, so that low- or high-blood sugar problems don’t
become a factor (grrrrrrrr . . .).
n Don’t fatigue your ears before a session; keep them rested and clear.
579
Studio Tips and Tricks  CHAPTER 22

Overview of the Production Process
In short, it’s always a good idea to plan out the session’s technical and musical
arrangement so as to most efﬁciently budget your time and well-being.
Again, it’s always wise to confer with the producer and/or engineer about the
way in which the musicians are to be tracked. Will the tracks be recorded in a
traditional multitrack fashion (with the basic rhythm tracks being laid down
ﬁrst, followed by overdub and sweetening tracks and ﬁnishing with vocals) or
will a different production style work best for your particular group’s taste and
organizational way of doing things? Communicating these details to all those
involved in the session will help smooth the studio setup process and keep a
basic game plan that can help keep the session reasonably on track.
4 Setting Up
Once the musicians have shown up at the studio, it’s extremely important that
all of the technical, musical, and emotional preparation be put into practice in
order to get the session off to a good start. Here are a few tips that can help:
n Show up at the studio on time or reasonably early. At some studios, the
billing clock starts on time (whether you’re there or not). Ask about their
setup policies: Is there another session before yours? Is there adequate
setup time to get prepared? Are there any charges for setup? What is the
studio’s cancellation policy in case of illness or unforeseen things that
could go wrong?
n Use new strings, chords, drumsticks and heads—and bring spares. It’s also
a good idea to know the location, phone number and hours of a local
music store, just in case.
n Tune up before the session starts, and stay in-tune regularly thereafter.
n Don’t use new or unfamiliar equipment (musical, hardware-wise and
especially not software-wise). Taking the time to troubleshoot or become
familiar with new equipment and software can cost you time and money.
The frustration could even result in a lost vibe, or worse! If you must use
a new toy or tool, it’s best to become “very” familiar with it beforehand.
n Take the time to make the studio a comfortable place in which to work.
You might want to adjust the lighting to match your mood ring, lay down
a favorite rug and/or bean bag, turn on your lava love light or put your
favorite stuffed toys on the furniture. Within reason, the place is yours to
have fun with!
5 Session Documentation
There are few things more frustrating than going back to an archived session
and ﬁnding that no information exists as to what instrument patch, mic type,
or outboard effect was used on a session (even one that’s DAW-based). The
importance of documenting a session in a separate written document or within
your DAW’s notepad apps can’t be overemphasized. The basic documentation
580

that relates to the who, what, where and when of a recording, mixdown,
mastering and duplication session should include such information as:
n Artists, engineers, musical and support staff who were involved with the
project (addresses, contracts, check photos or scans, studio invoices—
anything of importance)
n Session calendar dates and locations
n Individual song tempos, signatures, etc.
n Mic choice and placement (this might include photos, ﬂoor-plan drawings,
etc. for future overdub reference)
n Outboard equipment types and their settings (place these notes within
the DAW scratchpads or take photos and place them in the session
directory)
n Plug-in effects and their settings or general descriptions (you never know
if they’ll be available at a future time, so a description can help you to
duplicate it with another app). If it’s a hard-to-duplicate or specialized
effect, you might consider printing the effect onto its own, separate track
n Again, remember to record and save all MIDI-related tracks. Documenting
the instrument, patch and any other info can be really helpful. Printing
any MIDI instruments as an audio track will often make things easier,
later within the mix
To ease this process, the artist or producer might pull out a camera or camera
phone and start snapping pictures to document the event for prosperity as well
as for technical reasons. Also, it’s often a wise idea to bring along a high-quality
video camera to quietly document the setup or actual session for your fans as
extra “behind-the-scenes” online video content.
The more information that can be archived with a session (and its backups),
the better the chance that you’ll be able to duplicate the session in greater detail
at some point in the future. Just remember that it’s up to us to save and
document the music of today for the fans and future playback/mix technologies
of tomorrow. Basic session documentation guidelines can be found in the
Guidelines & Recommendation section atwww.grammy.org/recording-academy/
producers-and-engineers.
6 Recording
It’s obviously a foregone conclusion that no two recording sessions will ever
be exactly alike. In fact, in keeping with the “no rules” rule, they’re often radically
different from each other. During the recording session, the engineer watches
the level indicators and (if necessary) controls the faders to keep from getting
overload distortion. It’s also an engineer’s job to act as another set of production
ears by listening for both performance and quality factors. If the producer
doesn’t notice a mistake in the performance, the engineer just might catch it
and point it out. The engineer should always try to be helpful and remember
581
Studio Tips and Tricks  CHAPTER 22

Overview of the Production Process
that the producer and/or the band will have the ﬁnal say, and that their ﬁnal
judgment of the quality of a performance or recording must be accepted.
From both the engineer’s and musician’s standpoint, here are a few additional
pointers that can help the session go more smoothly:
n It’s always best to get the right sound and vibe onto disk or tape during
the session. If you need to do another take, do it! If you need to change
a mic, change it. Getting the right sound and vibe onto the track will almost
always result in less stress and a better ﬁnal product, rather than trying to
“ﬁx in the mix” at a later time.
n You might want to record “run-throughs,” just in case it’s magical.
n Know when to quit! If you’re pushing too hard or are tired, it’ll often
show.
n Technology doesn’t always make a good track; feeling, emotion and
musicality always does.
n Beware of adding extra parts or tracks onto a piece that doesn’t need it.
Remember, too much can sometimes be simply too much! Musicians and
techno-geeks alike often don’t know when to say “it’s done . . . let’s move
on.”
n Leave plenty of time for the vocal track(s). It’s not uncommon for a group
to spend most of their time and budget on getting the perfect drum or
guitar sound. It takes time and a clear focus to get the vocals right—be
prepared for Murphy’s law.
n If you mess up on a part, keep going, you might be able to ﬁx the bad
part by punching-in. If it’s really that bad, the engineer or producer will
hopefully stop you.
In his EQ Magazine article “The Performance Curve: How Do You Know Which
Take Is the One?” my buddy Craig Anderton laid out his experiences of how
different musicians will deal with the process of delivering a performance over
time. Being in front of a mic isn’t always easy, and we all deal with it differently.
Here’s a basic outline of his ﬁndings:
n Curves up ahead: With this type of performer, the ﬁrst couple of takes are
pretty good, then start to go downhill before ramping back up again, until
they hit their peak before going downhill really fast.
n The quick starter: This type starts out strong and doesn’t improve over time
in later performances. Live performers often fall into this category, because
they’re conditioned to get it right the ﬁrst time.
n The long ramp-up: These musicians often take a while to warm up to a
performance. After they hit their stride, you might get a killer take or a
few great ones that can be composited together into the perfect take.
n Anything goes: This category can vary widely within a performance. Often,
snippets can be taken from several takes into a single composite. You want
582

to record everything with this type of performer, because you just never
know what gem (or bad take) you’ll end up with.
n Rock steady: This one represents the consummate pro that is fully practiced
and delivers a performance that doesn’t waver from one take to the next;
however, you might record several takes to see which one has the most
feeling.
From the above examples, we can quickly draw the obvious conclusion that
there are all types of performers, and that it takes a qualiﬁed and experienced
producer and/or engineer to intuit just which type is in front of the mic and
to draw the best possible performance from him or her.
7 Mix-Down
Many of the same rules of preparation and taking good care of your ears and
yourself apply during the session’s mixdown phase. Here are a few tips:
n Regarding monitoring, it’s often a good idea to use reference monitors
that you trust. As such, it’s common practice for a mix engineer and/or
artist to request their favorite speakers or to bring their own into the studio
for the ﬁnal mix.
n Unlike during the 1970s, when excruciatingly high SPLs tended to be the
rule in most studios, recent decades have seen the reduction of monitor
levels to a more moderate 75- to 90-dB SPL. A good rule of thumb is that
if you have to shout to communicate in a room you’re probably monitoring
too loudly. From time to time, you can always jack it up to 11 to check
the mix at higher volumes and then turn it back down to a moderate level.
Taking occasional breaks isn’t a bad idea either.
n Listen on several speaker types—at home, in your car, on your iPod/phone.
Jot down any thoughts and comments that might come in handy, should
you need to go back and make adjustments. Strangely enough, leaving
the room and listening to the mix from further away (with the door open)
will often give clues as to how a mix will sound.
n If you have the time (i.e., are working in your own project room), you
might want to take a week off, and then go back and listen to the mixes
with a fresh perspective. You’d be surprised how much you’ll miss when
you’re under pressure.
8 Mastering
Now, comes the question—Do I/we master it ourselves or do we farm it out to
a pro?
n Read Chapter 20 on mastering. There are a lot of issues to think about!
n Find a mastering engineer that ﬁts your budget and style of music.
n If you’re going to do your own mastering, I’d urge you not to take the job
lightly. It takes serious time, practice and attention to detail to get the
583
Studio Tips and Tricks  CHAPTER 22

Overview of the Production Process
job done right. If you’re up to it, this can be a really rewarding and
important skill to have (it took me 2–3 years of concerted effort to feel
truly comfortable with mastering my own music). No matter how long
your learning/experience curve is—I simply urge you not to take this 
phase lightly.
n If you don’t have the time, skill or interest to master yourself, then take
the time to ﬁnd a mastering engineer that’s best for you and your music.
I’d also urge you to communicate with him or her to ﬁnd out their needs
as to how to best deliver the media to them, so they can do their job
without serious compromises.
9 Backup and Archive Strategies
The phrase “nothing lasts forever” is especially true in the digital domain of
lost 1’s and 0’s, damaged media and dead hard drives . . . you know, the “Oh
$@#%!” factor. It’s a basic fact that you never quite know what lies around the
techno bend, and it’s extremely important that you protect yourself as much as
is humanly possible against the inevitable. Of course, the answer to this digital
dilemma is to back up your data in the most reliable (and redundant) way
possible. Hardware and program software can (usually) be replaced; on the other
hand, whenever un-backed, valuable session sound ﬁles are lost—they’re lost!
Backing up a session can be done in several ways. Here are a few tips that can
help you avoid data loss:
n As you might expect, the most straightforward backup system is to copy
the session data, in its entirety, to the most appropriate media (most
commonly onto one or more hard drives).
n In the longer run (5 or more years), the most ironclad way to back up the
track data of a session is to print each track as its own .wav or .aif ﬁle.
Each track should always be recorded or exported as a contiguous ﬁle that
ﬂows from the beginning of the session (00:00:00:00 or appropriate
beginning point) to the end of that particular track. In this way, the
individual track ﬁles can be loaded into any type of DAW, at the beginning
point, for future processing and mixdown.
n In the case of a speciﬁc processing effect, you might want to save two
copies of the track—one that contains the original, unaltered sound and
one that contains the effected signal (or simply the effect alone).
n Those who want additional protection against the degradation of unproven
digital media may also want to back each track (or group of tracks) to the
individual tracks of a multitrack analog recorder (often major labels will
stipulate that this be done in the band’s contract for an important project).
n For those sessions that contain MIDI tracks, you should always keep these
tracks within the session (i.e., don’t delete them). These tracks might come
in very handy during a remix or future mixdown.
584

585
Studio Tips and Tricks  CHAPTER 22
n Whenever possible, make multiple backups and store them in separate
locations. Having a backup copy in your home (or bank vault) as well as
in the studio can save your proverbial butt in case of a ﬁre or any other
unforeseen situation. Remember the general backup rule of thumb: Data
is never truly backed up unless it’s saved on three drives (I would add—
and in two places)!
10 Household Tips
Producers, musicians, audio professionals and engineers spend a great deal of
time in the control room and studio. It only makes sense that this environment
should be laid out in a manner that’s aesthetically, functionally and acoustically
pleasing from a comfort, feng shui and cleanliness point of view. Creating a
good working environment that’s conducive to making good music is the goal
of every professional and project studio owner. Beyond the basics of creating a
well-designed facility from an acoustic and electronic standpoint, a number of
basic concepts should be kept in mind when building or designing a recording
facility, no matter how grand or humble. Here are a few helpful hints:
n Given the fact that an engineer spends a huge amount of time sitting on
his or her bum, it’s always wise to invest in both your and your clients’
posture and creature comforts by having comfortable, high-quality chairs
around for both the production team and the musicians (Figure 22.2a).
Of course, a functional workspace desk (Figure 22.2b) is always a big help
to those of us who spend huge amounts of time sitting in front of the
DAW. Posture, easy access and functionality, baby—it can’t be stressed
enough! Even something as simple as having an exercise ball in the room
can really help, when you’ve been sitting in a regular chair for too long.
n Velcro™ or tie-straps can be used to organize studio wiring bundles into
groups that can be laid out in ways that reduce clutter, improves
organization (by using color-coded straps) and makes the studio look more
professional.
n Most of us are guilty of cluttering up our workspace with unused gear,
papers—you know, junk! I know it’s hard, but a clean, uncluttered working
environment tells your clients a lot about you, your facility and your work
habits.
FIGURE 22.2
Functional and comfortable
furniture is a must in the
studio. (a) The venerable
Herman Miller Aeron® 
chair. (Courtesy of Herman
Miller, Inc., www.herman
miller.com) (b) The Argosy
Halo desk. (Courtesy of
Argosy Console, www.
argosyconsole.com)

Overview of the Production Process
586
n Unused cables, adapters and miscellaneous stuff can be sorted into boxes
and stacked for easy storage.
n Important tools and items that are used every day (such as screwdrivers,
masking tape or markers) could be stored in a rack-mounted drawer that
can be easily accessed without cluttering up your space—don’t forget to
pack a reliable LED ﬂashlight (your phone’s ﬂash or screen display will
also work in a pinch).
n Portable label printers can be used to identify cable runs within the studio,
identify patch points, I/O strip instrumentation . . . you name it.
11 Personal Skills and Tools
By now it should be painfully obvious that getting into music production, audio
production and all things recording, takes hard work, perseverance, blood,
sweat, tears and laughter. For every person who builds a personal career in audio
production, a large number won’t make it. There are a lot of people waiting in
line to get into what is perceived by many to be a glamorous biz. So, how do
you get to the front of the line? Well, folks, just as the best way to get to Carnegie
Hall is to practice—here are some key skills that are practically requirements:
n A ton of self-motivation
n Good networking skills
n A good, healthy attitude
n An ever-present willingness to learn
n The realization that “showing up is always huge!”
The business of art (the techno-arts of recording and music production being
no exception) is one that’s generally reserved for self-starters. Even if you get a
degree from XYZ College or recording school, there’s absolutely no guarantee
that anyone will be knocking on your door asking you to work for them. More
often than not, it takes a large dose of perseverance, talent, personality and luck
to make it. In fact, one of the best ways to get into the biz is to get down on
your knees and knight yourself on the shoulder with a sword (ﬁguratively 
or literally—it doesn’t matter) and say: “I am now a ____________!” Whatever
it is you want to be, simply become it . . . Shazammm! Make up a business
card, start a business, begin contacting artists to work with and begin making
the ﬁrst steps toward becoming the artist or businessperson that you want 
to be. Again—this is an industry of self-starters. If they (whomever “they” are)
won’t give you a job, one strong option is to get out and create one for your-
self.
There are many ways to get to the top of your own personal mountain. You
could get a diploma from a school of education or from the school of hard
knocks (it usually ends up being from both), but the goals and the paths are
up to you. Like a mentor of mine always said: “Failure isn’t a bad thing—not
trying is!”

Another huge part of the success equation lies in your ability to network with
other people. Like the venerable expression says: “It’s not [only] what you
know—it’s who you know.” Maybe you have an uncle or a friend in the business,
or a friend who has an uncle—you just never know where help might come
from next. This idea of getting to know someone, who knows someone else,
who knows someone else, is what makes the business world go around. Don’t
be afraid to put your best face forward and start meeting people. If you want
to play gigs around your region (or beyond), get to know a promoter or venue
manager and hang out without being too much in the way. You never know—
the music maven down the street might know someone who can help get your
feet in the proverbial door. The longer you stick with it, the more people you’ll
meet, thereby making a bigger and stronger network than you thought would
be possible.
Like a close buddy of mine always says, “Showing up is huge!” It’s the wise
person who realizes that being in the right place at the right time means being
at the wrong place hundreds and hundreds of times. You just never know when
lightning is going to strike—just try to be prepared and standing under the right
tree when it does.
Here are some more practical and immediate tips for musicians:
n Build a personal and/or band website: Making a great social network presence
and/or creating your own personal site helps to keep the world informed
of your gigs, projects, bio and general goings-on.
n Build a relationship with a music lawyer: Many music lawyers are open to
building relations that can be kicked into gear at a future time. Take the
time to ﬁnd a solicitor who is right for you. Does he or she understand
your personal music style? If you don’t have the bucks, is this person willing
to work with you and your budget, as your career grows?
n The same questions might be asked of a potential manager. This symbiotic
relationship should be built with care, honesty and safeguards (which is
just one of the many reasons you want to know a music lawyer).
n Copyright your music: Always protect your music by registering it with the
Library of Congress. It’s easy and inexpensive and can give you peace of
mind about knowing that the artistic property that you’re sending out into
the world is protected. Go to www.copyright.gov for more information
(www.copyright.gov/forms). Additional organizations also exist that can
help you get paid with on-air and online royalties.
n On a personal note as a musician, I’ve come to realize that making music
is about the journey—not necessarily the goal of being a star, or being the
big man/woman on campus. It’s about building friendships, collab -
orations, having good and bad times at gigs—and, of course, it’s all about
making music.
587
Studio Tips and Tricks  CHAPTER 22

Overview of the Production Process
588
12 Protect Your Investment
When you’ve spent several years amassing your studio through hard-earned
sweat-equity and bucks, it’s only natural that you’ll want to take the necessary
precautions to protect your investment.
Obviously, the best way to protect your data is through a rigorous and
straightforward backup scheme (the general rule is that something isn’t backed
up unless it’s saved in three places—preferably with one of the backups being
stored off-site). However, you’ll also want to take extra steps to protect your
hardware and software investments as well, by making sure that they’re properly
insured.
The best way to start the process of properly insuring your studio is to contact
your trusted insurance agent or broker. If you don’t have one, now’s the time
to get one. You might get some referrals from friends or people in your area
and give them a call, set up some appointments and get several quotes.
If you haven’t already done so, sit down and begin listing your equipment, their
serial numbers and replacement values. Next, you might consider taking pictures
or a home movie of your listed studio assets. These steps will help your agent
come up with an adequate replacement plan and will come in handy when
ﬁling a claim, should an unfortunate event occur. Being prepared isn’t just for
the Boy or Girl Scouts.
13 Protect Your Hardware
One of the best ways to ensure against harmful line voltage ﬂuctuations (both
above and below their nominal power levels) is to use a quality power
conditioner or an adequately powered uninterruptible power supply (UPS). In
short, a quality power conditioner works by providing a regulated power voltage
level that works within a speciﬁed tolerance that will protect your sensitive studio
equipment (such as a computer, bank of effects devices, etc.) with a clean and
constant voltage supply. It will also protect against power spikes in the mains
line that could damage your system.
14 Update Your Software
Periodic software updates might help to solve some of those annoying problems
that you’ve been dealing with in the studio. Often, the software that’s shipped
with new pieces of equipment will be quite out-of-date by the time it reaches
you. For this reason, it’s good to check the Web regularly to see if there’s a
newer version that can be loaded at the outset or periodically over the course
of the software’s life.
I will say, however, sometimes it’s wise to research an update (particularly if
it’s an important one)—it’s a rare update that will rise up and bite you and
your system in the butt—but it does happen (be especially careful of this when
traveling or when starting a new and important project).

15 Read Your Manuals
Unfortunately, I really am not a big reader, but taking an occasional glance at
my software manuals helps me get a better understanding of the ﬁner points
of my studio system. There are always new features, tips and tricks to be learned
that you never would have thought of otherwise.
16 A Word on Professionalism
Before we close this chapter, there’s one more subject that I’d like to touch on—
perhaps the most important one of all—professional demeanor. Without a doubt,
the life and job of a typical engineer, producer or musician isn’t always an easy
one. It often involves long hours and extended concentration with people who,
more often than not, are new acquaintances. In short, it can be a high-pressure
job. On the ﬂip side, it’s one that’s often full of new experiences, with demands
that change on almost a daily basis, and often involves you with exciting people
who feel passionately about their art and chosen profession.
It’s been my observation (and that of many I’ve known) that the best qualities
that can be exhibited by anyone in “The Biz” are:
n Having an innate willingness to experiment
n Being open to new ideas (ﬂexibility)
n Having a sense of humor
n Having an even temperament (this often translates as patience)
n Being open to communicating with others
n Being able to convey and understand the basic nuances of people from
all walks of life and with many different temperaments
The best advice I can possibly give is to be open, be patient and above all, BE
YOURSELF. Also, be extra patient with yourself. If you don’t know something,
ask. If you made a mistake (trust me, you will; we all do), admit it and don’t
be hard on yourself. It’s all part of the process of learning and gaining experience.
This last piece of advice might not be as popular as the others, but it might
come in handy someday: It’s important to be open to the fact that there are
many, many aspects to music and sound production, and you may ﬁnd that
your career calling might be better served in another branch of the biz (other
than the one that you’re studying or striving for). That’s totally OK! Change is
an important part of any creative process—that and taxes are the only constants
you can count on!
IN CONCLUSION
Obviously, the above tips are just part of an ever-changing list. The process of
producing, recording and mixing in any type of studio environment is an
ongoing, lifelong pursuit. Just when you think you’ve gotten it down, the
589
Studio Tips and Tricks  CHAPTER 22

technology or the nature of the project changes under your feet—hopefully,
you’ll be the better for it and will be open to learning a new process or piece
of gear.
Far more than just the technology, the process of coming up with your own
production style and applying these tools, toys and techniques in your own
way is what makes us artists—whether you’re in front of the proverbial glass or
behind it. Over time, your own list of studio tips and tricks will grow. Take the
time to write them down and pass them on to others, and be open to the advice
of your friends and colleagues. Use the trade mags, conventions and the Web
to lead you to new ideas. This way, you’re opening yourself up to new insights
to using the tools of your profession and to ﬁnding new ways of doing stuff.
Learning is an ongoing process—have lots of fun along the way!
590
In Conclusion

I’m sure you’ve heard the phrase “Those were the good old days.” I’ve usually
found it to be a catchall that refers to a time in one’s life that had a sense of
great meaning, relevance and all-around fun. Personally, I’ve never met a group
of people who seem to bring that sense of relevance and fun with them into
the present more than music and audio professionals, enthusiasts and students.
The fact that many of us refer to the tools of our profession as “toys” says a lot
about the way we view our work. Fortunately, I was born into that clan and
have reaped the beneﬁts all my life—the here and now are always, my “good
ol’ days”!
Music and audio industry professionals, by necessity, tend to keep their noses
to the workaday grindstone, however market forces and personal visions often
cause them to keep one eye focused on future technologies. These can be new
developments (such as advances in digital, touch and audio technologies),
rediscovering retro trends and techniques that are decades old (such as the
reemergence of tube technology and the reconditioning of older devices that
sound far too good to put out to pasture), or future technologies that excite the
imagination. Such is the time paradox of a music and audio professional, which
leads me to the book’s ﬁnal task: addressing the people and technologies in the
business of sound recording in the past, present and future.
YESTERDAY
I’ve always looked at the history of music and sound technology with a sense
of awe and wonder, although I really can’t explain why. Like so many in this
industry, I tend to get shivers when I see a wonderful old tape machine or an
original tube compressor. For no reason whatsoever, I get all giggly and woozy
when I read about some of the earlier consoles that were used to record industry
greats, see pictures from past recording sessions (Figure 23.1) or see an original
Ampex 200 (the ﬁrst commercially available professional tape machine). I
experience this same sense of awe when I read about my personal historical
heroes such as:
591
CHAPTER 23
Yesterday, Today 
and Tomorrow

Yesterday
592
n Nikola Tesla, who introduced us to alternating current, X-ray, the AC
electric motor, the laser—the full list is way too long to list here. You’ll
notice that I don’t put Edison in this class, as I found him to be a far
better businessman and exploiter than an inventor.
n Alan Dower Blumlein, inventor of stereo mic techniques, the 45º/45º
stereo record, the television scan tube, radar and more.
n John (Jack) T. Mullin (Figure 23.2a), who stumbled across a couple of
German magnetophones (Figure 23.2b) at the end of World War II and
was smart enough to send them back to San Francisco. With the help of
Alexander M. Poniatoff (founder of Ampex) and Bing Crosby (Figure
23.3a), Jack and his machines played a crucial role in bringing the magnetic
tape recorder into commercial existence.
n Mary C. Bell (Figure 23.3b) who was probably the ﬁrst woman sound
engineer, along with her husband, Bill Swartau, who was himself an audio
pioneer.
. . . and the list goes on.
FIGURE 23.1
Universal Recording,
Chicago. (a) Bill Putnam’s
prototype mixing console at
Universal Recording, 1957.
(Courtesy of Universal
Audio, www.uaudio.com) 
(b) Nat King Cole clowning
around with engineer Bill
Putnam (out of shot, in the
control room at left).
(Courtesy of Universal
Audio, www.uaudio.com)
FIGURE 23.2
The first tape machines
outside of Deutschland. 
(a) John (Jack) T. Mullin
(pictured left) with the
two original German
magnetophones. (Courtesy
of the late John [Jack] T.
Mullin) (b) Close-up of one
of the two original German
magnetophones.

593
Yesterday, Today and Tomorrow  CHAPTER 23
FIGURE 23.3
The early analog period. 
(a) Bing Crosby with the first
Ampex Model 200 machines
that allowed for tape editing
to splice different takes
together in post-production.
(b) Mary C. Bell in NBC’s
dubbing room #1 (April,
1948) inspecting broadcast
lacquer discs for on-air
programs. (Courtesy of Mary
C. Bell)
FIGURE 23.4
The analog professional
recording studio. (a) Gilfoy
Sound Studios, Inc., circa
1972. Notice that the room
is set up for quad surround!
(Courtesy of Jack Gilfoy,
www.jackgilfoy.com) 
(b) Little Richard at the
legendary LA Record Plant’s
Studio A (circa 1985)
recording “It’s a Matter of
Time” for the Disney film
Down and Out in Beverly
Hills. (Courtesy of the
Record Plant Recording
Studios, photo by Neil
Ricklen)
Every once in a techno-blue moon, major milestones come along that affect
almost every facet of information and entertainment technology. Such
milestones have ushered us from the Edison and Berliner era of acoustic
recordings, into the era of broadcasting, electrical recording and tape, to the
environment of the multitrack recording studio (Figure 23.4), and ﬁnally into
the age of the computer, digital media and the Web. With the introduction of
personal home recording equipment (Figure 23.5), the cassette-based porta-
studio and affordable mixer/console designs, that began in the 1980s, recording
technology was to forever change into being a medium that could be afforded
by the general masses.
When you get right down to it, the foundation of the modern information and
digital age was laid with the invention of the integrated circuit (IC). The IC has
likewise drastically changed the technology and techniques of present-day
recording by allowing circuitry to be easily designed and mass-produced at a
fraction of the size and cost of equipment that was made with tubes or discrete
transistors.

Yesterday
Advances in digital hardware and software have brought about new develop -
ments in equipment and production styles that have affected the ways in which
music is created. Integrating cost-effective yet powerful production computers
with digital mixing systems, modular digital multitracks, MIDI synths/samplers,
plug-in effects and instruments, digital signal processors, etc., gives us the recipe
for having a powerful production studio in our homes, bedrooms, or even on
the bus. Such laptop and desktop music studios have made it possible for more
and more people to create and distribute their own music with an unprecedented
degree of ease, quality, cost effectiveness and portability.
The people at New England Digital paved the way for integration of music and
digital audio production with the Synclavier system (Figure 23.6a)—a high-end
system that pushed the envelope of digital audio and gave the production
community an accurate taste of the future. It was Digidesign, however, that was
the ﬁrst to envision and create a cost-effective “studio-in-a-box” (Figure 23.6b).
This conceptual spark, which started a present-day goliath, helped create a
system that would offer the power of professional hard-disk-based audio at a
price that most music, audio and media producers could afford. (It’s important
594
FIGURE 23.5
The beginnings of the 
home recording revolution.
(a) Early multitrack home
recording system.
(Courtesy of Tascam, a
division of TEAC America,
www.tascam.com)
(b) Mackie 8-bus analog
mixing console (Courtesy
of Loud Technologies, Inc.,
www.mackie.com) and
ADAT 8-channel digital
recording system. (Courtesy
of Alesis, www.alesis.com)
FIGURE 23.6
The dawn of the DAW
revolution. (a) The New
England Digital Synclavier
system. (b) “Sound
Designer,” the first cost-
effective digital audio
workstation, was released
by Digidesign in the late
1980s. (Courtesy of Avid
Technology, Inc.,
www.avid.com)

to realize that previous to this, digital systems like Synclavier started at over
$100,000!) His goal (and those of countless others since) has been to create
an integrated system that would link together the many facets of audio, MIDI
and visual media, via the personal computer. Years later, this dream has totally
transformed music and audio production.
When it comes to understanding the tools, toys and techniques of our trade,
I’ve always felt that there are a lot of beneﬁts to be gained from looking back
into the past—as well as by gazing into the future. A wealth of experience in
design and application has been laid out for us. It’s simply there for the taking;
all we have to do is search it out and put it to good use.
On a ﬁnal note about the past, I’d like to invite you to visit my personal site
and browse through the free online booklet “The History of Recorded Sound”
(www.davidmileshuber.com/hrs). Here, you can follow the timeline of recording
from its beginning to recent time, so as to get a better idea of how the tools,
techniques and toys have developed using text, pictures and linked videos
(Figure 23.7). I hope you enjoy and beneﬁt from it!
595
Yesterday, Today and Tomorrow  CHAPTER 23
FIGURE 23.7
The History of Recorded
Sound—a free online
booklet. (www.davidmiles
huber.com/hrs)
TODAY
“Today” is a really difﬁcult subject to talk about, since new equipment comes
out on a monthly basis. However, it safely goes without saying that the following
concepts and tools help to sum up the “teens” at the dawn of the twenty-ﬁrst
century:
n retro tools and toys
n plug-ins
n apps
n the Web
n touch technology

Today
596
One of the cooler by-products of this digital age is an upsurge in “retro-future”
trends in music and technology. This comes to us in two ways: ﬁrst, digital
makes us long for the bygone sounds, feel and look of older retro tools (Figure
23.8a). These tube and retro-transistor boxes can be comfortably integrated into
either the traditional recording studio or the most up-to-date DAW project
studio, so as to give us “that sound.” Secondly, these retro devices can be
modeled and made into plug-ins (Figure 23.8b) that have much or all of the
sound of the original, which can then be integrated into any DAW system—all
at a cost that is usually a fraction of the original hardware version.
One of the more important aspects of today’s retro-reverence is a newfound
respect for tried-and-true techniques in recording. It’s a new understanding that
old ideas are not outdated but are tried-n-true techniques and tools that can
coexist with newer ones to make a musical statement. Instead of recording drums
in your bedroom, you might try recording them in a larger music room or your
local gym that has live acoustics, using both close and distant mic techniques
to get a bigger, fuller sound. You might even go further, by adding a plug-in or
impulse response reverb that simulates the acoustics of a famous recording
studio—your imagination, experience, willingness to learn and your ears are
your only limit.
On the toys side, plug-ins have been and will continue to be at the heart of
modern music and audio production innovation and for good reason, it allows
us to integrate the latest and greatest effect, sound, mutilation or enhancement
that has just arrived on the scene. They seamlessly integrate with our system to
breathe new life into our music—all in a way that often ranges from being cost-
effective or free.
On the mobile front, apps bring the on-the-go iWorld and the studio together,
they allow our phone or pad to integrate with our production lives in a way
that’s extremely portable and cost-effective. We can use our phone or pad to:
n wirelessly control and mix our DAW in an ever-growing number of ways
n call up the latest recreation of a synth in software form (in a way that can
be easily integrated into our system via MIDI, etc.)
n control and mix our live performances from on-stage or from the middle
of the performance ﬂoor
FIGURE 23.8
The retro and the virtual. 
(a) The Shadow Hills
Mastering Compressor.
(Courtesy of Shadow Hills
Industries, www.shadow
hillsindustries.com.
Used with permission)
(b) The Shadow Hills
Mastering Compressor plug-
in for the Apollo and the
UAD effects processing
card. (Courtesy of Universal
Audio, www.uaudio.com
© 2017 Universal Audio,
Inc. All rights reserved.
Used with permission)

597
Yesterday, Today and Tomorrow  CHAPTER 23
n compose a song on a plane or on the beach and import the tracks directly
into your DAW to continue on with the creative process
. . . there’s almost no limit to what is possible or what will be possible.
Of course, the ever-present mover and shaker of everyday modern life is the 
Web. Cyberspace made the creation of this book much easier for me as a writer 
(search engines and company sites make research a relative breeze) and, of course,
social media needs no introduction, being a major force in communication, 
self-expression and inter-connection.
Although many of the music share sites that contain ripped downloads of major
releases have been shut down, the biggies (such as iTunes and Google Play)
have sprung into megabuck action. Personal and indie music sites have allowed
upstart and established artists to directly sell their music, inform their fans about
upcoming tours and publish fanzine info to keep their public begging for more.
In fact, from a philosophical standpoint, I feel that the trend toward the
breakdown of the traditional music industry isn’t all bad news. I’m deﬁnitely
not alone in thinking that this shift has taken the overall power out of the hands
of a few and has given it back to the indies and the individual “many.” You
might even say that the shift toward downloadable distribution has taken music
production back into the home, where it began over a hundred years ago, when
family members gathering around the parlor piano. Now, instead, we slip into
our project or bedroom studios and play our hearts out—it’s often all part of
the same story of self-expression being told through a different medium.
Lastly, let’s touch on a powerful innovation that’s become increasingly affordable
and accessible: “Touch.” As it stares back at us from our cell phones, iPads, car
dashboards and beyond, touch technology has become an interactive part of
almost every aspect of our daily lives and for good reason, it seamlessly integrates
the virtual world with our physical world through the tactile feedback loop of
several of our major senses.
The world of recording is deﬁnitely being revolutionized through the sense of
touch. Companies like Slate Digital (Figure 23.9) have focused on the idea 
FIGURE 23.9
Slate MTx and MTi touch
controllers. (Courtesy of
Slate Media Technology,
www.stevenslate.com)

of cost-effectively integrating the DAW with the user through the use of a touch
screen and specially designed mixer software that makes the “hands-on” process
easier and more intuitive. Likewise, large-scale consoles are also getting on the
touch bandwagon for mostly the same reason—ease-of-use, instant tactile control
and visual feedback. For us users—it’s a beautiful thing.
TOMORROW
Usually, I tend to have a decent handle on the forces that might shape the
sounds and toys of tomorrow, but it’s getting increasingly harder to make
speciﬁc predictions in this fast-paced world. Today, there are far more choices
for gathering information and entertainment than by simply reading a book 
or watching TV. Now, we can interact with others in a high-speed, networked
environment that does far more than let us be just spectators; it lets us participate
and share our thoughts with others, which hopefully leads to increased know -
ledge, creative discourse and personal growth. This idea of inter com munication
through the web has changed the face of business, communi cation, and recre -
ation toward an e-based, on-demand commerce.
As I write this, digital audio has and will continue to become more portable,
more virtual and more powerful. For example, I have a killer laptop production
system that ﬁts snuggly into my audio backpack, an iPad that lets me produce
and perform my music wirelessly and my smartphone can practically cook eggs
for breakfast. Although my main studio involves keyboards, synths and music
controllers of various types, I’ve deﬁnitely welcomed the continued march
toward quality virtual instruments and useful plug-ins that seamlessly integrate
into my main or laptop DAW. I think I’ll always marvel at a computer’s ability
to be a chameleon (in its many incarnations)—one moment it’s a music
production system, next it’s a video editor, then a word processor, then a graphic
workstation, then a partridge in a neon pear tree that’ll let you talk to a buddy
across the world. This aspect of technology frees us to be creative in an amazing
number of ways that’s truly an astonishing joy.
HAPPY TRAILS
Before we wrap up the ninth edition, I’d like to take a moment to honor one
of the greatest forces driving humanity today (besides sex)—the dissemination
and digestion of information. Through the existence of quality books, trade
magazines, university and institute programs, workshops and the Web, a huge
base of information on almost any imaginable subject is now available to a
greater number of aspiring artists and technicians than ever before. These
resources often provide a strong foundation for those who are attending
accredited schools as well as those who are attending the street school of hard
knocks. No matter what your goals are in life (or in this business of music), I
urge you to jump in, read and surf through pages—keep your eyes and ears
open for new sounds, ideas, technologies and experiences. The knowledge and
skills you gain will always be well worth the expended time and effort.
598
Today

599
Yesterday, Today and Tomorrow  CHAPTER 23
On a ﬁnal note, I’d like to paraphrase Max Ehrmann’s “Desiderata,” who urges
us to “keep interested in our own career, however humble, as it’s an important
possession in the changing fortunes of time”. Through my work as a producer,
musician, writer and educator, I’ve been fortunate enough to know many
fascinating, talented and fun people. For some strange reason, I was born with
a strong drive to have music and production technology in my life. By “keeping
interested in my own career” and working my butt off (while having several
brushes with extreme luck), I’ve been able to turn this passion into a successful
career.
To me, all of this comes from following your bliss (as some might call it),
listening to reason (both your own and that of others you trust) and doing the
best work that you can (whatever it might be). As you know, thousands of able
and aspiring bodies are waiting in line to make it as an engineer, a successful
musician, a producer, etc. So how does that one person make it? By following
the same directions that it takes to get to Carnegie Hall—practice! Or as the 
T-shirt says, “Just do it!” Through perseverance, a good attitude and sheer luck,
you’ll be led through paths and adventures that you never thought were possible.
Remember, being in the right place at the right time, simply means being in
the wrong place a thousand times—“Showing up in life is huge!”
Have fun along the way!
FIGURE 23.10 
David Miles Huber, 4-time Grammy nominated artist/producer, practicing a music set in a Brussels
airport restaurant. (www.davidmileshuber.com)


1/4” connecters 119–20
3:1 distance rule 129, 161
5.0 surround 525
5.1 surround 520–4, 529–30, 553
7.1 surround and higher 525–6, 529–30,
553
45/45 cutting system 561–2
.aac 362
Ableton 267, 273, 312, 343, 345
absorption 6, 95–103, 505
Absynth 311
AC power 183, 397–9
AC3 codec 355, 529–30
accelerators 249
accent microphone 134, 139
accessories 263–4, 347
acetates 564
acoustics and studio design 72, 75–103,
327, 403–4, 494–6; absorption 96–101;
audio for visual/gaming 77–8; distant
miking 132–9; echo chamber 103;
frequency balance 92–101; isolation 75,
80–90; mic techniques 123, 126;
partitions 89–90; professional studio 4,
6, 76–7; project studio 78–9; reﬂections
93–6, 101–3; reverberation 101–3;
symmetry 91–2, see also microphone
techniques and placement; sound and
hearing
active combining ampliﬁers 393
active crossovers 502–3, 526–7
Active Sensing messages 304
active traps 101
adaptive ﬁlter 446–7
ADAT lightpipe 213–14, 232
additive synthesis 311
ADK Z-251 171
administration 24
Adobe Audition 446
ADSR 59
Advanced Audio Coding (AAC) 362
AEA A440 169
AES 210–12
Aftertouch 298
AGC 387
.aif 208, 235, 357, 528
Akai MPC1000 312
AKG C214 109, 170
AKG C460B/CK61 116
AKG C3000 117
AKG D112 167
AKG D321 116
Alesis 214
Alesis DM Dock 341
Alesis IO Dock 340
Alesis SR-18 317
aliasing 200, 204
all-thread 84
Amazon 553, 569
ambience 6, 35, 101, 490; miking 134–6,
144, 149; stereo 141, see also room
Ampex 179, 192, 591–3
ampliﬁers (amps) 389–95, 420, 511–12;
guitar 149; monitoring 502–3
amplitude 47–9, 52–4; DAW software 239,
241–3; effects 407, 411; envelope 59;
quantization 198–9, see also equalization
(EQ)
analog: aux send 409; and digital 244, 480,
483–4; effects 403–5; sync 372–3, 379,
385
analog tape recorder (ATR) 8–9, 30–2, 35–7,
175–93; backup/archiving 191–2; CLASP
601
Index

Index
192; cleanliness 189; degaussing 189;
dynamic range 421; editing 189–90;
emulation plug-ins 193; heads 180–2,
186, 188–90; mixing 456, 482–3;
monitoring modes 183–4; plug-in
emulation 420; print-through 186–7;
punch-in 184–5; surround re-issues
531–2; sync 381–2, 387; tape noise
187–8; track width 185–6; transport
178–80; vinyl mastering 564
analog-to-digital (A/D) 44–5, 201–2, 204,
215–16
Anderton, Craig 288, 330, 582
animation 368
anti-aliasing 204
Aphex Aural Exiter 419
Aphex Expander/Gate 430
API 1608 Console 459, 474
Apogee UV22 203, 546
Apollo 192, 230, 250–2, 435, 440, 546, 550
Apple 208, 529, 569; DAW 223, 235, 248;
multimedia 357, 361, 369, see also iOS;
Mac
apps 15, 234–5, 337–47, 512, 596
archiving 192, 260–1, 584–5
Argosy Halo 585
arrangers 21
artists 21, 418–19, 452, 589–90;
performances 582–3
artwork 40, 558, 577
ASCAP 572
ASIO 231–2
asperity noise 187–8
associations 28
ATH-M50x 500
ATT 362
attack 59, 423–6, 429–31, 448
attenuation pad 118
AU 248, 339
Audient 232, 461
audio books 18
Audio Cyclopedia 75
Audio Engineering Society (AES) 210–12
Audio Interchange File (AIF) 208, 235, 357,
528
Audio Stream Input/Output (ASIO) 231–2
Audio Units (AU) 248, 339
Audio-Video Interleave (AVI) 369
AudioBus 339
AudioSuite 248
Auria Pro 343
Auro3D 525–6
authoring 353, 355, 531
auto-locator devices 8
auto-tune 253, 439–40
AutoMap 316
automatic dialog replacement (ADR) 77
Automatic Gain Control (AGC) 387
automation 38–9, 221, 254, 441, 486–7,
490
auxiliary (aux) 409, 466–7
Avantone Active MixCubes 499
.avi 369
backup 259–61, 351–2, 584–5, 588; analog
191–2; patch data 305
bafﬂes 32
balanced line 119–20
balanced power 401
balloon example 45–6
bandpass ﬁlter 57, 414
bandwidth 207, 413, 447
bantam connector 120, 478
Barinworx Modus EQ bx1 551
barriers 52–3, 81–2
bass 52, 145, 416, 424, 502; guitar 151, 417,
426–7, 468; proximity effect 123–4;
subwoofer 509–11; surround 520,
523–7; traps 99–101
batteries 225–6, 400
BD-ROM 356
beat slicing 268–9
beats 68
beats-per-minute (bpm) 323, see also tempo
bedroom studio see project studio
Believe (Cher) 440
Bell, Alexander Graham 60
Bell, Mary C. 592–3
bells 56
Beyerdynamic 108, 168
bias current 182–3
BiCoastal Music 77
bidirectional microphone 113–14, 116;
placement techniques 148, 158;
proximity effect 124; stereo 141
binary 195–9, 204–5
binary-coded decimal (BCD) 374
binaural localization see spatial positioning
BIOS 226
biphase modulation 373
biscuit 565
bit 293–4
bit depth 197, 199, 202–3, 206–8, 421;
mastering 545–6; mixing 477, 482;
multimedia/web 356–8, 365
bit rate 202, 216, 236, 358, 360–2, 539
black burst 384–5
bleed see leakage
Blu-ray 207, 355, 357, 553; burning 559–60;
surround 518, 528–9, 531
602

Blue Book 354
Bluetooth 289
Blumlein technique 103, 141, 551, 592
BMI 572
bongos 160, 417
bookings 24
books 18
Boot Camp 223
Bose SoundLink 499
Boston Symphony Hall 7
bounce see export
boundary effects 52, 55, 93–4, 100, 505;
miking 136–7, 152; symmetry 91–2
brain see psychoacoustics
brass 146–8, 417, 424, 426
Bricasti M7 435
bricks 84, 97
Bridgeport Music et al. v. Dimension Films 314
Broadcast Wave Format (BWF) 208, 235–6,
357, 376
Brooks, Evan 377
Brown, James 317
brown-outs 399–400
buffer 232
Burl Audio 230
Bush, Billy 406
business 10–11, 25–6, 28, 41, 568–75,
586–9, 597
buzzes 397–402
bytes 293–4
Caballero, Emiliano 458
cabinets 150, 153–4
cabling 216–17, 226–9, 377, 478, 585–6;
grounding/power 397–402; MIDI
286–93; sync 382
camera 376, 581; adapter 340–1
capacitor 109
Capitol Records 2, 32, 130
car 421, 501
cardioid microphone 114–16; placement
techniques 149, 152, 158; proximity
effect 124; selection 166–7, 169–71;
stereo 140–1; surround Decca tree 144
career 26–9, 516, 586–9, 599
carving 489
cathode 389–90
caulk 83
CD 207–8, 279, 353–5, 421, 541, 553;
burning 558–9; database (CDDB) 353;
handling/care 560–1; manufacture
555–8
CD Baby PRO 572
CD Text 559
CD-Recordable (CD-R) 556
CD-RFS 354
CD-ROM/XA 354
CD-UDF 354
ceiling 86–7, 92–4
cello 58–9, 162
central processing unit see CPU
chairs 585
chamber 436
chamfer 84
change 1, 10–11
channel assignment 474–5; surround 521
channel fader 472–3, 481–2
channel input 463–6
Channel Pressure 298
Channel Voice 297
Cher 440
chirping 445
chorusing 433
Chrome 364
cinema see ﬁlm
CinemaScope 517
circuits 397–402
clarinet 164
CLASP 192
Class Compliance 340
classical 133–4, 147, 164, 454, 537
cleaning 189
click track 323–4, 579
clicks 446
clipping 209, 391, 394, 480–3
clock 215–17, 303–4
close miking 126–31, 155, 454
Closed Loop Analog Signal Processor
(CLASP) 192
cloud 260, 352
clutter 585–6
cochlea 64
codecs 338, 357, 359–63; surround 529–30;
video 368–9
Cole, Nat King 592
combination tones 68
combing 433
communication 221, 349
compact disc see CD
complex waves 57
compositing (comping) 35, 185, 245, 451,
582
compression (dynamics) 33–4, 38–9,
421–8, 491; DAW 250–1; mastering 537,
539, 545, 548–9; microphones 151, 163;
multiband 427–8
compression (ﬁle formats) 359–63, 368–9
compression (wave propagation) 46
computers 90, 222–6, 255–60, 350, 598;
MIDI and 306–8; mobile 337–47;
603
Index

Index
networking 260–1, see also hardware;
software
concave surfaces 52, 93
concept album 536
concert see live performance
concrete 84, 97
condenser microphone 109–12; 
boundary 137; directional response 
116; output characteristics 118;
placement techniques 145, 148, 152;
selection 169–71; transient response 
117
congas 160, 417
constant bit rate (CBR) 360
construction 83–8
Continue messages 304
contracts 571
Control Change 298–300
control panel 309
control room 6–9, 494; noise isolation 90;
reﬂections 95; symmetry 91–2
control room mix 457
controller ID 300–2, 332
controllers 233–5, 278–9; drums 318–19;
editing values 331–2; effects 441; iOS
343–5; keyboard 314–16; MIDI 291–2,
308–9, 319–20
converters: analog-to-digital (A/D) 44–5,
201–2, 204, 215–16; digital-to-analog
(D/A) 201–2, 205, 216
convex surfaces 51–2, 93
copy-and-paste 240–1, 329
copyright 314, 571, 577–8, 587
Core Audio 231–2, 338–40
corners 52, 91–2, 100
cost factors 75–6, 79
cover art 40, 558, 577
CPU 90, 223, 226, 255–6, 258, 406, 466;
MIDI 308–9; signal processing 247–9
Crosby, Bing 592–3
cross-fade 243, 541
crosstalk 382
Cubase/Nuendo 209, 220, 239, 406; dither
203; effects 250–1; groove tools 267,
275; iOS 344; mastering 546–7; MIDI
245, 299, 321, 326; mixing 462, 470,
486; monitoring 508, 519; ReWire 254;
sync 386
Cubasis 343
cue 33–4, 36; MIDI timecode 377–8; mix
457
Cuniberti, John 137
cut-and-paste 240–1, 329
cutoff frequency 413
cutting head 562–3
cycles 48, 51, 53–4
cymbal 59, 417
daisy chain 291–2
damping 155, 157–8
Dante 306
data 259–61, 352, 584–5, 588
data byte 294
databases 559
DAW see Digital Audio Workstation
DBX NR 188
DC 390, 394
de-esser 163, 251, 425
decay 59, 73–4, 102–3
Decca tree 143
decibel (dB) 60–4
decoder matrix 141–2
degaussing 189
delay 54, 70, 74, 251–2, 432–4; analog 404,
see also latency
desert island reference 450, 488, 491, 506,
528; mastering 536, 543
Desiderata 599
desktop computer 225
difference tones 68
diffraction 52–3
diffusion 94, 506
Digidesign 231, 248, 594
Digidesign DINR 444
Digidesign ICON 459
Digidesign S6 235
Digital Audio Workstation (DAW) 6, 8,
11–12, 30–9, 136, 219–64, 596–8;
analog integration 192, 405; dither 202;
documenting within 262; effects 405,
441; hardware 221–6; integration 220–1;
interconnectivity 226–9; interface
229–35; levels 209, 476; looping
267–73, 276–7, 280; mastering 541,
543–4, 547–8; MIDI 305, 326–8, 331–3;
mixdown 459–60, 483–8; monitoring
512; portable 15, 342–4; project studio
79; punch-in 185; software 237–55;
sound ﬁles 236–7; surround 522; sync
385–6; virtual insert point 465–6
digital delay lines (DDLs) 432–3
Digital Signal Processing (DSP) 171–2, 203,
219, 224, 266, 405; effects/plug-ins
247–52; noise reduction 443–4; signal
paths 409–10
digital technology 195–217; ampliﬁers
394–5; analog and 175, 480, 483–4;
dither 202–3; ﬁxed- vs. ﬂoating-point
203; Nyquist Theorem 200–1;
oversampling 201; quantization 198–9;
604

recording/reproduction process 203–5;
sampling 197–8; signal distribution
214–17; signal-to-error ratio 201; sound
ﬁle basics 206–10; sync 385;
transmission 210–14
Digital Theater System (DTS) 529
digital-to-analog (D/A) 201–2, 205, 216
digitally-controlled ampliﬁer (DCA) 394
direct current (DC) 390, 394
direct injection (DI) 128, 131–2, 137, 150–1
direct insert 465, 471
direct monitoring 232, 471
direct signal 72–3, 102, 435
Direct Streaming Digital (DSD) 357, 359
directional response 112–16, 172; mic
placement 145, 157; proximity/popping
124, 163
directories 259, 263, 328, 521
DirectX 248
disc cutting 561
Discogs 363
Disney, Walt 516
Display-Port 228
dissonance 56
distance rule 129, 161
distance techniques 9, 132–8, 149–50, 454;
drums 155; strings 161
distortion 67, 118, 410–11; analog tape
recording 183, 187–8; bit depth 207;
clipping 391, 394; dither 202; jitter 215;
loudness 209; mastering 546; mixing
462, 464, 480–3; Nyquist Theorem 200
distribution 40, 553–72; legal issues
569–72; online 565–9; signal 214–17
distribution ampliﬁers 393
dither 202–3, 546
DJing 18, 278–9, 346
docking 340–1
documentation 262–3, 278, 329, 405,
580–1
dogs 196
Dolby 356, 362, 517–18, 523–4
Dolby Atmos 525–6, 529
Dolby Digital (AC3) 355–6, 529–30
Dolby-A 188
domains 177
doors 87–8
double bass 162
doubling effect 433
down-mixing 530
downloads 40, 553–4, 565–9, 572, 597
DRA 356
drivers: audio protocols 231; monitoring
501–2; updating 257
DRM 361, 530
drum 135, 326; booth 155; controllers
318–19; hand 160–1; kit 154–60, 431;
loops 276; machine 317; pattern grid
326; risers 86, see also ﬂoor-tom; hi-hat;
kick drum; snare drum
dry track 138
DSP see Digital Signal Processing
DTS 355–6
DVD 212, 353, 355, 518, 528–9, 531, 553;
burning 559–60
dynamic microphone 106–7; directional
response 114; output characteristics 118;
placement techniques 145, 149, 151–2;
selection 166–7; transient response 117
dynamic range 33, 207–9, 249–51, 420–1;
compression 421–8; expansion 429–31;
limiting 428–9; mastering 545, 548–51;
mic placement 156–7, 163; mixdown 37,
469, 477, 481; noise reduction 443
ear 45, 56, 64–74, 506–7, 579; direction
perception 69–71; hearing loss 65–6,
507; loudness 60; psychoacoustics 66–8;
space perception 71–4
earbuds 500
early reﬂections 72, 102, 435
Earthquake 523
EBU 210–12
echo 94, 434; chamber 103, see also delay
edit 441; MIDI 285–6, 329–32;
nondestructive 239–41; software
238–43; tape 189–90
edit decision list (EDL) 381
effects 8, 38, 138, 247–52, 403–41;
automation/editing 441; compression
421–8; delay 432–4; dynamic range
420–1; equalization 411–20; expansion
429–30; limiting 428–9; mixdown
466–7, 490; multiple 440–1; noise gate
431; pitch shift 437–40; psychoacoustic
enhancement 437; reverb 434–6; signal
paths 407–11
Ehrmann, Max 599
eight-to-fourteen modulation (EFM) 556
electret-condenser microphone 112, 137,
169
electro-acoustic devices 50
electromagnetic induction 106–7
electromagnetic interference 401–2
electromagnetic noise 119, 400
electronic instruments 153–4, 308–20; iOS
346–7; keyboards 310–11; percussion
317–19; samplers 312–14, see also MIDI
electrostatic noise 118–19
electrostatic principle 109
605
Index

Index
email 352
enclosures 501–2
End of Exclusive (EOX) 303, 305
engineer 21–3, 30, 32–3, 35–6, 38–9;
assistant 23; maintenance 23–4;
mastering 23, 535–40; mixing 453–4;
monitoring 505; signal processing
418–19; tips/tricks 575, 578–83, 585
ensemble 21, 133, 244, 433, 454
envelope 59–60, 73
equal-loudness curve see Fletcher-Munson
equalization (EQ) 33, 37–9, 69; amps 392;
analog 182; DAW 249–50; drums 157;
mastering 442, 536, 547–8; mixing
467–9, 489–91; monitoring 494; signal
processing 411–20, 432; vocals 163
equivalent noise rating 118
error correction 204–5, 556
Ethernet 351, 369; audio over (AoE) 228–32
ethnic music 161
Europe 375
European Broadcast Union (EBU) 210–12,
375–6
even harmonics 56
expander 250, 429–30
export 237, 247, 254–5, 584; loops 277–8
EZ CD Audio Converter 558
FabFilter Pro-Q 250
Facebook 28
faders 38–9, 489–90
fans 90, 258
Fantasia 516
Fantom-X7 311
Fantom-XR 312
far-ﬁeld monitoring 496–7
Fast Fourier Transform (FFT) 444–5
feedback 145, 160–1; negative 391–2, 395
feeling, threshold of 65
ferrite beads 402
ﬁgure-8 113–14, 116, 167–8
ﬁles 206–10, 235–7, 518; loudness levels
209–10; size 358–61, 368–9;
transmission 210–14, see also formats
ﬁlm 16–17, 281, 421, 511, 516–18, 523,
525; acoustic studio design 77
ﬁlters 200–1, 204–5, 412–14, 464
ﬁnances 13–14, 576
FireWire 223, 225–6, 228, 230–1, 287–8,
306–7
FLAC 362, 529–30
ﬂanging 433
ﬂash card 355
ﬂats 89–90, 128, 153, 157
Fletcher-Munson curve 66, 425, 507, 563
ﬂipped mode 470, 473
ﬂoating 80, 84–5
ﬂoating point 203, 209
ﬂoor-tom 160, 417
ﬂoors, isolation 84–5
ﬂute 164–5
ﬂutter echo 94
FM synthesis 310–11, 364
FOH 345
foldback 471, 511
folders 259, 263, 328, 521
folding down 530
Foley 77
Form PA 571, 577
Form SR 571, 578
formants 146, 148, 161
formats 208; compressed 359–63;
immersive audio 528–31; MIDI ﬁles
328–9, 364; uncompressed 356–9
Fourier analysis 57
frame rate 368–9, 372, 374–5
Fraunhofer 360, 362
Free Lossless Audio Codec (FLAC) 362,
529–30
freelance engineers 23–4
freewheeling 380, 387
freezing 247, 249
French horn 148
frequency 47–53, 55; balance 75, 92–101;
EQ 411–18; isolation 81, 85–6; mixing
489; noise reduction 447; Nyquist
Theorem 200
frequency response 49–50, 58, 66, 182,
493–4; boundary effects 136–7;
microphones 116, 145, 156, 161–2
Front of House 345
front-to-back discrimination 115
functional traps 101
Funkhaus 73
Furman M-8Lx 399
furniture 585
Future Disc Mastering 40
gain 33–4, 391–2, 423–6, 429, 456, 464–5;
changing 242, 250; optimization 462;
structure 452–3; trim 463–4
Galaxy Studios 5, 17, 515
Galaxy Tape Echo 432
games 17–18, 77–8, 134, 518, 525
GarageBand 265, 346
gating 408, 431, 436, 447–8
gender 25
Genelec 497
General MIDI (GM) 284, 365–7
Germany 175, 592
606

gigs see live performance
Gilfoy Sound Studios 593
gobos 89–90, 128, 153, 157
Gone with the Wind 240
Good Rule 106, 130, 145, 416, 461, 468
Google 364, 597
Gracenote 353, 559–60
Grammys 363, 524
graphic equalizer 415
graphic user interfaces (GUIs) 272–3
graphics 367–8
Green Book 354
groove tools 265; controllers 278–9;
hardware 270–1; into DAW 277–8;
obtaining loops 279–80; software 271–7
grounding 397–402
grouping 475–6, 490
guitar 132, 148–51, 417, 426; bass 151, 417,
426–7, 468; effects 433; miking 148–51,
166; as transducer 43
gypsum 83–4
Haddy, Arthur 143
hall 436
Hammond organ 153
hand drum 160–1
hard drive 191, 223, 226, 256–60
hard knee 423
hardware 255–61; DAW 221–6; groove
270–1; MIDI 306–20; sequencers 320;
sync 383, see also controllers
harmonica 166
harmonics 55–9, 67, 69, 417; bass guitar
151; clipping 391; effects 437–8; vocals
162
Harper, Ben 126
HDCD 354
headphones 33–6, 500, 511–12
headroom 482
HEAR 507
Hear Technologies 512
hearing see ear; sound and hearing
hertz (Hz) 48
hi-hat 160, 417, 424
High-Deﬁnition Audio 206–8, 553–4
high-pass ﬁlter 413–14, 464
history 591–5; immersive audio 516–18
Hit Factory Criteria 493
hitpoint markers 268
home studio see project studio
horns 146–8, 417, 424, 426
House Ear Institute 507
house sync 384–5
housekeeping 585–6
HRM-16 512
Huber, David Miles 458, 599
hum 119, 131–2
humanizing 330–1, 433–4
hums 397–402
hypercardioid pattern 114, 116, 160, 168
I/O 229, 232, 307, 461–2; iOS 340–1;
patching 478–9
iConnect MIDI4plus 308
ID numbers 300–2, 332
iD22 507
imaging 91–2
immersive audio see surround
impedance (Z) 210–11, 216, 391;
microphones 110, 112, 118–20, 131,
149
in-ear monitor 500–1
in-the-box 8, 37–8, 459–60, 519–20
indexes 555–6
industry see business
information 598
inner ear 64
innovation 11
input strip 461, 484, see also I/O
insert 407–10, 465
insertion loss 412
instrument controllers 234
insurance 588
integrated circuit (IC) 593
integration 220–1, 350
Intel 228
interaural differences 69–71
interface 285; digital audio 229–35; MIDI
307; surround 520–1
interleaving 204–5, 255
International Telecommunications Union
(ITU) 522–3
Internet 3, 10–11, 18–19, 28, 229, 261, 279,
305; distribution 553–5, 565–9;
multimedia 349, 351–2, 369–70; radio
569; trends 597–8
Internet Service Provider (ISP) 351–2
iOS 15, 234, 288–9, 337–47, 596–8
IP address 352
ISO-9660 354
IsoAcoustics 499
isolation 6, 31, 75, 80–90; cabinets 150;
ceilings 86–7; control room 90; drums
85–6, 155, 157; ﬂoors 84–5; mic
placement 128–31, 136, 149; mixing
454–6; partitions 89–90; for reverb
102–3; vocals 163–4; walls 82–4;
windows/doors 87–8, see also leakage;
noise
iTunes 362–3, 574, 597
607
Index

Index
iZotope Insight 209, 508
iZotope Ozone 552
iZotope RX5 445–6
jacks 478–9; MIDI 289–90
Jackson, Michael 130–1, 142
jam sync 308, 380, 387
Japan 375
jazz 147, 158, 454
jitter 206, 215–16, 385
Joliet 354
K-Stereo Ambience Recovery 551
key 267, 330
key input 408, 431
keyboard 310–11; controllers 234, 314–16;
drum controller 318; electronic 153–4;
mic selection 166; piano 151–3;
sampling 312–13; workstation 320
Keys, Alicia 4
kick drum 417, 426, 431; miking 155–8,
166–7
kickboard 153
KMR Audio 5
Komplete Kontrol 234, 316, 406
Kontact Virtual Sampler 312
LA Record Plant 593
labeling 32–3, 38
lacquer 564
land 563
lane 245
laptops 225–6, 499, 598
latency 231–2
lathe 561–4
law 24–5, 574–8, 587
leakage 34–5, 55, 82–4, 116, 138, 454;
drums 156–60; guitar 150; mic
placement 128–31; off-axis 125; piano
151–3; vocals 163, see also isolation;
noise
least signiﬁcant bit (LSB) 205, 300–2, 546
Legacy 470
legal issues 569–72
Leslie cabinet 153–4
levels 33–4, 38–40, 59–64, 66–7; balancing
504–5; digital recording 209–10;
mastering 536–7, 546; metering 479–83;
mixdown 456, 489; monitoring 506–7,
see also dynamic range; gain
Lexicon plug-in 440
LFE (subwoofer) 509–11, 520, 523–9
Library of Congress 571, 577–8, 587
lightpipe 211, 213–14, 232, 521
limiter 33–4, 38, 250, 428–9, 550
line level 117
line trims 463
listening 450–2, 496–7, 499–501
Little Richard 593
live performance 273, 279, 284, 333, 436;
iOS 343–6; recording 16, 134, 324,
454–5
local area network (LAN) 260–1, 351
localization 69–71
locking 247, 249
logarithm (log) 60–1, 63
Logic 220, 462
London Bridge Studios 77
longitudinal timecode (LTC) 378–9, 381–2,
387
looping 266–8, 271–9, 346–7, 439
LoopMash 276, 347
lossless 207–8, 356–9, 362, 529–30
lossy 206, 359–63
Loudness Wars 209, see also gain; levels
low frequency effect (LFE) 509–11, 520,
523–9
low-pass ﬁlter 200–1, 204–5, 413–14, 464
LP see vinyl
LPCM 356
LSB 205, 300–2, 546
lyrics 262
M/S 141–3, 551
Mac 222–5, 231, 248, 306, see also Apple
MacBook Pro 225, 228
Macintosh HFS 354
Mackie DC16 345
Mackie FRS-2800 394
Mackie HR824mk2 495, 498
Mackie Onyx 461, 474, 476
Mackie Universal Control 233, 486
MADI 212
magnetic oxide 177, 180; shed 189
magnetism 176–84, 189
maintenance engineers 23
managers 24–6, 587
Manley Massive Passive 412
Manley Slam! 550
Manley Stereo Variable Mu 549
manuals 330, 589
manufacturing 40, 555–65
marimba 161
Mark of the Unicorn 248
markers 262
marketing 10–11, 41, 554, 566, 568
MAS 248
Maschine 278, 316
masking 69
Mass in C Minor (Mozart) 147
608

Massenburg, George 159, 515
Massy, Sylvia 464
master sync mode 35–6
master/slave 383–7
mastering 39–40, 477, 533–52, 583–4;
dither 202–3, 546; DIY? 541–5;
dynamics 548–51; engineer 23, 535–7;
EQ 547–8; for internet 566; loudness
209; ownership 571; plug-ins 551–2;
safety master 191–2; sound ﬁles 545–6;
surround 522; vinyl 564
materials 82–4, 87, 97
mathematics 47–8, 51, 60–1, 64, 80, 200
matrix 564
media see multimedia
Melodyne Editor 439
memory 156, 223–4, 226, 309, 312
metadata 353, 362–3, 376, 566–7, 569
meter display 424
metering 210, 479–83, 504–5, 527–8
metronome 323–4, 579
Meyer, Chris 377
microbar 65
microphone (mic) 31–6, 105–21, 513;
balanced/unbalanced 119–21; condenser
109–12; directional response 112–16;
dynamic 106–7; EQ 418–19; frequency
response 50, 116; harmonic balance 58;
low frequency rumble 123; off-axis
pickup 124–5; output characteristics
117–19; popping 124; preamps 121,
463–5; proximity effect 123–4; ribbon
107–8; selection 166–73; as transducer
43–5; transient response 117; trims 463
microphone techniques and placement 6,
122–66, 403–4; accent mic 139; brass
146–8; close placement 126–31; Decca
tree 143; distant placement 132–8; guitar
148–51; harmonica 166; keyboards
151–4; M/S 141–3; percussion 154–61;
psychoacoustics 69; recording direct
131–2; spaced pair 140;
stereo/immersive 139–43; strings 161–2;
surround 143–4; tuned percussion
160–1; voice 162–4; woodwind 164–6;
X/Y 140–1
Microsoft 208, 223–4, 231, 235, 248, 357,
361, 369
mid-side (M/S) 141–3, 551
MIDI 36–7, 281–335, 581; audio to
269–70, 327–8; computer and 306–8;
controller ID 300–2; DAW 234, 237,
245–6; General 284, 365–7; groove
software 273–4; instruments 308–20;
iOS 338, 341; keyboard 310–11;
keyboard controller 314–16; message
293–9; microphones 136, 138;
multimedia 363–7; music
notation/printing 334–5; percussion
317–19; ReWire 253–4; sampler 312–14;
sequencer 320–34; system
interconnections 287–93; System
messages 303–6
MIDI Clock 271
MIDI Manufacturers Association (MMA)
300
MIDI timecode (MTC) 246, 303, 308,
376–9, 381, 383–6
mini-jacks 119
minorities 25
mission statement 577
mixing 7, 37–9, 69, 89, 333, 449–91, 583;
auxiliary send 466–7; channel
assignment 474–5; channel fader 472–3;
channel input 463–6;
console/desk/board 7–8, 38, 458–61;
DAW 246–7; digital console/controller
483–8; dynamics 469; equalization
467–9; gain optimization 462–3;
grouping 475–6; immersive audio
518–21; iOS 342, 345; loudness 209;
mastering and 543–4; metering 479–83;
miking 135–6, 138, 142; monitoring
469–72, 477; output section 473–4, 477;
patch pay 478–9; signal path 460–2;
signal processing 418–19, 427; software
254–5; technology 458–60
MMA 300
mobile 14–15, 337–47, 350, 596
modes, MIDI 296–7
modulation noise 187–8
monitoring (audio) 33–4, 36, 39, 493–513,
583; analog tape recording 183–4; direct
insert 471; far-ﬁeld 496–7; immersive
audio 521–8; in-line 470–1; isolation
83–4; mixing 456–7, 469–72, 477; near-
ﬁeld 497–9, 506; psychoacoustics 67;
speaker design 501–5; symmetry 91;
taking care of your hearing 65–6, see also
speakers
monitors (screen display) 224, 257–8, 485
monophonic (mono) 140–2, 296–7, 508–9,
562
Moog 419
Mopho 311
most signiﬁcant bit (MSB) 284
motivation 27–8
MOTU Audio System (MAS) 231, 248
MOTU AVB 229
MOTU Ultralite 230
609
Index

Index
movies see ﬁlm
Mozart 147
MPEG 360, 369
MPEG-1 355
MPEG-2 355
MPEG-3 (MP3) 19, 360–1
MPEG-4 (MP4) 361, 529
MTC 246, 303, 308, 376–9, 381, 383–6
Mullin, Jack 175, 592
multiband 250–1, 447, 550
Multichannel Audio Digital Interface
(MADI) 212
multimedia 18, 349–70; delivery formats
356–63; delivery media 351–2; graphics
367-8; MIDI 363–7; physical media
353–6; video 368–9
multiport network 292–3
multitrack 30–1, 295, 324, 594
music industry see business
Musical Instrument Digital Interface see
MIDI
mute 146, 148, 472
Nanologue 347
National Television Standards Committee
(MTSC) 374–5
Native Instruments 316, 406
Native Kontrol Standard (NKS) 316
near-ﬁeld monitoring 497–9, 506
Needham, Mark 79
negative feedback 391–2, 395
neoprene 84–5
network access points (NAPs) 352
networking (computing) 226, 260–1, 351
networking (MIDI) 292–3
networking (social) 27–9, 587
Neumann KH310 498
Neumann M50 143
Neumann TLM102 170–1
Neve 121, 131, 212, 459, 549
New England Digital 594
nibble 294, 378
Nocturn 316, 406
noise 80, 84, 132, 258; analog tape
recording 187; calibration 527–8; dither
202–3; electromagnetic 119, 400;
electrostatic 118–19; gate 431, 447–8;
grounding/power 397–402; jitter 215;
line balance 119, 121; reduction 176,
188, 387, 429–30, 443–8, see also
isolation; leakage
non-drop code 374–5
nondestructive edit 239–41
normaled connection 478
normalization 242, 428, 546
notation printing 334–5
notch ﬁlters 415
Note-On/Off 297, 309, 320, 324
notepad 262
Novation 316, 406
Nuendo see Cubase
Nyquist Theorem 200–1, 205
obstacles 52–3, 81–2
octave 56
odd harmonics 56, 58
off-axis pickup 124–5
offset times 381–2
Olson, Harry F. 101
.omf 236
Omni On/Off 296–7
omnidirectional microphone 113–14, 116,
454, 495; Decca tree 143; off-axis
coloration 125; placement techniques
145, 148; proximity/popping 124
OmniMic V2 Precision Measurement System
495
Omnisphere 311
Open Media Framework Interchange
(OMFI) 236
operating system (OS) 222–4, 256, 406; iOS
338–47
operational ampliﬁer (op-amp) 391
Orange Book 354
orchestral music 4, 18, 139, 143
organ 133, 153, 417
oriented strand board (OSB) 85
OS X 231, 248
oscillators 431
output bus 473, 477; surround 522, see also
I/O
Oven Studios 4
overdrive 176, 394, 480
overdub 4, 6, 34–7, 89, 418; analog tape
recording 183–4; DAW 244–5;
microphones 144, 149, 164; mixing
457–8; monitoring 511; piano 153
overhead mics 158–9
overload distortion 118
oversampling 200
overtones 55–6, 67; brass 146–7; guitar
148–9; strings 161–2; woodwind 164
ownership 571
Oxford plug-ins 412, 419, 546
oxide 177, 180, 189
Oxide Tape Recorder 420
Ozone 546
PA 161
pad 267
610

pain, threshold of 65
Paisley Park 5
pan pots 472–4
Pandora 569
panning 71, 143, 149
parallel patching 479
parallel processing see sidechain
parametric equalizers 415
Parnell, Mandy 533
partials 55–6, see also overtones
partitions 89–90, 128, 153, 157
passband 413–14
patch bay 478–9
patch data 305
patch map 366–7
pattern sequencing 325
payment 572
PC 222–3, 248, 306
PCIe 228
PCM 204–5, 208, 235, 355, 357, 528–9
peak meter 483
peak program meters (PPMs) 479
peak-to-peak value 48, 59, 480–1
peaking ﬁlter 412–13
people 20
perceptual coding 359–60
percussion 56, 154–61, 417; General MIDI
367; MIDI 317–20; tuned 160–1
performance rights organizations (PROs)
572
period 51
peripherals 306–7
phantom power 108, 111–12, 288
phase 53–5, 93; -reverse 464; effect 432–3;
microphones 136, 140, 158–9;
monitoring 509–10, 513; speakers 
503–4
Phillips 357; sound ﬁles 211–13
phonorecord 571, 578
piano 151–3, 417
pickup see microphone
pink noise 495, 504, 527
pinna 64, 70–1
pitch 67–8; shift 252–3, 266–8, 270–2, 330,
437–40; vinyl manufacture 563–4
Pitch Bend Change 298
pixels 367–9
planning see preparation
plasterboard 84
plate 436
playback 203, 205; MIDI 332–3
plug-ins 310, 405–6, 596; analog tape
emulation 176, 193; DAW 237, 
248–53; groove/loop-based 276–7;
mastering 551–2; mixing 465–6; noise
reduction 444–8; sound-shaping 
419–20
PMC QB1-A 497
polarity 112–16; drum miking 156, 158;
mic selection 172; speakers 503–4; stereo
140–1
polyphonic 296–7, 311–13
Polyphonic Key Pressure 297
polyvinyl chloride (PVC) 177
Poniatoff, Alexander M. 592
popping 124, 145, 163
pops 446
port 309; vs. jack 292
portable studio 14–15, see also mobile
post-production 77
posture 585
power 63, 225–6, 397–402
power ampliﬁers 393–4, 511
power conditioning 399–400, 588
power phases 400–1
power supply 110–12
Powerplay Pro-8 512
practice 577, 579, 599
preampliﬁer (preamp) 392, 463–5;
microphone 110–12, 118–19, 121
Precision Limiter 550
Precision Multiband 550
preparation 29–30, 146, 323, 387, 573–4,
576–8; mastering 538–9; for mixing 
454
Presonus ADL-600 464
Presonus DigiMAX D8 121, 232
Presonus HP4 512
Presonus Studio 230, 238
Presonus StudioLive 484
pressure-zone trap 100
Primacoustic Recoil Stabilizer 499
print-through 186–7
printing: music notation 334–5, see also
export
Pro Convert 236
Pro Tools 220, 222, 248, 250–1, 275, 299,
321, 432, 521; mixing 462, 474, 486
processor see CPU
producers 21–2, 30, 540, 574–5, 578–83;
mixdown 38; overdub 36–7; signal
processing 418–19
Producers and Engineers Wing 191, 208,
357, 363, 524, 528
professional studio 3–11, 578–80; control
room 6–9; design 76–7; digital age
10–11; management 24; mic techniques
122; monitoring 511–13; retro
revolution 9–10
professionalism 589
611
Index

Index
Program Change 298
project studio 10–14, 23, 29, 597; acoustic
design 76, 78–9; making it pay for itself
13–14; mic techniques 122
Propellerhead 253–4, 268, 273–5, 310
Proper, Darcy 533
ProTools Multiband Dynamics 550
proximity effect 123–4, 156–7, 163
psychoacoustics 66–8, 359, 437, 447;
auditory perception 66–9; direction
perception 69–71; space perception 
71–4
publishing rights 570
pulse-code modulation see PCM
Pulse-Density Modulation 359
punch-in 35, 184–5, 243–4, 324–5, 451,
458
Putnam, Bill 592
PZM-6D 137
Q 249, 412–15
Quadraphonic Sound (Quad) 517, 520, 593
quantization 198–9, 201–2, 207, 330–1
quarter-frame messages 303, 378
quarter-wavelength trap 100
QuickTime 361, 369, 529
R/2R network 205
rack-toms 159–60, 417
Radial Engineering 137
radiation patterns 59
radio 252, 508–9, 572; internet 569
radio frequency (RF) 397, 400–1
Radiobu 78
RAID 260, 351
random access memory (RAM) 156, 223–4,
226, 309, 312
rarefaction 46
raster graphics 367
Raven 79, 234
RCA 117, 516
re-issues 531–2
read mode 487
read-only memory (ROM) 309
Real-Time Audio Suite (RTAS) 248
reamping 137–8
Reaper 238
Reason 254, 273–5
recording 30–4, 454–8; digital process
203–5; direct 131–2; iOS 341–4;
monitoring 511–13; preparation/tips
579, 581–3; sequencer 322–6; software
238–43, see also acoustics and studio
design; professional studio; project
studio
Recording Industry Association of America
(RIAA) 212
ReCycle 268–9
Red Book 353, 541, 558
redundant array of independent drives
(RAID) 260, 351
reference tones 191
reference track 452, 540
reﬂections 435; miking 136–7, 148;
monitoring 497, 505–6; sound/hearing
51–2, 55, 70, 72–3; studio design 6,
91–8, 102
regions 239–40
rehearsal 577, 579, 599
release 59, 424–6, 429–31, 448
reproduction see playback
research 19–20
resistance 118, 205, 390–1
resistors 110–11
resonances 497–8
Resource Information File Format (RIFF)
208, 235, 357
resumes 28
retro 9–10, 126, 175, 591, 596
reverberation (reverb) 38, 73–4; effects
processing 404, 434–6, 466; miking
141–2; studio design 75, 101–3
reverse 436
ReWire 253–4, 275, 277, 310
REX ﬁles 268–9
rhythm 265, see also groove tools
RIAA 212
ribbon microphone 107–8, 111, 145;
directional response 113–14; noise rating
118; selection 167–9; transient response
117
rights 570
ring tones 284
ripping 19, 314, 597
risers 85–6, 157
Rock Ridge 354
Rockwool 84, 89, 98
Roland 270, 317, 365
roll-off 124, 132, 149
Romeo 354
room 10, 72–3, 436; miking 126, 134–6,
144, 149; mixing 488, 490; monitoring
494–6, 506, see also acoustics and studio
design
root-mean-square (rms) 48, 59, 428, 480–1,
546
royalties 363, 570–2
Royer Labs R-121 108, 167–8
RT60 102–3
RTAS 248
612

rubber band control 254, 487–8
rumble 123
rundown 33, 36
running order 536, 540–1
S/MUX IV 213
S/PDIF 211–13
safety master 191–2
sales 41, 565–7, 569, 572
sample rate 196–201, 204, 206–7, 216, 236;
mastering 539, 545–6; multimedia/web
356–9; pitch shift 438; sync 385
sample-and-hold (S/H) 202, 204–5
samplers 312–14
samples see groove tools; looping
saturation 390–1, 420
saving ﬁles 254, 259–60, 405, 489–90; MIDI
328–9
sawtooth waves 57
saxophone 165
Schmitt, Al 125, 130
SCMS 212
scoring 245–6
scratch vocals 88, 164
screenshots 263
SD card 355
SDDS 355
Secure Digital Music Initiative (SDMI) 362
selectable frequency equalizer 414
selective synchronization (sel-sync) 183–4
sends 409–10, 465–7, 511
sensitivity rating 117
separation, acoustic 75, 101, 128, 152–3
separation loss 189
sequencing (electronic) 245–6, 273–4, 285,
295, 320–35; audio to MIDI 327–8;
MIDI documentation 329; MIDI editing
329–32; MIDI mixing 333; MIDI
playback 332–3; MIDI recording 322–6;
MIDI to audio 327; saving MIDI ﬁles
328–9
sequencing (running order) 536, 540–1
Serial Copy Management System (SCMS)
212
servers 260–1, 351–2
SESAC 572
setting up 580
Shadow Hills Mastering Compressor 596
sharing 261
shelving ﬁlter 413, 469
shielding 402
Shure MV88 342
Shure PGA181 169
Shure SM57 166
Shure SM58 107, 117
sibilance 162–3, 417
sidechain 408, 410–11, 431
Sides, Alan 122, 142–3
signal chain/path 407–11, 460–1; mastering
544–5
signal distribution 214–17
signal processor 403, 466–7, see also Digital
Signal Processing (DSP); effects; plug-ins
signal-to-error ratio 201, 207
signal-to-noise (S/N) ratio 187–8, 201–2,
456
simple waves 57
sine waves 48, 53–5, 57–8
single-ended noise reduction 445–7
Skibba, Martin 458
Skywalker Sound 17, 283
slap echo 94
Slate Digital 597
slip time 331
slope 414, 429; ratio 423
SMPTE 191, 246, 303, 308, 372–81, 385,
387; offset times 381–2; signal
distribution 382
snare drum 59, 158, 160, 417, 426–7, 431,
468; mic selection 166
social networking 587
Society of Motion Picture and Television
Engineers see SMPTE
sofﬁts 83–4, 496
soft knee 423
software 237–55, 257, 588–9; comping 245;
DJ 278–9; drum replacement 319; DSP
effects 247–8; DSP plug-ins 248–53;
exporting to ﬁle 254–5; ﬁxing sound
243–4; groove 271–7; MIDI
sequencing/scoring 245–6;
mixdown/effects automation 254; mixers
483–8; real-time mixing 246–7;
recording/editing 238–43; ReWire
253–4; sequencers 320–1; video/picture
sync 246, see also digital signal processing
(DSP); plug-ins
solid state drive (SSD) 223, 226
Solid State Logic (SSL) 212–13, 236; Duality
Console 458, 463, 469, 472, 477;
Nucleus 233
solo 33–4, 139, 472
Song Position Pointer (SPP) 303
Song Select messages 303
Sonnox 250, 412, 546
Sony 211–12, 357, 362
Sony MDR-7506 500
Sony/Phillips Digital Interface (S/PDIF)
211–13
sound ﬁles see ﬁles; formats
613
Index

Index
sound and hearing 43–74; diffraction 52–3;
direction perception 69–71; loudness
levels 60–4; pressure waves 45–6;
reﬂection 51–2; space perception 71–4;
transducers 43–5, see also ear
sound hole 148–9, 162
sound lock 88
sound-pressure level (SPL) 60–2, 65–6, 81,
583; miking 118, 146; monitoring
504–5, 507; surround 527–8
sound-shaping 419
Soundexchange 572
Soundness SoundSoap 444
soundtracks 281, 283, 516–17
spaced pair 140, 158–9, 161
Spark Drum Machine 278
spatial positioning 7, 37–8, 69–71, 437, 500
speakers 493–5, 583; active/passive 502–3,
526; balancing levels 504–5; design
501–5; frequency response 50; harmonic
balance 58; immersive 516–17; polarity
503–4; studio playback 513; surround
522–6; symmetry 91–2; types 496–501,
see also monitoring
specs 50, 206, 216
spectral content 411; analyzer 507–8
spring 436
square waves 57–8, 373, 379
SSD 223, 226
SSL see Solid State Logic
stampers 557, 565
standing waves 93–4, 494
Start messages 304
status byte 294
Steinberg 447; DAW 230–1, 248, 253–4;
groove tools 275–6; immersive 520–1;
iOS 341, 343, see also Cubase/Nuendo
stems 18, 539–40
step time 325, 334
stepped resistance network 205
stereophonic (stereo) 474, 509; + sub
509–10; compression 425; history
516–18; miking 122, 133–6, 139–43,
149, 152; mixing up/down 530–1
Stop messages 304
stop-band ﬁlter 204, 413
streaming 554, 568–9
strings 57–8, 161–2, 417
Studer A800 192
studio see acoustics and studio design;
professional studio; project studio
Studio Metronome 16
studio musicians 21
StudioLive 342, 345
studs 82–3
Stylus RMX 276
subcodes 555–6
subtractive synthesis 311
subwoofer 509–11, 520, 523–9
summing ampliﬁers 393
Super Audio CD (SACD) 357
supercardioid pattern 114, 158
surface density 81–2
surge protection 399–400, 588
surround 17, 515–32; authoring 531;
formats 528–31; history 516–18; miking
143–4, 149; mixing 518–21; monitoring
521–8; re-issues 531–2
sustain 59
Swartau, Bill 592
Swedien, Bruce 130–1, 142
symmetry 91–2, 494
Synchron Stage 7
synchronization (sync) 266, 308, 371–88;
jam sync 380; MIDI 376–9, 383–6;
monitoring mode 183–4; timecode
372–9
Synclavier 594–5
synthesizers (synths) 270, 274–5, 295–6,
305, 310–11, 347, 426
System messages 303–6; -Common 303; -
Exclusive (sys-ex) 289–90, 293, 304–6,
377; Real-Time 303; Reset 304
tagging see metadata
tails-out 186–7, 192
tape see analog tape recorder (ATR)
Tascam Digital InterFace (TDIF) 214
tax 13
TC Electronic 440, 531
TDM 248
technology: history 591–5; trends 595–9, see
also digital technology
Telefunken 107, 166–7, 173
television 350, 517–18
tempo 252, 266–9, 271–2, 323, 439; -based
delay 434, see also synchronization
(sync); time
temporal fusion 73
Tesla, Nikola 592
test pressing 558
Thomson Multimedia 360
thresholds: of hearing/feeling/pain 65;
signal processing 421–7, 446–8, 
549–50
Thunderbolt 223, 225–6, 228, 230–1,
287–8, 306
timbre 58, 67, 73, 127, 488
time 13, 215–17; -based effects 409, 411,
432–6; quantization 198–9, 201–2, 207,
614

330–1; sampling 197–8; shift/stretch
252–3, 266–8, 271–2, 438–9, see also
synchronization (sync); tempo
Time Domain Multiplex (TDM) 248
timecode word 373–5, see also MIDI
timecode (MTC); SMPTE
Timing Clock messages 303–4
Todd-AO 517
tonal balance see timbre
Toslink see lightpipe
total transport logic (TTL) 179
touch 234–5, 337, 483–4, 597–8
Touchable 345
Townsend Labs Sphere L22 171–2
track sheets 32–3
transducers 43–5, 64, 105, 196
transients 117, 156, 425–6, 429
transistors 389–90, 394
transitions 38, 541
transmission loss (TL) 81–4, 87–8
transmission standards 210–14
transports 178–80
transposition 330
trauma, acoustic 65
treble 132, 428
trends 595–9
triangle waves 57
triggering 319, 326
Trilogy Studios 4
triode 390
Tritschler, J. 9
trombone 59, 147
trumpet 146–7
TT connector 120, 478
tuba 147
tube (valve) 110–11, 171, 173, 404; amps
389–91; EQ 415
tumbas 160
Tune Request messages 303
TuneCore 572
tuning 34; drums 155–6; speakers 494–6,
510
turnover frequency 413
tweeters 523–4
UAD 192, 250–2, 428–9, 435, 440, 546,
550
UK 109, 471
Ultrapatch PX2000 478
unbalanced lines 120–1
underlay 84
Uniform Resource Locator (URL) 352
uninterruptible power supply (UPS) 400,
588
United States 375
Universal Audio 2–610 121
Universal Audio 2–610S 464
Universal Audio 1176LN 422
Universal Audio API Vision Strip 448
Universal Audio Precision Multiband 447
Universal Recording 592
universal serial bus see USB
up-mixing 530–1
updates 224, 257, 588
upper partials see overtones
US 572; Court of Appeals 314
USB 223, 225–7, 230–1, 287–8, 306–7
V-Control Pro 234, 344, 486
valve see tube
variable bit rate (VBR) 360
VCA grouping 476
vector graphics 368
Velcro 585
velocity 50–1, 72, 313
ventilation 258
Vertigo Sound VSM-3 142
vibraphones 161
vibrato 153–4
video 246, 368–9, 531; acoustic studio
design 77; audio for 16–17; sync 374–5,
379, 381, 384–8
video cassette recorder (VCR) 518
vintage 107, 171, 173
vinyl 517, 553; manufacture 561–5;
mastering 533–4
violin and viola 57–8, 161–2, 417
virtual pot (V-pot) 484
Virtual Studio Technology (VST) 231, 248
visibility 87, 90, 130
vocals 511; booths 89, 128, 130; effects 433;
miking 162–4, 166, 170; mixing 452,
469, 489; processing 417, 426; scratch
88, 164; studio tips 579, 582
vocoder 408
voice coil 107
voice (electronic) 296, 309, 313
voice (human) see vocals
voltage (V) 62, 389–91, 399–400
voltage-controlled ampliﬁer (VCA) 394
volume see dynamic range; gain; levels
VST 231, 248
VTR 379
VU metering 479–83
Wallace, Roy 143
wallboards 83
walls 52, 93–4, 148; isolation 82–4
Warm Audio EQP-WA 415
warping 253, 268
615
Index

wattage see power
.wav 208, 235, 357, 528
wave propagation 45–7
waveform 47–60; amplitude 47–8; envelope
59–60; frequency 47–50; harmonic
content 55–9; phase 53–5;
simple/complex 57; velocity 50;
wavelength 51–3
wavelength 51–3
Waves L1 Ultramaximizer 429
wavetable synthesis 311
WDM 231
website 567, 587, see also Internet
Wells, Frank 20
wet/dry 411
White Book 354
Wi-Fi 234
Williamson, Greg 346
windows 87–8
Windows Media Audio (WMA) 361
Windows (Microsoft) 208, 223–4, 231, 235,
248, 357
wireless 234, 288–9, 512
wiring see cabling
Wisseloord Studios 533
.wma 361
women 25
woodwind 164–6, 417
word length see bit depth
wordclock 216–17, 246
Wordpress 28
write mode 487
writer 570–1
X-fade 243, 541
X/Y stereo 135, 140–1; guitar 148–9
XLR 119–20
xylophones 56, 161
Yamaha HS8 498
Yamaha KX49 234
Yellow Book 354
YouTube 554, 569, 574
Z see impedance
Z channels 86–7
zero-crossing points 239
zones 313
616
Index

